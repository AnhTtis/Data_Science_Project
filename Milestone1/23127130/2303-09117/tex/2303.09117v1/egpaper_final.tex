\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbding}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{multicol}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Visual-Linguistic Causal Intervention for Radiology Report Generation}

\author{Weixing Chen\\
Sun Yat-sen University\\
{\tt\small chen867820261@gmail.com}\\
\and
Yang Liu\thanks{Corresponding author is Yang Liu}\\
Sun Yat-sen University\\
{\tt\small liuy856@mail.sysu.edu.cn}\\
\and
Ce Wang\\
Institute of Computing Technology, \\Chinese Academy of Sciences\\
{\tt\small wangce@ict.ac.cn}\\
\and
Guanbin Li\\
Sun Yat-sen University\\
{\tt\small liguanbin@mail.sysu.edu.cn}\\
\and
Jiarui Zhu\\
Hong Kong Polytechnic University\\
{\tt\small zhujiarui42@gmail.com}\\
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Liang Lin\\
Sun Yat-sen University\\
{\tt\small linliang@ieee.org}
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
   Automatic radiology report generation is essential for computer-aided diagnosis and medication guidance. Importantly, automatic radiology report generation (RRG) can relieve the heavy burden of radiologists by generating medical reports automatically from visual-linguistic data relations. However, due to the spurious correlations within image-text data induced by visual and linguistic biases, it is challenging to generate accurate reports that reliably describe abnormalities. Besides, the cross-modal confounder is usually unobservable and difficult to be eliminated explicitly. In this paper, we mitigate the cross-modal data bias for RRG from a new perspective, i.e., visual-linguistic causal intervention, and propose a novel Visual-Linguistic Causal Intervention (VLCI) framework for RRG, which consists of a visual deconfounding module (VDM) and a linguistic deconfounding module (LDM), to implicitly deconfound the visual-linguistic confounder by causal front-door intervention. Specifically, the VDM explores and disentangles the visual confounder from the patch-based local and global features without object detection due to the absence of universal clinic semantic extraction. Simultaneously, the LDM eliminates the linguistic confounder caused by salient visual features and high-frequency context without constructing specific dictionaries. Extensive experiments on IU-Xray and MIMIC-CXR datasets show that our VLCI outperforms the state-of-the-art RRG methods significantly. Source code and models are available at \url{https://github.com/WissingChen/VLCI}.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

Radiology images (e.g., X-Ray, MRI) are widely used in clinical procedures, providing important evidence for disease analysis and medical intervention~\cite{zhou2021review, elmore2015diagnostic}. Nevertheless, observing suspicious lesions and writing a coherent diagnosis report is time-consuming, even for experienced radiologists. Furthermore, inexperienced radiologists often fail to capture tiny abnormalities due to the high requirement for clinical knowledge. To relieve these issues~\cite{liu2021exploring},
automatic radiology report generation (RRG) has emerged and attracted growing interest in recent years.

Similar to image captioning, RRG extracts the features from the medical images and generates a reliable report.
However, the current RRG suffers from the following four challenges different from image captioning: 1) longer sentence generation, 2) more sophisticated linguistic and visual semantic patterns, 3) the abnormal regions within a radiology image are much smaller than that of the normal ones, and 4) the inherent anatomical structures within radiology images are challenging to be diagnosed, whereas entities in natural images are diverse and easily distinguishable. 
Therefore, these challenges present a substantial limit to modeling visual-linguistic interactions and learning informative cross-modal representations for accurate radiology report generation~\cite{chen2020generating}.
% 现有工作和不足
Great efforts have been devoted to solving these issues, such as additional knowledge~\cite{liu2021exploring}, memory-driven module~\cite{chen2020generating}, and comparison with normal samples~\cite{liu2021contrastive}. Actually, most of the previous methods aim to detect abnormalities, which can help discover potential lesions and list tiny abnormalities for accurate long-sequence generation. 
However, these methods usually focus on training computationally expensive models based on a large amount of samples and task-specific knowledge\footnote{These methods build the template or knowledge database laboriously, making it hard to transfer those approaches directly to other datasets~\cite{yang2022knowledge}}.
% 此外，这些方法的目的都是为了检测出异常，通过在bias中找到潜在病灶并列出来以声场长序列报告。
% 因此不依赖外部数据进行异常发现的轻量模型是解决该问题的关键。
Actually, there exist significant visual and linguistic biases in numerous image-text data, as shown in Figure~\ref{fig:bias} (a-c). Therefore, lightweight models that can mitigate the cross-modal data bias are more essential for RRG to accurately discover abnormalities and generate reports~\cite{you2021aligntransformer, nooralahzadeh2021progressive}.

\begin{figure}[t]
 \centering
    \includegraphics[width=1\linewidth]{fig/bias.pdf}
    \caption{The visual-linguistic spurious correlation examples on IU-Xray dataset, where the colored texts in images are from the radiology reports and also describe as the colored box in radiology images. 
    (a-b) shows the ground truth of two different RRG samples from the training set, and (c) is the sample from the testing set. 
    The visual confounder (the visual feature of the heart) and linguistic confounder (the description of heart size) from (a-b) lead to spurious correlations and cause the wrong description of (c). 
    (d) demonstrates the mechanism of causal intervention via the structural causal model (SCM).
    The Baseline tends to capture the spurious correlations and probability, while our VLCI can estimate the mediator by accumulating the probability of each sub-distribution and calculating the deconfounded probability of the correct word.
    }
    \label{fig:bias}
\end{figure}

%%%%%%%%% abnormal -> causal (unbias and help long sequence generation)
% abnormal -> spurious correlations
Actually, the most essential difficulty in abnormalities detection is the existence of the visual and linguistic biases that lead to entangled cross-modal features, i.e., spurious correlations, causing the incorrect report in the model prediction with confounders, as shown in Figure~\ref{fig:bias}. 
Here, we can consider some high-frequently appearing concepts in the linguistic and visual modalities as the confounders. %, as shown in Figure~\ref{fig:VLCI} (c).
% visual confounder -> heart contour, heart -> pleural effusion
Specifically, the ``enlarged heart" is frequently accompanied by ``pleural effusion", leading to a spurious correlation between the multi-modal feature of the heart and the pleural, which causes the neglect of ``small pleural effusion" in Figure~\ref{fig:bias}. 
Different from the cascaded transformer-based feature extraction method~\cite{nguyen2022grit}, the mediator $M$ (VLCI in Figure~\ref{fig:bias} (d)) can be considered as the intervention conditions that adjust the probability distribution of features to mitigate entanglement, rather than the direct feature extraction for report generation (Baseline in Figure~\ref{fig:bias} (d)). Therefore, it is challenging to generate accurate reports that reliably describe abnormalities due to the existence of confounder. 
% linguistic confounder -> bone, bony structures -> intact
% Besides, most bony structures are intact in the training reports, leading to the spurious correlation between the description of bone and the word ``intact", and therefore the baseline tends to ignore the word ``fracture" in Figure~\ref{fig:bias}(e).

% spurious correlations -> causal inference (back-door intervention)
To mitigate visual-linguistic spurious correlations, causal intervention for image captioning often assume that the confounder is observable and alleviates the problem via back-door intervention, by approximating the observable confounder using a well-trained visual object detector or a well-constructed linguistic dictionary to cut off the shortcut path~\cite{wang2020visual,liu2022show}. Similarly, there also exists the data bias
problem in RRG, and causal intervention can mitigate the visual-linguistic biases and improve the reliability of the generated report description. 
% limitations
% the universal clinic object detector and the confounder dictionaries
However, for RRG task, due to the complex data biases in visual and linguistic modalities, it is hard to represent the confounder explicitly. Fortunately, the front-door intervention gives a feasible way to calculate the confounder.
% front-door 
Therefore, we utilize front-door intervention to implicitly mitigate cross-modal confounder and discover the true visual-linguistic causality by introducing an additional mediator involved in RRG~\cite{liu2022cross, yang2021causal}. 
With front-door intervention, our model eliminates the spurious cross-modal correlations effectively and generates an accurate description of ``small pleural effusion", as shown in Figure~\ref{fig:bias}.

% 1. report 
% 2. bias and long sequence -> abnormal 
% 3. abnormal -> spurious correlations -> confounder
% 4. confounder -> deconfound | back door -> front door
% 5. approach

% motivation
Motivated by the effectiveness of causal inference in deconfounding such cross-modal bias, we propose a lightweight cross-modal causal intervention framework for RRG without the observable confounder assumption, named Visual-Linguistic Causal Intervention (\textbf{VLCI}), to mitigate the visual and linguistic data biases. 
We combine Prefix Language Modeling (PLM) and Masked Image Modeling (MIM) for cross-modal feature alignment in pre-training. To mitigate the visual and linguistic biases, we propose the visual deconfounding module (\textbf{VDM}) and linguistic deconfounding module (\textbf{LDM}) based on the causal front-door intervention paradigm.
% detail
The visual mediator is constructed by local detail information (e.g., lung texture) and global contour (e.g., pleural contour) from radiology images, targeting to discover and disentangle the visual feature. The linguistic confounder can be eliminated by the LDM, which estimates the change in the probability of word embedding caused by visual details and linguistic context. 
In summary, our main contributions are as follows:
\begin{itemize}
    % 适用多种数据情况
    % causal 主要贡献， VDM和LDM分别针对的问题是什么，你如何解决，要总结出来
    \item  To implicitly mitigate cross-modal confounders and discover the true cross-modal causality, we propose visual-linguistic causal front-door intervention modules VDM and LDM. The VDM aims to disentangle the region-based features from images in the encoder, and the LDM aims to eliminate the spurious correlations caused by the visual-linguistic embedding.
    % vlp 预训练你是有改进现有方法？有的话要体现出来
    \item To alleviate the problem of unpaired data when pre-training visual-linguistic RRG data, we combine the PLM and MIM for cross-modal pre-training in various data situations (e.g., unpaired, single modality), which is efficient and easy to implement.
    % 为什么不需要额外的标注数据
    \item We propose a lightweight Visual-Linguistic Causal Intervention (VLCI) framework for RRG, which introduces mediators without additional knowledge, to implicitly deconfound the visual-linguistic confounder by causal front-door intervention. Experimental results show that VLCI achieves state-of-the-art performance on two datasets IU-Xray and MIMIC-CXR.
\end{itemize}

%------------------------------------------------------------------------
\vspace{-10pt}
\section{Related Work}
\vspace{-5pt}

\subsection{Image Captioning}
Image captioning aims to understand image information and describe it in text, which mainly adopts the encoder-decoder framework~\cite{stefanini2022show}. Generally, image features extracted by the encoder are fed into the decoder, often based on recurrent neural networks (RNN) and transformers~\cite{devlin2018bert}. The recent work achieved great success in this task~\cite{anderson2018bottom, guo2020normalized, herdade2019image, li2019entangled}, which presented the spatial relationships of regional features, and rely on the integration of visual and semantic data to improve performance. 
Compared with the image captioning approaches, the RRG has similar structures~\cite{stefanini2022show}.
Nevertheless, image captioning usually generates a single sentence to describe the main entities, while the RRG focuses on the potential subtle abnormalities areas in medical images and generate longer sentence from more sophisticated visual-linguistic semantics.

\begin{figure*}[h!]
   \centering
  %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
\includegraphics[width=1\linewidth]{fig/VLCI.pdf}
   \caption{The overview of our Visual-Linguistic Causal Intervention (VLCI) framework, which consists of a Visual Representation Learning Module (VRLM), a Visual Deconfounding Module (VDM), and a Linguistic Deconfounding Module (LDM).
   The ResNet backbone uses the first three blocks of ResNet101, and the text embedding layers are weight-sharing. 
   Specifically, the VDM explores visual bias via local sampling and global sampling. The LDM estimates linguistic bias via a vocab dictionary and visual features.}
   \label{fig:VLCI}
\end{figure*}

\subsection{Radiology Report Generation}
Recently, RRG methods have followed the works of image captioning and have shown remarkable performance. For the issues above, the knowledge-aware module~\cite{liu2021exploring, yang2022knowledge, wang2022medical}, template retrieval module~\cite{liu2021contrastive}, and memory-driven module~\cite{chen2020generating, nooralahzadeh2021progressive} are used to generate useful reports. However, it still has some limitations. 
% knowledge
For the data bias, PPKED~\cite{liu2021exploring} and RG-GSK~\cite{yang2022knowledge} explored and distilled the different kinds of knowledge for RRG, which needs to be annotated. 
% template
Following that, CA~\cite{liu2021contrastive} and CMCL~\cite{liu2022competence} utilized the comparison of data differences to enact the training strategy, which needs more data to estimate distribution.
% memory
Chen et al. proposed to generate reports with a memory-driven transformer but inference with slow speed~\cite{chen2020generating}. Moreover, M2TR~\cite{nooralahzadeh2021progressive} and MSAT~\cite{wang2022medical} integrate memory into attention, while needing more computational resources to generate reports.
In summary, the visual-linguistic bias hinders the promotion of the application of RRG, while our lightweight VLCI implicitly mitigates cross-modal confounders and
discovers the true cross-modal causality by causal front-door intervention and reducing the dependency on additional annotation for discovering the abnormalities.

\subsection{Causal Inference}
Causality provides a new methodology to design robust models via the elimination of spurious correlation~\cite{glymour2016causal, pearl2018book,liu2022causal}. Causal inference estimates the hidden causal effects in the distribution while significantly improving the model's generalization. It mitigates confounders through back-door, front-door intervention, or counterfactual intervention~\cite{yang2021deconfounded}.
For example, Wang et al.~\cite{wang2020visual} improved Faster R-CNN by causal back-door intervention to obtain a more robust object detection model, which improves the performance of VQA and image captioning.
However, the confounder is usually  unobservable and elusive, thus front-door intervention and counterfactual intervention can be applied~\cite{yang2021causal, wang2021causal}.
Therefore, causal inference has achieved remarkable performance in cross-modal tasks ~\cite{liu2022cross,liu2022show}. Compared with the previous works that address VQA or image captioning, we focus on radiology report generation and propose visual-linguistic causal intervention, which simultaneously eliminates spurious correlations from visual and linguistic confounders.

\section{Method}

In this section, we first introduce the pre-training strategy, followed by the two essential cross-modal causal intervention modules, i.e., the Visual Deconfounding Module (VDM) and the Linguistic Deconfounding Module (LDM). Next, we describe how to integrate these two modules into the VLCI for cross-modal causal intervention.

\subsection{Overview}
%%% define target
A typical RRG model takes a radiology image $I\in\mathbb{R}^{C\times H\times W}$ as input and generates the corresponding report $R=\{w_1,w_2,…,w_n\}$ that contains critical information. 
As illustrated in Figure~\ref{fig:VLCI}, the VLCI employs the transformer structure to model $P(R|I)=\sum_{i=1}^{n}P(w_{i}|h_v, h_w)$, where $h_v$ is the visual feature extracted by an encoder and guides the prefix word embedding $h_w$ to generate the next word $w_i$ with visual-linguistic deconfounding. 
To ensure that the estimation of confounder $Z$ is caused by the prior $P(I)$ and $P(R)$, we leverage the Visual-Language Pre-training (VLP) model to construct the correlation between the visual contexts and linguistic concepts. 
 Meanwhile, due to the absence of a knowledge graph and a well-trained feature extractor. We innovatively leverage causal front-door intervention to eliminate implicitly the spurious correlations from visual and linguistic modalities, and it is integrated into VDM and LDM, respectively.

\subsection{Visual-Linguistic Pre-training (VLP)}
%%%% motivation
% procedure, overall
% 难点是非配对和各异的区域特征（病灶形态多样，很难区分）
In the medical pre-training framework, there exist two difficulties: (1) The unpaired data that only has a single modality is hard to be utilized in supervised learning, (2) heterogeneous data that makes it difficult to distinguish the region feature because the morphology of the same lesion varies greatly~\cite{zhou2021review}.
Since the cross-modal pre-training provides fine-grained regional features without regional label~\cite{xvlm}, we utilize PLM and MIM in linguistic and visual modeling to deal with unpaired data.
% 所以结合多模态预训练，实现V-T, T-V, V-V, T-T这几种情况
Therefore, we use a single encoder to extract multi-modal features and two weight-shared decoders to solve PLM and MIM tasks, respectively~\cite{wang2021simvlm, he2022masked}. In each block of the multi-modal encoder, the attention layer is weight-shared while the two feed-forward layers handle the corresponding modal feature respectively~\cite{yu2022coca} (Refer to Appendix A).

% SimVLM -> Prefix Language Model
Motivated by SimVLM~\cite{wang2021simvlm}, we extract image features from the first three blocks of ResNet101~\cite{he2016deep} as prefix tokens in PLM. Simultaneously, the text is divided randomly into two parts, of which one is generated by another under the guidance of the obtained image tokens. When the corresponding image is absent, the PLM can also be trained with only text modality, which is the same as SimVLM. 
Assume that $h_v\in\mathbb{R}^{\frac{HW}{P^2}\times d}$ is denoted as the image token extracted by the raw image $I$, where $P$ is the patch size, and $d$ is the embedding size. 
% Assume that $h_v\in\mathbb{R}^{k\times d}$ is denoted the image token extracted by the raw image $I$, where $k=\frac{HW}{P^2}$ is the length of the image tokens, $P$ is the patch size, and $d$ is the embedding size. 
Then% $\{w_{i},\}_{i={L}_{p}}^{L}$ 
$ \{{w}_{{n}_{p}}, \ldots, {w}_{n}  \}$
is the postfix sequence after the textual description $h_w$ of length $n_p\ge 0$. Thus, the formulation is as follows:
% In this proxy task, we considered images as prefixes and the training objective becomes:
\begin{equation}
    \mathcal{L}_{\textrm{PLM}}(\theta) = -\sum^{n}_{i=n_p}logP_\theta(w_{i}|h_v, h_{w_{<n_p}}),
\end{equation}
where $\theta$ is the trainable parameters of the model, $h_v$ is the visual embedding with a trainable 2D positional encoding, $h_w$ is learned for a fixed vocabulary and received by the encoder as the prefix, and $n$ is the report length. 

% Mask Image Model
% To further deal with unpaired images, we take advantage of \textbf{MIM}
To deal with unpaired images like MAE~\cite{he2022masked}, we take advantage of the MIM paradigm. Additionally, since the MIM is trained with pairwise data, the missing semantics of masked images can be provided by text to enhance the cross-modal association~\cite{geng2022multimodal}. 
Thus, we reconstruct the masked visual token via the semantics of the unmasking visual token and linguistic token, which can learn the tiny difference in the dataset~\cite{seo2022masked}. 
The target of the MIM can be formulated as follows:
\begin{equation}
    \mathcal{L}_{\textrm{MIM}}(\theta) = P_\theta(h_{vm}|h_{vv}, h_w),
\end{equation}
where ${h}_{vm}$ denotes the masked visual tokens extracted by the ResNet backbone, ${h}_{vv}$ is the unmasked tokens, and $h_w$ denotes the word tokens of the whole report. Then the ResNet and the multi-modal transformer encoder are utilized as VRLM in the downstream tasks.

\subsection{Visual-Linguistic Causal Intervention}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{fig/scm.pdf}
  \caption{Front-door causal intervention $P(R|do(h_v), do(h_w))$ is implemented by the mediator $M_v$ and $M_l$. It is the SCM of RRG with the confounder $Z_v$ and $Z_l$ in (a) and cut off the path $Z_v \to h_v$ and $Z_l \to h_w$ via blocking the back-door path $h_v\gets Z_v \to R$ and $h_v \gets h_w\gets Z_l \to R$ in (b) and get the SCM of VLCI in (c).}
  \label{fig:scm}
\end{figure}

After visual-linguistic pre-training, the trained visual-linguistic feature encoders still contain visual and linguistic biases from cross-modal confounders~\cite{yang2021deconfounded}. 
Therefore, we employ Pearl’s structural causal model (SCM)~\cite{glymour2016causal} to characterize the causal effect between visual and linguistic modalities when conducting RRG. 
As illustrated in Figure~\ref{fig:scm} (a), the causal effects $h_v\to R$ and $h_w\to R$ are affected by the confounder $Z=\{Z_v,Z_l\}$ from back-door paths $h_v \gets Z_v\to R$ and $h_w \gets Z_l\to R$  ~\cite{liu2022show}, respectively. In our SCM, the non-interventional prediction can be expressed by the Bayes rule:
\begin{equation}
\begin{aligned}
    &P(R|I) = P(R|h_v, h_w)\\
    &=\sum_{i=1}^{n}\sum_{z} P(w_{i}|h_v, h_w, Z=z)P(Z=z|h_v, h_w),
\end{aligned}
\end{equation}
% The confounders and mediator
where $Z$ brings the spurious correlation via $P(Z=z|h_v, h_w)$, leading to incorrect reports. 
Taking the example in Figure~\ref{fig:bias}, %For instance, 
when $P(Z=\textrm{``normal~heart''}|h_v=\textrm{``heart"}, h_w=\textrm{``normal''})$ is large while $P(Z=\textrm{``enlarged~heart"}|h_v=\textrm{``heart''}, h_w=\textrm{``normal"})$ is small, it will enlarge $P(R=\textrm{``no~pleural~effusion"}|h_v, h_w, Z=\textrm{``normal~heart"})$. %shown in Figure~\ref{fig:bias}.
To mitigate visual-linguistic confounders and uncover the cross-modal causal structure, we apply causal front-door intervention by introducing mediator $M_v$ and $M_l$, respectively, as shown in Figure~\ref{fig:scm}(b).
Generally, $Z_v$ is unobservable without a well-trained object detector, and the back-door path $h_v\gets Z_v \to R$ can be blocked by $M_v$ via learning the true causal effect $h \gets M_v \gets h_v \gets Z_v \to R$. 
The confounders and mediator can be shown intuitively in Figure\ref{fig:VLCI} (b).
Similarly, the intervention on the back-door path $h_v\gets h_w\gets Z_l\to R$ can be implemented by calculating the $M_l$ without well-constructed confounder dictionaries. 
Since the front-door intervention can eliminate unobservable confounders, we integrate the Front-door Intervention Module (FIM) into VDM and LDM. 
\subsubsection{Front-door Intervention Module (FIM)}
% why
% \noindent \textbf{Front-door Intervention Module.}
To cut off back-door paths $h_v \gets Z_v \to R$ and $h_w \gets Z_l \to R$ via $M_v$ and $M_l$ (SCM in Figure~\ref{fig:scm}(b)), we leverage the do calculus $do(\cdot)$~\cite{liu2022show, liu2022cross, lopez2017discovering, qi2020two}, which is formulated as:
\begin{equation}
\begin{aligned}
    &P(R|do(h_v), do(h_w)) = \\
    &\sum_{m}P(R|do(M=m))P(M=m|do(h_v), do(h_w)),
    \label{eq:do}
\end{aligned}
\end{equation}
where $M$ is the mediator containing $M_v$ and $M_l$.
%The causal path between $\{h_v, h_w\}$ and $R$ becomes $\{h_v, h_w\} \to M \to R \gets Z$. 
Since the intervention probability is equal to the conditional probability in the path $\{h_v, h_w\} \to M$, the back-door path between $M$ and $R$ can be blocked, and enlarge $P(R=\textrm{``small~pleural~effusion"}|h_v=\textrm{``heart"}, h_w=\textrm{``normal"}, M=\textrm{``pleural"})$, as the previously mentioned example, where the ``pleural" is the multi-modal feature of pleural in each condition of heart size.
% by blocking $M \gets \{h_v, h_w\} \gets Z \to R$. 
Consequently, we can assume that $\{h_v, h_w\}$ is feature $F$, and formulate Eq.~(\ref{eq:do}) as:
\begin{equation}
\begin{aligned}
    &P(R|do(h_v), do(h_w)) = P(R|do(F))=\\
    &\sum_{m}P(M=m|F)\sum_{\hat{F}}P(F=\hat{F})P(R|F=\hat{F},M=m).
    \label{eq:do_1}
\end{aligned}
\end{equation}
To further estimate $P(R|do(F))$, we implement the front-door causal intervention Eq.~(\ref{eq:do_1}) with the deep learning framework. Here, we adopt Normalized Weighted Geometric Mean (NWGM)~\cite{xu2015show} and approximate the Eq.~(\ref{eq:do_1}) as:
\begin{equation}
\begin{aligned}
    P(R|do(h_v), do(h_w)) \approx \textrm{Softmax}(g(h_w, h_v, \hat{M_v}, \hat{M_l})),
    \label{eq:do_v_2}
\end{aligned}
\end{equation}
where $g(\cdot)$ denote the network mapping functions, $\hat{M_v}$ and $\hat{M_l}$ denote the estimations of $M_v$ and $M_l$ via VDM and LDM. In Figure~\ref{fig:VLCI} (c), the FIM consists of two Attention Fusion (AF) layers and it is integrated into VDM and LDM.
\subsubsection{Visual Deconfounding Module (VDM)}
% 为什么要用local和gobal
% local 的high attn token 代表了局部的细节特征，global是类似心脏轮廓和位置的全局性的特征
% 心脏轮廓影响了胸积液的判断，同时肺部的纹理细节也可以有助于判断
In Figure~\ref{fig:VLCI}, we calculate the visual mediator $M_v$ via local feature $h_{vl}$ and global feature $h_{vg}$. The $h_{vl}$ is denoted as the local detail information acquired from Local Sampling, while the $h_{vg}$ is the contours and position feature acquired from Global Sampling~\cite{sun2022lesion}. 
For instance, the contour of the heart affects the determination of pleural effusion, and the texture of the lungs can also be the basis of detection.

\noindent\textbf{Local Sampling.} Inspire by TransFG \cite{he2022transfg}, we use the attention accumulated from the encoder to select top $k$ tokens that correspond to the report and only use these selected tokens as $h_{vl}\in\mathbb{R}^{k\times d}$, where $k=6$ for each head of attention. Then, $h_{vl}$ is enhanced via CaaM \cite{wang2021causal}, which further excavates the local internal relations. The $h_{vl}$ aims to obtain local critical details in the image, which can be used as the key basis for RRG.

\noindent\textbf{Global Sampling.} The global sampling is implemented by Down Sampling Transformer block, in which the $14\times 14$ visual tokens are down-sampled to $7\times 7$ as $h_{vg}\in\mathbb{R}^{49\times d}$. Max pooling in this block can better retain the global structure information in the image as the general features of the data itself. We formulate the operation as follows:
\begin{equation}
    h_{vg} = W[\textrm{P}(h_v) + \textrm{Attn}(\textrm{P}(\textrm{LN}(h_v))],
    \label{eq:dst}
\end{equation}
where P is the 2d max pooling layer, LN is layer normalization, Attn is the 2d relative attention \cite{dai2021coatnet}, and $W$ denotes the weights of linear layer.

Finally, the $h_{vl}$ is integrated with $h_{vg}$ to enhance local details with global structural information via Local-Global Fuse Module formulated as Eq.~(\ref{eq:LGFM}), namely mediator $M_v$. 
\begin{equation}
    M_v=\textrm{FFN}([\textrm{MHA}(h_{vl}, h_{vl}, h_{vl}), \textrm{MHA}(h_{vl}, h_{vg}, h_{vg})])
    \label{eq:LGFM}
\end{equation}
where MHA and FFN are the Multi-Head Attention layer and Feed-Forward Network layer, respectively. $[\cdot,\cdot]$ denotes concatenation.

\begin{table*}[h!]\footnotesize\renewcommand\arraystretch{0.8}
  \centering
  \setlength{\tabcolsep}{2mm}{
  \begin{tabular}{@{}llcccccccccc@{}}
    \toprule
    &Method             & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4  & CIDEr & Rouge-L & METEOR & Precision & Recall & F1\\
    \midrule
    &\multicolumn{11}{c}{\textbf{IU-Xray Dataset}} \\
    \midrule
    \multirow{5}{*}{\rotatebox{0}{\scriptsize Lightweight}}
                    &R2Gen\cite{chen2020generating}              & 0.470 & 0.304 & 0.219 & 0.165 &    /   & 0.371 & 0.187 & / & / & /\\ 
                    &CMCL\cite{liu2022competence}               & 0.473 & 0.305 & 0.217 & 0.162 &    /   & 0.378 & 0.186 & / & / & /\\ 
                    &PPKED\cite{liu2021exploring}              & 0.483 & 0.315 & 0.224 & 0.168 & 0.351  & 0.376 & 0.190 & / & / & /\\
                    &CA\cite{liu2021contrastive}                & 0.492 & 0.314 & 0.222 & 0.169 &    /   & 0.381 & 0.193 & / & / & /\\
                    &AlignTransformer\cite{you2021aligntransformer}   & 0.484 & 0.313 & 0.225 & 0.173 &    /   & 0.379 & \underline{0.204} & / & / & /\\
    \cmidrule{1-12}
    \multirow{2}{*}{\rotatebox{0}{\scriptsize Heavyweight}}
                    &M2TR\cite{nooralahzadeh2021progressive}               & 0.486 & \underline{0.317} & 0.232 & 0.173 &    /   & \textbf{0.390} & 0.192 & / & / & /\\
                    %&Self-boost\#         & 0.487 & \textbf{0.346} & \textbf{0.270} & \textbf{0.208} &  \textbf{0.452} & 0.359 & /\\
                    &RG-GSK\cite{yang2022knowledge}             & \textbf{0.496} & \textbf{0.327} & \underline{0.238} & \underline{0.178} &  \underline{0.382} & 0.381 & / & / & / & /\\
    \cmidrule{1-12}
                    &\textbf{VLCI (ours)}               & \underline{0.495} & \textbf{0.327} & \textbf{0.239} & \textbf{0.185} &  \textbf{0.449} & \underline{0.389} & \textbf{0.206} & / & / & /\\\hline
    \midrule
    &\multicolumn{11}{c}{\textbf{MIMIC-CXR Dataset}} \\
    \midrule
    \multirow{5}{*}{\rotatebox{0}{\scriptsize Lightweight}}
                    &R2Gen\cite{chen2020generating}             & 0.353 & 0.218 & 0.145 & 0.103 &    /   & 0.277  & 0.142 & 0.333 & 0.273 & 0.276\\ 
                    &CMCL\cite{liu2022competence}              & 0.334 & 0.217 & 0.140 & 0.097 &    /   & 0.281 & 0.133 & / & / & /\\ 
                    &PPKED\cite{liu2021exploring}             & 0.360 & 0.224 & 0.149 & 0.106 &  \underline{0.237} & 0.284 & 0.149 & / & / & /\\
                    &CA\cite{liu2021contrastive}                & 0.350 & 0.219 & 0.152 & 0.109 &    /   & 0.283 & 0.151 & 0.352 & 0.298 & 0.303\\
                    &AlignTransformer\cite{you2021aligntransformer}   & \underline{0.378} & 0.235 & 0.156 & 0.112 &    /   & 0.283 & \underline{0.158} & / & / & /\\
    \cmidrule{1-12}
    \multirow{3}{*}{\rotatebox{0}{\scriptsize Heavyweight}}
                    &M2TR\cite{nooralahzadeh2021progressive}               & \underline{0.378} & 0.232 & 0.154 & 0.107 &    /   & 0.272 & 0.145 & 0.240 & \textbf{0.428} & 0.308\\
                    %&Self-boost*         & 0.359 & 0.224 & 0.150 & 0.109 &  \underline{0.270} & 0.277& 0.141 \\
                    &RG-GSK\cite{yang2022knowledge}             & 0.363 & 0.228 & 0.156 & 0.115 &  0.203 & 0.284 & / & \textbf{0.458} & 0.348 & \underline{0.371}\\
                    &MSAT*\cite{wang2022medical}               & 0.373 & \underline{0.235} & \underline{0.162} & \textbf{0.120} &  \textbf{0.299} & \underline{0.298} & 0.143 & / & / & /\\
    \cmidrule{1-12}
                    &\textbf{VLCI (Ours)}               & \textbf{0.390} & \textbf{0.248} & \textbf{0.167} & \underline{0.119} &  0.168 & \textbf{0.302} & \textbf{0.172} & \underline{0.409} & \underline{0.390} & \textbf{0.398}\\
    \bottomrule
  \end{tabular}}
    %\vspace{-2pt}
  \caption{The performances of VLCI and other methods on IU-Xray and MIMIC-CXR datasets. The ${1}^{st}$ and ${2}^{nd}$ best results are bolded and underlined, respectively.
  % The performances of our model compared with baselines on the IU-Xray and MIMIC-CXR datasets. The best results are highlighted in bold while the second best is underlined. We cite 
  The method marked by * means its result is from \cite{wang2022medical},
  % for the baselines marked by \# the results are reported in \cite{wang2021self}; 
  while the rest is from \cite{yang2022knowledge}, and / means the absent result.
  }
  \label{tab:main}
\end{table*}

\subsubsection{Linguistic Deconfounding Module (LDM)}
% Textual Confounder
For linguistic deconfounding, we have some observations from the link $h_v \gets h_t\gets Z_l\to R$: (1) the linguistic contexts can affect the generation of the next word, and (2) the attended word features affect the attended visual features via cross-attention~\cite{liu2022show}.
% why, 因为文本的上下文和来自视觉的高注意力特征都会影响文本的生成
% 为什么在embedding space做，词频的差异也会带来很大的距离偏差，因此词向量的距离就不能很好地代表语义相关性
Additionally, the difference in word frequency brings a large distance deviation in embedding space, so the distance of word vectors cannot represent semantic relevance well~\cite{li2020sentence}. 
% METEOR 指标的明显提升
Thus, we calculate the linguistic mediator $M_l$ in embedding space via all vocabularies from the tokenizer as the global feature and use $h_{vl}$ obtained from the VDM, which estimates the current word frequency to adjust the distribution of $h_w$ (Figure~\ref{fig:VLCI}). 
% formulation
\begin{equation}
\begin{aligned}
   &h^{'}_{vl} = \textrm{FFN}(\textrm{MHA}(h_{vl}, \hat{w}, \hat{w})); \\
   &M_t = \textrm{FFN}(\textrm{MHA}(h^{'}_{vl}, h_{vl}, h_{vl}))
\end{aligned}
\end{equation}
where $\hat{w}$ denotes all word tokens from the tokenizer. Then, we build the causal link $h \gets M_l \gets ht \gets Z_l \to R$ to cut off the path $Z_l \to h_v$ via $M_l$.

In Figure~\ref{fig:VLCI}, the deconfounded visual and linguistic features are fed to the decoder to learn fused cross-modal features. The output layer is a linear projection with softmax operation, mapping probability into $N$-dimensional, where $N$ is the vocabulary size. Finally, the training target is to minimize the negative log-likelihood loss:
\begin{equation}
\begin{aligned}
    &\mathcal{L}_{\textrm{nll}}(\theta) = -\sum_{i=1}^{n}log[P_\theta(w_{i}|do(h_v), do(h_{w_{<i}}))]
\end{aligned}
\end{equation}

\section{Experiment}
\subsection{Experimental settings}
%\subsection{Datasets, Metrics and Settings}
\noindent \textbf{Dataset}.
%\noindent 
\textbf{IU-Xray}~\cite{jamiaocv080}, namely Indiana University Chest X-ray Collection, is a public radiology dataset widely used to evaluate the performance of RRG methods. It contains 7,470 chest images and 3,955 corresponding reports. We apply the same setting as R2Gen~\cite{chen2020generating} and tokenize the words more than three occurrences.
% with 2,955 image-text pairs
%\noindent 
\textbf{MIMIC-CXR}~\cite{johnson2019mimic} is a large-scale chest radiology dataset, with 377,110 images and 227,835 corresponding reports. We use the official split and tokenize the words with more than ten occurrences. 
% with 279,733 image-report pairs

\noindent \textbf{Evaluation Metrics}. We adopt the widely used NLG metrics, including BLEU~\cite{papineni2002bleu}, ROUGE-L~\cite{rouge2004package}, METEOR~\cite{banerjee2005meteor} and CIDEr~\cite{vedantam2015cider}. Since the RRG specifically focuses on the abnormality detection accuracy rather than the text fluency and similarity with the real report, we further adopt clinical efficacy (CE) metrics~\cite{chen2020generating, liu2021contrastive, nooralahzadeh2021progressive, yang2022knowledge}. It is calculated by the labels extracted from CheXpert~\cite{irvin2019chexpert}.


\noindent \textbf{Implementation Settings}. We use the first three blocks of ResNet101~\cite{he2016deep} to extract 1,024 feature maps, which are projected into 512 maps of size $14 \times 14$. 
The dimension of the latter VLCI layers and the number of attention heads are fixed to 512 and 8. 
The number of layers is 3 for both the encoder and the decoder. 
% A. framework of VLP, B. Mask rate C. Setting of Baseline
We adopt the same dataset during pretraining and fine-tuning. The batch size is set to 64 in pretraining and 16 in fine-tuning.
In the pre-training stage, we adopt the image mask rate of 85\% (Refer to Appendix B) for MIM. 
The VLP is trained by the AdamW optimizer with a warm-up step 10\% of the whole train step and the peak learning rate is 5e-4. The weight decay of the optimizer is set to 1e-2. 
The total epochs are set to 100 and 30 for the IU-Xray and MIMIC-CXR datasets, respectively. 
The model is finetuned by the Adam optimizer with an initial learning rate of 1e-5 and weight decay of 5e-5 for 10 and 3 epochs for the IU-Xray and MIMIC-CXR dataset, respectively.

\noindent \textbf{Baseline Models.}
We compare the proposed VLCI model with several state-of-the-art RRG models, which are divided into lightweight and heavyweight models (Refer to Appendix C). Specifically, the lightweight models have no more than 3-layer modules for both the encoder and the decoder. Most of them employ different modules for boosting model performances, %such as the knowledge-aware module, template retrieval module, and memory-drive module, 
which are computationally expensive. Note that the total parameters of our VLCI are comparable to the R2Gen, while VLCI is faster because our model gets rid of the dependency of calculating memory recursively.


\subsection{Quantitative Analysis}
% 总体性能 解释CIDEr差一点
% NLG
As shown in Table~\ref{tab:main}, our VLCI outperforms almost all the RRG methods. Specifically, compared with the lightweight AlignTransformer, the VLCI significantly boosts the BLEU-4 metric by 1.2\% on the IU-Xray dataset and 0.7\% on the MIMIC-CXR dataset.
Compared with heavyweight model M2TR, our lightweight VLCI boosts the METEOR metric by 1.4\% and 2.7\% on the IU-Xray and MIMIC-CXR datasets, respectively. Since the METEOR metric considers the synonyms, it shows the effectiveness of our causal intervention for discovering semantic correlation. But the performance of VLCI is slightly lower than RG-GSK on the IU-Xray dataset for the BLEU-1 metric, due to the fact that 
 BLEU-1 metric evaluates the single word performance of the reports. While our performance for the BLEU-1 metric is significantly improved on MIMIC-CXR dataset. It reveals that the visual-linguistic data bias and the spurious correlation among multiple words are more significant on large dataset MIMIC-CXR, which can fully take advantage of our cross-modal causal intervention. 

\begin{table}\footnotesize\renewcommand\arraystretch{0.7}
\centering
\setlength{\tabcolsep}{1mm}{
\begin{tabular}{@{}llcc@{}}
    \toprule
    Type & Abnormalities                  & Baseline  & VLCI \\
    \midrule
     & Lung Opacity                & 0.497     & 0.542 (\textcolor{green}{0.045})\\
    % 肺部浑浊
     & Lung Lesion                 & 0.924     & 0.929 (\textcolor{green}{0.005})\\
    % 肺部病变
     & Consolidation               & 0.617     & 0.862 (\textcolor{green}{0.245})\\
    % 质地致密化的一种病变
Lung & Pneumonia                   & 0.639     & 0.805 (\textcolor{green}{0.166})\\
    % 肺炎
     & Atelectasis                 & 0.657     & 0.673 (\textcolor{green}{0.016})\\
    % 肺不张
    & Edema                       & 0.509     & 0.746 (\textcolor{green}{0.237})\\
    % 水肿 (指肺
    \midrule
    &  Pneumothorax                & 0.564     & 0.940 (\textcolor{green}{0.376})\\
    % 气胸
Pleural & Pleural Effusion            & 0.551     & 0.727 (\textcolor{green}{0.176})\\
    % 胸积液
    & Pleural Other               & 0.957     & 0.958 (\textcolor{green}{0.001})\\
    % 胸部其他疾病
    \midrule
    & Enlarged Cardiomediastinum  & 0.473     & 0.632 (\textcolor{green}{0.159})\\
    % 纵隔增大
Other &  Cardiomegaly                & 0.454     & 0.598 (\textcolor{green}{0.144})\\
    % 心脏肥大
    & Fracture                    & 0.924     & 0.933 (\textcolor{green}{0.009})\\
    & Support Devices             & 0.761     & 0.701 (\textcolor{red}{-0.060})\\
    % 辅助设备，支架之类的
    % 骨折
    % \midrule
    % Normal                  & 0.733     & 0.684 (\textcolor{red}{-0.049})\\
    % 没有任何异常
    \bottomrule
  \end{tabular}}
    \caption{Evaluation of abnormality classification results (accuracy) on MIMIC-CXR. The numbers in the bracket mean the improvement with green and decrement with red.}
    % The markers $\heartsuit$ and $\clubsuit$ denote the disease and radiologic signs.}
  \label{tab:acc}
\end{table}
\begin{figure}[t]
%\begin{minipage}[b]{0.5\textwidth}
%\begin{flushleft}
%\makeatletter\def\@captype{table}
%    \makeatother
%\renewcommand\tabcolsep{0.8pt}\renewcommand\arraystretch{0.6}\scriptsize
%  \centering
%\begin{tabular}{@{}llcc@{}}
%    \toprule
%    Type & Abnormalities                  & Baseline  & VLCI \\
%    \midrule
%     & Lung Opacity                & 0.497     & 0.542 (\textcolor{green}{0.045})\\
    % 肺部浑浊
%     & Lung Lesion                 & 0.924     & 0.929 (\textcolor{green}{0.005})\\
    % 肺部病变
%     & Consolidation               & 0.617     & 0.862 (\textcolor{green}{0.245})\\
    % 质地致密化的一种病变
%Lung & Pneumonia                   & 0.639     & 0.805 (\textcolor{green}{0.166})\\
    % 肺炎
%     & Atelectasis                 & 0.657     & 0.673 (\textcolor{green}{0.016})\\
    % 肺不张
%    & Edema                       & 0.509     & 0.746 (\textcolor{green}{0.237})\\
    % 水肿 (指肺
%    \midrule
%    &  Pneumothorax                & 0.564     & 0.940 (\textcolor{green}{0.376})\\
    % 气胸
%Pleural & Pleural Effusion            & 0.551     & 0.727 (\textcolor{green}{0.176})\\
    % 胸积液
%    & Pleural Other               & 0.957     & 0.958 (\textcolor{green}{0.001})\\
    % 胸部其他疾病
%    \midrule
%    & Enlarged Cardiomediastinum  & 0.473     & 0.632 (\textcolor{green}{0.159})\\
    % 纵隔增大
%Other &  Cardiomegaly                & 0.454     & 0.598 (\textcolor{green}{0.144})\\
    % 心脏肥大
    %& Fracture                    & 0.924     & 0.933 (\textcolor{green}{0.009})\\
    %& Support Devices             & 0.761     & 0.701 (\textcolor{red}{-0.060})\\
    % 辅助设备，支架之类的
    % 骨折
    % \midrule
    % Normal                  & 0.733     & 0.684 (\textcolor{red}{-0.049})\\
    % 没有任何异常
    %\bottomrule
  %\end{tabular}
    %\caption{Evaluation of abnormality classification results (accuracy) on the MIMIC-CXR dataset. The numbers in the bracket mean the improvement with green and decrement with red. The labels are extracted by CheXpert~\cite{irvin2019chexpert}.}
    % The markers $\heartsuit$ and $\clubsuit$ denote the disease and radiologic signs.}
%    \vspace{-15pt}
%  \label{tab:acc}
%\end{flushleft}

%\end{minipage}
%\begin{minipage}[b]{0.5\textwidth}
 \centering
    %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
    \includegraphics[width=1\linewidth]{fig/result.pdf}
    \caption{Examples of generated reports on MIMIC-CXR. The baseline is a transformer with the same setting as our VLCI. 
    Different colors are applied to the target keywords. The uncertain and wrong words are underlined with italics and bold, respectively. 
    }
    \label{fig:example}
%\end{minipage}
\end{figure}

Similarly, for the Rough-L metric that calculates the recall of each word, the performance of VLCI and M2TR shows the similar phenomena.
However, for the CIDEr and BLEU-4 metrics, the performance of VLCI is lower than that of MSAT on MIMIC-CXR. These metrics tend to evaluate the similarity of the whole report, which is challenging for a long-sequence report with medical terminology. 
Thus, the knowledge-based approach (PPKED, RG-GSK, MSAT) with professional concepts can generate a more precise description of the disease and achieve better performance, while VLCI only intervenes in the causal effect without external knowledge. 
The CE metric is only applied to MIMIC-CXR dataset because the label extractor (CheXpert)~\cite{irvin2019chexpert} is specially designed for MIMIC-CXR to obtain class labels. Compared with the state-of-the-art lightweight CA in Table~\ref{tab:main}, VLCI improves the performance by 5.7\% in Precision, 9.2\% in Recall, and 9.5\% in F1-Score. This validates that VLCI can provide a more accurate clinic diagnosis rather than only generating a fluent report.
\vspace{-5pt}
\subsection{Qualitative Analysis}
\vspace{-5pt}
% abnormalities detection
To further analyze the clinic diagnosis from VLCI, we evaluate the abnormality used in the CE metric.
In Table~\ref{tab:acc}, VLCI boosts the performance of the abnormalities detection on the MIMIC-CXR dataset, especially the accuracy of ``Pneumothorax", ``Edema" and ``Consolidation". This is because our VLCI explores sufficient visual information and further produces more accurate and less biased descriptions by cross-modal causal intervention than the Transformer baseline. 
However, the estimation of some categories still keeps ambiguous, e.g., ``Lung Opacity". It reveals that VLCI can provide an comprehensive consideration of various radiologic signs to detect the abnormality but give less improvement for the single source abnormality. For example, whether ``Edema" is caused by the heart has different radiologic signs, while the increase in lung density can be considered as ``Lung Opacity". Thus, VLCI can capture the abnormality with complex causes more effectively, where exists more spurious correlations. Besides, Table~\ref{tab:acc} shows the unavailability of causal intervention in independent abnormalities, e.g. ``Support Devices".


\begin{table}\footnotesize\renewcommand\arraystretch{0.7}
  \centering
  \setlength{\tabcolsep}{1mm}{
  \begin{tabular}{@{}lcccc@{}}
    \toprule
    Method          & BLEU-1  & BLEU-4  & CIDEr & Rouge-L \\
    \midrule
    Baseline        & 0.433 & 0.148 & 0.501 & 0.345 \\ 
    w/ MAE           & 0.449 & 0.154 & 0.486 & 0.360 \\ 
    w/ VLP (MIM)     & 0.439 & \underline{0.162} & \textbf{0.602} & 0.362 \\
    w/ VLP (PLM)     & \textbf{0.467} & \textbf{0.165} & \underline{0.538} & \textbf{0.365} \\ 
    w/ VLP (PLM+MIM)  & \underline{0.466} & 0.160 & 0.431 & \underline{0.364} \\ 
    \midrule
    w/ VLP (PLM)*    & 0.448 & 0.151 & 0.399 & 0.349 \\ 
    \textbf{w/ VLP (PLM+MIM)}* (Ours)   & 0.452 & 0.161 & 0.522 & 0.351 \\ 
    \bottomrule
  \end{tabular}}
    %\vspace{-5pt}
  \caption{The performance of different pre-training methods on IU-Xray, the result marker by * means finetuning on downstream task with 5 epoch, while the rest only use the encoder with 100 epochs. }
  \label{tab:vlp}
\end{table}

We further conduct the qualitative analysis on MIMIC-CXR dataset via three intuitive generated examples of the baseline and the VLCI in Figure~\ref{fig:example}.
% visual
Particularly, as in Figure~\ref{fig:example} (a), the reference report consists of three abnormalities, the baseline neglects ``pleural effusion" and ``consolidation", while VLCI discovers all abnormalities accurately. It shows that our VDM can comprehensively perceive all essential visual features. 
% linguisticour 
Figure~\ref{fig:example} (b) shows an example where the same visual region is simultaneously discovered by the baseline and the VLCI, but leads to different descriptions. Our VLCI can accurately describe the heart, while the baseline is uncertain and even has a miscalculation of pneumonia. 
It shows that LDM can alleviate the semantic bias caused by word frequency in word embedding space.
% error
Figure~\ref{fig:example} (c) shows a normal case that only contains ``Lung Opacity". 
% 对报告的可视化做了删改，所以看不出流畅程度
% Compared to the Baseline, our VLCI can generate a more fluent report and indicates the normality. 
Both the Baseline and VLCI can generate a fluent report and indicates the normality.
But VLCI fails to capture the peribronchial opacities, which are the radiologic signs between ``Clear Lung" and ``Consolidation". This is because the ``Lung Opacity" only changes the pulmonary density and it is difficult to be discovered determinedly.



% Besides, pulmonary vascular congestion may be the symptom of edema, which the radiologist indicates, shown in Figure~\ref{fig:example} (c). Our VLCI can also discover these uncertain abnormalities, which further validates the effectiveness of our VLCI.
\subsection{Ablation Studies}
\subsubsection{Effectiveness of VLP}
In Table~\ref{tab:vlp}, we make a comparison with different pre-training methods. It shows that the cross-modal pre-training method has a more robust representation ability than the MIM with single-modality. 
Additionally, our cross-modal pre-training achieves comparable performance to the PLM model that only finetunes the encoder, while ours finetunes the whole model with fewer epochs. 

\begin{table}\footnotesize\renewcommand\arraystretch{0.7}
  \centering
  \setlength{\tabcolsep}{0.7mm}{
  \begin{tabular}{@{}llccc@{}}
    \toprule
    Dataset     & Method        & BLEU-4  & CIDEr & Rouge-L \\
    \midrule
                & Baseline      & 0.148 & 0.501 & 0.345 \\ 
                & Baseline$^{w\blacklozenge}$ (w/ VDM) &  0.160 & 0.521 & 0.364\\
                & Baseline$^{w\bullet}$ (w/ LDM)  &  0.155 & 0.509 & 0.361\\
                & Baseline$^{w\blacklozenge \bullet} $ (w/ VDM\&LDM)   & 0.163 & \underline{0.544} & 0.361 \\ 
                & R2Gen       & 0.165   & 0.493 & 0.360 \\
                & R2Gen$^{w\blacklozenge}$ (w/ VDM) & 0.171 & 0.553 & 0.370 \\
                & R2Gen$^{w\bullet}$ (w/ LDM)  & 0.166 & 0.546 & 0.360 \\
    IU-Xray     & R2Gen$^{w\blacklozenge\bullet}$ (w/ VDM\&LDM) & 0.173 & \textbf{0.628} & 0.368 \\
                \cmidrule{2-5}
                & Baseline$^{w\bigstar}$ (w/ VLP)    & 0.161 & 0.522 & 0.351 \\ 
                & Baseline$^{w\bigstar\blacklozenge}$ (w/ VLP\&VDM)   & \underline{0.176} & 0.514 & 0.377 \\
                & Baseline$^{w\bigstar \bullet}$ (w/ VLP\&LDM)   & 0.175 & 0.342 & \underline{0.382} \\
                & VLCI          & \textbf{0.185} & 0.449 & \textbf{0.389} \\
    \midrule
                & Baseline      & 0.101 & 0.137 & 0.274 \\ 
                & Baseline$^{w\blacklozenge \bullet} $ (w/ VDM\&LDM)   & 0.070 & 0.074 & 0.230 \\
                \cmidrule{2-5}
    MIMIC-CXR   & Baseline$^{w\bigstar}$ (w/ VLP)      & 0.108 & \underline{0.167} & 0.298 \\ 
                & Baseline$^{w\bigstar\blacklozenge}$ (w/ VLP\&VDM)   & 0.110 & 0.157 & 0.297 \\
                & Baseline$^{w\bigstar \bullet}$ (w/ VLP\&LDM)   & \underline{0.117} & 0.158 & \underline{0.299} \\
                & VLCI          & \textbf{0.119} & \textbf{0.168} & \textbf{0.302} \\
    \bottomrule
  \end{tabular}}
    %\vspace{-5pt}
  \caption{Ablation analysis of our VLCI. The Baseline is implemented by the transformer. The marker at Baseline and R2Gen \cite{chen2020generating} means the operation in the brackets.}
  \label{tab:ablation}
\end{table}
 
% Slight improvements from VDM and LDM
Furthermore, in Table~\ref{tab:ablation}, Baseline$^{w\blacklozenge \bullet}$ is significantly worse than baseline on MIMIC-CXR dataset, e.g., 0.101 $\to$ 0.070 for BLEU-4, while still keeping performance improvement on IU-Xray dataset. % compared with the baseline. 
This validates the significant feature complexity from the large-scale MIMIC-CXR dataset leads to unstable probability distribution estimation with causal intervention. 
% 为了关联，得到先验分布. mimic的数据集非常大且复杂以至于很难直接在intervention中得到先验分布。而相对于iu-xray这种小数据集，由于data bias反而容易得到关联。
Meanwhile, we find that the VLP can substantially boost the performance of the baseline, e.g., 0.148 $\to$ 0.161, 0.101 $\to$ 0.108 for BLEU-4 on IU-Xray and MIMIC-CXR datasets, respectively. The improvement is caused by the learned comprehensive concepts and context in the pre-training and the cross-modal feature alignment stage, which shows the importance of VLP. Similarly, The Rough-L is also barely improved due to the feature complexity and long sequence from MIMIC-CXR dataset. For example, although AlignTransformer achieves the same score of the Rough-L as CA on MIMIC-CXR dataset, it outperforms CA on all other metrics.


\subsubsection{Effectiveness of Causal Intervention}
% 与VLP相比，VLP + VDM 小数据集提升明显，但大的少；大数据集文本复杂；有视觉信息，但是不能准确描述
% 与VLP相比，VLP + LDM 大数据集提升明显，但小的少；大数据集文本复杂；更好的描述，但是缺少准确的视觉信息
% VLCI 小数据集的CIDEr明显不如 VDM + LDM；因为小数据集的文本相对单一且短，虽然报告整体显示但是用词准确度不如
\noindent \textbf{VDM}. %To disentangle visual features, we implement the visual deconfounding via the mediator that fuses local detail information and global contour position. 
In Table~\ref{tab:ablation}, Baseline$^{w\blacklozenge}$ and R2Gen$^{w\blacklozenge}$ can boost the performance compared to Baseline and R2Gen, which demonstrates the model-agnostic property of the VDM. 
% CIDEr metric?
However, the improvement of BLEU-4 between Baseline$^{w\bigstar \blacklozenge}$ and Baseline$^{w\bigstar}$ on IU-Xray dataset is more significant than that on MIMIC-CXR dataset. This is because the VDM can discover more essential visual information, but the report of the MIMIC-CXR dataset is more complex and the model fails to generate accurate descriptions. The performance degradation of CIDEr can further illustrate it.
In Figure~\ref{fig:hm}, the attention map from the encoder of our VLCI can truly focus on the dominated area of possible abnormalities rather than spurious correlations with biased visual concepts. This validates that the VDM is semantics-sensitive to capture dominant visual content by conducting visual causal intervention.

\begin{figure}[t]
 \centering
    %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
    \includegraphics[width=0.8\linewidth]{fig/local.pdf}
    \caption{The visualization of the attention map. (a) is an example from MIMIC-CXR dataset that the colored text should be discovered in the marked region of the image. The images in (b-c) are the attention maps of the baseline and our VLCI, respectively. (b) are the accumulated attention maps from the encoder for the selected local feature, and (c) are the response to the ``pleural" (decoder output). The labels shown in (d) are extracted by CheXpert.}
    \label{fig:hm}
\end{figure}

% 句子描述更准确
\noindent \textbf{LDM}. %To generate accurate reports after perceiving all essential visual features, the LDM can mitigate spurious correlations caused by cross-modal bias and adjust the semantic embedding space. 
Compared to the VDM, the LDM plays a more significant role in RRG because the sophisticated linguistic semantic patterns within reports are entangled and biased that require elaborate linguistic deconfounding. 
In Table~\ref{tab:ablation}, the performance drops without LDM, e.g., 0.119 $\to$ 0.110 for the BLEU-4 metric on MIMIC-CXR dataset. This shows the importance of adjusting semantic relevance in word embedding space. 
Compared with the baseline, the performance improvement of Baseline$^{w\bigstar \bullet}$ on MIMIC-CXR dataset demonstrates that the LDM can generate more accurate reports even with biased visual information. 
% CIDEr 的问题
However, the CIDEr metric on IU-Xray dataset shows the effectiveness of the combination of VDM and LDM, while ILVD obtains a lower score. This is due to the worse diversity on IU-Xray dataset, where Baseline$^{w\blacklozenge \bullet}$ and R2Gen$^{w\blacklozenge \bullet}$ can get higher CIDEr but lower BLEU-4 with inadequate multi-modal feature correlation.
In Figure~\ref{fig:hm} (c), the attention map of the baseline decoder shows an obvious redundancy response, while VLCI can capture dominated semantic information in a coarse-to-fine manner, which is more related to the abnormalities. These results show that LDM can capture more discriminative semantic information from linguistic modality by linguistic front-door intervention. 


%-------------------------------------------------------------------------
\section{Conclusion}

In this paper, we propose Visual-Linguistic Causal Intervention (VLCI) framework for RRG, to implicitly deconfound the visual-linguistic confounder by causal front-door intervention. To alleviate the problem of unpaired visual-linguistic data when pre-training, we combine the PLM and MIM for cross-modal pre-training. To implicitly mitigate cross-modal confounders and discover the true cross-modal causality, we propose visual-linguistic causal front-door intervention modules VDM and LDM. Experiments on IU-Xray and MIMIC-CXR datasets show that our VLCI can effectively mitigate visual-linguistic bias and outperforms the state-of-the-art methods. The lower computational cost and faster inference speed of VLCI promote its clinical application. We believe our work could inspire more causal reasoning methods in medical report generation. 

% \noindent \textbf{Extensibility}
%Compared with other RRG approaches such as knowledge-based, VLCI can be easily applied to other medical modal datasets (e.g., Fundus Fluorescein Angiography,  Lung CT-Scan), since reducing the dependency on additional annotation. The knowledge-based models are built by laborious databases and are hard to transfer to other datasets while ours implements Implicit Visual-Linguistic Causal Intervention without observable confounder.

%\noindent \textbf{Limitation}
%Although we demonstrate the validity of VLCI, the CIDEr metric shows a gap between non-knowledge-based and knowledge-based approaches. Therefore, we can improve the performance by building a more robust representation model with professional concepts, i.e., CLIP for medicine.

\clearpage
%%%%%%%%% REFERENCES

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\appendix

%%%%%%%%% BODY TEXT
\begin{figure*}[h!]
 \centering
    %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
    \includegraphics[width=0.93\linewidth]{fig/VLP.pdf}
    
    \caption{The overview of VLP. MHA and FFN are the Multi-Head Attention layer and Feed-Forward Network layer, respectively.
    }
    
    \label{fig:vlp}
\end{figure*}

\section{VLP Framework}
\label{sec:app_a}
In the pre-training phase, we proposed a novel approach that combines Prefix Language Modeling (PLM) and Masked Image Modeling (MIM) to address the challenges discussed in Section 3.2. The detailed Visual Language Pre-training (VLP) framework is illustrated in Figure~\ref{fig:vlp}, which demonstrates its ability to handle diverse data scenarios, such as unpaired and single-modal data.
%In the pre-training stage, we proposed to combine PLM and MIM to alleviate the difficulties discussed in Section~3.2. The detailed VLP framework is shown in Figure~\ref{fig:vlp}, which can handle various data situations, including unpaired data and single-modal data.

\section{Ablation on Mask Ratio}
\label{sec:app_b}

\begin{table}[h]\footnotesize\renewcommand\arraystretch{0.9}
  \centering
  \setlength{\tabcolsep}{1mm}{
  \begin{tabular}{@{}lcccc@{}}
    \toprule
    Masking Ratio           & BLEU-1  & BLEU-4  & CIDEr & Rouge-L \\
    \midrule
    Baseline            & 0.433 & 0.148 & 0.501 & 0.345 \\ 
    w/ 75\%             & 0.450 & 0.160 & 0.486 & \textbf{0.360} \\ 
    w/ 85\%             & \textbf{0.452} & \textbf{0.161} & \textbf{0.522} & 0.351 \\
    w/ 95\%             & 0.432 & 0.153 & 0.460 & 0.346 \\ 
    \bottomrule
  \end{tabular}}
    % \vspace{-5pt}
  \caption{We evaluated the performance of various masking ratios for MIM on the IU-Xray dataset. In our experiments, we pre-trained the VLP model for 100 epochs and then fine-tuned it in the baseline for an additional 5 epochs.}

  \label{tab:mask}
\end{table}

% We conducted ablation experiments on the impact of masking ratios on model performances, and the results are presented in Table~\ref{tab:mask}. 
%Our VLP achieved the best performance with a higher masking ratio of 85\%, which is contrary to MAE, whose optimal masking ratio is 75\%~\cite{he2022masked}. 
%We conclude that this is due to the cross-modal information correlations, where the masked information can be reconstructed by visible features from language and images. Furthermore, VLP tends to learn general features from the masked modality at higher masking ratios, while distinguishable features can be extracted by the complete information from another modality. To investigate whether further increasing the masking ratio would improve the performance, we experimented with a higher masking ratio of 95\%. However, the decreased results in Table\ref{tab:mask} indicate that this leads to too much information destruction.

We conducted ablation experiments to assess the impact of masking ratios on model performance, and the results are presented in Table~\ref{tab:mask}. Our VLP model achieved the best performance with a higher masking ratio of 85\%, which is in contrast to the optimal masking ratio of 75\% reported by MAE~\cite{he2022masked}. We attribute this difference to the cross-modal information correlations, where the masked information can be reconstructed by visible features from both language and images. Furthermore, VLP tends to learn general features from the masked modality at higher masking ratios, while distinguishable features can be extracted by the complete information from another modality. To explore whether increasing the masking ratio further would further improve the performance, we experimented with a higher masking ratio of 95\%. However, the decreased results in Table~\ref{tab:mask} indicates that this approach leads to excessive information loss.

\section{Model Scale}
\begin{table}[!t]\footnotesize
  \centering
  \setlength{\tabcolsep}{0.7mm}{
  \begin{tabular}{@{}lllccccc@{}}
    \toprule
    &Method              & Backbone &  \#Enc  &   \#Dec & $\mathcal{K}$ & $\mathcal{T}$ & $\mathcal{M}$ \\
    \midrule
    \multirow{5}{*}{\rotatebox{90}{Light}}
    &R2Gen\cite{chen2020generating}               & Resnet101         & 3 & 3 &  &  & $\surd$\\
    &CMCL\cite{liu2022competence}                & Resnet50          & / & $\spadesuit$ &  &  &  \\
    &PPKED\cite{liu2021exploring}               & Resnet152         & 2 & 1 & $\surd$ & $\surd$ &   \\
    &CA\cite{liu2021contrastive}                  & Resnet50          & $\clubsuit$ & $\spadesuit$ &   & $\surd$ &  \\
    &AlignTransformer\cite{you2021aligntransformer}    & Resnet50          & 3 & 3 &   &   & $\surd$  \\
    \midrule
    \multirow{3}{*}{\rotatebox{90}{Heavy}}
    &M2TR\cite{nooralahzadeh2021progressive}                & Densenet151       & 6 & 12 &   &  & $\surd$\\
    %Self-boost          & Resnet101         & 12 & $\spadesuit$ &   &   &    \\
    &RRG-GSK\cite{yang2022knowledge}             & Resnet101         & 12 & 3 & $\surd$ & $\surd$ &  \\
    &MSAT\cite{wang2022medical}                & CLIP              & 6 & 6 & $\surd$ &   & $\surd$ \\
    \midrule
    &\textbf{VLCI}                & Resnet101*        & 3 & 3 &  &  &  \\
    \bottomrule
  \end{tabular}}
    % \vspace{-5pt}
  \caption{The details of VLCI and several compared RRG models, the \#Enc and \#Dec denote the number of transformer layers in the encoder and decoder, respectively. The marker $\clubsuit$ means 2 Contrastive Attention, and $\spadesuit$ means Hierarchical LSTM. The backbone of VLCI is the first three blocks of Resnet101. Besides, we show the employed model boosting modules, including the knowledge-aware module $\mathcal{K}$, template retrieval module $\mathcal{T}$, and memory-drive module $\mathcal{M}$.}
  
  \label{tab:scale}
\end{table}
The RRG models used in our experiment can be categorized as heavyweight or lightweight, as shown in Table~\ref{tab:scale}. Models with no more than 3 layers in both the encoder and decoder are considered lightweight, while others are considered heavyweight. To enhance model performance, these models employ various modules, such as the knowledge-aware module, template retrieval module, and memory-driven memory. In contrast, our lightweight VLCI model only utilizes causal intervention and achieves a significant improvement in performance. Additionally, our VLCI model can be trained on the MIMIC-CXR dataset using only one NVIDIA RTX 3090, whereas the heavyweight MSAT model requires eight NVIDIA TESLA V100 GPUs.

\section{Structural Causal Model}
\begin{figure}[t]
 \centering
    \includegraphics[width=1\linewidth]{fig/sm.pdf}
    \caption{The demonstration of the Structural Causal Model (SCM), (a-d) is the fundamental structure of SCM, and (e-g) is the SCM for the description of VLCI.}
    \label{fig:sm}
\end{figure}
To clarify the mechanism of causal intervention, we introduce the Structural Causal Model (SCM) and its symbols, which are shown in Figure~\ref{fig:sm}. The SCM is a mathematical framework used in causal inference to model the relationships between variables and determine cause-and-effect relationships. It uses directed acyclic graphs (DAGs) to represent causal systems, where each variable is a node and the arrows show causal relationships between the variables.

% To make the mechanism of causal intervention clear, we describe the symbol of the Structural Causal Model(SCM), as shown in Figure\ref{fig:sm}. The Structural Causal Model (SCM) is a mathematical framework used in causal inference to model the relationships between variables and determine cause-and-effect relationships. SCM provides a way to represent a causal system using directed acyclic graphs (DAGs), where each variable is represented as a node and the arrows represent causal relationships between the variables. 

For example, in Figure~\ref{fig:sm}(a), the symbol $h_v \to h$ represents the visual feature affecting the multi-modal feature. In contrast, the multi-modal feature can be caused by the attended word feature, as shown in Figure~\ref{fig:sm}(b) ($h_v \to h \gets h_w$). This fork structure indicates that two variables share a common effect or outcome, such as the word "normal" and the visual feature of the heart, leading to the multi-modal feature of ``normal heart."

% For instance, $h_v \to h$ means the visual feature in Figure~\ref{fig:sm} (a), which can affect the multi-modal feature. Meanwhile, the multi-modal feature can be caused by the attended word feature, as shown in Figure~\ref{fig:sm} (b) $h_v \to h \gets h_w$. This fork structure is a pattern where two variables have a common effect or outcome variable, e.g., the word ``normal" and the visual feature of the heart leading to the multi-modal feature of ``normal heart". 

However, the attended word feature can influence the visual feature through cross-attention and cause confounding, as shown in Figure~\ref{fig:sm}(c) ($h_v \gets h_w \to h \gets h_v$). The word "normal heart" is the linguistic confounder $Z_l$ that affects the visual feature extraction of the pleura through $h_w$ and causes $h$ confounding, as shown in Figure~\ref{fig:sm}(e).

% However, the attended word feature can affect the visual feature via the cross-attention and cause confounding, as shown in Figure~\ref{fig:sm} (c) $h_v \gets h_w \to h \gets h_v$. And the word ``normal heart" is the linguistics confounder $Z_l$ that influences the visual feature extraction of pleura via $h_w$, as shown in Fihure\ref{fig:sm} (e).


To eliminate this back-door path, we can use causal intervention, which involves changing the value of a variable in the system to observe the resulting changes in other variables. The chain $h_v \to M_v \to h$ in Figure~\ref{fig:sm}(d) is a direct connection from cause to effect, allowing us to manipulate the variable ($M_v$) affected by the cause variable ($h_v$) and in turn affecting the effect variable ($h$). Thus, the multi-modal feature of "small pleural effusion" can be accurately extracted using $M_v$ (the visual feature of "pleural effusion" in each heart size situation). A similar operation can be applied to the linguistic modality to complete the multi-modal front-door intervention, as shown in Figure~\ref{fig:sm}(f, g).

% To cut off this back-door path, we can use the causal intervention, which is the process of changing the value of a variable in the system to observe the resulting changes in other variables. The chain $h_v \to M_v \to h$ in Figure~\ref{fig:sm} (d) is a straight connection from cause to effect, providing a manipulation of the variable ($M_v$) that is affected by the cause variable ($h_v$) and in turn affects the effect variable ($h$). Thus, the multi-modal feature of ``small pleural effusion" can be extracted exactly by $M_v$ (the visual feature of ``pleural effusion" in each situation of heart size). A similar operation in the linguistics modal can be implemented, completing the multi-modal front-door intervention, as shown in Figure~\ref{fig:sm} (f, g).

\section{Proof of Equation. 5}
\label{sec:app_d}
Assume that $F\triangleq\{h_v, h_w\}$, we can formulate the interventional probability as follows:
\begin{equation}
\begin{aligned}
    P(R|do(h_v), do(h_w)) = P(R|do(F)).
    \label{eq:do__1}
\end{aligned}
\end{equation}
To eliminate the unobservable confounder, we introduce the mediator $M$ to cut off the link $V\gets Z \to R$.
The total probability $P(R|do(F))$
% of Eq.~(\ref{eq:do__1}) 
can be represented as the following summation:
\begin{equation}
\begin{aligned}
    \sum_{m}P(R|do(F),M=m)P(M=m|do(F)),
    \label{eq:do__2}
\end{aligned}
\end{equation}
where $M$ is introduced by $F$ without the back-door path. Thus, the intervention probability is equal to the conditional probability in the path $F \to M$~\cite{liu2022cross}. Besides, there is no direct causal path between $F$ and $R$. In this way, the introduced summation in Eq.~(\ref{eq:do__2}) can be reformulated as:
\begin{equation}
\begin{aligned}
    &\sum_{m}P(R|do(F), do(M=m))P(M=m|F)\\
    &=\sum_{m}P(R|do(M=m))P(M=m|F).
    \label{eq:do__3}
\end{aligned}
\end{equation}
To estimate $P(R|do(M=m))$, we can apply the back-door intervention to cut off the link $M \gets F \gets Z \to R$~\cite{liu2022show}. Therefore, we have the intervention probability formulation
% we can formulate the intervention probability 
as follows:
\begin{equation}
\begin{aligned}
    &P(R|do(M=m))=\\
    &\sum_{\hat{F}}P(R|do(M=m),F=\hat{F})P(F=\hat{F}|do(M=m))=\\
    &\sum_{\hat{F}}P(F=\hat{F})P(R|F=\hat{F},M=m),
    \label{eq:do__4}
\end{aligned}
\end{equation}
where $\hat{F}$ is the selected features from $F$ not caused by $M$.
At last, via applying Eq.~({\ref{eq:do__4}}), we can further calculate Eq.~(\ref{eq:do__3}) as follows:
\begin{equation}
\begin{aligned}
    \sum_{m}P(M=m|F)\sum_{\hat{F}}P(F=\hat{F})P(R|F=\hat{F},M=m).
    \label{eq:do__5}
\end{aligned}
\end{equation}
So the proof of Equation. 5 is done.

\section{Visualization Result}

 \begin{figure*}[t!]
 \centering
    \includegraphics[width=0.93\linewidth]{fig/sm_result_err_iccv.pdf}
    
    \caption{
    The erroneous result of our VLCI models on the MIMIC-CXR dataset is presented. Thirteen kinds of abnormalities are marked with different markers and colors. Note that keywords in the reports are also marked with different markers and colors. Correctly identified abnormalities are marked in the corresponding color, while other descriptions in bold, italics, and underscores are incorrect. Descriptions marked only with underscores indicate repeated words.
    }
    
    \label{fig:sm_err}
\end{figure*}

In general, the evaluation of RRG's performance involves three key aspects: keyword detection, determination of whether the identified keywords indicate abnormalities, and generation of lengthy texts. The performance of each aspect can be assessed using specific evaluation metrics. The BELU-1 metric is used to evaluate the accuracy of identifying individual words, while the BELU-4 metric examines the precision of detecting individual abnormalities. On the other hand, the CIDEr metric assesses the overall coherence, logic, and similarity of the entire text. Our approach has yielded noteworthy improvement in both the BELU-1 and BELU-4 metrics. However, our method's performance falls considerably short of knowledge-based models in the CIDEr metric. 
The inferior performance is attributed to the fact that our method only relies on the report content of the training set, rather than utilizes the medical corpus provided from the knowledge graph. 
Specifically, the target reports in the training set only include the presence and location of pneumothorax. The limited information (e.g., lack of evidence and description) in the target reports makes it challenging for VLCI to learn such associations.

We further evaluated the errors produced by VLCI by presenting three samples in Figure~\ref{fig:sm_err}. While our method successfully detected multiple abnormalities, it failed to detect "Fracture" and "Support Device" in Figure~\ref{fig:sm_err} (a). This may consistent with our previous statement that single-source abnormalities are challenging to detect. Similarly, Figure~\ref{fig:sm_err} (b) shows that VLCI ignored some abnormalities, such as atelectasis and lung opacities, which are difficult to be disentangled from other abnormalities when a patient has multiple abnormalities at the same time. In Figure~\ref{fig:sm_err} (c), the ground truth indicated the presence of hydropneumothorax, a condition characterized by the presence of both gas and fluid in the chest, whereas pleural effusion only contains fluid. While VLCI correctly identified the presence of gas and fluid in the chest and proposed pneumothorax, it incorrectly estimated pleural effusion due to the lack of knowledge and incorrect estimation. In this example, our method produced messy text and failed to estimate pneumonia and lung consolidation. Furthermore, while VDM and LDM can help identify visual and language concepts, detecting highly specialized concepts with latent relationships that are not present in the data is difficult.

% We further evaluate the error from VLCI and present three samples in Figire~\ref{fig:sm_err}. Our method mentions multiple keywords but fails to detect "Fracture" and "Support Device", in Figure~\ref{fig:sm_err} (a). This is consistent with the statement in the paper that single-source abnormalities are difficult to detect. Similarly, in Figure~\ref{fig:sm_err} (b), some abnormalities were also ignored, such as atelectasis and lung opacities, which are easily missed when a patient has multiple abnormalities simultaneously. In Figure~\ref{fig:sm_err} (c), hydropneumothorax is a symptom of pneumothorax, indicating the presence of gas and fluid in the chest, while pleural effusion only contains fluid. The ground truth indicated hydropneumothorax, and VLCI also noticed the presence of gas and fluid in the chest and proposed pneumothorax. However, due to the lack of knowledge and incorrect estimation, pleural effusion was incorrectly estimated. Additionally, in this example, our method produced messy text and failed to estimate pneumonia and lung consolidation. VDM and LDM can help discover visual and language concepts, but latent relationships that are not present in the data are difficult to detect especially highly specialized associations.

 \begin{figure*}[t]
 \centering
    \includegraphics[width=0.93\linewidth]{fig/sm_result_iccv.pdf}
    
    \caption{
    The results of the Baseline and VLCI models on the MIMIC-CXR dataset are presented. Thirteen kinds of abnormalities are marked with different markers and colors. Note that keywords in the reports are also marked with different markers and colors. Correctly identified abnormalities are marked in the corresponding color, while other descriptions in bold, italics, and underscores are incorrect.
    }
    
    \label{fig:sm}
\end{figure*}

\end{document}
