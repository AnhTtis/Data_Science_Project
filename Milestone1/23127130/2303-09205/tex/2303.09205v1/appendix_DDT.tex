
\subsection{DDT with $B=0$: Proofs of Theorem \ref{thm:prob-win0} and Proposition \ref{prop:asymptoticDDT0}}\label{appx:DDT-0}

\subsubsection{Proof of Theorem \ref{thm:prob-win0}}
\begin{proof}
Let $\alpha_0,\beta_0 \in (0,1)$. We will consider for now that $\alpha_0 \leq \beta_0 \in (0,1)$, and we will compute the asymptotic success probability of $\A(\alpha_0, \beta_0)$.
We define the event
\begin{equation}\label{eq:conc_0comp}\tag{$\mathcal{C}_{\alpha_0}$}
    \forall \alpha_0 N \leq t \leq N \; : \;
            \left| |G^1_t| - \lambda t \right|
            < A_{\alpha_0} \sqrt{t \log N}
\end{equation}
we have from Lemma \ref{lem:conc-card} that $\mathcal{C}_{\alpha_0}$ is true with probability at least $1 - \frac{2}{\alpha_0 N}$, where  $A_{\alpha_0} \eqdef \dfrac{1+\sqrt{{\alpha_0}}}{2\sqrt{2}{\alpha_0}^{1/4}}$, hence we can write
\begin{align}
\Prob(\A(\alpha_0, \beta_0) \text{ succeeds}) \nonumber \label{eq:prob0-A20}
&= \Prob(\A(\alpha_0, \beta_0) \text{ succeeds} \mid \mathcal{C}_{\alpha_0})\left( 1 - O(1/N) \right)\\
&= \Prob(\A(\alpha_0, \beta_0) \text{ succeeds} \mid \mathcal{C}_{\alpha_0}) - O(1/N)
\end{align}
where the $O$ constant depends only on $\alpha_0$. Since $\mathcal{C}_{\alpha_0}$ only depends on the groups of the candidates and not their values, it is independent of the position of the best candidate, therefore by denoting $\tau$ the stopping time of $\A(\alpha_0, \beta_0)$ and $t^\star$ the position of best candidate we have

\begin{align}
\Prob(\A(\alpha_0, \beta_0) \text{ succeeds} \mid \mathcal{C}_{\alpha_0}) \nonumber \label{eq:prob1-A20}
&= \sum_{t=\alpha_0 N}^N \Prob(\tau = t \mid t = t^\star,  \mathcal{C}_{\alpha_0}) \Prob(t = t^\star)\\
&= \frac{1}{N} \sum_{t=\alpha_0 N}^N \Prob(\tau = t \mid t = t^\star,  \mathcal{C}_{\alpha_0}).
\end{align}
For $t \in \{ \alpha_0 N, \ldots , \beta_0 N - 1\}$, since group $G^2$ is still being observed, $\A(\alpha_0, \beta_0)$ will stop on $t$ only if $g_t = 1$ and $x_t$ is the first candidate of $G^1$ such that $x_t > \best G^1_{\alpha_0 N}$. Conditioning on $t = t^\star$, this is equivalent to simply having $g_t = 1$ and $\best G^1_{t-1} \in G^1_{\alpha_0 N-1}$ hence
\begin{align*}
\Prob(\tau = t\mid t = t^\star,  \mathcal{C}_{\alpha_0})
&= \Prob(g_t = 1 \text{ and } \best G^1_{t-1} \in G^1_{\alpha_0 N - 1} \mid t = t^\star,  \mathcal{C}_{\alpha_0})\\
&= \lambda \Prob(\best G^1_{t-1} \in G^1_{\alpha_0 N - 1} \mid \mathcal{C}_{\alpha_0})\\
&= \lambda \E \left[ \left. \frac{ |G^1_{\alpha_0 N - 1}|}{|G^1_{t-1}|} \right| \mathcal{C}_{\alpha_0} \right]
\end{align*}
Conditioning on $\mathcal{C}_{\alpha_0}$ we have that
\begin{align*}
    &|G^1_{\alpha_0 N - 1}| 
    = |G^1_{\alpha_0 N}| + O(1)
    = \lambda \alpha_0 N + O(\sqrt{N \log N})
    = \lambda \alpha_0 N\left( 1 + O\left(\sqrt{\log N / N}\right)\right)\\
    &|G^1_{t-1}|
    = |G^1_{t}| + O(1)
    = \lambda t + O(\sqrt{N \log N})
    = \lambda t \left( 1 + O\left(\sqrt{\log N / N}\right)\right)\\
\end{align*}
where the $O$ constants only depend on $\lambda$ and $\alpha_0$ because $\alpha_0 N \leq t \leq N$, this gives 
\begin{align}
\Prob(\tau = t \mid t = t^\star,  \mathcal{C}_{\alpha_0}) \nonumber \label{eq:prob2-A20}
&= \frac{\lambda^2 \alpha_0 N\left( 1 + O\left(\sqrt{\log N / N}\right)\right)}{\lambda t \left( 1 + O\left(\sqrt{\log N / N}\right)\right)}\\ 
&= \frac{\lambda \alpha_0 N}{t} + O\left( \sqrt{\frac{\log N}{N}} \right)\enspace.
\end{align}

For $t \in \{\beta_0 N, \ldots, N\}$, $\A(\alpha_0, \beta_0)$ stops on $t$ if $x_t$ is the best candidate of its group so far, i.e $r_t = 1$. In order for $\A(\alpha_0, \beta_0)$ to reach the step $t$ and not stop before it, we must have $\best G^1_{t-1} \in G^1_{\alpha_0 N - 1}$ and $\best G^2_{t-1} \in G^2_{\beta_0 N - 1}$, which means that no candidate before $x_t$ surpasses the maximum of its group from the exploration phase, and conditioning on $t = t^\star$, the previous condition is guarantees that the algorithm will stop on $t$, therefore
\begin{align*}
\Prob(\tau = t & \mid t = t^\star,  \mathcal{C}_{\alpha_0})\\
&= \Prob( \best G^1_{t-1} \in G^1_{\alpha_0 N - 1} \text{ and } \best G^2_{t-1} \in G^2_{\beta_0 N - 1} \mid \mathcal{C}_{\alpha_0})\\
&= \E \left[ \left. \frac{ |G^1_{\alpha_0 N - 1}|}{|G^1_{t-1}|} \times \frac{ |G^2_{\beta_0 N - 1}|}{|G^2_{t-1}|}\right| \mathcal{C}_{\alpha_0} \right],
\end{align*}
proceeding as we did for earlier in the proof, we obtain that
\begin{equation}\label{eq:prob3-A20}
\Prob(\tau = t \mid t = t^\star,  \mathcal{C}_{\alpha_0})
= \frac{\alpha_0 \beta_0 N^2}{t^2} + O\left( \sqrt{\frac{\log N}{N}} \right)
\end{equation}
where the $O$ constant only depend on $\lambda$ and $\alpha_0$. ($\alpha_0 \leq \beta_0 \leq 1)$\\
Combining Equations \ref{eq:prob0-A20}, \ref{eq:prob1-A20}, \ref{eq:prob2-A20} and \ref{eq:prob3-A20} we obtain that
\begin{align*}
\Prob(\A(\alpha_0, \beta_0)& \text{ succeeds})\\
&= \lambda \alpha_0 \sum_{t=\alpha_0 N}^{\beta_0 N - 1} \frac{1}{t}
+ \alpha_0 \beta_0 N \sum_{t=\beta_0 N}^{N} \frac{1}{t^2} + O\left(\sqrt{\frac{\log N}{N}}\right)\\
&= \lambda \alpha_0 \left( \log\frac{\beta_0}{\alpha_0} + O(1/N) \right)
  + \alpha_0 \beta_0 N \left( \frac{1}{\beta_0 N} - \frac{1}{N} + O(1/N^2) \right) + O\left(\sqrt{\frac{\log N}{N}}\right)\\
 &= \lambda \alpha_0 \log\frac{\beta_0}{\alpha_0} + \alpha_0 - \alpha_0 \beta_0 + O\left(\sqrt{\frac{\log N}{N}}\right)\enspace.
\end{align*}
The limit success probability is therefore 
\[
\Tilde{\Prob}(\A(\alpha_0, \beta_0) \text{ succeeds}) 
= \lambda \alpha_0 \log\frac{\beta_0}{\alpha_0} + \alpha_0 - \alpha_0 \beta_0,
\]
differentiating with respect to $\alpha_0$ then $\beta_0$ gives that this success probability is maximal over $\{\alpha \leq \beta \in (0,1) \}$ for the thresholds
\[
\alpha_0^\star =  \lambda \exp\left( \frac{1}{\lambda} - 2 \right) 
\quad \text{ and } \quad
\beta_0^\star = \lambda.
\]
Observe that we have indeed $\alpha_0 \leq \beta_0$ with equality only for $\lambda = 1/2$, and
\[
\Tilde{\Prob}(\A(\alpha_0^\star, \beta_0^\star))
=  \lambda^2 \exp\left( \frac{1}{\lambda} - 2 \right)\enspace.
\]

Now if we consider the case $\beta_0 \leq \alpha_0$, since $G^1, G^2$ play symmetric roles, we obtain that the optimal thresholds are $\alpha_0' = 1 - \lambda$ and $\beta_0' = (1-\lambda)^2\exp(1/(1-\lambda) - 2)$, and the limit success probability is 
$
\Tilde{\Prob}(\A(\alpha_0', \beta_0'))
=  (1-\lambda)^2 \exp\left( \frac{1}{1-\lambda} - 2 \right)\enspace.
$\\
Assuming that $\lambda \geq 1/2$, and given that the function $x \in (0,1) \mapsto x^2 \exp\left( \frac{1}{x} - 2 \right)$ is increasing, we deduce that 
\[
\Tilde{\Prob}(\A(\alpha_0', \beta_0')) \leq 
\Tilde{\Prob}(\A(\alpha_0^\star, \beta_0^\star)).
\]
and thus the result.
\end{proof}










\subsubsection{Proof of Proposition \ref{prop:asymptoticDDT0}}
\begin{proof}
Since $\tau \geq \alpha_0 N$ with probability 1, we have for any $w \leq \alpha_0$
\[
\Prob(\A_2(\alpha_0, \beta_0) \text{ succeeds} \mid \tau \geq w N)
= \Prob(\A_2(\alpha_0, \beta_0) \text{ succeeds})
\]
thus it converges to $\varphi_0((\alpha_0, \beta_0), \alpha_0)$ as proved in Theorem \ref{thm:prob-win0}. If $\alpha_0 \leq w < \beta_0$ then the success probability of $\A_2(\alpha_0, \beta_0)$ knowing that $\tau \geq w$ is the same as the success probability of $\A_2(w, \beta_0)$, and we have from Theorem \ref{thm:prob-win0} that is converges to $\varphi_0((\alpha_0, \beta_0);w)$. Moreover, we have from the proof of Theorem \ref{thm:prob-win0} that
\[
\Prob(\A_2(\alpha_0, \beta_0) \text{ succeeds} \mid \tau \geq w N)
= \lambda w \log \frac{\beta_0}{w} + w(1 - \beta_0) + O\left( \sqrt{\frac{\log N}{N}}\right),
\]
the $O$ constant depends in that case on $w$, but we can make it only dependent of $\alpha_0$ using the bounds $\alpha_0 \leq w \leq 1$.\\
Now for $\beta_0 \leq w \leq 1$, denoting $t^\star$ the position of the best candidate overall and $\tau$ the stopping time of the algorithm, we have
\begin{align*}
\Prob(\A_2(\alpha_0, \beta_0) \text{ succeeds} \mid \tau \geq wN)
&= \Prob(\tau = t^\star \mid \tau \geq wN)\\
&= \sum_{t=wN}^N \Prob(t=t^\star \mid \tau = t) \frac{\Prob(\tau = t)}{\Prob(\tau \geq wN)}\\
&= \frac{1}{\Prob(\tau \geq wN)} \sum_{t=wN}^N \Prob(\tau = t \mid t=t^\star) \Prob(t = t^\star)\\
&= \frac{1}{N\Prob(\tau \geq wN)} \sum_{t=wN}^N \Prob(\tau = t \mid t=t^\star).
\end{align*}
where the last equation is obtained using the Bayes rule.
By Lemma \ref{lem:conc-card}, the event
\begin{equation}
%\label{eq:conc_0comp}
\tag{$\mathcal{C}_{\alpha_0}$}
    \forall \alpha_0 N \leq t \leq N \; : \;
            \left| |G^1_t| - \lambda t \right|
            < A_{\alpha_0} \sqrt{t \log N}
\end{equation}
is true with probability at least $1 - \frac{2}{\alpha_0 N}$, where  $A_{\alpha_0}$ a constant depending on $\alpha_0$. We showed in the proof of Theorem \ref{thm:prob-win0} that
\[
\Prob(\tau = t \mid t = t^\star) = \Prob(\tau = t \mid t = t^\star, \mathcal{C}_{\alpha_0})(1 - O(1/N))
= \frac{\alpha_0 \beta_0 N^2}{t^2} + O\left( \sqrt{\frac{\log N}{N}}\right),
\]
with the same proof technique we derive
\begin{align*}
\Prob(\tau \geq wN \mid \mathcal{C}_{\alpha_0})
&= \Prob(\best G^1_{wN-1} \in G^1_{\alpha_0 N-1}, \best G^2_{wN-1} \in G^2_{\beta_0 N-1} \mid \mathcal{C}_{\alpha_0})\\
&= \E\left[ \left. \frac{|G^1_{\alpha_0 N - 1}|}{|G^1_{wN-1}|}
   \frac{|G^2_{\beta_0 N - 1}|}{|G^2_{wN-1}|} \right| \mathcal{C}_{\alpha_0} \right] \\
&= \left( \frac{\alpha_0}{w} +  O\left( \sqrt{\frac{\log N}{N}}\right) \right) \left( \frac{\beta_B}{w} +  O\left( \sqrt{\frac{\log N}{N}}\right) \right)\\
&= \frac{\alpha_0 \beta_0}{w^2} + O\left( \sqrt{\frac{\log N}{N}}\right),
\end{align*}
thus 
\[
\Prob(\tau \geq wN)
= \Prob(\tau \geq wN \mid \mathcal{C}_{\alpha_0})(1-O(1/N))
= \frac{\alpha_0 \beta_0}{w^2} + O\left( \sqrt{\frac{\log N}{N}}\right)\enspace.
\]
the $O$ can be made independent of $w$ using that $\alpha_0 \leq w \leq 1$. Finally we obtain
\begin{align*}
\Prob(\A_2(\alpha_0, \beta_0) &\text{ succeeds} \mid \tau \geq wN, \mathcal{C}_{\alpha_0})\\
&= \left( \frac{w^2}{\alpha_0 \beta_0} + O\left( \sqrt{\frac{\log N}{N}}\right) \right)\frac{1}{N} \sum_{t=wN}^N
\left( \frac{\alpha_0 \beta_0 N^2}{t^2} + O\left( \sqrt{\frac{\log N}{N}}\right) \right)\\
&= \left( \frac{w^2}{\alpha_0 \beta_0} + O\left( \sqrt{\frac{\log N}{N}}\right) \right) 
\left( \alpha_0 \beta_0\left(\frac{1}{w} - 1\right) + 
O\left( \sqrt{\frac{\log N}{N}}\right) 
\right)\\
&= w - w^2 + O\left( \sqrt{\frac{\log N}{N}}\right)\enspace.
\end{align*}
\end{proof}




\subsubsection{Strong optimality of the thresholds $\alpha^\star_0, \beta^\star_0$}
The following Proposition states that for any DDT algorithm with budget $B \geq 0$, if at some step $w N$ the algorithm still did not stop and does not have any comparisons left, then its success probability is upper bounded by $\varphi_0(w; (\alpha_0^\star, \beta_0^\star))$. In particular, this implies that whatever is the initial budget $B$, choosing $\alpha_0 = \alpha^\star_0, \beta_0 = \beta^\star_0$ is optimal.

\begin{proposition}\label{prop:DDT0-optanyw}
For all $w \in [0, 1]$ and $\lambda \geq 1/2$, the mapping $(\alpha_0, \beta_0) \mapsto\varphi_0(w; (\alpha_0, \beta_0))$ is maximized over $\alpha_0 \leq \beta_0$ at
\[
    \alpha_0^\star = \lambda \exp\left( \frac{1}{\lambda} - 2\right)
    \quad \text{ and } \quad
    \beta_0^\star  = \lambda\enspace.
\]

\end{proposition}


\begin{proof}
   Fix some $\lambda \geq 1/2$. For convenience, let us drop the subscript $0$ from $g_0$.
%    and let $g(a, b) = \lambda a\log(b/a) + a(1 - b)$. Then, $\phi_0$ can be written as
%    \[
% \varphi_0((\alpha, \beta);w)
% = \left\{
%     \begin{array}{ll}
%         g(\alpha, \beta) & \mbox{if } w < \alpha,\\
%         g(w, \beta) & \mbox{if } \alpha \leq w < \beta,  \\
%         g(w, w) & w\geq \beta.
%     \end{array}
% \right.
% \]
Our goal is to show that for all $w \in [0, 1]$ and all $\alpha \leq 
\beta$ we have
\begin{align*}
    \begin{cases}
        g(\alpha, \beta) & \mbox{if } w < \alpha,\\
        g(w, \beta) & \mbox{if } \alpha \leq w < \beta,  \\
        g(w, w) & w\geq \beta.
    \end{cases} \quad\leq\quad
    \begin{cases}
        g(\alpha^\star, \lambda) & \mbox{if } w < \alpha^\star,\\
        g(w, \lambda) & \mbox{if } \alpha^\star \leq w < \lambda,  \\
        g(w, w) & w\geq \lambda
    \end{cases}\enspace,
\end{align*}
where $\alpha^\star = \lambda \exp(\lambda^{-1}-2)$.
Let $\alpha^*(\beta) = \beta\exp((1-\beta)\lambda^{-1}-1)$, the value that maximizes $g(\alpha, \beta)$ over the first coordinate.
Note that $\alpha^*(\beta)$ is maximized at $\beta = \lambda$ and $\alpha^*(\lambda) \leq \lambda $ as long as $\lambda \geq 1/2$, implying that $\alpha^*(\beta) \leq \lambda$ for all $\beta \in [0,1]$.
The following properties of $g$ hold if $\alpha \leq \beta$, which are direct consequence of the fact that $g(\alpha, \beta)$ is concave on $\alpha$, concave on $\beta$, maximized in $(\alpha^\star, \lambda)$ and the maximum of $g(\alpha, \beta)$ over $\alpha$ is always upper bounded by $\lambda$:
\begin{enumerate}
    \item\label{item_prop:1} $g(\alpha, \beta) \leq g(\alpha^\star, \lambda)$ for all $\alpha, \beta$ \,\,\,(i.e., $g(\cdot, \cdot)$ is maximized at $(\alpha^\star, \lambda)$);
    \item\label{item_prop:2} $g(\alpha, \beta) \leq g(\alpha, \lambda)$ for all $\alpha, \beta$ \,\,\,(i.e., $g(\alpha, \cdot)$ is maximized at $\lambda$);
    \item\label{item_prop:3} $g(\alpha, \lambda) \leq g(w, \lambda)$ for all $\alpha^\star \leq w \leq \alpha$ \,\,\,(i.e., $g(\cdot, \lambda)$ decreases on $[\alpha^\star, 1]$);
    \item\label{item_prop:5} $g(\alpha, \beta) \leq g(\alpha, w)$ for all $\lambda \leq w \leq \beta$ and all $\alpha$ \,\,\,(i.e., $g(\alpha, \cdot)$ decreases on $[\lambda, 1]$);
    \item\label{item_prop:7} $g(\alpha, \beta) \leq g(w, \beta)$ for all $\lambda \leq w \leq \alpha$ and all $\beta$ (i.e., $g(\cdot, \beta)$ decreases on $[\alpha^*(\beta), 1]$ and hence on $[\lambda, 1]$);
\end{enumerate}
Fix some $\alpha \leq \beta$.\\
\textbf{Case 1 ($w \leq \alpha^{\star}$)}: by item \eqref{item_prop:1} the desired statement holds. \\
\textbf{Case 2 ($w \in [\alpha^\star, \lambda)$)}: thanks to items~\eqref{item_prop:2},\eqref{item_prop:3} if $w \in [\alpha^\star, \alpha)$,
item~\eqref{item_prop:2} if $\alpha^* \leq \alpha \leq w \leq \beta$,
item~\eqref{item_prop:2} if $w \in [\beta, \lambda)$
\begin{align*}
    \begin{cases}
        g(\alpha, \beta) & \mbox{if } w < \alpha,\\
        g(w, \beta) & \mbox{if } \alpha \leq w < \beta,  \\
        g(w, w) & w\geq \beta.
    \end{cases} \quad\leq\quad g(w, \lambda)\enspace.
\end{align*}
and the desired statement holds.\\
\textbf{Case 3 ($w \geq \lambda$)}: thanks to items~\eqref{item_prop:5},\eqref{item_prop:7} if $w \leq \alpha$ and $\beta \geq \alpha \geq \lambda$, item~\eqref{item_prop:5} if $w \in [\alpha, \beta)$ and $\alpha \geq \lambda$
\begin{align*}
    \begin{cases}
        g(\alpha, \beta) & \mbox{if } w < \alpha,\\
        g(w, \beta) & \mbox{if } \alpha \leq w < \beta,  \\
        g(w, w) & w\geq \beta.
    \end{cases} \quad\leq\quad g(w, w)
\end{align*}
and the desired statement holds.
\end{proof}















\subsection{DDT with $B \geq 1$: Proof of Theorem \ref{thm:asymptoticDDT} and associated Lemmas}\label{appx:DDT-B}
In this first Lemma we estimate the probability of the events $\{\rho_1 = t\}$ and $\{\rho_1 \geq t\}$ for any $t$.
\begin{lemma}\label{lem:rho1=t}
Let $\rho_1 = \min\{ t \geq \alpha_B N : r_t = 1\}$, then we have for any $t \in \{ \alpha_B N, \ldots, \beta_B N-1\}$
\[
\Prob(\rho_1 \geq t) = \frac{\alpha_B N}{t} +  O\left( \sqrt{\frac{\log N}{N}}\right),
\qquad
\Prob(\rho_1 = t) = \frac{\alpha_B N}{t^2} +  O\left( \sqrt{\frac{\log N}{N^3}}\right),
\]
and for any $t \in \{ \beta_B N, \ldots, N \}$
\[
\Prob(\rho_1 \geq t) = \frac{\alpha_B \beta_B N^2}{t^2} +  O\left( \sqrt{\frac{\log N}{N}}\right),
\qquad
\Prob(\rho_1 = t) = \frac{2 \alpha_B \beta_B N^2}{t^3} +  O\left( \sqrt{\frac{\log N}{N^3}}\right),
\]
where all the $O$ constants depend only on $\lambda$ and $\alpha_B$.
\end{lemma}
\begin{proof}
By Lemma \ref{lem:conc-card} we have that the event
\begin{equation}
%\label{eq:conc_0comp}
\tag{$\mathcal{C}_{\alpha_B}$}
    \forall \alpha_B N \leq t \leq N \; : \;
            \left| |G^1_t| - \lambda t \right|
            < A_{\alpha_B} \sqrt{t \log N}
\end{equation}
is true with probability at least $1 - \frac{2}{\alpha_B N}$, where  $A_{\alpha_B} \eqdef \dfrac{1+\sqrt{{\alpha_B}}}{2\sqrt{2}{\alpha_B}^{1/4}}$,
thus for any $t \geq \alpha_B N$ we have 
\[
\Prob(\rho_1 \geq t)
= \Prob(\rho_1 \geq t \mid \mathcal{C}_{\alpha_B})(1 - O(1/N))
\]
and the same goes for $\Prob(\rho_1 = t)$. Conditionally to $\mathcal{C}_{\alpha_B}$ we have for any $t \geq \alpha_B N - 1$
\begin{align*}
|G^1_t| &\geq |G^1_{t+1}|-1 \geq \lambda t -1 - A_{\alpha_B} \sqrt{t \log N},\\
|G^2_t| &\geq |G^2_{t+1}|-1 \geq (1-\lambda) t -1 - A_{\alpha_B} \sqrt{t \log N},
\end{align*}
and therefore if $N$ is large enough then $|G^1_t|, |G^2_t|$ are both positive.\\
If $\alpha_B N \leq t < \beta_B N$ then
\begin{align*}
\Prob(\rho_1 \geq t \mid \mathcal{C}_{\alpha_B})
&= \Prob(\best G^1_{ t-1} \in G^1_{\alpha_B N-1} \mid \mathcal{C}_{\alpha_B})
= \E\left[ \left. \frac{|G^1_{\alpha_B N - 1}|}{|G^1_{t-1}|} \right| \mathcal{C}_{\alpha_B} \right] \\
&= \frac{\lambda \alpha_B N + O(\sqrt{N \log N})}{\lambda t + O(\sqrt{t\log t})} 
= \frac{\alpha_B N}{t}\left( 1 + O\left( \sqrt{\frac{\log N}{N}}\right) \right)\left( 1 - O\left( \sqrt{\frac{\log t}{t}}\right) \right)\\
&= \frac{\alpha_B N}{t} +  O\left( \sqrt{\frac{\log N}{N}}\right),
\end{align*}
since $\alpha_B N \leq t \leq N$ we have that $\alpha_B N/t = O(1)$ and thus
\[
\Prob(\rho_1 \geq t)
= \left(\frac{\alpha_B N}{t} +  O\left( \sqrt{\frac{\log N}{N}}\right) \right) \left(1 - O(1/N) \right)
= \frac{\alpha_B N}{t} + O\left( \sqrt{\frac{\log N}{N}}\right)
\]
where the $O$ constant only depends on $\lambda$ and $\alpha_B$ because $\alpha_B N \leq t \leq N$. On the other hand, since the event $\{r_t = 1\}$ is independent of the past observations conditionally to $(g_t, |G^{g_t}_t|)$, we can write
\begin{align*}
\Prob(\rho_1 = t \mid \mathcal{C}_{\alpha_B})
&= \Prob(\rho_1 = t \mid \rho_1 \geq t, \mathcal{C}_{\alpha_B})\Prob(\rho_1 \geq t \mid \mathcal{C}_{\alpha_B})\\
&= \Prob(r_t = 1, g_t = 1 \mid \mathcal{C}_{\alpha_B})\Prob(\rho_1 \geq t \mid \mathcal{C}_{\alpha_B})\\
&= \lambda \E\left[ \left. \frac{1}{|G^1_t|} \right| \mathcal{C}_{\alpha_B} \right]
\Prob(\rho_1 \geq t \mid \mathcal{C}_{\alpha_B})\\
&= \left( \frac{1}{t} + O\left( \sqrt{\frac{\log N}{N^3}} \right)\right)\left( \frac{\alpha_B N}{t} +  O\left( \sqrt{\frac{\log N}{N}}\right) \right)
= \frac{\alpha_B N}{t^2} + O\left( \sqrt{\frac{\log N}{N^3}} \right),
\end{align*}
we deduce that
\[
\Prob(\rho_1 = t)
= \left( \frac{\alpha_B N}{t^2} + O\left( \sqrt{\frac{\log N}{N^3}} \right) \right) (1 - O(1/N))
= \frac{\alpha_B N}{t^2} + O\left( \sqrt{\frac{\log N}{N^3}} \right)\enspace.
\]

Similarly for $t \geq \beta_0 N$ we have
\begin{align*}
\Prob(\rho_1 \geq t \mid \mathcal{C}_{\alpha_B})
&= \Prob(\best G^1_{t-1} \in G^1_{\alpha_B N-1}, \best G^2_{t-1} \in G^2_{\beta_B N-1} \mid \mathcal{C}_{\alpha_B})\\
&= \E\left[ \left. \frac{|G^1_{\alpha_B N - 1}|}{|G^1_{t-1}|}
   \frac{|G^2_{\beta_B N - 1}|}{|G^2_{t-1}|} \right| \mathcal{C}_{\alpha_B} \right] \\
&= \left( \frac{\alpha_B N}{t} +  O\left( \sqrt{\frac{\log N}{N}}\right) \right) \left( \frac{\beta_B N}{t} +  O\left( \sqrt{\frac{\log N}{N}}\right) \right)\\
&= \frac{\alpha_B \beta_B N^2}{t^2} + O\left( \sqrt{\frac{\log N}{N}}\right),
\end{align*}
and also
\begin{align*}
\Prob(\rho_1 = t \mid \mathcal{C}_{\alpha_B})
&= \Prob(\rho_1 = t \mid \rho_1 \geq t, \mathcal{C}_{\alpha_B})\Prob(\rho_1 \geq t \mid \mathcal{C}_{\alpha_B})\\
&= \Prob(r_t = 1 \mid \mathcal{C}_{\alpha_B})\Prob(\rho_1 \geq t \mid \mathcal{C}_{\alpha_B})\\
&= \lambda \left( \lambda \E\left[ \left. \frac{1}{|G^1_t|} \right| \mathcal{C}_{\alpha_B} \right] 
+(1-\lambda) \E\left[ \left. \frac{1}{|G^2_t|} \right| \mathcal{C}_{\alpha_B} \right] \right)
\Prob(\rho_1 \geq t \mid \mathcal{C}_{\alpha_B})\\
&= \left( \frac{2}{t} + O\left( \sqrt{\frac{\log N}{N^3}} \right)\right)\left( \frac{\alpha_B \beta_B N^2}{t^2} +  O\left( \sqrt{\frac{\log N}{N}}\right) \right)\\
&= \frac{2\alpha_B \beta_B N^2}{t^3} + O\left( \sqrt{\frac{\log N}{N^3}} \right),
\end{align*}
we deduce $\Prob(\rho_1 \geq t), \Prob(\rho_1 = t)$ as in the case $\alpha_B \leq t \leq \beta_B$.
\end{proof}


\subsubsection{Success probability knowing $\rho_1$}
Now using Lemma \ref{lem:rho1=t}, we estimate the success probability of the algorithm conditionally to $\rho_1$.

\begin{lemma}\label{lem:prob-win-rho1}
For any $t \in \{\alpha_B N, \ldots, N\}$ we have
\[
\Prob(\A^B \text{ succeeds} \mid \rho_1 = t)
= \left\{
    \begin{array}{ll}
        \dfrac{\lambda t}{N} + (1-\lambda)\rd^{B-1}_{N,t+1} + O\left( \sqrt{\frac{\log N}{N}} \right)  & \mbox{if } \alpha_B \leq \frac{t}{N} < \beta_B \\
        \dfrac{t}{2N} + \dfrac{1}{2}\rd^{B-1}_{N,t+1} + O\left( \sqrt{\frac{\log N}{N}} \right) & \mbox{if } \frac{t}{N} \geq \beta_B
    \end{array}\enspace.
\right.
\]
where the $O$ constants depend on $\lambda$ and $\alpha_B$.
\end{lemma}


\begin{proof}
We denote in all the proof $t^\star$ the position of the best candidate overall.
For any $\alpha_B N \leq t \leq \beta_B N$ we have
\begin{align*}
\Prob(\A^B \text{ succeeds} \mid \rho_1 = t)
=& \Prob(\A^B \text{ succeeds}, R_t = 1 \mid \rho_1 = t)\\
& + \Prob(\A^B \text{ succeeds} \mid R_t \neq 1, \rho_1 = t)\Prob(R_t \neq 1 \mid \rho_1 = t),
\end{align*}
conditionally to $\rho_1 = t$, we have $\{ \A^B \text{ succeeds}, R_t = 1\} = \{t = t^\star \}$, because if $R_t = 1$ then $\A^B$ necessarily stops on $t$. Using this observation, the Bayes rule and Lemma \ref{lem:rho1=t} we obtain
\begin{align*}
\Prob(\A^B \text{ succeeds}, R_t = 1 \mid \rho_1 = t)
= \Prob(t = t^\star \mid \rho_1 = t)
= \Prob(\rho_1 = t \mid t = t^\star)\frac{\Prob(t = t^\star)}{\Prob(\rho_1=t)}\enspace,
\end{align*}
secondly, we have 
\[
\Prob(\A^B \text{ succeeds}, R_t \neq 1 \mid \rho^B_1 = t)
= \Prob(\A_{B-1} \text{ succeeds} \mid \rho^{B-1}_1 \geq t+1)
= \rd^{B-1}_{N,t+1}\enspace,
\]
in fact, if $\rho^B_1 = t$ and $R_t \neq 1$ then $\A^B$ uses a comparison but does not stop and moves to the next candidate, $\A^B$ is therefore exactly in the same state as $\A_{B-1}$ if the latter still has not used any comparison. Finally, using the Bayes rule again gives
\[
\Prob(R_t \neq 1 \mid \rho_1 = t)
= 1 - \Prob(\rho_1 = t \mid R_t = 1) \frac{\Prob(R_t = 1)}{\Prob(\rho_1 = t)}\enspace,
\]
it yields
\begin{align*}
\Prob(\A^B \text{ succeeds} \mid \rho_1 = t)
=& \Prob(\rho_1 = t \mid t = t^\star)\frac{\Prob(t = t^\star)}{\Prob(\rho_1=t)}\\
&\quad + \rd^{B-1}_{N,t+1} \left( 1 - \Prob(\rho_1 = t \mid R_t = 1) \frac{\Prob(R_t = 1)}{\Prob(\rho_1 = t)} \right)\\
=& \frac{\Prob(\rho_1 = t \mid t = t^\star)}{N \Prob(\rho_1 = t)}
+ \rd^{B-1}_{N,t+1} \left( 1 - \frac{\Prob(\rho_1 = t \mid R_t = 1)}{t\Prob(\rho_1 = t)} \right)\enspace.
\end{align*}
The expression above must be evaluated differently depending on the position of $t$ relatively to $\alpha_B N$ and $\beta_B N$. If $\alpha_B N \leq t < \beta_B N$ then conditionally to $t = t^\star$ we have $\{ \rho_1 = t \} = \{ \rho_1 \geq t, g_t = 1\}$, because if the latter event happens then the algorithm will by design make a comparison at $t$, and if $\rho_1 = t$ then necessary $g_t = 1$ since the algorithm can only select elements of $G^1$ at steps $\{\alpha_B N, \ldots, \beta_B N - 1\}$. Using this argument and Lemma \ref{lem:rho1=t} gives 
\begin{align*}
\frac{\Prob(\rho_1 = t \mid t = t^\star)}{\Prob(\rho_1 = t)}
&= \frac{\Prob(\rho_1 \geq t, g_t = 1 \mid t = t^\star)}{\Prob(\rho_1 = t)}
= \frac{\lambda \Prob(\rho_1 \geq t)}{\Prob(\rho_1 = t)}\\
&= \lambda \frac{\frac{\alpha_B N}{t}\left(1 + O\left( \sqrt{\frac{\log N}{N}} \right) \right)}{\frac{\alpha_B N}{t^2}\left(1 + O\left( \sqrt{\frac{\log N}{N}} \right) \right)}
= \lambda t\left(1 + O\left( \sqrt{\frac{\log N}{N}} \right)\right),
\end{align*}
the event $\{ \rho_1 \geq t \}$ is independent of $\{t = t^\star\}$ because it is measurable with respect to the ranks and groups of the past candidates, while $\{t = t^\star\}$ is independent of them.\\
Conditionally to $R_t = 1$ we can use the same argument to compute $\Prob(\rho_1 = t \mid R_t = 1)$, we obtain 
\[
\frac{\Prob(\rho_1 = t \mid R_t = 1)}{\Prob(\rho_1 = t)}
= \lambda t\left(1 + O\left( \sqrt{\frac{\log N}{N}} \right)\right),
\]
consequently,
\begin{align*}
\Prob(\A^B \text{ succeeds} \mid \rho_1 = t)
&= \frac{\lambda t}{N}\left(1 + O\left( \sqrt{\frac{\log N}{N}} \right)\right)
+ \rd^{B-1}_{t+1} \left( 1 - \lambda \left(1 + O\left( \sqrt{\frac{\log N}{N}} \right)\right) \right)\\
&= \frac{\lambda t}{N} + (1-\lambda)\rd^{B-1}_{t+1} + O\left( \sqrt{\frac{\log N}{N}} \right)\enspace.    
\end{align*}
with the $O$ constant only depending on $\lambda$ and $\alpha_B$.

Let us now consider $\beta_B N \leq t \leq N$. Since $\rho_1$ can belong to any of the two groups, and conditionally to $\{ t = t^\star \}$ we have that $\{ \rho_1 = t \} = \{ \rho_1 \geq t \}$, and we using Lemma \ref{lem:rho1=t}
\[
\frac{\Prob(\rho_1 = t \mid t = t^\star)}{\Prob(\rho_1 = t)}
= \frac{\Prob(\rho_1 \geq t)}{\Prob(\rho_1 = t)}
= \frac{\lambda \Prob(\rho_1 \geq t)}{\Prob(\rho_1 = t)}
= \frac{t}{2}\left(1 + O\left( \sqrt{\frac{\log N}{N}} \right)\right),
\]
and in the same way
\[
\frac{\Prob(\rho_1 = t \mid R_t = 1)}{\Prob(\rho_1 = t)}
= \frac{t}{2}\left(1 + O\left( \sqrt{\frac{\log N}{N}} \right)\right),
\]
we deduce that
\[
\Prob(\A^B \text{ succeeds} \mid \rho_1 = t)
= \frac{t}{2N} + \frac{1}{2}\rd^{B-1}_{t+1} + O\left( \sqrt{\frac{\log N}{N}} \right)\enspace.    
\]

\end{proof}



\subsubsection{Recursive formula for $\rd^B_{N,wN}$}

\begin{lemma}\label{lem:induction-Ubn}
For any $w \in [\alpha_B,\beta_B)$ we have 
\begin{align*}
\rd^B_{N,wN}
=& \lambda w \log\frac{\beta_B}{w} + w(1 - \beta_B)
+ (1-\lambda)w \left(\frac{1}{N} \sum_{t=wN}^{\beta_B N}
        \frac{\rd^{B-1}_{N,t+1}}{(t/N)^2}\right)\\
&\quad + w\beta_B \left(\frac{1}{N} \sum_{t=\beta_B N + 1}^{N} \frac{\rd^{B-1}_{N,t+1}}{(t/N)^3}\right)
+ O\left( \sqrt{\frac{\log N}{N}}\right),
\end{align*}
and for any $w \in [\beta_B, 1]$
\[
\rd^B_{N,wN}
= w - w^2 + w^2 \left( \frac{1}{N}\sum_{t=wN}^{N} \frac{\rd^{B-1}_{N,t+1}}{(t/N)^3} \right) + O\left( \sqrt{\frac{\log N}{N}}\right)\enspace.
\]
where the $O$ constants only depend on $\lambda$ and $\alpha_B$.
\end{lemma}


\begin{proof}
For any $w \in [\alpha_B,1]$, we can write
\begin{align}\label{eq:lemU-0}
\rd^B_{N,wN}
&= \sum_{t=wN}^{N} \Prob(\A^B \text{ succeeds} \mid \rho_1 = t)\Prob(\rho_1 = t \mid \rho_1 \geq wN)\\ \nonumber
&= \frac{1}{\Prob(\rho_1 \geq wN)}
    \sum_{t=wN}^{N} \Prob(\A^B \text{ succeeds} \mid \rho_1 = t)\Prob(\rho_1 = t).
\end{align}

If $w \geq \beta_B$ then by Lemmas \ref{lem:rho1=t} and \ref{lem:prob-win-rho1}
\begin{align}\nonumber
\sum_{t=wN}^{N} \Prob(\A^B& \text{ succeeds} \mid \rho_1 = t)\Prob(\rho_1 = t)\\ \nonumber
&= \sum_{t=wN}^{N} \left( \frac{t}{2N} + \frac{1}{2}\rd^{B-1}_{N,t+1} + O\left( \sqrt{\frac{\log N}{N}} \right) \right)
\left( \frac{2 \alpha_B \beta_B N^2}{t^3} +  O\left( \sqrt{\frac{\log N}{N^3}}\right) \right)\\ \nonumber
&= \sum_{t=wN}^{N} \left( 
\frac{\alpha_B \beta_B N}{t^2} + \frac{\alpha_B \beta_B N^2 \rd^{B-1}_{N,t+1}}{t^3} + O\left( \sqrt{\frac{\log N}{N^3}}\right)
\right)\\ \label{eq:lemU-1}
&= \alpha_B\beta_B\left( \frac{1}{w} - 1 \right)
+ \frac{\alpha_B\beta_B }{N}\sum_{t=wN}^{N} \frac{\rd^{B-1}_{N,t+1}}{(t/N)^3}
+ O\left( \sqrt{\frac{\log N}{N}}\right),
\end{align}
consequently, using Lemma \ref{lem:rho1=t} to estimate $\Prob(\rho_1 \geq wN)$ and substituting into Equation \ref{eq:lemU-0}
\begin{align*}
\rd^B_{N,wN}
&= \frac{1}{\Prob(\rho_1 \geq wN)}
    \sum_{t=wN}^{N} \Prob(\A^B \text{ succeeds} \mid \rho_1 = t)\Prob(\rho_1 = t)\\
&= \left( \frac{w^2}{\alpha_B\beta_B} + O\left( \sqrt{\frac{\log N}{N}}\right) \right) 
\left(
\alpha_B\beta_B\left( \frac{1}{w} - 1 \right)
+ \frac{\alpha_B\beta_B }{N}\sum_{t=wN}^{N} \frac{\rd^{B-1}_{N,t+1}}{(t/N)^3}
+ O\left( \sqrt{\frac{\log N}{N}}\right)
\right)\\
&= w - w^2 + w^2 \left( \frac{1}{N}\sum_{t=wN}^{N} \frac{\rd^{B-1}_{N,t+1}}{(t/N)^3} \right) + O\left( \sqrt{\frac{\log N}{N}}\right)\enspace.
\end{align*}

If we consider $w \in [\alpha_B, \beta_B)$ then again by Lemmas \ref{lem:rho1=t} and \ref{lem:prob-win-rho1}
\begin{align*}
\sum_{t=wN}^{\beta_B N-1} \Prob(\A^B& \text{ succeeds} \mid \rho_1 = t)\Prob(\rho_1 = t)\\
&= \sum_{t=wN}^{\beta_B N -1} \left( \frac{\lambda t}{N} + (1-\lambda)\rd^{B-1}_{N,t+1} + O\left( \sqrt{\frac{\log N}{N}} \right) \right)
\left( \frac{\alpha_B N}{t^2} +  O\left( \sqrt{\frac{\log N}{N^3}}\right) \right)\\
&= \sum_{t=wN}^{\beta_B N -1} \left( 
\frac{\lambda \alpha_B}{t} + (1-\lambda) \frac{\alpha_B N \rd^{B-1}_{N,t+1}}{t^2} + O\left( \sqrt{\frac{\log N}{N^3}}\right)
\right)\\
&= \lambda \alpha_B \log \frac{\beta_B}{w}
+ (1-\lambda) \frac{\alpha_B}{N} \sum_{t=wN}^{\beta_B N -1} \frac{\rd^{B-1}_{N,t+1}}{(t/N)^2}
+ O\left( \sqrt{\frac{\log N}{N}}\right),
\end{align*}
and using Equation \ref{eq:lemU-1} for $\beta_B$ instead of $w$ we get
\begin{align*}
\sum_{t=\beta_B N}^{N} \Prob(\A^B& \text{ succeeds} \mid \rho_1 = t)\Prob(\rho_1 = t)\\
&= \alpha_B( 1 - \beta_B )
+ \frac{\alpha_B\beta_B }{N}\sum_{t=\beta_B N}^{N} \frac{\rd^{B-1}_{N,t+1}}{(t/N)^3}
+ O\left( \sqrt{\frac{\log N}{N}}\right),
\end{align*}
finally, using Lemma \ref{lem:rho1=t} to estimate $\Prob(\rho_1 \geq wN)$ and by Equation \ref{eq:lemU-0}

\begin{align*}
\rd^B_{N,wN}
=& \left( \frac{w}{\alpha_B} + O\left(\sqrt{\frac{\log N}{N}}\right)\right)
    \sum_{t=w N}^{N} \Prob(\A^B \text{ succeeds} \mid \rho_1 = t)\Prob(\rho_1 = t)\\
=& \frac{w}{\alpha_B}\sum_{t=w N}^{N} \Prob(\A^B \text{ succeeds} \mid \rho_1 = t)\Prob(\rho_1 = t) + O\left(\sqrt{\frac{\log N}{N}}\right)\\
=& \lambda w \log\frac{\beta_B}{w}
+ (1-\lambda) \frac{w}{N} \sum_{t=wN}^{\beta_B N -1} \frac{\rd^{B-1}_{N,t+1}}{(t/N)^2}\\
& \quad + w( 1 - \beta_B )
+ \frac{w\beta_B }{N}\sum_{t=\beta_B N}^{N} \frac{\rd^{B-1}_{N,t+1}}{(t/N)^3}
+ O\left( \sqrt{\frac{\log N}{N}}\right),
\end{align*}
and this concludes the proof.
\end{proof}





\subsubsection{Proof of the Theorem}

Finally, we remind this classical result on Riemann sums that we will use to prove Theorem \ref{thm:asymptoticDDT}.
\begin{lemma}\label{lem:riemann-sum}
If $f$ is a continuous and piecewise $C^1$ function on $[a,b]$ then for any positive integer $N$ we have
\[
\left|
\frac{b-a}{N}\sum_{t=1}^N f\left( a + \frac{b-a}{N} \right)
- \int_a^b f(u)du 
\right|
\leq \left( \max_{[a,b]}|f'| \right) \frac{(b-a)^2}{2N}\enspace.
\]
\end{lemma}

Using the previous lemmas, we will now prove the theorem by induction over $B$.

\begin{proof}[Proof of Theorem \ref{thm:asymptoticDDT}]
For $B = 0$. we have by Proposition \ref{prop:asymptoticDDT0} that $\rd^0_{N,wN}$ converges to $\varphi_0(w)$, which is a continuous and piecewise $C^1$ function on $(0,1]$. Moreover, for any $w \in (0,1]$ we have from the proof of Proposition \ref{prop:asymptoticDDT0} that
\[
\rd^0_{N,wN} 
=\varphi_0(w) + O\left(\sqrt{\frac{\log N}{N}}\right)\enspace,
\]
with the $O$ constant depending only on $\lambda, \alpha_0$. We will show during the induction that this property is conserved and that the $O$ constants depend only on $\lambda, B$ and on the thresholds.\\
Now let $B \geq 1$ and assume that the result is true for $B-1$. For $w < \alpha_B$ it is immediately true because $w \mapsto \rd^B_{N,wN}$ is constant on $[0,\alpha_B]$ and equal to $\rd^B_{N,\alpha_B N}$. If $w \in [\alpha_B, \beta_B)$ then the induction hypothesis gives
\begin{align*}
\frac{1}{N} \sum_{t=wN}^{\beta_B N} \frac{\rd^{B-1}_{N,t+1}}{(t/N)^2}
&= \frac{1}{N} \sum_{t=wN}^{\beta_B N} \frac{1}{(t/N)^2}\left( \varphi_{B-1}\left(\frac{t+1}{N}\right) + O\left( \sqrt{\frac{\log N}{N}}\right) \right)\\
&= \frac{1}{N} \sum_{t=wN}^{\beta_B N} \frac{\varphi_{B-1}\left(\frac{t+1}{N}\right)}{(t/N)^2} + O\left( \sqrt{\frac{\log N}{N}}\right)\\
&= \int_w^{\beta_B} \frac{\varphi_{B-1}(u)}{u^2}du + O\left( \sqrt{\frac{\log N}{N}}\right)\enspace,
\end{align*}
where we used for the second equality that 
$\sum_{t=wN}^{\beta_B N} \frac{1}{t^2}
\leq \sum_{wN+1}^{\beta_B N}(\frac{1}{t-1} - \frac{1}{t})
= \frac{1}{wN}- \frac{1}{\beta_B N} \leq \frac{1}{\alpha_B N}
$,
and for the last one we used Lemma \ref{lem:riemann-sum} and the continuity of $\varphi_{B-1}$. The initial $O$ constant depends on $\lambda, \alpha_0, \ldots, \alpha_{B-1}$, to which we add $1/\alpha_B$ and $\sup_{[\alpha_B, \beta_B]}(\frac{d}{du}\frac{\varphi_{B-1}(u)}{u^2})$, which is well defined because $\varphi_{B-1}(u)/u^2$ is piecewise $C^1$ on $[\alpha_B,1]$ and which is a constant depending on $\lambda, B, (\alpha_0,\beta_0), \ldots, (\alpha_{B}, \beta_{B})$.\\
In the same fashion we prove that 
\[
\frac{1}{N} \sum_{t=\beta_B N + 1}^{N} \frac{\rd^{B-1}_{N,t+1}}{(t/N)^3}
= \int_{\beta_B}^1 \frac{\varphi_{B-1}(u)}{u^3}du + O\left( \sqrt{\frac{\log N}{N}}\right)\enspace,
\]
and that for $w \in [\beta_B, 1)$
\[
\frac{1}{N} \sum_{t=w N}^{N} \frac{\rd^{B-1}_{N,t+1}}{(t/N)^2}
= \int_{w}^1 \frac{\varphi_{B-1}(u)}{u^2}du + O\left( \sqrt{\frac{\log N}{N}}\right)\enspace,
\]
this equation with Lemma \ref{lem:induction-Ubn} prove that on both intervals $[\alpha_B, \beta_B)$ and $[\beta_B, 1]$ we can write
\[
\rd^B_{N,wN}
= \varphi_B(w) + O\left( \sqrt{\frac{\log N}{N}}\right)\enspace,
\]
with $\varphi_B$ is defined on $[\alpha_B, \beta_B)$ by
\begin{align*}
\varphi_B(w)
= \lambda w \log\frac{\beta_B}{w} + w(1 - \beta_B)
+ (1-\lambda)w \int_w^{\beta_B} \frac{\varphi_{B-1}(u)}{u^2}\diff u
+ w \beta_B \int_{\beta_B}^1 \frac{\varphi_{B-1}(u)}{u^3}\diff u\enspace,
\end{align*}
and on $[\beta_B,1)$ by
\[
\varphi_B(w)
= w - w^2 + w^2\int_w^1 \frac{\varphi_{B-1}(u)}{u^3}du.
\]
which can be expressed as stated in the theorem. We verify that it is continuous in $\beta_B$, and the induction hypothesis guarantees that $\varphi_B$ is continuous and piecewise $C^1$ on $[\alpha_B, 1]$.
\end{proof}












\subsection{Success probability of optimal DDT algorithm: Proof of Proposition \ref{prop:lb-optDDT} }\label{appx:lb-optDDT}

\begin{proof}[Proof of Proposition \ref{prop:lb-optDDT}]
Since $(\alpha^\star_b, \beta^\star_b)_{b=0}^B$ are optimal thresholds we have that the limit success probability of the algorithm is higher than the success probability of any other DDT algorithm, in particular if we choose all the thresholds equal to the same value 
\begin{align*}
\lim_{N \to \infty} \Prob( \A_2((\alpha^\star_b, \beta^\star_b)_{b=0}^B) \text{ succeeds})
&= \varphi_B(0; (\alpha^\star_b, \beta^\star_b)_{b=0}^B)\\
&\geq \sup_{\w \in (0,1)} \varphi_B(0; (\w, \w)_{b=0}^B)\\
&\geq \frac{1}{e} - \frac{1}{e(B+1)!}\enspace,
\end{align*}
where the last lower bound is deduced from Corollary \ref{cor:success_proba_bound_budget}.  We also have by Proposition \ref{prop:asymptoticDDT0} that $\lambda \geq 1/2$ then
\[
\varphi_B(0; (\alpha^\star_b, \beta^\star_b)_{b=0}^B)
\geq \sup_{(\alpha_0, \beta_0) \in (0,1)} \varphi_0(0; (\alpha_0, \beta_0))
\geq \lambda^2 \exp\left( \frac{1}{\lambda} - 2 \right)\enspace,
\]
with simple computations we obtain that the latter expression is lower bounded by $\frac{1}{e} - \frac{\lambda(1-\lambda)}{2}$, and this bound remains true also if $\lambda < 1/2$ (replace $\lambda$ by $1-\lambda$), hence
\[
\lim_{N \to \infty} \Prob( \A_2((\alpha^\star_b, \beta^\star_b)_{b=0}^B) \text{ succeeds})
\geq 
\frac{1}{e} - \frac{\lambda(1-\lambda)}{2}\enspace,
\]
which concludes the proof.
\end{proof}
