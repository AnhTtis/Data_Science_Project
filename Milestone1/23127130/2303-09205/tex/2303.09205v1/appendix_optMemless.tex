\subsection{Proof of Proposition \ref{prop:success-memless-skip} and additional properties}\label{appx:memless}


\subsubsection{Proof of Proposition \ref{prop:success-memless-skip}}
\begin{proof}
We show that the following claims hold
\begin{enumerate}[label=(\roman*)]
\item $\Prob(R_t = 1 \mid \F_{t-1}) = \frac{1}{t}$,
\item knowing $(g_t, N^1_{t-1})$, the distribution of $r_t$ is independent of $\F_{t-1}$,
\item if $s \geq t$ then $A_{s,1}$ is measurable with respect to $(N^1_{t-1},B_t,\{ g_u, r_u, \xi_{u,1} \}_{u=t}^{s})$.
\end{enumerate} 
\textbf{First claim} 
We remind that 
$\F_{t-1} = \{r_s,g_s, \act_{s, 1}, R_s\indic{\act_{s, 1} = \acomp}, \act_{s, 2}\indic{\act_{s, 1} = \acomp}\}_{s=1}^{t-1}$. Since the distributions of $\act_{s, 1}, \act_{s, 2}$ are measurable with respect to $\{r_s,R_s,g_s\}_{s=1}^{t-1}$, we only need to show the independence between $R_t$ and the past observations.\\  
We have that $\{R_t = 1\} = \{x_t > \max\{x_s\}_{s=1}^{t-1}\}$, this event is independent of the groups of the past candidates, and by Proposition \ref{prop:max2sets} it is also independent of their relative ranks, therefore $R_t$ is independent of $\F_{t-1}$ and 
$\Prob(R_t = 1 \mid \F_{t-1}) = \Prob(R_t = 1)
=\frac{1}{t}$.\\
\textbf{Second claim}
We only need to show the independence between $r_t$ and the past observations conditionally to $(g_t, N^1_{t-1})$. 
Knowing $G^1_t$, we have that $r_t$ is independent of the relative ranks of $\{x_1, \ldots, x_N\}$ by Proposition \ref{prop:max2sets}, thus it is independent of $\{r_s\}_{s<t}, \{R_s\}_{s<t}$. We also have by  Proposition \ref{prop:max2sets} that knowing $(g_t, N^1_{t-1})$, the probability of $\{r_t = 1\}$ is $1/N^{g_t}_t$ independently of $G^1_t$, where 
\[
N^{g_t}_t = \indic{g_t = 1}(N^1_{t-1} +1) + (1- \indic{g_t = 1})(t - N^1_{t-1}),
\]
hence knowing $(g_t, N^1_{t-1})$,$r_t$ is independent of $\{r_s,R_s,g_s\}_{s=1}^t$, thus independent of $\F_{t-1}$. \\
\textbf{Third claim} Let us assume for simplicity that the randomness in $\act_{s, 1}, \act_{s, 2}$ knowing $\F_t$ is due to some random noise $\xi_{s,1}, \xi_{s,2}$ independent of all the rest. We will prove the claim using a simple induction. For $s=t$ it is true because $\A$ is memory-less. Assume that it is true up to some $t\leq s < N$, $A_{s+1,1}$ is $(N^1_{s-1}, B_{s+1}, g_{s+1}, r_{s+1}, \xi_{s+1,1})-$measurable, and we have
\[
N^1_{s} = N^1_{t-1} + \sum_{u=t}^s \indic{g_t = 1}, \qquad
B_{s+1} = B_t - \sum_{u=t}^{s} \indic{\act_{u, 1} = \acomp},
\]
$N^1_{s}$ is therefore $(N^1_{s}, \{g_u\}_{u=t}^s)$-measurable and $B_{s+1}$ is $(B_t,\{ \act_{u, 1} \}_{t \leq u\leq s})$-measurable, using the induction hypothesis we deduce that it is measurable with respect to $(N^1_{t-1}, B_t,\{ g_u, r_u, \xi_{u,1} \}_{u=t}^{s})$, and thus the result.\\
\textbf{Proof of the proposition} 
Let $t^\star$ be the the position of the best candidates overall,
\[
\Prob(\A \text{ succeeds} \mid \tau \geq t, \F_{t-1}) 
= \Prob(\tau = {t^\star} \mid \tau \geq t, \F_{t-1}) 
= \E_T \left[ 
        \Prob(\tau = {t^\star} \mid \tau \geq t, \F_{t-1}, {t^\star}) 
    \right],
\]
if ${t^\star} < t$ then $\Prob(\A \text{ succeeds} \mid \tau \geq t, \F_{t-1}, T) = 0$, otherwise this probability only depends on the future actions of $\A$, because conditionally to $\{ \tau \geq t\}$, we have that 
\[
\tau = \min \{s \geq t \mid \act_{s, 1} = \astop \text{ or } \act_{s, 2} = \astop \}.
\]
Conditionally to $(N^1_{t-1}, B_t)$, we have that the distributions of $\{\act_{s, 1}, \act_{s, 2}\}_{s \geq t}$ are measurable with respect to $\{g_u, r_u, R_u\}_{u=t}^s$. The variables $\{g_u\}_{u \geq t}$ are independent of $\F_{t-1}$, and $\{r_u, R_u\}_{u \geq t}$ are also independent of $\F_{t-1}$ knowing $(N^1_{t-1}, B_t)$, hence $\{\act_{s, 1}, \act_{s, 2}\}_{s \geq t}$ are independent of $\F_{t-1}$ and thus the result.
\end{proof}







\subsubsection{Another useful property of memory-less algorithms}
The following proposition states that when a memory-less stops at some $t$, then knowing $(r_t,R_t,N^1_t)$ we can easily express its success probability.

\begin{proposition}\label{prop:success-memless-stop}
Under the same assumptions of the last proposition, we have
\begin{align*}
\Prob(\A \text{ succeeds} \mid \act_{t, 1} = \astop, r_t, g_t, B_t, N^1_t) 
&= \indic{r_t = 1}\frac{ N^{g_t}_t}{N},
\\
\Prob(\A \text{ succeeds} \mid \act_{t, 2} = \astop, r_t, g_t, B_t, N^1_t, R_t) 
&= \indic{R_t = 1}\frac{t}{N}.
\end{align*}
\end{proposition}

\begin{proof}
Let us denote $t^\star$ the position of the best candidate overall. Conditionally to $\act_{t,1} = \astop$, the $\{ \A \text{ succeeds} \};= \{ t^\star = t \}$. If $r_t \neq 1$ then with probability 1 we have $t^\star \neq t$, and if $r_t = 1$, then the only information about the rank of $x_t$ contained in $r_t, g_t, B_t, N^1_t$ is that $x_t$ is better than all the past candidates of $G^{g_t}$, i.e that it is the best among a collection of $N^{g_t}_t$ candidates, which happens with probability $1/N^{g_t}_t$. The success probability is therefore
\begin{align*}
\Prob(\A \text{ succeeds} &\mid \act_{t, 1} = \astop, r_t=1, g_t, B_t, N^1_t)\\
&= \Prob(t^\star = t \mid \act_{t, 1} = \astop, r_t=1, g_t, B_t, N^1_t)
= \Prob(t^\star = t \mid  r_t=1, g_t, N^1_t)\\
&= \frac{\Prob(t^\star = t \mid  g_t, N^1_t)}{\Prob(r_t=1 \mid g_t, N^1_t)}
= \frac{N^{g_t}_t}{N}.
\end{align*}
which means that it is null when $r_t \neq 1$ and equal to $N^{g_t}_t/N$ otherwise.\\

If $\act_{t, 2} = \astop$, then necessarily $(\act_{t, 1} = \acomp)$ and $R_t$ is known. If $R_t \neq 1$ then $\Prob(t^\star = t \mid \act_{t, 2} = \astop, R_t \neq 1, \F_t) = 0$, otherwise $R_t = 1$, implying that $r_t = 1$, thus $R_t = 1$ is the maximal information that we get from all the past on the absolute rank of $x_t$, thus 
\[
\Prob(\A \text{ succeeds} \mid \act_{t, 1} = \astop, R_t = 1)
= \Prob(t^\star = t \mid R_t = 1)
= \frac{t}{N}.
\]
\end{proof}








\subsection{Optimal algorithm via dynamic programming: Proofs of Section \ref{sec:opt-dynprog}}\label{appx:opt-dynprog}

\subsubsection{Proof of Proposition \ref{prop:dynprog0}}

\begin{proof}
Let $\A$ be any memory-less algorithm for the 2-groups secretary problem with no comparisons between the two groups, with a stopping time $\tau$, and let us denote for $t \geq 1$
\[
\rwd^0_{t,m} \eqdef \Prob (\A \text{ succeeds} \mid \tau \geq t, N^1_t).
\]

$\{\tau \geq t\}$ is $\F_{t-1}$-measurable, and the distribution of $r_t$ is independent of $\F_{t-1}$ knowing $(N^1_{t-1}, g_t)$, therefore $r_t$ is independent of $\{ \tau \geq t\}$, thus we have
\begin{align}
\rwd^0_{t,N^1_{t-1}t} \nonumber \label{eq:req-rwd0-0}
=& \E_{g_t}[ \Prob(r_t=1 \mid \tau \geq t, N^1_{t-1}, g_t)
\Prob (\A \text{ succeeds} \mid \tau \geq t, N^1_{t-1}, r_t=1, g_t) \\\nonumber
&\qquad + \Prob(r_t \neq 1 \mid \tau \geq t, N^1_{t-1}, g_t)
\Prob (\A \text{ succeeds} \mid \tau \geq t, N^1_{t-1}, r_t \neq 1, g_t) ]\\ \nonumber
=& \E_{g_t} \left[  \frac{1}{1 + N^{g_t}_{t-1}}
    \Prob (\A \text{ succeeds} \mid \tau \geq t, N^1_{t-1}, r_t=1, g_t)  \right.\\
&\qquad \left.+ \left( 1 - \frac{1}{1 + N^{g_t}_{t-1}}\right) 
    \Prob (\A \text{ succeeds} \mid \tau \geq t, N^1_{t-1}, r_t \neq 1, g_t) \right]\enspace.
\end{align}

If we denote $q$ the probability that $\act_{t, 1} = \askip$ knowing $(N^1_{t-1}, r_t, g_t)$, we have that

\begin{align} \label{eq:opt0-ineq}
\Prob (\A \text{ succeeds} \mid \tau \geq t, &N^1_{t-1}, r_t, g_t)\\ \nonumber
&= q\Prob (\A \text{ succeeds} \mid \tau \geq t, N^1_{t-1}, r_t, g_t, \act_{t, 1} = \askip) \\ \nonumber
& \qquad + (1-q) \Prob (\A \text{ succeeds} \mid \tau \geq t, N^1_{t-1}, r_t, g_t, \act_{t, 1} = \astop)\\ \nonumber
&\leq \max\{
    \Prob (\A \text{ succeeds} \mid \tau \geq t,  N^1_{t-1}, r_t, g_t, \act_{t, 1} = \askip)\\ \nonumber
&\hspace{35pt} + \Prob (\A \text{ succeeds} \mid \tau \geq t, N^1_{t-1}, r_t, g_t, \act_{t, 1} = \astop) 
\}
\end{align}
The first term can be simplified using the implication $(\tau \geq t, A_{t,1} = \askip) \implies (\tau \geq t+1)$ and the memory-less property of $\A$ gives for any value of $r_t$ that
\begin{align*}
\Prob (\A \text{ succeeds} &\mid \tau \geq t, N^1_{t-1}, r_t, g_t, \act_{t, 1} = \askip)\\
&= \Prob (\A \text{ succeeds} \mid \tau \geq t+1, N^1_t = N^1_{t-1} + \indic{g_t=1})\\
&= \rwd^0_{t+1, N^1_{t-1} +\indic{g_t=1}}
\end{align*}
The second term is null for $r_t \neq 1$. For $r_t=1$, we have that $(\act_{t, 1} = \astop) \iff ( \tau = t)$, the event $\{ \A \text{ succeeds}\}$ becomes equivalent to $\{t = t^\star\}$, and knowing $r_t, g_t, N^1_{t-1}$, it is independent of the distribution $Q_{t,1}$ of $\act_{t, 1}$ because $\A$ is memory-less, it yields
\begin{align*}
\Prob (\A \text{ succeeds} \mid \tau \geq t,  N^1_{t-1}, r_t=1, g_t, \act_{t, 1} = \astop)
= \Prob (t = t^\star \mid N^1_{t-1}, r_t=1, g_t)
= \frac{1+N^{g_t}_{t-1}}{N}.
\end{align*}
Substituting into Equation \ref{eq:req-rwd0-0} gives
\begin{align*}
\rwd^0_{t,m}
&\leq \E_{g_t} \left[ \frac{1}{1+N^{g_t}_{t-1}}\max\left\{  
    \rwd^0_{t+1,m+\indic{g_t=1}}, \frac{1+N^{g_t}_{t-1}}{N} \right\}
    + \left(1 - \frac{1}{1+N^{g_t}_{t-1}} \right) \rwd^0_{t+1,m+\indic{g_t=1}} \right]\\
&=  \E_{g_t} \left[   \max\left\{  
    \rwd^0_{t+1,m+\indic{g_t=1}}, \frac{1}{N} + \left(1 - \frac{1}{1+N^{g_t}_{t-1}} \right) \rwd^0_{t+1,m+\indic{g_t=1}}\right\} \right]\\
&= \lambda \max\left\{  
    \rwd^0_{t+1,m+1}, \frac{1}{N} + \left(1 - \frac{1}{1+m} \right) \rwd^0_{t+1,m+1}\right\}\\
&\qquad + (1 - \lambda) \max\left\{  
    \rwd^0_{t+1,m}, \frac{1}{N} + \left(1 - \frac{1}{t-m} \right) \rwd^0_{t+1,m}\right\}.
\end{align*}
\end{proof}



\subsubsection{Proof of Corollary \ref{cor:optAction0}}

\begin{proof}
If an algorithm is such that the previous dynamic programming inequality is an equality then it is necessarily optimal. In order to have that, we only need Inequality \ref{eq:opt0-ineq} from the previous proof to be an equality, and this can be achieved by stopping with probability 1 if 
\[
r_s = 1 \text{ and }  N \rwd^0_{s+1, N^1_s} \leq N^{g_s}_s\enspace.
\]
\end{proof}



\subsubsection{Proof of Proposition \ref{prop:dynprogb}}

\begin{proof}
Let $1 \leq t \leq N$ and assume that $B_t = b > 1$. The tree of all possible observations $(r_t, R_t)$ and actions $\act_{t, 1}, \act_{t, 2}$ that can happen at step $t$ are presented in Figure \ref{fig:tree-b>1}. Before trying to determine the optimal action at this step, let us start by establishing that some actions are never optimal when we still have a positive budget $b$: 
\begin{itemize}
    \item if $r_t \neq 1$, then stopping is not optimal because the success probability would be $0$,
    \item same thing if $R_t \neq 1$,
    \item if $r_t = 1$, stopping is not optimal because it is equivalent to comparing then stopping with probability 1 since $b>0$, therefore comparing then choosing an optimal action given $R_t$ is better than immediately stopping,
    \item a reasonable algorithm never skips after observing $R_t = 1$. In fact, since we consider memory-less algorithms, by Proposition \ref{prop:success-memless-stop} we deduce that skipping $x_t$ after observing $R_t$ has no impact on the success probability compared to skipping after just observing $r_t$, except having $B_{t+1} = b-1$ instead of $B_{t+1} = b$. A reasonable algorithm must therefore use a comparison only if it is ready to stop if $R_t = 1$. 
\end{itemize}

If we want to determine the optimal algorithm, the only sequences of actions and observations possible at step $t$ are $(r_t=1, \acomp, R_t=1, \astop)$, $(r_t=1, \acomp, R_t \neq 1, \askip)$, $(r_t=1, \askip)$, and $(r_t \neq 1, \askip)$.

\begin{figure}[h!]
\centering
\begin{tikzpicture}
\node {$(r_t,g_t)$}[grow=right][sibling distance = 1cm][level distance=2.5cm]
    child [yshift = -0.8cm]{node {$r_t \neq 1$}
        child {node {$\askip$}}
        child {node {\color{red} $\astop$} edge from parent [red]}
    }
    child [yshift = 0.8cm] {node {$r_t=1$}
        child {node {$\askip$}}
        child {node {\color{red} $\astop$} edge from parent [red]}
        child {node {$\acomp$}
            child [yshift = -.5cm]  {node {$R_t \neq 1$} 
                child {node {$\askip$}}
                child {node {\color{red} $\astop$} edge from parent [red]}
            }
            child [yshift = .5cm]  {node {$R_t=1$}
                child {node {\color{red} $\askip$} edge from parent [red]}
                child {node {$\astop$}}
            }
        }
    };
\end{tikzpicture}
\caption{Tree of possible observations and actions when a new observation $(r_t, g_t)$ arrives and $B_t > 0$.} 
\label{fig:tree-b>1}
\end{figure}

We showed in the proof of Proposition \ref{prop:success-memless-stop} that conditionally to $(g_t, N^1_{t-1})$, the distribution of $r_t$ is independent of $\F_{t-1}$, thus we have 
\[
\Prob(r_t = 1 \mid g_t, N^1_{t-1}) = \frac{1}{N^{g_t}_t},
\]
with $N^{g_t}_t = \indic{g_t = 1}(N^1_{t-1} + 1) + (1 - \indic{g_t = 1} )(t - N^1_{t-1})$. In particular, the event $\{ \tau \geq t\}$ is $\F_{t-1}$-measurable, hence
\begin{align*}
\Prob(\A \text{ succeeds} \mid \tau \geq t, B_t, N^1_{t-1}, g_t)
=& \frac{1}{N^{g_t}_t} \Prob(\A \text{ succeeds} \mid \tau \geq t, B_t, N^1_{t-1}, g_t, r_t=1)\\
&+ \left(1 - \frac{1}{N^{g_t}_t} \right) 
\Prob(\A \text{ succeeds} \mid \tau \geq t, B_t, N^1_{t-1}, g_t, r_t \neq 1)    
\end{align*}
If $r_t \neq 1$, the only possible action is $\act_{t, 1} = \askip$, consequently
\begin{align*}
\Prob(\A \text{ succeeds}& \mid \tau \geq t, B_t, N^1_{t-1}, g_t, r_t \neq 1)\\
&= \Prob(\A \text{ succeeds} \mid \tau \geq t+1, B_t, N^1_{t-1}, g_t, r_t \neq 1, \act_{t, 1}=\askip)\\
&= \Prob(\A \text{ succeeds} \mid \tau \geq t+1, B_{t+1} = B_t, N^1_{t} = N^1_{t-1} + \indic{g_t=1})\\
&=\rwd^{B_t}_{t+1, N^1_{t-1} + \indic{g_t=1}}.
\end{align*}
If $r_t = 1$, then
\begin{align}\label{eq:rwdb-max}
\Prob(\A &\text{ succeeds} \mid \tau \geq t, B_t, N^1_{t-1}, g_t, r_t=1)\\ \nonumber
&\leq \max \{
\Prob(\A \text{ succeeds} \mid \tau \geq t, B_t, N^1_{t-1}, g_t, r_t=1, \act_{t, 1} = \askip),\\ \nonumber
&\qquad \Prob(\A \text{ succeeds} \mid \tau \geq t, B_t, N^1_{t-1}, g_t, r_t=1, \act_{t, 1} = \acomp)\}\\ \nonumber
&= \max\{ 
\rwd^{B_t}_{t+1, N^1_{t-1} + \indic{g_t=1}},
\Prob(\A \text{ succeeds} \mid \tau \geq t, B_t, N^1_{t-1}, g_t, r_t=1, \act_{t, 1} = \acomp)\}
\end{align}
With the same arguments as in the proof of Proposition \ref{prop:success-memless-stop}, we have that 
\[
\Prob(R_t=1 \mid N^1_{t-1}, g_t, r_t=1, \act_{t, 1}, B_t)
= \Prob(R_t=1 \mid N^1_{t-1}, g_t, r_t=1)
= \frac{\Prob(R_t=1 \mid N^1_{t-1}, g_t)}{\Prob(r_t=1 \mid N^1_{t-1}, g_t)}
= \frac{N^{g_t}_t}{t},
\]
therefore
\begin{align*}
\Prob(\A \text{ succeeds} &\mid \tau \geq t, B_t, N^1_{t-1}, g_t, r_t=1, \act_{t, 1} = \acomp)\\
=& \frac{N^{g_t}_t}{t}
  \Prob(\A \text{ succeeds} \mid \tau \geq t, B_t, N^1_{t-1}, g_t, r_t=1, \act_{t, 1} = \acomp, R_t=1)\\
&+ \left(1 - \frac{N^{g_t}_t}{t} \right) \Prob(\A \text{ succeeds} \mid \tau \geq t, B_t, N^1_{t-1}, g_t, r_t=1, \act_{t, 1} = \acomp, R_t \neq 1)\\
=& \frac{N^{g_t}_t}{t}
  \Prob(\A \text{ succeeds} \mid R_t=1, \act_{t, 2} = \astop)\\
&+ \left(1 - \frac{N^{g_t}_t}{t} \right) \Prob(\A \text{ succeeds} \mid \tau \geq t+1, B_{t+1}=B_t - 1, N^1_t = N^1_{t-1} + \indic{g_t = 1})\\
=& \frac{N^{g_t}_t}{N} + \left(1 - \frac{N^{g_t}_t}{t} \right)
    \rwd^{B_t-1}_{t+1, N^1_{t-1} + \indic{g_t=1}}\\
\end{align*}
where we used in the second equality that the only possible action for the optimal algorithm after observing $R_t \neq 1$ is $\act_{t, 2} = \askip$ and the only action after $R_t = 1$ is $\act_{t, 2} = \astop$, and for the last equality we used Propositions \ref{prop:success-memless-stop} and \ref{prop:success-memless-skip}.\\
Substituting into Inequality \ref{eq:rwdb-max} yields
\begin{align*}
\Prob(\A \text{ succeeds}& \mid \tau \geq t, B_t, N^1_{t-1}, g_t, r_t=1)\\
&\leq \max\left\{
\rwd^{B_t-1}_{t+1, N^1_{t-1} + \indic{g_t=1}},
\frac{N^{g_t}_t}{N} + \left(1 - \frac{N^{g_t}_t}{t} \right)
\rwd^{B_t}_{t+1, N^1_{t-1} + \indic{g_t=1}}
\right\},
\end{align*}
and finally
\begin{align*}
\Prob(\A &\text{ succeeds} \mid \tau \geq t, B_t, N^1_{t-1}, g_t)\\
&\leq \frac{1}{N^{g_t}_t} \max\left\{
\rwd^{B_t}_{t+1, N^1_{t-1} + \indic{g_t=1}},
\frac{N^{g_t}_t}{N} + \left(1 - \frac{N^{g_t}_t}{t} \right)
\rwd^{B_t-1}_{t+1, N^1_{t-1} + \indic{g_t=1}}
\right\}
+ \left(1 - \frac{1}{N^{g_t}_t} \right) 
\rwd^{B_t}_{t+1, N^1_{t-1} + \indic{g_t=1}}\\
&=\max\left\{
\rwd^{B_t}_{t+1, N^1_{t-1} + \indic{g_t=1}},
\frac{1}{N} + \left(\frac{1}{N^{g_t}_t} - \frac{1}{t} \right)
\rwd^{B_t-1}_{t+1, N^1_{t-1} + \indic{g_t=1}}
+ \left(1 - \frac{1}{N^{g_t}_t} \right) 
\rwd^{B_t}_{t+1, N^1_{t-1} + \indic{g_t=1}}
\right\},
\end{align*}
If $B_t = b \geq 1$ and $N_{t-1}^1 = m$, the previous inequality translates, in expectation over $g_t$, as
\begin{align*}
\rwd^b_{t,m}
\leq& \lambda \max\left\{
\rwd^{b}_{t+1, m+1},
\frac{1}{N} + \left(\frac{1}{m+1} - \frac{1}{t} \right)
\rwd^{b-1}_{t+1, m+1}
+ \left(1 - \frac{1}{m+1} \right) 
\rwd^{b}_{t+1, m+1}
\right\}\\
&+ (1 - \lambda) \max\left\{
\rwd^{b}_{t+1, m},
\frac{1}{N} + \left(\frac{1}{t-m} - \frac{1}{t} \right)
\rwd^{b-1}_{t+1, m}
+ \left(1 - \frac{1}{t-m} \right) 
\rwd^{b}_{t+1, m}
\right\}.
\end{align*}

\end{proof}







\subsubsection{Proof of Corollary \ref{cor:optActionB}}
\begin{proof}
Similarly to the proof of Corollary \ref{cor:optAction0}, an algorithm is optimal if the inequality in Proposition \ref{prop:dynprogb} is an equality, and this true only if Inequality \ref{eq:rwdb-max} in the proof of Proposition \ref{prop:dynprogb} is an equality, which can be achieved by using a comparison with probability 1 if
\[
r_s = 1 \text{ and }
\rwd^{b}_{s+1, N^1_s}
- \left(1 - \frac{N^{g_s}_s}{s} \right) \rwd^{b-1}_{s+1, N^1_s}
<
\frac{ N^{g_s}_s}{N}\enspace.
\]
\end{proof}


