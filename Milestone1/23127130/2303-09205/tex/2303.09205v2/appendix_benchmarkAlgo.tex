\section{Non-asymptotic analysis of the unique-threshold algorithm}\label{appx:bench-algo}

While we only proved previously asymptotic guarantees for DDT algorithms, this section focuses on how fast the success probability converges to the asymptotic regime. For that, we consider a particular case of DDT algorithm where all the thresholds equal the same value $\w \in (0,1)$. We show, in this case, that the asymptotic success probability is independent of $\lambda$, but that the convergence rate strongly depends on it.
The approach considered here is presented in Algorithm~\ref{algo:bench-algo}. The proof techniques here differ significantly from the ones used for the asymptotic analysis of DDT algorithms. We explain this with more details in the next section.


\begin{algorithm}
\DontPrintSemicolon 
\caption{Benchmark algorithm with $B$ comparisons}\label{algo:bench-algo}
\SetKwInput{Input}{Input}
   \SetKwInOut{Output}{Output}
   \SetKwInput{Initialization}{Initialization}
   \Input{number of explorations $\w N$}
   \Initialization{budget $B$}
\For{$t=1,\ldots,N$}{
    Receive new observation: $(r_t, g_t) \in  \{1, \ldots, t\} \times \{1, 2\}$\;
    \If(\tcp*[f]{\small compare in-group}){$t \ge \w N \And r_t=1$}{%\label{algoline:better-than-best}
        
        \If(\tcp*[f]{\small check budget}){$B > 0$}{
            Update budget: $B \gets B - 1$\;


            \If(\tcp*[f]{\small compare inter-group}){$R_t=1$}{\label{algoline:compare-inter-group}
            Return: $t$
            }
        }
        \lElse
        {
        Return: $t$} 
        % \EndIf
    }
}

\end{algorithm}

\subsection{Main result}
We estimate in this section the success probability of Algorithm \ref{algo:bench-algo}. The parameters of the algorithm are the total number of candidates $N$, the fraction of skipped candidates in the exploration phase $\w$, and the budget of comparisons $B$. 
The main result of this section is concerned with the behavior of
\begin{align*}
    P_{N}^B(\alpha) \eqdef \Prob(\A(\w) \text{ succeeds})\enspace,
\end{align*}
where $\A(\w)$ is the Algorithm \ref{algo:bench-algo} with budget $B$ and exploration phase of size $\w N$. As in the main body of the paper, we assume here also that $\lambda \geq 1/2$. Our main result is the following theorem.

\begin{theorem}\label{thm:lb-PB(s,N)}
Let $\w \in (0,1)$ and $N \in \mathbb{N}$ be such that $\frac{N}{\log N} \geq \frac{(1 + 1/\sqrt{\w})^2}{2\sqrt{\w}(1 - \lambda)^2}$.
% The success probability $P_B(\w N,N)$ of Algorithm \ref{algo:bench-algo} with budget $B$ satisfies
Then,
\begin{align*}
    P_{N}^B(\alpha) \geq P_{\infty}^B(\alpha) - K_{\lambda,\w,B} \sqrt{\frac{\log N}{N}}\enspace,
\end{align*}
with
\begin{align*}
P_{\infty}^B(\alpha) 
&= \w^2 \sum_{b=0}^B \left( \frac{1}{\w} - \sum_{\ell=0}^{b} \frac{\log(1/ \w)^\ell}{\ell !}   \right) \\
\text{ and }\, \quad
K_{\lambda,\w,B} 
&= \frac{\sqrt{2}(B+1)\log(1/ \w)}{\lambda(1-\lambda)} 
  {+} \frac{\e}{\sqrt{\w}} \sum_{b=0}^{B-1} \log(1/ \w)^b\enspace.
\end{align*}
\end{theorem}
Let us comment on the above derived lower bound on the success probability of Algorithm \ref{algo:bench-algo}. The leading term depends neither on $N$ nor on $\lambda$ and, it corresponds exactly to the asymptotic success probability of Algorithm \ref{algo:bench-algo} and it only depends on $\w$ and $B$---the maximization of the leading term in $\w$ does not require the knowledge of $\lambda$. The second term decreases in $N$ and essentially corresponds to the speed of concentration of $|G^1_t|$ around its mean. However, the multiplicative constant depends on $\lambda$ and becomes arbitrarily large when $\lambda$ approaches $1$. We recall that both $\lambda$ and $B$ are assumed to be fixed constants that do not depend on $N$. 
The full proof of this Theorem is given in Section \ref{sec:proof-benchmark} below. However, we give here a sketch of the main steps.
\begin{proof}[Sketch of the proof]

We observe first that the times $\{ \rho_b \}_{b=1}^{B+1}$ when Algorithm \ref{algo:bench-algo} uses comparisons (as defined in Section \ref{sec:setup}) are such that for any $1 \leq b \leq B+1$
\begin{align*}
\rho_b = \min\left\{ t > \rho_{b-1} \mid  r_t = 1\right\}\enspace.
\end{align*}
with $\rho_0 \eqdef \w N$. These times are central in the analysis of Algorithm~\ref{algo:bench-algo} as they give a convenient decomposition of the success probability of the algorithm. In particular, we show that if $\tau$ is stopping time of Algorithm~\ref{algo:bench-algo}, then
\begin{align}
    \label{eq:benchmark_prob_decomposition}
        P_{N}^B(\w) = \sum_{b=1}^{B+1} p_{N}^b(\w)\enspace,
\end{align}
where 
$p_{N}^b(\w) \eqdef \Prob (\tau = t^\star \text{ and } \tau = \rho_b)$ for any $1 \leq b \leq B+1$.

We show that $p_{N}^1(\w) = \w - \w^2 - O(\sqrt{\log N / N})$ and that for $2 \leq b \leq B+1$
\[
p_{N}^b(\w) \geq \left(\w^2 N \sum_{i=r}^N \frac{1}{i^2} \sum_{s \leq t_1 < \ldots < t_{b-1} < i} \prod_{\ell=1}^{b-1} \frac{1}{t_\ell}\right)
- O\left( \sqrt{\frac{\log N}{N}} \right)
\]
with explicit $O$ constants.
Finally, a good estimation of the sum above gives the claimed result.
\end{proof}


\begin{remark}
While the previous theorem only establishes a lower bound of the form $P_{N}^B(\w) \geq P_{\infty}^B( \w) - O(\sqrt{\log N /N})$, one analogously obtains $P_{N}^B(\w) \leq P_{\infty}^B(\w) + O(\sqrt{\log N /N})$, implying that 
\[
\lim_{N \to \infty} P_{N}^B(\w) = P_{\infty}^B( \w)\enspace.
\]
Moreover, using Theorem \ref{thm:asymptoticDDT} giving the asymptotic success probability of DDT algorithms, we can verify by induction that setting all the thresholds equal to the same value $\w$ gives $P_{\infty}(\w)$.
\end{remark}

Although the success probability of Algorithm \ref{algo:bench-algo} with a good threshold choice converges to $1/\e$ as $B$ increases, a weakness of the algorithm is that its asymptotic success probability does not take into account the distribution of the groups $\lambda$. Indeed, when $\lambda$ approaches $1$, one should be able to reach nearly the $1/\e$ asymptotic success probability, since a naive algorithm that ignores the elements of the group $G^2$ and uses the classical reject-accept strategy only on $G^1$ results in a success probability of $\lambda/\e$. Yet, even in the case $\lambda$ goes to $1$, the asymptotic success probability of Algorithm \ref{algo:bench-algo} stays away from $1/\e$. 






\subsection{Proof of Theorem \ref{thm:lb-PB(s,N)}}\label{sec:proof-benchmark}


\begin{lemma}\label{lem:tau=rho}
With $B$ comparisons allowed, the stopping time $\tau$ of Algorithm \ref{algo:bench-algo} is such that 
\begin{enumerate}
    \item $\tau \in \{\rho_1, \ldots, \rho_{B+1} \}\cup\{\infty\}$;
    \item if $\tau = \rho_b$ for some $b \in \{2,\ldots, B+1\}$, then $g_{\rho_1}=g_{\rho_2}=\ldots=g_{\rho_{b-1}}$;
    \item denoting by $t^\star$ the index of the best candidate, the success probability of Algorithm \ref{algo:bench-algo} can be expressed as
\begin{align}
    \label{eq:benchmark_prob_decomposition_app}
        P^B_N(\w) = \sum_{b=1}^{B+1} p^b_N(\w)\enspace,
\end{align}
where 
\begin{align*}
   p^b_N(\w) \eqdef \Prob ( \tau = t^\star \text{ and } \tau = \rho_b)
   \quad 1 \leq b \leq B+1\enspace.
\end{align*}
%    \item if $x_{\max}$ is the best candidate, then the success probability of Algorithm \ref{algo:bench-algo} can be expressed as
%\begin{align}
%    \label{eq:benchmark_prob_decomposition}
%        P_B(s,N) = \sum_{b=1}^{B+1} p_b(s,N)\enspace,
%\end{align}
%where 
%\begin{align*}
%   p_b(s,N) \eqdef \Prob [ x_\tau = x_{\max} \text{ and } \tau = \rho_b]
%   \quad 1 \leq b \leq B+1\enspace.
%\end{align*}
\end{enumerate}
\end{lemma}
%\nic{The inequality $x_\tau > \best G^{g_\tau}_{\tau - 1}$ is strict in the proof while it is not in the algorithm. Which is better?}
\begin{proof}
% \paragraph{11}
    Let us prove the three claimed statements.\\
    \textbf{First claim.} 
    If the algorithm does not return any candidate, $\tau = \infty$ by convention. Otherwise, it must stop at some time $t \geq s$ such that $r_t = 1$, \textit{i.e.}, $\tau \in \{ \rho_b \mid b \geq 1\}$. Moreover, in that case and given the available budget $B$, the stopping time $\tau$ is upper bounded by $\rho_{B+1}$.\\
    %If the algorithm does not return any candidate, $\tau = \infty$ by convention. Otherwise, it must stop at some time $t \geq s$ such that $x_t \geq \best G^{g_t}_{t - 1}$, \textit{i.e.}, $\tau \in \{ \rho_b \mid b \geq 1\}$. Moreover, in that case and given the available budget $B$, the stopping time $\tau$ is upper bounded by $\rho_{B+1}$.\\
    %It is clear from the algorithm that we can only stop on $\tau$ satisfying $x_\tau > \best G^{g_\tau}_{\tau - 1}$ which means that $\tau \in \{ \rho_b \mid b \geq 1\}$ by Definition \ref{def:bench-algo-rho}, and since we have a budget of $B$ comparisons, this condition can be only satisfied $B+1$ times during a run of the algorithm. In the case where the algorithm does not return any candidate, we have by convention that $\tau = \infty$.\\
    %\textbf{Second claim.} Let $b \geq 2$. Assume that $\tau = \rho_b$ and, w.l.o.g., that $g_{\rho_1}  = 1$. Since $\tau \neq \rho_1$, then $\best G^1_{r-1} < \best G^2_{r-1}$. Indeed, otherwise the algorithm would have terminated at $\rho_1$. \nic{I am not convinced by this: $\tau \neq \rho_1$ implies $x_{\rho_1} \geq G_{\rho_1}^1$ and $x_{\rho_1} < G_{\rho_1}^2$ but it does not tell us how $\best G^1_{r-1}$ and $\best G^2_{r-1}$ compare.}
    %Assume that $\rho_2, \ldots, \rho_{b-1}$ are not all in  $G^1$ and let $\ell$ be the smallest index in $\{2, \ldots, b-1\}$ such that $g_{\rho_\ell} = 2$. On the one hand, since algorithm did not terminate at $\rho_\ell$, then no candidate in $\{x_r, \ldots, x_{\rho_\ell-1}\}$ is better than $\best G^2_{r-1}$. On the other hand, by definition of $\rho_\ell$, we have $x_{\rho_\ell} > \best G^2_{\rho_\ell-1} = \best G^2_{r-1} > \best G^1_{\rho_\ell-1}$. Thus, the algorithm must terminate on $\rho_\ell$ contradicting our assumption that $\rho_\ell < \rho_b = \tau$.\\
    \textbf{Second claim.}  Assume that $\tau = \rho_b$ for some $b \geq 2$. Moreover, let us assume without loss of generality that $g_{\rho_1}  = 1$. 
    %Since $\tau \neq \rho_1$, then $\best G^1_{\rho_1} < \best G^2_{\rho_1}$. Indeed, otherwise the algorithm would have terminated at $\rho_1$. 
    By contradiction, assume that the potential stopping times $\rho_1, \ldots, \rho_{b-1}$ are not all in the first group.
    Let $\ell \coloneqq \min\{ i \leq b-1 : g_{\rho_\ell} = 2 \}$.
    Let us show that no candidate in $\{x_r, \ldots, x_{\rho_\ell-1}\}$ is better than $\best G^2_{r-1}$.
    By definition of the index $\ell$, $ \best G^2_{\rho_\ell -1} = \best G^2_{\rho_0}$.
    Moreover, since the algorithm did not stop at $\rho_1,\ldots,\rho_{\ell-1}$, no candidate from group 1 in $\{x_r, \ldots, x_{\rho_\ell-1}\}$ is better than $\best G^2_{r-1}$.
    By definition of $\rho_\ell$,  
    \[
    x_{\rho_\ell} \geq \best G^2_{\rho_\ell-1} = \best G^2_{\rho_0} \geq \best G^1_{\rho_0}.
    \]
    Thus, the algorithm must terminate on $\rho_\ell$ contradicting our assumption that $\rho_\ell < \rho_b = \tau$. \\
    \textbf{Third claim.} The claim follows immediately from the first claim.
    % \[
    %     P_B(s,N) = \Prob [ x_\tau = x_{\max}]
    %              = \sum_{b=1}^{B+1} p_b(s,N)\enspace.
    % \]
\end{proof}


Lemma \ref{lem:tau=rho} shows that estimating $P^B_N(\w)$ comes down to estimating the probabilities $p^b_N(\w), b \in \{1, \ldots, B+1\}$. In order to do that
we will need a concentration inequality for controlling the cardinals of the sets $G^1_t$ and $G^2_t$ for $t \in \{s, \ldots, N\}$. We use for that we use Lemma \ref{lem:conc-card}, which was also used in the asymptotic analysis of DDT algorithms.


\begin{remark}
For any $t \geq 1$ we have
\begin{align*}
\left| |G^1_t| - \lambda t \right|
&= \left| t - |G^2_t| - \lambda t \right|\\
&= \left| |G^2_t| - (1 - \lambda) t \right|\enspace.
\end{align*}
Consequently, the above Lemma provides a concentration inequality for the cardinality of both $G^1_t$ and $G^2_t$ for any time $t$.
\end{remark}

The next sequence of results provide the control of $p_b(r, N)$ appearing in the decomposition of Eq.~\eqref{eq:benchmark_prob_decomposition_app}. We treat separately the case $b=1$ from the other cases.
This first Lemma is purely technical and provides some bounds that we use in the proofs. We remind that we assume $\lambda \geq 1/2$.
\begin{lemma}\label{lem:N-suff-large}
Let $N,s$ be two positive integers such that $s = \w N$ fro some $\w \in (0,1)$. If and if $\frac{N}{\log N} \geq \frac{(1 + 1/\sqrt{\w})^2}{2\sqrt{\w}(1 - \lambda)^2}$ then
\[
    1 \leq A_\w \sqrt{s \log s} \leq \frac{(1-\lambda)s}{2}
\]
\end{lemma}
\begin{proof}
The proof is immediate
\[
\frac{s}{\log s}
\geq \frac{\w N}{\log N}
\geq \frac{(1 +\sqrt{\w})^2}{2\sqrt{\w}(1 - \lambda)^2}
= \frac{4 A_\w^2}{(1-\lambda)^2}
\]
therefore $\sqrt{s/\log s} \geq 2 A_\w / (1-\lambda)$, multiplying by $(1-\lambda)\sqrt{s \log s}$ gives
\[
2 A_\w \sqrt{s \log s} \leq (1-\lambda)s.
\]
For the other inequality, using $1-\lambda \leq 1/2$ and observing that $A_\w \geq 1/\sqrt{2}$ we obtain
\[
s \geq \frac{s}{\log s} \geq \frac{4A_\w^2}{(1-\lambda)^2} 
\geq 8,
\]
thus $A_\w \sqrt{s \log s} \geq A_\w \sqrt{s} \geq \frac{1}{\sqrt{2}} 2 \sqrt{2}$.
\end{proof}


\begin{lemma}\label{lem:p1(s,N)}
% Assume that $s = \w N$.
Set $s = \w N$, if $\frac{N}{\log N} \geq \frac{(1 + 1/\sqrt{\w})^2}{2\sqrt{\w}(1 - \lambda)^2}$, then
\begin{align*}
p_N^1(\w)
\geq (\w - \w^2) \left( 1 - \frac{4A_\w}{\lambda(1 - \lambda)}\sqrt{\frac{\log s}{s}} \right)\enspace.
\end{align*}
\end{lemma}

\begin{proof} Let $N\geq2$ be an integer and let $s = \w N$ such that $s \in \{1, \ldots, N\}$. By definition of the probability $p^1_N(\w)$, the law of total probability gives
\begin{align*}
    p^1_N(\w)  
    %=\Prob [ \tau = t^\star \text{ and } \tau = \rho_1]
    = \frac{1}{N} \sum_{i=s}^N \Prob( \tau = \rho_1 = i \mid t^\star = i)\enspace.
 \end{align*}
Let $i \in \{s, \ldots, N\}$. Conditionally on the event that $t^\star=i$, one can check that the equalities $\tau = \rho_1 = i$ hold if and only if none of the candidates between times $s$ and $i{-}1$ are ranked first among the observed candidates inside their groups. Moreover, the latter event depends solely on the relative ranks of the first $i{-}1$ candidates, it is independent of the event that the $i$-th candidate is the best overall. Consequently, we can write
\begin{align*}
 \Prob( \tau = \rho_1 = i \mid t^\star = i) = \Prob\left(\cap_{j=s,\ldots,i-1} \{r_j \neq 1\} \right)\enspace.
\end{align*}
Since the ranks are mutually independent and uniformly distributed conditionally on $(G_{i-1}^1, G_{i-1}^2)$ (see Lemma~\ref{lem:rank_indep_unif}), we obtain
%Enumerating the cases constituting the event $\cap_{j=s,\ldots,i-1} \{r_j \neq 1\}$, we obtain 
\begin{align*}
    \Prob\left(\cap_{j=s,\ldots,i-1} \{r_j \neq 1\} \right)
    = \E \left[ \prod_{j = s}^{i-1} \left(1 - \frac{1}{|G^{g_j}_j|}\right) \right]
    =  \E \left[\frac{|G^1_{s-1}|}{|G^1_{i-1}|} \times \frac{|G^2_{s-1}|}{|G^2_{i-1}|} \right]\enspace.
\end{align*}
%with the convention that $-1/0=1$.

% %We recall that Lemma~\ref{lem:conc-card} ensures that the event
% \begin{align*}
%     \mathcal{C}_{s,N} = \left\{\forall t \in \{s, \ldots, N\} \qquad
%     \left| |G^1_t| - \lambda t \right| < A_\w \sqrt{t \log s}
%     \right\}
% \end{align*}
% holds with probability at least $1 - 2/s$.  Applying the Bayes rule, we then get
% \begin{align*}
%     \Pr\left(\min_{j=s,\ldots,i-1} r_j \neq 1 \right) \geq \Pr\left(\min_{j=s,\ldots,i-1} r_j \neq 1 \mid \mathcal{C}_{s,N}\right)\left(1-\frac{2}{s}\right)\enspace.
% \end{align*}
% Finally, enumerating the cases in which $\min_{j=s,\ldots,i-1} r_j \neq 1$ holds, we obtain
% \begin{align*}
%     \Pr\left(\min_{j=s,\ldots,i-1} r_j \neq 1 \mid \mathcal{C}_{s,N}\right)
%     = \E \left( \prod_{j = s}^{i-1} \left(1 - \frac{1}{|G^{g_j}_j|}\right) \,\big|\, \mathcal{C}_{s,N}\right)
%     =  \E \left( \left.  \frac{|G^1_{s-1}|}{|G^1_{i-1}|} \times \frac{|G^2_{s-1}|}{|G^2_{i-1}|} \right| \mathcal{C}_{s,N} \right)\enspace.
% \end{align*}
% \begin{align*}
%     p_1(s,N)  &=  \Prob [ x_\tau = x_{\max} \text{ and } \tau = \rho_1]\\
%     &= \frac{1}{N} \sum_{i=r}^N \Prob[ \rho_1 = \tau = i \mid x_i = x_{\max}]\\
%     &= \frac{1}{N} \sum_{i=r}^N \Prob[ \best G^1_{i-1} \in G^1_{r-1} \text{ and } \best G^2_{i-1} \in G^2_{r-1}]\\
%     &\geq \frac{1}{N} \sum_{i=r}^N \Prob[ \best G^1_{i-1} \in G^1_{r-1} \text{ and } \best G^2_{i-1} \in G^2_{r-1} \mid \mathcal{C}_{s,N}](1-2/s)\\
%     &= \frac{1}{N} \sum_{i=r}^N \E \left( \left.  \frac{|G^1_{r-1}|}{|G^1_{i-1}|} \times \frac{|G^2_{r-1}|}{|G^2_{i-1}|} \right| \mathcal{C}_{s,N} \right) \left(1 - \frac{2}{s} \right) \\
% \end{align*}
%\nic[inline]{Are you sure that $\Prob[ \rho_1 = \tau = i \mid x_i = x_{\max}] = \Prob[ \best G^1_{i-1} \in G^1_{r-1} \text{ and } \best G^2_{i-1} \in G^2_{r-1}]$? We have $\Prob[ \rho_1 = \tau = i \mid x_i = x_{\max}] = \Prob[ \best G^1_{i-1} \in G^1_{r-1} \text{ and } \best G^2_{i-1} \in G^2_{r-1} \mid x_i = x_{\max}]  \geq \Prob[ \best G^1_{i-1} \in G^1_{r-1} \text{ and } \best G^2_{i-1} \in G^2_{r-1}]$ but I am not 100\% confident about the equality.}
%\ziy[inline]{It's an equality because the event inside the proba depends only on the relative ranks of the elements of $G^1_{i-1}, G^2_{i-1}$, it is thus independent of their rank compared to $x_i$. This argument is very similar to the one used in the classical proof of the success probability in the standard secretary problem.}
%\nic[inline]{Thanks for the clarification, I agree! Let's add a sentence about this in the proof.}
%\nic[inline]{For full rigor, should we state somewhere that we use the convention $\max \varnothing \in \varnothing$ is true? It might happen if we only observe candidates from one group.}
%\ziy[inline]{yes i see. If we condition on $\mathcal{C}_{s,N}$ before it will be good ?}
%\nic[inline]{Yes indeed}
% \evg[inline]{Why don't you condition on $\mathcal{C}_{r-1,N}$ instead of $\mathcal{C}_{s,N}$? maybe you can win some constants, with the former, no?}
%
Let us now provide a lower bound on the expression inside the expectation, conditionally on the event $\mathcal{C}_{s,N}$. 
Note that, on this event, the denominators of this expression are guaranteed to be non-zero for thanks under the hypothesis of the Lemma. Indeed, $|G^1_{i-1}| \geq |G^1_{s-1}| \geq |G^1_{s}|-1$ by monotonicity of $(|G^1_{i}|)_i$ and, on $\mathcal{C}_{s,N}$, $|G^1_{s}| \geq \lambda \w N - A_\w \sqrt{s\log s}$. Therefore, Lemma \ref{lem:N-suff-large} gives that $|G^1_{i-1}| > 0$ holds almost surely conditionally on $\mathcal{C}_{s,N}$. A similar argument shows that the same holds for $|G^2_{i-1}|$.

Assume the event $\mathcal{C}_{s,N}$. Since the sequence $(|G^1_j|)_j$ is non-decreasing,
%\begin{align*}
%\frac{|G^1_{s}|-1}{|G^1_{i-1}|} \geq \frac{|G^1_s|}{|G^1_i|}\enspace.
%\end{align*}
we have
\begin{align*}
    \frac{|G^1_{s-1}|}{|G^1_{i-1}|} \geq \frac{|G^1_s| - 1}{|G^1_i|} \geq \frac{\lambda s - 2 A_\w\sqrt{s\log s}}{\lambda i + A_\w \sqrt{i \log s}} \geq \frac{s}{i}
      \left( 1 - \frac{2 A_\w}{\lambda}\sqrt{\frac{\log s}{s}} \right)
      \left( 1 - \frac{A_\w}{\lambda}\sqrt{\frac{\log s}{i}} \right)\enspace,
\end{align*}
using the fact that $1 \leq A_\w \sqrt{s \log s}$ for the second inequality and $(1+x)(1-x) \leq 1$ for $x \geq 0$ for the third inequality. Finally, using the inequality $(1-a)(1-b) \geq 1-a-b$ for $a, b \geq 0$ gives
\begin{align*}
\frac{|G^1_{s-1}|}{|G^1_{i-1}|} \geq \frac{s}{i} \left( 1 - \frac{3A_\w}{\lambda}\sqrt{\frac{\log s}{s}} \right)\enspace.
\end{align*}
%\nic{Is $|G^1_{i-1}|$ always non-zero on $\mathcal{C}_{s,N}$?}
%\ziy{\we have $|G^1_{i-1}| \geq |G^1_{r-1}|$. If $N$ is sufficiently large then $\mathcal{C}_{s,N}$ guarantees having $|G^1_{r-1}| > 0$ }
%\nic{Thanks I agree! Let's add a sentence about this at the beginning of the proof.}
%\nic{Actually, we might need to condition on $\mathcal{C}_{r-1, N}$ as suggested by Evgenii right? Because $\mathcal{C}_{s, N}$ does not give a direct control on $|G^1_{r-1}|$.}
%the first inequality is immediate because $t \mapsto |G^1_t|$ is non-decreasing and $|G^1_t| - |G^1_{t-1}| \in \{0,1\}$, the second one follows from $1 \leq A_\w \sqrt{r \log s}$ and the fact that $\mathcal{C}_{s,N}$ holds (we can use it since $i, r \in \{s,\ldots,N\}$). Then, we use the fact that $(1+x)(1-x) \leq 1$ for $x \geq 0$. Finally, Bernoulli's inequality gives
% \[
% \frac{|G^1_{r-1}|}{|G^1_{i-1}|}
% \geq \frac{s}{i}
%       \left( 1 - \frac{2A_\w}{\lambda}\sqrt{\frac{\log s}{s}} - \frac{A_\w}{\lambda}\sqrt{\frac{\log s}{i}} \right)\\
% \geq \frac{s}{i} \left( 1 - \frac{3A_\w}{\lambda}\sqrt{\frac{\log s}{s}} \right).
% \]
The same argument gives a similar lower bound on $\frac{|G^2_{s-1}|}{|G^2_{i-1}|}$ in which $\lambda$ is replaced by $1-\lambda$. 

Overall, we obtain the lower bound
\begin{align*}
p^1_N(\w) 
&\geq \frac{1}{N} \sum_{i=r}^N 
      \frac{s^2}{i^2} \left( 1 - \frac{3A_\w}{\lambda}\sqrt{\frac{\log s}{s}} \right) 
                      \left( 1 - \frac{3A_\w}{1-\lambda}\sqrt{\frac{\log s}{s}} \right)
                      \left( 1 -\frac{2}{s} \right)\enspace,
\end{align*}
that can be loosened to
\begin{align}
\label{eq:LB_p1}
p^1_N(\w) 
&\geq \left( 1 - \frac{4A_\w}{\lambda(1 - \lambda)}\sqrt{\frac{\log s}{s}} \right)
      \frac{s^2}{N} \sum_{i=s}^N \frac{1}{i^2}\enspace,
\end{align}
 using $\frac{2}{s} \leq 4 A_\w \sqrt{\frac{\log s}{s}} \leq \frac{A_\w}{\lambda(1 - \lambda)}\sqrt{\frac{\log s}{s}}$ and the inequality $(1-a)(1-b)\geq 1-a-b$ for $a,b\geq0$. Observing that
\begin{align*}
\frac{s^2}{N} \sum_{i=s}^N \frac{1}{i^2} 
\geq \frac{s^2}{N} \sum_{i=s}^{N-1} \frac{1}{i^2}
\geq \frac{s^2}{N} \int_{s}^N \frac{dx}{x^2} 
= \frac{s^2}{N} \left( \frac{1}{s} - \frac{1}{N} \right)
= \w - \w^2\enspace,
\end{align*}
we conclude by substituting the above inequality into \eqref{eq:LB_p1}.
% we obtain the claimed lower bound
% \begin{align*}
% p_1(s,N)
% \geq \left( 1 - \frac{4A_\w}{\lambda(1 - \lambda)}\sqrt{\frac{\log s}{s}} \right)(w - \w^2)\enspace.
% \end{align*}
\end{proof}

\begin{lemma}\label{lem:pb-sum}
Set $s = \w N$, if $\frac{N}{\log N} \geq \frac{(1 + 1/\sqrt{\w})^2}{2\sqrt{\w}(1 - \lambda)^2}$, then for any $b\geq 2$,
\[
p^b_N(\w) 
\geq
\left(
\frac{s^2}{N} \sum_{i=s}^N \frac{1}{i^2} \sum_{s \leq t_1 < \ldots < t_{b-1} < i} \prod_{\ell=1}^{b-1} \frac{1}{t_\ell}\right)
\left( 1 - \frac{2(b + 1)A_\w}{\lambda(1 - \lambda)} \sqrt{\frac{\log s}{s}} \right)\enspace,
\]
with $A_\w$ defined in Lemma \ref{lem:conc-card}.
\end{lemma}

\begin{proof}

Let $b \geq 2$. Using the law of total probability and the Bayes rule, we have
\begin{align*}
    p^b_N(\w) 
    %&= \Prob [ \tau = t^\star \text{ and } \tau = \rho_b]\\
    &= \frac{1}{N} \sum_{i=s}^N \Prob( \rho_b = \tau = i \mid t^\star = i)\\
    &= \frac{1}{N} \sum_{i=s}^N \sum_{r \leq t_1 < \ldots < t_{b-1} < i} \Prob( \rho_b = \tau = i, \rho_1 = t_1, \ldots, \rho_{b-1} = t_{b-1} \mid t^\star = i)\enspace.
\end{align*}
Fix  $s \leq i \leq N$ and $s{-}1 < t_1 < t_2 < \ldots < t_{b-1}<i$. Define the set $T \coloneqq \{t_1, t_2, \ldots, t_{b-1}\}$ and the events
\begin{align*}
\mathcal{W}_{T}^{(s,i)} &\coloneqq \bigcap_{j \in \{s, \ldots, i-1\}\setminus T} \{ r_j \neq 1\} \text{ and } \mathcal{B}_T \coloneqq \bigcap_{j \in T} \{r_j = 1, R_j \neq 1\}\enspace.
\end{align*}
Introducing $\mathcal{I}_{s, i, T} \coloneqq \mathcal{W}_{T}^{(s,i)} \cap \mathcal{B}_T \cap \{r_i =1, R_i=1\}$, one can check that
\begin{align*}
\Prob( \rho_b = \tau = i, \rho_1 = t_1, \ldots, \rho_{b-1} = t_{b-1} \mid t^\star = i) = \Prob(\mathcal{I}_{s, i, T} \mid t^\star=i)\enspace.
\end{align*}
Let us denote $g^\star_{< s}$ the group to which belongs the best candidate among the $s$ first ones, then  using the law of total probability, we have 
\[
\Pr(\mathcal{I}_{s, i, T} \mid t^\star=i)
= (1-\lambda) \Pr(\mathcal{I}_{s, i, T} \mid t^\star=i, g^\star_{< s} = 2)
+ \lambda \Pr(\mathcal{I}_{s, i, T} \mid t^\star=i, g^\star_{< s} = 1)\enspace,
\]
and we proved in Lemma~\ref{lem:tau=rho} that if $t_b = \tau$ then $g_{t_1} = g_{t_2} =  \ldots = g_{t_{b-1}}$, and if their value is equal to $g^\star_{< s}$, then the algorithm would have stopped at $t_1$ and the the probability of $\mathcal{I}_{s, i, T}$ would be null. Consequently,
\begin{align} \nonumber
\Prob(\mathcal{I}_{s, i, T} \mid t^\star=i)
=& (1-\lambda) \Pr((g_{t}=1)_{t \in T} \mid t^\star=i, g^\star_{< s} = 2 ) \Prob(\mathcal{I}_{s, i, T} \mid t^\star=i, g^\star_{< s} = 2, (g_{t}=1)_{t \in T}) 
\\ \nonumber
&+ \lambda \Prob((g_{t}=2)_{t \in T} \mid t^\star=i, g^\star_{< s} = 1)
\Prob(\mathcal{I}_{s, i, T} \mid t^\star=i, g^\star_{< s} = 1, (g_{t}=2)_{t \in T})
\\ \nonumber
=& (1 - \lambda)\lambda^{b-1} \Prob(\mathcal{I}_{s, i, T} \mid t^\star=i, g^\star_{< s} = 2, (g_{t}=1)_{t \in T})\\ \label{proofeq:pr(I)}
&+ \lambda (1-\lambda)^{b-1} \Prob(\mathcal{I}_{s, i, T} \mid t^\star=i, g^\star_{< s} = 1, (g_{t}=2)_{t \in T})
\enspace.
\end{align}
where the last equation holds because $T \subset \{ s, \ldots, i-1 \}$ and thus $(g_t)_{t \in T}$ are independent of $g_{< s}^\star$ and of the event $t^\star = i$.
%We want to control the probability of the intersection of events
%\begin{align*}
%\mathcal{W}_{T}^{(s,i)} \cap \mathcal{B}_T \cap \{r_i =1, R_i=1\}\enspace,
%\end{align*}
%conditionally on the event $t^\star=i$.
%\begin{align*}
%\Prob[ \rho_b = \tau = i, \rho_1 = t_1, \ldots, \rho_{b-1} = &t_{b-1} \mid t^\star = i, g_{t_1}=1] \\=& \lambda^{b-2} \Prob[\mathcal{W}_{T}^{(s,i)} \cap \mathcal{B}_T \cap \{r_i =1, R_i=1\} \mid t^\star=i, g_{t_1}=1] \enspace.
%\end{align*}
% The events $\mathcal{W}_{T}^{(s,i)}, \mathcal{B}_T$ and $\{r_i =1, R_i=1\}$ are mutually independent conditionally on $G_i \coloneqq (G_i^1, G_i^2)$. Moreover, the first two events are independent of the event $t^\star = i$. Hence, it suffices to compute the probability of each event separately.
% Since the ranks are mutually independent and uniformly distributed conditionally on $G_i$, we get
% \begin{align*}
% \Prob[\mathcal{W}_{T}^{(s,i)}\mid G_i, g_{t_1}=1] = \prod_{j \in \{s, \ldots, i-1\}\setminus T} \Prob[r_j \neq 1 \mid G_j^{g_j}] = \prod_{j \in \{1, \ldots, i-1\}\setminus T} \frac{\lvert G_j^{g_j} \rvert - 1}{\lvert G_j^{g_j} \rvert}
% \end{align*}
% and
% \begin{align*}
% \Prob[\mathcal{B}_T \mid G_i, g_{t_1}=1] = \prod_{j \in T} \Prob[r_j = 1, R_j \neq 1, g_j=1 \mid G_j, g_{t_1}=1] = \lambda^{b-2} \prod_{j \in T} \frac{1}{\lvert G_j^{1} \rvert} \frac{\lvert G_j^{2} \rvert}{\lvert G_j^{2} \rvert +1}\enspace.
% \end{align*}
% Moreover,
% \begin{align*}
% \Prob[r_i = 1, R_i = 1 \mid t^\star=i] = 1\enspace.
% \end{align*}
% %\begin{align*}
% %\Prob[r_i = 1, R_i = 1] = \mathbb{E}\left(\frac{1}{\lvert G_i^{g_i} \rvert} \frac{1}{\lvert G_i^{\bar{g_i}} \rvert + 1}\right)
% %\end{align*}
% Consequently, by conditional independence of $\mathcal{W}_{T}^{(s,i)}$, $\mathcal{B}_T$ and $\{r_i=1, R_i=1\}$,
% \begin{align*}
% \Prob[\mathcal{I}_{s, i, T} \mid t^\star=i, g_{t_1}=1] &= \mathbb{E}\left(\Prob[\mathcal{W}_{T}^{(s,i)}\mid G_i] \times \Prob[\mathcal{B}_T \mid G_i, g_{t_1}=1] \times \Prob[r_i=1, R_i=1 \mid t^\star = i] \right)\\
% &= \mathbb{E}\left(\left(\prod_{j=s}^{i-1} \frac{1}{\lvert G_j^{g_j}\rvert} \right) \times \left( \prod_{j \in \{1, \ldots, i-1\} \setminus T} (\lvert G_j^{g_j}\rvert - 1) \right)\times\left( \prod_{j \in T} \frac{\lvert G_j^{2} \rvert}{\lvert G_j^{2} \rvert +1}\right)\right)\\
% &= \mathbb{E}\left(\left(\prod_{j=s}^{i-1} \frac{\lvert G_j^{g_j}\rvert - 1}{\lvert G_j^{g_j}\rvert} \right) \times \left( \prod_{j \in T} \frac{\lvert G_j^{2} \rvert}{(\lvert G_j^{2} \rvert +1)( \lvert G_j^{1}\rvert - 1)}\right)\right)\\
% &= \mathbb{E}\left(\left(\frac{\lvert G_{s-1}^1 \rvert \cdot \lvert G_{s-1}^2 \rvert}{\lvert G_{i-1}^1 \rvert \cdot \lvert G_{i-1}^2 \rvert}\right) \times \left( \prod_{j \in T} \frac{\lvert G_j^{2} \rvert}{(\lvert G_j^{2} \rvert +1)( \lvert G_j^{1}\rvert - 1)}\right)\right)
% \end{align*}
% Observing that
% \begin{align*}
% \prod_{j \in T} \frac{\lvert G_j^{2}\rvert}{\lvert G_j^{2}\rvert + 1} \geq \prod_{k=1}^{b-1} \frac{k}{k+1} = \frac{1}{b}\enspace,
% \end{align*}
% we obtain the lower bound
% \begin{align*}
% \Prob[\mathcal{I}_{s, i, T} \mid t^\star=i, g_{t_1}=1] &\geq \frac{\lambda^{b-1}}{b} \mathbb{E}\left(\left(\frac{\lvert G_{s-1}^1 \rvert \cdot \lvert G_{s-1}^2 \rvert)}{\lvert G_{i-1}^1 \rvert \cdot \lvert G_{i-1}^2 \rvert}\right) \times \left( \prod_{j \in T} \frac{1}{ \lvert G_j^{1}\rvert - 1}\right)\right)\enspace.
% \end{align*}
Let us denote by $\Prob_G$ the probability measure $\Pr$ conditionally on the groups $G_i^1$, $G_i^2$, the events $\cap_{k=1, \ldots, b-1} \{g_{t_k}=1\}$, $\{g^\star_{<s} = 2\}$ and $\{t^\star=i\}$. In what follows, we derive an explicit formula for the probability
\begin{align*}
\Prob(\mathcal{I}_{s, i, T} \mid t^\star=i,g^\star_{<s} = 2 ,(g_{t}=1)_{t \in T}) = \mathbb{E}[\Prob_G(\mathcal{I}_{s, i, T})]\enspace.
\end{align*}
Note that a similar formula can be derived for $\Prob(\mathcal{I}_{s, i, T} \mid t^\star=i, (g_{t}=2)_{t \in T})$ following exactly the same steps.
Applying the chain rule, we can decompose the probability of interest as 
\begin{align*}
\Prob_G(\mathcal{I}_{s, i, T}) 
=& \Prob_G\left( \bigcap_{j \in \llbracket s, t_1-1\rrbracket} \{r_j \neq 1\} \right)\\
& \times\prod_{k=1}^{b-1}
\Prob_G \left( r_{t_k}=1, R_{t_k} \neq 1 \,\middle\vert\,  \bigcap_{j \in \llbracket s, t_{k}\rrbracket \setminus T} \{\ r_j \neq 1\}, \bigcap_{j \in \llbracket s, t_{k-1}\rrbracket \cap T} \{r_j=1, R_j \neq 1\}  \right)\\
&\times \Prob_G \left(\bigcap_{j \in \llbracket t_{k}, t_{k+1}-1\rrbracket} \{r_j \neq 1\} \,\middle\vert\,  \bigcap_{j \in \llbracket s, t_{k}\rrbracket \setminus T} \{\ r_j \neq 1\}, \bigcap_{j \in \llbracket s, t_{k}\rrbracket \cap T} \{r_j=1, R_j \neq 1\}\right)\\
&\times \Prob_G\left(r_i=R_i=1 \mid  \,\middle\vert\,  \bigcap_{j \in \llbracket s, i-1\rrbracket \setminus T} \{\ r_j \neq 1\}, \bigcap_{j \in \llbracket s, i-1\rrbracket \cap T} \{r_j=1, R_j \neq 1\} \right)\enspace.
\end{align*}
By mutual independence of the within-group ranks conditionally on the groups,
\begin{align*}
\Prob_G\left( \cap_{j \in \llbracket s, t_1-1\rrbracket} \{r_j \neq 1\} \right) = \prod_{j \in \llbracket s, t_1-1\rrbracket} \Prob(r_j \neq 1 \mid G_j^{g_j}) = \prod_{j \in \llbracket s, t_1-1\rrbracket} \frac{\lvert G_j^{g_j}\rvert-1}{\lvert G_j^{g_j} \rvert}\enspace,
\end{align*}
and, for any $1 \leq k \leq b-1$,
\begin{align*}
\Prob_G &\left(\bigcap_{j \in \llbracket t_{k}+1, t_{k+1}-1\rrbracket} \{r_j \neq 1\} \,\middle\vert\,  \bigcap_{j \in \llbracket s, t_{k}\rrbracket \setminus T} \{\ r_j \neq 1\}, \bigcap_{j \in \llbracket s, t_{k}\rrbracket \cap T} \{r_j=1, R_j \neq 1, g_j=1\}\right)\\
&= \Prob_G \left(\bigcap_{j \in \llbracket t_{k}, t_{k+1}-1\rrbracket} \{r_j \neq 1\} \right)\\
&= \prod_{j \in \llbracket t_{k}+1, t_{k+1}-1\rrbracket} \Prob(r_j \neq 1 \mid G_j^{g_j})\\
&= \prod_{j \in \llbracket t_{k}+1, t_{k+1}-1\rrbracket} \frac{\lvert G_j^{g_j}\rvert-1}{\lvert G_j^{g_j} \rvert}\enspace.
\end{align*}

Applying once again the chain rule, we get for any $1 \leq k \leq b-1$,
\begin{align*}
\Prob_G &\left( r_{t_k}=1, R_{t_k} \neq 1 \,\middle\vert\,  \bigcap_{j \in \llbracket s, t_{k}\rrbracket \setminus T} \{\ r_j \neq 1\}, \bigcap_{j \in \llbracket s, t_{k-1}\rrbracket \cap T} \{r_j=1, R_j \neq 1, g_j=1\}  \right) 
\\=& \Prob(r_{t_k}=1 \mid g_{t_k}=1, G_{t_k}^1)\\ &\times\quad  \Prob\left(R_{t_k}\neq 1 \,\middle\vert\,  \bigcap_{j \in \llbracket s, t_{k}\rrbracket \setminus T} \{\ r_j \neq 1\}, \bigcap_{j \in \llbracket s, t_{k-1}\rrbracket \cap T} \{r_j=1, R_j \neq 1, g_j=1\}, r_{t_k}=1\right)\enspace.
\end{align*}

First, notice that
$\Prob(r_{t_k}=1\mid g_{t_k}=1, G_{t_k}^1) = \frac{1}{\lvert G_{t_k}^1 \rvert}$. Then, 
\begin{align*}
\Prob_G & \left(R_{t_k} =  1 \,\middle\vert\,  \bigcap_{j \in \llbracket s, t_{k}\rrbracket \setminus T} \{\ r_j \neq 1\}, \bigcap_{j \in \llbracket s, t_{k-1}\rrbracket \cap T} \{r_j=1, R_j \neq 1, g_j=1\}, r_{t_k}=1, \right)\\
= &\Prob_G\left(x_{t_k} > \max_{1 \leq j \leq t_k-1} x_j \,\middle\vert\,  \bigcap_{j \in \llbracket s, t_{k}\rrbracket \setminus T} \{\ r_j \neq 1\}, \bigcap_{j \in \llbracket s, t_{k-1}\rrbracket \cap T} \{r_j=1, R_j \neq 1, g_j=1\}, r_{t_k}=1\right)\\
= &\Prob_G\left(x_{t_k} > \max_{\substack{1 \leq j \leq s-1 \\ g_j=2}} x_j \,\middle\vert\,  \bigcap_{j \in \llbracket s, t_{k}\rrbracket \setminus T} \{\ r_j \neq 1\}, \bigcap_{j \in \llbracket s, t_{k-1}\rrbracket \cap T} \{r_j=1, R_j \neq 1, g_j=1\}, r_{t_k}=1\right)\\
&= \Prob_G\left(x_{t_k} > \max_{\substack{1 \leq j \leq s-1 \\ g_j=2}} x_j \right)\\
&= \frac{1}{1 + \lvert G_{s-1}^2\rvert}\enspace.
\end{align*}
We used the crucial fact that, on the conditioning event, the best candidate from group $2$ up until time $i-1$ showed up among the first $s-1$ candidates. All in all, we obtain
\begin{align*}
\Prob_G(\mathcal{I}_{s, i, T}) &= \left(\prod_{j \in \llbracket s, t_1-1\rrbracket} \frac{\lvert G_j^{g_j}\rvert-1}{\lvert G_j^{g_j} \rvert}\right) \times \prod_{k=1}^{b-1} \left(\frac{1}{\lvert G_{t_k}^1\rvert} \frac{\lvert G_{s-1}^2\rvert}{1 + \lvert G_{s-1}^2\rvert}  \prod_{j \in \llbracket t_{k}+1, t_{k+1}-1\rrbracket} \frac{\lvert G_j^{g_j}\rvert-1}{\lvert G_j^{g_j} \rvert}\right)\\
&= \prod_{j \in \llbracket s, i-1\rrbracket} \frac{\lvert G_j^{g_j}\rvert-1}{\lvert G_j^{g_j} \rvert} \times \prod_{k=1}^{b-1} \frac{1}{\lvert G_{t_k}^{1}\rvert-1} \times \left(\frac{\lvert G_{s-1}^2\rvert}{1 + \lvert G_{s-1}^2\rvert} \right)^{b-1}\\
&= \frac{\lvert G_{s-1}^1 \rvert \lvert G_{s-1}^2 \rvert}{\lvert G_{i-1}^1 \rvert \lvert G_{i-1}^2 \rvert} \left(\frac{\lvert G_{s-1}^2\rvert}{1 + \lvert G_{s-1}^2\rvert} \right)^{b-1} \prod_{k=1}^{b-1} \frac{1}{\lvert G_{t_k}^{1}\rvert-1}\\
&\geq \frac{\lvert G_{s-1}^1 \rvert \lvert G_{s-1}^2 \rvert}{\lvert G_{i-1}^1 \rvert \lvert G_{i-1}^2 \rvert}
\left(1 - \frac{b-1}{1+\lvert G^2_{s-1} \rvert} \right)
\prod_{k=1}^{b-1} \frac{1}{\lvert G_{t_k}^{1}\rvert}
\enspace.
\end{align*}
where we used Bernoulli's inequality in the last line.
Conditioning on $\mathcal{C}_{s,N}$, similarly to the estimations we did in previous proofs, we obtain
\[
\frac{\lvert G^1_{s-1}\rvert}{\lvert G^1_{i-1}\rvert}
\geq \frac{s}{i}\left( 1 - \frac{3A_\w}{\lambda}\sqrt{\frac{\log s}{s}} \right)\enspace,
\]
and we have the same for $\lvert G^2_{s-1}\rvert / \lvert G^2_{i-1}\rvert$ replacing $\lambda$ by $1-\lambda$. We also have that
\begin{align*}
\frac{1}{{1+\lvert G^2_{s-1} \rvert}}
&\leq \frac{1}{\lambda s - A_\w \sqrt{s \log s}}\\
&\leq \frac{2}{\lambda s}
\leq \frac{\sqrt{2}A_\w}{\lambda} \sqrt{\frac{\log s}{s}}    
\end{align*}


because $A_\w \geq 1/\sqrt{2}$, and
\begin{align*}
\prod_{k=1}^{b-1} \frac{1}{\lvert G_{t_k}^{1}\rvert}
&\geq \prod_{k=1}^{b-1} \left(\frac{1}{\lambda t_k} \left( 1 - \frac{A_\w}{\lambda}\sqrt{\frac{\log s}{s}} \right) \right)\\
&\geq \left( \lambda^{-b+1} \prod_{k=1}^{b-1} \frac{1}{t_k} \right)
\left( 1 - \frac{(b-1)A_\w}{\lambda}\sqrt{\frac{\log s}{s}}
\right)\enspace.
\end{align*}
It yields using Bernoulli's inequality
\begin{align*}
\Prob_G(\mathcal{I}_{s, i, T} \mid \mathcal{C}_{s,N})
&\geq
\lambda^{-b+1} \frac{s^2}{i^2} \prod_{k=1}^{b-1} \frac{1}{t_k} \left( 1 -\left( \frac{3}{\lambda (1 - \lambda)} - \frac{(1+\sqrt{2})(b-1)}{\lambda} \right) A_\w \sqrt{\frac{\log s}{s}} \right)\enspace.
\end{align*}
Replacing $G^1$ by $G^2$ we obtain the same lower bound with $1-\lambda$ instead of $\lambda$, hence substituting into \ref{proofeq:pr(I)} gives
\begin{align*}
\Prob(\mathcal{I}_{s, i, T} \mid t^\star=i, \mathcal{C}_{s,N}) 
&\geq 
\frac{s^2}{i^2} \prod_{k=1}^{b-1} \frac{1}{t_k} \left( 
1 - \left( \frac{3}{\lambda} + (1-\lambda)\frac{(1+\sqrt{2})(b-1)}{\lambda} \right. \right.\\
& \hspace{2cm} \left. \left.+ \frac{3}{1 - \lambda} + \lambda\frac{(1+\sqrt{2})(b-1)}{1-\lambda}
\right)A_\w\sqrt{\frac{\log s}{s}}
\right)\\
&\geq \frac{s^2}{i^2} \prod_{k=1}^{b-1} \frac{1}{t_k} \left( 1 - \frac{(2b + 1)A_\w}{\lambda(1 - \lambda)} \sqrt{\frac{\log s}{s}} \right)\enspace,
\end{align*}
finally, since $\mathcal{C}_{s,N}$ is true with probability at least $2/s$ and given that $A_\w \geq 1/\sqrt{2}$ and $\lambda(1-\lambda) \leq 1/4$ we deduce that
\[
\Prob(\mathcal{C}_{s,N})
\geq 1 - \frac{2}{s} \geq 1 - \frac{A_\w}{\lambda(1-\lambda)}\sqrt{\frac{\log s}{s}}\enspace,
\]
and thus with Bernoulli's inequality
\[
\Prob(\mathcal{I}_{s, i, T} \mid t^\star=i)
\geq \frac{s^2}{i^2} \prod_{k=1}^{b-1} \frac{1}{t_k} \left( 1 - \frac{2(b + 1)A_\w}{\lambda(1 - \lambda)} \sqrt{\frac{\log s}{s}} \right)\enspace.
\]
the claim of the Lemma follows immediately from.
\[
p^b_N(\w)  = \frac{1}{N}\sum_{i=s}^N \sum_{s \leq t_1 < \ldots, t_{b-1} < i} \Prob(\mathcal{I}_{s, i, T} \mid t^\star=i)\enspace.
\]


\end{proof}

The following step is to estimate the sum appearing in the previous Lemma. We do that using computational results from Section \ref{sec:comp}.

\begin{lemma}\label{lem:pb(s,N)}
Assume that $s = \w N$. We have  for any $b\geq 2$,
\[
\frac{s^2}{N} \sum_{i=s}^N \frac{1}{i^2} \sum_{s \leq t_1 < \ldots < t_{b-1} < i} \prod_{\ell=1}^{b-1} \frac{1}{t_\ell}
\geq {p}^b_{\infty}( \w)
- \frac{\e}{s} \log(1/ \w)^{b-2},
\]
with $A_\w$ defined in Lemma \ref{lem:conc-card}, and  
\[
p^b_{\infty}( \w) = \w^2\left( \frac{1}{\w} - \sum_{\ell = 0}^{b-1} \frac{\log(1/ \w)^\ell}{\ell !} \right).
\]
\end{lemma}

\begin{proof}
Recall that $s = \w N$.
Lemma \ref{lem:pb-sum} gives that
\begin{equation}\label{eq:pb-Sb}
p^b_N(\w) 
\geq
\left( 1 - \frac{2(b+1)A_\w}{\lambda(1-\lambda)} \sqrt{\frac{\log s}{s}}\right)
\frac{s^2}{N} \sum_{i=r}^N \frac{1}{i^2} S_{b-1}(r,i),
\end{equation}

with 
\[
S_{b-1}(r,i) := \sum_{r \leq t_1 < \ldots < t_{b-1} < i} \prod_{\ell=1}^{b-1} \frac{1}{t_\ell},
\]
Lemma \ref{lem:Sm(s,N)} shows that 
\[
    S_{b-1}(r,i) \geq \frac{\log(i/s)^{b-1}}{(b-1)!} - \frac{e\log(1/ \w)^{b-2}}{s},
\]
therefore we have
\begin{align*}
\frac{s^2}{N} \sum_{i=r}^N \frac{1}{i^2} S_{b-1}(r,i)
&\geq \frac{s^2}{N} \sum_{i=r}^N \frac{1}{i^2} \left( \frac{\log(i/s)^{b-1}}{(b-1)!} 
     - \frac{e\log(1/ \w)^{b-2}}{s} \right)\\
&= \frac{s^2}{N(b-1)!} \sum_{i=r}^N \frac{\log(i/s)^{b-1}}{i^2}
   - e\log(1/ \w)^{b-2} \frac{s}{N} \sum_{i=r}^N \frac{1}{i^2}
\end{align*}
we immediately have
\begin{align*}
\frac{s}{N} \sum_{i=r}^N \frac{1}{i^2}
\leq \frac{s}{N} \int_{s}^N \frac{d}{x^2}
= \frac{s}{N} \left( \frac{1}{s} - \frac{1}{N} \right)
\leq \frac{1}{N},
\end{align*}
and using Lemmas \ref{lem:sum-int} then \ref{lem:Im} we have
\begin{align*}
\frac{s^2}{N(b-1)!} \sum_{i=r}^N \frac{\log(i/s)^{b-1}}{i^2}
&\geq \frac{s^2}{N(b-1)!}\left( \int_s^N  \frac{\log(x/s)^{b-1}}{x^2}dx
      - \frac{\log(1/ \w)^{b-1}}{s^2}\right)\\
&= \frac{s^2}{N} \left( \frac{1}{s} - \frac{1}{N} \sum_{\ell = 0}^{b-1}
    \frac{\log(1/ \w)^\ell}{\ell !} \right)
    - \frac{\log(1/ \w)^{b-1}}{N(b-1)!}\\
&= \left( \w - \w^2 \sum_{\ell = 0}^{b-1} \frac{\log(1/ \w)^\ell}{\ell !} \right)
    - \frac{\log(1/ \w)^{b-1}}{N(b-1)!}\\
&= \tilde{p}_b( \w) - \frac{\log(1/ \w)^{b-1}}{N(b-1)!}.
\end{align*}
We deduce that
\begin{align*}
\frac{s^2}{N} \sum_{i=r}^N \frac{1}{i^2} S_{b-1}(r,i)
&\geq \tilde{p}_b( \w)
    - \frac{\log(1/ \w)^{b-1}}{N(b-1)!}
    - \frac{e\log(1/ \w)^{b-2}}{N}\\
&\geq \tilde{p}_b( \w)
    - \frac{\e}{s} \log(1/ \w)^{b-2},
\end{align*}
where we used for the last inequality that $\frac{1}{(b-1)!} \leq e$, $\log(1/ \w) \leq 1/\w - 1$ and $\w N = s$.
\end{proof}


With the previous lemmas, we can finally prove Theorem \ref{thm:lb-PB(s,N)}. 

\begin{proof}[Proof of Theorem \ref{thm:lb-PB(s,N)}]
From Lemmas \ref{lem:p1(s,N)} and \ref{lem:pb(s,N)}, we have that for any $b \geq 1$
\begin{align*}
p^b_N(\w) 
\geq 
\left( p^b_{\infty}( \w) - \frac{\e \indic{b\geq2}}{s} \log(1/ \w)^{b-2} \right)
\left( 1 - \frac{2(b+1)A_\w}{\lambda(1-\lambda)} \sqrt{\frac{\log s}{s}}\right)\\
\geq 
\left( p^b_{\infty}( \w) - \frac{\e \indic{b\geq2}}{s} \log(1/ \w)^{b-2} \right)
\left( 1 - \frac{2(B+1)A_\w}{\lambda(1-\lambda)} \sqrt{\frac{\log s}{s}}\right)
\end{align*}
where 
\[
p^b_{\infty}( \w) = \w^2\left( \frac{1}{\w} - \sum_{\ell = 0}^{b-1} \frac{\log(1/ \w)^\ell}{\ell !} \right).
\]
Using Lemma \ref{lem:tau=rho}, and denoting $P^B_{\infty}( \w):= \sum_{b=1}^{B+1}p^b_{\infty}( \w)$, we deduce that
\begin{align*}
P^B_N(\w) 
&= \sum_{b=1}^{B+1} p_b(s,N)\\
&\geq 
\left(P^B_{\infty}( \w) - \frac{\e}{s}\sum_{b=0}^{B-1}\log(1/ \w)^b \right)
\left( 1 - \frac{2(B+1)A_\w}{\lambda(1-\lambda)} \sqrt{\frac{\log s}{s}}\right) \\
&\geq 
P^B_{\infty}( \w) - \frac{2(B+1)\tilde{P}_B( \w)A_\w}{\lambda(1-\lambda)}\sqrt{\frac{\log s}{s}} - \frac{\e}{s}\sum_{b=0}^{B-1}\log(1/ \w)^b.
\end{align*}
Lemma \ref{lem:sum-exp-remainder} with $x= \log(1/ \w)$ gives 
\begin{align*}
P^B_{\infty}( \w):=
&\w^2 \sum_{b=1}^{B+1} \left( \frac{1}{\w} - \sum_{\ell=1}^{b-1} \frac{\log(1/ \w)^\ell}{\ell !}   \right)\\
&= \w^2 \sum_{b=0}^B \left( \frac{1}{\w} - \sum_{\ell=0}^{b} \frac{\log(1/ \w)^\ell}{\ell !}   \right)\\
&\leq \w \log(1/ \w),    
\end{align*}

and thus we have
\begin{align*}
P^B_{\infty}( \w)A_\w 
&= P^B_{\infty}( \w)\frac{1 + \sqrt{\w}}{2\sqrt{2}\w^{\frac{1}{4}}}\\
&\leq \log(1/ \w) \w^{\frac{3}{4}} \frac{1 + \sqrt{\w}}{2\sqrt{2}}\\
&\leq \log(1/ \w) \sqrt{\frac{\w}{2}},
\end{align*}
It yields
\begin{align*}
P_N^B(\w) 
&\geq 
P^B_{\infty}( \w)
- \left( \frac{(B+1)\sqrt{2\w}\log(1/ \w)}{\lambda(1-\lambda)} + \e \sum_{b=0}^{B-1} \log(1/ \w)^b \right) \sqrt{\frac{\log s}{s}}\\
&\geq P^B_{\infty}( \w)
- \left( \frac{\sqrt{2}(B+1)\log(1/ \w)}{\lambda(1-\lambda)} + \frac{\e}{\sqrt{\w}} \sum_{b=0}^{B-1} \log(1/ \w)^b \right) \sqrt{\frac{\log N}{N}}\enspace,
\end{align*}
we used in the last line the inequality $\sqrt{\log s /s} \leq \sqrt{\log N / (\w N)}.$
\end{proof}
