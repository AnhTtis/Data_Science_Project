
\section{How to choose optimal thresholds ?}
\label{appx:choose-opt-thresh}

\subsection{Strong optimality of the thresholds $\alpha^\star_0, \beta^\star_0$}
The following Proposition states that for any DDT algorithm with budget $B \geq 0$, if at some step $w N$ the algorithm still did not stop and does not have any comparisons left, then its success probability is upper bounded by $\varphi_0(w; (\alpha_0^\star, \beta_0^\star))$, where $(\alpha_0^\star, \beta_0^\star) = (\lambda \exp(1/\lambda - 2), \lambda)$. In particular, this implies that whatever is the initial budget $B$, choosing $\alpha_0 = \alpha^\star_0, \beta_0 = \beta^\star_0$ is optimal.

\begin{proposition}\label{prop:DDT0-optanyw}
For all $w \in [0, 1]$ and $\lambda \geq 1/2$, the mapping $(\alpha_0, \beta_0) \mapsto\varphi_0(w; (\alpha_0, \beta_0))$ is maximized over $\{\alpha_0, \beta_0 \in (0,1)\}$ at
\[
    \alpha_0^\star = \lambda \exp\left( \frac{1}{\lambda} - 2\right)
    \quad \text{ and } \quad
    \beta_0^\star  = \lambda\enspace.
\]

\end{proposition}


\begin{proof}
   Fix some $\lambda \geq 1/2$. For convenience, let us drop the subscript $0$ from $g_0$.
Our goal is to show that for all $w \in [0, 1]$ and all $\alpha \leq 
\beta$ we have
\begin{align*}
    \begin{cases}
        g(\alpha, \beta) & \mbox{if } w < \alpha,\\
        g(w, \beta) & \mbox{if } \alpha \leq w < \beta,  \\
        g(w, w) & w\geq \beta.
    \end{cases} \quad\leq\quad
    \begin{cases}
        g(\alpha^\star, \lambda) & \mbox{if } w < \alpha^\star,\\
        g(w, \lambda) & \mbox{if } \alpha^\star \leq w < \lambda,  \\
        g(w, w) & w\geq \lambda
    \end{cases}\enspace,
\end{align*}
where $\alpha^\star = \lambda \exp(\lambda^{-1}-2)$.
Let $\alpha^*(\beta) = \beta\exp((1-\beta)\lambda^{-1}-1)$, the value that maximizes $g(\alpha, \beta)$ over the first coordinate.
Note that $\alpha^*(\beta)$ is maximized at $\beta = \lambda$ and $\alpha^*(\lambda) \leq \lambda $ as long as $\lambda \geq 1/2$, implying that $\alpha^*(\beta) \leq \lambda$ for all $\beta \in [0,1]$.
The following properties of $g$ hold if $\alpha \leq \beta$, which are direct consequence of the fact that $g(\alpha, \beta)$ is concave on $\alpha$, concave on $\beta$, maximized in $(\alpha^\star, \lambda)$ and the maximum of $g(\alpha, \beta)$ over $\alpha$ is always upper bounded by $\lambda$:
\begin{enumerate}
    \item\label{item_prop:1} $g(\alpha, \beta) \leq g(\alpha^\star, \lambda)$ for all $\alpha, \beta$ \,\,\,(i.e., $g(\cdot, \cdot)$ is maximized at $(\alpha^\star, \lambda)$);
    \item\label{item_prop:2} $g(\alpha, \beta) \leq g(\alpha, \lambda)$ for all $\alpha, \beta$ \,\,\,(i.e., $g(\alpha, \cdot)$ is maximized at $\lambda$);
    \item\label{item_prop:3} $g(\alpha, \lambda) \leq g(w, \lambda)$ for all $\alpha^\star \leq w \leq \alpha$ \,\,\,(i.e., $g(\cdot, \lambda)$ decreases on $[\alpha^\star, 1]$);
    \item\label{item_prop:5} $g(\alpha, \beta) \leq g(\alpha, w)$ for all $\lambda \leq w \leq \beta$ and all $\alpha$ \,\,\,(i.e., $g(\alpha, \cdot)$ decreases on $[\lambda, 1]$);
    \item\label{item_prop:7} $g(\alpha, \beta) \leq g(w, \beta)$ for all $\lambda \leq w \leq \alpha$ and all $\beta$ (i.e., $g(\cdot, \beta)$ decreases on $[\alpha^*(\beta), 1]$ and hence on $[\lambda, 1]$);
\end{enumerate}
Fix some $\alpha \leq \beta$.\\
\textbf{Case 1 ($w \leq \alpha^{\star}$)}: by item \eqref{item_prop:1} the desired statement holds. \\
\textbf{Case 2 ($w \in [\alpha^\star, \lambda)$)}: thanks to items~\eqref{item_prop:2},\eqref{item_prop:3} if $w \in [\alpha^\star, \alpha)$,
item~\eqref{item_prop:2} if $\alpha^* \leq \alpha \leq w \leq \beta$,
item~\eqref{item_prop:2} if $w \in [\beta, \lambda)$
\begin{align*}
    \begin{cases}
        g(\alpha, \beta) & \mbox{if } w < \alpha,\\
        g(w, \beta) & \mbox{if } \alpha \leq w < \beta,  \\
        g(w, w) & w\geq \beta.
    \end{cases} \quad\leq\quad g(w, \lambda)\enspace.
\end{align*}
and the desired statement holds.\\
\textbf{Case 3 ($w \geq \lambda$)}: thanks to items~\eqref{item_prop:5},\eqref{item_prop:7} if $w \leq \alpha$ and $\beta \geq \alpha \geq \lambda$, item~\eqref{item_prop:5} if $w \in [\alpha, \beta)$ and $\alpha \geq \lambda$
\begin{align*}
    \begin{cases}
        g(\alpha, \beta) & \mbox{if } w < \alpha,\\
        g(w, \beta) & \mbox{if } \alpha \leq w < \beta,  \\
        g(w, w) & w\geq \beta.
    \end{cases} \quad\leq\quad g(w, w)
\end{align*}
and the desired statement holds.
\end{proof}



While the recursive expression in Theorem \ref{thm:asymptoticDDT} on the success probability gives a way to numerically compute the optimal thresholds, it is still unclear if they are universal and do not depend on the initial budget.
For any initial budget $B \geq 0$, let $(\alpha^B_b, \beta^B_b)_{b=0}^B$ be some optimal choice of thresholds, i.e
\[
(\alpha^B_b, \beta^B_b)_{b=0}^B
\in \argmax_{(\alpha_b, \beta_b)_{b=0}^B} \varphi_B\left(w;(\alpha_b, \beta_b)_{b=0}^B\right) \enspace.
\]
For $B = 0$, Proposition \ref{prop:DDT0-optanyw} establishes that the optimal thresholds $\alpha^0_0, \beta^0_0$ maximize the mapping $(\alpha, \beta) \mapsto \varphi_0(w; (\alpha, \beta))$. Given that $\varphi\big(0,(\alpha^1_b, \beta^1_b)_{b=0,1}\big) = g_1(\alpha^1_1, \beta^1_1)$ and by definition of $g_1$, the previous statement implies that 
\begin{align*}
\varphi_1(0;(&\alpha^1_b, \beta^1_b)_{b=0,1})\\
&= g_0(\alpha^1_0, \beta^1_0)+ (1 - \lambda) \alpha^1_1 \int_{\alpha^1_1}^{\beta^1_1} \frac{\phi_{0}\left(u ; (\alpha^1_0, \beta^1_0)\right)}{u^2} \diff u {+} \alpha^1_1\beta^1_1 \int_{\beta^1_1}^1 \frac{\phi_{0}\left(u ; (\alpha^1_0, \beta^1_0) \right)}{u^3} \diff u\\
&\leq g_0(\alpha^0_0, \beta^0_0)+ (1 - \lambda) \alpha_1 \int_{\alpha_1}^{\beta_1} \frac{\phi_{0}\left(u ; (\alpha^0_0, \beta^0_0)\right)}{u^2} \diff u {+} \alpha_1\beta_1 \int_{\beta^1_1}^1 \frac{\phi_{0}\left(u ; (\alpha^0_0, \beta^0_0) \right)}{u^3} \diff u\\
&= \varphi_1(0;(\alpha^0_0, \beta^0_0), (\alpha^1_1, \beta^1_1))\enspace.
\end{align*}
Thus, the thresholds $((\alpha^0_0, \beta^0_0), (\alpha^1_1, \beta^1_1))$ are also optimal. We can prove by induction that for any $B \geq 0$, replacing $(\alpha^B_0, \beta^B_0)$ by $(\alpha^0_0, \beta^0_0)$ can only increase the success probability, deducing that optimal values $\alpha_0, \beta_0$ can be chosen independently of the initial budget.


Deriving this property for any $b \geq 1$ is more challenging. The main difficulty lies in the increasing complexity of functions $g_b$ compared to $g_0$. Nevertheless, numerical simulations show that indeed the optimal thresholds are independent of the initial budget.
If supported by theoretical analysis, the above would imply optimal thresholds $(\alpha^\star_b, \beta^\star_b)_{b\geq 0}$ can be obtained by setting $(\alpha^\star_0, \beta^\star_0)$ as in Theorem \ref{thm:prob-win0}, then for any $B \geq 1$ computing
\[
(\alpha^\star_B, \beta^\star_B) \in 
\argmax_{(\alpha_B, \beta_B)} \varphi_B\big{(}0;((\alpha^\star_b, \beta^\star_b)_{b=0}^{B-1}, (\alpha_B, \beta_B))\big{)} \enspace.
\]
Theoretically, and practically, the above characterization of the optimal thresholds is much simpler and more convenient. The rigorous analysis of this phenomenon is left for future work.


\subsection{Using the optimal memory-less algorithm}
In this section, we give a numerical simple way for computing the optimal thresholds, using the intuitions, presented in Section \ref{sec:opt-memless-asymptotic}, on the asymptotic behavior of the optimal memory-less algorithm. First, we prove the differential equation, stated in that section, verified by $\varphi_0(\cdot;(\alpha_0^\star, \beta_0^\star))$.
\begin{proposition}\label{prop:phi0-DE}
the function $w \mapsto \varphi_0(w;(\alpha_0^\star, \beta_0^\star))$ verifies the following differential equation
\[
- \varphi_0'(w;(\alpha_0^\star, \beta_0^\star))
= \left( \lambda - \frac{\varphi_0(w;(\alpha_0^\star, \beta_0^\star))}{w} \right)_{+}
+ \left( 1 - \lambda - \frac{\varphi_0(w;(\alpha_0^\star, \beta_0^\star))}{w} \right)_{+}.
\]
\end{proposition}

\begin{proof}
Let us denote simply $\varphi_0(w)$ instead of $\varphi_0(w;(\alpha_0^\star, \beta_0^\star))$. With Theorem \ref{thm:asymptoticDDT}, we can write $\varphi_0(w) = g_0(\max(\alpha_0^\star, w), \max(\beta_0^\star, w))$, where $(\alpha_0^\star, \beta_0^\star) = (\lambda \exp(1/\lambda - 2), \lambda)$. We will prove the result of the proposition on each of the intervals $[0,\alpha_0^\star), [\alpha_0^\star, \beta_0^\star)$ and $[\beta_0^\star, 1]$. Recall that $\lambda \geq 1/2$.

For $w \leq \alpha_0^\star$ we have $\varphi_0'(w) = \lambda \exp(1/\lambda - 2) = \lambda \alpha_0^\star$. Thus $\frac{\varphi_0(w)}{w} \geq \frac{\varphi_0(w)}{\alphaÂ£_0^\star} = \lambda$. Therefore
\[
- \varphi_0'(w)
= 0
= \left( \lambda - \frac{\varphi_0(w)}{w} \right)_{+}
+ \left( 1 - \lambda - \frac{\varphi_0(w)}{w} \right)_{+}.
\]

For $\alpha_0^\star \leq w < \beta_0^\star$ whe have
\[
\varphi_0(w) 
= g_0(w, \lambda)
= \lambda w \log(\lambda/w) + (1-\lambda)w, 
\]
and $\varphi_0(w) / w = \lambda \log(\lambda/w) + (1-\lambda)$, which is a decreasing function of $w$, and that equals $\lambda$ for $ w = \alpha_0^\star$ and $1-\lambda$ for $w = \lambda = \beta_0^\star$. Therefore $1-\lambda \leq \frac{\varphi_0(w)}{w} \leq \lambda$, and
\begin{align*}
- \varphi_0'(w)
&= \lambda \log(w/\lambda) + \lambda - (1 - \lambda)\\
&= \lambda - (\lambda \log(\lambda/w) + 1 - \lambda)\\
&= \left( \lambda - \frac{\varphi_0(w)}{w} \right)_{+}
+ \left( 1 - \lambda - \frac{\varphi_0(w)}{w} \right)_{+}. 
\end{align*}

Finally, for $w \geq \beta_0^\star$, we have
$\varphi_0(w) = w - w^2$, thus $\frac{\varphi_0(w)}{w} = 1 - w \leq 1 - \beta_0^\star =  1 - \lambda$, and 
\begin{align*}
- \varphi_0'(w)
&= 2w - 1
= (\lambda - (1-w)) + (1 - \lambda - (1-w))\\
&= \left( \lambda - \frac{\varphi_0(w)}{w} \right)_{+}
+ \left( 1 - \lambda - \frac{\varphi_0(w)}{w} \right)_{+}. 
\end{align*}
\end{proof}


\paragraph{Acceptance regions of Algorithm \ref{algo:opt-memless}}

\begin{figure}
    \centering
    \includegraphics[width=0.85\textwidth]{pics/acceptance_region_N50.pdf}
    \includegraphics[width=0.85\textwidth]{pics/acceptance_region_N1000.pdf}
    \caption{Acceptance regions of the optimal memory-less algorithm, with $\lambda = 0.6$, $B= 0$ and $N = 50, 1000$.}
    \label{fig:acceptance_opt}
\end{figure}

Figure \ref{fig:acceptance_opt} shows the acceptance region (dark green) of the optimal algorithm, when $\lambda = 0.6$ and $B=0$, for $N = 50,1000$, i.e., the region where it will return a candidate if $r_t = 1$.
On the x- and y-axes we display the time $t$ and possible group cardinal $|G_t^{1}|$ up to time $t$ respectively. The latter follows binomial random distribution with parameters $(\lambda, t)$ and tightly concentrates around its mean $|G_t^{1}| \approx \lambda t$ (resp. $|G_t^{2}| \approx (1-\lambda) t$) for already moderate values of $t$.
We first remark that the acceptance region depends on the group, since the group membership is stochastic with parameter $\lambda$, the algorithm tries to sufficiently explore either of the groups.
% if at a given step $t$ we have for example $|G^1_t| = 0$ and the candidate who arrives at the next step is in the group $t$, the algorithm will not return it because it did not sufficiently observe the elements of this group yet.
% The same remark is valid for group $G^2$.
Then, let us observe that the shape of the acceptance region is almost identical for $N = 50$ and $N = 1000$, i.e., the curve defining this region converges rapidly to a curve that only depends on $(t/N, |G^1|/N)$.
As recalled above, when $N$ is very large we have that $|G^1_t| \approx\lambda t$, thus, the acceptance region is defined only by a threshold at the intersection of the acceptance curve and the line $|G^1_t| \approx\lambda t$.
Numerically, this intersection for $N=1000$ occurs for $t/N \approx 0.428$ for $G^1$, and $t/N \approx 0.598$ for $G^2$, while the optimal thresholds of the DDT algorithm, given by Proposition \ref{prop:asymptoticDDT0}, are $\alpha_0^\star = 0.430$ and $\beta_0^\star = 0.6$.

The same observations hold for superior values of $B$ and with different values of $\lambda$, where the acceptance region is where the algorithm uses a comparison if $r_t = 1$. 
This shows that the optimal thresholds for any $\lambda$, any $B$, and any $g \in \{1,2\}$ can be estimated as the intersection of the acceptance region for $G^g$ and the line $(t,\lambda t)$, with $N = 500$ for example.

\paragraph{Optimal thresholds via Equation \eqref{eq:Phib}}
To fully exploit the asymptotic behavior of Algorithm \ref{algo:opt-memless}, for a given value of $\lambda$, the optimal thresholds $(\alpha_b^\star(\lambda), \beta_b^\star(\lambda))_{b=1}^B$ can be estimated by numerically solving Equations \eqref{eq:Phib} for $1 \leq b \leq B$, then taking for $b$ as thresholds the respective solutions to 
\[
\lambda \alpha 
= \Phi_b(\alpha) - (1-\lambda)\Phi_b(\alpha)
\quad \text{ and } \quad
(1-\lambda) \beta
= \Phi_b(\beta) - \lambda \Phi_b(\beta).
\]







