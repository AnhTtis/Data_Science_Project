\section{The case of two groups}\label{sec:2grps}

In this section, we delve into the particular case of two groups, and we demonstrate how leveraging different thresholds for each group can enhance the success probability. Let $\lambda \in (0,1)$ represent the probability of belonging to group $G^1$, and $1-\lambda$ the probability of belonging to group $G^2$. We examine the success probability of Algorithm $\A^B(\alpha,\beta)$, with threshold $\lfloor \alpha N \rfloor$ for group $G^1$ and $\lfloor \beta N \rfloor$ for group $G^2$, having a budget of $B$ comparisons. This algorithm is a specific instance of the $\DT$ family, wherein the thresholds depend only on the group, and not on the available budget. We call it a \textit{static double-threshold} algorithm.



We assume without loss of generality that $\alpha \leq \beta$, 
and we denote by $\C_N$ the event 
\[
(\C_N): \quad \forall t \geq 1: \max(||G^1_t| - \lambda t|\;,\;||G^2_t| - (1-\lambda) t|) \leq 4 \sqrt{t \log N}\;.
\]
This event provides control over the group sizes at each step. Lemma \ref{lem:conc-card} guarantees that $\C_N$ holds true with a probability at least $1-\frac{1}{N^2}$ for $N \geq 4$. 
Furthermore, for all $t \in [N]$, we denote by $\A^B_t(\alpha,\beta)$ the algorithm with acceptance thresholds $\max\{\lfloor \alpha N \rfloor, t\}$ and $\max\{\lfloor \beta N \rfloor, t\}$ respectively for groups $G^1$ and $G^2$, and we denote by $\U^B_{N,t,k}$ the probability 
\begin{equation}\label{eq:def-U}
\U^B_{N,t,k} = \Pr(\A^B_t(\alpha, \beta) \text{ succeeds},  g^*_{t-1} = k \mid \C_N)\;.    
\end{equation}
%By Lemma \ref{lem:memless}, the state of $\A^B(\alpha, \beta)$ at any step $t$  is completely determined by $(B_t, g^*_{t-1}, |G^1_{t-1}|)$, with the size of $G^2_{t-1}$ implicitly deduced as $|G^2_{t-1}| = t - 1 - |G^1_{t-1}|$. 
Similar to the analysis of the single-threshold algorithm, we establish in Lemma \ref{lem:recursion-U-alpha-beta} a recursion formula satisfied by $(\U^B_{N,t,k})_{B,t,k}$, which we later utilize to derive lower bounds on the asymptotic success probability of $\A^B(\alpha,\beta)$.
To prove this lemma, we study the probability distribution of the occurrence time $\rho_1$ of the first comparison made by $\A^B_t(\alpha, \beta)$, and we examine the algorithm's success probability following it. Essentially, if $\rho_1 = s$, we can compute the probability of stopping and the corresponding success probability, and the distribution of the state of the algorithm at step $s+1$, which yields the recursion. 
Using adequate concentration arguments and Lemma \ref{lem:recursion-U-alpha-beta}, we show the two following results, giving explicit recursive formulas satisfied by the limit of $\U^{B}_{N,t,k}$ when $N \to \infty$, respectively for $k=2$ and $k=1$.

















\begin{lemma}\label{lem:lim-2grp-k=2}
For all $B \geq 0$ and $w \in [\alpha, \beta]$, the limit $\phi^B_2(\alpha,\beta; w) = \lim_{N \to \infty}\U^B_{N,\lfloor w N \rfloor,2}$ exists, 
and it satisfies the following recursion
\begin{align*}
\phi^B_2(\alpha,\beta;w)
&= - \lambda w \log\big((1-\lambda) \tfrac{w}{\beta} + \lambda \big) + \frac{(1-\lambda)\beta w^2 }{(1-\lambda)w + \lambda \beta}  \sum_{b = 0}^B \left( \frac{1}{\beta} - \sum_{\ell = 0}^b \frac{\log(1/\beta)^\ell}{\ell !} \right)\\
& \quad + \indic{B > 0} w^2 \int_w^\beta \frac{ (1-\lambda)^2 w + \lambda(2-\lambda)u}{((1-\lambda)w+\lambda u)^2 u^2}  \phi^{B-1}_2(\alpha, \beta; u) du\;.
\end{align*}
Moreover, $\U^B_{N,\lfloor w N \rfloor,2} = \phi^B_2(\alpha,\beta; w) + O\Big( \sqrt{\tfrac{\log N}{N}} \Big)$.
\end{lemma}




























\begin{lemma}\label{lem:lim-2grp-k=1}
For all $B \geq 0$ and $w \in [\alpha, \beta]$, the limit $\phi^B_1(\alpha,\beta; w) = \lim_{N \to \infty}\U^B_{N,\lfloor w N \rfloor,1}$ exists, 
and it satisfies the following recursion
\begin{align*}
\phi^B_1(\alpha,\beta;w)
&= \lambda w \log\big(1-\lambda + \lambda \tfrac{\beta}{w}\big) 
+ \frac{\lambda w \beta^2}{(1-\lambda) w + \lambda \beta} \sum_{b = 0}^B \left( \frac{1}{\beta} - \sum_{\ell = 0}^b \frac{\log(1/\beta)^\ell}{\ell !} \right) \\
& \quad + \indic{B > 0} \lambda^2 w \int_w^\beta \frac{(u-w) \phi^{B-1}_2(\alpha, \beta; u)}{((1-\lambda)w + \lambda u)^2 u} du\;.
\end{align*}
Moreover, $\U^B_{N,\lfloor w N \rfloor,1} = \phi^B_1(\alpha,\beta; w) + O\Big( \sqrt{\tfrac{\log N}{N}} \Big)$.
\end{lemma}




We deduce that the asymptotic success probability of Algorithm $\A^B_{\lfloor w N\rfloor}$, conditioned on the event $\C_N$, exists and equals $\phi_1^B(\alpha,\beta;w) + \phi_2^B(\alpha,\beta;w)$. Additionally, by applying Lemma \ref{lem:pr-cond-C_N}, we eliminate the conditioning on $\C_N$, thus proving the following theorem.






\begin{theorem}\label{thm:success-2grps}
For all $0 < \alpha \leq \beta \leq 1$, The success probability of Algorithm $\A^B(\alpha,\beta)$ satisfies
\begin{align*}
\Pr(\A^B(\alpha,&\beta) \text{ succeeds}) - O\Big(\sqrt{\tfrac{\log N}{N}}\Big)\\
&= \lambda \alpha \log\big(\tfrac{\beta}{\alpha}\big) + \alpha \beta \sum_{b = 0}^B \left( \frac{1}{\beta} - \sum_{\ell = 0}^b \frac{\log(1/\beta)^\ell}{\ell !} \right)
+ \indic{B > 0} \alpha \int_\alpha^\beta \frac{ \phi^{B-1}_2(\alpha, \beta; u) du}{u^2}\;,
\end{align*}
with $\phi_2^B(\alpha,\beta;\cdot)$ defined in Lemma \ref{lem:lim-2grp-k=2}.
\end{theorem}


It is possible to use Theorem \ref{thm:success-2grps} and \ref{lem:lim-2grp-k=2} to numerically compute the success probability of $\A^B(\alpha, \beta)$. However, this computation is heavy due the recursion defining $\phi^B_2(\alpha, \beta;w)$. Moreover, it is difficult to prove a closed expression, and even more to compute the optimal thresholds.

By disregarding the term containing $\phi^2(\alpha, \beta; \cdot)$ in the theorem, we derive an analytical lower bound expressed as a function of the parameters $\lambda$, $B$, $\alpha$, and $\beta$, allowing a more effective threshold selection. In the subsequent discussion, for all $w \in (0,1]$ and $B \geq 0$, we denote by $S^B(w)$ the following sum:
\[
S^B(w) = \sum_{b = 0}^B \left( \frac{1}{w} - \sum_{\ell = 0}^b \frac{\log(1/w)^\ell}{\ell !} \right)\;.
\]

\begin{corollary}\label{cor:lb2grps}
Assume that $\lambda \geq 1/2$. Let
$
h^B: \beta \mapsto \min \left\{ \frac{\beta}{e} \exp\left(\frac{\beta S^B(\beta)}{\lambda}  \right)\, , \, \beta  \right\}
$,
and $\tilde{\alpha}_B, \tilde{\beta}_B$ the thresholds defined as $\tilde{\alpha}_B = h^B(\tilde{\beta}_B)$, and $\tilde{\beta}_B$ minimizing the mapping
\[
\beta \in [0,1] \mapsto \lambda h(\beta) \log\big(\tfrac{\beta}{h^B(\beta)}\big) + h^B(\beta) \beta S^B(\beta)\;,
\]
then the success probability of $\A^B(\tilde{\alpha}_B,\tilde{\beta}_B)$ satisfies
\[
\lim_{N \to \infty} \Pr(\A^B(\tilde{\alpha}_B,\tilde{\beta}_B) \text{ succeeds}) 
\geq \frac{1}{e} - \min\left\{ \frac{1}{e(B+1)!}, (\tfrac{4}{e}-1)\lambda(1-\lambda) \right\}\;.
\]
\end{corollary}



%If $\lambda < 1/2$, then by symmetry, choosing adequate thresholds yields the same lower bound on the asymptotic success probability of $\A^B(\alpha, \beta)$.
Therefore, in contrast to the single-threshold algorithm, the asymptotic success probability of $\A^B(\tilde{\alpha}_B,\tilde{\beta}_B)$ approaches $1/e$ both when the budget increases and when $\lambda$ approaches $0$ or $1$.



\subsection{Optimal memory-less algorithm for two groups}\label{sec:opt-dynprog-main}

In the following, an algorithm is called \textit{memory-less} if its actions at any step $t \in [N]$ depend only on the current observations $r_t, g_t, \indic{R_t = 1}$, the available budget $B_t$, and the cardinals $(|G^k_{t-1}|)_{k \in [K]}$. 


  We use in this section a dynamic programming approach to determine the optimal memory-less algorithm, which we denote by $\A_*$.
  
  Unlike previous sections, our analysis here is not asymptotic. By meticulously examining how various variables, including the precise number of candidates observed in each group, influence the success probability of $\A_*$, we rigorously analyze its state transitions and corresponding success probabilities to determine optimal actions at each step.
A full description and analysis of the optimal memory-less algorithm can be found in Section \ref{sec:opt-dynprog}. Here, we illustrate its actions through Figure \ref{fig:DPacceptance}. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/DPacceptanceRegions.pdf} % 
    \caption{Acceptance region of $\A_*$}
    \label{fig:DPacceptance}
\end{figure}
    

\paragraph{Computing optimal thresholds for the DT algorithm.}
Upon observing $(r_t,g_t)$, $\A_*$ makes a decision to accept or reject, where acceptance means $\astop$ if $B=0$ and $\acomp$ otherwise, depending on $t, |G^1_t|, B, g_t$. Figure \ref{fig:DPacceptance} shows its acceptance region (dark green), with $N = 500$, $\lambda = 0.7$, for $B \in \{0,1,2\}$ and for all possible values of $t\in [N]$, $|G^1_t| \leq t$, and $g_t \in \{1,2\}$.
The x- and y-axes display respectively the step $t$ and possible group cardinal $|G_t^{1}|$, which implies $|G_t^{2}|$, up to time $t$. The latter follows a binomial distribution with parameters $(\lambda, t)$, which tightly concentrates around its mean $|G_t^{1}| \approx \lambda t$ (and $|G_t^{2}| \approx (1-\lambda) t$) even for moderate values of $t$. Consequently, when $N$ is large, $|G^1_t| \approx\lambda t$, and the acceptance region is solely defined by a threshold at the intersection of the acceptance region and the line $|G^1_t| \approx\lambda t$.
This observation implies that $\A_*$ behaves as an instance of $\DT$ algorithms when the number of candidates is large. The corresponding thresholds, which we denote by $(\alpha_b^\star, \beta_b^\star)_{b \leq B}$, are necessarily optimal, and can be estimated as the intersection of the acceptance region for $G^k$ and the line $(t,\lambda t)$ for $k \in \{1,2\}$.




\paragraph{Alternative comparison model.} In the particular case of two groups, both comparison models introduced in Section \ref{sec:setup} are equivalent, as freely comparing a candidate with the best in their group and then making one costly comparison with the best candidate from the other group is sufficient to determine if they are the best so far. Therefore, all the results of the current section regarding the static double-threshold algorithm and optimal memory-less algorithm remain true in the alternative comparison model.






