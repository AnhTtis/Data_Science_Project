\section{Optimal memory-less algorithm for two groups}\label{sec:opt-dynprog}

In this section, we derive an optimal memoryless algorithm employing a dynamic programming approach. We analyze the state transitions depending on the algorithm's actions and the associated success probabilities for each state. Unlike previous sections, our study here is not asymptotic. Therefore, we do not rely on estimating the number of candidates in each group using concentration inequalities. Instead, we consider the exact number of candidates in each group as a parameter for decision-making at each step.



\subsection{Memoryless algorithms}
One distinctive feature of Dynamic Threshold algorithms is their decision-making process, which solely depends on the observations at each step and the available budget, without recourse to past comparison history. We designate algorithms exhibiting this characteristic as memory-less algorithms.


\begin{definition}\label{def:memless}
An algorithm $\A$ for the $(K,B)$-secretary problem is memory-less if its actions at any step $t \in [N]$ depend only on the current observations $r_t, g_t, \indic{R_t = 1}$, the available budget $B_t$, the cardinals $(|G^k_{t-1}|)_{k \in [K]}$.
\end{definition}


We assume that a memory-less algorithm is aware of the current step $t$ at any time, and knows the proportions of each group $(\lambda_k)_{k \in [K]}$.
However, in our analysis, the knowledge of group proportions is dispensable since we investigate the asymptotic success probabilities of $\DT$ algorithms. Indeed, for setting thresholds that depend on group proportions, if the smallest threshold is at least $\epsilon > 0$, regardless of group proportions, it suffices to observe the first $\lfloor \epsilon N \rfloor$ candidates, then estimate $\bar{\lambda}_k = \lfloor \epsilon N \rfloor^{-1} \sum_{t = 1}^{\lfloor \epsilon N \rfloor} \indic{g_t = k}$ for all $k \in [K]$. The algorithm can choose the thresholds using $(\bar{\lambda}_k)_{k \in [K]}$ instead of $({\lambda}_k)_{k \in [K]}$. As the number of candidates tends to infinity, $\bar{\lambda}_k$ becomes arbitrarily close to $\lambda_k$ with high probability, and so do the thresholds, assuming they are continuous functions of the group proportions. Though this introduces additional intricacies to the proofs, the fundamental proof arguments and the results remain the same.
While Definition \ref{def:memless} only includes deterministic algorithms, it can be easily extended to randomized algorithms, by considering the distributions of the actions instead of the actions themselves.


In the following lemma, we establish that the success probability of a memory-less algorithm, given the history up to step $t-1$, is contingent upon only a few parameters, which are the available budget $B_t$, the group to which the best-observed candidate belongs $g^*t$, and the sizes of the groups $(|G^k_t|){k \in [K]}$. Collectively, these parameters define the \textit{state} of a memory-less algorithm at step $t$, which entirely determines the success probability of the algorithm starting from that state.

\begin{lemma}\label{lem:memless}
For any memory-less algorithm $\A$ and $t \in [N]$, denoting by $\tau$ the stopping time of $\A$ and by $\F_{t-1}$ is the history of the algorithm up to step $t-1$, i.e. the set of all the observations and actions taken by the algorithm until step $t-1$, then 
\[
\Pr(\A \text{ succeeds} \mid \tau \geq t, g^*_t, \F_{t-1})
= \Pr(\A \text{ succeeds} \mid \tau \geq t, g^*_t, B_t, (|G^k_t|)_{k \in [K]})\;.
\]
\end{lemma}


\begin{proof}
Let $\A$ be a memory-less algorithm, and let us denote by $\tau$ its stopping time. Conditionally to the history of the algorithm until step $t-1$ and to the event $\{\tau \geq t\}$, the success probability of $\A$ depends on the future observations and the future actions of the algorithm.

Given that the algorithm is memory-less, at any step $s \geq t$, its actions $\act_{s,1}, \act_{s,2}$ depend on the observations $r_t, g_t, R_t$, the budget $B_t$ and $(|G^k_{s-1}|)_{k \in [K]}$. 

Conditionally to the cardinals of the groups at step $t-1$, the cardinals $(|G^k_{s-1}|)_{k \in [K]}$ are independent of the history $F_{t-1}$ because
\[
|G^k_{s-1}| = |G^k_{t-1}| + \sum_{u = t}^{s-1} \indic{g_u = k} \quad \forall k \in [K]\;,
\]
Moreover, since the candidates are observed in a uniformly random order, and the group memberships are also i.i.d random variables, then for all $s \geq 2$ distributions of $r_s, R_s$ depend only on the cardinals of each group at step $s-1$, on $g_s$ and $g^*_{s-1}$. Also $g^*_s$ is a function of $g^*_{s-1}, g_s$ and $\indic{R_s = 1}$:
\[
g^*_s = \indic{R_s \neq 1} g^*_{s-1} + \indic{R_s \neq 1} g_s\;,
\]
and the budget $B_s$ satisfies
\[
B_s = B_{s-1} - \indic{a_{s-1,2} = \acomp}\;.
\]

Therefore, Conditionally to the $B_t, (|G^k_{t-1}|)_{k \in [K]}, g^*_{t-1}$, 
the distributions of the observations and of the algorithm's actions at any step $s \geq t$ are independent of the history before step $t$.
\end{proof}



At any given step $t$, a memory-less algorithm has access to the available budget $B_t$ and the number of previous candidates belonging to each group. In the case of two groups, this information reduces to $(t, B_t, |G^1_{t-1}|)$, since $|G^2_{t-1}| = t-1 - |G^1_{t-1}|$.
The state of the algorithm, which fully determines its success probability, is given by the tuple $(t, B_t, |G^1_{t-1}|, g^*_{t-1})$. However, $g^*_{t-1}$ is not known to the algorithm, hence it must make decisions relying on the limited information it has, to maximize the expected success probability, where the expectation is taken over $g^*_{t-1}$.



\subsection{State transitions}\label{sec:state-transition}
For any memory-less algorithm $\A$, we denote by $\S_t(\A)$ its state at step $t$, which is a tuple $(t, b, m, \ell)$. Here, $t-1$ represents the count of previously rejected candidates, $b \geq 0$ denotes the available budget, $m < t$ indicates the number of prior candidates from group $G^1$, and $\ell \in \{1,2\}$ is the group containing the best-seen candidate so far.

To examine the state transitions of the algorithm, it is imperative to first understand the distribution of the new observations at any given step $t$, depending on $\S_t(\A)$. While the group membership $g_t$ of candidate $x_t$ is independent of $\S_t(\A)$, both $r_t$ and $R_t$ are contingent on it.



\begin{lemma}\label{lem:dynprog-rt-Rt}
For any memory-less algorithm $\A$ and state $(t,m,b,\ell)$, denoting by $k = 3-\ell$ the group index different from $\ell$, it holds that
\begin{align*}
&\Pr(r_t = 1 \mid \S_t(\A) = (t,m,b,\ell), g_t = \ell) 
= \frac{1}{t}\\
&\Pr(r_t = 1 \mid \S_t(\A) = (t,m,b,\ell), g_t = k) 
= \frac{|G^k_{t-1}|+t}{t(|G^k_{t-1}|+1)}\\
&\Pr(R_t = 1 \mid \S_t(\A) = (t,m,b,\ell), g_t = \ell, r_t = 1)
= 1\\
&\Pr(R_t = 1 \mid \S_t(\A) = (t,m,b,\ell), g_t = k, r_t=1) 
= \frac{|G^k_{t-1}|+1}{|G^k_{t-1}|+t}\;,
\end{align*}
where
\[
|G^k_{t-1}| = \left\{
    \begin{array}{ll}
        m & \mbox{if }\; k = 1 \\
        t-1-m & \mbox{if }\; k = 2 \;.
    \end{array}
\right.
\]
\end{lemma}

\begin{proof}
If $\S_t(\A) = (t,m,b,\ell)$, then in particular $g^*_{t-1} = \ell$, i.e. $\max G^\ell_{t-1} > \max G^k_{t-1}$, thus
\begin{align*}
\Pr(r_t = 1 \mid \S_t(\A) = (t,m,b,\ell), g_t = \ell) 
&= \Pr( x_t > \max G^\ell_{t-1} \mid \S_t(\A) = (t,m,b,\ell), g_t = \ell) \\
&= \Pr( x_t > \max x_{1:t-1} \mid \S_t(\A) = (t,m,b,\ell), g_t = \ell) \\
&= \frac{1}{t}\;,
\end{align*}
because the rank of $x_t$ among previous candidates is independent of their relative ranks and groups, thus independent of the state of the algorithm. Moreover, if $r_t = 1$, $g_t = 1$ and $g^*_{t-1} = \ell$, then $x_t$ is the better than the maximum of $G^\ell_{t-1}$, which is the maximum of $x_{1:t-1}$, thus necessarily $R_t = 1$,
\[
\Pr(R_t = 1 \mid \S_t(\A) = (t,m,b,\ell), g_t = \ell, r_t = 1) = 1\;.
\]

On the other hand, if $g_t = k \neq \ell = g^*_{t-1}$, assume that $|G^\ell_{t-1}| > 0$. It holds that
\begin{align*}
\Pr(r_t = 1 \mid \S_t(\A) = (t,m,b,\ell), g_t = k) 
&= \Pr( r_t = 1 \mid g^*_{t-1} = \ell, g_t = k, |G^1_{t-1}| = m) \\
&= \frac{\Pr( r_t = 1, g^*_{t-1} = \ell \mid g_t = k, |G^1_{t-1}| = m)}{\Pr(g^*_{t-1} = \ell \mid |G^1_{t-1}| = m)}\;.
\end{align*}
We have immediately that 
\[
\Pr(g^*_{t-1} = \ell \mid |G^1_{t-1}| = m)
= \Pr(\max G^\ell_{t-1} > \max G^k_{t-1} \mid |G^1_{t-1}| = m)
= \frac{|G^\ell_{t-1}|}{t-1}\;,
\]
and the numerator can be computed as
\begin{align}
\Pr( r_t = 1, g^*_{t-1} = \ell& \mid g_t = k, |G^1_{t-1}| = m)\\
&= \Pr( x_t > \max G^k_{t-1}, g^*_{t-1} = \ell\mid  |G^1_{t-1}| = m) \nonumber\\ 
&= \Pr(x_t > \max G^k_{t-1}, \max G^\ell_{t-1} > \max G^k_{t-1} \mid |G^1_{t-1}| = m)\nonumber \\
&= \Pr(x_t > \max G^\ell_{t-1} > \max G^k_{t-1} \mid |G^1_{t-1}| = m) \nonumber \\
&\quad + \Pr(\max G^\ell_{t-1} > x_t > \max G^k_{t-1} \mid |G^1_{t-1}| = m) \nonumber \\
&= \frac{1}{t}\cdot \frac{|G^\ell_{t-1}|}{t-1} + \frac{|G^\ell_{t-1}|}{t} \cdot \frac{1}{|G^k_{t-1}|+1} \nonumber \\
&= \frac{|G^\ell_{t-1}|}{t} \left( \frac{1}{t-1} + \frac{1}{|G^k_{t-1}| + 1} \right)\;, \label{eq:term(q)}
\end{align}
which yields
\begin{align*}
\Pr(r_t = 1 \mid \S_t(\A) = (t,m,b,\ell), g_t = k) 
&= \frac{t-1}{|G^\ell_{t-1}|} \cdot \frac{|G^\ell_{t-1}|}{t} \left( \frac{1}{t-1} + \frac{1}{|G^k_{t-1}| + 1} \right)\\
&= \frac{1}{t} \left( 1 + \frac{t-1}{|G^k_{t-1}| + 1} \right)\\
&= \frac{|G^k_{t-1}|+t}{t(|G^k_{t-1}|+1)}\;.
\end{align*}

Finally, 

\begin{align*}
\Pr(R_t = 1 \mid \S_t(\A) = (t,m,b,\ell), g_t = k, r_t = 1) 
&= \frac{\Pr(R_t = 1, r_t = 1, g^*_{t-1} = \ell \mid g_t = k, |G^1_{t-1}|) }{\Pr(r_t = 1, g^*_{t-1} = \ell \mid g_t = k, |G^1_{t-1}|)}\;.
\end{align*}
We computed the denominator term in \eqref{eq:term(q)}, and the numerator satisfies
\begin{align*}
\Pr(R_t = 1, r_t = 1, g^*_{t-1} = \ell \mid g_t = k, |G^1_{t-1}|)
&= \Pr(R_t = 1, g^*_{t-1} = \ell \mid g_t = k, |G^1_{t-1}|)\\
&= \Pr(x_t > \max G^\ell G^\ell_{t-1} > \max G^k_{t-1} \mid |G^1_{t-1}|)\\
&= \frac{1}{t} \cdot \frac{|G^\ell_{t-1}|}{t-1}\;,
\end{align*}
hence
\begin{align*}
\Pr(R_t = 1 \mid \S_t(\A) = (t,m,b,\ell), g_t = k, r_t = 1) 
&= \frac{ \frac{|G^\ell_{t-1}|}{t(t-1)} }{\frac{|G^\ell_{t-1}|}{t} \left( \frac{1}{t-1} + \frac{1}{|G^k_{t-1}| + 1} \right)}\\
&= \frac{1}{ 1 + \frac{t-1}{|G^k_{t-1}| + 1} }\\
&= \frac{|G^k_{t-1}| + 1}{|G^k_{t-1}| + t}\;.
\end{align*}
This concludes the proof when $|G^\ell_{t-1}| > 0$. If $|G^\ell_{t-1}| = 0$, then the same identities remain trivially true.
\end{proof}


Using the previous Lemma, we can fully characterize the possible state transitions of a memory-less algorithm.
First, the values of the parameters $|G^1_{t}|$ and $B_{t+1}$ are trivially determined based on the state $S_t$ at the beginning of step $t$, the observations $r_t$ and $g_t$, and the actions of the algorithm:
\[
|G^1_{t}| = |G^1_{t-1}| + \indic{g_t = 1}\;,
\quad B_{t+1} = B_t - \indic{\act_{t,1} = \acomp}\;,
\]
where $\act_{t,1}$ is the action taken by the algorithm, which only depends on the state $S_t$ since the algorithm is memory-less.

Regarding $g^*_t$, if $g_t = g^*_{t-1}$, then $g^*_t = g^*_{t-1}$ remains unchanged with probability $1$. However, if $g_t \neq g^*_{t-1}$ and $r_t = 1$, and if the algorithm skips the candidate without making a comparison, then $g^*_t$ is not deterministic based on the history alone. The probability that $g^*_t = g_t$ in this case is precisely the probability that $R_t = 1$, computed in Lemma \ref{lem:dynprog-rt-Rt}
\[
\Pr(g^*_t = g_t \mid \S_t(\A) = (t,m,b,\ell), g_t = k, r_t=1) 
= \frac{|G^k_{t-1}|+1}{|G^k_{t-1}|+t}\;.
\]





\subsection{Expected action rewards}\label{sec:expected-rwd}
In the following, we denote by $\A_*$ the optimal memory-less algorithm for two groups, and for all $B \geq 0$, $t \in [N]$, $m < t$ and $\ell \in \{1,2\}$, we denote by
\[
\V^B_{t,m,\ell} = \Pr\big(\A_* \text{ succeeds} \mid \tau \geq t, \S_t(\A_*) = (t,B,m,\ell) \big)\;,
\]
which is its success probability starting from state $(t,B,m,\ell)$.








We analyze the expected rewards and state transitions of algorithm $\A_*$ given its limited information access.
When the algorithm receives a new observation $(r_t, g_t)$:
\begin{itemize}
    \item If $r_t \neq 1$, the optimal action is to skip the candidate ($\askip$).
    \item If $r_t = 1$ and $B_t = 0$, the algorithm either stops or skips the candidate. However, if there is a positive budget $B_t$, stopping is suboptimal: it is always better to make a comparison first.
    \item If the algorithm chooses to make a comparison and observes $R_t$:
    \begin{itemize}
        \item If $R_t \neq 1$, the optimal action is to skip the candidate.
        \item If $R_t = 1$, the algorithm must decide whether to skip or stop. However, skipping after observing $R_t = 1$ is suboptimal compared to skipping immediately after observing $r_t = 1$, as the latter conserves the budget.
    \end{itemize}
\end{itemize}





In summary, any rational algorithm follows these decision rules:
\begin{itemize}
    \item If ($r_t \neq 1$) or ($r_t = 1$ and $R_t \neq 1$), then skip the candidate.
    \item If ($r_t = 1$ and $R_t = 1$), select the candidate.
\end{itemize}

Therefore, the main non-trivial decision to make is whether to reject or accept a candidate after observing $r_t = 1$.
Consider an algorithm $\A$ following these rules. At time $t$ with budget $B_t = b$ and $|G^1_{t-1}| = m$, if $g_t = k$ and $r_t = 1$, choosing an action $\act \in \{\askip, \astop, \acomp\}$ based on these rules leads to a new state $\S_{t+1}(\A) = F(t, b, m, k, \act)$, which is a random variable depending on $g^*_{t-1}$ and $R_t$. If $\act \in \{ \astop, \acomp\}$ and $R_t = 1$, then $\S_{t+1}(\A)$ is a final state: success or failure.



With this notation, we define $\rwd^B_{t,m}(\act)$ as the reward that $\A_*$ expects to gain by playing action $\act$ after observing $r_t = 1$ and $g_t = k$ in a state $S_t(\A_*) = (t,b,m,\cdot)$, where it ignores $g^*_{t-1}$
\[
\rwd^B_{t,m,k}(\act)
= \E[\Pr\big(\A_* \text{ succeeds} \mid \S_{t+1}(\A_*) = F(t,B,m,k,\act) \big) \mid S_t(\A_*) = (t,B,m,\cdot), r_t=1, g_t=k]\;.
\]
where the expectation is taken over $g^*_{t-1}$ and $R_t$.
The optimal memory-less action at any state $(t,B,m,\ell)$, knowing that $r_t=1, g_t=k$, is the one maximizing $\rwd^B_{t,m,k}(\act)$. 


\begin{lemma}\label{lem:action-reward}
Consider a state $S_t = (t,B,m,\cdot)$, and let $\{k, \ell\} = \{1,2\}$, $M_k = m + \indic{k = 1}$, then
\begin{align*}
&\rwd^B_{t,m,k}(\astop) = \frac{|G^k_t|}{N}\;,\\
&\rwd^B_{t,m,k}(\askip) = \frac{|G^1_t|}{t} \V^B_{t+1,M_k,1} + \frac{|G^2_t|}{t} \V^B_{t+1,M,2}\;,\\
&\rwd^B_{t,m,k}(\acomp) = \frac{|G^k_t|}{N} + \frac{|G^\ell_t|}{t}\left( \frac{|G^k_{t-1}|+1}{|G^k_{t-1}|+t}\cdot \frac{t}{N} + \frac{t-1}{|G^k_{t-1}|+t} \V^{B-1}_{t+1,M_k,\ell} \right)\;,
\end{align*}
\end{lemma}

Observe that, conditionally to $g_t$ and $|G^1_{t-1}|$, the cardinals of $G^1_{t-1}, G^2_{t-1}, G^1_{t}, G^2_{t}$ are all known: 
\[
|G^1_{t}| = |G^1_{t-1}| + \indic{g_t=1}\;,
\quad |G^2_{t}| = t - |G^1_{t}|, 
\quad |G^2_{t-1}| = t - 1 - |G^1_{t-1}|\;. 
\]



\subsection{Optimal actions and success probability}
Using Lemma \ref{lem:action-reward} and considering the potential state transitions based on the actions, we establish a recursion satisfied by $(\V^B_{t,m,\ell})_{t,B,m,\ell}$. We present the result without distinction between the cases $\ell = 1$ and $\ell = 2$. For simplicity, let $\lambda_k = \Pr(g_t = k)$ for $k = 1,2$, and define $M_k = m + \indic{k=1}$ for all $m \geq 0$. Additionally, for all $(B,t,m,k)$, define
\[
\delta^B_k = \indic{\rwd^B_{t,m,k}(\accept) \geq \rwd^B_{t,m,k}(\askip)}\;,
\]
where the action $\accept$ corresponds to $\acomp$ for $B>0$ and $\astop$ for $B=0$.

\begin{theorem}\label{thm:opt-memless}
For all $t \in [N]$, $m < t$ and $\{k,\ell\} = \{1,2\}$, the success probability of $\A_*$ with zero budget satisfies the recursion
\begin{align*}
\V^0_{t,m,\ell}
&= \lambda_\ell \left(\tfrac{\delta^0_\ell}{N} + \left(1 - \tfrac{\delta^0_\ell}{t}\right)\V^0_{t+1,M_\ell,\ell} \right) \\
&\quad+ \lambda_k \left( \tfrac{\delta^0_k}{N} + \tfrac{1-\delta^0_k}{t} \V^0_{t+1,M_k,k} + \left(1 - \tfrac{1}{t}\right)\left(2 - \delta^0_k - \tfrac{1}{|G^k_{t-1}|+1} \right) \V^0_{t+1,M_k,\ell} \right)\;,
\end{align*}
and for $B \geq 1$ it satisfies
\begin{align*}
\V^B_{t,m,\ell}
&= \lambda_\ell \left( \tfrac{\delta^B_\ell}{N} + \big( 1 - \tfrac{\delta^B_\ell}{t} \big) \V^B_{t+1,M_\ell,\ell} \right) \\
&\quad+ \lambda_k \left( \tfrac{\delta^B_k}{N} + \tfrac{\delta^B_k}{|G^k_{t-1}|+1}\big( 1 - \tfrac{1}{t}\big) \V^{B-1}_{t+1,M_k,\ell} + \tfrac{1-\delta^B_k}{t} \V^{B}_{t+1,M_k,k} + \big(1-\tfrac{1}{t}\big)\big( 1 - \tfrac{\delta^B_k}{|G^k_{t-1}|+1}\big)  \V^{B}_{t+1,M_k,\ell}  \right)\;,
\end{align*}
where $\V^B_{N+1,m,k} = 0$ for all $B \geq 0$ $m \leq N$ and $k \in \{1,2\}$.
\end{theorem}


\begin{proof}
Using the results from Section \ref{sec:state-transition} and \ref{sec:expected-rwd}, the actions of $\A_*$ and the resulting state transitions are as follows.
If the state of $\A_*$ at step $t$ is $\S_t(\A_*) = (t,B,m,\ell)$ for some $B \geq 1$, $m<t$ and $\ell \in \{1,2\}$:
If $g_t = \ell$, denoting by $M_\ell = m + \indic{\ell = 1}$, we have
\begin{itemize}
    \item with probability $1-1/t$: $r_t = 0$, and the algorithm rejects the candidate, transitioning to the state $(t+1,B,M_\ell,\ell)$.
    \item with probability $1/t$: $r_t = 1$, and necessarily $R_t = 1$, because $g_t = g^*_{t-1} = \ell$. 
    \begin{itemize}
        \item If $\rwd^B_{t,m,k}(\acomp) > \rwd^B_{t,m,k}(\askip)$, then the algorithm uses a comparison and observes $R_t = 1$, hence accepts the candidate. The success probability in that case is $t/N$.
        \item Otherwise, the candidate is rejected and the algorithm goes to state $(t+1,B,M_\ell,\ell)$
    \end{itemize}
\end{itemize}
On the other hand, if $g_t = k \neq g^*_{t-1}$, then denoting by $M_k = m + \indic{k = 1}$, we have
\begin{itemize}
    \item with probability $\frac{|G^k_{t-1}|(t+1)}{t(|G^k_{t-1}|+1)}$: $r_t = 0$, and the algorithm rejects the candidate, transitioning to the state $(t+1,B,M_k,\ell)$.
    \item with probability $\frac{|G^k_{t-1}| + t}{t(|G^k_{t-1}|+1)}$: $r_t = 1$
    \begin{itemize}
        \item If $\rwd^B_{t,m,k}(\acomp) > \rwd^B_{t,m,k}(\askip)$, then the algorithm uses a comparison
        \begin{itemize}
            \item with probability $\frac{|G^k_{t-1}| + 1}{|G^k_{t-1}|+t}$: $R_t = 1$ and the algorithm stops, its success probability is $t/N$
            \item with probability $\frac{t- 1}{|G^k_{t-1}|+t}$: $R_t = 0$, the candidate is rejected, and the algorithm goes to state $(t+1,B-1,M_k,\ell)$
        \end{itemize}
        \item Otherwise, the candidate is rejected and 
        \begin{itemize}
            \item with probability $\frac{|G^k_{t-1}| + 1}{|G^k_{t-1}|+t}$: the algorithm goes to state $(t+1,B, M_k,k)$
            \item with probability $\frac{t- 1}{|G^k_{t-1}|+t}$: the algorithm goes to state $(t+1,B, M_k,\ell)$
        \end{itemize}
    \end{itemize}
\end{itemize}

In the case of a zero budget, the algorithm compares $\rwd^B_{t,m,k}(\askip)$ to $\rwd^B_{t,m,k}(\acomp)$ instead of $\rwd^B_{t,m,k}(\acomp)$. If the algorithm decides to reject the candidate then the same state transition occurs. However, if the candidate is selected and if $g_t = g^*_{t-1} = \ell$ then the success probability is $t/N$. If On the other hand, if it is selected and $g_t = k \neq g^*_{t-1} = \ell$ then the probability that the current candidate is the best overall is $\frac{|G^k_{t-1}| + 1}{|G^k_{t-1}|+t} \times \frac{t}{N}$.

All in all, for $B = 0$, then
\begin{align*}
(\V^0_{t,m,\ell} \mid g_t = \ell)
&= \frac{1}{t}\left( \delta^0_\ell \frac{t}{N} + (1 - \delta^0_\ell) \V^0_{t+1,M_\ell,\ell}  \right) + \left(1 - \frac{1}{t} \right) \V^0_{t+1,M_\ell,\ell}\\
&= \left(1 - \tfrac{\delta^0_\ell}{t}\right)\V^0_{t+1,M_\ell,\ell} + \tfrac{\delta^0_\ell}{N}\\
(\V^0_{t,m,\ell} \mid g_t = k)
&= \tfrac{|G^k_{t-1}| + t}{t(|G^k_{t-1}|+1)}\left( \delta^0_k \tfrac{t(|G^k_{t-1}| + 1)}{N(|G^k_{t-1}|+t)} + (1-\delta^0_k) \left( \tfrac{|G^k_{t-1}| + 1}{|G^k_{t-1}|+t} \V^0_{t+1,M_\ell,l} + \tfrac{t- 1}{|G^k_{t-1}|+t} \V^0_{t+1,M_\ell,\ell} \right) \right)\\
&\quad + \tfrac{|G^k_{t-1}| + t}{t(|G^k_{t-1}|+1)}\V^0_{t+1,M_k,\ell}\\
&=  \tfrac{\delta^0_k}{N} + \tfrac{1-\delta^0_k}{t} \V^0_{t+1,M_k,k} + \left(1 - \tfrac{1}{t}\right)\left(2 - \delta^0_k - \tfrac{1}{|G^k_{t-1}|+1} \right) \V^0_{t+1,M_k,\ell}\;,
\end{align*}
and we deduce that 
\begin{align*}
\V^0_{t,m,\ell}
&= \lambda_\ell \left( \left(1 - \tfrac{\delta^0_\ell}{t}\right)\V^0_{t+1,M_\ell,\ell} + \tfrac{\delta^0_\ell}{N} \right) \\
&\quad+ \lambda_k \left( \tfrac{\delta^0_k}{N} + \tfrac{1-\delta^0_k}{t} \V^0_{t+1,M_k,k} + \left(1 - \tfrac{1}{t}\right)\left(2 - \delta^0_k - \tfrac{1}{|G^k_{t-1}|+1} \right) \V^0_{t+1,M_k,\ell} \right)\;.
\end{align*}

For $B \geq 1$, we obtain
\begin{align*}
(\V^B_{t,m,\ell} \mid g_t = \ell)
&= \frac{1}{t}\left( \delta^B_\ell \frac{t}{N} + (1 - \delta^B_\ell) \V^B_{t+1,M_\ell,\ell}  \right) + \left(1 - \frac{1}{t} \right) \V^B_{t+1,M_\ell,\ell}\\
&= \left(1 - \tfrac{\delta^B_\ell}{t}\right)\V^B_{t+1,M_\ell,\ell} + \tfrac{\delta^B_\ell}{N}\\
(\V^B_{t,m,\ell} \mid g_t = k)
&= \tfrac{|G^k_{t-1}| + t}{t(|G^k_{t-1}|+1)}\bigg[ \delta^B_k \left( \tfrac{|G^k_{t-1}| + 1}{|G^k_{t-1}|+t} \V^B_{t+1,M_k,k} + \tfrac{t - 1}{|G^k_{t-1}|+t} \V^B_{t+1,M_k,\ell} \right) \\
&\quad + (1-\delta^B_k) \left( \tfrac{|G^k_{t-1}| + 1}{|G^k_{t-1}|+t} \V^0_{t+1,M_k,l} + \tfrac{t- 1}{|G^k_{t-1}|+t} \V^0_{t+1,M_k,\ell} \right) \bigg] + \tfrac{|G^k_{t-1}| + t}{t(|G^k_{t-1}|+1)}\V^B_{t+1,M_k,\ell}\\
&=  \tfrac{\delta^B_k}{N} + \tfrac{\delta^B_k}{|G^k_{t-1}|+1}\big( 1 - \tfrac{1}{t}\big) \V^{B-1}_{t+1,M_k,\ell} + \tfrac{1-\delta^B_k}{t} \V^{B}_{t+1,M_k,k} + \big(1-\tfrac{1}{t}\big)\big( 1 - \tfrac{\delta^B_k}{|G^k_{t-1}|+1}\big)  \V^{B}_{t+1,M_k,\ell}\;,
\end{align*}
hence
\begin{align*}
\V^B_{t,m,\ell}
&= \lambda_\ell \left( \tfrac{\delta^B_\ell}{N} + \big( 1 - \tfrac{\delta^B_\ell}{t} \big) \V^B_{t+1,M_\ell,\ell} \right) \\
&\quad+ \lambda_k \left( \tfrac{\delta^B_k}{N} + \tfrac{\delta^B_k}{|G^k_{t-1}|+1}\big( 1 - \tfrac{1}{t}\big) \V^{B-1}_{t+1,M_k,\ell} + \tfrac{1-\delta^B_k}{t} \V^{B}_{t+1,M_k,k} + \big(1-\tfrac{1}{t}\big)\big( 1 - \tfrac{\delta^B_k}{|G^k_{t-1}|+1}\big)  \V^{B}_{t+1,M_k,\ell}  \right)\;,
\end{align*}
which concludes the proof.
\end{proof}




Implementing the optimal memory-less algorithm $\A_*$ with budget $B$ requires knowing the 
$(\rwd^b_{t,m,k}(\act))_{t,b,m,k}$ for $\act \in \{\askip, \astop, \acomp\}$, which depend themselves on the table $\big(\V^b_{t,m,k}\big)_{t,b,m,k}$.
Using Lemma \ref{lem:action-reward} and Theorem \ref{thm:opt-memless}, these tables can be computed in a $O(B N^2)$ time as described in Algorithm \ref{algo:computeTables}.


\begin{algorithm}[h!]
\DontPrintSemicolon 
\caption{ $(\V^b_{t,m,k})_{t,b,m,k}$ and $(\rwd^b_{t,m,k}(\act))_{t,b,m,k}$ for $\act \in \{\askip, \astop, \acomp\}$}\label{algo:computeTables}
\SetKwInput{Input}{Input}
   \SetKwInOut{Output}{Output}
   \SetKwInput{Initialization}{Initialization}
   \Input{Number of candidates $N$, available budget $B$, probability distribution of $g_t$: $\lambda_1, \lambda_2$}
   \Initialization{$\V^b_{N+1,m,k} \gets 0$ for all $b \leq B , m \leq N, k \in \{1,2\} $}
\For{$b = 1, \ldots, B$}{
    \For{$t=N, N-1,\ldots,1$}{
        \For{$m = 0, \ldots, t$}{
            Compute $\rwd^b_{t,m,k}(\act)$ for $k \in \{1,2\}$ and $\act \in \{\askip, \astop, \acomp\}$ using Lemma \ref{lem:action-reward}\;
            Compute $\V^b_{t,m,k}$ for $k \in \{1,2\}$ using Theorem \ref{thm:opt-memless}\;
        }
    }
}
Return: $(\V^b_{t,m,k})_{t,b,m,k}$, $(\rwd^b_{t,m,k}(\act))_{t,b,m,k}$\;
\end{algorithm}



After computing these tables, the optimal memory-less algorithm $\A_*$ can be implemented by following the rational decision rules outlined in Section \ref{sec:expected-rwd}, and when encountering $r_t = 1$ and needing to choose between accepting or rejecting the candidate, $\A_*$ selects the action that maximizes its expected reward given the information it has about the current state. A detailed description is provided in Algorithm \ref{algo:opt-memless}.

\begin{algorithm}[h!]
\DontPrintSemicolon 
\caption{Optimal memory-less algorithm $\A_*$}\label{algo:opt-memless}
\SetKwInput{Input}{Input}
   \SetKwInOut{Output}{Output}
   \SetKwInput{Initialization}{Initialization}
   \Input{Number of candidates $N$, available budget $B$, probability distribution of $g_t$: $\lambda_1, \lambda_2$}
   \Initialization{$b \gets B$, $m \gets 0$}
   Compute $(\V^b_{t,m,k})_{t,b,m,k}$ and $(\rwd^b_{t,m,k}(\act))_{t,b,m,k,\act}$ using Algorithm \ref{algo:computeTables}\;
\For{$t = 1,\ldots, N$}{
    Receive new observation $(r_t, g_t)$\;
    \If{$r_t = 1$}{
        \If{$b=0$ and $\rwd^0_{t,m,g_t}(\astop) > \rwd^0_{t,m,g_t}(\askip)$}{
        Return: $t$\;
        }
        \If{$b>0$ and $\rwd^b_{t,m,g_t}(\acomp) > \rwd^b_{t,m,g_t}(\askip)$}{
            $b \gets b - 1$\;
            \If{$R_t = 1$}{Return: t}
        }
    }
    $m \gets m + \indic{g_t = 1}$\;
}
\end{algorithm}



\vspace*{10cm}