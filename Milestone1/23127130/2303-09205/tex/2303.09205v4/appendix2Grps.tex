\section{Static Double-threshold algorithm for two groups}


\subsection{Recursion lemma}
We first prove a recursion satisfied by $\U^B_{N,t,k}$ \eqref{eq:def-U}, which we use to prove the subsequent results in this section.

\begin{lemma}\label{lem:recursion-U-alpha-beta}
For all $B \geq 0$, $t \in \{\lfloor \alpha N \rfloor, \ldots, \lfloor \beta N \rfloor - 1\}$, and $k \in \{1,2\}$,  $\U^B_{N,t,k}$ satisfies
\begin{align*}
\U^B_{N,t,k} 
&=   \frac{\lambda}{N} \sum_{s = t}^{\lfloor \beta N \rfloor-1} \Pr(\rho_1 \geq s, g^*_{t-1} = k \mid \C_N)\\
& \quad +  \frac{\indic{B>0}}{1-\lambda} \sum_{s = t}^{\lfloor \beta N \rfloor-1} \Pr(g^*_{t-1} = k, \rho_1 = s, R_s \neq 1 \mid \C_N) \U^{B-1}_{N,s+1,2} \\
&\quad + \Pr(\A^B_t(\alpha, \beta) \text{ succeeds},  g^*_{t-1} = k, \rho_1 \geq \beta N \mid \C_N) + O\big(\tfrac{1}{N}\big)\;.\\
\end{align*}
\end{lemma}

Assuming that $\rho_1 = s \in \{t, \ldots, \lfloor \beta N\rfloor -1 \}$, the first sum corresponds to the success probability if $R_s = 1$ and the algorithm selects the candidate $x_s$. The terms of the second sum represent the success probability after using a comparison at step $s$ but observing $R_t \neq 1$, resulting in the rejection of the candidate. Therefore, the available budget at step $s+1$ is $B-1$, and necessarily $g^*_s = 2$, because a comparison at step $s$ can only occur if $g_s = 1$ by definition of the algorithm. Hence, only the term $\U^{B-1}_{N,s+1,2}$ appears in the recursion, not $\U^{B-1}_{N,s+1,1}$. Finally, the last term represents the probability of success if no comparison has been made before step $\lfloor \beta N \rfloor$


\begin{proof}
Let $B \geq 0$. For all $t  \in \{\lfloor \alpha N \rfloor, \ldots,\lfloor\beta N \rfloor - 1)$ and $k \in \{1,2\}$, it holds that
\begin{align}
\U^B_{N,t,k} 
&= \Pr(\A^B_t(\alpha, \beta) \text{ succeeds},  g^*_{t-1} = k \mid \C_N)\nonumber\\
&= \sum_{s = t}^{\lfloor \beta N \rfloor-1} \Pr(\A^B_t(\alpha, \beta) \text{ succeeds},  g^*_{t-1} = k, \rho_1 = s \mid \C_N) \nonumber \\
&\qquad + \Pr(\A^B_t(\alpha, \beta) \text{ succeeds},  g^*_{t-1} = k, \rho_1 \geq \beta N \mid \C_N) \nonumber\\
&= \sum_{s = t}^{\lfloor \beta N \rfloor-1} \Pr(\A^B_t(\alpha, \beta) \text{ succeeds},  g^*_{t-1} = k, \rho_1 = s, R_s = 1 \mid \C_N) \label{eqref:decom-U-1}\\
& \quad + \sum_{s = t}^{\lfloor \beta N \rfloor-1} \Pr(\A^B_t(\alpha, \beta) \text{ succeeds},  g^*_{t-1} = k, \rho_1 = s, R_s \neq 1 \mid \C_N) \label{eqref:decom-U-2} \\
&\quad + \Pr(\A^B_t(\alpha, \beta) \text{ succeeds},  g^*_{t-1} = k, \rho_1 \geq \beta N \mid \C_N)\;. \label{eqref:decom-U-3}
\end{align}

For all $s \in \{t,\ldots, \lfloor \beta N \rfloor -1\}$, by definition of Algorithm $\A^B_t(\alpha, \beta)$, if $\rho_1 = s$ and $R_s = 1$ then the candidate $x_s$ is selected, and the algorithm succeeds if only if $x_s = \xmax$. Moreover, the event $\{\rho_1 = s\}$ is equivalent to $\{\rho_1 \geq s, g_s = 1, r_s = 1\}$, hence the terms in \eqref{eqref:decom-U-1} can be written as
\begin{align*}
\Pr(\A^B_t(\alpha, \beta) \text{ succeeds},  g^*_{t-1} = k, &\rho_1 = s, R_s = 1 \mid \C_N)\\
&= \Pr(x_s = \xmax,  g^*_{t-1} = k, \rho_1 = s, R_s = 1 \mid \C_N)\\
&= \Pr(x_s = \xmax,  g^*_{t-1} = k, \rho_1 \geq s, g_s = 1 \mid \C_N)\\
&= \frac{\Pr(g_s = 1 \mid \C_N)}{N} \Pr(\rho_1 \geq s, g^*_{t-1} = k \mid \C_N)\\
&= \left( \frac{\lambda}{N} + O\big(\tfrac{1}{N^3}\big)\right) \Pr(\rho_1 \geq s, g^*_{t-1} = k \mid \C_N)\;,
\end{align*}
where used for that the event $\{x_s = \xmax\}$ is independent of the group memberships, thus independent of $\C_N$, and that it is also independent of $\{\rho_1 \geq s, g^*_{t-1} = k\}$, because a realization of the latter event is determined only by the groups and relative ranks of the candidates $\{x_1,\ldots,x_{s-1}\}$. For the last equality, we used Lemma \ref{lem:pr-cond-C_N}.

Secondly, in the case where $\rho_1 = s$ and $R_s \neq 1$, if $B = 0$ then the algorithm selects candidate $x_s$ which is not the best overall, hence its probability of succeeding is zero. If $B \geq 1$, the algorithm makes a comparison but then rejects the candidate. Moreover, for $s \in [t,\beta N]$, if $\rho_1 = s$ then necessarily $g_s = 1$, and having $R_s \neq 1$ implies that $g^*_s = 2$.  The success probability of $\A^B_t(\alpha, \beta)$ given that $\rho_1 = s, R_s \neq 1$ is the same as the success probability of $\A^{B-1}_{s+1}(\alpha, \beta)$ given that $g^*_{s} = 2$. Therefore, the terms of \eqref{eqref:decom-U-2} satisfy
\begin{align*}
\Pr&(\A^B_t(\alpha, \beta) \text{ succeeds},  g^*_{t-1} = k, \rho_1 = s, R_s \neq 1 \mid \C_N) \\
&= \indic{B>0} \Pr(g^*_{t-1} = k, \rho_1 = s, R_s \neq 1 \mid \C_N) \Pr(\A^B_t(\alpha, \beta) \text{ succeeds} \mid g^*_{t-1} = k, \rho_1 = s, R_s \neq 1, \C_N)\\
&= \indic{B>0}\Pr(g^*_{t-1} = k, \rho_1 = s, R_s \neq 1 \mid \C_N) \Pr(\A^{B-1}_{s+1}(\alpha, \beta) \text{ succeeds} \mid g^*_{s} = 2, \C_N)\\
&= \indic{B>0}\Pr(g^*_{t-1} = k, \rho_1 = s, R_s \neq 1 \mid \C_N) \frac{\Pr(\A^{B-1}_{s+1}(\alpha, \beta) \text{ succeeds}, g^*_{s} = 2 \mid \C_N)}{\Pr(g^*_s = 2 \mid \C_N)}\\
&= \indic{B>0}\Pr(g^*_{t-1} = k, \rho_1 = s, R_s \neq 1 \mid \C_N) \frac{\U^{B-1}_{N,s+1,2}}{1-\lambda + O( \tfrac{1}{N^2})}\;,
\end{align*}
where we used again Lemma \ref{lem:pr-cond-C_N}. Given that the $O$ terms are independent of $s$, We deduce that 
\begin{align*}
\U^B_{N,t,k} 
&=  \left( \frac{\lambda}{N} + O\big(\tfrac{1}{N^3}\big)\right) \sum_{s = t}^{\lfloor \beta N \rfloor-1} \Pr(\rho_1 \geq s, g^*_{t-1} = k \mid \C_N)\\
& \quad + \indic{B>0} \Big(\frac{1}{1-\lambda} + O\big( \tfrac{1}{N^2}\big) \Big) \sum_{s = t}^{\lfloor \beta N \rfloor-1} \Pr(g^*_{t-1} = k, \rho_1 = s, R_s \neq 1 \mid \C_N) \U^{B-1}_{N,s+1,2} \\
&\quad + \Pr(\A^B_t(\alpha, \beta) \text{ succeeds},  g^*_{t-1} = k, \rho_1 \geq \beta N \mid \C_N)\\
&=   \frac{\lambda}{N} \sum_{s = t}^{\lfloor \beta N \rfloor-1} \Pr(\rho_1 \geq s, g^*_{t-1} = k \mid \C_N)\\
& \quad + \frac{\indic{B>0}}{1-\lambda} \sum_{s = t}^{\lfloor \beta N \rfloor-1} \Pr(g^*_{t-1} = k, \rho_1 = s, R_s \neq 1 \mid \C_N) \U^{B-1}_{N,s+1,2} \\
&\quad + \Pr(\A^B_t(\alpha, \beta) \text{ succeeds},  g^*_{t-1} = k, \rho_1 \geq \beta N \mid \C_N) + O\big(\tfrac{1}{N}\big)\;.
\end{align*}

\end{proof}






\subsection{Additional lemmas}

In the following two lemmas, we compute the probabilities appearing in Lemma \ref{lem:recursion-U-alpha-beta} for all $s \in \{t,\ldots, \lfloor \beta N \rfloor -1$ and $k\in \{1,2\}$

\begin{lemma}\label{lem:2grp-rho>s}
Let $\lfloor \alpha N \rfloor \leq t \leq s < \lfloor \beta N \rfloor$, and consider a run of Algorithm $\A_t^B(\alpha, \beta)$, then it holds that
\begin{align*}
\Pr(\rho_1 \geq s , g^*_{t-1} = 1 \mid \C_N)
&= \frac{\lambda t}{(1-\lambda)t + \lambda s} + O\Big( \sqrt{\tfrac{\log N}{N}} \Big)\\
\Pr(\rho_1 \geq s, g^*_{t-1} = 2 \mid \C_N)
&= \frac{(1-\lambda)t^2}{((1-\lambda)t + \lambda s)s} + O\Big( \sqrt{\tfrac{\log N}{N}} \Big)\;.
\end{align*}
\end{lemma}


\begin{proof}
Since there are only $2$ groups, the event $g^*_{t-1} = 1$ is equivalent to $\max G^2_{t-1} < \max G^1_{t-1}$.
For $s \in \{t, \ldots, \lfloor \beta N \rfloor\}$, Algorithm $\A_t^B(\alpha, \beta)$ only makes a comparison (or stops in the case of $B=0$) at step $s$ only if $g_s = 1$ and $r_s = 1$. Therefore, $\rho_1 \geq s$ if and only if no candidate belonging to $G^1_{t:s-1}$ surpasses the maximum value observed in $G^1_{t-1}$
\begin{align*}
\Pr(\rho_1 \geq s, g^*_{t-1} = 1 \mid \C_N)
&=\Pr(\max G^1_{t:s-1} < G^1_{t-1},  \max G^2_{t-1} < \max G^1_{t-1} \mid \C_N)\\
&= \Pr(\max (G^1_{t:s-1} \cup G^2_{t-1}) < G^1_{t-1}\mid \C_N)\\
&= \E\left[ \frac{|G^1_{t-1}|}{|G^1_{t-1}|+|G^1_{t:s-1}|+|G^2_{t-1}|} \; \Big| \; \C_N \right]\\
&= \E\left[ \frac{|G^1_{t-1}|}{t-1 +|G^1_{t:s-1}|} \; \Big| \; \C_N \right]\\
&= \frac{\lambda t + O(\sqrt{N\log N})}{t + \lambda(s-t) + O(\sqrt{N\log N})}\\
&= \frac{\lambda t}{(1-\lambda)t + \lambda s} + O\Big( \sqrt{\tfrac{\log N}{N}} \Big)\;.
\end{align*}
For the case $g_t^* = 2$, we obtain
\begin{align*}
\Pr(\rho_1 \geq s, g^*_{t-1} = 2 \mid \C_N)
&=\Pr(\max G^1_{t:s-1} < G^1_{t-1},  \max G^1_{t-1} < \max G^2_{t-1} \mid \C_N)\\
&= \max G^1_{t:s-1} < G^1_{t-1} < \max G^2_{t-1} \mid \C_N)\\
&= \E\left[ \frac{|G^2_{t-1}|}{|G^1_{t-1}|+|G^1_{t:s-1}|+|G^2_{t-1}|} \cdot \frac{|G^1_{t-1}|}{|G^1_{t:s-1}| + |G^1_{t-1}|} \; \Big| \; \C_N \right]\\
&= \E\left[ \frac{|G^2_{t-1}|}{t-1 +|G^1_{t:s-1}|} \cdot \frac{|G^1_{t-1}|}{|G^1_{s-1}|} \; \Big| \; \C_N \right]\\
&= \frac{(1-\lambda)t + O(\sqrt{N\log N})}{t+\lambda(s-t) + O(\sqrt{N\log N})} \cdot \frac{\lambda t + O(\sqrt{N\log N})}{\lambda s + O(\sqrt{N\log N})}\\
&= \frac{(1-\lambda)t^2}{((1-\lambda)t + \lambda s)s} + O\Big( \sqrt{\tfrac{\log N}{N}} \Big)\;.
\end{align*}
\end{proof}





\begin{lemma}\label{lem:2grp-rho=s}
Let $\lfloor \alpha N \rfloor \leq t \leq s < \lfloor \beta N \rfloor$, and consider a run of Algorithm $\A_t^B(\alpha, \beta)$, then 
\begin{align*}
\Pr(\rho_1 = s, R_s \neq 1, g^*_{t-1} = 1 \mid \C_N)
&= \frac{\lambda^2(1-\lambda)(s-t)t}{((1-\lambda)t + \lambda s)^2s} + O\Big( \sqrt{\tfrac{\log N}{N^3}} \Big)\\
\Pr(\rho_1 = s, R_s \neq 1, g^*_{t-1} = 2 \mid \C_N)
&= \frac{(1-\lambda)\left( (1-\lambda)^2 t + \lambda(2-\lambda)s\right)t^2}{((1-\lambda)t+\lambda s)^2s^2} + O\Big( \sqrt{\tfrac{\log N}{N^3}} \Big)\;.
\end{align*}
\end{lemma}

\begin{proof}
For Algorithm $\A_t^B(\alpha, \beta)$ and $s \in \{t, \ldots, \lfloor \beta N \rfloor-1\}$, $\rho_1 = s$ if and only if $x_s$ is the first element in $G^1$ since step $t$ for which $r_s = 1$, thus
\[
\rho_1 = s \iff g_s = 1 \text{ and } \max G^1_{t:s-1} < \max G^1_{t-1} < x_s\;.
\]
Furthermore, Lemma \ref{lem:pr-cond-C_N} gives that $\Pr(g_s = 1 \mid \C_N) = \Pr(g_s = 1) + O(1/N) = \lambda + O(1/N)$. Therefore, it holds that
\begin{align*}
\Pr(\rho_1 &= s, R_s \neq 1, g^*_{t-1} = 1 \mid \C_N)\\
&= \Pr( g_s = 1 \; , \; \max G^1_{t:s-1} < \max G^1_{t-1} < x_s \; , \; x_s < \max G^2_{s-1} \; , \; \max G^2_{t-1} < \max G^1_{t-1} \mid \C_N)\\
&= \Pr( g_s = 1 \mid C_N) \Pr( \max G^1_{t:s-1} < \max G^1_{t-1} < x_s < \max G^2_{t:s-1}, \max G^2_{t-1} < \max G^1_{t-1} \mid \C_N)\\
&= \Pr( g_s = 1 \mid C_N) \Pr( \max (G^1_{t:s-1} \cup  G^2_{t-1}) < \max G^1_{t-1} < x_s < \max G^2_{t:s-1} \mid \C_N)\\
&= (\lambda + O(\tfrac{1}{N}))\E\Big[ \tfrac{|G^2_{t:s-1}|}{|G^1_{t:s-1}|+|G^2_{t-1}|+|G^1_{t-1}|+1+ |G^2_{t:s-1}|} \cdot \tfrac{1}{|G^1_{t:s-1}|+|G^2_{t-1}|+|G^1_{t-1}|+1} 
\cdot \tfrac{|G^1_{t-1}|}{|G^1_{t:s-1}|+|G^2_{t-1}|+|G^1_{t-1}|} \; \Big| \; \C_N \Big]\\
&= (\lambda + O(\tfrac{1}{N}))\E\left[ \frac{|G^2_{t:s-1}|}{s}\cdot \frac{1}{t + |G^1_{t:s-1}|}\cdot \frac{ |G^1_{t-1}|}{t-1 +  |G^1_{t:s-1}|} \; \Big| \; \C_N \right]\\
&= (\lambda + O(\tfrac{1}{N}))\frac{(1-\lambda)(s-t) + O(\sqrt{N\log N})}{s(t+\lambda(s-t) + O(\sqrt{N\log N}))}\cdot \frac{\lambda t + O(\sqrt{N\log N})}{t + \lambda(s-t) + O(\sqrt{N\log N})}\\
&= \frac{\lambda^2(1-\lambda)(s-t)t}{((1-\lambda)t + \lambda s)^2s} + O\Big( \sqrt{\tfrac{\log N}{N^3}} \Big)\;.
\end{align*}
On the other hand, in the case where $g^*_{t-1} = 2$, we obtain
\begin{align*}
\Pr(\rho_1 &= s, R_s \neq 1, g^*_{t-1} = 2 \mid \C_N)\\
&= \Pr( g_s = 1 \; , \; \max G^1_{t:s-1} < \max G^1_{t-1} < x_s \; , \; x_s < \max G^2_{s-1} \; , \; \max G^1_{t-1} < \max G^2_{t-1} \mid \C_N)\\
&= \Pr( g_s = 1 \mid C_N) \Pr( \max G^1_{t:s-1} < \max G^1_{t-1} < x_s < \max G^2_{s-1}\; , \; \max G^1_{t-1} < \max G^2_{t-1} \mid \C_N)\\
&= \Pr( g_s = 1 \mid C_N) \Pr( a < b < x_s < \max(c,d)\; , \; b < c \mid \C_N)\;,
\end{align*}
where $a =  \max G^1_{t:s-1}$, $b = \max G^1_{t-1}$, $c = \max G^2_{t-1}$ and $d = \max G^2_{t:s-1}$. Let us denote by $\mathcal{E}$ the event $\{a < b < x_s < \max(c,d)\} \cap \{b < c\}$. It holds that
\begin{align*}
\mathcal{E} \cap \{c<d\}
&= \{a < b < x_s < \max(c,d)\} \cap \{b < c\} \cap \{c<d\}\\
&= \{ a<b<c<x_s<d \} \cup \{ a<b<x_s<c<d \}\\
&= \{ a<b<c<x_s<d \} \cup \big(\{ a<b<x_s<c \} \cap \{ c < d\}\big)\;,\\
\mathcal{E} \cap \{d<c\}
&= \{a < b < x_s < \max(c,d)\} \cap \{b < c\} \cap \{d<c\}\\
&= \{a < b < x_s < c\} \cap \{d<c\}\;,
\end{align*}
which yields
\begin{align*}
\mathcal{E} 
&= \big( \mathcal{E} \cap \{c<d\} \big) \big( \mathcal{E} \cap \{d<c\} \big)\\
&= \{ a<b<c<x_s<d \} \cup \big(\{ a<b<x_s<c \} \cap \{ c < d\}\big) \cup \big(\{ a<b<x_s<c \} \cap \{d<c\}\big)\\
&= \{ a<b<c<x_s<d \} \cup \{ a<b<x_s<c \}\;.
\end{align*}
The two events above are disjoint, and we have
\begin{align*}
\Pr&(a<b<c<x_s<d \mid \C_N)\\
&= \Pr(\max G^1_{t:s-1} < \max G^1_{t-1} < \max G^2_{t-1}< x_s < G^2_{t:s-1} \mid \C_N)\\
&= \E\left[\tfrac{|G^2_{t:s-1}|}{|G^1_{t:s-1}|+|G^1_{t-1}|+|G^2_{t-1}|+1+|G^2_{t:s-1}|}
\cdot \tfrac{1}{|G^1_{t:s-1}|+|G^1_{t-1}|+|G^2_{t-1}|+1} 
\cdot \tfrac{|G^2_{t-1}|}{|G^1_{t:s-1}|+|G^1_{t-1}|+|G^2_{t-1}|}
\cdot \tfrac{|G^1_{t-1}|}{|G^1_{t:s-1}|+|G^1_{t-1}|}
\cdot \mid \C_N \right]\\
&= \E\left[\frac{|G^2_{t:s-1}|}{s}
\cdot \frac{1}{t+|G^1_{t:s-1}|} 
\cdot \frac{|G^2_{t-1}|}{t-1+|G^1_{t:s-1}|}
\cdot \frac{|G^1_{t-1}|}{|G^1_{s-1}|}
 \; \Big| \; \C_N \right]\\
&=\frac{(1-\lambda)(s-t) + O(\sqrt{N \log N})}{s(t + \lambda(s-t) + O(\sqrt{N \log N}))} \cdot \frac{(1-\lambda)t + O(\sqrt{N \log N})}{t + \lambda(s-t) + O(\sqrt{N \log N})}\cdot \frac{\lambda t + O(\sqrt{N \log N})}{\lambda s + O(\sqrt{N \log N})}\\
&= \frac{(1-\lambda)^2 (s-t) t^2}{((1-\lambda)t + \lambda s)^2 s^2} + O\Big( \sqrt{\tfrac{\log N}{N^3}} \Big)\;.
\end{align*}
The probability of the second event is
\begin{align*}
\Pr(a < b < x_s < c \mid \C_N)
&= \Pr(\max G^1_{t:s-1} < \max G^1_{t-1} < x_s <  \max G^2_{t-1} \mid \C_N)\\
&= \E\left[ \tfrac{|G^2_{t-1}|}{|G^1_{t:s-1}| + |G^1_{t-1}| + 1 + |G^2_{t-1}|}
\cdot \tfrac{1}{|G^1_{t:s-1}| + |G^1_{t-1}| + 1}
\cdot \tfrac{|G^1_{t-1}|}{|G^1_{t:s-1}| + |G^1_{t-1}| }
 \;\Big|\; \C_N \right]\\
&= \E\left[ \frac{|G^2_{t-1}|}{t + |G^1_{t:s-1}|}
\cdot \frac{1}{|G^1_{s-1}| + 1}
\cdot \frac{|G^1_{t-1}|}{|G^1_{s-1}|}
 \;\Big|\; \C_N \right]\\
&= \frac{(1-\lambda)t + O(\sqrt{N \log N})}{t + \lambda(s-t) + O(\sqrt{N \log N})} \cdot \frac{1}{\lambda s + O(\sqrt{N \log N})} \cdot \frac{\lambda t + O(\sqrt{N \log N})}{\lambda s + O(\sqrt{N \log N})}\\
&= \frac{(1-\lambda)t^2}{\lambda((1-\lambda)t + \lambda s) s^2} + O\Big( \sqrt{\tfrac{\log N}{N^3}} \Big)\;.
\end{align*}
Finally, Lemma \ref{lem:pr-cond-C_N} shows that $\Pr( g_s = 1 \mid C_N) = \lambda + O(1/N^2)$, and we deduce 
\begin{align*}
\Pr(\rho_1 = s, R_s \neq 1, g^*_{t-1} = 2 \mid \C_N)
&= \lambda \Pr(\mathcal{E} \mid \C_N) + O(\tfrac{1}{N^2})\\
&= \lambda \Pr(a<b<c<x_s<d \mid \C_N) + \lambda \Pr(a < b < x_s < c \mid \C_N) + O(\tfrac{1}{N^2})\\
&= \frac{\lambda(1-\lambda)^2 (s-t) t^2}{((1-\lambda)t + \lambda s)^2 s^2} + \frac{\lambda(1-\lambda)t^2}{\lambda((1-\lambda)t + \lambda s) s^2} + O\Big( \sqrt{\tfrac{\log N}{N^3}} \Big)\\
&= \frac{(1-\lambda)t^2}{((1-\lambda)t+\lambda s)^2s^2}\left( \lambda(1-\lambda)(s-t) + (1-\lambda)t+\lambda s\right) + O\Big( \sqrt{\tfrac{\log N}{N^3}} \Big)\\
&= \frac{(1-\lambda)t^2}{((1-\lambda)t+\lambda s)^2s^2}\left((1-\lambda)^2 t + \lambda(2-\lambda)s \right) + O\Big( \sqrt{\tfrac{\log N}{N^3}} \Big)\;.\\
\end{align*}
\end{proof}





\begin{lemma}\label{lem:2grp-rho>betaN}
Let $\lfloor \alpha N \rfloor \leq t < \lfloor \beta N \rfloor$, and consider a run of Algorithm $\A_t^B(\alpha, \beta)$, then
\begin{align*}
\Pr(\A_t^B(\alpha,\beta) \text{ succeeds}&, \rho_1 \geq \beta N, g^*_{t-1} = 1 \mid \C_N)\\
&=  \frac{t}{\beta N} \U^B_{N, \lfloor \beta N \rfloor, 1}
+ \frac{\lambda(\beta N -t)t}{((1-\lambda) t + \lambda \beta N) \beta N} \U^B_{N, \lfloor \beta N \rfloor, 2} +  O\Big(\sqrt{\tfrac{\log N}{N}}\Big)\;, \\
\Pr(\A_t^B(\alpha,\beta) \text{ succeeds}&, \rho_1 \geq \beta N, g^*_{t-1} = 2 \mid \C_N)\\
&= \frac{t^2 }{((1-\lambda)t + \lambda \beta N) \beta N} \U^B_{N,\lfloor \beta N \rfloor,2} +  O\Big(\sqrt{\tfrac{\log N}{N}}\Big)\;.
\end{align*} 
\end{lemma}

\begin{proof}
Since Algorithm $\A_t^B(\alpha, \beta)$ is memoryless, if it does not stop before step $\lfloor \beta N \rfloor$, then its success probability is the same as that of $\A^B_{\lfloor \beta N \rfloor}(\alpha, \beta) = \A^B_0(\beta, \beta)$, which has the same threshold $\beta$ for both groups, if it is in the same state $(g^*_{\lfloor \beta N \rfloor - 1}, |G^1_{\lfloor \beta N \rfloor}|)$. In all the proof, $\rho_1$ is relative to Algorithm $\A_t^B(\alpha, \beta)$, not $\A^B_0(\beta, \beta)$. It holds that
\begin{align*}
\Pr&(\A_t^B(\alpha,\beta) \text{ succeeds}, \rho_1 \geq \beta N, g^*_{t-1} = 1 \mid \C_N)\\
&= \sum_{\ell \in \{1,2\}}\Pr(\A_t^B(\alpha,\beta) \text{ succeeds}, \rho_1 \geq \beta N, g^*_{t-1} = 1, g^*_{\lfloor \beta N \rfloor - 1} = \ell \mid \C_N)\\
&= \sum_{\ell \in \{1,2\}} \Pr(\rho_1 \geq \beta N, g^*_{t-1} = 1, g^*_{\lfloor \beta N \rfloor - 1} = \ell \mid \C_N))\\
&\hspace{1.35cm}\times \Pr(\A_t^B(\alpha,\beta) \text{ succeeds} \mid \rho_1 \geq \beta N, g^*_{t-1} = 1, g^*_{\lfloor \beta N \rfloor - 1} = \ell, \C_N) \\
&= \sum_{\ell \in \{1,2\}} \Pr(\rho_1 \geq \beta N, g^*_{t-1} = 1, g^*_{\lfloor \beta N \rfloor - 1} = \ell \mid \C_N) \Pr(\A^B_0(\beta,\beta) \text{ succeeds} \mid g^*_{\lfloor \beta N \rfloor - 1} = \ell, \C_N)\\
&= \sum_{\ell \in \{1,2\}} \Pr(\rho_1 \geq \beta N, g^*_{t-1} = 1, g^*_{\lfloor \beta N \rfloor - 1} = \ell \mid \C_N) \frac{\Pr(\A^B_0(\beta,\beta) \text{ succeeds}, g^*_{\lfloor \beta N \rfloor - 1} = \ell \mid \C_N)}{\Pr(g^*_{\lfloor \beta N \rfloor - 1} = \ell \mid \C_N)}\\
&= \sum_{\ell \in \{1,2\}} \Pr(\rho_1 \geq \beta N, g^*_{t-1} = 1, g^*_{\lfloor \beta N \rfloor - 1} = \ell \mid \C_N) \frac{\U^B_{N,\lfloor \beta N \rfloor,\ell}}{\Pr(g^*_{\lfloor \beta N \rfloor - 1} = \ell) + O(\tfrac{1}{N^2})}\;,\\
\end{align*} 
where we used Lemma \ref{lem:pr-cond-C_N} and the definition \eqref{eq:def-U} of $\U^B_{N,s,\ell}$. Let us now compute the probability of the event $\{\rho_1 \geq \beta N, g^*_{t-1} = 1, g^*_{\lfloor \beta N \rfloor - 1} = \ell\}$ conditional to $\C_N$.
For $\ell = 1$, we have 
\begin{align*}
\Pr&(\rho_1 \geq \beta N, g^*_{t-1} = 1, g^*_{\lfloor \beta N \rfloor - 1} = 1 \mid \C_N)\\
&= \Pr(\max G^1_{t:\lfloor \beta N \rfloor-1} < \max G^1_{t-1} \;,\; \max G^2_{t-1} < \max G^1_{t-1} \;,\; \max G^2_{\lfloor \beta N \rfloor-1} < \max G^1_{\lfloor \beta N \rfloor-1} \mid \C_N)\\
&= \Pr(\max x_{1:\lfloor \beta N \rfloor - 1} \in G^1_{t-1}\mid \C_N)\\
&= \E\left[ \frac{|G^1_{t-1}|}{\lfloor \beta N \rfloor-1} \Big| \C_N\right]\\
&= \frac{\lambda t + O(\sqrt{N \log N})}{\beta N + O(1)}
= \frac{\lambda t}{\beta N} + O\Big( \sqrt{\tfrac{\log N}{N}} \Big)\;.
\end{align*}
For $\ell = 2$, we first compute the following
\begin{align*}
\Pr(\rho_1 \geq \beta N, g^*_{t-1} = 1\mid \C_N)
&= \Pr( \max G^1_{t:\lfloor \beta N \rfloor-1} < \max G^1_{t-1} \;,\; \max G^2_{t-1} < \max G^1_{t-1} \mid \C_N)\\
&= \Pr( \max( G^1_{t:\lfloor \beta N \rfloor-1} \cup G^2_{t-1} ) < \max G^1_{t-1} \mid \C_N)\\
&= \E\left[ \frac{|G^1_{t-1}|}{|G^1_{t:\lfloor \beta N \rfloor-1}| + |G^2_{t-1}| + |G^1_{t-1}|} \;\Big|\; \C_N \right]\\
&= \frac{\lambda t + O(\sqrt{N \log N})}{t + \lambda(\beta N - t) + O(\sqrt{N \log N})}\\
&= \frac{\lambda t }{(1-\lambda) t + \lambda \beta N} + O\Big( \sqrt{\tfrac{\log N}{N}} \Big)\;,
\end{align*}
and it follows that
\begin{align*}
\Pr&(\rho_1 \geq \beta N, g^*_{t-1} = 1, g^*_{\lfloor \beta N \rfloor - 1} = 2 \mid \C_N)\\
&= \Pr(\rho_1 \geq \beta N, g^*_{t-1} = 1 \mid \C_N) - \Pr(\rho_1 \geq \beta N, g^*_{t-1} = 1, g^*_{\lfloor \beta N \rfloor - 1} = 1 \mid \C_N)\\
&= \frac{\lambda t }{(1-\lambda) t + \lambda \beta N} - \frac{\lambda t}{\beta N} +  O\Big( \sqrt{\tfrac{\log N}{N}} \Big)\\
&= \frac{\lambda(1-\lambda)(\beta N -t)t}{((1-\lambda) t + \lambda \beta N) \beta N}  + O\Big( \sqrt{\tfrac{\log N}{N}} \Big)\;.
\end{align*}
All in all, we deduce that 
\begin{align*}
\Pr(\A_t^B&(\alpha,\beta) \text{ succeeds}, \rho_1 \geq \beta N, g^*_{t-1} = 1 \mid \C_N)\\
&= \Big(\frac{\lambda t}{\beta N} + O\Big( \sqrt{\tfrac{\log N}{N}} \Big) \Big) \frac{\U^B_{N, \lfloor \beta N \rfloor, 1}}{\lambda + O(\tfrac{1}{N^2})}
+ \left(\frac{\lambda(1-\lambda)(\beta N -t)t}{((1-\lambda) t + \lambda \beta N) \beta N} + O\Big( \sqrt{\tfrac{\log N}{N}} \Big) \right) \frac{\U^B_{N, \lfloor \beta N \rfloor, 2}}{1-\lambda + O(\tfrac{1}{N^2})}\\
&= \Big(\frac{t}{\beta N} + O\Big( \sqrt{\tfrac{\log N}{N}} \Big) \Big) \U^B_{N, \lfloor \beta N \rfloor, 1}
+ \left(\frac{\lambda(\beta N -t)t}{((1-\lambda) t + \lambda \beta N) \beta N} + O\Big( \sqrt{\tfrac{\log N}{N}} \Big) \right) \U^B_{N, \lfloor \beta N \rfloor, 2}\\
&=  \frac{t}{\beta N} \U^B_{N, \lfloor \beta N \rfloor, 1}
+ \frac{\lambda(\beta N -t)t}{((1-\lambda) t + \lambda \beta N) \beta N} \U^B_{N, \lfloor \beta N \rfloor, 2} + O\Big(\sqrt{\tfrac{\log N}{N}}\Big) \;.
\end{align*} 

On the other hand, if $g^*_{t-1} = 2$ and $\rho_1 \geq \beta N$, then necessarily $g^*_{\lfloor \beta N \rfloor-1} = 2$, because no candidate in $G^1$ up to step $\lfloor \beta N \rfloor-1$ surpasses $\max G^1_{t-1}$, which is less than $\max G^2_{t-1}$. Therefore
\begin{align*}
\Pr(\A_t^B&(\alpha,\beta) \text{ succeeds}, \rho_1 \geq \beta N, g^*_{t-1} = 2 \mid \C_N)\\
&= \Pr(\rho_1 \geq \beta N, g^*_{t-1} = 2 \mid \C_N) \Pr(\A_t^B(\alpha, \beta) \mid  \rho_1 \geq \beta N, g^*_{t-1} = 2, \C_N)\\
&= \Pr(\max G^1_{t:\lfloor \beta N \rfloor-1} < \max G^1_{t-1} < \max G^2_{t-1} \mid \C_N) \Pr(\A_0^B(\beta, \beta) \mid  g^*_{\lfloor \beta N \rfloor-1} = 2, \C_N)\\
&= \E\left[ \frac{|G^2_{t-1}|}{t-1 + |G^1_{t:\lfloor \beta N \rfloor-1}|} \cdot \frac{|G^1_{t-1}|}{|G^1_{\lfloor \beta N \rfloor-1}|} \;\Big|\; \C_N \right] \frac{\Pr(\A_0^B(\beta, \beta),  g^*_{\lfloor \beta N \rfloor-1} = 2 \mid \C_N)}{\Pr(g^*_{\lfloor \beta N \rfloor-1} = 2 \mid \C_N)}\\
&= \frac{(1-\lambda)t + O(\sqrt{N\log N})}{t+ \lambda(\beta N -t) + O(\sqrt{N\log N})}\cdot \frac{\lambda t + O(\sqrt{N\log N})}{\lambda \beta N + O(\sqrt{N\log N})}\cdot \frac{\U^B_{N,\lfloor \beta N \rfloor,2}}{1 - \lambda + O(\tfrac{1}{N^2})}\\
&= \frac{t^2 }{((1-\lambda)t + \lambda \beta N) \beta N} \U^B_{N,\lfloor \beta N \rfloor,2} +  O\Big(\sqrt{\tfrac{\log N}{N}}\Big)\;.
\end{align*}
\end{proof}




In the following lemma, we compute the exact limit of $\U^B_{N,\lfloor \beta N \rfloor,k}$ when the number of candidates goes to infinity.

\begin{lemma}\label{lem:2grp-Ubb}
For all $B \geq 0$ and $k \in \{1,2\}$, 
\[
\U^B_{N,\lfloor \beta N \rfloor,k}
= \lambda_k \beta^2\sum_{b = 0}^B \left( \frac{1}{\beta} - \sum_{\ell = 0}^b \frac{\log(1/\beta)^\ell}{\ell !} \right) + O\Big( \sqrt{\tfrac{\log N}{N}}\Big)\;.
\]
\end{lemma}


\begin{proof}
By definition \eqref{eq:def-U} of $\U^B_{N,t,k}$, we have
\begin{align*}
\U^B_{N,t,k} 
&= \Pr(\A^B_{\lfloor \beta N \rfloor}(\alpha, \beta) \text{ succeeds},  g^*_{t-1} = k \mid \C_N)\;,
\end{align*}
and $\A^B_{\lfloor \beta N \rfloor}(\alpha, \beta)$ is simply the single-threshold algorithm with threshold $\beta N$ and budget $B$. Let $T = \lfloor \beta N \rfloor$. As in the proof of Lemma \ref{lem:single-thres-recursion}, we decompose the success probability of $\A^B_{\lfloor \beta N \rfloor}$ as follows
\begin{align*}
\U^B_{N,T,k} 
&= \Pr(\A^B_{T}(\alpha, \beta) \text{ succeeds},  g^*_{T-1} = k \mid \C_N)\\
&= \sum_{t=T}^N \sum_{\ell \in \{1,2\}} \Pr(\A^B_{T}(\alpha, \beta) \text{ succeeds}, \rho_1=t, g_t = \ell,  g^*_{T-1} = k \mid \C_N)\\
&= \sum_{t=T}^N \sum_{\ell \in \{1,2\}} \bigg(\Pr(\A^B_{T}(\alpha, \beta) \text{ succeeds}, \rho_1=t, R_t = 1, g_t = \ell,  g^*_{T-1} = k \mid \C_N)\\
& \hspace{1.9cm} + \Pr(\A^B_{T}(\alpha, \beta) \text{ succeeds}, \rho_1=t, R_t \neq 1, g_t = \ell,  g^*_{T-1} = k \mid \C_N) \bigg)\;.
\end{align*}

The terms appearing in the sums above were computed in the proof of Lemma \ref{lem:single-thres-recursion}. It follows respectively from \eqref{aligneq:single-thresh-Rt=1} and \eqref{aligneq:single-thresh-Rtneq1}, with $K = 2$, that
\begin{align*}
\Pr(\A^B_{T}(\alpha, \beta) \text{ succeeds}, \rho_1=t, R_t = 1&, g_t = \ell,  g^*_{T-1} = k \mid \C_N)
= \frac{\lambda_\ell \lambda_k}{N} (T/t)^2 +  O\Big( \sqrt{\tfrac{\log N}{N^3}}\Big)\;,\\
\Pr(\A^B_{T}(\alpha, \beta) \text{ succeeds}, \rho_1=t, R_t \neq 1&, g_t = \ell,  g^*_{T-1} = k \mid \C_N)\\
&= \left( \frac{T^2}{t^{3}} + O\Big( \sqrt{\tfrac{\log N}{N^3}}\Big) \right) \indic{B>0, k \neq \ell} \U^{B-1}_{N,t+1,k}  \;,
\end{align*}
where the $O$ terms are independent of $t$, they only depend on $\beta$. Therefore,
\begin{align*}
\U^B_{N,T,k} 
&= \left(1 + O\Big( \sqrt{\tfrac{\log N}{N}}\Big) \right)\sum_{t=T}^N \sum_{\ell \in \{1,2\}} \bigg(\frac{\lambda_\ell \lambda_k}{N} (T/t)^2 + \frac{T^2}{t^{3}} \indic{B>0, k \neq \ell} \U^{B-1}_{N,t+1,k} \bigg)\\
&= \left(1 + O\Big( \sqrt{\tfrac{\log N}{N}}\Big) \right)\sum_{t=T}^N \bigg(\frac{\lambda_k}{N} (T/t)^2 + \frac{T^2}{t^{3}} \indic{B>0} \U^{B-1}_{N,t+1,k} \bigg)\;.
\end{align*}
The first sum can easily be computed
\begin{align*}
\sum_{t=T}^N \frac{\lambda_k}{N} (T/t)^2
&= \frac{\lambda_k (T/N)^2}{N} \sum_{t=T}^N \frac{1}{(t/N)^2}\\
&= \lambda_k (\beta^2 + O(\tfrac{1}{N})) \int_\beta^1 \frac{du}{u^2} + O(\tfrac{1}{N})\\
&= \lambda_k \beta(1- \beta) + O(\tfrac{1}{N})\;.    
\end{align*}
Therefore,
\begin{align*}
\U^B_{N,T,k}
&= \left(1 + O\Big( \sqrt{\tfrac{\log N}{N}}\Big) \right)\bigg(\lambda_k \beta(1- \beta) +  \indic{B>0}\sum_{t=T}^N \frac{T^2}{t^{3}}\U^{B-1}_{N,t+1,k} + O(\tfrac{1}{N}) \bigg)\\
&= \lambda_k \beta(1- \beta) +  \indic{B>0}\sum_{t=T}^N \frac{T^2}{t^{3}}\U^{B-1}_{N,t+1,k} + O\Big( \sqrt{\tfrac{\log N}{N}}\Big) \;.
\end{align*}
Dividing by $\lambda_k$ yields
\begin{align*}
\big(\lambda_k^{-1} \U^B_{N,T,k} \big)
=  \beta(1- \beta) +  \indic{B>0}\sum_{t=T}^N \frac{T^2}{t^{3}} \big( \lambda_k^{-1} \U^{B-1}_{N,t+1,k} \big) + O\Big( \sqrt{\tfrac{\log N}{N}}\Big) \;.
\end{align*}
thus the double-indexed sequence $(\lambda_k^{-1}\U^b_{N,t,k})_{b,t}$ satisfies the same recursion and initial condition as $(\Pr(\A^b_t(0,0) \text{ succeeds}))_{b,t}$ (see proof of Lemma \ref{lem:single-thres-recursion}) with $\beta$ instead of $w$ and $K=2$. Therefore, we deduce immediately that:
\[
\lambda_k^{-1}\U^B_{N,T,k} = \beta^2 \sum_{b = 0}^B \left( \frac{1}{\beta} - \sum_{\ell = 0}^b \frac{\log(1/\beta)^\ell}{\ell !} \right) + O\Big( \sqrt{\tfrac{\log N}{N}}\Big)\;.
\]
\end{proof}


















\subsection{Proof of Lemma \ref{lem:lim-2grp-k=2}}

\begin{proof}
Using Lemmas \ref{lem:recursion-U-alpha-beta}, \ref{lem:2grp-rho>s}, \ref{lem:2grp-rho=s} and \ref{lem:2grp-rho>betaN} for $k=2$, we obtain for all $t \in \{ \lfloor\alpha N \rfloor, \ldots, \lfloor\beta N \rfloor - 1\}$
\begin{align*}
\U^B_{N,t,2} 
&=  \frac{\lambda}{N} \sum_{s = t}^{\lfloor \beta N \rfloor-1} \left(\frac{(1-\lambda)t^2}{((1-\lambda)t + \lambda s)s} + O\Big( \sqrt{\tfrac{\log N}{N}} \Big) \right)\\
& \quad +\frac{\indic{B>0}}{1-\lambda}  \sum_{s = t}^{\lfloor \beta N \rfloor-1} \left( \frac{(1-\lambda)\left( (1-\lambda)^2 t + \lambda(2-\lambda)s\right)t^2}{((1-\lambda)t+\lambda s)^2s^2} + O\Big( \sqrt{\tfrac{\log N}{N^3}} \Big)\right) \U^{B-1}_{N,s+1,2} \\
&\quad + \frac{t^2 }{((1-\lambda)t + \lambda \beta N) \beta N} \U^B_{N,\lfloor \beta N \rfloor,2} + O\Big(\sqrt{\tfrac{\log N}{N}}\Big)\;.
\end{align*}
The $O$ terms inside the sums depend on the ratios $t/N$ and $s/N$, but using that $\alpha \leq t/N \leq s/N \leq 1$, it can be made only dependent on $\alpha$ and the other constants of the problem. Moreover, Thus we can write
\begin{align*}
\U^B_{N,t,2} 
&= \frac{\lambda(1-\lambda) t^2}{N} \sum_{s = t}^{\lfloor \beta N \rfloor-1} \frac{1}{((1-\lambda)t + \lambda s)s} \nonumber\\
& \quad + \indic{B>0} t^2 \sum_{s = t}^{\lfloor \beta N \rfloor-1}  \frac{\left( (1-\lambda)^2 t + \lambda(2-\lambda)s\right)}{((1-\lambda)t+\lambda s)^2s^2}  \U^{B-1}_{N,s+1,2}\nonumber \\
&\quad +\frac{t^2 }{((1-\lambda)t + \lambda \beta N) \beta N} \U^B_{N,\lfloor \beta N \rfloor,2} + O\Big( \sqrt{\tfrac{\log N}{N}} \Big) \nonumber\\
&=   \lambda(1-\lambda)(\tfrac{t}{N})^2 \frac{1}{N} \sum_{s = t}^{\lfloor \beta N \rfloor-1} \frac{1}{((1-\lambda)\tfrac{t}{N} + \lambda \tfrac{s}{N})\tfrac{s}{N}} \nonumber\\
& \quad + \indic{B>0} (\tfrac{t}{N})^2 \frac{1}{N} \sum_{s = t}^{\lfloor \beta N \rfloor-1}  \frac{ (1-\lambda)^2 \tfrac{t}{N} + \lambda(2-\lambda)\tfrac{s}{N}}{((1-\lambda)\tfrac{t}{N}+\lambda \frac{s}{N})^2 (\tfrac{s}{N})^2}  \U^{B-1}_{N,s+1,2} \nonumber\\
&\quad +\frac{(t/N)^2 }{((1-\lambda)\tfrac{t}{N} + \lambda \beta) \beta} \U^B_{N,\lfloor \beta N \rfloor,2} + O\Big( \sqrt{\tfrac{\log N}{N}} \Big)\;.
\end{align*}
Taking $t = \lfloor wN \rfloor = wN + O(1)$ and using Riemann sum convergence properties yields
\begin{align}
(\tfrac{t}{N})^2 \frac{1}{N} \sum_{s = t}^{\lfloor \beta N \rfloor-1} \frac{1}{((1-\lambda)\tfrac{t}{N} + \lambda \tfrac{s}{N})\tfrac{s}{N}}
&= \frac{w^2}{N} \sum_{s = \lfloor w N \rfloor}^{\lfloor \beta N \rfloor-1} \frac{1}{((1-\lambda)w + \lambda \tfrac{s}{N})\tfrac{s}{N}} + O\big( \tfrac{1}{N}\big) \nonumber \\
&= w^2 \int_w^\beta \frac{du}{((1-\lambda)w + \lambda u)u} + O\big( \tfrac{1}{N}\big) \nonumber \\
&= \frac{w}{1-\lambda} \left( \int_w^\beta \frac{du}{ u} - \int_w^\beta \frac{du}{(1/\lambda-1)w + u}  \right) + O\big( \tfrac{1}{N}\big) \nonumber \\
&= \frac{w}{1 - \lambda} \left( - \log(w/\beta) - \log(1-\lambda + \lambda \beta/w) \right) + O\big( \tfrac{1}{N}\big) \nonumber \\
&= \frac{- w \log\big((1-\lambda) \tfrac{w}{\beta} + \lambda \big)}{1 - \lambda} + O\big( \tfrac{1}{N}\big)\;.
\end{align}
On the other hand,
\[
\frac{(t/N)^2 }{((1-\lambda)\tfrac{t}{N} + \lambda \beta) \beta} = \frac{w^2 }{((1-\lambda)w + \lambda \beta) \beta} + O(\tfrac{1}{N})\;,
\]
and Lemma \ref{lem:2grp-Ubb} gives for $k=2$ that $\phi^B_2(\alpha, \beta; \beta)$ exists, its expression is
\begin{equation}\label{eq:phiB2-bb}
\phi^B_2(\alpha, \beta; \beta)
= (1-\lambda)\beta^2 \sum_{b = 0}^B \left( \frac{1}{\beta} - \sum_{\ell = 0}^b \frac{\log(1/\beta)^\ell}{\ell !} \right)\;,       
\end{equation}
and it satisfies 
$\U^B_{N,\lfloor \beta N \rfloor,2}
= \phi^B_2(\alpha, \beta; \beta) + O\Big( \sqrt{\tfrac{\log N}{N}}\Big)$. Consequently,
\begin{align}
\U^B_{N,\lfloor w N \rfloor,2} 
&= - \lambda w \log\big((1-\lambda) \tfrac{w}{\beta} + \lambda \big) \nonumber\\
& \quad + \indic{B>0} \frac{w^2}{N} \sum_{s = \lfloor w N \rfloor}^{\lfloor \beta N \rfloor-1}  \frac{ (1-\lambda)^2 w + \lambda(2-\lambda)\tfrac{s}{N}}{((1-\lambda)w+\lambda \frac{s}{N})^2 (\tfrac{s}{N})^2}  \U^{B-1}_{N,s+1,2} \nonumber\\
&\quad +\frac{w^2 }{((1-\lambda)w + \lambda \beta) \beta} \phi^B_2(\alpha, \beta; \beta) + O\Big( \sqrt{\tfrac{\log N}{N}} \Big)\;. \label{aligneq:U-rec}
\end{align}



Using this equality, we will prove by induction over $B \geq 0$ that, for all $w \in [\alpha, \beta]$, the limit
$\phi^B(\alpha,\beta; w) :=  \lim_{N \to \infty} \U^B_{N,\lfloor w N \rfloor,2}$
exists, is continuous, and satisfies 
\[
\U^B_{N,\lfloor w N \rfloor,2} 
= \phi^B_2(\alpha,\beta; w) + O\Big( \sqrt{\tfrac{\log N}{N}} \Big)\;.
\]



\noindent
\textbf{Initialization.}
For $B=0$ and $w \in [\alpha, \beta]$, \eqref{aligneq:U-rec} gives immediately
\begin{align}
\U^B_{N,\lfloor wN \rfloor,2} 
&= - \lambda w \log\big((1-\lambda) \tfrac{w}{\beta} + \lambda \big) 
+\frac{w^2 }{((1-\lambda)w + \lambda \beta) \beta} \phi^B_2(\alpha, \beta; \beta) + O\Big( \sqrt{\tfrac{\log N}{N}} \Big)\;.
\end{align}

\noindent
\textbf{Induction.} Let $B \geq 1$, $w \in [\alpha, \beta]$, and assume that 
$\U^{B-1}_{N,\lfloor u N \rfloor,2} 
= \phi^B_2(\alpha,\beta; u) + O\Big( \sqrt{\tfrac{\log N}{N}} \Big)$ for all $u \in [\alpha, \beta]$, where the $O$ does not depend on $u$. Using this hypothesis for $u = \tfrac{s+1}{N}$, with $s \in \{t, \ldots,\lfloor \beta N\rfloor -1\}$, along with te continuity of $\phi^{B-1}_2(\alpha, \beta; \cdot)$ and Riemann sums convergence properties, yields
\begin{align*}
\frac{w^2}{N} \sum_{s = \lfloor w N \rfloor}^{\lfloor \beta N \rfloor-1} &  \frac{ (1-\lambda)^2 w + \lambda(2-\lambda)\tfrac{s}{N}}{((1-\lambda)w+\lambda \frac{s}{N})^2 (\tfrac{s}{N})^2}  \U^{B-1}_{N,s+1,2}\\
&= \frac{w^2}{N} \sum_{s = \lfloor w N \rfloor}^{\lfloor \beta N \rfloor-1}  \frac{ (1-\lambda)^2 w + \lambda(2-\lambda)\tfrac{s}{N}}{((1-\lambda)w+\lambda \frac{s}{N})^2 (\tfrac{s}{N})^2}  \phi^{B-1}_2(\alpha, \beta; \tfrac{s+1}{N}) + O\Big( \sqrt{\tfrac{\log N}{N}} \Big)\\
&= w^2 \int_w^\beta \frac{ (1-\lambda)^2 w + \lambda(2-\lambda)u}{((1-\lambda)w+\lambda u)^2 u^2}  \phi^{B-1}_2(\alpha, \beta; u) du + O\Big( \sqrt{\tfrac{\log N}{N}} \Big)\;.
\end{align*}
Therefore, we deduce by \eqref{aligneq:U-rec} that
\begin{align*}
\U^B_{N,\lfloor w N \rfloor,2} 
&= - \lambda w \log\big((1-\lambda) \tfrac{w}{\beta} + \lambda \big)\\
& \quad + w^2 \int_w^\beta \frac{ (1-\lambda)^2 w + \lambda(2-\lambda)u}{((1-\lambda)w+\lambda u)^2 u^2}  \phi^{B-1}_2(\alpha, \beta; u) du\\
&\quad +\frac{w^2 }{((1-\lambda)w + \lambda \beta) \beta} \phi^B_2(\alpha, \beta; \beta) + O\Big( \sqrt{\tfrac{\log N}{N}} \Big)\;.
\end{align*}
This proves that $\U^B_{N,\lfloor w N \rfloor,2} = \phi^B_2(\alpha,\beta;w) + O\Big( \sqrt{\tfrac{\log N}{N}} \Big)$, where 
\begin{align*}
\phi^B_2(\alpha,\beta;w)
&= - \lambda w \log\big((1-\lambda) \tfrac{w}{\beta} + \lambda \big)\\
& \quad + w^2 \int_w^\beta \frac{ (1-\lambda)^2 w + \lambda(2-\lambda)u}{((1-\lambda)w+\lambda u)^2 u^2}  \phi^{B-1}_2(\alpha, \beta; u) du\\
&\quad +\frac{w^2 }{((1-\lambda)w + \lambda \beta) \beta} \phi^B_2(\alpha, \beta; \beta)\;,
\end{align*}
where the expression of $\phi^B_2(\alpha, \beta; \beta)$ is given in \eqref{eq:phiB2-bb}.
\end{proof}










\subsection{Proof of Lemma \ref{lem:lim-2grp-k=1}}


\begin{proof}
Using Lemmas \ref{lem:recursion-U-alpha-beta}, \ref{lem:2grp-rho=s}, \ref{lem:2grp-rho>s} and \ref{lem:2grp-rho>betaN} for $k=2$, we obtain for all $t \in \{ \lfloor\alpha N \rfloor, \ldots, \lfloor\beta N \rfloor - 1\}$
\begin{align*}
\U^B_{N,t,1} 
&=   \frac{\lambda}{N} \sum_{s = t}^{\lfloor \beta N \rfloor-1} \left(\frac{\lambda t}{(1-\lambda)t + \lambda s} + O\Big( \sqrt{\tfrac{\log N}{N}} \Big) \right)\\
& \quad +  \frac{\indic{B>0}}{1-\lambda} \sum_{s = t}^{\lfloor \beta N \rfloor-1} \left( \frac{\lambda^2(1-\lambda)(s-t)t}{((1-\lambda)t + \lambda s)^2s} + O\Big( \sqrt{\tfrac{\log N}{N^3}} \Big) \right) \U^{B-1}_{N,s+1,2} \\
&\quad + \frac{t}{\beta N} \U^B_{N, \lfloor \beta N \rfloor, 1} + \frac{\lambda(\beta N -t)t}{((1-\lambda) t + \lambda \beta N) \beta N} \U^B_{N, \lfloor \beta N \rfloor, 2} +  O\Big(\sqrt{\tfrac{\log N}{N}}\Big)\\
&= \frac{\lambda^2 (t/N)}{N} \sum_{s = t}^{\lfloor \beta N \rfloor-1} \frac{1}{(1-\lambda)\tfrac{t}{N} + \lambda \tfrac{s}{N}}\\
& \quad + \indic{B>0}  \frac{\lambda^2(t/N)}{N} \sum_{s = t}^{\lfloor \beta N \rfloor-1} \frac{\tfrac{s}{N}-\tfrac{t}{N}}{((1-\lambda)\tfrac{t}{N} + \lambda \tfrac{s}{N})^2 \tfrac{s}{N}} \U^{B-1}_{N,s+1,2} \\
&\quad + \frac{t/N}{\beta} \U^B_{N, \lfloor \beta N \rfloor, 1} + \frac{\lambda(\beta - \tfrac{t}{N})\tfrac{t}{N}}{((1-\lambda) \tfrac{t}{N} + \lambda \beta) \beta} \U^B_{N, \lfloor \beta N \rfloor, 2} +  O\Big(\sqrt{\tfrac{\log N}{N}}\Big)\;.
\end{align*}
Consider in the following $t = \lfloor wN \rfloor = wN + O(1)$. Using Riemann sums convergence properties, we have
\begin{align*}
\frac{\lambda^2 (t/N)}{N} \sum_{s = t}^{\lfloor \beta N \rfloor-1} \frac{1}{(1-\lambda)\tfrac{t}{N} + \lambda \tfrac{s}{N}}
&= \lambda^2 w \int_w^\beta \frac{du}{(1-\lambda)w + \lambda u} + O(\tfrac{1}{N})\\
&= \lambda w \left[ \log((1-\lambda)w + \lambda u) \right]_w^\beta + O(\tfrac{1}{N})\\
&= \lambda w \log\big(1-\lambda + \lambda \tfrac{\beta}{w}\big)+ O(\tfrac{1}{N})\;.
\end{align*}
Since $\U^b_{N,s,k} \leq 1$ for all $b,s,k$, and $t = wN + O(1)$, as in the proof of Lemma \ref{lem:lim-2grp-k=2}, we obtain
\begin{align*}
\U^B_{N,\lfloor wN \rfloor,1} 
&= \lambda w \log\big(1-\lambda + \lambda \tfrac{\beta}{w}\big) + O(\tfrac{1}{N}) \nonumber\\
& \quad + \indic{B>0}  \frac{\lambda^2 w}{N} \sum_{s = \lfloor w N\rfloor}^{\lfloor \beta N \rfloor-1} \frac{\tfrac{s}{N}-w}{((1-\lambda)w + \lambda \tfrac{s}{N})^2 \tfrac{s}{N}} \U^{B-1}_{N,s+1,2} + O(\tfrac{1}{N}) \nonumber\\
&\quad + \frac{w}{\beta} \U^B_{N, \lfloor \beta N \rfloor, 1} + \frac{\lambda(\beta - w)w}{((1-\lambda) w + \lambda \beta) \beta} \U^B_{N, \lfloor \beta N \rfloor, 2} +  O\Big(\sqrt{\tfrac{\log N}{N}}\Big) \nonumber\\
&= \lambda w \log\big(1-\lambda + \lambda \tfrac{\beta}{w}\big) \nonumber\\
& \quad + \indic{B>0}  \frac{\lambda^2 w}{N} \sum_{s = \lfloor w N\rfloor}^{\lfloor \beta N \rfloor-1} \frac{\tfrac{s}{N}-w}{((1-\lambda)w + \lambda \tfrac{s}{N})^2 \tfrac{s}{N}} \U^{B-1}_{N,s+1,2} \nonumber \\
&\quad + \frac{w}{\beta} \phi^B_1(\alpha, \beta; \beta) + \frac{\lambda(\beta - w)w}{((1-\lambda) w + \lambda \beta) \beta} \phi^B_2(\alpha, \beta; \beta) +  O\Big(\sqrt{\tfrac{\log N}{N}}\Big)\;,
\end{align*}
where we used Lemma \ref{lem:2grp-Ubb} in the last inequality, which guarantees that $\U^B_{N, \lfloor \beta N \rfloor, k} = \phi^B_k(\alpha, \beta; \beta) + O\Big(\sqrt{\tfrac{\log N}{N}}\Big)$ for $k \in \{1,2\}$, with
\[
\phi^B_k(\alpha, \beta; \beta)
= \lambda_k\beta^2 \sum_{b = 0}^B \left( \frac{1}{\beta} - \sum_{\ell = 0}^b \frac{\log(1/\beta)^\ell}{\ell !} \right)\;.
\]
Denoting by $\phi^B(\alpha,\beta;\beta) = \phi^B_1(\alpha,\beta;\beta) + \phi^B_2(\alpha,\beta;\beta)$, i.e. $\phi^B(\alpha,\beta;\beta) = \tfrac{1}{\lambda_k}\phi^B_k(\alpha,\beta;\beta)$, we have
\begin{align*}
\frac{w}{\beta} \phi^B_1(\alpha, \beta; \beta) + \frac{\lambda(\beta - w)w}{((1-\lambda) w + \lambda \beta) \beta} \phi^B_2(\alpha, \beta; \beta)
&= \left( \frac{\lambda w}{\beta} + \frac{\lambda(1-\lambda)(\beta - w)w}{((1-\lambda) w + \lambda \beta) \beta}\right) \phi^B(\alpha, \beta; \beta)\\
&= \frac{\lambda w}{\beta}\left( 1 + \frac{(1-\lambda)(\beta - w)}{(1-\lambda) w + \lambda \beta}\right) \phi^B(\alpha, \beta; \beta)\\
&= \frac{\lambda w}{\beta}\left( \frac{\beta}{(1-\lambda) w + \lambda \beta}\right) \phi^B(\alpha, \beta; \beta)\\
&=  \frac{\lambda w}{(1-\lambda) w + \lambda \beta} \phi^B(\alpha, \beta; \beta)\;.
\end{align*}
Thus
\begin{align}
\U^B_{N,\lfloor wN \rfloor,1} 
&= \lambda w \log\big(1-\lambda + \lambda \tfrac{\beta}{w}\big) 
+ \frac{\lambda w}{(1-\lambda) w + \lambda \beta} \phi^B(\alpha, \beta; \beta) \nonumber\\
& \quad + \indic{B>0}  \frac{\lambda^2 w}{N} \sum_{s = \lfloor w N\rfloor}^{\lfloor \beta N \rfloor-1} \frac{\tfrac{s}{N}-w}{((1-\lambda)w + \lambda \tfrac{s}{N})^2 \tfrac{s}{N}} \U^{B-1}_{N,s+1,2} +  O\Big(\sqrt{\tfrac{\log N}{N}}\Big)\;. \label{aligneq:UB1-recursion}
\end{align}



Now, we will prove by induction over $B$ that $\U^B_{N, \lfloor w N \rfloor, 1} = \phi^B_1(\alpha, \beta; w) + O\Big(\sqrt{\tfrac{\log N}{N}}\Big)$ for all $w \in [\alpha, \beta]$, with $\phi^B_1(\alpha, \beta; \cdot)$ a continuous function satisfying the recursion stated in the Lemma.


\noindent
\textbf{Initialization} 
For $B = 0$, \eqref{aligneq:UB1-recursion} yields immediately for all $w \in [\alpha, \beta]$
\begin{align*}
\U^B_{N,\lfloor wN \rfloor,1} 
&= \lambda w \log\big(1-\lambda + \lambda \tfrac{\beta}{w}\big) 
+ \frac{\lambda w}{(1-\lambda) w + \lambda \beta} \phi^B(\alpha, \beta; \beta) +  O\Big(\sqrt{\tfrac{\log N}{N}}\Big)\;.
\end{align*}
\noindent
\textbf{Induction} Let $B \geq 1$, and assume that $\U^{B-1}_{N, \lfloor u N \rfloor, 1} = \phi^{B-1}_1(\alpha, \beta; u) + O\Big(\sqrt{\tfrac{\log N}{N}}\Big)$ for all $u \in [\alpha, \beta]$, and that $\phi^B_1(\alpha, \beta; \cdot)$ is continuous. Consequently, using Riemann sums convergence properties, it holds for all $w \in [\alpha, \beta]$ that
\begin{align*}
\frac{\lambda^2 w}{N} \sum_{s = \lfloor w N\rfloor}^{\lfloor \beta N \rfloor-1}& \frac{\tfrac{s}{N}-w}{((1-\lambda)w + \lambda \tfrac{s}{N})^2 \tfrac{s}{N}} \U^{B-1}_{N,s+1,2}\\
&= \frac{\lambda^2 w}{N} \sum_{s = \lfloor w N\rfloor}^{\lfloor \beta N \rfloor-1} \frac{\tfrac{s}{N}-w}{((1-\lambda)w + \lambda \tfrac{s}{N})^2 \tfrac{s}{N}} \left( \phi^{B-1}_2(\alpha, \beta; \tfrac{s+1}{N}) + O\Big(\sqrt{\tfrac{\log N}{N}}\Big) \right)\\
&= \frac{\lambda^2 w}{N} \sum_{s = \lfloor w N\rfloor}^{\lfloor \beta N \rfloor-1} \frac{\tfrac{s}{N}-w}{((1-\lambda)w + \lambda \tfrac{s}{N})^2 \tfrac{s}{N}} \phi^{B-1}_2(\alpha, \beta; \tfrac{s+1}{N}) + O\Big(\sqrt{\tfrac{\log N}{N}}\Big) \\
&= \lambda^2 w \int_w^\beta \frac{(u-w) \phi^{B-1}_2(\alpha, \beta; u)}{((1-\lambda)w + \lambda u)^2 u} du  + O\Big(\sqrt{\tfrac{\log N}{N}}\Big)\;,
\end{align*}
thus, we have by substituting into \eqref{aligneq:UB1-recursion}
\begin{align*}
\U^B_{N,\lfloor wN \rfloor,1} 
&= \lambda w \log\big(1-\lambda + \lambda \tfrac{\beta}{w}\big) 
+ \frac{\lambda w}{(1-\lambda) w + \lambda \beta} \phi^B(\alpha, \beta; \beta) \\
& \quad +  \lambda^2 w \int_w^\beta \frac{(u-w) \phi^{B-1}_2(\alpha, \beta; u)}{((1-\lambda)w + \lambda u)^2 u} du  +  O\Big(\sqrt{\tfrac{\log N}{N}}\Big)\\
&= \phi_1^B( \alpha, \beta; w) + O\Big(\sqrt{\tfrac{\log N}{N}}\Big)\;.
\end{align*}
\end{proof}



\subsection{Proof of Corollary \ref{cor:lb2grps}}
\begin{proof}
Assume that $\lambda \geq 1/2$. For any thresholds $0 < \alpha \leq \beta \leq 1$ Theorem \ref{thm:success-2grps} yields 
\[
\lim_{N \to \infty} \Pr(\A^B(\alpha,\beta) \text{ succeeds})
\geq \lambda \alpha \log\big(\tfrac{\beta}{\alpha}\big) + \alpha \beta S^B(\beta)\;.
\]
We will now determine thresholds maximizing this lower bound. For a fixed $\beta$, we have
\begin{align*}
\frac{\partial }{\partial \alpha} \big( \lambda \alpha \log\big(\tfrac{\beta}{\alpha}\big) + \alpha \beta S^B(\beta) \big)
\geq 0
&\iff \lambda \log\big(\tfrac{\beta}{\alpha}\big) - \lambda + \beta S^B(\beta) \geq 0\\
&\iff \lambda \log\big(\tfrac{\alpha}{\beta}\big) \leq  \beta S^B(\beta) - \lambda\\
&\iff \alpha \leq \frac{\beta}{e} \exp\left( \frac{\beta}{\lambda} S^B(\beta) \right)\;.
\end{align*}
This proves that, for fixed $\beta$ the lower bound $\lambda \alpha \log\big(\tfrac{\beta}{\alpha}\big) + \alpha \beta S^B(\beta)$ is maximized on $[0,\beta]$ for $\alpha = \min(\beta, \frac{\beta}{e} \exp( \tfrac{\beta}{\lambda} S^B(\beta) )) = h^B(\beta)$. With this choice of $\alpha$, the optimal choice of $\beta$ is the one maximizing the mapping $\beta \mapsto \lambda h^B(\beta) \log\big(\tfrac{\beta}{h^B(\beta)}\big) + h^B(\beta) \beta S^B(\beta)$. 

In particular, for $B = 0$, we obtain $\tilde{\alpha}_0 = \lambda \exp(\tfrac{1}{\lambda}-2)$ and $\tilde{\beta}_0 = \lambda$. They guarantee an asymptotic success probability of at least $\lambda^2 \exp(\tfrac{1}{\lambda}-2)$. Given that the sequence $(S^B(w))_B$ is non-decreasing for all $w \in (0,1]$, it holds for all $B \geq 0$ that
\begin{align*}
\lim_{N \to \infty} \Pr(\A^B(\tilde{\alpha}_B,\tilde{\beta}_B) \text{ succeeds})
&\geq \lambda \tilde{\alpha}_B \log\big(\tfrac{\tilde{\beta}_B}{\tilde{\alpha}_B}\big) + \tilde{\alpha}_B \tilde{\beta}_B S^B(\tilde{\beta}_B)\\
&= \max_{\alpha \leq \beta} \left\{ \lambda \alpha \log\big(\tfrac{\beta}{\alpha}\big) + \alpha \beta S^B(\beta) \right\}\\
&\geq \max_{\alpha \leq \beta} \left\{ \lambda \alpha \log\big(\tfrac{\beta}{\alpha}\big) + \alpha \beta S^0(\beta) \right\}\\
&= \lambda \tilde{\alpha}_0 \log\big(\tfrac{\tilde{\beta}_0}{\tilde{\alpha}_0}\big) + \tilde{\alpha}_0 \tilde{\beta}_0 S^0(\tilde{\beta}_0)\\
&= \lambda^2 \exp ( \tfrac{1}{\lambda} - 2)\\
&\geq \tfrac{1}{e} - (\tfrac{4}{e} - 1)\lambda(1-\lambda)\;.
\end{align*}
On the other hand, taking equal thresholds $\alpha = \beta = \tfrac{1}{e}$ then using Corollary \ref{cor:single-thresh-factorial-conv} with $K=2$ gives 
\begin{align*}
\lim_{N \to \infty} \Pr(\A^B(\tilde{\alpha}_B,\tilde{\beta}_B) \text{ succeeds})
&\geq \max_{\alpha \leq \beta} \left\{ \lambda \alpha \log\big(\tfrac{\beta}{\alpha}\big) + \alpha \beta S^B(\beta) \right\}\\
&\geq \frac{S^B(1/e)}{e^2}  \\
&\geq \frac{1}{e} - \frac{1}{e (B+1)!}\;.
\end{align*}
Thus, we deduce that
\[
\lim_{N \to \infty} \Pr(\A^B(\tilde{\alpha}_B,\tilde{\beta}_B) \text{ succeeds}) 
\geq \frac{1}{e} - \min\left\{ \frac{1}{e(B+1)!}, (\tfrac{1}{e}-1)\lambda(1-\lambda) \right\}\;.
\]
\end{proof}








\subsection{Proof of Theorem \ref{thm:success-2grps}}

\begin{proof}
By Lemmas \ref{lem:pr-cond-C_N}, \ref{lem:lim-2grp-k=2} and \ref{lem:lim-2grp-k=1}, The success probability of Algorithm $\A(\alpha,\beta)^B$ can be written as
\begin{align*}
\Pr(\A^B(\alpha,\beta) \text{ succeeds}) 
&= \Pr(\A^B_{\lfloor \alpha N \rfloor}(\alpha,\beta) \text{ succeeds} \mid \C_N) + O(\tfrac{1}{N^2})\\
&= \Pr(\A^B_{\lfloor \alpha N \rfloor}(\alpha,\beta) \text{ succeeds}, g^*_{\lfloor \alpha N\rfloor - 1} = 1 \mid \C_N) \\
&\quad + \Pr(\A^B_{\lfloor \alpha N \rfloor}(\alpha,\beta) \text{ succeeds}, g^*_{\lfloor \alpha N\rfloor - 1} = 2 \mid \C_N) + O(\tfrac{1}{N^2})\\
&= \U^B_{N,\lfloor \alpha N\rfloor,1} + \U^B_{N,\lfloor \alpha N\rfloor,2} + O(\tfrac{1}{N^2})\\
&= \phi_1^B( \alpha, \beta; \alpha) + \phi_2^B( \alpha, \beta; \alpha) + O\Big(\sqrt{\tfrac{\log N}{N}}\Big)\\
&= \lambda \alpha \log\big(1-\lambda + \lambda \tfrac{\beta}{\alpha}\big) 
+ \frac{\lambda \alpha \beta^2}{(1-\lambda) \alpha + \lambda \beta} \sum_{b = 0}^B \left( \frac{1}{\beta} - \sum_{\ell = 0}^b \frac{\log(1/\beta)^\ell}{\ell !} \right) \\
& \quad + \indic{B > 0} \lambda^2 \alpha \int_\alpha^\beta \frac{(u-\alpha) \phi^{B-1}_2(\alpha, \beta; u)}{((1-\lambda)\alpha + \lambda u)^2 u} du\\
&\quad - \lambda \alpha \log\big((1-\lambda) \tfrac{\alpha}{\beta} + \lambda \big) + \frac{(1-\lambda)\beta \alpha^2 }{(1-\lambda)\alpha + \lambda \beta}  \sum_{b = 0}^B \left( \frac{1}{\beta} - \sum_{\ell = 0}^b \frac{\log(1/\beta)^\ell}{\ell !} \right)\\
& \quad + \indic{B > 0} \alpha^2 \int_\alpha^\beta \frac{ (1-\lambda)^2 \alpha + \lambda(2-\lambda)u}{((1-\lambda)\alpha+\lambda u)^2 u^2}  \phi^{B-1}_2(\alpha, \beta; u) du + O\Big(\sqrt{\tfrac{\log N}{N}}\Big)\;,
\end{align*}
then, regrouping the terms yields
\begin{align*}
\Pr(&\A^B(\alpha,\beta) \text{ succeeds}) \\
&= \lambda \alpha \log\big(1-\lambda + \lambda \tfrac{\beta}{\alpha}\big) -  \lambda \alpha \log\big((1-\lambda) \tfrac{\alpha}{\beta} + \lambda \big)\\
& \quad + \frac{\alpha \beta}{(1-\lambda)\alpha + \lambda \beta} \left( \lambda \beta + (1-\lambda) \alpha  \right)  \sum_{b = 0}^B \left( \frac{1}{\beta} - \sum_{\ell = 0}^b \frac{\log(1/\beta)^\ell}{\ell !} \right)\\
&\quad + \indic{B > 0} \alpha \int_\alpha^\beta \left( \lambda^2 (1 - \tfrac{\alpha}{u}) + \tfrac{\alpha}{u} \big( (1-\lambda)^2 \tfrac{\alpha}{u} + \lambda(2-\lambda) \big) \right) \frac{ \phi^{B-1}_2(\alpha, \beta; u) du}{((1-\lambda)\alpha + \lambda u)^2} + O\Big(\sqrt{\tfrac{\log N}{N}}\Big)\;.
\end{align*}
Finally, observing that 
\begin{align*}
(1-\lambda)^2 \tfrac{\alpha}{u} + \lambda(2-\lambda) \big)
&= (1- \lambda)^2 \tfrac{\alpha^2}{u^2} + 2\lambda (1-\lambda) \tfrac{\alpha}{u} + \lambda^2\\
&= \tfrac{1}{u^2} \big( (1-\lambda) \alpha + \lambda u \big)^2\;,
\end{align*}
we deduce the result
\begin{align*}
\Pr&(\A^B(\alpha,\beta) \text{ succeeds})\\
&= \lambda \alpha \log\big(\tfrac{\beta}{\alpha}\big) + \alpha \beta \sum_{b = 0}^B \left( \frac{1}{\beta} - \sum_{\ell = 0}^b \frac{\log(1/\beta)^\ell}{\ell !} \right)
+ \indic{B > 0} \alpha \int_\alpha^\beta \frac{ \phi^{B-1}_2(\alpha, \beta; u) du}{u^2} + O\Big(\sqrt{\tfrac{\log N}{N}}\Big)\;,
\end{align*}
%where the $O$ term depends on $\alpha$.
\end{proof}







