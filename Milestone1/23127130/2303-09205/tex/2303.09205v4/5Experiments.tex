
\section{Numerical experiments}\label{sec:experiments}

In this section, we confirm our theoretical findings via numerical experiments, and we give further insight regarding the behavior of the algorithms we presented and how they compare to each other. In all the empirical experiments of this section, each point is computed over $10^6$ independent trials. The code used for the experiments is available at \href{https://github.com/Ziyad-Benomar/Addressing-bias-in-online-selection-with-limited-budget-of-comparisons}{github.com/Ziyad-Benomar/Addressing-bias-in-online-selection-with-limited-budget-of-comparisons}.




\subsection{Single-threshold algorithm}
Using Theorem \ref{thm:single-thresh}, the optimal threshold, for the single-threshold algorithm, can be computed numerically for fixed $K$ and $B$.
Figure \ref{fig:ST_KB} illustrates the optimal threshold and the corresponding success probability for $B \in \{0,\ldots,30\}$ and $K \in \{2,10,25,50\}$. For any $K \geq 2$, as the budget grows to infinity, the problem becomes akin to the standard secretary problem, leading the optimal threshold to converge to $1/e$. However, as discussed in Corollary \ref{cor:single-thresh-factorial-conv}, the convergence is slower when the number of groups $K$ is higher.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.35\textwidth]{figs/STthresh_KvsB.pdf} \quad \includegraphics[width=0.35\textwidth]{figs/STproba_KvsB.pdf}
    \caption{Single threshold algorithm: optimal threshold and corresponding success probability}
    \label{fig:ST_KB}
\end{figure}




Moreover, Theorem \ref{thm:single-thresh} reveals that the asymptotic success probability is independent of the probabilities of belonging to each group, and it is equal to a value smaller than $1/e$. This indicates a discontinuity of the success probability at the extreme points of the polygon defining the possible values of $(\lambda_k)_{k \in [K]}$. Figure \ref{fig:ST_discontinuity} illustrates this behavior for the case of two groups, with $N=500$ candidates, and $B\in \{0,1,2\}$. 
On the other hand, while our theoretical results study asymptotic success probabilities, 
they do not comprehend how the performance of the algorithms varies with the number of candidates. Figure \ref{fig:ST_Ngrows} shows that the success probability is better when the number of candidates is small, and it decreases to match the asymptotic expression when $N \to \infty$, represented with dotted lines, for $K \in \{2,3,4\}$, with $B = 3$. 


\begin{figure}[h!]
\centering
\begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/ST_lmb_discontinuity.pdf}
    \caption{Single threshold: success probability for $2$ groups, with $N = 500$ and $\lambda \in [0,1]$}
    \label{fig:ST_discontinuity}
\end{minipage} \hfill
\begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{figs/ST_Ngrows.pdf} 
    \caption{Convergence to the asymptotic success probability, with $\lambda_k = 1/K$ for all $k \in [K]$}
    \label{fig:ST_Ngrows}
\end{minipage}
\end{figure}











\subsection{The case of two groups}

In the case of two groups, Figure \ref{fig:ST_KB} shows that, even with a very limited budget, the single-threshold algorithm has a success probability almost indistinguishable from the upper bound $1/e$. Consequently, in the remaining experiments in the two-group scenario, we restrict ourselves to small budgets $B \leq 3$.

\begin{figure}[h!]
  \centering
  \begin{minipage}{0.49\textwidth}
    Theorem \ref{thm:opt-memless} shows a recursive formula for computing the success probability of the optimal memory-less algorithm $\A_*$ for all $N \geq 1$, $B \geq 0$, and $\lambda \in (0,1)$. Figure \ref{fig:DP-success-lb} displays this success probability, in solid lines, for $N=500$ and $B \in \{0,1,2\}$, along with the success probability of the static double-threshold algorithm $\A^B(\Tilde{\alpha}_B, \Tilde{\beta}_B)$ in dotted lines, where $\Tilde{\alpha}_B, \Tilde{\beta}_B$ are defined in Corollary \ref{cor:lb2grps}.
    The figure demonstrates that for $B = 0$, or $\lambda$ close to $0.5$, Algorithm $\A^B(\Tilde{\alpha}_B, \Tilde{\beta}_B)$ matches the performance of $\A_*$, despite having a much simpler structure. 
  \end{minipage}\hfill
  \begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/lowerBoundCor2grps.pdf}
    \caption{Success probability of $\A_*$, and the lower bound of Corollary \ref{cor:lb2grps}}\label{fig:DP-success-lb}
  \end{minipage}
\end{figure}

For $\lambda = 0.5$, both groups have symmetric roles, and the optimal thresholds $\tilde{\alpha}_B$ and $\tilde{\beta}_B$ to choose in the static-threshold algorithm are identical. Hence, the success probability of $\A^B(\Tilde{\alpha}_B, \Tilde{\beta}_B)$ for $\lambda = 0.5$ is exactly that of the single-threshold algorithm, which is independent of $\lambda$ (Theorem \ref{thm:single-thresh}). We deduce from this observation and Figure \ref{fig:DP-success-lb} that having different thresholds for each group yields a substantial improvement over the single-threshold algorithm when $\lambda$ is close to $0$ or $1$.



Finally, to emphasize that the dynamic programming algorithm $\A_*$ is equivalent to an instance of $\DT$ algorithm for large $N$, Figure \ref{fig:DPvsDDT} compares the empirical success probabilities of $\A_*$ (dotted lines) and the $\DT$ algorithm (solid lines) with thresholds $(\alpha^\star_B,\beta^\star_B)_{B \geq 0}$, computed as explained in Section \ref{sec:opt-dynprog-main}.

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.3\textwidth]{figs/DPvsDDT_05.pdf}
    %\hfill
    \includegraphics[width=0.3\textwidth]{figs/DPvsDDT_07.pdf}
    %\hfill
    \includegraphics[width=0.3\textwidth]{figs/DPvsDDT_095.pdf}
    \caption{Empirical success probabilities of $\A_*$ and the $\DT$ algorithm with optimal thresholds}\label{fig:DPvsDDT}
\end{figure}
For $\lambda \in \{0.5, 0.7, 0.95\}$, the figure confirms that, despite the intricate structure of the optimal memory-less algorithm, it does not surpass the performance of the $\DT$ algorithm with optimal thresholds when $N$ is large. Nonetheless, the analysis of the optimal memory-less algorithm is what enables the numerical computation of the optimal thresholds, as explained previously. Figure \ref{fig:threhsolds_alphas_betas} shows the optimal thresholds $\alpha_b^\star, \beta_b^\star$ for all $\lambda \in [0.5,1]$ and $B \in \{0,1,2\}$. 

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.3\textwidth]{figs/threshAlpha.pdf}
    \includegraphics[width=0.3\textwidth]{figs/threshBeta.pdf}
    \caption{$(\alpha_b^\star, \beta_b^\star)$ as functions of $\lambda$.}\label{fig:threhsolds_alphas_betas}
\end{figure}

These thresholds are continuous functions of $\lambda$, both converging to $1/e$ very rapidly as the budget increases. Indeed, for $B \geq 1$, they both become very close to $1/e$, as for $B = 0$, the optimal thresholds are exactly equal to $\alpha^\star_0 = \lambda \exp(\tfrac{1}{\lambda}-2)$ and $\beta^\star_0 = \lambda$, which correspond to the optimal thresholds described in Corollary \ref{cor:lb2grps} for $B = 0$ (See the proof of the corollary). 






