\section{Optimal memory-less algorithm for two groups}\label{sec:opt-dynprog}

In this section, we derive an optimal memoryless algorithm employing a dynamic programming approach. We analyze the state transitions depending on the algorithm's actions and the associated success probabilities for each state. Unlike previous sections, our study here is not asymptotic. Therefore, we do not rely on estimating the number of candidates in each group using concentration inequalities. Instead, we consider the exact number of candidates in each group as a parameter for decision-making at each step.

We recall that memoryless algorithms, at any given step $t$, have access to the available budget $B_t$ and the number of previous candidates belonging to each group. In the case of two groups, this information reduces to $(t, B_t, |G^1_{t-1}|)$, since $|G^2_{t-1}| = t-1 - |G^1_{t-1}|$.
The state of the algorithm, which fully determines its success probability, is given by the tuple $(t, B_t, |G^1_{t-1}|, g^*_{t-1})$. However, $g^*_{t-1}$ is not known to the algorithm, hence it must make decisions relying on the limited information it has, to maximize the expected success probability, where the expectation is taken over $g^*_{t-1}$.



\subsection{State transitions}\label{sec:state-transition}
For any memory-less algorithm $\A$, we denote by $\S_t(\A)$ its state at step $t$, which is a tuple $(t, b, m, \ell)$. Here, $t-1$ represents the count of previously rejected candidates, $b \geq 0$ denotes the available budget, $m < t$ indicates the number of prior candidates from group $G^1$, and $\ell \in \{1,2\}$ is the group containing the best-seen candidate so far.

To examine the state transitions of the algorithm, it is imperative to first understand the distribution of the new observations at any given step $t$, depending on $\S_t(\A)$. While the group membership $g_t$ of candidate $x_t$ is independent of $\S_t(\A)$, both $r_t$ and $R_t$ are contingent on it.



\begin{lemma}\label{lem:dynprog-rt-Rt}
For any memory-less algorithm $\A$ and state $(t,m,b,\ell)$, denoting by $k = 3-\ell$ the group index different from $\ell$, it holds that
\begin{align*}
&\Pr(r_t = 1 \mid \S_t(\A) = (t,m,b,\ell), g_t = \ell) 
= \frac{1}{t}\\
&\Pr(r_t = 1 \mid \S_t(\A) = (t,m,b,\ell), g_t = k) 
= \frac{|G^k_{t-1}|+t}{t(|G^k_{t-1}|+1)}\\
&\Pr(R_t = 1 \mid \S_t(\A) = (t,m,b,\ell), g_t = \ell, r_t = 1)
= 1\\
&\Pr(R_t = 1 \mid \S_t(\A) = (t,m,b,\ell), g_t = k, r_t=1) 
= \frac{|G^k_{t-1}|+1}{|G^k_{t-1}|+t}\;,
\end{align*}
where
\[
|G^k_{t-1}| = \left\{
    \begin{array}{ll}
        m & \mbox{if }\; k = 1 \\
        t-1-m & \mbox{if }\; k = 2 \;.
    \end{array}
\right.
\]
\end{lemma}




Using the previous Lemma, we can fully characterize the possible state transitions of a memory-less algorithm.
First, the values of the parameters $|G^1_{t}|$ and $B_{t+1}$ are trivially determined based on the state $S_t$ at the beginning of step $t$, the observations $r_t$ and $g_t$, and the actions of the algorithm:
\[
|G^1_{t}| = |G^1_{t-1}| + \indic{g_t = 1}\;,
\quad B_{t+1} = B_t - \indic{\act_{t,1} = \acomp}\;,
\]
where $\act_{t,1}$ is the action taken by the algorithm, which only depends on the state $S_t$ since the algorithm is memory-less.

Regarding $g^*_t$, if $g_t = g^*_{t-1}$, then $g^*_t = g^*_{t-1}$ remains unchanged with probability $1$. However, if $g_t \neq g^*_{t-1}$ and $r_t = 1$, and if the algorithm skips the candidate without making a comparison, then $g^*_t$ is not deterministic based on the history alone. The probability that $g^*_t = g_t$ in this case is precisely the probability that $R_t = 1$, computed in Lemma \ref{lem:dynprog-rt-Rt}
\[
\Pr(g^*_t = g_t \mid \S_t(\A) = (t,m,b,\ell), g_t = k, r_t=1) 
= \frac{|G^k_{t-1}|+1}{|G^k_{t-1}|+t}\;.
\]





\subsection{Expected action rewards}\label{sec:expected-rwd}
In the following, we denote by $\A_*$ the optimal memory-less algorithm for two groups, and for all $B \geq 0$, $t \in [N]$, $m < t$ and $\ell \in \{1,2\}$, we denote by
\[
\V^B_{t,m,\ell} = \Pr\big(\A_* \text{ succeeds} \mid \tau \geq t, \S_t(\A_*) = (t,B,m,\ell) \big)\;,
\]
which is its success probability starting from state $(t,B,m,\ell)$.








We analyze the expected rewards and state transitions of algorithm $\A_*$ given its limited information access.
When the algorithm receives a new observation $(r_t, g_t)$:
\begin{itemize}
    \item If $r_t \neq 1$, the optimal action is to skip the candidate ($\askip$).
    \item If $r_t = 1$ and $B_t = 0$, the algorithm either stops or skips the candidate. However, if there is a positive budget $B_t$, stopping is suboptimal: it is always better to make a comparison first.
    \item If the algorithm chooses to make a comparison and observes $R_t$:
    \begin{itemize}
        \item If $R_t \neq 1$, the optimal action is to skip the candidate.
        \item If $R_t = 1$, the algorithm must decide whether to skip or stop. However, skipping after observing $R_t = 1$ is suboptimal compared to skipping immediately after observing $r_t = 1$, as the latter conserves the budget.
    \end{itemize}
\end{itemize}





In summary, any rational algorithm follows these decision rules:
\begin{itemize}
    \item If ($r_t \neq 1$) or ($r_t = 1$ and $R_t \neq 1$), then skip the candidate.
    \item If ($r_t = 1$ and $R_t = 1$), select the candidate.
\end{itemize}

Therefore, the main non-trivial decision to make is whether to reject or accept a candidate after observing $r_t = 1$.
Consider an algorithm $\A$ following these rules. At time $t$ with budget $B_t = b$ and $|G^1_{t-1}| = m$, if $g_t = k$ and $r_t = 1$, choosing an action $\act \in \{\askip, \astop, \acomp\}$ based on these rules leads to a new state $\S_{t+1}(\A) = F(t, b, m, k, \act)$, which is a random variable depending on $g^*_{t-1}$ and $R_t$. If $\act \in \{ \astop, \acomp\}$ and $R_t = 1$, then $\S_{t+1}(\A)$ is a final state: success or failure.



With this notation, we define $\rwd^B_{t,m}(\act)$ as the reward that $\A_*$ expects to gain by playing action $\act$ after observing $r_t = 1$ and $g_t = k$ in a state $S_t(\A_*) = (t,b,m,\cdot)$, where it ignores $g^*_{t-1}$
\[
\rwd^B_{t,m,k}(\act)
= \E[\Pr\big(\A_* \text{ succeeds} \mid \S_{t+1}(\A_*) = F(t,B,m,k,\act) \big) \mid S_t(\A_*) = (t,B,m,\cdot), r_t=1, g_t=k]\;.
\]
where the expectation is taken over $g^*_{t-1}$ and $R_t$.
The optimal memory-less action at any state $(t,B,m,\ell)$, knowing that $r_t=1, g_t=k$, is the one maximizing $\rwd^B_{t,m,k}(\act)$. 


\begin{lemma}\label{lem:action-reward}
Consider a state $S_t = (t,B,m,\cdot)$, and let $\{k, \ell\} = \{1,2\}$, $M_k = m + \indic{k = 1}$, then
\begin{align*}
&\rwd^B_{t,m,k}(\astop) = \frac{|G^k_t|}{N}\;,\\
&\rwd^B_{t,m,k}(\askip) = \frac{|G^1_t|}{t} \V^B_{t+1,M_k,1} + \frac{|G^2_t|}{t} \V^B_{t+1,M,2}\;,\\
&\rwd^B_{t,m,k}(\acomp) = \frac{|G^k_t|}{N} + \frac{|G^\ell_t|}{t}\left( \frac{|G^k_{t-1}|+1}{|G^k_{t-1}|+t}\cdot \frac{t}{N} + \frac{t-1}{|G^k_{t-1}|+t} \V^{B-1}_{t+1,M_k,\ell} \right)\;,
\end{align*}
\end{lemma}

Observe that, conditionally to $g_t$ and $|G^1_{t-1}|$, the cardinals of $G^1_{t-1}, G^2_{t-1}, G^1_{t}, G^2_{t}$ are all known: 
\[
|G^1_{t}| = |G^1_{t-1}| + \indic{g_t=1}\;,
\quad |G^2_{t}| = t - |G^1_{t}|, 
\quad |G^2_{t-1}| = t - 1 - |G^1_{t-1}|\;. 
\]



\subsection{Optimal actions and success probability}
Using Lemma \ref{lem:action-reward} and considering the potential state transitions based on the actions, we establish a recursion satisfied by $(\V^B_{t,m,\ell})_{t,B,m,\ell}$. We present the result without distinction between the cases $\ell = 1$ and $\ell = 2$. For simplicity, let $\lambda_k = \Pr(g_t = k)$ for $k = 1,2$, and define $M_k = m + \indic{k=1}$ for all $m \geq 0$. Additionally, for all $(B,t,m,k)$, define
\[
\delta^B_k = \indic{\rwd^B_{t,m,k}(\accept) \geq \rwd^B_{t,m,k}(\askip)}\;,
\]
where the action $\accept$ corresponds to $\acomp$ for $B>0$ and $\astop$ for $B=0$.

\begin{theorem}\label{thm:opt-memless}
For all $t \in [N]$, $m < t$ and $\{k,\ell\} = \{1,2\}$, the success probability of $\A_*$ with zero budget satisfies the recursion
\begin{align*}
\V^0_{t,m,\ell}
&= \lambda_\ell \left(\tfrac{\delta^0_\ell}{N} + \left(1 - \tfrac{\delta^0_\ell}{t}\right)\V^0_{t+1,M_\ell,\ell} \right) 
+ \lambda_k \left( \tfrac{\delta^0_k}{N} + \tfrac{1-\delta^0_k}{t} \V^0_{t+1,M_k,k} + \left(1 - \tfrac{1}{t}\right)\left(2 - \delta^0_k - \tfrac{1}{|G^k_{t-1}|+1} \right) \V^0_{t+1,M_k,\ell} \right)\;,
\end{align*}
and for $B \geq 1$ it satisfies
\begin{align*}
\V^B_{t,m,\ell}
&= \lambda_\ell \left( \tfrac{\delta^B_\ell}{N} + \big( 1 - \tfrac{\delta^B_\ell}{t} \big) \V^B_{t+1,M_\ell,\ell} \right) \\
&\quad+ \lambda_k \left( \tfrac{\delta^B_k}{N} + \tfrac{\delta^B_k}{|G^k_{t-1}|+1}\big( 1 - \tfrac{1}{t}\big) \V^{B-1}_{t+1,M_k,\ell} + \tfrac{1-\delta^B_k}{t} \V^{B}_{t+1,M_k,k} + \big(1-\tfrac{1}{t}\big)\big( 1 - \tfrac{\delta^B_k}{|G^k_{t-1}|+1}\big)  \V^{B}_{t+1,M_k,\ell}  \right)\;,
\end{align*}
where $\V^B_{N+1,m,k} = 0$ for all $B \geq 0$ $m \leq N$ and $k \in \{1,2\}$.
\end{theorem}







Implementing the optimal memory-less algorithm $\A_*$ with budget $B$ requires knowing the 
$(\rwd^b_{t,m,k}(\act))_{t,b,m,k}$ for $\act \in \{\askip, \astop, \acomp\}$, which depend themselves on the table $\big(\V^b_{t,m,k}\big)_{t,b,m,k}$.
Using Lemma \ref{lem:action-reward} and Theorem \ref{thm:opt-memless}, these tables can be computed in a $O(B N^2)$ time as described in Algorithm \ref{algo:computeTables}.


\begin{algorithm}[h!]
\DontPrintSemicolon 
\caption{Computing $(\V^b_{t,m,k})_{t,b,m,k}$ and $(\rwd^b_{t,m,k}(\act))_{t,b,m,k}$}\label{algo:computeTables}
\SetKwInput{Input}{Input}
   \SetKwInOut{Output}{Output}
   \SetKwInput{Initialization}{Initialization}
   \Input{Number of candidates $N$, available budget $B$, probability distribution of $g_t$: $\lambda_1, \lambda_2$}
   \Initialization{$\V^b_{N+1,m,k} \gets 0$ for all $b \leq B , m \leq N, k \in \{1,2\} $}
\For{$b = 1, \ldots, B$}{
    \For{$t=N, N-1,\ldots,1$}{
        \For{$m = 0, \ldots, t$}{
            Compute $\rwd^b_{t,m,k}(\act)$ for $k \in \{1,2\}$ and $\act \in \{\askip, \astop, \acomp\}$ using Lemma \ref{lem:action-reward}\;
            Compute $\V^b_{t,m,k}$ for $k \in \{1,2\}$ using Theorem \ref{thm:opt-memless}\;
        }
    }
}
Return: $(\V^b_{t,m,k})_{t,b,m,k}$, $(\rwd^b_{t,m,k}(\act))_{t,b,m,k}$\;
\end{algorithm}



After computing these tables, the optimal memory-less algorithm $\A_*$ can be implemented by following the rational decision rules outlined in Section \ref{sec:expected-rwd}, and when encountering $r_t = 1$ and needing to choose between accepting or rejecting the candidate, $\A_*$ selects the action that maximizes its expected reward given the information it has about the current state. A detailed description is provided in Algorithm \ref{algo:opt-memless}.

\begin{algorithm}[h!]
\DontPrintSemicolon 
\caption{Optimal memory-less algorithm $\A_*$}\label{algo:opt-memless}
\SetKwInput{Input}{Input}
   \SetKwInOut{Output}{Output}
   \SetKwInput{Initialization}{Initialization}
   \Input{Number of candidates $N$, available budget $B$, probability distribution of $g_t$: $\lambda_1, \lambda_2$}
   \Initialization{$b \gets B$, $m \gets 0$}
   Compute $(\V^b_{t,m,k})_{t,b,m,k}$ and $(\rwd^b_{t,m,k}(\act))_{t,b,m,k,\act}$ using Algorithm \ref{algo:computeTables}\;
\For{$t = 1,\ldots, N$}{
    Receive new observation $(r_t, g_t)$\;
    \If{$r_t = 1$}{
        \If{$b=0$ and $\rwd^0_{t,m,g_t}(\astop) > \rwd^0_{t,m,g_t}(\askip)$}{
        Return: $t$\;
        }
        \If{$b>0$ and $\rwd^b_{t,m,g_t}(\acomp) > \rwd^b_{t,m,g_t}(\askip)$}{
            $b \gets b - 1$\;
            \If{$R_t = 1$}{Return: t}
        }
    }
    $m \gets m + \indic{g_t = 1}$\;
}
\end{algorithm}



Although the structure of Algorithm $\A_*$ is complicated, the discussion, following Figure \ref{fig:DPacceptance}n in the subsequent section provides additional insights into its behavior, offering a visual and simpler understanding of its decision-making rule.


%$b \gets b - 1$\;
%        \If{$R_t = 1$}{Return: t}
