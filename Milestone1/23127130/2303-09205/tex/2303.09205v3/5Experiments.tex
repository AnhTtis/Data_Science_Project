\section{Numerical experiments}\label{sec:experiments}

In this section, we confirm our theoretical findings via numerical experiments, and we give further insight regarding the behavior of the algorithms we presented and how they compare to each other. In all the empirical experiments of this section, each point is computed over $10^6$ independent trials.

\subsection{Single-threshold algorithm}
We begin our experimental analysis by examining the single-threshold algorithm, studied in Section \ref{sec:single-thresh}. Its asymptotic success probability, provided in Theorem \ref{thm:single-thresh}, depends on the number $K$ of distinct groups, the budget $B$, and the threshold $\w \in [0,1]$. The optimal threshold, maximizing the success probability, can be computed numerically for fixed $K$ and $B$.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth]{figs/STthresh_KvsB.pdf} \quad \includegraphics[width=0.4\textwidth]{figs/STproba_KvsB.pdf}
    \caption{Single threshold algorithm: optimal threshold and corresponding success probability}
    \label{fig:ST_KB}
\end{figure}

Figure \ref{fig:ST_KB} presents the optimal threshold for $B \in \{0,30\}$ and $K \in \{2,10,25,50\}$. For any $K \geq 2$, as the budget grows to infinity, the problem becomes akin to the standard secretary problem, leading the optimal threshold to converge to $1/e$. However, as discussed following Corollary \ref{cor:single-thresh-factorial-conv}, this convergence is slower when the number of groups $K$ is higher.


Moreover, Theorem \ref{thm:single-thresh} reveals that the asymptotic success probability is independent of the probabilities of belonging to each group, and it is equal to a value smaller than $1/e$. This indicates a discontinuity of the asymptotic success probability at the extreme points of the polygon defining the possible values of $(\lambda_k)_{k \in [K]}$. Figure \ref{fig:ST_discontinuity} illustrates this behavior for the case of two groups, with $N=500$ candidates.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{figs/ST_lmb_discontinuity.pdf} 
    \caption{Single threshold: success probability for $2$ groups, with $N = 500$ and $\lambda = \Pr(g_t = 1) \in [0,1]$}
    \label{fig:ST_discontinuity}
\end{figure}


The success probability we proved in Theorem \ref{thm:single-thresh} is only asymptotic, it is reached when the number of candidates is very high. Moreover, from Lemma \ref{lem:single-thres-recursion} and from the proof of the theorem, it can be deduced that the difference between the success probability for a given $N$ and the limit is $O\big(\sqrt{\tfrac{\log N}{N}}\big)$. However, this does not comprehend how the success probability varies with the number of candidates. Figure \ref{fig:ST_Ngrows} shows that the success probability is actually better when the number of candidates is small, and it decreases to match the asymptotic expression when $N \to \infty$, represented with dotted lines for $K \in \{2,3,4\}$. 


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{figs/ST_Ngrows.pdf} 
    \caption{Convergence to the asymptotic success probability, with $\lambda_k = 1/K$ for all $k \in [K]$}
    \label{fig:ST_Ngrows}
\end{figure}







\subsection{The case of two groups}
Moving to the case of two groups, Theorem \ref{thm:opt-memless} shows a recursive formula for computing the success probability of the optimal memory-less algorithm $\A_*$ for all $N \geq 1$, $B \geq 0$, and $\lambda \in (0,1)$. Figure \ref{fig:DP-success-lb} displays this success probability for $N=500$ and $B \in \{0,1,2\}$, along with the success probability of the double threshold algorithm $\A^B(\Tilde{\alpha}_B, \Tilde{\beta}_B)$, where $\Tilde{\alpha}_B, \Tilde{\beta}_B$ are defined in Corollary \ref{cor:lb2grps}.


\begin{figure}[h!]
  \centering
    \includegraphics[width=0.6\textwidth]{figs/lowerBoundCor2grps.pdf}
    \caption{Success probability of the optimal memory-less algorithm, and the lower bound of Corollary \ref{cor:lb2grps}}\label{fig:DP-success-lb}
\end{figure}

The figure demonstrates that for $B = 0$, or for $\lambda = 0.5$, Algorithm $\A^B(\Tilde{\alpha}_B, \Tilde{\beta}_B)$ matches the performance of $\A_*$ closely, despite having a much simpler structure. 

We recall that $\A_*$ has access to the available budget $B$, and the number of previous candidates observed in each group, which reduces to knowing $|G^1_t|$ in the case of two groups. Upon observing $(r_t,g_t)$, it makes a decision to accept or reject, where acceptance means $\astop$ if $B=0$ and $\acomp$ otherwise, depending on $t, |G^1_t|, B, g_t$. Figure \ref{fig:DPacceptance} shows the acceptance region (dark green) of the optimal algorithm, with $N = 500$, $\lambda = 0.7$, for $B \in \{0,1,2\}$ and for all possible values of $t\in [N]$, $|G^1_t| \leq t$, and $g_t \in \{1,2\}$.


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{figs/DPacceptanceRegions.pdf}
    \caption{Acceptance region of $\A_*$ depending on the budget}
    \label{fig:DPacceptance}
\end{figure}


The x- and y-axes display the step $t$ and possible group cardinal $|G_t^{1 / 2}|$ up to time $t$ respectively. The latter follows a binomial random distribution with parameters $(\lambda, t)$ and tightly clusters around its mean $|G_t^{1}| \approx \lambda t$ (and $|G_t^{2}| \approx (1-\lambda) t$) even for moderate values of $t$. Consequently, when $N$ is large, $|G^1_t| \approx\lambda t$, and the acceptance region is solely defined by a threshold at the intersection of the acceptance region and the line $|G^1_t| \approx\lambda t$.
This observation directly implies that $\A_*$ behaves as an instance of $\DT$ algorithms. The optimal $\DT$ thresholds for any $\lambda$, $B \geq 0$, and $g \in \{1,2\}$ can therefore be estimated as the intersection of the acceptance region for $G^g$ and the line $(t,\lambda t)$.



\begin{figure}[h!]
  \centering
    \includegraphics[width=0.48\textwidth]{figs/threshAlpha.pdf}
    \hfill
    \includegraphics[width=0.48\textwidth]{figs/threshBeta.pdf}
    \caption{Evolution of $(\alpha_b^\star, \beta_b^\star)$ as a function of $\lambda$.}\label{fig:threhsolds_alphas_betas}
\end{figure}



Figure \ref{fig:threhsolds_alphas_betas} shows the thresholds computed by this method, for all $\lambda \in [0.5,1]$ and $B \in \{0,1,2\}$. The thresholds $\alpha_b^\star, \beta_b^\star$ are continuous functions of $\lambda$, both converging to $1/e$ very rapidly as the budget increases. Indeed, for $B \geq 1$, they both become very close to $1/e$, as for $B = 0$, the optimal thresholds are exactly equal to $\alpha^\star_0 = \lambda \exp(\tfrac{1}{\lambda}-2)$ and $\beta^\star_0 = \lambda$, which correspond to the optimal thresholds described in Corollary \ref{cor:lb2grps} for $B = 0$ (See the proof of the corollary). Another remark is that when $\lambda$ approaches $1$, the thresholds $\alpha^\star_B$ for $G^1$ converge to $1/e$, as the problem again becomes equivalent to the standard secretary problem.



Figure \ref{fig:DPvsDDT} compares the empirical success probabilities of both $\A_*$ and the $\DT$ algorithm with thresholds $(\alpha^\star_B,\beta^\star_B)_{B \geq 0}$.

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.32\textwidth]{figs/DPvsDDT_05.pdf}
    \hfill
    \includegraphics[width=0.32\textwidth]{figs/DPvsDDT_07.pdf}
    \hfill
    \includegraphics[width=0.32\textwidth]{figs/DPvsDDT_095.pdf}
    \caption{Empirical comparison of the success probabilities of $\A_*$ and the $\DT$ algorithm with optimal thresholds}\label{fig:DPvsDDT}
\end{figure}


For $\lambda \in \{0.5, 0.7, 0.95\}$, the $\DT$ algorithm achieves a success probability very close to that of $\A_*$. While there may be slight differences for small values of $N$ when $B = 0$, the curves almost perfectly align for $B \geq 1$ and for larger values of $N$. This observation confirms that, despite the intricate structure of the optimal memory-less algorithm, in most scenarios, it does not surpass the performance of the $\DT$ algorithm with optimal thresholds. Nonetheless, the analysis of the optimal memory-less algorithm is what enables the numerical computation of the optimal thresholds, as explained previously.










