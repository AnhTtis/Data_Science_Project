\section{Double threshold algorithm for two groups}\label{sec:2grps}

In this section, we delve into the particular case of two groups, and we demonstrate how leveraging different thresholds for each group can enhance the success probability. Let $\lambda \in (0,1)$ represent the probability of belonging to group $G^1$, and $1-\lambda$ the probability of belonging to group $G^2$. We examine the success probability of Algorithm $\A^B(\alpha,\beta)$, with threshold $\lfloor \alpha N \rfloor$ for group $G^1$ and $\lfloor \beta N \rfloor$ for group $G^2$, having a budget of $B$ comparisons. This algorithm is a specific instance of the $\DT$ family, wherein the thresholds depend only on the group, and not on the available budget.

Note that in the case of two groups, the two comparison models proposed in Section \ref{sec:setup} are interchangeable, given that a single comparison, at any step $t$, between the current candidate $x_t$ and the best candidate in the other group suffices to decide if $R_t = 1$. Therefore, the guarantees we prove in this section hold for both models.

We assume without loss of generality that $\alpha \leq \beta$, 
and we denote by $\C_N$ the event 
\[
(\C_N): \quad \forall t \geq 1: \max(||G^1_t| - \lambda t|\;,\;||G^2_t| - (1-\lambda) t|) \leq 4 \sqrt{t \log N}\;.
\]
This event provides control over the group sizes at each step. Lemma \ref{lem:conc-card} guarantees that $\C_N$ holds true with a probability of at least $1-\frac{1}{N^2}$ for $N \geq 4$. 
Furthermore, for all $t \in [N]$, we denote by $\A^B_t(\alpha,\beta)$ the algorithm with acceptance thresholds $\max\{\lfloor \alpha N \rfloor, t\}$ and $\max\{\lfloor \beta N \rfloor, t\}$ respectively for groups $G^1$, $G^2$, and we denote by $\U^B_{N,t,k}$ the probability that $\A^B(\alpha, \beta)$ succeeds and $g^*_{t-1} = k$, conditional to the event $\C_N$,
\begin{equation}\label{eq:def-U}
\U^B_{N,t,k} = \Pr(\A^B_t(\alpha, \beta) \text{ succeeds},  g^*_{t-1} = k \mid \C_N)\;.    
\end{equation}
By Lemma \ref{lem:memless}, the state of $\A^B(\alpha, \beta)$ at any step $t$  is completely determined by $(B_t, g^*_{t-1}, |G^1_{t-1}|)$, with the size of $G^2_{t-1}$ implicitly deduced as $|G^2_{t-1}| = t - 1 - |G^1_{t-1}|$. 
Similar to the analysis of the single-threshold algorithm, we establish a recursion formula satisfied by $(\U^B_{N,t,k})_{B,t,k}$, which we later utilize to derive lower bounds on the asymptotic success probability of $\A^B(\alpha,\beta)$.


\begin{lemma}\label{lem:recursion-U-alpha-beta}
For all $B \geq 0$, $t \in \{\lfloor \alpha N \rfloor, \ldots, \lfloor \beta N \rfloor - 1\}$, and $k \in \{1,2\}$,  $\U^B_{N,t,k}$ satisfies
\begin{align*}
\U^B_{N,t,k} 
&=   \frac{\lambda}{N} \sum_{s = t}^{\lfloor \beta N \rfloor-1} \Pr(\rho_1 \geq s, g^*_{t-1} = k \mid \C_N)\\
& \quad +  \frac{\indic{B>0}}{1-\lambda} \sum_{s = t}^{\lfloor \beta N \rfloor-1} \Pr(g^*_{t-1} = k, \rho_1 = s, R_s \neq 1 \mid \C_N) \U^{B-1}_{N,s+1,2} \\
&\quad + \Pr(\A^B_t(\alpha, \beta) \text{ succeeds},  g^*_{t-1} = k, \rho_1 \geq \beta N \mid \C_N) + O\big(\tfrac{1}{N}\big)\;.\\
\end{align*}
\end{lemma}



To establish this lemma, we study the probability distribution of $\rho_1$ and examine the algorithm's success probability following the first comparison. Essentially, assuming that $\rho_1 = s \in \{t, \ldots, \lfloor \beta N\rfloor -1 \}$, the first sum corresponds to the success probability if $R_s = 1$ and the algorithm selects the candidate $x_s$. The terms of the second sum represent the success probability after using a comparison at step $s$ but observing $R_t \neq 1$, resulting in the rejection of the candidate. Therefore, the available budget at step $s+1$ is $B-1$, and necessarily $g^*_s = 2$, because a comparison at step $s$ can only occur if $g_s = 1$ by definition of the algorithm. Hence, only the term $\U^{B-1}_{N,s+1,2}$ appears in the recursion, not $\U^{B-1}_{N,s+1,1}$. Finally, the last term represents the probability of success if no comparison has been made before step $\lfloor \beta N \rfloor$. By computing the probabilities outlined in the lemma for all $t \leq s \leq \lfloor \beta N \rfloor -1$ and $k \in {1,2}$, we derive more explicit recursive formulas satisfied by the limit of $\U^{B}_{N,t,k}$ when $N \to \infty$, for $k=2$ and $k=1$ respectively in Lemma \ref{lem:lim-2grp-k=2} and \ref{lem:lim-2grp-k=1}.
















\begin{lemma}\label{lem:lim-2grp-k=2}
For all $B \geq 0$ and $w \in [\alpha, \beta]$, the limit $\phi^B_2(\alpha,\beta; w) = \lim_{N \to \infty}\U^B_{N,\lfloor w N \rfloor,2}$ exists, 
and it satisfies the following recursion
\begin{align*}
\phi^B_2(\alpha,\beta;w)
&= - \lambda w \log\big((1-\lambda) \tfrac{w}{\beta} + \lambda \big) + \frac{(1-\lambda)\beta w^2 }{(1-\lambda)w + \lambda \beta}  \sum_{b = 0}^B \left( \frac{1}{\beta} - \sum_{\ell = 0}^b \frac{\log(1/\beta)^\ell}{\ell !} \right)\\
& \quad + \indic{B > 0} w^2 \int_w^\beta \frac{ (1-\lambda)^2 w + \lambda(2-\lambda)u}{((1-\lambda)w+\lambda u)^2 u^2}  \phi^{B-1}_2(\alpha, \beta; u) du\;.
\end{align*}
Moreover, $\U^B_{N,\lfloor w N \rfloor,2} = \phi^B_2(\alpha,\beta; w) + O\Big( \sqrt{\tfrac{\log N}{N}} \Big)$.
\end{lemma}




























\begin{lemma}\label{lem:lim-2grp-k=1}
For all $B \geq 0$ and $w \in [\alpha, \beta]$, the limit $\phi^B_1(\alpha,\beta; w) = \lim_{N \to \infty}\U^B_{N,\lfloor w N \rfloor,1}$ exists, 
and it satisfies the following recursion
\begin{align*}
\phi^B_1(\alpha,\beta;w)
&= \lambda w \log\big(1-\lambda + \lambda \tfrac{\beta}{w}\big) 
+ \frac{\lambda w \beta^2}{(1-\lambda) w + \lambda \beta} \sum_{b = 0}^B \left( \frac{1}{\beta} - \sum_{\ell = 0}^b \frac{\log(1/\beta)^\ell}{\ell !} \right) \\
& \quad + \indic{B > 0} \lambda^2 w \int_w^\beta \frac{(u-w) \phi^{B-1}_2(\alpha, \beta; u)}{((1-\lambda)w + \lambda u)^2 u} du\;.
\end{align*}
Moreover, $\U^B_{N,\lfloor w N \rfloor,1} = \phi^B_1(\alpha,\beta; w) + O\Big( \sqrt{\tfrac{\log N}{N}} \Big)$.
\end{lemma}




Using the two preceding lemmas, we deduce that the limit as $N$ approaches infinity of the success probability of Algorithm $\A^B_{\lfloor w N\rfloor}$, conditioned on the event $\C_N$, exists and equals $\phi_1^B(\alpha,\beta;w) + \phi_1^B(\alpha,\beta;w)$. Additionally, by applying Lemma \ref{lem:pr-cond-C_N}, we eliminate the conditioning on $\C_N$, thus proving the following theorem.






\begin{theorem}\label{thm:success-2grps}
For all $0 < \alpha \leq \beta \leq 1$, The success probability of Algorithm $\A^B(\alpha,\beta)$ satisfies
\begin{align*}
\Pr&(\A^B(\alpha,\beta) \text{ succeeds})\\
&= \lambda \alpha \log\big(\tfrac{\beta}{\alpha}\big) + \alpha \beta \sum_{b = 0}^B \left( \frac{1}{\beta} - \sum_{\ell = 0}^b \frac{\log(1/\beta)^\ell}{\ell !} \right)
+ \indic{B > 0} \alpha \int_\alpha^\beta \frac{ \phi^{B-1}_2(\alpha, \beta; u) du}{u^2} + O\Big(\sqrt{\tfrac{\log N}{N}}\Big)\;,
\end{align*}
with $\phi_2^B(\alpha,\beta;\cdot)$ defined in Lemma \ref{lem:lim-2grp-k=2}.
\end{theorem}




Theorem \ref{thm:success-2grps} provides an expression for the success probability of Algorithm $\A^B(\alpha, \beta)$, which involves the function $\phi^2(\alpha, \beta; \cdot)$. This function is defined recursively in Lemma \ref{lem:lim-2grp-k=2}. However, exploiting this recursion analytically to derive a closed expression is challenging. By disregarding the term containing $\phi^2(\alpha, \beta; \cdot)$ in the theorem, we derive an analytical lower bound expressed as a function of the parameters $\lambda$, $B$, and the thresholds $\alpha, \beta$. This lower bound is more manageable, allowing an effective threshold selection. In the subsequent discussion, for all $w \in (0,1]$ and $B \geq 0$, we denote by $S^B(w)$ the following sum:
\[
S^B(w) = \sum_{b = 0}^B \left( \frac{1}{w} - \sum_{\ell = 0}^b \frac{\log(1/w)^\ell}{\ell !} \right)\;.
\]

\begin{corollary}\label{cor:lb2grps}
For all $\beta \in (0,1)$, let
\[h^B(\beta): \min \left\{ \frac{\beta}{e} \exp\left(\frac{\beta S^B(\beta)}{\lambda}  \right)\, , \, \beta  \right\}\;.
\]
If $\lambda \geq 1/2$, taking thresholds $\tilde{\alpha}_B, \tilde{\beta}_B$ such that $\tilde{\beta}_B$ minimizes the mapping
\[
\beta \mapsto \lambda h(\beta) \log\big(\tfrac{\beta}{h^B(\beta)}\big) + h^B(\beta) \beta S^B(\beta)
\]
on $[0,1]$, and $\tilde{\alpha}_B = h^B(\tilde{\beta}_B)$, guarantees 
\[
\lim_{N \to \infty} \Pr(\A^B(\tilde{\alpha}_B,\tilde{\beta}_B) \text{ succeeds}) 
\geq \frac{1}{e} - \min\left\{ \frac{1}{e(B+1)!}, (\tfrac{4}{e}-1)\lambda(1-\lambda) \right\}\;.
\]
\end{corollary}



If $\lambda < 1/2$, then by symmetry, choosing adequate thresholds yields the same lower bound on the asymptotic success probability of $\A^B(\alpha, \beta)$.
In contrast to the single-threshold algorithm, employing a distinct threshold for each group enables achieving a success probability approaching $1/e$ both when the budget increases and when $\lambda$ approaches $0$ or $1$, as proved in Corollary \ref{cor:lb2grps}.











