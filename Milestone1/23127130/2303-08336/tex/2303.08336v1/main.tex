%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}
\usepackage{subfigure}
\usepackage{algorithm}
\usepackage{algcompatible}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Progressive Frame Patching for FoV-based Point Cloud  Video Streaming}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
% \author{Ben Trovato}
% \authornote{Both authors contributed equally to this research.}
% \email{trovato@corporation.com}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
% \affiliation{%
%   \institution{Institute for Clarity in Documentation}
%   \streetaddress{P.O. Box 1212}
%   \city{Dublin}
%   \state{Ohio}
%   \country{USA}
%   \postcode{43017-6221}
% }

\author{Tongyu Zong}
\affiliation{%
  \institution{New York University}
  \city{Brooklyn}
  \country{USA}}
\email{tz1178@nyu.edu}

\author{Yixiang Mao}
\affiliation{%
  \institution{New York University}
  \city{Brooklyn}
  \country{USA}}
\email{ym1496@nyu.edu}

\author{Chen Li}
\affiliation{%
  \institution{New York University}
  \city{Brooklyn}
  \country{USA}}
\email{cl5089@nyu.edu}

\author{Yong Liu}
\affiliation{%
  \institution{New York University}
  \city{Brooklyn}
  \country{USA}}
\email{yongliu@nyu.edu}

\author{Yao Wang}
\affiliation{%
  \institution{New York University}
  \city{Brooklyn}
  \country{USA}}
\email{yw523@nyu.edu}

% \author{John Smith}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \streetaddress{1 Th{\o}rv{\"a}ld Circle}
%   \city{Hekla}
%   \country{Iceland}}
% \email{jsmith@affiliation.org}

% \author{Julius P. Kumquat}
% \affiliation{%
%   \institution{The Kumquat Consortium}
%   \city{New York}
%   \country{USA}}
% \email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
% \renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  Immersive multimedia applications, such as Virtual, Augmented and Mixed Reality, have become more and more practical and portable with the advances in hardware and software for acquiring and rendering 3D media as well as 5G/6G wireless networks. Many such applications require the delivery of volumetric video to users with six degrees of freedom (6-DoF) movements. Point Cloud has become a popular volumetric video format due to its flexibility and simplicity. A dense point cloud consumes much higher bandwidth than a 2D/360 degree video frame. User Field of View (FoV) is more dynamic with 6-DoF movement than 3-DoF movement. A user's view quality of a 3D object is affected by points occlusion and distance, which are constantly changing with user and object movements. To save bandwidth, FoV-adaptive streaming predicts a user's FoV and only downloads point cloud data falling in the predicted FoV. However, it is vulnerable to FoV prediction errors, which can be significant when a long buffer is utilized for smoothed streaming. In this work, we propose a multi-round progressive refinement framework for point cloud-based volumetric video streaming. Instead of sequentially downloading point cloud frames, our solution simultaneously downloads/patches multiple frames falling into a sliding time-window, leveraging on the inherent scalability of point-cloud coding. The optimal rate allocation among all tiles of active frames are solved analytically using the heterogeneous tile rate-quality functions calibrated by the predicted user  FoV. Multi-frame downloading/patching simultaneously takes advantage of the streaming smoothness resulted from long buffer and the FoV prediction accuracy at short buffer length. We evaluate our streaming solution using simulations driven by real point cloud videos, real bandwidth traces, and 6-DoF FoV traces of real users. The experiments demonstrate that our solution is robust against the bandwidth/FoV prediction errors, and can delivery high and smooth view quality in the face of bandwidth variations and dynamic user and point cloud movements.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
% \begin{CCSXML}
% <ccs2012>
%  <concept>
%   <concept_id>10010520.10010553.10010562</concept_id>
%   <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%   <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10010520.10010575.10010755</concept_id>
%   <concept_desc>Computer systems organization~Redundancy</concept_desc>
%   <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10010520.10010553.10010554</concept_id>
%   <concept_desc>Computer systems organization~Robotics</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10003033.10003083.10003095</concept_id>
%   <concept_desc>Networks~Network reliability</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
% </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Computer systems organization~Embedded systems}
% \ccsdesc[300]{Computer systems organization~Redundancy}
% \ccsdesc{Computer systems organization~Robotics}
% \ccsdesc[100]{Networks~Network reliability}
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002951.10003227.10003251.10003255</concept_id>
       <concept_desc>Information systems~Multimedia streaming</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Multimedia streaming}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{point cloud video, video streaming, progressive, view-adaptive, KKT}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
\label{sec:intro}
Volumetric video streaming will take telepresence to the next level by delivering full-fledged 3D information of the remote scene and facilitating six-degree-of-freedom (6-DoF) viewpoint selection to create a truly immersive experience. With the  fascinating advances in the key enabling technologies, ranging from high-fidelity volumetric capturing, 3D computer vision processing, high-throughput and low-latency 5G/6G data delivery, to economically-viable volumetric rendering, we are now at the verge of  completing the puzzle of teleporting holograms of real-world humans/creatures/objects through the global Internet to realize the full potentials of  Virtual/Augmented/Mixed Reality (XR in short).

Leaping from planar (2D) to volumetric (3D) video poses significant communication and computation challenges. 
%for volumetric video processing and delivery. 
A volumetric video consists of a sequence of frames that characterize the motions of one or multiple physical/virtual objects. Each frame is a 3D snapshot, in the form of point cloud or surface mesh\footnote{We will focus on the point cloud representation in this  work, although the proposed method can be extended to the mesh-based representation.}, captured by a 3D scanner, e.g., LiDAR camera, or a camera array using photogrammetry. A high-fidelity point cloud frame of a single object can easily contain one million points, each of which has three 32-bit Cartesian coordinates and three 8-bit color attributes. At 30 frames/second, the raw data rate of a point cloud video (PCV) of a single object reaches 3.6 Gbps. The raw data rate required to describe a 3D scene with multiple objects increases  proportionally.  
% Meanwhile, compressing high-volume PCV %\cite{li_jia2021convolutional} 
% for bandwidth-efficient transmission can consume substantial 
% computation resources on the source side. 

Meanwhile, on the receiver side, a PCV viewer enjoys 6-DoF viewpoint selection through translational ({\it x, y, z}) and head rotational ({\it pitch, yaw, roll}) movements. 
% %Viewer movements and PCV object motions are projected to the same virtual/augmented environment to create the illusion of co-existence and interaction.  
Given the relative position between the object and the viewer's viewpoint, the compressed PCV will be decompressed and rendered on a 2D or 3D display\footnote{3D displays, such as light-field or laser-based hologram displays, are still not accessible for most users. We will consider FoV-based 2D PCV rendering on flatscreens widely available on computers, TVs, mobile devices, and head-mount-displays. Our designs are applicable for PCV streaming to 3D displays when  available.  
}, which also consumes significant computation resources. %\cite{li_li2022plenoptic}. 
Furthermore, to facilitate seamless interaction and avoid motion sickness, the rendered view has to be delivered with short latency (e.g. $<$20 ms) after the viewer movement, the so called Motion-to-Photon (MTP) latency constraint \cite{cuervo2018creating_motion_to_photon_latency}. As a result,  PCV not only consumes more bandwidth and computation resources, it is additionally subject to stringent end-to-end latency budget for processing and delivery. 

{\it The goal of this work is to address the high-bandwidth, high-complexity and low-latency challenges of point cloud video by designing a novel progressive refinement streaming framework, where multiple frames falling into a sliding time window are patched simultaneously for multiple rounds, leveraging on the inherent scalability of point-cloud coding.} We will focus on video-on-demand applications, leaving the live PCV streaming and the ultimate challenge of realtime two-way interactive PCV streaming for future research. 

Towards developing this progressive streaming framework, we made the following contributions:
\begin{enumerate}
\item We design a novel {\it sliding-window based  progressive streaming framework} to gradually refine the spatial resolution of each tile as its playback time approaches and the FoV prediction accuracy improves. We investigate the trade-off between the need of  long-buffer for absorbing bandwidth variations  and the need of short-buffer for accurate FoV prediction.
% We investigate the {\it computation-communication trade-off in progressive downloading and post-processing}. 
\item We propose a novel point cloud tile rate-quality model that reflects real user Quality of Experience (QoE) both theoretically and empirically.

\item The tile rate allocation decision is  formulated as an utility maximization problem. To get the optimal solution, an analytical algorithm based on Karush–Kuhn–Tucker (KKT) conditions is developed.
% \item We further formulate the rate allocation problem as an optimal control decision making problem and solve it with iterative Linear Quadratic Regulator (iLQR), which further enhances the Quality of Experience (QoE) performance compared to KKT-based algorithm due to its calculation of future impacts of every action.

\item The superiority of our proposed algorithm is evaluated using PCV streaming simulations with rendered visual results.
\end{enumerate}

\section{Related Work}
\label{sec:related}
Impressive research have been accomplished for coding~\cite{360Coding1,360Coding2,liu_zhou2022exploring,Winstein_hsiao2022towards},  streaming~\cite{Jiasi2017,Nahrstedt_zink2019scalable,jiang_guan2019pano,qiu_he2018rubiks,360Delivery1,li2019very,Duanmu:SigComm,Liyang_MMSys18,sun2019twotier,mao2020low, sun2020flocking,liu_jiang2020qurate,Jacob2,qian_wang2022salientvr}, FoV prediction~\cite{bao2016shooting,Zhisheng_MM18,Nahrstedt_park2020seaware}, and edge-assisted delivery~\cite{mao_dai2019view,Chakareski_liu2020delivering,Nahrstedt_sarkar2021l3bou} of 360$^o$  video recently. 
%We have applied a subset of the previously stated design principles to  360$^o$  video coding and  streaming~\cite{Duanmu:ISCAS, Duanmu:SigComm,Liyang_MMSys18,sun2019twotier,mao2020low, sun2020flocking,Flocking-TMM22,li2019very}. 
However, a 360$^o$  video only covers the whole viewing sphere captured from a center position, 
%records 2D projection along each direction from the center, 
and is typically projected onto a 2D plane  and processed as a planar video. A viewer mostly makes 3-DoF head rotational movements. 
%rotate her head in 2-DoF (pitch and yaw). 
A PCV records the 3D space directly and allows 6-DoF viewpoint selection from different positions and angles.  



Several early PCV streaming systems, e.g.~\cite{park2019rate,qian_han2020vivo,groot2020,qian_zhang2022yuzu,qian_liu2022vues}, have shown promises.
% the design space of PCV coding, streaming, and FoV prediction is widely open to be holistically explored.
%They precode the PCV into multiple rate versions for rate adaptation. 
For example the ViVo system \cite{qian_han2020vivo} adopts frame-wise k-d tree representation using DRACO \cite{Draco}, which is simple but less efficient than the recently established MPEG G-PCC standard. It uses tile-based coding to enable FoV adaptation and employ non-scalable multiple-rate versions of the same PCV for rate adaptation. Moreover, it only performs prefetching over a short interval ahead, leading to limited robustness to network dynamics. Another study \cite{qian_zhang2022yuzu} considered delivering low-resolution PCV and used deep-learning models to enhance the resolution on  the receiver, which enhances coding efficiency but does not facilitate FoV adaptation. \cite{hosseini2018dynamic} extends the concept of dynamic adaptive streaming over HTTP (DASH) towards DASH-PC (DASH-Point Cloud) to achieve bandwidth and view adaptation for PCV streaming. It proposes a clustering-based sub-sampling approach for adaptive rate allocation, while our work is able to optimize the rate allocation explicitly based on the FoV prediction accuracy and tile utility function with diverse coefficients. \cite{park2019rate} proposes a window-based streaming buffer that supports the update of tile rates all over the buffer. However, it didn't exploit scalable coding for point cloud tiles and the tile rate allocation problem is solved by a heuristic greedy algorithm that may not obtain the optimal solution. Furthermore, it adopts a rather simple tile utility function that is not precise enough to model the true user QoE. We propose a more refined utility model that better reflect viewer's visual experience when viewing a point cloud tile from certain distance. Also we develop a theoretically optimal tile rate allocation algorithm based on the KKT Condition that outperforms the heuristic algorithm of \cite{park2019rate} in our evaluation.
%The tiling idea for 360 video was extended to PCV to achieve FoV-adaptive streaming, and each frame was delivered  independently. 

Most of the existing studies focused on streaming PCV of a single object without \textit{explicit} rate control. In this work, we  assume octree-based scalable PCV coding which simultaneously enables spatial scalability and FoV adaptation with fine granularity. Our proposed progressive streaming framework takes full advantage of scalable  PCV coding to enhance streaming robustness. 




%\noindent{\bf Point Cloud Video-on-Demand:}  

%\noindent{\bf Live Broadcast of Point Cloud:}  

\section{FoV-adaptive Coding and Streaming}
\label{sec:formulation}

\begin{figure*}[htb]
\centering{
\subfigure[Point Cloud in Voxel Space \label{fig:voxel}]{\includegraphics[width=0.3\linewidth,height=1.6in]{figs/cube.eps}} 
\hspace{0in}
\subfigure[Octree Representation \label{fig:octree1}]{\includegraphics[width=0.3\linewidth,height=1.6in]{figs/Octree.eps}}
\hspace{0in}
\subfigure[Tile-based Scalable PCV Frame Coding \label{fig:tiletree}]{\includegraphics[width=0.36\linewidth,height=1.6in]{figs/Tile_Tree.eps}}
}
\caption{Octree-based Scalable and FoV-adaptive Point Cloud Coding}
\label{fig:octree}
\end{figure*}


\subsection{Octree-based Scalable PCV Coding}
MPEG has considered point cloud coding (PCC) and recommended two different approaches \cite{graziosi2020overview}. The V-PCC approach projects a point cloud frame into multiple planes and assembles the projected planes into a single 2D frame, and code the resulting sequence of frames using previously established video coding standard HEVC. The G-PCC approach represents the geometry of all the points using an octree, and losslessly represents the octree using context-based entropy coding. The color attributes are coded following the octree structure as well using a wavelet-like transform. By leveraging the efficiency of HEVC, especially in exploiting temporal redundancy through motion compensated prediction, the V-PCC approach is more mature and is currently more efficient than the G-PCC for dense point clouds. However, V-PCC does not allow selective transmission of regions intersecting with the viewer's FoV or objects of interests. It is also not efficient for sparse clouds such as those captured by LiDAR sensors.  The spatial scalability can be indirectly enabled by coding the projected 2D video using spatial scalability, but this does not directly control the distance between the points and cannot be done at the region level.

A point cloud can be coded using an octree which is recursively constructed: the root node represents the entire space spanned by all the points, which is equally partitioned along the three dimensions into $2\times 2\times 2$ cubes; the root has eight children, each of which is the root of a sub-octree representing each of the eight cubes. The recursion stops at either an empty cube, or the maximum octree level $L$.  Each sub-octree rooted at level $l$ represents a cube with side lengths that are $1/2^l$ of the side lengths of the original space. To code the geometry, the root node records the $(x,y,z)$ coordinates of the center of the space, based on which the coordinates of the center of any cube at any level are uniquely determined. Consequently, each non-root node only needs one bit to indicate whether the corresponding cube is empty or not. For the octree in Fig.~\ref{fig:octree1}, the nodes with value $1$ at the three levels represent one, two and three non-empty cubes with side lengths of $1/2$, $1/4$ and $1/8$ in Fig.~\ref{fig:voxel}, respectively. All the non-empty nodes at level $l$ collectively represent the geometry information of the point cloud at the spatial granularity of $1/2^l$. This makes octree {\it spatially scalable}: with one more level of nodes delivered, the spatial resolution doubles (i.e., the distance between points is halved). The color attributes of a point cloud are coded following the octree structure as well, with each node stores the average color attributes of all the points in the corresponding cube. Scalable color coding can be achieved by only coding  the difference between a node and its parent. The serialized octree can be further losslessly compressed using entropy coding. With MPEG G-PCC, at each tree level $l$, the  status of a node ($0$ or $1$) is coded using context-based arithmetic coding, which uses a context consisting of its neighboring siblings that have been coded and its neighboring parent nodes to estimate the probability $p$ that the node is $1$.

% {\bf Yixiang: ...add description about the PCV compression scheme used in the evaluation.... entropy coding}

\subsection{Tile-based Designs}
 Considering the limited viewer Field-of-View (FoV) (dependent on the 6-DoF viewpoint), occlusions between objects and parts of the same object, and the reduced human visual sensitivity at long distances, only a subset of points of a PCV frame are {\it visible and discernible} to the viewer at any given time. FoV-adaptive PCV  streaming significantly reduces PCV bandwidth consumption by only streaming points visible to the viewer at the spatial granularities that are discernible at the current viewing distance. To support FoV-adaptive streaming, octree nodes are partitioned into slices that are selectively transmitted for {\bf FoV adaptability} and {\bf spatial scalability}. Each slice consists of a subset of nodes at one tree level that are independently decodable provided that the slice containing their parents is received. Without considering FoV adaptability, one can simply put all nodes in each tree level into a slice to achieve spatial scalability. The  sender will send slices from the top level to the lowest level allowed by the rate budget. To enable FoV adaptability, a popular approach known as tile-based coding is to partition the entire space into non-overlapping 3D tiles and code each tile independently using a separate tile octree. Only tiles  falling into the predicted FoV will be transmitted. To enable spatial scalability within a tile, we need to put the nodes at each level of the tile octree into a separate slice. As illustrated in Fig.~\ref{fig:tiletree}, each sub-octree rooted at level $L_T$ represents a 3D-tile with side length of $1/2^{L_T}$. Within each tile sub-octree, nodes down to the $L_B$ level are packaged into a base layer slice, and nodes at the lower layers are packaged into additional enhancement layer slices. 

When the streaming server is close to the viewer, one can conduct reactive FoV-adaptive streaming: the client collects and uploads the viewer's FoV to the server; the server renders the view within the FoV, and streams the rendered view as a 2D video to the viewer. To facilitate seamless interaction and avoid motion sickness, the rendered view has to be delivered with short latency (e.g. $<$20 ms) after the viewer movement, the so called Motion-to-Photon (MTP) latency constraint \cite{cuervo2018creating_motion_to_photon_latency}. To address the MTP challenge, we consider predictive streaming that predicts the viewer's FoVs for future frames and prefetches tiles within the predicted FoV~\cite{park2019rate,qian_han2020vivo,groot2020}. 

%A smaller tile enables finer granularity for FoV-based pruning, but substantially reduces the coding efficiency because: 1) each tile octree is coded independently with the context for entropy coding confined within the same tile tree, leading to low coding efficiency; 2) the tile root locations are coded independently within the slice header, and the header overhead dominates the slices in the first few levels; 3) grouping the first few levels into a single slice can reduce the header overhead, but it will further reduce the number of spatial layers.



 



%The Quality-of-Experience (QoE) of PCV viewers is determined by many factors, including the rendered frame quality and quality variations,  streaming continuity and latency, and responsiveness to viewpoint changes, etc.


\section{Progressive PCV Streaming}
Due to the close interaction with PCV objects, viewers are highly sensitive to  QoE impairments, such as black screen, freezes, restarts, excessively long latency, etc. Not only the network and viewer dynamics have direct QoE impacts, bandwidth and FoV prediction errors are also critical for the  effectiveness of predictive streaming. We propose a novel {\it progressive FoV-adaptive PCV  streaming design} to minimize the occurrence of the above impairments and deliver a high level of viewer QoE. 
%in the face of the network and viewer dynamics as well as the errors of predicting them.
%In a typical video streaming session, once the playback starts on the client side, all the video frames should be sequentially rendered and displayed for smooth streaming. 
\subsection{Progressive Downloading/Patching}
Most of the existing FoV-adaptive streaming solutions can be categorized as  {\it Sequential-Decision-Making (SDM)}: at some time point $\tau$ before the playback deadline $t$ of a frame, one predicts the viewer FoV at $t$,  downloads tiles falling into the predicted FoV at video rates determined by some rate adaptation algorithm, then repeats the process for the next frame. $t-\tau$ is the time interval for both FoV prediction and frame pre-feteching, which is upper bounded by the client side video buffer length. To achieve smooth streaming, a long pre-fetching interval is preferred to absorb the negative impact of  bandwidth variations.  
However, FoV prediction accuracy decays significantly at long prediction intervals. We have studied the optimal trade-off for setting the buffer length for on-demand streaming of 360$^o$ video in~\cite{Liyang_MMSys18,sun2019twotier}. 


 Scalable PCV coding opens up a new dimension to reconcile the conflicting desires for long streaming buffer and short FoV prediction interval. It allows us to {\bf progressively download and patch} tiles: when a frame's playback deadline is still far ahead, we only have a rough FoV estimate for it, and will download  low resolution slices overlapping with the predicted FoV; as the deadline approaches, we have more accurate FoV prediction, and will {\it patch} the frame by downloading additional enhancement slices. {\it Progressive streaming is promising to simultaneously achieve streaming smoothness and robustness against bandwidth variations and FoV prediction errors.} On one hand, as shown in Fig.~\ref{fig:seg_download}, each tile in each segment is downloaded over multiple rounds, the interval of which is $\Delta$. A segment consists of $S$ frames whose duration is also $\Delta$. At each download round, multiple segments are download simultaneously. And the final rendered quality of each tile is a function of the total downloaded rate (thanks to the scalable coding, there is minimal information redundancy between the multiple progressive downloads). The final rendered quality is less vulnerable to network bandwidth variations and FoV prediction errors at individual time instants. On the other hand, tile downloading and patching are guided by FoV predictions at multiple time instants with accuracy improving over time. If a tile falls into the predicted FoV in multiple rounds, the likelihood that it will fall in the actual FoV is high, and its quality will be {\it reinforced} by progressive downloading; if a tile only shows up once in the predicted FoV when its playback time is still far ahead, the chance for it to be in the actual FoV is small. Fortunately, the bandwidth wasted in downloading the low-resolution slices  is low.  


%Progressive streaming conducts {\bf Parallel-Decision-Making (PDM)}. For a tile $k$ in the frame to be rendered at $t$, we prefetch it in the previous $w$ rounds, from $t-w$ to $t-1$. Let $r_i(t,k)$ denote its download rate in round $t-i$. The final rate for this tile is $\sum_{i=1}^w r_i(t,k)$. Meanwhile, at download time $\tau$, all tiles of frames with playback time from $\tau+1$ to $\tau+w$ are being downloaded/patched, and the total rate is bounded by the available bandwidth, i.e., $\sum_{i=1}^{w} \sum_k r_i(\tau+i,k) \le B(\tau)$. One greedy solution for bandwidth allocation is to maximize the total expected quality enhancements for all the active frames, based on the current FoV estimations for them. Since FoV estimate improves as playback time approaches,  we should assign decreasing weights for frames $\tau+1$ to $\tau+w$ in the objective function.  
\subsection{Optimal Rate Allocation}
\label{sec:optimal}
Progressive streaming conducts {\bf Parallel-Decision-Making (PDM)}. For a tile $k$ in the frame to be rendered at $t$, we prefetch it in the previous $I$ rounds, from $t-I$ to $t-1$. Let $r_i(t,k)$ denote its download rate in round $t-i$. The final rate for this tile is $\sum_{i=1}^I r_i(t,k)$. 
Meanwhile, at download time $\tau$, all tiles of frames with playback time from $\tau+1$ to $\tau+I$ are being downloaded/patched at rates $\{r_i(\tau+i,k), 1 \le i \le I, \forall k\}$, and the total rate is bounded by the available bandwidth $B(\tau)$, i.e., \[\sum_{i=1}^{I} \sum_k r_i(\tau+i,k) \le B(\tau).\] 
One greedy solution for bandwidth allocation is to maximize the expected quality enhancements for all the active frames, based on the current FoV estimations:
\begin{align}
\label{eq:progressive}
& \underset{\{r_i(\tau+i,k)\}} {\mathbf{max}} \sum_{i=1}^{I} \sum_k w_i \Tilde{p}_{\tau+i,k} \nonumber \\ 
& \cdot \left \{Q\left(\sum_{j=i}^I r_j(\tau+i,k),\Tilde{d}_{\tau+i,k}\right) -Q\left(\sum_{j=i+1}^{I}r_j(\tau+i,k), \Tilde{d}_{\tau+i,k}\right)\right\},
\end{align}
where $Q(r,d)$ reflects the rendered quality of a tile at rate $r$ when viewed from distance $d$, $\Tilde{p}_{\tau+i,k}$ and $\Tilde{d}_{\tau+i,k}$ are the predicted view likelihood and view distance for tile $k$ of frame $\tau+i$, and $w_i$ is a  decreasing function of $i$, considering that FoV estimate accuracy drops for far away frames. Meanwhile, tile patching at time $\tau$ operates on top of the layers downloaded by the end of $\tau-1$, bandwidth allocation at time $\tau$ should also consider its impacts for the quality enhancements in the future rounds, leading to a stochastic optimal control problem.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.8\linewidth]{figs/seg_download.eps}}
\caption{Progressive Streaming Example with Sliding-window Size of $2$: in round $(i+l)\Delta$, all frames within segment $i+2$ are being downloaded for the first time, while frames within segment $i+1$ are being patched with enhancement layers. Frames of segment $i+2$ will be patched in the next round $(i+l+1)\Delta$.}
\label{fig:seg_download}
\end{figure}

\subsection{View-distance based Tile Utility Model}
\label{sec:qr_model_tile}
In this section, we explain in detail the proposed tile utility model $Q(r,d)$ in Equation~(\ref{eq:progressive}). Tile quality depends on its angular resolution $f_{ang}$, which is the number of points per degree that are visible to the user within the tile. There are many subjective study about the quality of rendered images or videos, but most of them only consider the impact of rate, while very few consider the distance between the viewer and the object, which can be very dynamic in PCV streaming. A subjective study in~\cite{qian_han2020vivo} suggests that distance have a significant impact on user QoE when users observe rendered point cloud object at different distances, but it didn't provide a specific utility model. A quality model of image with respect to both distance and resolution has been proposed in~\cite{westerink1989subjective}, but it doesn't fit point cloud video. For the traditional 2D video frames/tiles, we can infer the direct mapping from rate to quality, which is usually a logarithm function. However, it is not that clear how rate impacts quality of point cloud tile. Fortunately, it's easier to infer the logarithmic impact of angular resolution $f_{ang}$, which is the number of points per degree within a small tile. With more and more points inside the tile, the per-degree utility increases but more and more slowly. Therefore, let $log(c\cdot f_{ang})$ be the per-degree quality of a tile. Then,
\[Q(f_{ang}, \theta)=\theta \cdot log(c\cdot f_{ang}),\]
where $c$ is a constant factor to be determined by the saturation threshold. As shown in Fig.~\ref{fig:ang_resol}, $f_{ang}$ is decided  by the level of detail (LoD), which is the tile's octree level $L$, and the viewer's span-in-degree $\theta$ across the tile:
\[f_{ang}=\frac{2^L}{\theta}=\frac{d\cdot 2^L\cdot\pi}{l\cdot 180},\]
where $d$ is the distance between the viewer and the tile. Therefore, the tile utility model is:
\[Q(L,d)=log(f_{ang})=\frac{M}{d} \cdot log(c\cdot \frac{d\cdot 2^L}{M}),\]
where $M=l\cdot 180/\pi$.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\linewidth]{figs/ang_resol.eps}}
\caption{Tile Angular Resolution dependent on distance between the user and the tile as well as LoD of the tile.}
\label{fig:ang_resol}
\end{figure}

To achieve a level of detail $L$, all nodes up to level $L$ of the tile octree has to be delivered. Due to tile diversity, the data rate needed to code octree nodes up to level $L$ is highly tile-dependent. In other words, given a coding rate budget $r$, the achievable LoD is tile-dependent. Let $L_{i,k}(r)$ be the rate-LoD mapping for tile $k$ within frame $i$. In this work, we fit the mapping for every coded tile and each tile maintains a unique set of parameters:
\[L_{i,k}(r)=a_{i,k}log(b_{i,k}r+1),\]
which fits well with the actual data encoded by G-PCC, and it ensures $L_{i,k}=0$ at $r=0$. Thus,
\begin{equation}
\label{eq:qr_model}
Q_{i,k}(r,d)=\frac{M}{d} \cdot \left(a_{i,k}log2\cdot log(b_{i,k}\cdot r+1)+log(\frac{c\cdot d}{M})\right).
\end{equation}
Note that tile utility is not necessarily $0$ at $r=0$, because there is possibly one point inside the tile. For simplicity, we ignore the subscripts of $Q_{i,k}$ as $Q$.

Taking a tile for example, the utility curve with respect to tile rate and distance is shown below.
\begin{figure}[htbp]
\centerline{\includegraphics[width=\linewidth]{figs/utility_distance0.eps}}
\caption{Tile Utility Curve}
\label{fig:tile_utility}
\end{figure}

\subsection{Water-filling Rate Allocation Algorithm}
\label{sec:kkt}
We now develop an analytical algorithm for solving the utility maximization problem formulated in Sec.~\ref{sec:optimal}. In Equation~(\ref{eq:progressive}), the second term  $Q\left(\sum_{j=i+1}^{I}r_j(\tau+i,k), \Tilde{d}_{\tau+i,k}\right)$ represents the tile quality of the layers that have been downloaded to the buffer befor the current round, which is a given constant for the rate optimization at the current round. The utility maximization problem for each round is simplified as below, along with the constraints:
\begin{subequations}
\label{eq:neat_progressive}
\begin{align}
\label{eq:neat_progressive_obj}
& \underset{\{r_i(\tau+i,k)\}} {\mathbf{max}} \sum_{i=1}^{I} \sum_k w_i \Tilde{p}_{\tau+i,k} \cdot Q\left(\sum_{j=i}^I r_j(\tau+i,k),\Tilde{d}_{\tau+i,k}\right) \\
& \text{\it subject to} \notag \\
& \label{KKT:C1} r_i(\tau+i,k)\ge 0, \quad \forall 1 \le \tau \le T+I-1, \forall 1 \le i \le I, \forall k,\\
   & \label{KKT:C2} \sum_{j=i}^I r_j(\tau+i,k) \le R(\tau +i,k), \quad \forall 1 \le \tau \le T+I-1, \forall 1 \le i \le I, \forall k,\\
   & \label{KKT:C3} \sum_{i=1}^{I} \sum_k r_i(\tau+i,k) \le B(\tau), \quad \forall 1 \le \tau \le T+I-1,
\end{align}
\end{subequations}
where $T$ is the video length, and $R(\tau+i,k)$ is the maximum rate of each tile $(\tau+i,k)$. The tile rate allocation problem in our case is similar to the typical water-filling problem, where we ``fill'' the tiles in an optimal order based on their significance determined by the existing rates, probability to be viewed, distance from the viewer, and the calibration weights $\{w_i\}$.

Equation~\ref{eq:neat_progressive} is a nonlinear programming problem so that the Karush–Kuhn–Tucker (KKT) conditions serve as the first-order necessary conditions. Furthermore, the KKT conditions for this problem are also sufficient due to the fact that the tile utility model $Q(r,d)$ is concave in terms of $r$, and the inequality constraints~\ref{KKT:C1} and \ref{KKT:C2} are continuously differentiable and convex, and \ref{KKT:C3} is affine. Therefore, we apply KKT conditions to optimally solve this tile rate allocation problem.

\begin{algorithm}
% {\fontsize{9pt}{9pt}\selectfont
\caption{KKT Condition based Tile Rate Allocation}
\label{alg:kkt_alloc}
  \begin{algorithmic}[1]
 	\STATEx  {\textbf{Input:} update window size $I$, point cloud video, video length $T$, user FoV trace, bandwidth trace, utility coefficients of all tiles.}
        \FOR {$\tau$ in 1:$T$}
            \STATE {Predict FoV for all the tiles in the update window,}
            \STATEx {\indent \indent get viewing probability of tiles: $\Tilde p_{\tau+i,k}, 1 \le i \le I, \forall k$}
            \STATEx {\indent \indent get viewing distances from user viewpoint to tiles: $\Tilde d_{\tau+i,k}$,}
            \STATE {Call KKT Condition based optimization algorithm}
            \STATE {Allocate tile rates based on the results}
            \STATE {Evaluate quality for frames being watched by user}
        \ENDFOR
	% \STATEx {\textbf{Output:} }
 %        \IF {higher resolution tile $c_{i,h}$ in cache, where $h \geq r$}
	% 			    \STATE {$hit++$}
	% \ELSIF{lower resolution tiles $c_{i,l}$ in cache, where $l < r$}
 %            \IF { $S_T(i,r) > S_T(i,l)$}  
 %                \STATE{replace $c_{i,l}$ with $c_{i,r}$}
 %            \ENDIF
 %        \ELSE
 %            \STATE{add $c_{i,r}$ in cache}
 %        \ENDIF
 %        \WHILE{total tile size in cache $>$ Cache Size}        
 %            \STATE{get the lowest $S_T(j,k)$ from cache}
 %            \STATE{remove $c_{j,k}$ from cache}
 %        \ENDWHILE     
\end{algorithmic}
\end{algorithm}

\section{Evaluation}
\label{sec:evaluation}

In this section, we demonstrate the superiority of the proposed approach compared with several baselines.

\subsection{Setup}
\subsubsection{Datasets}
The point cloud videos we are using are from 8i website~\cite{d20178i}, which includes 4 videos, each with $300$ frames and framerate $30$ fps. The user FoV trace is from \cite{subramanyam2020user} which involves $26$ participants watching looped frames of the 8i videos on Unity game engine. The bandwidth trace is from NYU Metropolitan Mobile Bandwidth Trace~\cite{mei2019realtime}, which includes 4G bandwidth traces collected in NYC.

\subsubsection{FoV Prediction and Bandwidth Prediction}
FoV prediction method used in this work is linear regression, while other approaches like neural network based prediction model can also be used in our proposed framework. We predict future bandwidth by harmonic mean.

\subsection{Baselines}
\label{sec:baseline}
\begin{enumerate}
% \item \textbf{Homogeneous Tiles}: Tiles in point cloud video are more heterogeneous compared to 360 video and 2D video, due to the fact that different tiles have different number of points at the highest density level. Therefore, we must consider this tile heterogeneity of utility model when optimizing Equation~\ref{eq:neat_progressive}. We demonstrate this point by comparing with homogeneous-tile experiment where we assume all the tiles have the same coefficients in the utility model.
% \item \textbf{Uniform Distance}: Distance between each tile and the user viewpoint  plays an important role when evaluating the tile utility in PCV streaming, which makes PCV extremely different from 360 video and 2D video. To demonstrate the importance of distance, in this baseline we assume all the tiles have the same distance from the user viewpoint.
\item \textbf{Non-progressive Downloading}: This baseline is similar to the traditional video streaming in 2D video where we keep downloading new segments at the distant end of buffer. Every frame has only one chance to be downloaded, based on just one FoV prediction.
% \item \textbf{Greedy heuristic}: One state-of-art window based tiles rate allocation allocation is proposed in~\cite{park2019rate}, in which authors proposed a greedy heuristic algorithm without scalable PCV coding.
\item \textbf{Equal allocation}: To demonstrate the effectiveness of KKT based optimization approach, we compare the naive baseline where available bandwidth is equally allocated over all active tiles falling into the predicted FoV at each round. 
\item \textbf{Distance-unaware utility}: Distance between each tile and the user viewpoint  plays an important role when evaluating the tile utility in PCV streaming, which makes PCV extremely different from 360 video and 2D video. To demonstrate the importance of distance-aware utility model, in this baseline we assume a tile's utility is independent of the viewing distance.
\end{enumerate}

\subsection{Experimental Results}
\label{sec:experiment}
For each scenario or algorithm, we run the simulation  with a real user's FoV trace~\cite{subramanyam2020user} over one of the four 8i videos, \texttt{Longdress}, for more than one minute. The sliding window size is $I=20s$, and it moves forward every one second. 

Table~\ref{tab:ang_resol} and \ref{tab:frame_quality_per_degree} showcase that when both the bandwidth and the FoV oracles are available (Scenario A), non-progressive streaming is comparable with progressive streaming. But in the realistic setting with network bandwidth variations and FoV prediction errors (Scenario B), 
progressive solutions with constant and decreasing frame weights can deliver much higher quality in most cases, measured using both the \textit{quality per degree} and the \textit{delivered angular resolution} or the number of points per viewing degree, averaged over all FoV tiles.

\begin{table}[htb]%
\centering
    \centering
    % \vspace{-8mm}
    {\small
    \begin{tabular}{c|c|c|c|c|c} \hline
       Test  & Non- & Equal- & Distance- & \multicolumn{2}{c}{Progressive KKT}   \\ \cline{5-6} 
       Scenario & Progressive & Split & Unaware & Constant & Exp \\ \hline 
       A & 18.97, 3.09 & 17.33, 1.63 & 19.52, 1.71 & 19.50, 1.66 & N/A \\ \hline
       B & 6.19, 6.06 & 15.09, 2.06 & 14.83, 2.07 & 15.48, 1.85 & 16.87, 1.78 \\  \hline 
       % C &  & 15.20, 2.08 & & \\  \hline 
    \end{tabular}
    }
    \caption{\small Average Frame-Angular-Resolution}
    \label{tab:ang_resol}
   % \vspace{-5mm}
\end{table}

% \begin{table*}[htb]%
% \centering
%     \centering
%     % \vspace{-8mm}
%     {\small
%     \begin{tabular}{c|c|c|c|c|c} \hline
%        Test  & Non- & Equal- & Distance- & \multicolumn{2}{c}{Progressive KKT}   \\ \cline{5-6} 
%        Scenario & Progressive & Split & Unaware & Constant & Exp \\ \hline 
%        A & 19.02, 3.03 & 17.33, 1.54 & 19.37, 1.60 & 19.31, 1.56 & N/A \\ \hline
%        B & 6.53, 5.78 & 15.14, 2.07 & 14.83, 2.07 & 15.06, 1.93 & 17.08, 2.01 \\  \hline 
%        % C &  &  & & \\  \hline 
%     \end{tabular}
%     }
%     \caption{\small Average Frame-Angular-Resolution}
%     \label{tab:ang_resol}
%    % \vspace{-5mm}
% \end{table*}

% \begin{table*}[htb]%
% \centering
%     \centering
%     % \vspace{-8mm}
%     {\small
%     \begin{tabular}{c|c|c|c|c|c} \hline
%        Test  & Non- & Equal- & Distance- & \multicolumn{2}{c}{Progressive KKT}   \\ \cline{5-6} 
%        Scenario & Progressive & Split & Unaware & Constant & Exp \\ \hline 
%        A & -61.48, 57.98 & -86.33, 36.01 & -48.31, 29.75 & -46.04, 28.10 & N/A \\ \hline
%        B & -806.90, 451.14 & -134.13, 59.25 & -131.01, 58.89 & -123.13, 52.25 & -94.40, 46.74 \\  \hline 
%        % C &  &  & & \\  \hline 
%     \end{tabular}
%     }
%     \caption{\small Average Frame Quality}
%     \label{tab:frame_quality}
%    % \vspace{-5mm}
% \end{table*}

\begin{table}[htb]%
\centering
    \centering
    % \vspace{-8mm}
    {\small
    \begin{tabular}{c|c|c|c|c|c} \hline
       Test  & Non- & Equal- & Distance- & \multicolumn{2}{c}{Progressive KKT}   \\ \cline{5-6} 
       Scenario & Progressive & Split & Unaware & Constant & Exp \\ \hline 
       A & 5.45, 0.42 & 5.34, 0.39 & 5.49, 0.39 & 5.50, 0.38 & N/A \\ \hline
       B & 2.50, 1.53 & 5.15, 0.44 & 5.13, 0.41 & 5.16, 0.39 & 5.33, 0.41 \\  \hline 
       % C &  &  & & \\  \hline 
    \end{tabular}
    }
    \caption{\small Average Per-Degree Frame Quality}
    \label{tab:frame_quality_per_degree}
   % \vspace{-5mm}
\end{table}

\begin{table}[htb]%
\centering
    \centering
    % \vspace{-8mm}
    {\small
    \begin{tabular}{c|c|c|c|c|c} \hline
       Test  & Non- & Equal- & Distance- & \multicolumn{2}{c}{Progressive KKT}   \\ \cline{5-6} 
       Scenario & Progressive & Split & Unaware & Constant & Exp \\ \hline 
       % A & -61.48, 57.98 & -86.33, 36.01 &  & -46.04, 28.10 & N/A \\ \hline
       B & 13.49 KB & 6.34 KB & 12.53 KB & 11.78 KB & 5.19 KB \\  \hline 
       % C &  &  & & \\  \hline 
    \end{tabular}
    }
    \caption{\small Average Per-frame Wasted Bandwidth, which is the bandwidth used to download tiles that are not in the user's actual FoV.}
    \label{tab:wasted_rates}
   % \vspace{-5mm}
\end{table}

The following subsections analyzes in details the results for each individualized comparison. The figures below reflect more details for the above tables.

\subsubsection{\textbf{KKT based Progressive Downloading V.S. nonProgressive Downloading}}
\hfill\\
\textbf{Quality Supremacy}: From Fig.~\ref{fig:KKT-const_non-prog_ang_resol_bw5} and Fig.~\ref{fig:KKT-const_non-prog_frame_quality_per_degree_bw5}, we observe that except for the first $600$ frames where the FoV prediction accuracy for both algorithms is accurate, KKT-const, which use constant weights for all frames in KKT calculation, dominates non-progressive baseline in terms of both angular resolution and per-degree quality by a large gap on average. The reason is that the non-progressive baseline predicts FoV for every frame 20 seconds ahead and download the tiles only once  based on the predicted FoV, which is absolutely not accurate due to the large prediction interval. Therefore, several frames have almost zero angular resolution and pretty low per-degree quality in this case. In contrast, by progressive downloading, KKT-const predicts FoV and improves the tiles' rates for every frame for $20$ times, whose prediction accuracy becomes more and more accurate over time.

\textbf{Smoothness}: The large quality variation suffered by nonprogressive baseline is also due to the bandwidth variations. At each downloading round of non-progressive baseline, we allocate all the predicted available bandwidth to just one segment consisting of $30$ frames that are about to enter the end of buffer. In contrast, at each downloading round of KKT-const, it updates all the frames in the buffer simultaneously based on the optimization algorithm, which smooths the resulting frame quality evolution dramatically.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\linewidth]{figs/KKT-const_non-prog_ang_resol_bw5.eps}}
\caption{Angular Resolution per Frame: KKT-const v.s. non-progressive}
\label{fig:KKT-const_non-prog_ang_resol_bw5}
\end{figure}

% \begin{figure}[htbp]
% \centerline{\includegraphics[width=\linewidth]{figs/KKT-exp_non-prog_ave_frame_quality_bw5.eps}}
% \caption{Frame Quality}
% \label{fig:frame_quality}
% \end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=\linewidth]{figs/KKT-const_non-prog_frame_quality_per_degree_bw5.eps}}
\caption{Frame Quality per Degree: KKT-const v.s. non-progressive}
\label{fig:KKT-const_non-prog_frame_quality_per_degree_bw5}
\end{figure}

\textbf{Bandwidth Consumption}: At each  downloading time of KKT-const, for the frames farther away from playing, e.g., 20s ahead, since the FoV prediction is not so accurate we just download a base layer for the tiles within the predicted FoV, which doesn't waste too much bandwidth; while for the frames closer to user playback time, we patch the tiles within the more accurately predicted FoV by downloading additional enhancement layers. However, non-progressive baseline allocates all the bandwidth to just one segment based on inaccurate FoV prediction. Therefore, in both Table~\ref{tab:wasted_rates} and Fig.~\ref{fig:KKT-const_non-prog_wasted_rates_bw5} we observe that KKT-const helps save a lot of bandwidth over all the frames. If the users are watching a long video consisting of tens of thousands of frames, the wasted bandwidth is a huge waste of costs.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\linewidth]{figs/KKT-const_non-prog_wasted_rates_bw5.eps}}
\caption{Wasted Bandwidth per Frame: KKT-const v.s. non-progressive}
\label{fig:KKT-const_non-prog_wasted_rates_bw5}
\end{figure}

\textbf{Correlation between Quality Improvement and FoV prediction Accuracy}: To further demonstrate the motivation to develop the progressive downloading framework, we zoom in to see the correlation between the improved quality and the FoV prediction accuracy. We first calculate the FoV prediction accuracy difference between the front position of the buffer and the end of the buffer. The accuracy is represented by the tile overlap ratio between the predicted FoV and the ground-truth FoV: the larger the better. The we calculate the quality improvement between KKT-const and non-progressive baseline. When we put the two curves together in Fig.~\ref{fig:KKT-const_non-prog_frame_quality_per_degree_with_fov_acc_bw5}, we find an obvious correlation in between: if the FoV prediction at the front of buffer (closer to user playback time) is \textit{much better} than that at the end of buffer (way ahead of user playback time), the quality improvement of KKT-const against the non-progressive is much larger. The pattern is more obvious in the scatter-plot in Fig.~\ref{fig:corr_fovAcc_diff_KKT-const_non-prog_frame_quality_per_degree_diff_bw5}, where one point represents the two characteristics of one frame. The correlation coefficient is $\rho=0.83$.


\begin{figure}[htbp]
\centerline{\includegraphics[width=\linewidth]{figs/KKT-const_non-prog_frame_quality_per_degree_with_fov_acc_bw5.eps}}
\caption{Clear correlation is observed between the improvement of KKT-const against non-progressive and FoV prediction accuracy difference between the buffer front and the buffer end.}
\label{fig:KKT-const_non-prog_frame_quality_per_degree_with_fov_acc_bw5}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=\linewidth]{figs/corr_fovAcc_diff_KKT-const_non-prog_frame_quality_per_degree_diff_bw5.eps}}
\caption{The correlation coefficient is as high as $0.83$.}
\label{fig:corr_fovAcc_diff_KKT-const_non-prog_frame_quality_per_degree_diff_bw5}
\end{figure}

\subsubsection{\textbf{KKT-exp V.S. KKT-const}}
We further explore the design space of setting frame weights $\{w_i\}$ in Equation~(\ref{eq:neat_progressive_obj}). The frame weights depend on the confidence about the accuracy of FoV prediction for each frame. Therefore, intuitively, it should decrease as the prediction interval increases. We tried several different weight settings, including linear decreasing, history FoV prediction accuracy based assignment, and exponentially decreasing in terms of prediction interval. We found the exponentially decreasing frame weights performs the best, as shown in Table~\ref{tab:ang_resol}, \ref{tab:frame_quality_per_degree} and \ref{tab:wasted_rates}, as well as in  Fig.~\ref{fig:diff_KKT-exp_KKT-const_frame_quality_per_degree_bw5} and \ref{fig:KKT-exp_KKT-const_wasted_rates_bw5}. KKT-exp further boosts the per-degree frame quality on top of KKT-const without increasing too much quality variations. From Fig.~\ref{fig:KKT-exp_KKT-const_wasted_rates_bw5} and Table~\ref{tab:wasted_rates} we see a large amount of bandwidth is further saved by KKT-exp.

% KKT-exp vs KKT-const
\begin{figure}[htbp]
\centerline{\includegraphics[width=\linewidth]{figs/diff_KKT-exp_KKT-const_frame_quality_per_degree_bw5.eps}}
\caption{KKT-exp v.s. KKT-const: Per-degree Frame Quality Improvement}
\label{fig:diff_KKT-exp_KKT-const_frame_quality_per_degree_bw5}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=\linewidth]{figs/KKT-exp_KKT-const_wasted_rates_bw5.eps}}
\caption{KKT-exp v.s. KKT-const: Wasted Bandwidth per Frame}
\label{fig:KKT-exp_KKT-const_wasted_rates_bw5}
\end{figure}

\subsubsection{\textbf{KKT-exp V.S. Equal Allocation}}
To demonstrate the superiority of the KKT-based optimization for the tile rate  allocation problem in PCV streaming, we design an equal allocation baseline, where the predicted available bandwidth is equally split over all the tiles within the predicted FoV of every frame. We show the frame angular resolution comparison to the equal allocation baseline in Fig. \ref{fig:KKT-exp_ave_ang_resol_bw5}.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\linewidth]{figs/KKT-exp_ave_ang_resol_bw5.eps}}
\caption{KKT-exp v.s. Equal Allocation: Angular Resolution per Frame}
\label{fig:KKT-exp_ave_ang_resol_bw5}
\end{figure}

\subsubsection{\textbf{Distance-Aware V.S. Distance-Unaware}}
We emphasized that the distance between the user and each tile plays an important role in the tile quality perceived by the user. A longer distance increases the tile's angular resolution if the tile's LoD doesn't change. Therefore, when a user is predicted to be moving away from a point cloud object, it's necessary to reduce the rate allocated to the tiles within the future user FoV, so as to save bandwidth. We demonstrate this by comparing our algorithm with a baseline where we only adopt a simple traditional rate-based utility function $Q(r)=log(r+1)$ for each tile, which ignores the viewing distance differences between tiles within the same frame, as well as tiles from different frames. 

As shown in Table~\ref{tab:ang_resol} and in Fig.~\ref{fig:ang_resol_dist_unaware}, the average angular resolution is improved by $13.76\%$ when considering distance in the tile utility function. And the wasted bandwidth is reduced by $58.58\%$ as  observed in Table~\ref{tab:wasted_rates} and Fig.~\ref{fig:wasted_rates_dist_unaware}.

% KKT-exp vs distance-unaware
\begin{figure}[htbp]
\centerline{\includegraphics[width=\linewidth]{figs/dist-aware_dist-unaware_ang_resol_bw5.eps}}
\caption{Distance-Aware v.s. Distance-Unaware Utility: Angular Resolution per Frame}
\label{fig:ang_resol_dist_unaware}
\end{figure}

% \begin{figure}[htbp]
% \centerline{\includegraphics[width=\linewidth]{figs/KKT-exp_dist-unaware_frame_quality_bw5.eps}}
% \caption{KKT-exp v.s. Distance-Unaware Utility: Frame Quality}
% \label{fig:frame_quality}
% \end{figure}

% \begin{figure}[htbp]
% \centerline{\includegraphics[width=\linewidth]{figs/KKT-exp_dist-unaware_frame_quality_per_degree_bw5.eps}}
% \caption{KKT-exp v.s. Distance-Unaware Utility: Frame Quality per Degree}
% \label{fig:frame_quality_per_degree}
% \end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=\linewidth]{figs/dist-aware_dist-unaware_wasted_rates_bw5.eps}}
\caption{Distance-Aware v.s. Distance-Unaware Utility: Wasted Rates per Frame}
\label{fig:wasted_rates_dist_unaware}
\end{figure}

% \subsubsection{\textbf{Homogeneous Tiles and Uniform Distance}}

% \begin{table*}[htb]%
% \centering
%     \centering
%     % \vspace{-8mm}
%     {\small
%     \begin{tabular}{c|c|c|c} \hline
%        Test  & Homogeneous- & Uniform- & Hetero Tiles+   \\ 
%        Scenario & Tiles & Distance & Varied Distances  \\ \hline 
%        A &   &   & 19.50, 1.66 \\ \hline
%        B &  &  & 16.87, 1.78 \\  \hline 
%        % C &  &  & \\  \hline 
%     \end{tabular}
%     }
%     \caption{\small Heterogeneous Tiles+Non-uniform Distances v.s Homogeneous Tiles and Uniform Distance}
%     \label{tab:angResol_homo_dist}
%    % \vspace{-5mm}
% \end{table*}



\section{Conclusion}
\label{sec:conclusion}
As the volumetric video is on the rise, we explore the design space of point cloud video streaming which is challenging due to its high communication and computation requirements. By relying on the inherent scalability of octree-based point cloud coding, we proposed a novel {\it sliding-window based progressive streaming framework} to gradually refine the spatial resolution of each tile as its playback time approaches and FoV prediction accuracy improves. In this way, we managed to balance needs of long streaming buffer for absorbing bandwidth variations and short streaming buffer for accurate FoV prediction. We developed an analytically optimal algorithm based on the Karush–Kuhn–Tucker (KKT) conditions to solve the tile rate allocation problem.
% \item We further formulate the rate allocation problem as an optimal control decision making problem and solve it with iterative Linear Quadratic Regulator (iLQR), which further enhances the Quality of Experience (QoE) performance compared to KKT-based algorithm due to its calculation of future impacts of every action.
We also proposed a novel tile rate-utility model that explictly consider the viewing distance to better reflect the true user QoE. In the future, we will not only consider maximizing the total tile utility, but also controlling the quality variations between consecutive frames, which leads to a more complicated non-concave objective function, and introduces stronger coupling between rate allocations on adjacent frames. We will apply a nonlinear optimal control technique, namely, iterative Linear Quadratic Regulator (iLQR), to solve the optimal rate allocation problem.

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
% \begin{acks}
% To Robert, for the bagels and explaining CMYK and color spaces.
% \end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
% \bibliographystyle{ACM-Reference-Format}
% \bibliography{ref}
\InputIfFileExists{main.bbl}


%%
%% If your work has an appendix, this is the place to put it.
% \appendix


\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
