% !Tex root=paper.tex

\section{Interpretability}
Even though automata are generally regarded as interpretable models \cite{Shvo2021InterpretableSequenceClassification},
they can become quite complicated to grasp if too many different transisitions
to different states are possible.
Moreover, there may be multiple valid models for the same training data.
Therefore we introduce heuristics aimed at reducing their complexity
and making them more readable for humans.
While \cite{Shvo2021InterpretableSequenceClassification} introduced similar techniques as regularization
terms, we adapt them to improve the interpretability of the
resulting models as demonstrated in Figure~\ref{fig:interpretability}.
Note though, that the models obtained by different simplification heuristics
need not be equivalent and that thus the modifications may impede or even
improve the models accuracy.
In essence the following heuristics aim to visually declutter the graphical
representation of the resulting model and thus to focus the user's attention.
They are implemented by adding a penalty term to the objective function.
%All our heuristics can be added optionally to Problem~\ref{prob:...} with a
%p.I%enalization term and its effects can be seen in
%Figure~\ref{fig:interpretability}. They aim at three different goals:
\begin{itemize}
    \item \textit{Sink states}: We favor solutions that have a so-called sink
        state which can never be left once it is reached. By our design, all
        words ending in the sink state are classified as normal.
        
		To introduce a sink state $q_1$ (i.e., a non-final state with only self-loops) to the
		    automaton, we add the following constraints:
		\begin{align*}
		    \delta_{q_1, a, q_1} & = 1 & \forall a \in \Sigma \\
		    f_{q_1} & = 0 
		\end{align*}
		    Moreover we add
		\[
			\lambda_s \cdot \left( \sum_{q \in Q, a \in \Sigma} 1 - \delta_{q, a, q_1} \right)
		\]
		to the objective function, which penalizes each transition not targeting the sink state.
		The parameter $\lambda_l \in \mathbb{R}$ is a weight term to be chosen by the user. 
		Note that we need to have at least two states in our DFA to have a sink-state.
	%        
    \item \textit{Self-loops}: By penalizing transitions to other states
        we obtain models with a lot of self-loops. By convention,
        those are omitted in the graphical representation.

		To increase the number of self-loops for the resulting automata, we add
		\[
			\lambda_l \cdot \left( \sum_{q \in Q, a \in \Sigma} \sum_{q' \in Q \setminus \{q\}} \delta_{q, a, q'} \right)
		\]
		to the objective function. This term penalizes each transition
		where the source state is different from the destination state (i.e., not a self-loop).
		Here, $\lambda_l \in \mathbb{R}$ is a weight term to be chosen by the user. 
		%
    \item \textit{Parallel edges}: Similar to self-loops we prefer solutions
        where there is only one successor state. Thus, the automata will
        transition to the same state regardless of the next input $a \in
        \Sigma$.
        
		Similar to self-loops, we can increase the number of parallel edges by adding
		\[
			\lambda_p \cdot \left( \sum_{q \in Q} \sum_{q' \in Q} e_{q, q'} \right)
		\]
		to the objective function. 
		The boolean variable $e_{q, q'}$ is equal to $1$ if and only if there is at least one transition from $q$ to $q'$
		and can be computed using the following set of constraints:
		\begin{align*}
		    e_{q, q'} & \leq \sum_{a \in \Sigma} \delta_{q, a, q'} & \forall q, q' \in Q \\
		    e_{q, q'} & \geq \delta_{q, a, q'} & \forall q, q' \in Q, \forall a \in \Sigma
		\end{align*}
		These constraints simply model the boolean function $e_{q, q'} \leftrightarrow \bigvee_{a \in \Sigma} \delta_{q, a, q'}$.
		Again, $\lambda_p \in \mathbb{R}$ is a weight term to be chosen by the user.

\end{itemize}
Note that unlike to approaches in machine learning such as feature
visualization \cite{DBLP:series/lncs/MontavonBLSM19} for a particular input or local model representations
\cite{DBLP:conf/kdd/Ribeiro0G16} we aim to give a comprehensive explanation of the entire model.
This allows to better understand how a model generalizes and to inject domain
expertise by adapting the model.
\begin{figure}
    \centering
    \includegraphics[scale=0.60]{interpretability_fig.pdf}
    \caption{Depiction of four different automatas with different
    interpretability heuristics based on the same input dataset. Note that, beyond
    being different to interpret, they also define different models. It is up to the
    user to decide what type of readability to emphasize as there is no clear
    best model.}
    \label{fig:interpretability}
\end{figure}