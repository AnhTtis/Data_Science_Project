% !TeX root = paper.tex
% !TeX spellcheck = en_US

\section{Experimental Evaluation}
We implemented a prototype of both learning setups using Python. As a MILP solver, we use the industry-strength Guroibi Optimizer\footnote{\url{https://www.gurobi.com/solutions/gurobi-optimizer/}} via the Pyomo optimization modeling framework\footnote{\url{https://www.pyomo.org/}}~\cite{Bynum2021PyomoOptimizationModeling,Hart2011PyomoModelingSolving}.

We evaluated our setups on a data set generated by \citeauthor{Shvo2021InterpretableSequenceClassification}~\shortcite{Shvo2021InterpretableSequenceClassification} from the ALFRED benchmark set~\cite{Shridhar2020AlfredBenchmarkInterpreting}.
The data set contains sequences of step-by-step instructions (action plans), encoded as bit vectors, that achieve a specific goal in the ALFRED setting.
For each of the seven goals, we created a training and test set as follows:
We combine all plans for that goal with  plans for the other goals (i.e., anomalies) in a 9:1 ratio.
This set is split into train and test set in a 80:20 ratio.
The precise sizes of all train and test sets are displayed in Table~\ref{tbl:data-sets} together with the share of anomalies, rounded to the next percent points.

\begin{table}
  \centering
  \begin{tabular}{lrrrr}
    \toprule
    & \multicolumn{2}{c}{Set Sizes} & \multicolumn{2}{c}{Bounds} \\
    \multicolumn{1}{c}{Goal} & \multicolumn{1}{c}{Training} & \multicolumn{1}{c}{Test} & \multicolumn{1}{c}{Lower} & \multicolumn{1}{c}{Upper} \\
    \midrule
    0 & 250 & 62 & 0.09 & 0.10 \\
    1 & 352 & 84 & 0.10 & 0.11 \\
    2 & 336 & 81 & 0.10 & 0.11 \\
    3 & 306 & 75 & 0.09 & 0.10 \\
    4 & 310 & 76 & 0.09 & 0.10 \\
    5 & 319 & 76 & 0.11 & 0.12 \\
    6 & 368 & 88 & 0.09 & 0.10 \\
    \bottomrule
  \end{tabular}
  \caption{Overview of the data sets used for evaluation.}
  \label{tbl:data-sets}
\end{table}

On each training set, we learn DFAs using both setups. For single-bound learning, we examined all target automaton sizes between two and ten. As lower and upper acceptance bounds, we used the exact ratio of correct plans to anomalies, rounded to the lower and upper percent values. We then evaluated the performance of the learned DFAs on the test set.

We limiting solving to roughly 100 seconds of CPU time over all threads using Gurobi's \texttt{WorkLimit} parameter and ran the experiments were run on an Intel Core i9-7960X CPU.

\paragraph{Two-Bound DFA Learning.}

\begin{table}
  \centering
  \begin{tabular}{lrrr}
    \toprule
    \multicolumn{1}{c}{Goal} & \multicolumn{1}{c}{F1 Score} & \multicolumn{1}{c}{Time [ms]} & \multicolumn{1}{c}{\# states} \\
    \midrule
    0 & 1.00 &  70 & 2 \\
    1 & 0.67 &  52 & 2 \\
    2 & 0.46 & 152 & 2 \\
    3 & 1.00 & 110 & 2 \\
    4 & 1.00 &  64 & 2 \\
    5 & 0.77 &  68 & 2 \\
    6 & 0.77 & 123 & 2 \\
    \bottomrule
  \end{tabular}
  \caption{Time required, F1 score obtained and automaton size generated by two-bound learning.}
  % can not really be expressed as a graph, there is no usable x axis
  \label{tbl:tb-results}
\end{table}

For two-bound DFA learning, we examined the time required to build the model and solve it, the F1 score obtained on the training set as well as the number of states in the generated DFA. These are shown in Table~\ref{tbl:tb-results}. We started with a minimum of two states and observed that in every experiment, the first automaton with two states satisfied the constraints and was returned. This shows that for our training sets the anomaly patterns can satisfyingly be detected by automata with just two states. For three sets, a detector with a score of 1 was obtained and only for set 2, the score is less than 0.5. The performance is good, with the algorithm terminating in less than 200 milliseconds for all goals.

\paragraph{Single-Bound DFA Learning.}

\begin{figure}[tb]
  \raggedleft
  \pgfplotsset{
    width=.5\textwidth,
    height=.3\textwidth,
    filter discard warning=false,
    colormap/Dark2-7,
    label style={font=\footnotesize},
    tick label style={font=\footnotesize},
    legend style={font=\tiny}
  }
  \begin{tikzpicture}
    \begin{axis}[
      %enlargelimits=true,
      xtick distance=1,
      %ymax=8000,
      ytick distance=5,
      ylabel={Time [s]},
      legend pos=north west,
      legend columns=2
      ]

      \addlegendimage{mark=*, mark options={scale=0.75}, index of colormap={0 of Dark2-7}} \addlegendentry{Goal 0}
      \addlegendimage{mark=*, mark options={scale=0.75}, index of colormap={1 of Dark2-7}} \addlegendentry{Goal 1}
      \addlegendimage{mark=*, mark options={scale=0.75}, index of colormap={2 of Dark2-7}} \addlegendentry{Goal 2}
      \addlegendimage{mark=*, mark options={scale=0.75}, index of colormap={3 of Dark2-7}} \addlegendentry{Goal 3}
      \addlegendimage{mark=*, mark options={scale=0.75}, index of colormap={4 of Dark2-7}} \addlegendentry{Goal 4}
      \addlegendimage{mark=*, mark options={scale=0.75}, index of colormap={5 of Dark2-7}} \addlegendentry{Goal 5}
      \addlegendimage{mark=*, mark options={scale=0.75}, index of colormap={6 of Dark2-7}} \addlegendentry{Goal 6}

      \addplot[mark=*, mark options={scale=0.75}, index of colormap={0 of Dark2-7}, x filter/.expression={(\thisrow{correct_label} == 0 && \thisrow{bound_relax} == 0.00) ? x : nan}] table [x=states, y=time, col sep=comma] {lbo.csv};
      \addplot[mark=*, mark options={scale=0.75}, index of colormap={1 of Dark2-7}, x filter/.expression={(\thisrow{correct_label} == 1 && \thisrow{bound_relax} == 0.00) ? x : nan}] table [x=states, y=time, col sep=comma] {lbo.csv};
      \addplot[mark=*, mark options={scale=0.75}, index of colormap={2 of Dark2-7}, x filter/.expression={(\thisrow{correct_label} == 2 && \thisrow{bound_relax} == 0.00) ? x : nan}] table [x=states, y=time, col sep=comma] {lbo.csv};
      \addplot[mark=*, mark options={scale=0.75}, index of colormap={3 of Dark2-7}, x filter/.expression={(\thisrow{correct_label} == 3 && \thisrow{bound_relax} == 0.00) ? x : nan}] table [x=states, y=time, col sep=comma] {lbo.csv};
      \addplot[mark=*, mark options={scale=0.75}, index of colormap={4 of Dark2-7}, x filter/.expression={(\thisrow{correct_label} == 4 && \thisrow{bound_relax} == 0.00) ? x : nan}] table [x=states, y=time, col sep=comma] {lbo.csv};
      \addplot[mark=*, mark options={scale=0.75}, index of colormap={5 of Dark2-7}, x filter/.expression={(\thisrow{correct_label} == 5 && \thisrow{bound_relax} == 0.00) ? x : nan}] table [x=states, y=time, col sep=comma] {lbo.csv};
      \addplot[mark=*, mark options={scale=0.75}, index of colormap={6 of Dark2-7}, x filter/.expression={(\thisrow{correct_label} == 6 && \thisrow{bound_relax} == 0.00) ? x : nan}] table [x=states, y=time, col sep=comma] {lbo.csv};
    \end{axis}
  \end{tikzpicture}

  \begin{tikzpicture}
    \begin{axis}[
      xtick distance=1,
      xlabel={\# of states},
      ytick distance=0.2,
      ylabel={F1 score}]

      \addplot[mark=*, mark options={scale=0.75}, index of colormap={0 of Dark2-7}, x filter/.expression={(\thisrow{correct_label} == 0 && \thisrow{bound_relax} == 0.00) ? x : nan}] table [x=states, y=f1, col sep=comma] {lbo.csv};
      \addplot[mark=*, mark options={scale=0.75}, index of colormap={1 of Dark2-7}, x filter/.expression={(\thisrow{correct_label} == 1 && \thisrow{bound_relax} == 0.00) ? x : nan}] table [x=states, y=f1, col sep=comma] {lbo.csv};
      \addplot[mark=*, mark options={scale=0.75}, index of colormap={2 of Dark2-7}, x filter/.expression={(\thisrow{correct_label} == 2 && \thisrow{bound_relax} == 0.00) ? x : nan}] table [x=states, y=f1, col sep=comma] {lbo.csv};
      \addplot[mark=*, mark options={scale=0.75}, index of colormap={3 of Dark2-7}, x filter/.expression={(\thisrow{correct_label} == 3 && \thisrow{bound_relax} == 0.00) ? x : nan}] table [x=states, y=f1, col sep=comma] {lbo.csv};
      \addplot[mark=*, mark options={scale=0.75}, index of colormap={4 of Dark2-7}, x filter/.expression={(\thisrow{correct_label} == 4 && \thisrow{bound_relax} == 0.00) ? x : nan}] table [x=states, y=f1, col sep=comma] {lbo.csv};
      \addplot[mark=*, mark options={scale=0.75}, index of colormap={5 of Dark2-7}, x filter/.expression={(\thisrow{correct_label} == 5 && \thisrow{bound_relax} == 0.00) ? x : nan}] table [x=states, y=f1, col sep=comma] {lbo.csv};
      \addplot[mark=*, mark options={scale=0.75}, index of colormap={6 of Dark2-7}, x filter/.expression={(\thisrow{correct_label} == 6 && \thisrow{bound_relax} == 0.00) ? x : nan}] table [x=states, y=f1, col sep=comma] {lbo.csv};
    \end{axis}
  \end{tikzpicture}
  \caption{Time required and F1 score achieved by single-bound learning for increasing numbers of states.}
  \label{fig:lba-results}
\end{figure}

For single-bound DFA learning, we performed a similar evaluation. Since this setup requires a fixed target size, we learned automata for all sizes between two and ten. The results are shown in Figure~\ref{fig:lba-results}. For most goals, the time required increases with the number of states up to a point, then drops off again. For goal 2, the 9-state automaton learning timed out. In general, the time required by this setup exceeds that of the two-bound one. The number of states' impact on the F1 score is dependent on the goal. For goals 0, 2, and 5, the score remains mostly independent of size, for the others, some sizes cause a marked reduction in score. This indicates that overfitting on the training set may occur for these sizes. A size of 2--- the same size returned by two-bound learning---seems to be a good choice for generating anomaly detectors. It requires run times of less than 600 milliseconds and yields good scores for all goals except 2, for which no automaton (including that generated by the two-bound learning) achieves a high score. Going forward, we will focus on size 2 automata.

\paragraph{Loosened Bounds.}

\begin{figure}[tb]
  \raggedleft
  \pgfplotsset{
    width=.5\textwidth,
    height=.3\textwidth,
    filter discard warning=false,
    colormap/Dark2-6,
    label style={font=\footnotesize},
    tick label style={font=\footnotesize},
    legend style={font=\tiny}
  }
  \begin{tikzpicture}
    \begin{axis}[
      xtick distance=0.01,
      xlabel={Acceptance bound $\pm$},
      scaled x ticks={false},
      xticklabel style={/pgf/number format/fixed},
      ytick distance=0.2,
      ylabel={F1 score},
      legend style={
        at={(0.5,1.05)},
        anchor=south},
      legend columns=2]

      \addlegendimage{mark=*, mark options={scale=0.75}, index of colormap={0 of Dark2-6}} \addlegendentry{Goal 0, TB}
      \addlegendimage{mark=*, mark options={scale=0.75}, index of colormap={1 of Dark2-6}} \addlegendentry{Goal 2, TB}
      \addlegendimage{mark=*, mark options={scale=0.75}, index of colormap={2 of Dark2-6}} \addlegendentry{Goal 5, TB}
      \addlegendimage{mark=*, mark options={scale=0.75}, index of colormap={3 of Dark2-6}} \addlegendentry{Goal 0, SB}
      \addlegendimage{mark=*, mark options={scale=0.75}, index of colormap={4 of Dark2-6}} \addlegendentry{Goal 2, SB}
      \addlegendimage{mark=*, mark options={scale=0.75}, index of colormap={5 of Dark2-6}} \addlegendentry{Goal 5, SB}

      \addplot[mark=*, mark options={scale=0.75}, index of colormap={0 of Dark2-6}, x filter/.expression={(\thisrow{correct_label} == 0 && \thisrow{states} == 2) ? x : nan}] table [x=bound_relax, y=f1, col sep=comma] {tb.csv};
      \addplot[mark=*, mark options={scale=0.75}, index of colormap={1 of Dark2-6}, x filter/.expression={(\thisrow{correct_label} == 2 && \thisrow{states} == 2) ? x : nan}] table [x=bound_relax, y=f1, col sep=comma] {tb.csv};
      \addplot[mark=*, mark options={scale=0.75}, index of colormap={2 of Dark2-6}, x filter/.expression={(\thisrow{correct_label} == 5 && \thisrow{states} == 2) ? x : nan}] table [x=bound_relax, y=f1, col sep=comma] {tb.csv};
      \addplot[mark=*, mark options={scale=0.75}, index of colormap={3 of Dark2-6}, x filter/.expression={(\thisrow{correct_label} == 0 && \thisrow{states} == 2) ? x : nan}] table [x=bound_relax, y=f1, col sep=comma] {lbo.csv};
      \addplot[mark=*, mark options={scale=0.75}, index of colormap={4 of Dark2-6}, x filter/.expression={(\thisrow{correct_label} == 2 && \thisrow{states} == 2) ? x : nan}] table [x=bound_relax, y=f1, col sep=comma] {lbo.csv};
      \addplot[mark=*, mark options={scale=0.75}, index of colormap={5 of Dark2-6}, x filter/.expression={(\thisrow{correct_label} == 5 && \thisrow{states} == 2) ? x : nan}] table [x=bound_relax, y=f1, col sep=comma] {lbo.csv};
    \end{axis}
  \end{tikzpicture}
  \caption{F1 score achieved by two-bound (TB) and single-bound learning ($|Q| = 2$, SB) for increasingly loose bounds on goal sets 0, 2, and 5.}
  \label{fig:loose-results}
\end{figure}

Finally, we analyzed the impact of loosening the bounds for learning on the F1 score and the runtime. Since in reality, no per-trace ground truth may be available and the share of anomalies can only be estimated, we simulate this effect by increasing the bounds by a value between 0 and 0.05. For single-bound learning, the bound is reduced, for two-bound learning, the lower bound is reduced and the higher increased. No substantial impact on the runtime could be observed for all setups. However, the score is not robust to bound loosening, as can be seen from the selected results in Figure~\ref{fig:loose-results}. On goal 1, scores are mostly robust, for goal 2, the score improves for loosenings of 0.3 and greater, and for goal 5, the score fluctuates. This phenomena are consistent between learning setups. A possible explanation is that for training sets that contain an easily-recognizable class of correct data that “fits” within the loosened bounds, this class is learned as anomalous instead of truly anomalous data. Therefore, our setups require an accurate estimate of the share of anomalous data to operate correctly.

\paragraph{Summary.}

Both the two-bound and single-bound learning with size 2 are fast to apply and yield small automata that are nonetheless effective as anomaly detectors in most cases. However, the two-bound setup is slightly faster and yields better detectors for most goals, including goal 2. We conclude that while both learning setups are feasible for learning anomaly detectors, the first is slightly preferrable. However, both setups require precise bound information.

\paragraph{Threats to Validity.}

The validity of our approach requires anomalies to be detectable by a DFA. This property is guaranteed by the data set, since each strategy can only solve a single ALFRED goal. The internal validity of our results stems from Gurobi's deterministic behavior, which guarantees that the scoring results can be reproduced as-is. The observed running time showed standard deviations of less than 250 milliseconds and is therefore also reproducible. The external validity is limited by the ALFRED data set we used. While this set has seen widespread use, the results may not fully extend to different types of traces generated by different scenarios.
