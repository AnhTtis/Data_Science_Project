% !TeX root = paper.tex
% !TeX spellcheck = en_US
\section{Experimental Evaluation}
\label{sec:eval}

We implemented a prototype of the three learning algorithms in Python\footnote{\url{https://github.com/simonlutz-tudortmund/Interpretable-Anomaly-Detection}} using the industry-strength Gurobi Optimizer~\cite{gurobi} as an MILP solver.

We evaluated all three learning settings in the context of anomaly detection on three datasets: a modified version of the  ALFRED benchmark set~\cite{Shridhar2020AlfredBenchmarkInterpreting} and the two real-world log datasets HDFS~\cite{xu2009hfds} and BGL~\cite{oliner2007bgl}, provided by the Loghub system log dataset collection~\cite{He2020LoghubLargeCollection}. 
The exact dataset characteristics are shown in Table~\ref{tbl:tb-datasets}.

The ALFRED data set contains sequences of action plans, encoded as bit vectors, that achieve one of 7 goals in the ALFRED setting.
For each of the pairwise combinations of goals (i.e., 42 class combinations), we created a training and a test set with the elements of the first (normal) class and the elements of the second (anomalous) class in a 9:1 ratio. 

The HDFS data set contains system logs for a Hadoop Distributed File System hosted in a private cloud environment.
Each entry in the data set represents a sequence of system events %belonging to a single block ID
, labeled as either normal or anomalous by a set of expert rules. 
Similarly, BGL is a set of logs collected from a BlueGene/L supercomputer system, containing alert and non-alert messages. 
To keep the running time within the timeout of two hours, we restricted ourselves to words with a maximum length of 15 and 10, respectively.
%For both datasets we restricted ourselves to words with a maximum length of 15 and 10, respectively. This covers 19\% and 66\% of the datasets and was necessary to keep the running time within the timeout of two hours.

\begin{table}[tbp]
  \centering
  \setlength{\tabcolsep}{12pt}
  \begin{tabular}{lrrrrrr}
    \toprule
    \multicolumn{1}{c}{Dataset} & \multicolumn{1}{c}{$|S|$} & \multicolumn{1}{c}{$|\Sigma|$} & \multicolumn{1}{c}{\% anomalies}  & \multicolumn{1}{c}{LB} & \multicolumn{1}{c}{UB}\\
    \midrule
    ALFRED & 316-462 &  9 & $\approx$ 0.1 & 0.09-0.10 & 0.10-0.11 \\
    HDFS & 108237 &  13 & 0.0585 & 0.058 & 0.059 \\
    BGL & 198192 & 295 & 0.1766 & 0.176 & 0.177 \\    
    \bottomrule
  \end{tabular}
  \caption{Summary of the datasets used.}
  % can not really be expressed as a graph, there is no usable x axis
  \label{tbl:tb-datasets}
\end{table}

\paragraph{Performance analysis.}

\begin{figure}[tbp]
  \centering
  \begin{subfigure}{0.495\textwidth}
    \centering
    \includegraphics[width=\textwidth]{plots/f1_score_distribution.pdf}
    \caption{F1 score achieved.}
    \label{fig:f1_score}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.495\textwidth}
    \centering
    \includegraphics[width=\textwidth]{plots/runtime_distribution.pdf}
    \caption{Time required to solve the model.}
    \label{fig:runtime}
  \end{subfigure}
  \caption{Results by three approaches on the selected datasets.}
  \label{fig:results}
\end{figure}
% \vspace{-5em}

In our first experiment, we want to answer the research question asking which learning setting performs the best for detecting anomalies in sequential data.
For each of the three datasets, we randomly split the data into training and test set (with a 80:20 ratio) and averaged our results over 50 runs.
We examined both the time required to build and solve the model during training (with a two hour timeout) and the F1 score obtained on the test set.
We observed that in every experiment the Two-Bound DFA Learning algorithm was able to find a feasible solution of size two. 
Therefore, for the other two learning settings, we also learned an automaton of size 2.
The results of our experiments are displayed in Figure~\ref{fig:results}. 
They show that both the Two-Bound and Single-Bound learning setting were able to achieve high performance on the ALFRED and HDFS datasets.
The performance on the BGL dataset is slightly worse, which can be explained by the significantly larger alphabet compared to the length of the words. 
This means that simple patterns, such as a single letter, are sufficient to differentiate even two normal sequences.
These patterns may then be picked up by our algorithms instead of patterns separating normal sequences and anomalies.
In terms of running time there seems to be no significant difference between the first two learning settings.
However, the results show that the running time of the algorithms heavily depends on the dataset. 
This is to be expected, as the number of automata constraints $\Phi_{\dfa}^n$ scales with the number of words (and prefixes) in the sample $\sample$, yielding a longer running time.
Compared to the first two settings, the Distance-Based learning algorithm performs significantly worse, especially for the HDFS dataset.
This can be explained by the fact that the HDFS dataset contains words of different length (ranging from 2 to 15), thus the distance between two words is dominated by their difference in length.
The algorithm, by minimizing Equation~\ref{eq:distance}, will therefore separate words mostly based on their length, neglecting any other differences or patterns.


%Our first research question is to find out if the proposed learning approaches are feasible for learning anomaly detectors and how they perform on the selected datasets.
%For all our experiments, we examined the time required to build and solve the model for the training set and the F1 score obtained on the test set. 
%The results are shown in Figure~\ref{fig:results}. 
%We averaged our results over 50 runs and used an Intel Core i9-13900K CPU for the experiments.
%We observed that in every experiment we could find a feasible solution DFA with size two, so we focused on the size 2 automata. 
%The F1 scores shown in Figure~\ref{fig:f1_score} show that the two-bound learning approach and the single bound approaches could achieve high performance on ALFRED and HDFS, while the BGL setting was more difficult to solve. 
%This can be explained by the fact that the BGL dataset contains many more different events, which makes the alphabet size larger than in the other two datasets. 
%This makes it easier for our approaches to split the data, but also makes the automata more trivial, leading to a worse F1 score. 
%The runtime results in Figure~\ref{fig:runtime} are consistent with this, showing that the time required to build and solve is highly dependent on the size of the alphabet.

\paragraph{Loosened Bounds.}
For the Two-Bound and Single-Bound learning settings we assume to be given bounds on number of words to be accepted. 
In this second experiment, we will answer the research questions whether these learning algorithms are robust under imprecise bounds.
Focusing on the ALFRED dataset, we analyze the effect of loosening the learning bounds on the F1 score by increasing or decreasing the bounds by a value between 0 and 0.05.
The results are displayed in Figure~\ref{fig:loose-results}.
They show, that the F1 score is highest for the tightest bounds, as expected.
Furthermore, the performance of both the Two-Bound and Single-Bound learning algorithm is quite robust to loosening the bounds.
The F1 score drops only slightly even for a loosened bound of 0.05, which represents a 50\% deviation from the original number of anomalies.

\begin{figure}[tbp]
  \centering
  %\includegraphics[width=0.8\textwidth]{plots/alfred_bound_deviations.pdf}
  \includegraphics[scale=0.6]{plots/alfred_bound_deviations.pdf}
  \caption{F1 score for the ALFRED dataset with loosened bounds.}
  \label{fig:loose-results}
\end{figure}

%Our second research question is whether the learning approaches are robust to changes in the upper and lower bounds. Since in reality there may be no ground truth per trace and the proportion of anomalies can only be estimated, we simulate this effect by adjusting the bounds by a value between 0 and 0.05. For single-bound learning, the bound is lowered, for two-bound learning, the lower bound is lowered and the upper bound is raised. We have analyzed the effect of loosening the learning bounds on the F1 score on the ALFRED dataset, and the results are shown in Figure~\ref{fig:loose-results}. We see that the F1 score is highest for the tightest bounds, as expected. However, the performance of both the single-bound and two-bound learning approaches is quite robust to loosening the bounds. The F1 score drops only slightly even for a loosened bound of 0.05, which represents a 50\% deviation from the original number of accepted sequences.

\paragraph{Data Complexity.}
%In this paragraph, we want to briefly elaborate on data complexity and solution size in our experiments.
For all datasets, the Two-Bound setting always found a feasible solution of size two.
Since this seemed rather small, we also trained a DFA in the classical passive learning setting (i.e., with labels) on the same datasets.
The resulting automata had size four for the ALFRED dataset and size two for the HDFS dataset, while no DFA with up to thirty states could be found for BGL.
This shows that even small automata are capable of separating the normal sequences and anomalies.

\begin{figure}[tbp]
  \centering
  %\includegraphics[width=0.8\textwidth]{plots/alfred_bound_deviations.pdf}
  \includegraphics[height= 0.15\textheight, width= \textwidth]{plots/interpretability.pdf}
  \caption{F1 score for combinations of interpretability heuristics on the HDFS dataset.}
  \label{fig:f1_interpretability}
\end{figure}

\paragraph{Influence of Interpretability Heuristics.}
In this section we investigate the influence of our interpretability heuristics on the algorithm's overall performance.
For each possible combination of heuristics, we learned an automaton in the first learning setting on the HDFS dataset with tight bounds.
Since any reasonable automaton that includes a sink state has a minimum of three states, we set the minimum number of states of the learned DFAs to three.
The results, shown in Figure~\ref{fig:f1_interpretability}, indicate that increasing the number of self-loops improves the F1 score.
Including more parallel edges does not seem to affect the overall performance, whereas  introducing sink states greatly impedes the resulting F1 score.
%For minimal automata with a small number of states, enforcing only one successor state using parallel edges may not be possible, thus the resulting DFA stays the same as the baseline.
%Similarly, sacrificing one of the few states in a small automata to be a sink state is straight up detrimental to expressiveness.
 % since a small automaton is penalized for transitions between a sink state and other states.
When combining multiple heuristics, the overall performance behaves the same as in one of the experiments using only a single heuristic.
This indicates that the influence of one heuristic on the objective function outweighs the other heuristics.
We leave a more thorough evaluation of the interpretability heuristics as part of future work.

%Combining all heuristics improves performance due to, we assume, the self-loop objective dominating the sum of the heuristics' objective functions.

\begin{figure}[tbp]
  \centering
  %\includegraphics[width=0.8\textwidth]{plots/alfred_bound_deviations.pdf}
  \includegraphics[scale=0.6]{plots/alfred_num_states.pdf}
  \caption{F1 score for the ALFRED dataset with changing number of states.}
  \label{fig:f1_states}
\end{figure}

\paragraph{Influence of Automaton Size.}
In the second and third learning setting, we assume the size of the automaton to be given by the user.
In this section, we investigate how this influences the overall performance of the learned automata. 
We conducted experiments with varying sizes (ranging from $2$ to $10$) on the ALFRED dataset.
The results are displayed in Figure~\ref{fig:f1_states}.
They indicate that increasing the size of the learned automaton does not significantly impact its performance.
For the Single-Bound setting the performance remains roughly constant, while for the  Distance-Based approach there is only a slight increase in performance between size $2$ and $3$.
This indicates that once a reasonable solution is found the learning algorithm starts to only learn unreachable states when the size is increased.
These results may be connected to the complexity of our data and we leave a more thorough investigation as part of future work.




%Our third and final research question is whether the fact that we could find a two-state DFA with acceptable performance means that the data is simple enough to be fully captured by a DFA with a comparable number of states. 
%To prove this, we trained a DFA that would perfectly separate the normal and anomalous traces for a given dataset. 
%In the ALFRED dataset, we could find a DFAs with at most four states that perfectly separated the traces, while the complete HDFS dataset with trace lengths up to 15 could be separated by a DFA with only two states.
%In contrast, the BGL dataset was much more complex, even though the maximum trace length was 10, and we stopped searching for a perfect DFA after twenty states due to the runtime constraint.
%This result provides an additional argument for the feasibility of our approach, since it shows that if there is a DFA that can separate the traces, it is likely to be small enough to be learned by our approach.

%\paragraph{Summary.}
%In summary, both two-bound and single-bound learning with size 2 are fast to apply and yield small automata that are nonetheless effective as anomaly detectors in most cases. However, the two-bound setup is slightly faster and yields better detectors for most goals, including goal 2. We conclude that while both learning setups are feasible for learning anomaly detectors, the first is slightly preferrable. However, both setups require precise bound information.

