\section{Interpretability}
\label{sec:interpre}

Even though automata are generally regarded as interpretable models \cite{Shvo2021InterpretableSequenceClassification},
they can become unintuitive if there are too many different transitions
to different states.
%Moreover, there may be multiple valid models for the same training data.
Therefore, we introduce heuristics aimed at reducing their complexity
and making them more readable for humans.
While \cite{Shvo2021InterpretableSequenceClassification} introduced similar techniques as regularization terms, we adapt them to improve the interpretability of the resulting models, as demonstrated in Figure~\ref{fig:interpretability}.
Note though, that the model's obtained by different simplification heuristics need not be equivalent and that thus the modifications may impede or even improve the models accuracy (see Section~\ref{sec:eval}).
In essence, the following heuristics aim to visually streamline the graphical
representation of the resulting model and thus highlight the important structural insights.
They are implemented by adding a penalty term to the objective function.

\begin{itemize}
    \item \textit{Sink states}:  We favor solutions that have a so-called sink state, which can never be left once it is reached. By our design, all words ending in the sink state are rejected.
        
		To introduce a sink state $q_1$ (i.e., a non-final state with only self-loops) to the automaton, we add the following constraints:
		\begin{align*}
		    \delta_{q_1, a, q_1} & = 1 & \forall a \in \Sigma \\
		    f_{q_1} & = 0 
		\end{align*}
		    Moreover, we add
		\[
			\lambda_s \cdot \left( \sum_{q \in Q, a \in \Sigma} 1 - \delta_{q, a, q_1} \right)
		\]
		to the objective function, which penalizes each transition not targeting the sink state.
		The parameter $\lambda_l \in \mathbb{R}$ is a weight term to be chosen by the user. 
		Note that we need to have at least two states in our DFA to have a sink-state. \\[1ex]
	%        
    \item \textit{Self-loops}: By penalizing transitions to other states, we obtain models with a lot of self-loops. By convention, those are omitted in the graphical representation.

		To increase the number of self-loops for the resulting automata, we add
		\[
			\lambda_l \cdot \left( \sum_{q \in Q, a \in \Sigma} \sum_{q' \in Q \setminus \{q\}} \delta_{q, a, q'} \right)
		\]
		to the objective function. This term penalizes each transition
		where the source state is different from the destination state (i.e., not a self-loop).
		Here, $\lambda_l \in \mathbb{R}$ is a weight term to be chosen by the user. \\[1ex]
		%
    \item \textit{Parallel edges}: Similar to self-loops, we prefer solutions where there is only one successor state. Thus, the automata will transition to the same state regardless of the next element $a \in \Sigma$.
        
		Similar to self-loops, we can increase the number of parallel edges by adding
		\[
			\lambda_p \cdot \left( \sum_{q \in Q} \sum_{q' \in Q} e_{q, q'} \right)
		\]
		to the objective function. 
		The boolean variable $e_{q, q'}$ is equal to $1$ if and only if there is at least one transition from $q$ to $q'$
		and can be computed using the following set of constraints:
		\begin{align*}
		    e_{q, q'} & \leq \sum_{a \in \Sigma} \delta_{q, a, q'} & \forall q, q' \in Q \\
		    e_{q, q'} & \geq \delta_{q, a, q'} & \forall q, q' \in Q, \forall a \in \Sigma
		\end{align*}
		These constraints simply model the boolean function $e_{q, q'} \leftrightarrow \bigvee_{a \in \Sigma} \delta_{q, a, q'}$.
		Again, $\lambda_p \in \mathbb{R}$ is a weight term to be chosen by the user.

\end{itemize}


Note that our optimization-based approach is flexible with respect to the heuristics used: as long as a heuristic is expressible as part of the optimization problem, it can be applied. 
Furthermore, this approach is orthogonal to the original encoding and can, in principle, also be applied to other constraint-based learning algorithms for finite-state machines.

\begin{figure}[tbp]
    \centering
    \begin{subfigure}[t]{0.25\textwidth}
        \raggedright
        \includegraphics[scale=0.40]{figures/interpretability_baseline.pdf}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.25\textwidth}
        \raggedright
        \includegraphics[scale=0.40]{figures/interpretability_parallel.pdf}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.25\textwidth}
        \raggedright
        \includegraphics[scale=0.40]{figures/interpretability_loops.pdf}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.25\textwidth}
        \raggedright
        \includegraphics[scale=0.40]{figures/interpretability_sink.pdf}
    \end{subfigure}%
    \caption{Depiction of four different DFAs learned with different
    interpretability heuristics based on the same input dataset. 
    %Note that, beyond being different to interpret, they also define different languages.
    }
    \label{fig:interpretability}
\end{figure}


