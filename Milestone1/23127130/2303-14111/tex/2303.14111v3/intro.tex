\section{Introduction}
\label{sec:intro}

%\paragraph{Motivation automata learning}
In the last decades, the algorithmic learning of finite automata (or automata learning for short) has proven to be a successful tool in many application domains, ranging from pattern and  language recognition~\cite{garcia1990use} over robotics~\cite{rivest1989inference,rieger1995inferring} to automatic verification~\cite{oliveira2001efficient,groce2006adaptive,habermehl2005regular} and software testing~\cite{aichernig2018model}.
For reactive system verification, for instance, the goal of automata learning is to provide an appropriate abstraction of the system's inputâ€“output relations as a finite-state machine~\cite{hungar2003domain}.


%\paragraph{active and passive learning}
Traditionally, the literature on automata learning distinguishes two main settings: active and passive learning.
In active learning~\cite{Angluin87}, the algorithm (called learner) interacts with a so-called teacher.
This teacher has access to a regular language and is able to answer two types of queries.
Membership queries ask whether a specific word is in the target language and equivalence queries ask whether a conjectured automaton
is equivalent to the language in question.
In passive learning~\cite{BiermannF72,Trakhtenbrot1973FiniteA}, the learning algorithm is given a finite set of words which are labeled as positive or negative, i.e., whether they are contained in the regular target language or not, respectively.
Then, the objective is typically to learn a minimal automaton that accepts all positive words and rejects all negative ones. 

%\paragraph{gap + intro unsupervised automata learning}
While many advances in active and passive learning have expanded upon these seminal works, other important learning settings remain unexplored.
For instance, the field of unsupervised learning is a well-studied aspect in machine learning that, so far, has been ignored in the context of automata learning.
However, many important unsupervised learning problems, such as anomaly detection, also arise for automata and reactive systems.
Currently, they are addressed via use case specific solutions, which are hard to engineer and difficult to transfer to other settings.

To overcome this gap, this paper proposes a generic approach for unsupervised automata learning based on discrete optimization.
Similar to passive learning, we rely on a given, finite set of words but assume that, a priori, no additional information, such as positive or negative labels, is available.
While our ideas are applicable to many other unsupervised learning settings on sequential data, in this paper, we focus on a crucial application in unsupervised machine learning: anomaly detection (i.e., identifying patterns in the data that do not conform to expected behavior~\cite{DBLP:journals/csur/ChandolaBK09}).
This choice is motivated by the many application domains of anomaly detection, including cybersecurity, law enforcement, medicine, and fraud detection, to name but a few.

%%%%%%%%%%%%%%%%%%%%
% SD: @SL/DK: this feels relatively redundant w.r.t. Sec. 3 and pushes the actual contribution to page 3. Should we reduce this to the intuition and move some of the more technical stuff there?
%%%%%%%%%%%%%%%%%%%%

%\paragraph{contributions}
To be more precise, we aim to learn a DFA from a given finite multi-set $\sample$ of unlabeled sequences that can distinguish normal from anomalous sequences. To this end, we consider three unsupervised learning settings.
In the first setting, we assume two natural numbers $\ell, u \in \mathbb N$ with $\ell \leq u\leq\abs{\sample}$ to be given as input.
The task is then to learn a minimal DFA that accepts at least $\ell$ and at most $u$ sequences from~$\sample$.
Minimality refers to a minimal number of states and is a common requirement in automata learning~\cite{BiermannF72,DBLP:conf/icgi/HeuleV10,DBLP:conf/isola/LeuckerN12,DBLP:conf/aaai/NeiderGGT0021}.
The parameters $\ell$ and $u$, on the other hand, serve as an estimate for the lower and upper number of anomalies in the data set and are used to prevent degenerate DFAs (i.e., DFAs that accept or reject all sequences).
%To give more intuition, assume that we are given $n$ sequences and know that between 10\% and 20\% of the sequences are anomalies. Then, we aim to learn a minimal automaton that accepts between $\ell=0.1\cdot n$ and $u=0.2\cdot n$ of the sequences.
This setting operates under the assumption that normal sequences are drastically different from anomalies, allowing them to be separated by a rather simple pattern.
Hence, by looking for an automaton that is as compact as possible, the
classification of anomalies is performed automatically.

In the second setting the user does not need to specify both $\ell$ and $u$, but only one or the other (say, $\ell$).
Additionally, the user must fix a size $n \in \mathbb N$ of the resulting DFA.
The task then is to learn a DFA of size $n$ that accepts the smallest number $k \geq \ell$ of sequences from $\sample$.
In other words, $\ell$ serves as a lower bound on the assumed number of anomalies in the given data set.
In general, the choice of $n$ should be made carefully in this setting, as too large a number may hinder interpretability while, if $n$ is too small, the resulting DFAs may not be able to separate anomalies from normal sequences.

The last setting is motivated by the assumption that all normal sequences are similar to each other, i.e., have a small edit distance, while the anomalies are vastly different from the normal sequences, resulting in a high edit distance.
Under this assumption the user does not need to specify any bounds, but must still fix a size $n \in \mathbb N$ of the resulting DFA.
We learn a DFA that minimizes the distance between pairs of sequences classified as normal, while maximizing the distance between pairs of sequences where one is classified as normal and the other as an anomaly.

Our contributions in this paper are fourfold.
First, we show that learning a DFA of size $n$ from unlabeled data is \NP-complete. This result is in line with the classical learning of DFAs from positive and negative data, which is known to be \NP-complete as well~\cite{DBLP:journals/iandc/Gold78}. Consequently, the first learning problem lies within the complexity class FNP (i.e., the function problem extension of the decision problems in NP) and the second one lies within the class \NPO (i.e., the class of optimization problems whose decision variant lies in \NP).
The complexity of the third learning problem remains an open problem that we will leave as a part of future work.

Second, we develop three learning algorithms, one for each setting.
While in previous work, a DFA was learned by solving a series of constraint satisfaction problems, we reduce learning into a series of constraint optimization problems instead.
This allows us to specify an objective function, thus finding not just any solution but one that is optimized for additional regularization criteria.
The constraint optimization problems can then be solved by highly-optimized mixed-integer programming solvers.

Third, we propose novel regularization terms to enhance the interpretability of the learned DFAs.
In particular, we show how to augment our constraint optimization problems to maximize the number of self-loops and parallel edges (see Figure~\ref{fig:interpretability}).
This approach is orthogonal to the original encoding and can, in principle, also be applied to other constraint-based learning algorithms for finite-state machines.

Fourth, to show the practical feasibility of our three algorithms, we evaluate them empirically on three anomaly detection benchmarks.
We examine both the runtime and the anomaly detection performance for different configuration options and uncertainty w.r.t.~the anomaly frequency.


%---------- Related Work ----------
\subsubsection*{Related Work}
Automata learning has a long history, dating back to the 1970s~\cite{BiermannF72,Trakhtenbrot1973FiniteA}.
One typically distinguishes between active learning and passive learning.

Active learning was first introduced by Dana Angluin in 1987~\cite{Angluin87}.
In her work, Angluin showed that the class of regular languages can be learned efficiently by asking queries to a (minimally adequate) teacher.
Furthermore, she provided an appropriate learning algorithm - called the $\text{L}^*$ algorithm - which approximates the Myhill-Nerode congruence.
Since then, various major improvements to and variants of the original algorithm have been proposed~\cite{RivestS93,Kearns94,MalerP95,Irfan10,MertenHSM11,Howar12,aarts2014tomte,IsbernerS14,IsbernerHS14,Petrenko0GHO14,Frohme19,VaandragerGRW22}.

Passive learning, on the other hand, was pioneered by Biermann and Feldman~\cite{BiermannF72} and Trakhtenbrot and Barzdin~\cite{Trakhtenbrot1973FiniteA}.
Given a set of labeled data, a passive learning algorithm seeks to learn a minimal DFA consistent with the data. 
Algorithms such as Regular Positive Negative Inference (RPNI)~\cite{oncina1992inferring} and the Blue-fringe algorithm~\cite{DBLP:conf/icgi/LangPP98} first construct the prefix acceptor -- the most precise description of the data -- and then generalize it by merging its states while maintaining consistency with the data.
In 1987, Gold~\cite{DBLP:journals/iandc/Gold78} showed that passive learning is computationally hard (i.e., the corresponding decision problem is \NP-complete).
Thus, learning algorithms that use constraint solving have become the de~facto standard for constraint-based passive learning~\cite{DBLP:conf/cade/GrinchteinLP06,DBLP:conf/icgi/HeuleV10,DBLP:conf/atva/Neider12,DBLP:conf/nfm/NeiderJ13,DBLP:conf/aaai/NeiderGGT0021}.

%%%%%%%%%%%%%%%
% SD: @all de-facto standard is fairly bold -- e.g., TAG and TkT are both from 2022 and are both merge-based
%%%%%%%%%%%%%%%

Besides the traditional active and passive learning of deterministic finite automata, other learning settings have also been investigated.
Following the same underlying ideas, various algorithms have been proposed in the literature for learning more expressive state machines such as 
Mealy Machines with and without timers~\cite{Niese2003,ShahbazG09,DBLP:journals/corr/abs-2403-02019},
I/O automata~\cite{DBLP:conf/concur/AartsV10},
non-deterministic automata~\cite{BolligHKL09,BjorklundFK13}, 
alternating automata\cite{DBLP:conf/ijcai/AngluinEF15},
register automata~\cite{DBLP:conf/fm/AartsHKOV12,DBLP:journals/fac/CasselHJS16,DBLP:conf/ifm/GarhewalVHSLS20,DBLP:journals/ml/IsbernerHS14},
weighted automata~\cite{BergadanoV96,BalleM15,HeerdtKR020},
pushdown automata~\cite{DBLP:phd/dnb/Isberner15},
tree automata~\cite{DBLP:journals/tcs/KnuutilaS94,oncina1993inference}, among others.
In the context of incomplete information, Leucker and Neider~\cite{DBLP:conf/isola/LeuckerN12} proposed a variant of Angluin's $\text{L}^*$ algorithm for learning from an inexperienced teacher that sometimes may answer ``don't know'' to a membership query.
However, to the best of our knowledge, learning a Deterministic Finite Automaton completely from unlabeled data remains unexplored.