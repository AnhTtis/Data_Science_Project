% !Tex root=paper.tex
% !Tex program=pdflatex
% !TeX spellcheck = en_US


\section{Introduction}
\label{sec:intro}

Anomaly detection (i.e., identifying patterns in data that do not conform to expected behavior~\cite{DBLP:journals/csur/ChandolaBK09}) has evolved into a vibrant subfield of machine learning and data science.
Its many application domains include cyber security, law enforcement, medicine, and fraud detection, to name but a few.
Essentially, anomaly detection is applied whenever the correct functioning of a complex system is essential for safety or financial reasons.

The recent advances in machine learning have prompted the development of a host of deep learning techniques for anomaly detection (see the section on related work for a brief overview).
Many of these operate in an unsupervised setting, where the data is unlabeled (i.e., what does and does not constitute an anomaly is unknown).
To make this task tractable, one usually requires further auxiliary information, such as an upper and lower bound on the expected number of anomalies in the data.
Another approach is to fine-tune specific hyper-parameters of the learning algorithm to prevent it from producing a degenerate solution (i.e., one that classifies all or no data as anomalies).

While deep learning methods for anomaly detection have shown excellent performance, their inherent complexity and black-box nature make their decisions often hard to understand.
However, a lack of interpretability is often a severe obstacle to employing anomaly detection in practice.
Consider, for instance, a monitor for vital signs in an intensive care unit. If an anomaly is detected, it is imperative to understand the reason to initiate the proper treatment quickly.

This paper focuses on unsupervised learning of inherently interpretable anomaly detectors for sequential data.
More specifically, we consider the task of learning a deterministic finite automaton (DFA) from a given multi-set of unlabeled sequences, which accepts the anomalies and rejects the normal data.
This specific choice of model is motivated by \citeauthor{Shvo2021InterpretableSequenceClassification}~\shortcite{Shvo2021InterpretableSequenceClassification}, who have demonstrated that DFAs are inherently interpretable models for classifying sequential data.
Moreover, the authors have shown that DFAs can match the performance of deep LSTM networks on various tasks.

We consider two unsupervised learning setups.
In the first setting, we are given a finite multi-set $\sample$ of
unlabeled sequences and two natural numbers $\ell, u \in \mathbb N$ with $\ell \leq u\leq\abs{\sample}$.
The task is then to learn a minimal DFA that accepts at least $\ell$ and at most $u$ sequences from $\sample$.
Minimality refers to a minimal number of states and is a common requirement in automata learning~\cite{DBLP:journals/tc/BiermannF72,DBLP:conf/icgi/HeuleV10,DBLP:conf/isola/LeuckerN12,DBLP:conf/aaai/NeiderGGT0021}.
Here, we use it to ensure high interpretability in the sense of Occam's razor (i.e., smaller models are generally easier to understand than larger ones~\cite{Shvo2021InterpretableSequenceClassification,DBLP:conf/ijcai/0002FN20}).
The parameters $\ell$ and $u$, on the other hand, serve as an estimate for the lower and upper number of anomalies in the data set and are used to prevent degenerate DFAs (i.e., DFAs that accept or reject all sequences).
To give more intuition, assume that we are given $n$ sequences and know
that 10-20\% of the sequences are typically anomalies. Then our aim is
to learn a minimal automaton that accepts between $\ell=0.1\cdot n$
and $u=0.2\cdot n$ of the sequences. We operate under the assumption
that  it is presumably easier to separate regular sequences and outliers via a
regular language, rather than mix them. 
Hence by looking for an automaton that is as compact as possible, the
classification of anomalies is performed automatically:

The second setting alleviates the user's burden to specify both $\ell$ and $u$.
Instead, it assumes a multi-set $\mathcal S$, a size $n$ of the
resulting DFA, but only one bound to be given, say $\ell \in \mathbb N$.
In this setting, the task is to learn a DFA of size $n$ that accepts
the smallest number $k \geq \ell$ of sequences from $\mathcal S$.
In other words, $\ell$ serves as a lower bound on the assumed number
of anomalies in the given data set.

While this setting only requires one of the bounds to be known or specified,
it requires the user to specify the size $n$ of the resulting DFA as input.
Generally, the number of states $n$ should be chosen carefully in this
setting as numbers too high could hinder interpretability while, if $n$ is
too small, the resulting DFAs may not be able to separate anomalies
from normal sequences.

Our contributions are fourfold.
First, we show that these learning problems are computationally hard.
In fact, the first problem is \NP-complete and the second one lies within the class \NPO.
These results are in line with the classical learning of DFAs from positive and negative data, which is known to be \NP-complete as well~\cite{DBLP:journals/iandc/Gold78}.

Second, we develop two learning algorithms, one for each setting.
Since both settings are computationally hard, our algorithms follow a common approach in automata learning and reduce the tasks into a series of constraint optimization problems.
These problems can then be solved by highly-optimized mixed-integer programming solvers (Gurobi in our case~\cite{gurobi}).

Third, we propose novel regularization terms to enhance the interpretability of the learned DFAs.
In particular, we show how to augment our constraint optimization problems to maximize the number of self-loop and parallel edges.
This approach is orthogonal to the original encoding and can, in principle, also be applied to the classical passive learning algorithms for finite-state machines.

Fourth, we evaluate our two algorithms empirically on an benchmark based on the ALFRED data set~\cite{Shridhar2020AlfredBenchmarkInterpreting}.
We examine both the runtime and the anomaly detection performance for different configuration options and uncertainty w.r.t.~the anomaly frequency.


%---------- Related Work ----------
\subsubsection*{Related Work}

A large number of different anomaly detection methods can be found in the literature.
For instance, the survey by \citeauthor{DBLP:journals/csur/ChandolaBK09}~\shortcite{DBLP:journals/csur/ChandolaBK09} distinguishes between methods that are classification-based, clustering-based, based on nearest neighbors, statistical, information-theoretic, and spectral.
The algorithms developed in this paper are learning-based.

Within the learning-based methods, deep learning has evolved as a powerful technique.
Examples include variational autoencoders~\cite{DBLP:conf/www/XuCZLBLLZPFCWQ18,DBLP:journals/tnn/LiYWJ21}, adversarial neural networks~\cite{DBLP:conf/icann/LiCJSGN19,DBLP:conf/bigdataconf/GeigerLACV20}, and the paradigm of ``one-class classification''~\cite{DBLP:conf/icml/RuffGDSVBMK18}.
Another way to categorize learning-based methods is their setup.
Here, one typically distinguishes between supervised learning (i.e., the data is labeled as normal or anomalous), semi-supervised learning (i.e., the combination of a small amount of labeled data with a large amount of unlabeled data), and unsupervised learning (i.e., the data is not labeled).
Since obtaining labeled data for anomaly detection is often difficult or dangerous, this paper operates in an unsupervised setting.

A substantial drawback of deep neural networks is their black-box nature and high complexity, which makes their decision-making intransparent and hard to understand.
Hence, interpretable machine learning has evolved recently (see \citeauthor{molnar2022}~\shortcite{molnar2022} for an introduction).
A key distinction in this field (among others) is whether one explains a complex model post-hoc or trains an inherently interpretable one.
We follow the second paradigm, although inherently interpretable models can have a performance penalty.

Recently, various papers have argued for deterministic finite automata (DFAs) as a powerful yet interpretable model for sequence classification~\cite{DBLP:journals/corr/HammerschmidtVL16,Shvo2021InterpretableSequenceClassification}.
We support this proposition but want to point out that \citeauthor{Shvo2021InterpretableSequenceClassification}'s work is fundamentally different from ours: \citeauthor{Shvo2021InterpretableSequenceClassification} consider a supervised setup, whereas we consider an unsupervised one.
In fact, not much research has been devoted to unsupervised automata learning so far (see \citeauthor{DBLP:conf/ijcai/CarmelM95}~\shortcite{DBLP:conf/ijcai/CarmelM95} for a notable exception), let alone to interpretable anomaly detection.

Automata learning has a long history, dating back to the 1970s~\cite{DBLP:journals/tc/BiermannF72,Trakhtenbrot1973FiniteA}.
One typically distinguishes between active learning and passive learning.
In active learning~\cite{DBLP:journals/iandc/Angluin87}, the learning algorithm aims to learn a minimal DFA (in terms of the number of states) by querying an information source called the teacher.
In passive learning~\cite{DBLP:journals/tc/BiermannF72}, on the other hand, the learning algorithm seeks to learn a minimal DFA from a given set of labeled data.
Although our learning setup is unsupervised, it resembles the passive one.

\citeauthor{DBLP:journals/iandc/Gold78}~\shortcite{DBLP:journals/iandc/Gold78} showed that passive learning, as defined above, is computationally hard (i.e., the corresponding decision problem is NP-complete).
Thus, learning algorithms that use constraint solving have become the de~facto standard~\cite{DBLP:conf/icgi/HeuleV10,DBLP:conf/atva/Neider12,DBLP:conf/aaai/NeiderGGT0021}.
Since our learning task is also computationally hard, our two algorithms follow a similar approach and translate the learning task into a sequence of optimization problems in mixed integer linear programming.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
