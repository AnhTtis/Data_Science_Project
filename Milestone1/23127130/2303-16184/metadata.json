{
    "arxiv_id": "2303.16184",
    "paper_title": "VMesh: Hybrid Volume-Mesh Representation for Efficient View Synthesis",
    "authors": [
        "Yuan-Chen Guo",
        "Yan-Pei Cao",
        "Chen Wang",
        "Yu He",
        "Ying Shan",
        "Xiaohu Qie",
        "Song-Hai Zhang"
    ],
    "submission_date": "2023-03-28",
    "revised_dates": [
        "2023-03-29"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV",
        "cs.GR"
    ],
    "abstract": "With the emergence of neural radiance fields (NeRFs), view synthesis quality has reached an unprecedented level. Compared to traditional mesh-based assets, this volumetric representation is more powerful in expressing scene geometry but inevitably suffers from high rendering costs and can hardly be involved in further processes like editing, posing significant difficulties in combination with the existing graphics pipeline. In this paper, we present a hybrid volume-mesh representation, VMesh, which depicts an object with a textured mesh along with an auxiliary sparse volume. VMesh retains the advantages of mesh-based assets, such as efficient rendering, compact storage, and easy editing, while also incorporating the ability to represent subtle geometric structures provided by the volumetric counterpart. VMesh can be obtained from multi-view images of an object and renders at 2K 60FPS on common consumer devices with high fidelity, unleashing new opportunities for real-time immersive applications.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.16184v1"
    ],
    "publication_venue": "Project page: https://bennyguo.github.io/vmesh/"
}