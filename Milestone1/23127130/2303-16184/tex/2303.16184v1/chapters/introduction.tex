\section{Introduction}
Modern graphics engines mostly rely on polygonal meshes for efficient scene representation and rendering. However, when it comes to the problem of novel view synthesis, the rendering quality of mesh-based representations is greatly limited by the quality of the reconstructed geometry. On the other hand, volumetric representations like NeRF~\cite{nerf} have gained more and more attention for their superior view synthesis quality, but at the cost of slow rendering. Although researchers have devoted extensive efforts to enable real-time rendering of volumetric representations~\cite{fastnerf,plenoctrees,snerg,ngp,mobilenerf}, the storage of large amounts of volumetric data often causes severe memory issues, which prevents them from displaying on low-end devices or scaling to high-resolution scenes.

In this paper, we pursue a better representation of the geometry and appearance of an object for view synthesis from multi-view images. The representation should have the following characteristics:
\begin{itemize}
    \item Able to be rendered at a resolution of modern screens (1080P) in real-time (30FPS) on consumer-grade devices (laptops, tablets, mobile phones).
    \item Efficient in storage to model an object, occupying moderate disk space (<50MB).
    \item Has enough capability to model detailed geometry (such as thin structures) and view-dependent effects.
\end{itemize}
Besides these primary goals, we also expect the representation to be editable in some ways and have the potential to be integrated into modern graphics engines.

Mesh-based assets commonly adopted in graphics pipelines would meet most of these goals, i.e., high efficiency in rendering and storage, and great editability.
However, obtaining high-quality meshes and accurate textures of an object from casually-captured images is still an open problem.
On the other hand, the volumetric representation used by NeRF-based methods achieves remarkable performance for modeling object appearance, while being less competitive in all other aspects. Therefore, to achieve our goals, instead of improving the volumetric representation to have the abilities it initially does not have, it is more natural to base on the mesh representation to enhance its capability in geometry and appearance modeling with volumetric primitives. The key here is that the volumetric primitives are expected to be very sparse, modeling only the parts that the mesh struggles to model, e.g., thin structures, and only exist when having enough impact on image quality. In this way, the overhead brought by the volumetric part could be minimized, making it possible for the representation to achieve high efficiency and expressiveness at the same time. 

In practice, if we already have a mesh and a volume, combining the two to render jointly could simply be done by raymarching where the ray terminates at mesh surfaces, and compositing the volume color and surface color. However, the main difficulty lies in obtaining the mesh and volume from multi-view images, which is a non-trivial process, as direct optimization of either a mesh or an explicit volumetric grid can be very challenging under the multi-view setting~\cite{nvdiffrec,directvoxgo,plenoxels}. We thus draw inspiration from recent implicit neural surface reconstruction methods~\cite{volsdf,neus} where signed distance values can be converted to volume densities, and propose to obtain our hybrid representation from an implicit signed distance-density field. In this way, the signed distance field which represents a surface, and the density field which represents a volume can be jointly optimized by volume rendering. Through subsequent quality-preserving quantization steps, the signed distance field and the density field are converted to a triangular mesh and a sparse volume respectively. Furthermore, to model view-dependent effects while enabling efficient rendering and storage, we propose a RefBasis texture representation that mimics physically-based rendering. By representing the view-dependent specular color as the multiplication of a specular tint, a Fresnel attenuation factor, and an environment light given by learned basis functions, our texture representation achieves the best visual quality among existing real-time alternatives with significantly lower storage demand.

We implement a WebGL renderer for our VMesh representation and show that VMesh assets can be rendered in real-time frame rates at HD resolution on consumer-grade devices like mobile phones, tablets, and laptops. It achieves comparable view synthesis quality to existing real-time NeRF variants on various challenging scenes while offering very low storage costs. The explicit mesh surface also facilitates a wide range of applications, including shape deformation and texture editing.
