\section{Related Work}

\subsection{Real-Time View Synthesis}
Real-time rendering has long been an important goal pursued by view synthesis techniques, especially the ones based on neural rendering. Since NeRF~\cite{nerf} emerged as the new quality standard for view synthesis, many attempts have been made to achieve efficient rendering with NeRF while preserving its high quality. Although many of them succeed in efficient rendering on high-end GPUs, few can maintain real-time performance on consumer-grade devices. PlenOctrees~\cite{plenoctrees} utilizes spherical harmonics to represent the view-dependent color, enabling storing volume density and color information in a sparse grid organized by octrees, therefore eliminating the massive computational cost of MLP evaluations. SNeRG~\cite{snerg} applies a deferred rendering scheme so that the view-dependent color only needs to be computed once for each pixel. It uses a very small MLP for color computation to balance the quality and rendering speed. To further improve rendering efficiency, MobileNeRF~\cite{mobilenerf} optimizes a set of triangle faces instead of a volumetric field, taking advantage of the high efficiency of the graphics pipeline. It adopts a similar texture representation as SNeRG and is applicable on common devices like laptops and mobile phones. However, the improvement in rendering efficiency introduces trade-offs with storage. These methods often require hundreds or even thousands of megabytes of disk storage for a single object, as storing densely sampled volumetric data or irregular triangles is not efficient. Moreover, they all lack a surface representation, which makes it hard to support further processing such as editing, simulation, shadow cast, and collision detection, to name a few. VMesh proposed in this paper is based on a textured mesh and only relies on volumetric primitives to enhance its representational ability in certain regions, which makes it both memory-efficient and processing-friendly. Our work is different from those mesh-based inverse rendering methods in that we do not assume a strict physically-based rendering model. Even so, we take inspiration from these methods to design our texture representation, which is demonstrated to be powerful enough for high-quality view synthesis while being editable.

\subsection{Hybrid Volume-Mesh Representation}
Mesh has been widely utilized as geometry guidance for NeRF variants to achieve geometry editing or handle deformable objects. NeRF-Editing~\cite{nerf-editing} allows controllable shape deformation on a NeRF representation by first deforming the extracted mesh and then training a deformation field based on the mesh deformation. NeuMesh~\cite{neumesh} learns vertex features for an extracted mesh and uses it for volume rendering, which also allows view synthesis of deformed objects by directly deforming the proxy mesh. Methods for building head avatars~\cite{instant-volumetric-head-avatars} often take advantage of mesh-based parametric face models to guide the ray marching process. EyeNeRF~\cite{eyenerf} uses an explicit eyeball surface to compute reflection and refraction rays for ray marching. Neural Assets~\cite{neural-assets} extracts a proxy mesh from a trained NeRF to support shadow cast and collision detection. These methods only make use of the geometry of the mesh and solely rely on volume rendering to produce the visual appearance. In contrast, our VMesh representation uses mesh and volume to represent separate geometries, and the mesh can be directly used for rendering. The work of Loubet \textit{et al.}~\cite{hybrid-mesh-volume-lods} is most related to ours, where a hybrid volume-mesh representation is used to build LoDs for complex 3D assets as the mesh is inefficient for pre-filtering sub-resolution structures, especially at large LoD scales. The authors introduce a heuristic approach to automatically find sub-resolution geometry in a mesh, and perform pre-filtering by voxelization. We share the same intuition to represent subtle structures with volume but manage to obtain such representation from multi-view images for view synthesis instead of processing existing assets.
