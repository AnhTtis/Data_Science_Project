\section{Experiments}
We mainly conduct experiments on the NeRF-Synthetic~\cite{nerf} dataset, which contains 8 challenging scenes with thin structures, highly reflective materials, and complex texture patterns. As we aim at real-time free-viewpoint rendering on consumer devices, we select the baseline methods as follows:
\begin{itemize}
    \item Real-time NeRF variants, including \textbf{PlenOctrees}~\cite{plenoctrees}, \textbf{SNeRG}~\cite{snerg} and \textbf{MobileNeRF}~\cite{mobilenerf}. These methods all work without the need for high-end GPUs. We also compare with a compressed version of PlenOctrees, denoted as \textbf{PlenOctrees-Web}, and two SNeRG variants with lower voxel grid resolution, denoted as \textbf{SNeRG-750} and \textbf{SNeRG-500} respectively.
    \item \textbf{nvdiffrec}~\cite{nvdiffrec}, that extracts the 3D mesh, PBR material and lighting from images. The extracted assets can be directly rendered in traditional graphics pipelines.
\end{itemize}
In \cref{sec:exp-comparisons} we first compare VMesh with these alternatives on image quality, rendering speed at different resolutions on various kinds of consumer devices, and disk storage cost. We compare with \textbf{nvdiffrec} and \textbf{MobileNeRF} on geometry quality as they also produce polygonal meshes. Then we perform thorough ablation studies in \cref{sec:exp-ablation} to show how different design choices could affect the image quality and efficiency. We show some qualitative results on real-captured scenes in \cref{sec:real-captured-results} and present several applications based on our representation in \cref{sec:exp-applications}.

\subsection{Comparisons} \label{sec:exp-comparisons}
\input{figures/comparison}
\input{tables/comparison}
\input{tables/comparison-device}
We compare our VMesh representation with other alternatives on NeRF-Synthetic according to three aspects: (1) view synthesis quality on the test set in PSNR, SSIM and LPIPS; (2) rendering speed on a MacBook Pro (2020, M1) laptop measured with frames per second (FPS); (3) disk storage cost in megabytes. The results are shown in \cref{tab:comparison}, where the numbers are averaged across all 8 scenes. We also provide a bubble chart in \cref{fig:teaser} for more intuitive comparisons. In general, VMesh acts as a trade-off between volume rendering approaches (PlecOctrees, SNeRG, MobileNeRF) and mesh rendering approaches (nvdiffrec). Compared with volume rendering approaches, VMesh achieves competitive view synthesis quality with significantly higher rendering speed (2x faster than MobileNeRF, 5x faster than SNeRG) and lower storage cost (1/10 of MobileNeRF, 1/5 of SNeRG). The improvements could be attributed to the use of mesh for representing macro structures of the object, which is compact in storage and could fully utilize the efficiency of the graphics pipeline. Combined with our efficient texture representation, VMesh is able to render high-resolution images at real-time frame rates on various consumer-grade devices. In \cref{tab:comparison-device} we evaluate the rendering speed at different resolutions on three types of common devices: mobile phones, tablets, and laptops. nvdiffrec produces mesh assets with PBR materials, therefore serving as the upper bound for rendering efficiency. We can see from the table that the rendering speed of both SNeRG and MobileNeRF drops significantly as the resolution goes up, while VMesh maintains acceptable framerates even at 4K resolution. The reason for this discrepancy is that the computationally expensive raymarching processes or MLP evaluations are necessary for each pixel in SNeRG and MobileNeRF, which hinders efficient rendering at high resolutions. In contrast, our VMesh representation contains minimal amounts of volume, and our RefBasis texture representation is considerably more computationally efficient than tiny MLP. Some qualitative comparisons are shown in \cref{fig:comparison}, where we can see that VMesh does well in modeling view-dependent effects, and has enough representational ability in recovering subtle structures thanks to the volumetric counterpart.

\input{figures/comparison-geometry}
\input{tables/comparison-geometry}
In \cref{fig:comparison-geometry}, we qualitatively compare the geometry quality with MobileNeRF and nvdiffrec as they also produce explicit triangular meshes. We also report the average number of vertices and faces on NeRF-Synthetic in \cref{tab:comparison-geometry}. It can be seen that our method achieves the best geometry quality among the three. MobileNeRF optimizes for separate triangles without any connection, which greatly limits its capability in tasks other than view synthesis. nvdiffrec produces meshes with unexpected holes. We believe this originates from the strict PBR assumption, which lacks the capability of explaining global illuminations, bringing geometry artifacts. VMesh creates high-quality meshes that can be drastically simplified without compromising rendering quality. This is due to the fact that we utilize meshes to represent larger, macro-level structures, which can be represented accurately using a smaller number of faces.

\subsection{Ablation Studies} \label{sec:exp-ablation}
\subsubsection{The necessity of volume}
\input{figures/ablation-hybrid}
We first evaluate the necessity of the volumetric part of our VMesh representation. In \cref{fig:ablation-hybrid} we show results on the \textit{ficus} and \textit{ship} scene of the NeRF-Synthetic dataset. These two scenes contain objects with very subtle structures, such as twigs and thin ropes. With only the mesh part, these subtle structures are largely missing as they are hard (also inefficient) to be modeled with surfaces. Volumetric primitives, on the other hand, can be effective in handling these areas. Combining the two, VMesh can represent the challenging scenes very well.

\subsubsection{Texture representation}
\input{tables/ablation-texture}
We evaluate different choices of texture representations in \cref{tab:ablation-texture} to demonstrate the effectiveness of our RefBasis Texture. We investigate five common types of texture representations:
\begin{itemize}
    \item NeRF~\cite{nerf} texture, which takes the geometry feature vector and the viewing direction as input, and models color by an MLP.
    \item IDR~\cite{idr} texture, which takes the geometry feature vector, the viewing direction, and the local normal direction as input, and models color by an MLP.
    \item Ref-NeRF~\cite{ref-nerf} texture, which takes the roughness value, the incident angle, the reflected ray direction, and the geometry feature vector as input, models the specular color by an MLP, and combines with the diffuse color and the specular tint to get the final color.
    \item Spherical Harmonics (SH) texture, which models color by coefficients of SH functions.
    \item Learned Basis (LB) texture, which models color by coefficients of a set of learned basis functions. The basis functions are modeled by an MLP, taking the viewing direction as input.
\end{itemize}
For a fair comparison, we also experiment with variants of these representations (marked with -R in the table) where the viewing direction is replaced with the reflected ray direction. We compare on view synthesis quality of the NeRF-Synthetic test set in the contiguous stage, as well as the number of features N stored at each location to indicate storage costs, and whether the representation can be quantized (Q? column in the table, check mark \cmark for quantizable) for real-time application. For representations that cannot be quantized, we choose N to be close to the one we use. And for SH and LB, we use commonly adopted settings in existing works. As shown in the table, among all quantizable representations, our RefBasis representation achieves the best visual quality with significantly lower storage costs. We achieve such efficiency by storing most of the common information in the neural environment map, which only has to be stored per scene instead of per point. In comparison to non-quantizable representations, RefBasis places greater emphasis on rendering efficiency at the expense of a minor decrease in visual quality.  

\subsubsection{Training strategies}
\input{tables/ablation}
\input{figures/ablation}
We conduct ablation studies on some of the training strategies to demonstrate their importance in achieving promising view synthesis quality, as shown in \cref{fig:ablation} and \cref{tab:ablation}. Without predicted normal (w/o pred. normal), the normal calculated by automatic differentiation can be noisy, leading to inaccurate reflection directions. Removing the volume sparsity regularization in \cref{eqn:volume-sparsity} (w/o $\mathcal{L}_\text{sp}$) will allow the model to use volume to explain view-dependent effects, resulting in foggy blobs as can be seen in the figure. This will also bring unnecessary additional storage costs for the redundant volume content. Without the progressive training strategy for hash encoding illustrated in \cref{sec:implementation-details} (w/o prog. training), shape-radiance ambiguity is more likely to happen, causing incorrect geometry. In the mesh optimization stage, we show that it is very crucial to optimize the extracted mesh for high-quality view synthesis. As can be seen in the figure, the extracted mesh could be far from ideal, with missing structures and inaccurate surfaces. After optimization using silhouette and depth constraints from the contiguous stage, the mesh quality is largely improved. Even so, nearly all the quality loss comes from this stage, further demonstrating the importance of having accurate geometries. In the discretization stage, the fine-tuning step helps remove the seams for better visual quality.

\subsubsection{Storage optimization}
\input{tables/ablation-storage}
For the organization of volume data, we find that using a block size of 16 generally gives the best storage efficiency. We evaluate the impact of different mesh texture sizes on storage cost and show the results in \cref{tab:ablation-storage}. Using a texture resolution of 2048 consumes significantly larger storage (2.5x) but only brings marginal improvement in rendering quality (by +0.1db in PSNR). As a result, we employ a texture resolution of 1024 for all the experiments.

\subsection{Results on Real-Captured Scenes} \label{sec:real-captured-results}
\input{figures/real-scenes}
In \cref{fig:real-scenes} we show qualitative results on real-captured scenes from Mip-NeRF 360~\cite{mipnerf360}. It is essential to point out that this paper only focuses on achieving efficient view synthesis of the foreground object. The background, however, is modeled using a similar approach to NeRF++~\cite{nerf++} and is excluded in the real-time renderer. VMesh successfully generalize on real-world scenes and is able to recover the subtle structures of the foreground objects such as the spokes on the bicycle wheels (up) and the fine filaments on dried flowers (down). 

\subsection{Applications} \label{sec:exp-applications}
\input{figures/applications}
Benefiting from the mesh-based representation, we can intuitively perform various geometry and texture editing tasks which can be hard for volumetric-based methods. In \cref{fig:applications}  we show some examples of editing existing VMesh assets. In the shape deformation example (left), we use the ARAP (as-rigid-as-possible) method to deform the triangular mesh. Once the deformation is done, the deformed object can be instantly put into real-time view synthesis, without the need for re-training like in NeRF-Editing~\cite{nerf-editing}. In the texture painting (middle) and appearance editing (right) examples, we directly manipulate the diffuse and specular tint maps. This also shows that our RefBasis texture formulation is more flexible than existing alternatives like Spherical Harmonics or learned basis functions.
