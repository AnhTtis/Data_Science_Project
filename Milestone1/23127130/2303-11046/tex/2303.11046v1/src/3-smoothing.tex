\section{Smoothing methods}
\label{sec-smoothing}
In this section, the bilinear saddle-point problem and the prox-function are explained, followed by an overview of EGT.

\subsection{Bilinear saddle-point problems}
The bilinear saddle-point problems (BSPPs), which EGT covers, are written in the following form:
\begin{align}
    \min_{\bm x\in\X} \max_{\bm y\in\Y} \bm x^\T \bm A \bm y,
    \label{eqn-bspp}
\end{align}
where $\bm A\in\R^{n\times m}$ is a matrix and $\X \subset \R^n, \Y \subset \R^m$ are convex and compact sets.
The \textit{adjoint form} of the problem is given by
\begin{align}
    \max_{\bm y\in\Y} \min_{\bm x\in\X} \bm x^\T \bm A \bm y.
    \label{eqn-bspp-adjoint}
\end{align}
By Minimax Theorem, these two problems achieve the same optimal value.
In other words, the following equation holds for the optimal solution $\bm x^*\in\X$ and $\bm y^*\in\Y$ of~\eqref{eqn-bspp} and~\eqref{eqn-bspp-adjoint}:
\begin{align}
    \max_{\bm y\in\Y} (\bm x^*)^\T \bm A \bm y
    = 
    \min_{\bm x\in\X} \bm x^\T \bm A \bm y^*.
\end{align}
The error of the pair of the solutions $(\bar{\bm x}, \bar{\bm y})$ can be defined by 
\begin{align}
    \e(\bar{\bm x}, \bar{\bm y}) 
    := \max_{\bm y\in\Y} \bar{\bm x}^\T \bm A \bm y 
    - \min_{\bm x\in\X} \bm x^\T \bm A \bar{\bm y}
    \ge 0,
    \label{eqn-error}
\end{align}
and $\e(\bar{\bm x},\bar{\bm y}) = 0$ if and only if $\bar{\bm x}$ and $\bar{\bm y}$ are the optimal solutions of problems~\eqref{eqn-bspp} and~\eqref{eqn-bspp-adjoint}.

\subsection{Prox-functions}
BSPPs~\eqref{eqn-bspp}, the problems covered by EGT, can be regarded as non-smooth minimization problems because $f(\bm x):= \max_{\bm y\in\Y} \bm x^\T \bm A \bm y$ is generally non-smooth.
One way to $f$ is to consider a \textit{prox-function} which we define below.
\begin{definition}
\label{def-prox}
A function $d\colon S\to\R$ is called a prox-function on a convex compact set $S\subset\R^n$ when satisfying the following conditions:
\begin{itemize}
    \item $d$ is twice differentiable in $\ri S$;
    \item $d$ is $\sigma$-strongly convex in $\ri S$ with respect to some norm $\norm{\cdot}_S$ defined on $\R^n$, i.e.,
    \begin{align}
        \bm\xi^\T \nabla^2 d(\bm x) \bm\xi
        \ge \sigma\norm{\bm\xi}_S^2 \quad \forall \bm x\in\ri S,
        \forall \bm\xi\in\R^n;
    \end{align}
    \item $\min_{\bm x\in S} d(\bm x) = 0$.
\end{itemize}
\end{definition}
For some prox-function $d_2\colon\Y\to\R$ and parameter $\mu_2 > 0$, define the \textit{smoothing} of $f$ as
\begin{align}
    f_{\mu_2}(\bm x) 
    &:= \max_{\bm y\in\Y} \qty{
    \bm x^\T \bm A \bm y
    - \mu_2 d_2(\bm y)
    } 
    \label{eqn-f-mu}
    \\
    &= \mu_2 d_2^*\qty(
    \bm A^\T \bm x / \mu_2
    ).
\end{align}
Since $d_2$ is strongly convex, the maximizer of~\eqref{eqn-f-mu} is unique, then $d_2^*$ is differentiable, which in turn shows that $f_{\mu_2}$ is also differentiable.
Let $D_2 := \max_{\bm y\in\Y} d_2(\bm y)$.
By the definition of $f_{\mu_2}$, we have
\begin{align}
    f_{\mu_2}(\bm x) \le f(\bm x) 
    \le f_{\mu_2}(\bm x) + \mu_2 D_2
    \quad \forall \bm x \in \X,
    \label{eqn-f-f-mu}
\end{align}
then $f_{\mu_2}$ is a good smooth approximation of $f$ for small $\mu_2>0$.


\subsection{Excessive gap technique}
This section provides an overview of EGT.
Let $\phi(\bm y) := \min_{\bm x\in\X} \bm x^\T\bm A\bm y$. 
For some prox-function $d_1\colon\X\to\R$ and $\mu_1 > 0$, define the smooth approximation of $\phi$ similarly to~\eqref{eqn-f-mu}:
\begin{align}
    \phi_{\mu_1}(\bm y) 
    &:= \min_{\bm x\in\X}\qty{
    \bm x^\T\bm A\bm y + \mu_1 d_1(\bm x)
    },
\end{align}
and we also have 
\begin{align}
    \phi_{\mu_1}(\bm y) - \mu_1 D_1 \le \phi(\bm y) \quad \forall \bm y\in\Y,
    \label{eqn-phi-phi-mu}
\end{align}
where $D_1 := \max_{\bm x\in\X} d_1(\bm x)$.
Here we consider the following condition, called \textit{excessive gap condition}:
\begin{align}
    f_{\mu_2}(\bm x) \le \phi_{\mu_1}(\bm y).
    \label{eqn-egc}
\end{align}
If $(\bm x,\bm y)\in\X\times\Y$ and $\mu_1,\mu_2 > 0$ satisfy the excessive gap condition, the error of $(\bm x, \bm y)$, defined in~\eqref{eqn-error}, is bounded by
\begin{align}
    \e\qty(\bm x, \bm y)
    &= f(\bm x) - \phi(\bm y) \\
    &\le f_{\mu_2}(\bm x) - \phi_{\mu_1}(\bm y) + \mu_1D_1 + \mu_2D_2 \\
    &\le \mu_1 D_1 + \mu_2 D_2,
\end{align}
where the first inequality follows from~\eqref{eqn-f-f-mu} and~\eqref{eqn-phi-phi-mu}, and the second inequality is given by the excessive gap condition~\eqref{eqn-egc}.
The central concept of EGT is to find $(\bm x, \bm y)$ such that the excessive gap condition is satisfied for small $\mu_1, \mu_2 > 0$ in order to reduce $\e(\bm x, \bm y)$. To achieve this, the EGT algorithm consists of two parts: \textit{initialization} and \textit{shrinking}.
%The central concept of EGT is to obtain $(\bm x, \bm y)$ such that the excessive gap condition is satisfied with small $\mu_1,\mu_2>0$ to make $\e(\bm x, \bm y)$ small. For this purpose, the algorithm of EGT consists of two parts: \textit{initialization} and \textit{shrinking}.

\begin{itemize}
    \item The \textit{initialization} part: Algorithm~\ref{alg-init} requires $\mu_1,\mu_2>0$ satisfying $\mu_1\mu_2 \ge \norm{\bm A}^2/(\sigma_1\sigma_2)$, where 
    \begin{align}
        \norm{\bm A} := 
        \max_{\norm{\bm x}_\X=1}
        \max_{\norm{\bm y}_\Y=1}
        \bm x^\T \bm A \bm y
    \end{align}
    and $d_1, d_2$ must be $\sigma_1, \sigma_2$-strongly convex with respect to some norm $\norm{\cdot}_\X, \norm{\cdot}_\Y$, respectively.
    Then it generates $\qty(\bm x^0,\bm y^0)$ satisfying the excessive gap condition with its input $\mu_1,\mu_2$.

    \item The \textit{shrinking} part: Algorithm~\ref{alg-shrink} requires $\qty(\bm x^k,\bm y^k)$ and $\mu_1,\mu_2>0$ satisfying the excessive gap condition, and the step size $\tau\in(0,1)$ with $\tau^2/(1-\tau) \le \sigma_1\sigma_2\mu_1\mu_2/\norm{\bm A}^2$.
    Then it generates $\qty(\bm x^{k+1},\bm y^{k+1})$ satisfying the excessive gap condition with $(1-\tau)\mu_1,\mu_2$, i.e. we can shrink $\mu_1$ by a factor of $1-\tau$.
    The shrinking algorithm of the $\mu_2$ version can be obtained similarly.
\end{itemize}

By using Algorithms~\ref{alg-init} and~\ref{alg-shrink} with the parameters and choices between $\mu_1$ and $\mu_2$ to shrink presented in~\cite{nesterov2005excessive}, the solution $\qty(\bm x^k,\bm y^k)$ generated by EGT guarantees the following convergence result~\cite[Theorem 6.3]{nesterov2005excessive}:
\begin{align}
    \e(\bm x^k,\bm y^k) \le 
    \frac{4\norm{\bm A}}{k+1}\sqrt{\frac{D_1D_2}{\sigma_1\sigma_2}}.
    \label{eqn-conv}
\end{align}
Therefore, a small $\norm{\bm A}$ and large values for $\sigma_1/D_1, \sigma_2/D_2$ which we call \textit{substantial strongly convexity} of $d_1, d_2$, are required for better convergence, with some norm $\norm{\cdot}_\X, \norm{\cdot}_\Y$.
In this paper, we only consider the $L_1$-norm, which is desirable because $\norm{\bm A} = \max_{i,j} \abs{A_{ij}}$ does not depend on the dimensions of $\X$ and $\Y$.
In practice, we also require that $\nabla d_1^*$ and $\nabla d_2^*$ are easily computable.

\begin{algorithm}[H]
\caption{Initialization}
\label{alg-init}
\begin{algorithmic}[1]
\Require $\mu_1, \mu_2 > 0$ satisfying $\mu_1\mu_2 \ge \norm{\bm A}^2/(\sigma_1\sigma_2)$
\Ensure $\bm x^0, \bm y^0$ satisfying the excessive gap condition~\eqref{eqn-egc} with $\mu_1, \mu_2$
\State $\bar{\bm x} \gets \nabla d_1^*(\bm 0)$
\State $\bm y^0 \gets \nabla d_2^*(\bm A^\T\bar{\bm x} / \mu_2)$
\State $\bm x^0 \gets \nabla d_1^*(\nabla d_1(\bar{\bm x}) - \bm A \bm y^0/\mu_1)$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Shrinking ($\mu_1$ version)}
\label{alg-shrink}
\begin{algorithmic}[1]
\Require
\Statex $\bm x^k, \bm y^k, \mu_1, \mu_2$ satisfying the excessive gap condition~\eqref{eqn-egc}
\Statex $\tau \in (0, 1)$ satisfying $\tau^2/(1-\tau) \le \sigma_1\sigma_2\mu_1\mu_2/\norm{\bm A}^2$
\Ensure
\Statex $\bm x^{k+1}, \bm y^{k+1}$ satisfying the excessive gap condition with $(1-\tau)\mu_1, \mu_2$
\State $\bar{\bm x} \gets \nabla d_1^*(-\bm A \bm y^k / \mu_1)$
\State $\bar{\bm y} \gets \nabla d_2^*(\bm A^\T ((1-\tau) \bm x^k + \tau \bar{\bm x}) / \mu_2)$
\State $\bm y^{k+1} \gets (1-\tau) \bm y^k + \tau \bar{\bm y}$
\State $\bm x^{k+1} \gets (1-\tau) \bm x^k + \tau \nabla d_1^*(\nabla d_1(\bar{\bm x}) - \frac{\tau}{(1-\tau)\mu_1}\bm A\bar{\bm y})$
\end{algorithmic}
\end{algorithm}