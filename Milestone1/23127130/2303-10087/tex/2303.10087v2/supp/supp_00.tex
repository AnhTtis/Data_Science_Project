% 1. 7Scenes GT APRs + DFNet+NFF
% 2. Comparison with 3D/Multi-frame APR/Unlabeled APR


% \section{Time Complexity Ablation/Accelerated NFF}
% 8. Accelerated NFF post-processing and Time complexity ablation



% \section{DSAC*+NFF}
% \cs{Victor: Test DSAC+NFF on 7-Scenes}~

% w/ semantic vs. w/o semantic
% \clearpage
\section{Supplementary}
\subsection{Implementation Details}
% Here, we provide more implementation details in addition to the ones mentioned in Sec.\ 4.1 of the main paper.

\subsubsection{Architecture details}
The model is trained with the re-aligned and re-centred poses in SE(3), as in \cite{Mildenhall20}. 
We use a coarse-to-fine sampling strategy with 64 sampled points per ray in both stages. 
The width of the MLP layers is $128$ and we output $N_c=3$ and $N_f=128$ in the last layer of the fine stage MLP. 
For the exposure-adaptive ACT module, we compute the query image's histogram $\mathbf{y}_I$ in YUV color space and bin the luminance channel into $N_b=10$ bins. We then feed the binned histogram to 4-layer MLPs with a width of 32. The exposure-adaptive ACT module outputs the exposure compensation matrix $\mathbf{K}$ and the bias $\mathbf{b}$, which directly transform the integrated colors $\hat{\mathbf{C}}_{NFS}(\mathbf{r})$ of the main networks, with negligible computational overhead.
We run the APR refinement process for $m$ iterations per image using the direct feature matching loss $\mathcal{L}_{feature}$ with a learning rate of $1\times10^{-5}$. Our default value for $m$ is 50 unless specified, denoted as NeFeS$_{50}$. The NeFeS model renders features with a shorter side of 60 pixels and then upsample them using bicubic interpolation to 240 for feature matching.

\subsubsection{Progressive training schedule} The training process for the NeFeS network starts with the photometric loss only for $T_1=600$ epochs by setting $\lambda_1 =\lambda_2 = 0$ in Eq.\ (4). The color and density components of the model are trained with a learning rate of $5 \times 10^{-4}$ which is exponentially decayed to $8 \times 10^{-5}$ over 600 epochs. We randomly sample $1536$ rays per image and use a batch size of $4$. After $600$ epochs, we reset the learning rate to $5 \times 10^{-4}$ and switch on the feature loss ($\mathcal{L}_f$ in Eq.\ (6)) for the next $T_2=200$ epochs with $\lambda_1 =0.04, \lambda_2 = 0$. The fusion loss ($\mathcal{L}_{fusion}$ in Eq.\ (7)) is switched on for the last $T_3=400$ epochs with coefficients $\lambda_1 =0.02, \lambda_2 = 0.02$. During the third training stage $T_3$, instead of randomly sampling image rays, we randomly sample $N_{crop}\!=\!7$ image patches of size $S\times S$ where $S\!=\!16$.
% The fused feature maps have the same input and output size following common image restoration methods \cite{dong15SRCNN,kim16VDSR}. 
To extract image features (i.e.\ $\mathbf{F_{img}}(I, \cdot)$) as pseudo-groundtruth, we use the finest-level features from DFNet's \cite{chen2022dfnet} feature extractor module. We resize the shorter sides of the feature labels to $60$.





\subsection{Refinement for Different APRs Full}
This is the supplementary full table for Section 4.3 of the main paper (\cref{table:Refinemen_for_Diff_APRs_FULL}).
\input{supp/table_tex/table5}
\input{fig_tex/figure4_vis_colmap_gt_pose}
\subsection{Qualitative Comparisons}
In \cref{fig:7scenes_qualitative}, we qualitatively compare the refinement accuracy of different APR methods - namely PoseNet\cite{Kendall15,Kendall16,Kendall17}, MS-Transformer\cite{Shavit21multiscene}, DFNet \cite{chen2022dfnet} - with our method, i.e.\ DFNet+NeFeS$_{50}$. We can observe that our method produces the most accurate poses (compared to ground-truth) and has a significant improvement over DFNet in different scenes such as fire [$1000$-$1500$] and kitchen [$1000$-$1500$].
\input{supp/fig_tex/figure1}


\subsection{7-Scenes Dataset Details}
In Sec.\ 4.2 of the main paper, we mention the difference between the dSLAM-generated ground-truth pose and the SfM-generated ground-truth pose for the 7-Scenes dataset. We provide more details in this section.

\textbf{dSLAM vs. SfM GT pose}
Brachmann \emph{et al.} \cite{brachmann2021limits} identified imperfections in the original `ground-truth' (GT) poses generated by dSLAM in the 7-Scenes dataset. The erroneous GT poses originate from sensor asynchronization between the captured RGB images and depth maps. Therefore, Brachmann et al. employed SfM to regenerate a new set of `ground-truth' poses, which subsequently aligned and scaled to match the dSLAM-derived poses. As described in Sec.\ 4.2 of the main paper, we notice that when trained with the SfM ground-truth poses, the rendering quality of NeRF is noticeably boosted compared with using the dSLAM GT poses. The comparison between the trajectories of two sets of ground-truth poses is visualized in \cref{fig:vis_colmap_gt_pose}. An interesting observation is made based on the results presented in Table 2 of our main paper. We notice DFNet achieves superior performance when trained with SfM-grounded GT data, surpassing its performance as originally reported \cite{chen2022dfnet}. This phenomenon may be attributed to utilizing the improved synthetic dataset generated by NeRF during DFNet's \textit{Random View Synthesis} training.

\textbf{APR Comparisons with dSLAM GT.}
To supplement Table 2 of the main paper, we compare previous methods and our method when trained and evaluated using dSLAM GT poses. The results can be found in \cref{supp:table:7scene_dslam}. Note that the pose error is presented in cm/degree to emphasize the distinctions in translational accuracy. Despite NeFeS models being trained using suboptimal dSLAM GT poses in this experiment which reduces the quality of the feature rendering, our model is able to achieve SOTA performance on single-frame APR comparisons. Notably, Coordinet+LENS \cite{Moreau21} is the only single-frame APR technique that achieves our method's proximate outcomes (on translational error). However, it's pertinent to note that LENS requires several days to train a high-quality NeRF model per scene. In stark contrast, the NeFeS model requires a much shorter training duration of approximately 5-20 hours, accompanied by an inference speed over 110 times faster and obviated the need for manual parameter tuning, making NeFeS a notably more cost-effective prospect.

Furthermore, we experiment to see if the current dSLAM pose results can be improved if a better quality NeFeS model is used. We performed joint optimization of NeFeS and ground truth camera poses during training using the method introduced in NeRF-- -- \cite{wang2021nerfmm}. The outcomes reveal that while the NeFeS model attains an enhanced training PSNR from 23.33dB to 27.88dB and the median translation error improves by $1 cm$, the rotation error worsens by $0.07\degree$ since jointly optimizing the dSLAM GT training poses also slightly shifts the world coordinate system of the radiance fields. This refined model's performance is denoted as DFNet + NeFeS$^{- -}_{50}$, as indicated in \cref{supp:table:7scene_dslam}.


\input{supp/table_tex/table1}

\subsection{Comparison with Other Camera Localization Approaches}
Although our paper mainly focuses on test-time refinement on single-frame APR methods, it is only one family of approaches in camera relocalization (see our Related Work section). In \cref{supp:table:our_vs_other}, we compare geometry-based methods and sequential-based methods for camera localization, as well as adding several other single-frame APR methods, including some without code available publicly to support a more thorough comparison. The results on 7-Scenes dataset are evaluated using the original SLAM ground-truth pose, except methods marked by ``(COLMAP)'', which indicates the results evaluated using the COLMAP ground-truth pose for 7-Scenes. The methods that marked by ``(COLMAP to build 3D model)'' indicates COLMAP generated 3D models are used in training and evaluation.

We show that when compared with sequential-based APR methods, our method achieves very competitive results on Cambridge Landmark dataset and 7-Scenes dataset. In addition, for the first time, we show that a single-frame APR method can obtain accuracy of the same magnitude as 3D geometry-based approaches.

%\cs{The only single-frame APR method that is able to achieve closer results as ours is Coordinet+LENS scheme. However, it requires several days to train a high-quality NeRF model for each scene. Our NeFeS, on the other hand, only requires about 5-20 hrs to train and 110x+ faster to inference. Making it a much cheaper and yet superior method.}~

\input{supp/table_tex/table2}

\subsection{Featuremetric vs. Photometric Refinement}
In this section, we study the differences between feature-metric refinement and photometric refinement. Prior literature such as iNeRF \cite{yen2020inerf}, NeRF$--$ \cite{wang2021nerfmm}, BARF \cite{lin21barf}, GARF \cite{Chng22garf}, and NoPe-NeRF \cite{bian22nope}, have attempted to `invert' a NeRF model with photometric loss for pose optimization. 

However, directly comparing our featuremetric method with these methods would not be appropriate due to the following reasons:
\textbf{Firstly}, these methods \cite{wang2021nerfmm, lin21barf, Chng22garf, bian22nope} optimize both camera and NeRF model parameters simultaneously but are unsuitable for complex scenes with large motion (e.g.\ 360\degree scenes) since each frame's camera pose is initialized from an identity matrix.
% try to solve a more challenging task by optimizing both camera parameters and NeRF model parameters simultaneously. However, these methods are not well-suited for complex scenes with large motion (e.g., 360-degree scenes), as each initial camera pose is only from identity.
\textbf{Secondly}, these methods do not effectively handle exposure variations, resulting in suboptimal rendering quality.
\textbf{Thirdly}, even with a coarse camera pose initialization, photometric-based inversion methods cannot prevent drifting in refined camera poses, leading to misalignment with the ground truth poses of testing sequences.

Therefore, for a fair comparison with photometric methods, we define a photometric refinement model as the baseline model to compare with. Specifically, for the baseline model, the main architecture from the NeFeS model is maintained but without the feature outputs, and only the RGB colors $\hat{\mathbf{C}}(\mathbf{r})$ are used for photometric pose refinement. The performance of two cases with photometric refinement are demonstrated in \cref{supp:table:photometric-refinement}: first is a sparse photometric refinement that randomly samples pixel-rays, similar to iNeRF \cite{yen2020inerf} or BARF\cite{lin21barf}-like methods; and the other uses dense photometric refinement, which renders entire RGB images. The results indicate that our featuremetric refinement is more robust than all the photometric refinement baselines, as it achieves lower pose errors after $50$ iterations of optimization.

\input{supp/table_tex/table3}

\subsection{Benefit of splitting $lr_R$ and $lr_t$}
As described in Sec.\ 3.3 of the main paper, we find using different learning rates for translation and rotation components as beneficial for fast convergence when we directly refine the camera pose parameters. In this section, we use a toy experiment to illustrate how we determine to use this strategy. We select $20$\% of \textit{Cambridge: Shop Facade}'s test images and perform direct pose refinement for $20$ iterations using our NeFeS model. In \cref{supp:table:split-LRs}, we compare our \textit{different} learning rate setting with several cases of \textit{same} learning rate settings. The learning rate $lr_{R}=lr_t=0.003$ is used in \cite{lin21barf,Chng22garf} and $lr_{R}=lr_t=0.001$ is used in \cite{wang2021nerfmm,yen2020inerf}. We show that by utilizing a different learning rate strategy, the pose error converges much faster and is more stable for both camera position and orientation.

\input{supp/table_tex/table4}

\subsection{Runtime Analysis}

\textbf{Runtime cost.}
Due to better implementation flexibility, we used an unoptimized version of NeFeS in this study. The pytorch-based NeFeS currently runs at 6.9 fps per image including its backpropagation, which is 3x faster than DFNet's NeRF-Hist \cite{chen2022dfnet} and 110x faster than LENS's NeRF-W \cite{Moreau21}.
It is crucial to emphasize that further optimization can be pursued to attain commercial-level efficiency. For example, NeFeS can potentially be accelerated up to 66x using the C++/CUDA-based \texttt{tiny-cuda-nn} and \texttt{instant-ngp} \cite{muller2022instant} frameworks.% Consequently, the time required for NeFeS feature rendering and BP can be reduced to 2.2ms/iteration.

\textbf{Training cost.}
Our NeFeS can be trained in parallel with the APR method such as DFNet and takes roughly the same time as the underlying APR method (\textit{i.e.} \ 5-20 hrs depending on scene size). However, the NeFeS model only needs to be trained \textbf{once} and the same model can be applied to different APR methods.
%, hence, amortizing the training cost.


% \subsection{Discussion} 1. People work on APR due to its potential simplicity to train end-to-end. APR doesn't encode geometric information, dsac does encode geometric information. Approach like dsac already embedded strong geometric in its network which is a failure case of ours.  
% 2. our method is not scene agnostic, future work may involve few shot nerf
% \cs{Limitation section. People work on APR due to its potential simplicity to train end-to-end. APR doesn't encode geometric information, dsac does encode geometric information. Approach like dsac already embedded strong geometric in its network which is a failure case of ours.}~