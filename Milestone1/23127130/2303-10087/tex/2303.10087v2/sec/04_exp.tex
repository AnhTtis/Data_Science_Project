
\section{Experiments} \label{sec:experiments}
% \subsection{Implementation Details}
We implement our method in PyTorch \cite{paszke2019pytorch}. 
% The model is trained with the re-aligned and re-centred poses in SE(3), as in \cite{Mildenhall20}. 
% We use a coarse-to-fine sampling strategy with 64 sampled points per ray in both stages. 
% The width of the MLP layers is $128$ and we output $N_c=3$ and $N_f=128$ in the last layer of the fine stage MLP. 
% For the exposure-adaptive ACT module, we compute the query image's histogram $\mathbf{y}_I$ in YUV color space and bin the luminance channel into $N_b=10$ bins. We then feed the binned histogram to 4-layer MLPs with a width of 32. The exposure-adaptive ACT module outputs the exposure compensation matrix $\mathbf{K}$ and the bias $\mathbf{b}$, which directly transform the integrated colors $\hat{\mathbf{C}}_{NFS}(\mathbf{r})$ of the main networks, with negligible computational overhead.
% We run the APR refinement process for $m$ iterations per image using the direct feature matching loss $\mathcal{L}_{feature}$ (\textit{cf.}\ \cref{eq:featloss}) with a learning rate of $1\times10^{-5}$. Our default value for $m$ is 50 unless specified, denoted as NeFeS$_{50}$. The NeFeS model renders features with a shorter side of 60 pixels and then upsample them using bicubic interpolation to 240 for feature matching.
Implementation details about the NeFeS architecture, progressive training scheduling, and pose refinement can be found in the supplementary\footnote{Supplementary: Implementation Details}.

\input{table_tex/table2_7scenes_main.tex}
\subsection{Evaluation on Cambridge Landmarks}
We evaluate our proposed refinement method on Cambridge Landmarks \cite{Kendall15}~, which is a popular outdoor dataset used for benchmarking pose regression methods. The dataset contains handheld smartphone images of scenes with large exposure variations and covers an area of 875$m^2$ to 5600$m^2$. The training sequences contain 200-1500 samples, and test sets are captured from different sequences. 
For each test image, we refine the model using the approach in \cref{sec:PP_APR} for $m=50$ iterations.

We first test our method on top of an open-sourced SOTA single-frame APR method. \cref{table:1} summarizes the results of our method and existing APR methods. Our method achieves the best accuracy across all four scenes when coupled with the DFNet. In \cref{sec:generalization}, we demonstrate the performance of our method with other APR approaches. Particularly, our method improves DFNet by as much as 70.6\% compared to its scene average results. All the per-scene performances from the other compared methods are taken from their papers, except for MS-Transformer+PAE, which only reports the scene average median errors. We encourage our readers to check out supplementary for more thorough comparisons and ablations to the Cambridge Landmarks dataset.
\input{fig_tex/figure3_7scenes_dslam_vs_colmap}
\subsection{Evaluation on 7-Scenes}

We further evaluate our method on Microsoft 7-Scenes dataset \cite{Glocker13,Shotton13}, which includes seven indoor scenes ranging in size from $1m^3$ to $18m^3$ and up to 7000 training images for each scene. We sub-sampled the large training sequences in the dataset to 1000 images for training our NeFeS model. 
The original `ground truth' (GT) poses are obtained from RGB-D SLAM (dSLAM) \cite{Newcombe11KinectFusion}. However, we observe imperfection in the GT poses due to the asynchronous data between the RGB and depth sequences, and this results in low-quality NeRF renderings, as shown in \cref{fig:7scenes_dslam_vs_colmap}. Thus, we use alternative `ground truth' provided by \cite{brachmann2021limits} for our experiments. The authors \cite{brachmann2021limits} demonstrate that their camera poses reconstructed by COLMAP \cite{schoenberger2016sfm}, a structure-from-Motion (SfM) library, are more accurate for image-based relocalization. We refer the reader to our supplementary \footnote{Supplementary: 7-Scenes Dataset Details} for more details about the GT poses.

For a fair comparison, we use the SfM poses to re-train baseline APR methods using their official code, except for PoseNet, in which we use the open-sourced code from \cite{chen21}. We trained each APR method 3-4 times to select the best-performing model. 
We use DFNet + NeFeS$_{50}$ with the same settings as in Cambridge for our pose refinement experiment. We achieve state-of-the-art results (59\%+ better in scene average) in all scenes by running 50 optimization steps (see \cref{table:2}). The results by using the dSLAM ground-truth poses are included in the supplementary\footnote{Supplementary: APR Comparisons with dSLAM GT}, where we also achieve SOTA accuracy.

It is imperative to note that our method is not constrained by the number of optimization steps for the refinement process. In our experience, nearly 50\% of the entire pose error improvement is accomplished within the initial 10 steps. It is up to the user to find the optimal trade-off that fits their computational budget. A detailed analysis of this facet is provided in \cref{sec:iteration_ablation}.

\subsection{Refinement for Different APRs} \label{sec:generalization}
\input{table_tex/table3.tex}
\input{table_tex/table5.tex}

\input{fig_tex/figure6_disturb_test} 
\cref{table:3} shows the results of our method with different APR architectures. Our proposed method exhibits versatility, operating beneficially under various APR architectures, such as PoseNet (classic pose regression architecture), MS-Transformer (EfficentNet CNN backbones with transformer blocks), and DFNet (multi-task network that predicts domain invariant features and poses). 
A full table with per-scene results is provided in Supplementary. %The pose accuracy is improved with our method in all three APR methods and both datasets. 

% PoseNet is the classic pose regression architecture. MS-Transformer is denoted as MS-Trans., which combines EfficentNet CNN backbones with transformer blocks. DFNet is a multi-task network that predicts domain invariant features and poses.

\subsection{Optimize APR vs. Optimize Pose}

Besides boosting APR, our proposed approach can also refine the initial coarse camera pose, as outlined in \cref{sec:PP_Pose}. We first show a use case of this scenario by coupling our method with image retrieval, where the initial pose can only be optimized due to the non-differentiable nature of the retrieval process. Given a query image, we retrieve its nearest neighbor from the training data using NetVLAD \cite{arandjelovic2016netvlad} and use the associated pose as the initial pose. We set the learning rate to be $lr_R$ and $lr_t$ for rotation and translation components, respectively. Specifically, for indoor scenes, we set $lr_R=0.0087$ (corresponds to $0.5\degree$ in radiance) and $lr_t=0.01$. For outdoor scenes, we set $lr_R=0.01$ and $lr_t=0.1$. 
\cref{table:5} summarises the experimental results, indicating substantial improvements in pose accuracy over the NetVLAD retrieved coarse pose, exceeding the performance of many prior APR approaches.

We further conducted a controlled experiment to investigate whether performance disparities exist between two types of optimization: optimizing the APR's parameters or directly optimizing the pose itself. We evaluated both modes on the DFNet with NeFeS$_{50}$ refinement, as illustrated in \cref{table:4}. The result suggests that while both refinement approaches can effectively improve the pose accuracy, optimizing the neural network's parameters obtain better result than directly optimizing the pose itself, which is an interesting insight. Nevertheless, optimizing the pose remains is also valuable, particularly when the initial pose is derived from a non-differentiable or a black-box pose estimator.

\input{table_tex/table4.tex}

\subsection{Pose Refinement Bounds}

Our proposed refinement method relies on matching rendered features with query image features during test time,
so it may fail when there is not sufficient overlap between the two feature maps. 
To determine the bounds of our refinement method, we randomly perturb the ground-truth pose and determine the maximum perturbation at which our method stops converging. We jitter the orientation or position of the ground truth pose components separately while gradually increasing the magnitude of the perturbation. We use two scenes, an indoor scene (\textit{7-Scenes: Heads}) and an outdoor scene (\textit{Cambridge: Shop Facade}), to illustrate our results in \cref{fig:perturb_test}. We observe that our method cannot refine pose errors larger than $35\degree$. In case of translational errors, our method can refine errors up to $0.6m$ on \textit{Heads} (indoor scene) and up to $4m$ in \textit{Shop Facade} (outdoor scene). This difference may come from the discrepancy in scale between indoor and outdoor settings. For example, in the small-scale scene of \textit{Heads}, the camera is closer to the objects, hence even small movements lead to a large change in the rendering.

\subsection{NeFeS Ablation} \label{sec: nfs_ablation}
This section presents the ablation study of our NeFeS network. In \cref{table:6}a, we gradually remove the exposure-adaptive ACT and the Feature Fusion module and evaluate their impact on the performance of our approach on \textit{Cambridge Shop Facade}. The results demonstrate that the removal of each component leads to a deterioration in pose accuracy, indicating the effectiveness of both components. A noteworthy insight from our architecture design is that the superior pose estimation accuracy is attributed to integrating both Exposure-adaptive ACT and the Feature Fusion module. The feature fusion module can smooth potential noises (outliers) from directly-rendered 3D features.

In \cref{table:6}b, we compare our progressive training scheduling with the combined scheduling, where all three loss terms have been enabled simultaneously since the beginning of the training. The results reveal that the progressive training scheduling results in better accuracy, providing further support for our design decisions.

\input{table_tex/table6}

\subsection{Number of Iterations vs. Accuracy Trade-off} \label{sec:iteration_ablation}
\input{fig_tex/figure5_iter_test}
In \cref{fig:iter_test}, we plot the relationship between the number of optimization iterations and pose error. Both translation and rotation errors reduce significantly in only $10$ iterations. The errors start to plateau around $50$ steps. Although we can achieve even lower errors with more iterations, we think this strikes a balance between accuracy and efficiency, and explains how we set our previous experiments.

\subsection{Spatial vs Channel-wise Normalization}
As described in \cref{sec:PP_APR}, we empirically find spatial-wise normalized features yield higher accuracy than channel-wise normalized features when computing the feature cosine similarity loss $\mathcal{L}_{feature}$, evidenced by \cref{supp:table:channel_spatial_norm}. This is likely due to our spatially sensitive dense direct matching, akin to methods like Direct Sparse Odometry \cite{Engel17}. In contrast, channel-wise normalization can introduce inconsistencies among neighboring pixels.
\input{supp/table_tex/table6}