\section{Method} \label{sec:method}
In this section, we present a detailed outline of our approach. \cref{sec:PP_APR} provides a high-level overview of our refinement framework. \cref{sec:NFS} describes the architecture and training details of our proposed NeFeS network along with its two components: \textit{Exposure-adaptive Affine Color Transformation (ACT)} and \textit{Feature Fusion module}. 

\subsection{Refinement Framework for APR} \label{sec:PP_APR}
Given a query image $I$, an absolute pose regression (APR) network $\mathcal{F}$ directly regresses the camera pose $\hat{P}$ of $I$: $\hat{P} = \mathcal{F}(I)$.
The network is typically trained with ground truth image-pose pairs. 
While APR-based methods are much more efficient than geometry-based methods since they require only a single forward pass of the network, the quality of their predictions is often significantly worse than those of geometry-based methods due to the lack of any 3D geometry-based reasoning~\cite{Sattler19}.

In contrast to prior APR research, which attempts to improve APR by adding constraints to the training loss or making architectural changes to the backbone network, we propose an alternative method to refine the results of APR methods by backpropagating a feature-metric error at inference time. 
Our method has three major components (see \cref{fig:pipeline}): (1) a pretrained APR network, denoted as $\mathcal{F}$, which provides an initial pose; (2) a differentiable novel feature synthesizer $\mathcal{N}$ that directly renders dense feature maps given a camera pose; (3) an off-the-shelf feature extractor $\mathcal{G}$ that extracts the dense feature map of the query image. In our implementation, the feature extraction module from \cite{chen2022dfnet} is employed as the feature extractor $\mathcal{G}$.

The refinement procedure is as follows: (i) The query image $I$ is passed through the pretrained APR model $\mathcal{F}$ to predict a coarse camera pose $\hat{P}$.
(ii) The feature synthesizer $\mathcal{N}$ renders a dense feature map $f^{rend}\in \mathbb{R}^{n\times c}$ given the coarse camera pose $\hat{P}$, where $n=h\times w$, and $h$ and $w$ are the spatial dimensions of the feature map\footnote{Note: We treat the $n$ dimension as the feature rather than $c$ dimension.}. 
(iii) At the same time, the feature extractor $\mathcal{G}$ extracts a feature map $f^G=\mathcal{G}(I)$ from the query image, where $f^G\in \mathbb{R}^{n\times c}$. 
(iv) The pose $\hat{P}$ is iteratively refined by 
% either updating the parameters of $\mathcal{F}$ or directly updating the pose $\hat{P}$ through 
minimizing the feature cosine similarity loss $\mathcal{L}_{feature}$ \cite{chen2022dfnet}~ between $f^{rend}$ and $f^G$:

\begin{equation} \label{eq:featloss}
\mathcal{L}_{feature} =  \sum^{c}_{i=1}\left(1 -\frac{\langle f^{rend}_{:,i}\cdot f^{G}_{:,i} \rangle} {\|f^{rend}_{:,i}\|_{2}\cdot\|f^{G}_{:,i}\|_{2}}\right)
\end{equation}
where $f^{rend}_{:,i},f^{G}_{:,i}\in \mathbb{R}^{n}$, $\langle\cdot,\cdot\rangle$ denotes the inner product between two vectors and $\|\cdot\|_{2}$ represents the L2 norm. Different from the common feature matching literature's \cite{sun2021loftr,Li20} convention, our features are normalized along the spatial direction instead of the channel direction to ensure the consistency of the neighboring pixels. 

% \YB{Need to explain why we have cosine similarity loss for test-time refinement and $l_1$ loss (\cref{eq:featloss_1,eq:featloss_2}) during training}~

% \cs{It just experimentally works better...Heuristic design...}~

Our method can be regarded as post-processing to the initial pose $\hat{P}$. We do not save the updated weights of the APR method since we restart from the initial state when given a new query image.
\input{fig_tex/figure2_nfs_arch}
\subsection{Neural Feature Synthesizer} \label{sec:NFS}
% \kejie{Do we have to motivate our method or repeat the differences between our method and previous methods here (also saw this in the previous section)? It should be in related work or intro}
% Prior APR methods \cite{chen2022dfnet, chen21, Moreau21} improve pose accuracy through photometric consistency using rendered novel RGB views of the scene. 
% Recent works \cite{Kobayashi22,neff} extend the NeRF model to encode 3D point features, which are distilled using a feature-metric loss and have been shown to be more robust than the RGB field learned by NeRF. 
% Inspired by this, 
We propose a Neural Feature Synthesizer (NeFeS) model that directly renders dense feature maps of a given viewpoint to refine the predictions of an underlying APR network. Similar to NeRF-W \cite{martinbrualla2020nerfw}, our NeFeS architecture uses a base MLP module with \textit{static} and \textit{transient} heads that predict the static and transient density ($\sigma^{(s)}$ and $\sigma^{(\tau)}$) and view-dependent color ($c^{(s)}$ and $c^{(\tau)}$) respectively, given an input 3D position ($\mathbf{x}$) and viewing direction ($\mathbf{d}$). We use the frequency encoding \cite{vaswani2017attention,Mildenhall20} to encode all 3D positions and view directions. The transient head models the colors of the 3D points using an isotropic normal distribution and predicts a view-dependent variance value ($\beta^2$) for the transient color distribution.
% The original NeRF-W extends the volume rendering approach in NeRF \cite{} by allowing transient objects to be jointly estimated and disentangled from a static representation of the 3D world. Similar to NeRF, the static MLP in NeRF-W predicts a static view-independent density ($\sigma^{(s)}$) and view-dependent color ($c^{(s)}$) given a 3D 
% position ($\mathbf{x}$) and viewing direction ($\mathbf{d}$). 
% The transient component of NeRF-W models the transient pixel colors with an isotropic normal ditribution. In addition to the transient density ($\sigma^{(\tau)}$) and color ($c^{(\tau)}$), the transient MLP also predicts a view-dependent variance ($\beta^2$) for a given 3D position and viewing direction. 
To render the color of a given pixel, the original volume rendering formulation in NeRF \cite{Mildenhall20} is augmented to include the transient colors and densities, and the color of a given image-pixel ($\hat{\mathbf{C}}(\mathbf{r})$) is computed as a composite of the static and transient components. Here, $\mathbf{r}$ denotes the ray (corresponding to the pixel) on which points are sampled to compute the volume rendering quadrature approximation \cite{Max95}. The variances of sampled points along the corresponding ray are also rendered using only the transient densities (and \textit{not} the static densities) to obtain a per-pixel color variance $\beta(\mathbf{r})^2$. We refer reader to \cite{martinbrualla2020nerfw} for more mathematical details on the static+transient volume rendering formulation. 

We expand the output of the static MLP to also predict features for an input 3D position. The output dimension is $N_c + N_f$, where $N_f$ features are predicted along with RGB values. The per-pixel features are rendered using the same volume rendering quadrature approximation \cite{Max95}:
% We take the same Riemann sum \cite{Max95} approach as in NeRF to predict the per-pixel features:
\begin{equation}
    \begin{aligned} \hat{\mathbf{F}}_f(\mathbf{r}) = \sum_{i=1}^N T_i\left(1-\exp \left(-\sigma^{(s)}_i \delta_i\right)\right) \mathbf{f}_i, \\ T_i=\exp \left(-\sum_{j=1}^{i-1} \sigma^{(s)}_j \delta_j\right)\end{aligned}
\end{equation}
where $\mathbf{f}_i$ and $\sigma^{(s)}_i$ are the feature and density predicted by the static MLP for a sampled point on the ray, and $\delta_i$ is the distance between sampled quadrature points $i$ and $i+1$.

\cref{fig:NFS_arch} demonstrates the architecture of our proposed NeFeS model. We propose two crucial components in the rendering pipeline of our NeFeS architecture that ensure the robustness of our rendered features.

\textbf{Exposure-adaptive ACT.} In the context of camera relocalization, testing images may differ in exposure or lighting from training sequences. To address this, DFNet \cite{chen2022dfnet} proposed using the luminance histogram of the query image as a latent code input to the color prediction head of the NeRF MLP. However, since our NeFeS outputs both colors and features simultaneously, we find this approach perturbs the feature output values and causes instability. Ideally, the feature descriptors should be able to maintain local invariance even under varying exposure. Inspired by Urban Radiance Fields (URF) \cite{rematas22urban}, we propose to use an \textit{exposure-adaptive Affine Color Transformation (ACT)} which is a $3\times3$ matrix $\mathbf{K}$ and a $3$-dimensional bias vector $\mathbf{b}$ predicted by a $4$-layer MLP with the query image's luminance histogram $\mathbf{y}_I$. Unlike URF, which uses a pre-determined exposure code, we use the query image's histogram embedding for accurate appearance rendering of unseen testing images.
The final per-pixel color $\hat{\mathbf{C}}(\mathbf{r})$ is computed using the affine transformation as $\hat{\mathbf{C}}(\mathbf{r}) = \mathbf{K}\hat{\mathbf{C}}_{rend}(\mathbf{r}) + \mathbf{b}$, where $\hat{\mathbf{C}}_{rend}(\mathbf{r})$ is the rendered per-pixel color obtained using the static and transient MLPs.

\textbf{Feature Fusion Module} We propose a Feature Fusion module to fuse the rendered colors and features to produce the final feature map. The rendered colors and features are concatenated and fed into the fusion module consisting of three 3x3 convolutions, followed by a 5x5 convolution and a batch normalization layer. During inference, we render colors and features for all $H\!\times\!W$ image pixels and the resulting $H\!\times\!W\!\times\!(N_c+N_f)$ tensor is processed by the module. Note, for efficiency during training, we sample $S\!\times\!S$ regions to render and apply the loss only to those pixels each iteration.

We use $\mathcal{H}$ to represent the fusion module. The final output feature result is:
\begin{equation}
    \hat{\mathbf{F}}_{fusion}(\mathcal{R}) = \mathcal{H}(\hat{\mathbf{C}}(\mathcal{R}), \hat{\mathbf{F}}_f(\mathcal{R}))
\end{equation}
% \begin{equation}
%     \hat{\mathbf{F}}_{fusion}(\mathcal{R}) = \hat{\mathbf{F}}_f(\mathcal{R}) + \mathcal{H}(\hat{\mathbf{C}}(\mathcal{R}), \hat{\mathbf{F}}_f(\mathcal{R}))
% \end{equation}
where $\mathcal{R}$ is the sampled region as described above. 

We experimentally find that the fusion module produces more robust features than the input rendered features $\hat{\mathbf{F}}_f$. We refer readers to the supplementary for detailed ablations.

\textbf{Training the Feature Synthesizer} \label{sec:training}
The high-level concept of training the NeFeS is motivated by feature field distillation proposed in \cite{Kobayashi22}, which essentially distills the 2D backbone features into a 3D NeRF model. However, 2D features in our NeFeS need to be closely related to the direct matching formulation \cite{Irani99,chen21}. In this work, we use the trained 2D feature extractor from \cite{chen2022dfnet}~ to produce the feature labels due to its effectiveness in generating domain invariant features. %Since our pose refinement pipeline relies on the robustness between image features and rendered features, the effectiveness of the NFS model is strongly correlated to the final accuracy of our optimized camera poses. \YB{I don't understand this last statement.. Is this sentence necessary? needs to be clearer}

\input{table_tex/table1_cambridge_main.tex}
\textbf{Loss Functions.} The total loss used to train our NeFeS model consists of a photometric loss $\mathcal{L}_{rgb}$ and two $l_1$-based feature-metric losses:
\begin{equation}
    \mathcal{L} = \mathcal{L}_{rgb} + \lambda_1\mathcal{L}_{f} +  \lambda_2\mathcal{L}_{fusion}, 
    \label{eq:L_NFS}
\end{equation}
The photometric loss is defined as the negative log-likelihood of a normal distribution with variance $\beta(\mathbf{r})^2$:
\begin{equation} \label{eq:nerfw_color_loss}
    \begin{aligned} \mathcal{L}_{rgb}(\mathbf{r})= & \frac{1}{2 \beta_i(\mathbf{r})^2}\left\|\mathbf{C}(\mathbf{r})-\hat{\mathbf{C}}(\mathbf{r})\right\|_2^2 \\ & +\frac{1}{2} \log \beta(\mathbf{r})^2+\frac{\lambda_s}{K} \sum_{k=1}^K \sigma_k^{(\tau)}\end{aligned}
\end{equation}
where $\mathbf{r}$ is the ray direction corresponding to an image pixel, $\mathbf{C}_i(\mathbf{r})$ and $\hat{\mathbf{C}}_i(\mathbf{r})$ are the ground-truth and rendered pixel colors. The third term in \cref{eq:nerfw_color_loss} is a sum of the transient densities of all the points on ray $\mathbf{r}$ and 
is used to ensure that transient densities are sparse.

The feature losses are simply $l_1$ losses:
\begin{equation} \label{eq:featloss_1}
\mathcal{L}_{f}=\sum_{\mathbf{r} \in \mathcal{R}}\|\hat{\mathbf{F}}_f(\mathbf{r})-\mathbf{F_{img}}(I,\mathbf{r})\|_1.
\end{equation}
and
\begin{equation} \label{eq:featloss_2}
\mathcal{L}_{fusion}=\sum_{\mathbf{r} \in \mathcal{R}}\|\hat{\mathbf{F}}_{fusion}(\mathbf{r})-\mathbf{F_{img}}(I,\mathbf{r})\|_1.
\end{equation}
where $\mathbf{F_{img}}(I,\cdot)$ are the features extracted from the training images using the pre-trained 2D feature extractor \cite{chen2022dfnet}. Note that, $\mathcal{L}_f$ is applied to the rendered features $\hat{\mathbf{F}}_f$ and $\mathcal{L}_{fusion}$ is applied to the fused features $\hat{\mathbf{F}}_{fusion}$. We experimentally find that using $l_1$ gives more robust features than $l_2$ and cosine feature loss for the test time refinement.

\textbf{Progressive Training.} We propose using a progressive schedule to train the NeFeS model. We first train the color and density part of the network for $T_1$ epochs to bootstrap the correct 3D geometry for the network. For these epochs, only $\mathcal{L}_{rgb}$ is used. Then we add $\mathcal{L}_{f}$ with weight $\lambda_1$ for the next $T_2$ epochs to train the feature part of the static MLP. Since the ground-truth features may not be fully multi-view consistent, we apply \textit{stop-gradients} to the predicted density for the feature rendering branch. And finally, we add the feature fusion loss $\mathcal{L}_{fusion}$ with weight $\lambda_2$ for the last $T_3$ epochs. Since the feature fusion module takes both RGB images and 2D features as input, we randomly sample $N_{crop}$ patches of $S\!\times\!S$ regions of the image and features to increase training efficiency. According to our experiments, this progressive training schedule leads to better convergence and performance. In addition, we apply semantic filtering to improve the network training results. Specifically, we use an off-the-shelf panoptic segmentation method \cite{cheng22mask2former} to mask out temporal objects in the scene such as people and moving vehicles.

\subsection{Direct Pose Refinement} \label{sec:PP_Pose}
%\cs{Can we leave it here for better consistency to the paper? otherwise it might be strange to suddenly talk about it in section 4.4}~\\
While our method is primarily designed to optimize APR, it is also possible to directly optimize camera pose parameters. We explore this feature by showing a possible scenario wherein the source of the pose estimation is either a black box or cannot be optimized (e.g.\ the initial camera pose comes from image retrieval). In these settings, we can set up our proposed method to directly refine the camera poses. Specifically, given an estimated camera pose $\hat{P}= [\mathbf{R}|\mathbf{t}]$, where $\mathbf{R}$ is rotation and $\mathbf{t}$ is the translation component, our method optimizes the camera poses using tangent space backpropagation\footnote{We use the LieTorch \cite{Teed21lietorch} library for this.}. Additionally, we found that using two different learning rates for the translation and rotation parts helps achieve faster and more stable convergence for camera pose refinement. This is different from the standard convention used in \cite{wang2021nerfmm,lin21barf,Chng22garf,bian22nope}~. We refer our readers to supplementary material for more details.


