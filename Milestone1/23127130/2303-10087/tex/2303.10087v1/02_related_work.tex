\section{Related Work} \label{sec:related_work}
% \subsection{Absolute Pose Regression} 
\textbf{Absolute Pose Regression.} In absolute pose regression, the camera pose of an input image is directly regressed using a trained deep network. Several CNN-based backbone architectures have been proposed. PoseNet \cite{Kendall15, Kendall16, Kendall17} proposed the first APR solution using GoogLeNet-backbone and has been followed by many different architectures, including the hourglass network \cite{Melekhov17}, attention layers \cite{atloc, Shavit21multiscene,Shavit21}, separate layers for predicting the translation and rotation components \cite{Wu17,Naseer17}, or LSTM layers \cite{Walch17}. 

To further improve APR accuracy, some studies propose to utilize sequential information. Most sequential-based APRs utilize temporal constraints such as visual odometry \cite{Brahmbhatt18, Valada18, Radwan18}, motion \cite{moreau2022coordinet} or temporal \cite{clark2017vidloc} filtering, and multi-tasking \cite{Radwan18}. In addition, recent APR methods also obtain significant improvements thanks to advancements in novel view synthesis. One line of approaches focuses on generating a large amount of extra photo-realistic synthetic data \cite{Purkait18, Moreau21, chen2022dfnet} using randomly sampled virtual camera poses. Other approaches \cite{chen21, chen2022dfnet} use NeRF \cite{Mildenhall20,martinbrualla2020nerfw} as a direct matching module to perform unlabeled finetuning \cite{Brahmbhatt18} using extra images without ground-truth pose annotation. The latter approaches, however, usually take tens of hours of extra finetuning time and are based on the assumption that extra unlabeled data can be easily obtained. 

Most of the works above focus on means to improve the training of APRs. However, this work focuses on generic APR improvement during the test time. Although other solutions exist for APR's test-time post-processing, such as those that utilize extended Kalman filter \cite{moreau2022coordinet}, pose graph optimization \cite{Brachmann18}, or pose auto-encoders \cite{Shavit22PAE}, our methods can significantly improve arbitrary APR's performance on both camera positions and orientations and achieve SOTA results without leveraging unlabeled training.


% sequential
% recent
% NVS in training
% unlabeled training

% \cite{Shavit21multiscene,Shavit21}
% \cite{Brahmbhatt18,Kendall15,Walch17} propose to train the network with a weighted combination of translation and rotation losses. \cite{Brahmbhatt18} also uses a relative pose regression loss between pairs of images and proposes guiding the training process with visual odometry constraints. Some methods have shown improved performance with the use of synthetic \cite{Naseer17,Wu17} and unlabelled \cite{chen2022dfnet} data. VidLoc \cite{clark2017vidloc}, VLocNet \cite{Valada18} and VLocNet++ \cite{Radwan18} propose methods for pose regression based on a sequence of images.

% Other methods propose different strategies to train APR. Bayesian
% PoseNet [14] inserts Monte Carlo dropout to a Bayesian CNN that estimates
% pose with uncertainty.  

% \YB{talk about APR methods. Also, talk about refinement methods like PAE. Also, DF-Net, Direct-PN, LENS, etc.}
% \cs{a brief on NeRF}~
% Novel view synthesis methods have been effectively used to generate synthetic training data \cite{Purkait18,Moreau21} for training APR models.  Neural Radiance Field (NeRF) methods
% \cite{Mildenhall20,kaizhang2020,martinbrualla2020nerfw,barron2021mip,muller2022instant,yu_and_fridovichkeil2021plenoxels} 
% have proven to be effective in synthesizing high quality novel views of a scene. LENS \cite{Moreau21} leverages the scene geometry learned by a NeRF-W \cite{martinbrualla2020nerfw} model to sample relevant virtual camera poses for synthetic data generation. 

\input{fig_tex/figure1_pipeline}
% \subsection{Neural Radiance and Feature Fields}
\textbf{Neural Radiance and Feature Fields.} Neural Radiance Fields (NeRF) \cite{Mildenhall20} has attracted many researchers' interest in recent years and has significantly advanced novel view image synthesis and 3D surface reconstruction. The core of the Neural Radiance Field is an implicit 3D representation and differentiable volume rendering that allow self-supervised optimization from only RGB images without expensive 3D ground-truth annotations. iNeRF \cite{yen2020inerf} firstly shows that NeRF can be inverted to optimize the camera pose. Recent approaches such as NeRF$--$ \cite{wang2021nerfmm}, BARF \cite{lin21barf}, GARF \cite{Chng22garf}, and NoPe-NeRF \cite{bian22nope} show that by treating camera poses as learnable parameters, the NeRF can be simultaneously trained via joint optimization in simple, non-360\degree scenes. Parallel works such as NICE-SLAM \cite{niceslam} and iMAP \cite{imap} use the NeRF representation to predict accurate dense geometry and camera tracking in real-time. Direct-PN \cite{chen21} uses NeRF as a direct matching module to compute the photometric errors and propagate the error gradients back to the pose regression network. DFNet \cite{chen2022dfnet} extends this method to outdoor scenarios by extracting robust features of rendered images using a CNN feature extractor.

Recently, NeRF models have been extended to predict and render \textit{feature fields} in addition to the density and appearance fields. Typically, these feature fields are learned by supervision from a 2D feature extractor using volumetric rendering. \cite{neff,Kobayashi22} showed that these 3D feature fields outperform 2D baselines \cite{dino,clip_lseg} on downstream tasks such as 2D object retrieval or 3D segmentation. CLIP-Fields \cite{clip_fields} showed that the learned feature fields can serve as a scene memory useful for semantic robot navigation. To the best of our knowledge, our work is the first to explore the capability of neural feature fields in camera relocalization task and demonstrate their effectiveness as a test-time pose refinement module. 

% To the best of our knowledge, we are the first work to encode 3D geometric features in neural feature fields and demonstrate its effectiveness as a test-time pose refinement module.


% \subsection{Refinement Methods}
% \YB{PAE, etc. What all can we write here?}
%  \\ \\

% \textbf{Comments/notes:}

% \kejie{Why the fintuning in DF-Net cannot generalise to other methods? What about the limitations of other approaches (i.e. PAE, LENS)}\\

% \cs{Direct-PN does not work on outdoor scene}\\
% \cs{DFNet works fine but still distance from 3D based methods. the direct matching model DFNet$_{dm}$ requires extra unlabeled data to train for tens of hours}\\
% \cs{PAE only refines translation, not rotation. Also the improvement is extremely small}\\
% \cs{LENS is seq. APR. that use NeRF to generate synthetic datasets. Does not refine on test set.}\\
% \cs{NeRF-- etc. is more challenging since they try to reconstruct scene poses from scratch. However, they cannot handle large viewing change scene like ours and cannot handle large photometric distortion scenes like changing exposures in outdoor}