% 1. 7Scenes GT APRs + DFNet+NFF
% 2. Comparison with 3D/Multi-frame APR/Unlabeled APR


% \section{Time Complexity Ablation/Accelerated NFF}
% 8. Accelerated NFF post-processing and Time complexity ablation



% \section{DSAC*+NFF}
% \cs{Victor: Test DSAC+NFF on 7-Scenes}~

% w/ semantic vs. w/o semantic

\section{Implementation Details}
% \cs{Need someone to help with this to realign details with our main paper}~
% \textbf{Architecture of NFS} Our Neural Feature Synthesizer is written in PyTorch. The model is trained with the re-aligned and re-centered pose. The main body of the networks uses 64 coarse and fine sampling with MLP's width of 128. For the fine sampling network, we output $N_c=3$ and $N_f=128$ in the last MLP layer. For the exposure-adaptive ACT module, we compute the query image's histogram $\mathbf{y}_I$ in YUV color space and bin the luminance channel into $N_b=10$ bins. We then feed the binned histogram to 4-layer MLPs with a width of 32. The exposure-adaptive ACT module outputs the exposure compensation matrix $k$ and the bias $b$, which directly transform the integrated colors $\hat{\mathbf{C}}_{NFS}(\mathbf{r})$ of the main networks, with negligible computational overhead.

Here, we provide more implementation details in addition to the ones mentioned in Sec.\ 4.1 of the main paper.

\noindent \textbf{Progressive training schedule.} The training process for the NeFeS network starts with the photometric loss only for $T_1=600$ epochs by setting $\lambda_1 =\lambda_2 = 0$ in Eq.\ (4). The color and density components of the model are trained with a learning rate of $5 \times 10^{-4}$ which is exponentially decayed to $8 \times 10^{-5}$ over 600 epochs. We randomly sample $1536$ rays per image and use a batch size of $4$. After $600$ epochs, we reset the learning rate to $5 \times 10^{-4}$ and switch on the feature loss ($\mathcal{L}_f$ in Eq.\ (6)) for the next $T_2=200$ epochs with $\lambda_1 =0.04, \lambda_2 = 0$. The fusion loss ($\mathcal{L}_{fusion}$ in Eq.\ (7)) is switched on for the last $T_3=400$ epochs with coefficients $\lambda_1 =0.02, \lambda_2 = 0.02$. During the third training stage $T_3$, instead of randomly sampling image rays, we randomly sample $N_{crop}\!=\!7$ image patches of size $S\times S$ where $S\!=\!16$.
% The fused feature maps have the same input and output size following common image restoration methods \cite{dong15SRCNN,kim16VDSR}. 
To extract image features (i.e.\ $\mathbf{F_{img}}(I, \cdot)$) as pseudo-groundtruth, we use the finest-level features from DFNet's \cite{chen2022dfnet} feature extractor module. We resize the shorter sides of the feature labels to $60$.

% \textbf{Pose Refinement}
% As we described in the main manuscripts, our method can operate in two modes: \emph{Refinement of the parameters of APR} and \emph{Direct refinement of the coarse pose}. 
% % For the first mode, we set the APR refinement process to $50$ iterations per image using the direct feature matching loss $\mathcal{L}_{feature}$ with a learning rate of $1\times10^{-5}$. 
% For the second mode, we set the learning rate to be $lr_R$ and $lr_t$ for rotation and translation components, respectively. During both modes, the NFS model renders features with a shorter side of 60 pixels and then upsampls to 240 for feature matching.



\section{7-Scenes Dataset Details}
\input{fig_tex/figure4_vis_colmap_gt_pose}
In Sec.\ 4.3 of the main paper, we mention the difference between the dSLAM-generated ground-truth pose and the SfM-generated ground-truth pose for the 7-Scenes dataset. We provide more details in this section.

\textbf{dSLAM vs. SfM GT pose}
Brachmann \emph{et al.} \cite{brachmann2021limits} discovered imperfections in the original `ground-truth' poses generated by dSLAM in the 7-Scenes dataset. They used SfM to regenerate a new set of `ground-truth' poses, and re-aligned and re-scaled it to match with dSLAM poses. As described in Sec.\ 4.3 of the main paper, we notice that when trained with the SfM ground-truth poses, the rendering quality of NeRF is noticeably boosted compared with using the SLAM ground-truth poses. The comparison between the trajectories of two sets of ground-truth pose is visualized in \cref{fig:vis_colmap_gt_pose}. An interesting observation is made based on the results presented in Table 2 of our main paper. We note that DFNet is able to achieve superior performance when SfM GT is used, compared to its performance reported in the original paper \cite{chen2022dfnet}. This phenomenon may be attributed to utilizing the improved synthetic dataset generated by NeRF during DFNet's \textit{Random View Synthesis} \cite{chen2022dfnet} training.

\textbf{APR Comparisons with dSLAM GT.}
To supplement the Table 2 in the main paper, we provide the comparison between previous methods and our method when trained and evaluated using dSLAM ground-truth pose. Please refer \cref{supp:table:7scene_dslam} for the results. As shown, our method can also improve the state-of-the-art APR performance by a large margin using dSLAM GT.
\input{supp/table_tex/table1}

\section{Comparison to Sequential APR and 3D Approaches}
Our method is a single-frame APR method. In \cref{supp:table:our_vs_other}, we compare with geometry-based methods and sequential-based methods for camera localization. The results on 7-Scenes dataset are evaluated using the original SLAM ground-truth pose, except methods marked by ``COLMAP'' which indicates the results evaluated using the COLMAP ground-truth pose for 7-Scenes.
For the first time, we show that a single-frame APR method can obtain accuracy in the same magnitude as 3D geometry-based approaches. We also show that when compared with sequential-based APR methods, our method achieves competitive results on Cambridge dataset and 7-Scenes dataset.
%Geometry-based methods re-construct 3D scene from the input images. They generally have high accuracy but slow speed. Sequential-based APR methods regress camera pose using multiple frames instead of one. 
%Our method achieves the competitive accuracy compared with geometry-based methods. We regard this as an encouraging result because 
 
\input{supp/table_tex/table2}

\section{Comparisons with Photometric Refinement Methods}
In this section, we study the differences between feature-metric refinement and photometric refinement. Prior literature such as iNeRF \cite{yen2020inerf}, NeRF$--$ \cite{wang2021nerfmm}, BARF \cite{lin21barf}, GARF \cite{Chng22garf}, and NoPe-NeRF \cite{bian22nope}, have attempted to `invert' a NeRF model with photometric loss for pose optimization. 

However, directly comparing our featuremetric method with these methods would not be appropriate due to the following reasons:
\textbf{Firstly}, these methods \cite{wang2021nerfmm, lin21barf, Chng22garf, bian22nope} optimize both camera and NeRF model parameters simultaneously but are unsuitable for complex scenes with large motion (e.g.\ 360\degree scenes) since each frame's camera pose is initialized from an identity matrix.
% try to solve a more challenging task by optimizing both camera parameters and NeRF model parameters simultaneously. However, these methods are not well-suited for complex scenes with large motion (e.g., 360-degree scenes), as each initial camera pose is only from identity.
\textbf{Secondly}, these methods do not effectively handle exposure variations, resulting in suboptimal rendering quality.
\textbf{Thirdly}, even with a coarse camera pose initialization, photometric-based inversion methods cannot prevent drifting in refined camera poses, leading to misalignment with the ground truth poses of testing sequences.

Therefore, for a fair comparison with photometric methods, we define a photometric refinement model as the baseline model to compare with. Specifically, for thie baseline model, the main architecture from the NeFeS model is maintained but without the feature outputs, and only the RGB colors $\hat{\mathbf{C}}(\mathbf{r})$ are used for photometric pose refinement. The performance of two cases with photometric refinement are demonstrated in \cref{supp:table:photometric-refinement}: first is a sparse photometric refinement that randomly samples pixel-rays, similar to iNeRF \cite{yen2020inerf} or BARF\cite{lin21barf}-like methods; and the other uses dense photometric refinement, which renders entire RGB images. The results indicate that our featuremetric refinement is more robust than all the photometric refinement baselines, as it achieves lower pose errors after $50$ iterations of optimization.

\input{supp/table_tex/table3}

\section{Qualitative Comparisons}
% We show several visulization examples for qualitative comparisons on 7-Scenes dataset with several baseline APR methods in Fig. \ref{fig:7scenes_qualitative}.
In \cref{fig:7scenes_qualitative}, we qualitatively compare the refinement accuracy of different APR methods - namely PoseNet\cite{Kendall15,Kendall16,Kendall17}, MS-Transformer\cite{Shavit21multiscene}, DFNet\cite{chen2022dfnet} - with our method, i.e.\ DFNet+NeFeS$_{50}$. We can observe that our method produces the most accurate poses (compared to ground-truth) and has a significant improvement over DFNet in different scenes such as fire [$1000$-$1500$] and kitchen [$1000$-$1500$].
\input{supp/fig_tex/figure1}


\section{Benefit of splitting $lr_R$ and $lr_t$}
As described in Sec.\ 3.3 of the main paper, we find using different learning rates for translation and rotation components as beneficial for fast convergence when we directly refine the camera pose parameters. In this section, we use a toy experiment to illustrate how we determine to use this strategy. We select $20$\% of \textit{Cambridge: Shop Facade}'s test images and perform direct pose refinement for $20$ iterations using our NeFeS model. In \cref{supp:table:split-LRs}, we compare our \textit{different} learning rate setting with several cases of \textit{same} learning rate settings. We show that by utilizing a different learning rate strategy, the pose error converges much faster and is more stable for both camera position and orientation.

\input{supp/table_tex/table4}
% \section{Exposure-adaptive affine color transform}
% Feature Stability before \& after using affine color transform

% \cs{
% Things todo:\\
% % 1. Implemnetation details\\
% % 2. 7-scenes rephrase/dSLAM performance\\
% % 3. Visual 7-scenes (done)\\
% % 4. Ours (1-frame) vs. seq. APR vs. unlabeled APR vs. Geometry-based approach\\
% % 5. nerf--/inerf vs. ours\\
% % 6. Benefit in splitting LRr an LRt\\
% 7. supp video before \& after refine for different APR\\
% 8. supp video visualizaiton dSLAM vs. SfM rendering\\
% backup todos:\\
% 9. Exposure-adaptive ACT feature stability\\
% 10. w/ semantic vs. w/o semantic\\
% }~