
\section{Experiments} \label{sec:experiments}
% In the following, we discuss the implementation details in \cs{text here...}~
\subsection{Implementation Details}
We implement our method in PyTorch \cite{paszke2019pytorch}. 
The model is trained with the re-aligned and re-centred poses in SE(3), as in Mildenhall et al.\cite{Mildenhall20}. 
We use a coarse-to-fine sampling strategy \cite{Mildenhall20} with 64 sampled points per ray in both stages. 
The width of the MLP layers is $128$ and we output $N_c=3$ and $N_f=128$ in the last layer of the fine stage MLP. 
For the exposure-adaptive ACT module, we compute the query image's histogram $\mathbf{y}_I$ in YUV color space and bin the luminance channel into $N_b=10$ bins. We then feed the binned histogram to 4-layer MLPs with a width of 32. The exposure-adaptive ACT module outputs the exposure compensation matrix $\mathbf{K}$ and the bias $\mathbf{b}$, which directly transform the integrated colors $\hat{\mathbf{C}}_{NFS}(\mathbf{r})$ of the main networks, with negligible computational overhead.
% \cs{Change this...}~
% As we described earlier, 
% our method can operate in two modes: \textit{Refinement of APR network parameters} and \textit{Direct refinement of the coarse pose}. For the first mode, 
We run the APR refinement process for $m$ iterations per image using the direct feature matching loss $\mathcal{L}_{feature}$ (\textit{cf.}\ \cref{eq:featloss}) with a learning rate of $1\times10^{-5}$. Our default value for $m$ is 50 unless specified, denoted as NeFeS$_{50}$. The NeFeS model renders features with a shorter side of 60 pixels and then upsample them using bicubic interpolation to 240 for feature matching.
Further implementation details about NeFeS, progressive training scheduling, and pose refinement can be found in the supplementary.

% For the second mode, we set the learning rate to be $lr_R$ and $lr_t$ for rotation and translation components, respectively. 
% Specifically, for indoor scenes, we set $lr_R=0.0087$ (corresponds to $0.5\degree$ in radiance) and $lr_t=0.01$. For outdoor scenes, we set $lr_R=0.01$ and $lr_t=0.1$. During both modes, the NeFeS model renders features with a smaller resolution of $60\times60$ and then upsamples it using bicubic interpolation to $240\times240$ for feature matching.

% Further implementation details about NeFeS, progressive training scheduling, and pose refinement have been included in the supplementary.

% Our Neural Feature Synthesizer is written in PyTorch. The model is trained with the re-aligned and re-centered pose. The main body of the networks uses 64 coarse and fine sampling with MLP's width of 128. For the fine sampling network, we output $N_c=3$ and $N_f=128$ in the last MLP layer. For the exposure-adaptive ACT module, we compute the query image's histogram $\mathbf{y}_I$ in YUV color space and bin the luminance channel into $N_b=10$ bins. We then feed the binned histogram to 4-layer MLPs with a width of 32. The exposure-adaptive ACT module outputs the exposure compensation matrix $k$ and the bias $b$, which directly transform the integrated colors $\hat{\mathbf{C}}_{NFS}(\mathbf{r})$ of the main networks, with negligible computational overhead.

% We detail the implementation details of the NeFeS architecture and pose refinement. Further implementation details about NeFeS, progressive training scheduling, and pose refinement are included in the supplementary.

% \textbf{Architecture of NeFeS.} We implement our method in PyTorch \cite{paszke2019pytorch}. The model is trained with the re-aligned and re-centered pose. The main body of the networks uses 64 coarse and fine sampling with MLP's width of 128. For the fine sampling network, we output $N_c=3$ and $N_f=128$ in the last MLP layer.

% \textbf{Pose Refinement}
% As we described earlier, our method can operate in two modes: \textit{Refinement of the parameters of APR} and \textit{Direct refinement of the coarse pose}. For the first mode, we set the APR refinement process to $m$ iterations per image using the direct feature matching loss $\mathcal{L}_{feature}$ with a learning rate of $1\times10^{-5}$. For the second mode, we set the learning rate to be $lr_R$ and $lr_t$ for rotation and translation components, respectively. During both modes, the NeFeS model renders features with a shorter side of 60 pixels and then upsampls to 240 for feature matching.
\input{table_tex/table1_cambridge_main.tex}
\subsection{Evaluation on Cambridge Landmarks}
We evaluate our proposed refinement method on Cambridge Landmarks \cite{Kendall15}~, which is a popular outdoor dataset used for benchmarking pose regression methods. The dataset contains handheld smartphone images of scenes with large exposure variations and covers an area of 875$m^2$ to 5600$m^2$. The training sequences contain 200-1500 samples, and test sets are captured from different sequences. 
%We first demonstrate the upper bound of our method's performance by refining DFNet, one of the prior single-image SOTA methods. 
For each test image, we refine the model using the approach in \cref{sec:PP_APR} for $m=50$ iterations.

% The proposed pose refinement method is evaluated on a well-known outdoor Cambridge Landmarks \cite{Kendall15}~ dataset, which is commonly used in pose regression methods. The dataset is taken by handheld smart phones which contains scenes with large exposure variations and covers a spatial area of 875 $m^2$ to 5600 $m^2$. The training sequences contains 200+ to 1500 samples and test sets are captured from different sequences from different time. We firstly demonstrate the performance upper bound of our pose refinement method on refining the state-of-the art APR models. In this experiment, we choose to refine one of prior single-image SOTA methods DFNet using our proposed method. For each test image, we refine the model using the approach in \cref{sec:PP_APR} for $m=50$ iterations with our NFS-based direct feature matching optimization.

\cref{table:1} shows summarizes the results of our method and existing APR methods. 
Our method achieves the state-of-the-art accuracy across all four scenes when coupled with the DFNet \cite{chen2022dfnet}. In \cref{sec:generalization}, we demonstrate the performance of our method with other APR approaches.
Particularly, our method can improve by as much as 61.5\% compared to prior best scene average results. All the per-scene performances from compared methods are taken from their papers, except for MS-Transformer + PAE\cite{Shavit22PAE}, which only reports the scene average median errors.

\subsection{Evaluation on 7-Scenes}
\input{fig_tex/figure3_7scenes_dslam_vs_colmap}
\input{table_tex/table2_7scenes_main.tex}
% We further evaluate our method on Microsoft 7-Scenes \cite{Glocker13,Shotton13} dataset. The dataset consists of seven indoor scenes scaled from $1m^3$ to $18m^3$. The original `ground truth' (GT) poses are obtained from RGB-D SLAM \cite{Newcombe11KinectFusion}. However, since NeRF models are sensitive to the GT pose accuracy, we observe the GT poses provided by the original dataset are imperfect and lead to low-quality NeRF renderings as shown in \cref{fig:7scenes_dslam_vs_colmap}. We therefore use alternative `ground truth' data provided by \cite{brachmann2021limits}, where the camera poses are reconstructed by a Structure-from-Motion (SfM) approach COLMAP \cite{colmap1, colmap2}. For more details about GT poses, we refer the reader to our supplementary. Since the dataset contains relatively large training sequences ranging from 1000 to 7000 training images per scene, we sub-sample to 1000 images for training our NFS models.

% For fair comparison, we use SfM-based training set poses to re-train several representative APR methods that released official code, except for PoseNet, which we use the open-sourced code from \cite{chen21}. To ensure the best results of the compared APR methods, we each train the APR methods for 3-4 times to pick the best performance model. One interesting insight is that DFNet is able to achieve better performance than the original paper results on RGB-D SLAM GT. This is probably due to the DFNet uses NeRF to generate more synthetic training dataset during the training stage and improving NeRF quality is also beneficial for generating higher quality synthetic training set. For our pose refinement experiment, we use DFNet + NFS$_{50}$ in same settings as in Cambridge. The experiment results are summarized in \cref{table:2}. We achieve the state-of-the-art results in all scenes by coupling with DFNet with 50 optimization steps. The results of training and testing with the original ground-truth trajectory are included in the supplementary, where we also achieve the state-of-the-art accuracy.


We evaluate our method on Microsoft 7-Scenes dataset \cite{Glocker13,Shotton13}, which includes seven indoor scenes ranging in size from $1m^3$ to $18m^3$ and up to 7000 training images for each scene. We sub-sampled the large training sequences in the dataset to 1000 images for training our NeFeS model. 
The original `ground truth' (GT) poses are obtained from RGB-D SLAM (dSLAM) \cite{Newcombe11KinectFusion}. However, we observe imperfection in the GT poses due to the asynchronous data between the RGB and depth sequences, and this results in low-quality NeRF renderings, as shown in \cref{fig:7scenes_dslam_vs_colmap}. Thus, we use alternative `ground truth' provided by \cite{brachmann2021limits} for our experiments. The authors \cite{brachmann2021limits} demonstrate that their camera poses reconstructed by COLMAP, a structure-from-Motion (SfM) library, are more accurate for image-based relocalization. We refer the reader to our supplementary for more details about the GT poses.

For a fair comparison, we use the SfM poses to re-train baseline APR methods using their official code, except for PoseNet, in which we use the open-sourced code from \cite{chen21}. We trained each APR method 3-4 times to select the best-performing model. 
We use DFNet + NeFeS$_{50}$ with the same settings as in Cambridge for our pose refinement experiment. We achieve state-of-the-art results (54.9\%+ better in scene average) in all scenes by running 50 optimization steps (see \cref{table:2}). The results by using the original ground-truth poses are included in the supplementary, where we also achieve SOTA accuracy.










\subsection{Refinement for Different APRs} \label{sec:generalization}

\input{table_tex/table3.tex}
\input{table_tex/table5.tex}

% Our method is versatile and works under different configurations, such as optimizing multiple APR architectures or poses predicted by non-APR methods. 
\cref{table:3} shows the results of our method with different APRs. It shows that our proposed method exhibits versatility, operating beneficially under various APR architectures, such as PoseNet \cite{Kendall15}, MS-Transformer \cite{Shavit21multiscene}, and DFNet \cite{chen2022dfnet}. 
The pose accuracy is improved with our method in all three APR methods and both datasets.

% We evaluate our method on three APR architectures: 



%Noticeably, we observe over 38\% improvement from PoseNet and MS-Transformer, which demonstrate the capability of our method.

\subsection{Optimize APR vs. Optimize Pose}

Besides boosting APR, our proposed approach can also refine the initial coarse camera pose, as outlined in \cref{sec:PP_Pose}. We first show a use case of this scenario by coupling our method with image retrieval, where the initial pose can only be optimized due to the non-differentiable nature of the retrieval process. Given a query image, we retrieve its nearest neighbor from the training data using NetVLAD \cite{arandjelovic2016netvlad} and use the associated pose as the initial pose. In this experiment, we set the learning rate to be $lr_R$ and $lr_t$ for rotation and translation components, respectively. Specifically, for indoor scenes, we set $lr_R=0.0087$ (corresponds to $0.5\degree$ in radiance) and $lr_t=0.01$. For outdoor scenes, we set $lr_R=0.01$ and $lr_t=0.1$. 
\cref{table:5} summarises the experimental results, indicating substantial improvements in pose accuracy over the NetVLAD retrieved coarse pose, exceeding the performance of many prior APR approaches.



% In addition, as we described in \cref{sec:PP_Pose}, our method can also directly refine the coarse camera pose. We first show one use case of this scenario by coupling our method with image retrieval, where the initial pose can only be optimized since the retrieval process is not differentiable. Given a query image, we retrieve its nearest neighbor from the training data using NetVLAD \cite{arandjelovic2016netvlad} and use the associated pose as the initial pose. In this experiment, we set the learning rate to be $lr_R$ and $lr_t$ for rotation and translation components, respectively. Specifically, for indoor scenes, we set $lr_R=0.0087$ (corresponds to $0.5\degree$ in radiance) and $lr_t=0.01$. For outdoor scenes, we set $lr_R=0.01$ and $lr_t=0.1$. \cref{table:5} summarizes our experiment results and show decent improvement over pose accuracy from the NetVlad retrieved coarse pose, surpassing the performance of many existing APR approaches.



% Instead of 
% , we couple our method with image retrieval. Given a query image, we retrieve its nearest neighbor from the training data using NetVLAD \cite{arandjelovic2016netvlad} and use the associated pose as the initial pose. For all three APR methods, we update their parameters, while for image retrieval, we directly optimize the initial pose as the retrieval process is not differentiable. The results are summarized in \cref{table:3} and \cref{table:5}, respectively. As demonstrated, our method can refine the initial poses of all three APR methods and image retrieval under all scenes across both datasets.

% For the second mode, we set the learning rate to be $lr_R$ and $lr_t$ for rotation and translation components, respectively. 
% Specifically, for indoor scenes, we set $lr_R=0.0087$ (corresponds to $0.5\degree$ in radiance) and $lr_t=0.01$. For outdoor scenes, we set $lr_R=0.01$ and $lr_t=0.1$. During both modes, the NeFeS model renders features with a smaller resolution of shorter side of 60 pixels and then upsamples it using bicubic interpolation to a shorter side of 240 for feature matching.

% We also curious about the performance differences between directly optimizing APR and directly optimizing pose. Hence we conducted a controlled experiment over...

% We conducted a controlled experiment to understand if there are performance differences between our method's two modes: optimizing the APR method's parameters or directly optimizing the pose itself. We evaluate both modes on DFNet with NFS$_{50}$ refinement.

% Our method can refine the initial pose in two modes: optimizing the parameters of APR method or directly optimizing the pose itself. We evaluate both modes on DFNet. As shown in \cref{table:4}, the initial pose can be effectively refined in both modes, demonstrating the versatility of our method. In addition, we also observe that the performance of optimizing APR is better than optimizing pose itself, which justifies our choice to optimizing APR. Nevertheless, optimizing pose is still a useful capability, especially when the initial pose is obtained from a non-differentiable or a black-box pose estimator. 

% We conducted a controlled experiment to understand if there are performance differences between our method's two modes: optimizing the APR method's parameters or directly optimizing the pose itself. We evaluate both modes on DFNet with NFS$_{50}$ refinement (see \cref{table:4}). Though both refine modes can effectively improve pose accuracy, optimizing the neural net's parameters converges faster than directly optimizing the pose itself. Nevertheless, optimizing pose is still useful, especially when the initial pose is obtained from a non-differentiable or a black-box pose estimator. 

We further conducted a controlled experiment to investigate whether performance disparities exist between two types of optimization: optimizing the APR's parameters or directly optimizing the pose itself. We evaluated both modes on the DFNet with NeFeS$_{50}$ refinement, as illustrated in \cref{table:4}. The result suggests that while both refinement approaches can effectively improve the pose accuracy, optimizing the neural network's parameters converges faster than directly optimizing the pose itself. Nevertheless, optimizing the pose remains is also valuable, particularly when the initial pose is derived from a non-differentiable or a black-box pose estimator.

\input{table_tex/table4.tex}


\subsection{NeFeS Ablation} \label{sec: nfs_ablation}
% We provide the ablation study of our method in this section. In \cref{table:6}a, we remove the exposure-adaptive ACT and Feature Fusion layer one by one and evaluate each case on the scene of Cambridge Shop Facade. The result deteriorates as we gradually remove each component, which indicates the effectiveness of the two components. In addition, we also compare our proposed NFS with feature maps extracted by a CNN from NeRF rendered RGB images. Our NFS outperforms NeRF+CNN setting in both translational and rotational error, which shows the benefit of direct rendering of feature maps.

This section presents the ablation study of our NeFeS network. In \cref{table:6}a, we gradually remove the exposure-adaptive ACT and the Feature Fusion module and evaluate their impact on the performance of our approach on \textit{Cambridge Shop Facade}. The results demonstrate that the removal of each component leads to a deterioration in pose accuracy, indicating the effectiveness of both components. Furthermore, we compare our proposed NeFeS approach with feature maps extracted by a CNN from NeRF-rendered RGB images. Our NeFeS method outperforms the NeRF+CNN setting in terms of both translational and rotational error, highlighting the effectiveness of our model.

% In \cref{table:6}b, we compare our progressive training scheduling with the combined scheduling where all three components of the loss are switched on simultaneously. The results show the progressive training scheduling is better than the combined scheduling and justify our design. 

In \cref{table:6}b, we compare our progressive training scheduling with the combined scheduling, where all three loss terms have been enabled simultaneously since the beginning of the training. The results reveal that the progressive training scheduling results in better accuracy, providing further support for our design decisions.


\input{table_tex/table6}
% \input{table_tex/table7}
% 2. Training Scheduling (1. Color + Feature vs. 1. color, 2. feature, 3. Fusion)\\

\subsection{Pose Refinement Bounds}
\input{fig_tex/figure6_disturb_test}
% In order to investigate the capability of our method, we randomly perturb the ground truth pose and see the maximum error that the refinement can recover. We investigate refinement's performance bounds on recovering rotational and translational error separately by only perturbing either the rotation or translation of the ground truth pose. The magnitude of the perturbation is manually set and gradually increased while the direction of the perturbation is random. We select two scenes, \textit{7-Scenes Heads} and \textit{Cambridge Shop Facade}, to represent indoor and outdoor environments respectively. The results are provided in \cref{fig:perturb_test}. Our test reveals that our method is able to recover the rotational error up to $30^{\circ}$, before the model fails to converge. For translational error, our method can reduce the error up to $0.2m$ on Head scene and up to $2m$ on Shop Facade scene. The discrepancy arises from the fact that Head scene has a relatively smaller scale and the camera is close to the object, so a small movement would cause large appearance change in rendering. 

Our proposed refinement method relies on matching rendered features with query image features during test time,
so it may fail when there is not sufficient overlap between the two feature maps. 
To determine the bounds of our refinement method, we randomly perturb the ground-truth pose and determine the maximum perturbation at which our method stops converging. In our experiments, we jitter the orientation or position of the ground truth pose components separately while gradually increasing the magnitude of the perturbation. We use two scenes, an indoor scene (\textit{7-Scenes: Heads}) and an outdoor scene (\textit{Cambridge: Shop Facade}), to illustrate our results in \cref{fig:perturb_test}. We observe that our method cannot refine pose errors larger than $35\degree$. In case of translational errors, our method can refine errors upto $0.6m$ on \textit{Heads} (indoor scene) and upto $4m$ in \textit{Shop Facade} (outdoor scene). This difference may be caused because of the change in scale between indoor and outdoor settings. For example, in the small-scale scene of \textit{Heads}, the camera is closer to the objects, hence even small movements lead to a large change in the rendering.

%\cs{TODO: This tests the effective range of our methods, and the translation and angular range is wrong. For example:, heads doesn't even have a scale of 2 meters... Shop only works before 4 meters}~


\subsection{Number of Iterations vs. Accuracy Trade-off}
\input{fig_tex/figure5_iter_test}
In \cref{fig:iter_test}, we plot the relationship between the number of optimization iterations and pose error. Both translation and rotation errors drop by more than $40$\% in only $10$ iterations. The errors start to plateau around $50$ steps. Although we can achieve even lower errors with more iterations, we think this strikes a balance between accuracy and efficiency, and explains how we set our previous experiments.


% We plot the rotational error and translational error against the number of iterations of optimization in \cref{fig:iter_test}. Both errors drop rapidly with the increase in the number of iterations. The errors plateau out after 50 steps. Therefore, we set the number of refinement step to 50 to achieve the best balance between accuracy and speed. \cs{more insights here}~ \YB{takes 200ms per iteration}

% \subsection{NFF vs. NeRF RGB+CNN vs. RGB}
% \cs{Still benchmarking new arch.}~
% \input{table_tex/table8}
% \cref{table:8}

