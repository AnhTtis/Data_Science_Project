%CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[titletoc]{appendix}
\renewcommand{\appendixname}{Supplementary Material}
\usepackage[accsupp]{axessibility}

\DeclareMathOperator*{\argmin}{arg\,min}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

\newcommand{\xl}[1]{\textcolor[rgb]{0,0,1}{[\textbf{Arthur: #1}]}}
\newcommand{\Yu}[1]{\textcolor[rgb]{0.5,0,0.5}{[\textbf{Yu: #1}]}}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{5212} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Revisiting Multimodal Representation in Contrastive Learning: From Patch and Token Embeddings to Finite Discrete Tokens}

\author{
    Yuxiao Chen\textsuperscript{\rm 1\thanks{This work was done during a research internship at ByteDance.}},
    Jianbo Yuan\textsuperscript{\rm 2},
    Yu Tian\textsuperscript{\rm 2},
    Shijie Geng\textsuperscript{\rm 1,2},
    Xinyu Li\textsuperscript{\rm 2}, \\
    Ding Zhou\textsuperscript{\rm 2},
    Dimitris N. Metaxas\textsuperscript{\rm 1 \thanks{Dimitris N. Metaxas has been supported by NSF IUCRC CARTA-1747778, 2235405, 2212301, 1951890, 2003874.}},
    Hongxia Yang\textsuperscript{\rm 3} \\
    \textsuperscript{\rm 1}Rutgers University \quad
    \textsuperscript{\rm 2}ByteDance Inc. \quad
    \textsuperscript{\rm 3}Zhejiang University \\
    \texttt{\{yc984, sg1309, dnm\}@rutgers.edu},\\ \texttt{\{jianbo.yuan, yutian.yt, lixinyu.arthur, ding.zhou\}@bytedance.com} \\
    \texttt{hongxia.yang1@gmail.com} 
}
\maketitle
%%%%%%%%% ABSTRACT
\begin{abstract}
% Contrastive learning-based vision-language pre-training approaches, such as CLIP, have demonstrated great success in many vision-language tasks. These approaches achieve cross-modal alignment by encoding a matched image-text pair with similar feature embeddings that aggregate information from visual patches and language tokens. However, since visual patches and text tokens contain different levels of semantics and granularities, aligning the inputs directly using such embeddings is not trivial. To alleviate this issue, we present a novel multimodal representation that first grounds multimodal inputs to a fixed set of shared finite discrete tokens (FDT) based on the input contents. Each token represents certain high-level semantic concepts ranging from objects to actions and attributes. The embeddings of the activated tokens then form the FDT-based representations used for cross-modal alignment at a semantic level. 
% With sparse activation, we show that FDT are encouraged to denote matched visual and textual semantic concepts. 
% In addition, sharing FDT across all modalities provides a light-weighted way of cross-modal interactions for better cross-modal alignment. 
% Unlike previous discrete representation learning methods, FDT can be trained under the contrastive learning scheme from scratch without cold-start problems.
% Using both quantitative and qualitative analyses, we further demonstrate that simply replacing the image and text embeddings in CLIP-style models by FDT representations achieves better cross-modal alignment and superior performance in visual recognition and vision-language downstream tasks compared to previous approaches.
\vspace{-12pt}
Contrastive learning-based vision-language pre-training approaches, such as CLIP, have demonstrated great success in many vision-language tasks. These methods achieve cross-modal alignment by encoding a matched image-text pair with similar feature embeddings, which are generated by aggregating information from visual patches and language tokens. However, direct aligning cross-modal information using such representations is challenging, as visual patches and text tokens differ in semantic levels and granularities. To alleviate this issue, we propose a Finite Discrete Tokens (FDT) based multimodal representation. FDT is a set of learnable tokens representing certain visual-semantic concepts. Both images and texts are embedded using shared FDT by first grounding multimodal inputs to FDT space and then aggregating the activated FDT representations. The matched visual and semantic concepts are enforced to be represented by the same set of discrete tokens by a sparse activation constraint. As a result, the granularity gap between the two modalities is reduced. Through both quantitative and qualitative analyses, we demonstrate that using FDT representations in CLIP-style models improves cross-modal alignment and performance in visual recognition and vision-language downstream tasks.  Furthermore, we show that our method can learn more comprehensive representations, and the learned FDT capture meaningful cross-modal correspondence, ranging from objects to actions and attributes.\footnote{The source code can be found
at \textcolor{magenta}{\url{https://github.com/yuxiaochen1103/FDT}}.} 

%In addition, sharing FDT across all modalities provides a light-weighted way of cross-modal interactions for better cross-modal alignment


%, each of which is encouraged to selectively denote a matched visual-semantic concept including not only objects, but actions and attributes as well. %by sparse activation? 
%without introducing any fusion operations. In addition, FDT serve as multimodal anchors to capture information from each input with better completeness, which alleviates the encoder degradation problem that is commonly observed in vanilla CLIP models. 

% {\color{red}DNM: Can you say how much better? ie a percent? }%including image classification, cross-modal retrieval, etc.


%Cross-modal alignment is achieved by encoding a matched image-text pair to be close to each other through image and text encoders. In this paper, we present a new multimodal presentation grounded to a fixed set of shared, finite learnable tokens (FLT), which can be viewed as fine-grained abstractions of a unified visual-semantic joint space. Each input image or text is represented by a combination of the token representations based on the attention weights between the input and FLT. Our approach provides a light-weighted way of cross-modal interactions for better cross-modal alignment without introducing any fusion operations. In addition, FLT serve as multimodal anchors to capture information from each input with better completeness, which alleviates the encoder degradation problem that is commonly observed in vanilla CLIP models. We conduct extensive experiments to show that %, unlike other methods such as vector-quantization (VQ) and codebook learning, 
%the FLT can be trained with the same contrastive learning scheme from scratch without cold-start problems. With both quantitative and qualitative analysis, we further show that simply replacing the image and text embeddings in CLIP-style models by FLT-based representations achieves better cross-modal alignment and better performance in downstream tasks including image classification, cross-modal retrieval, and xxx task.

% TODO: emphasize the logic flow diff
\end{abstract}

%%%%%%%%% BODY TEXT
\vspace{-12pt}
\section{Introduction}
\label{sec:intro}
\input{sections/introduction_v2.tex}

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=0.9\linewidth]{figures/method_v4.pdf}
\end{center}
\caption{\textbf{Left:} Overview of the proposed method. Both the image and text information is encoded with shared FDT during cross-modal contrastive pre-training. \textbf{Right:} The process of grounding image or text features to FDT. The attention weights between visual patch/language token and FDT are first calculated, and then max-pooled over all visual patches/language tokens. The attention-weighted sum of FDT is calculated as the FDT-based features. }
\vspace{-10pt}
\label{fig:method}
\end{figure*}

\section{Related Work}
\label{sec:rw}
\input{sections/related_work.tex}


\section{Method}
\label{sec:method}
\input{sections/method.tex}

\section{Experiments}
\label{sec:exp}
\input{sections/experiment_v2.tex}

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage
\appendix
\input{sections/supp_final.tex}

\end{document}
