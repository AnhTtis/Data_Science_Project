\begin{figure}[t]
\begin{center}
    \includegraphics[width=\linewidth]{figures/logic_flow_v3.pdf}
\end{center}
\vspace{-10pt}
\caption{Comparison of different feature representation learning methods. \textbf{Left}: contrastive vision-language pre-training (CLIP). \textbf{Right}: CLIP with our proposed finite discrete tokens (FDT).}
\vspace{-10pt}
\label{fig:logic_flow}
\end{figure}

\vspace{-5pt}
Recently, the Contrastive Language-Image Pre-training (CLIP) framework \cite{clip, align} has demonstrated notable capabilities for learning powerful and transferable feature representations~\cite{zhang2022learning,zhang2023prompt,zhang2023parameter,zhang2022sine,gao2021clip,lin2022frozen}.~In this framework, models are trained to align text and image information in a two-stream approach where image and text representations are extracted through two separate encoders. The InfoNCE loss \cite{clip} is used to train the encoders which enforces the representations of matched image-text pairs to be closer, while those of unmatched pairs to be far apart (as shown in Figure \ref{fig:logic_flow} (Left)).


%The image encoder extracts information from local vision patches (vision transformers) or local windows (convolutional neural networks), and the text encoder aggregates information from each language token.

% However, directly aligning over the feature embeddings aggregated from patches and language tokens is challenging for two reasons. \textcolor{red}{First, the information conveyed by image patches and language tokens may have different semantics and granularities}. \Yu{Citations?} The image and feature embeddings are essentially represented by two different bases: patch and language token embeddings. The bases are different for different images and texts, since different images or texts have different patches or language tokens. It increases the difficulty of aligning cross-modal information. Secondly, the contribution of a patch or language token to the final representation is determined by intra-modal information only, and the information of a matched image-text pair is not always fully matched in the noisy pre-training datasets.   As a result, the learned encoders may fail to capture all the cross-modal shared semantic information \cite{learning_concepts}.

% However, CLIP does not consider the problem that the information in images and texts has naturally different granularities \cite{Gra_unif, image_infinity} (e.g., dogs with different visual appearances due to color and/or texture, can share the same description ``dog''). This is because the image and text encoders aggregate information from local visual patches and language tokens, respectively, which have different granularities. Recent work shows that directly aligning image and text features without considering the cross-modal information inconsistency is challenging. It may result in degraded representations \cite{tcl}, and the learned models may ignore important semantic concepts \cite{learning_concepts}. We argue that unifying the information granularities can help CLIP learn better representations.


However, the fact that the information conveyed in images and text captions is naturally of different levels of granularities \cite{Gra_unif, image_infinity} is not considered by such models. For example, an image of a dog also portrays various lower-level attributes, such as its breed, fur color, body size, and shape, while the textual description, such as ``a smiling dog'', is generally more abstract and compact. In CLIP, images and text captions are represented through the aggregation of visual patches and text tokens without explicitly aligning the visual and semantic concepts at the same level of granularity. It can cause challenges in multimodal representation learning, or even potentially result in performance degradation \cite{tcl}. Additionally, the learned models may overlook certain semantic concepts\cite{learning_concepts}.
Therefore, we argue that unifying the information granularities of images and texts can help generate better multimodal representations.


% Recent work \cite{image_infinity} demonstrates (DNM: proves means a theorem etc) that unifying the information granularities of images and texts can help the model learn better cross-modal alignment for text-to-image person re-identification (ReID). We hypothesize that unifying the information granularities that can also help CLIP.

% CLIP does not consider the problem. It directly aligns the feature embeddings aggregated from patches and language tokens, which also are inconsistent with granularities







% Previous work show that this inconsistency of granularities increases the difficulty of aligning image text information, and explicitly unifying the granularities can improve the performance.



% However, directly aligning over the feature embeddings aggregated from patches and language tokens ignore the fact that patches and language token information have different granularities. Specifically, image information is more fine-grained than text information \cite{Gra_unif, image_infinity}. For example, text images contain patterns, and texture information, while text captions coarsely describe the image as a dog. Previous work show that this inconsistency of granularities increases the difficulty of aligning image text information, and explicitly unifying the granularities can improve the performance.      

% They typically solve the problem by applying attention-based cross-modal fusion models \cite{soho,uniter,li2022unimo}. The models take both image patch and language token embeddings as input, and then extract relationships among patches and tokens by the self-attention mechanism. However, the models have tremendous computation costs, especially for retrieval tasks, which limits the application in the real world. \Yu{Citations?}
% to do more related works.

%consisting of a fixed set of 

%Motivated by the fact that image and text information is naturally discrete \cite{vqvae, peng2022beit2, wang2022beit3},

In this paper, we propose a new \textbf{F}inite \textbf{D}iscrete \textbf{T}okens (FDT) based representations.~FDT is a set of \emph{learnable tokens} that encode cross-modal shared semantic concepts. Both image and text are represented as the combinations of FDT shared between modalities so that the information granularities are unified (see Figure \ref{fig:logic_flow} (Right)). Figure \ref{fig:method} gives an overview of our method. For an image, its patch embeddings are first extracted by an image encoder. The correspondence between the FDT and the image is then measured by max pooling over the attention weights of FDT among all patches. Finally, the FDT-based representation of the image is calculated as the attention-weighted sum of FDT. The FDT-based embeddings for input texts can be constructed in the same way. %As a result, both the images and texts are represented by a shared set of FDT. 
The encoders and FDT are trained to pull close the FDT-based representations of matched image-text pairs while pushing away those of unmatched pairs by using the InfoNCE loss. %To further reduce the granularity gap between the two modalities, 
To the point of leveraging a shared FDT across modalities is to enforce the matched visual and semantic concepts to be represented by the same discrete tokens. For example, the visual patches of a dog and the word ``dog'' should activate the same subsets of FDT. We empirically demonstrate that this can be achieved by simply enforcing relatively sparse attention-weights between FDT and the inputs.

% \textcolor{red}{To do: explain why the methods can learn more comprehensive representations.DNM: This is very important since now you just say trust me I can do it.}


%are more easily aligned across modalities. To achieve this, %unlike previous work that applies a matching loss to enforce the activation on similar tokens \cite{liu2021cross}, or requires embeddings from both modalities to calculate the token activation \cite{li2022unimo}, 
% we show that simply enforcing a relatively sparse token activation encourages FDT to encode matched visual-semantic concepts without introducing any extra loss functions or cross-modal computations



% Our approach shares similar intuitions with the work that applies codebooks \cite{liu2021cross, li2022unimo} and dictionary to represent the information  to help cross-model aligning. However, the training of our FDT is fully differentiable and can be achieved with random initialization using conventional CLIP without suffering from the cold-start problems that is often observed in these VQ methods \cite{soho,li2022unimo,liu2021cross} (more details in Section \ref{sec:method:vq}). 


 % The FDT serves as the prior knowledge to guide encoders to extract cross-modal shared high-level semantic concepts, which not only facilitates cross-modal interactions but also helps encoders capture semantic information more comprehensively.

% The feature extraction process of the images and texts is independent, maintaining the efficient two-stream style.





% FDT bridge the cross-modal interactions by vision-to-token and language-to-token information exchange, while maintaining a two-stream structure.




% which is used in the contrastive vision-language pre-training.
% Because only FDT-based representations are used in the contrastive learning pre-training rather than the low-level continuous embeddings, the FDT are encouraged to denote high-level semantics instead of local information, upon which the cross-modal alignment is conducted directly (Figure \ref{fig:logic_flow} (c)). Intuitively, matched visual and semantic concepts should activate the same subset of tokens so that their FDT-based representations are more easily aligned across modalities. To achieve this, %unlike previous work that applies a matching loss to enforce the activation on similar tokens \cite{liu2021cross}, or requires embeddings from both modalities to calculate the token activation \cite{li2022unimo}, 
% we show that simply enforcing a relatively sparse token activation encourages FDT to encode matched visual-semantic concepts without introducing any extra loss functions or cross-modal computations. %that would break a two-stream model structure. 
% %As a result, each token is encouraged to encode matched visual-semantic concepts.%, and as a result, achieves better cross-modal alignment and improves the model's interpretability when used as multimodal representations.
% Our approach shares similar intuitions with VQ \cite{vqvae}, however, the training of FDT are fully differentiable and can be achieved with random initialization using conventional CLIP without suffering from the cold-start problems that is often observed in VQ methods \cite{soho,li2022unimo,liu2021cross} (more details in Section \ref{sec:method:vq}). 

% place holder

We conduct extensive experiments covering a wide range of pre-training settings and downstream tasks to evaluate the proposed method.~We conclude with the following key observations:~%(1) Our method significantly improves %the performances in various downstream tasks on the 15M benchmark over CLIP-styled models\cite{declip_benchmark}; 
(1) Our approach exhibits consistent performance enhancements across various pre-training dataset scales, CLIP-based pre-training frameworks \cite{declip}, and encoder architectures. %Our method can consistently achieve performance improvements when applied at different scales of pre-training datasets, different CLIP-based pre-training frameworks \cite{declip}, and different encoder architectures. 
Notably, our method outperforms CLIP by 5.0\% on zero-shot image classification when pre-training on 145M datasets, and by 33.4\% in image-text retrieval with 30M datasets; (2) Our method tends to alleviate the model degradation problem and learns more comprehensive feature representations than CLIP; (3) The learned FDT exhibit better: we visualize FDT's correspondent patches and language tokens, and the results show that FDT successfully capture and align visual-semantic concepts including objects, attributes, and actions. 


% Compared with non-sparse activation, our approach generates more meaningful FDT with higher quality in terms of representing the matched visual-semantic concepts. We argue that such content understanding ability is critical for multimodal reasoning tasks.Sparse constraints of attention weights is crucial for learning better cross-modal alignments. 

% the experiments show that our approach can  improve CLIP performance on the five downstream tasks Specifically when pre-training datasets. 

% We evaluate the proposed FDT-based representations by incorporating them into CLIP. The pre-trained models are tested on  The experiments show that our approach can consistently improve CLIP performance on the five downstream tasks Specifically, when pre-training datasets, including 15M, 30M, and 145M. Notably, Our methods outperform CLIP by 4.8 \% on zero-shot image classification and by 33.4 \% for image-text retrieval. We also evaluated the proposed method into improved CLIP-based pre-training frameworks, including DeCLIP \cite{declip} and FILIP \cite{filip}. The improvements demonstrate our proposed method is lug-in training strategies for various CLIP-based frameworks. We visualize FDT and their corresponding vision and language activation to show that FDT successfully captures high-level semantic concepts including not only objects, but attributes and actions as well. Compared with non-sparse activation, our approach generates more meaningful FDT with higher quality in terms of representing the matched visual-semantic concepts. We argue that such content understanding ability is critical for multimodal reasoning tasks



% We conduct extensive quantitative and qualitative analysis on vision and multimodal downstream tasks to show that, using FDT as multimodal representations to replace the conventional continuous embedding from individual encoders achieves: (1) Better cross-modal alignment with cross-modal interaction. Because cross-modal alignment is performed directly on FDT shared by all modalities, FDT-based representations bridge the gap between different modalities with cross-modal interactions by vision-to-token and language-to-token information exchange, while maintaining a two-stream structure. Replacing the image and text embeddings in CLIP by FDT representations yields better performance in vision and vision-language downstream tasks; (2) \textbf{TODO};  
%$+~4\%$ average accuracy in zero-shot image classification and $15+\%$ rsum in cross-modal retrieval; %(2) Less prune to encoder degradation. Because each token is activated as long as the matched visual or semantic content is present, and thus can serves as an anchor to help preserve the information with better completeness. We conduct both case studies and quantitative analysis to show that FDT-based representation capture the input content with better completeness; %which is supported by in general the higher similarity scores between input images and full descriptions over partial descriptions that is considered as positive; 
% and (3) Better interpretability. We visualize FDT and their corresponding vision and language activation to show that FDT successfully capture high-level semantic concepts including not only objects, but attributes and actions as well. Compared with non-sparse activation, our approach generates more meaningful FDT with higher quality in terms of representing the matched visual-semantic concepts. We argue that such content understanding ability is critical for multimodal reasoning tasks.