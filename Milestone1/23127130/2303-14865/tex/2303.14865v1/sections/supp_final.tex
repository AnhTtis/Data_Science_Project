
%%%%%%%%% TITLE - PLEASE UPDATE
%%%%%%%%% BODY TEXT
\section{Pre-training Implementation Details}
We implement the projecting function that maps patch or language token features to the FDT space as a fully-connected layer with GELU activation (see Section 3.2). Two different projecting functions are applied for mapping patch and language token features, respectively. We regularize the FDT using weight decay, with a rate of 0.1. We set the batch sizes as 4096, 8192, and 32768 when pretraining the models under the 15M, 30M, and 145M settings, respectively. To ensure a fair comparison with the DECLIP~\cite{declip} and FILIP~\cite{filip} models, we use the same data augmentation as these models when training the CLIP and CLIP+FDT models. Consequently, our reported results of the CLIP model on the 15M setting are better than those reported in the 15M benchmark~\cite{declip_benchmark}. We train ViT-B/32 based \cite{vit} models considering our limited computation resource.  The input image resolution is 224 $\times$ 224, and the maximal input language token number is 77. Following \cite{declip_benchmark}, we apply the AdamW optimizer \cite{loshchilov2018decoupled} with a weight decay rate of 0.1 during pre-training. The learning rate is first linearly increased to 0.001 with one epoch for warmup, and then decayed to 0 following the cosine strategy \cite{cosinedecay}.~We use NVIDIA A100 GPUs for pre-training.

\section{Downstream Implementation Details}
\subsection{Downstream Datasets}
\noindent{\textbf{Image Classification Tasks.~}}Following \cite{declip}, we evaluate our method on 11 datasets, including CIFAR-10~\cite{cifar10}, CIFAR-100~\cite{cifar10}, SUN397~\cite{sun397}, Stanford Cars~\cite{cars}, FGVC Aircraft~\cite{aircraft},
Describable Textures~\cite{dtd}, Oxford-IIIT Pets~\cite{pets}, Caltech-101~\cite{caltech}, Oxford Flowers 102~\cite{flower}, Food-101~\cite{food101}, and ImageNet-1K~\cite{imagenet}. 

\noindent{\textbf{Image-Text Retrieval.~}}Our method is tested on two standard benchmarks: Flickr30K~\cite{f30k} and MSCOCO~\cite{mscoco}. For MSCOCO, we report the results on the 5K setting.

\noindent{\textbf{Non-Linear Probe task.~}}We conduct the experiments on the VQAv2 dataset \cite{vqa}. Following the standard protocol \cite{meter}, we train the models with both training and validation data, and test the models on the test-dev set.

\subsection{Implementation Details}
\noindent{\textbf{Zero-shot Image Classification.~}}For a fair comparison, we use the domain-specific prompts and category names proposed by CLIP~\cite{clip}. Note that we do not report the results on the StanfordCars and Aircraft datasets, because the pertaining datasets contain few captions about the category names of these datasets. For example, only 0.04\% and 0\% of descriptions contain aircraft and car category names on the 15M setting.

\noindent{\textbf{Linear Probe Image Classification.~}}We train a logistic regression classifier using L-BFGS, following CLIP \cite{clip}. We set the maximum iterations number to 1,000, and determine the L2 regularization weights following DECLIP's hyperparameter sweeping strategy \cite{declip}. We do not report the results on the ImageNet-1K dataset, due to the high computational cost of conducting hyperparameter sweeping on the dataset.

\noindent{\textbf{Non-linear Probe Task.~}}The downstream task head consists of a fully-connected layer with GELU activation and a fully-connected layer. The extracted FDT features of images and questions are concatenated and then fed to the downstream task head to predict the answers. The encoders and FDT are frozen during the training. The downstream head is optimized by the AdamW optimizer \cite{loshchilov2018decoupled}. We set the learning rate as 0.005, and decay it to 0 following the cosine strategy \cite{cosinedecay}.


\section{Completeness Probing Experiment Details}
Given an image that contains $N$ objects, its \textit{matched sentence}
is ``An photo contains $o_1$, $o_2$ ..., $o_{N-1}$, and $o_N$'', where $o_i$ is the name of the $i$-th object in the images and all the objects are included. For the \textit{partially matched sentence}, we randomly remove an object and use the remaining $N-1$ objects to construct a caption. For example, if the $N$-th object is removed, the partially matched sentence is ``An photo contains $o_1$, $o_2$ ..., and $o_{N-1}$''. We can construct $N$ partially matched sentences for the image, resulting in $N$ \textit{sentence pairs} for the image. In our experiments, we obtain the object presence information of images based on the object detection annotations of the MSCOCO \cite{mscoco} dataset. We construct 305,723 sentence pairs using all images in the MSCOCO training split.

\section{FDT Visualization Details}

We use the model pre-trained on the 145M setting for visualization because it achieves the best performance. To visualize an FDT token, we first calculate its relevance score between patches/language tokens following Equations 4 and 6 without using max-pooing. We then display the relevance scores between the FDT token and the images corresponding to the top-5 most relevant patches, since we find that the patches alone cannot fully convey the object information. We increase the resolution by reducing the patch stride to 4, following the method proposed in \cite{amir2021deep}. For text modality, we show the top-5 most relevant language tokens of the FDT token. 


\section{Additional Experiment Results}

\subsection{Text-to-Image Retrieval Cases}

We further provide five cases for the text-to-image retrieval task in Figure \ref{fig:coco_case_supp}. We have the same observation that the images retrieved by the CLIP+FDT well match the text queries, while those retrieved by the CLIP models often overlook important concepts mentioned in the text queries.

\begin{figure}[t!]
\begin{center}
    \includegraphics[width=1\linewidth]{figures/t2i_supp.pdf}
\end{center}
\vspace{-5pt}
\caption{Examples show the top-5 retrieved images for the given text queries in the text-to-image retrieval task on MSCOCO.}
\label{fig:coco_case_supp}
\end{figure}
\vspace{-5pt}


\subsection{Visualization of Learned FDT}

We present eight learned FDT in Figure \ref{fig:fdt}. These cases further show that FDT can learn meaningful cross-modal correspondence. 

\begin{figure*}[t]
\begin{center}
    \includegraphics[width=0.8\linewidth]{figures/supp_code_v2.pdf}
\end{center}
\vspace{-5pt}
\caption{The top-5 most relevant image patches and text tokens of eight FDT tokens. Note that the redundant text tokens in the top-5 are removed. The color of the heatmap from blue to red denotes the relevance between patches and FDT from small to large.}
\label{fig:fdt}
\end{figure*}
\vspace{-5pt}



\subsection{Pretraining Data Scale}

The results of the models pretrained with different scales of training data are reported in Table \ref{tbl:scale_zs_cls}, \ref{tbl:scale_lp_cls}, \ref{tbl:scale_zs_itr}, and \ref{tbl:scale_vqa}.


\begin{table*}[t!]

\centering
\resizebox{0.6\linewidth}{!}{
\begin{tabular}{@{}ccccccccccc@{}}
\toprule
\multicolumn{1}{l}{} &
  \rotatebox{0}{C10} &
  \rotatebox{0}{C100} &
  \rotatebox{0}{F101} &
  \rotatebox{0}{PETS} &
  \rotatebox{0}{FLOWE} &
  \rotatebox{0}{SUN} &
  \rotatebox{0}{DTD} &
  \rotatebox{0}{CAL} &
  \rotatebox{0}{IN} &
  \rotatebox{0}{AVG} \\ \midrule
\multicolumn{1}{l}{15M}  &      &      &      &      &      &      &      &      &                      &                       \\ \midrule
CLIP                     & 60.4 & 33.5 & 39.6 & 23.1 & 54.0 & 42.0 & 17.0 & 65.5 & 37.0                 & 41.3                  \\
CLIP+FDT                & 67.7 & 39.9 & 42.9 & 25.8 & 55.5 & 45.5 & 26.5 & 69.6 & 39.3                 & 45.9 ($\uparrow$ 4.6) \\ \midrule
\multicolumn{1}{l}{30M}  &      &      &      &      &      &      &      &      & \multicolumn{1}{l}{} & \multicolumn{1}{l}{}  \\ \midrule
CLIP                     & 77.2 & 48.1 & 59.1 & 58.4 & 58.2 & 52.6 & 28.0 & 80.8 & 48.8                 & 56.8                  \\
CLIP+FDT                & 81.9 & 56.5 & 62.6 & 62.3 & 59.5 & 56.7 & 33.6 & 84.8 & 53.3                 & 61.2($\uparrow$ 4.4)  \\ \midrule
\multicolumn{1}{l}{145M} &      &      &      &      &      &      &      &      & \multicolumn{1}{l}{} & \multicolumn{1}{l}{}  \\ \midrule
CLIP                     & 80.9 & 53.9 & 69.1 & 68.9 & 59.3 & 52.1 & 43.0 & 90.1 & 59.0                 & 64.0                  \\
CLIP+FDT                & 87.1 & 63.7 & 73.5 & 77.0 & 65.0 & 56.2 & 47.7 & 90.5 & 60.4                 & 69.0($\uparrow$ 5.0)    \\ \bottomrule
\end{tabular}}
\vspace{-5pt}
\caption{Zero-shot image classification accuracy (\%) when using different scales of training data. The dataset names are abbreviated. C10/100 is CIFAR10/100. F101 is Food101. FLOW is Flowers. CAL is Caltech. IN is ImageNet-1K. ``AVG'' is the average accuracy over all datasets.}
\label{tbl:scale_zs_cls}
\end{table*}


\begin{table*}[t!]
\centering
\resizebox{0.65\linewidth}{!}{
\begin{tabular}{@{}cccccccccccc@{}}
\toprule
\multicolumn{1}{l}{} &
  \rotatebox{0}{C10} &
  \rotatebox{0}{C100} &
  \rotatebox{0}{F101} &
  \rotatebox{0}{PETS} &
  \rotatebox{0}{FLOW} &
  \rotatebox{0}{SUN} &
  \multicolumn{1}{l}{\rotatebox{0}{CARS}} &
  \rotatebox{0}{DTD} &
  \rotatebox{0}{CAL} &
  \rotatebox{0}{AIR} &
  \rotatebox{0}{AVG} \\ \midrule
\multicolumn{1}{l}{15M}  &      &      &      &      &      &      & \multicolumn{1}{l}{} &      &      &                          &                       \\ \midrule
CLIP                     & 88.3 & 68.6 & 72.1 & 72.5 & 92.6 & 69.5 & 29.8                 & 67.8 & 86.2 & 27.7                     & 67.5                  \\
CLIP+FDT                & 89.1 & 71.2 & 74.4 & 73.0   & 93.4 & 70.8 & 31.4                 & 69.4 & 87.7 & 27.9                     & 68.8 ($\uparrow$ 1.3) \\ \midrule
\multicolumn{1}{l}{30M}  &      &      &      &      &      &      & \multicolumn{1}{l}{} &      &      & \multicolumn{1}{l}{}     & \multicolumn{1}{l}{}  \\ \midrule
CLIP                     & 92.0   & 74.7 & 78.8 & 80.7 & 93.7 & 72.6 & 55.9                 & 71.4 & 88.6 & 29.7                     & 73.8                  \\
CLIP+FDT                & 93.8 & 77.8 & 81.6 & 82.6 & 94.5 & 74.3 & 54.4                 & 73.9 & 92.3 & 30.9                     & 75.6 ($\uparrow$ 1.8) \\ \midrule
\multicolumn{1}{l}{145M} &      &      &      &      &      &      & \multicolumn{1}{l}{} &      &      & \multicolumn{1}{l}{}     & \multicolumn{1}{l}{}  \\ \midrule
CLIP                & 95.2 & 80.6 & 86.1 & 87.5 & 96.5 & 76.3 & 87.6                 & 77.2 & 94.7 & 39.5  & 82.1                      \\
CLIP+FDT                     & 94.8 & 80.8 & 85.5 & 85.8 & 95.7 & 75.9 & 88.1                 & 78.5 & 94.6 & 42.9                     & 82.3 ($\uparrow$ 0.2)                 \\\bottomrule
\end{tabular}}
\vspace{-5pt}
\caption{Linear probing image classification accuracy (\%) when using different scales of training data. The dataset names are abbreviated. C10/100 is CIFAR10/100. F101 is Food101. FLOW is Flowers. CAL is Caltech. Air is Aircraft. ``AVG'' is the average accuracy over all datasets.}
\label{tbl:scale_lp_cls}
\end{table*}



\begin{table*}[t!]
\centering
\resizebox{0.85\linewidth}{!}{
\begin{tabular}{@{}lcccccccccccccc@{}}
\toprule
             & \multicolumn{7}{c}{Flickr30K}                   & \multicolumn{7}{c}{MSCOCO}                       \\ \midrule
 &
  \multicolumn{3}{c}{Image Retrieval} &
  \multicolumn{3}{c}{Text Retrieval} &
   &
  \multicolumn{3}{c}{Image Retrieval} &
  \multicolumn{3}{c}{Text Retrieval} &
   \\
             & R@1  & R@5  & R@10 & R@1  & R@5  & R@10 & rsum  & R@1  & R@5  & R@10 & R@1  & R@5  & R@10  & rsum  \\ \midrule
15M setting  &      &      &      &      &      &      &       &      &      &      &      &      &       &       \\ \midrule
CLIP         & 27.6 & 53.9 & 64.4 & 42.8 & 71.5 & 82.9 & 343.1 & 15.9 & 36.7 & 47.8 & 24.8 & 49.8 & 61.8  & 236.8 \\
CLIP + FDT  & 32.6 & 58.6 & 68.5 & 51.0 & 78.3 & 87.5 & 376.5 ($\uparrow$ 33.4) & 19.4 & 40.8 & 51.9 & 29.6 & 55.3 & 66.1  & 263.1 ($\uparrow$ 26.3) \\ \midrule
30M setting  &      &      &      &      &      &      &       &      &      &      &      &      &       &       \\ \midrule
CLIP         & 43.6 & 72.8 & 81.3 & 58.8 & 84.2 & 90.6 & 431.3 & 23.3 & 46.9 & 58.6 & 34.8 & 63.3 & 73.9  & 300.8 \\
CLIP + FDT  & 52.5 & 78.7 & 86.4 & 70.8 & 90.8 & 95.0 & 474.2 ($\uparrow$ 42.9) & 28.3 & 53.3 & 64.3 & 43.0 & 69.0 & 79.2  & 337.1 ($\uparrow$ 36.3)\\ \midrule
145M setting &      &      &      &      &      &      &       &      &      &      &      &      &       &       \\ \midrule
CLIP         & 52.6 & 78.5 & 86.4 & 67.9 & 89.9 & 94.5 & 469.8 & 29.3 & 54.1 & 65.4 & 42.1 & 67.1 & 77.2 & 335.2 \\
CLIP + FDT &
  56.3 &
  80.7 &
  87.6 &
  75.9 &
  93.6 &
  95.3 &
  489.4 ($\uparrow$ 19.6) &
  31.0 &
  55.7 &
  66.7 &
  46.4 &
  71.9 &
  81.3 &
  353.0 ($\uparrow$ 17.8) \\ \bottomrule
\end{tabular}}
\vspace{-5pt}
\caption{Zero-shot image-text retrieval results on the Flickr30K and MSCOCO (5K) datasets when using different scales of training data.}
\label{tbl:scale_zs_itr}
\end{table*}

\begin{table*}[t]
\centering
\resizebox{0.32\linewidth}{!}{
\begin{tabular}{@{}lcccc@{}}
\toprule
             & y/n  & number & other & overall               \\ \midrule
15M setting  &      &        &       &                       \\ \midrule
CLIP         & 67.7 & 31.9   & 33.6  & 47.5                  \\
CLIP + FDT  & 67.8 & 34.6   & 39.6  & 50.6 ($\uparrow$ 3.1) \\ \midrule
30M setting  &      &        &       &                       \\ \midrule
CLIP         & 69.7 & 34.8   & 37.8  & 50.6                  \\
CLIP + FDT  & 68.8 & 36.4   & 42.0  & 53.4 ($\uparrow$ 2.8) \\ \midrule
145M setting &      &        &       &                       \\ \midrule
CLIP         & 70.9 & 36.5   & 41.7  & 53.1                  \\
CLIP + FDT  & 71.5 & 37.9   & 45.2  & 55.2 ($\uparrow$ 2.1) \\ \bottomrule
\end{tabular}}
\vspace{-5pt}
\caption{Results of non-linear probing on VQA v2 dataset when using different scales of training data.}
\label{tbl:scale_vqa}
\end{table*}



\subsection{Image Encoder Architecture}

To evaluate the influence of encoder architectures on our methods, we pre-trained the models with different image encoder architectures. The results for various downstream tasks are reported in in Table \ref{tbl:enc_zs_cls}, \ref{tbl:enc_lp_cls}, \ref{tbl:enc_zs_itr}, and \ref{tbl:enc_vqa}. We also report the computation costs when using different encoder architectures in Table \ref{tbl:cost}.


\begin{table*}[t!]

\centering
\resizebox{0.7\linewidth}{!}{
\begin{tabular}{@{}ccccccccccc@{}}
\toprule
\multicolumn{1}{l}{} &
  \rotatebox{0}{C10} &
  \rotatebox{0}{C100} &
  \rotatebox{0}{F101} &
  \rotatebox{0}{PETS} &
  \rotatebox{0}{FLOW} &
  \rotatebox{0}{SUN} &
  \rotatebox{0}{DTD} &
  \rotatebox{0}{CAL} &
  \rotatebox{0}{IN} &
  \rotatebox{0}{AVG} \\ \midrule


ViT-B/32                     & 60.4 & 33.5 & 39.6 & 23.1 & 54.0 & 42.0 & 17.0 & 65.5 & 37.0                 & 41.3                  \\
ViT-B/32+FDT                & 67.7 & 39.9 & 42.9 & 25.8 & 55.5 & 45.5 & 26.5 & 69.6 & 39.3                 & 45.9 ($\uparrow$ 4.6) \\ \midrule
ViT-B/16                     & 64.6 & 32.1 & 49.7 & 25.7 & 59.7 & 43.4 & 21.3 & 67.9 &  42.1& 45.2                 \\
ViT-B/16+FDT                & 74.0 & 42.1 & 49.4 & 28.5 & 62.2 & 50.5 & 25.1 & 71.4 &                   45.6& 49.9 ($\uparrow$ 4.7)  \\ \midrule

SwinV2-B                     &  58.3& 23.3& 39.3& 20.0&  55.2& 40.1& 18.9& 62.1&     38.9& 39.6                  \\
SwinV2-B+FDT               & 58.9& 26.0& 44.7& 23.8& 55.4& 43.3& 21.4& 66.2& 42.3&   42.4 ($\uparrow$ 2.8)    \\ \bottomrule

\end{tabular}}
\vspace{-5pt}
\caption{Zero-shot image classification accuracy (\%) when using different image encoder architectures. The dataset names are abbreviated. C10/100 is CIFAR10/100. F101 is Food101. FLOW is Flowers. CAL is Caltech. IN is ImageNet-1K. ``AVG'' is the average accuracy over all datasets.}
\label{tbl:enc_zs_cls}

\end{table*}


\begin{table*}[t!]
\centering
\resizebox{0.7\linewidth}{!}{
\begin{tabular}{@{}cccccccccccc@{}}
\toprule
\multicolumn{1}{l}{} &
  \rotatebox{0}{C10} &
  \rotatebox{0}{C100} &
  \rotatebox{0}{F101} &
  \rotatebox{0}{PETS} &
  \rotatebox{0}{FLOW} &
  \rotatebox{0}{SUN} &
  \multicolumn{1}{l}{\rotatebox{0}{CARS}} &
  \rotatebox{0}{DTD} &
  \rotatebox{0}{CAL} &
  \rotatebox{0}{Air} &
  \rotatebox{0}{AVG} \\ \midrule
ViT-B/32                     & 88.3 & 68.6 & 72.1 & 72.5 & 92.6 & 69.5 & 29.8                 & 67.8 & 86.2 & 27.7                     & 67.5                  \\
ViT-B/32+FDT                & 89.1 & 71.2 & 74.4 & 73.0   & 93.4 & 70.8 & 31.4                 & 69.4 & 87.7 & 27.9                     & 68.8 ($\uparrow$ 1.3) \\ \midrule

ViT-B/16                & 89.2& 69.5& 80.3& 75.1& 95.9& 73.4& 33.4& 71.5& 88.3& 32.0& 68.8 \\ 

ViT-B/16+FDT                     &   89.3&  71.6& 82.3& 75.8& 96.1& 74.2&  34.0& 71.8& 88.6&  29.3& 71.3 ($\uparrow$ 2.5)                \\\midrule

SwinV2-B                     & 85.6&  65.1&  78.5&  71.4&  94.3& 72.3&                  30.8&  69.4&  85.9&  32.1& 68.5                  \\
SwinV2-B+FDT                &86.8 & 67.5 & 80.5 & 75.6 & 94.8& 73.1& 33.4&  72.7& 88.9&  34.0& 70.7   ($\uparrow$ 2.2)                   \\ \bottomrule
\end{tabular}}
\vspace{-5pt}
\caption{Linear probing image classification accuracy (\%) when using different image encoder architectures. The dataset names are abbreviated. C10/100 is CIFAR10/100. F101 is Food101. FLOW is Flowers. CAL is Caltech. Air is Aircraft. ``AVG'' is the average accuracy over all datasets.} 
\label{tbl:enc_lp_cls}

\end{table*}


\begin{table*}[t!]
\centering
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{@{}lcccccccccccccc@{}}
\toprule
                & \multicolumn{7}{c}{Flickr30K}                                     & \multicolumn{7}{c}{MSCOCO}                                        \\ \midrule
 & \multicolumn{3}{c}{Image Retrieval} & \multicolumn{3}{c}{Text Retrieval} &  & \multicolumn{3}{c}{Image Retrieval} & \multicolumn{3}{c}{Text Retrieval} &  \\
                & R@1  & R@5  & R@10 & R@1  & R@5  & R@10 & rsum                    & R@1  & R@5  & R@10 & R@1  & R@5  & R@10 & rsum                    \\ \midrule
ViT-B/32        & 27.6 & 53.9 & 64.4 & 42.8 & 71.5 & 82.9 & 343.1                   & 15.9 & 36.7 & 47.8 & 24.8 & 49.8 & 61.8 & 236.8                   \\
ViT-B/32+FDT   & 32.6 & 58.6 & 68.5 & 51.0   & 78.3 & 87.5 & 376.5 ($\uparrow$ 33.4) & 19.4 & 40.8 & 51.9 & 29.6 & 55.3 & 66.1 & 263.1 ($\uparrow$ 26.3) \\ \midrule
ViT-B/16        & 35.3 & 60.6 & 71.7 & 50.5 & 81.1 & 88.6 & 387.8                   & 19.3 & 41.3 & 52.8 & 29.7 & 54.3 & 66.2 & 263.6                   \\
ViT-B/16+FDT & 41.6 & 67.5 & 76.9 & 60.8 & 86.1 & 92.6 & 425.5($\uparrow$ 37.7)  & 23.4 & 46.7 & 58.0   & 35.3 & 60.4 & 71.6 & 295.4($\uparrow$ 31.8)  \\ \midrule
SwinV2-B        & 30.5 & 56.8 & 67.8 & 48.5 & 77.7 & 86.8 & 368.1                   & 17.7 & 38.4 & 49.7 & 26.0   & 52.1 & 63.7 & 247.6                   \\
SwinV2-B+FDT & 39.6 & 65.2 & 74.9 & 57.9 & 85.7 & 92.2 & 415.5($\uparrow$ 47.4)  & 22.3 & 44.9 & 56.2 & 33.8 & 60.1 & 71.0   & 288.3($\uparrow$ 40.7)  \\ \bottomrule
\end{tabular}}
\vspace{-5pt}
\caption{Zero-shot image-text retrieval results on the Flickr30K and MSCOCO (5K) datasets when using different image encoder architectures.}
\label{tbl:enc_zs_itr}
\end{table*}


\begin{table*}[t!]
\centering
\resizebox{0.4\linewidth}{!}{
\begin{tabular}{@{}lcccc@{}}
\toprule
                & y/n  & number & other & overall               \\ \midrule
ViT-B/32        & 67.7 & 31.9   & 33.6  & 47.5                  \\
ViT-B/32 + FDT   & 67.8 & 34.6   & 39.6  & 50.6 ($\uparrow$ 3.1) \\ \midrule
ViT-B/16        & 69.0 & 33.2   & 36.0  & 49.2                  \\
ViT-B/16 + FDT & 72.0   & 37.6   & 42.9  & 54.3($\uparrow$ 5.1)  \\ \midrule
SwinV2-B        & 67.8 & 29.4   & 32.1  & 46.5                  \\
SwinV2-B + FDT & 68.6 & 34.5   & 41.0    & 51.6($\uparrow$ 5.1)  \\ \bottomrule
\end{tabular}
}
\vspace{-5pt}
\caption{Results of non-linear probing on VQA v2 dataset when using different image encoder architectures.}
\label{tbl:enc_vqa}
\end{table*}

\begin{table*}[t!]
\centering
\resizebox{0.5\linewidth}{!}{
\begin{tabular}{@{}lcccc@{}}
\toprule
 & \#param & FLOPs & \begin{tabular}[c]{@{}c@{}}Training time\\ (s/iter)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Inference throughput\\ (image-text pairs/s)\end{tabular} \\ \midrule
CLIP-ViT-B/32     & 151M & 7.3G  & 0.50 &  808.5\\
CLIP-ViT-B/32+FDT & 161M & 9.4G  & 0.60 & 642.8  \\ \midrule
CLIP-ViT-B/16     & 150M & 20.5G & 1.15 & 315.7  \\
CLIP-ViT-B/16+FDT & 160M & 25.1G & 1.29 & 272.5  \\ \midrule
CLIP-Swin-B       & 151M & 18.4G & 1.41 & 258.3  \\
CLIP-Swin-B+FDT   & 161M & 20.5G & 1.51 & 248.1  \\ \bottomrule
\end{tabular}}
\vspace{-5pt}
\caption{Computation cost when using different image encoder architecture.}
\label{tbl:cost}
\end{table*}

\subsection{FDT Number}
The results of models trained with
different FDT numbers are shown in Table \ref{tbl:fdt_zs_cls}, \ref{tbl:fdt_lp_cls}, \ref{tbl:fdt_zs_itr}, and \ref{tbl:fdt_vqa}.

\begin{table*}[t!]
\centering
\resizebox{0.7\linewidth}{!}{
\begin{tabular}{@{}lcccccccccc@{}}
\toprule
FDT size &
  \rotatebox{0}{C10} &
  \rotatebox{0}{C100} &
  \rotatebox{0}{F101} &
  \rotatebox{0}{PETS} &
  \rotatebox{0}{FLOW} &
  \rotatebox{0}{SUN} &
  \rotatebox{0}{DTD} &
  \rotatebox{0}{CAL} &
  \rotatebox{0}{IN} &
  \rotatebox{0}{AVG} \\ \midrule
-     & 60.4          & 33.5          & 39.6          & 23.1          & 54.0 & 42.0          & 17.0          & 65.5          & 37.0          & 41.3          \\
8192  & \textbf{70.4} & \textbf{40.4} & 38.3          & 19.9          & 51.3 & 42.8          & 16.6          & 68.1          & 37.8          & 42.8          \\
16384 & 67.7          & 39.9          & \textbf{42.9} & \textbf{25.8} & 55.5 & \textbf{45.5} & \textbf{26.5} & 69.6          & 39.3          & \textbf{45.9} \\
24576 & 69.0          & 39.1          & 41.9          & 24.2          & \textbf{55.7} & 44.4          & 21.8          & \textbf{70.5} & \textbf{39.8} & 45.2          \\ \bottomrule
\end{tabular}}
\vspace{-5pt}
\caption{Zero-shot image classification accuracy (\%) of models  with different FDT sizes. The row whose FDT value is ``-'' represents the CLIP model. The dataset names are abbreviated. C10/100 is CIFAR10/100. F101 is Food101. FLOW is Flowers. CAL is Caltech. IN is ImageNet-1K. ``AVG'' is the average accuracy over all datasets.}
\label{tbl:fdt_zs_cls}
\end{table*}



\begin{table*}[t!]
\centering
\resizebox{0.7\linewidth}{!}{
\begin{tabular}{@{}cccccccccccc@{}}
\toprule
\multicolumn{1}{l}{FDT size} &
  \rotatebox{0}{C10} &
  \rotatebox{0}{C100} &
  \rotatebox{0}{F101} &
  \rotatebox{0}{PETS} &
  \rotatebox{0}{FLOW} &
  \rotatebox{0}{SUN} &
  \multicolumn{1}{l}{\rotatebox{0}{CARS}} &
  \rotatebox{0}{DTD} &
  \rotatebox{0}{CAL} &
  \rotatebox{0}{Air} &
  \rotatebox{0}{AVG} \\ \midrule
-                     & 88.3 & 68.6 & 72.1 & 72.5 & 92.6 & 69.5 & 29.8                 & 67.8 & 86.2 & 27.7                     & 67.5                  \\
8192  & 89.1 & 70.3 & 72.8 & 70.7 & \textbf{93.4} & 70.1 & 29.6 & 68.5 & 87.2 & 27.5 & 67.9 \\
16384 & 89.1 & \textbf{71.2} & 74.4 & \textbf{73.0} & \textbf{93.4} & \textbf{70.8} & \textbf{31.4} & 69.4 & \textbf{87.7} & 27.9 & \textbf{68.8} \\
24576 & \textbf{89.3} & 71.0 & \textbf{74.9} & 71.2 & \textbf{93.4} & 70.6 & 30.1 & \textbf{69.8} & 87.2 & \textbf{28.7} & 68.6                 \\ \bottomrule
\end{tabular}}
\vspace{-5pt}
\caption{Linear probing image classification accuracy (\%) of models  with different FDT sizes. The row whose FDT value is ``-'' represents the CLIP model. The dataset names are abbreviated. C10/100 is CIFAR10/100. F101 is Food101. FLOW is Flowers. CAL is Caltech. Air is Aircraft. ``AVG'' is the average accuracy over all datasets.}
\label{tbl:fdt_lp_cls}
\end{table*}

\begin{table*}[t!]
\centering
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{@{}lcccccccccccccc@{}}
\toprule
         & \multicolumn{7}{c}{Flickr30K}                   & \multicolumn{7}{c}{MSCOCO}                      \\ \midrule
 &
  \multicolumn{3}{c}{Image Retrieval} &
  \multicolumn{3}{c}{Text Retrieval} &
   &
  \multicolumn{3}{c}{Image Retrieval} &
  \multicolumn{3}{c}{Text Retrieval} &
   \\
FDT size & R@1  & R@5  & R@10 & R@1  & R@5  & R@10 & rsum  & R@1  & R@5  & R@10 & R@1  & R@5  & R@10 & rsum  \\ \midrule
-        & 27.6 & 53.9 & 64.4 & 42.8 & 71.5 & 82.9 & 343.1 & 15.9 & 36.7 & 47.8 & 24.8 & 49.8 & 61.8 & 236.8 \\
8192     & 32.7 & 58.3 & 68.7 & 50.6 & 77.4 & 86.9 & 374.6 & 18.5 & 40.4 & 51.7 & 29.1 & 53.6 & 64.8 & 258.1 \\
16384 &
  32.6 &
  58.6 &
  68.5 &
  \textbf{51.0} &
  \textbf{78.3} &
  \textbf{87.5} &
  376.5 &
  \textbf{19.4} &
  \textbf{40.8} &
  \textbf{51.9} &
  29.6 &
  55.3 &
  66.1 &
  \textbf{263.1} \\
24576 &
  \textbf{33.3} &
  \textbf{60.3} &
  \textbf{70.4} &
  50.4 &
  78.1 &
  86.0 &
  \textbf{378.5} &
  18.6 &
  40.3 &
  51.8 &
  \textbf{29.7} &
  \textbf{55.8} &
  \textbf{66.9} &
  \textbf{263.1} \\ \bottomrule
\end{tabular}

}
\vspace{-5pt}
\caption{Zero-shot image-text retrieval results on the Flickr30K and MSCOCO (5K) datasets of models with different FDT sizes. The row whose FDT value is ``-'' represents the CLIP model.}
\label{tbl:fdt_zs_itr}
\end{table*}


\begin{table*}[t!]
\centering
\resizebox{0.3\linewidth}{!}{
\begin{tabular}{@{}lcccc@{}}
\toprule
FDT size & y/n           & number        & other         & overall       \\ \midrule
-        & 67.7          & 31.9          & 33.6          & 47.5          \\
8192     & 68.1          & 33.3          & 38.5          & 50.1          \\
16384    & 67.8          & 34.6          & 39.6          & 50.6          \\
24576    & \textbf{68.7} & \textbf{35.2} & \textbf{40.3} & \textbf{51.4} \\ \bottomrule
\end{tabular}}
\vspace{-5pt}
\caption{Results of non-linear probing on VQA v2 dataset of models with different FDT sizes. The row whose FDT value is ``-'' represents the CLIP model.}
\label{tbl:fdt_vqa}
\end{table*}



\subsection{Sparse Constraints}
We report the results of the models  trained with and without sparse constraint in Table \ref{tbl:sparse_zs_cls}, \ref{tbl:sparse_lp_cls}, \ref{tbl:sparse_zs_itr}, and \ref{tbl:sparse_vqa}.


\begin{table*}[t!]
\centering
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{@{}lcccccccccc@{}}
\toprule
 &
  \rotatebox{0}{C10} &
  \rotatebox{0}{C100} &
  \rotatebox{0}{F101} &
  \rotatebox{0}{PETS} &
  \rotatebox{0}{FLOW} &
  \rotatebox{0}{SUN} &
  \rotatebox{0}{DTD} &
  \rotatebox{0}{CAL} &
  \rotatebox{0}{IN} &
  \rotatebox{0}{AVG} \\ \midrule
CLIP                              & 60.4 & 33.5 & 39.6 & 23.1 & 54.0 & 42.0 & 17.0 & 65.5 & 37.0 & 41.3 \\
CLIP+FDT$_{\mathrm{Softmax}}$ *   & 23.7 & 1.2  & 4.6  & 2.7  & 1.8  & 3.5  & 4.2  & 4.1  & 1.2  & 5.2  \\
CLIP+FDT$_{\mathrm{Sparsemax}}$ * & 59.9 & 24.7 & 17.3 & 20.9 & 35.1 & 31.2 & 20.8 & 56.8 & 25.0   & 32.4 \\ \midrule
CLIP+FDT$_{\mathrm{Softmax}}$     & 68.7 & 36.9 & 35.5 & 27.9 & 53.8 & 43.8 & 23.1 & 66.6 & 38.6 & 43.9 \\
CLIP+FDT$_{\mathrm{Sparsemax}}$   & 67.7 & 39.9 & 42.9 & 25.8 & 55.5 & 45.5 & 26.5 & 69.6 & 39.3 & 45.6 \\ \bottomrule
\end{tabular}}
\vspace{-5pt}
\caption{Zero-shot image classification accuracy (\%) of models trained with (Sparsemax) and without (Softmax) sparse constraints. The rows marked with ``*'' are the results when using FDT weights as features. The dataset names are abbreviated. C10/100 is CIFAR10/100. F101 is Food101. FLOW is Flowers. CAL is Caltech. IN is ImageNet-1K. ``AVG'' is the average accuracy over all datasets.}
\label{tbl:sparse_zs_cls}
\end{table*}


\begin{table*}[t!]
\centering
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{@{}cccccccccccc@{}}
\toprule
\multicolumn{1}{l}{} &
  \rotatebox{0}{C10} &
  \rotatebox{0}{C100} &
  \rotatebox{0}{F101} &
  \rotatebox{0}{PETS} &
  \rotatebox{0}{FLOW} &
  \rotatebox{0}{SUN} &
  \multicolumn{1}{l}{\rotatebox{0}{CARS}} &
  \rotatebox{0}{DTD} &
  \rotatebox{0}{CAL} &
  \rotatebox{0}{Air} &
  \rotatebox{0}{AVG} \\ \midrule
CLIP                     & 88.3 & 68.6 & 72.1 & 72.5 & 92.6 & 69.5 & 29.8                 & 67.8 & 86.2 & 27.7                     & 67.5                  \\
CLIP+FDT$_{\mathrm{Softmax}}$  & 88.0 & 71.7 & 74.8 & 71.9 & 93.8 & 70.4 & 30.5 & 69.8 & 87.3 & 28.6 & 68.7 \\
CLIP+FDT$_{\mathrm{Sparsemax}}$ & 89.1 & 71.2 & 74.4 & 73.0   & 93.4 & 70.8 & 31.4 & 69.4 & 87.7 & 27.9 & 68.8 \\\bottomrule
\end{tabular}}
\vspace{-5pt}
\caption{Linear probing image classification accuracy (\%) of models  trained with (Sparsemax) and without (Softmax) sparse constraints. The dataset names are abbreviated. C10/100 is CIFAR10/100. F101 is Food101. FLOW is Flowers. CAL is Caltech. Air is Aircraft. ``AVG'' is the average accuracy over all datasets.}
\label{tbl:sparse_lp_cls}
\end{table*}


\begin{table*}[t!]
\centering
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{@{}lcccccccccccccc@{}}
\toprule
                                  & \multicolumn{7}{c}{Flickr30K}                   & \multicolumn{7}{c}{MSCOCO}                      \\ \midrule
 & \multicolumn{3}{c}{Image Retrieval} & \multicolumn{3}{c}{Text Retrieval} &  & \multicolumn{3}{c}{Image Retrieval} & \multicolumn{3}{c}{Text Retrieval} &  \\
FDT size                          & R@1  & R@5  & R@10 & R@1  & R@5  & R@10 & rsum  & R@1  & R@5  & R@10 & R@1  & R@5  & R@10 & rsum  \\ \midrule
CLIP                              & 27.6 & 53.9 & 64.4 & 42.8 & 71.5 & 82.9 & 343.1 & 15.9 & 36.7 & 47.8 & 24.8 & 49.8 & 61.8 & 236.8 \\
CLIP+FDT$_{\mathrm{Softmax}}$ *   & 5.4  & 12.0 & 16.3 & 1.7  & 3.8  & 6.3  & 45.5  & 2.4  & 6.8  & 9.7  & 0.8  & 2.4  & 4.1  & 26.2  \\
CLIP+FDT$_{\mathrm{Sparsemax}}$ * & 10.5 & 29.8 & 39.2 & 32.5 & 59.8 & 70.6 & 242.4 & 6.0  & 16.5 & 24.1 & 18.3 & 40.5 & 52.1 & 157.5 \\ \midrule
CLIP+FDT$_{\mathrm{Softmax}}$     & 33.3 & 60.7 & 69.5 & 47.9 & 78.0   & 88.2 & 377.6 & 19.2 & 40.3 & 51.7 & 28.3 & 53.8 & 65.5 & 258.8 \\
CLIP+FDT$_{\mathrm{Sparsemax}}$   & 32.6 & 58.6 & 68.5 & 51.0 & 78.3 & 87.5 & 376.5 & 19.4 & 40.8 & 51.9 & 29.6 & 55.3 & 66.1 & 263.1 \\ \bottomrule
\end{tabular}}
\vspace{-5pt}
\caption{Zero-shot image-text retrieval results on the Flickr30K and MSCOCO (5K) datasets of models trained with (Sparsemax) and without (Softmax) sparse constraints. The rows marked with ``*'' are the results when using FDT weights as features.}
\label{tbl:sparse_zs_itr}

\end{table*}


\begin{table*}[t]
\centering
\resizebox{0.45\linewidth}{!}{
\begin{tabular}{@{}lcccc@{}}
\toprule
 & y/n           & number        & other         & overall       \\ \midrule
CLIP        & 67.7          & 31.9          & 33.6          & 47.5          \\
CLIP+FDT$_{\mathrm{Softmax}}$     & 65.7          & 31.9          & 36.2          & 47.9          \\
CLIP+FDT$_{\mathrm{Sparsemax}}$    & 67.8          & 34.6          & 39.6          & 50.6 \\ \bottomrule
\end{tabular}}
\vspace{-5pt}
\caption{Results of non-linear probing on VQAv2 dataset of models trained with (Sparsemax) and without (Softmax) sparse constraints.}
\label{tbl:sparse_vqa}
\end{table*}





%%%%%%%%% REFERENCES
% \clearpage
% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{egbib}
% }
