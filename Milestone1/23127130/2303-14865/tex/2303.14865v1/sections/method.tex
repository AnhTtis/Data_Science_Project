

% In this section, we first revisit the feature representations of CLIP in Section \ref{sec:clip}. We then describe our proposed FDT for solving the limitations of CLIP in Section \ref{sec:method:code}. Finally, we compare our method with dictionary learning and vector quantization in Section \ref{sec:method:dl} and Section \ref{sec:method:vq}, respectively.

\subsection{Revisiting Feature Representations in CLIP}
\label{sec:clip}
% \cite{clip}, given a batch of $N$ image-text pairs $\{(v_i, t_i)| i = 1,...,N\}$, where $v_i$ and $t_i$ are the i-th image and text, respectively Then, the encoders are trained to increase the feature similarity of matched pairs and decrease that of unmatched pairs.

In CLIP, the image and text features are the aggregation of the embeddings of image patches or language tokens, respectively. Specifically, the image encoder takes an image as input and extracts the patch or local region embeddings based on the self-attention \cite{vit}, or convolution operations \cite{resnet}. The obtained patch features are then aggregated as the final representation of the image $f_{v}$ by using the attention pooling or the [CLS] token \cite{clip,vit}, which can be formulated as: 
\vspace{-5pt}
\begin{align}
    \label{patch_att}
    w_{p_i} = \frac{e^{<f_{g},f_{p_i}>}}
    {\sum_{j}^{N_v}e^{<f_{g},f_{p_j}>}}, \\
    f_{v} = \sum_{i} ^ {N_v} (w_{p_i} \cdot f_{p_i}). 
\end{align}
Here, $w_{p_i}$ is the weight of $i$-th patch, which measures the importance of the patch to the final representation. $<,>$ is the inner-product function. $N_v$ is the number of patches, and $f_{p_i}$ denotes the embedding of $i$-th patch. $f_{g}$ is the [CLS] token embedding or the average-pooled patch embedding, which embeds the global image information.

% The image encoder first calculates the attention weight of each patch with respect to the [CLS] token embedding or the average-pooled patch embedding, and then uses the weighted sum of patch embeddings as the final image representation

%Intuitively, the importance of each patch is determined based on its relevance to global image information.



% \begin{equation}
% \label{clip_v}

% \end{equation}
 


% \textcolor{red}{To do: explain multi-head attention can also be formulated as the above equations.}


Similarly, for the text encoder, the extracted text representation of an input sentence can also be regarded as the weighted sum of language token embeddings:

\begin{equation}
\label{clip_t}
f_{t} = \sum_{i} ^ {N_t} (w_{t_i} \cdot f_{t_i}),
\end{equation}
where $N_t$ is the number of language tokens. $f_{t_i}$ is the embedding of the $i$-th language token. It is extracted with the self-attention operations \cite{bert,gpt}, which model the relationship among the language tokens. $w_{t_i}$ is the weight of the  $i$-th language token, which is calculated by the following Equation \ref{patch_att} using the text [CLS] token.


% \textcolor{red}{To do: bi-LSTM+mean pooling}


% \begin{equation}\label{clip_t}
%     f_{t} = \sum_{i} (\frac{e^{<f_{cls},f_{t_i}>}}{\sum_{j}e^{<f_{cls},f_{t_j}>}}* f_{t_i}),
% \end{equation}


% Equation~\ref{clip_v} and \ref{clip_t} suggest image or text are represented as the weighted sum of patch and text information, respectively.  In other words, patch and text features can be treated as the bases of image or text representation. However, the information conveyed by image patches and language tokens may have different semantics and granularities, and they are embedded in two different continuous spaces before aligning. Additionally, the bases are dynamic, since the patches of language tokens of different images or texts are different. It may increase the learning difficulty of aligning image and text features. 

Equations~\ref{patch_att} and \ref{clip_t} suggest that images or texts are represented by two different bases: visual patches and language tokens. However, the information conveyed by image patches and language tokens may have different semantic meanings and granularities. Additionally, the bases are dynamic, since the visual patches or language tokens of different images or texts are different. It may increase the difficulty of learning an optimal alignment between image and text features \cite{tcl,learning_concepts}. Thus, the encoders may fail to capture important semantic concepts shared in both modalities and may encode irrelevant information.



% Equation \ref{patch_att} shows that the contribution of different visual patches or language tokens is merely determined by intra-modality information. 

% \textcolor{red}{A claim from the ICLR version: both patches with rich/less information are contributed to the final modality feature representation. Invalid? since attention weights could be zeros?}

% \emph{weighted sum of patch/text features}:
% \begin{equation}\label{clip_summary}
%     f_v = \sum_{i}(w_{p_i}*f_{p_i}), f_t = \sum_{i}(w_{t_i}*f_{t_i}),
% \end{equation}
% the weights $w_{p_i}, w_{t_i}$ can be interpreted as the importance of that patch/token, respectively.



\subsection{FDT-based Representation}
\label{sec:method:code}
To address the aforementioned limitations of feature representation in CLIP, we propose the FDT-based representation. Figure \ref{fig:method} gives an overview of our proposed method. Instead of representing the image and text with different bases, FDT serve as the common bases for both the image and text representations. As a result, the granularities of cross-modal information are explicitly unified. Moreover, the FDT encode the semantic information shared by both modalities. It can be regarded as prior knowledge that guides image and text encoders to extract feature embeddings.
In the following, we elaborate on the steps necessary to achieve FDT-based representations:

\vspace{0.1cm}
\noindent{\textbf{Grounding to FDT.}} Let $\{c_i|i=1,..., C\}$ be FDT, where $C$ is the number of shared tokens, and $c_i$ is the $i$-th discrete token. Given an input image, its patch embeddings are first extracted using the image encoder. The extracted patch embeddings are then projected to the FDT space by using a projecting function. The relevance between the image and a token is obtained by calculating the inner product between the projected patch embeddings and the token, and selecting the maximal value, which can be formulated as
\begin{equation}
    \label{eq:img_relevance}
    r_i^v = \underset{j}{\mathrm{max}}<f_{p_j}, c_i>,
\end{equation}
where $r_i^v$ is the relevance between the image and the $i$-th tokens. Intuitively, the proposed patch-level relevance calculation mechanism may enjoy two advantages: (1) it can capture small objects that exist in a single patch; (2) it helps remove the influence of irrelevant noisy patches that have low relevance to all FDT.

The relevance between the image and FDT is normalized by a Softmax function, which generates the final weights of each token  as follows:
\vspace{-5pt}
\begin{equation}
    \label{eq:img_softmax}
    w^v_i = \frac{e^{r_i^v}}
    {\sum_{j}^C{e^{r_j^v}}},
\end{equation} where $w^v_i$ is the weight of the $i$-th token with respect to the image.  Similarly, the weight $w^t_i$ of the $i$-th token assigned by an input text can be calculated using
\vspace{-5pt}
\begin{equation}
    \label{eq:text_relevance}
    r_i^t = \underset{j}{\mathrm{max}}<f_{t_j}, c_i>,
\end{equation}
\begin{equation}
    \label{eq:text_softmax}
    w^t_i = \frac{e^{r_i^t}}
    {\sum_{j}^C{e^{r_j^t}}}.
\end{equation}

Intuitively, FDT can be treated as prior knowledge for the image or text information. With the help of FDT, the extracted features of both modalities are grounded to a shared manifold space, thus enabling the cross-modal interaction.


% The encoders should learn to assign large weights to the FDT that encode the information conveyed by the images, while assigning small weights to others.


% Intuitively, Equation \ref{eq:img_relevance} and \ref{eq:text_relevance} show that FDT serves as prior to encode the image or text encoder to extract information.

\vspace{0.1cm}
\noindent{\bf Normalizing Concept Weights with Sparse Constraints.} We expect the normalized weights of FDT to be sparse, since it can largely reduce noise and make the results more interpretable \cite{conv_dict, mm_dict_cls}. Additionally, we empirically show that sparsity is crucial for FDT to learn cross-modal correspondence, where a token corresponds to the same image and text semantic meaning. We use the Sparsemax function \cite{sparsemax} for sparser weights, which is defined as:
\begin{equation}
    \label{eq:sparsemax}
    \argmin_{\pmb{p}\in\Delta^{K-1}}{\| \pmb{p}- \pmb{r} \|^2},
\end{equation}
where $\pmb{r}$ is the vector consisting of the relevance score between the image or text and FDT (Equation \ref{eq:img_relevance} and \ref{eq:text_relevance}). This function first calculates a threshold, and then sets the weights below the threshold to zero for sparsity. In contrast, the commonly used Softmax function cannot explicitly assign FDT with exactly zero probabilities.

% Finally, the results are normalized to a sum of 1.

\vspace{0.1cm}
\noindent{\textbf{Generating FDT-based Embeddings.~}} Given the normalized weights, the FDT-based features of the image $f^{\mathrm{FDT}}_v$ and text $f^{\mathrm{FDT}}_t$ are the weighted sum of FDT:
\vspace{-5pt}
\begin{equation}
\label{fdt_v}
    f^{\mathrm{FDT}}_v = \sum_{i}^{C} w_i^v \cdot c_i
\end{equation}
\begin{equation}
\label{fdt_t}
     f^{\mathrm{FDT}}_t = \sum_{i}^{C} w_i^t \cdot c_i 
\end{equation}
Equations \ref{fdt_v} and \ref{fdt_t} show that image and text features are represented by the same base FDT, which explicitly unifies the granularities of image and text information.

% Note that we do not map the token to the key, value spaces as the common attention mechanism. Since we empirically find that it makes the loss does not decrease. We speculate this is because the projected tokens are too noisy to re the models during the initial during the training strategies.


Given the FTD-based features, the encoders and FDT are trained to make the similarity between FDT-based features of matched image-text pairs larger than those of unmatched pairs:
\begin{align}
    \label{eq:infonce}
    \mathcal{L} = -\frac{1}{N} \sum_{i}^{N} \log\frac{\mathrm{exp}\left(\mathrm{sim}\left(f^{\mathrm{FDT}}_{v_i}, f^{\mathrm{FDT}}_{t_i}\right) / \tau\right)}{\sum_{j=1}^{N} \mathrm{exp}\left(\mathrm{sim}\left(f^{\mathrm{FDT}}_{v_i}, f^{\mathrm{FDT}}_{t_j}\right) / \tau\right)} \nonumber \\ -\frac{1}{N} \sum_{i}^{N}\log\frac{\mathrm{exp}\left(\mathrm{sim}\left(f^{\mathrm{FDT}}_{t_i}, f^{\mathrm{FDT}}_{v_i}\right) / \tau\right)}{\sum_{j=1}^{N} \mathrm{exp}\left(\mathrm{sim}\left(f^{\mathrm{FDT}}_{t_i}, f^{\mathrm{FDT}}_{v_j}\right) / \tau\right)},
\end{align} 
where $N$ is the number of matched image-text pairs, $sim$ is the cosine similarity function, and $\tau$ is the temperature hyper-parameter.

Intuitively, the equation shows that FDT are updated based on both the image and text modalities, and thus FDT is trained to learn the information shared by both modalities.




% \subsubsection{Relation to Dictionary Learning}
% \label{sec:method:dl} 
% Dictionary learning has been proposed to learn the sparse representations of signals with respect to a dictionary. Specifically, given the dictionary matrix $D$, the sparse representation of a signal $x$ is the weights that linearly combine $D$ to reconstruct $x$:
% \begin{equation}
%     \alpha D = x.
% \end{equation}
% When used in learning multi-modal features, $D$ shares the same idea as our FDT $\mathcal{C}$ that encode all cross-modal information. However, dictionary learning is trained for the generative tasks. As a result, the learned features may lack of enough discrimintative capability. Moreover, the dictionary learning method requires to solve an optimization problem in a two-step style, which is relatively slow. In contrast, our FDT-based method can be trained in an end-to-end manner.


% \subsubsection{Relation to Vector Quantization}
% \label{sec:method:vq} 

% The vector quantization (VQ) based methods \cite{vqvae, soho, li2022unimo} use a codebook to encode all the type of information, which serves the same role as our FDT $\mathcal{C}$. It selects the most relevant K elements from the codebook to represent an image or a text:
% \begin{equation}
% \centering
%     g_1, . . . , g_K = \mathrm{Top}_{K}\{r_1,...,r_C\}, \: f = \mathrm{pool}(g_1, . . . , g_K ),
% \end{equation}
% where $g_i$ is the i-th selected element, and $\mathrm{pool}$ is a function that converts the $K$-th element to a vector. When being used to learn vision-language features, the VQ-based approaches suffer from the cold start problem \cite{soho,li2022unimo,crossdiscrete}. To solve the problem, the encoders are typically initialized with the weights trained on other tasks. 
% In contrast, our methods softly query elements with the attention mechanism shown in Equations \ref{eq:img_relevance} and \ref{eq:img_softmax}. Since the entire process is differentiable, our method can be easily trained end-to-end from scratch.


% The hard element selecting mechanism is non-differentiable, which makes the training of the model difficult. When using in learning vision-language feature, it suffers from cold start problem \cite{soho,li2022unimo,crossdiscrete}. 