\noindent\textbf{Vision and Language Pre-training.} Vision and language pre-training methods can be briefly classified into two-stream %\cite{clip,align,filip,declip} 
and single-stream %\cite{ALBEF,uniter,oscar,singh2022flava} 
models based on their architectures.~A typical two-stream model leverages individual encoders to extract continuous feature embeddings from the inputs, and enforces the embeddings of a matched image-text pair to be similar by using contrastive learning \cite{clip,align,geng2023hiclip} and additional self-supervised tasks \cite{declip,yang2022vision}. Inherited from the encoder design, these feature embeddings convey information aggregated from local vision patches and language tokens, which encompass different semantic levels and granularities and are constrained by how patches are generated. Therefore, we propose FDT-based representations to directly perform contrastive learning on FDT that denotes high-level vision-semantic concepts.~The single-stream approaches feed all inputs together into a unified encoder (mostly transformers) to enhance the cross-modal interactions for a better cross-modal alignment \cite{ALBEF,li2022unimo,singh2022flava,yang2022vision,chen2023more,chen2018you}. For simplicity, we also clarify models consisting of individual encoders followed by multimodal fusion operations (late-fusion) as one-stream, because it requires the inputs from all modalities for inference and hence does not support ANN, %and involves computationally heavy fusion layers 
similar to a typical one-stream model (early-fusion). To combine the best of both worlds, FDT-based representations bridge the gap between different modalities with cross-modal interactions by vision-to-token and language-to-token information exchange, while maintaining a two-stream structure.


\noindent\textbf{Vector-Quantization and Codebook.} %Similar to our intuition that discrete learning can be also applied in encoding images, 
Vector-quantization is first proposed for image generation showing that image information can be encoded by discrete representations (namely \emph{codebook}) \cite{vqvae}.~Each image patch is represented by its nearest-neighbor code's embedding, and the decoder reconstructs the input image based on these code embeddings.~Because finding \emph{nearest-neighbor} is non-differentiable, the codebook is trained either by minimizing the distance between the code and image patch embeddings when the encoder is stop-gradient, or by exponential moving averages (EMA). Applying VQ to multimodal pre-training is more challenging, as the codebook now needs to accommodate multimodal contents and is often found to be sensitive to initialization (cold-start problem).~To address these challenges, previous studies leverage encoder or code warm-up \cite{crossdiscrete}, knowledge distilled vision tokenizers from pre-trained vision-language models \cite{peng2022beit2}, one-stream models to enforce multimodal code learning \cite{soho,li2022unimo}, and a combination of these techniques \cite{wang2022beit3}. As a comparison, our approach is designed to be more intuitive where only differentiable operations are used and it can be trained end2end from scratch while still maintaining a two-stream structure for ANN in large-scale retrieval tasks. More technical details will be discussed in Section \ref{sec:method:code}.


\noindent\textbf{Dictionary Learning.} Dictionary learning is another group of discrete representation learning in addition to VQ \cite{mm_dict_cls,mm_dict_conv,conv_dict}. Given a dictionary matrix \cite{conv_dict}, the representation of a signal is the weights that can linearly combine the dictionary matrix to reconstruct the signal with minimal error. When learning multi-modal representations \cite{mm_dict_cls,mm_dict_conv}, a shared dictionary matrix is used for facilitating cross-modal information alignment and fusion. The dictionary is served as the cross-modal information anchor, which shares the same idea as our method. However, the models are trained to solve a slow optimization problem, and the feature learned by solving the reconstruction or generative problem may have limited discriminative capability. By contrast, our model is trained end-to-end to learn discriminative information.