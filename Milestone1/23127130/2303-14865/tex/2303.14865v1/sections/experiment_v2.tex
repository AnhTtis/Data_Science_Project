\subsection{Experimental Settings}
\noindent{\textbf{Pre-training Datasets.~}} We use four publicly available datasets, including YFCC-15M V2 \cite{declip_benchmark}, Conceptual Captions (CC3M) \cite{cc3m}, Conceptual 12M (CC12M) \cite{cc12m} and LAION115M \cite{li2022blip} datasets to pre-train our models. We construct three different pre-training settings, including \textbf{15M}, \textbf{30M}, and \textbf{145M} settings. Each of the settings uses different combinations of pre-training datasets, as shown in Table. The 15M setting is used for the ablation study and to compare our methods with state-of-the-art methods under a fair setup \cite{declip_benchmark}. The 30M and 145M settings are used to evaluate the  scalability of our model.

\begin{table}[h]
\centering
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{@{}ll@{}}
\toprule
Setting & Dataset                             \\ \midrule
15M     & YFCC-15M V2                         \\
30M     & YFCC-15M V2, CC3M, CC12M            \\
145M    & YFCC-15M V2, CC3M, CC12M, LAION115M \\ \bottomrule
\end{tabular}
}
\caption{The used pre-training datasets under different settings.}
\vspace{-10pt}
\label{tbl:setting}
\end{table}


% In the 15M setting, we only use the YFCC-15M V2 dataset to save training cost for ablation study, while in 30M settings, we use the three datasets.


\begin{table*}[t]
\centering
\resizebox{0.7\linewidth}{!}{
\begin{tabular}{@{}lcccccccccc@{}}
\toprule
\multicolumn{1}{l}{} &
  \rotatebox{0}{C10} &
  \rotatebox{0}{C100} &
  \rotatebox{0}{F101} &
  \rotatebox{0}{PETS} &
  \rotatebox{0}{FLOW} &
  \rotatebox{0}{SUN} &
  \rotatebox{0}{DTD} &
  \rotatebox{0}{CAL} &
  \rotatebox{0}{IN} &
  \rotatebox{0}{AVG} \\ \midrule

SLIP~\cite{slip}   & 50.7 & 25.5 & 33.3 & 23.5 & 49.0   & 34.7 & 14.4 & 59.9 & 34.3 & 36.1 \\
MS-CLIP-S~\cite{msclip}  & -    & -    & -    & -    & -    & -    & -    & -    & 36.7 & -    \\
CLIP~\cite{clip}                   & 60.4 & 33.5 & 39.6 & 23.1 & 54.0 & 42.0 & 17.0 & 65.5 & 37.0                 & 41.3                  \\
FILIP~\cite{filip}    & 65.1 & 34.2 & 43.2 & 24.1 & 52.8 & 50.8 & 24   & 68.9 & 39.5 & 44.7 \\
DeCLIP~\cite{declip}    & 72.8 & 40.3 & 49.9 & 36.2 & 60.1 & 48.8 & 26.4 & 72.7 & 43.2 & 50.0 \\
\midrule
CLIP+FDT (Ours)                & 67.7 & 39.9 & 42.9 & 25.8 & 55.5 & 45.5 & 26.5 & 69.6 & 39.3                 & 45.9 \\
% FILIP+FDT (Ours)                &63.8 & 41.0 & 49.8 & 38.5 & 60.9 & 47.2 & 25.5 & 71.0 & 41.9 & 48.8 ($\uparrow$ 4.1)  \\
DeCLIP+FDT (Ours)               & \textbf{75.7} &
  \textbf{45.2} &
    \textbf{52.9} &
  \textbf{40.7} &
  \textbf{64.6} &
  \textbf{52.0} &
  \textbf{30.7} &
  \textbf{76.2} &
  \textbf{45.8} &
  \textbf{53.8}    \\ \bottomrule
\end{tabular}}

\caption{Zero-shot image classification accuracy (\%) under the 15M setting. The dataset names are abbreviated. 
C10/100 is CIFAR10/100. F101 is Food101. FLOW is Flowers. CAL is Caltech. IN
is ImageNet-1K. ``AVG'' is the average accuracy over all datasets.}
\label{tbl:15m_zs}

\end{table*}

\begin{table*}[ht!]
\centering
\resizebox{0.7\linewidth}{!}{
\begin{tabular}{@{}lccccccccccc@{}}
\toprule
\multicolumn{1}{l}{} &
  \rotatebox{0}{C10} &
  \rotatebox{0}{C100} &
  \rotatebox{0}{F101} &
  \rotatebox{0}{PETS} &
  \rotatebox{0}{FLOW} &
  \rotatebox{0}{SUN} &
  \multicolumn{1}{l}{\rotatebox{0}{CARS}} &
  \rotatebox{0}{DTD} &
  \rotatebox{0}{CAL} &
  \rotatebox{0}{AIR} &
  \rotatebox{0}{AVG} \\ \midrule
SLIP~\cite{slip}  & 87.4 & 69.5 & 71.3 & 70.5 & 91.9 & 66.9 & 27.5 & 65.6 & 86.2 & 27.7 & 66.5 \\
MS-CLIP-S~\cite{msclip}  & 87.2 & 66.7 & 76.0   & 62.1 & 93.8 & 71.7 & 27.5 & 69.4 & 81.6 & \textbf{32.9} & 66.9 \\
CLIP~\cite{clip}                  & 88.3 & 68.6 & 72.1 & 72.5 & 92.6 & 69.5 & 29.8                 & 67.8 & 86.2 & 27.7                     & 67.5                  \\
FILIP~\cite{filip}   & 86.5 & 66.6 & 71.7 & 69.2 & 93   & 69.6 & 30.0   & 66.4 & 85.7 & 27.0   & 66.6 \\
DeCLIP~\cite{declip}   & 89.4 & 69.6 & 75.9 & 71.4 & 95.7 & 71.6 & 30.1 & 66.9 & 89.0   & 26.7 & 68.6 \\
\midrule
CLIP+FDT (Ours)               & 89.1 & \textbf{71.2} & 74.4 & 73.0   & 93.4 & 70.8 & 31.4                 & 69.4 & 87.7 & 27.9                     & 68.8 \\

% FILIP+FDT (Ours)                     &72.1 & 49.3 & 54.9 & 43.4 & 78.1 & 60.4 & 0.5 & 58.7 & 37.6 & 6.7 & \textcolor{red}{46.2}                  \\

DeCLIP+FDT (Ours)                & \textbf{89.8}& \textbf{71.2}& \textbf{77.7}& \textbf{73.9}& \textbf{95.7}& \textbf{72.9}& \textbf{33.7}& \textbf{69.6}& \textbf{89.4}& 26.9&  \textbf{70.1}                      \\
\bottomrule
\end{tabular}}
\caption{Linear probing image classification accuracy (\%) under the 15M setting. The dataset names are abbreviated. 
C10/100 is CIFAR10/100. F101 is Food101. FLOW is Flowers. CAL is Caltech. Air is Aircraft. ``AVG'' is the average accuracy over all datasets.}
\label{tbl:15m_lp}
\vspace{-10pt}
\end{table*}

\begin{table*}[ht!]
\centering
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{@{}lcccccccccccc@{}}
\toprule
            & \multicolumn{4}{c}{Flickr30K} & \multicolumn{4}{c}{MSCOCO} & \multicolumn{4}{c}{VQAv2}                       \\
 & \multicolumn{2}{c}{Image Retrieval} & \multicolumn{2}{c}{Text Retrieval} & \multicolumn{2}{c}{Image Retrieval} & \multicolumn{2}{c}{Text Retrieval} &  &  &  &  \\
            & R@1   & R@5   & R@1   & R@5   & R@1   & R@5  & R@1  & R@5  & y/n  & number & other   & overall               \\ \midrule
SLIP~\cite{slip}    & 23.3  & 47.2  & 35.7  & 65.8  & 13.2  & 31.3 & 21.0   & 44.6 & 69.8 & 34.3   & 38.1    & 50.7                  \\
MS-CLIP-S~\cite{msclip}  & -     & -     & -     & -     & 19.4  & 40.8 & 28.5 & 54.1 & -    & -      & -       & -                     \\
CLIP~\cite{clip}    & 27.6  & 53.9  & 42.8  & 71.5  & 15.9  & 36.7 & 24.8 & 49.8 & 67.7 & 31.9   & 33.6    & 47.5                  \\
FILIP~\cite{filip}      & 30.6  & 58.2  & 46.3  & 74.4  & 16.2  & 37.5 & 25.6 & 50.8 & 68.1 & 34.5   & 36.2    & 49.2                  \\
DeCLIP~\cite{declip}      & 35.5  & 63.0  & 51.2  & 80.7  & 19.6  & 41.9 & 30.1 & 55.6 & \textbf{70.3} & 34.9   & 36.9    & 50.4                  \\

\midrule
CLIP+FDT (Ours)  & 32.6  & 58.6  & 51.0  & 78.3  & 19.4  & 40.8 & 29.6 & 55.3 & 67.8 & 34.6   & 39.6    & 50.6 \\
% FILIP+FDT (Ours)   & 34.5  & 61.2  & 55.8  & 81.8  & 18.9  & 40.5 & 33.1 & 58.9 &      &        & to-test &                       \\
DECLIP+FDT (Ours)  & \textbf{39.4}  & \textbf{66.8}  & \textbf{57.0}  & \textbf{82.3}  & \textbf{22.5}  & \textbf{45.5} & \textbf{34.0} & \textbf{59.6} & 67.8 & \textbf{35.8}   & \textbf{41.3}    & \textbf{51.6} \\ \bottomrule
\end{tabular}
}

\caption{Results of the vision-language tasks under the 15M setting, including the zero-shot image-text retrieval on the Flickr30K and MSCOCO (5K) datasets, and the non-linear probing on VQA v2 dataset.}

\label{tbl:15m_vl}
\vspace{-10pt}
\end{table*}

\noindent{\textbf{Evaluation Protocols.~}}
Following previous work \cite{declip, filip, declip_benchmark}, our method is evaluated on three commonly-used downstream tasks, including zero-shot image classification, linear probe image classification, and zero-shot image-text retrieval. Moreover, we propose a non-linear probe task to evaluate the effectiveness of the learned features for VQA~\cite{vqa}. The FDT-based features are used for all the downstream tasks.

\noindent\emph{Zero-shot image classification.~} In this task, image categories are represented by the text descriptions generated from their names. After extracting the embeddings of these text descriptions and input images by pre-trained encoders, the category of an image can be predicted by choosing the one whose text descriptions have the largest cosine similarity score. 
Following the setting of CLIP and DeCLIP, we construct 80 prompts to evaluate the performance of different approaches. 
We use 9 of the 11 commonly used datasets \cite{declip} for evaluation. The StanfordCars and Aircraft datasets are not used, because the pre-training datasets contain few captions about car models or aircraft types.

\noindent\emph{Linear Probe Image Classification.~} A linear classifier is trained to predict the categories of images based on the FDT-based features of the images. We use 10 of the 11 commonly
used datasets for evaluation. We do not report the results on ImageNet-1K, since conducting hyperparameter sweeping on this dataset is computationally expensive.

% % The zero-shot image classification and image-text retrieval tasks. For the zero-shot image classification tasks, We use 9 of the 11 commonly used datasets \cite{declip}. The other two datasets, including Standfordcars and Aircraft datasets are no used, becuase the pre-training datasets contain little captions about the model of cars or aircarfts. 
% The prompt setting is followed \cite{declip,clip}.
\noindent\emph{Image-text retrieval.~}  The image-text retrieval task is evaluated on the Flickr30K \cite{f30k} and MSCOCO \cite{mscoco} dataset. The recalls at different K values (R@K, K = 1, 5, 10) are reported as the evaluation metrics. They are used to measure the percentage of relevant items that match the queries in top-K retrieved items.~We also report rsum, which is obtained by summing all R@K values.

\noindent\emph{Non-linear probe task.~} The task is to evaluate the capability of learned features for vision-language reasoning tasks. The FDT-based embeddings of an image and its questions are concatenated and fed to two fully-connected layers with non-linear activation to predict the answer. More details can be found in the supplementary materials.



\begin{table*}[t]
\centering
\resizebox{0.9\linewidth}{!}{
\centering
\begin{tabular}{@{}lccc|ccc|cccc@{}}
\toprule
 &
   &
  ZS CLS &
  LP CLS &
  \multicolumn{3}{c|}{ZS-Flickr30K} &
   &
  ZS-MSCOCO &
   &
  VQAv2 \\
 &
  Setting &
  AVG Acc &
  AVG Acc &
  IR R@1 &
  TR R@1 &
  rsum &
  IR R@1 &
  TR R@1 &
  rsum &
  overall \\ \midrule
CLIP &
  15M &
  41.3 &
  67.5 &
  27.6 &
  42.8 &
  343.1 &
  15.9 &
  24.8 &
  236.8 &
  47.5 \\
CLIP+FDT &
  15M &
  45.9($\uparrow$4.6) &
  68.8($\uparrow$1.3) &
  32.6($\uparrow$5.0) &
  51.0($\uparrow$8.2) &
  376.5($\uparrow$33.4) &
  19.4($\uparrow$3.5) &
  29.6($\uparrow$4.8) &
  263.1($\uparrow$26.3) &
  50.6($\uparrow$3.1) \\ \midrule
CLIP &
  30M &
  56.8 &
  73.8 &
  43.6 &
  58.8 &
  431.3 &
  23.3 &
  34.8 &
  300.8 &
  50.6 \\
CLIP+FDT &
  30M &
  61.2($\uparrow$ 4.4) &
  75.6 ($\uparrow$ 1.8) &
  52.5($\uparrow$8.9) &
  70.8($\uparrow$12.0) &
  474.2($\uparrow$42.9) &
  28.3($\uparrow$5.0) &
  43($\uparrow$8.2) &
  337.1 ($\uparrow$36.3) &
  53.4($\uparrow$2.8) \\ \midrule
CLIP &
  145M &
  64 &
  82.1 &
  52.6 &
  67.9 &
  469.8 &
  29.3 &
  42.1 &
  335.2 &
  53.1 \\
CLIP+FDT &
  145M &
  69.0($\uparrow$ 5.0) &
  82.3 ($\uparrow$ 0.2) &
  56.3($\uparrow$3.7) &
  75.9($\uparrow$8.0) &
  489.4($\uparrow$19.6) &
  31.0($\uparrow$1.7) &
  46.4($\uparrow$4.3) &
  353.0($\uparrow$17.8) &
  55.2($\uparrow$2.1) \\ \bottomrule
\end{tabular}

%no_vqa
% \begin{tabular}{@{}cccc|ccc|ccc@{}}
% \toprule
%      &            & ZS CLS  & LP CLS  & \multicolumn{3}{c|}{ZS-Flickr30K} &        & ZS-MSCOCO &       \\
%      & data-scale & AVG Acc & AVG Acc & IR R@1     & TR R@1    & rsum     & IR R@1 & TR R@1    & rsum  \\ \midrule
% CLIP & 15M        & 41.3    & 67.5    & 27.6       & 42.8      & 343.1    & 15.9   & 24.8      & 236.8 \\
% CLIP+FDT &
%   15M &
%   45.9($\uparrow$4.6) &
%   68.8($\uparrow$1.3) &
%   32.6($\uparrow$5.0) &
%   51.0($\uparrow$8.2) &
%   376.5($\uparrow$33.4) &
%   19.4($\uparrow$3.5) &
%   29.6($\uparrow$4.8) &
%   263.1($\uparrow$26.3) \\ \midrule
% CLIP & 30M        & 56.8    & 73.8    & 43.6       & 58.8      & 431.3    & 23.3   & 34.8      & 300.8 \\
% CLIP+FDT &
%   30M &
%   61.2($\uparrow$ 4.4) &
%   75.6 ($\uparrow$ 1.8) &
%   52.5($\uparrow$8.9) &
%   70.8($\uparrow$12.0) &
%   474.2($\uparrow$42.9) &
%   28.3($\uparrow$5.0) &
%   43($\uparrow$8.2) &
%   337.1 ($\uparrow$36.3) \\ \midrule
% CLIP & 145M       & 64      & 82.1    & 52.6       & 67.9      & 469.8    & 29.3   & 42.1      & 335.2 \\
% CLIP+FDT &
%   145M &
%   69.0($\uparrow$ 5.0) &
%   82.3 ($\uparrow$ 0.2) &
%   56.3($\uparrow$3.7) &
%   75.9($\uparrow$8.0) &
%   489.4($\uparrow$19.6) &
%   31.0($\uparrow$1.7) &
%   46.4($\uparrow$4.3) &
%   353.0($\uparrow$17.8) \\ \bottomrule
% \end{tabular}

}
\caption{Ablation study results when using different scales of training data. ``ZS'' means zero-shot. ``AVG'' is average.  ``ACC'' is accuracy. ``LP'' stands for linear prob.  ``CLS'' represents classification. ``IR'' and ``TR'' are image retrieval and text retrieval, respectively.}
\label{tbl:data_scale}
\vspace{-5pt}
\end{table*}


\begin{table*}[ht!]
\centering
\resizebox{0.9\linewidth}{!}{
% \begin{tabular}{@{}lcc|ccc|ccc@{}}
% \toprule
%               & ZS CLS  & LP CLS  & \multicolumn{3}{c|}{ZS-Flickr30K} &        & ZS-MSCOCO &       \\
%               & AVG Acc & AVG Acc & IR R@1     & TR R@1     & rsum    & IR R@1 & TR R@1    & rsum  \\ \midrule
% CLIP-ViT-B/32 & 41.3    & 67.5    & 27.6       & 42.8       & 15.9    & 24.8   & 24.8      & 236.8 \\
% CLIP-ViT-B/32+FDT &
%   45.9($\uparrow$4.6) &
%   68.8($\uparrow$1.3) &
%   32.6($\uparrow$5.0) &
%   51.0($\uparrow$8.2) &
%   19.4($\uparrow$3.5) &
%   29.6($\uparrow$4.8) &
%   29.6($\uparrow$4.8) &
%   263.1($\uparrow$26.3) \\ \midrule
% CLIP-ViT-B/16 & 45.2    & 68.8    & 35.3       & 50.5       & 19.3    & 29.7   & 34.8      & 300.8 \\
% CLIP-ViT-B/16+FDT &
%   49.9($\uparrow$4.7) &
%   71.3($\uparrow$2.5) &
%   41.6($\uparrow$6.3) &
%   60.8($\uparrow$10.3) &
%   23.4($\uparrow$4.1) &
%   35.3($\uparrow$5.6) &
%   43($\uparrow$8.2) &
%   337.1 ($\uparrow$36.3) \\ \midrule
% CLIP-Swin-B   & 39.6    & 68.5    & 30.5       & 48.5       & 17.7    & 26     & 42.1      & 335.2 \\
% CLIP-Swin-B+FDT &
%   42.4($\uparrow$2.8) &
%   70.7($\uparrow$2.2) &
%   39.6($\uparrow$9.1) &
%   57.9($\uparrow$9.4) &
%   22.3($\uparrow$4.6) &
%   33.8($\uparrow$7.8) &
%   46.4($\uparrow$4.3) &
%   353.0($\uparrow$17.8) \\ \bottomrule
% \end{tabular}
% \begin{tabular}{@{}llllllll@{}}
% \toprule
%               & ZS CLS AVG & LP CLS AVG & ZS-Flickr30K-IR & ZS-Flickr30K-TR & ZS MSCOCO-IR & ZS MSCOCO-TR & VQAv2 \\ \midrule
% CLIP-ViT-B/32 & 41.3       & 67.5       & 27.6            & 42.8            & 15.9         & 24.8         & 47.5  \\
% CLIP-ViT-B/32+FDT &
%   45.9($\uparrow$4.6) &
%   68.8($\uparrow$1.3) &
%   32.6($\uparrow$5.0) &
%   51($\uparrow$8.2) &
%   19.4($\uparrow$3.5) &
%   29.6($\uparrow$4.8) &
%   50.6($\uparrow$3.1) \\ \midrule
% CLIP-ViT-B/16 & 45.2       & 68.8       & 35.3            & 50.5            & 19.3         & 29.7         & 49.2  \\
% CLIP-ViT-B/16+FDT &
%   49.9($\uparrow$4.7) &
%   71.3($\uparrow$2.5) &
%   41.6($\uparrow$6.3) &
%   60.8($\uparrow$10.3) &
%   23.4($\uparrow$4.1) &
%   35.3$\uparrow$5.6) &
%    \\ \midrule
% CLIP-Swin-B   & 39.6       & 68.5       & 30.5            & 48.5            & 17.7         & 26.0           & 46.5  \\
% CLIP-Swin-B+FDT &
%   42.4($\uparrow$2.8) &
%   70.7($\uparrow$2.2) &
%   39.6($\uparrow$9.1) &
%   57.9($\uparrow$9.4) &
%   22.3($\uparrow$4.6) &
%   33.8($\uparrow$7.8) &
%   51.6($\uparrow$5.1)
%    \\ \bottomrule
% \end{tabular}
% \begin{tabular}{@{}lcc|ccc|ccc@{}}
% \toprule
%  &
%   ZS CLS &
%   LP CLS &
%   \multicolumn{3}{c|}{ZS-Flickr30K} &
%    &
%   ZS-MSCOCO &
%    \\
%  &
%   AVG Acc &
%   AVG Acc &
%   IR R@1 &
%   TR R@1 &
%   rsum &
%   IR R@1 &
%   TR R@1 &
%   rsum \\ \midrule
% CLIP-ViT-B/32 &
%   41.3 &
%   67.5 &
%   27.6 &
%   42.8 &
%   343.1 &
%   15.9 &
%   24.8 &
%   236.8 \\
% CLIP-ViT-B/32+FDT &
%   45.9($\uparrow$4.6) &
%   68.8($\uparrow$1.3) &
%   32.6($\uparrow$5.0) &
%   51.0($\uparrow$8.2) &
%   376.5($\uparrow$33.4) &
%   19.4($\uparrow$3.5) &
%   29.6($\uparrow$4.8) &
%   263.1($\uparrow$26.3) \\ \midrule
% CLIP-ViT-B/16 &
%   45.2 &
%   68.8 &
%   35.3 &
%   50.5 &
%   387.8 &
%   19.3 &
%   29.7 &
%   263.6 \\
% CLIP-ViT-B/16+FDT &
%   49.9($\uparrow$4.7) &
%   71.3($\uparrow$2.5) &
%   41.6($\uparrow$6.3) &
%   60.8($\uparrow$10.3) &
%   425.5($\uparrow$37.7) &
%   23.4($\uparrow$4.1) &
%   35.3($\uparrow$5.6) &
%   295.4 ($\uparrow$31.8) \\ \midrule
% CLIP-Swin-B &
%   39.6 &
%   68.5 &
%   30.5 &
%   48.5 &
%   368.1 &
%   17.7 &
%   26.0 &
%   247.6 \\
% CLIP-Swin-B+FDT &
%   42.4($\uparrow$2.8) &
%   70.7($\uparrow$2.2) &
%   39.6($\uparrow$9.1) &
%   57.9($\uparrow$9.4) &
%   415.5($\uparrow$47.4) &
%   22.3($\uparrow$4.6) &
%   33.8($\uparrow$7.8) &
%   288.3($\uparrow$40.7) \\ \bottomrule
% \end{tabular}

\begin{tabular}{lcc|ccc|cccc}
\toprule
 &
  ZS CLS &
  LP CLS &
  \multicolumn{3}{c|}{ZS-Flickr30K} &
   &
  ZS-MSCOCO &
   &
  VQAv2 \\
 &
  AVG Acc &
  AVG Acc &
  IR R@1 &
  TR R@1 &
  rsum &
  IR R@1 &
  TR R@1 &
  rsum &
  Overall \\ \midrule
CLIP-ViT-B/32 &
  41.3 &
  67.5 &
  27.6 &
  42.8 &
  343.1 &
  15.9 &
  24.8 &
  236.8 &
  47.5 \\
CLIP-ViT-B/32+FDT &
  45.9($\uparrow$4.6) &
  68.8($\uparrow$1.3) &
  32.6($\uparrow$5.0) &
  51.0($\uparrow$8.2) &
  376.5($\uparrow$33.4) &
  19.4($\uparrow$3.5) &
  29.6($\uparrow$4.8) &
  263.1($\uparrow$26.3) &
  50.6($\uparrow$3.1) \\ \midrule
CLIP-ViT-B/16 &
  45.2 &
  68.8 &
  35.3 &
  50.5 &
  387.8 &
  19.3 &
  29.7 &
  263.6 &
  49.2 \\
CLIP-ViT-B/16+FDT &
  49.9($\uparrow$4.7) &
  71.3($\uparrow$2.5) &
  41.6($\uparrow$6.3) &
  60.8($\uparrow$10.3) &
  425.5($\uparrow$37.7) &
  23.4($\uparrow$4.1) &
  35.3($\uparrow$5.6) &
  295.4 ($\uparrow$31.8) &
  54.3($\uparrow$5.1) \\ \midrule
CLIP-Swin-B &
  39.6 &
  68.5 &
  30.5 &
  48.5 &
  368.1 &
  17.7 &
  26.0 &
  247.6 &
  46.5 \\
CLIP-Swin-B+FDT &
  42.4($\uparrow$2.8) &
  70.7($\uparrow$2.2) &
  39.6($\uparrow$9.1) &
  57.9($\uparrow$9.4) &
  415.5($\uparrow$47.4) &
  22.3($\uparrow$4.6) &
  33.8($\uparrow$7.8) &
  288.3($\uparrow$40.7) &
  51.6($\uparrow$5.1) \\ \bottomrule
\end{tabular}

}
\caption{Ablation Study results when using different image encoder architectures. ``ZS'' means zero-shot. ``AVG'' is average.  ``ACC'' is accuracy. ``LP'' stands for linear prob.  ``CLS'' represents classification. ``IR'' and ``TR'' are image retrieval and text retrieval.}
\label{tbl:enc_abl}
\vspace{-10pt}
\end{table*}

\noindent{\textbf{Implementation Details.~}} We evaluate our method by incorporating it into two state-of-the-art contrastive vision-language pre-training approaches, namely CLIP \cite{clip} and DECLIP \cite{declip}. Our implementation is based on the open-source PyTorch implementation\footnote{https://github.com/Sense-GVT/DeCLIP} of the two methods. We use 16384 tokens, each with 512 dimensions. Please refer to the supplementary material for detailed information.


\begin{table*}[t]
\centering
\resizebox{0.75\linewidth}{!}{
\begin{tabular}{@{}lcc|ccc|cccc@{}}
\toprule
\multicolumn{1}{l}{} & ZS CLS  & LP CLS  & \multicolumn{3}{c|}{ZS-Flickr30K} &        & ZS-MSCOCO &       & VQAv2   \\
FDT size           & AVG Acc & AVG Acc & IR R@1     & TR R@1    & rsum     & IR R@1 & TR R@1    & rsum  & overall \\ \midrule
-                    & 41.3    & 67.5    & 27.6       & 42.8      & 343.1    & 15.9   & 24.8      & 236.8 & 47.5    \\
8192                 & 42.8    & 67.9    & 32.7       & 50.6      & 374.6    & 18.5   & 29.1      & 258.1 & 50.1    \\
16384                & \textbf{45.9}    & \textbf{68.8}    & 32.6       & \textbf{51.0}        & 376.5    & \textbf{19.4}   & 29.6      & \textbf{263.1} & 50.6    \\
24576                & 45.2    &68.6         & \textbf{33.3}       & 50.4      & \textbf{378.5}    & 18.6   & \textbf{29.7}      & \textbf{263.1} & \textbf{51.4}    \\ \bottomrule
\end{tabular}}
\caption{Results of the models with different FDT sizes. The row whose FDT value is ``-'' represents the original CLIP model. ``ZS'' means zero-shot. ``AVG'' is average.  ``ACC'' is accuracy. ``LP'' stands for linear prob.  ``CLS'' represents classification. ``IR'' and ``TR'' are image retrieval and text retrieval.}
\label{tbl:fdt_size}
\vspace{-5pt}
\end{table*}


\begin{table*}[ht!]
\centering
\resizebox{0.75\linewidth}{!}{
\begin{tabular}{@{}lcc|ccc|ccc|c@{}}
\toprule
                     & ZS CLS  & LP CLS  & \multicolumn{3}{c|}{ZS-Flickr30K} &        & ZS-MSCOCO &       & VQAv2   \\
                     & AVG Acc & AVG Acc & IR R@1     & TR R@1    & rsum     & IR R@1 & TR R@1    & rsum  & overall \\ \midrule
CLIP                 & 41.3    & 67.5    & 27.6       & 42.8      & 343.1    & 15.9   & 24.8      & 236.8 & 47.5    \\
CLIP+FDT$_{\mathrm{Softmax}}$ *   & 5.2     & -       &5.4            &1.7           &45.5          &2.4        &0.8           &26.2       &-         \\
CLIP+FDT$_{\mathrm{Sparsemax}}$ * & 32.4    & -       &10.5            &32.5           &242.4          & 6.0        & 18.3           & 157.5       &-      \\ \midrule
CLIP+FDT$_{\mathrm{Softmax}}$     & 43.9    &68.7         & 33.3       & 47.9      & 377.6    & 19.2   & 28.3      & 258.8 & 47.9    \\
CLIP+FDT$_{\mathrm{Sparsemax}}$   & 45.9    & 68.8    & 32.6       & 51.0      & 376.5    & 19.4   & 29.6      & 263.1 & 50.6    \\ \bottomrule
\end{tabular}
}
\caption{Results of models trained with (Sparsemax) and without (Softmax) sparse constraints. The rows marked with ``*'' are the results when using FDT weights as features (see Section \ref{sec_exp_sparse}). ``ZS'' means zero-shot. ``AVG'' is average.  ``ACC'' is accuracy. ``LP'' stands for linear prob.  ``CLS'' represents classification. ``IR'' and ``TR'' are image retrieval and text retrieval.} 
\label{tbl:sparse}
\vspace{-5pt}
\end{table*}

\subsection{Comparison with State-of-the-Art Approaches}
We compare our method with the state-of-the-art CLIP family approaches on the benchmark proposed in \cite{declip_benchmark}. In this benchmark, methods are compared fairly by pre-training them using the same training recipe and data (our 15M setting).~Note that the original paper only reports the results for zero-shot classification on the ImageNet dataset, and the results of other tasks are obtained by directly applying the released checkpoints for evaluation.

The results for zero-shot image classification, linear prob image classification, and vision-language reasoning tasks are reported in Table \ref{tbl:15m_zs}, \ref{tbl:15m_lp}, and \ref{tbl:15m_vl}, respectively. 
First, we observe that using the proposed FDT-based representation with CLIP (i.e., CLIP+FDT) can achieve significant performance improvement over CLIP on all the downstream tasks. 
Notably, CLIP+FDT can outperform FILIP \cite{filip}, which aligns image and text information at the fine-grained patch and language token levels. The results suggest that aligning global cross-modal information in a unified space is more effective than directly aligning fine-grained patches and language tokens with different granularities.
Interestingly, the linear probe results show that CLIP+FDT can learn a comparable image encoder with DeCLIP, which applies various self-supervised pretext tasks that have already been proven effective for visual recognition. One possible reason is that aligning the information in a unified space helps our model better leverage semantic supervision signals in the language domain. We can also see that our method can significantly improve DeCLIP for all the tasks and achieve state-of-the-art performance on the benchmark. It shows that our approach is compatible with self-supervised learning tasks to improve CLIP. Moreover, FDT can improve the VQAv2 task, which requires the capability of collaborative multi-modal reasoning and content understanding.


% The results for zero-shot image classification, linear prob, and vision language tasks are reported in Table \ref{tbl:15m_zs}, \ref{tbl:15m_lp}, and \ref{tbl:15m_vl}, respectively.  First, we can observe that using the proposed FDT-based representation with CLIP (CLIP+FDT) can achieve significant performance improvement over CLIP on all the downstream tasks. Notably, CLIP+FDT can outperform FILIP \cite{filip}, which aligns image text information at the patch and language token level. \textcolor{red}{The results show that aligning cross-modal information with a unified space at the global level is more effective than at the finer-grained level with an inconsistent gra level.} Interestingly, the results on the linear probe task show CLIP+FDT can learn a comparable image encoder with DeCLIP, which applies various self-supervised tasks that are effective for visual recognition. One possible reason is that aligning the information in a unified space can better leverage semantic supervision signals existing in the language domain. We can also see that our method can also significantly improves DeCLIP for all the tasks, and achieve state-of-the-art performance on the benchmark. It demonstrates that our method is compatible with self-supervised learning tasks to improve CLIP. Moreover, we find that using FDT can improve the VQAv2 tasks which require collaborative multi-modal reasoning and content understanding.


% Notably, using FDT with CLIP can achieve comparable performance with DeCLIP The results indicate that our method can help the CLIP learn better visual encoders.


% can outperform the FILIP and SLIP, which improve CLIP by incorporating self-supervised learning and token-level alignment. Moreover, incorporating our method into DeCLIP can further improve the performance of DeCLIP, and achieve the state of the art performances. 

% We can see that our method consistently improves CLIP and DeCLIP for the linear prob tasks shown in Table \ref{tbl:15m_lp}. Notably, using FDT with CLIP can achieve comparable performance with DeCLIP The results indicate that our method can help the CLIP learn better visual encoders.

% Our model also achieves significant performance for VL tasks.  

% \subsection{Vision Tasks}

% The zero-shot image classification results are reported in Table \ref{tbl:zs_cls}, \ref{}. We can see that our proposed methods can achieve significant performance improvement over the three baseline methods on both the 15M and 30M setting, demonstrating the generality of our method. We also notice that our methods can achieve noticeable improvement for the datasets that requires the model to recognize the fine-grained distinction among classes, such as PETS and Flowers. One possible reason is that our method can help the model to extract fine-grained information.






% \subsection{Visual-Language Tasks}

% We report the results for the zero-shot image-text retrieval tasks on Table \ref{tbl:zs_cls}. The results show that incorporating our methods significantly outperforms the correspondent baseline approaches. We speculate that this is because our proposed method learned better alignment between text and images.  The consistent improvement on the 15M and 30M datasets demonstrate that our method can scale well as dataset size increases.
%




%FILIP       &      &      &      &      &      &      &       \\
%FILIP+Code  &      &      &      &      &      &      &       \\ \hline




\subsection{Ablation Study}
In this section, we conduct ablation studies to investigate how different factors influence the performance of our approach. These factors include the pre-training data scale, image encoder architecture, and several design choices of our method. Throughout the ablation study, we use the CLIP model as the baseline to save computation costs.


\noindent{\textbf{Pretraining Data Scale.}} We evaluate the performance of our methods on different pre-training data scales by further pre-training the model on 30M and 145M data. According to the results presented in Table \ref{tbl:data_scale}, our method still achieves improved performance for all the downstream tasks when pre-trained on larger datasets. We also note that the improvement for the linear probing setting is minor when pre-trained on 145M data. We assume this is because the performance of the model saturates. To further improve the performance of the image encoder, a more vision-specific training task is needed. Note that using FDT still achieves significant performance improvements on 145M data for other tasks. Interestingly, our model achieves significant improvements on the 30M data. One possible reason is that our FDT can benefit significantly from cleaning supervision information in the CC3M \cite{cc3m} and CC12M \cite{cc12m} datasets. We have similar observations for the VQAv2 task.

\noindent{\textbf{Image Encoder Architecture.~}} We evaluate the influence of different image encoder architectures on our proposed method, and the results are reported in Table \ref{tbl:enc_abl}. We observe that our method still significantly outperforms CLIP when using different types of image encoders. Additionally, FDT slightly adds an average of 6\% more parameters, 13\% more training time, and 12\% less throughput when using different encoder architectures. The detailed results can be found in the supplementary materials.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}

% Interestingly, we find that the improvement is more significant when using Swin-B \cite{swin}.~One possible reason is that the image embedding extracted from Swin-B is largely inconsistent with the text features, since the Swin architecture is designed with an inductive bias for image data \cite{swin}.~Our FDT features can effectively bridge the gap between these features, and enhance performance.

\noindent{\textbf{FDT Numbers.}} The performance of models trained with different learnable token numbers are shown in Table \ref{tbl:fdt_size}. We can see that using 8192 tokens can already achieve an improvement over CLIP. Increasing the FDT size to 16384 obtains a more significant improvement than 8192, since it can encode more types of information. Furthermore, growing the FDT size to 24576 achieves a slight improvement over 16384 for the zero-shot image-text retrieval task on the Flickr30K dataset and VQA task. We set the FDT size as 16384 in our implementation because it achieves the best performance-efficiency tradeoff.


\noindent{\textbf{Sparse Constraints.}} 
\label{sec_exp_sparse}
% We first demonstrate that applying sparse constraints help the model learn better cross-modal correspondence, where the same cross-modal information is represented by using the same subset of FDT. 
In this section, we aim to demonstrate that applying sparse constraints helps the model learn better cross-modal correspondence, where the same cross-modal information is represented using the same subset of FDT. To this end, we evaluate the performance when using the FDT weights (Equation \ref{eq:img_softmax}, \ref{eq:text_softmax} and \ref{eq:sparsemax}) of each image or sentence as the features for zero-shot image classification and image-text retrieval tasks. The results are reported in Table~\ref{tbl:sparse}. From the table, we can see that using sparse constraints (Sparsemax) achieves significantly better performance for all tasks. The results demonstrate that adding sparse constraints to FDT weights can lead to better cross-modal correspondence. 
Additionally, we can also see that without sparse constraints (Softmax), FDT-based features can also achieve significant performance over CLIP. Adding a sparse constraint (Sparsemax) achieves a larger performance improvement. This is because the granularities are further unified by representing the same cross-modal information with the same token set.


% We first compare the code usage ratio, which is the number of tokens whose weights are large than zeros divided by tokens numbers for a image and sentences. We can see that using sparsemax function significantly reduces code weights, which demonstrates that using sparsemax can obtain more sparse code weights. We find that using sparsemax funtion ac


% between the models trained with softmax and sparsemax, 


% We can see that without sparse constraints, the model use all the code to represent an image or text. Using the constraints significantly reduces the code usage by 73.4\%. More importantly, we compare the performance when using the codebook weight $W$ as features. It shows that using sparse constraints achieves significantly better results. It means that using sparse constraints can better align the  usage based on the visual-semantic similarity of image-text pairs.

\begin{figure}[t!]
\begin{center}
    \includegraphics[width=\linewidth]{figures/t2i_case_study.pdf}
\end{center}
\vspace{-10pt}
\caption{Examples shows the top-5 retrieved images for the given text queries for the text-to-image retrieval task on MSCOCO.}
\label{fig:coco_case}
\vspace{-15pt}
\end{figure}


\begin{figure*}[ht!]
\begin{center}
    \includegraphics[width=0.95\linewidth] {figures/code_vis_v2.pdf}
\end{center}
\vspace{-5pt}
\caption{Example of the top-5 most relevant image patches and text tokens of four FDT tokens. Note that the redundant text tokens in the top-5 are removed. The color of the heatmap from blue to red denotes the relevance between patches and FDT from small to large.}
\label{fig:code_vis}
\vspace{-10pt}
\end{figure*}




\subsection{Analysis of the Completeness of Alignment}

Since the granularities of image and text information are inconsistent, the learned model may fail to capture key semantic concepts \cite{learning_concepts}. In this experiment, we empirically evaluate whether unifying the granularities through the proposed FDT can alleviate the problem.  The model pretrained on the 145M dataset is used for this evaluation.

To this end, we design a probing experiment on the MSCOCO dataset. Using the object detection annotations in the training split of MSCOCO, we construct 305,723 sentence pairs. For each sentence pair, one \emph{matched sentence} describes all objects in an image, while the other \emph{partially matched sentence} only captures part of the objects. Please refer to the supplementary material for more details about how we constructed these sentence pairs. 

We then use pre-trained models to extract the embeddings of images and sentences and compute the similarity scores between the images and these constructed sentences. If the learned model comprehensively captures the semantic concepts, the similarity between an image and its matched sentence should be higher than that between the partially matched sentence. We found that the CLIP+FDT models can meet our expectation in 68.2 \% of all sentence pairs, surpassing the CLIP model by 7.6\%. The results demonstrate that FDT can help the CLIP model more comprehensively capture various semantic concepts. We assume that this is because the FDT serve as the prior knowledge that guides encoders to extract cross-modally shared high-level semantic concepts. This not only facilitates cross-modal interactions but also helps encoders capture semantic information from images and texts more comprehensively.

In addition, we show two cases for the text-to-image retrieval task in Figure \ref{fig:coco_case}. We can see that the images retrieved by CLIP ignore some important concepts described in the text queries. For example, in terms of the text query ``baseball players entertaining a crowd of spectators'', four out of the five images retrieved by the CLIP models contain baseball players only but with no spectators. Moreover, the image containing spectators is ranked lower than the two images without spectators. In contrast, FDT can retrieve images that contain both baseball players and spectators. More results are provided in the supplementary material. 


% for the text query ``Three elephants standing in the water'', the top-2 retrieved images contain no water, while the image contains water only ranked in the third and fourth retrieved samples. ``spectators'' and ``water'' 



% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \begin{table}[t]
% \caption{Results of models with and without sparse constraints of code weights. ``code usage ratio'' are the mean average ratio of code whose weights are larger than 0 calculated on the MS-COCO dataset. ZS-CLS is the average zero-shot image classification result; and ZS-retrieval is the results of zero-shot image-text retrieval tasks; ``code weights'' means using the code weights as feature for the downstream tasks.}
% \label{tbl:sparse}
% \centering
% \resizebox{1\linewidth}{!}{
% \begin{tabular}{@{}lccccc@{}}
% \toprule
% \multicolumn{1}{c}{} &
%   code usage ratio &
%   \begin{tabular}[c]{@{}c@{}}ZS-CLS  \\ code weight\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}ZS-retrieval \\ code weights\end{tabular} &
%   ZS-CLS &
%   ZS-retrieval \\ \midrule
% Softmax &
%   100.0 &
%   5.2 &
%   26.2 &
%   43.8 &
%   258.9 \\
% Sparsemax &
%   26.6 &
%   34.4 &
%   157.5 &
%   45.9 &
%   263.1 \\ \bottomrule
% \end{tabular}}
% \end{table}


\subsection{Visualization of Learned FDT}
To explicitly show the cross-modal correspondence learned by our FDT, we visualize the top-5 most relevant image patches and text tokens (using Equation \ref{eq:img_relevance} and \ref{eq:text_relevance}) of four FDT tokens in Figure~\ref{fig:code_vis}. The MSCOCO dataset and the model pretrained on the 145M dataset are used for visualization. The example cases show that each token captures different types of cross-modal correspondence, including actions (jump/jumping), objects, and attributes (orange color). Moreover, the learned FDT can potentially detect correspondent patches from the images. For example, the second token has high relevance values with patches of cats, while having low relevance with other patches. More results can be found in the supplementary material. 


\section{Conclusions}
In this paper, we introduce a new multimodal presentation using finite discrete tokens (FDT). Specifically, a set of learnable tokens shared by all modalities are used to represent multimodal information conveyed in the image and text modalities. %The multimodal inputs are first grounded to a subset of FDT. The combination of the activated FDT embeddings is then used as the representations of the multimodal inputs.
Our approach is a light-weighted way of fulfilling cross-modal interaction, where FDT serves as multimodal anchors to capture information from each input with better completeness. This help alleviate the model degradation problem commonly observed in vanilla CLIP models.~Our FDT can be trained with the contrastive learning scheme from scratch without cold-start problems.~Both quantitative and qualitative results demonstrate that FDT representations achieve better cross-modal alignment and performance on various downstream tasks, including image classification, cross-modal retrieval, and VQA. Additionally, the learned FDT capture meaningful cross-modal correspondence, ranging
from objects to actions and attributes.




% \section{FDT Analysis}
% \subsection{Cross-modal Interaction and Alignment}
% % sparsemax
% % 
% \subsection{Representation Degradation}
% \subsection{Limitations}
% % data and model scale
% % not adding fusion

% \section{Conclusions}