%XXXXXX; whizzy chapter -fmt efmt 
\documentclass[12pt,english]{article}
\usepackage[margin=1.5in]{geometry}

\usepackage{mdwlist}
\usepackage{enumerate}
% Special symbols, etc.
\usepackage{amssymb,amsbsy,latexsym}
\usepackage{amsmath}
\usepackage{graphics, subfigure, float}
\usepackage{fp, calc}
\usepackage{hyperref}
\usepackage{url}

% Encoding settings
\usepackage[T1]{fontenc} 
\usepackage{fourier}
\usepackage{bm}

% AMS Math packages

\usepackage{amscd,amsthm}

% Graphics
%\usepackage[dvips]{graphicx,epsfig,color}
%\usepackage{subfigure}
%\usepackage{pstricks}
%\usepackage{pst-node} 
%\usepackage{pst-plot}
%\usepackage{pst-math}
\usepackage{pst-all}
%\usepackage{pst-3dplot}
\usepackage{pstricks-add}
\usepackage{pst-func}
\newpsobject{showgrid}{psgrid}{subgriddiv=1,griddots=10,gridlabels=6pt}
\usepackage{verbatim, comment}
\usepackage{datetime}

\newtheoremstyle{theorem}{1em}{1em}{\slshape}{0pt}{\bfseries}{.}{ }{}
\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}
\newtheorem{question}{Question}
\newtheorem{homework}{Homework}
\newtheorem{conjecture}{Conjecture}
\newtheorem*{theorem*}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem*{corollary*}{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem*{claim*}{Claim}
\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}
\newtheorem*{remark*}{Remark}
\newtheorem{algorithm}{Algorithm}

\def\rem#1{{\marginpar{\raggedright\scriptsize #1}}}

\providecommand{\setN}{\mathbb{N}}
\providecommand{\setZ}{\mathbb{Z}}
\providecommand{\setQ}{\mathbb{Q}}
\providecommand{\setR}{\mathbb{R}}
\newcommand{\E}{\mathop{\mathbb{E}}}
\newcommand{\ME}{\pazocal{E}}
\newcommand{\Var}{\textrm{Var}}
\newcommand{\Vol}{\textrm{Vol}}
\newcommand{\rank}{\textrm{rank}}
\renewcommand{\span}{\textrm{span}}
\newcommand{\inte}{\mathrm{int}}
%\newcommand{\span}{\textrm{span}}
\newcommand{\tr}{\textrm{Tr}}
\newcommand{\nd}{\mathrm{nd}}

        \def\drawRect#1#2#3#4#5{
           \FPeval{\x2}{#2 + #4} 
           \FPeval{\y2}{#3 + #5} 
           \pspolygon[#1](#2,#3)(\x2,#3)(\x2,\y2)(#2,\y2)
        }

% ---------------------------------------------------------------------------------------
% THESE 2 LINES HAVE RECENTLY CREATED PROBLEMS AND THE MESSAGE "Too many math alphabets"
%\usepackage{calrsfs} % now e.g. \pazocal{I} gives a nice I
%\DeclareMathAlphabet{\pazocal}{OMS}{zplm}{m}{n}
%----
% QUICK FIX TO KEEP THE \pazocal COMMAND ALIVE
%\def\pazocal#1{\mathcal{#1}}
\def\pazocal#1{#1}
% ---------------------------------------------------------------------------------------

% Takes care, that preview-latex shows a preview of this environment
\usepackage[displaymath,textmath,graphics, subfigure, floats]{preview} %sections,
\PreviewEnvironment{center} 
\PreviewEnvironment{pspicture} 

\makeatother

\title{The Subspace Flatness Conjecture and Faster Integer Programming}
\author{Victor Reis\thanks{University of Washington, Seattle. Email: {\tt voreis@uw.edu}.} \;\; and \; Thomas Rothvoss\thanks{University of Washington, Seattle. Email: {\tt rothvoss@uw.edu}. Supported by NSF CAREER grant 1651861 and a David \& Lucile Packard Foundation Fellowship.}}
\date{}
%\date{\today}


\begin{document}

\maketitle

\begin{abstract}
  
  In a seminal paper, Kannan and Lov\'asz (1988) considered a quantity $\mu_{KL}(\Lambda,K)$
  which denotes the best volume-based lower bound on the \emph{covering radius} $\mu(\Lambda,K)$ of a convex
  body $K$ with respect to a lattice $\Lambda$. Kannan and Lov\'asz proved that $\mu(\Lambda,K) \leq n \cdot \mu_{KL}(\Lambda,K)$ and the Subspace Flatness Conjecture by Dadush (2012) claims a $O(\log n)$ factor suffices, which would match
  the lower bound from the work of Kannan and Lov\'asz.
  We settle this conjecture up to a constant in the exponent by proving that  $\mu(\Lambda,K) \leq O(\log^{7}(n)) \cdot \mu_{KL} (\Lambda,K)$. Our proof is
  based on the Reverse Minkowski Theorem due to Regev and Stephens-Davidowitz (2017).
  Following the work of Dadush (2012, 2019), we obtain a $(\log n)^{O(n)}$-time randomized algorithm to
  solve integer programs in $n$ variables.
  Another implication of our main result is a near-optimal \emph{flatness constant} of $O(n \log^{8}(n))$.
\end{abstract}

%\tableofcontents

\section{Introduction}

Lattices are fundamental objects studied in various areas of mathematics and computer science.
Here, a \emph{lattice} $\Lambda$ is a discrete subgroup of $\setR^n$. If $B \in \setR^{n \times k}$ is a matrix with linearly independent columns
$b_1,\ldots,b_k$, then we may write a lattice in the form $\Lambda(B) := \{ \sum_{i=1}^k y_ib_i: y_i \in \setZ\}$. In mathematics, lattices are the central object of study in the geometry of numbers with many applications
for example to number theory, see e.g. \cite{KannanLovasz-CoveringMinima-AnnalsOfMath1988}. On the computer science side, lattices found applications for example in lattice-based cryptography~\cite{LWE-RegevJACM09} and cryptanalysis~\cite{KnapsackCryptosystems-Odlyzko1990}. One of the most important
algorithms at least in this area is the \emph{LLL-algorithm} by Lenstra, Lenstra and Lov\'asz~\cite{LLL1982} which finds an approximately orthogonal basis for a given lattice in polynomial time. One of the consequences
of the LLL-reduction is a polynomial time $2^{n/2}$-approximation algorithm for the problem of finding a \emph{shortest vector} in a lattice. We should also mention that
the problem of finding a shortest vector in any norm can be solved in time $2^{O(n)}$ using a variation of the sieving algorithm~\cite{SVP-Sieving-Algo-AKS-STOC2001} while in the Euclidean norm, even
the closest vector to any given target vector can be found in time $2^{O(n)}$~\cite{CVP-Voronoi-Algo-MicciancioVoulgaris-STOC2010}. A more general problem with tremendous applications in combinatorial optimization and operations research is the one of finding an integer point in an arbitrary convex body or polytope.
Lenstra~\cite{IPinFixedDim-Lenstra1983} used the then-recent lattice basis reduction algorithm to solve any $n$-variable integer program in time $2^{O(n^2)}$. This was later improved by Kannan~\cite{n-to-n-algos-for-SVP-CVP-Kannan-MOR1987} to $n^{O(n)}$ and then by Dadush~\cite{DadushThesis2012} to $2^{O(n)}n^n$.

A parameter appearing in the geometry of numbers is the \emph{covering radius}  
\[
  \mu(\Lambda,K) := \min\big\{ r \geq 0 \mid \Lambda + rK = \textrm{span}(\Lambda) \big\}
\]
of a lattice $\Lambda \subseteq \setR^n$ with respect to a compact convex set $K \subseteq \setR^n$ with $\textrm{span}(\Lambda) = \textrm{affine.hull}(K)$. This quantity seems to be substantially harder computationally, in the sense
that the question whether $\mu(\Lambda, K)$ is at least/at most a given threshold seems to neither be in
${\bf NP}$ nor  in ${\bf coNP}$. In terms of approximating $\mu(\Lambda,K)$, one can quickly observe that one has the lower bound
of $\mu(\Lambda,K) \geq (\frac{\det(\Lambda)}{\Vol_n(K)})^{1/n}$, simply  because for $r<(\frac{\det(\Lambda)}{\Vol_n(K)})^{1/n}$, the average density of the translates $\Lambda + rK$ is less than 1.
However, this lower bound may be arbitrarily far off the real covering radius, for example if $\Lambda = \setZ^2$
and $K = [-\frac{1}{M},\frac{1}{M}] \times [-M,M]$ with $M \to \infty$.
On the other hand, for any subspace $W \subseteq \setR^n$ one trivially has $\mu(\Lambda,K) \geq \mu(\Pi_W(\Lambda),\Pi_W(K))$,
where $\Pi_W$ is the orthogonal projection into $W$. Hence, following Kannan and Lov\'asz~\cite{KannanLovasz-CoveringMinima-AnnalsOfMath1988}, one might instead consider the best volume based lower bound for any projection, i.e.
\[
 \mu_{KL}(\Lambda,K) := \max_{\substack{W \subseteq \textrm{span}(\Lambda)\textrm{ subspace} \\ d := \dim(W)}} \Big(\frac{\det(\Pi_W(\Lambda))}{\Vol_d(\Pi_W(K))}\Big)^{1/d}
\]
Kannan and Lov\'asz~\cite{KannanLovasz-CoveringMinima-AnnalsOfMath1988} indeed provide an upper bound of
% Kannan Lovasz (Annals, 1988): Cor 3.11 on page 593
\[
 \mu_{KL}(\Lambda,K) \leq \mu(\Lambda,K) \leq n \cdot \mu_{KL}(\Lambda,K)
\]
On the other hand, they also construct a simplex $K \subseteq \setR^n$ for which $\mu(\setZ^n,K) \geq \Omega(\log(n)) \cdot \mu_{KL}(\setZ^n,K)$ holds.
Dadush~\cite{DadushThesis2012} states the following conjecture, attributing it to Kannan and Lov\'asz~\cite{KannanLovasz-CoveringMinima-AnnalsOfMath1988}: %  The following is attributed to (it may be found explicitly stated in )
\begin{conjecture}[Subspace Flatness Conjecture] \label{conj:KL}% Conjecture 7.4.5 in Dadush's 2012 thesis
  For any full rank lattice $\Lambda \subseteq \setR^n$ and any convex body $K \subseteq \setR^n$ one has
\[
 \mu_{KL}(\Lambda,K) \leq \mu(\Lambda,K) \leq O(\log(n)) \cdot \mu_{KL}(\Lambda,K)
\]
\end{conjecture}
%Intuitivly, the conjecture states that for the covering radis
Dadush also realized the tremendous implications of this conjecture to optimization and showed that it would imply a $O(\log n)^n$-time algorithm to solve $n$-variable integer programs, assuming that the subspace $W$ attaining $\mu_{KL}(\Lambda,K)$ could also be found in the same time.
Later, Dadush and Regev~\cite{TowardsReverseMinkowskiDadushRegevFOCS16} conjectured a \emph{Reverse Minkowski-type Inequality}, which intuitively says that
any lattice without dense sublattices should contain only few short vectors. Among other applications,
they proved that this conjecture (with some logarithmic loss) would
imply Conjecture~\ref{conj:KL} at least for the case that $K$ is an ellipsoid.
%For the longest time, progress on the Subspace Flatness Conjecture seemed elusive, until 
%more recently, Regev and Stephens-Davidowitz proved the following rather deep result: %~\cite{Regev-SD-ReverseMinkowskiTheoremSTOC17} that is the crucial ingredient for our argument is as follows: % have proven the following:
The conjecture of \cite{TowardsReverseMinkowskiDadushRegevFOCS16} was then resolved by Regev and Stephens-Davidowitz~\cite{Regev-SD-ReverseMinkowskiTheoremSTOC17}
with a rather ingenious proof. More precisely, they prove the following: 
%proved the following rather deep result:
\begin{theorem}[Reverse Minkowski Theorem~\cite{Regev-SD-ReverseMinkowskiTheoremSTOC17}] \label{thm:ReverseMinkowskiTheorem}
  Let $\Lambda \subseteq \setR^n$ be a lattice that satisfies $\det(\Lambda') \geq 1$ for all sublattices $\Lambda' \subseteq \Lambda$.
  Then for a large enough constant $C>0$ and $s = C\log(n)$ one has  $
   \rho_{1/s}(\Lambda) \leq \frac{3}{2}
  $.
\end{theorem}
Here, $\rho_t(x) := \exp(-\pi \|x/t\|_2^2)$ is the \emph{discrete Gaussian} for parameter $t>0$.
%Intuitively, the Reverse Minkowski Theorem says that any lattice without dense sublattices has only few short
%vectors. %But using a connection shown earlier by Dadush~\ref{TowardsReverseMinkowskiDadushRegevFOCS16},
% Theorem~\cite{thm:ReverseMinkowskiTheorem} also
% Moreover, Theorem~\ref{thm:ReverseMinkowskiTheorem} also implies that the Subspace Flatness Conjecture holds true whenever $K$ is an ellipsoid (with a somewhat weaker bound of $O(\log^{3/2} n)$).
Also, \cite{Regev-SD-ReverseMinkowskiTheoremSTOC17} tighten the reduction to the Subspace Flatness Conjecture and
show that it holds for any ellipsoid with a factor of  $O(\log^{3/2} n)$.
While for any convex body $K$, there is an ellipsoid $\pazocal{E}$ and a center $c$ so that $c+\pazocal{E} \subseteq K \subseteq c+n\pazocal{E}$~\cite{John1948}, this factor of $n$ is the best possible, and hence there does
not seem to be a blackbox reduction from the general case of Conjecture~\ref{conj:KL} to the one of ellipsoids. % does not
%offer a blackbox improvement for arbitrary convex bodies.



\subsection{Our contribution}

Our main result is as follows:
\begin{theorem} \label{thm:KLConj} 
  For any full rank lattice $\Lambda \subseteq \setR^n$ and any convex body $K \subseteq \setR^n$
  one has
  \[
   \mu_{KL}(\Lambda,K) \leq \mu(\Lambda,K) \leq O(\log^7 (n)) \cdot \mu_{KL}(\Lambda,K).
  \]
\end{theorem}
We will break the proof into two parts that can be found in Sections~\ref{sec:InductiveStep} and~\ref{sec:MainProof}.
Our result is constructive in the following sense: %\rem{V: Should we state this more generally in terms of an arbitrary convex body?}
\begin{theorem} \label{thm:FindingSubspaceIn2ToN} %\rem{T: I think it should be $O(\log^8(n))$ here..?}
  Given a full rank lattice $\Lambda := \Lambda(B)$ and a convex body $K \subseteq \setR^n$ with $c+r_0B_2^n \subseteq K \subseteq r_1 B_2^n$, there is a randomized
  algorithm to find a subspace $W \subseteq \setR^n$ with $d := \dim(W)$ so that
  \[
    \mu(\Lambda,K) \leq O(\log^8 (n)) \cdot \Big(\frac{\det(\Pi_W(\Lambda))}{\Vol_d(\Pi_W(K))}\Big)^{1/d}.
  \]
  The running time of that algorithm is $2^{O(n)}$ times a polynomial in $\log(\frac{1}{r_0})$, $\log(r_1)$ and in the encoding length of $B$.
\end{theorem}
Here, a separation oracle suffices for $K$. See Section~\ref{sec:FindingSubspace} for a proof.
Following the framework layed out by Dadush~\cite{DadushThesis2012}, this implies a faster algorithm for integer programming: 
\begin{theorem} \label{thm:SolvingIPinLogNtoN}
%  Given $A \in \setR^{m \times n}$ and $b \in \setR^m$ as input, there is a randomized algorrithm that
%  with high probability finds a point in $K \cap \setZ^n$ with $K := \{ x \in \setR^n \mid Ax \leq b\}$
%  or correctly decides that there is none. The running time is $O(\log n)^n$ times a polynomial in the
%  encoding length of $A$ and $b$.
  Given a convex body $K \subseteq r B_2^n$ represented by a separation oracle and a lattice $\Lambda = \Lambda(B)$, there is a randomized
  algorithm that with high probability finds a point in $K \cap \Lambda$ or  correctly decides that there is none.
  The running time is  $(\log n)^{O(n)}$ times a polynomial in $\log(r)$ and the encoding length of $B$.
\end{theorem}
The proof can be found in Section~\ref{sec:IP}. For convenience, we rephrase Theorem~\ref{thm:SolvingIPinLogNtoN} without the terminology of a separation oracle: 
\begin{theorem} \label{thm:SolvingExplicitIPinLogNtoN}
  Given $A \in \setQ^{m \times n}$, $b \in \setQ^m$ and $c \in \setR^n$, the integer linear program
  $
  \max\{ c^Tx \mid Ax \leq b, x \in \setZ^n\}
  $
  can be solved in time $(\log n)^{O(n)}$ times a polynomial in the encoding length of $A$, $b$ and $c$.
\end{theorem}
An immediate consequence of our main result (Theorem~\ref{thm:KLConj}) is that $K$ can be replaced by a larger \emph{symmetric} body without decreasing the covering radius significantly:
\begin{theorem} \label{thm:CoveringRadiusKvsKminusK} %\rem{T: Got an upgrade from Cor to Thm}
For any full rank lattice $\Lambda \subseteq \setR^n$ and any convex body $K \subseteq \setR^n$ one has \[\mu(\Lambda,K-K) \leq \mu(\Lambda,K) \leq O(\log^7(n)) \cdot \mu(\Lambda,K-K).\]
\end{theorem}
This in turn implies that the \emph{flatness constant} in dimension $n$ is bounded by $O(n\log^{8}(n))$,
which is an improvement from the previously known bound of $O(n^{4/3} \log^{O(1)} (n))$ obtained by combining the result of Rudelson~\cite{Rudelson1998DistancesBN} with \cite{BanaszczykLitvakPajorSzarekMOR99Flatness}. More precisely,
for any convex body $K \subseteq \setR^n$ with $K \cap \setZ^n = \emptyset$, there is a direction $c \in \setZ^n \setminus \{ \bm{0}\}$ so that at most $O(n \log^{8}(n))$ many hyperplanes of the form $\langle c, x \rangle = \delta$ with $\delta \in \setZ$ intersect $K$. More generally, the following holds:
\begin{theorem} \label{thm:FlatnessConstant}
  For any convex body $K \subseteq \setR^n$ and any full rank lattice $\Lambda \subseteq \setR^n$ one has
  \[
   \mu(\Lambda,K) \cdot \lambda_1(\Lambda^{*}, (K-K)^{\circ}) \leq O(n \log^{8}(n)).
  \]
\end{theorem}
We will prove Theorem~\ref{thm:CoveringRadiusKvsKminusK} and Theorem~\ref{thm:FlatnessConstant} in Section~\ref{sec:Implications}.




\section{Preliminaries}

First, we collect a few auxiliary results for later. We write $A \lesssim B$ if there is a universal constant
$C>0$ so that $A \leq C \cdot B$ holds.

\subsection{Lattices}
%Recall that the \emph{discrete Gaussian} is $\rho_s(x) := \exp(-\pi\|x/s\|_2^2)$ where $s>0$ and $x \in \setR^n$.
%A rather deep result of Regev and Stephens-Davidowitz~\cite{Regev-SD-ReverseMinkowskiTheoremSTOC17} that is the crucial ingredient for our argument is as follows: % have proven the following:
%\begin{theorem}[Reverse Minkowski Theorem~\cite{Regev-SD-ReverseMinkowskiTheoremSTOC17}] \label{thm:ReverseMinkowskiTheorem}
%  Let $\Lambda \subseteq \setR^n$ be a lattice that satisfies $\det(\Lambda') \geq 1$ for all sublattices $\Lambda' \subseteq \Lambda$.
%  Then for a large enough constant $C>0$ and $t = C\log(n)$ one has  $   \rho_{1/t}(\Lambda) \leq \frac{3}{2} $.
%\end{theorem}
For a lattice $\Lambda = \Lambda(B)$ given by a matrix $B \in \setR^{n \times k}$ with linearly independent
columns, we define the \emph{rank} as $\rank(\Lambda) := k = \dim(\textrm{span}(\Lambda))$ and the \emph{determinant}
as $\det(\Lambda) = \sqrt{\det_k(B^TB)}$. A lattice $\Lambda \subseteq \setR^n$ with $\rank(\Lambda)=n$ has \emph{full rank}.
As noted earlier, the \emph{discrete Gaussian} is $\rho_s(x) := \exp(-\pi\|x/s\|_2^2)$ where $s>0$ and $x \in \setR^n$. For a discrete set $S \subseteq \setR^n$ we write
$\rho_s(S) = \sum_{x \in S} \rho_s(x)$. For a lattice $\Lambda \subseteq \setR^n$, we define the
\emph{dual lattice} as $\Lambda^* := \{ x \in \textrm{span}(\Lambda) \mid \left<x,y\right> \in \setZ \; \forall y \in \Lambda \}$.
%The following is a
%standard fact: \rem{V: Should we just cite your lecture notes or is there an older source for this?}
A consequence of the \emph{Poisson Summation Formula} is as follows: 
\begin{lemma} \label{lem:GeneralLatticeShiftApx}
  For any full rank lattice $\Lambda \subseteq \setR^n$, vector $u \in \setR^n$ and any $s>0$ one has
  \[
   |\rho_s(\Lambda + u)-s^n\det(\Lambda^*)| \leq s^n\det(\Lambda^*) \cdot \rho_{1/s}(\Lambda^* \setminus \{\bm{0}\}).
  \]
\end{lemma}
We recommend the excellent notes of Regev~\cite{RegevLectureNotes2009} for background.


\subsection{Stable lattices and the canonical filtration}

A subspace $W \subseteq \setR^n$ is a \emph{lattice subspace} of a lattice $\Lambda \subseteq \setR^n$
if $\textrm{span}(W \cap \Lambda) = W$. Similarly, a sublattice $\Lambda' \subseteq \Lambda$ is called
\emph{primitive} if there is a subspace $W$ with $\Lambda \cap W = \Lambda'$.
For a lattice $\Lambda$ and a primitive sublattice $\Lambda' \subseteq \Lambda$, we define
the \emph{quotient lattice} as $\Lambda / \Lambda' := \Pi_{\textrm{span}(\Lambda')^{\perp}}(\Lambda)$.
A lattice $\Lambda \subseteq \setR^n$ is called \emph{stable}, if $\det(\Lambda)=1$
and $\det(\Lambda') \geq 1$ for all sublattices $\Lambda' \subseteq \Lambda$.
We denote  $\textrm{nd}(\Lambda) := \det(\Lambda)^{1/\textrm{rank}(\Lambda)}$ as the \emph{normalized determinant}.
One can prove that the extreme points of the 2-dimensional
convex hull of the points $\big\{ (\textrm{rank}(\Lambda'), \ln(\det(\Lambda'))) \mid \textrm{sublattice }\Lambda' \subseteq \Lambda \big\}$ correspond to a unique chain of nested sublattices $\{ \bm{0}\} = \Lambda_0 \subset \Lambda_1 \subset \ldots \subset \Lambda_k = \Lambda$. That chain is called the \emph{canonical filtration}.
Moreover, the quotient lattices $\Lambda_i / \Lambda_{i-1}$ are all scalars of a stable lattice and
one can prove that $\textrm{nd}(\Lambda_i / \Lambda_{i-1})$ are strictly increasing in $i$.
We refer to the thesis of~\cite{PhDThesisStephens-Davidowitz2017} for details.

 \iftrue
 \begin{center}
   \psset{unit=0.8cm}
  \begin{pspicture}(0,-0.5)(5,4)
    \cnode*(0,0){2.5pt}{L0}
    \cnode*(1,0.1){2.5pt}{L1}
    \cnode*(2,0.25){2.5pt}{L2}
    \cnode*(4,0.8){2.5pt}{L3}
  %  \cnode*(5,1){2.5pt}{L4}
    \cnode*(6,2){2.5pt}{L5}
    \pspolygon[fillstyle=solid,fillcolor=lightgray,linestyle=none](0,2)(L0)(L1)(L2)(L3)(L5)
    \psline(0,2)(L0)(L1)(L2)(L3)(L5)
    \psaxes[ticks=none,labels=none]{->}(0,0)(0,-0.5)(6,3.5) \rput[c](6,8pt){$\textrm{rank}(\Lambda')$}
    \psdots(L0)(L1)(L2)(L3)(L5)
    \rput[l](5pt,3.5){$\ln(\det(\Lambda'))$}
    \nput{180}{L0}{$\Lambda_0 = \{ \bm{0}\}$}
%    \nput{-90}{L1}{$\Lambda_1$}
    \nput{-90}{L2}{$\Lambda_{i-1}$}
    \nput{-60}{L3}{$\Lambda_i$}
    \nput{0}{L5}{$\Lambda_k=\Lambda$}
    %\psplot[algebraic=true,linestyle=dashed,linecolor=blue]{0}{6}{0.275*x-0.3}% slope: (0.8-0.25)/2=0.275
  \end{pspicture}
\end{center}
\fi

It will be useful to replace the canonical filtration by an \emph{approximate} filtration where
the normalized determinants grow exponentially. 
% quotient lattices of similar normalized determinant are grouped together.
We make the following definition:
\begin{definition}
  We call a lattice $\Lambda \subseteq \setR^n$ \emph{$t$-stable} with $t \geq 1$ if
  the following holds:
  \begin{enumerate}
  \item[(I)] For any sublattice $\tilde{\Lambda}  \subseteq \Lambda$ one has  $\nd(\tilde{\Lambda}) \geq t^{-1}$.
  \item[(II)] For any sublattice $\tilde{\Lambda}  \subseteq \Lambda^*$ one has   $\nd(\tilde{\Lambda}) \geq t^{-1}$.
  \end{enumerate}
\end{definition}
Note that a lattice is 1-stable if and only if it is stable. We can similarly define $t$-stable filtrations:
\begin{definition}
  Given a lattice $\Lambda \subseteq \setR^n$, we call a sequence $\{ \bm{0}\} = \Lambda_0 \subset \ldots \subset \Lambda_{k} = \Lambda$ a \emph{$t$-stable filtration} of $\Lambda$ if the following holds:
  \begin{enumerate}
  \item[(a)] The normalized determinants $r_i := \nd(\Lambda_i/\Lambda_{i-1})$ satisfy
    $r_1 < \ldots < r_{k}$.
  \item[(b)] The lattices $\frac{1}{r_i} (\Lambda_i / \Lambda_{i-1})$ are $t$-stable for all $i=1,\ldots,k$.
  \end{enumerate}
  We call a $t$-stable filtration \emph{well-separated} if additionally the following holds: 
  \begin{enumerate}
  \item[(c)] One has $r_i \leq \frac{1}{2} r_{i+2}$ for all $i=1,\ldots,k-2$.
  \end{enumerate}
%  we say that the sequence is a .
\end{definition}

For example, the canonical filtration is $1$-stable. It turns out we can make any $t$-stable filtration well-separated:
%We will need a version of $t$-stable filtrations where the normalized determinants are decaying exponentially:

  %The notion is also symmetric:
%\begin{lemma}
%A lattice $\Lambda$ is $t$-stable if and only if $\Lambda^*$ is $t$-stable.
%\end{lemma}
%Now we come to the construction of an approximate filtration:
\begin{theorem}\label{thm:ApproxFilt}
% \rem{T: I reverted $\tilde{\Lambda}_i \to \Lambda_i$. I think it's more important that the statement is clean than the proof (also the proof is deferred to the appendix).} %\label{thm:ApproximateFiltration}
 % There is a universal constant $C>0$ so that the following is true:
  Given a lattice $\Lambda \subseteq \setR^n$ and a $t$-stable filtration $\{ \bm{0}\} = \Lambda_0 \subset \ldots \subset \Lambda_{k} = \Lambda$, in polynomial time we can compute an increasing subsequence of indices $0 = \ell(0) < \ldots < \ell(\tilde{k}) = k$ so that
    \begin{enumerate}
  \item[(a')] For $\tilde{r}_i := \nd(\Lambda_{\ell(i)} / \Lambda_{\ell(i-1)})$ one has $\tilde{r}_1 < \ldots < \tilde{r}_{\tilde{k}}$ and $\tilde{r}_i \leq \frac{1}{2} \tilde{r}_{i+2}$ for all $i$.
  \item[(b')] The lattices $\frac{1}{\tilde{r}_i} (\Lambda_{\ell(i)}/\Lambda_{\ell(i-1)})$ are $2t$-stable for all $i = 1,\ldots,\tilde{k}$.
    \end{enumerate}
\end{theorem}
We defer the proof to Appendix~\ref{appendix:ApproximateFiltration}. Using the canonical filtration
as input to Theorem~\ref{thm:ApproxFilt} yields:

 \begin{corollary} \label{cor:ExistenceTwoStableWellSeparatedFiltration}
   For any lattice $\Lambda \subseteq \setR^n$, there exists a 2-stable well-separated filtration $\{ \bm{0}\} = \Lambda_0 \subset \ldots \subset \Lambda_{k} = \Lambda$.
 % For any lattice $\Lambda \subseteq \setR^n$, there exists a sequence $\{ \bm{0}\} = \Lambda_0 \subset \ldots \subset \Lambda_{k} = \Lambda$
 %  so that
 %  \begin{enumerate}
 %  \item[(a)] The normalized determinants $r_i := \det(\Lambda_i/\Lambda_{i-1})^{1/\textrm{rank}(\Lambda_i/\Lambda_{i-1})}$ satisfy
 %    $r_1 < r_2 < \ldots < r_{k}$ and $r_i \leq \frac{1}{2} r_{i+2}$ for all $i=1,\ldots,k-2$.
 %  \item[(b)] The lattices $\frac{1}{r_i} (\Lambda_i / \Lambda_{i-1})$ are $2$-stable for all $i=1,\ldots,k$.
 %  \end{enumerate}
\end{corollary}

%The proof that we give here is slightly incomplete, but presumably it could be made formal:




We collect a few more properties of $t$-stable lattices: %\rem{T: Will this be needed with general $t$-stable?}
\begin{lemma} \label{lem:PropertiesOf2StableLattices} %[Reverse Minkowski Theorem] \label{thm:ReverseMinkowskiTheorem}
  Let $\Lambda$ be a $t$-stable lattice. Then for $s = C\log(n)$ with $C>0$ large enough the following holds: 
  \begin{enumerate}
  \item[(a)] $\Lambda^*$ is $t$-stable.
  \item[(b)]   $\rho_{1/(st)}(\Lambda) \leq \frac{3}{2}$. % and   $\rho_{1/t}(\Lambda^*) \leq \frac{3}{2}$.
  \item[(c)] For any $u \in \setR^n$ one has $\frac{\rho_{st}(\Lambda + u)}{\rho_{st}(\Lambda)} \geq \frac{1}{3}$.
  \end{enumerate}
\end{lemma}
\begin{proof}
  (a) is immediate from the definition of $t$-stability. Next, let $s = C\log(n)$ be the parameter from
  Theorem~\ref{thm:ReverseMinkowskiTheorem}.
  For (b), 
  we can see that for any $\Lambda' \subseteq t\Lambda$ one has $\det(\Lambda') \geq 1$
  and so the Reverse Minkowski Theorem (Theorem~\ref{thm:ReverseMinkowskiTheorem}) applies to
  the lattice $t\Lambda$. Then  $\rho_{1/(st)}(\Lambda) = \rho_{1/s}(t\Lambda) \leq \frac{3}{2}$ which gives $(b)$.
  For (c), applying Lemma~\ref{lem:GeneralLatticeShiftApx} twice gives
  \[
   \frac{\rho_{st}(\Lambda + u)}{\rho_{st}(\Lambda)} \geq \frac{(st)^n \det(\Lambda^*) \cdot (1-\rho_{1/(st)}(\Lambda^* \setminus \{\bm{0}\}))}{(st)^n \det(\Lambda^*) \cdot (1+\rho_{1/(st)}(\Lambda^* \setminus \{ \bm{0}\}))} \stackrel{(a)+(b)}{\geq} \frac{1-\frac{1}{2}}{1+\frac{1}{2}} = \frac{1}{3}. \qedhere
  \]
\end{proof}




\subsection{The $\ell$-value and volume estimates}

We review a few results from convex geometry that can all be found in the textbook
by Artstein-Avidan, Giannopoulos and Milman~\cite{AsymptoticGeometricAnalysis-Book2015}.
A set $K \subseteq \setR^n$ is called a \emph{convex body} if it is convex, compact (i.e. bounded and closed) and
has a non-empty interior $\textrm{int}(K)$. We denote $B_2^n := \{ x \in \setR^n \mid \|x\|_2 \leq 1\}$
and $S^{n-1} := \{ x \in \setR^n \mid \|x\|_2=1\}$ as the Euclidean ball and sphere, resp.
Let $\nu_n := \Vol_n(B_2^n)$. The \emph{relative interior}
of $K$ is $\textrm{rel.int}(K) := \{ x \in K \mid \exists \varepsilon > 0: (x + \varepsilon \cdot B_2^n) \cap \textrm{affine.hull}(K) \subseteq K \}$.
A set $Q$ is called \emph{symmetric} if $-Q=Q$.
For a symmetric convex set $Q$, the norm $\|x\|_Q$ is defined as the least scaling $r \ge 0$ so that $x \in rQ$.
We define the \emph{width} of a convex body $K$ as $w(K) := \E_{\theta \sim S^{n-1}}[\max\{ \left<\theta,x-y\right> : x,y \in K\}]$.
For a compact convex  $K \subseteq \setR^n$ with $\bm{0} \in \textrm{rel.int}(K)$ we denote its \emph{polar} by $K^\circ := \{ y \in \textrm{span}(K) : \langle x, y \rangle \le 1 \ \forall x \in K\}$.
Recall the following basic facts.
\begin{lemma}[Properties of polarity] \label{lem:PropertiesOfPolarity}
  For two convex bodies $K,Q \subseteq \setR^n$ with $\bm{0} \in \textrm{int}(K)$ and $\bm{0} \in \textrm{int}(Q)$ the following holds: 
  \begin{enumerate}
  \item[(a)] One has $(K^{\circ})^{\circ} = K$.
 \item[(b)] For any subspace $F \subseteq \setR^n$ one has $\Pi_F(K)^{\circ} = K^{\circ} \cap F$.
 \item[(c)] One has $(K \cap Q)^{\circ} = \textrm{conv}(K^{\circ} \cup Q^{\circ})$.
 \item[(d)] One has $(-K)^{\circ} = -K^{\circ}$.
 \end{enumerate}
\end{lemma}
We write $N(0,I_n)$ as the standard Gaussian distribution on $\setR^n$.  
The \emph{$\ell$-value} of a compact symmetric convex set $Q$ is $ \ell_Q = \E_{x \sim N(\bm{0},I_{\textrm{span}(Q)})}[\|x\|_Q^2]^{1/2}$. Note that for any symmetric convex body $Q$ one has $\ell_Q = \Theta(\sqrt{n}) \cdot w(Q^{\circ})$. 
%\rem{T: Maybe let's use $K$ for (non-symmetric bodies and $Q$ for symmetric. Also it seems we only need $\ell_Q$ only for symmetric bodies. So I am restricting some lemma to the symmetric case then. It might be confusing for some readers if  $\| \cdot \|_Q$ happens to not be a norm.}
%We know the following:
One of the most powerful tools in convex geometry is the fact that any symmetric convex body can be linearly transformed
so that in terms of the average width, it comes within a logarithmic factor of the Euclidean ball:
\begin{theorem}[Figiel, Tomczak-Jaegerman, Pisier] \label{thm:PisierRescaling}
For any symmetric convex body $Q \subseteq \setR^n$, there is an invertible linear map $T : \setR^n \to \setR^n$ so that $\ell_{T(Q)} \cdot \ell_{(T(Q))^{\circ}} \leq O(n\log n)$. 
\end{theorem}
%So the map $A$ in Theorem~\ref{thm:PisierRescaling} also satisfies $\ell_{A(K)} \cdot \ell_{A(K)^{\circ}} \leq O(n \log n)$.
%Recall that $\ell_{B_2^n} = \Theta(\sqrt{n})$ and $\ell_{\sqrt{n}B_2^n} = \Theta(1)$.
We state two estimates that will be crucial for our later arguments:
\begin{lemma} \label{lem:MonotonicityLValue}
  Let $Q \subseteq \setR^n$ be a symmetric convex body. Then for any subspace $U \subseteq \setR^n$, one has $\ell_{Q \cap U} \leq \ell_Q$.
\end{lemma}
\begin{proof}
  Indeed, one has
\[
  \ell^2_Q = \mathbb{E}_{z \sim N(\bm{0}, I_U)}\big[ \mathbb{E}_{y \sim N(\bm{0}, I_{U^\perp})} [\|z + y\|_Q^2]\big] \geq \mathbb{E}_{z \sim N(\bm{0}, I_U)} \big[\big\| z + \underbrace{\mathbb{E}_{y \sim N(\bm{0}, I_{U^\perp})} [y]}_{=\bm{0}}\big\|_Q^2\big] = \ell^2_{Q \cap U},
\]
where the inequality follows from Jensen's inequality and the convexity of $y \mapsto \|z + y\|_Q^2$.
\end{proof}

\begin{lemma} \label{lem:LvalueOfProjectedIntersection} 
  Let $Q \subseteq \setR^n$ be a symmetric convex body. For any subspaces $V \subset W \subseteq \setR^n$,  %with $d := \dim(W)-\dim(V)$
  one has $\ell_{\Pi_{V^{\perp}}(Q \cap W)} \le \ell_Q$.
\end{lemma}
\begin{proof}
  We have  $\ell_{\Pi_{V^{\perp}}(Q \cap W)} \le \ell_{Q \cap W \cap V^{\perp}} \le \ell_Q$
  using that $\Pi_{V^{\perp}}(Q \cap W) \supseteq Q \cap W \cap V^{\perp}$ and using Lemma~\ref{lem:MonotonicityLValue}. %
%First we show that for any subspace $U \subseteq \setR^n$ of positive dimension one has $\ell_{K \cap U} \le \ell_K$. 
\end{proof}

The following classical result says that among all bodies with identical volume, the Euclidean ball minimizes the width. %, see e.g. \cite{AsymptoticGeometricAnalysis-Book2015}.
\begin{theorem}[Urysohn Inequality] \label{thm:UrysohnInequality}
  For any convex body $K \subseteq \setR^n$ one has
  \[
    w(K) \geq 2 \cdot \Big(\frac{\Vol_n(K)}{\Vol_n(B_2^n)}\Big)^{1/n}.
  \]
\end{theorem}


% Another auxiliary result that we will need is a bound on the volume of projections. A dual version that will be useful is the following:
% {}
% \begin{theorem}[Theorem 1 in~\cite{Rudelson1998SectionsOT}]\label{thm:RudelsonSection}
% Let $K \subseteq \setR^n$ be a convex body and $W \subseteq \setR^n$ be a $d$-dimensional subspace. Then
% \[
% \Vol_d((K-K) \cap W)^{1/d} \lesssim \min\Big\{\frac{n}{d}, \sqrt{d}\Big\} \cdot \max_{\bm{x} \in \setR^n} \Vol_d ((K+\bm{x}) \cap W)^{1/d}.
% \]
% \end{theorem}
% In order to relate volumes of sections and projections, we use polarity. The Blaschke-Santal\'o inequality and its reverse will be useful: 

% \begin{theorem}\label{lem:Blaschke-Santalo-Inequality}
% For any convex body $K \subseteq \mathbb{R}^n$ there exist constants $C_1$ and $C_2$ so that
% \[C_1 \le \min_{\bm{x} \in \setR^n} \Big(\frac{\Vol_n(K) \cdot \Vol_n((K-\bm{x})^\circ)}{\Vol_n(B^n_2)}\Big)^{1/n} \le C_2. \]
% \end{theorem}
%Moreover, $\Pi_F$ denotes the orthogonal projection into a subspace $F$.

We also require the following volume estimate. The proof is somewhat involved and we defer it to Appendix~\ref{appendix:VolumeOfProjections}. Roughly speaking, Proposition~\ref{prop:VolumeOfProjectionVsSymmetrizer} is a polar version of Rudelson's inequality on sections of the difference body~\cite{Rudelson1998SectionsOT}.
Here $\textrm{bary}(K) := \frac{1}{\Vol_n(K)}\int_K x \; dx$ is the \emph{barycenter} or \emph{centroid} of a convex body $K$.
\begin{proposition} \label{prop:VolumeOfProjectionVsSymmetrizer}
Let $K \subseteq \setR^n$ be a convex body so that $\textrm{bary}(K^{\circ}) = \bm{0}$ and let $F \subseteq \setR^n$ be a $d$-dimensional subspace. 
Then 
\[
 \Vol_d(\Pi_F(K))^{1/d} \lesssim \Big(\frac{n}{d}\Big)^3 \cdot \Vol_d(\Pi_F(K \cap -K))^{1/d}.
\]
\end{proposition}

%\rem{T: Source for Lem~\ref{lem:TranslateSoThatPolarIsCentered}}
It may not be obvious, but one can translate any convex body so that the polar has its barycenter at the origin. See for example Chapter 10.5 in the textbook of Schneider~\cite{schneider_2013}.
\begin{lemma} \label{lem:TranslateSoThatPolarIsCentered}
For any convex body $K \subseteq \setR^n$ there is a unique point $s(K) \in \textrm{int}(K)$ (known as the \emph{Santal\'o point}) so that $\textrm{bary}( (K-s(K))^{\circ}) = \bm{0}$.
\end{lemma}
% Then the main technical lemma is the following:

% \begin{lemma} \label{lem:VolumeOfProjection}
%   Let $K \subseteq \setR^n$ be a convex body with $\bm{0} \in \inte(K)$ and  $\ell_{K \cap -K} \cdot \ell_{(K \cap -K)^{\circ}} \leq O(n \log n)$ and $\ell_{K \cap -K} = \sqrt{n}$. Fix a subspace $W \subseteq \setR^n$ with $d := \dim(W)$. Then one has \\

%   \noindent (a) if $K$ is symmetric then $\Vol_d(\Pi_W(K))^{1/d} \leq O(\log(n)) \cdot \frac{\sqrt{n}}{d}$. \\
%   (b) $\Vol_d(\Pi_W(K))^{1/d} \leq \min \{\frac{n}{d}, \sqrt{d}\} \cdot O(\log(n)) \cdot \frac{\sqrt{n}}{d}$.
% \end{lemma}
% \begin{proof}
% First we prove (a), with $K = K \cap -K$. We apply Urysohn's inequality, so that
% \[
% \Big(\frac{\Vol_d(\Pi_W(K))}{\Vol_d(B^W_2)}\Big)^{1/d} \le w(\Pi_W(K)) \lesssim \frac{1}{\sqrt{d}} \ell_{(\Pi_W(K))^\circ} = \frac{1}{\sqrt{d}} \ell_{K^\circ \cap W} \stackrel{\textrm{Lem}~\ref{lem:LvalueOfProjectedIntersection}}{\leq} \frac{1}{\sqrt{d}} \ell_{K^\circ} \lesssim \frac{\sqrt{n} \log n}{\sqrt{d}}.
% \]
% It remains to recall that $\Vol_d(B^W_2)^{1/d} \lesssim \frac{1}{\sqrt{d}}$.
% For an incomplete proof of (b), we use the same argument, and it remains to argue that
% \[
% \Vol_d(\Pi_W(K))^{1/d} \lesssim \min \Big\{\frac{n}{d} , \sqrt{d}\Big\} \cdot \Vol_d(\Pi_W (K \cap -K))^{1/d}.
% \]
% To compute polars, note for a convex $Q$ with $\bm{0} \in \inte(Q)$ we have $\|\bm{x}\|_{Q^\circ} = \max_{y \in Q} \langle \bm{x} , \bm{y} \rangle$. Therefore $(K \cap -K)^\circ = \mathrm{conv}\{K^\circ, -K^\circ\}$ and $\Pi_W(K)^\circ = K^\circ \cap W$. Further, for any convex $Q$ with $\bm{0} \in \inte(Q)$,
% \[\frac{1}{2} (Q-Q) \subseteq  \mathrm{conv}\{Q, -Q\} \subseteq Q-Q\] 

% Thus by Theorem~\ref{lem:Blaschke-Santalo-Inequality} it's equivalent to show when $\bm{0} \in \inte(K)$, 

% \[\Vol_d((K^\circ-K^\circ) \cap W)^{1/d} \lesssim  \min \Big\{\frac{n}{d} , \sqrt{d}\Big\} \cdot \min_{\bm{x} \in \setR^n} \Vol_d ((K+\bm{x})^\circ \cap W)^{1/d}.\]

% What we do know is that for some $\bm{x} \in \setR^n$,

% \[\Vol_d((K^\circ-K^\circ) \cap W)^{1/d} \lesssim  \min \Big\{\frac{n}{d} , \sqrt{d}\Big\} \cdot \Vol_d ((K^\circ + \bm{x}) \cap W)^{1/d}.\]

% In particular this allows us to get a bound on the volume of $\Pi_W((K^\circ + \bm{x})^\circ)$ for some $\bm{x} \in \setR^n$ instead of $\Pi_W (K)$, which doesn't seem as helpful.
% \end{proof}

%\begin{remark}
%  Actually we only need Lemma~\ref{lem:VolumeOfProjection} if either $d \geq \Omega(\frac{n}{\log(n)})$,
%  or if $d$ is arbitrary but then we have a slack of say $n^{100}$.
%\end{remark}


\subsection{Properties of the covering radius}%$

%For two orthogonal lattices $\Lambda_1,\Lambda_2$, the covering radius with respect to the
%Euclidean norm satisfies $\mu(\Lambda_1 \oplus \Lambda_2)^2 = \mu(\Lambda_1)^2 +\mu(\Lambda_2)^2$.
%For an arbitrary norm $\| \cdot \|_K$, we can still recover a triangle-type inequality.
While the set $K$ may not be symmetric, the sets $\Lambda$ and $\setR^n$ are symmetric, which implies
the following:
\begin{lemma}[Properties of the covering radius] \label{lem:PropertiesOfCoveringRadius}
  For any lattice $\Lambda \subseteq \setR^n$ and any compact convex set $K \subseteq \setR^n$ with $\textrm{span}(\Lambda) = \textrm{affine.hull}(K)$. Then
  \begin{enumerate}
  \item[(a)] $\mu(\Lambda,K) = \mu(\Lambda,K+u)$ for all $u \in \mathrm{span}(\Lambda)$.
  \item[(b)] $\mu(\Lambda,K) = \min\{ r \geq 0 \mid (x + rK) \cap \Lambda \neq \emptyset \; \forall x \in \mathrm{span}(\Lambda)\}$.
  \end{enumerate}
\end{lemma}


We need a triangle inequality for the covering radius: 
\begin{lemma} \label{lem:TriangleIneqGenCoveringRadius}
  Let $\Lambda \subseteq \setR^n$ be a lattice and let $\Lambda' \subseteq \Lambda$ be a primitive sublattice.
  Then for any compact convex set $K \subseteq \setR^n$ with $\bm{0} \in \textrm{rel.int}(K)$ and $\textrm{span}(\Lambda)=\textrm{span}(K)$ one has
  \[
    \mu(\Lambda,K) \leq  \mu(\Lambda',K \cap W) + \mu(\Lambda / \Lambda',\Pi_{W^{\perp}}(K)), 
  \]
  where $W := \textrm{span}(\Lambda')$. %Moreover, $\mu(\Lambda/\Lambda',\Pi_{W^{\perp}}(K)) \leq \mu(\Lambda,K)$.
\end{lemma}
\begin{proof}
  W.l.o.g. we may assume that $\Lambda$ has full rank, so $\bm{0} \in \textrm{int}(K)$.
  Following the characterization in Lemma~\ref{lem:PropertiesOfCoveringRadius}.(b), we fix an $x \in \setR^n$.
  For $r_1 := \mu(\Pi_{W^{\perp}}(\Lambda),\Pi_{W^{\perp}}(K))$
  we know that  $\Pi_{W^{\perp}}(x + r_1K) \cap \Pi_{W^{\perp}}(\Lambda) \neq \emptyset$. That means there is a $u_1 \in r_1K$ and a lattice point $y \in \Lambda$   so that $\Pi_{W^{\perp}}(x+u_1) = \Pi_{W^{\perp}}(y)$.
  Next, for $r_2 := \mu(\Lambda \cap W, K \cap W)$ we know that $(x+u_1-y + r_2 \cdot (K \cap W)) \cap (\Lambda \cap W) \neq \emptyset$
  which is equivalent to $(x+u_1 + r_2 \cdot (K \cap W)) \cap (y+(\Lambda \cap W)) \neq \emptyset$.
  Let $u_2 \in r_2 \cdot (K \cap W)$ be the vector so that $x+u_1+u_2 \in \Lambda$. Then $u_1+u_2 \in (r_1+r_2)K$
  by convexity, so $(x + (r_1+r_2) \cdot K) \cap \Lambda \neq \emptyset$.
  \iftrue
\begin{center} %\rem{T: We'll remove the picture later. For now it helps me check the argument.}
\psset{unit=1.6cm}
 \begin{pspicture}(-2,-1)(3,2.5)
    \psline[linewidth=2pt,linecolor=gray](-1.5,0)(2.5,0)\rput[c](2.5,5pt){$W$}
    \psline[linewidth=2pt,linecolor=gray](0,-1)(0,2.5)\rput[l](5pt,2.5){$W^{\perp}$}
    \psline[linestyle=dotted](-1.5,1)(2.5,1)
    \psline[linewidth=2.5pt,linecolor=darkgray](0,1)(0,1.9)\rput[l](3pt,1.7){$r_1\Pi_{W^{\perp}}(K)$}
    \rput[c](1.8,1.4){\pspolygon[fillstyle=solid,fillcolor=lightgray,linecolor=darkgray](0.4,-0.4)(-0.1,0.5)(-0.2,-0.2)}
 %   \rput{-45}(1.8,1.4){\psellipse[fillstyle=solid,fillcolor=lightgray](0,0)(0.55,0.15)}
   \rput[c](0,2){\psdots(-1,0)(0,0)(1,0)(2,0)}
    \psdots(-1,0)(0,0)(1,0)(2,0)
   \psdots(-1.5,1)(-0.5,1)(0.5,1)(1.5,1)(2.5,1)
   \rput[c](0,-2){\psdots(-1.5,1)(-0.5,1)(0.5,1)(1.5,1)(2.5,1)}
%   \psaxes[arrowsize=5pt]{->}(0,0)(-1.5,-1)(2.5,2.5) %\rput[c](2.5,5pt){$$}
   \psline[linestyle=dotted](1.8,1.4)(1.8,0) \rput[c](1.8,-10pt){$\Pi_{W}(x)$}
   \psline[linestyle=dotted](1.8,1.4)(0,1.4) \rput[r](-5pt,1.4){$\Pi_{W^{\perp}}(x)$}
   \cnode[fillstyle=solid,fillcolor=white](1.8,1.4){3pt}{x}\nput[labelsep=2pt]{120}{x}{$x$}
   \multido{\N=-1+1}{4}{\cnode[fillstyle=solid,fillcolor=white](0,\N){3pt}{A}}
   \cnode*(0.5,1){2.5pt}{y2} \nput[labelsep=2pt]{90}{y2}{$y$}
   \cnode[fillstyle=solid,fillcolor=white](0,1){3pt}{p1} \nput[labelsep=2pt]{-135}{p1}{$\Pi_{W^{\perp}}(y)$}
   \pnode(2.2,1){A2}
   \ncline[arrowsize=5pt]{->}{x}{A2}\naput[labelsep=2pt]{$u_1$}
   \cnode*(2.5,1){3pt}{p2}
   \ncline[arrowsize=5pt]{->}{A2}{p2}\nbput[labelsep=2pt]{$u_2$}
   \rput[l](1.9,1.7){$x+r_1K$} % with $r = \mu(\Pi_{W^{\perp}}(\Lambda),\Pi_{W^{\perp}}(K))$}
  \end{pspicture}
\end{center}
\fi
%%%%%%%%
\end{proof}
%\begin{proof} \rem{T: This is needed for non-symmetric convex sets. Proof needs generalization.}
%  Consider any $\bm{x} \in \textrm{span}(\Lambda)$. Let $\tilde{\bm{y}} \in \Pi_{W^{\perp}}(\Lambda)=\Lambda/\Lambda'$ be the vector so that
%  \[
%   \|\Pi_{W^{\perp}}(\bm{x}) - \tilde{\bm{y}}\|_{\Pi_{W^{\perp}}(K)} \leq \mu(\Pi_{W^{\perp}}(\Lambda),\Pi_{W^{\perp}}(K)) %\leq \mu(\Lambda / \Lambda',K)
%  \]
%  Let $\bm{y} \in \Lambda $ be a lattice vector with $\Pi_W(\bm{y}) = \tilde{\bm{y}}$.
%  By property of projection, there is a vector $\bm{a}$ so that $\bm{x}+\bm{a} \in \bm{y} + W$ and $\|\bm{a}\|_K = \|\Pi_{W^{\perp}}(\bm{y})-\Pi_{W^{\perp}}(\bm{x}) \|_{\Pi_{W^{\perp}}(K)} \leq \mu(\Lambda / \Lambda',\Pi_{W^{\perp}}(K))$.
%  Then there is a vector $\bm{b}$ so that $\bm{x}+\bm{a}+\bm{b} \in \bm{y}+\Lambda' \subseteq \Lambda$ with $\|\bm{b}\|_K \leq \mu(\Lambda',K)$.
%  Note that $\mu(\Lambda',K) = \mu(\Lambda',K \cap W)$ and $\|\bm{a}+\bm{b}\|_K \leq \|\bm{a}\|_K+\|\bm{b}\|_K \leq \mu(\Lambda/\Lambda',\Pi_{W^{\perp}}(K)) + \mu(\Lambda',K \cap W)$ by the triangle inequality.
%As the projection of a covering is again a covering, one also has  $\mu(\Pi_{W^{\perp}}(\Lambda),\Pi_{W^{\perp}}(K)) \leq \mu(\Lambda,K)$. % already in the proof of Lemma~\ref{lem:MuAtLeastMuKL} as projections cannot increase the covering radius. 
%\end{proof}


The natural extension of Lemma~\ref{lem:TriangleIneqGenCoveringRadius} to a filtration is as follows: 
\begin{lemma} \label{lem:TriangleIneqForFiltration}
  Let $\Lambda \subseteq \setR^n$ be a lattice with any sequence of sublattices
  $\{ \bm{0} \} = \Lambda_0 \subset \Lambda_1 \subset \ldots \subset \Lambda_k= \Lambda$. Then for any compact convex set $K \subseteq \setR^n$ with $\bm{0} \in \textrm{rel.int}(K)$ and $\textrm{span}(\Lambda)=\textrm{span}(K)$, one has
  \[
  \mu(\Lambda,K) \leq \sum_{i=1}^k \mu\left( \Lambda_i/\Lambda_{i-1}, \Pi_{\textrm{span}(\Lambda_{i-1})^{\perp}}(K \cap \textrm{span}(\Lambda_i)) \right).
  \]
\end{lemma}
\begin{proof}
We can use the previous lemma to show by induction over $i_0=k,k-1,\ldots,1$ that % for any $i_0 \in [k]$,
\[\mu(\Lambda,K) \leq \mu(\Lambda_{i_0 - 1}, K \cap \textrm{span}(\Lambda_{i_0-1})) + \sum_{i=i_0}^k \mu\left( \Lambda_i/\Lambda_{i-1}, \Pi_{\textrm{span}(\Lambda_{i-1})^{\perp}}(K \cap \textrm{span}(\Lambda_i)) \right). \]
Indeed, for $i_0 = k$ this is exactly Lemma~\ref{lem:TriangleIneqGenCoveringRadius}. If it holds for some $i_0 > 1$, then
\begin{eqnarray*}
  \mu(\Lambda_{i_0 - 1}, K \cap \textrm{span}(\Lambda_{i_0-1})) &\le& \mu(\Lambda_{i_0 - 2}, K \cap \textrm{span}(\Lambda_{i_0-2})) + \\
                                                                & & \mu\left( \Lambda_{i_0-1}/\Lambda_{i_0-2}, \Pi_{\textrm{span}(\Lambda_{i_0-2})^{\perp}}(K \cap \textrm{span}(\Lambda_{i_0 -1})) \right), 
\end{eqnarray*}
since $\textrm{span}(\Lambda_{i_0-2}) \subset \textrm{span}(\Lambda_{i_0-1})$. So the claim follows by induction, and taking $i_0 := 1$ yields the statement.
\end{proof}


\subsection{Properties of $\mu_{KL}$}
We also need the following fact: 
\begin{lemma} \label{lem:MonotonicityOfMuKL}
  For any lattice $\Lambda \subseteq \setR^n$, compact convex set $K$ with $\textrm{span}(\Lambda) = \textrm{affine.hull}(K)$
  and subspace $V \subseteq \textrm{span}(\Lambda)$ one has
  $\mu_{KL}(\Pi_{V}(\Lambda),\Pi_{V}(K)) \leq \mu_{KL}(\Lambda,K)$.
\end{lemma}
\begin{proof}
  Let $W \subseteq V$ be the subspace attaining the left side with $\dim W = d$. Then
  \[
 \mu_{KL}(\Pi_{V}(\Lambda),\Pi_{V}(K)) = \Big(\frac{\det(\Pi_W(\Pi_V(\Lambda)))}{\Vol_d(\Pi_W(\Pi_V(K)))}\Big)^{1/d} = \Big(\frac{\det(\Pi_W(\Lambda))}{\Vol_d(\Pi_W(K))}\Big)^{1/d} \leq \mu_{KL}(\Lambda,K)
  \]
  using that $\Pi_W(\Pi_V(x)) = \Pi_W(x)$ for all $x \in \setR^n$ as  $W \subseteq V$.
\end{proof}


\subsection{Approximate stable lattices and the covering radius}

%\begin{definition}
%We say that a symmetric convex body $K$ is in \emph{$\ell$-position} if $w(K) = 1$ and $w(K^{\circ}) \leq O(\log n)$.  
% \end{definition}
Using the Reverse Minkowski Theorem it would not be hard to prove that for any stable lattice $\Lambda \subseteq \setR^n$ one has $\mu(\Lambda,B_2^n) \leq O(\sqrt{n} \log(n))$. In this section, we show how to generalize this
to  $t$-stable lattices and to general symmetric convex bodies.
For a symmetric convex body $Q$, we consider the following quantity 
\[
 \beta(Q) = \sup_{\Lambda \subseteq \setR^n\textrm{ lattice}} \sup_{u \in \setR^n} \frac{\rho_1((u + \Lambda) \setminus Q)}{\rho_1(\Lambda)}
\]
Note that always $0 < \beta(Q) \leq 1$. Intuitively, a body $Q$ with $\beta(Q) \ll 1$ is large enough that
for any lattice a substantial fraction of the discrete Gaussian weight has to fall in $Q$.
% We know the following standard estimate:
As part of the celebrated Transference Theorem, Banaszczyk showed how to relate the $\ell$-value of a body to its $\beta$-value: %the following as part of his celebrated transference theorem:
\begin{lemma}[Banaszczyk~\cite{Banaszczyk1996TransferenceTheoremsForGeneralConvexBodies}] \label{lem:BanaszczykLKvsBetaK}
  For any $\varepsilon>0$, there is a $\delta > 0$ so that the following holds: for any
  symmetric convex body $Q \subseteq \setR^n$ with $\ell_Q \leq \delta$ one has $\beta(Q) \leq \varepsilon$.
\end{lemma}
Next, we can get a fairly tight upper bound on the covering radius of a $t$-stable lattice: %\rem{T: Needed for $t$-stable?}
\begin{proposition} \label{prop:CovRadiusOf2StableLattice}
  Let $\Lambda \subseteq \setR^n$ be a full rank lattice that is the $r$-scaling of a $t$-stable lattice and let $Q \subseteq \setR^n$ be a symmetric convex body.
  Then $\mu(\Lambda,Q) \leq O(\log n) \cdot t \cdot r \cdot \ell_Q$.
\end{proposition}
\begin{proof}
  Let $\varepsilon>0$ be a small enough constant that we determine later.
  Let $\delta$ be the constant so that Lemma~\ref{lem:BanaszczykLKvsBetaK} applies (w.r.t. $\varepsilon$).
  The claim is invariant under scaling $Q$, hence we may scale $Q$ so that $\ell_Q \leq \delta$ and consequently $\beta(Q) \leq \varepsilon$.
  We may also scale the lattice so that $\Lambda$ is $t$-stable (i.e. $r=1$). % and in particular $\frac{1}{2} \leq \textrm{nd}(\Lambda) \leq 2$.
  It suffices to prove that under these assumptions,  $\mu(\Lambda,Q) \leq s \cdot t$ where $s := C\log(n)$
  is the parameter from Lemma~\ref{lem:PropertiesOf2StableLattices}. 
  Now suppose for the sake of contradiction that there is a translate $u \in \setR^n$ so that $(u+\Lambda) \cap stQ = \emptyset$.
  Since $\beta(Q) \leq \varepsilon$, we know that
  \[
    \rho_1\Big( \Big(\frac{u}{st} + \frac{\Lambda}{st}\Big) \setminus Q\Big) \leq \varepsilon \rho_1\Big(\frac{\Lambda}{st}\Big)
  \]
  is true. Multiplying the sets and parameters by $st$ gives
  \[
 \rho_{st}((u+\Lambda) \setminus stQ) \leq \varepsilon \rho_{st}(\Lambda). \quad (*)
  \]
 Using that $\Lambda$ is $t$-stable, we get
  \[
\frac{1}{3}\rho_{st}(\Lambda) \stackrel{\textrm{Lem~\ref{lem:PropertiesOf2StableLattices}}}{\leq} \rho_{st}(u+\Lambda) \stackrel{(u+\Lambda) \cap stQ = \emptyset}{=} \rho_{st}((u + \Lambda) \setminus stQ) \stackrel{(*)}{\leq} \varepsilon \rho_{st}(\Lambda).
  \]
  Then choosing $\varepsilon \in (0, \frac{1}{3})$ gives a contradiction. \qedhere
%   Now we turn to the general case when $K$ is not necessarily symmetric. We may bound
%   \[\mu(\Lambda, K) \le \mu(\Lambda, K \cap -K) \le O(\log n) \cdot \mathrm{nd}(\Lambda) \cdot \ell_{K \cap -K} \le O(\log n) \cdot \ell_K,  \]
%   where in the last inequality we used 
%   \begin{eqnarray*}
% \ell_{K \cap -K}^2 &=& \E_{\bm{x} \sim N(\bm{0}, \bm{I}_n)} [\|\bm{x}\|_{K \cap -K}^2] \\ &=& \E_{\bm{x} \sim N(\bm{0}, \bm{I}_n)} [\max\{\|\bm{x}\|_{K},\|\bm{x}\|_{-K} \}^2] \\ &\le& \E_{\bm{x} \sim N(\bm{0}, \bm{I}_n)} [\|\bm{x}\|_{K}^2 + \|\bm{x}\|_{-K}^2] \le 2 \ell_K^2.\qedhere 
%   \end{eqnarray*}
% 
\end{proof}



\section{The inductive step\label{sec:InductiveStep}}

%We want to outline a candidate approach to prove that $\mu(\Lambda,K) \leq \textrm{polylog}(n) \cdot \mu_{KL}(\Lambda,K)$
%for any lattice and any symmetric convex body $K$.
We will spend the next two sections proving our main Theorem~\ref{thm:KLConj} by induction over $n$. At each step, we split the lattice $\Lambda$ and the convex body $K$ into a subspace section of dimension at least $n/2$ and a projection where most of the work will go into analyzing the subspace section. First, we give a self-contained
description of the inductive step, then later in Section~\ref{sec:MainProof} we describe the main part of the induction.
%The main technical work towards proving step that we need for an inductive proof of Theorem~\ref{thm:KLConjForSymConvex} will be the following:
\begin{proposition} \label{prop:MainArgument}
  Let $\Lambda \subseteq \setR^n$ be a full rank lattice and let $K \subseteq \setR^n$
  be a convex body so that $\textrm{bary}(K^{\circ}) = \bm{0}$. Set $K_{\textrm{sym}} := K \cap (-K)$ and assume that  $\ell_{K_{\textrm{sym}}} \cdot \ell_{K_{\textrm{sym}}^{\circ}} \leq O(n \log n)$.
  Consider a well-separated 2-stable filtration  $\{ \bm{0} \} = \Lambda_{0} \subset \ldots \subset \Lambda_k = \Lambda$. Let $i^* \in \{ 1,\ldots,k\}$ be the minimal index so that $\textrm{rank}(\Lambda_{i^*}) \geq \frac{n}{2}$. Set $U := \textrm{span}(\Lambda_{i^*})$. Then
  $\mu(\Lambda \cap U, K_{\textrm{sym}} \cap U) \leq  C_0\log^6 (n) \cdot \mu_{KL}(\Lambda,K)$
  where $C_0>0$ is a universal constant.
\end{proposition}
% The proof of Prop~\ref{prop:MainArgument} will take the remainder of this section.
\begin{proof}
We fix the lattice $\Lambda$, the filtration, the set $K$ and the index $i^*$.
%We may scale $K$ so that $\ell_K = \sqrt{n}$, meaning that $K$ should behave much like the ball $B_2^n$.
We define
\[
  d_i := \rank(\Lambda_i/\Lambda_{i-1}) \quad \textrm{and} \quad r_i := \mathrm{nd}(\Lambda_i/\Lambda_{i-1}) = \det(\Lambda_i/\Lambda_{i-1})^{1/d_i}
\]
which are the rank and normalized determinants of the quotient lattices in the filtration.
Recall that $r_1 < r_2 < \ldots < r_k$ with $r_i \leq \frac{1}{2}r_{i+2}$ for all $i$.
We will relate the covering radius to the quantities $r_i$ and $d_i$ and then relate those
to $\mu_{KL}(\Lambda,K)$.
We split the argument into several parts and first prove a rather crude looking upper bound on the covering radius
of any individual quotient lattice. \\ %Note that for the Euclidean ball we know that  $\mu(\Lambda_i/\Lambda_{i-1},B_2^n) \leq O(r_i\sqrt{d_i})$, hence in our bound we potentially loose a factor of $\tilde{O}(\sqrt{\frac{n}{d_i}})$.
{\bf Claim I.} \emph{For any $i \in \{ 1,\ldots,k\}$ one has \[\mu\big(\Lambda_i/\Lambda_{i-1},\Pi_{\mathrm{span}(\Lambda_{i-1})^{\perp}}(K_{\mathrm{sym}} \cap \mathrm{span}(\Lambda_i))\big) \lesssim \log (n) \cdot r_i \cdot \ell_{K_{\mathrm{sym}}}.\]}
\noindent {\bf Proof of Claim I.}  We abbreviate $K_i := \Pi_{\textrm{span}(\Lambda_{i-1})^{\perp}}(K_{\textrm{sym}} \cap \textrm{span}(\Lambda_i))$. Then as $K_i$ is symmetric and $\frac{1}{r_i}(\Lambda_i/\Lambda_{i-1})$ is 2-stable, we have % by Lemma~\ref{lem:LvalueOfProjectedIntersection} we have
  \[
  \mu\big(\Lambda_i/\Lambda_{i-1},K_i\big) \stackrel{\textrm{Prop~\ref{prop:CovRadiusOf2StableLattice}}}{\lesssim} \log(n) \cdot r_i \cdot \ell_{K_i} \stackrel{\textrm{Lem~\ref{lem:LvalueOfProjectedIntersection}}}{\leq} \log(n) \cdot r_i \cdot \ell_{K_{\textrm{sym}}}.  \quad \quad \qed %\q \lozenge
  \]
%We know from the work of Regev and Stephens-Davidowitz that $r_i \sqrt{d_i} \lesssim \mu_{KL}(\Lambda,B_2^n)$. We generalize that
%inequality while again we expect to lose a  $\tilde{O}(\sqrt{\frac{n}{d_i}})$ factor. \\

\noindent {\bf Claim II.} \emph{For any $d$-dimensional subspace $W \subseteq \setR^n$ one has
  \[\Big(\frac{\Vol_d(\Pi_W(K))}{\Vol_d(B_2^d)}\Big)^{1/d} \lesssim \Big(\frac{n}{d}\Big)^{3} \cdot \frac{\ell_{K_{\mathrm{sym}}^{\circ}}}{\sqrt{d}}.
  \]}
\noindent {\bf Proof of Claim II.} Using the volume estimate from Proposition~\ref{prop:VolumeOfProjectionVsSymmetrizer} with the assumption that the barycenter of $K^{\circ}$ lies at the origin, we obtain
  \begin{eqnarray*}
    \Big(\frac{\Vol_d(\Pi_W(K))}{\Vol_d(B_2^d)}\Big)^{1/d} &\stackrel{\textrm{Prop~\ref{prop:VolumeOfProjectionVsSymmetrizer}}}{\lesssim}& \Big(\frac{n}{d}\Big)^3 \cdot \Big(\frac{\Vol_d(\Pi_W(K_{\textrm{sym}}))}{\Vol_d(B_2^d)}\Big)^{1/d} \\ &\stackrel{\textrm{Thm~\ref{thm:UrysohnInequality}}}{\lesssim}& \Big(\frac{n}{d}\Big)^3 \cdot w(\Pi_W(K_{\mathrm{sym}}) ) \\
    &\stackrel{\textrm{Lem~\ref{lem:PropertiesOfPolarity}}}{\lesssim}& \Big(\frac{n}{d}\Big)^{3} \cdot \frac{\ell_{K_{\textrm{sym}}^{\circ} \cap W}}{\sqrt{d}} \stackrel{\textrm{Lem~\ref{lem:MonotonicityLValue}}}{\leq} \Big(\frac{n}{d}\Big)^{3} \cdot \frac{\ell_{K_{\textrm{sym}}^{\circ}}}{\sqrt{d}}.
  \end{eqnarray*}
  Here we used the Urysohn inequality to relate volume to width as well as the fact that $\Pi_W(K_{\textrm{sym}})^{\circ} = K_{\textrm{sym}}^{\circ} \cap W$
  by Lemma~\ref{lem:PropertiesOfPolarity}. \qed
  
\noindent {\bf Claim III.} \emph{For any $i \in \{ 1,\ldots,k\}$ one has $r_i \lesssim (\frac{n}{d_i})^3 \cdot \frac{\ell_{K_{\mathrm{sym}}^{\circ}}}{d_i} \cdot \mu_{KL}(\Lambda,K)$.} \\
\noindent {\bf Proof of Claim III.}  Let $W := \textrm{span}(\Lambda_{i-1})^{\perp}$ and let $d := \dim(W) = \rank(\Lambda/\Lambda_{i-1})$. Then
  using that $r_i \leq r_{i+1} \leq \ldots \leq r_k$ one has
  \begin{eqnarray*}
    \sqrt{d_i}\cdot r_i &\leq& \sqrt{\sum_{j \geq i} d_j} \cdot \Big( \prod_{j \geq i} r_j^{d_j}\Big)^{1/\sum_{j \geq i} d_j} \\
    &=& \sqrt{d} \cdot \det(\Lambda/ \Lambda_{i-1})^{1/d} \\
                  &\stackrel{(*)}{\lesssim}&  \Big(\frac{\Vol_d(\Pi_W(K))}{\Vol_d(B_2^d)}\Big)^{1/d} \cdot  \underbrace{\Big(\frac{\det(\Pi_W(\Lambda))}{\Vol_d(\Pi_W(K))}\Big)^{1/d}}_{\leq \mu_{KL}(\Lambda,K)} \\
    &\stackrel{(**)}{\lesssim}& \left(\frac{n}{d}\right)^{3} \cdot \frac{\ell_{K_{\textrm{sym}}^{\circ}}}{\sqrt{d}} \cdot \mu_{KL}(\Lambda,K) \\
  &\stackrel{d_i \leq d}{\leq}& \left(\frac{n}{d_i}\right)^{3} \cdot \frac{\ell_{K_{\textrm{sym}}^{\circ}}}{\sqrt{d_i}} \cdot \mu_{KL}(\Lambda,K).
  \end{eqnarray*}
  In $(*)$ we used that $\Vol_d(B_2^d)^{1/d} = \Theta(\frac{1}{\sqrt{d}})$ (see e.g. \cite{AsymptoticGeometricAnalysis-Book2015}). In $(**)$ we used Claim II.   Rearranging gives the claim. \qed
 
%Assuming these two claims we can prove the  main claim. %proposition:
%\begin{proof}[Proof of Prop~\ref{prop:MainArgument}]
  Now we approach the main claim. Recall that  $U := \textrm{span}(\Lambda_{i^*})$ and  $\textrm{rank}(\Lambda_{i^*}) \geq \frac{n}{2}$. First we have
  \begin{eqnarray*}
    \mu(\Lambda \cap U, K_{\textrm{sym}} \cap U) &\stackrel{\textrm{Lem~\ref{lem:TriangleIneqForFiltration}}}{\leq}& \sum_{i=1}^{i^*} \mu\left( \Lambda_i/\Lambda_{i-1}, \Pi_{\textrm{span}(\Lambda_{i-1})^{\perp}}(K_{\textrm{sym}} \cap \textrm{span}(\Lambda_i)) \right)  \\
      &\stackrel{\textrm{Claim I}}{\lesssim}& \log (n) \cdot \ell_{K_{\textrm{sym}}} \cdot \sum_{i=1}^{i^*} r_i \lesssim \log (n) \cdot \ell_{K_{\textrm{sym}}} \cdot r_{i^*} 
  \end{eqnarray*}
  as $r_1 \leq \ldots \leq r_{i^*}$ and $r_{i} \leq \frac{1}{2}r_{i+2}$ for all $i$. 
  We distinguish two cases:
  \begin{itemize}
  \item {\bf Case I.} $k-i^*>10\log(n)$. In this case, the second part of the filtration contains many quotient lattices and their normalized determinant is exponentially increasing in $i$. In particular
    $r_{i^*} \leq \frac{1}{n^5} r_k$. Then even using the loose bound of $r_k \leq n^3 \cdot \ell_{K^\circ_{\textrm{sym}}} \cdot \mu_{KL}(\Lambda,K)$ implied by Claim III, we obtain
    \begin{eqnarray*}
      \mu(\Lambda \cap U,K_{\textrm{sym}} \cap U)  &\lesssim& \log(n) \cdot \ell_{K_{\textrm{sym}}} \cdot r_{i^*}  \\
&\lesssim& \log(n) \cdot \underbrace{\ell_{K_{\textrm{sym}}} \cdot \ell_{K_{\textrm{sym}}^{\circ}}}_{\lesssim n \log(n)} \cdot n^3 \cdot \mu_{KL}(\Lambda,K) \cdot \underbrace{\frac{r_{i^*}}{r_k}}_{\leq n^{-5}} \\
                                          &\lesssim&  \mu_{KL}(\Lambda,K).
    \end{eqnarray*}
  \item {\bf Case II.} $k-i^* \leq 10\log(n)$. In this case, there is some index $i$ with $i^* \leq i \leq k$ so that $d_i \geq \Omega(\frac{n}{\log(n)})$.
    Then  we can use this index to bound
    \begin{eqnarray*}
      \mu(\Lambda \cap U,K_{\textrm{sym}} \cap U) &\lesssim& \log(n) \cdot r_{i^*} \cdot \ell_{K_{\textrm{sym}}} \\
                                   &\stackrel{r_{i^*} \leq r_i}{\leq}& \log(n) \cdot r_i \cdot \ell_{K_{\textrm{sym}}} \\
                                   &\stackrel{\textrm{Claim III}}{\leq}& \log(n) \cdot \Big(\frac{n}{d_i}\Big)^3 \cdot \frac{1}{d_i} \cdot \underbrace{\ell_{K_{\textrm{sym}}^{\circ}} \ell_{K_{\textrm{sym}}}}_{\lesssim n \log(n)} \cdot \mu_{KL}(\Lambda,K) \\
                                   &\stackrel{d_i \gtrsim \frac{n}{\log(n)}}{\lesssim}& \log^6(n) \cdot \mu_{KL}(\Lambda,K). \qedhere
    \end{eqnarray*}
  \end{itemize}
\end{proof}
%It might be instructive to compare this proof with an analogue of \cite{


\section{Completing the main proof\label{sec:MainProof}}

Using Proposition~\ref{prop:MainArgument} we can finish the proof of our main theorem. %the Kannan-Lov\'asz conjecture. % for symmetric convex sets.
\begin{proof}[Proof of Theorem~\ref{thm:KLConj}]
  Consider a full rank lattice $\Lambda \subseteq \setR^n$ and a convex body $K \subseteq \setR^n$. We will prove by induction over $n$ that %\rem{T: I gave the constant $C_0$ a name here. The reason is that in an inductive proof it is important to make sure that the constant does not grow over the inductive step.}
  \[
  \mu(\Lambda,K) \leq C_0\log^7(n) \cdot \mu_{KL}(\Lambda,K),
\]
where $C_0$ is the constant from Proposition~\ref{prop:MainArgument}.
  %where $\rho(n)$ is the bound from Prop~\ref{prop:MainArgument}.
%W.l.o.g. we may assume that $\Lambda$ has full rank and that $K$ is full-dimensional.
By Lemma~\ref{lem:TranslateSoThatPolarIsCentered}, we may translate $K$ so that the barycenter of $K^{\circ}$ is $\bm{0}$. As the claim is invariant under linear transformations, we may also assume that the symmetrizer $K_{\textrm{sym}} := K \cap (-K)$ satisfies $\ell_{K_{\textrm{sym}}} \cdot \ell_{K_{\textrm{sym}}^{\circ}} \leq O(n \log n)$ by Theorem~\ref{thm:PisierRescaling}.
By Cor~\ref{cor:ExistenceTwoStableWellSeparatedFiltration}, there exists a well-separated 2-stable filtration $\{ \bm{0} \} = \Lambda_{0} \subset \ldots \subset \Lambda_k = \Lambda$.
%  Following Proposition~\ref{prop:MainArgument}, consider the 2-approximate canonical filtration $\{ \bm{0} \} = \Lambda_{0} \subset \ldots \subset \Lambda_k = \Lambda$
  We fix the minimal index  $i^* \in \{ 1,\ldots,k\}$  so that   $\textrm{rank}(\Lambda_{i^*}) \geq \frac{n}{2}$.
  Set   $W := \textrm{span}(\Lambda_{i^*})$. Then
  \begin{eqnarray*}
    \mu(\Lambda,K) &\stackrel{\textrm{Lem~\ref{lem:TriangleIneqGenCoveringRadius}}}{\leq}& \mu(\Lambda \cap W,K \cap W) + \mu(\Pi_{W^{\perp}}(\Lambda),\Pi_{W^{\perp}}(K)) \\
    &\stackrel{K \supseteq K_{\textrm{sym}}}{\leq}& \mu(\Lambda \cap W,K_{\textrm{sym}} \cap W) + \mu(\Pi_{W^{\perp}}(\Lambda),\Pi_{W^{\perp}}(K)) \\
                   &\stackrel{\textrm{Prop~\ref{prop:MainArgument}}+\textrm{induction}}{\leq}& C_0\log^6(n) \cdot \mu_{KL}(\Lambda,K) + C_0\log^7(\underbrace{\dim(W^{\perp})}_{\leq n/2}) \cdot \underbrace{\mu_{KL}(\Pi_{W^{\perp}}(\Lambda),\Pi_{W^{\perp}}(K))}_{\leq \mu_{KL}(\Lambda,K)} \\
    &\stackrel{\textrm{Lem~\ref{lem:MonotonicityOfMuKL}}}{\leq}& C_0\underbrace{\log^6(n) \cdot \Big(1+\log\Big(\frac{n}{2}\Big)\Big)}_{=\log^7(n)} \cdot \mu_{KL}(\Lambda,K). \qedhere
  \end{eqnarray*}
\end{proof}


We should point out that Regev and Stevens-Davidowitz~\cite{Regev-SD-ReverseMinkowskiTheoremSTOC17}
prove that in the Euclidean case one has $\mu(\Lambda,B_2^n) \leq O(\log^{3/2}(n)) \cdot \mu_{KL}(\Lambda,B_2^n)$.
Our proof could be seen as a generalization of their argument in the sense that \cite{Regev-SD-ReverseMinkowskiTheoremSTOC17} also relate both notions of covering radii to the quantities $r_i$ and $d_i$ by proving that
\[
  \mu(\Lambda,B_2^n) \leq O(\log n) \cdot \sqrt{\sum_{i=1}^k d_i r_i^2} \leq O(\log^{3/2} n) \cdot \mu_{KL}(\Lambda,B_2^n)
\]
On the other hand, for them the ``standard'' canonical filtration suffices and they do not require an inductive step. Implicitly, our induction causes $O(\log n)$ many re-centering and rescaling operations using the result of
Figiel, Tomczak-Jaegerman and Pisier (Theorem~\ref{thm:PisierRescaling}). This circumvents the issue that %we do not have a tight analogue of Theorem~\ref{thm:PisierRescaling} for asymmetric convex bodies.
%Also note that clearly it can happen that
the covering radius might be dominated by a subspace of dimension $d$ with $d \ll n$ which may not affect the $\ell$-position of the body sufficiently. Then implicitly the induction will contain an iteration where $d$ is relatively large compared to the current ambient dimension. 
It may also be instructive to reconsider the proof of Prop~\ref{prop:MainArgument} in the case that $K = B_2^n$.
Then in Claim I, we would obtain the inequality $\mu(\Lambda_{i}/\Lambda_{i-1},K_i) \lesssim \log(n) \cdot r_i \cdot \sqrt{n}$ while actually the much stronger bound of $\mu(\Lambda_{i}/\Lambda_{i-1},K_i) \lesssim \log(n) \cdot r_i \cdot \sqrt{d_i}$ holds. Similarly in Claim III we incur a loss that is a polynomial in $\frac{n}{d_i}$.
The trick is that there are at most $O(\log n)$ indices with relevant contributions and their loss
can always be charged to an index with higher normalized determinant.
%This loss seems necessary as in general $K_{sym}$ may not The trick is only reason why the proof still works is that  %, one has $\ell_{K_i} \approx \sqrt{d_i}$


\section{Finding the subspace $W$ in single-exponential time\label{sec:FindingSubspace}}

In this section, we prove Theorem~\ref{thm:FindingSubspaceIn2ToN} which guarantees that a suitable subspace subspace
$W$ can be found in time $2^{O(n)}$ at the expense of an additional logarithmic factor in the approximation guarantee.
It will be convenient to first apply a linear transformation to well-scale $K$. This can be done in polynomial time
and is a standard argument, see Lemma~\ref{lem:WellScaleKorDecideKisThin} for details. Hence, for us it suffices to prove the following: 
\begin{theorem} \label{thm:SubspaceWellConditioned}
Given a full rank lattice $\Lambda \subseteq \setR^n$ and a convex body $K \subseteq \setR^n$ such that $B^n_2 \subseteq K \subseteq (n+1)^{3/2} B^n_2$, there exists a randomized $2^{O(n)}$-time algorithm to compute a subspace $W \subseteq \setR^n$ with $d := \dim (W)$ so that
\[\mu(\Lambda, K) \lesssim \log^8 (n) \cdot \Big(\frac{\det(\Pi_W(\Lambda))}{\Vol_d(\Pi_W(K))}\Big)^{1/d}. \]
\end{theorem}

The main technical tool will be the following result of Dadush, which is the only step in the algorithm which takes exponential time:

\begin{theorem}[Theorem 6.4. in~\cite{Dadush-Finding-DenseLatticeSubspacesSTOC19}] \label{thm:StableFiltrationAlgorithm}
Given a lattice $\Lambda \subseteq \setR^n$ one can compute an $O(\log n)$-stable filtration of $\Lambda$ in $2^{O(n)}$ time with probability at least $1-2^{-\Omega(n)}$.
\end{theorem}

The following algorithm mimics the proof in Section 4:

\begin{center}
  \psframebox{\begin{minipage}{14cm}
{\sc Find-Subspace} \\
{\bf Input:} Convex body $K \subseteq \setR^n$ so that $B^n_2 \subseteq K \subseteq (n+1)^{3/2} B^n_2$, full rank lattice $\Lambda \subseteq \setR^n$ \\
 {\bf Output:} Subspace $W \subseteq \setR^n$ satisfying Theorem~\ref{thm:SubspaceWellConditioned}
\begin{enumerate*}
\item[(1)] Compute an approximate Santal\'o point $\tilde{x}$ such that $\|s(K) - \tilde{x}\|_2 \le 1$
\item[(2)] Shift $K' := K - \tilde{x}$
\item[(3)] Set $K_{\mathrm{sym}} := K' \cap (-K')$ and compute an invertible linear map $T$ so that
  \[\ell_{T(K_{\mathrm{sym}})} \cdot \ell_{(T (K_{\mathrm{sym}}))^{\circ}} \leq C \cdot n \log n \]
\item[(4)] Set $K' \gets T(K)$ and $\Lambda' \gets T(\Lambda)$
\item[(5)] Compute an $O(\log n)$-stable filtration  $\{\bm{0}\} = \Lambda_0 \subset \ldots \subset
      \Lambda_k = \Lambda'$
\item[(6)] Compute a well-separated $O(\log n)$-stable filtration  $\{\bm{0}\} = \Lambda'_0 \subset \ldots \subset
      \Lambda'_{k'} = \Lambda'$
\item[(7)] Set $i^*$ as the minimal index with $\rank(\Lambda'_{i^*}) \ge \frac{n}{2}$
\item[(8)] Set $W_i := \span(\Lambda'_{i})^\perp$. %Set $W_{\cap} := \span(\Lambda'_{i})^\perp$ where $i \in \{1, \dots, k'\}$ minimizes $\frac{\det(\Pi_{\span(\Lambda'_{i})^\perp}(\Lambda'))}{\Vol_{\rank(\Lambda'/\Lambda'_{i})} (\Pi_{\span(\Lambda'_{i})^\perp} (K'))}$
\item[(9)] Recursively call $W_{\Pi} := {\textsc{Find-Subspace}}(\Pi_{\span(\Lambda'_{i^*})^\perp} (K')), \Pi_{\span(\Lambda'_{i^*})^\perp} (\Lambda'))$
\item[(10)] Return $W := T^{-1} W'$ where $W' := \underset{W \in \{W_1,\ldots,W_{k'}, W_\Pi\}}{\mathrm{argmin}} \Big\{ \frac{\det(\Pi_{W}(\Lambda'))}{\Vol_{\dim(W)} (\Pi_{W} (K'))} \Big\}$.
\end{enumerate*}
\end{minipage}}
\end{center}

We will need several volume computations in the algorithm, for which we use the following theorem:

\begin{theorem}[\cite{Kannan1997RandomWA}] \label{thm:VolumeComputation}
Given a convex body $K \subseteq \setR^n$ with $r\cdot B^n_2 \subseteq K \subseteq R \cdot B^n_2$, there exists a randomized algorithm which outputs a positive number $\zeta$ with $\Vol_n(K)/\zeta \in [1-\varepsilon, 1 + \varepsilon]$. The runtime is polynomial in $n$, $1/\varepsilon$, $\log(1/r)$ and $\log(R)$. 
\end{theorem}


Now, we can prove the main result for this section: 
\begin{proof}[Proof of Theorem~\ref{thm:SubspaceWellConditioned}]
First we justify the running time of $2^{O(n)}$, later we discuss the approximation guarantee.
  We will show further down in Section~\ref{sec:ComputingApxSantalo} that we can indeed compute an approximate Santal\'o point $\tilde{x}$ for step (1) in polynomial time. In order to compute the linear map $T$, it is well-known~\cite{10.5555/2095116.2095230} that it suffices to solve the (stochastic) convex program
\[\inf_T \E_{x \sim N(\bm{0}, I_n)} [\|Tx\|_K]\]
\[\text{subject to } T \succeq 0, \det(T) \ge 1,\]
which can be done in randomized polynomial time. Theorem~\ref{thm:StableFiltrationAlgorithm} yields a filtration for step (5), which can be refined into a well-separated filtration by Theorem~\ref{thm:ApproxFilt}. Step (10) requires computation of determinants, which can be done in polynomial time via Gaussian elimination, and the volume of a convex body, which can also be done in randomized polynomial time using Theorem~\ref{thm:VolumeComputation}.
The runtime $T(n)$ of ${\textsc{Find-Subspace}}$ satisfies the recursion $T(n) \le 2^{O(n)} + T(n/2)$, which can be resolved to $T(n) \leq 2^{O(n)}$.

Next, we justify the approximation guarantee. From the same argument in Section 3 and 4 one can see that the returned subspace satisfies
\[\mu(\Lambda, K) \lesssim \log^8 (n) \cdot \Big(\frac{\det(\Pi_W(\Lambda))}{\Vol_d(\Pi_W(K))}\Big)^{1/d}, \]
where we have taken into account that we pay an additional $\log n$ factor from Proposition~\ref{prop:CovRadiusOf2StableLattice} as our filtration is only guaranteed to be $O(\log(n))$-stable. Another subtle point is that we are using only an
approximate Santal\'o point. Hence it remains to generalize Proposition~\ref{prop:VolumeOfProjectionVsSymmetrizer}
and show that the approximation costs us at most another constant factor: %, since we only compute an approximate Santal\'o point:

\noindent {\bf Claim.} \emph{Let $K \subseteq \setR^n$ be a convex body so that $B^n_2 \subseteq K$ and $\|s(K)\|_2 \le 1$. Let $F \subseteq \setR^n$ be a $d$-dimensional subspace. Then denoting $K_\mathrm{sym} := K \cap (-K)$,}

\[ \Vol_d(\Pi_F(K))^{1/d} \lesssim \Big(\frac{n}{d}\Big)^3 \cdot \Vol_d(\Pi_F(K_\mathrm{sym}))^{1/d}.\]
{\bf Proof of Claim.} By Lemma~\ref{lem:TranslateSoThatPolarIsCentered} and Proposition~\ref{prop:VolumeOfProjectionVsSymmetrizer}, we know that denoting $\tilde{K}_{\mathrm{sym}} := (K-s(K)) \cap (-K+s(K))$, we have
\[ \Vol_d(\Pi_F(K))^{1/d} \lesssim \Big(\frac{n}{d}\Big)^3 \cdot \Vol_d(\Pi_F(\tilde{K}_{\mathrm{sym}} ))^{1/d}.\]
Since $- s(K) \subseteq B^n_2 \subseteq K$, it follows that $K - s(K) \subseteq K + K = 2K$, so that $\tilde{K}_{\mathrm{sym}} \subseteq 2 K_{\mathrm{sym}}$ and $\Vol_d(\Pi_F(\tilde{K}_{\mathrm{sym}} ))^{1/d} \le 2 \cdot \Vol_d(\Pi_F(K_\mathrm{sym}))^{1/d}.$
\end{proof}

\subsection{Computing an approximate Santal\'o point\label{sec:ComputingApxSantalo}}

It remains to compute an approximation to the Santal\'o point for step (1):
\begin{proposition}\label{prop:approxSantalo}
Given a convex body $K \subseteq \setR^n$ with $B^n_2 \subseteq K \subseteq (n+1)^{3/2} \cdot B^n_2$ and $\delta>0$, there exists a randomized algorithm with running time polynomial in $n$ and $\frac{1}{\delta}$, which returns an approximate Santal\'o point $\tilde{x}$ such that $\|s(K) - \tilde{x}\|_2 \le \delta$.
\end{proposition}

To approximately compute $s(K)$, we rely on its original characterization:

\begin{theorem}[\cite{Santalo1949}] The Santal\'o point $s(K)$ is the unique minimizer of the convex function $f : \textrm{int}(K) \to \setR$ given by $f(x) = \Vol_n((K-x)^\circ)$.
\end{theorem}

We point out that the Santal\'o point is also known in the interior point method literature as the \textit{analytic center} of the \textit{universal barrier} $\log f$~\cite{Nesterov1994InteriorpointPA}. The universal barrier is known to be $n$-self-concordant for any convex body $K$~\cite{doi:10.1287/moor.2020.1113} which allows fast optimization using Newton iterations. To keep the exposition simpler, we instead rely on the following general convex optimization guarantee of Gr\"otschel, Lov\'asz and Schrijver. Here, $d(x,K) := \inf\{ \|x-y\|_2 : y \in K\}$ denotes the distance of $x$ to $K$.

\begin{theorem}[Theorem 4.3.13 in \cite{Groetschel1988}] \label{WeakConstrainedConvexOpt}
Given a rational number $\varepsilon > 0$, a convex body $K \subseteq \setR^n$ (given by a separation oracle, a center $c$ and radii $r, R$ with $c + r B^n_2 \subseteq K \subseteq c + R B^n_2$) and a convex function $f : \setR^n \to \setR$ (given by an oracle that, for $x \in \setQ^n$ and $\delta > 0$, returns a rational number $t$ with $|f(x) - t| \le \delta$), there exists a polynomial time algorithm which outputs $\tilde{x}$ with $d(\tilde{x},K) \le \varepsilon $ and $f(\tilde{x}) \le f(x) + \varepsilon $ for all $x$ with $x + \varepsilon  B^n_2 \subseteq K$.
\end{theorem} %\rem{T: Polynomial in what? Also what accuracy $\delta$ will be needed to get a fixed value of $\varepsilon$?}
%Here, for the accuracy $\delta$, a value suffices that is a polynomial in $n$ and $1/\varepsilon$.
The following result will be useful to argue that the Santal\'o point cannot be too close to the boundary of $K$:

\begin{theorem}[Theorem 4.1. in \cite{Lovasz1995}] \label{CentroidEllipsoid}
For any convex body $K \subseteq \setR^n$ there exists an ellipsoid $\pazocal{E}$ with $\pazocal{E} \subseteq K - \mathrm{bary}(K) \subseteq (n+1) \cdot \pazocal{E}$.
\end{theorem}


We will also need a lemma that ensures points with small function value are close to $s(K)$. For $t > 0$, we define the \emph{Santal\'o region}
  \[
  S(K,t) := \big\{ x \in K \mid \Vol_n(K) \cdot \Vol_n( (K-x)^{\circ}) \leq t \cdot \nu_n^2 \big\}
\]

\begin{lemma}[Lemma 7 in \cite{SantaloRegions}] \label{SantaloRegionContainment}
  Given a convex body $K \subseteq \setR^n$, we have
  \[S(K,t) \subseteq s(K) + \frac{4}{n} \cdot \Big(\frac{t \nu_n^2}{\Vol_n (K) \Vol_n ((K-s(K))^\circ)} - 1\Big)^{1/2} \cdot \pazocal{E}((K-s(K))^\circ),\]
where $\pazocal{E}(K) := \E_{x \sim K} [xx^\top]^{1/2} \cdot B^n_2$.
\end{lemma}
We want to rewrite Lemma~\ref{SantaloRegionContainment} in a more convenient way. Let $t_K>0$ be the minimum value so that $S(K,t_K) \neq \emptyset$. Note that indeed $S(K,t_K) = \{ s(K) \}$
and $\Vol_n(K) \cdot \Vol_n((K-s(K))^{\circ}) = t_K \cdot \nu_n^2$.
% By Blaschke-Santal\'o we know that $t_K \leq 1$.
Hence the statement of Lemma~\ref{SantaloRegionContainment} then says that for any $\varepsilon \geq 0$ one has
\[
 S(K,(1+\varepsilon^2)t_K) \subseteq s(K) + \frac{4}{n} \cdot \varepsilon \cdot \pazocal{E}((K-s(K))^{\circ}).
\]


Now we have all the ingredients to approximately compute the Santal\'o point:
{}
\begin{proof}[Proof of Proposition~\ref{prop:approxSantalo}]
By Lemma~\ref{lem:TranslateSoThatPolarIsCentered}, we know that $\mathrm{bary}((K - s(K))^\circ) = \bm{0}$, so that Theorem~\ref{CentroidEllipsoid} yields some ellipsoid $\pazocal{E}^\circ$ with $\pazocal{E}^\circ \subseteq (K - s(K))^\circ \subseteq (n+1) \cdot \pazocal{E}^\circ$, or equivalently $\frac{1}{n+1} \pazocal{E} \subseteq K - s(K) \subseteq \pazocal{E}$. In particular, we have $B^n_2 - s(K) \subseteq \pazocal{E}$, and by symmetry, $s(K) - B^n_2 \subseteq \pazocal{E}$, so by convexity $B^n_2 \subseteq \pazocal{E}$ and therefore $s(K) + \frac{1}{n+1} B^n_2 \subseteq K$. Since $\frac{s(K)}{\| s(K)\|_2} \in B^n_2$ and $\|s(K)\|_2 \le (n+1)^{3/2}$, we conclude that $(1 + \frac{1}{(n+1)^{5/2}}) s(K) \in K$. 

Next, we define the slightly shrunk body $\tilde{K} := (1 + \frac{1}{(n+1)^{5/2}})^{-1} K$ and the function $f : \setR^n \to \setR_{> 0}$ with
\[
  f(x) = \begin{cases} \Vol_n((K-x)^\circ) & \textrm{if } x \in \tilde{K} \\ \infty & \textrm{otherwise.} \end{cases}
\]
We have $(1 - (1 + \frac{1}{(n+1)^{5/2}})^{-1}) B^n_2 \subseteq K-x \subseteq 2 (n+1)^{3/2} B^n_2$ for all $x \in \tilde{K}$ %, the value $\ln(f(x))$ is upper bounded by a polynomial in $n$ for all $x \in \tilde{K}$
and so by Theorem~\ref{thm:VolumeComputation} we can approximate $f(x)$ for all $x \in \tilde{K}$ to arbitrary accuracy in polynomial time. We may then apply Theorem~\ref{WeakConstrainedConvexOpt} which yields an approximate minimizer $\tilde{x}$ with $f(\tilde{x}) \le (1 + \varepsilon^2) f(s(K))$ for  $\varepsilon := \frac{n}{4(n+1)} \cdot \delta$. Then by Lemma~\ref{SantaloRegionContainment}, it follows that $\tilde{x} \in s(K) + \frac{4}{n} \cdot \varepsilon \cdot \pazocal{E}((K-s(K))^\circ)$. Since $(K-s(K))^\circ \subseteq (n+1) B^n_2$, it also follows that $\pazocal{E}((K-s(K))^\circ) \subseteq (n+1) B^n_2$, and we conclude $\tilde{x} \in s(K) + \delta B^n_2$.
\end{proof}

\section{Integer programming in time $(\log n)^{O(n)}$\label{sec:IP}}

%{\bf TODO. Basically reproduce from Dadush's thesis.}
Next, we show that integer programming can be solved in time $(\log n)^{O(n)}$. In fact,
this is a known consequence of Theorem~\ref{thm:FindingSubspaceIn2ToN}.
We do not claim any original contribution for this section,
but we reproduce the arguments of Dadush~\cite{DadushThesis2012} to be self-contained.
As it is common in the literature, we only state the dependence of running times on $n$;
all running times that involve a convex set $K \subseteq r B_2^n$ and a lattice $\Lambda = \Lambda(B)$ also contain
a not mentioned factor that is polynomial in $\log(r)$ and in the encoding length of $B$.
%By some preprocessing we may also assume that $K$ is a convex body and $\Lambda$ has full rank --- otherwise,
%replace both objects with their intersection with the  affine subspace $\textrm{span}(\Lambda) \cap \textrm{affine.hull}(K)$. 

First, we describe the intuition behind Dadush's algorithm. Consider a convex body $K \subseteq \setR^n$
and a lattice $\Lambda \subseteq \setR^n$; the goal is to find a point in $K \cap \Lambda$.
We compute a subspace $W \subseteq \setR^n$ in time $2^{O(n)}$ that certifies the covering radius $\mu(\Lambda,K)$ up to
a factor $\rho(n) := \Theta(\log^{8} (n))$. %\rem{T: Changed to $\rho = \Theta(\log^8(n))$.}
Consider the points  $X := \Pi_W(K) \cap \Pi_W(\Lambda)$ in the projection on $W$.
For each $x \in K \cap \Lambda$, we also have $\Pi_W(x) \in X$. Note that the reverse may not be true in the
sense that it is entirely possible that $K \cap \Lambda = \emptyset$ while $X \neq \emptyset$.
However, we are guaranteed that all lattice points
in $K$ must be in one of the $(n-d)$-dimensional fibers of the projection, i.e.
\[
 K \cap \Lambda \subseteq \bigcup_{y \in X} \big((K \cap \Pi_W^{-1}(y)) \cap \Lambda\big).
\]
\iftrue
\begin{center}
  \psset{unit=1.3cm}
  \begin{pspicture*}(-4.5,-2.5)(4.5,2.5)
    \pspolygon[fillstyle=solid,fillcolor=lightgray](-0.8,1)(-0.8,1.5)(1,1.8)(2.1,1.7)(1,1.2) \rput[c](0.5,1.5){$K$}
    \psline[linecolor=blue!50!white,linewidth=2pt](5,2.5)(-5,-2.5)\pnode(3.5,1.75){A}\nput{-45}{A}{$\blue{W}$}
%    \rput[c](0,-0.5){\pspolygon[fillstyle=solid,fillcolor=lightgray](-3.2,-0.2)(-3.2,0.2)(3.2,0.2)(3.2,-0.2)}
    \multido{\n=-2+1}{5}{\multido{\N=-4+1}{9}{\psdots(\N,\n)}}
 %   \rput[c](0,-0.5){$K$}
    % \drawRect{}{-3.2}{-0.2}{6.4}{6.4}
    \cnode*(0,0){2.5pt}{origin} \nput[labelsep=2pt]{-45}{origin}{$\bm{0}$}
    \psline[linecolor=darkgray,linewidth=3pt](2.4,1.2)(-0.24,-0.12)
 %   \psdots[linecolor=blue,linewidth=1.5pt](-4,-2)(-2,-1)(0,0)(2,1)(4,2)
  %  \rput[c](-0.24,-0.12){\psline[linestyle=dotted](0,0)(-1,2)}
  %  \rput[c](2.4,1.2){\psline[linestyle=dotted](0,0)(-1,2)}
    \multido{\N=0+1}{4}{\rput[c](\N,0){\psline[linestyle=dotted,linecolor=gray](3,-6)(-3,6)}}
    \multido{\N=0+1}{3}{\rput[c](\N,1){\psline[linestyle=dotted,linecolor=gray](3,-6)(-3,6)}}
    \multido{\N=0+0.4,\n=0+0.2}{7}{\psdots[linecolor=blue,linewidth=1.5pt](\N,\n)}
    \pnode(0,0){A}\pnode(-0.5,-1.0){B} \ncline[linecolor=blue,nodesepB=3pt]{->}{B}{A} \nput[labelsep=2pt]{-90}{B}{$\blue{X}$}
    \pnode(1.0,0.5){A}\pnode(2,-0.5){B} \ncline[linecolor=black,nodesepB=3pt]{->}{B}{A} \nput[labelsep=2pt]{0}{B}{$\Pi_{W}(K)$}
    \psplot[algebraic=true,linewidth=2pt,linecolor=darkgray]{-0.75}{-0.52}{-2*x}
    \psplot[algebraic=true,linewidth=2pt,linecolor=darkgray]{-0.3}{-0.05}{-2*(x-0.5)}
    \psplot[algebraic=true,linewidth=2pt,linecolor=darkgray]{0.17}{0.43}{-2*(x-1.0)}
    \psplot[algebraic=true,linewidth=2pt,linecolor=darkgray]{0.63}{0.9}{-2*(x-1.5)}
    \psplot[algebraic=true,linewidth=2pt,linecolor=darkgray]{1.1}{1.32}{-2*(x-2.0)}
    \psplot[algebraic=true,linewidth=2pt,linecolor=darkgray]{1.63}{1.73}{-2*(x-2.5)}
    \pspolygon[fillstyle=none,fillcolor=lightgray](-0.8,1)(-0.8,1.5)(1,1.8)(2.1,1.7)(1,1.2)% redraw borders of K
    % \psplot[algebraic=true,linewidth=2pt,linecolor=darkgray]{2.0}{2.5}{-2*(x-3.0)}
   
  \end{pspicture*}
\end{center}
%\fi
The algorithm enumerates $X$ and then recurses on all the fibers. In order for this algorithm to be
efficient we need to (i) bound the cardinality $|X|$ and (ii) be able to enumerate $X$. For 
(ii), note that it is possible that $W = \setR^n$ and hence we would not gain anything by treating $\Pi_W(K) \cap \Pi_W(\Lambda)$ as a general integer programming problem.


For convex bodies $A,B \subseteq \setR^n$, the \emph{covering number} $N(A,B) := \min\{ N \mid \exists x_1,\ldots,x_N \in \setR^n: A \subseteq \bigcup_{i=1}^N (x_i + B)\}$ is the minimum number of translates of $B$ needed to cover $A$.
  For a convex body $K \subseteq \setR^n$ and a full rank lattice $\Lambda \subseteq \setR^n$ we define
\[
 G(\Lambda,K) := \max_{x \in \setR^n} |(K + x) \cap \Lambda|.
\]
In words, $G(\Lambda,K)$ denotes the maximum number of lattice points that any shift of $K$
contains. Note that even if $K \cap \Lambda = \emptyset$, $G(\Lambda,K)$ might still be arbitrarily large.
However, algorithmically the quantity $G(\Lambda,K)$ is useful:
\begin{theorem}[\cite{EnumerateLatticeAlgosDadushPeikertVempalaFOCS11,NearOptDetAlgoForMEllipsoid-DadushVempalaPNAS13}\label{thm:LatticeEnumeration}]
  Given a convex body $K \subseteq \setR^n$ and a full rank lattice $\Lambda \subseteq \setR^n$,
  one can enumerate all points in $K \cap \Lambda$ in deterministic time $2^{O(n)} \cdot G(\Lambda,K)$.
\end{theorem}
We briefly sketch the algorithm behind Theorem~\ref{thm:LatticeEnumeration}:
We use the method of Dadush and Vempala~\cite{NearOptDetAlgoForMEllipsoid-DadushVempalaPNAS13} to compute an \emph{$M$-ellipsoid} $\pazocal{E}$ of $K$ which has the property that $N(K,\pazocal{E}),N(\pazocal{E},K) \leq 2^{O(n)}$.
Their deterministic algorithm takes time $2^{O(n)}$. In particular this means that $2^{-\Theta(n)} \leq \frac{G(\Lambda,K)}{G(\Lambda,\pazocal{E})} \leq 2^{\Theta(n)}$. %and translates $x_1,\ldots,x_N$ with $N \leq 2^{O(n)}$
Next, we compute\footnote{At least in the case that $\pazocal{E}$ is an $M$-ellipsoid for $K$, one may find
  those translates with $N \leq 2^{O(n)}N(K,\pazocal{E})$ with ease.
  After applying a linear transformation, we may assume that $\pazocal{E} = \sqrt{n} B_2^n$. Then take all translates $x + \pazocal{E}$ with $x \in \setZ^n$ that intersect $K$.} the translates $x_1,\ldots,x_N$ with $N \leq 2^{O(n)}$ so that $K \subseteq \bigcup_{i=1}^N (x_i + \pazocal{E})$.
Then we can use the following argument by Dadush, Peikert and Vempala~\cite{EnumerateLatticeAlgosDadushPeikertVempalaFOCS11} to enumerate all lattice points in $(x_i + \pazocal{E}) \cap \Lambda$. After applying
a linear transformation, it suffices to compute all points in $(t+B_2^n) \cap \Lambda$ for $t \in \setR^n$.
Let $R \subseteq \Lambda \setminus \{ \bm{0}\}$ be the \emph{Voronoi-relevant} vectors, which are all the
vectors that define a facet of the \emph{Voronoi cell} of $\Lambda$. It is known that $|R| \leq 2^{n+1}$
and moreover the set $R$ can be computed in time $2^{O(n)}$ by the algorithm of \cite{CVP-Voronoi-Algo-MicciancioVoulgaris-SICOMP2013}.
Next, consider the graph $H = (\Lambda,E)$ with edges $E = \{ \{ x,y\} : x,y \in \Lambda\textrm{ and }x-y \in R\}$.
Then it follows from the work of \cite{CVP-Voronoi-Algo-MicciancioVoulgaris-SICOMP2013} that the subgraph induced by $\Lambda \cap (t + B_2^n)$ is connected. Hence, one can compute
the closest lattice point to $t$ (again using \cite{CVP-Voronoi-Algo-MicciancioVoulgaris-SICOMP2013}) and then
traverse the subgraph. 

%For each $i \in [N]$, we can randomly \emph{sparsify} the lattice $\Lambda$. 
%We guess the translate $\bm{y} + \pazocal{E}$ so that $\bm{x}^* \in K \cap (\bm{y} + \pazocal{E})$. Now we randomly \emph{sparsify} the lattice $\Lambda$.
%More precisely, we can compute a shifted random sublattice $z + \Lambda' \subseteq \Lambda$ so that with constant probability
%one has  $|(x_i + \pazocal{E}) \cap (z + \Lambda')| = 1$. Moreover, each lattice point
%in $(x_i + \pazocal{E}) \cap \Lambda$ has approximately the same probability of being the lone survivor.
%The point in $(x_i + \pazocal{E}) \cap (z + \Lambda')$ can then be computed in time $2^{O(n)}$
%using the algorithm of Micciancio and Voulgaris~\cite{CVP-Voronoi-Algo-MicciancioVoulgaris-STOC2010}.
%Repeating this sampling procedure $2^{O(n)}G(\Lambda,K)$ times will produce all lattice points in $K$ with high probability. 


Next, we require an upper bound on $G(\Lambda,K)$ in terms of the volume of $K$ and density of $\Lambda$.
Surprisingly, such an upper bound exists if we additionally control the covering radius.
We reproduce Dadush's proof as the argument is key to understanding the algorithm:
\begin{lemma} \label{lem:UpperBoundOnLatticePoints}
  For any full rank lattice $\Lambda \subseteq \setR^n$ and any convex body $K \subseteq \setR^n$ one has
  \[
  G(\Lambda,K) \leq 2^n \max\{ \mu(\Lambda,K)^n,1\} \cdot \frac{\Vol_n(K)}{\det(\Lambda)}.
  \]
\end{lemma}

\begin{proof}
  After a linear transformation and scaling by $\max\{ \mu(\Lambda,K),1\}$, the statement is
  equivalent to the following simpler claim: \\
  {\bf Claim.} \emph{For any convex body $K \subseteq \setR^n$ with $\mu(\setZ^n,K) \leq 1$ and any $x \in \setR^n$ one has $|K \cap (x+\setZ^n)| \leq 2^n \Vol_n(K)$.} \\
{\bf Proof of Claim.}
The claim is invariant under translating $K$, hence we may assume that $\bm{0} \in K$.
Let $\equiv$ be the equivalence relation on pairs $x,y \in K$ that is defined by  $x \equiv y \Leftrightarrow x-y \in \setZ^n$. We define a set $V \subseteq K$ by selecting one element from each equivalence class w.r.t. $\equiv$. It would not
matter much which element was selected, but let us make the canonical choice of choosing the lexicographically
minimal one. In other words, we choose 
  \[
   V = \big\{ x \in K \mid x \leq_{\textrm{lex}} y \quad \forall y \in (x + \setZ^n) \cap K\big\}
  \]
  where $\leq_{\textrm{lex}}$ is the standard lexicographical ordering.
 \iftrue
  \begin{center}
    \psset{unit=0.7cm}
    \begin{pspicture}(-2,-2)(2,2)
      \pspolygon[fillstyle=solid,fillcolor=lightgray,linewidth=0.75pt](-1.25,-2)(-2,-1.25)(-2,2)(2,2)(2,-2)\rput[c](1.5,1.5){$K$}
      \pspolygon[fillstyle=solid,fillcolor=gray,linestyle=none](-1,-2)(-1.25,-2)(-2,-1.25)(-2,-0.25)(-1.25,-1)(-1,-1)
      \psline[linewidth=1.5pt](-1,-2)(-1.25,-2)(-2,-1.25)(-2,-0.25)\rput[c](-1.5,-1.3){$V$}
      \multido{\N=-2+1}{5}{\multido{\n=-2+1}{5}{\psdots(\N,\n)}}
      \cnode*(0,0){2.0pt}{origin}\nput[labelsep=2pt]{0}{origin}{$\bm{0}$}
    \end{pspicture}
  \end{center}
 \fi
As we select at most one element from
  each equivalence class, we certainly have $\textrm{Vol}_n(V) \leq 1$. On the other hand,
  $\mu(\setZ^n,K) \leq 1$ implies that for all $x \in \setR^n$ one has  $(x+\setZ^n) \cap K \neq \emptyset$.
  That in turn means that every equivalence class has a member in $K$ and so $\Vol_n(V) \geq 1$.
  Together this gives $\textrm{Vol}_n(V)=1$.
  Next, we note that by construction all translates $x+V$ with $x \in \setZ^n$ are disjoint. Moreover, for $x \in  K \cap \setZ^n$ one has that $x + V \subseteq K + K =2K$. Then 
  \[
  |K \cap \setZ^n| = \sum_{x \in K \cap \setZ^n} \underbrace{\Vol_n(x+V)}_{=1} \stackrel{\textrm{disj.}}{=} \Vol_n\Big(\bigcup_{x \in K \cap \setZ^n} (x+V)\Big) \leq \Vol_n(2K),
  \]
which gives the claim.
\end{proof}



%\rem{V: Would be nice to incorporate this preprocessing step before calling \textsc{Find-Subspace} (and also after each recursive call)}
%\begin{lemma}[Lemma 7.2.1 in~\cite{DadushThesis2012}]
%Let $K \subseteq \setR^n$ be a $(a_0, R)$-circumscribed convex set given by a separation oracle, let $\Lambda \subseteq \setR^n$ denote a lattice given by a basis $B \in \setQ^{n \times n}$, and let $H$ be an affine subspace. There exists a $2^{O(n)}$-time algorithm which either decides that $K \cap \Lambda \cap H = \emptyset$ or returns a shift $p \in \Lambda$, and sublattice $\Gamma' \subseteq \Gamma$, a vector $a_0' \in \span(\Lambda')$, a separation oracle for the convex set $K' := (K - p) \cap (a'_0 + R B^n_2) \cap \span(\Lambda')$, and an ellipsoid $\pazocal{E}' \subseteq \span(\Lambda')$ with center $c'$, so that $K \cap \Lambda \cap H = (K' \cap \Lambda') + p$ and $c' + \pazocal{E}' \subseteq K' \subseteq c' + (n+1)^{3/2} \pazocal{E}'$. 
%\end{lemma}


One technicality we have to deal with is that Theorem~\ref{thm:FindingSubspaceIn2ToN} requires a lower bound
on the \emph{inradius} of $K$. Hence we run a preprocessing step: if there is no suitable lower bound for the
inradius, then the lattice points of $K$ are all contained in an easy-to-find hyperplane.
\begin{lemma} \label{lem:WellScaleKorDecideKisThin}
  Given a compact convex set $K \subseteq r B_2^n$ and a lattice $\Lambda = \Lambda(B)$. Then in time polynomial in $n$,
  times a polynomial in $\log(r)$ and the encoding length of $B$ one can find at least one of the following: 
  \begin{enumerate}
  \item[(a)] An ellipsoid $\pazocal{E}$ and center $c$ so that $c + \frac{1}{(n+1)^{3/2}}\pazocal{E} \subseteq K \subseteq c+\pazocal{E}$.
  \item[(b)] A vector $a \in \setR^n \setminus \{ \bm{0}\}$ and $\beta \in \setR$ so that $K \cap \Lambda \subseteq \{ x \in \setR^n \mid \left<a,x\right> = \beta\}$.
  \end{enumerate}
\end{lemma}
\begin{proof}
  % The ellipsoid method can be modified to find an en
  We may assume that $\textrm{rank}(\Lambda)=n$, otherwise any $a$ orthogonal to $\textrm{span}(\Lambda)$ will satisfy $(b)$.
  Next, we use a variant of the ellipsoid method from \cite{Groetschel1988} (see also Lemma~2.5.10 in \cite{DadushThesis2012}) to find a pair  $(c,\pazocal{E})$ in time polynomial in $n$, $\log(r)$ and $\log(\frac{1}{\varepsilon})$ 
  so that either (a) holds, or   $K \subseteq c+\pazocal{E}$ and $\Vol_n(\pazocal{E}) \leq \varepsilon$. 
   Suppose the latter happens. Then using Minkowski's Theorem in $(*)$ and the Blaschke-Santal\'o-Bourgain-Milman Theorem (Theorem~\ref{thm:BSBM}) in $(**)$ we obtain
   \begin{eqnarray*}
     \lambda_1(\Lambda^*,\pazocal{E}^{\circ}) \stackrel{(*)}{\lesssim} \Big(\frac{\det(\Lambda^*)}{\Vol_n(\pazocal{E}^{\circ})}\Big)^{1/n}  \stackrel{(**)}{\lesssim}  \Big(\frac{\Vol_n(\pazocal{E})}{\det(\Lambda) \cdot \nu_n^2}\Big)^{1/n}
     \lesssim n \cdot \Big(\frac{\varepsilon}{\det(\Lambda)}\Big)^{1/n} \leq \frac{1}{2} \cdot 2^{-n/2}
   \end{eqnarray*}
   for a suitable choice of $\varepsilon>0$. Then the LLL-algorithm~\cite{LLL1982} can find a dual lattice vector $a \in \Lambda^* \setminus \{ \bm{0}\}$ with $\|a\|_{\pazocal{E}^{\circ}} \leq 2^{n/2} \cdot \lambda_1(\Lambda^*,\pazocal{E}^{\circ}) \leq \frac{1}{2}$. That vector $a$ with $\beta := \lceil \left<a,c\right> \rfloor$ will  satisfy $(b)$. \qedhere
%  We run a modified ellipsoid method that maintains an enclosing ellipsoid $E \supseteq K$ starting with $E := R B_2^n$. In each iteration, we either decide that $E$ scaled by a factor $\frac{1}{(n+1)^{3/2}}$ is contained in $K$.
  
\end{proof}
We are now ready to state the complete algorithm. As mentioned earlier, we denote $\rho(n) := \Theta(\log^8(n))$ % \rem{T: Check $\rho(n)$!}
as the approximation factor from Theorem~\ref{thm:FindingSubspaceIn2ToN}.  

\begin{center}
  \psframebox{\begin{minipage}{14cm}
{\sc Dadush's algorithm} \\
{\bf Input:} Compact convex set $K \subseteq \setR^n$, lattice $\Lambda \subseteq \setR^n$ \\
{\bf Output:} Point $x \in K \cap \Lambda$ or decision that there is none
\begin{enumerate*}
\item[(1)] Use Lemma~\ref{lem:WellScaleKorDecideKisThin}. If case (b) happens, obtain hyperplane $H$ with $K \cap \Lambda \subseteq H$. Recurse on $\textsc{Dadush}(K \cap H, \Lambda \cap H)$ and return the answer.
\item[(2)] Compute a subspace $W \subseteq \setR^n$ with $d := \dim(W)$ and $R := (\frac{\det(\Pi_W(\Lambda))}{\Vol_d(\Pi_W(K))})^{1/d}$ so that $R \leq \mu(\Lambda,K) \leq \rho(n) \cdot R$.
\item[(3)] Set $\tilde{K} := \min\{ \rho(n) \cdot R,1 \} \cdot (K-c)+c$ for some $c \in K$.
\item[(4)] Compute an $M$-ellipsoid $\pazocal{E} \subseteq W$ for $\Pi_W(\tilde{K})$
\item[(5)] Compute $N \leq 2^{O(d)}$ points $x_1,\ldots,x_N \in W$ so that $\Pi_W(\tilde{K}) \subseteq \bigcup_{i=1}^N (x_i + \pazocal{E})$. 
\item[(6)] Compute $X := \Pi_W(\tilde{K}) \cap \Pi_W(\Lambda) = \big(\bigcup_{i=1}^N ((x_i+\pazocal{E}) \cap \Pi_W(\Lambda))\big) \cap \Pi_W(\tilde{K})$
\item[(7)] Recursively call ${\textsc{Dadush}}(\tilde{K} \cap \Pi_{W}^{-1}(x),\Lambda \cap \Pi_W^{-1}(x))$ for all $x \in X$ and return any found lattice point (if there is any)
\end{enumerate*}
\end{minipage}}
\end{center}
Here, to be more informative, we have expanded the blackbox from Theorem~\ref{thm:LatticeEnumeration}
into lines (4)-(6).
The reader may also note a subtlety here that we have not discussed so far: if $K$ is very large
so that $\mu(\Lambda,K) \ll 1$, then we may shrink $K$ to a smaller body $\tilde{K} \subseteq K$
as long as we ensure that still $\mu(\Lambda, \tilde{K}) \leq 1$. We can now finish the analysis:
\begin{theorem}
Dadush's algorithm finds a point in $K \cap \Lambda$ in time $(\log n)^{O(n)}$ if there is one.
\end{theorem}%$
\begin{proof}
 If the algorithm recurses in (1), the claim is clear by induction. So assume otherwise. 
  First we argue correctness of the algorithm.
  Let  $s := \min\{ \rho(n) \cdot R,1\} \in [0,1]$  and recall that $\tilde{K} \subseteq K$ is a scaling of $K$
  by a factor of $s$. After step (3), the algorithm searches for a lattice point in $\tilde{K}$ rather than in the
  original body $K$. If $s<1$, then the covering radius of the shrunk body is $\mu(\Lambda,\tilde{K}) = \frac{1}{\rho(n) \cdot R} \mu(\Lambda,K) \leq 1$. In other words, even though we continue the search in the strictly smaller body $\tilde{K}$, we are still guaranteed that $\tilde{K} \cap \Lambda \neq \emptyset$. 
 Next, we discuss the running time of the algorithm. We estimate that 
  \begin{eqnarray*}
  G(\Pi_W(\Lambda),\Pi_W(\tilde{K})) &\stackrel{\textrm{Lem~\ref{lem:UpperBoundOnLatticePoints}}}{\leq}& 2^d \max\big\{ \mu(\Pi_W(\Lambda),\Pi_W(\tilde{K}))^d,1\big\} \cdot \frac{\Vol_d(\Pi_W(\tilde{K}))}{\det(\Pi_W(\Lambda))} \\
                                          &\leq& 2^d \max\Big\{ \Big(\underbrace{\frac{\rho(n) R}{s}}_{\geq 1}\Big)^d,1\Big\} \cdot s^d \cdot \underbrace{\frac{\Vol_d(\Pi_W(K))}{\det(\Pi_W(\Lambda))}}_{=R^{-d}} \\
    &=& 2^d \cdot (\rho(n) R)^d \cdot R^{-d} = (2 \rho(n))^d.
  \end{eqnarray*}
  Here we use that $\mu(\Pi_W(\Lambda),\Pi_W(\tilde{K})) \leq \mu(\Lambda,\tilde{K}) = \frac{1}{s} \cdot \mu(\Lambda,K) \leq \frac{\rho(n) \cdot R}{s}$. Then $|X| \leq G(\Pi_W(\Lambda),\Pi_W(\tilde{K})) \leq 2^d\rho(n)^d$ and by Lemma~\ref{lem:UpperBoundOnLatticePoints}, the computation of $X$ in (4)-(6) takes time $2^{O(d)} \rho(n)^d$.
Now, let $T(n)$ be the maximum running time of the algorithm on $n$-dimensional instances. Then we have the recursion
\[
 T(n) \leq \max_{d \in \{ 1,\ldots,n\}} \Big\{ 2^{O(n)} + (O(1) \cdot \rho(n))^d \cdot T(n-d)\Big\} \quad \textrm{and} \quad T(1)=\Theta(1)
\]
which indeed resolves to  $T(n) \leq O(\rho(n))^n$.
\end{proof}


We also explain how Dadush's algorithm can be used to solve integer linear programs in time $(\log n)^{O(n)}$.
Again, the arguments used are standard. Details on the estimates can be found in the book of Schrijver~\cite{TheoryOfLPandILP-Schrijver1999}. 
\begin{proof}[Proof of Theorem~\ref{thm:SolvingExplicitIPinLogNtoN}]
  Consider an arbitrary integer linear program $\max\{ c^Tx \mid Ax \leq b, x \in \setZ^n\}$. One can
  compute a number $M$ in time polynomial in $n$ and the encoding length of $A$ and $b$ so that
  if the IP is bounded and feasible, then the optimum value is the same as $\max\{ c^Tx \mid Ax \leq b, \|x\|_{\infty} \leq M, x \in \setZ^n\}$.
  Next, by applying binary search, it suffices to find an integer point in the compact convex
  set $K = \{ x \in \setR^n \mid c^Tx \geq \delta, Ax \leq b, \|x\|_{\infty} \leq M\}$ for which Theorem~\ref{thm:SolvingIPinLogNtoN} applies.
\end{proof}

\section{Implications of Theorem~\ref{thm:KLConj}\label{sec:Implications}}

Here we derive a few implications of our main result. The following classical inequality will be useful here:

\begin{lemma}[\cite{Rogers1957TheDB}] \label{lem:RSineq}
For any convex set $K \subseteq \setR^n$ we have $\Vol_n (K-K) \le {2n \choose n} \cdot \Vol_n (K)$.
\end{lemma}

We restate Theorem~\ref{thm:CoveringRadiusKvsKminusK}, which yields a nearly tight relationship between the covering radii of $K$ and $K-K$. We remark that it remains an open question whether the two quantities are equal up to a constant.

\begin{theorem*}[Theorem~\ref{thm:CoveringRadiusKvsKminusK}] %\label{cor:CoveringRadiusKvsKminusK}
For any full rank lattice $\Lambda \subseteq \setR^n$ and any convex body $K \subseteq \setR^n$, one has \[\mu(\Lambda,K-K) \leq \mu(\Lambda,K) \leq O(\log^{7} (n)) \cdot \mu(\Lambda,K-K).\]
\end{theorem*}

\begin{proof}
Let $W$ denote the subspace attaining $\mu_{KL}(\Lambda, K)$ with $\dim W = d$. We can use Theorem~\ref{thm:KLConj} to upper bound 
 \begin{eqnarray*}
\mu(\Lambda, K) \leq O(\log^{7} (n)) \cdot \mu_{KL} (\Lambda, K) & = & O(\log^{7} (n)) \cdot \Big(\frac{\det(\Pi_W(\Lambda))}{\Vol_d(\Pi_W(K))}\Big)^{1/d} \\ & \stackrel{\textrm{Lem~\ref{lem:RSineq}}}{\le} & O(\log^{7} (n)) \cdot 4 \cdot \Big(\frac{\det(\Pi_W(\Lambda))}{\Vol_d(\Pi_W(K-K))}\Big)^{1/d} \\ & \le & O(\log^{7} (n)) \cdot \mu_{KL}(\Lambda, K-K)\\ & \le & O(\log^{7} (n)) \cdot \mu(\Lambda, K-K). \qedhere
 \end{eqnarray*}
\end{proof}

This in turn implies that the \emph{flatness constant} in dimension $n$ is bounded by $O(n\log^{8}(n))$:
\begin{theorem*}[Theorem~\ref{thm:FlatnessConstant}]
  For any convex body $K \subseteq \setR^n$ and any full rank lattice $\Lambda \subseteq \setR^n$, one has
  \[
   \mu(\Lambda,K) \cdot \lambda_1(\Lambda^{*}, (K-K)^{\circ}) \leq O(n \log^{8}(n)).
  \]
\end{theorem*}

\begin{proof}
  Banaszczyk~\cite{Banaszczyk1996TransferenceTheoremsForGeneralConvexBodies} proved that for any symmetric convex
  body $Q \subseteq \setR^n$ one has $\mu(\Lambda,Q) \cdot \lambda_1(\Lambda^*,Q^{\circ}) \leq O(n \log(n))$. 
  Setting $Q := K-K$ (which is a symmetric convex body) one then has
  $\mu(\Lambda,K) \cdot \lambda_1(\Lambda^{*}, (K-K)^{\circ}) \leq O(\log^7 n) \cdot \mu(\Lambda,K-K) \cdot \lambda_1(\Lambda^{*}, (K-K)^{\circ}) \leq O(n\log^{8}(n))$ by Theorem~\ref{thm:CoveringRadiusKvsKminusK}.
%  an $O(n \log n)$ bound follows from the result of~\cite{TransferenceTheorems-Banaszczyk93}. Now the theorem follows from Corollary~\ref{cor:CoveringRadiusKvsKminusK}.
\end{proof}


\paragraph{Acknowledgement.} The authors are grateful to Daniel Dadush for numerous discussions on
related topics and a careful read of a preliminary draft.

\bibliographystyle{alpha}
\bibliography{KLconjecture}


\appendix


\section{The approximate canonical filtration} \label{appendix:ApproximateFiltration}

In this chapter, we prove Theorem~\ref{thm:ApproxFilt}. The proof idea is rather simple: given a $t$-stable
filtration $\{ \bm{0}\} = \Lambda_0 \subset \ldots \subset \Lambda_k = \Lambda$, we select one index from every density class in order to make the filtration well-separated. But before we come to the main argument, we require two lemmas.
\begin{lemma}[Grayson's parallelogram rule~\cite{ajm/1118669693}] \label{lem:ParallelogramRule}
For any two lattices $\Lambda, \Lambda' \subseteq \setR^n$,
\[\det(\Lambda) \cdot \det (\Lambda') \ge \det(\Lambda + \Lambda') \cdot \det(\Lambda \cap \Lambda'). \]
\end{lemma}
A proof may also be found in Chapter 2 of \cite{PhDThesisStephens-Davidowitz2017}.
The $t$-stable filtration can be used to obtain lower bounds on the determinant of any sublattice: 

%
%\begin{lemma} \label{lem:CanonicalFiltrationLowerBound}
%  Let $\Lambda \subseteq \setR^n$ be any lattice and let $\{ \bm{0}\} = \Lambda_0 \subset \Lambda_1 \subset \ldots \subset \Lambda_k = \Lambda$ be the canonical filtration. Let $r_i := \det(\Lambda_i / \Lambda_{i-1})^{1/\textrm{rank}(\Lambda_i/\Lambda_{i-1})}$ be the normalized determinant. Then for any sublattice $\tilde{\Lambda} \subseteq \Lambda$ and any $i \in \{ 1,\ldots,k\}$ the following holds:
%  \[
%    \det(\tilde{\Lambda}) \geq \det(\Lambda_i) \cdot r_i^{\textrm{rank}(\tilde{\Lambda})-\textrm{rank}(\Lambda_i)} = \det(\Lambda_{i-1}) \cdot r_i^{\textrm{rank}(\tilde{\Lambda})-\textrm{rank}(\Lambda_{i-1})}.
%  \]
%\end{lemma}
%\begin{proof}
%  Note that the slope of the segment between points $i-1$ and $i$ on the canonical plot is
%  \[
%   \frac{\ln(\det(\Lambda_i)) - \ln(\det(\Lambda_{i-1}))}{\textrm{rank}(\Lambda_i)-\textrm{rank}(\Lambda_{i-1})} = \ln(r_i)
% \]
% Hence by convexity, the point $(\textrm{rank}(\tilde{\Lambda}),\ln(\det(\tilde{\Lambda})))$ has to lie above the line with slope $\ln(r_i)$ that goes through the point $(\textrm{rank}(\Lambda_i),\ln(\det(\Lambda_i)))$. That means
% \begin{eqnarray*}
%   & & \ln(\det(\tilde{\Lambda})) \geq \ln(r_i) \cdot (\textrm{rank}(\tilde{\Lambda})-\textrm{rank}(\Lambda_i)) + \ln(\det(\Lambda_i)) \\
%   &\Leftrightarrow& \det(\tilde{\Lambda}) \geq r_i^{\textrm{rank}(\tilde{\Lambda})-\textrm{rank}(\Lambda_i)} \cdot \det(\Lambda_i)  = \det(\Lambda_{i-1}) \cdot r_i^{\textrm{rank}(\tilde{\Lambda})-\textrm{rank}(\Lambda_{i-1})}.
% \end{eqnarray*}

\begin{lemma}\label{lem:CanonicalFiltrationLowerBound}

Let $\Lambda \subseteq \setR^n$ be any lattice and let $\{ \bm{0}\} = \Lambda_0 \subset \Lambda_1 \subset \ldots \subset \Lambda_k = \Lambda$ be a $t$-stable filtration.  Then for any sublattice $\tilde{\Lambda} \subseteq \Lambda$ we have the inequality
\[ \nd(\tilde{\Lambda}) \ge t^{-1} \cdot \nd(\Lambda_1). \]
\end{lemma}
\begin{proof} %\rem{T: I expanded a little and fixed a few indices. Please check!}
  Let $r_i := \nd(\Lambda_i / \Lambda_{i-1}) = \det(\Lambda_i / \Lambda_{i-1})^{1/\textrm{rank}(\Lambda_i/\Lambda_{i-1})}$ be the normalized determinant. We prove by induction on $i \in \{1, \dots, k\}$ that the result holds for all lattices $\tilde{\Lambda} \subseteq \Lambda_i$. The base case follows as $\Lambda_1 = \Lambda_1 / \Lambda_0$ is a scalar of the $t$-stable
  lattice $\frac{1}{\textrm{nd}(\Lambda_1)}\Lambda_1$. Now suppose that $\tilde{\Lambda} \subseteq \Lambda_i$ for some $i > 1$. Note that $\Lambda_+ := \tilde{\Lambda} + \Lambda_{i-1}$ satisfies $\Lambda_{i-1} \subseteq \Lambda_+ \subseteq \Lambda_{i}$, so that $\Lambda_+ / \Lambda_{i-1} \subseteq \Lambda_i/\Lambda_{i-1}$ and $\nd(\Lambda_+ /\Lambda_{i-1}) \ge t^{-1} \cdot r_i >t^{-1}\cdot r_1$. By Lemma~\ref{lem:ParallelogramRule},
  \[
    \det(\tilde{\Lambda}) \cdot\det(\Lambda_{i-1})  \ge \det(\tilde{\Lambda} + \Lambda_{i-1}) \cdot \det(\tilde{\Lambda} \cap \Lambda_{i-1}).
  \]
  Factoring out $\Lambda_{i-1}$ gives
  \[
   \det(\tilde{\Lambda}) \geq \det(\Lambda_+ / \Lambda_{i-1}) \cdot \det(\tilde{\Lambda} \cap \Lambda_{i-1}).
  \]
  Hence
  \[
    \nd(\tilde{\Lambda}) \ge \nd(\Lambda_+/\Lambda_{i-1})^{\mathrm{rank}(\Lambda_+ / \Lambda_{i-1})/\mathrm{rank}(\tilde{\Lambda})} \cdot \nd(\tilde{\Lambda} \cap \Lambda_{i-1})^{\mathrm{rank}(\tilde{\Lambda} \cap \Lambda_{i-1})/\mathrm{rank}(\tilde{\Lambda})} \ge t^{-1} \cdot r_1
  \]
  where we used the inductive hypothesis on $\tilde{\Lambda} \cap \Lambda_{i-1} \subseteq \Lambda_{i-1}$ together with the fact that $\mathrm{rank}(\Lambda_+/\Lambda_{i-1}) + \mathrm{rank}(\tilde{\Lambda}\cap \Lambda_{i-1}) = \mathrm{rank}(\tilde{\Lambda})$. 

\end{proof}



Now, we come to the main argument:
\begin{proof}[Proof of Theorem~\ref{thm:ApproxFilt}]
  Let $r_i := \nd(\Lambda_i / \Lambda_{i-1})$ and $d_i := \textrm{rank}(\Lambda_i / \Lambda_{i-1})$. For $\ell \in \setZ$ denote $I_{\ell} := \{ i \in [k] : 2^{\ell} \leq r_i < 2 \cdot 2^{\ell}\}$.
  We define a sequence of indices $0=\ell(0)<\ell(1)<\ldots<\ell(\tilde{k}) = k$ that contains precisely the largest index $i$ in each $I_{\ell}$ with $I_{\ell} \neq \emptyset$ plus the index $\ell(0) = 0$.
  We set  $\tilde{\Lambda}_j := \Lambda_{\ell(j)}$ and $\tilde{r}_j := \nd(\tilde{\Lambda}_j / \tilde{\Lambda}_{j-1})$. First, 
  consider an index $\ell$ with $I_{\ell} \neq \emptyset$. Let $i_{\min},i_{\max} \in I_{\ell}$ be the minimal and maximal indices in $I_\ell$. Then
  \begin{eqnarray*}% first step: Lem~\ref{lem:PropertiesQuotientLattice}}.(a)
    \det(\Lambda_{i_{\max}} / \Lambda_{i_{\min}-1})^{1/\textrm{rank}(\Lambda_{i_{\max}}/\Lambda_{i_{\min}-1})} &=& \Big(\prod_{i=i_{\min}}^{i_{\max}} \det(\Lambda_{i}/\Lambda_{i-1})\Big)^{1 / \sum_{i=i_{\min}}^{i_{\max}} \textrm{rank}(\Lambda_i/\Lambda_{i-1})} \\
    &=& \Big(\prod_{i=i_{\min}}^{i_{\max}} r_i^{d_i}\Big)^{1/\sum_{i=i_{\min}}^{i_{\max}} d_i}.
  \end{eqnarray*}
  Note that this value is a weighted geometric average of $r_i$-values for $i \in I_{\ell}$.
  From this it immediately follows that $\tilde{r}_1 < \ldots < \tilde{r}_k$ and $\tilde{r}_j \leq \frac{1}{2}\tilde{r}_{j+2}$ for all $j$, i.e. (a') holds.  
  % Now, let us clean up the notation and focus on the sparser filtration  $\{ \bm{0}\} = \Lambda_0 \subset \ldots \subset \Lambda_k = \Lambda$ that is the outcome of the above process.
  It remains to show that the quotient lattices are scalars of $2t$-stable lattices. Fix some index $j \in [\tilde{k}]$ and let $\Lambda' := \frac{1}{\tilde{r}_j} (\tilde{\Lambda}_{j}/\tilde{\Lambda}_{j-1})$. First note that by assumption, the filtration  $\{\bm{0}\} = \Lambda'_0 \subset \cdots \subset \Lambda'_{k'} := \Lambda'$ given by $\Lambda'_i := \frac{1}{\tilde{r}_j} (\Lambda_{\ell(j-1) + i}/\Lambda_{\ell(j-1)})$ with $k' := \ell(j) - \ell(j-1)$ is also $t$-stable
  because $\Lambda_{i+1}'/\Lambda_i' = \frac{1}{\tilde{r}_j} (\Lambda_{\ell(j-1)+i+1}/\Lambda_{\ell(j-1)+i})$.

  %Note that the weighted geometric average of these would be between 1 and 2.
  We will prove the following
  two statements.
  \begin{enumerate}
  \item[$(I)$] For any sublattice $\tilde{\Lambda} \subseteq \Lambda'$ one has $\nd(\tilde{\Lambda}) \geq (2t)^{-1}$.
  \item[$(II)$] For any sublattice $\tilde{\Lambda} \subseteq (\Lambda')^*$ one has $\nd(\tilde{\Lambda}) \geq (2t)^{-1}$.
  \end{enumerate}
  First we show $(I)$. We apply Lemma~\ref{lem:CanonicalFiltrationLowerBound} on $\Lambda'$ to obtain
  \[
   \nd(\tilde{\Lambda}) \ge t^{-1} \cdot \nd(\Lambda'_1) \ge t^{-1} \cdot \frac{r_{\ell(j-1)+1}}{\tilde{r}_j} \geq (2t)^{-1},
  \]
  since both numerator and denominator belong to the same interval $[2^\ell, 2 \cdot 2^\ell)$ for some $\ell \in \mathbb{Z}$.
  Next, we prove $(II)$. Given the filtration $\{\bm{0}\} = \Lambda'_0 \subset \cdots \subset \Lambda'_{k'} = \Lambda'$ with $U_i := \span(\Lambda'_i)$, the dual filtration is given by $\{\bm{0}\} = (\Lambda')^*_0 \subset \cdots \subset (\Lambda')^*_{k'} = (\Lambda')^*$ with $(\Lambda')^*_i := \Lambda^* \cap U_{k'-i}^\perp$ and determinant $\det((\Lambda')^*_i) = \det((\Lambda')^*) \cdot \det(\Lambda'_{k'-i}) = \det(\Lambda'_{k'-i})$, see for example~\cite{Dadush-Finding-DenseLatticeSubspacesSTOC19}. %\rem{T: Are you certain that the indices here are right?}
  Since quotients of the dual filtration are duals of the quotients of the original filtration, the dual filtration is also $t$-stable. We then apply Lemma~\ref{lem:CanonicalFiltrationLowerBound} on $(\Lambda')^*$:
 \[
   \nd(\tilde{\Lambda}) \ge t^{-1} \cdot \nd( (\Lambda')^*_1 ) = t^{-1} \cdot (r'_{k'})^{-1} = t^{-1} \cdot \Big(\frac{r_{\ell(j)}}{\tilde{r}_j}\Big)^{-1} \stackrel{r_{\ell(j)} \leq 2\cdot \tilde{r}_j}{\geq} (2t)^{-1}. \qedhere
  \]

%\begin{eqnarray*}% First step: \textrm{Lem~\ref{lem:PropertiesLatticeSubspace}}.(b)
%  \det((\Lambda')^* \cap W) &=& \det(\Lambda' \cap W^{\perp}) \cdot \det((\Lambda')^*) \\ &=& \frac{\det(\Lambda' \cap W^{\perp})}{\det(\Lambda')} \\ &\stackrel{\textrm{Lem~\ref{lem:CanonicalFiltrationLowerBound}}.(a)}{\geq}& \Big(\frac{r_{i(j)}}{\tilde{r}_j}\Big)^{\textrm{rank}(\Lambda' \cap W^{\perp})-\textrm{rank}(\Lambda')}
%  = \Big(\frac{r_{i(j)}}{\tilde{r}_j}\Big)^{-d} \stackrel{r_{i(j)} \leq 2\cdot \tilde{r}_j}{\geq} 2^{-d}
%\end{eqnarray*}
%where we apply Lem~\ref{lem:CanonicalFiltrationLowerBound}.(a) with index $i=k'$.
\end{proof}


\section{Volumes of projections of convex bodies} \label{appendix:VolumeOfProjections}

%{\bf TODO (Need to copy-paste the proof here).}

In this section, we prove the volume estimate from Prop~\ref{prop:VolumeOfProjectionVsSymmetrizer}.
First, we revisit a few known facts that we require later.
%\begin{proposition} \label{prop:VolumeOfProjectionVsSymmetrizer}
%Let $K \subseteq \setR^n$ be a convex body so that $\textrm{bary}(K^{\circ}) = \bm{0}$ and let $F \subseteq \setR^n$ be a $d$-dimensional subspace. 
%Then
%\[
% \Vol_d(\Pi_F(K))^{1/d} \lesssim \Big(\frac{n}{d}\Big)^3 \cdot \Vol_d(\Pi_F(K \cap -K))^{1/d}
%\]
%\end{proposition}


\subsection{Preliminaries}

%We introduce a few useful facts.
For a hyperplane $H = \{ x \in \setR^n \mid \left<a,x\right> = \beta\}$
we write $H_{\geq} := \{ x \in \setR^n \mid \left<a,x\right> \geq \beta\}$ as the corresponding (closed) half-space.
The following can be found e.g. in \cite{AsymptoticGeometricAnalysis-Book2015}, Chapter 8:
\begin{theorem}[Blaschke-Santal\'o-Bourgain-Milman] \label{thm:BSBM}
  For any symmetric convex body $K \subseteq \setR^n$ one has
  \[
    C_1^n \nu_n^2\leq  \Vol_n(K) \cdot \Vol_n(K^{\circ}) \leq C_2^n \nu_n^2
  \]
  where $C_1,C_2>0$ are constants.
\end{theorem}
The lower bound is true for any convex body --- for the upper bound we have to be careful how to center $K$.
The following can be found e.g. in \cite{OnBlaschkeSantalo-MeyerPajor1990}.
\begin{lemma} \label{lem:VolKminusXPolarConvex}
Let $K \subseteq \setR^n$ be a convex body and let $F : \textrm{int}(K) \to \setR_{\geq 0}$ be the function with $F(x) := \Vol_n( (K-x)^{\circ})$. Then $F$ is strictly convex.
\end{lemma}
In particular, Lemma~\ref{lem:VolKminusXPolarConvex} implies that for any convex body $K$, there is a
unique minimizer $x^* \in \textrm{int}(K)$ of the function $x \mapsto \Vol_n( (K-x)^{\circ})$. That minimizer
is called the \emph{Santal\'o point} of $K$. If the Santal\'o point of $K$ happens to be $\bm{0}$, then
$\textrm{bary}(K^{\circ}) = \bm{0}$, see Lemma~\ref{lem:TranslateSoThatPolarIsCentered}.
The following will be useful: 
\begin{lemma}[\cite{SantaloRegions}] \label{lem:ApproximateBlaschkeSantalo}
  Let $K \subseteq \setR^n$ be a convex body and let $H$ be a hyperplane with $\Vol_n(K \cap H_{\geq}) = \delta \Vol_n(K)$ for some $0<\delta \leq \frac{1}{2}$. 
Then there is an $x \in H \cap \textrm{int}(K)$ so that
  \[
  \Vol_n(K) \cdot \Vol_n( (K-x)^{\circ}) \leq \frac{1}{4\delta(1-\delta)} \cdot \nu_n^2.
  \]
\end{lemma}
%For example one can derive from this lemma that
If either $\textrm{bary}(K) = \bm{0}$ or $\textrm{bary}(K^{\circ}) = \bm{0}$,
then $\Vol_n(K) \cdot \Vol_n(K^{\circ}) \leq \nu_n^2$, which is called the \emph{Blaschke-Santal\'o inequality}.
But Lemma~\ref{lem:ApproximateBlaschkeSantalo} also allows for approximate versions.
We will use the following generalization of Gr\"unbaum's Lemma:
\begin{theorem}[\cite{MYROSHNYCHENKO20182516}] \label{lem:GruenbaumForSections}
  For any convex body $K \subseteq \setR^n$ with barycenter at $\bm{0}$, any $d$-dimensional subspace $F \subseteq \setR^n$ and any hyperplane $H \subseteq \setR^n$ through $\bm{0}$ one has
  \[
    \frac{\Vol_d(K \cap F \cap H_{\geq})}{\Vol_d(K \cap F)} \geq \Big(\frac{d}{n+1}\Big)^{d}.
  \]
\end{theorem}


One can verify that for $d=n$ one recovers the constant $(\frac{n}{n+1})^n \geq \frac{1}{e}$ from Gr\"unbaum's result.
The probably most important ingredient is a powerful estimate by Rudelson that compares
sections of $K$ with sections of the \emph{difference body} $K-K$.
\begin{theorem}[Rudelson~\cite{Rudelson1998SectionsOT}] \label{thm:DifferenceBodyEstimateRudelson}
  Let $K \subseteq \setR^n$ be any convex body and let $F \subseteq \setR^n$ be a $d$-dimensional subspace. Then
  \[
    \Vol_d((K-K) \cap F)^{1/d} \leq \frac{Cn}{d} \cdot \max_{x \in \setR^n} \left\{ \Vol_d(K \cap (F+x))^{1/d} \right\}
  \]
  where $C>0$ is a universal constant.
\end{theorem}
The maximum volume slice will be approximately attained at the barycenter, which is a result due to
Fradelizi~\cite{SectionsOfConvexBodiesThroughCentroidFradelizi1997}.
%It seems inconvinient to have to argue
\begin{lemma} \label{lem:FradeliziSectionsThroughBarycenterApxMaximal}
  Let $K \subseteq \setR^n$ be a convex body with barycenter $\bm{0}$ and let $F$ be any $d$-dimensional
  subspace. Then
  \[
    \max_{x \in \setR^n} \left\{ \Vol_d(K \cap (F+x))^{1/d} \right\} \leq \frac{n+1}{d+1} \cdot \Vol_d(K \cap F)^{1/d}.
  \]
\end{lemma}



\subsection{Two auxiliary lemmas}

We require two auxiliary lemmas that might be new: 
\begin{lemma} \label{lem:ConvexBodyWithApproxBarycenterAtOrigin}
  Let $P \subseteq \setR^n$ be a convex body with the property that every hyperplane $H$ through
  the origin satisfies $\Vol_n(P \cap H_{\geq}) \geq \delta \cdot \Vol_n(P)$ for some $0<\delta \leq 1$. Then
  \[
    \Vol_n(P) \cdot \Vol_n(P^{\circ}) \leq \frac{1}{\delta} \cdot \nu_n^2.
  \]
\end{lemma}
\begin{proof}
  For $t \geq 1$, recall that the \emph{Santal\'o region} is defined as
  \[
  S(P,t) := \big\{ x \in P \mid \Vol_n(P) \cdot \Vol_n( (P-x)^{\circ}) \leq t \cdot \nu_n^2 \big\}
\]
%One can interpret the result of ? as the fact that $S(P,t) \neq \empty$ for $t \geq C^n$.
Then by Prop~1.(i) in \cite{SantaloRegions}, the set $S(P,t)$ is (strictly) convex. %Suppose for $t := \frac{1}{4\delta(1-\delta)}$,
Suppose that one has $\bm{0} \notin S(P,\frac{1}{\delta})$. Then there must be a halfspace $H$ through $\bm{0}$ so that $S(P,\frac{1}{\delta}) \subseteq H_{<}$.
By Lemma~\ref{lem:ApproximateBlaschkeSantalo}, there is a point $x^* \in H \cap \textrm{int}(P)$ so that $x^* \in S(P,\frac{1}{\delta})$.
That is a contradiction.
%$Vol_n(P) \cdot Vol_n( (P-x)^{\circ}) \leq \frac{1}{4\delta(1-\delta)} Vol_n(B_2^n)$ or phrased differently $x^* \in $
\end{proof}

\begin{lemma} \label{lem:ApproxSantaloForSectionThroughBarycenter}
  Let $Q \subseteq \setR^n$ be a convex body with $\textrm{bary}(Q) = \bm{0}$. Then for any $d$-dimensional subspace $F \subseteq \setR^n$ one has
  \[
   \Vol_d(Q \cap F) \cdot \Vol_d( (Q \cap F)^{\circ}) \leq \Big(\frac{n+1}{d}\Big)^d  \cdot \nu_d^2.
  \]
\end{lemma}
\begin{proof}
  We set $P := Q \cap F$ which is a $d$-dimensional object. %we think of an object in $\setR^d$
  By Lemma~\ref{lem:GruenbaumForSections}, we know that for any hyperplane $H$ through the origin, one has
  $\frac{\Vol_d(P \cap H_{\geq})}{\Vol_d(P)} \geq (\frac{d}{n+1})^d =: \delta$. Hence the assumption of
  Lemma~\ref{lem:ConvexBodyWithApproxBarycenterAtOrigin} is satisfied for this value of $\delta$
  and the claim follows.
\end{proof}



\subsection{Proof of Proposition~\ref{prop:VolumeOfProjectionVsSymmetrizer}}

Now, we finally have everything together to prove the main estimate. % Prop~\ref{prop:VolumeOfProjectionVsSymmetrizer}.
\begin{proof}[Proof of Prop~\ref{prop:VolumeOfProjectionVsSymmetrizer}]
 Recall that $K$ is a convex body with $\textrm{bary}(K^{\circ}) = \bm{0}$. We denote $K_{\textrm{sym}} := K \cap (-K)$, which is a symmetric convex body. First, we provide an estimate for the polar of the bodies in question: 
 \begin{eqnarray*}
   \Vol_d(\Pi_F(K_{\textrm{sym}})^{\circ})^{1/d} &\stackrel{(1)}{\leq}& \Vol_d((K^{\circ} - K^{\circ}) \cap F)^{1/d} \quad \quad (*) \\
                                       &\stackrel{(2)}{\lesssim}& \frac{n}{d} \cdot \max_{x \in \setR^n} \left\{ \Vol_d(K^{\circ} \cap (F+x))^{1/d} \right\} \\
   &\stackrel{(3)}{\leq}& \Big(\frac{n}{d}\Big)^2 \cdot \Vol_d(K^{\circ} \cap F)^{1/d} 
  \end{eqnarray*}
  Here we use in (1) that $\Pi_F(K_{\textrm{sym}})^{\circ} = K_{\textrm{sym}}^{\circ} \cap F = (K \cap (-K))^{\circ} \cap F = \textrm{conv}(K^{\circ} \cup (- K^{\circ})) \cap F \subseteq (K^{\circ} -K^{\circ}) \cap F$ by Lemma~\ref{lem:PropertiesOfPolarity}. In (2) we use Rudelson's inequality (see Lemma~\ref{thm:DifferenceBodyEstimateRudelson}). In (3) we use Fradelizi's result (Lemma~\ref{lem:FradeliziSectionsThroughBarycenterApxMaximal}) with the assumption that $\textrm{bary}(K^{\circ}) = \bm{0}$.
  Now, we come to the main claim. We have
  \begin{eqnarray*}
    \Vol_d(\Pi_F(K_{\textrm{sym}}))^{1/d} &\stackrel{(4)}{\gtrsim}& \frac{\nu_d^2}{\Vol_d(\Pi_F(K_{\textrm{sym}})^{\circ})^{1/d}} \\
                                &\stackrel{(*)}{\gtrsim}& \Big(\frac{d}{n}\Big)^2 \cdot \frac{\nu_d^2}{\Vol_d(K^{\circ} \cap F)^{1/d}} \\
                                &\stackrel{(5)}{\gtrsim}& \Big(\frac{d}{n}\Big)^3 \cdot \Vol_d( (K^{\circ} \cap F)^{\circ} )^{1/d} \\
    &\stackrel{(6)}{=}& \Big(\frac{d}{n}\Big)^{3} \cdot  \Vol_d( \Pi_F(K) )^{1/d}.
  \end{eqnarray*}
  In (4) we use the Blaschke-Santal\'o-Bourgain-Milman inequality (Theorem~\ref{thm:BSBM}).
  In (5) we apply Lemma~\ref{lem:ApproxSantaloForSectionThroughBarycenter} for the body $Q := K^{\circ}$
  using again that $\textrm{bary}(K^{\circ}) = \bm{0}$. In (6) we use that $(K^{\circ} \cap F)^{\circ} = \Pi_F(K)$ by Lemma~\ref{lem:PropertiesOfPolarity}.
\end{proof}


\end{document}


