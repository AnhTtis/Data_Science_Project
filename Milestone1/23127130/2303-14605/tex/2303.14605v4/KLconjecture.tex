%XXXXXX; whizzy chapter -fmt efmt 
\documentclass[12pt,english]{article}
\usepackage[margin=1.5in]{geometry}

\usepackage{mdwlist}
\usepackage{enumerate}
% Special symbols, etc.
\usepackage{amssymb,amsbsy,latexsym}
\usepackage{amsmath}
\usepackage{graphics, subfigure, float}
\usepackage{fp, calc}
\usepackage{hyperref}
\usepackage{url}

% Encoding settings
\usepackage[T1]{fontenc} 
\usepackage{fourier}
\usepackage{bm}

% AMS Math packages

\usepackage{amscd,amsthm}

% Graphics
%\usepackage[dvips]{graphicx,epsfig,color}
%\usepackage{subfigure}
%\usepackage{pstricks}
%\usepackage{pst-node} 
%\usepackage{pst-plot}
%\usepackage{pst-math}
\usepackage{pst-all}
%\usepackage{pst-3dplot}
\usepackage{pstricks-add}
\usepackage{pst-func}
\newpsobject{showgrid}{psgrid}{subgriddiv=1,griddots=10,gridlabels=6pt}
\usepackage{verbatim, comment}
\usepackage{datetime}

\newtheoremstyle{theorem}{1em}{1em}{\slshape}{0pt}{\bfseries}{.}{ }{}
\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}
\newtheorem{question}{Question}
\newtheorem{homework}{Homework}
\newtheorem{conjecture}{Conjecture}
\newtheorem*{theorem*}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem*{corollary*}{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem*{claim*}{Claim}
\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}
\newtheorem*{remark*}{Remark}
\newtheorem{algorithm}{Algorithm}

\def\rem#1{{\marginpar{\raggedright\scriptsize #1}}}

\providecommand{\setN}{\mathbb{N}}
\providecommand{\setZ}{\mathbb{Z}}
\providecommand{\setQ}{\mathbb{Q}}
\providecommand{\setR}{\mathbb{R}}
\newcommand{\E}{\mathop{\mathbb{E}}}
\newcommand{\ME}{\pazocal{E}}
\newcommand{\Var}{\textrm{Var}}
\newcommand{\Vol}{\mathrm{Vol}}
\newcommand{\rank}{\textrm{rank}}
\renewcommand{\span}{\textrm{span}}
\newcommand{\inte}{\mathrm{int}}
%\newcommand{\span}{\textrm{span}}
\newcommand{\tr}{\textrm{Tr}}
\newcommand{\nd}{\mathrm{nd}}

        \def\drawRect#1#2#3#4#5{
           \FPeval{\x2}{#2 + #4} 
           \FPeval{\y2}{#3 + #5} 
           \pspolygon[#1](#2,#3)(\x2,#3)(\x2,\y2)(#2,\y2)
        }

% ---------------------------------------------------------------------------------------
% THESE 2 LINES HAVE RECENTLY CREATED PROBLEMS AND THE MESSAGE "Too many math alphabets"
%\usepackage{calrsfs} % now e.g. \pazocal{I} gives a nice I
%\DeclareMathAlphabet{\pazocal}{OMS}{zplm}{m}{n}
%----
% QUICK FIX TO KEEP THE \pazocal COMMAND ALIVE
%\def\pazocal#1{\mathcal{#1}}
\def\pazocal#1{#1}
% ---------------------------------------------------------------------------------------

% Takes care, that preview-latex shows a preview of this environment
\usepackage[displaymath,textmath,graphics, subfigure, floats]{preview} %sections,
\PreviewEnvironment{center} 
\PreviewEnvironment{pspicture} 

\makeatother

\title{The Subspace Flatness Conjecture and Faster Integer Programming}
\author{Victor Reis\thanks{University of Washington, Seattle. Email: {\tt voreis@uw.edu}.} \;\; and \; Thomas Rothvoss\thanks{University of Washington, Seattle. Email: {\tt rothvoss@uw.edu}. Supported by NSF CAREER grant 1651861, a David \& Lucile Packard Foundation Fellowship and NSF grant 2318620 \emph{AF: SMALL: The Geometry of Integer Programming and Lattices}.}}
\date{}
%\date{\today}


\begin{document}

\maketitle

\begin{abstract}
  
  In a seminal paper, Kannan and Lov\'asz (1988) considered a quantity $\mu_{KL}(\Lambda,K)$
  which denotes the best volume-based lower bound on the \emph{covering radius} $\mu(\Lambda,K)$ of a convex
  body $K$ with respect to a lattice $\Lambda$. Kannan and Lov\'asz proved that $\mu(\Lambda,K) \leq n \cdot \mu_{KL}(\Lambda,K)$ and the Subspace Flatness Conjecture by Dadush (2012) claims a $O(\log(2n))$ factor suffices, which would match
  the lower bound from the work of Kannan and Lov\'asz.
  We settle this conjecture up to a constant in the exponent by proving that  $\mu(\Lambda,K) \leq O(\log^{3}(2n)) \cdot \mu_{KL} (\Lambda,K)$. Our proof is
  based on the Reverse Minkowski Theorem due to Regev and Stephens-Davidowitz (2017).
  Following the work of Dadush (2012, 2019), we obtain a $(\log(2n))^{O(n)}$-time randomized algorithm to
  solve integer programs in $n$ variables.
  Another implication of our main result is a near-optimal \emph{flatness constant} of $O(n \log^{3}(2n))$.
\end{abstract}

%\tableofcontents

\section{Introduction}

Lattices are fundamental objects studied in various areas of mathematics and computer science.
Here, a \emph{lattice} $\Lambda$ is a discrete subgroup of $\setR^n$. If $B \in \setR^{n \times k}$ is a matrix with linearly independent columns
$b_1,\ldots,b_k$, then we may write a lattice in the form $\Lambda(B) := \{ \sum_{i=1}^k y_ib_i: y_i \in \setZ\}$. In mathematics, lattices are the central object of study in the geometry of numbers with many applications
for example to number theory, see e.g. \cite{KannanLovasz-CoveringMinima-AnnalsOfMath1988}. On the computer science side, lattices found applications for example in lattice-based cryptography~\cite{LWE-RegevJACM09} and cryptanalysis~\cite{KnapsackCryptosystems-Odlyzko1990}. One of the most important
algorithms at least in this area is the \emph{LLL-algorithm} by Lenstra, Lenstra and Lov\'asz~\cite{LLL1982} which finds an approximately orthogonal basis for a given lattice in polynomial time. One of the consequences
of the LLL-reduction is a polynomial time $2^{n/2}$-approximation algorithm for the problem of finding a (nonzero) \emph{shortest vector} in a lattice. We should also mention that
the problem of finding a shortest vector in any norm can be solved in time $2^{O(n)}$ using a variation of the sieving algorithm~\cite{SVP-Sieving-Algo-AKS-STOC2001} while in the Euclidean norm, even
the closest vector to any given target vector can be found in time $2^{O(n)}$~\cite{CVP-Voronoi-Algo-MicciancioVoulgaris-SICOMP2013}. A more general problem with tremendous applications in combinatorial optimization and operations research is the one of finding an integer point in an arbitrary convex body or polytope.
Lenstra~\cite{IPinFixedDim-Lenstra1983} used the then-recent lattice basis reduction algorithm to solve any $n$-variable integer program in time $2^{O(n^2)}$. This was later improved by Kannan~\cite{n-to-n-algos-for-SVP-CVP-Kannan-MOR1987} to $n^{O(n)}$ and then by Dadush~\cite{DadushThesis2012} and by Dadush, Eisenbrand and Rothvoss~\cite{FromApxToExactIP_DER_Arxiv2022} to $2^{O(n)}n^n$.


%$
A parameter appearing in the geometry of numbers is the \emph{covering radius}  
\[
  \mu(\Lambda,K) := \min\big\{ r \geq 0 \mid \Lambda + rK = \textrm{span}(\Lambda) \big\}
\]
of a lattice $\Lambda \subseteq \setR^n$ with respect to a compact convex set $K \subseteq \setR^n$ with $\textrm{span}(\Lambda) = \textrm{affine.hull}(K)$. This quantity seems to be substantially harder computationally, in the sense
that the question whether $\mu(\Lambda, K)$ is at least/at most a given threshold seems to be neither in
${\bf NP}$ nor  in ${\bf coNP}$. In terms of approximating $\mu(\Lambda,K)$, one can quickly observe that one has the lower bound
of $\mu(\Lambda,K) \geq (\frac{\det(\Lambda)}{\Vol_n(K)})^{1/n}$, simply  because for $r<(\frac{\det(\Lambda)}{\Vol_n(K)})^{1/n}$, the average density of the translates $\Lambda + rK$ is less than 1.
However, this lower bound may be arbitrarily far off the real covering radius, for example if $\Lambda = \setZ^2$
and $K = [-\frac{1}{M},\frac{1}{M}] \times [-M,M]$ with $M \to \infty$.
On the other hand, for any subspace $W \subseteq \setR^n$ one trivially has $\mu(\Lambda,K) \geq \mu(\Pi_W(\Lambda),\Pi_W(K))$,
where $\Pi_W$ is the orthogonal projection into $W$. Hence, following Kannan and Lov\'asz~\cite{KannanLovasz-CoveringMinima-AnnalsOfMath1988}, one might instead consider the best volume based lower bound for any projection, i.e.
\[
 \mu_{KL}(\Lambda,K) := \max_{\substack{W \subseteq \textrm{span}(\Lambda)\textrm{ subspace} \\ d := \dim(W)}} \Big(\frac{\det(\Pi_W(\Lambda))}{\Vol_d(\Pi_W(K))}\Big)^{1/d}.
\]
Kannan and Lov\'asz~\cite{KannanLovasz-CoveringMinima-AnnalsOfMath1988} indeed provide an upper bound of
% Kannan Lovasz (Annals, 1988): Cor 3.11 on page 593
\[
 \mu_{KL}(\Lambda,K) \leq \mu(\Lambda,K) \leq n \cdot \mu_{KL}(\Lambda,K).
\]
On the other hand, they also construct a simplex $K \subseteq \setR^n$ for which $\mu(\setZ^n,K) \geq \Omega(\log(2n)) \cdot \mu_{KL}(\setZ^n,K)$ holds.
Dadush~\cite{DadushThesis2012} states the following conjecture, attributing it to Kannan and Lov\'asz~\cite{KannanLovasz-CoveringMinima-AnnalsOfMath1988}: %  The following is attributed to (it may be found explicitly stated in )


\begin{conjecture}[Subspace Flatness Conjecture] \label{conj:KL}% Conjecture 7.4.5 in Dadush's 2012 thesis
  For any full rank lattice $\Lambda \subseteq \setR^n$ and any convex body $K \subseteq \setR^n$ one has
\[
 \mu_{KL}(\Lambda,K) \leq \mu(\Lambda,K) \leq O(\log(2n)) \cdot \mu_{KL}(\Lambda,K).
\]
\end{conjecture}
%Intuitivly, the conjecture states that for the covering radis
Dadush also realized the tremendous implications of this conjecture to optimization and showed that it would imply a $O(\log(2n))^n$-time algorithm to solve $n$-variable integer programs, assuming that the subspace $W$ attaining $\mu_{KL}(\Lambda,K)$ could also be found in the same time.
Later, Dadush and Regev~\cite{TowardsReverseMinkowskiDadushRegevFOCS16} conjectured a \emph{Reverse Minkowski-type Inequality}, which intuitively says that
any lattice without dense sublattices should contain only few short vectors. Among other applications,
they proved that this conjecture would
imply Conjecture~\ref{conj:KL} (with some logarithmic loss) at least for the case that $K$ is an ellipsoid.
%For the longest time, progress on the Subspace Flatness Conjecture seemed elusive, until 
%more recently, Regev and Stephens-Davidowitz proved the following rather deep result: %~\cite{Regev-SD-ReverseMinkowskiTheoremSTOC17} that is the crucial ingredient for our argument is as follows: % have proven the following:
The conjecture of \cite{TowardsReverseMinkowskiDadushRegevFOCS16} was then resolved by Regev and Stephens-Davidowitz~\cite{Regev-SD-ReverseMinkowskiTheoremSTOC17}
with a rather ingenious proof. More precisely, they prove the following: 
%proved the following rather deep result:
\begin{theorem}[Reverse Minkowski Theorem~\cite{Regev-SD-ReverseMinkowskiTheoremSTOC17}] \label{thm:ReverseMinkowskiTheorem}
  Let $\Lambda \subseteq \setR^n$ be a lattice that satisfies $\det(\Lambda') \geq 1$ for all sublattices $\Lambda' \subseteq \Lambda$.
  Then for a large enough constant $C>0$ and $s = C\log(2n)$ one has  $
   \rho_{1/s}(\Lambda) \leq \frac{3}{2}
  $.
\end{theorem}
Here, one has $\rho_t(x) := \exp(-\pi \|x/t\|_2^2)$ where $t>0$ and for a discrete set $S \subseteq \setR^n$ we abbreviate $\rho_t(S) := \sum_{x \in S} \rho_t(x)$. To understand the power of this result compared to classical arguments,
note that from $\det(\Lambda') \geq 1$ for all $\Lambda' \subseteq \Lambda$ one can derive that each vector $x \in \Lambda \setminus \{ \bm{0}\}$ has length $\|x\|_2 \geq 1$ and so by a standard packing argument
we know that for any $r \geq 1$ one has $|\Lambda \cap rB_2^n| \leq (3r)^n$, which is exponential in $n$. On the other hand, again under the assumption that $\det(\Lambda') \geq 1$ for all $\Lambda' \subseteq \Lambda$, the Reverse Minkowski Theorem implies that
$|\Lambda \cap rB_2^n| \leq \exp( \Theta(\log^2(2n)) \cdot r^2)$ which is quasi-polynomial in $n$.
%Intuitively, the Reverse Minkowski Theorem says that any lattice without dense sublattices has only few short
%vectors. %But using a connection shown earlier by Dadush~\ref{TowardsReverseMinkowskiDadushRegevFOCS16},
% Theorem~\cite{thm:ReverseMinkowskiTheorem} also
% Moreover, Theorem~\ref{thm:ReverseMinkowskiTheorem} also implies that the Subspace Flatness Conjecture holds true whenever $K$ is an ellipsoid (with a somewhat weaker bound of $O(\log^{3/2} n)$).
Also, \cite{Regev-SD-ReverseMinkowskiTheoremSTOC17} tighten the reduction to the Subspace Flatness Conjecture and
show that it holds for any ellipsoid with a factor of  $O(\log^{3/2}(2n))$.
While for any convex body $K$, there is an ellipsoid $\pazocal{E}$ and a center $c$ so that $c+\pazocal{E} \subseteq K \subseteq c+n\pazocal{E}$~\cite{John1948}, this factor of $n$ is the best possible, and hence there does
not seem to be a blackbox reduction from the general case of Conjecture~\ref{conj:KL} to the one of ellipsoids. % does not
%offer a blackbox improvement for arbitrary convex bodies.



\subsection{Our contribution}

Our main result is as follows:
\begin{theorem} \label{thm:KLConj} 
  For any full rank lattice $\Lambda \subseteq \setR^n$ and any convex body $K \subseteq \setR^n$
  one has
  \[
   \mu_{KL}(\Lambda,K) \leq \mu(\Lambda,K) \leq O(\log^3 (2n)) \cdot \mu_{KL}(\Lambda,K).
  \]
\end{theorem}
We will break the proof into two parts that can be found in Section~\ref{sec:WholeMainProof}.
Our result is constructive in the following sense: %\rem{V: Should we state this more generally in terms of an arbitrary convex body?}
\begin{theorem} \label{thm:FindingSubspaceIn2ToN} %\rem{T: I think it should be $O(\log^8(n))$ here..?}
  Given a full rank lattice $\Lambda := \Lambda(B)$ and a convex body $K \subseteq \setR^n$ with $c+r_0B_2^n \subseteq K \subseteq r_1 B_2^n$, there is a randomized
  algorithm to find a subspace $W \subseteq \setR^n$ with $d := \dim(W)$ so that
  \[
    \mu(\Lambda,K) \leq O(\log^4 (2n)) \cdot \Big(\frac{\det(\Pi_W(\Lambda))}{\Vol_d(\Pi_W(K))}\Big)^{1/d}.
  \]
  The running time of that algorithm is $2^{O(n)}$ times a polynomial in $\log(\frac{1}{r_0})$, $\log(r_1)$ and in the encoding length of $B$.
\end{theorem}
Here, a separation oracle suffices for $K$. See Section~\ref{sec:FindingSubspace} for a proof.
Following the framework layed out by Dadush~\cite{DadushThesis2012}, this implies a faster algorithm
to find a lattice point in a convex body:
%for integer programming: 
\begin{theorem} \label{thm:SolvingIPinLogNtoN}
%  Given $A \in \setR^{m \times n}$ and $b \in \setR^m$ as input, there is a randomized algorrithm that
%  with high probability finds a point in $K \cap \setZ^n$ with $K := \{ x \in \setR^n \mid Ax \leq b\}$
%  or correctly decides that there is none. The running time is $O(\log n)^n$ times a polynomial in the
%  encoding length of $A$ and $b$.
  Given a convex body $K \subseteq r B_2^n$ represented by a separation oracle and a lattice $\Lambda = \Lambda(B)$, there is a randomized
  algorithm that with high probability finds a point in $K \cap \Lambda$ or  correctly decides that there is none.
  The running time is  $(\log(2n))^{O(n)}$ times a polynomial in $\log(r)$ and the encoding length of $B$.
\end{theorem}
The proof can be found in Section~\ref{sec:IP}. %For convenience, we rephrase Theorem~\ref{thm:SolvingIPinLogNtoN} without the terminology of a separation oracle:
Applying Theorem~\ref{thm:SolvingIPinLogNtoN} to integer programming we obtain the following: 


\begin{theorem} \label{thm:SolvingExplicitIPinLogNtoN}
  Given $A \in \setQ^{m \times n}$, $b \in \setQ^m$ and $c \in \setQ^n$, the integer linear program
  $
  \max\{ c^Tx \mid Ax \leq b, x \in \setZ^n\}
  $
  can be solved in time $(\log(2n))^{O(n)}$ times a polynomial in the encoding length of $A$, $b$ and $c$.
\end{theorem}
An immediate consequence of our main result (Theorem~\ref{thm:KLConj}) is that $K$ can be replaced by a larger \emph{symmetric} body without decreasing the covering radius significantly:
\begin{theorem} \label{thm:CoveringRadiusKvsKminusK} %\rem{T: Got an upgrade from Cor to Thm}
  For any full rank lattice $\Lambda \subseteq \setR^n$ and any convex body $K \subseteq \setR^n$ one has
  \[\mu(\Lambda,K-K) \leq \mu(\Lambda,K) \leq O(\log^3(2n)) \cdot \mu(\Lambda,K-K).\]
\end{theorem}
Another consequence is that the \emph{flatness constant} in dimension $n$ is bounded by $O(n\log^{3}(2n))$,
which is an improvement from the previously known bound of $O(n^{4/3} \log^{O(1)} (2n))$ obtained by combining the result of Rudelson~\cite{Rudelson1998DistancesBN} with \cite{BanaszczykLitvakPajorSzarekMOR99Flatness}. % More generally, we show the following holds:
\begin{theorem} \label{thm:FlatnessConstant}
  For any convex body $K \subseteq \setR^n$ and any full rank lattice $\Lambda \subseteq \setR^n$ one has
  \[
   \mu(\Lambda,K) \cdot \lambda_1(\Lambda^{*}, (K-K)^{\circ}) \leq O(n \log^{3}(2n)).
  \]
\end{theorem}
It is well known that Theorem~\ref{thm:FlatnessConstant} can also be rephrased in the following convenient form:
\begin{corollary} \label{cor:FlatnessConstantSimple}
  Let $K \subseteq \setR^n$ be a convex body with $K \cap \setZ^n = \emptyset$.
  Then there is a vector $c \in \setZ^n \setminus \{ \bm{0}\}$ so that at most $O(n \log^{3}(2n))$ many hyperplanes of the form $\langle c, x \rangle = \delta$ with $\delta \in \setZ$ intersect $K$.
\end{corollary}
%More precisely, for any convex body $K \subseteq \setR^n$ with $K \cap \setZ^n = \emptyset$, there is a direction $c \in \setZ^n \setminus \{ \bm{0}\}$ so that at most $O(n \log^{3}(2n))$ many hyperplanes of the form $\langle c, x \rangle = \delta$ with $\delta \in \setZ$ intersect $K$.
We will prove Theorem~\ref{thm:CoveringRadiusKvsKminusK}, Theorem~\ref{thm:FlatnessConstant}
and Corollary~\ref{cor:FlatnessConstantSimple} in Section~\ref{sec:Implications}.


\section{Preliminaries}

In this section, we introduce the tools that we rely on later. We write $A \lesssim B$ if there is a universal constant
$C>0$ so that $A \leq C \cdot B$ holds. We write $A \asymp B$ if both $A \lesssim B$ and $B \lesssim A$ hold.

\subsection{Lattices}
%Recall that the \emph{discrete Gaussian} is $\rho_s(x) := \exp(-\pi\|x/s\|_2^2)$ where $s>0$ and $x \in \setR^n$.
%A rather deep result of Regev and Stephens-Davidowitz~\cite{Regev-SD-ReverseMinkowskiTheoremSTOC17} that is the crucial ingredient for our argument is as follows: % have proven the following:
%\begin{theorem}[Reverse Minkowski Theorem~\cite{Regev-SD-ReverseMinkowskiTheoremSTOC17}] \label{thm:ReverseMinkowskiTheorem}
%  Let $\Lambda \subseteq \setR^n$ be a lattice that satisfies $\det(\Lambda') \geq 1$ for all sublattices $\Lambda' \subseteq \Lambda$.
%  Then for a large enough constant $C>0$ and $t = C\log(n)$ one has  $   \rho_{1/t}(\Lambda) \leq \frac{3}{2} $.
%\end{theorem}
For a lattice $\Lambda = \Lambda(B)$ given by a matrix $B \in \setR^{n \times k}$ with linearly independent
columns, we define the \emph{rank} as $\rank(\Lambda) := k = \dim(\textrm{span}(\Lambda))$ and the \emph{determinant}
as $\det(\Lambda) = \sqrt{\det_k(B^TB)}$. A lattice $\Lambda \subseteq \setR^n$ with $\rank(\Lambda)=n$ has \emph{full rank}.
%As noted earlier, the \emph{discrete Gaussian} is $\rho_s(x) := \exp(-\pi\|x/s\|_2^2)$ where $s>0$ and $x \in \setR^n$. For a discrete set $S \subseteq \setR^n$ we write $\rho_s(S) = \sum_{x \in S} \rho_s(x)$. % <-- already define this earlier.
For a lattice $\Lambda \subseteq \setR^n$, we define the
\emph{dual lattice} as $\Lambda^* := \{ x \in \textrm{span}(\Lambda) \mid \left<x,y\right> \in \setZ \; \forall y \in \Lambda \}$.
Recall that $\det(\Lambda) \cdot \det(\Lambda^*) = 1$.
%The following is a
%standard fact: \rem{V: Should we just cite your lecture notes or is there an older source for this?}
A consequence of the \emph{Poisson Summation Formula} is as follows: 
\begin{lemma} \label{lem:GeneralLatticeShiftApx}
  For any full rank lattice $\Lambda \subseteq \setR^n$, vector $u \in \setR^n$ and any $s>0$ one has
  \[
   |\rho_s(\Lambda + u)-s^n\det(\Lambda^*)| \leq s^n\det(\Lambda^*) \cdot \rho_{1/s}(\Lambda^* \setminus \{\bm{0}\}).
  \]
\end{lemma}

A set $K \subseteq \setR^n$ is called a \emph{convex body} if it is convex, compact (i.e. bounded and closed) and
has a non-empty interior $\textrm{int}(K)$. A set $Q$ is called \emph{symmetric} if $-Q=Q$.
For a symmetric convex set $Q$, the norm $\|x\|_Q$ is defined as the least scaling $r \ge 0$ so that $x \in rQ$. For a lattice $\Lambda$ and a symmetric convex body $Q$ we denote
the length of the shortest vector by
\[
  \lambda_1(\Lambda,Q) := \min_{x \in \Lambda \setminus \{ \bm{0}\}} \|x\|_Q.
\]
Later we will also need a classical bound on short vectors in a lattice:

\begin{theorem}[Minkowski's First Theorem] \label{thm:Minkowski}
Let $\Lambda \subseteq \setR^n$ be a full rank lattice and $Q \subseteq \setR^n$ be a symmetric convex body. Then $\lambda_1 (\Lambda, Q) \le 2 \Big(\frac{\det(\Lambda)}{\Vol_n(Q)}\Big)^{1/n}$.
\end{theorem} 

We recommend the excellent notes of Regev~\cite{RegevLectureNotes2009} for background.


\subsection{Stable lattices and the canonical filtration}

A subspace $W \subseteq \setR^n$ is a \emph{lattice subspace} of a lattice $\Lambda \subseteq \setR^n$
if $\textrm{span}(W \cap \Lambda) = W$. Similarly, a sublattice $\Lambda' \subseteq \Lambda$ is called
\emph{primitive} if there is a subspace $W$ with $\Lambda \cap W = \Lambda'$.
For a lattice $\Lambda$ and a primitive sublattice $\Lambda' \subseteq \Lambda$, we define
the \emph{quotient lattice} as $\Lambda / \Lambda' := \Pi_{\textrm{span}(\Lambda')^{\perp}}(\Lambda)$.
In many ways one can imagine that the quotient operation
factors $\Lambda$ into two lattices $\Lambda'$ and $\Lambda / \Lambda'$.
In particular $\Lambda'$ and $\Lambda / \Lambda'$ are orthogonal and $\det(\Lambda) = \det(\Lambda') \cdot \det(\Lambda / \Lambda')$. 

A lattice $\Lambda \subseteq \setR^n$ is called \emph{stable} if $\det(\Lambda)=1$
and $\det(\Lambda') \geq 1$ for all sublattices $\Lambda' \subseteq \Lambda$. That means a stable lattice does not
contain any sublattice that is denser than the lattice itself. One can easily verify that
for example $\setZ^n$ is stable.
We denote  $\textrm{nd}(\Lambda) := \det(\Lambda)^{1/\textrm{rank}(\Lambda)}$ as the \emph{normalized determinant}.
One can prove that the extreme points of the 2-dimensional
convex hull of the points $\big\{ (\textrm{rank}(\Lambda'), \ln(\det(\Lambda'))) \mid \textrm{sublattice }\Lambda' \subseteq \Lambda \big\}$ correspond to a unique chain of nested sublattices $\{ \bm{0}\} = \Lambda_0 \subset \Lambda_1 \subset \ldots \subset \Lambda_k = \Lambda$. That chain is called the \emph{canonical filtration}. It is useful to observe that each $\Lambda_i$ in this sequence is the unique densest sublattice of $\Lambda$ with given dimension $\rank(\Lambda_i)$.
Moreover, the quotient lattices $\Lambda_i / \Lambda_{i-1}$ are all scalars of a stable lattice and
one can prove that $\textrm{nd}(\Lambda_i / \Lambda_{i-1})$ are strictly increasing in $i$.
We refer to the thesis of~\cite{PhDThesisStephens-Davidowitz2017} for details.

 \iftrue
 \begin{center}
   \psset{unit=0.8cm}
  \begin{pspicture}(0,-1.8)(5,3.6)
     \pnode(0,0){A0}
    \pnode(1,-1){A1}
    \pnode(2,-1.5){A2}
    \pnode(4,-0.25){A3}
    \pnode(5,1){4}
    \pnode(6,3){L5}

    \cnode*(0,0){2.5pt}{L0}
    \cnode*(1,-1){2.5pt}{L1}% was (1,0.1)
    \cnode*(2,-1.5){2.5pt}{L2}% was (2,0.25)
    \cnode*(4,-0.25){2.5pt}{L3}% was (4,0.8)
  %  \cnode*(5,1){2.5pt}{L4}
    \cnode*(6,2){2.5pt}{L5}% was (6,2)
    \pspolygon[fillstyle=solid,fillcolor=lightgray,linestyle=none](0,2)(L0)(L1)(L2)(L3)(L5)
    \psline(0,2)(L0)(L1)(L2)(L3)(L5)
    \psaxes[ticks=none,labels=none,arrowsize=5pt]{->}(0,0)(0,-1)(6,3.5) \rput[c](6,8pt){$\textrm{rank}(\Lambda')$}
    \psdots(L0)(L1)(L2)(L3)(L5)
    \rput[l](5pt,3.5){$\ln(\det(\Lambda'))$}
    \nput{180}{L0}{$\Lambda_0 = \{ \bm{0}\}$}
%    \nput{-90}{L1}{$\Lambda_1$}
    \nput{-90}{L2}{$\Lambda_{i-1}$}
    \nput{-60}{L3}{$\Lambda_i$}
    \nput{0}{L5}{$\Lambda_k=\Lambda$}
    %\psplot[algebraic=true,linestyle=dashed,linecolor=blue]{0}{6}{0.275*x-0.3}% slope: (0.8-0.25)/2=0.275
  \end{pspicture}
\end{center}
\fi

It will be useful to replace the canonical filtration by an \emph{approximate} filtration where
the normalized determinants grow exponentially. 
% quotient lattices of similar normalized determinant are grouped together.
We make the following definition:
\begin{definition}
  We call a lattice $\Lambda \subseteq \setR^n$ \emph{$t$-stable} with $t \geq 1$ if
  the following holds:
  \begin{enumerate}
  \item[(I)] For any sublattice $\tilde{\Lambda}  \subseteq \Lambda$ one has  $\nd(\tilde{\Lambda}) \geq t^{-1}$.
  \item[(II)] For any sublattice $\tilde{\Lambda}  \subseteq \Lambda^*$ one has   $\nd(\tilde{\Lambda}) \geq t^{-1}$.
  \end{enumerate}
\end{definition}
Note that a lattice is 1-stable if and only if it is stable. We can similarly define $t$-stable filtrations:
\begin{definition}
  Given a lattice $\Lambda \subseteq \setR^n$, we call a sequence $\{ \bm{0}\} = \Lambda_0 \subset \ldots \subset \Lambda_{k} = \Lambda$ a \emph{$t$-stable filtration} of $\Lambda$ if the following holds:
  \begin{enumerate}
  \item[(a)] The normalized determinants $r_i := \nd(\Lambda_i/\Lambda_{i-1})$ satisfy
    $r_1 < \ldots < r_{k}$.
  \item[(b)] The lattices $\frac{1}{r_i} (\Lambda_i / \Lambda_{i-1})$ are $t$-stable for all $i=1,\ldots,k$.
  \end{enumerate}
  We call a $t$-stable filtration \emph{well-separated} if additionally the following holds: 
  \begin{enumerate}
  \item[(c)] One has $r_i \leq \frac{1}{2} r_{i+2}$ for all $i=1,\ldots,k-2$.
  \end{enumerate}
%  we say that the sequence is a .
\end{definition}


For example, the canonical filtration is $1$-stable. It turns out we can make any $t$-stable filtration well-separated:
%We will need a version of $t$-stable filtrations where the normalized determinants are decaying exponentially:
  %The notion is also symmetric:
%\begin{lemma}
%A lattice $\Lambda$ is $t$-stable if and only if $\Lambda^*$ is $t$-stable.
%\end{lemma}
%Now we come to the construction of an approximate filtration:
% ----
% VERSION FROM THE FOCS'23 SUBMISSION:
% \begin{theorem}\label{thm:ApproxFilt}
%   Given a lattice $\Lambda \subseteq \setR^n$ and a $t$-stable filtration $\{ \bm{0}\} = \Lambda_0 \subset \ldots \subset \Lambda_{k} = \Lambda$, in polynomial time we can compute an increasing subsequence of indices $0 = \ell(0) < \ldots < \ell(\tilde{k}) = k$ so that
%     \begin{enumerate}
%   \item[(a')] For $\tilde{r}_i := \nd(\Lambda_{\ell(i)} / \Lambda_{\ell(i-1)})$ one has $\tilde{r}_1 < \ldots < \tilde{r}_{\tilde{k}}$ and $\tilde{r}_i \leq \frac{1}{2} \tilde{r}_{i+2}$ for all $i$.
%   \item[(b')] The lattices $\frac{1}{\tilde{r}_i} (\Lambda_{\ell(i)}/\Lambda_{\ell(i-1)})$ are $2t$-stable for all $i = 1,\ldots,\tilde{k}$.
%     \end{enumerate}
%   \end{theorem}
% ----
\begin{theorem}\label{thm:ApproxFilt}
  Given a lattice $\Lambda \subseteq \setR^n$ and a $t$-stable filtration $\{ \bm{0}\} = \Lambda_0 \subset \ldots \subset \Lambda_{k} = \Lambda$, in polynomial time we can compute a $2t$-stable well-separated filtration $\{ \bm{0}\} = \tilde{\Lambda}_0 \subseteq \ldots \subseteq \tilde{\Lambda}_{\tilde{k}} = \Lambda$.
\end{theorem}
We defer the proof to Appendix~\ref{appendix:ApproximateFiltration}. Using the canonical filtration
as input to Theorem~\ref{thm:ApproxFilt} yields:

 \begin{corollary} \label{cor:ExistenceTwoStableWellSeparatedFiltration}
   For any lattice $\Lambda \subseteq \setR^n$, there exists a 2-stable well-separated filtration $\{ \bm{0}\} = \Lambda_0 \subset \ldots \subset \Lambda_{k} = \Lambda$.
 % For any lattice $\Lambda \subseteq \setR^n$, there exists a sequence $\{ \bm{0}\} = \Lambda_0 \subset \ldots \subset \Lambda_{k} = \Lambda$
 %  so that
 %  \begin{enumerate}
 %  \item[(a)] The normalized determinants $r_i := \det(\Lambda_i/\Lambda_{i-1})^{1/\textrm{rank}(\Lambda_i/\Lambda_{i-1})}$ satisfy
 %    $r_1 < r_2 < \ldots < r_{k}$ and $r_i \leq \frac{1}{2} r_{i+2}$ for all $i=1,\ldots,k-2$.
 %  \item[(b)] The lattices $\frac{1}{r_i} (\Lambda_i / \Lambda_{i-1})$ are $2$-stable for all $i=1,\ldots,k$.
 %  \end{enumerate}
\end{corollary}

%The proof that we give here is slightly incomplete, but presumably it could be made formal:




We collect a few more properties of $t$-stable lattices: %\rem{T: Will this be needed with general $t$-stable?}
\begin{lemma} \label{lem:PropertiesOf2StableLattices} %[Reverse Minkowski Theorem] \label{thm:ReverseMinkowskiTheorem}
  There is a universal constant $C>0$ so that the following holds: 
  Let $\Lambda$ be a $t$-stable lattice for $t \geq 1$. Then for $s = C\log(2n)$ one has
  \begin{enumerate}
  \item[(a)] $\Lambda^*$ is $t$-stable.
  \item[(b)]   $\rho_{1/(st)}(\Lambda) \leq \frac{3}{2}$. % and   $\rho_{1/t}(\Lambda^*) \leq \frac{3}{2}$.
  \item[(c)] For any $u \in \setR^n$ one has $\frac{\rho_{st}(\Lambda + u)}{\rho_{st}(\Lambda)} \geq \frac{1}{3}$.
  \end{enumerate}
\end{lemma}
\begin{proof}
  (a) is immediate from the definition of $t$-stability. Next, let $s = C\log(2n)$ be the parameter from
  Theorem~\ref{thm:ReverseMinkowskiTheorem}.
  For (b), 
  we can see that for any $\Lambda' \subseteq t\Lambda$ one has $\det(\Lambda') \geq 1$
  and so the Reverse Minkowski Theorem (Theorem~\ref{thm:ReverseMinkowskiTheorem}) applies to
  the lattice $t\Lambda$. Then  $\rho_{1/(st)}(\Lambda) = \rho_{1/s}(t\Lambda) \leq \frac{3}{2}$ which gives (b).
  For (c), applying Lemma~\ref{lem:GeneralLatticeShiftApx} twice gives
  \[
   \frac{\rho_{st}(\Lambda + u)}{\rho_{st}(\Lambda)} \geq \frac{(st)^n \det(\Lambda^*) \cdot (1-\rho_{1/(st)}(\Lambda^* \setminus \{\bm{0}\}))}{(st)^n \det(\Lambda^*) \cdot (1+\rho_{1/(st)}(\Lambda^* \setminus \{ \bm{0}\}))} \stackrel{(a)+(b)}{\geq} \frac{1-\frac{1}{2}}{1+\frac{1}{2}} = \frac{1}{3}. \qedhere
  \]
\end{proof}




\subsection{The $\ell$-value and volume estimates\label{sec:LValueAndVolEstimates}}

We review a few results from convex geometry that can all be found in the textbook
by Artstein-Avidan, Giannopoulos and Milman~\cite{AsymptoticGeometricAnalysis-Book2015}.
We denote $B_2^n := \{ x \in \setR^n \mid \|x\|_2 \leq 1\}$
and $S^{n-1} := \{ x \in \setR^n \mid \|x\|_2=1\}$ as the Euclidean ball and sphere, resp.
Let $\nu_n := \Vol_n(B_2^n)$. The \emph{relative interior}
of $K$ is $\textrm{rel.int}(K) := \{ x \in K \mid \exists \varepsilon > 0: (x + \varepsilon \cdot B_2^n) \cap \textrm{affine.hull}(K) \subseteq K \}$.

We define the \emph{mean width} of a convex body $K$ as $w(K) := \E_{\theta \sim S^{n-1}}[\max\{ \left<\theta,x-y\right> : x,y \in K\}]$.
For a compact convex  $K \subseteq \setR^n$ with $\bm{0} \in \textrm{rel.int}(K)$ we denote its \emph{polar} by $K^\circ := \{ y \in \textrm{span}(K) : \langle x, y \rangle \le 1 \ \forall x \in K\}$.
Recall the following basic facts.
\begin{lemma}[Properties of polarity] \label{lem:PropertiesOfPolarity}
  For two convex bodies $K,Q \subseteq \setR^n$ with $\bm{0} \in \textrm{int}(K)$ and $\bm{0} \in \textrm{int}(Q)$ the following holds: 
  \begin{enumerate}
  \item[(a)] One has $(K^{\circ})^{\circ} = K$.
 \item[(b)] For any subspace $F \subseteq \setR^n$ one has $\Pi_F(K)^{\circ} = K^{\circ} \cap F$.
 \item[(c)] One has $(K \cap Q)^{\circ} = \textrm{conv}(K^{\circ} \cup Q^{\circ})$.
 \item[(d)] One has $(-K)^{\circ} = -K^{\circ}$.
 \end{enumerate}
\end{lemma}


We write $N(\bm{0},I_n)$ as the standard Gaussian distribution on $\setR^n$.  The \emph{$\ell$-value} of a symmetric convex $Q \subseteq \setR^n$ is defined as
\[
  \ell_Q = \E_{x \sim N(\bm{0},I_n)}[\|x\|_Q^2]^{1/2}.
\]
One may think of $\ell_Q$ as the ``average thinness'' of $Q$. It turns out that the $\ell$-value is also related to the mean width. To see this, note that $\| \cdot \|_{Q^{\circ}}$ is the \emph{dual norm}
to $\| \cdot \|_Q$, i.e. for all $x \in \setR^n$ one has
$
  \|x\|_{Q^{\circ}} = \max\{ \left<x,y\right> :  y \in Q \}
$.
Then
\begin{equation} \label{eq:LQPolarVsWidth}
  \ell_{Q^{\circ}} = \E_{x \sim N(\bm{0},I_n)}[\|x\|_{Q^{\circ}}^2]^{1/2} = \E_{\bm{x} \sim N(\bm{0},I_n)}\big[\max\{ \left<x,y\right>^2 : y \in Q\}\big]^{1/2}.
\end{equation}
We can see that the right hand side of \eqref{eq:LQPolarVsWidth} almost matches the definition of $w(Q)$. In fact, one can prove:
\begin{lemma} \label{lem:LValueVsMeanWidth}
  For any symmetric convex body $Q \subseteq \setR^n$ one has  $
     \ell_{Q^{\circ}} \asymp \sqrt{n} \cdot w(Q).
  $
\end{lemma}
For a positive semidefinite matrix $\Sigma$
we write $N(\bm{0},\Sigma)$ as the Gaussian with mean $\bm{0}$ and covariance matrix $\Sigma$
and for a subspace $U \subseteq \setR^n$ we write $I_U$ as the identity matrix on that subspace. 
Occasionally we will need to refer to the $\ell$-value of a compact symmetric convex set $Q$ that
is not necessarily full-dimensional. In that case we extend the definition to $ \ell_Q = \E_{x \sim N(\bm{0},I_{\textrm{span}(Q)})}[\|x\|_Q^2]^{1/2}$. %Note that for any symmetric convex body $Q$ one has $\ell_Q = \Theta(\sqrt{n}) \cdot w(Q^{\circ})$. 
%\rem{T: Maybe let's use $K$ for (non-symmetric bodies and $Q$ for symmetric. Also it seems we only need $\ell_Q$ only for symmetric bodies. So I am restricting some lemma to the symmetric case then. It might be confusing for some readers if  $\| \cdot \|_Q$ happens to not be a norm.}
%$


We say that a symmetric convex body $Q$ is in \emph{$\ell$-position} if $\ell_Q \cdot \ell_{Q^{\circ}} \leq O(n \log(2n))$.
One of the most powerful tools in convex geometry is that every symmetric convex body can indeed be brought into $\ell$-position: %the fact that any symmetric convex body can be linearly transformed
%so that in terms of the average width, it comes within a logarithmic factor of the Euclidean ball:
\begin{theorem}[Figiel, Tomczak-Jaegerman, Pisier] \label{thm:PisierRescaling}
For any symmetric convex body $Q \subseteq \setR^n$, there is an invertible linear map $T : \setR^n \to \setR^n$ so that $\ell_{T(Q)} \cdot \ell_{(T(Q))^{\circ}} \leq O(n\log(2n))$. 
\end{theorem}
By Lemma~\ref{lem:LValueVsMeanWidth}, the conclusion of Theorem~\ref{thm:PisierRescaling} is equivalent to
$w(T(Q)) \cdot w(T(Q)^{\circ}) \leq O(\log(2n))$. Moreover one can prove that for any symmetric convex body $Q$ one has $w(Q) \cdot w(Q^{\circ}) \gtrsim w(B_2^n)^2 \gtrsim 1$. Then one can interpret Theorem~\ref{thm:PisierRescaling} as every symmetric convex body can be linearly transformed so that in terms of mean width and
average thinness it is within a $O(\log(2n))$-factor of the Euclidean ball. For the sake of comparison,
we note that the bound that could be obtained via the more classical John's Theorem~\cite{John1948} would be of the order of $\sqrt{n}$. We would like to point out that Theorem~\ref{thm:PisierRescaling} is only known for symmetric convex bodies, and it is open to what extent it generalizes to the non-symmetric case.

%So the map $A$ in Theorem~\ref{thm:PisierRescaling} also satisfies $\ell_{A(K)} \cdot \ell_{A(K)^{\circ}} \leq O(n \log n)$.
%Recall that $\ell_{B_2^n} = \Theta(\sqrt{n})$ and $\ell_{\sqrt{n}B_2^n} = \Theta(1)$.
We state two estimates concerning monotonicity of the $\ell$-value that will be crucial for our later arguments:
\begin{lemma} \label{lem:MonotonicityLValue}
  Let $Q \subseteq \setR^n$ be a symmetric convex body. Then for any subspace $U \subseteq \setR^n$, one has $\ell_{Q \cap U} \leq \ell_Q$.
\end{lemma}
\begin{proof}
  Indeed, one has
\[
  \ell^2_Q = \mathbb{E}_{z \sim N(\bm{0}, I_U)}\big[ \mathbb{E}_{y \sim N(\bm{0}, I_{U^\perp})} [\|z + y\|_Q^2]\big] \geq \mathbb{E}_{z \sim N(\bm{0}, I_U)} \big[\big\| z + \underbrace{\mathbb{E}_{y \sim N(\bm{0}, I_{U^\perp})} [y]}_{=\bm{0}}\big\|_Q^2\big] = \ell^2_{Q \cap U},
\]
where the inequality follows from Jensen's inequality and the convexity of $y \mapsto \|z + y\|_Q^2$.
\end{proof}

\begin{lemma} \label{lem:LvalueOfProjectedIntersection} 
  Let $Q \subseteq \setR^n$ be a symmetric convex body. For any subspaces $V \subset W \subseteq \setR^n$,  %with $d := \dim(W)-\dim(V)$
  one has $\ell_{\Pi_{V^{\perp}}(Q \cap W)} \le \ell_Q$.
\end{lemma}
\begin{proof}
  We have  $\ell_{\Pi_{V^{\perp}}(Q \cap W)} \le \ell_{Q \cap W \cap V^{\perp}} \le \ell_Q$
  using that $\Pi_{V^{\perp}}(Q \cap W) \supseteq Q \cap W \cap V^{\perp}$ and using Lemma~\ref{lem:MonotonicityLValue}. %
%First we show that for any subspace $U \subseteq \setR^n$ of positive dimension one has $\ell_{K \cap U} \le \ell_K$. 
\end{proof}


The following classical result says that among all bodies with identical volume, the Euclidean ball minimizes the mean width. %, see e.g. \cite{AsymptoticGeometricAnalysis-Book2015}.
\begin{theorem}[Urysohn Inequality I] \label{thm:UrysohnInequality}
  For any convex body $K \subseteq \setR^n$ one has
  \[
    w(K) \geq 2 \cdot \Big(\frac{\Vol_n(K)}{\Vol_n(B_2^n)}\Big)^{1/n}.
  \]
\end{theorem}
A slight variant of this inequality will be handy for us:
\begin{corollary}[Urysohn Inequality II] \label{cor:UrysohnInequalityII}
For any symmetric convex body $Q \subseteq \setR^n$ one has $\Vol_n(Q)^{1/n} \lesssim \frac{\ell_{Q^{\circ}}}{n}$.
\end{corollary}
\begin{proof}
  Applying Urysohn's Inequality I we obtain
  \[
 \Vol_n(Q)^{1/n} \stackrel{\textrm{Thm~\ref{thm:UrysohnInequality}}}{\lesssim} w(Q) \cdot \underbrace{\Vol_n(B_2^n)^{1/n}}_{\lesssim 1/\sqrt{n}} \stackrel{\textrm{Lem~\ref{lem:LValueVsMeanWidth}}}{\lesssim} \frac{\ell_{Q^{\circ}}}{n}.
\]
Here we use in particular that $\Vol_n(B_2^n) \leq (\frac{2e}{\sqrt{n}})^n$.
\end{proof}

The following can be found e.g. in \cite{AsymptoticGeometricAnalysis-Book2015}, Chapter 8:
\begin{theorem}[Blaschke-Santal\'o-Bourgain-Milman] \label{thm:BSBM}
  For any symmetric convex body $K \subseteq \setR^n$ one has
  \[
    C_1^n \nu_n^2\leq  \Vol_n(K) \cdot \Vol_n(K^{\circ}) \leq C_2^n \nu_n^2,
  \]
  where $C_1,C_2>0$ are constants.
\end{theorem}
% Another auxiliary result that we will need is a bound on the volume of projections. A dual version that will be useful is the following:
% {}
% \begin{theorem}[Theorem 1 in~\cite{Rudelson1998SectionsOT}]\label{thm:RudelsonSection}
% Let $K \subseteq \setR^n$ be a convex body and $W \subseteq \setR^n$ be a $d$-dimensional subspace. Then
% \[
% \Vol_d((K-K) \cap W)^{1/d} \lesssim \min\Big\{\frac{n}{d}, \sqrt{d}\Big\} \cdot \max_{\bm{x} \in \setR^n} \Vol_d ((K+\bm{x}) \cap W)^{1/d}.
% \]
% \end{theorem}
% In order to relate volumes of sections and projections, we use polarity. The Blaschke-Santal\'o inequality and its reverse will be useful: 

% \begin{theorem}\label{lem:Blaschke-Santalo-Inequality}
% For any convex body $K \subseteq \mathbb{R}^n$ there exist constants $C_1$ and $C_2$ so that
% \[C_1 \le \min_{\bm{x} \in \setR^n} \Big(\frac{\Vol_n(K) \cdot \Vol_n((K-\bm{x})^\circ)}{\Vol_n(B^n_2)}\Big)^{1/n} \le C_2. \]
% \end{theorem}
%Moreover, $\Pi_F$ denotes the orthogonal projection into a subspace $F$.
Let $b(K) := \frac{1}{\Vol_n(K)}\int_K x \; dx$ denote the \emph{barycenter} or \emph{centroid} of a convex body $K$. 
We will run into the issue that we need to control the volume of a non-symmetric convex body $K$,
but Theorem~\ref{thm:PisierRescaling} only holds for symmetric ones. A popular strategy in
convex geometry is to translate $K$ so that $b(K) = \bm{0}$ and then consider the
\emph{inner symmetrizer} $K \cap -K$ which by construction is a symmetric convex body contained in $K$ which captures much of the geometry of $K$.
For example a classical result by Milman and Pajor says that $\Vol_n(K \cap -K) \geq 2^{-n}\Vol_n(K)$.
However, in our case we need a more powerful estimate that was proven by Vritsiou~\cite{vritsiou2023regular} in the context of showing the existence of regular $M$-ellipsoids for non-symmetric convex bodies.  
\begin{proposition}[\cite{vritsiou2023regular}, Corollary 11] \label{prop:VolumeOfProjectionVsSymmetrizer}
Let $K \subseteq \setR^n$ be a convex body so that $b(K) = \bm{0}$ and let $F \subseteq \setR^n$ be a $d$-dimensional subspace. 
Then 
\[
 \Vol_d(\Pi_F(K))^{1/d} \lesssim \Big(\frac{n}{d}\Big)^5 \cdot \log\Big(\frac{en}{d}\Big)^2  \cdot \Vol_d(\Pi_F(K \cap -K))^{1/d}.
\]
\end{proposition}
On a previous preprint, we had shown an inequality with better exponent when the body is
centered so that $b(K^{\circ}) = \bm{0}$, i.e. the origin is the \emph{Santal\'o point} of $K$.
However, algorithmically the barycenter is much easier to compute and the exponent only affects
the implicit universal constant in our main result, hence we choose to work with Vritsiou's estimate.
% for a different centering of the body, and using the barycenter simplifies our algorithm.
For the interested reader, the bound with the Santal\'o point as center can be found in v2 on arXiv and also independently in~\cite{vritsiou2023regular}.



We prove a custom-tailored inequality for later:
\begin{lemma} \label{lem:VolOfProjVsLValueOfSymmetrizer}
  Let $K \subseteq \setR^n$ be a convex body with $b(K) = \bm{0}$ and let $F \subseteq \setR^n$ be a $d$-dimensional subspace. Then
  \[(\Vol_d(\Pi_F(K)))^{1/d} \lesssim \Big(\frac{n}{d}\Big)^{6} \cdot \frac{\ell_{(K \cap -K)^{\circ}}}{d}.\]
\end{lemma}
\begin{proof}
  We abbreviate $K_{\textrm{sym}} := K \cap -K$.
  Using the volume estimate from Proposition~\ref{prop:VolumeOfProjectionVsSymmetrizer} with the assumption that the barycenter of $K$ lies at the origin, we obtain
  \begin{eqnarray*}
    (\Vol_d(\Pi_F(K)))^{1/d} &\stackrel{\textrm{Prop~\ref{prop:VolumeOfProjectionVsSymmetrizer}}}{\lesssim}& \Big(\frac{n}{d}\Big)^6 \cdot (\Vol_d(\Pi_F(K_{\textrm{sym}})))^{1/d} \\
    &\stackrel{\textrm{Cor~\ref{cor:UrysohnInequalityII}}}{\lesssim}& \Big(\frac{n}{d}\Big)^6 \cdot \frac{\ell_{(\Pi_F(K_{\textrm{sym}}))^{\circ}}}{d} \\
                             %&\stackrel{\textrm{Thm~\ref{thm:UrysohnInequality}}}{\lesssim}& \Big(\frac{n}{d}\Big)^3 \cdot \frac{w(\Pi_F(K_{\mathrm{sym}}))}{\sqrt{d}} \\
    &\stackrel{\textrm{Lem~\ref{lem:PropertiesOfPolarity}}}{=}& \Big(\frac{n}{d}\Big)^{6} \cdot \frac{\ell_{K_{\textrm{sym}}^{\circ} \cap F}}{d} \\ &\stackrel{\textrm{Lem~\ref{lem:MonotonicityLValue}}}{\leq}& \Big(\frac{n}{d}\Big)^{6} \cdot \frac{\ell_{K_{\textrm{sym}}^{\circ}}}{d}.
  \end{eqnarray*}
  Here we also used the fact that $(\Pi_F(K_{\textrm{sym}}))^{\circ} = K_{\textrm{sym}}^{\circ} \cap F$. 
\end{proof}
  
% Then the main technical lemma is the following:

% \begin{lemma} \label{lem:VolumeOfProjection}
%   Let $K \subseteq \setR^n$ be a convex body with $\bm{0} \in \inte(K)$ and  $\ell_{K \cap -K} \cdot \ell_{(K \cap -K)^{\circ}} \leq O(n \log n)$ and $\ell_{K \cap -K} = \sqrt{n}$. Fix a subspace $W \subseteq \setR^n$ with $d := \dim(W)$. Then one has \\

%   \noindent (a) if $K$ is symmetric then $\Vol_d(\Pi_W(K))^{1/d} \leq O(\log(n)) \cdot \frac{\sqrt{n}}{d}$. \\
%   (b) $\Vol_d(\Pi_W(K))^{1/d} \leq \min \{\frac{n}{d}, \sqrt{d}\} \cdot O(\log(n)) \cdot \frac{\sqrt{n}}{d}$.
% \end{lemma}
% \begin{proof}
% First we prove (a), with $K = K \cap -K$. We apply Urysohn's inequality, so that
% \[
% \Big(\frac{\Vol_d(\Pi_W(K))}{\Vol_d(B^W_2)}\Big)^{1/d} \le w(\Pi_W(K)) \lesssim \frac{1}{\sqrt{d}} \ell_{(\Pi_W(K))^\circ} = \frac{1}{\sqrt{d}} \ell_{K^\circ \cap W} \stackrel{\textrm{Lem}~\ref{lem:LvalueOfProjectedIntersection}}{\leq} \frac{1}{\sqrt{d}} \ell_{K^\circ} \lesssim \frac{\sqrt{n} \log n}{\sqrt{d}}.
% \]
% It remains to recall that $\Vol_d(B^W_2)^{1/d} \lesssim \frac{1}{\sqrt{d}}$.
% For an incomplete proof of (b), we use the same argument, and it remains to argue that
% \[
% \Vol_d(\Pi_W(K))^{1/d} \lesssim \min \Big\{\frac{n}{d} , \sqrt{d}\Big\} \cdot \Vol_d(\Pi_W (K \cap -K))^{1/d}.
% \]
% To compute polars, note for a convex $Q$ with $\bm{0} \in \inte(Q)$ we have $\|\bm{x}\|_{Q^\circ} = \max_{y \in Q} \langle \bm{x} , \bm{y} \rangle$. Therefore $(K \cap -K)^\circ = \mathrm{conv}\{K^\circ, -K^\circ\}$ and $\Pi_W(K)^\circ = K^\circ \cap W$. Further, for any convex $Q$ with $\bm{0} \in \inte(Q)$,
% \[\frac{1}{2} (Q-Q) \subseteq  \mathrm{conv}\{Q, -Q\} \subseteq Q-Q\] 

% Thus by Theorem~\ref{lem:Blaschke-Santalo-Inequality} it's equivalent to show when $\bm{0} \in \inte(K)$, 

% \[\Vol_d((K^\circ-K^\circ) \cap W)^{1/d} \lesssim  \min \Big\{\frac{n}{d} , \sqrt{d}\Big\} \cdot \min_{\bm{x} \in \setR^n} \Vol_d ((K+\bm{x})^\circ \cap W)^{1/d}.\]

% What we do know is that for some $\bm{x} \in \setR^n$,

% \[\Vol_d((K^\circ-K^\circ) \cap W)^{1/d} \lesssim  \min \Big\{\frac{n}{d} , \sqrt{d}\Big\} \cdot \Vol_d ((K^\circ + \bm{x}) \cap W)^{1/d}.\]

% In particular this allows us to get a bound on the volume of $\Pi_W((K^\circ + \bm{x})^\circ)$ for some $\bm{x} \in \setR^n$ instead of $\Pi_W (K)$, which doesn't seem as helpful.
% \end{proof}

%\begin{remark}
%  Actually we only need Lemma~\ref{lem:VolumeOfProjection} if either $d \geq \Omega(\frac{n}{\log(n)})$,
%  or if $d$ is arbitrary but then we have a slack of say $n^{100}$.
%\end{remark}


\subsection{Properties of the covering radius}%$

%For two orthogonal lattices $\Lambda_1,\Lambda_2$, the covering radius with respect to the
%Euclidean norm satisfies $\mu(\Lambda_1 \oplus \Lambda_2)^2 = \mu(\Lambda_1)^2 +\mu(\Lambda_2)^2$.
%For an arbitrary norm $\| \cdot \|_K$, we can still recover a triangle-type inequality.
While the set $K$ may not be symmetric, the sets $\Lambda$ and $\setR^n$ are symmetric, which implies
the following:
\begin{lemma}[Properties of the covering radius] \label{lem:PropertiesOfCoveringRadius}
  Consider a lattice $\Lambda \subseteq \setR^n$ and a compact convex set $K \subseteq \setR^n$ with $\textrm{span}(\Lambda) = \textrm{affine.hull}(K)$. Then
  \begin{enumerate}
  \item[(a)] $\mu(\Lambda,K) = \mu(\Lambda,K+u)$ for all $u \in \mathrm{span}(\Lambda)$.
  \item[(b)] $\mu(\Lambda,K) = \min\{ r \geq 0 \mid (x + rK) \cap \Lambda \neq \emptyset \; \forall x \in \mathrm{span}(\Lambda)\}$.
  \end{enumerate}
\end{lemma}


We need a triangle inequality for the covering radius: 
\begin{lemma} \label{lem:TriangleIneqGenCoveringRadius}
  Let $\Lambda \subseteq \setR^n$ be a lattice and let $\Lambda' \subseteq \Lambda$ be a primitive sublattice.
  Then for any compact convex set $K \subseteq \setR^n$ with $\bm{0} \in \textrm{rel.int}(K)$ and $\textrm{span}(\Lambda)=\textrm{span}(K)$ one has
  \[
    \mu(\Lambda,K) \leq  \mu(\Lambda',K \cap W) + \mu(\Lambda / \Lambda',\Pi_{W^{\perp}}(K)), 
  \]
  where $W := \textrm{span}(\Lambda')$. %Moreover, $\mu(\Lambda/\Lambda',\Pi_{W^{\perp}}(K)) \leq \mu(\Lambda,K)$.
\end{lemma}
\begin{proof}
  W.l.o.g. we may assume that $\Lambda$ has full rank, so $\bm{0} \in \textrm{int}(K)$.
  Following the characterization in Lemma~\ref{lem:PropertiesOfCoveringRadius}.(b), we fix an $x \in \setR^n$.
  For $r_1 := \mu(\Pi_{W^{\perp}}(\Lambda),\Pi_{W^{\perp}}(K))$
  we know that  $\Pi_{W^{\perp}}(x + r_1K) \cap \Pi_{W^{\perp}}(\Lambda) \neq \emptyset$. That means there is a $u_1 \in r_1K$ and a lattice point $y \in \Lambda$   so that $\Pi_{W^{\perp}}(x+u_1) = \Pi_{W^{\perp}}(y)$.
  Next, for $r_2 := \mu(\Lambda \cap W, K \cap W)$ we know that $(x+u_1-y + r_2 \cdot (K \cap W)) \cap (\Lambda \cap W) \neq \emptyset$
  which is equivalent to $(x+u_1 + r_2 \cdot (K \cap W)) \cap (y+(\Lambda \cap W)) \neq \emptyset$.
  Let $u_2 \in r_2 \cdot (K \cap W)$ be the vector so that $x+u_1+u_2 \in \Lambda$. Then $u_1+u_2 \in (r_1+r_2)K$
  by convexity, so $(x + (r_1+r_2) \cdot K) \cap \Lambda \neq \emptyset$.
  \iftrue
\begin{center} %\rem{T: We'll remove the picture later. For now it helps me check the argument.}
\psset{unit=1.6cm}
 \begin{pspicture}(-2,-1)(3,2.5)
    \psline[linewidth=2pt,linecolor=gray](-1.5,0)(2.5,0)\rput[c](2.5,5pt){$W$}
    \psline[linewidth=2pt,linecolor=gray](0,-1)(0,2.5)\rput[l](5pt,2.5){$W^{\perp}$}
    \psline[linestyle=dotted](-1.5,1)(2.5,1)
    \psline[linewidth=2.5pt,linecolor=darkgray](0,1)(0,1.9)\rput[l](3pt,1.7){$r_1\Pi_{W^{\perp}}(K)$}
    \rput[c](1.8,1.4){\pspolygon[fillstyle=solid,fillcolor=lightgray,linecolor=darkgray](0.4,-0.4)(-0.1,0.5)(-0.2,-0.2)}
 %   \rput{-45}(1.8,1.4){\psellipse[fillstyle=solid,fillcolor=lightgray](0,0)(0.55,0.15)}
   \rput[c](0,2){\psdots(-1,0)(0,0)(1,0)(2,0)}
    \psdots(-1,0)(0,0)(1,0)(2,0)
   \psdots(-1.5,1)(-0.5,1)(0.5,1)(1.5,1)(2.5,1)
   \rput[c](0,-2){\psdots(-1.5,1)(-0.5,1)(0.5,1)(1.5,1)(2.5,1)}
%   \psaxes[arrowsize=5pt]{->}(0,0)(-1.5,-1)(2.5,2.5) %\rput[c](2.5,5pt){$$}
   \psline[linestyle=dotted](1.8,1.4)(1.8,0) \rput[c](1.8,-10pt){$\Pi_{W}(x)$}
   \psline[linestyle=dotted](1.8,1.4)(0,1.4) \rput[r](-5pt,1.4){$\Pi_{W^{\perp}}(x)$}
   \cnode[fillstyle=solid,fillcolor=white](1.8,1.4){3pt}{x}\nput[labelsep=2pt]{120}{x}{$x$}
   \multido{\N=-1+1}{4}{\cnode[fillstyle=solid,fillcolor=white](0,\N){3pt}{A}}
   \cnode*(0.5,1){2.5pt}{y2} \nput[labelsep=2pt]{90}{y2}{$y$}
   \cnode[fillstyle=solid,fillcolor=white](0,1){3pt}{p1} \nput[labelsep=2pt]{-135}{p1}{$\Pi_{W^{\perp}}(y)$}
   \pnode(2.2,1){A2}
   \ncline[arrowsize=5pt]{->}{x}{A2}\naput[labelsep=2pt]{$u_1$}
   \cnode*(2.5,1){3pt}{p2}
   \ncline[arrowsize=5pt]{->}{A2}{p2}\nbput[labelsep=2pt]{$u_2$}
   \rput[l](1.9,1.7){$x+r_1K$} % with $r = \mu(\Pi_{W^{\perp}}(\Lambda),\Pi_{W^{\perp}}(K))$}
  \end{pspicture}
\end{center}
\fi
%%%%%%%%
\end{proof}
%\begin{proof} \rem{T: This is needed for non-symmetric convex sets. Proof needs generalization.}
%  Consider any $\bm{x} \in \textrm{span}(\Lambda)$. Let $\tilde{\bm{y}} \in \Pi_{W^{\perp}}(\Lambda)=\Lambda/\Lambda'$ be the vector so that
%  \[
%   \|\Pi_{W^{\perp}}(\bm{x}) - \tilde{\bm{y}}\|_{\Pi_{W^{\perp}}(K)} \leq \mu(\Pi_{W^{\perp}}(\Lambda),\Pi_{W^{\perp}}(K)) %\leq \mu(\Lambda / \Lambda',K)
%  \]
%  Let $\bm{y} \in \Lambda $ be a lattice vector with $\Pi_W(\bm{y}) = \tilde{\bm{y}}$.
%  By property of projection, there is a vector $\bm{a}$ so that $\bm{x}+\bm{a} \in \bm{y} + W$ and $\|\bm{a}\|_K = \|\Pi_{W^{\perp}}(\bm{y})-\Pi_{W^{\perp}}(\bm{x}) \|_{\Pi_{W^{\perp}}(K)} \leq \mu(\Lambda / \Lambda',\Pi_{W^{\perp}}(K))$.
%  Then there is a vector $\bm{b}$ so that $\bm{x}+\bm{a}+\bm{b} \in \bm{y}+\Lambda' \subseteq \Lambda$ with $\|\bm{b}\|_K \leq \mu(\Lambda',K)$.
%  Note that $\mu(\Lambda',K) = \mu(\Lambda',K \cap W)$ and $\|\bm{a}+\bm{b}\|_K \leq \|\bm{a}\|_K+\|\bm{b}\|_K \leq \mu(\Lambda/\Lambda',\Pi_{W^{\perp}}(K)) + \mu(\Lambda',K \cap W)$ by the triangle inequality.
%As the projection of a covering is again a covering, one also has  $\mu(\Pi_{W^{\perp}}(\Lambda),\Pi_{W^{\perp}}(K)) \leq \mu(\Lambda,K)$. % already in the proof of Lemma~\ref{lem:MuAtLeastMuKL} as projections cannot increase the covering radius. 
%\end{proof}


The natural extension of Lemma~\ref{lem:TriangleIneqGenCoveringRadius} to a filtration is as follows: 
\begin{lemma} \label{lem:TriangleIneqForFiltration}
  Let $\Lambda \subseteq \setR^n$ be a lattice with any sequence of sublattices
  $\{ \bm{0} \} = \Lambda_0 \subset \Lambda_1 \subset \ldots \subset \Lambda_k= \Lambda$. Then for any compact convex set $K \subseteq \setR^n$ with $\bm{0} \in \textrm{rel.int}(K)$ and $\textrm{span}(\Lambda)=\textrm{span}(K)$, one has
  \[
  \mu(\Lambda,K) \leq \sum_{i=1}^k \mu\left( \Lambda_i/\Lambda_{i-1}, \Pi_{\textrm{span}(\Lambda_{i-1})^{\perp}}(K \cap \textrm{span}(\Lambda_i)) \right).
  \]
\end{lemma}
\begin{proof}
We can use the previous lemma to show by induction over $i_0=k,k-1,\ldots,1$ that % for any $i_0 \in [k]$,
\[\mu(\Lambda,K) \leq \mu(\Lambda_{i_0 - 1}, K \cap \textrm{span}(\Lambda_{i_0-1})) + \sum_{i=i_0}^k \mu\left( \Lambda_i/\Lambda_{i-1}, \Pi_{\textrm{span}(\Lambda_{i-1})^{\perp}}(K \cap \textrm{span}(\Lambda_i)) \right). \]
Indeed, for $i_0 = k$ this is exactly Lemma~\ref{lem:TriangleIneqGenCoveringRadius}. If it holds for some $i_0 > 1$, then
\begin{eqnarray*}
  \mu(\Lambda_{i_0 - 1}, K \cap \textrm{span}(\Lambda_{i_0-1})) &\le& \mu(\Lambda_{i_0 - 2}, K \cap \textrm{span}(\Lambda_{i_0-2})) + \\
                                                                & & \mu\left( \Lambda_{i_0-1}/\Lambda_{i_0-2}, \Pi_{\textrm{span}(\Lambda_{i_0-2})^{\perp}}(K \cap \textrm{span}(\Lambda_{i_0 -1})) \right), 
\end{eqnarray*}
since $\textrm{span}(\Lambda_{i_0-2}) \subset \textrm{span}(\Lambda_{i_0-1})$. So the claim follows by induction, and taking $i_0 := 1$ yields the statement.
\end{proof}


\subsection{Properties of $\mu_{KL}$}
We also need the following fact: 
\begin{lemma} \label{lem:MonotonicityOfMuKL}
  For any lattice $\Lambda \subseteq \setR^n$, compact convex set $K$ with $\textrm{span}(\Lambda) = \textrm{affine.hull}(K)$
  and subspace $V \subseteq \textrm{span}(\Lambda)$ one has
  $\mu_{KL}(\Pi_{V}(\Lambda),\Pi_{V}(K)) \leq \mu_{KL}(\Lambda,K)$.
\end{lemma}
\begin{proof}
  Let $W \subseteq V$ be the subspace attaining the left side with $\dim W = d$. Then
  \[
 \mu_{KL}(\Pi_{V}(\Lambda),\Pi_{V}(K)) = \Big(\frac{\det(\Pi_W(\Pi_V(\Lambda)))}{\Vol_d(\Pi_W(\Pi_V(K)))}\Big)^{1/d} = \Big(\frac{\det(\Pi_W(\Lambda))}{\Vol_d(\Pi_W(K))}\Big)^{1/d} \leq \mu_{KL}(\Lambda,K),
  \]
  using that $\Pi_W(\Pi_V(x)) = \Pi_W(x)$ for all $x \in \setR^n$ as  $W \subseteq V$.
\end{proof}


\subsection{Approximate stable lattices and the covering radius}

%\begin{definition}
%We say that a symmetric convex body $K$ is in \emph{$\ell$-position} if $w(K) = 1$ and $w(K^{\circ}) \leq O(\log n)$.  
% \end{definition}
Using the Reverse Minkowski Theorem it would not be hard to prove that for any stable lattice $\Lambda \subseteq \setR^n$ one has $\mu(\Lambda,B_2^n) \leq O(\sqrt{n} \log(2n))$. In this section, we show how to generalize this
to  $t$-stable lattices and to general symmetric convex bodies.
For a symmetric convex body $Q$, we consider the following quantity 
\[
 \beta(Q) = \sup_{\Lambda \subseteq \setR^n\textrm{ lattice}} \sup_{u \in \setR^n} \frac{\rho_1((u + \Lambda) \setminus Q)}{\rho_1(\Lambda)}.
\]
Note that always $0 < \beta(Q) \leq 1$. Intuitively, a body $Q$ with $\beta(Q) \ll 1$ is large enough that
for any lattice a substantial fraction of the discrete Gaussian weight has to fall in $Q$.
% We know the following standard estimate:
As part of the celebrated Transference Theorem, Banaszczyk showed how to relate the $\ell$-value of a body to its $\beta$-value: %the following as part of his celebrated transference theorem:
\begin{lemma}[Banaszczyk~\cite{Banaszczyk1996TransferenceTheoremsForGeneralConvexBodies}] \label{lem:BanaszczykLKvsBetaK}
  For any $\varepsilon>0$, there is a $\delta > 0$ so that the following holds: for any
  symmetric convex body $Q \subseteq \setR^n$ with $\ell_Q \leq \delta$ one has $\beta(Q) \leq \varepsilon$.
\end{lemma}
Next, we can get a fairly tight upper bound on the covering radius of a $t$-stable lattice: %\rem{T: Needed for $t$-stable?}
\begin{proposition} \label{prop:CovRadiusOf2StableLattice}
  Let $\Lambda \subseteq \setR^n$ be a full rank lattice that is the $r$-scaling of a $t$-stable lattice and let $Q \subseteq \setR^n$ be a symmetric convex body.
  Then $\mu(\Lambda,Q) \leq O(\log(2n)) \cdot t \cdot r \cdot \ell_Q$.
\end{proposition}
\begin{proof}
  Let $\varepsilon>0$ be a small enough constant that we determine later.
  Let $\delta$ be the constant so that Lemma~\ref{lem:BanaszczykLKvsBetaK} applies (w.r.t. $\varepsilon$).
  The claim is invariant under scaling $Q$, hence we may scale $Q$ so that $\ell_Q \leq \delta$ and consequently $\beta(Q) \leq \varepsilon$.
  We may also scale the lattice so that $\Lambda$ is $t$-stable (i.e. $r=1$). % and in particular $\frac{1}{2} \leq \textrm{nd}(\Lambda) \leq 2$.
  It suffices to prove that under these assumptions,  $\mu(\Lambda,Q) \leq s \cdot t$ where $s := C\log(2n)$
  is the parameter from Lemma~\ref{lem:PropertiesOf2StableLattices}. 
  Now suppose for the sake of contradiction that there is a translate $u \in \setR^n$ so that $(u+\Lambda) \cap stQ = \emptyset$.
  Since $\beta(Q) \leq \varepsilon$, we know that
  \[
    \rho_1\Big( \Big(\frac{u}{st} + \frac{\Lambda}{st}\Big) \setminus Q\Big) \leq \varepsilon \rho_1\Big(\frac{\Lambda}{st}\Big).
  \]
 Multiplying the sets and parameters by $st$ gives
  \[
 \rho_{st}((u+\Lambda) \setminus stQ) \leq \varepsilon \rho_{st}(\Lambda). \quad (*)
  \]
 Using that $\Lambda$ is $t$-stable, we get
  \[
\frac{1}{3}\rho_{st}(\Lambda) \stackrel{\textrm{Lem~\ref{lem:PropertiesOf2StableLattices}}}{\leq} \rho_{st}(u+\Lambda) \stackrel{(u+\Lambda) \cap stQ = \emptyset}{=} \rho_{st}((u + \Lambda) \setminus stQ) \stackrel{(*)}{\leq} \varepsilon \rho_{st}(\Lambda).
  \]
  Then choosing $\varepsilon \in (0, \frac{1}{3})$ gives a contradiction. \qedhere
%   Now we turn to the general case when $K$ is not necessarily symmetric. We may bound
%   \[\mu(\Lambda, K) \le \mu(\Lambda, K \cap -K) \le O(\log n) \cdot \mathrm{nd}(\Lambda) \cdot \ell_{K \cap -K} \le O(\log n) \cdot \ell_K,  \]
%   where in the last inequality we used 
%   \begin{eqnarray*}
% \ell_{K \cap -K}^2 &=& \E_{\bm{x} \sim N(\bm{0}, \bm{I}_n)} [\|\bm{x}\|_{K \cap -K}^2] \\ &=& \E_{\bm{x} \sim N(\bm{0}, \bm{I}_n)} [\max\{\|\bm{x}\|_{K},\|\bm{x}\|_{-K} \}^2] \\ &\le& \E_{\bm{x} \sim N(\bm{0}, \bm{I}_n)} [\|\bm{x}\|_{K}^2 + \|\bm{x}\|_{-K}^2] \le 2 \ell_K^2.\qedhere 
%   \end{eqnarray*}
% 
\end{proof}


\section{Overview}

Goal of this section is to provide the reader with an overview and some intuition concerning the proof of our main result, Theorem~\ref{thm:KLConj}.
First, we want to prove the inequality from Theorem~\ref{thm:KLConj} (with an even better exponent)
in the special case that both the lattice and the body $K$ are well-scaled. We will not actually
use Prop~\ref{prop:MainIneqforWellScaledCase} later in this form, but it will provide us with the
idea for a general proof strategy.
\begin{proposition} \label{prop:MainIneqforWellScaledCase}
  Let $\Lambda \subseteq \setR^n$ be a full rank 2-stable lattice and let $K$ be a convex body with $b(K)=\bm{0}$
  so that $K \cap -K$ is in $\ell$-position. Then $\mu(\Lambda,K) \leq O(\log^2(2n)) \cdot \mu_{KL}(\Lambda,K)$.
\end{proposition}
\begin{proof}
  We denote the inner symmetrizer by $K_{\textrm{sym}}:= K \cap -K$. Then applying the estimate for stable lattices
  from Prop~\ref{prop:CovRadiusOf2StableLattice} we can upper bound the covering radius:
  \[
   \mu(\Lambda,K) \stackrel{K \supseteq K_{\textrm{sym}}}{\leq} \mu(\Lambda,K_{\textrm{sym}}) \stackrel{\textrm{Prop~\ref{prop:CovRadiusOf2StableLattice}}}{\lesssim} \log(2n) \cdot \ell_{K_{\textrm{sym}}}
  \]
  Next, we lower bound $\mu_{KL}(\Lambda,K)$ by simply choosing the subspace $W := \setR^n$ as witness. Then
  \[
   \mu_{KL}(\Lambda,K) \geq \Big(\frac{\det(\Lambda)}{\Vol_n(K)}\Big)^{1/n} \stackrel{(*)}{\gtrsim} \frac{1}{\Vol_n(K_{\textrm{sym}})^{1/n}} \stackrel{\textrm{Cor~\ref{cor:UrysohnInequalityII}}}{\gtrsim} \frac{n}{\ell_{K_{\textrm{sym}}^{\circ}}} \stackrel{\ell\textrm{-position}}{\gtrsim} \frac{\ell_{K_{\textrm{sym}}}}{\log(2n)},
 \]
 where we use in $(*)$ that $\det(\Lambda) \geq 2^{-n}$ and $\Vol_n(K_{\textrm{sym}}) \geq 2^{-n}\Vol_n(K)$.
  Combining both inequalities gives the claim.
\end{proof}
Next, we want to develop a proof strategy that works for general $\Lambda$ and $K$.
Translating $K$ and applying a linear transformation to both $\Lambda$ and $K$ does not
affect the claim, hence we may assume that $K$ has the barycenter at $\bm{0}$ and the
symmetrizer $K_{\textrm{sym}} := K \cap -K$ is in $\ell$-position. But in general, $\Lambda$
will not be a 2-stable lattice and we cannot expect that one can always choose the 
subspace  $W = \setR^n$ as witness like in Prop~\ref{prop:MainIneqforWellScaledCase}.


But we know by Cor~\ref{cor:ExistenceTwoStableWellSeparatedFiltration} that the lattice $\Lambda$
admits a $2$-stable well-separated filtration $\{ \bm{0}\} = \Lambda_0 \subset \ldots \subset \Lambda_k = \Lambda$. Let us abbreviate
$ d_i := \rank(\Lambda_i / \Lambda_{i-1})$ and $r_i := \det(\Lambda_i / \Lambda_{i-1})^{1/d_i}$.
Then each quotient lattice $\frac{1}{r_i} \Lambda_i / \Lambda_{i-1}$ is a 2-stable lattice of dimension $d_i$ and hence an
argument similar to Prop~\ref{prop:MainIneqforWellScaledCase} becomes feasible.

We can use the triangle inequality that we developed in Lemma~\ref{lem:TriangleIneqForFiltration} to obtain
\begin{eqnarray*}
 \mu(\Lambda,K) \stackrel{K \supseteq K_{\textrm{sym}}}{\leq} \mu(\Lambda,K_{\textrm{sym}}) \stackrel{\textrm{Lem~\ref{lem:TriangleIneqForFiltration}}}{\leq} \sum_{i=1}^k \mu(\Lambda_{i}/\Lambda_{i-1},K_i) &\stackrel{\textrm{Prop~\ref{prop:CovRadiusOf2StableLattice}}}{\lesssim}& \log(2n) \sum_{i=1}^k r_i \ell_{K_i} \\ &\lesssim& \log(2n) \cdot r_k \ell_K,
\end{eqnarray*}
where  $K_i := \Pi_{\textrm{span}(\Lambda_{i-1})^{\perp}}(K_{\textrm{sym}} \cap \textrm{span}(\Lambda_i))$.
Here we have used that the sequence $r_1<\ldots<r_k$ is geometrically increasing.
This provides a convenient upper bound on the covering radius in terms of the relative
determinant of the last quotient lattice in the filtration (which is the sparsest one). However we cannot
avoid wondering whether we gave up too much by bounding $\ell_{K_i} \leq \ell_K$.


Next, we want to lower bound $\mu_{KL}(\Lambda,K)$. The only natural choices for a witness subspace seem to come from the filtration. Hence for some index $i \in \{ 1,\ldots,k\}$ we want to understand what can be obtained by choosing $W := \textrm{span}(\Lambda_{i-1})^{\perp}$, meaning we project out the densest $i-1$ of the quotient lattices. Then
abbreviating $d := \dim(W) = d_i + \ldots + d_k$ we have
\[
 \mu_{KL}(\Lambda,K) \geq \Big(\frac{\det(\Pi_W(\Lambda))}{\Vol_d(\Pi_W(K))}\Big)^{1/d} \stackrel{(*)}{\gtrsim} r_i \cdot \Big(\frac{d}{n}\Big)^6  \frac{d}{\ell_{K_{\textrm{sym}}}^{\circ}} \stackrel{\ell\textrm{-position}}{\gtrsim} r_i \cdot \log(2n) \cdot \Big(\frac{d}{n} \Big)^7 \cdot \ell_{K_{\textrm{sym}}} .
\]
In $(*)$ we use that $\Pi_W(\Lambda) = \Lambda / \Lambda_{i-1}$ and so $\det(\Pi_W(\Lambda))^{1/d}$ is a geometric mean of factors that are all at least $r_i$. Here we also use Lemma~\ref{lem:VolOfProjVsLValueOfSymmetrizer} to bound $\Vol_d(\Pi_W(K))$.
It seems the only direct comparison can be obtained when letting  $i := k$ in which case
we have
\[
\mu(\Lambda,K) \lesssim  \log^2(2n) \cdot \Big(\frac{n}{d_k}\Big)^7 \cdot \mu_{KL}(\Lambda,K).
\]
Hence, we can conclude Theorem~\ref{thm:KLConj} if $d_k$ is close $n$, i.e.
the last quotient subspace is large. But of course this is not necessarily true. In fact, the issue
is more substantial. If $K_{\textrm{sym}}$ is in $\ell$-position with $\ell_{K_{\textrm{sym}}}$ and $\ell_{K_{\textrm{sym}}^{\circ}}$ known
and $W$ is a $d$-dimensional subspace,
then this determines $\Vol_d(\Pi_W(K))^{1/d}$ only up to a polynomial factor in $\frac{n}{d}$.
Hence the information that we considered so far is simply too weak to
approximate $\mu(\Lambda,K)$ up to a polylogarithmic factor.
% conclude which subspace approximately attains $\mu_{KL}(\Lambda,K)$.
But fortunately there is a fix: instead of upper
bounding the whole covering radius $\mu(\Lambda,K)$, we only estimate the covering radius
corresponding to the less important half of the filtration. This means we will
need to iterate the argument, which comes at the expense of a another logarithmic factor, but it will work!

\section{Proof of the main theorem\label{sec:WholeMainProof}}

We will spend the next two subsections proving our main Theorem~\ref{thm:KLConj} by induction over $n$. At each step, we split the lattice $\Lambda$ and the convex body $K$ into a subspace section of dimension at least $n/2$ and a projection where most of the work will go into analyzing the subspace section.

\subsection{The inductive step\label{sec:InductiveStep}}

%We want to outline a candidate approach to prove that $\mu(\Lambda,K) \leq \textrm{polylog}(n) \cdot \mu_{KL}(\Lambda,K)$
%for any lattice and any symmetric convex body $K$.
First, we give a self-contained
description of the inductive step, then later in Section~\ref{sec:MainProof} we describe the main part of the induction.
%The main technical work towards proving step that we need for an inductive proof of Theorem~\ref{thm:KLConjForSymConvex} will be the following:
% \begin{proposition} \label{prop:MainArgument}
%   Let $\Lambda \subseteq \setR^n$ be a full rank lattice and let $K \subseteq \setR^n$
%   be a convex body so that $\textrm{bary}(K^{\circ}) = \bm{0}$. Set $K_{\textrm{sym}} := K \cap (-K)$ and assume that  $\ell_{K_{\textrm{sym}}} \cdot \ell_{K_{\textrm{sym}}^{\circ}} \leq O(n \log n)$.
%   Consider a well-separated 2-stable filtration  $\{ \bm{0} \} = \Lambda_{0} \subset \ldots \subset \Lambda_k = \Lambda$. Let $i^* \in \{ 1,\ldots,k\}$ be the minimal index so that $\textrm{rank}(\Lambda_{i^*}) \geq \frac{n}{2}$. Set $U := \textrm{span}(\Lambda_{i^*})$. Then
%   $\mu(\Lambda \cap U, K_{\textrm{sym}} \cap U) \leq  C_0\log^2 (n) \cdot \mu_{KL}(\Lambda,K)$
%   where $C_0$ is a universal constant.
% \end{proposition}
\begin{proposition} \label{prop:MainArgument}
  There is a universal constant $C_0 > 0$ so that the following holds: 
  For any full rank lattice $\Lambda \subseteq \setR^n$ and any convex body $K \subseteq \setR^n$
  with $b(K) = \bm{0}$,
  there exists a primitive sublattice $\Lambda' \subseteq \Lambda$ with $\rank(\Lambda') \geq n/2$ so that
  \[
    \mu\big(\Lambda', (K \cap -K) \cap \textrm{span}(\Lambda')\big) \leq  C_0\log^2 (2n) \cdot \mu_{KL}(\Lambda,K).
  \]
\end{proposition}


%The proof of Prop~\ref{prop:MainArgument} will take the remainder of this section.
\begin{proof}
  Set $K_{\textrm{sym}} := K \cap (-K)$. The claim is invariant under applying a linear transformation to $K$ and $\Lambda$. Hence we may assume that $K_{\textrm{sym}}$ is in $\ell$-position, i.e.   $\ell_{K_{\textrm{sym}}} \cdot \ell_{K_{\textrm{sym}}^{\circ}} \leq O(n \log (2n))$.
  Consider a well-separated 2-stable filtration  $\{ \bm{0} \} = \Lambda_{0} \subset \ldots \subset \Lambda_k = \Lambda$ which exists by Cor~\ref{cor:ExistenceTwoStableWellSeparatedFiltration}.
  We will later choose the lattice $\Lambda'$ from one of the lattices $\Lambda_i$ in the filtration,
  but we postpone the choice for now.
%  Each of the quotient lattices $\Lambda_i \setminus \Lambda_{i-1}$ is a scalar of a stable lattice,
%  so our strategy will be
%We fix the lattice $\Lambda$, the filtration, the set $K$ and the index $i^*$.
%We may scale $K$ so that $\ell_K = \sqrt{n}$, meaning that $K$ should behave much like the ball $B_2^n$.
We define
\[
  d_i := \rank(\Lambda_i/\Lambda_{i-1}) \quad \textrm{and} \quad r_i := \mathrm{nd}(\Lambda_i/\Lambda_{i-1}) = \det(\Lambda_i/\Lambda_{i-1})^{1/d_i},
\]
which are the rank and normalized determinants of the quotient lattices in the filtration.
Recall that $r_1 < r_2 < \ldots < r_k$ with $r_i \leq \frac{1}{2}r_{i+2}$ for all $i$.

\noindent {\bf Claim I.} \emph{For any $i \in \{ 1,\ldots,k\}$ one has $\mu(\Lambda_{i}, K_{\textrm{sym}} \cap \textrm{span}(\Lambda_{i})) \lesssim \log(2n) \cdot r_{i} \cdot \ell_{K_{\textrm{sym}}}$.} \\
{\bf Proof of Claim I.} We abbreviate $K_j := \Pi_{\textrm{span}(\Lambda_{j-1})^{\perp}}(K_{\textrm{sym}} \cap \textrm{span}(\Lambda_j))$. Then $K_j$ is convex and symmetric and $\frac{1}{r_j}(\Lambda_j/\Lambda_{j-1})$ is a 2-stable lattice. Hence we can bound the covering radii of the
individual quotient lattices by
  \begin{equation} \label{eq:CoveringRadiusQuotientLattice}
  \mu\big(\Lambda_j/\Lambda_{j-1},K_j\big) \stackrel{\textrm{Prop~\ref{prop:CovRadiusOf2StableLattice}}}{\lesssim} \log(2n) \cdot r_j \cdot \ell_{K_j} \stackrel{\textrm{Lem~\ref{lem:LvalueOfProjectedIntersection}}}{\leq} \log(2n) \cdot r_j \cdot \ell_{K_{\textrm{sym}}}. 
  \end{equation}
   Then using the triangle inequality for the covering radius we bound
  \begin{eqnarray*}
    \mu(\Lambda_{i}, K_{\textrm{sym}} \cap \textrm{span}(\Lambda_{i})) &\stackrel{\textrm{Lem~\ref{lem:TriangleIneqForFiltration}}}{\leq}& \sum_{j=1}^{i} \mu\left( \Lambda_j / \Lambda_{j-1},K_j \right)  \\
      &\stackrel{\eqref{eq:CoveringRadiusQuotientLattice}}{\lesssim}& \log (2n) \cdot \ell_{K_{\textrm{sym}}} \cdot \sum_{j=1}^{i} r_j \\ &\lesssim& \log (2n) \cdot \ell_{K_{\textrm{sym}}} \cdot r_{i}, 
  \end{eqnarray*}
  using in the last step that $r_1 < \ldots < r_{i}$ and $r_{j} \leq \frac{1}{2}r_{j+2}$ for all $j$. \qed
  
  In the following we abbreviate $d_{\geq i} := \sum_{j=i}^k d_j$. \\
  \noindent {\bf Claim II.} \emph{For any $i \in \{ 1,\ldots,k\}$ one has $\mu_{KL}(\Lambda,K) \gtrsim \frac{r_i}{\log(2n)} \cdot (\frac{d_{\geq i}}{n})^7 \cdot \ell_{K_{\textrm{sym}}}$.} \\
  \noindent {\bf Proof of Claim II.}
  We choose the subspace $W := \textrm{span}(\Lambda_{i-1})^{\perp}$ as witness and note that $\Pi_W(\Lambda) = \Lambda / \Lambda_{i-1}$. Abbreviating $d := \dim(W) = \rank(\Lambda/\Lambda_{i-1}) = d_{\geq i}$ we have
  \begin{equation} \label{eq:detVsRi}
    \det(\Lambda / \Lambda_{i-1})^{1/d} = \Big( \prod_{j = i}^k r_j^{d_j}\Big)^{1/\sum_{j = i}^k d_j} \geq r_i,
  \end{equation}
  where the middle expression denotes a geometric average of values $r_i < r_{i+1} < \ldots < r_k$.
  Then lower bounding the covering radius proxy with the witness $W$ gives
  \begin{eqnarray*}
    \mu_{KL}(\Lambda,K) &\geq& \Big(\frac{\det(\Pi_W(\Lambda))}{\Vol_d(\Pi_W(K))}\Big)^{1/d} \\
    &\stackrel{\eqref{eq:detVsRi}}{\geq}& \frac{r_i}{\Vol_d(\Pi_W(K))^{1/d}} \\ &\stackrel{\textrm{Lem~\ref{lem:VolOfProjVsLValueOfSymmetrizer}}}{\gtrsim}& r_i\cdot \Big(\frac{d}{n}\Big)^6 \cdot \frac{d}{\ell_{K_{\textrm{sym}}^{\circ}}} \stackrel{\ell\textrm{-position}}{\gtrsim} \frac{r_i}{\log(2n)} \cdot \Big(\frac{d}{n}\Big)^7 \cdot \ell_{K_{\textrm{sym}}}, 
  \end{eqnarray*}
using $\ell_{K_{\textrm{sym}}} \cdot \ell_{K_{\textrm{sym}}^{\circ}} \lesssim n \log(2n)$ in the last step. \qed
  
  Combining Claim I and Claim II with the same index $i$  gives
  \[
  \mu(\Lambda_i,K_{\textrm{sym}} \cap \textrm{span}(\Lambda_i)) \lesssim \log^2(2n) \cdot \Big(\frac{n}{d_{\geq i}}\Big)^7 \cdot \mu_{KL}(\Lambda,K).
  \]
%Assuming these two claims we can prove the  main claim. %proposition:
%\begin{proof}[Proof of Prop~\ref{prop:MainArgument}]
  Now, let $i^* \in \{ 1,\ldots,k\}$ be the minimal index so that $\textrm{rank}(\Lambda_{i^*}) \geq \frac{n}{2}$.
  Then $d_{\geq i^*} \geq \frac{n}{2}$ by minimality. Hence $\Lambda' := \Lambda_{i^*}$ satisfies the claim.
\end{proof}
%It might be instructive to compare this proof with an analogue of \cite{


\subsection{Completing the main proof\label{sec:MainProof}}

Using Proposition~\ref{prop:MainArgument} we can finish the proof of our main theorem. %the Kannan-Lov\'asz conjecture. % for symmetric convex sets.
\begin{proof}[Proof of Theorem~\ref{thm:KLConj}]
  Consider a full rank lattice $\Lambda \subseteq \setR^n$ and a convex body $K \subseteq \setR^n$. We will prove by induction over $n$ that %\rem{T: I gave the constant $C_0$ a name here. The reason is that in an inductive proof it is important to make sure that the constant does not grow over the inductive step.}
  \[
  \mu(\Lambda,K) \leq C_0\log^3(2n) \cdot \mu_{KL}(\Lambda,K),
\]
where $C_0 \geq 1$ is the constant from Proposition~\ref{prop:MainArgument}. The
claim is true for $n=1$, hence assume $n \geq 2$ from now on.
  %where $\rho(n)$ is the bound from Prop~\ref{prop:MainArgument}.
  %W.l.o.g. we may assume that $\Lambda$ has full rank and that $K$ is full-dimensional.
The claim is invariant under translations of $K$, hence we may assume that $b(K) = \bm{0}$.
%By Lemma~\ref{lem:TranslateSoThatPolarIsCentered}, we may translate $K$ so that the barycenter of $K^{\circ}$ is $\bm{0}$. As the claim is invariant under linear transformations, we may also assume that the symmetrizer $K_{\textrm{sym}} := K \cap (-K)$ satisfies $\ell_{K_{\textrm{sym}}} \cdot \ell_{K_{\textrm{sym}}^{\circ}} \leq O(n \log n)$ by Theorem~\ref{thm:PisierRescaling}.
%By Cor~\ref{cor:ExistenceTwoStableWellSeparatedFiltration}, there exists a well-separated 2-stable filtration $\{ \bm{0} \} = \Lambda_{0} \subset \ldots \subset \Lambda_k = \Lambda$.
%  Following Proposition~\ref{prop:MainArgument}, consider the 2-approximate canonical filtration $\{ \bm{0} \} = \Lambda_{0} \subset \ldots \subset \Lambda_k = \Lambda$
%  We fix the minimal index  $i^* \in \{ 1,\ldots,k\}$  so that   $\textrm{rank}(\Lambda_{i^*}) \geq \frac{n}{2}$.
%  Set   $W := \textrm{span}(\Lambda_{i^*})$. Then
Let $\Lambda' \subseteq \Lambda$ be the primitive sublattice from Prop~\ref{prop:MainArgument} and
set $W := \textrm{span}(\Lambda')$. Then
  \begin{eqnarray*}
    \mu(\Lambda,K) &\stackrel{\textrm{Lem~\ref{lem:TriangleIneqGenCoveringRadius}}}{\leq}& \mu(\Lambda \cap W,K \cap W) + \mu(\Pi_{W^{\perp}}(\Lambda),\Pi_{W^{\perp}}(K)) \\
    &\stackrel{K \supseteq K_{\textrm{sym}}}{\leq}& \mu(\Lambda \cap W,K_{\textrm{sym}} \cap W) + \mu(\Pi_{W^{\perp}}(\Lambda),\Pi_{W^{\perp}}(K)) \\
                   &\stackrel{\substack{\textrm{Prop~\ref{prop:MainArgument}} \\ +\textrm{ind.}}}{\leq}& C_0\log^2(2n) \cdot \mu_{KL}(\Lambda,K) + C_0\log^3(2\underbrace{\dim(W^{\perp})}_{\leq n/2}) \cdot \underbrace{\mu_{KL}(\Pi_{W^{\perp}}(\Lambda),\Pi_{W^{\perp}}(K))}_{\leq \mu_{KL}(\Lambda,K)} \\
    &\stackrel{\textrm{Lem~\ref{lem:MonotonicityOfMuKL}}}{\leq}& C_0\underbrace{\log^2(2n) \cdot \Big(1+\log(n)\Big)}_{=\log^3(2n)} \cdot \mu_{KL}(\Lambda,K). \qedhere
  \end{eqnarray*}
\end{proof}


We should point out that Regev and Stevens-Davidowitz~\cite{Regev-SD-ReverseMinkowskiTheoremSTOC17}
prove that in the Euclidean case one has $\mu(\Lambda,B_2^n) \leq O(\log^{3/2}(2n)) \cdot \mu_{KL}(\Lambda,B_2^n)$.
Our proof could be seen as a generalization of their argument in the sense that \cite{Regev-SD-ReverseMinkowskiTheoremSTOC17} also relate both notions of covering radii to the quantities $r_i$ and $d_i$
as defined in Prop~\ref{prop:MainArgument} by proving that
\[
  \mu(\Lambda,B_2^n) \leq O(\log (2n)) \cdot \sqrt{\sum_{i=1}^k d_i r_i^2} \leq O(\log^{3/2}(2n)) \cdot \mu_{KL}(\Lambda,B_2^n).
\]
On the other hand, for them the ``standard'' canonical filtration suffices and they do not require an inductive step. Implicitly, our induction causes $O(\log(2n))$ many re-centering and rescaling operations using the result of
Figiel, Tomczak-Jaegerman and Pisier (Theorem~\ref{thm:PisierRescaling}). This circumvents the issue that %we do not have a tight analogue of Theorem~\ref{thm:PisierRescaling} for asymmetric convex bodies.
%Also note that clearly it can happen that
the covering radius might be dominated by a subspace of dimension $d$ with $d \ll n$, which may not affect the $\ell$-position of the body sufficiently. Then implicitly the induction will contain an iteration where $d$ is relatively large compared to the current ambient dimension. 
It may also be instructive to reconsider the proof of Prop~\ref{prop:MainArgument} in the case that $K = B_2^n$.
Then in \eqref{eq:CoveringRadiusQuotientLattice}, we would obtain the inequality $\mu(\Lambda_{j}/\Lambda_{j-1},K_j) \lesssim \log(2n) \cdot r_j \cdot \sqrt{n}$ while actually the much stronger bound of $\mu(\Lambda_{j}/\Lambda_{j-1},K_j) \lesssim \log(2n) \cdot r_j \cdot \sqrt{d_j}$ holds. %Similarly in Claim III we incur a loss that is a polynomial in $\frac{n}{d_i}$.
The trick is that using a well-separated filtration the arising loss can be efficiently bounded. 
%there are at most $O(\log(2n))$ indices with relevant contributions and their loss
%can always be charged to the indices with higher normalized determinant.
%This loss seems necessary as in general $K_{sym}$ may not The trick is only reason why the proof still works is that  %, one has $\ell_{K_i} \approx \sqrt{d_i}$


\section{Finding the subspace $W$ in single-exponential time\label{sec:FindingSubspace}}

In this section, we prove Theorem~\ref{thm:FindingSubspaceIn2ToN}, which guarantees that a suitable subspace subspace
$W$ can be found in time $2^{O(n)}$ at the expense of an additional logarithmic factor in the approximation guarantee.
It will be convenient to first apply a linear transformation to well-scale $K$. This can be done in polynomial time
and is a standard argument, see Lemma~\ref{lem:WellScaleKorDecideKisThin} for details. Hence, for us it suffices to prove the following: 
\begin{theorem} \label{thm:SubspaceWellConditioned}
Given a full rank lattice $\Lambda \subseteq \setR^n$ and a convex body $K \subseteq \setR^n$ such that $B^n_2 \subseteq K \subseteq (n+1)^{3/2} B^n_2$, there exists a randomized $2^{O(n)}$-time algorithm to compute a subspace $W \subseteq \setR^n$ with $d := \dim (W)$ so that
\[\mu(\Lambda, K) \lesssim \log^4 (2n) \cdot \Big(\frac{\det(\Pi_W(\Lambda))}{\Vol_d(\Pi_W(K))}\Big)^{1/d}. \]
\end{theorem}

The main technical tool will be the following result of Dadush, which is the only step in the algorithm which takes exponential time:

\begin{theorem}[Theorem 6.4. in~\cite{Dadush-Finding-DenseLatticeSubspacesSTOC19}] \label{thm:StableFiltrationAlgorithm}
Given a lattice $\Lambda \subseteq \setR^n$ one can compute an $O(\log (2n))$-stable filtration of $\Lambda$ in $2^{O(n)}$ time with probability at least $1-2^{-\Omega(n)}$.
\end{theorem}

The following algorithm mimics the proof in Section 4:

\begin{center}
  \psframebox{\begin{minipage}{14cm}
{\sc Find-Subspace} \\
{\bf Input:} Convex body $K \subseteq \setR^n$ so that $B^n_2 \subseteq K \subseteq (n+1)^{3/2} B^n_2$, full rank lattice $\Lambda \subseteq \setR^n$ \\
 {\bf Output:} Subspace $W \subseteq \setR^n$ satisfying Theorem~\ref{thm:SubspaceWellConditioned}
\begin{enumerate*}
\item[(1)] Compute an approximate barycenter $\tilde{x}$ such that $\|b(K) - \tilde{x}\|_2 \le 1$
\item[(2)] Shift $K' := K - \tilde{x}$
\item[(3)] Set $K_{\mathrm{sym}} := K' \cap (-K')$ and compute an invertible linear map $T$ so that
  \[\ell_{T(K_{\mathrm{sym}})} \cdot \ell_{(T (K_{\mathrm{sym}}))^{\circ}} \leq C \cdot n \log (2n) \]
\item[(4)] Set $K' \gets T(K)$ and $\Lambda' \gets T(\Lambda)$
\item[(5)] Compute an $O(\log (2n))$-stable filtration  $\{\bm{0}\} = \Lambda_0 \subset \ldots \subset
      \Lambda_k = \Lambda'$
\item[(6)] Compute a well-separated $O(\log (2n))$-stable filtration  $\{\bm{0}\} = \Lambda'_0 \subset \ldots \subset
      \Lambda'_{k'} = \Lambda'$
\item[(7)] Set $i^*$ as the minimal index with $\rank(\Lambda'_{i^*}) \ge \frac{n}{2}$
\item[(8)] Set $W_{i^*} := \span(\Lambda'_{i^*})^\perp$. %Set $W_{\cap} := \span(\Lambda'_{i})^\perp$ where $i \in \{1, \dots, k'\}$ minimizes $\frac{\det(\Pi_{\span(\Lambda'_{i})^\perp}(\Lambda'))}{\Vol_{\rank(\Lambda'/\Lambda'_{i})} (\Pi_{\span(\Lambda'_{i})^\perp} (K'))}$
\item[(9)] Recursively call $W_{\Pi} := {\textsc{Find-Subspace}}(\Pi_{\span(\Lambda'_{i^*})^\perp} (K')), \Pi_{\span(\Lambda'_{i^*})^\perp} (\Lambda'))$
\item[(10)] Return $W := T^{-1} W'$ where $W' := \underset{W \in \{W_{i^*}, W_\Pi\}}{\mathrm{argmin}} \Big\{ \Big(\frac{\det(\Pi_{W}(\Lambda'))}{\Vol_{\dim(W)} (\Pi_{W} (K'))}\Big)^{1/\dim(W)} \Big\}$.
\end{enumerate*}
\end{minipage}}
\end{center}

We will need several volume computations in the algorithm, for which we use the following theorem:

\begin{theorem}[\cite{Kannan1997RandomWA}] \label{thm:VolumeComputation}
Given a convex body $K \subseteq \setR^n$ with $r\cdot B^n_2 \subseteq K \subseteq R \cdot B^n_2$, there exists a randomized algorithm which outputs a positive number $\zeta$ with $\Vol_n(K)/\zeta \in [1-\varepsilon, 1 + \varepsilon]$. The runtime is polynomial in $n$, $1/\varepsilon$, $\log(1/r)$ and $\log(R)$. 
\end{theorem}

In fact,~\cite{Kannan1997RandomWA} also computes an approximation to the barycenter of $K$:

\begin{theorem}[\cite{Kannan1997RandomWA}] \label{thm:BarycenterComputation}
Given a convex body $K \subseteq \setR^n$ with $B^n_2 \subseteq K \subseteq (n+1)^{3/2} \cdot B^n_2$ and $\delta>0$, there exists a randomized algorithm with running time polynomial in $n$ and $\frac{1}{\delta}$, which returns an approximate barycenter $\tilde{x}$ such that $\|b(K) - \tilde{x}\|_2 \le \delta$.
\end{theorem}



Now, we can prove the main result for this section: 
\begin{proof}[Proof of Theorem~\ref{thm:SubspaceWellConditioned}]
First we justify the running time of $2^{O(n)}$, later we discuss the approximation guarantee.
  We first apply Theorem~\ref{thm:BarycenterComputation} to compute an approximate barycenter $\tilde{x}$ and shift $K' := K-x$. Theorem~\ref{thm:StableFiltrationAlgorithm} yields a filtration for step (5), which can be refined into a well-separated filtration by Theorem~\ref{thm:ApproxFilt}. Step (10) requires computation of determinants, which can be done in polynomial time via Gaussian elimination, and the volume of a convex body, which can also be done in randomized polynomial time using Theorem~\ref{thm:VolumeComputation}.
The runtime $T(n)$ of ${\textsc{Find-Subspace}}$ satisfies the recursion $T(n) \le 2^{O(n)} + T(n/2)$, which can be resolved to $T(n) \leq 2^{O(n)}$.

Next, we justify the approximation guarantee. From the same argument in Section 3 and 4 one can see that the returned subspace satisfies
\[\mu(\Lambda, K) \lesssim \log^4 (2n) \cdot \Big(\frac{\det(\Pi_W(\Lambda))}{\Vol_d(\Pi_W(K))}\Big)^{1/d}, \]
where we have taken into account that we pay an additional $\log (2n)$ factor from Proposition~\ref{prop:CovRadiusOf2StableLattice} as our filtration is only guaranteed to be $O(\log(2n))$-stable. Another subtle point is that we are using only an
approximate barycenter. Hence it remains to generalize Proposition~\ref{prop:VolumeOfProjectionVsSymmetrizer}
and show that the approximation costs us at most another constant factor: %, since we only compute an approximate Santal\'o point:

\noindent {\bf Claim.} \emph{Let $K \subseteq \setR^n$ be a convex body so that $B^n_2 \subseteq K$ and $\|b(K)\|_2 \le 1$. Let $F \subseteq \setR^n$ be a $d$-dimensional subspace. Then denoting $K_\mathrm{sym} := K \cap (-K)$,}

\[ \Vol_d(\Pi_F(K))^{1/d} \lesssim \Big(\frac{n}{d}\Big)^5 \cdot \log\Big(\frac{en}{d}\Big)^2  \cdot \Vol_d(\Pi_F(K_\mathrm{sym}))^{1/d}.\]
{\bf Proof of Claim.} By Proposition~\ref{prop:VolumeOfProjectionVsSymmetrizer}, we know that denoting $\tilde{K}_{\mathrm{sym}} := (K-b(K)) \cap (-K+b(K))$, we have
\[ \Vol_d(\Pi_F(K))^{1/d} \lesssim \Big(\frac{n}{d}\Big)^5 \cdot \log\Big(\frac{en}{d}\Big)^2  \cdot  \Vol_d(\Pi_F(\tilde{K}_{\mathrm{sym}} ))^{1/d}.\]
Since $- b(K) \subseteq B^n_2 \subseteq K$, it follows that $K - b(K) \subseteq K + K = 2K$, so that $\tilde{K}_{\mathrm{sym}} \subseteq 2 K_{\mathrm{sym}}$ and $\Vol_d(\Pi_F(\tilde{K}_{\mathrm{sym}} ))^{1/d} \le 2 \cdot \Vol_d(\Pi_F(K_\mathrm{sym}))^{1/d}.$
\end{proof}

\section{Integer programming in time $(\log(2n))^{O(n)}$\label{sec:IP}}

%{\bf TODO. Basically reproduce from Dadush's thesis.}
Next, we show that integer programming can be solved in time $(\log(2n))^{O(n)}$. In fact,
this is a known consequence of Theorem~\ref{thm:FindingSubspaceIn2ToN}.
We do not claim any original contribution for this section,
but we reproduce the arguments of Dadush~\cite{DadushThesis2012} to be self-contained.
As it is common in the literature, we only state the dependence of running times on $n$;
all running times that involve a convex set $K \subseteq r B_2^n$ and a lattice $\Lambda = \Lambda(B)$ also contain
a not mentioned factor that is polynomial in $\log(r)$ and in the encoding length of $B$.
%By some preprocessing we may also assume that $K$ is a convex body and $\Lambda$ has full rank --- otherwise,
%replace both objects with their intersection with the  affine subspace $\textrm{span}(\Lambda) \cap \textrm{affine.hull}(K)$. 

First, we describe the intuition behind Dadush's algorithm. Consider a convex body $K \subseteq \setR^n$
and a lattice $\Lambda \subseteq \setR^n$; the goal is to find a point in $K \cap \Lambda$.
We compute a subspace $W \subseteq \setR^n$ in time $2^{O(n)}$ that certifies the covering radius $\mu(\Lambda,K)$ up to
a factor $\rho(n) := \Theta(\log^{4} (2n))$. %\rem{T: Changed to $\rho = \Theta(\log^8(n))$.}
Consider the points  $X := \Pi_W(K) \cap \Pi_W(\Lambda)$ in the projection on $W$.
For each $x \in K \cap \Lambda$, we also have $\Pi_W(x) \in X$. Note that the reverse may not be true in the
sense that it is entirely possible that $K \cap \Lambda = \emptyset$ while $X \neq \emptyset$.
However, we are guaranteed that all lattice points
in $K$ must be in one of the $(n-d)$-dimensional fibers of the projection, i.e.
\[
 K \cap \Lambda \subseteq \bigcup_{y \in X} \big((K \cap \Pi_W^{-1}(y)) \cap \Lambda\big).
\]
\iftrue
\begin{center}
  \psset{unit=1.3cm}
%   \begin{pspicture*}(-4.5,-2.5)(4.5,2.5)
%     \pspolygon[fillstyle=solid,fillcolor=lightgray](-0.8,1)(-0.8,1.5)(1,1.8)(2.1,1.7)(1,1.2) \rput[c](0.5,1.5){$K$}
%     \psline[linecolor=blue!50!white,linewidth=2pt](5,2.5)(-5,-2.5)\pnode(3.5,1.75){A}\nput{-45}{A}{$\blue{W}$}
% %    \rput[c](0,-0.5){\pspolygon[fillstyle=solid,fillcolor=lightgray](-3.2,-0.2)(-3.2,0.2)(3.2,0.2)(3.2,-0.2)}
%     \multido{\n=-2+1}{5}{\multido{\N=-4+1}{9}{\psdots(\N,\n)}}
%  %   \rput[c](0,-0.5){$K$}
%     % \drawRect{}{-3.2}{-0.2}{6.4}{6.4}
%     \cnode*(0,0){2.5pt}{origin} \nput[labelsep=2pt]{-45}{origin}{$\bm{0}$}
%     \psline[linecolor=darkgray,linewidth=3pt](2.4,1.2)(-0.24,-0.12)
%  %   \psdots[linecolor=blue,linewidth=1.5pt](-4,-2)(-2,-1)(0,0)(2,1)(4,2)
%   %  \rput[c](-0.24,-0.12){\psline[linestyle=dotted](0,0)(-1,2)}
%   %  \rput[c](2.4,1.2){\psline[linestyle=dotted](0,0)(-1,2)}
%     \multido{\N=0+1}{4}{\rput[c](\N,0){\psline[linestyle=dotted,linecolor=gray](3,-6)(-3,6)}}
%     \multido{\N=0+1}{3}{\rput[c](\N,1){\psline[linestyle=dotted,linecolor=gray](3,-6)(-3,6)}}
%     \multido{\N=0+0.4,\n=0+0.2}{7}{\psdots[linecolor=blue,linewidth=1.5pt](\N,\n)}
%     \pnode(0,0){A}\pnode(-0.5,-1.0){B} \ncline[linecolor=blue,nodesepB=3pt]{->}{B}{A} \nput[labelsep=2pt]{-90}{B}{$\blue{X}$}
%     \pnode(1.0,0.5){A}\pnode(2,-0.5){B} \ncline[linecolor=black,nodesepB=3pt]{->}{B}{A} \nput[labelsep=2pt]{0}{B}{$\Pi_{W}(K)$}
%     \psplot[algebraic=true,linewidth=2pt,linecolor=darkgray]{-0.75}{-0.52}{-2*x}
%     \psplot[algebraic=true,linewidth=2pt,linecolor=darkgray]{-0.3}{-0.05}{-2*(x-0.5)}
%     \psplot[algebraic=true,linewidth=2pt,linecolor=darkgray]{0.17}{0.43}{-2*(x-1.0)}
%     \psplot[algebraic=true,linewidth=2pt,linecolor=darkgray]{0.63}{0.9}{-2*(x-1.5)}
%     \psplot[algebraic=true,linewidth=2pt,linecolor=darkgray]{1.1}{1.32}{-2*(x-2.0)}
%     \psplot[algebraic=true,linewidth=2pt,linecolor=darkgray]{1.63}{1.73}{-2*(x-2.5)}
%     \pspolygon[fillstyle=none,fillcolor=lightgray](-0.8,1)(-0.8,1.5)(1,1.8)(2.1,1.7)(1,1.2)% redraw borders of K
%     % \psplot[algebraic=true,linewidth=2pt,linecolor=darkgray]{2.0}{2.5}{-2*(x-3.0)}
   
%   \end{pspicture*}
  \begin{pspicture*}(-4.5,-2.5)(4.5,2.5)% changed (1,1.8) -> (1,2.2)
    \pspolygon[fillstyle=solid,fillcolor=lightgray](-0.8,1)(-0.8,1.5)(1,2.2)(2.1,1.7)(1,1.2)\rput[c](0.5,1.5){$K$}%
    \psline[linecolor=blue!50!white,linewidth=2pt](5,2.5)(-5,-2.5)\pnode(3.5,1.75){A}\nput{-45}{A}{$\blue{W}$}%
%    \rput[c](0,-0.5){\pspolygon[fillstyle=solid,fillcolor=lightgray](-3.2,-0.2)(-3.2,0.2)(3.2,0.2)(3.2,-0.2)}
 %   \rput[c](0,-0.5){$K$}
    % \drawRect{}{-3.2}{-0.2}{6.4}{6.4}
    \cnode*(0,0){2.5pt}{origin}\nput[labelsep=2pt]{-45}{origin}{$\bm{0}$}%
    \psline[linecolor=darkgray,linewidth=3pt](2.4,1.2)(-0.24,-0.12)%
 %   \psdots[linecolor=blue,linewidth=1.5pt](-4,-2)(-2,-1)(0,0)(2,1)(4,2)
  %  \rput[c](-0.24,-0.12){\psline[linestyle=dotted](0,0)(-1,2)}
  %  \rput[c](2.4,1.2){\psline[linestyle=dotted](0,0)(-1,2)}
    \multido{\N=0+1}{4}{\rput[c](\N,0){\psline[linestyle=dotted,linecolor=gray](3,-6)(-3,6)}}%
    \multido{\N=0+1}{3}{\rput[c](\N,1){\psline[linestyle=dotted,linecolor=gray](3,-6)(-3,6)}}%
    \psplot[algebraic=true,linewidth=2pt,linecolor=darkgray]{-0.75}{-0.52}{-2*x}
    \psplot[algebraic=true,linewidth=2pt,linecolor=darkgray]{-0.35}{-0.05}{-2*(x-0.5)}
    \psplot[algebraic=true,linewidth=2pt,linecolor=darkgray]{0.08}{0.43}{-2*(x-1.0)}
    \psplot[algebraic=true,linewidth=2pt,linecolor=darkgray]{0.5}{0.9}{-2*(x-1.5)}
    \psplot[algebraic=true,linewidth=2pt,linecolor=darkgray]{0.92}{1.32}{-2*(x-2.0)}
    \psplot[algebraic=true,linewidth=2pt,linecolor=darkgray]{1.52}{1.73}{-2*(x-2.5)}

    \pspolygon[fillstyle=none,fillcolor=lightgray](-0.8,1)(-0.8,1.5)(1,2.2)(2.1,1.7)(1,1.2)% redraw borders of K
    \multido{\n=-2+1}{5}{\multido{\N=-4+1}{9}{\psdots(\N,\n)}}%
    \multido{\N=0+0.4,\n=0+0.2}{7}{\psdots[linecolor=blue,linewidth=1.5pt](\N,\n)}
    \pnode(0,0){A}\pnode(-0.5,-1.0){B} \ncline[linecolor=blue,nodesepB=3pt]{->}{B}{A} \nput[labelsep=2pt]{-90}{B}{$\blue{X}$}
    \pnode(1.0,0.5){A}\pnode(2,-0.5){B} \ncline[linecolor=black,nodesepB=3pt]{->}{B}{A} \nput[labelsep=2pt]{0}{B}{$\Pi_{W}(K)$}
    % \psplot[algebraic=true,linewidth=2pt,linecolor=darkgray]{2.0}{2.5}{-2*(x-3.0)}   
\end{pspicture*}
\end{center}
%\fi
The algorithm enumerates $X$ and then recurses on all the fibers. In order for this algorithm to be
efficient we need to (i) bound the cardinality $|X|$ and (ii) be able to enumerate $X$. For 
(ii), note that it is possible that $W = \setR^n$ and hence we would not gain anything by treating $\Pi_W(K) \cap \Pi_W(\Lambda)$ as a general integer programming problem.


For convex bodies $A,B \subseteq \setR^n$, the \emph{covering number} $N(A,B) := \min\{ N \mid \exists x_1,\ldots,x_N \in \setR^n: A \subseteq \bigcup_{i=1}^N (x_i + B)\}$ is the minimum number of translates of $B$ needed to cover $A$.
  For a convex body $K \subseteq \setR^n$ and a full rank lattice $\Lambda \subseteq \setR^n$ we define
\[
 G(\Lambda,K) := \max_{x \in \setR^n} |(K + x) \cap \Lambda|.
\]
In words, $G(\Lambda,K)$ denotes the maximum number of lattice points that any shift of $K$
contains. Note that even if $K \cap \Lambda = \emptyset$, $G(\Lambda,K)$ might still be arbitrarily large.
However, algorithmically the quantity $G(\Lambda,K)$ is useful:
\begin{theorem}[\cite{EnumerateLatticeAlgosDadushPeikertVempalaFOCS11,NearOptDetAlgoForMEllipsoid-DadushVempalaPNAS13}\label{thm:LatticeEnumeration}]
  Given a convex body $K \subseteq \setR^n$ and a full rank lattice $\Lambda \subseteq \setR^n$,
  one can enumerate all points in $K \cap \Lambda$ in deterministic time $2^{O(n)} \cdot G(\Lambda,K)$.
\end{theorem}
We briefly sketch the algorithm behind Theorem~\ref{thm:LatticeEnumeration}:
We use the method of Dadush and Vempala~\cite{NearOptDetAlgoForMEllipsoid-DadushVempalaPNAS13} to compute an \emph{$M$-ellipsoid} $\pazocal{E}$ of $K$ which has the property that $N(K,\pazocal{E}),N(\pazocal{E},K) \leq 2^{O(n)}$.
Their deterministic algorithm takes time $2^{O(n)}$. In particular this means that $2^{-\Theta(n)} \leq \frac{G(\Lambda,K)}{G(\Lambda,\pazocal{E})} \leq 2^{\Theta(n)}$. %and translates $x_1,\ldots,x_N$ with $N \leq 2^{O(n)}$
Next, we compute\footnote{At least in the case that $\pazocal{E}$ is an $M$-ellipsoid for $K$, one may find
  those translates with $N \leq 2^{O(n)}N(K,\pazocal{E})$ with ease.
  After applying a linear transformation, we may assume that $\pazocal{E} = \sqrt{n} B_2^n$. Then take all translates $x + \pazocal{E}$ with $x \in \setZ^n$ that intersect $K$.} the translates $x_1,\ldots,x_N$ with $N \leq 2^{O(n)}$ so that $K \subseteq \bigcup_{i=1}^N (x_i + \pazocal{E})$.
Then we can use the following argument by Dadush, Peikert and Vempala~\cite{EnumerateLatticeAlgosDadushPeikertVempalaFOCS11} to enumerate all lattice points in $(x_i + \pazocal{E}) \cap \Lambda$. After applying
a linear transformation, it suffices to compute all points in $(t+B_2^n) \cap \Lambda$ for $t \in \setR^n$.
Let $R \subseteq \Lambda \setminus \{ \bm{0}\}$ be the \emph{Voronoi-relevant} vectors, which are all the
vectors that define a facet of the \emph{Voronoi cell} of $\Lambda$. It is known that $|R| \leq 2^{n+1}$
and moreover the set $R$ can be computed in time $2^{O(n)}$ by the algorithm of \cite{CVP-Voronoi-Algo-MicciancioVoulgaris-SICOMP2013}.
Next, consider the graph $H = (\Lambda,E)$ with edges $E = \{ \{ x,y\} : x,y \in \Lambda\textrm{ and }x-y \in R\}$.
Then it follows from the work of \cite{CVP-Voronoi-Algo-MicciancioVoulgaris-SICOMP2013} that the subgraph induced by $\Lambda \cap (t + B_2^n)$ is connected. Hence, one can compute
the closest lattice point to $t$ (again using \cite{CVP-Voronoi-Algo-MicciancioVoulgaris-SICOMP2013}) and then
traverse the subgraph. 

%For each $i \in [N]$, we can randomly \emph{sparsify} the lattice $\Lambda$. 
%We guess the translate $\bm{y} + \pazocal{E}$ so that $\bm{x}^* \in K \cap (\bm{y} + \pazocal{E})$. Now we randomly \emph{sparsify} the lattice $\Lambda$.
%More precisely, we can compute a shifted random sublattice $z + \Lambda' \subseteq \Lambda$ so that with constant probability
%one has  $|(x_i + \pazocal{E}) \cap (z + \Lambda')| = 1$. Moreover, each lattice point
%in $(x_i + \pazocal{E}) \cap \Lambda$ has approximately the same probability of being the lone survivor.
%The point in $(x_i + \pazocal{E}) \cap (z + \Lambda')$ can then be computed in time $2^{O(n)}$
%using the algorithm of Micciancio and Voulgaris~\cite{CVP-Voronoi-Algo-MicciancioVoulgaris-SICOMP2013}.
%Repeating this sampling procedure $2^{O(n)}G(\Lambda,K)$ times will produce all lattice points in $K$ with high probability. 


Next, we require an upper bound on $G(\Lambda,K)$ in terms of the volume of $K$ and density of $\Lambda$.
Surprisingly, such an upper bound exists if we additionally control the covering radius.
We reproduce Dadush's proof as the argument is key to understanding the algorithm:
\begin{lemma} \label{lem:UpperBoundOnLatticePoints}
  For any full rank lattice $\Lambda \subseteq \setR^n$ and any convex body $K \subseteq \setR^n$ one has
  \[
  G(\Lambda,K) \leq 2^n \max\{ \mu(\Lambda,K)^n,1\} \cdot \frac{\Vol_n(K)}{\det(\Lambda)}.
  \]
\end{lemma}

\begin{proof}
  After a linear transformation and scaling by $\max\{ \mu(\Lambda,K),1\}$, the statement is
  equivalent to the following simpler claim: \\
  {\bf Claim.} \emph{For any convex body $K \subseteq \setR^n$ with $\mu(\setZ^n,K) \leq 1$ and any $x \in \setR^n$ one has $|K \cap (x+\setZ^n)| \leq 2^n \Vol_n(K)$.} \\
{\bf Proof of Claim.}
The claim is invariant under translating $K$, hence we may assume that $\bm{0} \in K$.
Let $\equiv$ be the equivalence relation on pairs $x,y \in K$ that is defined by  $x \equiv y \Leftrightarrow x-y \in \setZ^n$. We define a set $V \subseteq K$ by selecting one element from each equivalence class w.r.t. $\equiv$. It would not
matter much which element was selected, but let us make the canonical choice of choosing the lexicographically
minimal one. In other words, we choose 
  \[
   V = \big\{ x \in K \mid x \leq_{\textrm{lex}} y \quad \forall y \in (x + \setZ^n) \cap K\big\},
  \]
  where $\leq_{\textrm{lex}}$ is the standard lexicographical ordering.
 \iftrue
  \begin{center}
    \psset{unit=0.7cm}
    \begin{pspicture}(-2,-2)(2,2)
      \pspolygon[fillstyle=solid,fillcolor=lightgray,linewidth=0.75pt](-1.25,-2)(-2,-1.25)(-2,2)(2,2)(2,-2)\rput[c](1.5,1.5){$K$}
      \pspolygon[fillstyle=solid,fillcolor=gray,linestyle=none](-1,-2)(-1.25,-2)(-2,-1.25)(-2,-0.25)(-1.25,-1)(-1,-1)
      \psline[linewidth=1.5pt](-1,-2)(-1.25,-2)(-2,-1.25)(-2,-0.25)\rput[c](-1.5,-1.3){$V$}
      \multido{\N=-2+1}{5}{\multido{\n=-2+1}{5}{\psdots(\N,\n)}}
      \cnode*(0,0){2.0pt}{origin}\nput[labelsep=2pt]{0}{origin}{$\bm{0}$}
    \end{pspicture}
  \end{center}
 \fi
As we select at most one element from
  each equivalence class, we certainly have $\textrm{Vol}_n(V) \leq 1$. On the other hand,
  $\mu(\setZ^n,K) \leq 1$ implies that for all $x \in \setR^n$ one has  $(x+\setZ^n) \cap K \neq \emptyset$.
  That in turn means that every equivalence class has a member in $K$ and so $\Vol_n(V) \geq 1$.
  Together this gives $\textrm{Vol}_n(V)=1$.
  Next, we note that by construction all translates $x+V$ with $x \in \setZ^n$ are disjoint. Moreover, for $x \in  K \cap \setZ^n$ one has that $x + V \subseteq K + K =2K$. Then 
  \[
  |K \cap \setZ^n| = \sum_{x \in K \cap \setZ^n} \underbrace{\Vol_n(x+V)}_{=1} \stackrel{\textrm{disj.}}{=} \Vol_n\Big(\bigcup_{x \in K \cap \setZ^n} (x+V)\Big) \leq \Vol_n(2K),
  \]
which gives the claim.
\end{proof}



%\rem{V: Would be nice to incorporate this preprocessing step before calling \textsc{Find-Subspace} (and also after each recursive call)}
%\begin{lemma}[Lemma 7.2.1 in~\cite{DadushThesis2012}]
%Let $K \subseteq \setR^n$ be a $(a_0, R)$-circumscribed convex set given by a separation oracle, let $\Lambda \subseteq \setR^n$ denote a lattice given by a basis $B \in \setQ^{n \times n}$, and let $H$ be an affine subspace. There exists a $2^{O(n)}$-time algorithm which either decides that $K \cap \Lambda \cap H = \emptyset$ or returns a shift $p \in \Lambda$, and sublattice $\Gamma' \subseteq \Gamma$, a vector $a_0' \in \span(\Lambda')$, a separation oracle for the convex set $K' := (K - p) \cap (a'_0 + R B^n_2) \cap \span(\Lambda')$, and an ellipsoid $\pazocal{E}' \subseteq \span(\Lambda')$ with center $c'$, so that $K \cap \Lambda \cap H = (K' \cap \Lambda') + p$ and $c' + \pazocal{E}' \subseteq K' \subseteq c' + (n+1)^{3/2} \pazocal{E}'$. 
%\end{lemma}


One technicality we have to deal with is that Theorem~\ref{thm:FindingSubspaceIn2ToN} requires a lower bound
on the \emph{inradius} of $K$. Hence we run a preprocessing step: if there is no suitable lower bound for the
inradius, then the lattice points of $K$ are all contained in an easy-to-find hyperplane.
\begin{lemma} \label{lem:WellScaleKorDecideKisThin}
  Given a compact convex set $K \subseteq r B_2^n$ and a lattice $\Lambda = \Lambda(B)$. Then in time polynomial in $n$,
  times a polynomial in $\log(r)$ and the encoding length of $B$ one can find at least one of the following: 
  \begin{enumerate}
  \item[(a)] An ellipsoid $\pazocal{E}$ and center $c$ so that $c + \frac{1}{(n+1)^{3/2}}\pazocal{E} \subseteq K \subseteq c+\pazocal{E}$.
  \item[(b)] A vector $a \in \setR^n \setminus \{ \bm{0}\}$ and $\beta \in \setR$ so that $K \cap \Lambda \subseteq \{ x \in \setR^n \mid \left<a,x\right> = \beta\}$.
  \end{enumerate}
\end{lemma}
\begin{proof}
  % The ellipsoid method can be modified to find an en
  We may assume that $\textrm{rank}(\Lambda)=n$, otherwise any $a$ orthogonal to $\textrm{span}(\Lambda)$ will satisfy (b).
  Next, we use a variant of the ellipsoid method from \cite{Groetschel1988} (see also Lemma~2.5.10 in \cite{DadushThesis2012}) to find a pair  $(c,\pazocal{E})$ in time polynomial in $n$, $\log(r)$ and $\log(\frac{1}{\varepsilon})$ 
  so that either (a) holds, or   $K \subseteq c+\pazocal{E}$ and $\Vol_n(\pazocal{E}) \leq \varepsilon$. 
   Suppose the latter happens. Then using Minkowski's Theorem (Theorem~\ref{thm:Minkowski}) in $(*)$ and the Blaschke-Santal\'o-Bourgain-Milman Theorem (Theorem~\ref{thm:BSBM}) in $(**)$ we obtain
   \begin{eqnarray*}
     \lambda_1(\Lambda^*,\pazocal{E}^{\circ}) \stackrel{(*)}{\lesssim} \Big(\frac{\det(\Lambda^*)}{\Vol_n(\pazocal{E}^{\circ})}\Big)^{1/n}  \stackrel{(**)}{\lesssim}  \Big(\frac{\Vol_n(\pazocal{E})}{\det(\Lambda) \cdot \nu_n^2}\Big)^{1/n}
     \lesssim n \cdot \Big(\frac{\varepsilon}{\det(\Lambda)}\Big)^{1/n} \leq \frac{1}{2} \cdot 2^{-n/2},
   \end{eqnarray*}
   for a suitable choice of $\varepsilon>0$. Then the LLL-algorithm~\cite{LLL1982} can find a dual lattice vector $a \in \Lambda^* \setminus \{ \bm{0}\}$ with $\|a\|_{\pazocal{E}^{\circ}} \leq 2^{n/2} \cdot \lambda_1(\Lambda^*,\pazocal{E}^{\circ}) \leq \frac{1}{2}$. That vector $a$ with $\beta := \lceil \left<a,c\right> \rfloor$ will  satisfy (b). \qedhere
%  We run a modified ellipsoid method that maintains an enclosing ellipsoid $E \supseteq K$ starting with $E := R B_2^n$. In each iteration, we either decide that $E$ scaled by a factor $\frac{1}{(n+1)^{3/2}}$ is contained in $K$.
  
\end{proof}
We are now ready to state the complete algorithm. As mentioned earlier, we denote $\rho(n) := \Theta(\log^4(2n))$ % \rem{T: Check $\rho(n)$!}
as the approximation factor from Theorem~\ref{thm:FindingSubspaceIn2ToN}.  

\begin{center}
  \psframebox{\begin{minipage}{14cm}
{\sc Dadush's algorithm} \\
{\bf Input:} Compact convex set $K \subseteq \setR^n$, lattice $\Lambda \subseteq \setR^n$ \\
{\bf Output:} Point $x \in K \cap \Lambda$ or decision that there is none
\begin{enumerate*}
\item[(1)] If $n = 1$, use binary search to find integer multiple of $\lambda_1 (\Lambda, [-1,1])$ in $K$ or certify none exists.
\item[(2)] Use Lemma~\ref{lem:WellScaleKorDecideKisThin}. If case (b) happens, obtain hyperplane $H$ with $K \cap \Lambda \subseteq H$. Recurse on $\textsc{Dadush}(K \cap H, \Lambda \cap H)$ and return the answer.
\item[(3)] Compute a subspace $W \subseteq \setR^n$ with $d := \dim(W)$ and $R := (\frac{\det(\Pi_W(\Lambda))}{\Vol_d(\Pi_W(K))})^{1/d}$ so that $R \leq \mu(\Lambda,K) \leq \rho(n) \cdot R$.
\item[(4)] Set $\tilde{K} := \min\{ \rho(n) \cdot R,1 \} \cdot (K-c)+c$ for some $c \in K$.
\item[(5)] Compute an $M$-ellipsoid $\pazocal{E} \subseteq W$ for $\Pi_W(\tilde{K})$.
\item[(6)] Compute $N \leq 2^{O(d)}$ points $x_1,\ldots,x_N \in W$ so that $\Pi_W(\tilde{K}) \subseteq \bigcup_{i=1}^N (x_i + \pazocal{E})$. 
\item[(7)] Compute $X := \Pi_W(\tilde{K}) \cap \Pi_W(\Lambda) = \big(\bigcup_{i=1}^N ((x_i+\pazocal{E}) \cap \Pi_W(\Lambda))\big) \cap \Pi_W(\tilde{K})$.
\item[(8)] Recursively call ${\textsc{Dadush}}(\tilde{K} \cap \Pi_{W}^{-1}(x),\Lambda \cap \Pi_W^{-1}(x))$ for all $x \in X$ and return any found lattice point (if there is any).
\end{enumerate*}
\end{minipage}}
\end{center}
Here, to be more informative, we have expanded the blackbox from Theorem~\ref{thm:LatticeEnumeration}
into lines (5)-(7).
The reader may also note a subtlety here that we have not discussed so far: if $K$ is very large
so that $\mu(\Lambda,K) \ll 1$, then we may shrink $K$ to a smaller body $\tilde{K} \subseteq K$
as long as we ensure that still $\mu(\Lambda, \tilde{K}) \leq 1$. We can now finish the analysis:
\begin{theorem}
Dadush's algorithm finds a point in $K \cap \Lambda$ in time $(\log(2n))^{O(n)}$ if there is one.
\end{theorem}%$
\begin{proof}
 If the algorithm recurses in (2), the claim is clear by induction. So assume otherwise. 
  First we argue correctness of the algorithm.
  Let  $s := \min\{ \rho(n) \cdot R,1\} \in [0,1]$  and recall that $\tilde{K} \subseteq K$ is a scaling of $K$
  by a factor of $s$. After step (4), the algorithm searches for a lattice point in $\tilde{K}$ rather than in the
  original body $K$. If $s<1$, then the covering radius of the shrunk body is $\mu(\Lambda,\tilde{K}) = \frac{1}{\rho(n) \cdot R} \mu(\Lambda,K) \leq 1$. In other words, even though we continue the search in the strictly smaller body $\tilde{K}$, we are still guaranteed that $\tilde{K} \cap \Lambda \neq \emptyset$. 
 Next, we discuss the running time of the algorithm. We estimate that 
  \begin{eqnarray*}
  G(\Pi_W(\Lambda),\Pi_W(\tilde{K})) &\stackrel{\textrm{Lem~\ref{lem:UpperBoundOnLatticePoints}}}{\leq}& 2^d \max\big\{ \mu(\Pi_W(\Lambda),\Pi_W(\tilde{K}))^d,1\big\} \cdot \frac{\Vol_d(\Pi_W(\tilde{K}))}{\det(\Pi_W(\Lambda))} \\
                                          &\leq& 2^d \max\Big\{ \Big(\underbrace{\frac{\rho(n) R}{s}}_{\geq 1}\Big)^d,1\Big\} \cdot s^d \cdot \underbrace{\frac{\Vol_d(\Pi_W(K))}{\det(\Pi_W(\Lambda))}}_{=R^{-d}} \\
    &=& 2^d \cdot (\rho(n) R)^d \cdot R^{-d} = (2 \rho(n))^d.
  \end{eqnarray*}
  Here we use that $\mu(\Pi_W(\Lambda),\Pi_W(\tilde{K})) \leq \mu(\Lambda,\tilde{K}) = \frac{1}{s} \cdot \mu(\Lambda,K) \leq \frac{\rho(n) \cdot R}{s}$. Then $|X| \leq G(\Pi_W(\Lambda),\Pi_W(\tilde{K})) \leq 2^d\rho(n)^d$ and by Lemma~\ref{lem:UpperBoundOnLatticePoints}, the computation of $X$ in (5)-(7) takes time $2^{O(d)} \rho(n)^d$.
Now, let $T(n)$ be the maximum running time of the algorithm on $n$-dimensional instances. Then we have the recursion
\[
 T(n) \leq \max_{d \in \{ 1,\ldots,n\}} \Big\{ 2^{O(n)} + (O(1) \cdot \rho(n))^d \cdot T(n-d)\Big\} \quad \textrm{and} \quad T(1)=\Theta(1),
\]
which indeed resolves to  $T(n) \leq O(\rho(n))^n$.
\end{proof}


We also explain how Dadush's algorithm can be used to solve integer linear programs in time $(\log(2 n))^{O(n)}$.
Again, the arguments used are standard. Details on the estimates can be found in the book of Schrijver~\cite{TheoryOfLPandILP-Schrijver1999}. 
\begin{proof}[Proof of Theorem~\ref{thm:SolvingExplicitIPinLogNtoN}]
  Consider an arbitrary integer linear program $\max\{ c^Tx \mid Ax \leq b, x \in \setZ^n\}$. One can
  compute a number $M$ in time polynomial in $n$ and the encoding length of $A$ and $b$ so that
  if the IP is bounded and feasible, then the optimum value is the same as $\max\{ c^Tx \mid Ax \leq b, \|x\|_{\infty} \leq M, x \in \setZ^n\}$.
  Next, by applying binary search, it suffices to find an integer point in the compact convex
  set $K = \{ x \in \setR^n \mid c^Tx \geq \delta, Ax \leq b, \|x\|_{\infty} \leq M\}$ for which Theorem~\ref{thm:SolvingIPinLogNtoN} applies.
\end{proof}

\section{Implications of Theorem~\ref{thm:KLConj}\label{sec:Implications}}

Here we derive a few implications of our main result. The following classical inequality will be useful here:

\begin{lemma}[\cite{Rogers1957TheDB}] \label{lem:RSineq}
For any convex set $K \subseteq \setR^n$ we have $\Vol_n (K-K) \le {2n \choose n} \cdot \Vol_n (K)$.
\end{lemma}

We restate Theorem~\ref{thm:CoveringRadiusKvsKminusK}, which yields a nearly tight relationship between the covering radii of $K$ and $K-K$. We remark that it remains an open question whether the two quantities are equal up to a constant.

\begin{theorem*}[Theorem~\ref{thm:CoveringRadiusKvsKminusK}] %\label{cor:CoveringRadiusKvsKminusK}
For any full rank lattice $\Lambda \subseteq \setR^n$ and any convex body $K \subseteq \setR^n$, one has \[\mu(\Lambda,K-K) \leq \mu(\Lambda,K) \leq O(\log^{3} (2n)) \cdot \mu(\Lambda,K-K).\]
\end{theorem*}

\begin{proof}
Let $W$ denote the subspace attaining $\mu_{KL}(\Lambda, K)$ with $\dim W = d$. We can use Theorem~\ref{thm:KLConj} to upper bound 
 \begin{eqnarray*}
\mu(\Lambda, K) \lesssim \log^{3} (2n) \cdot \mu_{KL} (\Lambda, K) & = & \log^{3} (2n) \cdot \Big(\frac{\det(\Pi_W(\Lambda))}{\Vol_d(\Pi_W(K))}\Big)^{1/d} \\ & \stackrel{\textrm{Lem~\ref{lem:RSineq}}}{\lesssim} & \log^{3} (2n) \cdot 4 \cdot \Big(\frac{\det(\Pi_W(\Lambda))}{\Vol_d(\Pi_W(K-K))}\Big)^{1/d} \\ & \lesssim & \log^{3} (2n) \cdot \mu_{KL}(\Lambda, K-K)\\ & \lesssim & \log^{3} (2n) \cdot \mu(\Lambda, K-K). \qedhere
 \end{eqnarray*}
\end{proof}

This in turn implies that the \emph{flatness constant} in dimension $n$ is bounded by $O(n\log^{3}(2n))$:
\begin{theorem*}[Theorem~\ref{thm:FlatnessConstant}]
  For any convex body $K \subseteq \setR^n$ and any full rank lattice $\Lambda \subseteq \setR^n$, one has
  \[
   \mu(\Lambda,K) \cdot \lambda_1(\Lambda^{*}, (K-K)^{\circ}) \leq O(n \log^{3}(2n)).
  \]
\end{theorem*}

\begin{proof}
  First we show a slightly worse bound of $O(n \log^4(2n))$. Banaszczyk~\cite{Banaszczyk1996TransferenceTheoremsForGeneralConvexBodies} proved that for any symmetric convex
  body $Q \subseteq \setR^n$ one has $\mu(\Lambda,Q) \cdot \lambda_1(\Lambda^*,Q^{\circ}) \leq O(n \log(2n))$. 
  Setting $Q := K-K$ (which is a symmetric convex body) one then has by Theorem~\ref{thm:CoveringRadiusKvsKminusK}
  \[\mu(\Lambda,K) \cdot \lambda_1(\Lambda^{*}, Q^{\circ}) \leq O(\log^3(2n)) \cdot \mu(\Lambda,Q) \cdot \lambda_1(\Lambda^{*}, Q^{\circ}) \leq O(n\log^{4}(2n)).\]

  Now we give the argument of the stronger bound of $O(n \log^3(2n))$ which is due to Dadush.
  Let $W$ denote the subspace attaining $\mu_{KL}(\Lambda, K)$ with $\dim W = d$. By Theorem~\ref{thm:KLConj},
 \begin{eqnarray*}
\mu(\Lambda, K) \lesssim \log^{3} (2n) \cdot \mu_{KL} (\Lambda, K) & = & \log^{3} (2n) \cdot \Big(\frac{\det(\Pi_W(\Lambda))}{\Vol_d(\Pi_W(K))}\Big)^{1/d} \\ & \stackrel{\textrm{Lem~\ref{lem:RSineq}}}{\lesssim} & \log^{3} (2n) \cdot 4 \cdot \Big(\frac{\det(\Pi_W(\Lambda))}{\Vol_d(\Pi_W(Q))}\Big)^{1/d} \\ & \stackrel{\textrm{Lem~\ref{thm:BSBM}}}{\asymp} & \log^{3} (2n) \cdot d \cdot \Big(\frac{\Vol_d(Q^\circ \cap W)}{\det(\Lambda^* \cap W)}\Big)^{1/d} \\ & \stackrel{\textrm{Thm~\ref{thm:Minkowski}}}{\lesssim} & n\log^{3} (2n) \cdot \frac{2}{\lambda_1(\Lambda^* \cap W, Q^\circ \cap W)}.
 \end{eqnarray*}
Here, we have used that $\Pi_W(\Lambda)^* = \Lambda^* \cap W$.
 Since $\lambda_1(\Lambda^*, Q^\circ) \le \lambda_1(\Lambda^* \cap W, Q^\circ \cap W)$, the theorem follows.
%  an $O(n \log n)$ bound follows from the result of~\cite{TransferenceTheorems-Banaszczyk93}. Now the theorem follows from Corollary~\ref{cor:CoveringRadiusKvsKminusK}.
\end{proof}


We also explain the proof of Corollary~\ref{cor:FlatnessConstantSimple} which again is standard: 
\begin{corollary*}[Cor~\ref{cor:FlatnessConstantSimple}]
  Let $K \subseteq \setR^n$ by a convex body with $K \cap \setZ^n = \emptyset$.
  Then there is a vector $c \in \setZ^n \setminus \{ \bm{0}\}$ so that at most $O(n \log^{3}(2n))$ many hyperplanes of the form $\langle c, x \rangle = \delta$ with $\delta \in \setZ$ intersect $K$.
\end{corollary*}
\begin{proof}
  We apply Theorem~\ref{thm:FlatnessConstant} for the lattice $\Lambda := \setZ^n$ so that
  $\Lambda^* = \setZ^n$. Then $K \cap \setZ^n = \emptyset$ implies that $\mu(\setZ^n,K) > 1$
  and so $\lambda_1(\setZ^n, (K-K)^{\circ}) \lesssim n \log^3(2n)$. Let $c \in \setZ^n \setminus \{ \bm{0}\}$ be the vector attaining this bound. Then
  revisiting the definition of the dual norm (Sec~\ref{sec:LValueAndVolEstimates}) we have
  $
    \max\{ \left<c,x-y\right> : x,y \in K\} = \|c\|_{(K-K)^{\circ}} 
  $.
  That means at most $\|c\|_{(K-K)^{\circ}} +1 \lesssim n \log^3(2n)$ hyperplanes
  of the form $\left<c,x\right> = \delta$ with $\delta \in \setZ$ intersect $K$.
\end{proof}
%We also remind the reader why Theorem~\ref{thm:FlatnessConstant} implies a bound on the flatness constant. Let $K \subseteq \setR^n$ be a convex body with $K \cap \setZ^n = \emptyset$. Then $\mu(\setZ^n , K) > 1$, so that Theorem~\ref{thm:FlatnessConstant} yields a vector $c \in (\setZ^n)^* \setminus \{\bm{0}\} = \setZ^n \setminus \{\bm{0}\}$ with $c \in O(n \log^3(2n)) (K-K)^\circ$, i.e. $\langle c , x-y\rangle \le O(n \log^3(2n))$ for all $x, y\in K$. It follows that at most $O(n \log^3(2n))$ hyperplanes of the form $\langle c,x \rangle = \delta$ with $\delta \in \setZ$ intersect $K$.


\paragraph{Acknowledgement.} The authors are grateful to Daniel Dadush for numerous discussions on
related topics, a careful read of a preliminary draft, and for the proof of the improved bound in Theorem~\ref{thm:FlatnessConstant}. The authors would also like to thank the anonymous reviewers who made several helpful suggestions.

\bibliographystyle{alpha}
\bibliography{KLconjecture}


\appendix


\section{The approximate canonical filtration} \label{appendix:ApproximateFiltration}

In this chapter, we prove Theorem~\ref{thm:ApproxFilt}. The proof idea is rather simple: given a $t$-stable
filtration $\{ \bm{0}\} = \Lambda_0 \subset \ldots \subset \Lambda_k = \Lambda$, we select one index from every density class in order to make the filtration well-separated. But before we come to the main argument, we require two lemmas.
\begin{lemma}[Grayson's parallelogram rule~\cite{ajm/1118669693}] \label{lem:ParallelogramRule}
For any two lattices $\Lambda, \Lambda' \subseteq \setR^n$,
\[\det(\Lambda) \cdot \det (\Lambda') \ge \det(\Lambda + \Lambda') \cdot \det(\Lambda \cap \Lambda'). \]
\end{lemma}
A proof may also be found in Chapter 2 of \cite{PhDThesisStephens-Davidowitz2017}.
The $t$-stable filtration can be used to obtain lower bounds on the determinant of any sublattice: 

%
%\begin{lemma} \label{lem:CanonicalFiltrationLowerBound}
%  Let $\Lambda \subseteq \setR^n$ be any lattice and let $\{ \bm{0}\} = \Lambda_0 \subset \Lambda_1 \subset \ldots \subset \Lambda_k = \Lambda$ be the canonical filtration. Let $r_i := \det(\Lambda_i / \Lambda_{i-1})^{1/\textrm{rank}(\Lambda_i/\Lambda_{i-1})}$ be the normalized determinant. Then for any sublattice $\tilde{\Lambda} \subseteq \Lambda$ and any $i \in \{ 1,\ldots,k\}$ the following holds:
%  \[
%    \det(\tilde{\Lambda}) \geq \det(\Lambda_i) \cdot r_i^{\textrm{rank}(\tilde{\Lambda})-\textrm{rank}(\Lambda_i)} = \det(\Lambda_{i-1}) \cdot r_i^{\textrm{rank}(\tilde{\Lambda})-\textrm{rank}(\Lambda_{i-1})}.
%  \]
%\end{lemma}
%\begin{proof}
%  Note that the slope of the segment between points $i-1$ and $i$ on the canonical plot is
%  \[
%   \frac{\ln(\det(\Lambda_i)) - \ln(\det(\Lambda_{i-1}))}{\textrm{rank}(\Lambda_i)-\textrm{rank}(\Lambda_{i-1})} = \ln(r_i)
% \]
% Hence by convexity, the point $(\textrm{rank}(\tilde{\Lambda}),\ln(\det(\tilde{\Lambda})))$ has to lie above the line with slope $\ln(r_i)$ that goes through the point $(\textrm{rank}(\Lambda_i),\ln(\det(\Lambda_i)))$. That means
% \begin{eqnarray*}
%   & & \ln(\det(\tilde{\Lambda})) \geq \ln(r_i) \cdot (\textrm{rank}(\tilde{\Lambda})-\textrm{rank}(\Lambda_i)) + \ln(\det(\Lambda_i)) \\
%   &\Leftrightarrow& \det(\tilde{\Lambda}) \geq r_i^{\textrm{rank}(\tilde{\Lambda})-\textrm{rank}(\Lambda_i)} \cdot \det(\Lambda_i)  = \det(\Lambda_{i-1}) \cdot r_i^{\textrm{rank}(\tilde{\Lambda})-\textrm{rank}(\Lambda_{i-1})}.
% \end{eqnarray*}

\begin{lemma}\label{lem:CanonicalFiltrationLowerBound}

Let $\Lambda \subseteq \setR^n$ be any lattice and let $\{ \bm{0}\} = \Lambda_0 \subset \Lambda_1 \subset \ldots \subset \Lambda_k = \Lambda$ be a $t$-stable filtration.  Then for any sublattice $\tilde{\Lambda} \subseteq \Lambda$ we have the inequality
\[ \nd(\tilde{\Lambda}) \ge t^{-1} \cdot \nd(\Lambda_1). \]
\end{lemma}
\begin{proof} %\rem{T: I expanded a little and fixed a few indices. Please check!}
  Let $r_i := \nd(\Lambda_i / \Lambda_{i-1}) = \det(\Lambda_i / \Lambda_{i-1})^{1/\textrm{rank}(\Lambda_i/\Lambda_{i-1})}$ be the normalized determinant. We prove by induction on $i \in \{1, \dots, k\}$ that the result holds for all lattices $\tilde{\Lambda} \subseteq \Lambda_i$. The base case follows as $\Lambda_1 = \Lambda_1 / \Lambda_0$ is a scalar of the $t$-stable
  lattice $\frac{1}{\textrm{nd}(\Lambda_1)}\Lambda_1$. Now suppose that $\tilde{\Lambda} \subseteq \Lambda_i$ for some $i > 1$. Note that $\Lambda_+ := \tilde{\Lambda} + \Lambda_{i-1}$ satisfies $\Lambda_{i-1} \subseteq \Lambda_+ \subseteq \Lambda_{i}$, so that $\Lambda_+ / \Lambda_{i-1} \subseteq \Lambda_i/\Lambda_{i-1}$ and $\nd(\Lambda_+ /\Lambda_{i-1}) \ge t^{-1} \cdot r_i >t^{-1}\cdot r_1$. By Lemma~\ref{lem:ParallelogramRule},
  \[
    \det(\tilde{\Lambda}) \cdot\det(\Lambda_{i-1})  \ge \det(\tilde{\Lambda} + \Lambda_{i-1}) \cdot \det(\tilde{\Lambda} \cap \Lambda_{i-1}).
  \]
  Factoring out $\Lambda_{i-1}$ gives
  \[
   \det(\tilde{\Lambda}) \geq \det(\Lambda_+ / \Lambda_{i-1}) \cdot \det(\tilde{\Lambda} \cap \Lambda_{i-1}).
  \]
  Hence
  \[
    \nd(\tilde{\Lambda}) \ge \nd(\Lambda_+/\Lambda_{i-1})^{\mathrm{rank}(\Lambda_+ / \Lambda_{i-1})/\mathrm{rank}(\tilde{\Lambda})} \cdot \nd(\tilde{\Lambda} \cap \Lambda_{i-1})^{\mathrm{rank}(\tilde{\Lambda} \cap \Lambda_{i-1})/\mathrm{rank}(\tilde{\Lambda})} \ge t^{-1} \cdot r_1
  \]
  where we used the inductive hypothesis on $\tilde{\Lambda} \cap \Lambda_{i-1} \subseteq \Lambda_{i-1}$ together with the fact that $\mathrm{rank}(\Lambda_+/\Lambda_{i-1}) + \mathrm{rank}(\tilde{\Lambda}\cap \Lambda_{i-1}) = \mathrm{rank}(\tilde{\Lambda})$. \end{proof}



Now, we come to the main argument:
\begin{proof}[Proof of Theorem~\ref{thm:ApproxFilt}]
  Let $r_i := \nd(\Lambda_i / \Lambda_{i-1})$ and $d_i := \textrm{rank}(\Lambda_i / \Lambda_{i-1})$. For $\ell \in \setZ$ denote $I_{\ell} := \{ i \in [k] : 2^{\ell} \leq r_i < 2 \cdot 2^{\ell}\}$.
  We define a sequence of indices $0=\ell(0)<\ell(1)<\ldots<\ell(\tilde{k}) = k$ that contains precisely the largest index $i$ in each $I_{\ell}$ with $I_{\ell} \neq \emptyset$ plus the index $\ell(0) = 0$.
  We set  $\tilde{\Lambda}_j := \Lambda_{\ell(j)}$ and $\tilde{r}_j := \nd(\tilde{\Lambda}_j / \tilde{\Lambda}_{j-1})$. First, 
  consider an index $\ell$ with $I_{\ell} \neq \emptyset$. Let $i_{\min},i_{\max} \in I_{\ell}$ be the minimal and maximal indices in $I_\ell$. Then
  \begin{eqnarray*}% first step: Lem~\ref{lem:PropertiesQuotientLattice}}.(a)
    \det(\Lambda_{i_{\max}} / \Lambda_{i_{\min}-1})^{1/\textrm{rank}(\Lambda_{i_{\max}}/\Lambda_{i_{\min}-1})} &=& \Big(\prod_{i=i_{\min}}^{i_{\max}} \det(\Lambda_{i}/\Lambda_{i-1})\Big)^{1 / \sum_{i=i_{\min}}^{i_{\max}} \textrm{rank}(\Lambda_i/\Lambda_{i-1})} \\
    &=& \Big(\prod_{i=i_{\min}}^{i_{\max}} r_i^{d_i}\Big)^{1/\sum_{i=i_{\min}}^{i_{\max}} d_i}.
  \end{eqnarray*}
  Note that this value is a weighted geometric average of $r_i$-values for $i \in I_{\ell}$.
  From this it immediately follows that $\tilde{r}_1 < \ldots < \tilde{r}_k$ and $\tilde{r}_j \leq \frac{1}{2}\tilde{r}_{j+2}$ for all $j$, i.e. (a') holds.  
  % Now, let us clean up the notation and focus on the sparser filtration  $\{ \bm{0}\} = \Lambda_0 \subset \ldots \subset \Lambda_k = \Lambda$ that is the outcome of the above process.
  It remains to show that the quotient lattices are scalars of $2t$-stable lattices. Fix some index $j \in [\tilde{k}]$ and let $\Lambda' := \frac{1}{\tilde{r}_j} (\tilde{\Lambda}_{j}/\tilde{\Lambda}_{j-1})$. First note that by assumption, the filtration  $\{\bm{0}\} = \Lambda'_0 \subset \cdots \subset \Lambda'_{k'} := \Lambda'$ given by $\Lambda'_i := \frac{1}{\tilde{r}_j} (\Lambda_{\ell(j-1) + i}/\Lambda_{\ell(j-1)})$ with $k' := \ell(j) - \ell(j-1)$ is also $t$-stable
  because $\Lambda_{i+1}'/\Lambda_i' = \frac{1}{\tilde{r}_j} (\Lambda_{\ell(j-1)+i+1}/\Lambda_{\ell(j-1)+i})$.

  %Note that the weighted geometric average of these would be between 1 and 2.
  We will prove the following
  two statements.
  \begin{enumerate}
  \item[$(I)$] For any sublattice $\tilde{\Lambda} \subseteq \Lambda'$ one has $\nd(\tilde{\Lambda}) \geq (2t)^{-1}$.
  \item[$(II)$] For any sublattice $\tilde{\Lambda} \subseteq (\Lambda')^*$ one has $\nd(\tilde{\Lambda}) \geq (2t)^{-1}$.
  \end{enumerate}
  First we show $(I)$. We apply Lemma~\ref{lem:CanonicalFiltrationLowerBound} on $\Lambda'$ to obtain
  \[
   \nd(\tilde{\Lambda}) \ge t^{-1} \cdot \nd(\Lambda'_1) \ge t^{-1} \cdot \frac{r_{\ell(j-1)+1}}{\tilde{r}_j} \geq (2t)^{-1},
  \]
  since both numerator and denominator belong to the same interval $[2^\ell, 2 \cdot 2^\ell)$ for some $\ell \in \mathbb{Z}$.
  Next, we prove $(II)$. Given the filtration $\{\bm{0}\} = \Lambda'_0 \subset \cdots \subset \Lambda'_{k'} = \Lambda'$ with $U_i := \span(\Lambda'_i)$, the dual filtration is given by $\{\bm{0}\} = (\Lambda')^*_0 \subset \cdots \subset (\Lambda')^*_{k'} = (\Lambda')^*$ with $(\Lambda')^*_i := \Lambda^* \cap U_{k'-i}^\perp$ and determinant $\det((\Lambda')^*_i) = \det((\Lambda')^*) \cdot \det(\Lambda'_{k'-i}) = \det(\Lambda'_{k'-i})$, see for example~\cite{Dadush-Finding-DenseLatticeSubspacesSTOC19}. %\rem{T: Are you certain that the indices here are right?}
  Since quotients of the dual filtration are duals of the quotients of the original filtration, the dual filtration is also $t$-stable. We then apply Lemma~\ref{lem:CanonicalFiltrationLowerBound} on $(\Lambda')^*$:
 \[
   \nd(\tilde{\Lambda}) \ge t^{-1} \cdot \nd( (\Lambda')^*_1 ) = t^{-1} \cdot (r'_{k'})^{-1} = t^{-1} \cdot \Big(\frac{r_{\ell(j)}}{\tilde{r}_j}\Big)^{-1} \stackrel{r_{\ell(j)} \leq 2\cdot \tilde{r}_j}{\geq} (2t)^{-1}. \qedhere
  \]

%\begin{eqnarray*}% First step: \textrm{Lem~\ref{lem:PropertiesLatticeSubspace}}.(b)
%  \det((\Lambda')^* \cap W) &=& \det(\Lambda' \cap W^{\perp}) \cdot \det((\Lambda')^*) \\ &=& \frac{\det(\Lambda' \cap W^{\perp})}{\det(\Lambda')} \\ &\stackrel{\textrm{Lem~\ref{lem:CanonicalFiltrationLowerBound}}.(a)}{\geq}& \Big(\frac{r_{i(j)}}{\tilde{r}_j}\Big)^{\textrm{rank}(\Lambda' \cap W^{\perp})-\textrm{rank}(\Lambda')}
%  = \Big(\frac{r_{i(j)}}{\tilde{r}_j}\Big)^{-d} \stackrel{r_{i(j)} \leq 2\cdot \tilde{r}_j}{\geq} 2^{-d}
%\end{eqnarray*}
%where we apply Lem~\ref{lem:CanonicalFiltrationLowerBound}.(a) with index $i=k'$.
\end{proof}

\end{document}


