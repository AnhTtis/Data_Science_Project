\documentclass[10pt]{article} % For LaTeX2e
\usepackage[preprint]{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
%\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
% \input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{stmaryrd}

\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing,angles,quotes,arrows.meta}
\usepackage{colortbl}

%wrapped figures
\usepackage{wrapfig}
\usepackage{cutwin}



\newcommand{\norm}[1]{|| #1 ||}
\newcommand{\abso}[1]{| #1 |}


\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\G}{\mathbb{G}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\N}{\mathbb{N}}
\renewcommand{\S}{\mathbb{S}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\sph}{\mathbb{S}}


\newcommand{\dd}{\mathrm{d}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\sign}{sign}
\newcommand{\sgn}{\mathrm{sgn}}



\newcommand{\calC}{\mathcal{C}}
\newcommand{\calE}{\mathcal{E}}
\newcommand{\calI}{\mathcal{I}}
\newcommand{\calH}{\mathcal{H}}
\newcommand{\calK}{\mathcal{K}}
\newcommand{\calL}{\mathcal{L}}
\newcommand{\calN}{\mathcal{N}}
\newcommand{\calNC}{\mathcal{NC}}
\newcommand{\calNS}{\mathcal{NS}}
\newcommand{\calNR}{\mathcal{NR}}
\newcommand{\calNN}{\mathcal{NN}}
\newcommand{\calP}{\mathcal{P}}
\newcommand{\calPP}{\mathcal{PP}}
\newcommand{\calR}{\mathcal{R}}
\newcommand{\calRP}{\mathcal{RP}}
\newcommand{\calRPP}{\mathcal{RPP}}
\newcommand{\calRT}{\mathcal{RT}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calY}{\mathcal{Y}}
\newcommand{\calZ}{\mathcal{Z}}

\newcommand{\calS}{\mathcal{S}}
\newcommand{\calU}{\mathcal{U}}
\newcommand{\calV}{\mathcal{V}}
\newcommand{\calT}{\mathcal{T}}

\newcommand{\Stab}{\mathrm{Stab}}
\newcommand{\Rast}{\mathrm{RAST}}

\newcommand{\one}{\mathds{1}}

\newcommand{\sse}{\subseteq}

\newcommand{\D}{\mathrm{D}}
\newcommand{\twoD}{{$2\D$}}
\newcommand{\threeD}{{$3\D$}}

\DeclareMathOperator{\SO}{SO}
\DeclareMathOperator{\SU}{SU}
\DeclareMathOperator{\Ort}{O}
\DeclareMathOperator{\SE}{SE}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\T}{T}

\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\SL}{SL}
\DeclareMathOperator{\PGL}{PGL}
\DeclareMathOperator{\PSL}{PSL}
\DeclareMathOperator{\PL}{PL}

\DeclareMathOperator{\Proj}{P}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\ran}{im} %byt mot im?
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Hom}{L}
\DeclareMathOperator{\Homm}{Hom}
\DeclareMathOperator{\THom}{TL}
\DeclareMathOperator{\Ad}{Ad}
\newcommand{\rmT}{\mathrm{T}}

\newcommand{\rR}{\mathrm{R}}

\newcommand{\veps}{\varepsilon}

\newcommand{\transrott}{\textsc{TransRot}}
\newcommand{\onedtranss}{\textsc{OneDTrans}}
\newcommand{\twodtranss}{\textsc{TwoDTrans}}

\newcommand{\widecheck}[1]{{#1}^{\vee}}
\newcommand{\comm}[1]{ \{ #1 \} } 
\newcommand{\sprod}[1]{\langle #1 \rangle}
\newcommand{\ang}[1]{#1^\circ}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% For varying group in equation labels. See https://tex.stackexchange.com/questions/430319/labels-and-tags-with-parametric-dependence
\DeclareRobustCommand\groupG{G}
\DeclareRobustCommand\groupH{H}
\DeclareRobustCommand\galg{\mathfrak{g}}
\DeclareRobustCommand\halg{\mathfrak{h}}

\newcommand{\myparagraph}[1]{\emph{{#1}.}}
% \let\myparagraph\paragraph

\usepackage{xfrac}

\usepackage{svg}
\usepackage{picinpar}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{lscape}
\usepackage{xcolor}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{thmtools,thm-restate}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{innercustomthm}{Assumption}
\newenvironment{customass}[1]
  {\renewcommand\theinnercustomthm{#1}\innercustomthm}
  {\endinnercustomthm}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}



\newtheorem{example}[theorem]{Example}

\usepackage{enumerate}
\usepackage{dsfont}

\title{Optimization Dynamics of Equivariant and Augmented Neural Networks}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{
      \name \!\!Oskar Nordenfors \email oskar.nordenfors@umu.se\\
      \addr Department of Mathematics and Mathematical Statistics \\
      Umeå University
      \AND
      \name Fredrik Ohlsson \email fredrik.ohlsson@umu.se\\
      \addr Department of Mathematics and Mathematical Statistics \\
      Umeå University
      \AND\name Axel Flinth\email axel.flinth@umu.se\\
      \addr Department of Mathematics and Mathematical Statistics \\
      Umeå University
      }



% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\def\month{12}  % Insert correct month for camera-ready version
%\def\year{2023} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=Ls1E16bTj8}} % Insert correct link to OpenReview for camera-ready version

%%%%%%%%%%%%%%%%%%%%%%%%%   OUR MACROS  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\aug}{\mathrm{aug}}
\newcommand{\FA}{\mathrm{FA}}
\newcommand{\erw}{\mathbb{E}} % Av gammal vana använder jag(Axel) macrot \erw ('Erwartungswert') för väntevärde

\newcommand{\res}{\mathrm{res}}
\newcommand{\eqv}{\mathrm{eqv}}
\newcommand{\nom}{\mathrm{nom}}

\newcommand{\tensortwo}{{\otimes 2}}
\newcommand{\otensor}{\otimes}
\newcommand{\hess}{\nabla^2}
\newcommand{\spec}{\mathrm{spec}}
\newcommand{\indrho}{\overline{\rho}}


\newcommand{\perm}{\mathrm{perm}}
\newcommand{\trans}{\mathrm{tr}}
\newcommand{\rot}{\mathrm{rot}}

\newcommand{\calD}{\mathcal{D}}

\newcommand{\calF}{\mathcal{F}}


\newcommand{\nominal}{\textsc{Nom}}
\newcommand{\augm}{\textsc{Aug}}
\newcommand{\eqvi}{\textsc{Equi}}

\newcommand{\permm}{\textsc{Perm}}
\newcommand{\transs}{\textsc{Trans}}
\newcommand{\rott}{\textsc{Rot}}


\begin{document}

\maketitle

\begin{abstract}
We investigate the optimization of neural networks on symmetric data, and compare the strategy of constraining the architecture to be equivariant to that of using data augmentation. Our analysis reveals that the relative geometry of the admissible and the equivariant layers, respectively, plays a key role. Under natural assumptions on the data, network, loss, and group of symmetries, we show that compatibility of the spaces of admissible layers and equivariant layers, in the sense that the corresponding orthogonal projections commute, implies that the sets of equivariant stationary points are identical for the two strategies. If the linear layers of the network also are given a unitary parametrization, the set of equivariant layers is even invariant under the gradient flow for augmented models. Our analysis however also reveals that even in the latter situation, stationary points may be unstable for augmented training although they are stable for the manifestly equivariant models.
\end{abstract}

\section{Introduction}
In machine learning, the general goal is to find 'hidden' patterns in data. However, there are sometimes symmetries in the data that are known a priori. Incorporating these manually should, heuristically, reduce the complexity of the learning task. In this paper, we are concerned with training neural networks on data exhibiting symmetries that can be formulated as equivariance under a group action. A standard example is the case of translation invariance in image classification.

More specifically, we want to theoretically study the connections between two general approaches to incorporating symmetries. The first approach is to construct equivariant models by means of \emph{architectural design}. This framework, known as \emph{Geometric Deep Learning}~\citep{bronstein2017geometric, bronstein2021geometric}, exploits the geometric origin of the group $G$ of symmetry transformations by choosing the linear layers and nonlinearities to be equivariant (or invariant) with respect to the action of $G$. In other words, the symmetry transformations commute with each linear (or affine) map in the network, which results in an architecture which manifestly respects the symmetry $G$ of the problem. One prominent example is the spatial weight sharing of convolutional neural networks (CNNs) which are equivariant under translations. Group equivariant convolution networks (GCNNs) \citep{cohen2016group, weiler2018learning,kondorGeneralizationEquivarianceConvolution2018} extends this principle to an exact equivariance under more general symmetry groups.  The second approach is agnostic to model architecture, and instead attempts to achieve equivariance during training via \emph{data augmentation}, which refers to the process of extending the training to include synthetic samples obtained by subjecting training data to random symmetry transformations. 

Both approaches have their benefits and drawbacks. Equivariant models use parameters efficiently through weight sharing along the orbits of the symmetry group, but are difficult to implement and computationally expensive to evaluate in general, since they entail numerical integration over the symmetry group (see, e.g.,~\citet{kondorGeneralizationEquivarianceConvolution2018}). Data augmentation, on the other hand, is agnostic to the model structure and easy to adapt to different symmetry groups. However, the augmentation strategy is by no means guaranteed to achieve a model which is exactly equivariant: the hope is that model will 'automatically' infer invariances from the data, but there are few theoretical guarantees. Also, augmentation in general entails an inefficient use of parameters and an increase in model complexity and training required. 

In this paper, we study and compare the training dynamics of the two strategies as follows. We consider a nominal architecture (i.e. not equivariant by design) defined by restricting the linear layers of a multilayer perceptron (i.e. fully connected neural network without biases) to a certain affine subspace $\calL$. In this way, we can treat many commonly used architectures, such as CNNs, transformer architectures, recurrent neural networks (RNNs), etc. We then train it using gradient flow, either on augmented data or while restricting the weights to also lie in the space of equivariant linear maps $\calH_G$. Our results apply to all compact groups.

Our analysis reveals that under a few natural assumptions, including that the augmentation is performed with respect to the \emph{Haar measure}, and a \emph{compatibility assumption} (that the orthogonal projections onto $\rmT\calL$ and onto $\calH_G$ commute) a surprisingly simple relation between the sets of  stationary points $S^{\aug}$ and $S^{\eqv}$  \emph{lying in $\calE\vcentcolon= \calL \cap \calH_G$} of the augmented model and the restricted model respectively: %In particular, we study their stationary points, i.e., the potential limit points of the training, in $\calE$. Our analysis reveal the following (surprisingly) simple relations between them: %The equivariant models (of course) have optima for their gradient flow there and so are guaranteed to produce a trained model which respects the symmetry. However, the dynamics under augmented training has previously not been exhaustively explored.
%Our main contributions are as follows:  First, we provide a technical statement (Lemma \ref{lem:ind_rep_aug_risk}) about the augmented risk: We show that it can be expressed as the nominal risk averaged over the symmetry group acting on the layers of the MLP. Using this formulation of the augmented risk, we apply standard methods from the analysis of dynamical systems to show: %to the gradient descent for the equivariant and augmented models to show:
\begin{enumerate}[(i)]
    \item $S^\eqv = S^\aug$ (Theorem \ref{theo:gradients_equal}). In other words,  augmentation neither introduces new equivariant stationary points, nor does it exclude existing ones, compared to restricting the architecture.
    \item A stationary point in $\calE$ can simultaneously be stable for the equivariant strategy while unstable for the the augmented one, but not vice-versa. (Theorem \ref{th:stable}). In other words, while the equivariant and augmented models have the same stationary points in $\calE$, some of them may be impossible to actually obtain during training for the augmented ones. In particular, it is not guaranteed that the augmented model has any local minima in $\calE$.
\end{enumerate}
%In addition, if $\calL$ is parametrized using an the subspace $\calE$ is invariant under the gradient flow of the augmented model (Corollary \ref{cor:eqv_subs_invariant}). In other words, if the augmented model is equivariantly initialized, it will remain equivariant during training. 
We also show that under additional assumptions on the implementation of the architecture (related to how $\calL$ is parametrized), $\calE$ becomes an invariant set under the augmented flow (Theorem \ref{theo:gradients_equal}, part 2). Finally, we perform some simple numerical experiments to illustrate our findings.

\subsection{Related work}
% General refs for construction equivariant networks


The group theory based model for group augmentation we use here is heavily inspired by the framework developed in \citet{chen2020group}. Augmentation and manifest invariance/equivariance have been studied from this perspective in a number of papers \citep{lyle2019analysis,lyle2020benefits, mei2021learning,elesedy2021provably}. More general models for data augmentation have also been considered \citep{dao2019kernel}. Previous work has mostly been concerned with so-called kernel and \emph{feature-averaged} models, and in particular, fully general neural networks as we treat them here have not been considered. The works have furthermore mostly been concerned with proving statistical properties of the models, and not with studying their dynamics at training. An exception is~\citet{lyle2020benefits}, in which it is proven that in linear scenarios, the equivariant models are optimal, but little is known about more involved models.

The dynamics of training \emph{linear equivariant networks} (i.e., MLPs without nonlinearities) has been given some attention in the literature. Linear networks is a simplified, but nonetheless popular theoretical model for analysing neural networks~\citep{bah2022learning}. In \citet{lawrence2021implicit}, the authors analyse the implicit bias of training a linear neural network with one fully connected layer on top of an equivariant backbone using gradient descent. They also provide some numerical results for non-linear models, but no comparison to data augmentation is made. In \citet{chen2023implicit}, completely equivariant linear networks are considered, and an equivalence result between augmentation and restriction is proven for binary classification tasks. However, more realistic MLPs involving non-linearities are not treated at all.%However, as their results show (e.g Prop. 3.2 in \cite{chen2023implicit}), the non-inclusion of nonlinearities simplifies the structure of them a lot, so that the setting that we are consider really is different from theirs.}

Empirical comparisons of training equivariant and augmented non-equivariant models are common in the literature. Most often, the augmented models are considered as baselines for the evaluation of the equivariant models. More systemic investigations include \citet{gandikotaTrainingArchitectureHow2021,muller2021,gerken22a}. Compared to previous work, our formulation differs in that the parameter of the augmented and equivariant models are defined on the same vector spaces, which allows us to make a stringent mathematical comparison.


\section{Mathematical framework}
\label{sec:opt_sym_models}
Let us begin by setting up the framework (see Figure \ref{fig:enter-label}). We let $X$ and $Y$ be vector spaces and $\calD(x,y)$ be a joint distribution on $X \times Y$. We are concerned with training an neural network $\Phi_A : X \to Y$ so that $y \approx \Phi_A(x)$ is probable (with respect to $\calD$). The network has the form
\begin{align} \label{def:MLP}
    x_0 = x, \quad x_{i+1} = \sigma_i(A_ix_i), \quad i\in [L] = \{0, \dots, L-1\}, \quad  \Phi_A(x) = x_{L},
\end{align}
where $A_i: X_i \to X_{i+1}$ are linear maps (layers) between (hidden) vector spaces $X_i$ with $X=X_0$ and $Y=X_L$, and $\sigma_i : X_{i+1}\to X_{i+1}$ are non-linearities. %\footnote{Bias terms can also be included via the standard trick to write affine maps as linear -- see Appendix \ref{app:bias}.} 
Note that $A=(A_i)_{i \in [L]}$ parametrizes the network since the non-linearities are assumed to be fixed. Let us denote the space of all possible parameters $\calH = \oplus_{i \in [L]} \Homm(X_i,X_{i+1})$.

Note that while the network is well-defined for any choices of $A_i\in \Homm(X_i,X_{i+1})$, we may restrict the layer to some subset of $\Homm(X_i,X_{i+1})$ to define other architectures. Here, we assume that layers are confined to an \emph{affine subspace} $\mathcal{L} \sse \calH$. 
We will refer to the latter as the space of \emph{admissible maps}. Note that this model, while simple, encompasses many popular architectures (fully connected layers with and without bias, residual layers, convolutional layers, recurrent networks and also attention layers). We explain this in detail in Appendix \eqref{app:archs}. 

Two simple examples are fully connected layers without bias, and convolutional layers. In the former case, $\calL=\calH$, and in the latter, $\calL = \bigoplus_{i \in [L]} \mathrm{C}(X_i,X_{i+1})$, where $C(X_i,X_{i+1})$ denotes the linear subspace of convolutional operators between $X_i$ and $X_{i+1}$. While simple, these examples encapsulate many aspects of the framework, and serve well as a guide for our development. 



\subsection{Representation theory and equivariance} Throughout the paper, we aim to make the neural network \emph{equivariant} towards a group of symmetry transformations of the space $X \times Y$. That is, we consider a group $G$ acting on the vector spaces $X$ and $Y$ through \emph{representations} $\rho_X$ and $\rho_Y$, respectively. A representation $\rho$ of a group $G$ on a vector space $V$ is a map from the group $G$ to the group of invertible linear maps $\mathrm{GL}(V)$ on $V$ that respects the group operation, i.e. $\rho(gh)=\rho(g)\rho(h)$ for all $g,h \in G$. The representation $\rho$ is unitary if $\rho(g)$ is unitary for all $g \in G$. 

Given representations $\rho_U$ and $\rho_V$ on vector spaces $U$ and $V$ respectively, we may naturally define a lifted representation on $\Homm(U,V)$ as follows:
\begin{align} \label{eq:liftrep}
    \overline{\rho}(g)M = \rho_V(g)M \rho_U(g)^{-1}, \ M \in \Homm(U,V)
\end{align}
It is easy to see that if $\rho_U$ and $\rho_V$ are unitary, so is $\overline{\rho}$.



In this paper, we are concerned with training \emph{equivariant} models. A function $f: X\to Y$ is called equivariant with respect to $G$ if $f\circ \rho_X(g)  = \rho_Y(g) \circ f$ for all $g\in G$ -- that is, applying $f$ to a transformed example $\rho(g)x$ yields the same result as first applying $f$ and then transforming it with $\rho_Y(g)$. 

 For future reference, let us define the space of linear equivariant maps between $U$ and $V$ as $\Homm_G(U,V)$. Note that the lifted representation $\overline{\rho}$ is connected to equivariance of linear maps: We have $\overline{\rho}(g)M=M$ for all $g\in G$ if and only if $M\in \Homm_G(U,V)$.
 
We recall some important examples of representations.
\begin{example}
    A simple, but important, representation is the trivial one, $\rho^{\mathrm{triv}}(g)=\id$ for all $g\in G$. If we equip $Y$ with the trivial representation, the equivariant functions $f:X\to Y$ are the invariant ones.
\end{example}

\begin{example}
 \label{ex:trans}
    $\Z_N^2$ acts through translations on images $x \in \R^{N,N}$: $(\rho^{\mathrm{trans}}(k,\ell)x)[i,j]= x[i-k,j-\ell]$.
\end{example}

\begin{example}
    $\Z_4$ acts via discrete rotations of $\pi/2$ on images $x\in \R^{N,N}$: If  $\omega : [N]^2\to [N]^2$ describes the rotation of $\pi/2$ counter clockwise in pixel space, the representation is given by $(\rho^{\mathrm{rot}}(k)x)[\ell] = x[\omega^k\ell]$, $\ell \in [N]^2$.
\end{example}


\begin{figure}
    \centering
    \includegraphics[width=.95\textwidth]{figures/sketch.png}
    \caption{A graphical summary of our framework. The difference between the nominal network and the augmented one lies in the data, the difference between the nominal network and the equivariant one lies in restricting the layers.}
    \label{fig:enter-label}
\end{figure}




%Further examples %related to the experiments, and the popular architectures mentioned abo in subsequent sections 
%are provided in conjunction with an experiment in Appendix~\ref{app:ex_rep}. 


\subsection{Two strategies for obtaining equivariant models}
Let us now consider the task of training the model $\Phi_A$ to fit an equivariant target, i.e. an equivariant function $f:X \to Y$ for which the training data and labels fulfill  $y = f(x)$. While possible, it is far from clear that a simple risk minimization
\begin{align}
    \min_{A\in \calL} R(A) = \min_{A \in \calL} \erw_{\calD}(\ell(\Phi_A(x),y)), \label{eq:nomrisk}
\end{align}
where $\ell: Y \times Y \to \R$ is some loss-function, yields an $A\in\calL$ so that $\Phi_A$ is an equivariant function. That is, a minimization of the nominal risk \eqref{eq:nomrisk} does not take advantage of the inductive bias of the equivariance of the ground truth $f$. We will in this paper analyze two strategies for doing so.


\paragraph{Strategy 1: Manifest equivariance} The first method of enforcing equivariance is to constrain the layers to be manifestly equivariant. That is, we assume that $G$ is acting also on all hidden spaces $X_i$ through representations $\rho_i$, where $\rho_0 = \rho_X$ and $\rho_L=\rho_Y$, and constrain each layer $A_i$ to be equivariant: i.e., we choose $A_i$ to lie in the space $\Homm_G(X_i,X_{i+1})$ of equivariant maps. Defining $\calH_G = \bigoplus_{i \in [L]} \Homm_G(X_i,X_{i+1})$, we hence constrain the $A\in \calL$ to the  \emph{equivariant subspace}
\begin{align}%*}
    \calE = \calL \cap \calH_G.
\end{align}%*}
If we in addition assume that all non-linearities $\sigma_i$ are equivariant, it is straight-forward to show that $\Phi_A$ is \emph{exactly} equivariant under $G$ (see also Lemma \ref{lem:ind_rep_network}). We will refer to these models as \emph{equivariant}. A convenient way to formulate this strategy is to choose an $A$ which solves the following optimization problem
\begin{align}
    \min_{A\in \calE} R(A). \label{eq:eq_risk} % \min_{A\in \calL} R(\calP_\calE A). 
\end{align}
The set $\calE$, or rather $\calH_G$, has been extensively studied in the setting of geometric deep learning and explicitly characterized in many important cases \citep{ maron2018invariant, cohen2018generalGCNN, kondorGeneralizationEquivarianceConvolution2018, weiler2019general, maron2019universality,  aronsson2022homogenous}. In \citet{finzi2021practical}, a general method for determining $\mathcal{H}_G$ numerically directly from the $\rho_i$ and the structure of the group $G$ is described.

%Here,  $\calP_{\calE}$ is the metric projection onto $\calL$, i.e., the map  that maps $A \in \calL$ to the closest point to $A$ in $\calL$ with respect to the Hilbert Schmidt norm $\norm{A}^2 = \sum_{i\in [L]}\tr(A_i^*A_i)$. We will refer to $R(\calP_\calE A)$ as the \emph{equivariant risk} $R^{\eqv}(A)$.



%Let us introduce some more notation $\Hom_G(X_i,X_{i+1})$ is, as the intersection of the affine space $\Hom(X_{i},X_{i+1})$ with the $\Homm_G(X_i,X_{i+1})$ either empty, or an affine space. We will assume that we are always in the latter case. Then, we can write $\Hom_G(X_i,X_{i+1})$ as $L_0 +\THom_G(X_i,X_{i+1})$, where $L_0 \in \Hom(X_i,X_{i+1}) \cap \Homm_G(X_i,X_{i+1})$, and $\THom(X_i,X_{i+1})$ is a linear subspace of $\Homm_G(X_i,X_{i+1})$. The notation is explained by the fact that when viewed as a manifold, $\THom_G(X_i,X_{i+1})$ is the tangent space of $\Hom_G(X_i,X_{i+1})$. Let us also define
%\begin{align*}
%    \rmT\calE = \bigoplus_{i \in [L]} \THom_G(X_i,X_{i+1})
%\end{align*}
%We now define two projections
%\begin{align*}
%    \calP_{\calE} : \calL \to \calL, A \mapsto \argmin_{E \in \mathcal{E}} \norm{A-E}, \quad
%    \Pi_\calE : \calL \to \calL, V \mapsto \argmin_{W \in \rmT\calE} \norm{V-W},
%\end{align*}
%where the norm is given by the Frobenius/Hilbert-Schmidt-norm: $\norm{A}^2=\tr(A^*A)$. It is clear that $\Pi_E$ is nothing but the orthogonal projection onto $\rmT\calE$, and that $\calP_\calE(L_0 + W) = L_0 + \Pi_\calE W$ when $W\in L$ is orthogonal to $L_0$.

\paragraph{Strategy 2: Data augmentation} The second method we consider is to augment the training data. To this end, we define a new distribution on $X\times Y$ by drawing samples $(x,y)$ from $\calD$ and subsequently \emph{augmenting} them by applying the action of a randomly drawn group element $g \in G$ on both data $x$ and label $y$. Training on this augmented distribution can be formulated as optimizing the \emph{augmented risk}
\begin{align}
    R^\aug(A) = \int_G\erw_{\calD}(\ell(\Phi_A(\rho_X(g)x),\rho_Y(g)y)) \, \dd \mu(g) \label{eq:augmented_risk}
\end{align}
Here, $\mu$ is the normalised \emph{Haar} measure on the group, which is defined through its invariance with respect to the action of $G$ on itself; if $h$ is distributed according to $\mu$ then so is $gh$ for all $g\in G$. This property of the Haar measure will be crucial in our analysis. Choosing another measure would cause the augmentation to be biased towards certain group elements, and is not considered here. The normalised Haar measure exists since we have assumed that $G$ is compact~\citep{krantz2008geometric}. Note that if the data $\calD$ already is symmetric in the sense that $(x,y) \sim (\rho_X(g)x, \rho_Y(g)y)$, the augmentation acts trivially.

\begin{remark} 
    Equation \eqref{eq:augmented_risk} is a simplification -- in practice, the actual function that is optimized is an empirical approximation of $R^\aug$ formed by sampling of the group $G$. Our results are hence about an 'infinite-augmentation limit' that still should have high relevance at least in the 'high-augmentation region' due to the law of large numbers. To properly analyse this transition carefully is important, but beyond the scope of this work.
\end{remark}

\begin{table}[]
    \centering
    \begin{minipage}{0.74\textwidth}
    \begin{tabular}{l|| l | l | l}
      & Def. & Clarification & Projection \\
         \hline $\calH$ &  $\bigoplus_{i \in [L]} \Homm(X_i,X_{i+1})$ &  All possible layers & -- \\
         $\calL$ & -- %$\bigoplus_{i \in [L]} \Hom(X_i,X_{i+1})$ 
         & Admissible layers &  $\Pi_\calL$ \\
         $\calH_G$ & $\bigoplus_{i \in [L]}$ $\Homm_G(X_i,X_{i+1})$ & All equivariant layers & $\Pi_G$ \\
         $\calE$ & $\calL \cap \calH_G$ & Admissible equiv. layers & $\Pi_\calE$
    \end{tabular}
    \end{minipage}
    \begin{minipage}{0.25\textwidth}
    \includegraphics[width=1\textwidth]{figures/planes.png}
    \end{minipage}
    \caption{Four important spaces. Note that the orthogonal projections technically are onto the tangent spaces $\rmT\calL$ and $\rmT\calE$.}
    \label{tab:my_label}
\end{table}



\section{The dynamics of gradient flow near $\calE$}


As has been outlined in the previous sections, the three training strategies can be formulated as three differing optimization problems:
\begin{align*}
    \nominal: \ \min_{A\in \calL} R(A) \qquad \augm: \ \min_{A \in \calL} R^{\aug}(A) \qquad \eqvi: \ \min_{A \in \calE} R(A).
\end{align*}
%we optimize the \emph{nominal risk}
%\begin{align}%*}
%    R(A) = \erw_{\calD}(\ell(\Phi_A(x),y)),
%\end{align}%*}
%where $\ell: Y \times Y \to \R$ is a loss function, using gradient descent, over the space $\calL$. In fact, to simplify the analysis, 
We will study  \emph{parametrized gradient flows} corresponding to these optimization problems. Concretely, let us write $\calL = A_0 + \rmT\calL$ and $\calE = A_0 + \rmT \calE$ for some common 'base-point' $A_0\in \calE$ and respective tangent spaces $\rmT\calL$ and $\rmT\calE$. Letting $L: \R^{\mathrm{dim}\, \calL}\to\calH$ and $E:\R^{\mathrm{dim}\, \calE} \to \calH$ be embedding operators with $\im(L) =\rmT\calL$ and $\im(E) =\rmT\calE$, we solve the optimization problems via applying gradient flow to the functions
\begin{align} \label{eq:redlosses}
    \rR^\nom(c)\vcentcolon= R(A_0 + Lc), \quad \rR^\aug(c)\vcentcolon= R^\aug(A_0+Lc), \quad \rR^\eqv(c)\vcentcolon= R(A_0+Ec),
\end{align}
respectively. This strategy is by far the most common in practice, and entails evolving the coefficient vectors according to $\dot{c} = - \nabla \rR^\nom(c)$, and so forth. By applying the chain rule, we furthermore see that this causes the following evolutions of the $A$:
\begin{align} \label{eq:flows}
    \nominal: \dot{A} =  -LL^*\nabla R(A), \quad \augm: \dot{A}= -LL^*\nabla R^\aug(A), \quad \eqvi: \dot{A} = -EE^*\nabla R(A).
\end{align}
To go a bit more into detail, consider for instance the $\nominal$ dynamics. First, we have $\dot{A}=L\dot{c}$ and $\dot{c} = - \nabla  \rR^\nom(c)$. The chain rule furthermore implies $\nabla  \rR^\nom(c) = L^*\nabla R(A_0+Lc) = L^*\nabla R(A)$, which yields the formula.


%and furthermore has a simple geometric interpretation.
%\begin{lemma} \label{lem:umdeutung}
%    Assume that $L$ and $E$ are partially unitary, i.e $L^*L=\id$, $E^*E=\id$. Then, if the coefficient vector $c$ follows gradient flow of the parametrized losses \eqref{eq:redlosses}, the layers $A=A_0 + Lc$ and so forth will follow the dynamics
%    \begin{align*}
  %      \dot{A} = - \Pi_\calL \nabla R(A), \quad \dot{A} = - \Pi_\calL \nabla R^\aug(A), \quad \dot{A} = - \Pi_\calE \nabla R(A),
  %  \end{align*}
 %   where $\Pi_\calL$ and $\Pi_\calE$ are the orthogonal projections onto $\calL$ and $\calE$, respectively.
%\end{lemma}
%\begin{proof}
%    The chain rule implies that $\nabla \rR^\nom(c) = L^*\nabla R(A_0+Lc)$, so that $\dot{A}=L\dot{c} = -LL^*\nabla R(A)= - \Pi_\calL \nabla R(A)$, where we use the partial unitarity assumption in the final step. The argumentation for the other flows is identical.
%\end{proof}
\begin{remark}\label{rem:embedding}
   A special case  is the embedding operators $L$, or $E$, being 'partially unitary' (in the sense that $L^*L=\id$, or $E^*E=\id$). In this case, $LL^*$ equals the orthogonal projection $\Pi_\calL$ onto $\rmT\calL$, and likewise, $EE^*$ equals the orthogonal projection $\Pi_\calE$ onto $\rmT\calE$. Consequently, $A$ evolves according to projected gradient flows in this case.
\end{remark}

To conduct our analysis, we need three global assumptions.

\begin{customass}{1}
    The group $G$ is compact, and is acting on all hidden spaces $X_i$ through unitary representations $\rho_i$. The spaces $\Homm_G(X_i,X_{i+1})$ are all non-empty. \label{ass:unitary_reps}
\end{customass}
\begin{customass}{2}
The non-linearities $\sigma_i : X_{i+1} \to X_{i+1}$ are equivariant. \label{ass:equiv_nonlin}
\end{customass}
\begin{customass}{3}
    The loss $\ell$ is invariant, i.e. $\ell(\rho_Y(g)y,\rho_Y(g)y') = \ell(y,y')$, $y,y' \in Y$, $g\in G$. \label{ass:equiv_loss}
\end{customass}

Let us briefly comment on these assumptions. First, the compactness assumption is needed to ensure the existence of the normalised Haar measure. While this includes all finite groups, and the orthogonal groups $\SO(n)$ and $\mathrm{O}(n)$, it should be noted that it excludes for example the group of all rigid motions $\SE(n)$. The non-emptyness assumption is needed for the restriction strategy to be well defined. The assumption of unitarity, i.e., that  $\rho_i(g)$ preserves the inner product $\sprod{\rho_i(g)x'_i,\rho_i(g)x_i} = \sprod{x'_i,x_i}$ for all $g \in G$ and $x_i,x'_i \in X_i$, is not a true restriction:  As long as all $X_i$ are finite-dimensional, we can (since $G$ is compact) redefine the inner products on $X_i$ to ensure that all $\rho_i$ become unitary. The second assumption is required for the equivariant strategy to be sound -- if the $\sigma_i$ are not equivariant, they will explicitly break equivariance of $\Phi_A$ even if $A \in \calE$. The third assumption guarantees that the loss-landscape is 'unbiased' towards group transformations, which is certainly required to train any model respecting the symmetry group. %\textcolor{red}{can we motivate this better?}

We also note that all assumptions are in many settings quite weak. We already commented on Assumption \ref{ass:unitary_reps}. As for Assumption \ref{ass:equiv_nonlin}, note that any non-linearity acting pixel-wise on an image will be equivariant to any representation acting by moving around the pixels of the image, for instance translations and rotations. In the same way, any loss comparing images pixel by pixel will satisfy Assumption \ref{ass:equiv_loss}. This is not to say that there are not cases where point-wise non-linearities fail to satisfy the equivariance assumptions. For example, when dealing with point-cloud data, or steerable networks incorporating transformations under general representations of $G$, not any 'point-wise' non-linearity is equivariant, and more care needs to be taken. We refer to, e.g.~\citep{gerken22a,deng2021vector}, for a more in-depth discussion and examples of constructions of equivariant non-linearities in these cases. Let us finally point out that if we are trying to learn an invariant function the final representation $\rho_Y$ is trivial and Assumption \ref{ass:equiv_loss} is trivially satisfied. We discuss more cases in the Appendix \ref{app:archs}.



% Define gradient flow for the three models
% Linearized dynamics

\subsection{The compatibility condition}
We now come to the analysis of the stationary points, lying in $\calE$, of the three dynamics in Equation \ref{eq:flows}. Our main result will be that a geometric condition on the relation of $\calL$ and $\calE$ will imply that the stationary points of $\augm$ and $\eqvi$ on the subspace $\calE$ are exactly the same. Let us formulate the condition.
\begin{definition} \label{ass:compability} We say that the \emph{compatibility  condition} is satisfied for a space of admissible maps $\calL$ if $\Pi_\calL$ commutes with the orthogonal projection $\Pi_G$ onto $\calH_G$.
\end{definition}
The compatibility condition is equivalent to the following arguably more useful statement.
\begin{lemma}\label{lem:commutativity}
    The compatibility condition is equivalent to $\Pi_\calL\Pi_G = \Pi_\calE$.   
\end{lemma}
The simple proof is given in Appendix \ref{sec:proofs}. A common case when the compatibility condition holds is when $\calL$ is invariant to the lifted representation \eqref{eq:liftrep} of $G$  on $\calH$ (defined layerwise $(\overline{\rho}(g)A)_i =\overline{\rho}(g)A_i$). 
\begin{proposition} \label{prop:invcomp}
    If $\calL$ is invariant under all  $\overline{\rho}(g)$, $g\in G$, the compatibility condition \ref{ass:compability} is satisfied.
\end{proposition}


To prove both Proposition \ref{prop:invcomp} and the main result, we will need the following well-known relation, which can be found in e.g. \citet[Prop. 2.8]{fultonharris}, sometimes referred to as the \emph{twirling formula}. For the convenience of the reader, we provide a proof in Appendix \ref{sec:proofs}.

\begin{lemma} \label{lem:ind_rep_orth_proj} \emph{(Twirling formula)} Letting $\mu$ denote the Haar measure of the group $G$, we have
\begin{align*} 
    \Pi_{G}A = \int_G \overline{\rho}(g)A \, \dd \mu(g).
\end{align*}
\end{lemma}
    
Let us now prove Proposition \ref{prop:invcomp}.
\begin{proof}[Proof of Proposition \ref{prop:invcomp}]
    Suppose that $\overline{\rho}(g)$ leaves $\calL$ invariant. Then it also leaves $\rmT\calL$ invariant. Thus, for any $A\in \rmT\calL$ and $B\in \rmT\calL^\perp$ we have
    \begin{align*}
    \sprod{A,\overline{\rho}(g)B} = \sprod{\overline{\rho}(g)^{-1}A,B}=\sprod{\overline{\rho}(g^{-1})A,B} = 0
    \end{align*}
    by unitarity of representations. That is to say, each $\overline{\rho}(g)$ leaves the orthogonal complement $\rmT\calL^\perp$ invariant, so that each $\overline{\rho}(g)$ commutes with $\Pi_\calL$. By Lemma \ref{lem:ind_rep_orth_proj}, $\Pi_G$ hence commutes with $\Pi_\calL$.
    %Now, $\Pi_G$ commuting with $\Pi_\calL$ shows that $\Pi_\calL\Pi_G=\Pi_G\Pi_\calL$ is an orthogonal projection. Its range is furthermore contained in the range of both projections, i.e. in $\calL \cap \calH_G = \calE$. For $A\in \calE$, it is furthermore clear that $\Pi_\calL\Pi_G A = \Pi_\calL A = A$, so that $\Pi_\calL\Pi_G=\Pi_\calE$. 
\end{proof}
For most canonical examples, $\calL$ is indeed invariant under $\overline{\rho}$. For instance, for the fully connected architectures without bias terms, $\calL=\calH$, so the invariance holds trivially. In the A
ppendix \ref{app:comp}, we show that $\calL$ is invariant under $\overline{\rho}$ for many other architectures and group actions as well. We also show that Proposition \ref{prop:invcomp} does not give a necessary condition for the compatibility condition to hold. Let us here limit the discussion to one interesting case.

\begin{example}\label{ex:skewvscross}
 Consider $U=V=\R^{N,N}$, the discrete rotation action $\rho^\rot$ of $\Z_4$ on it (let us write $\rho$ instead of $\rho^\rot$ to simplify notation), and the set of convolutional operators 
 \begin{align*}
     \calC = \bigg\{ C_\varphi \in \Homm(U,V) \, \bigg\vert \, (C_\varphi x)[\ell] = \sum_{j \in [N]^2} x[\ell-j]\varphi[j], \, \ell \in [N]^2 \text{ for a } \varphi: [N]^2\to \R \bigg\}
 \end{align*}
 as the nominal architecture. The lifted $\overline{\rho}$ is acting directly on the filter: $\overline{\rho}(k)C_\varphi = C_{\rho(k)\varphi}$. To show that, we need to show that $\rho(k) C_\varphi \rho(k)^{-1}x = C_{\rho(k)\varphi} x$ for all $x\in U, k \in \Z_4$. We have
 \begin{align*}
     (C_{\rho(k)\varphi} x)[\ell] &= \sum_{j \in [N]^2} x[\ell-j](\rho(k)\varphi)[j] = \sum_{j \in [N]^2} x[\ell-j]\varphi[\omega^k j] = \sum_{j \in [N]^2} x[\ell-\omega^{-k}j]\varphi[j] \\
     &= \sum_{j \in [N]^2} (\rho(k)^{-1}x)[\omega^k\ell-j]\varphi[j] =(C_\varphi \rho(k)^{-1}x) [\omega^k \ell] = (\rho(k)C_\varphi \rho(k)^{-1}x) [\ell].
 \end{align*}
 This immediately shows that $\overline{\rho}(k)\calC \sse \calC$ for all $k$. However, in practice, one might also want to restrict the filters, e.g. by restricting its support to a set $\Omega \subseteq [N]^2$, i.e. to 
 \begin{align*}
     \calL_\Omega =\{ C_\varphi\in \calC \, \vert \, \supp \varphi \sse \Omega \}
 \end{align*}
 A common choice is to restrict the filters to only be $3\times 3$ or $5\times 5$ pixels. For odd $N$ this corresponds to letting $\Omega$ equal a small square in the center of the image (note that $N$ can always be made odd through zero-padding). For such rotation symmetric $\Omega$, the space $\calL_\Omega$ again becomes invariant under all $\overline{\rho}(k)$.

 We could however consider other choices for $\Omega$, such as in Figure \ref{fig:supports}. For a pattern like the left, cross-shaped one, $\overline{\rho}(k)\calL_\Omega \sse \calL_\Omega$ holds, since rotating a cross-shaped filter yields another cross-shaped filter. Thus, for convolution operators given by such a filter, the compatibility condition holds by Lemma \ref{prop:invcomp}. For non-symmetric $\Omega$ such as the right one, we do not have $\overline{\rho}(k)\calL_\Omega \sse \calL_\Omega$. In fact, one can even show that the compatibility condition \ref{ass:compability} is not satisfied for $\calL_\Omega$ in that case. We postpone these somewhat involved calculations to Appendix \ref{app:comp} -- and return to the example in the numerical experiments in Section \ref{sec:numerics}.

\begin{figure}
        \centering
        \begin{tikzpicture}[scale=0.5]
            \draw[step=1cm] (0,0) grid (3,3);
            \draw[fill=gray!50] (1,1) rectangle (2,2);
            \draw[fill=gray!50] (0,1) rectangle (1,2);
            \draw[fill=gray!50] (2,1) rectangle (3,2);
            \draw[fill=gray!50] (1,0) rectangle (2,1);
            \draw[fill=gray!50] (1,2) rectangle (2,3);
            \draw[step=1cm] (5,0) grid (8,3);
            \draw[fill=gray!50] (5,2) rectangle (6,3);
            \draw[fill=gray!50] (5,1) rectangle (6,2);
            \draw[fill=gray!50] (7,1) rectangle (8,2);
            \draw[fill=gray!50] (6,0) rectangle (7,1);
            \draw[fill=gray!50] (6,2) rectangle (7,3);
        \end{tikzpicture}
        
        \caption{On the left: filter with cross-shaped support. On the right: filter with skew support. Grey indices correspond to non-zero indices. White indices correspond to zeroed-out indices.}
        \label{fig:supports}
    \end{figure}
 
 %Consider a set $\Omega$ of indices, and let $\Hom(U,V) = \{C_\varphi : U\to U , x\mapsto \varphi * x \, \vert \, \supp \varphi \sse \Omega\}$ be the set of convolutional operators with filters supported on $\Omega$. Note that such a restriction is often made in practice, e.g. to make the filters localized.

   % It is not hard to show that $\overline{\rho}^\rot$ acts directly on the filter: $\overline{\rho}^\rot(k)C_\varphi = C_{\rho^\rot(k)\varphi}$. This means that if $\Omega$ is not symmetric in the sense that $\rho^\rot(k)\Omega \sse \Omega$, for all $k$, $\overline{\rho}^\rot(k)$ may transform an operator in $\Hom(U,U)$ to one outside it.
\end{example}


 


\subsection{Stationary points}
We can now formulate and prove the main result of the paper.
\begin{theorem}\label{theo:gradients_equal}
Under the compatibility condition \ref{ass:compability}, we have
        \begin{enumerate}
            \item The sets of stationary points on $\calE$, $S^{\nom}$, $S^\eqv$, and $S^\aug$, of $\nominal$, $\eqvi$ and $\augm$, respectively, i.e.
            \begin{align*}
                S^{\nom}\vcentcolon =\{&A \in \calE \, \vert \, LL^*\nabla R(A)=0\}, \quad S^\eqv \vcentcolon=\{A \in \calE \, \vert \, EE^*\nabla R(A)=0\},\\
                &\mathrm{and} \quad S^\aug \vcentcolon = \{A \in \calE \, \vert \, LL^*\nabla R^{\aug}(A)=0\},
            \end{align*}
            satisfy $S^{\nom}\sse S^{\eqv} = S^{\aug}$.
            \item If the embedding operators $L$ and $E$ are partially unitary, then $\calE$ is an invariant set of the gradient flow of $\augm{}$.
        \end{enumerate}
        %1. Consider the sets of stationary points $S^\eqv$ and $S^\aug$  of $\eqvi$ and $\augm$ on $\calE$, respectively, i.e.
       % \begin{align*}
            %S^\eqv \vcentcolon=\{A \in \calE \, \vert \, A \text{ is stationary for $\eqvi$}\}, \quad S^\aug \vcentcolon= \{A \in \calE \, \vert \, A \text{ is stationary for $\augm$}\}.
        %\end{align*}
        %Under the compatibility assumption \ref{ass:compability}, these are equal: $S^\eqv = S^\aug$. Also, any stationary point of $\nominal{}$ in $\calE$ is a stationary point of $\eqvi{}$ and $\augm{}$, i.e.
        %\begin{align*}
        %    S^{\mathrm{nom}}\vcentcolon=\{A \in \calE \, \vert \, A \text{ is stationary for $\nominal$}\}  \sse S^\aug = S^\eqv.
       % \end{align*}
        %2. If in addition to the compatibility condition the embedding operators $L$ and $E$ are partially unitary, $\calE$ is an invariant set of the gradient flow of $\augm{}$. 
\end{theorem}

\begin{remark}
    \begin{enumerate}
        \item Note that Theorem \ref{theo:gradients_equal} only applies to \emph{points in $\calE$}. It does not say anything about stationarity of points that are not in $\calE$.
        \item Although we have formulated our theory for gradient flow, Theorem \ref{theo:gradients_equal} directly extends to (non-stochastic) gradient descent -- a point is stationary for the gradient flow if and only if it is stationary for gradient descent. Since $\calE$ is a vector space (and not a more general manifold), the same argument goes through for the second part. The extension to SGD is more complicated, and deemed outside the scope of this work.
    \end{enumerate}
\end{remark}

%\begin{theorem}Under the invariance assumption \ref{ass:invariance}, the following holds:\ref{theo:gradients_equal}
 %   \begin{enumerate}[(a)]
  %      \item $\calE$ is an invariant set under the gradient flow of \augm{}. Hence, if initialized on $\calE$, that gradient flow will stay there.
   %     \item The set of stationary points $S^{\eqv}$ of \eqvi{} is equal to the set of stationary points $S^{\aug}$ of \augm{} in $\calE$. 
    %    \item Any stationary point of \nominal{} in $\calE$ is necessarily a stationary point of \eqvi{} and \augm{}.
    %\end{enumerate}
%\end{theorem}


The key to prove Theorem \ref{theo:gradients_equal} is the following auxiliary result
\begin{lemma}
\label{lem:ind_rep_network}
    %Under Assumptions \ref{ass:equiv_nonlin} and \ref{ass:equiv_loss}, 
    The augmented risk can be expressed as
    \begin{align} \label{eq:augrisk}
        R^\aug(A) = \int_{G}R(\overline{\rho}(g)A) \, \dd \mu(g).
    \end{align}
    Consequently for $A\in \calE$
    \begin{align*}
        \nabla R^\aug(A) = \Pi_G \nabla R(A).
    \end{align*}
\end{lemma}

\begin{proof}
It is enough to show that for $A \in \calL$ and $g \in G$, we have
    \begin{align}%*} 
        \Phi_A(\rho_X(g) x) = \rho_Y(g)\Phi_{\overline{\rho}(g)^{-1}A}(x). \label{eq:layer_transform}
    \end{align}%*}
    Once that has been proven, the statement follows from Assumption \ref{ass:equiv_loss}:
    \begin{align*}
        R^\aug(A) &= \int_G \erw_\calD(\ell(\Phi_A(\rho_X(g)x),\rho_Y(g)y)) \dd \mu \stackrel{\eqref{eq:layer_transform}}{=}  \int_G \erw_\calD(\ell(\rho_Y(g)\Phi_{\overline{\rho}(g)^{-1}A}(x),\rho_Y(g)y)) \dd \mu  \\
        & \stackrel{\text{Ass. \ref{ass:equiv_loss}}}{=}  \int_G \erw_\calD(\ell(\Phi_{\overline{\rho}(g)^{-1}A}(x),y)) \dd \mu =   \int_{G}R(\overline{\rho}(g)^{-1}A) \, \dd \mu(g) = \int_{G}R(\overline{\rho}(g)A) \, \dd \mu(g), \nonumber
    \end{align*}
    where the final step is a property of the Haar measure: if $g\sim \mu$, then $g^{-1}\sim \mu$.

    We proceed with the proof of \eqref{eq:layer_transform}. Using the notation from \eqref{def:MLP}:  $x_i$ denotes  the output of  layer $i$ of the network $\Phi_A$ when it acts on the input $x \in X$. Also, for $g \in G$, let $x_i^g$ denote the outputs of each layer of the network $\Phi_{\overline{\rho}(g)^{-1}A}$ when acting on the input $\rho_X(g)^{-1}x$. If we can show that 
    \begin{align}\rho_i(g) x_i^g = x_i, \quad i\in[L+1], \label{eq:ind}\end{align}
    then the claim follows: For $i=L$, it reads $\rho_L(g)x_L^g = x_L$, which actually means $\rho_Y(g) \Phi_{\overline{\rho}(g)^{-1}A}(\rho_X(g)^{-1}x)= \Phi_A(x)$, which clearly is equivalent to \eqref{eq:layer_transform}.
    
    We show \eqref{eq:ind} via induction. The case $i=0$ is clear:
        $\rho_{X}(g) x_0^g = \rho_X(g) \rho_X(g)^{-1}x = x =x_0$. As for the induction step, we have
    \begin{align*}%*}
        \rho_{i+1}(g) x_{i+1}^g &= \rho_{i+1}(g) \sigma_i\big((\overline{\rho}(g)^{-1}A_i) x_{i}^g\big) \stackrel{\text{Def. }\overline{\rho}}{=} \rho_{i+1}(g) \sigma_i(\rho_{i+1}(g)^{-1}A_i \rho_i(g) x_{i}^g) \\& \stackrel{\text{Ass. \ref{ass:equiv_nonlin}}}{=} \sigma_i(\rho_{i+1}(g) \rho_{i+1}(g)^{-1}A_i\rho_i(g) x_{i}^g)  = \sigma_i(A_i\rho_i(g) x_i^g) \stackrel{\text{Ind. ass.}}{=} \sigma_i(A_i x_{i}) = x_{i+1} \,, \nonumber
    \end{align*}%*}
    and the claim follows.


    Now differentiate \eqref{eq:augrisk} with respect to $A$ to yield the the equality
\begin{align*}
    \nabla R^{\aug}(A) = \int_{G} \overline{\rho}(g)^*\nabla R(\overline{\rho}(g)A) \dd \mu(g).
\end{align*}
If $A\in \calE$, $\overline{\rho}(g)A=A$ for all $g$. This, and the twirling formula (Lemma \ref{lem:ind_rep_orth_proj}) implies, by unitarity of $\overline{\rho}$ and a property of the Haar measure, that the above equals $\Pi_G\nabla R(A)$, which was the claim.
\end{proof}

Now we have all the tools necessary to prove the main result.
%\begin{proof}[Proof of Theorem \ref{theo:gradients_equal}]
%    We only treat the case of partially unitary embedding operators here, the other case is treated in the appendix. Lemma \ref{lem:ind_rep_network} namely shows that under the compatibility condition, we have 
 %  \begin{align*}
  %      \Pi_\calL \nabla R^\aug(A) = \Pi_\calL \Pi_G \nabla R(A) = \Pi_\calE \nabla R(A).
   % \end{align*}
   % Now, Lemma \ref{lem:umdeutung} shows that in the case of partially unitary embedding operators, the above vector fields determine the gradient flows of $\augm{}$ and $\eqvi{}$ on $\calE$, respectively. Their equality immediately shows the invariance under $\calE$ of $\augm{}$, and the equality of the stationary points of $\eqvi{}$ and $\augm{}$. As for the equality to the stationary points of $\calL$, notice that $\rmT \calE \sse \rmT \calL$ by definition. A stationary point of $\nominal{}$ is characterized by $\Pi_\calL \nabla R(A)=0$, i.e $\nabla R(A) \in \rmT\calL^\perp \sse \rmT\calE^\perp$, which means that $\Pi_\calE \nabla R(A)=0$, i.e. that $A$ is a stationary point of $\eqvi{}$, and therefore also for $\augm{}$, by the previous point.
%\end{proof}


\begin{proof}[Proof of Theorem \ref{theo:gradients_equal}] \underline{Part 1:} Equation \eqref{eq:flows} together with Lemma \ref{lem:ind_rep_network} tells us that stationary points of $\nominal{}$, $\augm{}$ and $\eqvi{}$ on $\calE$, respectively, are characterized by
\begin{align*}
    LL^*\nabla R(A) = 0, \qquad LL^*\nabla R^\aug(A)= LL^*\Pi_G \nabla R(A)=0 \qquad EE^*\nabla R(A) =0
\end{align*}
We will show that under the compabilitity condition, (a) $S^\mathrm{nom}  \sse S^\eqv$, (b) $S^\eqv \sse S^\aug $ and (c) $ S^\aug  \sse S^\eqv$, which implies the statement.

\underline{(a)} If $LL^*\nabla R(A)=0$, then by linearity and injectivity of $L$ we have $L^*\nabla R(A)=0$. In other words, we have $\nabla R(A) \in \ker L^* = (\ran L)^\perp = \rmT \calL^\perp$. Now notice that $\rmT \calE \sse \rmT \calL$ by definition, which implies that $\rmT\calL^\perp\sse \rmT \calE^\perp$. Hence, $\nabla R(A)$ is also in $ \rmT\calE ^\perp$, implying $E^*\nabla R(A) =0$, and therefore also $EE^*\nabla R(A)=0$. Thus, $S^{\nom}\sse S^{\eqv}$.

\underline{(b)} If $EE^*\nabla R(A)=0$, then by similar arguments as in (a) we have $\Pi_\calE \nabla R(A) = 0$. By Lemma \ref{lem:commutativity}, $\Pi_\calL \Pi_G \nabla R(A) = 0$, i.e. $\Pi_G \nabla R(A) \in \rmT\calL^\perp = \ker L^*$, so that $LL^*\Pi_G \nabla R(A) =0$. Thus, $S^{\eqv} \sse S^{\aug}$.

\underline{(c)} If $LL^*\Pi_G \nabla R(A)=0$, then by again similar arguments as in (a) we have $\Pi_\calL \Pi_G \nabla R(A)=0$. By Lemma \ref{lem:commutativity}, $\Pi_\calE \nabla R(A) = 0$, i.e. $\nabla R(A) \in \rmT\calE^\perp = \ker E^*$, so that $EE^* \nabla R(A) =0$. Thus, $S^{\aug}\sse S^{\eqv}$.

\underline{Part 2:} When $L$ is partially unitary, as we saw in Remark \ref{rem:embedding}, we have $LL^*=\Pi_\calL$, and consequently $\nabla R^\aug(A) = \Pi_{\calL}\Pi_G \nabla R(A)$ for $A\in \calE$, which by Lemma \ref{lem:commutativity} equals $\Pi_\calE \nabla R(A)$ under the compatibility condition. Hence, the gradients at points in $\calE$ lie in $\rmT\calE$, which implies that $\calE$ is an invariant set of the gradient flow of \augm{}.   
\end{proof}


\newcommand{\conv}{\mathrm{conv}}



\subsection{Stability }
Let us now consider the stability of stationary points in $\calE$ for the three gradient flows. Note that when we say that a point $A \in \calE$ is stable for $\nominal{}$, what we really mean is that the $c$ so that $A= A_0+Lc$ is a stable point for the dynamics $\dot{c} = - \nabla \rR^{\mathrm{nom}}(c)$, and similarly for $\augm$ and \eqvi.

It is well known that as long as the Hessian of a function $f$ has no zero-eigenvalues, it can be used to classify the stability of a point $x_0$ under the gradient flow $\dot{x} = - \nabla f(x)$: $x_0$ is a stable point, more specifically a strict local minimum of $f$, if and only if $f''(x_0)$ is positive definite. Consequently, we will study the Hessians of $\rR^\nom$, $\rR^\aug$ and $\rR^\eqv$. We begin with the following statement about the Hessian of $R^\aug$ in points $A\in \calE$. 
\begin{lemma} \label{lem:aug_hess}
    Let $A\in \calE$. If we decompose $V\in \calH$ as $X+Y$ with $X\in \calH_G$ and $Y\in \calH_G^\perp$, we have
    \begin{align*}
        (R^\aug)''(A)[V,V] = R''(A)[X,X] + \int_{G} R''(A)[\overline{\rho}(g)Y,\overline{\rho}(g)Y] \dd \mu(g).
    \end{align*}
\end{lemma}
\begin{proof}
    Applying the chain rule to \eqref{eq:layer_transform} yields
\begin{align*}
    (R^\aug)''(A)[V,V] = \int_{G} R''(\overline{\rho}(g)A)[\overline{\rho}(g)V,\overline{\rho}(g)V] \dd \mu(g) = \int_{G} R''(A)[\overline{\rho}(g)V,\overline{\rho}(g)V] \dd \mu(g),
\end{align*}
since $\overline{\rho}(g)A=A$ for $A\in \calE$. We now expand  $R''(A)[\overline{\rho}(g)(X+Y),\overline{\rho}(g)(X+Y)]$. Since $X\in \calH_G$, $\overline{\rho}(g)X = X$, and we obtain
\begin{align*}
 &\int_{G} \Big(R''(A)[X,X] + R''(A)[X,\overline{\rho}(g)Y] + R''(A)[\overline{\rho}(g)Y,X] + R''(A)[\overline{\rho}(g)Y,\overline{\rho}(g)Y]\Big)\dd \mu(g) \\
 &\quad = R''(A)[X,X] +R''(A)[X,\Pi_G Y] + R''(A)[\Pi_G Y,X] + \int_G R''(A)[\overline{\rho}(g)Y,\overline{\rho}(g)Y]\dd \mu(g) ,
\end{align*}
where we applied the twirling formula $\int_G \overline{\rho}(g)Y \, \dd \mu(g) = \Pi_G Y$ and linearity. It is now only left to note that $\Pi_GY=0$, since $Y \in \calH_G^\perp$.
\end{proof}
We can now analyse the stability of our flows.
\begin{theorem} \label{th:stable}
    Under the compatibility condition, we have
    \begin{enumerate}
        \item If $A \in \calE$ is a strictly stable point for $\nominal$, it is also strictly stable for $\augm{}$ and $\eqvi{}$.
        \item If $A \in \calE$ is a strictly stable point for $\augm{}$, it is also strictly stable for $\eqvi{}$
    \end{enumerate}
\end{theorem}
\begin{proof} In the following, let $A$ denote a point in $\calE$, $c$ the coefficient vector so that $A= A_0+Lc$, and $e$ the one so that $A=A_0+Ee$. Note that such coefficient vectors exist due to $A\in \calE$.

    \underline{Part 1:} First, if $A\in \calE$ is a stable point for $\nominal$, it surely is stationary, and therefore by Theorem \ref{theo:gradients_equal} a stationary point also for $\augm{}$ and $\eqvi$. Also, we must have $(\rR^{\mathrm{nom}})''(c)[d,d]>0$ for all $d\neq 0$. The chain rule reveals that the latter means that $R''(A)[Ld,Ld] > 0$ for all $d\neq 0$, or equivalently $R''(A)[V,V]>0$ for all $0 \neq V\in \rmT\calL$. This in particular implies that  $$(\rR^\eqv)''(e)[f,f] = R''(A)[Ef,Ef]>0$$ for all $f\neq 0 $, since $0 \neq Ef\in \rmT\calE \sse \rmT \calL$, so that $A$ is stable also for $\eqvi$. As for $\augm{}$, we can for each $d\neq 0$ write $Ld = X+Y$ for $X\in \calH_G$ and $Y\in \calH_G^\perp$, with not both $X$ and $Y$ equal to $0$, and by Lemma \ref{lem:aug_hess} obtain
    \begin{align*}
        (\rR^\aug)''(c)[d,d]  = (R^\aug)''(A)[Ld,Ld] = R''(A)[X,X] + \int_{G} R''(A)[\overline{\rho}(g)Y,\overline{\rho}(g)Y] \dd \mu(g) >0,
    \end{align*}
    so that $A$ is stable also for $\augm$. The strict inequality follows from the fact that if $X=0$, we must have $Y\neq 0$, and therefore $\overline{\rho}(g)Y\neq 0$ for all $g\in G$.

    \underline{Part 2:} Suppose that $A\in\calE$ is a stable point for $\augm{}$. We want to show that $(\rR^\eqv)''(e)[f,f] = R''(A)[Ef,Ef]>0$ for all $f\neq 0$. $Ef$ can, as a vector in $\rmT\calE\sse \rmT\calL$ we written as $Ld$ for some $d\neq 0$. In the decomposition $Ld=X+Y$ as in Lemma \ref{lem:aug_hess}, $Y$ must be zero, since $Ld \in \rmT\calE \sse \calH_G$. Lemma \ref{lem:aug_hess} therefore implies that
    \begin{align*}
        (\rR^\aug)''(c)[d,d] = (R^\aug)''(A)[Ld,Ld] = R''(A)[Ld,Ld] = R''(A)[Ef,Ef].
    \end{align*}
    Since $A$ is stable for $\augm$, we have $R''(A)[Ef,Ef]=(\rR^\aug)''(c)[d,d]>0$, which completes the proof.
\end{proof}

Theorem \ref{th:stable} gives a more nuanced meaning to Theorem \ref{theo:gradients_equal}. Although we have an equality of stationary points $S^\eqv=S^\aug$, it may very well be that a point in $\calE$ is stable for $\eqvi$ but not for $\augm$. The contrary is however not possible. In this sense, using the $\eqvi$ flow is preferable if one searches for stationary points in $\calE$ -- they will then more often be stable. In the Appendix \ref{app:stability}, we explicitly construct examples with points that are stable for $\eqvi$ but not for $\augm$.%, and stable for $\augm$ but not for $\nominal$.

\begin{remark}
    Let us quickly comment on the extension of Theorem \ref{th:stable} to gradient descent and SGD. As for gradient descent, we note that a strictly stable point for the gradient flow is also stable for gradient descent \emph{if the learning rate is small enough}. Any quantitative statement would require assumptions on the Hessian of $R$ at $A$ (e.g. lower bounds on (restricted) eigenvalues), which would be impossible to check a priori. Again, the extension of the theorem to a stochastic setting is more involved, and will probably also depend on the 'degree of randomness' (e.g. batch sizes), and we leave it to future work. 
\end{remark}

Let us end this section by, utilizing similar ideas as above, derive an interesting statement about decoupling of the dynamics in $\calE$ and in $\rmT\calE^\perp$ which holds when $L$ and $E$ are partially unitary and the compatibility condition holds.
\begin{proposition} 
For $A\in \calL$, write $A = X+ Y$ with $X \in \calE$ and $Y \in \rmT\calE^{\perp}$. Assuming that the compatibility condition holds, and assuming that the embedding operator $L$ is partially unitary, the gradient flow of $R^\aug$ decouples in the following sense:
\begin{align*}
    \begin{cases}
        \dot{X} &= - \Pi_\calE\nabla R(X)  \quad   \quad \ \, +  \mathcal{O}(\Vert{Y}\Vert^2)\\
        \dot{Y} &= - \Pi_\calL (R^{\aug})''(X)Y   + \mathcal{O}(\Vert Y\Vert^2).
    \end{cases}
\end{align*}
In particular, when also $E$ is partially unitary, $X$ follows the $\eqvi$ dynamics up to $\mathcal{O}(\Vert Y \Vert^2)$.
\end{proposition}
\begin{proof}
    We have already argued that in this case, the dynamics are given by are given by $\dot{A} = - \Pi_\calL\nabla R^\aug(A)$. Performing a Taylor expansion of $\nabla R^\aug(A)$ in $X$ yields
    \begin{align*}
        \Pi_\calL\nabla R^\aug(A) = \Pi_\calL\nabla R^\aug(X) + \Pi_\calL (R^\aug)''(X)Y + \mathcal{O}(\vert Y \vert^2).
    \end{align*}
    Since $X \in \calE$ and the compatibility condition holds, we have $\Pi_\calL \nabla R^\aug(X) = \Pi_\calL \Pi_G \nabla R(X) = \Pi_\calE \nabla R(X)$.

    Now, to determine the dynamics of $X$, let us test the equation $\dot{A} = - \Pi_\calL\nabla R^\aug(A)$ with an arbitrary $W\in \rmT\calE$. We namely have $\sprod{\dot{A},W}=\sprod{\dot{X},W}$ for such $W$. Furthermore, $\sprod{\Pi_\calL\nabla R^\aug(X),W} = \sprod{\Pi_\calE \nabla R(X),W} = \sprod{\nabla R(X),W}$, and
    \begin{align*}
        \sprod{\Pi_\calL (R^\aug)''(X)Y,W} = \sprod{(R^\aug)''(X)Y, \Pi_\calL W} = (R^\aug)''(X)[Y, \Pi_\calL W].
    \end{align*}
    Since $W \in \rmT\calE \sse \rmT \calL$, we have $\Pi_\calL W = W$.  Now, similarly as in the proof of Lemma \ref{lem:aug_hess}, we apply the chain rule to $R^\aug$ and the fact that $X\in \calE \sse \calH_G$ to derive that 
    \begin{align*}
        (R^\aug)''(X)[Y, W] = \int_{G} R''(X)[\overline{\rho}(g)Y,\overline{\rho}(g)W] \, \dd \mu(g) = R''(X)[\Pi_G Y , W].
    \end{align*} 
    where we used that $W\in \rmT \calE \sse \calH_G$, and also the twirling formula. Now, since $A= X+Y$, $Y$ must be in $\rmT \calL$, and consequently $Y = \Pi_\calL Y$. Consequently, appealing to the compatibility condition, $\Pi_G Y = \Pi_G \Pi_\calL Y  = \Pi_\calE Y = 0 $, since $Y\in \rmT\calE^\perp$. Thus, $(R^\aug)''(X)[Y, W] =0$, and we get
    \begin{align*}
        \sprod{\dot{X},W} = - \sprod{\Pi_\calE \nabla R(X),W} + O(\Vert Y\Vert^2), \quad W\in \rmT\calE.
    \end{align*}
    To determine the dynamics for $Y$ is easier: Here, we only need to note that $\sprod{\Pi_\calE \nabla R(X), V} = 0$ for $V \in \rmT\calE^\perp$ arbitrary to arrive at
    \begin{align*}
        \sprod{\dot{Y},V} &= - \sprod{\Pi_\calL (R^{\aug})''(X)Y,V}  \ + \mathcal{O}(\Vert Y\Vert^2), \quad V\in \rmT\calE^\perp.
    \end{align*}

      Now it is only left to note that in the case of partially unitary $E$, $\dot{X} = - \Pi_\calE\nabla R(X)$ are the $\eqvi$ dynamics.
\end{proof}

%Immediate consequences
%\begin{itemize}
%    \item $\calE$ is an invariant set of the augmented dynamics.
%    \item Stationary points in $\calE$ are equal.
%    \item If strict minimum for augmented, then also for equivariant, but not vice versa.
%\end{itemize}


\section{Experiments} \label{sec:numerics}

\begin{figure}[b]
        \centering
        \begin{tikzpicture}[scale=0.5]
            \draw (0,0) rectangle (1,3);
            \node at (0.5,1.5) {$X$};
            \node at (0.5,-1) {$\R^{28\times 28}$};
            \node at (4,1) {$\tanh$, LayerNorm};
            \node at (4,2) {Conv, Pool,};
            \draw (7,0) rectangle (8,3);
            \node at (7.5,1.5) {$X_1$};
            \node at (7.5,-1) {$\R^{32\times14\times 14}$};
            \node at (11,1) {$\tanh$, LayerNorm};
            \node at (11,2) {Conv, Pool,};
            \draw (14,0) rectangle (15,3);
            \node at (14.5,1.5) {$X_2$};
            \node at (14.5,-1) {$\R^{64\times7\times 7}$};
            \node at (18,1) {$\tanh$, LayerNorm};
            \node at (18,2) {Conv,};
            \draw (21,0) rectangle (22,3);
            \node at (21.5,1.5) {$X_3$};
            \node at (21.5,-1) {$\R^{64\times7\times 7}$};
            \node at (25,1) {Fully-Connected};
            \node at (25,2) {Flatten,};\draw[thick,decorate,decoration={brace,amplitude=10pt},yshift=2pt] (-.5,3.9) -- (22.5,3.9) node [black,midway,xshift=0pt,yshift=18pt] {$\rho^{\mathrm{rot}}$};
            \draw (28,0) rectangle (29,3);
            \node at (28.5,1.5) {$Y$};
            \node at (28.5,-1) {$\R^{10}$};
            \node at (28.5,4) {$\rho^{\mathrm{triv}}$};
            \draw[->] (1.5,1.5) -- (6.5,1.5);
            \draw[->] (8.5,1.5) -- (13.5,1.5);
            \draw[->] (15.5,1.5) -- (20.5,1.5);
            \draw[->] (22.5,1.5) -- (27.5,1.5);
        \end{tikzpicture}
        
        \caption{The architecture consists of three convolutional layers with filters $\varphi$ having support as in Figure \ref{fig:supports} (left or right), followed by a flattening and then a fully-connected layer.}
        \label{fig:architecture}
    \end{figure}

We perform an experiment in order to showcase the difference between a network whose layers obey the compatibility assumption and one where they do not. Additionally, we test training the networks using stochastic gradient descent (SGD) with varying batch sizes, to investigate to which extent our theoretical results (which are only about gradient flow) also apply to them.

The code for the experiment was written in Python, using the PyTorch library. The code can be accessed at \href{https://github.com/usinedepain/eq_aug_dyn/tree/main}{github.com/usinedepain/eq\_aug\_dyn/}. %The code can be accessed at \textbf{LINK TO GITHUB GOES HERE}.
%\begin{example}
%Consider the group $\mathbb{Z}_4$ acting on the space of square images by discrete rotations of $\pi/2$ radians. Consider a CNN where the convolutional filters have support given by the cross shape in Figure \ref{fig:supports}. Let $\mathcal{L}$ be the space of permissible layers, i.e., those where the filter has cross-shaped support. The cross shaped filter fulfills the conditions of Proposition \ref{prop:invcomp}, since we can view $\mathbb{Z}_4$ as if it is acting on the filters and a rotation of a cross-shaped filter by an integer multiple of $\pi/2$ yields another cross-shaped filter. Thus, with the cross-shaped support, the layers fulfill the projection formula $\Pi_{\mathcal{L}}\Pi_{\mathbb{Z}_4} = \Pi_{\mathcal{E}}$, by Proposition \ref{prop:invcomp}.

%On the other hand, let us consider a CNN where the convolutional filters have support given by the skew shape in Figure \ref{fig:supports}. Let $\mathcal{L}$ be the space of permissible layers, i.e., those where the filter has skew-shaped support. If we let $A\in \mathcal{H}$ be a layer, then we have that $\Pi_{\mathbb{Z}_4}$ corresponds to averaging the filter corresponding to $A$ over its orbit, which yields a filter where the corners all have the same value, the sides all have the same value, and, by an appropriate choice of $A$, the value at the corners is non-zero. Applying $\Pi_{\mathcal{L}}$ to such a filter means zeroing out every corner except the top left corner and zeroing out the center. This yields a filter in $\mathcal{L}$ which is not in $\mathcal{E}$, since the corners are not all equal.
%\end{example}
\subsection{Experiment description}
We consider two nominal architectures consisting of convolutional layers with a fully connected last layer, and equivariant pooling and layer normalization layers -- see Figure \ref{fig:architecture}. The two architectures differ in the choice of support for the convolutional filters -- one is the cross support (denoted by Cross in Figure \ref{fig:distance}) and one is the skew support (denoted by Skew in Figure \ref{fig:distance}), as in Figure \ref{fig:supports}.  These were trained in $\eqvi$ mode for $250$ epochs, manifestly invariant to the rotation action of $\mathbb{Z}_4$, to classify MNIST \citep{lecun1998a}. We used SGD as the optimizer, with an MSE loss with the labels as one-hot vectors; the learning rate was set to $5\cdot 10^{-4}$ and the batch size was set to 100. For the cross shaped support, we ran two experiments, with partially unitary and non-unitary embedding operator $L$ (denoted by Non-Unitary in Figure \ref{fig:distance}), in $\augm$ mode, resulting in a total of three experiments. Basis vectors for the non-unitary embedding $L$ were drawn as cross-shaped filters with i.i.d. Gaussian entries. We trained 30 networks in $\eqvi$ mode for each configuration. %The $3\times3$ convolutional filters in two of the CNNs had support in a cross pattern and the filters in the last CNN had a skew pattern (see Figure \ref{fig:supports}), with values in all other indices set to 0. The CNNs were trained for 250 epochs. 
Starting from these in total 90 networks, we then switched mode to $\augm$ for ten more epochs of training with batch sizes 1, 25, and 100, and for 50 more epochs of training with gradient descent, with a learning rate of $2.5\cdot 10^{-4}$. The reason we lowered the learning rate for this part of the experiment was to make the SGD 'closer' to a flow. The reason we trained for 50 epochs with gradient descent compared with the 10 epochs of training with minibatch SGD is that gradient descent only updates once per pass over the entire data set. %their architectures were 'let loose' from their manifestly invariant state to allow non-equivariant layers and training was continued on augmented data for 10 more epochs with the learning rate set to $2.5\cdot 10^{-4}$. 
%The non-unitary $L$ were drawn according to a Gaussian distribution among all possible embeddings. 
The images were normalized with respect to mean and standard deviation before being sent to the first layer.
%For the CNN with cross shape supported filters we used non-orthogonal parametrization. The augmentation was done by adding to the training data three rotated versions of each image; one rotated by $\pi/2$ radians, one rotated by $\pi$ radians, and one rotated by $3\pi/2$ radians. 

%The architecture of the CNNs used in the experiment can be seen in Figure \ref{fig:architecture}. The first convolutional layer has 1 channel in and 32 channels out, the second has 32 in and 64 out, and the third has 64 in and 64 out.

The purpose of the experiment was to observe how quickly the layers of the different CNNs would drift away from $\mathcal{E}$ when using augmentation. After the $\eqvi$ mode epochs, we will likely be close to a stationary point of the equivariant flow. This point will also be stationary for the augmented flow if the compatibility condition holds. By Example \ref{ex:skewvscross}, this is the case for the cross support, but not for the skew support. Hence, we can expect the latter to drift away faster. For the two cross support architectures, we also expect to see a difference, since $\calE$ only is an invariant set when using partially unitary $L$, by Theorem \ref{theo:gradients_equal} (part 2). Hence, the non-unitary version should be more prone to drift. Furthermore, since our theory holds for gradient descent, but not necessarily for SGD which can 'jump away' from $\calE$, we also expect that we see the least amount of drift when training with gradient descent, and the most drift when training with SGD with a batch size of 1. %the CNN with cross support fulfills the compatibility assumption \ref{ass:compability} and the CNN with skew support does not. Hence, in the light of Theorem \ref{theo:gradients_equal}The architecture with partially unitary $L$ satisfies the conditions of the second part of Theorem \ref{theo:gradients_equal}, while the one without partially orthogonal $L$ does not.


\subsection{Results and analysis}
After each batch in the augmented training, the distance of the layers from  $\calE$ was recorded. In Figure \ref{fig:distance}, we plot, for each batch size, on the left, the mean distance over each full epoch, and on the right, the distance after each batch in the first epoch. In the top left sub-figure we plot the distance after each epoch of training with gradient descent. In essence, our above hypotheses are confirmed: The architecture with cross support and partially unitary $L$ stays close to $\calE$, whereas the others drift. The skew-support architecture drifts more in absolute numbers, but not too much should be read into this -- it may very well look different for other nominal architectures. Furthermore, we see that the layers of the networks trained with (non-stochastic) gradient descent stay comparatively close to $\calE$. In numbers, the distance for the cross supports after 50 batches of training with a batch size of 100 is on the order of 5e-3, which is quite close to the corresponding values for the skew supports as well as for non-unitary embedding, whereas after 50 epochs of training with gradient descent the distance for the cross supports is on the order of 1e-7, which is much smaller than the corresponding values for the skew supports as well as for non-unitary embedding. Also note that the non-unitary cross architectures have some outliers -- we believe that the cause for this is numerical problems when initializing the $\augm$ training - this entails solving a linear set of equations $Lc = A_0$, with a potentially ill-conditioned $L$. %The experiment was performed 30 timesBy the above discussion, we would expect that layers of the CNN with cross-shaped support for the filters and partially orthogonal parametrization would leave $\calE$ more slowly during the augmented training than the layers of CNN with skew-shaped support for the filters, since by Theorem \ref{theo:gradients_equal} the stationary points for $\augm{}$ are not necessarily the same as the stationary points for $\eqvi{}$. We would also expect that of the two CNNs with cross-shaped support for the filters, that the layers of the CNN with partially orthogonal parametrization would leave $\calE$ more slowly during the augmented training than the layers of the CNN with non-orthogonal parametrization, since for the former $\calE$ is invariant for $\augm{}$, but not for the latter.
\begin{figure}[h]
    \centering
    \includegraphics[width=.99\textwidth]{figures/distance_from_E.pdf}
    \caption{Top left sub-figure (gradient descent): distance in 2-norm of layers from $\calE$ per epoch of augmented training.\\
    On the left of the other sub-figures: distance in 2-norm of layers from $\calE$ averaged over the batches per epoch of augmented training. On the right of the other sub-figures: distance in 2-norm of layers from $\calE$ during the first epoch of augmented training.\\
    The fainter lines are the individual experiments and the thicker lines are the medians over all experiments.\\
    The noticeable outliers in faint yellow are most likely due to numerical errors in calculating the non-unitary embedding operator by solving a linear system. Best viewed in color.}
    \label{fig:distance}
\end{figure}

The cross shaped architecture in this case seems to be relatively stable for minibatch SGD with batch sizes 25 and 100. %-- that it initially drifts in the first epoch is probably due to the stochastic nature of $SGD$ -- %understanding this better is interesting future work. 
It should however be noted that although it appears so in the left figure of the top right and bottom left sub-figures, it does not converge to $0$, but rather to  around 1e-3. This is not a contradiction -- Theorem \ref{th:stable} indicates that $\calE$ may be unstable for $\augm$ also in the cross-shaped case. The general trend is however as we expected. The experiments indicate that as long as the batch sizes are large enough, the dynamics of stochastic gradient descent are close enough to the gradient descent for our theoretical results to still give a good picture. It is important to note that for SGD with batch size of 1, the cross support also drifts from $\calE$ quite dramatically, differing from the other cases. To investigate for which 'degrees of stochasticity' our theoretical results are still relevant is interesting future work.

In the appendix, we perform another experiment testing another architecture with different symmetry group. In essence, the results are again in accordance with the theory. They reveal connections between the symmetry group, representations and stability of $\calE$ that is subject to further theoretical work.


\section{Conclusion}
In this paper we set out to investigate the relationship between the gradient flows of networks equivariant by design and  nominal networks trained on augmented data during training. It turns out that the geometry of the spaces of admissible and equivariant layers, $\calL$ and $\calH_G$ respectively, is key. Under the \emph{compatibility condition} $\Pi_\calL\Pi_G=\Pi_G\Pi_\calL$, we showed that the stationary points in the equivariant space $\calE$ are the same for the equivariant and augmented flows, but that they do not necessarily share the set of equivariant stable points. Furthermore, we showed that $\calE$ is invariant for the augmented flow given unitary parametrizations of our layers. In fact, to first order approximation the dynamics of the augmented flow decouples in $\calE$ and $\rmT\calE^{\perp}$ in this case.

\subsection{Practical take-aways}
What practical recommendations can be extracted from our results? One lesson is that in order to promote equivariance through augmentation, one should consider the option of initializing all parameters in an equivariant way. Also, one should check that the compatibility condition holds - as we have seen, this will for most popular, reasonable architectures, already be the case.

Does our work give a definitive answer to whether one should augment the data or restrict the architecture? The short answer is no. Our results can be used to argue both for augmentation and for restriction: First, advocates of either strategy can argue that 'their' strategy have the exact same set of equivariant stationary point, and hence does not 'miss' anything the other strategy can find. 'Restrictioners' can point to their strategy making more points on $\mathcal{E}$ stable, and hence a stronger bias. The latter can however, by 'augmenters', also be argued to be indicative of the restriction strategy inducing more 'bad' local minima on $\calE$. These can be escaped, and a better minimum on $\mathcal{E}$ can later be found, via an 'expedition' in $\mathcal{H} \backslash \mathcal{E}$. That expedition is however not guaranteed to find its way back to $\mathcal{E}$. However, our results provide a better understanding of the dynamics occurring in the two strategies, which in of itself is important.

\subsection{Future work}
This work is an initial foray into the effects of data augmentation on the dynamics of gradient flows during training for symmetric tasks. Future work includes further investigating the role of the symmetry group $G$, and extending the results to a stochastic setting.


\subsubsection*{Acknowledgement}
The authors would like to thank the anonymous reviewers of all versions of this manuscript, whose insightful comments have helped to significantly improve it. All authors were supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation. The computations were enabled by resources provided by the National Academic Infrastructure for Supercomputing in Sweden (NAISS) and the Swedish National Infrastructure for Computing (SNIC) at C3Se Chalmers, partially funded by the Swedish Research Council through grant agreements no. 2022-06725 and no. 2018-05973.

% The four groups.

%\subsubsection*{Acknowledgement}
%This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation. The computations were enabled by resources provided by the National Academic Infrastructure for Supercomputing in Sweden (NAISS) at Chalmers University of Technology partially funded by the Swedish Research Council through grant agreement no. 2022-0672.
\bibliography{biblio}
\bibliographystyle{plainnat}

\appendix

\section{Incorporating common architectures into the framework} \label{app:archs}

Before describing how some common architectures can be incorporated into the framework, let us point out a general construction. Instead of specifying $\calL$ as a whole, we may choose affine spaces $\Hom(X_{i},X_{i+1})$ of operators within each space $\Homm(X_i,X_{i+1})$, and then in the end set $\calL \vcentcolon=  \bigoplus_{i\in [L]} \Hom(X_i, X_{i+1})$. This in particular makes it clear that many of the constructions below can easily be combined with each other. Note that in the case that an architecture is built by such a direct sum, the orthogonal projector $\Pi_\calL$ is also applied component-wise, through projecting each layer to the respective tangent space $\rmT\Hom(X_i,X_{i+1})$. In the layer-wise constructions below we will refer to the vector spaces spaces as $U$ and $V$ instead of $X_i$ and $X_{i+1}$ to simplify notation.



\paragraph{Fully connected without bias } Letting $\calL = \calH$ corresponds to a fully connected MLP layer with no bias. 

\paragraph{Fully connected with bias} Allowing for bias terms corresponds to using affine instead of linear maps in each layer. Affine maps can however be considered as linear maps on a lifted space: Given spaces $U$ and V, we set $\widetilde{U} = U \oplus \R$, $\widetilde{V} = V \oplus \R$ and define
\begin{align*}
    \Hom(\widetilde{U}, \widetilde{V}) \vcentcolon= \left\{\begin{bmatrix}
            A & b \\ 0 & 1
        \end{bmatrix} \, \bigg\vert \, A \in \Homm(U,V), b\in V\right\}.
\end{align*}
Extending the nonlinearity $\sigma$ to $\widetilde{\sigma}(x,1)\vcentcolon=(\sigma(x),1)$, we obtain
    \begin{align*}
        \widetilde{\sigma}\bigg(\begin{bmatrix}
            A & b \\ 0 & 1
        \end{bmatrix} \begin{bmatrix}
            x \\ 1
        \end{bmatrix}\bigg) = \begin{bmatrix}
            \sigma(Ax +b) \\ 1
        \end{bmatrix},
    \end{align*}
    which emulates a fully connected layer with bias.
    Identifying the original data-points $x_i \in U$ with $(x_i,1)\in U\oplus \R$, we can thus include layers with bias terms into our framework.

    Note that representations $\rho_U$, $\rho_V$ also need to be extended: $\widetilde{\rho}_U(g)(x,1) = (\rho_U(g)x,1)$, and so on. It is not hard to show that these nonlinearities are equivariant if and only if the original ones are. The equivariance of the affine maps $x\mapsto Ax+b$ are furthermore equivalent to the equivariance of the corresponding linear maps in $\Homm(\widetilde{U},\widetilde{V})$

    \paragraph{Convolutional layers}  Since the space of convolutional operators $x \mapsto \varphi *x$ is a linear one, we may choose $\Hom(X_i,X_{i+1})$ equal to it. We can even restrict the filters $\varphi$ to lie an affine space, such as a space of filter having a certain support. Note that every operator in this space already is equivariant with respect to translations, but that equivariance with respect to other groups could still be relevant. 

    \paragraph{Residual connections} Residual layers are layers of the form $U \to U$, $x' = x + \sigma(Ax)$. They  can be modelled via introducing an additional intermediate vector space $W = U \oplus U $, and setting
    \begin{align*}
        &\Hom(U,W) = \left\{\begin{bmatrix}
            \id  \\ A
        \end{bmatrix} \, \bigg\vert \, A \in \Homm(U,U)\right\}, \ \widetilde{\sigma}_{W}(x,y)\vcentcolon=(x,\sigma(y)), \\ 
        &\Hom(W,U) = \left\{\begin{bmatrix}
            \id & \id 
        \end{bmatrix} \right\}, \widetilde{\sigma}_{U} = \id
    \end{align*}
   Now, for  $\widetilde{A}\in \mathrm{Hom}(U,W)$ and $E \in \mathrm{Hom}(W,U)$, we have 
    \begin{align*}
        \widetilde{\sigma}_{U}(E \widetilde{\sigma}_{W}(\widetilde{A}x)) = \widetilde{\sigma}_{U}(E\widetilde{\sigma}_{W}(x,Ax)) = \widetilde{\sigma}_{U}(E(x,\sigma(Ax))) = \widetilde{\sigma}_{U}(x + \sigma(Ax))=x+\sigma(Ax).
    \end{align*}
    It should be clear that more general residual connections can be incorporated in a similar manner.

    As for group actions, it is in this context natural to assume that the representations of $G$ on the in- and output spaces  -- only then will $\id$ be equivariant. We can then define $\rho_{W}(g)(x,y) = (\rho(g)x,\rho(g)y)$ -- a layer $(\id,A)\in \Hom(U,W)$ is then equivariant if and only if $A\in \Hom(U,U)$ is. 

    

    \paragraph{Attention layers} Attention layers \citep{vaswani2017attention} map sequences $u \in U^n$ to sequences $v\in V^n$, where $U$ and $V$ are vector spaces, through the formula
    \begin{align*}
        w_i= \sum_{j} \alpha_{ij} \nu_j , \quad \alpha_{i,j} = \mathrm{softmax}(\sprod{q_i,k_{\cdot}})_j,
    \end{align*}
    where $\sprod{q_i,k_{\cdot}}=(\sprod{q_i,k_{\ell}})_{\ell\in [n]}$. Here, $q\in W^n$ and $k\in W^n$ are the so called \emph{query} and \emph{key} sequences, with values in a third vector space $W$, $\nu\in V^n$ is the \emph{value} sequence, and $\mathrm{softmax}$ is the function:
    \begin{align*}
        \mathrm{softmax}(p)_i = \frac{e^{p_i}}{\sum_j e^{p_j}}.
    \end{align*}
    Each sequence $(\alpha_{i,\cdot})_{j \in [n]}$ is hence a probability distribution with respect to which one takes an expected value of the values $(\nu_j)_{j \in [n]}$ to produce a new value $w_i$. The queries, keys and values are calculated via applying linear maps $Q: U \to W$, $K: U \to W$ and $N: U \to V$ elementwise, i.e. $q_i = Qu_i$, $k_j = Ku_j$ and $\nu_i = Nu_i$ (there are variations where these maps are non-linear; in this treatment we will assume they are linear). To simplify notation, we write $q=Qu$, $k=Ku$ and $\nu=Nu$.

    We can include such attention layers in our framework as follows: First, let us, similarly to above, introduce an intermediate space $Z = W^n \bigoplus W^n \bigoplus V^n$, and set
    \begin{align*}
        \Hom(U^N,Z) &= \{ u \to (Qu,Ku,Nu) \, \vert \, Q,K\in \Homm(U,W), N \in \Homm(U,V) \}, \\
        \sigma_{Z}^{\mathrm{att}}(q,k,\nu) &= \bigg(0,0,\big(\sum_j \alpha_{ij}\nu_j\big)_{i\in [n]}\bigg), \text{ with } \alpha_{i,j } =  \mathrm{softmax}(\sprod{q_i,k_{\cdot}})_j\\
        \Hom(Z,V^n)&=  \{ (q,k,\nu) \to \nu\}, \quad \sigma_{V^n}^{\mathrm{att}} = \id.
    \end{align*}
    Note that these definitions may seem unnecessarily complicated, but they make sure that domain and ranges of all non-linearities are the same, which our framework formally requires. It is not hard to check that, in sequence, applying a linear map in $L(U^n,Z)$, $\sigma^{\mathrm{att}}_Z$, the linear map in $L(Z,V^n)$ and finally $\sigma_{V^n}^{\mathrm{att}}$ to a sequence in $U^n$ has the effect of an attentional layer.

    As for the group actions, we imagine unitary representations $\varrho_{\mathrm{in}}$ and $\varrho_{\mathrm{out}}$ are given on the input and output-spaces $U$ and $V$, respectively.   
    These are naturally extended to actions on $U^n$ and $V^n$ through $(\rho_U(g)u)_i = \varrho_{\mathrm{in}}(g)u_i$ and $(\rho_{V}(g)v)_i= \varrho_{\mathrm{out}}(g)v_{i}$. In order to incorporate the intermediate spaces, we assume that a unitary representation $\varrho_{\mathrm{key}}$ on the key-query space $W$ is given, which in the same vain can be extended to a representation $\rho_{W}$ on $W^n$. If we then define  $\rho_{Z}(g)(q,k,\nu) =(\rho_{W}(g)q, \rho_{W}(g)k, \rho_V(g)\nu) $ on $Z$, $\sigma_{Z}^{\mathrm{att}}$ becomes equivariant:
    \begin{align*}
        \sigma_{Z}^{\mathrm{att}}(\rho_{Z}(g)(q,k,\nu)) &= \sigma_{Z}^{\mathrm{att}}(\rho_{W}(g)q, \rho_{W}(g)k, \rho_V(g)\nu) \\
        &=\left(0,0, \sum_j \mathrm{softmax}(\sprod{\varrho_{\mathrm{key}}(g)q_i, \varrho_\mathrm{key}(g)k_{\cdot}})_j \varrho_{\mathrm{out}}(g)\nu_j\right) \\
        & =\left(0,0, \varrho_{\mathrm{out}}(g)\sum_j \mathrm{softmax}(\sprod{q_i, k_{\cdot}})_j \nu_j\right) \\ 
        &= \rho_{Z}(g) \sigma_{Z}^{\mathrm{att}}(q,k,\nu),
    \end{align*}
    where the unitarity of $\varrho_{\mathrm{key}}$ was used in step three.  It should be noted that a map in $\Hom(U^n,Z)$ is equivariant if and only if $Q$, $K$ and $N$ are. The equivariance of the map in $L(Z,V^n)$ and of $\sigma^{\mathrm{att}}_{V^n}$ is also clear.

    

    \underline{$\mathrm{SE}(3)$ transformers}  Let us argue that modulo a few minor technicalities, the prominent $\mathrm{SE}(3)$-transformer \citep{fuchs2020se} emerges as $G$-equivariant transformers in this way: In this setting, the in and out features are vector fields $f_i : \R^3\to \calV$, where $\calV$ is a space on which the special euclidean group $\mathrm{SE}(3)$ is acting, of the form $v_i \delta_{x_i}$, where $\delta_{x_i}$ are Dirac deltas for some fixed positions $x_i$. Since $\mathrm{SE}(3)$ is not compact, we must restrict the $Q$, $K$ and $N$-maps in $\Hom(U^n,Z)$ to a priori be convolutions, i.e. equivariant to translations -- the symmetry group then becomes $\mathrm{SO}(3)$. By further letting the $\mathrm{softmax}$ operation occur over neighborhoods of each $i$ in an a-priori given graph, letting $Q$, $K$ and $N$ sample more general vector fields in the positions $x_i$, and adding a self-attention mechanism, i.e. in essence an additional linear residual connection of each feature $f_i$ to itself, we obtain the architecture. Since this section is already unnecessarily technical as it is, we choose to omit the technical details.

    \emph{Remark:} It seems that this formulation of an equivariant transformer architecture is novel -- the authors have not seen it in this generality in the literature (although, as we have seen, special cases of it have been -- we would also like to mention Vector Neurons \citep{deng2021vector}). It is unclear whether this more general formulation can be used to obtain new practically relevant architectures. We deem the detailed investigation of this matter beyond the scope of this work.

    \paragraph{Message passing} A popular way to process graph data are so-called \emph{message passing networks}. Let's discuss a simple version of them here -- linear message maps. In one sense, they are simplified attention networks: Again, features $u_i\in U$ on nodes of a graph are processed by the same linear maps $S: U\to V$ and $I: U \to V$, and then mixed:
    \begin{align*}
        v_i = Su_i + \sum_{j}a_{ij} Iu_j.
    \end{align*}
    The only difference is that the coupling coefficients $a_{ij}$ here are constant, given by a weighted adjacency matrix of the graph. Note that we here include a separate 'self-connection map' $S$ -- this would correspond to self-attention above. Note that message passing layers are inherently equivariant towards permutations (of both the features and the adjacency matrix, corresponding to a reordering of the graph labels).

    
    
    Because of their similarities to attention layers, we can realize them in a similar manner as above. The input space consists of pairs of feature sequences and adjacency matrix, say $\calU = U^n \bigoplus \R^{n,n}$, and the output space is similarly $\calV = V^n \bigoplus \R^{n,n}$. We define an intermediate space $\calZ = V^n \bigoplus V^n \bigoplus \R^{n,n}$ and define
    \begin{align*}
        \Hom(\calU,\calZ) &= \{ (u,A) \mapsto (Su,Iu,A) \, \vert \, S,I \in \Homm(U,V) \}, \quad \sigma_\calZ^{\mathrm{MP}}(s,\iota,A) = \big(0, (s_i+\sum_{j} A_{i j}\iota_j)_i, A\big) \\
        \Hom(\calZ,\calV) &= \{(s,\iota, A)\mapsto (\iota, A) \}, \quad \sigma_\calV^{\mathrm{MP}} = \id.
    \end{align*}
    By slightly changing the formulations, other versions of message passing could also easily be treated, i.e. versions with non-linear message maps or versions with edge-features.

    Representations acting separately on each feature $u_i$, as in the transformer example, can immediately be included into this framework. Let us also note that regular representations with respect to subgroups of the permutation group $S_n$ naturally can be included. To be more precise, let $G \sse S_n$ be a subgroup, and  $\varrho_{\mathrm{in}}$, $\varrho_{\mathrm{out}}$ be representations of $G$ on $U$ and $V$, respectively. We can then accomodate actions on $U^n$, $V^n$ and $\R^{n,n}$ of the following form:
    \begin{align*}
        (\rho_U(\pi)u)_i = \varrho_{\mathrm{in}}(\pi)u_{\pi^{-1}(i)}, \quad (\rho_V(\pi)v)_i = \varrho_{\mathrm{out}}(\pi)v_{\pi^{-1}(i)}, \quad (\rho_{\mathrm{adj}}(\pi)A)_{i,j} = A_{\pi^{-1}(i)\pi^{-1}(j)}
    \end{align*}
    These, of course, induce natural representations $\rho_\calU$, $\rho_\calV$ of $G$ on $\calU$ and $\calV$. If we define the representation on $\calZ$ as $\rho_\calZ(\pi)(s,\iota,A) = (\rho_\calU(\pi)s, \rho_\calV(\pi)\iota, \rho_{\mathrm{adj}}(\pi)A)$, $\sigma_\calZ^{\mathrm{MP}}$ becomes equivariant. We have
    \begin{align*}
       \sigma_\calZ^{\mathrm{MP}}(\rho_\calZ(\pi)(s,\iota,A)) = \sigma_\calZ^{\mathrm{MP}}(\rho_\calV(\pi)s,\rho_\calV(\pi)\iota,\rho_{\mathrm{adj}}(\pi)A) = \big(0, \big((\rho_\calV(\pi)s)_i+\sum_{j} (\rho_{\mathrm{adj}}(\pi)A)_{ij}(\rho_\calV(\pi)\iota)_j\big)_i, \rho_{\mathrm{adj}}(\pi)A\big)
    \end{align*}
    We calculate
    \begin{align*}
         (\rho_\calV(\pi)s)_i+\sum_{j} (\rho_{\mathrm{adj}}(\pi)A)_{ij}(\rho_\calV(\pi)\iota)_j &= \varrho_{\mathrm{out}}(\pi)s_{\pi^{-1}(i)}  + \sum_{j} A_{\pi^{-1}(i)\pi^{-1}(j)}\iota_{\pi^{-1}(j)} \\
         &= \varrho_{\mathrm{out}}(\pi)\big(s_{\pi^{-1}(i)}  + \sum_{j} A_{\pi^{-1}(i)j}\iota_{j}\big) = \rho_\calV(\pi) \big( \big(s_{i}  + \sum_{j} A_{i j}\iota_{j}\big)_{i}\big)_i.
    \end{align*}
    Hence,
    \begin{align*}
        \sigma_Z^{\mathrm{MP}}(\rho_\calZ(\pi)(s,\iota,A)) = \bigg(\rho_\calV(\pi)0, \rho_\calV(\pi) \bigg( \big(s_{i}  + \sum_{j} A_{ij}\iota_{j}\big)\bigg), \rho_{\mathrm{adj}}(\pi)A \bigg) = \rho_\calZ(\pi) \sigma_Z^{\mathrm{MP}}(s,\iota,A).
    \end{align*}
    Also, the only operator in $\Hom(\calZ,\calV)$ and $\sigma_\calV^{\mathrm{MP}}$ trivially become equivariant.
    
    


    \paragraph{Recurrent architectures} In recurrent architectures, the same layers are applied several times. Since constraining layers to be equal is a linear operation, they can easily be included in our framework. For instance, the recurrent architecture
    \begin{align*}
        x_{i+1} = \sigma_i(Mx_i), \quad i \in [L]
    \end{align*}
    where all $A_i$ equal a common linear map $M$, can be described through
    \begin{align*}
        \calL = \{ A \in \calH \, \vert \, A_i=A_{i+1}, i \in [L] \}.
    \end{align*}





\section{The compatibility condition} \label{app:comp}
 Here, we discuss the compatibility condition a bit more at length than in the main paper. In many of the  examples presented in the previous section, the compatibility condition is immediate due to Proposition \ref{prop:invcomp}:
 \begin{itemize}
     \item Fully connected with bias.
     \item Residual connections.
     \item Attention layers.
     \item Message passing layers.
     \item Recurrent architectures (when all $\rho_i$ are equal).
 \end{itemize}
 In all of these cases, for the types of representations discussed above, it is not hard to show that all operators $\overline{\rho}(g)$ leaves the space $\calL$ invariant. Generally and intuitively speaking, this is due to the 'linear parts' of the maps being unrestricted. As an example, let us carry out the details for the residual connections. For a map $\Lambda = [\id, M]^T$, we have
 \begin{align*}
     \overline{\rho}(g)\Lambda  = [\rho(g)\id\rho(g)^{-1}, \rho(g)A\rho(g)^{-1}] = [\id,  \rho(g)A\rho(g)^{-1}],
 \end{align*}
 so that $\overline{\rho}(g)\Lambda$ is again a map in $\Homm(U,V)$, since $\rho(g)A\rho(g)^{-1}$ still is linear. The other cases are dealt with similarly.

Now let us describe a few more slightly more involved examples, beginning with the case of convolutions supported on non-symmetric $\Omega$.



\paragraph{Convolutions with skew support}
 Let us finish the discussion on $\calL_\Omega$ begun in Example \ref{ex:skewvscross}, beginning with a description of the orthogonal projection onto $\calL_\Omega$.
\begin{lemma}
     Let $P_\Omega$ be the linear map that maps convolutional operators with filter $\varphi$ to the convolutional operator with filter $\one_\Omega \cdot \varphi$, i.e. zeroing out indices outside $\Omega$, and is zero  on $\calC^\perp$. Then,
     \begin{align*}
         \Pi_{\calL_\Omega} = P_\Omega \Pi_\calC.
     \end{align*}
 \end{lemma}
 \begin{proof}
     Let us first show that $P_\Omega \Pi_\calC$ defines a projection, i.e. that $P_\Omega \Pi_\calC P_\Omega \Pi_\calL = P_\Omega \Pi_\calC$. This is not hard -- applying $\Pi_\calC$ to an $A$ yields a convolutional operator, say $C_\varphi$, which is then mapped to $C_{\one_\Omega \cdot \varphi}$ by $P_\Omega$. This operator is however not changed by $\Pi_C$ (since it is convolutional), and multiplying the filter with $\one_\Omega$ again does not change it. Hence, $P_\Omega \Pi_\calC P_\Omega \Pi_\calL A = C_{\one_\Omega\cdot \varphi} = P_\Omega \Pi_\calC A$.

     Now let us show that it is orthogonal. For this, we need to show that $\sprod{A- P_\Omega \Pi_\calC A,P_\Omega \Pi_\calC B}=0$ for all $A$ and $B$. Let us first notice that $\sprod{A-\Pi_\calC A, P_\Omega \Pi_\calC B} =0 $, since $A-\Pi_\calC A \in \calC^\perp$ and $P_\Omega \Pi_\calC B \in \calC$. It hence suffices to show that $\sprod{ \Pi_\calC A - P_\Omega \Pi_\calC A, P_\Omega \Pi_\calC B} = 0 $. %To simplify notation, let us write $\Pi_\calC A= C_\varphi$ and $\Pi_\calC B = C_\psi$. 
     Let us first calculate $\sprod{C_\varphi, C_\psi}$ for two given filters. We have by definition
     \begin{align*}
         \sprod{C_\varphi,C_\psi} = \sum_{i \in [N]^2} \sprod{C_\varphi e_i, C_\psi e_i},
     \end{align*}
     where $e_i$ is the canonical basis of $\R^{N,N}$. We have
     \begin{align*}
         (C_\varphi e_i)[\ell] = \sum_{k \in [N]^2} e_i[\ell-k] \varphi(k) = \varphi[\ell-i],
     \end{align*}
     since $e_i[\ell-k]=1$ precisely when $k=\ell-i$ (and zero otherwise). Hence
     \begin{align*}
         \sum_{i \in [N]^2} \sprod{C_\varphi e_i, C_\psi e_i} = \sum_{i \in [N]^2} \varphi[\ell-i]\psi[\ell-i] = N^2 \sprod{\varphi, \psi}.
     \end{align*}
     Now, writing $\Pi_\calC A = C_\varphi$ and $\Pi_\calC B= C_\psi$, we get
     \begin{align*}
         \sprod{C_\varphi - P_\Omega C_\varphi, P_\Omega C_\psi} = \sprod{C_{\one_{\Omega^c}\cdot \varphi}, C_{\one_\Omega \cdot \psi}} = N^2 \sprod{\one_{\Omega^c}\cdot \varphi, \one_\Omega \cdot \psi} = 0, 
     \end{align*}
     since the filters in the last expression have disjoint supports.
     
     It remains to show that the range of $P_\Omega \Pi_\calC$ is equal to $\calL_\Omega$. But this is easy -- it is clear that the operator maps any operator to a filter operator with filter supported on $\Omega$, i.e. into $\Omega$, and that it does not do anything to an operator already there.
 \end{proof}

 Let us now show that $\Pi_{Z_4}$ acts directly on the filter for a convolutional operator.

 \begin{lemma}\label{lem:filterprojection}
     For $C_\varphi \in \calC$ and the rotation action of $\Z_4$, we have $\Pi_{\Z_4} C_\varphi = C_{\Pi_{\Z_4} \varphi}$.
 \end{lemma}
 \begin{proof}
    This follows from the twirling formula (Lemma \ref{lem:ind_rep_orth_proj}) and the fact that the representations act on the filter. We have
    \begin{align*}
        \Pi_{\Z_4} C_{\varphi}=\int_{\Z_4} \overline{\rho}(g) C_\varphi \, \dd\mu(g)=\int_{\Z_4}  C_{\rho(g)\varphi} \, \dd\mu(g) = C_{\int_{\Z_4} \rho(g) \varphi \, \dd\mu(g)} = C_{\Pi_{\Z_4}\varphi},
        \end{align*}
        where the penultimate step follows by linearity (note that here the integral is actually a sum).
 \end{proof}
Now we can construct an operator for which the compatibility condition $\Pi_{\calL_{\Omega}}\Pi_{\Z_4} = \Pi_{\Z_4}\Pi_{\calL_{\Omega}}$ is not fulfilled. Consider a convolutional operator $C_\varphi$ given by a filter with support as in Figure \ref{fig:supports} (right) where the top left element is non-zero. For such an operator we have that applying $\Pi_{\calL_\Omega}$ to it does not change it at all. Subsequently applying $\Pi_{\Z_4}$ is by Lemma \ref{lem:filterprojection} the same as averaging rotations of the filters. Since we have assumed that the top left element is non-zero, this yields a filter where all corner elements are non-zero. An operator given by such a filter is surely not in $\calL_\Omega$, which $\Pi_{\calL_{\Omega}}\Pi_{\Z_4} C_\varphi$ is.
 

 \paragraph{Diagonal operators} Let us discuss a case in which Proposition \ref{prop:invcomp} does not apply, but the compatibility condition is still satisfied. We consider a single layer, let the input and output space be equal to $\R^3$, and $G=\mathrm{O}(3)$ be the orthogonal group acting in the canonical way on both spaces. It is then not hard to realize that $\calH_G = \mathrm{span} \, \id$ (shooting flies with cannons, one can appeal to $\R^3$ being an irrep of the action, and then to Schur's Lemma, but one can also deduce this by more elementary means).

 Now consider the architecture choice of only using diagonal operators as linear layers:
 \begin{align*}
     \calL_{\mathrm{diag}} = \{ A \in \Homm(\R^3,\R^3) \, \vert \, A_{ij}=0, i\neq j \}.
 \end{align*}
 This corresponds to treating the 'channels' of the in-vectors $x$ separately. Then, $\calL$ is not invariant under all $\overline{\rho}(g)$ -- the eigenvectors of the map $\overline{\rho}(g)D$ for a $D\in \calL_{\mathrm{diag}}$ with non-constant diagonal are given by $\overline{\rho}(g)e_0, \overline{\rho}(g)e_1, \overline{\rho}(g)e_2$, which are surely not always equal to $e_0,e_1,e_2$, which is necessary for $\overline{\rho}(g)D$ to be diagonal. However, the compatibility condition is satisfied: since $\calH_G  = \mathrm{span} \, \id \sse \rmT \calL$, $\Pi_\calL\Pi_G = \Pi_G\Pi_\calL = \Pi_G$.

\paragraph{Subgroups} Consider any group $G$ and choose $H$ as a subgroup of $G$. For vector spaces $U,V$ on which $G$ is acting, now set 
 \begin{align*}
     \calL = \calH_H = \{A \in \calH \, \vert \, A\rho_U(h) = \rho_V(h)A, h \in H\},
 \end{align*}
  i.e. the linear operators that are only equivariant to the action of the subgroup. $\calL$ is then in general not invariant to the action of $G$. The diagonal operators above is in fact an example of this -- here, $G=\mathrm{O}(3)$, and the subgroup is the group of permutations of components. 

  Note that just as in the special case of diagonal operators, the compatibility condition is always satisfied, since $\calL=\calH_H \supseteq \calH_G$.
\paragraph{Upper triangular matrices and shifts}
Consider as input and output space $U=V=\R^N\bigoplus\R^N \sim \R^{2N}$ and the action of $S_2= \{\id, \varsigma\}$ on this space by interchanging the coordinates, i.e $\rho(\varsigma)(x,y)=(y,x)$. Let $\calL$ be the space of linear maps described by upper triangular matrices, i.e.,
\begin{align*}
    \calL =  \{ K \in \Homm(\R^{2N},\R^{2N}) \, \vert \, K_{ij}=0 , i>j\}.
\end{align*}
Alternatively, each element in $\calL$ can be written as a block matrix
\begin{align*}
    \begin{bmatrix}
        A^\triangledown & B \\ 0 & C^\triangledown
    \end{bmatrix}, \quad A, B,C \in \R^{N,N}
\end{align*}
where we introduced the notation $A^\triangledown$ for the matrix obtained by putting all elements in $A$ under the diagonal to zero. This type of layer can be though of as 'causal' -- if we interpret $u=(x,y)$ as a time series $y_{n-1},y_{n-2}, \dots y_0, x_{n-1}, \dots$, the value $A(u)[n-i]$ only depends on the first $i$ values in $u$, i.e., the values 'before' time $i$. 

To see what $\calE$ is in this setting, let us notice that the lifted representation of $S_2$ on $\Homm(U,V)$ is as follows:
\begin{align}
    \overline{\rho}(\varsigma)\left(  \begin{bmatrix}
        A & B \\ D & C
    \end{bmatrix}\right) = \rho(\varsigma) \begin{bmatrix}
        A & B \\ D & C
    \end{bmatrix}\rho(\varsigma)^{-1}= \begin{bmatrix}
        0 & \id \\ \id & 0
    \end{bmatrix}\begin{bmatrix}
        A & B \\ D & C
    \end{bmatrix}\begin{bmatrix}
        0 & \id \\ \id & 0
    \end{bmatrix} =\begin{bmatrix}
        C & D \\ B & A
    \end{bmatrix} \label{eq:shift}
\end{align}
Thus, a map is in $\calH_G$ if and only if $A=C$ and $B=D$. If it should also be in $\calL$, $A$ and $C$ need to be upper triangular, and $B=D=0$. Thus
\begin{align*}
    \calE = \bigg\{  \begin{bmatrix}
        A^\triangledown & 0 \\ 0 & A^\triangledown
    \end{bmatrix} \, \big\vert \, A\in \R^{N,N} \bigg\}
\end{align*}

We claim that the compatibility condition is not satisfied for this setting. First, $\calL$ is clearly not invariant under the action of $S_2$, so that Proposition \ref{prop:invcomp} does not apply. By appealing to \eqref{eq:shift} and the twirling formula, we obtain:
\begin{align*}
    \Pi_{S_2}  \begin{bmatrix}
        A & B \\ D & C
    \end{bmatrix} = \begin{bmatrix}
        \tfrac{1}{2}(A+C) &  \tfrac{1}{2}(B+D) \\  \tfrac{1}{2}(B+D) &  \tfrac{1}{2}(A+C)
    \end{bmatrix}
\end{align*}
It is further trivial to see that
\begin{align*}
     \Pi_{\calL}  \begin{bmatrix}
        A & B \\ D & C
    \end{bmatrix} =   \begin{bmatrix}
        A^\triangledown & B \\ 0 & C^\triangledown
    \end{bmatrix}.
\end{align*}
Hence,
\begin{align*}
    \Pi_\calL \Pi_{S_2}  \begin{bmatrix}
        A & B \\ D & C
    \end{bmatrix} &= \Pi_\calL \begin{bmatrix}
        \tfrac{1}{2}(A+C) &  \tfrac{1}{2}(B+D) \\  \tfrac{1}{2}(B+D) &  \tfrac{1}{2}(A+C)
    \end{bmatrix} = \begin{bmatrix}
        \tfrac{1}{2}(A+C)^\triangledown &  \tfrac{1}{2}(B+D) \\  0 &  \tfrac{1}{2}(A+C)^\triangledown
    \end{bmatrix}, \text{ and } \\
     \Pi_{S_2} \Pi_\calL  \begin{bmatrix}
        A & B \\ D & C
    \end{bmatrix} &= \Pi_{S_2}\begin{bmatrix}
        A^\triangledown & B \\ 0 & C^\triangledown
    \end{bmatrix} = \begin{bmatrix}
        \tfrac{1}{2}(A+C)^\triangledown &  \tfrac{1}{2}B \\  \tfrac{1}{2}B &  \tfrac{1}{2}(A+C)^\triangledown
    \end{bmatrix}.
\end{align*}
These are for $B\neq 0$ not equal, and thus the compatibility condition is not fulfilled.


\section{The converse of Theorem \ref{th:stable}.1 does not hold}\label{app:stability}
We construct an architecture for which a point $A\in S^\eqv =S^\aug$ is stable for $\eqvi$, but not for $\augm$. 

\begin{itemize}
\item \textbf{Architecture} The model is a simple one-layer network:
\begin{align*}
    \Psi_\lambda : \R^N \to \R, x \mapsto e^{\sprod{\lambda,x}},
\end{align*}
where $\lambda$ can be chosen freely in $\R^N$. This corresponds to letting $X= \R^N$,  $Y = \R$ and  $\sigma(z) = e^z$.   The vector $\lambda \in \R^N$ can be viewed as $A_0 \in \Homm(\R^N,\R)$ 
and $\calL = \calH$.

\item \textbf{Symmetry group} We consider the group of translations $\Z_N$ acting canonically through $\rho^{\tr}$ on $\R^N$ and trivially on $\R$. Then, $\calE = \calH_{\Z_N} = \mathrm{span}\,\mathds{1}$.
\item \textbf{Loss} 
We use the following loss:
\begin{align*}
    \ell(y,y') = -yy'.
\end{align*}

\item \textbf{Data} We consider a finite dataset $(x_i,y_i) \in \R^N \times \R$, $i \in [m]$ with the property
\begin{align} \label{eq:balanced}
    \frac{1}{m}\sum_{i \in [m]} y_i x_i = 0.
\end{align}
Furthermore, the dataset has the property that all $x_i$ are either in $\calE$ or in $\calE^{\perp}$. Say, $x_i$, $i\in I$ are in $\calE$ and $x_j$, $j\in J$ are in $\calE^{\perp}$. The corresponding labels satisfy $y_i<0$, for all $i\in I$ and $y_j>0$, for all $j\in J$. We also assume that $\calE=\mathrm{span}\,\{x_i\,\vert\,i\in I\}$, and that $J$ is nonempty.
\end{itemize}
Both $\ell$ and $\sigma$ are equivariant, so that Assumptions \ref{ass:unitary_reps}--\ref{ass:equiv_nonlin} are all satisfied, as is trivially the compatibility condition. We will now show that $\lambda=0$ (which is in $\calE$) is a stable stationary point for $\eqvi$, but not for $\augm$. The nominal risk is given by $R(\lambda)=-\frac{1}{m}\sum_{i\in [m]} y_i\sigma(\sprod{\lambda,x_i})$. By the chain rule, the gradient for the nominal risk is given by
\begin{align*}
    \nabla R(\lambda) = -\frac{1}{m}\sum_{i\in [m]} y_i\sigma'(\sprod{\lambda,x_i})x_i.
\end{align*}
Since $\lambda=0$, it follows that $\sigma'(\sprod{\lambda,x_i})=1$. Then $\nabla R(\lambda)=0$, by \eqref{eq:balanced}. Thus, $\lambda$ is a stationary point for $\nominal$ and thus also for $\augm$ and $\eqvi$, by Theorem \ref{theo:gradients_equal}.

Again by the chain rule,
\begin{align*}
    R''(\lambda) = -\frac{1}{m}\sum_{i\in [m]} y_i\sigma''(\sprod{\lambda,x_i})x_ix_i^*.
\end{align*}
Now, let us show that for $v\in \calE$, $R''(\lambda)[v,v]>0$. By the proof of Theorem \ref{th:stable}, this is enough to show that $\lambda=0$ is a stable point for $\eqvi$. But because of the structure of the dataset we have
\begin{align*}
    R''(0)[v,v] = -\frac{1}{m}\sum_{i\in [m]} y_i\sigma''(0)\sprod{v,x_i}^2 = -\frac{1}{m}\sum_{i\in I} y_i\sigma''(0)\sprod{v,x_i}^2 >0,
\end{align*}
since $y_i<0$ for $i\in I$. To show that $\lambda=0$ is unstable for $\augm$ we show that there exists $w\in \calE^{\perp}$ such that $(R^{\mathrm{aug}})''(0)[w,w]<0$. By Lemma \ref{lem:aug_hess}, we have for $w\in \calE^{\perp}$ that
\begin{align*}
    (R^{\mathrm{aug}})''(0)[w,w]=-\int_{\Z_N}\frac{1}{m}\sum_{i\in [m]} y_i\sigma''(0)\sprod{\rho^{\tr}(k)w,x_i}^2 \, \dd\mu(k).
\end{align*}
By unitarity of $\rho^{\tr}$ we have that $\rho^{\tr}(k)w\in \calE^{\perp}$. Thus, the sum above is only over $i\in J$, so that all terms are non-negative. By choosing $w=x_j$ for some $j\in J$, we ensure that at least one term is strictly positive. Thus, $(R^{\mathrm{aug}})''(0)[w,w] <0$ for that $w$, and $\lambda$ is unstable for $\augm$.
\section{Proofs} \label{sec:proofs}
Here, we present proofs left out in the main text.

\begin{proof}[\textbf{Proof of Lemma \ref{lem:commutativity}}]
   If $\Pi_\calL\Pi_G=\Pi_\calE$, $\Pi_\calL\Pi_G$ is self-adjoint as an orthogonal projection. Consequently, $\Pi_\calL\Pi_G = (\Pi_\calL\Pi_G)^*=\Pi_G^*\Pi_\calL^*=\Pi_G\Pi_\calL$, i.e., $\Pi_G$ and $\Pi_L$ commute.

   If we on the other hand assume that $\Pi_G$ and $\Pi_\calL$ commute, $\Pi_\calL\Pi_G$ becomes self-adjoint. It furthermore is idempotent, since $$(\Pi_\calL \Pi_G)^2= \Pi_\calL \Pi_G \Pi_\calL \Pi_G = \Pi_\calL \Pi_G^2\Pi_\calL = \Pi_\calL \Pi_G\Pi_\calL = \Pi_\calL^2\Pi_G = \Pi_\calL \Pi_G.$$ Hence, $\Pi_\calL \Pi_G$ is an orthogonal projection. Its range is included in both $\rmT\calL$ and $\calH_G$ -- the former is clear, and the latter follows from $\Pi_\calL\Pi_G = \Pi_G \Pi_\calL$. Hence, the range is included in $\rmT\calE$. However, if $A\in \rmT\calE$, it must be $\Pi_\calL\Pi_G A = \Pi_\calL A = A$, since $A\in \calH_G$ and $A\in \rmT\calL$. Hence, the range of the operator is in fact equal to $\rmT\calE$, and we are done.
\end{proof}
\begin{proof}[\textbf{Proof of Lemma \ref{lem:ind_rep_orth_proj}}]
    To simplify the exposition of the argument, let us define the operator $P$ by
    \begin{align}%*}
        PA = \int_{G} \overline{\rho}(g)A \, \dd \mu(g).
    \end{align}%*}
    It needs to be shown that $P=\Pi_G$. To do so,  we first need to show that $PA \in \calH_G$ for any $A\in \calH$. To do this, it suffices to prove that $\overline{\rho}(g)PA = PA$ for any $g\in G$. Using the fact that $\overline{\rho}$ is an representation of $G$, and the invariance of the Haar measure, we obtain
    \begin{align*}
        \overline{\rho}(g)PA = \int_{G}\overline{\rho}(g)\overline{\rho}(h)A \, \dd \mu(h) = \int_G \overline{\rho}(gh) A \, \dd \mu(h) = \int_G \overline{\rho}(h')A \, \dd \mu(h') = PA.
    \end{align*}
    Next, we need to show that $PA=A$ for any $A\in \calH_G$. But since $\overline{\rho}(g)A=A$ for such $A$, we immediately obtain
    \begin{align*}
        PA = \int_{G}\overline{\rho}(g)A \, \dd \mu(g) = \int_G A \, \dd \mu(g) = A.
    \end{align*}
    Consequently, $P: \calH \to \calH_G$ is a projection. Finally, to establish that $P$ is also orthogonal, we need to show that $\sprod{A-PA,B}=0$ for all $A\in \calH$, $B\in \calH_G$. This is a simple consequence of the unitarity of $\overline{\rho}$ and the fact that $\overline{\rho}(g)B=B$ for all $B\in \calH_G$ and $g\in G$: 
    \begin{align*}
        \sprod{PA, B} = \int_G \sprod{\overline{\rho}(g)A,B} \, \dd \mu(g) =\int_G \sprod{\overline{\rho}(g)A,\overline{\rho}(g)B} \, \dd \mu(g) =\int_G \sprod{A,B} \, \dd \mu(g)  = \sprod{A,B} \,,
    \end{align*}
    which completes the proof that $P = \Pi_{G}$.
\end{proof}


    


\section{Experiments}
In this appendix we present additional information about the experiment in the main paper, and also describe an additional experiment we conducted.
\subsection{Experiment details: Compatibility condition and unitarity of embedding operator}
\paragraph{Architecture} The architecture (Figure \ref{fig:architecture}) used for this experiment consisted of three convolutional layers, followed by a fully-connected linear layer. The structure of the first two convolutional layers was as follows. A convolution with a $3\times 3$ filter and zero padding with one layer of zeroed border pixels (zero padding destroys any translational equivariance, but not the rotational equivariance), followed by average pooling with a $2\times 2$ pooling window and a stride of 2, followed by a $\tanh$ activation function, lastly followed by a layer normalization. The structure of the third convolutional layer was the same as the first two, without the average pooling. The first convolutional layer had one channel in and 32 channels out, the second had 32 channels in and 64 channels out, and the third had 64 channels in and 64 channels out. In the linear layer we flatten the parameter tensor into a vector in $\mathbb{R}^{3136}$ and map it linearly into $\mathbb{R}^{10}$ with a fully-connected layer.

As discussed earlier any activation function (e.g. $\tanh$) applied pixel-wise to an image will be equivariant to rotations by $\pi/2$ radians. It follows that average pooling with a $2\times 2$ window and a stride of 2 applied to a square image of even dimensions is also equivariant to rotations. This is true since any even dimensional square pixel grid can be subdivided into some number of $2\times 2$ grids which will rotate along with the whole grid. Note that rotations of pixels within an individual sub grid does not change the average value of its elements. See also Figure \ref{fig:pool_rot}.
%which will rotate along with the whole grid. Since the rotation within an individual sub grid does not change the average value its elements, the pooling acts equivariantly. 

\begin{figure}[b]
    \centering
\includegraphics[width=.8\textwidth]{figures/pool_rot.png}
    \caption{Average pooling with a $2\times 2$ window and a stride of $2$ is equivariant to rotations of square images of even size.}
    \label{fig:pool_rot}
\end{figure}

The layer normalization \citep{ba2016layer} technique can be described as follows. Given features $x_i$ $i \in [m]$ in some vector space $U$, the normalized features are
\begin{align*}
    \overline{x}_i = x_i - \tfrac{1}{m} \sum_{j \in [m]} x_j.
\end{align*}
This operation is by linearity equivariant to any representation.



\paragraph{Hardware}
The $30\cdot 3\cdot 4$ trials of the experiment were run in parallel on a super computer using NVIDIA Tesla T4 GPUs with 16GB RAM. We estimate the total computation time to be about 315 hours.





\subsection{An additional experiment}
 
% \newcommand{\transrott}{\textsc{TransRot}}
 %   \newcommand{\onedtranss}{\textsc{OneDTrans}}
  %  \newcommand{\twodtranss}{\textsc{TwoDTrans}}

A natural question to ask is if there are differences in the behaviour of the \augm{} model depending on which symmetry group is used. To this end, we perform an additional experiment. Here, we let the $\nominal$ architecture consist of fully connected layers without bias -- i.e. $\calL = \calH$ (making the compatibility condition trivial), taking images as input, say of size $N\times N$. Before being sent to the first layer, the images are normalized by subtracting $.5$ from every pixel in the image. The number of channels in the early layers are then $1$, $32$ and $32$, respectively. The non-linearities are here chosen as leaky ReLU's, except for the last one which is a $\mathrm{SoftMax}$, and we use a cross-entropy loss. Note that all of these non-linear operations are equivariant to any representation acting pixelwise on images. A detailed sketch of the architecture is given in Figure \ref{fig:transarc} .

We now consider four different symmetry groups on $\R^{N,N}$
\begin{itemize}
    \item \transs~ $\Z_N^2$ acting through translations, i.e. through $\rho^{\tr}$.
    \item \rott~$\Z_4$ acting through rotations, i.e. through $\rho^{\rot}$.
    \item \onedtranss~$\Z_N$ acting through translations in the $x$-direction, i.e.
    \begin{align}%*}
        (\rho^{\tr_0}(k)x)_{i,j} = x_{i-k,j}
    \end{align}%*}
    \item \transrott~The semi-direct product $\Z_N^2 \rtimes \Z_4$ acting through
    \begin{align}%*}
        \rho^{\mathrm{trot}}(\iota,k)x = \rho^{\rot}(k)\rho^{\tr}(\iota)x, \quad \iota \in \Z_N^2, k \in \Z_4.
    \end{align}%*}
    This can, in the same way $\Z_4$ is a discretization of the full group  $\mathrm{SO}(2)$ of rotations in the plane, be thought of as a discretization of the group $\mathrm{SE}(2)$ of isometries in the plane.
\end{itemize}
These actions induce actions on the early layers $(\R^{N,N})^m$. We let all groups act trivially on the late layers. 

We now initialize each architecture on a random point in $\calE$, and train each architecture in $\nominal$, $\augm$ and $\eqvi$ mode from there -- note that we in contrast to the experiment in the main paper do not first train the models in $\eqvi$ mode. We only use partially unitary $L$, and here use gradient descent, i.e. accumulate the gradient over the entire epoch before updating the layer parameters. We train the models on MNIST modifieid it in two ways to keep the experiments light. First, we train our models only on the 10000 test examples (instead of the 60000 training samples). Secondly, we subsample the $28 \times 28$ images to images of size $14 \times 14$ (and hence set $N=14$) using \texttt{opencv}'s \cite{opencv_library} built-in \textsc{Resize} function. This simply to reduce the size of the networks. Note that the size of the early (non-equivariant) layers of the model are proportional to the $N^4$, making this downsizing justifiable.




    \begin{figure}
        \centering
        \begin{tikzpicture}[scale=0.5]
            \draw (0,0) rectangle (1,3);
            \node at (0.5,1.5) {$X$};
            \node at (0.5,-1) {$\R^{N\times N}$};
            \node at (4,1) {leaky Relu};
            \node at (4,2) {Linear layer};
            \draw (7,0) rectangle (8,3);
            \node at (7.5,1.5) {$X_1$};
            \node at (7.5,-1) {$(\R^{N,N})^{32}$};
            \node at (11,1) {leaky Relu};
            \node at (11,2) {Linear layer};
            \draw (14,0) rectangle (15,3);
            \node at (14.5,1.5) {$X_2$};
            \node at (14.5,-1) {$(\R^{N,N})^{32}$};
            \node at (18,1) {LayerNorm};
            \node at (18,0.25) {leaky Relu};
            \node at (18,2) {Linear layer};
            \draw (21,0) rectangle (22,3);
            \node at (21.5,1.5) {$X_3$};
            \node at (21.5,-1) {$\R^{32}$};
            \node at (25,1) {Softmax};
            \node at (25,2) {Linear layer};\draw[thick,decorate,decoration={brace,amplitude=10pt},yshift=2pt] (-.5,3.9) -- (22.5,3.9) node [black,midway,xshift=0pt,yshift=18pt] {$\rho$};
            \draw (28,0) rectangle (29,3);
            \node at (28.5,1.5) {$Y$};
            \node at (28.5,-1) {$\R^{10}$};
            \node at (28.5,4) {$\rho^{\mathrm{triv}}$};
            \draw[->] (1.5,1.5) -- (6.5,1.5);
            \draw[->] (8.5,1.5) -- (13.5,1.5);
            \draw[->] (15.5,1.5) -- (20.5,1.5);
            \draw[->] (22.5,1.5) -- (27.5,1.5);
        \end{tikzpicture}
        
         \caption{The nominal architecture for the second set of experiments. Note that the symmetry group differ between experiments.}
    \label{fig:transarc}
    \end{figure}
   


We run each experiment $30$ times on Tesla A40 GPUs situated on a cluster, resulting in about 80 hours of GPU time.  In Figure \ref{fig:mnist_different groups}, we plot the evolution of the values $\Vert A_{\mathcal{E}^\perp} \Vert$ against the evolution of $\Vert A - A^0 \Vert$. The opaque lines in each plot is formed by the average values for all thirty runs, whereas the fainter lines are the 30 individual runs. We see that $\calE$ is not stable under the augmented flow in any of the experiments, but that the augmented flows stay closer to $\calE$ than the nominal ones -- this is consistent with our theoretical results.


Note that the scales in the figures are different, to assess the amount the augmented model drifts \emph{relative} to the other ones. We have chosen the limits for the axes as follows:
\begin{itemize}
    \item The $x_{\mathrm{left}}$-limit in both subplots is chosen as $1.5$ times the maximal (with respect to the $50$ training epochs) median (with respect to the $30$ runs) value of $\Vert{A-A_0}\Vert$ for the \eqvi~model.
    \item The $y_{\mathrm{left}}$-limit in the left subplot  is chosen as $1.5$ times the maximal (with respect to the $50$ training epochs) median (with respect to the $30$ runs) value of $\Vert\Pi_{\calE^\perp}A\Vert$ for the \nominal~model.
    \item The $y_{\mathrm{right}}$-limit in the left subplot is given by $\lambda\cdot y_{\mathrm{left}}$, where $\lambda>0$ is a factor common for all four groups. $\lambda$ is chosen so that $y_{\mathrm{right}}$ is equal to $1.5$ times the maximal (with respect to the $50$ training epochs) median (with respect to the $30$ runs) value of $\Vert{\Pi_{\calE^\perp}A}\Vert$ for the \augm~model \emph{for the \transs~ experiment}.
\end{itemize}
In this way, we ensure that the coordinate systems are on the same scale relative to the \nominal~and \eqvi~models. It is hence not surprising that the \nominal~ curves look the same in all the plots -- it is very much that way by design. The same is however not true for the \augm~ curve, which seems to depend heavily on the symmetry group used. 

\begin{figure} 
    \center
    \includegraphics[width=.8\textwidth]{figures/MNIST_varying_group.png}
    \caption{Results from the experiments of the appendix. \label{fig:mnist_different groups}}
\end{figure}

To exactly explain this dependence is beyond the scope of this work. However, a simple counting of the dimension of the spaces $\rmT \calE$ in each case seems to give some clue.  In Table \ref{tab:dimcalc_two}, we write down the dimensions of $\Homm_G(U,V)$ for the different $U$ and $V$ appearing in the experiments (the simple but tedious justifications of these numbers are given in the next subsection). Using these numbers, one calculates that the relative dimensions $\rmT \calE /\rmT \calL$ for the different groups approximately equal
\begin{align}%*}
    \begin{tabular}{lllll }
         \transs~: & $5.1 \cdot 10^{-3}$ &  & \rott~: &$0.25$\\
        \onedtranss~: & $7.1\cdot 10^{-2} $ &  & \transrott~:& $ 1.3 \cdot 10^{-3} $
    \end{tabular}.
\end{align}
We see a clear trend -- the smaller the dimension of $\rmT\calE$, the the slower the $\augm$ architecture seems to deviate. This is not surprising -- the closer $\rmT\calE$ is to $\rmT\calL$, the closer the augmented gradients and Hessians are to their non-augmented counterparts. To formally analyse this effect, in particular for more complicated $\calL$ than considered here, is interesting future work.
\begin{table}[b]
    \centering
    \begin{tabular}{| l|c | c | c | c | l | }
    \hline Experiment & $U$ & $V$ & $\dim \Homm(U,V)$ & $\dim \Homm_G(U,V)$ & \# in model  \\
    \hline \rott    & $\R^{N,N}$           & $\R^{N,N}$          & $N^4$ & $\tfrac{1}{4}N^4$ & $32 + 32 \cdot 32$ \\
                    & $\R^{N,N}$           & $\R$                & $N^2$ & $\tfrac{1}{4}N^2$    & $32 \cdot 32$ \\
                    & $\R$                 & $\R$                & 1     & 1   & $32\cdot 10$ \\
    \hline \transs & $\R^{N,N}$ & $\R^{N,N}$ & $N^4$ & $N^2$ & $32 + 32\cdot 32$ \\
                    & $\R^{N,N}$           & $\R$            & $N^2$ & $1$     & $32 \cdot 32$ \\
                        & $\R$                 & $\R$            & 1     & 1   & $32\cdot 10$ \\
    \hline \onedtranss  & $\R^{N,N}$           & $\R^{N,N}$      & $N^4$ & $N^3$ & $32 + 32 \cdot 32$ \\
                        & $\R^{N,N}$           & $\R$            & $N^2$ & $N$     & $32 \cdot 32$ \\
                        & $\R$                 & $\R$            & 1     & 1   & $32\cdot 10$ \\
    \hline \transrott  & $\R^{N,N}$           & $\R^{N,N}$       & $N^4$ & $\tfrac{1}{4}N^2$ & $32 + 32 \cdot 32$ \\
                       & $\R^{N,N}$           & $\R$             & $N^2$ & 1     & $32 + 32 \cdot 32$ \\
                       & $\R$                 & $\R$             & 1     & 1   & $32\cdot 10$ \\
    \hline
    \end{tabular}

    \hspace{.5cm}
    \caption{Dimension calculations for the experiments \rott~, \onedtranss~and \transrott~in the appendix.}
    \label{tab:dimcalc_two}
\end{table}









\subsubsection{The spaces $\Homm_G(U,V)$ for the various groups} \label{sec:dims}
Here, we derive descriptions of $\Homm_G(U,V)$ in the four cases. These derivations are all conceptually easy, but tedious, and are only included for completeness. 


\paragraph{$\Z_N^2$ and \transs~} It is well known that a linear operator $\R^{N,N}\to \R^{N,N}$ is equivariant to circular translations (i.e. the $\transs~$-action) if and only if it is a convolution. Consequently, the dimension of the space $\Homm_{\Z_N^2}(\R^{N,N}, \R^{N,N})$ is $N$. As for $\Homm_{\Z_N^2}(\R^{N,N}, \R)$, notice that we by duality might as well may describe all $X\in\R^{N,N}$ that are not changed by any translation -- and those are clearly exactly the ones which are constant. Hence, $\dim(\Homm_{\Z_N^2}(\R^{N,N},\R)) = 1$.

\paragraph{$\Z_N$ and \onedtranss~} To describe the space $\Hom_{\Z_N}(\R^{N,N},\R^{N,N})$, let us begin by introducing some notation. First, every element in $\R^{N,N}$ can equivalently be described as a collection of rows. This can be written compactly as 
    \begin{align}%*}
        X = \sum_{k \in [N]} e_k x_k^*
    \end{align}%*}
    where $x_k \in \R^N$ are the rows, and $e_k$ is the $k$:th canonical unit vector.
    Correspondingly, each operator $A : \R^{N,N} \to \R^{N,N}$ decomposes into an array of $N^2$ operators $A_{\ell,k}: \R^{N}\to \R^N$:
    \begin{align}
        A(x) = \sum_{k, \ell \in [N]} e_\ell (A_{\ell,k}x_k)^*. \label{eq:decomp}
    \end{align}
    With this notation introduced, we can conveniently describe the space $\Hom_{\Z_N}(\R^{N,N},\R^{N,N})$
\begin{proposition}
    Using the notation \eqref{eq:decomp}, $\Hom_{\Z_N}(\R^{N,N},\R^{N,N}) $ is characterized as the set of operators for which each $A_{\ell,k}$ is convolutional. In particular, the dimension of the space is $N^2\cdot N = N^3$.
\end{proposition}
\begin{proof}
    Somewhat abusing notation, let us denote the action of $\Z_N$ on $\R^N$ also as $\rho^{\tr_0}$. We then have
    \begin{align}%*}
        \rho^{\tr_0}(n)(e_kx^*) = e_k\rho^{\tr_0}(n)(x)^* 
    \end{align}%*}
    Hence,
    \begin{align}%*}
        A(\rho^{tr_0}(n)x) =  \sum_{k, \ell \in [N]} e_\ell (A_{\ell,k}\rho^{\tr_0}(n)x_k)^* \\
        \rho^{\tr_0(n)}(Ax) = \sum_{k, \ell \in [N]}  e_\ell  (\rho^{\tr_0}(n)A_{\ell,k}x_k)^*
    \end{align}%*}
    These two expressions are equal for any $x$ if and only if $A_{\ell,k}\rho^{\tr_0}(n)x_k =\rho^{\tr_0}(n)A_{\ell,k}x_k$  for any $x_k$ and $\ell,k$, i.e., when every $A_{\ell,k}$ is translation equivariant, which is equivalent to each $A_{\ell,k}$ being a convolution operator.
\end{proof}

\newcommand{\trot}{\mathrm{trot}}
As before, it is not hard to realize what the space $\Hom_{\Z^N}(\R^{N,N},1)$ must be: Interpreted as a matrix $X = \sum_{k\in [N]}e_kx_k^*$, $X$ is in $\Hom_{\Z^N}(\R^{N,N},1)$ if and only if each $x_k$ is constant. Correspondingly, the projection is given by taking means along the $x$-direction, and the dimension of the space in particular is $N$.
    % describe setup
    \paragraph{$\Z_4$ and \rott~} 
    A map $K$ in $\Homm(\R^{N,N},\R^{N,N})$ can be described by an array $A[i,j]$ of real numbers, $i,j \in [N]^2$. The lifted representation $\overline{\rho}^\rot$ acts on this matrix as follows:
    \begin{align*}
        (\overline{\rho}^\rot(k)A)[i,j] = A[\omega^k i, \omega^k j], \quad k \in \Z_4.
    \end{align*}
    This makes it clear that a map is in $\Homm_{\Z_4}(\R^{N,N},\R^{N,N})$ if an only if $A$ is constant on each orbit $\{ (\omega^k i,\omega^k j) \, \vert \, i,j \in [N]^2, k \in \Z_4\}$. When $N$ is even, all such orbits contain $4$ elements, which implies that $\dim \Homm_{\Z_4}(\R^{N,N},\R^{N,N})= \dim \Homm(\R^{N,N},\R^{N,N}) / 4 = N^4/4$. We may apply exactly the same argument to argue that $\Homm_{\Z_4}(\R^{N,N},\R) = \dim \Homm(\R^{N,N},\R)/4 = N^2/4$. 

    \paragraph{$\Z_{N}^2 \rtimes \Z_4$ and \transrott~} It is clear that an element is invariant to the action $\Z_{N}^2\rtimes \Z_4$, if and only if it is invariant to the subgroups $\Z_{N}^2$ and $\Z_4$. This immediately implies that, using the notation of Example \ref{ex:skewvscross}
    \begin{align*}
        \Homm_{\Z_{N}^2 \rtimes \Z_4}(\R^{N,N}, \R^{N,N}) &\sse  \Homm_{\Z_{N}^2}(\R^{N,N}, \R^{N,N}) = \calC \\
        \Homm_{\Z_{N}^2 \rtimes \Z_4}(\R^{N,N}, \R) &\sse  \Homm_{\Z_{N}^2}(\R^{N,N}, \R) = \mathrm{span} \, \mathds{1},
    \end{align*}
    i.e. that linear maps must be convolutions, and that linear forms must be constant. The constant form is clearly rotation invariant, so that $ \Homm_{\Z_{N}^2 \rtimes \Z_4}(\R^{N,N}, \R)$ indeed equals $\mathrm{span} \, \mathds{1}$. As for the rotation invariant convolutions, we now from the discussion in Example \ref{ex:skewvscross} that the rotation acts directly on the convolutional filter. This filter is hence constant if and only if it is constant on orbits of the rotation group, of which there (for even $N$) exists exactly $N^2/4$. Hence, $\Homm_{\Z_{N}^2 \rtimes \Z_4}(\R^{N,N}, \R^{N,N}) = N^2/4$, as claimed.
    
\end{document}