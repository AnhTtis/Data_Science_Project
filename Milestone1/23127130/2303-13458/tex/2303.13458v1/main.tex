\documentclass{article}
% if you need to pass options to natbib, use, e.g.:
     %\PassOptionsToPackage{numbers, compress}{natbib}
     \PassOptionsToPackage{numbers}{natbib}
% before loading neurips_2022


% ready for submission
% \usepackage{neurips_2022}
% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2022}
% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{neurips_2022}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% OUR PACKAGES %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{dsfont}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% OUR MACROS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\aug}{\mathrm{aug}}
\newcommand{\FA}{\mathrm{FA}}
\newcommand{\End}{\mathrm{End}}
\newcommand{\Hom}{\mathrm{Hom}}
\newcommand{\erw}{\mathbb{E}} % Av gammal vana använder jag(Axel) macrot \erw ('Erwartungswert') för väntevärde
\newcommand{\id}{\mathrm{id}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\res}{\mathrm{res}}
\newcommand{\eqv}{\mathrm{eqv}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\tensortwo}{{\otimes 2}}
\newcommand{\otensor}{\otimes}
\newcommand{\hess}{\nabla^2}
\newcommand{\spec}{\mathrm{spec}}
\newcommand{\indrho}{\overline{\rho}}

\newcommand{\perm}{\mathrm{perm}}
\newcommand{\trans}{\mathrm{tr}}
\newcommand{\rot}{\mathrm{rot}}

\newcommand{\calD}{\mathcal{D}}
\newcommand{\calL}{\mathcal{L}}
\newcommand{\calI}{\mathcal{I}}
\newcommand{\calE}{\mathcal{E}}
\newcommand{\sprod}[1]{\langle #1 \rangle}

\newcommand{\K}{\mathbb{K}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}

\newtheorem{example}{Example}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem{prop}{Proposition}


\title{Optimization Dynamics of Equivariant and Augmented Neural Networks}

\author{
Axel Flinth\\Department of Mathematics\\and Mathematical Statistics\\Umeå University\\Umeå, SE-901 87\\ \texttt{axel.flinth@umu.se}
\And
Fredrik Ohlsson\\Department of Mathematics\\and Mathematical Statistics\\Umeå University\\Umeå, SE-901 87\\ \texttt{fredrik.ohlsson@umu.se}
}

\begin{document}

\maketitle

\begin{abstract}
We investigate the optimization of multilayer perceptrons on symmetric data. We compare the strategy of constraining the architecture to be equivariant to that of using augmentation. We show that, under natural assumptions on the loss and non-linearities, the sets of equivariant stationary points are identical for the two strategies, and that the set of equivariant layers is invariant under the gradient flow for augmented models. Finally, we show that stationary points may be unstable for augmented training although they are stable for the equivariant models.
\end{abstract}

\section{Introduction}
In machine learning, the general goal is to find 'hidden' patterns in data. However, there are sometimes symmetries in the data that are known a priori. Incorporating these manually should, heuristically, reduce the complexity of the learning task. In this paper, we are concerned with training networks, more specifically multilayer perceptrons (MLPs) as defined below, on data exhibiting symmetries that can be formulated as equivariance under a group action. A standard example is the case of translation invariance in image classification.

More specifically, we want to theoretically study the connections between two general approaches to incorporating symmetries in MLPs. The first approach is to construct equivariant models by means of \emph{architectural design}. This framework, known as \emph{Geometric Deep Learning}~\cite{bronstein2017geometric, bronstein2021geometric}, exploits the geometric origin of the group $G$ of symmetry transformations by choosing the linear MLP layers and nonlinearities to be equivariant (or invariant) with respect to the action of $G$. In other words, the symmetry transformations commute with each linear (or affine) map in the network, which results in an architecture which manifestly respects the symmetry $G$ of the problem. One prominent example is the spatial weight sharing of convolutional neural networks (CNNs) which are equivariant under translations. Group equivariant convolution networks (GCNNs) \cite{cohen2016group, weiler2018learning,kondorGeneralizationEquivarianceConvolution2018} extends this principle to an exact equivariance under more general symmetry groups.  The second approach is agnostic to model architecture, and instead attempts to achieve equivariance during training via \emph{data augmentation}, which refers to the process of extending the training to include synthetic samples obtained by subjecting training data to random symmetry transformations. 

Both approaches have their benefits and drawbacks. Equivariant models use parameters efficiently through weight sharing along the orbits of the symmetry group, but are difficult to implement and computationally expensive to evaluate in general. Data augmentation, on the other hand, is agnostic to the model structure and easy to adapt to different symmetry groups. However, the augmentation strategy is by no means guaranteed to achieve a model which is exactly invariant: the hope is that model will 'automatically' infer invariances from the data, but there are few theoretical guarantees. Also, augmentation generalentails an inefficient use of parameters and an increase in model complexity and training required. 

In this paper we use representation theory to compare and analyze the dynamics of gradient descent training of MLPs adhering to either the equivariant or augmented strategy. In particular, we study their equivariant stationary points, i.e., the potential limit points of the training. The equivariant models (of course) have equivariant optima for their gradient flow and so are guaranteed to produce a trained model which respects the symmetry. However, the dynamics of linear networks under augmented training has previously not been exhaustively explored.

Our main contributions are as follows:  First, we provide a technical statement (Lemma \ref{lem:ind_rep_aug_risk}) about the augmented risk: We show that it can be expressed as the nominal risk averaged over the symmetry group acting on the layers of the MLP. Using this formulation of the augmented risk, we apply standard methods from the analysis of dynamical systems to the gradient descent for the equivariant and augmented models to show:
\begin{enumerate}
    \item The equivariant subspace is invariant under the gradient flow of the augmented model (Corollary \ref{cor:eqv_subs_invariant}). In other words, if the augmented model is equivariantly initialized it will remain equivariant during training.
    \item The set of equivariant stationary points for the augmented model is identical to that of the equivariant model (Corollary \ref{cor:stationary_points}). In other words, compared to the equivariant approach, augmentation introduces no new equivariant stationary points, nor does it exclude existing ones.
    \item The set of equivariant strict local minima for the augmented model is a subset of the corresponding set for the equivariant model. (Proposition \ref{prop:minima_E}). In other words, the existence of a stable equivariant minimum is not guaranteed by augmentation.
\end{enumerate}
In addition, we perform experiments on three different learning tasks, with different symmetry groups, and discuss the results in the context of our theoretical developments.

\section{Related work}
The group-theory based model for group augmentation we use here is heavily inspired by the framework developed in \cite{chen2020group}. Augmentation and manifest invariance/equivariance have been studied from this perspective in a number of papers \cite{lyle2019analysis,lyle2020benefits, mei2021learning,elesedy2021provably}. More general models for data-augmentation have also been considered \cite{dao2019kernel}. Previous work has mostly mostly been concerned with so-called kernel and \emph{feature-averaged} models (see Remark \ref{rem:fa} below), and in particular, fully general MLPs as we treat them here have not been considered. The works have furthermore mostly been concerned with proving statistical properties of the models, and not study their dynamic at training. An exception is~\cite{lyle2020benefits}, in which it is proven that in linear scenarios, the equivariant models are optimal, but little is known about more involved models.

In \cite{lawrence2021implicit}, the dynamics of training \emph{linear equivariant networks} (i.e., MLPs without nonlinearities) is studied. The linear networks is a simplified, but nonetheless popular theoretic model for analysing neural networks~\cite{bah2022learning}. In the paper, the authors analyse the implicit bias of training a linear neural network with one fully connected layer on top of an equivariant backbone using gradient descent. They also provide some numerical results for non-linear models, but no comparison to data augmentation is made. 

Empirical comparisons of training equivariant and augmented non-equivariant models are common in the in the literature. Most often, the augmented models are considered as baselines for the evaluation of the equivariant models. More systemic investigations include \cite{gandikotaTrainingArchitectureHow2021,muller2021,gerken22a}. Compared to previous work, our formulation differs in that the parameter of the augmented and equivariant models are defined on the same vector spaces, which allows us to make a stringent mathematical comparison.

\section{Mathematical framework}
\label{sec:opt_sym_models}
Let us begin by setting up the framework. We let $X$ and $Y$ be vector spaces and $\calD(x,y)$ be a joint distribution on $X \times Y$. We are concerned with training an MLP $\Phi_A : X \to Y$ so that $y \approx f(x)$. The MLP has the form
\begin{align} \label{def:MLP}
    x_0 = x, \quad x_{i+1} = \sigma_i(A_ix_i), \quad i\in [L] = \{0, \dots, L-1\}, \quad  \Phi_A(x) = x_{L},
\end{align}
where $A_i: X_i \to X_{i+1}$ are linear maps (layers) between (hidden) vector spaces $X_i$ with $X=X_0$ and $Y=X_L$, and $\sigma_i : X_{i+1}\to X_{i+1}$ are non-linearities. Note that $A=(A_i)_{i \in [L]}$ parameterizes the network since the non-linearities are assumed to be fixed. We denote the vector space of possible linear layers with 
$\mathcal{L} = \prod_{i \in [L]} \Hom(X_i,X_{i+1})$, where $\prod$ refers to the direct product. To train the MLP, we optimize the \emph{nominal risk}
\begin{align*}
    R(A) = \erw_{\calD}(\ell(\Phi_A(x),y)),
\end{align*}
where $\ell: Y \times Y \to \R$ is a loss function, using gradient descent. In fact, to simplify the analysis, we will mainly study the \emph{gradient flow} of the model, i.e., set the learning rate to be infinitesimal.\footnote{Note that in practice training will probably involve an empirical risk and/or stochastic gradient descent, but the focus of our theoretical development is this idealized model.}

\subsection{Representation theory and equivariance}Throughout the paper, we aim to make the MLP \emph{equivariant} towards a group of symmetry transformations of the space $X \times Y$. That is, we consider a group $G$ acting on the vector spaces $X$ and $Y$ through \emph{representations} $\rho_X$ and $\rho_Y$, respectively. A representation $\rho$ of a group $G$ on a vector space $V$ is a map from the group $G$ to the space of invertible linear maps $\End(V)$ on $V$ that respects the group operation, i.e. $\rho(gh)=\rho(g)\rho(h)$ for all $g,h \in G$. The representation $\rho$ is unitary if $\rho(g)$ are unitary for all $g \in G$. A function $f: X\to Y$ is called equivariant with respect to $G$ if $f\circ \rho_X(g)  = \rho_Y(g) \circ f$ for all $g\in G$. We denote the space of equivariant \emph{linear} maps $U \to V$ by $\Hom_G(U,V)$.

Let us recall some important examples that will be used throughout the paper.
\begin{example}
    A simple, but important, representation is the trivial one, $\rho^{\mathrm{triv}}(g)=\id$ for all $g\in G$. If we equip $Y$ with the trivial representation, the equivariant functions $f:X\to Y$ are the invariant ones.
\end{example}

\begin{example}
 \label{ex:perm}
    The canonical action of the permutation group $S_N$ on $\R^N$ is defined through 
    \begin{align*}
        [\rho^{\perm}(\pi)v]_i = v_{\pi^{-1}(i)}, i \in [n],
    \end{align*}
    i.e., an element acts via permuting the elements of a vector. This action induces an action on the tensor space $(\R^N)^{\otimes k} = \R^N \otimes \R^N \otimes \dots \otimes \R^N$:
    \begin{align*}
        [\rho^\perm(\pi)T]_{i_0, \dots, i_{k-1}} = T_{\pi^{-1}(i_0), \dots, \pi^{-1}(i_{k-1})}.
    \end{align*}
    which is important for graphs. For instance, when applied to $\R^N \otimes \R^N$, it encodes the effect a re-ordering of the nodes of a graph has on the adjacency matrix on the graph.
\end{example}

\begin{example}
 \label{ex:trans}
    The group $\Z_N^2$ acts through translations on images $x \in \R^{N,N}$:
    \begin{align*}
        (\rho^\trans(k,\ell)x)_{i,j} = x_{i-k,j-\ell}
    \end{align*}
\end{example}

\begin{example}
\label{ex:rot}
    The group $C_4 \cong \Z_4$ acts on images $x\in \R^{N,N}$ through rotations by multiples of $90^\circ$. That is, $\rho(k)= \rho(1)^k$, $k=0,1,2,3$ where
    \begin{align*}
        (\rho^\rot(1)x)_{i,j} = x_{-j,i}.
    \end{align*}
\end{example}

\subsection{Training equivariant models}
In order to obtain a model $\Phi_A$ which respects the symmetries of a group $G$ acting on $X \times Y$, we should incorporate them in our model or training strategy. Note that the distribution $\calD(x,y)$ of training data is typically not symmetric in the sense $(x,y) \sim (\rho_X(g)x, \rho_Y(g)y)$. Instead, in the context we consider, symmetries are usually inferred from, e.g., domain knowledge of $X \times Y$.  

We now proceed to formally describe two (popular) strategies for training MLPs which respect equivariance under $G$.

\paragraph{Strategy 1: Manifest equivariance} The first method of enforcing equivariance is to constrain the layers to be manifestly equivariant. That is, we assume that $G$ is acting also on all hidden spaces $X_i$ through representations $\rho_i$, where $\rho_0 = \rho_X$ and $\rho_L=\rho_Y$, and constrain each layer $L_i$ to be equivariant. In other words, we choose the layers $L \in \calL$ in the \emph{equivariant subspace}
\begin{align*}
    \calE = \prod_{i \in [L]}\Hom_G(X_i,X_{i+1})
\end{align*}
If we in addition assume that all non-linearities $\sigma_i$ are equivariant, it is straight-forward to show that $\Phi_A$ is \emph{exactly} equivariant under $G$ (see also Lemma \ref{lem:ind_rep_network}). We will refer to these models as \emph{equivariant}. The set $\calE$ has been extensively studied in the setting of geometric deep learning and explicitly characterized in many important cases \cite{ maron2018invariant, cohen2018generalGCNN, kondorGeneralizationEquivarianceConvolution2018, weiler2019general, maron2019universality,  aronsson2022homogenous}. In \cite{finzi2021practical}, a general method for determining $\mathcal{E}$ numerically directly from the $\rho_i$ and the structure of the group $G$ is described.

Defining $\Pi_\calE: \calL \to \calE$ as the orthogonal projection onto $\calE$, a convenient formulation of the strategy, which is the one we will use, is to optimize the \emph{equivariant risk}
\begin{align}
\label{eq:eqvrisk}
    R^{\eqv}(A) = R(\Pi_\calE A).
\end{align}

\paragraph{Strategy 2: Data augmentation} The second method we consider is to augment the training data. To this end, we define a new distribution on $X\times Y$ by drawing samples $(x,y)$ from $\calD$ and subsequently \emph{augmenting} them by applying the action of a randomly drawn group element $g \in G$ on both data $x$ and label $y$. Training on this augmented distribution can be formulated as optimizing the \emph{augmented risk}
\begin{align*}
    R^\aug(A) = \int_G\erw_{\calD}(\ell(\Phi_A(\rho_X(g)x),\rho_Y(g)y)) \, \dd \mu(g) 
\end{align*}
Here, $\mu$ is the (normalised) \emph{Haar} measure on the group~\cite{krantz2008geometric}, which is defined through its invariance with respect to the action of $G$ on itself; if $h$ is distributed according to $\mu$ then so is $gh$ for all $g\in G$. This property of the Haar measure will be crucial in our analysis. Choosing another measure would cause the augmentation to be biased towards certain group elements, and is not considered here.  Note that if the data $\calD$ already is symmetric in the sense that $(x,y) \sim (\rho_X(g)x, \rho_Y(g)y)$, the augmentation acts trivially.

In our analysis, we want to compare the two strategies when training the same model. We will make three global assumptions.

\begin{assumption} The group $G$ is acting on all hidden spaces $X_i$ through unitary representations $\rho_i$. \label{ass:unitary_reps}
\end{assumption}
\begin{assumption} The non-linearities $\sigma_i : X_{i+1} \to X_{i+1}$ are equivariant. \label{ass:equiv_nonlin}
\end{assumption}
\begin{assumption}
    The loss $\ell$ is invariant, i.e. $\ell(\rho_Y(g)y,\rho_Y(g)y') = \ell(y,y')$, $y,y' \in Y$, $g\in G$. \label{ass:equiv_loss}
\end{assumption}

Let us briefly comment on these assumptions. The first assumption is needed for the restriction strategy to be well defined. The technical part -- the unitarity -- is not a true restriction:  As long as all $X_i$ are finite-dimensional and $G$ is compact, we can redefine the inner products on $X_i$ to ensure that all $\rho_i$ become unitary . The second assumption is required for the equivariant strategy to be sound -- if the $\sigma_i$ are not equivariant, they will explicitly break equivariance of $\Phi_A$ even if $A \in \calE$. The third assumption guarantees that the loss-landscape is 'unbiased' towards group transformations, which is certainly required to train any model respecting the symmetry group. %\textcolor{red}{can we motivate this better?}

We also note that all assumptions are in many settings quite weak -- we already commented on Assumption \ref{ass:unitary_reps}. As for Assumption \ref{ass:equiv_nonlin},  note that e.g. any non-linearity acting pixel-wise on an image will be equivariant to both translations and rotations. In the same way,  any loss comparing images pixel by pixel will be, so that Assumption \ref{ass:equiv_loss} is satisfied. Furthermore, if we are trying to learn an invariant function the final representation $\rho_Y$ is trivial and Assumption \ref{ass:equiv_loss} is trivially satisfied.

\begin{remark} \label{rem:fa}
   Before proceeding, let us mention a different strategy to build equivariant models: \emph{feature averaging} \cite{lyle2019analysis}. This strategy refers to altering the model by averaging it over the group:
    \begin{align*}
        \Phi^{\FA}_A(x)  := \int_{G} \rho_Y(g)^{-1}\Phi_A(\rho_X(g)x)\, \dd\mu(g).
    \end{align*}
    In words, the value of the feature averaged network at a datapoint $x$ is obtained by calculating the outputs of the original model $\Phi_A$ on transformed versions of $x$, and averaging the re-adjusted outputs over $G$. Note that the modification of an MLP here does not rely on explicitly controlling the weights. It is not hard to prove (see e.g. \cite[Prop. 2]{lyle2020benefits}) that under the invariance assumption on $\ell$, 
    \begin{align}
        R^\aug(A) =  \erw(\ell(\Phi^\FA_A(x),y)).  \label{eq:agrisk}
    \end{align}
\end{remark}

\subsection{Induced representations and their properties}The representations $\rho_i$ naturally induce representations $\overline{\rho}_i$ of $G$ on $\Hom(X_i,X_{i+1})$
\begin{align*}
    \overline{\rho}_i(g) A_i = \rho_{i+1}(g) A_i \rho_i(g)^{-1},
\end{align*}
and from that a representation $\overline{\rho}$ on $\calL$ according to $(\overline{\rho}(g)A)_i = \overline{\rho}_i(g)A_i$. Since the $\rho_i$ are unitary, with respect to the appropriate canonical inner products, so are $\overline{\rho}_i$ and $\overline{\rho}$. 

Before proceeding we establish some simple, but crucial facts, concerning the induced representation $\overline{\rho}$ and the way it appears in the general framework. We will need the following to well-known lemmas. Proofs are presented in Appendix \ref{app:proofs}.
\begin{lemma}
\label{lem:ind_rep_layers}
    $A \in \calE$ if and only if $\,\overline{\rho}(g) A = A$ for all $g\in G$.
\end{lemma}
\begin{lemma}
\label{lem:ind_rep_orth_proj}
    For any $A \in \calL$ the orthogonal projection $\Pi_\calE$ is given by
    \begin{align*}
        \Pi_\calE A = \int_{G} \overline{\rho}(g)A \, \dd \mu(g).
    \end{align*}
\end{lemma}

We now prove a relation between transforming the input and transforming the layers of an MLP.
\begin{lemma}
\label{lem:ind_rep_network}
    Under Assumption \ref{ass:equiv_nonlin}, for any $A \in \calL$ and $g \in G$ we have
    \begin{align*}
        \Phi_A(\rho_X(g) x) = \rho_Y(g)\Phi_{\overline{\rho}(g)^{-1}A}(x).
    \end{align*}
    In particular, $\Phi_A$ is equivariant for every $A \in \calE$.
\end{lemma}

\begin{proof}
    The in particular part follows from $\overline{\rho}(g)^{-1}A=A$ for $A \in \calE$. To prove the main statement,  we use the notation \eqref{def:MLP}:  $x_i$ denotes  the outputs of each layer of $\Phi_A$ when it acts on the input $x \in X$. Also, for $g \in G$, let $x_i^g$ denote the outputs of each layer of the network $\Phi_{\overline{\rho}(g)^{-1}A}$ when acting on the input $\rho_X(g)^{-1}x$. If we show that $\rho_i(g) x_i^g = x_i$ for $i=[L+1]$, the claim follows. We do so via induction. The case $i=0$ is clear:
        $\rho_{X}(g) x_0^g = \rho_X(g) \rho_X(g)^{-1}x = x =x_0$. As for the induction step, we have
    \begin{align*}
        \rho_{i+1}(g) x_{i+1}^g &= \rho_{i+1}(g) \sigma_i(\overline{\rho}_i(g)^{-1}A_i x_{i}^g) = \rho_{i+1}(g) \sigma_i(\rho_{i+1}(g)^{-1}A_i \rho_i(g) x_{i}^g) \\&= \sigma_i(\rho_{i+1}(g) \rho_{i+1}(g)^{-1}A_i\rho_i(g) x_{i}^g)  = \sigma_i(A_i\rho_i(g) x_i^g) = \sigma_i(A x_{i}) = x_{i+1} \,,
    \end{align*}
    where in the second step, we have used the definition of $\overline{\rho}_i$, in the third, Assumption \eqref{ass:equiv_nonlin}, and the fifth step follows from the induction assumption. 
\end{proof}
The above formula has an immediate consequence for the augmented loss.
\begin{lemma}
\label{lem:ind_rep_aug_risk}
    Under Assumptions \ref{ass:equiv_nonlin} and \ref{ass:equiv_loss}, the augmented risk can be expressed as
    \begin{align} \label{eq:augrisk}
        R^\aug(A) = \int_{G}R(\overline{\rho}(g)A) \, \dd \mu(g).
    \end{align}
    %In particular, $R^\aug$ is invariant under $\overline{\rho}$.
\end{lemma}
\begin{proof}
    From Lemma \ref{lem:ind_rep_network} and Assumption \eqref{ass:equiv_loss}, it follows that for any $g \in G$ we have $
        \ell(\Phi_{\overline{\rho}(g)^{-1}A}(x), y) = \ell( \rho_Y(g)^{-1}\Phi_A(\rho_X(g)x),y) =  \ell(\Phi_A(\rho_X(g)x), \rho_Y(g)y)$.
    Taking the expectation with respect to the distribution $\calD$, and then integrating over $g \in G$ yields
    \begin{align*}
        R^{\aug}(A) = \int_{G}R(\overline{\rho}(g)^{-1}A) \, \dd \mu(g) = \int_{G}R(\overline{\rho}(g^{-1})A) \, \dd \mu(g) \,.
    \end{align*}
    Using the fact that the Haar measure is invariant under inversion proves the statement.
\end{proof}
We note the likeness of \eqref{eq:augrisk} to \eqref{eq:agrisk}: In both equations, we are averaging risks of transformed models over the group. However, in \eqref{eq:agrisk}, we average over transformations of the \emph{input data}, whereas in \eqref{eq:augrisk}, we average over transformations of the \emph{layers}. The latter fact is crucial -- it will allow us analyse the dynamics of gradient flow.

Before moving on, let us introduce one more notion. When considering the dynamics of training we will encounter elements of the vector space $\calL \otimes \calL$, which also carries a representation $\overline{\rho}^\tensortwo$ of $G$ induced by $\overline{\rho}$ according to    
\begin{align*}
    \overline{\rho}^\tensortwo(g)(A\otimes B) = (\overline{\rho}(g)A) \otimes (\overline{\rho}(g)B)\,.
\end{align*}
We refer to the vector space of elements invariant under this action as $\calE^\tensortwo$, and the orthogonal projection onto it as $\Pi_{\calE^\tensortwo}$. As for $\calL$, the induced representation on $\calL \otimes \calL$ can be used to express the orthogonal projection.
\begin{lemma}
\label{lem:ind_rep_orth_proj_L2}
    For any $A,B \in \calL$ the orthogonal projection $\Pi_{\calE^\tensortwo}$ is given by
    \begin{align*}
        \Pi_{\calE^\tensortwo}(A \otimes B) = \int_{G} \overline{\rho}^\tensortwo(g)(A \otimes B) \, \dd \mu(g).
    \end{align*}
\end{lemma}

An important property of the space $\calE^\tensortwo$, which we can describe as a subspace of the space of bilinear forms on $\calL$,  is that it is diagonal with respect to the orthogonal decomposition $\calL = \calE \oplus \calE^{\perp}$ induced by $\Pi_{\calE}$. We have
\begin{lemma}
\label{lem:E2_diagonal}
    For any $M \in \calL \otimes \calL$, $A \in \calE$ and $B \in \calE^{\perp}$ we have $$\text{(i) }\left(\Pi_{\calE^\tensortwo}M\right)[A,A] = M[A,A]\text{ and } \text{(ii) }\left( \Pi_{\calE^\tensortwo} M \right) [A,B] = 0 \, . $$
\end{lemma}

\section{Dynamics of the gradient flow} %descent}
We have now established the framework of optimization for symmetric models that we need to investigate the gradient descent for the equivariant and augmented models. In particular, we want to compare the gradient descent dynamics of the two models as it pertains to the equivariance with respect to the symmetry group $G$ during training. To this end, we consider the gradient flows of the nominal, equivariant and augmented risks
\begin{equation*}
    \dot{A} = -\nabla R(A) \,, \quad \dot{A} = -\nabla R^{\eqv}(A) \,, \quad \dot{A} = -\nabla R^{\aug}(A) \,, \quad A \in \calL\,.
\end{equation*}

\subsection{Equivariant stationary points}
We first establish the relation between the gradients of the equivariant and augmented models for an equivariant initial condition.  
\begin{prop}
\label{prop:grad_E}
    For $A \in \calE$ we have $\nabla R^\aug(A) = \Pi_\calE \nabla R(A) = \nabla R^{\eqv}(A)$.
\end{prop}
\begin{proof}
    Taking the derivative of \eqref{eq:augrisk} the chain rule yields
    \begin{align*}
        \sprod{\nabla R^\aug(A),B} = \int_{G} \sprod{\overline{\rho}(g)^{-1}\nabla R(\overline{\rho}(g)A),B} \, \dd \mu(g) = \int_{G} \sprod{\nabla R(\overline{\rho}(g)A), \overline{\rho}(g)B} \, \dd \mu(g) \,,
    \end{align*}
    where $B\in \calL$ is arbitrary and we have used the unitarity of $\overline{\rho}$. Using that $\overline{\rho}(g)A=A$ for every $A \in \calE$ (Lemma \ref{lem:ind_rep_orth_proj}), we see that the last integral equals
    \begin{align*}
        \int_{G} \sprod{\nabla R(A), \overline{\rho}(g)B} \, \dd \mu(g) = \sprod{\nabla R(A), \Pi_\calE B} = \sprod{\Pi_\calE \nabla R(A), B}\,,
    \end{align*}
    where we have used orthogonality of $\Pi_{\calE}$ in the final step, which establishes the first equality of the proposition.
    
    The second equality follows immediately from the chain rule applied to \eqref{eq:eqvrisk}
    \begin{align*}
        \nabla R^\eqv(A) = \Pi_\calE \nabla R(\Pi_\calE A) = \Pi_\calE \nabla R(A)\,,
    \end{align*}
    where we have used that $\Pi_{\calE}$ is self-adjoint and $\Pi_\calE A = A$ for every $A \in \calE$ in the last step.
\end{proof} 

A direct consequence of Proposition \ref{prop:grad_E} is that for any $A \in \calE$ we have $\nabla R^{\aug}(A) \in \calE$ for the gradient, which establishes the following important result.
\begin{cor}
\label{cor:eqv_subs_invariant}
    The equivariant subspace $\calE$ is invariant under the gradient flow of $R^{\aug}$.
\end{cor}
%Of course, $\calE$ is trivially invariant under the gradient flow of the equivariant risk $R^{\eqv}$.

A further immediate consequence of Proposition \ref{prop:grad_E} is the fact that if the initialization of the networks is equivariant, the gradient descent dynamics of the augmented and equivariant models will be identical. In particular, we have the following result.
\begin{cor}
\label{cor:stationary_points}
    $A^* \in \calE$ is a stationary point of the gradient flow of $R^{\aug}$ if and only if it is a stationary point of the gradient flow of $R^{\eqv}$.
\end{cor}

\subsection{Stability of the equivariant stationary points}
We now consider the stability of equivariant stationary points, and more generally of the equivariant subspace $\calE$, under the augmented gradient flow of $R^{\aug}$. Of course, $\calE$ is manifestly stable under the equivariant gradient flow of $R^{\eqv}$. To make statements about the stability we establish the connection between the Hessians of $R$, $R^\eqv$ and $R^\aug$, which can be considered as bilinear forms on $\calL$, i.e. as elements of the tensor product space $\calL \otimes \calL$. 

\begin{prop}
\label{prop:hess_E}
    For $A \in \calE$ we have $\hess R^\aug (A) = \Pi_{\calE^\tensortwo}\hess R(A)$ and $\hess R^\eqv(A) =  \Pi_{\calE}^\tensortwo \hess R(A)$.
\end{prop}
\begin{proof}
    Taking the second derivative of \eqref{eq:augrisk} yields
    \begin{align*}
        \hess R^\aug(A)[B,C] = \int_G \hess R(\overline{\rho}(g)A)[\indrho(g)B, \indrho(g)C] \, \dd \mu(g) = \int_G \indrho^\tensortwo(g)\hess R(A)[B, C] \, \dd \mu(g) \,,
    \end{align*}
    where $B,C\in\calL$ are arbitrary and we have used $\indrho(g)A=A$ for $g\in G$ and $A\in \calE$ and the definition of $\indrho^\tensortwo$. Lemma \ref{lem:ind_rep_orth_proj_L2} then yields the first equality.
    
    The second statement again follows directly from the chain rule twice applied to \eqref{eq:eqvrisk}
    \begin{align*}
        \hess R^\eqv(A) = \Pi_{\calE}^\tensortwo \hess R(\Pi_\calE A) = \Pi_{\calE}^\tensortwo \hess R(A) \,,
    \end{align*}
    where we have used $\Pi_\calE A = A$ for $A \in \calE$ and the fact that $\Pi_\calE$ is self-adjoint.
\end{proof}

\begin{prop}
\label{prop:minima_E}
    For $A^* \in \calE$ the following implications hold:
    \begin{itemize}
    \item[i)] If $A^*$ is a strict local minimum of $R$, it is a strict local minimum of $R^{\aug}$. 
    \item[ii)] If $A^*$ is a strict local minimum of $R^{\aug}$, it is a strict local minimum of $R^{\eqv}$.
    \end{itemize}
\end{prop}
\begin{proof}
    \emph{i)} Assume $\nabla R(A^*) = 0$ and $\nabla^2R(A^*)$ positive definite. From Proposition \ref{prop:grad_E} we then have $\nabla R^{\aug}(A^*) = \Pi_{\calE}\nabla R(A^*) = 0$. Furthermore, for $B \neq 0$ Proposition \ref{prop:hess_E} implies
    \begin{align*}
       \nabla^2 R^{\aug}(A^*) [B,B] = \Pi_{\calE^\tensortwo} \nabla^2 R(A^*)[B,B] = \int_{G} \nabla^2 R(A^*)[\indrho(g)B, \indrho(g)B] \, \dd \mu(g) > 0 \,,
    \end{align*}
    where in the last step we have used the fact that the integrand is positive since $\indrho(g)B \neq 0$ for any $g \in G$ and $B \neq 0$, and $\nabla^2R(A^*)[B,B] > 0$ for $B \neq 0$.
    
    \emph{ii)} Assume $\nabla R^{\aug}(A^*) = 0$ and $\nabla^2R^{\aug}(A^*)$ positive definite. From Proposition \ref{prop:grad_E} we have $\nabla R^{\eqv}(A^*) = \nabla R^{\aug}(A^*) = 0$. Proposition \ref{prop:hess_E} implies that $\nabla^2 R^{\eqv}(A^*) [B,B]$ equals
    \begin{align*}
         \nabla^2 R(A^*)[\Pi_{\calE}B,\Pi_{\calE}B] = \Pi_{\calE^\tensortwo} \nabla^2 R(A^*)[\Pi_{\calE}B, \Pi_{\calE}B]  = \nabla^2 R^{\aug}(A^*) [\Pi_{\calE}B, \Pi_{\calE}B]\,.
    \end{align*}
 we used that $\Pi_\calE B \in \calE$, together with the first part of Lemma \ref{lem:E2_diagonal}.  Consequently, $\nabla^2 R^{\eqv}(A^*)[B,B] > 0$ for $\Pi_{\calE} B \neq 0$ which completes the proof.
\end{proof}

The converse of Proposition \ref{prop:minima_E} is not true. A a concrete counterexample is provided in Appendix \ref{app:counterexample}.

Finally, let us remark an interesting property of the dynamics of the augmented gradient flow of $R^{\aug}$ near $\calE$. Decomposing $A$ near $\calE$ as $A = x + y$, with $x \in \calE$ and $y \in \calE^{\perp}$, with $y \approx 0$, and linearising in the deviation $y$ from the equivariant subspace $\calE$ yields
\begin{align*}
    \dot{x} + \dot{y} = - \nabla R^{\aug}(x) - y^*\nabla^2 R^{\aug}(x)\, + \mathcal{O}(\Vert{y}\Vert^2).
\end{align*}
From Proposition \ref{prop:grad_E} we have $\nabla R^{\aug}(x) \in \calE$, and Proposition \ref{prop:hess_E} (ii) together with Lemma \ref{lem:E2_diagonal} implies that $y^* \nabla^2 R^{\aug}(x) \in \calE^{\perp}$. Consequently, the dynamics approximately  decouple:
\begin{align*}
    \begin{cases}
        \dot{x} &= - \nabla R^{\aug}(x)  \quad \ \ \, +  \mathcal{O}(\Vert{y}\Vert^2)\\
        \dot{y} &= - y^*\nabla^2 R^{\aug}(x)  \ + \mathcal{O}(\Vert{y}\Vert^2)
    \end{cases} \,.
\end{align*}

We observe that Proposition \ref{prop:grad_E} now implies that the dynamics restricted to $\calE$ is identical to that of the equivariant gradient flow, and that the stability of $\mathcal{E}$ is completely determined by the spectrum of $\nabla^2R^{\aug}$ restricted to $\calE^\perp$. Furthermore, as long the parameters are close to $\calE$, the dynamics of the part of $A$ in $\mathcal{E}$ for the two models are almost equal.

In terms of training augmented models with equivariant initialization, these observations imply that while the augmented gradient flow restricted to $\calE$ will converge to the local minima of the corresponding equivariant model, it may diverge from the equivariant subspace $\calE$ due to noise and numerical errors as soon as $\nabla^2 R^\aug(x)$ restricted to $\calE^\perp$ has negative eigenvalues. We leave it to future work to analyse this further.

\section{Experiments} \label{sec:experiments}
\newcommand{\normal}{\textsc{Normal}}
\newcommand{\nominal}{\textsc{Nominal}}
\newcommand{\augm}{\textsc{Augmented}}
\newcommand{\eqvi}{\textsc{Equivariant}}

We perform some simple experiments to study the dynamics of the gradient flow \emph{near the equivariant subspace} $\calE$. From our theoretical results, we expect the following.
\begin{enumerate}[(i)]
    \item The set $\calE$ is invariant, but not necessarily stable, under the gradient flow of $R^\aug$.
    \item The dynamics in $\calE^\perp$ for $R^{\aug}$ should (initially) be much 'slower' than in $\calE$, in particular compared to the nominal gradient flow of $R$.
\end{enumerate}

We consider three different (toy) learning tasks, with different symmetry groups and data sets:
\begin{itemize}
    \item[1.] Permutation invariant graph classification (using small synthetically generated graphs)
    \item[2.] Translation invariant image classification (on a subsampled version of MNIST \cite{lecun1998a})
    \item[3.] Rotation equivariant image segmentation (on synthetic images of simple shapes). 
\end{itemize}
The general setup is as follows: We consider a group $G$ acting on vector spaces $(X_i)_{i=0}^L$. We construct a multilayered perceptron $\Phi_A:X_0 \to X_L$ as above. The non-linearities are always chosen as non-linear functions $\R\to \R$ applied elementwise, and are therefore equivariant under the actions we consider. Importantly, to mitigate a vanishing gradient problem, we incorporate batch normalization layers into the models. This entails a slight deviation from the setting in the previous sections, but importantly does not break the invariance or equivariance under $G$ at test time. We build our models with PyTorch \cite{NEURIPS2019_9015}. Detailed descriptions of e.g. the choice of intermediate spaces, non-linearities and data, are provided in Appendix \ref{app:experiments}. In the interest of reproducibility, we also provide the code at \href{https://github.com/usinedepain/eq_aug_dyn}{\texttt{https://github.com/usinedepain/eq\_aug\_dyn}}.

We initialize $\Phi_A$ with equivariant layers $A^0 \in \calE$ by drawing matrices randomly from a standard Gaussian distribution, and then projecting them orthogonally onto $\calE$. We train the network on (finite) datasets $\calD$ using gradient descent in three different ways. 
\begin{itemize}
    \item \nominal: A standard gradient descent, with gradient accumulated over the entire dataset (to emulate the 'non-empirical' risk $R$ as we have defined it here). That is, the data is fed forward through the MLP in mini-batches as usual, but each gradient is calculated by taking the average over all mini-batches.
    \item  \augm: As \nominal, but with $N^\aug$ passes over data where each mini-batch is augmented using a randomly sampled group element $g \sim \mu$. The gradient is again averaged over all passes, to model the augmented risk $R^{\aug}$ as closely as possible.
    \item \eqvi: As in \nominal, but the gradient is projected onto $\calE$ before the gradient step is taken. This corresponds to the equivariant risk $R^{\eqv}$ and produces networks which are manifestly equivariant.
\end{itemize}
 The learning rate $\tau$ is equal to $\tau=10^{-5}$ in all three experiments. In the limit $\tau \to 0$ and $N^\aug \to \infty$, this exactly corresponds to letting the layers evolve according to gradient flow with respect to $R$, $R^\aug$ and $R^\eqv$, respectively. 

For each task, we  train the networks for $50$ epochs. After each epoch we record $\Vert{A-A^0}\Vert$, i.e.~the distance from the starting position $A^0$, and $\Vert A_{\calE^\perp} \Vert$, i.e.~the distance from $\calE$ or equivalently the 'non-equivariance'. Each experiment is repeated 30 times, with random initialisations. 

\subsection{Results}
In Figures \ref{fig:permutations}, \ref{fig:translations} and \ref{fig:rotations}, we plot the evolution of the values $\Vert A_{\mathcal{E}^\perp} \Vert$ against the evolution of $\Vert A - A^0 \Vert$. The opaque line in each plot is formed by the average values for all thirty runs, wheres the fainter lines are the 30 individual runs.

In short, the experiments are consistent our theoretical results. In particular, we observe that the equivariant submanifold is consistently unstable (i) in our repeated augmented experiments. In Experiment 1 and 2, we also observe the hypothesized 'stabilising effect' (ii) on the equivariant subspace: the \augm~model stays much closer to $\calE$ than the \nominal~model - the shift orthogonal to $\calE$ is smaller by several orders of magnitude. For the rotation experiments, the \augm~and \nominal~models are much closer to each other, but note that also here, the actual shifts orthogonal to $\calE$ are very small compared to the total shifts $\Vert{A-A^0}\Vert$ -- on the order $10^{-7}$ compared to $10^{-3}$. 

The reason of the different behaviour in the rotation experiment can at this point only be speculated on. The hyperparameter $N^\aug$, the specific properties of the $\Pi_{\calE}$ operator, the sizes and slight differences architectures (e.g., the fact that the group $G$ is acting non-trivially on all spaces for the rotation example) of the models likely all play a part. We leave the closer study of these matters to further work.

\begin{figure}
\centering
\includegraphics[height=.23cm]{figures/labels.png}\hfill
\subfloat[Experiment 1.\label{fig:permutations}]{
    \includegraphics[height=3.9cm]{figures/perm_lay_redux.png}
    }
\subfloat[Experiment 2.\label{fig:translations}]{
    \includegraphics[height=3.9cm]{figures/trans_lay_redux_extra.png}
    }
\subfloat[Experiment 3.\label{fig:rotations}]{
    \includegraphics[height=3.8cm]{figures/rot_lay_redux_extra.png}
    }\hfill
\caption{Results of the experiments. Shown are plots of $\Vert A_{\calE^\perp} \Vert$ versus $\Vert A-A^0 \Vert$ for the three models. Opaque lines correspond to average values, transparent lines to individual experiments. For Experiment 1 and 2, the two plots depict the same data with different scales on the $\Vert A_{\calE^\perp} \Vert$-axis. \label{fig:results}}
\end{figure}

\section{Conclusion}
In this paper we investigated the dynamics of gradient descent for augmented and equivariant models, and how they are related. In particular, we showed that the models have the same set of equivariant stationary points, but that the stability of these points may differ. Furthermore, when initialized to the equivariant subspace $\calE$, the dynamics of the augmented model is identical to that of the equivariant one. In a first order approximation, dynamics on $\mathcal{E}$ and $\mathcal{E}^{\perp}$ even decouple for the augmented models.

These findings have important practical implications for the two strategies for incorporating symmetries in the learning problem. The fact that their equivariant stationary points agree implies that there are no equivariant configurations that cannot be found using manifest equivariance. Hence, the more efficient parametrisation of the equivariant models neither introduces nor excludes stationary points compared to the less restrictive augmented approach. Conversely, if we can control the potential instability of the non-equivariant subspace $\calE^{\perp}$ in the augmented gradient flow, it will find the same equivariant minima as its equivariant counterpart. One way to accomplish the latter would be to introduce a penalty proportional to $\Vert{A_{\calE^{\perp}}}\Vert^2$ in the augmented risk.
%\begin{align*}
%    \widetilde{R}^{\aug}(A) = \int_G R(\indrho(g)A) \, \dd \mu(g) + \lambda \Vert{A_{\calE^{\perp}}}\Vert^2 \,.
%\end{align*}

Although we showed that the dynamics \emph{on} $\calE$ is identical for the augmented and equivariant models, and understand their behaviour \emph{near} $\calE$, our results say nothing about the dynamics \emph{away from} $\calE$ for the augmented model. Indeed, there is nothing stopping the augmented gradient flow from leaving $\calE$ -- although initialized very near it -- or from coming back again. %What we can say is that if it converges to a point on $\calE$, that point is a local minimum also for the equivariant model. 
To analyse the global properties of the augmented gradient flow, in particular to calculate the spectra of $\hess R^{\aug}$ in concrete cases of interest, is an important direction for future research.

% The bibliography
\newpage
\bibliography{biblio}
\bibliographystyle{plainnat}


% The Appendices
\newpage
\appendix
\section{Collected proofs of theoretical results}
\label{app:proofs}
In this appendix, we collect proofs of the theoretical results that were not given in Section \ref{sec:opt_sym_models}. Most of the claims  are well-known, and we include them mainly to keep the article self-contained.

We begin by proving a small claim we will use implicitly throughout.
\begin{lemma}
    The induced representations $\overline{\rho}_i$ on $\Hom(X_i,X_{i+1})$, $\overline{\rho}$ on $\calL$ and $\overline{\rho}^\tensortwo$ on $\calL^\tensortwo$ are unitary with respect to the canonical inner products.
\end{lemma}
\begin{proof}
Let us begin by quickly establishing that the $\overline{\rho}_i$ are representations:
\begin{align*}
    \overline{\rho}_i(g)\overline{\rho}_i(h)A_i = \rho_{i+1}(g) \rho_{i+1}(h) A_i \rho_i(h)^{-1} \rho_i(g)^{-1} = \rho_{i+1}(gh)A_i \rho_i(gh)^{-1} = \overline{\rho}(gh) A_i
\end{align*}
Now let us move on to the unitarity. We have 
\begin{align*}
    \sprod{\overline{\rho}_i(g)A_i,\overline{\rho}_i(g)B_i} &= \tr((\rho_{i+1}(g) A_i \rho_i(g)^{-1})^*\rho_{i+1}(g) B_i \rho_i(g)^{-1}) \\
    &= \tr( \rho_i(g) A_i^*\rho_{i+1}(g)^{-1}\rho_{i+1}(g)B_i \rho_i(g)^{-1}) \\
    &= \tr(A_i^*B_i \rho_i(g)^{-1}\rho_i(g)) = \tr(A_i^*B_i) = \sprod{A_i,B_i}
\end{align*}
which immediately implies
\begin{equation*}
    \sprod{\overline{\rho}(g)A,\overline{\rho}(g)B} = \sum_{i\in [L]} \sprod{\overline{\rho}_i(g)A_i,\overline{\rho}_i(g)B_i} = \sum_{i \in [L]} \sprod{A_i,B_i} =\sprod{A,B}
\end{equation*}
proving the unitarity of $\overline{\rho}_i$ and then $\overline{\rho}$. Unitary of $\overline{\rho}^\tensortwo$ then follows from the fundamental properties of the tensor product:
\begin{align*}
    \sprod{\overline{\rho}^\tensortwo(g) (A \otimes B),\overline{\rho}^\tensortwo(g)(C \otimes D)} &= \sprod{(\overline{\rho}(g) A \otimes \overline{\rho}(g) B),(\overline{\rho}(g) C \otimes \overline{\rho}(g)D)} \\&= \sprod{\overline{\rho}(g) A , \overline{\rho}(g) C} \sprod{\overline{\rho}(g) B , \overline{\rho}(g)D} = \sprod{A,C} \sprod{B,D} \\
    &= \sprod{(A \otimes B),(C \otimes D)}  \,.
\end{align*}
\end{proof}

Next, we prove the lemmas concerning the induced representation $\overline{\rho}$.
\begin{proof}[Proof of Lemma \ref{lem:ind_rep_layers}]
    The statement follows immediately from the fact the a tuple of layers $A\in\calL$ is in $\calE$ if and only if $\rho_{i+1}(g) A_i= A_i \rho_i(g)$, which is equivalent to $A_i = \rho_{i+1}(g)A_i\rho_i(g)^{-1} = (\overline{\rho}(g)A)_i$ for all $i \in [L]$ and $g\in G$.
\end{proof}

\begin{proof}[Proof of Lemma \ref{lem:ind_rep_orth_proj}]
    Let $P$ be the operator on $\calL$ defined by 
    \begin{align*}
        PA = \int_{G} \overline{\rho}(g)A \, \dd \mu(g).
    \end{align*}
    To show that $P = \Pi_{\calE}$, we first need to show that if $PA \in \calE$ for any $A\in \calL$. To do this, it suffices by Lemma \ref{lem:ind_rep_layers} to prove that $\overline{\rho}(g)PA = PA$ for any $g\in G$. Using the fact that $\overline{\rho}$ is an representation of $G$, and the invariance of the Haar measure, we obtain
    \begin{align*}
        \overline{\rho}(g)PA = \int_{G}\overline{\rho}(g)\overline{\rho}(h)A \, \dd \mu(h) = \int_G \overline{\rho}(gh) A \, \dd \mu(h) = \int_G \overline{\rho}(h')A \, \dd \mu(h') = PA.
    \end{align*}
    Next, we need to show that $PA=A$ for any $A\in \calE$. But since $\overline{\rho}(g)A=A$ for such $A$, we immediately obtain
    \begin{align*}
        PA = \int_{G}\overline{\rho}(g)A \, \dd \mu(g) = \int_G A \, \dd \mu(g) = A.
    \end{align*}
    Consequently, $P: \calL \to \calE$ is a projection. Finally, to establish that $P$ is also orthogonal, we need to show that $\sprod{A-PA,B}=0$ for all $A\in \calL$, $B\in \calE$. This is a simple consequence of the unitarity of $\overline{\rho}$ and Lemma \ref{lem:ind_rep_layers},
    \begin{align*}
        \sprod{PA, B} = \int_G \sprod{\overline{\rho}(g)A,B} \, \dd \mu(g) =\int_G \sprod{\overline{\rho}(g)A,\overline{\rho}(g)B} \, \dd \mu(g) =\int_G \sprod{A,B} \, \dd \mu(g)  = \sprod{A,B} \,,
    \end{align*}
    which completes the proof that $P = \Pi_{\calE}$.
\end{proof}

\begin{proof}[Proof of Lemma \ref{lem:ind_rep_network}]
    As before, $x_i$ denotes  the outputs of each layer of $\Phi_A$ when acting on the input $x \in X$. For $g \in G$, let $x_i^g$ denote the outputs of each layer of the network $\Phi_{\overline{\rho}(g)^{-1}A}$ when acting on the input $\rho_X(g)^{-1}x$. To show that $\Phi_A(\rho_X(g) x) = \rho_Y(g)\Phi_{\overline{\rho}(g)^{-1}A}(x)$ it is clearly enough show that $\rho_i(g) x_i^g = x_i$ for $i=[L]$ which we do by induction. The case $i=0$ is clear,
    \begin{align*}
        \rho_{X}(g) x_0^g = \rho_X(g) \rho_X(g)^{-1}x = x =x_0\,,
    \end{align*}
    and the induction step is
    \begin{align*}
        \rho_{i+1}(g) x_{i+1}^g &= \rho_{i+1}(g) \sigma_i(\overline{\rho}_i(g)^{-1}A_i x_{i}^g) = \rho_{i+1}(g) \sigma_i(\rho_{i+1}(g)^{-1}A_i \rho_i(g) x_{i}^g) \\&= \sigma_i(\rho_{i+1}(g) \rho_{i+1}(g)^{-1}A_i\rho_i(g) x_{i}^g)  = \sigma_i(A_i\rho_i(g) x_i^g) = \sigma_i(A x_{i}) = x_{i+1} \,,
    \end{align*}
    where in the second step, we have used the definition of $\overline{\rho}_i$, in the third, Assumption \eqref{ass:equiv_nonlin}, and the fifth step follows from the induction assumption.

    That $\Phi_A$ is equivariant for every $A\in \calE$ finally follows from the observation that for such $A$ we have $\overline{\rho}(g)^{-1}A=A$.
\end{proof}


\begin{proof} [Proof of Lemma \ref{lem:ind_rep_orth_proj_L2}]
    Replacing $\calL$ with $\calL \otimes \calL$, $\calE$ with $\calE^{\tensortwo}$ and $\indrho$ with $\indrho^{\tensortwo}$, the proof is identical to that of Lemma \ref{lem:ind_rep_orth_proj}.  
\end{proof}

\begin{proof} [Proof of Lemma \ref{lem:E2_diagonal}]
    For $A \in \calE$ we have $\indrho(g)A = A$ for any $g \in G$ according to Lemma \ref{lem:ind_rep_layers}. Consequently,
    \begin{align*}
          \left( \Pi_{\calE^\tensortwo}M \right) [A,A] &= \int_G \indrho^{\tensortwo}(g) M [A,A] d\mu(g) = \int_G M[\indrho(g) A, \indrho(g) A] d\mu(g) \\ &= \int_G M[A,A] d\mu(g)= M[A,A]\,.
    \end{align*}
    which proves the first statement. To prove the second, we use that for $B \in \calE^{\perp}$, $\Pi_{\calE} B = 0$ by definition. A similar calculation as above now yields
    \begin{align*}
        \left( \Pi_{\calE^\tensortwo}M \right) [A,B] &= \int_G \indrho^{\tensortwo}(g) M [A,B] d\mu(g) = \int_G M[\indrho(g) A, \indrho(g) B] d\mu(g) \\ &= \int_G M[A,\indrho(g) B] d\mu(g) = M[A, \Pi_{\calE} B]= 0 \,,
    \end{align*}
    where we have used the bilinearity of $M$ repeatedly to complete the proof.
\end{proof}

\section{The converse of Proposition \ref{prop:minima_E} does not hold} \label{app:counterexample}
    To construct a counterexample, it is clearly enough to construct $U \in \calL\otimes \calL$ such that  $U$ is not positive definite but $\Pi_{\calE^\tensortwo}U$, and $V \in \calE^{\otimes 2}$ sutch that $V$ is not positive definite but $\Pi_{\calE}^\tensortwo V$ is.
    
    We choose $\calL = \R^N$ and $G = S_N$ with the canonical representation $\indrho = \rho^{\perm}$ as in Example \ref{ex:perm}. With $e_i$, $i=1,\ldots,N$, the standard ONB of $\calL$ we can construct the equivariant subspaces of $\calL$ and $\calL\otimes \calL$ using
    \begin{align*}
        \mathds{1} = \sum_{i=1}^N e_i \in \calL \,, \quad \id = \sum_{i=1}^N e_i \otimes e_i \in \calL^\tensortwo 
    \end{align*}
    as $\calE = \mathrm{Span}\{\mathds{1}\}$ and $\calE^{\otimes 2} = \mathrm{Span}\{ \id, \mathds{1} \otimes \mathds{1}\}$~\cite{maron2018invariant}. Furthermore, ONBs of the two spaces are given by  $$ B_{\calE}^1 = \tfrac{1}{\sqrt{N}} \mathds{1} \qquad \text{and }     B_{\calE^\tensortwo}^1 = \tfrac{1}{\sqrt{N}} \id , B_{\calE^\tensortwo}^2 = \tfrac{1}{\sqrt{N(N-1)}}(\mathds{1} \otimes \mathds{1}- \id) \, , $$ respectively.
    
Now consider the matrix $U = (N+1) e_1 \otimes e_1 -  e_2 \otimes e_2 \in \calL^\tensortwo$. It is not positive definite, since $U[e_2,e_2] = -1$. However, as a direct calculation reveals, its projection to $\calE^\tensortwo$, $\Pi_{\calE^{\otimes 2}} U = \id$, clearly is.
    
 Second, we can construct a matrix $V = - \id + \frac{2}{N}(\mathds{1} \otimes \mathds{1}) \in \calE^\tensortwo$ 
 which is not positive definite, since its restriction to $\calE^\perp$ is $-\id$. However, the projection    $\Pi_{\calE}^{\otimes 2} V =  \tfrac{1}{N} \mathds{1} \otimes \mathds{1}$
 is positive definite when restricted to $\calE$.


\section{Experimental details}
\label{app:experiments}
Here, we provide a more detailed description of the experiments in Section \ref{sec:experiments} in the main paper. Each experiment used a single NVIDIA Tesla T4 GPU with 16GB RAM, but many of them were performed in parallell on a cluster. The experiments presented here took in total about 75 GPU hours. 

The code is provided at \href{https://github.com/usinedepain/eq_aug_dyn}{\texttt{https://github.com/usinedepain/eq\_aug\_dyn}}.

\subsection{Experiment 1: Permutation invariant graph classification}
In the first experiment, we train a model to detect whether a graph is connected or not. The model takes adjacency matrices $M \in \R^{N} \otensor \R^N$ of the graph as input. The training task is obviously invariant to permutations of the adjacency matrix (where the action of $S_N$ on the matrix space was defined in Example \ref{ex:perm} in the main paper), since such a permutation does not change the underlying graph. The invariance of the network is conveniently expressed by letting $S_N$ acting trivially on the output space $Y=\R$.

\paragraph{Data} We consider graphs of size $N=10$ drawn according to a 'block-Erdös-Renyi model':  We divide the nodes into two randomly drawn clusters $I,J$, $I \cap J= \emptyset$, $I\cap J = \{0,\dots,10\}$. Within each cluster, a bidirected edge is added between each pair of nodes with a probability of $0.5$, and in between the clusters with a probability of $0.05$. The graphs are subsequently checked for connectivity by checking that all entries of $\sum_{i=0}^N A^i$ are strictly positive, which clearly is sufficient. In this manner, we generate a $1000$ graphs and labels.

\paragraph{Model} The model set up is as follows: Before the first layer, the inputs are 'normalized' by subtracting $.5$ from each entry. Then, the first layer  is chosen to map from the input space $\R^N \otimes \R^N$ into another space of adjacency matrices matrices $(\R^N \otimes \R^N)^{32}$, (on which $S_N$ is acting according to $\rho^\perm$, defined in Example \ref{ex:perm}, on each component). Then, a group pooling layer is used, that is, a map into $\R^{64}$ on which $S_N$ is acting trivially. This is followed by a batch normalization layer, and then two layers $\R^{64}\to \R^{32}$ and $\R^{32} \to \R$. The group acts trivially on each of the final spaces.  In other words, all models are equipped a fully connected head. All but the last non-linearities are chosen as leaky ReLUs, whereas the last one is a sigmoid function. Note that the non-linearities are applied pointwise in the early layers, so that they are trivially equivariant to the group action. We use a binary loss. A graphical depiction of the architecture is given in Figure \ref{fig:permarc}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/vis_perm_arc.png}
    \caption{The setup for the models of Experiment 1.}
    \label{fig:permarc}
\end{figure}

\paragraph{Projection operators} To project the layers onto $\calE$, we use the results of \cite{maron2018invariant}. In said paper, bases for the space invariant under the (induced) representation $\rho^{\perm}$ on $(\R^N)^{\otimes k}$ are given. In this context, the first layer consists of a $(32\times 1)$ array of elements in $(\R^N \otimes \R^N)\otimes (\R^N \otimes \R^N) = (\R^N)^{\otimes 4}$, the second is an $(32 \times 32)$ array of elements in $\R \otimes (\R^N \otimes \R^N) = (\R^N)^{\otimes 2}$, and the final ones are simply arrays of real numbers (and in particular, $\calE$ is the entire space).

\subsection{Experiment 2: Translation invariant image classification}
In the next experiment, we train a model for classification on MNIST. The input space  here is $X_0 = \R^{N,N}$, where $N$ is the width/height of the image. On $X_0$, we let the translation group $\Z_N^2$ act as in Example \ref{ex:trans}. The classification task is invariant to this action, whence we again let $\Z_N^2$ act trivially on the output space $\R^{10}$ of probability distribution on the ten classes.

\paragraph{Data} We use the public dataset MNIST \cite{lecun1998a} in our experiments, but modify it in two ways to keep the experiments light. First, we train our models only on the 10000 test examples (instead of the 60000 training samples). Secondly, we subsample the $28 \times 28$ images to images of size $14 \times 14$ (and hence set $N=14$) using \texttt{opencv}'s \cite{opencv_library} built-in \textsc{Resize} function. This simply to reduce the size of the networks. Note that the size of the early (non-equivariant) layers of the model are proportional to the $(\text{image width})^4$.

\paragraph{Model} We again begin by 'normalizing' the output by subtracting $.5$ from each pixels. The actual architecture then begins with two initial layers mapping between spaces on which $\Z_N^2$ acts according to $\rho^{\tr}$ (in each component) : Layer $1$ maps from $\R^{N,N}\to (\R^{N,N})^{32}$ and layer $2$ from $(\R^{N,N})^{32}$ to $(R^{N,N})^{32}$. We then again apply a group pooling layer followed by a 'fully connected head': The third layer maps into $\R^{32}$, on which $\Z_N^2$ is acting trivially, and the final one between $\R^{32}$ and $\R^{10}$. After the pooling layer, we add a batch normalization layer. The non-linearities are again chosen as leaky ReLU:s, except for the last one, which is a SoftMax. The equivariance of the non-linearities are again induced from them acting elementwise on the first three spaces. We use a cross-entropy loss. A graphical depiction of the setup is given in \ref{fig:transarc}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/vis_trans_arc.png}
    \caption{The setup for the models of Experiment 2.}
    \label{fig:transarc}
\end{figure}

\paragraph{Projection operators} It is well-known that a linear operator on $\R^{N,N}$ is equivariant if and only if it is a convolutional operator. Hence, for the first two layers, the projection is carried out via projecting each component onto the space spanned by
\begin{align*}
    C^{k\ell} = \sum_{i,j \in [N]} (e_i \otimes e_j) \otimes (e_{i+k} \otimes e_{i+\ell}) , \quad k,\ell \in [N]^2.
\end{align*}
The third layers consist of arrays of functionals on $\R^{N,N}$. The only linear invariant such is (up to scale) taking the mean. Here, the projection in this space simply consists of averaging the layer. After that, just as above, $\calE$ is the entire space, and the projection is trivial.

\subsection{Experiment 3: Rotation equivariant image segmentation}
In the final experiment, we consider a simple, rotation invariant, segmentation task for synthetically generated data. The input space consist of images in $\R^{N,N}$, and the output space $(\R^{N,N})^2$ of pairs of segmentation masks. This task is hence 'truly' equivariant.

\paragraph{Data} We generate $3000$ synthetic images as follows: With equal probability, we generate a regular pentagon or an equilateral triangle (with vertices on the unit circle), scale it using a uniformly randomly chosen  factor in $[0.7,0.8]$, and place it randomly on a canvas. The resulting image, along with two segmentation masks, each indicating where each shape is present in the image (and in particular zero everywhere if the particular object is not present at all) are converted to $14 \times 14$ pixels images. Importantly, all triangles, and pentagons, have the same orientation, so that the image distribution is not invariant under discrete rotations of $90^\circ$. Examples of generated images are showcased in Figure \ref{fig:simpshape}.

\begin{figure}
    \centering
    \includegraphics[width=.25\textwidth]{figures/simpshape_ex1.png}\includegraphics[width=.25\textwidth]{figures/simshape_ex2.png}
    \caption{Two examples of generated images for Experiment 3.}
    \label{fig:simpshape}
\end{figure}

\paragraph{Model} In contrast to the previous examples, the action on the output space is not trivial. Therefore, the model set up is less convoluted : We use intermediate spaces $(\R^{N,N})^{32}$,$(\R^{N,N})^{32}$ and $(\R^{N,N})^{16}$ on which the rotation group acts according to $\rho^\rot$ (on each component). Before the final non-linearity, we use a batch norm layer. All non-linearities are leaky ReLUs, except for the last one, which is a sigmoid non-linearity. A visualization is given in Figure \ref{fig:rotarc}. The loss function is
\begin{align*}
    \ell((y_{\mathrm{pent}},y_{\mathrm{tri}}),y_{\mathrm{pent}}',y_{\mathrm{tri}}') = \frac{1}{N^2} &\sum_{k\in [N]^2} \mathrm{BCL}((y_{\mathrm{pent}}(k),y_{\mathrm{pent}}'(k))) \\
    &+ \frac{1}{N^2} \sum_{k\in [N]^2}\mathrm{BCL}(y_{\mathrm{tri}}(k),y_{\mathrm{tri}}'(k)))
\end{align*}
where $\mathrm{BCL}$ is the binary cross-entropy loss, and $y_{\mathrm{pent}}$, $y_{\mathrm{tri}}$ are the pentagon and triangle segmentation masks, respectively. Note that the pixel-wise nature of the loss implies that it is invariant under rotations of the masks.

\paragraph{Projection} Since the group in this case only consists of four elements, and the Haar measure is the uniform one, we can calculate the projection of the layers by explicitly carrying out the integration
\begin{align*}
    \Pi_{\calE} A = \tfrac{1}{4}\sum_{k \in Z_4} \overline{\rho}(k) A.
\end{align*}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/vis_rot_arc.png}
    \caption{The setup for the models of Experiment 3}
    \label{fig:rotarc}
\end{figure}

\end{document}
