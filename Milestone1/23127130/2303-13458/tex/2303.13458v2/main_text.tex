
\maketitle

\definecolor{blue}{RGB}{0,0,0}


\begin{abstract}
We investigate the optimization of multilayer perceptrons on symmetric data. We compare the strategy of constraining the architecture to be equivariant to that of using augmentation. We show that, under natural assumptions on the loss and non-linearities, the sets of equivariant stationary points are identical for the two strategies, and that the set of equivariant layers is invariant under the gradient flow for augmented models. Finally, we show that stationary points may be unstable for augmented training although they are stable for the equivariant models.
\end{abstract}

\section{Introduction}
In machine learning, the general goal is to find 'hidden' patterns in data. However, there are sometimes symmetries in the data that are known a priori. Incorporating these manually should, heuristically, reduce the complexity of the learning task. In this paper, we are concerned with training networks, more specifically multilayer perceptrons (MLPs) as defined below, on data exhibiting symmetries that can be formulated as equivariance under a group action. A standard example is the case of translation invariance in image classification.

More specifically, we want to theoretically study the connections between two general approaches to incorporating symmetries in MLPs. The first approach is to construct equivariant models by means of \emph{architectural design}. This framework, known as \emph{Geometric Deep Learning}~\cite{bronstein2017geometric, bronstein2021geometric}, exploits the geometric origin of the group $G$ of symmetry transformations by choosing the linear MLP layers and nonlinearities to be equivariant (or invariant) with respect to the action of $G$. In other words, the symmetry transformations commute with each linear (or affine) map in the network, which results in an architecture which manifestly respects the symmetry $G$ of the problem. One prominent example is the spatial weight sharing of convolutional neural networks (CNNs) which are equivariant under translations. Group equivariant convolution networks (GCNNs) \cite{cohen2016group, weiler2018learning,kondorGeneralizationEquivarianceConvolution2018} extends this principle to an exact equivariance under more general symmetry groups.  The second approach is agnostic to model architecture, and instead attempts to achieve equivariance during training via \emph{data augmentation}, which refers to the process of extending the training to include synthetic samples obtained by subjecting training data to random symmetry transformations. 

Both approaches have their benefits and drawbacks. Equivariant models use parameters efficiently through weight sharing along the orbits of the symmetry group, but are difficult to implement and computationally expensive to evaluate in general, since they entail numerical integration over the symmetry group (see, e.g.,~\cite{kondorGeneralizationEquivarianceConvolution2018}). Data augmentation, on the other hand, is agnostic to the model structure and easy to adapt to different symmetry groups. However, the augmentation strategy is by no means guaranteed to achieve a model which is exactly equivariant: the hope is that model will 'automatically' infer invariances from the data, but there are few theoretical guarantees. Also, augmentation in general entails an inefficient use of parameters and an increase in model complexity and training required. 

In this paper, we study equivariant MLPs, which can be characterized by their weights lying in a special subspace $\calE$ of the entire parameter space $\calL$ (both formally defined below). We use representation theory to compare and analyze the dynamics of gradient flow training of MLPs adhering to either the equivariant or augmented strategy. In particular, we study their stationary points, i.e., the potential limit points of the training, in $\calE$. The equivariant models (of course) have optima for their gradient flow there and so are guaranteed to produce a trained model which respects the symmetry. However, the dynamics under augmented training has previously not been exhaustively explored.

Our main contributions are as follows:  First, we provide a technical statement (Lemma \ref{lem:ind_rep_aug_risk}) about the augmented risk: We show that it can be expressed as the nominal risk averaged over the symmetry group acting on the layers of the MLP. Using this formulation of the augmented risk, we apply standard methods from the analysis of dynamical systems to show: %to the gradient descent for the equivariant and augmented models to show:
\begin{enumerate}[(i)]
    \item The subspace $\calE$ is invariant under the gradient flow of the augmented model (Corollary \ref{cor:eqv_subs_invariant}). In other words, if the augmented model is equivariantly initialized, it will remain equivariant during training.
    \item The set of stationary points in $\calE$ for the augmented model is identical to that of the equivariant model (Corollary \ref{cor:stationary_points}). In other words, compared to the equivariant approach, augmentation introduces no new equivariant stationary points, nor does it exclude existing ones.
    \item The set of strict local minima in $\calE$ for the augmented model is a subset of the corresponding set for the equivariant model. (Proposition \ref{prop:minima_E}). In other words, the existence of a stable equivariant minimum is not guaranteed by augmentation.
\end{enumerate}
In addition, we perform experiments on different learning tasks, with different symmetry groups, and discuss the results in the context of our theoretical developments.

\section{Related work}
% General refs for construction equivariant networks


The group theory based model for group augmentation we use here is heavily inspired by the framework developed in \cite{chen2020group}. Augmentation and manifest invariance/equivariance have been studied from this perspective in a number of papers \cite{lyle2019analysis,lyle2020benefits, mei2021learning,elesedy2021provably}. More general models for data augmentation have also been considered \cite{dao2019kernel}. Previous work has mostly mostly been concerned with so-called kernel and \emph{feature-averaged} models (see Remark \ref{rem:fa} below), and in particular, fully general MLPs as we treat them here have not been considered. The works have furthermore mostly been concerned with proving statistical properties of the models, and not study their dynamic at training. An exception is~\cite{lyle2020benefits}, in which it is proven that in linear scenarios, the equivariant models are optimal, but little is known about more involved models.

\textcolor{blue}{The dynamics of training \emph{linear equivariant networks} (i.e., MLPs without nonlinearities) has been given some attention in the literature. Linear networks is a simplified, but nonetheless popular theoretical model for analysing neural networks~\cite{bah2022learning}. In \citep{lawrence2021implicit}, the authors analyse the implicit bias of training a linear neural network with one fully connected layer on top of an equivariant backbone using gradient descent. They also provide some numerical results for non-linear models, but no comparison to data augmentation is made. In \cite{chen2023implicit}, completely equivariant linear networks are considered, and an equivalence result between augmentation and restriction is proven for binary classification tasks. However, more realistic MLPs involving non-linearities are not treated at all.} %However, as their results show (e.g Prop. 3.2 in \cite{chen2023implicit}), the non-inclusion of nonlinearities simplifies the structure of them a lot, so that the setting that we are consider really is different from theirs.}

Empirical comparisons of training equivariant and augmented non-equivariant models are common in the literature. Most often, the augmented models are considered as baselines for the evaluation of the equivariant models. More systemic investigations include \cite{gandikotaTrainingArchitectureHow2021,muller2021,gerken22a}. Compared to previous work, our formulation differs in that the parameter of the augmented and equivariant models are defined on the same vector spaces, which allows us to make a stringent mathematical comparison.

\section{Mathematical framework}
\label{sec:opt_sym_models}
Let us begin by setting up the framework. We let $X$ and $Y$ be vector spaces and $\calD(x,y)$ be a joint distribution on $X \times Y$. We are concerned with training an MLP $\Phi_A : X \to Y$ so that $y \approx \Phi_A(x)$. The MLP has the form
\begin{align} \label{def:MLP}
    x_0 = x, \quad x_{i+1} = \sigma_i(A_ix_i), \quad i\in [L] = \{0, \dots, L-1\}, \quad  \Phi_A(x) = x_{L},
\end{align}
where $A_i: X_i \to X_{i+1}$ are linear maps (layers) between (hidden) vector spaces $X_i$ with $X=X_0$ and $Y=X_L$, and $\sigma_i : X_{i+1}\to X_{i+1}$ are non-linearities.\footnote{Bias terms can also be included via the standard trick to write affine maps as linear -- see Appendix \ref{app:bias}.} Note that $A=(A_i)_{i \in [L]}$ parameterizes the network since the non-linearities are assumed to be fixed. We denote the vector space of possible linear layers with 
$\mathcal{L} = \bigoplus_{i \in [L]} \Hom(X_i,X_{i+1})$, where $\Hom(X_i,X_{i+1})$ is the space of linear maps $X_i \to X_{i+1}$. To train the MLP, we optimize the \emph{nominal risk}
\begin{align}%*}
    R(A) = \erw_{\calD}(\ell(\Phi_A(x),y)),
\end{align}%*}
where $\ell: Y \times Y \to \R$ is a loss function, using gradient descent. In fact, to simplify the analysis, we will mainly study the \emph{gradient flow} of the model, i.e., set the learning rate to be infinitesimal.%\footnote{Note that in practice training will probably involve an empirical risk and/or stochastic gradient descent, but the focus of our theoretical development is this idealized model.}

\subsection{Representation theory and equivariance}Throughout the paper, we aim to make the MLP \emph{equivariant} towards a group of symmetry transformations of the space $X \times Y$. That is, we consider a group $G$ acting on the vector spaces $X$ and $Y$ through \emph{representations} $\rho_X$ and $\rho_Y$, respectively. A representation $\rho$ of a group $G$ on a vector space $V$ is a map from the group $G$ to the group of invertible linear maps $\mathrm{GL}(V)$ on $V$ that respects the group operation, i.e. $\rho(gh)=\rho(g)\rho(h)$ for all $g,h \in G$. The representation $\rho$ is unitary if $\rho(g)$ is unitary for all $g \in G$. A function $f: X\to Y$ is called equivariant with respect to $G$ if $f\circ \rho_X(g)  = \rho_Y(g) \circ f$ for all $g\in G$. We denote the space of equivariant \emph{linear} maps $U \to V$ by $\Hom_G(U,V)$.

Let us recall some important examples that will be used throughout the paper.
\begin{example}
    A simple, but important, representation is the trivial one, $\rho^{\mathrm{triv}}(g)=\id$ for all $g\in G$. If we equip $Y$ with the trivial representation, the equivariant functions $f:X\to Y$ are the invariant ones.
\end{example}

\begin{example}
 \label{ex:trans}
    $\Z_N^2$ acts through translations on images $x \in \R^{N,N}$: $(\rho^\trans(k,\ell)x)_{i,j} = x_{i-k,j-\ell}$.
\end{example}

Further examples related to the experiments in subsequent sections are provided in Appendix~\ref{app:ex_rep}. 

\subsection{Training equivariant models}
In order to obtain a model $\Phi_A$ which respects the symmetries of a group $G$ acting on $X \times Y$, we should incorporate them in our model or training strategy. Note that the distribution $\calD(x,y)$ of training data is typically not symmetric in the sense $(x,y) \sim (\rho_X(g)x, \rho_Y(g)y)$. Instead, in the context we consider, symmetries are usually inferred from, e.g., domain knowledge of $X \times Y$. We now formally describe two strategies for training MLPs which respect equivariance under $G$.

\paragraph{Strategy 1: Manifest equivariance} The first method of enforcing equivariance is to constrain the layers to be manifestly equivariant. That is, we assume that $G$ is acting also on all hidden spaces $X_i$ through representations $\rho_i$, where $\rho_0 = \rho_X$ and $\rho_L=\rho_Y$, and constrain each layer $L_i$ to be equivariant. In other words, we choose the layers $L \in \calL$ in the \emph{equivariant subspace}
\begin{align}%*}
    \calE = \bigoplus_{i \in [L]}\Hom_G(X_i,X_{i+1})
\end{align}%*}
If we in addition assume that all non-linearities $\sigma_i$ are equivariant, it is straight-forward to show that $\Phi_A$ is \emph{exactly} equivariant under $G$ (see also Lemma \ref{lem:ind_rep_network}). We will refer to these models as \emph{equivariant}. The set $\calE$ has been extensively studied in the setting of geometric deep learning and explicitly characterized in many important cases \cite{ maron2018invariant, cohen2018generalGCNN, kondorGeneralizationEquivarianceConvolution2018, weiler2019general, maron2019universality,  aronsson2022homogenous}. In \cite{finzi2021practical}, a general method for determining $\mathcal{E}$ numerically directly from the $\rho_i$ and the structure of the group $G$ is described.

Defining $\Pi_\calE: \calL \to \calE$ as the orthogonal projection onto $\calE$, a convenient formulation of the strategy, which is the one we will use, is to optimize the \emph{equivariant risk}
\begin{align}
\label{eq:eqvrisk}
    R^{\eqv}(A) = R(\Pi_\calE A).
\end{align}

\paragraph{Strategy 2: Data augmentation} The second method we consider is to augment the training data. To this end, we define a new distribution on $X\times Y$ by drawing samples $(x,y)$ from $\calD$ and subsequently \emph{augmenting} them by applying the action of a randomly drawn group element $g \in G$ on both data $x$ and label $y$. Training on this augmented distribution can be formulated as optimizing the \emph{augmented risk}
\begin{align}
    R^\aug(A) = \int_G\erw_{\calD}(\ell(\Phi_A(\rho_X(g)x),\rho_Y(g)y)) \, \dd \mu(g) \label{eq:augmented_risk}
\end{align}
Here, $\mu$ is the (normalised) \emph{Haar} measure on the group~\cite{krantz2008geometric}, which is defined through its invariance with respect to the action of $G$ on itself; if $h$ is distributed according to $\mu$ then so is $gh$ for all $g\in G$. This property of the Haar measure will be crucial in our analysis. Choosing another measure would cause the augmentation to be biased towards certain group elements, and is not considered here.  Note that if the data $\calD$ already is symmetric in the sense that $(x,y) \sim (\rho_X(g)x, \rho_Y(g)y)$, the augmentation acts trivially.

\begin{remark} \textcolor{blue}{
    \eqref{eq:augmented_risk} is a simplification -- in practice, the actual function that is optimized is an empirical approximation of $R^\aug$ formed by sampling of the group $G$. Our results are hence about an 'infinite-augmentation limit' that still should have high relevance at least in the 'high-augmentation region' due to the law of large numbers. To properly analyse this transition carefully is important, but beyond the scope of this work.}
\end{remark}

In our analysis, we want to compare the two strategies when training the same model. We will make three global assumptions.

\begin{assumption} The group $G$ is acting on all hidden spaces $X_i$ through unitary representations $\rho_i$. \label{ass:unitary_reps}
\end{assumption}
\begin{assumption} The non-linearities $\sigma_i : X_{i+1} \to X_{i+1}$ are equivariant. \label{ass:equiv_nonlin}
\end{assumption}
\begin{assumption}
    The loss $\ell$ is invariant, i.e. $\ell(\rho_Y(g)y,\rho_Y(g)y') = \ell(y,y')$, $y,y' \in Y$, $g\in G$. \label{ass:equiv_loss}
\end{assumption}

Let us briefly comment on these assumptions. The first assumption is needed for the restriction strategy to be well defined. The technical part -- the unitarity -- is not a true restriction:  As long as all $X_i$ are finite-dimensional and $G$ is compact, we can redefine the inner products on $X_i$ to ensure that all $\rho_i$ become unitary. The second assumption is required for the equivariant strategy to be sound -- if the $\sigma_i$ are not equivariant, they will explicitly break equivariance of $\Phi_A$ even if $A \in \calE$. The third assumption guarantees that the loss-landscape is 'unbiased' towards group transformations, which is certainly required to train any model respecting the symmetry group. %\textcolor{red}{can we motivate this better?}

We also note that all assumptions are in many settings quite weak -- we already commented on Assumption \ref{ass:unitary_reps}. As for Assumption \ref{ass:equiv_nonlin},  note that e.g. any non-linearity acting pixel-wise on an image will be equivariant to both translations and rotations by multiples of $\pi/2$. In the same way,  any loss comparing images pixel by pixel will be, so that Assumption \ref{ass:equiv_loss} is satisfied. Furthermore, if we are trying to learn an invariant function the final representation $\rho_Y$ is trivial and Assumption \ref{ass:equiv_loss} is trivially satisfied.

\begin{remark} \label{rem:fa}
   Before proceeding, let us mention a different strategy to build equivariant models: \emph{feature averaging} \cite{lyle2019analysis}. This strategy refers to altering the model by averaging it over the group:
    \begin{align}%*}
        \Phi^{\FA}_A(x)  := \int_{G} \rho_Y(g)^{-1}\Phi_A(\rho_X(g)x)\, \dd\mu(g).
    \end{align}%*}
    In words, the value of the feature averaged network at a datapoint $x$ is obtained by calculating the outputs of the original model $\Phi_A$ on transformed versions of $x$, and averaging the re-adjusted outputs over $G$. Note that the modification of an MLP here does not rely on explicitly controlling the weights. It is not hard to prove (see e.g. \cite[Prop. 2]{lyle2020benefits}) that under the invariance assumption on $\ell$, 
    \begin{align}
        R^\aug(A) =  \erw(\ell(\Phi^\FA_A(x),y)).  \label{eq:agrisk}
    \end{align}
\end{remark}

\subsection{Lifted representations and their properties} We can lift the representations $\rho_i$ to representations $\overline{\rho}_i$ of $G$ on $\Hom(X_i,X_{i+1})$ through
\begin{align}%*}
    \overline{\rho}_i(g) A_i = \rho_{i+1}(g) A_i \rho_i(g)^{-1},
\end{align}%*}
and from that derive a representation $\overline{\rho}$ on $\calL$ according to $(\overline{\rho}(g)A)_i = \overline{\rho}_i(g)A_i$. Since the $\rho_i$ are unitary, with respect to the appropriate canonical inner products, so are $\overline{\rho}_i$ and $\overline{\rho}$. 

Before proceeding we establish some simple, but crucial facts, concerning the lifted representation $\overline{\rho}$ and the way it appears in the general framework. We will need the following two well-known lemmas. Proofs are presented in Appendix \ref{app:proofs}.
\begin{lemma}
\label{lem:ind_rep_layers}
    $A \in \calE$ if and only if $\,\overline{\rho}(g) A = A$ for all $g\in G$.
\end{lemma}
\begin{lemma}
\label{lem:ind_rep_orth_proj}
    For any $A \in \calL$ the orthogonal projection $\Pi_\calE$ is given by
    \begin{align}%*}
        \Pi_\calE A = \int_{G} \overline{\rho}(g)A \, \dd \mu(g).
    \end{align}%*}
\end{lemma}

We now prove a relation between transforming the input and transforming the layers of an MLP.
\begin{lemma}
\label{lem:ind_rep_network}
    Under Assumption \ref{ass:equiv_nonlin}, for any $A \in \calL$ and $g \in G$ we have
    \begin{align}%*}
        \Phi_A(\rho_X(g) x) = \rho_Y(g)\Phi_{\overline{\rho}(g)^{-1}A}(x).
    \end{align}%*}
    In particular, $\Phi_A$ is equivariant for every $A \in \calE$.
\end{lemma}

\begin{proof}
    The in particular part follows from $\overline{\rho}(g)^{-1}A=A$ for $A \in \calE$. To prove the main statement,  we use the notation \eqref{def:MLP}:  $x_i$ denotes  the outputs of each layer of $\Phi_A$ when it acts on the input $x \in X$. Also, for $g \in G$, let $x_i^g$ denote the outputs of each layer of the network $\Phi_{\overline{\rho}(g)^{-1}A}$ when acting on the input $\rho_X(g)^{-1}x$. If we show that $\rho_i(g) x_i^g = x_i$ for $i=[L+1]$, the claim follows. We do so via induction. The case $i=0$ is clear:
        $\rho_{X}(g) x_0^g = \rho_X(g) \rho_X(g)^{-1}x = x =x_0$. As for the induction step, we have
    \begin{align}%*}
        \rho_{i+1}(g) x_{i+1}^g &= \rho_{i+1}(g) \sigma_i(\overline{\rho}_i(g)^{-1}A_i x_{i}^g) = \rho_{i+1}(g) \sigma_i(\rho_{i+1}(g)^{-1}A_i \rho_i(g) x_{i}^g) \\&= \sigma_i(\rho_{i+1}(g) \rho_{i+1}(g)^{-1}A_i\rho_i(g) x_{i}^g)  = \sigma_i(A_i\rho_i(g) x_i^g) = \sigma_i(A x_{i}) = x_{i+1} \,, \nonumber
    \end{align}%*}
    where in the second step, we have used the definition of $\overline{\rho}_i$, in the third, Assumption \eqref{ass:equiv_nonlin}, and the fifth step follows from the induction assumption. 
\end{proof}
The above formula has an immediate consequence for the augmented loss.
\begin{lemma}
\label{lem:ind_rep_aug_risk}
    Under Assumptions \ref{ass:equiv_nonlin} and \ref{ass:equiv_loss}, the augmented risk can be expressed as
    \begin{align} \label{eq:augrisk}
        R^\aug(A) = \int_{G}R(\overline{\rho}(g)A) \, \dd \mu(g).
    \end{align}
    %In particular, $R^\aug$ is invariant under $\overline{\rho}$.
\end{lemma}
\begin{proof}
    From Lemma \ref{lem:ind_rep_network} and Assumption \eqref{ass:equiv_loss}, it follows that for any $g \in G$ we have $
        \ell(\Phi_{\overline{\rho}(g)^{-1}A}(x), y) = \ell( \rho_Y(g)^{-1}\Phi_A(\rho_X(g)x),y) =  \ell(\Phi_A(\rho_X(g)x), \rho_Y(g)y)$.
    Taking the expectation with respect to the distribution $\calD$, and then integrating over $g \in G$ yields
    \begin{align}%*}
        R^{\aug}(A) = \int_{G}R(\overline{\rho}(g)^{-1}A) \, \dd \mu(g) = \int_{G}R(\overline{\rho}(g^{-1})A) \, \dd \mu(g) \,.
    \end{align}%*}
    Using the fact that the Haar measure is invariant under inversion proves the statement.
    %The invariance of $R^{\aug}$ under $\overline{\rho}$ then follows immediately from the fact that for every $A \in \calL$ and $g \in G$ we have
    %\begin{align}%*}
    %    R^{\aug}(\overline{\rho}(g)A) &= \int_{G}R(\overline{\rho}(h)\overline{\rho}(g)A) \, \dd \mu(h) = \int_{G}R(\overline{\rho}(hg)A) \, \dd \mu(h) \\ &=  \int_{G}R(\overline{\rho}(h')A) \, \dd \mu(h') = R^{\aug}(A).
    %\end{align}%*}
\end{proof}
We note the likeness of \eqref{eq:augrisk} to \eqref{eq:agrisk}: In both equations, we are averaging risks of transformed models over the group. However, in \eqref{eq:agrisk}, we average over transformations of the \emph{input data}, whereas in \eqref{eq:augrisk}, we average over transformations of the \emph{layers}. The latter fact is crucial -- it will allow us analyse the dynamics of gradient flow.

Before moving on, let us introduce one more notion. When considering the dynamics of training we will encounter elements of the tensor product $\calL \otimes \calL$, which also carries a representation $\overline{\rho}^\tensortwo$ of $G$ lifted by $\overline{\rho}$ according to    
\begin{align}%*}
    \overline{\rho}^\tensortwo(g)(A\otimes B) = (\overline{\rho}(g)A) \otimes (\overline{\rho}(g)B)\,.
\end{align}%*}
We refer to the vector space of elements invariant under this action as $\calE^\tensortwo$, and the orthogonal projection onto it as $\Pi_{\calE^\tensortwo}$. As for $\calL$, the induced representation on $\calL \otimes \calL$ can be used to express the orthogonal projection.
\begin{lemma}
\label{lem:ind_rep_orth_proj_L2}
    For any $A,B \in \calL$ the orthogonal projection $\Pi_{\calE^\tensortwo}$ is given by
    \begin{align}%*}
        \Pi_{\calE^\tensortwo}(A \otimes B) = \int_{G} \overline{\rho}^\tensortwo(g)(A \otimes B) \, \dd \mu(g).
    \end{align}%*}
\end{lemma}
\textcolor{blue}{The space $\calE^\tensortwo$ consists bilinear forms on $\calL$. Importantly, if we represent them as matrices with respect to an ONB which can be subdivided into an ONB of $\calE$ and one of $\calE^\perp$, it will be block diagonal. This can be formulated as follows:}
\begin{lemma}
\label{lem:E2_diagonal}
    For any $M \in \calL \otimes \calL$, $A \in \calE$ and $B \in \calE^{\perp}$ we have
    \begin{align}
        \text{(i) }&\left(\Pi_{\calE^\tensortwo}M\right)[A,A] = M[A,A] \,, \\
         \text{(ii) }&\left( \Pi_{\calE^\tensortwo} M \right) [A,B] = 0 \,.
    \end{align}
\end{lemma}

\section{Dynamics of the gradient flow} %descent}
We have now established the framework of optimization for symmetric models that we need to investigate the gradient flow for the equivariant and augmented models. In particular, we want to compare the gradient flow dynamics of the two models as it pertains to the equivariance with respect to the symmetry group $G$ during training. To this end, we consider the gradient flows of the nominal, equivariant and augmented risks
\begin{equation}%*}
    \dot{A} = -\nabla R(A) \,, \quad \dot{A} = -\nabla R^{\eqv}(A) \,, \quad \dot{A} = -\nabla R^{\aug}(A) \,, \quad A \in \calL\,.
\end{equation}%*}

\subsection{Equivariant stationary points}
We first establish the relation between the gradients of the equivariant and augmented models for an initial condition $A\in\calE$.  
\begin{prop}
\label{prop:grad_E}
    For $A \in \calE$ we have $\nabla R^\aug(A) = \Pi_\calE \nabla R(A) = \nabla R^{\eqv}(A)$.
\end{prop}
\begin{proof}
    Taking the derivative of \eqref{eq:augrisk} the chain rule yields
    \begin{align}%*}
        \sprod{\nabla R^\aug(A),B} = \int_{G} \sprod{\overline{\rho}(g)^{-1}\nabla R(\overline{\rho}(g)A),B} \, \dd \mu(g) = \int_{G} \sprod{\nabla R(\overline{\rho}(g)A), \overline{\rho}(g)B} \, \dd \mu(g) \,,
    \end{align}%*}
    where $B\in \calL$ is arbitrary and we have used the unitarity of $\overline{\rho}$. Using that $\overline{\rho}(g)A=A$ for every $A \in \calE$ (Lemma \ref{lem:ind_rep_orth_proj}), we see that the last integral equals
    \begin{align}%*}
        \int_{G} \sprod{\nabla R(A), \overline{\rho}(g)B} \, \dd \mu(g) = \sprod{\nabla R(A), \Pi_\calE B} = \sprod{\Pi_\calE \nabla R(A), B}\,,
    \end{align}%*}
    where we have used orthogonality of $\Pi_{\calE}$ in the final step, which establishes the first equality of the proposition.
    
    The second equality follows immediately from the chain rule applied to \eqref{eq:eqvrisk}
    \begin{align}%*}
        \nabla R^\eqv(A) = \Pi_\calE \nabla R(\Pi_\calE A) = \Pi_\calE \nabla R(A)\,,
    \end{align}%*}
    where we have used that $\Pi_{\calE}$ is self-adjoint and $\Pi_\calE A = A$ for every $A \in \calE$ in the last step.
\end{proof} 

A direct consequence of Proposition \ref{prop:grad_E} is that for any $A \in \calE$ we have $\nabla R^{\aug}(A) \in \calE$ for the gradient, which establishes the following important result.
\begin{cor}
\label{cor:eqv_subs_invariant}
    The equivariant subspace $\calE$ is invariant under the gradient flow of $R^{\aug}$.
\end{cor}
%Of course, $\calE$ is trivially invariant under the gradient flow of the equivariant risk $R^{\eqv}$.

A further immediate consequence of Proposition \ref{prop:grad_E} is the fact that if the initialization of the networks is equivariant, the gradient flow dynamics of the augmented and equivariant models will be identical. In particular, we have the following result.
\begin{cor}
\label{cor:stationary_points}
    $A^* \in \calE$ is a stationary point of the gradient flow of $R^{\aug}$ if and only if it is a stationary point of the gradient flow of $R^{\eqv}$.
\end{cor}

\subsection{Stability of the equivariant stationary points}
We now consider the stability of equivariant stationary points, and more generally of the equivariant subspace $\calE$, under the augmented gradient flow of $R^{\aug}$. Of course, $\calE$ is manifestly stable under the equivariant gradient flow of $R^{\eqv}$. To make statements about the stability we establish the connection between the Hessians of $R$, $R^\eqv$ and $R^\aug$, which can be considered as bilinear forms on $\calL$, i.e. as elements of the tensor product space $\calL \otimes \calL$. 

\begin{prop}
\label{prop:hess_E}
    For $A \in \calE$ we have $\hess R^\aug (A) = \Pi_{\calE^\tensortwo}\hess R(A)$ and $\hess R^\eqv(A) =  \Pi_{\calE}^\tensortwo \hess R(A)$.
\end{prop}
\begin{proof}
    Taking the second derivative of \eqref{eq:augrisk} yields
    \begin{align}%*}
        \hess R^\aug(A)[B,C] = \int_G \hess R(\overline{\rho}(g)A)[\indrho(g)B, \indrho(g)C] \, \dd \mu(g) = \int_G \indrho^\tensortwo(g)\hess R(A)[B, C] \, \dd \mu(g) \,,
    \end{align}%*}
    where $B,C\in\calL$ are arbitrary and we have used $\indrho(g)A=A$ for $g\in G$ and $A\in \calE$ and the definition of $\indrho^\tensortwo$. Lemma \ref{lem:ind_rep_orth_proj_L2} then yields the first equality.
    
    The second statement again follows directly from the chain rule twice applied to \eqref{eq:eqvrisk}
    \begin{align}%*}
        \hess R^\eqv(A) = \Pi_{\calE}^\tensortwo \hess R(\Pi_\calE A) = \Pi_{\calE}^\tensortwo \hess R(A) \,,
    \end{align}%*}
    where we have used $\Pi_\calE A = A$ for $A \in \calE$ and the fact that $\Pi_\calE$ is self-adjoint.
\end{proof}

\begin{prop}
\label{prop:minima_E}
    For $A^* \in \calE$ the following implications hold:
    \begin{itemize}
    \item[i)] If $A^*$ is a strict local minimum of $R$, it is a strict local minimum of $R^{\aug}$. 
    \item[ii)] If $A^*$ is a strict local minimum of $R^{\aug}$, it is a strict local minimum of $R^{\eqv}$.
    \end{itemize}
\end{prop}
\begin{proof}
    \emph{i)} Assume $\nabla R(A^*) = 0$ and $\nabla^2R(A^*)$ positive definite. From Proposition \ref{prop:grad_E} we then have $\nabla R^{\aug}(A^*) = \Pi_{\calE}\nabla R(A^*) = 0$. Furthermore, for $B \neq 0$ Proposition \ref{prop:hess_E} implies
    \begin{align}%*}
       \nabla^2 R^{\aug}(A^*) [B,B] = \Pi_{\calE^\tensortwo} \nabla^2 R(A^*)[B,B] = \int_{G} \nabla^2 R(A^*)[\indrho(g)B, \indrho(g)B] \, \dd \mu(g) > 0 \,,
    \end{align}%*}
    where in the last step we have used the fact that the integrand is positive since $\indrho(g)B \neq 0$ for any $g \in G$ and $B \neq 0$, and $\nabla^2R(A^*)[B,B] > 0$ for $B \neq 0$.
    
    \emph{ii)} Assume $\nabla R^{\aug}(A^*) = 0$ and $\nabla^2R^{\aug}(A^*)$ positive definite. From Proposition \ref{prop:grad_E} we have $\nabla R^{\eqv}(A^*) = \nabla R^{\aug}(A^*) = 0$. Proposition \ref{prop:hess_E} implies that $\nabla^2 R^{\eqv}(A^*) [B,B]$ equals
    \begin{align}%*}
         \nabla^2 R(A^*)[\Pi_{\calE}B,\Pi_{\calE}B] = \Pi_{\calE^\tensortwo} \nabla^2 R(A^*)[\Pi_{\calE}B, \Pi_{\calE}B]  = \nabla^2 R^{\aug}(A^*) [\Pi_{\calE}B, \Pi_{\calE}B]\,,
    \end{align}%*}
 where we used that $\Pi_\calE B \in \calE$, together with the first part of Lemma \ref{lem:E2_diagonal}.  Consequently, $\nabla^2 R^{\eqv}(A^*)[B,B] > 0$ for $\Pi_{\calE} B \neq 0$ which completes the proof.
\end{proof}

The converse of Proposition \ref{prop:minima_E} is not true. A a concrete counterexample is provided in Appendix \ref{app:counterexample}.

Finally, let us remark an interesting property of the dynamics of the augmented gradient flow of $R^{\aug}$ near $\calE$. Decomposing $A$ near $\calE$ as $A = x + y$, with $x \in \calE$ and $y \in \calE^{\perp}$, with $y \approx 0$, and linearising in the deviation $y$ from the equivariant subspace $\calE$ yields
\begin{align}%*}
    \dot{x} + \dot{y} = - \nabla R^{\aug}(x) - y^*\nabla^2 R^{\aug}(x)\, + \mathcal{O}(\Vert{y}\Vert^2).
\end{align}%*}
From Proposition \ref{prop:grad_E} we have $\nabla R^{\aug}(x) \in \calE$, and Proposition \ref{prop:hess_E} (ii) together with Lemma \ref{lem:E2_diagonal} implies that $y^* \nabla^2 R^{\aug}(x) \in \calE^{\perp}$. Consequently, the dynamics approximately  decouple:
\begin{align}%*}
    \begin{cases}
        \dot{x} &= - \nabla R^{\aug}(x)  \quad \ \ \, +  \mathcal{O}(\Vert{y}\Vert^2)\\
        \dot{y} &= - y^*\nabla^2 R^{\aug}(x)  \ + \mathcal{O}(\Vert{y}\Vert^2)
    \end{cases} \,.
\end{align}%*}

We observe that Proposition \ref{prop:grad_E} now implies that the dynamics restricted to $\calE$ is identical to that of the equivariant gradient flow, and that the stability of $\mathcal{E}$ is completely determined by the spectrum of $\nabla^2R^{\aug}$ restricted to $\calE^\perp$. Furthermore, as long the parameters are close to $\calE$, the dynamics of the part of $A$ in $\mathcal{E}$ for the two models are almost equal.

In terms of training augmented models with equivariant initialization, these observations imply that while the augmented gradient flow restricted to $\calE$ will converge to the local minima of the corresponding equivariant model, it may diverge from the equivariant subspace $\calE$ due to noise and numerical errors as soon as $\nabla^2 R^\aug(x)$ restricted to $\calE^\perp$ has negative eigenvalues. \textcolor{blue}{This could potentially be mitigated by introducing a penalty proportional to $\Vert{A_{\calE^{\perp}}}\Vert^2$ in the augmented risk. } We leave it to future work to analyse this further. 

\section{Experiments} \label{sec:experiments}
\newcommand{\normal}{\textsc{Normal}}
\newcommand{\nominal}{\textsc{Nom}}
\newcommand{\augm}{\textsc{Aug}}
\newcommand{\eqvi}{\textsc{Equi}}

\newcommand{\permm}{\textsc{Perm}}
\newcommand{\transs}{\textsc{Trans}}
\newcommand{\rott}{\textsc{Rot}}

We perform some simple experiments to study the dynamics of the gradient flow \emph{near the equivariant subspace} $\calE$. From our theoretical results, we expect the following.
\begin{enumerate}[(i)]
    \item The set $\calE$ is invariant, but not necessarily stable, under the gradient flow of $R^\aug$.
    \item The dynamics in $\calE^\perp$ for $R^{\aug}$ should (initially) be much 'slower' than in $\calE$, in particular compared to the nominal gradient flow of $R$.
\end{enumerate}

We consider three different learning tasks, with different symmetry groups and data sets:
\begin{itemize}
\item[\permm] Permutation invariant graph classification (using small synthetically generated graphs) 
    \item[\transs]  Translation invariant image classification (on a subsampled version of  MNIST \cite{lecun1998a}) 
    \item[\rott] Rotation equivariant image segmentation (on synthetic images of simple shapes). 
\end{itemize}

%\begin{tabular}{l l}
    %\item[\permm]
 %   \permm & Permutation invariant graph classification (using small synthetically generated graphs) \\
    %\item[\transs] 
   %  \transs & Translation invariant image classification (on a subsampled version of MNIST \cite{lecun1998a}) \\
    %\item[\rott] 
   % \rott & Rotation equivariant image segmentation (on synthetic images of simple shapes). 
   % \end{tabular}
%\end{itemize}
The general setup is as follows: We consider a group $G$ acting on vector spaces $(X_i)_{i=0}^L$. We construct a multilayered perceptron $\Phi_A:X_0 \to X_L$ as above. The non-linearities are always chosen as non-linear functions $\R\to \R$ applied elementwise, and are therefore equivariant under the actions we consider. \textcolor{blue}{To mitigate the vanishing gradient problem, we use layer normalization \cite{ba2016layer} which can be accommodated as an equivariant nonlinearity $\sigma_i$.} We build our models with PyTorch \cite{NEURIPS2019_9015}. Detailed descriptions of e.g. the choice of hidden spaces, non-linearities and data, are provided in Appendix \ref{app:experiments}. In the interest of reproducibility at \href{https://github.com/usinedepain/eq_aug_dyn}{\texttt{https://github.com/usinedepain/eq\_aug\_dyn}}.

We initialize $\Phi_A$ with equivariant layers $A^0 \in \calE$ by drawing matrices randomly from a standard Gaussian distribution, and then projecting them orthogonally onto $\calE$. We train the network on (finite) datasets $\calD$ using gradient descent in three different ways. 
\begin{itemize}
    \item[\nominal] A gradient descent, with gradient accumulated over the entire dataset (to emulate the 'non-empirical' risk $R$ as we have defined it here): Data is fed forward through the MLP in mini-batches as usual, but gradients are calculated by taking the averages over mini-batches.
    \item[\augm] As \nominal~, but with $N^\aug$ passes over data where each mini-batch is augmented using a randomly sampled group element $g \sim \mu$. The gradient is again averaged over all passes, to model the augmented risk $R^{\aug}$ as closely as possible.
    \item[\eqvi] As \nominal~, but the gradient is projected onto $\calE$ before the gradient step is taken. This corresponds to the equivariant risk $R^{\eqv}$ and produces networks which are manifestly equivariant.
\end{itemize}
 The learning rate $\tau$ is equal to $5\cdot 10^{-5}$ in all three experiments. In the limit $\tau \to 0$ and $N^\aug \to \infty$, this exactly corresponds to letting the layers evolve according to gradient flow with respect to $R$, $R^\aug$ and $R^\eqv$, respectively. For each task, we  train the networks for $50$ epochs. After each epoch we record $\Vert{A-A^0}\Vert$, i.e.~the distance from the starting position $A^0$, and $\Vert A_{\calE^\perp} \Vert$, i.e.~the distance from $\calE$ or equivalently the 'non-equivariance'. Each experiment is repeated 30 times, with random initialisations. 

\subsection{Results}
In Figure \ref{fig:results}, we plot the evolution of the values $\Vert A_{\mathcal{E}^\perp} \Vert$ against the evolution of $\Vert A - A^0 \Vert$. The opaque line in each plot is formed by the average values for all thirty runs, whereas the fainter lines are the 30 individual runs.

In short, the experiments are consistent with our theoretical results. In particular, we observe that the equivariant submanifold is consistently unstable (i) in our repeated augmented experiments. In \permm~and \transs, we also observe the hypothesized 'stabilising effect' (ii) on the equivariant subspace: the \augm~model stays much closer to $\calE$ than the \nominal~model -- the shift orthogonal to $\calE$ is smaller by several orders of magnitude. For \rott, the \augm~and \nominal~models are much closer to each other, but note that also here, the actual shifts orthogonal to $\calE$ are very small compared to the total shifts $\Vert{A-A^0}\Vert$ -- on the order $10^{-7}$ compared to $10^{-3}$. 

\textcolor{blue}{The reason for the different behaviour in the rotation experiment cannot be deduced from our theory. Our hypothesis is that it is the different symmetry groups, or rather different spaces $\calE$, plays a role here. Proposition \ref{prop:grad_E} tells us that the difference between gradients of the \nominal~and \augm~models are given by the component orthogonal to $\calE$ of $\nabla R$. Hence, if $\calE$ is of low dimension, it can be assumed that a lot of the gradient energy disappears. A good proxy for the size is the relation between $\dim \calE$ and $\dim \calL$. In Appendix \ref{app:dimcalc}, we calculate these fractions and find it to be much larger for \rott~than in the other experiments. This is in agreement with the augmented \rott~model staying closer to its \nominal~counterpart than the other cases. In Appendix \ref{app:extraexp}, we investigate this further (empirically) by repeating the \transs~experiment for other groups. The trend continues: The lower $\dim\calE/\dim\calL$, the closer the augmented model stays to $\calE$.} \textcolor{blue}{Needless to say, this argument is purely heuristic. Finding a proper theoretical explaination for this is however beyond the scope of this paper.}

\begin{figure}
\centering
\includegraphics[width=.83\textwidth]{figures/reruns.png}
\caption{Results of the experiments: $\Vert A_{\calE^\perp} \Vert$ is plotted versus $\Vert A-A^0 \Vert$. Opaque lines are to mean values, transparent lines are individual experiments. For  \permm~and \transs~, the two plots depict the same data with different scales on the $\Vert A_{\calE^\perp} \Vert$-axis. \label{fig:results}}
\end{figure}

%\paragraph{Model} The input space is $X_0 = \R^N \otimes \R^N$ (adjacency matrices). We first apply a linear map to $\calA^{32}$, and then a pointwise leaky ReLU (with negative slope $.01$, i.e. as the default in PyTorch). Next, we map to $\R^{32}$, apply a pointwise leaky ReLU, and apply a batch normalization layer. Then, fully connected layers to $\R^{64}$, $\R^{32}$ are intertwined with leaky ReLUs, and a a final map to $\R^1$, followd by a sigmoid function (due to the binary nature of the learning task).  The negative slope parameter is $0.01$ (i.e., the default in PyTorch). Note that the batch normalization layer technically cannot be modelled as in the theoretical section, since the running means are calculated from the data at training, but does not break the equivariance of the model. We use a binary cross-entropy loss.

%\paragraph{Representations} The action of $S_N$ is trival on all spaces $\R^{k}$, and the obvious extension of the one on $\calA$ on $\calA^{32}$. Hence, the first layer $A_0$ is an array of size $1\times 32$ of maps $\calA \to \calA$. The next layer is an array of size $32 \times 32$ of maps $\calA \to \R$, and the rest are simple linear maps between standard euclidean spaces. The later layers are hence by construction always invariant, and do not have to be projected in the projection step. As for the former, the space of equivariant maps $\calA \to \calA$ and $\calA \to \R$ were characterized in [cite]. In particular, the basis of the equivariant maps as described in ... is used to explicitly perform the projection on $\mathrm{Hom}(X_0,X_1)$ and $\mathrm{Hom}(X_1,X_2)$.

%\paragraph{Augmentation hyper parameters} We traverse the data 25 times per epoch when training the  \aug model. We use mini-batches of size $200$. 

\section{Conclusion}
In this paper we investigated the dynamics of gradient descent for augmented and equivariant models, and how they are related. In particular, we showed that the models have the same set of equivariant stationary points, but that the stability of these points may differ. Furthermore, when initialized to the equivariant subspace $\calE$, the dynamics of the augmented model is identical to that of the equivariant one. In a first order approximation, dynamics on $\mathcal{E}$ and $\mathcal{E}^{\perp}$ even decouple for the augmented models.

These findings have important practical implications for the two strategies for incorporating symmetries in the learning problem. The fact that their equivariant stationary points agree implies that there are no equivariant configurations that cannot be found using manifest equivariance. Hence, the more efficient parametrisation of the equivariant models neither introduces nor excludes equivariant stationary points compared to the less restrictive augmented approach. Conversely, if we can control the potential instability of the non-equivariant subspace $\calE^{\perp}$ in the augmented gradient flow, it will find the same equivariant minima as its equivariant counterpart. One way to accomplish the latter would be to introduce a penalty proportional to $\Vert{A_{\calE^{\perp}}}\Vert^2$ in the augmented risk.
%\begin{align}%*}
%    \widetilde{R}^{\aug}(A) = \int_G R(\indrho(g)A) \, \dd \mu(g) + \lambda \Vert{A_{\calE^{\perp}}}\Vert^2 \,.
%\end{align}%*}

\textcolor{blue}{This work is a first step towards properly understanding the effects of augmentation on the dynamics of gradient flows, but more research is needed. For instance, a}lthough we showed that the dynamics \emph{on} $\calE$ is identical for the augmented and equivariant models, and understand their behaviour \emph{near} $\calE$, our results say nothing about the dynamics \emph{away from} $\calE$ for the augmented model. Indeed, there is nothing stopping the augmented gradient flow from leaving $\calE$ -- although initialized very near it -- or from coming back again. %What we can say is that if it converges to a point on $\calE$, that point is a local minimum also for the equivariant model. 
To analyse the global properties of the augmented gradient flow, in particular to calculate the spectra of $\hess R^{\aug}$ in concrete cases of interest, is an important direction for future research.

% The acknowledgement and bibliography
\newpage
\subsubsection*{Acknowledgement}
This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation. The computations were enabled by resources provided by the National Academic Infrastructure for Supercomputing in Sweden (NAISS) at Chalmers University of Technology partially funded by the Swedish Research Council through grant agreement no. 2022-0672.
\bibliography{biblio}
\bibliographystyle{plainnat}

% The Appendices
\newpage
\appendix
\section{Collected proofs of theoretical results}
\label{app:proofs}
In this appendix, we collect proofs of the theoretical results that were not given in Section \ref{sec:opt_sym_models}. Most of the claims  are well-known, and we include them mainly to keep the article self-contained.

We begin by proving a small claim we will use implicitly throughout.
\begin{lemma}
    The lifted representations $\overline{\rho}_i$ on $\Hom(X_i,X_{i+1})$, $\overline{\rho}$ on $\calL$ and $\overline{\rho}^\tensortwo$ on $\calL^\tensortwo$ are unitary with respect to the canonical inner products.
\end{lemma}
\begin{proof}
Let us begin by quickly establishing that the $\overline{\rho}_i$ are representations:
\begin{align}%*}
    \overline{\rho}_i(g)\overline{\rho}_i(h)A_i = \rho_{i+1}(g) \rho_{i+1}(h) A_i \rho_i(h)^{-1} \rho_i(g)^{-1} = \rho_{i+1}(gh)A_i \rho_i(gh)^{-1} = \overline{\rho}(gh) A_i
\end{align}%*}
Now let us move on to the unitarity. We have 
\begin{align}%*}
    \sprod{\overline{\rho}_i(g)A_i,\overline{\rho}_i(g)B_i} &= \tr((\rho_{i+1}(g) A_i \rho_i(g)^{-1})^*\rho_{i+1}(g) B_i \rho_i(g)^{-1}) \\
    &= \tr( \rho_i(g) A_i^*\rho_{i+1}(g)^{-1}\rho_{i+1}(g)B_i \rho_i(g)^{-1}) \nonumber \\
    &= \tr(A_i^*B_i \rho_i(g)^{-1}\rho_i(g)) = \tr(A_i^*B_i) = \sprod{A_i,B_i} \nonumber
\end{align}%*}
which immediately implies
\begin{equation}%*}
    \sprod{\overline{\rho}(g)A,\overline{\rho}(g)B} = \sum_{i\in [L]} \sprod{\overline{\rho}_i(g)A_i,\overline{\rho}_i(g)B_i} = \sum_{i \in [L]} \sprod{A_i,B_i} =\sprod{A,B}
\end{equation}%*}
proving the unitarity of $\overline{\rho}_i$ and then $\overline{\rho}$. Unitary of $\overline{\rho}^\tensortwo$ then follows from the fundamental properties of the tensor product:
\begin{align}%*}
    \sprod{\overline{\rho}^\tensortwo(g) (A \otimes B),\overline{\rho}^\tensortwo(g)(C \otimes D)} &= \sprod{(\overline{\rho}(g) A \otimes \overline{\rho}(g) B),(\overline{\rho}(g) C \otimes \overline{\rho}(g)D)} \\&= \sprod{\overline{\rho}(g) A , \overline{\rho}(g) C} \sprod{\overline{\rho}(g) B , \nonumber\overline{\rho}(g)D} = \sprod{A,C} \sprod{B,D} \\
    &= \sprod{(A \otimes B),(C \otimes D)} \nonumber \,.
\end{align}%*}
\end{proof}

Next, we prove the lemmas concerning the lifted representation $\overline{\rho}$.
\begin{proof}[Proof of Lemma \ref{lem:ind_rep_layers}]
    The statement follows immediately from the fact the a tuple of layers $A\in\calL$ is in $\calE$ if and only if $\rho_{i+1}(g) A_i= A_i \rho_i(g)$, which is equivalent to $A_i = \rho_{i+1}(g)A_i\rho_i(g)^{-1} = (\overline{\rho}(g)A)_i$ for all $i \in [L]$ and $g\in G$.
\end{proof}

\begin{proof}[Proof of Lemma \ref{lem:ind_rep_orth_proj}]
    Let $P$ be the operator on $\calL$ defined by 
    \begin{align}%*}
        PA = \int_{G} \overline{\rho}(g)A \, \dd \mu(g).
    \end{align}%*}
    To show that $P = \Pi_{\calE}$, we first need to show that if $PA \in \calE$ for any $A\in \calL$. To do this, it suffices by Lemma \ref{lem:ind_rep_layers} to prove that $\overline{\rho}(g)PA = PA$ for any $g\in G$. Using the fact that $\overline{\rho}$ is an representation of $G$, and the invariance of the Haar measure, we obtain
    \begin{align}%*}
        \overline{\rho}(g)PA = \int_{G}\overline{\rho}(g)\overline{\rho}(h)A \, \dd \mu(h) = \int_G \overline{\rho}(gh) A \, \dd \mu(h) = \int_G \overline{\rho}(h')A \, \dd \mu(h') = PA.
    \end{align}%*}
    Next, we need to show that $PA=A$ for any $A\in \calE$. But since $\overline{\rho}(g)A=A$ for such $A$, we immediately obtain
    \begin{align}%*}
        PA = \int_{G}\overline{\rho}(g)A \, \dd \mu(g) = \int_G A \, \dd \mu(g) = A.
    \end{align}%*}
    Consequently, $P: \calL \to \calE$ is a projection. Finally, to establish that $P$ is also orthogonal, we need to show that $\sprod{A-PA,B}=0$ for all $A\in \calL$, $B\in \calE$. This is a simple consequence of the unitarity of $\overline{\rho}$ and Lemma \ref{lem:ind_rep_layers},
    \begin{align}%*}
        \sprod{PA, B} = \int_G \sprod{\overline{\rho}(g)A,B} \, \dd \mu(g) =\int_G \sprod{\overline{\rho}(g)A,\overline{\rho}(g)B} \, \dd \mu(g) =\int_G \sprod{A,B} \, \dd \mu(g)  = \sprod{A,B} \,,
    \end{align}%*}
    which completes the proof that $P = \Pi_{\calE}$.
\end{proof}

%\begin{proof}[Proof of Lemma \ref{lem:ind_rep_network}]
%    As before, $x_i$ denotes  the outputs of each layer of $\Phi_A$ when acting on the input $x \in X$. For $g \in G$, let $x_i^g$ denote the outputs of each layer of the network $\Phi_{\overline{\rho}(g)^{-1}A}$ when acting on the input $\rho_X(g)^{-1}x$. To show that $\Phi_A(\rho_X(g) x) = \rho_Y(g)\Phi_{\overline{\rho}(g)^{-1}A}(x)$ it is clearly enough show that $\rho_i(g) x_i^g = x_i$ for $i=[L]$ which we do by induction. The case $i=0$ is clear,
%    \begin{align}%*}
%        \rho_{X}(g) x_0^g = \rho_X(g) \rho_X(g)^{-1}x = x =x_0\,,
%   \end{align}%*}
%    and the induction step is
%    \begin{align}%*}
%        \rho_{i+1}(g) x_{i+1}^g &= \rho_{i+1}(g) \sigma_i(\overline{\rho}_i(g)^{-1}A_i x_{i}^g) = \rho_{i+1}(g) \sigma_i(\rho_{i+1}(g)^{-1}A_i \rho_i(g) x_{i}^g) \\&= \sigma_i(\rho_{i+1}(g) \rho_{i+1}(g)^{-1}A_i\rho_i(g) x_{i}^g)  = \sigma_i(A_i\rho_i(g) x_i^g) = \sigma_i(A x_{i}) = x_{i+1} \nonumber \,,
%    \end{align}%*}
%    where in the second step, we have used the definition of $\overline{\rho}_i$, in the third, Assumption \eqref{ass:equiv_nonlin}, and the fifth step follows from the induction assumption.

%    That $\Phi_A$ is equivariant for every $A\in \calE$ finally follows from the observation that for such $A$ we have $\overline{\rho}(g)^{-1}A=A$.
%\end{proof}


\begin{proof} [Proof of Lemma \ref{lem:ind_rep_orth_proj_L2}]
    Replacing $\calL$ with $\calL \otimes \calL$, $\calE$ with $\calE^{\tensortwo}$ and $\indrho$ with $\indrho^{\tensortwo}$, the proof is identical to that of Lemma \ref{lem:ind_rep_orth_proj}.  
\end{proof}

\begin{proof} [Proof of Lemma \ref{lem:E2_diagonal}]
    For $A \in \calE$ we have $\indrho(g)A = A$ for any $g \in G$ according to Lemma \ref{lem:ind_rep_layers}. Consequently,
    \begin{align}%*}
          \left( \Pi_{\calE^\tensortwo}M \right) [A,A] &= \int_G \indrho^{\tensortwo}(g) M [A,A] d\mu(g) = \int_G M[\indrho(g) A, \indrho(g) A] d\mu(g) \\ &= \int_G M[A,A] d\mu(g)= M[A,A]\,.
    \end{align}%*}
    which proves the first statement. To prove the second, we use that for $B \in \calE^{\perp}$, $\Pi_{\calE} B = 0$ by definition. A similar calculation as above now yields
    \begin{align}%*}
        \left( \Pi_{\calE^\tensortwo}M \right) [A,B] &= \int_G \indrho^{\tensortwo}(g) M [A,B] d\mu(g) = \int_G M[\indrho(g) A, \indrho(g) B] d\mu(g) \\ &= \int_G M[A,\indrho(g) B] d\mu(g) = M[A, \Pi_{\calE} B]= 0 \,,
    \end{align}%*}
    where we have used the bilinearity of $M$ repeatedly to complete the proof.
\end{proof}

\section{Examples of representations}
\label{app:ex_rep}
In addition to the examples provided in the main part of the paper, the following representations will feature in the experimental design described below.
\begin{example}
 \label{ex:perm}
    The canonical action of the permutation group $S_N$ on $\R^N$ is defined through $(\rho^{\perm}(\pi)v)_i = v_{\pi^{-1}(i)}, i \in [n]$, i.e., an element acts via permuting the elements of a vector. This action induces an action on the tensor space $(\R^N)^{\otimes k} = \R^N \otimes \R^N \otimes \dots \otimes \R^N$: $(\rho^\perm(\pi)T)_{i_0, \dots, i_{k-1}} = T_{\pi^{-1}(i_0), \dots, \pi^{-1}(i_{k-1})}$ which is important for graphs. For instance, when applied to $\R^N \otimes \R^N$, it encodes the effect a re-ordering of the nodes of a graph has on the adjacency matrix on the graph.
\end{example}

\begin{example}
\label{ex:rot}
    $C_4 \cong \Z_4$ acts on images $x\in \R^{N,N}$ through rotations by multiples of $90^\circ$. That is, $\rho^{\rot}(k)= \rho^{\rot}(1)^k$, $k=0,1,2,3$ where $(\rho^\rot(1)x)_{i,j} = x_{-j,i}$.
\end{example}

\section{The converse of Proposition \ref{prop:minima_E} does not hold} \label{app:counterexample}
    To construct a counterexample, it is clearly enough to construct $U \in \calL\otimes \calL$ such that  $U$ is not positive definite but $\Pi_{\calE^\tensortwo}U$, and $V \in \calE^{\otimes 2}$ such that $V$ is not positive definite but $\Pi_{\calE}^\tensortwo V$ is.
    
    We choose $\calL = \R^N$ and $G = S_N$ with the canonical representation $\indrho = \rho^{\perm}$ as in Example \ref{ex:perm}. With $e_i$, $i=1,\ldots,N$, the standard ONB of $\calL$ we can construct the equivariant subspaces of $\calL$ and $\calL\otimes \calL$ using
    \begin{align}%*}
        \mathds{1} = \sum_{i=1}^N e_i \in \calL \,, \quad \id = \sum_{i=1}^N e_i \otimes e_i \in \calL^\tensortwo 
    \end{align}%*}
    as $\calE = \mathrm{Span}\{\mathds{1}\}$ and $\calE^{\otimes 2} = \mathrm{Span}\{ \id, \mathds{1} \otimes \mathds{1}\}$~\cite{maron2018invariant}. Furthermore, ONBs of the two spaces are given by  $$ B_{\calE}^1 = \tfrac{1}{\sqrt{N}} \mathds{1} \qquad \text{and }     B_{\calE^\tensortwo}^1 = \tfrac{1}{\sqrt{N}} \id , B_{\calE^\tensortwo}^2 = \tfrac{1}{\sqrt{N(N-1)}}(\mathds{1} \otimes \mathds{1}- \id) \, , $$ respectively.
    
Now consider the matrix $U = (N+1) e_1 \otimes e_1 -  e_2 \otimes e_2 \in \calL^\tensortwo$. It is not positive definite, since $U[e_2,e_2] = -1$. However, as a direct calculation reveals, its projection to $\calE^\tensortwo$, $\Pi_{\calE^{\otimes 2}} U = \id$, clearly is.
    
 Second, we can construct a matrix $V = - \id + \frac{2}{N}(\mathds{1} \otimes \mathds{1}) \in \calE^\tensortwo$ 
 which is not positive definite, since its restriction to $\calE^\perp$ is $-\id$. However, the projection    $\Pi_{\calE}^{\otimes 2} V =  \tfrac{1}{N} \mathds{1} \otimes \mathds{1}$
 is positive definite when restricted to $\calE$.


\section{Experimental details}
\label{app:experiments}
Here, we provide a more detailed description of the experiments in Section \ref{sec:experiments} in the main paper. Each experiment used a single NVIDIA Tesla T4 GPU with 16GB RAM, but many of them were performed in parallell on a cluster. The experiments presented here took in total about 75 GPU hours. 

The code is provided at \href{https://github.com/usinedepain/eq_aug_dyn}{\texttt{https://github.com/usinedepain/eq\_aug\_dyn}}.

\subsection{ The \permm~experiment}
In the \permm~experiment, we train a model to detect whether a graph is connected or not. The model takes adjacency matrices $M \in \R^{N} \otensor \R^N$ of the graph as input. The training task is obviously invariant to permutations of the adjacency matrix (where the action of $S_N$ on the matrix space was defined in Example \ref{ex:perm} in the main paper), since such a permutation does not change the underlying graph. The invariance of the network is conveniently expressed by letting $S_N$ acting trivially on the output space $Y=\R$.

\paragraph{Data} We consider graphs of size $N=10$ drawn according to a stochastic block model:  We divide the nodes into two randomly drawn clusters $I,J$, $I \cap J= \emptyset$, $I\cup J = \{0,\dots,10\}$. Within each cluster, a bidirected edge is added between each pair of nodes with a probability of $0.5$, and in between the clusters with a probability of $0.05$. The graphs are subsequently checked for connectivity by checking that all entries of $\sum_{i=0}^N A^i$ are strictly positive, which clearly is sufficient. In this manner, we generate a $1000$ graphs and labels.

\paragraph{Model} The model set up is as follows: Before the first layer, the inputs are 'normalized' by subtracting $.5$ from each entry. Then, the first layer  is chosen to map from the input space $\R^N \otimes \R^N$ into another space of adjacency matrices matrices $(\R^N \otimes \R^N)^{32}$, (on which $S_N$ is acting according to $\rho^\perm$, defined in Example \ref{ex:perm}, on each component). Then, a group pooling layer is used, that is, a map into $\R^{64}$ on which $S_N$ is acting trivially. This is followed by a layer normalization layer, and then two layers $\R^{64}\to \R^{32}$ and $\R^{32} \to \R$. The group acts trivially on each of the final spaces.  In other words, all models are equipped a fully connected head. All but the last non-linearities are chosen as leaky ReLUs, whereas the last one is a sigmoid function. Note that the non-linearities are applied pointwise in the early layers, so that they are trivially equivariant to the group action. We use a binary loss. A graphical depiction of the architecture is given in Figure \ref{fig:permarc}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/vis_perm_arc.png}
    \caption{The setup for the models of \permm.}
    \label{fig:permarc}
\end{figure}

\paragraph{Projection operators} To project the layers onto $\calE$, we use the results of \cite{maron2018invariant}. In said paper, bases for the space invariant under the (lifted) representation $\rho^{\perm}$ on $(\R^N)^{\otimes k}$ are given. In this context, the first layer consists of a $(32\times 1)$ array of elements in $(\R^N \otimes \R^N)\otimes (\R^N \otimes \R^N) = (\R^N)^{\otimes 4}$, the second is an $(32 \times 32)$ array of elements in $\R \otimes (\R^N \otimes \R^N) = (\R^N)^{\otimes 2}$, and the final ones are simply arrays of real numbers (and in particular, $\calE$ is the entire space).

\subsection{The \transs~experiment}
In the \transs~experiment, we train a model for classification on MNIST. The input space  here is $X_0 = \R^{N,N}$, where $N$ is the width/height of the image. On $X_0$, we let the translation group $\Z_N^2$ act as in Example \ref{ex:trans}. The classification task is invariant to this action, whence we again let $\Z_N^2$ act trivially on the output space $\R^{10}$ of probability distribution on the ten classes.

\paragraph{Data} We use the public dataset MNIST \cite{lecun1998a} in our experiments, but modify it in two ways to keep the experiments light. First, we train our models only on the 10000 test examples (instead of the 60000 training samples). Secondly, we subsample the $28 \times 28$ images to images of size $14 \times 14$ (and hence set $N=14$) using \texttt{opencv}'s \cite{opencv_library} built-in \textsc{Resize} function. This simply to reduce the size of the networks. Note that the size of the early (non-equivariant) layers of the model are proportional to the $(\text{image width})^4$.

\paragraph{Model} We again begin by 'normalizing' the output by subtracting $.5$ from each pixels. The actual architecture then begins with two initial layers mapping between spaces on which $\Z_N^2$ acts according to $\rho^{\tr}$ (in each component) : Layer $1$ maps from $\R^{N,N}\to (\R^{N,N})^{32}$ and layer $2$ from $(\R^{N,N})^{32}$ to $(\R^{N,N})^{32}$. We then again apply a group pooling layer followed by a 'fully connected head': The third layer maps into $\R^{32}$, on which $\Z_N^2$ is acting trivially, and the final one between $\R^{32}$ and $\R^{10}$. After the pooling layer, we add a layer normalization layer. The non-linearities are again chosen as leaky ReLU:s, except for the last one, which is a SoftMax. The equivariance of the non-linearities are again lifted from them acting elementwise on the first three spaces. We use a cross-entropy loss. A graphical depiction of the setup is given in \ref{fig:transarc}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/vis_trans_arc.png}
    \caption{The setup for the models of \transs.}
    \label{fig:transarc}
\end{figure}

\paragraph{Projection operators} It is well-known that a linear operator on $\R^{N,N}$ is equivariant if and only if it is a convolutional operator. Hence, for the first two layers, the projection is carried out via projecting each component onto the space spanned by
\begin{align}%*}
    C^{k\ell} = \sum_{i,j \in [N]} (e_i \otimes e_j) \otimes (e_{i+k} \otimes e_{i+\ell}) , \quad k,\ell \in [N]^2.
\end{align}%*}
The third layers consist of arrays of functionals on $\R^{N,N}$. The only linear invariant such is (up to scale and interpreted as a member of $\R^{N,N}$) the constant matrix. Hence, the projection in this space simply consists of averaging the layer. After that, just as above, $\calE$ is the entire space, and the projection is trivial.

{\color{blue}The convolution operators are furthermore well-known to be characterized as diagonal operators in the Fourier domain. We may therefore implement the  projection onto $\calE$ efficiently by first transforming into Fourier domain, extract the diagonal, and then transforming back. For completeness, let us quickly prove that this really yields the orthogonal projection. % This works for partial directions as well : Just use '$\mathbb{F} = \calF \otimes \id$' instead
\begin{prop} \label{prop:projection_trans}
    For $N\in \N$, let the translation group $G= \Z_N^2$ act on the space $\mathbb{C}^{N,N} \sim \mathbb{C}^{[N]^2}$. If we let $\calF : \C^{N,N}\to \C^{N,N}$ denote the orthogonal two-dimensional Fourier transform, and $\diag: \C^{N,N} \to \C^{N,N}$ the operator that kills all off-diagonal values, the orthogonal projection onto $ \Hom_{G}(\mathbb{C}^{N,N}, \mathbb{C}^{N,N})$ is given by
    \begin{align}%*}
        \Pi_\calE(A) = \calF^{-1}\diag(\calF A \calF^{-1})\calF.
    \end{align}%*}
    %The restriction of the above operator to $\R^{N,N}$ is a real operator.
\end{prop}
\begin{proof}
    Let us begin by showing that $P = \calF^{-1}\diag(\calF A \calF^{-1})\calF$ defines an orthogonal projection. As for the 'projection' part, note that for any $A$, the matrix $D=\diag(\calF A \calF^{-1})$ will of course be diagonal, so that $\diag(D)=D$. Consequently,
    \begin{align}%*}
        P^2(A) = P(\calF^{-1}D\calF) = \calF^{-1}\diag (D) \calF = \calF^{-1}D\calF = P(A).
    \end{align}%*}
    The orthogonality can now be shown via arguing that $P$ is self-adjoint. This is furthermore not hard: Due to the orthogonality of $\calF$, $A \mapsto \calF A \calF^{-1}$ and $A \mapsto \calF^{-1}A \calF$ are self-adjoint, as is obviously $\diag$ (with respect to the standard scalar product on $\C^{N,N}$).

    It is now only left to show that the range of $P$ is equal to the space $\Hom_{G}(\mathbb{C}^{N,N}, \mathbb{C}^{N,N})$, which is the space of translations equivariant operators $ \mathbb{C}^{N,N} \to  \mathbb{C}^{N,N}$. It is however well-known that those exactly correspond to convolutional operators $C_\phi(v) = \phi*v$. The convolution theorem now states
    \begin{align}%*}
        \calF(C_\phi v) = \calF(\phi * v) = \calF(\phi) \cdot \calF (v) = \calD(\calF(\phi)) \calF v,
    \end{align}%*}
    where $\calD: \C^N \mapsto \C^{N,N}$ is the operator that inserts a vector into the diagonal of a matrix. In other words, $C_\phi = \calF^{-1}\calD(\calF(\phi)) \calF$, which makes it obvious that $\mathrm{ran} P \subseteq \calE$. Furthermore, by setting $A= \calF^{-1}\calD(\calF(\phi))\calF$, we see that we can write every convolution operator as an image under $P$. In conclusion, $P = \Pi_\calE$.

    %The final claim is easiest to prove via direct calculation. Represented in the canonical basis of $\R^{N,N}$, we have $$\calF_{\iota,\kappa} = N^{-1}\omega_N^{\sprod{\kappa,\iota}}, \quad \kappa,\iota \in [N]^2, \omega_N= e^{\frac{i2\pi}{N}}$$ notice that due to ortogonality $\calF^{-1}=\calF^*$. In particular, $(\calF^*)_{\iota, \kappa} = \overline{\calF_{\kappa,\iota}}$ for $\kappa,\iota \in [N]^2$. Consequently, for each $\iota \in [N]$ and $M\in \Hom(\C^{N,N},C^{N,N}) \sim \C^{[N]^2,[N]^2}$
    %\begin{align}%*}
    %    (\calF M \calF^{-1})_{\iota,\lambda} &= \sum_{\kappa,\nu \in[N]^2} \calF_{\iota,\kappa} M_{\kappa,\nu}\overline{\calF_{\lambda,\nu}} = \frac{1}{N^2}\sum_{\kappa,\nu \in [N]^2} M_{\kappa,\nu} \omega_N^{\sprod{\kappa,\iota}-{\lambda,\nu}} \\
    %\end{align}%*}
    %By evaluating this for a real operator $M$ and $\iota=\lambda$ and utilizing the fact that $\overline{\omega^{\sprod{\kappa,\iota}}}=\omega^{-\sprod{\kappa,\iota}}$, we see that $\diag(\calF M\calF^{-1})$ is a diagonal with $d_{-k} = \overline{d_k}$. If we then evaluate the above for such a matrix, a real number appears.
\end{proof}

}
%We first normalize the image by subtracting $0.5$ from each pixel (whose value lies between $0$ and $1$) and apply linear maps from $X_0$ to $X_1=\calI^{32}$, and then from $X_1$ to $X_2= \calI^{32}$, intertwined with pointwise leaky ReLUs. Then, we apply a layer, linearly map to $X_3=\R^{32}$, apply a leaky ReLU, linearly map to $X_10$ and then finally apply the SoftMax function to produce score in the $10$ classes. We use a cross-entropy loss. Again, the batch normalization is added to escape a vanishing gradient problem, and does not break the equivariance.

%\paragraph{Representations} The translation group $\Z_N^2$ act as in Example... on $\calI$ and $\calI^{32}$. The action on the final euclidian spaces is trivial. Accordingly, the first and second layers are arrays of size $1\times 32$, and $32\times 32$, respectively, of $\Z_N^2$-equivariant maps $\calI\to \calI$. The latter are well known to be the convolution operators, or equivalently circular matrices. The third layer is an array of maps $\calI \to \R$ -- these correspond to elements in $\calI$ invariant under the action $\Z_N^2$. The only such element is, up to scale, the constant image. Again, the final layers map between spaces on which the group action is linear, and no explicit projection needs to be made.

%\paragraph{Augmentation hyperparameters} We traverse the data $25$ times per epoch when training the $\aug$ model. We use minibatches of size $25$.
%In the second experiment, we train a model to detect whether a graph is connected or not. We consider graphs of size $N=10$ drawn according to a 'block-Erds-Renyi model': i.e., we divide the graphs into two equally size clusters, with a higher connection probability within the clusters than between. The training task is obviously invariant to permutations of the adjacency matrix (where the action of $S_N$ on the matrix space was defined in Example ...), since such a permutation does not change the underlying graph. %We divide the nodes into two clusters $\set{0, \dots, 4}$ and $\set{5,\dots,9}$. Within a cluster, a bidirected edge is added between each pair of nodes with a probability of $0.5$, and in between the clusters with a probability of $0.05$. In this manner, we generate a $10000$ graphs and labels.

%The model set-up is visualized in Figure ... We start with two layers mapping between spaces of adjacency matrices (on which the permutation group is acting as in Example)... We then apply an invarizing layer, followed by a batch normalization layer, and then three layers between spaces on which the $S_n$ acts trivial -- or in other words, a fully connected head. All but the last non-linearities are chosen as leaky RELUs, whereas the last one is a sigmoid function. 
\subsection{The \rott~experiment}
In the final experiment, we consider a simple, rotation invariant, segmentation task for synthetically generated data. The input space consist of images in $\R^{N,N}$, and the output space $(\R^{N,N})^2$ of pairs of segmentation masks. This task is hence 'truly' equivariant.

\paragraph{Data} We generate $3000$ synthetic images as follows: With equal probability, we generate a regular pentagon or an equilateral triangle (with vertices on the unit circle), scale it using a uniformly randomly chosen  factor in $[0.7,0.8]$, and place it randomly on a canvas. The resulting image, along with two segmentation masks, each indicating where each shape is present in the image (and in particular zero everywhere if the particular object is not present at all) are converted to $14 \times 14$ pixels images. Importantly, all triangles, and pentagons, have the same orientation, so that the image distribution is not invariant under discrete rotations of $90^\circ$. Examples of generated images are showcased in Figure \ref{fig:simpshape}.

\begin{figure}
    \centering
    \includegraphics[width=.25\textwidth]{figures/simpshape_ex1.png}\includegraphics[width=.25\textwidth]{figures/simshape_ex2.png}
    \caption{Two examples of generated images for \rott.}
    \label{fig:simpshape}
\end{figure}

\paragraph{Model} In contrast to the previous examples, the action on the output space is not trivial. Therefore, the model set up is less convoluted : We use hidden spaces $(\R^{N,N})^{32}$,$(\R^{N,N})^{32}$ and $(\R^{N,N})^{16}$ on which the rotation group acts according to $\rho^\rot$ (on each component). Before the final non-linearity, we use a batch norm layer. All non-linearities are leaky ReLUs, except for the last one, which is a sigmoid non-linearity. A visualization is given in Figure \ref{fig:rotarc}. The loss function is
\begin{align}%*}
    \ell((y_{\mathrm{pent}},y_{\mathrm{tri}}),y_{\mathrm{pent}}',y_{\mathrm{tri}}') = \frac{1}{N^2} &\sum_{k\in [N]^2} \mathrm{BCL}((y_{\mathrm{pent}}(k),y_{\mathrm{pent}}'(k))) \\
    &+ \frac{1}{N^2} \sum_{k\in [N]^2}\mathrm{BCL}(y_{\mathrm{tri}}(k),y_{\mathrm{tri}}'(k))) \nonumber
\end{align}%*}
where $\mathrm{BCL}$ is the binary cross-entropy loss, and $y_{\mathrm{pent}}$, $y_{\mathrm{tri}}$ are the pentagon and triangle segmentation masks, respectively. Note that the pixel-wise nature of the loss implies that it is invariant under rotations of the masks.

\paragraph{Projection operators} Since the group in this case only consists of four elements, and the Haar measure is the uniform one, we can calculate the projection of the layers by explicitly carrying out the integration
\begin{align}%*}
    \Pi_{\calE} A = \tfrac{1}{4}\sum_{k \in \mathbb{Z}_4} \overline{\rho}(k) A.
\end{align}%*}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/vis_rot_arc.png}
    \caption{The setup for the models of Experiment 3}
    \label{fig:rotarc}
\end{figure}

{\color{blue}
\section{The relation between $\dim(\calE)$ and the 'empirical stability' of $\calE$.}

\begin{table}[b]
    \centering
    \begin{tabular}{| l|c | c | c | c | l | }
    \hline Experiment & $U$ & $V$ & $\dim \Hom(U,V)$ & $\dim \Hom_G(U,V)$ & \# in model  \\
    \hline  \permm  & $\R^{N}\otimes \R^N$ & $\R^N \otimes \R^N$ & $N^4$ & 15  & 32 \\
                    & $\R^N \otimes \R^N$  & $\R$                & $N^2$ & $2$ & $32\cdot 64$ \\
                    & $\R$                 & $\R$                & 1     & 1   & $64 \cdot 32+ 32\cdot 1$ \\
    \hline \transs  & $\R^{N,N}$           & $\R^{N,N}$          & $N^4$ & $N^2$ & $32 + 32 \cdot 32$ \\
                    & $\R^{N,N}$           & $\R$                & $N^2$ & 1     & $32 \cdot 32$ \\
                    & $\R$                 & $\R$                & 1     & 1   & $32\cdot 10$ \\
    \hline  \rott   & $\R^{N,N}$           & $\R^{N,N}$          & $N^4$ & $\tfrac{1}{4}\cdot N^4$ & $32 + 32\cdot 32 + $  \\
                    &                      &                     &       &                         & $32 \cdot 16+ 16 \cdot 2$ \\
    \hline
    \end{tabular}

    \hspace{.5cm}
    \caption{Dimension calculations for the experiments \permm~, \transs~and \rott~from the main paper. }
    \label{tab:dimcalc_one}
\end{table}
\textbf{}

In the main text, we sketched a heuristic argument relating the relative dimension of the space $\calE$ in the space $\calL$ with how close the \augm~model stayed to $\calE$. In this section, we first want to carry out these computations. To further investigate the link, we also provide an additional experiment. A quick glance of the calculations are given in Tables \ref{tab:dimcalc_one} and \ref{tab:dimcalc_two}

\subsection{The experiments in Section 5} \label{app:dimcalc}
\paragraph{\permm} As mentioned before, the spaces of equivariant linear maps was described in \cite{maron2018invariant}. Therein, it was shown that the dimension of (under the permutation group) equivariant linear maps from $\R^N \otimes \R^N$ to itself is equal to $15$, and from $\R^N \otimes \R^N$ to itself is $2$ (as long as $N\geq 2$, as was pointed out in \cite{finzi2021practical}). The dimensions of the corresponding whole spaces are $N^4$ and $N^2$, respectively. In the architecture we are proposing, we use $32$ maps $\R^N \otimes \R^N$ in the first layer, and $32 \cdot 64$ maps $\R^N \otimes \R^N \to \R$ in the second layer. In the layers thereafter, all  $64 \cdot 32+ 32\cdot 1$ maps are equivariant. Hence
\begin{align}%*}
    \frac{\dim \calE}{\dim \calL} = \frac{15 \cdot 32 + 5 \cdot 32 \cdot 64 + 64 \cdot 32 + 32 \cdot 1}{  N^4 \cdot 32+ N^2 \cdot 32 \cdot 64 + 64 \cdot 32 + 32 \cdot 1} \approx 2.43 \cdot 10^{-2}.
\end{align}%*}
where we plugged in $N=10$ in the final step.

\paragraph{\transs} Here, the space of equivariant linear maps $\R^{N,N} \to \R^{N,N}$ is the space of convolution operators. On $\R^{N,N}$, there are $N^2$ of those. There is up to scale only one equivariant map $\R^{N,N}\to \R$, given by taking the mean of the matrix entries. The space of all linear maps in the two cases of course have dimension $N^4$ and $N^2$, respectively. Since we use two initial layers of $32$ and $32\cdot 32$ maps $\R^{N,N}\to \R^{N,N}$, respectively, one layer of $32\times 32$ maps $\R^{N,N}\to \R$ and $32\cdot 10$ maps from $\R\to\R$ (where again all maps are equivariant), we obtain
\begin{align}%*}
\frac{\dim \calE}{\dim \calL} = \frac{N^2 \cdot 32 + N^2 \cdot 32\cdot 32 + 1\cdot 32 \cdot 32 + 32 \cdot 10 }{ N^4 \cdot 32 + N^4 \cdot 32\cdot 32 + N^2\cdot 32 \cdot 32 + 32 \cdot 10} \approx 5.1 \cdot 10^{-3}.
\end{align}%*}
where we plugged in $N=14$ in the final step.

\paragraph{\rott} Here, a map from $\R^{N,N}$ to $\R^{N,N}$ is clearly equivariant if and only if its matrix representation, as a function from $([N]^2)^2$ to $R$, is constant along all orbits
\begin{align}%*}
\{(\iota_0,\iota_1),(r^{-1}(\iota_0), r(\iota_1)), (r^{-2}(\iota_0), r^2(\iota_1)),(r^{-3}(\iota_0), r^3(\iota_1))\},
\end{align}%*}
where $\iota_i=[\iota_i(0),\iota_i(1)] \in [N]^2$ and
\begin{align}%*}
    r(\iota) = (-\iota(1),\iota(0))
\end{align}%*}
Since $N=14$, i.e. even, each such orbit has $4$ elements, and they are of course disjoint. Therefore, the relative dimension between the space of equivariant maps $\R^{N,N}\to \R^{N,N}$ and the space of all such maps is equal to $1/4$. Since it is only such maps that appear in the model used in the experiment,
\begin{align}%*}
    \frac{\dim \calE}{\dim \calL} = \frac{1}{4}=0.25
\end{align}%*}
in this case.


\subsection{An additional experiment}  \label{app:extraexp}
    \newcommand{\transrott}{\textsc{TransRot}}
    \newcommand{\onedtranss}{\textsc{OneDTrans}}
    \newcommand{\twodtranss}{\textsc{TwoDTrans}}
In the main paper, not only $G$ (and accordingly $\calE$) was varying in between the experiment, but also the nominal model architectures and datasets, which makes it unclear if it is only $G$ that plays a role in the different speeds of drift of the augmented model from (or say 'empirical stability' of) $\calE$ . We therefore perform an additional experiment in which all models use the same underlying architecture and dataset, namely the one of the \transs~experiment, and only vary the underlying symmetry group acting on $\R^{N,N}$. We use four groups and actions.
\begin{itemize}
    \item \transs~ $\Z_N^2$ acting through translations, as in the \transs~experiment.
    \item \rott~$\Z_4$ acting through rotations, as in the \rott~experiment.
    \item \onedtranss~$\Z_N$ acting through translations in the $x$-direction, i.e.
    \begin{align}%*}
        (\rho^{\tr_0}(k)x)_{i,j} = x_{i-k,j}
    \end{align}%*}
    \item \transrott~The semi-direct product $\Z_N^2 \rtimes \Z^4$ acting through
    \begin{align}%*}
        \rho^{\mathrm{trot}}(\iota,k)x = \rho^\rot(k)\rho^\tr(\iota)x, \quad \iota \in \Z_N^2, k \in \Z_4.
    \end{align}%*}
    This can, in the same way $\Z_4$ is a discretization of the full group  $\mathrm{SO}(2)$ of rotations in the plane, be thought of as a discretization of the group $\mathrm{SE}(2)$ of isometries in the plane.
\end{itemize}
% calculations
The relative dimensions $\dim \calE / \dim \calL$ are in these cases (approximately) given by
\begin{align}%*}
    \begin{tabular}{lllll }
         \transs~: & $5.1 \cdot 10^{-3}$ &  & \rott~: &$0.25$\\
        \onedtranss~: & $7.1\cdot 10^{-2} $ &  & \transrott~:& $ 1.3 \cdot 10^{-3} $
    \end{tabular}.
\end{align}%*}
These numbers are the results of similar calculations as above, with the data from Table \eqref{tab:dimcalc_two} -- the (conceptually simple but technical) arguments for the validity of non-trivial entries are given in Section \ref{sec:dims}. According to it, we expect the relative drift of the augmented model compared to the nominal one from $\calE$ should be the largest for \rott, followed by \onedtranss, \transs~and \transrott, in decreasing order. 

\begin{table}[b]
    \centering
    \begin{tabular}{| l|c | c | c | c | l | }
    \hline Experiment & $U$ & $V$ & $\dim \Hom(U,V)$ & $\dim \Hom_G(U,V)$ & \# in model  \\
    \hline \rott    & $\R^{N,N}$           & $\R^{N,N}$          & $N^4$ & $\tfrac{1}{4}N^4$ & $32 + 32 \cdot 32$ \\
                    & $\R^{N,N}$           & $\R$                & $N^2$ & $\tfrac{1}{4}N^2$    & $32 \cdot 32$ \\
                    & $\R$                 & $\R$                & 1     & 1   & $32\cdot 10$ \\
    \hline \onedtranss  & $\R^{N,N}$           & $\R^{N,N}$      & $N^4$ & $N^3$ & $32 + 32 \cdot 32$ \\
                        & $\R^{N,N}$           & $\R$            & $N^2$ & $N$     & $32 \cdot 32$ \\
                        & $\R$                 & $\R$            & 1     & 1   & $32\cdot 10$ \\
    \hline \transrott  & $\R^{N,N}$           & $\R^{N,N}$       & $N^4$ & $\tfrac{1}{4}N^2$ & $32 + 32 \cdot 32$ \\
                       & $\R^{N,N}$           & $\R$             & $N^2$ & 1     & $32 + 32 \cdot 32$ \\
                       & $\R$                 & $\R$             & 1     & 1   & $32\cdot 10$ \\
    \hline
    \end{tabular}

    \hspace{.5cm}
    \caption{Dimension calculations for the experiments \rott~, \onedtranss~and \transrott~in the appendix.}
    \label{tab:dimcalc_two}
\end{table}

As mentioned, we repeat the exact same experiment as \transs~for the new groups\footnote{Resulting in about 60 more hours of GPU time, on Tesla A40 GPUs situated on a cluster}.  We then plot the results similarly as in the experiments in the main paper, in Figure \ref{fig:mnist_different groups}. Note that the scales in the figures are different, to assess the amount the augmented model drifts \emph{relative} to the other ones. We have chosen the limits for the axes as follows:
\begin{itemize}
    \item The $x_{\mathrm{left}}$-limit in both subplots is chosen as $1.5$ times the maximal (with respect to the $50$ training epochs) median (with respect to the $30$ runs) value of $\Vert{A-A_0}\Vert$ for the \eqvi~model.
    \item The $y_{\mathrm{left}}$-limit in the left subplot  is chosen as $1.5$ times the maximal (with respect to the $50$ training epochs) median (with respect to the $30$ runs) value of $\Vert\Pi_{\calE^\perp}A\Vert$ for the \nominal~model.
    \item The $y_{\mathrm{right}}$-limit in the left subplot is given by $\lambda\cdot y_{\mathrm{left}}$, where $\lambda>0$ is a factor common for all four groups. $\lambda$ is chosen so that $y_{\mathrm{right}}$ is equal to $1.5$ times the maximal (with respect to the $50$ training epochs) median (with respect to the $30$ runs) value of $\Vert{\Pi_{\calE^\perp}A}\Vert$ for the \augm~model \emph{for the \transs~ experiment}.
\end{itemize}
In this way, we ensure that the coordinate systems are on the same scale relative to the \nominal~and \eqvi~models. It is hence not surprising that the \nominal~ curves look the same in all the plots -- it is very much that way by design. The same is however not true for the \augm~ curve, and that is telling us something -- in fact, this behaviour is in accordance with our hypothesis. This further strengthens the argument that the dimension of the subspace $\calE$ plays a crucial role in the amount of regularizing effect augmentation has. Judging by the quite small difference between \transs~and \transrott~, although the relative dimensions differ by a factor $4$, further suggests that hypothesizing a simple linear relationship is to naive -- more work needs to be done here.



\begin{figure}[h]  
    \center
    \includegraphics[width=.8\textwidth]{figures/MNIST_varying_group.png}
    \caption{Results from the experiments of the appendix. \label{fig:mnist_different groups}}
\end{figure}



% experiments

% proofs

\subsubsection{The spaces $\Hom_G(U,V)$ and projection operators} \label{sec:dims}
Here, we derive what $\Hom_G(U,V)$ look like for the groups $\Z_N$ and $\Z^4 \rtimes \Z_N$, and the corresponding projection operators. These derivations are all conceptually easy, but technical, and are only included for completeness. 

\paragraph{$\Z_N$ and \onedtranss~} To describe the space $\Hom_{\Z_N}(\R^{N,N},\R^{N,N})$, let us begin by introducing some notation. First, every element in $\R^{N,N}$ can equivalently be described as a collection of rows. This can be written compactly as 
    \begin{align}%*}
        X = \sum_{k \in [N]} e_k x_k^*
    \end{align}%*}
    where $x_k \in \R^N$ are the rows, and $e_k$ is the $k$:th canonical unit vector.
    Correspondingly, each operator $A : \R^{N,N} \to \R^{N,N}$ decomposes into an array of $N^2$ operators $A_{\ell,k}: \R^{N}\to \R^N$:
    \begin{align}
        A(x) = \sum_{k, \ell \in [N]} e_\ell (A_{\ell,k}x_k)^*. \label{eq:decomp}
    \end{align}
    With this notation introduced, we can conveniently describe the space $\Hom_{\Z_N}(\R^{N,N},\R^{N,N})$
\begin{prop}
    Using the notation \eqref{eq:decomp}, $\Hom_{\Z_N}(\R^{N,N},\R^{N,N}) $ is characterized as the set of operators for which each $A_{\ell,k}$ is a convolutional operator. In particular, the dimension of the space is $N^2\cdot N = N^3$.
\end{prop}
\begin{proof}
    Somewhat abusing notation, let us denote the action of $\Z_N$ on $\R^N$ also as $\rho^{\tr_0}$. We then have
    \begin{align}%*}
        \rho^{\tr_0}(n)(e_kx^*) = e_k\rho^{\tr_0}(n)(x)^* 
    \end{align}%*}
    Hence,
    \begin{align}%*}
        A(\rho^{tr_0}(n)x) =  \sum_{k, \ell \in [N]} e_\ell (A_{\ell,k}\rho^{\tr_0}(n)x_k)^* \\
        \rho^{\tr_0(n)}(Ax) = \sum_{k, \ell \in [N]}  e_\ell  (\rho^{\tr_0}(n)A_{\ell,k}x_k)^*
    \end{align}%*}
    These two expression are equal for any $x$ if and only if $A_{\ell,k}\rho^{\tr_0}(n)x_k =\rho^{\tr_0}(n)A_{\ell,k}x_k$  for any $x_k$ and $\ell,k$, i.e., when every $A_{\ell,k}$ is translation equivariant, which is equivalent to each $A_{\ell,k}$ being a convolution operator.
\end{proof}
\newcommand{\F}{\mathbb{F}}
Not surprisingly, we can again calculate the projection onto the space of equivariant operators with the help of the Fourier transform. However, the Fourier transform should here only be applied partially. Let us make this explicit. Let $\calF:\C^N \to \C^N$ be the one-dimensional orthogonal Fourier transform. We then define the row-wise partial Fourier transformation $\F: \C^{N,N} \to \C^{N,N}$ through
    \begin{align}%*}
        \F\left(\sum_{k\in[N]} e_k x_k^*\right) = \sum_{k \in \N} e_k(\calF x_k)^*.
    \end{align}%*}
We can now show that $A\in \Hom_{\Z^N}(\C^{N,N},\C^{N,N})$ is related to $\F A \F^{-1}$ being 'partially diagonal', in the following sense.
\begin{lemma}
Let  $\calF:\C^N \to \C^N$ be the one-dimensional Fourier transform. Then, an operator $A$ is in $\Hom_{\Z^N}(\C^{N,N},\C^{N,N})$ if and only if, in the notation \ref{eq:decomp}, each   $\calF A_{\ell,k} \calF^{-1}$ is diagonal.
\end{lemma}
\begin{proof}
    It is not hard to show that
    \begin{align}%*}
        (\F A \F^{-1})_{\ell,k} = \calF A_{\ell,k} \calF^{-1}.
    \end{align}%*}
    Now, it is well known that $A_{\ell,k}$ is convolutional if and only if $\calF A_{\ell,k} \calF^{-1}$ is diagonal. The claim follows.
\end{proof}

By applying exactly the same argument as in Proposition \ref{prop:projection_trans}, we may now derive
\begin{prop}
    The orthogonal projection from $\Hom(\C^{N,N},\C^{N,N})$ onto $\Hom_{\Z^N}(\C^{N,N},\C^{N,N})$ is given by
    \begin{align}%*}
        \F^{-1} \diag_0(\F A\F^{-1})\F,
    \end{align}%*}
    where $\diag_0$ is the operator that kills all off-diagonal elements in all operators $A_{\ell,k}$ in the decomposition \ref{eq:decomp}.
\end{prop}
\newcommand{\trot}{\mathrm{trot}}
As before, it is not hard to realize what the space $\Hom_{\Z^N}(\R^{N,N},1)$ must be: Interpreted as a matrix $X = \sum_{k\in [N]}e_kx_k^*$, $X$ is in $\Hom_{\Z^N}(\R^{N,N},1)$ if and only if each $x_k$ is constant. Correspondingly, the projection is given by taking means along the $x$-direction, and the dimension of the space in particular is $N$.
    % describe setup
    \paragraph{$\Z_4$ and \rott~} Here, we have in fact already described the space $\Hom_{\Z_4}(\R^{N,N}, \R^{N,N})$ and the projection operator in Appendix \ref{app:experiments}. Let us here just record that the exact same idea goes through for the space $\Hom_{\Z_4}(\R^{N,N}, 1)$ -- the projection can again be calculated via explicit integration, and its elements are again characterized by having constant values on orbits of length $4$. Correspondingly, $\dim \Hom_{\Z_4}(\R^{N,N}, 1)= \tfrac{1}{4}N^2$ 

    \paragraph{\transrott~ and $\Z_{N}^2 \rtimes \Z_4$} Here, the semi-direct product structure immediately implies a relationship between the projection operators. First, let us notice that the lift $\overline{\rho}^{\trot}$ is given by $\overline{\rho}^{\mathrm{trot}}(\iota,k) = \overline{\rho}^\rot(k)\overline{\rho}^\tr(\iota)$. This fact together with the explicit integration formula
    \begin{align}%*}
        \Pi_\calE = \int_{\Z_{N}^2 \rtimes \Z_4} \overline{\rho}^{\trot}(g) \mathrm{d}\mu(g) = \frac{1}{4N^2} \sum_{k \in Z_4 , \iota \in \Z_N^2}  \overline{\rho}^\rot(k)\overline{\rho}^\tr(\iota),
    \end{align}%*}
    where we explicitly used that the Haar measure is the uniform one, implies the following
    \begin{prop}
        For $G$, let  $P_{G}$ denote the projection operators onto $\Hom_{G}(\R^{N,N},V)$ where $V$ is either $\R$ or $\R^{N,N}$. Then,
        \begin{align}%*}
            P_{\Z_N^2 \rtimes \Z^4} =  P_{\Z_4}  P_{\Z_N^2}.
        \end{align}%*}
    \end{prop}
    The above in particular means that the space $\calE$ is the intersection between the space $\calE$ for the respective groups. Specifically, this means that the space $\Hom_{\Z_N \rtimes \Z_4}(\R^{N,N},\R^{N,N})$ is the space of convolution operators whose convolutional filter is constant on orbits under $\Z_4$, and therefore (in the even case) has dimension $\frac{N^2}{4}$. For $\Hom_{\Z_N \rtimes \Z_4}(\R^{N,N},\R)$, the space is still one-dimensional -- it is given by the convolution with the constant filter (which in particular is constant on orbits)..
    
\section{Including bias terms in the framework} \label{app:bias}
Let us here show how bias terms can be incorporated into our framework by applying the standard trick of writing an affine map as a linear one on an extended space. Indeed, given an affine map $D: U \to V, x \mapsto Ax + b$ for some $A\in \Hom(U,V)$ and $b\in V$, we may define a linear map $D^\circ: U^\circ \to V^\circ$, where for a vector space $X$, we wrote $X^\circ = X \oplus \R$, through
\begin{align}%*}
    D^\circ(x,\lambda) = (Ax + \lambda \cdot b, \lambda).    
\end{align}%*}
Note that $D^\circ(x,1)=(D(x),1)$. Hence, $D^\circ$ restricted to $U \times \{1\}$ acts exactly as $D$. These maps form a linear space that we call $\Hom^\circ(U^\circ,V^\circ)$. We can further extend a representation $\rho$ of $G$ on $U$ to one on $U^\circ$ via $\rho^\circ(g)(u,\lambda)= (\rho(g)u,\lambda)$. If $\rho$ is unitary, $\rho^\circ$ also is. 

It is not hard to show that the affine map $D: X_i \to X_{i+1}$ is equivariant with respect to $\rho_i,\rho_{i+1}$ if and only if the linear map $D^\circ$ is equivariant with respect to $\rho_i^\circ$, $\rho_{i+1}^\circ$. In particular, the lifted representation $\overline{\rho}_i^\circ$ maps $\Hom^\circ(X^i,X^{i+1})$ onto itself, and $D^\circ$ is equivariant if and only if $\overline{\rho}(g)D^\circ=D^\circ$ for all $g$. Finally, the non-linearities $\sigma_i$ and loss $\ell$ can be extended to $\sigma_i^\circ : X_i^\circ \to X_{i+1}^\circ, (x,\lambda) \to (\sigma(x),\lambda)$, and $\ell: Y^\circ \times Y^\circ \to R$, $\ell^\circ((x,\lambda),(y,\kappa))= \ell(x,y)$. Equivariance of $\sigma_i$ and $\ell$ is equivalent to equivariance of $\sigma^\circ$ and $\ell^\circ$, respectively.

Hence, in short, by extending all hidden spaces $X_i$ to $X_i^\circ$, the non-linearities, losses and representations accordingly, and restricting the  $A_i$ to live in $\Hom^\circ(X_i^\circ,X_{i+1}^\circ)$, we obtain a framework that for all intents and purposes works exactly like the one we study in this paper, but also allows for the inclusion of bias terms. All arguments in the paper carry over verbatim in this extended framework.}