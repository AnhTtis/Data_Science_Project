{
    "arxiv_id": "2303.13458",
    "paper_title": "Optimization Dynamics of Equivariant and Augmented Neural Networks",
    "authors": [
        "Oskar Nordenfors",
        "Fredrik Ohlsson",
        "Axel Flinth"
    ],
    "submission_date": "2023-03-23",
    "revised_dates": [
        "2024-08-12"
    ],
    "latest_version": 4,
    "categories": [
        "cs.LG",
        "math.OC"
    ],
    "abstract": "We investigate the optimization of neural networks on symmetric data, and compare the strategy of constraining the architecture to be equivariant to that of using data augmentation. Our analysis reveals that that the relative geometry of the admissible and the equivariant layers, respectively, plays a key role. Under natural assumptions on the data, network, loss, and group of symmetries, we show that compatibility of the spaces of admissible layers and equivariant layers, in the sense that the corresponding orthogonal projections commute, implies that the sets of equivariant stationary points are identical for the two strategies. If the linear layers of the network also are given a unitary parametrization, the set of equivariant layers is even invariant under the gradient flow for augmented models. Our analysis however also reveals that even in the latter situation, stationary points may be unstable for augmented training although they are stable for the manifestly equivariant models.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.13458v1",
        "http://arxiv.org/pdf/2303.13458v2",
        "http://arxiv.org/pdf/2303.13458v3",
        "http://arxiv.org/pdf/2303.13458v4"
    ],
    "publication_venue": "v3: Completely revised manuscript: New framework for neural nets, new main result (involving compability condition), new experiments, new author. v2: Revised manuscript. Mostly small edits, apart from new experiments (see Appendix E)"
}