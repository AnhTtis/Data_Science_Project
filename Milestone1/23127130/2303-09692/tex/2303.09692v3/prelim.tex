\section{Preliminaries}
\label{sec:prelim}
\subsection{Unifying Theories of Programming}
In UTP~\cite{Hoare1998,Woodcock2004}, the meaning (denotational semantics) of programs is given as predicates, called ``programs-as-predicates''~\cite{Hoare1984}. In this approach, the alphabetised relational calculus~\cite{Woodcock2004}, a combination of standard predicate calculus operators and Tarski's relation algebra~\cite{Tarski1941}, is used as the basis for its semantic model to denote programs as binary relations between initial observations and their subsequent observations. An alphabetised relation is an alphabet-predicate pair $\left(\alpha P, P\right)$ where the accompanying alphabet of $P$, $\alpha P$, is composed of undashed variables ($x$) and dashed variables ($x'$), representing observations made initially and subsequently. %Additionally, all free variables of $P$ are contained in $\alpha P$. 
For example, a program $\left(x' := x + 1\right)$ with two observable variables $x$ and $y$ can be modelled as a relational predicate $\left(x' = x + 1 \land y' = y\right)$ with its alphabet $\{x,x',y,y'\}$.

Alphabetised predicates are presented in this representation of UTP through alphabetised expressions $[V,S] \uexpr$, parametric over the value type $V$ and the observation space $S$ and defined as total functions $S \fun V$. Predicates\footnote{In the rest of the paper, we use expressions, predicates, and relations to refer to alphabetised counterparts for simplicity.} are boolean expressions: $[S] \upred \defs [\bool, S] \uexpr$, an expression whose value type is boolean. Relations are predicates over a product space: $[S_1, S_2] \urel \defs [S_1 \cross S_2] \upred$, where $S_1$ and $S_2$ are the initial and final observation space, corresponding to undashed variables (input alphabet) and dashed variables (output alphabet). {Here $\urel$ means the UTP relations.} \emph{Homogeneous} relations have the same initial and final observation space: $[S] \hrel \defs [S, S] \urel$.

The denotational semantics of a sequential program is given as relations by the composition of constructors, including conditional, assignment, skip, sequential composition, and nondeterministic choice. These constructors are defined below.
\begin{definition}[Constructs of sequential programs]
    \label{def:utp_relation}
    \begin{align*}
        & \left(P \lhd b \rhd Q\right) \defs \left(b \land P\right) \lor \left(\lnot b \land Q\right)\tag*{(conditional)} \label{def:ucond} \\
        & \left(x :=_A e\right) \defs \left(x' = e \land w' = w \right)\tag*{(assignment)} \label{def:uassign} \\
        & \II \defs \left({v' = v}\right) \tag*{(skip)} \label{def:uskip} \\
        & P ; Q \defs \left(\exists s_0 @ P[s_0/s'] \land Q[s_0/s]\right)\tag*{(sequential composition)} \label{def:useq} \\
        & P \sqcap Q \defs \left(P \lor Q\right) \qquad{\text{ if } \alpha P = \alpha Q}\tag*{(nondeterminism)} \label{def:unondeter}
    \end{align*}
\end{definition}

Particularly, we need to emphasise the type of programs and their alphabets. In \ref{def:ucond}, {$b$ is type $[S_1]\upred$, $P$ and $Q$ are of type $[S_1,S_2]\urel$}, and $\alpha b \subseteq \alpha P = \alpha Q$. In \ref{def:uassign}, $A$ is the observation space of a program, including variable $x$ and a set $w$ of other variables. We also use a simple syntax $w'=w$ here to denote conjunctions of equations over each variable in $w$. For example, if $w = \{y, z\}$, then $\left(w' = w\right) \defs \left(y' = y \land z' = z\right)$. The subscript $A$ in $:=_A$ is usually omitted because it can automatically be derived from its context. Skip $\II$ \ref{def:uskip} is a special assignment where no variable changes {(here $v$ denotes the set of all variables)}, so the observation space stays the same. In sequential composition $P ; Q$ \ref{def:useq}, two relations $P$ of type $[S_1, S_2]\urel$ and $Q$ of type $[S_2, S_3]\urel$ are composed because the output alphabet $S_2$ of $P$ is the same as the input alphabet $S_2$ of $Q$. The relational composition gives a program of type $[S_1, S_3]\urel$ with $s_0:S_2$ denotes the entire state, $P[s_0/s']$ for the substitution of the final observation $s'$ of $P$ by $s_0$, and $Q[s_0/s]$ for the substitution of the initial observation $s$ of $Q$ by $s_0$. Nondeterministic choice $P \sqcap Q$ \ref{def:unondeter} is simply a disjunction of relations {if they have the same alphabets. The introduction of the new notation $\sqcap$ emphasises this condition.}

UTP uses refinement to deal with program development or correctness. A specification $S$ is refined by a program $P$, denoted as $S \refinedby P$, if and only if that $P$ implies $S$ is universally closed. For example, $\left(x := x + 1\right)$ is a refinement of $\left(x' > x\right)$ because for any $x$ and $x'$, $\left(x'=x+1\right) \implies \left(x' > x\right)$. Relations of type $[S_1, S_2]\urel$, for any given $S_1$ and $S_2$, are partially ordered by $\refinedby$ where $\ufalse$ and $\utrue$, special relations whose predicates are $\pfalse$ and $\ptrue$, are at its extremes: $\utrue \refinedby P \refinedby \ufalse$.

\subsection{Isabelle/UTP}
Isabelle/UTP~\cite{Foster2020} provides a shallow embedding of UTP's alphabetised relational calculus on top of Isabelle/HOL. In Isabelle/UTP, variables are modelled as algebraic structures using Lenses~\cite{Foster2009,Foster2016} to represent observations. Each observable variable $x$ is a lens ($\lens{\mathcal{V}}{\mathcal{S}}$), equipped with a pair of functions $\lget{x}: \mathcal{S} \fun \mathcal{V}$ and $\lput{x}: \mathcal{S} \fun \mathcal{V} \fun \mathcal{S}$, to query and update a view (of type $\mathcal{V}$) of an observation space (of type $\mathcal{S}$).
In this model, an alphabetised predicate $\left(x = y + 1\right)$ with two observable variables $x$ and $y$ is expressed as $\left(\lambda s. \lget{x} s = \lget{y} s + 1\right)$. A relation is a predicate over a product space, manipulated through a product lens $S_1 \lprod S_2$. The early relational example $\left(x' := x + 1\right)$, therefore, can be expressed as 
%
\begin{align*}
\lambda s. \lget{x} \left(\lget{\lsnd} s\right) = \lget{x}\left(\lget{\lfst} s\right) + 1 \land \lget{y}\left(\lget{\lsnd} s\right) = \lget{y}\left(\lget{\lfst} s\right) \tag*{[Lens representation]} \label{eqn:lens_representation} 
\end{align*}
%
where $\lfst$ and $\lsnd$ are the lenses to project the first and the second element of a product space. By substituting the state $s$ with a pair $(s, s')$, this expression can be simplified to 
\begin{align*}
    \lambda (s, s'). \lget{x} s' = \lget{x} s + 1 \land \lget{y} s' = \lget{y} s \tag*{[Simplified lens representation]} \label{eqn:simp_lens_representation}
\end{align*}
%
However, writing UTP expressions this way is tedious and not very useful and intuitive for good programming practice because too many implementation details are presented. For this reason, Isabelle/UTP implemented a lifted parser to provide a transparent conversion between the lens's representation and the programming syntax like $\left(x' := x + 1\right)$. We denote this representation of UTP expressions as $\usexpr{expr}$, such as $\usexpr{x' : = x + 1}$, which is converted to \ref{eqn:lens_representation}.

In Isabelle/UTP, we use $\vv$ to denote the universe alphabet of a program. In other words, it is the set of all observable variables. We also use $\vv'$ to denote the set of all dashed observable variables. For the previous example, $\vv$ denotes $\{x,y\}$ and $\vv'$ denotes $\{x', y'\}$.

\subsection{Probabilistic predicative programming}
Predicative programming~\cite{Hehner1984a,Hehner1984}, {or programs-as-predicates~\cite{Hehner1993}}, describes programs using first-order semantics or relational semantics as boolean expressions (predicates). A program has its input denoted by undashed variables and output denoted by dashed variables. Predicative programming also uses refinement for program correctness.

Probabilistic predicative programming~\cite{Hehner2004,Hehner2011} generalises predicative programming from boolean to probabilistic. Notations are introduced for probabilistic programming, such as skip, assignment, conditional choice, probabilistic choice, sequential composition (conditional probability), parallel composition (joint probability), and recursion. Except for parallel composition, these constructors deal with probabilistic programs whose outputs are distributions or distribution programs. Parallel composition can deal with probabilistic programs whose outputs might not be distributions (non-distribution programs), and uses normalisation to give a distribution program. 

This programming supports the subjective Bayesian approach through parallel composition. From a given distribution program, we can learn a new fact by placing the fact in parallel with the distribution program to allow beliefs to be updated. 

To reason about the termination of loops, a time variable is introduced to count iterations. This gives more information (time) than just termination~\cite{Hehner1999}. In this programming, the expected value of a number expression $e$ according to a distribution program $P$ is just the sequential composition of $P$ and $e$. If $e$ is a boolean expression, the sequential composition gives the probability that $e$ is valid after the execution of $P$. 
With the time variable, the programming allows reasoning about the average termination time. For example, on average, it takes 
% Moreover, the expected number of iterations is given by $\Sigma_n^{\infty} n * 2 ^ {(-n)} = 2$
two flips of a fair coin to see heads or tails. 
The termination probability of a loop (\isopbf{while} $b$ \isopbf{do} $P$) can be specified using the sequential composition of the solution (of the loop) and the negation of the loop condition ($\lnot b$). If the result is 1, it means the loop almost surely terminates. If the result is not 1, the loop may diverge.

\subsection{Complete lattices and fixed-point theorems}
% \subsubsection{Partially ordered sets}
% A partially ordered set (poset) $(X,\leq)$ satisfies:
% \begin{align*}
%     & a \leq a & \tag*{[reflexive]} \label{law:poset_reflexive} \\
%     &a \leq b \land b \leq a \implies a = b & \tag*{[antisymmetric]} \label{law:poset_antisym} \\
%     &a \leq b \land b \leq c \implies a \leq c& \tag*{[transitive]} \label{law:poset_transitive}
% \end{align*}
% 
% \subsubsection{Total ordered sets}
% A totally ordered set $(X,\leq)$ is a poset but with any two elements in $X$ are comparable.
% \begin{align*}
% 	\forall a, b \in X \bullet a \leq b \lor b \leq a \tag*{[total or linear]} \label{law:total}
% \end{align*}
% 
% \subsubsection{Chains}
% 
% \begin{definition}[Chain]\label{def:chain}
%     $(S, \leq)$ is called a \textbf{chain} (or a \textbf{totally ordered subset}) if $S$ is a subset of $X$ (in a poset $(X, \leq)$) and $(S, \leq)$ is totally ordered \ref{law:tatal}.
% \end{definition}
% 
% \begin{definition}[Chain-complete]
% 	A poset $X$ is \emph{chain-complete}~\cite{Moschovakis2006} or \textbf{inductive} if every chain in $X$ has a least upper bound.
% \end{definition}
% 
% \begin{definition}[$\omega$-chain-complete]
% 	A poset $X$ is \emph{$\omega$-chain-complete}~\cite{Mashburn1983} if every countable chain in $X$ has a supremum or least upper bound.
% \end{definition}
% 
% \begin{definition}[$\omega$-chain-continuous]
% 	A function $f$ from $X$ to a poset $Q$ is \emph{$\omega$-chain-continuous}~\cite{Mashburn1983} if every non-empty countable chain $C$ in $X$ has a supremum and $f\left(\thsup{X} C\right) = \thsup{Q} f(C)$.
% \end{definition}
% 
% \paragraph{Transfinite iterations}
% \textbf{Ordinal} numbers (positions or ranks in a set) are introduced to distinguish with \textbf{cardinal} numbers (size of a set) to deal with infinite sets. For finite sets, ordinal and cardinal coincide.\footnote{\url{https://en.wikipedia.org/wiki/Ordinal_number\#Ordinals_extend_the_natural_numbers}} 
% \begin{quotation}
% Ordinal numbers are intimately linked with \emph{well-ordered sets} (\emph{totally ordered sets} in which every non-empty subset has a least element). Ordinals can be used to label elements of any well-ordered sets. 
% \end{quotation}
% There are three types of ordinal numbers: 
% \begin{inparaenum}
% \item zero;
% \item a successor ordinal (the least ordinal number greater than the ordinal);
% \item a limit ordinal.
% \end{inparaenum}
% 
% $\omega$ is the smallest limit ordinal, and larger than any natural numbers. It is the least upper bound of natural numbers. $\omega$ is also the smallest infinite ordinal. 
% 
% \begin{definition}[Transfinite induction]
% Extension of general mathematical induction to well-ordered sets.  
% \end{definition}

A partially ordered set (poset) $(X, \leq)$ is a complete lattice if every subset of $X$ has a supremum and an infimum.
\begin{align*}
	& \forall A \subseteq X \bullet \left(\thinf{} A\right) \in X  \tag*{(Inf exists)} \label{law:inf_exists} \\ 
	& \forall A \subseteq X \bullet \left(\thsup{} A\right) \in X  \tag*{(Sup exists)} \label{law:sup_exists}
\end{align*}

We use a tuple $\left(X, \leq, <, \bot, \top, \tinf, \tsup, \thninf, \thnsup\right)$ to represent a complete lattice $\left(X, \leq\right)$ with a strict binary relation $<$, the bottom element $\bot$, the top element $\top$, the infimum $\tinf$ of two elements, the supremum $\tsup$ of two elements, the infimum $\thninf$ of a (finite or infinite) set of elements, and the supremum $\thnsup$ of a (finite or infinite) set of elements. 

A complete lattice satisfies more laws below. 
\begin{align*}
	& x \leq x \tag*{(reflexive)} \label{law:reflexive} \\
	& x \leq y \land y \leq z \implies x \leq z \tag*{(transitive)} \label{law:transitive} \\
	& x \leq y \land y \leq x \implies x = y	 \tag*{(antisym)} \label{law:antisym}\\
	%
	& x \sqcap y \leq x \tag*{(inf\_le1)} \label{law:inf_le1} \\
	& x \sqcap y \leq y \tag*{(inf\_le2)} \label{law:inf_le2} \\
	& x \leq y \land x \leq z \implies x \leq y \sqcap z \tag*{(inf\_greatest)} \label{law:inf_greatest} \\
	& \left(x \leq y\right) \equiv \left(x \sqcap y = x\right) \tag*{(inf\_iff)} \label{law:inf_iff} \\
	%
	& x \leq x \sqcup y \tag*{(sup\_ge1)} \label{law:sup_ge1} \\
	& y \leq x \sqcup y \tag*{(sup\_ge2)} \label{law:sup_ge2} \\
	& y \leq x \land z \leq x \implies y \sqcup z \leq x \tag*{(sup\_least)} \label{law:sup_least} \\
	& \left(x \leq y\right) \equiv \left(x \sqcup y  = y\right) \tag*{(sup\_iff)} \label{law:sup_iff} \\
	%
	& x \in A \implies \thinf{}A \leq x \tag*{(Inf\_lower)} \label{law:Inf_lower} \\
	& (\forall x. x \in A \implies z \leq x) \implies z \leq \thinf{} A \tag*{(Inf\_greatest)} \label{law:Inf_greatest} \\
	%
	& x \in A \implies x \leq \thsup{}A  \tag*{(Sup\_upper)} \label{law:Sup_upper} \\
	& (\forall x. x \in A \implies x \leq z) \implies \thsup{} A \leq z\tag*{(Sup\_least)} \label{law:Sup_least} \\
	%
	& \thinf{} \{\} = \top  \tag*{(Inf\_empty)} \label{law:Inf_empty} \\
	& \thsup{} \{\} = \bot  \tag*{(Sup\_empty)} \label{law:Sup_empty}
\end{align*}

% \textbf{Pre-order} satisfies Laws~\ref{law:reflexive} and \ref{law:transitive}. A \textbf{partial order} is a pre-order that satisfies Law~\ref{law:antisym}. A \textbf{total} partial order satisfies Law~\ref{law:total}.
% 
% \begin{definition}[Directed sets]
% A nonempty subset $D$ of $X$ in a pre-order $(X, \leq)$ is \textbf{directed} if 
% \begin{align*}
% 	\forall x, y \in D \bullet (\exists z \in D \bullet x \leq z \land y \leq z) \tag*{[Directed set]} \label{law:directed}
% \end{align*}
% $\emptyset$ is not a directed set.
% \end{definition}
% 
% \begin{thm}\label{thm:total_directed}
% Every totally ordered set is directed.
% \end{thm}
% \begin{proof}
% Suppose $(X,\leq)$ is a totally ordered set, and so for any two elements $a, b \in X$, then 
% 
% \begin{align*}
% 	& a \leq b \lor b \leq a	\tag*{[Law~\ref{law:total}]}
% 	\\ 
% 	& \implies
% 	\left(\exists z \in X | z = max(a,b) \bullet a \leq z \land b \leq z\right)
% \end{align*}
% \qed
% \end{proof}
% 
% \begin{thm}
% 	Every complete lattice is \emph{chain-complete}.
% \end{thm}
% 
% \begin{proof}
% 	Suppose $(X, \leq)$ is a complete lattice, then according to Law~\ref{law:sup_exists} every subset $A$ of $X$ has both a supremum (or least upper bound) and an infimum. And so every chain (also a subset of $X$) in $X$ has a supremum. This complete lattice is, therefore, \textbf{chain-complete}.
% \end{proof}

%\begin{definition}[Monotonic functions]
%$f: X \to X$ is monotonic if
%\begin{align*}
%	\forall x, y \in X \bullet (x \leq y \implies f(x) \leq f(y)) \tag*{[Monotonic]} \label{law:monotonic}
%\end{align*}
%\end{definition}

Monotonic and antimonotonic functions in order theory are characterised using $\mono$ and $\antimono$ defined below.
\begin{definition}[Monotone and anti-monotone]
    Provided $(X, \leq)$ and $(X', \leq')$ are posets and $f$ is a function of type $X \fun X'$, then
    \begin{align*}
        & \mono(f) \defs \forall x @ \forall y @ x \leq y \implies f(x) \leq' f(y) \tag*{(monotone)} \label{def:mono}\\ 
        & \antimono(f) \defs \forall x @ \forall y @ x \leq y \implies f(y) \leq' f(x) \tag*{(anti-monotone)} \label{def:antimono}
    \end{align*}
\end{definition}

Ascending and descending chains are monotonic and antimonotonic functions whose domain is natural numbers. 
\begin{definition}[Chains]
    Provided $(X, \leq)$ is a {complete lattice} and $f$ is a function of type $\nat \fun X$, then
    \begin{align*}
        & \incseq(f) \defs \mono(f) \tag*{(ascending chain)} \label{def:incseq}\\ % in Isabelle, mono function has a different type signature from incseq where $f$ is (nat => 'a::order) 
        & \decseq(f) \defs \antimono(f) \tag*{(descending chain)} \label{def:decseq}
    \end{align*}
\end{definition}
{We particularly define $\incseq$ and $\decseq$ to be over complete lattices which we are interested in this paper. This is to simplify the specification of premises in lemmas and theorems because $\incseq$ and $\decseq$ impose a type restriction to complete lattices directly. Otherwise, we need additional premises if we use the more general $\mono$ and $\antimono$.}

We show the application of a monotonic function $f$ to an ascending chain $c$ is also an ascending chain.
\begin{thm}
    We fix $c:\nat \fun X$ and $f: X \fun Y$, then 
    $\incseq(c) \land \mono(f) \implies \incseq(\lambda n @ f(c(n)))$
\end{thm}

We show the application of a monotonic function $f$ to a descending chain $c$ is also a descending chain.
\begin{thm}
    We fix $c:\nat \fun X$ and $f: X \fun Y$, then 
    $\decseq(c) \land \mono(f) \implies \decseq(\lambda n @ f(c(n)))$
\end{thm}

If $f$ is an ascending or descending chain, its limit is the supremum or infimum of the chain.
% LIMSEQ_SUP in Isabelle/HOL
% LIMSEQ_INF in Isabelle/HOL
\begin{thm}[Limit as supremum and infimum]
    \label{thm:limit_as_sup_inf}
    Provided $(X, \leq)$ is a complete lattice and also totally ordered, and we fix $f:\nat \fun X$, then 
    \begin{align*}
        & incseq(f) \implies f \tendsto \left({\thnsup n @ f\left(n\right)} \right) \tag*{(limit as supremum)}\label{thm:limit_as_sup}\\
    & decseq(f) \implies f \tendsto \left({\thninf n @ f\left(n\right)} \right)\tag*{(limit as infimum)}\label{thm:limit_as_inf}
    \end{align*}
\end{thm}
Here we use $f \tendsto v$ to denote the limit of $f$ is $v$: ${\displaystyle \lim_{n \to \infty} f(n) = v}$. The definition of the limit of a sequence is given below.
\begin{definition}[Limit of a sequence]\label{def:tendsto}
    A sequence $f$ converges to $v$ if and only if
    \begin{align*}
        & \forall \varepsilon:\real > 0 \bullet \exists N:\nat \bullet \forall n \geq N \bullet |f(n) - v| < \varepsilon
    \end{align*}
\end{definition}

% \begin{thm}\label{thm:monotonic_chain}
% Suppose $(X, \leq$) and $(X', \leq'$) are complete lattice. $f: X \to X'$ is monotonic and $S \subseteq X$ is a chain in $X$, then 
% \begin{align*}
% 	& f(S) := \left\{f(d). d \in S\right\} \tag*{[$f(S)$ is also a chain in $X'$]} \label{law:monotonic_chain_is_chain} \\
% 	&\thsup{X'} f (S) \leq' f(\thsup{X} S) \tag*{[Sup ordered]} \label{law:Sup_monotonic}
% \end{align*}
% \end{thm}
% 
% \begin{proof}
% For $a$ and $b$ in $S$, from Law~\ref{law:chain}, we know that $a \leq b \lor b \leq a$.
% If $a \leq b$, then $f(a) \leq f(b)$ as $f$ is \ref{law:monotonic}. If $b \leq a$, then $f(b) \leq f(a)$ as $f$ is \ref{law:monotonic}. So $F(S)$ is a \ref{law:chain}.
% 
% % Since $(X, \leq$) is a complete lattice, then $\thsup{} S \in X$ by Law~\ref{law:sup_exists}.
% %Since $(X', \leq'$) is a complete lattice, then $\thsup{} \left\{f(d). d \in S\right\} \in X'$  by Law~\ref{law:sup_exists}.
% 
% \begin{align*}
% 	& \forall x \in S \bullet x \leq \thsup{X} S \tag*{[Law~\ref{law:Sup_upper}]}\\
% 	& \implies \forall x \in S \bullet f(x) \leq' f(\thsup{X} S) & \tag*{[$f$ is \ref{law:monotonic}]}\\
% 	& \implies \left(\forall fx \in f(S) \bullet fx \leq f(\thsup{X} S)\right) \tag*{[Definition of $f(S)$~\ref{law:monotonic_chain_is_chain}]}\\
% 	& \implies \thsup{X'} f (S) \leq' f(\thsup{X} S) \tag*{[Law~\ref{law:Sup_least}]}
% \end{align*}
% \qed
% \end{proof}

% \begin{definition}[Scott continuity]
% Suppose $(X, \leq$) and $(X', \leq'$) are complete lattice. $f: X \to X'$ is monotonic. $f$ is \textbf{continuous	} if, for every non-empty chain $S \subseteq X$,
% \begin{align*}
% 	f\left(\thsup{X} S\right) = \thsup{X'} \left\{f(d). d \in S\right\} \tag*{[Continuous]} \label{law:continuity}
% \end{align*}
% \end{definition}

% \begin{thm}
% 	A subset $S$ of a complete lattice $(X, \leq)$: $\{S_n | n \in \nat \}$, is an increasing countable chain~\cite{Moschovakis2006,Pitts2012} or $\omega$-chain~\cite{Winskel1993TheFS} if $S_0 \leq S_1 \leq S_2 \leq ...$. This chain has a supremum because of complete lattice, denoted by $\thsup{$n \geq 0$} \{S_n | n \in \nat \}$, or $\thsup{$n \geq 0$} S_n$, or $\bigsqcup\limits_{n = 0}^{\infty} S_n$
% \end{thm}

\begin{thm}[Knasterâ€“Tarski fixed-point theorem]
    \label{thm:tarski_fixed_point}
    Provided $(X, \leq)$ is a complete lattice and $F: X \fun X$ is monotonic, the set of fixed points of $F$ also forms a complete lattice. The least fixed point is the infimum of the pre-fixed points. 
    \begin{align*}
        &\thlfp{}~F \defs \thninf{} \left\{u : X | F(u) \leq u\right\} \tag*{(least fixed point)} \label{def:tarski_lfp}
    \end{align*}

The greatest fixed point is the supremum of the post-fixed points. 
    \begin{align*}
        &\thgfp{}~F \defs \thnsup{} \left\{u : X | u \leq F(u)\right\} \tag*{(great fixed point)} \label{def:tarski_gfp}
    \end{align*}
\end{thm}

\begin{definition}[Scott continuity~\cite{Abramsky1995}]
Suppose $(X, \leq$) and $(X', \leq'$) are complete lattices. A function $F: X \to X'$ is \textbf{Scott-continuous} or \textbf{continuous} if, for every non-empty chain $S \subseteq X$,
\begin{align*}
	F\left(\thsup{X} S\right) = \thsup{X'} F(S) \tag*{(continuous)} \label{law:continuity}
\end{align*}
Here we use $F(S)$ to denote the set $\left\{d \in S @ F(d)\right\}$, the relational image of $S$ under $F$ or the range of $F$ domain restricted to $S$.
\end{definition}

In the original definition of Scott continuity, both $X$ and $X'$ are directed-complete partial orders (dcpo). We fix them to be complete lattices because every complete lattice is a dcpo~\cite{Abramsky1995}, and we only consider complete lattices in this paper. If $X$ and $X'$ are identical lattices, the subscript of $\thsup{}$ in Definition~\ref{law:continuity} can be omitted.

\begin{thm}[Monotonicity]
    \label{thm:cont_mono}
   A continuous function is also monotonic. 
\end{thm}

\begin{thm}[Kleene fixed-point theorem]
    \label{thm:kleene_fixed_point_theorem}
    Provided $(X, \leq)$ is a complete lattice with a least element $\bot$ and a top element $\top$, and $F: X \fun X$ is continuous, then $F$ has a least fixed point $\thlfp{}~F$ and a greatest fixed point $\thgfp{}~F$.
    \begin{align*}
        \thlfp{}~F &= \thsup{$n \geq 0$} F^n(\bot) \tag*{(least fixed point)} \label{def:kleene_lfp}\\
        \thgfp{}~F &= \thinf{$n \geq 0$} F^n(\top)\tag*{(greatest fixed point)} \label{def:kleene_gfp}
    \end{align*}
    Here we use $\thsup{$n \geq 0$} F^n(\bot)$ to denote $\thnsup\left\{n : \nat @ F^n(\bot)\right\}$
\end{thm}

\begin{proof}
    We prove $\thlfp{}~F$ is a fixed point first and then prove $\thlfp{}~F$ is the least one.
\begin{align*}
& F \left(\thlfp{}~F\right) \\
 = & \quad \cmt{Definition~\ref{def:kleene_lfp}} \\
& F \left(\thsup{$n \geq 0$} F^n(\bot)\right) \\
 = & \quad \cmt{Continuity Definition~\ref{law:continuity}} \\
& \thsup{$n \geq 0$} F \left(F^n(\bot)\right) \\
= & \quad \cmt{Defintion of $F^n$: $F(F^m(x)) = F^{m+1}(x)$} \\
& \thsup{$n \geq 0$} \left( F^{n+1}(\bot)\right) \\
= & \quad \cmt{Rewrite index} \\
& \thsup{$m \geq 1$} \left( F^{m}(\bot)\right) \\
= & \quad \cmt{Law~\ref{law:sup_iff} and $\bot$ is the least element} \\
& \bot \sqcup \left(\thsup{$m \geq 1$} \left( F^{m}(\bot)\right)\right) \\
= & \quad \cmt{Definition of $F^0$: $F^0(\bot)=\bot$} \\
& F^{0}(\bot) \sqcup \left(\thsup{$m \geq 1$} \left( F^{m}(\bot)\right)\right) \\
= & \quad \cmt{Definition of $\thsup{$m \geq 0$}$} \\
& \thsup{$m \geq 0$} \left( F^{m}(\bot)\right) \\
= & \quad \cmt{Rewite index} \\
& \thsup{$n \geq 0$} \left( F^{n}(\bot)\right) \\
= & \quad \cmt{Definition~\ref{def:kleene_lfp}} \\
& \thlfp{}~F
%
\end{align*}
So $\thlfp{}~F$ is a fixed point of $F$.

Suppose $fb$ is also a fixed point of $F$ that is, $F (fb) = fb$.
\begin{align*}
& \quad \cmt{$\bot$ is the least element} \\
& \bot \leq fb \\
\implies & \quad \cmt{$F$ is continuous and so is monotonic by Theorem~\ref{thm:cont_mono}} \\
& F (\bot) \leq F (fb) \\
\implies & \quad \cmt{$F(fb)=fb$} \\
& F (\bot) \leq fb \\
\implies & \quad \cmt{$F^{2} (\bot) = F\left(F(\bot)\right)$ and $F$ is monotonic} \\
& F^{2} (\bot) \leq F (fb)\\
\implies & \quad \cmt{$F(fb)=fb$} \\
& F^2 (\bot) \leq fb \\
& ... \\
\implies & \quad \cmt{Induction} \\
& F^{n} (\bot) \leq fb \\
\implies & \quad \cmt{Law~\ref{law:Sup_least}} \\ 
& \thsup{$n \geq 0$}\left(F^n(\bot)\right) \leq fb \\
= & \quad \cmt{Definition~\ref{def:kleene_lfp}} \\
& \thlfp{}~F \leq fb
\end{align*}
% \begin{align*}
% & F (fb) = fb \\
% \implies & \quad \cmt{$\bot \leq fb$ and monotonicy of $F$ Theorem~\ref{thm:cont_mono}} \\
% & \bot = F^{0} (\bot) \leq F (fb) = fb \\
% \implies & \quad \cmt{$F^{1} (\bot) = F(\bot)$} \\
% & F^{1} (\bot) \leq F (fb) = fb \\
% \implies & \quad \cmt{$F^{2} (\bot) = F\left(F(\bot)\right)$} \\
% & F^{2} (\bot) \leq F (fb) = fb \\
% & ... \\
% \implies & \quad \cmt{Induction} \\
% & F^{n} (\bot) \leq F (fb) = fb \\
% \implies & \quad \cmt{Law~\ref{law:Sup_least}} \\ 
% & \thsup{$n \geq 0$}\left(F^n(\bot)\right) \leq fb \\
% = & \quad \cmt{Definition~\ref{def:kleene_lfp}} \\
% & \thlfp{}~F \leq fb \\
% \end{align*}
%
So $\thlfp{}~F$ is the least fixed point.

Similarly, we prove $\thgfp{}~F$ is a fixed point of $F$ and is also the greatest.
\end{proof}
% Reference: ~\cite{Dubut2022}. %\citep{Dubut2022}.

\subsection{Summation over topological space}
Summation considered in this paper could range over an infinite set, called infinite sums. We use corresponding theories in Isabelle/HOL to deal with convergence and infinite sums. 

We say a function $f$ is \emph{summable} on a (potentially infinite) set $A$, denoted as $\summable(f, A)$ if the sum of $f$ on $A$ converges to a particular value. The convergence is expressed as the existence of a limit of $f$ over finite subsets $B$ of $A$ when $B$ is approaching $A$. In Isabelle/HOL, the limit is generalised to arbitrary topological space using filters~\cite{Hoelzl2013}. Its definition is parametrised over two filters. To the infinite sums, they are the open neighbourhood filter, interpreted as ``for all points in some open neighbourhood of a point'' and the subset inclusion ordered at-top filter, interpreted as ``for sufficiently large finite subsets when it approaches its top $A$''. The infinite sums of $f$ over $A$, denoted as $\infsum x \in A @ f(x)$, is the limit if $\summable(f, A)$ and 0 otherwise. The definitions of summable and finite sums can be found in Isabelle/HOL.

We list some laws of summation below.
\begin{thm}
    \label{thm:summation}
    \begin{align*}
        & c \neq 0 \land \summable(f, A) \implies \summable\left(\lambda x @ f(x)/c, A\right) \tag*{(division by constant summable)} \label{thm:summation_cdiv_summable} \\ 
        & c \neq 0 \land \summable(f, A) \implies \infsum x \in A @ f(x)/c = \left(\infsum x \in A @ f(x)\right)/c  \tag*{(division by constant)} \label{thm:summation_cdiv} \\ 
        & \summable(f, A) \implies \summable\left(\lambda x @ f(x)*c, A\right) \tag*{(multiplication of constant summable)} \label{thm:summation_cmult_right_summable} \\ 
        & \summable(f, A) \implies \infsum x \in A @ f(x)*c = \left(\infsum x \in A @ f(x)\right)*c \tag*{(multiplication of constant)} \label{thm:summation_cmult_right} \\ 
        & \summable(f, A) \implies \summable\left(\lambda x @ c*f(x), A\right) \tag*{(multiplication of constant summable)} \label{thm:summation_cmult_left_summable} \\ 
        & \summable(f, A) \implies \infsum x \in A @ c*f(x) = c*\left(\infsum x \in A @ f(x)\right) \tag*{(multiplication of constant)} \label{thm:summation_cmult_left} \\ 
        & \summable(f, A) \land \summable(g, A) \implies \summable\left(\lambda x @ f(x)+g(x), A\right) \tag*{(addition summable)} \label{thm:summation_add_summable} \\ 
        & \summable(f, A) \land \summable(g, A) \implies \infsum x \in A @ f(x)+g(x) = \infsum x \in A @ f(x) + \infsum x \in A @ g(x) \tag*{(addition)} \label{thm:summation_add} \\ 
        & \summable(f, A) \land \summable(g, A) \implies \infsum x \in A @ f(x)-g(x) = \infsum x \in A @ f(x) - \infsum x \in A @ g(x) \tag*{(subtraction)} \label{thm:summation_minus} 
    \end{align*}
\end{thm}
