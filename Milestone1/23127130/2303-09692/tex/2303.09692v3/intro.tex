\section{Introduction}
\label{sec:intro}

\paragraph{\textbf{Motivations}}
Probabilistic programming combines general computer programming, statistical inference, and formal semantics to help systems make decisions when facing uncertainty. Probabilistic programs are ubiquitous but are particularly important in machine intelligence applications. Probabilistic algorithms have been used in practice for a long time in autonomous robots, self-driving cars, and artificial intelligence. There are tools for automated formal verification, particularly model checkers. However, many challenges remain, such as the following. What is the mathematical meaning of a probabilistic program? Are probabilistic programming languages expressive enough to capture rich features in real-world applications, such as epistemic and aleatoric uncertainty, discrete and continuous distributions, and real-time? How can we compare two programs? How can we implement a probabilistic specification as a probabilistic program? Can formal verification be largely automated and scaled to large systems without sacrificing accuracy? Does a probabilistic program almost surely terminate? What is the expected runtime of this program? The work presented in this paper, probabilistic unifying relations (ProbURel), takes a step towards our vision to tackle these challenges.

Uncertainty is essential to cyber-physical systems, particularly in autonomous robotics where we work. Such systems are subjected to various uncertainties, including real-world environments and physical robotic platforms, which present significant challenges for robots. Robots are usually equipped with probabilistic control algorithms to deal with these uncertainties. For example, a modern robot may use SLAM for localisation and mapping, the value iteration algorithm for probabilistic planning~\cite{Thrun2005}.

Probabilistic algorithms are intrinsically more difficult to program and analyse than non-probabilistic algorithms. For a specific input, the output of a probabilistic algorithm might be a distribution of possible outputs, not just a single output. Outputs with low probabilities are rare, which makes testing difficult. Probabilistic behaviour may be challenging to capture and model correctly because assumptions may not be obvious and may be left implicit. We need more precision in understanding an autonomous robot that may impose safety-critical issues on humans and their environments. One approach to addressing these challenges is providing probabilistic programs with formal syntax, semantics, and verification techniques to ensure they behave as expected in a real-world environment.

%  \paragraph{\textbf{Our probabilistic vision}}
%  Recently, we presented a probabilistic extension~\cite{Ye2022} to RoboChart~\cite{Miyazawa2019}, a state machine-based DSL for robotics, to allow the modelling of probabilistic behaviour in robot control software. 
%  RoboChart~\cite{Miyazawa2019,Ye2022} is a core notation in the RoboStar%
%  \footnote{%
%    \url{robostar.cs.york.ac.uk}.%
%  } framework~\cite{Cavalcanti2021} that brings modern modelling and verification technologies into software engineering for robotics. In this framework, three key elements are models, formal mathematical semantics for models, and automated modelling and verification tool support.  
%  RoboChart is a UML-like architectural and state machine modelling notation featuring discrete time and probabilistic modelling. It has formal semantics: state machines and architectural semantics~\cite{Miyazawa2019} based on the CSP process algebra~\cite{Hoare1985,Roscoe2011} and time semantics~\cite{Miyazawa2019} based on \emph{tock}-CSP~\cite{Baxter2021,Roscoe2011}. CSP is a formal notation to describe concurrent systems where processes interact using communication. In the framework, robot hardware and control software are captured in robotic platforms, and controllers of RoboChart. The environment is captured in RoboWorld~\cite{Cavalcanti2021a} whose semantics is based on \textit{\textsf{CyPhyCircus}}~\cite{Foster2020a}, a hybrid process algebra, because of the continuous nature of the environment.
%  
%  While RoboChart and RoboWorld are high-level specification languages, RoboSim~\cite{Cavalcanti2019} is a cycle-based simulation-level notation in the RoboStar framework. RoboSim also has semantics in CSP and \emph{tock}-CSP. A RoboSim model can be automatically transformed from a RoboChart model directly, and its correctness is established by refinements~\cite{Roscoe2011} in CSP.
%  
%  Probability is used to capture uncertainties from physical robots and the environment and randomisation in controllers. The probabilistic extension in the RoboStar framework requires its semantic extension to either base on process algebras and hybrid process algebras or have the richness to deal with probabilistic concurrent and reactive systems. The features of its probabilistic semantics that we consider in this big vision include discrete and continuous distributions, discrete time, nondeterminism, concurrency, and refinement. The type system and the comprehensive expression language of RoboChart~\cite{Miyazawa2019}, additionally, are based on those of the Z notation~\cite{Spivey1992,Woodcock1996} and include mathematical data types such as relations and functions, quantifications, and lambda expressions. Because of such richness in semantics and language features of RoboChart, the formal verification support of RoboChart requires theorem proving and model checking.
%  
%  Our immediate thought is to consider existing probabilistic extensions to process algebras, including CSP-based~\cite{Morgan1996,Morgan2005,Nunez1995,Gomez1997,Kwiatkowska1998,Georgievska2012}, CCS-based~\cite{Hansson1990,Giacalone1990,Yi1992,Vanglabbeek1995}, and ACP-based~\cite{Andova2002}. The main difference between these extensions is how existing constructs or operators, particularly nondeterministic and external choice, interact with probabilistic choice. We also looked at extensions based on probabilistic transition systems~\cite{Larsen1991,Bloom1989,Jonsson2001} and automata~\cite{Wu1997,Hartmanns2015}. To preserve the distributivity of existing operators over probabilistic choice, some algebraic properties are lost, such as the congruence for hiding and asynchronous parallel composition~\cite{Kwiatkowska1998}, idempotence for nondeterministic choice~\cite{Morgan1996a}, or even no standard nondeterministic choice~\cite{Gomez1997,Seidel1995}. The critical problem, however, is the lack of tool support for these extensions. For example, FDR~\cite{T.GibsonRobinson2014}, a refinement model checker for CSP and tock-CSP, cannot verify the probabilistic extensions in CSP. For this reason, we explored other solutions. 
%  
%  In~\cite{Woodcock2019,Ye2022}, we give probabilistic semantics of RoboChart on probabilistic designs~\cite{Ye2021} in Hoare and He's Unifying Theories of Programming (UTP)~\cite{Hoare1998} and then use the theorem prover Isabelle/UTP~\cite{Foster2020}, an implementation of UTP in Isabelle/HOL, to verify probabilistic models. Probabilistic designs are an embedding of standard non-probabilistic designs into the probabilistic world. The theory of probabilistic designs gives probabilistic semantics to the imperative nondeterministic probabilistic sequential programming language \emph{pGCL}~\cite{McIver2005}, but not reactive aspects of RoboChart. We have thought about lifting probabilistic designs into probabilistic reactive designs. Still, the main obstacle is the complexity of reasoning about probabilistic distributions in probabilistic designs because distributions are captured in a dedicated variable $prob$, representing a probability mass function. In particular, the definition~\cite{Ye2021} of sequential composition includes an existential quantification over intermediate distributions. The proof of sequential composition needs to supply a witness for the intermediate distributions, which is helpful but non-trivial.
%  
%  We also gave RoboChart probabilistic semantics~\cite{Ye2022,Miyazawa2020} in the PRISM language~\cite{Kwiatkowska2011}. We developed plugins for RoboTool,\footnote{\url{www.cs.york.ac.uk/robostar/robotool/}} an accompanying tool for RoboChart, to support automated verification through probabilistic model checking using PRISM. PRISM, however, employs a closed-world assumption: systems are not subjected to environmental inputs. To verify a RoboChart model, such as a high voltage controller\footnote{\url{github.com/UoY-RoboStar/hvc-case-study/tree/prism_verification/sbmf}} for a painting robot~\cite{Murray2020} and an agricultural robot\footnote{\url{github.com/UoY-RoboStar/uvc-case-study}} for UV-light treatment using PRISM, we need to constrain the environmental input and verify its expected outputs through an additional PRISM module being in parallel with the corresponding PRISM model that is automatically transformed from the RoboChart model. Finally, the safety and reachability properties of the RoboChart model (checked by the trace refinement in FDR) become deadlock freedom problems in PRISM. However, this cannot verify other properties like liveness, which requires failures-divergences refinement in CSP and FDR.
%  
%  The research question that we aim to answer is a probabilistic semantic framework that 
%  \begin{enumerate*}[label={(\arabic*)}]
%  %\begin{inparaenum}
%      \item has rich semantics to capture our probabilistic vision, 
%      \item is simple and flexible to allow further extensions, and
%      \item supports theorem proving.
%  %\end{inparaenum}
%  \end{enumerate*}
%  This question is comprehensive and needs a research programme, instead of a project, to address it. The work we present in this paper is our first step to answering this question. We need a probabilistic semantic framework that achieves the following:
%  \begin{enumerate*}[label={(\arabic*)}]
%  %\begin{inparaenum}
%      \item it gives semantics to imperative deterministic, probabilistic sequential programming languages with the support of discrete distributions and time,
%      \item it is simple and flexible to allow further extensions towards nondeterminism and concurrency and supports continuous distributions and time (and so hybrid), and 
%      \item it supports theorem proving.
%  %\end{inparaenum}
%  \end{enumerate*}
{
According to Gordon et al.~\cite{Gordon2014}, probabilistic programming includes two basic constructs to draw values from probabilistic distributions such as uniform distributions and condition values of variables. % in addition to other standard constructs for non-probabilistic programming. 
Probabilistic inference is the problem in probabilistic programming to compute explicit probability distributions or to compute relevant probabilities for particular events from probabilistic programs. 
}

\paragraph{\textbf{{Examples}}}
We illustrate a few examples and informally discuss their modelling and inference.

\begin{example}[The (forgetful) Monty Hall problem]
    \label{ex:monty}
The problem~\cite{Wikipedia2023} is a puzzle based on an American television game show \emph{Let's Make a Deal}. It is named after its original host, Monty Hall. Suppose you are the contestant and are given a choice of opening one of three doors. Behind one door is a car, and behind the others are goats. You pick a door, say No. 1, and the host, who knows what is behind each door, opens another door, say No. 3 and reveals a goat. He then asks, ``Do you want to pick door No. 2 instead of your original choice?'' The problem is simple: should you change your choice to maximise your chance to win a car?

To model this problem, we define three variables $p$, $c$, and $m$ for the door number having the prize (car), the contestant chooses, and the Monty chooses. We model the problem below. %We show below a sketch of the pseudo code for this problem.
% \begin{lstlisting}[language=PPL,]
% p drawn a value from the uniform distribution on {0..2}
% c drawn a value from the uniform distribution on {0..2}
% if p=c then
%   m chooses other two doors, (c+1)%3 or (c+2)%3, with equal probability (1/2)
% else
%   m chooses another door, 3-c-p, that has no prize
% c := c (without change of choice) or c := 3-c-m (with change to another one)
% \end{lstlisting}
% \begin{align*}
%     &p:=rand({0..2)};
%     c:=rand({0..2)};\\
%     &\text{if} (p=c)\left\{m:=(c+1)\%3 \pchoice{1/2} m:=(c+2)\%3\right\} 
%     \text{else} \{m:=3-c-p\} \\
% \end{align*}
\begin{lstlisting}[language=PPL,]
p := rand({0..2}); c := rand({0..2});//Prize and contestant's choice are random
if(p=c) { // If the contestant's choice is the prize, 
  m := (c+1)%3 pc{1/2} m := (c+2)%3; // Monty randomly chooses other two doors
} else { 
  m := 3-c-p; // Monty chooses another door which has no prize
}/*
c := c // (without change of choice) */
c := 3-c-m; // If the contestant changes the choice
\end{lstlisting}
We use two constructs: \lstinppl{rand(S)} to draw a random value from set $S$ and \lstinppl{P pc\{r\} Q}, a binary probabilistic choice, to choose $P$ with probability $r$ and $Q$ with probability $1-r$.

The last line corresponds to the strategy for the contestant to change the initial choice. The question becomes ``which strategy will have the higher winning (c=p) probability?''

Suppose now that Monty forgets which door has the prize behind it. He opens either of the doors not chosen by the contestant. The contestant switches their choice to that door if the prize is revealed ($m=p$).  So the contestant will surely win. However, should the contestant switch if the prize is not revealed ($m \neq p$)? In this forgetful Monty, the new knowledge ($m \neq p$) is learned. 

Accordingly, we model the forgetful Monty problem below. 
\begin{lstlisting}[language=PPL,]
{
  p := rand({0..2)}; c := rand({0..2}); 
  m := (c+1)%3 pc{1/2} m := (c+2)%3; // Monty has no knowledge of p
} || (m != p) 
\end{lstlisting}
We introduce another construct \lstinppl{P || Q} to model the new knowledge (encoded in $Q$, e.g. \lstinppl{m != p} denoting the prize is not revealed) learned after $P$ is executed. So the question becomes how the distribution is updated after learning the new evidence? What is the winning probability?  
\end{example}

\begin{example}[Classification - COVID-19 diagnosis]
    \label{ex:covid}
We consider people using a COVID-19 test to diagnose if they may or may not have contracted COVID-19. The test result is binary and could be positive or negative. The test, however, is imperfect. It doesn't always give a correct result. 

We model the prior, the test, and the first test result positive below.
\begin{lstlisting}[language=PPL,]
{
  c := True pc{p1} c := False; // Prior probability of a person having COVID 
  // A test 
  if c { ct := Pos pc{p2} ct := Neg; } // True positive and True negative
  else { ct := Pos pc{p3} ct := Neg; } // False positive and False negative
} || (ct = Pos) // Learn the result is positive 
\end{lstlisting}
In the program, \lstinppl{c} and \lstinppl{ct} denotes if a person has COVID or not, and the test result; and \lstinppl{p1}, \lstinppl{p2}, and \lstinppl{p3} are parameters of this program.
We are interested in several questions. How likely is a randomly selected person to have COVID-19 if the first test result is positive?  Is it necessary to have the second test to reassure the result? 

Taken the second test into account, the new program is as follows.
\begin{lstlisting}[language=PPL,]
{
  { ... } // The previous program
  // The second test 
  if c { ct := Pos pc{p2} ct := Neg; }
  else { ct := Pos pc{p3} ct := Neg; }
} || (ct = Pos) // Learn the result is positive again 
\end{lstlisting}

But how much can the second test contribute to the diagnosis? How the result changes if the parameters are changed? 
\end{example}

\begin{example}[Robot localisation (RL)]
    \label{ex:robot}
A circular room has two doors and a wall. A robot with a noisy door sensor maps position to $\clz door$ or $\clz wall$. Doors are at positions 0 and 2; position 1 is a blank wall. % We define a predicate $door(p) \defs p = 0 \lor p = 2$ and 
We introduce a program variable $bel \in \{0 \upto 2\}$ to denote the position of the robot that we believe. 
When the reading of the door sensor is $\clz door$, it {is} four times more likely to be right than wrong {and likewise when the reading is wall}. %The likelihood functions are defined below.

The following program models two sensor readings and one movement in between the readings.
\begin{lstlisting}[language=PPL,]
{
  {
    bel := rand({0..2}); // Prior for bel is uniformly distributed 
  } || (3*door(bel)+1) ; // Likelihood function for the sensor result door
  bel := (bel + 1) % 3; // Move to the right
} || (3*wall(bel)+1) ; // Likelihood function for the sensor result wall 
\end{lstlisting}
The \lstinppl{door(bel)} (or \lstinppl{wall(bel)}) is a function returning 1 if the \lstinppl{bel} is 0 or 2 (or 1) and returning 0 otherwise.

We are interested in questions like how many measurements and moves are necessary to estimate the robot's location accurately.
\end{example}

\begin{example}[Flip a coin till heads]
    \label{ex:coin}
We consider the simplest probabilistic program with a loop: flip a coin until the outcome is heads, defined as follows.
\begin{lstlisting}[language=PPL,]
c := heads;
while (c=tail) { 
  c := heads pc{p} c:= tail; 
}
\end{lstlisting}
The \lstinppl{p} above is a parameter denoting the probability of getting a heads for a coin flip. It is 1/2 for a fair coin.
% \begin{align*}
%     c := heads; \text{while}(c \neq heads) \{ x := heads \pchoice{p} x := tail \} \tag*{(coin)} \label{eqn:coin}
% \end{align*}
Does this program terminate? What is the probability distribution on termination? How is the distribution related to \lstinppl{p}? What is the semantics of this program? What is its expected runtime?
\end{example}

\begin{example}[Throw a pair of dice]
    \label{ex:dice}
This example~\cite{Hehner2011} is about throwing a pair of dice till they have the same outcome. We model it below.
\begin{lstlisting}[language=PPL,]
while (d1 != d2) {
  d1 := rand({1..6}); 
  d2 := rand({1..6}); 
}
\end{lstlisting}
This is slightly complex than the coin program in Example~\ref{ex:coin} because two variables are declared. 
Does this program terminate? What is the probability distribution on termination? What is the semantics of this program? Is it still as simple as the coin example? What is its expected runtime?
%\begin{align*}
%    d1 := a; d2 := b; \text{while}(d1 \neq d2) \{ d1 := rand(\{1..6\}); d2 := rand(\{1..6\})\} \tag*{(coin)} \label{eqn:coin}
%\end{align*}
\end{example}

\begin{example}[One-dimensional simple random walk (SRW)]
    \label{ex:srw}
Grimmett and Welsh~\cite{Grimmett1986} defined various random walks. A random walk is \emph{simple} if at each time step it can move only to its next (or neighbouring) positions randomly in one of the lattice directions. A \emph{symmetric} simple random walk has the equal probability for each direction. 
It is also the Gambler's Ruin Problem with an absorbing barrier at 0. 
We model it as a probabilistic program below.
\begin{lstlisting}[language=PPL,]
x := m; // m is the initial position of $x$ 
while (x > 0) { 
  x := x + 1 pc{p} x := x - 1 
}
\end{lstlisting}
In the program, \lstinppl{m} and \lstinppl{p} are parameters.
The program with \lstinppl{p=1/2} (that is, symmetric) is widely studied, for example, in \cite{Hurd2003,McIver2005an,McIver2017,Chatterjee2020}. Unlike the coin and dice examples where each experiment (flip a coin or throw a pair of dice) is independent, each experiment in this example is not independent because the value of $x$ is updated. 

Does this program terminate? How does the termination relate to the parameter \lstinppl{p}? What is the probability distribution on termination? What is the semantics of this program? What is its expected runtime?
\end{example}

{
There are several challenges to model and answer the questions of these programs: 
\begin{enumerate*}[label={(\arabic*)}]
    \item the capability to model the learning process using conditional probability and joint probability as used in the Bayesian approach,
    \item the reasoning about probabilistic loops to give them a precise semantics (probability invariant) and their termination,
    \item the inference to get exact probability distributions and exact expected runtime, especially for programs with loops, and
    \item the guarantee of the correctness of the analysis. 
\end{enumerate*}
A considerable amount of literature has been published on addressing these problems, but none of them can address all these challenges.

McIver and Morgan's weakest pre-expectation~\cite{McIver2005bn} is based on the \emph{pGCL}~\cite{Morgan1999,McIver2005bn}, an extension of Dijkstra's Guarded Command Language (GCL)~\cite{Dijkstra1976} with a probabilistic choice construct. It is mechanised in High-Order Logic (HOL)~\cite{Gordon1993} by Hurd et al.~\cite{Hurd2005}, enabling verification of partial correctness of probabilistic programs. However, pGCL does not support conditional probability, so it cannot model the examples: the forgetful Monty, COVID, and the robot localisation, presented in Examples~\ref{ex:monty} to~\ref{ex:robot}. %It cannot reason about exact probability distributions of probabilisticÂ loops such as the coin, the dice, and the random walk examples.

Based on the weakest pre-expectation semantics, Kaminski~\cite{Kaminski2019a} developed an advanced weakest precondition calculus which supports conditioning in cpGCL~\cite{Olmedo2018} using an \textbf{observe} statement and expected runtimes in the calculus~\cite{Kaminski2016}. The observe statement, however, conditions only boolean expressions. It cannot support the general likelihood functions as discussed in Example~\ref{ex:robot} where the two functions are real-valued expressions. The expected runtime analysis~\cite{Kaminski2018} is based on upper-bounds. It cannot reason about the exact expected runtimes which requires the reasoning of exact probability distributions.

Barthe et al.~\cite{Barthe2018} presented ELLORA, an assertion-based logic for probabilistic programs, implemented in the EasyCrypt theorem prover~\cite{Barthe2014}. However, pGCL does not support conditional probability, so it cannot model the examples: the forgetful Monty, COVID, and the robot localisation.

Schr\"oer et al.~\cite{Schroeer2023} developed expectation-based reasoning using a deductive verification infrastructure, based on the weakest pre-expectation semantics. Similarly, it cannot reason about the exact probability distributions and the expected runtimes. For example, the random walk is verified to be almost-surely terminated, but without its semantics or exact distributions. It is also not able to model the general likelihood functions.

Hehner's probabilistic predicate programming (PPP)~\cite{Hehner2004,Hehner2011} can model and reason about all these examples. But PPP is not formalised and implemented in any tool for automated verification.
}

{
The work presented in this paper aims to support modelling and analysis of these probabilistic programs and answer the questions which we are interested in. Additionally, as discussed later in Sect.~\ref{sec:concl:vision}, we aim to pursue a probabilistic semantic framework 
\begin{enumerate*}[label={(\arabic*)}]
    \item having an expressive language with rich semantics to model systems not only from abstract specification level but also concrete implementation level;
    \item able to unify different probabilistic models and programmings, so their tools can be integrated;
    \item extendible to support more features like more discrete distributions, nondeterminism, continuous distributions, time, communication and concurrency, because these features are essential in modelling robotic applications;
    \item providing a practical and decidable method to approximate the semantics for probabilistic loops because it is non-trivial to construct an invariant and prove it for a loop; and
    \item most importantly, supporting theorem proving because these programs usually have unbounded variables and infinite state space.
\end{enumerate*}
}

\paragraph{\textbf{Our approach}}
Our previous work probabilistic RoboChart~\cite{Ye2022} and probabilistic designs~\cite{Ye2021} model aleatoric uncertainty describing the natural randomness of physical processes. Another category is epistemic uncertainty due to the lack of knowledge of information, which is reducible by gaining more knowledge. {Usual probabilistic choice can model aleatoric uncertainty but not epistemic uncertainty because it requires the capability to update distributions or beliefs after learning new knowledge. To model this process, for example, in the Bayesian approach, conditional and joint probabilities should be supported.} 
% Many machine learning algorithms such as Naive Bayes Classification, \ldots 
In this paper, we present a probabilistic programming language, called \emph{probabilistic unifying relations} (ProbURel), based on Hehner's probabilistic predicative programming~\cite{Hehner2004,Hehner2011}, to model both aleatoric and epistemic uncertainty. This programming uses the subjective Bayesian approach to reason about epistemic uncertainty.

In Hehner's original work~\cite{Hehner2011}, a probabilistic program is given relational semantics, and its syntax is a mixture of relations and arithmetic. The presentation of syntax and semantics in the paper is not formal. For example, the semantics of a probabilistic \emph{ok} (skip) is given as $ok = \left(x' = x\right) \times \left(y' = y\right)$. There is a benefit to introducing semantics using examples, but it lacks formalisation. The operators like $=$ and $\times$ are not formally defined, and the types for variables and expressions are not given. The lack of this information makes the paper not easily accessible to readers, particularly for researchers aiming to use the work for automated reasoning of probabilistic programs. Therefore, our first contribution to this paper is formalising its syntax and semantics. We introduce a notation called Iverson brackets, such as $\ibracket{r}$, to establish a correspondence between relations $r$ and arithmetic ($0$ or $1$). For \emph{ok}, we could formalise it as $\ibracket{v' = v}$ where $v$ denotes the state space (composed of all variables) of a program. This notation separates relations ($v'=v$) with arithmetic, so expressions and operators in a program all have clear meanings or definitions depending on their contexts (relations or arithmetic) where the contexts can be easily derived because of the separation. 

In addition to syntax and semantics, the semantics for probabilistic loops are not formally presented and argued. Hehner proposed a more straightforward but more potent (than total correctness) approach~\cite{Hehner1999}: partial correctness + time to deal with the termination of loops (for conventional programs) with extra information about run time. His approach introduces a time variable $t$ with a healthiness condition, strict incremental for each iteration. The variable $t$ can be discrete or continuous and is an extended natural or real number to have $\infty$ for nontermination. The same approach is also applied to probabilistic programs~\cite{Hehner2011}, but the partial correctness of probabilistic loops is not formally reasoned about. 
%cannot be constructed from the semantics for other constructs like conditional and sequential composition. 
% Mechanisation of the semantics of probabilistic programs in a theorem prover cannot be achieved. 
For this reason, our second contribution in this paper is to bridge the semantic gap for probabilistic loops by establishing its semantics using fixed-point theorems: specifically Kleene's fixed-point theorem, to construct the fixed points using iterations. The advantage of having an iterative (or constructive) fixed point includes both theoretical semantics and practical computation or approximation. A hint of this would be possible to verify probabilistic loops using both theorem proving and model checking (based on approximation). 

To give the semantics using the fixed-point theorem, we define a complete lattice $\left([0,1], \leq\right)$ over the real unit interval (the real numbers between 0 and 1 inclusive). We restrict to the unit interval simply because probability values are between 0 and 1. To apply Kleene's theorem, we prove the loop function is Scott-continuous for the state space in which only finite states have positive probabilities. Then we define the semantics of loops as the least fixed point (\emph{lfp}) of the function where \emph{lfp} can be calculated iteratively as the supremum of the ascending Kleene chain (from the bottom of the complete lattice). This bridges the semantics gap, but it is still challenging to calculate \emph{lfp} because the chain is infinite. We, therefore, also present the strongest fixed point (\emph{gfp}) of the function, calculated iteratively as the infimum of the descending Kleene chain (from the top of the complete lattice) of the function. We prove a unique fixed point theorem where \emph{lfp} and \emph{gfp} are the same based on particular assumptions. The unique fixed point theorem makes reasoning about loops much more accessible because it is unnecessary to calculate \emph{lfp}. Instead, a fixed point must be constructed and proved with the unique fixed point theorem. This, eventually, is consistent with the loop semantics using Hehner's more straightforward approach. In particular, our semantics can be mechanised and automated.

In Kleene's theorem, the ascending and descending chains start from a pointwise constant function 0 and 1 (the bottom and the top of the complete lattice). The two pointwise functions are not distributions (where probabilities of the state space sum to 1). Indeed, the pointwise function 0 is a subdistribution (the probabilities sum to less than or equal to 1), and the pointwise function 1 is a superdistribution (the probabilities sum to larger than 1).  For this reason, our third contribution is to extend the semantic domain of the probabilistic programming language from distributions to subdistributions and superdistributions. Eventually, constructs like conditional, probabilistic choice, and sequential composition will not be restricted to programs that are distributions. This brings us to the required semantics to use Kleene iterations for the semantics of loops. 

The introduction of Iverson brackets also has another benefit: the relations inside the brackets can be easily characterised using alphabetised relations in UTP because relations in both Hehner's work and UTP are of the predicative style. Relations in our probabilistic programs are indeed UTP alphabetised relations, which allows us to reason about probabilistic programs using the existing theorem prover Isabelle/UTP for UTP. Our final contribution, therefore, is to mechanise the semantics of the probabilistic programming language in Isabelle/UTP. Our reasoning is primarily automated thanks to the various relation tactics in Isabelle/UTP. Six examples presented in this paper are all verified.
All definitions and theorems in this paper are mechanised, and accompanying icons (\isalogo) {encoding a hyperlink (available only in the electronic version of this paper)} to corresponding repository artefacts.

\paragraph{\textbf{Paper structure}}
The remainder of this paper is organised as follows. We review related work in Sect.~\ref{sec:relwork}. Section~\ref{sec:prelim} provides the necessary background for further presentation of our work in the subsequent sections. In Sect.~\ref{sec:ureal}, we define the complete lattice over the unit interval and then lift it to a complete lattice over pointwise functions. Section~\ref{sec:program} formalises our probabilistic programming with Iverson brackets defined. We also present proven algebraic laws for each construct. In Sect.~\ref{sec:rec}, we present the semantics of probabilistic loops and the fixed point theorem. Afterwards, we illustrate our reasoning approach using six examples. Two are classification problems in machine learning, and two contain probabilistic loops (see Sect.~\ref{sec:ex_cases}). Finally, we discuss future work in Sect.~\ref{sec:concl}. 
