
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Based on                                         %
% commit: 139fd3bff22e188421a9c425abf208d754b2b033 %
% Tag: submission_computers_and_security           %
% 2022-08-14 20:50:29                              %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}


\ifCLASSOPTIONcompsoc
    \usepackage[caption=false, font=normalsize, labelfont=sf, textfont=sf]{subfig}
\else
    \usepackage[caption=false, font=footnotesize]{subfig}
\fi

\usepackage{tikz}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[ruled,vlined]{algorithm2e}


\usepackage{listings}
\lstset{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}


\makeatletter
\newcommand{\removelatexerror}{\let\@latex@error\@gobble}
\makeatother


\newcommand*\numcircledtikz[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=1.2pt] (char) {#1};}} 

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

% Notice
\pagestyle{empty}
\onecolumn
Clustered Federated Learning Architecture for Network Anomaly Detection in Large Scale Heterogeneous IoT Networks

\vspace{1 cm}

{\Large \textbf{Notice:} An updated version of this work has been submitted to Computers \& Security for possible publication.}

\twocolumn

\clearpage
\pagenumbering{arabic}
\pagestyle{plain}

\title{Clustered Federated Learning Architecture for Network Anomaly Detection in Large Scale Heterogeneous IoT Networks
\thanks{
The European commission financially supported this work through Horizon Europe program under the IDUNN project (grant agreement number 101021911). It was also partially supported by the Ayudas Cervera para Centros Tecnológicos grant of the Spanish Centre for the Development of Industrial Technology (CDTI) under the project EGIDA (CER-20191012), and by the Basque Country Government under the ELKARTEK program, project REMEDY - REal tiME control and embeddeD securitY (KK-2021/00091). Urko Zurutuza is part of the Intelligent Systems for Industrial Systems research group of Mondragon Unibertsitatea (IT1676-22), supported by the Department of Education, Universities and Research of the Basque Government.
}
}

\author{
\IEEEauthorblockN{Xabier Sáez-de-Cámara\IEEEauthorrefmark{1}\IEEEauthorrefmark{2}\IEEEauthorrefmark{3}, Jose Luis Flores\IEEEauthorrefmark{1}, Cristóbal Arellano\IEEEauthorrefmark{1}, Aitor Urbieta\IEEEauthorrefmark{1} and Urko Zurutuza\IEEEauthorrefmark{2}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}Ikerlan Technology Research Centre, Basque Research and Technology Alliance (BRTA)\\Arrasate-Mondragón, Spain\\\{xsaezdecamara, jlflores, carellano, aurbieta\}@ikerlan.es}\IEEEauthorblockA{\IEEEauthorrefmark{2}Mondragon Unibertsitatea\\Arrasate-Mondragón, Spain\\uzurutuza@mondragon.edu}\IEEEauthorblockA{\IEEEauthorrefmark{3}Corresponding author}
}

\maketitle

\begin{abstract}

There is a growing trend of cyberattacks against Internet of Things (IoT) devices; moreover, the sophistication and motivation of those attacks is increasing. The vast scale of IoT, diverse hardware and software, and being typically placed in uncontrolled environments make traditional IT security mechanisms such as signature-based intrusion detection and prevention systems challenging to integrate. They also struggle to cope with the rapidly evolving IoT threat landscape due to long delays between the analysis and publication of the detection rules. Machine learning methods have shown faster response to emerging threats; however, model training architectures like cloud or edge computing face multiple drawbacks in IoT settings, including network overhead and data isolation arising from the large scale and heterogeneity that characterizes these networks.

This work presents an architecture for training unsupervised models for network intrusion detection in large, distributed IoT and Industrial IoT (IIoT) deployments. We leverage Federated Learning (FL) to collaboratively train between peers and reduce isolation and network overhead problems. We build upon it to include an unsupervised device clustering algorithm fully integrated into the FL pipeline to address the heterogeneity issues that arise in FL settings. The architecture is implemented and evaluated using a testbed that includes various emulated IoT/IIoT devices and attackers interacting in a complex network topology comprising 100 emulated devices, 30 switches and 10 routers. The anomaly detection models are evaluated on real attacks performed by the testbed's threat actors, including the entire Mirai malware lifecycle, an additional botnet based on the Merlin command and control server and other red-teaming tools performing scanning activities and multiple attacks targeting the emulated devices.

\end{abstract}

\begin{IEEEkeywords}
Anomaly detection, Botnet, Internet of Things, Intrusion detection, Machine learning, Network security
\end{IEEEkeywords}


\section{Introduction}

The ever-growing adoption of the Internet of Things (IoT) is enabling manufacturers multiple benefits such as productivity boosts, increased automation, cost savings, and the minimization of production errors and waste. This is achieved by connecting a multitude of Internet-enabled smart devices, Cyber-Physical Systems (CPS) and other ``things'' with the ability to collect, monitor, analyze and share data to make decisions and interact with physical processes, often with little or no human intervention~\cite{Boyes2018}. However, this high level of connectivity also brings a higher risk of cybersecurity breaches and a bigger attack surface both for domestic and industrial IoT devices~\cite{Sisinni2018, Rehman2019}. Especially nowadays that many solutions are being replaced with commercial off-the-shelf devices~\cite{Boyes2018} that prioritize fast market adoption and convenience over security~\cite{Jalali2019}.

An important consequence of the mass adoption of IoT and its high interconnectivity, coupled with poor security practices and multiple sources of vulnerabilities~\cite{Neshenko2019}, is that it has given rise to malware designed to target and exploit these devices. One of the most notable examples is the 2016~Mirai worm~\cite{Antonakakis2017}, which exploited the widespread use of weak or hardcoded passwords to compromise a diverse set of devices from various manufacturers. The infection reached a peak of 600,000~devices, which were used to launch thousands of organized Distributed Denial of Service (DDoS) attack campaigns against telecommunication providers, game servers and particulars. The sophistication of those types of attacks continues to grow. Newer Mirai variants developed after the public release of its source code~\cite{Miraisrc} and other advanced IoT malware such as Mozi\footnote{https://malpedia.caad.fkie.fraunhofer.de/details/elf.mozi} and VPNFilter\footnote{https://malpedia.caad.fkie.fraunhofer.de/details/elf.vpnfilter} include additional functionality to exploit software vulnerabilities targeting different protocols such as Telnet, HTTP, SSH, UPnP, SMB, Modbus, FTP, MQTT, etc.~\cite{Vervier2018}. Moreover, the compromised devices are not only used to perform DDoS attacks but also cryptocurrency mining, spamming or advertisement click fraud~\cite{Kambourakis2019}. Exposed industrial IoT (IIoT) systems are also the targets of numerous attacks that may pose additional risks due to the critical nature of these devices, including ransomware, sabotage, intellectual property theft, or be used as a pivot point to infiltrate into other systems in the IT or OT infrastructure~\cite{Sadeghi2015, McLaughlin2016}.

Several mitigation strategies have been proposed to defend against these threats. For instance, the use of specialized Operating Systems (e.g., FreeRTOS, RIOT, Contiki-NG) with a smaller attack surface compared to general-purpose ones, the removal of nonessential services, reliable update mechanisms, event loggers and basic hardening operations~\cite{Kambourakis2017}. However, those mitigations do not guarantee a secure environment; misconfigurations, the discovery of new vulnerabilities and zero-days still makes IoT devices prone to attacks~\cite{Meneghello2019}. To overcome those problems, Intrusion Detection Systems (IDS) and Intrusion Prevention Systems (IPS) are commonly deployed as an additional security layer to detect and prevent a wide range of malicious activities.

Based on the detection technique, IDS systems can be broadly classified as signature-based or anomaly-based~\cite{RadoglouGrammatikis2019}. Signature-based IDSs match the incoming data with a predefined set of rules of known attacks. They usually offer good detection for known attacks and low false positive rates; however, a major disadvantage of signatures is that they are unable to detect new attacks for which no rules have been defined, or fail to detect some modifications of known attacks due to the usage of packed binaries, string modifications, and obfuscation techniques by malware distributors~\cite{Antonakakis2017}. Moreover, the rapidly evolving IoT threat landscape makes signature-based IDSs struggle to keep up with new IoT threats as a result of long delays between the malware analysis and the publication of the corresponding rule~\cite{costin2018iot}. For example, in~\cite{Vervier2018} the authors collected a total of 3,385~suspicious files dropped by IoT attackers during six months; 85.2\% of those files were not detected by VirusTotal. Similarly, IP address and domain blocklists are also not particularly effective caused by rapid changes in botnet control infrastructure~\cite{Vervier2018}. On the other hand, anomaly-based IDSs aim to model the normal behavior of the devices and raise alarms when deviations from this model are detected. Machine Learning (ML) and Deep Learning (DL) methods have shown promising results in this area, exhibiting more flexibility and generalization than traditional signature-based detection methods~\cite{Ferrag2020}.

Despite the advantages of ML, incorporating this type of solutions needs a solid infrastructure and training data availability. From the point of view of model training infrastructure, traditional cloud-based centralized architectures have generally been adequate for training complex ML models in many applications and environments. In this type of centralized settings, the data generated by each client is sent to a server and the ML training is offloaded to the cloud. After the training, the model inference or prediction is usually offered as a service for the clients. Alternatively, clients can download the trained model for local inference. However, for IoT settings, this traditional centralized architectures exhibit many problems due to the massive scale and heterogeneity of IoT deployments.

In large-scale IoT settings, where high volumes of data are generated, problems such as high bandwidth consumption, network resource congestion and load balancing arise, leading to issues like packet losses, transmission delays, high latency, traffic peaks, etc.~\cite{Yu2018} that can adversely affect the training process or even make cloud training infeasible. In addition, due to the potentially global nature of IoT networks, the infrastructure can be under the control of multiple vendors or service providers. In this situations, deploying ML applications that comply with regulations such as the General Data Protection Regulation (GDPR)~\cite{GDPR} and respect users' privacy concerns can be challenging. To address some of those issues, proposals to shift the computation towards the ``edge'' of the network are being made, closer to the devices where the data is being generated~\cite{Yu2018, Zhang2019}. While edge computing can alleviate some of the mentioned problems of centralized architectures, other additional issues like data islands and isolation arise, which can hinder the application of ML because it effectively reduces the volume of data available for training.

A promising alternative that could address the network overhead, privacy and data isolation issues and is gaining significant attention is Federated Learning (FL). FL is a ML setting introduced in 2016 by McMahan et al.~\cite{Konecny2016} with the objective to train a single model (the global model) from data distributed at multiple remote devices (clients). The most particular characteristic about FL is that each device's local training dataset does not leave the device; instead, each client independently computes some local model update and communicates the results to a central server, which aggregates the local updates from all the clients to train the global model iteratively. Data is kept locally on each device, and only model updates are transmitted to the aggregation server, which preserves data privacy requirements. Since model updates are typically smaller than the size of the dataset, network overhead problems can also be reduced. Additionally, data isolation is minimized because multiple clients participate in training the global model.

However, there are still some difficulties to be considered for a practical FL deployment. Even though FL assumes that the data generation does not follow Independent and Identically Distributed (IID) assumptions across all the clients, in practice, highly non-IID settings can hinder global model convergence~\cite{Kairouz2019}. This can happen in highly heterogeneous settings such as large IoT networks composed of devices communicating with a diverse set of protocols.



To address the described issues, we propose a FL architecture for training anomaly-based IDS in large networks of heterogeneous IoT devices. To aggregate knowledge from all the devices, the system will leverage the FL framework to collaboratively train the anomaly detection models between multiple participants without sending each device's local data, thus reducing network overhead and tackling data isolation and privacy considerations.

In particular, to address the mentioned global model convergence problems that arise in typical FL settings with heterogeneous clients, we propose a clustered FL process that can be divided into two steps. First, before the local models are aggregated in the initial FL round, the local partially trained models from all the clients are clustered in a fully unsupervised way based on similarities between model parameters, following the hypothesis that clients with similar data distributions will converge towards models with similar parameter values. In this step, each client is assigned to a cluster center. Then, an independent FL training process is started for each identified cluster of devices. The contributions can be summarized as follows:

\begin{itemize}
    \item We propose and test a clustered FL architecture for unsupervised anomaly detection IDS model training applied to a network of heterogeneous IoT devices. We test and optimize different FL aggregation functions. The detection model is based on autoencoders trained on network traffic data.

    \item We propose an unsupervised model fingerprinting for device clustering method to address global model convergence problems in heterogeneous FL settings. The method is performed on the local model updates; thus, there is no need to send additional metadata to the FL server, incorporate external fingerprinting tools or perform manual clustering. The method is fully integrated into the FL pipeline and does not need human intervention.

    \item We evaluate the clustered FL architecture on an emulated network scenario based on the Gotham testbed~\cite{GothamTestbed}. The scenario includes 78 IoT and IIoT devices communicating with a diverse set of protocols (including MQTT, CoAP and RTSP) and different network behavior to emulate a heterogeneous environment. The IoT devices interact with 12 different cloud layer services and applications. Additionally, the scenario includes 10 attacker machines executing real IoT threats.

    \item We provide experimental results for the trained FL models. Including comparisons with a state-of-the-art approach.
\end{itemize}


The rest of this paper is structured as follows. Section~\ref{sec:relatedwork} covers the related work. Section~\ref{sec:proposedsystemmodel} discusses the proposed system model, including the FL process, the model fingerprinting for device clustering algorithm and the autoencoder anomaly detection model. Section~\ref{sec:experimentalsetup} details the IoT testbed and the data generation and collection setup. Section~\ref{sec:implementation} describes the implementation methodology of the experiments, and section~\ref{sec:results} shows the results. Finally, the paper is concluded in section~\ref{sec:conclusions}.

\section{Related Work} \label{sec:relatedwork}

In this section, we are going to describe state-of-the-art publications related to the proposals in this work. First, we will review manuscripts that apply FL techniques for IoT intrusion or anomaly detection. Then, we will describe works about clustered FL methods.






















\subsection{Federated Learning for IoT Intrusion and Anomaly Detection}

Recently, several proposals have emerged that use FL techniques for IoT intrusion detection. In~\cite{Nguyen2019} Nguyen et al. present D\"Iot, an unsupervised system for network anomaly detection applied to consumer IoT devices for detecting Mirai-like worm behavior. First, all the devices are grouped based on their network behavior using an external fingerprinting tool. Then, the FL process trains multiple global models, each one of them specific to an IoT device type group. However, one limiting factor in this approach is that a software for automatically identifying IoT device types must be available in each gateway prior to the FL process; making the model training and the device grouping not fully integrated into the same process and requiring additional time to deploy and train the system. Applied in a similar environment, Rey et al. develop in~\cite{Rey2021} a framework based on FL to detect cyberattacks against IoT devices using the N-BaIoT dataset. Additionally, they evaluate several adversarial attacks against the proposed FL framework. In~\cite{Popoola2021} Popoola et al. uses the Bot-IoT and N-BaIoT datasets to train a single supervised classification global model in a FL setting and compares it with centralized and localized architectures. Another comparison between a FL intrusion detection scheme with a centralized and on-device training is shown by Rahman et al. in~\cite{Rahman2020}. 

Other proposals focus on training models, or ensembles of models, that combine different input data types or views. Attota et al. propose in~\cite{Attota2021} an IDS using a multi-view ensemble of models trained with FL; three specific models are trained for each different view (network packets, unidirectional flow and bidirectional flow). Features are selected via a Grey Wolves optimization process. To combine the prediction of the three models, a random forest classifier is used. Similarly, Quin et al.~\cite{Qin2021} introduce a greedy feature selection algorithm to obtain appropriate feature sets according to a single attack type that each device wants to detect. They suggest training multiple global models by grouping the devices based on the feature set selected in each client and initiating an independent FL process for each group. However, in practice, this grouping method requires prior knowledge of attacks that may not be available in a realistic environment and leaks feature set information to the aggregation server. Additionally, devices can be under multiple types of attacks at different time intervals, which will not be detected based on this method. Zhao et al.~\cite{Zhao2019} train a single multi-task model in a FL setting to simultaneously perform network anomaly detection, traffic classification and Tor traffic identification using multiple input datasets.

Alternative architectures such as hierarchical FL are also being explored for IoT intrusion detection. Wang et al.~\cite{Wang2021} describes an FL architecture based on four levels and assumes some of them are untrusted. Saadat et al.~\cite{Saadat2021} compares a standard FL architecture with a hierarchical one in terms of model training loss progression and testing accuracy for the training of an IDS using a supervised multilayer neural network on the NSL-KDD dataset. Wei et al. apply it into a 5G network~\cite{Wei2021}. 

For more industrial approaches, in~\cite{Li2021} Li et al. present an IDS for industrial CPSs based on a FL scheme combined with a Paillier cryptosystem to increase the security of the model updates during the training. A recent example by Mothukuri et al.~\cite{Mothukuri2021} shows a FL-based IDS for IoT networks. They use a dataset composed of labeled network traffic data from industrial Modbus protocol. Kelli et al.~\cite{Kelli2021} propose an IDS for industrial DNP3 protocol specific attack detection combining FL and active learning to perform local model personalization for each client.

Outside of the network intrusion detection field, FL settings for IoT devices have also been proposed in sectors such as healthcare~\cite{Schneble2019, Chen2020} and predictive maintenance~\cite{Liu2021} to name a few.

Most of the proposed approaches use supervised methods for model training, while unsupervised methods do not receive as much attention in the literature. In a real deployment, obtaining labeled network data to train the models is not viable at a practical level. Extending FL to unsupervised methods is still an open challenge~\cite{Kairouz2019}. Additionally, only a few papers consider the heterogeneity of IoT devices. In the cases where the heterogeneity is considered, they require a manual segmentation of the IoT devices~\cite{Schneble2019}, hardcoded device properties such as the 6-tuple in~\cite{Wei2021}, prior knowledge of attack types that target the IoT devices~\cite{Qin2021} or the help of external tools that are not fully integrated into the FL training pipeline~\cite{Nguyen2019}. Moreover, most datasets for intrusion detection were not designed to be applicable to large distributed IoT environments; therefore, many researchers resort to artificially partitioning the dataset to simulate distributed environments in which to apply FL, which is not indicative of a realistic heterogeneous IoT environment. Furthermore, most articles limit themselves to the order of 10~participating clients or less in the FL process, which does not reflect typical IoT environments, making it difficult to draw conclusions on the applicability of FL for IoT anomaly detection.

\subsection{Clustered Federated Learning}

Even though FL is based on the assumption that data is non-IID, in practice, it can show convergence problems in those cases when trying to learn a single global model when dealing with a high number of heterogeneous clients~\cite{Kairouz2019}. To address this issue, in~\cite{Sattler2020} Sattler et al. propose Clustered FL, which groups the client population into clusters based on the cosine similarity of clients' gradient updates. The clustering is performed as a post processing step after the FL has converged. Ghosh et al.~\cite{Ghosh2019} present an outlier robust clustering algorithm based on K-means that also takes into account an adversarial setting. In~\cite{Briggs2020a}, Briggs et al. introduce a clustering step to group clients according to the similarity of their local updates using hierarchical clustering methods. Then, FL is performed on each group independently.

To the best of our knowledge, we have not found any articles addressing IoT device clustering methods that are fully integrated into the FL training pipeline applied to anomaly-based network intrusion detection.


\section{Proposed System Model} \label{sec:proposedsystemmodel}

This section first shows a high-level overview of the proposed system architecture and the targeted deployment setting. Then, we present the proposed clustered FL architecture, describing our contributions on top of the standard FL process to include the integrated model fingerprinting for the device clustering step. We finally have a brief review of autoencoder neural networks for anomaly detection.


\subsection{Deployment Setting and Architecture}

The proposed architecture to train the IDS is depicted in \figurename~\ref{fig:architecture}. It comprises many clients and a central aggregation server and also shows potential attackers.

\begin{figure}
	\centering
	\includegraphics{images/proposed_architecture}
	\caption{Proposed system architecture. Each IoT device (FL clients) holds a copy of the ML model for local training and inference. The FL training process is mediated by the aggregation server. The FL aggregation server can also be part of the IoT cloud, but here it is shown separately for clarity.}
	\label{fig:architecture}
\end{figure}


\subsubsection{Clients}
The proposed system is devised to operate in a large network of heterogeneous IoT devices such as gateways, CPS and industrial machines that communicate using different protocols. Those devices can be located in different network segments or geographically distributed, which may influence their behavior. The devices are constantly sending/receiving data to/from the cloud layer.

Each device is considered a client in the FL process. They are responsible for capturing relevant data, local ML model training, and model inference for anomaly detection after the training is completed. No training data is transmitted to the aggregation server, only model parameters and minimal metadata relevant to the FL process. Devices are expected to perform lightweight ML tasks, but for low powered IoT devices such as sensors and actuators, the system is expected to be deployed at the hub or gateway level. In this work, we prioritize the use of lightweight ML models for anomaly detection to limit the computational overhead during model training or inference.

\subsubsection{Aggregation server}
It coordinates all the FL training process by initializing and distributing the model and training hyperparameters to the clients, receiving model updates from the clients, performing the model fingerprinting and device clustering, running the per cluster aggregation of the received models and sending the corresponding aggregated global model to each client. These steps are explained in the next subsection.

\subsubsection{Attackers}
In this paper, we consider two primary threat models. The first one considers external actors that remotely scan the IoT devices in the network, find vulnerable devices to exploit and remotely compromise them. The second assumes a local adversary compromising one or many IoT devices within the protected network and leveraging them to launch attacks against other devices in the same network or target external victims. In section~\ref{sec:experimentalsetup} we will detail the different threat actors, the performed attacks and malicious behavior.


\subsection{Clustered Federated Learning Process for Heterogeneous Devices}

For the FL deployment, we consider a typical cross-device setting~\cite{Kairouz2019} with a large number of devices. However, due to availability guarantees required by many IoT devices, especially in industrial settings, we expect most devices to participate in the FL training process. This allows the server to maintain persistent state for each client to perform the clustering step. In this work, we assume that no IoT device is infected prior to the FL model training and that none of them behave in an adversarial manner. Model poisoning attacks against FL are outside the scope of this paper, even if already considered in some academic works~\cite{Bagdasaryan2020}.

Our clustered FL builds upon the generalized FL setting proposed by Reddi et al.~\cite{Reddi2021}. This generalized FL setting, described in Algorithm~\ref{alg:fl}, improves over standard FL optimization methods such as the popular Federated Averaging (FedAvg)~\cite{McMahan2017} by including adaptive optimization methods.

\begin{figure}[!t]
\removelatexerror
\begin{algorithm}[H]
\SetAlgoLined
\SetKw{KwIn}{in}
\SetKw{KwInparallel}{in parallel}
\SetKwInput{KwInput}{Input}
\DontPrintSemicolon

 \SetKwFunction{FLocalTrain}{LocalTrain}
 
 \SetKwProg{Fn}{Function}{:}{}
 

  \Fn{\FLocalTrain{$\mathbf{w}$, epochs}}{
    \For{local epoch e $\leftarrow$ 1 \KwTo epochs}{
        \For{batch b \KwIn local training data}{
            $\mathbf{g} \leftarrow$ compute gradient\;
            $\mathbf{w} \leftarrow$ \textsc{ClientOpt}($\mathbf{w}$, $\mathbf{g}$, $\eta$, e)\;
        }
    }
    \KwRet $\mathbf{w}$, number of local training samples\;
 }
 \;
 
 \KwInput{A set of clients $\cal C$, initialized model $\mathbf{W_0}$}
 \KwResult{Trained global model $\mathbf{W_G}$}
 
 E $\leftarrow$ number of local epochs\;
 R $\leftarrow$ total federated learning rounds\;
 $\mathbf{W_G} \leftarrow \mathbf{W_0}$\;
 \For{federated learning round t = 1, 2, \ldots, R}{
    \ForEach{client $c \in \cal C$ \KwInparallel}{
        receive $\mathbf{W_G}$ from the server\;
        $\mathbf{W_c}, n_c \leftarrow$ \FLocalTrain{$\mathbf{W_G}$, E}\;
        local model delta $\Delta_c \leftarrow \mathbf{W_c} - \mathbf{W_G}$\;
        send $(\Delta_c, n_c)$ to the server\;
    }
    
    $n \leftarrow \sum_{i \in \cal C} n_i$\;
    pseudogradient $\mathbf{g_G} \leftarrow -\sum_{i \in \cal C} \frac{n_i}{n} \Delta_i$\;
    $\mathbf{W_G} \leftarrow $ \textsc{ServerOpt}($\mathbf{W_G}$, $\mathbf{g_G}$, $\eta_s$, $t$)\;
 }
 
 
 \caption{Generalized federated learning process. The \textsc{ClientOpt}, \textsc{ServerOpt}, their respective learning rates ($\eta$, $\eta_s$) and the pseudogradient concepts are explained in detail by Reddi et al.~\cite{Reddi2021}}
 \label{alg:fl}
\end{algorithm}
\end{figure}


\begin{figure}[!t]
\removelatexerror
\begin{algorithm}[H]
\SetAlgoLined
\SetKw{KwIn}{in}
\SetKw{KwInparallel}{in parallel}
\SetKwInput{KwInput}{Input}
\DontPrintSemicolon

 \SetKwFunction{FModelFingerprinting}{ModelFingerprinting}
 \SetKwFunction{FLocalTrain}{LocalTrain}
 \SetKwProg{Fn}{Function}{:}{}
 
 \Fn{\FModelFingerprinting{$weight\_list$}}{
  $\cal W \leftarrow $ empty list \;
  
  \For{$\mathbf{w}$ \KwIn $weight\_list$}{
   append flattened $\mathbf{w}$ to $\cal W$\;
  }
  
  $\cal W \leftarrow $ apply PCA dimensionality reduction to $\cal W$\;
  
  $\cal S \leftarrow$ empty list \;
  $\cal L \leftarrow$ empty list \;
  \For{n $\leftarrow$ 2 \KwTo max number of clusters}{
   K-means clustering of $\cal W$ into n clusters\;
   append clustering labels to $\cal L$\;
   append clustering quality score to $\cal S$
  }
  
  $K \leftarrow$ number of clusters with optimal score in $\cal S$\;
  
  \KwRet labels from $\cal L$ corresponding to $n=K$, $K$\;
 }
 \;
 
 
 \KwInput{A set of clients $\cal C$}
 \KwResult{A set of global models}
 initialize model $\mathbf{W_0}$ on server\;
 $\epsilon \leftarrow$ number of local epochs for clustering\;
 
 \ForEach{client c $\in \cal C$ \KwInparallel}{
    receive $\mathbf{W_0}$ from the server\;
    $\mathbf{W_c}, n_c \leftarrow$ \FLocalTrain{$\mathbf{W_0}$, $\epsilon$}\;
    send $\mathbf{W_c}$ to the server
 }
 $\cal W \leftarrow$ list of all the received $\mathbf{W_{c \in \cal C}}$ \;
 
 $\cal L$, $K$ $\leftarrow$ \FModelFingerprinting{$\cal W$}\;
 
 \ForEach{label $k \in \{1,...,K\}$ \KwInparallel}{
    ${\cal C}_k$ $\leftarrow$ subset of clients $\in \cal C$ with labels ${\cal L} = k$\;
    $\mathbf{W_{G}^{C=k}} \leftarrow$ average of $\cal W$ with labels ${\cal L} = k$\;
    $\mathbf{W_{G}^{C=k}} \leftarrow $\texttt{FederatedLearning}$({\cal C}_k, \mathbf{W_{G}^{C=k}})$\;
 }

 \caption{Proposed clustered federated learning for heterogeneous clients. The \texttt{LocalTrain} and \texttt{FederatedLearning} functions are described in Algorithm~\ref{alg:fl}.}
 \label{alg:heterogeneousfl}
\end{algorithm}
\end{figure}

The proposed clustered FL is described in Algorithm~\ref{alg:heterogeneousfl}. First, the aggregation server initializes the model weights~$\mathbf{W_0}$ and selects the training hyperparameters. Then, the server sends those values to all the participating clients. In the next step, each client partially trains $\mathbf{W_0}$ using only its local data for $\epsilon$~epochs. The local training is performed using the \textsc{ClientOpt}~\cite{Reddi2021} gradient-based optimizer to minimize the local training loss. \textsc{ClientOpt} is an abstraction for optimizers such as SGD, Adam or RMSprop. After the local training, each client sends the partially trained model to the aggregation server. The aggregation server collects all the local models and uses them to group the clients into $K$~clusters based on similarities between the trained model parameters (weights and biases). The grouping process is discussed in more detail in the next subsection.

For each identified cluster~$k$, an independent FL process is executed in parallel (Algorithm~\ref{alg:fl}). We perform multiple FL rounds (R rounds) until the global model for each cluster converges, resulting in a set of $K$~global models. The model aggregation function is generalized in \textsc{ServerOpt} as described in Algorithm~\ref{alg:fl}. The \textsc{ClientOpt} and \textsc{ServerOpt} abstraction allows to incorporate momentum or other adaptive optimization methods to both client-side training and server-side model aggregation compared to the FedAvg algorithm~\cite{Reddi2021}. The popular FedAvg aggregation method can be considered a special case where \textsc{ServerOpt} is set to SGD with server learning rate~$\eta_s = 1.0$.

\subsection{Model Fingerprinting for Device Clustering}

In a network of heterogeneous devices, the underlying data distribution might not be IID. In a FL setting, a single global model complex enough could be able to fit the data properly; however, training a complex model in IoT devices might not be possible due to hardware constraints. Consequently, we will group the devices with similar behavior to create a set of global models specifically tailored to each group of devices. With this method, each IoT device is assigned a group label in an unsupervised manner that is going to be used during the FL process.

The main advantages of using the locally trained model updates as inputs for the clustering method are that \textit{i)} there is no need to integrate any external device fingerprinting algorithms or manual methods, \textit{ii)} does not require waiting for a certain amount of time to identify the devices before starting the model training process and \textit{iii)} everything is completely integrated into the FL training pipeline.

As detailed in Algorithm~\ref{alg:heterogeneousfl}, the first step for the model fingerprinting consists of partially training each local model for $\epsilon$~epochs, and sending the partially trained model to the aggregation server. Then, the server flattens the parameters (weights and biases) of each model and performs Principal Component Analysis (PCA) to reduce the dimensionality of the parameters. The reduced dimensionality helps speed up the computation of the clustering step and can limit the problems of clustering high dimensionality data in models with a considerable number of parameters. We use the K-means algorithm with the k-means++ initialization scheme~\cite{Arthur2006} to cluster the reduced dimensionality data. The hypothesis is that clients with similar data distributions will converge to models with similar parameter (weights and biases) values, provided that all clients start from the same initial random model~$\mathbf{W_0}$.

Due to the unsupervised nature of our proposal, we will use internal clustering validation metrics to select an optimal value for the number of clusters~$K$. Internal validation metrics do not rely on any external data and are mainly based on measures such as the compactness of samples within the same cluster and separation between different clusters~\cite{Liu2010}. Specifically, we will evaluate the following internal validation metrics: Silhouette~\cite{Rousseeuw1987}, Davies-Bouldin~\cite{Davies1979} and S Dbw~\cite{Halkidi2001} scores to select the value of~$K$.


\subsection{Anomaly Detection Model}

In this paper we are going to employ autoencoder neural networks as the anomaly detection models, which have already been used in similar domains for network-based attack detection~\cite{Mirsky2018,Meidan2018}. We are going to prioritize lightweight autoencoders (small number of parameters) which makes them especially suitable for our deployment scenario because it not only requires less computational load for model training or inference in constrained devices, but also reduces the network traffic volume between the devices and the aggregation server during the FL rounds due to less number of parameters compared to more complex models.

Autoencoders are unsupervised neural networks that attempt to replicate the input data on their output layer under some constraints to avoid learning the identity function. The autoencoder is composed of two networks, the encoder and the decoder. The encoder takes the input features~$\mathbf{x} \in \mathbb{R}^n$ and transforms it into a hidden encoded space ~$\mathbf{h} \in \mathbb{R}^e$, where~$e < n$ to impose a constrain to avoid learning the identity function. Then, the decoder transforms~$\mathbf{h}$ into~$\mathbf{x^\prime} \in \mathbb{R}^n$. The objective of the autoencoder is to minimize the mean squared error (MSE, reconstruction error) between~$\mathbf{x}$ and~$\mathbf{x^\prime}$ as in equation~\eqref{eq:mse}. The autoencoder is trained using the loss function shown in equation~\eqref{eq:loss_function}, which in addition to the MSE, it includes the~$L_2$ regularization term.

\begin{equation}
	MSE = \frac{1}{n} \sum_{i=1}^{n} (x_i - x_i^\prime)^2.
	\label{eq:mse}
\end{equation}

\begin{equation}
	\mathcal{L} = MSE + \lambda \sum_{i} w_{i}^{2}
	\label{eq:loss_function}
\end{equation}

Once the autoencoder is trained, the reconstruction error between the input and output layers is used as a measure of the anomaly level in the input data.

\section{IoT Testbed and Experimental Setup} \label{sec:experimentalsetup}

In this section, we present the experimental setup. We begin with a description of the selected IoT testbed used to extract the dataset and implement the experiments, including a description of all the considered IoT device types and attackers. We then continue to detail the training and validation network traffic datasets generated with the testbed. For the validation datasets, we include a list of the performed malicious activities according to our threat model.

\subsection{IoT Testbed}

The experimental setup is based on the Gotham Testbed~\cite{GothamTestbed}, a testbed to perform large-scale IoT security experiments in a realistic and reproducible way. Gotham is built on top of the GNS3~\cite{Grossmann} network emulator, and it includes a repository of Docker images and QEMU-based virtual machines to emulate various IoT/IIoT devices, malware samples, servers and networking equipment such as switches and routers. To generate real network traffic traces, the emulated devices run real production libraries, network switching software (Open vSwitch) and routing operating systems (VyOS), as well as real malware samples and red-teaming tools, which are going to be briefly described in this section.

To create the IoT scenario, we use the default case study scenario presented at~\cite{GothamTestbed} composed of many heterogeneous nodes. The scenario comprises three main networks connected by 10 routers and 30 switches: the city network, the cloud network and the threat network. The full details are explained in~\cite{GothamTestbed}, but here we will present a summary of the different emulated devices which are relevant for the discussion in the following sections.

\subsubsection{City network devices}

The city network contains the emulated IoT/IIoT devices. The devices communicate with the cloud network using various protocols, including MQTT, CoAP (two protocols that are specifically designed and well-suited for machines with constrained resources and the IoT paradigm~\cite{Minerva2015}) and RTSP. Aditionally, to increase the protocol heterogeneity, the devices generate background traffic such as DNS, NTP and ICMP. In total, there are 12 device templates to simulate a heterogeneous environment, as shown in Table~\ref{tab:iot_device_templates}.

\begin{table}[t]
	\small
	\centering
	
	\caption{Included IoT/IIoT device templates in the testbed scenario.}
	\label{tab:iot_device_templates}
	
	\begin{tabularx}{1.0\linewidth}{p{0.34\linewidth}|p{0.13\linewidth}|X}
		\toprule
		Template name          & Instances & Main protocol \\
		\midrule
		Air quality            & 1         & MQTT (plain)      \\
		Building monitor       & 5         & MQTT (plain)      \\
		City power             & 1         & CoAP (plain)      \\
		Combined cycle         & 10        & CoAP (plain)      \\
		Combined cycle tls     & 5         & CoAP (DTLS)      \\
		Cooler motor           & 15        & MQTT (plain and TLS)      \\
		Domotic monitor        & 5         & MQTT (plain)      \\
		Hydraulic system       & 15        & MQTT (plan and TLS)      \\
		IP camera street       & 2         & RTSP      \\
		IP camera museum       & 2         & RTSP      \\
		IP camera consumer     & 2         & RTSP      \\
		Predictive maintenance & 15        & MQTT (plain and TLS)      \\
		\bottomrule
	\end{tabularx}
	
\end{table}

Each device template presented in Table~\ref{tab:iot_device_templates} has a distinct behavior. The telemetry payload size (from 10~bytes per payload to more than 7500~bytes per payload), format (JSON, XML, Base64) and periodicity of the communications vary between the templates. The telemetry data is transmitted in plain text for some nodes and over an encrypted channel using TLS or DTLS for other nodes. Some brokers at the cloud network accept unauthenticated clients, while others require clients to be authenticated with a username and a password before sending the data. For the MQTT-based nodes, some templates only publish to a single topic, while others publish to multiple topics. The CoAP-based nodes also serve different numbers of resources, nine for the City power and five resources for the Combined cycle nodes. Regarding data transmission behavior, some devices open a single connection with the cloud at the beginning of the transmission and keep it alive by periodically sending telemetry data and keep-alive messages. Other nodes open a new connection to the cloud, send the data and then close the connection each time they need to send telemetry.

The Gotham testbed scenario includes a total of 78~instances of those device templates, as shown in Table~\ref{tab:iot_device_templates}. To increase the heterogeneity, each instance has small random deviations and jitter following a normal distribution in the periodicity of the communications.




\subsubsection{Cloud network devices}

The cloud network includes the necessary cloud services to enable communication with the devices in the city and threat networks. The services include many MQTT brokers, CoAP clients and IP camera streaming servers. Additionally, the cloud network includes nodes to provide DNS and NTP services.

\subsubsection{Threat network devices}

To launch realistic attacks, the testbed scenario includes three threat actors.




\paragraph{Maroni Crime Family}
It includes the real Mirai~\cite{Antonakakis2017} malware. All the nodes in this threat actor were created based on the published Mirai source code~\cite{Miraisrc}, adapted and compiled to run on the testbed. The nodes include the (i) Mirai bot, (ii) Mirai Command and Control (C\&C) server, (iii) Mirai scan listener, (iv) Mirai loader and the (v) Mirai download server. All the nodes in this threat actor allow the execution of the whole Mirai malware lifecycle. We use this threat actor to perform the following attack activities:

\begin{itemize}
	\item \textbf{(A1) Mirai C\&C communication}: Includes the periodic communication between the Mirai bots and the Mirai C\&C server.
	
	\item \textbf{(A2) Mirai network scanning}: Each bot infected with Mirai scans the network in a pseudorandom order sending TCP SYN packets to the Telnet 23 and 2323 ports.
	
	\item \textbf{(A3) Mirai brute forcing}: If the Mirai scanner detects an open telnet port, it tries to brute force the credentials using a list of common IoT username and passwords.
	
	\item \textbf{(A4) Mirai reporting}: After a successful brute forcing, the Mirai bot reports the victim's IP address, port, username and password to the Mirai scan listener.
	
	\item \textbf{(A5) Mirai ingress tool transfer}: Includes the infection phase of Mirai. The Mirai loader connects to vulnerable nodes listed in the Mirai scan listener server and proceeds to download and execute the malware.
	
	\item \textbf{(A6) Mirai remote command execution}: The Mirai bot master connects to the Mirai C\&C and sends commands to the bots to perform subsequent DoS attacks against other targets in the network.
	
	\item \textbf{(A7) Mirai denial of service attacks}: The following list enumerates the performed DoS attacks by the Mirai bots against the targets (it does not include all attack types supported by Mirai): (i) UDP plain attack, (ii) UDP attack, (iii) Valve Source engine attack, (iv) DNS attack, (v) TCP ACK attack, (vi) TCP SYN attack, (vii) GRE IP attack, (viii) GRE Ethernet attack. All attacks were performed for a duration of 10s. All attacks targeted other IoT devices in the city network, except for the DNS attack, which targeted the DNS server at the cloud network.
\end{itemize}

\paragraph{Falcone Crime Family}
This threat actor includes the Merlin~\cite{Tuyl} cross-platform post-exploitation C\&C server, the Merlin agents and the hping3~\cite{Sanfilippo} TCP/IP packet assembler and analyzer. The Merlin server supports multiple protocols for C\&C (http/1.1 clear-text, http/1.1 over TLS, HTTP/2, HTTP/2 clear-text (h2c), http/3) and can execute code on the victims under its control. When a victim device is compromised with the Merlin agent, it starts communicating with the Merlin C\&C and becomes part of its botnet. We use this threat actor to perform the following attacks:

\begin{itemize}
	\item \textbf{(A8) Merlin C\&C communication}: Periodic communication between the IoT nodes infected with the Merlin agent and the Merlin C\&C server.
	
	\item \textbf{(A9) Merlin ingress tool transfer}: The Merlin C\&C server transfers the hping3 binary into each of the compromised victims through the C\&C channel.
	
	\item \textbf{(A10) Merlin remote command execution}: The Merlin bot master connects to the Merlin C\&C and sends commands to the bots to perform subsequent DoS attacks against other targets in the network.
	
	\item \textbf{(A11) Merlin denial of service attacks}: The DoS attacks are implemented using hping3: (i) ICMP echo-request, (ii) UDP, (iii) TCP SYN and (iv) TCP ACK flood attacks. Each attack generates approximately 5000~packets at 1~ms/packet rate. The UDP flood payload consists of 512~bytes of random data, with TTL set to 64 and TOS to 0, which corresponds to the default values for the UDP attack in Mirai.
\end{itemize}

\paragraph{Calabrese Crime Family}
It includes many nodes performing network-wide scanning operations and launching attacks against the devices. It is comprised of nodes that include the Nmap and Masscan scanners and the AMP-Research tool for implementing amplification attacks against the CoAP servers. We use this threat actor to perform the following attacks:

\begin{itemize}
	\item \textbf{(A12) Network-wide scans}: Masscan is used to scan the city network for the TCP ports 80,8000-8100,5683 at three different packet rates: (i) 100, (ii) 1000 and (iii) 10000~packets/s. Nmap is used to scan some IoT nodes to check for open UDP ports using three strategies: (iv) the 5683 port, (v) 600 random ports and (vi) top 1000 UDP ports.
	
	\item \textbf{(A13) CoAP amplification attack}: The attacker leverages a CoAP device in the city network to launch an amplification attack against a victim for a duration of 10s.
\end{itemize}

\subsection{Data Generation and Collection Method}

The Gotham testbed allows the capturing of network traffic traces for dataset generation. The data captured under normal traffic conditions (without attacks) will be used to train the clustered FL anomaly detection models. Then, the validation data is captured, consisting of two sets: validation-normal and validation-attack. The validation-attack is further divided into different datasets depending on the attack scenario.

All this data is captured in a federated way. Each device holds its own part of the data (captured on its network interface), as shown in \figurename~\ref{fig:architecture}. This data is never aggregated into a single dataset.

\subsubsection{Normal traffic data}

This data is composed of the normal behavior of the city network IoT/IIoT devices periodically communicating the telemetry and background data with the cloud. Network packet traces are collected for each device and saved in pcap format while the scenario runs without any attack. This dataset will be used for feature preprocessing, hyperparameter selection and the clustered FL training.

The normal traffic data has been captured in a period including the first two hours (the first hour for the IP camera related devices, due to the high data volume they create), generating a total of 3.3~GB of raw packet data for all the 78~devices in the network. 

\subsubsection{Validation-normal traffic data}

The validation-normal dataset consists of traces including only the normal behavior of the city network devices captured with the same methodology from the previously described normal dataset. However, it is extracted later so that it does not include the same events. It includes captures over a two-hour period (one hour for the IP cameras) starting after the end of the normal traffic capture. This data is not used during training; it will only be used for the anomaly threshold selection.

\subsubsection{Validation-attack traffic data}

While the city network devices are performing their normal activities, the attacker nodes become active and start launching the previously mentioned attacks against the city network IoT/IIoT devices. The validation-attack traffic data is captured during this period and consists of both normal and attacking traces.

We configure the testbed's three threat actors to create five attacking scenarios. From each scenario we extract network packet captures from the city network devices.

\paragraph{validation-attack-mirai-scan-load}

Some city network devices are first configured to make them vulnerable to Mirai as detailed in~\cite{GothamTestbed}. The testbed's Mirai bot node is activated (\textbf{A1}) and starts scanning all city network devices (\textbf{A2}). When vulnerable devices are identified, the Mirai bot performs the (\textbf{A3}) and (\textbf{A4}) activities. After a vulnerable device is reported, the (\textbf{A5}) activity is performed to integrate the device into the botnet. After becoming part of the botnet, the device repeats the described Mirai lifecycle.

\paragraph{validation-attack-mirai-cnc-dos}

This data also includes Mirai malware activity, but in this case we recompile the Mirai bot binary to disable the scanning and brute forcing modules. This modification is done to make Mirai stealthier. The modified Mirai bot is manually installed in some city network devices. After executing the bot, the C\&C communication activity starts (\textbf{A1}). By connecting to the Mirai C\&C server, we command each bot (\textbf{A6}) to launch multiple DoS attacks (\textbf{A7}) against random targets in the testbed.

\paragraph{validation-attack-merlin-cnc-dos}

The Merlin agent is installed in some city network devices. After executing the agents, they connect to the Merlin server (\textbf{A8}). For each bot, the Merlin server performs (\textbf{A9}) and (\textbf{A10}). Finally, each bot is instructed to launch DoS attacks (\textbf{A11}) against random targets in the testbed.

\paragraph{validation-attack-masscan}

Network traffic data is captured from the city network devices while they are being scanned by the Masscan node (\textbf{A12}).

\paragraph{validation-attack-scan-amplification}

This data is captured on the CoAP-based city network devices. First, Nmap is used to scan the network (\textbf{A12}) to search for CoAP devices; then, those devices are leveraged to perform (\textbf{A13}) attacks against random targets in the testbed.

\subsection{Machine Learning and Federated Learning Setup}

We used the PyTorch~\cite{Paszke2019} Python library to implement the ML models and training procedures. The FL model aggregation (the \textsc{ServerOpt} server-side optimization) is also implemented using the PyTorch library directly. For the client local training process, we used GNU Parallel~\cite{Tange2011} to coordinate and execute all the jobs in parallel. We implemented the clustering algorithms, validation metrics, dimensionality reduction, etc. with scikit-learn~\cite{scikit-learn}.

\section{Implementation} \label{sec:implementation}

This section will describe the methodology followed to perform the experimentation. A visual representation of all the steps is shown in \figurename~\ref{fig:implementation_pipeline}. We first explain the network data processing step, which includes filtering, feature extraction and preprocessing. Then, we detail the autoencoder model selection procedure. Next, we describe the implementation for the clustered FL process, starting from the model fingerprinting for device clustering, followed by the federated hyperparameter tuning and then the FL training process for each identified cluster. Finally, we review the trained models' anomaly detection evaluation process and metrics. We additionally explain the baseline comparisons done with other state-of-the-art IDS methods.

\begin{figure*}
	\centering
	\includegraphics[width=1.0\linewidth]{images/implementation_method.pdf}
	\caption{Implementation method pipeline.}
	\label{fig:implementation_pipeline}
\end{figure*}

\subsection{Network Data Processing}

After collecting the dataset, the raw pcap files are first filtered, then relevant network features are selected, and finally, those features are preprocessed to make them suitable as input to the ML models. Note that the dataset is federated and not centralized; each device holds its fraction of data.

The first step consists of filtering the raw pcap files to drop all IPv6 and ARP packets. The filtered packets are passed to the feature extraction process. For each network packet in the filtered pcap file, a set of 11~features are extracted as listed in Table~\ref{tab:packet_features_description}. The source and destination IP addresses were discarded to prevent the model from learning the machines themselves instead of the attacking nature. The main reason for using those features is that attacking patterns from IoT malware such as Mirai includes options to craft packets with tweaked values for the payload size, IP header fields, TCP flags, among others~\cite{Miraisrc}. Additionally, the attack packet payload usually includes randomized or some fixed values that can lead to high or low entropy. By selecting those features and training on the normal traffic dataset, the model learns the distribution of normal IoT communication. Deviations from it (large MSE between the input and the autoencoder reconstructed output) allows to potentially detect not only Mirai, but in general other malware with similar network attacking behavior and C\&C communication like the attacks performed with hping3 and Merlin C\&C as described in the previous section.

Due to the different orders of magnitude of some features, and the mixture of both numerical and categorical variables, a feature preprocessing step is necessary before using them as inputs for the ML models.

\begin{table}[t]
    \small
    \centering
    
    \caption{Selected packet feature names and descriptions.}
    \label{tab:packet_features_description}

    \begin{tabularx}{1.0\linewidth}{l|l}
    \toprule
        Feature name & Description \\
    \midrule
        \texttt{len} & Full packet length in bytes. \\
        \texttt{iat} & Inter arrival time from the previous packet. \\
        \texttt{h} & Entropy (base 2) of the full packet. \\
        \texttt{ip\_tos} & IP type of service. \\
        \texttt{ip\_flags} & IP flags (MF, DF, R bits). \\
        \texttt{ip\_ttl} & IP time to live. \\
        \texttt{ip\_proto} & IP protocol (TCP, UDP, ICMP). \\
        \texttt{src\_port} & Source port number. \\
        \texttt{dst\_port} & Destination port number. \\
        \texttt{tcp\_flags} & TCP flags (F, S, R, P, A, U, E, C, N). \\
        \texttt{tcp\_win} & TCP window size. \\
    \bottomrule
    \end{tabularx}

\end{table}

The numerical features are normalized by the maximum value of each field defined in the TCP/IP stack. \texttt{len} is divided by 1514 (the Ethernet maximum transmission size plus the header), both \texttt{ip\_tos} and \texttt{ip\_ttl} are divided by 255 and \texttt{tcp\_win} is divided by 65535. \texttt{h} is divided by 8 and \texttt{iat} is transformed with the natural logarithm of \texttt{iat} plus one. The categorical variables \texttt{ip\_flags}, \texttt{ip\_proto} and \texttt{tcp\_flags} are directly one-hot encoded.

\subsubsection{Source and destination port feature processing}


The \texttt{src\_port} and \texttt{dst\_port} feature processing requires special consideration. The application port numbers are numerical features that can take $2^{16}$ different integer values; however, treating the ports just as a numerical feature does not maintain the semantics of the services that use those port numbers. In other words, port numbers that are numerically close to each other does not mean that the programs that communicate with those ports are used to perform similar tasks.

In this work, we are going to discretize the source and destination port numbers into a smaller number of bins using two different strategies: three-range discretization and hierarchical discretization. After the discretization, the bin numbers are one-hot encoded. In section~\ref{sec:results}, we will evaluate the differences between the two strategies and select the most appropriate one for this use case.

\paragraph{Three-range discretization}

The source and destination port numbers are divided into the three ranges assigned by the Internet Assigned Numbers Authority (IANA)~\cite{RFC6335}: the System Ports, or Well Known Ports, from 0 to 1023; the User Ports, or Registered Ports, from 1024 to 49151; and the Dynamic Ports, or Ephemeral Ports, from 49152 to 65535. These ranges are large, and might not capture the semantics of the ports.

After all the previously mentioned transformations and using the three-range discretization for the source and destination ports, the 11~features of Table~\ref{tab:packet_features_description} are transformed into a set of 27~features.

\paragraph{Hierarchical discretization}

The source and destination port numbers are discretized based on the generalization hierarchy presented in~\cite{Zurutuza2008} and adapted to include information about the MQTT, CoAP and RTSP ports used in the testbed. The used hierarchy is summarized in Table~\ref{tab:port_hierarchichal_discretization}. When classifying a port number, bins from the top of the table have precedence over the bins from the bottom.

After all the previously mentioned transformations and using the hierarchical discretization for the source and destination ports, the 11~features of Table~\ref{tab:packet_features_description} are transformed into a set of 69~features.

\begin{table}[t]
	\small
	\centering
	
	\caption{Port number generalization hierarchy.}
	\label{tab:port_hierarchichal_discretization}
	
	\begin{tabularx}{1.0\linewidth}{p{0.27\linewidth}|X}
		\toprule
		Bin name           & Ports                                                            \\
		\midrule
		mqttPorts          & 1883, 8883                                                     \\
		coapPorts          & 5683, 5684                                                     \\
		rtspPorts          & 8554, 8322, 8000 -- 8003, 1935, 8888                 \\
		httpPorts          & 80, 280, 443, 591, 593, 777, 488, 1183, 1184, 2069, 2301, 2381, 8008, 8080  \\
		mailPorts          & 24, 25, 50, 58, 61, 109, 110, 143, 158, 174, 209, 220, 406, 512, 585, 993, 995 \\
		dnsPorts           & 42, 53, 81, 101, 105, 261                                           \\
		ftpPorts           & 20, 21, 47, 69, 115, 152, 189, 349, 574, 662, 989, 990                    \\
		shellPorts         & 22, 23, 59, 87, 89, 107, 211, 221, 222, 513, 614, 759, 992                 \\
		remoteExecPorts    & 512, 514                                                        \\
		authPorts          & 13, 56, 113, 316, 353, 370, 749, 750                                  \\
		passwordPorts      & 229, 464, 586, 774                                                \\
		newsPorts          & 114, 119, 532, 563                                                \\
		chatPorts          & 194, 258, 531, 994                                                \\
		printPorts         & 35, 92, 170, 515, 631                                              \\
		timePorts          & 13, 37, 52, 123, 519, 525                                           \\
		dbmsPorts          & 65, 66, 118, 150, 156, 217                                          \\
		dhcpPorts          & 546, 547, 647, 847                                                \\
		whoisPorts         & 43, 63                                                          \\
		netbiosPorts       & 137 -- 139                                                 \\
		kerberosPorts      & 88, 748, 750                                                     \\
		RPCPorts           & 111, 121, 369, 530, 567, 593, 602                                    \\
		snmpPorts          & 161, 162, 391                                                    \\
		privilegedPorts    & 0 -- 1023                                                    \\
		nonprivilegedPorts & 1024 -- 65535                                                \\
		\bottomrule
	\end{tabularx}
	
\end{table}

\subsection{Autoencoder Model Selection}

In FL, there is a much larger set of hyperparameters to be tuned compared to a typical centralized ML setting. Those parameters include the ML model itself (number of layers, number of nodes per layer, activation functions, etc.), client-side optimizer algorithm \textsc{ClientOpt} and learning rate~$\eta$, server-side optimizer algorithm \textsc{ServerOpt} and learning rate~$\eta_s$, number of local training epochs~$E$, number of FL rounds~$R$ and number of clients sampled per FL round~$M$. Due to the infeasibility to explore all the combinations simultaneously, we are going to simplify the search tuning those hyperparameters step by step using different subsets of combinations. Additionally, considering the unsupervised nature of the problem (or rather semi-supervised, given that it is trained on normal data without attacks), the selection is going to be based on those values that minimize the MSE loss in fewer number of rounds/epochs.

First, we start defining the general architecture of the autoencoder. We select a small subset of the normal traffic data (corresponding to various IoT clients) and use it to explore different autoencoder models. This exploration is not performed in a federated way. Each dataset is partitioned into 80\% train and 20\% validation. Among the tested models, we selected the simplest one (fewer number of parameters) that produced low enough validation loss to minimize overfitting problems.

The number of nodes for the input and output layer of the autoencoder is fixed to the same number as the input feature dimensions (which can be 27 or 69, depending on the discretization method for the source and destination ports). For the encoder part, we evaluated different combinations with 1, 2 and 3~hidden layers, with each following layer having half as many nodes as the previous one~$\lfloor \frac{\text{\# nodes previous layer}}{2} \rfloor$, and a symmetric decoder. The selected autoencoder model and hyperparameters will be used in the next step: device clustering.

\subsection{Device Clustering}

Using the selected autoencoder model from the previous step, the device clustering process begins, which consists of the first phase detailed in Algorithm~\ref{alg:heterogeneousfl}.

The FL server initializes the selected autoencoder model and distributes it to all IoT clients (78 nodes in the city network). Each client locally trains the model for $\epsilon$~epochs using as the \textsc{ClientOpt} optimizer, the optimizer selected from the previous step. The partially trained models are uploaded back to the server to start the model fingerprinting and clustering process. As detailed in Algorithm~\ref{alg:heterogeneousfl}, the server flattens the parameters of each model and performs PCA to reduce the dimensionality of the parameters. We are going to select the number of components needed to explain at least 90\% of the variance. We use the K-means algorithm with the k-means++ initialization scheme to cluster the models, and hence the clients.

The experiments are repeated for different values of $\epsilon = 1, 2, 4, 8, 16$ and $32$, and the optimal number of clusters~$K$ is automatically selected based on the analysis of the following internal validation metrics: Silhouette, Davies-Bouldin and S Dbw.

\subsection{Federated Hyperparameter Tuning}

In this step we are going to tune the rest of the FL hyperparameters. Each cluster identified in the previous step will have its own federated hyperparameter tuning. First, we are going to tune the \textsc{ClientOpt} and \textsc{ServerOpt} optimizer algorithms. Then, for the selected \textsc{ClientOpt} and \textsc{ServerOpt}, we are going to further refine the client and server learning rates. While in the previous step of autoencoder model selection the client optimizer and learning rates were selected, these values might not be optimal for the FL training process.

The \textsc{ClientOpt} and \textsc{ServerOpt} is tuned by comparing multiple combinations of SGD (with and without momentum) and Adam optimizers both for the clients and the server. SGD is tested without momentum and with momentum set to $0.9$ (as suggested in~\cite{Wang2021a}), for Adam two combinations are tested: $\beta_1 = 0.9$, $\beta_2 = 0.999$, $\epsilon = 1\times10^{-8}$ (default values defined in PyTorch) and $\beta_1 = 0.9$, $\beta_2 = 0.99$, $\epsilon = 0.001$ (as suggested in~\cite{Wang2021a}). We select the optimizer combination that minimizes the average evaluation loss across all the clients (each client trains on 80\% of its local data and evaluates on the remaining 20\%) in the fewer number of FL training rounds.

Then, the $\eta$ and $\eta_s$ learning rates are refined via grid search. For all the hyperparameter tuning we set $E = 1$, and use the same random seed to initialize the ML model parameters in order to reduce the effects of the random model initialization noise.

\subsection{Clustered Federated Learning}

After the device clustering and federated hyperparameter tuning steps, we perform the clustered federated learning using the fine-tuned \textsc{ClientOpt} and \textsc{ServerOpt} optimizers and their respective learning rates. we perform $R$ FL rounds to train the $K$ global models, one for each identified cluster. In this case, we repeat the process for different values of the number of local training epochs $E = 1, 2, 4$ and $8$ to evaluate its effect in the training process.












\subsection{Anomaly Detection}

After training the models with FL, we are going to evaluate the anomaly detection performance of the resulting $K$ global models. Recall that at this step, each device holds a local copy of the global model that corresponds to its cluster. To estimate the unsupervised anomaly detection capabilities, we are going to use the validation-normal and validation-attack datasets.

\subsubsection{Threshold selection}

For each client, we will first evaluate the trained global model on its corresponding validation-normal dataset to estimate the anomaly detection threshold. Note that the evaluation is local; therefore, each device will compute its own threshold value. We will opt for a simplistic approach and select the largest MSE from the validation-normal dataset as the threshold. Packets with $\text{MSE} > \text{threshold}$ will be considered anomalous. Then, we will evaluate the same model on the multiple validation-attack datasets to identify all the anomalous packets.

\subsubsection{Anomaly detection performance}

We are going to compare the detected anomalies with the ground truth labels based on the IP addresses of the known attacker nodes, victim nodes and the cloud nodes (The IP addresses are never used for model training). In a real deployment, prior labeling of the network data might not be feasible, but here it will give us an estimate of the performance of the global models to detect the attacks considered in our threat model; however, note that the manual labeling process is a heuristic and might misclassify some packets. We provide the standard confusion matrix metrics: true positives (TP), false negatives (FN), false positives (FP), true negatives (TN) and its derivate metrics, including accuracy, F1 score and Matthews correlation coefficient (MCC).

\subsection{Baseline experimental comparison}

For the baseline experimental comparison, we are considering Kitsune~\cite{Mirsky2018}. Kitsune is a state-of-the-art network IDS that uses an ensemble of autoencoders trained in an unsupervised and online manner. The similarities of being unsupervised and based on autoencoders make it an interesting comparison; however, there are some fundamental differences between Kitsune and the proposed method. First, Kitsune does not use FL to train the model; it is deployed in each machine and only uses local data. Second, Kitsune uses features based on temporal statistics of network packets taken over multiple damped windows; instead, we extract features obtained from each packet in isolation. Third, Kitsune is trained in an online manner, so the training is performed using one sample at a time instead of multiple training iterations over batches of the data. Another difference is that Kitsune does not filter IPv6 or ARP packets.

\section{Results} \label{sec:results}

In this section, we present the results obtained from the experiments described in section~\ref{sec:implementation}.

\subsection{Autoencoder Model Selection}

As previously stated, the input and output shapes of the autoencoder are the same as the number of input data feature dimensions. Depending on the selected source and destination port discretization method from the network data processing step, the dimensions are 27 or 69 for the three-range and hierarchical discretization, respectively. For the autoencoder model selection, we will consider both cases. The distinction between the two methods will be shown later in the device clustering results.

We detected no significant improvement in the validation loss after 2~hidden encoder layers, irrespective of the port discretization method. Thus, for the three-range discretization method, the final autoencoder model is a two hidden layer encoder with 13~and~6 nodes, respectively, and a symmetric decoder with 6~and~13 nodes. For the hierarchical discretization, the encoder layers include 34~and~17 nodes with a symmetric decoder. We use the $ReLU$~activation function after each layer. The optimizer is Adam with a $1\times10^{-3}$ learning rate, $L_2$ regularization weight from equation~\eqref{eq:loss_function} $\lambda = 1\times10^{-5}$ and a batch size of~32.

\subsection{Device Clustering}

The clustering experiments are repeated for the two port discretization methods and multiple values of the local training epochs $\epsilon = 1, 2, 4, 8, 16$ and $32$ using the client optimizer parameters obtained from the previous autoencoder model selection step. For each value of $\epsilon$, to identify the optimal number of clusters~$K$, we perform K-means clustering with $K$ ranging from $2$ to $40$ cluster centroids. The results for $\epsilon = 4$ using the three-range port discretization method are shown in \figurename~\ref{fig:clus_4e_port_three}, while the results for $\epsilon = 4$ using the hierarchical discretization method are shown in \figurename~\ref{fig:clus_4e_port_hier}.

The unsupervised clustering quality scores are shown in \figurename~\ref{fig:clus_4e_3p_unsup} and \figurename~\ref{fig:clus_4e_hp_unsup}. For Silhouette higher scores represent better clusters, for Davies-Bouldin and S\_Dbw lower scores represent better clusters. The dotted vertical line marks the selected $K$ for each discretization method. For the three-range discretization, the Silhouette score is maximized at $K=16$, and the Davies-Bouldin score shows a dip at the same point. For S\_Dbw the score is monotonously decreasing; however 16 is a good number of clusters based on the elbow method. For the hierarchical discretization method, the Silhouette score is maximized at $K=8$ and both Davies-Bouldin and S\_Dbw show a dip at that point. A 2D projection of the model fingerprints and the clustering results is represented in \figurename~\ref{fig:clus_4e_3p_pca} for $K=16$ and \figurename~\ref{fig:clus_4e_hp_pca} for $K=8$.

Due to the differences in the clustering results depending on the port discretization strategy, we perform an additional experiment. For each strategy, we measure the similarity between the K-means clustering labels and the ground truth clustering using the adjusted Rand index, adjusted mutual information score and the V-measure score. To create the ground truth labeling, we assign each instance a label based on its template type according to the data in Table~\ref{tab:iot_device_templates} (12 different labels). The results are shown in \figurename~\ref{fig:clus_4e_3p_sup} and \figurename~\ref{fig:clus_4e_hp_sup}. For the three-port discretization, the score is maximized when $K=12$ (in contrast to the $K=16$ from the internal validation metrics). The results for the hierarchical discretization method in \figurename~\ref{fig:clus_4e_hp_sup} show a maximum in $K=8$ (same results as with the internal validation metrics) and overall higher scores compared to the previous method.

Using the hierarchical discretization method we obtain agreement in the optimal value for $K$ between the unsupervised clustering and the similarity with the ground truth scores. It also creates a clearer distinction between the clusters (\figurename~\ref{fig:clus_4e_hp_pca}) compared to the three-port discretization method (\figurename~\ref{fig:clus_4e_3p_pca}). Additionally, it may be desirable to lean towards small $K$ values so that the FL process benefits from a larger cohort size for each cluster. From now on, we are going to use the hierarchical discretization method and $K=8$ for the rest of the experimentation.

The device clustering results from \figurename~\ref{fig:clus_4e_hp_pca} are shown in Table~\ref{tab:clustering_results}. All the IP camera related devices are grouped into the same cluster. Interestingly, for devices of the same type, the clustering method can distinguish between those communicating via plain text or over an encrypted channel; for example, the Predictive maintenance devices in Cluster 5 and Cluster 6 or the Combined cycle in Cluster 2 and Cluster 7. Clusters 0, 2 and 4 are composed of heterogeneous devices; however, the devices in the same cluster communicate using the same primary protocol: MQTT, CoAP and RTSP, respectively.

For values of $\epsilon$ greater than 8, the number of identified clusters tends to increase. For the smaller $\epsilon$ values, the results are similar to $\epsilon = 4$. In some cases the number of clusters decreases to 7, merging the clusters of the same device types that communicate in plain or over encrypted channel.

\begin{figure*}
	\centering
	
	\subfloat[Unsupervised clustering quality scores.\label{fig:clus_4e_3p_unsup}]{%
		\includegraphics[width=0.32\linewidth]{images/3ports_score_unsupervised.pdf}
	}
	\hfill
	\subfloat[Clustering quality scores compared to ground truth labeling.\label{fig:clus_4e_3p_sup}]{%
		\includegraphics[width=0.32\linewidth]{images/3ports_score_groundtruth.pdf}
	}
	\hfill
	\subfloat[2D projection of the model fingerprints clustered into $K = 16$ groups.\label{fig:clus_4e_3p_pca}]{%
		\includegraphics[width=0.32\linewidth]{images/3ports_2d_map.pdf}
	}
	
	\caption{Device clustering results for $\epsilon = 4$ using the three-range discretization strategy for the source and destination ports.}
	\label{fig:clus_4e_port_three}
\end{figure*}

\begin{figure*}
	\centering
	
	\subfloat[Unsupervised clustering quality scores.\label{fig:clus_4e_hp_unsup}]{%
		\includegraphics[width=0.32\linewidth]{images/hierports_score_unsupervised.pdf}
	}
	\hfill
	\subfloat[Clustering quality scores compared to ground truth labeling.\label{fig:clus_4e_hp_sup}]{%
		\includegraphics[width=0.32\linewidth]{images/hierports_score_groundtruth.pdf}
	}
	\hfill
	\subfloat[2D projection of the model fingerprints clustered into $K = 8$ groups.\label{fig:clus_4e_hp_pca}]{%
		\includegraphics[width=0.32\linewidth]{images/hierports_2d_map.pdf}
	}
	
	\caption{Device clustering results for $\epsilon = 4$ using the hierarchical discretization strategy for the source and destination ports.}
	\label{fig:clus_4e_port_hier}
\end{figure*}

\begin{table}[t]
	\small
	\centering
	
	\caption{Unsupervised clustering results using the hierarchical port discretization strategy for $\epsilon = 4$ and $K = 8$.}
	\label{tab:clustering_results}
	
	\begin{tabularx}{1.0\linewidth}{l|X}
		\toprule
		Cluster name   & Cluster contents                                                      \\
		\midrule
		Cluster 0      & Air quality (x1), Building monitor (x5), Domotic monitor (x5)         \\
		Cluster 1      & Hydraulic system (x15)                                                \\
		Cluster 2      & City power (x1), Combined cycle (x10)                                 \\
		Cluster 3      & Cooler motor (x15)                                                    \\
		Cluster 4      & IP camera museum (x2), IP camera street (x2), IP camera consumer (x2) \\
		Cluster 5      & Predictive maintenance (x10)                                          \\
		Cluster 6      & Predictive maintenance (x5)                                         \\
		Cluster 7      & Combined cycle tls (x5)                                               \\
		\bottomrule
	\end{tabularx}
	
\end{table}

\subsection{Federated Hyperparameter Tuning}

To select the \textsc{ClientOpt} and \textsc{ServerOpt} optimizer algorithms, for each cluster we performed 16~trials consisting of different combinations of SGD and Adam as defined in Table~\ref{tab:hyperparameter_trials}. Client learning rates are fixed to $\eta = 1\times10^{-3}$ and the $L_2$ regularization weight from equation~\eqref{eq:loss_function} is set to $\lambda = 1\times10^{-5}$.

We show the results of the mean evaluation loss progression for 100~FL rounds and all the trials for the Clusters 0, 2 and 4 in \figurename~\ref{fig:clientopt_serveropt}, as these three clusters are more complex than the others because they are formed by heterogeneous devices. In general, including adaptive optimization methods for \textsc{ClientOpt}, \textsc{ServerOpt} or both provides faster convergence and smaller losses compared to the standard SGD; however, some combinations have difficulty to converge, showing an increasing loss trend as in Trial~10 for Cluster 0 (\figurename~\ref{fig:opt_clus0}). For Cluster 0, Trial 9 clearly shows faster convergence speeds and a smaller evaluation loss after 100~FL rounds. Trial 12 also shows a similar evaluation loss at the last round, but at a much slower convergence rate. For Clusters 2 and 4 (\figurename~\ref{fig:opt_clus2} and \figurename~\ref{fig:opt_clus4}), Trials 9 and 10 show the best performance. Trial 10 from Cluster 2 reaches smaller loss than Trial 9; however, by fine tuning the Trial 9 learning rates it can reach same loss values.

The client and server learning rates ($\eta$ and $\eta_s$, respectively) for each cluster are fine tuned by performing a grid search varying both values simultaneously. The results are shown in the heat maps from \figurename~\ref{fig:gridsearch}. The heat maps show the logarithm of the evaluation loss after 60~FL rounds; darker colors show a smaller loss. For the three cases, many combinations achieve a similar low loss; we are going to select the combination with a smaller loss for all cases.

The final optimizer selection are as follows. Cluster 0 \textsc{ClientOpt} is Adam1 with $\eta = 0.005$, and \textsc{ServerOpt} is SGD with $\eta_s = 0.75$. Cluster 2 \textsc{ClientOpt} is Adam1 with $\eta = 0.005$,  and \textsc{ServerOpt} is SGD with $\eta_s = 1.25$. Cluster 4 \textsc{ClientOpt} is Adam1 with $\eta = 0.001$, and \textsc{ServerOpt} is SGD with $\eta_s = 1.5$.

\begin{table}[t]
	\small
	\centering

	\caption{\textsc{ClientOpt} and \textsc{ServerOpt} combinations for each hyperparameter tuning trial. ``SGD'' is SGD without momentum, ``SGDm'' refers to SGD with momentum $0.9$, ``Adam1'' refers to $\beta_1 = 0.9$, $\beta_2 = 0.999$, $\epsilon = 1\times10^{-8}$ and ``Adam2'' refers to Adam $\beta_1=0.9$, $\beta_2=0.99$, $\epsilon=10^{-3}$. $\lambda$ is set to $1\times10^{-5}$ for all trials.}
	\label{tab:hyperparameter_trials}

	\begin{tabularx}{1.0\linewidth}{l|Xl|Xl}
		\toprule
		Trial    & \textsc{ClientOpt} & $\eta$           & \textsc{ServerOpt} & $\eta_s$ \\
		\midrule
		Trial 1  & SGD                & $1\times10^{-3}$ & SGD                & $1.0$      \\
		Trial 2  & SGD                & $1\times10^{-3}$ & SGDm               & $1.0$      \\
		Trial 3  & SGD                & $1\times10^{-3}$ & Adam1              & $1\times10^{-2}$      \\
		Trial 4  & SGD                & $1\times10^{-3}$ & Adam2              & $1\times10^{-2}$      \\
		Trial 5  & SGDm               & $1\times10^{-3}$ & SGD                & $1.0$      \\
		Trial 6  & SGDm               & $1\times10^{-3}$ & SGDm               & $1.0$      \\
		Trial 7  & SGDm               & $1\times10^{-3}$ & Adam1              & $1\times10^{-2}$      \\
		Trial 8  & SGDm               & $1\times10^{-3}$ & Adam2              & $1\times10^{-2}$      \\
		Trial 9  & Adam1              & $1\times10^{-3}$ & SGD                & $1.0$      \\
		Trial 10 & Adam1              & $1\times10^{-3}$ & SGDm               & $1.0$      \\
		Trial 11 & Adam1              & $1\times10^{-3}$ & Adam1              & $1\times10^{-2}$      \\
		Trial 12 & Adam1              & $1\times10^{-3}$ & Adam2              & $1\times10^{-2}$      \\
		Trial 13 & Adam2              & $1\times10^{-3}$ & SGD                & $1.0$      \\
		Trial 14 & Adam2              & $1\times10^{-3}$ & SGDm               & $1.0$      \\
		Trial 15 & Adam2              & $1\times10^{-3}$ & Adam1              & $1\times10^{-2}$      \\
		Trial 16 & Adam2              & $1\times10^{-3}$ & Adam2              & $1\times10^{-2}$      \\
		\bottomrule
	\end{tabularx}
\end{table}

\begin{figure*}
	\centering
	
	\subfloat[Cluster 0 (MQTT).\label{fig:opt_clus0}]{%
		\includegraphics[width=0.32\linewidth]{images/cluster_mqtt_clientopt_serveropt.pdf}
	}
	\hfill
	\subfloat[Cluster 2 (CoAP).\label{fig:opt_clus2}]{%
		\includegraphics[width=0.32\linewidth]{images/cluster_coap_clientopt_serveropt.pdf}
	}
	\hfill
	\subfloat[Cluster 4 (Camera).\label{fig:opt_clus4}]{%
		\includegraphics[width=0.32\linewidth]{images/cluster_cam_clientopt_serveropt.pdf}
	}
	
	\caption{Federated hyperparameter tuning, \textsc{ClientOpt} and \textsc{ServerOpt} optimizer selection. $E = 1$. The trials are defined in Table~\ref{tab:hyperparameter_trials}.}
	\label{fig:clientopt_serveropt}
\end{figure*}

\begin{figure*}
	\centering
	
	\subfloat[Cluster 0 (MQTT). Grid search for Trial 9.\label{fig:gs_clus0}]{%
		\includegraphics[width=0.32\linewidth]{images/cluster_mqtt_lrgs_adam1_sgd.pdf}
	}
	\hfill
	\subfloat[Cluster 2 (CoAP). Grid search for Trial 9.\label{fig:gs_clus2}]{%
		\includegraphics[width=0.32\linewidth]{images/cluster_coap_lrgs_adam1_sgd.pdf}
	}
	\hfill
	\subfloat[Cluster 4 (Camera). Grid search for Trial 9.\label{fig:gs_clus4}]{%
		\includegraphics[width=0.32\linewidth]{images/cluster_cam_lrgs_adam1_sgd.pdf}
	}
	
	\caption{Federated hyperparameter tuning, $\eta$ and $\eta_s$ learning rate grid search. The values represent base~10 logarithm of the evaluation loss after 60~FL rounds.}
	\label{fig:gridsearch}
\end{figure*}


\subsection{Clustered Federated Learning}

The final FL training process is performed using the client-side and server-side optimizers and learning rates obtained after the federated hyperparameter tuning described in the previous step for each identified cluster. We repeated the experiments for different values of the number of local training epochs $E = 1, 2, 4$ and 8. Increasing the number of local training epochs generally leads to lower loss values and fewer FL rounds to reach convergence at the expense of more local computation time. However, we also observed an increased variance in the loss distribution across the devices of the cluster when using large numbers of local training epochs. The training results for $E=4$ local epochs and $R=100$ FL rounds are shown in \figurename~\ref{fig:boxplot_fl_nofl} for Clusters 0, 2 and 4. Each boxplot shows the evaluation loss distribution across the devices of the cluster at a certain FL round.

\begin{figure*}
	\centering
	
	\subfloat[Cluster 0 (MQTT).\label{fig:boxplot_clus0}]{%
		\includegraphics[width=0.32\linewidth]{images/cluster_mqtt_e4_boxplot.pdf}
	}
	\hfill
	\subfloat[Cluster 2 (CoAP).\label{fig:boxplot_clus2}]{%
		\includegraphics[width=0.32\linewidth]{images/cluster_coap_e4_boxplot.pdf}
	}
	\hfill
	\subfloat[Cluster 4 (Camera).\label{fig:boxplot_clus4}]{%
		\includegraphics[width=0.32\linewidth]{images/cluster_cam_e4_boxplot.pdf}
	}
	
	\caption{Clustered FL training progression for $E=4$ local training epochs and $R=100$ FL rounds (blue boxplots). It is compared with isolated edge training where each device trains on its own dataset for $R \times E = 400$ epochs (orange boxplots).}
	\label{fig:boxplot_fl_nofl}
\end{figure*}


\subsubsection{Training baseline}

As a training baseline, we performed additional experiments to compare the training evaluation loss progression between FL and isolated edge training, where each device trains on its local data without cooperation. In isolated training, each device in the cluster starts with a random initialization of the autoencoder and trains it using the same client-side optimizer as in the FL case. The training is performed for a total of $R \times E$~epochs so that the amount of local training performed by each device is comparable to the FL case. The comparison is shown in \figurename~\ref{fig:boxplot_fl_nofl}.

For Clusters 0 and 2, there is a noticeable gap in the evaluation loss between the FL and isolated training methods, where FL shows a faster convergence rate, especially in early rounds. For Cluster 4, while FL shows a lower average loss, the loss distribution is similar to the isolated training.

This difference might be explained due to the different training data volumes generated by each device. Cluster 4 devices generate a much larger data volume because they are comprised of image streaming devices, ranging between 300 to 800 MB of raw pcap data; this extensive training data can benefit local isolated training. However, the raw pcap data for Cluster 0 devices ranges approximately between 230 KB to 270 KB. For Cluster 2 devices, the raw data is between 100 KB to 170 KB. This suggests the advantages of using FL for devices that generate a low volume of training data samples.

\subsection{Anomaly Detection}

Here we provide the anomaly detection performance results for clusters 0, 2 and 4 by evaluating the trained global models from the previous step on the multiple validation-attack datasets described in section~\ref{sec:experimentalsetup}. The number of packets (normal and anomalous) after filtering the pcaps is shown in table~\ref{tab:valid-attack-numpackets}. As explained in section~\ref{sec:implementation}, the anomaly threshold of each device is selected so that there are no false positives in the device's validation-normal dataset. The attack packets are considered as the positive class.

\begin{table}[t]
	\small
	\centering
	
	\caption{The number of packets after the IPv6 and ARP filtering step in the validation-attack datasets.}
	\label{tab:valid-attack-numpackets}
	
	\begin{tabularx}{1.0\linewidth}{X|l|l|l}
		\toprule
		validation-attack-* & Cluster 0 & Cluster 2 & Cluster 4 \\ \midrule
		mirai-scan-load     & 110,354   & 70,592    & 888,833   \\
		mirai-cnc-dos       & 6,810,612 & 7,777,427 & 7,924,688 \\
		merlin-cnc-dos      & 32,282    & 31,014    & 868,586   \\
		masscan             & 816       & 571       & 203,814   \\
		scan-amplification  & n/a       & 68,422    & n/a       \\ \bottomrule
	\end{tabularx}
\end{table}

\subsubsection{Cluster 0 (MQTT)}

We evaluate the global model of the Cluster 0 on the four validation-attack datasets captured form one instance of the Building monitor device.

For the mirai-scan-load data, the reconstruction error of all anomalous packets are above the threshold and the normal packets have a low reconstruction error except for a single false positive: TP, FN, FP, TN = 108532, 0, 1, 1821 (0.9999 accuracy, 0.9999 F1 and 0.9997 MCC). Similarly, for the masscan data the model correctly classified all packets: TP, FN, FP, TN = 528, 0, 0, 288 (1.0 accuracy, F1 and MCC).

For the mirai-cnc-dos and merlin-cnc-dos datasets, some false negatives are reported. In the mirai-cnc-dos case, the C\&C activity and seven out of eight DoS attacks were all correctly classified; however, some (but not all) of the attack packets corresponding to the same time frame when the DNS attack was being performed were below the anomaly threshold: TP, FN, FP, TN = 6743222, 66190, 0, 1200 (0.9903 accuracy, 0.9951 F1 and 0.1328 MCC). For the merlin-cnc-dos case, the C\&C activity, ingress tool transfer and three out of four attacks were all correctly classified. The model did not detect the anomalous packets corresponding to the ICMP flood attack: TP, FN, FP, TN = 25828, 5277, 0, 1177 (0.8365 accuracy, 0.9073 F1 and 0.3891 MCC). The reconstruction error scatter plot for the merlin-cnc-dos case is shown in \figurename~\ref{fig:anomaly_flids_c0_merlin}.

\subsubsection{Cluster 2 (CoAP)}

The global model of the Cluster 2 is evaluated on the five validation-attack datasets captured from one instance of the Combined cycle device.

This model correctly classified all normal and anomalous packets for all the validation-attack datasets except for a single false negative packet. The mirai-scan-load case obtained: TP, FN, FP, TN =  70195, 0, 0, 397 (1.0 accuracy, F1 and MCC). For the mirai-cnc-dos data: TP, FN, FP, TN =  7777173, 0, 0, 254 (1.0 accuracy, F1 and MCC). In the merlin-cnc-dos case: TP, FN, FP, TN =  30754, 0, 0, 260 (1.0 accuracy, F1 and MCC). The masscan data obtained: TP, FN, FP, TN =  522, 0, 0, 49 (1.0 accuracy, F1 and MCC). And lastly, the scan-amplification: TP, FN, FP, TN =  68237, 1, 0, 184 (0.9999 accuracy, 0.9999 F1 and 0.9973 MCC). The reconstruction error for the scan-amplification dataset is shown in \figurename~\ref{fig:anomaly_flids_c2_nmapampli}.

\subsubsection{Cluster 4 (Camera)}

We evaluate the global model of the Cluster 4 on the four validation-attack datasets captured from one instance of the IP camera museum.

For the mirai-scan-load data, the reconstruction error of all anomalous packets are above the threshold and the normal packets are correctly classified except for a false positive: TP, FN, FP, TN =  81604, 0, 1, 807228 (0.9999 accuracy, 0.9999 F1 and 0.9999 MCC). This case is shown in \figurename~\ref{fig:anomaly_flids_c4_miraiscanload}. The number of false positives and false negatives is slightly increased in the mirai-cnc-dos dataset, part (but not all) of the packets corresponding to the time frame where the Mirai GRE IP and GRE Ethernet attacks are below the threshold: TP, FN, FP, TN =  7424929, 224, 4, 499531 (0.9999 accuracy, 0.9999 F1 and 0.9997 MCC).

For the merlin-cnc-dos dataset, all the packets were correctly classified: TP, FN, FP, TN =  30990, 0, 0, 837596 (1.0 accuracy, F1 and MCC). Similarly, the packets of the masscan dataset were also correctly classified: TP, FN, FP, TN =  548, 0, 0, 203266 (1.0 accuracy, F1 and MCC).


\begin{figure*}
	\centering
	
	\subfloat[Cluster 0 global model evaluated on validation-attack-merlin-cnc-dos dataset for the same cluster. (A) start Merlin agent. (B) hping3 upload. (C), (D), (E) and (F) ICMP, UDP, SYN and ACK attacks, respectively. (G) stop Merlin agent. The (C) event packets are below the threshold.\label{fig:anomaly_flids_c0_merlin}]{%
		\includegraphics[width=0.32\linewidth]{images/cluster_mqtt_building_merlin.pdf}
	}
	\hfill
	\subfloat[Cluster 2 global model evaluated on validation-attack-scan-amplification dataset for the same cluster. (A)--(B) Nmap random port scan. (C) Nmap port 5683 scan. (D)--(E) Nmap top 1000 ports scan. (F) CoAP amplification attack.\label{fig:anomaly_flids_c2_nmapampli}]{%
		\includegraphics[width=0.32\linewidth]{images/cluster_coap_comcyc1_nmapyampli.pdf}
	}
	\hfill
	\subfloat[Cluster 4 global model evaluated on validation-attack-mirai-scan-load dataset for the same cluster. (A) testbed's Mirai bot is started. (B) This Cluster 4 device (IP camera museum) gets infected with Mirai. Packets labeled as 'Others' are other Mirai-infected IoT devices scanning the network.\label{fig:anomaly_flids_c4_miraiscanload}]{%
		\includegraphics[width=0.32\linewidth]{images/cluster_cam_museum1_miraiscanload.pdf}
	}
	
	\caption{Anomaly detection examples. Dotted line indicates the anomaly threshold, packets with MSE above the threshold are considered anomalous.}
	\label{fig:anomaly_flids}
\end{figure*}

\subsection{Baseline experimental comparison}

For the comparison with Kitsune, we use its publicly available Python implementation~\cite{KitsuneRepo}. We configure Kitsune to use the default parameters ($m = 10$ maximum size for any autoencoder in the ensemble layer). Kitsune does not use FL, so for each device on which we deploy it, we use its corresponding normal traffic data for training (the first 10\% to learn Kitsune's feature mapping and the remaining 90\% for the training of the autoencoder ensemble itself). Then, it is evaluated on the device's corresponding validation-attack datasets. In this comparison experiment, we are not primarily interested in the results of the anomaly detection metrics; however, we are interested in what kind of attacks or malicious behavior detection our proposed method differs from Kitsune.

Regarding the mirai-scan-load dataset, the measured metrics ranged from 0.9729 -- 0.9771 accuracy, 0.9861 -- 0.9883 F1 and 0.3518 -- 0.4525 MCC depending on the device type. Overall, most packets related to the Mirai scanning, brute-forcing and malware loading stages appeared above the anomaly threshold. However, most Mirai C\&C related traffic went undetected.

The results on the Mirai C\&C related traffic are best observed on the mirai-cnc-dos datasets, shown in \figurename~\ref{fig:flids_vs_kits_miraidos_kits}. The measured metrics are 0.9998 accuracy, 0.9999 F1 and 0.7397 MCC. Kitsune correctly detected as anomalous all the performed DoS attacks, but failed to detect the C\&C related traffic. During the period between the Mirai bot activation and the first attack, the device periodically communicates with the Mirai C\&C server. This traffic went undetected for Kitsune as its reconstruction error is close to the error for the normal traffic. In contrast, while our proposed method failed to detect some packets related to the DoS attacks, all the Mirai C\&C traffic is well separated from the normal activity as shown in \figurename~\ref{fig:flids_vs_kits_miraidos_flids}.

The masscan dataset also shows significant differences between Kitsune and the proposed clustered FL model. The measured metrics ranged from 0.7781-0.8122 accuracy, 0.8026-0.8865 F1 and 0.4665-0.5971 MCC. All the low volume scanning activity and a significant number of packets from the medium volume scanning activity were below Kitsune's threshold as shown in \figurename~\ref{fig:flids_vs_kits_masscan_kits}. However, our proposed method detected all activity irrespective of the scanning rate as shown in \figurename~\ref{fig:flids_vs_kits_masscan_flids}.

Unlike Mirai's C\&C behavior, Kitsune was able to detect the Merlin C\&C activity, which is noisier than Mirai's. Some packets related to the ICMP attack went undetected; however, all attacks included packets above the anomaly threshold: 0.9791 accuracy, 0.9891 F1 and 0.7392 MCC. Most anomalous packets from the scan-amplification data were also correctly classified: 0.9912 accuracy, 0.9955 F1 and 0.5988.


\begin{figure*}
	\centering
	
	\subfloat[Proposed clustered FL method.\label{fig:flids_vs_kits_miraidos_flids}]{%
		\includegraphics[width=0.49\linewidth]{images/cluster_mqtt_building_miraidos.pdf}
	}
	\hfill
	\subfloat[Kitsune.\label{fig:flids_vs_kits_miraidos_kits}]{%
		\includegraphics[width=0.49\linewidth]{images/kitsune_building1_miraidos.pdf}
	}
	
	\caption{Anomaly detection results for the validation-attack-mirai-cnc-dos dataset on one of the Building monitor devices. (A) Start Mirai bot on the device. (B)--(C) DoS attacks. (D) stop Mirai bot. Dotted line indicates the anomaly threshold.}
	\label{fig:flids_vs_kits_miraidos}
\end{figure*}


\begin{figure*}
	\centering
	
	\subfloat[Proposed clustered FL method.\label{fig:flids_vs_kits_masscan_flids}]{%
		\includegraphics[width=0.49\linewidth]{images/cluster_mqtt_building_masscan.pdf}
	}
	\hfill
	\subfloat[Kitsune.\label{fig:flids_vs_kits_masscan_kits}]{%
		\includegraphics[width=0.49\linewidth]{images/kitsune_building1_masscan_noarp.pdf}
	}
	
	\caption{Anomaly detection results for the validation-attack-masscan dataset on one of the Building monitor devices. (A) Masscan node performs low volume scan. (B) Masscan node performs medium volume scan. (C) Masscan node performs high volume scan. Dotted line indicates the anomaly threshold.}
	\label{fig:flids_vs_kits_masscan}
\end{figure*}

\section{Conclusions} \label{sec:conclusions}

In this work, we have proposed a clustered FL architecture that allows training anomaly and intrusion detection models in large networks of heterogeneous IoT devices. The proposed FL architecture does not need supervised data labeling, making it appropriate for real deployments where precise network traffic labeling is not feasible. To address the problems that arise with FL in heterogeneous environments, the proposed architecture includes an unsupervised device clustering algorithm that works by inspecting the parameters of the partially trained models. This clustering method is fully integrated into the FL training pipeline. It does not rely on any external fingerprinting tools or manual clustering methods, which can ease the implementation of FL-based architectures in deployment settings.

The architecture was implemented and evaluated on an emulated testbed comprised of multiple heterogeneous IoT and IIoT devices running real production libraries that generate traffic with a diverse set of network protocols. The proposed device clustering method showed successful grouping of the devices with similar communication patterns. Additionally, training using FL exhibited a faster model convergence rate compared to the isolated edge method, especially for the devices that generate low volumes of training data.

The global models were evaluated on real attacks showing low false positive rates and high detection for most of the attacks. While few DoS-based attacks were not correctly classified as anomalous for some of the device clusters, the proposed model successfully detected stealthier malicious actions such as the Mirai C\&C heartbeat packets and slow scanning activities. In contrast, comparison with the ML-based Kitsune network IDS showed that it correctly detected those DoS attacks but misclassified stealthier activity. This can indicate that for a more comprehensive detection, we could deploy alongside the clustered FL model a simpler model that, for instance, uses the frequency of packets over a time window to detect generic volumetric attacks.

Lastly, we note that the unsupervised model training assumes that the devices are operating in a normal condition (i.e., during the training phase, the devices are not compromised). This assumption might not hold for some adversarial settings. Future work can include exploring how compromised or adversarial devices in the network affect the unsupervised device clustering stage of the proposed method. Compromised devices might deviate from other normal devices that should belong to the same cluster. This drift might be indicative of anomalous behavior, and the device can be flagged or filtered out before the FL process starts.




\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,referencias,misreferencias}

\end{document}
