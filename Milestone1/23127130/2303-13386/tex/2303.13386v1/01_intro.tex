\section{Introduction}\label{sec:intro}

While pretrained language models demonstrate massive improvements on a wide range of \ac{NLP} tasks, it remains challenging to apply them to specialised domains \citep{ramponi-plank-2020-neural}.
To acquire domain-specific task knowledge, a conventional approach is to perform domain-specific pretraining --- usually \ac{MLM} on in-domain raw text --- followed by finetuning with in-domain task-annotated data \citep{lee2020biobert,gu2021domain,boecking2022making}.
However, this approach requires in-domain task labels that can be expensive to acquire. Another approach is to train a model with the usually abundant general-domain task labels and directly transfer to the new domain \citep{romanov2018lessons, ma-etal-2021-zero}, but the transfer performance is often limited by the domain gap. Past studies on zero-shot domain transfer or unsupervised domain adaptation have explored methods to transfer task knowledge from a source domain to an unseen target domain \citep{ramponi-plank-2020-neural,ganin2015unsupervised}, but they usually require external modules to perform feature or domain alignment and are not always easily applicable to pretrained language models. In particular, there is little understanding of how we can leverage and combine domain-specific knowledge and general-domain task knowledge in the context of the recent success of text-to-text architectures in transfer learning. 

To close this gap, we propose \model, a novel compositional zero-shot domain-transfer framework based on the \ac{SOTA} transfer learning model \ac{T5} \citep{raffel2019exploring}. Throughout, the `zero-shot' setup refers to zero-shot \emph{domain} transfer with no access to labelled \emph{in-domain} data.\footnote{The definition of `zero-shot' in this paper follows recent studies \citep{pan-etal-2022-task, zhao-etal-2022-domain}, and is similar to unsupervised domain adaptation, as discussed in  \Cref{sec:rw}. Another similar usage of ‘zero-shot' is found in cross-lingual setups where no task labels are accessible in the target test language but labels in the same task are available in a source language. Note that this definition is different from ‘zero-shot learning’ traditionally used to refer to the prediction of unseen classes.} By ``compositional'' we mean that \model{} is able to combine seen task labels and domain text to acquire an unseen combination of task domain knowledge.

As shown in \Cref{fig:front_page},
\model combines domain knowledge and task knowledge by making the best use of in-domain free text and general-domain task labels, which are typically accessible and abundant.
For example, in the context of \ac{NLI}, \model can learn domain-specific semantics (e.g., ``bony abnormalities'' is a synonym of ``osseous abnormalities'') from in-domain free text and transferable task knowledge from general-domain task labels (e.g., negation indicates contradiction) to infer domain-specific task knowledge (e.g., ``There are no bony abnormalities'' contradicts ``There are osseous abnormalities'').

We apply \model to \ac{NLI}, summarisation and text embedding learning, which are fundamental applications across many domains, and we explore zero-shot domain transfer to the high-value and highly-specialised domain of biomedicine and its extremely low-resource subdomain of radiology. Due to their specialisation, obtaining labelled data in these domains is expensive and time-consuming. For example, the radiology-specific \ac{NLI} dataset (RadNLI) \citep{miura-etal-2021-improving} contains only 960 manually-labelled examples as development and test data and no training data is available.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{imgs/compositionality.pdf}
    \caption{By combining task knowledge from general domain data and domain knowledge from in-domain unlabelled text, our text-to-text model \model learns to solve in-domain tasks.
    }
    \label{fig:front_page}
\end{figure}

The key to \model's compositional transfer is {\it continual multi-task pretraining to simultaneously acquire domain and task knowledge}: we jointly train \ac{T5} with \ac{MLM} on in-domain unlabelled data and general-domain tasks (\ac{NLI} and summarisation).
To better acquire the transferable task knowledge from the general-domain task labels, we propose a multi-task setup we call NLGU.
As depicted in \Cref{fig:nli}, NLGU gives each task two formulations:  \ac{NLG} (label-to-data generation), and \ac{NLU} (data-to-label prediction).
\ac{NLU} enables label prediction when tested in an unseen domain and forces model sensitivity to the conditioned label, assisting \ac{NLG}.
Meanwhile, \ac{NLG} enables downstream tasks such as summarisation or data augmentation.
This enables \model to generate its own \ac{NLI} in-domain task data for further finetuning (a process we call self-finetuning), or to generate positive and negative examples for improving text embeddings by contrastive learning \citep{oord2018representation}.

Our experiments show the effectiveness of \model in zero-shot domain transfer, and our proposed multi-task compositional approach achieves large gains compared with sequential training with T5 across all tasks. 
In particular, we achieve \ac{SOTA} zero-shot domain transfer performance on RadNLI~\cite{romanov2018lessons}, outperforming baselines including \acp{LLM}, sequential training approaches and task-specific baselines by large margins. 
We also identify several key insights through extensive analysis:
1) All three key components (in-domain MLM, NLGU, self-finetuning) in \model are important for transfer success while multi-task learning with in-domain MLM is the key for combining domain and task knowledge.
2) Scaling up model size significantly improves transfer performance.
3) \model is able to solve challenging domain-specific task examples, indicating it acquires domain-specific task knowledge through compositional transfer.
 
To summarise, we present the following major contributions:
1) We propose \model, a general framework for compositional transfer learning with text-to-text models, and show multi-task training is superior to sequential training in the models' domain transfer. 
2) With a novel NLGU training strategy combining generation and understanding, \model can be used for both classification and generation tasks.\footnote{Notice that the tasks are limited to those that can have pairwise input instead of single sentence input.}
With the latter, \model can perform self-finetuning to further improve transfer performance.
3) We show the effectiveness of \model in zero-shot domain transfer, achieving \ac{SOTA} zero-shot performance in radiology \ac{NLI}.
4) Comprehensive analysis demonstrates the inner workings of \model's compositional transfer. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{imgs/continual_pretraining.pdf}
    \caption{
        Continual pretraining of \model on general-domain tasks (warm colors) and in-domain unlabelled text (blue). For task training, we form both \ac{NLG} and \ac{NLU} variants of \ac{NLI} and summarisation. All training is performed simultaneously, exploiting the unified text-to-text framework of \ac{T5}.
    }
    \label{fig:nli}
\end{figure*}


\begin{table*}[t]
\small
\centering
\scalebox{0.83}{
    \centering
    \begin{tabular}{l l l c}
    \toprule
        & Setting & Prompt (Input) & Output \\
        \midrule
        \multirow{2}{*}{\rotatebox{90}{\ac{NLI}}} & \ac{NLG}: $(x_1, y) \rightarrow x_2$ &   \textbf{Generate a} \{\texttt{label}\} \textbf{sentence of:} \{\texttt{premise}\} & \{\texttt{hypothesis}\} \\
        & \ac{NLU}: $(x_1, x_2)\rightarrow y$ &
        \{\texttt{premise}\} \textbf{Question:} \{\texttt{hypothesis}\} \textbf{True, False or Neither?} & \{\texttt{True | False | Neither}\} \\
        \midrule
        \multirow{2}{*}{\rotatebox{90}{Sum.}} & \ac{NLG}: $(x_1, y) \rightarrow x_2$ &  \textbf{Generate a} \{\texttt{label}\} \textbf{summary of:} \{\texttt{document}\} & \{\texttt{summary}\} \\
        & \ac{NLU}: $(x_1, x_2) \rightarrow y$  &     \{\texttt{document}\} \textbf{Question:} \{\texttt{summary}\} \textbf{True or False?} & \{\texttt{True  | False}\}\\
        \bottomrule
        
    \end{tabular}
    }
    \caption{
        Prompts used for task-specific training with NLGU for both \ac{NLI} and summarisation (Sum).
        For \ac{NLI} $x_1$: premise, $x_2$:  hypothesis, and the label ($y$) is one of \{entailed, neutral, contradictory\}.
        For summarisation $x_1$: document, $x_2$: summary, and the label ($y$) is one of \{entailed, contradictory\}.
    }
    \label{tab:prompts_and_outputs}
\end{table*}