

\begin{abstract}

% New abstract (shortened to address TACL reviewer D comment)
% old abstract available in hist/

Label scarcity is a bottleneck for improving task performance in specialised domains.
We propose a novel compositional transfer learning framework (\model%
\footnote{\model (read as ``dot five''): \textbf{D}omain Compositional Zer\textbf{O}-shot \textbf{T5}.}) for zero-shot domain transfer. Without access to in-domain labels, \model jointly learns domain knowledge (from \acl{MLM} of unlabelled in-domain free text) and  task knowledge (from task training on more readily available general-domain data) in a multi-task manner.
To improve the transferability of task training, we design a strategy named NLGU: we simultaneously train \ac{NLG} for in-domain label-to-data generation which enables data augmentation for self-finetuning and \ac{NLU} for label prediction.
We evaluate \model on the biomedical domain and the resource-lean subdomain of radiology, focusing on \acl{NLI}, text summarisation and embedding learning.
\model demonstrates the effectiveness of compositional transfer learning through multi-task learning. In particular, \model outperforms the current \acl{SOTA} in zero-shot transfer by over 7 absolute points in accuracy on RadNLI.
We validate \model with ablations and a case study demonstrating its ability to solve challenging NLI examples requiring in-domain expertise.
\end{abstract}


\acresetall