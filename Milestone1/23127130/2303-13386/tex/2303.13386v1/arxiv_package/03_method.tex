\section{Method}
\label{sec:method}

To achieve compositional transfer, \model acquires domain knowledge and task knowledge via continual pretraining (see \Cref{fig:nli}).
Specifically, we optimise a joint loss function composed of an in-domain masked language model loss (``domain-\ac{MLM}'') and a general-domain task-specific loss:

\begin{equation}
    \mathcal{L}_{\text{joint}} = \lambda  \mathcal{L}_{\text{domain-MLM}} + (1 - \lambda) \mathcal{L}_{\text{task}}
    \label{eq:joint}
\end{equation}
We set $\lambda = 0.5$ but explore tuning it in \Cref{sec:setup}.

We use \ac{T5}, an encoder-decoder generative language modelling framework \citep{raffel2019exploring}, to learn a conditional sequence generator $P(\text{output}|\text{input})$.
\ac{T5} is chosen for two reasons:
1) It is a strong transfer learning model, and
2) it can unify classification and generation, which has potential to further boost transfer performance (see NLGU discussion in \Cref{sec:gen_dom_task_pretr}). 
We use the same pretraining objective (cross-entropy with teacher-forcing) as in \ac{T5}.

We detail the two loss components for continual pretraining in \cref{sec:in_dom_unsup_pretr,sec:gen_dom_task_pretr}.
Once the model has been continually pretrained, it can be used to perform zero-shot domain transfer on a task.
Task-specific designs for inference are given in \cref{sec:task_specific_inference}.

\subsection{Continual Pretraining with In-domain \ac{MLM}}
\label{sec:in_dom_unsup_pretr}

For $\mathcal{L}_{\text{domain-MLM}}$ we use the \ac{MLM} loss \citep{devlin2019bert} to continually pretrain a \ac{T5} on in-domain free text:
Given a piece of sampled radiology or biomedical text, we randomly mask 15\% of its tokens and ask the model to denoise the masked input sequence, i.e., generate the masked tokens.

\subsection{Continual Pretraining on General-domain Tasks}
\label{sec:gen_dom_task_pretr}
For $\mathcal{L}_{\text{task}}$, we define ($x_1$, $x_2$) as a text pair that denotes (\textit{premise}, \textit{hypothesis}) for \ac{NLI}, and (\textit{document}, \textit{summary}) for summarisation.
The standard \ac{NLI} task assigns labels from $y$: \{entailment, neutral, contradiction\}, and the task is $(x_1, x_2) \rightarrow y$.
For summarisation, the task is usually cast as $x_1 \rightarrow x_2$.
We follow \citet{sanh2022multitask} to adopt a multi-task learning strategy to train summarisation and \ac{NLI} simultaneously.
Hence, the basic setup of task learning would be: \ac{NLI} as a discriminative \ac{NLU} task plus summarisation as an \ac{NLG} task. 

\paragraph{NLGU: Simultaneous \ac{NLG} and \ac{NLU}}
One immediate question is whether we can turn each task into both \ac{NLG} and \ac{NLU} (i.e., adding \ac{NLG} for \ac{NLI} and \ac{NLU} for summarisation).
For \ac{NLI}, we can add label-to-data \ac{NLG} to generate pseudo in-domain text for data augmentation, performing $(x_1,y)\rightarrow x_2$ (the label $y$ is used as control code).
For summarisation, we can also follow \ac{NLI} to add a \ac{NLU} task that predicts whether a document-summary pair is entailed (the correct match) or contradictory (a counterfactual summary) (\Cref{sec:setup}).
This \ac{NLU} component aims to improve the factuality of generated text as it encourages the model to distinguish counterfactuals and true summaries. With the hypothesis that performing \ac{NLG} and \ac{NLU} simultaneously will mutually benefit each other, we propose NLGU, meaning joint training of \ac{NLG} and \ac{NLU}.
With NLGU, we unify both summarisation and \ac{NLI} into $(x_1, x_2)\rightarrow y$ for \ac{NLU} and $(x_1,y)\rightarrow x_2$ for \ac{NLG}.
The conditional generator then simultaneously optimises two losses:
\begin{equation}
     \mathcal{L}_{\text{task}} = \gamma \mathcal{L}_{(x_1, x_2)\rightarrow y} + \mathcal{L}_{(x_1,y)\rightarrow x_2}  
\end{equation}
We set $\gamma=10$ to balance the two losses since $x_2$ is usually much longer than $y$ (the classification label).
\ac{NLU} and \ac{NLG} are both trained with sequence-to-sequence generation, and differ only in the input prompt and the expected output (\Cref{tab:prompts_and_outputs}).
The prompt for $\mathcal{L}_{(x_1, x_2)\rightarrow y}$ is from \citet{brown2020language}.
The prompts for summarisation are akin to those for \ac{NLI}, with \texttt{premise} and \texttt{hypothesis} replaced with \texttt{document} and \texttt{summary} respectively, and we only use \{entailment, contradiction\} relations.

\subsection{Task-specific Designs for In-domain Zero-shot Inference}
\label{sec:task_specific_inference}

After continual pretraining, we zero-shot-transfer the trained model to three applications in specialised domains without requiring labels from these domains:
1) \ac{NLI},
2) summarisation, and
3) text embedding learning.

\paragraph{\ac{NLI} (with self-finetuning)}
While the model is capable of directly performing \ac{NLI} after training on general-domain \ac{NLI} task labels with $(x_1, x_2)\rightarrow y$,  we propose an additional step, self-finetuning, to boost transfer performance (\Cref{sec:ablation}).
We first use the model's \ac{NLG} capabilities to generate pseudo in-domain \ac{NLI} data:
We sample a set of sentences from the target domain as premises, and prompt the pretrained model to generate hypotheses (the \ac{NLG} task) with each of the three control codes (labels).
This pseudo-in-domain \ac{NLI} dataset is then used as additional training data to finetune the same model to perform the NLU task: $(x_1,x_2) \rightarrow y$.
The resulting finetuned model is then used for zero-shot \ac{NLI} transfer.

\paragraph{Text summarisation}
We directly prompt the model after continual pretraining to summarise in-domain documents.
We use the same prompt as pretraining:
``Generate an entailed summary of: \{\texttt{document}\}''.
The output summary is then compared against the gold summary.
Since this is already a task of text generation, i.e., $(x_1, y) \rightarrow x_2$, we cannot exploit self-finetuning as for \ac{NLI} since we cannot improve generation from training on the model's own generated pseudo data. 

\paragraph{Text embedding learning}

\model can be directly used as a generator for data augmentation. Apart from creating more pseudo NLI task data to improve NLI, \model can improve domain-specific embedding learning in general. To do so, we sample a set of in-domain sentences as anchors, and prompt the trained model to generate entailed and contradictory sentences to form positive and negative pairs for each anchor.
With beam search size of five, we sample the top-$k$ most probable sequences as the entailed (positives) and contradictory (negatives) sentences of the anchor%
\footnote{We experimented generating one, three, and five pairs of positives and negatives and found three is the best in our setup. We thus use three across all models.
}. Given the collected anchors and positive/negative sentences, we finetune a \ac{SOTA} sentence embedding model with a contrastive loss.
Specifically, we continually finetune the all-mpnet-base-v2\footnote{\scriptsize{\url{https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-\\with-1b-training-pairs/7354}}. \\ \scriptsize\url{https://huggingface.co/sentence-transformers/all-mpnet-base-v2}
} model with a variant of InfoNCE~\citep{oord2018representation} modified to handle multiple positives~\citep{miech2020end}. The learned embedding space is then used for query-document retrieval or for computing text similarity.
