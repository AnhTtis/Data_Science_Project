\begin{table}
    \small
    \centering
    \scalebox{0.83}{
    \begin{tabular}{lcc}
        \toprule
        Setting & RadNLI (acc.) & Sum. (NEM)  \\
        \midrule
        \modellarge\  (full model) &  \textbf{82.1} & \textbf{.082} \\
        \midrule
        (1) no in-domain MLM & 63.5 & .015 \\
        (2) no NLGU \& (3) & 59.0 & .052 \\
        (3) no self-finetuning & 49.6  & - \\
        \bottomrule
    \end{tabular}
    }
    \caption{
        Ablation study on \model components, evaluated on radiology. Removing \ac{MLM} removes in-domain text during pretraining. Removing NLGU reduces NLI to purely discriminative (thus also disabling self-finetuning) and summarisation to purely generative tasks. Self-finetuning is only used for NLI tasks. 
         Sum. = summarisation, NEM = named-entity matching metric. Note that the component is removed one by one but not incrementally.
    }
    \label{table:ablation_main}
\end{table}