\section{Experiment}
\label{sec:exp}

We introduce our experimental setup in \Cref{sec:setup}, briefly discuss baseline approaches in \Cref{sec:baselines} and then present results in \Cref{sec:main_results}.

\subsection{Experimental Setup}
\label{sec:setup}

Details of the datasets used for training and evaluation are given in \Cref{tab:datasets}.

\paragraph{Pretraining datasets}

\input{table_datasets}

As our continual pretraining is a multi-task process, we balance the in-domain and general-domain datasets in each batch via up/downsampling as needed:
For radiology, we upsample MIMIC-CXR samples via duplication, whereas for biomedicine we downsample PubMed abstracts, in each case matching the general-domain task dataset size.
We also balance the number of samples coming from each task, downsampling the summarisation dataset to roughly match that of NLI (`\# Examples' in \Cref{tab:datasets}). Experiments with \model$_\text{small}$ (\Cref{fig:joint_loss_weighting}) indicate that downstream task performance could be boosted by tuning the relative prevalence of the data sources, with a task-dependent optimal value. In this proof-of-concept study, we fix a ratio of 1:1.

\begin{figure}
    \centering
    % \includegraphics[width=0.8\linewidth]{imgs/mean_attention_weights.pdf}
    % \includegraphics[width=0.8\linewidth]{}
    \includegraphics[width=1\linewidth]{imgs/weighting_experiment.pdf}
    \caption{Varying the prevalence of in-domain \ac{MLM} and task data in \model$_\text{small}$ training.}
    \label{fig:joint_loss_weighting}
\end{figure}

We generate counterfactual summaries of Gigaword based on \citet{rajagopal2022counterfactual}. Specifically, we run a named entity recognition model on the documents from the Gigaword summarisation training data, specifically the ``\texttt{en\_core\_web\_sm}'' trained \spacy pipeline~\cite{Honnibal_spaCy_Industrial-strength_Natural_2020}. For each document that contains a named entity, we randomly sample an entity and replace it with a different named entity of the same category from the training corpus. This is our `counterfactual' example\footnote{Note that a counterfactual is not always a contradiction. We approximate contradiction this way and use the `contradictory' control code in our experiments for consistency.}. We also filter out noisy data when the generated counterfactual contains UNK or \#. The resulting dataset, as listed in \Cref{tab:datasets}, consists of 50\% document-`wrong summary' pairs (i.e.\ 500k pairs), one for each true document-summary pair. 
To create pseudo NLI data for the self-finetuning process, we use all premises from the RadNLI/MedNLI development set and generate one entailed, one neutral, and one contradictory hypothesis for each premise. In total, we have 1440 and 4185 pseudo examples for RadNLI and MedNLI respectively. 

\paragraph{Evaluation datasets and metrics}
All the evaluation datasets are from domain-specific tasks (\Cref{tab:datasets}).
For NLI, we report accuracy and macro-$F_1$ (out of 100, for legibility) on the test set of RadNLI and MedNLI.
For summarisation in radiology, we evaluate on findings-to-impression%
\footnote{In a radiology report, the "findings" section is a detailed description and the "impression" section is a summary of the findings with follow-up recommendation.
} summarisation on the test split of the Open-I dataset \cite{demner2016preparing}.
For biomedical summarisation, we create an abstract-to-title summarisation dataset, `PubMed ShortSum'.
The data for this task is sampled from PubMed and filtered to abstracts shorter than 1000 characters.
Compared with the traditional article-to-abstract PubMed summarisation task which evaluates long summary generation for long text \cite{cohan-etal-2018-discourse}, PubMed ShortSum evaluates extreme summarisation for short text and is a more comparable task to our general domain Gigaword summarisation.   
For summarisation evaluation, we use standard lexical metrics (BLEU-4, ROUGE-L) and domain-specific factuality metrics: \ac{NEM} for both radiology \cite{miura-etal-2021-improving} and biomedical \cite{alambo2022entity} summarisation, and CheXbert \citep{smit-etal-2020-combining}%
\footnote{The average of the weighted-$F_1$ score across 14 pathological observations labelled by CheXbert.
} for radiology. 

We evaluate embeddings trained for the biomedical domain on MedSTS \citep{yanshan2020medsts}, a clinical text similarity benchmark.
Since the radiology domain has no text similarity datasets available, we design an impression-to-findings retrieval task on the Open-I test set, and report Accuracy@1/5/10.
This retrieval task can also evaluate embedding quality as it requires the model to differentiate text from same/different reports by encoding texts from matching findings-impression pairs (from the same report) with similar representations. 

\paragraph{Training details}
%
Models are trained for 10 epochs with validation loss used for checkpoint selection. We use distributed data parallelism on eight GPUs with the largest batch size permissible given computational constraints, resulting in batch sizes of 1024, 512, and 128 for small, base, and large models. With a dataset of  $\sim$8M samples, we thus train the large model for $\sim$64,000 steps per epoch.
We use AdaFactor \citep{shazeer2018adafactor} with learning rates of $10^{-3}$ for MIMIC-CXR and $2\times10^{-5}$ for PubMed pretraining.

\subsection{Baselines}\label{sec:baselines}
We have three categories of baselines: 
(1) task-specific zero-shot baseline models reported from the literature (where applicable);
(2) \acp{LLM} including T0 and GPT-3;
(3) sequential training first on in-domain unlabelled data and then on general-domain task labels. 
All the baseline models in our study must satisfy one constraint: not using any in-domain labels for the task, but they may differ in the required training resources (detailed comparison is found in \Cref{tab:baseline_comparison}). 
We compare with (2) as \acp{LLM} are known to be excellent zero-shot and few-shot learners for an unseen task, and should serve as a reasonable baseline for domain transfer.
We provide (3) as a straightforward baseline to \emph{sequentially} combine in-domain MLM training and general-domain task training as opposed to our proposed multi-task training. 

\input{table_baseline_comparison}
\paragraph{Task-specific zero-shot baselines}
We compare with the strongest task-specific zero-shot models from the literature.
For the \ac{NLI} task, we compare with \citet{miura-etal-2021-improving} and \citet{boecking2022making}, which both finetune a BERT model with MedNLI training data and then test on RadNLI.
\citet{boecking2022making} performs better as they use radiology-specific BERT model.
Note that MedNLI is a \textit{nearby-domain} corpus rather than general-domain task data, and in fact there has not been successful attempts in the literature to transfer general-domain \ac{NLI} to RadNLI. Note that in the later sequential training section we will establish such baselines from finetuning CXR-BERT on general-domain \ac{NLI}. 
For MedNLI, we compare with the best transfer learning results so far, ESIM (MultiNLI) which was trained on MultiNLI datasets \citep{romanov2018lessons}.
For radiology summarisation, to our knowledge, we are the first to report results on direct transfer from general-domain summarisation.
For biomedical summarisation, since we use a new dataset (PubMed ShortSum), there is no prior comparison.

\paragraph{\Aclp{LLM}}
T0~\citep{sanh2022multitask} and {GPT-3}~\citep{brown2020language} are massively pretrained language models that can be used off-the-shelf for zero-shot or few-shot inference.
T0 is pretrained with multiple tasks including general-domain summarisation datasets (but \emph{not} \ac{NLI}), and shows strong transfer ability \citep{sanh2022multitask}. T0 can be seen as a strong general-domain summarisation model and also strong zero-shot domain transfer baseline on summarisation. T0 is also particularly effective in transferring to unseen tasks. Therefore, we include T0 as a zero-shot baseline for NLI even though it has not been trained with any NLI data.
We test T0 (3B) and the most powerful T0++ (11B) model.
GPT-3 \citep{brown2020language} (\texttt{davinci}) is a massive language model with 175B parameters, pretrained on raw text with an autoregressive language modelling objective.

In the general domain, both models are shown to have performed reasonably well on \ac{NLI} and summarisation with prompting.
We test their zero-shot-inference capabilities in our experiments, following the original papers for prompt design. For the \ac{NLI} task, both T0 models and GPT-3 use the ANLI prompt template described in \citet{brown2020language}: "<premise> Question: <hypothesis> True, False or Neither?". For the summarisation task, T0 used the prompt: "<document> $\backslash$n === $\backslash$n Generate a title for this article:". For GPT-3 summarisation, we used the prompt ("<document>$ \backslash$n$ \backslash$n Tl;dr:") as recommended in the OpenAI GPT-3 playground example\footnote{\scriptsize\url{https://beta.openai.com/examples/default-tldr-summary}}.
Since GPT-3 benefits when few-shot examples are incorporated in the prompt, we create two additional baselines (GPT-3-NLI and GPT-3-GW%
\footnote{These are still zero-shot baselines as they do not use in-domain task examples.
}) that perform in-context learning of the task from general-domain \ac{NLI} training data (30 examples, randomly selected) and Gigaword summarisation training data (20 examples, randomly selected) respectively (\Cref{tab:datasets}).

\paragraph{Sequential training \label{par: sequential training}}
The most straightforward way to exploit both in-domain unlabelled data and task labels is to first train on in-domain MLM and then further finetune on general-domain task labels\footnote{This baseline category is similar to contemporaneous work \cite{pan-etal-2022-task} where domain-task transfer is achieved through sequential in-domain \emph{off-task} training followed by general-domain \emph{in-task} training. Here we do not use in-domain task data of any kind.}. We provide two variants of this baseline. The first type performs continual training with general-domain task labels from \ac{SOTA} domain-specific pretrained models. We adopt SciFive~\cite{phan2021scifive}, a T5 model pretrained on large biomedical corpora, CXR-BERT-General~\cite{boecking2022making}, a radiology-specialised BERT model, and the PubMed-specific PubMedBERT~\cite{gu2021domain}. For finetuning these models we use the same general-domain task data as provided to \model, where for the BERT models we only do finetuning on NLI. This results in baseline models SciFive$_{\text{large}}$-NLI, SciFive$_{\text{large}}$-GW (summarisation), CXR-BERT-NLI, and PubMedBERT-NLI. We further improve SciFive$_{\text{large}}$-NLI by including our proposed self-finetuning stage (SciFive$_{\text{large}}$-NLI + SFT). Since there is no radiology-pretrained T5 model, we compare with SciFive on both domains.

The second baseline type strictly compares multi-task training (\model) and sequential training. Here, we first pretrain T5 with in-domain MLM, and then continually pretrain on the general-domain task data, ensuring other factors remain the same including the training duration, use of NLGU, and use of self-finetuning where appropriate. We call this setting \modelsequential.

\subsection{Main Results}
\label{sec:main_results}


\paragraph{\ac{NLI} (\Cref{table:zeroshot_nli})}

\modellarge\ establishes new \ac{SOTA} for zero-shot domain transfer on RadNLI and competitive results on MedNLI (\Cref{table:zeroshot_nli}).
On RadNLI, \modellarge\ reaches an impressive 82.1\% on accuracy and is the best performing model. It outperforms the strongest reported number from the literature (CXR-BERT) by more than 15\%, and our  baseline CXR-BERT-NLI by almost 7\%. Comparing \model to \modelsequential on RadNLI reveals the benefit of \emph{multitask} training for compositional transfer.

On MedNLI, \modellarge\ outperforms ESIM (MultiNLI) by almost 20\% (accuracy), but does not quite reach the 75.7\% accuracy achieved by PubMedBERT-NLI, which establishes a new \ac{SOTA} in zero-shot domain transfer on MedNLI --- supervised \ac{SOTA} is 86.6\% \citep{phan2021scifive}. Although factors such as tokenisation and pretraining strategies may contribute, we speculate that the domain gap between MedNLI and our biomedical pretraining corpus explains the weaker performance of \model on MedNLI. MedNLI was sourced from \emph{clinical} notes in MIMIC-III, which differ distributionally from biomedical articles in PubMed. Supporting this hypothesis, we observed that \model pretrained on radiology text, and the \emph{sequential} baseline \modelsequential achieved similar performance on MedNLI (70\% accuracy), indicating that results on MedNLI may not fully reflect compositional domain knowledge transfer in our setup. In this case, a strong NLI-specific model is most performant, while lacking potentially-advantageous versatile text generation/summarisation capabilities.

\input{table_nli}


\paragraph{Summarisation (\Cref{table:summarisation})} 

\modellarge\ achieves competitive performance compared with the best model in radiology (GPT-3-GW) and biomedical domains (T0 models) (\Cref{table:summarisation}).
In radiology, \modellarge\ is the second-best model. That the strongest performing models on summarisation are \acp{LLM} with substantially many parameters is not surprising; we observe in \Cref{sec:scale_up} that \model too enjoys scaling effects.
Most importantly, we again demonstrate the benefit from multi-task compositional transfer as \modellarge\ significantly outperforms both \modelsequential and SciFive-GW across all metrics in both domains. This further verifies that a na\"{i}ve sequential training on these two sources does not lead to effective compositional knowledge transfer.  
We also acknowledge it is more difficult to perform domain transfer for generation tasks in general: we cannot perform the data augmentation NLG and self-finetuning pipeline as it amounts to training the model to generate its own outputs.

\input{table_summarisation}

\paragraph{Text embedding learning (\Cref{table:text_embeddings})}

The \model-generated examples greatly improve the SOTA sentence embedding model's capability on both impression-to-findings retrieval in radiology and semantic textual similarity  (MedSTS) in the biomedicine domain (\Cref{table:text_embeddings}).
This is evidence that \model-generated sentences are of high quality and have captured semantic similarity and contradiction required for learning a good embedding model.  
We also compare with an ablated version of \model without in-domain MLM to generate data and find that the full model performs better across the board.
This shows the importance of domain training for generating good in-domain examples.
We explore this further in \Cref{sec:ablation}. 

\input{table_embeddings}

\section{Further Analysis}
\label{sec:analysis}

In this section, we demonstrate the importance of individual components of \model (\Cref{sec:ablation}) and explore the role of model size (\Cref{sec:scale_up}). Finally, we provide fine-grained analysis on RadNLI to verify whether \model has indeed acquired domain-specific task knowledge from compositional transfer (\Cref{sec:other_analysis}). 

\subsection{Ablation Study}\label{sec:ablation}
\input{table_ablation}

Through ablations, we probe the contributions of key components of \model:
1) In-domain MLM,
2) NLGU (combining \ac{NLU} and \ac{NLG}) (\Cref{sec:in_dom_unsup_pretr}), and
3) self-finetuning for zero-shot \ac{NLI} (\Cref{sec:task_specific_inference}).
We conduct these ablations on the radiology domain on \modellarge.
The results are shown in \Cref{table:ablation_main}.

We observe that all components are essential to the success of the model.
In-domain \ac{MLM} is especially important for summarisation, without which the model fails in zero-shot transfer as it often just extracts a random subsequence of the document.
Removing NLGU harms both NLI and summarisation.
Training without NLGU removes the NLG component from NLI and therefore disables self-finetuning.
Self-finetuning is the most important component for boosting NLI performance, without which the model's accuracy drops more than 30\%. As shown in \Cref{table:zeroshot_nli}, SciFive also benefits from self-finetuning in this way.
This indicates that the pseudo in-domain NLI task data generated by NLGU is crucial.
Training without NLGU also removes the NLU task for summarisation and brings down the performance, indicating that having an NLU task can also benefit generation. 
 
We hypothesise that NLU improves NLG by forcing the model to be more sensitive to the control code in the prompt, leading to improved pseudo-data generation and better summarisation.
To test this, following \citet{tang-etal-2018-analysis}, we compute the maximum attention weights across all attention heads to the control codes in the prompt when generating an NLI hypothesis (\Cref{fig:attention_nlgu}).
We compare \modellarge\ trained with or without NLU.
We see that the full model attends more on the control codes, suggesting that NLU is increasing label conditionality during generation.
\Cref{tab:nli_generated} shows some examples:
When required to generate an entailment, the model can usually correctly paraphrase the original sentence;
for negation, the model is usually able to correctly identify modifiers to flip the logic (e.g., change ``increase'' to ``decrease'' and adding or removing ``no'');
for neutral, the model generates a thematically related sentence but not directly negating or agreeing with the original sentence. 

\input{table_nlg} 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{imgs/max_attention_weights.pdf}
    \caption{
        Maximum attention weights assigned to control code \{\texttt{label}\} (``entailed'', ``neutral'', ``contradictory'') in the prompt in NLI hypothesis generation, averaged over $100$ randomly sampled examples from the RadNLI dev set.
        Error bars represent standard deviation.
    }
    \label{fig:attention_nlgu}
\end{figure}

\subsection{Effect of Scaling Up}
\label{sec:scale_up}

We have so far reported results on a large T5 model (770M parameters).
In \Cref{fig:scale_up}, we plot the performance of small (70M) and base (220M) \model models with their ablated versions for RadNLI and radiology summarisation, showing a clear trend of increasing performance as the model size grows.
Interestingly, this scaling effect disappears when we remove in-domain MLM, revealing the importance of domain training for larger models, especially for summarisation.
This is possibly because, without domain training, scaling up the model leads to overfitting to the general-domain task data.
The compositional transfer framework from \model however regularises the model for more complex knowledge acquisition, and thus is able to harness the power from larger models.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs/ablation_size_v3.pdf}
    \caption{
        Scaling-up effect on RadNLI (left) and Open-I summarisation (right).
        Both the full model and its ablated versions are compared.
        Note that self-finetuning is only applicable for the \ac{NLI} tasks.
    }
    \label{fig:scale_up}
\end{figure}

\subsection{Evidence of Compositional Transfer in \model: A Case Study on RadNLI}
\label{sec:other_analysis}
Although RadNLI is a radiology-specific \ac{NLI} dataset, we observe that some examples may be solvable using general-domain task knowledge (e.g., syntactic cues) alone.
A general-purpose \ac{NLI} model will likely detect that `There is no pneumothorax' contradicts `There is a pneumothorax' without requiring radiology-specific knowledge such as an understanding of pneumothorax.
Therefore, higher performance on RadNLI may not strictly guarantee the model has acquired in-domain knowledge.
To quantify how much of \model's transfer success is due to the acquisition of the previously unseen domain-specific task knowledge versus from direct application of the general-domain task knowledge, we manually annotated each of the 480 sentence pairs in the RadNLI test set by whether it could be solved without particular medical expertise%
\footnote{We determined 228 (47\%) pairs could be solved without medical/radiological expertise, 177 (37\%) could not, and the remaining 75 (16\%) were ambiguous. Ambiguous cases were excluded from the analysis.
}.
Examples are shown in \Cref{table:radnli_examples}.


\input{table_radnli_examples}

\input{table_domain_specific_nli}

\Cref{tab:radnli_analysis} compares three models on these subsets: \modellarge (a), \modelsequential (b), and \modellarge\ \emph{without} in-domain MLM (c) (equivalent to `T5$_\text{large}\rightarrow$Task'). We further test with and without self-finetuning to probe its capacity to strengthen domain-specific competence.

While \modellarge\ achieves the best performance overall, it is specifically on challenging domain-specific cases that it outperforms \modelsequential, an increase of 15 points in $F_1$.
For example, in \Cref{table:radnli_examples}, only \modellarge\ is able to solve the second example which requires radiology-specific knowledge (the model should know cardiac silhouette includes heart size; and if the heart is top normal, then it should not be enlarged).
This demonstrates the role of compositional transfer for inferring the otherwise unseen in-domain task knowledge (in this case, radiology \ac{NLI} knowledge) solving challenging cases that require expertise.

The two ablated versions help understand where this domain-specific task knowledge is acquired.
In-domain \ac{MLM} training is key as removing it (c) significantly decreases the performance on domain-expert cases in particular, producing a model which cannot benefit from self-finetuning at all for such cases. This is because without in-domain MLM, the model is not able to generate good-quality pseudo in-domain labels in the first place, and therefore self-finetuning has little effect on the expert cases. Introducing in-domain data sequentially (b) resolves the performance gap on non-expert cases, but still underperforms on domain-specific cases relative to multi-task training (a).
We conclude that the compositional fusion of task and domain knowledge happens during \model's multi-task pretraining phase with in-domain MLM as the key element, and that domain-specific competence is elicited through self-finetuning. 