\section{Conclusion and Discussion}
\label{sec:conc}

We propose \model, a compositional transfer learning framework to solve domain-specific NLP tasks without requiring in-domain task labels.
We show the effectiveness of \model on zero-shot transfer to multiple tasks in the biomedicine and radiology domains.
\model significantly outperforms T5 sequential training across all tasks, and achieves zero-shot \ac{SOTA} in radiology NLI with massive gains. 
We also conduct extensive analyses to identify the contribution from each model component and the benefits from scaling up the model size, and demonstrate direct evidence of domain-specific task knowledge learned from \model's compositional transfer.

Limitations of this work include the challenge of drawing clear boundaries between domains and the necessarily incomplete exploration of hyperparameters and configurations. For example, general domain texts may contain biomedical or radiology sources, and our `biomedical' \ac{NLI} evaluation set leans strongly clinical, introducing a degree of domain shift. Investigation of the weighting of terms in the loss reveals the potential to improve performance through more exhaustive hyperparameter search - we emphasise that this was a proof-of-concept study and although \model performs favourably, zero-shot domain transfer could be further pushed, especially if only a single downstream task is required.

The proposed NLGU method and subsequent self-finetuning was critical for improving downstream task performance. However, we observed an intermittent negative effect wherein the model would attempt to solve the \ac{NLU} task when presented with an unusually long prompt. Further work can be done to refine this approach. For example, the benefit of NLGU in resource-rich domains is unclear. As our focus is on domain transfer and we do not evaluate on general-domain tasks, we leave such experimentation to future study.

Finally, we acknowledge that it is non-trivial to apply our full framework to single-sentence/paragraph classification tasks. While our most basic setup (compositional training of in-domain MLM and vanilla task training) can still be transferable to any task format, NLGU and self-finetuning would currently only work for tasks that involve pairs of texts.
Nonetheless, we believe \model proves to be a highly effective zero-shot domain transfer framework which will be beneficial to domain-specific applications beyond radiology and biomedicine.

\section*{Acknowledgments}
The authors would like to thank the anonymous TACL reviewers and editors for their detailed feedback and helpful suggestions.