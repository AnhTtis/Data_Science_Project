
\begin{table}
\small
\setlength{\tabcolsep}{4pt}
\centering
\scalebox{0.9}{
    \begin{tabular}{llll@{}}
    \toprule
    \textbf{Baselines} & \textbf{In-domain} & \textbf{General domain} &  \\
     & \textbf{Text} & \textbf{\ac{NLI}/Summ.}  \\
    \midrule
    \rowcolor{cambridgeblue}
     BERT {\scriptsize -- \citeauthor{miura-etal-2021-improving}}  & \cmark & $\sim$ / $-$  \\
    \rowcolor{cambridgeblue}

    CXR-BERT {\scriptsize -- \citeauthor{boecking2022making}} & \cmark & $\sim$ / $-$   \\
    \rowcolor{cambridgeblue}

    ESIM {\scriptsize -- \citeauthor{Chen2017EnhancedLF}}  & \xmark & \cmark / $-$  \\

    \rowcolor{Gray}
    T0 \& T0++ &  \xmark & \xmark / \cmark  \\
    \rowcolor{Gray}
    GPT-3 & \xmark & \xmark / \xmark  \\
    \rowcolor{Gray}
    GPT-3-\{NLI, GW\} & \xmark & \cmark / \cmark   \\

    \rowcolor{Pink}
    CXR-BERT-NLI & \cmark& \cmark / $-$  \\
    \rowcolor{Pink}
    PubMedBERT-NLI & \cmark& \cmark / $-$  \\
    \rowcolor{Pink}
    SciFive$_{\text{large}}$-\{NLI, GW\} & \cmark & \cmark / \cmark  \\
         \rowcolor{Pink}
     \modelsequential & \cmark & \cmark / \cmark  \\ 
     \model & \cmark & \cmark / \cmark \\

    \bottomrule
    \end{tabular}
    }
    \caption{
        Baseline comparisons grouped into three categories: (1) task-specific zero-shot baselines \colorbox{cambridgeblue}{(green)}, (2) large language models \colorbox{Gray}{(grey)} and (3) sequential training on in-domain text and general-domain task labels \colorbox{Pink}{(pink)}. "\cmark" and "\xmark" specify whether the given data source was used for training. "Summ." means summarisation. Models only evaluated on NLI do not require summarisation data, hence "$-$". "$\sim$" indicates that BERT and CXR-BERT were fine-tuned on MedNLI, a `near-domain' NLI dataset. 
    }
    \label{tab:baseline_comparison}
\end{table}