\begin{table}[tbh]
    \small
    \centering
    \scalebox{0.86}{
    \begin{tabular}{@{}lcc@{}}
        \toprule
        \textbf{Model} & \textbf{Accuracy} & \textbf{$F_1$-score}  \\
        \midrule
        
        \multicolumn{3}{c}{\textbf{Radiology (RadNLI)}} \\
        % \midrule
        %\addlinespace[5pt]
        \rowcolor{cambridgeblue}
        BERT \cite{miura-etal-2021-improving}  & 53.3 & -\\ % BERT-MedNLI
        %  BERT-SNLI (TODO) &  \\
        \rowcolor{cambridgeblue}
        CXR-BERT \cite{boecking2022making} & 65.2 & -\\
      
        %\addlinespace[5pt]
        \rowcolor{Gray}
        T0 (3B)  & 24.2 & 21.2 \\
        \rowcolor{Gray}
        T0++ (11B) & 35.4 & 33.3 \\
        \rowcolor{Gray}
        GPT-3  & 22.1 & 18.9 \\
        \rowcolor{Gray}
        GPT-3-NLI & 26.7 & 25.6 \\

        \rowcolor{Pink}
        CXR-BERT-NLI & 75.0	& 73.5 \\ % our fine-tuned version
       
        \rowcolor{Pink} SciFive$_{\text{large}}$-NLI & 47.5  & 35.4 \\
         \rowcolor{Pink} SciFive$_{\text{large}}$-NLI + SFT & 70.2  & 66.3 \\ 
        \rowcolor{Pink}  \modelsequential & 78.3  & 75.6 \\
        \textbf{\modellarge} &  \textbf{82.1} & \textbf{79.8} \\
        
        \midrule
        
        \multicolumn{3}{c}{\textbf{Biomedicine (MedNLI)}} \\
        \rowcolor{cambridgeblue}
        ESIM (MultiNLI) \cite{Chen2017EnhancedLF} & 51.7 & -\\

        \rowcolor{Gray}
        T0 (3B)  & 37.0 & 23.9 \\
        \rowcolor{Gray}
        T0++ (11B) & 55.2 & 44.6 \\
        \rowcolor{Gray}
        GPT-3  & 39.9 & 38.5 \\
        \rowcolor{Gray}
        GPT-3-NLI & 39.2 & 28.8\\
        \rowcolor{lightblue}

        \rowcolor{Pink}
         PubMedBERT-NLI & \textbf{75.7} &	\textbf{75.8} \\ 
        \rowcolor{Pink}\rowcolor{Pink} SciFive$_{\text{large}}$-NLI & 50.1 & 41.4 \\
        \rowcolor{Pink}
        SciFive$_{\text{large}}$-NLI  + SFT & 67.3 & 65.8 \\

        \rowcolor{Pink} \modelsequential & 71.4  & 71.5 \\
        \textbf{\modellarge} &  71.2 & 69.9\\
        \bottomrule
    \end{tabular}
    }
    \caption{Zero-shot \ac{NLI} results, showing micro accuracy and macro $F_1$. BERT and CXR-BERT are trained on MedNLI, we reproduce numbers from \citet{miura-etal-2021-improving} and \citet{boecking2022making} respectively. ESIM \cite{Chen2017EnhancedLF} is the highest-performing directly-transferred model reported by \citet{romanov2018lessons}. T0 \cite{sanh2022multitask} and GPT-3 \cite{brown2020language} baselines were conducted by us, matching stated hyperparameters where possible. Models with ``-NLI'' are fine-tuned or prompted baselines. `SFT' means with self-fine-tuning.}
    \label{table:zeroshot_nli}
\end{table}