\section{Related Work}
\label{sec:rw}

\paragraph{Cross-task transfer with text-to-text models}
\ac{T5} \citep{raffel2019exploring} unifies \ac{NLP} tasks under a seq-to-seq framework and solves them using a single model.
T0 \citep{sanh2022multitask}, FLAN \citep{wei2022finetuned}, MetaICL \citep{min2021metaicl}, and ExT5 \citep{aribandi2022ext} build on top of this idea and explore pretraining \ac{T5} with a massive collection of \ac{NLP} datasets with diverse natural language prompts.
Among them, T0, FLAN, and MetaICL investigate pretraining on a set of tasks, and then zero-shot transfer to another set of unseen tasks.

\paragraph{Domain-specific pretraining}
\citet{gururangan-etal-2020-dont} shows continual training on domain and task data can adapt pretrained models for new domains and tasks. Both BioBERT \citep{lee2020biobert} and BlueBERT \citep{peng-etal-2019-transfer} apply the BERT pretraining protocol (i.e., masked language modelling and next sentence prediction) on \ac{PMC} or PubMed articles.
They continue pretraining BERT checkpoints instead of training from scratch. \citet{gu2021domain} demonstrates the importance of domain-specific vocabulary and pretraining from scratch when in-domain text is abundant, and produces PubMedBERT by pretraining on PubMed articles.
Similar to PubMedBERT, SciBERT \citep{beltagy-etal-2019-scibert} pretrains from scratch on a mix of both PMC and computer science publications. \citet{boecking2022making} introduces CXR-BERT which is pretrained on biomedical and radiology corpora. 
SciFive \citep{phan2021scifive} continually pretrains T5 checkpoints on PubMed abstracts with seq-to-seq MLM. 
We compare to finetuned versions of SciFive, PubMedBERT and CXR-BERT in \Cref{sec:main_results}.

\paragraph{Zero-shot domain transfer learning}
Training in one domain and directly testing on another domain has been a prevalent paradigm in zero-shot cross-domain transfer \citep{miura-etal-2021-improving,boecking2022making,agrawal2022large}.
A similar zero-shot setup is also frequently seen in other transfer learning scenarios such as cross-lingual zero-shot learning \citep{conneau-etal-2018-xnli, conneau-etal-2020-unsupervised}. Our summarisation experiment is most similar to such a direct zero-shot setup. 
Concurrently, 
\citet{pan-etal-2022-task} also proposes to combine in-domain training and out-of-domain task knowledge. They proposed a zero-shot in-domain question answering model by finetuning a general-domain RoBERTa model with first domain-specific NER and then general-domain question answering. 
This study is the closest to our approach, with several key differences: Their method requires in-domain labels (in-domain NER) whereas we do not require any in-domain task labels. They only test on question answering whereas we show a more diverse range of evaluation datasets. Additionally, they do sequential training whereas we perform multi-task training. Finally, their model is not generative and therefore it cannot perform NLGU and self-finetuning as we did in our approach (see \Cref{sec:method}).

Our proposed NLGU and self-finetuning strategies are closely related with cross-domain data augmentation. A line of work in information retrieval generates ``in-domain'' pseudo training data leveraging unlabelled in-domain texts.
As an example, \citet{ma-etal-2021-zero,wang-etal-2022-gpl} train a passage-to-query generator for synthesising in-domain queries for the task of zero-shot passage retrieval. Similarly, The NLG component in our proposed NLGU strategy can also perform data augmentation but with better granularity and diversity as we can generate label-conditioned task data to create both positive and negative examples.

Besides zero-shot transfer in NLP, unsupervised domain adaptation (which also assumes labels in current domain and unlabelled data in the target domain) is a long-standing research topic in machine learning in general \citep{huang2006correcting,pan2010domain,ganin2015unsupervised, ramponi-plank-2020-neural}. Many conventional unsupervised domain adaptation methods require external components to align domains on the feature/embedding level. For example, \citet{pan2010cross} proposes applying spectral feature
alignment to align domain-specific words across domains into unified clusters. \citet{ganin2015unsupervised} adds a domain classifier that promotes domain-invariant features via a gradient reversal layer. These methods are not always immediately suitable for the recent pretrained language models especially the text-to-text models. In comparison, our approach exploits the task unifying nature of text-to-text models which contain the inherent transfer learning abilities and requires minimal architecture changes.