\section{Introduction}

Foundation models\footnote{Foundation models can roughly be defined as large pre-trained machine learning models that are used as a starting point for various computational tasks.} that are trained on large-scale internet data serve as the base for an increasing number of deployed applications in the real world~\citep{bommasani2021opportunities}.
Models such as GPT-3/4~\citep{brown2020language,https://doi.org/10.48550/arxiv.2303.08774}, Stable Diffusion~\citep{rombach2021highresolution}, and Codex~\citep{chen2021evaluating} are actively being integrated into a variety of products like Duolingo's Language Learning App,\footnote{\url{https://blog.duolingo.com/duolingo-max/}} Stability AI's DreamStudio,\footnote{\url{https://stability.ai/}} GitHub's CoPilot,\footnote{\url{https://github.com/features/copilot}} and more.
Researchers are grappling with the legality and ethics of developing and deploying these models using data broadly collected from the internet.
Many have raised concerns about using uncurated internet data for model development, touching on issues of privacy~\citep{carlini2021extracting} and fairness~\citep{bender2021dangers}.
But as foundation models are deployed in ways that can harm the markets of the original data creators, particularly when generating content similar to the original data, intellectual property rights become a growing concern.
In this paper, we study the legal challenges of building and deploying foundation models from the perspective of intellectual property, focusing mainly on copyright.

Under United States ("U.S.") law, copyright for a piece of creative work is assigned ``the moment it is created and fixed in a tangible form that it is perceptible either directly or with the aid of a machine or device''~\citep{copyrightoffice}.
The breadth of copyright protection means that most of the data that is used for training the current generation of foundation models is copyrighted material.
For example, \citet{bookcorpus} pointed out that the BookCorpus contains copyrighted data under restrictive licenses and has been used to train large foundation models including GPT-3~\citep{brown2020language} and BERT~\citep{devlin2018bert}.
Similarly, The Pile \citep{gao2020pile} contains Books3, a dataset of copyrighted and commercially sold books downloaded from Bibliotik, a torrent tracker for books and learning materials~\citep{books3,biderman2022datasheet}. 
More generally, most foundation models are trained on data obtained from webcrawls like C4~\citep{2019t5} or OpenWebText~\citep{openwebtext}. 
Since most online content has copyright protections attached at creation, using them for certain purposes could be considered infringement.\footnote{We note that there are nuances to even the infringement point, since some uses that respect robots.txt specifications might have an implied license as described in \citet{fields}. This is unlikely to apply to all generated model outputs, however, and we discuss this further in \S~\ref{sec:filtering}.}
Researchers, at least in the United States, have long relied on the legal doctrine of \emph{fair use} to avoid liability from using copyrighted data. 
Fair use allows the public to use copyrighted material for certain types of purposes---even without a license---especially when the end-product is \emph{transformative}.
For example, when releasing potentially copyrighted content in the past, individuals and organizations have relied on rough guesses for what constitutes fair use. A common approach is to release snippets: 5-grams~\citep{generalindex}, 11-grams~\citep{bitext}, or several pages~\citep{ebooks}. 

\citet{lemley2020fair} have pointed out that training a machine learning model on copyrighted data is likely considered fair use in circumstances where the final model does not directly generate content. 
For example, training a model on a corpus of popular books solely for predicting the similarity of two passages is transformative and likely falls under fair use.\footnote{Though recent litigation points out that no court has actually weighed in on the matter of whether model training is fair use~\citep[Complaint at 23]{githublitigation}.}
However, when it comes to training and deploying foundation models for \textit{generative} use cases, the analysis becomes more complex.
This is because these models are usually capable of generating content similar to copyrighted data, and deploying them can potentially impact economic markets that benefit the original data creators. 
For these scenarios, legal scholars argue that fair use may not apply~\citep{lemley2020fair,sobel2017artificial,levendowski2018copyright}.






By expanding the capabilities of models, machine learning researchers and practitioners have stumbled into the muddy waters of fair use.
As a result, websites like Getty Images have banned AI-generated content~\citep{getty}, and lawsuits have been filed against products using foundation models, namely GitHub Copilot and Stable Diffusion~\citep{githublitigation,stablediffusionlitigation,gettylawsuit}.
In this work, we shed light on this subject matter for machine learning researchers and highlight that significant additional work is required to de-risk foundation model deployments for generative use cases, focusing primarily on U.S. laws.

First, we provide an overview of U.S. case law on the fair use doctrine.\footnote{We examine U.S. fair use doctrine, rather than international doctrines, for two reasons. First, companies have specifically pointed to fair use as a defense for their use of foundation models. For example, former Github CEO Nat Friedman pointed to fair use when referring to Github's Copilot deployment. \textit{See} \href{https://twitter.com/natfriedman/status/1409914420579344385}{https://twitter.com/natfriedman/status/1409914420579344385} Second, the expertise of the authors is in U.S. law.} 
We draw analogies to foundation model use cases. We supplement these with a review of prior experiments, as well as novel experiments, and illustrate that foundation models can produce content that is sufficiently similar to copyrighted material. Furthermore, the case law suggests that even certain types of transformations of the training data would not be considered fair use.
Thus, the risk of infringement is real, and fair use will not cover every scenario where a foundation model is created or used.
The exact amount of risk is unclear, and the law will evolve with ongoing litigation.

Second, we overview technical mitigation strategies that will reduce this risk in accordance with the current state of the fair use doctrine. 
\citet{grimmelmann2015copyright} stated that ``paying attention to robotic readership refocuses our attention on the really fundamental questions: what is copyright, and what is it for? To say that human readers count and robots don’t is to say something deep about the nature of reading as a social practice, and about what we want robots—and humans—to be.''
\citet{lemley2020fair} suggested that humans and AI should be held to similar standards when it comes to copyright.
If this is the case, it is the job of machine learning researchers and practitioners, working together with legal practitioners, to ensure that foundation models create transformative content which would pass muster under the same fair use analysis as provided to a human.
To get there, new strategies and techniques will need to be developed, taking steps to ensure that foundation models behave in more transformative and novel ways. We call for more research to align technical mitigation strategies with fair use, including better output filtering mechanisms relying on higher-level semantics and new innovation in training-time techniques like extraction-preventative learning from human feedback. 
Developing these mitigation strategies is an important research challenge for machine learning and natural language processing and would bring practices in the two fields into better alignment with the law.



Lastly, we argue that a co-evolution of technical mitigation strategies and law can help establish a middle ground where the positive impact of foundation models is realized while reducing the harms to data creators' intellectual property rights. 
With the current uncertainties of fair use doctrine, as \citet{sobel2017artificial} and others noted, the law may sway to one extreme or another. On one hand it could lead to overly permissive interpretations of fair use that could allow \textit{any} generative AI use, disregarding the rights of data creators. Or it could lead to overly restrictive interpretations of fair use that could broadly prevent foundation model training and use, concentrating power among entities that have already acquired vast quantities of licensed data.
By developing and deploying strong technical mitigation strategies, it may be possible to lessen the risk of such extreme legal outcomes.
And the law should take into account the existence and strength of such technical mitigation strategies.
This could involve a multi-pronged approach: considering technical mitigations in fair use assessments, clarifying the status of DMCA protections for foundation models, or developing DMCA-like safe harbors for deployments that use \textit{strong} technical mitigation efforts, pursuing policy strategies for reducing harms to labor, and more.
Realizing this middle ground requires the participation of a much broader community including the data creators impacted by foundation models, technologists, legal professionals, among many others. 
We encourage more multidisciplinary work to further the co-evolution of law, policy, and technical methods for mitigating intellectual property harms.


Overall, the goal of this work is to act both as a guide and call-to-action for ML researchers and practitioners to actively pursue technical mitigation strategies. We hope that this guide helps instill a better understanding that fair use is not a panacea, and that a nuanced comprehension of the legal landscape's intricacies is vital to effectively navigate potential pitfalls and uncertainties. Furthermore, this work may also prove useful to lawyers and policymakers, providing them with more insight into potential technical details of foundation models, including technical mitigation strategies, and how they might play a role in the developing legal best practices and potential reforms.










