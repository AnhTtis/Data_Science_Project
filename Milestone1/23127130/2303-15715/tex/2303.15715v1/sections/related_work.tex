
\section{Related Work}

While we have generally referenced related work throughout this paper, here we briefly highlight several areas of work that we build on and survey.
Related work to our own can fall into two categories: (1) examining technically how models regurgitate training data; (2) understanding copyright law as applied to machine learning systems. 

\paragraph{Ethical Considerations of Foundation Models.} A number of other works have noted the potential risks and harms of using foundation models. \citet{bommasani2021opportunities}, \citet{bender2021dangers}, and \citet{henderson2018ethical} all provide high level overviews of the potential risks from language model or foundation model deployments. \citet{weidinger2022taxonomy} taxonomize the risks of language models, noting copyright infringement and effects on creative economies.

\paragraph{Technical Examinations of Regurgitation.} Several works have demonstrated how various factors affect generation of memorized content~\citep{carlini2019secret,lee2022language,carlini2022quantifying,kandpal2022deduplicating,carlini2021extracting,yu2023bag}. These works have consistently found that generative models memorize or plagiarize content. The percentage of verbatim outputs varies depending on extraction strategy and the model, but varies from 0.007\%~\citep{kandpal2022deduplicating} to 4.85\%~\citep{lee2022language} (variation comes from methodology of sampling and similarity metric).

\paragraph{Legal work examining copyright and Artificial Intelligence.} On the legal side, a large body of work has covered potential legal risks and challenges of machine learning~\citep{sobel2017artificial,burk2019algorithmic,lemley2019remedies,gillotte2019copyright,lemley2020fair,franceschelli2022copyright,guadamuz2017androids,grimmelmann2015copyright,mccann2021copyright,mcjohn2020fair,levendowski2018copyright,samuelson2017saving,lim2022ai,samuelson2021text}. 
Many of these note how fair use law might apply in different ways to machine learning models and how outcomes are uncertain.

\citet{levendowski2018copyright} points out that more expansive notions of copyright law could help with challenges of bias and equity by allowing the inclusion of more data into models. This is countered by \citet{maori}, \citet{reed2021fair}, and others who have pointed out that data can be used from marginalized communities without their say by leveraging fair use law. This could take away their voice in their data's governance.

Others have examined how machine learning or algorithms can be used for mitigating infringement risk at a high level, including \citet{elkin2017fair,scheffler2022formalizing}. But others have pointed out that such filtering strategies can have harmful effects~\citep{bartholomew2014death,boroughf2015next,lim2022ai,levendowski2018copyright}.

\citet{tang2022class,tang2021copyright} discusses the challenges (and benefits) of bringing class action litigation against new technologies not unlike foundation models. They describe how class action lawsuits can act as a licensing mechanism at scale when it is nearly impossible to aggregate licenses from many singleton data creators.

Unlike many of these other works, we marry the doctrinal discussion of fair use to technical mitigation strategies. We provide a short primer on fair use doctrine as it applies to foundation models before highlighting potential deficiencies in current risk mitigation strategies that have been employed. This acts as a survey of some similar discussions in prior work but also expands it with experiments and concrete examples of foundation model uses. Our aim is to speak to both machine learning researchers and legal professions to point out the exciting \textit{technical} research agenda that would make foundation models more in line with fair use as well as policy-relevant considerations for the evolution of the law.



\paragraph{Alignment.} A significant amount of recent work has focused on the AI alignment problem, broadly defined, where researchers have sought to align foundation model outputs with societal values. Some of the technical mitigation strategies we propose here can be related to this line of work. This includes, for example, making FMs more aligned with human preferences and more likely to follow instructions~\citep{christiano2017deep,ziegler2019fine,ouyang2022training,wei2021finetuned,sanh2021multitask}. 
\citet{hendrycks2021unsolved} provide a survey of unsolved challenges in AI Safety, including alignment.
Broadly, our proposal can be thought of as contributing to the better alignment between Artificial Intelligence on one hand, and law \& policy requirements on the other. 

\paragraph{Data Governance and Curation.}
The recent literature on data governance and curation discusses fair use in machine learning~\citep{jernite2022data,paullada2021data,ganguli2022predictability}. For instance, \cite{jernite2022data} weigh in the stakes of data creators and examine their \emph{property rights} when developing the data governance framework. 
\cite{paullada2021data} survey legal issues with benchmark datasets and comment on the nuances and the novelty of rising problems involving large-scale machine learning and copyright law. 
Our work is related to these prior works but goes deeper into the legal nuances with concrete case studies and state-of-the-art model artifacts obtained from real experiments. 


