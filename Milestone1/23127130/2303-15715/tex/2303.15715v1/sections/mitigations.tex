


\section{Technical Mitigation}
We analyzed the applicability of fair use standards to foundation models and studied various scenarios in different domains.  We have shown that what constitutes fair use is contextual and requires reasoning about a higher-level semantic space that is directly tied to the expression of ideas.
In contrast, most technical work on copyright evaluation and mitigation focuses on near-verbatim overlap, which we argue is insufficient on its own~\citep{githubcopilotrecitation,liang2022holistic,https://doi.org/10.48550/arxiv.2302.10870}.
We survey existing and potential tools, advocating for the development of new technical mitigation strategies that are tailored to fair use doctrine.


There are major challenges to this task: contextual information relevant to fair use determination may be missing (e.g., the specific usage pattern of the content produced by a model); legal scholars themselves recognize that fair use judgement cannot be reduced to an algorithm~\citep{burk2019algorithmic};
and there is often disagreement on how fair use assessments of foundation models will or should be assessed.
Nonetheless, when non-transformative content generation is possible, it will be important to adopt technical strategies that go beyond verbatim text matching to increase the likelihood of a successful fair use defense and to respect the rights of data creators.\footnote{Note that these mitigation strategies will generally be more important for models that are deployed and accessible to the public, but secondary liability might also affect model development if the model is released without restriction (and later deployed).
If a model is developed without release (or via restricted release for research purposes), mitigation strategies may be less important.} 

We consider four types of approaches: data and output filtering (\S \ref{sec:filtering}); instance attribution (\S \ref{sec:instance}); differentially private training (\S \ref{sec:dp}); and fair use alignment via learning from human feedback (\S \ref{sec:rlhf}).
For each, we assess current examples and suggest paths forward to ensure closer alignment to fair use.
We emphasize that it would be prudent to take a mixed approach, leveraging each of these mechanisms to ensure that model outputs are truly transformative and in line with existing notions of fair use.
Within each of these strategies are exciting new research agendas. How does one identify what a parody is? How does one distinguish facts from creative expression? How do we think about content similarity across different dimensionalities relevant to fair use? How do we train models to learn only high-level concepts from the material that they ingest, while still outputting coherent low-level outputs?
These research agendas not only help us align more with fair use, but drive models to function more as though they are inspired by existing creative expression to generate new and wholly transformative content, as opposed to remixing. 
Figures~\ref{fig:innovations} and \ref{fig:deployment_v_training} help situate mitigation strategies and necessary innovations.



\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.6\textwidth]{figures/conforming_to_fair_use.pdf}
        \caption{In the long run, with additional research, technical mitigation strategies can help address some aspects of fair use, such as identifying protected parodies, discerning creative expression from facts, and identifying non-transformative outputs. But they will not cover other times of considerations like the target market, the purpose of the outputs as a whole, whether the use is commercial, and any good faith actions by model creators and hosts.
        }
    \label{fig:innovations}
\end{figure}

\begin{figure}[!htbp]
        \centering

        \includegraphics[width=.6\textwidth]{figures/classifying_mitigation_strategies.pdf}

    \caption{Data filtering, reinforcement learning form human feedback, differentially private training are all strategies that must be persued at training time. Output filtering and instance attribution will usually be implemented at model deployment time. We note, though, that some future strategies might have components that are found at both model training and model deployment time. For example, an instance attribution method might require training a model in a particular way and then invoking that component and deployment time.}
    \label{fig:deployment_v_training}
\end{figure}


\subsection{Data and Output Filtering}
\label{sec:filtering}

\paragraph{Data Filtering.} There are two main types of data filtering that we will consider that control the content that a foundation model is trained on.

\noindent \textit{Underlying licenses, copyright status, and opt-outs.} The first type of data filtering approach for mitigating the risk is to not train on copyrighted (or restrictively licensed) material at all. 
\citet{li2022competition} filter their dataset of source code collected from GitHub by license to train AlphaCode. 
Similarly, \citet{Kocetkov2022TheStack} filter their code dataset for permissive licenses.
Given the plethora of open-license software, code-based methods can mitigate some of the risks by training on open data.

It is important to remember, though, that even if training data is filtered to permissible data, if it has source attribution requirements crediting people's code still remains a problem if code (even with permissive licenses) is reproduced by the model, and many open source and Creative Commons licenses contain provisions that models cannot feasibly comply with notwithstanding additional research, as we will discuss in conjunction with Instance Attribution strategies (\S~\ref{sec:instance}). As a result, even model creators relying on seemingly open licenses with source attribution requirements may have to implement other strategies in this section. 

And even when datasets are put under a license, the license might only apply to the \textit{collection}, not every underlying datapoint. So, for example, if the C4 dataset is released under a ODC-BY license\footnote{\url{https://huggingface.co/datasets/c4}} it may be that the collection is under this license, but each underlying piece of data is under different license terms.

Furthermore, if web-crawled data is used, restricting it to data that respects robots.txt opt-outs can make a fair use argument more tractable, though not guaranteed. As we noted before, in \citet{fields}, respect for the robots.txt file was considered in the fair use assessment with the court because it gave the plaintiff opportunity to opt out. 
This is likely why many webcrawl-based models rely on the CommonCrawl dataset as a source. Its webcrawl automatically respects robots.txt opt-outs and does not crawl every webpage in full.
It is possible then that future fair use assessments could consider respecting the robots.txt opt-out---or implementing other opt-out mechanisms---favorably, as was the case in \citet{fields}. Conversely, ignoring a robots.txt opt-out could negatively impact a fair use assessment.
However, \citet{kapoornaryanan} have argued that there are structural critiques of opt-out mechanisms beyond the current state of the law.

That being said, the general approach of filtering out any copyrightable content entirely from a dataset (and building foundation models with the remaining data) may not be possible for many practical settings where little to no open-domain or permissively-licensed material is available. It is unclear whether restricting a foundation model to only public domain, non-copyrightable, or otherwise permissively-licensed data could yield a strong foundation model in all domains---though though this is an avenue well worth researching and understanding.\footnote{In particular, a swath of content from the early 1920s is rapidly entering the public domain, increasing the amount of training data available to use without restriction. But this data also is likely to bear undesirable features from the era, including racist or mysognistic content.} It may also bias datasets, reducing the efficacy of the model and creating other types of legal and technical problems \citep{levendowski2018copyright}. Nonetheless, entities that already retain large and diverse amounts of licensed or public domain data can readily train models using primarily data-filtering strategies. Adobe, for example, recently did so for image generation models.\footnote{\url{https://www.theverge.com/2023/3/21/23648315/adobe-firefly-ai-image-generator-announced}} 




\noindent \textit{Data Quality for Less Memorization.} Another type of data filtering seeks to remove duplicates from the training set~\citep{dallemitigations,kandpal2022deduplicating}. 
The goal here is to identify sufficiently similar examples and remove all but one of them. 
The fewer times a model sees an example during training, the less likely it will memorize it~\citep{lee2021deduplicating,kandpal2022deduplicating}. 
This deduplication approach is empirically useful but does not absolutely prevent memorization and regurgitation.
Deduplication may also be difficult. For example, if a dataset house thousands of distinct images of a given NBA player with a distinctive tattoo, it may be difficult to deduplicate all of these images in a way that prevents the model from learning to reproduce the tattoo verbatim. Thus, situations like \citet{take21} might still occur with this strategy.

\begin{tcolorbox}
\noindent \textbf{Potential Research Questions.} Can foundation models be trained to perform equally well on totally open domain data? Can deduplication schemes take into account high-level semantic similarity in line with fair use without significantly hurting performance? How can we separate fact from expression in training data filtering? For example, \citet{henderson2022pile} suggest that a contextual approach is needed to filter input content for privacy and toxicity. Could such a contextual approach provide useful for data filtering in copyright contexts as well?
\end{tcolorbox}







\paragraph{Output Filtering.}
Assuming the model is already trained on copyrighted material and were to be deployed, one simple idea for preventing training data from being reproduced is to apply a filter during model inference so that any output that mirrors the training data can be detected.
This approach was benchmarked by Copilot’s developers~\citep{githubcopilotrecitation}.
Aside from experiencing technical challenges related to increased model inference cost, this approach can be flawed when applied to contexts where a violation of fair use occurs with the non-exact reproduction of copyrighted material.
For instance, \cite{verbatim-not-private} showed that minimally modified style-transfer prompts can evade filters developed based on the verbatim match criterion.
And, though it is unclear whether OpenAI instituted an output filter for our Harry Potter scenario in \S \ref{sec:text}, we were able to bypass it with a simple instruction.
To capture these sorts of transformations, output filtering techniques will need to go beyond simple surface-level matching.

Based on the case law we discussed, a more fair-use-aligned output filtering approach would focus on detecting transformations unlikely to be fair use, such as direct translations and abridgements.
It would ideally also take into account situations where reproduction of content is permitted, including parodies, or factual content. 

Other structural factors, such as the nature of the original training task versus the target task, could help also reduce potential risks.
For example, building a model that predicts sentiment from a corpus of books is likely to be transformative, and outputting the answer to a math question, without an explanation, would likely be accepted as factual content.
However, if the model goes beyond the simple mathematical answer and outputs a verbatim explanation from a textbook, then it might be more problematic in some cases.
So, restricting the structure of a model's outputs to these sorts of short, factual outputs can be one potential strategy.

There is an exciting new research agenda that would build an output filter which captures some notions of transformativeness under the fair use doctrine. Using such an output filtering mechanism, generation would be biased toward more unique and transformative content, likely to significantly lower -- but not eliminate -- the risk of infringement liability. Developing such an output filter can be challenging due to the (near) amorphous nature of fair use standards~\citep{burk2019algorithmic}, but filtering need not capture the fair use standard perfectly. Instead, filtering should simply reduce the risk of infringement liability.
As such, we believe this is an interesting research direction and there is a tractable path toward risk reduction.

\begin{tcolorbox}
\noindent \textbf{Potential Research Questions.} How can we develop new high-level semantic similarity measures that capture some aspects of transformativeness for output filtering? How can we separate fact from expression in output filtering? How we can we identify parodied content? How can we make robust output filters that prevent users from bypassing them? How can we make output filters that are robust to user manipulations? How can we use output filtering in a way that doesn't induce model biases?
\end{tcolorbox}





\subsection{Instance Attribution}
\label{sec:instance}
\emph{Instance attribution} refers to methods that assign attribution scores to training examples to understand the contribution of individual examples (or group of examples) to (test-time) model predictions~\citep{koh2017understanding,ghorbani2019data,jia2019towards,pezeshkpour2021empirical,ilyas2022datamodels}. 
These approaches tend to adopt techniques such as leave-one-out retraining or influence functions to understand model behavior, fix mislabeled examples, and debug model errors~\citep{koh2017understanding}.\footnote{As \citet[at 5-6]{feldman2020neural} note, instance attribution mechanisms could be related to Shapley values---though Shapley values are typically used for attribution of model outputs to input features, e.g., \citet{sundararajan2020many}.}

One application of instance attribution is in determining the source of a generated output. The attribution scores can provide information on whether the output was influenced by a particular copyrighted text (or texts). Accurate attribution scores can then be used as a measure for evaluating the copyright infringement risk associated with the output, and to implement an output filter that prevents any output that heavily relies on a single source. 

Instance attribution can also address the credit assignment problem by providing a clear attribution page that lists all works which contributed to the output, along with licensing information, to comply with creative commons license attribution guidelines. This might help mitigate DMCA \S 1202-type claims.
In an idealized setting, one can imagine a scenario where every output created an attribution page that enumerated any work that contributed a non-negligible amount to the output, along with licensing information.
And in other cases, one might seek to have a post-hoc mechanism to delete information about a particular training example from a model~\cite{bourtoule2021machine}---such as if a DMCA request for takedown is provided. 

While promising, current techniques in instance attribution tend to suffer from difficulties in scaling due to high computational cost (e.g., leave-k-out retraining can be costly)~\citep{feldman2020neural,zhang2021counterfactual} or being inaccurate or erroneous when applied to complex but realistic model classes~\citep{basu2020influence,ghorbani2019interpretation,sogaard2021revisiting}.


It's worth noting that retrieval-augmented methods~\cite{guu2018generating,guu2020retrieval}, which perform attribution fundamentally and not post-hoc, are another approach to instance attribution. These models have the potential to overcome some of the limitations of post-hoc instance attribution methods, and they may also offer other advantages, making them a promising direction for future research.


\begin{tcolorbox}
\noindent \textbf{Potential Research Questions.} How can we use instance attribution to identify which \textit{exact} training data points contributed to any given output? How can we ensure that no single datapoint contributes more than a \textit{de minimis} amount to any given output? How can we make instance attributions scalable for runtime attribution?
\end{tcolorbox}




\subsection{Differentially Private Training}
\label{sec:dp}

\emph{Differential privacy} (DP) is a formal privacy guarantee that has been adopted in the U.S. Census and big tech (e.g., Smart Compose, telemetry collection)~\citep{smartcompose-dp,erlingsson2014rappor,ding2017collecting,bittau2017prochlo,cheu2019distributed}. 
In the machine learning context, the guarantee says that no adversary can  distinguish, with high probability, between a model trained with a particular training example and one trained without~\citep{dwork2014algorithmic}. 
In other words, model parameters do not vary substantially with the inclusion or exclusion of individual instances. 
Machine learning researchers have theoretically and empirically shown that models trained with strong levels of DP guarantee are limited in memorizing training data, and extracting or reconstructing training data from DP-trained models can be close to infeasible~\citep{guo2022bounding,carlini2019secret}. 
Hence, machine learning with DP guarantees appears to be a natural option for building useful data-driven applications with low copyright-related legal risks.




However, there are three main challenges with operationalizing DP to ensure fair use.
First, machine learning with DP has often been reported to suffer from high computational costs~\citep{carlini2019secret}.
Recent works have developed substantial improvements to address this drawback through the use of better software primitives~\citep{anil2021large} and training techniques~\citep{li2021large,yu2021differentially,de2022unlocking,sander2022tan}. 

Second, selecting appropriate \emph{privacy leakage parameters} is difficult. 
DP guarantees are usually stated with desired privacy leakage parameters (e.g., the $\epsilon$ parameter in pure-DP~\citep{dwork2014algorithmic}) that are set by hand in practice.
These parameters introduce an inherent \emph{privacy-utility} trade-off in which the smaller the parameters, the more the privacy (less memorization and regurgitation) and worse the model performance. 
Setting these parameters can therefore be tricky given that ideal target values tend to be application- and domain-dependent, and that downstream consequences of different choices are difficult to measure and interpret. 
While there is flourishing research on the topic~\citep{lee2011much}, none has studied this with the goal of leveraging DP to mitigate copyright-related risks. 

Third, it is difficult to define what constitutes a single example that should not be memorized.\footnote{The issue has been extensively studied in the privacy literature. \textit{See, e.g.,} \citet{kifer2011no} for examples in social networks. }
Intuitively stated, DP treats each example in a dataset as a secret.
If a certain secret appears frequently enough, a DP algorithm can still reveal it (since to the algorithm, this frequently occurring secret is a piece of common knowledge). 
Therefore, when applied to address copyright issues, the division of the dataset into individual instances needs to be taken with great care in order for the guarantee to be meaningful from the copyright standpoint. 
Below, we outline hypothetical scenarios where DP algorithms don't give the desired mitigation effects.



\begin{hypothetical}{Differentially Private Lyric Generation.}{text}
 Imagine that a developer intends to train a machine learning model to aid musicians to create lyrics. 
    The developer scrapes copyrighted lyrics of songs from music websites. 
    However, the lyrics of the same song are scraped multiple times, each of which is treated as a single example in the dataset. 
    Additionally, the developer isn't careful about removing duplicates before training the model with DP. 
    The final model thus ends up reproducing verbatim chunks of lyrics of certain songs.
    The lyricist whose lyrics were reproduced by the deployed model sues an end user who wrote a song with the help of this model.
\end{hypothetical}

\begin{hypothetical}{Differential Privacy and Trademarks.}{text}
Imagine a text-to-image model was trained with lots of images that have the \emph{same} trademark (e.g., the trademark is positioned in similar locations on each image and likely to be memorized). 
    Since there is a strong correlation between examples in the training set, the image-level DP guarantee does not prevent the model from generating images that contain the blob of trademark symbol or text. This was one real-world challenge that was cited for DALL·E's filtering technique, noting that it can create real trademarks and logos~\citep{mishkin2022risks}. And recently, litigation by Getty Images explicitly cited trademark infringement due to its watermark being regurgitated in generated images~\citep{gettystability}.
\end{hypothetical}

The above examples highlight that to leverage DP in a meaningful way, one needs to ensure that the division of data is handled at a semantic level that is meaningful in fair use standards.
Finding out the ``right'' semantic level is an interesting topic of future research. 
In addition, exact or fuzzy data de-duplication based on the target semantic level is likely useful to attain the ideal benefit of the DP guarantee~\citep{lee2021deduplicating,kandpal2022deduplicating}.

Recently, \cite{https://doi.org/10.48550/arxiv.2302.10870} introduced \emph{near access-freeness} (NAF) as a mathematical guarantee of copyright protection, along with a few practical algorithms for attaining the guarantee. 
The NAF guarantee is similar in spirit to the DP guarantee (both leverage indistinguishability as the core concept), but is different in their precise semantics as well as the algorithmic primitives.
In broad strokes, the NAF guarantee is attained for a model trained on copyrighted material, if the model generates in a manner similar to a model trained without that material. 
Technically, to achieve the guarantee, the proposed algorithms require that a single copyrighted material ``appear'' in at most a single (or a constant many) training example(s) in the original dataset. 
Applying a pure surface-level data deduplication scheme is insufficient to attain the above prerequisite, and better deduplication schemes based on higher-level understandings of similarity are likely required.
While this NAF guarantee, like other approaches, is not a panacea and requires more research to align with fair use, it is another powerful tool worth pursuing and tailoring to fair use standards.


\begin{tcolorbox}
\noindent \textbf{Potential Research Questions.} How can we identify higher-level similarity features to leverage differential privacy or NAF in a way that is in line with fair use? What are privacy budgets would be acceptable under fair use doctrine that would prevent significant degredations in performance?
\end{tcolorbox}

\subsection{Learning from Human Feedback}
\label{sec:rlhf}


Learning from human feedback~\citep{ouyang2022training} trains models to generate outputs that are aligned with human preferences and values. 
However, these approaches---and similar ones aimed at promoting helpfulness~\citep{wei2021finetuned,sanh2021multitask}---should also consider the copyright risk. Human feedback might reward verbatim generations of copyrighted content. 
For example, if a model is rated purely by how well it follows instructions, the highest reward for "Read me a Harry Potter book verbatim" would be to read the entire book verbatim, which could infringe on the source material's distribution rights.

To address this issue, human annotation frameworks in these approaches can take into account the copyright implications of rating systems and instruction following, particularly when incorporating human feedback at scale. 
For example, in current feedback-based learning mechanisms, human labelers are asked to rate model generations based on a Likert scale or pairwise comparisons.
A method for learning a reward function that both maximizes the capability of the model and respects fair use could add an additional question, where human labelers would be provided with the closest copyrighted content and asked to flag any content that is not sufficiently transformative from the copyrighted material.
Models can then be trained with this feedback incorporated.

This approach could be viewed as an extension of existing approaches to reducing the harmfulness of models~\citep{bai2022training,bai2022constitutional}. This approach provides no certifiable guarantee and it could be susceptible to reward misspecification. 
Nonetheless, it may be a useful component in reducing copyright violations, as it leverages existing mechanisms and ongoing research for value alignment.

As models improve in their capabilities, taking into account longer contexts and following instructions more closely, it might become easier to regurgitate non-transformative material. Asking a code-generating model in the coming years to ``Implement a chess playing app'' might copy the GPL-licensed Stockfish app in its entirety, increasing the likelihood of potential risks.\footnote{A scenario based on the real litigation of Stockfish against Chessbase that did not involve generative models, but involved the copying of the Stockfish neural network and surrounding code by Chasebase. \textit{See} \href{https://stockfishchess.org/blog/2021/our-lawsuit-against-chessbase/}{https://stockfishchess.org/blog/2021/our-lawsuit-against-chessbase/}.}
But at the same time, capable models might be better able to understand the idea of transformation and be easier to align from a copyright perspective. This highlights the importance of mitigation strategies like extractive-preventative RLHF that can balance improved capabilities with fair use.%


\begin{tcolorbox}
\noindent \textbf{Potential Research Questions.} How can we make models that follow instructions but don't allow users to easily bypass output filters? How can we train advanced models that follow instructions but in totally creative ways transformative from the training data? Is there a way to instill some partial knowledge of fair use so that models can reason about their own outputs can keep them in line with fair use?
\end{tcolorbox}




