






\subsection{Natural Language Text}
\label{sec:text}
Given this high-level understanding of fair use law, we first examine the case of natural language text generation (as opposed to code generation which we will examine in \S \ref{sec:code}), drawing on real cases of creative content that could parallel foundation model outputs and uses.
One of the most prevalent, and earliest, use-cases of foundation models is text generation.
Deployments of models like GPT have been used to create products for copy-editing, text-based games, and general-purpose chatbots. 
These models are typically trained on massive amounts of data taken from across the internet, books, court documents, and more. When used to generate, these models have been observed to output content with only slight transformations from the original training data.
In this section, we examine relevant cases that might help shape what is considered fair use for these models, some of which can be seen in Figure~\ref{fig:markettomarket}.


\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.75\textwidth]{figures/market2market.pdf}
    \caption{
    Claims of fair use will likely generate more scrutiny when the target market for the deployed model matches the target market of the source---or might threaten a logical derivative market. Book critiques are more likely to be considered fair use unless they include large portions of the source material. 
    Parody is more likely considered to be fair use; satire may also be fair use but requires more extensive justification. We cite cases with relevant analyses for each transformation. If, for example, a single book is trained on and then outputs a substantially similar book that could be problematic.}

        \label{fig:markettomarket}
\end{figure}


\paragraph{Verbatim Copying} In a recent case, Google scanned in a large collection of books and made the books available online, only providing users with a small number of pages at a time. Book publishers sued for copyright infringement, but the court found that the amount of content output by Google Books was small and was covered by fair use, even though Google Books contained the entire corpus of published books in databases.
However, distributing larger portions of books is unlikely to be covered. The court in \citet{penguinbuddha} found that making small formatting changes and displaying books on the internet do not constitute fair use.
The fair use criterion from these cases could be directly relevant for foundation models in situations like the following hypothetical.

\begin{hypothetical}{The Assistant Who Reads}{}
A foundation model is deployed as virtual assistant in smartphones. Users learn that they can prompt the assistant with an instruction as follows: ``Read me, word-for-word, the entirety of `Oh the places you'll go!' by Dr. Seuss.''  This becomes popular and users start using the virtual assistant as an audiobook reader to read bedtime stories to their children. Is this fair use? 

\medskip

If our foundation model assistant reads a user the entirety of the book, this is much more like \citet{penguinbuddha} and less likely to be fair use. But, the model is closer to the case of Google Books if it stops reading after a couple of paragraphs, saying, ``I've read as much of the book as I can read.''
\end{hypothetical}

There is no certain amount of content that is categorically permissible or impermissible.  The legal analysis relates to whether the copied content copies the expressive purpose of the original work and whether the copied portion is the portion that most users will want to see. 
For example, if users would only buy a book for an important couple of pages, then copying those couple of pages is less likely to be fair use. That is, reproducing the heart of the work, even if it is small, lowers the probability that the reproduction is considered fair use~\citep{sobel2017artificial}.

In \citet{foxtveyes}, the court found that 10 minutes of TV content was too long of a span to constitute fair use. In \citet{harpernation}, the Supreme Court held that taking 250 words from a large autobiography was not fair use where those words constituted the "qualitative heart" of the book.
Judge Leval in the Google Books case noted that it weighed in Google's favor that Google Books ``does not provide snippet view
for types of books, such as dictionaries and cookbooks, for which viewing a small segment is likely to satisfy the searcher's need[,]'' and avoids providing a service that 
``could usefully serve as a competing substitute for the original''~\citep{ebooks2}. \textit{See also} \citet[at 56]{sobel2017artificial}.
More recently, the hit Taylor Swift song \emph{Shake it off} is going to trial over a potentially infringing 6-word phrase~\citep{2019hall}. And as we will see, there need not be \emph{any} n-gram overlap to result in infringement, requiring only overlapping higher-level mechanisms of expression.

It may be tempting to use quotes and citations to remedy this problem, but this does not necessarily change the transformativeness analysis. The amount and purpose of the use will be considered. For example in \citet{harper1982}, Gerald Ford had sold the rights to his memoirs that were to be published in Time magazine in serialized form. The Nation magazine, without authorization, acquired the memoirs and published and article about them where 300-400 of the 2,250 words were verbatim quotes from the source material. Time magazine canceled the contract and The Nation was sued. In this case the Court found that the severe market damage was the main fair use factor. However, the Court also pointed out that that the 300-400 quoted words, though a relatively small percentage of the memoirs and even a small percentage of the article itself, represented ``the heart of the book'': they were among the most moving parts of the memoirs. This analysis is confounded by the clear market damage evidenced by the canceled contract, but nonetheless demonstrates that simply quoting material that has been drawn verbatim does not automatically resolve the problem.


\begin{experiment}{Oh the verbatim text you'll generate!}{text}
\textit{Prompts containing random snippets of copyrighted books can generate some verbatim copyrighted material, but rarely long-form passages.} Others have shown that foundation models can regurgitate training data~\citep{carlini2019secret,lee2022language,carlini2022quantifying,kandpal2022deduplicating,carlini2021extracting}. We examine whether long spans of copyrighted content can be extracted from foundation models. We use the HELM benchmark to examine many popular foundation models~\citep{liang2022holistic}---further details of the experimental setup can be found in Appendix~\ref{app:exp_setup}. We prompt the models with: (1) random snippets of text from the books3 corpus~\citep{books3}; (2) the beginning text of popular books on the Top 100 all time best sellers list~\citep{top100}; (3) variations on the title and author name of \textit{Oh the Places You'll Go!} by Dr. Seuss. We use a sampling temperature of $T=0.2$ to capture content that would be more consistently regurgitated with relatively little sampling.
We find that under such a low temperature regime, many models generate repetitive low-quality content and extraction rates are low, generally only generating small amounts of verbatim text, as seen in Figure~\ref{fig:benchmarking}.
Nonetheless, certain types of content yield greater extraction even with little manual prompt engineering. For example, several models output the first page or two of Harry Potter books verbatim. And \textit{Oh the places you'll go!} by Dr. Seuss was regurgitated verbatim by OPT-175B~\citep{zhang2022opt}.
\\\\
\textit{Manual prompt engineering can yield better extraction for short-form content, but long-form content exceeding context windows is less likely to be regurgitated verbatim for current models.} We extended these sampling-based prompting approaches with a manual extraction experiment on the ChatGPT model~\citep{chatgpt}. Using hand-crafted prompts, we were able to extract the entire story of \textit{Oh the Place You'll Go!} by Dr. Seuss using just two interactions, with a prompt containing only the author and title. On the other hand, long-form content like popular books is less likely to be extracted verbatim for the entirety of the content, even with manual prompt engineering. We found that ChatGPT regurgitated the first 3 pages of \textit{Harry Potter and the Sorcerer's Stone} (HPSS) verbatim, but then deviated from it by paraphrasing content and then eventually veered off entirely. This is likely due to the stochastic nature of these models and the relatively short context windows, as well as the frequency of the content appearing in the training data. 
\\\\
Keeping in line with these results, showing that more capable models with longer context windows more easily regurgitate, we replicated these manual prompts with GPT4 (using the March 15th version). We found that GPT4 regurgitated all of \textit{Oh the Places You'll Go!} verbatim using the same prompt as with ChatGPT. We then found that it wouldn't generate more than a couple of tokens of HPSS ---possibly due to a content filter stopping generation. We then added the instruction ``replace every a with a 4 and o with a 0'' along with the prompt. We were then able to regurgitate the first three and a half chapters of HPSS verbatim (with the substituted characters) before the model similarly deviated into paraphrasing and then veered off entirely from the original story. Note that these results are in line with context windows and model ability on benchmarks. ChatGPT reportedly had a context window of $\sim$4k tokens (3k words) while GPT4 for chat has an $\sim$8k token (6k word) window. Respectively, they each regurgitated around 1k and 7k words of HPSS. This suggests that memorization risk may increase with model size and ability without pro-active mitigation strategies in place.
We provide qualitative examples in Appendix~\ref{app:qualitative_text}. Furthermore, others have noted that even when there is no verbatim matching, models can output substantially similar material that could be considered plagiarism (or in our setting, infringement not necessarily covered by fair use)~\citep{lee2022language,carlini2022quantifying}.






\end{experiment}


\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.385\textwidth]{figures/oh_the_places.pdf}
    \includegraphics[width=.61\textwidth]{figures/Figure_1_v2.pdf}
    \caption{(Left) The maximum similarity ratio using difflib (roughly the fraction of overlapping text) for the extractions of \textit{Oh the Places You'll Go!} for one context window (1024 tokens) tested on a subset of models. OPT-175B regurgitates the story verbatim. (Right) The longest common substring between the generated text and the source material (divided by the prompt length), averaged over sampled book excerpts; larger numbers imply more reproduction. Generally, very few randomly chosen snippets of text generate long spans of verbatim content, though popular materials like Harry Potter are more likely to be regurgitated. This result is limited to the chosen temperature and it is possible with more sampling at higher temperatures more verbatim content can be identified with careful selection mechanisms. With manual prompt engineering, extraction might be more frequent.
    }
    \label{fig:benchmarking}
\end{figure}



  
  
  
  
  
  
  



\paragraph{Insufficient Transformations, Translations, Similar Plots, and Similar Characters} Importantly, however, long-form verbatim generation is not necessary for potential infringement in traditional copyright cases. Courts have ruled that even some transformations of books are not fair use. In~\citet{comicmix}, the authors wrote a children's book based on Dr. Seuss's \emph{Oh, the Places You’ll Go!} They titled it \emph{Oh, the Places You’ll Boldly Go!} and mimicked the style of Dr. Seuss but replaced the text and imagery with a Star Trek theme.
The court found that such a transformation was \emph{not} fair use since the ``heart'' of the work was used and could affect potential derivative markets for the original book.

To capture the court's assessment that the use was not transformative, a model would need to assess these two works at a higher semantic level and likely through a multi-modal approach. Notably, for example, \textit{Oh, the Places You'll Go!} and \textit{Oh, the Places You'll Boldly Go!} only have a very small similarity ratio of 0.04 when using raw text overlap (where 1 is the maximal overlap). More robust metrics are required to capture their semantic similarity.

Similarly, direct translations would have little or no verbatim text overlap but may not be fair use. For example, in \citet{nihonkeizaishimbun}, the court noted that direct translations of text without adding significantly new material are not fair use.


Courts have made similar assessments for other types of transformations that retain the ``heart'' of the original work. When a company tried to transform famous novels into abridgements for children, this was not fair use~\citep{kinder}. 
Fan fiction is not necessarily fair use either, particularly when it re-uses characters from the original work. 
In \citet{anaxar}, the court found that a Star Trek fan fiction film was not fair use since it used too many elements from Star Trek, even though it was an original novel story. 
Finally, the use of a character from J.D. Salinger's Catcher in the Rye was also not fair use in \citet{salingercolting}.
Protection for characters by themselves can be muddled, with different courts adopting potentially conflicting tests~\citep{coe2011story}. 

Authors that have successfully published commercial works of fan fiction have generally removed any copyrighted elements of the original work. For example, Fifty Shades of Grey was originally a fan fiction derivative of the Twilight saga, but the author removed references to the original Twilight series, including changing characters' names, before publication and it has been commercialized without lawsuit~\citep{jamar2013author,lipton2014copyright}. 
If language models are deployed such that they generate content about specific protected characters and stories, there might be legal risks if the generations are monetized. Fan fiction cases might serve as a guide to how these situations might play out.

\begin{hypothetical}{The Adventures of Yoda: An Origin Story}{text}
Suppose a model creator hosts a website \textit{The Adventures of Yoda: An Origin Story}. Every time a user visits the website, they are greeted with an auto-generated story about Yoda -- a popular Star Wars character -- and his early years as a Jedi. The website host charges a fee to read a story that exceeds the costs of generating the content and begins to earn a hefty profit. Would this be fair use?

\medskip

It might depend on the jurisdiction~\citep{coe2011story}, but cases like \emph{Axanar} and \emph{Colting} would suggest that there is some risk in this scenario.  Some cases have successfully enforced copyrights in fictional characters or even fictional items such as the Batmobile~\citep{batmobile}, though most plaintiffs only file suit when the generated content is monetized at a larger scale, for example trying to produce a full-length movie in the case of \textit{Axanar}. 
\end{hypothetical}



Given the precedent on fair use in this area, the idealized goal of any mitigation strategy is to ensure that generated content maximizes the capability and utility of the model while minimizing any similarity to copyrighted training data, according to a high-level similarity metric capturing the copyrightable ``heart'' of a piece of content.


\paragraph{Facts, Parodies, and Other Considerations} However, this assessment of similarity is made more complicated by other factors.
Factual content cannot be copyrighted, only expressive content can.
As a result, models that generate news based on factual content, but do not actually keep any creative expression from the original text, provide less legal risk than models that generate creative content from other creative content.  And ideas and common plot structures are not copyrightable.  The mere fact that a foundation model generates text that bears some high-level similarity to the basic plot of a work does not indicate that it has made a copy of that work. It may instead be an indication that those similarities are common across many of the source works on which the model is trained. And where ``copyrightable material is bound up with uncopyrightable material [like factual content], copyright protection is `thin' ''~\citep[at 1198]{oracle}.

Thus, for example, a foundation model trained on all web content to answer factual questions is less likely to pose legal risks if the expressive form of the content is sufficiently novel. This is because facts are not copyrightable. For instance, answering the question "Who is the current president?" would probably be fine, even if trained on copyrighted material, as the fact itself is not copyrightable. However, the line between legality and infringement becomes blurrier when it comes to questions and answers about fictional characters.

\begin{hypothetical}{Tell Me Some Facts}{text}

Consider \textit{The Harry Potter AI Encyclopedia}, a website that hosts a question-answering (QA) model trained to answer anything and everything about Harry Potter, which charges a profit-generating rate. Is this fair use? 

\medskip

In \citet{warnerbrosrdr}, the defendants wanted to create and sell a Harry Potter Lexicon. Judge Patterson considered the creation to be transformative, but the fact that entries in the Encyclopedia contained lengthy verbatim copies of text from the novels, including more "colorful literary device[s]" or "distinctive description[s]" than "reasonably necessary for the purpose of creating a useful and complete reference guide," complicated the issue. As a result, there was a finding that this was \textit{not} fair use. The question of whether or not QA systems like the ``The Harry Potter AI Encyclopedia'' constitute fair use requires a nuanced analysis of the specific circumstances, but as with other analyses will largely weigh on the amount of material taken from the original content.
\end{hypothetical}

Additionally, parodies are frequently considered fair use. But understanding what is a parody in the context of fair use can be semantically complicated.
In \citet{campbellacuff}, the Supreme Court explained that ``Parody needs to mimic an original to make its point, and so has some claim to use the creation of its victim’s (or collective victims’) imagination, whereas satire can stand on its own two feet and so requires justification for the very act of borrowing.'' An illustrative case of this distinction is \citet{juice}. In this case, the defendants published a book called \emph{The Cat NOT in the Hat! A Parody by Dr. Juice}. The derivative book used the rhyme scheme, thematic and narrative elements, and other identifiers of the original book, but it instead described the trial of O.J. Simpson. Despite having parody in the name, the court argued that this was satire, as it commented on other events not in the original material, and ruled that it was not fair use.\footnote{Recall that satire is ``the use of humor, irony, exaggeration, or ridicule to expose and criticize people's stupidity or vices, particularly in the context of contemporary politics and other topical issues.'' And a parody is ``an imitation of the style of a particular writer, artist, or genre with deliberate exaggeration for comic effect.'' \textit{See} \url{https://languages.oup.com/google-dictionary-en/}.} The court argued that parodies \textit{require} copying the source material to some extent to provide commentary on the material itself, whereas satire could use another form to provide the same commentary.  Notably, however, \textit{Dr. Seuss} is viewed as an outlier in the law, and the Supreme Court itself did not hold that satire was unprotected, just that it required greater justification than parody~\citep{ochoa2018dr}. But this illustrates the subtle understanding and nuances required to successfully use a parody argument in the foundation model setting.

\begin{hypothetical}{The Parody Prompt}{text}
Consider an API where users can input their favorite characters and receive a story back.
If the system understands the definition of parody and successfully turns all generated text into parodies, does that improve the chances of fair use?

\medskip

Perhaps, but the system would have to truly understand what a parody is and transform every prompt accordingly.
To get a sense of whether current strong models can accomplish such a task, we provide a few qualitative examples from the original ChatGPT model (original November 15th version), as seen in Figure~\ref{fig:parodies}. In these two qualitative samples, we find that when asked for a parody some samples will look more like a reconstruction of the original plotline than a true parody. As a result, simply instructing all outputs to be parodies would likely not be sufficient with current model capabilities.
\end{hypothetical}

\begin{figure}
    \centering
    \begin{tcolorbox}
\begin{tabular}{p{.45\textwidth}|p{.45\textwidth}}
"Space: the final frontier. These are the voyages of the starship Enterprise. Its five-year mission: to explore strange new worlds, to seek out new life and new civilizations, to boldly go where no man has gone before.

\smallskip

Captain James T. Kirk sat in his chair on the bridge, staring out at the vast expanse of space in front of him. He was a man of action, a born leader, and he was not afraid to take risks.

\smallskip

"Captain, we're receiving a distress call from the planet below," said Mr. Spock, the ship's science officer.

\smallskip

"On screen," said Kirk.

\smallskip

The viewscreen flickered to life, revealing a planet covered in lush green forests and sparkling blue oceans. But something was wrong. The sky was dark and stormy, and the ground was shaking violently.

\smallskip

"Captain, it appears that the planet is experiencing a catastrophic event," said Spock.

\smallskip

"Red alert," said Kirk. "All hands to battle stations. Mr. Sulu, set a course for the planet. Mr. Chekov, prepare to beam down a rescue team. Let's go save some lives."

\smallskip

The Enterprise soared through space, its engines roaring as it raced towards the troubled planet. Kirk and his team beamed down to the surface, ready for anything.

\smallskip

But what they found was not what they expected. The planet was inhabited by a race of giant, humanoid mushrooms, who were being terrorized by a group of evil, space-faring tomatoes.

\smallskip

"Fascinating," said Spock, his eyebrows raised in surprise.

\smallskip

"We can't let these poor mushrooms suffer," said Kirk. "Engage the tomatoes. Fire at will!"

\smallskip

The Enterprise's phasers blazed, cutting down the tomato invaders in a hail of energy. The mushrooms cheered and celebrated, grateful for their salvation.

\smallskip

"Another victory for the Enterprise," said Kirk, a smile on his face. "Onward, to new adventures and new frontiers. Let's boldly go where no man has gone before."
 &   
 Once upon a time, in a galaxy far, far away, there was a great battle between the forces of the Empire and the rebels of the Alliance.

\smallskip

The Empire, led by the evil Emperor Palpatine, was determined to crush the rebels and take over the galaxy. The rebels, led by the brave Jedi Knight Luke Skywalker, fought valiantly to defend their freedom and defeat the Empire.

\smallskip

But things were not going well for the rebels. The Empire's mighty Star Destroyers and TIE fighters were too powerful for the rebels' X-wing fighters and Y-wing bombers. The rebels were on the verge of defeat.

\smallskip

Then, just when all hope seemed lost, a new hero emerged to save the day. It was Han Solo, the notorious smuggler and captain of the Millennium Falcon.

\smallskip

With his quick wit and expert piloting skills, Han Solo led the rebels in a daring attack on the Empire's Death Star. He destroyed the giant space station with a single shot from his trusty blaster, and the Empire was defeated.

\smallskip

The rebels cheered and celebrated their victory, but their celebration was short-lived. For the Emperor had not been killed in the explosion of the Death Star. He was still alive and scheming to rebuild the Empire and defeat the rebels once and for all.

\smallskip

The saga continued, with the rebels facing new challenges and dangers as they fought to keep the Empire at bay. But they were not alone. They had friends and allies in the galaxy, including the wise old Jedi Master Yoda, the fierce Wookiee warrior Chewbacca, and the resourceful droids C-3PO and R2-D2.

\smallskip

Together, the rebels and their allies faced the dark forces of the Empire, and the fate of the galaxy hung in the balance. May the force be with them.
 \\
&\\
\centering \textbf{Generate a parody in the theme of Star Trek.} &   \centering \textbf{Generate a parody in the theme of Star Wars.}
\end{tabular}
\end{tcolorbox}
    \caption{Qualitative examples of asking ChatGPT to generate parodies. Note that the Star Wars example is more of a summary than a parody, indicating that the model does not always understand what a parody is. Thus, to ensure that generated content is truly a parody, and thus more likely to be fair use, more work may need to be done to capture the semantic nature of a parody, something which has not received a significant amount of examination for long-form generative content.
    }
    \label{fig:parodies}
\end{figure}



These many nuances of fair use law for text show the complexity of filtering for fair use content. It is easy to both over- and under-filter content, and simple n-gram / word-level overlap will not fully capture these elements. Even with a similarity metric that accounts for the ``heart'' of a given work, one would need to consider whether the underlying content is factual or a parody. Better alignment with legal notions of transformativeness will help navigate this space.\footnote{We note that while ``AI Alignment'' is a broad term referring to many distinct areas of research. One could consider steering an AI's goals toward designers' intended goals and avoiding adverse outcomes~\citep{yudkowsky2016ai}. AI values alignment might make an agent more in line with certain moral values~\citep{gabriel2020artificial}. In our setting we will refer informally to alignment as something different. It is aligning AI outputs and behavior with legal standards to be more in line with governing legal frameworks. In this particular case, aligning output filters will require more than n-gram overlap to be most in line with fair use doctrine.}


























\subsection{Code}\label{sec:code}
\label{sec:code}
While natural language text and code generation share many commonalities in the way that models are trained, in fair use assessments they have each spawned distinctive case law with slightly varied assessments.
Like in natural language text cases, in software cases, literal infringement (verbatim copying) is unlikely to be fair use when it comprises a large portion of the code base. Several tests exist to try and examine non-literal infringement, such as the Abstraction-Filtration-Comparison test and the Structure, Sequence and Organization (SSO) test~\citep{bloch2022some}. 
These will determine if there was infringement in the first place by isolating the copyrightable, expressive aspects of the code.
This might, for example, include ``inter-modular relationships, parameter lists, and macros.''~\citep[at 702]{altai}. But judges have admitted that ``[t]o be frank, the exact contours of copyright protection for non-literal program structure are not completely clear.”~\citep[at 712]{altai}.
As a result, ``[i]n software copyright cases, it is often quite difficult to prove nonliteral infringement because courts have recognized that many nonliteral elements of programs, such as algorithms, are not within the scope of protection that copyright law provides''~\citep{bloch2022some}. 
Non-expressive, functional, elements are not copyrightable and thus also narrow the scope of liability.
For more discussion on non-expressive fair use,  the interested reader can refer to \citet[at 7-12]{sobel2017artificial}.
And when the amount copied is small, the overall product is sufficiently different from the original one, or the code is sufficiently transformative, then fair use may be indicated under current standards~\citep{asay2017transformative,oracle}.





\begin{experiment}{Reproducing Code Licensed Under GPL}{text}
Many machine learning models of code are trained on data collected from GitHub repositories whose licenses belong to the General Public License (GPL) series.
Therefore, the natural question is whether models could reproduce large chunks of such code, given the restrictiveness of such licenses.
To study this, we simply sample from the Codex models \texttt{text-cushman-001}, \texttt{text-davinci-001}, and \texttt{text-davinci-002} via the OpenAI API, prompting them using randomly chosen function signatures from the Linux kernel repository (licensed under GPL-2.0).\footnote{\url{https://github.com/torvalds/linux}}
To capture inexact matches with large degrees of overlap, we measure the similarity between the reference code (function bodies) and the model generation (samples) with MossPlus~\citep{schleimer2003winnowing}, a program commonly used to detect plagiarism which has been adopted by academic institutions and in copyright and criminal theft cases.
Figure~\ref{fig:similar_code} shows that models can generate function implementations that substantially overlap with reference implementations; Appendix~\ref{app:similar_code} contains selected examples.
In more qualitative experiments, we prompted ChatGPT to regurgitate small portions of GPL-licensed code with only the filename of the licensed file. See Appendix~\ref{app:qualitative_text} for details.
\end{experiment}

\begin{figure}[htb]
\begin{center}
\begin{minipage}[t]{0.42\linewidth}
\centering
{\includegraphics[width=0.9\linewidth]{assets/plagiarize_freq_v2.pdf}} \\
(a) frequency of sampling large matches
\end{minipage}
\begin{minipage}[t]{0.42\linewidth}
\centering
{\includegraphics[width=0.9\textwidth]{assets/plagiarize_avg_top_percentage_v2.pdf}} \\
(b) average match \% of large matches
\end{minipage}
\end{center}
\caption{
Codex models can produce function implementations that substantially overlap with reference implementations when prompted with function signatures (each function signature is one line of code).
\textbf{Left:} The frequency of producing a large match is below $1\%$ but nonzero for all three models.
\textbf{Right:} The average match percentages of the large match samples for each model is beyond $45$\%.
Match percentages are reported by MossPlus which can capture non-exact matches but occasionally reports false positives. 
Here, we mark a sample as a large match to the reference if its overlap with the reference exceeds $20\%$ as reported by MossPlus.
The $20\%$ threshold is chosen inspired by common values used in plagiarism detection for flagging submissions
for further manual inspection~\citep{mason2019collaboration}.
}
\label{fig:similar_code}
\end{figure}


It is important to note that copyright protection for code is more limited compared to that for creative works such as text or music \citep[558-560]{samuelson2017saving}. Functional aspects of code are not protected by copyright, meaning that copying larger segments of code verbatim might be allowed in cases where the same level of similarity would not be permissible for text or music. Nonetheless, for software generated by foundation models, the more the generated content can be transformed from the original structure, sequence, and organization, the better. Due to the blurry line between ideas and expression in code, preventing large-scale verbatim copying and encouraging transformations at every scale of the code will significantly reduce infringement risk.

Provided that the non-transformative generated code is short and used for an overall transformative purpose, like \textit{Google v. Oracle}, traditional copyright claims are less likely to succeed. As models scale from generating small snippets of code to generating entire codebases that are not transformative, risks may increase and more investment in mitigation strategies will help reduce the risk of litigation.


Other concerns beyond infringement have have been raised for code-generation models. For example, some have expressed concerns that code generation products can output their usernames verbatim in generated code.\footnote{\url{https://twitter.com/kevin\_zakka/status/1494482506824835078}}
While short usernames may not necessarily be copyrightable, there may be questions surrounding the right of publicity in such cases.
The right of publicity gives people economic rights to their identity. So, for example, video game companies cannot simply use athletes' likenesses without compensating them. \textit{See, e.g., } \citet{davisea,hartea}. The right of publicity does not explicitly have a fair use doctrine, but courts have read the First Amendment to protect transformative works.\footnote{Scholars have argued that the fair use doctrine applied to the right of publicity can lead to arbitrary results. See \citet{dougherty2003all} and \citet{volokh2003freedom}, as well as \citet[p. 2815]{weisbord2015copyright}, describing this debate.} Similarly, DMCA \S 1202 claims (which we will discuss in \S \ref{sec:considerations}) are another potential concerns. These considerations, however, are not specific to code and would be applicable to other forms of media as well.







\subsection{Generated Images}
\label{sec:images}
The third commonly produced category of generative AI is image generation.

\paragraph{Complexities of fair use with images.} As with code or text data, it is unlikely that verbatim generation of images would yield a successful fair use defense. And others have found that it is possible in some circumstances to extract training data from image generation foundation models~\citep{somepalli2022diffusion,https://doi.org/10.48550/arxiv.2301.13188}.
As \citet{somepalli2022diffusion} and others note, however, as foundation models for image generation train on more data, they are less likely to output content similar to the training data on average.
These cases are more likely to be fair use.

But generated images, and generated art in particular, have their own complexities when it comes to fair use, with sometimes conflicting outcomes. For example, in a recent case, a video game company used the likeness of a WWE wrestler in a video game. The wrestler had tattoos that the company faithfully replicated in the game. The tattoo artist sued for infringement and a jury determined that this was not covered by fair use~\citep{take21}. A similar case involving tattoos on athletes in video-games against the same company came out the exact opposite way~\citep{take22}. The split decision in such cases demonstrates the evolving and stochastic nature of fair use determinations. This means that it is possible for small portions of an image, like the tattoo on a player's arm, to trigger copyright problems that are not guaranteed a fair use defense. Consider the following hypothetical.

\begin{hypothetical}{Generate Me Video-Game Assets.}{text}
One direction for generative art is creating video game assets. There are already mechanisms to generate 3D models from text~\citep{poole2022dreamfusion}. Consider a situation where a video game company builds a machine learning model into their system that generates art on the fly within the game to populate a virtual world dynamically. The game is a hit, but artists begin to notice that their artwork shows up in the game with only slight modifications, for example on tattoos for video game characters. Is this fair use? While their lawsuit is not guaranteed to succeed, there is still some risk for the video game company if the outcome follows \citet{take21}.
\end{hypothetical}

\paragraph{Style Transfer.} What about more abstract scenarios, where art is generated in different styles? There are two components to this. First, let us consider the rights of the original image that is being transformed into a different style. Relevant is a case that was recently argued before the Supreme Court for clarification. In the case of \citet{warhol}, Andy Warhol created silkscreen works that depicted the musician Prince. These silkscreens were based on Lynn Goldsmith's photograph of Prince. The silkscreen work evinced the ``distinct aesthetic sensibility that many would immediately associate with Warhol's signature style — the elements of which are absent from the Goldsmith photo''~\citep{warhol}. Nonetheless, the Court of Appeals ruled that this was not fair use. The court emphasized that the derivative art must have a ```fundamentally different and new' artistic purpose and character, such that the secondary work stands apart from the `raw material' used to create it.'' The court noted that ``the secondary work's transformative purpose and character must, at a bare minimum, comprise something more than the imposition of another artist's style on the primary work such that the secondary work remains both recognizably deriving from, and retaining the essential elements of, its source material.''

This analysis immediately calls to mind a consistent prompt used for foundation models for generative art: ``Draw [Image X] in the style of [Artist Y].'' It is not yet clear how the Supreme Court will rule on this case, but its outcome will likely directly impact the scenario of style transfer in generative images. If the Supreme Court rules that Andy Warhol's painting was fair use, then style transfer is more likely to be fair use. If, however, the court rules that this is not fair use it is less likely that style transfer will be fair use without significant transformation. There are nuances here, however. If the user provides the original image to be style-transfered, the model deployer may be less liable (since this behaves more like a photo editing software). If the model deployer only takes text and it generates copyrighted Image X in a different style, then the model deployer is more akin to Andy Warhol, rather than a photo editing software.

Second, one might consider the rights of the artist whose style is being mimicked. An artist's general style, however, is not copyrightable and courts have not readily afforded style appropriation much protection when the underlying depicted subject matter is different~\citep{brownlee1993safeguarding}. While there is some nuance, prompting generative models to illustrate something in someone's art style is unlikely to create liability unless distinctive components of their art are re-used. For example, a prompt like ``Campbell's Soup Cans by Andy Warhol in the Style of Picasso'' might be more risky if it recreates the original Warhol piece too closely. But a more generic style-transfer prompt like, ``A random dog in the style of Andy Warhol'' is more likely to be fair use (assuming, again, that the output itself is sufficiently transformative from Andy Warhol's works).

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.48\textwidth]{figures/ents.pdf}
    \hfill \includegraphics[width=.48\textwidth]{figures/enttypes.pdf}
    \caption{The entity types and the most frequently cited entities in the Krea AI OpenPrompts corpus.}
    \label{fig:analysis}
\end{figure}

\begin{experiment}{Campbell's Soup Cans by Andy Warhol in the Style of Picasso.}{text}
How users formulate prompts can give some insights into typical uses and associated intellectual property risks. For example, if users ask to generate mundane, generic images, in particular art styles, this might be less risky than if users try to generate specific copyrighted works of art. 
We analyze a dataset of 10M prompts posted to the Stable Diffusion Discord channel by members of the community to better understand prompting patterns of users.\footnote{\url{https://github.com/krea-ai/open-prompts}} We use named entity recognition as a proxy for understanding typical use cases.
As seen in Figure~\ref{fig:analysis}, we find that the most common named entity type used in prompts are people's names, including the names of artists like Greg Rutokowski, who is referenced 1.2M times. This suggests that users in this community often try to generate images in particular artist styles, which is more likely to be fair use as long as the content itself is sufficiently transformative.
However, there are other queries which specifically look to generate known works of art, which would tend towards more risk of losing a fair use defense if the model complies. 
As seen in Appendix~\ref{fig:woa_analysis}, many of the most commonly referenced works of art (as defined by the named entity recognition system) tend to be large franchises such as Star Wars, Cyberpunk 2077, Game of Thrones, etc.
Feeling helpless against the use of their own art in generative foundation models, artists have sometimes explicitly tried to generate images from such franchises in the hopes that companies like Disney file suit against the model hosts~\citep{mickeywars}.
If the final artwork too closely matches the works of art from these artists, resulting litigation might reflect the current litigation against the Andy Warhol estate.  But merely producing different art in the same style is less likely to be sufficient for liability, as noted above. And if users provide an input image to the model of copyright material, this might shift liability onto the user since the model acts more like a style transfer system than a system generating copyrighted material.
\end{experiment}

Finally, we note that there may be other intellectual property considerations with images as well, including the right to publicity and trademark infringement. Recently, litigation was filed by Getty Images which included trademark infringement claims since generated images also occasionally added a Getty Images watermark.\footnote{\url{https://copyrightlately.com/pdfviewer/getty-images-v-stability-ai-complaint/}}




\section{Additional Considerations}
\label{sec:considerations}
We also consider several additional points that are adjacent to fair use but merit mention.

\textbf{Licensing and Attribution.} Since licenses will determine who has permissions to use what data---and foundation models themselves---in this section we briefly discuss licensing issues that might be relevant. In all cases, if no license is found to apply some foundation model uses will fall back to fair use arguments described throughout the rest of this work.

\noindent \textit{Attribution Licenses and Creative Commons.} The details of licenses for the underlying training data can create challenges for all parties in the model pipeline. For example, \citet{stewart2021rise} described a scenario where photographers released their images under an open license that requires source attribution. Websites using the photos did not properly provide attribution, and the photographers sued for infringement. Courts in these cases must fall back to the fair use analysis. In the context of foundation models, this suggests that relying on attribution-based permissive licenses does not generally solve copyright issues, as foundation models rarely, if ever, provide proper attribution---though we discuss the use of instance attribution methods as one potential mitigation strategy in \S \ref{sec:instance}.
Indeed, in many cases, it can be difficult to determine which training examples actually contributed to a given generation. Somewhat ironically, even if model creators and hosts rely on open-source content to train their models, they may nonetheless have to rely on fair use if they cannot or do not endeavor to attribute credit properly, and they may even face the risk of contract liability or DMCA \S 1202 claims regardless of fair use.

\textit{Implied Licenses and Common Crawl.} On the other hand, many creators voluntarily post their works on the internet with permissions for web crawling. It is well-established that merely posting something on the internet does not waive the intellectual property interest in the work, but many data creators use an industry-standard ``robots.txt" file to affirmatively to include their website and data in caches and search indexes.
In \citet{fields} a district court held that Google could cache web content that did not disallow scraping via robots.txt, suggesting that there was an implied license and thus the use was not infringement.
This license only extended to caching in that case, which does not necessarily reflect the uses of foundation models we discuss throughout this work, so it is unlikely to cover all the use cases we describe here.
And the bounds of the uses covered by the robots.txt file are untested in court.\footnote{Though in another subsequent litigation one other district court was assessing whether the same implied licensing argument extended to RSS feeds and the court noted that. ``It is not clear to the court at this time that [an RSS feed and a search engine] are functionally equivalent as far as the relevant legal doctrine is concerned. Because this court 
lacks the required technical expertise to resolve that question, the  court cannot rule, as a matter of law, that the defendant is not liable at this juncture''~\citep[at 2]{righthaven}. And in \citet{meltwater} the court found that there was no implied license for building a news aggregator that excerpted and republished clips of news articles. But in this case the court relied, in part, on the fact that the plaintiff did not implement the robots.txt protocol so could not have opted in to crawling at all.
}
While the issue of whether the implied license extends to foundation model training has not been resolved in litigation, it is possible that an outcome like \citet{fields} would extend to \textit{some} foundation model uses---in particular, for building a cached dataset and training a model.

It is worth noting that the use of a robots.txt header or other opt-out mechanism has implications for fair use also. Datasets and models like C4~\citep{2019t5} and LAION-400M~\citep{schuhmann2021laion}, rely on CommonCrawl data which is crawled only if users explicitly allow it through their robots.txt file.
CommonCrawl is able to host a snapshot of the internet largely because of fair use arguments. As the organization's director argues, there is a transformation into a different---not easily human-readable---format, the organization does not take a snapshot of entire webpages, and the use itself is transformative (from actively presenting content to caching content) and for the public benefit~\citep{commoncrawlforbes}.
In \citet{fields}, respect for the robots.txt file also was considered in the fair use assessment with the court noting that Google in good faith followed industry standards that would prevent caching (respecting disallowing crawling via a robots.txt).
It is possible, then, that providing an opt-out mechanism for data creators and respecting the robots.txt opt-out mechanism will be taken into account in assessing a fair use argument, as it was in \citet{fields}.\footnote{Note, however, that there are structural critiques of opt-out mechanisms beyond the current state of the law as noted by \citet{kapoornaryanan}.}
 
\noindent \textit{Licensing Foundation Models.} Recently, some open-source model creators 
have attempted to shift liability via the licensing mechanism by including a clause that says ``Sharing of copyrighted or licensed material in violation of its terms of use'' and ``Sharing content that is an alteration of copyrighted or licensed material in violation of its terms of use''~\citep{rombach2021highresolution,openrail}. 
It is unlikely that this will significantly change the liability of model creators and deployers. The mere announcement of a party's beliefs does not change the rights and obligations under copyright law. In a different context, the court in \citet{limewire} examined whether it was sufficient for Limewire to force the user to check a box saying, ``I will not use LimeWire for copyright infringement.'' Without additional mitigation measures, the court found that the agreement mechanism did not constitute a meaningful effort to mitigate infringement.
What such a license \textit{does} provide, however, is the ability to revoke the right of a model user or deployer to use the model. However, this would require the resources to legally enforce this license provision and pursue action to force a party to stop using the model.

Some have argued that a patchwork of \textit{non-commercial} releases and licensing structures can reduce liability for model creators who later seek to commercialized some aspects of the work~\citep{laundering}. While it is true that the non-commercial nature of a work will be taken into account in a fair use assessment, it does not automatically prevent successful litigation~\citep{beebe2020empirical}.

\noindent \textit{Removing licensing information.} Even if model creators rely on data under open-source licenses, there may be other issues that arise from removing licensing information. DMCA \S 1202 creates liability if someone intentionally removes copyright management information (CMI) or knowingly distributes content with the CMI removed. Fair use is not typically a defense for this form of liability, though some have noted that there is still room for an interpretation that includes fair use protections here~\citep{lim2010survey}. In a scenario where courts rule that fair use does not apply to \S 1202 claims, how would model creators comply with its requirements? Overall, it is unclear and current litigation, namely \citet{githublitigation}, is actively resolving such claims and will help shape the direction of this analysis. It is worth pointing out three difficulties in complying with and analyzing DMCA \S 1202 claims, however.

First, it is unclear what form factor of foundation models would comply with DMCA \S 1202 in its strictest form, even if a license is otherwise permissive. Courts have noted that the CMI must be transmitted with the work~\citep{jacobs2012gutters}, for example. Would this mean that all generative outputs need to append license information for samples that most contributed to that output? It may be tempting to have a catch-all page that points to all the training data and licenses, but it is not clear whether this would qualify as being transmitted with the work. 

Second, in some cases courts have dismissed DMCA \S 1202 claims when the distributed content is not identical. \textit{See, e.g.,} \citet{dmca1202identical} which dismissed a \S 1202 argument where ring engravings were similar noting that ``while the works may be substantially similar, Defendant did not make identical copies of Plaintiff’s works and then remove engraved CMI.'' The opinion in this case also pointed to other cases with similar holdings. For example, in \citet{kelly} plaintiff used thumbnail images without CMI, but the court found that this did not violate the DMCA the plaintiff's original, full-sized images retained the CMI.
Whether this would hold in other cases is unclear. If it did, it may mean that if foundation models generated only transformative content then \S 1202 claims would be less likely to succeed.

Third, DMCA \S 1202 contains an intent requirement. To satisfy this requirement, one court has required that the plaintiff show: ``(1) the existence of CMI in connection with a copyrighted work; and (2) that a defendant `distribute[d] ... works [or] copies of works'; (3) while `knowing that [CMI] has been removed or altered without authority of the copyright owner or the law'; and (4) while `knowing, or ... having reasonable grounds to know' that such distribution `will induce, enable, facilitate, or conceal an infringement.' ''~\citep{2020mango}. The intent requirements of \S 1202(b) do not map easily to the automated generation of content.\footnote{Though that is a more general issue with the application of fault-based legal doctrines to AI~\citep{lemley2019remedies}.}

Again, how DMCA \S 1202 will play out in litigation like \citet{githublitigation} will determine what mitigation strategies are necessary to pursue. In the interim, strategies like instance attribution could help meet some of the \S 1202 requirements even in their strictest form.



\textbf{Allocating Liability.} It may also not always be clear who is liable for an infringement. Is Adobe liable for every user that alters an image using their software and posts it as an infringing artwork? Likely not. 
Liability may shift depending on what parties engage in what conduct. We will briefly describe liability in order of the model use pipeline (model creation, model deployment, and then model use). The brunt of this assessment will be highly contextual, but generally any liability will stem from the production of some non-transformative samples from a model that are not covered by fair use. Much of what we describe here is not resolved in the law, so we aim to describe different potential outcomes as opposed to offering definitive answers.

\textit{User.} The user of a model (someone who queries a model and uses its output), is likely liable for their own use of the model outputs. If the model provides non-transformative material and the model user commercializes the output they will undergo the same assessment as in any other case.

\textit{Deployer.} The model deployer may also face liability for distributing content not defensible by fair use. As in the various hypothetical scenarios we discuss throughout this work, if the model outputs non-transformative content and the deployer distributes this content on the web for profit, it is functionally no different than providing a website with access to that non-transformative content, incurring the same liability and the same analysis. If the model deployer adds filters and safeguards to prevent the user from generating content not covered by fair use, they will reduce their liability, not merely by reducing the likelihood of infringement but by making a product that is not designed to facilitate infringement. Consider the earlier case of Google Books. In theory, a user might be able to reconstruct an entire book from the Google Books snippets by bypassing Google's restrictions through the use of proxy servers. Although it was in principle possible to bypass the mitigations, Google Books was generally considered fair use because it was not designed or specially adapted for the purpose of facilitating that infringement. Similarly, if a model deployer puts in place a number of safeguards, but a determined user bypasses them, the deployer will have more ability to defend themselves. This can be seen in, for example, \citet{limewire}, \citet{groksterremend}, and \citet{aimster}. In these cases, various file-sharing services were being sued for inducement of infringement, a secondary liability. The court took into account whether the services provided meaningful mitigation measures preventing infringing material from being distributed.

If the model user is the one that \textit{uploads} copyrighted content and the model transforms this content (e.g., adding a filter) before giving it back to the user, liability is more likely to rest with the user. This is more like photo editing software or text editing software. A potential example of this is style transfer for diffusion models. If the user uploads a copyrighted work and asks the model to transform it into a different style, it is more likely that the liability falls to the user who uploaded the image if they later try to resell it. This is like Andy Warhol taking a photograph and transforming it into a piece of art. However, if the user simply asks the website to generate an image from a prompt, and the model generates a copyrighted image, then the liability might fall more on the model deployer if they are profiting off of the distribution of the material. The extent to which the model takes the input image and turns it into something non-transformative might also be taken into account. For example, if the model takes a novel image of a dog and transforms it into a known copyrighted image of a dog, this might create more liablity for the model deployer.

\textit{Creator.} Throughout this work we will not generally cover liablity to the model creator for the model weights themselves. It is unresolved as to whether the model parameters themselves are infringing, and thus whether the model trainers are liable. \citet{lemley2020fair} have argued that the training process is generally fair use since the model weights themselves are transformative, and thus generally liability would not pass on to model creators.
\citet{sobel2017artificial} argued that if the training process does not result in an expressive model, training is fair use.
Others have argued that the model functions as a compressed database of the training data~\citep{stablediffusionlitigation}, thus making the model creators liable. However, this is not necessarily true of all the training data and the likelihood of verbatim (or significantly similar) extraction can reduce with the amount of training data. For example, \citet{somepalli2022diffusion} found that extraction of training data from diffusion models is less likely if there is more diverse training data.

When the model is capable of outputting both transformative and non-transformative content, it is also unresolved how the model itself (and model creators) should be treated as a function of secondary liability. The extraction of non-transformative content, according to our experiments and others, is often not straightforward. It requires effort on the part of model users and deployers to identify an extraction mechanism. If this is the case, one might instead argue that remedies should be limited to specific instances of extracted non-transformative content, not the model as a whole, which does not generate infringing output in the ordinary case. The model creator might also be insulated from liability on other fair use factors. For example, if they released the model under a non-commercial license and actively prevented its use for commercial purposes, they might argue that the nature of their model was non-commercial, increasing the likelihood of a fair use defense at this part of the liability chain.\footnote{Though, again, some have argued that this process has been abused~\citep{laundering} and it is not assured.} As with other issues in this work all of this is actively being litigated and will be shaped over the coming years.





\textbf{DMCA Safe Harbor.} 
The Digital Millennium Copyright Act (``DMCA'') is a U.S. law created to address digital copyright issues that came about with the advancement of technology. The DMCA safe harbor provisions protect online service providers from legal responsibility for copyright infringement claims.
DMCA protections might vary depending on a number of considerations, but we emphasize that they are not guaranteed for all model deployments. We examine several of these considerations here.

\textit{DMCA protections for generative foundation models are uncertain.} At first glance, it may seem like the Digital Millennium Copyright Act (DMCA) would protect machine learning model hosts. Like in other hosted sites, they would need to meet the relevant requirements like using a registered agent under DMCA \S 512(c)(2). Then they could put up a take-down request form and add filters for the offending model output when served with a take-down request under the DMCA \S 512(c) safe harbor.\footnote{Though, as we will discuss in \S \ref{sec:instance}, detecting and taking down this content in generative models can be particularly difficult.}  An internet company that has a notice-and-takedown scheme in place is not liable for hosting infringing content posted by a third party.

But it is not obvious that the DMCA safe harbors apply to \emph{generated} content.
For example, in 2019, Amazon lost on a motion to dismiss when its algorithms selected copyrighted material to host on the website~\citep[Order on Motion to Dismiss]{williamsanoma}. The court was unconvinced that Amazon was eligible for safe harbor under the DMCA. They stated that to establish safe harbor the content must be stored ``at the direction of the user”~\citep[at 1052]{mavrix}. 
This \emph{may} mean that generated content does not have the same safe harbor and that post-hoc take-downs are not sufficient to reduce liability.\footnote{This, however, is quite uncertain. The court's decision is non-binding as it is a district court decision.}
As such, filtering of generated content before a takedown request is ever received may be \emph{more important} while the courts determine the applicability of DMCA to generated content.

\textit{It might matter where the data comes from.} This also implies that DMCA protections may vary on who the model host and creator are. For example, a website hosting models uploaded by users might find it easier to argue for DMCA protection because the website itself is not creating or selecting the content (assuming that it follows other requirements like using a registered agent under DMCA \S 512(c)(2)).
On the other hand, if a company were to create and host a model that itself selects content provided by others, like Amazon did in \emph{Williams Sonoma v. Amazon}, it is unclear whether courts would agree that DMCA protections would apply.

Another unclear variation on DMCA eligibility rests on the source of the training data: is the data user-contributed or creator-contributed?
This might mean that, for example, DMCA safe harbors might be more likely to apply when a model is refined via Reinforcement Learning from Human Feedback (RLHF)~\citep{ouyang2022training} based on user ratings or updated automatically via user-generated data.
These modes of method training and deployment are more akin to users uploading content to YouTube, and might help with arguments \textit{for} DMCA protections.
Things might become more murky if, for example, an RLHF model is trained on user data but this data is modified by creator-hired annotators.
If model creators themselves curate and scrape data, then host the model themselves, this might be more akin to the \textit{Williams Sonoma} case (which, again, is not a settled rule or a binding decision, but shows the variation of outcomes that is possible in the current state of the law).

\textit{It is unclear what's the best mechanism for DMCA takedowns with generative models.} Even if it applies, it is unclear how the DMCA notice-and-takedown scheme would work as applied to foundation models, but the most likely ``take down'' approach might actually look more like output filtering with a safe harbor. As we will discuss in \S \ref{sec:instance}, instance unlearning is a nascent research area and retraining a model without a taken down datapoint could be exceedingly costly.
The most likely approach in the near term is to ``take down" model outputs that were too similar to a copyrighted work via a filtering mechanism since foundation models generate content on demand for users rather than hosting persistent content that can easily be taken down. But new research is needed to identify new and improved mechanisms for handling takedown requests in this relatively new setting.

\textit{The Copyright Office has a DMCA anti-circumvention exemption for text and data mining.} Finally, the Copyright Office also provides an exemption to DMCA's anti-circumvention requirements in \S 1201 in the case of non-commercial data mining.\footnote{37 CFR 201. \textit{See also} previous work on exemptions by \citet{sag2018new} and \citet{carroll2019copyright}.}
This may allow non-commercial researchers to, say, remove digital rights management software to train on video or text content.\footnote{We provide a note of caution, however, as this does not mean that researchers can necessarily bypass restrictions on scraping or violations of terms of use, which can carry other penalties unrelated to copyright law.}

Overall, DMCA protections are far from guaranteed, so model creators and deployers cannot rely on its safe harbor provisions to reduce liability. Instead, they must take a more proactive approach. Moreover, some previously-discussed provisions add to potential liabilities like \S 1202, creating additional compliance challenges.

\textbf{Sovereign Immunity.} 
State universities might be immune to the sort of copyright liabilities we describe here. As a result, a hypothetical state university hosting a foundation model, even one that regurgitates verbatim content, might test the boundaries of sovereign immunity jurisprudence. After the Supreme Court's ruling in \citet{2020allen}, it could potentially mean that state universities could train and host foundation models on copyrighted data without taking any mitigation strategies and nonetheless would not suffer monetary damages. We note, though, that there is much more nuance here. In particular, this does not immunize the university from, for example, contractual claims. And injunctive relief, where the university is ordered to cease the infringing conduct but does not face monetary damages, still remains a potential remedy in federal court. \citet{perlmutter2021copyright}, the Register of Copyrights and Director
U.S. Copyright Office, discusses state sovereign immunity after \textit{Allen} in more depth. In particular, they found that the rates of infringement by state actors after the \textit{Allen} decision were higher than expected and has asked Congress to take action to change this status quo.







\textbf{Good faith.} Judges occasionally consider whether the use was undertaken in good faith, for better or for worse.
For example, in \citet{fields} the court took into account ``Google's good faith in operating its system cache'' in assessing fair use: following industry standards for opting out.\footnote{\textit{See also}
\citet{harper1982} (``Also relevant to the character of the use is the propriety of the defendant’s conduct. Fair use presupposes good faith and fair dealing.'') (cleaned up); discussion by \citet[at 954-57]{carroll2019copyright}.}
Though untested, it is possible that judges may take into account the use of technical mitigation strategies as good faith efforts to stay within the bounds of fair use. Conversely a lack of any technical mitigation strategy might also be negatively considered.
We note, however, that fair use itself does not turn on good faith in general and the Supreme Court has cast doubt on whether good faith should be involved in the fair use assessment. \textit{See, e.g., } discussion by \citet[at 281-84]{myers2021muddy}.


\paragraph{Non-U.S. Perspectives.}  We take a fair use-oriented approach, focusing on U.S. law, as this is the most likely to be permissive of using copyrighted content. Fair use, or its equivalents, will look quite different across countries and outcomes will differ. 
\citet{mccann2021copyright} suggests that Canadian law might follow a similar approach to what we describe here, where generative models might have to follow Canada's \textit{fair dealing} doctrine. McCann also suggests that under Canadian law model parameters might not be copyrightable at all.
Israel's Ministry of Justice issued an opinion stating that training machine learning models is likely to be fair use according to Israeli law with similar caveats to U.S. fair use law.\footnote{\url{https://www.gov.il/BlobFolder/legalinfo/machine-learning/he/machine-learning.pdf}} In particular the opinion notes that the breadth of the training data matters---so training on one book is less likely to be fair use than training on all books. For generative models it also considers the target market and what the outputs are.\footnote{See, e.g., \citet{elkin2020transplanting} for a more general comparison of fair use law in the United States and Israel.}
Other countries may not have fair use standards at all or have standards that would create difficulties for training foundation models, let alone deploying them.
For this reason, some governments have explicitly provided exemptions for training models on copyrighted data, though often only for non-commercial uses~\citep{JapanCopyright,eucopyright,ukipo}.
Others have tried to require certain mechanisms, like content filters, to prevent infringement in content uploaded to websites~\citep{eufilterseff}.


\paragraph{Ethical and non-legal perspectives.} Our work seeks to illuminate the potential legal risks of generative foundation models and to argue that we need more research and work to bring foundation models more in line with the status quo of fair use doctrine---particularly given the many uncertainties of fair use doctrine as applied to foundation models. But legality does not necessarily imply alignment with some ethical frameworks.

Others have noted that U.S. copyright law---and fair use in particular---is not always aligned with non-utilitarian perspectives, like moral rights~\citep{ciolino1997rethinking}. For example, stakeholders like artists and authors may argue that they have a moral right to make sure their work is not used to train AI systems, even if it is permissible from a utilitarian fair use perspective.
Some argue that this disconnect may overpower a group's control over their cultural heritage. For example, \citet{reed2021fair} ``evaluates fair use as a gatekeeping mechanism for unauthorized uses of copyrighted culture, one which empowers courts to sanction or disapprove of cultural appropriations to further copyright’s goal of promoting creative production.'' \citet{maori} frames this as an extension of colonization.
All of these considerations fundamentally can come into conflict with existing principles of fair use and case law in the United States.\footnote{Though we note that \citet{bair2017rational} argued there is less of a disconnect than typically perceived between moral rights and fair use in some cases.}

Even foundation models that transform content into creative new innovations
without mimicking any particular style or training data point could have massive impacts on labor.
This is why many have pointed out that this dilemma of how to treat foundation models fundamentally requires thinking more deeply about the underlying goals of copyright law and fair use~\citep{grimmelmann2015copyright,sobel2017artificial,lemley2020fair}.
It is possible that some strategies could be pursued that would compensate data creators even when model training meets existing fair use standards, but these should be handled with care to avoid an alternative outcome that aggregates power in other desirable ways. For example, forcing licensing mechanisms or opt-in approaches for all data could consolidate power in those companies that already have licenses to enormous amounts of data, like YouTube or Facebook. Or they could create powerful intermediaries that aggregate data licenses without actually sufficiently compensating data creators.\footnote{This has been discussed in many other contexts. For example, \citet{reichman1999database} pointed out over twenty years ago how a push to form aggregated databases risked of wrapping up databases in licensing schemes that prevented important research and innovation.}
Identifying new policy mechanisms to balance all of these considerations and interests is vital, but beyond the scope of this work.

