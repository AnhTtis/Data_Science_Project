
\documentclass{article} %

\usepackage{iclr2023_conference,times}

\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{xurl}
\usepackage[authoryear]{natbib}

\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{xcolor}         %
\usepackage{graphicx}
\usepackage{multirow}

\usepackage{wrapfig}
\usepackage[many]{tcolorbox}

\usepackage{listings}
\usepackage[scaled=0.85]{FiraMono}

\usepackage{color}
\definecolor{myfavblue}{rgb}{0.05, 0.2, 0.8}
\definecolor{keywords}{RGB}{255,0,90}
\definecolor{comments}{RGB}{0,0,113}
\definecolor{red}{RGB}{160,0,0}
\definecolor{green}{RGB}{0,150,0}
\definecolor{C0}{rgb}{0.12156862745098039, 0.4666666666666667, 0.7058823529411765}  %

\definecolor{myblue}{HTML}{3182bd}
\definecolor{myred}{HTML}{de2d26}

\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\hypersetup{
    colorlinks=true,
    linkcolor=mydarkblue,
    citecolor=mydarkblue,
    filecolor=mydarkblue,
    urlcolor=mydarkblue
}

\def\startorange{\begingroup\color{orange}}
\def\endorange{\endgroup}

\def\startred{\begingroup\color{red}}
\def\endred{\endgroup}

\def\startblue{\begingroup\color{blue}}
\def\endblue{\endgroup}

\def\startgreen{\begingroup\color{green}}
\def\endgreen{\endgroup}

\lstset{basicstyle=\ttfamily, keywordstyle=\bfseries}
\lstset{numbers=left,escapeinside={(*@}{@*)}}

\def\setallcolors{%
\gdef\mybasiccolor{\color{black}}%
\gdef\mykwcolor{\color{blue}}%
\gdef\myndkwcolor{\color{darkgray}}%
\gdef\myidcolor{\color{black}}%
\gdef\mycommentcolor{\color{purple}}%
\gdef\mystringcolor{\color{red}}%
}
\def\setallgray{%
\gdef\mybasiccolor{\color{gray}}%
\gdef\mykwcolor{\color{gray}}%
\gdef\myndkwcolor{\color{gray}}%
\gdef\myidcolor{\color{gray}}%
\gdef\mycommentcolor{\color{gray}}%
\gdef\mystringcolor{\color{gray}}%
}
\def\setallred{%
\gdef\mybasiccolor{\color{red}}%
\gdef\mykwcolor{\color{red}}%
\gdef\myndkwcolor{\color{red}}%
\gdef\myidcolor{\color{red}}%
\gdef\mycommentcolor{\color{red}}%
\gdef\mystringcolor{\color{red}}%
}
\def\setallgreen{%
\gdef\mybasiccolor{\color{green}}%
\gdef\mykwcolor{\color{green}}%
\gdef\myndkwcolor{\color{green}}%
\gdef\myidcolor{\color{green}}%
\gdef\mycommentcolor{\color{green}}%
\gdef\mystringcolor{\color{green}}%
}

\newcommand{\pl}[1]{\textcolor{red}{[PL: #1]}}
\newcommand{\ph}[1]{\textcolor{red}{[PH: #1]}}



\newtcbtheorem[number within=section]{experiment}{Experiment}{
  breakable,
  colback=green!5,
  colframe=green!35!black,
  fonttitle=\bfseries}{x}

\newtcbtheorem[number within=section]{hypothetical}{Hypothetical}{
  breakable,
  colback=blue!5,
  colframe=blue!35!black,
  fonttitle=\bfseries}{x}

\newcommand*{\Chen}[1]{\textcolor{blue}{[Chen: #1]}}


\title{Foundation Models and Fair Use}

\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

\author{
Peter Henderson\thanks{Equal Contribution. Correspondence to
\href{mailto:phend@cs.stanford.edu}{phend@cs.stanford.edu}, 
\href{mailto:lxuechen@cs.stanford.edu}{lxuechen@cs.stanford.edu}. \copyright \hspace{0em} Peter Henderson, Xuechen Li, Dan Jurafsky, Tatsunori Hashimoto, Mark A. Lemley, \& Percy Liang}, Xuechen Li\samethanks, \textbf{Dan Jurafsky, Tatsunori Hashimoto, Mark A. Lemley, Percy Liang}\\
Stanford University
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\usepackage{array}
\usepackage{lipsum}

\begin{document}


\maketitle


\input{sections/abstract}
\input{sections/introduction}



\section{Foundation Models and Fair Use}

We first briefly define foundation models and introduce fair use law as well as its applicability to foundation models. 
To provide a better understanding of the risks, we then examine concrete precedential cases related to fair use and how they might apply to foundation models.
We conduct this analysis for cases related to text, code, and visual art. 
To accompany our examination of U.S. case law, we include hypothetical scenarios of model deployments and how they might exceed the bounds of the fair use doctrine under current law. We also provide experiments to show that current foundation models are capable of generating content that is not transformative.


This section proceeds as follows. Section~\ref{sec:fmintro} provides a brief overview of foundation models. 
Section~\ref{sec:definitions} provides definitions of actors involved in the foundation model development and deployment process and what roles they play. Section~\ref{sec:fair} provides a high-level overview of fair use doctrine in the United States. Sections~\ref{sec:text}, \ref{sec:code}, and \ref{sec:images} provide in-depth examples of case law and foundation model scenarios to help elucidate potential risks.  

\subsection{Foundation Models}
\label{sec:fmintro}
Foundation models are machine learning models trained on broad data (typically scraped from the internet) generally using self-supervision at scale~\citep{bommasani2021opportunities}. 
Most foundation models are not trained to accomplish specific tasks but rather to capture useful general information in the data.
For instance, most autoregressively pretrained language models (e.g., GPT-3~\citep{brown2020language}, PaLM~\citep{chowdhery2022palm}, or Chinchilla~\citep{hoffmannempirical}) are trained to predict the next word given a sequence. 
Most text-to-image models, for example DALL·E~\citep{ramesh2021zero}, are trained to capture the distribution of images given a text prompt. 
These models can then be tuned to align more with human preferences~\citep{ouyang2022training} or be adapted for specific tasks.
Foundation models can be used for generating content. This includes models like GPT-3~\citep{brown2020language} for text, Codex~\citep{chen2021evaluating} for code, and DALL·E~\citep{ramesh2021zero} for images.
Alternatively, they can be used for \textit{non-generative} purposes. These would typically output one value, rather than having a longer free-form output. For example, they might classify text in different ways, or predict a numerical value from an image. This includes (for the most part) models like BERT~\citep{devlin2018bert} or CLIP~\citep{radford2021learning}.
Importantly, most foundation models can be modified to operate for either type of task, and many tasks will be somewhere on the spectrum between generative and non-generative tasks.\footnote{This spectrum between generative and non-generative tasks is important to understand as it may have some impact on the fair use analysis and we discuss how technical mitigation strategies can take this into account in Section~\ref{sec:filtering}.}



Millions of users now use foundation model products. ChatGPT, a generalist chatbot from OpenAI, has grown to an estimated 100M daily active users.\footnote{\url{https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/}}
Midjourney's users produce millions of generated images per day.\footnote{\url{https://www.theregister.com/2022/08/01/david_holz_midjourney/}}
As foundation models are expanded into more products, deployments will only scale to more and more users. An increasingly growing list of companies has plans to deploy similar products to ChatGPT, from Microsoft's Bing Chat\footnote{\url{https://www.bing.com/new}} to Google's Bard,\footnote{\url{https://blog.google/technology/ai/bard-google-ai-search-updates/}} and more. We categorize the high-profile instances by the domain of the data in Table~\ref{tab:catalog}.

\begin{table}[!htbp]
    \centering
    \begin{tabular}{c|p{10cm}}
    \toprule
         Domain & Products\\
         \midrule
         \multirow{4}{*}{Text}
          & General Purpose API (e.g., \href{https://openai.com/api}{OpenAI GPT API}) or general chat-based agents (e.g., \href{https://chat.openai.com/}{ChatGPT}) \\
          & Write blogs and marketing material (e.g., \url{copy.ai})\\
          & Custom generated stories (e.g., \url{https://novelai.net/}) \\
          & Text-based adventure games (e.g., \url{https://aidungeon.io/}) \\
\hline
         \multirow{2}{*}{Code}
         & Generate code (e.g., \href{https://github.com/features/copilot}{Github CoPilot})\\
         & Pair programming with an AI assistant (e.g., \href{https://blog.replit.com/ai}{Replit})\\
\hline
        \multirow{2}{*}{Images}
        & Generate images from text (e.g., \href{https://labs.openai.com/}{OpenAI Dall-E}, \href{{https://techcrunch.com/2022/10/12/microsoft-expands-azure-openai-service-with-dall-e-2-in-preview/}}{Azure OpenAI Service},
        \href{https://designer.microsoft.com/}{Microsoft Designer},
        \href{https://github.com/CompVis/stable-diffusion}{Stable Diffusion}, 
        \href{https://www.midjourney.com/}{Midjourney}) \\
         \bottomrule
    \end{tabular}
    
    \caption{We enumerate a small fraction of advertised foundation model deployments and products provided via APIs or other interfaces, demonstrating that these systems are being deployed as products in a wide range of areas.
    }
    \label{tab:catalog}
\end{table}





\subsection{Definitions and Roles}
\label{sec:definitions}
Before our discussion, we define several actors. 
The \textit{data creator} creates data that a model might be trained on. The \emph{data curator} collects data and a \textit{data host} distributes
data that a model is trained on. The \emph{model creator} trains the model on this data. The \emph{model deployer} hosts a model and provides access to it via an API, potentially creating revenue from serving the model. The \emph{model user} uses the model for downstream tasks, potentially creating revenue with the output of the model.
These actors may all be the same person or entity, or they may be different people or entities.

We primarily discuss the potential for a data intellectual property (IP) owner (the \textit{data creator}) to bring a case against foundation model deployers, users, and creators. 
While there is certainly risks of liability for data curators, this has long been discussed in other work. 
We will also focus on liability as a result of the model outputs themselves, not the training process or the model parameters.\footnote{See discussions by, e.g., \citet{mccann2021copyright,lemley2020fair,grimmelmann2015copyright,sobel2017artificial} for more examination of model parameters and model training.}
Instead, we focus on whether those weights can be \textit{used} in an infringing way and thus incur liability.


\subsection{Fair Use}
\label{sec:fair}

In the United States, the legal doctrine of \textit{fair use} provides some relief from liability for using copyrighted material without a license. The fair use defense is determined by considering four factors: (1) the {purpose and character} of the use, including whether the use is of a commercial nature or is for nonprofit educational purposes (\textbf{transformativeness}); (2) the \textbf{nature} of the copyrighted work (fair use strongly favored if original work is factual as opposed to creative); (3) the \textbf{amount and substantiality} of the portion used in relation to the copyrighted work as a whole; (4) the \textbf{effect} of the use upon the potential market for or value of the copyrighted work. \textit{See} 17 U.S.C. \S 107. 
It is important to note that every factor will play \textit{some} role in the court's decision-making process, but the interaction between them is not always clear.

We will briefly provide an overview of each fair use factor in this section, but we stress that fair use doctrine is murky and evolving.
In any common law setting, a case-by-case review helps outline the contours of the doctrine, so we will subsequently review relevant case law to help shine a light on how fair use doctrine might handle foundation models.
Within the topics we discuss, we provide a descriptive survey of the current state of fair use doctrine and how it could relate to foundation models to the extent possible. However, there will be significant nuances and room to maneuver depending on the exact structure of a deployment and training procedure.

\paragraph{Transformativeness.} When the original work is transformative, this weighs heavily in favor of fair use. Empirical studies have found that the transformativeness factor tends to be most dispositive in legal analyses and is heavily emphasized in assessments of fair use~\citep{asay2020transformative}.
For example, when Google copied parts of the Java API for Android, the Supreme Court found that this was fair use. 
It took into account that the amount of code copied (a small percentage of the derivative code base), and the end product was transformative~\citep{oracle}. 
Similarly, Google Books can show portions of books to users because the percentage of the displayed book is small and the use case is transformative (from the original use of reading a book cover-to-cover to a new use case of searching quickly through a book)~\citep{ebooks}. 

For scenarios concerning machine learning and AI, some legal scholars believe that fair use covers most types of model training where the resulting model functions differently than the input data, particularly when the model targets a different economic market~\citep{lemley2020fair,carroll2019copyright}.
In part, these arguments sometimes analogize to cases related to \emph{intermediate copying}---as long as the ``defendant’s end product was a transformative new work and the copying was a necessary step to get there,'' the copying of copyrighted material is covered by fair use~\citep{lemley2020fair}.
For foundation models that are not applied in a generative context, this argument can be a good fit. 
For example, training a recommendation system or search engine on copyrighted books is likely sufficiently transformative from the purpose of the original book and its target markets, and is thus likely to be considered fair use---as in Google Books.

However, the story may be different for generative use cases. For generative models like DALL-E or GPT that produce creative outputs, the situation is less likely to be problematic if the outputs do not copy a substantial portion of any existing work but instead transform the input into totally different outputs, in line with the fair use doctrine~\citep{sobel2017artificial,lemley2020fair}.
When the downstream product based on such a model is \emph{not} transformative (e.g., model outputs similar content to copyrighted training data, or the application's market is similar to original data markets), courts may decide that the generated content, the model deployment, and even potentially the model parameters themselves are not covered by fair use~\citep{sobel2017artificial}.

Consider a generative foundation model trained on copyrighted books. 
In the extreme case if the model is trained on a book such that it can verbatim reproduce the entire book consistently (no matter what input is provided), then this is not transformative and could be problematic. Would this be any different than redistributing the book if you provide a mechanism to store and output the book?
In the more common scenario, foundation models used for generative tasks will fall into a gray area where they produce some content that looks similar to the original training data and some content that looks substantially different.
A key question is whether the generated content that looks similar to the original training data is still transformative enough that it would meet this fair use factor.

How much transformation is needed? In general, what kinds of transformations are acceptable depends on the context, but overall, fair use mostly requires transformations of low-level content (relatively low n-gram overlap) as well as higher-level concepts (no similar storylines with specific repeated non-generic structures). This fundamentally means that more technical research is needed to keep models covered by fair use, as we will discuss throughout this work.
In later sections we will cite relevant case law where each of the transformations (except for parodies) was found not to be fair use. For example, Figure~\ref{fig:markettomarket} in \S \ref{sec:text} illustrates how a generative foundation model trained on books might be used to produce different types of outputs and what cases might illustrate similar situations. 
These cases help us outline the level of the transformation necessary to stay within the current confines of fair use doctrine.
While we will also briefly discuss other fair use factors, we will primarily focus on transformativeness throughout this article as it is a key component of a fair use defense.

It is also worth noting that we mainly focus on transformativeness of the outputs themselves, but the \textit{purpose} of the machine learning model could be transformative. For example, in \citet{fields} one consideration was whether the use-case of caching a webpage for tracking changes was a transformative purpose from displaying the webpage for viewer consumption. We generally do not address these considerations as much throughout this work, though they may play a large role in litigating fair use. At a high level though, we will assume that the foundation model is used for a sufficiently similar purpose to the original training data that analysis will fall to model outputs. For example, training on books to generate abridgements or training on webcrawls to generate web pages. Given the large amount of diverse data ingested by models and the wide range of use-cases, it is likely that there will be deployments or models whose purpose competes with part of the original data source.

\paragraph{Nature of the copyrighted work.} There are many nuances as to what can be copyrighted. For example, an idea cannot be copyrighted, only the expression of that idea. Facts also cannot be copyrighted, only the expression of those facts. As a result, courts will consider the components of the original work that were used and whether they should receive protection under copyright law.

\paragraph{Amount and Substantiality.} A critical point is how much content was taken from the original work. A \textit{de minimis} amount is acceptable. For example, one can quote the original work as long as the quotes are not a substantial portion of the original work. This was a critical factor in the Google Books case since Google does not display significant portions of books~\citep{ebooks}.  Importantly, the intermediate copying of a work in its entirety may not count against fair use if the intermediate copy is used to generate output that is not itself infringing~\citep{1992sega,2000sony}.

\paragraph{Effect on Market.} Closely tied to transformativeness, if the new product has some effect on the market (or a potential market derivative market) for the original work, this will be taken into account. 
So, using a model trained on books to create a derivative book in the same market will be more likely to affect the original market.
But the market effect must be from infringement, not merely from competition from a noninfringing work.

While non-commercial distribution does not automatically imply fair use, it improves the likelihood of a successful fair use defense. A recent empirical study found that 36 of 47 ($\sim 77\%$) analyzed case opinions involving a non-commercial application found fair use~\citep{beebe2020empirical}. But we caution non-commercial researchers from assuming that they are automatically covered by fair use even in more extreme cases of infringement. In \citet{gsu}, professors at Georgia State University made copies of books available to students on the school's internal system. While the court found most instances of this to be fair use, it did identify four instances that were not fair use.\footnote{In this particular case it stated that it weighted each of the fair use factors as ``25\% for factor one [purpose and character], 5\% for factor two [nature of the copyrighted work], 30\% for factor three [amount and substantiality] and 40\% for factor four [effect of the use on the potential market]''~\citep{gsu}.}



\input{sections/examples_and_analysis}

\input{sections/mitigations}

















\section{Forward-looking Agenda}


As demonstrated throughout this work, the risk of copyright violation and litigation, even with fair use protection, is a real concern. To mitigate these risks, we recommend that foundation model practitioners consider implementing the mitigation strategies outlined here and pursuing other novel research in this area. There is significant, exciting, technical research required to make technical mitigation strategies robust and aligned with fair use doctrine. We reinforce that machine learning researchers \textit{must} play a role in providing viable mitigation mechanisms to demonstrate that models are truly covered by fair use. 

\paragraph{Preventing extreme outcomes in the evolution of fair use law by advancing mitigation strategies.} Legal scholars have noted that there might be two extreme outcomes for fair use and machine learning~\citep{sobel2017artificial}. 
On one hand, there is a possibility that courts may rule that foundation models are widely acceptable under fair use regardless of the likelihood of infringement or efforts at mitigation, which could have adverse effects on the income of data creators and disregard the ethical and moral rights attached to their work. 
On the other hand, there is a possibility that courts may declare that generative foundation models cannot be trained on unlicensed copyrighted data in most cases.
This scenario could lead to a concentration of power for companies that have retained licenses to large amounts of data; companies like YouTube or Facebook might be able to leverage large amounts of user-contributed data where others would be shut out of model training.
Neither of these two outcomes is ideal. 
As litigation progresses, identifying mechanisms to prevent extreme outcomes will be critical. For example, it is important to demonstrate that not all forms of foundation models are inherently infringing and that some of their potential risks can be effectively managed through technical means.

With better demonstrations of co-evolving technical mitigation strategies, the law might find a middle ground that allows model training and deployment with sufficient effort to implement objectively strong mitigation strategies.
Courts may consider the reasonable efforts of model builders and deployers to mitigate copyright risk, both in deciding fair use and in determining whether they can face indirect infringement liability.
Trademark courts have taken a similar approach, for example in~\citet{tiffanyebay}.
As such, advancing research in this area (with methods such as improved similarity metrics) may help in preventing extreme outcomes in legal settings.\footnote{But, again, technical mitigation strategies will only go so far in the fair use assessment and will not (and should not) automatically guarantee that any one deployment is acceptable under fair use doctrine.}

\paragraph{We should not over-zealously filter.} There must be a balance to filtering. Well intentioned but strict filtering mandates adopted by other countries have been criticized and criticized for their impacts on free speech~\citep{eufilterseff}. Similarly, YouTube's content ID system, a large-scale filtering approach, has been criticized for not following fair use standards and being overaggressive in its filtering~\citep{bartholomew2014death,boroughf2015next}.  \citet{levendowski2018copyright} points out that restrictive views of fair use doctrine can exacerbate biases and that fair use can help create fairer systems. While mitigation strategies will help prevent undesirable outcomes, it is important to develop strategies that carefully align with fair use standards, as we have previously discussed. This means that factual content should not necessarily be filtered, neither should parodies, or short form regurgitation used for commentary. And evolutions of fair use doctrine or further policymaking should consider the distributive effects of preventing access to certain types of data for model creation.

\paragraph{Policymakers could consider how and if DMCA (or similar) safe harbors should apply to foundation models.}  As we have seen, there are various ways, including filtering, to mitigate the risk of copyright infringement in the output of foundation models, but none will entirely eliminate the risk of liability. 
Even when trained on presumably permissively licensed datasets, for example, it is difficult (if not impossible) to determine the provenance of every piece of data and filter it out.
Users might post content to seemingly permissively-licensed databases that they do not actually have the rights for. There may even be uncertainty about whether a piece of content is \textit{actually} in the public domain or whether that status has been revoked.\footnote{This is not a hypothetical, in \citet{golan} the Supreme Court found that revoking a work's public domain status is not unconstitutional. In that case, a group of artists had relied on the public domain status of some works whose copyright status was later restored as part of the Uruguay Round Agreements Act.}
And even if foundation model practitioners implement strong mitigation strategies, the amorphous nature of fair use doctrine may make it difficult to know what kinds of content will be covered by fair use \textit{ex ante}.

With the uncertainty of DMCA protections (discussed in \S~\ref{sec:considerations}), the law may need to adapt to this reality, and it could do so, for instance, by clarifying the role of safe harbors for models that implement sufficiently strong mitigation strategies. Policymakers could make clear that DMCA protections apply to this setting or they could identify other more suitable safe harbor mechanisms. This may provide more balance than general-purpose text and data mining exemptions seen in other countries, but again are not a panacea. 
Such safe harbors would have to be structured to consider the strength of the implemented mitigation strategies to ensure that they are not abused.


\paragraph{Pursuing other remedies beyond technical mitigation.}
Importantly, even if technical mitigation strategies managed to keep foundation models within the confines of fair use, these models may still create harms in many other ways---including disrupting creative industries, exploiting labor, and more. See extensive discussion by, \textit{e.g., }~\citet{bender2021dangers,bommasani2021opportunities,blodgett2020language,maori}. It is important to note that we do not suggest that technical mitigation strategies will solve everything and neither will fair use doctrine. Our goal here is to point out that currently there is more work to be done even \textit{within} the confines of fair use to make foundation models more in line with case law. Other strategies to prevent harms should be pursued in conjunction with the strategies we outline here, but they should be carefully weighed against other potential harms from excluding data under overly restrictive copyright standards \citep{levendowski2018copyright}. 
For example, complementary approaches to what we describe here could include statutory licensing schemes, taxation and redistribution, or other policy mechanisms.
While these may be worthy of considering, each may have its own challenges and are outside the scope of this work.
Furthermore, there are other aspects of fair use that we do not consider here, and there well may be cases where technical mitigation strategies will still not be enough for fair use.



\input{sections/related_work}

\section{Conclusion}

We reviewed U.S. fair use standards and analyzed the risks of foundation models when evaluated against those standards in a number of concrete scenarios with real model artifacts.
Additionally, we also discussed mitigation strategies and their respective strengths and limitations.
As the law is murky and evolving,
our goal is to delineate the legal landscape and present an exciting research agenda that will improve model quality overall, further our understanding of foundation models, and help make models more in line with fair use doctrine.
By pursuing mitigation strategies that can respect the ethics and legal standards of intellectual property law, machine learning researchers can help shape the law going forward.
But we emphasize that even if fair use is met to the fullest, the impacts to some data creators will be large. We suggest that further work is needed to identify policies that can effectively manage and mitigate these impacts, where the technical mitigation strategies we propose here will fundamentally fall short.
We hope that this guide will be useful to machine learning researchers and practitioners, as well as lawyers, judges, and policymakers thinking about these issues.
We emphasize, again, that even if foundation models are covered by fair use the impacts on labor might be con


\section*{Acknowledgements}
This work was done at the Center for Research on Foundation Models (CRFM), and we would also like to thank the Stanford Institute for Human-Centered Artificial Intelligence (HAI) for supporting this work.
We thank Alex Aiken for generously providing us with access to MossPlus---the commercial version of Moss. 
We thank Rishi Bommasani, Dilip Arumugam, Mark Krass, and Jieru Hu for helpful discussions and feedback. 
We thank Tony Lee for supporting our experiments with the CRFM infrastructure. 
PH is funded by the OpenPhilanthropy AI Fellowship.
XL is supported by a Stanford Graduate Fellowship. 
TH and DJ was supported by a grant from OpenPhilanthropy.
Note, ML was hired as counsel for \citet{stablediffusionlitigation} after a near-final draft of this work was written. This work reflects the personal opinions and research of the authors. It does not reflect the position of any other entity or person, nor does it constitute legal advice.
 
\bibliographystyle{iclr2023_conference}
\bibliography{refs}

\newpage
\clearpage
\appendix
\input{sections/appendix}

















\end{document}
