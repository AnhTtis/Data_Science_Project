\section{Background and related work}
\label{sec:relatedwork}

   % P1
   \noindent \textbf{Visual representation learning with language} Common VL tasks such as phrase grounding and visual question answering leverage objectives that align and/or merge image and text features \cite{alayrac2022flamingo, kim2021vilt, li2022blip, li2021align, wang2022image, wang2021simvlm,  yu2022coca, yuan2021florence}. Alignment achieved with large-scale contrastive learning \cite{hadsell2006dimensionality} has powered the traditional vision task of image classification by enabling impressive zero-shot capability (\eg ALIGN \cite{jia2021scaling}, CLIP \cite{radford2021learning}). The primary mechanism for adapting models like CLIP to recognition is through creating prompts (\eg ``a photo of a [classname]")  for all classes and using the text encoder to convert prompts into classifier weights. Recent methods have exploited this open-vocabulary capability of CLIP to provide attribute context with LLM-based class descriptions \cite{menon2022visual, pratt2022does}. It is still unclear the extent to which VL models for recognition can consider attributes. As such, we conduct more in-depth experiments within the ``classification via description" task of \cite{menon2022visual}, highlighting limited utility of attributes in zero-shot recognition with CLIP. 
      
   Our analysis also hones in on object detection, which typically entails a predefined class list and bounding box annotations. The use of text embeddings has expanded the detection vocabulary \cite{kamath2021mdetr, li2022grounded, zhang2022glipv2, bansal2018zero}, and large-scale image-caption datasets and weakly supervised objectives have enabled ``cheaper" supervision  \cite{ye2019cap2det,desai2021virtex}. Open-vocabulary detection \cite{zareian2021open}, which involves training on base classes and using region-text alignment to extend to novel classes, has become especially popular. Recent open-vocabulary detectors leverage CLIP through mechanisms such as distillation, prompting, and pseudo-labeling \cite{bangalath2022bridging, du2022learning, gao2022open, gu2021open, wu2023aligning, wu2023cora, zhong2022regionclip, zhou2022detecting}. Our work impacts this area as we gauge the attribute sensitivity of the CLIP model widely used with these approaches. Additionally, through \cite{zareian2021open}, we study fine-grained region-word alignment \cite{chen2020uniter, kim2021vilt, li2022grounded}, which is tailored to region-level tasks \cite{yao2023detclipv2}. We in particular examine the impact of attribute context in detection through comparing results to \cite{zareian2021open}. 
   
    \noindent \textbf{Bias and sensitivity measurement of embeddings in VL tasks} Our work relates to efforts to understand the biases in embeddings from VL models. Such probing has highlighted that grounded/aligned embeddings encode social biases \cite{ross2020measuring} and lack sensitivity to composition and word order \cite{thrush2022winoground, yuksekgonul2022and}. Our investigation more thoroughly analyzes embeddings with respect to attributes. For example, we gauge whether visual embeddings are sensitive to attribute presence and meaning when grounding to contextualized word embeddings (from \cite{devlin2018bert}). The work of \cite{bertasius2020cobe} relates as it involves use of contextualized object embeddings to detect object states (\eg \textit{sliced tomato}, \textit{tomato in a bowl}). Our work instead explores if \textit{enhancing the attribute sensitivity} of contextualized object embeddings impacts more general object detection and fine-grained text-region retrieval tasks. 

    \noindent \textbf{Contrastive negative sampling} ``Hard" negative samples can benefit contrastive learning \cite{kalantidis2020hard, robinson2020contrastive}. We explore negative sampling of captions to enhance attribute context in region-word pretraining and CLIP finetuning. Past works have created negatives by replacing nouns \cite{gupta2020contrastive} and by perturbing word order \cite{yuksekgonul2022and}. We alternatively test more attribute-tailored strategies by replacing only adjectives in captions, randomly/plausibly based on a dataset. We exhibit that order perturbations \cite{yuksekgonul2022and} do \textit{not} help fine-grained text-region retrieval, while adjective negatives do. We also show that adjective negatives improve vs. a generic caption sampling baseline on Visual Genome Attribution \cite{yuksekgonul2022and},
    and that adjective negatives benefit detection in region-word pretraining. Concurrent work \cite{doveh2023teaching} has also shown the value of adjective negatives, though our work uniquely shows value in detection. \cite{cascante2023going} also leverages attribute perturbations, but alternatively with synthetic visual data. 

    \noindent \textbf{Attributes in vision tasks} Our work considers object attributes, described through adjectives in captions, with respect to object learning and model capabilities. Past work has explored attributes with respect to direct prediction \cite{pham2021learning}, compositional zero-shot recognition with objects \cite{saini2022disentangling, nayak2022learning}, and use as a bridge between base and novel classes in zero-shot classification \cite{lampert2013attribute, xu2020attribute}. With respect to localization, attributes have served as signals to spatially constrain object learning \cite{jerbi2020learning, xiao2017weakly} and as part of an open-vocabulary attribute detection task (detecting all attributes with an object) \cite{bravo2022open}. Our work is unique as we analyze attributes \textit{as context for objects}, gauging impact in tasks like retrieval and detection. 

    


