 
\section{Experimental results and analysis} 
    \label{sec:results}

    In Section \ref{insights}, we analyze the attribute sensitivity of VL alignment. For OVR-CNN region-word grounding, we test removing context in the captions used for pretraining detection (Fig. \ref{remove_fig}) and perturbing captions in unsupervised phrase grounding (Fig. \ref{attribute_pg_fig2}). For CLIP image-text alignment, we test perturbing the text prompts for classification via description (Fig. \ref{classviadesc_fig}). In Section \ref{evaluate}, we further evaluate how attribute context sensitivity impacts practical downstream tasks. We evaluate the impact of attribute sensitivity on an \textit{object-focused} task, in particular open-vocabulary detection with OVR-CNN (Table \ref{negative_strategies}/\ref{soa}). We also evaluate models on two \textit{fine-grained} tasks that require attribute knowledge, namely, text-region retrieval and object attribution (Table \ref{clipretrieval}/\ref{retrieval}).


    \subsection{Gauging the role of attribute context}
        \label{insights}
        \input{figures/remove_fig.tex}

        \input{figures/attribute_pg_fig}


        \input{tables/l1_l6_detection.tex}


        \noindent \textbf{Attribute context has limited impact in region-word pretraining for object detection.} We first examine the role of attributes through \textit{removing all ``amod" from captions} during VL pretraining with OVR-CNN. Open-vocabulary detection results for baseline OVR-CNN \cite{zareian2021open} are shown in Fig. \ref{remove_fig}. Note that the max. drop from training with to without adjectives is -0.36 AP$_{50}$ (base), and there are not discernible drops in target/generalized settings. \textit{These results point to attribute context being wasted and not helpful when learning object grounding}, and thus serve as inspiration for our investigation of ways to boost use of attribute context. 
        


        \noindent \textbf{Contextualizing object grounding does not result in embeddings with high sensitivity to attribute meaning.} As outlined in Sec. \ref{rwgroundcase}, we contextualize grounding in OVR-CNN as one strategy to integrate attribute context. Then through unsupervised phrase grounding, we gauge sensitivity to attribute meaning and analyze whether the attributes contextualizing an object noun (\eg ``a \textit{red} \underline{car}") impact performance. Fig. \ref{attribute_pg_fig2} shows AP@IoU=30:10:50 for (1) OVR-CNN with contextualization and (2) OVR-CNN with contextualization \textit{and} plausible adjective/noun negatives, on the four region-word grounding scenarios of interest (baseline grounding, removing adjectives, changing adjectives plausibly, and changing adjectives randomly). On the left of Fig. \ref{attribute_pg_fig2}, we find that with default contextualization, changing adjectives plausibly/randomly yields similar AP to using baseline captions or captions with removed adjectives (a max. difference of 0.13 AP@IoU=30:10:50). \textit{These observations are counterintuitive}, as embeddings can be contextualized by incorrect adjectives, yet ground similarly to when there are correct adjectives. We posit that the model may be sensitive to caption structure, where object embeddings with different adjectives are close together, and the model does not have an incentive to differentiate them. Such lack of sensitivity to attribute meaning motivates our exploration of \textit{adjective negatives}; we show the effects on the right in Fig. \ref{attribute_pg_fig2}. Contextualization aptly becomes less aligned with incorrect adjectives, 
        reaching notable drops when changing plausible/randomly with respect to the baseline
        (-1.36\%/-1.57\% respectively). In Sec. \ref{evaluate}, we show the importance of sensitivity in detection and retrieval.

        
        \input{figures/classviadesc}



        \noindent \textbf{Describing classes in terms of attributes alone is ineffective.} We measure CLIP's sensitivity to attributes with classification via description. As outlined in Sec. \ref{sens_method}, zero-shot inference is performed on ImageNetV2 using CLIP default prompting, LLM-based sets of object feature descriptions \cite{menon2022visual}, and LLM-based single-sentence descriptions of objects \cite{pratt2022does}. In Fig.~\ref{classviadesc_fig}, we show the results of removing/changing adjectives and removing class names in terms of top1/5 accuracy. Removing/changing adjectives results in \emph{insignificant drops} vs. the baseline with \cite{menon2022visual}, and slightly bigger drops with the \cite{pratt2022does}-like method (-4.1\% drop baseline to changing),  
        potentially as a result of more adjective-dense descriptions (supported in the supp.).
        However, \textit{removing class names} results in close to \textit{ten times more substantial} drops (max -40.5\% top1 accuracy). These results bring into question the model's ability to leverage attribute descriptions since \emph{class names drive performance}. Such results also limit the appeal of using attribute descriptions for new/custom objects with names not in the pretraining set.

            
    \subsection{Evaluating context enhancement strategies}
        \label{evaluate}
           
        \noindent \textbf{Enhancing context sensitivity helps open-vocabulary detection in base and generalized settings.} We evaluate OVR-CNN with four strategies to boost attribute context in region-word pretraining: (1) contextualized grounding, (2) adjective negative sampling (plausible/random), (3) noun negative sampling (random), and (4) language encoder/projection layer training, with results shown at an experimental scale in Table \ref{negative_strategies}. Compared to the baseline \cite{zareian2021open}, combining all strategies, in both plausible and random adjective negative cases, provides the largest gains in base-only and generalized (all-class) settings (\eg +3.0 and +2.5 AP$_{50}$ respectively with plausible). In Table \ref{soa}, we also present a proof-of-concept showing that enhancing attribute context improves the results reported in \cite{zareian2021open} in 4/5 settings (+0.9-1.0 AP$_{50}$ in base-only and all generalized settings). \textit{Such results highlight value in better using context, especially attributes, when learning grounding for detection}.
        \input{tables/soa.tex}

        Breaking down Table \ref{negative_strategies}, a key observation is that plausible/random adjective negatives, when used with contextualized grounding, result in (comparable) base and generalized gains over all other baselines (+0.5 and +0.4 AP$_{50}$ with plausible). These results can be ascribed to increased attention to attribute meaning that is obtainable with contextualized grounding, but not with context-free grounding since embeddings do not vary with context. \textit{There is marked benefit to learning to ground objects with attribute signals for detection.} Still, there is a tradeoff between contextualized and context-free grounding. Contextualized models result in top AP$_{50}$ in base-only and all generalized settings, but context-free results in top AP$_{50}$ in target-only. These results can be attributed to using contextualized embeddings and \textit{not} adjective negatives, since all contextualized methods obtain worse target performance than the best context-free method. We reason that the drop is due to the need for a prompt: we use a simple ``A/an $<$objName$>$." (see Sec. \ref{rwgroundcase}), but this prompt may be suboptimal to represent the large variance of contextualized embeddings for an object. Training with box annotations in base may allow visual embeddings to adjust to prompts, explaining base gains, but with no target training, adjustment cannot occur. 
        We surmise that recent work in context optimization \cite{du2022learning} can overcome this challenge. The noun negatives notably improve target-only vs. contextualized (+1.1 AP$_{50}$), showing that differentiating nouns in the same context may also help.
        
        We further inspect the plausible case by comparing class-by-class results using models in row 2/4 of Table \ref{negative_strategies}. Notably, the classes with top AP$_{50}$ gains are \textit{oven} (+4.6), \textit{bear} (+4.3), \textit{horse} (+3.6), and \textit{frisbee} (+3.4). Upon inspection of the corpus, these are commonly described in captions with visually distinctive adjectives  that may help grounding such as colors (\eg ``yellow frisbee"). Overall, we observe that 32/48 classes improve in AP$_{50}$ with adjective negatives.


        \noindent \textbf{Adjective negatives increase CLIP's fine-grained utility in multiple tasks.} We use text-region retrieval and attribution as fine-grained tasks to evaluate attribute-object understanding. Table \ref{clipretrieval} shows these results comparing strategies for finetuning CLIP on COCO: (1) choosing a random negative caption, (2) order-perturbing adjectives/nouns \cite{yuksekgonul2022and}, (3) random adjective sampling, and (4)  plausible adjective sampling. On retrieval,
        random adjective sampling is generally most effective across values of $k$, plausible is second, and both strategies outperform a random caption baseline and the order-perturbing captions of \cite{yuksekgonul2022and}. \textit{The fine-grained differentiation needed for retrieval is aided best by adjective negatives}. On the attribution task, the order-perturbing negatives perform best, which makes sense given that attribution involves determining the correct order of adjectives and nouns. It is notable that adjective negatives improve on this task \textit{and} retrieval vs. a random caption baseline, \emph{unlike the order-perturbing captions}. This shows adjective negatives achieve more generalizable attribute-object understanding across tasks. Adjective negatives similarly improve in retrieval for OVR-CNN (Table \ref{retrieval}). Plausible and random adjective sampling are more competitive in this scenario, though random sampling has highest R@1/P@1 and plausible sampling P/R@5/10. We surmise that random adjective sampling may solidify easier retrievals 
        by comparing to a wide array of adjectives, while plausible sampling may help the model differentiate between tougher cases as plausible adjectives serve as more realistic, \textit{harder} negatives.
        
        %In general, the results show that training with adjective negatives is simple and effective to improve fine-grained utility.


        \input{tables/clipretrieval}

        \input{tables/retrieval}


    

    %\noindent \textbf{Further analysis and discussion}

    % Possible
    % Attribute type breakdown - PG
    % Distance in language embedding space
    % Visualizations
    % Maybe something with usneen attr
    % Maybe test on UnRel (domain shift) 
    % PT on other datasets Concaps
    % Test on other datsets (LVIS, Flickr, VAW) 
 
