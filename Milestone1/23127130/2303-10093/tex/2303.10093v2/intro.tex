\section{Introduction}
    \label{sec:intro}

    \input{figures/intro_fig}

    Natural language has been shown to provide a strong signal for training visual representations. A visual-text alignment model can be pretrained with image-caption data and used for downstream tasks like object recognition, detection, and retrieval. While the text embeddings that represent object nouns are often used as classifier weights, the impact and utility of other caption context, especially attributes, are less clear. Consider the Fig. \ref{intro_fig} caption: ``A very \textbf{large} \textbf{furry} \textbf{brown} \underline{bear} on a \underline{rock} by the \underline{water}.''
    The model can learn grounding using only nouns (underlined), but \textit{bear} can also be learned in the context of its attributes (bolded adjectives). Do alignment models use attribute context to learn \textit{bear}? Do they distinguish between \textit{brown bear} and \textit{black bear}?

    Attributes in captions can aid object recognition and detection in various ways. Attributes can serve as a proxy for a fine-grained category which is not explicitly mentioned (\eg a \textit{small, young cat} is a \textit{kitten}). They can ensure that the alignment model is paying attention to the right features, rather than dataset artifacts (\eg that a \textit{bear} is being grounded as such because it is \textit{brown/black}, rather than because its background is a \textit{forest}). They can be used to specify subcategories, \eg when a user desires detections that match a certain property (\eg a \textit{red car}, but not a \textit{blue car}).  

    With this motivation, our goal is to better understand the connection between attributes and objects in vision-language (VL) models. In particular, we explore considerations such as whether models leverage attributes in captions as important signals for object learning and whether VL models can use object and attributes effectively in fine-grained tasks (\eg recognizing ``a large bird with a brightly colored bill"). We refer to a model's capabilities in using attribute information as \textit{attribute sensitivity}.
    
    We offer an extensive sensitivity analysis of two popular alignment paradigms for VL models, region-word grounding and whole image-text alignment, which we study with OVR-CNN \cite{zareian2021open} and CLIP \cite{radford2021learning}, respectively. We investigate if model decisions consider attribute presence and meaning, specifically testing how attribute perturbations affect unsupervised phrase grounding and classification via description tasks. \textit{We find an overall lack of sensitivity to attribute meaning}, inspiring an investigation into adjective-based contrastive negative sampling of captions. Our exploration results in strategies that increase the benefits from attribute context, exhibited through improvements in practical use cases of open-vocabulary object detection, fine-grained text-region retrieval, and object attribution.
    

    Our main contributions are insights into these questions:
    \begin{enumerate}[nolistsep,noitemsep]
    %[itemsep=1mm, parsep=0pt]

         \item Does attribute context play an impactful role in VL pretraining for object detection? 

         \item Does learning to ground objects to contextualized word embeddings utilize attribute meaning? 

         \item Do VL models perform well at tasks where objects are described with/in terms of attributes? 

         \item Can contrastive negative sampling of captions increase a model's ability to use attribute context? 
         
         \item What sampling mechanisms are most effective? 

    \end{enumerate}
    
    
