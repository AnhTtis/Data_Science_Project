\section{Methodology}
    \label{sec:methods}

    When VL models learn alignment for recognition and detection, the \textit{utility} of attributes in captions is considerably underlooked. We study object representation sensitivity to attributes through case studies in region-word grounding (OVR-CNN \cite{zareian2021open}) and image-text alignment (CLIP \cite{radford2021learning}). This section outlines these frameworks, our measurement methods, and our strategies to enhance attribute context. 
    
\subsection{Vision-language frameworks of study}

    We study contrastive frameworks that learn in each iteration using a batch $\mathcal{B}$ of image-caption pairs ($\mathcal{B}_I$ and $\mathcal{B}_C$ for just images and captions, respectively). A score $\langle I, C\rangle$ is computed to quantify the relative matching between an image $I$ and a caption $C$. Image-to-text and text-to-image contrastive objectives are as shown in Eqs. \ref{gen_loss_i2t} and \ref{gen_loss_t2i}:

    \begin{equation} \label{gen_loss_i2t}
        { \mathcal{L}_{I \rightarrow T}(I) = - \log \frac{\exp \langle I, C\rangle}{\sum_{C'\in \mathcal{B}_C } \exp \langle I, C'\rangle} } 
    \end{equation}

    \begin{equation} \label{gen_loss_t2i}
        { \mathcal{L}_{T \rightarrow I}(C) = - \log \frac{\exp \langle I, C\rangle}{\sum_{I'\in \mathcal{B}_I } \exp \langle I', C\rangle} } 
    \end{equation}

    Methods may differ in terms of how the scoring function is defined and whether losses include additional components such as temperature or normalization constants. 


    \subsubsection{Case study: Region-word grounding}
    \label{rwgroundcase}
        We explore region-word grounding to learn fine-grained alignment for detection. We specifically consider OVR-CNN \cite{zareian2021open}, which employs a weakly supervised, region-word grounding pretraining task to learn class embeddings for open-vocabulary detection with Faster R-CNN \cite{ren2015faster}. The model resembles PixelBERT \cite{huang2020pixel}, using ResNet-50 \cite{he2016deep}, a pretrained BERT \cite{devlin2018bert}, and a BERT-like multimodal model. The total loss $\mathcal{L}(I, C)$ comprises four objectives: masked language modeling ($\mathcal{L}_{MLM}$), image-to-text matching ($\mathcal{L}_{ITM}$), and two contrastive grounding terms ($\mathcal{L}_{G}(C)$ and $\mathcal{L}_{G}(I)$). $\langle I,C \rangle$ for OVR-CNN is defined with Eqs. \ref{groundingscore_contextfree} and \ref{activitycoeff}, where the dot product is taken between each word token $e_{j}^{C}$ ($n_C$ total produced from BERT's input layer) and each region token $e_{i}^{I}$ ($n_I$ total from ResNet then projected into the language embedding space with a V2L layer):
    
        \begin{equation}        
            \label{groundingscore_contextfree}
                {\langle I,C \rangle  = \frac{1}{n_C}\sum_{j=1}^{n_C}  \sum_{i=1}^{n_I} a_{i,j} \langle e_{i}^{I}, e_{j}^{C} \rangle} 
        \end{equation}
    
        \begin{equation}        
            \label{activitycoeff}
                { a_{i,j} = \frac{\exp \langle e_{i}^{I}, e_{j}^{C} \rangle}{\sum_{i'=1}^{n_I} \exp \langle e_{i'}^{I}, e_{j}^{C} \rangle} } 
        \end{equation}    
    
        Notably, this default alignment mechanism uses \textit{context-free} word embeddings ($e_{j}^{C}$ in BERT), which do not change with surrounding language context (\eg \textit{orange} has the same embedding in the captions ``\underline{orange} basketball" and ``eating an \underline{orange}"). We reason that this type of grounding contributes to misalignment of concepts, potentially inhibiting the benefits of attribute context. More recent models (\eg CLIP \cite{radford2021learning}) also align visual regions to text embeddings contextualized through transformers. For expansive insights, we experiment with contextualization in OVR-CNN by altering Eq.~\ref{groundingscore_contextfree} to use $f_{j}^{C}$, which are BERT's \textit{output} embeddings that change with context (unlike BERT's $e_{j}^{C}$ which are static). Eq. \ref{groundingscore_contextualized} shows this change:

        \begin{equation} \label{groundingscore_contextualized}
        	{\langle I,C \rangle = \frac{1}{n_C}\sum_{j=1}^{n_C}  \sum_{i=1}^{n_I} a_{i,j} \langle f_{i}^{I}, f_{j}^{C} \rangle } 
    	\end{equation}
    
        Since word embeddings are dynamically contextualized, visual regions for an object are grounded to a \textit{collection} of embeddings instead of one. Naive integration of such embeddings into detection results in poor performance. We use the following training recipe to effectively use contextualized embeddings in detection: (1) using a prompt ``A/an $<$objName$>$." when changing a class embedding for object $k$ from $e_{k}^{\mathcal{C}}$ to $f_{k}^{\mathcal{C}}$, (2) allowing the language encoder to update in the grounding pretraining task, and (3) allowing the V2L layer to update in finetuning. These strategies provide the training flexibility needed to thoroughly evaluate attribute sensitivity with contextualized embeddings.

    \subsubsection{Case study: CLIP image-text alignment}
    
         Open-vocabulary detectors that have come after OVR-CNN notably leverage CLIP \cite{bangalath2022bridging, du2022learning, gao2022open, gu2021open, wu2023aligning, wu2023cora, zhong2022regionclip, zhou2022detecting}. Their ability to use attribute context is thus highly dependent on the attribute sensitivity of CLIP. We study CLIP's attribute sensitivity for insights that generalize to various methods. The alignment objective of CLIP notably differs from OVR-CNN in that it aligns embeddings corresponding to entire images and text descriptions rather than to regions and words. More specifically, an image $I$ and caption $C$ are processed by CLIP's image and text encoders to produce normalized feature representations $z_i^I$ and $z_j^C$. $\langle I,C \rangle$ for CLIP is defined in Eq. \ref{groundingscore_clip}, where $\langle z_i^I,z_j^C\rangle$ is a dot product:

        \begin{equation}        
            \label{groundingscore_clip}
        	{\langle I,C \rangle  = \langle z_i^I,z_j^C \rangle } 
	  \end{equation}

        A temperature $\tau$ is also used with the losses in Eqs. \ref{gen_loss_i2t} and \ref{gen_loss_t2i}. Due to CLIP's large size and scale, we focus on finetuning representations, rather than pretraining from scratch. 
        
\subsection{Analyzing model sensitivity to attributes}
    \label{sens_method}

      We aim to measure how influential attribute context is to a model's decision (\eg classification, grounding). We reason that in an attribute-sensitive model, the \textit{presence} of attributes should help decisions, as this information is complementary to objects. Additionally, the \textit{meaning} of attributes should be respected. Object representations should be more aligned when correct attributes are used than when incorrect attributes are used. Our mechanism for exploring these considerations is through \textit{removing} and \textit{changing} attribute context in the text for a task, as removal tests presence, and changing tests meaning. In this section, we outline our measurement methodology, for which we explore prior tasks that can show attribute sensitivity while fitting each alignment mechanism. In particular, we use unsupervised phrase grounding \cite{nebbia2022doubling} for OVR-CNN and classification via description \cite{menon2022visual} for CLIP, as shown in Fig. \ref{attr_sens}. 

     \noindent \textbf{Isolating objects and attribute context} For analysis of OVR-CNN (and for training as outlined in Sec. \ref{enhance}), we define a vocabulary $\mathcal{V}$ to be the nouns corresponding to objects in a dataset $\mathcal{D}$. In our study, $\mathcal{D}$ is COCO \cite{chen2015microsoft}, with 118,287 images and 5 captions per image. We build $\mathcal{V}$ from the synonym list of COCO class names provided in \cite{lu2018neural}, with plural terms added. The vocabulary $\mathcal{V}$ captures various terms for each class (\eg \textit{jet}, \textit{aircraft}, \textit{planes} for \textit{airplane}). We identify a class attribute as any adjectival modifier (``amod") with dependency on a class synonym in $\mathcal{D}$, detected with \cite{Honnibal_spaCy_Industrial-strength_Natural_2020}. The unique adjectives for each class make up respective \textbf{plausible} sets, containing attribute properties across the dataset (\eg a \textit{frisbee} is \textit{red}/\textit{green}/etc.). Unique adjectives across all classes make up the \textbf{random} set. We provide further details and statistics in the supp. material.
          
     \noindent \textbf{Measuring attribute sensitivity in region-word grounding} OVR-CNN is analyzed using unsupervised phrase grounding \cite{nebbia2022doubling}, a task that returns a bounding box $b$ for a text query $t$. Given an image-caption pair ($I$, $C$), we ask: if $I$ has a \textit{red car}, are visual regions for that car grounded better when using the car embedding in the caption ``a red \underline{car}..." than when using the embedding in ``a blue \underline{car}..." or ``a \underline{car}..."? Put another way, we test if the model leverages attribute meaning when grounding object regions to contextualized word embeddings. While a model could align visual regions for \textit{car} independently of attributes (\eg with context-free embeddings), we reason that bag-of-words behavior may result since embeddings are the same in cases like ``a red car and blue truck"/``a blue car and red truck". Also, the model would not be fully leveraging capabilities of contextualized embeddings, where a region-word objective can encode attribute information within a contextualized object grounding, such that the model dynamically learns to represent \textit{red car} vs. \textit{blue car}. 

     In this setup, we test four grounding scenarios: (1) using the \textit{baseline caption}, containing ground-truth attributes in adjective form (\eg ``a \textit{yellow} \underline{banana} on the table"); (2) using a caption that has object adjectives \textit{removed} (\eg ``a \underline{banana} on the table"); (3) using a caption that has object adjectives changed \textit{plausibly} according to our sets (\eg ``a \textit{rotten} \underline{banana} on the table"); and (4) using a caption that has adjectives changed \textit{randomly} to be any intra-corpus (\eg ``a \textit{red} \underline{banana} on the table"). In an attribute-sensitive model, we expect the top-performing grounding to have the most information (\eg ``yellow \underline{banana}"). We expect removal performance to drop vs. this baseline as objects are less specified. We reason that changing adjectives should make attributes \textit{incorrect} and thus hurt vs. the baseline.
     In the plausible case, we expect the dataset to cover disjoint states (\eg \textit{wooden} vs. \textit{plastic} spoon). While multiple attributes could be valid for an object, in practice, we find such cases rare.
     On 100 random samples, we find that 84\% of captions changed plausibly and 92\% changed randomly are not reasonably correct. We expect changing plausibly to thus result in a smaller drop from the baseline vs. changing randomly.
     

     \input{figures/attr_sens}

     To compute groundings, for each caption token $j$, if it matches an object term in $\mathcal{V}$, one or more bounding boxes are generated from the binary map of region-word similarity $\langle f_{i}^{I}, f_{j}^{C} \rangle$  such that $\langle f_{i}^{I}, f_{j}^{C} \rangle$  $\geq th_{sim}$. In the supp. material, we test at values $th_{sim}$=5, 10, 15 and show trends are not sensitive to this threshold. Then for all captions which mention that object, these boxes are compared to the ground-truth at various IoU thresholds, producing AP@$t$ values. We use $t$=30,40,50 as non-aggressive thresholds suitable for unsupervised inference. The average AP@IoU=30:10:50 is reported over all classes in the COCO validation set.
     
     
     %To gauge sensitivity to attribute presence and meaning, we compare AP@$t$ for four scenarios, perturbing COCO object-modifying adjectives as such: (1) leaving adjectives as is (\eg ``a \textit{yellow} banana on the table"), (2) dropping adjectives (\eg ``a banana on the table"), (3) changing adjectives plausibly to be another adjective mentioned with the object in the corpus (\eg ``a \textit{rotten} banana on the table"), and (4) changing adjectives randomly to be any other in the corpus (\eg ``a \textit{red} banana on the table").
     
    \noindent \textbf{Measuring attribute sensitivity in CLIP image-text alignment} We analyze CLIP's attribute sensitivity through classification via description\cite{menon2022visual}, which adds attribute context to object prompts to aid zero-shot inference. In \cite{menon2022visual}, for each class \textit{c} in a dataset $\mathcal{D}$, GPT-3 is prompted to produce a list of descriptors $D(c)$. The descriptors contain attributes relevant to the object, along with $c$ to condition the attributes. For instance, the descriptors produced for \textit{toucan} are ``a/an \underline{toucan} which (is/has/etc) \textit{large}, \textit{brightly colored} bill.", ``a/an \underline{toucan} which (is/has/etc) \textit{long}, \textit{pointed} wings.", etc. To classify an image $I$, each descriptor $d$ serves as a prompt. The score for each class is computed using the average CLIP logits, $\phi(I,d)$, over each $d$, shown in Eq. \ref{score_orig}:

    \begin{equation}
        \label{score_orig}
        s(c, I) = \frac{1}{|D(\textit{c})|}\sum_{d \in D(\textit{c})} \phi(I,d)
    \end{equation}
    
     We select $\mathcal{D}$ to be ImageNetV2 \cite{recht2019imagenet} and use GPT-3 (\textit{davinci-002}) to produce descriptors. We also test producing only a single-sentence description to simulate the related method \cite{pratt2022does} (\eg ``A \underline{toucan} is a \textit{large} bird with a \textit{distinctive large, brightly colored} bill."). With both setups, we test sensitivity through removing and changing, detected as ``ADJ" with \cite{Honnibal_spaCy_Industrial-strength_Natural_2020}, but unlike OVR-CNN, we only use random changing (and not plausible) since the descriptors do not have a vocabulary like COCO. Then to further stress test CLIP, in a given inference, we remove \textit{all} class names by replacing them with ``a/an object". This experiment gauges whether CLIP can interpret objects from attribute-only descriptions (\eg a \textit{small}, \textit{white}, \textit{round} object with \textit{red seams} is a \textit{baseball}). In the supp. material, we provide specific details, examples, and linguistic properties for descriptors.

    %Since attribute impact may be partially masked with descriptor averaging (not all descriptors have adjectives), we also test querying LLMs to provide a concise, single-sentence description, similar to \cite{pratt2022does}. , which contains only fundamental attributes for testing.
    

\subsection{Enhancing model sensitivity to attributes}
    \label{enhance}

    We also hypothesize that \textit{enhancing} attribute context's role can help tasks like object detection and text-region retrieval. We specifically experiment with \textit{adjective-based negative caption sampling}, where a negative for a caption $C$ includes the same words, just with an adjective replaced (\eg for ``\textit{blue} car in the street", \textit{blue} is replaced with \textit{red}). We reason that these negatives can encourage models to capture attribute meaning when learning objects, increasing the model's fine-grained utility. In pretraining specifically, another benefit is that attributes may help ``guide" object grounding to the correct regions (\eg a \textit{car} to a \textit{red} region). 

    We test negative sampling in OVR-CNN pretraining and CLIP finetuning, both with COCO. We explore two replacement methods: (1) choosing a \textit{random} adjective from the corpus and (2) choosing a \textit{plausible} adjective for a noun, such that it is mentioned intra-dataset with the respective class term. Through these strategies, we aim to gauge whether it is beneficial to contrast disjoint states in a dataset with \textit{plausible} (\eg \textit{wooden} vs. \textit{metal spoon}) or if simple \textit{random} adjectives suffice.  Table \ref{example_caps} shows examples of plausible and random captions. To implement in training, for each caption in $\mathcal{B}_C$ with an adjective detected, a negative caption is added to a batch $\mathcal{B}_N$. The loss in Eq.~\ref{gen_loss_i2t} becomes:

    \begin{equation}        \label{groundingloss_image_with_negatives}
    	{ \mathcal{L}_{I \rightarrow T}(I) = - \log \frac{\exp \langle I, C\rangle}{\sum_{C'\in \mathcal{B_C} + \mathcal{B_N} } \exp \langle I, C'\rangle} } 
    \end{equation}

     A potential shortcut with region-word grounding is that a model can solve the task by grounding just adjectives rather than object words. To encourage OVR-CNN to consider objects and attributes, we use noun negatives (using the same caption, but replacing nouns with random ones from $\mathcal{D}$). For CLIP, if no adjective-noun pair is detected, we add a random caption to $\mathcal{B}_N$. We also compare the plausible and random strategies to order-perturbing sampling \cite{yuksekgonul2022and}, since order perturbations can influence attention to attributes (\eg ``red car and blue truck" vs. ``red truck and blue car"). We test this strategy by perturbing order when possible (i.e. the caption has adjectives and nouns); as with other strategies, we sample a random caption otherwise. 

    \input{tables/example_negatives}



        
        
     