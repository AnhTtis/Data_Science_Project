    Note phrase detection here, add more recent detectors 

\section{Methodology for measuring the attribute sensitivity of object representations}

    The overall research question we investigate is whether vision-language models, and open-vocabulary object detectors in particular, can recognize and detect objects when considered with attributes.  
    
    Current attribute-based evaluation of vision-language pretraining for open-vocabulary detection is lacking, so we outline a methodology to understand the \textit{attribute sensitivity} of models. 

    \subsection{Vision-Language Models of Study}

    Many state-of-the-art open-vocabulary detectors nowadays leverage CLIP [source], often for knowledge distillation [source] or for pseudolabeling [source]. Therefore gauging attribute sensitivity in such detectors can first be performed with analysis of CLIP. We therefore investigate the attribute sensitivity of CLIP as a recognition model, in addition to its evaluation as part of open-vocabulary detection methods.  

    CLIP is notably trained with an image-text alignment objective, while fine-grained region-word
    objectives are noted to be especially suitable for detection [source]. Additionally, with CLIP, due to its large-scale, pretraining is not feasible, and we believe it is impactful to understand the role of attributes in pretraining for detection. Therefore another model we choose to investigate is OVR-CNN.

     We study \textit{region-word alignment} to learn grounding for object detection and model an approach based on OVR-CNN \cite{zareian2021open}, a state-of-the-art method for open-vocabulary detection. OVR-CNN is modeled as a PixelBERT-like \cite{huang2020pixel} architecture with three encoders in pretraining: ResNet-50 \cite{he2016deep} for visual features, a pretrained BERT \cite{devlin2018bert} for text features, and a 6-layer BERT-like model for multimodal features. The total loss $\mathcal{L}(I, C)$ comprises four objectives: masked language modeling ($\mathcal{L}_{MLM}$), image-to-text matching ($\mathcal{L}_{ITM}$), and two contrastive grounding terms ($\mathcal{L}_{G}(C)$ and $\mathcal{L}_{G}(I)$) which are defined for an image $I$ and caption $C$ in Eqs.~\ref{groundingloss_image} and \ref{groundingloss_caption}: 

   \begin{equation} \label{groundingloss_image}
	{ L_G(I) = - \log \frac{\exp \langle I, C\rangle _ G}{\sum_{C'\in \mathcal{B_C} } \exp \langle I, C'\rangle _ G} } 
    \end{equation}

    \begin{equation}    \label{groundingloss_caption}
    	{ L_G(C) = - \log \frac{\exp \langle I, C\rangle _ G}{\sum_{I'\in \mathcal{B_I}} \exp \langle I', C\rangle _ G} } 
    \end{equation}

    These contrastive objectives operate over a batch of captions $\mathcal{B_C}$ and images $\mathcal{B_I}$, maximizing a grounding score $\langle I,C \rangle _{G}$ for corresponding image-caption pairs (minimizing otherwise). Eqs.~\ref{groundingscore_contextfree} and \ref{activitycoeff} break down this score:

    \begin{equation}        
        \label{groundingscore_contextfree}
            {\langle I,C \rangle _{G} = \frac{1}{n_C}\sum_{j=1}^{n_C}  \sum_{i=1}^{n_I} a_{i,j} \langle e_{i}^{I}, e_{j}^{C} \rangle _{L} } 
    \end{equation}

    \begin{equation}        
        \label{activitycoeff}
            { a_{i,j} = \frac{\exp \langle e_{i}^{I}, e_{j}^{C} \rangle _{L}}{\sum_{i'=1}^{n_I} \exp \langle e_{i'}^{I}, e_{j}^{C} \rangle _{L}} } 
    \end{equation}

    Here, the dot product $\langle$$e_{i}^{I}$, $e_{j}^{C}$$\rangle_L$ represents the similarity between an image region embedding $e_{i}^{I}$ and a caption word embedding $e_{j}^{C}$. The attention coefficient $a_{i,j}$ represents this similarity normalized over all $n_I$ image tokens. To compute $e_{i}^{I}$, an $h \times w$ image $I$ is tokenized into $h/32 \times w/32$ region tokens. These tokens are processed by the image encoder to produce region embeddings $r_{i}^{I}$, which then are projected into the language embedding space with a vision-to-language (V2L) layer to become $e_{i}^{I}$. To compute $e_{j}^{C}$, a caption $C$ is tokenized then converted using the input embedding layer of BERT into a length $n_C$ vector of such word embeddings. During training, BERT processes each $e_{j}^{C}$ to produce a contextualized word embedding $f_{j}^{C}$, which is further input along with $e_{i}^{I}$ into the multimodal encoder to produce multimodal features $m_{i}^{I}$ and $m_{j}^{C}$. We differentiate between language embeddings by referring to $e_{j}^{C}$ as \textit{context-free} (since it does not change depending on language context) and $f_{j}^{C}$ as \textit{contextualized}. 

    (Paragraph on contextualization) 

    For downstream open-vocabulary detection, the visual encoder and V2L layer are transferred into Faster R-CNN \cite{ren2015faster}. When region proposal ($r_{i}^{I}$) features are computed, they are projected by the V2L layer into  $e_{i}^{I}$. Classification is performed in training and inference by computing the similarity between $e_{i}^{I}$ and each $e_{k}^{\mathcal{C}}$, which represents the \textit{class embedding} for each concept $k$ in a set $\mathcal{C}$ (in addition to a background embedding). We emphasize that by default the V2L layer is frozen in finetuning, and BERT is frozen in pretraining, while other architectural components are trained. For more on OVR-CNN, refer to \cite{zareian2021open}. We illustrate our novel context enhancement strategies (outlined in Section \ref{enhance}) added to this approach in Fig. \ref{approach_fig}.

    We also explore a variant where we contextualize the grounding objective 

    Our first strategy stems from the observation that detection methods often use \textit{static} word embeddings to describe object classes. For instance, OVR-CNN's pretraining task grounds regions to the input word embeddings of BERT ($e_{j}^{C}$). These embeddings are in a sense ``context-free", as they do not change with surrounding language context (\eg \textit{orange} has the same embedding in the captions ``\underline{orange} basketball" and ``eating an \underline{orange}"). We hypothesize this type of grounding can limit detection success due to non-optimal alignment of different concepts. 
    We additionally posit that such grounding prevents the model from learning fine-grained concepts (\eg a \textit{red car}) in a structurally sound way (such as through language contextualization), and the model instead captures \textit{red} and \textit{car} separately akin to bag-of-words (little distinction between ``red car'' and ``blue car on a red background'').
    %We additionally posit that such grounding prevents the model from learning fine-grained concepts based on language contextualization (\eg a \textit{red car} region could not be aligned to the \textit{car} embedding in ``red \underline{car}").
    
    To address these challenges, we alter the grounding process to learn to align regions with \textit{contextualized} word embeddings. In OVR-CNN, this strategy corresponds to altering Eq.~\ref{groundingscore_contextfree} to use the $f_{j}^{C}$ outputs from BERT, changing the projected image embedding to the contextualized space ($e_{i}^{I}$ to $f_{i}^{I}$), and changing
    the class embedding $e_{k}^{\mathcal{C}}$ to $f_{k}^{\mathcal{C}}$ in detection. The contextualized grounding score $\langle I,C \rangle _{G}$ can be thus computed as:

    \begin{equation} \label{groundingscore_contextualized}
    	{\langle I,C \rangle _{G} = \frac{1}{n_C}\sum_{j=1}^{n_C}  \sum_{i=1}^{n_I} a_{i,j} \langle f_{i}^{I}, f_{j}^{C} \rangle _{L} } 
	\end{equation}

    A key observation is that visual regions for an object noun are grounded to a \textit{collection} of embeddings instead of one single embedding since contextualized embeddings are \textit{dynamic} (i.e. changing with surrounding words in the caption). This consideration causes naive integration of such embeddings into detection to result in poor performance. We propose a series of strategies to enable effective use of contextualized embeddings in detection: (1) using a prompt ``a/an \underline{$<$categoryName$>$}." in the class embeddings  (similar to CLIP \cite{radford2021learning}), (2) allowing the language encoder to update when grounding is learned (pretraining), and (3) allowing the V2L layer to update in finetuning. These strategies provide the flexibility needed in the training process to benefit from the use of contextualized embeddings.

    \subsection{Measurement Strategies}

         \textbf{Text-region retrieval and fine-grained detection}
    
            We highlight our methods for analyzing the impact of attributes. 

            Based on the OVAD dataset, we consider colors, patterns, materials, etc. 

            \input{methodology}
    
        \textbf{Classification by description}

            

            For CLIP 
    
        \textbf{Unsupervised word grounding}

            Additionally with this approach, 

        \textbf{Performance on downstream tasks}
    We consider to two primary methods: 

    (1) 

    - Building an attirubte list 

    \input{gauging_sens}

    
\section{Strategies to enhance the role of attribute context}

    Next, we describe the strategies that we explore in order to overcome the lack of attribute sensitivity in open-vocabulary models.  

    \textbf{Adaptation of CLIP}

        If extra time, image sampling too 

        Add equation

    \textbf{Pretraining OVR-CNN}

        In the case of pretraining with a region-word alignment, we find that a random noun negative is helpful

        We investigate a handful of strategies due to computational costs
        Could focus more on attribute negatives rather than noun 

\section{Downstream task results with enhance attribute sensitivity}

\section{Conclusions and ideas for futrue work}

    Emphasize attribute-sensitive and good base/generalized performance
 %Our approach differs from COBE in focus (we use attributes rather than more general object states), data (we use noisier captions instead of cleaner instructional video narrations), and level of supervision (we do not use bounding boxes when learning alignment).     
    
    %With motivation from the study of biases in grounded VL embeddings \cite{ross2020measuring, thrush2022winoground}, we explore how visual regions are sensitive to the words that contextualize object noun embeddings (particularly adjectives) through unsupervised phrase grounding. We also show that using contextualized embeddings in open-vocabulary detection is \textit{nontrivial} and thus provide insights into pros and cons of their use along with strategies to maximize their effectiveness in detection.

        %We emphasize that by default the V2L layer is frozen in finetuning, and BERT is frozen in pretraining, while other architectural components are trained. For more on OVR-CNN, refer to \cite{zareian2021open}. 


        
   % Also, since the word embeddings do not change with sentence location/context (no distinction between ``car" in ``red \underline{car} and blue truck" and ``blue \underline{car} and red truck"), the model may not have the incentive to learn that an image particularly contains a \textit{blue car}, and not just \textit{blue} and \textit{car} separately. An example highlights this scenario in Fig. [ref]. 
    
    %there may not be an incentive to ground objects and their attributes together, and the model may not learn 

    %With grounding to contextualized word embeddings, a model can learn to ground a visual concept with consideration of attribute context. 
    
    %For example, for an image region with a \textit{red car}, its visual embedding may be close to the word embedding of \textit{car} in the caption ``red \underline{car} and blue truck", but not to the \textit{car} word embedding in the caption ``blue \underline{car} and red truck". Such contextualization could be a structurally sound way to learn attribute-object concepts. 

    

    
    %\cite{datta2019align2ground,gupta2020contrastive, plummer2017phrase, li2022adapting, parcalabescu2020exploring, wang2019phrase, xiao2017weakly}. 
    
    
   % General strategies for phrase grounding learn by training directly on region-phrase correspondences (\eg \cite{plummer2017phrase}), using weak supervision with image-text data and off-the-shelf detectors \cite{datta2019align2ground, gupta2020contrastive, xiao2017weakly}, and learning without matching image-text data through the use of external knowledge \cite{wang2019phrase, parcalabescu2020exploring} or adaptation of CLIP \cite{li2022adapting}.


   
        
   
    
    % \cite{shahmohammadi2022language}, which has studied the impact of visual grounding to contextualized word embeddings for NLP tasks. We rather study the impact of visual grounding to contextualized embeddings for the traditionally vision-based task of object detection. 

 
    % A major component of our analysis is unsupervised phrase grounding. General strategies for phrase grounding have learned through training directly on region-phrase correspondences (\eg \cite{plummer2017phrase}), using weak supervision with image-text data and off-the-shelf detectors \cite{datta2019align2ground, gupta2020contrastive, xiao2017weakly}, and learning without matching image-text data through the use of external knowledge \cite{wang2019phrase, parcalabescu2020exploring} or adaptation of CLIP \cite{li2022adapting}. Methods are often compared on datasets such as Flickr30K Entities \cite{plummer2015flickr30k}, ReferItGame \cite{kazemzadeh2014referitgame},  Visual Genome \cite{krishna2017visual}, and COCO Captions \cite{chen2015microsoft} in terms of accuracy and/or recall@$k$. Phrase grounding is involved in our work as region-word alignment can be framed as a weakly supervised phrase grounding pretext task. We note that this learning differs from many of the dedicated phrase grounding methods as \textit{no bounding boxes are used to learn region-word alignment}. 

   %   Phrase grounding is a multimodal task that involves localizing bounding box regions in an image using a textual description.
    
   % Another manner in which we use phrase grounding is as an unsupervised evaluation task to gauge visual embedding sensitivity to adjectives during grounding. 
    
    
    %For this purpose, we adapt the phrase grounding task used to study OVR-CNN in \cite{nebbia2022doubling}. In this approach, an image $I$ is tokenized into regions, a caption $C$ is tokenized into words, and an attention coefficient $a_{i,j}$ is computed for each region-word token pair. For a given caption token $j'$, if it corresponds to a COCO object term, one or more bounding boxes are generated from the connected components of the binary map of $a_{i,j}$ such that $a_{i,j'}$ $\geq th_{attn}$. These boxes serve as unsupervised phrase grounding predictions for a given object. Then for all captions which mention that COCO object, these boxes are compared versus the ground-truth boxes, producing a value for mAP@$t$. Our adaptation differs from this approach by instead grounding using similarity to contextualized word embeddings. We also provide a unique method to measure the sensitivity of visual embeddings to the attributes in contextualized word embeddings.