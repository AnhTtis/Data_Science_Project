\section{Gauging attribute sensitivity in vision-language models}
    
    Our first thrust is to evaluate attribute understanding and impact in state-of-the-art open-vocabulary methods. While many state-of-the-art methods use CLIP [source], we choose to explore OVR-CNN [source] as a more fundamental pretraining task. We argue that it is important to explore attribute context \textit{from scratch} during pretraining, which is expensive with CLIP. Additionally, OVR-CNN is trained with a region-word alignment objective, which is fitting for fine-grained tasks, and has become a more recent focus [source]. Given a lack of current evaluation, we present methods to evaluate the role of attributes. 

    \textbf{Open-vocabulary models struggle at tasks that require differentiation between objects and attributes.}

        For detection, we consider fine-grained OVAD and zero-shot instance retrieval. Also use text-region retrieval to evaluate the recognition only capability. 
        
    \textbf{Attribute-based descriptions are not sufficient for classification} 
    
        We show that descriptions by exploring recent methods [source, source] that leverage LLMs to generate class descriptions that can be used as prompts with CLIP. 
    
        (Show an example of a class description) 

        Insight useful in general 
    
    \textbf{Attribute context may be ignored in pretraining.}
        
    \textbf{Visual regions embeddings may not take into account attribute meaning.}