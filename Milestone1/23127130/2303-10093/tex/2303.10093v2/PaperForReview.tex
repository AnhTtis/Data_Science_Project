% WACV 2024 Paper Template
% based on the CVPR 2023 template (https://media.icml.cc/Conferences/CVPR2023/cvpr2023-author_kit-v1_1-1.zip) with 2-track changes from the WACV 2023 template (https://github.com/wacv-pcs/WACV-2023-Author-Kit)
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[accsupp]{axessibility}
%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review,algorithms]{wacv}      % To produce the REVIEW version for the algorithms track
%\usepackage[review,applications]{wacv}      % To produce the REVIEW version for the applications track
%\usepackage{wacv}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{wacv} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{chapterbib}
\usepackage{colortbl}
\usepackage{color}
\usepackage[dvipsnames]{xcolor}
\definecolor{Gray}{gray}{0.77}
\definecolor{DarkGray}{gray}{0.43}
\definecolor{LLGray}{gray}{0.91}
\definecolor{LGray}{gray}{0.94}
\definecolor{LightCyan}{rgb}{0.88,1,1}
\definecolor{Greenish}{rgb}{0.10,0.60,0.30}
\definecolor{ForestGreen}{RGB}{34,139,34}
\definecolor{Wood}{RGB}{150,111,51}
\definecolor{DarkBrown}{RGB}{150,78,2}
\definecolor{Pinkish}{RGB}{255,102,220}
\definecolor{Blueish}{RGB}{2,78,150}

\newcolumntype{a}{>{\columncolor{Gray}}c}
\newcolumntype{g}{>{\columncolor{LGray}}c}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\wacvPaperID{86} % *** Enter the WACV Paper ID here
\def\confName{WACV}
\def\confYear{2024}













\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE

\title{Investigating the Role of Attribute Context in Vision-Language Models for Object Recognition and Detection}

%\title{Amplifying Attribute Context in Region-Word Pretraining for \\Object Detection}

\author{    Kyle Buettner\textsuperscript{\rm 1},
    Adriana Kovashka\textsuperscript{\rm 1,2}\\
\textsuperscript{1}Intelligent Systems Program, \textsuperscript{2}Department of Computer Science,
University of Pittsburgh, PA, USA\\
{\tt\small buettnerk@pitt.edu, kovashka@cs.pitt.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}
\maketitle
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% MAIN CONTENT 

\input{abstract}

\input{intro.tex}

\input{relatedwork}

\input{methodology}

\input{evaluationmethods}

\input{results}

\input{conclusion} 

{\small\bibliographystyle{ieee_fullname}\bibliography{egbib}}

\clearpage

%
\thispagestyle{plain}

\section{Supplementary Material}
The supplementary material contains two sections. Section \ref{datagen_supp} outlines how we generate data for training and evaluation. Section \ref{pg_supp} shows further experiments for phrase grounding analysis of OVR-CNN.

\subsection{Data Generation}
\label{datagen_supp}

\subsubsection{Classification via description}

We present details regarding data generation for measurement of CLIP's attribute sensitivity, particularly for use in the classification by description task on ImageNet-v2 \cite{recht2019imagenet}. Overall, three ``styles" of CLIP prompts are used in inference: (1) CLIP's default ``a photo of" prompts, (2) LLM-based \textit{multiple descriptor} prompts, and (3) LLM-based \textit{single sentence} prompts.

To produce (1), all class names from ImageNet-v2 are filled into the following template and processed by CLIP's text encoder to produce classifier weights:

\begin{quote}
``a photo of a $<$category$>$."
\end{quote}

For instance, CLIP's classifier would contain the text encodings of  ``a photo of a petri dish", ``a photo of a basketball", etc. (all 1,000 classes).

For (2), we use the methodology of Menon and Vondrick \cite{menon2022visual} to produce descriptions with attribute context. For every class in ImageNet-v2, we 
prompt GPT-3 (\textit{davinci-002}, max token length 100, temperature 0.7) with a \textit{multiple descriptor} prompt template: 

\begin{quote}
   Q: What are useful features for distinguishing a lemur in a photo? \\
    A: There are several useful visual features to tell there is a lemur in a photo: \\
    - four-limbed primate \\
    - black, grey, white, brown, or red-brown \\
    - wet and hairless nose with curved nostrils \\
    - long tail \\
    - large eyes \\
    - furry bodies \\
    - clawed hands and feet \\
    Q: What are useful features for distinguishing $<$category$>$ in a photo? \\
    A: There are several useful visual features to tell there is/are $<$category$>$ in a photo:
\end{quote}

Output descriptors are returned in the format of the lemur example. We further process these outputs with the following CLIP prompt template:

\begin{quote}
    $<$category$>$ which (is/has/etc) $<$descriptor$>$.
\end{quote}

There are thus multiple prompts for each class. There would be seven prompts for the lemur example, for instance. These would appear as:

\begin{quote}
    - a lemur which (is/has/etc) a four-limbed primate. \\
    - a lemur which (is/has/etc) black, grey, white, brown, or red-brown. \\
    - ... 
\end{quote}

The average CLIP similarity between the image of interest and each descriptor (Eq. 7 in main paper) is used as the score for each class in classification.

For (3), we use a \textit{single-sentence} description-based prompt template for GPT-3, in particular the one from \cite{pratt2022does}: 

\begin{quote}
    Q: What does a lorikeet look like? Describe with one sentence. \\
    A: A lorikeet is a small to medium-sized parrot with a brightly colored plumage. \\
    Q: What does $<$category$>$ look like? Describe with one sentence. \\
    A: 
\end{quote}

We directly use each class's result from GPT-3 as a respective CLIP prompt. For instance, the lorikeet's CLIP prompt would be:

\begin{quote}
    A lorikeet is a small to medium-sized parrot with a brightly colored plumage.
\end{quote}

The lemur's CLIP prompt could be: 

\begin{quote}
    A lemur is a small, four-limbed primate with large eyes, a long tail, and a slender body. 
\end{quote}

After generating the prompts for (1)-(3), we remove/change adjectives detected with
 spaCy \cite{Honnibal_spaCy_Industrial-strength_Natural_2020} (v3.5.3) in all class prompts for inference. For class name removal, we replace \textit{all} class names with ``a/an object" when creating the CLIP classifier. As an example, consider possible prompts used to create CLIP's class embeddings with (3): 

 \begin{quote}
    (P1) A baseball is a round, stitched ball made of leather or synthetic materials, typically white with red stitching. \\
    (P2) A hockey puck is a flat, disk-shaped object made of hard rubber, often black in color, used in the sport of ice hockey. \\
    ...\\
    (P1000) A basketball is a round, inflatable ball with a synthetic or leather cover, black lines, and typically orange in color.
 \end{quote}

 Removed class names would change the classifier to: 
 \begin{quote}
    (P1) An object which is a round, stitched ball made of leather or synthetic materials, typically white with red stitching. \\
    (P2) An object which is a flat, disk-shaped object made of hard rubber, often black in color, used in the sport of ice hockey. \\
    ...\\
    (P1000) An object which is a round, inflatable ball with a synthetic or leather cover, black lines, and typically orange in color.
 \end{quote}
 
 
In Table \ref{tab:cvd}, we provide key statistics regarding the descriptors generated for this analysis.

\input{tables/cvd_dataset_stats}


\subsubsection{Analyzing COCO}

    Given the role of COCO \cite{chen2015microsoft, lin2014microsoft} in pretraining OVR-CNN, finetuning OVR-CNN/CLIP, and gauging the attribute sensitivity of OVR-CNN, we provide specific details regarding its usage. We describe in detail how we identify objects of interest and attribute context belonging to those objects, as well as statistics related to the data creation process.
    
\input{figures/coco_adj_counts}


    For all of these tasks, we create a vocabulary $\mathcal{V}$ of terms/phrases corresponding to COCO class names (\eg ``car", ``fire hydrant", ``teddy bear"). We build $\mathcal{V}$ from the synonym list of COCO class names provided in \cite{lu2018neural}, with plural terms also added. Adjectives used for specific COCO objects are detected using the spaCy dependency parser \cite{Honnibal_spaCy_Industrial-strength_Natural_2020}. For each caption, we traverse the parse tree and mark all ``amod" with dependency on a class term in $\mathcal{V}$, taking note of which class each term belongs to. We also mark terms not explicitly detected as ``amod", but connected through coordinating conjunctions (``cc"). We use per-class lists to create the \textit{plausible} sets. Some example top occurring plausible adjectives are shown in Table \ref{tab:counts_attr}. All unique adjectives detected across all classes lie in the \textit{random} set. We show the counts of unique adjectives detected across COCO classes in Fig. \ref{coco_adj}.  These sets are used to sample negative caption adjectives in pretraining and to change plausibly/randomly in unsupervised phrase grounding. 


    \input{tables/count_attributes}
    \input{tables/coco_dataset_stats}
    \input{figures/phrase_grounding_revamped}
    
    For OVR-CNN, COCO is used in our study in pretraining (2017train as an image-caption dataset), in finetuning (2017train/2017val for detection), and in unsupervised phrase grounding (2017val as an image-caption evaluation set). For CLIP, COCO is used in finetuning (2017train/2017val for image-text matching). COCO is used with both models for text-region retrieval (2017val). We provide further details of the detected adjectives in train/val in Table \ref{coco_stats}. For phrase grounding, we choose one caption for each image to use, resulting in 5,000 test cases. 



\subsection{Threshold experiments: Unsupervised phrase grounding}
\label{pg_supp}



A hyperparameter for unsupervised phrase grounding is $th_{sim}$, which determines how bounding boxes are created from similarity maps. We find in practice that the attribute sensitivity trends of interest (i.e. relative AP@$t$ differences between baseline/removal and baseline/changing) hold across values of this parameter for default contextualization and a model with adjective negatives. Fig. \ref{pg_supp_thresh} shows three values we experiment with to support this finding.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}

% Notes from WACV template
% cref
% subfigure 
% et al
% order citations 
% centering 
% include graphics with linewidth 