    
 \setlength{\tabcolsep}{1pt}
\begin{table}
    \begin{center}
    \begin{tabular}{c|c|c|c|cccc|}
    \hline
    \scriptsize Context. & \scriptsize Prompt & \scriptsize BERT  & \scriptsize V2L & \multicolumn{3}{c}{\scriptsize AP$_{50}$ }  \\
    
    \scriptsize Embed. && \scriptsize Trained & \scriptsize Trained & \scriptsize Base & \scriptsize Target & \scriptsize Generalized \\
    
    \hline\hline
     &  & & & \small 32.78 & \small 15.80 & \small 26.25 \\
     
    &  & \checkmark & & \small 33.68 & \small 17.62 & \small 27.76 \\
    
    &  & \checkmark & \checkmark & \small 34.08 \scriptsize  $\pm$ 0.01 & \small \textbf{19.09} \scriptsize  $\pm$ 0.72 & \small \textbf{28.28} \scriptsize  $\pm$ 0.27  \\
    
    \hline
     \checkmark &  & &  & \small 32.51 & \small 6.74 & \small 22.31 \\
     
     \checkmark & \checkmark  & & & \small 31.99 & \small 10.97 & \small 23.23  \\
     
     \checkmark &  \checkmark & \checkmark & & \small 34.06 & \small 15.32 & \small 26.35\\
     
     \checkmark &  \checkmark & \checkmark & \checkmark & \small \textbf{35.18} \scriptsize $\pm$ 0.13 & \small 16.67 \scriptsize $\pm$ 0.26 & \small 28.26 \scriptsize $\pm$ 0.20 \\
     
    \hline
    \end{tabular}
    \end{center}
    \caption{\textbf{Adapting OVR-CNN to effectively use contextualized embeddings (COCO)}. 
    %AP$_{50}$ is shown in base, target, and generalized settings for open-vocabulary detection, using contextualized and context-free embeddings and various other settings. We show that 
    Naive use of contextualized embeddings does not help AP$_{50}$, but improved use is enabled with a prompt (``a/an \underline{$<$categoryName$>$}."), unfreezing BERT in PT, and unfreezing the V2L layer in FT. While helping noticeably in the base setting, we still observe a tradeoff with target performance. 
    }
    \label{context_strategies}
\end{table}