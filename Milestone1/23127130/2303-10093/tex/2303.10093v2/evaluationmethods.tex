 \section{Evaluation}

        We evaluate context enhancement on one object-focused task (open-vocabulary detection) and two fine-grained tasks (text-region retrieval/object attribution).

        \noindent \textbf{Datasets} For image-caption training, we use COCO Captions \cite{chen2015microsoft}, and for finetuning open-vocabulary detection, we use COCO Objects \cite{lin2014microsoft}, (2017 train/val for both).
        The class split for open-vocabulary detection is the same as \cite{bansal2018zero,zareian2021open} (48 base and 17 target classes). For retrieval, we use the 2,000 image COCO val subset with object and attribute annotations from OVAD \cite{bravo2022open}. For attribution, we use ARO Visual Genome Attribution (VGA) \cite{yuksekgonul2022and}, with 28,748 examples.

    
     \noindent \textbf{Open-vocabulary object detection} This task considers \textit{base}/\textit{target} class sets with/without bounding box annotations. A detector (i.e. Faster R-CNN \cite{ren2015faster}) is trained only on base classes, and there are three evaluation settings: 
     \textit{base} classes only, \textit{target} only, and \textit{generalized}. As in \cite{zareian2021open}, base only and target only classify over the respective set, while in generalized, prediction is performed over the union of base and target classes, and results are reported within each group and overall. We report $AP_{50}$ as the metric, as in \cite{zareian2021open}. 
     
    
    \noindent \textbf{Text-region retrieval} We pose fine-grained text-to-region retrieval as a use case where attribute-object understanding is needed. We input a set of texts $\mathcal{T}$ for which each text \textit{t} contains an attribute \textit{a} from the set $\mathcal{A}$ and an object \textit{o} from the set  $\mathcal{O}$ (\eg \textit{red car}). The goal is to return as output top-scoring regions that are correct if they contain the correct attribute \textit{and} object (\eg for \textit{red car}, non-red cars would be incorrect). We select $\mathcal{A}$ to be colors, patterns (striped, dotted, etc.), and materials (metal, wooden, etc.) in OVAD \cite{bravo2022open} and $\mathcal{O}$ to contain all COCO objects. Since OVAD's annotations are dense, we exclude attribute-object pairs that are not described in language due to being inherent (\eg \textit{metal car}) and use attribute-object pairs with greater than 10 annotations. Overall, we use 323 attribute-object pairs (273 with colors, 42 materials, and 8 patterns).  In evaluation, every ground-truth box in OVAD is considered a possible retrieval ($\approx$14,300 samples). For CLIP, we input crops for each GT box to the image encoder and use similarity between image and text features to rank retrievals. For OVR-CNN, we compute the region embedding $f_{i}^{I}$ for each box. Then for all text \textit{t} in $\mathcal{T}$, we compute the dot product between the average word embedding $f_{j}^{t}$ of its attribute and object text tokens (\eg average($f_{red}$, $f_{car}$)) and every $f_{i}^{I}$. We report recall@$k$ (a true positive is when at least one retrieval of the correct attribute and object is within the top $k$).
    We also report precision@$k$, the proportion of correct retrievals in the top $k$. We do not directly evaluate on OVAD since the task has the different goal of detecting all attributes rather than differentiating between categories described with attributes.

    \noindent \textbf{Object attribution} For CLIP, we consider object attribution (with the VGA dataset \cite{yuksekgonul2022and}) as a relevant benchmark for image-text matching.     This task involves selecting the correct text for an image, given two choices with different order (\eg ``the crouched cat and the open door" vs. ``the open cat and the crouched door"). Note that this task is \textit{complementary} to the retrieval task, in that they both test attribute understanding, but text-region retrieval focuses more on fine-grained differentiation among \emph{plausible} attribute-object pairs (\textit{blue car} vs. \textit{red car} vs. \textit{blue truck}), while attribution focuses on intra-caption ordering where negative pairs are often \emph{implausible} (\eg ``crouched door''). 

    \noindent \textbf{Training}  Full-scale comparison to \cite{zareian2021open} uses 8 Quadro RTX 5000 GPUs and settings from \cite{zareian2021open}. For other OVR-CNN results, we pretrain using 4 NVIDIA GeForce GTX 1080 Ti with memory 11 GB. Pretraining uses 80k iter., batch size  (BS) 16, and learning rate (LR) 0.01 that scales down 10x after 40k/70k steps. For COCO finetuning, we use 4 GPUs, 
         75k iter., BS 8, and LR 0.005 that scales down after 30k/60k steps.
         CLIP finetuning is performed using OpenCLIP \cite{ilharco_gabriel_2021_5143773}, for 5 epochs using BS 64 and LR 1e-6 on 1 Quadro RTX 5000. CLIP's image encoder is ViT-B/32.
         