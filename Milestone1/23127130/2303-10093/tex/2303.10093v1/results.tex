\section{Experiments} %Results and analysis}
\label{results_stuff}

   \subsection{Evaluation settings}

        \noindent \textbf{Datasets} For vision-language pretraining, we use COCO Captions \cite{chen2015microsoft}, and for finetuning open-vocabulary detection, we use COCO Objects \cite{lin2014microsoft}. The 2017 train split is used for all forms of COCO training, while 2017 val is used for evaluation. In context removal experiments, we finetune with the smaller MiniCOCO \cite{HoughNet} (25k images). The class split for open-vocabulary detection is the same as \cite{bansal2018zero,zareian2021open} (48 base and 17 target classes). For text-region retrieval, we use the 2,000 image subset of COCO images with dense object and attribute annotations from the OVAD dataset \cite{bravo2022open}.  

        \noindent \textbf{Open-vocabulary object detection} We select open-vocabulary object detection as the primary evaluation method for our context enhancement strategies due to it being a state-of-the-art task that benefits from region-word alignment (and language). There are two sets of classes: a \textit{base} set with bounding box annotations and a \textit{target} set for which there are not any bounding box annotations (i.e. unknown classes). The architecture for this setting is Faster R-CNN \cite{ren2015faster}. This detector is trained only on the base classes, resulting in three evaluation settings with different class embeddings used: \textit{base} classes only, \textit{target} only, and \textit{generalized}.
        %setting with both base and target classes. 
        As in \cite{zareian2021open}, base-only and target-only use classifier predictions over the respective set only, while the generalized setting performs prediction over the union of base and target classes, and reports results within each group and overall.
        %As done in prior work \cite{zareian2021open}, AP$_{50}$ is reported overall for base-only and target-only scenarios, while in the generalized scenario,  AP$_{50}$ is reported on just the base classes, just the target classes, and all classes.  


        \noindent \textbf{Unsupervised phrase grounding} We use unsupervised phrase grounding as a mechanism to measure (after pretraining) how sensitive visual region embeddings are to the attributes that contextualize object word embeddings (\eg is the visual region more similar to \textit{red} \underline{car} or \textit{blue} \underline{car}?). For a given caption token $j$, if it corresponds to a COCO object term, one or more bounding boxes are generated from the connected components of the binary map of region-word similarity $\langle f_{i}^{I}, f_{j}^{C} \rangle$  such that $\langle f_{i}^{I}, f_{j}^{C} \rangle$  $\geq th_{sim}$ ($th_{sim}$=10 in our case). These boxes serve as unsupervised phrase grounding predictions for a given object. Then for all captions which mention that COCO object, these boxes are compared to the ground-truth boxes, producing a value for AP@$t$ ($t$=30 in our case). We perform this process over all captions $C$ and images $I$ in the COCO validation set to produce an overall average AP@$t$.

             %In particular, we compute the region-word similarity $\langle f_{i}^{I}, f_{j}^{C} \rangle$ for all image and caption tokens in an image-caption pair. 

         %  We modify the OVR-CNN-based unsupervised grounding task in \cite{nebbia2022doubling}
   %rather than attention $a_{i,j}$.
    
        \noindent \textbf{Text-region retrieval} We pose a fine-grained text-to-region retrieval scenario to show a use case where attribute sensitivity is beneficial. In particular, we consider as input  a set of texts $\mathcal{T}$ for which each text \textit{t} contains an attribute \textit{a} from the set $\mathcal{A}$ and an object \textit{o} from the set  $\mathcal{O}$ (\eg \textit{red car}). The goal is to return as output top-scoring bounding box regions in images that are correct if they contain a ``red car" (non-red cars would be incorrect). We select $\mathcal{A}$ to be the colors in OVAD \cite{bravo2022open} and $\mathcal{O}$ to contain all COCO objects. We then use every GT box in this set as potential retrievals (approximately 14,300 samples). The region embedding $f_{i}^{I}$ for each box is obtained as the OVR-CNN embedding after processing feature maps with ROIAlign and the V2L layer. For all text \textit{t} in $\mathcal{T}$, we compute the dot product similarity between the average contextualized word embedding $f_{j}^{t}$ of its attribute and object text tokens  (\eg the average $f_{red}$ and $f_{car}$) and every $f_{i}^{I}$. We report recall@$k$, where we define a true positive as when at least one retrieval of the correct attribute and object is within the top $k$ (false negative otherwise). We also report precision@$k$ where we consider the proportion of correct retrievals out of the top $k$.
        
        \noindent \textbf{Training details} For direct comparison to state-of-the-art (full-scale), we use 8 Quadro RTX 5000 GPUs and the same settings as OVR-CNN \cite{zareian2021open}. For all other results (experimental scale), we pretrain using 4 NVIDIA GeForce GTX 1080 Ti GPUs with memory 11 GB. Pretraining requires 80k iterations, a batch size of 16, and a learning rate of 0.01 that scales down by a factor of 10 after 40k and 70k steps. For COCO finetuning, we use 4 GPUs, while for MiniCOCO finetuning, we use 2 GPUs. 
        Finetuning for COCO requires 75k, batch size of 8, and learning rate of 0.005 that scales down after 30k and 60k steps (80k, 4, 0.005, 12k, 24k respestively for MiniCOCO).
        %Finetuning for MiniCOCO requires 80k iterations, batch size of 4, and a learning rate of 0.005 that scales down after 12k and 24k steps. 
        Unless specified, other settings follow OVR-CNN. 

    \subsection{Results and analysis}
    %\subsection{Experiments} 
    \label{insights}


        We overview experiments leading up to our top method, with contextualized grounding and the adjective-noun negative sampling strategy, called \textit{Attribute-Sensitive OVR-CNN}. Our model with only contextualized grounding is called \textit{Contextualized OVR-CNN}. We refer to baseline OVR-CNN as using \textit{context-free} grounding. 

        \input{tables/l1_l6_detection.tex}


        \input{figures/remove_fig.tex}




        \noindent \textbf{How is context used in detection by default?} Motivating the need for context enhancement, we pretrain using captions with removed adjectives (\eg \textit{red}, \textit{striped}), prepositional phrases (\eg \textit{in the kitchen}, \textit{besides a cactus}), or verb phrases (\eg \textit{drinking a beverage}, \textit{is on the street}), ignoring expressions with terms in $\mathcal{V}$ (\eg \textit{car}, \textit{dog}). 
        Results for OVR-CNN with context-free grounding (the baseline) and contextualized grounding (our top approach in Table \ref{context_strategies}) are shown in Fig. \ref{remove_fig}. With context-free grounding, removing non-object noun context generally has small, negative effects (adjectives have notably limited effects with -0.12 AP$_{50}$ at the maximum). \textit{This result points to wasted information in the captions} and is in line with expectations, as the lack of contextualization with grounded word embeddings inhibits the use of context.  For contextualized grounding, we observe the negative effects of context removal on detection are much more significant, with a notable maximal drop of -0.90 AP$_{50}$ due to adjective removal. \textit{Language context becomes more important in detection when using contextualized grounding, especially with respect to adjectives}. This observed importance of adjectives in language context guides our subsequent focus on attributes.   
        
        \input{tables/soa.tex}
           


        \noindent \textbf{Does enhancing context sensitivity benefit object detection?} Our overall approach, \textit{Attribute-Sensitive OVR-CNN}, demonstrates substantial improvements vs. the OVR-CNN baseline, as shown in Table \ref{negative_strategies}. In particular, base-only and generalized (all-class) improvements are +3.0 and +2.5 AP$_{50}$ vs. the original OVR-CNN, and the results in these settings also outperform a competitive context-free baseline with our BERT/V2L unfreezing strategies added (best context-free). The results translate to full-scale (Table \ref{soa}), with our method outperforming state-of-the-art in base-only and all generalized settings (+0.9-1.0 AP$_{50}$). It is notable that \textit{our method performs well on target classes in the generalized setting}  despite not performing well in the target-only setting. This indicates the utility of our method in the practical setting when there is a mix of trained/untrained classes. We also observe a large standard error in the target-only setting with respect to the best context-free model, indicating that our method may sometimes compare. 
        
        Breaking down the results in Table \ref{negative_strategies}, we can see that the adjective negatives directly lead to base (+0.56 AP$_{50}$) and generalized (+0.46 AP$_{50}$) increases, \textit{emphasizing the benefit of attribute sensitivity in downstream detection}. The noun negative component of our top strategy seems to be primarily responsible for target gains seen vs.  \textit{Contextualized OVR-CNN}, making the target class performance in the generalized case competitive with the top context-free result.
        %in Table \ref{context_strategies}. 
        This highlights that differentiating between nouns in the same context during pretraining may help make target prediction more robust. While such strategy helped phrase grounding in \cite{gupta2020contrastive}, our work emphasizes that it is \textit{the combination of adjective and noun negatives} that drives the largest performance gains in the generalized setting. 

        We further show examples from a class-by-class analysis of \textit{Attribute-Sensitive OVR-CNN} and its removed version without an adjective negative in Table \ref{class_by_class}. Over 3 trials, we observe that 32/48 classes improve in AP$_{50}$. The classes that improve the most on average are \textit{oven}, (+4.64 AP$_{50}$), \textit{bear} (+4.34 AP$_{50}$), \textit{horse} (+3.61 AP$_{50}$), and \textit{frisbee} (+3.36 AP$_{50}$). Notably, these classes are commonly described in captions with visually distinctive adjectives such as colors (\eg yellow, blue) and states (\eg polar, open). We reason that \textit{Attribute-Sensitive OVR-CNN} is able to use such information to more accurately ground objects. 

        \input{tables/class_by_class}


     \noindent \textbf{How are contextualized word embeddings best used for object detection?} \label{contextualized_results}
        Given the lack of exploration into using contextualized embeddings in region-word alignment for object detection, we experiment to see how grounding to contextualized word embeddings ($f_{j}^{C}$) in pretraining (see Equation \ref{groundingscore_contextualized}) compares to OVR-CNN's strategy to ground to BERT's context-free, input word embeddings ($e_{j}^{C}$). Specifically we pretrain with context-free and contextualizing grounding, then finetune with appropriate class embeddings (BERT's \verb+input_embeddings+, $e_{k}^{\mathcal{C}}$,  for context-free, and BERT's \verb+encoded_tokens+, $f_{k}^{\mathcal{C}}$,  for contextualized). 

        
        We discover that \textit{effectively using contextualized embeddings in object detection is nontrivial.} As shown in Table   \ref{context_strategies}, a number of strategies are needed for contextualized performance to be competitive with context-free: (1) prompting to contextualize the class embedding (\eg ``a/an \underline{$<$categoryName$>$}."); (2) unfreezing the language encoder in pretraining to give flexibility in grounding; and (3) unfreezing the V2L encoder in finetuning to give flexibility in supervised detection. We reason that the prompt helps to reduce task shift by matching the caption-like form from pretraining, similar to CLIP \cite{radford2021learning}. Furthermore, contextualized grounding may especially benefit from unfreezing BERT as the language embeddings may need significant adjustment to be appropriate for grounding. Similarly, unfreezing the V2L layer in detection training may give the model more flexibility to bring together the collection of contextualized embeddings that correspond to a single concept. 
        
        Overall, these strategies allow contextualized grounding to significantly improve versus the baseline OVR-CNN (+2.40 AP$_{50}$ in base, +2.01 AP$_{50}$ in generalized). Compared to a more competitive context-free baseline with our training strategies (minus a prompt which is non-applicable to context-free embeddings), the contextualized grounding approach improves in the base setting significantly (+1.10 AP$_{50}$). \textit{The results show benefits to strategic contextualized grounding alone.} However, it can also be observed that BERT and V2L training helps the OVR-CNN baseline as well, resulting in the top target performance. Even with the target performance, our contextualized grounding approach is competitive in the generalized setting, and there is a notably high variance in the target setting for BERT/V2L-trained OVR-CNN. In any case, these results indicate that \textit{training on bounding boxes may be especially important for contextually grounded models}, which we surmise to be a result of there being a large variance of visual embeddings for a given category due to the dynamic nature of contextualized embeddings. Nonetheless, we are able to improve in the generalized case with our adjective-noun sampling strategy and bridge the target gap to some degree, though improving contextualized grounding further in the target-only scenario is worthy of future exploration.
        
        \input{tables/contextfree_vs_contextualized.tex}

    
    \noindent \textbf{Are contextualized models sensitive to the meaning of attributes?} One question we analyze with contextualized grounding is how well do models learn an object with its adjective (\eg what a \textit{red car} is?). In particular, we use unsupervised word grounding between image-caption pairs to measure if visual regions for a given object and adjective get aligned to be especially similar to object word embeddings contextualized by such adjective. We test four region-word grounding scenarios by modifying COCO object-modifying adjectives in the following ways: (1) leaving adjectives as is (\eg ``a \underline{yellow} banana on the table"), (2) dropping adjectives (\eg ``a banana on the table"), (3) changing adjectives plausibly to be another adjective mentioned with the object in the corpus (\eg ``a \underline{green} banana on the table"), and (4) changing adjectives randomly to be any other adjective in the corpus (\eg ``a \underline{blue} banana on the table"). We show AP$_{30}$ results with \textit{Contextualized OVR-CNN} and \textit{Attribute-Sensitive OVR-CNN} in Fig. \ref{attribute_pg_fig}.
    
    For \textit{Contextualized OVR-CNN}, we find that \emph{removing adjectives hurts} (-0.32 AP$_{30}$) \emph{more than either method of changing adjectives}, which indicates that 
    for visual regions representing a \textit{yellow banana}, there is a stronger alignment with the banana word embedding in ``a yellow \underline{banana} on the table" than ``a \underline{banana} on the table". On the other hand, there is stronger alignment between the \textit{yellow banana} visual regions and ``a rotten \underline{banana} on the table"/``a blue \underline{banana} on the table" than ``a \underline{banana} on the table".  \textit{These observations are counterintuitive}, as word embeddings can be contextualized by incorrect adjectives, yet ground better than a plausible caption just missing an adjective. The negligible difference between plausibly and randomly changed attribute scenarios also brings into question how much grounding adheres to the meaning of adjectives and \textit{highlights a lack of attribute sensitivity in default contextualization}. Alternatively, it can be observed with \textit{Attribute-Sensitive OVR-CNN} that changing adjectives \emph{hurts more than removing adjectives, and changing adjectives randomly hurts more than changing plausibly,} which is the more \textit{correct} contextualization. The attribute-sensitive model thus more accurately captures the meanings of objects with their attributes.


    \noindent \textbf{What types of attributes do models gain sensitivity to?} Further analyzing which attributes the model especially learns, we perform the removal experiments with custom sets of common adjectives ($>$10 times with COCO nouns in $\mathcal{D}$): \textit{color}, \textit{age} (\eg old, young), \textit{size} (\eg large, wide), \textit{quantity} (\eg assorted, few), and \textit{object properties} (\eg general/class-specific ones like wheeled and chopped). Fig. \ref{type_fig} shows these results: \textit{Attribute-Sensitive OVR-CNN} gains sensitivity to objects described with visually distinctive colors and properties. The properties are likely the most well-learned by the model and useful for downstream tasks.
    
 
    \input{figures/attribute_pg_fig}
  
    \input{figures/type_fig}

    
    \noindent \textbf{Can enhanced attribute sensitivity aid more fine-grained object-based tasks?} Lastly, to demonstrate how a model with contextualized grounding may further benefit from added attribute-sensitivity, we consider text-region retrieval as a downstream task. Results (precision@$k$ and recall@$k$) for retrieving certain colors of object are shown in Table \ref{retrieval}. There are notable improvements in fine-grained scenarios across values of $k$. This experiment highlights potential benefits from an attribute-sensitive model in differentiating between \textit{both} attributes and objects across categories. 
    

    \input{tables/retrieval}


    

    %\noindent \textbf{Further analysis and discussion}

    % Possible
    % Attribute type breakdown - PG
    % Distance in language embedding space
    % Visualizations
    % Maybe something with usneen attr
    % Maybe test on UnRel (domain shift) 
    % PT on other datasets Concaps
    % Test on other datsets (LVIS, Flickr, VAW) 
 
