
    %Vision-language pretraining has emerged as a powerful mechanism for adding flexibility and scalability to computer vision tasks. 
    
    Vision-language pretraining to learn a fine-grained, region-word alignment between image-caption pairs has propelled progress in open-vocabulary object detection. We observe that region-word alignment methods are typically used in detection with respect to only object nouns, and the impact of other rich context in captions, such as attributes, is unclear. In this study, we explore how language context affects downstream object detection and propose to enhance the role of context. In particular, we show how to strategically contextualize the grounding pretraining objective for improved alignment. We further hone in on attributes as especially useful object context and propose a novel adjective and noun-based negative sampling strategy for increasing their focus in contrastive learning. Overall, our methods enhance object detection when compared to the state-of-the-art in region-word pretraining. We also highlight the fine-grained utility  of an attribute-sensitive model through text-region retrieval and phrase grounding analysis. 
    
    
    %Overall, our methods enhance object detection vs. the state-of-the-art OVR-CNN open-vocabulary detector by +0.9, +1.0, and +0.9 AP$_{50}$ on base, target, and all classes in the generalized setting, and by +3.0, +2.6, and +2.5 AP$_{50}$ with respect to base, target, and generalized prediction in a smaller scale setting. We further demonstrate our method's fine-grained utility through text-region retrieval and phrase grounding analysis.  
        
    