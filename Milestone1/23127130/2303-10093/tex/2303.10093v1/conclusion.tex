\section{Conclusion}

    % Conclusion
    In this study, we answer fundamental questions regarding the role of language context in region-word alignment for object-level tasks like detection. We find that existing contextual elements (i.e. adjectives, prepositional phrases, verb phrases) have limited effects on object learning when grounding is learned with context-free word embeddings. Grounding to contextualized embeddings improves the use of context, but strategies are needed to maximize their effectiveness in detection. Context can be made especially impactful through increasing the attribute sensitivity of a model, as demonstrated through a contrastive attribute-noun negative sampling strategy. Overall, our methods improve the state-of-the-art in open-vocabulary object detection. Insights can serve to guide future multimodal pretraining methods for object detection to better leverage the non-object context in language. Future work may consider methods to improve target-only performance or to take advantage of object relations and actions for object detection.

\textbf{Ethics note} Training with natural language likely introduces biases in model decisions. To avoid perpetuating harmful gender, racial, or other discriminatory biases, care should be taken with use of methods, models, and datasets.  