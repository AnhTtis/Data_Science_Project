\begin{figure}[t]
    \centering
    \includegraphics[scale=0.265]{images/intro_bear.PNG}
    \caption{\textbf{How does language context impact region-word alignment and downstream detection?} In captions, objects (\eg bear) are often described with rich contextual information (\eg attributes like large, furry, brown) which we hypothesize can impact the effectiveness of object grounding and therefore downstream detection. We show that prior work in detection with region-word alignment does not fully benefit from context such as attributes. We thus provide strategies to better leverage important object context. 
     }
    \label{intro_fig}
\end{figure}

% Such strategy can enable better understanding of fine-grained categories (\eg tuk tuk).

% Multimodal pretraining for object-level tasks involves learning to ground visual regions to caption word embeddings. 