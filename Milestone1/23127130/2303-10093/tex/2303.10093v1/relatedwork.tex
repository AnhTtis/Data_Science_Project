\section{Background and related work}

   % P1
   \noindent \textbf{Visual representation learning with language} Vision-language (VL) tasks such as image-text retrieval, phrase grounding, and visual question answering \cite{alayrac2022flamingo, kim2021vilt, li2022blip, li2021align, wang2022image, wang2021simvlm,  yu2022coca, yuan2021florence} commonly pretrain with objectives that align and/or merge image and text features. Alignment achieved with contrastive learning \cite{hadsell2006dimensionality} has powered the more traditional vision task of image classification by enabling impressive zero-shot capability (\eg ALIGN \cite{jia2021scaling}, CLIP \cite{radford2021learning}). Our work is focused on the finer-grained object detection task, for which language has helped text-conditioned prediction \cite{kamath2021mdetr, li2022grounded, zhang2022glipv2}, weak supervision \cite{ye2019cap2det}, mixed supervision \cite{desai2021virtex}, zero-shot inference \cite{bansal2018zero}, and most relevantly open-vocabulary inference (introduced by OVR-CNN \cite{zareian2021open}). While other high-performing open-vocabulary detectors since OVR-CNN have been proposed, they distill and prompt knowledge from large VLMs like CLIP \cite{du2022learning, gu2021open, zhong2022regionclip}, which is trained to learn alignment between a whole image and text description. We rather aim to explore region-word alignment \cite{chen2020uniter, kim2021vilt, li2022grounded} as a more \textit{fine-grained, targeted pretraining mechanism for object detection} and thus consider OVR-CNN as our task of study with its region-word grounding objective. Notably, we find that OVR-CNN does not benefit significantly from caption context and show that altering its pretraining objectives can enhance the positive impact of language context in detection.
   

    % P2

    % as a way to have regions grounded to fine-grained categories (\eg red car, blue car). 
    
    % P3
    \noindent \textbf{Contextualized embeddings in VL tasks} We exploit dynamic, contextualized word embeddings with our grounding and contrastive negative sampling strategies. While the use of contextualized embeddings from language models (\eg \cite{devlin2018bert}, \cite{liu2019roberta}) is common in NLP and vision tasks, they have not been well-studied with respect to region-word alignment and detection. With motivation from the study of biases in grounded VL embeddings \cite{ross2020measuring, thrush2022winoground}, we explore how visual regions are sensitive to the words that contextualize object noun embeddings (particularly adjectives) through unsupervised phrase grounding. We also show that using contextualized embeddings in open-vocabulary detection is \textit{nontrivial} and thus provide insights into pros and cons of their use along with strategies to maximize their effectiveness in detection.
    
    Regarding similar vision works that exploit contextualized embeddings, COBE \cite{bertasius2020cobe} trains a fine-grained detector by having it predict a contextualized embedding that represents a given object state (\eg sliced tomato, tomato in a bowl). Our approach differs from COBE in focus (we use attributes rather than more general object states), data (we use noisier captions instead of cleaner instructional video narrations), and level of supervision (we do not use bounding boxes when learning alignment). Alternatively, the phrase grounding task of Gupta \etal \cite{gupta2020contrastive} uses contextualized word embeddings with context-preserving, noun-based negative caption samples. We also leverage noun-based negatives, but rather in conjunction with adjective negatives in order to ground objects with their attributes. \textit{We show that this combination is more effective than just having noun negatives in object detection} and also gives the model fine-grained utility due to its rich semantic understanding.


    \noindent \textbf{Attributes in vision tasks} We focus on object attributes, described through adjectives in captions, as context that can enhance object learning and model capabilities. Past work has focused on attributes with respect to their direct prediction \cite{pham2021learning}, their compositional zero-shot recognition with objects  \cite{saini2022disentangling}, and their use as a bridge between base and novel classes in zero-shot classification \cite{lampert2013attribute, xu2020attribute}. Specifically in localization tasks, attributes have served as signals to spatially constrain object learning \cite{jerbi2020learning,xiao2017weakly}. Our work is unique as we investigate the use of attributes in \textit{pretraining for open-vocabulary object detection}. Rather than enforce strict spatial constraints, we leverage a contrastive mechanism to encourage the model to gain sensitivity to object attributes, which we show leads to benefits in detection. 

    
    
    %\cite{datta2019align2ground,gupta2020contrastive, plummer2017phrase, li2022adapting, parcalabescu2020exploring, wang2019phrase, xiao2017weakly}. 
    
    
   % General strategies for phrase grounding learn by training directly on region-phrase correspondences (\eg \cite{plummer2017phrase}), using weak supervision with image-text data and off-the-shelf detectors \cite{datta2019align2ground, gupta2020contrastive, xiao2017weakly}, and learning without matching image-text data through the use of external knowledge \cite{wang2019phrase, parcalabescu2020exploring} or adaptation of CLIP \cite{li2022adapting}.


   
        
   
    
    % \cite{shahmohammadi2022language}, which has studied the impact of visual grounding to contextualized word embeddings for NLP tasks. We rather study the impact of visual grounding to contextualized embeddings for the traditionally vision-based task of object detection. 

 
    % A major component of our analysis is unsupervised phrase grounding. General strategies for phrase grounding have learned through training directly on region-phrase correspondences (\eg \cite{plummer2017phrase}), using weak supervision with image-text data and off-the-shelf detectors \cite{datta2019align2ground, gupta2020contrastive, xiao2017weakly}, and learning without matching image-text data through the use of external knowledge \cite{wang2019phrase, parcalabescu2020exploring} or adaptation of CLIP \cite{li2022adapting}. Methods are often compared on datasets such as Flickr30K Entities \cite{plummer2015flickr30k}, ReferItGame \cite{kazemzadeh2014referitgame},  Visual Genome \cite{krishna2017visual}, and COCO Captions \cite{chen2015microsoft} in terms of accuracy and/or recall@$k$. Phrase grounding is involved in our work as region-word alignment can be framed as a weakly supervised phrase grounding pretext task. We note that this learning differs from many of the dedicated phrase grounding methods as \textit{no bounding boxes are used to learn region-word alignment}. 

   %   Phrase grounding is a multimodal task that involves localizing bounding box regions in an image using a textual description.
    
   % Another manner in which we use phrase grounding is as an unsupervised evaluation task to gauge visual embedding sensitivity to adjectives during grounding. 
    
    
    %For this purpose, we adapt the phrase grounding task used to study OVR-CNN in \cite{nebbia2022doubling}. In this approach, an image $I$ is tokenized into regions, a caption $C$ is tokenized into words, and an attention coefficient $a_{i,j}$ is computed for each region-word token pair. For a given caption token $j'$, if it corresponds to a COCO object term, one or more bounding boxes are generated from the connected components of the binary map of $a_{i,j}$ such that $a_{i,j'}$ $\geq th_{attn}$. These boxes serve as unsupervised phrase grounding predictions for a given object. Then for all captions which mention that COCO object, these boxes are compared versus the ground-truth boxes, producing a value for mAP@$t$. Our adaptation differs from this approach by instead grounding using similarity to contextualized word embeddings. We also provide a unique method to measure the sensitivity of visual embeddings to the attributes in contextualized word embeddings.

