\section{Approach}

    \input{figures/approach_fig}

    In typical uses of language for detection, visual regions are grounded to caption word embeddings in pretraining, and word embeddings for \textit{object nouns} are used for supervision or inference downstream (\eg in an open-vocabulary manner). We hypothesize that the \textit{other} language elements in captions (\eg adjectives, prepositions, verbs, other nouns) may impact detection, as pretraining can leverage the rich information they provide about an object's appearance, qualities, and state. We aim to explore this underlooked aspect of vision-language pretraining for detection through studying context in existing region-word alignment work and exploring strategies to \textit{enhance} the use of context. 
    
\subsection{Baseline region-word alignment method}

    We study \textit{region-word alignment} to learn grounding for object detection and model an approach based on OVR-CNN \cite{zareian2021open}, a state-of-the-art method for open-vocabulary detection. OVR-CNN is modeled as a PixelBERT-like \cite{huang2020pixel} architecture with three encoders in pretraining: ResNet-50 \cite{he2016deep} for visual features, a pretrained BERT \cite{devlin2018bert} for text features, and a 6-layer BERT-like model for multimodal features. The total loss $\mathcal{L}(I, C)$ comprises four objectives: masked language modeling ($\mathcal{L}_{MLM}$), image-to-text matching ($\mathcal{L}_{ITM}$), and two contrastive grounding terms ($\mathcal{L}_{G}(C)$ and $\mathcal{L}_{G}(I)$) which are defined for an image $I$ and caption $C$ in Eqs.~\ref{groundingloss_image} and \ref{groundingloss_caption}: 

   \begin{equation} \label{groundingloss_image}
	{ L_G(I) = - \log \frac{\exp \langle I, C\rangle _ G}{\sum_{C'\in \mathcal{B_C} } \exp \langle I, C'\rangle _ G} } 
    \end{equation}

    \begin{equation}    \label{groundingloss_caption}
    	{ L_G(C) = - \log \frac{\exp \langle I, C\rangle _ G}{\sum_{I'\in \mathcal{B_I}} \exp \langle I', C\rangle _ G} } 
    \end{equation}

    These contrastive objectives operate over a batch of captions $\mathcal{B_C}$ and images $\mathcal{B_I}$, maximizing a grounding score $\langle I,C \rangle _{G}$ for corresponding image-caption pairs (minimizing otherwise). Eqs.~\ref{groundingscore_contextfree} and \ref{activitycoeff} break down this score:

    \begin{equation}        
        \label{groundingscore_contextfree}
            {\langle I,C \rangle _{G} = \frac{1}{n_C}\sum_{j=1}^{n_C}  \sum_{i=1}^{n_I} a_{i,j} \langle e_{i}^{I}, e_{j}^{C} \rangle _{L} } 
    \end{equation}

    \begin{equation}        
        \label{activitycoeff}
            { a_{i,j} = \frac{\exp \langle e_{i}^{I}, e_{j}^{C} \rangle}{\sum_{i'=1}^{n_I} \exp \langle e_{i'}^{I}, e_{j}^{C} \rangle _{L}} } 
    \end{equation}

    Here, the dot product $\langle$$e_{i}^{I}$, $e_{j}^{C}$$\rangle_L$ represents the similarity between an image region embedding $e_{i}^{I}$ and a caption word embedding $e_{j}^{C}$. The attention coefficient $a_{i,j}$ represents this similarity normalized over all $n_I$ image tokens. To compute $e_{i}^{I}$, an $h \times w$ image $I$ is tokenized into $h/32 \times w/32$ region tokens. These tokens are processed by the image encoder to produce region embeddings $r_{i}^{I}$, which then are projected into the language embedding space with a vision-to-language (V2L) layer to become $e_{i}^{I}$. To compute $e_{j}^{C}$, a caption $C$ is tokenized then converted using the input embedding layer of BERT into a length $n_C$ vector of such word embeddings. During training, BERT processes each $e_{j}^{C}$ to produce a contextualized word embedding $f_{j}^{C}$, which is further input along with $e_{i}^{I}$ into the multimodal encoder to produce multimodal features $m_{i}^{I}$ and $m_{j}^{C}$. We differentiate between language embeddings by referring to $e_{j}^{C}$ as \textit{context-free} (since it does not change depending on language context) and $f_{j}^{C}$ as \textit{contextualized}. 

    For downstream open-vocabulary detection, the visual encoder and V2L layer are transferred into Faster R-CNN \cite{ren2015faster}. When region proposal ($r_{i}^{I}$) features are computed, they are projected by the V2L layer into  $e_{i}^{I}$. Classification is performed in training and inference by computing the similarity between $e_{i}^{I}$ and each $e_{k}^{\mathcal{C}}$, which represents the \textit{class embedding} for each concept $k$ in a set $\mathcal{C}$ (in addition to a background embedding). We emphasize that by default the V2L layer is frozen in finetuning, and BERT is frozen in pretraining, while other architectural components are trained. For more on OVR-CNN, refer to \cite{zareian2021open}. We illustrate our novel context enhancement strategies (outlined in Section \ref{enhance}) added to this approach in Fig. \ref{approach_fig}.   


\subsection{Enhancing context in region-word alignment}
\label{enhance}

    We describe two primary strategies to leverage language context in pretraining: contextualized grounding and an adjective-noun contrastive negative sampling strategy that increases attention to objects with their attributes. 
    
    \noindent \textbf{Contextualized grounding} Our first strategy stems from the observation that detection methods often use \textit{static} word embeddings to describe object classes. For instance, OVR-CNN's pretraining task grounds regions to the input word embeddings of BERT ($e_{j}^{C}$). These embeddings are in a sense ``context-free", as they do not change with surrounding language context (\eg \textit{orange} has the same embedding in the captions ``\underline{orange} basketball" and ``eating an \underline{orange}"). We hypothesize this type of grounding can limit detection success due to non-optimal alignment of different concepts. 
    We additionally posit that such grounding prevents the model from learning fine-grained concepts (\eg a \textit{red car}) in a structurally sound way (such as through language contextualization), and the model instead captures \textit{red} and \textit{car} separately akin to bag-of-words (little distinction between ``red car'' and ``blue car on a red background'').
    %We additionally posit that such grounding prevents the model from learning fine-grained concepts based on language contextualization (\eg a \textit{red car} region could not be aligned to the \textit{car} embedding in ``red \underline{car}").
    
    To address these challenges, we alter the grounding process to learn to align regions with \textit{contextualized} word embeddings. In OVR-CNN, this strategy corresponds to altering Eq.~\ref{groundingscore_contextfree} to use the $f_{j}^{C}$ outputs from BERT, changing the projected image embedding to the contextualized space ($e_{i}^{I}$ to $f_{i}^{I}$), and changing
    the class embedding $e_{k}^{\mathcal{C}}$ to $f_{k}^{\mathcal{C}}$ in detection. The contextualized grounding score $\langle I,C \rangle _{G}$ can be thus computed as:

    \begin{equation} \label{groundingscore_contextualized}
    	{\langle I,C \rangle _{G} = \frac{1}{n_C}\sum_{j=1}^{n_C}  \sum_{i=1}^{n_I} a_{i,j} \langle f_{i}^{I}, f_{j}^{C} \rangle _{L} } 
	\end{equation}

    A key observation is that visual regions for an object noun are grounded to a \textit{collection} of embeddings instead of one single embedding since contextualized embeddings are \textit{dynamic} (i.e. changing with surrounding words in the caption). This consideration causes naive integration of such embeddings into detection to result in poor performance. We propose a series of strategies to enable effective use of contextualized embeddings in detection: (1) using a prompt ``a/an \underline{$<$categoryName$>$}." in the class embeddings  (similar to CLIP \cite{radford2021learning}), (2) allowing the language encoder to update when grounding is learned (pretraining), and (3) allowing the V2L layer to update in finetuning. These strategies provide the flexibility needed in the training process to benefit from the use of contextualized embeddings.

    
%as opposed to the possible incorrect regions (\eg \textit{street}, \textit{field}, respectively).  

    \noindent \textbf{Increasing sensitivity to objects with their attributes} In Sec. \ref{results_stuff}, it is shown that detection does not benefit significantly from adjectives in pretraining. We reason this should be the case, as adjectives provide signals that can help guide the grounding of objects to the correct regions (\eg a \textit{car} should be grounded to the \textit{red} region with the caption ``a red car"; similar is true for the caption ``a striped zebra"). We hypothesize that the model can leverage these attributes by learning to align visual object regions to be especially similar to the \textit{contextualized embedding of that object with its corresponding attribute} (\eg close to ``a red \underline{car}" but not ``a blue \underline{car}"), in effect learning such fine-grained concept. However, we find that \textit{contextualized grounding is not attribute-sensitive in this way by default}. 
    
    We thus propose a strategy that leverages \textit{negative caption samples} in contrastive learning to make the model more attribute-sensitive. As shown in Table \ref{example_caps}, for a given caption, we keep all words the same, but replace the adjective modifying an object with another \textit{plausible} adjective (i.e. one that is mentioned with that object in the caption corpus); using a random adjective performed worse. In this way, the model needs to pay attention to attributes of objects (\eg \textit{green}, \textit{rotten} for \textit{bananas}). As we want the model to ground to objects \textit{contextualized by their attributes}, we also leverage a noun negative, just replacing the noun of an object with a random one from the corpus. These two negatives together encourage the model to consider fine-grained concepts as a whole (\eg \textit{green banana}, \textit{rotten banana}). Implementing this strategy can be accomplished by using the grounding scores in Eq.~\ref{groundingscore_contextualized} and  integrating a batch of negative captions $B_{N}$ into the grounding loss in Eq.~\ref{groundingloss_image}:
    

    \input{tables/example_negatives}

    

    \begin{equation}        \label{groundingloss_image_with_negatives}
    	{ L_G(I) = - \log \frac{\exp \langle I, C\rangle _ G}{\sum_{C'\in \mathcal{B_C} + \mathcal{B_N} } \exp \langle I, C'\rangle _ G} } 
	\end{equation}

   
    
  
    


\subsection{Identifying objects and their context}
    \label{extraction_approach}

    %Outlining how we consider context for our experiments, 
    We define a vocabulary $\mathcal{V}$ to be the nouns which correspond to objects in a dataset $\mathcal{D}$. For our study, $\mathcal{D}$ is selected to be COCO Captions \cite{chen2015microsoft}, with 118,287 images and 5 captions per image. We build $\mathcal{V}$ from the synonym list of all COCO class names provided in \cite{lu2018neural}, with plural terms also added. With this methodology, $\mathcal{V}$ captures various terms for each COCO class, such as \textit{jet}, \textit{aircraft}, and \textit{planes} for \textit{airplane}. We further extract three primary caption components that we reason can impact object alignment (through grounding or multimodal objectives): (1) \textbf{adjectives} (a caption can describe a car as \textit{red} and \textit{shiny}, or a knife as \textit{sharp} and \textit{metal}); (2) \textbf{verb phrases} (a person can be \textit{playing tennis} or \textit{feeding a pet}); and (3) \textbf{prepositional phrases} (a baseball can be \textit{in a glove} or \textit{on the field}). We isolate these components with the Python SpaCy library \cite{Honnibal_spaCy_Industrial-strength_Natural_2020}, using a dependency parser to gather ``amod" (adjectival modifiers) and the Berkeley neural parser \cite{kitaev2018constituency} for prepositional and verb phrases. 

    %Regarding usage of this context, we conduct experiments where 
    In experiments, we remove each context component during pretraining and measure the effect in detection. As removing constituents that contain object nouns in $\mathcal{V}$ would likely hurt detection (i.e. weak supervision would be unused), we ignore phrases or adjectives that contain any words in $\mathcal{V}$ in these experiments. The extracted ``amod" are additionally used in our unsupervised phrase grounding analysis of attribute sensitivity, as well as our adjective and noun negative sampling strategy. In particular, negatives are used for captions in $\mathcal{D}$ which contain a term in $\mathcal{V}$ that is modified by an adjective. The terms in $\mathcal{V}$ capture both base and target classes, allowing us to evaluate our strategy in open-vocabulary detection with both cases. 
    %This method is also notably applicable even if $\mathcal{V}$ is not restricted to COCO.
  

        

        
        
     