\section{Related Work}


\paragraph{3D Surface Reconstruction} 
Classical approaches for surface reconstruction from point clouds \cite{berger2017survey} are either parametric \cite{amenta1998new} or implicit with mostly linear function bases, \eg, grids or radial basis functions \ \cite{carr2001reconstruction,kazhdan2006poisson}. 
Recent works have developed methods for surface reconstruction using neural networks, which consist of a non-linear function space, making these methods non-convex. Those methods differ by how they choose to represent the 3D reconstruction. \cite{groueix2018papier,williams2019deep,hanocka2020point2mesh} employ a parametric point of view. Such discretizations do not yield watertight reconstruction, and/or lack topological detail. A more flexible solution is the Implicit Neural Representation (INR). INRs based methods \cite{park2019deepsdf,mescheder2019occupancy,atzmon2019sal,chen2019learning,gropp2020implicit,sitzmann2020implicit,lipman2021phase,ben2022digs} show great progress in leveraging the inductive bias of MLPs to represent smooth surfaces, using additional losses and regularizers. \rebb{For example, \cite{lipman2021phase} introduce a perturbed Dirichlet loss (\ie, norm of gradient) to push for a unique and regular occupancy solution; \cite{ben2022digs} incorporates a Divergence loss (\ie, absolute value of the divergence of the gradient of trained distance field) for encouraging the learned field to  resemble a gradient field of a true distance functions. } 
Neural Spline \cite{williams2021neural} does not use neural networks directly, rather derive a kernel-based formulation arising from infinitely-wide shallow networks. Shape As points (SAP) \cite{peng2021shape} represent the surface using a differentiable Poisson solver and contouring process. 




\paragraph{Grid-based representations} Recent works suggested to reduce, completely or partially, the use of neural networks in implicit representations and replacing it with a grid-based data structure. This is due to the heavy computational resources required in optimizing and evaluating neural networks. Plenoxels \cite{yu2021plenoxels} propose a view-dependent sparse voxel model and show comparable results to NeRF \cite{mildenhall2020nerf} and a speedup of two orders of magnitude. Neural Geometric Level of Detail \cite{takikawa2021nglod} uses an octree-based feature volume and a small MLP to represent SDF.  \cite{mueller2022instant} shows fast training of INR's using a small neural network augmented by a multiresolution hash table with trainable features. Similar to DeepSDF \cite{park2019deepsdf}, both work used 3D supervision for learning the SDF. 