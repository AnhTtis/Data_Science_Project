
\renewcommand*\thetable{\Roman{table}}
\renewcommand*\thefigure{\Roman{figure}}
\setcounter{table}{0}
\setcounter{figure}{0}

\section*{Appendix}
\appendix

\section{Computing  $\nabla f(w_I)$}

We will use the notation set in Section 3. For the losses in Eq.~3 in the main paper (the normal term) and Eq.~11 we require computing $\nabla f(w_I)$, where $w_I$ is the center of the voxel of interest. For simplicity we will consider the voxel $[0,h]^3$. The 8 trilinear basis functions for this voxel are \begin{equation}
    \varphi_{abc}(x,y,z) = 
    \frac{1}{h^3}\begin{Bmatrix}
x & \text{if }a=0\\ 
h-x & \text{if }a=1 
\end{Bmatrix}\cdot
\begin{Bmatrix}
y & \text{if }b=0\\ 
h-y & \text{if }b=1 
\end{Bmatrix}\cdot
\begin{Bmatrix}
z & \text{if }c=0\\ 
h-z & \text{if }c=1 
\end{Bmatrix}
\end{equation}
where the corners of the voxel are indexed by $a,b,c\in\set{0,1}$. Given function values at these corner nodes, $f_{abc}$, the trilinear interpolant of $f$ inside the voxel is
\begin{equation}
    f_{abc}(x,y,z) = \sum_{a,b,c\in\set{0,1}}f_{abc} \varphi_{abc}(x,y,z).
\end{equation}
Taking the gradient of this interpolant we get
\begin{equation}
    \nabla f_{abc}(x,y,z) = \sum_{a,b,c\in\set{0,1}}f_{abc} \nabla \varphi_{abc}(x,y,z)
\end{equation}
and for the center voxel point,  $(x,y,z)=\frac{1}{2}(h,h,h)$, we have
\begin{equation}
    \nabla \varphi_{abc}\parr{\frac{h}{2},\frac{h}{2},\frac{h}{2}} = \frac{1}{4h}\brac{(-1)^a, (-1)^b, (-1)^c}.
\end{equation}
Similarly we can compute the gradient at an arbitrary point $(x, y, z)$ inside a voxel.



\section{Implementation Details}\label{s:implementation_details}
For all experiments we follow a coarse-to-fine approach. We start optimizing at a $64\times64\times64$ grid resolution, then scale up to $128\times128\times128$ and finish at $256\times256\times256$. At each scale up we initialize the higher resolution grid values, $f_I$, by using trilinear interpolation within the voxels of the coarser grid. 
After each up-sampling, we prune grid voxels by removing those with an SDF value higher/lower than threshold $\pm t
$, where $t\in \set{0.4, 0.9}$ enabling faster training and lower memory consumption; this is especially useful at the highest resolution grid. 
For 30 and 21 minutes running time budgets we prune with $t=0.9$, and for 15 and 8 minutes with $t=0.4$. For 30 minutes budget we perform 5 epochs at 64 resolution, 5 epochs at 128 and 3 at 256. For the rest of running time budgets we perform 2 epochs at each resolution, except for the 8 minutes budget, where at 256 resolution we perform only 1 epoch.
\rebb{Each epoch consists of 12800 iterations. At each training iteration the batch is composed by sampling random 10\% of the \emph{active} voxels (those which are left after pruning).}

For all the final experiments we set $\lambda_p=0.1$, $\lambda_n=10^{-5}$, $\lambda_v=1e-4$, $\lambda_c=1e-6$, $\epsilon=1e-2$. As for the optimizer, we use Adam \citep{kingma2014adam} with a constant learning rate of $0.001$, $\beta_1 = 0.9$ and $\beta_2 = 0.999$. All models are trained with a single NVIDIA Quadro GP-100 GPU.

\rebb{We did a grid search for all the hyper-parameters in the range of $10^{-6}$ to $10^{-1}$ with multiplicative steps of $10^{-1}$. We observed minimal performance difference. For all benchmark datasets we use the exact same hyper-parameters. More specifically, for the two proposed new losses -- Viscosity and Coarea -- we observe no performance change in the ranges $[5e-3, 5e-2]$ and $[5e-7, 5e-6]$, respectively, which allows for consistency across scenes with fixed hyper-parameters (see Fig. 7 and 8).}

We will publish the source code which reproduces the experimental results upon the paper acceptance.



\section{Training time}

In this section, we present qualitative and quantitative results of VisCo for different running time budgets. We experiment with the running time versus reconstruction quality trade-offs and show that short time training produces comparable reconstruction quality to longer time training.
In Tab. 1 we show quantitative results and in Fig. 1 qualitative results. Note that reducing the running time from 30 mins to 8 mins only marginally reduces reconstruction metrics, while qualitatively produces indistinguishable reconstruction results. The different running times versions were created mostly by reducing the number of epochs per resolution from $5$ down to $2$ (see more details in Sec.~\ref{s:implementation_details}). We strongly believe that further significant speedups are possible with a more efficient implementation. 

\rebb{
Below we report average time and memory footprint required for a single training iteration on NVIDIA Quadro GP100 GPU. Because of the pruning applied to the grid, we need to learn only a sparse set of the grid values (we call them \emph{active}).   
\begin{itemize}
\item $64^3$ resolution ($57\%$ of the grid values are active): 2.3 msec, 975MB VRAM
\item $128^3$ resolution ($31\%$ of the grid values are active): 8.6 msec, 1070MB VRAM
\item $256^3$ resolution ($30\%$ of the grid values are active): 25.8 msec, 1650MB VRAM
\end{itemize}
For neural networks (INRs) every point evaluation requires forward and backward in a network involving all networkâ€™s parameters in general, while for a grid we only require nearby grid function values (learnable parameters). Typical iteration times for NN (taken from DiGS) are:
\begin{itemize}
\item 66.5K params: 5.2-12.0 msec
\item 2.1M params: 17.5 msec
\end{itemize}

}
\input{tables/times}

\begin{figure}[!t]%
  \begin{center}
  \includegraphics[width=\textwidth]{figs/time.png}
  \end{center}\vspace{-20pt}
  \caption{Qualitative results of VisCo for different training time budgets on the surface reconstruction benchmark~\cite{williams2019deep}. Note that reduction of the training time does not result in inferior reconstruction. The models trained for 30 mins and 8 mins produce indistinguishable reconstruction results.}\label{fig:time}%
\end{figure}


\rebb{
\section{Daratech Coarea Effect}\label{s:coareavs}
In this section we further study why in Tab. 3,  Daratesh seem to have better reconstruction w/o Coarea loss. Visual inspection reveals higher qualitative result for the mesh reconstructed with the Coarea loss although it has a higher quantiative error. We observe small holes when removing the Coarea loss, see Fig.~\ref{fig:coareavs}.
}

\begin{figure}
  \begin{center}
  \includegraphics[width=0.9\textwidth]{figs/fig_coarea.png}
  \end{center}
  \caption{\rebb{Visual comparison for Daratech between all loses vs. w/o Coarea. Reconstructed meshes from Tab. 3. Note small holes when removing the Coarea loss.}}\label{fig:coareavs}
\end{figure}