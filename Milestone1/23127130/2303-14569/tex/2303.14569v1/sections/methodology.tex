\section{Method}
\label{s:method}

We consider the 3D euclidean space $\Real^3$ with points $p=(x,y,z)\in\Real^3$. We discretize the unit cube $\gC=[0,1]^3$ with a 3D voxel grid $\gG=\set{p_I}$, with nodes $p_I$ indexed by $I=(i,j,k)$, $i,j,k\in [n]=\set{1,\ldots,n}$, \ie, $p_I=(x_{ijk},y_{ijk},z_{ijk})$. We denote by $h=n^{-1}$, and by $N=n^3$ the total number of nodes.   
We represent our reconstructed surface as a zero level of a scalar function $f$ defined over the cube $\gC$. $f$ is defined by prescribing its values at the grid's nodes $f_I\in\Real$ and trilinear interpolating in each voxel. We will denote by $f(p)$ the interpolated value at point $p$. 

Given an input point cloud consisting of $m$ points $q_k\in\Real^3$ with or without (unit norm) normals $n_k\in \Real^3$, $k\in [m]$, our goal is to compute $f$ so that its zero level set approximates the unknown surface, \ie, 
\begin{equation}
    \gS = \set{p\in\gC \ \vert \ f(p)=0}.
\end{equation}
Our approach to compute $f$ is to minimize a loss function of the form
\begin{equation}
    \gL = \gL_{\text{data}} + \gL_{\text{prior}}
\end{equation}
where 
\begin{equation}\label{e:loss_data}
    \gL_{\text{data}} = \frac{\lambda_{\text{p}}}{m}\sum_{k=1}^m \abs{f(q_k)}^2 + \frac{\lambda_{\text{n}}}{m}\sum_{k=1}^m \norm{\nabla f(q_k) - n_k}^2
\end{equation}
where $\norm{\cdot}$ is the standard euclidean norm in $\Real^3$, $\nabla f(p) \in \Real^3$ is the gradient of $f$ sampled at point $p$. Note that $\nabla f$ is defined in interior of voxels, which is generically where the input points $q_k$ resides. $\gL_{\text{data}}$ is the standard data loss encouraging the zero level to pass through the input points $q_k$, and its normals (defined by gradients of $f$) to coincide with input normals $n_k$. 

The prior, $\gL_{\text{prior}}$, is the main contribution of this work, where we combine two novel losses,
\begin{equation}
    \gL_{\text{prior}} = \lambda_{\text{v}} \gL_{\text{viscosity}} + \lambda_{\text{c}} \gL_{\text{coarea}}
\end{equation}
Intuitively, the viscosity loss optimizes for a smooth Signed Distance Function (SDF) solutions, avoiding auxiliary bad minima of the Eikonal equation, while the coarea loss strives to minimize the area of the zero level surface. Our loss has $4$ hyper-parameters $\lambda_{\text{p}},\lambda_{\text{n}},\lambda_{\text{v}},\lambda_{\text{c}}$. We provide more details on these priors next. 


\subsection{Viscosity Loss}\label{ss:viscosity_loss}
The goal of the viscosity loss is to make $f$ approximate an SDF over $\gC$. Given boundary conditions asking $f$ to vanish on some closed compact surface $\gS$, the SDF solves the Eikonal equation PDE, \ie, $\norm{\nabla f(p)}=1$, in a certain well defined sense (viscosity). This motivated some previous work to directly optimize the Eikonal loss \citep{gropp2020implicit,sitzmann2020implicit}
\begin{equation}\label{e:loss_eikonal}
    \gL_{\text{eikonal}} = \int_\gC \Big (\norm{\nabla f(p)}-1\Big )^2 dp
\end{equation}
\begin{wrapfigure}[14]{r}{0.28\textwidth}\vspace{-15pt}
  \begin{center}
    \includegraphics[width=0.25\textwidth]{figs/illustrations/eikonl_1d.png}
  \end{center}
  \caption{Two global minimizers of the Eikonal loss over a grid in 1D. Top solution is not an SDF. }\label{fig:eikonal_1d}
\end{wrapfigure}
Unfortunately, the Eikonal loss has many undesirable minima which are not SDFs. Figure \ref{fig:eikonal_1d} shows a 1D example: both depicted solutions (denoted $f$) vanish at the input points $q_1,q_2$ (black points) and globally minimize the Eikonal loss over the grid (grid points are shown in blue). The INR works mentioned above use neural networks for representing $f$ which injects an inductive bias avoiding these bad minima, however on grids, minimizing \eqref{e:loss_eikonal} cannot avoid these solutions. See, \eg, middle column in Figure \ref{fig:teaser}. 

More classical Eikonal solvers do work with grids however use mostly fast marching or sweeping methods \citep{osher1988fronts,sethian1996fast,zhao2005fast,chacon2012fast}. Namely, use a special discretization of the Eikonal equation favoring the viscosity  solution of the Eikonal \cite{rouy1992viscosity}, and update node values according to a moving front \cite{sethian1996fast}. Since this discretization is up-wind (will only propagate values in one direction) and requires choosing the maximal among its solution, its success in adaptation to a loss is not clear. 

We use a different approach to build a loss favoring SDF solutions over grids motivated by the vanishing viscosity method \cite{crandall1983viscosity}. Namely, adding to the Eikonal PDE a small perturbation of the Laplacian of $f$ (denoted by $\Delta f$), \ie, $\norm{\nabla f(p)}-1 - \eps\Delta f(p)=0$, makes the PDE semi-linear elliptic \citep{calder2018lecture}, and hence with suitable boundary conditions it is uniquely solvable inside $\gS$ with a smooth solution, approaching the viscosity positive distance function to the boundary as $\eps\too 0$. Similarly, for $1-\norm{\nabla f(p)} - \eps \Delta f(p)=0$ the solution approaches the negative distance function inside the domain. 
Motivated by the vanishing viscosity principle we suggest the following viscosity loss:
\begin{equation}\label{e:loss_viscosity_eikonal}
\gL_{\text{viscosity}} = \int_\gC \Big((\norm{\nabla f (p)}-1)\mathrm{sign}(f(p)) - \eps \Delta f(p)\Big)^2 dp
\end{equation}
We discretize this loss over the grid $\gG$ by replacing the first order derivatives and second order derivatives with symmetric finite  differences, \ie,
\begin{align*}
    D_x f_I=D_x f_{i,j,k} = \frac{f_{i+1,j,k}-f_{i-1,j,k}}{2h}, \quad D^2_x f_I = D^2_x f_{i,j,k}=\frac{f_{i+1,j,k}-2f_{i,j,k}+f_{i-1,j,k}}{h^2}
\end{align*}
and similarly for $D_y$ and $D_z$. We use these discrete operators to approximate the gradient $\widehat{\nabla} f(p_I) = (D_x f_I, D_y f_I, D_z f_I)$ and Laplacian $\widehat{\Delta}f(p_I) = D_x^2f_I + D_y^2 f_I + D_z^2 f_I$. The discretized viscosity loss now takes the form
\begin{equation}
    \widehat{\gL}_{\text{viscosity}} = \frac{1}{N}\sum_{I} \Big((\|\widehat{\nabla} f (p_I)\|-1)\mathrm{sign}(f(p_I)) - \eps \widehat{\Delta} f(p_I)\Big)^2
\end{equation}



\subsection{Coarea loss}\label{ss:coarea_loss}
The coarea loss is approximating the area of the zero level set, and therefore incorporating it in the optimization pushes the reconstructed surface to be economic in area. 

First, similarly to  \citep{yariv2021volume} we use the centered Laplace CDF
\begin{equation}
   \Psi\beta(s)= \begin{cases}
   \frac{1}{2}\exp\parr{\frac{s}{\beta}} & s\leq 0 \\ 1-\frac{1}{2}\exp\parr{-\frac{s}{\beta}} & s\geq  0
   \end{cases}
\end{equation} to transform the SDF $f$ to a smooth approximation of the indicator function:
\begin{equation}
    \chi_\beta(p)=\Psi\beta (-f(p))
\end{equation}
As $\beta\too 0$, $\chi_\beta$ converges to an indicator function leading to $1$ inside $\gS$ and $0$ outside. The coarea loss is defined as 
\begin{equation}
    \gL_{\text{coarea}} = \int_\gC \norm{\nabla \chi_\beta (p)} dp
\end{equation}
To understand why this loss approximates the area of $\gS$ we can use the coarea formula \citep{rindler2018calculus}:
\begin{equation}\label{e:coarea}
    \int_\gC \norm{\nabla \chi_\beta(p)}dp = \int_{-\infty}^{\infty} \mathrm{area}(\chi_\beta^{-1}(s))ds,
\end{equation}
where $\chi_\beta^{-1}(s)=\set{p\ \vert \ \chi_\beta(p)=s}$ is the preimage of the value $s$. Since $\chi_x(p)\in [0,1]$ the r.h.s.~integral can be restricted to the interval $[0,1]$, and therefore the coarea loss averages the area of the level sets of $\chi_\beta$. Next,  $$\chi_\beta^{-1}(s)= \set{p\ \vert \ \Psi\beta (-f(p)) = s } = \{p\ \vert \ f(p) = -\Psi\beta^{-1} (s) \} = f^{-1}(-\Psi\beta^{-1} (s)),$$
\begin{wrapfigure}[11]{r}{0.28\textwidth}\vspace{-20pt}
  \begin{center}
  \includegraphics[width=0.25\textwidth]{figs/semi.png}
  \end{center}
  \caption{Reconstruction of a semisphere point cloud (white dots) without (left) and with (right) coarea loss. }\label{fig:coarea_semisphere}
\end{wrapfigure}

which shows that the level set $s\in (0,1)$ of $\chi_\beta$ is the level set $-\Psi\beta^{-1}(s)$ of the SDF $f$. As $\beta\too 0$, $-\Psi\beta^{-1}(s)\too 0$ for all $s\in (0,1)$ (and uniformly in $(\eps,1-\eps)$ for fixed $\eps>0$). Therefore the average of the level set area (\ie, the r.h.s.~of \eqref{e:coarea}) converges to the area of $f^{-1}(0)=\gS$. Figure \ref{fig:teaser} (right) shows how removing the coarea loss introduces an extraneous zero level set, and hence results in an undesired surface part. Figure \ref{fig:coarea_semisphere} shows a comparison of a reconstruction of semisphere with and without coarea. In the experiments section we provide more ablation tests with the coarea and viscosity losses.

To discretize the coarea loss we let $w_I$ denote the centers of grid's voxels, and note that $\nabla \chi_\beta(w_I) = \Phi_\beta(-f(w_I))\nabla f(w_I)$, where 
\begin{equation*}
    \Phi_\beta(s) = \frac{1}{2\beta}\exp\parr{\frac{\abs{s}}{\beta}}
\end{equation*}
is the PDF of the Laplace distribution, and $\nabla f(w_I)$ is computed as a linear combination of the voxel's corner values $f_{I_1},\ldots,f_{I_8}$, see more details in the Appendix. We end up with the discretized loss:
\begin{equation}
    \widehat{\gL}_{\text{coarea}} = \frac{1}{N}\sum_{I}\Phi_\beta(-f(w_I))\norm{\nabla f(w_I)}
\end{equation}
This loss is usually incorporated with a small hyper-parameter $\lambda_{\text{c}}$ with the purpose of eliminating redundant surface parts.

