% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{bbm}
\usepackage{dsfont}
\usepackage{algorithm,algcompatible}
\usepackage{algpseudocode}
\usepackage{makecell}
% \usepackage{dblfloatfix}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{5233} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Supplementary Material of Marching-Primitives: \\Shape Abstraction from Signed Distance Function}

\author{Weixiao Liu$^{1, 2}$ \quad Yuwei Wu$^{1}$ \quad Sipu Ruan$^{1}$ \quad Gregory S. Chirikjian$^{1}$\\
$^1$National University of Singapore \quad $^2$Johns Hopkins University\\
% Institution1 address\\
{\tt\small \{mpewxl, yw.wu, ruansp, mpegre\}@nus.edu.sg}
}

\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
In this supplementary material, we provide the details about the derivations, discussions and experiment settings.
In Sec. \ref{sec:approxsdf}, we provide the detailed derivation of the approximation of the SDF of a superquadric.
In Sec. \ref{sec:initialization}, the primitive initialization strategy in the connectivity marching step is detailed.
In Sec. \ref{sec:derivation}, we provide the derivation of the probabilistic primitive marching step.
Furthermore, Sec. \ref{sec:removal} elaborates the fail-safe primitive removal criterion.
The overview of the Marching-Primitive algorithm is summarized into pseudo-code in Sec. \ref{sec:pseudo-code}.
Finally, in Sec. \ref{sec:implementation}, we detail the experiment implementation and show more qualitative examples.
  
\end{abstract}

%%%%%%%%% BODY TEXT
%-------------------------------------------------------------------------
\section{Approximation of superquadric SDF}
\label{sec:approxsdf}

Recall the signed radial distance of a point $\mathbf{x}_i$ to a general-posed superquadric in Eq. (2) of the paper
\begin{equation}
% d_{\boldsymbol{\theta}}(\mathbf{x}_i)=\Big\|\left( 1 - f^{-\frac{\epsilon_1}{2}}(g^{-1}\circ\mathbf{x}_i) \right)(g^{-1}\circ\mathbf{x}_i)\Big\|_{2}
d_{\boldsymbol{\theta}}(\mathbf{x}_i)=\left( 1 - f^{-\frac{\epsilon_1}{2}}(g^{-1}\circ\mathbf{x}_i) \right)\|g^{-1}\circ\mathbf{x}_i\|_{2}
\label{eqn:radial_distance_re}
\end{equation}
The derivation is as follows (with an illustration shown in Fig.\ref{fig:approxSDF}).
The radial distance of a point $\mathbf{x}_i$ to the surface of a superquadric is defined as 
\begin{equation}
    \|\mathbf{x}_i-\mathbf{q}_i\|_2
\end{equation}
$\mathbf{q}_i$ is where the vector from the center of the superquadric frame to $\mathbf{x}_i$ intersects the surface.
Therefore, when viewed from the superquadric frame $g$, the vectors $g^{-1}\circ \mathbf{q}_i$ and $g^{-1}\circ \mathbf{x}_i$ are colinear, \ie 
\begin{equation}
    g^{-1}\circ \mathbf{q}_i = \alpha (g^{-1}\circ \mathbf{x}_i)\text{, where }\alpha\in \mathbb{R}
\end{equation}
Note that $g^{-1}\circ \mathbf{q}_i$ lies on the surface of superquadric.
Thus, substituting it into the implicit equation of the superquadric (Eq. (1) in the paper), we obtain 
\begin{equation}
    \alpha = f^{-\frac{\epsilon_1}{2}}(g^{-1}\circ\mathbf{x}_i)
\end{equation}
Therefore,
\begin{equation}
\begin{aligned}
    \|\mathbf{x}_i-\mathbf{q}_i\|_2=&\|g^{-1}\circ\mathbf{x}_i-g^{-1}\circ\mathbf{q}_i\|_2\\
    =&\|(1-\alpha)g^{-1}\circ\mathbf{x}_i\|_2
\end{aligned}
\end{equation}
Considering the inside/outside of $\mathbf{x}_i$ relative to the superquadric surface, the signed radial distance is thus Eq.\eqref{eqn:radial_distance_re}.
%------------------------------------------
\begin{figure} [!ht]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/approxSDF_compressed.pdf} %0.95
    \caption{Signed distance approximated by signed radial distance.}
    \label{fig:approxSDF}
    \vspace{-0.5cm}
\end{figure}
%------------------------------------------

\section{Primitive Initialization}
\label{sec:initialization}
In Sec. 3.1 of the paper, the geometric primitive is initialized as an ellipsoid for each volume of interest (VOIs) detected in the target SDF.
This is achieved by first finding out the smallest bounding-box encompassing the connected voxels that form each of the VOI.
For example, the lengths of a bounding-box are $l_x$, $l_x$ and $l_z$, and the centroid locates at $\mathbf{x}_c$.
Ideally, the primitive is initialized as an ellipsoid centered at $\mathbf{x}_c$ with scales proportional to the lengths correspondingly.
Recall that a superquadric $\boldsymbol{\theta}$ is parameterized by
\begin{equation}
    \boldsymbol{\theta}= \{\epsilon_1, \epsilon_2, a_x, a_y, a_z, \mathbf{R}, \mathbf{t}\}
\end{equation}
Then, the initialized ellipsoid is a special case of the superquadrics
\begin{equation}
    \boldsymbol{\theta}_{init}=\{1, 1, \gamma l_x, \gamma l_y, \gamma l_z, \mathbf{I}, \mathbf{x}_c\}
\end{equation}
where $\gamma$ is the initial scale ratio, $\mathbf{I}$ is the identity rotation matrix.
However, if the VOI is nonconvex, the centroid might lie in the exterior space.
In this situation, it has the risk of activating the auto-degeneration mechanism.
Therefore, instead of $\mathbf{t}=\mathbf{x}_c$, we constrain the initial location $\mathbf{t}$ within the interior of the target shape:
\begin{equation}
    \mathbf{t}=\argmin_{\mathbf{x}_i\in\mathbf{V},d(\mathbf{x}_i)\leq 0}\|\mathbf{x}_i-\mathbf{x}_c\|_2
\end{equation}
where $\mathbf{V}$ is the current set of voxel points, $d(\mathbf{x}_i)$ is the target signed distance evaluated at voxel point $\mathbf{x}_i$.
$\boldsymbol{\theta}_{init}$ works as the initial input to the subsequent probabilistic primitive marching step.
The concepts are visualized in Fig.\ref{fig:conceptsupp} for better understanding.
%------------------------------------------
\begin{figure} [!tp]
    \centering
    \includegraphics[width=0.65\columnwidth]{figures/suppconcept_compressed.pdf} %0.95
    \caption{Visualizations of concepts. (a) Blue dots indicate the detected VOI. (b) The smallest bounding-box encompassing the VOI. (c) Initialization of the primitive as an ellipsoid. (d) The final marched superquadric capturing the local geometry.}
    \label{fig:conceptsupp}
    \vspace{-0.5cm}
\end{figure}
%------------------------------------------


\section{Derivation of the Probabilistic Marching}
\label{sec:derivation}
In this section, we provide the detailed derivation of the probabilistic primitive marching in Sec. 3.4 of the paper.
Based on the probabilistic model $p\big(d(\mathbf{x}_i)|\boldsymbol{\theta}_k, z_{ik}\big)$ (Eq.8 in the paper), the likelihood of the superquadric parameter $\boldsymbol{\theta}_k$ and variance $\sigma^2$ given the target SDF is
\begin{equation}
\begin{split}
    L(\boldsymbol{\theta}_k, \sigma^2)=&\prod_{\mathbf{x}_i\in\mathbf{V}}p(d(\mathbf{x}_i), z_{ik}|\boldsymbol{\theta}_k)\\
    =&\prod_{\mathbf{x}_i\in\mathbf{V}}p\big(d(\mathbf{x}_i)|\boldsymbol{\theta}_k, z_{ik}\big)p(z_{ik}|\boldsymbol{\theta}_k)\\
    =&\prod_{\mathbf{x}_i\in\mathbf{V}}p_0(\mathbf{x}_i)^{1-z_{ik}}\mathcal{N}\big(d_i|d_{\boldsymbol{\theta}_k}(\mathbf{x}_i), \sigma^2\big)^{z_{ik}}p(z_{ik})
\end{split}
\end{equation}
where
\begin{equation}
    p_0(\mathbf{x}_i) = \frac{\mathds{1}_{d_i\in[-t,0)}}{t} \quad d_i\doteq d(\mathbf{x}_i)
\end{equation}
and $p(z_{ik})$ is the prior probability of the correspondence between the $i$th voxel point and the $k$th primitive, which is independent of $\boldsymbol{\theta}_k$, \ie $p(z_{ik}|\boldsymbol{\theta}_k)=p(z_{ik})$.
As discussed in the paper, we assume that $z_{ik}$ is subjected to a Bernoulli prior distribution $B(p_0)$, \ie $p(z_{ik}=1)=p_0$.
The definitions of other variables can be found in the paper.
Our goal is to find the optimal $\boldsymbol{\theta}_k$ and $\sigma^2$ that maximize the likelihood function.
This is equivalent to minimizing the negative log-likelihood
\begin{equation}
\begin{split}
    l(\boldsymbol{\theta}_k, \sigma^2)=&-\log L(\boldsymbol{\theta}_k, \sigma^2)\\
    =&-\sum_{\mathbf{x}_i\in \mathbf{V}}\log \Bigg[p(z_{ik})p_0(\mathbf{x}_i)^{1-z_{ik}}c(\sigma^2)^{z_{ik}}\\
    &\exp\bigg(-\frac{1}{2}\Big(\frac{d(\mathbf{x}_i)-d_{\boldsymbol{\theta}_k}(\mathbf{x}_i)}{\sigma}\Big)^2 \bigg)^{z_{ik}}\Bigg]\\
    =&\sum_{\mathbf{x}_i\in \mathbf{V}} \bigg[\frac{z_{ik}}{2}\Big(\frac{d(\mathbf{x}_i)-d_{\boldsymbol{\theta}_k}(\mathbf{x}_i)}{\sigma}\Big)^2-\log p(z_{ik})\\
    &-z_{ik}\log c(\sigma^2)-(1-z_{ik})\log p_0(\mathbf{x}_i)\bigg]
\end{split}
\end{equation}
where $c(\sigma^2)=(2\pi \sigma^2)^{-\frac{1}{2}}$ is the normalizing coefficient of the Gaussian distribution.
By ignoring the terms independent of $\boldsymbol{\theta}_k$ and $\sigma^2$, it is equivalent to minimize
\begin{equation}
    l'(\boldsymbol{\theta}_k, \sigma^2)=\sum_{\mathbf{x}_i\in \mathbf{V}} z_{ik}\bigg[\frac{\big(d(\mathbf{x}_i)-d_{\boldsymbol{\theta}_k}(\mathbf{x}_i)\big)^2}{2\sigma^2}-\log c(\sigma^2)\bigg]
\label{eqn:mle}
\end{equation}
Unlike $d(\mathbf{x}_i)$ which is observed from the target signed distance, the correspondence $z_{ik}$ is a latent variable that cannot be observed.
Therefore, it is intractable to solve Eq.\ref{eqn:mle} directly.
Our algorithm solves the problem in a two-step expectation-maximization fashion.
That is, $z_{ik}$ is replaced with 
\begin{equation}
% \begin{split}
    P_{ik}\doteq E\big(z_{ik}|\boldsymbol{\theta}_k, d(\mathbf{x}_i)\big)=p\big(z_{ik}=1|\boldsymbol{\theta}_k, d(\mathbf{x}_i)\big)
% \end{split}
\label{eqn:expect}
\end{equation}
Eq.\ref{eqn:expect} is the conditional expectation of $z_{ik}$ given the current estimation of $\boldsymbol{\theta}_k$ and the target SDF, whose value can be calculated by Eq.9 in the paper.
Subsequently, we derive that the minimization of Eq.\ref{eqn:mle} is equivalent to Eq.10 in the paper, where we use an adaptive activation subset $\mathbf{V}_a$ instead of the whole voxel space $\mathbf{V}$ to boost performance.
After we obtain the updated primitive estimation $\boldsymbol{\theta}_k$, the variance $\sigma^2$ of the Gaussian distribution can be updated in closed form by solving
\begin{equation}
\begin{split}
    \quad&\frac{\partial l'}{\partial \sigma^2}=0\\
    \Leftrightarrow
    &\sum_{\mathbf{x}_i\in \mathbf{V}_a}P_{ik}
    \bigg[\frac{\big(d(\mathbf{x}_i)-d_{\boldsymbol{\theta}_k}(\mathbf{x}_i)\big)^2-\sigma^2}{2\sigma^4}\bigg]=0\\
    \Leftrightarrow
    &\quad \sigma^2=\frac{\sum_{\mathbf{x}_i\in \mathbf{V}_a}P_{ik}\big(d(\mathbf{x}_i)-d_{\boldsymbol{\theta}_k}(\mathbf{x}_i)\big)^2}{\sum_{{\mathbf{x}_i\in \mathbf{V}_a}}P_{ik}}
\end{split}
\end{equation}

\section{Primitive Removal Criterion}
\label{sec:removal}
In this section, we detail the fail-safe primitive removal criterion introduced in Sec. 3.5 in the paper.
Our method counts the number of positive (exterior), negative (interior), and inactive voxels encompassed by the recovered primitive, which we denote as $N_+$, $N_-$ and $N_0$, respectively.
The inactive voxels are those already fitted by recovered primitives, which is defined by Eq.7 in the paper.
Our algorithm removes the primitive from the representation if
\begin{equation}
    N_-<1 \quad or\quad \frac{N_+}{N_+ +N_- + N_0}\geq 0.5
\end{equation}
The first criterion removes the auto-degenerated primitive that shrinks to a point.
The second one is a fail-safe checking criterion, which removes the primitive that significantly contradicts the target SDF.


\section{Overview of the Algorithm}
\label{sec:pseudo-code}
In this section, we briefly summarize the Marching-Primitives algorithm into a pseudo-code (Algorithm.\ref{code:algorithm}) to give an overview of the structure.
Note that $V_-$ in the fourth row indicates the sets of voxels with negative signed distance, \ie interior of the shape.
The Marching-Primitives can be roughly separated into 2 parts.
Firstly, it marches on the signed distance domain (row 5-15) to find VOIs by analysing the connectivity.
Then for each VOI, the algorithm continues to march on the voxelized space domain (row 16-24) to grow a primitive capturing the local geometry of the VOI.
The algorithm terminates when all the interior volumes are well captured by the primitive representation $\boldsymbol{\Theta}$.

\begin{algorithm}
\caption{Marching-Primitives}\label{alg:marching}
\begin{algorithmic}[1]
    \State \textbf{Input:} voxel set $\mathbf{V}$, with target SDF $d(\cdot)$
    \State \textbf{Output:} primitive set $\boldsymbol{\Theta}$
    \State $\boldsymbol{\Theta} \gets \{\}$
    \While{$\mathbf{V}_-\neq\emptyset$}
         \State generate marching sequence $T^c$ \Comment{Eq.4 in paper}
         \For{$t_m^c$ in $T^c$}
             \If{$t_m^c>$ termination threshold}
                 \State \textbf{return} $\boldsymbol{\Theta}$
             \Else
                 \State calculate VOIs $\bar{S}_m$ \Comment{Eq.6 in paper}
                 \If{$\bar{S}_m\neq \emptyset$}
                    \State\textbf{break for}
                 \EndIf
             \EndIf
         \EndFor
         \For{$\mathcal{S}_k$ in $\bar{S}_m$}
             \State initialize primitive $\boldsymbol{\theta}_k^{init}$ \Comment{Eq.2 in supplement}
             \While{not converged}
                 \State march correspondence $P_{ik}$ \Comment{Eq.9 in paper}
                 \State update primitive $\boldsymbol{\theta}_k$ \Comment{Eq.10 in paper}
             \EndWhile
             \State $\boldsymbol{\theta}_k \rightarrow \boldsymbol{\Theta}$ \textbf{if} $\boldsymbol{\theta}_k$ valid \Comment{Eq.10 in supplement}
             \State $\mathbf{V}=\mathbf{V}-\{\mathbf{x}_i, d(\mathbf{x}_i)\leq 0 \land d_{\boldsymbol{\theta}_k}(\mathbf{x}_i)\leq0\}$
         \EndFor
    \EndWhile
    \State \textbf{return} $\boldsymbol{\Theta}$
\end{algorithmic}
\label{code:algorithm}
\end{algorithm}


\section{Implementation and Additional Results}
\label{sec:implementation}
\subsection{Metrics}
In this section, we provide details on the two metrics used to evaluate the experiments.

\textbf{Chamfer $L_1\text{-distance}$}:
The common Chamfer $L_1\text{-distance}$ is defined as follows:
\begin{equation}
\label{eq:chamfer}
\begin{split}
 D_{\text {chamfer }}(\mathbf{X}, \mathbf{Y})  = & \frac{1}{M} \sum_{\mathbf{y}_{j} \in \mathbf{Y}} \min _{\mathbf{x}_{i} \in \mathbf{X}}\left\|\mathbf{y}_{j}-\mathbf{x}_{i}\right\|_1 +\\
 & \frac{1}{N} \sum_{\mathbf{x}_{i} \in \mathbf{X}} \min _{\mathbf{y}_{j} \in \mathbf{Y}}\left\|\mathbf{x}_{i}-\mathbf{y}_{j}\right\|_1
\end{split}
\end{equation}
where $\mathbf{X}=\{\mathbf{x_i}\}$ denotes the points sampled from the predicted model, $\mathbf{Y}=\{\mathbf{y_j}\}$ denotes the points sampled from the original model, and $N$ and $M$ is the number of points of the sets $\mathbf{X}$ and $\mathbf{Y}$, respectively. 
For D-FAUST dataset, it provides a dense point cloud for each human model, which we take as $\mathbf{Y}$.
ShapeNet, on the other hand, does not provide point cloud representation for the object. 
So, we need to sample points densely on each face of the original mesh.
To obtain $\mathbf{X}$, we apply the equal-distance sampling strategy  \cite{Liu_2022_CVPR} on each superquadric surface $\boldsymbol{\theta}_k \in \boldsymbol{\Theta}$ of the predicted model to get a point set $\boldsymbol{\Gamma}_k$. However, some points from $\boldsymbol{\Gamma}_k$
might lie inside of another superquadric $\boldsymbol{\theta}_l, l \neq k$, \ie, those points are on the inside of the 3D model. Therefore, we need to remove those interior points by forming a subset $\Tilde{\boldsymbol{\Gamma}}_k \subset \boldsymbol{\Gamma}_k$,
\begin{equation}
    \Tilde{\boldsymbol{\Gamma}}_k =\{ \boldsymbol{\gamma}^k_i \mid \boldsymbol{\gamma}^k_i \in \boldsymbol{\Gamma}_k, f(\boldsymbol{\gamma}^k_i, \boldsymbol{\theta}_l) \geq 0, \forall \boldsymbol{\theta}_l \in \boldsymbol{\Theta} \}
\end{equation}
where $f(.)$ denotes the inside-outside function of the superquadric.
By taking the union of all the point sets $\{\Tilde{\boldsymbol{\Gamma}}_1,\Tilde{\boldsymbol{\Gamma}}_2,...,\Tilde{\boldsymbol{\Gamma}}_K \}$, we obtain a point cloud representation for the predicted model, which we treat as $\mathbf{X}$.
For both ShapeNet and D-FAUST, we further downsample $\mathbf{X}$ and $\mathbf{Y}$ to 50K-60K points for calculating the Chamfer distance.
The first term of Eq.\ref{eq:chamfer} computes how far on average the closest point of the predicted model is to the original mesh, and the second term calculates how far on average the closest point of the original mesh is to the predicted model. 
Thus, a lower value of Chamfer distance implies a better abstraction accuracy in terms of surface fitness. 

\textbf{Intersection over Union (IoU)}:
The definition of IoU is shown as follows:
\begin{equation}
\label{eq:iou}
\operatorname{IoU} = \frac{V\left(S_{\text {pred}} \cap S_{\text {original}}\right)}{V\left(S_{\text {pred}} \cup S_{\text {original}}\right)},
\end{equation}
where $S_{\text {pred}}$ is the predicted primitive-based model obtained by our algorithm, $S_{\text {original}}$ is the original mesh model, and $V(.)$ computes the volume.
It is difficult, if not impossible, to obtain the volume of the intersection or union of two models.
Therefore, we approximate the volume with the Monte Carlo method.
Firstly, we sample a set of points $\boldsymbol{\Phi}$ uniformly with a predefined density inside the bounding box of $S_{\text {pred}} \cup S_{\text {original}}$.
We use $100^3$ points for ShapeNet and $64^3$ points for DFAUST.
% the resolutions of the voxel grid.
The number is far more than the previous papers, expecting a more accurate evaluation.
Then, for each point $\mathbf{x} \in \boldsymbol{\Phi}$, we check if it is inside of the original mesh and the predicted model, respectively.
We approximate  $V\left(S_{\text {pred}} \cap S_{\text {original}}\right)$ to be the number of points that are on the inside of both $S_{\text {original}}$ and $S_{\text {pred}}$, and approximate $V\left(S_{\text {pred}} \cup S_{\text {original}}\right)$ with the number of points that are on the inside of either $S_{\text {original}}$ or $S_{\text {pred}}$. 
If two models match perfectly, the IoU will be 1 and if two models disjoint from each other, the IoU is 0. 
% In other words, a higher IoU indicates a better abstraction accuracy in terms of volumetric occupation. 

\subsection{Implementation Details}
In this section, we elaborate on the parameters implemented in the experiment.
All the experiments use the settings provided as follows, if not specified in the experiment section in the paper.
The truncation threshold for the target and source SDF is 1.3 times the input grid interval.
In the connectivity marching step, the common ratio of the geometric sequence $\alpha$ is $4/5$;
The minimum size of the valid connected volume $N_c=5$;
The primitive initial scale ratio $\gamma=0.1$;
The terminating marching threshold is 0.01 times the negative truncation threshold.
For the probabilistic model, the parameter of the Bernoulli prior distribution is set as $p_0=0.01$;
The variance $\sigma^2$ is initialized as the truncation threshold.
During the primitive update step, we set the activation distance $a$ as 3.5 times the truncation threshold.
The source code of our algorithm is implemented in MATLAB.
The experiments are conducted on a computer running Intel Core i9-9900K CPU.
The baseline method SQ\cite{paschalidou2019superquadrics} is trained and tested on an NVIDIA RTX3090 GPU.

All the methods consume different types of input.
We use the official codes and configurations of \cite{paschalidou2019superquadrics, wu2022primitive}.
For SQs, occupancy grids of resolution $32^3$ are generated from meshes by the provided code.
We had to and tried to modify their network to consume occupancy grids of $128^3$.
Relatively incremental improvement is observed (\eg, chair IoU $0.30\rightarrow 0.34$).
Therefore, we used the original network and followed the official configuration for consistency with the previous literature.
We use 1000 and 200 points from objects and each superquadric for the loss function, respectively.
For NB, we densely sample points from mesh and uniformly downsample to around 3500 (ShapeNet) and 5500 (DFAUST) points and set the number of initial components $K=30$ following the settings in \cite{wu2022primitive}.

\subsection{Number of Parts}
The number of parts used is not and cannot be predefined in all the methods.
SQs \cite{paschalidou2019superquadrics} has a hyper-parameter to limit the maximum number set to 20.
The training is unsupervised, and the network learns to predict the number of components.
NB \cite{wu2022primitive} infers the number via the Chinese Restaurant Process, splitting/merging when probabilistically necessary with no limits.
Our method grows parts as needed.
Since there is no ground truth and shapes vary greatly even in the same category, it is hard to quantify the correct number, which makes statistics less meaningful.
Our result is satisfying qualitatively as shown in Fig.\ref{fig:rebuttal}.
Our method can successfully separate different parts if they possess different geometric semantics (\eg telling apart cuboids, cylinders, and balls).
Therefore, it is semantically interpretable.
In many cases (\eg, Fig.\ref{fig:rebuttal}), the segmentation coincides with the human-defined semantics, though not trained to.
\begin{figure}[!tp]
  \centering
  \includegraphics[width=1\columnwidth]{figures/colorparts_compressed.pdf} %0.9
  \caption{Shape abstraction results with the number of parts. Recovered primitives are colored in different colors.}
  \label{fig:rebuttal}
  \vspace{-0.5cm}
\end{figure}

\subsection{Time Performance}
The proposed method (MPS) has an average runtime of 6.7s on ShapeNet and 2.5s on DFAUST per item.
Time varies on the complexity of objects and grid resolutions.
For an intuitive example, the chair, table, and human in Fig.\ref{fig:rebuttal} take 3.9s, 3.6s, and 2.8s, respectively.
The complex Reading Room takes 146s for resolution $400^3$ and 30s for $200^3$.

%------------------------------------------------------------------------
\begin{figure*}[!ht]
    \centering 
    \includegraphics[scale=0.075]{figures/suppairplane_compressed.pdf} % 0.075
    \caption{Shape Abstraction results on airplanes. From left to right: SQs, Non-parametric Bayesian (NB), Marching-Primitives with ellipsoids (MPE), Marching-Primitives with superquadrics (MPS), and the ground truth (pre-processed watertight mesh).}
\vspace{-0.0cm}
\end{figure*}
%------------------------------------------------------------------------
%------------------------------------------------------------------------
\begin{figure*}[!ht]
    \centering 
    \includegraphics[scale=0.075]{figures/suppchair_compressed.pdf} % 0.075
    \caption{Shape Abstraction results on chairs. From left to right: SQs, Non-parametric Bayesian (NB), Marching-Primitives with ellipsoids (MPE), Marching-Primitives with superquadrics (MPS), and the ground truth (pre-processed watertight mesh).}
\vspace{-0.0cm}
\end{figure*}
%------------------------------------------------------------------------
%------------------------------------------------------------------------
\begin{figure*}[!ht]
    \centering 
    \includegraphics[scale=0.078]{figures/suppbenchsofa_compressed.pdf} % 0.078
    \caption{Shape Abstraction results on benches and sofas. From left to right: SQs, Non-parametric Bayesian (NB), Marching-Primitives with ellipsoids (MPE), Marching-Primitives with superquadrics (MPS), and the ground truth (pre-processed watertight mesh).}
\vspace{0.5cm}
\end{figure*}
%------------------------------------------------------------------------
%------------------------------------------------------------------------
\begin{figure*}[!ht]
    \centering 
    \includegraphics[scale=0.078]{figures/supptable_compressed.pdf} % 0.078
    \caption{Shape Abstraction results on tables. From left to right: SQs, Non-parametric Bayesian (NB), Marching-Primitives with ellipsoids (MPE), Marching-Primitives with superquadrics (MPS), and the ground truth (pre-processed watertight mesh).}
\vspace{0.5cm}
\end{figure*}
%------------------------------------------------------------------------
%------------------------------------------------------------------------
\begin{figure*}[!ht]
    \centering 
    \includegraphics[scale=0.078]{figures/supplamp_compressed.pdf} % 0.078
    \caption{Shape Abstraction results on lamps. From left to right: SQs, Non-parametric Bayesian (NB), Marching-Primitives with ellipsoids (MPE), Marching-Primitives with superquadrics (MPS), and the ground truth (pre-processed watertight mesh).}
\vspace{0.5cm}
\end{figure*}
%------------------------------------------------------------------------
%------------------------------------------------------------------------
\begin{figure*}[!ht]
    \centering 
    \includegraphics[scale=0.078]{figures/suppbottlephonecabinet_compressed.pdf} % 0.078
    \caption{Shape Abstraction results on bottles, phones and cabinets. From left to right: SQs, Non-parametric Bayesian (NB), Marching-Primitives with ellipsoids (MPE), Marching-Primitives with superquadrics (MPS), and the ground truth.}
\vspace{0.5cm}
\end{figure*}
%------------------------------------------------------------------------

%------------------------------------------------------------------------
\begin{figure*}[!ht]
    \centering 
    \includegraphics[scale=0.078]{figures/suppspeakerwatermailrifle_compressed.pdf} % 0.078
    \caption{Shape Abstraction results on speakers, water-crafts, mailboxes and rifles. From left to right: SQs, Non-parametric Bayesian (NB), Marching-Primitives with ellipsoids (MPE), Marching-Primitives with superquadrics (MPS), and the ground truth.}
\vspace{0.5cm}
\end{figure*}
%------------------------------------------------------------------------

\subsection{Additional Results}
Due to the limited length of the paper, in this Supplementation Material, we prepare more qualitative comparisons on the ShapeNet dataset.
From the additional results, we further demonstrate that our method is able to achieve high accuracy shape abstraction.
Our method not only well captures the geometry of different objects in a same category, but also is generalizable among various categories without the need of fine-tuning.



%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
