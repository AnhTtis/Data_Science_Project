
\section{Experiments}
\label{experiments}

We begin by validating \putouralg's performance on multiple SSL benchmarks against state-of-the-art methods. Then, we analyse the main components of \putouralg to verify their contribution towards the overall performance, and we perform ablations on important hyperparameters.

\subsection{Experimental Settings}
\noindent \textbf{Datasets.} We evaluate \putouralg on five SSL benchmarks. Following~\cite{Sohn_fixmatch20, xie2019_uda, arazo_pseudo}, we evaluate on \textbf{CIFAR-10(100)}~\cite{cifar100} datasets, which comprises 50,000 images of 32x32 resolution of 10(100) classes; as well as the more challenging \textbf{Mini-ImageNet} dataset proposed in~\cite{mini_imagenet}, having 100 classes with 600 images per class (84x84 each). We use the same train/test split as in~\cite{label_prop} and  create splits for 4 and 10 labeled images per class to test \putouralg in the low-label regime. We also test \putouralg's performance on the \textbf{DomainNet}~\cite{peng2019moment_domainnet} dataset, which has 345 classes from six visual domains: \emph{Clipart}, \emph{Infograph}, \emph{Painting}, \emph{Quickdraw}, \emph{Real}, and \emph{Sketch}. We evaluate on the \emph{Clipart} and \emph{Sketch} domains to verify our method's efficacy in different visual domains and on imbalanced datasets.  Finally, we evaluate on \textbf{ImageNet}~\cite{russakovsky2015imagenet_large} SSL protocol as in~\cite{caron2020unsupervised, chen2020simple, caron2021emerging, assran2021semi}.
% , whereby a percentage of the labeled data is used to train the model together with all the unlabeled data. 
In all our experiments, we focus on the low-label regime. %Wwe also run one experiment in Mini-ImageNet on moderate data regime to have a more comprehensive view.

\smallskip
\noindent \textbf{Implementation Details.}
For CIFAR-10(100), we follow previous work and use WideResent-28-2(28-8)~\cite{zagoruyko2016_wideresnet} as our encoder. We use a 2-layer projection MLP with an embedding dimension $d=64$. The models are trained using SGD with a momentum of 0.9 and weight decay of 0.0005(0.001) using a batch size of 64 and $\mu=7$. We set the threshold $\tau=0.95$ and train our models for 1024 epochs for a fair comparison with the baselines. However, we note that our model needs substantially fewer epochs to converge (see Fig.~\ref{fig:analysis_plots}-b and c). We use a learning rate of 0.03 with a cosine decay schedule. We use random horizontal flips for weak augmentations and RandAugment~\cite{cubuk2020_randaugment} for strong ones. For the larger datasets: ImageNet and DomainNet, we use a Resnet-50 encoder and $d=128$, $\mu=5$ and $\tau=0.7$ and follow the same hyperparameters as in~\cite{Sohn_fixmatch20} except that we use SimCLR~\cite{chen2020simple} augmentations for the strong view. For \putouralg-specific hyperparameters, we consistently use the same parameters across all experiments: we set $n$ to 250 (corresponding to $K$=200 for CIFARs, and Mini-ImageNet, and 4800 for ImageNet), and dual learning rate $\lambda = 20$, mixing ratio $\alpha=0.8$, and temperature $T=0.1$.


\noindent \textbf{Baselines.}
Since our method bears the most resemblance with CoMatch~\cite{li2021comatch}, we compare against it in all our experiments. CoMatch uses graph contrastive learning to refine pseudo-labels but uses a memory bank to store the last n-samples embeddings to build the graph. Additionally, we compare with state-of-the-art SSL method (DebiasPL)~\cite{wang2022debiased}, which proposes a pseudo-labeling debiasing plug-in to work with various SSL methods in addition to an adaptive margin loss to account for inter-class confounding. Finally, we also compare with the seminal method FixMatch and its variant with Distribution alignment (DA). We follow Oliver~\etal~\cite{oliver_realistic} recommendations to ensure a fair comparison with the baselines, where we implement/adapt all the baselines using the same codebase to ensure using the same settings across all experiments. As for ImageNet experiments, we also compare with representation learning baselines such as SwAV~\cite{caron2020unsupervised}, DINO~\cite{caron2021emerging}, and SimCLR~\cite{chen2020simple}, where we report the results directly from the respective papers. We also include results for \putouralg and DebiasPL with additional pretraining (using MOCO~\cite{he2020momentum}) and the Exponential Moving Average Normalisation method proposed by~\cite{cai2021exponential} to match the settings used in~\cite{wang2022debiased, cai2021exponential}.

\subsection{Results and Analysis}
\noindent \textbf{Results.}
Similar to prior work, we report the results on the test sets of respective datasets by averaging the results of the last 10 epochs of training. For CIFAR and Mini-ImageNet, we report the average and standard deviation over 5 different labeled splits, whereas we report for only 1 split on larger datasets (ImageNet and DomainNet). Different from most previous work, we only focus on the very low-label regime (2, 4, and 8 samples per class, and 0.2\% for ImageNet). As shown in Tab.~\ref{tab:cifar_results} - 
\ref{tab:imagenet_results}, we observe that \putouralg outperforms baselines in almost all the cases showing a clear advantage in the low-label regime. It also exhibits less variance across the different splits (and the different runs within each split). These results suggest that besides achieving high accuracy, \putouralg shows robustness and consistency across splits in low-data regime.

% \MH{I think we can use the next para to highlight our methods strengths. Right now, it sounds like, thanks to DINO, our method works well on DomainNet. We're only using part of DINO, so, the self-supervised loss is tailord to fit with rest of our architecture. please consider revising the para below to highlight our strengths}
% Surprisingly, \putouralg performs particularly well on DomainNet. Although, we are yet to investigate further the root cause behind such specific improvement, we hypothesise that this is due to including the self-supervised loss which already learns rich robust representations without any label as shown in recent work~\cite{caron2021emerging}. This is consistent with recent findings~\cite{nassar2022lava} which show consistent performance gains in visual transfer learning when using a similar self-supervised loss for pretraining. 
Notably, our method performs particularly well on DomainNet. Unlike ImageNet and CIFARs, DomainNet is an imbalanced dataset, and prior work~\cite{tan2020class} shows that it suffers from high level of label noise. This shows that our method is also more robust to noisy labels. This can be explained in context of our co-training approach:  using the prototypical neighbourhood label to smooth the softmax label is an effective way to minimise the effect of label noise. In line with previous findings~\cite{li2021learning}, since in prototypical learning, all the instances of a given class are used to calculate a class prototype which is then used as a prediction target, it results in representations which are more robust to noisy labels.  

Finally, on ImageNet (Tab.~\ref{tab:imagenet_results}), we improve upon the closest baseline with gains of 2.2\% in the challenging 0.2\% setting; whereas we slightly fall behind PAWS~\cite{assran2021semi} in the 10\% regime, again confirming our method's usefulness in the label-scarce scenario.

\begin{figure*}[h!]
 \centering
  \scalebox{0.99}{\includegraphics[width=0.97\textwidth]{figures/analysis_plots.pdf}}
 \caption{\textbf{Analysis Plots.} {\bf (a)}: Average disagreement between cluster and classifier pseudo-labels versus ground truth accuracy of the different pseudo-labels. The accuracy gap between refined pseudo-labels (green) and the cluster's and classifier's (blue and orange) decreases with disagreement rate (dashed black) showing that refinement indeed helps. {\bf (b), (c):} Convergence plots on CIFAR10/100 show that \putouralg converges faster due to the additional self-supervised training signal. {\bf (d): } \putouralg w and w/out consistency loss. }
 \label{fig:analysis_plots}
 \vspace{-2mm}
\end{figure*}

\begin{figure}[h!]
 \centering
  \scalebox{0.99}{\includegraphics[width=0.47\textwidth]{figures/image_comparison.pdf}}
 \caption{The middle panel shows the most prototypical images of CIFAR-10 classes as identified by our model. Left (resp. right) panels show images which have more accurate classifier (resp. cluster) pseudo-labels. Cluster labels are more accurate for prototypical images while classifier labels are more accurate for images with distinctive features (\eg truck wheels) even if not so prototypical. Such diversity of views is key to the success of our co-training method.}
 \label{fig:image_comparison}
 \vspace{-2mm}
\end{figure}

\input{tables/imagenet_results.tex}
% \noindent \textbf{Analysis and Ablations}
\noindent \textbf{How does refinement help?}
First, we would like to investigate the role of pseudo-labeling refinement in improving SSL performance. Intuitively, since we perform refinement by combining pseudo-labels from two different sources (the classifier predictions in probability space and the cluster labels in the prototypical space), we expect that there will be disagreements between the two and hence considering both the views is the key towards the improved performance. To validate such intuition, we capture a fine-grained view of the training dynamics throughout the first 300 epochs of CIFAR-10 with 40 labeled instances scenario, including: samples' pseudo-labels before and after refinement as well as their cluster pseudo-labels in each epoch. This enables us to capture disagreements between the two pseudo-label sources up to the individual sample level.
In Fig.~\ref{fig:analysis_plots}-a, we display the average disagreement between the two sources over the initial phase of the training overlaid with the classifier, cluster and refined pseudo-label accuracy. We observe that initially, the disagreement (dashed black line) is high which corresponds to a larger gap between the accuracies of both heads. As the training proceeds, we observe that disagreement decreases leading to a respective decrease in the gap. Additionally, we witness that the refined accuracy curve (green) is almost always above the individual accuracies (orange and blue) which proves that, indeed, the synergy between the two sources improves the performance. 

On the other hand, to get a qualitative understanding of where each of the pseudo-labeling sources helps, we dig deeper to classes and individual samples level where we investigate which classes/samples are the most disagreed-upon (on average) throughout the training. In Fig.~\ref{fig:image_comparison}, we display the most prototypical examples of a given class (middle) as identified by the prototypical scores obtained in the embedding space. We also display the examples which on average are always correctly classified in the prototypical space (right) opposed to those in the classifier space (left). As expected, we find that samples which look more prototypical, albeit with less distinctive features (\eg blurry), are the ones almost always correctly classified with the prototypical head; whereas, samples which have more distinctive features but are less prototypical are those correctly classified by the discriminative classifier head. This again confirms our intuitions about how co-training based on both sources helps to refine the pseudo-label.
	
Finally, we ask: is it beneficial to use the entire dataset pseudo-label history to perform refinement or is it sufficient to just use a few samples? To answer this question, we use only a subset of the samples in each cluster (sampled uniformly at random) to calculate cluster pseudo-labels in Eqn.~\ref{eqn:clust_pl}. For CIFAR-10 with 20 and 40 labels, we find that this leads to about 1-2\% (4-5\%) average drop in performance, if we use half (quarter) of the samples in each cluster.  This reiterates the usefulness of our approach to leverage the history of all samples (at a lower cost) opposed to a limited history of samples. 

\smallskip
\noindent \textbf{Role of self-supervised loss.}
Here, we are interested to tear apart our choice of self-supervised loss and its role towards the performance. To recap, our intuition behind using that loss is to boost the learning signal in the initial phase of the training when the model is still not confident enough to retain samples for pseudo-labeling. As we see in Fig.~\ref{fig:analysis_plots}-b and c. there is a significant speed up of our model's convergence compared to baseline methods with a clear boost in the initial epochs. Additionally, to isolate the potentially confounding effect of our other ingredients, we display in Fig.~\ref{fig:analysis_plots}-d the performance of our method with and without the self-supervised loss which leads to a similar conclusion. Finally, to validate our hypothesis that instance-consistency loss is more useful than instance-discrimination, we run a version of \putouralg with an instance-discrimination loss akin to that of SimCLR. This version completely collapsed and did not converge at all. We attribute this to: 1) as verified by SimCLR authors, such methods work best with large batch sizes to ensure enough negative examples are accounted for; and 2) these methods treat each image as its own class and contrast it against every other image and hence are in direct contradiction with the image classification task; whereas instance-consistency losses only ensure that the representations learnt are invariant to common factors of variations such as: color distortions, orientation, \etc. and are hence more suitable for semi-supervised image classification tasks.

\input{tables/ablations.tex}

\smallskip
\noindent \textbf{Ablations.}
Finally, we present an ablation study about the important hyperparametes of \putouralg. Specifically, we find that $n$ (minimum samples in each cluster) and $\alpha$ (mixing ratio between classifier pseudo-label and cluster pseudo-label) are particularly important. Additionally, we find that the projection dimension needs to be sufficiently large for larger datasets (we use $d=64$ for CIFARs and 128 for all others). In Tab.~\ref{tab:ablation}, we present ablation results on CIFAR-10 with 80 labeled instances.

