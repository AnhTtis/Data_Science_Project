\section{Background}
\label{sec:background}

We begin by reviewing existing SSL approaches with a special focus on relevant methods in the low-label regime. 

\smallskip
\noindent \textbf{Confidence-based pseudo-labeling} is an integral component in most of recent SSL methods~\cite{Sohn_fixmatch20, li2021comatch, wang2022debiased, nassar2021all, kuo2020featmatch}. However, recent research shows that using a fixed threshold underperforms in low-data settings because the model collapses to the few easy-to-learn classes early in the training. Some researchers combat this effect by using class-~\cite{zhang2021flexmatch} or instance-based~\cite{xu2021dash} adaptive thresholds, or by aligning~\cite{Berthelot_RemixMatch19} or debiasing~\cite{wang2022debiased} the pseudo-label distribution by keeping a running average of pseudo-labels to avoid the inherent imbalance in pseudo-labels. Another direction focuses on pseudo-label refinement, whereby the classifier's predictions are adjusted by training another projection head on an auxiliary task such as weak-supervision via language semantics~\cite{nassar2021all}, instance-similarity matching~\cite{zheng2022simmatch}, or graph-based contrastive learning~\cite{li2021comatch}. Our method follows the refinement approach, where we employ online constrained clustering to leverage nearest neighbours information for refinement. Different from previous methods, our method is fully online and hence allows using the entire prediction history in one training epoch to refine pseudo-labels in the subsequent epoch with minimal memory requirements.    

\smallskip
\noindent \textbf{Consistency Regularization} combined with pseudo-labeling underpins many recent state-of-the-art SSL methods~\cite{xie2019_uda, li2019decoupled, Sajjadi_NIPS16, Berthelot_MixMatch19, Sohn_fixmatch20, li2021comatch, kuo2020featmatch}; it exploits the smoothness assumption~\cite{van2020survey} where the model is expected to produce similar pseudo-labels for minor input perturbations. The seminal FixMatch~\cite{Sohn_fixmatch20} and following work~\cite{nassar2021all, li2021comatch, wang2022debiased} leverage this idea by obtaining pseudo-labels through a weak form of augmentation and applying the loss against the model's prediction for a strong augmentation. Our method utilises a similar approach, but different from previous work, we additionally apply an instance-consistency loss in our projection embedding space.

% adding a loss to enforce prediction consistency over semantic-preserving augmentations of unlabeled images. Berthelot \etal~\cite{Berthelot_MixMatch19} average predictions across multiple augmentations whereas more recent methods~\cite{Sohn_fixmatch20, nassar2021all, li2021comatch, wang2022debiased} use a pair of weak and strong augmentations for each image. 

\smallskip
\noindent \textbf{Semi-supervision via self-supervision} is gaining recent popularity due to the incredible success of self-supervised learning for model pretraining. Two common approaches are: 1) performing self-supervised pretraining followed by supervised fine-tuning on the few labeled samples~\cite{chen2020simple, chen2020big, grill2020bootstrap, nassar2022lava, caron2021emerging}, and 2) including a self-supervised loss to the semi-supervised objective to enhance training~\cite{zhai2019s4l, lucas2022barely, wallin2022doublematch, li2021comatch, zheng2022simmatch}. However, the choice of the task is crucial: tasks such as instance discrimination~\cite{he2020momentum, chen2020simple}, which treats each image as its own class, can hurt semi-supervised image classification as it partially conflicts with it. Instead, we use an instance-consistency loss akin to that of~\cite{caron2021emerging} to boost the initial training signal by leveraging samples which are not retained for pseudo-labeling in the early phase of the training.





