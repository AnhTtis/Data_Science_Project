\section{\putouralg}
\label{sec:method}
\noindent \textbf{Preliminaries.} We consider a semi-supervised image classification problem, where we train a model using $M$ labeled samples and $N$ unlabeled samples, where $N>>M$. We use mini-batches of labeled instances, $\mathcal{X} = \{(\vx_j, \vy_j)\}_{j=1}^B$ and unlabeled instances, $\mathcal{U} = \{\Vec{u}_i\}_{i=1}^{\mu \cdot B}$, where the scalar $\mu$ denotes the ratio between the number of unlabeled and labeled examples in a given batch, and $\vy$ is the one-hot vector of the class label $c \in \{1, \dots, C\}$. We employ an encoder network $f$ to get latent representations $f(.)$. We attach a softmax classifier $g(\cdot)$, which produces a distribution over classes $\vp = g \circ f$. Moreover, we attach a projector  $h(\cdot)$, an MLP followed by an $\ell_2$ norm layer, to get a normalised embedding $\vq \in \mathbb{R}^{d} = h \circ f$. Following~\cite{Sohn_fixmatch20}, we apply weak augmentations $\Acal_w(\cdot)$ on all images and an additional strong augmentation~\cite{cubuk2020_randaugment} $\Acal_s(\cdot)$ only on unlabeled ones. 
% Finally, we define a reliability mask for unlabeled samples $\eta$ as per: 
% Which denotes whether an unlabeled sample is reliable .

\smallskip
\noindent \textbf{Motivation.} Our aim is to refine pseudo-labels before using them to train our model in order to minimise confirmation bias in label-scarce SSL. We achieve this via a co-training approach (see Fig.~\ref{fig:main_protocon}). For each image, we obtain two pseudo-labels and combine them to obtain our final pseudo-label $\hat{\vp}^w$. The first is the classifier softmax prediction ${\vp}^w$ based on a weakly augmented image, whereas the second is an aggregate pseudo-label $\vz^a$ describing the sample's neighbourhood. To ensure the two labels are based on sufficiently different representations, we define an image neighbourhood\footnote{\scriptsize We use ``neighbourhood'' and ``cluster'' interchangeably.} via online clustering in an embedding space obtained via projector $h$ and trained for prototypical consistency instead of class prediction.  Projector $h$ and classifier $g$ are jointly trained with the encoder $f$, while interacting over pseudo-labels. The classifier is trained with pseudo-labels which are refined based on their nearest neighbours in the embedding space, whereas the projector $h$ is trained using prototypes obtained based on the refined pseudo-labels to impose structure on the embedding space.

\smallskip
\noindent \textbf{Prototypical Space.}
Here, we discuss our procedure to learn our embedding space defined by $h$. Inspired by prototypical learning~\cite{snell2017prototypical}, we would like to encourage well-clustered image projections in our space by attracting samples to their class prototypes and away from others. Hence, we employ a contrastive objective using the class prototypes as targets rather than the class labels. We calculate class prototypes at the end of a given epoch by knowledge of the ``reliable'' samples in the previous epoch. Specifically, by employing a memory bank of $\Ocal(2N)$, we keep track of samples hard pseudo-labels $\{\hat{y}_i = \argmax(\hat{\vp}^w_i) \forall \vu_i \in \Ucal\}$ in a given epoch; as well as a reliability indicator for each sample $\eta_i = \mathds{1} (\max (\hat{\vp}^w_i) \geq \tau)$ denoting if its max prediction exceeds the confidence threshold $\tau$. Subsequently, we update the prototypes $\Pcal \in \mathbb{R}^{C \times d}$ as the average projections (accumulated over the epoch) of labeled images and reliable unlabeled images. Formally, let $\Ical^x_c = \{i | \forall \vx_i \in \Xcal, y_i=c \}$ be the indices of labelled instances with true class $c$, and $\Ical^w_c = \{i | \forall \vu_i \in \Ucal, \eta_i=1, \hat{y}_i=c \}$ be the indices of the reliable unlabelled samples with hard pseudo-label $c$. The normalised prototype for class $c$ can then be obtained as per:
\begin{equation}
	\bar{\Pcal}_c = \frac{\sum_{i \in \Ical^x_c \cup \Ical^w_c} \vq_i}{|\Ical^x_c| + |\Ical^w_c|} , \quad \Pcal_c = \frac{\bar{\Pcal}_c}{||\bar{\Pcal}_c||_2}
\end{equation}

% \MH{$\Pcal$ is a vector.. consider replacing $\Pcal$ with a vector notation. since $\vp$ is already used in Eq.1, we'll have to pick another symbol}  

\noindent Subsequently, in the following epoch, we minimize the following contrastive prototypical consistency loss on unlabeled samples:
\begin{equation}
    \Lcal_{p} = - \frac{1}{\mu B} \sum_{i=1}^{\mu B} \log \frac{\exp(\vq^s_i \cdot \Pcal_{\hat{y}_i} / T)}{\sum_{c=1}^{C} \exp(\vq^s_i \cdot \Pcal_c / T))}, 
\end{equation}
where $T$ is a temperature parameter. Note that the loss is applied against the projection of the strong augmentations to achieve consistency regularisation as in~\cite{Sohn_fixmatch20}.

\input{algorithm.tex}

\smallskip
\noindent \textbf{Online Constrained K-means}
Here, the goal is to cluster instances in the prototypical space as a training epoch proceeds, so the cluster assignments (capturing the neighbourhood of each sample) are used to refine their pseudo-labels in the following epoch. We employ a mini-batch version of K-means~\cite{sculley2010web}. To avoid collapsing to one (or a few) imbalanced clusters, we ensure that each cluster has sufficient samples by enforcing a constraint on the lowest acceptable cluster size. Given our $N$ unlabeled projections, we cluster them into $K$ clusters defined by centroids $\Qcal = [\mathbf{c}_1, \cdots, \mathbf{c}_K] \in \mathbb{R}^{d \times K}$. We use the constrained K-means objective proposed by~\cite{bradley2000constrained}:
\begin{footnotesize}
\begin{align}
\label{eqn:ckmeans}
\min_{\Qcal, \mu \in \Delta} \sum_{i=1,k=1}^{i=N,k=K}\mu_{i,k}\|\vq_i - \mathbf{c}_k\|_2^2 \quad s.t.\quad \forall k\quad \sum_{i=1}^N \mu_{i,k}\geq \gamma
\end{align}
\end{footnotesize}

where $\gamma$ is the lower-bound of cluster size, $\mu_{i,k}$ is the assignment of the $i$-th unlabeled sample to the $k$-th cluster, and $\Delta=\{\mu|\forall i,\ \sum_{k}\mu_{i,k}=1,\forall i,k, \mu_{i,k}\in[0,1]\}$ is the domain of $\mu$. Subsequently, to solve Eqn.~\ref{eqn:ckmeans} in an online mini-batch manner, we adopt the alternate solver proposed in~\cite{qian2022unsupervised}. For a fixed $\Qcal$, the problem for updating $\mu$ can be simplified as an assignment problem. By introducing dual variables $\rho_k$ for each constraint $\sum_i \mu_{i,k} \geq \gamma$, the assignment can be obtained by solving the problem:
\begin{eqnarray}
\label{eqn:assignment}
\max_{\mu_{i}\in\Delta}  \sum_k s_{i,k} \mu_{i,k} + \sum_k\rho_k^{t-1} \mu_{i,k}
\end{eqnarray}
where $s_{i,k} = \vq_i^\top \mathbf{c}_k$ is the similarity between the projection of unlabeled sample $\vu_i$ and the $k$-th cluster centroid, and $t$ is the mini-batch iteration counter. Eqn.~\ref{eqn:assignment} can then be solved with the closed-form solution:
\begin{eqnarray}
\label{eq:solution}
\mu_{i,k} = \left\{\begin{array}{cc}1&k=\arg\max_k s_{i,k}+\rho_k^{t-1}\\0&o.w.\end{array}\right.
\end{eqnarray}
After assignment, dual variables are updated as\footnote{\scriptsize Refer to~\cite{qian2022unsupervised} and supplements for proofs of optimality and more details.} :
\begin{eqnarray}
\label{eq:finalrho}
\rho_k^t = \max\{0,\rho_k^{t-1} - \lambda \frac{1}{B}\sum_{i=1}^B( \mu_{i,k}^t-\frac{\gamma}{N})\}
\end{eqnarray}
where $\lambda$ is the dual learning rate.
Finally, we update the cluster centroids after each mini-batch\footnote{\scriptsize See supplements for a discussion about updating the centers every mini-batch opposed to every epoch.} as:
\begin{equation}
\label{eq:updatemulti}
\bar{\mathbf{c}_{k}}^t = \frac{\sum_i^m\mu_{i,k}^t\vq_i^t}{\sum_i^m \mu_{i,k}^t}, \quad \mathbf{c}_{k}^t = \frac{\bar{\mathbf{c}_{k}}^t}{||\bar{\mathbf{c}_{k}}^t||_2}
\end{equation}
where $m$ denotes the total number of received instances until the $t$-th mini-batch. Accordingly, we maintain another memory bank ($\Ocal(2N)$) to store two values for each unlabeled instance: its cluster assignment in the current epoch $a(i) = \{k | \mu_{i,k} = 1\}$ and the similarity score $s_{i, a(i)}$ (\ie the distance to its cluster centroid). 

\smallskip
\noindent \textbf{Cluster Pseudo-labels} are computed at end of each epoch by querying the memory banks. The purpose is to obtain a distribution over classes $C$ for each of our clusters based on its members. For a given cluster $k$, we obtain its label $\vz^k = [z^k_1, \cdots, z^k_C]$ as the average of the pseudo-labels of its cluster members weighted by their similarity to its centroid. Concretely, let $\Ical^k_c = \{i | \forall \vu_i \in \Ucal , a(i) = k, \hat{y}_i = c\}$ be the indices of unlabeled samples which belong to cluster $k$ and have a hard pseudo-label $c$. The probability of cluster $k$'s members belonging to class $c$ is given as:
\begin{align}
\label{eqn:clust_pl}
    z^k_c = \frac{\sum_{i \in \Ical^k_c} \quad s_{i, a(i)}}{\sum_{b=1}^C \sum_{j \in \Ical^k_b} s_{j, a(j)}}
\end{align}

\smallskip
\noindent \textbf{Refining Pseudo-labels.}
At any given epoch, we now have two pseudo-labels for an image $\vu_i$: the unrefined pseudo-label $\vp^w_i$ as well as a cluster pseudo-label $\vz^{a(i)}$ summarising its prototypical neighbourhood in the previous epoch. Accordingly, we apply our refinement procedure as follows: first, as recommended by~\cite{Berthelot_RemixMatch19, li2021comatch}, we perform distribution alignment ($DA(\cdot)$) to encourage the marginal distribution of pseudo-labels to be close to the marginal of ground-truth labels\footnote{\scriptsize $DA(\vp^w) = \vp^w / \bar{\vp^w}$, where $\bar{\vp^w}$ is a running average of $\vp^w$ during training.}, then we refine the aligned pseudo-label as per:
\vspace{-1.5mm}
\begin{align}
    \hat{\vp}^w_i = \alpha \cdot DA(\vp^w_i) + (1 - \alpha) \cdot \vz^{a(i)}
\end{align}
Here, the second term acts as a regulariser to encourage $\hat{\vp}^w$ to be similar to its cluster members' and $\alpha$ is a trade-off scalar parameter. Importantly, the refinement here leverages information based on the entire training set last-epoch information. This is in contrast to previous work~\cite{li2021comatch, zheng2022simmatch} which only stores a limited history of soft pseudo-labels for refinement, due to more memory requirement ($\Ocal(N \times C)$).

\smallskip
\noindent \textbf{Classification Loss.}
With the refined pseudo-label, 
% we follow the standard approach in recent methods and 
we apply the unlabeled loss against the model prediction for the strong augmentation as per:
\vspace{-1.5mm}
\begin{equation}
\label{eqn:loss_u}
	\Lcal_u = \frac{1}{\mu B} \sum_{i=1}^{\mu B} \eta_i \cdot \mathrm{CE}(\hat{\vp}_i^w,\vp^s_i),
\end{equation}
where $\mathrm{CE}$ denotes cross-entropy.
However, unlike ~\cite{Sohn_fixmatch20, Berthelot_MixMatch19}, we do not use hard pseudo-labels or sharpening, but instead use the soft pseudo-label directly.
Also, we apply a supervised classification loss over the labeled data as per:
\vspace{-1.5mm}
\begin{equation}
	\Lcal_x = \frac{1}{B} \sum_{i=1}^{B} \mathrm{CE}(\vy_i,\vp^x_i)),
\end{equation}

% Instead, we achieve entropy minimization via our prototypical loss discussed next. However, we use an efficient memory bank to store the hard pseudo-label $\hat{y} = \argmax(\hat{\vp}^w)$ to use later for our refinement procedure. Note that such memory bank is of $\Ocal(N)$, for $N$ samples, which is significantly smaller than storing soft pseudo-labels ($\Ocal(N \times C)$). Later, we convert the hard pseudo-labels into soft ones using our clustering-based approach.   

% \smallskip
\noindent \textbf{Self-supervised Loss.}
Since we use confidence as a measure of reliability (see Eqn.~\ref{eqn:loss_u}), early epochs of training suffer from limited supervisory signal when the model is not yet confident about unlabeled samples, leading to slow convergence and unstable training. Our final ingredient addresses this by introducing a consistency loss in the prototypical space on samples which fall below the confidence threshold $\tau$. We draw inspiration from instance-consistency self-supervised methods such as BYOL~\cite{grill2020bootstrap} and DINO~\cite{caron2021emerging}. In contrast to contrastive instance discrimination~\cite{he2020momentum, chen2020simple}, the former imposes consistency between two (or more) views of an image without using negative samples. Thereby, we found it to be more aligned with classification tasks than the latter. Formally, we treat the projection $q$ as soft classes score over $d$ dimensions, and obtain a distribution over these classes via a sharpened softmax ($SM(\cdot)$). We then enforce consistency between the weak and strong views as per: 
% \begin{footnotesize}
\begin{align}
\footnotesize
    \Lcal_c = \frac{1}{\mu B} \sum_{i=1}^{\mu B} (1-\eta_i) \cdot \mathrm{CE}(SM(\vq^w_i / 5T), SM(\vq^s_i / T)) 
\end{align}
% \end{footnotesize}
Note that, as in DINO \cite{caron2021emerging}, we sharpen the target distribution more than the source's to encourage entropy minimization~\cite{Grandvalet_EntropMin05}. Unlike DINO, we do not use a separate EMA model to produce the target, we just use the output of the model for the weak augmentation. Note that this does not lead to representation collapse~\cite{grill2020bootstrap} because the network is also trained with additional semi-supervised losses.

\smallskip
\noindent \textbf{Final Objective.} We train our model using a linear combination of all four losses $\Lcal = \Lcal_x + \lambda_u \Lcal_u + \lambda_p \Lcal_p + \lambda_c \Lcal_c$. Empirically, we find that fixing $\forall \lambda = 1$, the coefficients to modulate each loss, works well across different datasets. Algorithm~\ref{alg:code} describes one epoch of \putouralg training.
% \noindent \textbf{Final Objective.} Our model is trained with a linear combination of all four losses as per $\Lcal = \Lcal_x + \lambda_u \Lcal_u + \lambda_p \Lcal_p + \lambda_c \Lcal_c$, with coefficients to modulate the contribution of each loss. Empirically, we find that setting $\forall \lambda = 1$ works well across different datasets. Algorithm~\ref{alg:code} describes one epoch of \putouralg training.

\subsection{Design Considerations}
\noindent \textbf{Number of Clusters} is a crucial parameter in our approach. In essence, we refine a sample prediction obtained by the classifier by aggregating information from its $n$ nearest neighbours. However, naively doing nearest-neighbour retrieval has two limitations: 1) it requires storing image features throughout an epoch which is memory expensive; and 2) it requires a separate offline nearest-neighbour retrieval step. Instead, we leverage online clustering to identify nearest-neighbours on-the-fly. To avoid tuning $K$ for each dataset, we tuned $n$ once instead, then $K$ can be simply calculated as $K= N/n$. Additionally, we fix $\gamma=0.9n$ to ensure that each cluster contains sufficient samples to guarantee the quality of the cluster pseudo-label while relaxing clusters to not necessarily be equi-partitioned. Empirically, we found that using $n=250$ works reasonably well across datasets. To put it in context, this corresponds to $K=4800$ for ImageNet, and $K=200$ for CIFAR datasets.

\smallskip
\noindent \textbf{Multi-head Clustering} is another way to ensure robustness of our cluster pseudo-labels. To account for K-means stochastic nature, we can employ multi-head clustering to get different cluster assignments based on each head, at negligible cost. Subsequently, we can average the cluster pseudo-labels across the different heads. In practice, we find that for large datasets \eg ImageNet, cluster assignments slightly vary between heads so it is useful to use dual heads, while for smaller datasets, a single head is sufficient. 

\input{tables/cifar_and_mini_results.tex}

\smallskip
\noindent \textbf{Memory Analysis.} \putouralg is particulary useful due to its ability to leverage the entire prediction history in an epoch to approximate class density over neighbourhoods (represented by cluster pseudo-labels) with low memory cost. Particularly, it requires an overall of $\Ocal(4N + K \times C)$: $4N$ to store hard pseudo-labels, reliability, cluster assignments, and similarity scores; and $K \times C$ to store the cluster pseudo-labels. In contrast, if we were to employ a naive offline refinement approach, this would require $\Ocal(N \times d)$ to store the image embeddings for an epoch. For ImageNet dataset this translates to 9.6M memory units for \putouralg opposed to 153.6M for the naive approach\footnote{considering $d=128$} which is a 16$\times$ reduction in memory;  beside, eliminating the additional time needed to perform nearest neighbour retrieval.

\input{tables/domainnet_results.tex}