\section{Introduction}
\label{sec:introduction}

\begin{figure}[!t]
 \centering
  \includegraphics[width=0.47\textwidth]{figures/ProtoCon_teaser_nobg.pdf}
 \caption{\putouralg refines a pseudo-label of a given sample by knowledge of its neighbours in a prototypical embedding space. Neighbours are identified in an online manner using constrained K-means clustering. Best viewed zoomed in.}
 \vspace{-5mm}
 \label{fig:high_level_diagram}
\end{figure}

Semi-supervised Learning (SSL)~\cite{van2020survey, chapelle2009semi} leverages unlabeled data to guide learning from a small amount of labeled data; thereby, providing a promising alternative to costly human annotations.
In recent years, SSL frontiers have seen substantial advances through confidence-based pseudo-labeling~\cite{lee2013_pseudo, Sohn_fixmatch20, xie2019_uda, li2021comatch, wang2022debiased}. In these methods, a model iteratively generates pseudo-labels for unlabeled samples which are then used as targets to train the model. To overcome confirmation bias~\cite{arazo_pseudo, nassar2021all} \ie, the model being biased by training on its own wrong predictions, these methods only retain samples with high confidence predictions for pseudo-labeling; thus ensuring that only reliable samples are used to train the model. While confidence works well in moderately labeled data regimes, it usually struggles in label-scarce settings\footnote{\scriptsize We denote settings with less than 10 images per class as ``label-scarce.''}. This is primarily because the model becomes over-confident about the more distinguishable classes~\cite{nguyen2015deep, hein2019relu} faster than others, leading to a collapse.

In this work, we propose \putouralg, a novel method which addresses such a limitation in label-scarce SSL. Its key idea is to complement confidence with a label refinement strategy to encourage more accurate pseudo-labels. 
To that end, we perform the refinement by adopting a co-training~\cite{blum1998_co_training} framework: for each image, we obtain two different labels and combine them to obtain our final pseudo-label. The first is the model's softmax prediction, whereas the second is an aggregate pseudo-label describing the image's neighbourhood based on the pseudo-labels of other images in its vicinity. However, a key requirement for the success of co-training is to ensure that the two labels are obtained using sufficiently different image representations~\cite{van2020survey} to allow the model to learn based on their disagreements. As such, we employ a non-linear projection to map our encoder's representation into a different embedding space. We train this projector jointly with the model with a prototypical consistency objective to ensure it learns a different, yet relevant, mapping for our images. Then we define the neighbourhood pseudo-label based on the vicinity in that embedding space. In essence, we minimise a sample bias by smoothing its pseudo-label in class space via knowledge of its neighbours in the prototypical space. 


% To that end, instead of relying on the linear classifier output (prone to over-confidence) as our pseudo-label, we first refine it by ensuring it is consistent with its neighbouring images in
% a non-linear projection space (less prone to over-confidence). 
% \EA{I'd replace the last sentence with some nicer version of this: Our intuition is that the overconfidence is the byproduct of indistinguishable representations that are then overfit by the classifier (typically a simple linear one) to  the small labelled set. In \putouralg a secondary nonlinear head projects the representations to an alternative space in which their consistency is measured. This inconsistency is measured by evaluating the neighboring projections to evaluate local properties. The rationale is that for a given image, the representations may be simple to overfit with a linear classifier are not necessarily consistent with other samples in their vicinity. }
% More specifically, besides the linear classifier, we use a projection head to map images into a separate embedding space. Subsequently, for a confident unlabeled sample, we set its pseudo-label as a trade-off between its classifier output and the aggregate (pseudo)-labels of its neighbours in the embedding space. To ensure that such a projection learns sufficiently different image representations, we train it using a prototypical consistency objective which encourages image projections to cluster around their class prototypes (rather than predict their class labels). 
% \EA{subsequently, we perform \emph{online} clustering to effectively and efficiently probe the fine-grained local consistency with classifier's predictions. (maybe merge with the next paragraph.}
% \EA{this sentence reads a bit out of place.} 
% In this work, we propose \putouralg, a novel method which addresses label-scarce SSL. Its key idea is to complement confidence with a label refinement strategy to prevent such collapse and encourage more accurate pseudo-labels. To that end, we adjust the original pseudo-labels (\ie softmax predictions) by leveraging a different representation of the image: besides the classifier head, we use a separate projection head to embed images into a latent space trained for prototypical consistency. In such space, the training objective is for images to cluster around their class prototypes (rather than predict their class labels) hence encouraging homogeneous image clusters~\cite{snell2017prototypical}. Subsequently, at pseudo-labeling time, we refine the original classifier predictions via a weighted average with an aggregate label of the image's neighbourhood in the prototypical space. In effect, we minimise confirmation bias by smoothing a sample's pseudo-label in class space via knowledge of its neighbours' in the prototypical space. 

% We show that such co-training strategy leads to more accurate pseudo-labels and hence an improved performance.  

% First, besides the usual softmax class scores for each image, we obtain a  to a prototypical embedding space (via a separate MLP head) trained using a prototypical~\cite{snell2017prototypical} consistency objective. Subsequently, we refine the original softmax labels based on the sample's nearest neighbours in the prototypical space. In effect, we minimise confirmation bias by smoothing a sample's pseudo-label via knowledge of its neighbours' predictions history. Intuitively,  To that end, the neighbours are defined in an embedding space trained using a prototypical consistency objective to encourage well-formed clusters around class prototypes. 

% In this work, we propose \putouralg, a novel SSL method which addresses the shortcomings of earlier methods in label-scarce regimes. For a given unlabeled sample, instead of only relying on its classifier softmax output in the class probability space, we first project images to another embedding space trained using a prototypical~\cite{snell2017prototypical} consistency objective. Subsequently, we refine the original softmax labels based on the sample's nearest neighbours in the prototypical space. In effect, we minimise confirmation bias by smoothing a sample's pseudo-label via knowledge of its neighbours' predictions history. Intuitively,  To that end, the neighbours are defined in an embedding space trained using a prototypical consistency objective to encourage well-formed clusters around class prototypes. 

% \EA{again I think it is better to give an intuition first. this is more what you did not why you did it}

% \EA{why? this part is important but is not justified. say that the neighborhood provides information about how the modal maps the input in the embedding space. intuitively if an instance is surrounded by dissimilar labels, it means the model prediction is potentially an anomaly and cannot be trusted. Effectively, using this approach we have a simple, efficient and yet effective approach to modeling the density of the representations in the embedding space that allows us to reason about the confidence in predictions.}



Additionally, we design our method to be fully online, enabling us to scale to large datasets at a low cost.
We identify neighbours in the embedding space on-the-fly as the training proceeds by leveraging online K-means clustering. This alleviates the need to store expensive image embeddings~\cite{li2021comatch}, or to utilise offline nearest neighbour retrieval~\cite{li2021learning, zheng2022simmatch}. However, applying naive K-means risks collapsing to only a few imbalanced clusters making it less useful for our purpose. Hence, we employ a constrained objective~\cite{bradley2000constrained} lower bounding each cluster size; thereby, ensuring that each sample has enough neighbours in its cluster. We show that the online nature of our method allows it to leverage the entire prediction history in one epoch to refine labels in the subsequent epoch at a fraction of the cost required by other methods and with a better performance.
%   More specifically, unlike previous refinement methods~\cite{li2021comatch, zheng2022simmatch}, we neither use large memory banks to store image embeddings nor we perform an offline nearest neighbour retrieval step to recognise neighbours. Instead, we leverage online K-means clustering to identify neighbours on-the-fly as we train the model. \EA{I think if people have done that maybe it is better to say that before discussing your method, highlight the issue, give intuition of your own work and then discuss details. When it is written this way, it reads very defensively as "we don;t have much contribution, people had this idea before but we are different"}
% However, applying K-means naively risks collapsing to only a few imbalanced clusters making it less useful for our purpose. Hence, we employ a constrained K-means objective~\cite{bradley2000constrained} lower bounding each cluster size; thereby ensuring that each sample has enough neighbours in its cluster. Subsequently, upon each epoch of training, we calculate aggregate \emph{clusters pseudo-labels} which we use to refine \emph{samples pseudo-labels} in the subsequent epoch via a weighted average between the two \EA{you have not said what is the cluster pseudo-label}. The online nature of our method allows it to make use of the pseudo-label history of the entire dataset to do the label refinement at a fraction of the cost required by other methods and with a significantly better performance.

\putouralg's final ingredient addresses another limitation of confidence-based methods: since the model only retains high confident samples for pseudo-labeling, the initial phase of the training usually suffers from a weak training signal due to fewer confident predictions. In effect, this leads to only learning from the very few labeled samples which destabilises the training potentially due to overfitting~\cite{lucas2022barely}. To boost the initial training signal, we adopt a self-supervised instance-consistency~\cite{grill2020bootstrap, caron2021emerging} loss applied on samples that fall below the threshold. Our choice of loss is more consistent with the classification task as opposed to contrastive instance discrimination losses~\cite{he2020momentum, chen2020simple} which treat each image as its own class. This helps our method to converge faster without loss of accuracy.
% \EA{does it have a value?}

\begin{figure*}[h!]
 \centering
  \scalebox{0.9}{\includegraphics[width=0.97\textwidth]{figures/ProtoCon_main_fig.pdf}}
 \caption{\textbf{Method overview.} A soft pseudo-label $p^w$ is first obtained based on the weak view. Then it is refined using the sample's cluster pseudo-label $z^a$ before using it as target in $\Lcal_u$. Clustering assignments $a$ are calculated online using the projections of the weak samples $q^w$ in the embedding space $h$ which is trained via a prototypical loss $\Lcal_p$. Prototype targets are updated once after each epoch by averaging the accumulated projections of reliable samples for each class throughout the epoch. Cluster pseudo-labels are updated after each epoch using the cluster assignments/scores of all the samples and their respective hard pseudo-labels $\hat{y}$. Finally, the self-supervised loss $\Lcal_c$ ensures consistency between projections $q^s$ and $q^w$.}
 \label{fig:main_protocon}
 \vspace{-5mm}
\end{figure*}



We demonstrate \putouralg's superior performance against comparable state-of-the-art methods on 5 datasets including CIFAR, ImageNet and DomainNet. Notably, \putouralg achieves 2.2\%, 1\% improvement on the SSL ImageNet protocol with 0.2\% and 1\% of the labeled data, respectively. Additionally, we show that our method exhibits faster convergence and more stable initial training compared to baselines, thanks to our additional self-supervised loss. In summary, our contributions are:
\begin{itemize}
\setlength{\itemsep}{1pt}
\setlength{\parsep}{1pt}
\setlength{\parskip}{0.5pt}
    \item We propose a memory-efficient method addressing confirmation bias in label-scarce SSL via a novel label refinement strategy based on co-training.
    \item We improve training dynamics and convergence of confidence-based methods by adopting self-supervised losses to the SSL objective.
    \item We show state-of-the-art results on 5 SSL benchmarks.
\end{itemize}