\section{Implementation Details}\label{app:impl}
We will release code and weights, along with the specific configurations. We follow very similar configurations to \cite{he2022masked, feichtenhofer2022masked}, since we found them to also work well for \model.

\paragraph{\model architecture.}
We follow the standard ViT architecture \cite{dosovitskiy2020image}, which has a stack of Transformer blocks, each of which consists of a multi-head attention block and an MLP block, with LayerNorm (LN). A linear projection layer is used after the encoder to match the width of the decoder. We use sine-cosine position embeddings for both the encoder and decoder. For the projection and target networks, we do average pooling on the encoder features and follow the architecture of \cite{bardes2021vicreg}, which consist of linear layer projecting the features up  to twice the size of the encoder and two blocks of linear layer  the preserve the size of the features, followed by BatchNormalization and a ReLu non-linearity.

We extract features from the encoder output for fine-tuning and linear probing. We use the class token from the original ViT architecture, but remark that similar results are obtained without it (using average pooling).
\vspace{-5pt}
\paragraph{Pre-training.}
The default settings can be found in Table \ref{tab:impl_vicmae_pretrain}. We do not perform any color augmentation, drop path or gradient clipping. We initialize our transformer layer using xavier\_uniform \cite{glorot2010understanding}, as it is standard for Transformer architectures. We use the linear \textit{lr} scaling rule \cite{goyal2017accurate}: \textit{lr} = \textit{base\_lr}$\times$batchsize / 256.
\vspace{-5pt}
\paragraph{End-to-end finetuning.}
We follow common practice for end-to-end finetuning. Default settings can be found in Table \ref{tab:impl_vicmae_finetune}. Similar to \cite{he2022masked}, we use layer-wise \textit{lr} decay.
\vspace{-5pt}
\paragraph{Linear evaluation.}
We follow \cite{he2022masked, feichtenhofer2022masked} for linear evaluation results. As previous work has found \cite{chen2021empirical}, we do not use common regularization techniques, mixup, cutmix, and drop path, likewise, we set the weight decay to zero. We add an extra BatchNorm layer without the affine transformation after the encoder features. Default settings can be found in Table \ref{tab:impl_vicmae_linear}.
\vspace{-5pt}

\begin{table}[ht]
\small
\centering
\begin{tabular}{rc}
\toprule
config & value \\
\midrule
optimizer & AdamW \cite{loshchilov2017decoupled} \\
base learning rate & 1.5e-4 \\
weight decay & 0.05 \\
optimizer momentum & $\beta_1, \beta_2{=}0.9, 0.95$ \cite{chen2020generative} \\
batch size & 4096 \\
learning rate schedule & cosine decay \cite{loshchilov2016sgdr} \\
warmup epochs \cite{goyal2017accurate} & 40 \\
epochs & 800 \\
augmentation & hflip, crop [0.5, 1] \\
contrastive loss weight $\lambda$ & 0.025 \\
contrastive loss schedule & 0 until epoch 200 then 0.025 \\
\bottomrule
\end{tabular}
\vspace{-.5em}
\caption{\textbf{Pre-training setting.}}
\label{tab:impl_vicmae_pretrain} \vspace{-.5em}
\end{table}

\begin{table}[ht]
\small
\centering
\begin{tabular}{rc}
\toprule
config & value \\
\midrule
optimizer & AdamW \\
base learning rate & 1e-3 \\
weight decay & 0.05 \\
optimizer momentum & $\beta_1, \beta_2{=}0.9, 0.999$ \\
layer-wise lr decay \cite{clark2020electra,bao2021beit} & 0.75 \\
batch size & 1024 \\
learning rate schedule & cosine decay \\
warmup epochs & 5 \\
training epochs & 100 \\
augmentation & RandAug (9, 0.5) \cite{cubuk2020randaugment} \\
label smoothing \cite{szegedy2016rethinking} & 0.1 \\
mixup \cite{zhang2017mixup} & 0.8 \\
drop path \cite{huang2016deep} & 0.1\\
\bottomrule
\end{tabular}
\vspace{-.5em}
\caption{\textbf{End-to-end fine-tuning setting.}}
\label{tab:impl_vicmae_finetune} \vspace{-.5em}
\end{table}

\begin{table}[t]
\small
\centering
\begin{tabular}{rc}
\toprule
config & value \\
\midrule
optimizer & SGD \\
base learning rate & 0.1 \\
weight decay & 0 \\
optimizer momentum & 0.9 \\
batch size & 4096 \\
learning rate schedule & cosine decay \\
warmup epochs & 10 \\
training epochs & 90 \\
augmentation & RandomResizedCrop \\
\bottomrule
\end{tabular}
\vspace{-.5em}
\caption{\textbf{Linear evaluation setting.}
\label{tab:impl_vicmae_linear}}
\end{table}

\section{Semi-supervised evaluation on ImageNet.}
\label{app:semi_sup_imagenet}
We also test \model on the problem of Semi-Supervised evaluation on the ImageNet dataset. The setting consists on training on a subset of the training data and testing on the whole validation data. We chose subsets of size 5\%, 10\%, 25\%, 50\%, 75\% and 100\% of the whole training set of ImageNet. We compare our model against MAE \cite{he2022masked} + SiamSiam \cite{chen2021exploring}, and MAE \cite{he2022masked} + VicReg \cite{bardes2021vicreg}. Results are shown on Table \ref{tab:semi_sup_imagenet}, and show the supperiority of \model over simple combinations of contrastive learning and masked image modeling.

\begin{table}[ht]
\centering
\small
\setlength\tabcolsep{1.8pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lcccccc}
\toprule
\textbf{Percentage of data} & \textbf{5\%} & \textbf{10\%} & \textbf{25\%} & \textbf{50\%} & \textbf{75\%} & \textbf{100\%} \\
\midrule
MAE \cite{he2022masked} + SiamSiam \cite{chen2021exploring} & 7.15 & 23.41 & 39.73 & 54.94 & 62.88 & 67.44 \\
MAE \cite{he2022masked} + VicReg \cite{bardes2021vicreg} & 47.48 & 56.63 & 66.62 & 73.00 & 75.29 & 77.41 \\
\model (ours) & \textbf{50.25} & \textbf{58.22} & \textbf{67.65} & \textbf{73.97} & \textbf{75.80} & \textbf{77.89} \\
\bottomrule
\end{tabular}
\caption{\textbf{Semi-supervised evaluation on ImageNet.} We performed end-to-end finetuning using the settings in \ref{tab:impl_vicmae_finetune}, but disable RandAug and MixUp for this experiment.}
\label{tab:semi_sup_imagenet} \vspace{-.5em}
\end{table}
