\section{Conclusion}
\label{sec:conclusion}

In this work, we have introduced \model, a method that allows to use unlabeled videos to learn useful representation for image recognition tasks. We achieve this randomly sampling frames from a video and using contrastive learning to pull together frames from the same video and push apart frames from different videos, likewise we also use masked image modeling on each frame to learn good local features of the scene presented in each frame. The main contribution of our work is showing that is possible to combine masked image modeling and contrastive learning by pooling the local representations of the MAE prediction heads into a global representation that is used for contrastive learning. %Experimental evaluation on the task of transfer learning from video pre-training to image recognition tasks show the advantage of this method over previous state-of-the art.% by an improvement of 1.58\% points in absolute accuracy while still maintaining good performance on the task of video action recognition.
%We believe this work is an important step forward for future video pre-training efforts, as our approach is powerful, yet is comprised of a simple additions to previous existing methods. We recognize that while our work improves over the previous state-of-the art the gap between image pre-training and video-pre-training still exists and future work is necessary to close this gap. Nevertheless, our results suggest that there is potential in using video pre-training as a paradigm for learning general visual representations.
The design choices that we have taken, when designing ViC-MAE show that our work is easily extensible in various different ways. For example, improvements in contrastive learning for images can be directly adapted into our framework. Likewise, pixel reconstruction can be replaced by features that are important for video representation like object correspondence, or optical flow.% in a similar way that the MaskFeat model \cite{wei2022masked} does for images. The use of other transformer backbones like the Swin transformer \cite{liu2022swin} can also improve on the results and help extend our methodology to other image recognition tasks like object recognition and image segmentation.