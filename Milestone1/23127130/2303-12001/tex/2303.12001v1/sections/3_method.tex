\section{Method}
We propose \model for space-time feature learning, which works using contrastive learning at the time level and masked image modelling at the space level.

\subsection{Background}
We provide here some background terminology and review of closely related methods that we build upon.
\paragraph{Masked image modeling.}
Masked image modeling provides a way to learn visual representations in a self supervised manner. These methods learn representations by first masking out parts of the input and then training a model to fill in the blanks using a simple reconstruction loss. In order to do this, we use an encoder $f_{\theta}$ that takes the non-masked input and learns a representation $x$, such that a decoder $d_{\phi}$ can reconstruct the masked part of the input. More formally, let $x$ be the representation learned by the encoder for masked image $I$ with mask $M$ such that $f_{\theta}(I \odot M)$. A decoder $d$ is then applied to obtain the first loss over masked and unmasked tokens $d_{\phi}(x)$. This defines the following reconstruction loss which is only computed over masked tokens:
\begin{equation}
    \begin{aligned}
    \mathcal{L}^{\text{MASK}}_{I} = \left \|d_{\phi}(f_{\theta}(I \odot M)) \odot (1 -M) %\\
    - I\odot (1-M ) \right \|^2_2.
    \end{aligned}
    \label{eq: recons}
\end{equation}

\paragraph{Contrastive learning.}
In common image-level contrastive methods, learning with negatives is achieved by pushing the representation of the positive pairs (different augmented views of the same image) to be close to each other while pulling the representation of negative pairs further apart. More formally, let $I$ and $I'$ be two augmented views of the same image. Contrastive learning uses a siamese network with a predictor encoder $\mathcal{P}$ and a target encoder $\mathcal{T}$ \cite{xu2021rethinking, chen2020simple}. The output of these networks are $l_2$-normalized to be $p = \mathcal{P}(I) / 	\lVert \mathcal{P}(I) \rVert_2$ and $z = \mathcal{T}(I') / 	\lVert \mathcal{T}(I') \rVert_2$. Given a positive pair from a minibatch of size $N$, the other $2(N-1)$ examples are treated as negative examples. The objective then is to minimize the Info-NCE loss as defined in \cite{oord2018representation}. When learning with negatives $\mathcal{P}$ and $\mathcal{T}$ typically share the same architecture and weights.

\paragraph{Negative-free representation learning.} 
Global visual representation learning without negative examples has been achieved recently using a variety of methods, achieving similar performance to contrastive learning methods. By not using negative examples, the objective becomes simpler, just minimizing the cosine feature distance for two different views of the same input. The issue with this type of optimization is that it can lead to representation collapse~\cite{grill2020bootstrap, chen2021exploring}. There are several ways to avoid representation collapse such as the methods proposed by Chen~et~al.~\cite{chen2021exploring} (SiamSiam) and Bardes~et~al.~\cite{bardes2021vicreg} (VicReg). Siamsiam introduces an asymmetry between the predictor encoder $\mathcal{P}$ and the target encoder $\mathcal{T}$ by adding one extra multi-layer perceptron to the predictor encoder to stop the gradients that are backpropagated from the loss of the target network. VicReg instead uses two regularization terms: (i) A term that maintains the variance of each embedding dimension above a threshold, and  (ii) A term that decorrelates each pair of variables. The variance term (i) forces the embedding vectors of samples
within a batch to be different while the covariance term (ii) prevents the collapse of the representations.

\subsection{Combining MAE with Contrastive Methods.}
One trivial way to combine MAE with contrastive learning methods is to use the $[\text{CLS}]$ token of the transformer   as a global video feature representation. This representation allows us to use any contrastive learning loss without modifications to the underlying ViT-B/16 transformer encoder.

This combination works as follows: Sample two frames $I_i, I_j$ from a video and perform patch-level masking. The two frames are processed by the ViT-B/16 model $f_{\theta}$ producing token representations $f_{\theta}(I_i) = \{x_i^{\text{CLS}}, x_i^1, x_i^2, \cdots, x_i^L\}$, where $L$ is the sequence length of the transformer model. This is divided into two disjoint sets. The set $\{x_i^1, x_i^2, \cdots, x_i^L\}$ represents the local features of the frame $i$ and are used for masked image modeling following Eq. \ref{eq: recons}. Then, the $x_i^{\text{CLS}}$ token can be used as a global representation with a contrastive loss. 

We experiment with this approach using the SiamSiam loss~\cite{chen2021exploring} and the VicReg loss~\cite{bardes2021vicreg}. We review here these methods and how to combine them with MAEs, but the reader is referred to the original works for a more in-depth explanation of these methods~\cite{chen2021exploring, bardes2021vicreg}.

\paragraph{SiamSiam.}
A combination of SiamSiam and MAE, which we refer to as {\em MAE + SiamSiam} uses the  $x_i^{\text{CLS}}$ token which represents the global video representation as follows: We pass $x_i^{\text{CLS}}$ to a projector network $\mathcal{P}$ to obtain $p_i \triangleq  \mathcal{P}(x_i^{\text{CLS}}) / 	\lVert \mathcal{P}(x_i^{\text{CLS}}) \rVert_2$. A similar procedure is followed for frame $j$, but the global representation is not passed to the projector network $\mathcal{P}$ in order to obtain $z_j \triangleq  x_i^{\text{CLS}} / 	\lVert x_i^{\text{CLS}} \rVert_2$. The SiamSiam objective is then applied as follows:  
\begin{equation}
    \begin{aligned}
    \mathcal{L}^{\text{ \tiny SiamSiam}}_{p_i,z_j} = \lVert p_i - z_j \rVert^2_2 = 2 (1 - p_i \cdot z_j).
    \end{aligned}
    \label{eq: pos_loss_siamsiam}
\end{equation}

\paragraph{VicReg.}

A combination of VicReg and MAE, which we refer to as {\em MAE + VicReg} uses the $x_i^{\text{CLS}}$ token which represents the global video representation as follows: We pass it to a projector network $\mathcal{P}$ to obtain $p_i \triangleq  \mathcal{P}(x_i^{\text{CLS}}) / 	\lVert \mathcal{P}(x_i^{\text{CLS}}) \rVert_2$, we repeat this procedure for frame $j$ using the target network $\mathcal{T}$ to obtain $z_i \triangleq  \mathcal{T}(x_i^{\text{CLS}}) / 	\lVert \mathcal{T}(x_i^{\text{CLS}}) \rVert_2$. The loss is calculated at the embedding level on $p_i$ and $z_j$. The video frames are processed in batches, let us denote $P = [p^1, \cdots, p^n]$ and $Z = [z^1, \cdots, z^n]$, where each $p^m$ and $z^m$ are the global representation of video $m$ after the projector network and target network respectively in a batch of size $n$ vectors of dimention $d$. Let us denote by $p_l$ the vector composed of each value at dimension $l$ in all vectors in $P$. The variance loss of VicReg is then calculated as follows:
\begin{equation}
    \begin{aligned}
    v(P) = \frac{1}{d} \sum_{l=1}^d \text{max}(0, \gamma - S(p_i, \epsilon)),
    \end{aligned}
    \label{eq: var_loss_vicreg}
\end{equation}
where $S(z, \epsilon) = \sqrt{\text{Var}(z) + \epsilon}$ and $\gamma$ is a constant target value for the standard deviation, fixed to 1. The covariance loss of VicReg can be calculated as:
\begin{equation}
    \begin{aligned}
    c(P) = \frac{1}{d} \sum_{l\neq k}^d [\text{Cov}(p^m)]^2_{l,k},
    \end{aligned}
    \label{eq: cov_loss_vicreg}
\end{equation}
where $\text{Cov}(p^m) = \frac{1}{N - 1} \sum_m (p^m - \bar{p})(p^m - \bar{p})^T$. The final VicReg loss over the batch  is defined as:
\begin{equation}
    \begin{aligned}
    \mathcal{L}^{\text{\tiny VicReg}}_{p_i,z_j} & = \frac{\lambda}{n}\lVert p_i - z_j \rVert^2_2  + \mu \left[ v(P) + v(Z)\right] +  \nu \left[ c(P) + c(Z)\right].
    \end{aligned}
    \label{eq: loss_vicreg}
\end{equation}
We perform experiments using these two combinations of MAE and contrastive losses as baseline comparisons for our method but found them to be underperforming with only contrastive or only masked methods. In other words, it is not trivial to adapt constrastive learning methods to be used in combination with masked autoencoders.%, see Section \ref{sec:results}, Table \ref{tab:ours_vs_contrastive_loss} for more details.

\subsection{\model}
Building on masked image modeling and image-level similarity learning, we propose to learn spatio-temporal representations by using masking image modeling at the frame level and image level similarity at the time level. % in a similar way to the Video Frame Similarity framework proposed in \cite{xu2021rethinking}.
This means that each video frame is pulled towards a global video representation in the latent space. This can lead to representations that are invariant to object deformations, appearance changes and viewpoint variations. See Figure \ref{fig:vic-mae} for a general overview of our model.

%\paragraph{Loss functions.}

Given a video with $T$ frames $\{I_1, I_2, \cdots, I_T\}$, we sample two frames $I_i, I_j$ as a positive pair input during one step of training. After an input image tokenizer layer we obtain a set of patch-level token representations $X_i$ and $X_j$ for each frame. Then, we apply token masking by generating a different random mask $M_i$ and $M_j$ and apply them to both of the corresponding input frames to obtain a subset of input visible tokens $X_i^{(v)}$ and $X_j^{(v)}$. These visible token sets are then forwarded to a ViT encoder which computes a set of representations $f_{\theta}(X_i^{(v)})$ and $f_{\theta}(X_j^{(v)})$ respectively. Finally, for the first frame we compute $\hat{I}_i = d_{\phi}(f_{\theta}(X_i^{(v)} + f_m))$ where we have added a mask token $f_m$ to let the decoder know which patches were masked and allows to predict patch-shaped outputs through $\hat{I}_i$. These output patches are then trained to minimize the $\ell_2$ loss with the true patches in the input image: 
\begin{equation}
    \mathcal{L}_i^{\text{MASK}} = \lVert \hat{I}_i - I_i \rVert_2^2.
\end{equation}
So far we have described only a standard masked autoencoder (MAE). In order to apply contrastive pre-training we use a separate prediction branch in the network by applying a global pooling operator $\Omega$ over the output representations $f_{\theta}(X_i^{(v)})$ from the main branch and $f_{\theta}(X_j^{(v)})$ from the siamese copy of the network.  %We take the visible tokens $X_v$, which are the \textit{local} representation $f_{\theta}(I_i) = \{x_i^{\text{CLS}}, x_i^1, x_i^2, \cdots, x_i^L\}$ learned by the encoder and apply an aggregation operator on them to obtain a \textit{global} representation.
%We found this step to be crucial for our model to work, notice that the masked image modeling which encourages representations that are meaningful to be able to reconstruct the image help to avoid the collapse of the representations. 
This step simplifies the formulation of our method and avoids using additional complicated losses or the \texttt{gradient-stop} operator to avoid feature representation collapse since the pooled features can not default to the zero vector as they also are being trained to reconstruct patches. We experiment using various aggregation methods including \textit{mean} pooling, \textit{max} pooling, and \textit{generalized mean} (GeM) pooling \cite{radenovic2018fine}.

%\begin{equation*}
%    \begin{aligned}
%        \bar{x}_i = \left(\frac{1}{L} \sum_l^L (x_i^l)^p \right)^{\frac{1}{p}},
%    \end{aligned}
%    \label{eq: gem}
%\end{equation*}
%where $p$ is a learnable parameter, initialized to the value of 3.

These global representations are then forwarded to a predictor encoder $\mathcal{P}$ and a target encoder $\mathcal{T}$ to obtain frame representations:
$$p_i \triangleq  \mathcal{P}(\Omega(f_{\theta}(X_i^{(v)}))) / 	\lVert \mathcal{P}(\Omega(f_{\theta}(X_i^{(v)})))) \rVert_2,$$
and 
$$z_j \triangleq  \mathcal{T}(\Omega(f_{\theta}(X_j^{(v)}))) / 	\lVert \mathcal{T}(\Omega(f_{\theta}(X_j^{(v)})))) \rVert_2$$
respectively.  The predictor network $\mathcal{P}$ and target network $\mathcal{T}$ are symmetrical and we use standard blocks designed for contrastive learning~\cite{bardes2021vicreg, chen2020simple, chen2021exploring}. These blocks consist of a Linear $\rightarrow$ BatchNorm1d $\rightarrow$ ReLU block repeated $2$ times. From these representations, we apply the InfoNCE contrastive learning loss as follows: 
\begin{equation}
    \begin{aligned}
    \mathcal{L}^{\text{NEG}}_{p_i,z_j} = -\log \frac{\text{exp}(p_i \cdot z_j / \tau)}{\sum^{2N}_{k=1} \mathbbm{1} [p_i \neq z_k] \text{exp}(p_i \cdot z_k / \tau)},
    \end{aligned}
    \label{eq: neg_loss}
\end{equation}
where the denominator includes a set of negative pairs with representations $z_k$ computed for frames from other videos in the same batch, $\mathbbm{1} [p_i \neq z_k] \in \{0, 1\}$ is an indicator function evaluating to $1$ when $p_I \neq z_k$ and $\tau$ denotes a temperature parameter. 

The final loss is $\mathcal{L} = \mathcal{L}^{\text{MASK}} + \lambda \mathcal{L}^{\text{NEG}}$, where $\lambda$ is a hyperpameter controlling the relative influence of both losses. In practice, we use an schedule to gradually introduce the contrastive loss and let the model learn good local features at the beginning of training.

% \begin{algorithm}[H]
%   \caption{\model \ pytorch pseudocode.}
%   \label{alg:method}
%     \definecolor{codeblue}{rgb}{0.25,0.5,0.5}
%     \definecolor{codekw}{rgb}{0.85, 0.18, 0.50}
%     \newcommand{\algofontsize}{11.0pt}
%     \lstset{
%       backgroundcolor=\color{white},
%       basicstyle=\fontsize{\algofontsize}{\algofontsize}\ttfamily\selectfont,
%       columns=fullflexible,
%       breaklines=true,
%       captionpos=b,
%       commentstyle=\fontsize{\algofontsize}{\algofontsize}\color{codeblue},
%       keywordstyle=\fontsize{\algofontsize}{\algofontsize}\color{black},
%     }
% \begin{lstlisting}[language=python]
% # f: encoder network, d: decoder network
% # h: predictor mlp
% # N: batch size, L: sequence length of trasnformer, D: dimension of the representations
% # mse_loss: Mean square error loss function, ce_loss: Cross-entropy loss, relu: ReLU activation function
% # sample: sample two frames of a video, mask: randomly mask out a part of the images 

% for v in loader: # load a video batch with N samples
%     # two randomly selected frames from video v
%     I_i, I_j = sample(v)

%     # randomly mask parts of the image
%     x_m_i, m_i = mask(I_i)
%     x_m_j, m_j = mask(I_j)
    
%     # compute representations
%     x_i = f(x_m_i) # N x L x D
%     x_j = f(x_m_i) # N x L x D

%     # decode the representations
%     I_p_i = d(x_i)
%     I_p_j = d(x_j)
    
%     # mean pooling on the representations
%     p_i = x_i.mean(dim=1)
%     z_i - x_j.mean(dim=1)

%     # normalize the representations
%     p_i = l2_normalize(p_i, dim=1)
%     z_i = l2_normalize(z_i, dim=1)

    
    
%     # variance loss
%     std_z_a = torch.sqrt(z_a.var(dim=0) + 1e-04)
%     std_z_b = torch.sqrt(z_b.var(dim=0) + 1e-04)
%     std_loss = torch.mean(relu(1 - std_z_a)) + torch.mean(relu(1 - std_z_b))
    
%     # covariance loss
%     z_a = z_a - z_a.mean(dim=0)
%     z_b = z_b - z_b.mean(dim=0)
%     cov_z_a = (z_a.T @ z_a) / (N - 1)
%     cov_z_b = (z_b.T @ z_b) / (N - 1)
%     cov_loss = off_diagonal(cov_z_a).pow_(2).sum() / D 
%                     + off_diagonal(cov_z_b).pow_(2).sum() / D

%     # loss
%     loss = lambda * sim_loss + mu * std_loss + nu * cov_loss

%     # optimization step
%     loss.backward()
%     optimizer.step()
% \end{lstlisting}
% \end{algorithm}