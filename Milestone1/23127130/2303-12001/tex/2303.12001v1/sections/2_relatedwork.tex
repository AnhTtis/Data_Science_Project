\section{Related Work}
Our work is related to general self-supervised learning methods from video, and methods specifically targeting image representation learning from video in some form. In this section, we provide a summary for representative works.% but it is not an exhaustive survey.

\paragraph{Self-supervised learning from videos.}
Self-supervised learning in the video domain provides a way to exploit the time dimension as a learning signal that encourages models to learn a richer representation in comparison to learning only from images.
In the past few years, this has involved the creation of pretext training tasks that leverage prior knowledge about videos such as frame continuity and forecasting \cite{srivastava2015unsupervised,walker16,vondrick16,mathieu16,lotter17,diba17}, object tracking \cite{agrawal2015learning,wang15,pathak2017learning,wang19}, and others.
Other approaches also leverage facts about videos, but design the training in a contrastive learning paradigm \cite{bardes2021vicreg,chen2021exploring,wu2021contrastive,xu2021rethinking,gordon2020watching,parthasarathy2022self}. These methods leverage the temporal continuity of videos to sample negative and positive pairs for training under a contrastive learning objective.
More recently, self-supervised learning approaches based on masked auto encoders (MAE) \cite{he2022masked} rely on masked image modeling adapted to video data to pre-train models. These models can be later used for transfer learning to downstream tasks \cite{feichtenhofer2022masked,wei2022masked,tong22} .
These methods train models by learning to reconstruct missing patches from a video in the form of either spatiotemporal patches or random patches.
In contrast to the aforementioned works, our approach combines the discriminative representation learning of contrastive methods with the generative learning of masked image modeling methods in a unified pre-training strategy that is applicable to image and video downstream tasks.


% Self-supervised learning over videos has focused on designing pretext task to learn good representations. These span many approaches that leverage temporal coherence, object tracking and optical flow \cite{agrawal2015learning, pathak2017learning, srivastava2015unsupervised, kulkarni2019unsupervised, jabri2020space}. More recently, other approaches have leveraged, masked autoencoding, contrastive learning, and other pretext tasks, like reconstructing HoG features among others, \cite{wei2022masked, feichtenhofer2022masked, sermanet2018time}. But these models have mostly focused on evaluating the learned representation using video downstream tasks such as action recognition, motion segmentation and object tracking. It is natural to think that natural deformations, object movement, and occlusions that occur on videos provide a strong signal for learning representation that are good for \textit{images}, and recent works \cite{feichtenhofer2022masked, gordon2020watching} have attempted to use the representation learned from video by using masked autoencoding and then deflating the temporal dimension or using frame-based methods.

% The works more similar to our own in this regard are \cite{gordon2020watching, parthasarathy2022self}. These utilize contrastive learning at the time dimension to learn strong representations from videos that are evaluated in image downstream tasks. Our work mainly differs in its use of masked autoencoding at the image level to bypass the need of using strong augmentations that are necessary for contrastive learning methods to work.

\paragraph{Learning image representations from video.}
While datasets such as ImageNet provide a large and diverse source of data for the development of perceptual systems, it still an incomplete representation of the world and how it is experienced by visual recognition models at test time.
Image datasets lack a temporal dimension which provides a richer source of information for intelligent agents in the form of object deformations, temporal occlusions, multiple views, lighting changes, and more.
This missing information causes models developed from image datasets to lack robustness when used in real world applications in which the inputs will be a continuous stream of frames in the form of video.
To this end, there have been recent works that focus on learning robust image representations from video data.
Video Contrastive Noise Estimation (VINCE) \cite{gordon2020watching} argues that video provides natural image augmentations for free, and these can improve performance over artificially produced augmentations and even pretraining on ImageNet.
Video Frame-level Similarity (VFS) \cite{xu2021rethinking} shows that using the time dimension to learn correspondences can produce models learned on video datasets that transfer to downstream image tasks.
In \cite{wu2021contrastive}, they use cycle consistency that first maps an image from a video into a similar image to another video, and then map that image back to the closest frame within the initial video which could vary slightly from the origin of the cycle.
Feichtenhofer~et.al~\cite{feichtenhofer2022masked} uses masked visual modeling for video representation learning but the model is shown to be useful on image level tasks.
More recently, Piergiovanni~et.al.~\cite{piergiovanni2022rethinking} proposed models that can simultaneously learn from image and video datasets while Parthasarathy~et.al~\cite{parthasarathy2022self} proposes a video dataset curation procedure that addresses the domain mismatch between video and image datasets. 
In contrast, our method aims to learn representations from any video dataset using \model which learns representations from video that generalize to both image and video datasets.

% Using comntrastive methods
% Watching the world go by: Representation learning from unlabeled videos. https://arxiv.org/pdf/2003.07990.pdf learning form the R2V2 dataset and transfering to imagenet
% Rethinking self-supervised correspondence learning: A video frame-level similarity perspective. https://arxiv.org/pdf/2103.17263.pdf  Only one experiment in imagenet from something somewthing v2 they do very poorly 
% Contrastive Learning of Image Representations with Cross-Video Cycle-Consistency https://arxiv.org/abs/2105.06463 only one experiment transfering from R2V2 to imagenet 
% Using masked autoencoder methods
% Masked Autoencoders As Spatiotemporal Learners https://arxiv.org/abs/2205.09113 only one experiment with vit-l trasnfering from K400 to imagenet 
%Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video Learning https://arxiv.org/pdf/2212.03229v1.pdf \textbf{only one experiment with vit-b trasnfering from K600 to imagenet

% other methods
% SELF-SUPERVISED VIDEO PRETRAINING YIELDS STRONG IMAGE REPRESENTATIONS https://arxiv.org/pdf/2210.06433.pdf experiment transfering from a dataset they created to PASCAL and COCO datsets no imagenet experiments
