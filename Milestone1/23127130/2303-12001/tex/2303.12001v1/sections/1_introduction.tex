\section{Introduction}
Self-supervised visual representation learning has led to great success in image benchmarks~\cite{chen2020simple, he2020momentum, caron2021emerging, he2022masked}. This success has been mainly driven by two paradigms: Joint-embedding methods and masked image modeling (MIM). Joint-embedding methods learn representations that are invariant to specific transformations, these methods are either contrastive \cite{chen2020simple, he2020momentum, caron2021emerging}, or negative free methods \cite{chen2021exploring, bardes2021vicreg}. More recently, masked image modeling has emerged as a successful alternative to joint embedding methods. These methods work by randomly masking out parts of the input and forcing a model to predict the masked parts \cite{bao2021beit, he2022masked, feichtenhofer2022masked, wei2022masked}. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/figure-1-maex.pdf}
    %\vspace{-0.2in}
    \caption{\model operates over video frames using masked image modeling at the frame level and contrastive learning at the temporal level. Since our model operates over video frames, it can take advantage of viewpoint and temporal consistency which are absent in data augmentations over isolated images. %Moreover, our contrastive loss avoids representation collapse by effectively combining patch reconstruction losses.
    }
    \label{fig:vic-mae_idea}
    %\vspace{-0.25in}
    % Powerpoint for this figure is uploaded.
    % https://drive.google.com/file/d/1EdtyqtCo3SbRZw8OVEIPiJucbVMPn3pA/view?usp=sharing
\end{figure}


% 4) image classification / video classification

% 1) masked image modeling

% 2) contrastive learning

% 3) learning from videos

% 4) these too papers https://arxiv.org/pdf/2112.10740.pdf https://arxiv.org/pdf/2206.01204.pdf
Self-supervised methods from the image domain have been successfully replicated for \textit{video} representation learning with remarkable success \cite{feichtenhofer2022masked, wei2022masked, qian2021spatiotemporal, feichtenhofer2021large}. These methods yield strong video feature representations that transfer to a range of downstream video recognition tasks. However, there is still a gap in performance in the {\em video-to-image} transfer learning setting where it is difficult to obtain good image features by relying solely on video pre-training. 
%But, the representations learned by these models still do not obtain good performance on \textit{images}. 
Learning from video should also yield good image representations since videos naturally contain complex changes in pose, viewpoint, deformations, among others. These variations can not be simulated through the standard image augmentations used in joint-embedding methods or in MIM methods. In this work, we propose Video Contrastive Masked AutoEncoding (\model) and show that our method improves {\em video-to-image} transfer performance while maintaining performance on video representation learning.


The work proposed by Gordeon et.al.~\cite{ gordon2020watching} uses two distinct frames from a video as augmentations for instance discrimination similar to contrastive methods getting good results for video benchmarks but still relatively modest results in image benchmarks i.e.~ImageNet. Feichtenhofer et.al.~\cite{feichtenhofer2022masked} uses a simple masked image modelling objective (pixel reconstruction) that obtains state-of-the-art results on video benchmarks and very strong results on ImageNet but still below the same method applied only on images. More recently, Parthasarathy et.al.~\cite{parthasarathy2022self} becomes the first to obtain results that rival ImageNet results by modifying the MoCLR \cite{tian2021divide} framework to videos using a larger crop size, temporal augmentations, and multi-scale contrastive pooling, but most importantly; this work devises a data collection methodology to address the domain mismatch. Given these encouraging results, we take a step back and ask the questions: Do we really need to collect more data to obtain good image representations from video? Are negative examples as used in \cite{gordon2020watching, parthasarathy2022self} actually needed to learn good video-to-image representations? Can we combine the simplicity of masked image modeling over the same frame and contrastive learning over different frames to learn global video representations?



With these question in mind, we propose to leverage contrastive learning and masked image modeling for videos in a single framework which we refer as \model (\textbf{Vi}deo \textbf{C}ontrastive MAE). As illustrated in Figure \ref{fig:vic-mae_idea}, we sample two frames from a single video and use contrastive learning over the time dimension to make the representation learned by the encoder similar. This forces the encoder to learn a global representation for videos, and then we use masked image modeling over single frames with a simple reconstruction loss to encourage the encoder to also learn local features of the frames of the video. We also attempted to combine MAE with standard contrastive methods such as VicReg~\cite{bardes2021vicreg}, and SiamSiam~\cite{chen2021exploring} by using the $[\text{CLS}]$ token as a global video representation, but found that this simple strategy is insufficient to obtain good image recognition performance. Instead we propose to aggregate the local features learned by the MAE encoder using a global pooling layer.  %for which we try many methods including \textit{mean}, \textit{max}, and \textit{generalized mean pooling}, 
 Then we use this aggregated global feature representation using a contrastive loss over this global video representation. We found this approach to be superior to  using the $[\text{CLS}]$ token. We use the ViT architecture~\cite{dosovitskiy2020image} as our base model as this is the standard architecture used for previous masked image modeling methods~\cite{feichtenhofer2022masked, wei2022masked}. Our models are then finetuned for various image recognition tasks to demonstrate the transfer capabilities of our method. Based on our experiments, we report the following findings:

\begin{enumerate}[label=(\roman*)]
  \item Training with large frame gaps improves image classification performance. Joint-embedding methods usually require strong augmentations, which in our video pre-training setting come naturally from choosing large gaps in between sampled frames.
  \item Training with negative pairs surpasses methods that only train with positive samples. This is in line with results for other methods that train on videos and evaluate on images~\cite{ gordon2020watching, parthasarathy2022self}.
  \item Training with strong image transformations as augmentations is not necessary. This is in contrast to other works that still need to apply strong color and view augmentations to achieve good results~\cite{ gordon2020watching, parthasarathy2022self}. 

\end{enumerate}

Our contributions can be summarized as follows: (1) We obtain the best {\em video-to-image} transfer learning results in the Imagenet-1k benchmark, (2) We propose \model by combining contrastive learning with masked image modeling and show that our proposed method achieves superior accuracy than strong alternatives based on exisiting methods (VicReg, Siamsiam), and (3) We show superior transfer learning accuracy on a wide array of downstream image classification tasks compared to a baseline MAE pre-trained network.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.98\textwidth]{figures/model-mae.pdf}
    \vspace{-0.1in}
    \caption{\textbf{\model} inputs two distant frames from a video using a siamese backbone (shared weights), and randomly masks them, before passing them trough a ViT-Base model which learns a representation of local features using masked image modeling. A global representation of the video is then constructed by global pooling of the local features learned by the ViT-Base model trained to reconstruct individual patches using an $\ell_2$ loss. A standard predictor and a target encoder are used with a contrastive learning loss over the batch dimension to pull global representations closer for frames in the same video and push apart representations from frames in different videos. The use of an aggregation layer before the predictor network aids to avoid collapse of the learned global representations.}
    \label{fig:vic-mae}
\end{figure*}