\section{Experiment Settings}
\label{sec:experiments}

We perform experiments to demonstrate the performance of our method on fine-tuning tasks on the ImageNet benchmark, as well, as other image recognition datasets. For reference we also evaluate our method on the Kinetics dataset for action recognition to show that our model is able to maintain performance on video benchmarks. Full details are in Appendix \ref{app:impl}.


\begin{table}[t!]
\small
%\resizebox{\linewidth}{!}{%
\setlength\tabcolsep{3pt}
\renewcommand{\arraystretch}{1.2}
\centering
\begin{tabular}{lccccccc}
\toprule

 &  && \multicolumn{2}{c}{\textbf{ImageNet-1K}} && \multicolumn{2}{c}{\textbf{Kinetics-400} \textdagger} \\

\cmidrule{4-5}\cmidrule{7-8}
 
\textbf{Method} & \textbf{Pre-train.} && Top-1 & Top-5 && Top-1 & Top-5 \\ 

\midrule

Scratch & - && 71.39 & 88.45 && - & - \\
TubeViT \cite{piergiovanni2022rethinking}& 
K600 && 81.40 & - && \textbf{88.6} & \textbf{97.6} \\
MAE \cite{feichtenhofer2022masked, he2022masked}$^*$ & 
K400 && 81.34 & 95.4 && 81.3 & 94.9 \\

%\midrule

\model (ours) &
K400 && 82.80 & 96.6 && 81.5 & 95.1 \\
\model (ours) & 
MiT && \textbf{82.98} & \textbf{96.8} && 81.0 & 94.6 \\
\bottomrule
\end{tabular}
%}
\caption{\textbf{Transfer learning results from video pre-training to the ImageNet dataset}. The pre-training data is a video dataset (MiT, K600 or K400). All self-supervised method are evaluated end-to-end with supervised finetuning on IN1K. Best results are inn bold. \textdagger Kinetics-400 results are from models trained on any of the aforementioned video datasets and evaluated on a Kinetics-400. (*) The transfer results from Kinetics-400 to Imagenet-1K for MAE were obtained by replicating the results based on correspondence with the original authors.}
\label{tab:main_result}
\vspace{-0.2in}
\end{table}

%\subsection{Self-supervised Pre-training}

\vspace{0.04in}
\noindent
\textbf{Architecture}. We use the standard Vision Transformer (ViT) architectures~\cite{dosovitskiy2020image} and conduct experiments fairly across benchmarks and methods using the ViT-B/16 configuration. For masked image modeling we use a small decoder as proposed by He~et.al~\cite{he2022masked}. For the contrastive learning part we experiment with two alternatives.
\begin{itemize}
    \item \textit{MAE + \{SiamSiam or VicReg\}}. The predictor consists of the backbone network $f_{\theta}$ and a projector followed by a predictor as in Bardes~et.al~\cite{bardes2021vicreg}. The target encoder consists of the backbone $f_{\theta}$ and the projector, which are shared between the two encoders. 
    \item \textit{ViC-MAE}. The predictor and the target networks share the same architecture consisting of the backbone network $f_{\theta}$ and a projector following Bardes~et.al~\cite{bardes2021vicreg}.
\end{itemize}
When using the MAE + \{SiamSiam or VicReg\} combinations, we use the $[\text{CLS}]$ token from the ViT architecture which is typically used to capture a global feature from the transformer network and is used to fine-tune the network for downstream tasks such as classification.% when using our method we aggregate the  the local features of the ViT, using mean pooling, max pooling and generalized mean pooling \cite{radenovic2018fine}. % IS THIS THE REFERENCE FOR GENERALIZED MEAN POOLING??? 

\vspace{0.04in}
\noindent
\textbf{Pre-Training}. We adopt the Moments in Time~\cite{momentsintime} and the Kinetics-400 dataset~\cite{kinetics-400} for self supervised pre-training. They consist on $\sim$1000K and $\sim$300K videos of varied length respectively. We sample frames from these videos using distant sampling, which consists of splitting the video in non-overlapping sections and sampling one frame from each section. Frames are resized to 224px size, horizontal flipping and random cropping with a scale range of $[0.5, 1]$, are used as the only data augmentation, unless specified otherwise.

\vspace{0.04in}
\noindent
\textbf{Settings}. \model pre-training follows previously used configurations~\cite{he2022masked, feichtenhofer2022masked}. We use the AdamW optimizer with a batch size of 512. We evaluate the pre-training quality by end-to-end finetuning. When evaluating on video datasets we follow the common practice of multi-view testing: taking $K$ temporal clips ($K=7$ on Kinetics) and for each clip taking 3 spatial views to cover the spatial axis (this is denoted as $K \times 3$). The final prediction is the average of all views.

\section{Results and Ablations}
\label{sec:results}
We first perform experiments to analyze the different elements of the \model framework. All the experiments are under the \textit{learning with negative pairs} setting using mean pooling over the ViT-B/16 features unless specified otherwise. Linear evaluation and end-to-end finetuning runs are done over 100 epochs.

\subsection{Main result}

\begin{table}[t!]
\centering
\setlength\tabcolsep{10pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lcc}
 \toprule
 \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{ImageNet-1K}} \\
\cmidrule{2-3}
& Top-1 & Top-5 \\ 
\midrule
MAE \cite{he2022masked} + SiamSiam \cite{chen2021exploring} & 58.58 & 82.88 \\
MAE \cite{he2022masked} + VicReg \cite{bardes2021vicreg} & 63.86 & 84.07 \\
\model (ours) & \textbf{67.66} & \textbf{86.22}\\
\bottomrule
\end{tabular}
\caption{\textbf{Combining MAE and contrastive methods is not trivial.} Linear evaluation on
the ImageNet-1K dataset using types of contrastive learning. We use the $[\text{CLS}]$ token as the global video representation and apply common contrastive methods, but these do not result on the best performance, which is obtained with our method.}
\label{tab:ours_vs_contrastive_loss}
\end{table}


% add citation
% rerun failed runs so that all of then have the same number of epochs
\begin{table*}[h]
\small
\centering
\setlength\tabcolsep{1.8pt}
\renewcommand{\arraystretch}{1.2}
 % \resizebox{\textwidth}{!}{%
    \begin{tabular}{lccccccccccccc}
    \toprule
    % \textbf{Model} & Pre-train. & Food \cite{bossard2014food} & CIFAR10 \cite{krizhevsky2009learning} & CIFAR100 \cite{krizhevsky2009learning} & Birdsnap \cite{berg2014birdsnap} & SUN397 \cite{xiao2010sun} & Cars \cite{krause20133d} & Aircraft \cite{maji2013fine} & VOC2007 \cite{pascal-voc-2007} & DTD \cite{cimpoi2014describing} & Pets \cite{parkhi2012cats} & Caltech101 \cite{fei2004learning} & Flowers \cite{nilsback2008automated} \\
    % \cite{bossard2014food, krizhevsky2009learning, berg2014birdsnap, xiao2010sun, krause20133d, maji2013fine, cimpoi2014describing, parkhi2012cats, fei2004learning, nilsback2008automated}

    \textbf{Model} & Pre-train. & Food & CIFAR10  & CIFAR100  & Birdsnap  & SUN397  & Cars & Aircraft & VOC2007  & DTD  & Pets  & Caltech101  & Flowers  \\
    \midrule
    %Supervised \cite{chen2020simple} & IN-1K  & 75.20 & 95.70 & 81.20 & 56.40 & 64.90 & 68.80 & 63.80 & 83.80 & 78.70 & 92.30 & 94.10 & 94.20 \\

    %\midrule
    
    %MAE \cite{he2022masked} & IN-1K & 77.27 & \textbf{95.76} & \textbf{80.59} & \textbf{48.63} & \textbf{66.22} & 60.29 & 61.49 & 84.60 & \textbf{79.28} & 89.63 & \textbf{94.35} & \textbf{95.46} \\

    %\midrule
    

MAE \cite{he2022masked} \textdaggerdbl& K400 & 74.54 & 94.86 & 79.49 & 46.51 & 64.33 & \textbf{60.10} & \textbf{63.24} & 83.07 & 78.01 & \textbf{89.49} & 93.28 & 93.38 \\

MAE \cite{he2022masked} \textdaggerdbl& MiT & 76.23 & 94.47 & 79.50 & 47.98 & 65.32 & 59.48 & 60.67 & 83.46 & 78.21 & 88.42 & 93.08 & 94.17 \\

\model (ours) & K400 & 76.56 & 93.64 & 78.80 & 47.56 & 64.75 & 58.96 & 60.14 & 83.74 & 78.53 & 87.65 & 92.27 & 93.35 \\

\model (ours) & MiT & \textbf{77.39} & \textbf{94.92} & \textbf{79.88} & \textbf{48.21} & \textbf{65.64} & 59.76 & 60.96 & \textbf{84.77} & \textbf{79.27} & 88.85 & \textbf{93.53} & \textbf{94.62} \\
    \bottomrule
    \end{tabular}
% }
\caption{\textbf{Comparison of transfer learning performance of our approach} with supervised baselines across 12 natural image classification datasets. All results correspond to linear evaluation. Best results are shown in bold. \textdaggerdbl MAE trained on MiT and K400 randomly sample a frame from the video to compute a reconstruction loss; these models are trained and evaluated by us.}
\label{tab:model-performance}
\end{table*}

Our main result evaluates {\em video-to-image} transfer learning and we use as our testbed the ImageNet-1K benchmark. We present our results in Table~\ref{tab:main_result}, along with video downstream accuracy on Kinetics-400. We compare ourselves fairly to previous reported results in the literature that also use the ViT/B-16 backbone. The previous reported state of the art comes from the work of Piergiovanni et al. \cite{piergiovanni2022rethinking}, which use a novel Tube sampling methodology that allows them to train on video and images at the same time and obtains 81.40\% top-1 accuracy on end-to-end finetuning transferring from the Kinetics-600 dataset. Our method surpasses this result by an absolute improvement of 1.58\% points of accuracy transferring from the Moments in Time dataset. However TubeViT is still the best model under a ViT/B-16 architecture on the Kinetics-400 benchmark. Another key result from this table is that our method even when trained on Kinetics-400 still performs best than other methods in {\em video-to-image} transfer. 
%For fairness, we also compare with same model on action recognition, where the TubeViT model \cite{piergiovanni2022rethinking} obtains the best result by a large margin, more than $7 \%$ in absolute accuracy. We remark that this result is expected since the focus of our model is to obtain representations from video that are useful for images, but it also shows that our model still maintains modest performance on video benchmarks.
Other previous results on the same problem that use different backbones include  the works of Gordon et al. \cite{gordon2020watching},  Xu \& Wang \cite{xu2021rethinking}, and  Wu \& Wang \cite{wu2021contrastive}. However these all use a ResNet-50 backbone and obtain 54.5\%, 33.8\%, and 55.6\% top-1 accuracies on linear evaluation. Since those works are not using the same setting we chose not to include them alongside the others.  %Notice that our best performing model obtains 67.66\% accuracy on the same evaluation, but since these results were not the main point of these works, we presented them here for completeness. 

Combining MAE with contrastive learning is non trivial, and we set to test these by comparing our model with  MAE models with alternative contrastive learning objectives Siamsiam~\cite{chen2021exploring} and VicReg~\cite{bardes2021vicreg}. We present our results using linear evaluation on Table~\ref{tab:ours_vs_contrastive_loss}. We use the $[\text{CLS}]$ token as the global video representation for contrastive pre-training. We can notice in this table that competing methods underperform compared to our model which uses pooling of the local features by an absolute margin of $>3\%$ over the \mbox{\em MAE + VicReg} model. See Appendix \ref{app:semi_sup_imagenet} for an evaluation of our method against baselines that combine MAE with contrastive learning on the problem of semi-supervised learning on ImageNet.

\subsection{Transfer learning performance.}
We evaluate transfer learning performance of our model across a diverse array of 12 downstream image classification tasks~\cite{bossard2014food, krizhevsky2009learning, berg2014birdsnap, xiao2010sun, krause20133d, maji2013fine, cimpoi2014describing, parkhi2012cats, fei2004learning, nilsback2008automated}. Table \ref{tab:model-performance} shows the results of four models based on a ViT/B-16 backbone. We perform linear evaluation (See appendix for details on the metrics used to evaluate each of these models). We train two models using two video datasets. The first model is a baseline MAE model pre-trained on randomly sampled frames from the videos on the Moments in time dataset and the Kinetics-400 dataset. The second model is our full \model model pre-trained on each of the same two datasets. Our model significantly outperforms the other baselines on 9 out of 12 datasets, whereas the MAE trained on Kinetics is superior on only 3 (i.e. Cars, Aircraft and Pets). 

\subsection{Ablations}
We investigate in this section the effect of various frame-lavel image transformations used to augment the data, the effect of our choice of frame separation, and the choice of pooling operator.

\vspace{0.02in}
\noindent
\textbf{Augmentations.} We perform an ablation study to check whether the use of strong color augmentations on the target encoder is necessary as it is crucial in standard self-supervised methods for images. The results are presented in Table \ref{tab:ablation_aug}. Using only color augmentations meaning that the sampled frame in the target encoder is color augmented but not spatially augmented the performance is reduced by $>2\%$ on linear evaluation on the Imagenet dataset. Using a combination of strong color augmentations and spatial augmentations, though it increases performance; it is not superior to using only strong spatial augmentations. This is stark contrast with previous methods that necessitate strong color augmentationg to be able to learn using contrastive learning.  In the following experiments, we only use strong spatial augmentations and discard the use of color augmentations entirely. 

\begin{table}[t!]
\centering
\setlength\tabcolsep{12pt}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{ccc}
\toprule
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Frame}\\ \textbf{separation}\end{tabular}} & \multicolumn{2}{c}{\textbf{ImageNet-1K}} \\
\cmidrule{2-3}
 & Top-1 & Top-5 \\ 
 \midrule 
 
0 & 63.25 & 83.34 \\
2 & 64.47 & 84.31 \\
4 & 65.25 & 84.64 \\
8 & 65.89 & 84.91 \\ 
\midrule
D & 67.66 & 86.22\\
\bottomrule
\end{tabular}
\caption{\textbf{Ablation on frame separation}. Linear evaluation on the ImageNet-1K dataset using different frame separartion. 0 means sample the same frame. D stands for distant sampling and the rest are using continuos sampling.}
\label{tab:ablation_frame}
\vspace{-0.2in}
\end{table}

\vspace{0.02in}
\noindent
\textbf{Frame separation}. This is an essential design component of our framework, and in this experiment we aim to see the effect of frame separation on the performance of our method. We follow the two methods of sampling frames from Xu~et.al~\cite{xu2021rethinking}. Results are shown in Table \ref{tab:ablation_frame}.
The first method is \textit{Continuous sampling},  which consists in selecting a starting index $i$ and then sampling a frame in the interval $(i, i+\delta]$, where $\delta$ is the frame separation. A frame separation of $0$ indicates that the predictor and the target networks receive the same frame. The second method is \textit{distant sampling}, where the video is split into $n$ intervals of the same size, where $n$ is the number of frames to use for contrastive learning and then one frame is selected randomly from each interval. In our experiment, we observe that increasing the frame separation when using \textit{continous sampling} increases the performance of the model. We observe the best performance using \textit{distant sampling} with $n=2$ (labelled $D$ in Table \ref{tab:ablation_frame}). We posit that further increasing frame separation offers potentially stronger augmentations. In the following experiments, we only use strong spatial augmentations combined with distant frame sampling. 

\begin{table}[t!]
\centering
\setlength\tabcolsep{6pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{cccc}
\toprule
 \multirow{2}{*}{\textbf{Model}}&  \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Pooling} \\ \textbf{type}\end{tabular} }& \multicolumn{2}{c}{\textbf{ImageNet-1K}} \\\cmidrule{3-4}
 & & Top-1 & Top-5 \\ 
\midrule
\model (Ours) & GeM & 66.92 & 85.50 \\
\model (Ours) & max & 67.01 & 85.59 \\
\model (Ours) & mean & 67.66 & 86.22\\
\bottomrule
\end{tabular}
\caption{\textbf{Ablation on pooling type}. Linear evaluation on the ImageNet-1K dataset using different types of pooling. The hyperparmeter $\lambda$ is set to $0.025$ and introduced using an schedule.}
\label{tab:ablation_loss}
\vspace{-0.2in}
\end{table}



\vspace{0.02in}
\noindent
\textbf{Pooling type}. Since this is an important step in our proposed method, we test which operator $\Omega$ used to aggregate local features performs best at producing global features. We report our results in Table \ref{tab:ablation_loss}. We try common types of pooling (\textit{mean, max}) as well as, \textit{generalized mean pooling}. %We notice that using a method to aggregate the features is an important component of our method and without it, the model trained using masked image modeling and contrastive learning in tandem diverge. 
We found \textit{mean} to be more effective in creating a global representation for the video, and we use it for all other experiments.


\subsection{Limitations}
Having shown that \model is able to learn useful representations from video data that transfer well to image classification and that surpasses previous models on the same set-up, we contextualize our results by discussing state of the art results in these problems and limitations of our method.

%\vspace{0.02in}
%\noindent
%\textbf{Comparison to prior video-pretraining}. 
Comparing with models of a similar computational budget our model is able to perform on par with previous results on the Kinetics-400 dataset. However, compared to TubeViT \cite{piergiovanni2022rethinking} our model still underpeforms by $7.1\%$ points in absolute accuracy. A model that works well across both images and video might still need pre-training in both domains. 
%Compared to the original MAE \cite{feichtenhofer2022masked} trained on video, our model is on par with this model with only a difference of $0.2\%$ points in absolute accuracy on our favor. 
Compared to MaskFeat \cite{wei2022masked}, our model underperforms by $0.7\%$ points in absolute accuracy (82.2\% vs. 81.5\%). Our model nevertheless is able to surpass the MViTv1-B model \cite{fan2021multiscale}, the TimeSformer model \cite{bertasius2021space} and the ViVit-B model \cite{arnab2021vivit} by $0.3\%$, $0.8\%$, and $1.5\%$ points in absolute accuracy respectively (81.2\%, 80.7\%, and 80\% vs. 81.5\%). Compared to models that only perform contrastive learning on videos our model underperforms compared to DINO~\cite{caron2021emerging} by $1\%$ points in absolute accuracy (82.5\% vs. 81.5\%). These results contextualize \model against high performing models that are using either stronger backbones or additional supervision. We posit that a model trained on a combination of video and image data is likely to perform best across domains. 

\begin{table}[t!]
\centering
\setlength\tabcolsep{10pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{cccc}
\toprule
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Color}\\ \textbf{Augm}.\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Spatial}\\ \textbf{Augm.}\end{tabular}} & \multicolumn{2}{c}{\textbf{ImageNet-1K}} \\
\cmidrule{3-4}
 &  & Top-1 & Top-5 \\ 
 \midrule
\checkmark &  & 65.40 & 84.03 \\
& \checkmark & 67.66 & 86.22 \\
\checkmark & \checkmark & 66.03 & 85.01\\
\bottomrule
\end{tabular}
\caption{\textbf{Ablation on different augmentations}. Linear evaluation on the ImageNet-1K dataset using different augmentations. Color augs include random color jitter, grayscale conversion and gaussian blur. Spatial augs are random resized crop and horizontal flip.}
\label{tab:ablation_aug}
\end{table}

 %\vspace{0.02in}
%\noindent
%\textbf{Comparison to ImageNet pretraining}. 
Finally, we compare our \model to a number of state of the art in-domain Imagenet-pretrained models trained with a similar computational budget. We found that most models trained on video including our model, underperform most of the models in this category. The domain gap between any video dataset and images on Imagenet-1k still seems not to have been closed.
Compared to a model that uses masked image modeling, the original MAE~\cite{he2022masked} and to the MaskFeat model~\cite{wei2022masked}, our model underperforms by $0.7\%$ points in absolute accuracy (83.6\% \& 83.6\% vs . 82.98\%, respectively). Compared to a model that uses contrastive learning, DINO~\cite{caron2021emerging}, MoCov2~\cite{chen2020improved}, and BeiT~\cite{bao2021beit} our model underperforms by $1.1\%$, $1\%$, and $0.3\%$ points in absolute accuracy (84 \%, 83.9\%, and 83.2\% vs 82.9\% respectively). These results show that the gap from models pre-trained purely on video still exists but we believe \model is a step forward in closing that gap.