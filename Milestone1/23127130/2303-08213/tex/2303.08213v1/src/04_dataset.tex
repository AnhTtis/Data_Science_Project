\section{Dataset Creation}\label{sec:datasets}
We curated three large scale datasets to understand the  landscape of privacy nutrition labels and their consistency with privacy policies: (1) Privacy label dataset for apps on play store(n=2.6M), (2) Privacy label dataset for apps on apple store (n=1.6M), (3) Privacy policies for the apps present on both the platforms (n=1.5M). From hereon, we will refer to (1) as Data Safety Section (DSS) Dataset and (2) as Apple Privacy Label (APL) dataset. The overview of the data collection pipelines is shown in Figure~\ref{fig:methodology}. The policy dataset size is lower than the number of apps because (a) multiple apps can have the same developer and list the same privacy policy, (b) some apps may use generic policy templates hosted by third party websites, and (c) not all apps have a working privacy policy URL. Further, we note that we only work with policies and privacy labels that are present in the English Language. 


\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{figures/methodology.pdf}
  \caption{Overview of methodology}
  \label{fig:methodology}
\end{figure}


\subsection{Google Data Safety Section}
Google required app developers to complete the data safety section by July 20, 2022. Starting from Jun 20, 2022, we collected periodic snapshots of  the data safety sections of all the apps present on the play store. The final snapshot was collected on Sept 9, 2022.
To collect the data safety section, we start with the APK list provided by Androzoo~\cite{androzoo}. This APK list consists of android app ids from various sources such as google play store, amazon fire store etc. This list is updated on a nightly basis, that ensures that we get the most up to date list of apps present on the play store. We use the list to extract the package names for the apps. The package names are unique and can be used to construct the apps' page on the play store. For example, the URL for any app can be constructed by appending the package name to ``\url{https://play.google.com/store/apps/details?id=}''. Additionally, to filter out the apps that not available on play store, we perform a validation step by checking the existence of the URL constructed using the package name.

After obtaining the valid package names, we use a customized version of publicly available google play store scraper library,\newline~\texttt{google-play-scraper} ~\cite{}. The customization\footnote{Note that the library was later updated in August to include the data safety section} was required to capture the data safety section, along with the app meta data. Specifically, the response object contains the practices mentioned in the data safety section as well as app name, number of installs, privacy policy URL, app developers' website etc.  We store the privacy policy URL for each app separately which is later used to create the privacy policy dataset(discussed below). Finally, we store the parsed data in a database for further analysis. 

 The total time to retrieve data for 2.3M apps is between 24 to 48 hours using 4 local machines. We note here that Google required app developers to complete Data Safety Section by July 20, 2022. By collecting data before and after the deadline, we are able to capture developers' practice before- and after- the required date which allows us to study the addition and evolution of data safety section at scale.

% \subsubsection{Methodology}
% % Google Play Store doesn't have an API that can be used to get the list of all the apps in the Play Store. So, we used the Androzoo[] android apk list to extract the package names of the API. All apps on the Google Play website are referred to as https://play.google.com/store/apps/details?id=<APK Package>. This helped us to scrape the Play Store using a modified google-play-scraper to parse both the app details, app permissions and the data safety section of an app.

% % Using these package names we could navigate to the app listing on the Google Play website. 
% \paragraph*{\textbf{App Selection}}
% We collected an initial list of 2.2 million app ids from the Androzoo APK list~\cite{}. We then used a custom modified version of google-play-scraper~\cite{} to parse the metadata along with the Data Safety section of the apps from the Google Play website. For every apps in our list, we collected metadata, permissions list, and a parsed version of the data safety section. 

% We started collecting data on June 20th and each week we updated our apk list and re-downloaded the app details and the corresponding data safety portions. We continued to do this until August when all Play Store apps were expected to have data safety sections. Lastly, we scraped again in September for to get a final set of apps.

% % \begin{figure}[t]
% %   \centering
% %   \includegraphics[width=\columnwidth]{figures/apple_methodology.pdf}
% %   \caption{Apple Methodology}
% %   \label{fig:apple_methodology}
% % \end{figure}

% While we started with 2.2M apps by September we had over 2.5M. 

% We would parse the data collected at the end of the weekly scraping. The data was parsed into two data files. One corresponds to the apps that have a Data Safety Section and the other corresponded to the latest data of all apps scraped till then. We parsed data into the following categories app name; developer name; developer website; downloads; privacy policy; genre type; data collected; the purpose for each type of data being collected; data shared; the purpose of data being shared.


\subsection{Apple Privacy Label} \label{apple_label_methodology}
For the Apple privacy label, we collected our data on November 13, 2022. We first start by parsing the XML site map for the app store using the library \texttt{ultimate-sitemap-parser}~\cite{}. The sitemap consists of the \textit{appIds} and unique URLs for the apps present in the app store. In our parsing, we found that the unique number of apps on the App store was 1.6M. Using the unique appIds, we call the Apple Store Catalogue API~\cite{} to extract the metadata and the privacy nutrition label for the app. The API response is a JSON string that contains the privacy nutrition label, including the privacy types, data categories, data types, and purpose. The JSON response also contains the associated metadata of the app such as the developer website, app name, privacy policy URL, genre, etc. In addition to the API calls, we used the unique URLs to perform headless crawls to extract the privacy policy link shown on the app store page. The privacy policy is then extracted (as described below) for further analysis. 

We leverage cloud functions to perform the API calls to extract the privacy nutrition labels. The API call can occasionally result in errors if too many requests are sent. To avoid overwhelming the servers, we wait for 2 hours before retrying the failed instances. Using 11 instances of google cloud functions, and the entire scraping process finished in 14-16 hours. From these apps, we were able to analyze 1.38M apps; the rest were filtered out due to non-English content. At the end of the scrape, we had 955K apps with Apple Privacy Label out of 1.38M apps in our set (69.2\%). We also obtained 750k Unique policy links from our the apps. Note that the developers can use generic privacy policy templates hosted by third party websites to fulfil the requirements. 

\subsection{Privacy Policy}
We build the privacy policy dataset using the privacy policy URLs extracted during privacy labels scraping, as described above. Using the URLs, we retrieve the raw HTML of the policies and perform text extraction  and text cleaning. We then pass it to the is-policy classifier which determines whether the webpage is a privacy policy. We note here that although 
both the platform require the app developers to add privacy policy link with the app, in many cases the privacy policy URL is either not available or is not valid. 

\subsubsection*{\textbf{Text Extraction and Cleaning}} Starting with the privacy policy URLs, we use the \texttt{requests} library to obtain the raw HTML of the page. Next, we clean the HTML using the Boilerpipe~\cite{} library which removes the headers and footers from the web page. We then extract the text using the BeautifulSoup~\cite{} library. Following prior works~\cite{}, we filter out the instances where the text is smaller than 100 words as they are more likely to be error messages than privacy policies. We note here that some websites may store policy in PDF files. We use the PyPDF2~\cite{} library to extract text from the PDF files. We filter out the instances where the policy is stored in other non-standard formats such as google documents. The non-standard nature of these documents make it challenging to extract text from these documents; for example, in some cases, we are required to request access to view the documents. In our dataset, we find that roughly 3\% of the apps have privacy policies as google documents. While filtering these policies affects our coverage, it doesn't affect the overall results. Finally, as our analysis pipeline relies on the English language, we also filter out non-English policies using the language detection library langid.py~\cite{}. 

\subsubsection*{\textbf{Is-Policy Classification}} After extracting text from the privacy policies, we pass it through a binary classifier which determines whether the text corresponds to a privacy policy. The classifier is a 1-D  Convolutional Neural Network(CNN), based on classifiers mentioned in the prior works~\cite{TOPS, GDPR}, achieving an accuracy of 99\% on the testing set. It was trained with the same corpus of 1000 privacy policies as in ~\cite{}. More details about the training are present in Appendix~\ref{}. 


On the app store, we found that 1.01M apps out of 1.38M apps had 713K unique privacy policy URLs. 
% ZZZ apps had non-unique privacy policy.
On the play store, we obtained 1.12M unique privacy policy URLs. In total, we had 1.3M unique privacy policy URLs. 
\todo[inline]{Add in the numbers from the pipeline here}


\subsection{Ethical Considerations}
For our datasets, we collected data only from publicly available web pages and APIs. While our data collection scripts add additional load to Google and Apple's servers, we were careful to not abuse these resources. In particular, we added back-off strategies in case of errors and waited for sufficient time before retrying for the failed cases. Furthermore, for privacy policy extraction, we were respectful of robots.txt and only extracted HTML when the website allowed us to.