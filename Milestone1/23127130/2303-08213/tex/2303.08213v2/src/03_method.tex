\section{Data Collection Pipeline}
\label{sec:measurement}
We show an overview of the data collection pipeline in  \cref{fig:meas_pipeline}. We begin by scraping the metadata and privacy labels for the apps from Google Play and Apple App Store (\cref{sec:privacy_labels}). We then design a classification pipeline to automatically annotate the policies of the mobile apps (\cref{sec:privacy_policy}). Finally, we identify cross-listed apps between Google Play and Apple App Store (\cref{sec:cross_apps}).


% In the following sections, we use the resulting datasets to answer our \textit{Research Questions (RQs)}.
% \todo[inline]{All apps with DSS have PP except 13. there are 994K apps with DSC}

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{figures/good_pdfs/methodology_asmit_cropped.pdf}
  \caption{Overview of the data collection pipeline. RQs here refer to the \textit{Research Questions} introduced in \cref{sec:intro}}
  \label{fig:meas_pipeline}
\end{figure}



\subsection{Privacy Labels}
\label{sec:privacy_labels}
First, we describe the collection method for our privacy labels (both DSS and APL) datasets.\smallskip


\noindent\textbf{Google Data Safety Section.}
\label{sec:google_dsc}
We collected 10 snapshots of the Data Safety Sections for 2.6M apps present on the play store between June 20, 2022, and Nov 25, 2022. Google required app developers to complete the data safety section by July 20, 2022. By collecting data before and after this date, we are able to capture how the app developers responded to Google's requirement for adding a data safety section to their apps. Note that we captured weekly snapshots from June 20 to Aug 1, which includes the date set by Google. The remaining two snapshots were taken on Sept 9 and Nov 25. 
 
To collect the data safety section, we start with the apk list provided by Androzoo~\cite{androzoo}. This daily updated list consists of up-to-date Android app ids from various sources, including those from the Google Play store. Using the app ids and a 
% Using the app id, we retrieve the app's package name and construct the app URL on Google Play. Then,we use a 
customized version of publicly available google play store scraper library~\texttt{google-play-scraper}~\cite{google-play-scraper}, we capture the metadata of each app, including its data safety sections and the link to the privacy policy.  We used four local machines to perform the scraping. The total time to retrieve data for \nnumber{2.6M} apps, from Google Play, is between 24 to 48 hours. We note that this set also includes apps with very low download counts. To ensure that our statistical analysis is not skewed by these apps, we filter out apps that have fewer than 1000 downloads resulting in a total of \nnumber{1.14M} apps with \nnumber{573k} having privacy labels. We refer to this dataset as \textbf{DSS Dataset}.

Performing the longitudinal analysis, we find that during the period of June 20, 2022, to November 25, 2022, the number of apps with DSS increased from 28.76\% to 47.71\%. The largest change was observed between July 13, 2022, and July 26, 2022, when the percentage of apps went from 35\% to 37.4\% in 13 days. Interestingly, we find that 5\% of apps removed their DSS over the course of our data collection. Specifically, \nnumber{27K} updated their play store page to remove the privacy label. Off these apps, \nnumber{8.6K} apps were deleted, including \nnumber{1.3K} apps which had over 100K downloads. For example, ~\textit{Sport Prediction} with 1M app download had a DSS as of August 1, 2022, but did not have it by Nov 25, 2022.  Furthermore, we find that \nnumber{1.3K} apps updated their DSS to reflect a change in their practices \textit{i.e.} they updated DSS to add or remove privacy practices. We investigate the factors responsible for these changes in \cref{sec:developer}.

\smallskip
%and \nnumber{598k} unique privacy policies

% \kassem{missing stats here}


% The response object from the scraper contains the practices mentioned in the data safety section as well as the meta data such as app name, number of installs, privacy policy URL, app developers' website etc.  Finally, we store the parsed data in a database for further analysis. The privacy policy URL for each app is stored separately which is later used to create the privacy policy dataset (\Cref{sec:privacy_policy}). 




% . The customization\footnote{Note that the library was later updated in August to include the data safety section} was required 


% We use the list to extract the package names for the apps. The package names are unique and can be used to construct the apps' page on the play store. For example, the URL for any app can be constructed by appending the package name to ``\url{https://play.google.com/store/apps/details?id=}''. 
% % Additionally, to filter out the apps that not available on play store, we perform a validation step by checking the existence of the URL constructed using the package name.



 
 % We note here that Google required app developers to complete Data Safety Section by July 20, 2022. 


\noindent\textbf{Apple Privacy Labels.} 
\label{sec:apple_label_methodology}
In this work, we do not perform a longitudinal analysis for APLs as Balash et al~\cite{balash2022longitudinal} performed a similar study earlier in 2022. Instead, we collect a single snapshot of the Apple Privacy Labels (APLs) on November 13, 2022, for consistency.
To curate the dataset, we begin by parsing the XML site map for the app store.\footnote{We  used the \texttt{ultimate-sitemap-parser} library.} Using the URLs from the sitemap,
we use the Apple Store Catalogue API to extract the metadata for each app, including the privacy nutrition label and a link to the privacy policy. We performed the crawl using 11 instances of google cloud functions to scrape \nnumber{1.6M} apps in 15 hours.

We extracted information for 1.38M apps out of the 1.6M apps available, filtering out those with non-English content. As a result, we obtained 955K (69.2\%) with APLs. In comparison, Balash et al~\cite{balash2022longitudinal} in March 2022 found that 60.5\% of apps had Apple Privacy Labels. The higher percentage in our study suggests that new APLs are still being added to apps. We refer to this dataset as \textbf{APL Dataset}.

% The API response is a JSON string that contains the privacy nutrition label, including the privacy types, data category, data types and purpose. The JSON response also contains the associated meta data of the app such as developer website, app name, privacy policy URL, genre etc. In addition to the API calls, we used the unique URLs to perform headless crawls to extract the privacy policy link shown on the app store page.  

% We leverage cloud functions to perform the API calls to obtain the privacy nutrition labels. The API call can occasionally result in errors if too many requests are sent. To avoid overwhelming the servers, we wait for 2 hours before retrying the failed instances. Using   the scraping process finished in 14-16 hours. 

% Using the privacy policy URLs extracted during privacy labels scraping, we retrieve the raw HTML of the policies and extract the cleaned text. We then pass the extracted text to a classifier that determines whether the webpage is a privacy policy. Although  both platforms require app developers to add a privacy policy link to their app, we found that the privacy policy URL is either unavailable or is invalid in many cases (more details later). Then, we develop a new taxonomy for privacy policies that accommodates the privacy labels. We train a suite of classifiers that annotate a privacy policy using labels from the developed taxonomy. 
% \subsubsection{Privacy Policy Dataset} 


\subsection{Privacy Policy Analysis}
\label{sec:privacy_policy}
We build a privacy policy analysis pipeline to automatically annotate the privacy policies of the apps (the second component in \cref{fig:meas_pipeline}). This annotation allows us to analyze the consistency between the privacy labels and privacy policies of each app at a scale (\cref{sec:policy_inconsistency}).\smallskip

\noindent
\textbf{Text Extraction and Cleaning.}
Starting with the privacy policy URL, we crawl the corresponding webpage, clean the HTML by removing the headers and footers, and extract the text using the \texttt{BeautifulSoup} library~\cite{richardson2007beautiful}. We use the \texttt{PyPDF2}~\cite{pypdf2} library to extract text from privacy policies that are PDF documents. Following prior works~\cite{harkous2018polisis}, we apply three exclusion criteria. First, we filter out the instances where the text length is less than 100 words. Second, we filter out the instances where the policy is stored in non-standard formats, such as images. Finally, as our analysis pipeline relies on the English language, we filter out non-English policies (330K for Play Store, 230K for App Store) using the language detection library \texttt{polyglot}~\cite{polyglot}.\smallskip
% \kassem {add the missing references.}\smallskip
% The non-standard nature of these documents make it challenging to extract text from these documents; for example, in some cases, we are required to request access to view the documents. In our dataset, we find that roughly 3\% of the apps have privacy policies as google documents. While filtering these policies affect our coverage, it doesn't affect the overall results as it is a small fraction. 

% applying the \texttt{requests},  library to obtain the raw HTML of the page. Next, we clean the HTML using the Boilerpipe~\cite{} library which removes the headers and footers from the web page. We then extract the text using the BeautifulSoup~\cite{} library. 
% \noindent
% \textbf{Is-Policy Classification}
% After extracting text from the privacy policies, we pass it through a binary classifier which determines whether it corresponds to a privacy policy. We follow the same methodology in previous works~\cite{wagner2022privacy, linden2018privacy} by training a 1-D  Convolutional Neural Network (CNN) classifier. We used a dataset of 2,000 documents to develop the classifier: 1,000 positive samples from the ACL/COLING 2014 privacy policies' dataset released by Ramanath et al.~\cite{RamanathLSS14} and 1,000 negative random samples from the top 500 Alexa websites. We split the data into 80\% training set and a 20\% testing set, and the classifier yielded a 99.09\% accuracy on the testing set. More details about the classifier architecture and the training are present in the \Cref{sec:pol_classifier}.

% On the App Store, we found that \nnumber{1.01M} apps out of \nnumber{1.38M} apps had \nnumber{713K} unique privacy policy URLs.  On the Play Store, we obtained \nnumber{598k} unique privacy policy URLs. In total, we had over \nnumber{1.1M} unique privacy policy URLs. 

\textbf{Policy Classification.}
To extract the practices from privacy policies, we follow the pipelines from existing works~\cite{harkous2018polisis, linden2018privacy} that use OPP-115 taxonomy~\cite{wilson2016creation}. However, in our case, not all practices mentioned in privacy labels can be extracted as the underlying taxonomy either lacks these classes or the datasets do not have sufficient samples. For example, a data category used in APL, \textit{Sensitive Info} is missing from the taxonomy. To overcome this limitation, we extend the existing taxonomy by adding missing classes. Further, two of the authors perform annotation for the added classes. The annotators had an overlapping set of 200 segments to measure the inter-annotator agreement. We find that the annotators showed high agreement with a Cohen's Kappa value of ($\kappa = \red{.85})$. We provide more details about the taxonomy and the annotation in \cref{appendix:taxonomy}.

Following prior works~\cite{wagner2022privacy, srinath2020privacy}, we use DistilBERT~\cite{sanh2019distilbert} to train the classification models. We then use these models to extract privacy practices from policies. This approach allows us to accurately extract the practices from privacy policies, even for those practices that were not covered by the existing taxonomy. \cref{tab:policy_results} shows the performance of our classifier on a held-out test set. We note that our classifiers outperform previous classifiers, primarily because we added new annotations. 
%A full comparison for classes can be found in \cref{appendix:taxonomy}.

\begin{table}[t]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Category} & \textbf{CNN ~\cite{o2015introduction}} & \textbf{BERT ~\cite{devlin-etal-2019-bert}} & \textbf{Ours}\\
\midrule
\rowcolor{aliceblue}First-party-collection-share  & 82 & 91 & \textbf{98} \\
Third-party-sharing-collection & 81 & 90 & \textbf{96} \\
\rowcolor{aliceblue}Identifiability & 77 & 91 & \textbf{97}\\
Does-does-not & 86 & 93 & \textbf{96} \\
\rowcolor{aliceblue} Encryption-in-transit  & N/A & N/A & \textbf{99} \\
Data Deletion Option  & N/A & N/A & \textbf{91} \\
\bottomrule
\end{tabular}
\caption{Selected Classifiers' performance on the test set. For the performance of all classifiers, please see \cref{tab:privacy_label_classifier_stats} in  \cref{appendix:taxonomy}.}
% \todo[inline]{Paul: Update CNN and BERT stats}
\label{tab:policy_results}
% \vspace{-8mm}%
\end{table}

% It is important to note that our approach not only improves the accuracy of extracting the practices from policies but also enables us to cover a wide range of practices that were not covered by the existing taxonomy. This, in turn, allows us to provide a more comprehensive analysis of the privacy practices of the apps. Overall, this new dataset and the approach to extract practices from policies are a significant contribution to the field of privacy analysis in mobile apps.

\iffalse

\subsubsection{Privacy Policy Taxonomy}
The second stage of the privacy analysis pipeline is annotating the policies with labels. This requires developing a taxonomy that is consistent with privacy labels for mobile apps.\smallskip

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{figures/good_pdfs/privacy_label_taxonomy.pdf}
  \caption{Privacy Label Taxonomy}
  \label{fig:privacy_taxonomy}
\end{figure}
\noindent
\textbf{Limitations of Existing Taxonomy}
In 2015, Wilson et al~\cite{wilson2016creation} first introduced a privacy policy taxonomy to describe privacy practices mentioned in the privacy policies in a structured manner. \cref{fig:opp_taxonomy} in the appendix shows this taxonomy, which is similar to that in \cref{fig:privacy_taxonomy}.  The top level of the hierarchical taxonomy covers high-level privacy categories, such as third-party-sharing, data retention, etc. The lower level consists of a set of privacy attributes, each covering a set of values. For example, \textit{Personal Information Type} as an attribute can have the \textit{Financial}, \textit{Health}, and \textit{Location} as the values. Along with the taxonomy, the authors also released the \textit{OPP-115 dataset} that consists on 115 privacy policies. These policies have been manually annotated by multiple annotators to capture 23K fine-grained data practices. 
%For more details on the taxonomy and the associated OPP-115 dataset, please refer to \url{https://usableprivacy.org/data}.


Prior works~\cite{harkous2018polisis, srinath2020privacy} have used OPP-115 taxonomy and the associated dataset to build machine learning classifiers that tag segments of the policy with the labels from the taxonomy. However, there are two limitations to directly using the taxonomy (and existing frameworks such as Polisis~\cite{harkous2018polisis}) to compare privacy practices between privacy labels and privacy policies. First, the OPP-115 taxonomy was developed for privacy policies of websites, which is vastly different than the ecosystem of applications (both Android and iOS). In particular, the applications have access to sensitive data types, which are present in the privacy labels. This taxonomy, while having some overlap with the APL and DSS privacy labels, does not cover such app-specific data types. For example, \texttt{app activity}, a data category covering  users' interactions within the application, is not covered in the taxonomy. Second, the OPP-115 dataset has limited annotations for the lower-level attributes that overlap with the private labels. For example, \textit{Encryption in Transit}, which is a separate practice covered in Data Safety Section, only has less than 100 labeled instances in OPP-115 dataset. 


We address these limitations by incorporating the missing labels to the existing OPP-115 taxonomy. We derive a \textit{Privacy Label Taxonomy} (\cref{fig:privacy_taxonomy}) as a union of a subset of the original OPP-115 taxonomy with the new labels from APL and DSS. To build the taxonomy, we first identify the categories from the taxonomy that are relevant to privacy labels, thus creating a subset of the original taxonomy. We then add the missing categories to get the new taxonomy. Additionally, we annotate data for the new (and some existing) categories to facilitate the development of machine learning classifiers.\smallskip

\noindent
\textbf{Identifying relevant categories from OPP-115:} As discussed in Sec.~\ref{sec:background}, privacy labels consist of high-level privacy practices, data categories, and purposes for the use of data. The high-level categories \textit{First-party-data-collection} and \textit{Third-party-sharing-collection} from the OPP-115 taxonomy are relevant as they map directly to Data Safety Section' \textit{Data Collection} and \textit{Data Sharing} privacy types. Further, APL covers the first-party collection and sharing practices implicitly through \textit{Data Linked to You} and \textit{Data Not Linked to You}. Similarly, the attribute level categories \textit{Purpose}, \textit{Data Type}, and \textit{Identifiable} are relevant. 


For example, in Apple Privacy Label, the \textit{Data Used to Track You} privacy type includes the data that is linked with third-party data for targeted advertising. It also includes cases when the data is shared with a data broker. Note here that linking can be done by both the app developers (by using data obtained from a third party) or by sharing the data with a third party. Thus, this privacy practice can be represented with \textit{Advertising or Marketing} purpose of \textit{First-party-collection-use} and \textit{Third-party-sharing}.
%For example, for APL, \textit{Data Linked to You} can be represented as \textit{First-party-collection} and \textit{Identifiable}. 
At this stage, we drop the categories absent in the privacy labels. For example, \textit{Policy Change} is a high-level category in OPP-115 which is not present in the \textit{Privacy Label Taxonomy}.\smallskip

% To create a map from privacy labels to the taxonomy, we ensure coverage across all the three levels. 

 % Similarly, for Google Data Safety Card, the privacy types \textit{Data Collection} and \textit{Data Sharing} directly map to \textit{First-party-collection-use} and \textit{Third-party-sharing} respectively. However, as per documentation, \textit{Data Deletion} corresponds to when the app ``Provides a way for you to request that your data be deleted, or automatically deletes or anonymizes your data within 90 days''. As there is no specific way to get this information from the taxonomy, we create a separate high level practice for Data Deletion. 
\noindent
\textbf{Adding New Categories}
As indicated earlier, the OPP-115 taxonomy misses some of the lower-level data categories and purposes. A complete list of missing elements is shown in \cref{} in the Appendix. We add these missing elements and adapt the OPP-115 taxonomy to \textit{Privacy Label Taxonomy}. 


Apart from the high-level categories from the taxonomy, we also added two high-level categories: \textit{Data Deletion Option} and \textit{Encryption in Transit}. Both the categories are part of \textit{Security Practices} privacy type from DSS. \textit{Data Deletion} corresponds to when the app ``Provides a way for you to request that your data be deleted, or automatically deletes or anonymizes your data within 90 days''. As there is no specific way to get this information from the taxonomy, we create a separate high-level practice for Data Deletion. For \textit{Secure Data Transfer}, there is a low-level element in the taxonomy that covers the practice, however, since the other categories from the taxonomy in the hierarchy are not related, we added \textit{Secure Data Transfer} as a high-level category. Also note that since there were fewer than 100 annotations for this category, we also perform additional annotations and increase the dataset size, as discussed in the next section. Finally, Table~\ref{table:label_to_taxonomy} \todo[inline]{UPDATE} shows how the privacy labels are mapped to the new taxonomies. 


\subsubsection{Privacy Policy Classification} 
\label{sec:policy}


We now describe the development of machine learning classifiers for the adopted taxonomy. The OPP-115 dataset consists of 115 privacy policies which have been manually annotated by multiple annotators to capture 23K fine-grained data practices. We leverage these annotations to train models for categories that are common to our taxonomy. For the labels that we added (e.g. \textit{App Activity}), we annotate data ourselves and add it to the dataset. \smallskip

\noindent
\textbf{Creating Annotation Set} For our \textit{Privacy Label Taxonomy}, we were able to  we have data missing for \red{13} \todo[inline]{CHECK} elements. Curating the candidate set for missing categories is a major challenge due to label imbalance. To address this issue, we follow the approach used by Harkous et al~\cite{harkous2022hark} and use the task of \textit{Natural Language Inference} (NLI) to curate the candidate set. The NLI tasks consist of a hypothesis and a premise, and the objective is to determine if the hypothesis is true (\texttt{entailment}), false (\texttt{contradiction}) or undetermined (\texttt{neutral}) given the premise~\cite{}. For example, if the premise is: \textit{``Your data is safely and completely removed from our servers or retained only in anonymized form.''} and the hypothesis is \textit{``Data deletion is being discussed''}, then this instance will receive an entailment. On the other hand, if the hypothesis were \textit{``Policy change is being discussed''}, then the label would be neutral. This method of using NLI-based sampling to reduce the annotation effort has been shown to be effective by Harkous et al~\cite{harkous2022hark}.

We start by creating hypotheses for each of the missing categories that we have. For example, for \textit{Data Deletion Option}, we created two hypotheses: ``Data deletion is being discussed'' and ``Data Anonymization is being discussed''. For the NLI task, we used the T5-Large model checkpoint from \texttt{Huggingface}. This model is already trained on MultiNLI task~\cite{} which consists of a multi-genre dataset covering a large variety of domains.  Next, we run the NLI model and get weak labels for all the missing categories. Note that these are weak labels that are later manually annotated to create the training set. In Appendix.~\ref{},\todo[inline]{UPDATE} we provide more details on the NLI based sampling, along with a full set of hypotheses used for each category.\smallskip

\noindent
\textbf{Annotation Details:} Using the NLI sampling approach, we curated the candidate set with 2000 segments for each of the missing categories. These segments are roughly balanced based on the weak labels assigned by the NLI model. For each class, we then randomly sample 500 segments to annotate. Two of the authors annotated the segments and created the training set. The annotators had an overlapping set of 200 segments to measure the inter-annotator agreement. We find that the annotators showed high agreement with a Cohen's Kappa value of ($\kappa = \red{.85})$. \todo[inline]{UPDATE}The annotation was performed using label-sleuth framework~\cite{}. The framework supports active learning with a basic BOW model in order to facilitate annotation.\smallskip

\noindent
\textbf{Classification Details:} Large language models like BERT~\cite{}, T5~\cite{} etc have shown remarkable performance using small training sets. Thus, for our purposes, we use the \texttt{distilbert-base-uncased}~\cite{} model consisting of 67M parameters. This model is the distilled version of BERT~\cite{}. It has 40\% fewer parameters, can run 60\% faster, and performs only slightly worse (~5\%) than the original bert-base-uncased model  on several natural language tasks. Additionally, we also perform domain adaptation by pre-training the DistilBert model on privacy policy text with the Masked Language Model (MLM) task. In particular, we pre-trained the model with the default hyperparameters, with a batch size of 256 for 24800 steps on a single NVIDIA A100 GPU.

 We then use the new pre-trained model to train the category classifiers for the Privacy Label Taxonomy. We use a classification head on the model after adding a linear classification. For classification, the data is split into testing (20\%) and training sets (80\%). The performance on the test set for classifiers corresponding to the \textit{Privacy Label Taxonomy} categories are shown in Table.~\ref{}\todo[inline]{UPDATE}. We also compare our performance with two prior works which use OPP-115 dataset to build the models. Note that for categories that are common, our classifier performs comparatively with the prior works. A notable exception is for \textit{Secure Transfer} where the prior work is trained on OPP-115 dataset (with limited data for this class), whereas we trained using our annotation and were able to achieve better performance. We used the OPP-115 annotations for \textit{Secure Transfer} as an additional test set (with 64 positive annotations) and found that our model predicted the right class 97\% of the time, showcasing the quality of our annotations and their generalization capabilities.  We would also like to note that our slightly worse performance than BERT~\cite{} can be attributed to the fact that we used Distilbert~\cite{} which is 60\% faster and has 40\% fewer parameters than BERT, with a small loss in performance.  


\begin{table}[t]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Category} & \textbf{CNN ~\cite{}} & \textbf{BERT ~\cite{}} & \textbf{Here}\\
\midrule
\rowcolor{aliceblue}First-party-collection-share  & 50 & 0.96 & 0.98 \\
Third-party-sharing-collection & 50 & 0.98 & 0.96 \\
\rowcolor{aliceblue}Identifiability & 100 & 0.97 & 0.97\\
Does-does-not & 50 & 0.98 & 0.96 \\
\rowcolor{aliceblue}Secure Transfer  & 50 & 0.96 & 0.99 \\
Data Deletion Option  & TODO & TODO & 0.91 \\
\rowcolor{aliceblue} (DC) App Activity & TODO & TODO & 0.93 \\
(DC) App Info and Performance & TODO & TODO & 0.93 \\
\rowcolor{aliceblue} (DC) Sensitive Info  & TODO & TODO & 0.97 \\
(DC) Location  & TODO & TODO & 0.99 \\
\rowcolor{aliceblue} (DC) Health and Fitness  & TODO & TODO & 0.97 \\
(DC) Device or Other ID  & TODO & TODO & 0.94 \\
\rowcolor{aliceblue} (DC) Photos and Videos  & TODO & TODO & 0.96 \\
(DC) Web Browsing  & TODO & TODO & 0.96 \\
\rowcolor{aliceblue} (DC) Contacts  & TODO & TODO & 0.87 \\
(Purp) Account Management  & TODO & TODO & 0.92 \\
\rowcolor{aliceblue} (Purp) Developer Communication  & TODO & TODO & 0.93 \\
(Purp) Personalization & TODO & TODO & 0.98 \\
\bottomrule
\end{tabular}
\caption{Classifiers' performance on the test set. The training was done on one GPU with early stopping.}
\todo[inline]{Paul: Update CNN and BERT stats}
\label{tab:cookie_classifier}
\end{table}

\fi

\subsection{Identifying Cross-Listed Apps}
\label{sec:cross_apps}
We next describe the process to identify cross-listed apps across the two platforms ((3) in \cref{fig:meas_pipeline}). We use the resulting dataset to compare the privacy labels of apps across the two platforms (\cref{sec:consistency_cross}). Identifying two versions of the same app across platforms is challenging due to the lack of unique identifiers~\cite{hooda2022skillfence}. 




To uniquely map apps across platforms, we develop a heuristic based on combinations of pseudo-identifiers, such as the app name, developer name, privacy policy, and developer website. We start with the apps which have the same name across both platforms (n=\nnumber{220K}). Next, if the privacy policy of the apps matches, then we treat them as a unique match (n=\nnumber{85K}). In some cases, like the \textit{NTLC Catalog} app on the Play Store and App Store, the app developers can include platform-specific identifiers in the URLs for privacy policies. To capture these instances, we match the first level domain of the privacy policy URLs and identify them as unique matches (n=\nnumber{54K}). Finally, while providing privacy policy links is highly encouraged in both platforms, some apps do not contain the link to the privacy policy. To further increase the coverage, we also match the first-level domain of the developer website, which is present on both platforms. Using these criteria, we are further able to get \nnumber{25K} matches. This way, we obtain a total of \nnumber{165K} apps that have instances in both Apple play store and Google play store.\smallskip


\noindent
\textbf{Manual Verification:} To assess whether our heuristic results in false positive matches, two of the authors manually verified 150 app pairs identified using each of the three heuristics and found that no app from Google Play Store was matched to an incorrect app from App Store. It is worth noting that for our analysis, having an accurately mapped set is more important than capturing all instances of cross-listed apps. \smallskip

\noindent
\textbf{Cross-listed Apps Dataset}
Using the method described above, we find a total of \nnumber{165K} cross-listed apps. Among these apps, we find that \nnumber{5\%} have privacy nutrition labels only on the Google Play Store, \nnumber{20.2}\% have the label only on the Apple App store, \nnumber{60.8}\% have labels on both the platforms and \nnumber{13.9}\% do not have a privacy nutrition label on either platform. The higher rate of privacy labels for the App store can be understood as Apple enforced nutrition labels on their platform earlier than Google, giving more time for developers to add the details in the APL.

\subsection{Ethical Considerations}
We collected data only from publicly available web pages and APIs. While our data collection scripts might load Google and Apple's servers, we were careful to not abuse these resources. In particular, we added back-off strategies in case of errors and waited for sufficient time before retrying for the failed cases. Furthermore, for privacy policy extraction, we were respectful of robots.txt and only extracted HTML when the website allowed us to.

\subsection{Observations}
Our measurement pipeline results in two initial observations. First, app developers have been slow to add privacy labels to their apps, even after the hard deadlines have passed. Privacy labels are present only for 69\% of the apps on the Apple app store and \nnumber{50.2}\% of the apps on the Google play store (as of November 2022). Second, our measurement pipeline produced large-scale datasets for Apple Privacy Label (n=955K), Google Data Safety Section (n=\nnumber{573K}), and privacy policies (n=\nnumber{598K}) corresponding to the apps. In addition, we generated a new \textit{Privacy Label Taxonomy} by adding missing elements to OPP-115 taxonomy. We also supplement the existing privacy policy datasets by adding annotations for the new categories. 



% While there are several pseudo-identifies such as the app name, developer name, privacy policy, and developer website, none of them constitute unique identifiers.

 % We also find that many of the apps do not provide access to their privacy policies. Furthermore, on the play store - 99\% of the apps with a Data Safety Card have a privacy policy. However, only \nnumber{58.3\%} of apps had unique privacy policies, of which only \nnumber{9.8\%} had invalid or unreachable links. 


% After extracting the policy text, we break the text down into consecutive coherent segments. As machine learning classifiers have limitation on the length of input sentence, this practice is regularly followed in the literature. For segmentation, we follow the methodology from Harkous et al~\cite{} in which we use \textit{GraphSeg}~\cite{} to generate semantically coherent segments. \kassem{we need to incorporate these sentences here.}

% Another potential scenario is that apps can have different names but the same privacy policy and the same developer name. In such cases, we can do a fuzzy string matching and find apps with similar names. While capturing these instances will increase our coverage, it might also result in higher number of false positives as different apps developed by the same developer might get flagged with this approach. For example, \texttt{app1} and \texttt{app2} have the same developer and the same privacy policy but have different functionality. Our objective with the analysis of privacy labels for cross-listed apps is to understand whether developers disclose their privacy practices similarly on the two platforms. For our purposes, having an accurately mapped set for is more important than captures all instances cross-listed apps. Therefore, we rely on the strong set of heuristics to curate this set.\smallskip

% We curate two large scale datasets to understand the landscape of privacy labels. The first is a dataset of data safety section for \nnumber{1.13M} apps on Google Play. We refer to this dataset as the Data Safety Section--\textbf{DSC dataset}. The second is a dataset of privacy nutrition labels for \nnumber{573k} apps on Apple App Store. We refer to this dataset as the Apple Privacy Label--\textbf{APL dataset}. 

% In the following, we describe how we collected both datasets.

%The policy dataset size is lower than the number of apps because (a) multiple apps can have the same developer and list the same privacy policy, (b) some apps may use generic policy templates hosted by third party websites, and (c) not all apps have a working privacy policy URL. Further, we note that we only work with policies and privacy labels that are present in the English Language. 



% For example the game ~\textit{Mini Militia - Doodle Army 2} with over 200M downloads on the Google Play Store\footnote{https://play.google.com/store/apps/details?id=com.appsomniacs.da2} has a DSS but lacks an APL as on Apple App Store\footnote{https://apps.apple.com/us/app/mini-militia-doodle-army-2/id405885221} it was last updated on Nov 30, 2020, before the introduction of the APL but was last updated on Oct 19, 2022, as of writing this paper, on the Play Store.
% Similarly, ~\textit{slither.io} with over 500M downloads on Google Play Store\footnote{https://play.google.com/store/apps/details?id=air.com.hypah.io.slither} lacks a DSS as it was last updated on Dec 20, 2019 on the Play Store, but has an APL\footnote{https://apps.apple.com/us/app/slither-io/id1091944550} with the last update being Mar 19, 2022.

% For example, the same developer can build multiple apps across both platforms, each of the apps having their own privacy label. However, these apps might have the same privacy policy, developer name, and developer website. Further, a developer might also use different URLs to show privacy policies, such as \url{GreatDay HR}, or use different URLs to show same the policy like \url{Safe Wi-Fi} by Verizon

% \kassem{fill the app name here} 

% \todo[inline]{Asmit: Apps with specifically different PP or just different PP Urls?}

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=\columnwidth]{figures/cross_listing.pdf}
%   \caption{Pipeline for identifying cross listed apps}
%   \label{fig:cross_listed}
% \end{figure}
