The object instance segmentation task is to identify and delineate distinct objects stored in containers in a warehouse. In the context of robotic object manipulation, instance segmentation is used to inform downstream robotic processes such as grasp generation, motion planning, and placement. Accuracy of instance segmentation can have an impact on picking success, object identification, and defects introduced in the process. For example, under-segmentation can result in picking multiple objects at a time, while over-segmentation can result in a bad choice of grasp leading to damage or dropping of objects. Fig.\ \ref{fig:segmentation_subsets}(a) shows manually annotated object segments on the pick-image. Presence of deformable and transparent objects in clutter makes the task challenging.

Our object instance segmentation dataset contains 50K+ images of objects stored in containers in a warehouse with 500K+ annotations. The annotations include instance-level segmentation masks and bounding box for two classes (object and container). Technicians with task-specific training generated high-quality annotations for object boundaries and object class which are verified by two additional quality assurance technicians.

%The verified images and annotations are added to the dataset.

% Object instance segmentation 
%Instance segmentation is used in object manipulation to identify and outline distinct objects presented in storage units. The resulting object segments are useful for subsequent item understanding, grasp detection, and manipulation planning.
% Good instance segmentation can increase picking eligibility, improve identification success, as well as reduce pick defects and item damage.

%Accurate segmentation is one of the critical enablers for scaling high-performing robotic manipulation at Amazon.
%Reliable segmentation at the Amazon scale is very challenging. The segmentation algorithm needs to generalize to millions of unique items in Amazon warehouses, many of which are unseen. The algorithm must be robust in heavy clutter and occlusion, as objects are packed tightly in containers. Finally, the algorithm needs to transfer successfully to changing containers, lighting conditions, and ... 

%Meanwhile, academic researchers in robotic manipulation are facing similar challenges. Robots working in an unstructured environment need to handle a wide variety of seen and novel items, presented in totes, drawers, shelves, and on tabletops. To operate reliably with this diversity, the robot needs to learn the object concepts from a wide distribution of object data to generalize to unseen items. 

%We present the large-scale Amazon warehouse instance segmentation dataset. The dataset contains over 50k images of Amazon products placed in containers and has over 500k instance annotations. 
%The dataset aims to inspire research into object segmentation in clutter and to provide the robotic community with a wide distribution of real-world objects for learning and benchmarking. 
%The annotations include instance-level segmentation masks, bounding boxes, and class labels. Amazon's internal ML labeling team hand labeled the pixel-wise instance masks and class labels. The associates undergo task-specific training and auditing to produce high-quality class labels and segmentation with exact boundaries. Meanwhile, all labeled images are examined by two verifiers to detect defects such as incomplete and missing segments, in-precise boundaries, and wrong classifications. Images and labels are only used when both verifiers vote no issue. 

% We divide the object segmentation dataset into three subsets: 1) \textit{mix-object-tote} which comprises images of objects in yellow and blue totes with region of interest cropped to the boundary of the tote. 2) \textit{zoomed-out-tote-transfer-set} which comprises images of objects in a yellow tote with the tote centered in the image and covering 50\% of the image, and 3) \textit{same-object-transfer-set} which comprises multiple instances of the same object stored in packaging containers (Fig.\ \ref{fig:segmentation_subsets}). The three subsets in the dataset enable us to understand the impact of variation in background, container, and object distribution. The mix-object-tote subset comprises 44,253 images and 467,225 annotations. It has the highest degree of clutter with 10.5 object instances per image. The zoomed-out-tote-transfer-set subset comprises 5,837 images and 43,401 annotations with an average of 7.5 instances per image. The same-object-transfer-set subset comprises 3,323 images and 12,664 annotations with an average of 3.8 instances per image. 

We divide the object segmentation dataset into three subsets. The primary set, \textit{mix-object-tote}, comprises 44,253 images and 467,225 annotations of objects in yellow and blue storage totes. The totes contain a heterogeneous clutter of objects with an average of 10.5 object segments (ranging from 1 to 50 segments) in each image. The other two subsets, namely \textit{zoomed-out-tote-transfer-set} and \textit{same-object-transfer-set} (Fig.\ \ref{fig:segmentation_subsets}(b) and (c)) enable us to understand the impact of variation in data distribution. The \textit{zoomed-out-tote-transfer-set} subset with 5,837 images and 43,401 annotations captures images of containers from a different warehouse. It poses a transfer learning challenge due to significant differences in background, scale, and object distribution. The \textit{same-object-transfer-set} subset contains 3,323 images and 12,664 annotations. It captures a common and visually challenging scenario in warehouses where multiple instances of the same object are tightly packed in a container. 

%We divide the dataset into three subsets to highlight the challenge of transferring learned knowledge across tasks. The first and largest subset is called the mix-object-tote. It comprises images of mixed objects in yellow or blue totes. The second subset is named the zoomed-out-tote-transfer-set. It contains mixed objects placed in a yellow tote but captured with sensors placed further away from the tote and under different lighting. The third subset is named the same-object-transfer-set. It contains multiple instances of the same object placed tightly packed in different storage containers (Fig.~\ref{fig:segmentation_items}).

\begin{figure}
	\centering
	\includegraphics[width = 0.5\textwidth]{images/three-itemsets.jpg}
	\caption{(a) Segmentation annotation overlaid on an image from {\it mix-object-tote}. Each identifiable item is segmented regardless of its size and occlusion. Multiple objects in the same package are considered as one object and is delineated by the boundary of the package. In particular, items wrapped in transparent packaging are segmented by the peripheral of the package, although other products may be seen through them. (b-c) Example images from {\it zoomed-out-tote-transfer-set} and {\it same-object-transfer-set} subsets representing variations in background, scale, and clutter.}
	\label{fig:segmentation_subsets}
	\vspace{-0.2in}
\end{figure}



%The mix-object-tote is the largest dataset among the three subsets, containing 44,253 images and 467,225 annotations. On average, it has the highest clutter level of 10.5 instances per tote. The zoomed-out-tote-transfer-set contains 5,837 images and 43,401 annotations, averaging 7.5 objects per tote. The same-object-transfer-set dataset has 3,323 images and 12,664 annotations, averaging 3.8 objects per scene. We adopt a class-agnostic labeling scheme where each pick scene image is labeled with two classes: Tote and Object. Examples of labeled images are shown in Fig.~\ref{fig:segmentation_rules}:

% Add (b) to Fig. 3(a)
%\begin{figure}
%	\centering
%	\includegraphics[width = 0.5\textwidth]{images/segmentation_masks.png}
%	\caption{Each identifiable distinct item is segmented regardless of its size and occlusion. Multiple objects in the same package are considered as one object and is delineated by the boundary of the package. In particular, items wrapped in transparent packaging (e.g., plastic bags) are segmented by the peripheral of the packages, although other products may be seen through them.}
%	\label{fig:segmentation_rules}
%\end{figure}

%Accuracy and speed are both critical metrics used to evaluate segmentation algorithms for warehouse picking. 
To establish a performance baseline, we trained Matterport's implementation of Mask R-CNN~\cite{matterport_maskrcnn_2017, DBLP:journals/corr/HeGDG17} with ResNet-50 backbone~\cite{DBLP:journals/corr/HeZRS15} on the {\it mix-object-tote} dataset. Default training schedule (for MS-COCO) and hyper-parameters were used along with a train-valid-test split of 0.7:0.15:0.15. % and the images are split by timestamps, so similar images from the same order do not exist across splits. . 
Table~\ref{table:segmentation_results_inference} shows the results for our baseline experiment. Mean average precision ($mAP$) for a threshold of 0.5 ($mAP_{50}$) and 0.75 ($mAP_{75}$) are used to evaluate the performance of the baseline model on test set. 
%For computation time we cite the 5 fps as reported in ~\cite{DBLP:journals/corr/HeGDG17}, and we test the segmentation accuracy on the testing split of all three subsets, and the evaluation results ($mAP_{50}$ and $mAP_{75}$) are reported in :


We observe that applying model weights trained on {\it mix-object-tote} to the {\it zoomed-out-tote-transfer-set} ($mAP_{50}=0.25$) and {\it same-object-transfer-set} subsets ($mAP_{50}=0.11$) yields poor results. While techniques like transfer learning can improve performance on a new scenario when a reasonable amount of domain-specific labeled data is available, labeling specifically for each variation is time-consuming, if feasible at all. The ultimate goal is to readily transfer segmentation to new scenarios with minimal additional annotations.

%It should be noted that directly applying weight trained on mix-object-tote to the two untrained subsets yields poor results. While techniques like transfer learning can significantly improve performance on a new task when a reasonable amount of task-specific labeled data is available, labeling specifically for each variation is time-consuming, if feasible at all. The ultimate goal is to readily transfer segmentation to new tasks with minimal additional annotations. 

\setlength{\tabcolsep}{4pt}
\begin{table}
\centering
\caption{Mask R-CNN performance for object segmentation task. The model was trained on \textit{mix-object-tote} dataset}
\label{table:segmentation_results_inference}
\begin{tabular}{@{}rccc@{}}
    \hline
 & mix-object-tote & zoomed-out-tote- & same-object- \\ 
 & & transfer-set & transfer-set \\ \hline
$mAP_{50}$  & 0.72 & 0.25 & 0.11 \\ \hline
$mAP_{75}$  & 0.61 & 0.19 & 0.10\\ \hline
\end{tabular}
\end{table}
\setlength{\tabcolsep}{1.4pt}

We observe that segmentation performance for our baseline model has a strong correlation to the level of clutter. Fig.~\ref{fig:segmentation_clutter} shows that the performance drops significantly as the number of ground-truth object instances increases in the image. The $mAP_{50}$ score drops sharply from 0.95 when the tote has one to five object instances to a low of 0.38 when there are more than 26 object instances in the image. This motivates developing algorithms that are robust against clutter and occlusion to further improve object segmentation performance.

\begin{figure}[h]
	\centering
	\includegraphics[width = 0.45\textwidth]{images/segmentation_performance}
	\caption{Performance on {\it mix-object-tote} with varying degree of clutter.}
	\label{fig:segmentation_clutter}
	\vspace{-0.15in}
\end{figure}















