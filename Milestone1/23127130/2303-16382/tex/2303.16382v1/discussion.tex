In this work we introduced ARMBench, a large-scale, object-centric benchmark dataset for robotic manipulation in warehouses. The object segmentation benchmark presents challenges relating to clutter, deformable and transparent packaging as well as the problem of degrading performance with different backgrounds and storage configurations. The identification benchmark presents an open set recognition challenge on a wide variety of objects. Additionally, images of the same object can vary significantly due to differences in configurations and packaging variations while images of two different objects can appear similar. Missing reference images and high precision requirement makes the benchmark well suited to evaluate uncertainty estimation algorithms.
Finally, the defect detection benchmark presents a unique set of challenges such as detection of multi-pick, opening, and deconstruction of packages. Annotations and baselines are provided both for a single-shot as well as video-based detection of such events. 

Our intention is for this dataset to grow over time with a goal to increase the number of unique objects, environments, and benchmark tasks. Large-scale sensor data and fine-grained attributes of objects will enable learning generalizable representations that could transfer to other visual perception tasks. We further plan to enrich our dataset with 3D data and annotations, and propose new benchmark tasks. 
% Both 3D data and robot-object interaction forces will be used to create benchmarks for multi-modal perception tasks. 
% Our hope is that the research community will use this data to learn objectness and benchmark new methods on our dataset.
%While not considered in the current work, the robotic setup already captures 3d data of the pick and place scenes. This data can be used to create benchmarks for tasks such as object pose estimation and shape completion. The set-up is also well-suited to study continuous perception tasks such as tracking and reconstruction. Furthermore, additional object priors such as text descriptors, dimensions, weights, product categories etc. and sensing modalities such as depth, force, weight etc. can be used to study multi-modal perception problems.