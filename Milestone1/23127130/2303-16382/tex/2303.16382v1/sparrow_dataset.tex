\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
% you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed


\usepackage{color,soul}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{lineno}
\usepackage{hyperref}

%\usepackage[default]{roboto}  
% Amazon Warehouse Dataset: A large-scale, object-centric dataset for Robotic Manipulation in Warehouses
\title{\LARGE \bf
	{ARMBench}: An Object-centric Benchmark Dataset\\ for Robotic Manipulation
}

\author{Chaitanya Mitash$^{1}$, Fan Wang$^{1}$, Shiyang Lu$^{2}$, Vikedo Terhuja$^{1}$,\\ Tyler Garaas$^{1}$, Felipe Polido$^{1}$, Manikantan Nambi$^{1}$% <-this % stops a space
\thanks{$^{1}$Amazon Robotics, MA, USA. \{cmitash, fanwanf, terhuja, tggaraas, polidof, mnambi\}@amazon.com}%
\thanks{$^{2}$Computer Science Department, Rutgers University, NJ, USA. shiyang.lu@rutgers.edu. Work done during a co-op at Amazon Robotics.}%
}
\begin{document}
	\maketitle

	%%%%%%%%% ABSTRACT
	\begin{abstract}
	This paper introduces Amazon Robotic Manipulation Benchmark ({ARMBench}), a large-scale, object-centric benchmark dataset for robotic manipulation in the context of a warehouse. Automation of operations in modern warehouses requires a robotic manipulator to deal with a wide variety of objects, unstructured storage, and dynamically changing inventory. Such settings pose challenges in perceiving the identity, physical characteristics, and state of objects during manipulation. Existing datasets for robotic manipulation consider a limited set of objects or utilize 3D models to generate synthetic scenes with limitation in capturing the variety of object properties, clutter, and interactions. We present a large-scale dataset collected in an Amazon warehouse using a robotic manipulator performing object singulation from containers with heterogeneous contents. ARMBench contains images, videos, and metadata that corresponds to 235K+ pick-and-place activities on 190K+ unique objects. The data is captured at different stages of manipulation, i.e., pre-pick, during transfer, and after placement. Benchmark tasks are proposed by virtue of high-quality annotations and baseline performance evaluation are presented on three visual perception challenges, namely 1) object segmentation in clutter, 2) object identification, and 3) defect detection. ARMBench can be accessed at \href{http://armbench.com/}{http://armbench.com}
	%\vspace{-0.1in}
	% This paper introduces the Amazon Warehouse (AWARE) dataset, a large-scale dataset and benchmarks for robotic manipulation of warehouse objects. Automating operations in modern warehouses will require a robotic manipulator to deal with a clutter of heterogeneous objects with a wide variety of physical properties on a continuously changing inventory. Such interactions pose challenges in sensing the identity, physical characteristics, and state of objects before, during, and after manipulation. Existing datasets consider a limited set of objects or utilize 3D object models to generate synthetic scenes. Accurately modeling the variety of object properties, clutter, and interactions can be challenging. We present a large-scale dataset collected in an Amazon warehouse using a robotic manipulator performing object singulation from a heterogeneous pile. The AWARE dataset contains images, videos and metadata corresponding to 185+ pick activities over 150K+ unique objects. The images are captured at different stages of manipulation, i.e. pre-pick, during transfer, and after placement. Benchmark tasks are proposed by virtue of high-quality annotations and baseline performance evaluation on three visual perception challenges, namely object segmentation in clutter, object identification, and defect detection. The dataset, annotations, and benchmark tasks can be accessed here: link here
	%This paper introduces the Amazon Warehouse (AWARE) dataset, a large-scale dataset that captures the challenges of robotic manipulation within a modern warehouse setting. Many operations in warehouses require a robotic manipulator to deal with a heterogeneous clutter of objects with a wide variety of physical properties on a continuously changing inventory. Such interactions pose complex challenges in terms of sensing the identity, characteristics and state of objects before, during and after manipulation. Existing datasets either consider a limited set of objects and operate under a closed-set assumption or utilize 3d object models to generate synthetic scenes. Accurately modeling the variety of object properties, clutter configurations and interactions can be challenging. To that end, a large-scale dataset is collected within an Amazon warehouse using a robotic manipulator singulating objects from a heterogeneous pile with pick-and-place operations. The AWARE dataset contains images, videos and metadata corresponding to 185+ pick activities over 150K+ unique objects. The images are captured at different stages of manipulation, i.e. pre-pick, transfer and after placement. Benchmark tasks are proposed by virtue of high-quality annotations and baseline performance evaluation on three visual perception challenges, namely object segmentation in clutter, object identification and defect detection. The dataset, annotations and benchmark tasks are anticipated to be continually growing and can be accessed here: link here
	\end{abstract}

	\section{Introduction}
	\label{sec:introduction}
	\input{introduction}

	\section{Related Work}
	\label{sec:related_work}
	\input{related_work}

	\section{ARMBench Dataset}
	\label{sec:dataset}
	\input{dataset}

	\section{Object Segmentation}
	\label{sec:segmentation}
	\input{segmentation}

	\section{Object Identification}
	\label{sec:identification}
	\input{identification}

	\section{Defect Detection}
	\label{sec:defect_detection}
	\input{defect_detection}

	\section{Discussion and Future Work}
	\label{sec:discussion}
	\input{discussion}
	
	\section{Acknowledgements}
	We would like to thank the Sparrow \cite{Sparrow2022} team members for deployment and operation of the robotic workcell, Aalekh (Raj) Ray Chaudhury and the Go-AI team for data annotation support and the Item-matrix team for curating the reference image dataset. We would also like to thank Joey Durham, Andy Marchese, Clay Flannigan, Parris Wellman, Jane Shi and Kapil Katyal for their valuable feedback.
	
	\bibliographystyle{IEEETranS}
	\bibliography{bibtex}	
\end{document}
