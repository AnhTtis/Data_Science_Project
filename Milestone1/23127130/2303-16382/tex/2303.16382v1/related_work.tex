\begin{figure*}[h!]
	\includegraphics[width=\linewidth, keepaspectratio]{images/contributions-new.jpg}
	\caption{(left) Benchmark tasks and annotation statistics on the {ARMBench} dataset. (right) Distribution of product-groups and object dimensions for 190,00+ unique objects in the dataset.}
	\label{fig:contributions}
	\vspace{-0.1in}
\end{figure*}

\subsection{Benchmarking in Robotic Manipulation}

A recent benchmarking effort for robotic manipulation \cite{calli2021guest} considers challenges in mechanical design, grasp planning and deformable object manipulation but does not focus on the complexities of underlying perception tasks. An annual competition \cite{hodan2018bop} benchmarks performance of relevant perception algorithms such as object detection, segmentation and 6D pose estimation over a collection of datasets \cite{Brachmann2014Learning6O, Hodan2017TLESSAR, Xiang2018PoseCNNAC}. The Amazon robotics challenge \cite{Correll2018AnalysisAO, Eppner2017LessonsFT} initiated the development of other relevant datasets \cite{Rennie2016ADF, Leitner2017TheAP, zeng2017multi}. While these datasets present interesting challenges in terms of variety of configurations and large occlusions, they are limited in terms of the number of object instances with a maximum of 42 unique objects.

\subsection{Object Segmentation}

Object instance segmentation refers to simultaneously predicting pixel-level instance-mask and corresponding class labels. The technique has been widely applied in autonomous driving~\cite{zhang2016instancelevel, Brabandere2017, RenZ16}, video surveillance~\cite{gajjar2017human, Salscheider2021}, and robotics~\cite{Xie2021, Danielczuk2018, Liao2010}.  The introduction of large-scale labeled dataset such as MS-COCO~\cite{Lin2014MicrosoftCC}, PASCAL VOC~\cite{PASCAL2009}, and Cityscapes~\cite{DBLP:journals/corr/CordtsORREBFRS16} has significantly advanced the state-of-the-art in detection and segmentation, particularly for common object categories from WordNet~\cite{Fellbaum1998}. These datasets serve as a standard benchmark for evaluating computer vision models but are not representative of objects a robotic manipulator would interact with. Representative datasets such as the MVTec D2S dataset~\cite{DBLP:journals/corr/abs-1804-08292} are limited in size and diversity of objects. To obtain data at scale, a common strategy is to generate synthetic data with physics simulators and rendering tools~\cite{danielczuk2019segmenting, DBLP:journals/corr/MahlerLNLDLOG17, DBLP:journals/corr/abs-1906-07480, Xie2021, DBLP:journals/corr/abs-1809-10790}. Nevertheless, synthetic datasets are limited by the availability of high-quality 3D object models, and inherently carry a {\it sim2real} gap~\cite{ DBLP:journals/corr/abs-1809-10790}. This work introduces a real-world, large-scale dataset for object segmentation that captures a wide variety of objects and configurations relevant to robotic manipulation.

\subsection{Object Identification}

Object identification refers to the task of exactly identifying the object specified by an image segment. In an open-set setting it is often posed as an image retrieval problem i.e., given a query image and a database of candidate images, rank them according to their similarity to the query image. Various approaches have been used to tackle this problem such as aggregating pre-defined local features \cite{sivic2003video, philbin2007object, jegou2010aggregating, tolias2016image}, computing similarity metrics over features derived from large-scale image classiÔ¨Åcation training \cite{babenko2014neural, tolias2015particular, garcia2019learning}, and metric learning with pairs of matching and non-matching images \cite{radenovic2018fine, tolias2020learning}. Common benchmarks for image retrieval consider landmark datasets \cite{philbin2007object, philbin2008lost, weyand2020google} and retail datasets such as DeepFashion \cite{liu2016deepfashion, ge2019deepfashion2}, Online Products dataset \cite{oh2016deep}, RPC \cite{wei2019rpc}, RP2K \cite{peng2020rp2k}, Products-10K \cite{bai2020products}, and AliProducts \cite{cheng2020weakly}. The product images in these datasets are online store images, customer images or photos from retail stores. Alternatively, the images in the proposed dataset are representative of how objects are stored in a warehouse, with different types of packaging and in cluttered configurations. The robotic manipulation context not only provides a unique set of challenges for object identification in terms of occlusion and viewpoint variations but also imposes stringent requirements in terms of precision.

\subsection{Defect Detection}

Few image datasets exist for objects with defects. Prior research on visual defect detection has focused on surface defects for individual objects such as LED chips\cite{Lin2019}, fabrics \cite{Blanes2019}, and metals \cite{Yanqi2021}. The DAGM 2007 Competition dataset \cite{DAGM2007} comprises 6,900 synthetic images with six different types of surface defects. The MVTec Anomaly Detection dataset comprises 5,354 color images corresponding to 15 object and texture categories with 70 different types of defects such as scratches, dents, contaminations, and structural changes \cite{Bergmann2019}. This is the first dataset to capture defects in the context of robotic manipulation. 

Datasets for defect detection in videos primarily focus on anomaly detection methods for events such as throwing objects, loitering, running \cite{Liu2018}, crowded scenes \cite{Li2013}, and anomalous pedestrian patterns \cite{Landi2019}. These datasets contain a limited number of videos (10-50) per activity. Video classification datasets exists in the domain of sports activities \cite{Karpathy2014}, human actions \cite{Kay2017}, and holistic video understanding \cite{diba2020large}. Similar to existing research on using videos for understanding context and actions, videos can be used to understand events in robotic manipulation process such as successful and defective activities. There exists no large-scale video datasets for this purpose.
