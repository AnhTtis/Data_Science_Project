% Distribution of damage types between multi-pick, open, and deconstruction
% Can we obtain distribution of taxonomy
% Distribution of object categories: Probably GL code based



% Problem statement and setup
% From a business standpoint, we care about multi-view/multi-model classification
% From a computer vision standpoint, we care about single image and single video classification


The defect detection task is to identify if a robotic manipulation activity resulted in a defect. Two types of robot-induced defects are included in the dataset: 1) \textit{multi-pick}, and 2) \textit{package-defect}. \textit{Multi-pick} is used to describe activities where multiple objects were picked and transferred from the source container to the destination container. \textit{Package-defect} is used to describe activities where the object packaging \textit{open}ed and/or the object separated into multiple parts (\textit{deconstruction)}. Two subclasses, \textit{open} and \textit{deconstruction}, are defined for package-defect.  %In warehouse operations, open packages need to be inspected to ensure integrity of the object, whereas in the case of deconstruction, the operation will be interrupted to recover the fallen object parts. 
Fig.\ \ref{fig:defects} shows examples of multi-pick and package-defect in our dataset. Multi-pick are often observed when there is a high degree of clutter, there are multiple instances of the same object, or when objects of significantly different sizes are placed together. Fig.\ \ref{fig:defects} (a-c) shows package-defect on a variety of objects. Defects on deformable objects like plastic bags can be challenging for visual detection. %While the defects presented in this dataset are generated by a vacuum-based end-effector, they generalize to grasping end-effectors as well. Additionally, vacuum end-effectors are ubiquitous in object handling for their relatively low hardware and algorithmic complexity.

Our dataset comprises 19,303 images of objects from multiple viewpoints (Transfer-images) and 4,070 videos of pick-and-place activities that resulted in a defect. % In addition, 100,000 images of objects and 100,000 videos of activities that did not result in a defect are included in the dataset (defined as {\it nominal}). 
Videos are excluded from our dataset for multi-pick defect as such defects are not observable along specific viewpoints. Multi-view Transfer-images are best suited to detect multi-pick defect. On the other hand, \textit{open} and \textit{deconstruction} defects can happen at any time during an activity. As a result, they are best captured using videos. The dataset includes 100,000 images of objects and videos of activities that do not have any defects and are defined as \textit{nominal} activities. 
Tables\ \ref{tab:defect_image_baseline} and \ref{tab:defect_video_baseline} shows the distribution of defect types in our dataset. In addition to Transfer-images, the dataset includes Pick-image and Place-image that provide context for an activity. 

%Our dataset includes images and videos for activites that resulted in a defect. For each activity, one video was recorded from a static viewpoint (Webcam) that captures the whole manipulation process for each activity. Four images (Transfer-RGB) show the object being manipulated from different viewpoints during the activity, one image (Pick-RGB) shows the object in the source container before being picked, and one image (Place-RGB) shows the object in the destination container after placement.

%Fig.\ \ref{fig:defects}(a)--(c) shows package-defects for different types of objects in our dataset. Fig.\ \ref{fig:defects}(a) shows and open box with contents of the box falling out (Fig.\ \ref{fig:defects})(a). However, an open bag is very hard to differentiate from a closed/sealed bag even to an expert eye (Fig.\ \ref{fig:defects}(d). Occlusions and homogeneity of materials makes multi-pick detection challenging. Fig.\ \ref{fig:defects}(g) shows a multi-pick defect where only a small part of one of the objects is visible and in Fig.\ \ref{fig:defects}(h) two instances of the same object was picked which is hard to differentiate. 

%Three types of defects may appear in each activity: 1) \textit{multi-pick}, 2) \textit{open}, and 3) \textit{deconstruction}. \textit{Multi-pick} means that multiple objects are picked and transferred by the end-effector. \textit{Open} means that an object package is opened during an activity. \textit{Deconstruction} means that an object separates into two or more parts. \textit{Deconstruction} can happen either due to an object breaking into multiple parts or the contents of a package falling out. Sometimes, more than one types of defect can appear in a single activity, e.g., a shoe box \textit{open}s and then \textit{deconstruct}s (shoes fall out). % Thus, the defect detection task is a multiclass classification problem on both images and videos. 
%Early detection of defects can enable the system to take corrective action like place the defective object in the source container or in a special container for processing. 

A two-step process was used to annotate data. A technician operating our system labeled each activity as successful/nominal (a single object transferred from the source container to the destination container), \textit{multi-pick}, \textit{open}, or \textit{deconstruction} defect. Expert annotators verified the annotations for each activity and augmented the annotations for Transfer-images as multi-pick, or package-defect if a defect was observable, and as {nominal} if no defect was observable in the image. In addition to the defect type, we also provide segmentation polygons for the objects to enable development of models that can benefit from additional attention cues. For video annotations, expert annotators verified the type of package defect, i.e., \textit{open} and \textit{deconstruction}, observed in the video. In addition to the type of defect observed in each video, the index of the first frame where a defect becomes observable is also provided to enable development of real-time defect detection methods. 



\begin{figure}[t]
	\centering
	\includegraphics[width=0.5\textwidth]{images/defects.jpg} %defects/defects_02
	\caption{ Multi-view images in the defect detection dataset showing (a)--(c) \textit{package-defect} and (d)--(f) \textit{multi-pick} defect for different types of objects. }
	\label{fig:defects}
	\vspace{-0.1in}
\end{figure}

To establish a baseline for defect detection, we performed two experiments. In the first experiment, we train an image classifier with ResNet-50 \cite{DBLP:journals/corr/HeZRS15} backbone, global average pooling, and focal loss for predicting the type of defect observed in the Transfer-images. In the second experiment, we trained a multi-scale vision transformer model (MViT-B) \cite{fan2021MultiscaleVT} for action classification on videos. Since a defect can be introduced at any time during the manipulation process, we uniformly sampled 32 frames ($\sim$5\,FPS) from each video for training. %The sampled frames are cropped given an universal region of interest across all videos and resized to 224x224 resolution. 
The classification head outputs a two-channel vector that predicts binary classification on two categories: \textit{open} and \textit{deconstruction}. %The model is pretrained on  Kinetics-400 dataset \cite{}, and finetuned on our defect detection dataset. 
%For both experiments, we downsampled the nominal category to 1x of the number of samples in the defect category to compensate for class imbalance. 
We used a train-test split of 0.7:0.3 for multi-pick and package-defects. The nominal category in the train set was downsampled to match the size of the defect category to compensate for class imbalance. 10,000 samples from the nominal category were added to our test set. 

% Metrics and evaluation
Table\ \ref{tab:defect_image_baseline} and \ref{tab:defect_video_baseline} show performance of baseline models for defect detection on images and videos. We used recall and false positive rate (fpr) as metrics to evaluate performance over defect classes. A missed defect (lower recall) is more expensive than classifying a nominal activity as defective (fpr). Results for image defect detection shows that multi-picks are a harder to detect than package-defects. On the other hand, results for video defect detection show that open defects are harder to detect than deconstruction. There is significant scope for improvement in defect detection methods to be effective in warehouses operations which typically require high recall ($>$0.95) and low fpr ($<$0.01). 
% For defect detection using multi-view images, we evaluate models for single-view metrics ($P_{si}$, $FNR_{si}$, $FPR_{si}$) and multi-view metrics ($P_{mi}$, $FNR_{mi}$, $FPR_{mi}$).


%\noindent\textbf{Video Classification Baseline}
%Three baselines are performed for the video classification task: 1) R(2+1)D, 2) SlowFast and 3) MViT-B. ResNet-50 is used as the backbone of the first two CNN-based models. Since an object can be damaged at any stage of the manipulation process, we uniformly sample 32 frames ($\sim$5 FPS) from each video for training. The sampled frames are cropped given an universal RoI across all videos and resized to 224x224 resolution. The classfication head outputs a two-channel vector that does binary classification on the two categories: \textit{open} and \textit{deconstruction}. All the baseline models are pretrained on Kinetics-400 dataset, and finetuned on the proposed dataset for 50 epochs. For SlowFast network, we set the default speed ratio $\alpha = 4$ and channel ratio $\beta = 8$. During testing, two strategies are performed. The first one is the same as training, i.e. uniformly sample 32 frames from each video and make one prediction for each video. The second strategy is to make multiple predictions on shorter video clips (views). If defections are found in any clip, then the whole video is predicted as defected. In practice, 32 frames are sampled from each $3s$ clip, where a clip is sampled every $1s$ (stride = 1). This strategy can also give us a rough idea of when the defection happens.





% \setlength{\tabcolsep}{6pt}
% \begin{table}[t]
% 	\centering
% 	\caption{Distribution of defect types in the dataset}
% 	\label{tab:defect_dataset}
% %	\begin{tabular}{c|c|c|c|c}
% %		\hline  
% %		\multirow{3}{*}{} & \multirow{2}{*}{Nominal} &  \multirow{2}{*}{Multipick}  & \multicolumn{2}{c}{Package defect} \\  \cline{4-5}
% %		&                   &  &   Open & Deconstruction \\  \cline{1-5}
% %		Video      &    100,000  &    -      &  XX &    XX  \\ \hline
% %		Image     &   100,000  &  7,813    &     \multicolumn{2}{c}{5,759}     \\ \hline		
% %	\end{tabular}
% 	\begin{tabular}{c|c|c|c}
% 	\hline  
% 	 & {nominal} &  {multi-pick}  & {package-defect} \\  \hline
% %	&                   &  &   Open & Deconstruction \\  \cline{1-5}
% 	Video      &    100,000  &    -    &    4,075  \\ 
% 	Image     &   100,000  &  7,813    &    11,490   \\ \hline		
% \end{tabular}
% \end{table}
% \setlength{\tabcolsep}{1.4pt}

\begin{table}[t]
	\centering
	\caption{Baseline for single-view image defect detection}
	\label{tab:defect_image_baseline}
	%\begin{tabular}{|c|c|c|c|c|}
	%	\hline 
	%	\multirow{2}{*}{Model} & \multirow{2}{*}{Model}  &  \multicolumn{3}{c|}{Metric} \\   \cline{3-5}
	%	 & &  FPR  & FNR  & Precision \\  \hline
	%	 Image &  ResNet-50 &  XX  & XX  & XX \\ \hline
	%	 Video & XX & XX & XX & XX \\ \hline
	%\end{tabular}
	\setlength{\tabcolsep}{3pt}
	\begin{tabular}{c|c||c|c||c}
	    \hline 
	    {model} & {metric}  &  {multi-pick}  & { package-defect} & combined \\
	    \hline  
		\multirow{3}{*}{ResNet-50 \cite{DBLP:journals/corr/HeZRS15}}   & count     &  7,813  & 11,490  & 19,303 \\& recall     &  0.34  & 0.73  & 0.57 \\ %\cline{2-6}
									                	               & fpr        &  0.05  & 0.05  & 0.05  \\  \hline
									                	             %  precision  &  0.54  & 0.81      \\ \hline
	\end{tabular}
\end{table}
\setlength{\tabcolsep}{1.4pt}

\begin{table}[t]
	\centering
	\caption{Baseline for video defect detection}
	\label{tab:defect_video_baseline}
	%\begin{tabular}{|c|c|c|c|c|}
	%	\hline 
	%	\multirow{2}{*}{Model} & \multirow{2}{*}{Model}  &  \multicolumn{3}{c|}{Metric} \\   \cline{3-5}
	%	 & &  FPR  & FNR  & Precision \\  \hline
	%	 Image &  ResNet-50 &  XX  & XX  & XX \\ \hline
	%	 Video & XX & XX & XX & XX \\ \hline
	%\end{tabular}
	\setlength{\tabcolsep}{3pt}
	\begin{tabular}{c|c||c|c||c}
	    \hline 
		model & metric  & open & deconstruction & combined \\ 
		\hline
		\multirow{3}{*}{MViT-B \cite{fan2021MultiscaleVT}}   
		& count     &  2,951  &     2,165  &   4,070 \\
		& recall    &  0.69   &     0.79   &   0.73 \\ %\cline{2-6}
	    & fpr 	    &  0.23   &     0.03   &   0.13 \\ \hline
									            	      %   &            &  0.21  &     0.63    \\ \hline
	\end{tabular}
	%\vspace{-0.15in}
\end{table}
\setlength{\tabcolsep}{1.4pt}


%At $T_1$ we have four viewpoints ($V_i, i \in [0, 4]$) of the objects. Damage defects are annotated for each object as well as for each viewpoint. Viewpoint specific annotations are provided by the expert annotator during post processing. Multiple viewpoints are useful to detecting defects as defects might not be visible from certain viewpoints. We provide instance masks for the object manipulated in images at $T_0$ and at $T_1$ for all viewpoints $V_i$. Fig.\  \ref{fig:damageExamples}a shows an object that is pristine condition at $T_0$ and is damaged at $T_1$. Fig.\  \ref{fig:damageExamples} shows multiple objects being picked by the system. The annotations provided by the technician and verified or corrected by the expert is used to generate annotations for videos provided for defects. 

%Multi-class annotations are provided for videos: 1) multi-pick 2) open and 3) deconstruction. For our video data, we also provide time stamps on when the defect started and ended. For multi-picks the start time is defined by the when mutiple objects are observable. This typically happens when the robot is above the source tote. The end time for multi-pick is typically at the end of the manipulation process when multiple objects are dropped in the destination. For open packages, the start time is defined by when the object packaging opens and the end time is when the object is placed in the destination container. For deconstruction damages, the start time is defined as the time when a part of the object separates from the part of the object held by the end-effector. The end time is defined by when separated part is static in the scene. 


