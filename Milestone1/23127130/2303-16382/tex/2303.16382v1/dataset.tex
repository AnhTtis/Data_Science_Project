The ARMBench dataset presents: 1) a collection of sensor data acquired by a robotic manipulation workcell performing pick-and-place operation, 2) metadata and reference images for objects in containers, 3) a set of annotations acquired either automatically, by virtue of the system design, or via manual labeling, and 4) tasks and metrics to benchmark perception algorithms for robotic manipulation. Fig.\ \ref{fig:contributions} illustrates the benchmark tasks and variety of objects captured in the dataset. The dataset captures diversity in objects with respect to Amazon product categories as well as physical characteristics such as size, shape, material, deformability, appearance, fragility, etc. 

The data collection platform is a robotic manipulation workcell performing pick-and-place operation in a warehouse \cite{Sparrow2022}. The workcell contains a robotic arm mounted with a vacuum-based end-effector. It is presented with a heterogeneous collection of objects placed in unstructured configurations within a container (storage tote). The robotic arm is tasked with picking one object at a time (singulation) and place it on moving trays until the container is empty. The empty container ejects the workcell and is replaced by a new container. While the operation is completely autonomous, it includes a human-in-the-loop to monitor the status of each pick-and-place activity, annotate, and resolve any defects during manipulation. Multiple imaging sensors are placed in the workcell to facilitate and validate the pick-and-place operation. Following is a list of sensor data (Fig.\ \ref{fig:intro}) associated with each pick activity:
\begin{itemize}
\item Pick-image: A 5\,MP camera is used to capture a top-down image of the container.
% \item Pick-3D: Two Ensenso sensors capture the 3D point cloud of the source container.
\item Transfer-images: Multiple 5\,MP cameras are placed on different sides in the workcell to capture the moving object from different viewpoints.
% \item Transfer-Barcode: Multiple Cognex barcode sensors are used to scan the barcode of the object during transfer.
\item Place-image: A top-down view of the object is captured once it is placed on the tray.
\item Video: A camera is mounted to capture 720p videos of pick-and-place manipulation processes at 30\,FPS
\end{itemize}
Additionally, the following metadata (Fig.\ \ref{fig:contributions} (b)) is available by virtue of a warehouse tracking system:
\begin{itemize}
\item Container-manifest: A list of objects present in the container along with data such as product description, coarse dimensions, and weight.
\item Reference images: One or more images of objects from previous operations within the warehouse.
\end{itemize}
The sensor data and metadata were consumed by perception algorithms required to autonomously operate the robotic workcell. Benchmarking against these algorithms would not only optimize a manipulation task such as the one used for data collection but also enable more complex and intentional manipulation. This work considers a subset of such perception tasks namely object segmentation, object identification, and defect detection. These are critical not only to make informed grasping and motion decisions but also to track the state of the objects and containers within the warehouse. The following sections will describe these tasks and present the challenges using annotations, baseline algorithms, and evaluation metrics.