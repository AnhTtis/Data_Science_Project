


Object identification (ID) is the task of exactly identifying an image segment as one of the objects within a database. In the robotic manipulation context, this task is applicable both before and after picking the object. In the pre-pick stage, identifying an object segment within the tote allows accessing any stored models or attributes of the object from past experience which can be used for manipulation planning purposes. In the post-pick stage, ID has access to the segment of the object being manipulated both within the tote as well as when it is attached to the robotic arm. Accurately identifying which object is being transferred from one container to another is critical to tracking the object within a warehouse, thereby maintaining a container-manifest. This also allows posing ID as an image retrieval challenge. The pick and transfer images with segments of the target object (acquired using an instance segmentation algorithm) are treated as {\it query images}. The reference images for all objects in the container manifest are treated as {\it gallery} images. The challenge is to compare the {\it query images} to {\it gallery images} to find a match.

The benchmark dataset for this task contains 235K+ labeled pick activities corresponding to 190K+ unique objects. Each pick activity comprises one query image from the pick scene and up to three query images from the transfer phase. A ground-truth ID annotation is automatically acquired using multiple barcode scanners that are placed in the workcell. In cases where no barcode is scanned, a human operator manually scans the barcode of the object after it ejects the workcell. Pick activities are accompanied by a set of reference images for objects in the container manifest. These images are captures of the object from previous operations within the warehouse. While up to six reference images are sampled per object, reference images are not available for some objects. Such cases are representative of scenarios when a new object is introduced into the warehouse. To tackle such cases, an ID algorithm needs to model some notion of confidence and prevent false-positive prediction. A {\it test} set is sampled for evaluating baseline algorithms on the dataset. This set contains 50,000 pick activities where at least one reference image is available for the picked object. Another set ({\it test-uncertainty}) is derived from the {\it test} set by ignoring reference images of the picked object for 20\% of the cases. This set is used to evaluate the behavior of ID on novel objects coming into the warehouse.

\setlength{\tabcolsep}{6pt}
\begin{table}[h]
	\centering
	\caption{Evaluating Top-k Object Retrieval}
	\label{table:id_results}
	\begin{tabular}{c|c|c|c}
		\hline
		recall@k (pre/post-pick) & k=1 & k=2 & k=3\\
		\hline
		ResNet50-RMAC \cite{tolias2015particular} & 71.7 / 72.2 & 81.9 / 82.9 & 87.2 / 88.2\\
		DINO-ViTS \cite{caron2021emerging} & 77.2 / 79.5 & 87.3 / 89.4 & 91.6 / 93.5\\
		\hline
		\multicolumn{4}{c}{test-uncertainty} \\
		\hline
		ResNet50-RMAC \cite{tolias2015particular} & 57.7 / 58.0 & 65.7 / 66.5 & 70.2 / 70.7\\
		DINO-ViTS \cite{caron2021emerging} & 61.9 / 63.6 & 69.9 / 71.6 & 73.3 / 74.8\\
		\hline
	\end{tabular}
\end{table}
\setlength{\tabcolsep}{1.4pt}

 Table\ \ref{table:id_results} shows results of object retrieval with baseline algorithms on the two sets. For the first baseline, a 512d image descriptor is extracted from a ResNet50 backbone via aggregating features \cite{tolias2015particular} pre-trained for classification on ImageNet dataset \cite{russakovsky2015imagenet}. The second baseline utilizes a 384d feature vector pre-trained via self-supervision \cite{caron2021emerging} on a vision transformer. A cosine similarity is computed between feature embeddings for {\it query} and {\it gallery} images to get the closest match. Evaluation is performed both over pre-pick (pick image only) and post-pick (pick and transfer images) scenarios. Although transfer images are significantly different in terms of presentation to reference images, they provide multiple views of the object which improves the overall retrieval rate. Fig.\ \ref{fig:id-cases} shows some of the challenges associated with ID on this dataset. Large variations in appearance for the same object and the similarities between different objects makes the dataset challenging. The challenge increases with the size of container manifest as seen in Fig.\ \ref{fig:id-plots}(left). Fig.\ \ref{fig:id-plots}(right) shows the precision-recall curve obtained based on a rank-ratio confidence metric computed as $(1-\frac{c_2}{c_1})$, where $c_1, c_2$ are softmax probabilities corresponding to the first and second ranked objects. The plot highlights recall rates at high precision values as mis-identifications can lead to costly scenarios, such as an object getting lost within the warehouse. While methods like contrastive learning over the training set can improve the top-1 retrieval rate, achieving a high recall rate within the precision constraints would require methods to perform uncertainty estimation and leverage additional modalities such as text and dimensions. 

\begin{figure}[t!]
	\includegraphics[width=\linewidth, keepaspectratio]{images/id-cases_c.pdf}
	\vspace{-0.2in}
	\caption{Challenging cases for Object Identification}
	\label{fig:id-cases}
%	\vspace{-0.2in}
\end{figure}

\begin{figure}[t]
	\includegraphics[width=\linewidth, keepaspectratio]{images/id-plots-new.pdf}
	\caption{Container manifest size (left) is indicative of the number of images that the algorithm needs to select from. The precision-recall curve (right) shows the need for a confidence model to prevent false-positive predictions.}
	\label{fig:id-plots}
\end{figure}
