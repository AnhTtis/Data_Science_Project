% !TEX TS-program = XeLaTeX
% !TEX encoding = UTF-8 Unicode
\documentclass[12pt,a4paper]{article}
\usepackage{localrus}
\usepackage{hyperref}
\usepackage[margin=20mm]{geometry}
\usepackage{enotez}
\DeclareInstance{enotez-list}{custom}{paragraph}
{
 heading = \section*{#1} ,
 notes-sep = \baselineskip ,
 format = \normalfont ,
 number = \enmark{#1}
}
\DeclareTranslation{Russian}{enotez-title}{Примечания}
%\usepackage{showkeys}
%\usepackage{refcheck}

\newcommand{\cnd}{\mskip 1mu|\mskip 1mu}


\begin{document}

\title{Последнее открытие Колмогорова?\\ (Колмогоров и алгоритмическая статистика)}
\author{Н.\,К.\,Верещагин\thanks{Московский государственный университет имени Ломоносова, Москва, Россия, Национальный исследовательский университет <<Высшая школа экономики>>, Москва, Россия. Статья была подготовлена в рамках Программы фундаментальных исследований ВШЭ}, А.\,Л.\,Семёнов\thanks{Московский государственный университет имени Ломоносова}, А.\,Шень\thanks{LIRMM, Univ Montpellier, CNRS, Montpellier, France. Supported by 
ANR-21-CE48-0023 FLITTLA grant.}}
\date{}
\maketitle

Последней темой математических занятий Колмогорова была область, известная теперь как \emph{алгоритмическая теория информации} (или \emph{колмогоровская сложность}) и во многом возникшая именно благодаря Колмогорову. Он не был единственным или даже первым человеком, которому пришла в голову естественная идея измерять количество информации в конечном объекте с помощью теории алгоритмов, определяя его как длину минимального описания. Рей Соломонов высказал эту идею на несколько лет раньше, в начале 1960-х годов\endnote{В феврале 1960 года Соломонов пишет в своём отчёте~\cite{1960solomonoff}:
\begin{quote}
\eng{Consider a very long sequence of symbols --- e.g., a passage of English text, or a long mathematical derivation. We shall consider such a sequence of symbols to be ``simple'' and have a high a priori probability, if there exists a very brief description of this sequence --- using, of course, some sort of stipulated description method. More exactly, if we use only the symbols $0$ and $1$ to express our description, we will assign the probability $2^{-N}$ to a sequence of symbols, if its shortest possible binary description contains $N$ digits.}
\end{quote}
Далее понятие ``binary description'' уточняется так:
\begin{quote}
\eng{Suppose that we have a general purpose digital computer $M_1$ $\langle\ldots\rangle$ 

Any finite string of $0$'s and $1$'s is an acceptable input to $M_1$. The output of $M_1$ (when it has an output) will be a (usually different) string of symbols, usually in an alphabet other than the binary. If the input string $S$ to machine $M_1$ gives output string $T$, we shall write
\[
M_1(S)=T.
\]
Under these conditions, we shall say that ``$S$ is a description of $T$ with respect to machine $M_1$''.  If $S$ is the shortest such description of $T$, and $S$ contains $N$ digits, then we will assign to the string $T$ the a priori probability $2^{-N}$.}
\end{quote}
В том же отчёте вводится понятие универсальной машины (которая может моделировать любую другую, если предварительно добавить ко входу подходящий префикс, и потому даёт кратчайшие --- с точностью до аддитивной константы --- описания). Но целью Соломонова было дать определение априорной вероятности, некоторого распределения на всех конечных объектах, и  тут возникает очевидная проблема: если сложить предлагаемые им величины $2^{-N}$ для кратчайших описаний объектов, сумма не обязана быть меньше $1$. Соломонов предлагает разные способы преодолеть эту трудность, но сам отмечает возникающие проблемы. В статье 1964 года~\cite{1964-1solomonoff} он пишет:

\begin{quote}
\eng{The author feels that the proposed systems are consistent and meaningful, but at the present time, this feeling is supported only by heuristic reasoning and several nonrigorous demonstrations. [p.~4]}
\end{quote}

И дальше в той же статье:
\begin{quote}
\eng{The author feels that Eq. (1)} [некоторый способ обойти трудности, связанные с тем, что длины кратчайших описаний не задают распределения вероятностей] \eng{is likely to be correct or almost correct, but that the methods of working with the problems of Sections 4.1 to 4.3 are \emph{more} likely to be correct than Eq.~(1). If Eq.~(1) is found to be meaningless, inconsistent, or somehow gives results that are intuitively unreasonable, then Eq.~(1) should be modified in ways that do not destroy the validity of the methods used in Sections 4.1 to 4.3 [p.~10]}
\end{quote}

Легко представить себе, какое впечатление такого рода рекомендации производили на людей, привыкших к математической строгости. Но сама идея априорной вероятности и её связи со сложностью (как длиной кратчайшего описания), предложенная Соломоновым, оказалась впоследствии ключевой в алгоритмической теории информации как с технической, так и с философской теории зрения. Соломонов отмечает принципиальную разницу между этими подходами:

\begin{quote}
\eng{Another objection to the method outlined is that Equation (1) uses only the ``minimal binary description'' of the sequences it analyzes. It would seem that if there are several different methods of describing a sequence, each of these methods should be given \emph{some} weight in determining the probability of that sequence. [p.~16]}
\end{quote}
}~\cite{1960solomonoff,1964-1solomonoff,1964-2solomonoff},
но работы Соломонова оставались почти не замеченными до публикаций Колмогорова (первая из них была в 1965 году), в которых Колмогоров на него сослался. Вероятно, помимо известности Колмогорова, тут важно было то, что у Колмогорова были отчётливые определения, доказательства и ясно изложенная мотивировка.

Основных  публикаций Колмогорова было две: статьи 1965 года~\cite{1965kolmogorov} и 1968--1969 года~\cite{1969kolmogorov} (английский вариант заметки, написанной на основе доклада, сделанного в сентябре 1967 года на симпозиуме по теории информации в Италии, вышел в 1968 году, а русский --- в 1969; на ту же тему Колмогоров делал доклад 31 октября 1967 года на заседании Московского математического общества\endnote{%
Вот его резюме, опубликованное в~\cite{1968umn}.
\begin{quote}
\textbf{Заседание 31 октября 1967~г.}

\textbf{1}. \textit{А.\,Н.\,Колмогоров} <<Несколько теорем об алгоритмической энтропии и алгоритмическом количестве информации>>.

Алгоритмический подход к основам теории информации и теории вероятностей в течение нескольких лет после его возникновения не получил большого развития, так как некоторые вопросы, возникшие в самом начале пути, оставались без ответа. Сейчас положение несколько изменилось. В частности, установлено, что разложение энтропии $H(x,y)\sim H(x)+H(y\cnd x)$ и формула $J(x\cnd y)\sim J(y\cnd x)$ верны в алгоритмической концепции лишь с точностью до членов порядка $O[\log H(x,y)]$ (Левин, Колмогоров).

Утверждавшееся ранее кардинальное отличие алгоритмического определения бернуллиевской последовательности (простейшего <<коллектива>>) от определения Мизеса -- Чёрча конкретизировано в виде теоремы: существуют бернуллиевские по Мизесу -- Чёрчу последовательности $x=(x_1,x_2,\ldots)$  с плотностью единиц $p=1/2$, начальные отрезки которых имеют энтропию (<<сложность>>) $H(x^n)=H(x_1,x_2,\ldots,x_n)=O(\log n)$ (Колмогоров). Для понимания доклада достаточно знакомства с понятием вычислимой функции на интуитивном, не формальном уровне.
\end{quote}
Здесь сложность называется энтропией и обозначается буквой $H$ вместо $K$ (а $J$ обозначает взаимную информацию). 
}). Помимо этого, был опубликован ещё доклад в Ницце на математическом конгрессе 1970 года~\cite{1970kolmogorov} --- но с большой задержкой, только в 1983 году. Публикация по материалам доклада Колмогорова и Успенского на конгрессе общества Бернулли (1987,~\cite{1987kolmogorov}) была подготовлена Успенским уже практически без участия Колмогорова (как отмечает сам Успенский) --- увы, в это время Колмогоров уже тяжело болел. Так что об идеях Колмогорова, не доведённых до стадии доказанных (и опубликованных) теорем, приходится судить по работам его учеников и сотрудников, коротким опубликованным резюме его докладов и по воспоминаниям их слушателей.

В этом обзоре мы попытаемся проследить историю развития идей Колмогорова, воплотившихся в три подхода к алгоритмической статистике (сложность с ограниченными ресурсами, структурная функция и стохастические объекты).

\section{Сложность с ограничениями на ресурсы}

В статье 1965 года~\cite{1965kolmogorov}, где даётся определение колмогоровской сложности конечного объекта как длины кратчайшей программы, его порождающей, и доказывается существование оптимального способа программирования, при котором сложность минимальна с точностью до константы, есть параграф 4, <<Заключительные замечания>>. В нём говорится:
\begin{quote}
Изложенная в \S3 концепция обладает одним существенным недостатком: она не учитывает <<трудности>> переработки программы $p$ и объекта $x$ в объект $y$. Введя надлежащие определения, можно доказать точно формулируемые математические предложения, которые законно интерпретировать как указание на существование таких случаев, когда объект, допускающий очень короткую программу, то есть обладающий очень малой сложностью $K(x)$, может быть восстановлен по коротким программам лишь в результате вычислений совершенно не реальной длительности. В другом месте я предполагаю изучить зависимость необходимой сложности программы $K^t(x)$ от допустимой трудности $t$ её переработки в объект~$x$. Сложность $K(x)$, которая была определена в \S3, появится при этом в качестве минимума $K^t(x)$ при снятии ограничений на величину~$t$.
\end{quote}

В этом абзаце по существу содержится определение сложности с ограниченными ресурсами (скажем, временем работы): $K^t(x)$ есть минимальная длина программы, порождающей объект $x$ за время не более~$t$. При увеличении $t$ эта величина уменьшается и с какого-то момента становится равной обычной колмогоровской сложности~$K(x)$.

Осторожно сформулированное замечание о том, чт\'{о} можно сделать и потом законно интерпретировать, <<введя надлежащие определения>>, тоже легко уточнить: какова бы ни была (быстро растущая)  вычислимая функция $t(n)$ с натуральными аргументами и значениями, для всех $n$ можно найти слово $x_n$ длины $n$, для которого $K^{t(n)}(x_n)\ge n$, но $K(x_n)\le O(\log n)$. В качестве такого слова можно взять, скажем, первое слово $x$ длины $n$, для которого $K^{t(n)}(x)\ge n$. (Такие слова существуют, потому что есть даже слова длины $n$, у которых сложность без ограничения времени не меньше $n$.) Это слово можно (пусть за очень большое время) вычислить, зная $n$ и моделируя работу всех программ длины до $n$ за время $t(n)$. Поэтому его сложность (без ограничения времени) не больше $O(\log n)$.

Сложность объекта --- это его числовая характеристика; если же мы рассматриваем сложность с ограничением на ресурсы, то для каждого $x$ получаем уже не число, а убывающую функцию $t\mapsto K^t(x)$, ту самую, которую Колмогоров предполагал изучить в другом месте. Но больше Колмогоров на эту тему ничего не опубликовал.\endnote{%
В популярном докладе 1965 года в институте философии АН СССР, стенограмма которого~\cite{1965kolmogorovtalk} сохранилась, Колмогоров говорил:
\begin{quote}
Можно показать (это будет также, в некоторой мере, развито в статье, которую я всё время упоминаю [Колмогоров предполагал опубликовать статью в \emph{Успехах математических наук}, но этот план остался нереализованным --- публикаторы предполагают, что он имел в виду~\cite{1970kolmogorov}, но это ошибка, этот текст предполагался для публикации в сборнике докладов математического конгресса в Ницце]), что заведомо могут быть случаи, когда, скажем, решение какой-нибудь просто поставленной задачи может быть дано в виде сравнительно короткой формулы, записанной символическими знаками. Но достоверно известно, что если вы при решении желаете избежать чрезвычайно большого объёма вычислительной работы, то такое решение может быть записано только значительно более длинно. Математика уже в состоянии такого рода теоремы доказывать. И, собственно говоря, такого рода теоремы должны заменить очень популярную в литературе теорему Гёделя.
\end{quote}
} Возможно, это было связано с двумя трудностями:

\begin{itemize}
\item Определение сложности с ограниченными ресурсами оказывается \emph{ма\-шин\-но-за\-ви\-си\-мым}, поскольку использованные ресурсы зависят от вычислительной модели и непонятно, какую выбрать.

\item Попытки что-либо доказать для сложности с ограничением на время (что кажется наиболее естественным и интересным подходом) немедленно наталкиваются на проблему перебора (равны ли классы P и NP).
\end{itemize}

Обе эти трудности можно обойти, если оценивать ресурсы очень грубо, настолько, что различия между моделями становятся неважным. В своём докладе на заседании Московского математического общества, судя по опубликованному резюмe\endnote{%
Приводим для удобства читателей полный текст этого резюме~\cite{1972umn}:
\begin{quote}
\textbf{Заседание 23 ноября 1971~г.}

\textbf{1}. \textit{А.\,Н.\,Колмогоров} <<Сложность задания и сложность построения математических объектов>>.

$1^\circ$. При организации машинных вычислений приходится иметь дело с оценками а) сложности программы, б) используемого объёма памяти, в) длительности вычисления. Доклад посвящён группе работ, где аналогичные понятия употребляются несколько более отвлечённым образом.

$2^\circ$. В 1964--1965 гг. было замечено, что минимальная длина $K(x)$ двоичной записи программы, задающей построение объекта $x$, может быть определена инвариантно с точностью до аддитивной константы (Соломонов, А.\,Н.\,Колмогоров). Это позволило сделать понятие \emph{сложности определения} конструктивных математических объектов исходным пунктом нового изложения основ теории информации (А.\,Н.\,Кол\-мо\-го\-ров, Левин) и теории вероятностей (А.\,Н.\,Колмогоров, Мар\-тин-Лёф, Шнорр, Левин).

$3^\circ$. Труднее освободить от связи с техническими особенностями специальных видов <<машин>> характеристики <<необходимого объёма памяти>>, или <<необходимой длительности работы>>. Но некоторые результаты можно извлечь уже из аксиоматической <<машинно-независимой>> теории широкого класса аналогичных характеристик (Блюм, 1967). Пусть $\Pi(p)$ --- некая характеристика <<сложности построения>> объекта $x=A(p)$ по программе $p$, а $\Lambda(p)$ обозначает длину программы $p$. Формула $K^n\Pi(x)=\inf\,\{\Lambda(p): x=A(p), \Pi(p)=n\}$ [вероятно, опечатка и имеется в виду $\Pi(p)\le n$] определяет <<$n$-сложность определения>> объекта $x$ (при невыполнимости условия нижняя грань считается бесконечной).

$4^\circ$. Теорема Бардзиня [здесь и далее опечатка: правильно <<Барздинь>>] о сложности $K(M_a)$ начальных отрезков перечислимого множества натуральных чисел (1968) и результаты Бардзиня, Кановича и Петри, относящиеся к соответствующим сложностям $K^n\Pi(M_a)$, имеют общематематический интерес, так как проливают некоторый новый свет на роль выходов за пределы ранее употребляемой формализации в развитии математики. Обзор состояния очерченной выше проблематики был сделан в форме, свободной от обременительного технического аппарата.

\end{quote}
}, Колмогоров указывал на такую возможность:

\begin{quote}
Труднее освободить от связи с техническими особенностями специальных видов <<машин>> характеристики <<необходимого объёма памяти>>, или <<необходимой длительности работы>>. Но некоторые результаты можно извлечь уже из аксиоматической <<машинно-независимой>> теории широкого класса аналогичных характеристик (Блюм, 1967). Пусть $\Pi(p)$ --- некая характеристика <<сложности построения>> объекта $x=A(p)$ по программе $p$, а $\Lambda(p)$ обозначает длину программы $p$. Формула $K^n\Pi(x)=\inf\,\{\Lambda(p): x=A(p), \Pi(p)=n\}$ [вероятно, опечатка и имеется в виду $\Pi(p)\le n$] определяет <<$n$-сложность определения>> объекта $x$ (при невыполнимости условия нижняя грань считается бесконечной).
\end{quote}

Но никаких подробностей там не приводится.

Что касается второго препятствия, то Колмогоров был одним из первых, кто оценил значение NP-полноты (и побудил Левина опубликовать работу на эту тему), и, конечно, понимал, что попытки перенесения результатов о сложности на случай ограниченных ресурсов немедленно с ней столкнутся.

{\emergencystretch=4mm
Как пишут Лонгпре и Мокас в своей статье 1993 года~\cite{1993longpre}, ``\eng{As Kolmogorov stated himself (Levin, Private communication), the problem of whether symmetry holds in time bounded environment has interesting connections to complexity theory}''. В этой статье они доказывают, что симметрия информации (формула для сложности пары) с ограничениями на время противоречит существованию односторонних функций (а также, следуя диссертации Лонгпре~\cite{1986longpre}, приводят доказательство симметрии информации с ограничениями на память).

}

Так или иначе, в  1980-е годы разные варианты сложности с ограниченными ресурсами вошли в обиход --- например, Сипсер~\cite{1983sipser} использовал их для доказательства того, что класс BPP содержится в полиномиальной иерархии (результат, впоследствии улучшенный Гачем до $\Sigma_2 \cap \Pi_2$, см. ту же статью Сипсера).

Ещё раньше, в 1970-е годы, на сочетание малой сложности и большой сложности с ограниченными ресурсами обратил внимание Беннет. В статье 1977 года~\cite{1977chaitin} Чейтин цитирует Беннета и пишет:

\begin{quote}
\eng{%
C.H.~Bennett [55] has suggested another approach [to give a quantitative structural characterization\ldots of degree of organization] based on the notion of ``logical depth''. A structure is deep ``if it is superficially random but subtly redundant, in other words, if almost all its algorithmic probability is contributed by slow-running programs. A string's logical depth should reflect the amount of computational work required to expose its buried redundancy''. It is Bennett's thesis that ``a priori the most probable explanation of `organized information' such as a sequence of bases in a naturally occurring DNA molecule is that it is the product of an extremely long evolutionary process'' $\langle\ldots\rangle$
We are grateful to C.H.~Bennett for permitting us to present his notion of logical depth in print for the first time\ldots}
\end{quote}
Здесь [55] --- ссылка на неопубликованную рукопись Беннета ``On the Thermodynamics of Computation'' (без даты).

 В 1979 году Адлеман~\cite{1979adleman} приводит пример ситуации, когда сложность описания убывает, если мы разрешаем более длинные вычисления: число $2^{136}+1$ просто описывается и не требует большого времени для восстановления, случайное число такого же размера имеет длинное описание независимо от ограничений на время, в то время как число, являющееся \emph{наибольшим простым делителем $2^{136}+1$}, имеет короткое описание (приведённое только что), но его восстановление по этому описанию требует больших усилий (разложение на множители вычислительно сложное дело). 

В 1988 году уже сам Беннет пишет статью~\cite{1988bennett} в юбилейном сборнике в честь 50-летия универсальной машины Тьюринга. Там он пишет:\endnote{%
Приведённый фрагмент взят из резюме (Abstract), в самой статье Беннет пишет более подробно:
\begin{quote}
\eng{A string is said to be compressible by $k$ bits if its minimal program is $\ge k$ bits shorter than the string itself. A simple counting argument shows that at most a fraction $2^{-k}$ of strings of length $\le n$ bits can have this property. This fact justifies calling strings that are incompressible, or nearly so, algorithmically random. Like the majority of strings one might generate by coin tossing, such strings lack internal redundancy that could be exploited to encode them concisely $\langle\ldots\rangle$ 

The relation between universal computer programs and their outputs has long been regarded [40] as a formal analog of the relation between theory and observation in science, with the minimal-sized program representing the most economical, and therefore a priori most plausible, explanation of its output. This analogy draws its authority from the ability of universal computers to execute all formal deductive processes and their presumed ability to simulate all processes of physical causation. Accepting this analogy, one is then led to accept the execution of the minimal program as representing its output’s most plausible causal history, and a logically ``deep'' or complex object would then be one whose most plausible origin, via an effective process, entails a lengthy computation. Just as the plausibility a scientific theory depends on the economy of its assumptions, not on the length of the deductive path connecting them with observed phenomena, so a slow execution time is not evidence against the plausibility of a program; rather, if there are no comparably concise programs to compute the same output quickly, it is evidence of the nontriviality of that output.

A more careful definition of depth should not depend only on the minimal program, but should take fair account of all programs that compute the given output, for example giving two $k+1$ bit programs the same weight as one $k$-bit program.}
\end{quote}

Здесь [40] --- это работа Соломонова 1964 года~\cite{1964-1solomonoff}.}

\begin{quote}
\eng{Some mathematical and natural objects (a random sequence, a sequence of zeros, a perfect crystal, a gas) are intuitively trivial, while others (e.g. the human body, the digits of $\pi$) contain internal evidence of a nontrivial causal history. We formalize this distinction by defining an object’s ``logical depth'' as the time required by a standard universal Turing machine to generate it from an input that is algorithmically random\ldots}
\end{quote}
Поиски адекватной формализации Беннет начинает с такого варианта определения \emph{глубины} (depth):
\begin{quote}
\eng{%
\textbf{Tentative Definition 0.1}: A string’s depth might be defined as the execution time of its minimal program.

The difficulty with this definition arises in cases where the minimal program is only a few bits smaller than some much faster program, such as a print program, to compute the same output $x$. In this case, slight changes in $x$ may induce arbitrarily large changes in the run time of the minimal program, by changing which of the two competing programs is minimal. Analogous instability manifests itself in translating programs from one universal machine to another.}
\end{quote}

Более правильно, пишет Беннет, рассматривать зависимость между временем работы и длиной программы (ту самую зависимость, о которой говорил Колмогоров):
\begin{quote}
\eng{%
\textbf{Tentative Definition 0.2}: A string’s depth at significance level $s$ be defined as the time required to compute the string by a program no more than $s$ bits larger than the minimal program.}
\end{quote}
(В дальнейшем Беннет модифицирует это определение,  но мы сейчас не будем в это углубляться --- как и в его определения глубины для бесконечных последовательностей.)

Аналогичные определения  появляется и в более поздних работах --- например, в~\cite{2006AFMV} определяется \eng{computational depth} (независимой переменной считается время; колмогоровская сложность обозначается буквой $C$. а $C^t$ обозначает сложность с ограничением $t$ на время работы программы): 
\begin{quote}
\textbf{Definition 4.1} Let $t$ be a time bound. The time-$t$ depth of $x$ is 
\[
D^t(x) = C^t(x) - C(x).
\]
\end{quote}

Вернёмся к вопросу о зависимости сложности с ограничением на ресурсы от выбранной вычислительной модели. Эта зависимость оказывается несущественной на <<астрономически больших>> временах, когда мы измеряем время (или память, это уже не важно) с точностью до любого вычислимого преобразования.  Следуя~\cite[Section 2.2]{1983gacs}, мы будем рассматривать функцию $B(n)$, определённую как максимальное число сложности не больше $n$. Другой вариант определения, эквивалентный с точностью до замены аргумента на $O(1)$: наибольшее время работы интерпретатора оптимального языка программирования на входах длины не больше~$n$. (Аналогичное определение с числом состояний машин Тьюринга традиционно называют <<busy beaver function>>.)

Теперь можно для данного конечного объекта $x$ рассмотреть функцию 
\[
k\mapsto K^{B(k)} (x)
\] 
Она убывает с ростом $k$ и заведомо перестаёт меняться около $K(x)$ (потому что время работы минимальной программы для $x$ не превосходит $B(K(x))$. Как выяснилось, та же (с логарифмической точностью) функция появляется и в двух других ситуациях --- для двухчастных описаний и для стохастичности, тоже впервые рассмотренных Колмогоровым. К ним мы сейчас и перейдём.

\section{Двухчастные описания и\\ структурная функция Колмогорова}
\label{sec:structure}

Будем бросать несимметричную монету, у которой одна сторона выпадает, скажем, вдвое чаще другой. Запишем результаты в виде последовательности из $n$ нулей и единиц. Почти наверняка (при большом $n$) в этой последовательности будет около трети нулей и двух третей единиц. (Мы считаем, что единица выпадает чаще.) Сложность этой последовательности будет существенно меньше $n$ (в отличие от случая симметричной монеты). Эту последовательность можно задать с помощью такого двухчастного описания:
\begin{itemize}
\item сначала указать число нулей и единиц;
\item затем указать порядковый номер этой последовательности среди всех последовательностей с таким числом нулей и единиц.
\end{itemize}
Первая часть требует $O(\log n)$, а вторая часть --- примерно $C_{n}^{n/3}$ битов, что близко к $nH(1/3)$, где $H$ обозначает функцию Шеннона
\[
H(p) = p \log\frac{1}{p}+ (1-p)\log\frac{1}{1-p}
\]
(так что первая часть мала по сравнению со второй). С большой вероятностью (по описанному распределению) такое описание результата нашего эксперимента будет близким к оптимальному, и колмогоровская сложность результата будет близка к $nH(1/3)$.

Вообще, если в результате эксперимента получен некоторый объект $x$ (будем считать его двоичным словом), который содержится в некотором конечном множестве $A$, то можно рассмотреть \emph{двухчастное описание} объекта~$x$.
\begin{itemize}
\item Первая часть состоит из описания конечного множества $A$, для чего требуется $K(A)$ битов (если использовать оптимальный способ описания);
\item Вторая часть состоит из порядкового номера $x$ в множестве $A$, для чего требуется $\log_2 |A|$ битов.
\end{itemize}
Необходимые уточнения: конечное множество $A$ можно закодировать каким-то естественным способом в виде двоичного слова (по которому можно выписать все элементы $A$, скажем, в виде списка всех его элементов с разделителями), и говоря о сложности $K(A)$, мы имеем в виду сложность этого слова (выбор способа кодирования меняет её не более чем на $O(1)$-слагаемое). Говоря о порядковом номере, мы имеем в виду номер в каком-то естественном порядке (скажем, в лексикографическом).

Насколько это двухчастное описание близко к оптимальному --- другими словами, насколько сумма $K(A)+\log_2 |A|$ превышает $K(x)$? (Меньше эта сумма быть не может --- на то оптимальное описание и оптимально.) Это зависит от выбора $A$ и от $x$. В нашем примере с большой вероятностью (по бернуллиеву распределению, соответствующему нашей несимметричной монете) мы получим описание, близкое к оптимальному. Эту близость можно интерпретировать так: наше множество $A$ (состоящее из всех последовательностей данной длины с данным числом единиц) представляет собой хорошую модель для результатов эксперимента. А если мы выберем другое множество $A$, скажем, состоящее из всех последовательностей длины $n$, то двухчастное описание будет иметь $O(\log n)+n$ (длина второй части уже $n$), поэтому такое множество $A$ --- плохая модель, она не улавливает некоторые закономерности в экспериментальных данных.

С другой стороны, можно рассмотреть в качестве модели множество $A$, которое состоит из единственного элемента $x$. Тогда его сложность будет такой же, как у $x$, зато $\log_2 |A|=0$, так что двухчастное описание будет оптимальным. Но его недостатком будет <<чрезмерная конкретность>> --- в модель будет включено то, что на самом деле является случайным обстоятельством. Желательно этого избегать --- другими словами, из оптимальных двухчастных описаний выбирать те, у которых первая часть мала (а вторая, соответственно, велика).

Вот как Колмогоров излагал этот подход в своём докладе на заседании Московского математического общества 16~апреля 1974~года (согласно опубликованному резюме, см.~\cite{1974umn}:

\begin{quote}

\noindent
\textbf{Заседание 16 апреля 1974 г.}

\textbf{1}. \textit{А.\,Н.\,Колмогоров} <<Сложность алгоритмов и объективное определение случайности>>.

Любому конструктивному объекту $x$ можно поставить в соответствие функцию $\Phi_x(k)$ от натурального числа $k$ --- логарифм минимума мощности содержащего элемент $x$ множества, допускающего определение сложности не более~$k$. Если сам элемент $x$ допускает простое определение, то функция $\Phi$ принимает значение единица [опечатка: следует читать <<ноль>>] уже при небольших $k$. Если такого простого определения нет, элемент в негативном смысле <<случаен>>. Но он <<позитивно вероятностно случаен>> лишь в случае, если функция $\Phi$, получив при сравнительно небольшом значении $k=k_0$ значение $\Phi_0$, далее меняется приблизительно по закону $\Phi(k)=\Phi_0-(k-k_0)$.

\end{quote}

Другими словами,  <<позитивно вероятностно случайны>> те объекты, для которых существует близкое к оптимальному двухчастное описание со сравнительно небольшой первой частью.  (В самом деле, функция $\Phi_x(k)$ достигает нуля, когда множество одноэлементно, и тогда $k=K(x)$, так что $\Phi_0+k_0=K(x)$ для <<позитивно вероятностно случайных>> $x$.)

По-видимому, тот же подход излагался Колмогоровым в его докладе на международной конференции в Таллине в 1973 году. 
Сохранилась фотография Колмогорова, делающего доклад, сделанная Файном~\cite{1973fine};
\begin{center}
\includegraphics[width=0.9\textwidth]{Kolm_complexity_lect.jpg}
\end{center}
видно, что на доске написано то же определение (только вместо $\Phi$ написано $H$).

К сожалению, содержание этого доклада известно только по пересказам и этой фотографии.\endnote{Сохранилась (в коллекции Дынкина~\cite{1973dynkin}) аудиозапись лекции Колмогорова на какой-то международной конференции --- возможно, это первая часть его выступления в Таллине, но там изложение довольно подробное и медленное,  до определения функции $H$ дело в записанной части не доходит, так что трудно судить, действительно ли это лекция в Таллине. (Во всяком случае, это не последнее выступление, отражённое в~\cite{1983kolmogorovconf}, там переводчик чередовался с Колмогоровым.)}

Вот как излагается определение Колмогорова в~\cite[p.~32]{1985cover}:\endnote{%
Вот ещё один пересказ того же доклада~\cite[p.~175--176]{1991CT}, 
\begin{quote}
\eng{%
We begin with a definition of the smallest set containing  $x^n$ that is describable in no more than $k$ bits.

\textit{\textbf{Definition}}. The \emph{Kolmogorov structure function} $K_k(x^n\cnd n)$ of a binary string $x\in \{0,1\}^n$ is defined as
\[
K_k(x^n\cnd n)=\min_{p: l(p)\le k;\  \mathcal{U}(p,n)=S,\  x^n\in S\subseteq\{0,1\}^n} \log |S| \eqno (7.104)
\]
The set $S$ is the smallest set which can be described with no more than $k$ bits and which includes $x^n$. By $\mathcal{U}(p,n)=S$, we mean that running the program $p$ with data $n$ on the universal computer $\mathcal{U}$ will print out the indicator function of the set $S$.

\textit{\textbf{Definition}}.  For a given small constant $c$, let $k^*$ be the least $k$ such that
\[
K_k(x^n\cnd n) + k \le K(x^n\cnd n)+c\eqno(7.105)
\]
Let $S^{**}$ be the corresponding set and let $p^{**}$ be the program that prints out the indicator function of $S^{**}$. Then we shall say that $p^{**}$ is a \emph{Kolmogorov minimal sufficient statistic} for $x^n$.

\begin{center}
$\langle\ldots\rangle$
\end{center}
The concept of the Kolmogorov structure function was defined by Kolmogorov at a talk at the Tallin[n] conference in 1973, but these results were not published. 
}
%V'yugin [267] has shown that there are some very strange sequences $x^n$ that reveal their structure arbitrary slowly in the sense that $K_k(x^n\cnd n)=n-k$, $k<K(x^n\cnd n)$. [опечатка?]
\end{quote}


%[267] --- это~\cite{1987vyugin}

% Ещё там пишут, что Zurek [606]--[608] addresses the fundamental questions of Maxwell’s demon and the second law of thermodynamics by establishing the physical consequences of Kolmogorov complexity.

%Zurek : [606] W. H. Zurek. Algorithmic randomness and physical entropy. Phys. Rev. A, 40:4731–4751, Oct. 15 1989. [607] W. H. Zurek. Thermodynamic cost of computation, algorithmic complexity and the information metric. Nature, 341(6238):119–124, Sept. 1989. [608] W. H. Zurek (Ed.) Complexity, Entropy and the Physics of Information (Proceedings of the 1988 Workshop on the Complexity, Entropy and the Physics of Information). Addison-Wesley, Reading, MA, 1990.
}%\endnote

\begin{quote}
\eng{%
Consider the function $H_k\colon\{0,1\}^n\to N$, 
\[
H_k(x)= \min_{p\colon l(p)\le k} \log |S|,
\]
where is minimum is taken over all subsets $S\subseteq \{0,1\}^n$, such that $x\in S$, $U(p)=S$, $l(p)\le k$. This definition was introduced by Kolmogorov in a talk at the Information Theory Symposium, Tallin[n], Estonia, in 1974. Thus $H_k(x)$ is the log of the size of the smallest set containing $x$ over all sets specifiable by a program of $k$ or fewer bits. Of special interest is the value
\[
k^*(x) = \min \{k\colon H_k(x)+k = K(x)\}.
\]
Note that $\log |S|$ is the maximal number of bits necessary to describe an arbitrary element $x\in S$. Thus a program for $x$ could be written in two stages: ``Use $p$ to print the indicator function for $S$; the desired sequence is the $i$th sequence is a lexicographical ordering of the elements of this set.'' This program has length $l(p)+\log|S|$, and $k^*(x)$ is the length of the shortest program for which this $2$-stage description is as short as the best $1$-stage description $p^*$. We observe that $x$ must be maximally random with respect to $S$ --- otherwise the $2$-stage description could be improved, contradicting the minimality of $K(x)$. Thus $k^*(x)$ and its associated program $p$ constitute a minimal sufficient description for $x$.
}%\eng
\end{quote}

Однако есть и другие изложения содержания доклада Колмогорова на той же конференции (см. ниже в разделе о стохастических объектах).
\smallskip

Двухчастные описания (в немного другой форме) рассматривались и другими авторами. Коппель так описывает этот подход в~\cite{1988koppel}:
\begin{quote}
\eng{%
\ldots 3. What is the sophistication of the string, that is, what is the minimal amount of planning which must have gone into the generation of the string? More picturesquely, if the string is being broadcast by some unknown source, what in the minimal amount of intelligence we must attribute to that source? [p.~435]

\ldots both simple strings and random strings are not sophisticated, in the sense explained in Question 3. $\langle\ldots\rangle$

The relationship between sophistication and complexity can be made more precise in the following way. One part is a description of the string's structure, and the other part specifies the string from among the class of strings sharing this structure (Cover 1985). The sophistication of a string in the size of that part of the description which describes the string's structure. Thus, for example, the description of the structure of a random string is empty and thus, though its complexity is high, ite sophistication is low.

$\langle \ldots \rangle$

Having defined complexity in terms of a two-part description, it is easy to sort out the ``sophistication'' form the complexity.

\textbf{Definition}. \emph{The $c$-sophistication of $S$}, 
\begin{multline*}
\textit{SOPH}_c(S) = \min\{|P|\ \mid\ \exists F\, \text{$(P,D)$ \emph{is a description of $S$}}\\ 
\text{\emph{and $|P|+|D|\le H(S)+c$}}\}.
\end{multline*}
 We call a description $(P,D)$ of $S$, \emph{$c$-minimal} if $|P|+|D|\le H(S)+c$. We call a program $P$ a \emph{$c$-minimal program} for $S$ if $P$ is the shortest program such that for some $D$, $(P,D)$ is a $c$-minimal description of $S$. Thus $\textit{SOPH}_c(S)$ is the length of a $c$-minimal program for~$S$.

The $c$-minimal program for $S$ is that part of the description of $S$ which compresses $S$ --- it represents the structure of $S$. The range of this program constitutes the class of strings which share the structure of $S$.
}%
\end{quote}
Здесь Cover 1985 --- процитированная выше работа~\cite{1985cover}. Пара $(P,D)$ состоит из программы $P$ и исходного данного $D$, при этом требуется, чтобы программа $P$ была <<тотальной>>, то есть останавливалась на всех $D$. Тогда можно рассмотреть конечное множество $A$ всех слов $P(D')$, где $D'$ имеет ту же длину, что и $D$ (``the range of this program''). Программа $P$ (вместе с двоичной записью длины $D$, которой мы пренебрегаем) задаёт множество $A$, а длина $D$ равна двоичному логарифму размера множества~$A$, так что мы приходим почти к тому же определению, что и Колмогоров. Техническая разница, помимо длины $D$, состоит в том, что Коппель считает, что всякое описание двоичного слова $S$ является описанием и всех его префиксов. Кроме того, Коппель рассматривает ту же кривую в других координатах: начав с допустимого превышения $c$ длины двухчастного описания над сложностью $S$ (обозначаемой $H(S)$), он смотрит, начиная с какой длины двухчастных описаний такое возможно. (Cм. также другие публикации Коппеля того времени:~\cite{1987koppel,1991koppelatlan}.)

Если описывать этот подход в совсем общих (чтобы не сказать <<философских>>, ср. обзор~\cite{1992li-vitanyi}) терминах, можно сказать так. Одна из задач науки --- построение моделей, которые позволяют описывать результаты наблюдейни и экспериментов. При этом важно, чтобы модель была бы существенно проще описываемых ей результатов: если в модели больше параметров, чем в экспериментальных данных, ей описываемых, то её ценность невелика (можно вспомнить Оккама с его бритвой и Маха\endnote{А заставшие советскую систему образования вспомнят <<Материализм и эмпириокритицизм>> Вл.~Ильина (В.\,И.\,Ульянова, Н.\,Ленина) с его (теперь) забавной руганью в адрес Маха:

\begin{quote}
Старая погудка, почтеннейший г. профессор! Это буквальное повторение Беркли, говорившего, что материя есть голый абстрактный символ. Но голеньким-то на самом деле ходит Эрнст Мах, ибо если он не признаёт, что <<чувственным содержанием>> является объективная, независимо от нас существующая, реальность, то у него остается одно <<голое абстрактное>> \emph{Я}, непременно большое и курсивом написанное \emph{Я} = <<сумасшедшее фортепиано, вообразившее, что оно одно существует на свете>>. Если <<чувственным содержанием>> наших ощущений не является внешний мир, то значит ничего не существует, кроме этого голенького \emph{Я}, занимающегося пустыми <<философскими>> вывертами. Глупое и бесплодное занятие!
\end{quote}
} c его принципом <<экономии мышления>>). Скажем, триумф небесной механики выражался в том, что зная небольшое число параметров (текущие координаты, скорости и массы планет и Солнца), можно с большой точностью описать огромный массив результатов наблюдений за несколько веков.

Возникает вопрос, как в этих терминах описать адекватные вероятностные модели. Пусть мы, наблюдая бросания монеты, пришли к выводу, что они описываются моделью независимых равновероятных испытаний. Это никак не позволяет коротко описать результат нашего эксперимента --- в чём же тогда наше достижение? Чем хороша вероятностная модель (равномерное распределение на множестве всех последовательностей нулей и единиц)? Подход с двухчастными описаниями отвечает на этот вопрос так. Во-первых, модель эта проста. Во-вторых, основанные на ней двухчастные описания практически всегда близки к оптимальным.

Второе, конечно, не может быть проверено. Пусть мы бросили монету и записали результаты. Мы не можем вычислить колмогоровскую сложность и сказать, что она близка к длине (функция колмогоровской сложности не вычислима). Но по крайней мере возможность дискредитации модели остаётся: если кто-то предъявит простое описание для последовательности результатов бросаний, он тем самым покажет, что модель плохая. (Тут можно вспомнить Поппера с его фальсифицируемостью.)

Техническое замечание: мы (как и Колмогоров) для простоты рассматривали конечные множества в качестве моделей (что соответствует равномерному распределению вероятностей на элементах этих конечных множеств). Но аналогичную теорию можно построить, заменив конечные множества вычислимыми распределениями вероятностей (или распределениями с рациональными значениями), тогда вместо длины двухчастного описания с помощью распределения $P$ надо рассматривать величину $K(P)+\log_2 (1/P(x))$ и смотреть, насколько она превосходит $K(x)$. Можно переходить от таких распределений к равномерным ценой логарифмических потерь в <<качестве>>, так что с обычной для алгоритмической теории информации логарифмической точностью разницей между распределениями и множествами (в качестве моделей) можно пренебречь.

Многие люди (с той или иной степенью математической отчётливости) предлагали рассматривать двухчастные описания. Риссанен формулирует ``minimum description length principle'', хотя и не вполне отчётливо, в своей статье 1978 года~\cite{1978rissanen},  более отчётливая формулировка, следующая идее Колмогорова, дана в статье 1999 года~\cite{1999rissanen}\endnote{%
Вот что пишет Риссанен о двухчастных описаниях в~\cite{1999rissanen}:
       \begin{quote}
\eng{First, a `summarizing property' of data may be formalized as a subset $A$ where the data belongs along with other sequences sharing this property. Hence, the property $A$ need not specify the sequence completely. We may now think of programs consisting of two parts, where the first part describes optimally the set $A$ with the number of bits given by the Kolmogorov complexity $K(A)$, and the second part merely describes $x^n$ in $A$ with about $\log |A|$ bits, $|A|$ denoting the number of elements in $A$. The sequence $x^n$ gets then described in $K(A)+\log|A|$ bits. We may now ask for a set $\hat{A}$ for which $K(\hat{A})$ is minimal subject to the constraint that for an increasing length sequence $x^n$, $K(\hat{A})+\log|\hat{A}|$ agrees with the Kolmogorov complexity $K(x^n)$ to within a constant not depending on $n$. The set $\hat{A}$, or its defining program, may be called Kolmogorov's \emph{minimal sufficient statistic} for the description of $x^n$. The bits describing $\hat{A}$ are then the `interesting' bits in the program (code) while the rest, about $\log|\hat{A}|$ in number, are non-informative noise bits.}
        \end{quote}
Тут важно, что первая часть задаёт список как конечный объект; если вместо этого рассматривать программу, перечисляющую $A$ (но не останавливающуюся, когда все элементы $A$ перечислены, ничего интересного не получится (см. обсуждение в~\cite{1999shen}).
}.  См. также обзор Грюнвальда~\cite{2004grunwald} и статьи~\cite{2000vitanyi-li,2000gao-li-vitanyi,2006vitanyi}.\endnote{%
Вот как Грюнвальд описывает \eng{``crude, two-part version of MDL principle (informally stated)'' (p.~11)
\begin{quote}
The best point hypothesis $H$ $\langle\ldots\rangle$ to explain the data $D$ is the one which minimizes the sum $L(H) + L(D\cnd H)$, where
\begin{itemize}
\item
$L(H)$ is the length, in bits, of the description of the hypothesis; and
\item
$L(D\cnd H)$ is the length, in bits, of the description of the data when encoded
with the help of the hypothesis
\end{itemize}

$\langle\ldots\rangle$

We can typically find a very complex point hypothesis (large $L(H)$) with a very good fit (small $L(D\cnd H)$). We can also typically find a very simple point hypothesis (small $L(H)$) with a rather bad fit (large $L(D\cnd H)$). The sum of two description lengths will be minimized at a hypothesis that is quite (but not too) `simple', with a good (but not perfect) fit.

\end{quote}
}%\eng
Технически это описание нуждается в поправке: минимальная сумма заведомо достигается для описания, сконцентрированного в экспериментальной точке (одноэлементного множества); мы хотим найти не сильно худшее (с точки зрения длины) двухчастное описание, в котором первая часть была бы простой.
}%\endnote

Наконец, ещё в 1968 году (то есть до Колмогорова) идею двухчастного описания высказывали Уоллес и Болтон\endnote{%
По крайней мере так можно интерпретировать их слова~\cite[p.~186]{1968wallaceboulton}
\begin{quote}
\ldots \eng{if the observed distribution of the $S$ given points is markedly non-uniform, the economics of Shannon's theorem can be realized in part by adding to the $S$ messages a further message which approximately describes the average density distribution of the $S$ points, and using this approximate non-uniform distribution as the basis for encoding the attribute messages. The message length needed to describe the average density distribution must be considered as adding to the lengths of the messages now used to encode the attributes of the things, because the receiver of the composite message has no \emph{a priori} knowledge of the distribution.}
\end{quote}
Здесь первая часть описания состоит в приближённом распределении (распространённом по независимости на несколько испытаний), а вторая часть соответствует логарифму вероятности описываемой точки по этому распределению.
} (см.~\cite{1968wallaceboulton}).


Теперь мы переходим к третьему подходу, предложенному Колмогоровым (стохастичности), а потом объясним, в каком смысле они все эквивалентны (дают одну и ту же кривую).

\section{Стохастичность}

В 1981 году Колмогоров, который незадолго до этого стал заведующим кафедры математической логики (ныне она называется кафедрой математической логики и теории алгоритмов), объявил семинар <<Сложность определений и сложность вычислений>> (который в разных формах продолжает существовать до сих пор, см.~\cite{2023kolmogorovseminar}) и сделал там несколько докладов.  Сохранились краткие заметки трёх его докладов: 28 октября и 26 ноября 1981 года, а также 14 октября 1982 года.\endnote{% 
Поскольку о математических интересах и занятиях Колмогорова в последние годы его жизни (а в 1981 году он уже был болен, плохо ходил и  видел, ему было трудно говорить, но он ещё мог выступать у доски) сохранилось мало информации, приведём (немного отредактированные) заметки, сделанные одним из авторов (А.Ш.) на этих докладах.
\bigskip

\textbf{28 октября 1981}

\medskip
Определим  
\[
H_B^k(x)=\min\{l(p) \mid B(p)=x \ \text{и сложность вычисления $B(p)$ не больше $k$}\}
\]

\begin{center}
\includegraphics[scale=1]{1981-10-28-drawing.png}
\end{center}

Пример убывания $H_B^k$ при возрастании $k$: интерполяция позволяет уменьшить объём таблицы за счёт возрастания времени работы [при получении значений из таблицы]

\medskip

\[
| I(x\colon y)-I(y\colon x) | = O(\log H(x,y))
\]

Логарифмов не надо бояться, так же как и констант. 

Зачем нужны другие виды алгоритмической энтропии?

[Другие темы для семинара:] 

Булева сложность умножения чисел и матриц (Карацуба, Тоом, Штрассен)

Хотим задавать функцию из $[-1,1]$ в $[-1,1]$ с точностью до $\varepsilon$, рассматриваем схемную сложность. Рассматриваем $A_r$ --- функции, допускающие аналитическое продолжение в эллипс, нижняя оценка получается с помощью $\varepsilon$-ёмкости (Колмогоров, Тихомиров). По большей части функции разных классов приближаются как самые худшие из них, но как найти индивидуальную функцию, которая приближается не лучше всех других?

Будем обращать булевые функции данной сложности, и смотреть максимальную сложность обратной функции (при данной сложности прямой). 

\bigskip

\textbf{26 ноября 1981}

\medskip

Пусть $x_1\ldots x_N$ --- последовательность нулей и единиц с $M$ единицами, тогда сложность не больше $NH(p)$, где $p=M/N$. Пусть она почти максимальна.

Конечный объект $\omega$ является $(\alpha,\beta)$-стохастичным (где $\alpha$ и $\beta$ --- некоторые параметры, малые по сравнению со сложностью $\omega$), если существует конечное множество $\Omega$, для которого
\begin{itemize}
\item $\omega \in  \Omega$;
\item $K(\Omega)\le \alpha$;
\item $K(\omega)\ge K(\Omega)-\beta$ [опечатка: должно быть  $\log |\Omega|-\beta$ в правой части]
\end{itemize}
Пусть $N(k,\alpha,\beta)$ --- количество $(\alpha,\beta)$-стохастических объектов сложности не больше $k$. Пусть, скажем, $\alpha,\beta = \varepsilon k $ или $\alpha,\beta = \sqrt{k}$. Что можно сказать про асимптотику $N(k,\alpha,\beta)/2^k$? Будет ли $N(k,\alpha,\beta)/2^k \to 0$, если  $k$ растёт и $\alpha/k, \beta/k \to 0$?

\bigskip
 
\textbf{14 октября 1982}

\medskip

Пусть $x_1\ldots x_n$ --- последовательность нулей и единиц длины $n$, в которой $m$ единиц. Максимальная сложность таких последовательностей примерно $nH(m/n)$, где $H$ --- шенноновская энтропия. Будем рассматривать стохастические последовательности, то есть те, у которых сложность не меньше этой оценки минус какое-то $\beta$.

Если мы произведём какую-то достаточно простую выборку из $n_1$ элементов, останется $n_2$ элементов ($n_1+n_2=n$), посчитаем единицы в выбранных и оставшихся, пусть будет $m_1$ и $m_2$. При естественных ограничениях ($n_1$ и $n_2$ не слишком малы) частоты $m_1/n_1$ и $m_2/n_2$ будут близки к $m/n$. В самом деле, сложность последовательно не больше $n_1H(m_1/n_1)+n_2H(m_2/n_2)$ плюс сложность выборки, а она должна быть близка к $nH(m/n)$ [остаётся воспользоваться выпуклостью]. Что будет для зависимого выбора? Восстановление на этой основе свойств цепей Маркова. Последовательности с данными частотами переходов. Какие будут стохастическими? Доказать вероятностные свойства цепей Маркова для таких последовательностей.

Пусть $X_1,\ldots,X_N$ --- конечная последовательность целых чисел с заданными средним и дисперсией. Будет ли последовательность максимальной сложности подчиняться закону Гаусса? Видимо, да: если с условиями
\[
\int_0^\infty f(x)\,dx=1, \quad \int_0^\infty x^2 f(x)\, dx =1
\]
максимизировать 
\[
\int_0^\infty f(x)\ln f(x),
\]
то решение будет иметь вид $ce^{-ax^2}$.

[См.~\cite{1987asarin,1987asarin2}].

\begin{center}
 $*$ \qquad $*$ \qquad $*$
\end{center}

Вероятно, эти доклады были практически последними математическими выступлениями Колмогорова. В промежутке между ними Колмогоров сделал доклад на советско-японском симпозиум по теории вероятностей и математической статистике (23--29 августа 1982 года), труды которого были изданы в 1983 году~\cite{1983proceedings}. В своём письме от 6 августа он пишет оргкомитету: 
\begin{quote}
Глубокоуважаемые коллеги!

Благодарю за приглашение сделать на симпозиуме доклад. Однако при предварительном обсуждении возможности моего доклада на Симпозиуме я не учел того обстоятельства, что рабочим языком симпозиума является только английский. Сам по себе такой порядок целесообразен и я не хочу, чтобы он нарушался ради меня. Поэтому мне приходится отклонить ваше предложение. Если несмотря на это вы хотите каким-либо способом отметить мое положение старейшего вероятностника, то можно, например, предоставить мне в начале конференции вступительное слово (15 минут на русском языке с 15-минутным английским переводом). В этом вступительном слове я помещу некоторое содержательное высказывание о тенденциях развития нашей науки. Во всяком случае, постараюсь постараюсь быть полезным симпозиуму. Приеду поездом 22 августа.
\end{quote}

Доклад Колмогорова был записан (видимо, запись не сохранилась) и для публикации был расшифрован, а точнее говоря, реконструирован (этим занимались А.\,К.\,Звонкин и А.\,Шень по просьбе А.\,А.\,Новикова) --- запись была крайне некачественная, голос Колмогорова переводчика был едва слышен (и голос переводчика немногим лучше), так что текст, особенно во второй части, скорее представляет собой реконструкцию взглядов Колмогорова по его докладам на семинаре по сложности определений и сложности вычислений и более ранним публикациям. Вот что там написано об определении случайного элемента множества~\cite{1983kolmogorovconf} ($A$ --- оптимальный способ описания, фиксируемый при определении условной сложности):
\begin{quote}
Now we can define the concept of a ``random'', or, to be more precise, $\Delta$-random object in a given finite set $M$ (here $\Delta$ is a number). Namely, we shall say that $X\in M$ [в тексте опечатка: знак принадлежности пропущен] is $\Delta$-random in $M$ if
\[
K_A(X\cnd M) \ge \log_2 |M| - \Delta,
\]
[в тексте опечатка: вместо $M$ стоит $Y$] where $|M|$ [в тексте опечатка: написано просто $M$] denotes the number of elements in $M$. We shall call \emph{random in $M$} the $\Delta$-random objects in $M$, $\Delta$ being comparatively small. Thus we receive the definition of a random finite object which can be regarded as a final one.
\end{quote}

В 1986 году состоялся первый конгресс всемирного обещства Бернулли (открылся 8 сентября 1986 года). Колмогоров уже не смог приехать, и на открытии конгресса В.\,А.\,Ус\-пенс\-кий зачитал его приветственное обращение к участникам~\cite{1987kolmogorovspeech}:

\begin{quote}
Глубокоуважаемые дамы и господа! Разрешите приветствовать вас в день открытия Конгресса.

Мне представляется знаменательным, что Общество, принявшее имя Бернулли, Общество, объединяющее специалистов лишь в одной отрасли математики --- теории вероятностей и математической статистики, сумело организовать собрание своих сочленов столь представительное, что оно сравнимо с международными математическими конгрессами. Но если задуматься, то этому, казалось бы, парадоксальному явлению можно найти своё объяснение.

Один из знаменитых членов семейства Бернулли, Якоб Бернулли, вошёл в историю науки многими своими достижениями. Но две его заслуги должны быть отмечены особо. Он является родоначальником теории вероятностей, получив в ней первый серьёзный результат, повсюду известный как теорема Бернулли. Но, кроме этого, не следует забывать, что он, по-существу [дефис в тексте], явился родоначальником и комбинаторики. Начала этой науки были использованы им при доказательстве своей теоремы, но он пошёл в области комбинаторики и значительно дальше, открыв, в частности, замечательную последовательность чисел, носящих его имя. Эти числа постоянно встречаются в научных исследованиях вплоть до нашего времени.

Все мы ощущаем, что одним из основных требований к математике, которые предъявляет ей наше время, является исследование очень сложных систем. А сложность, с одной стороны, очень тесно соприкасается со случайностью, а с другой --- требует в какой-то мере расширения и самой комбинаторики.

Всё это даёт надежду на то, что с течением времени Общество Бернулли будет всё более и более увеличивать своё влияние в математическом мире. Желаю участникам Конгресса всего наилучшего.
\end{quote}
После этого (на пленарном заседании Конгресса, в торжественной обстановке оперного театра, из оркестровой ямы) Успенский сделал доклад, авторами которого (и последующей статьи~\cite{1987kolmogorov}) были указаны Колмогоров и Успенский. Как пишет Успенский в предисловии статьи по материалам этого доклада, <<хотя основание содержание статьи (в особенности её первых двух глав) опирается на идеи и публикации А.\,Н.\,Колмогорова, первый автор (являющийся непосредственным учителем второго) не имел возможности ознакомиться с окончательным вариантом текста>>,  так что приведённое в этом докладе определение стохастичности скорее воспроизводит обсуждения на семинарах. 

}%\endnote

В первом из этих докладов Колмогоров, среди прочего, привёл определение сложности с ограничением на время (и иллюстрирующий его пример: добавление к математическим таблицам домашинного времени правил по интерполяции --- что было обычной практикой --- позволяло сократить объём таблиц за счёт усложнения пользования ими). 

Во втором докладе он привёл определение стохастического объекта (объекта, допускающего <<статистическую интерпретацию>>). Оно было впоследствии воспроизведено в~\cite[с.~1337]{1983shen}, где было показано существование нестохастических объектов и оценена их доля:
\begin{quote}
\textbf{Определение} (А.\,Н.\,Колмогоров). Пусть $\alpha,\beta$ --- натуральные числа. Число $x$ будем называть \emph{$(\alpha,\beta)$-стохастическим}, если существует такое конечное множество $A\subset \mathbb{N}$, что
\[
x\in A, \qquad K(A)\le \alpha, \quad K(x) \ge \log_2|A|-\beta;
\]
здесь через $|A|$ обозначено число элементов множества $A$
\end{quote}
(а через $K$ --- колмогоровская сложность).

Замысел этого определения понятен, если сравнить его с замечанием Колмогорова из статьи~\cite{1965kolmogorov}:
\begin{quote}
За пределами этой заметки остаётся и применение построений \S3 к новому обоснованию теории вероятностей. Грубо говоря, здесь дело идёт о следующем. Если конечное множество $M$ из очень большого числа элементов $N$ допускает определение при помощи программы длины пренебрежимо малой по сравнению с $\log_2N$, то почти все элементы $M$ имеют сложность $K(x)$, близкую к $\log_2N$. Элементы $x\in M$ этой сложности и рассматриваются как <<случайные>> элементы множества $M$.
\end{quote}
Для данного $x$ мы ищем простое множество $A$ (условие $K(A)\le\alpha$), в котором этот элемент <<случаен>> (условие $K(x)\ge \log_2 |A|-\beta$). 

Если само по себе множество $A$ имеет заметную сложность, то логично добавить $A$ к условию во второй части определения (оценке сложности $x$), как это делается в~\cite{1987kolmogorov}:

\begin{quote}
\ldots \emph{дефект случайности} элемента $y$ относительно $M$ есть, по определению, 
\[
d(y\cnd M)=\log_2 |M|- H(y\cnd M),
\]
где $|M|$ есть мощность множества $M$.   $\langle\ldots\rangle$

\ldots мы скажем, что $y\in M$ является $\Delta$-случайным относительно $M$, если $d(y\cnd M)\le \Delta$. Тогда достаточно случайные элементы множества $M$ могут быть определены как те, которые являются $\Delta$-случайными при достаточно малых $\Delta$. Именно такое определение было предложено в [35]. $\langle\ldots\rangle$

Возникает следующий естественный вопрос: существуют ли <<абсолютно неслучанйе объекты>>, то есть объекты, имеющие большой дефект случайности относительно всякого простого множества? \ldots Ответ оказался положительным: такие объекты существуют. $\langle\ldots\rangle$

Прокомментируем сказанное. Перед статистиком может стоять такая задача --- объяснить результат эксперимента как типический. Это значит предложить статистическую гипотезу, иными словами, включить результат эксперимента в такое множество возможных исходов, в котором полученный в действительности результат будет выглядеть как типический. Говоря в математических терминах, статистик, получив результат $y$, должен найти простое множество $A$, содержащее $y$ в качестве типического элемента.

\end{quote}

Здесь [35] --- текст доклада~\cite{1983kolmogorovconf} по нашей нумерации. Сложность обозначается буквой $H$ (а не $K$, как в многих других работах Колмогорова).

По-видимому, никто из участников колмогоровского семинара 1981--1982 годов не слышал более ранних выступлений Колмогорова и не видел резюме его доклада 1974 года --- поэтому никто и не спросил Колмогорова, как в его картине мира предложенное им определение стохастичности (видимо, ранее в его выступлениях и публикациях не появлявшееся) связано со структурной функцией. В опубликованных вскоре работах участников семинара~\cite{1983shen,1985vyugin,1987vyugin,1999vyugin} изучается определение $(\alpha,\beta)$-стохастичности, а о структурной функции ничего не говорится.

Странным образом некоторые участники конференции в Таллине 1973 года связывают сделанный там доклад с понятием стохастичности. В статье~\cite[page 858]{1989CGG} говорится:

\begin{quote}
\eng{Kolmogorov introduced (see [K462]) the quantity 
\[
d(x\cnd S) = \log |S| - K(x\cnd S)
\]
as the defect of randomness of a string $x$ with respect to set $S$. $\langle\ldots\rangle$

At a Tallin[n] conference in 1973, Kolmogorov proposed a variant of the function
\[
\delta_x(k) = \min_S\{d(x\cnd S)\colon K(S)<k, x\in S\},
\]
considering it an interesting characteristic of the object~$x$. If $x$ is some sequence of experimental results, then the set $S$ can be considered to be the extraction of those features in $x$ that point to nonrandom regularities. At the point $k^*(x)$ where the decreasing function $\delta_x(k)$ becomes zero (or less than some constant agreed on in advance), we can say that it is useless to explain $x$ in greater detail than by giving the set $S^*$ such that $d(x\cnd S^*)=\delta_x(k^*)$. Indeed, the added explanation would be as large as the number of extra bits it accounts for. The set $S^*$ expresses all the relevant structure in the sequence $x$, the remaining details of $x$ being conditionally maximally random. For example, $S^*$ would describe the Mona Lisa up to brush strokes, and $k^*$, the length of description of $S^*$, is the ``structural complexity of $x$''.

The set $S^*$ plays the role of a universal minimal sufficient statistics for $x$.}

\end{quote}
Здесь [K462] --- ссылка на~\cite{1983kolmogorovconf} в нашей нумерации (но явного определения дефекта в этой статье нет). Может быть, ``a variant of'' в этом описании отражает желание авторов обзора~\cite{1989CGG} использовать понятия, введённые Колмогоровым впоследствии?

Так или иначе возникает естественный вопрос: Колмогоров предложил два критерия для ситуации, в которой конечное множество слов $S$ является хорошей моделью для слова $x$:
\begin{itemize}
\item если двучастное описание, построенное на основе $S$, близко по длине к оптимальному, то есть мал <<дефект оптимальности>>
\[
\delta(x\cnd S) = K(S)+\log_2|S| - K(x);
\]
\item если мал дефект случайности
\[
d(x\cnd S) = \log_2 |S| - K(x\cnd S).
\]
\end{itemize}
Равносильны ли эти два критерия или нет? Несложно заметить (пользуясь формулой для сложности пары), что дефект оптимальности больше и превосходит дефект случайности на $K(S\cnd x)$ (с логарифмической точностью):
\[
\delta(x\cnd S) = d(x\cnd S)+K(S\cnd x)+O(\log n),
\]
если длина слова $x$ и сложность множества $S$ не превосходят $n$. Этой разницей можно было бы пренебречь, если ограничиваться (наиболее философски интересным) случаем, когда множество $S$ простое: тогда его условная сложность при известном $x$ тем более мала. Но в общем случае, как замечено в~\cite{2001gacs-tromp-vitanyi}, это различие существенно. Скажем, если взять независимо два случайных слова $x,y$ длины $n$, и рассмотреть множество $S=\{x,y\}$ как модель для одного из них, то $\delta(x\cnd S)$ примерно равно $n$ (поскольку сложность множества $S$ близка к $2n$, эта часть двухчастного описания будет основной), в то время как $d(x\cnd S)$ близко к нулю ($\log_2 |S|$ равен всего лишь~$1$).

Интереснее другой вопрос: пусть для некоторого $x$ удалось найти модель (множество $S$) с какими-то значениями $K(S)$ и $d(x\cnd S)$. Можно ли найти (возможно, другое) множество $S'$, у которого $K(S')$ почти не превосходит $K(S)$, а $\delta(x\cnd S')$ почти не превосходит $d(x\cnd S)$? Этот вопрос был упомянут как открытый в~\cite{2001gacs-tromp-vitanyi}, но вскоре Верещагин и Витаньи~\cite{2004vereshchaginvitanyi} показали, что это утверждение действительно верно, тем самым установив, что два подхода Колмогорова (со структурной функцией и с дефектом случайности) задают одну и ту же кривую (с логарифмической точностью). Более того, как выяснилось, та же самая кривая получается и из определения сложности с ограниченными ресурсами. Сейчас мы сформулируем более подробно эти результаты и опишем их историю.

\section{Равносильность трёх подходов}

Как оказалось, все три описанных выше подхода к алгоритмической статистике эквиваленты (с логарифмической точностью). Каждый из них позволяет сопоставить со словом (длины $n$) некоторую кривую на плоскости. Оказывается, что при подходящем выборе системы координат эти кривые совпадают с точностью $O(\log n)$. 

Повторим описания этих кривых в подходящей системе координат. Пусть $x$ --- слово длины~$n$.

\begin{itemize}
\item \textbf{Сложность с ограниченными ресусами}. 
Обозначим через $B(k)$ максимальное натуральное число, имеющее сложность не более $k$. Рассмотрим функцию
\[
k \mapsto K^{B(k)}(x) - K(x)
\]
(показывающую, насколько сложность с ограниченными ресурсами больше неограниченной). Эта же кривая в других координатах появляется в определении logical depth (computational depth).

\item \textbf{Структурная функция}.
Рассмотрим функцию
\[
k \mapsto k + H_k(x) - K(x), 
\]
где $H_k(x)$ --- структурная функция Колмогорова (которую Колмогоров также обозначал $\Phi_k$, см. раздел~\ref{sec:structure}). Другими словами, для каждого $k$ мы берём минимальный дефект оптимальности $\delta(x\cnd S)$ по всем множествам $S$ сложности не выше~$k$. Эта же кривая в других координатах появляется в определении sophistication.

\item\textbf{Стохастичность}.
Рассмотрим функцию
\[
k \mapsto \min \{l \colon \text{$x$ является $(k,l)$-стохастическим}\}.
\]
Другими словами, для каждого $k$ мы берём минимальный дефект случайности $d(x\cnd S)$ по всем множествам $S$ сложности не выше $k$.
\end{itemize}

\begin{center}
\fbox{\fbox{\parbox{0.75\textwidth}{%
\textbf{Основная теорема алгоритмической статистики}\\ Для любого слова $x$ длины $n$ три указанных функции совпадают с точностью $O(\log n)$}}}
\end{center}

Говоря о точности $O(\log n)$, мы допускаем изменения обеих координат (и аргумента, и значения). Более формально следовало бы рассмотреть надграфики этих функций (когда ордината больше значения функции в абсциссе) и сказать, что каждый из них находится в $O(\log n)$-окрестности любого другого.\endnote{%
Можно дать ещё одно описание этой кривой (с точностью до логарифмических слагаемых), уже не имеющее отношения к Колмогорову --- а восходящее к~\cite{2001gacs-tromp-vitanyi}: пусть $m\ge K(x)$; рассмотрим алгоритм, который (получив $m$) перечисляет всех слова сложности не больше $m$. Слово $x$ окажется среди них --- посмотрим, сколько слов появятся \emph{после} $x$ в этом перечислении. Пусть их примерно $2^s$, тогда отметим точку $(m-s,s)$ на кривой (что соответствует описанию сложности $m-s$ и размера $s$).}

Как мы уже говорили, эквивалентность второго и третьего подходов была доказана в работе~\cite{2004vereshchaginvitanyi}. Что касается первого, то его эквивалентность остальным доказывается в~\cite{2017abst}. По-видимому, это первая работа, в которой была установлена связь между сложностью с ограничением на ресурсы и другими подходами в указанной форме (совпадение двух кривых), хотя некоторые родственные результаты были и раньше --- в частности, уже в~\cite{1987koppel} утверждалось, что некоторые родственные числовые характеристики для бесконечных последовательностей (но не кривые) близки.\endnote{%
Для Колмогорова было принципиальным рассмотрение именно конечных объектов, которое могло бы лечь в основу обоснования теории вероятностей. Рассмотрение бесконечных объектов с этой точки зрения были, так сказать, <<архитектурными излишествами>> --- вот письмо В.\,А.\,Успенскому от 02.06.1983: 

\begin{quote}
Дорогой Владимир Андреевич!

Я, конечно, не могу иметь ничего против того, чтобы Вы с Шенем и Семёновым написали статью <<О различных алгоритмических определениях понятия ``бесконечная случайная последовательность''>>. Однако с публикацией статьи именно в Успехах имеется одно затруднение. Я не считаю задачу определения бесконечной случайной последовательности центральной задачей. Чтобы понять мое отношение к делу, посмотрите, пожалуйста,

1. \S2 моих <<Основных понятий теории вероятностей>>,

2. мою статью в сборнике <<Математика, ее метод и значение>>,

3. мою статью в Санкхиа <<О таблицах случайных чисел>> [\cite{1963kolmogorov} в нашей нумерации]

4. прилагаемый текст моего доклада в Ницце [\cite{1970kolmogorov} в нашей нумерации].

О бесконечных случайных последовательностях здесь говорится в \S 6 и 8 доклада в Ницце. Бесконечные случайные последовательности остаются красивым привеском, в котором нет большой необходимости. Так же обстоит дело и при традиционном изложении теории вероятностей. Различают два типа предельных теорем: предельные теоремы в <<схеме последовательностей>> и предельные теоремы в <<схеме серий>>. Вполне обоснованно считают схему серий более близкой к реальным потребностям.
\end{quote}

С тех пор применение понятий алгоритмической теории информации к бесконечным объектам достигло новых высот, прежде всего благодаря специалистам по теории рекурсии, которые установили удивительные связи с понятием сложности и случайности (скажем, обнаружили удивительное понятие $K$-тривиальных последовательностей в теории тьюринговых степений). Множество результатов такого рода можно найти в монографиях~\cite{2010downeyhirschfeldt,2012nies}. Но действительно они интересны с точки зрения теории рекурсии и <<чистой математики>> и вряд ли дают что-то непосредственно для обоснования теории вероятностей, хотя, скажем, алгоритмическая теория информации (в частности, работы Шнорра по определению случайности бесконечных последовательностей с помощью мартингалов) и была одним из источников вдохновения для игрового подхода к теории вероятностей~\cite{2001vovkshafer}.
}%\endnote
; см. также~\cite{2007antunesfortnow,2010bauwens}.

Что на эту тему знал и предполагал Колмогоров? Подозревал ли он, что три введённых им определения окажутся равносильными? Вряд ли сейчас найдутся какие-то свидетельства этого, но сам факт, что он предложил три разных естественных способа характеризовать <<статистические свойства>> конечного объекта некоторой кривой (в дополнении к числовой характеристике --- сложности), и они оказались эквивалентным, представляется замечательным достижением, которое вполне можно назвать <<последним открытием Колмогорова>>.

\begin{center}
$*$\qquad $*$ \qquad $*$
\end{center}

В этой статье мы старались проследить за развитием идей Колмогорова, и обзор дальнейших достижений алгоритмической статистики выходит за её рамки. Доказательства сформулированных утверждений об эквивалентности различных подходов, а также изложение некоторых дальнейших результатов можно найти в~\cite{2016vereshchaginshen} (а обсуждение без доказательств --- в~\cite{2015vereshchaginshen}). См. также последнюю главу в~\cite{2013SUV}.
\medskip

Авторы глубоко признательны Альберту Николаевичу Ширяеву, в частности, за большие усилия по сохранению памяти об Андрее Николаевиче Колмогорове, а также всем участникам семинаров кафедры математической логики и теории алгоритмов, LIF (Mar\-seille) и LIRMM (Montpellier), где обсуждались упомянутые в этом обзоре понятия и результаты. Огромное влияние на авторов оказали Владимир Андреевич Успенский (1930--2018) и Андрей Альбертович Мучник (1958--2007), о которых авторы помнят с благодарностью.

\printendnotes[custom]

\begin{thebibliography}{9}
\raggedright

\bibitem{1960solomonoff}
R.J.~Solomonoff, \emph{V-131. A preliminary report on a general theory of inductive inference}. February 4, 1960. Zator company, $140\frac{1}{2}$ Mount Auburn Street, Cambridge 28, Mass. Contract AF 49(638)-376. Air Force Office of Scientific Research, Air Research and Development Command, United States Air Force, Washington 25, D.C., available at~\url{http://raysolomonoff.com/publications/rayfeb60.pdf}

\bibitem{1963kolmogorov}
A.N.~Kolmogorov, On tables of random numbers, \emph{Sankhya. The Indian Journal of Statistic}, 1963, Series A, \textbf{25}(4), 369--376. \url{https://www.jstor.org/stable/25049284}. Reprinted in \emph{Theoretical Computer Science}, \textbf{207}, 387--395 (1998), \url{https://www.sciencedirect.com/science/article/pii/S0304397598000759}

\bibitem{1964-1solomonoff}
R.J.~Solomonoff, A Formal Theory of Inductive Inference. Part I, \emph{Information and control}, \textbf{7}(1), 1--22 (1964), \url{https://doi.org/10.1016/S0019-9958(64)90223-2}

\bibitem{1964-2solomonoff}
R.J.~Solomonoff, A Formal Theory of Inductive Inference. Part II, \emph{Information and control}, \textbf{7}(2), 224--254 (1964), \url{https://doi.org/10.1016/S0019-9958(64)90131-7}

\bibitem{1965kolmogorovtalk}
А.\,Н.\,Колмогоров. Стенограмма доклада <<Понятие ``информация'' и основы теории вероятностей>>. В книге: \emph{Колмогоров и кибернетика} под редакцией Д.\,А.\,Поспелова, Я.\,И.\,Фета. Новосибирск, ИВМиМГ СО РАН, 2001. -- 159 с. (Вопросы истории информатики. Выпуск 2), с.~118--142, \url{https://archive.org/details/kolmogorov-1965talk}

\bibitem{1965kolmogorov}
А.\,Н.\,Колмогоров, Три подхода к определению понятия <<количество информации>>, \emph{Проблемы передачи информации}, том 1, вып.~1, 3--7 (1965), \url{https://www.mathnet.ru/rus/ppi68}. (Поступила в редакцию 9 января 1965 года)
English version: Three approaches to the quantitative definition of information, International Journal  Comput. Math., \textbf{2}, 157--168.
% K320 в 1989CGG, английский перевод K494

%1968 K343 A few theorems on algorithmic entropy and algorithmic precision of information, UMN 23(2), 201
\bibitem{1968umn}
В Московском математическом обществе. Заседания Московского математического общества. \emph{Успехи математических наук}, XXIII, вып. 2 (140), 1968, март--апрель, с.~201, \url{https://www.mathnet.ru/rus/rm5615}


\bibitem{1968wallaceboulton}
S.C.~Wallace, D.M.~Boulton, An information measure for classification, \emph{Computer Journal}, \textbf{11}(2), 185--194, \url{https://academic.oup.com/comjnl/article/11/2/185/378628}.

\bibitem{1969kolmogorov}
А.\,Н.\,Колмогоров, К логическим основам теории информации и теории вероятностей, \emph{Проблемы передачи информации}, том 5, вып. 3, 3--7 (1969), \url{https://www.mathnet.ru/rus/ppi1805}. Английский вариант: Andrei N.~Kolmogorov, Logical Basis for Information Theory and Probability Theory, IEEE Transactions on information theory, IT-14,  no.~5, 662--664 (September 1968, received December 13, 1967, based on an invited lecture at the International Symposium on Information Theory, San Remo, Italy, September, 1967.)
% K354 в CGG, английский вариант K493

\bibitem{1970kolmogorov}
А.\,Н.\,Колмогоров. Комбинаторные основания теории информации и исчисления вероятностей. \emph{Успехи математических наук}, том 38, выпуск 4(232), июль-август 1983, 27--36, \url{https://www.mathnet.ru/rus/rm2940}. Оригинальная рукопись 1970 года: \url{https://archive.org/details/kolmogorov83-manuscript}
%K461

\bibitem{1972umn} 
В Московском математическом обществе. Заседания Московского математического общества, \emph{Успехи математических наук}, XXVII, выпуск 2, с.~159, \url{https://www.mathnet.ru/rus/rm5033}
%K384 

\bibitem{1973fine}
Terrence L.Fine, Photo of Kolmogorov giving a talk in Tallinn, \url{https://commons.wikimedia.org/wiki/File:Kolm_complexity_lect.jpg}. Description: ``taken at the 1973 Soviet Information Theory Symposium (may not be the exact title) held in Tallinn, Estonian SSR. Kolmogorov delivers his talk. A.M. Yaglom is also on the picture.''

\bibitem{1973dynkin}
Аудиозапись выступления Колмогорова из коллекции Е.\,Б.\,Дынкина (\url{https://ecommons.cornell.edu/handle/1813/17350}), там она упомянута как интервью Колмогорова Дынкину около 1975 года, но это не интервью, а выступление на международной конференции (на втором плане слышен перевод). Возможно, выступление в Таллине 1973~года. Вариант с субтитрами (неполными): \url{https://www.youtube.com/watch?v=GN519NP2JXI}

\bibitem{1974umn}
В Московском математическом обществе. Заседания Московского математического общества. \emph{Успехи математических наук}, XXIX, выпуск 4 (178), 1974, июль--август. \url{https://www.mathnet.ru/rus/rm7215}

\bibitem{1977chaitin}
Gregory Chaitin, Algorithmic information theory, \emph{IBM Journal of Research and Development}, \textbf{21}(4), 350--359 (1977), \url{https://dl.acm.org/doi/10.1147/rd.214.0350}

\bibitem{1978rissanen}
J.~Rissanen, Modeling by shortest data description, \emph{Automatica}, \textbf{14}(5), 465--471, \url{https://www.sciencedirect.com/science/article/abs/pii/0005109878900055}, see also \url{https://msol.people.uic.edu/ECE531/papers/Modeling%20By%20Shortest%20Data%20Description.pdf}.

\bibitem{1979adleman}
Leonard M.~Adleman, \emph{Time, Space and Randomness}, MIT report LCS/TM-131, April, 1979. Available at \url{https://people.cs.rutgers.edu/~allender/papers/adleman.time.space.randomness.pdf}

\bibitem{1983proceedings}
\emph{Probability Theory and Mathematical Statistics. Proceedings of the Fourth USSR--Japan Symposium, held in Tbilisi, USSR, August 23--29, 1982}, edited by Jurii V.~Prokhorov, Kiyosi Itô (Lecture Notes in Mathematics, v.~1021). Springer, 1983, \url{https://doi.org/10.1007/BFb0072896}. 

\bibitem{1983kolmogorovconf}
A.N.~Kolmogorov, On logical foundations of probability theory.  In~\cite{1983proceedings}, 1--5. (1983)
%K462

\bibitem{1983gacs}
P\'eter G\'acs, On the relation between descriptional complexity and algorithmic probability, \emph{Theoretical Computer Science}, \textbf{22}(1--2), 71--93, see \url{https://doi.org/10.1016/0304-3975(83)90139-1}.

\bibitem{1983sipser}
Michael Sipser, A complexity theoretic approach to randomness,  \emph{STOC'83: Proceedings of the 15th annual ACM symposium on Theory of computing}, 330--335 (December 1983), see \url{https://dl.acm.org/doi/10.1145/800061.808762}.

\bibitem{1983shen}
А.\,Х.\,Шень, Понятие $(\alpha,\beta)$-стохастичности по Колмогорову и его свойства. \emph{Доклады Академии наук СССР}, \textbf{271}(6), 1337--1340 (1983), \url{https://www.mathnet.ru/rus/dan9984}.

\bibitem{1985cover}
Thomas M.~Cover, Kolmogorov Complexity, Data Compression, and Inference. In: \emph{The Impact of Processing Techniques on Communications}, edited by J.K.~Skwirzhynski, Martinus Nijhoff Publishers, 1985, \url{https://isl.stanford.edu/~cover/papers/paper65.pdf}. 

\bibitem{1985vyugin}
В.\,В.\,Вьюгин, О нестохастических объектах, \emph{Проблемы передачи информации}, \textbf{21}, выпуск 2, 3--9 (1985), \url{https://www.mathnet.ru/rus/ppi979}.

\bibitem{1986longpre}
Luc Longpr\'e, \emph{Resource Bounded Kolmogorov Complexity, A Link Between Computational Complexity and Information Theory}, Ph.D thesis, TR 86-776, August 1986, Department of Computer Science, Cornell University, Ithaca, NY14853, see \url{https://ecommons.cornell.edu/handle/1813/6616}.

\bibitem{1987kolmogorovspeech}
А.\,Н.\,Колмогоров. Приветствие участникам Первого Всемирного конгресса Общества Бернулли (зачитанное В.\,А.\,Успенским). \emph{Теория вероятностей и её применения}, \textbf{32}(2), 218, \url{https://www.mathnet.ru/rus/tvp1408}.
%K474 Welcoming speech by Andrei Nikolaevich Kolmogorov to the participants of the First World Congress of the Bernoulli Society, Theory Probab. Appl. 32 200. 

\bibitem{1987kolmogorov}
А.\,Н.\,Колмогоров, В.\,А.\,Успенский, Алгоритмы и случайность, \emph{Теория вероятностей и её применения}, том 32, выпуск 3 (июль--сентябрь 1987), 425--455, \url{https://www.mathnet.ru/rus/tvp1437}.
%K475 Algorithms and Randomness Theory Probability Applications 32, 389-412 (with V.A.Uspensky)

\bibitem{1987asarin}
Е.\,А.\,Асарин, О некоторых свойствах случайных в алгоритмическом смысле конечных объектов. \emph{Доклады АН СССР}, 1987, том 295, номер 4, 782--785. \url{https://www.mathnet.ru/rus/dan8027}.

\bibitem{1987asarin2}
Е.\,А.\,Асарин, О некоторых свойствах $\Delta$-случайных по Колмогорову конечных последовательностей. \emph{Теория вероятностей и её применения}, 1987, том 32, выпуск 3, 556--558. \url{https://www.mathnet.ru/rus/tvp1450}.

\bibitem{1987vyugin}
В.\,В.\,Вьюгин, О дефекте случайности конечного объекта относительно мер с заданными границами их сложности, \emph{Теория вероятностей и её применения}, \textbf{32}(3), 558--563 (1987), \url{https://www.mathnet.ru/rus/tvp1451}. Английский перевод: V.V.~V'yugin, On the defect of randomness of a finite object with respect to measures with given complexity bounds, \emph{Theory Prob. Appl.}, \textbf{32}, 508--512 (1987), DOI:10.1137/1132071 (published in 1988).

\bibitem{1987koppel}
Moshe Koppel, Complexity, Depth and Sophistication, \emph{Complex systems}, \textbf{1}, 1087--1091 (1987), \url{https://www.complex-systems.com/abstracts/v01_i06_a04/}.

\bibitem{1988bennett}
Charles H.~Bennett, Logical Depth and Physical Complexity, in \emph{The Universal Turing Machine --- a Half-Century Survey}, Rolf Herken, editor, Oxford University Press, 1988, 227--257.

\bibitem{1988koppel} 
Moshe Koppel, Structure, in \emph{The Universal Turing Machine --- a Half-Century Survey}, Rolf Herken, editor, Oxford University Press, 1988, 435--452.

\bibitem{1989CGG}
Thomas M.~Cover, Peter Gacs, Robert M.~Gray, Kolmogorov contributions to information theory and algorithmic complexity, \emph{The Annals of Probability}, 1989, vol.~17, no.~3, 840--865. 

\bibitem{1991koppelatlan}
Moshe Koppel, Henri Atlan, 
An Almost Machine-Independent Theory of Program-Length Complexity, Sophistication, and Induction,
\emph{Information Sciences}, \textbf{56}, 23--33 (1991).

\bibitem{1991CT}
Thomas M.~Cover, Joy A.~Thomas, \emph{Elements of information theory}, New York: Wiley, 1991. Second edition was published in 2006.
% exists in Internet Archive

\bibitem{1992li-vitanyi}
Ming Li, Paul M.B.~Vit\'anyi, Inductive Reasoning and Kolmogorov Complexity, \emph{Journal of Computer and System Sciences}, \textbf{44}(2), 343--384 (1992), \url{https://doi.org/10.1016/0022-0000(92)90026-F} (page 353 is missing). Preliminary version: M. Li and P. M. B. Vitanyi, \emph{Proceedings. Structure in Complexity Theory Fourth Annual Conference, Eugene, OR, USA, 1989},165--185, \url{https://doi.org/10.1109/SCT.1989.41823}

\bibitem{1993longpre}
Luc Longpr\'e, Sarah Mocas, Symmetry of information and one-way functions, \emph{Information Processing Letters}, \textbf{46}(2), 95--100 (1993), \url{https://doi.org/10.1016/0020-0190(93)90204-M}.

\bibitem{1999rissanen}
Rissanen, J.~Hypotheses selection and testing by the MDL principle,
\emph{Computer Journal}, \textbf{42}(4), 260--269 (1999), \url{https://ieeexplore.ieee.org/document/8138703}.

\bibitem{1999shen}
Alexander Shen, Discussion on Kolmogorov complexity and statistical analysis, \emph{Computer Journal}, \textbf{42}(4), 340--342 (1999), \url{https://doi.org/10.1093/comjnl/42.4.340}

\bibitem{1999vyugin}
V.V.~Vyugin, Algorithmic complexity and stochastic properties of finite binary sequences, \emph{Computer Journal}, \textbf{42}(4), 294--317 (1999), \url{https://doi.org/10.1093/comjnl/42.4.294}

\bibitem{2000gao-li-vitanyi}
Qiong Gao, Ming Li, Paul Vit\'anyi, Applying MDL to learn best model granularity,
\emph{Artificial Intelligence}, \textbf{121}, 1--29 (2000), \url{https://homepages.cwi.nl/~paulv/papers/ai00.pdf}.

\bibitem{2001gacs-tromp-vitanyi}
P\'eter G\'acs, John T.~Tromp, Paul M.B.~Vit\'anyi, Algorithmic Statistics, \emph{IEEE Transactions on Information Theory}, \textbf{47}(6), 2443--2463 (September  2001), \url{https://ieeexplore.ieee.org/abstract/document/945257}.

\bibitem{2000vitanyi-li}
Paul M.B.~Vit\'anyi, Ming Li, Minimum Description Length Induction,
Bayesianism, and Kolmogorov Complexity, \emph{IEEE Transactions on Information Theory}, \textbf{46}(2), 446--464 (March 2000), \url{https://homepages.cwi.nl/~paulv/papers/mdlindbayeskolmcompl.pdf}.

\bibitem{2001vovkshafer}
Glenn Shafer, Vladimir Vovk, \emph{Probability and Finance: It's Only a Game}, Wiley, 2001, see also~\url{http://www.probabilityandfinance.com/2001_book/index.html}.

\bibitem{2004grunwald}
Peter Grunwald, \emph{A tutorial introduction to the minimum description length principle}, \url{https://arxiv.org/abs/math/0406077}.

\bibitem{2004vereshchaginvitanyi}
Nikolai Vereshchagin, Paul Vit\'{a}nyi, Kolmogorov’s Structure Functions and Model Selection, \emph{IEEE Transactions on Information Theory}, \textbf{50}(12), 3265--3290 (December 2004). Previous version: 47th FOCS (2002). See also: \url{https://arxiv.org/pdf/cs/0204037.pdf}.

\bibitem{2006AFMV}
Luis Antunes, Lance Fortnow, Dieter van Melkebeek, N.V. Vinodchandran,
Computational depth: Concept and applications, \emph{Theoretical Computer Science}, \textbf{354}(3), 391--404 (April 2006), see \url{https://www.sciencedirect.com/science/article/pii/S0304397505008790}.

\bibitem{2006vitanyi}
Paul M.B.~Vit\'anyi, Meaningful information, \emph{IEEE TRansactions on Information Theory}, \textbf{52}(10), 4627--4626 (October 2006). See also: \url{https://arxiv.org/pdf/cs/0111053v3.pdf}.

\bibitem{2007antunesfortnow}
Luis Antunes, Lance Fortnow, Sophistication revisited, \emph{Theory of Computing Systems}, \textbf{45}, 150--161, \url{https://doi.org/10.1007/s00224-007-9095-5} (2007) Preliminary version: 30th ICALP (2003).

\bibitem{2010bauwens}
Bruno Bauwens, Computability in statistical hypotheses testing, and characterizations of independence and directed influences in time series using Kolmogorov complexity, Ph.D. thesis, University of Ghent, 2010. ISBN 978-90-8578-356-5, available at~\url{https://biblio.ugent.be/publication/1107852}.

\bibitem{2010downeyhirschfeldt}
Rodney G.~Downey, Denis R.~Hirschfeldt, \emph{Algorithmic Randomness and Complexity}, Springer, 2010, \url{https://doi.org/10.1007/978-0-387-68441-3}.

\bibitem{2012nies}
Andre Nies, \emph{Computability and Randomness}, Oxford University Press, 2009, \url{https://global.oup.com/academic/product/computability-and-randomness-9780199652600}.

\bibitem{2013SUV}
Н.\,К.\,Верещагин, В.\,А.\,Успенский, А.\,Шень, \emph{Колмогоровская сложность и алгоритмическая случайность}. М., МЦНМО, 2013. См. также~\url{https://hal-lirmm.ccsd.cnrs.fr/lirmm-00786255/document}. English version: A.~Shen, V.A.~Uspensky, N.~Vereshchagin, \emph{Kolmogorov Complexity and Algorithmic Randomness}, American Mathematical Society, 2017, see~\url{https://www.lirmm.fr/~ashen/kolmbook-eng-scan.pdf}, \url{https://hal-lirmm.ccsd.cnrs.fr/lirmm-01803620v1/document}.

\bibitem{2015vereshchaginshen}
Nikolay Vereshchagin, Alexander Shen, 
Algorithmic Statistics Revisited. In: Vovk, V., Papadopoulos, H., Gammerman, A. (eds) \emph{Measures of Complexity. Festschrift for Alexey Chervonenkis}.  Springer, Cham. \url{https://doi.org/10.1007/978-3-319-21852-6_17}, 235--252 (2015), see also~\url{https://arxiv.org/pdf/1504.04950.pdf}.

\bibitem{2016vereshchaginshen}
Nikolay Vereshchagin, Alexander Shen, 
Algorithmic Statistics: Forty Years Later. In: Adam Day, Michael Fellows, Noam Greenberg, Bakhadyr Khoussainov, Alexander Melnikov, Frances Rosamond, editors, \emph{Computability and Complexity. Essays Dedicated to Rodney G. Downey on the Occasion of His 60th Birthday}.  Springer, Cham. \url{https://doi.org/10.1007/978-3-319-21852-6_17} (2016), 669--737,  see also~\url{https://arxiv.org/abs/1607.08077}.

\bibitem{2017abst}
Lu\'\i s Antunes, Bruno Bauwens, Andr\'e Souto, Andreia Teixeira, Sophistication vs Logical Depth, \emph{Theory of Computing Systems}, \text{60}, 280--298 (2017), \url{https://link.springer.com/article/10.1007/s00224-016-9672-6}, see also~\url{https://arxiv.org/pdf/1304.8046.pdf}.

%arxiv2208.11237 Epstein Kolmogorov Birthday Paradox

\bibitem{2023kolmogorovseminar}
Материалы заседаний <<колмогоровского семинара>> (в последние годы он стал международным и происходит on-line), \url{https://www.youtube.com/channel/UC20xHyxD6FqItj2N6y3eSZg}.

\end{thebibliography}

\end{document}
\section{Статья Колмогорова~1965 года}

\begin{quote}
\textbf{\S4, Заключительные замечания}. Изложенная в \S3 концепция обладает одним существенным недостатком: она не учитывает <<трудности>> переработки программы $p$ и объекта $x$ в объект $y$. Введя надлежащие определения, можно доказать точно формулируемые математические предложения, которые законно интерпретировать как указание на существование таких случаев, когда объект, допускающий очень короткую программу, то есть обладающий очень малой сложностью $K(x)$, может быть восстановлен по коротким программам лишь в результате вычислений совершенно не реальной длительности. В другом месте я предполагаю изучить зависимость необходимой сложности программы $K^t(x)$ от допустимой трудности $t$ её переработки в объект~$x$. Сложность $K(x)$, которая была определена в \S3, появится при этом в качестве минимума $K^t(x)$ при снятии ограничений на величину~$t$.

За пределами этой заметки остаётся и применение построений \S3 к новому обоснованию теории вероятностей. Грубо говоря, здесь дело идёт о следующем. Если конечное множество $M$ из очень большого числа элементов $N$ допуская определение при помощи программы длины пренебрежимо малой по сравнению с $\log_2N$, то почти все элементы $M$ имеют сложность $K(x)$, близкую к $\log_2N$. Элементы $x\in M$ этой сложности и рассматриваются как <<случайные>> элементы множества $M$. Не вполне завершённое изложение этой идеи можно найти в статье~[2].
\end{quote}

(c. 10--11 в~\cite{1965kolmogorov}, упомянутая в тексте статья [2] --- статья в Sankhya 1963 года~\cite{1963kolmogorov}).

\section{Гач, Тромп, Витаньи}

P\'eter G\'acs, John T.~Tromp, Paul M.B.~Vit\'anyi, Algorithmic Statistics, \emph{IEEE Transactions on Information Theory}, \textbf{47}(6), 2443--2463 (September  2001), \url{https://ieeexplore.ieee.org/abstract/document/945257}

\begin{quote}
At a Tallinn conference in 1973, A.N.~Kolmogorov formulated the approach to an individual data-to-model relation, based on a two-part conde separating the \emph{structure} of a string from meaningless \emph{random} features, rigorously in terms of Kolmogorov complexity (attribution in [17], [4]). Cover [4], [5] interpreted this approach as a (sufficient) statistics. Following Shen [17] (see also [21], [18], [20]), this can be generalized to computable probability mass functions for which the data is ``typical''. Related aspects of ``randomness deficiency'' (formally defined later in (IV.1)) were formulated in [12], [13], and studied in [17], [21].  $\langle\ldots\rangle$ Despite its evident epistemological prominence in the theory of hypothesis selection and prediction, only selected aspects of algorithmic sufficient statistics have been studied before, for example, as related to the ``Kolmogorov structure function'' [17], [4], and ``absolutely nonstochastic objects'' [17], [21], [18], [22], notions also defined or suggested by Kolmogorov at the mentioned meeting.  [p.~2445]

$\langle\ldots\rangle$

\ldots a set $S$ is ``typical'' with respect to $x$ if $x$ is an element of $S$ that is ``typical'' in the sense of having small \emph{randomness deficiency} $\delta^*_S(x)=\log |S| - K(x\cnd S*)$\ldots [here $S^*$ is the shortest program for $S$]

A set $S$ is ``optimal'' if the best two-part description consisting of a description of $S$ and a straightforward description of $x$ as an element of $S$ by an index of size $\log|S|$ is as consise as the shortest one-part description of $x$. This implies that optimal sets are typical sets. [p.~2446, second column]
\end{quote}

The mapping to our numbering:
[4]$\to$\cite{1985cover}, \
[5]$\to$\cite{1991CT}, \
[12]$\to$\cite{1983kolmogorovconf}, \
[13]$\to$\cite{1987kolmogorov}, \
[17]$\to$\cite{1983shen}, \
[18]$\to$\cite{1999shen}, \
[20]$\to$\cite{2000vitanyi-li}, \
[21]$\to$\cite{1987vyugin}, \
[22]$\to$\cite{1999vyugin}.




\section{Вьюгин о дефектах}

В.\,В.\,Вьюгин:

В \cite{1985vyugin} используется определение дефекта $d(x\cnd A)=\log_2 A - K(x\cnd A)$ из~\cite{1983kolmogorovconf} и изучается априорная вероятность получить нестохастический объект.

\begin{quote}
Математический аппарат алгоритмической теории информации, основанной работами А.\,Н.\,Колмогорова и Р.\,Дж.\,Со\-ло\-мо\-но\-ва, позволяет рассматривать величины $K(\mathbf{P})$ --- сложность вероятностной меры \textbf{P} --- и $d_{\mathbf{P}}(x)$ --- дефект случайности конечной последовательности $x$ относительно меры $\mathbf{P}$ (чем больше величина $d_{\textbf{P}}(x)$, тем <<менее случайна>> $x$ относительно $\textbf{P}$. На основе этих понятий рассматривается естественная статистическая характеристика конечной последовательности $x$ --- функция 
\[
\beta_x(\alpha)=\min_{K(\mathbf{P})\le \alpha}d_{\mathbf{P}}(x).
\]
Цель данной работы состоит в изучении типов кривых $\beta_x(\alpha)$ для различных $x$.
\end{quote}

Источник: \cite{1987vyugin}; дефект определяется далее в терминах равномерных дефектов и устанавливается (замечание на с.~559(, что он близок к $d_A(x)=\log_2 |A|-K(x\cnd A)$

