% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{algorithm}
\usepackage{multirow}
% \usepackage[noend]{algorithmic}
%\usepackage{algorithmic}
\usepackage{paracol}
\usepackage{algpseudocode}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{marvosym}
\usepackage[detect-all]{siunitx}



% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{DDMM-Synth: A Denoising Diffusion Model for Cross-modal Medical Image Synthesis with Sparse-view Measurement Embedding}
%
\titlerunning{DDMM-Synth}
%\titlerunning{DDMM-Synth with Sparse-view Measurement Embedding}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Xiaoyue Li\inst{1} \and Kai Shang\inst{1} \and
Gaoang Wang\inst{2}\textsuperscript{\Letter} \and
Mark D.\ Butala\inst{1}\textsuperscript{\Letter}}
%
\authorrunning{Li. et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
% \institute{ZJU-UIUC Institute, Zhejiang University International Campus, Haining, Zhejiang 314400, P.R. China}
\institute{Zhejiang University-University of Illinois Urbana-Champaign
Institute, and College of Information Science and Electronics, Zhejiang University, China \email{markbutala@intl.zju.edu.cn} \and Zhejiang University-University of Illinois Urbana-Champaign
Institute, and College of Computer Science and Technology,
Zhejiang University, Zhejiang University, China \email{gaoangwang@intl.zju.edu.cn}}



% \author{Anonymous}
% %
% % \authorrunning{Li. et al.}
% % First names are abbreviated in the running head.
% % If there are more than two authors, 'et al.' is used.
% %
% \institute{Anonymous Organization\\
% \email{**@******.***}}

\maketitle              % typeset the header of the contribution
%
\begin{abstract}
%叙述现有的挑战，针对挑战，提出什么样的模型去解决的；在哪些数据集上，取得了什么样的结果
% Reducing the radiation dose in computed tomography (CT) is important to mitigate radiation-induced risks. One mitigation option to limit the radiation dosage is to employ a well-trained model to compensate for the incomplete information and map sparse-view measurements to a CT reconstruction. Yet the prior knowledge solely based on sparsely sampled measurements is theoretically insufficient to uniquely characterize an object and the learned model can be fragile on unseen sparse sampled number. Another approach is medical image translation, e.g., from magnetic resonance imaging (MRI) to CT image translation. However, there is no explicit transformation describing their relationship in addition to the fact that image translation in the MRI-only case can potentially introduce unreliable information into the synthesized CT images. In this work, we propose a novel framework called the denoising diffusion model for medical image synthesis (DDMM-Synth) to close these performance gaps described above. DDMM-Synth operates by combining the merits of an MRI-guided diffusion model with a new range and null space CT measurement decomposition sampling scheme. Specifically, we first train an MRI-guided diffusion model to capture the priors of textures related to CT images. Once the model is trained, the null-space sparse CT measurements are simpled integrated into the inference stage and the projection density can be adjusted a posterior as is required for a particular clinical application. Quantitative evaluations using two publicly available datasets confirm that DDMM-Synth can achieve superior performance versus other state-of-the-art supervised-learning-based baselines with fair experimental configurations.
Reducing the radiation dose in computed tomography (CT) is important to mitigate radiation-induced risks. One option is to employ a well-trained model to compensate for incomplete information and map sparse-view measurements to the CT reconstruction. However, reconstruction from sparsely sampled measurements is insufficient to uniquely characterize an object in CT, and a learned prior model may be inadequate for unencountered cases. Medical modal translation from magnetic resonance imaging (MRI) to CT is an alternative but may introduce incorrect information into the synthesized CT images in addition to the fact that there exists no explicit transformation describing their relationship. To address these issues, we propose a novel framework called the denoising diffusion model for medical image synthesis (DDMM-Synth) to close the performance gaps described above. This framework combines an MRI-guided diffusion model with a new CT measurement embedding reverse sampling scheme. Specifically, the null-space content of the one-step denoising result is refined by the MRI-guided data distribution prior, and its range-space component derived from an explicit operator matrix and the sparse-view CT measurements is directly integrated into the inference stage. DDMM-Synth can adjust the projection number of CT a posteriori for a particular clinical application and its modified version can even improve the results significantly for noisy cases. Our results show that DDMM-Synth outperforms other state-of-the-art supervised-learning-based baselines under fair experimental conditions. 
\keywords{Medical image synthesis \and Denoising diffusion probabilistic model \and Computed tomography reconstruction.}
\end{abstract}
%
\section{Introduction}

Computed tomography (CT) plays an essential role in medical diagnosis. However, obtaining CT measurements with complete projections, i.e., the sinogram, can potentially lead to dangerous radiation exposure, threatening the health of patients, especially for individuals with chronic conditions or undergoing cancer treatment. Reducing the radiation dose is vital but unavoidably increases artifacts in CT images, which could compromise a clinical diagnosis.\\
\indent The advent of data-driven methods offers fresh means to mitigate these aforementioned issues. Some prior studies have investigated the reconstruction of CT images from sparse-view measurements by learning to directly obtain CT reconstructions from partial measurements \cite{zhu2018image,shen2019patient,wurfl2018deep,wei20202}. Nevertheless, most of these are supervised learning techniques and need re-train a dedicated model when the measurement process changes, which lacks generalizability and flexibility. Moreover, for ill-posed problems like CT, sparsely sampled measurements are insufficient to uniquely specify an object. Medical image translation is another approach to solve the above problem \cite{han2017mr,chartsias2017adversarial,armanious2020medgan,bahrami2020new} which generally operates by determining a mapping from the source modality to the target modality, usually with specific focus on the translation from Magnetic Resonance Imaging (MRI) to CT. Since there exists no explicit and reliable relationship between them and sometimes MRI exhibit occluded sections  (see, e.g, the left-most image in the second row of Fig.~\ref{fig2}), image translation with only MRI images opens the possibility to introduce unreliable information into the synthesized CT images, which affects the diagnostic quality. Recently, the diffusion model \cite{song2019generative,ho2020denoising,song2020score,song2021scorebased} has emerged as a competitive candidate to realize high-fidelity synthetic results and has attracted the attention of medical imaging researchers \cite{jalal2021robust,peng2022towards,chung2022score,song2022solving}. Diffusion model consists of a forward stage that gradually injects Gaussian noise into a sample over sufficient steps and a reverse stage to iteratively denoise and finally recover the noiseless image.
% Different from other generative models such as generative adversarial networks (GANs) which are typically uninterpretable and prone to mode collapse in the training phase, 
Diffusion models have the advantage of analytical explainability, utilizing an optimal transport process that results in high-appearance similarity \cite{khrulkov2022understanding}, consistently achieving state-of-the-art performance in numerous image synthesis tasks \cite{croitoru2022diffusion}. \\
\indent In this paper, we introduce a new methodology using a diffusion model guided by both MRI and sampled prior CT information embedding for medical image synthesis, termed DDMM-Synth, which is designed as an attempt to deal with some clinical diagnoses, e.g., in evaluating traumatic pelvic injury \cite{smith2019advanced,nuchtern2015significance}, which need depend on multiple imaging modalities, aiming to obtain more realistic and reliable medical image synthesis while reducing the radiation risk to patients. In the DDMM-Synth framework, a pretrained diffusion model conditioned on MRI was utilized to provide valuable prior knowledge for CT reconstruction. Inspired by the great potential of reverse sampling via a range-null space decomposition \cite{schwab2019deep,wang2022gan,wang2022zero}, we then introduce the null-space measurement inference (N-SMI) block used to insert the refined null-space contents and necessary scanning information from sparse-view CT measurement into the reverse inference process (see Fig.~\ref{fig_process_flow}) without additional training. Intuitively, DDMM-Synth introduces guidance in the denoising process by manipulating the denoising object at each step with the known measurements, transforming both by the pseudo-inverse of the measurement matrix. \\
\indent Our main contributions are as follows: (1) We propose a novel diffusion model that integrates both an implicit data distribution prior mapping from MRI to CT images and effective information derived from sparse sampled CT measurements. This integrated approach enables higher-fidelity synthesis of CT images, which better reflects the structural and anatomical details of low-dose generated CT images. (2) Our method does not require a fixed measurement and can be adapted to any particular measurement scenario as long as the mapping from images to measurements is linear, which is generally the case in clinical applications. (3) Different from existing measurement-conditioned diffusion models which only work on noiseless measurements, our modified method DDMM-Synth-noise improves the synthesized results significantly for noisy measurements, enabling the use of our model even in challenging conditions.

%目前有很多医疗领域的问题用DDPM解决，但是大多数工作只关注了一个modality的，尤其是MRI，很少有人关注CT或者多模态的转换工作。并且大多数情况都忽略了现实情况下仪器本身或者病人轻微移动带来的噪声干扰。

% (3) our method is notably different from previous ``plug-and-play'' methods for medical images \cite{peng2022towards,chung2022score} which usually use matrix transposition to transform denoising CT images and measurements, replacing it with the pseudo-inverse for maintaining the scale of singular vectors related to nonzero singular values.
% \begin{itemize}\setlength{\itemsep}{10pt}
% \item We propose a novel diffusion model that integrates both implicit feature mapping between MRI and CT images and the informative prior information derived from sparse sampled CT measurements. This integrated approach enables the high-fidelity synthesis of CT images, which better reflects the structural and anatomical details of low-dose generated CT images.
% \item Our method does not need a fixed measurement and can be flexibly adapted to any different measurement processes at test time as long as the mapping from images to measurements is linear, which is robust to particular clinical applications.
% \item Different from other existing measurement-conditioned diffusion models which can only work on noiseless measurement, our modified version DDMM-Synth-noise can improve the synthesized results significantly even with arbitrary noisy situations, whereby we can tackle challenging real-world clinical situations.

% 我们的方法是作用在x0上面的，它的好处为；并且大多数方法不可以处理加噪声的情况，我们的方法可以在有噪声加入的情况下取得较好的结果

% \item Our model can be easily extended to other multi-modal medical image syntheses, e.g., multi-modal K-space undersampled MRI synthesis. More details are demonstrated in the experiments section.  Works well pm out-of-distribution (OOD) data.
% \end{itemize}

% While GAN models have emerged as a gold standard in recent years, they are not without any limitations. ... 
% MR->CT可以，但因为纹理及明暗度的干扰 (e.g., the interference of contrast and brightness)，image-to-image得到的结果容易产生偏差；而low-dose CT to high-dose CT又是一个病态问题，无法得到真实解。我们试图结合两个条件，病人只需经过低剂量的CT检查和相应的MR检查，即可得到接近于真实的CT数据
% However, CT images are often acquired using scanners from different vendors  with customized acquisition standards, resulting in significantly different texture feature

% MRI has currently become prominent in neurology with the use of high-field scanners, given that MRI contrasting agent is less likely to cause an allergic reaction compared with X-ray or CT scan using iodine-based substances. 

%During the last decade, the use of MR imaging to support radiation therapy (RT) treatment planning has considerably increased, as MR provides high contrast of tissues compared to CT images, enabling better delineation of organs. 

% Because of their complementary characteristics, multi-modality imaging with MRI and CT is often used in clinical practice, e.g., radiotherapy requires both CT and MRI, because CT provides an electron density distribution indispensable for treatment planning while MR outlines tumors and soft tissues. However, simultaneous CT-MRI is still a research area, and CT and MRI scans are currently performed in separation, which is not only expensive but also brings about nonrigid misalignment between MRI and CT images. Developing a simultaneous CT-MRI device may be a solution of this problem, and we have conducted studies to propose top-level designs of such a device.
% DDPM 先验强制协调图像，所以能够从其他区域再现纹理，并修复语义上有意义的内容。pixel-wise 和 perceptual loss 会导致生成模型朝着纹理填充而不是语义修复方向更新
% GANs 侧重于纹理合成，例如背景填充和删除对象，但难以进行语义合成。我们利用了预训练去噪扩散概率模型（DDPM）的高表达能力，因此将其用作通用图像修复的先验。
% 我们提出了一个高保真度的稀疏视角CT重建的新范式。
% This recovery is an ill-posed problem given nonlinear variations in tissue signals across modalities. Tomography imaging is in general an ill-posed inverse problem since there may be multiple reconstructed objects that are consistent with the same measurements.

% We exploit to take advantage of both MRI and few sparse measurement knowledge containing high-frequency information (e.g., edges and content details) to guide the diffusion model to more accurately extract and merge features. 

% is to provide the electron density information for facilitating dose calculation in the absence of full medical measurements. (5) maybe 加快采样速度，且可以更好地拟合到真实数据分布区间

% This is particularly applicable to clinical protocols, where patients are scanned multiple times during disease management for the purpose of guiding the interventional procedure or monitoring the therapeutic response. 


% \section{Methods}
% \subsection{Denoising Diffusion Probabilistic Model}

% A diffusion model consists of two stages: a forward stage that gradually injects $i.i.d.$ Gaussian noise into a sample $\mathbf{x}_0 \sim q\left(\mathbf{x}_0\right)$ over sufficiently large step $T$ and a reverse stage to iteratively denoise, and finally recover an original noiseless image. According to the properties of the Gaussian distribution, the noise-perturbed images at any step can be directly obtained from the original image $\mathbf{x}_0$,
% \begin{equation}
%     q\left(\mathbf{x}_t \mid \mathbf{x}_0\right)=\mathcal{N}\left(\mathbf{x}_t;\sqrt{\bar{\alpha}_t}\mathbf{x}_0,(1-\bar{\alpha}_t)\mathbf{I}\right),\label{eq.3}
% \end{equation}
% where $T$ is the total number of noising steps, $\beta_t \in (0, 1)$ is predefined value, which is used to determine the range of incremental Gaussian noise at each iteration, $\mathcal{N}$ means a Gaussian distribution. Besides, $\alpha_t = 1 - \beta_t$ and $\bar{\alpha}_t=\prod\limits_{i=1}^T\alpha_i$. 

% Technically, the reverse denoising process can be defined as the posterior distribution $p\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0\right)$ with mean ${\mu}_t\left(\mathbf{x}_t,\mathbf{x}_0\right)=\dfrac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t-\epsilon\dfrac{1-\alpha_t}{\sqrt{\alpha_t}}\right)$. Here a network $\epsilon_\theta\left(\mathbf{x}_{t-1},\mathbf{x}_t\right)$ is trained aiming to predict the only uncertain variable $\epsilon$ for each reverse step $t$ and provide the estimated mean $\mu_\theta(\mathbf{x}_t, t)$ and variance $\sigma_\theta(\mathbf{x}_t, t)$,

% \begin{equation}
%     p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right)=\mathcal{N}\left(\mathbf{x}_{t-1};\mu_\theta(\mathbf{x}_t,t),\sigma_\theta(\mathbf{x}_t,t)\right).
% \end{equation}

%%%%%%%

% \begin{gather}    p_\theta(\mathbf{x}_{0:T})=p_\theta(\mathbf{x}_T)\prod\limits_{t=1}^{T}p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right),\\
%     p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right)=\mathcal{N}\left(\mathbf{x}_{t-1};\mu_\theta(\mathbf{x}_t,t),\Sigma_\theta(\mathbf{x}_t,t)\right)
% \end{gather}
% Moreover, the posterior probability $q\left(\mathbf{x}_{t-1}\right)$ given by the condition $\left(\mathbf{x}_t,\mathbf{x}_0\right)$ can be derived from Eq. ~\ref{eq.1} and Eq. ~\ref{eq.3}:
% \begin{gather}
%     q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t,\mathbf{x}_0\right):=\mathcal{N}\left(\mathbf{x}_{t-1};\widetilde{\mu}_t(\mathbf{x}_t,\mathbf{x}_0),\widetilde{\beta}_t\mathbf{I}\right)\label{eq.6},\\
%     \widetilde{\mu}_t(\mathbf{x}_t,\mathbf{x}_0)=\dfrac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\mathbf{x}_t+\dfrac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\mathbf{x}_0,\\
%     \widetilde{\beta}_t=\dfrac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t
% \end{gather}
% Based on the variational method, the optimization objective is to optimize the variational lower bound:
% \begin{gather}    
% \mathbf{L}=\mathbf{L}_0+\sum_{t=2}^{T} \mathbf{L}_{t-1}+\mathbf{L}_T, \\
% \mathbf{L}_0=-\log p_\theta\left(\mathbf{x}_0 \mid \mathbf{x}_1\right), \\
% \mathbf{L}_{t-1}=K L\left(q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0\right) \| p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right)\right), \\
% \mathbf{L}_T=K L\left(q\left(\mathbf{x}_T \mid \mathbf{x}_0\right) \| p_\theta\left(\mathbf{x}_T\right)\right),
% \end{gather}
% where $KL$ means the Kullback-Leibler divergence between two probability distributions. Note that $\mathbf{L}_{T}$ can be ignored since $q\left(\mathbf{x}_T \mid \mathbf{x}_0\right)$  doesn't has any learnable parameters and $\mathbf{x}_T$ is a Gaussian noise.

% With the reparameterization trick, the optimization objective can be simplified as 
% \begin{equation}
%     \mathbf{L}_{t-1}^{\text{simple}}=\mathbb{E}_{\mathbf{x}_0, \epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{I})}\left[\left \| \epsilon-\epsilon_\theta \left(\sqrt{\bar{\alpha}_t}\mathbf{x}_0+\sqrt{1-\bar{\alpha}_t} \epsilon, t\right)\right \|^2\right]
% \end{equation}

% To recover the noiseless image in the reverse stage, we can train a network $p_\theta$ to provide the noise vector $\epsilon_\theta$ each reverse step and provide the estimated mean $\mu_\theta(\mathbf{x}_t, t)$ and variance $\sigma_\theta(\mathbf{x}_t, t)$.

\begin{figure}[!t]
\centering
\includegraphics[width=\textwidth]{process_flow_v6.png}
\caption{High-level illustration of the reverse sampling process for DDMM-Synth. The green block is the network $\epsilon_\theta$, which is conditioned by MRI $\mathbf{m}$. The blue block is the null-space measurement inference (N-SMI) block, where the measurement matrix $\mathbf{A}$ and measurement $\mathbf{y}$ are used to guide the inference process. Here we show two N-SMI blocks: one for the case with noise and the other without.} \label{fig_process_flow}
\end{figure}

\section{DDMM-Synth}

DDMM-Synth aims to synergistically combine MRI tissue information and a sparse sampled CT reconstruction to yield synthetic CT images that preserve the detailed tissue features encoded in the MRI while maintaining data consistency with CT images. Our model employs a conditional denoising diffusion model and a null-space measurement decomposition-based reverse sampling process. The core principles of DDMM-Synth are detailed below.

% \subsection{Discretization of Radon Transform} 
% \subsubsection{Problem Statement}

% The standard forward model in CT is given by
% \begin{equation}
%     \mathbf{y}=\mathbf{A}\mathbf{x}+\mathbf{n},
% \end{equation}
% where $\mathbf{y}$ denotes the measurements, $\mathbf{A}$ is the linear measurement operator that acts as the intermediary between the measurement and an unknown image domains, $\mathbf{x}$ are the discretized, unknown densities to be reconstructed, and $\mathbf{n}$ represents acquisition noise. 

% In order to handle the continuous variables digitally, we replace the continuous-to-discrete model to discrete model, where $h(\mathbf{r})$ is approximated as 
% \begin{equation}
% h_d(\mathbf{r})=\sum_{n=1}^N[x]_n \psi_n(\mathbf{r}),
% \end{equation}
% where $d$ stands for discretization of $h(\mathbf{r})$, $[x]_n$ is the $n$-th element of the vector $\mathbf{x}$ and $\psi_n(\mathbf{r})$ is the $n$-th expansion function. The system can be expressed as
% \begin{equation}
% \mathbf{y} \approx \mathcal{A} h_d(\mathbf{r})=\sum_{n=1}^N[x]_n \mathcal{A} \psi_n(\mathbf{r}) \equiv \mathbf{A} \mathbf{x},
% \end{equation}
% where $\mathbf{A}$ is linear measurement matrix that acts as an intermediary between the measurement domain and an original unknown image domain, which is constructed using $\mathcal{A}$ and $\left\{\psi_n(\mathbf{r})\right\}_{n=1}^N$. A well-known expansion function for two-dimensional objects $h(\mathbf{r})$ with $\mathbf{r}=(s1, s2)$ is the pixel expansion function. It can be expressed as:
% \begin{equation}
% \psi_n(\mathbf{r})= \begin{cases}1, & \text { if }\left|s_1-s_{1n}\right| \text { and }\left|s_2-s_{2n}\right| \leq \frac{\gamma}{2} \\ 0, & \text { otherwise }\end{cases},
% \end{equation}
% where $\mathbf{r}_n=\left(s_{1n}, s_{2n}\right)$ means the coordinates of the $n$-th grid point of a uniform Cartesian lattice and $\gamma$ denotes the interval between the lattice points. 

% Diffusion models can solve such inverse problems via Eq. 1, assuming that the problem-specific scores for all noise levels are available. While it is possible to train a conditional diffusion model for a specific $\mathbf{A}$, it is computationally expensive to do this for a large family of problems, such as sparse reconstruction in medical imaging. Therefore, we wish to utilize more commonly available problem-agnostic score models that are not trained specifically for the target inverse problem. 
% If ∇xtlog pt(xt|y) can be effectively approximated with Sθ(xt; σt), then we can directly plug it in Eq. 1 to solve the inverse problem.

\subsubsection{Conditional Denoising Process}
Technically, the reverse denoising process can be defined as the posterior distribution $p\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0\right)$ with mean ${\mu}_t\left(\mathbf{x}_t,\mathbf{x}_0\right)=\dfrac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t-\epsilon\dfrac{1-\alpha_t}{\sqrt{\bar{\alpha}_t}}\right)$ and variance $\sigma^2_t=\dfrac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t$ to predict the noiseless objects $\mathbf{x}_0 \sim q\left(\mathbf{x}\right)$ from random Gaussian noise $\mathbf{x}_T$ from a $T$-step forward process, where $\alpha_t = 1 - \beta_t$ and $\bar{\alpha}_t=\prod\limits_{i=1}^T\alpha_i$, $\beta_t \in (0, 1)$ is a predefined value. $q\left(\mathbf{x}\right)$ means the data distribution for the training dataset. The network $\epsilon_\theta\left(\mathbf{x}_{t-1},\mathbf{x}_t\right)$ is trained aiming to predict the only uncertain variable $\epsilon$ for each reverse step $t$ and then provide the estimated mean $\mu_\theta(\mathbf{x}_t, t)$ to obtain $\mathbf{x}_{t-1}$. 

% Since $\mathbf{x}_0$ is unknown, according the Bayesian rule, $\mathbf{x}_0$ can be estimated by $\mathbf{x}_t$ in the posterior distribution can be converted to be only related to $\mathbf{x}_t$ and $\epsilon$.

Following the conditional model utilized in \cite{saharia2022image}, the first step of DDMM-Synth is to adapt the diffusion model to be  conditional by concatenating each MRI image with a corresponding input noisy CT image when training the network $\epsilon_\theta$. Guided by the MRI images $\mathbf{m}$, the objective can be modified as follows:
\begin{equation}
    \mathbf{L}_{t-1}^{\text{conditional}}=\mathbb{E}_{\mathbf{x}_0, \epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{I})}\left[\left \| \epsilon-\epsilon_\theta \left(\sqrt{\bar{\alpha}_t}\mathbf{x}_0+\sqrt{1-\bar{\alpha}_t} \epsilon, \mathbf{m}, t\right)\right \|^2\right].
\end{equation}

% The training procedure is listed in Table. 
\subsubsection{Inference via Range-Null Space Decomposed Medical Measurement}

% Reconstructing the tomography imaging is a long-standing problem due to its critical application value and its ill-posed nature. Some traditional methods can be usually summaried as follow:
% \begin{equation}
%     \hat{\mathbf{x}}=\underset{\mathbf{x}}{\arg \min } \frac{1}{2 \sigma^2}\|\mathbf{A} \mathbf{x}-\mathbf{y}\|_2^2+\lambda \mathcal{R}(\mathbf{x})
% \end{equation}
% Where $\frac{1}{2 \sigma^2}\|\mathbf{A} \mathbf{x}-\mathbf{y}\|_2^2$ is the data-fidelity term while the image-prior term $\lambda \mathcal{R}(\mathbf{x})$ focuses on regularizing the reconstructed images within the original distribution space via some hand-craft prior knowledge. While incorporating prior knowledge can aid in mitigating artifacts in medical imaging, such techniques are often insufficient in reconstructing realistic details and compromised by smoothing the results. Moreover, it is challenging to adjust the regularization to balance the realness and data consistency.\\


The standard forward model in CT is given by
\begin{equation}
    \mathbf{y}=\mathbf{A}\mathbf{x}+\mathbf{n},
\end{equation}
where $\mathbf{y}$ denotes the measurements, $\mathbf{A}$ is the linear measurement operator, $\mathbf{x}$ are the discretized, unknown densities to be reconstructed, and $\mathbf{n}$ represents acquisition noise. \\
\indent Traditional CT can generally be summarized as a trade-off between a data-fidelity term and an image-prior term. The former models the physical process to ensure data consistency while the latter is used to regularize the reconstructed images via hand-crafted priors. However, traditional techniques cannot faithfully reconstruct realistic details and compromise by smoothing the results. Moreover, it is challenging to adjust the regularization to balance the data consistency and the priors.\\
\indent From the perspective of the range-null space decomposition \cite{schwab2019deep,wang2022gan,wang2022zero}, data consistency is exclusively linked to range-space content and can be accurately computed. Thus, the fundamental challenge lies in identifying the appropriate null-space content that should be consistent with prior information to yield high-fidelity results. Neglecting acquisition noise, the measurement process can be decomposed as
\begin{equation}
    \mathbf{A}\mathbf{x} \equiv \mathbf{A}\mathbf{A}^\dagger \mathbf{A} \mathbf{x} + \mathbf{A}(\mathbf{I} - \mathbf{A}^\dagger \mathbf{A}) \mathbf{x} \equiv \mathbf{A} \mathbf{x} + \mathbf{0} \equiv \mathbf{y},
\end{equation}
where $\mathbf{A}^\dagger$ is the pseudo-inverse matrix that satisfies $\mathbf{A}\mathbf{A}^\dagger\mathbf{A} \equiv \mathbf{A}$, $\mathbf{A}^\dagger \mathbf{A} \mathbf{x}$ is the portion in the range-space of $\mathbf{A}$, $(\mathbf{I} - \mathbf{A}^\dagger \mathbf{A})\mathbf{x}$ is the portion in the null-space.
% Note that range-space part is constant since $\mathbf{A}^\dagger \mathbf{A} \mathbf{x} = \mathbf{A}^\dagger \mathbf{y}$. 

For ill-posed problems like CT, there exist infinite $\hat{\mathbf{x}}$ that satisfy the constraint $\mathbf{A}\hat{\mathbf{x}}=\mathbf{y}$. The solution $\hat{\mathbf{x}}$ can be derived as $\hat{\mathbf{x}}=\mathbf{A}^\dagger\mathbf{y}+(\mathbf{I}-\mathbf{A}^\dagger\mathbf{A})\bar{\mathbf{x}}$, where $\mathbf{A}^\dagger\mathbf{y}$ aims to satisfy the data consistency and $\bar{\mathbf{x}}$ determines the data distribution priors. On account of the outstanding abilities to fit data distributions with diffusion models, here we replace the null-space portion of one-step denoising result during the inference process, aiming for a higher-fidelity synthetic result that also satisfies the original data distribution prior mapping from the corresponding MRI guidance. The reverse inference process of the diffusion model can be modified as follows:
\begin{gather}
    p\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t,\mathbf{x}_0\right):=\mathcal{N}\left(\mathbf{x}_{t-1};\mu_t(\mathbf{x}_t,\hat{\mathbf{x}}_{0 \mid t}),\sigma^2_t\mathbf{I}\right),\\
    \hat{\mathbf{x}}_{0 \mid t}=\mathbf{A}^\dagger\mathbf{y}+(\mathbf{I}-\mathbf{A}^\dagger\mathbf{A})\mathbf{x}_{0 \mid t}=\mathbf{x}_{0 \mid t}-\mathbf{A}^\dagger(\mathbf{A}\mathbf{x}_{0 \mid t}-\mathbf{y}),\label{eq.x0t}
\end{gather}
where $\mathbf{x}_{0 \mid t}$ can be derived from $\mathbf{x}_t$ and the noise model $\epsilon_\theta$ as follows:
\begin{equation}
    \mathbf{x}_{0 \mid t}=\dfrac{1}{\sqrt{\bar{\alpha_t}}}\left(\mathbf{x}_t-\epsilon_\theta(\mathbf{x}_t,\mathbf{m}, t)\sqrt{1-\bar{\alpha}_t}\right).
\end{equation}
Then, we employ a step-by-step reverse sampling with $p(\mathbf{x}_{t-1} \mid \mathbf{x}_t,\hat{\mathbf{x}}_{0 \mid t})$ and finally obtain the denoised CT image.\\
% The details of reverse inference stage are given in Algo.~\ref{alg:sampling-of-ddmm}.\\
\indent Importantly, the range-null space decomposed measurement can be further extended to the noisy case by modifying Eq.~\ref{eq.x0t} as:
\begin{equation}
   \hat{\mathbf{x}}_{0 \mid t} = \mathbf{A}^\dagger\mathbf{y}+(\mathbf{I}-\mathbf{A}^\dagger\mathbf{A})\mathbf{x}_{0 \mid t}\notag \\ 
   = \mathbf{x}_{0 \mid t} - \mathbf{A}^\dagger(\mathbf{A}\mathbf{x}_{0 \mid t} - \mathbf{A}\mathbf{x})+\mathbf{A}^\dagger\mathbf{n}, \quad \mathbf{A}^\dagger\mathbf{n} \sim \mathcal{N}(\mathbf{0}, \sigma^2_{\mathbf{n}}\mathbf{I}).
\end{equation}

To account for the noise added into $\hat{\mathbf{x}}_{0 \mid t}$, we define a scaling parameter $\gamma_t$ to adjust the range-space correction. Then, the reverse process can be rewritten as:
\begin{align}
    \hat{\mathbf{x}}_{0 \mid t} &= \mathbf{x}_{0 \mid t} - \gamma_t\mathbf{A}^\dagger(\mathbf{A}\mathbf{x}_{0 \mid t} - \mathbf{A}\mathbf{x} - \mathbf{n}) \\
    &= \mathbf{x}_{0 \mid t} - \gamma_t\mathbf{A}^\dagger(\mathbf{A}\mathbf{x}_{0 \mid t} - \mathbf{A}\mathbf{x}) + \sigma_{\mathbf{n}}\gamma_t\epsilon_{\mathbf{n}}, \quad \epsilon_{\mathbf{n}} \sim \mathcal{N}(\mathbf{0},\mathbf{I}).
\end{align}
Next, the denoising process in the DDPM sampling strategy can be written as:
\begin{equation}
    \mathbf{x}_{t-1}=\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\hat{\mathbf{x}}_{0 \mid t} + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\mathbf{x}_t+\sigma_t\epsilon, \quad \epsilon \sim \mathcal{N}(\mathbf{0},\mathbf{I}). \label{denoise_process}
\end{equation}
Given that the total noise level cannot exceed the noise distribution of Eq.~\ref{eq.x0t} in Section~2, i.e., $\mathcal{N}(\mathbf{0},\sigma^2_t\mathbf{I})$, Eq.~\ref{denoise_process} can be expressed as:
\begin{gather}
     \mathbf{x}_{t-1}=\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\bigl[\mathbf{x}_{0 \mid t} - \gamma_t\mathbf{A}^\dagger(\mathbf{A}\mathbf{x}_{0 \mid t} - \mathbf{A}\mathbf{x})\bigr] + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\mathbf{x}_t+\epsilon_{measure}+\epsilon_{extra},\\
     \epsilon_{measure} = \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\sigma_{\mathbf{n}}\gamma_t\epsilon_{\mathbf{n}} \sim \mathcal{N}\biggl(\mathbf{0}, \Bigl[\frac{\gamma_t\sqrt{\bar{\alpha}_{t-1}}\beta_t\sigma_{\mathbf{n}}}{1-\bar{\alpha}_t}\Bigr]^2 \mathbf{I}\biggr),\\
     \epsilon_{extra} \sim \mathcal{N}(\mathbf{0},\phi_t\mathbf{I}),\\
     \epsilon = \epsilon_{measure} + \epsilon_{extra} \sim \mathcal{N}\biggl(\mathbf{0},\Bigl[\frac{\gamma_t\sqrt{\bar{\alpha}_{t-1}}\beta_t\sigma_{\mathbf{n}}}{1-\bar{\alpha}_t}\Bigr]^2 + \phi_t\biggr)=\mathcal{N}(\mathbf{0},\sigma^2_t\mathbf{I}).
\end{gather}
Here the total noise in the one-step denoising process can be divided into measurement noise $\epsilon_{measure}$ and extra noise $\epsilon_{extra}$ components, where $\phi_t$ scales the extra noise component in order to agree with the general noise variance. 
To assure the general noise follows the Gaussian distribution $\mathcal{N}(\mathbf{0},\sigma^2_t\mathbf{I})$. Following the two principles proposed by \cite{wang2022zero}, for simplicity, $\phi_t$ and $\gamma_t$ were set as:
\begin{gather}
    \phi_t = \sigma^2_t - \big(\frac{\gamma_t\sqrt{\bar{\alpha}_{t-1}}\beta_t\sigma_{\mathbf{n}}}{1-\bar{\alpha}_t}\bigr)^2,\notag
    \gamma_t= \begin{cases}1, & \sigma_t \geq \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t\sigma_{\mathbf{n}}}{1-\bar{\alpha}_t} \\  \frac{\sigma_t(1-\bar{\alpha}_t)}{\sqrt{\bar{\alpha}_{t-1}}\beta_t\sigma_{\mathbf{n}}}, & \sigma_t<\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t\sigma_{\mathbf{n}}}{1-\bar{\alpha}_t}\end{cases}.\notag
\end{gather}

% To mitigate the additional noise $\mathbf{A}^\dagger\mathbf{n}$ (which we assume to follow the $\mathcal{N}(\mathbf{0}, \sigma^2_{\mathbf{n}}\mathbf{I})$ distribution), the reverse process can be modified as:
% \begin{gather}
%     \hat{\mathbf{x}}_{0 \mid t} = \mathbf{x}_{0 \mid t} - \gamma_t\mathbf{A}^\dagger(\mathbf{A}\mathbf{x}_{0 \mid t} - \mathbf{y}),\\
%     p(\mathbf{x}_{t-1} \mid \mathbf{x}_t,\hat{\mathbf{x}}_{0 \mid t}) = \mathcal{N}(\mathbf{x}_{t-1}; \mu_t(\mathbf{x}_t,\hat{\mathbf{x}}_{0 \mid t}),\phi_t\mathbf{I}),
% \end{gather}
% where $\gamma_t$ adjusts the range-space correction and $\phi_t$ scales the noise in a one-step denoising process. Following the two principles proposed by \cite{wang2022zero} and one-step denoising equation $\mathbf{x}_{t-1}=\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\hat{\mathbf{x}}_{0 \mid t} + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\mathbf{x}_t+\sigma_t\epsilon$, the parameters are set as:
% \begin{gather}
%     \phi_t = \sigma^2_t - (\frac{\gamma_t\sqrt{\bar{\alpha}_{t-1}}\beta_t\sigma_{\mathbf{n}}}{1-\bar{\alpha}_t})^2,
%     \gamma_t= \begin{cases}1, & \sigma_t \geq \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t\sigma_{\mathbf{n}}}{1-\bar{\alpha}_t} \\  \frac{\sigma_t(1-\bar{\alpha}_t)}{\sqrt{\bar{\alpha}_{t-1}}\beta_t\sigma_{\mathbf{n}}}, & \sigma_t<\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t\sigma_{\mathbf{n}}}{1-\bar{\alpha}_t}\end{cases}.
% \end{gather}

An overview of the reverse sampling process is shown in Fig.~\ref{fig_process_flow}. The N-SMI block employed, either noiseless or noisy, can be selected depending on the severity of the acquisition noise. 
% A detailed derivation can be found in the supplementary material.

% \begin{paracol}{2} % start two columns
% % first algorithm
% \begin{minipage}{0.5\textwidth}
%     \begin{algorithm}[H]\algsetup{linenosize=\scriptsize} \scriptsize
%         \captionsetup{font=scriptsize}
%         \caption{Reverse stage of DDMM-Synth}
%         \label{alg:sampling-of-ddmm}
%         \begin{algorithmic}[1]
%             \STATE $ \mathbf{x}_T \sim \mathcal{N}(\mathbf{0},\mathbf{I}) $
%             \FOR {$t=T,...,1$}
%                 \STATE $L=\text{min}\{T-t, l\}$
%                 \STATE $\mathbf{x}_{t+L} \sim q(\mathbf{x}_{t+L}|\mathbf{x}_t)$
%                 \FOR {$j=L,...,0$}
%                     \STATE $\mathbf{x}_{0|t+j}=\frac{1}{\sqrt{\overline{\alpha}_{t+j} } } (\mathbf{x}_{t+j}-\mathcal{\epsilon}_\theta(\mathbf{x}_{t+j},\mathbf{m}, t+j)\sqrt{1-\overline{\alpha}_{t+j}})$
%                     \STATE $\hat{\mathbf{x}}_{0|t+j} = \mathbf{A}^\dagger \mathbf{y} + (\mathbf{I} - \mathbf{A}^\dagger \mathbf{A})\mathbf{x}_{0|t+j}$
%                     \STATE $\mathbf{x}_{t+j-1} \sim p(\mathbf{x}_{t+j-1}|\mathbf{x}_{t+j},\hat{\mathbf{x}}_{0|t+j})$
%                 \ENDFOR
%             \ENDFOR
%             \RETURN $\mathbf{x}_0$
%         \end{algorithmic}
%     \end{algorithm}
% \end{minipage}

% % switch column
% \switchcolumn

% % second algorithm
% \begin{minipage}{0.5\textwidth}
%     \begin{algorithm}[H]\algsetup{linenosize=\scriptsize} \scriptsize
%     \setcounter{algorithm}{1}
%         \captionsetup{font=scriptsize}
%         \caption{Reverse stage of DDMM-Synth-noise}
%         \label{alg:sampling-of-ddmm-noise}
%         \begin{algorithmic}[1]
%             \STATE $ \mathbf{x}_T \sim \mathcal{N}(\mathbf{0},\mathbf{I}) $
%             \FOR {$t=T,...,1$}
%                 \STATE $L=\text{min}\{T-t, l\}$
%                 \STATE $\mathbf{x}_{t+L} \sim q(\mathbf{x}_{t+L}|\mathbf{x}_t)$
%                 \FOR {$j=L,...,0$}
%                     \STATE $\mathbf{x}_{0|t+j}=\frac{1}{\sqrt{\overline{\alpha}_{t+j} } } (\mathbf{x}_{t+j}-\mathcal{\epsilon}_\theta(\mathbf{x}_{t+j},\mathbf{m}, t+j)\sqrt{1-\overline{\alpha}_{t+j}})$
%                     \STATE $\hat{\mathbf{x}}_{0|t+j} = \mathbf{x}_{0|t+j} - \gamma_{t+j}\mathbf{A}^\dagger (\mathbf{A}\mathbf{x}_{0|t+j}-\mathbf{y})$
%                     \STATE $\mathbf{x}_{t+j-1} \sim p(\mathbf{x}_{t+j-1}|\mathbf{x}_{t+j},\hat{\mathbf{x}}_{0|t+j})$
%                 \ENDFOR
%             \ENDFOR
%             \RETURN $\mathbf{x}_0$
%         \end{algorithmic}
%     \end{algorithm}
% \end{minipage}

% % end two columns
% \end {paracol}

\section{Experiments and Results}
\subsubsection{Experimental Setup}
We evaluate the performance of DDMM-Synth on two public datasets. The first is the Gold Atlas project dataset \cite{nyholm2018mr}, which is a multi-modal pelvic MRI-CT dataset where the CT images have been co-registered to fit the anatomy of paired T2 MRI. Following \cite{boni2020mr}, images from site 2 and site 3 were selected as the training dataset and data from site 1 was used as the test dataset. The first and last three axial slices were removed from each volume to avoid aliasing effects. This scheme can also test the robustness of DDMM-Synth as the data in the test dataset were acquired from a different scanner than the training dataset. We have also tested on the BRATS2018 dataset \cite{menze2014multimodal}. Images from 50 low grade glioma (LGG)
were selected as the training set while another 15 patients are selected as the test dataset. Note that we conducted the task under the assumption of using the T2 MRI as the reference (as the dataset does not include CT) and FLAIR as the target to further confirm the effectiveness of our method. Two metrics were used to evaluate the performance: peak signal-to-noise ratio (PSNR) and structural similarity (SSIM).\\ 
% and Frechet Inception Distance (FID) \cite{heusel2017gans}.\\
% \subsubsection{Implementation Details}
\indent We base the implementation on the SR3 model \cite{saharia2022image}. The total number of epochs was set to 100, the U-Net was trained by the Adam optimizer with a learning rate of $1\times{10}^{-4}$. During the training phase, we set $T = 2000$. Using the DDIM sampler \cite{song2020denoising} to accelerate the reverse process, we reduced the number of time steps from 2000 to 100 steps. We simulate sinograms with a parallel-beam geometry using projection angles uniformly distributed over 180$^\circ$. 

\subsubsection{Baseline Methods}
We compare DDMM-Synth with several state-of-the-art generative models: pix2pix \cite{isola2017image}, pGAN \cite{dar2019image}, medSynth \cite{nie2018medical}, and Self-attention GAN (SAGAN) \cite{zhang2019self}. For fair comparisons among competing methods, the same modified 70$\times$70 patchGAN discriminator is used for all methods. We also modified SAGAN which was originally for unconditional cases by incorporating self-attention modules into the pGAN. In addition, all the competing models are conditioned by concatenating MRI images with corresponding sparse-view CT images with 23 projection angles along the channel dimension. 

% \textbf{pix2pix:} A conditional GAN (cGAN) based image-to-image translation network with U-Net backbone as the generator and patchGAN architecture as the discriminator.

% \textbf{pGAN:} A modified pix2pix model with ResNet backbone was considered. Its generator contains a cascade of residual CNN blocks. Except for the LSGAN loss and the content loss, pGAN introduced the perceptual loss during the training phase.

% \textbf{Self-Attention GAN:} A GAN model with self-attention modules incorporated into the generator and discriminator. Here we changed the original unconditional SAGAN to as the conditional one by plugging the self-attention modules into the pGAN.

% \textbf{medSynth:} A generative adversarial approach with residual U-Net backbone was considered. The generator contains a long-term residual unit from the first to the last layer. 

% \begin{table}
% \caption{Performance for CT synthesis task in the pelvic MRI-CT dataset. Higher PSNR, higher ssim and lower FID are better. Boldface indicates the best results and $N_p$ is the number of projections over 180$^\circ$.}\label{tab1}
% \centering
% \begin{tabular}{lccclll} \toprule
% Method                                            && $N_p$                     & PSNR                       & SSIM              & FID         \\ \midrule
% pix2pix \cite{isola2017image}                     && 23                              & 28.98                      & 0.877    &                  \\
% pGAN \cite{dar2019image}                          && 23                              & 29.19                      & 0.893    &                 \\
% medSynth \cite{nie2018medical}                    && 23                              & 30.03                      & 0.891    &                   \\
% SAGAN \cite{zhang2019self}                        && 23                              & 31.01                      & 0.912    &                  \\ \midrule
% \multicolumn{1}{c}{\multirow{3}{*}{DDMM-Synth (\textbf{Ours})}} && 10              &  29.82                     & 0.861    &              \\
% \multicolumn{1}{c}{}                            && 20                              &  33.12                     &  0.936     &  \\ 
% \multicolumn{1}{c}{}                            && 23                              & \pmb{33.79}                & \pmb{0.941}   & \\\bottomrule
% \end{tabular}
% \end{table}

% @{}lllll@{}

\begin{table}
\caption{Performance evaluation of the CT synthesis task. Boldface indicates the best results and $N_p$ is the number of projections over 180$^\circ$.}\label{tab1}

\setlength{\tabcolsep}{2.78mm}
\centering
\begin{tabular}{@{}lccccc@{}} 
\toprule
                        &  & \multicolumn{2}{c}{Gold Atlas}                        & \multicolumn{2}{c}{BRATS2018} \\ \cmidrule{3-6} 
  Method                                              & $N_p$                             &  PSNR                      & SSIM                      & PSNR          & SSIM          \\ \midrule
pix2pix \cite{isola2017image}                                      & 23                                               & 28.98$\pm1.87$               & 0.887$\pm0.019$                     & 24.01$\pm1.91$   & 0.797$\pm0.028$         \\ 
pGAN \cite{dar2019image}                                           & 23                                               & 29.19$\pm1.64$                     & 0.893$\pm0.012$                     & 24.16$\pm1.44$ & 0.810$\pm0.027$         \\ 
medSynth \cite{nie2018medical}                                     & 23                                               & 30.03$\pm1.81$                     & 0.891$\pm0.023$                   & 24.32$\pm2.06$ & 0.813$\pm0.031$         \\ 
SAGAN \cite{zhang2019self}                                         & 23                                               & 31.01$\pm2.01$                     & 0.912$\pm0.021$                    & 26.62$\pm1.97$ & 0.826$\pm0.027$         \\ \midrule
DDMM-Synth & 10            & 29.82$\pm2.10$                     & 0.861$\pm0.020$                     & 26.47$\pm1.85$         & 0.796$\pm0.025$         \\ 
           & 20            & 33.12$\pm1.99$                     & 0.936$\pm0.019$                     & 27.03$\pm1.27$         & 0.863$\pm0.021$        \\ 
           & 23            & \pmb{33.79}$\pm1.98$               & \pmb{0.941}$\pm0.019$               & \pmb{27.54}$\pm1.25$   & \pmb{0.872}$\pm0.022$  \\ \bottomrule
\end{tabular}
\end{table}

\subsubsection{Results and Discussion}
Table~\ref{tab1} lists the evaluation results. In most cases, DDMM-Synth has the best quantitative results compared to the comparison methods. We consider the case of changing the projection number at the reverse sampling stage, the synthetic CT results with different numbers of projections on the Gold Atlas project dataset can be found in Fig.~\ref{fig3}. We found our results with 20 projections are even better than the competing methods using 23 measurements. Representative target images are displayed in Fig.~\ref{fig1} and Fig.~\ref{fig2}. As can be seen, non-attentional GANs suppress noise, but sacrifice certain details and shift local structures, especially as depicted in the magnified region of interest (ROI). Attention-augmented models offer more noticeable performance benefits than non-attentional GANs, but clear artifacts remain. Compared to the baseline methods, DDMM-Synth synthesizes CT images with fewer artifacts and with more reliable structural feature recovery and higher anatomical fidelity, particularly in the ROI. The results emphasize that diffusion-based models currently have the unparalleled ability to fit data distributions and generate high-quality images. We also explore the performance of our model in noisy cases. Since Frechet Inception Distance (FID) \cite{heusel2017gans} performs better in evaluating visible noises, here we replace the metric with FID scores. Fig.~\ref{noise_case} shows the significant denoising improvement of DDMM-Synth-noise in comparison to DDMM-Synth.

\begin{figure}[!t]
\centering
\includegraphics[width=1\textwidth]{competing_method_mr_to_ct_fig_v5.png}
\caption{DDMM-Synth was compared to other baseline generative methods using the Gold Atlas project dataset.} \label{fig1}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=1\textwidth]{competing_method_multi_mri_fig_v3.png}
\caption{DDMM-Synth was compared to other baseline generative methods using the BRATS2018 dataset.} \label{fig2}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=1\textwidth,height=0.220\textwidth]{different_num_sparse_view_ct.png}
\caption{Examples of synthetic CT results with different numbers of projections on the Gold Atlas project dataset. Here we show the results with 10, 20 and 23 projections, respectively, over 180$^\circ$.} \label{fig3}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=1\textwidth,height=0.260\textwidth]{noise_case_V2.png}
\caption{DDMM-Synth-noise was able to solve noisy cases with $\mathbf{\sigma}_n$ equal to 0.1, 0.2, and 0.3 respectively. FID scores are shown at the bottom to evaluate each case.} \label{noise_case}
\end{figure}

% \begin{figure}[htp]
% \centering
% \includegraphics[width=0.995\textwidth,height=0.17\textwidth]{competing_method_multi_mri_fig.png}
% \caption{DDMM-Synth was demonstrated against other generative methods on the BRATS2018 dataset. } \label{fig2}
% \end{figure}
% The synthetic CT obtained using different methods are shown along with reference MRI image and ground truth CT image.


% Fig. \ref{} shows the results obtained using different methods. It can be seen that U-Net [35] failed to eliminate image noise completely. With DDPM, image noise was effectively removed while the structures were well kept. With only 15 NFE, the DPM-Solver could not denoise LDCT images well. Because of a too few number of sampling steps, residual noise and newly generated noise are evident in the resultant images sampled with DPM-Solver 15 NFE. Remarkably, after NFE was increased to 50, the DPM-solver delivered a competitive performance relative to that of DDPM. It can be seen that in the magnified region of interest (ROI), DPM-Solver 50 NFE well maintained the vasculature, which was blurred by U-Net. Quantitatively, DPM-Solver 50 NFE outperformed DDPM. Fig. 3 shows the absolute error maps of the abdominal results shown in Fig. 2 in reference to the NDCT image. It further demonstrates the efficient and effective sampling performance of the DPM-Solver, whose error map is the least noisy. 

% Table 1 shows the averaged quantitative evaluation of different methods over the whole test set. It can be seen that DDPM and DPM-Solver 50 NFE have the best quantitative scores. Especially, DPM-Solver 50 NFE quantitatively outperforms DDPM, which means that sampling with the DPM-Solver gives a superior noise suppression performance.

% Table 2 shows the averaged computational costs of different methods. Without any need for iterative sampling, U-Net has the highest processing efficiency. However, the denoising performance of U-Net is of limited value in clinical applications. DDPM requires a very high inference cost, which is hardly acceptable in practice. The DPM-Solver allows an excellent acceleration, while keeping or even improving the sampling quality. It can be seen that DPM-Solver 50 NFE achieves a 20× speedup compared to DDPM while maintaining denoising performance.


\subsubsection{Ablation Study}
We conducted a set of ablation studies to quantitatively assess the performance of two conditional elements and the way of inserting conditions into DDMM-Synth.
%不加MR image (unsupervised)；不加sparse-view CT condition；在U-net结构上concat上一个条件；用null-space decomposition的方法加condition
First, we investigate the benefit of adding sparse-view CT information into DDMM-Synth. The modified CT null-space content was ablated from the reverse sampling stage and the variant method was adjusted to be a pure image translation model with paired MRI images as input. Then, to examine the contribution of MRI images, we removed the conditional guidance (MRI images) during DDMM-Synth training. Third, we investigated how to utilize CT information in DDMM-Synth. As opposed to guiding from the reverse inference stage using a null-space CT measurement, we resorted to directly adding sparse-view CT information into the training phase, which means concatenating sparse-view CT images with paired MRI as input of the noise model $\epsilon_\theta$. Fig.~\ref{fig4} displays the results of the ablation studies on the MRI-CT dataset. The first two columns show the results without MRI and sparse-view CT information embedding respectively, while the third column presents the results generated by the scheme of adding both MRI and CT during training. The evaluation metrics are shown as well. From Fig.~\ref{fig4}, it can be seen that DDMM-Synth images more closely resemble real CT images and exhibit greater spatial acuity when compared to the other three variants. Note that even though the evaluation results of the variant without MRI-guidance perform well, strip artifacts introduced by sparse sampled measurement are apparent. To summarize, the results indicate that MRI can offer more texture detail when mapped to CT images while the embedding of sparse CT prior information ensures greater data consistency. In addition, the guidance mode of DDMM-Synth outperforms the conditional guidance scheme of directly concatenating two conditions.

\begin{figure}[!t]
\centering
\includegraphics[width=1\textwidth,height=0.35\textwidth]{ablation_study_v6.png}
\caption{Ablation study: DDMM-Synth was compared against three variant models. Here we show the results of two samples from the test dataset.} \label{fig4}
\end{figure}
% The synthetic samples are shown for the model trained without MRI information, the reverse inference stage with no sparse-view CT guidance and another conditional guidance scheme of directly concatenating MRI and sparse-view CT together when training the model.

\section{Conclusion}
In this study, we introduced a novel medical image synthesis approach tailored to MRI to CT translation based on a conditional diffusion model and range-null space decomposition. DDMM-Synth is able to improve the contextual mapping from MRI to CT and while preserving data consistency of CT sparse-view scanning, even for noisy cases. Our model achieves superior synthesis quality compared to recent image translation methods. DDMM-Synth holds great promise for medical image synthesis tasks.

% The reverse process of DDMM-Synth can be modified as a fast ODE solver, e.g., DPM-Solver and DPM-Solver+ or combined with DDIM for higher sampling efficiency.

\subsubsection{Limitations and Future Study}

Though our model shows superiority in CT synthesis, we should note the limitations of this method. Since MRI and CT are scanned by different devices, the image registration between them must occur first to ensure the model can accurately map tissue features from MRI to CT images. Inspired by \cite{kong2021breaking}, the addition of an image registration block into the model could provide a mitigation strategy and will be considered in future work. It is also worth noting that the inference of diffusion model is relatively slow. Our future efforts will also focus on improving the inference speed of DDMM-Synth.

% \subsubsection{Reproducibility Statement}

% e.g., DDNM can be deployed to score-based models (Song & Ermon, 2019; Song et al., 2020) or
% combined with DDIM (Song et al., 2021a) to accelerate the sampling speed.

% thereby improving capture of contextual relations while maintaining localization power.

% for accurate reserve diffusion sampling

% SynDiff achieves superior quality compared to state-of-the-art GAN and diffusion models, and it holds great promise for high-fidelity medical image translation.

% DDPM requires a very high inference cost, which is hardly acceptable in practice. The DPM-Solver allows an excellent acceleration, while keeping or even improving the sampling quality. It can be seen that DPM-Solver 50 NFE achieves a 20× speedup compared to DDPM while maintaining denoising performance.

% However, diffusion and score-matching models suffer from a well-known disadvantage: relying on a long Markov chain to generate results in a relatively slow speed.  Remarkably,



% \paragraph{Sample Heading (Fourth Level)}
% The contribution should contain no more than four levels of
% headings. Table~\ref{tab1} gives a summary of all heading levels.




% \noindent Displayed equations are centered and set on a separate
% line.
% \begin{equation}
% x + y = z
% \end{equation}
% Please try to avoid rasterized images for line-art diagrams and
% schemas. Whenever possible, use vector graphics instead (see
% Fig.~\ref{fig1}).


% \begin{theorem}
% This is a sample theorem. The run-in heading is set in bold, while
% the following text appears in italics. Definitions, lemmas,
% propositions, and corollaries are styled the same way.
% \end{theorem}
%
% the environments 'definition', 'lemma', 'proposition', 'corollary',
% 'remark', and 'example' are defined in the LLNCS documentclass as well.
%
% \begin{proof}
% Proofs, examples, and remarks have the initial word in italics,
% while the following text appears in normal font.
% \end{proof}
% For citations of references, we prefer the use of square brackets
% and consecutive numbers. Citations using labels or the author/year
% convention are also acceptable. The following bibliography provides
% a sample reference list with entries for journal
% articles~\cite{ref_article1}, an LNCS chapter~\cite{ref_lncs1}, a
% book~\cite{ref_book1}, proceedings without editors~\cite{ref_proc1},
% and a homepage~\cite{ref_url1}. Multiple citations are grouped
% \cite{ref_article1,ref_lncs1,ref_book1},
% \cite{ref_article1,ref_book1,ref_proc1,ref_url1}.
%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}
\begin{thebibliography}{10}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }
\providecommand{\doi}[1]{https://doi.org/#1}

\bibitem{zhu2018image}
Zhu, B., Liu, J.Z., Cauley, S.F., Rosen, B.R., Rosen, M.S.: Image
  reconstruction by domain-transform manifold learning. Nature
  \textbf{555}(7697),  487--492 (2018)

\bibitem{shen2019patient}
Shen, L., Zhao, W., Xing, L.: Patient-specific reconstruction of volumetric
  computed tomography images from a single projection view via deep learning.
  Nature biomedical engineering  \textbf{3}(11),  880--888 (2019)

\bibitem{wurfl2018deep}
W{\"u}rfl, T., Hoffmann, M., Christlein, V., Breininger, K., Huang, Y.,
  Unberath, M., Maier, A.K.: Deep learning computed tomography: Learning
  projection-domain weights from image domain in limited angle problems. IEEE
  Transactions on Medical Imaging  \textbf{37}(6),  1454--1463 (2018)

\bibitem{wei20202}
Wei, H., Schiffers, F., W{\"u}rfl, T., Shen, D., Kim, D., Katsaggelos, A.K.,
  Cossairt, O.: 2-step sparse-view {CT} reconstruction with a domain-specific
  perceptual network. arXiv preprint arXiv:2012.04743  (2020)

\bibitem{han2017mr}
Han, X.: {MR}-based synthetic {CT} generation using a deep convolutional neural
  network method. Medical physics  \textbf{44}(4),  1408--1419 (2017)

\bibitem{chartsias2017adversarial}
Chartsias, A., Joyce, T., Dharmakumar, R., Tsaftaris, S.A.: Adversarial image
  synthesis for unpaired multi-modal cardiac data. In: Simulation and Synthesis
  in Medical Imaging: Second International Workshop, SASHIMI 2017, Held in
  Conjunction with MICCAI 2017, Qu{\'e}bec City, QC, Canada, September 10,
  2017, Proceedings 2. pp. 3--13. Springer (2017)

\bibitem{armanious2020medgan}
Armanious, K., Jiang, C., Fischer, M., K{\"u}stner, T., Hepp, T., Nikolaou, K.,
  Gatidis, S., Yang, B.: Medgan: Medical image translation using {GAN}s.
  Computerized Medical Imaging and Graphics  \textbf{79},  101684 (2020)

\bibitem{bahrami2020new}
Bahrami, A., Karimian, A., Fatemizadeh, E., Arabi, H., Zaidi, H.: A new deep
  convolutional neural network design with efficient learning capability:
  Application to {CT} image synthesis from {MRI}. Medical Physics
  \textbf{47}(10),  5158--5171 (2020)

\bibitem{song2019generative}
Song, Y., Ermon, S.: Generative modeling by estimating gradients of the data
  distribution. Advances in Neural Information Processing Systems  \textbf{32}
  (2019)

\bibitem{ho2020denoising}
Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models.
  Advances in Neural Information Processing Systems  \textbf{33},  6840--6851
  (2020)

\bibitem{song2020score}
Song, Y., Sohl-Dickstein, J., Kingma, D.P., Kumar, A., Ermon, S., Poole, B.:
  Score-based generative modeling through stochastic differential equations.
  arXiv preprint arXiv:2011.13456  (2020)

\bibitem{song2021scorebased}
Song, Y., Sohl-Dickstein, J., Kingma, D.P., Kumar, A., Ermon, S., Poole, B.:
  Score-based generative modeling through stochastic differential equations.
  In: International Conference on Learning Representations (2021),
  \url{https://openreview.net/forum?id=PxTIG12RRHS}

\bibitem{jalal2021robust}
Jalal, A., Arvinte, M., Daras, G., Price, E., Dimakis, A.G., Tamir, J.: Robust
  compressed sensing mri with deep generative priors. Advances in Neural
  Information Processing Systems  \textbf{34},  14938--14954 (2021)

\bibitem{peng2022towards}
Peng, C., Guo, P., Zhou, S.K., Patel, V.M., Chellappa, R.: Towards performant
  and reliable undersampled {MR} reconstruction via diffusion model sampling.
  In: Medical Image Computing and Computer Assisted Intervention--MICCAI 2022:
  25th International Conference, Singapore, September 18--22, 2022,
  Proceedings, Part VI. pp. 623--633. Springer (2022)

\bibitem{chung2022score}
Chung, H., Ye, J.C.: Score-based diffusion models for accelerated mri. Medical
  Image Analysis  \textbf{80},  102479 (2022)

\bibitem{song2022solving}
Song, Y., Shen, L., Xing, L., Ermon, S.: Solving inverse problems in medical
  imaging with score-based generative models. In: International Conference on
  Learning Representations (2022),
  \url{https://openreview.net/forum?id=vaRCHVj0uGI}

\bibitem{khrulkov2022understanding}
Khrulkov, V., Oseledets, I.: Understanding {DDPM} latent codes through optimal
  transport. arXiv preprint arXiv:2202.07477  (2022)

\bibitem{croitoru2022diffusion}
Croitoru, F.A., Hondru, V., Ionescu, R.T., Shah, M.: Diffusion models in
  vision: A survey. arXiv preprint arXiv:2209.04747  (2022)

\bibitem{smith2019advanced}
Smith, L.G., Milliron, E., Ho, M.L., Hu, H.H., Rusin, J., Leonard, J.,
  Sribnick, E.A.: Advanced neuroimaging in traumatic brain injury: An overview.
  Neurosurgical Focus  \textbf{47}(6), ~E17 (2019)

\bibitem{nuchtern2015significance}
N{\"u}chtern, J.V., Hartel, M., Henes, F.O., Groth, M., Jauch, S.Y., Haegele,
  J., Briem, D., Hoffmann, M., Lehmann, W., Rueger, J.M., et~al.: Significance
  of clinical examination, {CT} and {MRI} scan in the diagnosis of posterior
  pelvic ring fractures. Injury  \textbf{46}(2),  315--319 (2015)

\bibitem{schwab2019deep}
Schwab, J., Antholzer, S., Haltmeier, M.: Deep null space learning for inverse
  problems: Convergence analysis and rates. Inverse Problems  \textbf{35}(2),
  025008 (2019)

\bibitem{wang2022gan}
Wang, Y., Hu, Y., Yu, J., Zhang, J.: {GAN} prior based null-space learning for
  consistent super-resolution. arXiv preprint arXiv:2211.13524  (2022)

\bibitem{wang2022zero}
Wang, Y., Yu, J., Zhang, J.: Zero-shot image restoration using denoising
  diffusion null-space model. arXiv preprint arXiv:2212.00490  (2022)

\bibitem{saharia2022image}
Saharia, C., Ho, J., Chan, W., Salimans, T., Fleet, D.J., Norouzi, M.: Image
  super-resolution via iterative refinement. IEEE Transactions on Pattern
  Analysis and Machine Intelligence  (2022)

\bibitem{nyholm2018mr}
Nyholm, T., Svensson, S., Andersson, S., Jonsson, J., Sohlin, M., Gustafsson,
  C., Kjell{\'e}n, E., S{\"o}derstr{\"o}m, K., Albertsson, P., Blomqvist, L.,
  et~al.: {MR} and {CT} data with multiobserver delineations of organs in the
  pelvic area — part of the {G}old {A}tlas project. Medical Physics
  \textbf{45}(3),  1295--1300 (2018)

\bibitem{boni2020mr}
Boni, K.N.B., Klein, J., Vanquin, L., Wagner, A., Lacornerie, T., Pasquier, D.,
  Reynaert, N.: {MR} to {CT} synthesis with multicenter data in the pelvic area
  using a conditional generative adversarial network. Physics in Medicine \&
  Biology  \textbf{65}(7),  075002 (2020)

\bibitem{menze2014multimodal}
Menze, B.H., Jakab, A., Bauer, S., Kalpathy-Cramer, J., Farahani, K., Kirby,
  J., Burren, Y., Porz, N., Slotboom, J., Wiest, R., et~al.: The multimodal
  brain tumor image segmentation benchmark (brats). IEEE transactions on
  medical imaging  \textbf{34}(10),  1993--2024 (2014)

\bibitem{song2020denoising}
Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. arXiv
  preprint arXiv:2010.02502  (2020)

\bibitem{isola2017image}
Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with
  conditional adversarial networks. In: Proceedings of the {IEEE} Conference on
  Computer Vision and Pattern Recognition. pp. 1125--1134 (2017)

\bibitem{dar2019image}
Dar, S.U., Yurt, M., Karacan, L., Erdem, A., Erdem, E., Cukur, T.: Image
  synthesis in multi-contrast {MRI} with conditional generative adversarial
  networks. IEEE Transactions on Medical Imaging  \textbf{38}(10),  2375--2388
  (2019)

\bibitem{nie2018medical}
Nie, D., Trullo, R., Lian, J., Wang, L., Petitjean, C., Ruan, S., Wang, Q.,
  Shen, D.: Medical image synthesis with deep convolutional adversarial
  networks. IEEE Transactions on Biomedical Engineering  \textbf{65}(12),
  2720--2730 (2018)

\bibitem{zhang2019self}
Zhang, H., Goodfellow, I., Metaxas, D., Odena, A.: Self-attention generative
  adversarial networks. In: International conference on machine learning. pp.
  7354--7363. PMLR (2019)

\bibitem{heusel2017gans}
Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans
  trained by a two time-scale update rule converge to a local nash equilibrium.
  Advances in Neural Information Processing Systems  \textbf{30} (2017)

\bibitem{kong2021breaking}
Kong, L., Lian, C., Huang, D., Hu, Y., Zhou, Q., et~al.: Breaking the dilemma
  of medical image-to-image translation. Advances in Neural Information
  Processing Systems  \textbf{34},  1964--1978 (2021)

\end{thebibliography}

%
% \begin{thebibliography}{8}
% \bibitem{ref_article1}
% Author, F.: Article title. Journal \textbf{2}(5), 99--110 (2016)

% \bibitem{ref_lncs1}
% Author, F., Author, S.: Title of a proceedings paper. In: Editor,
% F., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.
% Springer, Heidelberg (2016). \doi{10.10007/1234567890}

% \bibitem{ref_book1}
% Author, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,
% Location (1999)

% \bibitem{ref_proc1}
% Author, A.-B.: Contribution title. In: 9th International Proceedings
% on Proceedings, pp. 1--2. Publisher, Location (2010)

% \bibitem{ref_url1}
% LNCS Homepage, \url{http://www.springer.com/lncs}. Last accessed 4
% Oct 2017
%\end{thebibliography}
\end{document}
