
\section{Simulation Details}
\label{sec:attacks-details}

To complement the theoretical analysis, we conducted simulations of a PoW blockchain with bandwidth constraints. We evaluated several download rules with and without the presence of attackers. The simulations were written as event-driven simulations using Python's simpy package.\footnote{Source code: \gitSourceUrl}

Nodes in our simulation generate blocks in a Poisson process with rate proportional to their mining power. We assume the mining difficulty is fixed, and do not include any adjustment by a difficulty adjustment algorithm (DAA). In fact, DAAs tend to worsen processing problems as they increase the block creation rate if the chain does not grow fast enough---which in turn requires more download from nodes. 

Nodes process blocks one at a time according to the priority dictated by the processing policy, at a rate determined by their capacity. They are allowed to preempt their current task if new information (headers that are published, blocks that they mined) presents them with a higher priority targets. Since queues can grow large if nodes do not manage to process all blocks in a timely manner, we maintain priority queues of bounded size (typically 100) and evict low priority tasks from the queue as needed. If nodes do keep up, queues remain small, and all is well. If however queues grow large, it is usually safe to discard low priority tasks, since higher priority alternatives are arriving at a fast pace, advertised by peers that continue to mine. The high rate of incoming header announcements implies the node will never manage to process all low priority blocks unless their priority changes (in which case they will be re-advertised).

As preemption of downloads may cause nodes to alternate between downloads, we run the risk of wasting work if we discard partially processed information. We therefore allow nodes to retain partial work in an LRU cache of size 10. Cached entries allow nodes to resume processing where they left off. (We note that in practice, it may be difficult to cache information, and that in realistic settings such caching mechanisms may be targeted by an adversary that will flood nodes with incorrect information that they cannot validate prior to completing the processing of the entire block.)

Except where we note otherwise, headers are assumed to propagate instantly in the simulations. Block headers in the PoW settings contain the proof-of-work itself, which can be easily validated. We therefore assume the adversary never publishes headers it did not actually mine.
To remain close to the theoretical analysis, we model all processing tasks as dependent only on the resources available to the node itself. In reality, things are much more complex: nodes typically propagate blocks in a P2P network, which means both the overlay network topology and the underlying internet topology both greatly impact block download rates and performance. Block processing in turn, behaves differently and does not depend on the topology. With bandwidth nodes need to decide on ways to balance incoming and outgoing bandwidth between their peers, and attackers may try to isolate nodes via eclipse attacks~\cite{eclipse,eclipse2,eclipse3,eclipse4,eclipse1}. Our simplified setting allows us to focus more on the priority rules in isolation from the effects of topology and other P2P related issues that are bandwidth-specific.

\paragraph{An Example Run}
%
\Figref{experiment-trace} is an example of a trace generated by our simulation for a simple setting with only 5 nodes. The x-axis is time, and each node's timeline is represented horizontally at a different height along the y-axis. Blocks that are created are shown as squares, placed at the time of their creation, and arrows point to their parent blocks. Each block is named $h.j$ to denote that it is the $j$'th block of height $h$ to be created. 

The timeline of each node also depicts the blocks it is processing at any particular time. For example, block $1.1$ is created within the first second of the simulation by Node $3$ and other nodes begin to process it immediately. This work concludes before the next block is mined. However, that is not always the case. Block $3.2$ for example, is mined by Node $1$ at around time $4$, but a previous block at this height (block $3.1$) was mined earlier. Node $1$ had not finished validating it, and therefore did not mine on top of it. 

Finally, it is possible to see processing tasks that are preempted and resumed later. For example, Node $0$ is in the process of validating block $3.2$ when block $4.1$ is advertized. It stops its current download since $4.1$ represents a longer chain. Node $0$ resumes the download of block $3.2$ one second later. 

Each point in our simulation result graphs is typically computed from multiple repetitions of the same experiment. 
We normalize time so that a block is created by the honest nodes once every time unit in expectation. We thus consider the basic time unit as $1$ second, and the total block creation rate as $\blkratetimeHon=1$.
Each time we start the chain at the genesis block and run for a period of $5,000$ to $10,000$ seconds (depending on the experiment). Bandwidth is measured in units of blocks per second.
The standard deviation of values plotted is typically well below 1\% of the values themselves. Error bars are thus too small to properly appear in the plot, and were not added.



\subsection{Chain Growth Rate at Capacity}
\label{sec:experiment-growth}

The rate at which the chain grows without the presence of an attacker sets a bound on the security of the system: if the chain grows at a rate $\blkratetimeGrowth$, then an attacker mining at that rate or above is able to overtake the blockchain at will. 

As a baseline comparison for other simulations, we consider a a network of 100 honest nodes without the presence of an attacker, and measure the rate of growth of the chain $\blkratetimeGrowth$ in two scenarios:
(a) Non-zero header delay $\DeltaHeader>0$, and infinite processing speed $\bwtime = \infty$.
(b) No header delay $\DeltaHeader=0$, and a constant processing rate $\bwtime < \infty$.
%
The first case is equivalent to the bounded delay model, and blocks arrive at all nodes exactly $\DeltaHeader$ seconds after they are created. The second scenario is in our model and includes queueing delays of blocks.
%
To properly compare between the two models, we note that $\DeltaHeader^{-1}$ can be considered as an effective rate at which a single block is propagated. 

\import{./figures/}{fig-experiment-growth.tex}

\Figref{experiment-growth} depicts the results of the simulations. It shows that indeed as bandwidth decreases (or header propagation time increases) the chain grows at a slower pace. 
It is perhaps surprising to see that some growth of the chain occurs even when download rates are below the rate needed to download all blocks produced (\eg, a processing rate of $1/2$ allows nodes to download at most $1/2$ of the blocks that are created). The reason progress still takes place at extremely low rates is that nodes that are behind create blocks that others do not need to process, and hence their blocks, which do not contribute to the height of the chain, at least do not waste resources. 

\Figref{experiment-growth} also shows that the limited capacity case is slightly worse than the bounded delay setting for comparable delays. 
We note that in our simulation, even in the limitted capacity case, if blocks of the same height are created, the first one is advertised to all nodes instantly and is downloaded first. This results in this block being downloaded in a coordinated manner by all nodes, and thus most likely extended. This is the same as in the bounded delay setting. Queuing delays (\ie, delaying a block while we are downloading its parent) occur only on rare occasions---when a miner has single-handedly managed to mine several consecutive blocks (a rare occurence in our highly decentralized setting).





%

%

%

%



%

%


