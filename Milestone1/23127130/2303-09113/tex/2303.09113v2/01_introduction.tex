\section{Introduction}
\label{sec:introduction}

The goal of a blockchain protocol is to create a \emph{secure} and \emph{decentralized} ledger of transactions.
This protocol is run by a network of nodes, each with certain capabilities in terms of communication rates and computing power.
In this work, we study the connection between these \emph{processing capacities}
%
of individual nodes and the security of the system.
%

In order to remain secure under adversaries controlling up to $50\%$ of the network,
blockchain protocols have been parameterized 
to leave a `security margin' between the block production rate under normal operation, and each node's capacity limits.
%
%
For instance, Bitcoin only produces one block of transactions per ten minutes, though it usually only takes a few seconds for a node to download and process each block \cite{decker}.
%
%
%
%
%
%
%
%
%
%
On the other hand, protocols that push close to the limits
of their nodes
%
become insecure as the processing capacities of nodes are overwhelmed (such as Solana \cite{solanastop, solanastop2, solanastopagain}).
%
%
%
The natural question that arises is: 
%
%
given a capacity limit of nodes, what is the trade-off between the block production rate and the fraction of adversarial power that the protocol tolerates?
%
%
%
%
%
%
%

In this work, we focus on Nakamoto consensus (NC) protocols (a.k.a.\ longest chain (LC) consensus~\cite{nakamoto_paper})---a popular class of blockchain protocols
that can be instantiated using various Sybil resistance mechanisms such as proof-of-work (PoW)~\cite{nakamoto_paper,backbone} and proof-of-stake (PoS)~\cite{david2018ouroboros,badertscher2018ouroboros,sleepy,snowwhite}.
%
%
%
In NC protocols, a continuously running lottery selects which nodes can produce the next block and when they can do so.
A selected node collects pending transactions, creates a new block extending the longest chain of blocks it sees,
pushes the block's \emph{header} to the network,
and makes its transaction \emph{content} available for download.
%
%
%

In order to extend a chain, nodes must first \emph{process}, \ie, download and verify, the content of blocks in that chain, to ensure that the content is available and valid.
The lottery makes block production occur at random times, which means the download and verification load of the network is bursty. 
With limited processing capacity, during times of high load, new blocks will be \emph{queued} for processing. This leads to a queuing delay, in addition to the processing delay of 
blocks.
%
Nodes cannot produce (`\emph{mine}') new blocks extending blocks that they have not yet processed, so the growth of the honest nodes' chain slows down. Since the security of NC is based on the honest chain outgrowing any adversarial chain, 
the reduced growth of the honest nodes' chain
%
%
makes it easier for an adversary to 
%
%
%
attack the system.
%
%
%


\import{./figures/}{fig-comparison-bddelay-bdbandwidth.tex}


Moreover, the adversary can selectively delay the release of blocks that it produces, so the network load can be adversarially controlled to some extent. These blocks may not extend the longest chain or may otherwise be invalid.
Thus, to ensure that nodes can process the `most important' blocks first, they must use a `scheduling policy': given a set of new block headers, which blocks to download and verify first.
%
%
%
Since a node extends their longest chain to produce new blocks, the most obvious policy is to first process blocks along the longest chain that the node has seen. Indeed, this policy can be found in major implementations of Nakamoto consensus (Bitcoin~\cite{btcdevp2pnetworkheadersfirst}, Cardano~\cite{cardanoforkchoice3}).

%

%


%


NC security depends critically on the \emph{delay} between an honest node producing a new block and any other honest node being able to mine on top of it.
How should this delay be modeled?
To analyze the security of Nakamoto consensus, previous work~\cite{kiayias2017ouroboros, backbone, dem20, sleepy, ren, tight_bitcoin,pss16,kiffer2018better} has considered the `bounded delay' model. This model assumes that all honest nodes are able to mine on a block after a maximum delay of $\Delta$ seconds after the block is published. Using this model, the works~\cite{dem20, tight_bitcoin} give a tight characterization of the trade-off between the fraction $\beta$ of adversarial nodes and the block production rate $\blkratetime$ for a given delay bound $\Delta$. 
However, the bounded delay model assumes that the processing time of each block is bounded by $\Delta$, \emph{irrespective of the total processing load}. Thus, the model fails to capture the earlier discussed effects of queuing.
%
%

The bounded delay analysis \cite{dem20,tight_bitcoin} predicts that the \emph{private attack} is the worst-case attack strategy. That is, under parameters where the private attack fails, all other attacks fail, too. In the private attack, the adversary produces a chain of blocks that it keeps private until it becomes longer than the public longest chain, which it can then displace.
%
%
Note that during this attack, the adversary does not release any blocks.
%
In this case, 
the scheduling policy ensures that whenever an honest block is proposed at a new height, each honest node first processes that block and thereafter can produce a new block extending it. 
Therefore, under the private attack, the honest nodes' chain grows at the same rate as under the bounded delay model with $\Delta$ taken to be the time to process one block (this is further explained and experimentally validated in \cref{sec:experiments}).
Hence, the bounded delay analysis can be used to calculate the fraction of adversarial power with which the private attack succeeds, shown in \cref{fig:comparison-bddelay-bdbandwidth}.
%
%
We ask whether there are stronger attacks in which the adversary 
%
adds to the network load to increase queuing delays, an effect that was not captured by the bounded delay model.

\begin{result}
We show a \emph{\teaserattack} (\cref{sec:teaser-attack}) which is stronger than the private attack, \ie, it succeeds in regions of $(\lambda, \beta)$ where the private attack does not succeed (\cref{fig:comparison-bddelay-bdbandwidth}).
\end{result}

In this attack, the adversary `teases' honest nodes to process a longer chain it announces, but makes this effort `useless' by not releasing the block contents for the entire chain.
The adversary effectively doubles the network load and queuing delays while also building a longer chain to break security. This halves the maximum secure block rate $\lambda$.

While \emph{insecurity} can be shown through attacks in experiments,
\emph{security} (\ie, non-existence of any attacks)
cannot be shown in experiments, but requires mathematical proof.

To study queuing effects on blockchain security, we adopt the `bounded bandwidth' model proposed in \cite{bwlimitedposlc}.
As per this model, we consider the scheduling policy as a part of the protocol description as it affects the security of the protocol.
\emph{Henceforth, though we adopt the word `bandwidth', we continue to mean
`capacity' in the wider sense, \ie, rate 
limits on communication, computation,
and storage. Similarly, we use `download' to mean `process' in the
wider sense.}

\begin{result}
Using the bounded bandwidth model, we characterize a region of block mining rate $\blkratetime$ and adversarial fraction $\beta$ for which we prove that proof-of-work Nakamoto consensus, with a wide range of suitable scheduling policies, is secure (\cref{thm:safety-and-liveness-pow}). This region is shown in \cref{fig:comparison-bddelay-bdbandwidth}.
\end{result}

In proof-of-stake, the block production lottery does not depend on the block content or the parent block, to avoid grinding attacks \cite{pos_paper}. However, this allows the adversary to produce equivocations---multiple blocks with the same lottery but different content and/or parents---and send them to different nodes.
Analysis in the bounded delay model predicted that this new attack vector does not change the security region \cite{dem20,pos_consistency}.
However, since nodes cannot always predict which of two conflicting blocks will eventually be part of the chain, they may waste processing capacity on blocks that are later discarded.
Thus, the attacker has infinitely many blocks which it can use to increase the network load and queuing delays (more than in PoW), as observed in \cite{bwlimitedposlc}.

%

\begin{result}
We show an \emph{\PoSteaserattack} (\cref{sec:pos-teaser-attack}) which extends the \teaserattack using equivocations. This attack succeeds with probability $\epsilon$ if $\lambda = \Omega\left(\frac{1}{\log(1/\epsilon)}\right)$.
%
\end{result}

Therefore, in PoS, as the security error probability goes to zero, the throughput goes to zero as well.
This was not the case in PoW because the adversary had a budget constraint: blocks spent to drive up congestion for an attack today cannot be spent tomorrow, and vice versa. To re-introduce the budget constraint into PoS NC,
we propose \emph{\equivocationremoval}: nodes download at most one of possibly many equivocating blocks (\cref{sec:pos}).
Additionally, nodes use \emph{equivocation proofs} to collectively remove the content of equivocating blocks from the ledger, so that no node needs to download the equivocating blocks to form the ledger.
A deadline for considering equivocation proofs ensures that content is not removed from the ledger after it is confirmed.
We present the protocol \emph{\sapos} (\emph{\SanitizingProofOfStake}) with these modifications.

\begin{result}
\Sapos is secure for the same block production rates and adversarial fractions as PoW NC (\cref{thm:safety-and-liveness-pos}).
\end{result}

\import{./figures/}{fig-comparison-bddelay-bdbandwidth-pos.tex}

Based on \cref{thm:safety-and-liveness-pow,thm:safety-and-liveness-pos}, we calculate the bandwidth sufficient to secure PoW NC and \sapos with the parameters of major PoW/PoS blockchain implementations (\cref{fig:bitcoin-cardano-resilience-bandwidth}).

\import{./figures/}{fig-bitcoin-cardano-resilience-bandwidth.tex}

\Equivocationremoval in \sapos comes with a drawback:
At the time of block production, an honest node might not yet have learned about equivocating blocks in its prefix, and as a result might add transactions to the newly produced block that at execution turn out invalid, due to \equivocationremoval. This \emph{lack of predictable transaction validity} leads to
attacks where the adversary spams the ledger with transactions that are later invalidated.
Moreover, the funding source of such transactions may also be invalidated, so no fees can be claimed for the resources these transactions occupy.

\begin{result}
We present mechanisms (\cref{sec:throughputloss}) to ensure that miners receive fees for every transaction they include, and thus ensure that spamming the chain with invalidated transactions comes at a cost to the adversary.
\end{result}


\subsection{Related Works}
\label{sec:introduction-relatedworks}

Several earlier works have analyzed the security of 
PoW~\cite{backbone,nakamoto_paper,dem20,pss16,kiffer2018better,ren,tight_bitcoin} and 
PoS~\cite{kiayias2017ouroboros,david2018ouroboros,badertscher2018ouroboros,sleepy,snowwhite,dem20,pos_paper} Nakamoto consensus
in the bounded delay model.
Our analysis builds on tools from several of these works, primarily pivots~\cite{sleepy} (or Nakamoto blocks~\cite{dem20}), and convergence opportunities~\cite{pss16, sleepy, kiffer2018better} (or similar~\cite{dem20,ren}).

%
%
%
%
%
%
%
%


%

%
%
%
%
%
%
%
%
%
%

Limitations of the bounded delay model have been observed in previous work.
To use the bounded delay model to set the protocol's block production rate, one needs to find the value of the bound $\Delta$. 
%
This is tricky because unlike the bandwidth limit, which is a physical limit of the hardware used, delay depends on the network load.
%
One approach is to set the delay to the `time taken to process one block', \ie, $\Delta = 1/\bwtime$.
While this may be reasonable at rates much smaller than the bandwidth (as processing queues are mostly empty), queuing delay breaks this bound otherwise.
We also see that parameterizing using $\Delta = 1/\bwtime$ suffices for defending against the private attack.
For security against other attacks that may manipulate queuing delays, a more conservative approach is to set the delay to be at the tail of the probability distribution of the delay.
In theory, given an enqueuing and dequeuing process, it is possible to characterize the distribution of the queuing delay, and this approach is taken in \cite{near-optimal-thruput}.
In practice, the delay distribution can be estimated through network experiments \cite{decker,kiffer2021under}.
Another work \cite{longest-chain-random-delay} analyzes security in a random (\iid) delay model.

The problem is that the network load, hence queuing delay, is not purely a random process, but is controlled by the adversary.
Experiments cannot show us the security impact under all possible adversarial manipulations of queuing delays.
In analytical work \cite{bwlimitedposlc}, the bounded bandwidth model captures this effect.
Since the analysis in \cite{bwlimitedposlc} focuses on proof-of-stake, it allows the adversary to use infinitely many equivocations of each block it produces. Therefore, \emph{at every moment} the adversary has infinitely many blocks to add to the network load.
As a result, the earlier analysis proves security only when $\blkratetime = O\left(\frac{1}{\log(1/\epsilon)}\right)$ where $\epsilon$ is the desired maximum security error probability. 
In fact, our \PoSteaserattack succeeds with probability $\epsilon$ when $\blkratetime = \Omega\left(\frac{1}{\log(1/\epsilon)}\right)$, showing that the dependence of $\blkratetime$ on $\epsilon$ is not just a result of weak analysis.
\Cref{fig:comparison-bddelay-bdbandwidth-pos} shows the security results of \cite{bwlimitedposlc} and the parameters under which the \PoSteaserattack succeeds, as the security error probability varies.
%

Applying the analysis of \cite{bwlimitedposlc} to PoW is too pessimistic because the adversary 
%
can not repeatedly re-use
its blocks
%
at every moment. Our first improvement over \cite{bwlimitedposlc} is a new analysis technique which accounts for the budget constraint in PoW to eliminate the dependence on $\epsilon$.
In PoS however, our \PoSteaserattack shows that the dependence of block rate on the security error probability requires not just tighter analysis, but a change to protocol and/or scheduling policy. Thus, our second improvement over \cite{bwlimitedposlc} is to design \sapos which overcomes the dependence of block rate on $\epsilon$ in PoS.
It is also worth noting that while \cite{bwlimitedposlc} shows that PoS NC is insecure with the `longest header chain' download rule and secure with the `freshest block' rule, we show that both PoW NC and \sapos are secure with a wide range of download rules including the ones considered in \cite{bwlimitedposlc}. Moreover, since our \PoSteaserattack works against all those download rules, it indicates that securing PoS NC requires a change such as \equivocationremoval and not just a different download rule. See \cref{fig:comparison-bddelay-bdbandwidth-pos} to compare the security regions of the standard PoS NC protocol and our \SaPoS protocol.


%
%
%
%
%
%

%
%
%
%
%
%

%
%



%

%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%



%


%

%
%
%

%
Capacity limits apply not only to downloads, but also to computational processing of blocks.
%
For instance, to validate a block, an Ethereum validator must execute all smart contracts in it.
While download and validation are similar in that the time taken increases with the number of transactions, they are different in that validation is hard to parallelize due to transactions that depend on each other.
A line of work \cite{adding-concurrency-smart-contracts, speculative-concurrency-eth} studies methods to parallelize execution of smart contracts to 
%
make use of multicore architectures.



\subsection{Overview of Methods}
\label{sec:introduction-methods}

%
\subsubsection{New Analysis Technique}
\label{sec:introduction-methods-analysis}

%

\import{./figures/}{fig-analysis-comparison-sleepy.tex}

%
Traditional Nakamoto consensus security analysis (\cref{fig:analysis-comparison-sleepy}(a)) is based on the notion of a \emph{pivot}~\cite{sleepy} (or \emph{Nakamoto block}~\cite{dem20}).
A pivot is a point of time in which a block is produced by an honest node (\ie, it includes pending transactions) with 
an additional property that in every time interval around the pivot, there are more honest than adversarial block production opportunities.
A probabilistic argument shows that typically pivots happen frequently.
A combinatorial argument shows that the pivot block remains in the longest chains of all honest nodes forever.
%
%
%
%
Safety and liveness of NC with suitable parameters follow swiftly.

%
%
%
%
%
In the bounded delay network model,
the qualities required for the probabilistic and combinatorial argument, respectively, 
are equivalent. As a result, it has not been widely observed that these properties are actually not identical.
In the bounded bandwidth model, these properties are no longer equivalent.
Our first conceptual contribution is to
%
decompose pivots' probabilistic/combinatorial qualities into \emph{\sltpps} and \emph{\sltcps} (\cref{fig:analysis-comparison-sleepy}(b)).
\sltPps are honest block production events where in every time interval around them there are more
%
honest than adversarial block production opportunities (same as pivots in the bounded delay analysis).
\sltCps are honest block production events where in every time interval around them there are more \emph{chain growth} events than non-chain-growth events (chain growth occurs 
only when an honest block is produced \emph{and soon downloaded} by all honest nodes). 
%

Some \sltpps no longer turn into \sltcps under bounded bandwidth, because adversarial block 
%
release can delay the download of honestly produced blocks, and thus some honest block production opportunities might not translate to chain growth.
Our second technical contribution is a combinatorial argument to show that if there is a sufficiently high density of \sltpps over a long time interval, then one of these \sltpps is typically a \sltcp.
This relies on the adversary's limited budget of blocks it can spam with.

%
The original probabilistic argument of Sleepy~\cite{sleepy} guarantees only a fairly low density of \sltpps.
Thus, our third technical contribution is to show, using a Chernoff-style tail bound for weakly dependent random processes, that long time intervals typically have a high density of \sltpps.
%
This completes the analysis for PoW NC.


%
\subsubsection{\EquivocationRemoval}
\label{sec:introduction-methods-pos}

The above analysis does not hold for the vanilla PoS NC protocols in \cite{sleepy,snowwhite,kiayias2017ouroboros,bwlimitedposlc} because the analysis relies on the adversary having a limited budget of blocks that it can make honest nodes download. However, in PoS NC, the adversary has unlimited budget through equivocations.
Indeed, our \PoSteaserattack in \cref{sec:pos-teaser-attack} exploits this to show that the vanilla PoS NC is not secure under the same parameters as PoW NC.

%
%
%
%
%
%

%
%
In \sapos, we stipulate that per block production opportunity, every honest node downloads at most one block.
This makes honest nodes immune to the effects of equivocation spamming.
However, we need to ensure that honest nodes can still switch from one chain to another longer chain, both of which might contain different equivocating blocks from the same block production opportunity.
%
For this, note that headers of two equivocating blocks from the same 
block production opportunity
%
can serve as a \emph{succinct equivocation proof} to convince other nodes that an equivocation was committed.
Therefore, in \sapos, if an honest node sees an equivocation for a block in its longest chain, it publishes an equivocation proof in the block that it produces, which allows all nodes to consistently treat the equivocating block's content as \emph{empty} without downloading it.
%

%
A caveat so far is that an adversary could reveal an equivocation late and cause inconsistent ledgers across honest nodes and/or time.
%
%
To avoid this, we enforce a deadline for how late an equivocation proof can be included in the chain.
%
%
Our analysis shows how to parameterize the 
%
deadline
and the NC protocol's confirmation time such that, if any honest node has removed the content of any equivocating block on its longest chain, then an appropriate equivocation proof is timely included on-chain, and all honest nodes remove the block's content before it reaches the output ledger.


%
\subsubsection{Ensuring Fees Get Paid despite Lack of Predictable Validity}
\label{sec:introduction-methods-throughputloss}

%

%
%
%

%

%

\Equivocationremoval in \sapos leads to \emph{lack of predictable transaction validity},
\ie, honest nodes do not know whether transactions they include in their block will be valid, since the content of blocks in the prefix may later be removed due to an equivocation. 
This risks that the adversary gets to spam the ledger with invalid transactions for free. 
%
In one solution to prevent this, we focus on guaranteeing \textit{transaction fees} are always paid regardless of equivocations, by introducing 
%
%
%
%
 \emph{gas deposit accounts} that can only be used to pay transaction fees.
Any deposit to such an account takes effect only after the deadline has passed for the inclusion of any equivocation proof that might lead to removal of transactions from the deposit's prefix.
This gives honest block producers a lower bound on the account's balance 
%
which they can use to reliably determine whether a transaction can pay fees.
%
%
