{
    "arxiv_id": "2303.14116",
    "paper_title": "Improving Prediction Performance and Model Interpretability through Attention Mechanisms from Basic and Applied Research Perspectives",
    "authors": [
        "Shunsuke Kitada"
    ],
    "submission_date": "2023-03-24",
    "revised_dates": [
        "2024-01-24"
    ],
    "latest_version": 1,
    "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.MM"
    ],
    "abstract": "With the dramatic advances in deep learning technology, machine learning research is focusing on improving the interpretability of model predictions as well as prediction performance in both basic and applied research. While deep learning models have much higher prediction performance than traditional machine learning models, the specific prediction process is still difficult to interpret and/or explain. This is known as the black-boxing of machine learning models and is recognized as a particularly important problem in a wide range of research fields, including manufacturing, commerce, robotics, and other industries where the use of such technology has become commonplace, as well as the medical field, where mistakes are not tolerated. This bulletin is based on the summary of the author's dissertation. The research summarized in the dissertation focuses on the attention mechanism, which has been the focus of much attention in recent years, and discusses its potential for both basic research in terms of improving prediction performance and interpretability, and applied research in terms of evaluating it for real-world applications using large data sets beyond the laboratory environment. The dissertation also concludes with a summary of the implications of these findings for subsequent research and future prospects in the field.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.14116v1"
    ],
    "publication_venue": "The bulletin of Graduate School of Science and Engineering, Hosei University, Vol.64 (03/2023). This article draws heavily from arxiv:2009.12064, arxiv:2104.08763, arxiv:1905.07289, and arxiv:2204.11588",
    "doi": "10.15002/00026672"
}