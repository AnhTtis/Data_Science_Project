\IEEEPARstart{D}{eep} learning~\cite{lecun2015deep} is one of the machine learning (ML) models and has contributed greatly to the current development of artificial intelligence (AI).
Compared to traditional ML models such as decision trees~\cite{breiman1984classification} and support vector machines~\cite{hearst1998support}, DL models have been shown to dramatically improve prediction performance in various fields because they can automatically learn important features from data to realize a goal through the training.
DL models, which do not require knowledge about the subject, have demonstrated predictive and generative abilities beyond human capabilities, especially in the fields of computer vision (CV)~\cite{krizhevsky2012imagenet, he2016deep} and natural language processing (NLP)~\cite{vaswani2017attention, devlin2019bert}.
Since DL models will be used more frequently in the future, it is necessary for users to be able to interpret the validity of the prediction results of these models and the basis for them, in terms of the reliability and practicality of the models.

While the recent DL model has excellent prediction performance, it is difficult to interpret and explain the model prediction due to the complex architecture of the model. 
This black box and/or not transparent nature is an important issue that needs to be resolved~\cite{castelvecchi2016can}.
Explainable AI (XAI) is a field that seeks to explain the predictions of ML/DL models.
This field has been studied for more than 40 years~\cite{scott1977explanation, swartout1985explaining}, and classical XAI provided explanations based on roles constructed by humans carefully.
Recent DL models have complex neural network (NN) structures consisting of various nonlinear transformations, making a human interpretation of the internal inference process very difficult.
The interpretability of the predictions is recognized as a particularly important issue in a wide range of research fields, including manufacturing~\cite{sharp2018survey}, e-commerce~\cite{zhang2020explainable}, and robotics~\cite{karoly2020deep}, where the use of DL models is becoming common, as well as in the medical field~\cite{singh2020explainable} and autonomous driving~\cite{omeiza2021explanations}, where mistakes are not tolerated.

\subsection{Objectives and Requirements for XAI}

Explainability for ML/DL models can be taken to mean a variety of things.
In this section, we clarify our position on the interpretation and usage of these terms for ML/DL models in this work, with examples of what is stated in the literature in the related fields.
Previous studies have attempted to clarify the purpose and requirements for XAI~\cite{arrieta2020explainable, adadi2018peeking, guidotti2018survey}.
Additionally, there are examples of companies such as Google~\footnote{\url{https://cerre.eu/wp-content/uploads/2020/07/ai_explainability_whitepaper_google.pdf}} and Amazon~\footnote{\url{https://pages.awscloud.com/rs/112-TZM-766/images/Amazon.AI.Fairness.and.Explainability.Whitepaper.pdf}}, which place ML at the center of their business, discussing the provision of explanations from the standpoint of providing their systems and services.

XAI should consider the audience because the contents and details of the reasons to be presented by depending on the audience.
As a promising definition of XAI, Arrieta et al.~\cite{arrieta2020explainable} define it as follows:
\begin{quote}
    \textit{Given an audience, an explainable Artificial Intelligence is one that produces details or reasons to make its functioning clear or easy to understand.}
\end{quote}
Additionally, Arrieta et al.~\cite{arrieta2020explainable}, in light of the previous XAI research, have identified the following nine goals for the research: (1) \textbf{trustworthiness}, (2) \textbf{causality}, (3) \textbf{transferability}, (4) \textbf{informativeness}, (5) \textbf{confidence}, (6) \textbf{fairness}, (7) \textbf{accessibility}, (8) \textbf{interactivity}, (9) \textbf{privacy awareness}.

We agree with the above definition of Arrieta et al.~\cite{arrieta2020explainable} and aim to achieve the following goal for the audiences: ``\textit{building XAI that provides reasons and details that make understanding easier.}''
As described below, this work is divided into discussions of basic and applied perspectives, each of which aims to define and improve ``model interpretability'' as follows.
In the basic research perspective, the goal is to provide an interpretation that makes the basis for the predictions of multiple interpretation methods identical.
This goal focuses on (1) trustworthiness, which is the state of trust that the model will function as intended when the problem is solved, and (5) confidence, which is the state of stability in the behavior of the model.
In the applied research perspective, assuming that the audience is the operator of the service, the goal is to provide an interpretation that better supports the operator's decision-making.
This goal focuses on (4) informativeness, the state in which humans can extract the information necessary for decision-making when solving real-world problems.

Throughout this work, we will focus on the interpretation of each word in the input sentence/document in the prediction of NLP models.
By making it possible for audiences to interpret where the model contributes predictions to the input, we believe that the above definition of XAI is satisfied.
The ability to interpret the contribution of inputs from the DL model is useful for validating the model behavior, analyzing errors, and making decisions when operating the model in the real world.

\subsection{XAI Methods to Interpret the Prediction Results}

There are two major approaches to the black box nature of DL models: (1) designing transparent models~\cite{lundberg2020local,saarela2021comparison,banerjee2006convex} and (2) providing post-hoc explanations for model predictions~\cite{selvaraju2017grad,sundararajan2017axiomatic,zhang2020explainable}.
For (1), the design of transparent models involves research into understanding the architecture of the model itself~\cite{guidotti2018survey} and the learning algorithms of the model\cite{banerjee2006convex}.
In DL models, these works have limited applicability to the target model architecture, making them difficult to apply to existing models. 
They typically have limited prediction performance as they attempt to provide human-interpretable explanations.
For (2), the post-hoc explanation methods involve visualization of factors that influence predictions~\cite{simonyan2013deep, sundararajan2017axiomatic}, and providing analytical explanations with concrete examples if applicable~\cite{kenny2021explaining}.
The post-hoc approach is widely used today because it generally applies to DL models and is easier than designing transparent models.

In the following, we explain the visualization approaches of importance based on the gradient for the prediction result and the learned attention weights in the post-hoc explanation, which is nowadays the mainstream interpretation of the predictions.
First, we define the formulas that will be used throughout this work with respect to these approaches.

We consider a recurrent-neural network (RNN) model for NLP task.
The input of the model is word sequence $\bm{x} = (x_1, x_2, \cdots, x_T) \in \mathcal{V}^T$ of the length $T$ where the words taken from vocabulary $\mathcal{V}$.
The output of the model is $\hat{\bm{y}} \in \mathbb{R}^{\mathcal{C}}$ corresponds to ground truth $\bm{y} \in \mathbb{R}^{\mathcal{C}}$, where $\mathcal{C}$ is the set of class labels.
We introduce the following short notation for the word sequence $(x_1, x_2, \cdots, x_T)$ as $(x_t)_{t=1}^{T}$.
Let $\bm{w}_t \in \mathbb{R}^{d}$ be a $d$-dimensional word embedding corresponding to $x_t$.
We represent each word with the word embeddings to obtain $(\bm{w}_t)_{t=1}^{T} \in \mathbb{R}^{d \times T}$.
The word embeddings is encoded with encoder $\textbf{Enc}$ to obtain the $m$-dimensional hidden states:
\begin{equation}
    \bm{h}_t = \textbf{Enc}(\bm{w}_t, \bm{h}_{t-1}) \in \mathbb{R}^{m},
\end{equation}
where $\bm{h}_0$ is the initial hidden state, and it is regarded as a zero vector.

\subsubsection{Gradient-based Approach}

The gradient-based approach estimates the contribution of input $\bm{x}$ to ground truth $\bm{y}$ or prediction $\hat{\bm{y}}$ by computing the partial derivative of $\bm{x}$ with respect to $\bm{y}$ or $\hat{\bm{y}}$. 
Here we use the term gradient for $\partial \bm{y} / \partial \bm{x}$.
The goal of the gradient-based approach is to estimate attribution maps $\bm{g}^c = (g_i^c)_{i=1}^{T}$ for each word.
The attribution maps $\bm{g}^c$ are considered to capture the importance of each input word for a particular output class $c \in \mathcal{C}$.

Following the advent of AlexNet~\cite{krizhevsky2012imagenet}, shortly after DL first came into the limelight, an early gradient-based approach was proposed by Simonyan et al.~\cite{simonyan2013deep} and has long supported the interpretation of DL model predictions.
They used a formulation similar to the above to present the attribute map to the user as a saliency map based on its absolute value:
\begin{equation}
    \bm{g}^{c}_{\mathrm{saliency}} = \left|\frac{\partial \bm{y}}{\partial \bm{x}}\right|.
\end{equation}
The core idea of gradient-based methods is to map gradient information \textit{backward} into the input space based on labels or inference results.

Because gradient-based approaches are applicable to all DL models trained by backpropagation, they have been used for many years in various fields for insight into the internal workings of models and for error analysis~\cite{selvaraju2017grad, shrikumar2017learning, sundararajan2017axiomatic}.
Since then, gradient-based approaches such as GradCAM~\cite{selvaraju2017grad}, DeepLIFT~\cite{shrikumar2017learning}, and Integrated Gradient~\cite{sundararajan2017axiomatic} have been proposed to provide interpretations of predictions that provide deeper human insight, different from the rule-based approaches that were common during classical XAI.
There is a large body of literature claiming that gradient-based approaches can be used to explain the importance of input features~\cite{aubakirova2016interpreting,karlekar2018detecting}.

Gradient-based approaches have a variety of advantages.
First, gradient-based methods are fast and efficient in the computation of attribution maps.
It is easily scalable because it does not depend on the number of input features, and it can be easily computed with high performance with the support of deep learning frameworks.
Second, it can be applied to existing models and any network architecture with very few lines of code by overriding the gradient of nonlinearity in the computational graph.
It is easy to implement because there is no need to implement custom layers or operations.

On the other hand, there are limitations to gradient-based approaches.
The biggest problem is that the attribution maps presented are often visually noisy~\cite{smilkov2017smoothgrad}.
The gradient-based approaches assume that small changes in input features cause small changes in predictions, which may not always be the case~\cite{koh2017understanding}.
Additionally, they are sensitive to the choice of input baseline, which can affect the attribution scores~\cite{sundararajan2017axiomatic, ancona2017towards}.
The gradient-based approaches are not able to capture interactions between features and, therefore, may not provide a complete explanation of the model's predictions~\cite{fong2017interpretable}.

\subsubsection{Attention-based Approach}

In recent DLs, attention mechanisms~\cite{bahdanau2014neural,vaswani2017attention} are used to focus more \textit{attention} on specific parts of the input as the model processes it.
The effectiveness of the mechanism was initially validated in machine translation~\cite{bahdanau2014neural} and image captioning~\cite{xu2015show}, but it is now being expanded in a wide variety of tasks.
The attention mechanism works by assigning weights to each part of the input.

The key component in the attention mechanism is an attention score function $\mathcal{S}(\cdot, \cdot)$.
The score function maps a query $\bm{Q}$ and key $\bm{K}$ to attention score $\tilde{a}_t$ for the $t$-th word.
For the NLP tasks, we consider $\bm{K} = (\bm{h}_t)_{t=1}^{T}$.
The attention scores $\tilde{\bm{a}} = (\tilde{a}_t)_{t=1}^{T}$ projected to sum to 1 by a alignment function $\mathcal{A}$, which results in the attention weights $\bm{a}$:
\begin{equation}
    \bm{a} = \mathcal{A}(\bm{K}, \bm{Q}) = \phi(\mathcal{S}(\bm{K}, \bm{Q})),
\end{equation}
where $\phi$ is a projection function to probability, such as softmax.
The weight $\bm{a}$ indicates the importance of that portion to the current task.
These weights are then used to selectively focus on the most important parts of the input to generate output.
We then compute weighted instance vector $\bm{h}_{\bm{a}}$ as a weighted sum of the hidden states:
\begin{equation}
    \bm{h}_{\bm{a}} = \sum_{i=1}^{T} a_{t} \bm{h}_t
\end{equation}
Finally, $\bm{h}_{a}$ is fed to a prediction layer, which outputs a probability distribution over the set of categories $\mathcal{C}$.

The score function conventionally uses the following additive attention formulation:
\begin{equation}
    \mathcal{S}_{\mathrm{add}}(\bm{K}, \bm{Q})_i = \bm{v}^\top \tanh{(\bm{W}_1 \bm{K}_t + \bm{W}_2 \bm{Q})},
\end{equation}
where $W_1, W_2 \in \mathbb{R}^{d \times M}$ and $\bm{v} \in \mathbb{R}^{d}$ are the learnable parameters.
Currently, the Transformer and other variants often use the following scaled-dot product attention formulation:
\begin{equation}
    \mathcal{S}_{\mathrm{prod}}(\bm{K}, \bm{Q}) = \frac{\bm{K}^\top \bm{Q}}{\sqrt{m}}
\end{equation}
In this summary, each evaluation is carried out with additive attention formulation as the score function.

While the gradient-based approach can interpret the prediction of a model without any interpretation/explanation mechanism, the attention-based DL model allows for the interpretation of the model by visualizing the learned attention weights.
Explanations based on learned weights of attention mechanisms~\cite{bahdanau2014neural} can be shown in a \textit{forward} direction for DL model inputs, giving a clear explanation that appeals to human intuition.
Since the attention mechanism introduced in conventional RNN is generally placed before the last layer (or the last layer of the encoder in encoder-decoder architecture), the attention weights are learned to emphasize the part that contributes to the final prediction.
For the Transformer model~\cite{vaswani2017attention}, which has been attracting much interest in recent years, attention rollout~\cite{abnar2020quantifying} has been proposed, which visualizes the result of the matrix product of the attention scores.

\subsection{Discussion on Post-hoc Explanation Approaches}

Although the attention mechanism contributes significantly to predictive performance and model interpretability, researchers find that the mechanism still suffers in the predictive interpretation of the model.
In a claim that surprised the field, Jain and Wallace~\cite{jain2019attention} argued that ``attention is not an explanation.''
They reported the following two major problems in a simple RNN-based model with the attention mechanism, especially by NLP task:
(1) There is not necessarily a strong correlation between the regions estimated to be important by attention weights and those obtained by gradients.
(2) Small perturbations to the attention mechanism lead to unintended predictive changes, and those adversarial perturbations that deceive the mechanism lead to large predictive errors.
For (1), the authors reported that Kendall's rank correlations~\cite{kendall1938new} of importance indicated by the learned attention weight and word importance calculated by the gradient show almost no correlation.
For (2), they found that small perturbations to the attention mechanisms lead to unintended prediction changes, while adversarial perturbations that deceive the mechanism lead to large prediction errors.

Against the weak explanatory nature of the attention mechanisms reported above, refutational analyses and methods to overcome them have been proposed in recent years~\cite{wiegreffe2019attention,serrano2019attention,grimsley2020attention,wang2016attention}.
On the analytical side, Wiegreffe and Pinter~\cite{wiegreffe2019attention} argued that Jain and Wallace~\cite{jain2019attention}'s claim depends on the definition of explanation and that testing it requires considering different aspects of the model with attention mechanisms in a rigorous experimental setting.
Furthermore, similar to Wiegreffe and Pinter~\cite{wiegreffe2019attention}, several studies argue that the effects of explanations by attention mechanisms vary depending on the definition of explanatory properties~\cite{serrano2019attention, grimsley2020attention, wang2020gradient}.
With regard to improving the interpretability of the attention mechanism, there are some studies that re-examined the structure of the mechanism itself~\cite{mrini2020rethinking} or proposed a training technique that makes the attention mechanism robust to perturbations \textit{indirectly}~\cite{bento2021timeshap}.
On the other hand, to our knowledge, no technique has been proposed to \textit{directly} address the vulnerability of attention mechanisms to perturbation, as pointed out in Jain and Wallace~\cite{jain2019attention}.

In the light of the definition of XAI in the Arrieta et al.~\cite{arrieta2020explainable}, if each interpretation approach provides a different interpretation, it will have a negative impact on the audience, especially in the trustworthiness and confidence.
As described above, various studies have been conducted on the interpretability of DL models, focusing on the gradient- and attention-based approaches. 
However, a comprehensive synthesis of these multiple approaches to obtain reliable and robust interpretations is an important basic research direction, and there remains room for further research.

\subsection{Interpreting Deep Learning Models in Real-World Applications}

While the above discussion focused on the interpretation of DL models in the basic research perspectives, this section describes the provision of interpretation in DL models that are operated in the real world.
To begin with, we emphasize that there are still few studies that have validated using real-world datasets under a practical situation.
We speculate that this may be because information about applications in the real world is often confidential.
Therefore, DL-based models, including attention-based models, have been developed in fields such as machine translation~\cite{bahdanau2014neural, vaswani2017attention}, machine reading comprehension~\cite{devlin2019bert}, and image classification~\cite{dosovitskiy2020image}.
However, the development and evaluation of such models have been carried out on relatively small and well-organized data sets, limited to so-called laboratory environments.
One of the root causes of the limitation is the lack of publicly available dataset, which is important for the research area.
The effectiveness of DL models outside of these data-available areas remains to be developed.
So far, attention-based DL models have been reported to perform well on various tasks.
On the other hand, it remains unclear how the DL models perform on noisier, more imbalanced, and diverse real-world data that may deviate from the benchmark dataset.

The literature evaluating its interpretability in DL models operating in the real world is even less extensive than the literature evaluating its performance on real-world datasets.
According to Arrieta et al.~\cite{arrieta2020explainable} and the industry tech giants, XAI is expected to be an AI that provides such audience/operators with details and reasons to clarify their own behavior or to facilitate understanding.
Furthermore, there is a certain need to build a DL model that can provide high prediction performance and prediction evidence, which have not been achieved by conventional ML models.
In sum, there is currently limited research on developing models that can be interpreted for practical use of large real-world data.
This point remains an important applied research issue.

\subsection{The Structure of the Dissertation}

The remainder of the dissertation on which this bulletin is based is organized as follows.
The first two are chapters on aspects of basic research, and the next two are chapters on aspects of applied research. 

\begin{itemize}
    \item \textbf{Chapter 2} describes adversarial training for attention mechanisms based on~\cite{kitada2021attention}.
    \item \textbf{Chapter 3} describes virtual adversarial training for attention mechanisms based on~\cite{kitada2022making}.
    \item \textbf{Chapter 4} describes conversion prediction for ad creatives based on~\cite{kitada2019conversion}.
    \item \textbf{Chapter 5} describes discontinuation prediction for ad creatives based on~\cite{kitada2022ad}.
    \item \textbf{Chapter 6} provides a discussion of the applicability, interpretability, and development of the proposed method throughout the dissertation.
    \item \textbf{Chapter 7} contains the conclusion section of the dissertation.
\end{itemize}