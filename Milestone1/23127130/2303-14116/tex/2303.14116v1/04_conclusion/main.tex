This summary provided an overview of the author's dissertation.
The dissertation discussed both basic and applied research on how attention mechanisms improve the performance and interpretability of machine learning models, especially in NLP-related tasks.
Focusing on the black-box nature of deep learning models, which have recently achieved significant results in various fields, we attempted to develop new training techniques and models that help to interpret the prediction results for the input words.
For the further development of applications of deep learning models, this black box nature is a serious barrier to analyzing the behavior of the models and using them in situations where prediction failures are not tolerated.
To the best of our knowledge, where there is a trade-off between prediction performance and interpretability, we have pioneered a new technique that aims to improve it.

Although the proposals within the dissertation are mainly validated using RNN models, we believe that the proposed methods are generic and can be applied to all models with attention mechanisms that will emerge in the future.
In interpreting the prediction evidence for each input word, we discussed the application of the proposals which entail a certain degree of interpretability.
Our ideas provide a positive effect on those that follow, and inform the emergence of further basic research into the development of the ideas, as well as efforts to support real-world operations.

Because there are several other options for providing explanations to DNNs besides attention mechanisms, we will confirm the applicability of the ideas in this method to these options in the future. 
Additionally, we will discuss the interpretability of the models in business applications, such as recommendation systems, to see if our method can be applied to such systems.