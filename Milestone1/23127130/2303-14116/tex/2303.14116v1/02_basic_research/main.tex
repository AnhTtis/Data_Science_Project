In terms of the basic research for this work, we describe the vulnerability of the attention mechanism, which is essential to DL models, to perturbations and countermeasures against it.
We further discuss the interpretability of our technique after its application.
The majority of prior studies have suggested that interpretation is possible by investigating where the attention mechanisms assign large weight to the model inputs.
On the other hand, we believed that the mechanism was vulnerable to noise, which would negatively affect prediction performance and interpretability.

To overcome the above challenges, a natural idea is: to introduce adversarial perturbation to the attention mechanism that is vulnerable to perturbation so that it learns to be robust to noise.
We focus on adversarial training (AT)~\cite{goodfellow2014explaining} to deal with adversarial examples~\cite{szegedy2013intriguing} that produce inaccurate model output.
AT~\cite{goodfellow2014explaining} was first proposed in the field of image recognition as a method to overcome the weak point where the model can be fooled by input small noise/perturbation that is imperceptible to humans.
Since then, AT has been introduced in the NLP field and has demonstrated its usefulness in areas which is difficult for the DL model to predict, even when the text is mixed with metaphorical expressions that are understandable to humans (e.g., fake news detection~\cite{tariq2022adversarial}).
While AT in NLP is often applied to the input space, its effects when applied to attention mechanisms remain unclear.

The basic research perspective of this work proposes a new training technique that focuses on the vulnerability to perturbations of the attention mechanism and contributes significantly to prediction performance and prediction interpretation.
We consider using AT, in which perturbations are applied to deceive the mechanisms, exploiting adversarial perturbations.
By employing the proposed technique, DL model will be trained to pay stronger attention to areas that are more important for prediction, which is expected to not only improve prediction performance but also improve model interpretability.

\input{figures/tex/attention_at.tex}

\subsection{AT for Attention Mechanism}\label{sec:at_for_attention}

Inspired by AT~\cite{goodfellow2014explaining}, a powerful regularization technique for enhancing model robustness, we aim to overcome the vulnerability of the attention mechanism to perturbations.
We propose a general training technique for natural language processing tasks, including AT for attention (Attention AT) and more interpretable AT for attention (Attention iAT) as shown in Fig.~\ref{fig:kitada2021attention/figure1}. 
The proposed techniques improved the prediction performance and the model interpretability by exploiting the mechanisms with AT. 
In particular, Attention iAT boosts those advantages by introducing adversarial perturbation, which enhances the difference in the attention of the sentences.
Evaluation experiments with ten open datasets revealed that AT for attention mechanisms, especially Attention iAT, demonstrated (1) the best performance in nine out of ten tasks and (2) more interpretable attention (i.e., the resulting attention correlated more strongly with gradient-based word importance) for all tasks.
Additionally, the proposed techniques are (3) much less dependent on perturbation size in AT.

\input{figures/tex/attention_vat.tex}

\subsection{VAT for Attention Mechanism}\label{sec:vat_for_attention}

AT has successfully reduced the disadvantage of being vulnerable to perturbations to the attention mechanisms by considering adversarial perturbations. 
However, this technique requires label information, and thus, its use is limited to supervised settings. 
We explore the approach of incorporating virtual AT (VAT)~\cite{miyato2018virtual} into the attention mechanisms, by which adversarial perturbations can be computed even from unlabeled data.
To realize this approach, we propose two general training techniques, namely VAT for attention mechanisms (Attention VAT) and ``interpretable'' VAT for attention mechanisms (Attention iVAT), which extend AT for attention mechanisms to a semi-supervised setting, as shown in Fig.~\ref{fig:kitada2022making/figure1}.
In particular, Attention iVAT focuses on the differences in attention; thus, it can efficiently learn clearer attention and improve model interpretability, even with unlabeled data. 
Empirical experiments based on six public datasets revealed that our techniques provide better prediction performance than conventional AT-based as well as VAT-based techniques, and stronger agreement with evidence that is provided by humans in detecting important words in sentences. 
Moreover, our proposal offers these advantages without needing to add the careful selection of unlabeled data. 
That is, even if the model using our VAT-based technique is trained on unlabeled data from a source other than the target task, both the prediction performance and model interpretability can be improved.