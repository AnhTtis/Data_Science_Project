% \documentclass{article}
\section{
		Appendix
	}
% \begin{document}
% \maketitle
% \thispagestyle{empty}
\appendix
\section{Datasets}
	\begin{figure}[!t]
		\begin{subfigure}{0.48\linewidth}
			\includegraphics[width=1.0\textwidth]{figure/gl_en.pdf}
% 			\vspace{-1.5 em}
			\caption{Global encoder.}
% 			\label{fig:gmm-vis}
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.48\linewidth}
			\includegraphics[width=1.0\textwidth]{figure/gl_de.pdf}
% 			\vspace{-1.5 em}
			\caption{Global decoder.}
% 			\label{fig:lpc-vis}
		\end{subfigure}
% 		\vspace{-0.5 em}
		\caption{Details of the global transformer.}
		\vspace{-1.5 em}
		\label{fig:global_transformer}
	\end{figure}
	
	\noindent\textbf{3DPW.}
	3DPW~\cite{3DPW} is a challenging in-the-wild consisting of 60 videos, which are captured by a phone at 30 fps. Moreover, IMU sensors are utilized to obtain the near ground-truth SMPL parameters, \ie, pose and shape. We utilize the official split to train and test our model, where the training, validation, and test sets are comprised of 24, 12, and 24 videos, respectively. For evaluation, we report MPVPE on 3DPW because it has ground-truth shape annotation.

	\noindent\textbf{Human3.6M.}
	Human3.6M~\cite{H36M} is a large-scale dataset collected under a controlled indoor environment and includes 3.6M video frames. Folloing~\cite{TCMR,MPS-net}, we train the model on 5 subjects (i.e., S1, S5, S6, S7, and S8) and test it on 2 subjects (i.e., S9 and S11). We set the frame rate of the dataset to 25 fps for training and testing.
	
	\noindent\textbf{MPI-INF-3DHP.}
    MPI-INF-3DHP~\cite{MPII3D}] is a complex dataset captured at indoor and outdoor scenes with a markerless motion capture system. The 3D human pose annotations are computed by the multiview method.
    The training and testing sets are comprised of 8 and 6 subjects, respectively. Each subject has 16 videos captured in the indoor or outdoor environment. The total video frames are 1.3M.
    Following previous works~\cite{TCMR, MPS-net}, we utilize the official training and testing split. 

	\noindent\textbf{InstaVariety.}
	InstaVariety is a 2D human pose dataset collected from Instagram. It consists of 28K videos, and the video length is an average of 6 seconds. The 2D annotation is generated from Openpose~\cite{ZheCao2018OpenPoseRM}. Following~\cite{TCMR, MPII3D}, we use this dataset for training.
	
	
	\noindent\textbf{PoseTrack.}
    PoseTrack~\cite{PoseTrack} is also a 2D human dataset for multi-person pose estimation and tracking, which consists of 1.3K videos. Following~\cite{TCMR}, we use 792 videos for training. 
    
    \begin{figure}[!t]
		\includegraphics[width=0.4\textwidth]{figure/local_trans.pdf}
% 		\vspace{-2.0 em}
		\caption{Details of the local transformer.}
% 		\vspace{-1.0 em}
		\label{fig:local_transformer}
	\end{figure}
	
	\begin{table}[!t]
		\small
		\setlength{\tabcolsep}{5. pt}
		\begin{center}
			\begin{tabular}{l | c | c | c | c}
				%\specialrule{0pt}{.05em}{.05em}
				\toprule[2pt]
				\normalsize
				 & PA-MPJPE$\downarrow$ & MPJPE$\downarrow$ & MPVPE$\downarrow$ & Accel$\downarrow$ \\
				\midrule[1pt]
				 $w/o~Detach$ & 50.9 & 81.2 & 96.4 & 6.6 \\
				\cellcolor{Gray}$w/~Detach$ & \cellcolor{Gray}\textbf{50.6} & \cellcolor{Gray}\textbf{80.7} & \cellcolor{Gray}\textbf{96.3} & \cellcolor{Gray}\textbf{6.6} \\
				\bottomrule[1pt]
				%\specialrule{0pt}{.05em}{.05em}
			\end{tabular}
		\end{center}
		\vspace*{-1.8 em}
		\caption{Gradient detachment}
		\vspace*{-.5 em}
		\label{tab:gradient}
	\end{table}

	\begin{table}[!t]
		\vspace*{-.5 em}
		\small
		\setlength{\tabcolsep}{6. pt}
		\begin{center}
			\begin{tabular}{l | c | c | c | c}
				%\specialrule{0pt}{.05em}{.05em}
				\toprule[2pt]
				\normalsize
				 & PA-MPJPE$\downarrow$ & MPJPE$\downarrow$ & MPVPE$\downarrow$ & Accel$\downarrow$ \\
				\midrule[1pt]
				Fix & 52.1 & 83.4 & 99.5 & 6.4 \\
				\cellcolor{Gray}Learnable& \cellcolor{Gray}\textbf{50.6} & \cellcolor{Gray}\textbf{80.7} & \cellcolor{Gray}\textbf{96.3} & \cellcolor{Gray}\textbf{6.6} \\
				\bottomrule[1pt]
				%\specialrule{0pt}{.05em}{.05em}
			\end{tabular}
		\end{center}
		\vspace*{-1.8 em}
		\caption{Position embedding}
		\vspace*{-2.2em}
		\label{tab:position}
	\end{table}

\begin{figure*}[!t]
    \centering
    \begin{subfigure}{0.32\linewidth}
    \animategraphics[width=5.4cm,height=5.68cm, autoplay, loop]{2}{gif_1/our/000}{01}{16}
        \caption{\small Ours}
        
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
    \animategraphics[width=5.4cm,height=5.68cm, autoplay, loop]{2}{gif_1/mps/000}{01}{16}
        \caption{\small MPS-Net~\cite{MPS-net}}
        
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
    \animategraphics[width=5.4cm,height=5.68cm, autoplay, loop]{2}{gif_1/tcmr/000}{01}{16}
        \caption{\small TCMR~\cite{TCMR}}
        
    \end{subfigure}
    \vspace{-0.5 em}
    \caption{Comparison with other methods~\cite{TCMR, MPS-net}. \textbf{Please use Adobe Acrobat to view it.}}
    \label{fig:more_1}
    \vspace{-0.5 em}
\end{figure*}

\begin{figure*}[!t]
    \centering
    \animategraphics[width=17cm,height=4.8cm, autoplay, loop]{2}{gif_2/0000}{00}{20}
    \vspace{-0.5 em}
    \caption{An Example of internet video. We sample every ten frames. \textbf{Please use Adobe Acrobat to view it.}}
    \label{fig:more_2}
    \vspace{-0.5 em}
\end{figure*}

\section{Model details}
    \noindent\textbf{Global transformer.}
    The model details are shown in Figure~\ref{fig:global_transformer}. We utilize two layers of the encoder block with 512 dimensions. In the global decoder, we only apply one layer of the decoder block with 256 dimensions. The position embedding is learnable.
	


	
	\noindent\textbf{Local transformer.}
	As shown in Figure~\ref{fig:local_transformer}, the encoder block is similar to the global encoder. We set three layers of the encoder block with 256 channel sizes. In addition, we employ cross-attention to the decoder and set the layer to one. The channel size is the same as the encoder.


\begin{figure*}[!t]
    \centering
    \animategraphics[width=17cm,height=4.8cm, autoplay, loop]{2}{gif_3/00}{01}{19}
    \vspace{-0.5 em}
    \caption{An Example of internet video. \textbf{Please use Adobe Acrobat to view it.}}
    \vspace{-0.5 em}
    \label{fig:more_3}
\end{figure*}
\section{Effect of gradient detachment}
Table~\ref{tab:gradient} shows the effect of gradient detachment. 
When we do not backward propagate the path of global estimation to HSCR, GLoT achieves the best performance. It is intuitively reasonable that fixing one is easier for optimization. In addition, $w/o~Detach$ also obtains good results. 

\section{Effect of position embedding}
In Table~\ref{tab:position}, we report the results of the different types of position embeddings. The learnable embedding obtains the best performance. 

\begin{figure}[!t]
		\begin{subfigure}{0.625\linewidth}
			\includegraphics[width=1.0\textwidth]{figure/failure_1.pdf}
			\vspace{-1.5 em}
% 			\caption{An Occlusion case}
			\label{fig:failure_1}
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.335\linewidth}
			\includegraphics[width=1.0\textwidth]{figure/failure_2.pdf}
			\vspace{-1.5 em}
% 			\caption{Our results via LPC. }
			\label{fig:failure_2}
		\end{subfigure}
		\vspace{-0.5 em}
		\caption{Some failure cases.}
		\vspace{-0.5 em}
		\label{fig:failure}
\end{figure}

\section{Inference time (GPU: V100) and MACs}

\begin{table}[htb]
	\small
	\vspace*{-0.75 em}
	\setlength{\tabcolsep}{6 pt}
	\begin{center}
		\begin{tabular}{l | c | c | c }
			%\specialrule{0pt}{.05em}{.05em}
			\toprule[2pt]
			\normalsize
			Model & MACs (M) & Time (ms) & PA-MPJPE\\
			\midrule[1pt]
			TCMR & 861.8 & \textbf{11.7} & 52.7\\
			\cellcolor{Gray}MPS-Net & \cellcolor{Gray}318.4 & \cellcolor{Gray}17.6  & \cellcolor{Gray}52.1\\
			Ours w/ Residual & \textbf{287.9} & 13.0 & 51.5\\
			\cellcolor{Gray}Ours w/ HSCR & \cellcolor{Gray}288.1 & \cellcolor{Gray}16.2 & \cellcolor{Gray}\textbf{50.6}\\
			\bottomrule[1pt]
			%\specialrule{0pt}{.05em}{.05em}
		\end{tabular}
	\end{center}
	\vspace*{-1.5 em}
	\caption{Inference time and MACs.}
	\vspace*{-1. em}
	\label{tab:infer}
\end{table}

We provide the results of inference time and MACs in Table~\ref{tab:infer}. 
Our model achieves the lowest MACs. 
For inference time, our model (w/ HSCR) is slower than TCMR~\cite{TCMR} but faster than the previous SOTA method MPS-Net~\cite{MPS-net}. We analyze that the reason for slower than TCMR is the self-attention mechanism used in our model and MPS-Net.
Although our model (w/ HSCR) is slower than TCMR by 4.5 ms, it shows a significant improvement in PA-MPJPE.
Besides, we provide the inference time of our model (w/ Residual) for comparing the time consumption of the HSCR. 
It is worth noting that our model (w/ Residual) reduces 1.2 PA-MPJPE with a time consumption of only 1.3 ms compared with TCMR.


\section{Input length of the global encoder}
\begin{table}[htb]
	\small
	\vspace*{-1.5 em}
	\setlength{\tabcolsep}{6.5 pt}
	\begin{center}
		\begin{tabular}{l | c | c | c | c}
			%\specialrule{0pt}{.05em}{.05em}
			\toprule[2pt]
			\normalsize
			length & PA-MPJPE$\downarrow$ & MPJPE$\downarrow$ & MPVPE$\downarrow$ & Accel$\downarrow$ \\
			\midrule[1pt]
			32 & 51.2 & 82.0 & 98.3 & 6.7 \\
			\cellcolor{Gray}24 &
			\cellcolor{Gray}51.2 & \cellcolor{Gray}82.7 & \cellcolor{Gray}98.5 & \cellcolor{Gray}6.7 \\
			16 & \textbf{50.6} & \textbf{80.7} &
			\textbf{96.3} & 
			\textbf{6.6} \\
			\bottomrule[1pt]
			%\specialrule{0pt}{.05em}{.05em}
		\end{tabular}
	\end{center}
	\vspace*{-1.5 em}
	\caption{Input length of the global encoder.}
	\vspace*{-1. em}
	\label{input_length}
	
\end{table}
% （16帧 常用，不同xx 有价值 ）

Although the 16-frame input length is commonly used in this task, we consider that exploring more input lengths is valuable.
In Table~\ref{input_length}, we supply the study of longer input lengths, 24 and 32.
The 16-frame setting achieves the best results. A possible reason is that our lightweight global encoder can not sufficiently model longer temporal relations. 
%For improving the longer input length, we need to adjust some parameter of our structure.



\section{More qualitative results}
We show the comparison results with other methods in Figure~\ref{fig:more_1}. We observe (1) The results of MPS-Net~\cite{MPS-net} suffer from insufficient local details. (2) The results of TCMR~\cite{TCMR} do not capture the actual human global location of the frames. Figure~\ref{fig:more_2} and~\ref{fig:more_3} are multi-person internet videos, we first use a multi-object tracker to process videos and then utilize our method for each tracked person, following the previous methods~\cite{TCMR, MPS-net}.

\section{Failure cases}

As shown in Figure~\ref{fig:failure}, we provide some failure cases, mainly including occlusion. We divide the occlusion into two types, \ie, object occlusion~(Left Figure) and truncation of the frame~(Right Figure, some joints are outside of the frame). 
We consider that these cases are caused by long-term occlusion, which means the input frames are all occluded by the object or truncated by the camera, leading to failures in temporal modeling. 
% These problems may be solved by sparsely sampling the input frames. 
% We will explore the sampling strategy 
% The solution could be to increase the length of input frames and sparsely sample frames instead of continuously sampling. In addition, the work~\cite{GeorgiosPavlakos2020HumanMR} may be a good method to process the truncation of humans like the Right Figure.

% \section{More qualitative results}
% Figure~\ref{fig:more_1} shows qualitative results compared with other methods~\cite{TCMR,MPS-net}.
\section{Future works}
We plan to use this framework in similar tasks, \ie, hand pose and shape estimation~\cite{JavierRomero2017EmbodiedHM, zimmermann2019freihand}. This task will provide a more robust hand representation for downstream tasks, \eg, sign language recognition~\cite{XiaolongShen2022StepNetSP}.
Moreover, we believe that exploring multi-person interaction in a video would be a good idea. While there are some methods in image-based tasks to deal with occlusion problems caused by multiple people, video-based methods in this area are still unexplored.

% \end{document}


% \end{document}
