% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{animate}
% \usepackage{graphicx}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[accsupp]{axessibility}
\usepackage{multirow}
\usepackage{color, colortbl}
\definecolor{Gray}{gray}{0.9}
% \def\eg{\textit{e.g.}} 
% \def\ie{\textit{i.e.}} 
% \def\etal{\textit{et~al.}} 
% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}

\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{252} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}
	
	%%%%%%%%% TITLE - PLEASE UPDATE
	\title{
	
		Global-to-Local Modeling for Video-based 3D Human Pose and Shape Estimation
	}
	
	\author{
		Xiaolong Shen$^{1,2*}$, Zongxin Yang$^{1}$, Xiaohan Wang$^1$, Jianxin Ma$^2$, Chang Zhou$^2$, Yi Yang$^1$ \\
		$^1$ ReLER, CCAI, Zhejiang University \space \space \space $^2$ DAMO Academy, Alibaba Group \\
	}
	
	\maketitle
	
	
	
	
	%%%%%%%%% ABSTRACT
	\begin{abstract} 
		Video-based 3D human pose and shape estimations are evaluated by intra-frame accuracy and inter-frame smoothness.
		Although these two metrics are responsible for different ranges of temporal consistency, existing state-of-the-art methods treat them as a unified problem and use monotonous modeling structures (\textit{e.g.}, RNN or attention-based block) to design their networks. However, using a single kind of modeling structure is difficult to balance the learning of short-term and long-term temporal correlations, and may bias the network to one of them, leading to undesirable predictions like global location shift, temporal inconsistency, and insufficient local details. 
		To solve these problems, we propose to structurally decouple the modeling of long-term and short-term correlations in an end-to-end framework, Global-to-Local Transformer (GLoT). 
		First, a global transformer is introduced with a Masked Pose and Shape Estimation strategy for long-term modeling. The strategy stimulates the global transformer to learn more inter-frame correlations by randomly masking the features of several frames.
		Second, a local transformer is responsible for exploiting local details on the human mesh and interacting with the global transformer by leveraging cross-attention. 
		Moreover, a Hierarchical Spatial Correlation Regressor is further introduced to refine intra-frame estimations by decoupled global-local representation and implicit kinematic constraints. 
		Our GLoT surpasses previous state-of-the-art methods with the lowest model parameters on popular benchmarks, \textit{i.e.}, 3DPW, MPI-INF-3DHP, and Human3.6M. Codes are available at \url{https://github.com/sxl142/GLoT}.
		
	\end{abstract}
	\renewcommand{\thefootnote}{*}
	\footnotetext{This work was done during an internship at Alibaba.}
	%%%%%%%%% BODY TEXT
	\section{Introduction}
	\label{sec:intro}
	%as a human motion explainor,
	%Following the previous works~\cite{TCMR, MPS-net}, we predict the mid frame parameter when takeing sevral frames in the model. 
	%The task is to give human mesh when taking a single image or video sequence. 
	\begin{figure}[!t]
		\centering
		\begin{subfigure}{1\linewidth}
			\centering
			\includegraphics[width=0.9\textwidth]{figure/tcmr_fig.pdf}
			\vspace{-0.7 em}
			\caption{TCMR~\cite{TCMR} results. Global location shift, shifting to the left.}
			
			\label{fig:tcmr-vis}
		\end{subfigure}
		\vfill
		\begin{subfigure}{1\linewidth}
			\includegraphics[width=0.9\textwidth]{figure/mps_fig.pdf}
			\vspace{-0.7 em}
			\centering
			\caption{MPS-Net~\cite{MPS-net} results. Insufficient local details.}
			\label{fig:mps-vis}
		\end{subfigure}
		\vfill
		\begin{subfigure}{1\linewidth}
			\includegraphics[width=0.9\textwidth]{figure/ours_fig.pdf}
			\vspace{-0.7 em}
			\centering
			\caption{Our results.}
			\label{fig:our-vis}
		\end{subfigure}
		\vspace{-1.7 em}
		\caption{Our motivation. With the help of global-local cooperative modeling, our results avoid the global location shift and complement local details on intra-frame human meshes.}
		\vspace{-1.8 em}
		\label{fig:motivation}
	\end{figure}
	

	% Automatically recovering a human mesh sequence from a monocular video plays a crucial role in various applications, \ie, AR/VR, Robotics, and computer graphics. This technology can reduce the need for motion capture devices and manual 3D annotations, providing abundant human motion templates from outdoor scenes for downstream tasks, \eg, Animation of 3D Avatars. The typical procedure for video-based human mesh recovery is to predict parametric model parameters, \eg, SMPL~\cite{SMPL}, which indirectly controls the generation of the human mesh. 

        Automatically recovering a sequence of human meshes from a monocular video plays a pivotal role in various applications, \eg, AR/VR, robotics, and computer graphics. This technology can potentially reduce the need for motion capture devices and manual 3D annotations, providing human motion templates for downstream tasks, \eg, the animation of 3D avatars. By utilizing parametric human models (\ie, SMPL~\cite{SMPL}) with well-defined artificial joint and shape structures, the popular procedure for video-based human mesh recovery involves indirectly regressing the SMPL parameters. 
        However, effectively integrating deep neural networks with parametric artificial models to leverage multi-knowledge representations~\cite{yang2021multiple} for better estimation accuracy still remains an open problem.
        % However, how to effectively combine deep neural networks with parametric artificial models and construct superior multi-knowledge representations~\cite{yang2021multiple} still remains an open problem.

	% Unlike image-based human mesh recovery, video-based human mesh recovery is a temporal understanding problem that requires not only intra-frame accuracy but also inter-frame smoothness. 
 In video-based human mesh recovery, temporal understanding poses a crucial challenge that necessitates maintaining both intra-frame accuracy and inter-frame smoothness. 
 Previous methods~\cite{VIBE,TCMR,MPS-net} mainly design deep networks to model long-term and short-term correlations simultaneously. For instance, VIBE~\cite{VIBE} utilizes Recurrent Neural Network (RNN)~\cite{GRU} to model correlations. TCMR~\cite{TCMR} and MPS-Net~\cite{MPS-net} consist of a temporal encoder and a temporal integration. The temporal encoder includes two types, RNN-based or attention-based methods, while the temporal integration employs attentive methods, \ie, one-step or multi-step integration, to aggregate the representation extracted by the temporal encoder.
	
	\begin{figure}[!t]
		\centering
		\includegraphics[width=0.4\textwidth]{figure/parameters.pdf}
		\vspace{-0.7 em}
		\caption{
			Model parameters vs. Performance.
		}
		\vspace{-0.8 em}
		\label{fig:para}
		\vspace{-1.0 em}
	\end{figure}
	
	Even though these methods improve intra-frame accuracy or inter-frame smoothness to some extent, designing a coupled model to handle different ranges of temporal consistency, \ie, long-term and short-term~\cite{yang2021collaborative,yang2021associating}, is ineffective and inefficient for this task, as shown in Figure~\ref{fig:para}.
	We empirically find that the single type of modeling structures leads to undesirable predictions as shown in Figure~\ref{fig:motivation}. For instance, RNN-based TCMR~\cite{TCMR} suffers from a global location shift. Attention-based MPS-Net~\cite{MPS-net} captures the proper location of human meshes in the video sequence, but the mesh shape does not fit with the human in the images.
	We consider that the coupled modeling of long-term and short-term dependencies may not balance these two sides. It leads the RNN-based method to fall into the local dependency and does not capture the global location of the human mesh in the video. In contrast, the attention-based method tends to capture the long-term dependency and does not capture sufficient local details.
	Moreover, regressing the SMPL parameters from a coupled representation also confuses the model, \eg, fine-grained intra-frame human mesh structure requires more short-term local details. 
	
	% To solve the above problems, we propose to design a multi-knowledge model, Global-to-Local Transformer~(GLoT), that merges knowledge from deep networks~\cite{Transformer} and human prior structures~\cite{SMPL, ZiniuWan2021EncoderDecoderWM}, while decoupling the short-term and long-term temporal modeling. 
        To solve the above problems, we propose to use information from both deep networks~\cite{Transformer} and human prior structures~\cite{SMPL, ZiniuWan2021EncoderDecoderWM} in a joint manner. The proposed method, namely global-to-Local Transformer (GLoT), decouples the short-term and long-term temporal modeling.
        The framework is composed of Global Motion Modeling and Local Parameter Correction. 
	In global modeling, we introduce a global transformer with a Masked Pose and Shape Estimation~(MPSE) strategy for capturing the long-range global dependency~(\eg, proper global location, motion continuity). Specifically, the strategy randomly masks the features of several frames, and then the model predicts the SMPL parameters for the masked frames, which further helps the global transformer mine the coherent consistency of human motion and guides it to seize the inter-frame correlation from a global view. 
	Under the local view, we introduce a local transformer and a Hierarchical Spatial Correlation Regressor (HSCR) for exploiting the short-term inter-frame detail and learning the intra-frame human mesh structure. 
	% Concretely, we select the nearby frames of the mid-frame and feed them into the local encoder. Then the local decoder utilizes the mid-frame as a query to match the memory of the global transformer encoder and generates a decoupled global-local representation of the mid-frame. 
    To achieve this, we introduce nearby frames of the mid-frame and process them through the local encoder, which utilizes the mid-frame as a query to match the global transformer encoderâ€™s memory, generating a disentangled global-local representation of the mid-frame.
	% Finally, HSCR corrects the global estimation through the decoupled global-local representation and implicit kinematic constraints.
	Finally, HSCR employs human kinematic structures to constrain the refinement of decoupled global-local representation and improve global estimation.
	
	With the help of global-local cooperative modeling, our model obtains the best intra-frame accuracy and inter-frame smoothness. For example, compared with the previous state-of-the-art method~\cite{MPS-net}, our model significantly reduces the PA-MPJPE, MPJPE, and MPVPE by 1.5 $mm$, 3.6 $mm$, and 3.4 $mm$, respectively, on the widely used dataset 3DPW\cite{3DPW}, while preserving the lowest Accel metric representing the inter-frame smoothness. Moreover, our model remarkably decreases the model parameters, as shown in Figure~\ref{fig:para}.
	Our contributions can be summarized as follows:
	%With the help of our\ycnote{no our} designed GLoT, our method achieves\ycnote{achieve is weird i think.} accurate and smooth human pose and shape estimation on several datasets.
	%At the same time, the model parameters are less than the previous state-of-the-art method by 60 \%, shown in Figure~\ref{fig:para}.
	%The target of it is that we need to mine short-range detailed local context for correcting the coarse results provided by the LoT. 
	%For xxx, cross-attention is introduced to hinge the two Transformer for exchange the local-global information.
	%Following a coarse-to-fine learning strategy, global motion modeling gives a coarse result through LoT, while local parameter correction refines the coarse result by SoT and HSCR.
	%Moverover, following the coarse-to-fine schema, we further propose a hierarchical Kinematic regressor for complementing the spatial correlation to the static features. Concretely, this module take the coarse result and the local feature as input, then iteratively regressor the pose rotation matrix. 
	%We propose a coarse-to-fine learing framework in video-based human pose and shape estimation, and further design a long-short term Cascaded transformer network. Long-term Transformer with masked pose estimation strategy focus on capture the human motion continuty with a long-range global view, while Short-term Transformer tagets to percepr short range local human body structure. For balance the consistent and smooth and global local imformation exchange, A cross-attention is introduced to combine the long-short term Transformer.
	\begin{itemize}
		\item 
		% To our best knowledge, we make the first attempt to decouple the modeling of long-term and short-term correlations in video-based 3D human pose and shape estimation and propose a Global-to-Local Transformer~(GLoT), which merges knowledge from deep networks and human prior structures.
            To our best knowledge, we make the first attempt to decouple the modeling of long-term and short-term correlations in video-based 3D human pose and shape estimation. The proposed Global-to-Local Transformer (GLoT) merges the knowledge from deep networks and human prior structures, improving our method's accuracy and efficiency.
		\item 
		In GLoT, we carefully design two components, \ie, Global Motion Modeling and Local Parameter Correction, for learning inter-frame global-local contexts and intra-frame human mesh structure, respectively.
		\item 
		We conduct extensive experiments on three widely-used datasets. Our results show that GLoT outperforms the previous state-of-the-art method~\cite{MPS-net}, while achieving the lowest model parameters.
		
	\end{itemize}
	\begin{figure*}[!t]
		\centering
		\includegraphics[width=0.9\textwidth]{figure/Overview.pdf}
		\vspace{-1.5 em}
		\caption{An overview of our GLoT. 
			GLoT includes two branches, \ie,  Global Motion Modeling~(GMM) and Local Parameter Correction~(LPC). We first extract static features from pretrained ResNet-50~\cite{ResNet}, following~\cite{VIBE, TCMR, MPS-net}. Then the static features $S$ are separately processed by random masking and nearby frame selection for feeding them ($S^l, S^s $) into global and local transformers. Last, Hierarchical Spatial Correlation Regressor~(HSCR) corrects the global results~$\Theta_{mid}^l$ obtained by GMM with the decoupled global-local representation $f_{gl}$ and the inside kinematic structure. Note that our method utilizes T frames to predict the mid-frame, following ~\cite{TCMR, MPS-net}.
		}
		%RM means random masking, while NFS represents nearby frames selection.
		\label{fig:Overview}
		\vspace{-1.2 em}
	\end{figure*}
	
	\section{Related work}
	\subsection{Image-based human pose and shape estimation}
	There are two lines of methods in the image-based human pose and shape estimation. The first one is the SMPL-based regression method. 
	Kanazawa \etal \cite{HMR} design an end-to-end framework named Human Mesh Recovery (HMR) for predicting the shape and 3D joint angles of the SMPL model while introducing adversarial training.
	Some methods~\cite{GeorgiosPavlakos2018LearningTE, GeorgiosGeorgakis2020HierarchicalKH, MohamedOmran2018NeuralBF, PARE, zhang2021pymaf} integrate prior knowledge, \ie, 2D joint heatmaps and silhouette, semantic body part segmentation, multi-scale contexts, and kinematic prior, with a CNN model for estimating SMPL parameters. Moreover, 3D pose estimation~\cite{GeorgiosPavlakos2016CoarsetoFineVP, MPII3D} is highly related to this task. For example, Li \etal \cite{li2021hybrik} incorporate a 3D pose estimation branch to use inverse kinematics.
	Kolotouros \etal~\cite{SPIN} unify two paradigms, \ie, optimization-based and regression-based methods, into a general framework.
	The other line~\cite{GraphCMR, moon2020i2l, metro_lin} is to directly regress human mesh without a parametric model (SMPL).
	% 3D Human pose estimation is highly related to this task. There two types of methods, \ie, one-stage and two stage, to predict the 3D joint location. One-stage method~\cite{li20153d, pavlakos2017coarse, sun2018integral} directly infer 3D pose from images, while two-stage methods\cite{martinez2017simple} first utilize the pretrained 2D network to predict 2D keypoints and then lifts it to 3D by designed 2D-to-3D lifting networks.
	Even though these image-based methods achieve good results, they suffer from unstable human motion when applied to video sequences. 
	
	\subsection{Video-based human pose and shape estimation}
	
	%		xxx proposes TCMR to solve these problem in VIBE. Specifically, xx removes the residual connection in VIBE because the static features damages the model capacity of learning temproal consistency when applying the residual connection on the model structure. Furthermore, a temproal aggregation module is introdued for further constrain the front and behind frame of mid frame and aggregate these frames to one frame. 
	%Although the aforementioned method improves the in some way, they still exist several limitations, \eg, not modeling the global-local context well, 
	%However, adversiral training to learn motion prior is usually not stable. 
	%the main difference of Temporal encoder is that caputruing local or no-local context . and  In XXX, we show that tcmr exactly improves the consistent and xx in per-frame metrics and solves the unsmooth of VIBE, but it is still easy to sink into inaccurate prediction due to over-smooth in some difficult enviorment. We argure that simply apply sup on the front and behind frames of the mid frames using the gt of mid frames causes the model learn over-smooth case because of the smaller cost over training phase in some xxx enviorment. Most likely TCMR, MPS consists of temporal encoder and temporal aggregation module. The different points is that MPS-net utilize more powerful temporal encoder, non-local network, which capuring the human continuaty  global denpendcy. Similar to TCMR, xxx utilizes multiple frames aggregation througn a attenvive module. However, These method also lead the model fall into local minima, \eg, learn an average feature, but in current frame the camero and shape parameter is highly related to the current frame. 
	%Hence, when considering the temporal information is 
	Video-based human pose and shape estimation mainly consists of SMPL-based regression methods. Kanazawa \etal~\cite{Insta} propose HMMR that learns a dynamics representation of humans from videos and allows the model to predict future frames. 
	Kocabas \etal~\cite{VIBE} introduce a motion prior provided by a large-scale Mocap dataset~\cite{AMASS} to guide the model in learning a kinematically reasonable human motion structure via adversarial training. Although the motion prior helps the model to capture the human kinematic structure, it still suffers from temporally inconsistent. Hence, a series of methods emerged to solve this problem.
	MEVA~\cite{MEVA} utilizes VAE~\cite{VAE} to encode the motion sequence and generate coarse human mesh sequences. The corresponding human meshes are then further refined via a residual connection. 
	TCMR~\cite{TCMR} and MPS-Net~\cite{MPS-net} can be unified into a general framework, \ie temporal encoder, and temporal integration. TCMR utilizes GRU~\cite{GRU} to encode video representation of three different input lengths and then integrates three frames, \ie, mid-frame, front and rear frames of mid-frame, with an attentive module.
	MPS-Net replaces the GRU with a Non-local based~\cite{Non_local} motion continuity attention (MoCA) module, which helps to learn non-local context relations. Moreover, instead of the one-step attentive aggregation of three frames in TCMR, MPS-Net utilizes a hierarchical attentive feature integration (HAFI) module for multi-step adjacent frames integration.
	Although these methods improve the per-frame accuracy or temporal consistency in some way, a coupled temporal modeling structure leads to some problems, \eg, global location shift, motion inconsistency, and intra-frame inaccuracy.
	
	\subsection{Vision Transformers}
	The Transformer~\cite{Transformer} is first introduced in natural language processing (NLP). Transformer-based structures are effective for capturing long-range dependency and are suitable for learning global context due to the natural property of the self-attention mechanism. 
	Inspired by the success of Transformer in NLP, Alexey \etal \cite{VIT} first propose ViT and successfully obtain good results in image classification compared to convolutional architectures. 
	Subsequently, many works~\cite{MatthijsDouze2020TrainingDI, T2T, ZeLiu2021SwinTH, zhao2022centerclip} have emerged to improve computing efficiency and enhance representation ability. 
    Moreover, recent studies~\cite{yang2021associating,yang2022decoupling,liang2023local,li2022locality,zhou2022survey} have notably employed transformers to accomplish dense video tasks~\cite{miao2021vspw,miao2022large}. 
    % Moreover, some methods~\cite{SixiaoZheng2021RethinkingSS, PVT} are proposed for applying the transformer to dense vision tasks.
	Except for the design of the structure, some strategies for pretraining transformers, \eg, masked Language modeling~(MLM)~\cite{BERT} in NLP, masked image modeling~(MIM)~\cite{MAE, BEIT} in CV, enhance the representation of transformers.
	
	\section{Method}
	\subsection{Overview}
	Figure~\ref{fig:Overview} shows an overview of our Global-to-Local Transformer~(GLoT), which includes two branches, namely Global Motion Modeling~(GMM) and Local Parameter Correction~(LPC).
	Given an RGB video $\textbf{I}$ with T frames, $\textbf{I} = \{I_t\}_{t=1}^{T}$, we first utilize a pretrained ResNet-50~\cite{ResNet, SPIN} to extract static features $\textbf{S} =  \{s_t\}_{t=1}^{T}, s_t \in \mathbb{R}^{2048}$. Note the static features are saved on disk as we do not train the ResNet-50. 
	 We then feed these static features into GMM and LPC to obtain the final human mesh. Next, we elaborate on each branch of this framework as follows. 
	%	Our GLoT consists of two branches, \ie Global Motion Modeling~(GMM) and Local Parameters Correction~(LPC). We first	ramdomly mask several static features  into GMM to obtain coarse results, then we apply SoT to local static features chosen by NFS and further exchange information with the LoT encoder. Finally, we put the decoded global-local representation with coarse results processed by GMM into HSCR for refining the coarse results.
	
	
	\subsection{Global Motion Modeling}
	\label{sec:gmm}
	   The Global Motion Modeling involves three components, \ie a global transformer, a Masked Pose and Shape Estimation strategy, and an iterative regressor proposed by HMR~\cite{HMR}. For convenience in describing the overall process, we refer to the static features as static tokens. 
	
	\noindent\textbf{Global Transformer.}
	Recently, transformers have shown a powerful ability to model long-range global dependency owing to the self-attention mechanism. It is suitable for this task to capture long-term dependency and learn temporal consistency in human motion. 
	
	\noindent\textbf{Masked Pose and Shape Estimation.}
	Moreover, inspired by MIM~\cite{MAE, BEIT}, we propose a simple yet effective strategy named Masked Pose and Shape Estimation, which helps the global transformer further mine the inter-frame correlation on human motion. Specifically, we randomly mask several static tokens and predict only the SMPL parameters of the masked locations by leveraging the learned correlation with other unmasked tokens during training.
	
	\noindent\textbf{Human Prior Padding.}
	To reduce computation costs, the global transformer does not encode the masked tokens. Therefore, we need to pad tokens onto the masked location during the decoding phase. 
	We propose to use the SMPL mean template as padding for the masked tokens.
	The SMPL template represents the mean of the SMPL parameter distribution and thus has the smallest average difference to a random pose sampled from the distribution. Considering Transformers are built on a stack of residual connections, the learning will be easier from an input initialization which has a smaller residual difference from the output prediction. To handle the SMPL mean template, we utilize an MLP for dimension transformation and refer to it as an SMPL token.

	\noindent\textbf{Iterative Regressor.}
	The iterative regressor, first proposed by HMR~\cite{HMR}, has been shown to provide good estimations in some recent works~\cite{VIBE, TCMR, MPS-net}. It iteratively regresses the residual parameters of the previous step, starting from the mean SMPL parameters.
	Although the regressor may overlook some local details, \eg, human kinematic structure, it is well-suited to provide an initial global estimation under the global-to-local framework.
	
	The complete process of GMM can be described as follows, we first randomly mask some static tokens along the temporal dimension, $\textbf{S}^{l} \in \mathbb{R}^{(1-\alpha)T  \times 2048}$, $\alpha$ is a mask ratio. Then we apply the global encoder to these unmasked tokens. During the global decoder phase, following~\cite{MAE}, we pad the SMPL tokens onto the masked location and send the whole sequence into the global decoder to obtain a long-term representation. Finally, we apply the iterative regressor to the long-term representation to achieve global initial mesh sequences. We formulate the SMPL parameters obtained by GMM as follows,  $\Theta^l =\{\theta^l, \beta^l, \phi^l\}, \theta^l  \in \mathbb{R}^{T  \times (24 \times 6)}, \beta^l  \in \mathbb{R}^{T  \times 10}, \phi^l  \in \mathbb{R}^{T  \times 3}$. $\theta$ and $\beta$ are the pose and shape parameters that control the joint rotation and mesh shapes by the parametric model SMPL~\cite{SMPL}. 
	Due to insufficient 3d data annotation, $\phi$ represents a set of pseudo camera parameters predicted by the model, which project the 3d coordinates onto the 2d space for weakly supervising the model by abundant 2d annotated data.
	
	%	The LoT encoder and decoder blocks are shown in Figure~\ref{fig:self-atten}.
	
	%	\begin{figure}[!t]
		%		\centering
		%		\begin{subfigure}{0.3\linewidth}
			%			\includegraphics[width=1.09\textwidth]{figure/self-atten.pdf}
			%			\caption{Encoder and Decoder Blocks of LoT}
			%			\label{fig:self-atten}
			%		\end{subfigure}
		%		\hfill
		%		\begin{subfigure}{0.6\linewidth}
			%			\includegraphics[width=1.0\textwidth]{figure/cross-atten.pdf}
			%			\caption{Decoder Blocks of SoT, the SoT encoder block is the same as LoT}
			%			\label{fig:cross-atten}
			%		\end{subfigure}
		%		\caption{Design of Transformer Blocks}
		%		\label{fig:blocks}
		%	\end{figure}
	
	\subsection{Local Parameter Correction}
	
	Local Parameter Correction consists of two components, \ie a local transformer and a Hierarchical Spatial Correlation Regressor. 
	
	\noindent\textbf{Local Transformer.}
	As it is well known, the motion of a human body in a mid-frame is significantly influenced by its nearby frames. To capture the short-term local details in these frames more effectively, we introduce a local transformer.
	Specifically, we select the nearby frames for short-term modeling, $\textbf{S}^s = \{s_t\}_{t=\frac{T}{2} - w}^{\frac{T}{2} + w}, s_t \in \mathbb{R}^{2048}$, $w$ is the length of nearby frames. We utilize the local encoder on these selected tokens. 
	The local decoder is different from the global transformer, which decodes the features representing not only global-wise human motion consistency but also local-wise fine-grained human mesh structure through a cross-attention mechanism. The cross-attention function can be defined as follow,
	\begin{equation}
		\begin{aligned}
			CrossAtten(Q_{s}^{mid}, K_{l}, V_{l})
			&= Softmax (\frac{Q_{s}^{mid}K_{l}^T}{\sqrt[]{C}})V_{l},
			\label{eq: ca}
		\end{aligned}
	\end{equation}
	where $Q_{s}^{mid}$ is a query of the mid-token, $K_{l}$ and $V_{l}$ are key and value of the global encoder.
	%	The SoT encoder and decoder blocks are shown in Figure~\ref{fig:blocks}.
	
	\begin{figure}[!t]
		\centering
		\includegraphics[width=0.4\textwidth]{figure/kinematic.pdf}
		\caption{
			Human kinematic structure and an example of regressing and correcting one joint location.
		}
		%RM means random masking, while NFS represents nearby frames selection.
		\label{fig:kinematic}
		\vspace{-1em}
	\end{figure}
	
	\noindent\textbf{Hierarchical Spatial Correlation Regressor.}
	\label{sef:hscr}
	%		We consider that this regressor can achieve coarse prediction with a global perspective but does not model the fine-grained local detail of human mesh structure. 
	%	 Static features has an obvious drawback that lacks explict imformation representing the spatial corelation due to pooling operation.  
	Previous methods~\cite{VIBE, TCMR, MPS-net} utilize the regressor from the pioneering work HMR~\cite{HMR} to estimate the SMPL parameters. However, this approach overlooks the inherent human joint correlation, \ie, kinematic structure. Although the iterative regressor can produce a good estimation of SMPL parameters, it still requires local details to create a kinematically reasonable and rendered accurate human mesh.
	
	Hence, inspired by~\cite{ZiniuWan2021EncoderDecoderWM}, we propose a Hierarchical Spatial Correlation Regressor. Considering that directly regressing $\theta$ according to the kinematic structure may cause the model to fall into sub-optimal solutions due to a local view. Therefore, we add initial global prediction and decoupled global-local representation to this regressor, which allows the model to focus on adjusting the initial global estimation from a global-to-local perspective. 
	Specifically, modeling the local intra-frame human mesh structure need to learn the joint correlation inside the kinematic structure. As shown in Figure~\ref{fig:kinematic}, the kinematic structure can be described as the current joint rotation matrix being constrained by its ancestral joints. For example, when estimating the child joint (10), we need to compute the parent joints (0, 1, 4, 7) step-by-step.
	The total process of regressing parameters can be formulated as follows,
	\begin{equation}
		%\vspace*{-0.1em}
		\begin{aligned}
			\theta_i^{aj} &= Concat(\{\theta_{i,j}^s\}_{j=1} ^ {len(aj)}) \\
			\theta^s_i &= M_{\theta_i^s}(f_{gl}, \theta^{aj}_i, \theta^l_i ) \\
			\theta^r &= \theta^l + \theta^s, \\
			\beta^r &= \beta^l + M_{\beta}(f_{gl}, \beta^l), \\
			\phi^r &= \phi^l + M_{\phi}(f_{gl}, 	\phi^l),
			\label{eq: hscr}
		\end{aligned}
	\end{equation}
	where $M_{\theta^s_i}, M_{\beta}$ and $ M_{\phi}$ are Multilayer Perceptron (MLP), $len(aj)$ is the number of the ancestral joints of the current joint $i$, $f_{gl}$ is a decoupled global-local representation. Note $\theta$ is composed of 6D rotation representations of 24 joints.
	$\theta = \{\theta_i\}_{i=1}^{24}, \theta_i \in \mathbb{R}^6$.
	
	
	\subsection{Loss function}
	In GMM, we apply $\mathcal{L}_2$ loss to the SMPL parameters $\Theta^l$ and 3D/2D joints location, following the previous method \cite{VIBE, MPS-net, TCMR}. Note that we only compute the loss of masked location. Moreover, we empirically discover that constraints on the velocity of the predicted 3D/2D joint location can help the model learn motion consistency and capture the long-range dependency better when applying the Masked Pose and Shape Estimation strategy to the global transformer. The velocity loss can be defined as follows,
	\begin{equation}
		%\vspace*{-0.1em}
		\begin{aligned}
			\mathcal{L}_{vel\_2d} &= \sum_{t=1}^{T-1} m_t || (jt_{2d}^{t+1} -  jt_{2d}^{t}) -  (gt_{2d}^{t+1} -  gt_{2d}^{t}) ||_2  \\
			\mathcal{L}_{vel\_3d} &= \sum_{t=1}^{T-1} m_t || (jt_{3d}^{t+1} -  jt_{3d}^{t}) - (gt_{3d}^{t+1} -  gt_{3d}^{t})||_2
			\label{eq: loss}
		\end{aligned}
	\end{equation}
	where $jt$ is a predicted 2d/3d joint location, $gt$ is the ground truth of the 2d/3d joint location. $\mathbf{M} = \{m_i\}_{i=1}^{T-1}, \mathbf{M} \in \mathbb{R}^{T-1}$ is a mask vector. $m_i$ is 1 when masking the $i$ location, otherwise, $m_i$ is 0.
	In LPC, we apply the same loss as the GMM, except that we only constrain the mid-frame.
	
	\begin{table*}[htb]
		\small
		\setlength{\tabcolsep}{1.4 pt}
		%\renewcommand\arraystretch{}
		\centering
		
		%\resizebox{\textwidth}{!}{%
			\begin{tabular}{l|cccc|ccc|ccc|c}
				\toprule[2pt]
				\multirow{2}{*}{Method}& \multicolumn{4}{c|}{3DPW} & \multicolumn{3}{c|}{MPI-INF-3DHP} & \multicolumn{3}{c|}{Human3.6M} & \multicolumn{1}{c}{number of} \\
				%\cmidrule(lr){2-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}
				& PA-MPJPE $\downarrow$ & MPJPE $\downarrow$ & MPVPE $\downarrow$ & Accel $\downarrow$ & PA-MPJPE $\downarrow$ & MPJPE $\downarrow$ & Accel $\downarrow$ & PA-MPJPE $\downarrow$ & MPJPE $\downarrow$ & Accel $\downarrow$ & input frames \\
				\midrule[1pt]
				\midrule[1pt]
				VIBE~\cite{VIBE} & 57.6 & 91.9 & - & 25.4 & 68.9 & 103.9 & 27.3 & 53.3 & 78.0 & 27.3 & \textbf{16} \\
				\cellcolor{Gray}MEVA~\cite{MEVA} & \cellcolor{Gray}54.7 &\cellcolor{Gray}86.9 &\cellcolor{Gray}- & \cellcolor{Gray}11.6 & \cellcolor{Gray}65.4 & \cellcolor{Gray}96.4 & \cellcolor{Gray}11.1 & \cellcolor{Gray}53.2 & \cellcolor{Gray}76.0 & \cellcolor{Gray}15.3 & \cellcolor{Gray}90 \\
				TCMR~\cite{TCMR} & 
				52.7 & 
				86.5 & 102.9& 
				7.1 & 
				63.5& 
				97.3 & 
				8.5 & 
				52.0 & {73.6} & {3.9} & 
				\textbf{16} \\
				\cellcolor{Gray}MPS-Net~\cite{MPS-net} & 
				\cellcolor{Gray}52.1 & \cellcolor{Gray}84.3 & \cellcolor{Gray}99.7& \cellcolor{Gray}7.4 & 
				\cellcolor{Gray}62.8 &\cellcolor{Gray} 96.7 & \cellcolor{Gray}9.6 &
				\cellcolor{Gray}47.4 &\cellcolor{Gray} 69.4 & \cellcolor{Gray}\textbf{3.6} & \cellcolor{Gray}\textbf{16} \\
				\textbf{GLoT(Ours)} &
				%3dpw
				\textbf{50.6} & \textbf{80.7} & \textbf{96.3}& \textbf{6.6} & 
				%mpi
				\textbf{61.5} &
				\textbf{93.9} & \textbf{7.9} &
				%h36m
				\textbf{46.3} & \textbf{67.0} & \textbf{3.6} & \textbf{16}
				\\
				\bottomrule[1pt]
				% 			\bottomrule
				
			\end{tabular}%
			%}
		\vspace{-0.5 em}
		\caption{
			Evaluation of state-of-the-art video-based methods on 3DPW~\cite{3DPW}, MPI-INF-3DHP~\cite{MPS-net}, and Human3.6M~\cite{H36M}. 
			All methods use 3DPW training set in training phase, but do not utilize Human3.6M SMPL parameters from Mosh~\cite{Mosh}.
			The number of input frames follows the protocol of each paper.
		}
		\vspace{-1.em}
		\label{tab:sota_video}
	\end{table*}
	\section{Implementation details}
	Following previous methods~\cite{VIBE, TCMR, MPS-net}, we set the input length $T$ to 16. and use the same data processing procedure as TCMR~\cite{TCMR}. We set the mini-batch $N$ to 64 and initialize the learning rate to 1e-4. Moreover, we apply the Cosine Annealing~\cite{IlyaLoshchilov2016SGDRSG} scheduler and linear warm-up to the Adam~\cite{DiederikPKingma2014AdamAM} optimizer. Our model is trained on one Nvidia Tesla V100 GPU. Since our model predicts the mid-frame of the input sequence, the predicted frame may fall outside the clip boundary. To handle this issue, we use nearest-padding, which duplicates the nearest boundary frame to pad the missing part. For example, when estimating frame 0, we duplicate frame 0 for 7 times. 
	More details are provided in the supplementary material.
	
	\section{Experiments}
	We begin by introducing the evaluation metrics and datasets. Next, we report the evaluation results of our model and compare them with previous state-of-the-art methods. Finally, we provide ablation studies and qualitative results to further support our findings.
	
	\noindent\textbf{Evaluation Metrics.} Following previous methods~\cite{Insta, VIBE, TCMR, MPS-net}, we report several intra-frame metrics, including Mean Per Joint Position Error~(MPJPE), Procrustes-aligned MPJPE~(PA-MPJPE), and Mean Per Vertex Position Error~(MPVPE). Additionally, we provide a result for acceleration error (Accel) to verify inter-frame smoothness.
	
	\noindent\textbf{Datasets.} The training datasets include 3DPW~\cite{3DPW}, Human3.6M~\cite{HMR}, MPI-INF-3DHP~\cite{MPII3D}, InstaVariety~\cite{Insta}, Penn Action~\cite{Penn}, and PoseTrack~\cite{PoseTrack}, following~\cite{TCMR}. We use 3DPW, Human3.6M, and MPI-INF-3DHP for evaluation. More details are provided in the supplementary material.
	
	\subsection{Comparison with state-of-the-art methods}
	\noindent\textbf{Video-based methods.}
	As shown in Table \ref{tab:sota_video}, our GLoT surpasses existing methods on 3DPW, MPI-INF-3DHP, and Human3.6M, for both intra-frame metrics~(PA-MPJPE, MPJPE, MPVPE) and inter-frame metric~(Accel). This indicates that our proposed Global-to-Local Modeling method is effective for modeling the long-range dependency~(\eg, the proper global location, coherent motion continuity) and learning the local details~(\eg, intra-frame human mesh structure).
	For example, GLoT reduces PA-MPJPE by 1.5 $mm$ , MPJPE by 3.6 $mm$, MPVPE by 3.4 $mm$, and Accel by 0.8 $mm/s^2$ compared with MPS-Net~\cite{MPS-net} on 3DPW.
	\begin{table}[t]
		\small
		\setlength{\tabcolsep}{1.2pt}
		\centering
		
		%\vspace*{-0.4em}
		%\resizebox{\textwidth}{!}{%
			\begin{tabular}{ll|cccc}
				\toprule[2pt]
				
				\multicolumn{2}{c}{\multirow{2}{*}{Method}} & \multicolumn{4}{|c}{3DPW} \\
				& & PA-MPJPE $\downarrow$ & MPJPE $\downarrow$ & MPVPE $\downarrow$ & Accel $\downarrow$ \\
				\midrule[1pt]
				\midrule[1pt]
				\multirow{5}{*}{\rotatebox[origin=c]{90}{single image}}  & HMR~\cite{HMR} & 76.7 & 130.0 & - & 37.4 \\
				& \cellcolor{Gray}GraphCMR~\cite{GraphCMR} & \cellcolor{Gray}70.2 & \cellcolor{Gray}- & \cellcolor{Gray}- & \cellcolor{Gray}- \\
				& SPIN~\cite{SPIN} &  59.2 & 96.9 & 116.4 & 29.8 \\
				& \cellcolor{Gray}I2L-MeshNet~\cite{moon2020i2l} & \cellcolor{Gray}57.7 & \cellcolor{Gray}93.2 & \cellcolor{Gray}110.1 & \cellcolor{Gray}30.9 \\
				&PyMAF~\cite{zhang2021pymaf} & 58.9 & 92.8 & 110.1 & - \\
				\midrule[1pt]
				\multirow{5}{*}{\rotatebox[origin=c]{90}{video}}
				& HMMR~\cite{Insta} & 72.6 & 116.5 & 139.3 & 15.2 \\
				& \cellcolor{Gray}VIBE~\cite{VIBE} & \cellcolor{Gray}56.5 & \cellcolor{Gray}93.5 & \cellcolor{Gray}113.4 & \cellcolor{Gray}27.1 \\
				& TCMR~\cite{TCMR} & 55.8 & 95.0 & 111.5 & {7.0} \\ 
				& \cellcolor{Gray}MPS-Net~\cite{MPS-net} & \cellcolor{Gray}{54.0} & \cellcolor{Gray}91.6 & \cellcolor{Gray}109.6 & \cellcolor{Gray}{7.5} \\
				& \textbf{GLoT(Ours)} & \textbf{53.5} & \textbf{89.9} & \textbf{107.8} & \textbf{6.7} \\
				
				\bottomrule[1pt]
				
			\end{tabular}%
			\vspace*{-0.5em}
			\caption{
				Evaluation of state-of-the-art methods on 3DPW~\cite{3DPW}. All methods do not use 3DPW~\cite{3DPW} on training.
			}
			\vspace{-0.5 em}
			\label{tab:sota-img-video}
			
		\end{table}
		\begin{table}[!t]
			\small
			\setlength{\tabcolsep}{4.8 pt}
			%		\vspace{-1em}
			\begin{center}
				\begin{tabular}{l | c | c | c | c}
					%\specialrule{0pt}{.05em}{.05em}
					\toprule[2pt]
					\normalsize
					Module & PA-MPJPE$\downarrow$ & MPJPE$\downarrow$ & MPVPE$\downarrow$ & Accel$\downarrow$ \\
					\midrule[1pt]
					GMM & 52.6 & 84.1 & 101.0 & 6.9 \\
					\cellcolor{Gray}GMM + LPC & \cellcolor{Gray}\textbf{50.6} & \cellcolor{Gray}\textbf{80.7} & \cellcolor{Gray}\textbf{96.3} & \cellcolor{Gray}\textbf{6.6} \\
					\bottomrule[1pt]
					%\specialrule{0pt}{.05em}{.05em}
				\end{tabular}
			\end{center}
			\vspace{-1.5 em}
			\caption{Ablation studies of Global Motion Modeling~(GMM) and Local Parameter Correction~(LPC).}
			\vspace{-1.5 em}
			\label{tab:gmmlpc}
		\end{table}
		Next, we analyze the existing problems of the previous method. 
		First, VIBE~\cite{VIBE}, TCMR~\cite{TCMR}, and MPS-Net~\cite{MPS-net} all design a single type of modeling structure to simultaneously model long-term and short-term, leading to undesirable results like global location shift and insufficient local details shown in Figure~\ref{fig:motivation}.
		Second, MEVA follows a coarse-to-fine schema to model human motion, which requires too many frames as input and is ineffective for short videos.
		
		\noindent\textbf{Single image-based and video-based methods.} We compare our GLoT with the previous single image-based and video-based methods on the challenging in-the-wild 3DPW~\cite{3DPW}. Note that all methods do not utilize the 3DPW in the training phase. 
		As shown in Table~\ref{tab:sota-img-video}, our GLoT outperforms existing single image-based and video-based methods on all metrics, which validates the effectiveness of our method.
		For example, our model surpasses the video-based MPS-Net on PA-MPJPE, MPJPE, MPVPE, and Accel by 0.5 $mm$, 1.7 $mm$, 1.8 $mm$ and 0.8 $mm/s^2$, respectively. In summary, our model achieves a smooth mesh sequence and accurate human meshes compared with previous methods. The results confirm that our proposed GLoT is effective for modeling long-range dependencies and learning local details, leading to better performance on the challenging 3DPW dataset.
		
		\subsection{Ablation studies}
		To validate the effectiveness of our GLoT, we conduct a series of experiments on 3DPW~\cite{3DPW} with the same setting as Table~\ref{tab:sota_video}.
            Initially, we verify two branches proposed in GLoT, \ie, the Global Motion Modeling and Local Parameter Correction. Subsequently, we delve into the Masked Pose and Shape Estimation strategy, in addition to analyzing the types of mask tokens employed in the Global Motion Modeling phase. Lastly, we validate the influence of varying lengths of nearby frames and the effectiveness of the Hierarchical Spatial Correlation Regressor.
		% First, we verify the effectiveness of each branch proposed in GLoT, \ie Global Motion Modeling and Local Parameter Correction. 
		% Second, we study the Masked Pose and Shape Estimation strategy, as well as the types of mask tokens used in Global Motion Modeling.
		% Last, we validate the impact of different lengths of nearby frames and the Hierarchical Spatial Correlation Regressor used in Local Parameter Correction.
		
		\begin{table}[t]
			\small
			\centering
			\setlength\tabcolsep{4pt}
			\def\arraystretch{1.0}
			
			%\vspace*{-0.5em}
			%\resizebox{1\linewidth}{!}{
				\begin{tabular}{l|cccc}
					\toprule[2pt]
					Mask ratio~(\%) & PA-MPJPE$\downarrow$ & MPJPE$\downarrow$ & MPVPE$\downarrow$ & Accel$\downarrow$ \\
					\midrule[1pt]
					0 & 51.7 & 82.3 & 98 & 7.3  \\
					
					\cellcolor{Gray}12.5 &\cellcolor{Gray}51.3 & \cellcolor{Gray}82& \cellcolor{Gray}97.4& \cellcolor{Gray}7.1 \\
					
					25& 51.0 & 82.1 & 97.5 & 7.0  \\
					
					\cellcolor{Gray}37.5 & \cellcolor{Gray}51.0 & \cellcolor{Gray}81.4 & \cellcolor{Gray}97.2 & \cellcolor{Gray}6.8  \\
					
					50   & \textbf{50.6} & \textbf{80.7} &  \textbf{96.3} &  6.6 \\ 
					
					\cellcolor{Gray}62.5& \cellcolor{Gray}50.8 & \cellcolor{Gray}81.7 & \cellcolor{Gray}97.7 & \cellcolor{Gray}6.6 \\
					
					75  &51.4 & 82.9 & 99.1 & 6.8 \\
					
					\cellcolor{Gray}87.5  & \cellcolor{Gray}52.8 & \cellcolor{Gray}86.6 & \cellcolor{Gray}104.1 & \cellcolor{Gray}\textbf{6.5} \\
					\bottomrule[1pt]
				\end{tabular}
				% }
			\vspace*{-0.5 em}
			\caption{Ablation studies of different mask ratios.}
			\vspace{-0.5 em}
			\label{table:mask_ratio}
		\end{table}
		
		\noindent\textbf{Global Motion Modeling~(GMM) and Local Parameter Correction~(LPC).}
		We first study the GMM branch as shown in Table~\ref{tab:gmmlpc}. Our results indicate that the GMM branch alone achieves competitive results, reducing the MPJPE by 0.2~$mm$  and Accel by 0.5~$mm$ compared to MPS-Net~\cite{MPS-net}. Furthermore, our method outperforms TCMR~\cite{TCMR} in all metrics. Overall, our model shows the powerful learning long-dependency ability of the global transformer and its potential to improve intra-frame accuracy.
		Next, we combine the LPC branch into this framework, which surpasses all methods with a larger margin in terms of both intra-frame accuracy and inter-frame smoothness. It shows that our global-to-local cooperative modeling helps the model to learn coherent motion consistency and inherent human structure.
		
		
		\begin{table}[t]
			\small
			\centering
			\setlength\tabcolsep{2pt}
			\def\arraystretch{1.0}
			
			%\vspace*{-0.5em}
			%\resizebox{1\linewidth}{!}{
				\begin{tabular}{l|c|c|c|c}
					%\specialrule{.1em}{.05em}{.05em}
					\toprule[2pt]
					Types of mask token & PA-MPJPE$\downarrow$ & MPJPE$\downarrow$ &MPVPE$\downarrow$  & Accel$\downarrow$ \\ 
					\midrule[1pt]
					SMPL Token & \textbf{50.6} & \textbf{80.7} &  \textbf{96.3} &  \textbf{6.6} \\
					\cellcolor{Gray}Learnable Token& \cellcolor{Gray}51.5 & \cellcolor{Gray}82.5 & \cellcolor{Gray}98.5 & \cellcolor{Gray}6.9 \\ 
					\bottomrule[1pt]
					%\specialrule{.1em}{.05em}{.05em}
				\end{tabular}
				% }
			\vspace*{-0.5 em}
			\caption{Ablation studies of types of mask tokens.}
			\vspace{-0.5 em}
			\label{tab:types of mask token}
		\end{table}
		
		\begin{table}[!t]
			\small
			\setlength{\tabcolsep}{6.5 pt}
			\begin{center}
				\begin{tabular}{l | c | c | c | c}
					%\specialrule{0pt}{.05em}{.05em}
					\toprule[2pt]
					\normalsize
					Module & PA-MPJPE$\downarrow$ & MPJPE$\downarrow$ & MPVPE$\downarrow$ & Accel$\downarrow$ \\
					\midrule[1pt]
					Residual & 51.5 & 81.7 & 97.2 & 6.7 \\
					\cellcolor{Gray}HSCR & \cellcolor{Gray}\textbf{50.6} & \cellcolor{Gray}\textbf{80.7} & \cellcolor{Gray}\textbf{96.3} & \cellcolor{Gray}\textbf{6.6} \\
					\bottomrule[1pt]
					%\specialrule{0pt}{.05em}{.05em}
				\end{tabular}
			\end{center}
			\vspace*{-1.5 em}
			\caption{Ablation studies of Hierarchical Spatial Correlation Regressor~(HSCR).}
			\vspace*{-1.5 em}
			\label{tab:ab_hscr}
		\end{table}
		
		
		
		%	\noindent\textbf{Effect of velocity loss.}
		
		\noindent\textbf{Masked Pose and Shape Estimation Strategy.}
		We randomly mask several static features during training. When testing, we do not mask any features.
		Table~\ref{table:mask_ratio} shows the influence of different mask ratios. 
		We find that (1) From the perspective of the Accel metric, applying the masked strategy helps the model to capture the long-range dependency. The Accel metric generally shows a reducing tendency when we gradually increase the mask ratio. The 87.5\% mask ratio obtains the best Accel performance, which is in line with the intuitive understanding that the model tends to learn overly smooth meshes when applying a higher mask ratio. (2) In intra-frame metrics, the mined long-term dependency also provides global contexts for the next local modeling phase. When selecting a 50\% mask ratio, our model achieves the best performance. A mask ratio between 37.5 \% to 62.5 \% is appropriate for improving the model performance.
		
		\noindent\textbf{Different types of the mask token.}
		As shown in Table~\ref{tab:types of mask token}, we draw several conclusions from the experiments with different mask tokens.
		(1) Simply applying a learnable token to masked locations obtains better performance than the previous state-of-the-art method, MPS-Net. 
		(2) When employing the SMPL token processed from the SMPL mean template, our model obtains the best result. 
		(3) We consider that applying the SMPL token complements a human mesh prior, helping the model to learn human inherent structure.
		
		\begin{table}[t]
			\small
			\setlength{\tabcolsep}{7 pt}
			%\def\arraystretch{1.0}
			
			
			\begin{center}
				\begin{tabular}{l | c c c c}
					\toprule[2pt]
					Length & PA-MPJPE$\downarrow$ & MPJPE$\downarrow$ & MPVPE$\downarrow$ & Accel$\downarrow$ \\
					\midrule[1pt]
					2 & 51.9 & 83 &	98.7 & 6.7 \\
					
					\cellcolor{Gray}3 &	\cellcolor{Gray}51.2 &	\cellcolor{Gray}84.2 & 	\cellcolor{Gray}100.0 &	\cellcolor{Gray}6.7 \\
					
					4 & \textbf{50.6} & \textbf{80.7} &  \textbf{96.3} &  \textbf{6.6} \\
					
					\cellcolor{Gray}5 & 	\cellcolor{Gray}51.3 & 	\cellcolor{Gray}81.7 &  	\cellcolor{Gray}97.2 &  	\cellcolor{Gray}6.7 \\
					
					6 & 51.7 & 82.4 &  98.4 &  6.6 \\
					\bottomrule[1pt]
				\end{tabular}
			\end{center}
			\vspace{-1.5 em}
			\caption{Ablation studies of different lengths of nearby frames. The length of 2 means the total frames feeding to the local transformer will be $2 + 2 + 1 = 5$, including the mid-frame.}
			% 		\vspace{-0.5em}
			\label{tab:nearby}
		\end{table}
		
		
		\begin{figure}[!t]
			\begin{subfigure}{0.48\linewidth}
				\includegraphics[width=1.0\textwidth]{figure/kine_gmm.pdf}
				\vspace{-1.5 em}
				\caption{Our results of GMM. }
				\label{fig:gmm-vis}
			\end{subfigure}
			\hfill
			\begin{subfigure}{0.48\linewidth}
				\includegraphics[width=1.0\textwidth]{figure/kine_lpc.pdf}
				\vspace{-1.5 em}
				\caption{Our results via LPC. }
				\label{fig:lpc-vis}
			\end{subfigure}
			\vspace{-0.5 em}
			\caption{Qualitative results of Hierarchical Spatial Correlation Regressor (HSCR). (a) The wrist rotation is unreasonable. (b) HSCR corrects the wrist rotation. It shows that our model learns implicit kinematic constraints.}
			% \vspace{-0.5 em}
			\label{fig:hscr_kine}
		\end{figure}
		
		\begin{figure}[!t]
			\begin{subfigure}{0.48\linewidth}
				\includegraphics[width=0.8\textwidth]{figure/en_atten_0mask.pdf}
				\centering
				\vspace{-0.5 em}
				\caption{Encoder attention weights.}
				\label{fig:en_atten_0mask}
			\end{subfigure}
			\hfill
			\begin{subfigure}{0.48\linewidth}
				\includegraphics[width=0.8\textwidth]{figure/de_atten_0mask.pdf}
				\centering
				\vspace{-0.5 em}
				\caption{Decoder attention weights.}
				\label{fig:de_atten_0mask}
			\end{subfigure}
			
			\begin{subfigure}{0.48\linewidth}
				\includegraphics[width=0.8\textwidth]{figure/en_atten_5mask.pdf}
				\centering
				\vspace{-0.5 em}
				\caption{Encoder attention weights.}
				\vspace{-0.7 em}
				% 		\caption{Visualizations of attention weights}
				\label{fig:en_atten_5mask}
			\end{subfigure}
			\hfill
			\begin{subfigure}{0.48\linewidth}
				\includegraphics[width=0.8\textwidth]{figure/de_atten_5mask.pdf}
				\centering
				\vspace{-0.5 em}
				\caption{Decoder attention weights.}
				\vspace{-0.7 em}
				\label{fig:de_atten_5mask}
			\end{subfigure}
			\caption{Attention visualizations.~(a),(b). The results without using the masking strategy. (c), (d). The results of the masking strategy.}
			\vspace{-1.5 em}
			\label{fig:atten}
		\end{figure}
		
		\begin{figure}[!t]
			\includegraphics[width=0.5\textwidth]{figure/vis_2.pdf}
			\vspace{-2 em}
			\caption{Qualitative comparison with previous state-of-the-art methods~\cite{TCMR, MPS-net}.}
			\vspace{-1.5 em}
			\label{fig:vis_2}
		\end{figure}
		
		
		
		
		
		
		\noindent\textbf{Hierarchical Spatial Correlation Regressor.}
		Table~\ref{tab:ab_hscr} shows the evaluation results of our HSCR. Residual means that we do not utilize the implicit kinematic constraints in Sec.~\ref{sef:hscr} and use the residual connection to replace them. We find that HSCR significantly reduces the PA-MPJPE, MPJPE, and MPVPE by 0.9 $mm$, 1.0 $mm$, and 0.9 $mm$, respectively, when compared with the residual connection. 
		Moreover, our model achieves better results compared with other previous video-based methods when merely using the residual connection.
		This indicates that our Global-to-Local framework substantially improves the performance of video-based 3D human mesh recovery.
		
		\noindent\textbf{The different lengths of nearby frames.}
		As shown in Table~\ref{tab:nearby}, we observe  the following: (1) when setting the nearby length to four, our model achieves the best performance on all metrics. (2) the length of four means the input frames of the local transformer are nine, nearly half of the input frames of the global transformer. We consider that setting it to half of the input frames of the global transformer is a good solution. (3) Although other lengths result in worse performance than four frames, they are still competitive with other methods~\cite{VIBE, TCMR, MPS-net}. For example, when the length is set to two, our method surpasses the previous state-of-the-art MPS-Net with 0.2 $mm$ PA-MPJPE, 1.3 $mm$ MPJPE, 1.0 $mm$ MPVPE, 0.7 $mm/s^2$ Accel.
		
		\subsection{Qualitative evaluation}
		
		\noindent\textbf{Hierarchical Spatial Correlation Regressor~(HSCR).}
		In Figure~\ref{fig:hscr_kine}, we study the proposed HSCR from a visualization perspective. It shows that the kinematic failure in global estimation is corrected by HSCR, which validates the effectiveness of our method.
		
		\noindent\textbf{Masked Pose and Shape Estimation strategy.}
		In Figure~\ref{fig:atten}, we provide attention visualizations of the global transformer and make several observations. (1) Compared between (a) and (c), the encoder with masking strategy attends to more nearby frames and is more centralized, indicating that it is not limited by the several nearby frames like (a). (2) The masking strategy in the decoder pushes the model to focus on further frames, while the non-masked decoder is more distributed.
	(3) The encoder and decoder in (c) and (d) show a cooperative relationship. This achieves decoupled long-term and short-term modeling.
		The encoder captures the nearby frame correlations while the decoder attends to the further frame correlations. 
		
		
		\noindent\textbf{Comparison with previous methods.}
		In Figure~\ref{fig:vis_2}, we compare our method to previous approaches~\cite{TCMR, MPS-net}, and provide sequences of an alternate viewpoint. We observe that TCMR suffers global location shift and provides inaccurate meshes while MPS-Net captures the actual location, but the local details are insufficient.
		\begin{figure}[!t]
			\includegraphics[width=0.5\textwidth]{figure/vis_3.pdf}
			\vspace{-2 em}
			\caption{Qualitative results of GLoT on challenging internet videos.}
			%		\vspace{-1.5 em}
			\label{fig:vis_3}
			\vspace{-1.5 em}
		\end{figure}
		
		\noindent\textbf{Qualitative results on internet videos.}
		In Figure~\ref{fig:vis_3}, we validate our method on challenging internet videos and provide rendered meshes of an alternate viewpoint, demonstrating that our method successfully captures human motion from different perspectives. More qualitative results are provided in the supplementary material.
		
		\section{Conclusion}
		We propose a Global-to-Local Modeling~(GloT) method for video-based 3d human pose and shape estimation, which captures the long-range global dependency and local details (\eg, global location, motion consistency, and intra-frame human meshes) by combining deep networks and human prior structures.
		Through global-local cooperative modeling, GLoT achieves state-of-the-art performance on three widely used datasets.
		Furthermore, GLoT shows potential for handling in-the-wild internet videos, which could help the annotation of 3D meshes and provide various motion sequence templates for downstream tasks. 
	
\noindent\textbf{Acknowledgements.} This work was supported by the Fundamental Research Funds for the Central Universities (No. 226-2022-00051).

		
		
		
		
		
		
		%%%%%%%%% REFERENCES
		{\small
			\bibliographystyle{ieee_fullname}
			\bibliography{egbib}
		}
		\include{sup}
	\end{document}
