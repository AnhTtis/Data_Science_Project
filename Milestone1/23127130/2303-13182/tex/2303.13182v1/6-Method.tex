\section{Grasp Network: CMG-Net}
\label{sec:CMG}
In this section, we introduce a simple grasp pose estimation network: CMG-Net (Contact-based Multi-finger Grasping Network).
For an input point cloud, we aim to generate a set of grasp poses.
Fig.~\ref{fig:CMGNet} shows CMG-Net's structure, consisting of three stages,
1) grasp points segmentation, which extracts features from the point cloud by PointNet++~\cite{DBLP:conf/nips/QiYSG17}, and then classifies the contact points to determine whether they are graspable. And we predict the finger ID (e.g., finger 1, 2, 3) and finger projection corresponding to the possible contact points.
2) preliminary grasp pose prediction, which predicts an initial pose using our proposed grasp representation,
and 3) grasp pose refinement, which refines the grasping pose by predicting the supporting joints.

\begin{figure}[ht]
\centering
\includegraphics[width=0.98\columnwidth]{./fig/network_final_4.pdf}
\caption{The CMG-Net framework.}
\label{fig:CMGNet}
\end{figure}

\subsection{Learning to Predict Grasp Points}
\label{sub:LearningPredictGrasp}

We use PointNet++ \cite{DBLP:conf/nips/QiYSG17} as our backbone due to its simplicity and high success rate on various challenging tasks.
We use its set-abstraction (SA) layers and feature propagation (FP) layers to build an asymmetric U-shaped network with an encoder and a decoder.
The encoder uses multiple SA blocks to extract abstract features from point clouds and the decoder utilizes an equal number of FP blocks to gradually interpolate the abstracted features.


We also incorporate the point normals into the network. The actual input data size is increased to $N \times 6$, where $N$ is the number of points.
The extracted point cloud features, as shown in Fig.~\ref{fig:CMGNet}, are used by the \emph{contact segmentation module} to predict whether a point is graspable.
We design a cross-entropy loss $\mathcal{F}_{cls}$ for classification training:
\begin{align}
\mathcal{L}_{gp}=\mathcal{F}_{cls}\left(c^{p}, {c^{g}}\right),
\end{align}
where $c^{p}$ is the prediction result of segmentation head for each grasp sample and $c^{g}$ is its corresponding label.

\subsection{Grasp Pose Estimation}
We further predict the corresponding joint angles of the given dexterous hand based on the graspable points and combine them with finger projection to calculate a preliminary grasp pose using Eq.~\ref{xy_to_pose}.
Specifically,
We use \emph{Finger Projection Prediction}, which contains a multi-layer perceptron (MLPs) network with fully connected layers, ReLU, and batch normalization, to predict finger projection $x^p$ and $y^p$. The finger projection $x$ and $y$ will be predicted as a group $xy^p$. The loss function is
\begin{equation}
\mathcal{L}_{fp}=\frac{1}{N_{\text{c}}} \sum_i{\left\|{xy}_i^p-{xy}_i^{l}\right\|} \delta_{\text{c}},
\end{equation}
where $N_\mathrm{c}$ is the total number of contact points. $xy_i^{l}$ is the ground truth finger projection from the contact points, $\delta_c$ is an indicator, and it yields 1 if $xy_i$ corresponds to contact points and 0 otherwise.
The joint angle $\theta_{m}$ is evenly divided into $n_m$ bins. In our implementation, we set the subdivision angle $\phi_{m} = \frac{7\pi}{9}$.
%
For each grasp sample, we calculate its bin classification label ${bin}_{m}^l$ and residual label  ${res}_{m}^l$ as follows
\begin{equation}
\begin{aligned}
{bin}_{m}^l &=
\left\lfloor\frac{{\theta}_m}{\phi_m}\right\rfloor, \\
{res}_m^l &=\frac{1}{{\phi}_m}\left({\theta}_m^l-\left({bin}_m  {\phi}_m+\frac{{\phi}_m}{2}\right)\right).
\end{aligned}
\end{equation}

The loss of main joint is formulated as
\begin{equation} \label{bin_loss}
\mathcal{L}_{m}=\mathcal{F}_{c l s}\left(bin_m^{p}, {bin_m^{l}}\right) + \mathcal{F}_{res}\left(res_m^{p}, {res_m^{l}}\right),
\end{equation}
where $bin_m^l$ and $res_m^l$ are ground-truth bin assignment and residual for a given grasp $p$. $bin_m^p$ and $res_m^p$ are their corresponding predicted values.

Analogically, the spread joints $\theta_{ms}$ has the same form
{\small
\begin{align}
\mathcal{L}_{ms}=\mathcal{F}_{c l s}\left(bin_{ms}^{p}, {bin_{ms}^{l}}\right) + \mathcal{F}_{res}\left(res_{ms}^{p}, {res_{ms}^{l}}\right).
\end{align}
}

According to Eq.~\ref{xy_to_pose}, we finally obtain a preliminary grasp pose $\mathbf{G}=\{\emph{x}, \emph{y}, \theta_{m},\theta_{ms}\}$.


\subsection{Grasp Pose Refinement}
To improve grasp quality and create realistic grasp poses, a joint prediction layer is used to guarantee the accuracy of supporting joints $\theta_{s1}$ and $\theta_{s2}$.
More specifically, $\theta_{s1}$ and $\theta_{s2}$ are uniformly divided into $n_{s1}$ and $n_{s2}$ bins, respectively.
For each point $p_i$, we compute the bin classification label and the residual label as
\begin{align}
\mathcal{L}_{s_i}=\mathcal{F}_{c l s}\left(bin_{s_i}^{p}, {bin_{s_i}^{l}}\right) + \mathcal{F}_{res}\left(res_{s_i}^{p}, {res_{s_i}^{l}}\right),
\end{align}
where $i\in \{1, 2\}$ represents one supporting joint. $bin_{s_i}$ and $res_{s_i}$ are classification and residual labels, respectively.


\subsection{Total Loss}
The total loss function for training consists of three major components: grasp point prediction $\mathcal{L}_{gp}$, preliminary grasp generation using finger projection $\mathcal{L}_{fp}$, and grasp refinement using joint prediction $\mathcal{L}_{joint}$, which can be formally expressed as
\begin{align}
\mathcal{L}_{total}=\alpha\mathcal{L}_{gp} + \beta\mathcal{L}_{fp} + \gamma\mathcal{L}_{joint},
\end{align}
where
\begin{align}
\mathcal{L}_{joint} = \gamma_1\mathcal{L}_{m} + \gamma_2\mathcal{L}_{ms} + \gamma_3\mathcal{L}_{s_i}.
\end{align}

In our implementation, we set $\alpha=1$, $\beta=\gamma=5$, $\gamma_1=\gamma_2=\gamma_3=1$, and use the cross-entropy loss~\cite{shannon1948mathematical} and Huber (smooth-L1) loss~\cite{DBLP:journals/pami/RenHG017} for all classification and regression tasks, respectively.
