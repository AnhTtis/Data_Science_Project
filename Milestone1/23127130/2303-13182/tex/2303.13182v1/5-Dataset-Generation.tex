\section{Dataset Generation}\label{sec:dataset}
In this section, we explain the steps to generate our grasp dataset used in training.
For a given three-finger robotic hand, we first generate high-quality grasps for each object using \textit{GraspIt!}~\cite{DBLP:journals/ram/MillerA04} and keep those grasps that satisfy the grasp quality requirements. We obtain a grasp dataset consisting of contact information for each object.
Then we select a number of objects, randomly place them on the table, and examine all the grasps in a simulator. Those collision-free grasps and valid grasp poses remain. Finally, we place a camera in the scene and obtain a single-shot point cloud from a viewpoint. We map the grasp pose from the world coordinate system to the camera coordinate system.

\subsection{Single Object Grasp Dataset with Contacts}

To improve the diversity in shape, texture, and size, we select 80 objects from existing datasets~\cite{DBLP:conf/cvpr/FangWGL20,DBLP:journals/corr/CalliWSSAD15, DBLP:journals/corr/ChangFGHHLSSSSX15, xiangsapien}. Then we generate our grasp dataset for these objects in two steps.

First, we generate grasp poses and configurations for a single object. To uniformly sample the points on a single object for grasping, We down-sample the object mesh models to achieve a uniform distribution of sampling points
with their normal
in voxel space. For a sample, grasp candidates are searched in three dimensions $S_1 \times S_2 \times S_3$, where $S_1$ is the gripper depths, $S_2$ is the in-place rotation angle, and $S_3$ is the angle of spread joint. Given a set of $S_1 \times S_2 \times S_3$, we let fingers close in until they touch the object. We compute its $\epsilon$\emph{-quality}~\cite{ferrari1992planning} and save its grasp pose and configuration as a grasp annotation if the $\epsilon$\emph{-quality} is greater than a specified threshold. In our dataset, we generate 15000 high-quality grasp annotations for each object.

Second, we generate the contacts between the robotic hand and the objects for those high-quality grasps. Here, we ignore a grasp if its fingertips have no contact with the given object. We use the $k$-means algorithm to compute the clustering of the contact points and thus generate the corresponding contact point for each fingertip.

\subsection{Scene Grasp Dataset Generation}
To generate grasps in a scene from an arbitrary viewpoint, we first take a number of objects randomly from the object dataset and place them on the table in a stable pose. Second, for each object in the scene, we retrieve grasps from the grasp dataset and perform grasping tasks in the simulation environment. We filter out those grasps exhibiting collisions and invalid poses. Third, we place a camera to capture the scene from any viewpoint. The captured point cloud is used to obtain the grasp poses $\mathbf{G}$ and contact positions in the camera coordinate system.
