\section{Experiments and Results}\label{sec:results}
In this section, we first describe the experimental setup, including the dataset usage, evaluation metric, and implementation details. Then we demonstrate our experimental results in simulation and real scenarios. In our experiments and comparison, we observe that our approach can generate dense and robust grasps with high success and completion rates compared to the baseline approaches.

\subsection{Dataset Usage}
Our dataset has more than 80 categories distributed among 5000 different scenarios.
The network outputs a point-wise grasp pose and hand configurations for a single viewpoint cloud. We consider a grasp attempt is performed successful if the object can lift at least 30$cm$ high.

\subsection{Simulation Experiments}
We implement our CMG-Net using PyTorch \cite{DBLP:conf/nips/PaszkeGMLBCKLGA19} on NVIDIA  GPUs. Our end-to-end network is optimized using the Adam optimizer with a batch size of 32 and a learning rate of 0.004. Our input is a point cloud converted from the depth map captured using a depth camera. We randomly down-sample the point cloud to retain a reasonable number of points (e.g., 20,000).

\begin{table}[h]
\centering
\caption{Comparison in Simulation}
\begin{tabular}{c|cccc}
             & SR(\%) & CR(\%) & Quality \\ \midrule
GraspIt!     &   48 &   51 &     0.63    \\ \cmidrule(lr){1-4}
Multi-FinGan &  56  &   64 &    0.76    \\ \cmidrule(lr){1-4}
Ours         &  76  &  81  &   0.86
\end{tabular}\label{main_results}
\end{table}

\subsubsection{Evaluation Metrics}
To demonstrate the performance, we introduce three metrics~\cite{DBLP:conf/icra/LundellCLVWRMK21,DBLP:journals/corr/abs-2103-04783,qin2020s4g}:
		 \emph{Grasp Success Rate (SR)} is the rate of the number of successful grasps to the number of total attempts.
		 \emph{Grasp Completion Rate (CR)} is the rate of the number of objects grasped to the total number of objects after 1.5 times the number of grasp attempts.
		 \emph{Grasps Quality} often refers to $\epsilon$\emph{-quality}  metric~\cite{ferrari1992planning},  representing the radius of the largest 6D ball centered on the origin that can be surrounded by the convex hull of the wrench space~\cite{borst2004grasp}.


\subsubsection{Results}
The experimental results (Table~\ref{main_results}) show that our network can be well incorporated with the proposed representation. In comparison, our approach shows significant improvement against $GraspIt!$ and Multi-FinGan in terms of success rate, completion rate, and grasp quality.
Fig.~\ref{sim_res} shows a few simulation experiment results, demonstrating our method can handle small objects very well and generate high-quality grasps for complex scenes.


\begin{figure}[htbp]
  \subfloat[\label{sim_small}For small object.]{
      \includegraphics[width=0.45\columnwidth]{fig/small_object.pdf}
  }
  \subfloat[\label{sim_com}In cluttered scenes.]{
      \includegraphics[width=0.45\columnwidth]{fig/complex_scene_sim.pdf}
  }
  \caption{Grasp poses in different situations.}
  \label{sim_res}
\end{figure}

\subsubsection{Ablation Studies}
To improve the quality of handling small objects, we combine the SA and FP layers in PointNet++ through a U-shape network for feature extraction. In addition, we reduce the required regression dimensions by predicting the finger projection, which significantly improves the prediction quality of grasps and reduces the training cost. Table~\ref{ase} shows the enhancements resulting from each of the two modules. From the results, we can see that our proposed finger projection can achieve significant improvement.
\begin{table}[htbp]
\centering
\caption{Ablation Study}
\begin{tabular}{l|cccc}
              & U-Shaped & Finger-Pro & SR(\%) & Quality \\ \midrule
Baseline      &                  &              &  39      &    0.56  \\ \cmidrule(lr){1-5}
+U-Shaped     &    \checkmark    &              &  46      &    0.65  \\ \cmidrule(lr){1-5}
+Finger-Pro   &                  & \checkmark   &  72      &    0.80  \\ \cmidrule(lr){1-5}
Ours          &    \checkmark    & \checkmark   &  76      &    0.86
\end{tabular}\label{ase}
\end{table}

\subsection{Real-world Experiments}

We demonstrate that the network trained from the simulation data works well in a real environment. We use a \textit{Franka Emika Panda} equipped with a three-finger dexterous hand in our real-world experiment. We use an \textit{Intel RealSense D435} camera to capture the depth images. In order to remove the background and desk points for grasping prediction,
we perform depth image segmentation using a segmentation network~\cite{DBLP:conf/corl/XiangXMF20}.
Then the object point clouds are fed into our CMG-Net to obtain the final grasp. For each input, we keep the top 20 best grasps tried in the real field scene. A grasp is successful when an object can be picked up and stably moved to a specified location. Fig.~\ref{real_e} shows a snapshot of the real-world experiment environment and the objects grasped during the experiment.
There are three experimental scenarios with 3, 6, and 9 objects to be grasped, respectively. We obtain the average of 6 trials for each scenario. In these real scenarios, the objects are all novel for robotic hand.

The experimental result is shown in Table~\ref{real_world_results}. We obtained a success rate up to $74.4\%$ and a completion rate up to $86.1\%$ in the 6-object scenario. The results look similar even for the 9-object scenario. Our work demonstrates a good adaptability for a cluttered environment with many unknown objects.

\begin{figure}[ht]
\centering
\includegraphics[width=0.98\columnwidth]{fig/real_world.pdf}
\caption{A real-world experimental environment for object manipulation using a three-finger robotic hand and the given objects.}\label{real_e}
\end{figure}

\begin{table}[h]
\centering
\caption{Real Hardware Experiment Results}
\begin{tabular}{c|ccc}
Objects      & 3 & 6 & 9 \\ \midrule
SR(\%)       &  76.5  & 74.4  &  72.0    \\ \cmidrule(lr){1-4}
CR(\%)       &  88.9  & 86.1  &  83.3
\end{tabular}\label{real_world_results}
\end{table}



