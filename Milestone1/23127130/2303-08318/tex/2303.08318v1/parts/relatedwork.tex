\section{Related Work}

\subsection{Video Tagging}
Video tagging aims to find keywords that can describe the core content of a video. All methods can be categorized into two types based on the information they use.
% Based on the information they use, we categorize the existing methods into basic methods and sophisticated methods. 
\textbf{Basic methods} leverage information only from video content: textual, visual, and audio. There are two sub-types. Key-phrase extraction methods extract tags from video descriptions \cite{joint_kpe, autophrase, tag_from_danmu}, and rank them according to their relevance with core semantics \cite{joint_kpe, kotkov_tagranking_2021}. Multi-label classification methods assign tags from a predefined word set to videos \cite{nextvlad, hvu, wu_tencent_mmads_1st_2021}. 
\textbf{Sophisticated methods} leverage extra information besides video content, such as tag graph with knowledge \cite{ml_gcn, cma, li_mall_2022, jin_transfusion_2021}, query log \cite{taggnn}, user behavior \cite{jit2r}, and user profile \cite{wei_personalized_2019, li_long-tail_2019}.
Among all the above extra information, tag graph
% with knowledge can model tag semantic dependencies \cite{cma} and relieve long-tail phenomenon \cite{li_long-tail_2019}, hence it 
has attracted the most attention. Generally, the tag graph is built either from tag co-occurrence \cite{ml_gcn, cma, kssnet} or common knowledge bases \cite{li_zeroshot_2015, kssnet, li_long-tail_2019}. However, the former can only provide statistical relations, and the latter has low tag coverage. Contrary to the tag graph, social network information (user follow, specifically) has not been touched in video tagging.

\vspace{-1em}
\subsection{Heterogeneous Graph Neural Network}
Heterogeneous networks are graphs with more than one meta-paths. 
% Meta-path is a triplet of types: <source node, edge, destination node>. 
Heterogeneous graph neural networks are widely applied to derive a better node representation for them \cite{zhang_heteGNNref1_2021, liu_heteGNNref2_2021, zhang_heteGNNref3_2021, fan_heteGNNref4_2021}. For example, \citeauthor{han} \cite{han} proposed a method which is based on two-level attention. Concretely, node-level attention measures the importance of neighbors within each meta-path, and semantic-level attention measures the importance of different meta-paths. \citeauthor{hgt} \cite{hgt} proposed a method in which node and edge-type dependent parameters are used to parameterize the graph transformer network of each meta-path. \citeauthor{simple_hgn} \cite{simple_hgn} proposed a method to demonstrate that a simple graph attention network combined with three well-known techniques can outperform all previous models.

% \subsection{Long-tailed Multi-label Classification}
% There are only two existing long-tailed classification methods for multi-label setting specifically \cite{dbloss, guo_co_training_2021}. DBLoss \cite{dbloss} adjust binary cross entropy loss by using weight re-balance and negative tolerant regularization. Because re-balancing technique will hurt performance in head classes, \citeauthor{guo_co_training_2021} \cite{guo_co_training_2021} proposed to train two classifiers collaboratively on uniform and re-balanced sampled dataset to jointly improve performance on head and tail classes.
% There are three types of long-tailed multi-label learning methods \cite{longtailed_survey}: class re-balancing \cite{dbloss}, information augmentation \cite{mengmeng_longtailed_2019} and module improvement \cite{guo_co_training_2021}. Class re-balancing methods generally improve the performance of tail classes by directly raise their weights in loss \cite{dbloss}. However, it may also hurt the performance of head classes. Information augmentation methods usually incorporate external knowledge \cite{mengmeng_longtailed_2019} to aid the learning process of tail classes. Module improvement methods generally use assistant modules \cite{guo_co_training_2021} or model ensemble to solve long-tailed  problems.