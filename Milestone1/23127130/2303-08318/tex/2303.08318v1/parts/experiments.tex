\section{Experiments}

%%%%%%%%% dataset stat %%%%%%%%
\input{tables/dataset_stat}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Dataset}
\subsubsection{Data Collection and Preparation}
We obtained a large-scale and real-world micro-video dataset from three verticals (apparel, cosmetics, and food) on Kuaishou platforms. We randomly selected 500,000 micro-videos on the featured page from July 14 to August 14, 2021. Their tags are obtained from a variety of human-based signals (e.g., query-watch) and the online video tagging system which leverages only video textual descriptions. The follow relations among video creators is collected on August 14, and we filtered robot users along with their videos. We excluded too short (< 1 sec) or too long (> 60 secs) micro-videos, removed the non-visual tags, and filtered videos with less than 2 tags.

To quantify the dataset quality, we manually evaluated 1000 randomly sampled micro-videos, and the tagging accuracy and recall is 91.3\% and 66.2\%, respectively. Our dataset is split into 3 partitions: train (80\%), validate (10\%), and test (10\%).

%%%%%%%%% main results %%%%%%%%
\input{tables/performance_comp}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Tag Ontology Construction}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/ontology_construction_roc.pdf}
    \vspace{-1em}
    \caption{ROC of subtopic discovery}
    \label{fig:onto_cons_roc}
    \vspace{-2em}
\end{figure}

We constructed the tag ontology according to our proposed method in \autoref{sec:ontology_construction}. We annotated 1,000 out of 768,798 co-occurred tag pairs. The two thresholds $\delta_r,\epsilon_r$ are defined according to the precision and recall of subtopic discovery. Concretely, we chose $\delta_r$ where the precision is 90\%, and $\epsilon_r$ where the recall is 90\%. As illustrated in \autoref{fig:onto_cons_roc}, we measured the construction performance of subtopic discovery, and our method outperforms the rule-based method FBVO \cite{fang_folksonomy-based_2016}, unsupervised method OntoJob \cite{vrolijk_ontojob_2022} and OntoCQA \cite{Suryamukhi_modeling_2021} in a clear margin. Our constructed ontology covers 96.67\% of tags, and only 2.94\% of tags are covered through $\epsilon_r$ relaxation.


\subsubsection{Dataset Statistics}

We presented the statistics of our dataset in \autoref{table:dataset_stat}. Their diverged statistics lead to different results in the following experiments. The apparel dataset has the richest visual information (the \#videos per tag), and the food dataset has the lowest social network and tag ontology density (the least \#followers per video and the second least \#subtopics per tag).


\subsection{Experimental Settings}

\subsubsection{Experimental Details}

The tag word embedding is derived from the last hidden layer of BERT. The video frame embedding is acquired from TSN backboned by SwinTransformer. 
% Instead of dense sampling, the number of sampled frames for each video $F$ is set to 16, because micro-videos are generally very short.
In every GNN layer, we sample 4 neighbors for each type of relations following the practice of Deep Graph Library \cite{wang2019dgl} because of the GPU memory limit. The number of layers $L$ is set to 2. We set the batch size to be 1024, and used AdamW optimizer with a learning rate of 0.0005. We set feature and edge dropout to 0.2. Following \citeauthor{ganin_grl_2015} \cite{ganin_grl_2015} to stabilize the adversarial training, we gradually changed $\lambda$ from 0 to $\lambda_0$ according to the schedule:
$\lambda = \lambda_0 \left( 2/(1+exp(-\gamma p))-1 \right)$,
%\begin{equation}
%    \lambda = \lambda_0 \left( \frac{2}{1+exp(-\gamma p)}-1 \right),
%\end{equation}
where $p$ linearly changes from 0 to 1 in the training process. The $\gamma$ and $\lambda_0$ was set to 20 and 0.0005 for all datasets, respectively. All the settings are applied equally to all baseline methods.

\subsubsection{Evaluation Metrics} We adopted three common metrics for multi-label classification: mean Average Precision (mAP), Precision@K (P@K), and Recall@K (R@K). The P@K and R@K measure the tagging performance of top K tags, while mAP measures the overall ranking results of all tags.




\subsection{Baselines}
In order to verify our RADAR model, we compared it with the following baselines:
\begin{itemize}
    \item Video tagging methods: ML-GCN \cite{ml_gcn}, NeXtVLAD \cite{nextvlad, wu_tencent_mmads_1st_2021}, CMA \cite{cma}, and MALL-CNN \cite{li_mall_2022}. 
    We compared them to demonstrate the effectiveness of incorporating extra social networks and tag graph information. Note that NeXtVLAD, CMA, and MALL-CNN have sequence-level feature $\mathbf{h}\in\mathbb{R}^{F\times d}$ as input, with an extra aggregation module instead of meaning pooling in our method.
    \item Heterogeneous GNN methods: HAN \cite{han}, TagGNN \cite{taggnn}, HGT \cite{hgt}, and Simple-HGN \cite{simple_hgn} are four representative methods on heterogeneous GNN, and Simple-HGN is the state-of-the-art. 
    We compared our RADAR with them to demonstrate the effectiveness of our RADAR model.
\end{itemize}


\subsection{Performance Comparison}

Compared with baselines, our results are summarized in \autoref{table:comp_results}. By analyzing this table, we gained the following observations:
\begin{itemize}
    \item The heterogeneous GNN methods generally outperform the video tagging baselines. Their main difference is that tagging baselines better leverage video frame features, and GNN methods leverage tag ontology and extra social network. The improvement indicates that for micro-videos, incorporating social influence and tag relations is more effective than modeling frame features.
    \item Our proposed model RADAR outperforms all heterogeneous GNN methods consistently. Compared with the best-performing baselines, RADAR obtains relative mAP gains with 1.9\% in Apparel, 3.8\% in Cosmetics, and 7.9\% in Food dataset. The improvement demonstrates the superiority of RADAR in video tagging.
    \item Simple-HGN is the only heterogeneous GNN baseline whose relevant rank differs among datasets. Notably, its performance is the best in Food while the second-worst in Apparel dataset within all heterogeneous GNN baselines. Coincidentally, this relevant rank is opposite to the datasets' social network and tag ontology density. The denser \textit{is\_followed\_by} and \textit{is\_subtopic\_of} relations are, the worse Simple-HGN performs. Considering that Simple-HGN is the start-of-the-art method for general heterogeneous GNN, we attributed this phenomenon to the fact that video tagging is quite different from the previous tasks. In video tagging, The video feature from frames and tag feature from language models are two modalities with large semantic gaps. This phenomenon also demonstrates the necessity of our adversarial aggregation network which aggregates the visual and linguistic information.
\end{itemize}




\subsection{Ablation Studies}

In this section, we carried out several experiments to further analyze the effectiveness of our model, as reported in \autoref{table:ablation}. Concretely, we first explored the contributions of different relations in the video-tag network. We then analyzed the effectiveness of each component including Gated Graph Transformer (GGT) and Adversarial Aggregation Network (AAN).

\subsubsection{Ablation of different relations} We compared our model with the following variants: 1) \textbf{w/o followed by}, removing \textit{is\_followed\_by} relations; 2) \textbf{w/o subtopic}, removing \textit{is\_subtopic\_of} relations; and 3) \textbf{w/o subtopic \& followed by}, removing both relations. 

%%%%%%%%% ablation results %%%%%%%%
\input{tables/ablation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The result of \textbf{w/o followed by} is consistently lower than of \textbf{w/o subtopic} in three datasets, suggesting that the social network information contributes more than tag ontology. 

Note that if we remove \textit{is\_followed\_by} from our full model, the performance drop is more significant than we remove \textit{is\_followed\_by} from \textbf{w/o subtopic}. 
In another view, the addition of \textit{is\_followed\_by} relations will bring larger improvement when the \textit{is\_subtopic\_of} relations exist. Therefore, the combination of the two relations is not the naive summation of the two relations. This is evidence that the two relations have complementary information, and thus the separation of common and unique information in the AAN module is necessary.

\subsubsection{Ablation of GGT} We replaced the gated residual network in GGT with a residual network to be the variant \textbf{w/o gated residual}, and replaced the separate attention with mutual attention to be the variant \textbf{w/ mutual attention}. Compared with our full model, the performance of \textbf{w/o gated residual} and \textbf{w/ mutual attention} consistently dropped in all datasets. This indicates that it is important to filter irrelevant information for modeling Behavior Spread, and separate attention is a useful design under the modality gap.
 
\subsubsection{Ablation of AAN} To validate the impact of our proposed AAN, we conducted a series of experiments by introducing four variants: 1) \textbf{w/ concatenation}, replacing AAN with vector concatenation and a linear transformation layer, which is the simplest multi-modal aggregation method; 2) \textbf{w/ attention}, replacing AAN with attention module in \cite{han}; 3) \textbf{w/ LMF}, replacing AAN with Low-rank Multi-modal Fusion (LMF) \cite{liu_lmf_2018}, which is the best multi-modal vector aggregation method; and 4) \textbf{w/o $\boldsymbol{L}_{adv}$}, removing the adversarial loss $L_{adv}$. 

From \autoref{table:ablation}, we observed that our method outperforms all the multi-modal vector aggregation methods consistently. The improvement is not owing to the increased parameters, because \textbf{w/ LMF} has the most parameters. Compared with \textbf{w/o $\boldsymbol{L}_{adv}$}, we attributed the improvement to the separation of common and unique information in the visual and linguistic messages.

\subsubsection{Bidirectional edges}
We also explored the effect of bidirectional edges. Specifically, in the variant \textbf{w/ bidirectional edges}, we added \textit{is\_supertopic\_of} and \textit{has\_video} relations to the network. We also applied AAN to video nodes since they have two types of inbound messages. Note that we did not add \textit{follows} relations, because we hope the new videos do not send messages to the original graph for inductive learning. Although this variant slightly improves the performance, we do not adopt this setting considering the extra computational burden.



\subsection{Sensitivity Analysis}

We investigated how different choices of hyper-parameters affect performance. We explored two main parameters: the number of GNN layer $L$ and the maximal weight of unique loss $\lambda_0$.

As illustrated in \autoref{fig:sensitivity_analysis}, our model performs best when it has two GNN layers, which is common in graph neural networks due to over-smoothing. Our model achieves the best performance when $\lambda_0$ is around 0.0005 to 0.001, which indicates an appropriate balance between unique and common loss. Note that $\lambda_0=0$ is different from \textbf{w/o $\boldsymbol{L}_{adv}$}. Although both of them have no common information, the discriminator $D^l$ in the latter variant is removed, so it has no unique information as well. We observed that \textbf{w/o $\boldsymbol{L}_{adv}$} will cause a larger performance drop compared with setting $\lambda_0=0$. This indicates that the unique information is more important.

\begin{figure}[t]
    \centering
    
    \begin{subfigure}{.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/ablation_layer.png}  
        \label{fig:sensi_L}
    \end{subfigure}
    \begin{subfigure}{.655\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/ablation_lambda.png}  
        \label{fig:sensi_lambda0}
    \end{subfigure}
    \vspace{-2em}
    \caption{Sensitivity analysis}
    \label{fig:sensitivity_analysis}
\end{figure}