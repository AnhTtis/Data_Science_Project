\subsection{Tag Ontology Construction} \label{sec:ontology_construction}
In this subsection, we construct the tag ontology, a graph composed of tags and \textit{is\_subtopic\_of} relations. It is naturally a Directed Acyclic Graph (DAG), because a cycle made of \textit{is\_subtopic\_of} relations would cause a paradox. Ontology construction consists of two steps: subtopic discovery and DAG construction, as summarized in \autoref{fig:ontology_construction}. In subtopic discovery, we discover subtopic relations among tags using semi-supervised classification based on hand-crafted features. These features are derived from tag statistics in video-tag annotations so that we can cover all tags. This step will produce a DAG violated graph. Following that, graphical constraints are imposed to derive a DAG as the final tag ontology. 

% Why tag ontology ?
% On the definition of tag knowledge graph, we need to achieve a trade-off in the complexity of relation types. Simple relations like co-occur are easy to acquire but can only present weak semantics, and strong relations such as those in knowledge bases are hard to derive from video-tag annotations. 
% For example, $\textmd{tag}_A\rightarrow \textmd{tag}_B\rightarrow \textmd{tag}_C \rightarrow \textmd{tag}_A$ means $\textmd{tag}_A$ is a subtopic of itself.

% Inspired by FBVO \cite{fang_folksonomy-based_2016} and ConcepT \cite{liu_concept_2019}, we present to create tag ontology using visual information, tag statistics and graphical constraints. 
% Our difference with FBVO and ConcepT is that we introduce semi-supervision into subtopic discovery procedure, and we include subtopic classification score into graphical constraints.


\begin{figure*}[t]
    \includegraphics[width=0.9\linewidth]{figures/ontology_construction.pdf}
    \vspace{-1em}
    \caption{Tag Ontology Construction. (a)(b) \textit{Is\_subtopic\_of} relations are discovered, and form a DAG violated graph. (c) All tags are sorted, and then the reversal links are pruned. (d)(e) The pruned tag graph is equivalent to a DAG, which is our tag ontology.}
    \label{fig:ontology_construction}
\end{figure*}


\subsubsection{Subtopic Discovery} \label{subtopic_discovery}

We presume that \textit{if two tags have subtopic relations, they will co-occur in at least one micro-video}. Based on this, we formulate subtopic discovery as a binary classification problem on co-occurred tags. Formally, for each co-occurred tag pair $(u,v)$, $u,v\in\mathcal{V}_{\textmd{tag}}$, we predict their subtopic score $r(u,v)$:
\begin{align}
    r(u,v) &= g\left(\mathbf{k}(u,v)\right), \label{eq:sub_dis_classification} \\
    r(u,v) &=
        \begin{cases}
            1 & \text{ if } \textmd{$v$ is a subtopic of $u$}, \\
            0 & \text{ if } \textmd{no relationship between $u$ and $v$},
        \end{cases}
\end{align}
where $g(\cdot)$ can be an arbitrary classifier, $\mathbf{k}(u,v)\in\mathbb{R}^{d_r}$ is our hand-crafted features, and $d_r$ is the dimension of feature. The features are designed to reflect the following two properties between tags:

\textbf{Semantic overlap} refers to the overlap of meaning between two tags. If two tags $u$ and $v$ have subtopic relations, they must have semantic overlap. We measure the semantic overlap using Point-wise Mutual Information $\textmd{PMI}(u,v)\in\mathbb{R}$:
\begin{equation}
    \textmd{PMI}(u,v) = \textmd{log}\left ( \frac{p(u,v)}{p(u)p(v)} \right ),
\end{equation}
and Point-wise Kullback-Leibler divergence $\textmd{PKL}(u,v)\in\mathbb{R}$:
\begin{equation}
    \textmd{PKL}(u,v) = p(u,v)\textmd{log}\left ( \frac{p(u,v)}{p(u)p(v)} \right ),
\end{equation}
where $p(u,v)$ is the probability that $u,v$ occur in the same micro-video, and $p(u), p(v)$ are the occurrence probability of $u,v$, respectively. Compared with PMI, PKL is less biased towards the rare-occurred tags due to the additional $p(u,v)$.

\textbf{Semantic broadness} means the broadness of tag meaning. Broader semantics indicates a higher level in tag ontology (e.g., ``food'' has broader semantics than ``cake''). We measure it using two features. One is tag transfer probability $p(u|v)\in\mathbb{R}$ from $v$ to $u$:
\begin{equation}
    p(u|v) = \frac{\left |\textmd{videos that have tag $u$ and $v$}\right | }{\left |\textmd{videos that have tag $v$}\right | }.
\end{equation}
Intuitively, the larger $p(u|v)$ is, the more likely that $v$ is a subtopic of $u$. For example, given that tag ``cake'' is a subtopic of ``food'', and ``cake'' is used, it will be very likely to see ``food'' as well (i.e., large $p(\textmd{food}|\textmd{cake})$). Empirically, this probability has proven to be effective in industrial concept mining system \cite{liu_concept_2019}. The other feature is tag entropy $H(u)\in\mathbb{R}$:
\begin{equation}
    H(u) = - \sum_{w}{p(u|w)\textmd{log}(p(u|w))}.
\end{equation}
Intuitively, a tag with higher $H(u)$ has more and stronger inbound tag transfers from others, indicating a higher level in tag ontology. $H(u)$ is also used by FBVO \cite{fang_folksonomy-based_2016} to derive visual ontology.

With the semantic overlap and broadness features above, we can define our hand-crafted feature $\mathbf{k}(u,v)$ containing six basic features: $p(u|v)$, $p(v|u)$, $H(u)$, $H(v)$, $PMI(u,v)$, $PKL(u,v)$, and two second order features: $log\left ( p(u|v)/p(v|u) \right )$, $log\left ( H(u)/H(v) \right )$. Based upon these, the classification in \autoref{eq:sub_dis_classification} is in a semi-supervised manner. Concretely, the labels come from a small portion of the co-occurred tag pairs. Meanwhile, the features can be computed from all co-occurred tags to leverage information in unlabeled data.

For each tag pair ($u,v$), we can estimate the confidence score of $v$ being a subtopic of $u$: $\hat{r}(u,v)$. We keep those tag pairs whose $\hat{r}$ are larger than a threshold $\delta_r$, and link them into a directed graph. 


\subsubsection{DAG Construction}
Since not all predictions are correct, there will be DAG violations definitely, as exemplified in \autoref{fig:ontology_construction}(b). Thus, we impose graphical constraints to derive the final DAG. % as summarized in \autoref{fig:ontology_construction}(c)-(e).

As illustrated in \autoref{fig:ontology_construction}(c), we first sort all tags according to their entropy $H$ in descending order. Considering higher entropy means a higher level in tag ontology, we then keep \textit{is\_subtopic\_of} relations only from tags with lower entropy to higher ones. Afterward, the sorted tag list can be seen as the topological sorting of a DAG, which can be our tag ontology. Note that there might be isolated components, because lots of edges are filtered when we keep $\hat{r}(u,v)>\delta_r$ in subtopic discovery. For those components, we relax the constraint until they are not isolated or $\hat{r}(u,v)< \epsilon_r$, and link the remaining isolated ones to the tag with the highest entropy.


% From visual feature of videos, we can derive the visual hierarchy score $q(u|v)$ to measure the significance that tag $v$ is subtopic of tag $u$. Following \citeauthor{fang_folksonomy-based_2016} \cite{fang_folksonomy-based_2016}, in the first step, we calculate the visual distance $\mathbf{d}(u,v)\in\mathbb{R}^{d_v}$ between tag $u$ and $v$ by estimating the discrepancies between their included videos:
% \begin{equation}
%     \mathbf{d}(u,v) = \frac{1}{\left | \mathcal{V}^u_{\textmd{video}} \right | \cdot  \left | \mathcal{V}^v_{\textmd{video}} \right |}
%     \sum_{p\in\mathcal{V}^u_{\textmd{video}}, q\in\mathcal{V}^v_{\textmd{video}}}{K_{\sigma}\left(\mathbf{f}(p)-\mathbf{f}(q)\right)},
% \end{equation}
% where $\mathcal{V}_{\textmd{video}}^{u}$ and $\mathcal{V}_{\textmd{video}}^{v}$ is the training videos set containing tag $u$ and $v$ respectively, and $K_{\sigma}(\mathbf{f}(p)-\mathbf{f}(q))=exp\left( -\frac{\left \| \mathbf{f}(p)-\mathbf{f}(q) \right \|^2 }{2\sigma^2} \right)$ is the Gaussian kernel function. The second step is the calculation of tag visual similarity score $m(u,v)\in\mathbb{R}$:
% \begin{equation}
%     m(u,v) = \textmd{MeanPool}\left( \textmd{Sigmoid}\left(\mathbf{d}(u,v)\right) \right).
% \end{equation}
% In the final step, we treat $m(u,v)$ as probability mass and derive visual hierarchy $q(u|v)$ in the form of conditional probability:
% \begin{equation}
%     q(u|v) = \frac{m(u,v)}{\sum_{w}{m(w,v)}}.
% \end{equation}
% $q(u|v)$ can be viewed as visual transfer probability from $v$ to $u$, the high $q(u|v)$ suggests that $u$ may be a higher level tag than $v$.


% \begin{algorithm}[t]
%     \caption{Tag Ontology Construction Algorithm}
%     \label{alg:ontology_construction}
%     \KwIn{
%         Tag set $\mathcal{V}_{\textmd{tag}}$;
%         tag entropy $\mathcal{H}=\{H(u)|u\in\mathcal{V}_{\textmd{tag}}\}$;
%         subtopic probability $\mathcal{S}=\{ \hat{s}_{v\rightarrow u}|u,v\in\mathcal{V}_{\textmd{tag}} \}$;
%         threshold $\delta$.
%     }
%     \KwOut{A DAG of tag ontology $\boldsymbol{G}^*=\{\mathcal{V}^*,\mathcal{E}^*\}$}
%     \BlankLine
%     \For{$p\in\mathcal{V}_{\textmd{tag}}$ in descending tag entropy order}{
%         $\mathcal{V}^* \Leftarrow p$;
        
%         $\mathcal{V}_p = \{ v | \hat{s}_{v\rightarrow p}\ge\delta, v\in\mathcal{V}_{\textmd{tag}} \}$;
        
%         \For{$c\in\mathcal{V}_p$}{
%             \If{$H(p)>H(c)$}{
%                 $\mathcal{E}^*\Leftarrow (c\rightarrow p)$;
                
%                 $\mathcal{V}^*\Leftarrow c$;
%             }
%         }
        
%     }
    
%     Output $\boldsymbol{O}$
% \end{algorithm}