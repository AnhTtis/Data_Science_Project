\subsection{RADAR Architecture} \label{sec:method}

\subsubsection{Overall RADAR Architecture} \label{sec:RADAR_overview}
To derive a better video and tag representation, we design RADAR, an $L$ layer graph neural network. Considering that tag nodes have two types of inbound edges, while video nodes have only one, we apply different GNN propagation strategies to them. Concretely, we divide propagation into two stages: \textit{intra-relation message passing} for both video and tag nodes and \textit{cross-relation message aggregation} for tag nodes only. Correspondingly, each GNN layer in RADAR has two components: \textbf{Gated Graph Transformer} (GGT) and \textbf{Adversarial Aggregation Network} (AAN), as illustrated in \autoref{fig:overview}(b).

\begin{figure*}[t]
    \centering
    %\includegraphics[width=0.99\linewidth]{figures/GGT_AAN.pdf}
    \includegraphics[width=0.8\linewidth]{figures/GGT_AAN.pdf}
    \vspace{-1em}
    \caption{(a) GGT aggregates neighbor information using graph transformer, and filters irrelevant information using gated residual network.
    (b) ANN separates subtopic and video messages into common and unique information, and aggregates them.}
    \label{fig:ggt_aan}
    % For each node $t$, Gated Graph Transformer (GGT) receives its neighbors $s\in\mathcal{N}^l_r(t)$ of a specific relation $r$ as input. GGT first aggregates neighbor information using graph transformer, and then filters irrelevant information using gated residual network to derive the neighborhood message $\mathbf{m}^l_r(t)$ for node $t$. For each tag node $t$, Adversarial Aggregation Network (AAN) receives subtopic $\mathbf{m}_{r_1}^l$ and video $\mathbf{m}_{r_2}^l$ messages from GGT. ANN separates these messages into common ($\mathbf{c}_{r_1}^l$, $\mathbf{c}_{r_2}^l$) and unique ($\mathbf{u}_{r_1}^l$, $\mathbf{u}_{r_2}^l$) information, and then aggregates them to derive a new tag representation $\mathbf{h}^l$. We ensure the separation effectiveness by adversarial learning using discriminator $D^l$ and Gradient Reversal Layer (GRL).
\end{figure*}

In the \textit{intra-relation message passing} stage, GGT is applied for both tag and video nodes to derive message information propagated from their neighbors. Formally, video $v$ receives message $\mathbf{m}_{r_3}^l(v)\in\mathbb{R}^d$ passed from its $r_3$ neighbors:
\begin{equation} \label{eq:follow_msg}
    \mathbf{m}_{r_3}^l(v) = \textmd{GGT}\left( \mathbf{h}^{l-1}(v), \left\{\mathbf{h}^{l-1}(s)|s\in\mathcal{N}^l_{r_3}(v)\right\} \right),
\end{equation}
and video representation is updated by:
\begin{equation} \label{eq:follow_msg}
    \mathbf{h}^l(v) = \mathbf{m}_{r_3}^l(v),
\end{equation}
where the superscript $l$ denotes the ($l$)-th layer, $\mathbf{h}^l(v)\in\mathbb{R}^d$ is the output representation of node $v$, and $\mathcal{N}^l_{r_3}(v)$ are node $v$'s neighbors of $r_3$ relations. 
Meanwhile, tag $t$ receives messages $\mathbf{m}_{r_1}^l(t)$, $\mathbf{m}_{r_2}^l(t)\in\mathbb{R}^d$ passed from its $r_1$ and $r_2$ neighbors, respectively:
\begin{empheq}[left=\empheqlbrace]{align} \label{eq:two_message_tag_node}
    &\mathbf{m}_{r_1}^l(t) = \textmd{GGT}\left( \mathbf{h}^{l-1}(t), \left\{\mathbf{h}^{l-1}(s)|s\in\mathcal{N}^l_{r_1}(t)\right\} \right),\\
    &\mathbf{m}_{r_2}^l(t) = \textmd{GGT}\left( \mathbf{h}^{l-1}(t), \left\{\mathbf{h}^{l-1}(s)|s\in\mathcal{N}^l_{r_2}(t)\right\} \right),
\end{empheq}
where $\mathcal{N}^l_{r_1}(t), \mathcal{N}^l_{r_2}(t)$ are node $t$'s neighbors of $r_1$ and $r_2$ relations, respectively.

For each video, the neighborhood messages carry information about the social influence of its \textit{is\_followed\_by} neighbors. Since not all videos are created through imitation, in GGT we use a gated mechanism to filter irrelevant information for tackling Behavior Spread modeling challenge. 
For each tag $t\in\mathcal{V}_{\textmd{tag}}$, the neighborhood messages bring linguistic and visual knowledge from \textit{is\_subtopic\_of} and \textit{has\_tag} neighbors, respectively. Therefore, we propose AAN to solve the visual-linguistic aggregation challenge in the next stage.

In the \textit{cross-relation message aggregation} stage, AAN is applied for tag nodes only to aggregate subtopic messages $\mathbf{m}_{r_1}^l(t)$ and video messages $\mathbf{m}_{r_2}^l(t)$. For each tag $t\in\mathcal{V}_{\textmd{tag}}$, its representation is updated by:
\begin{equation} \label{eq:aggregation}
    \mathbf{h}^l(t) = \textmd{AAN}\left(\mathbf{m}^l_{r_1}(t), \mathbf{m}^l_{r_2}(t)\right).
\end{equation}

After updating node representations following the above two stages in each GNN layer, we get the final representation $\mathbf{h}^L$.


\subsubsection{Gated Graph Transformer} \label{sec:ggt}


The goal of GGT is to derive a message information $\mathbf{m}^l_r(t)\in\mathbb{R}^d$ propagated from their neighbors in the ($l$)-th layer, where $r$ is the neighbor relation. As illustrated in \autoref{fig:ggt_aan}(a), we apply graph transformer to aggregate neighborhood information, and gated residual network to filter irrelevant information. Formally, we denote the central node as target $t$ and neighbor node as source $s\in\mathcal{N}^l_r(t)$. The GGT is parameterized by target node type $a_t$, source node type $a_s$, and neighbor relation $r$.

Inspired by the Transformer architecture \cite{transformer, hgt}, we first treat target node $t$ as the query $\mathbf{q}_r^l(t)\in\mathbb{R}^d$, and source nodes $s\in\mathcal{N}^l_r(t)$ as both keys $\mathbf{k}_r^l(s)\in\mathbb{R}^d$ and values $\mathbf{v}_r^l(s)\in\mathbb{R}^d$:
\begin{empheq}[left=\empheqlbrace]{align}
    &\mathbf{q}_r^l(t) = \textmd{Q-Linear}^l_{a_t} \left( \mathbf{h}^{l-1}(t) \right),\\
    &\mathbf{k}_r^l(s) = \textmd{K-Linear}^l_{a_s} \left( \mathbf{h}^{l-1}(s) \right),\\
    &\mathbf{v}_r^l(s) = \textmd{V-Linear}^l_{a_s} \left( \mathbf{h}^{l-1}(s) \right)\mathbf{W}_r^{\textmd{MSG}},
\end{empheq}
where $\textmd{Q-Linear}^l_{a_t}$, $\textmd{K-Linear}^l_{a_s}$, $\textmd{V-Linear}^l_{a_s}$ are the linear transformation layers corresponding to specific node types, and $\mathbf{W}_r^{\textmd{MSG}}\in\mathbb{R}^{d\times d}$ is the message matrix of relation $r$.

Then, we calculate dot product between query and keys, and apply softmax to measure the importance of source node $s$ with respect to target node $t$ as $\alpha_r^l(t,s)\in\mathbb{R}$:
\begin{equation} \label{graph_attention}
    \alpha_r^l(t,s) = \frac{exp \left( \mathbf{q}_r^l(t)\mathbf{W}_r^{\textmd{ATT}}\mathbf{k}_r^l(s)^T / \sqrt{d} \right)}
    {\sum_{w\in\mathcal{N}^l_r(t)}{exp \left(\mathbf{q}_r^l(t)\mathbf{W}_r^{\textmd{ATT}}\mathbf{k}_r^l(w)^T / \sqrt{d} \right)}},
\end{equation}
where $\mathbf{W}_r^{\textmd{ATT}}\in\mathbb{R}^{d\times d}$ is an edge-based projection matrix. Note that the softmax is done within source nodes of the same relation $r$, and we name it separate attention. This is different from the mutual attention in the existing graph transformer \cite{hgt} in which normalization is done within neighbors of all relations. We adopt separate attention because the different relations in the video-tag network are substantially different, and can not be weighed together.

Afterward, we can get the output of graph transformer $\mathbf{o}^l(t)\in\mathbb{R}^d$ by weighted summation of values:
\begin{equation}
    \mathbf{o}_r^l(t) = \sum_{s\in\mathcal{N}^l_r(t)}{\alpha_r^l(t,s)\mathbf{v}_r^l(s)}.
\end{equation}

Finally, we propose the \textbf{gated residual network} to derive neighborhood message $\mathbf{m}^l_r(t)$:
\begin{empheq}[left=\empheqlbrace]{align}
    &\mathbf{z}_r^l(t) = \textmd{Sigmoid} \left(
    \mathbf{A}^l_r \mathbf{h}^{l-1}(t) + 
    \mathbf{B}^l_r \mathbf{o}_r^l(t)
    \right), \\
    &\mathbf{m}_r^l(t) =  \mathbf{z}_r^l(t) \odot \mathbf{o}_r^l(t) + 
    \left(\mathbf{1}-\mathbf{z}_r^l(t)\right) \odot \left(\mathbf{P}^l_r \mathbf{h}^{l-1}(t) \right ),
\end{empheq}
where $\mathbf{z}_r^l(t)\in\mathbb{R}^d$ is the gating values, $\odot$ denotes element-wise multiplication, and $\mathbf{A}^l_r$, $\mathbf{B}^l_r$, $\mathbf{P}^l_r\in\mathbb{R}^{d\times d}$ are linear transform matrices with respect to relation $r$ in the ($l$)-th layer. The gated mechanism generally filters irrelevant information. For \textit{is\_followed\_by} neighbors, it suppresses neighbors when videos are created not from imitation. For \textit{is\_subtopic\_of} neighbors, it keeps only general semantics as an abstraction of subtopic tags.

For simplicity, we omit GELU activation function after $\mathbf{P}^l_r$, bias vector after $\mathbf{A}^l_r$, $\mathbf{B}^l_r$, and $\mathbf{P}^l_r$, and superscript $l$ in $\mathbf{W}_r^{\textmd{MSG}}$ and $\mathbf{W}_r^{\textmd{ATT}}$.




\subsubsection{Adversarial Aggregation Network} \label{sec:aan}

The AAN is designed only for tag nodes to aggregate linguistic and visual knowledge from subtopic message and video messages, respectively. Both messages can be divided into unique (exists in one), and common information (exists in both). Existing methods such as concatenation or attention mechanism \cite{han} are sub-optimal due to the ignorance of the redundancy in common information. Concretely, concatenation duplicates the common information, and attention mechanism tends to overlook the unique information \cite{lu_aan_ref_ReID_2020, li_aan_ref_CTR_2020}. 

In our AAN, we propose to separate common and unique information in order to remove the redundant information while keeping the complementary one. Formally, for each target tag node $t\in\mathcal{V}_{\textmd{tag}}$, the incoming messages $\mathbf{m}_{r_1}^l(t)$ and $\mathbf{m}_{r_2}^l(t)$ are first separated into common message $\mathbf{c}_{r_1}^l(t)\in\mathbb{R}^d$, $\mathbf{c}_{r_2}^l(t)\in\mathbb{R}^d$:
\begin{empheq}[left=\empheqlbrace]{align}
    &\mathbf{c}_{r_1}^l(t) = C^l \left(
        \mathbf{m}_{r_1}^l(t)
    \right), \\
    &\mathbf{c}_{r_2}^l(t) = C^l \left(
        \mathbf{m}_{r_2}^l(t)
    \right),
\end{empheq}
and unique message $\mathbf{u}_{r_1}^l(t)$, $\mathbf{u}_{r_2}^l(t)$:
\begin{empheq}[left=\empheqlbrace]{align}
    &\mathbf{u}_{r_1}^l(t) = U^l_{r_1} \left(
        \mathbf{m}_{r_1}^l(t)
    \right), \\
    &\mathbf{u}_{r_2}^l(t) = U^l_{r_2} \left(
        \mathbf{m}_{r_2}^l(t)
    \right),
\end{empheq}
where $C^l(\cdot)$, $U^l_{r_1}(\cdot)$ and $U^l_{r_2}(\cdot)$ are linear transformation layers for learning common features, and unique features for the \textit{is\_subtopic\_of} and \textit{has\_tag} relation, respectively.
% where $C^l(\cdot)$ is the linear transformation layer for learning common features, and $U^l_{r_1}(\cdot), U^l_{r_2}(\cdot)$ are two linear transformation layers for learning unique features for the \textit{is\_subtopic\_of} and \textit{has\_tag} relation, respectively.

In order to ensure the above separation is successful, we need to keep the distributions of $\mathbf{c}_{r_1}^l(t), \mathbf{c}_{r_2}^l(t)$ as close as possible, and those of $\mathbf{u}_{r_1}^l(t), \mathbf{u}_{r_2}^l(t)$ as far as possible. There are two potential methods. One is to measure the distance between distributions using mutual information. Its drawback is that only the upper and lower bound of mutual information can be estimated. Thus, we adopt the other method, i.e., adversarial training, which is built around a min-max game. A discriminator is used to distinguish two unique features $\mathbf{u}_{r_1}^l(t), \mathbf{u}_{r_2}^l(t)$ while being confused about the common ones $\mathbf{c}_{r_1}^l(t), \mathbf{c}_{r_2}^l(t)$. Formally, we have:
\begin{equation} \label{eq:l_adv}
    \underset{D^l,U^l_{r_1},U^l_{r_2}}{\textmd{min}}\underset{C^l}{\textmd{max}} \ \mathcal{L}^l_{\textmd{adv}} = \mathcal{L}^l_u + \lambda \mathcal{L}^l_c,
\end{equation}
\begin{empheq}[left=\empheqlbrace]{align}
    &\mathcal{L}^l_u = - \textmd{log}\left( D^l \left( \mathbf{u}_{r_1}^l(t) \right) \right) -
        \textmd{log}\left( 1 - D^l \left(\mathbf{u}_{r_2}^l(t) \right) \right), \\
    &\mathcal{L}^l_c = - \textmd{log}\left( D^l \left( \mathbf{c}_{r_1}^l(t) \right) \right) - 
        \textmd{log}\left( 1 - D^l \left(\mathbf{c}_{r_2}^l(t) \right) \right),
\end{empheq}
where $D^l$ is a binary linear classifier served as the discriminator, $\lambda$ is a trade-off parameter, and $\mathcal{L}^l_c, \mathcal{L}^l_u$ are discrimination loss for common and unique features, respectively.

After common-unique separation, we directly aggregate them to derive the output representation $\mathbf{h}^l(t)$ for tag node $t$:
\begin{empheq}[left=\empheqlbrace]{align}
    &\mathbf{c}^l(t) = \textmd{MeanPool} \left( \mathbf{c}_{r_1}^l(t) + \mathbf{c}_{r_2}^l(t) \right), \\
    &\mathbf{u}^l(t) = \textmd{MaxPool} \left( \mathbf{u}_{r_1}^l(t) + \mathbf{u}_{r_2}^l(t) \right), \\
    &\mathbf{h}^l(t) = \frac{1}{2} \left( \mathbf{u}^l(t) + 
        \mathbf{c}^l(t) \right).
\end{empheq}


\subsubsection{Training and Inference} \label{sec:train_inference}
We compute the semantic similarity between a video and all tags as our predictions. Specifically, for each video node $v$ and tag node $t$, the probability whether node $v$ has tag $t$ is estimated as $\hat{s}(v,t)\in\mathbb{R}$:
\begin{equation}
    \hat{s}(v,t) = \textmd{Sigmoid} \left( \mathbf{h}^L(v) \cdot \left( \mathbf{h}^L(t) \right)^T \right).
\end{equation}

During training, we apply the binary cross-entropy (BCE) loss $L_\textmd{tag}$ as tagging loss:
\begin{equation}
    \mathcal{L}_{\textmd{tag}} = \textmd{BCE} \left( y(v,t), \hat{s}(v,t) \right),
\end{equation}
where $y(v,t)$ is the ground truth for whether video $v$ has tag $t$.

We use gradient reversal layer (GRL) \cite{ganin_grl_2015} to implement the min-max game in \autoref{eq:l_adv} for end-to-end training. The final loss $L$ is composed of classification loss and adversarial training loss:
\begin{equation}
    \mathcal{L} = \mathcal{L}_{\textmd{tag}} + \sum_{l=1}^L{\mathcal{L}^l_{\textmd{adv}}}.
\end{equation}

During inference, a new video has no out-coming relations, because it has no tags (no \textit{has\_tag} relations), and only previously-uploaded videos can influence the newly-uploaded ones (no out-coming \textit{is\_followed\_by} relations). Hence the node representation of old nodes can be computed in advance. Thus, when a new video is added to the video-tag network, we only need to calculate its in-coming \textit{is\_followed\_by} messages $\mathbf{m}^l_{r_3}(t)$ to get the video representation. Therefore, our model can achieve inductive inference.