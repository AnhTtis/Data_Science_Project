\section{Experiment}
\subsection{Datasets}
We use two large-scale datasets for comparison, Rico~\cite{deka2017rico} and PubLayNet~\cite{zhong2019publaynet}.
As we mention in \cref{subsec:layout_diffusion_unconditional}, an element in a layout for each dataset is described by the five attributes.
For preprocessing, we set the maximum number of elements per layout $M$ to 25. If a layout contains more elements, we discard the whole layout.

We provide an overview of each dataset.
\textbf{Rico} is a dataset of user interface designs for mobile applications containing 25 element categories such as text button, toolbar, and icon.
We divide the dataset into 35,851 / 2,109 / 4,218 samples for train, validation, and test splits.
\textbf{PubLayNet} is a dataset of research papers containing five element categories, such as table, image, and text. We divide the dataset into 315,757 / 16,619 / 11,142 samples for train, validation, and test splits.


\subsection{Evaluation Metrics}
We employ two primary metrics: FID and Maximum IoU (Max.).
These metrics take into account both fidelity and diversity~\cite{heusel2017gans}, which are two mutually complementary properties widely used in evaluating generative models.
FID~\cite{heusel2017gans} captures the similarity of generated data to real ones in feature space. We employ an improved feature extraction model for layouts \cite{Kikuchi2021} instead of a conventional method~\cite{lee2020neural} to compute FID.
Maximum IoU~\cite{Kikuchi2021} measures the conditional similarity between generated and real layouts.
The similarity is measured by computing optimal matching that maximizes average IoU between generated and real layouts that have an identical set of categories.
For reference, we show the FID and Maximum IoU computed between the validation and test data as~\emph{Real data}.


\subsection{Tasks and Baselines}
We test LayoutDM on six tasks for evaluation.

\noindent
\textbf{Unconditional} generates layouts without any conditional input or constraint. \\
\textbf{Category$\rightarrow$size+position (C$\rightarrow$S+P)} is a generation task conditioned on the category of each element~\cite{kong2022blt}. \\
\textbf{Category+size$\rightarrow$ position (C+S$\rightarrow$P)} is conditioned on the category and size of each element. \\
\textbf{Completion} is conditioned on a small number of elements whose attributes are all known. Given a complete layout, we randomly sample from $0\%$ to $20\%$ of elements. \\
\textbf{Refinement} is conditioned on a noisy layout in which only geometric information is perturbed~\cite{rahman2021ruite}.
Following RUITE~\cite{rahman2021ruite}, we synthesize the input layout by adding random noise to the size and position of each element.
We sample noise from a standard normal distribution with a mean of 0 and a variance of 0.01. \\
\textbf{Relationship} is conditioned on the category of each element and some relationship constraints between the elements~\cite{lee2020neural}. Following CLG-LO~\cite{Kikuchi2021}, we employ the size and location relationships and randomly sample $10\%$ relationships between elements for the experiment.

The first four tasks handle basic layout fields.
We include a few task-agnostic models for comparison using existing controllable layout generation methods or simple adaptation of generative models in the following:

\noindent \textbf{LayoutTrans} is a simple autoregressive model~\cite{gupta2021layout} trained on a element-level shuffled layout, following \cite{paschalidou2021atiss}.
We set a variable generation order to c$\rightarrow$w$\rightarrow$h$\rightarrow$x$\rightarrow$y. \\
\noindent \textbf{MaskGIT$^\ast$} is originally a non-autoregressive model for unconditional fixed-length data generation~\cite{chang2022maskgit}.
We use \texttt{[PAD]} to enable variable-length generation. \\
\noindent \textbf{BLT} is a non-autoregressive model with layout-specific decoding strategy~\cite{kong2022blt}. \\
\noindent \textbf{BART} is a denoising autoencoder that can solve both comprehension and generation tasks based on Transformer encoder-decoder backbone~\cite{lewis2020bart}.
We randomly generate a number of \texttt{[MASK]} tokens from a uniform distribution between one and the sequence length, and perform masking based on the number. \\
\noindent \textbf{VQDiffusion$^\ast$} is a diffusion-based model originally for text-to-image generation~\cite{gu2022vector}.
We adapt the model for layout using $K=C+4B+2$ tokens, including \texttt{[PAD]}.

\subsection{Implementation Details}
We re-implement most of the models since there are few official implementations publicly available except~\cite{gupta2021layout,Kikuchi2021,kong2022blt}\footnote{Unfortunately, most datasets have no official train-val-test splits, and previous approaches work on different splits and pre-processing strategies. Furthermore, models for FID computation also vary. Thus, we cannot directly compare our results with the reported figure in the literature.}.
We train all the models on the two datasets with three independent trials and report the average of the results.

LayoutDM follows VQDiffusion for hyper-parameters unless specified, such as configurations for $p_{\theta}$ and the transition matrix parameters \ie $\alpha_{t}$ and $\gamma_{t}$.
We set the loss weight $\lambda=0.1$ (in \cref{eq:total_loss}) and the diffusion timesteps $T=100$.
For optimization, we use AdamW~\cite{loshchilov2019decoupled} with learning rate of $5.0 \times 10^{-4}$, $\beta_{1}=0.9$, and $\beta_{2}=0.98$.

Many models, including LayoutDM, use Transformer~\cite{vaswani2017attention} encoder backbone.
We define a shared configuration as follows:
4 layers, 8 attention heads, 512 embedding dimensions, 2048 hidden dimensions, and $0.1$ dropout rate.
For other models with extra modules, we adjust the number of hidden dimensions to roughly match the number of parameters for a fair comparison.
We randomly shuffle elements in the layout to avoid fixed-order generation during training.
We search best hyper-parameters to obtain the best FID using the validation set.

\input{tables/comparison/inpainting.tex}
\input{tables/visualization/inpainting.tex}
\input{tables/comparison/unconditional.tex}

\subsection{Quantitative Evaluation} \label{sec:quantitative_evaluation}
\paragraph{C$\rightarrow$S+P, C+S$\rightarrow$P, Completion}
In these tasks, we inject conditions by masking.
We summarize comparisons in \cref{tab:comparison_inpainting}.
As task-specific models, we include LayoutVAE~\cite{jyothi2019layoutvae}, NDN-none~\cite{lee2020neural}, and LayoutGAN++~\cite{Kikuchi2021} for C$\rightarrow$S+P. We also adapt these models for C+S$\rightarrow$P.
LayoutDM outperforms other models except LayoutTrans~\cite{gupta2021layout} in completion.
The significant performance gap between LayoutDM and VQDiffusion* suggests the contribution of our proposals to go beyond the simple discrete diffusion models discussed in \cref{subsec:layout_diffusion_unconditional}.
Results in the completion suggest that a combination of padding and diffusion models is the primal key to the generation quality.
We find that FID and Maximum IoU are not highly correlated only in the completion task. We conjecture that Maximum IoU may become unstable when categories are also predicted, unlike the C$\rightarrow$S+P and C+S$\rightarrow$P tasks where categories are given.

\cref{fig:comparison_inpainting} shows the qualitative results of some models, including LayoutDM.
We can see that LayoutDM generates high-quality layouts with fewer layout aesthetics violations, such as misalignment and overlap, given diverse conditions.

\paragraph{Unconditional Generation}
\cref{tab:comparison_unconditional} summarizes the results of unconditional generation.
Unconditional layout generation methods often assume fixed order for element generation \eg top-to-bottom rather than random order for better generation quality by constraining the prediction.
For reference, we additionally report the results of LayoutTrans~\cite{gupta2021layout} trained on the fixed element order (LayoutTrans-fixed).
Although we design LayoutDM's primarily for conditional generation, LayoutDM achieves the best FID under random element order setting.
We conjecture that BLT's poor performance is due to train-test mask distribution inconsistency caused by their hierarchical masking strategy for training.
BLT masks a randomly sampled number of fields from a single semantic group \ie category, position, or size.
However, decoding starts with all masked tokens in inference.
The alignment metric of Real data stays at 0.109 in Rico.
Too small alignment values of LayoutTrans and MaskGIT can be a signal of producing trivial outputs in Rico.



\input{tables/comparison/refinement.tex}
\input{tables/visualization/refinement.tex}
\paragraph{Refinement}
Our LayoutDM performs this task with a combination of the strong constraints of element categories, \ie, setting $\bm{z}^\mathrm{known} = \{(c_{1}, \texttt{[MASK]}, \ldots, \texttt{[MASK]}), \ldots\}$, and the weak constraints that geometric outputs appear near noisy inputs.
As an example of the weak constraint, we describe a constraint that imposes the x-coordinate estimate of $i$-th element close to the noisy continuous observation $\hat{x}_i$.
We denote a sliced vector of the prior term $\bm{\pi}$ in \cref{eq:prior_addition} that corresponds to the x-coordinate of $i$-th element as $\bm{\pi}_x^i \in \mathbb{R}^K$ and define by:
\begin{equation}
    \left[ \bm{\pi}_x^i \right]_j =
    \begin{cases}
        1 & \text{if}\ |\mathrm{loc}(j)-\hat{x}_{i}| < m~\text{and}~j \in X \\
        0 & \text{otherwise},
    \end{cases}
\end{equation}
where $m$ is a hyper-parameter indicating a margin,
$X$ is a set of indices denoting tokens for $x$ in the vocabularies,
and $\mathrm{loc}(j)$ is a function that returns the centroid value of $j$-th token in the vocabularies.
We define similar constraints for the other geometric variables and elements.

We summarize the performance in \cref{tab:refinement}.
We additionally report DocSim~\cite{patil2020read} (Sim) to measure the similarity of a predicted and its corresponding ground truth layout.
Imposing noisy geometric fields as a weak prior significantly improves the masking-only model and makes the performance much closer to RUITE~\cite{rahman2021ruite}, which is a denoising model not applicable to other layout tasks.
We compare some results in \cref{fig:comparison_refinement}.
Both LayoutDM and RUITE successfully recover complete layouts from non-trivially noisy layouts.


\paragraph{Relationship}
\input{tables/comparison/relationship.tex}
\input{tables/visualization/relationship.tex}
\input{tables/ablation_diffusion.tex}

We use \cref{eq:prior_addition_by_gradient} to incorporate the relational constraints during the sampling step of LayoutDM.
We follow~\cite{Kikuchi2021} to employ the loss functions penalizing size and location relationships between elements that do not match user specifications.
We define the loss functions for continuous bounding boxes, and we have to convert the predicted discrete bounding boxes to continuous ones in a differentiable manner.
Given estimated probabilities of discrete x-coordinates $p(x)$, for example, we compute the continuous x-coordinate $\bar{x}$ by %
$\bar{x} = \sum_{n \in X} p(x\!=\!n) \, \mathrm{loc}(n)$.
Similar conversion applies to the other attributes.
Empirically, we find that
applying the logit adjustment multiple times (three times in our experiments)
to each diffusion step moderately improves performance.


We compare LayoutDM with two task-specific approaches: NDN-partial~\cite{lee2020neural} and CLG-LO based on LayoutGAN++~\cite{Kikuchi2021}.
We show the results in \cref{fig:fid_violation_plots_relationship}.
We additionally report constraint violation error rates~\cite{Kikuchi2021}.
LayoutDM can control the strength of the logit adjustment as in \cref{eq:prior_addition} and produces an FID-violation trade-off curve.
LayoutDM is comparable to NDN-partial in Rico and outperforms NDN-partial by a large margin in PubLayNet.
Although LayoutDM is inferior to CLG-LO in both datasets, note that the average runtime of CLG-LO is 4.0s, which is much slower than 0.5s in LayoutDM.
We show some results of LayoutDM in \cref{fig:comparison_relationship}.

\subsection{Speed-Quality Trade-off}
Runtime is also essential for a controllable generation.
We show a speed-quality trade-off curve for C+S$\rightarrow$P as shown in \cref{fig:tradeoff_time_fid}.
The Transformer encoder-only models, such as LayoutDM and BLT, can achieve fast generation at the sacrifice of quality.
We employ fast-sampling strategy employed in discrete diffusion models~\cite{austin2021structured} for LayoutDM by $p_{\theta}(\bm{z}_{t-\Delta}|\bm{z}_{t}) \propto \sum_{\tilde{\bm{z}}_{0}} q(\bm{z}_{t-\Delta},\bm{z}_{t}|\tilde{\bm{z}}_{0}) \tilde{p}_{\theta}(\tilde{\bm{z}}_{0}|\bm{z}_{t})$, where $\Delta \in \mathbb{N}$ indicates a step size for generation in $\frac{T}{\Delta}$ steps.
Despite being a task-agnostic model, LayoutDM achieves the best quality-speed trade-off except for task-specific LayoutGAN++~\cite{Kikuchi2021} that runs under 10ms.

\subsection{Ablation Study}  \label{sec:ablation_study}
We investigate whether techniques in \cref{subsec:layout_diffusion_unconditional} improve the performance.
First, we evaluate a choice of quantization methods for the geometric fields of elements.
Instead of KMeans, we compute centroids for the quantization by:
\begin{itemize}[noitemsep,nolistsep,leftmargin=*]
    \item Uniform: This is dataset-agnostic quantization, which is popular in previous works~\cite{arroyo2021variational,gupta2021layout,kong2022blt}. Following~\cite{gupta2021layout}, we choose $\{0.0, \frac{1}{B}, \ldots, \frac{B - 1}{B}\}$ and $\{\frac{1}{B}, \ldots, \frac{B - 1}{B}, 1.0\}$ for the position and size, respectively.
    \item Percentile: we sort the data into equally sized groups and obtain average values for each group as the centroids. This is dataset-specific quantization similar to KMeans.
\end{itemize}
We show the result at the bottom of \cref{tab:ablation_model_components}.
We additionally report the Alignment metric (Align.) used in~\cite{Kikuchi2021} since the choice of the quantization affects the alignment between elements.
Compared to Linear and Percentile, KMeans quantization significantly improves both FID and Alignment.
We confirm that our modality-wise diffusion and decoupled positional encoding both moderately improve the performance, as we show at the top half of \cref{tab:ablation_model_components}.