\section{Introduction}
Graphic layouts play a critical role in visual communication.
Automatically creating a visually pleasing layout has tremendous application benefits that range from authoring of printed media~\cite{zhong2019publaynet} to designing application user interface~\cite{deka2017rico}, and there has been a growing research interest in the community.
The task of layout generation considers the arrangement of elements, where each element has a tuple of attributes, such as category, position, or size, and depending on the task setup, there could be optional control inputs that specify part of the elements or attributes.
Due to the structured nature of layout data, it is crucial to consider relationships between elements in a generation.
For this reason, current generation approaches either build an autoregressive model~\cite{gupta2021layout,arroyo2021variational} or develop a dedicated inference strategy to explicitly consider relationships~\cite{lee2020neural,Kikuchi2021,kong2022blt}.




\begin{figure}[t]
    \centering
    \includegraphics[width=\hsize]{images/teaser.pdf}
    \caption{
        Overview of LayoutDM.
        \textbf{Top}: LayoutDM is trained to gradually generate a complete layout from a blank state in discrete state space.
        \textbf{Bottom}: During sampling, we can steer LayoutDM to perform various conditional generation tasks without additional training or external models.
    }
    \label{fig:teaser}
\end{figure}

In this paper, we propose to utilize discrete state-space diffusion models~\cite{hoogeboom2021argmax,austin2021structured,gu2022vector} for layout generation tasks.
Diffusion models have shown promising performance for various generation tasks, including images and texts~\cite{ho2020denoising}.
We formulate the diffusion process for layout structure by \emph{modality-wise discrete diffusion}, and train a denoising backbone network to progressively infer the complete layout with or without conditional inputs.
To support variable-length layout data, we extend the discrete state-space with a special \texttt{PAD} token instead of the typical end-of-sequence token used in autoregressive models.
Our model can incorporate complex layout constraints via \textit{logit adjustment}, so that we can refine an existing layout or impose relative size constraints between elements without additional training.

We discuss two key advantages of LayoutDM over existing models for conditional layout generation.
Our model avoids the immutable dependency chain issue~\cite{kong2022blt} that happens in autoregressive models~\cite{gupta2021layout}.
Autoregressive models fail to perform conditional generation when the condition disagrees with the pre-defined generation order of elements and attributes.
Unlike non-autoregressive models~\cite{kong2022blt}, our model can generate variable-length elements.
We empirically show in \cref{sec:quantitative_evaluation} that naively extending non-autoregressive models by padding results in suboptimal variable length generation while padding combined with our diffusion formulation leads to significant improvement.


We evaluate LayoutDM on various layout generation tasks tackled by previous works~\cite{kong2022blt,lee2020neural,rahman2021ruite,paschalidou2021atiss} using two large-scale datasets, Rico~\cite{deka2017rico} and PubLayNet~\cite{zhong2019publaynet}.
LayoutDM outperforms task-agnostic baselines in the majority of cases and shows promising performance compared with task-specific baselines.
We further conduct an ablation study to prove the significant impact of our design choices in LayoutDM, including quantization of continuous variables and positional embedding.

We summarize our contributions as follows:
\begin{itemize}[noitemsep,nolistsep,leftmargin=*]
    \item We formulate the discrete diffusion process for layout generation and propose a modality-wise diffusion and a padding approach to model highly structured layout data.
    \item We propose to inject complex layout constraints via masking and logit adjustment during the inference,
    so that our model can solve diverse tasks in a single model.
    \item We empirically show solid performance for various conditional layout generation tasks on public datasets.
\end{itemize}


