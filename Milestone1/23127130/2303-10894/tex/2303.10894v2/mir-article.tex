%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdfxelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
% \documentclass[sn-mathphys]{sn-jnl}% Math and Physical Sciences Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style
%%\documentclass[sn-standardnature]{sn-jnl}% Standard Nature Portfolio Reference Style
%%\documentclass[default]{sn-jnl}% Default
\documentclass[default,iicol]{sn-jnl}% Default with double column layout

%%%% Standard Packages
%%<additional latex packages if required can be included here>
%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published
%%%%  by Springer Nature. The guidance has been prepared in partnership with
%%%%  production teams to conform to Springer Nature technical requirements.
%%%%  Editorial and presentation requirements differ among journal portfolios and
%%%%  research disciplines. You may find sections in this template are irrelevant
%%%%  to your work and are empowered to omit any such section if allowed by the
%%%%  journal you intend to submit to. The submission guidelines and policies
%%%%  of the journal take precedence. A detailed User Manual is available in the
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

\jyear{2021}%

%% as per the requirement new theorem styles can be included as shown below

\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}%
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\uumbered% uncomment this for unnumbered level heads
\definecolor{mygray}{gray}{.94}
\newcommand{\textBC}[2]{\textbf{\textcolor{#1}{#2}}}


\usepackage{color, colortbl}
\usepackage{xcolor}
\usepackage{url}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue,
    % citecolor=green
}
\definecolor{ggray}{RGB}{127,127,127}
\definecolor{reda}{RGB}{202,0,0}
\definecolor{mycellreda}{RGB}{202,0,0}
\definecolor{redb}{RGB}{217,148,143}
\definecolor{myyellow}{RGB}{190,144,0}
\definecolor{mygreen}{RGB}{0,136,51}
\definecolor{myblue}{RGB}{0,102,204}
\definecolor{mycellblue}{RGB}{0,102,204}
% \usepackage{amsmath,amssymb,amsfonts}
% \usepackage{algorithmic}
% \usepackage{graphicx}
% \usepackage{textcomp}

\begin{document}

% \title[Article Title]{Article Title}
\title[OOD-MAE]{\textbf{M$^{2}$SNet: Multi-scale in  Multi-scale Subtraction Network for Medical  Image  Segmentation}}
%%=============================================================%%
%% Prefix   -> \pfx{Dr}
%% GivenName    -> \fnm{Joergen W.}
%% Particle -> \spfx{van der} -> surname prefix
%% FamilyName   -> \sur{Ploeg}
%% Suffix   -> \sfx{IV}
%% NatureName   -> \tanm{Poet Laureate} -> Title after name
%% Degrees  -> \dgr{MSc, PhD}
%% \author*[1,2]{\pfx{Dr} \fnm{Joergen W.} \spfx{van der} \sur{Ploeg} \sfx{IV} \tanm{Poet Laureate}
%%                 \dgr{MSc, PhD}}\email{iauthor@gmail.com}
%%=============================================================%%

\author[1,3]{\fnm{Xiaoqi} \sur{Zhao}}
% \author[1]{\fnm{Jing} \sur{Zhang}$\textsuperscript{\Letter}$}
\author[1]{\fnm{Hongpeng} \sur{Jia}}
\author[1,4]{\fnm{Youwei} \sur{Pang}}
\author[2]{\fnm{Long} \sur{Lv}}
\author[2]{\fnm{Feng} \sur{Tian}}
\author[1]{\fnm{Lihe} \sur{Zhang}}
\author[2]{\fnm{Weibing} \sur{Sun}}
\author[1]{\fnm{Huchuan} \sur{Lu}}

\affil[1]{\orgname{IIAU-Lab, Dalian University of Technology}, \orgaddress{\city{Dalian}, \country{China}}}
\affil[2]{\orgname{Zhongshan Hospital of Dalian University}, \orgaddress{\city{Dalian}, \country{China}}}
\affil[3]{\orgname{Yale University}, \orgaddress{\country{USA}}}
\affil[4]{\orgname{Nanyang Technological University}, \orgaddress{\country{Singapore}}}

%%==================================%%
%% sample for unstructured abstract %%
%%==================================%%

\abstract{Accurate medical image segmentation is critical for early medical diagnosis. Most existing methods are based on U-shape structure and 
use element-wise addition or concatenation to fuse different level features progressively in decoder. 
%
However, both the two operations easily generate plenty of redundant information, which will weaken the complementarity between different level features, resulting in inaccurate localization and blurred edges of lesions. 
%
To address this challenge, we propose a general multi-scale in multi-scale subtraction network (M$^{2}$SNet) to finish diverse segmentation from medical image. 
%
Specifically, we first design a basic subtraction unit (SU) to produce the difference features between adjacent levels in encoder. Next, we expand the single-scale SU to the intra-layer multi-scale SU, which can provide the decoder with both pixel-level and structure-level difference information. Then, we pyramidally equip the multi-scale SUs at different levels with varying receptive fields, thereby achieving the inter-layer multi-scale feature aggregation and obtaining rich multi-scale difference information. In addition, we build a training-free network ``LossNet'' to comprehensively supervise the task-aware features from bottom layer to top layer, which drives our multi-scale subtraction network to capture the detailed and structural cues simultaneously. 
%
Without bells and whistles, our method performs favorably against most state-of-the-art methods under different evaluation metrics on eleven datasets of four different medical image segmentation tasks of diverse image modalities, including color colonoscopy imaging, ultrasound imaging, computed tomography (CT), and optical coherence tomography (OCT). The source code can be available at \url{https://github.com/Xiaoqi-Zhao-DLUT/MSNet}.
}

%%================================%%
%% Sample for structured abstract %%
%%================================%%

% \abstract{\textbf{Purpose:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
%
% \textbf{Methods:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
%
% \textbf{Results:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
%
% \textbf{Conclusion:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.}

\keywords{Medical Image Segmentation, Subtraction Unit, Multi-scale in Multi-scale, Difference Information, LossNet}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{fig/msnet++_msnet_unet_unet++_comparison.pdf}
\caption{Illustration of different medical image segmentation architectures. }
\label{fig:msnet++_msnet_unet_unet++_comparison}
\end{figure*}
\section{Introduction}
\label{sec:introduction}
%
As % 问题背景介绍
the important role in computer-aided diagnosis system, accurate medical image segmentation technique can provide the doctors with great guidance for making clinical decisions. There are three general challenges in accurate segmentation: \textbf{Firstly}, U-shape structures~\cite{FPN,UNet} have received considerable attention due to their abilities of utilizing multi-level information to reconstruct high-resolution feature maps. In UNet~\cite{UNet}, the up-sampled feature maps are concatenated with feature maps skipped from the encoder and convolutions and non-linearities are added between up-sampling steps, as shown in Fig.~\ref{fig:msnet++_msnet_unet_unet++_comparison} (a). Subsequent UNet-based methods design diverse feature enhancement modules via attention mechanism~\cite{ResUnet++,PraNet,UniMRSeg}, gate mechanism~\cite{BMPM,GateNet,GateNetv2}, transformer technique~\cite{UTNet,nnWNet,TransUNet_MIA}, as shown in Fig.~\ref{fig:msnet++_msnet_unet_unet++_comparison} (b). UNet++~\cite{UNet++} uses nested and dense skip connections to reduce the  semantic  gap  between  the feature  maps  of  encoder  and  decoder, as shown in Fig.~\ref{fig:msnet++_msnet_unet_unet++_comparison} (c). Generally speaking, different level features in encoder have different characteristics. High-level ones have more semantic information which helps localize the objects, while low-level ones have more detailed information which can capture the subtle boundaries of objects. The  decoder leverages the level-specific and cross-level characteristics to generate the final high-resolution prediction. Nevertheless, the aforementioned methods directly use an element-wise addition or concatenation to fuse any two level features from the encoder and transmit them to the decoder. 
These simple operations do not pay more attention to differential information between different levels. This drawback not only generates redundant information to dilute the really useful features but also weakens the characteristics of level-specific features, which results in that the network can not balance 
accurate localization and subtle boundary refinement.
\textbf{Secondly}, due to the limited receptive field, a single-scale convolutional kernel is difficult to capture context information of size-varying objects. Some methods~\cite{FPN,UNet,UNet++,U2Net,MINet,ZoomNeXt} rely on the inter-layer multi-scale features and progressively integrate the semantic context and texture details from diverse scale representations. Others~\cite{GateNetv2,Spider,UniMRSeg} focus on extracting the intra-layer multi-scale information based on the atrous spatial pyramid pooling module~\cite{ASPP} (ASPP) or DenseASPP~\cite{DenseASPP} in their networks. However, the ASPP-like multi-scale convolution modules will produce many extra parameters and computations. Many methods~\cite{BMPM,UCNet_RGBDSOD,PFNet_COD,BDRAR_Shadow,AFFPN_Shadow} usually equip several ASPP modules into the encoder/decoder blocks of different levels, while some ones~\cite{R3Net,DMRA_RGBDSOD,CoNet_RGBDSOD,Rank-Net_COD} install it on the highest-level encoder block. 
\textbf{Thirdly}, the form of the loss function directly provides the direction for the gradient optimization of the network. In segmentation field, there are many loss functions are proposed to supervise the prediction at the different levels, such as the L1 loss, cross-entropy loss and weighted cross-entropy loss~\cite{FCN} in the pixel level, the SSIM~\cite{SSIM} loss and uncertainty-aware loss~\cite{ZoomNet} in the region level, the IoU loss, Dice loss and consistency-enhanced loss~\cite{MINet} in the  global level. Although these basic loss functions and their variants have different optimization characteristics, the designs of complex manual math forms are really time-consuming for many researches. In order to obtain comprehensive performance, models usually integrate a variety of loss functions, which places great demands on the training skills of the researchers. Therefore, we think that it is necessary to introduce an intelligent loss function without complex manual designs to comprehensively supervise the segmentation prediction.

In this paper, we propose a novel multi-scale in multi-scale subtraction network (M$^{2}$SNet) for general medical image segmentation. 
%
Firstly, we design a subtraction unit (SU) and apply it to each pair of adjacent level features. The SU highlights the useful difference information between the features and eliminates the interference from the redundant parts.
%
Secondly, we collect the extreme multi-scale information with the help of the proposed multi-scale in multi-scale subtraction module. For the inter-layer multi-scale information, we pyramidally concatenate multiple subtraction units to capture the large-span cross-level information. Then, we aggregate level-specific features and multi-path cross-level differential features and then generate the final prediction in decoder. For the intra-layer multi-scale information, we improve the single-scale subtraction unit to the multi-scale subtraction unit through a group of full one filters with different kernel sizes, which can achieve naturally multi-scale subtraction aggregation without introducing extra parameters. As shown in Fig.~\ref{fig:msnet++_msnet_unet_unet++_comparison}, MSNet equips the inter-layer multi-scale subtraction module and M$^{2}$SNet has both the inter-layer and intra-layer multi-scale subtraction structures.  
%
Thirdly, we propose a LossNet to automatically supervise the extracted feature maps from bottom layer to top layer, which can optimize the segmentation from detail to structure with a simple L2-loss function.



Our main contributions are summarized as follows:
\begin{itemize} 
\item  We present a new segmentation framework by replacing traditional addition or concatenation feature fusion with an efficient subtraction aggregation.  
\item  We propose a simple yet general multi-scale in multi-scale subtraction network (M$^{2}$SNet) for diverse medical image segmentation. With multi-scale in multi-scale module, the multi-scale complementary information from lower order to higher order among different levels can be effectively obtained, thereby comprehensively enhancing the perception of organs or lesion areas.
\item  We design an efficient intra-layer multi-scale subtraction unit (MSU). Due to the low parameters and computation of MSU, it can be equipped for all cross-layer aggregations in our M$^{2}$SNet.
\item  We build a general training-free loss network to implement the detail-to-structure supervision in the feature levels, which provides the important supplement to the loss design based on the prediction itself.
\item  We verify the effectiveness of the M$^{2}$SNet on four challenge medical segmentation tasks: polyp segmentation, breast cancer segmentation, lung infection and OCT layer segmentation corresponding to the color colonoscopy imaging, ultrasound imaging, computed tomography (CT), and optical coherence tomography (OCT) image input modality, respectively. In addition, M$^{2}$SNet won the second place in the MICCAI2022 GOALS International Ophthalmology Challenge.
\end{itemize}

% \textit{Compared with the MICCAI version~\cite{MSNet} of this work, the following extensions are made. 
% \textbf{\uppercase\expandafter{\romannumeral1})} Based on the structure of the original single-scale subtraction unit, we develop the stronger intra-layer multi-subtraction unit and construct the multi-scale in multi-scale subtraction network (M$^{2}$SNet). Meanwhile, M$^{2}$SNet carries forward the low FLOPs spirit of the previous MSNet.
% \textbf{\uppercase\expandafter{\romannumeral2})} We re-organize the introduction and add more thorough related works in Sec.~\ref{sec:relatedwork}. 
% \textbf{\uppercase\expandafter{\romannumeral3})} We report much more extensive experimental results that demonstrate the superiority of M$^{2}$SNet in $4$ popular medical segmentation tasks.
% \textbf{\uppercase\expandafter{\romannumeral4})} We verify the performance of the M$^{2}$SNet for multi-class segmentation in the  
% \href{https://aistudio.baidu.com/aistudio/competition/detail/230/0/introduction}{MICCAI 2022 Challenge: Glaucoma Oct Analysis and Layer Segmentation (GOALS)}
% We won \href{https://aistudio.baidu.com/aistudio/competition/detail/230/0/leaderboard}{the second place (2/100)}. 
% \textbf{\uppercase\expandafter{\romannumeral5})} We further provide more implementation details and thorough ablation studies at qualitative and quantitative aspects.
% \textbf{\uppercase\expandafter{\romannumeral6})} We perform in-depth analyses and discussions for our multi-scale subtraction unit.}

\section{Related Work}
\label{sec:relatedwork}
\subsection{Medical Image Segmentation Network}\label{subsec:misn}
According to the characteristics of different organs or lesions, we classify existing medical image segmentation methods into two types: medical-general and medical-specific one.
\\
\textbf{Medicine-general Methods.}
With the U-Net~\cite{UNet} achieving stable performance in the medical image segmentation field, the U-shape structure with encoder-decoder has become the basic segmentation baseline. U-Net++~\cite{UNet++} integrates both the long connection and short connection, which can reduce the semantic
gap between the feature maps of the encoder and decoder sub-networks. For attention U-Net~\cite{Attention_UNet}, an attention gate is embedded in each transition layer between the encoder and decoder block, which can automatically learn to focus on target structures of varying shapes and sizes. 
%
More recently, Transformer-based approaches~\cite{Segmenter,SegFormer,SwinUNet} have gained prominence by exploiting self-attention to capture long-range dependencies beyond the limited receptive fields of CNNs. SegFormer~\cite{SegFormer} and SwinUNet~\cite{SwinUNet} combine hierarchical feature extraction with attention-driven decoding, while hybrid designs such as UTNet~\cite{UTNet} and TransUNet~\cite{TransUNet_MIA} embed Transformer modules into U-shaped frameworks to infuse global context into local representations. Despite their improved expressiveness, these models often suffer from quadratic complexity and limited local inductive bias, which can hinder scalability and fine-grained predictions.
%
In response, state-space–based architectures have emerged as an efficient alternative. Methods built on Mamba~\cite{Mamba}, such as Sigma~\cite{Sigma}, SegMamba~\cite{Segmamba} and U-Mamba~\cite{U-mamba}, replace attention with structured recurrence, achieving linear time complexity and reduced memory footprint. By combining the inherent structural bias of state-space models with CNNs’ local detail preservation, hybrid CNN–Mamba designs offer a promising route to scalable, high-resolution, and real-time medical image segmentation.
\\
\textbf{Medicine-specific Methods.}
In the polyp segmentation task, SFA~\cite{SFA} and PraNet~\cite{PraNet}, focus on recovering the sharp boundary between a polyp and its surrounding mucosa. The former proposes a selective feature aggregation structure and a boundary-sensitive loss function under a shared encoder and two mutually constrained decoders. The latter utilizes a reverse attention module to establish the relationship between the region and boundary cues. UM-Net~\cite{UM-Net} introduces color transfer to reduce color–polyp dependency and incorporating uncertainty estimation with variance correction to enhance the reliability of segmentation results. 
In addition, Ji \textit{et al.}~\cite{PNSNet} utilize spatio-temporal information to build the video polyp segmentation model.  
%
In the lung infection task, Paluru \textit{et al.}~\cite{Anam-Net} propose an anamorphic depth embedding-based lightweight CNN to segment anomalies in chest CT images. Inf-Net~\cite{Inf-Net} builds the implicit reverse attention and explicit edge attention to model the boundaries. BCS-Net~\cite{BCS-Net} has three progressive boundary context-semantic reconstruction blocks, which can help the decoder to capture the piecemeal region for lung infection. 
%
In the breast segmentation task, Byra \textit{et al.}~\cite{SKUNet} develop a selective kernel via an attention mechanism to adjust the receptive fields of the U-Net, which can further improve the segmentation accuracy of breast tumors. Chen \textit{et al.}~\cite{NU-net} propose a nested U-net to achieve robust representation of breast tumors by exploiting different depths and sharing weights. 
%%%%%%%%%%%%%%%%

We can see that the medicine-general methods are usually towards general challenges (i.e., rich feature representation, multi-scale information extraction and cross-level feature aggregation). And, the medicine-specific methods propose targeted solutions based on the characteristics of the current organ or lesion, such as designing a series of attention mechanisms, edge enhancement modules, uncertainty estimation, etc. 
%
However, both general medicine-general and medicine-specific models rely on a large number of addition or concatenation operations to achieve feature fusion, which weakens the specificity parts among complementary features. Our proposed multi-scale subtraction module naturally focuses on extracting difference information, thus providing the decoder with efficient targeted features.


\subsection{Multi-scale Feature Extraction}\label{subsec:multiscale}
Scale cues play an important role in capturing contextual information of objects. Inspired by the scale-space theory that has been widely validated as an effective and theoretically sound framework, more and more multi-scale methods are proposed. Compared with single-scale features, multi-scale features are beneficial to address naturally occurring scale variations. This characteristic can help the medical segmentation models perceive lesions with different scales. According to the form, current multi-scale based methods can be roughly divided into two categories, namely, the inter-layer multi-scale structure and the intra-layer multi-scale structure. Inter-layer multi-scale structure is based on features with different scales extracted by the feature encoder and progressively aggregates them in decoder, such as the U-shape~\cite{UNet,UNet++,FPN,ACSNet,PraNet,U2Net,DSS,MINet} architecture.
{Among them, dense skip connections are widely used in decoder because of the advantages in gradient backpropagation and information aggregation. U-Net++~\cite{UNet++} undergoes  a dense convolution block and re-designs skip pathways to transform the connectivity of the encoder and decoder sub-networks. ICUnet++~\cite{ICUnet++} replaces the convolutional layer of U-Net++ with inception structure and add attention gate  module before each skip connection to filter interference information. DSSNet~\cite{DSSNet} and CPFP~\cite{CPFP_RGBDSOD} use a fluid pyramid integration strategy to make better use of multi-scale cross-modal/level features. MINet~\cite{MINet} proposes the self-interaction and aggregate interaction strategy to avoid the interference in feature fusion caused by large resolution differences. }
Intra-layer multi-scale structure usually equips the multi-scale pluggable modules, such as ASPP~\cite{ASPP}, DenseASPP~\cite{DenseASPP}, FoldASPP~\cite{GateNetv2}, and PAFEM~\cite{DANet_RGBDSOD} to construct the parallel multi-branch convolution layers with different dilated rates to obtain a rich combination of receptive fields. 
In this work, we apply our subtraction unit to both inter-layer and intra-layer multi-scale feature fusion, which can fully show the flexibility and effectiveness of the subtraction unit, and its general gains will be more convincing. It should be emphasized that we naturally aggregate multi-scale subtractive features in the decoder without focusing on skip connection designs.
\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{fig/Pipeline.pdf}
\caption{Overview of the proposed multi-scale subtraction network.}
\label{fig:Pipeline}
\end{figure*}
\subsection{Loss Method}\label{subsec:loss}
Most loss functions in image segmentation are based on cross-entropy or coincidence measures. The traditional cross-entropy loss treats the categories information equally.  Long \textit{et al.}~\cite{FCN} propose a weighted cross-entropy loss (WCE) for each class to offset the class imbalance in the data. Lin \textit{et al.}~\cite{Focal-loss} introduce the weights of difficult and easy samples to propose the Focal loss.  Dice loss~\cite{V-net} is proposed as the loss function of coincidence measurement in V-Net, which can effectively suppress problems caused by category imbalance. Tversky loss~\cite{Tversky-loss} is a regularized version of Dice loss to control the contribution of accuracy and recall to the loss function. Wong \textit{et al.}~\cite{EL-LOSS} propose exponential logarithmic loss (EL Loss) through the weighted summation of Dice loss and WCE loss to improve the segmentation accuracy of small structure objects. Taghanaki \textit{et al.}~\cite{Combo-loss} find that there is a risk in using the loss function based on overlap alone, and propose the como-loss to combine Dice loss as a regularization term with WCE loss to deal with the problem of input and output imbalance. Although these various loss functions have different effects at different levels, it is indeed time-consuming and laborious to manually design these complex functions. To this end, we propose the automatic and comprehensive segmentation loss structure, coined as the LossNet.

\section{Method}
The M$^{2}$SNet architecture is shown in Fig.~\ref{fig:Pipeline}, in which there are five encoder blocks ($\mathbf{E}^i$, $i \in \left \{1, 2, 3, 4, 5 \right \}$), a multi-scale in multi-scale subtraction module (MMSM) and four decoder blocks ($\mathbf{D}^i$, $i \in \left \{1, 2, 3, 4 \right \}$). We adopt the Res2Net-50 as the backbone to extract five levels of features. First, we separately adopt a $3 \times 3$ convolution for feature maps of each encoder block to reduce the channel to $64$, which can decrease the number of parameters for subsequent operations. Next, these different level features are fed into the MMSM and output five complementarity 
enhanced features (${CE}^i$, $i \in \left \{1, 2, 3, 4, 5 \right \}$). Finally, each ${CE}^i$ progressively participates in the decoder and generates the final prediction. In the training phase, both the prediction and ground truth are input into the LossNet to achieve supervision. We describe the multi-scale in multi-scale subtraction module in Sec.~\ref{sec:msm} and give the details of LossNet in Sec.~\ref{sec:lossnet}.
\subsection{Multi-scale in Multi-scale Subtraction Module}\label{sec:msm}
We use  $F_{A}$ and $F_{B}$ to represent adjacent level feature maps. They all have been activated by the ReLU operation. We  define a basic subtraction unit (SU):
  \begin{equation}\label{equ:1}
 \begin{split}
     SU = Conv(\vert F_{A} \ominus F_{B} \vert ),
 \end{split}
\end{equation}
 where $\ominus$ is the element-wise subtraction operation, $ \vert \cdot \vert$ calculates the absolute value and $Conv(\cdot)$ denotes the convolution layer. Directly performing single-scale subtraction on the features of element positions is only to establish the difference relationship on the isolated pixel level, without considering that the lesion may have the characteristics of regional clustering. Compared to the MICCAI version~\cite{MSNet} of MSNet with the single-scale subtraction unit, we design a powerful intra-layer multi-scale subtraction unit (MSU) and improve MSNet to M$^{2}$SNet. As shown in Fig.~\ref{fig:MSU}, we utilize the multi-scale convolution filters with fixed full one weights of size $1 \times 1$, $3 \times 3$ and $5 \times 5$ to calculate the detail and structure difference values according to the pixel-pixel and region-region pattern. Using multi-scale filters with fixed parameters not only can directly capture the multi-scale difference clues between initial feature pairs at matched spatial locations, but also achieve efficient training without introducing additional parameter burdens. Therefore, M$^{2}$SNet can maintain the same low computation as MSNet and achieve higher precision performance. The entire multi-scale subtraction process can be formulated as: 
\begin{equation}\label{equ:2}
  \begin{split}
     MSU = Conv(  \quad \quad \quad \quad \quad \quad     \\
     \vert Filter( F_{A} )_{1\times1}\ominus Filter( F_{B} )_{1\times1} \vert + \\ \vert Filter( F_{A} )_{3\times3}\ominus Filter( F_{B} )_{3\times3} \vert + \\ \vert Filter( F_{A} )_{5\times5}\ominus Filter( F_{B} )_{5\times5} \vert ),
 \end{split}
 \end{equation}
 where $Filter(\cdot)_{n \times n}$ represents the full one filter of size $n \times n$. The MSU can capture the complementary information of $F_{A}$ and $F_{B}$ and highlight their differences from texture to structure, thereby providing richer information for the decoder.

To obtain higher-order complementary information across multiple feature levels,  
we horizontally and vertically concatenate multiple MSUs to calculate a series of differential features with different orders and receptive fields. The detail of the multi-scale in multi-scale subtraction module can be found in Fig.~\ref{fig:Pipeline}. We aggregate the scale-specific feature ($MS^{i}_{1}$) and cross-scale differential features ($MS^{i}_{n \neq 1}$) between the corresponding level and any other levels to generate complementarity enhanced feature ($CE^{i}$). This process can be formulated as follows:
\begin{equation}\label{equ:3}
 \begin{split}
     CE^{i} = Conv(\sum_{n=1}^{6-i}MS^{i}_{n} ) \quad i=1, 2, 3, 4, 5.
 \end{split}
\end{equation}
Finally, all $CE^{i}$ participate in decoding and then the polyp region is segmented.  
\subsection{LossNet}\label{sec:lossnet}

\begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{fig/MS-Unit-v3.pdf}
\caption{Detailed diagram of multi-scale subtraction unit.}
\label{fig:MSU}
\end{figure}

In the proposed model, the total training loss can be written as:
\begin{equation}\label{equ:4}
 \begin{split}
    \mathcal{L}_{total}=\mathcal{L}_{IoU}^w +\mathcal{L}_{BCE}^w + \mathcal{L}_{f},
 \end{split}
\end{equation}
 where $\mathcal{L}_{IoU}^w$ and $\mathcal{L}_{BCE}^w$ represent the weighted IoU loss and binary cross-entropy (BCE) loss which have been widely adopted in segmentation tasks. We use the same definitions as in ~\cite{PraNet,F3Net,BASNet} and their effectiveness has been validated in these works. Different from them, we extra use a LossNet to further optimize the segmentation from detail to structure. Specifically, we use an ImageNet pre-trained classification network, such as VGG-16, to extract the multi-scale features of the prediction and ground truth, respectively. Then, their feature difference is computed as loss $\mathcal{L}_{f}$: 
\begin{equation}\label{equ:5}
 \begin{split}
    \mathcal{L}_{f} = {l}_{f}^{1} + {l}_{f}^{2} + {l}_{f}^{3} + {l}_{f}^{4}.
 \end{split}
\end{equation}
Let ${F}_{P}^{i}$ and ${F}_{G}^{i}$ separately represent the $i$-th level feature maps extracted from the prediction and ground truth. The ${l}_{f}^{i}$ is calculated as their Euclidean distance (L2-Loss), which is supervised at the pixel level:
\begin{equation}\label{equ:5}
 \begin{split}
    {l}_{f}^{i} = \vert\vert {F}_{P}^{i} -  {F}_{G}^{i} \vert\vert_{2}, \quad i=1, 2, 3, 4.
 \end{split}
\end{equation}
As can be seen from Fig~\ref{fig:P_loss}, the low-level feature maps contain rich boundary information and the high-level ones depict location information. Thus, the LossNet can generate comprehensive supervision at the feature levels. 
\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{fig/P_loss.pdf}
\caption{Illustration of LossNet.}
\label{fig:P_loss}
\end{figure}


\section{Experiments}
\subsection{Datasets}
Extensive experiments are conducted to verify the effectiveness of the proposed framework on four different types of medical segmentation tasks with data from varied image modalities, including color colonoscopy imaging, ultrasound imaging, computed tomography (CT), and optical coherence tomography (OCT).

\textbf{Polyp Segmentation.}
According to GLOBOCAN 2020 data, colorectal cancer is the third most common cancer worldwide and the second most common cause of death. It usually begins as small, noncancerous (benign) clumps of cells called polyps that form on the inside of the colon. We evaluate the proposed model on five benchmark datasets: CVC-ColonDB~\cite{CVC-ColonDB}, ETIS~\cite{ETIS}, Kvasir~\cite{Kvasir}, CVC-T~\cite{CVC-T} and CVC-ClinicDB~\cite{CVC-ClinicDB}. We adopt the same training set as the latest image polyp segmentation method~\cite{PraNet}, that is, $900$ samples from the Kvasir and $550$ samples from the CVC-ClinicDB~\cite{CVC-ClinicDB} are used for training. 
The remaining images and the other three datasets are used for testing. Besides, there are some video-based polyp datasets, including the CVC-300\cite{cvc-300} and CVC-612\cite{CVC-ClinicDB}. We follow the latest video polyp segmentation method to split the videos from CVC-300 (12 clips) and CVC-612 (29 clips)
into 60\% for training, 20\% for validation, and 20\% for testing.

\textbf{Lung Infection.}
A novel viral pneumonia that emerged in early 2020 rapidly spread worldwide, creating an unprecedented public-health challenge. At present, only a few public lung CT datasets are available for infection-area segmentation. To obtain a relatively sufficient sample size for training, we slice one publicly available dataset~\cite{COVID-19_dataset1} and merge it with another public dataset~\cite{COVID-19_dataset2}, resulting in 1,277 high-quality CT images through uniform sampling. These are then split into 894 images for training and 383 images for testing.


\textbf{Breast Ultrasound Segmentation.}
Breast cancer is one of the most dreaded cancers in 
women~\cite{Breast_cancer}. Segmenting the lesion region from 
breast ultrasound images is essential for tumor diagnosis. BUSI~\cite{BUSI} dataset contains $780$ images of $600$ female patients. Among them, there are $133$ normal cases, $437$ benign tumors, and $210$ malignant tumors. We follow the popular breast ultrasound segmentation methods~\cite{SKUNet,NU-net} to perform four-fold cross-validation on BUSI. 

\textbf{OCT Layer Segmentation.} The OCT images are often used to diagnose and monitor retinal diseases more accurately based on abnormality quantification and retinal layer thickness computation both in research centers and clinic routines. At present, many scholars have been studying the segmentation of fundus structure in macular OCT scans, but few focus on parapapillary circular scans. To fully show the generalization of M$^{2}$SNet in different medical tasks, we take our M$^{2}$SNet to participate in the \textcolor{orange}{MICCAI 2022 Challenge: Glaucoma Oct Analysis and Layer Segmentation (GOALS)}\footnote{\url{https://conferences.miccai.org/2022/en/MICCAI2022-CHALLENGES.html}}. 
It requests participants to segment three layers, which has positive significance for the diagnosis of glaucoma, including retinal nerve fiber layer (RNFL), ganglion cell-inner plexiform layer (GCIPL), and choroid layer, as shown in Fig.~\ref{fig:GOALS_show}. The GOALS2022~\cite{GOALS} dataset~\cite{GOALS_OMIA} contains $300$ circumpapillary OCT. There are three equal groups with $100$ OCT images for the training process, the preliminary competition process and the final process, respectively. The GOALS2022 challenge attracts $100$ teams from all over the world to participate, and we finally won \textcolor{orange}{the second place (2/100)}\footnote{\url{https://github.com/Xiaoqi-Zhao-DLUT/MSNet}}.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{fig/GOALS_show.pdf}
\caption{Visualization of the fundus OCT layer segmentation.}
\label{fig:GOALS_show}
\end{figure}



\subsection{Evaluation Metrics}
There are many popular metrics used in different medical segmentation branches. mean Dice (mDice), mean IoU (mIoU), the weighted F-measure ($F_{\beta}^{w}$)~\cite{Fwb}, S-measure ($S_{\alpha}$)~\cite{S-m}, E-measure ($E_\phi^{max}$)~\cite{Em} and mean absolute error (MAE) are widely used in polyp segmentation. Following~\cite{Inf-Net}, five metrics are employed for quantitative evaluation, including Precision, Recall, Dice
Similarity Coefficient (DSC)~\cite{DSC}, S-measure and MAE. Jaccard, Precision, Recall, Dice and Specificity~\cite{AAU-net,NU-net} are more commonly used for breast tumor segmentation. For OCT layer segmentation, GOALS2022~\cite{GOALS_OMIA} adopts the Dice coefficient and mean Euclidean distance (MED) to evaluate segmentation bodies and edges, respectively.  The lower value is better for the MAE and MED, and higher is better for others.

\begin{table}[t]
  \centering
  \scriptsize
  \renewcommand{\arraystretch}{1.1}
  \setlength\tabcolsep{5pt}
  \caption{ Quantitative comparisons on image polyp segmentation datasets. Top $2$ scores are highlighted in \textBC{mycellreda}{red} and \textBC{mycellblue}{blue}, respectively.  ``$\dagger$'' represents the medicine-specific method. 
  }\label{tab:image_polyp_comparison}
  \resizebox{\columnwidth}{!}
  {
 \input{table/image_polyp_comparison.tex}
  }
\end{table}

\begin{table}[!t]
\centering
\scriptsize
  \renewcommand{\arraystretch}{1.1}
\setlength\tabcolsep{5pt}
\caption{ Quantitative comparisons on video polyp segmentation datasets. Top $2$ scores are highlighted in \textBC{mycellreda}{red} and \textBC{mycellblue}{blue}, respectively.  ``$\dagger$'' and ``$\bigstar$'' represent the medicine-specific method and the video polyp method, respectively. 
  }
  \label{tab:video_polyp_comparison}
\resizebox{\columnwidth}{!}
{\input{table/video_polyp_comparison.tex}}
\end{table}

\begin{table}[t]
\centering
\scriptsize
  % \renewcommand{\arraystretch}{1.1}
\setlength\tabcolsep{5pt}
\caption{Quantitative comparisons on the lung infection CT dataset. Top $2$ scores are highlighted in \textBC{mycellreda}{red} and \textBC{mycellblue}{blue}, respectively.  ``$\dagger$'' represents the medicine-specific method. 
  }
  \label{tab:COVID_comparison}
\resizebox{\columnwidth}{!}
{\input{table/COVID_comparison.tex}}
\end{table}

\begin{table}[!t]
\centering
\scriptsize
  % \renewcommand{\arraystretch}{1.1}
\setlength\tabcolsep{3pt}
\caption{ Quantitative comparisons on the breast ultrasound dataset. Top $2$ scores are highlighted in \textBC{mycellreda}{red} and \textBC{mycellblue}{blue}, respectively.  ``$\dagger$'' represents the medicine-specific method. 
  }
  \label{tab:BUSI_comparison}
\resizebox{\columnwidth}{!}
{\input{table/BUSI_comparison.tex}}
\end{table}

\begin{table}[!t]
\centering
\scriptsize
  % \renewcommand{\arraystretch}{1.1}
\setlength\tabcolsep{2pt}
\caption{The leaderboard of MICCAI2022 Challenge: Glaucoma Oct Analysis and Layer Segmentation (GOALS). Top $2$ scores are highlighted in \textBC{mycellreda}{red} and \textBC{mycellblue}{blue}, respectively. 
  }
  \label{tab:OCT_layer_comparison}
\resizebox{\columnwidth}{!}
{\input{table/OCT_layer_comparison.tex}}
\end{table}

\begin{table}[!t]
  \centering
  \scriptsize
  \renewcommand{\arraystretch}{1.1}
  \setlength\tabcolsep{5pt}
  \caption{ The FLOPs, parameters and speed of different methods. The best and worst results are shown in \textBC{mycellreda}{red} and \textBC{mycellblue}{blue}, respectively. 
  }\label{tab:FLOPs_para}
  \resizebox{\columnwidth}{!}
  {\input{table/FLOPs_para.tex}
}
\end{table}

\subsection{Implementation Details}
Our model is implemented based on the PyTorch framework and trained on a single 2080Ti GPU with mini-batch size $16$.  We resize the inputs to $352 \times 352$ and employ a general multi-scale training strategy as most methods~\cite{F3Net,GCPANet,Rank-Net_COD,SPNet_RGBDSOD,PraNet,MSNet}. Random horizontally flipping and random rotate data augmentation are used to avoid overfitting. For the optimizer, we adopt the stochastic gradient descent (SGD). The momentum and weight decay are set as $0.9$ and $0.0005$, respectively.  Maximum learning rate is set to $0.005$ for backbone and $0.05$ for other parts. Warm-up and linear decay strategies  are  used to  adjust  the  learning  rate. 
For any medical image sub-tasks, the above training strategy is used for all the multi-scale subtraction models involved in this paper. 
The difference among these models is only in the number of training epochs due to different convergence speeds. Specifically, the number of training epochs settings in the  polyp segmentation, lung infection, breast tumor segmentation and OCT layer segmentation are $50$, $200$, $100$ and $100$, respectively.



 \begin{table*}[!t]
  \centering
  \scriptsize
  \renewcommand{\arraystretch}{1.1}
  \setlength\tabcolsep{2pt}
    \caption{{Quantitative results at different training and inference codes frameworks. ``$^{\blacktriangle}$'' and ``$^{\blacktriangledown}$'' represent the model trained on the MSNet~\cite{MSNet} and nnU-Net~\cite{nnU-Net} code framework, respectively.}}\label{tab:ablation_study_2}
     \resizebox{\linewidth}{!}
     {
\input{table/ablation_study_2.tex}
	}
\end{table*}

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{fig/visual_comparison.pdf}
\caption{Visual comparison of different medicine-general and medicine-specific methods.}
\label{fig:visual_comparison}
\end{figure*}



\subsection{Comparisons with Medicine-general and Medicine-specific Methods}
For a fair comparison, we compare not only with medicine-specific methods but also with representative medicine-general methods, including UNet~\cite{UNet}, UNet++~\cite{UNet++}, Attention U-Net~\cite{Attention_UNet}, UTNet~\cite{UTNet} and TransUNet~\cite{TransUNet}. Based on the open-source codes, we retrain these medicine-general methods on the same training sets as our models. 
\\
\noindent$\bullet$ In Tab.~\ref{tab:image_polyp_comparison}, among $30$ scores of all \textit{\textbf{image polyp}} datasets, our multi-scale subtraction models (MSNet + M$^{2}$SNet) achieve the best performance in terms of all six metrics. The M$^{2}$SNet even outperforms the video-based polyp segmentation method PNS-Net~\cite{PNSNet} on the \textit{\textbf{video polyp}} datasets, as shown in Tab.~\ref{tab:video_polyp_comparison}.
\\
\noindent$\bullet$ 
Tab.~\ref{tab:COVID_comparison} shows performance comparisons on the \textit{\textbf{lung infection CT}} datasets. Compared to the second best method (Inf-Net~\cite{Inf-Net}), M$^{2}$SNet achieves an important improvement of $1.5\%$, $6.6\%$ and $14.3\%$ in terms of DSC, Precision and MAE, respectively.
\\
\noindent$\bullet$ 
Tab.~\ref{tab:BUSI_comparison} shows performance comparisons with \textit{\textbf{breast tumor segmentation}} methods. Following most methods~\cite{SKUNet,NU-net} in this field, we adopt the four-fold cross-validation strategy. 
M$^{2}$SNet achieves the best performance in terms of the Jaccard, Precision and Dice metrics, which outperforms representative transformer-based methods, UTNet and TransUNet. Moreover, M$^{2}$SNet has the smallest mean standard deviation ($0.97$) under the five metrics, which indicates its performance stability.
\\
\noindent$\bullet$ 
% 通常来说不同的训练和推理框架会产生不同的final performance. 为了公平的比较，我们对m2snet训练了两个版本，一个是基于msnet的框架一个是基于流行的nnunet的框架。受益于nnunet框架，m2snet的性能可以进一步提升。 
{Generally speaking, different training and inference frameworks will produce different final performance. For a fair comparison, we train two versions of M$^{2}$SNet, one version follows the MSNet~\cite{MSNet} framework and the other based on the popular nnU-Net~\cite{nnU-Net} framework. Benefiting from the nnU-Net framework with many effective tricks, the performance of  M$^{2}$SNet can be further improved, as shown in Tab.~\ref{tab:ablation_study_2}.}
\\
\noindent$\bullet$ 
In Tab.~\ref{tab:OCT_layer_comparison}, we list the top $10$ \textit{\textbf{OCT layer segmentation}} results in MICCAI2022 GOALS Challenge. 
% More details please see the  \textBC{orange}{\href{https://aistudio.baidu.com/aistudio/competition/detail/230/0/leaderboard}{leaderboard}}. 
Based on the M$^{2}$SNet, we won the second place (2/100) according to the weighted score of six results in three different layers. It is worth noting that our method ranks the top $1$ in four out of six metrics.
\\
\noindent$\bullet$ 
As can be seen from Tab.~\ref{tab:image_polyp_comparison} - Tab.~\ref{tab:BUSI_comparison}, our M$^{2}$SNet consistently surpasses other medicine-general methods in all medical segmentation sub-branches. In Tab.~\ref{tab:FLOPs_para}, we list the FLOPs and parameters of different medicine-general methods. It can be seen that our method has only 9GB in FLOPs, which has obvious advantages in terms of computational efficiency.
\\
\noindent$\bullet$ 
{In Tab.~\ref{tab:FLOPs_para}, we compare the model efficiency in terms of FLOPs, parameters and inference speed. It can be seen that our method ranks first in terms of FLOPs, which has obvious advantages among both medicine-general and medicine-specific methods. As can be seen from Tab.~\ref{tab:image_polyp_comparison} - Tab.~\ref{tab:BUSI_comparison}, our M$^{2}$SNet consistently surpasses other methods in all medical segmentation sub-branches. Therefore, the proposed M$^{2}$SNet achieves good balance on accuracy and efficiency.}
\\
\noindent$\bullet$ 
Fig.~\ref{fig:visual_comparison} depicts a qualitative comparison with other methods. It can be seen that the results of M$^{2}$SNet have greater advantages in terms of detection accuracy, completeness, and sharpness across different image modalities.



% \begin{figure}[t]
% \centering
% \includegraphics[width=\linewidth]{fig/abalation.png}
% \caption{Visual output from Multi-scale Subtraction Unit in different levels}
% \label{fig:ablation_study_visual_encoder_transition}
% \end{figure}


\begin{table}[t]
\centering
\scriptsize
  % \renewcommand{\arraystretch}{1.1}
\setlength\tabcolsep{2pt}
\caption{Quantitative comparison of unified models on three different medical lesion segmentation tasks. The best results are shown in \textBC{mycellreda}{red}.  
  }
  \label{tab:comparison_unified_model}
\resizebox{\columnwidth}{!}
{\input{table/comparison_unified_model.tex}}
\end{table}


 \begin{table}[!t]
  \centering
  \scriptsize
  \renewcommand{\arraystretch}{1.1}
  \setlength\tabcolsep{1pt}
    \caption{Ablation experiments of the subtraction unit, inter-layer multi-scale aggregation and LossNet.}\label{tab:ablation_study_1}
     \resizebox{\columnwidth}{!}
     {
	\input{table/ablation_study_1.tex}
	}
\end{table}








\begin{table*}[!t]
  \centering
  \scriptsize
  \renewcommand{\arraystretch}{1.1}
  \setlength\tabcolsep{2pt}
    \caption{{Ablation experiments of the loss functions.} }\label{tab:ablation_study_3}
     \resizebox{\linewidth}{!}
     {
\input{table/ablation_study_3.tex}
	}
\end{table*}

 \begin{table*}[!t]
  \centering
  \scriptsize
  \renewcommand{\arraystretch}{1.1}
  \setlength\tabcolsep{2pt}
    \caption{{Ablation experiments of the kernel scales and weights in intra-layer multi-scale subtraction.} }\label{tab:ablation_study_4}
     \resizebox{\linewidth}{!}
     {
\input{table/ablation_study_4.tex}
	}
\end{table*}




\begin{figure*}[!t]
\centering
\includegraphics[width=\linewidth]{fig/aspp_denseaspp_ours_structure.pdf}
\caption{Illustration of different multi-scale modules applied in the subtraction unit.}
\label{fig:aspp_denseaspp_ours_structure}
\end{figure*}

 \begin{table*}[!t]
  \centering
  \scriptsize
  \renewcommand{\arraystretch}{1.1}
  \setlength\tabcolsep{2pt}
    \caption{Quantitative comparisons of different multi-scale styles in intra-layer multi-scale subtraction design. Positive and negative gains are highlighted in \textBC{mycellreda}{red} and \textBC{mycellblue}{blue}, respectively. }\label{tab:ablation_study_5}
     \resizebox{\linewidth}{!}
     {
\input{table/ablation_study_5.tex}
	}
\end{table*}

\subsection{Comparisons with Unified and Generalist Methods}

In recent years, with the rapid development of large‐scale and foundation models, there has been a growing interest in building \emph{unified} and \emph{generalist} models that can solve multiple tasks with a single set of model parameters.  
%
These models~\cite{UniverSeg,SegGPT,SAM2} aim to break the traditional paradigm of training task‐specific networks by introducing shared representations, prompt mechanisms, or in‐context learning strategies so that one model can flexibly adapt to diverse downstream tasks.  
%
Following the evaluation protocol of SAM-Eva~\cite{SAM-Eva}, we evaluate our proposed M$^{2}$SNet against representative unified and generalist methods on three challenging cross‐modality, cross‐lesion segmentation tasks, including lung infection, breast lesion, and polyp segmentation.  

As shown in Tab.~\ref{tab:comparison_unified_model}, UniverSeg~\cite{UniverSeg}, SegGPT~\cite{SegGPT}, and SAM~2~\cite{SAM2} exhibit clear performance drops when directly applied to cross‐modality and cross‐lesion scenarios. These models mainly rely on prompt embeddings or in‐context examples to handle unseen tasks but still struggle with the strong context‐dependence and heterogeneous appearances in medical images.  
%
Spider~\cite{Spider}, a unified context‐dependent segmentation model, achieves better performance than the above generalist models, demonstrating the effectiveness of high‐level concept matching mechanisms.  
%
Our M$^{2}$SNet consistently surpasses all the competitors across all three tasks, obtaining the highest Dice and mIoU scores. In our experiments, we jointly train M$^{2}$SNet on the training sets of the three tasks. Thanks to the implicit prompts naturally embedded in modality and lesion types, M$^{2}$SNet can perform joint learning of all data under a single parameter set, thus simultaneously achieving unified modelling and superior performance. By contrast, other methods are trained with a much broader range of tasks and domains but still underperform on these specific cross‐modality, cross‐lesion settings.  
%
Although the scope of M$^{2}$SNet is narrower than that of some generalist models, the results provide an important insight: future universal and unified models can be organized into multiple levels or hierarchies of unification to better balance task coverage and performance for real‐world applications.


% 因为在进行cross-modality cross-lesion统一时，modality和lesion自带了隐式prompt可以天然的在一套参数下，对全部的数据进行joint learning，因此在完成统一任务，又能获得更高的performance. 虽然不如其他通才或统一模型的范围广，但也给予了我们更多的启示，今后对于通才和统一可以划分为多个层面和等级，以便未来更多的实际应用。



\subsection{Ablation Study}
We take the common FPN network as the baseline to analyze the contribution of each component. 
\subsubsection{Effectiveness of the subtraction unit, inter-layer multi-scale subtraction aggregation}
The results are shown in Tab.~\ref{tab:ablation_study_1}. These defined feature subscripts are the same as those in Fig~\ref{fig:Pipeline}. 
First, we apply the basic subtraction unit (SU) to the baseline to get a  series of $SU_{2}^{i}$  features to participate in the feature aggregation calculated by Equ.~\ref{equ:1}. The gap between the `` + $SU_{2}^{i}$ ''  and the baseline demonstrates the effectiveness of the SU.  It can be seen that the usage of SU has a significant improvement on the ColonDB dataset compared to the baseline, with the gain of 7.8\%, 7.4\%, 6.7\% and 4.4\% in terms of mDice, mIoU, $F_\beta^w$, and $E_\phi^{max}$, respectively. Next, we gradually add  $SU_{3}^{i}$, $SU_{4}^{i}$ and $MS_{5}^{i}$ to achieve inter-layer multi-scale aggregation. The gap between the `` + $SU_{5}^{i}$ ''  and the `` + $SU_{2}^{i}$ '' quantitatively demonstrates the effectiveness of inter-layer multi-scale subtraction strategy. Next, we evaluate the benefit of $\mathcal{L}_{f}$.  Compared to the  `` + $SU_{5}^{i}$ '' model,  the  `` + $\mathcal{L}_{f}$ '' achieves significant performance improvement on the ETIS dataset, with the gain of 11.8\%, 14.1\%, 13.0\% and 5.5\% in terms of mDice, mIoU, $F_\beta^w$, and $E_\phi^{max}$, respectively. Besides,  we replace all subtraction units with the element-wise addition units (AU) and compare their performance. It can be seen that our subtraction units have significant advantage and no additional parameters are introduced.

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{fig/ms_module_all_feature_visual_1.pdf}
\caption{Visualization of the feature maps in the multi-scale in multi-scale subtraction module.}
\label{fig:ms_module_all_feature_visual_1}
\end{figure*}


\subsubsection{Effectiveness of loss function}
%在表5中，我们对损失函数的有效性进行了详细的验证。M2SNet和M2SNet (w/o Lf)的gap展现了lossnet能够对众多的医学分割任务具有通用的有效性。此外，没有lossnet的辅助，M2SNet本身的性能也具有优越性，超越了表1-表4中大多数方法。M2SNet和M2SNet (w/o wbce+wiou) 的gap揭示了the weighted IoU loss and binary cross-entrop的必要性，which为lossnet提供一个基础的分割区域的引导使得Lossnet更加能够专注于病灶区域的监督而不是分散于背景区域，如图4所示
{In Tab.~\ref{tab:ablation_study_3}, we thoroughly verify the effectiveness of the loss function used in M$^{2}$SNet. The gap between M$^{2}$SNet and M$^{2}$SNet (w/o $\mathcal{L}_{f}$ ) demonstrates the general effectiveness of LossNet for different medical segmentation tasks. At the same time, without the auxiliary of LossNet, the performance of M$^{2}$SNet itself is good enough to surpass most of the methods in Tab.~\ref{tab:image_polyp_comparison} - Tab.~\ref{tab:BUSI_comparison}. 
The gap between M$^{2}$SNet and M$^{2}$SNet (w/o  $\mathcal{L}_{IoU}^w$ + $\mathcal{L}_{BCE}^w$) shows the necessity of the weighted IoU and BCE loss, which provide a basic foreground region guidance for LossNet to focus on supervising multi-level lesion regions without distracting in the background area. }
\subsubsection{Effectiveness of the intra-layer multi-scale subtraction design} 
Compared to the previous MSNet, the M$^{2}$SNet replace all the original single-scale subtraction unit with the stronger multi-scale subtraction unit. 
{As shown in Tab.~\ref{tab:ablation_study_4}, more scales of feature fusion can help improve the overall performance and multi-scale filters of size $[1, 3, 5]$ and $[1, 3, 5, 7]$ have close performance. Next, we replace the fixed full one weights with Gaussian weights. It can be seen that Gaussian weights significantly degrade the performance of the model in all tasks. We think that the subtraction unit should try to maintain the characteristic distribution of the input features itself, but the Gaussian weights rigidly changes the spatial distribution of the original features, causing an extra burden for the subsequent decoder. Therefore, we choose the multi-scale convolution filters with fixed full one weights of size $[1, 3, 5]$ as the final setting.} 
To further show the advantages of our intra-layer multi-scale design, we apply other popular multi-scale modules (i.e., ASPP~\cite{ASPP} and DenseASPP~\cite{DenseASPP}) to the subtraction unit and these architectures are shown in Fig.~\ref{fig:aspp_denseaspp_ours_structure}. In Tab.~\ref{tab:ablation_study_5}, we thoroughly compare both the efficiency and accuracy of these three structures. It can be seen that ``M$^{2}$SNet (Ours)'' has a significant performance gain in terms of fourteen metrics on four challenges datasets under different tasks. However, the other two models not only increase the computational burden by more than 50\%, but also produce negative gains in multiple datasets. Therefore, the proposed intra-layer multi-scale subtraction design can be taken as a new baseline for future research in subtraction family. 



To more intuitively show the differential information from different scales, we visualize all the features of the multi-scale in multi-scale subtraction module, as shown in Fig~\ref{fig:ms_module_all_feature_visual_1}. We can see that the multi-scale in multi-scale subtraction module can clearly highlight the difference between high-level features and other level features and propagate its localization effect to the low-level ones. At the same level, the intra-layer multi-scale aggregation design can comprehensively capture both the subtle and regional difference features. Thus, both the global structural information and local boundary information is well depicted in the enhanced features of different levels.
 


\section{Discussion}
\textbf{Multi-scale Subtraction Unit}:
Different from previous addition and concatenation operations, 
using subtraction in multi-level structure make resulted features input to the
decoder have much less redundancy among different levels and their level-specific
properties are significantly enhanced. In this work, we further explore the potential of the subtraction unit in intra-layer multi-scale fusion. How to improve the accuracy while maintaining the same efficiency as the single-scale one is the key challenge. We provide the solution of using multi-scale convolution filters with fixed parameters. Compared to the single-scale design, multi-scale subtraction unit can enable the network to collect more complementary information both in pixel-pixel and neighbor-neighbor levels. The advantages of multi-scale subtraction unit in terms of efficiency and accuracy can be seen in Tab.~\ref{tab:ablation_study_5}. Multi-scale information extraction and feature aggregation are two general problems in the field of computer vision. Our multi-scale subtraction unit can solve both of them at once. We think this new paradigm can drive more researches on the subtraction operation in the future.
\\
\textbf{LossNet}: LossNet is similar in form to perception loss~\cite{Ploss} that has been applied in many tasks, such as style transfer and inpainting.  While in those vision tasks, the perception-like loss is mainly used to speed the convergence of GAN and obtain high frequency information and ease checkerboard artifacts, but it does not bring obvious accuracy improvement. In our paper, the inputs are binary segmentation masks, LossNet can directly target the geometric features of the lesion and perform joint supervisions from the contour to the body, thereby improving the overall segmentation accuracy.

\section{Conclusion}
In this paper, we rethink previous addition-based or concatenation-based methods and present a simple yet general multi-scale in multi-scale subtraction network (M$^{2}$SNet) for more efficient medical image segmentation.   
%
Based on the proposed intra-layer multi-scale subtraction unit, we pyramidally aggregate adjacent levels to extract lower-order and higher-order cross-level complementary information and combine with level-specific information to enhance multi-scale feature representation. 
%
Besides, we design a loss function based on a training-free network to supervise the prediction from different feature levels, which can optimize the segmentation on both structure and details during the backward phase.  
%
Experimental results on $11$ benchmark datasets towards $4$ medical segmentation tasks demonstrate that the proposed model outperforms various state-of-the-art methods. 




\section*{Acknowledgments}
 This work was supported by Dalian Science and Technology Innovation Foundation under Grant 2023JJ12GX015, and by the National Natural
 Science Foundation of China under Grant 62276046 and 62431004.  (Corresponding author: Lihe Zhang.)

 
\section*{Conflicts of Interests}
The authors declared that they have no conflicts of interest to this work.

\bibliographystyle{unsrt}
\bibliography{mir-article}

%%===================================================%%
%% For presentation purpose, we have included        %%
%% \bigskip command. please ignore this.             %%
%%===================================================%%




%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

%\bibliography{sn-bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl

%% Default %%
%%\input sn-sample-bib.tex%
% \begin{thebibliography}{9}


% \bibitem{bib1} B. Ran, D. E. Boyce. {\sl Modeling Dynamic Transportation Network}, Berlin, Germany: Springer-Verlag, pp. 69-83,1996. (Book style)

% \bibitem{bib2} G. O. Young. Synthetic structure of industrial plastics. Plastics, 2nd ed., J. Peters, Ed., New York, USA: McGraw-Hill, pp. 15-64, 1964. (Book style with paper title and editor)

% \bibitem{bib3} L. C. Su, F. Zhu. Design of a novel omnidrectional stereo vision system. {\sl Acta Automatica Sinica}, vol. 32, no. 1, pp. 67-72, 2006. (in Chinese) (Periodical style)

% \bibitem{bib4} R. Roychoudhury, S. Bandyopadhyay, K. Paul. Adistributed mechanism for topology discovery in ad hoc wireless networks using mobile agents. In {\sl Proceedings of IEEE First ual Workshop on Mobile and Ad hoc Networking and Computing}, IEEE, Piscataway, USA, pp. 145-146,2000. (Published Conference Proceedings style)

% \bibitem{bib5} O. Hryniewicz. An evaluation of the reliability of complex systems using shadowed sets and fuzzy lifetime data. {\sl International Journal of Automation and Computing}, to be published.

% \bibitem{bib6} W. Zhang. Reinforcement Learning for Job-shop Scheduling, Ph. D. dissertation, Oregon State University, USA,1996. (Dissertation style)

% \bibitem{bib7} IEEE Criteria for Class IE Electric Systems, IEEE Standard 308, 1969. (Standards style)

% \bibitem{bib8} R. C. Reily, J. L. Mack. The Self-organization of Spatially Invariant Representations, Technical Report PDP. CNS.92.5, Department of Psychology, Mellon University, USA, 1993. (Report style)

% \bibitem{bib9} J. P. Wilkinson. Nonlinear Resonant Circuit Devices, U.S. Patent 362412, July 1990. (Patent style)

% \bibitem{bib10} J. Jones. {\sl Networks}, 2nd ed., [Online], Available: http://www.atm.com, May 10, 1991.

% \bibitem{bib11} R. J. Vidmar. On the use of atmospheric plasmas as electromagnetic reflectors. {\sl IEEE Transactions on Plasma Science}, vol. 21, no. 3, pp. 945-999, [Online], Available: http://www.halcyon.com, July 20-25, 1999.




% \end{thebibliography}

% \begin{figure}[h]%
% \centering
% \includegraphics[width=0.3\textwidth]{First.eps}
% %\caption{First author}%\label{fig1}
% \end{figure}

% %\begin{biography}
% \noindent{\bf First Author} and the other authors may include
% biographies at the end of the paper. The author's educational
% background is listed. The degrees should be listed with type of
% degree in what field, which institution, city, state or country, and
% the year degree was earned. Listing military and work experience,
% including summer and fellowship jobs at the end of this paragraph.
% The current job must have a location.

% The second paragraph uses the pronoun of the person (he or she) and
% not the author$'$s last name. Information concerning previous
% publications may be included. Try not to list more than three books
% or published articles. The format for listing publishers of a book
% within the biography is: title of book (city, state: publisher name,
% year) similar to a reference. Current and previous research
% interests should be stated.

% The third paragraph begins with the author$'$s title and last name
% (e.g., Dr. Smith, Prof. Jones, Mr. Kajor, Ms. Hunter). List any
% memberships in professional societies. Finally, list any awards and
% work for committees and publications. If a photograph (above 600
% dpi) is provided, the biography will be indented around it. The
% photograph is placed at the top left of the biography. Personal
% hobbies will be deleted from the biography. Please see the following
% example for more details.

% E-mail: mir@ia.ac.cn (Corresponding author)

% ORCID iD: 0000-0000-0000-0000




% %\end{biography}


% \begin{figure}[h]%
% \centering
% \includegraphics[width=0.3\textwidth]{Second.eps}
% %\caption{Second author}%\label{fig1}
% \end{figure}

% %\begin{biography}
% \noindent{\bf Second Author }\quad received the B.Sc. and M.Sc.
% degrees in mechanical engineering from *** University, China in 1977
% and 1984, respectively, and the Ph.D. degree in computing from ***
% University, UK in 1992. In 1994, he was a faculty member at ***
% University, China and in 1996 at *** University, USA. Currently, he
% is a professor in Department of Information System Engineering at
% *** University, China. Prof. Author received research award from
% Science Foundation, and the Best Paper Award of the IS International
% Conference in 2000 and 2006, respectively. He is a member of SICE,
% IEE and IEEE.

% His research interests include robotics, feedback control systems,
% and control theory.

% E-mail: mir@ia.ac.cn

% ORCID iD: 0000-0000-0000-0000

% %\end{biography}

% \begin{figure}[h]%
% \centering
% \includegraphics[width=0.3\textwidth]{Third.eps}
% %\caption{Third author}%\label{fig1}
% \end{figure}

% %\begin{biography}
% \noindent{\bf Third Author }\quad received the B.Sc. and M.Sc.
% degrees in mechanical engineering from *** University, China in 1977
% and 1984, respectively, and the Ph.D. degree in computing from ***
% University, UK in 1992. In 1994, he was a faculty member at ***
% University, China and in 1996 at *** University, USA. Currently, he
% is a professor in Department of Information System Engineering at
% *** University, China. Prof. Author received research award from
% Science Foundation, and the Best Paper Award of the IS International
% Conference in 2000 and 2006, respectively. He is a member of SICE,
% IEE and IEEE.

% His research interests include robotics, feedback control systems,
% and control theory.

% E-mail: mir@ia.ac.cn

% ORCID iD: 0000-0000-0000-0000

\end{document}
