\documentclass[journal,twoside,web]{ieeecolor}
\usepackage{tmi}
\usepackage{cite}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{url}
\usepackage{array}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{soul}
\usepackage{threeparttable}
\usepackage{times}
\usepackage{epsfig}
\usepackage{xcolor}
\usepackage{color, colortbl}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\newcommand{\highlight}[1]{\textcolor{orange}{#1}}
\newcommand{\newcomment}[1]{\textcolor{blue}{#1}}

\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\def\eg{\emph{e.g}.} \def\Eg{\emph{E.g}.}
\def\ie{\emph{i.e}.} \def\Ie{\emph{I.e}.}
\def\cf{\emph{c.f}.} \def\Cf{\emph{C.f}.}
\def\etc{\emph{etc}.} \def\vs{\emph{vs}.}
\def\wrt{w.r.t.} \def\dof{d.o.f.}
\def\etal{\emph{et~al}.}

\definecolor{lightgray}{gray}{0.9}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\definecolor{mygray}{gray}{.94}
\newcommand{\textBC}[2]{\textbf{\textcolor{#1}{#2}}}


\definecolor{ggray}{RGB}{127,127,127}
\definecolor{reda}{RGB}{202,0,0}
\definecolor{mycellreda}{RGB}{202,0,0}
\definecolor{redb}{RGB}{217,148,143}
\definecolor{myyellow}{RGB}{190,144,0}
\definecolor{mygreen}{RGB}{0,136,51}
\definecolor{myblue}{RGB}{0,102,204}
\definecolor{mycellblue}{RGB}{0,102,204}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\markboth{\journalname, VOL. XX, NO. XX, XXXX 2023}
{Author \MakeLowercase{\textit{et al.}}: Preparation of Papers for IEEE TRANSACTIONS ON MEDICAL IMAGING}
\begin{document}
\title{M$^{2}$SNet: Multi-scale in  Multi-scale Subtraction Network for Medical  Image  Segmentation}
\author{Xiaoqi Zhao, Hongpeng Jia, Youwei Pang, Long Lv, Feng Tian, Lihe Zhang, Weibing  Sun, Huchuan Lu
\thanks{This work was supported by the National Natural Science Foundation of China \#62276046 and \#61876202, and the Liaoning Natural Science Foundation \#2021-KF-12-10. (Corresponding author: Lihe Zhang.)
 }
\thanks{X. Zhao, H. Jia, Y. Pang, L. Zhang and H. Lu are with the School of Information and Communication Engineering, Dalian University of Technology, Dalian, China. (e-mail: zxq@mail.dlut.edu.cn; a2916956058@163.com; lartpang@mail.dlut.edu.cn; zhanglihe@dlut.edu.cn; lhchuan@dlut.edu.cn).}
\thanks{L. Lv, F. Tian, W. Sun are with the Department of Urology, Affiliated Zhongshan Hospital of Dalian University, Dalian, Liaoning, China. (e-mail: lvlong113@126.com; tianfeng73@163.com; weibingsun\_dyfemw@163.com).}
}

\maketitle

\begin{abstract}
Accurate medical image segmentation is critical for early medical diagnosis. Most existing methods are based on U-shape structure and
use element-wise addition or concatenation to fuse different level features progressively in decoder.
%
However, both the two operations easily generate plenty of redundant information, which will weaken the complementarity between different level features, resulting in inaccurate localization and blurred edges of lesions.
%
To address this challenge, we propose a general multi-scale in multi-scale subtraction network (M$^{2}$SNet) to finish diverse segmentation from medical image.
%
Specifically, we first design a basic subtraction unit (SU) to produce the difference features between adjacent levels in encoder. Next, we expand the single-scale SU to the intra-layer multi-scale SU, which can provide the decoder with both pixel-level and structure-level difference information. Then, we pyramidally equip the multi-scale SUs at different levels with varying receptive fields, thereby achieving the inter-layer multi-scale feature aggregation and obtaining rich multi-scale difference information. In addition, we build a training-free network ``LossNet'' to comprehensively supervise the task-aware features from bottom layer to top layer, which drives our multi-scale subtraction network to capture the detailed and structural cues simultaneously.
%
Without bells and whistles, our method performs favorably against most state-of-the-art methods under different evaluation metrics on eleven datasets of four different medical image segmentation tasks of diverse image modalities, including color colonoscopy imaging, ultrasound imaging, computed tomography (CT), and optical coherence tomography (OCT). The source code can be available at \url{https://github.com/Xiaoqi-Zhao-DLUT/MSNet}.


\end{abstract}

\begin{IEEEkeywords}
Medical Image Segmentation, Subtraction Unit, Multi-scale in Multi-scale, Difference Information, LossNet.
\end{IEEEkeywords}

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{data/msnet++_msnet_unet_unet++_comparison.pdf}
\caption{Illustration of different medical image segmentation architectures. }
\label{fig:msnet++_msnet_unet_unet++_comparison}
\end{figure*}

\section{Introduction}
\label{sec:introduction}
%
\IEEEPARstart{A}{s} % 问题背景介绍
the important role in computer-aided diagnosis system, accurate medical image segmentation technique can provide the doctors with great guidance for making clinical decisions. There are three general challenges in accurate segmentation: \textbf{Firstly}, U-shape structures~\cite{FPN,UNet} have received considerable attention due to their abilities of utilizing multi-level information to reconstruct high-resolution feature maps. In UNet~\cite{UNet}, the up-sampled feature maps are concatenated with feature maps skipped from the encoder and convolutions and non-linearities are added between up-sampling steps, as shown in Fig.~\ref{fig:msnet++_msnet_unet_unet++_comparison} (a). Subsequent UNet-based methods design diverse feature enhancement modules via attention mechanism~\cite{ResUnet++,PraNet}, gate mechanism~\cite{BMPM,GateNet}, transformer technique~\cite{UTNet,TransUNet}, as shown in Fig.~\ref{fig:msnet++_msnet_unet_unet++_comparison} (b). UNet++~\cite{UNet++} uses nested and dense skip connections to reduce the  semantic  gap  between  the feature  maps  of  encoder  and  decoder, as shown in Fig.~\ref{fig:msnet++_msnet_unet_unet++_comparison} (c). Generally speaking, different level features in encoder have different characteristics. High-level ones have more semantic information which helps localize the objects, while low-level ones have more detailed information which can capture the subtle boundaries of objects. The  decoder leverages the level-specific and cross-level characteristics to generate the final high-resolution prediction. Nevertheless, the aforementioned methods directly use an element-wise addition or concatenation to fuse any two level features from the encoder and transmit them to the decoder.
These simple operations do not pay more attention to differential information between different levels. This drawback not only generates redundant information to dilute the really useful features but also weakens the characteristics of level-specific features, which results in that the network can not balance
accurate localization and subtle boundary refinement.
\textbf{Secondly}, due to the limited receptive field, a single-scale convolutional kernel is difficult to capture context information of size-varying objects. Some methods~\cite{FPN,UNet,UNet++,U2Net,MINet} rely on the inter-layer multi-scale features and progressively integrate the semantic context and texture details from diverse scale representations. Others~\cite{GateNet,DANet_RGBDSOD,R3Net,CoNet_RGBDSOD,JLDCF_RGBDSOD} focus on extracting the intra-layer multi-scale information based on the atrous spatial pyramid pooling module~\cite{ASPP} (ASPP) or DenseASPP~\cite{DenseASPP} in their networks. However, the ASPP-like multi-scale convolution modules will produce many extra parameters and computations. Many methods~\cite{BMPM,UCNet_RGBDSOD,PFNet_COD,BDRAR_Shadow,AFFPN_Shadow} usually equip several ASPP modules into the encoder/decoder blocks of different levels, while some ones~\cite{R3Net,DMRA_RGBDSOD,CoNet_RGBDSOD,Rank-Net_COD} install it on the highest-level encoder block.
\textbf{Thirdly}, the form of the loss function directly provides the direction for the gradient optimization of the network. In segmentation field, there are many loss functions are proposed to supervise the prediction at the different levels, such as the L1 loss, cross-entropy loss and weighted cross-entropy loss~\cite{FCN} in the pixel level, the SSIM~\cite{SSIM} loss and uncertainty-aware loss~\cite{ZoomNet} in the region level, the IoU loss, Dice loss and consistency-enhanced loss~\cite{MINet} in the  global level. Although these basic loss functions and their variants have different optimization characteristics, the designs of complex manual math forms are really time-consuming for many researches. In order to obtain comprehensive performance, models usually integrate a variety of loss functions, which places great demands on the training skills of the researchers. Therefore, we think that it is necessary to introduce an intelligent loss function without complex manual designs to comprehensively supervise the segmentation prediction.

In this paper, we propose a novel multi-scale in multi-scale subtraction network (M$^{2}$SNet) for general medical image segmentation.
%
Firstly, we design a subtraction unit (SU) and apply it to each pair of adjacent level features. The SU highlights the useful difference information between the features and eliminates the interference from the redundant parts.
%
Secondly, we collect the extreme multi-scale information with the help of the proposed multi-scale in multi-scale subtraction module. For the inter-layer multi-scale information, we pyramidally concatenate multiple subtraction units to capture the large-span cross-level information. Then, we aggregate level-specific features and multi-path cross-level differential features and then generate the final prediction in decoder. For the intra-layer multi-scale information, we improve the single-scale subtraction unit to the multi-scale subtraction unit through a group of full one filters with different kernel sizes, which can achieve naturally multi-scale subtraction aggregation without introducing extra parameters. As shown in Fig.~\ref{fig:msnet++_msnet_unet_unet++_comparison}, MSNet equips the inter-layer multi-scale subtraction module and M$^{2}$SNet has both the inter-layer and intra-layer multi-scale subtraction structures.
%
Thirdly, we propose a LossNet to automatically supervise the extracted feature maps from bottom layer to top layer, which can optimize the segmentation from detail to structure with a simple L2-loss function.



Our main contributions are summarized as follows:
\begin{itemize}
\item  We present a new segmentation framework by replacing traditional addition or concatenation feature fusion with an efficient subtraction aggregation.
\item  We propose a simple yet general multi-scale in multi-scale subtraction network (M$^{2}$SNet) for diverse medical image segmentation. With multi-scale in multi-scale module, the multi-scale complementary information from lower order to higher order among different levels can be effectively obtained, thereby comprehensively enhancing the perception of organs or lesion areas.
\item  We design an efficient intra-layer multi-scale subtraction unit (MSU). Due to the low parameters and computation of MSU, it can be equipped for all cross-layer aggregations in our M$^{2}$SNet.
\item  We build a general training-free loss network to implement the detail-to-structure supervision in the feature levels, which provides the important supplement to the loss design based on the prediction itself.
\item  We verify the effectiveness of the M$^{2}$SNet on four challenge medical segmentation tasks: polyp segmentation, breast cancer segmentation, COVID-19 lung infection and OCT layer segmentation corresponding to the color colonoscopy imaging, ultrasound imaging, computed tomography (CT), and optical coherence tomography (OCT) image input modality, respectively. In addition, M$^{2}$SNet won the second place in the MICCAI2022 GOALS International Ophthalmology Challenge.
\end{itemize}

\textit{Compared with the MICCAI version~\cite{MSNet} of this work, the following extensions are made.
\textbf{\uppercase\expandafter{\romannumeral1})} Based on the structure of the original single-scale subtraction unit, we develop the stronger intra-layer multi-subtraction unit and construct the multi-scale in multi-scale subtraction network (M$^{2}$SNet). Meanwhile, M$^{2}$SNet carries forward the low FLOPs spirit of the previous MSNet.
\textbf{\uppercase\expandafter{\romannumeral2})} We re-organize the introduction and add more thorough related works in Sec.~\ref{sec:relatedwork}.
\textbf{\uppercase\expandafter{\romannumeral3})} We report much more extensive experimental results that demonstrate the superiority of M$^{2}$SNet in $4$ popular medical segmentation tasks.
\textbf{\uppercase\expandafter{\romannumeral4})} We verify the performance of the M$^{2}$SNet for multi-class segmentation in the  \textBC{orange}{\href{https://aistudio.baidu.com/aistudio/competition/detail/230/0/introduction}{MICCAI 2022 Challenge: Glaucoma Oct Analysis and Layer Segmentation (GOALS)}}. We won \textBC{orange}{\href{https://aistudio.baidu.com/aistudio/competition/detail/230/0/leaderboard}{the second place (2/100)}}.
\textbf{\uppercase\expandafter{\romannumeral5})} We further provide more implementation details and thorough ablation studies at qualitative and quantitative aspects.
\textbf{\uppercase\expandafter{\romannumeral6})} We perform in-depth analyses and discussions for our multi-scale subtraction unit.}


\section{Related Work}
\label{sec:relatedwork}
\subsection{Medical Image Segmentation Network}\label{subsec:misn}
According to the characteristics of different organs or lesions, we classify existing medical image segmentation methods into two types: medical-general and medical-specific one.
\\
\textbf{Medicine-general Methods.}
With the U-Net~\cite{UNet} achieving stable performance in the medical image segmentation field, the U-shape structure with encoder-decoder has become the basic segmentation baseline. U-Net++~\cite{UNet++} integrates both the long connection and short connection, which can reduce the semantic
gap between the feature maps of the encoder and decoder sub-networks. For attention U-Net~\cite{Attention_UNet}, an attention gate is embedded in each transition layer between the encoder and decoder block, which can automatically learn to focus on target structures of varying shapes and sizes. Recently, the Transformer~\cite{Transformer} architecture has achieved success in many natural language processing tasks. Some works~\cite{UTNet,TransUNet} explore its effectiveness for the medical vision tasks.
%
UTNet~\cite{UTNet} is a simple but powerful hybrid transformer architecture, which applies the self-attention module in both the encoder and decoder to capture remote dependencies of different scales with minimal overhead.
Another representative transformer-based model is the TransUNet~\cite{TransUNet}, which encodes strong global
context by treating the image features as sequences and utilizes the low-level CNN features via a u-shaped hybrid architectural design.
\\
\textbf{Medicine-specific Methods.}
In the polyp segmentation task, SFA~\cite{SFA} and PraNet~\cite{PraNet}, focus on recovering the sharp boundary between a polyp and its surrounding mucosa. The former proposes a selective feature aggregation structure and a boundary-sensitive loss function under a shared encoder and two mutually constrained decoders. The latter utilizes a reverse attention module to establish the relationship between the region and boundary cues. In addition, Ji \textit{et al.}~\cite{PNSNet} utilize spatio-temporal information to build the video polyp segmentation model.
In the COVID-19 lung infection task, Paluru \textit{et al.}~\cite{Anam-Net} propose an anamorphic depth embedding-based lightweight CNN to segment anomalies in COVID-19 chest CT images. Inf-Net~\cite{Inf-Net} builds the implicit reverse attention and explicit edge attention to model the boundaries. BCS-Net~\cite{BCS-Net} has three progressive boundary context-semantic reconstruction blocks, which can help the decoder to capture the piecemeal region for lung infection. In the breast segmentation task, Byra \textit{et al.}~\cite{SKUNet} develop a selective kernel via an attention mechanism to adjust the receptive fields of the U-Net, which can further improve the segmentation accuracy of breast tumors. Chen \textit{et al.}~\cite{NU-net} propose a nested U-net to achieve robust representation of breast tumors by exploiting different depths and sharing weights.
%%%%%%%%%%%%%%%%

We can see that the medicine-general methods are usually towards general challenges (i.e., rich feature representation, multi-scale information extraction and cross-level feature aggregation). And, the medicine-specific methods propose targeted solutions based on the characteristics of the current organ or lesion, such as designing a series of attention mechanisms, edge enhancement modules, uncertainty estimation, etc.
%
However, both general medicine-general and medicine-specific models rely on a large number of addition or concatenation operations to achieve feature fusion, which weakens the specificity parts among complementary features. Our proposed multi-scale subtraction module naturally focuses on extracting difference information, thus providing the decoder with efficient targeted features.


\subsection{Multi-scale Feature Extraction}\label{subsec:multiscale}
Scale cues play an important role in capturing contextual information of objects. Inspired by the scale-space theory that has been widely validated as an effective and theoretically sound framework, more and more multi-scale methods are proposed. Compared with single-scale features, multi-scale features are beneficial to address naturally occurring scale variations. This characteristic can help the medical segmentation models perceive lesions with different scales. According to the form, current multi-scale based methods can be roughly divided into two categories, namely, the inter-layer multi-scale structure and the intra-layer multi-scale structure. The former is based on features with different scales extracted by the feature encoder and progressively aggregates them in decoder, such as the U-shape~\cite{UNet,UNet++,FPN,ACSNet,PraNet,U2Net,DSS,MINet} architecture.
The latter usually equips the multi-scale pluggable modules, such as ASPP~\cite{ASPP}, DenseASPP~\cite{DenseASPP}, FoldASPP~\cite{GateNet}, and PAFEM~\cite{DANet_RGBDSOD} to construct the parallel multi-branch convolution layers with different dilated rates to obtain a rich combination of receptive fields. Different from them, we propose the multi-scale in multi-scale subtraction module with the extreme multi-scale information through introducing both inter-layer and intra-layer multi-scale at once. And, the intra-layer multi-scale subtraction unit focuses on mining the self-difference properties of the pairs of features from pixel-pixel to region-region. The whole process is very efficient without extra parameters compared to single-scale operations.
\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{data/Pipeline.pdf}
\caption{Overview of the proposed multi-scale subtraction network.}
\label{fig:Pipeline}
\end{figure*}
\subsection{Loss Method}\label{subsec:loss}
Most loss functions in image segmentation are based on cross-entropy or coincidence measures. The traditional cross-entropy loss treats the categories information equally.  Long \textit{et al.}~\cite{FCN} propose a weighted cross-entropy loss (WCE) for each class to offset the class imbalance in the data. Lin \textit{et al.}~\cite{Focal-loss} introduce the weights of difficult and easy samples to propose the Focal loss.  Dice loss~\cite{V-net} is proposed as the loss function of coincidence measurement in V-Net, which can effectively suppress problems caused by category imbalance. Tversky loss~\cite{Tversky-loss} is a regularized version of Dice loss to control the contribution of accuracy and recall to the loss function. Wong \textit{et al.}~\cite{EL-LOSS} propose exponential logarithmic loss (EL Loss) through the weighted summation of Dice loss and WCE loss to improve the segmentation accuracy of small structure objects. Taghanaki \textit{et al.}~\cite{Combo-loss} find that there is a risk in using the loss function based on overlap alone, and propose the como-loss to combine Dice loss as a regularization term with WCE loss to deal with the problem of input and output imbalance. Although these various loss functions have different effects at different levels, it is indeed time-consuming and laborious to manually design these complex functions. To this end, we propose the automatic and comprehensive segmentation loss structure, coined as the LossNet.

\section{Method}
The M$^{2}$SNet architecture is shown in Fig.~\ref{fig:Pipeline}, in which there are five encoder blocks ($\mathbf{E}^i$, $i \in \left \{1, 2, 3, 4, 5 \right \}$), a multi-scale in multi-scale subtraction module (MMSM) and four decoder blocks ($\mathbf{D}^i$, $i \in \left \{1, 2, 3, 4 \right \}$). We adopt the Res2Net-50 as the backbone to extract five levels of features. First, we separately adopt a $3 \times 3$ convolution for feature maps of each encoder block to reduce the channel to $64$, which can decrease the number of parameters for subsequent operations. Next, these different level features are fed into the MMSM and output five complementarity
enhanced features (${CE}^i$, $i \in \left \{1, 2, 3, 4, 5 \right \}$). Finally, each ${CE}^i$ progressively participates in the decoder and generates the final prediction. In the training phase, both the prediction and ground truth are input into the LossNet to achieve supervision. We describe the multi-scale in multi-scale subtraction module in Sec.~\ref{sec:msm} and give the details of LossNet in Sec.~\ref{sec:lossnet}.
\subsection{Multi-scale in Multi-scale Subtraction Module}\label{sec:msm}
We use  $F_{A}$ and $F_{B}$ to represent adjacent level feature maps. They all have been activated by the ReLU operation. We  define a basic subtraction unit (SU):
  \begin{equation}\label{equ:1}
 \begin{split}
     SU = Conv(\vert F_{A} \ominus F_{B} \vert ),
 \end{split}
\end{equation}
 where $\ominus$ is the element-wise subtraction operation, $ \vert \cdot \vert$ calculates the absolute value and $Conv(\cdot)$ denotes the convolution layer. Directly performing single-scale subtraction on the features of element positions is only to establish the difference relationship on the isolated pixel level, without considering that the lesion may have the characteristics of regional clustering. Compared to the MICCAI version~\cite{MSNet} of MSNet with the single-scale subtraction unit, we design a powerful intra-layer multi-scale subtraction unit (MSU) and improve MSNet to M$^{2}$SNet. As shown in Fig.~\ref{fig:MSU}, we utilize the multi-scale convolution filters with fixed full one weights of size $1 \times 1$, $3 \times 3$ and $5 \times 5$ to calculate the detail and structure difference values according to the pixel-pixel and region-region pattern. Using multi-scale filters with fixed parameters not only can directly capture the multi-scale difference clues between initial feature pairs at matched spatial locations, but also achieve efficient training without introducing additional parameter burdens. Therefore, M$^{2}$SNet can maintain the same low computation as MSNet and achieve higher precision performance. The entire multi-scale subtraction process can be formulated as:
\begin{equation}\label{equ:2}
  \begin{split}
     MSU = Conv(  \quad \quad \quad \quad \quad \quad     \\
     \vert Filter( F_{A} )_{1\times1}\ominus Filter( F_{B} )_{1\times1} \vert + \\ \vert Filter( F_{A} )_{3\times3}\ominus Filter( F_{B} )_{3\times3} \vert + \\ \vert Filter( F_{A} )_{5\times5}\ominus Filter( F_{B} )_{5\times5} \vert ),
 \end{split}
 \end{equation}
 where $Filter(\cdot)_{n \times n}$ represents the full one filter of size $n \times n$. The MSU can capture the complementary information of $F_{A}$ and $F_{B}$ and highlight their differences from texture to structure, thereby providing richer information for the decoder.

To obtain higher-order complementary information across multiple feature levels,
we horizontally and vertically concatenate multiple MSUs to calculate a series of differential features with different orders and receptive fields. The detail of the multi-scale in multi-scale subtraction module can be found in Fig.~\ref{fig:Pipeline}. We aggregate the scale-specific feature ($MS^{i}_{1}$) and cross-scale differential features ($MS^{i}_{n \neq 1}$) between the corresponding level and any other levels to generate complementarity enhanced feature ($CE^{i}$). This process can be formulated as follows:
\begin{equation}\label{equ:3}
 \begin{split}
     CE^{i} = Conv(\sum_{n=1}^{6-i}MS^{i}_{n} ) \quad i=1, 2, 3, 4, 5.
 \end{split}
\end{equation}
Finally, all $CE^{i}$ participate in decoding and then the polyp region is segmented.
\subsection{LossNet}\label{sec:lossnet}

\begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{data/MS-Unit-v3.pdf}
\caption{Detailed diagram of multi-scale subtraction unit.}
\label{fig:MSU}
\end{figure}

In the proposed model, the total training loss can be written as:
\begin{equation}\label{equ:4}
 \begin{split}
    \mathcal{L}_{total}=\mathcal{L}_{IoU}^w +\mathcal{L}_{BCE}^w + \mathcal{L}_{f},
 \end{split}
\end{equation}
 where $\mathcal{L}_{IoU}^w$ and $\mathcal{L}_{BCE}^w$ represent the weighted IoU loss and binary cross-entropy (BCE) loss which have been widely adopted in segmentation tasks. We use the same definitions as in ~\cite{PraNet,F3Net,BASNet} and their effectiveness has been validated in these works. Different from them, we extra use a LossNet to further optimize the segmentation from detail to structure. Specifically, we use an ImageNet pre-trained classification network, such as VGG-16, to extract the multi-scale features of the prediction and ground truth, respectively. Then, their feature difference is computed as loss $\mathcal{L}_{f}$:
\begin{equation}\label{equ:5}
 \begin{split}
    \mathcal{L}_{f} = {l}_{f}^{1} + {l}_{f}^{2} + {l}_{f}^{3} + {l}_{f}^{4}.
 \end{split}
\end{equation}
Let ${F}_{P}^{i}$ and ${F}_{G}^{i}$ separately represent the $i$-th level feature maps extracted from the prediction and ground truth. The ${l}_{f}^{i}$ is calculated as their Euclidean distance (L2-Loss), which is supervised at the pixel level:
\begin{equation}\label{equ:5}
 \begin{split}
    {l}_{f}^{i} = \vert\vert {F}_{P}^{i} -  {F}_{G}^{i} \vert\vert_{2}, \quad i=1, 2, 3, 4.
 \end{split}
\end{equation}
As can be seen from Fig~\ref{fig:P_loss}, the low-level feature maps contain rich boundary information and the high-level ones depict location information. Thus, the LossNet can generate comprehensive supervision at the feature levels.
\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{data/P_loss.png}
\caption{Illustration of LossNet.}
\label{fig:P_loss}
\end{figure}


\section{Experiments}
\subsection{Datasets}
Extensive experiments are conducted to verify the effectiveness of the proposed framework on four different types of medical segmentation tasks with data from varied image modalities, including color colonoscopy imaging, ultrasound imaging, computed tomography (CT), and optical coherence tomography (OCT).

\textbf{Polyp Segmentation.}
According to GLOBOCAN 2020 data, colorectal cancer is the third most common cancer worldwide and the second most common cause of death. It usually begins as small, noncancerous (benign) clumps of cells called polyps that form on the inside of the colon. We evaluate the proposed model on five benchmark datasets: CVC-ColonDB~\cite{CVC-ColonDB}, ETIS~\cite{ETIS}, Kvasir~\cite{Kvasir}, CVC-T~\cite{CVC-T} and CVC-ClinicDB~\cite{CVC-ClinicDB}. We adopt the same training set as the latest image polyp segmentation method~\cite{PraNet}, that is, $900$ samples from the Kvasir and $550$ samples from the CVC-ClinicDB~\cite{CVC-ClinicDB} are used for training.
The remaining images and the other three datasets are used for testing. Besides, there are some video-based polyp datasets, including the CVC-300\cite{cvc-300} and CVC-612\cite{CVC-ClinicDB}. We follow the latest video polyp segmentation method to split the videos from CVC-300 (12 clips) and CVC-612 (29 clips)
into 60\% for training, 20\% for validation, and 20\% for testing.

\textbf{COVID-19 Lung Infection.}
Coronavirus Disease 2019 (COVID-19) spread
globally in early 2020, causing the world to face an existential health crisis. At present, there are few public COVID-19 lung CT
datasets for infection segmentation. To have relatively
sufficient samples for training, we slice the public dataset~\cite{COVID-19_dataset1} and merge with the public datasets~\cite{COVID-19_dataset2} to obtain $1,277$ high-quality CT images by uniform sampling. And then, we further divide them into $894$ training images and $383$ testing images.

\textbf{Breast Ultrasound Segmentation.}
Breast cancer is one of the most dreaded cancers in
women~\cite{Breast_cancer}. Segmenting the lesion region from
breast ultrasound images is essential for tumor diagnosis. BUSI~\cite{BUSI} dataset contains $780$ images of $600$ female patients. Among them, there are $133$ normal cases, $437$ benign tumors, and $210$ malignant tumors. We follow the popular breast ultrasound segmentation methods~\cite{SKUNet,NU-net} to perform four-fold cross-validation on BUSI.

\textbf{OCT Layer Segmentation.} The OCT images are often used to diagnose and monitor retinal diseases more accurately based on abnormality quantification and retinal layer thickness computation both in research centers and clinic routines. At present, many scholars have been studying the segmentation of fundus structure in macular OCT scans, but few focus on parapapillary circular scans. To fully show the generalization of M$^{2}$SNet in different medical tasks, we take our M$^{2}$SNet to participate in \textBC{orange}{\href{https://aistudio.baidu.com/aistudio/competition/detail/230/0/introduction}{MICCAI 2022 Challenge: Glaucoma Oct Analysis and Layer Segmentation (GOALS)}}. It requests participants to segment three layers, which has positive significance for the diagnosis of glaucoma, including retinal nerve fiber layer (RNFL), ganglion cell-inner plexiform layer (GCIPL), and choroid layer, as shown in Fig.~\ref{fig:GOALS_show}. The GOALS2022~\cite{GOALS} dataset contains $300$ circumpapillary OCT. There are three equal groups with $100$ OCT images for the training process, the preliminary competition process and the final process, respectively. The GOALS2022 challenge attracts $100$ teams from all over the world to participate, and we finally won \textBC{orange}{\href{https://aistudio.baidu.com/aistudio/competition/detail/230/0/leaderboard}{the second place (2/100)}}.
\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{data/GOALS_show.pdf}
\caption{Visualization of the fundus OCT layer segmentation.}
\label{fig:GOALS_show}
\end{figure}

\begin{table}[t]
  \centering
  \scriptsize
  \renewcommand{\arraystretch}{1.1}
  \setlength\tabcolsep{5pt}
  \caption{ Quantitative comparisons on image polyp segmentation datasets. Top $2$ scores are highlighted in \textBC{mycellreda}{red} and \textBC{mycellblue}{blue}, respectively.  ``$\dagger$'' represents the medicine-specific method.
  }\label{tab:image_polyp_comparison}
  \resizebox{\columnwidth}{!}
  {
 \input{table/image_polyp_comparison.tex}
  }
\end{table}

\begin{table}[!t]
\centering
\scriptsize
  \renewcommand{\arraystretch}{1.1}
\setlength\tabcolsep{5pt}
\caption{ Quantitative comparisons on video polyp segmentation datasets. Top $2$ scores are highlighted in \textBC{mycellreda}{red} and \textBC{mycellblue}{blue}, respectively.  ``$\dagger$'' and ``$\bigstar$'' represent the medicine-specific method and the video polyp method, respectively.
  }
  \label{tab:video_polyp_comparison}
\resizebox{\columnwidth}{!}
{\input{table/video_polyp_comparison.tex}}
\end{table}

\subsection{Evaluation Metrics}
There are many popular metrics used in different medical segmentation branches. mean Dice (mDice), mean IoU (mIoU), the weighted F-measure ($F_{\beta}^{w}$)~\cite{Fwb}, S-measure ($S_{\alpha}$)~\cite{S-m}, E-measure ($E_\phi^{max}$)~\cite{Em} and mean absolute error (MAE) are widely used in polyp segmentation. Following~\cite{Inf-Net}, five metrics are employed for quantitative evaluation, including Precision, Recall, Dice
Similarity Coefficient (DSC)~\cite{DSC}, S-measure and MAE. Jaccard, Precision, Recall, Dice and Specificity~\cite{AAU-net,NU-net} are more commonly used for breast tumor segmentation. For OCT layer segmentation, \textBC{orange}{\href{https://aistudio.baidu.com/aistudio/competition/detail/230/0/introduction}{GOALS2022}} adopts the Dice coefficient and mean Euclidean distance (MED) to evaluate segmentation bodies and edges, respectively.  The lower value is better for the MAE and MED, and higher is better for others.

\begin{table}[t]
\centering
\scriptsize
  % \renewcommand{\arraystretch}{1.1}
\setlength\tabcolsep{5pt}
\caption{Quantitative comparisons on the COVID-19 lung CT dataset. Top $2$ scores are highlighted in \textBC{mycellreda}{red} and \textBC{mycellblue}{blue}, respectively.  ``$\dagger$'' represents the medicine-specific method.
  }
  \label{tab:COVID_comparison}
\resizebox{\columnwidth}{!}
{\input{table/COVID_comparison.tex}}
\end{table}

\begin{table}[!t]
\centering
\scriptsize
  % \renewcommand{\arraystretch}{1.1}
\setlength\tabcolsep{3pt}
\caption{ Quantitative comparisons on the breast ultrasound dataset. Top $2$ scores are highlighted in \textBC{mycellreda}{red} and \textBC{mycellblue}{blue}, respectively.  ``$\dagger$'' represents the medicine-specific method.
  }
  \label{tab:BUSI_comparison}
\resizebox{\columnwidth}{!}
{\input{table/BUSI_comparison.tex}}
\end{table}

\begin{table}[!t]
\centering
\scriptsize
  % \renewcommand{\arraystretch}{1.1}
\setlength\tabcolsep{2pt}
\caption{The leaderboard of \textBC{orange}{\href{https://aistudio.baidu.com/aistudio/competition/detail/230/0/leaderboard}{MICCAI2022 Challenge: Glaucoma Oct Analysis and Layer Segmentation (GOALS)}}. Top $2$ scores are highlighted in \textBC{mycellreda}{red} and \textBC{mycellblue}{blue}, respectively.
  }
  \label{tab:OCT_layer_comparison}
\resizebox{\columnwidth}{!}
{\input{table/OCT_layer_comparison.tex}}
\end{table}

\begin{table}[!t]
  \centering
  \scriptsize
  \renewcommand{\arraystretch}{1.1}
  \setlength\tabcolsep{5pt}
  \caption{ The FLOPs and parameters of different methods. The best and worst results are shown in \textBC{mycellreda}{red} and \textBC{mycellblue}{blue}, respectively.
  }\label{tab:FLOPs_para}
  \resizebox{\columnwidth}{!}
  {\input{table/FLOPs_para.tex}
}
\end{table}

\subsection{Implementation Details}
Our model is implemented based on the PyTorch framework and trained on a single 2080Ti GPU with mini-batch size $16$.  We resize the inputs to $352 \times 352$ and employ a general multi-scale training strategy as most methods~\cite{F3Net,GCPANet,Rank-Net_COD,SPNet_RGBDSOD,PraNet,MSNet}. Random horizontally flipping and random rotate data augmentation are used to avoid overfitting. For the optimizer, we adopt the stochastic gradient descent (SGD). The momentum and weight decay are set as $0.9$ and $0.0005$, respectively.  Maximum learning rate is set to $0.005$ for backbone and $0.05$ for other parts. Warm-up and linear decay strategies  are  used to  adjust  the  learning  rate.
For any medical image sub-tasks, the above training strategy is used for all the multi-scale subtraction models involved in this paper.
The difference among these models is only in the number of training epochs due to different convergence speeds. Specifically, the number of training epochs settings in the  polyp segmentation, COVID-19 Lung Infection, breast tumor segmentation and OCT layer segmentation are $50$, $200$, $100$ and $100$, respectively.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{data/visual_comparison.pdf}
\caption{Visual comparison of different medicine-general and medicine-specific methods.}
\label{fig:visual_comparison}
\end{figure}

 \begin{table}[!t]
  \centering
  \scriptsize
  \renewcommand{\arraystretch}{1.1}
  \setlength\tabcolsep{1pt}
    \caption{Ablation experiments of the subtraction unit, inter-layer multi-scale aggregation and LossNet.}\label{tab:ablation_study_1}
     \resizebox{\columnwidth}{!}
     {
	\input{table/ablation_study_1.tex}
	}
\end{table}

\begin{figure*}[!t]
\centering
\includegraphics[width=\linewidth]{data/aspp_denseaspp_ours_structure.pdf}
\caption{Illustration of different multi-scale modules applied in the subtraction unit.}
\label{fig:aspp_denseaspp_ours_structure}
\end{figure*}


 \begin{table*}[!t]
  \centering
  \scriptsize
  \renewcommand{\arraystretch}{1.1}
  \setlength\tabcolsep{2pt}
    \caption{Ablation experiments of the intra-layer multi-scale subtraction design. Positive and negative gains are highlighted in \textBC{mycellreda}{red} and \textBC{mycellblue}{blue}, respectively. }\label{tab:ablation_study_2}
     \resizebox{\linewidth}{!}
     {
\input{table/ablation_study_2.tex}
	}
\end{table*}



\subsection{Comparisons with State-of-the-art Methods}
For a fair comparison, we compare not only with medicine-specific methods but also with representative medicine-general methods, including UNet~\cite{UNet}, UNet++~\cite{UNet++}, Attention U-Net~\cite{Attention_UNet}, UTNet~\cite{UTNet} and TransUNet~\cite{TransUNet}. Based on the open-source codes, we retrain these medicine-general methods on the same training sets as our models.
\\
\noindent$\bullet$ In Tab.~\ref{tab:image_polyp_comparison}, among $30$ scores of all \textit{\textbf{image polyp}} datasets, our multi-scale subtraction models (MSNet + M$^{2}$SNet) achieve the best performance in terms of all six metrics. The M$^{2}$SNet even outperforms the video-based polyp segmentation method PNS-Net~\cite{PNSNet} on the \textit{\textbf{video polyp}} datasets, as shown in Tab.~\ref{tab:video_polyp_comparison}.
\\
\noindent$\bullet$
Tab.~\ref{tab:COVID_comparison} shows performance comparisons on the \textit{\textbf{COVID-19}} datasets. Compared to the second best method (Inf-Net~\cite{Inf-Net}), M$^{2}$SNet achieves an important improvement of $1.5\%$, $6.6\%$ and $14.3\%$ in terms of DSC, Precision and MAE, respectively.
\\
\noindent$\bullet$
Tab.~\ref{tab:BUSI_comparison} shows performance comparisons with \textit{\textbf{breast tumor segmentation}} methods. Following most methods~\cite{SKUNet,NU-net} in this field, we adopt the four-fold cross-validation strategy.
M$^{2}$SNet achieves the best performance in terms of the Jaccard, Precision and Dice metrics, which outperforms representative transformer-based methods, UTNet and TransUNet. Moreover, M$^{2}$SNet has the smallest mean standard deviation ($0.97$) under the five metrics, which indicates its performance stability.
\\
\noindent$\bullet$
In Tab.~\ref{tab:OCT_layer_comparison}, we list the top $10$ \textit{\textbf{OCT layer segmentation}} results in MICCAI2022 GOALS Challenge. More details please see the  \textBC{orange}{\href{https://aistudio.baidu.com/aistudio/competition/detail/230/0/leaderboard}{leaderboard}}. Based on the M$^{2}$SNet, we won the second place (2/100) according to the weighted score of six results in three different layers. It is worth noting that our method ranks the top $1$ in four out of six metrics.
\\
\noindent$\bullet$
As can be seen from Tab.~\ref{tab:image_polyp_comparison} - Tab.~\ref{tab:BUSI_comparison}, our M$^{2}$SNet consistently surpasses other medicine-general methods in all medical segmentation sub-branches. In Tab.~\ref{tab:FLOPs_para}, we list the FLOPs and parameters of different medicine-general methods. It can be seen that our method has only 9GB in FLOPs, which has obvious advantages in terms of computational efficiency.
\\
\noindent$\bullet$
Fig.~\ref{fig:visual_comparison} depicts a qualitative comparison with other methods. It can be seen that the results of M$^{2}$SNet have greater advantages in terms of detection accuracy, completeness, and sharpness across different image modalities.



% \begin{figure}[t]
% \centering
% \includegraphics[width=\linewidth]{data/abalation.png}
% \caption{Visual output from Multi-scale Subtraction Unit in different levels}
% \label{fig:ablation_study_visual_encoder_transition}
% \end{figure}


\subsection{Ablation Study}
We take the common FPN network as the baseline to analyze the contribution of each component.
\subsubsection{Effectiveness of the subtraction unit, inter-layer multi-scale subtraction aggregation and LossNet}
The results are shown in Tab~\ref{tab:ablation_study_1}. These defined feature subscripts are the same as those in Fig~\ref{fig:Pipeline}.
First, we apply the basic subtraction unit (SU) to the baseline to get a  series of $SU_{2}^{i}$  features to participate in the feature aggregation calculated by Equ.~\ref{equ:1}. The gap between the `` + $SU_{2}^{i}$ ''  and the baseline demonstrates the effectiveness of the SU.  It can be seen that the usage of SU has a significant improvement on the ColonDB dataset compared to the baseline, with the gain of 7.8\%, 7.4\%, 6.7\% and 4.4\% in terms of mDice, mIoU, $F_\beta^w$, and $E_\phi^{max}$, respectively. Next, we gradually add  $SU_{3}^{i}$, $SU_{4}^{i}$ and $MS_{5}^{i}$ to achieve inter-layer multi-scale aggregation. The gap between the `` + $SU_{5}^{i}$ ''  and the `` + $SU_{2}^{i}$ '' quantitatively demonstrates the effectiveness of inter-layer multi-scale subtraction strategy. Next, we evaluate the benefit of $\mathcal{L}_{f}$.  Compared to the  `` + $SU_{5}^{i}$ '' model,  the  `` + $\mathcal{L}_{f}$ '' achieves significant performance improvement on the ETIS dataset, with the gain of 11.8\%, 14.1\%, 13.0\% and 5.5\% in terms of mDice, mIoU, $F_\beta^w$, and $E_\phi^{max}$, respectively. Besides,  we replace all subtraction units with the element-wise addition units (AU) and compare their performance. It can be seen that our subtraction units have significant advantage and no additional parameters are introduced.
\subsubsection{Effectiveness of the intra-layer multi-scale subtraction design}
Compared to the previous MSNet, the M$^{2}$SNet replace all the original single-scale subtraction unit with the stronger multi-scale subtraction unit. As can be seen from Tab.~\ref{tab:image_polyp_comparison} - Tab.~\ref{tab:BUSI_comparison}, M$^{2}$SNet shows significant improvement over MSNet on ten datasets of three tasks. To further show the advantages of our intra-layer multi-scale design, we apply other popular multi-scale modules (i.e., ASPP~\cite{ASPP} and DenseASPP~\cite{DenseASPP}) to the subtraction unit and these architectures are shown in Fig.~\ref{fig:aspp_denseaspp_ours_structure}. In Tab.~\ref{tab:ablation_study_2}, we thoroughly compare both the efficiency and accuracy of these three structures. It can be seen that ``M$^{2}$SNet (Ours)'' has a significant performance gain in terms of fourteen metrics on four challenges datasets under different tasks. However, the other two models not only increase the computational burden by more than 50\%, but also produce negative gains in multiple datasets. Therefore, the proposed intra-layer multi-scale subtraction design can be taken as a new baseline for future research in subtraction family.

To more intuitively show the differential information from different scales, we visualize all the features of the multi-scale in multi-scale subtraction module, as shown in Fig~\ref{fig:ms_module_all_feature_visual_1}. We can see that the multi-scale in multi-scale subtraction module can clearly highlight the difference between high-level features and other level features and propagate its localization effect to the low-level ones. At the same level, the intra-layer multi-scale aggregation design can comprehensively capture both the subtle and regional difference features. Thus, both the global structural information and local boundary information is well depicted in the enhanced features of different levels.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{data/ms_module_all_feature_visual_1.pdf}
\caption{Visualization of the feature maps in the multi-scale in multi-scale subtraction module.}
\label{fig:ms_module_all_feature_visual_1}
\end{figure}

\section{Discussion}
\textbf{Multi-scale Subtraction Unit}:
Different from previous addition and concatenation operations,
using subtraction in multi-level structure make resulted features input to the
decoder have much less redundancy among different levels and their level-specific
properties are significantly enhanced. In this work, we further explore the potential of the subtraction unit in intra-layer multi-scale fusion. How to improve the accuracy while maintaining the same efficiency as the single-scale one is the key challenge. We provide the solution of using multi-scale convolution filters with fixed parameters. Compared to the single-scale design, multi-scale subtraction unit can enable the network to collect more complementary information both in pixel-pixel and neighbor-neighbor levels. The advantages of multi-scale subtraction unit in terms of efficiency and accuracy can be seen in Tab.~\ref{tab:ablation_study_2}. Multi-scale information extraction and feature aggregation are two general problems in the field of computer vision. Our multi-scale subtraction unit can solve both of them at once. We think this new paradigm can drive more researches on the subtraction operation in the future.
\\
\textbf{LossNet}: LossNet is similar in form to perception loss~\cite{Ploss} that has been applied in many tasks, such as style transfer and inpainting.  While in those vision tasks, the perception-like loss is mainly used to speed the convergence of GAN and obtain high frequency information and ease checkerboard artifacts, but it does not bring obvious accuracy improvement. In our paper, the inputs are binary segmentation masks, LossNet can directly target the geometric features of the lesion and perform joint supervisions from the contour to the body, thereby improving the overall segmentation accuracy.

\section{Conclusion}
In this paper, we rethink previous addition-based or concatenation-based methods and present a simple yet general multi-scale in multi-scale subtraction network (M$^{2}$SNet) for more efficient medical image segmentation.
%
Based on the proposed intra-layer multi-scale subtraction unit, we pyramidally aggregate adjacent levels to extract lower-order and higher-order cross-level complementary information and combine with level-specific information to enhance multi-scale feature representation.
%
Besides, we design a loss function based on a training-free network to supervise the prediction from different feature levels, which can optimize the segmentation on both structure and details during the backward phase.
%
Experimental results on $11$ benchmark datasets towards $4$ medical segmentation tasks demonstrate that the proposed model outperforms various state-of-the-art methods.

% \appendices

% \section*{Appendix and the Use of Supplemental Files}
% Appendices, if needed, appear before the acknowledgment. If an appendix is not
% critical to the main message of the manuscript and is included only for thoroughness
% or for reader reference, then consider submitting appendices as supplemental materials.
% Supplementary files are available to readers through IEEE \emph{Xplore\textregistered}
% at no additional cost to the authors but they do not appear in print versions.
% Supplementary files must be uploaded in ScholarOne as supporting documents, but for
% accepted papers they should be uploaded as Multimedia documents. Refer readers
% to the supplementary files where appropriate within the manuscript text using footnotes.
% \footnote{Supplementary materials are available in the supporting documents/multimedia tab.
% Further instructions on footnote usage are in the Footnotes section on the next page.}

% \section*{Acknowledgment}
% The preferred spelling of the word ``acknowledgment'' in American English is
% without an ``e'' after the ``g.'' Use the singular heading even if you have
% many acknowledgments. Avoid expressions such as ``One of us (S.B.A.) would
% like to thank $\ldots$ .'' Instead, write ``F. A. Author thanks $\ldots$ .'' In most
% cases, sponsor and financial support acknowledgments are placed in the
% unnumbered footnote on the first page, not here.

% \section*{References and Footnotes}

% \subsection{References}
% All listed references must be cited in text at least once. Use number citations
% that are placed in square brackets and inside the punctuation.

% Multiple references are each numbered with separate brackets.
% When citing a section in a book, please give the relevant page numbers.
% In text, refer simply to the reference number. Do not use ``Ref.'' or
% ``reference'' except at the beginning of a sentence:
% ``Reference \cite{b3} shows $\ldots$ .'

% Reference numbers are set flush left and form a column of their own, hanging
% out beyond the body of the reference. The reference numbers are on the line,
% enclosed in square brackets. In all references, the given name of the author
% or editor is abbreviated to the initial only and precedes the last name.
% List the names of all authors if there are six or fewer co-authors,
% otherwise list the primary author's name followed by \emph{at al.}
% Use commas around Jr., Sr., and III in names. Abbreviate conference titles.
% When citing IEEE transactions, provide the issue number, page range, volume number,
% year, and/or month if available. When referencing a patent, provide the day and
% month of issue or application. References may not include all information;
% please obtain and include relevant information. Do not combine references.
% There must be only one reference with each number. If there is a
% URL included with the print reference, it can be included at the end of the reference.

% Other than books, capitalize only the first word in a paper title, except
% for proper nouns and element symbols. For papers published in translation
% journals, please give the English citation first, followed by the original
% foreign-language citation. See the end of this document for formats and
% examples of common references. For a complete discussion of references and
% their formats, see the IEEE style manual at
% \underline{https://journals.ieeeauthorcenter.ieee.org/your-role-in-article-p}
% \discretionary{}{}{}\underline{roduction/ieee-editorial-style-manual/}.

% \subsection{Footnotes}
% Number footnotes separately using superscripts.\footnote{Place the actual
% footnote at the bottom of the column in which it is cited; do not put
% footnotes in the reference list (endnotes).}
% It is recommended that footnotes be avoided (except for
% the unnumbered footnote with the receipt date on the first page).
% Instead, try to integrate the footnote information into the text.
% Use letters for table footnotes (see Table \ref{table}).

% \section{References}

% \begin{itemize}

% \item \emph{Basic format for books:}\\
% J. K. Author, ``Title of chapter in the book,'' in \emph{Title of His Published Book, x}th ed. City of Publisher, (only U.S. State), Country: Abbrev. of Publisher, year, ch. $x$, sec. $x$, pp. \emph{xxx--xxx.}\\
% See \cite{b1,b2}.

% \item \emph{Basic format for periodicals:}\\
% J. K. Author, ``Name of paper,'' \emph{Abbrev. Title of Periodical}, vol. \emph{x, no}. $x, $pp\emph{. xxx--xxx, }Abbrev. Month, year, DOI. 10.1109.\emph{XXX}.123456.\\
% See \cite{b3}--\cite{b5}.

% \item \emph{Basic format for reports:}\\
% J. K. Author, ``Title of report,'' Abbrev. Name of Co., City of Co., Abbrev. State, Country, Rep. \emph{xxx}, year.\\
% See \cite{b6,b7}.

% \item \emph{Basic format for handbooks:}\\
% \emph{Name of Manual/Handbook, x} ed., Abbrev. Name of Co., City of Co., Abbrev. State, Country, year, pp. \emph{xxx--xxx.}\\
% See \cite{b8,b9}.

% \item \emph{Basic format for books (when available online):}\\
% J. K. Author, ``Title of chapter in the book,'' in \emph{Title of
% Published Book}, $x$th ed. City of Publisher, State, Country: Abbrev.
% of Publisher, year, ch. $x$, sec. $x$, pp. \emph{xxx--xxx}. [Online].
% Available: \underline{http://www.web.com}\\
% See \cite{b10}--\cite{b13}.

% \item \emph{Basic format for journals (when available online):}\\
% J. K. Author, ``Name of paper,'' \emph{Abbrev. Title of Periodical}, vol. $x$, no. $x$, pp. \emph{xxx--xxx}, Abbrev. Month, year. Accessed on: Month, Day, year, DOI: 10.1109.\emph{XXX}.123456, [Online].\\
% See \cite{b14}--\cite{b16}.

% \item \emph{Basic format for papers presented at conferences (when available online): }\\
% J.K. Author. (year, month). Title. presented at abbrev. conference title. [Type of Medium]. Available: site/path/file\\
% See \cite{b17}.

% \item \emph{Basic format for reports and handbooks (when available online):}\\
% J. K. Author. ``Title of report,'' Company. City, State, Country. Rep. no., (optional: vol./issue), Date. [Online] Available: site/path/file\\
% See \cite{b18,b19}.

% \item \emph{Basic format for computer programs and electronic documents (when available online): }\\
% Legislative body. Number of Congress, Session. (year, month day). \emph{Number of bill or resolution}, \emph{Title}. [Type of medium]. Available: site/path/file\\
% \textbf{\emph{NOTE: }ISO recommends that capitalization follow the accepted practice for the language or script in which the information is given.}\\
% See \cite{b20}.

% \item \emph{Basic format for patents (when available online):}\\
% Name of the invention, by inventor's name. (year, month day). Patent Number [Type of medium]. Available: site/path/file\\
% See \cite{b21}.

% \item \emph{Basic format}\emph{for conference proceedings (published):}\\
% J. K. Author, ``Title of paper,'' in \emph{Abbreviated Name of Conf.}, City of Conf., Abbrev. State (if given), Country, year, pp. \emph{xxxxxx.}\\
% See \cite{b22}.

% \item \emph{Example for papers presented at conferences (unpublished):}\\
% See \cite{b23}.

% \item \emph{Basic format for patents}$:$\\
% J. K. Author, ``Title of patent,'' U.S. Patent \emph{x xxx xxx}, Abbrev. Month, day, year.\\
% See \cite{b24}.

% \item \emph{Basic format for theses (M.S.) and dissertations (Ph.D.):}
% \begin{enumerate}
% \item J. K. Author, ``Title of thesis,'' M.S. thesis, Abbrev. Dept., Abbrev. Univ., City of Univ., Abbrev. State, year.
% \item J. K. Author, ``Title of dissertation,'' Ph.D. dissertation, Abbrev. Dept., Abbrev. Univ., City of Univ., Abbrev. State, year.
% \end{enumerate}
% See \cite{b25,b26}.

% \item \emph{Basic format for the most common types of unpublished references:}
% \begin{enumerate}
% \item J. K. Author, private communication, Abbrev. Month, year.
% \item J. K. Author, ``Title of paper,'' unpublished.
% \item J. K. Author, ``Title of paper,'' to be published.
% \end{enumerate}
% See \cite{b27}--\cite{b29}.

% \item \emph{Basic formats for standards:}
% \begin{enumerate}
% \item \emph{Title of Standard}, Standard number, date.
% \item \emph{Title of Standard}, Standard number, Corporate author, location, date.
% \end{enumerate}
% See \cite{b30,b31}.

% \item \emph{Article number in~reference examples:}\\
% See \cite{b32,b33}.

% \item \emph{Example when using et al.:}\\
% See \cite{b34}.

% \end{itemize}

% \begin{thebibliography}{00}

% \bibitem{b1} G. O. Young, ``Synthetic structure of industrial plastics,'' in \emph{Plastics,} 2\textsuperscript{nd} ed., vol. 3, J. Peters, Ed. New York, NY, USA: McGraw-Hill, 1964, pp. 15--64.

% \bibitem{b2} W.-K. Chen, \emph{Linear Networks and Systems.} Belmont, CA, USA: Wadsworth, 1993, pp. 123--135.

% \bibitem{b3} J. U. Duncombe, ``Infrared navigation---Part I: An assessment of feasibility,'' \emph{IEEE Trans. Electron Devices}, vol. ED-11, no. 1, pp. 34--39, Jan. 1959, 10.1109/TED.2016.2628402.

% \bibitem{b4} E. P. Wigner, ``Theory of traveling-wave optical laser,'' \emph{Phys. Rev}., vol. 134, pp. A635--A646, Dec. 1965.

% \bibitem{b5} E. H. Miller, ``A note on reflector arrays,'' \emph{IEEE Trans. Antennas Propagat}., to be published.

% \bibitem{b6} E. E. Reber, R. L. Michell, and C. J. Carter, ``Oxygen absorption in the earth's atmosphere,'' Aerospace Corp., Los Angeles, CA, USA, Tech. Rep. TR-0200 (4230-46)-3, Nov. 1988.

% \bibitem{b7} J. H. Davis and J. R. Cogdell, ``Calibration program for the 16-foot antenna,'' Elect. Eng. Res. Lab., Univ. Texas, Austin, TX, USA, Tech. Memo. NGL-006-69-3, Nov. 15, 1987.

% \bibitem{b8} \emph{Transmission Systems for Communications}, 3\textsuperscript{rd} ed., Western Electric Co., Winston-Salem, NC, USA, 1985, pp. 44--60.

% \bibitem{b9} \emph{Motorola Semiconductor Data Manual}, Motorola Semiconductor Products Inc., Phoenix, AZ, USA, 1989.

% \bibitem{b10} G. O. Young, ``Synthetic structure of industrial
% plastics,'' in Plastics, vol. 3, Polymers of Hexadromicon, J. Peters,
% Ed., 2\textsuperscript{nd} ed. New York, NY, USA: McGraw-Hill, 1964, pp. 15-64.
% [Online]. Available:
% \underline{http://www.bookref.com}.

% \bibitem{b11} \emph{The Founders' Constitution}, Philip B. Kurland
% and Ralph Lerner, eds., Chicago, IL, USA: Univ. Chicago Press, 1987.
% [Online]. Available: \underline{http://press-pubs.uchicago.edu/founders/}

% \bibitem{b12} The Terahertz Wave eBook. ZOmega Terahertz Corp., 2014.
% [Online]. Available:
% \underline{http://dl.z-thz.com/eBook/zomega\_ebook\_pdf\_1206\_sr.pdf}. Accessed on: May 19, 2014.

% \bibitem{b13} Philip B. Kurland and Ralph Lerner, eds., \emph{The
% Founders' Constitution.} Chicago, IL, USA: Univ. of Chicago Press,
% 1987, Accessed on: Feb. 28, 2010, [Online] Available:
% \underline{http://press-pubs.uchicago.edu/founders/}

% \bibitem{b14} J. S. Turner, ``New directions in communications,'' \emph{IEEE J. Sel. Areas Commun}., vol. 13, no. 1, pp. 11-23, Jan. 1995.

% \bibitem{b15} W. P. Risk, G. S. Kino, and H. J. Shaw, ``Fiber-optic frequency shifter using a surface acoustic wave incident at an oblique angle,'' \emph{Opt. Lett.}, vol. 11, no. 2, pp. 115--117, Feb. 1986.

% \bibitem{b16} P. Kopyt \emph{et al., ``}Electric properties of graphene-based conductive layers from DC up to terahertz range,'' \emph{IEEE THz Sci. Technol.,} to be published. DOI: 10.1109/TTHZ.2016.2544142.

% \bibitem{b17} PROCESS Corporation, Boston, MA, USA. Intranets:
% Internet technologies deployed behind the firewall for corporate
% productivity. Presented at INET96 Annual Meeting. [Online].
% Available: \underline{http://home.process.com/Intranets/wp2.htp}

% \bibitem{b18} R. J. Hijmans and J. van Etten, ``Raster: Geographic analysis and modeling with raster data,'' R Package Version 2.0-12, Jan. 12, 2012. [Online]. Available: \underline {http://CRAN.R-project.org/package=raster}

% \bibitem{b19} Teralyzer. Lytera UG, Kirchhain, Germany [Online].
% Available:
% \underline{http://www.lytera.de/Terahertz\_THz\_Spectroscopy.php?id=home}, Accessed on: Jun. 5, 2014

% \bibitem{b20} U.S. House. 102\textsuperscript{nd} Congress, 1\textsuperscript{st} Session. (1991, Jan. 11). \emph{H. Con. Res. 1, Sense of the Congress on Approval of}  \emph{Military Action}. [Online]. Available: LEXIS Library: GENFED File: BILLS

% \bibitem{b21} Musical toothbrush with mirror, by L.M.R. Brooks. (1992, May 19). Patent D 326 189 [Online]. Available: NEXIS Library: LEXPAT File: DES

% \bibitem{b22} D. B. Payne and J. R. Stern, ``Wavelength-switched pas- sively coupled single-mode optical network,'' in \emph{Proc. IOOC-ECOC,} Boston, MA, USA, 1985, pp. 585--590.

% \bibitem{b23} D. Ebehard and E. Voges, ``Digital single sideband detection for interferometric sensors,'' presented at the \emph{2\textsuperscript{nd} Int. Conf. Optical Fiber Sensors,} Stuttgart, Germany, Jan. 2-5, 1984.

% \bibitem{b24} G. Brandli and M. Dick, ``Alternating current fed power supply,'' U.S. Patent 4 084 217, Nov. 4, 1978.

% \bibitem{b25} J. O. Williams, ``Narrow-band analyzer,'' Ph.D. dissertation, Dept. Elect. Eng., Harvard Univ., Cambridge, MA, USA, 1993.

% \bibitem{b26} N. Kawasaki, ``Parametric study of thermal and chemical nonequilibrium nozzle flow,'' M.S. thesis, Dept. Electron. Eng., Osaka Univ., Osaka, Japan, 1993.

% \bibitem{b27} A. Harrison, private communication, May 1995.

% \bibitem{b28} B. Smith, ``An approach to graphs of linear forms,'' unpublished.

% \bibitem{b29} A. Brahms, ``Representation error for real numbers in binary computer arithmetic,'' IEEE Computer Group Repository, Paper R-67-85.

% \bibitem{b30} IEEE Criteria for Class IE Electric Systems, IEEE Standard 308, 1969.

% \bibitem{b31} Letter Symbols for Quantities, ANSI Standard Y10.5-1968.

% \bibitem{b32} R. Fardel, M. Nagel, F. Nuesch, T. Lippert, and A. Wokaun, ``Fabrication of organic light emitting diode pixels by laser-assisted forward transfer,'' \emph{Appl. Phys. Lett.}, vol. 91, no. 6, Aug. 2007, Art. no. 061103.~

% \bibitem{b33} J. Zhang and N. Tansu, ``Optical gain and laser characteristics of InGaN quantum wells on ternary InGaN substrates,'' \emph{IEEE Photon. J.}, vol. 5, no. 2, Apr. 2013, Art. no. 2600111

% \bibitem{b34} S. Azodolmolky~\emph{et al.}, Experimental demonstration of an impairment aware network planning and operation tool for transparent/translucent optical networks,''~\emph{J. Lightw. Technol.}, vol. 29, no. 4, pp. 439--448, Sep. 2011.

% \end{thebibliography}
\bibliography{tmi}
\bibliographystyle{IEEEtran}
\end{document}
