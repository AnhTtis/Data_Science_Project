\section{Related Work}

\subsection{Pattern discovery on systematic AI errors}

Systematic errors, sometimes coined as blind spots or unknown-unknowns \cite{BeatMachineChallengingHumans}, refer to model's failure over a group of instances that share similar semantics. There are various approaches for discovering such patterns, including algorithmic, human, or hybrid techniques.

A number of studies have shown that fully algorithmic techniques can help automatically discover unknown-unknowns \cite{lakkaraju2017identifying, coveragebasedutility}. Recently, several studies have also been proposed to advance the methods towards discovering automatic slices or subclasses that are semantically coherent \cite{DominoDiscoveringSystematicErrorsCrossModal, SpotlightGeneralMethodDiscoveringSystematic}, or to propose a framework for evaluating blindspot discovery methods in a unified manner \cite{EvaluatingSystemicErrorDetectionMethods}.

On the other hand, researchers have also explored how human intelligence can identify blind spots where automatic techniques alone do not work. Several studies \cite{BeatMachineChallengingHumans, ContradictMachineHybridApproach, HybridHumanAIWorkflowsUnknown, InvestigatingHumanMachineComplementarity} demonstrated that a well-designed crowdsourcing study can detect problematic instances. Hybrid workflows to leverage the abilities of both humans and machines \cite{HybridHumanAIWorkflowsUnknown, lakkaraju2017identifying, han2021iterative, chung2018unknownexamples} have also been explored throughout several studies in proposing collaborative human-AI workflow \cite{HybridHumanAIWorkflowsUnknown} or generating text descriptions \cite{han2021iterative} about spurious patterns.

While these studies demonstrate how human intelligence plays a significant role, tool support is still lacking to guide practitioners to inspect, identify, and mitigate systematic errors. In our study, we provide a workflow and systematic support for inspecting which systematic errors are attributed to interpretable concepts.

\subsection{Visual analytics for ML diagnostics}
Visual analytics tools in recent years have evolved to offer interactive ways for inspecting the machine learning process. In general, these tools aim to better visualize the predictive results in a model-agnostic manner or present the structure of the model in a model-specific way. Model-agnostic approaches propose to better visualize machine learning results regardless of model types. Many visualizations among them are largely designed on the grounds of confusion matrix as tree or flow diagram \cite{shen2020designing, VisualizingSurrogateDecisionTrees}, comparative visual design \cite{ManifoldModelAgnosticFrameworkInterpretation, ExplainExploreVisualExplorationMachine, olson2021contrastive, kaul2021improvingcounterfactuals, krause2017workflow}, radial \cite{VisualMethodsAnalyzingProbabilistic} or multi-axes based layout \cite{SquaresSupportingInteractivePerformance}. On the other hand, model-specific inspections also gained attention to support the inspection of a deep neural network inside its layers, neurons, or activations \cite{liu2017analyzingtraining, ShapeShopUnderstandingDeepLearning, TopoActVisuallyExploringShape, DeepVIDDeepVisualInterpretation}.

Visual analytic tools can also help inspect and explain the potential cause of systematic failures such as a shifted or skewed distribution of the training examples termed as out-of-distribution \cite{OoDAnalyzerInteractiveAnalysisOutofDistribution}, covariate or concept shift \cite{DiagnosingConceptDriftVisual} or machine biases \cite{FairVisVisualAnalyticsDiscovering, FairSightVisualAnalyticsFairness, WhatIfToolInteractiveProbing}. The OoD analyzer \cite{OoDAnalyzerInteractiveAnalysisOutofDistribution} presented a grid-based layout to visualize the distributional differences in training and test sets. The problem of concept drift was tackled and presented as visualizations in a 2D heatmap visualization \cite{DiagnosingConceptDriftVisual} or distribution-based scatterplot \cite{ConceptExplorerVisualAnalysisConcept}. Other interactive tools such as Deblinder \cite{DiscoveringValidatingAIErrorsCrowdsourced}, SEAL \cite{SEALInteractiveToolSystematicError}, or Error Analysis \cite{erroranalysis} have recently been proposed to mitigate systematic errors with subclass labeling or user-generated report. Compared to previous work, our study aims to promote a human-in-the-loop workflow consisting of tasks to identify biased patterns and their association/attribution aspects with the perspective of spurious associations.

% Recent visualization studies also proposed how to better explain them with counterfactuals [BF-1], or to present them in a form of report [BF-3]. 


\subsection{Understanding model with concept interpretability}

The XAI methods to explain the behavior of black box models \cite{InterpretabilityFeatureAttributionQuantitative, AutomaticConceptbasedExplanations, BayesianCaseModelGenerative, ConceptWhiteningInterpretable2020} have been recently expanded to a concept-level sensitivity. The method called TCAV (Testing Concept Activation Vector) \cite{InterpretabilityFeatureAttributionQuantitative} provides a post-hoc method to explain the global influence of a concept in a pre-trained model. ACE (Automatic Concept Extraction) \cite{AutomaticConceptbasedExplanations} was proposed to identify and filter interpretable concepts from the meaningful clusters of segments on the basis of TCAV. In \cite{ConceptWhiteningInterpretable2020}, Concept Whitening (CW) purposefully alters batch normalization layers to a concept whitening layer to learn an interpretable latent space. Especially, the whitening step in this method points out that the concept space needs to be preprocessed to better align concept vectors.

These concept-level interpretability methods, however, require the human ability to observe and extract semantically meaningful concepts \cite{AutomaticConceptbasedExplanations}. There are various ways to identify and extract concepts in collaboration with humans and systems \cite{AutomaticConceptbasedExplanations, NeuroCartographyScalableAutomaticVisualSummarization, zhao2021humanintheloopextraction, DASHVisualAnalyticsDebiasingImage,  ConceptExplorerVisualAnalysisConcept, ProtoSteerSteeringDeepSequence, AnchorVizFacilitatingSemanticData, ConceptVectorTextVisualAnalytics, VisualConceptProgrammingVisualAnalytics}. ConceptExtract \cite{zhao2021humanintheloopextraction} aimed to support concept extraction and classification in a human-in-the-loop workflow and visual tools. In DASH \cite{kwon2022dash}, problematic biases from irrelevant concepts can be identified through observations from users, which were proposed to be mitigated through random image generation using GAN techniques. ConceptExplainer \cite{ConceptExplorerVisualAnalysisConcept} was designed to explore the concept associations focusing on validating conceptual overlapping between classes, especially serving as a concept exploration tool for non-expert users. In \cite{VisualConceptProgrammingVisualAnalytics}, a self-supervised technique was proposed to automatically extract visual vocabulary to allow experts to refine the labeled data and understand the concepts.

Unlike existing work, our study proposes an interactive workflow of exploring concepts for the purpose of inspecting systematic errors and spurious concept associations behind them. Similar to \cite{WhatDidMyAILearn}, our human-in-the-loop workflow aims to promote the sensemaking of practitioners specifically in the problem of systematic errors where they can iteratively work on subsetting, contrasting patterns in instances, and hypothesizing spurious associations.


% All these methods including.. share the idea of defining a concept vector with a group of semantically coherent segments. While we take the approach of pre-processing steps on concept space in [] and sensitivity, we expand the utility of concept exploration towards inspecting model's false behaviors. In our study, we demonstrate that using concept interpretability can help not only interpreting the concept association towards misclassificaitons, and tracing back ... then removing the biases to further improve the quality of classification.





