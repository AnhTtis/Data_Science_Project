\section{Challenges}
\label{sec:challenges}
In this section, we summarize several challenges in inspecting and making sense of systematic errors. This is based on the review of literature \cite{BeatMachineChallengingHumans, HybridHumanAIWorkflowsUnknown, ContradictMachineHybridApproach, SpotlightGeneralMethodDiscoveringSystematic, DominoExtractingComparingManipulating, WhyShouldTrustYou, InterpretabilityFeatureAttributionQuantitative, DASHVisualAnalyticsDebiasingImage} that tackle pattern discovery in systematic errors and post-hoc explainability methods by examining their research problem, limitations, and future work. For each of the four challenges (C1-C4), we also identify the need for tool support due to the lack of capability in existing methods otherwise it is challenging for practitioners to achieve the task: \\\vspace{-5pt}

\hangindent=2em \textbf{C1. Systematic errors are not easily diagnosable.} Systematic errors are critical but hardly recognizable among thousands of instances in high-dimensional representational spaces \cite{BeatMachineChallengingHumans, HybridHumanAIWorkflowsUnknown}. It is important to support practitioners (1) to narrow down the scope of classes that are prone to systematic errors and (2) to inspect errors with respect to the model's confidence, to allow users to diagnose the potential blind spots. \\\vspace{-5pt}

\hangindent=2em \textbf{C2. Practitioners cannot easily discover how interpretable concepts are associated with errors or test hypotheses based on their knowledge or observations.} Humans are known to classify images with semantic concepts \cite{HybridHumanAIWorkflowsUnknown, BeatMachineChallengingHumans, ContradictMachineHybridApproach}. However, existing tools rarely leverage this ability to discover and hypothesize concept associations against misclassifications \cite{SpotlightGeneralMethodDiscoveringSystematic, DominoExtractingComparingManipulating}. To hypothesize and validate the association, practitioners should be able to easily capture patterns and concepts that exclusively appear over misclassifications in a certain class. The hypothesized concepts need to be readily defined to be tested. \\\vspace{-5pt}

\hangindent=2em \textbf{C3. Spurious associations are not immediately recognizable and understandable, with few clues for intervention.} Once an association is hypothesized, users need to (1) make sense of how concepts are associated with certain classes then cause misclassifications in another class and (2) quantitatively verify the degree of associations and attributions. While these aspects of systematic errors were separately studied \cite{DominoExtractingComparingManipulating, WhyShouldTrustYou, InterpretabilityFeatureAttributionQuantitative} or have not been explored in existing work, practitioners should be able to understand what causes systematic errors and precisely verify spurious associations in an explainable and measurable manner. \\\vspace{-5pt}
    
\hangindent=2em \textbf{C4. The sources of biases in spurious associations are not traceable and and thus the biases are difficult to mitigate.} The spurious associations, once detected, should be mitigated with strategies to correct biased concept patterns and be properly evaluated as to whether the association is mitigated as expected. While most of studies \cite{DASHVisualAnalyticsDebiasingImage, DiscoveringValidatingAIErrorsCrowdsourced, BeatMachineChallengingHumans, HybridHumanAIWorkflowsUnknown, ContradictMachineHybridApproach, DominoExtractingComparingManipulatinga} do not provide a way to efficiently trace back to the source of biases in training instances or in-the-wild datasets, practitioners are still required to find a way to precisely determine and evaluate mitigation strategies in practice.