\section{Introduction}
\label{sec:introduction}
Classification models learn from data to generalize the associations between data samples and their target classes. However, researchers have increasingly observed that this process can easily lead to systematic errors. This phenomenon, also referred to as ``AI blindspots'' \cite{BeatMachineChallengingHumans, ContradictMachineHybridApproach}, arises when data samples do not well-represent the associations within the possible world of classes. These blind spots are difficult to prevent when training samples exhibit spontaneous patterns as undesired properties of target classes and thus create spurious associations.

\textbf{Motivating example.} To illustrate the problem, consider a simple scenario of an image classification model that predicts whether each image contains cats or dogs, as illustrated in Figure \ref{fig:teaser}. Suppose this model was trained with a dataset where (1) important/core patterns are \textit{missing} (e.g., whiteness or blackness) and (2) periphery/background/undesirable concepts (e.g., a dog with grass background) are \textit{misleading} due to biases towards a certain class. When deploying the model, test cases with these spurious associations will cause systematic errors (e.g., white dogs misclassified as cats, black cats, or cats with grass backgrounds misclassified as dogs). In high-stakes applications such as medical diagnosis or predictive policing, this leads to potential harm to individuals whose attributes are missing from the training set or associated with arbitrary patterns \cite{crawford2016there}.

% \begin{figure}[!ht]
%     \centering
%     \includegraphics[width=0.95\columnwidth]{figures/problem.pdf}
%     \caption{\label{fig:problem}
%     \textbf{An illustration of spurious associations between three concepts and target classes which lead to the model's failure on the test set.}
%     }
% \end{figure}

Despite its consequences, such blind spots are difficult to detect, reason about, and prevent in practice. {While state-of-the-art techniques have proposed ways to detect blind spots \cite{coveragebasedutility, lakkaraju2017identifying} or underperforming subclasses and slices \cite{DominoDiscoveringSystematicErrorsCrossModal, SpotlightGeneralMethodDiscoveringSystematic}, they do not explain how misclassifications are \textit{attributed} to particular patterns, and thus provide little insight to overcome the blind spots. Recent XAI methods advancing concept-level interpretability \cite{AutomaticConceptbasedExplanations, InterpretabilityFeatureAttributionQuantitative, ConceptWhiteningInterpretable2020} also do not help make sense of an interpretation of those spurious associations (e.g., whether unexpected patterns were associated with certain classes and led to systematic failures). The standard machine learning pipeline lacks the ability to provide awareness and a deeper insight into the systematic errors or {\it biases} introduced by the training data and to recommend strategies to mitigate such biases. In practice, it is costly to explore and verify all possible patterns, many of which are noisy and not semantically meaningful using automated techniques. While recent visual analytics systems \cite{AutomaticConceptbasedExplanations, zhao2021humanintheloopextraction, kwon2022dash,  ConceptExplorerVisualAnalysisConcept} leverage the human ability to identify interpretable patterns, they do not support the inspection and interpretation of systematic errors or help mitigate spurious associations.

Given the profound impact of AI blindspots on a wide range of downstream applications, in this work, we aim to counter systematic errors by promoting an informed practice that reveals and reduces blind spots in the machine-learning pipeline. We propose a workflow to guide users in identifying, evaluating, and making sense of potential blind spots using analytical measures and visualization, as well as tools that help examine and reduce patterns associated with systematic errors. In the case of image classification, a {\it concept} is referred to as a higher-level feature representation that can be associated with a conceptual entity or a group of pixels in images that are semantically relevant to parts of an image (e.g., blackness, grass) \cite{AutomaticConceptbasedExplanations}. By allowing practitioners to see concept patterns associated with a target class in a biased manner, the workflow guides practitioners to generate hypotheses and evaluate strategies to overcome the machine's blind spots. Built on the recent discussion on different causes of systematic errors \cite{DominoExtractingComparingManipulating, SpotlightGeneralMethodDiscoveringSystematic}, our work further investigates two undesirable \textbf{spurious associations} arising from the model training process: the {\it missing} and {misleading} associations. As shown in Fig. \ref{fig:teaser}, missing associations refer to patterns that could have existed in a given class but are completely missed by the model; misleading associations refer to patterns that are disproportionately more likely to appear in an alternative class -- both contribute to the systematic errors the model produces. Furthermore, our workflow enables users to disentangle the model's systematic errors through an interactive analysis of two separate but interrelated explanation targets: 1) \textbf{association}: how is a hypothesized concept learned to be a pattern disproportionately associated with a particular class in the training process (i.e., are concepts biased towards a certain class?), and 2) \textbf{attribution}: how are misclassifications attributed to certain concepts? The spurious associations, once identified, can be evaluated to answer whether an association between a concept and a certain class can be mitigated by removing the source of bias. To facilitate such a workflow, we develop \name, a visual analytics system for countering \textbf{E}rrors from \textbf{S}purious \textbf{C}oncept \textbf{A}ssociations via interactive ins\textbf{PE}ction. Our system is designed to enable ML practitioners to make sense of spurious associations in countering systematic errors. The system incorporates a suite of visual components to help identify and hypothesize suspicious concept patterns as well as modules for validating the hypotheses and mitigating unwanted biases. To summarize, our contributions include:

\begin{itemize}
    \vspace{-0.5mm}
    \item \textbf{A prognostic workflow to guide the inspection of spurious associations.} We provide a range of tasks as a workflow to \textit{diagnose}, \textit{identify}, \textit{validate}, and \textit{mitigate} the associations between concepts and target classes. It further guides users to make sense of errors produced by spurious associations with recommendations to mitigate those errors.
    \item \textbf{\name, a visual analytic system for countering errors by integrating statistical and visual components to guide the inspection of spurious concept associations.} As a part of system design, we present two novel visualizations, \contrastiveview to help users capture the patterns that exclusively appear in certain classes by comparing confusion cases, and \conceptassociationplot which helps better make sense of two connected aspects of spurious associations.
    \item \textbf{A quantitative method to measure concept associations and debias spurious associations.} We propose a set of statistical metrics: 1) combined concept association: to quantify the degree of relative associations of concepts within each image, 2) between-class disparity: to measure the disparity of concept associations between concepts, and 3) debias method: to remove the spurious associations between a concept and a certain class.
    \item \textbf{Extensive evaluations.} We demonstrate the utility of our study through extensive evaluations including case studies, expert interviews, and controlled user experiments. Two usage scenarios showcase how diverse concepts can be detected in the image classification settings. We further evaluate how the design of our system allows users to effectively detect diverse concepts via controlled experiments.
    \vspace{-2mm}
\end{itemize}
