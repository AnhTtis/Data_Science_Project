\section{User Study}
\label{sec:user-study}

To evaluate the effectiveness and utility of \name, we conduct a controlled user study to compare ESCAPE with a baseline system. In the study, we set up the baseline system with a setting of visualizations and interactions consisting of two views: a confusion matrix visualization and a list of images with three information including its true and predicted class, and model confidence. While this is a workaround due to the inability to run interactive tools from relevant studies (i.e., tools are not available or requires auxiliary data), the baseline system represents a simple and standard setting that ML practitioners commonly use when debugging misclassifications in practice. The design of our controlled experiment also can be viewed as comparing two versions of our system, between the system version A only with \confusionmatrix and the version B with all functionality of \name, which allows us to test the utility of other key components in \name such as \instancespace, \contrastiveview, \conceptassociationplot, and \debiasview.  

\textbf{Study overview.} In the study, we recruited 22 participants (15 male and 7 female) via university email listserv. All participants had machine learning or statistical analysis experiences who are studying Information and Computer science or Statistics (Bachelor’s: 3, Master’s: 6, Doctorate: 13). Each participant was assigned to use either ESCAPE or baseline system for us to conduct a between-subject experiment. In an hour-long session, participants were given 15 minutes of introduction to the study and the walk-through of the system, and were tasked with thinking aloud to perform tasks and answer questions summarized in the below section. Finally, participants were asked to complete usability questions using a 5-point Likert scale. Participants conducted the study via either an online session using video conferencing or in-person session and were rewarded with \$15 upon the completion of the study.

\textbf{Questions and Tasks.} In the study, participants were asked to perform four tasks designed to test the utility of workflow (T1-T4) by answering Q1-Q8 in Cats\&Dogs image classification setting illustrated in Section \ref{sec:evaluation}: \\

\textbf{T1.} Diagnose: Inspect the degree of misclassifications (Q1-Q3) \\
\indent \textbf{T2.} Identify: Discover and hypothesize the associations between concepts and target classes (Q4) \\
\indent \textbf{T3.} Validate: Rank the given concepts in the order two aspects of spurious associations (Q5-Q6) \\
\indent \textbf{T4.} Mitigate: Determine a concept to debias and the number of concept-associated instances to mitigate the spurious association (Q7-Q8) \\

For T1 and T2 (Q1-Q4), we provided a system without any predefined concepts to let participants find out and hypothesize concepts based on their own observations. In the second half of the study (for T3 and T4, Q5-Q8), participants were given seven predefined concepts and are evaluated in terms of how the given system helped them capture the concept associations that caused misclassified cats.

\begin{table}[]
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Concept - Class} & \textbf{ESCAPE} & \textbf{Baseline} \\ \midrule
Cage - Cat               & 8/11 (72.7\%)   & 2/11 (18.2\%)     \\
Grass - Dog              & 7/11 (63.6\%)   & 1/11 (9\%)        \\
Person - Dog             & 6/11 (54.5\%)   & 4/11 (36.3\%)     \\
Red/Pink object - Cat    & 1/11 (9\%)      & 0/11 (0\%)        \\
Jean/Skyblue - Dog       & 0/11 (0\%)      & 1/11 (9\%)        \\ \bottomrule
\end{tabular}
\label{table:user-study-result-1}
\caption{\label{table:user-study-result-1} The count of identified associations between concept-class pairs by \name and baseline groups in the user study (T2).}
\vspace{-5pt}
\end{table}

\textbf{Results.} The result in Table \ref{table:user-study-result-1} shows that participants in ESCAPE successfully performed T1-T3 better than baseline users. In Diagnosis task (T1), \name users accurately diagnosed the degree of misclassifications at instance (11/11, 100\%) and class level (11/11, 100\%) compared to baseline users (5/11 (45\%), 1/11 (9\%) for each). In the task of identifying the associations (T2), more portion of \name users accurately detected the association of three concepts (cage, person, and grass) associations with a class (Table \ref{table:user-study-result-1}). On the other hand, two other concepts were identified by only a few participants from both groups. This may be due to less noticeable patterns of two concepts appearing in the test set as these concepts were the least biased towards certain class (details in Table \ref{table:experiment-result-1}). In addition to these five concepts, both groups explored a variety of concepts in their task. These concepts included 26 unique concepts across diverse types such as background (inside, shelter), appearance (triangle ear, tongue), color, size and pose (small, facing front) or combination of features (black furs without person). Lastly, according to the result of subjective ratings in a 5-point Likert scale to test the usefulness, interactiveness, and usability of two systems and the one-tailed two sample t-test, participants found \name was more useful (\name: $\mu$ = 4.91, $\sigma$ = 0.3, baseline: $\mu$ = 3.8, $\sigma$ = 0.94, $p$=0.001) and interactive (\name: $\mu$ = 4.33, $\sigma$ = 0.82, baseline: $\mu$ = 3.7, $\sigma$ = 1.21, $p$=0.039) than baseline system. However, \name was evaluated as not easier to use than baseline due to its higher complexity with various components although the difference was not statistically significant (\name: $\mu$ = 4.08, $\sigma$ = 0.89, baseline: $\mu$ = 4.4, $\sigma$ = 0.52, $p$=0.081).

\textbf{Behavioral patterns.} In addition to study results, we analyze the behavioral patterns of participants in inspecting systematic errors while using two systems. Three findings highlight how the design of \name promotes a careful inspection of systematic errors and mitigation of biases. \\

\textbf{Finding 1. \name users exhibited an increased awareness of the concept of model confidence.} During the study, four users reported their preferences to investigate unknown-unknowns, ``\textit{I want to look at high confidence error first,}'' ``\textit{I wasn’t aware of how errors can be categorized by model confidence but I can see how it is critical.}'' These four users commonly investigated instances in the order of unknown-unknowns, known-unknowns, and true predictions by adjusting the confidence score filter. They also mentioned their increasing awareness of unknown-unknowns and the importance of inspecting model confidence during the study.

\textbf{Finding 2. Practitioners tend to contrast predictive cases to find the patterns of misclassifications.} In T2, we could observe that 8 of 11 baseline users actively compared at least one or two pairs of confusion cases. Most of them noticeably paid attention to comparing FP with counterfactual cases whose pairs are both true cats (TN vs. FP: 6/11 (54.5\%) users) or ones predicted as dogs (TP vs. FP: 5/11 (45.5\%) users) that are minimally different. While this proves how ML practitioners prefer to compare different cases to find out patterns, it also gave the rationale behind the design of our \contrastiveview that incorporated the way practitioners by nature analyze the pattern.

\textbf{Finding 3. \name users tend to have different decision criteria for determining how to best debias.} In T4, we noticed that participants determined the number of instances to debias with different decision criteria. While most of them (8/11, 45\%) chose the optimal point (i.e., elbow point), others preferred to choose the threshold that maximally removes the biases (2/11, 18\%) or to debias the maximum number of instances (1/11, 9\%). This showed the debias tolerance slider in \debiasplot to be effective in incorporating their preference and recommending the optimal point.
