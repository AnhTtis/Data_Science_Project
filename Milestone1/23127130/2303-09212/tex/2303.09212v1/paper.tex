% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage{makecell}
\usepackage{cite}
\usepackage{amssymb}
\usepackage{utfsym}
\usepackage{ragged2e}
\usepackage{amsmath}
\usepackage{booktabs}
% included packages
\usepackage[hidelinks,colorlinks=true,linkcolor=blue,citecolor=blue]{hyperref}
\usepackage{makecell}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{float}
\usepackage{authblk}

\begin{document}

%
\title{GDDS: Pulmonary Bronchioles Segmentation with Group Deep Dense Supervision }
%\title{GDDS : A Simple-Yet-Effective Supervision Towards Fine-scale Bronchial Segmentation}%\thanks{Supported by organization x.}}
%
\titlerunning{GDDS}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
% #\author{Paper ID:1937}
\author{Mingyue Zhao\inst{1} \and
Shang Zhao\inst{1}\and
Quan Quan\inst{2}\and
Li Fan\inst{3}\and
Xiaolan Qiu\inst{4}\and
Shiyuan Liu\inst{3}\and
S.Kevin Zhou\inst{1,2}}

% \emailrunning{mingyuezhao@mail.ustc.edu.cn}
% #\authorrunning{Paper ID:1937}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Center for Medical Imaging, Robotics, Analytical Computing \& Learning (MIRACLE), School of Biomedical Engineering, University of Science and Technology of China, Suzhou 215123, China \and
Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China\and
Department of Radiology, Second Affiliated Hospital, Naval Medical University, Shanghai  200003, China \and
Suzhou Key Laboratory of Microwave Imaging, Processing and Application Technology
Suzhou Aerospace Information Research Institute}
% \email{mingyuezhao@mail.ustc.edu.cn}\\
% \and
% Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
% \email{lncs@springer.com}\\
% \url{http://www.springer.com/gp/computer-science/lncs} \and
% ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
% \email{\{abc,lncs\}@uni-heidelberg.de}}
%

\maketitle              % typeset the header of the contribution
%

\begin{abstract}
Airway segmentation, especially bronchioles segmentation, is an important but challenging task because distal bronchus are sparsely distributed and of a fine scale. Existing neural networks usually exploit sparse topology to learn the connectivity of bronchioles and inefficient shallow features to capture such high-frequency information, leading to the breakage or missed detection of individual thin branches. To address these problems, we contribute a new bronchial segmentation method based on Group Deep Dense Supervision (GDDS) that emphasizes fine-scale bronchioles segmentation in a simple-but-effective manner. First, Deep Dense Supervision (DDS) is proposed by constructing local dense topology skillfully and implementing dense topological learning on a specific shallow feature layer. %Furthermore, the foreground-specific consistency constraint between the deep dense head and the segmentation head encourages the network to pay more attention to fine structures. 
 % DDS is coupled with group supervision to form GDDS. The strengthening of the shallow features further empowers the network to capture even more small airways that are not easily discernible to naked eyes. 
 GDDS further empowers the shallow features with better perception ability to detect bronchioles, even the ones that are not easily discernible to the naked eye. Extensive experiments on the BAS benchmark dataset have shown that our method promotes the network to have a high sensitivity in capturing fine-scale branches and outperforms state-of-the-art methods by a large margin (+12.8\% in BD and +8.8\% in TD) while only introducing a small number of extra parameters.
\keywords{Bronchioles segmentation \and Group deep dense supervision \and Dense topological learning.}
\end{abstract}
%
%
%
\section{Introduction}
Airway segmentation plays an increasingly vital role in the diagnostic and interventional procedures of many lung diseases. Notably, quantitative CT-based bronchi morphological parameters, such as airway lumen diameter, wall thickness and branching patterns, are important disease phenotypes for further understanding of disease progression and therapeutic interventions in chronic obstructive pulmonary disease (COPD)\cite{lancet_COPD_2022} and asthma. Nevertheless, the small size and blurred airway walls of peripheral bronchi make the manual depiction of airway trees time-consuming, error-prone, and overtly subjective. 

% Recently, deep learning-based methods have been widely used in automatic airway segmentation~\cite{qin_airwaynet-se_2020,wang_naviairway_2022,Qin_Bronchiole-Sensitive_2020,wu_two-stage_airwaysegmentation_2022,zhang_differentiable_2022,yu_breakBronchiReconstruction_2022}.  Furthermore, efforts are made to model a better-connected airway tree by combining the advantages of traditional methods on the basis of CNN. Zhang \emph{et al.} \cite{zhang_differentiable_2022} proposed a Differentiable Topology-Preserved Distance Transform framework to improve the topology completeness of the segmentation.To improve the sensitivity of the network to peripheral bronchi, network architectures are optimized or scale-aware loss functions are introduced. Qin \emph{et al.} \cite{qin_Tubule-Sensitive_learning_2021} proposed feature recalibration and decoder-side attention distillation to detect more peripheral bronchi. Zheng \emph{et al.} \cite{Zheng_Refined_Local_2021} proposed a local-imbalance-based weight and BP-based weight enhancement strategy to improve intra-class imbalance. 

% However, there are still limitations to be overcome, especially for bronchioles segmentation, which can be summarized as follows: 1) \textbf{Sparse connectivity supervision}. The CNN-based network does not explicitly perceive the connectivity of the distal bronchi, leading to the breakage of individual branches~\cite{joint_unet_graph,born_three-step_nodate}. Although some topology-preserving strategies~\cite{shit2021cldice,yu_breakBronchiReconstruction_2022} were proposed to improve this problem, the connectivity supervision is still too sparse to find more bronchioles. 2) \textbf{Inadequate shallow feature learning}. Conventional segmentation networks struggle in capturing high-frequency information to cope with such a scattered and sparse distribution of fine bronchioles~\cite{joint_unet_graph,qin_airwaynet-se_2020}. Although existing approaches have attempted to address this problem by introducing scale-aware loss functions to pay more attention to thin airways, shallow feature learning for fine-scale bronchi is still inadequate~\cite{Zheng_Refined_Local_2021,wang_naviairway_2022}. 
Recently, deep learning methods have been widely used in this task~\cite{qin_airwaynet-se_2020,wang_naviairway_2022,Qin_Bronchiole-Sensitive_2020,wu_two-stage_airwaysegmentation_2022,zhang_differentiable_2022,yu_breakBronchiReconstruction_2022}. However, the segmentation of bronchioles remains very challenging for convolutional neural networks (CNNs) due to limited receptive field or shallow features with insufficient attention to detail, showing that individual branches are broken or cannot be detected. On one hand, from the connectivity of detecting bronchioles perspective, some existing methods attempt to introduce topology-preserving strategies~\cite{wang_pointscatter_2022,shit2021cldice,zhang_differentiable_2022,hu2022homotopy_warping,zhang2022progressive} to improve the connectivity of tubular structures like bronchioles,
% Wang \emph{et al.}~\cite{wang_pointscatter_2022} employed ``scatter regions" to learn the centerline of the tubular structure on a decoder-side shallow layer
such as centerline-based guidance~\cite{shit2021cldice,zhang2022progressive,wang_pointscatter_2022}, homotopy warping-based loss~\cite{hu2022homotopy_warping}, etc. We categorize these topology-preserving strategies as sparse topology learning, which usually emphasizes the supervision of individual topology-related key voxel points.
Whereas \emph{sparse topology learning has insufficient power to improve the connectivity of bronchioles segmentation}.
% Whereas these loss functions usually emphasize the supervision of individual voxels, and this \emph{sparse topology learning is not sufficient to improve the connectivity of bronchioles}.
On the other hand, from the sensitivity of detecting bronchioles perspective, some researchers dedicate to improving the sensitivity by optimizing network architectures or introducing new scale-aware loss functions. Qin \emph{et al.} \cite{qin_Tubule-Sensitive_learning_2021} propose feature recalibration and attention distillation to pay more attention to fine structures. Zheng \emph{et al.} \cite{Zheng_Refined_Local_2021} propose a local-imbalance-based weight and BP-based weight enhancement strategy to improve intra-class imbalance. Although these methods have achieved a certain improvement in the detection of bronchioles, \emph{the shallow features of the network are not employed adequately, which really matter to cope with such a scattered and sparse distribution of bronchioles}.
% Zhang \emph{et al.} \cite{zhang_differentiable_2022} proposed a Differentiable Topology-Preserved Distance Transform framework to improve the topology completeness of the segmentation.
% 2) The airway segmentation suffers significantly from intra-class multi-scale imbalance. The network is likely to fall into local gradient erosion or gradient expansion when learning for different scales of airways. We observe that the network's performance for bronchial segmentation in the training set is often unsatisfactory. It indicates that in full-scale training mode, the network is often inadequate for fine-scale bronchi learning.

To tackle these two concerns above, we propose a high bronchiole-sensitive segmentation method based on Group Deep Dense Supervision. The contributions of this paper are as follows. 1) To achieve dense topological learning, we propose a simple-but-effective supervision approach, named \textbf{Deep Dense Supervision (DDS)}. We exploit voxel-wise airway annotation to construct local dense topology skillfully and implement dense topological learning in a manner similar to deep supervision. This is the first attempt of dense topology learning on tubular structure segmentation. 2) To strengthen the bronchiole-sensitive feature learning in shallow layers, we combine the DDS with group supervision, forming \textbf{Group Deep Dense Supervision (GDDS)}. GDDS simultaneously improves the sensing power of peripheral bronchial voxels and optimizes the overall connectivity of the airway tree with a few extra parameters. 3) Extensive experiments and standardized metric evaluation comparisons reveal the superiority of our method in bronchioles extraction.
\begin{figure}[htb]
\includegraphics[width=0.95\textwidth]{figure/GDDS.drawio.png}
\caption{Overview of our proposed method for pulmonary airway segmentation.} \label{deep_dense_supervision}
\end{figure}

\section{Methodology} \label{sec:method}
% To focus on the fine-scale segmentation, we first divide the airway into two parts with a novel metric of generation as demonstrated in Fig~\ref{}: Fine-scale parts which are 4th generation; coarse-scale parts which are bigger than 4th. 
% due to the property of mesasuring more than 3mm in diameter and the remaining bronchioles are usally smaller in size, even less than 1mm 
Given the tree structure of the airway, we present a generation-aware training paradigm for overall airway segmentation, as shown in Fig.~\ref{deep_dense_supervision}. 
Referring to \cite{tu_human_2013}, trachea and bronchi lower than $4^{th}$ generation usually measure more than 3mm in diameter and the remaining bronchioles are usually smaller in size, even less than 1mm. In the generation-aware training paradigm, we divide the full label into two parts: i) trachea and bronchi lower than $4^{th}$ generation; ii) the remaining bronchioles.
Subsequently, we perform separate parallel training. The final full-scale airway segmentation result is the combination of the outputs of these two models. Specifically, UNet\cite{3dunet} is exploited to segment the trachea and bronchi. In this paper, we will focus on precise bronchioles segmentation, which will be illustrated in detail. 
% 
% DDS enables the model to sensitively perceive topological changes and connectivity of airways by augmenting the shallow feature layers.




% \subsection{Generation-aware Training}
% \par The airway tree modeling can be formulated as $p = F(x)$, where $p$ denotes the predicted probability of airway at each voxel of the input volume $x$. $y$ is the corresponding label of the airway. Refering to \cite{tu_human_2013}, trachea and bronchi lower than $4^{th}$ generation usually measure more than 3 mm in diameter and the remaining bronchioles usually have a smaller size, even less than 1mm in size. Thus, in generation-aware training manner, we split the full label into two parts according to the generations of airway branchesï¼Œ which serve as supervisory signals for each of the two models. Given that low-generation branches have been well delineated by many traditional algorithms, U-Net is exploited as the model architecture for the segmentation of the trachea and bronchi. In this paper, we will focus on discussing the precise segmentation of high-generation bronchioles. The final full-scale airway segmentation result is obtained by taking the largest connected component after the combination of the outputs of two models.

\subsection{Deep Dense Supervision}
% \subsection{Method Details}
% The overview of GDDS is illustrated in Fig.\ref{deep_dense_supervision}.
%\subsubsection{Dense topological learning} 
\noindent{\textbf{Dense topological learning.}}
The shallow feature learning of the network is important for the detection of fine structures such as bronchioles. Deep supervision \cite{lee2015deeply} enhances the representation of the low-level features by imposing additional label supervision on different shallow layers. But it may damage hierarchical representation due to the unbiased emphasis on shallow features. This promotes us to ponder: \emph{for the bronchioles segmentation task, which level of shallow layers is more critical? How to design more appropriate label supervision to guide the learning of shallow features?} 
To address the above concerns, we perform dense topological learning on a specific shallow feature layer.
% Inspired by~\cite{wang_pointscatter_2022}, the vectors at each spatial position of the down-sampled feature map have a better awareness of local topology. It improves the topological consistency of the segmentation layer while optimizing shallow feature learning. , we extend sparse topological supervision for skeleton extraction to dense topological learning and propose a deep dense supervision strategy.

% Generally speaking, the shallow feature learning of the network is very important for the detection of fine structures such as bronchioles. Based on the above dicussion, two crucial issues are 
% \emph{1) which levels of shallow feature representation are really beneficial for airway segmentation tasks, and 2) how can supervision signals be designed to better cater to the task characteristics.}
% In this section, we first detail two key modules: (1) Deep dense supervision; and (2) Group deep dense 

Given an input image $x$ and corresponding airway label $y$ shaped with  $H \times W \times L$. Here we feed the image into the network and obtain the corresponding decoder-side $n \times$down-sampled feature map $f \in \mathbb{R} ^{c\times h \times w \times l}$, where $h$, $w$, $l$ and $c$ indicate the height, width, length and channel dimension, respectively. We have $h = H/n, w = W/n, l =L/n$. Given any point $j$ on the spatial position of the feature map $f$, its feature vector is $\mathbf{\upsilon}_j\in \mathbb{R}^{c\times 1}$. $x^j$ denotes the corresponding  $n \times n\times n$-sized region in the image $x$. Inspired by~\cite{wang_pointscatter_2022}, we argue that the vectors at each spatial position of the down-sampled feature map have a better awareness of local topology. Thus, our goal is to find an effective supervision scheme to enable $\mathbf{\upsilon}_j$ to fully learn the dense topological shape of the bronchioles in area $x^j$.

To achieve this, we first design a dense topological supervision signal on feature map $f$. Specifically, we transform the label $y$ into $y_{n} \in \mathbb{R} ^{n^3 \times h \times w\times l} $ by expanding each cube of size $n \times n \times n$ in $y$ by channel with a stride of $n$, as illustrated in Fig.~\ref{deep_dense_supervision}. Here we define the $n$-fold dense topological transformation on $\kappa$ as $DTT(\kappa,n)$, then we have $y_{n} = DTT(y,n)$.

In addition, a deep dense head following the feature map $f$ is employed to develop the supervision with the local dense topology $y_{n}$. It consists of two $1\times1 \times1 $ convolution blocks and a sigmoid function. The output channel dimension of the deep dense head is $n^3$. Thus, the objective function of the deep dense head is defined as:
\begin{equation}
    \mathcal{L} _{\phi } = FL ( \hat{p}_n ,DTT(y,n)),
\end{equation}
where $\hat{p}_n$ is the predicted probability map of the deep dense head, and $FL$ is the Focal loss~\cite{lin2017focal}.

\noindent{\textbf{Foreground-emphasized consistency constraint.}}
% \subsubsection{Foreground-specific consistency constraint} 
To gain insight into the role of DDS, we perform inverse $DTT$ on the output probability map of the deep dense head, $i.e.$ $\hat{p} = DTT^{-1}(\hat{p}_n,n)$. As shown in Fig.~\ref{activation}, compared with the segmentation head, the deep dense head has a higher response in small airways and low-contrast lumen regions. It enhances the representation of shallow features in fine structure detection and dense topology perception. A comparison of Fig.~\ref{activation}(a) and Fig.~\ref{activation}(b) shows that the addition of DDS boosts the sensitivity of the segmentation layer to these hard regions. In view of this, we further introduce a foreground-emphasized consistency constraint between the output of the deep dense head and the segmentation head, which is formulated as :
\begin{equation}
    \mathcal{L} _{\xi } = FL (y_n*\hat{p}_n*DTT(p,n)+(1-y_n)*((\hat{p}_n+DTT(p,n))/2),y_n),
\end{equation}
where $p$ denotes the predicted probability map of the segmentation head, which is also the output of the whole model. Note that we perform consistency enhancement of the foreground region under the supervision of $y_n$ instead of aligning the outputs of the two heads directly for consistency. In this manner, $\hat{p}_n(i)*DTT(p,n)(i)$ appears as a pseudo-confidence of $i^{th}$ airway voxel under the down-sampled dimension and the loss function $\mathcal{L} _{\xi }$ would penalize heavily on $i$ if it has moderate confidence in any of the prediction map of $\hat{p}_n$ or ${p}_n = DTT(p,n)$.
\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{figure/activation.png}
\caption{Comparsion of probability maps of the segmentation head before DDS (a), the segmentation head after DDS (b) and the deep dense head (c).} \label{activation}
\end{figure}

\subsection{Group Deep Dense Supervision}
On top of DDS, group supervision (GS) is introduced, forming GDDS, to further develop direct supervision enhancement at all levels of the shallow feature layers. 
As illustrated in Fig. \ref{deep_dense_supervision}, feature maps in each convolutional layer in the encoder are upsampled to the input size. Then we form encoder group supervision by concatenating those features. The same goes for the decoding stage. DiceFocal loss is imposed on both stages: 
\begin{equation}
\mathcal{L} _{\zeta } = -(\frac{2 {\textstyle \sum_{i\in x}^{}p(i)y(i)} }{\sum_{i\in x}^{}(p(i)+y(i))+\varepsilon}
+ \frac{1}{\left | x \right |} \sum_{i\in x}^{}(1-p^\prime(i))^2log(p^\prime(i))), 
\end{equation}
where $p^{\prime}(i) = p(i)$ if $y(i) = 1$. Otherwise, $ p^\prime(i) = 1 - p(i)$. $\varepsilon$ is a parameter used to avoid division by zero.
Then, we have the total loss function as follows:
\begin{equation}
\mathcal{L} =\mathcal{L}_{\zeta }^{en}+\mathcal{L}_{\zeta }^{de}+\alpha *\mathcal{L}_{\phi }+ \beta* \mathcal{L}_{\xi },
\end{equation}
where $\mathcal{L}_{\zeta }^{en}$ and $\mathcal{L}_{\zeta }^{de}$ are group supervision loss of encoder and decoder-side respectively. $\alpha$ and $\beta$ are weighting factors of DDS loss and consistency loss. During the inference stage, we only treat the decoder-side GS as the segmentation head, $i.e.$ the output of decoder-side GS is the final segmentation result.


%\subsection{Uncertainty-based Sampling}

\section{Experiment and Results}
\noindent{\textbf{Datasets.}}
% \subsubsection{Datasets}
%Our experiments were conducted on three datasets, including %two public datasets and one private dataset:
We conduct our experiments on the Binary Airway Segmentation (BAS) Dataset \cite{qin_airwaynet-se_2020}. It consists of 90 CT scans from two public datasets (20 cases from EXACT'09 and 70 cases from LIDC-IDRI). The pixel spacing ranges from 0.5 to 0.82 mm, and the slice thickness ranges from 0.5 to 1.0 mm. In our experiments, 5-fold cross-validation is conducted, meaning that there are 72 cases for training and 18 cases for testing.

Accurate centerline extraction and branch-level airway labeling are indispensable for model training and evaluation. 
% we observe that previous work\cite{Qin_Bronchiole-Sensitive_2020,qin_airwaynet-se_2020,wang_naviairway_2022} has typically used some skeleton extraction algorithms to obtain the corresponding centerline directly from the voxel-wise airway annotation, and then parse the centerline to obtain the branch labeling results. 
However, we observe that i) Existing airway parsing methods~\cite{Qin_Bronchiole-Sensitive_2020,qin_airwaynet-se_2020,wang_naviairway_2022} may fail in some cases due to the complex airway morphology. ii) Different parsing methods lead to inconsistent assessment for airway segmentation. Therefore, we implement a more standardized airway parsing. Specifically, the centerlines are first extracted using MIMICs software and then \textit{manually corrected by a panel of well-trained experts}. Branch-level labeling results are obtained according to the parent-child relationship between branch centerlines. Fig. \ref{airway_parsing} shows the comparison between our method and~\cite{qin_Tubule-Sensitive_learning_2021}. Affected by airway morphology and surface smoothness,~\cite{qin_Tubule-Sensitive_learning_2021} produces redundant centerlines in centerline extraction, especially for lower airway branches. This further leads to failures in branch-level parsing, manifested by a branch being split into multiple parts or bifurcation errors between branches. The centerlines and branch labelings obtained by us are more consistent with the actual airway structure and also have great potential for centerline extraction and semantic segmentation tasks. We will \textit{made it public to benefit the community}.
\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figure/parsing.pdf}
\caption{Visualisation of airway parsing results. Centerline extraction results of (b)\cite{qin_Tubule-Sensitive_learning_2021} and (c) ours. Branch-level labeling results of (d)\cite{qin_Tubule-Sensitive_learning_2021} and (e) ours. Each color represents a branch. (f) shows the generations of the airway and each color represents a generation.} \label{airway_parsing}
\end{figure}

% \paragraph{Airway Tree Modeling 2022 (ATM2022) dataset}, collected from the public LIDC-IDRI dataset and the Shanghai Chest hospital, contains 500 CT scans(300 cases for train, 50 cases for external validation and 150 cases for testing). Removing one scan that was incorrectly labeled, we employed the remaining 299 scans for model training.
\noindent{\textbf{Implementation Details.}}
%\subsubsection{Implementation Details}
%preprocessing
The preprocessing includes image value truncation, normalization and DL-based lung extraction~\cite{lungmask_2020}. We apply a sliding window to sample patches from each CT image with the size of $144 \times 144 \times 144$ in low-generation airway segmentation and $80 \times 80 \times 80$ in bronchioles segmentation. On-the-fly data augmentation includes random contrast adjustment, random axis flipping, and random rotation between (-$15^{\circ }, 15^{\circ }$). Our models are trained by Adam optimizer with an initial learning rate of 0.03. The learning rate is divided by 10 in the $20^{th}$, $40^{th}$, and $60^{th}$ epoch. All the experiments are trained until convergence. The testing results are thresholded by 0.5 for binarization. Experimentally, we set $\alpha = \beta = 0.8$ by default. 

\noindent{\textbf{Evaluation Metrics.}}
%\subsubsection{Evaluation Metrics}
We adopt volumetric-based and topology-based metrics for evaluation, including Branches Detected (BD), Tree-length Detected (TD), True Positive Rate (TPR), and False Positive Rate (FPR). The definitions can refer to~\cite{qin_Tubule-Sensitive_learning_2021}. Note that: i) Different from  \cite{qin_Tubule-Sensitive_learning_2021}, a branch is considered detected only when 80\% branch voxels are correctly classified; ii) Only the largest component of segmentation results are evaluated on these metrics.

\noindent{\textbf{Quantitative Results.}}
%\subsubsection{Quantitative Results}
With the backbone of UNet~\cite{3dunet} and WingsNet~\cite{zheng_alleviating_2021}, we compare our method with two classic segmentation networks and four state-of-the-art methods, which are listed in the first column of Table \ref{tab:table8}. All comparison methods except \cite{joint_unet_graph} are implemented according to their official codes and trained from scratch. The values of Juarez \emph{et al}.~\cite{joint_unet_graph} are the reported results in \cite{qin_airwaynet-se_2020}. BD$^{*}$ and TD$^{*}$ measure the branch detected and tree length detected in fine-scale, \emph{i.e.} bronchioles no lower than $4^{th}$ generation in the annotation.

\input{Table/SOTA}
\input{Table/Ablation}
\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{figure/results.pdf}
\caption{Visualisation of segmentation results. (a) is a hard case and (b) is a mild case in the test set of BAS. To emphasize the performance of the proposed method for bronchioles, only TP and FN at fine-scale are shown.} \label{result}
\end{figure}

Results in Table \ref{tab:table8} demonstrate that on the two backbones, GDDS has achieved consistent and substantial improvements. In particular, compared with other methods, GDDS with WingsNet achieves the best performance on BD (90.5\%), BD$^{*}$ (90.2\%), TD (95.8\%), TD$^{*}$(95.4\%), and TPR(98.4\%). This reveals that our proposed method has a better perception of fine structures. We analyze the main causes of false positives generated by algorithms: i) GDDS can detect more real thin branches than doctor annotations; and ii) gradient erosion occurs at some boundaries of the fuzzy lumen and wall of the tube, leading to over-segmentation. 
For reason i), referring to~\cite{wang_naviairway_2022}, we randomly select a $1$-fold test set and calculate the branch ratio (BR), which denotes the number of branches in the model segmentation over the number of branches in the reference segmentation. The average results after review by three experts show that our algorithm can achieve 116.3\% BR. In fact, small airways concentrate more attention from experts in diagnosis. This is because the small airways are frequently involved early in the course of lung disease~\cite{small_airway}.
% place more emphasis on the importance of the small airways, which often indicate airway disease. 
For example, in COPD, a reduction in the number of small airways and remodeling as important phenotypes allow doctors to intervene early before irreversible pathological changes occur in the lungs~\cite{small_airway_COPD}. Hence, our work has shown some levels of clinical value in assisting doctors to rapidly locates these small airways.

To demonstrate the effectiveness of generation-aware training and GDDS, several comparative studies are carried out in Table \ref{tab:ablation}. Firstly, the significant improvement of results delivered by the generation-aware training confirms that the detection of thin branches requires specialized training strategies. Besides, we compare the effect of different downsample rates $n$ in DDS. $n = 2$ shows the best performance, which outperforms DS with relatively considerable margins (+5.5\% in BD, +6.9\% in BD${^*}$, +3.6\% in TD, and +5.3\% in TD${^*}$). This shows that dense topological supervision strategies based on the specific shallow feature are more effective both in the improvement of connectivity and sensitivity. Furthermore, the addition of group supervision allows GDDS to gain superior performance. It is noteworthy that GDDS significantly improves the sensitivity to small airways with only 0.02\% additional parameters and without any post-processing operations.

\noindent{\textbf{Qualitative Results.}}
%\subsubsection{Qualitative Results}
Fig.~\ref{result} gives an intuitive performance comparison for the above methods on a hard case and a mild case. Compared with other methods, our algorithms can effectively identify more bronchioles, surpassing others by a large margin.  

\section{Conclusion}
Toward bronchioles segmentation, we propose a simple-but-effective supervision pattern - GDDS. We effectively enhance dense topological perception by performing deep dense supervision. Coupling with group supervision, GDDS further improves the sensing power of bronchioles and optimizes the overall connectivity of the airway tree. Extensive experiments show that our method gains considerable improvement in the detection of bronchioles  with a few extra parameters. Future works include optimizing the boundary segmentation between the airway lumen and the wall, designing a better network for more effective segmentation, and employing airway segmentation for COPD early diagnosis.

























%
% the environments 'definition', 'lemma', 'proposition', 'corollary',
% 'remark', and 'example' are defined in the LLNCS documentclass as well.
%

% For citations of references, we prefer the use of square brackets
% and consecutive numbers. Citations using labels or the author/year
% convention are also acceptable. The following bibliography provides
% a sample reference list with entries for journal
% articles~\cite{ref_article1}, an LNCS chapter~\cite{ref_lncs1}, a
% book~\cite{ref_book1}, proceedings without editors~\cite{ref_proc1},
% and a homepage~\cite{ref_url1}. Multiple citations are grouped
% \cite{ref_article1,ref_lncs1,ref_book1},
% \cite{ref_article1,ref_book1,ref_proc1,ref_url1}.
%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{ref}
%
% \begin{thebibliography}{8}
% \bibitem{}
% Author, F.: Article title. Journal \textbf{2}(5), 99--110 (2016)

% \bibitem{ref_lncs1}
% Author, F., Author, S.: Title of a proceedings paper. In: Editor,
% F., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.
% Springer, Heidelberg (2016). \doi{10.10007/1234567890}

% \bibitem{ref_book1}
% Author, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,
% Location (1999)

% \bibitem{ref_proc1}
% Author, A.-B.: Contribution title. In: 9th International Proceedings
% on Proceedings, pp. 1--2. Publisher, Location (2010)

% \bibitem{ref_url1}
% LNCS Homepage, \url{http://www.springer.com/lncs}. Last accessed 4
% Oct 2017
% \end{thebibliography}
\end{document}
