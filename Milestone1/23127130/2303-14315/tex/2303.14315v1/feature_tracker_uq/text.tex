\begin{abstract}
In state estimation algorithms that use feature tracks as input, it is customary to assume that the errors in feature track positions are zero-mean Gaussian. Using a combination of calibrated camera intrinsics, ground-truth camera pose, and depth images, it is possible to compute ground-truth positions for feature tracks extracted using an image processing algorithm. We find that feature track errors are not zero-mean Gaussian and that the distribution of errors is conditional on the type of motion, the speed of motion, and the image processing algorithm used to extract the tracks.
\end{abstract}


\section{Introduction}

Many state estimation algorithms assume that measurements are zero-mean Gaussian. This is an explicit assumption in the Kalman Filter and its nonlinear variants \cite{thrun_probabilistic_2005, barrau_invariant_2018} and implicitly built-into the optimization problem of bundle adjustment algorithms \cite{mur-artal_orb-slam:_2015} and outlier-rejection algorithms \cite{civera_1-point_2009}. With extensive calibration with respect to temperature and mechanical alignment, the zero-mean Gaussian assumption is sufficient for the measurements of sensors such as inertial measurement units (IMUs) \cite{vectornav_imu_calibration, tedaldi_robust_2014}, even if it is still not completely true: Extended Kalman Filters (EKFs) that rely on these IMUs are deployed on safety-critical systems actively in use.

Even though several well-known algorithms for Simultaneous Localization and Mapping (SLAM) rely on the often-deployed EKF (e.g. \cite{jones_visual-inertial_2011,Geneva2020ICRA,bloesch_iterated_2017}), SLAM is still an active area of research. The existence of recently-released and actively used research benchmark datasets \cite{hilti_benchmark, tartanair2020iros} indicate that the robotics and computer vision communities still believe that performance of SLAM and an understanding of its failure cases are still insufficient, even after three decades of development \cite{early_slam_tutorial}. This motivates an examination into the fundamental assumptions of SLAM.

This manuscript visits the assumption that feature tracks, the ``measurements" of any indirect visual SLAM algorithm, contain only zero-mean Gaussian error. The covariance of the feature tracks is typically a tuning parameter to for all features at all times. We show that the feature track errors are not zero-mean Gaussian and furthermore, that the errors are conditional on the type of motion, the speed of motion, and the type of feature tracker used to extract the feature tracks. To our knowledge, this is the first study of the mean and covariance of feature tracks \emph{conditional} on the factors that affect them.

The organization of the paper is as follows. Section \ref{sec:feature_track_uq} details the methods. Section \ref{sec:feature_tracker_experiment_details} presents some key figures, and summarizes the error distribution of feature trackers. Section \ref{sec:discussion} ends with some concluding remarks. Additional figures from the experiment are given in the Appendix.



\subsection{Related Work}

\paragraph{Performance of feature detectors and descriptors conditional on nuisances.} The main metric used to evaluate feature detectors is \emph{repeatability} \cite{mikolajczyk_comparison_2005}, or the probability that a feature detector will detect the same feature across multiple images of the same scene under different illuminations and viewpoints. Other metrics are \emph{entropy} \cite{heinly_comparative_2012}, the spread of detected features over an image, and \emph{recall} \cite{aanaes_interesting_2012}, the number of features that are likely ``matchable'' to features in another image of the same scene. On the other hand, the primary metrics used to evaluate feature descriptors are \emph{precision} and \emph{recall}, calculated using pairs of ``matches'' that are found using the descriptor \cite{mikolajczyk_performance_2005}. The evaluation of feature detectors requires multiple images of the same scene. The evaluation of descriptors originally used the same datasets as the evaluation of detectors. To disentangle the problem of detecting features from the evaluation of feature description, two comprehensive datasets of image patches was released in 2017 \cite{balntas_hpatches_2017, maier_ground_2017}. At around the same time, \cite{schonberger_comparative_2017} evaluated both learned and handcrafted feature detectors and descriptors. Of most interest to us are \cite{heinly_comparative_2012}, which used a small dataset containing pure rotation, pure scaling, and illumination changes to evaluate the performance of various detector/descriptor combinations condition on each, \cite{zhao_image_2020}, which extended the datasets used in \cite{heinly_comparative_2012}, and \cite{aanaes_interesting_2012}, which evaluated the performance of feature detectors conditional on change in view angle and lighting condition. Tangentially interesting are \cite{hauagge_image_2012}, which released a dataset of image pairs that are geometrically consistent, but contain large changes in style (e.g. summer vs. winter) and lighting; and \cite{sattler_benchmarking_2018}, which contains groups of image sequences with similar motions, but large outdoor illumination changes.


\paragraph{Learning or Fitting a Covariance Matrix to Feature Tracks.} Early works sought to compute covariance of feature location using information in the RGB image. \cite{kanazawa_we_2001} approximated the covariance with the Hessian of the image centered at the feature point was the covariance of a detected feature -- the idea is that the sharper the curvature given by the Hessian, the more likely a convolutional filter will find the correct location of the feature. \cite{nickels_estimating_2002} contains a sum-of-squared-differences formula for computing feature track covariance. \cite{zeisl_estimation_2009} contains a formula for computing the covariance matrix of SIFT and SURF features. Later on, \cite{sheorey_uncertainty_2015} and \cite{wong_uncertainty_2017} present two methods to model the mean and covariance of Lucas-Kanade feature tracks. With the exception  of \cite{sheorey_uncertainty_2015}, which assumes that the location of a feature track could be a Gaussian Mixture Model, all other models assume that uncertainty is zero-mean Gaussian.



\section{Method}
\label{sec:feature_track_uq}

We wish to characterize the dependence of \textbf{mean error}, \textbf{mean absolute error}, \textbf{covariance}, \textbf{outlier ratio}, and \textbf{feature lifetime} on motion type, speed, tracker type, and when available, lighting. The types of motion investigated are:
\begin{itemize}
\item \textbf{Sideways motion} -- Linear movement with no rotation.
\item \textbf{Fixating motion} -- Moving in a constant radius around a central object. The camera is always pointed directly at the central object, creating some rotation.
\item \textbf{Forwards motion} -- Driving-like motion. The primary change frame-to-frame is scale. Points near the center of an image will stay near the center in subsequent frames.
\item \textbf{AR/VR motion} -- Movement that consists of mostly rotations around a persistent scene.
\end{itemize}
To vary speed, we skip frames at regular intervals from the image sequences. Nominal speed, or a speed of 1.00, means that all frames are used. A speed of 2.00 means that the feature tracker will only see every other frame, and a speed of 3.00 means that the feature tracker will only see one in every three frames. We do not test speeds below 1.00. The exact speeds tested depends on dataset. Finally, we also investigate the effect of two types of feature trackers:
\begin{itemize}
\item \textbf{Lucas-Kanade Sparse Optical Flow} \cite{lucas_iterative_1981}
\item \textbf{Correspondence Tracker} using the SIFT descriptor \cite{lowe_object_1999}. Although computationally expensive, the SIFT descriptor was chosen because of its availability and its performance when used in state estimation tasks \cite{schonberger_comparative_2017}. The descriptor of a feature track is set at the first frame it is detected and never updated.
\end{itemize}

We have chosen \emph{not} to study lens distortion, since this would require multiple similar datasets collected with different cameras. All images in all datasets either have been preprocessed to remove lens distortions, or simulated without lens distortions. Since the Lucas-Kanade tracker is differential, we also choose not to study a differential correspondence tracker that updates the descriptor of a feature track at every frame.



\subsection{Equations}

Consider a feature $i$ that was first detected at time $t^i_0$. If a depth image is available at time $t^i_0$ and $g_{sc}(t^i_0)$ is known, we may fix the feature's position in the spatial frame, $X_s^i$:
\begin{equation}
\begin{aligned}
    X^i_c(t^i_0) &= \pi^{-1}_K(x_p(t^i_0), Z^i_0) \\
    X^i_s &= g_{sc}(t^i_0) \circ X_c(t^i_0) \\
    \label{eq:fixing_Xs}
\end{aligned}
\end{equation}
In the above equation, $Z^i_c(t^i_0)$ is the third coordinate, or depth, of $X^i_c(t^i_0)$. Once, $X_s^i$ is fixed, we can then calculate the \textbf{``ground-truth feature track"} $\bar x_p^i(t)$:
\begin{equation}
    \bar x^i_p(t) = \pi_K(g_{sc}^{-1}(t) \circ X_s^i).
    \label{eq:gt_tracks}
\end{equation}
Some datasets provide a ground-truth point-cloud generated by a single lidar scan rather than a stream of depth images. A lidar scan is a point cloud with $M \sim 10^7$ points in the lidar frame $L$, which is defined as the camera frame at a particular time $t_L$: $\mathbf P_L = \{ P^0_L, P^1_L, \dots, P^M_L \}$. We can calculate the pixel coordinates of each point $j$ in $\mathbf P_L$: 
\begin{equation}
\pi_K(\mathbf P_L) = \{ \pi_K(P^0_L), \pi_K(P^1_L), \dots, \pi_K(P^M_L) \}
\label{eq:laser_scan_proj}
\end{equation}
Feature tracks visible at time $t_L$ can be associated with the nearest point in $\pi_K(\mathbf P_L)$. Suppose the nearest point in $\pi_K(\mathbf P_L)$ to feature $i$ is $P^j_L$. Then, the ground-truth track of feature $i$ is
\begin{equation}
\begin{aligned}
    X^i_s &= g_{sc}(t_L) \circ P^j_L \\
    \bar x^i_p(t) &= \pi_K(g_{sc}^{-1}(t) \circ X^i_s).
    \label{eq:dtu_px_groundtruth}
\end{aligned}
\end{equation}
Once we have a ground-truth feature track for feature $i$, we can calculate the error signal for that feature:
\begin{equation}
    e^i(t) = x_p^i(t) - \bar x_p^i(t)
    \label{eq:px_error_def}
\end{equation}
where $x_p^i(t)$ is the observed track. 


For datasets that provide a ground-truth point cloud at a single frame, the \textbf{mean error at timestep $t$} is
\begin{equation}
    \mu(t) = \frac{1}{M(t)} \sum_{i=1}^{M(t)} e^i(t)
    \label{eq:mean_error_at_time}
\end{equation}
where $M(t)$ is the number of tracked features at time $t$. The \textbf{mean absolute error at timestep $t$}
\begin{equation}
    \kappa(t) = \frac{1}{M(t)} \sum_{i=1}^{M(t)} |e^i(t)|.
    \label{eq:mean_abs_error_at_time}
\end{equation}
Similarly, the \textbf{covariance at timestep $t$} is calculated by
\begin{equation}
    \Sigma(t) = \frac{1}{M(t)-1} \sum_{i=1}^{M(t)} e^i(t) e^i(t)^T.
    \label{eq:cov_at_time}
\end{equation}
It is only possible to compute $\mu(t)$, $\kappa(t)$, and $\Sigma(t)$ for features that are visible at time $t_L$, when the laser scan was acquired.


For datasets that provide a stream of depth images, we use different definitions of mean error, mean absolute error, and covariance. We can also use all features and not just those visible in a particular frame. The \textbf{mean error after $k$ timesteps} is
\begin{equation}
    \nu(k) = \frac{1}{\Psi(k)} \sum_{i=1}^{\Phi(k)} e^i(t^i_0+k\delta_t)
    \label{eq:mean_error_after_timesteps}
\end{equation}
where $\Psi(k)$ is the number of features in the entire dataset tracked for at least $k$ timesteps and $\delta_t$ is the length of each timestep. The \textbf{mean absolute error after $k$ timesteps is:}
\begin{equation}
    \eta(k) = \frac{1}{\Psi(k)} \sum_{i=1}^{\Psi(k)} |e^i(t^i_0+k\delta_t)|
    \label{eq:mean_abs_error_after_timesteps}
\end{equation}
where $\Phi(k)$ is the number of features tracked for at least $k$ timesteps and $\delta_t$ is the length of each timestep. Finally, the \textbf{covariance after $k$ timesteps} is given by
\begin{equation}
    \Phi(k) = \frac{1}{\Psi(k)-1} \sum_{i=1}^{\Psi(k)} e^i(t^i_0+k\delta_t) e^i(t^i_0+k\delta_t)^T.
    \label{eq:cov_after_timesteps}
\end{equation}
When depth data is available at all frames, we define the feature's 3D location at the frame it is first detected and use equations \eqref{eq:mean_error_after_timesteps}, \eqref{eq:mean_abs_error_after_timesteps}, \eqref{eq:cov_after_timesteps}. %

At each frame, a feature tracker will attribute some features in one frame to the features in the previous frame. Let $F(t)$ be the total number of features in the frame at time $t$. The features in each frame will consist of $f_0(t)$ correct attributions, $f_1(t)$ incorrect attributions, and $f_2(t)$ new features, where $f_0(t) + f_1(t) + f_2(t) = F(t)$ and $f_0(t) + f_1(t) \leq F(t-1)$. Outlier rejection algorithms are used to determine $f_0(t)$ and $f_1(t)$ in real-time. The \textbf{outlier ratio} is defined as:
\begin{equation}
\frac{f_1(t)}{F(t-1)}.
\end{equation}

Finally, the \textbf{feature lifetime} of a feature track is the total number of consecutive frames in which it found and successfully attributed. A feature is ``born" at the frame it is first detected and ``dies" if a feature is not found for a single frame.


\section{Experiment Details}
\label{sec:feature_tracker_experiment_details}

\subsection{Feature Tracker Configuration}
\label{sec:feature_tracker_configuration}

We used the feature tracker is the \texttt{Tracker} object integrated with XIVO, our in-house SLAM system. The tracker is configured to use the AGAST corner detector \cite{mair_adaptive_2010}, and to track between 1000 and 1200 features at a time. The AGAST corner detector was chosen for its speed and because it detects a large number of features in most scenes. The feature tracker was configured to track up to 1200 features per scene. We use RANSAC with $p=0.995$ and an error threshold of 3 pixels to reject outliers. More details on the \texttt{Tracker} object and XIVO can be found in Appendix \ref{chapter:about_xivo}.

Since the tracker software was programmed to be part of a larger system and not specifically for these experiments, the implementation of the Correspondence Tracker is not ideal. If a feature is visible in frames 0-5, but is not detected in frame 2, the tracker will drop the feature at frame 2 and initialize a new one in frame 3. This behavior is consistent with the definition of feature lifetime given in the previous section, but is not the ideal implementation for a Correspondence Tracker because there is always a possibility that a corner detector will not find the corner in one frame, or that a descriptor will be just a little too different in one particular frame because of lighting. A more ideal implementation of the Correspondence Tracker would drop frames after a $N_m$ missed frames, where $N_m > 1$ is an experimentally determined number. The definition of feature lifetime would also be changed to accommodate this more complex behavior. As a result of this choice, the distribution of feature lifetimes for the Correspondence Tracker are shorter than they otherwise would be. Furthermore, our experiments will fail to characterize trends that only appear at higher speeds.


\subsection{Dataset-Specific Details}

\paragraph{DTU Point Features Dataset.}

The DTU Point Features Dataset \cite{aanaes_interesting_2012} consists of sixty scenes of fixating motion. In the dataset, one or more objects is placed at the center of stage lit with up to 19 LEDs. A camera is mounted on a robot arm and moved in a precise manner at the stage. At each of 119 fixed locations, the camera acquires an image lit with one of the 19 LEDs, enabling lighting experiments using image-based relighting. The dataset contains a laser scan of the scene at a single frame, called the Key Frame. The original image size is 1600 $\times$ 1200. For speed, we use 800 $\times$ 600 px. grayscale versions of the images instead of the full resolution images.

We make use of the first 49 frames of each scene, or Arc 1 (see Figure \ref{fig:dtu_light_stage}). The Key Frame is Frame 25. We calculate mean error $\mu$, mean absolute error $\kappa$, and covariance $\Sigma$ using equations \eqref{eq:mean_error_at_time}, \eqref{eq:mean_abs_error_at_time}, and \eqref{eq:cov_at_time}. Since 3D data is only available at the Key Frame, calculation of errors and covariances only includes features that exist in Frame 25. Therefore, there is a bias towards longer tracks, as all short tracks that don't exist in Frame 25 are all tossed out. Since the ``ground-truth" position of each feature in 3D is defined by its position in Frame 25, all results will therefore show that Frame 25 has zero covariance and the lowest errors. Statistics on feature lifetime and outlier rejection, however, do include features that do not exist in Frame 25.

To compute the ground-truth location of a feature track, we must associate a feature track to a point in a laser scan point cloud (eq. \eqref{eq:dtu_px_groundtruth}). Since the point cloud does not cover every pixel in the image, associations between features and laser scan points are only made if the pixel value of the laser scan point (eq. \eqref{eq:laser_scan_proj}) is less than 0.25 pixels from the feature.  Associating a pixel to a laser scan point with the incorrect depth measurement will result in a very large calculated means in equation \eqref{eq:mean_error_at_time}. Even with the low 0.25 pixel threshold, this bad association can still happen around edges and corners of objects. So that our analyses do not include very many of these poor depth associations, we throw out feature tracks whose maximum error is greater than the 90th percentile.

Since the DTU Point Features dataset was designed to enable image-based relighting, we also investigated the effects of directional light in addition to speed and the tracker used. We tested the same directional lights as  \cite{aanaes_interesting_2012}. The position of each directional light is shown in Figure \ref{fig:dtu_light_stage}.




\begin{figure}
    \centering
    \includegraphics[width=3.2in]{feature_tracker_uq/annotated_paths_of_interest.png}
    \includegraphics[width=3.2in]{feature_tracker_uq/annotated_light_stage_setup.png}
    \caption{\textbf{An Illustration of the Light Stage Setup in the DTU Point Features Dataset.}  \textbf{Left:} The locations at which images were acquired in the DTU Point Features dataset form three arcs and a linear path. Laser scans of the scenes were collected at the Key Frame (front and center). Frames from Arc 1 (circled in blue) are used for this experiment. \textbf{Right:} Red circles depict the location of 19 physical LEDs used to light the scene, which are spaced out over the scene. At each camera position in the left figure, the authors of the DTU Point Features dataset acquired 19 images. In each image, exactly one of the 19 LEDs is switched on. Acquiring 19 images in each location this way facilitates experiments in lighting using image-based relighting. Diffuse lighting can be simulated by using all 19 photographs from each position equally. More intense directional lighting can be simulated by weighting some LEDs more than others. In our experiments, we vary lighting from back-to-front (BF0-BF7) and left-to-right (LR0-LR9) as the camera follows the motion of Arc 1. Lights LR0 - LR9 and BF0 - BF7 are calculated by using Gaussian-weights on the 19 lights with $\sigma=20$cm; Light LR6 is highlighted in green. Figures are reprinted and annotated with permission.}
    \label{fig:dtu_light_stage}
\end{figure}



\paragraph{KITTI Vision Suite.} The raw data \cite{Geiger2012CVPR} in the KITTI Vision Suite consists RGB, GPS, IMU, and Lidar data captured from a moving vehicle. The motion captured in the images is predominantly forwards. The Lidar data was then processed into a separate benchmark dataset of depth images for single-image depth prediction and depth completion \cite{uhrig_sparsity_2017}. We make use stream \texttt{Image02}. Sequences containing ``still frames" (e.g. significant amount of waiting at a traffic light), are excluded. Excluding sequences containing still frames leaves 28 scenes for our experiments. Although this is fewer scenes than the DTU dataset, it is still more frames because most sequences are longer than 49 frames.

Since 3D data is available at every frame, we define a feature's 3D position using the depth image from the very first frame where it was detected. Therefore, we use mean error $\nu$ (eq. \eqref{eq:mean_error_after_timesteps}), absolute error $\eta$ (eq. \eqref{eq:mean_abs_error_after_timesteps}), and covariance $\Phi$ (eq. \eqref{eq:cov_after_timesteps}). To avoid errors due to bad depth measurements, we throw out the tracks whose maximum L2 error are above the 90th percentile and only calculate $\nu$, $\eta$, and $\Phi$ at timesteps where there are at least 100 features (see Fig. \ref{fig:kitti_avg_feats}).



\paragraph{Simulated Supplementary Data.} For AR/VR motions and sideways motions, we collected simulated RGB-D data in Gazebo. The simulation consisted of a Microsoft Kinect, modified so that RGB and depth data would be co-located, mounted on a Hector quadrotor \cite{hector_quadrotor} in ROS Melodic. The scene consisted of large objects from the Open Source Robotics Foundation's Gazebo Model Library. Images have a resolution of 800 $\times$ 600 pixels. In the subsequent sections, we refer to these datasets as ``Gazebo Linear" and ``Gazebo AR/VR". The AR/VR trajectory used to collect data is shown in Figure \ref{fig:gazebo_arvr_traj}.

In the Gazebo Linear dataset, we throw out tracks whose errors are above the 80-th percentile due to drift that naturally occurs when using the Lucas-Kanade Tracker in an environment containing straight and crisp edges parallel to the direction of motion. More details are given in Figure \ref{fig:gazebo_linear_error_throwout}. In the Gazebo AR/VR dataset, the we throw out tracks whose errors are above the 90-th percentile, as motions are no longer parallel to the straight edges.


\begin{figure}
\centering
\includegraphics[width=0.48\textwidth]{feature_tracker_uq/gazebo_arvr_figs/ARVR_translation_gt.pdf}
\includegraphics[width=0.48\textwidth]{feature_tracker_uq/gazebo_arvr_figs/ARVR_rotation_gt.pdf}
\caption{\textbf{The trajectory generated for the AR/VR scenario.} The commanded trajectory used to collected the AR/VR data was generated from the translation (left) and rotation (right) plotted above. Translation is generated point-to-point using haversines and rotation is generated from slerping.}
\label{fig:gazebo_arvr_traj}
\end{figure}



\subsection{Results}

Overall, we find that mean error, mean absolute error, covariance, feature lifetime, and outlier ratio are all dependent on the type of motion, the tracker used, and the speed. For the DTU Point Features dataset, we found no dependence on the existence of directional light unless the directional light happened to cause tracking failure at high speeds. In Tables \ref{tab:dtu_summary_table} - \ref{tab:gazebo_arvr_summary_table}, we list the exact dependence of mean error, mean absolute error, feature lifetime, covariance, and outlier ratio on each independent variable. Differences in Tables \ref{tab:dtu_summary_table} - \ref{tab:gazebo_arvr_summary_table} lead us to conclude that feature tracks are dependent on motion, tracker, and speed, but not the existence of directional light.

One notable difference between the Lucas-Kanade and Correspondence Trackers is that feature tracks produced by the Lucas-Kanade Tracker drift steadily while the Correspondence Tracker does not. This is because the Lucas-Kanade Tracker is differential, i.e. the characterization of a feature will slightly change frame to frame. For the Correspondence Tracker, this is not true. Therefore, the location of the feature track will drift, and the direction and magnitude of drift is dependent on the direction of motion. With left-to-right fixating motion, drift is positive (see Figure \ref{fig:dtu_diffuse_1.00_meanerror}). With left-to-right linear motion, drift is negative, and also larger (see Figure \ref{fig:gazebo_linear_LK_meanerror}). In AR/VR motion, the direction of drift changes with motion (see Figure \ref{fig:gazebo_arvr_LK_meanerror}). The flipside is that the Lucas-Kanade tracker generates features with a longer lifetime (see Figures \ref{fig:dtu_track_lifetime}, \ref{fig:kitti_feature_lifetime}, \ref{fig:gazebo_linear_feature_lifetime}, \ref{fig:gazebo_arvr_feature_lifetime}). When motion is fixating, the Correspondence Tracker also drifts about the direction of motion (see Figure \ref{fig:dtu_mean_error_sideways}).

Finally, we note that the zero-mean Gaussian assumption holds when motion is predominantly forwards and we are using the Correspondence Tracker (see Figures \ref{fig:kitti_match_meanerror} and \ref{fig:kitti_match_cov}). All figures supporting the assertions in this section are given in the Appendix.






\begin{table}[htp]
    \centering
    \begin{tabular}{p{1in}|p{1.0in}|p{1.0in}|p{2.5in}}
                & \textbf{Tracker} & \textbf{Lighting} & \textbf{Speed} \\
    \hline
    $\mu(t)$ & No (fig. \ref{fig:dtu_diffuse_1.00_meanerror}) & No (figs. \ref{fig:dtu_lighting_mu_LK}, \ref{fig:dtu_lighting_mu_match}) & No (figs. \ref{fig:dtu_match_diffuse_mean_error_varyspeed}, \ref{fig:dtu_LK_mean_varyspeed}) \\
    \hline
    $\kappa(t)$ & Yes (fig. \ref{fig:dtu_diffuse_1.00_MAE_cov}) & No (fig. \ref{fig:dtu_diffuse_1.00_MAE_cov}) & Yes for Correspondence Tracker (fig. \ref{fig:dtu_match_diffuse_MAE_varyspeed}), No for Lucas-Kanade Tracker (fig. \ref{fig:dtu_LK_MAE_varyspeed})\\
    \hline
    $\Sigma(t)$ & Yes (fig. \ref{fig:dtu_diffuse_1.00_MAE_cov}) & No (fig. \ref{fig:dtu_lighting_sigma_LK}, \ref{fig:dtu_lighting_sigma_match}) & Yes for Correspondence Tracker (fig. \ref{fig:dtu_match_diffuse_cov_varyspeed}), No for Lucas-Kanade Tracker (fig. \ref{fig:dtu_LK_cov_varyspeed})\\
    \hline
    Feature Lifetime & Yes (fig. \ref{fig:dtu_track_lifetime}) & No  (fig. \ref{fig:dtu_lighting_feature_lifetimes}) & Yes (fig.  \ref{fig:dtu_active_features}) \\
    \hline
    Outlier Ratio & Yes (figs. \ref{fig:dtu_track_outliers_lights}, \ref{fig:dtu_track_outliers_speed}) & No (fig.  \ref{fig:dtu_track_outliers_lights}) & Yes  (fig. \ref{fig:dtu_track_outliers_speed}) \\
    \end{tabular}
    \caption{\textbf{DTU Point Features Results Summary.} Cells contain whether or not the dependent variables in the left column are affected by the independent variables listed in the top row. Entries also contain figure numbers containing justification. The ``Tracker" and ``Lighting" columns contain references to figures containing plots at nominal speed. Although not indicated in the table, Figures \ref{fig:dtu_speed2.00_percent_outlier} - \ref{fig:dtu_LK_cov_speed12.00} in the Appendix show that the existence of directional lighting continues to not affect outlier ratio, mean error, mean absolute error, and covariance at higher speeds for both the Lucas-Kanade and Correspondence Trackers.}
    \label{tab:dtu_summary_table}
\end{table}



\begin{table}[htp]
    \centering
    \begin{tabular}{p{1in}|p{1.5in}|p{2.50in}}
                & \textbf{Tracker}  & \textbf{Speed} \\
    \hline
    $\nu(t)$ & No (fig. \ref{fig:kitti_1.00_meanerror}) & Yes (figs. \ref{fig:kitti_LK_meanerror}, \ref{fig:kitti_match_meanerror}) \\
    \hline
    $\eta(t)$ & Yes (fig. \ref{fig:kitti_1.00_error_cov}) & No for Correspondence Tracker (fig. \ref{fig:kitti_match_MAE}), Yes for Lucas-Kanade Tracker (figs. \ref{fig:kitti_LK_MAE}) \\
    \hline
    $\Phi(t)$ & Yes (fig. \ref{fig:kitti_1.00_error_cov}) & No for Correspondence Tracker (fig. \ref{fig:kitti_match_cov}), Yes for Lucas-Kanade Tracker (fig. \ref{fig:kitti_LK_cov}) \\ 
    \hline
    Feature Lifetime & Yes (fig. \ref{fig:kitti_feature_lifetime}) & Yes (fig. \ref{fig:kitti_avg_feats}) \\
    \hline
    Outlier Ratio & Yes (fig. \ref{fig:kitti_outlier_ratio}) & Yes (fig. \ref{fig:kitti_outlier_ratio})\\
    \end{tabular}
    \caption{\textbf{KITTI Results Summary.} Cells contain whether or not the dependent variables in the left column are affected by the independent variables listed in the top row. Entries also contain figure numbers containing justification.}
    \label{tab:kitti_summary_table}
\end{table}



\begin{table}[htp]
    \centering
    \begin{tabular}{p{1in}|p{1.5in}|p{2.5in}}
                & \textbf{Tracker}  & \textbf{Speed} \\
    \hline
    $\nu(t)$ & Yes (fig. \ref{fig:gazebo_linear_1.00_meanerror}) & No for Correspondence Tracker (fig. \ref{fig:gazebo_linear_match_meanerror}), Yes for Lucas-Kanade Tracker   (fig. \ref{fig:gazebo_linear_LK_meanerror}) \\
    \hline
    $\eta(t)$ & Yes (fig. \ref{fig:gazebo_linear_1.00_error_cov}) & Yes (figs. \ref{fig:gazebo_linear_LK_MAE}, \ref{fig:gazebo_linear_match_MAE}) \\
    \hline
    $\Phi(t)$ & Yes (fig. \ref{fig:gazebo_linear_1.00_error_cov}) & Yes (figs.  \ref{fig:gazebo_linear_match_cov}, \ref{fig:gazebo_linear_LK_cov}) \\ 
    \hline
    Feature Lifetime & Yes (fig. \ref{fig:gazebo_linear_feature_lifetime}) & Yes (fig. \ref{fig:gazebo_linear_avg_feats}) \\
    \hline
    Outlier Ratio & Yes (fig. \ref{fig:gazebo_linear_outlier_ratio}) &  No for Correspondence Tracker, Yes for Lucas-Kanade Tracker (fig. \ref{fig:gazebo_linear_outlier_ratio})\\
    \end{tabular}
    \caption{\textbf{Gazebo Linear Results Summary.} Cells contain whether or not the dependent variables in the left column are affected by the independent variables listed in the top row. Entries also contain figure numbers containing justification.}
    \label{tab:gazebo_linear_summary_table}
\end{table}


\begin{table}[htp]
    \centering
    \begin{tabular}{p{1in}|p{1.5in}|p{2.5in}}
                & \textbf{Tracker}  & \textbf{Speed} \\
    \hline
    $\nu(t)$ & Yes (fig. \ref{fig:gazebo_arvr_1.00_meanerror}) & No (fig. \ref{fig:gazebo_arvr_LK_meanerror}, \ref{fig:gazebo_arvr_match_meanerror}) \\
    \hline
    $\eta(t)$ & Yes (fig. \ref{fig:gazebo_arvr_1.00_error_cov}) & Yes for Correspondence   Tracker (fig. \ref{fig:gazebo_arvr_match_MAE}), No for Lucas-Kanade  Tracker (fig. \ref{fig:gazebo_arvr_LK_MAE})  \\
    \hline
    $\Phi(t)$ & Yes (fig. \ref{fig:gazebo_arvr_1.00_error_cov}) &  Yes for Correspondence   Tracker (fig. \ref{fig:gazebo_arvr_match_cov}), No for Lucas-Kanade Tracker (fig. \ref{fig:gazebo_arvr_LK_cov})  \\ 
    \hline
    Feature Lifetime & Yes (fig. \ref{fig:gazebo_arvr_feature_lifetime}) & Yes (fig. \ref{fig:gazebo_arvr_avg_feats}) \\
    \hline
    Outlier Ratio & Yes (fig. \ref{fig:gazebo_arvr_outlier_ratio}) & No for Correspondence Tracker, Yes for Lucas-Kanade Tracker (fig. \ref{fig:gazebo_arvr_outlier_ratio})\\
    \end{tabular}
    \caption{\textbf{Gazebo AR/VR Results Summary.} Cells contain whether or not the dependent variables in the left column are affected by the independent variables listed in the top row. Entries also contain figure numbers containing justification.}
    \label{tab:gazebo_arvr_summary_table}
\end{table}


\section{Discussion}
\label{sec:discussion}

Other than the caveat about the Correspondence Tracker noted in Section \ref{sec:feature_tracker_configuration}, the main limitation of this work is that there are more variables we could have tested, but chose not to. Examples of variables we chose not to test are the choice of feature detector and descriptor, and characteristics in the scene. For example, would the Correspondence Tracker have as little drift when moving forwards in an indoor environment and comparing BRIEF descriptors? Testing for conditionality on more variables inevitably leads to an unmanageable experiment, so we chose to lock in the feature detector and descriptor to well-performing available options and let the dataset dictate available scenes. Nevertheless, our work is a first step in characterizing the dependence of mean error, mean absolute error, covariance, feature lifetime, and outlier ratio on motion, tracker, speed, and the existence of directional lighting. The main conclusion is that the common zero-mean Gaussian assumption is rarely true. This conclusion motivates a few areas of future work.

The most immediate direction of future work is to continue to use the Extended Kalman Filter and dynamically adapt filter parameters, such as covariance estimates and the number of tracked features, to the scene. Since feature tracks are not zero-mean, covariance estimates will have to be enlarged so that feature tracks containing the extra bias are not outliers. Machine learning approaches to adapting the covariance already exist \cite{vega-brown_cello_2013, liu_deep_2018}. Since statistical methods are not often desirable in safety critical systems, it is of interest to compare performance when covariance is adjusted by a learned model to when covariance is adjusted by a finite state machine. While this approach is the most immediate, it does not address the fact that it brings no convergence guarantees in a downstream state estimation process and will therefore require extensive testing for each application.

The second area of future work is to adapt existing state estimation algorithms to accommodate feature tracks that are not zero-mean Gaussian. It may not be possible, however, to design a filter that is both computationally tractable, guaranteed to converge, and simple enough to implement on a complex, realistic system. This motivates the study in the next chapter, and the third area of future work.

The third direction of future work is to adjust individual feature tracks \emph{before} they are used by a state estimation algorithm that assumes that measurements are zero-mean Gaussian. This is the approach used for IMUs: errors in IMUs measurements are primarily dependent on temperature and mechanical alignment errors, so IMU measurements are adjusted for temperature and known mechanical misalignments before they are passed to a downstream computer. For feature tracks, the calibration table would be more complex, as it is dependent on speed, motion type, and the type of tracker used.


