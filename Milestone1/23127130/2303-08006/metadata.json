{
    "arxiv_id": "2303.08006",
    "paper_title": "Data-Efficient Learning of Natural Language to Linear Temporal Logic Translators for Robot Task Specification",
    "authors": [
        "Jiayi Pan",
        "Glen Chou",
        "Dmitry Berenson"
    ],
    "submission_date": "2023-03-09",
    "revised_dates": [
        "2023-03-15"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CL",
        "cs.RO"
    ],
    "abstract": "To make robots accessible to a broad audience, it is critical to endow them with the ability to take universal modes of communication, like commands given in natural language, and extract a concrete desired task specification, defined using a formal language like linear temporal logic (LTL). In this paper, we present a learning-based approach for translating from natural language commands to LTL specifications with very limited human-labeled training data. This is in stark contrast to existing natural-language to LTL translators, which require large human-labeled datasets, often in the form of labeled pairs of LTL formulas and natural language commands, to train the translator. To reduce reliance on human data, our approach generates a large synthetic training dataset through algorithmic generation of LTL formulas, conversion to structured English, and then exploiting the paraphrasing capabilities of modern large language models (LLMs) to synthesize a diverse corpus of natural language commands corresponding to the LTL formulas. We use this generated data to finetune an LLM and apply a constrained decoding procedure at inference time to ensure the returned LTL formula is syntactically correct. We evaluate our approach on three existing LTL/natural language datasets and show that we can translate natural language commands at 75\\% accuracy with far less human data ($\\le$12 annotations). Moreover, when training on large human-annotated datasets, our method achieves higher test accuracy (95\\% on average) than prior work. Finally, we show the translated formulas can be used to plan long-horizon, multi-stage tasks on a 12D quadrotor.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.08006v1"
    ],
    "publication_venue": "Accepted in ICRA 2023"
}