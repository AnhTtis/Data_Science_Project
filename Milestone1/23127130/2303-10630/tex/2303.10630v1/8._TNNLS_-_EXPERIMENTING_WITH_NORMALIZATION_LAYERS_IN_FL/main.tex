
\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{array}
\usepackage{booktabs}
\usepackage[caption=false,font=footnotesize]{subfig}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{float}
\usepackage{multirow}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{pdfsync}
%\usepackage{subfigure}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}

\let\comment\undefined
\usepackage{changes}
\definechangesauthor[name=Marco]{MA}
\definechangesauthor[name=Bruno]{BC}


\begin{document}
\title{Experimenting with Normalization Layers in Federated Learning on non-IID scenarios} 
\author{Bruno Casella, Roberto Esposito, Antonio Sciarappa, Carlo Cavazzoni, Marco Aldinucci
\thanks{Bruno Casella, Roberto Esposito and Marco Aldinucci are with the Computer Science Dpt. of the University of Turin, Corso Svizzera 185, Turin, Italy (emails: [bruno.casella, roberto.esposito, marco.aldinucci]@unito.it). \\
Antonio Sciarappa and Carlo Cavazzoni are with Leonardo S.p.A., Italy (emails: antonio.sciarappa.ext@leonardo.com, carlo.cavazzoni@leonardo.com) \\
 \copyright 2022 IEEE. Personal use of this material is permitted. Permission from
IEEE must be obtained for all other uses, in any current or future media,
including reprinting/republishing this material for advertising or promotional
purposes, creating new collective works, for resale or redistribution to servers
or lists, or reuse of any copyrighted component of this work in other works.}}

\markboth{IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. XX, NO. X, XX XXXX}%
{Experimenting with Normalization Layers in Federated Learning on non-IID scenarios}

\maketitle

\begin{abstract}
  Training Deep Learning (DL) models require large, high-quality datasets, often assembled with data from different institutions. Federated Learning (FL) has been emerging as a method for privacy-preserving pooling of datasets employing collaborative training from different institutions by iteratively globally aggregating locally trained models. One critical performance challenge of FL is operating on datasets not independently and identically distributed (non-IID) among the federation participants. Even though this fragility cannot be eliminated, it can be debunked by a suitable optimization of two hyper-parameters: layer normalization methods and collaboration frequency selection. In this work, we benchmark five different normalization layers for training Neural Networks (NNs), two families of non-IID data skew, and two datasets. Results show that Batch Normalization, widely employed for centralized DL, is not the best choice for FL, whereas Group and Layer Normalization consistently outperform Batch Normalization. Similarly, frequent model aggregation decreases convergence speed and mode quality.    
%Most of the typical Deep Learning (DL) techniques, such as Neural Networks (NNs), include a layer of normalization for improving the model's training and stabilizing the convergence. The most employed normalization layer in DL is Batch Normalization (BN), which standardizes the layers' input for each mini-batch. The training of DL models requires large high-quality datasets, but most of the time, institutions do not have such datasets, and aggregating data from different owners can bring up privacy and security issues. Federated Learning (FL) is a cooperative learning approach which has emerged as an effective way to address these concerns by only moving local DL models instead of data, which are kept in their original organization. One critical challenge of FL is managing real-world data that are usually not independently and identically distributed (non-IID) among the federation participants. Recent years have seen the growth of many FL algorithms aiming at tackling the non-IID setting; however, neither of those techniques outperforms the other in all the non-IID scenarios. The first proposed FL algorithm is FederatedAveraging (FedAvg), where the main principle is to average the weights in all the layers of the NN. The other common FL algorithms are modified versions of FedAvg, so the main principle is still the same. We found that, although BN is the standard normalization layer for classic DL, it can be detrimental to the quality of the federated models. This work extends the typical hyperparameter tuning of DL parameters to FL and provides an empirical assessment of the federated behaviour of BN and some of its most famous alternatives, such as Group Normalization (GN), Layer Normalization (LN), Instance Normalization (IN) and Batch Renormalization (BRN), in common uniform and non-IID settings. Moreover, additional tests have been conducted for the most promising normalization layers to show how performances are affected by the batch size, the number of epochs per round and the number of clients of the federation. Results show that GN and LN versions of NNs outperform all the others in almost all the cases, placing them as state-of-the-art normalization layers for FL.
\end{abstract}


\begin{IEEEkeywords}
Federated Learning, Federated Averaging, non-IID, Neural Networks, Normalization Layers, Batch Normalization
\end{IEEEkeywords}


\section{Introduction}
\label{sec:introduction}
\IEEEPARstart{T}{he} constant development of Information and Communication Technologies has boosted the availability of computational resources and data, leading us to the Big Data era, where data-driven approaches have become a fundamental aspect of everyday decisions. Both computational resources and data are ubiquitous and inherently distributed. All public and private sectors, from scientific research to companies, take benefit from a vast amount of diverse data to support the growth of their business and to develop more accurate Artificial Intelligence (AI) systems.

Data is often spread and segregated in silos across different institutions and even different business units of the same organization. It is essential to make data accessible to all the partners to train high-quality models and exploit the entire data's value~\cite{kairouz2021advances}. Many recent open science works have encouraged data sharing between institutions in order to improve research possibilities, create collaborations and publish reproducible results. For example, data sharing across countries has been a crucial information tool during the CoViD-19 pandemic~\cite{callaghan2020data}.

However, data is often not shareable due to issues like privacy, security, ownership, trust and economic reasons. For instance, the European regulation GDPR~\cite{voigt2017eu} places stringent constraints on the possibility of sharing sensitive data between parties; industrial companies do not share their data because leveraging it is seen as a competitive advantage. Also, exposing data to other institutions can raise concerns like lack of ownership and lack of trust.

To address these problems, model-sharing strategies (MSS) have emerged as an alternative to data sharing. In MSS, the idea is to share AI models between the involved parties in order to achieve collaboration without sharing raw data. In these approaches, the AI model can range from simpler Machine Learning (ML) algorithms like linear regression to more complex models such as those learned by Deep Learning techniques using Neural Networks (NNs). Recent years have seen the growth of different model-sharing approaches ranging from the "model-to-data remote access" approaches to Federated Learning~\cite{mcmahan2017communication}. In "model-to-data remote access" approaches, AI models are run remotely directly on the machines that hold the data and security is enforced by leveraging secure remote connections and Trusted Execution Enviroments (TEEs) enclaves. Federated Learning has also emerged as a  popular approach. In FL, the involved parties collaborate by aggregating locally trained models into a globally shared one. The process is usually iterative and based on NNs (FedAvg)~\cite{mcmahan2017communication}, even if recently methods based on non-NN distributed boosting algorithms have been proposed ~\cite{polato2022boosting}. These algorithms allow parties to aggregate any kind of model without making assumptions about the kind of model being aggregated or assuming a training procedure based on gradient descent \cite{23:praise-fl:pdp}.


FL is a distributed ML technique originally proposed by Google in 2016 to deal with sensitive data of mobile devices~\cite{mcmahan2017communication}.
FL is an iterative version of model-sharing: clients (the data owners) create a federation (hence the name) together with the server and build a shared model based on the following steps: \emph{1)} clients send their metadata, like the number of classes, training set size, test set size and shape of the input features, to the server, that initializes a model based on the received metadata characteristics; \emph{2)} the server sends the initialized model to all the participants of the federation; \emph{3)} after performing one or more steps of gradient descent, clients send the trained model back to the server; \emph{4)} server acts as an aggregator performing a combination (a function like average, sum, maximum, minimum and so on) of the received models. The aggregated model is now sent to the clients, and steps \emph{3)} and \emph{4)} are repeated until a specified number of rounds are performed, or a convergence criterion is met. The first proposed FL algorithm is the FedAvg~\cite{mcmahan2017communication} algorithm, where the aggregation function used to combine models is the average. In this way, all datasets are kept within the proprietary organizations, and the only information that gets exchanged is the model parameters, that in the case of NN, are matrices of floating point numbers representing the weights and the biases associated with the neurons.

Federated Learning performs well when the data is independently and identically distributed (IID) among the involved institutions. Unfortunately, real-world data is often non-IID, and it is well known that this scenario poses critical issues to FL~\cite{kairouz2021advances}. In a non-IID setting, the data statistics of a single client may be unrepresentative of the global statistics and make the model diverge from the intended solution. Interestingly, Huang at al. show that if the loss surface of the optimization problem is both smooth and convex (which is hardly true in a real scenario), then FedAvg will also converge when the data is non-IID \cite{li2019convergence}.

Recent works have proposed several FL algorithms to cope with non-IIDness problems, such as FedProx~\cite{li2020federated}, FedNova~\cite{wang2020tackling}, SCAFFOLD~\cite{karimireddy2020scaffold}, and FedCurv~\cite{shoham2019overcoming}, which has been tested in \cite{li2022federated, casella2022benchmarking}. Notice that all these algorithms are modified versions of FedAvg and they preserve the principle underneat FedAvg: to average the weights in all the layers of the NN. Most of the common NN architectures employ Batch Normalization (BN)~\cite{ioffe2015batch}, a technique for improving the training of NNs to make them converge faster and to a more stable solution. BN works by standardizing the layers' input for each mini-batch. 

In this work, we investigate two aspects of the training FL models which, differently from cenytralized case, they happen to be hyper-parameters that can be optimized: the normalization layers and the frequency of models aggregation (epochs per round). We show that the most popular normalization layer (BN) does not couple well with FL for non-IID data and that substituting BN with alternative normalization FL a better model can be produced for both the non-IID and IID case. We also show that building a global model aggregating local models at each epoch is not a good strategy, neither for quality of the model nor for the execution time. We  experiment with two network architectures and five different normalization layers on two public image datasets: MNIST~\cite{lecun1998gradient} and CIFAR-10~\cite{krizhevsky2009learning}.
%We will experiment with two network architectures, namely ResNet-18 \cite{he2016deep} and EfficientNet-B0~\cite{tan2019efficientnet}, with BN, and with several normalization layers, namely: Group Normalization (GN)~\cite{wu2018group}, Layer Normalization (LN)~\cite{ba2016layer}, Instance Normalization (IN)~\cite{ulyanov2016instance} and Batch Renormalization (BRN)~\cite{ioffe2017batch}. The networks will be trained on two public image datasets: MNIST~\cite{lecun1998gradient} and CIFAR-10~\cite{krizhevsky2009learning}. Non-IIDness will be simulated by splitting the datasets to simulate most common non-IID settings. Specifically, we will simulate a feature distribution skew and a label distribution skew, in addition to the IID setting. 
Results show that the performance of the networks is strictly related to the type of normalization layer adopted.  

The main contributions of this work are:
\begin{itemize}
  \item We provide benchmarks for five different normalization layers: BN, GN, LN, IN, BRN;
	\item We provide results of experiments on FedAvg on two non-IID settings considering a feature distribution skew and a labels distribution skew (in addition to the IID case). To the best of our knowledge, this is the first work providing empirical evidence on the behaviour of these normalization layers in common non-IID cases;
	\item for the most promising normalization layers, we ran extensive tests to discuss how performances are affected by the following factors:
	\begin{enumerate}
		\item Batch size.
		\item Number of epochs per round (E).
		\item Number of clients.
	\end{enumerate}
 	\item We show that choosing the right normalization layer and a suitable number of local gradient descent steps is crucial for obtaining good performances. 
\end{itemize}
This work extends the typical search for optimization of machine learning parameters to federated learning.

The rest of the paper is organized as follows. In Section~\ref{sec:related-work}, we introduce and discuss recent related works. In Section~\ref{sec:normalization-layers}, the most used normalization layers are reviewed. In Section~\ref{sec:non-IID-data}, the most typical non-IID scenarios are described. Section~\ref{sec:experiments} shows and discusses experimental results. Finally, Section~\ref{sec:conclusions} concludes the paper.


\section{Related Work}
\label{sec:related-work}
\noindent The main challenges in FL are statistical heterogeneity (non-iidness) and systems heterogeneity (variability of the devices of the federation). In this work, we address the former. In \cite{kairouz2021advances}, the most common non-IID data settings that are quantity skew, labels quantity skew (prior shift), feature distribution skew (covariate shift), same label but different features and same features but different labels, are reviewed. To the best of our knowledge, there are only a few benchmarks for FL dealing with non-IID data. Li et al. in \cite{li2022federated} report the analysis of FedAvg~\cite{mcmahan2017communication}, FedNova~\cite{wang2020tackling}, FedProx~\cite{li2020federated} and SCAFFOLD~\cite{karimireddy2020scaffold} on nine public image datasets, including MNIST~\cite{lecun1998gradient} and CIFAR10~\cite{krizhevsky2009learning}, split according to three of the previous mentioned non-IID partition strategies, i.e. quantity skew, labels quantity skew and three different versions of feature distribution skew: noise-based, synthetic and real-world feature imbalance. Authors show that none of those algorithms outperforms others in all the cases and that non-iidness degrades the performance of FL systems in terms of accuracy, especially in the case of labels quantity skew. Another recent work~\cite{casella2022benchmarking} reports an empirical assessment of the behaviour of FedAvg and FedCurv~\cite{shoham2019overcoming} on MNIST, CIFAR10 and MedMNIST~\cite{yang2021medmnist}. Datasets are split according to the same non-IID settings of \cite{li2022federated}. Authors show that aggregating models at each epoch is not necessarily a good strategy: performing local training for multiple epochs before the aggregation phase can significantly improve performance while also reducing communication costs. FedAvg produced better models in most non-IID settings despite competing with an algorithm explicitly developed to deal with these scenarios (FedCurv). 

Results in \cite{casella2022benchmarking} also confirmed literature sentiment: labels quantity skew and its pathological variant are the most detrimental ones for the algorithms. The same non-IID partitions have already been tested in \cite{polato2022boosting}, which proposes a novel technique of non-gradient-descent FL on tabular datasets.
Our paper extends \cite{casella2022benchmarking}, deepening the experiments about the number of epochs per round, a hyper-parameter that, if tuned appropriately, can lead to large performance gains. Moreover, we aim to investigate which type of normalization layer better fits FL on non-IID data. Indeed, when data are non-IID, batch statistics do not represent the global statistics, leading NNs equipped with BN to poor results. The most common alternatives to BN are: Group Normalization (GN)~\cite{wu2018group}, Layer Normalization (LN)~\cite{ba2016layer}, Instance Normalization (IN)~\cite{ulyanov2016instance} and Batch Renormalization (BRN)~\cite{ioffe2017batch}. To the best of our knowledge, there are no works benchmarking normalization layers for FL on non-IID data. A previous work \cite{casella2022transfer}, proposing a novel form of Transfer Learning through test-time parameters' aggregation, shows that a NN with Batch Normalization~\cite{ioffe2015batch} does not learn at all, while performance improves only when using Group Normalization~\cite{wu2018group}. Andreaux et al. propose a novel FL approach by introducing local-statistic BN layers \cite{andreux2020siloed}. Their method, called SiloBN, consists in only sharing the learned BN parameters $\gamma$ and $\beta$ across clients, while BN statistics $\mu$ and $\sigma^2$ remain local, allowing the training of a model robust to the heterogeneity of the different centres. SiloBN showed better intra-centre generalization capabilities than existing FL methods. FedBN~\cite{li2021fedbn} is a FL algorithm that excludes BN layers from the averaging step, outperforming both FedAvg and FedProx in a feature distribution skew setting.



\section{Normalization Layers}
\label{sec:normalization-layers}
\noindent The majority of the FL algorithms simply apply an aggregation function (like averaging) to all the components of a NN, including weights and biases of the normalization layers. Most of the common NN architectures, like residual networks~\cite{he2016deep}, adopt BN~\cite{ioffe2015batch} as normalization layer. However, in contexts like Federated or Transfer Learning, BN may not be the optimal choice, especially when dealing with non-IID data. In this chapter will be reviewed the main characteristics of Batch Normalization and several possible alternatives like Group Normalization (GN)~\cite{wu2018group}, Layer Normalization (LN)~\cite{ba2016layer}, Instance Normalization (IN)~\cite{ulyanov2016instance} and Batch Renormalization (BRN)~\cite{ioffe2017batch}.
\subsection{Batch Normalization}
\label{ssec:batch-norm}
Batch Normalization has seen a recent extensive adoption by neural networks for their training. The key issue that BN tackles is Internal Covariate Shift (ICS), which is the change in the distribution of the data (or network activations), i.e. the input variables of training and test sets. Informally, at each epoch of training, weights are updated, input data are different, and the algorithm faces difficulties. This results in a slower and more difficult training process because lower learning rates and careful parameter initialization are then required. BN attempts to reduce ICS by normalizing activations to stabilize the mean and variance of the layer's inputs. This accelerates training by allowing the use of higher learning rates and reduces the impact of the initialization. During training, BN normalizes the output of the previous layers along the batch size, height and width axes to have zero mean and unit variance:
$$\hat{x}_i = \frac{x_i - \mu_m}{\sqrt{\sigma^2_m + \epsilon}}$$
where $x, \mu_m$ and $\sigma^2_m$ are respectively the input, the mean and the variance of a minibatch $m$, and $\epsilon$ is arbitrarily constant greater than zero used for stability in case the denominator is zero. BN also adds two learnable parameters, $\gamma$ and $\beta$ that are a scaling and a shifting step, to fix the representation in case the normalization alters what the layer represents: $y_i = \gamma \hat{x_i} + \beta$. Normalized activations will depend on the other samples contained in the minibatch. In the test phase, BN can not calculate statistics; otherwise, it will learn from the test dataset, so it uses the moving averages of minibatch means and variances of the training set. In the case of IID mini-batches, statistical estimations will be accurate if the batch size is large; otherwise, inaccuracies will be compounded with depth, reducing the quality of the models. Non-IID data can have a more detrimental effect on models equipped with BN because batch statistics do not represent global statistics, leading to even worse results. Therefore there is a need to investigate alternatives to BN that can work well with non-IID data and small batch sizes.
\subsection{Group Normalization}
\label{ssec:group-norm}
Group Normalization is a simple alternative to BN. It divides the channels into different groups and computes within each group the mean $\mu_i$ and the variance $\sigma_i$ along the height and width axes. GN overcomes the constraint on the batch size because it is completely independent of the other input features in the batch, and its accuracy is stable in a wide range of batch size. Indeed, GN has a 10.6$\%$ lower error than BN on ResNet-50~\cite{he2016deep} trained on ImageNet~\cite{wu2018group}. 
The number of groups G is a pre-defined hyperparameter which needs to divide the number of channels C. When G=C, it means that each group contains one channel, and GN becomes Instance Normalization, while when G=1, it means that one group contains all the channels, and GN becomes Layer Normalization. Both Instance and Layer Normalizations are described below.
\subsection{Instance Normalization}
\label{ssec:instance-norm}
Instance Normalization is another alternative to BN, firstly proposed for improving NN performances in image generation. It can be seen as a Group Normalization with G=C or as a BN with a batch size of one, so applying the BN formula to each input feature individually. Indeed, IN computes the mean $\mu_i$ and the variance $\sigma_i$ along the height and width axes. As stated before, BN suffers from small batch sizes, so we expect that experiments made with IN will produce worse results than the ones with BN or GN, which can exploit the dependence across the channels.
\subsection{Layer Normalization}
\label{ssec:layer-norm}
Layer Normalization was first proposed to stabilize hidden state dynamics on Recurrent Neural Networks (RNNs)~\cite{ba2016layer}. It computes the mean and the variance along the channel, height and width axes. LN overcomes the constraint on the batch size because it is completely independent of the other input features in the batch. LN performs the same computation both at training and inference times. It can be seen as a GN with G=1, so with only one group controlling all the channels. As a result, when there are several distributions to be learned among the group of channels, it can perform worse than GN.
\subsection{Batch Renormalization}
\label{ssec:batch-renorm}
Batch Renormalization~\cite{ioffe2017batch} is an extension of BN that ensures training and inference models generate the same outputs that depend on individual examples rather than the entire minibatch. BRN is an augmentation of a network which contains batch normalization layers with a per-dimension affine transformation applied to the normalized activations to ensure the match between training and inference models. Reducing the dependence of activation of each sample with other samples in the minibatch can result in a performance increase when data are non-IID.




\section{Non-IID Data}
\label{sec:non-IID-data}
\noindent The most common non-IID data settings are reviewed in \cite{kairouz2021advances} that lists five different partitioning strategies: 1) quantity skew, 2) labels quantity skew (prior shift), 3) feature distribution skew (covariate shift), 4) same labels but different features and 5) same features but different labels. In this paper, we consider the same distributions tested in \cite{casella2022benchmarking, li2022federated} apart from quantity skew, which is not treated. Indeed, \cite{casella2022benchmarking, li2022federated} showed that quantity skew does not hurt the performance of FL models, probably because it results in a different quantity of samples per client, but the distribution of samples is uniform, which is easy to deal with. In this paper, labels quantity skew, which is the most detrimental to the FL models' performance, has been extensively tested in a lot of scenarios to show how it is possible to overcome its difficulties. The cases adopted (both IID and non-IID) are briefly described.
\begin{itemize}
	\item \textbf{Uniform Distribution (IID)}: each client of the federation holds the same amount of data, and the distribution is uniform among parties. This is the simplest case for FL algorithms because the distribution is IID.
	\item \textbf{Labels Quantity Skew}: the marginal distributions of labels $P(y_i)$ vary across parties, even if $P(x_i|y_i)$ is the same. This especially happens when dealing with real FL applications where clients of the federation are distributed among different world regions. Certain data are present only in some countries, leading to the labels quantity skew. In this work, we adopted the simplest version of labels quantity skew, where each client holds samples belonging to only a fixed amount of classes. In our experiments, we used two as number of classes per client. Other versions of labels quantity skew can be the Dirichlet labels skew (each client holds samples such that classes are distributed according to the Dirichlet function) and the Pathological labels skew (data are firstly sorted by label and then divided in shards). 
	\item \textbf{Feature Distribution Skew}: the marginal distributions $P(x_i)$ vary across parties, even if $P(y|x)$ is shared. This can happen in a lot of ML scenarios; for example, in handwriting recognition, the same words can be written with different styles, stroke widths, and slants. The covariate shift was obtained according to the procedure described in \cite{polato2022boosting}: samples are distributed among clients according to the results of a Principal Component Analysis (PCA) performed on the data.
\end{itemize}



\section{Experiments}
\label{sec:experiments}
\noindent Our experiments have been realized using OpenFL~\cite{reina2021openfl}, the new framework for FL developed by Intel Internet of Things Group and Intel Labs. OpenFL is a Python 3 library for FL that enables organizations to collaboratively train a model without sharing sensitive information. OpenFL is DL framework-agnostic. Training of statistical models may be done with any deep learning framework, such as TensorFlow or PyTorch, via a plugin mechanism.
OpenFL is based on a Director-Envoy workflow which uses long-lived components in a federation to distribute more experiments in the federation. The Director is the central node of the federation. It starts an Aggregator for each experiment, sends data to connected collaborator nodes, and provides updates on the status. The Envoy runs on Collaborator nodes connected to the Director. When the Director starts an experiment, the Envoy starts the Collaborator to train the global model.
All the experiments were computed in a distributed environment with ten collaborators. Each collaborator is run on an Intel Xeon CPU (8 cores per CPU), 1 Tesla T4 GPU. The code used for experimental evaluation is publicly available at \href{https://github.com/alpha-unito/Benchmarking-Normalization-Layers-in-Federated-Learning-for-Image-Classification-Tasks-on-non-IID}{https://github.com/alpha-unito/Benchmarking-Normalization-Layers-in-Federated-Learning-for-Image-Classification-Tasks-on-non-IID}.

\noindent\textbf{Dataset}: We tested FedAvg on MNIST~\cite{lecun1998gradient} and CIFAR10~\cite{{krizhevsky2009learning}}, that are default benchmarks in ML literature. The details of the datasets are summarized in Table \ref{tab:dataset-stats}.

\begin{table}
\centering
\caption{\label{tab:dataset-stats}Statistics of the datasets.\\}
\label{tab:freq}
\begin{tabular}{llll}
\toprule
\textbf{Dataset}  & \textbf{Train samples} & \textbf{Test samples} &  \textbf{\# labels} \\
\midrule
MNIST    & 60.000        & 10.000       & 10        \\
\midrule
CIFAR10  & 50.000        & 10.000       & 10        \\
\bottomrule
\end{tabular}
\end{table}

\noindent\textbf{Preprocessing}: both datasets were not rescaled: MNIST images are 28x28 while CIFAR10 images are 32x32. As for data augmentation, we performed random horizontal flips and random crop with a probability of 50\%. All the datasets were normalized according to their mean and standard deviation.

\noindent\textbf{Model}: We employed ResNet-18~\cite{he2016deep} and EfficientNet-B0~\cite{tan2019efficientnet} as classification models, trained by minimizing the cross-entropy loss with mini-batch gradient descent using the Adam optimizer with learning rate $10^{-3}$. The local batch size was 128. We used two networks to show that the results are not model-dependent (See \ref{sec:appendix} for the EfficientNet-B0's results). The scores of baseline models and federated experiments on the uniform and non-IID settings (section. \ref{ssec:norm-layers-non-IID}, \Cref{tab:non-federated,tab:LABELSSKEW-setting,tab:COVARIATE-setting}) are the average ($\pm$ standard deviation) over five runs. For the extensive experiments on batch size, number of local training steps, and number of clients, we tested only ResNet-18 for only one run.

\noindent\textbf{Normalization Layers}: All the normalization layers described before, i.e. Batch Norm, Group Norm, Instance Norm, Layer Norm and Batch Renormalization, have been applied to the classification model in each experiment. For the most promising normalization layers have been ran additional tests to study the impact of the batch size, the number of epochs per round and the number of clients. For BN, we set the momentum, i.e. the importance given to the previous moving average, to 0.9, according to the SOTA~\cite{moreau2022benchopt} for ResNet-18. For GN, the number of channels must be divisible by the number of groups, so we set the number of groups to 32 for ResNet-18 (one of the possible divisors) and 8 for EfficientNet-B0 (the only possible divisor). All the other normalization layers have been used with their standard PyTorch configuration.

Top-1 accuracy has been employed as a classification metric to compare the performance. Results show the best aggregated model's accuracy. The learning curve of all the experiments can be studied from \Cref{fig:non-iid} to \Cref{fig:CLIENTS}. Table \ref{tab:non-fed-setting} reports about a non-federated baseline, i.e., the typical AI scenario where the data are centralized. The remaining tables show the performance of FedAvg in different data partitioning scenarios and for different values of some hyperparameters such as batch size, number of epochs per round and number of clients.


\begin{table*}
\centering
\caption{\label{tab:non-fed-setting}Accuracy in the non-federated setting.\\}
\label{tab:non-federated}
\begin{tabular}{lccccccc} 
\toprule
\textbf{Dataset}            & \textbf{BN}  & \textbf{GN}  & \textbf{IN} & \textbf{LN} & \textbf{BRN}\\ 

\midrule
MNIST        & $99.22\% \pm 0.07\%$          &   $99.29\% \pm 0.06\%$      &    $11.36\% \pm 0.00\%$       &    $\textbf{99.32\%} \pm \textbf{0.07\%}$        &    $99.00\% \pm 0.20\%$       \\
\midrule
CIFAR10      & $\textbf{82.60\%} \pm \textbf{0.24\%}$          &    $81.61\% \pm 0.24\%$     &    $9.89\% \pm 0.00\%$        &    $82.06\% \pm 0.40\%$        &    $81.77\% \pm 0.23\%$                 \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Normalization Layers and non-IID data}
\label{ssec:norm-layers-non-IID}
This subsection presents the results of the three data partitioning scenarios presented: uniform, labels quantity skew and covariate shift. \Cref{tab:UNIFORM-setting} shows that normalization levels have a huge impact on the performance of a NN, ranging from very poor levels to almost reaching the level of accuracy in the centralized case.
ResNet-18-LN performs slightly better than BN and GN while outperforming IN and BRN in the uniform setting (\cref{subfig-1:uniform}). In both the labels quantity skew and the covariate shift scenarios, both GN and LN outperform all the other normalization layers; however, they require more training steps to converge, as shown in \cref{subfig-2:labels} and \cref{subfig-3:covariate}.
IN does not learn in FL; indeed, since both MNIST and CIFAR10 have ten classes, ResNet-18-IN's performance is like tossing a coin. 
BRN seems to have a very long learning curve; in fact, it needs a lot of training rounds to reach convergence. However, its performance is still far from the best performances of BN, GN and LN. For this reason, the following subsections will report results only for the most promising normalization layers: BN, GN and LN.


\begin{table*}
\centering
\caption{\label{tab:UNIFORM-setting}Accuracy in the uniform setting.\\}
\label{tab:UNIFORM-setting}
\begin{tabular}{lccccc} 
\toprule
\textbf{Dataset}        & \textbf{BN}  & \textbf{GN}  & \textbf{IN} & \textbf{LN} & \textbf{BRN}\\ 

\midrule
MNIST       & $97.12\% \pm 0.24\%$               & $98.26\% \pm 0.19\%$             & $11.28\% \pm 0.04\%$                 & $\textbf{98.51\%} \pm \textbf{0.06\%}$                 & $86.54\% \pm 4.24\%$                 \\
\midrule     
CIFAR10     & $47.12\% \pm 0.82\%$               & $53.99\% \pm 0.23\%$               & $10.04\% \pm 0.07\%$                 & $\textbf{59.06\%} \pm \textbf{0.25\%}$          & $32.36\%  \pm 1.07\%$                \\           
\bottomrule
\end{tabular}
\end{table*}



\begin{table*}
\centering
\caption{\label{tab:LABELSSKEW-setting}Accuracy in the labels quantity skew setting.\\}
\label{tab:LABELSKEW}
\begin{tabular}{lccccc} 
\toprule
\textbf{Dataset}        & \textbf{BN}  & \textbf{GN}  & \textbf{IN} & \textbf{LN} & \textbf{BRN}\\ 

\midrule
MNIST       & $82.25\% \pm 7.70\%$               & $97.32\% \pm 0.46\%$             & $18.53\% \pm 4.18\%$                 & $\textbf{97.68\%} \pm \textbf{0.22\%}$                 & $74.66\% \pm 3.20\%$                 \\
\midrule     
CIFAR10     & $38.02\% \pm 1.64\%$               & $\textbf{58.91\%} \pm \textbf{3.10\%}$               & $19.00\% \pm 5.47\%$                 & $57.98\% \pm 3.88\%$          & $27.34\%  \pm 5.56\%$                \\           
\bottomrule
\end{tabular}
\end{table*}



\begin{table*}
\centering
\caption{\label{tab:COVARIATE-setting}Accuracy in the covariate shift setting.\\}
\label{tab:COVARIATE}
\begin{tabular}{lccccc} 
\toprule
\textbf{Dataset}        & \textbf{BN}  & \textbf{GN}  & \textbf{IN} & \textbf{LN} & \textbf{BRN}\\ 

\midrule
MNIST       & $96.42\% \pm 0.25\%$               & $\textbf{97.96\%} \pm \textbf{0.19\%}$             & $11.51\% \pm 0.23\%$                 & $97.37 \pm 1.63\%$                 & $91.01\% \pm 1.12\%$                 \\
\midrule     
CIFAR10     & $44.58\% \pm 1.96\%$               & $51.04\% \pm 2.83\%$               & $10.08\% \pm 0.23\%$                 & $\textbf{55.54} \pm \textbf{2.22\%}$          & $26.20\%  \pm 1.48\%$                \\           
\bottomrule
\end{tabular}
\end{table*}


\begin{figure*}[!ht]
  \subfloat[\normalfont\footnotesize{IID MNIST}\label{subfig-1:uniform}]{%
    \includegraphics[width=0.33\textwidth]{figures/mnist_uniform.png}
  }
  \subfloat[\normalfont\footnotesize{Non-IID (labels quantity skew) MNIST}\label{subfig-2:labels}]{%
    \includegraphics[width=0.33\textwidth]{figures/mnist_labels.png}
  }
  \subfloat[\normalfont\footnotesize{Non-IID (covariate shift) MNIST}\label{subfig-3:covariate}]{%
    \includegraphics[width=0.33\textwidth]{figures/mnist_covariate.png}
  }
  \hfill
  \subfloat[\normalfont\footnotesize{IID CIFAR10}\label{subfig-1:cifaruniform}]{%
    \includegraphics[width=0.33\textwidth]{figures/cifar10_uniform.png}
  }
  %\hfill
  \subfloat[\normalfont\footnotesize{Non-IID (labels quantity skew) CIFAR10}\label{subfig-2:cifarlabels}]{%
    \includegraphics[width=0.33\textwidth]{figures/cifar10_labels.png}
  }
  \subfloat[\normalfont\footnotesize{Non-IID (covariate shift) CIFAR10}\label{subfig-3:cifarcovariate}]{%
    \includegraphics[width=0.33\textwidth]{figures/cifar10_covariate.png}
  }
  \caption{\textbf{Accuracies of ResNet-18 on the uniform and non-IID cases.} BN, GN, and LN require few rounds to reach convergence in uniform setting, while they need more training steps to converge in non-IID scenarios. It is clearly shown that BRN requires a very long learning curve and that IN does not learn in FL.}
  \label{fig:non-iid}
\end{figure*}





\subsection{Normalization Layers and batch size}
\label{ssec:norm-layers-batch-size}

\begin{table}
\centering
\caption{\label{tab:BATCH}Accuracy in the labels quantity skew setting as the batch size varies.\\}
\label{fig:BATCH}
\begin{tabular}{lrccc} 
\toprule
\textbf{Dataset} & \textbf{Batch size}     & \textbf{BN}  & \textbf{GN} & \textbf{LN}\\ 

\midrule
\multirow{7}{*}{MNIST} &  8    & $94.77\%$               & $97.85\%$               & $98.32\%$                    \\
 
& 16        & $81.02\%$               & $97.92\%$               & $98.83\%$                    \\

& 32        & $98.00\%$               & $97.93\%$               & $98.36\%$                    \\ 

& 64        & $73.89\%$               & $95.91\%$               & $98.13\%$                 \\

& 128       & $87.07\%$               & $96.84\%$               & $97.39\%$                \\

& 256       & $90.33\%$               & $97.22\%$               & $97.39\%$                \\

& 512       & $87.86\%$               & $97.97\%$               & $96.99\%$               \\
\midrule
\multirow{7}{*}{CIFAR10} &  8   & $40.23\%$               & $59.95\%$               & $66.63\%$                    \\

& 16        & $45.41\%$               & $61.87\%$               & $56.42\%$                    \\

& 32        & $42.26\%$               & $55.61\%$               & $52.70\%$                    \\ 

& 64        & $43.02\%$               & $59.43\%$               & $55.07\%$                 \\

& 128       & $38.02\%$               & $60.17\%$               & $60.36\%$                \\

& 256       & $46.69\%$               & $56.18\%$               & $56.92\%$                \\

& 512       & $35.86\%$               & $51.92\%$               & $59.73\%$               \\
\bottomrule
\end{tabular}
\end{table}

\iffalse
\begin{figure*}[!ht]
  \subfloat[\normalfont\footnotesize{Batch = 8}\label{subfig-1:batch8}]{%
    \includegraphics[width=0.33\textwidth]{figures/mnist_batch8.png}
  }
  \hfill
  \subfloat[\normalfont\footnotesize{Batch = 128}\label{subfig-2:batch128}]{%
    \includegraphics[width=0.33\textwidth]{figures/mnist_batch128.png}
  }
  \subfloat[\normalfont\footnotesize{Batch = 512}\label{subfig-3:batch512}]{%
    \includegraphics[width=0.33\textwidth]{figures/mnist_batch512.png}
  }
  \caption{\textbf{Accuracies on MNIST and different batch sizes.} Accuracy degrades when the batch size becomes too large.}
  \label{fig:batch-size}
\end{figure*}
\fi

\begin{figure}[H]
  \includegraphics[width=0.48\textwidth]{figures/cifar10_batch_histo.png}
  \caption{\textbf{Accuracies on CIFAR10 and different batch sizes.} Accuracy degrades when the batch size becomes too large.}
  \label{fig:batch-size}
\end{figure}

We examined the effect of a range of batch sizes on training NNs with different normalization layers (Tab. \ref{tab:BATCH} and Fig.\ref{fig:batch-size}). We trained ResNet-18 on both MNIST and CIFAR10 with a batch size of 8, 16, 32, 64, 128, 256 and 512.

In Tab.\ref{tab:BATCH} we can see that GN and LN variants of ResNet-18 consistently outperform BN (batch sizes 8 and 16). In all three variants, the accuracy degrades when the batch size becomes too large (in almost all cases, there is a significant drop in performance when passing batch size 256 to 512). A possible explanation for this phenomenon is that, as stated in~\cite{keskar2017onlarge}, "the lack of generalization ability is due to the fact that large-batch methods tend to converge to sharp minimizers of the training function". This is especially true in contexts such as FL, where clients have fewer data than in centralized scenarios, and therefore increasing the batch size has a greater effect.

\subsection{Normalization layers and number of epochs per round}
\label{ssec:norm-layers-epochs-round}

We considered two types of experiments to study how accuracy is affected by the number of epochs per round:
\begin{itemize}
	\item Fix the number of rounds and increase the number of local training steps (Tab.\ref{fig:EPOCHS}).
	\item Fix the number of training epochs to 1000 and vary the ratio of epochs to rounds (Tab. \ref{fig:norm-epochs-round}).
\end{itemize}

It can be noted that models take benefit from more local steps of gradient descent before doing aggregation. Indeed, accuracy increases as E increases. A possible explanation is that this happens because clients of the federation share a similar loss function shape, and going more and more towards the local minima can be beneficial to reach global optima. 

Interestingly, when E=1, BN converges quickly, while GN and LN require more training steps to converge. However, when E increases to 10 or 100, BN also requires more rounds to reach convergence, while the learning curves of GN and LN are unaffected by significant changes.

These results can also be analyzed from a communication point of view: with the same amount of epochs, less communication achieves better results. For example, on CIFAR10, ResNet-GN with E=2 and 500 rounds achieves higher accuracy than ResNet-GN with E=1 and 1000 rounds (Fig.\ref{fig:epochs_fixed}). 
This means that perhaps counter-intuitively, training locally before performing aggregation can boost the modelâ€™s accuracy. This seems to indicate that pursuing local optimizations can lead to better approximations of the local optima. However, at a certain point, increasing E and reducing the number of rounds decreases the performance. This pattern is clearly visible with all the normalization layers and in both datasets. \Cref{fig:norm-epochs-round} shows that we always need an appropriate ratio of epochs to round. 


\begin{figure*}[!ht]
  \subfloat[BN\label{subfig-1:epochs1}]{%
    \includegraphics[width=0.33\textwidth]{figures/cifar10_epochs1.png}
  }
  \hfill
  \subfloat[GN\label{subfig-2:epochs10}]{%
    \includegraphics[width=0.33\textwidth]{figures/cifar10_epochs10.png}
  }
  \subfloat[LN\label{subfig-3:epochs100}]{%
    \includegraphics[width=0.33\textwidth]{figures/cifar10_epochs100.png}
  }
  \caption{\textbf{Accuracies on CIFAR10 and different epochs per round.} Accuracy increases as BN converges lates as E increases, while GN and LN follow an inverse pattern.}
  \label{fig:epochs}
\end{figure*}

\begin{figure*}[!ht]
  \subfloat[BN\label{subfig-1:epochs_bn}]{%
    \includegraphics[width=0.33\textwidth]{figures/cifar10_bn_epochs.png}
  }
  \hfill
  \subfloat[GN\label{subfig-2:epochs_gn}]{%
    \includegraphics[width=0.33\textwidth]{figures/cifar10_gn_epochs.png}
  }
  \subfloat[LN\label{subfig-3:epochs_ln}]{%
    \includegraphics[width=0.33\textwidth]{figures/cifar10_ln_epochs.png}
  }
  \caption{\textbf{Accuracies on CIFAR10 fixing the number of epochs to 1000 and varying the ratio of epochs to rounds.}}
  \label{fig:epochs_fixed}
\end{figure*}


\begin{table}
\centering
\caption{\label{tab:EPOCHS}Accuracy in the labels quantity skew setting as the number of epochs per round varies.\\}
\label{fig:EPOCHS}
\begin{tabular}{lrcccc} 
\toprule
\textbf{Dataset} & \textbf{Epochs}  & \textbf{BN}  & \textbf{GN} & \textbf{LN}\\ 

\midrule
\multirow{3}{*}{MNIST} &   1       & $87.07\%$               & $96.84\%$               & $97.39\%$                    \\

& 10     & $79.82\%$               & $98.83\%$               & $98.95\%$                  \\

& 100    & $92.88\%$               & $97.83\%$               & $99.04\%$                 \\

\midrule
\multirow{3}{*}{CIFAR10} & 1     & $38.02\%$               & $60.17\%$               & $60.36\%$                    \\

& 10     & $55.53\%$               & $54.89\%$               & $63.68\%$                  \\

& 100    & $54.19\%$               & $73.32\%$               & $68.40\%$                 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}
\centering
\caption{\label{tab:norm-epoch-round}Comparison between different epochs per rounds in labels quantity skew setting.\\}
\label{fig:norm-epochs-round}
\begin{tabular}{lrcccc} 

  
\toprule
\textbf{Dataset} &  \textbf{Epochs}      &  \textbf{Rounds}                     & \textbf{BN} & \textbf{GN} & \textbf{LN}  \\ 
\midrule
\multirow{7}{*}{MNIST} &
1       & 1000                    & $91.15\%$                & $98.92\%$               & $98.77\%$           \\
             

& 2       & 500                    & $84.68\%$               & $98.85\%$               & $99.05\%$           \\


& 5        & 200                   & $92.42\%$               & $98.33\%$               & $99.12\%$        \\


& 10        & 100                  & $79.40\%$               & $98.46\%$               & $98.69\%$    \\


& 20        & 50                   & $77.86\%$               & $97.17\%$               & $97.26\%$         \\


& 50        & 20                   & $54.18\%$               & $79.77\%$               & $74.40\%$    \\


& 100        & 10                  & $60.29\%$               & $78.02\%$               & $78.86\%$            \\
\midrule
\multirow{7}{*}{CIFAR10} &
1       & 1000                    & $43.12\%$               & $56.94\%$               & $57.03\%$           \\
             

& 2       & 500                   & $50.09\%$               & $65.14\%$               & $62.63\%$           \\


& 5        & 200                  & $47.85\%$               & $63.68\%$               & $64.56\%$        \\


& 10        & 100                 & $51.37\%$               & $51.23\%$               & $61.22\%$    \\


& 20        & 50                  & $48.16\%$               & $54.33\%$               & $65.36\%$         \\


& 50        & 20                  & $38.92\%$               & $56.53\%$               & $52.67\%$    \\


& 100        & 10                 & $33.06\%$               & $48.24\%$               & $50.56\%$            \\

\bottomrule
\end{tabular}
\end{table}

\subsection{Normalization layers and number of clients}
\label{ssec:norm-layers-number-clients}

\begin{table}
\centering\caption{\label{tab:clients}Accuracy in the labels quantity skew setting as the number of collaborators varies.\\}
\begin{tabular}{lrccc} 
\toprule
\textbf{Dataset} & \textbf{Clients}  & \textbf{BN}  & \textbf{GN} & \textbf{LN}\\ 

\midrule
\multirow{4}{*}{MNIST} &   2         & $99.02\%$               & $99.61\%$               & $99.85\%$                    \\

& 4    & $91.85\%$               & $99.47\%$               & $98.36\%$                 \\

& 8    & $88.38\%$               & $96.49\%$               & $97.68\%$                \\

& 10   & $87.07\%$               & $96.84\%$               & $97.39\%$            \\
                           
\midrule
\multirow{4}{*}{CIFAR10} & 2         & $44.57\%$               & $74.15\%$               & $75.63\%$                    \\

& 4    & $55.30\%$               & $50.50\%$               & $67.69\%$                 \\

& 8    & $51.27\%$               & $60.75\%$               & $65.00\%$                \\

& 10   & $38.02\%$               & $60.17\%$               & $60.36\%$                \\
\bottomrule
\end{tabular}
\end{table}


We tested the scalability of FL by measuring the effect of the number of clients of the federation, as shown in Fig.~\ref{fig:CLIENTS}, and considering two types of experiments:
\begin{itemize}
	\item a labels quantity skew split of the dataset across a different number of clients (namely 2, 4, 8 and 10). Results are reported in \Cref{tab:clients}. 
	\item a uniform dataset split across clients, but considering only some parties. Here the idea is to show how increasing the number of participants, and so the quantity of data, can be beneficial to the federation. Results are reported in \Cref{tab:uniform_clients}.
\end{itemize}

We can observe (\Cref{tab:clients}) that the accuracy significantly increases when decreasing the number of clients. Indeed, when the number of parties is small, the amount of local data increases, leading to better local models, and also aggregating fewer models can result in less information loss. Moreover, we can note the importance of normalization layers in FL: GN and LN variants of ResNet-18 in the ten clients scenario perform better than BN in a two clients scenario on CIFAR10, while on MNIST there is only a slightly drop. \\
\Cref{tab:uniform_clients} shows the results in an IID scenario considering only some shards of the dataset. In this case, the amount of local data remains the same in each configuration; however, the federation's total amount of data varies according to the number of parties. Increasing the quantity of data in the federation by increasing the number of clients benefits the aggregated model. 

\begin{figure}[H]
  \includegraphics[width=0.48\textwidth]{figures/cifar10_2vs10clients.png}
  \caption{\textbf{Accuracies on CIFAR10 and different number of clients.} Accuracy of ResNet-18-BN on 2 parties is still lower than ResNet-18-GN/LN on 10 parties.}
  \label{fig:CLIENTS}
\end{figure}


\iffalse
\begin{table}
\centering\caption{\label{tab:weak_clients}Accuracy in the labels quantity skew setting using only some shards of the dataset.\\}
\label{tab:weak_clients}
\begin{tabular}{lrcccc} 
\toprule
\textbf{Dataset} & \textbf{Clients}      &  \textbf{Rounds}                     & \textbf{BN}  & \textbf{GN} & \textbf{LN}\\ 

\midrule
\multirow{8}{*}{MNIST} &
\multirow{2}{*}{2}       
                           & 20                   & 96.43\%               & \textbf{99.80\%}               & 98.94\%                    \\
                           & & 200                  & 97.32\%              &  \textbf{99.80\%}               & 99.32\%               \\ 
\cline{2-6}
& \multirow{2}{*}{4}  
                           & 20                   & 94.59\%               & 97.46\%               & \textbf{97.59\%}                    \\
                           & & 200                  & 96.09\%              &  98.12\%               & \textbf{98.32\%}               \\ 
\cline{2-6}
& \multirow{2}{*}{8}  
                           & 20                   & 97.39\%               & 96.75\%               & \textbf{97.83\%}                    \\
                           & & 200                  & 98.14\%              &  97.78\%               & \textbf{98.76\%}               \\ 
\cline{2-6}
& \multirow{2}{*}{10}  
                           & 20                   & \textbf{80.75\%}               & 77.54\%               & 66.29\%                    \\
                           & & 200                  & 86.65\%              &  96.83\%               & \textbf{97.39\%}               \\ 
                           
\midrule
\multirow{8}{*}{CIFAR10} &
\multirow{2}{*}{2}       
                           & 20                   & \textbf{56.88\%}               & 48.31\%               & 45.18\%                    \\
                           & & 200                  & 62.99\%              &  58.98\%               & \textbf{67.06\%}               \\ 
\cline{2-6}
& \multirow{2}{*}{4}  
                           & 20                   & 35.70\%               & \textbf{54.49\%}               & 50.47\%                    \\
                           & & 200                  & 39.86\%              &  \textbf{61.13\%}               & 50.92\%               \\ 
\cline{2-6}
& \multirow{2}{*}{8}  
                           & 20                   & 38.72\%               & \textbf{52.87\%}               & 32.22\%                    \\
                           & & 200                  & 43.39\%              &  \textbf{60.94\%}               & 55.63\%               \\ 
\cline{2-6}
& \multirow{2}{*}{10}  
                           & 20                   & 32.63\%               & \textbf{45.51\%}               & 33.16\%                    \\
                           & & 200                  & 34.90\%              &  \textbf{59.85\%}               & 57.55\%               \\ 
\bottomrule
\end{tabular}
\end{table}
\fi

\begin{table}
\centering\caption{\label{tab:uniform_clients}Accuracy in the \textit{i.i.d.} setting using only some shards of the dataset.\\}
\begin{tabular}{lrccc} 
\toprule
\textbf{Dataset} & \textbf{Clients}  & \textbf{BN}  & \textbf{GN} & \textbf{LN}\\ 

\midrule
\multirow{4}{*}{MNIST} &   2         & $94.64\%$               & $96.26\%$               & $97.38\%$                    \\

& 4    & $96.18\%$               & $97.52\%$               & $98.10\%$                 \\

& 8    & $96.79\%$               & $97.98\%$               & $98.34\%$                \\

& 10   & $97.12\%$               & $98.26\%$               & $98.51\%$            \\
                           
\midrule
\multirow{4}{*}{CIFAR10} & 2         & $41.80\%$               & $45.37\%$               & $51.73\%$                    \\

& 4    & $43.47\%$               & $48.74\%$               & $54.55\%$                 \\

& 8    & $45.81\%$               & $53.18\%$               & $58.36\%$                \\

& 10   & $47.12\%$               & $53.99\%$           & $59.06\%$                \\
\bottomrule
\end{tabular}
\end{table}



\section{Conclusions}
\label{sec:conclusions}
This work aims to improve the effectiveness of federated learning, focusing on hyper-parameter optimization, starting from understanding which hyper-parameters specifically affect the training of a federated model differently from centralized training. We specifically focused on layer normalization, which is also a hyper-parameter of centralized training, and frequency of model aggregation, which is not an issue in centralized training. 

We experimented with two network architectures and five normalization layers on two public image datasets. We tested Batch, Group, Instance, Layer Normalization and Batch Renormalization in the uniform, label quantity skew and covariate shift settings. Although BN is the state-of-the-art for classical ML techniques, in our experiments, GN and LN outperformed the other normalization layers in all the FL partitioning strategies. 

Through extensive experimentation, we analyzed how the batch size, the number of epochs per round, the number of rounds and the number of clients of the federation affect the aggregated model performance. These additional tests have been conducted in the labels quantity skew scenario, which is the most challenging for FL algorithms, considering the best three normalization layers: BN, GN and LN. 

GN and LN outperform BN in almost all the tests. Results show that regardless of the batch size, GN and LN consistently outperform BN, although batch size affects the model's performance in all cases. Unexpectedly, we observed that the plot of the quality of the model against the frequency of model aggregation (epochs per round) consistently exhibits a maximum at a few epochs per round. For FL, the number of epochs per round exhibits similar behaviour of batch size for centralized training. 

Eventually, we tested the scalability of FL systems. We noted that FL is not scalable under strong scalability assumption, i.e. increasing the number of clients while maintaining constant the size of local datasets. However, GN and LN on ten clients still outperform BN on two clients. The scalability has also been tested in the IID scenario under the weak scalability assumption, i.e. increasing the number of clients while maintaining constant the size of the local dataset per client. In this case, the federation's data changes with the number of clients, and the model's performance increases with the number of parties.

\section*{Acknowledgements}
\label{sec:acknowledgements}
This work has been supported by the Spoke "FutureHPC $\&$ BigDataâ€ of the ICSC â€“ Centro Nazionale di Ricerca in "High Performance Computing, Big Data and Quantum Computing", funded by European Union â€“ NextGenerationEU, and by the European Union within the H2020 RIA â€œEuropean Processor Initiativeâ€”Specific Grant Agreement 2â€ G.A. 826647, \href{https://www.european-processor-initiative.eu/}{https://www.european-processor-initiative.eu/}.

%% Define the bibliography file to be used
%\bibstyle{spmpsci}
\bibliographystyle{IEEEtran}
\bibliography{main}





\begin{IEEEbiographynophoto}{Bruno Casella}
is currently a PhD student in Modeling and Data Science, at the University of Turin.
He received the B.S. degree in Computer Engineering in 2020, and the M. Sc. in Data Science for management from the University of Catania. His current research interests include federated learning, transfer learning and distributed computing.
\end{IEEEbiographynophoto}

x\begin{IEEEbiographynophoto}{Roberto Esposito}
is an associate professor at the University of Turin. He received the PhD in Computer Science in 2003 from the University of Turin.

\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Antonio Sciarappa}
is currently a researcher at the Leonardo Labs of Genova. He received the PhD in Theoretical Particle Physics in 2015 from the SISSA. His current research interests include parallel and distributed computing.

\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Carlo Cavazzoni}
is currently Senior Vice President of Cloud Computing and Head of the High Performance Computing of Leonardo S.p.A. He received the PhD in Computational Physics from the SISSA.

\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Marco Aldinucci}
is a full professor and P.I. of the Parallel Computing research group at the University of Turin. He received his PhD from the University of Pisa (2003), and he has been a researcher at the Italian National Research Council (CNR). 


\end{IEEEbiographynophoto}

\section*{Appendix}
\label{sec:appendix}
\noindent Accuracies on the uniform and non-iid data settings using and EfficientNet-B0~\cite{tan2019efficientnet}.

\begin{table}[H]
%\centering
\caption{\label{tab:UNIFORM-efficient}Accuracy in the uniform setting with EfficientNet-B0\\}
\label{tab:UNIFORM-efficient}
\begin{tabular}{lccccc} 
\toprule
\textbf{Dataset}   & \textbf{BN}  & \textbf{GN}  & \textbf{IN} & \textbf{LN} & \textbf{BRN}\\ 

\midrule
MNIST     & $65.09\%$               & $95.07\%$             & $11.26\%$                 & $96.12\%$          & $11.02\%$                 \\
\midrule     
CIFAR10   & $29.88\%$               & $29.93\%$            & $10.08\%$                 & $38.98\%$          & $10.48\%$                \\     
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
%\centering
\caption{\label{tab:LABELSSKEW-efficient}Accuracy in the labels quantity skew setting with EfficientNet-B0\\}
\label{tab:LABELSKEW-efficient}
\begin{tabular}{lccccc} 
\toprule
\textbf{Dataset}   & \textbf{BN}  & \textbf{GN}  & \textbf{IN} & \textbf{LN} & \textbf{BRN}\\ 

\midrule
MNIST     & $55.83\%$               & $91.91\%$             & $24.13\%$                 & $94.35\%$          & $45.69\%$                 \\
\midrule     
CIFAR10   & $29.98\%$               & $27.53\%$            & $14.92\%$                 & $37.71\%$          & $22.61\%$                \\     
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
%\centering
\caption{\label{tab:COVARIATE-efficient}Accuracy in the covariate shift setting with EfficientNet-B0\\}
\label{tab:COVARIATE-efficient}
\begin{tabular}{lccccc} 
\toprule
\textbf{Dataset}   & \textbf{BN}  & \textbf{GN}  & \textbf{IN} & \textbf{LN} & \textbf{BRN}\\ 

\midrule
MNIST     & $65.10\%$               & $94.86\%$             & $11.19\%$                 & $95.81\%$          & $61.90\%$                 \\
\midrule     
CIFAR10   & $26.78\%$               & $28.01\%$            & $10.16\%$                 & $48.10\%$          & $10.01\%$                \\     
\bottomrule
\end{tabular}
\end{table}

\end{document}


