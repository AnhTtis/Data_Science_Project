\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{subfig}
\usepackage{graphicx}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{float}
\usepackage{makecell}
\usepackage{caption}
\usepackage{booktabs}
\usepackage[table]{xcolor}

\usepackage{multirow}
\usepackage{tabularx,verbatim}
% \usepackage{tablestyles}
\usepackage{xspace}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{color, xcolor} % colors

\definecolor{mygray}{gray}{.9}
\definecolor{honeydew}{rgb}{0.94, 1.0, 0.94}
\definecolor{magicmint}{rgb}{0.67, 0.94, 0.82}
\definecolor{lightcyan}{rgb}{0.88, 1.0, 1.0}
\definecolor{lightgreen}{rgb}{0.86, 0.82, 1.0}
\newcommand{\rh}[1]{{\color{green}[rh: #1]}}
\newcommand{\yh}[1]{{\color{cyan}[yh: #1]}}

\usepackage[pagebackref=false, breaklinks=true, letterpaper=true, colorlinks, urlcolor=gray,
            citecolor=citecolor, linkcolor=linkcolor, bookmarks=false]{hyperref}
\definecolor{citecolor}{HTML}{0071BC}
\definecolor{linkcolor}{HTML}{ED1C24}

\colorlet{dark-green}{green!80!black}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\textcolor{dark-green}{\ding{51}}}%
\newcommand{\xmark}{\ding{55}}%


\newcolumntype{*}{>{\global\let\currentrowstyle\relax}}
\newcolumntype{^}{>{\currentrowstyle}}
\newcommand{\rowstyle}[1]{\gdef\currentrowstyle{#1}#1\ignorespaces}

\definecolor{dt}{gray}{0.7}  %
\definecolor{lightgreen}{HTML}{D8ECD1}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
% \usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{1747} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\newcommand{\yy}[1]{\textcolor{blue}{yuyang: #1}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
% \ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE

\title{Revisit Parameter-Efficient Transfer Learning: A Two-Stage Paradigm}
% Revisit Parameter-Efficient Tuning: A Two-Stage Approach
% \author{Hengyuan Zhao\\
% National University of Singapore\\
% {\tt\small hengyuan.z@u.nus.edu}
% \and
% Hao Luo\\
% Alibaba-Inc\\
% {\tt\small michuan.lh@alibaba-inc.com}
% \and
% Yuyang Zhao\\
% National University of Singapore\\
% {\tt\small yuyang.zhao@u.nus.edu}
% \and
% Pichao Wang\\
% Alibaba-Inc\\
% {\tt\small pichaowang@gmail.com}
% \and
% Fan Wang\\
% Alibaba-Inc\\
% {\tt\small fan.w@alibaba-inc.com}
% \and
% Mike Zheng Shou\\
% National University of Singapore\\
% {\tt\small mikeshou@nus.edu}
% }
\author{Hengyuan Zhao$^1$,
Hao Luo$^2$,
Yuyang Zhao$^3$,
Pichao Wang$^2$,
Fan Wang$^2$,
Mike Zheng Shou$^1$\\
$^1$Show Lab, National University of Singapore, $^2$Alibaba Group, $^3$ National University of Singapore\\
{\tt\small (hengyuan.z, yuyang.zhao)@u.nus.edu, (michuan.lh, fan.w)@alibaba-inc.com} \\ 
{\tt\small (pichaowang, mike.zheng.shou)@gmail.com}
}





\maketitle
% Remove page # from the first page of camera-ready.
% \ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\begin{abstract}

    Parameter-Efficient Transfer Learning (PETL) aims at efficiently adapting large models pre-trained on massive data to downstream tasks with limited task-specific data.  
    In view of the practicality of PETL, previous works focus on tuning a small set of parameters for each downstream task in an end-to-end manner while rarely considering the task distribution shift issue between the pre-training task and the downstream task.
    In this paper, we propose a novel two-stage paradigm, where the pre-trained model is first aligned to the target distribution, and then the task-relevant information is leveraged for effective adaptation.
    Specifically, the first stage is to narrow the task distribution shift by tuning the scale and shift in the LayerNorm layers. In the second stage, to efficiently learn the task-relevant information, we propose a Taylor expansion-based importance score to identify task-relevant channels for the downstream task and then only tune such a small portion of channels, making the adaptation to be parameter-efficient.
    Overall, we present a promising new direction for PETL, and the proposed paradigm achieves state-of-the-art performance on the average accuracy of 19 downstream tasks. Codes will be available \href{https://github.com/showlab/TTC-Tuning}{here}. 

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

% Large vision transformer models, such as those proposed in~\cite{dosovitskiy2020image, liu2021swin, he2021masked}, have demonstrated exceptional performance on image classification tasks. Inspired by the success of large language models, such as GPT3~\cite{brown2020language}, T5~\cite{raffel2020exploring}, and Gshard\cite{lepikhin2020gshard}, in applied general intelligence, there is growing interest in leveraging large vision transformer models for computer vision applications. 
% 
Large vision transformer models~\cite{dosovitskiy2020image,liu2021swin,pvt} have demonstrated exceptional performance on large-scale image classification tasks~\cite{deng2009imagenet}.
Inspired by the successful usage of large language models~\cite{devlin2018bert,brown2020language,raffel2020exploring,lepikhin2020gshard}, there is a growing interest in leveraging the pre-trained knowledge from large vision transformer models for downstream tasks.
The most common and direct way is to fine-tune the whole model on the small downstream dataset.
Nevertheless, fine-tuning all the parameters (\textit{aka} full fine-tuning) on a small dataset can lead to two severe challenges: (1) full fine-tuning is prone to overfitting when the tuned massive weights of pre-trained models are not comparable with the limited downstream training data; (2) the high computation costs and storage requirements of a large number of model parameters (since each task requires storing a separate model) make it harder to be applied to extreme storage-constrained devices.

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.86\linewidth]{paperfigs/paradigm_v2.pdf}
\end{center}
 \vspace{-0.2in}
\caption{An illustration of our new paradigm.}
\label{fig:paradigm}
\vspace{-0.2in}
\end{figure}



% \begin{figure}[t]
% \begin{center}
% \includegraphics[width=0.9\linewidth]{paperfigs/distribution_c.pdf}
% \end{center}
%  \vspace{-0.1in}
% \caption{t-SNE visualization of final \textsc{[CLS]} token of test set from SVHN, EuroSAT and Clevr datasets. Before and after tuning refer to the process of tuning the LayerNorm layers. When training linear heads, there is often a domain gap in different types of downstream tasks, and the features are less discriminating. However, after tuning the LayerNorm layers, the \textsc{[CLS]} token distributions become more compact, resulting in significant improvements in performance.} 
% % While the Full finetuning and SSF approaches achieved accuracy scores of 87.4\% and 90.2\%, respectively, using only LayerNorm resulted in even higher performance. With the additional use of our Taylor-score based method, we were able to further improve performance to 91.6\%.
% \label{fig:distribution}
% \vspace{-0.1in}
% \end{figure}

% \begin{figure}[t]
% \begin{center}
% \includegraphics[width=0.3\linewidth]{paperfigs/svhn_ic_bar.pdf}
% \includegraphics[width=0.3\linewidth]{paperfigs/EuroSAT_ic_bar.pdf}
% \includegraphics[width=0.3\linewidth]{paperfigs/CLEVR_ic_bar.pdf}
% \end{center}
%  \vspace{-0.1in}
% \caption{We have identified important channels from the last layer of ViT-B on SVHN, EuroSAT, and Clevr datasets. This suggests that different tasks may prioritize different channels within the same layer. The yellow lines represent the important channels, with the corresponding value indicating the absolute value of importance score.}
% \label{fig:ic-show}
%  \vspace{-0.2in}
% \end{figure}


% One main limitation of VPT is that it relies on hand-crafted selection to determine the optimal prompt length for each task. This can be inflexible when applying the method to new tasks, as it can be computationally expensive to search for the best prompt length. Different from prompt method, Adapter-Like methods \cite{chen2022adaptformer, adapter, jie2022convolutional} inject a MLP-like module alongside the multi-head self-attention or MLP block in ViT \cite{dosovitskiy2020image}. However, the number of trainable parameters in these methods is not small and produce inferior performance, regardless of whether the structure of the residual module is MLP-like \cite{adapter, chen2022adaptformer} or convolution-like \cite{jie2022convolutional}.


% Recent research has proposed parameter-efficient transfer leaning (PETL) methods, such as VPT~\cite{vpt}, Adapter~\cite{adapter}, and SSF~\cite{SSF}, that show promising results surpass the full fine-tuning by only tuning a small number of parameters for downstream tasks.
% In addition to prompt-based and adapter-based methods, a strong baseline called SSF has been proposed and achieved high performance by only scaling and shifting the features in each transformer layer. 
To address the above two problems, recent research works investigate parameter-efficient transfer learning (PETL)~\cite{vpt,adapter,SSF}, aiming at efficiently adapting large models to downstream tasks with limited data. 
Mainstream PETL methods can be categorized into two types:
% There are two mainstream types for PETL methods:
(1) designing additional modules (adapter~\cite{adapter} or visual prompt~\cite{vpt}) to learn task-relevant information; (2) narrowing the task distribution shift between the pre-trained and downstream tasks via feature scaling and shifting~\cite{SSF}.
% Inspired by the effectiveness of the two approaches, we revisit the PETL from the perspective of \textcolor{red}{V1: distribution shift and present a novel two-stage paradigm in this paper,} \ie, first narrowing the global distribution shift and then exploring the task-relevant information.
Inspired by the effectiveness of the two approaches, we revisit PETL from the perspective of both task distribution shift and add the task-relevant module and present a novel two-stage paradigm called TTC-Tuning in this paper.

For narrowing task distribution shift, SSF~\cite{SSF} inserts additional scale and shift parameters into MLP, MHSA and LayerNorm components to modulate the features. 
Instead of using additional parameters, tuning normalization layers is a common way to align distributions in transfer learning tasks~\cite{wang2020tent}. Thus, we follow the concept of modulating features but propose a more effective and efficient technique to align the task distribution, \ie, tuning the layer normalization (LayerNorm) parameters.
As shown in Fig.~\ref{fig:distribution}, LayerNorm tuning can greatly improve the discrimination ability of the pre-trained model on downstream tasks.
Compared with SSF~\cite{SSF}, LayerNorm tuning uses less than 15\% parameters (0.03M \textit{v.s.} 0.21M) but outperforms SSF by 0.5\% in the absolute top-1 accuracy.
Therefore, we adopt LayerNorm tuning as our first step for task distribution alignment.

% Motivated by this apporach, we investigate the distribution shifts issue and find that tuning LayerNorm layer is a superior tuning method that surpasses SSF's 0.5\% top-1 accuracy while using only 0.03M parameters, compared to SSF's 0.21M parameters. We adopts t-SNE visualization of \textsc{[CLS]} token to compare the distribution between original feature and the feature extracted after tuning the LayerNorm layer in Fig.~\ref{fig:distribution}. 
% % This finding is in line with the results of AdaBN~\cite{li2016revisiting}, which demonstrated that adaptively recomputing the mean and variance of Batch Normalization~\cite{ioffe2015batch} on the target domain could realize domain adaptation without tuning.
% The phenomenon of tuning the parameters of normalization layers is in line with the finding of transfer learning tasks, \eg, test-time adaptation~\cite{wang2020tent}, which modifies the mean and variance to adapt to the testing data.
% Noted, previous PETL methods seldomly noticing the distribution shift phenomenon when fine-tuning on a downstream task. Here, we revisit the PETL from the persepective of distribution shift and consider align the pre-trained to target distribution as the first step of large model transfer learning.

\begin{figure}[t]
\begin{center}
 \vspace{-0.2in}
\includegraphics[width=0.8\linewidth]{paperfigs/distribution_c.pdf}
\end{center}
 \vspace{-0.2in}
\caption{The t-SNE visualization of final \textsc{[CLS]} token of the test set from SVHN, EuroSAT, and Clevr tasks. ``Original" represents the feature extracted from the original backbone while ``w/ LayerNorm" tuning means extracted from the backbone tuned with LayerNorm only.}
% The selected three different tasks from VTAB~\cite{zhai2019large} demonstrate the effectiveness of tuning LayerNorm, which heavily improve performance and make the features more separable.
% While the Full finetuning and SSF approaches achieved accuracy scores of 87.4\% and 90.2\%, respectively, using only Layernorm resulted in even higher performance. With the additional use of our Taylor-score based method, we were able to further improve performance to 91.6\%.
\label{fig:distribution}
\vspace{-0.2in}
\end{figure}

\begin{figure*}[t]
\begin{center}
\vspace{-0.4in}
\includegraphics[width=0.8\linewidth]{paperfigs/importantchannels_c.pdf}
\end{center}
 \vspace{-0.1in}
\caption{We have identified task-relevant channels from the last layer of ViT-B on three tasks. This suggests that different tasks may prioritize different channels within the same layer.}
% The red lines represent the task-relevant channels, with the corresponding value indicating the density value of importance score.
\label{fig:ic-show}
 \vspace{-0.2in}
\end{figure*}

% Furthermore, in addition to aligning the distribution, we also found that some domains, such as medical imaging (Camelyon dataset) and satellite imaging (EuroSAT), deviate significantly from the ImageNet21K pre-trained domain, and simply changing the mean and variance of the features can lead to inferior performance. Considering more general applications of agnostic downstream task, it is necessary to introduce a learnable module to adapt the pre-trained representations as previous PETL approaches proposed. Different from previous PETL methods~\cite{vpt, adapter,lora, jie2022convolutional, noah, chen2022adaptformer} mainly focus on adopting a parameter-efficient tuning module without consider the task-relevant information explicitly and consider all the channels equally, we hypothesize and corroborate experimentally that channel inequality exists in different tasks and we may only need to tune a small portion task-relevant channels. Similar findings also demonstrated by a recent few-shot study \cite{luo2022channel}, channel bias varies across tasks that prioritize different discriminative features in images. 
% % As a result, different tasks may necessitate different channels for adaptation, and it is feasible that not all channels play an equal role in downstream adaptation. 
% To test this hypothesis, we investigated the contribution of individual channels in downstream adaptation. As shown in Fig.~\ref{fig:ic-show}, different tasks have different task-relevant channels in the same layer. To select the task-relevant channels, we propose a taylor expansion-based importance score and utilize a simple linear layer to transform these task-relevant channels.

% \textcolor{red}{V1: Furthermore, after aligning the distribution, some domains, such as medical images (Camelyon dataset~\cite{Veeling2018qh}) and game images (Clevr-Count~\cite{johnson2017clevr}), still deviate significantly from the ImageNet21K pre-trained domain due to the large domain gap. Simply scaling and shifting the feature cannot lead to satisfactory improvement.
% Instead, the specially designed modules~\cite{vpt} can achieve better performance in such situations, which inspires us to introduce task-relevant information in our second stage.  
% Previous PETL methods~\cite{vpt, adapter,lora, jie2022convolutional, noah, chen2022adaptformer} mainly propose parameter-efficient tuning modules to implicitly leverage the task-relevant information by adding tokens or adapting the whole features. 
% In contrast, inspired by the channel bias in few-shot learning~\cite{luo2022channel} and model pruning~\cite{li2016pruning},  we hypothesize and corroborate experimentally that channel inequality exists in different tasks and we can explicitly leverage such task-relevant information to tune a small portion task-relevant channels, which can gain comparable or even better performance.
% To verify this hypothesis, we investigate the contributions of individual channels in downstream adaptation. The contributions are measured by a proposed Taylor expansion-based importance score.
% As shown in Fig.~\ref{fig:ic-show}, different tasks have different task-relevant channels in the same layer. 
% Thus, we can select the task-relevant channels based on the contributions and utilize a simple adapter to transform such channels for efficient adaptation. The contributions are summarized as follows:}
%

Besides aligning the task distribution, TTC-Tuning also considers adding task-relevant module which has been shown to be crucial by previous studies. 
For some challenge datasets, such as medical images (Camelyon dataset~\cite{Veeling2018qh}) and 3D scene images (Clevr-Count~\cite{johnson2017clevr}), only aligning the task distribution lead to inferior improvement due to the large knowledge gap between the downstream task and pre-trained model. Previous PETL methods~\cite{vpt, adapter,lora, jie2022convolutional, noah, chen2022adaptformer, SSF} mainly propose parameter-efficient tuning modules to implicitly leverage the task-relevant information by adding tokens or adapting the whole features. However, these methods treat each parameter as equivalent and just insert some fixed modules to automatically adapt the whole network to the downstream tasks. Here, we raise an essential question: %\textit{can we evaluate the importance of each parameter for a specific downstream task and then fine-tune more task-relevant parameters?}
\textit{can we identify the important parameters for a specific downstream task and then fine-tune only these task-relevant parameters?}

Inspired by the channel bias in few-shot learning~\cite{luo2022channel} and model pruning~\cite{li2016pruning}, we hypothesize and experimentally validate that channel inequality exists in different tasks. 
We can explicitly leverage such task-relevant information to tune only a small portion of task-relevant channels, leading to comparable or even better performance.
To verify this hypothesis, we investigate the contributions of individual channels in downstream adaptation. The contributions are measured by a proposed Taylor expansion-based importance score.
As shown in Fig.~\ref{fig:ic-show}, different tasks have different task-relevant channels in the same layer. 
Thus, we can select the task-relevant channels based on the contributions and utilize a simple adapter to transform such channels for efficient adaptation. 

In summary, our contributions are three-fold:
\begin{itemize}
    \item We propose a new two-stage paradigm to solve the PETL from the perspective of both task distribution shift and add task-relevant tunable module.
    \item We experimentally verify the effectiveness of only tuning the LayerNorm layer to align distributions and develop a novel tuning module that first selects task-relevant channels via the proposed Taylor expansion-based importance score. Such designs lead to a few extra parameters.
    \item Our novel paradigm outperforms the previous state-of-the-art method SSF~\cite{SSF} with a 1.7\% increase in accuracy across 19 downstream tasks. This result highlights the effectiveness of our approach and its potential to make a significant impact in various applications.
\end{itemize}
% \noindent\textbf{Main contributions.}
% % (1) By revisit the previous PETL methods, we propose a new paradigm to solve the PETL that offers a fresh perspective on investigating distribution shifts before learning task-relevant information. 
% % Our new paradigm contains two steps: aligning distribution and learning task-relevant information.
% (1) We propose a new two-stage paradigm to solve the PETL from the perspective of distribution shift, where we first
% narrow the global distribution shift and then explore the task-relevant information.

% (2) We introduce a novel tuning module that first selects task-relevant channels via proposed taylor expansion-based importance score and then tunes such channels for efficient adaptation.
% % for tuning while adding significantly fewer parameters (only 0.13\% of ViT-B). 

% (3) Our novel paradigm outperforms the previous state-of-the-art method SSF~\cite{SSF} with a 1.7\% increase in accuracy across 19 downstream tasks. This result highlights the effectiveness of our approach and its potential to make a significant impact in various applications.



\section{Related Work}
\raggedbottom

\subsection{Vision Transformers}

Transformers \cite{vaswani2017attention} have shown remarkable performance on natural language processing and computer vision tasks. Numerous vision transformers \cite{chen2021crossvit,d2021convit,dong2022cswin,ali2021xcit,fan2021multiscale,han2021transformer,rao2021dynamicvit,yuan2021tokens,touvron2021going,liu2021swin,wang2021kvt,zhou2021elsa} have been proposed following the pioneering work of ViT \cite{dosovitskiy2020image}. Most of these models gradually increase in size to achieve state-of-the-art results and learn rich representations through various architectural designs. 
% Notably, they are primarily trained on natural datasets, making them highly transferable to other domains and tasks.
Adopting these models for downstream tasks significantly reduces the training complexity and delivers promising results rapidly. Given a plain Vision Transformer (ViT)~\cite{dosovitskiy2020image} with $L$ layers and an input image $I \in \mathbb{R}^{3 \times H \times W}$ that first divided into $N$ non-overlapped patches and then passed into an embedding layer projected into $D$ dimensions. Each transformer layer includes a multi-head self-attention block (MHSA) and a multi-perceptron block (MLP). 

\subsection{Parameter-Efficient Transfer Learning}

PETL focuses on adapting the pre-trained model on a downstream task with a few parameters. 
Two lines of PETL approaches have been proposed recently. On the one hand, learning task-relevant information by applying prompts \cite{vpt,liu2022prompt,xing2022class,zheng2022prompt,nie2022pro,wang2022fine} to the input tokens or adding a trainable module \cite{adapter, chen2022adaptformer, jie2022convolutional,chen2022conv, zhang2023multimodal} to adapt pre-trained information have acquired promising results for the performance and efficiency. On the other hand, aligning the distribution between pre-trained and downstream tasks has been shown to be a strong baseline, as demonstrated in~\cite{SSF}.

\noindent \textbf{Task-relevant modules.}
\textbf{VPT}~\cite{vpt} injects the prompts into the transformer layer's input tokens with a small number of extra parameters. However, one main limitation of VPT is that it relies on hand-crafted selection to determine the optimal prompt length for each task. This can be inflexible when applying the method to new tasks. VPT includes two variants VPT-Shallow and VPT-Deep associated with the number of inserted layers. VPT-Shallow only inserts prompts into the first transformer layer $L_1$ and VPT-Deep inserts all the transformer layers. Given the input tokens $x \in \mathbb{R}^{(N + 1) \times D}$ and the prompts $P \in \mathbb{R}^{n \times D}$ that contains $n$ prompts with dimension $D$, we can formulate the combined tokens $x^{\prime}$ is 
\begin{equation}
    x^{\prime} = [x;P],
    \label{eqn:eq2}
\end{equation}
where $x^{\prime} \in \mathbb{R}^{(N+n+1) \times D}$ will be passed into the following MHSA and MLP blocks.

\textbf{Adapter}~\cite{adapter} proposes an MLP-like module, a successful design that 
% first projects the original dimensional features into a smaller dimension with one nonlinear layer and projects it back to the original dimensions. This approach 
adopts a residual pathway to keep the original information and transform task-relevant information by learning a down-projection $W_{down} \in \mathbb{R}^{D^{\prime} \times D}$ (where $D^{\prime} \ll D$) and an up-projection $W_{up} \in \mathbb{R}^{D \times D^{\prime}}$ with a nonlinearity activation operation $\Phi$. Given an input tokens $x^l \in \mathbb{R}^{(N + 1) \times D}$ in $l$-th layer, the output of a adapter block is
\begin{equation}
    x_{out}^l = x^l + [W^l_{up}\Phi(W^l_{down}[x^l]^T)]^T,
    \label{eqn:eq1}
\end{equation}
where $[ \cdot ]^T$ represents transpose operation. However, the number of trainable parameters in Adapter-like methods is not small and produces inferior performance.
Besides, LoRA \cite{lora} optimizes a low-rank decomposition matrix with a low intrinsic dimension to project the matrices of query, key, and value used in the MHSA block in ViT. Furthermore, a neural architecture search algorithm called NOAH \cite{noah} has been proposed, which incorporates Adapter~\cite{adapter}, LoRA~\cite{lora}, and VPT~\cite{vpt} into its network search space.

\noindent \textbf{Narrow task distribution shift.} \textbf{SSF}~\cite{SSF} In addition to the above prompt-based and adapter-based methods, a recently introduced technique called SSF that has shown promising results involves scaling and shifting the features of the pre-trained model. SSF~\cite{SSF} leverages two learnable vectors $\gamma \in \mathbb{R}^D$ and $\beta \in \mathbb{R}^D$ to scale and shift the feature map in each transformer operation (\ie, Linear operation or LayerNorm operation). Assuming the input of SSF module is $x \in \mathbb{R}^{(N+1) \times D}$, the output $y \in \mathbb{R}^{(N+1) \times D}$ can be written as following
\begin{equation}
    y = \gamma \ast x + \beta,
    \label{eqn:eq3}
\end{equation}
where $\ast$ is the Hadamard product. Motivated by this work, we extend this method to tuning the LayerNorm layer to reduce the distribution shifts and demonstrate the effectiveness on multi-downstream tasks.


\section{Approach}

% We suggest a two-stage paradigm for achieving parameter-efficient transfer learning, as illustrated in Fig.~\ref{fig:overview}. In the first stage, we aim to align the distribution by adjusting the LayerNorm layer while keeping the original backbone components fixed. In the second stage, we utilize a Taylor expansion-based Importance Score (TIS) to identify the most relevant channels for the downstream task. In summary, our paradigm offers an efficient and effective approach to knowledge transfer.
We propose a two-stage paradigm for achieving parameter-efficient transfer learning, as shown in Fig.~\ref{fig:overview}. In the first stage, we align the task distribution by tuning the LayerNorm layer while keeping the other components of the original backbone frozen. In the second stage, we use a Taylor expansion-based Importance Score (TIS) to identify the most relevant channels for the downstream task, by computing gradients on the training set with stage1's model. Then, we introduce the TTC-Module, a tunable module that transforms the task-relevant channels while freezes other channels.

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.48\linewidth]{paperfigs/svhn_kl_d.pdf}
\includegraphics[width=0.48\linewidth]{paperfigs/eurosat_kl_d.pdf}
\end{center}
 \vspace{-0.2in}
\caption{We analyzed the distribution of JSD for the \textsc{[CLS]} token on the SVHN and EuroSAT tasks. The JSD between the original feature and the feature generated by SSF is represented by ``SSF," while the JSD between the original feature and the feature produced after tuning the LayerNorm layer is represented by ``LayerNorm Tuning".}
\label{fig:KL-compare}
\vspace{-0.2in}
\end{figure}

\begin{figure*}[t]
\begin{center}
\vspace{-0.2in}
\includegraphics[width=0.82\linewidth]{paperfigs/overview.pdf}
\end{center}
 \vspace{-0.2in}
\caption{An overview of our novel paradigm to parameter-efficient transfer learning.}
\label{fig:overview}
\vspace{-0.2in}
\end{figure*}

\subsection{Narrow the Task Distribution Shift}
In this section, we first briefly review the Layer Normalization (LN)~\cite{ba2016layer}. LN is a widely used normalization technique in transformers~\cite{vaswani2017attention, dosovitskiy2020image} to solve the problem of the inconsistent amount of input tokens in natural language processing tasks and provide valid normalization in the MLP block. We empirically find that for PETL, tuning LayerNorm layer could efficiently change the mean and variance of feature distribution as mentioned in Fig.~\ref{fig:distribution}.
% Unlike Batch Normalization (BN)~\cite{ioffe2015batch} has empirical mean and variance factors which are calculated during training phase and used in testing phase, LN only has two learnable parameters scaling and bias factors. 
% Therefore, when coming a new downstream task, we may need to tune the scaling and bias factors of LN to adapt the whole distribution to downstream domain. 
Assuming the input $x \in \mathbb{R}^{B \times (N +1) \times D}$, the output $y \in \mathbb{R}^{B \times (N +1) \times D}$ can be formulated as:
\begin{equation}
    y = \frac{x - E[x]}{\sqrt{\text{Var}[x]+\epsilon}} \ast \gamma + \beta,
    \label{eqn:eq4}
\end{equation}
where $\gamma$ and $\beta$ are scaling and bias factors, respectively. $E[\cdot]$ and $\text{Var}[\cdot]$ are expectations and variances that will lead to zero mean and unit variance.

Second, we analyze the statistics of the last \textsc{[CLS]} token to compare the effectiveness of tuning LN and SSF module. In particular, we assume the baseline as the original model distribution and compute the distances between this distribution with the distribution of LN tuning and SSF, respectively. Consider two probability distributions are $p$ and $q$, we use Jensen–Shannon Divergence (JSD)~\cite{endres2003new} as the metric to compute the distance $\mathcal{L}$ as:
\begin{equation}
\begin{split}
    \mathcal{L} &= \frac{1}{2}(\mathcal{KL}(\log(p), m) + \mathcal{KL}(\log(q), m)),\\
    m &= (p + q) / 2,
    \label{eqn:eq5}
\end{split}
\end{equation}
where $\mathcal{KL}$ is Kullback–Leibler divergence~\cite{kullback1951information}. 
Figure~\ref{fig:KL-compare} displays the distance distributions, where the blue histogram represents the distance between the original model and the model trained with SSF, while the pink histogram compares the distance of the original model with the LayerNorm model. Examining the JSD distribution, the range and the covariance of SSF are larger than LayerNorm. Notably, a significant number of samples are located at zero, indicating that the distribution is the same as the original model. On the other hand, a considerable number of samples are located far away from the original model, suggesting that SSF may fit some samples while ignoring others. In contrast, our LayerNorm tuning has a more compact distribution and appears unbiased towards any particular sample.
% Since many distances in SSF are zero, SSF is unable to transform certain images away from the original distribution. In contrast, our tuning LayerNorm layer is able to effectively transform the distribution from the source to the target.

\subsection{Task-Relevant Channel Selection using Taylor Expansion-Based Importance Scores}

% After aligning the distribution between the pre-trained and downstream tasks, we observed that certain tasks still do not achieve better performance, despite the distribution being adjusted. We note that the network weights are closely related to the task labels~\cite{li2016revisiting}, and thus we aim to select task-relevant weights by feeding the downstream training set. Various methods~\cite{luo2017thinet,li2016pruning, he2018soft, wangvtc} for selecting network weights have been studied in the fields of network pruning and compression. To this end, we propose a Taylor expansion-based Importance Score (TIS) to evaluate the importance of each weight.

While aligning the distribution between pre-trained and downstream tasks can be effective for small distribution shifts, to handle various task distribution shifts, we need to introduce an extra learnable module that has been proved crucial by other PETL methods proposed~\cite{vpt,adapter,lora,noah}. However, unlike these methods treat each channel equally during fine-tuning, we hypothesize that only tuning a small portion of full channels is enough for adaptation. We note that the network weights are closely related to the task labels as mentioned in ~\cite{li2016revisiting}, and thus we aim to select task-relevant weights by feeding the downstream training set. Various methods~\cite{luo2017thinet,li2016pruning, he2018soft, wangvtc} for selecting network weights have been studied in the fields of network pruning and compression. To this end, we propose a Taylor expansion-based Importance Score (TIS) to evaluate the importance of each weight.

We conjecture that the task-relevant weights highly influence the network output, and removing these weights will drastically influence the loss value. Thus, the importance of weight can be quantified by the difference in loss induced by removing this weight. Given a subset $\{x,y\}$ randomly sampled from training set, the importance score $I_{w_i^j}$ of a weight parameter $w_i^j \in \mathbb{R}^{1 \times 1}$ can be formulated by
\begin{equation}
    I_{w_i^j} = (\mathcal{L}(\mathcal{F}(x, \mathcal{W}), y | w_i^j=0) - \mathcal{L}(\mathcal{F}(x, \mathcal{W}), y))^2,
    \label{eqn:eq6}
\end{equation}
where $\mathcal{L}$ is the task-specific loss (the cross-entropy loss in this paper), $\mathcal{F}$ is the transformer network, $\mathcal{W}$ is the total model weights and $y$ are the labels of data $x$. As previous studies~\cite{molchanov2019importance, wangvtc, yang2021nvit} point out that this score can be approximated with the first-order Taylor expansion. Thus, the final importance score $\hat{I}_{w_i^j}$ of a weight parameter $w_i^j$ can be rewritten as:
\begin{equation}
    \hat{I}_{w_i^j} = \frac{\partial{\mathcal{L}(x)}}{\partial{w_i^j}} \cdot w_i^j.
    \label{eqn:eq7}
\end{equation}
Thus the importance score $\hat{I}_{w_i^j}$ can be represented with a gradient term and the weight parameter $w_i^j$.

 Up to this point, we can use the above score to evaluate the task-relevant weights. However, our method aims to find task-relevant channels of a given feature map. Thus, we need to translate the task-relevant weights to task-relevant channels. As shown in Fig.~\ref{fig:weightm}, we first decompose the process of linear operation. Assuming a weight matrix $\textbf{W}\in \mathbb{R}^{D \times D}$ and a feature map $\textbf{X} \in \mathbb{R}^{(N+1) \times D}$, we can get the output $\textbf{Y}\in \mathbb{R}^{(N+1) \ \times D}$ as:
\begin{equation}
    \textbf{Y} = [\textbf{W}\textbf{X}^T]^T.
    \label{eqn:eq8}
\end{equation}
We define each weight $w_i \in \mathbb{R}^{1 \times D}$ in $\textbf{W}$ and each token $x^T_i \in \mathbb{R}^{D \times 1}$ in $\textbf{X}^T \in \mathbb{R}^{D \times (N+1)}$. Summing the items of output $\textbf{Y}$ in channel-wise we can get:
\begin{equation}
\begin{split}
    \text{Sum}(\textbf{Y}, dim=1) = [&w_1(x^T_1+x^T_2+...+x^T_{N+1}); \\
    &w_2(x^T_1+x^T_2+...+x^T_{N+1});\\
    & ...\\
    &w_D(x^T_1+x^T_2+...+x^T_{N+1});].
    \label{eqn:eq9}
\end{split}
\end{equation}
Thus, we can find that task-relevant weights $w_i$ could represent the task-relevant channels of a given feature map $\textbf{Y}$. Calculating the importance score of a weight $w_i$ could be approximated by summing over Eq.~\ref{eqn:eq7} of all the parameters in $w_i$, \ie, the final importance score $\mathcal{S}_i$ can be calculate as:
\begin{equation}
    \mathcal{S}_i = \sum_{j\in \mathcal{J}}\hat{I}_{w_i^j},
    \label{eqn:eq10}
\end{equation}
where $\mathcal{J}$ represents the index set of a weight $w_i$ and $w_i^j \in \mathbb{R}^{1 \times 1}$ is a parameter in $w_i$. 

% \noindent\textbf{Tuning task-relevant channels \vs task-relevant weights.}
% Note that if we consider $K$ task-relevant weights, the number of extra parameters will be $K \times D$, and the number of FLOPs will be $N \times K \times D$. On the other hand, focusing on $K$ task-relevant channels only requires a $K \times K$ linear layer for tuning, with the FLOPs count of $N \times K \times K$. In this paper, we set our default values as $K=96$ and $D=768$, and the number of extra parameters for tuning task-relevant weights is 8 times higher than that for tuning task-relevant channels. Therefore, to maintain a lower number of extra parameters for storage, it is better to tune task-relevant channels instead. We also attempted to tune the task-relevant weights directly, but the results for 19 downstream tasks were inferior compared to tuning the task-relevant channels as illustrated in Tab.~\ref{tab:vit-vtab-weights-channels}. We hypothesize that the linear combination of task-relevant channels will contribute more to the task performance.

\begin{figure}[t]
\begin{center}
 % \vspace{-0.1in}
\includegraphics[width=0.65\linewidth]{paperfigs/weightmatrix.pdf}
\end{center}
 \vspace{-0.2in}
\caption{Illustration of the Decomposition of a Linear Operation.}
\label{fig:weightm}
\vspace{-0.2in}
\end{figure}

\subsection{Task-Relevant Module}
\label{sec:adapt}
% Having obtained the importance scores to select the top-$K$ task-relevant channels in a feature, it is natural to only fine-tune these channels for efficient model tuning. This approach is named the task-relevant Channel Tuning Module (ICTM), and an overview of the ICTM is depicted in Fig. \ref{fig:}.
% Unlike other parameter-efficient tuning methods, our ICTM only contains a linear layer, rather than an MLP-like adapter \cite{adapter} or the prompt \cite{vpt}. This design is easy to implement. To preserve the original robust representations, we use a residual shortcut to retain the original information. After obtaining the transformed features, we replace the selected task-relevant features with the transformed ones to obtain the final output features. Fig. \ref{fig:} shows two ways of inserting the ICTM into the ViT architecture. ``Ours-MLP" represents inserting the ICTN after the MLP block, while ``Our-Attn" indicates placing the ICTM after the MHSA block but before the MLP block. In the following experiments, "Ours-Attn" is the default injection position compared to other baselines."

Having obtained Taylor expansion-based Importance score, we will select top-$K$ task-relevant channels of each feature map in transformer layers. Assuming a feature map is $x \in \mathbb{R}^{(N+1) \times D}$, we will select $K$ largest value of importance score vector $\mathcal{S}=[\mathcal{S}_1, \mathcal{S}_2, ..., \mathcal{S}_i \in \mathbb{R}^{1 \times 1}, 1\leq i \leq D]$ in this feature map. The selected feature $x^{\prime} \in \mathbb{R}^{(N+1) \times K}$ then be fed into a trainable linear layer and outputs the transformed feature
\begin{equation}
    x^{\prime\prime} = x^{\prime} + \mathrm{Linear}(x^{\prime}),
    \label{eqn:eq11}
\end{equation}
where $x^{\prime\prime} \in \mathbb{R}^{(N+1) \times K}$ will be passed into the next operation in the transformer layer and $\mathrm{Linear}(\cdot)$ is linear layer operation that the only new involved layer with parameters $K \times K$. Here, we adopt a shortcut connection to preserve the original information and prevent error accumulation across the transformer layers. This strategy helps alleviate the training difficulty.
% We name such a plug-and-play tuning module as Taylor-based Task-relevant Channel tuning Module (TTC-Module). An illustration of our tuning module is display in Fig.~\ref{fig:overall}, we use the first-step trained model to determine the task-relevant channels and unfreeze the LayerNorm layer and our proposed TTC-Module during fine-tuning phase.

% \noindent\textbf{Overall}
% To achieve parameter-efficient transfer learning, we propose a two-stage approach, as illustrated in Fig.~\ref{fig:overview}. In the first stage, we focus on aligning the distribution gap by tuning the LayerNorm layer while freezing the other components of the original backbone. In the second stage, we introduce a Taylor-based Importance Score to identify the most relevant channels for the downstream task by calculating gradients on the training set. We then present our plug-and-play tuning module, the TTC-Module, which adapts the task-relevant information to improve model performance. Overall, our paradigm represents an effective and efficient means of transferring knowledge.

\noindent\textbf{Tuning channels \vs Tuning weights.}
Note that if we consider $K$ task-relevant weights, the extra parameters will be $K \times D$, and the FLOPs will be $N \times K \times D$. On the other hand, focusing on $K$ task-relevant channels only requires a $K \times K$ linear layer for tuning, with the FLOPs count of $N \times K \times K$. In this paper, we set our default values as $K=96$ and $D=768$, and the number of extra parameters for tuning task-relevant weights is 8 times larger than that for tuning task-relevant channels. Therefore, to maintain fewer extra parameters for storage, it is better to tune task-relevant channels instead. We also attempted to tune the task-relevant weights directly, but the results for 19 downstream tasks were inferior compared to tuning the task-relevant channels as illustrated in Tab.~\ref{tab:vit-vtab-weights-channels}. We hypothesize that the linear combination of task-relevant channels will contribute more to task performance.

% \noindent\textbf{Compared with Adapter.} 
% Our proposed TTC-Module shares some similarities with Adapter~\cite{adapter}, but there are two key differences between the two approaches. Firstly, while Adapter uses an MLP-like structure with both a down-projection and an up-projection, our tuning module consists of a simple linear layer only. Secondly, Adapter's down-projection is a latent method for selecting task-relevant channels, whereas our tuning module applies an interpretable Taylor expansion-based importance score directly to the original features to select the $K$ most relevant channels. This results in fewer parameters and a significant improvement in final performance compared to Adapter.

% \begin{figure}[t]
% \begin{center}
% \includegraphics[width=0.92\linewidth]{paperfigs/tuningoperation.pdf}
% \end{center}
%  \vspace{-0.1in}
% \caption{Illustration of our proposed task-relevant channel tuning and default inserting position used in this paper.}

% \label{fig:overall}
% \vspace{-0.1in}
% \end{figure}


\begin{table*}[t]
\vspace{-0.1in}
\begin{center}
\large
\resizebox{0.92\linewidth}{!}{
\begin{tabular}{lc|ccccccc|cccc|cccccccc|c}
% \toprule
\multirow{2}{*}{} &  & \multicolumn{7}{c}{\textbf{Natural}} & \multicolumn{4}{c}{\textbf{Specialized}} & \multicolumn{8}{c}{\textbf{Structured}} \\
% \midrule
& \rotatebox{90}{\# Params (M)} & \rotatebox{90}{Cifar100} & \rotatebox{90}{Caltech101}  & \rotatebox{90}{DTD} & \rotatebox{90}{Flower102} & \rotatebox{90}{Pets} & \rotatebox{90}{SVHN} & \rotatebox{90}{Sun397} & \rotatebox{90}{Camelyon} & \rotatebox{90}{EuroSAT} & \rotatebox{90}{Resisc45} & \rotatebox{90}{Retinopathy} & \rotatebox{90}{Clevr-Count} & \rotatebox{90}{Clevr-Dist} & \rotatebox{90}{DMLab} & \rotatebox{90}{KITTI-Dist} & \rotatebox{90}{dSpr-Loc} & \rotatebox{90}{dSpr-Ori} & \rotatebox{90}{sNORB-Azim} & \rotatebox{90}{sNORB-Ele} & \rotatebox{90}{\textbf{Average}}\\
\midrule
\rowstyle{\color{dt}}\multirow{1}{*}{Full~\cite{vpt}} & \rowstyle{\color{dt}}85.8 & \rowstyle{\color{dt}}68.9 & \rowstyle{\color{dt}}87.7 & \rowstyle{\color{dt}}64.3 & \rowstyle{\color{dt}}97.2 & \rowstyle{\color{dt}}86.9 & \rowstyle{\color{dt}}87.4 & \rowstyle{\color{dt}}38.8 & \rowstyle{\color{dt}}79.7 & \rowstyle{\color{dt}}95.7 & \rowstyle{\color{dt}}84.2 & \rowstyle{\color{dt}}73.9 & \rowstyle{\color{dt}}56.3 & \rowstyle{\color{dt}}58.6 & \rowstyle{\color{dt}}41.7 & \rowstyle{\color{dt}}65.5 & \rowstyle{\color{dt}}57.5 & \rowstyle{\color{dt}}46.7 & \rowstyle{\color{dt}}25.7 & \rowstyle{\color{dt}}29.1 & \rowstyle{\color{dt}}65.6\\
% \midrule
% \multirow{1}{*}{Linear} & 0 & 64.4 & 85.0 & 63.2 & 97.0 & 86.3 & 36.6 & 51.0 &  78.5 & 87.5 & 68.5 & 74.0 & 34.3 & 30.6 & 33.2 & 55.4 & 12.5 & 20.0 & 9.6 & 19.2 & 53.0\\
\rowstyle{\color{dt}}\multirow{1}{*}{Linear$^{\star}$} & \rowstyle{\color{dt}}0.04 & \rowstyle{\color{dt}}61.5 & \rowstyle{\color{dt}}88.4 & \rowstyle{\color{dt}}73.9 & \rowstyle{\color{dt}}97.9 & \rowstyle{\color{dt}}86.8 & \rowstyle{\color{dt}}41.8 & \rowstyle{\color{dt}}51.0 & \rowstyle{\color{dt}} 80.7 & \rowstyle{\color{dt}}88.6 & \rowstyle{\color{dt}}76.1 & \rowstyle{\color{dt}}74.1 & \rowstyle{\color{dt}}35.4&\rowstyle{\color{dt}}30.3&\rowstyle{\color{dt}}35.7&\rowstyle{\color{dt}}59.8&\rowstyle{\color{dt}}16.4&\rowstyle{\color{dt}}24.3&\rowstyle{\color{dt}}18.0&\rowstyle{\color{dt}}22.6&\rowstyle{\color{dt}}56.0\\
% \midrule
\multirow{1}{*}{Bias~\cite{vpt}} & 0.14 & 72.8&87.0&59.2&97.5&85.3&59.9&51.4&78.7&91.6&72.9&69.8&61.5&55.6&32.4&55.9&66.6&40.0&15.7&25.1&62.1\\
% \midrule
\multirow{1}{*}{VPT-Shallow~\cite{vpt}} & 0.11 & 77.7&86.9&62.6&97.5&87.3&74.5&51.2&78.2&92.0&75.6&72.9&50.5&58.6&40.5&67.1&68.7&36.1&20.2&34.1&64.9 \\
\multirow{1}{*}{VPT-Deep~\cite{vpt}} & 0.60 & \colorbox{lightgreen}{\textbf{78.8}}&90.8&65.8&98.0&88.3&78.1&49.6&81.8&\colorbox{lightgreen}{\textbf{96.1}}&83.4&68.4&68.5&60.0&46.5&72.8&73.6&47.9&\colorbox{lightgreen}{\textbf{32.9}}&37.8&69.4 \\
\multirow{1}{*}{Adapter~\cite{adapter}} & 0.27 &69.2&90.0&68.0&98.8&89.9&82.8&\underline{54.3}&84.0&94.9&81.9&75.5&\underline{80.9}&\underline{65.3}&48.6&78.3&74.8&48.5&29.9&\underline{41.6}&71.4\\
% \multirow{1}{*}{LoRA} & 0.29 & 67.1&91.4&69.4&98.8&90.4&85.3&54.0&84.9&95.3&84.4&73.6&\colorbox{lightgreen}{\textbf{82.9}}&\colorbox{lightgreen}{\textbf{69.2}}&49.8&78.5&75.7&47.1&\colorbox{lightgreen}{\textbf{31.0}}&\colorbox{lightgreen}{\textbf{44.0}}&72.3 \\
\multirow{1}{*}{SSF~\cite{SSF}} & 0.24 & 69.0&\colorbox{lightgreen}{\textbf{92.6}}&\colorbox{lightgreen}{\textbf{75.1}}&\colorbox{lightgreen}{\textbf{99.4}}&\colorbox{lightgreen}{\textbf{91.8}}&\underline{90.2}&52.9&\underline{87.4}&\underline{95.9}&\colorbox{lightgreen}{\textbf{87.4}}&\underline{75.5}&75.9&62.3&\underline{53.3}&\underline{80.6}&\underline{77.3}&\underline{54.9}&29.5&37.5 &\underline{73.1}\\
\midrule
% \rowcolor{mygray}
% \multirow{1}{*}{LayerNorm} & 0.08 &74.9&91.6&\textbf{75.2}&99.2&91.4&90.5&55.5&86.6&\textbf{95.9}&87.1&76.1&80.6&65.0&53.1&80.9&75.5&55.4&25.5&37.5&73.6\\
% \midrule
% \rowcolor{lightgreen}
\multirow{1}{*}{TTC-Tuning (ours)} & \colorbox{lightgreen}{\textbf{0.19}} &\underline{78.4}&\underline{92.4}&\underline{74.0}&\colorbox{lightgreen}{\textbf{99.4}}&\underline{91.6}&\colorbox{lightgreen}{\textbf{91.6}}&\colorbox{lightgreen}{\textbf{56.0}}&\colorbox{lightgreen}{\textbf{88.3}}&94.6&\colorbox{lightgreen}{\textbf{87.4}}&\colorbox{lightgreen}{\textbf{76.5}}&\colorbox{lightgreen}{\textbf{82.0}}&\colorbox{lightgreen}{\textbf{65.5}}&\colorbox{lightgreen}{\textbf{54.3}}&\colorbox{lightgreen}{\textbf{82.3}}&\colorbox{lightgreen}{\textbf{82.2}}&\colorbox{lightgreen}{\textbf{55.4}}&\underline{30.9}&\underline{39.1}&\colorbox{lightgreen}{\textbf{74.8}}\\
% \bottomrule
\end{tabular}
}
\end{center}
\vspace{-.2in}
\caption{\textbf{Comparisons with state-of-the-art PETL methods on the VTAB-1K benchmark with ViT-B/16}. ``$^{\star}$" means the model has been retrained to produce better results. The entries noted by {\color{dt}grey} represents the baseline algorithms. The best and second-best results of PETL methods are noted by \colorbox{lightgreen}{\textbf{green}} and \underline{underline}, respectively.}
% Average results are calculated across 19 datasets. ``\# Params'' denotes the backbone's average number of trainable parameters.
\label{tab:vit-vtab-full}
\end{table*}

\begin{table*}[h]
% \vspace{-0.1in}
\begin{center}
\large
\resizebox{0.92\linewidth}{!}{
\begin{tabular}{lc|ccccccc|cccc|cccccccc|c}
% \toprule
\multirow{2}{*}{} &  & \multicolumn{7}{c}{\textbf{Natural}} & \multicolumn{4}{c}{\textbf{Specialized}} & \multicolumn{8}{c}{\textbf{Structured}} \\
% \midrule
& \rotatebox{90}{\# Params (M)} & \rotatebox{90}{Cifar100} & \rotatebox{90}{Caltech101}  & \rotatebox{90}{DTD} & \rotatebox{90}{Flower102} & \rotatebox{90}{Pets} & \rotatebox{90}{SVHN} & \rotatebox{90}{Sun397} & \rotatebox{90}{Camelyon} & \rotatebox{90}{EuroSAT} & \rotatebox{90}{Resisc45} & \rotatebox{90}{Retinopathy} & \rotatebox{90}{Clevr-Count} & \rotatebox{90}{Clevr-Dist} & \rotatebox{90}{DMLab} & \rotatebox{90}{KITTI-Dist} & \rotatebox{90}{dSpr-Loc} & \rotatebox{90}{dSpr-Ori} & \rotatebox{90}{sNORB-Azim} & \rotatebox{90}{sNORB-Ele} & \rotatebox{90}{\textbf{Average}}\\
\midrule


 \rowstyle{\color{dt}} Full~\cite{vpt} &  \rowstyle{\color{dt}} 86.7 & \rowstyle{\color{dt}}72.2& \rowstyle{\color{dt}}88.0& \rowstyle{\color{dt}}71.4& \rowstyle{\color{dt}}98.3& \rowstyle{\color{dt}}89.5& \rowstyle{\color{dt}}90.1& \rowstyle{\color{dt}}45.0& \rowstyle{\color{dt}}86.6& \rowstyle{\color{dt}}96.9& \rowstyle{\color{dt}}87.7& \rowstyle{\color{dt}}79.4& \rowstyle{\color{dt}}75.7& \rowstyle{\color{dt}}59.8& \rowstyle{\color{dt}}54.6& \rowstyle{\color{dt}}78.6& \rowstyle{\color{dt}}79.4& \rowstyle{\color{dt}}53.6& \rowstyle{\color{dt}}34.6& \rowstyle{\color{dt}}40.9&\rowstyle{\color{dt}}74.2\\

\rowstyle{\color{dt}} Linear~\cite{vpt} & \rowstyle{\color{dt}}0.04&\rowstyle{\color{dt}}61.4&\rowstyle{\color{dt}}90.2&\rowstyle{\color{dt}}74.8&\rowstyle{\color{dt}}99.5&\rowstyle{\color{dt}}90.2&\rowstyle{\color{dt}}42.7&\rowstyle{\color{dt}}55.8&\rowstyle{\color{dt}}81.5&\rowstyle{\color{dt}}90.1&\rowstyle{\color{dt}}82.1&\rowstyle{\color{dt}}69.4&\rowstyle{\color{dt}}39.1&\rowstyle{\color{dt}}35.9&\rowstyle{\color{dt}}40.1&\rowstyle{\color{dt}}65.0&\rowstyle{\color{dt}}20.3&\rowstyle{\color{dt}}26.0&\rowstyle{\color{dt}}14.3&\rowstyle{\color{dt}}27.8&\rowstyle{\color{dt}}56.4\\
% \midrule
\multirow{1}{*}{Bias~\cite{vpt}} & 0.29 &73.0&86.8&65.6& \colorbox{lightgreen}{{\textbf{97.7}}}&87.5&\underline{56.4}&\underline{52.3}&80.4&91.6&76.1&\underline{72.5}&47.3&48.5&34.7&\underline{66.2}&57.6&36.2& \colorbox{lightgreen}{{\textbf{34.7}}}& \colorbox{lightgreen}{{\textbf{66.2}}}&62.1\\

\multirow{1}{*}{VPT-Deep~\cite{vpt}} & 0.24 &\colorbox{lightgreen}{{\textbf{79.6}}}&\underline{90.8}&\colorbox{lightgreen}{{\textbf{78.0}}}&99.5&\underline{91.4}&42.3&51.7&\underline{84.9}& \colorbox{lightgreen}{{\textbf{96.2}}}&\underline{85.0}&72.0&\underline{67.6}&\underline{59.4}&\underline{50.1}&61.3&\underline{74.4}&\underline{50.6}&25.7&25.7&\underline{68.6}\\
% \rowcolor{mygray}
% \multirow{1}{*}{LayerNorm} & 0.08 &74.6&92.2&76.6&99.5&91.4&42.3&51.7&84.9&96.2&85.0&72.0&67.6&59.4&50.1&61.3&74.4&50.6&25.7&25.7&68.6\\
% \rowcolor{lightgreen}
\multirow{1}{*}{TTC-Tuning (ours)} & 0.19 & \underline{76.1} & \colorbox{lightgreen}{{\textbf{92.4}}} & \underline{76.6}& \colorbox{lightgreen}{{\textbf{99.7}}}& \colorbox{lightgreen}{{\textbf{92.8}}} &  \colorbox{lightgreen}{{\textbf{88.5}}} &  \colorbox{lightgreen}{{\textbf{55.1}}}& \colorbox{lightgreen}{{\textbf{88.0}}}& \underline{95.8}& \colorbox{lightgreen}{{\textbf{87.5}}}& \colorbox{lightgreen}{{\textbf{75.4}}}& \colorbox{lightgreen}{{\textbf{82.3}}} & \colorbox{lightgreen}{{\textbf{62.5}}} &  \colorbox{lightgreen}{{\textbf{52.4}}} & \colorbox{lightgreen}{{\textbf{83.4}}} & \colorbox{lightgreen}{{\textbf{82.6}}}& \colorbox{lightgreen}{{\textbf{54.3}}} & \underline{30.6} & \underline{39.8}& \colorbox{lightgreen}{{\textbf{74.5}}}\\
% \bottomrule
\end{tabular}
}
\end{center}
\vspace{-.2in}
\caption{\textbf{Comparisons with state-of-the-art methods on the VTAB-1K benchmark with Swin-B}.}
\label{tab:swin-vtab-full}
\vspace{-.2in}
\end{table*}

% \begin{table*}[t]
% % \vspace{-0.1in}
% \begin{center}
% \large
% \resizebox{0.96\linewidth}{!}{
% \begin{tabular}{lc|ccccccc|cccc|cccccccc|c}
% % \toprule
% \multirow{2}{*}{} &  & \multicolumn{7}{c}{\textbf{Natural}} & \multicolumn{4}{c}{\textbf{Specialized}} & \multicolumn{8}{c}{\textbf{Structured}} \\
% % \midrule
% & \rotatebox{90}{\# Params (M)} & \rotatebox{90}{Cifar100} & \rotatebox{90}{Caltech101}  & \rotatebox{90}{DTD} & \rotatebox{90}{Flower102} & \rotatebox{90}{Pets} & \rotatebox{90}{SVHN} & \rotatebox{90}{Sun397} & \rotatebox{90}{Camelyon} & \rotatebox{90}{EuroSAT} & \rotatebox{90}{Resisc45} & \rotatebox{90}{Retinopathy} & \rotatebox{90}{Clevr-Count} & \rotatebox{90}{Clevr-Dist} & \rotatebox{90}{DMLab} & \rotatebox{90}{KITTI-Dist} & \rotatebox{90}{dSpr-Loc} & \rotatebox{90}{dSpr-Ori} & \rotatebox{90}{sNORB-Azim} & \rotatebox{90}{sNORB-Ele} & \rotatebox{90}{\textbf{Average}}\\
% \midrule
% % \multirow{1}{*}{Linear} & 0 & 64.4 & 85.0 & 63.2 & 97.0 & 86.3 & 36.6 & 51.0 &  78.5 & 87.5 & 68.5 & 74.0 & 34.3 & 30.6 & 33.2 & 55.4 & 12.5 & 20.0 & 9.6 & 19.2 & 53.0\\
% % \multirow{1}{*}{Full} & 85.8 & 68.9 & 87.7 & 64.3 & 97.2 & 86.9 & 87.4 & 38.8 & 79.7 & 95.7 & 84.2 & 73.9 &  56.3 & 58.6 & 41.7 & 65.5 & 57.5 & 46.7 & 25.7 & 29.1 & 65.6\\

% \multirow{1}{*}{Linear$^{\star}$} & 0.04 & 61.5 & 88.4 & 73.9 & 97.9 & 86.8 & 41.8 & 51.0 &  80.7 & 88.6 & 76.1 & 74.1 &35.4&30.3&35.7&59.8&16.4&24.3&18.0&22.6&56.0\\
% % \midrule
% % \rowcolor{lightcyan}
% \multirow{1}{*}{LayerNorm (LN)} & 0.08 &74.9&91.6&\textbf{75.2}&99.2&91.4&90.5&55.5&86.6&\textbf{95.9}&87.1&76.1&80.6&65.0&53.1&80.9&75.5&55.4&25.5&37.5&73.6\\

% % \multirow{1}{*}{Taylor (Init. w/o LayerNorm)} & 0.11 &74.9&91.6&\textbf{75.2}&99.2&91.4&90.5&55.5&86.6&\textbf{95.9}&87.1&76.1&80.6&65.0&53.1&80.9&75.5&55.4&28.2&37.5&73.7\\

% \multirow{1}{*}{w/o LN init. + w/o LN tuning} & 0.15& 74.7& 91.6&73.6&99.1&90.8&90.2&52.9&87.4&95.4&86.4&75.1&80.5&63.8&51.5&80.6&78.9&55.5&28.6&40.2&73.1\\

% \multirow{1}{*}{w/ LN init. + w/o LN tuning} & 0.15&77.3&91.7&72.9&99.4&91.1&90.6&54.4&84.2&94.3&87.3&75.4&\textbf{82.0}&65.1&53.0&80.9&82.1&\textbf{55.9}&28.8&38.2&73.9\\


% % \rowcolor{lightgreen}
% \multirow{1}{*}{w/o LN init. + w/ LN tuning} & 0.19 & 75.0&91.9&73.9&99.4&90.8&90.8&54.6&87.6&95.8&\textbf{87.6}&75.3&80.8&64.0&52.4&80.5&82.1&55.3&29.0&40.1&74.1\\

% \multirow{1}{*}{w/ LN init. + w/ LN tuning} & 0.19 &\textbf{78.4}&
% \textbf{92.4}&74.0&\textbf{99.4}&\textbf{91.6}&\textbf{91.6}&\textbf{56.0}&\textbf{88.3}&94.6&87.4&\textbf{76.5}&\textbf{82.0}&\textbf{65.5}&\textbf{54.3}&\textbf{82.3}&\textbf{82.2}&55.4&\textbf{30.9}&\textbf{39.1}&\textbf{74.8}\\
% % \bottomrule
% \end{tabular}
% }
% \end{center}
% \vspace{-.2in}
% \caption{\textbf{Evaluation of our proposed Paradigm}. We retrain the Linear model as the baseline. ``w/LN init." represents use the tuned parameter of LayerNorm layer as the initialization while ``w/LN tuning" represents tuning LayerNorm layer during fine-tuning phase.}

% \label{tab:vit-vtab-full-evalmethod}

% \end{table*}


\begin{table*}[t]
\vspace{-0.2in}
\begin{center}
\large
\resizebox{0.92\linewidth}{!}{
\begin{tabular}{c|cccc|ccccccc|cccc|cccccccc|c}
% \toprule
& &&&  & \multicolumn{7}{c}{\textbf{Natural}} & \multicolumn{4}{c}{\textbf{Specialized}} & \multicolumn{8}{c}{\textbf{Structured}} & \\
% \midrule
No. &\rotatebox{90}{LN Tuning Stage1} & \rotatebox{90}{LN Tuning Stage2} & \rotatebox{90}{TTC} & \rotatebox{90}{\# Params (M)} & \rotatebox{90}{Cifar100} & \rotatebox{90}{Caltech101}  & \rotatebox{90}{DTD} & \rotatebox{90}{Flower102} & \rotatebox{90}{Pets} & \rotatebox{90}{SVHN} & \rotatebox{90}{Sun397} & \rotatebox{90}{Camelyon} & \rotatebox{90}{EuroSAT} & \rotatebox{90}{Resisc45} & \rotatebox{90}{Retinopathy} & \rotatebox{90}{Clevr-Count} & \rotatebox{90}{Clevr-Dist} & \rotatebox{90}{DMLab} & \rotatebox{90}{KITTI-Dist} & \rotatebox{90}{dSpr-Loc} & \rotatebox{90}{dSpr-Ori} & \rotatebox{90}{sNORB-Azim} & \rotatebox{90}{sNORB-Ele} & \rotatebox{90}{\textbf{Average}}\\
\midrule
% \multirow{1}{*}{Linear} & 0 & 64.4 & 85.0 & 63.2 & 97.0 & 86.3 & 36.6 & 51.0 &  78.5 & 87.5 & 68.5 & 74.0 & 34.3 & 30.6 & 33.2 & 55.4 & 12.5 & 20.0 & 9.6 & 19.2 & 53.0\\
% \multirow{1}{*}{Full} & 85.8 & 68.9 & 87.7 & 64.3 & 97.2 & 86.9 & 87.4 & 38.8 & 79.7 & 95.7 & 84.2 & 73.9 &  56.3 & 58.6 & 41.7 & 65.5 & 57.5 & 46.7 & 25.7 & 29.1 & 65.6\\

1&\xmark & \xmark & \xmark & 0.04 & 61.5 & 88.4 & 73.9 & 97.9 & 86.8 & 41.8 & 51.0 &  80.7 & 88.6 & 76.1 & 74.1 &35.4&30.3&35.7&59.8&16.4&24.3&18.0&22.6&56.0\\
% \midrule
% \rowcolor{lightcyan}
2&\cmark & \xmark & \xmark & 0.08 &74.9&91.6&\colorbox{lightgreen}{{\textbf{75.2}}}&99.2&91.4&90.5&55.5&86.6&\colorbox{lightgreen}{{\textbf{95.9}}}&87.1&76.1&80.6&65.0&53.1&80.9&75.5&55.4&25.5&37.5&73.6\\

% \multirow{1}{*}{Taylor (Init. w/o LayerNorm)} & 0.11 &74.9&91.6&\textbf{75.2}&99.2&91.4&90.5&55.5&86.6&\textbf{95.9}&87.1&76.1&80.6&65.0&53.1&80.9&75.5&55.4&28.2&37.5&73.7\\

3&\xmark & \xmark & \cmark & 0.15& 74.7& 91.6&73.6&99.1&90.8&90.2&52.9&87.4&95.4&86.4&75.1&80.5&63.8&51.5&80.6&78.9&55.5&28.6&\colorbox{lightgreen}{{\textbf{40.2}}}&73.1\\

4&\cmark & \xmark & \cmark & 0.15&77.3&91.7&72.9&99.4&91.1&90.6&54.4&84.2&94.3&87.3&75.4&\colorbox{lightgreen}{{\textbf{82.0}}}&65.1&53.0&80.9&82.1&\colorbox{lightgreen}{{\textbf{55.9}}}&28.8&38.2&73.9\\


% \rowcolor{lightgreen}
5&\xmark & \cmark & \cmark & 0.19 & 75.0&91.9&73.9&99.4&90.8&90.8&54.6&87.6&95.8&\colorbox{lightgreen}{{\textbf{87.6}}}&75.3&80.8&64.0&52.4&80.5&82.1&55.3&29.0&40.1&74.1\\

6&\cmark &\cmark & \cmark & 0.19 &\colorbox{lightgreen}{{\textbf{78.4}}}&
\colorbox{lightgreen}{{\textbf{92.4}}}&74.0&\colorbox{lightgreen}{{\textbf{99.4}}}&\colorbox{lightgreen}{{\textbf{91.6}}}&\colorbox{lightgreen}{{\textbf{91.6}}}&\colorbox{lightgreen}{{\textbf{56.0}}}&\colorbox{lightgreen}{{\textbf{88.3}}}&94.6&87.4&\colorbox{lightgreen}{{\textbf{76.5}}}&\colorbox{lightgreen}{{\textbf{82.0}}}&\colorbox{lightgreen}{{\textbf{65.5}}}&\colorbox{lightgreen}{{\textbf{54.3}}}&\colorbox{lightgreen}{{\textbf{82.3}}}&\colorbox{lightgreen}{{\textbf{82.2}}}&55.4&\colorbox{lightgreen}{{\textbf{30.9}}}&39.1&\colorbox{lightgreen}{{\textbf{74.8}}}\\
% \bottomrule
\end{tabular}
}
\end{center}
\vspace{-.2in}
\caption{\textbf{Evaluation of our proposed Paradigm}.}
\vspace{-.2in}
\label{tab:vit-vtab-full-evalmethod}
\end{table*}


\section{Experiments}

% This section compares our approach with other state-of-the-art PETL baselines on the VTAB-1K benchmark, using ViT and Swin Transformer backbones. In addition, we analyze the channel selection strategy, insert location, insert layers, and the number of selected channels to further verify the effectiveness. 


\subsection{Experiments on VTAB-1K Benchmark}

\noindent\textbf{Dataset.}
VTAB-1K~\cite{zhai2019large} contains 19 visual classification tasks which cover a broad spectrum of domains and semantics in three groups, \ie, \textit{Natural}, \textit{Specialized}, and \textit{Structured}. The \textit{Natural} group contains 7 classic classification datasets~\cite{krizhevsky2009learning, fei2004learning, cimpoi14describing, nilsback2006visual, parkhi2012cats, netzer2011reading, xiao2010sun} of natural images. The \textit{Specialized} group involves 4 datasets~\cite{Veeling2018qh, helber2019eurosat, cheng2017remote, kaggle2015retinopathy} of two special scenarios: medical and remote-sensing. The \textit{Structured} group has 8 datasets~\cite{johnson2017clevr, beattie2016deepmind, geiger2013vision, matthey2017dsprites, lecun2004learning}, mainly focusing on understanding the structure of a scene, such as object counting, and depth prediction. Each task of VTAB-1K contains 1000 training images.
Following \cite{vpt, SSF}, we use the 800-200 \textsc{train-val} split to determine the hyperparameters and the entire 1000 training data to train the final model. 
% We report the average top-1 accuracy on the \textsc{test} set over three random runs.
We report the average top-1 accuracy on the \textsc{test} set. 

\noindent\textbf{Baselines and state-of-the-art approaches.}
We compare our method with three baselines, Full fine-tuning, Linear, and Bias, and three state-of-the-art methods Adapter~\cite{adapter}, VPT~\cite{vpt}, and SSF~\cite{SSF}. Bias method only updates all the bias terms in the pre-trained backbone. 
% \textbf{Adapter} injects an additional MLP module into each transformer layer. 
% \textbf{LoRA} adopts an optimized low-rank matrix to the MHSA module in the transformer layers. 
% \textbf{VPT} is a visual prompt algorithm to incorporate the prompts with tokens into the backbone. \textbf{SSF} propose the scaling and shifting factors to chaging the mean and variance of features.

\noindent\textbf{Performance with ViT backbone.}
We compare our TTC-tuning with the above 7 baselines in Tab.~\ref{tab:vit-vtab-full}. We use ViT-B/16 as the backbone and insert TTC-Module in each transformer layer. The default $K$ is set to 96, 1/8 of the total channels, leading to the trainable parameter number being only 0.11M.
\textbf{First}, our TTC-Tuning achieves the average accuracy of 74.8\% on the 19 downstream tasks, outperforming the full fine-tuning on 18 out of 19 tasks and gains the improvement of 6.2\%, 3.3\%, and 13.9\% in the three groups, respectively, with only additional 0.13\% of the backbone parameters. Such results reflect that TTC-Tuning can greatly reduce the storage space and alleviate the overfitting problem commonly occurring in full fine-tuning large models.
\textbf{Second}, compared with Adapter~\cite{adapter} that treats all the channels equally, selecting a part of task-relevant channels for each downstream task is more effective and efficient, outperforming it by 3.4\% in average accuracy. Moreover, our TTC-Tuning outperforms VPT~\cite{vpt} by 5.0\%, 4.3\%, and 6.5\% in the three groups, respectively.
\textbf{Third}, compared with the distribution alignment method SSF~\cite{SSF}, our TTC-Tuning surpasses it by 1.7\%. 
These results demonstrate that instead of aligning distribution only (\ie, SSF) or learning task-relevant information (\ie, VPT, Adapter), leveraging the two-stage paradigm can maintain lower-level parameter costs and improve the performance.

\noindent\textbf{Performance with Swin Transformer Backbone.}
To verify the effectiveness of TTC-Tuning with different backbones, we apply TTC-Tuning on hierarchical transformers, \ie, Swin-B~\cite{liu2021swin}. We use the same setting of inserting TTC-Module as in the ViT backbone.
% inserting the TTC-Module after the ``MHSA'' block in the transformer layer
Considering deep layers contain more semantic information in the hierarchical structure, instead of applying TTC-Module on all the transformer layers, we insert it to the last half of the layers in the stage3 and all layers of the stage4 of the Swin-B to keep a similar level of trainable parameters.
The results of Tab. \ref{tab:swin-vtab-full} show that TTC-Tuning outperforms \textbf{Full fine-tuning} in all three groups with only 0.2\% parameters while other methods cannot. In addition, compared with PETL method, TTC-Tuning outperforms VPT~\cite{vpt} by 6.2\%, 2.2\%, and 7.6\% in the three groups, respectively. 
% It demonstrates that our TTC-tuning is superior to using the prompts in the Swin transformer in terms of effectiveness and efficiency. 
% All the results suggest that our TTC-tuning is applicable for other vision transformer architectures and can achieve significant performance with only a tiny amount of trainable parameters.
All the results above suggest that our TTC-tuning is also applicable for the hierarchical transformers and can yield much more improvement than other PETL methods.

\noindent\textbf{Complexity Analysis.} In our analysis, we consider a ViT-B backbone with $L$ layers and $D$ dimensions, along with $N$ tokens for a single image. We also assume that the intermediate dimension of Adapter~\cite{adapter} is $D^{\prime}$, that the prompt length of VPT~\cite{vpt} is $n$, and that the total insert times of SSF~\cite{SSF} is $m$ in the whole ViT-B backbone. Finally, we compare our proposed TTC-Module approach to Adapter, VPT, and SSF in terms of parameters and FLOPs, as summarized in Tab.~\ref{tab:complexity}. Notably, our selection of $K$ as $\frac{1}{8}D$ is fairly small compared to $D$. When we compare our approach to SSF, we find that the number of parameters for TTC-Module is $\frac{1}{64}LDD$, while the number of parameters for SSF is $mLD$. Examining the ViT-B backbone, we find that $m=74$ and $\frac{1}{64}D=12$, our parameters and FLOPs are smaller than SSF. Overall, our analysis suggests that TTC-Module may offer a more efficient and effective approach to transfer learning.


\begin{table}[t]
\small
\centering
% \vspace{-.1in}
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{l|cccc}
\toprule
& Adapter & VPT-Deep & SSF  & TTC-Module \\
\midrule
\multirow{1}{*}{\# Extra Parameters} & $2LDD^{\prime}$& $nLD$ & $mLD$ & $LKK$ \\
% \midrule
\multirow{1}{*}{\# Extra FLOPs} & $2NLDD^{\prime}$ & $2n(2N+n)LD$ & $mNLD$ & $NLKK$  \\
\bottomrule
\end{tabular}
}
% \vspace{-.1in}
\caption{A complexity analysis of Adapter~\cite{adapter}, VPT~\cite{vpt}, SSF~\cite{SSF}, and our proposed TTC-Module.}
\label{tab:complexity}
\vspace{-0.2in}
\end{table}

\subsection{Evaluation}

\begin{table*}[th]
\vspace{-.3in}
\centering
\subfloat[
\textbf{Channel Selection}.
\label{tab:ablation:differentSS}
]{
\begin{minipage}{0.18\linewidth}{\begin{center}
\scriptsize
\setlength{\tabcolsep}{1.2mm}{
\begin{tabular}{c|cc}
% \toprule
 & Acc. & Params. (M) \\
\midrule
\multirow{1}{*}{Linear$^{\star}$} & 61.5 & 0.08 \\
% \midrule
\multirow{1}{*}{RC-1} & 72.1 & 0.23 \\
% \midrule
\multirow{1}{*}{RC-2} &74.1& 0.23  \\
\multirow{1}{*}{RC-3} &71.3& 0.23  \\
\multirow{1}{*}{L2 Norm} &75.4 & 0.23  \\
\multirow{1}{*}{TIS} & \colorbox{lightgreen}{{\textbf{78.4}}}& 0.23 \\
% \bottomrule
\end{tabular}}
\end{center}}\end{minipage}}
\hspace{2em}
\subfloat[
\textbf{Insert depth}.
\label{tab:ablation:insertdepth}
]{
\begin{minipage}{0.16\linewidth}{
\begin{center}
\scriptsize
\setlength{\tabcolsep}{1.5mm}{
\begin{tabular}{c|cc}
% \toprule
\multirow{1}{*}{Layers } & Acc. & Params. (M) \\
\midrule
\multirow{1}{*}{Linear$^{\star}$} & 61.5 & 0.08 \\
\multirow{1}{*}{0} & 74.9 & 0.12 \\
% \midrule
\multirow{1}{*}{2} & 77.8 & 0.13 \\
\multirow{1}{*}{4} & 77.7 &  0.15\\
\multirow{1}{*}{8} & 77.9 &  0.19\\
\multirow{1}{*}{12} & \colorbox{lightgreen}{{\textbf{78.4}}} & 0.23\\
% \bottomrule
\end{tabular}}
\end{center}}\end{minipage}
}
\hspace{2em}
\subfloat[
\textbf{Insert position}. 
\label{tab:ablation:insetrposition}
]{
\begin{minipage}{0.22\linewidth}{\begin{center}
\scriptsize
\setlength{\tabcolsep}{1.5mm}{
\begin{tabular}{c|cc}
% \toprule
\multirow{1}{*}{Insert Position} & Acc. & \# Params. (M) \\
\midrule
\multirow{1}{*}{Full} & 68.9& 86.7\\
\multirow{1}{*}{Linear$^{\star}$} & 61.5 & 0.08 \\
\multirow{1}{*}{LayerNorm$^{\star}$} & 73.6 & 0.11 \\
\multirow{1}{*}{MHSA} & 78.4 &  \multirow{1}{*}{0.23}\\
\multirow{1}{*}{MLP} & \colorbox{lightgreen}{{\textbf{78.6}}} & \multirow{1}{*}{0.23}\\
\multirow{1}{*}{MHSA+MLP} & 77.1 &  \multirow{1}{*}{0.34} \\
% \bottomrule
\end{tabular}}
\end{center}}\end{minipage}
}
\hspace{2em}
\subfloat[
\textbf{Different $K$}.
\label{tab:ablation:differntK}
]{
\begin{minipage}{0.18\linewidth}{\begin{center}
\scriptsize
\setlength{\tabcolsep}{1.5mm}{
\begin{tabular}{c|cc}
% \toprule
\multirow{1}{*}{Top-$K$ } & Acc. & Params. (M) \\
\midrule
\multirow{1}{*}{Linear$^{\star}$} & 61.5 & 0.08 \\
\multirow{1}{*}{32} & 78.2 & 0.13 \\
% \midrule
\multirow{1}{*}{64} & 77.7 & 0.17\\
\multirow{1}{*}{96} & \colorbox{lightgreen}{{\textbf{78.4}}} &  0.23 \\
\multirow{1}{*}{128} & 77.4 & 0.31 \\
\multirow{1}{*}{192} & 76.5 & 0.56\\
% \bottomrule
\end{tabular}
}
\end{center}}\end{minipage}
}
\vspace{-.1in}
\caption{\textbf{Evaluation of different designs}. Acc.: Top-1 accuracy (\%); Params.: parameters (M). \text{Linear$^{\star}$} represents the baseline results for better comparison. }

% (a) Comparing the effectiveness of various methods for selecting task-relevant channels. (b) Testing the impact of inserting different layers in ViT-B. (c) Examining the performance with varying insertion positions. (d) Investigating the effect of different number of selected channels.
 
\label{tab:ablations}
\vspace{-.1in}
\end{table*}

\begin{figure*}[t]
\begin{center}
% \vspace{-0.2in}
\includegraphics[width=0.23\linewidth]{paperfigs/svhn_jsd_difflayers_norm1_scale.pdf}
\includegraphics[width=0.23\linewidth]{paperfigs/svhn_jsd_difflayers_norm1_bias.pdf}
\includegraphics[width=0.23\linewidth]{paperfigs/svhn_jsd_difflayers_norm2_scale.pdf}
\includegraphics[width=0.23\linewidth]{paperfigs/svhn_jsd_difflayers_norm2_bias.pdf}
\end{center}
 \vspace{-0.3in}
\caption{Comparison of parameter shift after tuning the LayerNorm layer (stage1) and jointly tuning LayerNorm and TTC-Module (stage2).}
\label{fig:scalebiaschange}
\vspace{-0.2in}
\end{figure*}

% \noindent\textbf{Impacts of Two Stages.} 
% To evaluate the effectiveness of tuning the LayerNorm layer and our proposed TTC-Module, we conducted experiments by removing the LayerNorm (LN) initialization of each LayerNorm layer and not tuning the LayerNorm layer during the fine-tuning phase in Tab.~\ref{tab:vit-vtab-full-evalmethod}. Since all variants require training a new classification head for each task, we retrained the linear layer to facilitate better comparison of the improvements.
% For the first stage of the evaluation, we only tuned the LN layer and observed an average improvement of 0.5\% top-1 accuracy over the SSF baseline with only 15\% of the parameters.
% For the second stage, we evaluated the task-relevant tuning by first removing the LN's parameter initialization of the backbone and only tuning the TTC-Module, which achieved a top-1 accuracy of 73.1\%, the same as the SSF baseline. When we loaded the trained LN's parameters, the performance slightly improved to 73.9\%, surpassing the SSF baseline by 0.8\%. Similarly, when we loaded the trained LN's parameters with LN tuning, the performance increased by 0.7\% compared to without initial LN's parameters (from 74.1\% to 74.8\%).
\noindent\textbf{Ablation Studies.}
To evaluate the effectiveness of tuning the LayerNorm layer and our proposed TTC-Module, we conduct ablation studies on the two components  in our two-stage paradigm in Tab.~\ref{tab:vit-vtab-full-evalmethod}.
\textbf{First}, in the first stage, we finetune LayerNorm to align task distribution, which has already outperformed the previous best model (73.6\% (2nd row of Tab.~\ref{tab:vit-vtab-full-evalmethod}) \textit{v.s.} 73.1\% (SSF). \textbf{Second}, when combining the second stage on top of the LayerNorm tuning, the TTC-Module can yield an improvement of 1.2\%.
\textbf{Third}, to further verify the effectiveness of TTC-Module, we insert TTC directly into the baseline \textit{Linear} model, gaining an improvement of 17.1\% (4th row). 
\textbf{Fourth}, we tune the LayerNorm and TTC-Module together in one stage (5th row), achieving an accuracy of 74.1\%, worse than the two-stage paradigm by 0.7\%.
All the results above demonstrate the effectiveness of the proposed LayerNorm tuning, TTC-Module, and the necessity of a two-stage paradigm.


\noindent\textbf{Tuning Channels \vs Tuning Weights}
As illustrated in Sec.~\ref{sec:adapt}, tuning selected $K$ channels via a linear adapter only use $\frac{K}{D}=\frac{1}{8}$ parameters of directly tuning the weights of ViT layer. In addition, with fewer learnable parameters, the model is less prone to overfitting to the small dataset.
We compare the performance of tuning weights and tuning channels in Tab.~\ref{tab:vit-vtab-weights-channels}. The number of parameters of tuning weights is relatively high (0.88M) while tuning channels with only 0.11M parameters can gain 7.8\% improvements in total 19 downstream tasks.

\noindent\textbf{Effectiveness of Task-Relevant Channel Selection.}
To verify the effectiveness and necessity of the proposed Taylor expansion-based Importance Score (TIS) channel selection, we compare three channel selection strategies in Tab.~\ref{tab:ablation:differentSS}. These strategies included Random Channel Selection (RC), L2 Norm, and Taylor expansion-based Importance Score (TIS). RC selects $K$ channels randomly, and to reduce the impact of outliers, we randomly selected three sets of channels (RC-1/2/3). L2 Norm determines task-relevant channels based on the L2 Norm of features in each channel. The task-relevant strategies achieve better and more robust performance than RC. In addition, our TIS can select more important and representative channels than L2 Norm, outperforming it by 3.0\%. 
% We also mask the selected task-relevant channels in different $K$ as shown in Fig.~\ref{}.

\noindent\textbf{Insert Depth.}
Insert depth is one important factor that influences performance. We report the results when inserting TTC-Module to the last $l$ layers of ViT-B in Tab.~\ref{tab:ablation:insertdepth}. Without the TTC-Module (stage one only), the accuracy is 74.9\%, while the accuracy gradually improved to 78.4\% with the increase of insert depth. 
Upon analyzing the results in Tab.~\ref{tab:ablation:insertdepth}, we observe that inserting the TTC-Module only in the last two layers achieved an accuracy of 77.8\%, indicating that deeper layers contribute more to the final results. Notably, when we remove the TTC-Module in the first four layers, the accuracy was 77.9\%, with only a 0.5\% gap to the best result of 78.4\%.

\noindent\textbf{Insert Position.}
We evaluated the insertion position of our TTC-Module, as shown in Tab.~\ref{tab:ablation:insetrposition}. Specifically, we insert the module after the MHSA and MLP blocks, respectively. Our findings indicate that inserting the module after the MLP block yields better results, consistent with similar findings for SSF. Additionally, we insert our module after both blocks, which led to lower performance at 77.1\% with an increase in parameters. We conjecture that only one position is enough for adaptation, and repeat adaptation will increase the difficulty of optimization.

\noindent\textbf{Number of selected channels $K$.}
% The number of selected channels ($K$) is the most task-relevant hyperparameter of the TTC-Module, as it influences the model architecture and the number of trainable parameters. 
The number of selected channels ($K$) is the most important hyperparameter related to the design of TTC-Module, influencing the model architecture and the number of trainable parameters.
Unlike VPT, which selects the best prompt length for each task, we use the same $K$ for all tasks for a fair comparison. In Tab.~\ref{tab:ablation:differntK}, as we increase the value of $K$, the performance improves and peaks at $K=96$.  When further increasing the learnable channels, the performance degrades. 
We hypothesize that a larger value of $K$ may involve too much task-irrelevant information and can make tuning hyperparameters more difficult.

\noindent\textbf{Analysis.}
In Figure~\ref{fig:scalebiaschange}, we analyzed the parameter shift after tuning the LayerNorm layer (stage1) and jointly tuning LayerNorm and TTC-Module (stage2). Our findings indicate that deeper layers led to larger shifts in the weight and bias of both ``Norm1" and ``Norm2" (the two LayerNorm layers in ViT-B). Specifically, we observed obvious deviations in the weight parameter of the shallow layers of "Norm2" which differed from "Norm1". We also evaluate the representation ability to conduct stage1 tuning in Tab.~\ref{tab:} by utilizing KNN~\cite{cover1967nearest} algorithm to cluster the feature of \textsc{[CLS]} token. The results suggest that stage1 is indeed effective in improving representation ability.


\subsection{Experiments on Domain Generalization}
In addition to evaluating the model on test data of the same distribution, modern deep neural networks commonly suffer from performance degradation when the testing distribution is different from that of the training set, \ie, domain shift, which is inevitable in a real-world application. 
% To alleviate this problem, domain generalization~\cite{zhou2021mixstyle,zhao2022shade} is investigated in the community, which aims at training a model with one or multiple source domains but can perform well on other unseen target domains. To verify the generalization ability of our TTC-tuning, we follow ~\cite{noah} to conduct experiments on ImageNet and its variants.
\noindent\textbf{Dataset.}
We use the ImageNet-1K~\cite{deng2009imagenet} as the source domain with 16-shot per category and evaluate our model on ImageNetV2~\cite{recht2019imagenet}, ImageNet-Sketch~\cite{wang2019learning}, ImageNet-A~\cite{hendrycks2021natural}, and ImageNet-R~\cite{hendrycks2021many}.
% ImageNetV2~\cite{recht2019imagenet} is collected from different sources from ImageNet-1K with the same protocol, and ImageNet-Sketch~\cite{wang2019learning} contains the sketch images of ImageNet classes. Both of them use the same classes as ImageNet-1K. ImageNet-A~\cite{hendrycks2021natural} and ImageNet-R~\cite{hendrycks2021many} contains the adversarially-filtered images and renditions of ImageNet data of a 200-class subset, respectively.


% \textbf{Baselines}

\noindent\textbf{Results.}
In Tab.~\ref{tab:dg}, we compare our TTC-tuning with Adapter~\cite{adapter}, VPT~\cite{vpt}, LoRA~\cite{lora}, and NOAH~\cite{noah} on the above datasets. We can make two observations. \textbf{First}, TTC-tuning outperforms the previous best method~(NOAH) on three of the four target datasets and achieves comparable performance on ImageNetV2. Specifically, TTC-tuning yields an improvement of 0.9\% on ImageNet-R over NOAH. \textbf{Second}, our TTC-tuning achieves an accuracy of 75.5\% on the source domain, greatly outperforming previous methods by 4\%. 
% Since the backbone model is pre-trained on ImageNet-21K, the results on ImageNet-1K show that TTC-tuning can 
% better enhance the knowledge transfer from superset to subset. The two observations demonstrate the superiority of our TTC-tuning over previous fine-tuning techniques on strong generalization ability.
% 
Since the backbone model is pre-trained on ImageNet-21K, the results on ImageNet-1K show that TTC-tuning can better align the superset's complex distribution with the subset's relatively simple distribution. The two observations demonstrate the superiority of our TTC-tuning over previous PETL techniques on strong generalization ability.



\begin{table}[]
% \vspace{-0.1in}
\small
\centering
\resizebox{0.3\textheight}{!}{
\begin{tabular}{l|c|cccc}
\toprule
\multirow{2}{4em}{} & {\textbf{Source}} & \multicolumn{4}{c}{\textbf{Target}} \\
% & \multirow{2}{*}{{\#} Params (M)}
% \midrule
& ImageNet & -V2 & -Sketch  & -A & -R \\
\midrule
\multirow{1}{*}{Adapter~\cite{adapter}}  & 70.5 & 59.1 & 16.4 & 5.5 & 22.1 \\
% \midrule
\multirow{1}{*}{VPT~\cite{vpt}}  & 70.5 & 58.0 & 18.3 & 4.6 & 23.2 \\
% \midrule
\multirow{1}{*}{LoRA~\cite{lora}}  & 70.8 & 59.3 & 20.0 & 6.9 & 23.3 \\
\multirow{1}{*}{NOAH~\cite{noah}} & 71.5 & \colorbox{lightgreen}{{\textbf{66.1}}} & 24.8 & \colorbox{lightgreen}{{\textbf{11.9}}} & 28.5 \\
\midrule
\multirow{1}{*}{TTC-Tuning (ours)} & \colorbox{lightgreen}{{\textbf{75.5}}} & 65.9 & \colorbox{lightgreen}{{\textbf{25.6}}} & \colorbox{lightgreen}{{\textbf{11.9}}} & \colorbox{lightgreen}{{\textbf{29.4}}}\\
\bottomrule
\end{tabular}
}
\vspace{-.1in}
\caption{Comparison with previous methods on domain generalization.}
\label{tab:dg}
\vspace{-0.1in}
\end{table}



% \begin{table*}[h]
% % \vspace{-0.1in}
% \begin{center}
% \large
% \resizebox{0.96\linewidth}{!}{
% \begin{tabular}{lc|ccccccc|cccc|cccccccc|c}
% % \toprule
% \multirow{2}{*}{} &  & \multicolumn{7}{c}{\textbf{Natural}} & \multicolumn{4}{c}{\textbf{Specialized}} & \multicolumn{8}{c}{\textbf{Structured}} \\
% % \midrule
% & \rotatebox{90}{\# Params (M)} & \rotatebox{90}{Cifar100} & \rotatebox{90}{Caltech101}  & \rotatebox{90}{DTD} & \rotatebox{90}{Flower102} & \rotatebox{90}{Pets} & \rotatebox{90}{SVHN} & \rotatebox{90}{Sun397} & \rotatebox{90}{Camelyon} & \rotatebox{90}{EuroSAT} & \rotatebox{90}{Resisc45} & \rotatebox{90}{Retinopathy} & \rotatebox{90}{Clevr-Count} & \rotatebox{90}{Clevr-Dist} & \rotatebox{90}{DMLab} & \rotatebox{90}{KITTI-Dist} & \rotatebox{90}{dSpr-Loc} & \rotatebox{90}{dSpr-Ori} & \rotatebox{90}{sNORB-Azim} & \rotatebox{90}{sNORB-Ele} & \rotatebox{90}{\textbf{Average}}\\
% \midrule


% \multirow{1}{*}{Tuning task-relevant weights} & 0.95&69.9&91.4&67.8&94.7&87.7&90.7&44.5&83.2&92.1&86.2&75.2&64.7&59.6&49.7&80.3&79.1&55.2&\colorbox{lightgreen}{\textbf{31.3}}&36.7&67.0\\

% % \rowcolor{lightgreen}
% \multirow{1}{*}{Tuning task-relevant channels} & 0.19 &\colorbox{lightgreen}{\textbf{78.4}}&
% \colorbox{lightgreen}{\textbf{92.4}}&\colorbox{lightgreen}{\textbf{74.0}}&\colorbox{lightgreen}{\textbf{99.4}}&\colorbox{lightgreen}{\textbf{91.6}}&\colorbox{lightgreen}{\textbf{91.6}}&\colorbox{lightgreen}{\textbf{56.0}}&\colorbox{lightgreen}{\textbf{88.3}}&\colorbox{lightgreen}{\textbf{94.6}}&\colorbox{lightgreen}{\textbf{87.4}}&\colorbox{lightgreen}{\textbf{76.5}}&\colorbox{lightgreen}{\textbf{82.0}}&\colorbox{lightgreen}{\textbf{65.5}}&\colorbox{lightgreen}{\textbf{54.3}}&\colorbox{lightgreen}{\textbf{82.3}}&\colorbox{lightgreen}{\textbf{82.2}}&\colorbox{lightgreen}{\textbf{55.4}}&30.9&\colorbox{lightgreen}{\textbf{39.1}}&\colorbox{lightgreen}{\textbf{74.8}}\\
% % \bottomrule
% \end{tabular}
% }
% \end{center}
% \vspace{-.2in}
% \caption{The results of directly tuning task-relevant weights and task-relevant channels on VTAB-1K benchmark.}

% \label{tab:vit-vtab-weights-channels}
% \end{table*}


\begin{table}[]
% \vspace{0.1in}
\begin{center}
% \small
\resizebox{0.94\linewidth}{!}{
\begin{tabular}{l|c|cccc}
\toprule
\multirow{2}{4em}{} & \multirow{2}{*}{{\#} Params (M)} & \multicolumn{4}{c}{VTAB-1K} \\
% \midrule
& & Natural & Specialized  & Structured & Average \\

\midrule
\multirow{1}{*}{Tuning weights} & 0.88+0.08 & 78.1 & 84.2 & 57.1 & 72.9 \\
\multirow{1}{*}{Tuning Channels} & 0.11+0.08 & \colorbox{lightgreen}{{\textbf{83.4}}}$_{+5.3}$ & \colorbox{lightgreen}{{\textbf{86.7}}}$_{+2.5}$ & \colorbox{lightgreen}{{\textbf{61.5}}}$_{+4.4}$ & \colorbox{lightgreen}{{\textbf{74.8}}}$_{+1.9}$ \\

\bottomrule
\end{tabular}
}
\end{center}
\vspace{-0.2in}
\caption{We evaluated the effects of tuning task-relevant weights and channels on 19 downstream tasks. The a+b notation represents the combination of parameters introduced by both external and internal modules.}
% The a+b notation represents the combination of parameters introduced by both external TTC-Module and internal components such as "Linear" and "LayerNorm".
\label{tab:vit-vtab-weights-channels}
\vspace{-0.1in}
\end{table}

\begin{table}[]
% \vspace{0.1in}
\begin{center}
\scriptsize
% \resizebox{0.8\linewidth}{!}{
\begin{tabular}{l|ccc}
\toprule
Method & SVHN & EuroSAT & Clevr-Count \\
\midrule

w/o stage1 & 34.50& 84.44& 29.00\\
w/ stage1 & 91.23& 92.96&80.33\\

\bottomrule
\end{tabular}
% }
\end{center}
\vspace{-0.2in}
\caption{We evaluate the stage1 using the KNN algorithm to test the representation ability of \textsc{[CLS]} token.}
\label{tab:}
\vspace{-0.2in}
\end{table}

\section{Conclusion}
Since previous PETL methods can be divided into two streams: learning task-relevant information and aligning the distributions between pre-trained and downstream tasks, we first propose a two-stage paradigm by combining the two lines of approaches. We first narrow the distribution shifts and propose a Taylor expansion-based importance score to select task-relevant channels for efficient adaption. In summary, our novel paradigm represents a new direction emphasizing the importance of considering distribution shifts when fine-tuning downstream tasks.



{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}