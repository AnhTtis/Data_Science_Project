\documentclass[12pt]{article}
%\documentclass{extarticle}
%\documentclass[ecta,nameyear, draft]{econsocart}
%% Packages:
\usepackage[utf8]{inputenc}
\usepackage[LGR, T1]{fontenc}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{cancel}
\usepackage{float}
%\usepackage{enumitem}  
\usepackage[inline, shortlabels]{enumitem}
\usepackage[reqno]{amsmath} % leqno if want equation number to the left by default
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{eurosym}
\usepackage{geometry}
\usepackage{dsfont}
%\usepackage[backend=bibtex, maxcitenames=2, natbib=true,style=authoryear]{biblatex}
\usepackage{setspace}
\usepackage{natbib}
\usepackage{sectsty} % section fontsize
\usepackage[colorlinks]{hyperref}
\hypersetup{citecolor=blue,linkcolor=blue, urlcolor=blue}
\usepackage{pgfplots}
\usepackage{tikz}
\usetikzlibrary{datavisualization}
\usetikzlibrary{datavisualization.formats.functions}
\usetikzlibrary{positioning}


%% Commands/Theorems:
\theoremstyle{plain} % https://en.wikibooks.org/wiki/LaTeX/Theorems
\newtheorem{theorem}{Theorem}%[section]
\newtheorem{assumption}{Assumption}
\newtheorem{subassumption}{Assumption}[assumption]
\renewcommand{\thesubassumption}{\theassumption\alph{subassumption}}
\newtheorem{subassumptionbis}{}[assumption]
\renewcommand{\thesubassumptionbis}{(\roman{subassumptionbis})}
\newtheorem*{normalization}{Normalization}
%\newtheorem{proposition}{Proposition}
\newtheorem{proposition}{Proposition}
\newenvironment{assumptionbis}[1]
  {\renewcommand{\theassumption}{$D$\ref{#1}}%
   \addtocounter{assumption}{-1}%
   \begin{assumption}}
  {\end{assumption}}
\newenvironment{normalizationbis}[1]
  {\renewcommand{\thenormalization}{$D$\ref{#1}}%
   \addtocounter{normalization}{-1}%
   \begin{normalization}}
  {\end{normalization}}

%\newtheorem{Lemma}[theorem]{Lemma}
\newtheorem{Lemma}{Lemma}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
%\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
%\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newtheorem{condition}{Condition}

%\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}

\newcommand*{\myfont}{\fontfamily{qpl}\selectfont}
\DeclareTextFontCommand{\textmyfont}{\myfont}

\renewcommand{\j}{\ensuremath{\text{j}}}
\newcommand{\qedblack}{\tag*{$\blacksquare$}}


\makeatletter
\newcommand{\leqnomode}{\tagsleft@true}
\newcommand{\reqnomode}{\tagsleft@false}
\makeatother


% Definitions
\def\mydoubleq#1{``#1''}
\def\mysingleq#1{`#1'}

% Notation for epsilon, Epsilon, eta, Eta
\DeclareSymbolFont{upgreek}{LGR}{cmr}{m}{n}
\SetSymbolFont{upgreek}{bold}{LGR}{cmr}{bx}{n}
%\DeclareMathSymbol{\Epsilon}{\mathord}{upgreek}{`E}
%\DeclareMathSymbol{\Eta}{\mathord}{upgreek}{`H}
% % and no need to specify eta and epsilon in this case. 
%

% https://tex.stackexchange.com/questions/183830/changing-math-fonts-for-greek-letters for the codes
\DeclareMathSymbol{\Epsilon}{\mathalpha}{letters}{"0F}
\DeclareMathSymbol{\Eta}{\mathalpha}{letters}{"11}
\DeclareMathSymbol{\epsilon}{\mathalpha}{letters}{`e}
\DeclareMathSymbol{\eta}{\mathalpha}{letters}{`h}



% Other:
\setcounter{MaxMatrixCols}{10}
\let\origtheassumption\theassumption


% Geometry of the document
%\geometry{left=.9in,right=.6in,top=1in,bottom=.9in}
\geometry{left=1.25in,right=1.25in,top=1.25in,bottom=1.25in}
%\sectionfont{\fontsize{16}{15}\selectfont}

\usepackage{changepage}
\newenvironment{widerequation}{%
    \begin{adjustwidth}{-2cm}{-2cm}\begin{equation}}
    {\end{equation}\end{adjustwidth}}
    



\usepackage{titlesec}
\titlespacing*{\section}{0pt}{19pt}{7pt}
\titlespacing*{\subsection}{0pt}{14pt}{5pt}
\titlespacing*{\subsubsection}{0pt}{12pt}{5pt}
\titlespacing*{\paragraph}{0pt}{9pt}{9pt}
\titleformat{\section}{\normalfont\fontsize{16}{15}\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalfont\fontsize{14}{15}\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalfont\fontsize{12}{15}\bfseries}{\thesubsubsection}{1em}{}

\allowdisplaybreaks





\begin{document}



\title{{\fontsize{14}{20} \selectfont 
\vspace{-1cm}
\textbf{\textmyfont{Don't (fully) exclude me, it's not necessary! \\
Identification with semi-IVs}}}}

\author{{\fontsize{12}{20} Christophe Bruneel-Zupanc}\footnote{E-mail address: \href{mailto:christophe.bruneel@gmail.com}{christophe.bruneel@gmail.com}. I am especially grateful to Jad Beyhum and Geert Dhaene for their tremendous help. I also thank Edoardo Ciscato, Laurens Cherchye, Ferre de Graeve, Olivier de Groote, Jan de Loecker, Alexandre Gaillard, Iris Kesternich, Thierry Magnac, Erwin Ooghe, Christian Proebsting, Jo Van Biesebroeck, Frank Verboven, Frederic Vermeulen, and Mariana Zerpa for their very helpful suggestions and comments. I acknowledge financial support from the Research Fund KU Leuven through the grant STG/21/040.} \\
{\fontsize{12}{20} Department of Economics, KU Leuven} } %\vspace{0.25cm}}
\date{{\fontsize{10}{20} \today}} 

\vspace{-3cm}
\maketitle


%\vspace{-0.75cm}


\begin{abstract}
\renewcommand{\baselinestretch}{1.2} 
{\normalsize 
\noindent This paper proposes a novel approach to identify models with a discrete endogenous variable, that I study in the general context of nonseparable models with continuous potential outcomes.
I show that nonparametric identification of the potential outcome and selection equations, and thus of the individual treatment effects, can be obtained with \textit{semi-instrumental variables} (semi-IVs), which are relevant but only \textit{partially excluded} from the potential outcomes, i.e., excluded from one or more potential outcome equations, but not necessarily all. This contrasts with the full exclusion restriction imposed on standard instrumental variables (IVs), which is stronger than necessary for identification: IVs are only a special case of valid semi-IVs. In practice, there is a trade-off between imposing stronger exclusion restrictions, and finding semi-IVs with a larger support and stronger relevance assumptions. Since, in empirical work, the main obstacle for finding a valid IV is often the full exclusion restriction, tackling the endogeneity problem with semi-IVs instead should be an attractive alternative. \\ 

\noindent \textbf{Keywords:} exclusion restriction, instrumental variable, identification, nonseparable models, individual treatment effects.} \\ 
\end{abstract}




\setstretch{1.30} % check that gives 32 lines/page. Seems ok. 
\setlength{\abovedisplayskip}{6pt} % for skips before/after equations/align
\setlength{\belowdisplayskip}{6pt}





\pagebreak

\section{Introduction}

Since the seminal work of \cite{heckman1979}, endogenous selection is recognized as one of the main problems arising in empirical economics. A convenient strategy to deal with selection is to rely on instrumental variables (IVs), which are independent of the unobservables, relevant for the selection and fully excluded from the potential outcomes \citep[...]{angrist1990, angristkrueger1991, angristimbens1994, card1995, angristimbensrubin1996}. However, in practice, valid instruments satisfying these conditions are generally hard to find. In particular, the exclusion restriction is especially difficult to satisfy. Even for commonly used instruments, the exclusion restriction is often controversial. Furthermore, theoretically valid instruments may not be used in practice if they are only weakly relevant for the selection. \\ 
\indent This paper provides a new approach to solve the endogeneity problem and identify models with a discrete selection equation and continuous potential outcomes \citep{chernozhukovhansen2005, vuongxu2017, bruneel2022}. Instead of looking for an instrumental variable that is fully excluded from all the potential outcomes, one can look for \textit{semi-instrumental variables} (semi-IVs) which are relevant for the selection, but only \textit{partially excluded} from the potential outcomes, i.e., excluded from one or more, but not necessarily all, potential outcome equations.
I show that semi-IVs are sufficient to obtain nonparametric identification of the whole model: potential outcomes, selection probabilities, counterfactual mapping between the potential outcomes and thus, individual treatment effects. %), by solving a system of partial differential equations.
The full exclusion restriction imposed on standard IVs is, in fact, stronger than necessary and is only a special case of partial exclusion restrictions: IVs are a particular example of semi-IVs. \\
\indent For example, to identify the returns to college \citep{angristkrueger1991, card1995, carneiroheckmanvytlacil2011}, one may use two semi-IVs: the average local earnings of college-goers, and of non-goers at the time of the decision to go to college (at $17$ years old). The idea is that, the average local earnings of college-goers are likely to be excluded from the subsequent earnings of non-goers since they did not follow college education (especially when one controls for the local earnings of non-goers as the other semi-IV). However, the average local earnings of college-goers are probably not excluded from the subsequent earnings of college-goers, because of time auto-correlation or permanent local market conditions for example. 
Vice-versa, the local earnings of non-goers is excluded from the earnings of goers, but not from the earnings of non-goers. 
In other words, each semi-IV is only partially excluded from one of the two potential outcomes, but not from both.  
Here, both semi-IVs should also be relevant for the selection into education: ceteris paribus, the better the local market of college-goers, the more likely one is to go to college, and vice-versa. 
Importantly, the two semi-IVs are allowed to be (imperfectly) correlated with each other. Hence the partial exclusion restriction means that, conditional on the non-excluded semi-IV, the other semi-IV is excluded. 
In general, selection-specific variables (e.g., costs and benefits associated with each alternative) can naturally serve as semi-IVs, but valid semi-IVs are not limited to these, and I provide other examples in the paper. \\
%
%
\indent In a generalization, this paper provides necessary conditions for the validity of a set of partial exclusion restrictions, which can be checked a priori. The main necessary conditions are that, first, the joint support of the semi-IVs is larger than the number of `unique' (after we apply the partial exclusion) potential outcomes to be identified. Thus, a lower bound is that the joint support of the semi-IVs must be larger than the number of selection alternatives. Second, for each potential outcome, at least one partial exclusion restriction is imposed. 
Intuitively, the cost of finding a non-fully excluded valid semi-IVs is that it requires a larger support than a standard IV, and thus a stronger relevance condition: there is a trade-off between relevance and exclusion. In practice, the full exclusion restriction is often the main obstacle for finding a valid IV, and finding relevant semi-IVs may be comparatively easier. In short, semi-IVs extend the toolkit to address endogeneity problems. \\
\indent For practical use, I also provide a closed-form formula for the average treatment effect, identified with semi-IVs, in a simple model with additive shocks and a homogenous treatment. It allows to quickly check (part of) the relevance condition on the semi-IVs and to have a first estimate before proceeding to the estimation of more complicated models with nonseparable unobserved heterogeneity, which are also completely identified (up to the individual treatment effects) with semi-IVs. \\



\noindent \textbf{Related literature} \\
\noindent There is a large literature on the identification of models with (nonseparable) endogeneity, and of treatment effects in such models \citep{angristimbens1994, angristimbensrubin1996, heckmanvytlacil1999, chesher2003, neweypowell2003, florensetal2008, imbensnewey2009}. This paper studies identification in a general framework where the endogenous selection variable is discrete and the potential outcomes are continuous nonparametric functions of a nonseparable unobserved variable. This unobserved variable also affects the discrete choice, hence endogeneity arises. The main identifying assumption is rank invariance: the potential outcomes are monotone with respect to the common unobserved variable. In this sense, this framework is close to the IV quantile treatment effect model (IVQTE) of \cite{chernozhukovhansen2005}, or \cite{vuongxu2017} in the binary selection case. The closest paper is \cite{bruneel2022}, which shares exactly the same model but with fully excluded standard IVs. 
The main contribution of this paper is to show that the nonparametric identification of this model can be obtained using semi-IVs which are only partially excluded from the potential outcomes. 
Specifically, I show that the model yields a system of partial differential equations and is identified if and only if there exists a unique strictly increasing solution to this system. The identification proof departs from the previous literature \citep[except for][]{bruneel2022} because I consider the identification of a general discrete number of alternatives in the selection equation under a weaker relevance condition on the instruments than the one usually imposed \citep{chernozhukovhansen2005}. These differences turn out to be essential for the proof of identification with semi-IVs. Indeed, even when the selection variable is binary, if the exclusion restriction is only partial, we have to identify more than two potential outcomes, and the identification arguments of \cite{vuongxu2017} in the binary case cannot be naturally extended. Moreover, the strong relevance condition of \cite{chernozhukovhansen2005} (nonsingularity of a matrix of selection probabilities) is likely to be locally violated, even in simple discrete models with standard IVs, and hence even more with semi-IVs, for which the relevance condition is more demanding due to the larger support. Hence the importance of being able to obtain point identification of the model with a weaker relevance assumption: as in \citeauthor{vuongxu2017}'s (\citeyear{vuongxu2017}) analysis of the binary case, I exploit the knowledge that the potential outcomes are monotone with respect to the unobserved heterogeneity to show that the model is still point identified even if the instrument is not relevant everywhere, as long as it is only irrelevant locally, on a set of isolated points. \\
%
%
%
\indent Many papers study the identification of nonseparable models by relaxing some of the assumptions imposed on IVs \citep[see][for a recent review of the use of IVs in econometrics]{imbens2014}. For example, \cite{mogstadetal2021} obtain identification under a partial monotonicity condition. \cite{kitagawa2021} shows the identification power of different independence conditions imposed on the instrument. \cite{dhaultfoeuillefevrier2015}, \cite{torgovitsky2015}, and \cite{neweystouli2021} show identification of nonseparable models with continuous endogenous variables when the instruments have only limited support. Given the crucial role of IVs for identification, some papers also provide tests of instrument validity and/or exclusion restrictions \citep{kitagawa2015, caetanoetal2016, andresenhuber2021, dhaultfoeuilleetal2021}. 
Part of the literature shows that identification can sometimes be obtained without instruments, provided that certain specific conditions hold, e.g., conditional partial independence \citep{mastenpoirier2018}, identification at infinity \citep{dhaultfoeuilleetal2018}, identification based on heteroskedasticity \citep{kleinvella2010, lewbel2012, lewbel2018}, or local irrelevance of the instrument \citep{dhaultfoeuilleetal2021}. 
Finally, \cite{kolesaretal2013} show that identification can be obtained from many invalid instruments which may be correlated with the outcome (and thus not fully excluded) if the direct effects of these invalid instruments on the outcomes are uncorrelated with the effects of the instruments on the endogenous (continuous) regressor. In this case, one can use multiple invalid instruments to construct a valid instrument that can be used for identification. \\
%
\indent The identification approach in this paper is novel and different from the aforementioned literature. I prove nonparametric identification of a general model with nonseparable unobserved heterogeneity, a discrete endogenous selection variable, and continuous potential outcomes, but with a relaxed exclusion restriction regarding the effect of the instrument on the outcomes. Contrary to the literature, I do not compensate the relaxation of the full exclusion restriction with any additional structure on the model (e.g., specific distribution of the error terms, parametric specification of the model, ...). %, independence of the instrument with the selection...). 
This is because I only \textit{partially} relax the full exclusion restriction. The main idea behind the identification with semi-IVs is still to exploit a variable that is partially excluded from the potential outcomes, except that its exclusion can be considerably more flexible than what has been imposed so far in the literature: full exclusion is merely a special case of the partial exclusion required for identification. 
Identification via targeted exclusion restrictions has been partly used in extended or generalized Roy models \citep{heckmanhonore1990, heckmanvytlacil2007a, bayeretal2011, frenchtaber2011, dhaultfoeuillemaurel2013}, but to the best of my knowledge, this type of reasoning has never been applied in the general context of treatment effects or nonseparable simultaneous equation models à la \cite{chernozhukovhansen2005}, with a general selection equation. This has important implications for empirical work because henceforth, researchers will be able to look for semi-IVs and not only fully excluded IVs when trying to address endogeneity problems. \\ 
%
%
\indent Section \ref{section_framework} presents the main identification arguments of the simultaneous equations framework when the selection variable is binary. Section \ref{section_generalization} generalizes the arguments to the case with selection into more than two alternatives and with a general set of partial exclusion restrictions, not limited to the example of the previous section. After the generalization, Section \ref{section_ate} provides a practical closed-form solution for the average treatment effect in a simplified model with additive heterogeneity. Section \ref{section_conclusion} concludes. 







\section{The Model}\label{section_framework}

In this section, I present the model and the principal identification results. First, I focus on the case where the endogenous variable (treatment) is binary, $D=0$ or $D=1$, to simplify the exposition of the constructive identification arguments. However the identification does not rely on the binarity of the selection variable, and I provide a generalization with a discrete endogenous variable and general semi-instrumental variables in Section \ref{section_generalization}. 




\subsection{The (binary) Framework}\label{subsection_framework_binary}
Consider the nonparametric, nonseparable, triangular system of simultaneous equations 
\begin{align}\label{system_binary}
\left\{
    \begin{array}{l}
        Y_d = q_d(X, W_d, \Eta), \\
        D = b(X, W_0, W_1, \Eta, \Epsilon),  
    \end{array}
\right. \text{ for } d=0, 1.
\end{align}
Refer to the first line as the potential outcome equations and to the second line as the selection equation. $D \in \{0, 1\}$ is an observed (endogenous) binary choice/treatment, and $\{Y_d\}$ are continuous latent potential outcomes. The researcher only observe a continuous outcome $Y$, which corresponds to the outcome in the selected alternative $D$, i.e., $Y = \sum^{1}_{d=0} Y_d \ \mathds{1}\{D=d\}.$
%\begin{align*}
%	Y = \sum^{1}_{d=0} Y_d \ \mathds{1}\{D=d\}.
%\end{align*}
$X$ is a vector of observed covariates which affect the outcomes and the selection. The nonseparable heterogeneity scalar shock, $\Eta$, is unobserved and is responsible for the heterogeneity of outcomes at fixed $x$, $d$ and $w_d$. The nonseparable shock $\Eta$ also affect the selection of the binary choice, yielding an endogeneity problem with respect to the outcome. 
In addition to $\Eta$, there exist separable shocks, $\Epsilon$, which only affect the selection $D$. 
$W_0$ and $W_1$ are two binary semi-Instrumental Variables (semi-IVs) which are relevant for the selection of the endogenous variable $D$, but are excluded from (only) one of the two potential outcomes (hence the ``partially excluded'' terminology). This is the main difference with the usual nonseparable triangular model with an instrumental variable \citep{chernozhukovhansen2005, vuongxu2017}, where the IV would enter the selection equation, but be completely excluded from the potential outcomes. \\
%
%
%
\indent The model also satisfy the following conditions. 
\begin{assumption}[Monotonicity]\label{ass_monot}
For each $d$, the potential outcomes functions are continuously differentiable and 
\begin{align*}
	\frac{\partial q_d(x, w_d, \eta)}{\partial \eta} > 0.
\end{align*}
\end{assumption}
\begin{normalization}\label{contshock} Conditional on $X=x$, $\Eta$ is continuously distributed as $\mathcal{U}(0,1)$.
\end{normalization}

\begin{assumption}[Independence]\label{indep_shock} Conditional on $X=x$, $(W_0, W_1)$ are independent from $\Eta$. 
\end{assumption}


\begin{assumption}[Support of the semi-IV]\label{supp_pev} For all $x$ and for each $(w_0, w_1)$ in $\{0,1\}^2$, $\textrm{Pr}(W_0=w_0, W_1=w_1 |X =x) \neq 0$. \end{assumption}

\begin{assumption}[Regularity]\label{regularity} The selection function $b(x, w_0, w_1, \Eta, \Epsilon)$ and the $\Epsilon$ shocks are such that $0 < \textrm{Pr}(D=d | X=x, W_0=w_0, W_1=w_1, \Eta=\eta) < 1$ for all $\eta, x$ and for each $(w_0, w_1) \in \{0,1\}^2$ and each $d=0,1$. 
\end{assumption}





\indent Under Assumption \ref{ass_monot}, there is a one-to-one mapping between $\Eta$ and $Y_d$ for every $d, x, w_d$. This kind of monotonicity condition has been widely used for identification \citep{matzkin2003}, and it means we only identify monotone effects of the unobserved nonseparable source of heterogeneity, $\eta$. A very important limitation of Assumption \ref{ass_monot} is that it requires a strictly continuous potential outcome for each alternative $d$. This rules out discrete response outcomes for example. 
Since the same $\Eta$ affects both potential outcome in the model (\ref{system_binary}), the monotonicity implies \textit{rank invariance}. Indeed, the higher $\Eta$, the higher the relative rank in the distribution of $Y_0$ and $Y_1$. The rank invariance condition is slightly different from the standard rank invariance \citep{chernozhukovhansen2005}, because here the semi-IVs also enter the potential outcome functions. 
Notice also that $\Eta$ is not directly identifiable from the data, so I normalize it to the uniform distribution, such that only the rank is identified. \\ % (Assumption \ref{contshock})
\indent Assumption \ref{indep_shock} is a standard independence assumption between the semi-IVs and the unobserved variable. Importantly, $W_0$ and $W_1$ may be correlated with each other, as long as they are not perfectly correlated (which would violate Assumption \ref{supp_pev}). \\
\indent Assumption \ref{supp_pev} insures that all the combinations of the semi-IVs occur in the data. Here it means that the support of $Z=(W_0, W_1)$ contains $2\times2=4$ different values which are all observed with non zero probability for all $X=x$. \\
\indent I do not require independence between $\Epsilon$ and $\Eta$ or between $\Epsilon$ and $(W_0, W_1)$. The shock $\Epsilon$ is only present in the model to generate the selection probabilities, and I only require the regularity Assumption \ref{regularity} that we observe the full support of both potential outcomes $Y_d$ for each values of $(d, w_d, x)$.  \\




\noindent \textbf{Example:} \\

\begin{figure}[h]
\centering
% https://tex.stackexchange.com/questions/51228/how-to-increase-the-horizontal-distance-between-nodes
%\begin{tikzpicture}[node distance={25mm}, thick, main/.style = {draw, circle}] 
\begin{tikzpicture}[node distance={25mm}, thick] 
\node (D) {$D$}; 
\node (W0) [above left=0.5cm and 2cm of D]  {$W_0$}; 
\node (W1) [below left=0.5cm and 2cm of D]  {$W_1$}; 
\node (h) [above right=2cm and 1.cm of D] {$\Eta$}; 
\node (Y) [right=2.5cm of D]{$Y$}; 
\node (Y1) [below right=0.5cm and 3cm of D] {$Y_1$}; 
\node (Y0) [above right=0.5cm and 3cm of D] {$Y_0$}; 
\draw[->] (D) -- (Y); 
\draw[dotted] (Y) -- (Y1); 
\draw[dotted] (Y) -- (Y0); 
\draw[->] (W0) -- (D); 
\draw[->] (W1) -- (D);
\draw[dashed, ->] (W0) -- (Y0); 
\draw[dashed, ->] (W1) -- (Y1);
\draw[dashed] (W0) to [out=180,in=-180,looseness=1.5] (W1); 
\draw[dashed, ->] (h) -- (D); 
\draw[dashed, ->] (h) -- (Y0);
\draw[dashed, ->] (h) to [out=360,in=360,looseness=2] (Y1); 
\end{tikzpicture} 
\caption{Graph of the binary framework (without covariates $X$)}
\end{figure}

\noindent Consider the standard example of the returns to education \citep{angristkrueger1991}. The binary decision is to go to college $(D=1)$ or not $(D=0)$. The potential outcomes are the earnings of the individuals if they went to college ($Y_1$), or not ($Y_0$). $X$ represents demographics which influence both the choice and the outcomes. The nonseparable shock $\Eta$ represents individuals unobserved ability. Ceteris paribus, a higher ability yields higher income, regardless of the chosen education, demographics and local labour market conditions (rank invariance). $\Epsilon$ represents other unobserved factors influencing the decision to study or not, the preference for studying for example. 
For the semi-IVs, $W_0$ is a variable that is excluded from the potential earnings of educated individuals (controlling for $W_1$) but not necessarily excluded from the earnings of individuals who did not go to college, and vice versa for $W_1$. The local earnings or unemployment rate of college-goers ($W_1$) and non-goers ($W_0$) at the time of the decision (at $17$ years old) can be used as such semi-IVs. $W_0$ may not be excluded from the current earnings of non-goers (because of time auto-correlation) but it is likely to be excluded from the current earnings of college-goers, especially controlling for $W_1$. 
About the other assumptions, $W_0$ and $W_1$ should be independent from the individual specific ability draw, and, both semi-IVs should be relevant for the selection into education: everything else equal, if one expects better labor market conditions after college, one is more likely to go to college. %The exact relevance requirement is slightly more complicated and will appear clearly in the next section. 
Obviously, $W_0$ and $W_1$ are correlated with each other here, the support condition only requires that they are not perfectly correlated, i.e., that there are some markets with relatively low earnings/unemployment rate for uneducated individuals but high income for educated individuals and vice versa. 







\subsection{Identification}

We observe $(D, Y, X, W_0, W_1)$ for every individual, with $Y= (1-D) Y_0 + D Y_1$. 
For all $(x, \eta, w_0, w_1) \in \mathcal{X}\times \mathcal{H} \times \{0, 1\}^2$, I study nonparametric identification of the two potential outcome functions, $q_d(x, w_d, \eta)$ and of the selection probability.
The analysis in this section is all conditional on $X=x$, but I suppress this dependence for ease of notation.
In this case, the observable data is fully characterized by the joint distribution of $(D, Y)$ given $(W_0, W_1)$, i.e., by 
\begin{align*}
	F_{D, Y | w_0, w_1}(d, y) = \textrm{Pr}(D=d, Y=y | W_0=w_0, W_1=w_1). 
\end{align*}
Notice that, even though $W_0$ is excluded from $Y_1$, the joint density is still conditional on both semi-IVs since they are both relevant for the selection ($D$).  
Let us also introduce a natural notation, $q_{dw_d}(\eta) := q_d(w_d, \eta)$. This conveys the idea that we have four conditional potential outcomes: $y_{dw_d} \in \mathcal{Y}_{dw_d} = \text{Supp}(Y_{dw_d}) = [ q_{dw_d}(0), q_{dw_d}(1) ]$ (by continuity of $q(\cdot)$ and uniformity of $\Eta$) for $(d, w_d) \in \{0, 1\}^2$. \\




\noindent \textbf{Identification with the semi-IVs:} \\ 
Given our model and assumptions, for $\eta \in [0, 1]$, $(w_0, w_1) \in \{0, 1\}^2$, we have
\begin{align}\label{main_equation_binary}
\eta &= \quad \textrm{Pr}(\Eta \leq \eta) \nonumber \\
&= \quad \textrm{Pr}(\Eta \leq \eta| W_0=w_0, W_1=w_1) \nonumber \\
&= \quad \textrm{Pr}(\Eta \leq \eta,  D = 0 | \ W_0=w_0, W_1=w_1)   \nonumber\\ 
&\quad + \ \textrm{Pr}(\Eta \leq \eta,  D = 1 | \ W_0=w_0, W_1=w_1)  \nonumber \\
&= \quad \textrm{Pr}(Y_0 \leq q_{0w_0}(\eta),  D = 0 | \ W_0=w_0, W_1=w_1) \nonumber \\
&\quad + \ \textrm{Pr}(Y_1 \leq q_{1w_1}(\eta),  D = 1 | \ W_0=w_0, W_1=w_1)\nonumber \\ 
&= F_{D, Y | w_0, w_1}(0, q_{0w_0}(\eta)) + F_{D, Y | w_0, w_1}(1, q_{1w_1}(\eta)).  %\\
\end{align}
The first equality comes from the fact that $\Eta$ is uniform. The second one uses the independence of the semi-IVs from $\Eta$. Then I just rewrite the equation using the law of total probability and the next equality follows from the monotonicity of $q_d$ and from the fact that $Y=Y_d$ when $D=d$. The last equation is only a change on notation. \\
\indent Let us take the derivative of each equation with respect to $\eta$ to obtain a system of differential equations instead: 
\begin{align*}\label{system_equa_diff1}
	f_{D, Y | w_0, w_1}\Big( 0, q_{0w_0}(\eta)\Big) \ \frac{\partial q_{0w_0}(\eta)}{\partial \eta} + f_{D, Y | w_0, w_1}\Big(1, q_{1w_1}(\eta)\Big) \  \frac{\partial q_{1w_1}(\eta)}{\partial \eta} = 1.
\end{align*}
This can be rewritten under matrix form as: 
%\vspace{-1\baselineskip}
{\small
\begin{widerequation}
	\begin{bmatrix}
		f_{D, Y | 0, 0}\big( 0, q_{00}(\eta)\big) & 0  & f_{D, Y | 0, 0}\big( 1, q_{10}(\eta)\big) & 0 \\
		0 & f_{D, Y | 1, 0}\big( 0, q_{01}(\eta)\big)  & f_{D, Y | 1, 0}\big( 1, q_{10}(\eta)\big) & 0 \\
		f_{D, Y | 0, 1}\big( 0, q_{00}(\eta)\big) & 0  & 0 & f_{D, Y | 0, 1}\big( 1, q_{11}(\eta)\big) \\
		0 & f_{D, Y | 1, 1}\big( 0, q_{01}(\eta)\big) & 0  & f_{D, Y | 1, 1}\big( 1, q_{11}(\eta)\big) \\
	\end{bmatrix} 
	\begin{bmatrix}
		\partial q_{00}(\eta)/\partial \eta \\
		\partial q_{01}(\eta)/\partial \eta \\
		\partial q_{10}(\eta)/\partial \eta \\
		\partial q_{11}(\eta)/\partial \eta \\
	\end{bmatrix} = 	\begin{bmatrix}
		1 \\
		1 \\
		1 \\
		1 \\
	\end{bmatrix}, \nonumber
\end{widerequation} }
\begin{align}\label{system_equa_diff}
\text{i.e., } \quad	M(\mathbf{q}(\eta)) \ \frac{\partial \mathbf{q}(\eta)}{\partial \eta} = \begin{bmatrix}
		1 &
		1 &
		1 &
		1 &
	\end{bmatrix}^T, 
\end{align}
where $\mathbf{q}(\eta)$ is the vector of the four potential outcomes: $q_{00}(\eta),$ $q_{01}(\eta),$ $q_{10}(\eta),$ and $q_{11}(\eta)$. 
The joint densities of $(D, Y)$ given $(W_0, W_1)$ are observed in the data. So, for any vector $\mathbf{y} = [y_{00}, y_{01}, y_{10}, y_{11}]^T$ where $y_{dw_d} \in \mathcal{Y}_{dw_d} = \text{Supp}(Y_{dw_d})$, we can compute $M(\mathbf{y})$ as 
%\vspace{-1\baselineskip}
{
\begin{align*}
		M(\mathbf{y}) = \begin{bmatrix}
		f_{D, Y | 0, 0}\big( 0, y_{00}\big) & 0  & f_{D, Y | 0, 0}\big( 1, y_{10}\big) & 0 \\
		0 & f_{D, Y | 1, 0}\big( 0, y_{01}\big)  & f_{D, Y | 1, 0}\big( 1, y_{10}\big) & 0 \\
		f_{D, Y | 0, 1}\big( 0, y_{00}\big)  & 0  & 0 & f_{D, Y | 0, 1}\big( 1, y_{11}\big) \\
		0 & f_{D, Y | 1, 1}\big( 0, y_{01}\big)  & 0  & f_{D, Y | 1, 1}\big( 1, y_{11}\big) \\
	\end{bmatrix}. \nonumber
\end{align*}}


\noindent For any $\Eta=h$, we have a quasilinear system of $|\text{Supp}(W_0)|\times |\text{Supp}(W_1)| = 2\times 2$ differential equations for four unknowns: $q_{00}(\eta), q_{01}(\eta), q_{10}(\eta), q_{01}(\eta)$ that we want to identify. Note that, thanks to Assumption \ref{regularity}, for each $(d, w_d)$, $q_{dw_d}(0)$ and $q_{dw_d}(1)$ are directly identified in the data as the observable minimum and maximum values of $Y$ given that $D=d$ and $W_d=w_d$. Thus, the potential outcomes $\mathbf{q}(\cdot)$ are identified, if and only if, starting from known $\mathbf{q}(0)$, there exist a unique \textit{strictly increasing solution} to the system (\ref{system_equa_diff}) (that goes to from $\mathbf{q}(0)$ to $\mathbf{q}(1)$).\footnote{Alternatively, we can solve the system backward, starting from $\mathbf{q}(1)$.} \\
%
\indent In contrast to previous studies \citep{neweypowell2003, chernozhukovhansen2005}, I will not proceed with the identification of the outcome functions pointwise. Instead, I want to identify the entire $q_{dw_d}(\cdot)$ functions, for all $\eta$, by solving the system of differential equations. By proceeding separately, point by point, we do not exploit one important feature of the model here: $q_{dw_d}(\cdot)$ are monotone. Thus, the system is identified if and only if there exist a unique function $\mathbf{q}(\eta)$ which is strictly increasing for all $\eta$. There may also be some other non-increasing solutions to the system, but these are ruled out by monotonicity. The monotonicity brings additional identification power which cannot be used if we proceed to the identification pointwise. To the best of my knowledge, \cite{vuongxu2017} are the only others to exploit the identification power of the known monotonicity, but their method only applies to the binary case with standard IVs. As in \cite{bruneel2022}, I do it in a more generalized manner here. By writing the problem as a system of differential equations, I can clearly express the identifying conditions, and extend the identification to any dimensional matrix (not only the binary case with a fully excluded IV, i.e., a $2\times2$ matrix implicitely). \\
%
%
%
%
%
\indent Let me now provide a sufficient condition for the uniqueness of a strictly increasing solution to system (\ref{system_equa_diff}). 
Denote $p_{D|\Eta, w_0, w_1}(d, \eta) = \textrm{Pr}(D=d | \Eta = \eta, W_0=w_0, W_1=w_1)$, the selection probabilities. %, or the Conditional Choice Probabilities (CCP) following \cite{bruneel2022} and building on the original name given by \cite{hm1993}.


\begin{assumption}[Relevance]\label{relevance} The matrix of selection probabilities  
\begin{align*}
		\tilde{M}(\eta) = \begin{bmatrix}
		p_{D|\Eta, 0, 0}(0, \eta) & 0  & p_{D|\Eta, 0, 0}(1, \eta) & 0 \\
		0 & p_{D|\Eta, 1, 0}(0, \eta) & p_{D|\Eta, 1, 0}(1, \eta) & 0\\
		p_{D|\Eta, 0, 1}(0, \eta) & 0  & 0 & p_{D|\Eta, 0, 1}(1, \eta)  \\
		0 & p_{D|\Eta, 1, 1}(0, \eta)  & 0 & p_{D|\Eta, 1, 1}(1, \eta)  \\
	\end{bmatrix}, 
\end{align*}
has full rank for all $\eta \in \mathcal{H} \backslash \mathcal{K}$. $\mathcal{K}$ is a (possibly empty) finite set containing $K$ ($\geq 0$) isolated values $\eta_k$, at which there is a rank-one deficiency, i.e., $\text{rank} \big(\tilde{M}(\eta_k)\big) = \text{Number of rows of }\tilde{M}(h_k) - 1$. 
\end{assumption}


\begin{theorem}[Identification]\label{identification_theorem}
Suppose that Assumptions \ref{ass_monot}-\ref{relevance} hold and that the observable joint distributions are drawn from the model (\ref{system_binary}). 
%the observable reduced form joint distributions are compatible with the model (\ref{system_binary}). 
Then there exists a unique set of four strictly increasing potential outcome functions, $q_{dw_d}(\eta)$, mapping $[0, 1]$ into $\mathcal{Y}_{dw_d}$ (for each $d = 0, 1$ and $w_d = 0, 1$) which solve the system of differential equation (\ref{system_equa_diff}).  
%For every reduced form compatible with the structural model, there exist unique conditional continuous choice (CCC) functions $c_d(h)$ (for $d = 0, ..., J$) mapping $[0, 1]$ into $\mathcal{C}_d$, that are strictly increasing and satisfy
\end{theorem} 

\begin{proof}
See Appendix \ref{appendix_identification_proof}. 	
\end{proof} 

\vspace{0.5\baselineskip}

\indent The identification Assumption \ref{relevance} is a relevance condition on the effect of the semi-IVs on the selection. One can show that 
\begin{align}\label{irrelevant_odds}
	&\text{det}\big(\tilde{M}(\eta)\big) \neq 0 \nonumber \\ 
	&\iff \frac{p_{D|\Eta, 0, 0}(1, \eta)/\big(1-p_{D|\Eta, 0, 0}(1, \eta)\big)}{p_{D|\Eta, 0, 1}(1, \eta)/\big(1-p_{D|\Eta, 0, 1}(1, \eta)\big)} \neq \frac{p_{D|\Eta, 1, 0}(1, \eta)/\big(1-p_{D|\Eta, 1, 0}(1, \eta)\big)}{p_{D|\Eta, 1, 1}(1, \eta)/\big(1-p_{D|\Eta, 1, 1}(1, \eta)\big)}. \\ \nonumber
\end{align}
\vspace{-2\baselineskip}

\noindent In other words, the matrix is non invertible when the odds ratio of choosing $D=d$ instead of the alternative $D=d'$ when $W_d = 0$ over the same odds when $W_d = 1$, is independent from $W_{d'}$.  Trivially, condition (\ref{irrelevant_odds}) is violated when $W_0$ is irrelevant, i.e., when $p_{D|\Eta,0,w_1}(1, \eta) = p_{D|\Eta,1,w_1}(1, \eta)$ for all $w_1 \in \{0, 1\}$ (or similarly when $W_1$ is irrelevant).\footnote{Notice that if $p_{D|\Eta,0,0}(1, \eta) = p_{D|\Eta,1,0}(1, \eta)$ but $p_{D|\Eta,0,1}(1, \eta) \neq p_{D|\Eta,1,1}(1, \eta)$, the relevance condition (\ref{irrelevant_odds}) still holds. So $W_0$ can be irrelevant conditional on some $W_1=w_1$, as long as it is not irrelevant for the other value of $w_1$.}  This is expected: as with a standard instrumental variable, if the IV is irrelevant for the selection, we lose identification.
%if the effect of the instrument is irrelevant on the selection, we lose identification.
In addition to the separate relevance of the semi-IVs, condition (\ref{irrelevant_odds}) also requires a \textit{joint relevance} of the two semi-IVs for identification. \\
%
%
%
%
%
\indent This relevance condition is the main identification condition because it is, in fact, related to the solvability of the quasilinear system of differential equations (\ref{system_equa_diff}), i.e., to the invertibility of $M(\mathbf{q}(\eta))$. Indeed, note that for all $d, w_d$, the joint density is 
\begin{align}\label{property_joint_density}
	f_{D, Y | w_0, w_1}\big( d, y_{dw_d}\big) = 	p_{D|\Eta, w_0, w_1} \big(d, \eta_{dw_d}(y_{dw_d})\big) \ \frac{\partial \eta_{dw_d} ( y_{dw_d} )}{\partial y_{dw_d}}, 
%	\textrm{Pr}(D=d|\Eta=h_{dw_d}(y_{dw_d}), W_0=w_0, W_1=w_1) \ \frac{\partial h_{dw_d} ( y_{dw_d} )}{\partial y_{dw_d}}, 
\end{align}
where we define $\eta_{dw_d}(\cdot): \mathcal{Y}_{dw_d} \rightarrow [0, 1]$ as $\eta_{dw_d}(y_{dw_d}) = q_{dw_d}^{-1}(y_{dw_d})$. By monotonicity, these inverse functions exist, are uniquely defined, and strictly increasing. Thus, one can easily check that, for the \textit{true potential outcomes}, $\mathbf{q}(\eta)$, we have $\eta_{dw_d}(q_{dw_d}(\eta)) = \eta$, and thus
\begin{align*}
	M(\mathbf{q}(\eta)) &= \tilde{M}\big(\eta\big) H\big(\mathbf{q}(\eta)\big), \\
\text{where } H(\mathbf{q}(\eta)) &= \begin{bmatrix}
		\frac{\partial \eta_{00} ( q_{00}(\eta) )}{\partial y_{00}} & 0  & 0 & 0 \\
		0 & \frac{\partial \eta_{01} ( q_{01}(\eta) )}{\partial y_{01}} & 0  & 0  \\
		0 & 0 & \frac{\partial \eta_{10} ( q_{10}(\eta) )}{\partial y_{10}} & 0 \\
		0 & 0 & 0 & \frac{\partial \eta_{11} ( q_{11}(\eta) )}{\partial y_{11}} \\
	\end{bmatrix}. 
\end{align*}
Given the monotonicity assumption, all the diagonal elements of $H(\mathbf{q}(\eta))$ are strictly positive, so this matrix is always invertible. As a consequence, 
\begin{align*}
	det\big(\tilde{M}\big(\eta\big)\big) \neq 0 \iff det\big(M\big(\mathbf{q}(\eta)\big)\big) \neq 0.
\end{align*}
In other words, in the identifying assumption \ref{relevance}, the relevance condition (\ref{irrelevant_odds}) is a condition on the invertibility of $M(\mathbf{q})$ along the true path (defined by the true potential outcome functions) in the quasilinear system of differential equations (\ref{system_equa_diff}). When $M(\mathbf{q})$ is invertible, we can rewrite (\ref{system_equa_diff}) as a standard system of differential equations
%\begin{align}\label{system_equa_diff_invert}
%\frac{\partial \mathbf{q}(\eta)}{\partial \eta} = \Big(M\big(\mathbf{q}(\eta)\big)\Big)^{-1} \begin{bmatrix} 1 & 1 & 1 & 1  \end{bmatrix}^T.  
%\end{align}
\begin{align}\label{system_equa_diff_invert}
\frac{\partial \mathbf{q}(\eta)}{\partial \eta} = \Big(M\big(\mathbf{q}\big)\Big)^{-1} \begin{bmatrix} 1 & 1 & 1 & 1  \end{bmatrix}^T.  
\end{align}
The system (\ref{system_equa_diff_invert}) defines a unique derivative solution if $M(\mathbf{q})^{-1}$ exists and is well defined. This suggests why Assumption \ref{relevance} is an identifying assumption.  \\
\indent In the case where the relevance condition (\ref{irrelevant_odds}) holds for all $\eta \in [0, 1]$ (i.e., $K=0$ in Assumption \ref{relevance}), then the system of differential equations can be written as (\ref{system_equa_diff_invert}) for all $\eta$, and there exists a unique solution starting from the known $\mathbf{q}(0)$ to the system (\ref{system_equa_diff}) by applying the Picard-Lindelöf Theorem for nonlinear system of differential equations. Hence Theorem \ref{identification_theorem} is easy to prove in this case. 
Notice that the identification invertibility condition only needs to hold at the true potential outcome functions $\mathbf{q}(\eta)$. Because if one starts on the true path of outcomes (which we do by starting from $\mathbf{q}(0)$), the differential equation will never deviate from it under Assumption \ref{relevance}. 
%
%
%
%
\indent Now, I did not exploit the knowledge that the solutions $\mathbf{q}(\cdot)$ must be strictly increasing with respect to $\eta$. Monotonicity allows me to identify the solution to system (\ref{system_equa_diff}), even if the Picard-Lindelöf conditions do not always hold, i.e., even if the determinant of $M$ is equal to zero on a set of isolated point at which we cannot write the system under inverted form (\ref{system_equa_diff_invert}). 
The main idea why we preserve point identification of the solution is that, when we reach an isolated critical point for the determinant of $M$, if there is only a rank-one deficiency, there is only two possible solutions to the original quasilinear system of differential equations (\ref{system_equa_diff}), and only one of these two solutions is strictly increasing. Therefore, thanks to the monotonicity restriction, our potential outcomes are still point identified by the unique strictly increasing solution.\footnote{If the rank deficiency at the singularities is greater than $1$, there may be more than two solutions at that point and the model would not be identified anymore. However, in the model of this section with binary selection, if the selection probabilities are such that $M$ is non invertible, the rank of $M$ is always $3$ (except in the extreme case where the probabilities are equal to one, which is ruled out by assumption). So the restriction to rank-one deficiency is irrelevant here. %If there is a rank deficiency, it is always one in the binary model described here, so this restriction is irrelevant. \\ 
It may be more restrictive in the extension to a discrete number of alternatives where we could possibly have larger rank deficiency. However, even there, it is unlikely that several semi-IVs become irrelevant exactly at the same value of $\eta$, so the restriction to rank deficiencies of one (instead of more) is still weak. } \\
\indent This seemingly technical point is in fact quite important in practice. Indeed, in many practical applications, even with simple models, it is possible that the determinant of $\tilde{M}(\eta)$ crosses zero at some isolated values of $\eta$. In this binary model here, it would mean that the odds ratio given of $D=1$ over $W_1$ is independent from $W_0$ on a set of isolated points. This is perfectly plausible if both odds ratio are constantly evolving with different slopes for example. In the extension to the discrete case with a larger number of alternatives and thus higher dimensional matrix $M$, observing rank deficiencies is even more likely, even with only fully excluded IVs \citep{bruneel2022}. Therefore, exploiting monotonicity allows to identify a much wider class of models. \\ 
%
%
%
\indent Unfortunately, if the determinant is zero on an interval, then we can only have set identification of the potential outcomes on the sets where the determinant is non zero. \\ 
%
\indent Finally, note that when we identify $\mathbf{q}(\eta)$, we identify the entire model, i.e., we also identify the selection probabilities, $p_{D|\Eta, w_0, w_1}(d, \eta)$ for all $\eta, d, w_0, w_1$. Indeed, if $\mathbf{q}(\eta)$ are identified, the inverse functions $h_{dw_d}(\cdot)$ are also identified. So, using property (\ref{property_joint_density}), we identify the selection probabilities as 
\begin{align*}
	p_{D|\Eta, w_0, w_1} \big(d,  \eta \big) = f_{D, Y | w_0, w_1}\big( d, q_{dw_d}(\eta) \big)  	 \ \bigg/ \ \frac{\partial \eta_{dw_d} ( q_{dw_d}(\eta) )}{\partial y_{dw_d}}. 
%	\textrm{Pr}(D=d|\Eta=h_{dw_d}(y_{dw_d}), W_0=w_0, W_1=w_1) \ \frac{\partial h_{dw_d} ( y_{dw_d} )}{\partial y_{dw_d}}, 
\end{align*}

In fact, once the potential outcomes, the selection probabilities and the unobserved $\Eta$ are identified for every individuals, we identify the counterfactual outcomes of any individual if they selected another alternatives. Thus we also identify the individual treatment effect. 
%the individual treatment effect: i.e., what would have been the outcome of the individ












\section{General identification with partial exclusion restrictions}\label{section_generalization}

The main requirement for identification of the model is that the system of differential equations (\ref{system_equa_diff}) has a unique solution, i.e., that the matrix $\tilde{M}$ is invertible (except possibly on a set of isolated points). The underlying problem is that, in a problem with endogenous selection, without instruments, we have a system of $1$ equation for $J$ unknowns: 
\begin{align*}
	\eta = \sum^J_{d=1} F_{D, Y}(q_d(\eta)). 
\end{align*}
An instrumental variable $Z$ solves this identification problem by yielding $J$ equations, one for each value of $Z$, using the independence of $Z$ from $\Eta$. And thanks to the full exclusion restriction, it does not yield any additional unknown potential outcome functions to identify. So we have a system of $J$ equations for $J$ unknowns, 
\begin{align*}
	\eta = \sum^J_{d=1} F_{D, Y|Z}(q_d(\eta))
\end{align*}
which can be identified under appropriate relevance condition. 
But, as illustrated in the previous section, the full exclusion restriction imposed on IVs is merely a special case of all the partial exclusion restrictions which can be applied to recover identification. With semi-IVs whose support is larger than $J$, we do not need to impose full exclusion anymore, there are many other combination of partial exclusion restrictions which yield identification. I describe this generalization in this section, and provide (easily checkable) necessary conditions on the set of partial exclusion restrictions for identification. Once this is satisfied, identification comes from an appropriate relevance condition on the effect of the semi-IVs on the selection, as in the previous example. 
Once again, this section is all conditional on $X=x$ and I suppress this dependence for ease of notation.




\subsection{General Framework}

\indent Let us denote $Z$ a single general semi-IV with discrete Support $\mathcal{Z}=\{1, ..., N_Z\}$ of size $|\mathcal{Z}|=N_Z$.\footnote{About the terminology: I use interchangeably semi-IVs in plural (in the previous section) or singular (in this section). But essentially the discrete semi-IVs can be written as a single `semi-IV', $Z$.} 
%
%
%
\noindent The framework described in Section \ref{section_framework} extends naturally. Let us focus on a general nonparametric, nonseparable, triangular system of simultaneous equations
\begin{align}\label{system_general}
\left\{
    \begin{array}{l}
        Y_d = q_d(Z, \Eta), \\
        D = b(Z, \Eta, \Epsilon), 
    \end{array}
\right. \text{ for } d = 1, ..., J.
\end{align}
We consider a selection with a discrete number of alternatives, $D \in \{1, ..., J\}$. Assumptions \ref{ass_monot}-\ref{regularity} still hold with trivial adjustments to the setup changes (replacing the $W_d$ by $Z$). The support condition (Assumption \ref{supp_pev}) now becomes very standard and require that $\textrm{Pr}(Z=z | X=x) > 0$ for all $z \in \mathcal{Z}$ (as in any IV setup).  
Given our model and assumptions, for $\eta \in [0, 1]$, $z \in \mathcal{Z}$, we obtain the counterpart to system (\ref{main_equation_binary}), 
\begin{align}\label{main_equation_general}
\eta 
%&= \quad \textrm{Pr}(\Eta \leq \eta) \nonumber \\
%&= \quad \textrm{Pr}(\Eta \leq \eta| Z=z_i) \nonumber \\
%&= \quad \sum^{J-1}_{d=0} \textrm{Pr}(\Eta \leq \eta,  D = d | \ Z=z_i)  \nonumber \\
%&= \quad \sum^{J-1}_{d=0}  \textrm{Pr}(Y_d \leq q_{d}(\eta, z_i),  D = d | \ Z=z_i) \nonumber \\ 
= \quad \sum^{J}_{d=1} F_{D, Y | z}(d, q_{dz}(\eta)).  %\\
\end{align}
%
%
\noindent This is a system of $N_Z$ equations, each equation with $J$ different unknowns. 
Let us denote $N_d$ the number of different $q_{dz}(\cdot)$ functions we need to identify in alternative $d$. Without exclusion restriction, $N_d = N_Z$ for each $d$. Therefore, without any exclusion restriction, we have a vector of $J \times N_Z$ different potential outcome functions, $q_{dz}(\eta)$, to identify, denoted
\begin{align*}
	\mathbf{q}(\eta) = 
	\begin{bmatrix} 
	q_{11}(\eta) &
 	%q_{02}(\eta) &
 	 \cdots &
 	 q_{1N_Z}(\eta) &
 	 q_{21}(\eta)  &
 	  \cdots &
 	 q_{2N_Z}(\eta) &
 	 %\cdots &
 	 %q_{(J-1)1}(\eta) &
 	  \cdots &
 	 q_{JN_Z}(\eta)
 \end{bmatrix}^T \text{for all } \eta.
 \end{align*}
This yields a system of $N_Z$ equations with $J\times N_Z$ unknowns overall: this is not solvable. \\
%
\indent For identification, one needs to impose partial exclusion restrictions. 
What is a partial exclusion restriction? It is an equality assumption on the potential outcome functions, such that, for two values of the instrument, $(z, k)$, the functions $q_{dz}(\cdot) = q_{dk}(\cdot)$ for all $\eta$.\footnote{I consider only \textit{within alternative} partial exclusion restrictions here. One could imagine \textit{inter-alternatives exclusions}, for example assume that $q_{0z}(\cdot) = q_{1z}(\cdot)$, i.e., assume that $D$ has no effect on the potential outcome given some values of the instrument.} 
For example, standard IVs impose the full exclusion restriction that $q_{dz}(\cdot) = q_d(\cdot)$ for all $\eta$ and for each $z$. But this is only the strongest possible exclusion assumption, and identification can be obtained with only partial exclusion restrictions. \\
%
%
\indent The partial exclusion restrictions map all the potential outcomes $q_{dz}(\cdot)$ (for all $z$) to some `unique' potential outcomes, $\tilde{q}_{d1}(\cdot), \tilde{q}_{d2}(\cdot), ..., \tilde{q}_{dN_d}(\cdot)$, for each $d \in \{1, ..., J\}$.
Once we apply the exclusion restrictions, we end up with $N_Q \leq N_Z\times J$ unique potential outcome functions to identify, with $N_Q = \sum^{J}_{d=1} N_d$. Following the new notation, the $N_Q\times1$ potential outcome vector we want to identify is now 
\begin{align*}
	\mathbf{\tilde{q}}(\eta) = 
	\begin{bmatrix} 
	\tilde{q}_{11}(\eta) &
 	%q_{02}(\eta) &
 	 \cdots &
 	 \tilde{q}_{1N_1}(\eta) &
 	 \tilde{q}_{21}(\eta)  &
 	  \cdots &
 	 \tilde{q}_{2N_2}(\eta) &
 	 %\cdots &
 	 %q_{(J-1)1}(\eta) &
 	  \cdots &
 	 \tilde{q}_{JN_{J}}(\eta)
 \end{bmatrix}^T \text{for all } \eta.
 \end{align*}
 The partial exclusion restrictions can be represented as the following $N_Z \times N_Q$ mapping between $q_{dz}$ and $\mathbf{\tilde{q}}$, 
\begin{align*}
	\mathbf{\chi}(\mathbf{q}, \mathbf{\tilde{q}}) &= 
	\begin{bmatrix} 
	\chi_{11} & \chi_{21} & \cdots & \chi_{J1} \\ 
	\chi_{12} & \ddots & & \vdots \\ 
	\vdots & & \ddots & \vdots \\
	\chi_{1N_Z} & \chi_{2N_Z} & \cdots & \chi_{JN_Z} \\
	\end{bmatrix}, \\
	%\vspace{-2\baselineskip} \\
	%\\
\text{where } \chi_{dz} &= \underbrace{\begin{bmatrix}
 	\mathds{1}\{q_{dz} = \tilde{q}_{d1} \} &  \mathds{1}\{ q_{dz} = \tilde{q}_{d2} \} & \cdots & \mathds{1}\{ q_{dz} = \tilde{q}_{dN_d} \} \\ 
 \end{bmatrix}}_{1 \times N_d},
\end{align*}
and where $q_{dz} = \tilde{q}_{dk}$ means $q_{dz}(\eta) = \tilde{q}_{dk}(\eta)$ for all $\eta$.
Naturally, each row will be multiplied with $\mathbf{q}$, hence the relationship. And for each $(d, z)$, $\chi_{dz}$ only contains exactly one non zero value (equal to $1$), because any original potential outcome is only mapped to one potential outcome in $\mathbf{\tilde{q}}$. 
Notice that $\chi$ does not depend on $\eta$, as it only represents the mapping between the functions. \\
%
%
%
%
%
\indent Now that we have defined the partial exclusion restrictions, let us follow the analysis as in Section \ref{section_framework}. 
First, let us define the $N_Z \times N_Q$ matrix
\begin{align*}
	\mathcal{F}(\mathbf{q}(\eta)) &= 
	\begin{bmatrix}
		f_{11}(q_{11}(\eta)) & f_{21}(q_{21}(\eta)) & \cdots & f_{J1}(q_{J1}(\eta)) \\ 
		f_{12}((q_{12}(\eta)) & \ddots & & \vdots \\ 
		\vdots & & \ddots & \vdots \\
		f_{1N_Z}(q_{1N_Z}(\eta)) & \cdots & \cdots & f_{JN_Z}(q_{JN_Z}(\eta)) \\
	\end{bmatrix}, \\
	%\\ 
	\text{where } f_{dz}(q_{dz}(\eta)) &= \begin{bmatrix}
	f_{D, Y|z}\big(d, q_{dz}(\eta)\big) & f_{D, Y|z}\big(d, q_{dz}(\eta)\big) & \cdots & f_{D, Y|z}\big(d, q_{dz}(\eta)\big) 
	\end{bmatrix}, 
\end{align*}
i.e., for each $(d, z)$, $f_{dz}(q_{dz}(\eta))$ is a vector of $f_{D, Y|z}\big(d, q_{dz}(\eta)\big)$ repeated $N_d$ times. %\\
Then, the equivalent of the $M$ matrix in this general case is the $N_Z \times N_Q$ matrix
\begin{align*}
	M(\mathbf{\tilde{q}}(\eta)) = \mathbf{\chi}(\mathbf{q}, \mathbf{\tilde{q}}) \odot \mathcal{F}(\mathbf{q}(\eta)),
\end{align*}
where $\odot$ represents the Hadamard product. Because of the mapping $\chi$, $M$ is a function of the $N_Q$ `unique' potential outcomes $\mathbf{\tilde{q}}$, and not of the original potential outcomes $\mathbf{q}$. 
Now, take the derivative of the main system (\ref{main_equation_general}) with respect to $\eta$ to obtain the quasilinear system of $N_Z$ differential equations: 
\begin{align}\label{system_equa_diff_general}
M(\mathbf{\tilde{q}}(\eta)) \ \frac{\partial \mathbf{\tilde{q}}(\eta)}{\partial \eta} = \begin{bmatrix}
		1 &
		\cdots &
		1 
	\end{bmatrix}^T. 
\end{align}

\indent Exactly as in the previous section, the system is identified if and only if there exists a unique (strictly increasing) solution $\mathbf{\tilde{q}}(\eta)$ to (\ref{system_equa_diff_general}). Again this comes down to a relevance condition on the selection probabilities. Let us denote the probability counterpart of $\mathcal{F}$ at the true outcome functions by  
\begin{align*}
	\mathcal{P}(\eta) &= 
	\begin{bmatrix}
		p_{11}(\eta) & p_{21}(\eta) & \cdots & p_{J1}(\eta) \\ 
		p_{12}(\eta) & \ddots & & \vdots \\ 
		\vdots & & \ddots & \vdots \\
		p_{1N_Z}(\eta) & \cdots & \cdots & p_{JN_Z}(\eta) \\
	\end{bmatrix}, \\
	%\\ 
	\text{where }\quad  p_{dz}(\eta) &= \begin{bmatrix}
	p_{D|\Eta, z}(d, \eta) & p_{D|\Eta, z}(d, \eta) & \cdots & p_{D|\Eta, z}(d, \eta)\big) 
	\end{bmatrix} \text{ for each } d, z.
\end{align*}
Then, as for $M$, we have
\begin{align*}
	\tilde{M}(\eta) = \chi \odot \mathcal{P}(\eta). 
\end{align*}
%
%
%
Again, $\tilde{M}$ is related to $M$. Indeed, rewrite the property of the joint density (\ref{property_joint_density}), to obtain for all $d, z$, 
\begin{align}\label{property_joint_density_general}
	f_{D, Y | z}\big( d, y_{dz}\big) &= 	p_{D|\Eta, z} \big(d, \eta_{dz}(y_{dz})\big) \ \frac{\partial \eta_{dz} ( y_{dz} )}{\partial y_{dz}}, \nonumber \\
	&= p_{D|\Eta, z} \big(d, \eta_{dz}(y_{dz})\big) \ \sum^{N_d}_{k=1} \frac{\partial \tilde{\eta}_{dk} ( y_{dk} )}{\partial y_{dk}} \mathds{1}\{ \eta_{dz} = \tilde{\eta}_{dk} \},
%	\textrm{Pr}(D=d|\Eta=h_{dw_d}(y_{dw_d}), W_0=w_0, W_1=w_1) \ \frac{\partial h_{dw_d} ( y_{dw_d} )}{\partial y_{dw_d}}, 
\end{align}
where we define the inverse functions as before, $\eta_{dz}(\cdot): \mathcal{Y}_{dz} \rightarrow [0, 1]$ as $\eta_{dz}(y_{dz}) = q_{dz}^{-1}(y_{dz})$. Similarly for all the unique different outcomes, $\tilde{q}_{dk}$,  we have the inverse $\tilde{\eta}_{dk}(y_{dk}) = \tilde{q}_{dk}^{-1}(y_{dk})$, where $\tilde{\mathcal{Y}}_{dk} = \mathcal{Y}_{dz}$ for the $z$ mapped to $k$.
By monotonicity, these inverse functions exist, are uniquely defined, and strictly increasing. Thus, one can easily check that, for the \textit{true potential outcomes}, $\mathbf{\tilde{q}}(\eta)$, we have $\tilde{\eta}_{dk}(\tilde{q}_{dk}(\eta)) = \eta$, and thus
\begin{align}\label{relation_M}
	M(\mathbf{\tilde{q}}(\eta)) &= \tilde{M}\big(\eta\big) H\big(\mathbf{\tilde{q}}(\eta)\big), 
\end{align}
\begin{align*}
\text{where } H(\mathbf{\tilde{q}}(\eta)) = \text{diag}\Bigg(\ &\frac{\partial\tilde{\eta}_{11}(\tilde{q}_{11}(\eta))}{\partial y_{11}}, \quad ...\quad, \frac{\partial\tilde{\eta}_{1N_1}(\tilde{q}_{0N_0}(\eta))}{\partial y_{0N_0}}, \\
 &\ \frac{\partial\tilde{\eta}_{21}(\tilde{q}_{21}(\eta))}{\partial y_{21}}, \quad ... \quad, \frac{\partial\tilde{\eta}_{2N_2}(\tilde{q}_{2N_2}(\eta))}{\partial y_{2N_2}}, \\
 & \ \frac{\partial\tilde{\eta}_{J1}(\tilde{q}_{J1}(\eta))}{\partial y_{J1}}, \quad ... \quad, \frac{\partial\tilde{\eta}_{JN_{J}}(\tilde{q}_{JN_{J}}(\eta))}{\partial y_{JN_{J}}} \Bigg).
\end{align*}
This matrix multiplication property holds because, all the elements of the first column of $M$ correspond to the unique potential outcome $\tilde{q}_{11}$, the second column to $\tilde{q}_{12}$, and so on and so forth. 
Given the monotonicity assumption, all the elements of $N_Q \times N_q$ matrix $H(\mathbf{q}(\eta))$ are strictly positive, so this matrix is always invertible. %\\
%
%
Hence, from the relation between $M$ and $\tilde{M}$ (\ref{relation_M}), we can derive the relevance assumption and identification theorem as before. \\

\vspace{-1\baselineskip}

\begin{assumption}[Relevance (General case)]\label{relevance_general} The $N_Z \times N_Q$ matrix of selection choice probabilities $\tilde{M}(\eta)$ has rank $N_Q$ for all $\eta \in \mathcal{H} \backslash \mathcal{K}$. $\mathcal{K}$ is a (possibly empty) finite set containing $K$ ($\geq 0$) isolated values $\eta_k$, at which there is rank-one deficiency, i.e., $\text{rank} \big(\tilde{M}(\eta_k)\big) = N_Q - 1$.	
\end{assumption}



\begin{theorem}[Identification (General case)]\label{identification_theorem_general}
Suppose that Assumptions \ref{ass_monot}-\ref{regularity} (adjusted to the general case) and \ref{relevance_general} hold and that the observable joint distributions are drawn from the model (\ref{system_general}). Then there exists a unique set of $N_Q$ strictly increasing potential outcome functions $\mathbf{\tilde{q}}(\eta)$ mapping $[0, 1]$ into $\mathbf{\mathcal{Y}}$ which solve the system of differential equation (\ref{system_equa_diff_general}).  
\end{theorem} 

\begin{proof}
Same as the proof of Theorem \ref{identification_theorem} (with trivial notational adjustments, and using the submatrix $M_q$ instead of $M$).
\end{proof} 

\indent The only small adjustment to the relevance condition is that here, $M$ (and $\tilde{M}$) are of size $N_Z \times N_Q$, so if $N_Z > N_Q$, the problem may be overidentified. Hence, we only require that there exists (at least) one subset of $N_Q$ values of $Z$ for which the model is identified. If there are several combination of $Z$ for which the condition is satisfied, the model is overidentified. \\
\indent The main identification arguments, remain exactly the same. It relies on the fact that, under the relation (\ref{relation_M}), if a $N_Q\times N_Q$ submatrix $\tilde{M}_Q(\eta)$ is invertible, the corresponding submatrix $M_Q(\eta)$ is also invertible, i.e., 
\begin{align*}
	det\big(\tilde{M}_Q\big(\eta\big)\big) \neq 0 \iff det\big(M_Q\big(\mathbf{\tilde{q}}(\eta)\big)\big) \neq 0.
\end{align*} 
When these matrices are invertible, there is a unique solution to the quasilinear system of differential equation (\ref{system_equa_diff_general}) (adjusted by selecting only $N_Q$ rows among the $N_Z$ possibilities). %\\




\subsection{Discussion and examples of partial exclusion restrictions}

\begin{condition}[Exclusion Necessary Condition]\label{necessary_cond}
A necessary condition for Assumption \ref{relevance_general} to be satisfied is that the $N_Z \times N_Q$ mapping $\chi(\mathbf{q}, \mathbf{\tilde{q}})$ has rank $N_Q$. 
\end{condition} 

\noindent Again, identification relies on the invertibility of (a subset of) the matrix $\tilde{M}(\eta)$, which is equal to $\chi \odot \mathcal{P}(\eta)$ here. Therefore, even before checking the relevance assumption \ref{relevance_general}, identification requires the necessary condition \ref{necessary_cond} on the partial exclusion restrictions, represented by the mapping $\chi$: if $\chi$ has rank lower than $N_Q$, then the relevance Assumption \ref{relevance_general} is not satisfied. This directly yields simple conditions to check for the partial exclusion restrictions to be valid for identification. 
First, $N_Q \leq N_Z$, i.e., there are less unique potential outcomes to identify than the number of values in the support of the semi-IV. Note also that $N_Q \geq J$, thus instruments with $N_Z = J$, must satisfy a full exclusion restrictions with respect to the potential outcomes in order to be valid to identify the model. 
Second, there must be at least one partial exclusion restriction per alternative $d$. Indeed, if there is no exclusion restriction in alternative $d'$, then, $N_{d'} = N_Z$ and $N_Q = \sum_{d=1}^J N_d > N_Z$. \\
\indent Overall, there are many combinations of partial exclusion restrictions which are valid for the identification of the model, provided that the relevance condition on the selection probabilities is also satisfied. The advantage is that $\chi$ only depends on the modelling assumptions (the exclusion restrictions), it is completely separated from the selection probabilities in the data, thus the necessary condition \ref{necessary_cond} can be checked a priori. And then identification requires a separate relevance condition which can be tested with the estimation in the data. 
 I provide several examples of valid partial exclusion restrictions and of how the relevance condition adjust to them in what follows. \\




\noindent \textbf{Identification with $J=2$, $N_Z = 2$:} \textit{full exclusion, standard IV.}\\ 
The support of $Z$ is only as large as the number of alternatives, we have no extra identification power, we can only identify $N_Q=2$ unique potential outcome functions by imposing $2$ exclusion restrictions (i.e., complete exclusion restriction):
\begin{align*}
\left\{
  \begin{array}{ll}
	q_{11}(\eta) = q_{12}(\eta) := \tilde{q}_{11}(\eta), \\
	q_{21}(\eta) = q_{22}(\eta) := \tilde{q}_{21}(\eta),
\end{array}
\right. \text{ and } 	\chi(\mathbf{q}, \mathbf{\tilde{q}}) = \begin{bmatrix}
		1 & 1 \\
		1 & 1 \\
	\end{bmatrix}.
\end{align*}
The necessary condition \ref{necessary_cond} is satisfied and we can proceed with the identification. The rest of the analysis follows naturally and we obtain identification under the relevance condition that $\tilde{M}(\eta)$ is invertible (except possibly on a set of isolated points), with 
\begin{align*}
		\tilde{M}(\eta) = \begin{bmatrix}
		p_{D|\Eta, 1}(1, \eta) & p_{D|\Eta, 1}(2, \eta) \\
		p_{D|\Eta, 2}(1, \eta)  &  p_{D|\Eta, 2}(2, \eta)  \\
	\end{bmatrix}.  
\end{align*} 
The matrix $\tilde{M}(\eta)$ is invertible if and only if
\begin{align*}
	&p_{D|\Eta, 1}(1, \eta)p_{D|\Eta, 2}(2, \eta) - p_{D|\Eta, 2}(1, \eta)p_{D|\Eta, 1}(2, \eta) \neq 0.
\end{align*}
Since $p_{D|\Eta, z}(1, \eta) = 1 - p_{D|\Eta, z}(2, \eta)$ when $J=2$, this is simply equivalent to 
\begin{align*}
	p_{D|\Eta, 2}(1, \eta) - p_{D|\Eta, 1}(1, \eta) \neq 0, 
\end{align*}
In other words, the two potential outcomes are identified if and only if the instrumental variable is relevant for all $\eta$ (except possibly on a set of isolated values). 
This is the standard Instrumental Variable scenario with full exclusion restriction. For $J\geq 2$ and $N_Z=J$ the identification follows this line, except that the relevance condition is less trivial to interpret. See \cite{bruneel2022} for example.\\









\noindent \textbf{Identification with $J=2$, $N_Z = 3$:} \textit{conditional semi-IV.}\\
When $J=2$, we can already find valid semi-IVs which are not standard IVs with $N_Z = 3$. In this case, we can identify $N_Q = 3$ unique potential outcome functions by imposing $J \times N_Z - N_Q = 3$ partial exclusion restrictions: 
\begin{align*}
\left\{
  \begin{array}{ll}
	q_{11}(\eta) = q_{12}(\eta) = q_{13}(\eta) := \tilde{q}_{11}(\eta), \\
	q_{21}(\eta) := \tilde{q}_{21}(\eta), \\
	q_{22}(\eta) = q_{23}(\eta) := \tilde{q}_{22}(\eta), \\
\end{array}
\right. \text{ and } 	\chi(\mathbf{q}, \mathbf{\tilde{q}}) = \begin{bmatrix}
		1 & 1 & 0 \\
		1 & 0 & 1 \\
		1 & 0 & 1 \\
	\end{bmatrix}.
\end{align*}
To make it explicit, there two exclusion restrictions in the first line ($q_1$ is independent from $Z$), no exclusion in the second and one in the third line. The necessary condition is satisfied ($\chi$ is invertible), we can proceed with the relevance requirements. We obtain 
\begin{align*}
		\tilde{M}(\eta) = \begin{bmatrix}
		p_{D|\Eta, 1}(1, \eta) & 0  & p_{D|\Eta, 1}(2, \eta) \\
		0 & p_{D|\Eta, 2}(1, \eta)  &  p_{D|\Eta, 2}(2, \eta)  \\
		0 & p_{D|\Eta, 3}(1, \eta) & p_{D|\Eta, 3}(2, \eta) \\
	\end{bmatrix},  
\end{align*} 
which is invertible if and only if
\begin{align*}
	&p_{D|\Eta, 1}(1, \eta) \ \Big( p_{D|\Eta, 2}(1, \eta)p_{D|\Eta, 3}(2, \eta) - p_{D|\Eta, 2}(2, \eta)p_{D|\Eta, 3}(1, \eta) \Big) \neq 0.
\end{align*}
Since $p_{D|\Eta, z}(1, \eta) = 1 - p_{D|\Eta, z}(2, \eta)$ when $J=2$, this is equivalent to 
\begin{align*}
	p_{D|\Eta, 1}(1, \eta) \ \Big( p_{D|\Eta, 3}(2, \eta) - p_{D|\Eta, 2}(2, \eta) \Big) \neq 0, 
\end{align*}
In other words, the $3$ unique potential outcomes are identified if and only if $p_{D|\Eta, 1}(1, \eta) > 0$ and $p_{D|\Eta, 3}(2, \eta) - p_{D|\Eta, 2}(2, \eta) \neq 0$. Conditional on $Z \neq 1$, we are back in the standard IV case in a sense, and identification only requires that the instrument is relevant, i.e., that $Z=2$ gives different selection probabilities than $Z=3$ (except possibly on a set of isolated values of $\eta$). The semi-IV $Z$ can be thought of as a conditional semi-IV in this case. \\
\indent \textit{Empirical example:} in terms of practical application, to proceed with the education example, $Z$ could be the local price of college with $Z=1=$ low price, $Z=2=$ medium price, $Z=3=$ high price. If we assume that after a given threshold of prices, the schools are of the same quality, then the impact of college education on earnings is the same for the higher prices $Z=2$ and $Z=3$. Obviously, the price of college is completely excluded from the earnings of individuals who did not go to college. Finally, the relevance condition is also most likely satisfied, because even though they do not impact the outcomes, everything else equal individuals will be less likely to go to college if the price is higher. \\




\noindent \textbf{Revisiting the binary model ($J=2, N_Z = 4$):} \textit{selection-specific semi-IV.}\\
Let us revisit the model of Section \ref{section_framework} using a single semi-IV $Z$. In this section, $W_0, W_1$ were both binary, so the equivalent $Z$ would have a support of size $N_Z=4$, with
\begin{align*}
	Z = \left\{
    \begin{array}{ll}
        1 & \mbox{if } W_0 = 0, W_1 = 0, \\
        2 & \mbox{if } W_0 = 1, W_1 = 0, \\
        3 & \mbox{if } W_0 = 0, W_1 = 1, \\
        4 & \mbox{if } W_0 = 1, W_1 = 1. \\
   \end{array} 
\right.
\end{align*}
Then, we implicitely imposed the four following partial exclusion restrictions: 
\begin{align*}%\label{exclusion_baseline}
\left\{
  \begin{array}{ll}
	q_{11}(\eta) &= q_{13}(\eta) := \tilde{q}_{11}(\eta), \\
	q_{12}(\eta) &= q_{14}(\eta) := \tilde{q}_{12}(\eta), \\
	q_{21}(\eta) &= q_{22}(\eta) := \tilde{q}_{21}(\eta), \\
	q_{23}(\eta) &= q_{24}(\eta) := \tilde{q}_{22}(\eta), 
\end{array}
\right. \text{ and } 	\chi(\mathbf{q}, \mathbf{\tilde{q}}) = \begin{bmatrix}
		1 & 0 & 1 & 0 \\
		0 & 1 & 1 & 0 \\
		1 & 0 & 0 & 1 \\
		0 & 1 & 0 & 1
	\end{bmatrix},
\end{align*}
to identify $N_Q=4$ different functions. Condition \ref{necessary_cond} holds. %and we can proceed forward with the identification. \\
The rest of the analysis follows similarly and we obtain identification under the relevance condition that $\tilde{M}(\eta)$ is invertible (except possibly on a set of isolated points), with 
\begin{align*}
		\tilde{M}(\eta) = \begin{bmatrix}
		p_{D|\Eta, 1}(1, \eta) & 0  & p_{D|\Eta, 1}(2, \eta) & 0 \\
		0 & p_{D|\Eta, 2}(1, \eta) & p_{D|\Eta, 2}(2, \eta) & 0\\
		p_{D|\Eta, 3}(1, \eta) & 0  & 0 & p_{D|\Eta, 3}(2, \eta)  \\
		0 & p_{D|\Eta, 4}(1, \eta)  & 0 & p_{D|\Eta, 4}(2, \eta)  \\
	\end{bmatrix}.  
\end{align*} 
Relevance is satisfied under the same condition on the odds ratios as before, except that we replaced $(w_0, w_1)$ by the corresponding $z$ in equation (\ref{irrelevant_odds}). \\ 








\noindent \textbf{Discrete model ($J>2, N_Z > J$):} \textit{selection-specific semi-IV (discrete).}\\
The binary framework of Section \ref{section_framework} easily extends to a discrete one. 
Consider the nonparametric, nonseparable, triangular system of simultaneous equations
\begin{align}\label{system_discrete_example}
\left\{
    \begin{array}{l}
        Y_d = q_d(X, W_d, \Eta), \\
        D = b(X, W_0, W_1, ..., W_{J-1}, \Eta, \Epsilon), 
    \end{array}
\right.
\end{align}
where $W_d$ are $J$ alternative specific binary semi-IVs which only affect their corresponding potential outcomes. 
Then, we can define a single variable $Z$, whose support corresponds to all the combinations of the $J$ binary $W_d$, yielding $N_Z = 2^J$. On the other hand, in this model we have only two potential outcomes per alternatives: $\tilde{q}_{d1}$ and $\tilde{q}_{d2}$ for both values of the alternative specific $W_d$. Meaning that there is only a total of $J\times 2$ unknown to identify, with a semi-IV whose support is of a much larger size $2^J$. Thus, we could remove some of the (implicit) partial exclusion restrictions imposed by the model here and still achieve identification of a larger number of potential outcomes. For example, we could allow some of the $Y_d$ to depend on $W_d$ but also on (several) other $W_k$. \\
\indent The identification procedure follows as described in the general case, and we may have overidentification because the support of the semi-IVs is larger than $N_Q$. \\
%
%
\indent \textit{Empirical Example:}  
A natural example for this discrete setup is the extension of the binary one. We could study the returns to education but for different major choices, school choices, or more generally occupation choices ($D$). Then the $W_d$ would naturally be occupation specific average incomes. Given that I choose to study engineering, the wage of engineers (when I make my schooling decision) is not excluded from my outcomes (because the wages are correlated in time), but the wage of every other occupations is. %\\
Now, since we have some additional identification power, we can relax some of the partial exclusion restrictions for occupations which are closely interconnected, and whose average earnings may not be excluded from each other. For example, the average income of economists in the private sector maybe not be excluded from the average income of economists in academia, as they can be used as an outside option to negotiate.  %\\











\section{Average Treatment Effect in an additive model}\label{section_ate}

Obviously in the general setup, the individual treatment effect are identified, so the average treatment effect (ATE) also is. Here I would like to show that in a much simpler model with additive $\Eta$ and homogenous treatment, the ATE has a simple closed-form formula. This provides most of the intuition needed to understand the identification with semi-IVs, and can also be used in practice to build easy-to-implement estimators of treatment effects with semi-IVs. 
Again, I abstract from covariates $X$ (as if everything was done conditional on $X=x$), but they can be trivially included. \\
\indent Focus again on the example of the first section with two binary semi-IVs, $W_0$ and $W_1$. 
Imagine the potential outcomes have the following linear form:
\begin{align*}
	&Y = \beta + \delta D + \alpha_0 (1-D) W_0 + \alpha_1 D W_1 + \Eta \\
	\quad \text{ with } \quad &\mathbb{E}[\Eta | W_0=w_0, W_1=w_1] = 0 \text{ for all } (w_0, w_1) \in \{0, 1\}^2. 
\end{align*}
We change the normalization and now assume that $\Eta$ has mean zero (instead of $\Eta \sim \mathcal{U}(0,1)$). With the additivity of $\Eta$, we still have monotonicity of $Y_d$ with respect to $\Eta$ and we also have rank invariance: ceteris paribus, the higher $\Eta$, the higher $Y$. 
The average treatment effect is homogenous here, and given by $\delta$.\footnote{We can also obtain a relatively simple closed-form formula in a model where the treatment effect is not homogenous. For example if we assume $Y = \beta + \delta D + \alpha_0 (1-D) W_0 + \alpha_1 D W_1 + \theta_d \Eta$. But this is probably less common, so I focus on the simple homogenous treatment effect case here.} \\
%
\indent Since $\mathbb{E}[\Eta | W_0=w_0, W_1=w_1] = 0$, for all $(w_0, w_1) \in \{0,1\}^2$, we have:
\begin{align}\label{eq_ATE}
	\mathbb{E}[Y | W_0=w_0, W_1=w_1] = &\quad \beta + \delta \textrm{Pr}(D=1|W_0=w_0, W_1=w_1) \nonumber \\
	&+ \alpha_0 w_0 \textrm{Pr}(D=0|W_0=w_0, W_1=w_1) \nonumber \\
	&+ \alpha_1 w_1 \textrm{Pr}(D=1|W_0=w_0, W_1=w_1).
\end{align}
Denote $p_{D|w_0, w_1}(1) = \textrm{Pr}(D=1|W_0=w_0, W_1=w_1)$. Since $p_{D|w_0, w_1}(1) = 1-p_{D|w_0, w_1}(0)$, we can rewrite (\ref{eq_ATE}) as:
\begin{align}\label{system_ATE}
\begin{bmatrix}
	\mathbb{E}[Y | W_0=0, W_1=0] \\
	\mathbb{E}[Y | W_0=1, W_1=1] \\
	\mathbb{E}[Y | W_0=0, W_1=1] \\
	\mathbb{E}[Y | W_0=1, W_1=1]
\end{bmatrix} = 
	\underbrace{\begin{bmatrix}
		1 & p_{D|0, 0}(1) & 0 & 0 \\
		1 & p_{D|1, 0}(1) & 1- p_{D|1, 0}(1)& 0\\
		1 & p_{D|0, 1}(1) & 0 & p_{D|0, 1}(1)\\
		1 & p_{D|1, 1}(1) & 1- p_{D|1, 1}(1)& p_{D|1, 1}(1)\\
	\end{bmatrix}}_{:= A}
	\begin{bmatrix}
		\beta \\
		\delta \\
		\alpha_0 \\
		\alpha_1 
	\end{bmatrix}.
\end{align}
Thus, if $A$ is invertible, we have a closed-form formula for the coefficients given by
\begin{align}\label{solution_ATE}
	\begin{bmatrix}
		\beta \\
		\delta \\
		\alpha_0 \\
		\alpha_1 
	\end{bmatrix} = A^{-1} \begin{bmatrix}
	\mathbb{E}[Y | W_0=0, W_1=0] \\
	\mathbb{E}[Y | W_0=1, W_1=1] \\
	\mathbb{E}[Y | W_0=0, W_1=1] \\
	\mathbb{E}[Y | W_0=1, W_1=1]
\end{bmatrix}. 
\end{align}
Now, 
\begin{align*}
	\text{det}(A) = &\quad p_{D|0, 0}(1) p_{D|1, 1}(1) (1 - p_{D|1, 0}(1)) (1 - p_{D|0, 1}(1)) \\
	& - p_{D|1, 0}(1) p_{D|0, 1}(1) (1-p_{D|0, 0}(1)) (1-p_{D|1, 1}(1)),
\end{align*}
and det($A$)$=0$ if and only if
\begin{align*}
	\frac{p_{D|0, 0}(1)/(1-p_{D|0, 0}(1))}{p_{D|0, 1}(1)/(1-p_{D|0, 1}(1))} = \frac{p_{D|1, 0}(1)/(1-p_{D|1, 0}(1))}{p_{D|1, 1}(1)/(1-p_{D|1, 1}(1))}. 
\end{align*}
This is the same odds ratio condition as the relevance condition (\ref{irrelevant_odds}) in Section \ref{section_framework}, except that this time it is a condition on the overall probabilities (not conditional on $\Eta$). The intuition behind the identification of the ATE is the same as the intuition behind identification of the general nonseparable model. \\
\indent Now, in particular, one can show that the closed-form formula for the homogenous ATE is weighted sum (weighted by the selection probabilities) of the difference between the expected outcomes conditional on the semi-IVs, 
\begin{align*}
	\delta = \quad &(1-p_{D|1, 0}(1)) p_{D|0, 1}(1) \Big[ \mathbb{E}[Y | W_0=1, W_1=1] - \mathbb{E}[Y | W_0=0, W_1=0] \Big] \\
	+ &(1-p_{D|1, 1}(1)) p_{D|0, 1}(1) \Big[ \mathbb{E}[Y | W_0=0, W_1=0] - \mathbb{E}[Y | W_0=1, W_1=0] \Big] \\
	+ &(1-p_{D|1, 0}(1)) p_{D|1, 1}(1) \Big[ \mathbb{E}[Y | W_0=0, W_1=0] - \mathbb{E}[Y | W_0=0, W_1=1] \Big].
\end{align*}




%\pagebreak


\section{Conclusion}\label{section_conclusion}

This paper proposes a new tool to address endogeneity problems. I generalize the concept of IVs and show that nonparametric identification of models with discrete endogeneity can still be obtained under weaker partial exclusion restrictions, with semi-IVs. Given the difficulty to find fully excluded IVs in practice, being able to search for semi-IVs instead should have a large empirical impact: it significantly extends the toolkit of applied researchers.  \\
\indent There are several directions for further work. First, theoretically, the intuition behind the identification with semi-IVs is general and not necessarily restricted to models with discrete selection, continuous potential outcomes and discrete semi-IVs. For example, this work can be trivially adjusted in the case of continuous semi-IVs instead of binary semi-IVs. 
If we relax the rank invariance, we can probably still identify general models. The loss is that, without rank invariance, one cannot recover the joint distribution of (unconditional) potential outcomes, and thus, cannot identify individual treatment effects. But, in general, average treatment effects should still be identifiable. 
In the same spirit, one could also identify average treatment effect on discrete (or binary) potential outcomes. %\\
Identification of general models where the endogenous variable is continuous is probably more difficult, because then it is unclear how the relevance condition would adjust. But under specific assumptions, identification can probably still be obtained. \\
\indent Then, comes the question of the best way to implement semi-IV estimation in practice. Section \ref{section_ate} provides a natural first step in the implementation of the estimation of ATE with semi-IVs. But one may also be interested in more non-parametric approaches to estimate the general model. \\
\indent Finally, the most interesting avenue is the use of semi-IVs in empirical work to address endogeneity problems in labor, education, industrial organization, macroeconomics... In particular, one maybe able to find semi-IVs to answer questions for which no IV was available. Semi-IVs can also help identifying and estimating complex structural dynamic models \citep{bruneel2022}. 





\pagebreak



%\newpage
\bibliographystyle{ecta}
{\begin{spacing}{0}
\setlength{\baselineskip}{0pt} 
\footnotesize
%\setlength{\baselineskip}{0pt} 
\bibliography{references}
\end{spacing}
}



\pagebreak



\pagenumbering{arabic}% resets `page` counter to 1
%\renewcommand*{\thepage}{A\arabic{page}}
\appendix




\noindent {\LARGE \textbf{Supplementary materials: online appendices}} %\\ 
\section{Proof of Theorem \ref{identification_theorem} }\label{appendix_identification_proof}

\subsection{Main Proof}
To prove identification Theorem \ref{identification_theorem}, we need to show that there exists a unique strictly increasing solution $\mathbf{q}(\eta)$, starting from known $\mathbf{q}(0)$, to the quasilinear system of differential equations
\begin{align}\label{system_equa_diff_appendix}
M(\mathbf{q}(\eta)) \ \mathbf{q}'(\eta) = \underbrace{ \begin{bmatrix}
		1 \\
		\vdots \\
		1 \\
	\end{bmatrix}}_{:= V},
\end{align}
where $M$ is a square matrix of size $N_M \times N_M$. Denote $V$ the $N_M \times 1$ vector of 1. 

\indent Existence of the solution is trivial: the reduced form joint densities are drawn from the model, so, by construction, the true $\mathbf{q}(\cdot)$ solves the system (\ref{system_equa_diff_appendix}). \\
\indent The challenge is to prove uniqueness. If the relevance Assumption \ref{relevance} holds with $K=0$, then $M(\mathbf{q})$ is always invertible on the true solution path. So, starting from initial values which belongs to the true path (the known $\mathbf{q}(0)$), we never deviate from it, and by the Picard-Lindelöf Theorem the solution is uniquely defined by
\begin{align}\label{eq_appendix_invertible}
	\mathbf{q}'(\eta) = \Big(M(\mathbf{q}(\eta)) \Big)^{-1} V.
\end{align}
%
%
%
\indent Now, if $K > 0$, $M(\mathbf{q}(\eta))$ is not invertible on a set of $K$ isolated singularities. Obviously, between the isolated singularities, $M(\mathbf{q})$ is invertible and we can proceed as previously using (\ref{eq_appendix_invertible}) to solve the differential equation. But at a singularity point, denoted $\mathbf{\tilde{q}}$,  $M(\mathbf{\tilde{q}})$ is not invertible, we cannot use (\ref{eq_appendix_invertible}). We show that there exist only one strictly increasing solution crossing the isolated singularities, and thus the solution is still identified despite the singularities.\footnote{For the proof, one could mostly directly refer to \cite{marszalek2005}, who build upon the unstable manifold theorem and show that there can only be at most two trajectories smoothly crossing a geometric singularity and that they do so in opposite direction. From there, there is not much additional work required to show that there is a unique strictly increasing solution. But instead I am going to detail the entire proof.} \\
%
%
%
\indent Let us rewrite (\ref{system_equa_diff_appendix}) as a quasilinear autonomous system of differential equations, 
\begin{align}\label{autonomous_system}
M(\mathbf{q}) \ \mathbf{q}' = V. 
\end{align}
Using the matrix property that $M \times adj(M) = det(M) \times I$, define the \textit{canonical system}: 
\begin{align}\label{appendix_canonical}
	\underbrace{\textrm{det}\big(M(\mathbf{q})\big)}_{1\times1} \underbrace{\mathbf{q}'}_{N_M \times 1} &= \underbrace{\textrm{adj} \big(M(\mathbf{q})\big)}_{N_M \times N_M} \ \underbrace{V}_{N_M\times1}, \nonumber \\
	\iff   \omega(\mathbf{q}) \mathbf{q}' &= g(\mathbf{q}),
\end{align} 
where $\omega(\mathbf{q}) := $ det$(M(\mathbf{q}))$ and $g(\mathbf{q}) := \textrm{adj} \big(M(\mathbf{q})\big) V$. 
A solution to the canonical system (\ref{appendix_canonical}) also solves the original system (\ref{autonomous_system}). When $\omega(\mathbf{q}) \neq 0$, $M$ is invertible and (\ref{eq_appendix_invertible}) can be written as
\begin{align}\label{eq_appendix_invertible2}
	\mathbf{q}'(\eta) = g(\mathbf{q})/\omega(\mathbf{q}).
\end{align}  
%
%
%

\indent Let us focus the $\eta$-transformed system of nonlinear differential equations (or desingularized field) 
\begin{align}\label{desingularized}
	\mathbf{x}'(t) = g(\mathbf{x}(t)), 
\end{align}
where $x$ corresponds to a change of variable that comes from the time reparametrization from $\eta$ to $t$ (call $\eta$ and $t$, "time" variable to follow the standard differential equation terminology). The desingularized system (\ref{desingularized}) depicts exactly the same trajectory behavior as the original system 
\begin{align}\label{canonical_v2}
	\omega(\mathbf{q}(\eta)) \mathbf{q}'(\eta) = g(\mathbf{q}(\eta)).
\end{align} 
Indeed, $\omega(\mathbf{q}) = \textrm{det}\big(M(\mathbf{q})\big)$ is only a scalar: it does not influence the mapping between the variables in $\mathbf{q}$, only the speed of convergence of the derivatives with respect to the "time" variable ($\eta$ or $t$ here). Most importantly, the sign of the determinant will determine the orientation of the trajectory. If $\omega(\mathbf{q}) < 0$, the orientation is reversed: for a solution to (\ref{desingularized}) that was going forward in $t$, the corresponding same trajectory solution to (\ref{appendix_canonical}) will go backwards (and at a rescaled speed) in $\eta$, and vice versa. 
However, in terms of the mapping between the $N_M$ different subvariables $q_i$ in  $\mathbf{q}$ and $x_i$ in $\mathbf{x}$, i.e., in terms of trajectories, the solution to the desingularized system (\ref{desingularized}) will be the same as the solution to the (\ref{canonical_v2}). 
%\citep[see for example][Section 4.4., p. 164 for more details]{riaza2008}. \\ % also cf page 23 of my notes Differential Equation - Papers, Proofs and others. 
%
%
\noindent This mapping between the trajectories of quasilinear systems and their counterpart desingularized system is well established, but let me be more specific.\footnote{See for example \cite{marszalek2005} or \cite{riaza2008} (Section 4.4., p. 164).} %\\ % also cf page 23 of my notes Differential Equation - Papers, Proofs and others.} %, following \cite{riaza2008} (Section 4.4., p. 164 for more details) or \cite{marszalek2005} for example.
A time reparametrization from $\eta$ to $t$ converts the trajectories of (\ref{desingularized}) into those of (\ref{canonical_v2}). Let $\mathbf{x}(t)$ be a solution of (\ref{desingularized}), where $\mathbf{x}(t)$ are all regular points for $t \in (t_0, t_1)$ with $t_0 < 0 < t_1$ (i.e., around a regular $\mathbf{x}^*=\mathbf{x}(0)$ for example). Define the change of variable (or time-reparametrization)
\begin{align}\label{variable_change}
	\eta = \gamma(t) = \int^t_0 \omega(\mathbf{x}(s))ds. 
\end{align}
Since we assume that $x(t)$ are all regular points for $t \in (t_0, t_1)$, it implies that $\omega(\mathbf{x}(t)) = \text{det}\big(M(\mathbf{x}(t))\big)$ does not change sign in $(t_0, t_1)$, and $\gamma$ is a diffeomorphism of $(t_0, t_1)$ onto some interval $(\eta_0, \eta_1)$. If $\omega(\mathbf{x}(t)) > 0$ on this interval, $\gamma(t)$ is increasing and the variable change (time-transformation) preserves orientation. If $\omega(\mathbf{x}(t)) < 0$ onto this interval, then $\gamma(t)$ is decreasing and the orientation is now reversed. In any case, 
\begin{align*}
	\mathbf{q}(\eta) = \mathbf{x}(\gamma^{-1}(\eta)),
\end{align*}
solves (\ref{canonical_v2}) since 
\begin{align*}
	\mathbf{q}'(\eta) = \frac{d\mathbf{x}\big(\gamma^{-1}(\eta)\big)}{dt} \frac{d\gamma^{-1}(\eta)}{d\eta} = \frac{g\big(\mathbf{x}(\gamma^{-1}(\eta))\big)}{\omega\big(\mathbf{x}(\gamma^{-1}(\eta))\big)} = \frac{g(\mathbf{q}(\eta))}{\omega(\mathbf{q}(\eta))}.
\end{align*}
Now this holds for regular points. But if the singularities are isolated points, the trajectories can also be continuously extended through these singularities \citep{marszalek2005}. 
As a consequence, we can study the trajectories of (\ref{desingularized}) to find the ones of (\ref{canonical_v2}), which is much more convenient. \\
%
%
%
%
%
%
%
\indent Before studying the solutions to (\ref{desingularized}) through the singularities, let me provide some properties of the singularities that we have. Denote $\mathbf{\tilde{q}}$ any isolated singularity which belongs to the true potential outcome functions $\mathbf{q}(\eta)$  and at which rank$\big(M(\mathbf{\tilde{q}})\big)=N_M - 1$ (only one rank deficiency). In this case, the singular point $\mathbf{\tilde{q}}$ is noncritical, i.e.,
\begin{align*}
	\textrm{det}(M(\mathbf{\tilde{q}})) = 0 &\text{ and } \textrm{det}(M(\mathbf{\tilde{q}}))' \neq 0.\footnotemark
\end{align*} % det' != 0 is implied by the rank being N-1. See for e.g. Rabier 1989 proof
\footnotetext{See \cite{rabier1989} for the relation between the rank deficiency of one and the noncriticality of the singularity.} %\\
%\vspace{-2\baselineskip}
%
\noindent Moreover, $\mathbf{\tilde{q}}$ belongs to the true solution path, i.e., there exists an $\eta^k \in [0, 1]$ such that $\mathbf{\tilde{q}} = \mathbf{q}(\eta^k)$. So, at $\mathbf{\tilde{q}}$, (\ref{autonomous_system}) is still satisfied, $M(\mathbf{\tilde{q}}) \ \mathbf{\tilde{q}}' = V.$ Thus, $V$ belongs to the image of $M(\mathbf{\tilde{q}})$, Im($M(\mathbf{\tilde{q}})$), because there exists the true $\mathbf{\tilde{q}}'=\mathbf{q}(\eta^k)'$ such that $V \in \text{Im}(M(\mathbf{\tilde{q}}))$. This means that $\mathbf{\tilde{q}}$ is a particular type of singularity, named I-singularity \citep[for Image-Singularity,][]{sotomayor2001} or geometric singularity \citep{marszalek2005}. Since Ker$\big($adj($M(\mathbf{\tilde{q}}))\big)$ = Range$\big(M(\mathbf{\tilde{q}})\big)$, $V \in $ Ker$\big($adj($M(\mathbf{\tilde{q}}))\big)$. It implies that at any of our specific singularities,
\begin{align*}
	g(\mathbf{\tilde{q}}) = \textrm{adj} \big(M(\mathbf{\tilde{q}})\big) \ V = 0.\footnotemark
\end{align*} % cf page 18 of my notes Differential Equation - Papers, Proofs and others. - intuition comes from Sotomayor and Zhitormiskii possibly
%
%
%
%
%
\footnotetext{Remark: the fact that $g(\mathbf{\tilde{q}}) = 0$ can also be proven analytically using the special property that underlying $M$ there is $\tilde{M}$ which is a matrix of probabilities in which all the rows sum to one, while $V$ is a vector composed of $1$. After some computation, using for example the Cramer's rule you find that $g(\mathbf{\tilde{q}}) = 0$ if $\tilde{q}$ is a singularity on the optimal path. } 
\indent Let us study the solution of the desingularized system (\ref{desingularized}) through any of our $K$ isolated singularity $\mathbf{\tilde{q}}$. We have $g(\mathbf{\tilde{q}}) = 0$, so $\mathbf{\tilde{q}}$ is not a singularity but an \textit{equilibrium point} (or fixed point) for the desingularized system (\ref{desingularized}). We need to study the stability of this equilibrium point of a nonlinear system of differential equations. To do this, we study how small vector of $N_M$ deviations, $\epsilon$, from the equilibrium point evolve in time: do they converge back to the equilibrium (in which case the equilbrium is attracting) or do they "escape" from it (repelling) in forward time? Let us take a first order Taylor approximation around the equilibrium point 
\begin{align*}
	g(\mathbf{\tilde{q}} + \epsilon) = g(\mathbf{\tilde{q}}) + g'(\mathbf{\tilde{q}}) \ \epsilon,
\end{align*}
where $g'(\mathbf{\tilde{q}})$ is the $N_M \times N_M$ Jacobian of $g$ at $\mathbf{\tilde{q}}$. Combine this with the desingularized system (\ref{desingularized}), to obtain the system of the evolution of the deviation around $\mathbf{\tilde{q}}$:
\begin{align}\label{system_deviation}
	\epsilon'(t) = g'(\mathbf{\tilde{q}}) \ \epsilon(t). 
\end{align}
Now, we can show that the $N_M \times N_M$ Jacobian $g'(\mathbf{\tilde{q}})$ has rank $2$ or less. So it has at most two nonvanishing real eigenvalues. We can also show that these eigenvalues are of opposite signs (see Appendix \ref{appendix_subproofs} for the proofs). This implies that there is (at most) two solutions crossing any equilibrium point $\mathbf{\tilde{q}}$ of the desingularized system (\ref{desingularized}): one stable trajectory, denoted $\mathbf{x}_s(t)$ and one unstable, denoted $\mathbf{x}_u(t)$. The stable one will converge to the equilibrium from both sides as time goes forward, while the unstable one will diverge from it as time goes forward. \\
\indent Now recall that the solution to the original system (\ref{canonical_v2}) corresponds to the solution to the desingularized system, but with a time-reparametrization that depends on the value of the determinant, $\omega$. 
Imagine only one of the two trajectory of the solutions mentioned above is strictly increasing with respect to all $q$. Then this trajectory corresponds to our existing unique strictly increasing solution, the model is identified. When changing the time parametrization from $t$ to $\eta$, the determinant converts the orientation such that the solution in terms of $\mathbf{q}(\eta)$ is strictly increasing (while the corresponding $\mathbf{x}(t)$ could not have been by definition). \\
\indent Now, even if both trajectories go in the strictly increasing direction, there is only one of these two solutions that will be rescaled as being strictly increasing with respect to $\eta$. To understand this, note that locally around $\mathbf{\tilde{q}}$, the singular set containing $\mathbf{\tilde{q}}$ is of dimension $1$, Ker$\big(M(\mathbf{\tilde{q}})\big) = 1$. Since $M(\mathbf{\tilde{q}})$ only has positive entries, $M(\mathbf{\tilde{q}}) v_{\mathbf{\tilde{q}}} = 0$ requires that $v_{\mathbf{\tilde{q}}}$ does not contain only positive entries. For any $\mathbf{\tilde{q}}$, the eigenvector of the corresponding singular set, $v_{\mathbf{\tilde{q}}}$, is not strictly positive. Thus, our true solution, $\mathbf{q}(\eta)$, which is strictly positive, will be linearly independent from $v_{\mathbf{\tilde{q}}}$, and will cross the singular set transversally at the singularity $\mathbf{\tilde{q}}$. 
If there are two trajectories yielding potentially strictly increasing solutions, both will cross the singular set transversally at the singularity. But then, "above" the singularity, one of these solutions is converging to the fixed point in the desingularized system, while the other is diverging from it. And both of these solutions will be (locally) rescaled by the same value of the determinant $\omega$ because they are on the same side of the singular set defined by $v_{\mathbf{\tilde{q}}}$: their orientations will be either both preserved, or both reversed. So it cannot be that both are re-oriented such that the corresponding $\mathbf{q}_s(\eta)$ and $\mathbf{q}_u(\eta)$ are both strictly increasing. There can only be a unique strictly increasing solution among those two, and this solution identifies the unique strictly increasing potential outcomes of the model. $\hfill\blacksquare$


















%\pagebreak 
\subsection{Additional proofs}\label{appendix_subproofs}
\noindent \textit{Proof that rank $g'(\mathbf{\tilde{q}}) \leq 2$}: \\
Recall that $M \times adj(M) = det(M)\times I$. Thus, introduce $H(q) := M(q) g(q) = det(M(q)) V$. This holds for every $q$.  
The Jacobian of $H$ taken at $\mathbf{\tilde{q}}$ gives
\begin{align*}
H'(\mathbf{\tilde{q}}) =
	\begin{bmatrix}
		\partial \textrm{det}\big(M(\mathbf{\tilde{q}})\big)/\partial q_1 \times 1 & \cdots & \partial \textrm{det}\big(M(\mathbf{\tilde{q}})\big)/\partial q_N \times 1 \\
		\vdots & \ddots & \vdots \\
		\partial \textrm{det}\big(M(\mathbf{\tilde{q}})\big)/\partial q_1  \times 1& \cdots & \partial \textrm{det}\big(M(\mathbf{\tilde{q}})\big)/\partial q_N \times 1 \\
	\end{bmatrix},
\end{align*}
where I simply renamed all the choices into $q_i$ for $i \in \{1,..., N_M\}$. The multiplication by $1$ comes from the different values of $V$ (all equal to $1$). So, rank$\big(H'(\mathbf{\tilde{q}})\big) = 1$. \\
Now, denote $g'(q)$ the Jacobian of $g$ at $q$. Since $H(q) = M(q) \times g(q)$, and since $g(\mathbf{\tilde{q}})=0$, we have:
\begin{align*}
	H'(\mathbf{\tilde{q}}) = M(\mathbf{\tilde{q}}) \times g'(\mathbf{\tilde{q}}). 
\end{align*}
Using Sylvester's inequality we have 
\begin{align*}
	\textrm{rank}\big(H'(\mathbf{\tilde{q}}) \big) = \textrm{rank}\big(M(\mathbf{\tilde{q}}) \times g'(\mathbf{\tilde{q}})\big) \geq \textrm{rank}\big(M(\mathbf{\tilde{q}})\big) + \textrm{rank}\big(g'(\mathbf{\tilde{q}})\big) - N_M, 
\end{align*}
and since $\textrm{rank}\big(M(\mathbf{\tilde{q}})\big) = N_M - 1$, we obtain 
\begin{align*}
	\textrm{rank}\big(g'(\mathbf{\tilde{q}})\big) \leq 2. \qedblack \\
\end{align*}





\noindent \textit{Proof that the eigenvalues of $g'(\mathbf{\tilde{q}})$ are of opposite sign}: \\
Rename the outcomes $q_1, q_2, ..., q_N$ for simplicity. In $M(\mathbf{q})$, each $q_j$ only appears in the $j^{th}$ column by construction. So, in adj$\big(M(\mathbf{q})\big)$, the $i^{th}$ row will not contain any $q_i$. Then multiply the adjugate matrix by $V$, and obtain that the $i^{th}$ element of $g(\mathbf{q})$ is independent from $q_i$. As a consequence, the diagonal of the Jacobian $g'(\mathbf{q})$ is filled with zeros. For any matrix, the sum of its eigenvalues must be equal to its trace. Here we have (at most) two nonvanishing real eigenvalues and the trace is zero. So the two eigenvalues must be of opposite sign. $\hfill\blacksquare$








\end{document}
