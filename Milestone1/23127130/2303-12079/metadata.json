{
    "arxiv_id": "2303.12079",
    "paper_title": "OmniTracker: Unifying Object Tracking by Tracking-with-Detection",
    "authors": [
        "Junke Wang",
        "Dongdong Chen",
        "Zuxuan Wu",
        "Chong Luo",
        "Xiyang Dai",
        "Lu Yuan",
        "Yu-Gang Jiang"
    ],
    "submission_date": "2023-03-21",
    "revised_dates": [
        "2023-03-22"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV"
    ],
    "abstract": "Object tracking (OT) aims to estimate the positions of target objects in a video sequence. Depending on whether the initial states of target objects are specified by provided annotations in the first frame or the categories, OT could be classified as instance tracking (e.g., SOT and VOS) and category tracking (e.g., MOT, MOTS, and VIS) tasks. Combing the advantages of the best practices developed in both communities, we propose a novel tracking-with-detection paradigm, where tracking supplements appearance priors for detection and detection provides tracking with candidate bounding boxes for association. Equipped with such a design, a unified tracking model, OmniTracker, is further presented to resolve all the tracking tasks with a fully shared network architecture, model weights, and inference pipeline. Extensive experiments on 7 tracking datasets, including LaSOT, TrackingNet, DAVIS16-17, MOT17, MOTS20, and YTVIS19, demonstrate that OmniTracker achieves on-par or even better results than both task-specific and unified tracking models.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.12079v1"
    ],
    "publication_venue": null
}