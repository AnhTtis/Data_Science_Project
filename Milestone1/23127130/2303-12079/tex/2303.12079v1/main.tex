\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
% Include other packages here, before hyperref.
\usepackage{booktabs}
\usepackage{bbding}
\usepackage{enumitem}
\usepackage[ruled]{algorithm2e}
\usepackage[dvipsnames]{xcolor}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{diagbox}
\usepackage{bm}
\usepackage{newfloat}
\usepackage{xspace}

\usepackage{caption} 
\captionsetup[table]{skip=2pt}

\definecolor{citecolor}{RGB}{0, 113, 188}
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=citecolor]{hyperref}
\newcommand{\system}{OmniTracker\xspace}
\iccvfinalcopy % *** Uncomment this line for the final submission
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\begin{document}

%%%%%%%%% TITLE
\title{\system: Unifying Object Tracking by Tracking-with-Detection}

\author{Junke Wang$^{1,2}$,~Dongdong Chen$^{3}$,~Zuxuan Wu$^{1,2}$,~Chong Luo$^{4}$,~Xiyang Dai$^{3}$, ~Lu Yuan$^{3}$, ~Yu-Gang Jiang$^{1,2}$
\\
$^{1}$Shanghai Key Lab of Intell. Info. Processing, School of CS, Fudan University \\
$^{2}$Shanghai Collaborative Innovation Center on Intelligent Visual Computing \\
$^{3}$Microsoft Cloud + AI, $^{4}$Microsoft Research Asia 
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
Object tracking (OT) aims to estimate the positions of target objects in a video sequence. Depending on whether the initial states of target objects are specified by provided annotations in the first frame or the categories, OT could be classified as instance tracking (\eg, SOT and VOS) and category tracking (\eg, MOT, MOTS, and VIS) tasks. Combing the advantages of the best practices developed in both communities, we propose a novel tracking-with-detection paradigm, where tracking supplements appearance priors for detection and detection provides tracking with candidate bounding boxes for association. Equipped with such a design, a unified tracking model, \system, is further presented to resolve all the tracking tasks with a fully shared network architecture, model weights, and inference pipeline. Extensive experiments on 7 tracking datasets, including LaSOT, TrackingNet, DAVIS16-17, MOT17, MOTS20, and YTVIS19, demonstrate that \system  achieves on-par or even better results than both task-specific and unified tracking models. 
\end{abstract}


%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
As a fundamental task in computer vision, object tracking (OT)~\cite{bertinetto2016fully,li2019siamrpn++,dicle2013way,bae2014robust,bergmann2019tracking} enjoys a wide range of application prospects, such as augment reality and autonomous driving. Currently, OT can be divided into two categories: 1) instance tracking (\ie, SOT~\cite{chen2020siamese} and VOS~\cite{oh2019video}), where the target objects of arbitrary classes are specified by the annotations in the first frame, and 2) category tracking (\ie, MOT~\cite{xiang2015learning}, MOTS~\cite{voigtlaender2019mots}, and VIS~\cite{yang2019video}), where all objects of specific categories are expected to be detected in a sequence and associated between adjacent frames. The divergent setups require customized methods with carefully designed architectures and hyper-parameters, leading to complex training and redundant parameters~\cite{yan2022towards}. In contrast, humans possess the capability to address various tracking tasks by nature. The increasing demand for human-like AI has motivated us to consider the possibility of designing a unified model for various types of tracking tasks.

Despite its promise, realizing the above goal requires non-trivial effort considering the distinction between the mainstream solutions developed in both categories of tracking tasks. Instance tracking typically treats the tracking of specified targets as a detection problem given the template bounding boxes, which we term as \textit{tracking-as-detection}. Specifically, they either crop a search region from the tracking frame based on the trajectory for SOT~\cite{li2019siamrpn++,yan2021learning,mayer2021learning,lin2021swintrack}, or match with the spatial-temporal memory which stores historical predictions for VOS~\cite{oh2019video,cheng2021stcn,cheng2022xmem,wang2022look}. The search region or memory readout is then fed into a detection head~\cite{carion2020end,ge2021yolox} or mask decoder~\cite{cheng2021stcn} to predict the bounding boxes or masks of target objects directly. Category tracking ~\cite{voigtlaender2019mots,athar2020stem,cao2020sipmask,li2021spatial,wu2022defense,zhang2022bytetrack,aharon2022bot}, on the other hand, extensively adopts the \textit{tracking-by-detection} paradigm by sequentially detecting all objects of specific classes on each frame and linking them based on the spatial correlation and appearance similarity. We summarize the pipelines of both paradigms in Figure~\ref{fig:conceptual} (a),(b), which illustrates that the critical difference between them lies in the role of ``detector'' and ``tracker'', \ie, whether the ``tracker'' supplements the ``detector'' with spatial and appearance priors, or the ``detector'' provides the ``tracker'' with candidate bounding boxes for association.

\begin{figure*}[t]
  \centering
   \includegraphics[width=\linewidth]{./conceptual.pdf}
   \vspace{-0.25in}
   \caption{Comparisons between different tracking paradigms. In \textit{tracking-as-detection}, the tracker delineates a search region or matches with the memory for the detector, while in \textit{tracking-by-detection}, the detector predicts the bounding boxes for the tracker to associate. We combine the advantages of them and propose a novel \textit{tracking-with-detection}, where a Reference-guided Feature Enhancement (RFE) module supplements with detector with the appearance priors, and the tracker then associates all the detected boxes with the existing trajectories according to their spatial and appearance correlation.}
   \label{fig:conceptual}
\end{figure*}

In this paper, we argue that neither of these two manners fully captures the essence of tracking: unlike object detection in the image domain, tracking utilizes spatial and visual cues to match the detected objects with existing trajectories. The results could in turn, provide a crucial reference for the detection in the next frame. In contrast, purely exploiting the priors from the ``tracker'' to assist the ``detector'' (tracking-as-detection) would result in trajectory deviations after a few tracking failures~\cite{yan2022towards}, and simply applying the ``detector'' to predict the boxes on each frame independently for the ``tracker'' (tracking-by-detection) neglects the temporal information in the detection stage. 

To address this issue, we introduce a novel \textit{tracking-with-detection} paradigm, where a Reference-guided Feature Enhancement (RFE) module is introduced to supplement the detector with appearance priors obtained from the tracker. When deployed to the instance tracking tasks, we enhance the tracking frame with RoIAlign~\cite{he2017mask} features of the previously tracked boxes through cross-attention. While for the category tracking tasks, considering the target objects are often occluded or blurred, we instead adopt the downsampled feature map of the previous frame. With this, the RFE module could not only adapt to different tracking tasks with exactly the same set of parameters, but also function as a task indicator for the detector. Finally, the enhanced features are input to the detector to perform object detection on the full image rather than a cropped area. Such a paradigm enables the unification of various tracking tasks by inheriting the advantages of both tracking-as-detection and tracking-by-detection.

Equipped with the proposed paradigm, we further present \system, a unified tracking model. It is built on top of the Deformable DETR~\cite{zhu2021deformable} and supports five tracking tasks, including SOT, VOS, MOT, MOTS, and VIS, within a fully shared network architecture, model weights, and inference pipeline. Recent literature ~\cite{meng2021conditional,wang2022anchor} suggest that object queries in transformer detectors can be enriched with abundant appearance and positional information through interaction with local image features. Inspired by this, we combine the well-learned queries with their corresponding RoI~\cite{he2017mask} features as the identity embedding of different instances. These embeddings are supervised using a contrastive ReID loss to learn how to associate detected objects between different frames. During inference, we maintain a memory bank for each detected instance to store historical identity embeddings for long-range matching. Extensive experiments are conducted on 7 popular tracking benchmarks, including LaSOT~\cite{fan2019lasot}, TrackingNet~\cite{muller2018trackingnet}, DAVIS16-17~\cite{pont20172017}, MOT17~\cite{milan2016mot16}, MOTS20~\cite{voigtlaender2019mots}, and YTVIS19~\cite{yang2019video}, and the results demonstrate \system achieves new state-of-the-art or at least competitive results on various tracking tasks. 

%-------------------------------------------------------------------------
\section{Related Work}
\label{sec:related}

\subsection{Task-specific Tracking Models}
\label{subsec:tracking_models}
Over the years, a wide variety of task-specific tracking models~\cite{fan2019siamese,li2019evolution,oh2019video,cheng2021stcn,zeng2022motr,chu2023transmot,voigtlaender2019mots,porzi2020learning,wang2021end,yang2021crossover} have been proposed to continuously improve the tracking performance in specific scenarios. Single object tracking (SOT)~\cite{smeulders2013visual}, also known as visual object tracking, requires the trackers to estimate the location of an arbitrary object of interest in a video sequence. Siamese tracking methods~\cite{bertinetto2016fully,li2018high,li2019evolution,li2019siamrpn++,fan2019siamese,chen2020siamese,yu2020deformable,lin2021swintrack} have dominated this field for quite a long time, which train deep models to learn the matching between the target object and a search region cropped from the tracking frame. Video object segmentation (VOS) replaces the description of target objects in SOT with instance masks, which therefore relies on more fine-grained matching to obtain the per-pixel predictions. To this end, spatial-temporal memory networks~\cite{oh2019video,seong2021hierarchical,cheng2021mivos,cheng2021stcn,cheng2022xmem,wang2022look} have been widely explored to separate targeted objects
from the background through memory reading and updating in an online manner. 

\begin{figure*}[t]
  \centering
   \includegraphics[width=\linewidth]{./network.pdf}
   \vspace{-0.25in}
   \caption{Overview of the proposed \system, which consists of a backbone network to extract the multi-scale frame features, a Reference-guided Feature Enhancement (REF) module to model the correlation between the target objects and the tracking frame, and a deformable DETR-based detector to predict the bounding boxes and instance masks. Note that we share the network architecture and inference pipeline for all the tracking tasks. IT: instance tracking, CT: category tracking.}
   \label{fig:network}
\end{figure*}


Unlike the above-mentioned tracking of specified instances, another type of tracking task is dedicated to detecting all objects belonging to particular categories throughout a video and tracking their trajectories. Multiple object tracking (MOT)~\cite{berclaz2011multiple}, for example, aims to track all the pedestrians or vehicles on crowded streets. By far, tracking-by-detection~\cite{dicle2013way,bae2014robust,xiang2015learning,bergmann2019tracking,sun2020transtrack,zhang2021fairmot,liu2022online,zhang2022bytetrack} is the most popular solution for MOT, where all the frames are fed into a powerful detector~\cite{ge2021yolox} first and then the detected boxes are associated temporally according to their Intersection-of-Union (IoU) and appearance similarity. Multiple object tracking and segmentation (MOTS)~\cite{voigtlaender2019mots} additionally annotates instance masks on the MOT benchmark~\cite{milan2016mot16} to extend it to a more challenging problem. Due to the inherent correlation between both tasks, most MOTS models~\cite{voigtlaender2019mots,athar2020stem,wu2021track,meinhardt2022trackformer} are developed upon advanced MOT trackers by appending a mask head. Video instance segmentation (VIS)~\cite{yang2019video} shares the same task definition with MOTS but focuses on more open scenarios and more abundant categories. More precisely, it is an extension of instance segmentation~\cite{hafiz2020survey} in the video domain, which simultaneously performs detection, segmentation, and tracking of instances in videos collected from the YouTube~\cite{bertasius2020classifying,wang2021end,hwang2021video,yang2021crossover,wu2022defense}. 


\subsection{Unified Tracking Models}
\label{subsec:unified_tracking_models}
The customized tracking models developed in various tracking tasks lead to complicated training processes and poor generalization ability, which motivates a series of works~\cite{wang2019fast,wu2021track,wang2021different} exploring a unified architecture to solve multiple tracking tasks. UniTrack~\cite{wang2021different} argues that the core of object tracking lies in visual representation learning, based on which different tracking tasks could be resolved by propagation (\ie, SOT and VOS) or association (\ie, MOT, MOTS, and PoseTrack~\cite{andriluka2018posetrack}) between the learned features. To this end, they apply a shared appearance model and multiple parameter-free heads to address the above five tracking tasks within the same framework. Nevertheless, the remarkable discrepancies between different heads prevent UniTrack~\cite{wang2021different} from making use of large-scale tracking datasets for training and thus limiting its performance. UTT~\cite{ma2022unified} takes a further step by unifying SOT and MOT within a completely homogeneous architecture, which however does not support the per-pixel prediction tasks, \eg, VOS and MOTS. 

Comparatively, Unicorn~\cite{yan2022towards} achieves the grand unification of object tracking for the first time. It bridges the gap between different tracking tasks with a delicately designed target prior, which is the input to the detection head to indicate the objects to be detected. Despite the competitive performance on a wide variety of tracking benchmarks, there still exists twofold problems for Unicorn: 1) the inference pipelines for different tasks vary significantly, for example, they directly pick the box or mask with the highest confidence score as the final tracking result for SOT$\&$VOS, but implement the embedding association between detected boxes and existing trajectories for MOT$\&$MOTS; 2) they train the model on box tasks and mask tasks separately. By contrast, we jointly train \system on all the tracking datasets, and share the same network architecture, model weights, and inference pipeline for different tasks. 

%-------------------------------------------------------------------------
\section{Method}
\label{sec:method}

Our goal is to unify different tracking tasks, including SOT, VOS, MOT, MOTS, and VIS, within a fully shared network architecture, model weights, and inference pipeline. To achieve this, we insert a Reference-guided feature Enhancement (RFE) module into a powerful detector~\cite{zhu2021deformable}, so as to supplement the detector with appearance priors obtained from previous tracking results. The overall framework of \system is illustrated in Figure~\ref{fig:network}. Next, we introduce the architecture of \system (Sec.~\ref{subsec:arch}), the loss functions for training (Sec.~\ref{subsec:loss}), and the unified online tracking (Sec.~\ref{subsec:tracker}), respectively. 

\subsection{Architecture}
\label{subsec:arch}

Given a video sequence $\mathcal{V} = [\textit{X}_{1}, \textit{X}_{2}, ..., \textit{X}_{T}]$, tracking aims to estimate the location of moving objects over time. For instance tracking tasks, the target objects are annotated in the first frame, in the form of bounding boxes $b_{0}$ = $
\{(x_{1}, y_{1}), (x_{2}, y_{2})\}$ for SOT, or instance masks $M_{0}$ for VOS. While for category tracking tasks, \eg, MOT, MOTS, and VIS, all detected objects need to be tracked. Taking the tracking frame $\textit{X}_{t} \in \mathcal{R}^{3 \times H \times W}$ as inputs, we first adopt a backbone network to extract a pyramid of multi-scale features $F = \{f_{t}^{i}\}_{i=i}^{L}$, where $f_{t}^{i} \in \mathcal{R}^{C_{i} \times H/2^{i} \times W/2^{i}}$.

\vspace{0.05in}
\noindent \textbf{RFE module.} Unlike object detection that localizes objects in a static image, tracking continuously updates the positions of target objects in a video sequence. This inspires us to supplement the backbone features of the tracking frame with the appearance priors from the previous tracking results. We discard the location priors here since the occlusions or movements of objects may result in a remarkable change of box coordinates between adjacent frames. More specifically, we enhance the feature pyramid of $\textit{X}_{t}$ with the RoIAlign~\cite{he2017mask} features of the target objects in $\textit{X}_{t-1}$ for the instance tracking tasks, while for the category tracking tasks, we instead downsample the feature map of $\textit{X}_{t-1}$ to provide the temporal contextual information:
\begin{equation}
h_{t-1}^{i}=\left\{
\begin{aligned}
\mathrm{RoIAlign}(f_{t-1}^{i}, \hat{b}_{t-1}) &, \text{instance tracking tasks} \\
\mathrm{DownSample}(f_{t-1}^{i}) &, \text{category tracking tasks}
\end{aligned}
\right.
\label{eq:roi_down}
\end{equation}

where $\hat{b}_{t-1}$ denote the tracked boxes in $\textit{X}_{t-1}$ and initialized with the ground-truth bounding boxes in the first frame\footnote{Note that for VOS, we calculate the bounding boxes by calling $\rm{torchvision.ops.masks\_to\_boxes}$. }. Taking $f_{t}^{i}$ as query, $h_{t-1}^{i}$ as key and value, we then model the correlation between them through cross-attention:
\begin{equation}
   g_{t}^{i} = \mathrm{CrossAttn}(f_{t}^{i}, h_{t-1}^{i}),
\end{equation}
where the normalization and flattening operations are omitted for simplicity. After that, $g_{t}^{i}$ is input to a MLP block to calculate the enhanced features $\widetilde{f}_{t}^{i}$. In this way, we obtain the refined feature pyramid $\widetilde{F} = \{\widetilde{f}_{t}^{i}\}_{i=i}^{L}$.

\vspace{0.05in}
\noindent \textbf{Deformable DETR.} We apply Deformable DETR~\cite{zhu2021deformable} as our object detector, which learns a set of object queries $q \in \mathcal{R}^{N \times C_{q}}$ to decode box coordinates and class labels. Specifically, taking $\widetilde{F}$ as input, a transformer encoder first exchanges the information between different scales through multi-scale deformable self-attention. After that, a transformer decoder is applied to interact the object queries with the image features through deformable cross-attention. Finally, we input the updated queries $\widetilde{q}_{t}$ to a 3-layer Feed-Forward Network (FFN) and a linear classification layer to produce the bounding boxes and category predictions, respectively. For the mask generation, we gather the multi-scale features from transformer encoder through a FPN architecture~\cite{lin2017feature} to produce a high-resolution feature map $m_{t} \in \mathcal{R}^{C_{p} \times H/8 \times W/8}$ and employ $\widetilde{q}_{t}$ to generate instance-aware kernel weights $\omega$. With this, the instance masks $\hat{M}_{t}$ are predicted by a $1\times1$ convolution~\cite{tian2020conditional}:
\begin{equation}
    \hat{M}_{t} = \mathrm{CondConv_{1\times1}}(m_{t}, \omega),
\end{equation}

\subsection{Loss Functions}
\label{subsec:loss}
\noindent \textbf{Per-frame detection loss.} We formulate object detection as a set prediction problem~\cite{carion2020end,zhu2021deformable} and resolve the label assignment from an optimal transport perspective~\cite{ge2021ota,wu2022defense}. We consider both classification accuracy and box GIoU~\cite{rezatofighi2019generalized} to compute the transport cost. In this way, each ground truth is assigned to $K$ predictions with the lowest costs, where $K$ is dynamically calculated for each sample. The overall per-frame detection loss is a weighted sum of multiple terms:
\begin{equation}
    \mathcal{L}_{det} = \mathcal{L}_{cls} + \lambda_{1} \mathcal{L}_{box} + \lambda_{2} \mathcal{L}_{mask},
\end{equation}
where a binary cross-entropy loss is adopted as $\mathcal{L}_{cls}$,  $\mathcal{L}_{box}$ is a combination of L1 loss and the GIoU loss, and $\mathcal{L}_{mask}$ is a combination of the dice loss~\cite{milletari2016v} and the focal loss~\cite{lin2017focal}. We set $\lambda_{1}$ and $\lambda_{2}$ to 2.0 by default.

\vspace{0.05in}
\noindent \textbf{Contrastive ReID loss.} Several works~\cite{yao2021efficient,wang2022anchor} reveal that the interaction with local image features empowers the learnable queries in DETR-like object detectors~\cite{carion2020end,zhu2021deformable} with both appearance and positional information. To this end, we combine these queries with the corresponding RoIAlign~\cite{he2017mask} features for instance-level association between different frames. For the $k_{th}$ query $\widetilde{q}_{t}^{k}$, we first extract its RoIAlign features $r_{t}^{k}$ from $f_{t}^{L}$, the backbone feature with a size of $H/8 \times W/8$, and then send them to an MLP block to generate an identity embedding $e_{t}^{k}$:
\begin{equation}
    e_{t}^{k} = \mathrm{MLP}(\mathrm{concat}(\widetilde{q}_{t}^{k}, r_{t}^{k})),
\end{equation}
During training, we randomly sample a reference frame from the same video, and learn the discriminative identity embedding in a contrastive manner:
\begin{equation}
    \mathcal{L}_{reid} = \mathbf{log}[1 + \sum_{e_{ref}^{+}}\sum_{e_{ref}^{-}}\mathrm{exp}(e_{t} \cdot e_{ref}^{-} - e_{t} \cdot e_{ref}^{+})]. 
\end{equation}
where $e_{ref}$ denotes the identity embeddings in the reference frame. We select $p$ predictions with the least cost as positives and $n$ predictions with the highest costs as negatives. Finally, our model is supervised with $\mathcal{L}_{det}$ + $\beta$ $\mathcal{L}_{reid}$, where $\beta$ is a trade-off parameter and set to 10 empirically.

\subsection{Unified Online Tracking}
\label{subsec:tracker}
In contrast to Unicorn~\cite{yan2022towards}, we adopt the same tracking pipeline for different tasks during inference, where the localization and appearance information are combined for the association between the detected boxes and existing trajectories. Specifically, we maintain a memory bank for each trajectory, which stores the historical identity embeddings to utilize the temporal information for more robust matching. Specifically, assume there are $N$ detected objects in the $t_{th}$ frame and $M$ trajectories with the corresponding memory banks $\{e_{k}^{m}\}_{k=1}^{t-1}$, for $m=1..M$\footnote{For SOT, $M$ is always 1, and for VOS, $M$ is equal to the number of instances annotated in the first frame.}, we first calculate the temporally weighted sum of the identity embeddings stored in each memory bank: $\widetilde{e}^{m} = \sum_{k=1}^{t-1} \frac{k}{t-1} \times e_{k}^{m}$, and then compute the bi-directional similarity between $\{\widetilde{e}^{m}\}_{m=1}^{M}$ and the identity embeddings of the detected instances:
\begin{equation}
    s_{n, m} = [\frac{\mathrm{exp}(\widetilde{e}^{m} \cdot e_{t}^{n})}{\sum_{h=1}^{N}\mathrm{exp}(\widetilde{e}^{m} \cdot e_{t}^{h})} + \frac{\mathrm{exp}(\widetilde{e}^{m} \cdot e_{t}^{n})}{\sum_{h=1}^{M}\mathrm{exp}(\widetilde{e}^{h} \cdot e_{t}^{n})} ] / 2, 
\end{equation}
where $e_{t}^{n}$ is the identity embedding of the $n_{th}$ instance in the $t_{th}$ frame. 

We also adopt Kalman filter~\cite{brown1997introduction} as the motion model to keep track of the detected objects and predict their locations in the tracking frame~\cite{zhang2021fairmot,zhang2022bytetrack,aharon2022bot}. With this, we could calculate the IoU between the $N$ detected bounding boxes and the estimated locations of $M$ trajectories by Kalman filter. Then the boxes with low IoU are filtered out: 
\begin{equation}
\begin{aligned}
    M_{iou} &= \rm{IoU}  <  \tau, \\
    S &= S[M_{iou}].
\end{aligned}
\end{equation}
where $\tau$ is set to 0.25 empirically. Finally, taking $1 - S$ as the cost matrix, we resolve the box assignment problem with the Hungarian algorithm~\cite{kuhn1955hungarian}. The unmatched boxes whose detection scores are larger than $\tau_{new}$ will be initialized as a new track, and we set $\tau_{new} = 1.0$ for the instance tracking tasks.


%-------------------------------------------------------------------------
\section{Experiments}
\label{sec:exp}

\subsection{Implementation Details}
\noindent \textbf{Training.} The complete training process consists of three stages: during the first stage, we pre-train the model on COCO~\cite{lin2014microsoft} for object detection and instance segmentation following Unicorn~\cite{yan2022towards}. Then we follow the common practice in VIS methods~\cite{hwang2021video,wu2022seqformer,wu2022defense} to randomly crop the same image from COCO twice to form a pseudo key-reference frame pair. Finally, the proposed \system is fine-tuned on the training splits of various tracking datasets, during which two images are randomly sampled as key and reference frames respectively. Note that we treat COCO~\cite{lin2014microsoft} as an additional downstream dataset and perform joint training on it to make the training process more stable. We implement 6 dataloaders for COCO, SOT (TrackingNet~\cite{muller2018trackingnet}, LaSOT~\cite{fan2019lasot}, GOT10K~\cite{huang2019got}), VOS (DAVIS17~\cite{pont20172017}), MOT (MOT17~\cite{milan2016mot16}), MOTS (MOTS20~\cite{voigtlaender2019mots}), and VIS (YTVIS19~\cite{yang2019video}), and iteratively feed one of them to the model for joint training. We use different classifiers for different tasks and set the category annotation of SOT and VOS to 0. Multi-scale training is applied and the shortest side ranges from 736 to 864, following~\cite{yan2022towards}. The batch sizes for the three stages are 32, 32, and 16, and the training iterations are 185k, 148k, and 144k. The parameters are optimized with AdamW~\cite{loshchilov2018decoupled}. The initial learning rates are all set to $1e^{-4}$ for three stages, and decayed by 0.1 after 148k, 59.2k, and 8k iterations. The RoI size and downsampled size in Equation~\ref{eq:roi_down} are both set to 7. We adopt Swin Transformer~\cite{liu2021swin}-Tiny and Swin Transformer-Large as backbones in the main experiments, and Swin Transformer-Tiny as the backbone in the ablation. Since Unicorn~\cite{yan2022towards} only reports the results with ConvNeXt~\cite{liu2022convnet}-Large as the backbone, we also reproduce their method with Swin Transformer-Tiny~\cite{liu2021swin} as the backbone for a fair comparison, which is denoted as Unicorn-T$^{*}$. Note that we conduct ablation studies on the validation split of MOT17 for MOT unless otherwise mentioned.

\begin{table}[t]
\centering
  \renewcommand\arraystretch{1.1}
  \setlength{\tabcolsep}{0pt} % let TeX compute the intercolumn space
  \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lc | cccc | cccc @{}}
    \toprule
    \small
    \multirow{2}*{\textbf{Method}} && \multicolumn{3}{c}{\textbf{LaSOT}} && \multicolumn{3}{c}{\textbf{TrackingNet}}  \\
    ~ && Suc & P$_{norm}$ & P && Suc & P$_{norm}$ & P \\
    \midrule
    SiamFC~\cite{bertinetto2016fully} && 33.6 & 42.0 & 33.9 && 57.1 & 66.3 & 53.3 \\
    %D3S~\cite{lukezic2020d3s} && - & - & - && 72.8 & 76.8 & 66.4 \\
    %ATOM~\cite{danelljan2019atom} && 51.5 & 57.6 & 50.5 && 70.3 & 77.1 & 64.8 \\
    SiamPRN++~\cite{li2019siamrpn++} && 49.6 & 56.9 & 49.1 && 73.3 & 80.0 & 69.4 \\
    %DiMP~\cite{bhat2019learning} && 56.9 & 65.0 & 56.7 && 74.0 & 80.1 & 68.7 \\
    KYS~\cite{bhat2020know} && 55.4 & 63.3 & - && 74.0 & 80.0 & 68.8 \\
    Ocean~\cite{zhang2020ocean} && 56.0 & 65.1 & 56.6 && - & - & - \\
    AutoMatch~\cite{zhang2021learn} && 58.2 & - & 59.9 && 76.0 & - & 72.6 \\
    PrDiMP~\cite{danelljan2020probabilistic} && 59.8 & 68.8 & 60.8 && 75.8 & 81.6 & 70.4 \\
    TrDiMP~\cite{wang2021transformer} && 63.9 & - & 61.4 && 78.4 & 83.3 & 73.1 \\
    Siam R-CNN~\cite{voigtlaender2020siam} && 64.8 & 72.2 & - && 81.2 & 85.4 & 80.0 \\
    TransT~\cite{chen2021transformer} && 64.9 & 73.8 & 69.0 && 81.4 & 86.7 & 80.3 \\
    KeepTrack~\cite{mayer2021learning} && 67.1 & 77.2 & 70.2 && - & - & -\\
    STARK~\cite{yan2021learning} && 67.1 & 77.0 & - && 82.0 & 86.9 & - \\
    SwinTrack~\cite{lin2021swintrack} &&  67.2 & - & 70.8 && 81.1 & - & 78.4 \\
    AiATrack~\cite{gao2022aiatrack} && - & 79.4 & 73.8 && - & 87.8 & 80.4 \\
    OSTrack~\cite{ye2022joint} && - & 78.7 & 75.2 && - & 87.8 & 82.0 \\
    SimTrack-L~\cite{chen2022backbone} && - & 79.7 & - && - & 87.4 & - \\
    MixFormer-L~\cite{cui2022mixformer} && - & \textcolor{blue}{79.9} & \textcolor{blue}{76.3} && - & \textcolor{blue}{88.9} & \textcolor{blue}{83.1} \\
    \midrule
    UniTrack~\cite{wang2021different} && 35.1 & - & 32.6 && - & - & - \\
    UTT~\cite{ma2022unified} && 64.6 & - & 67.2 && 79.7 & - & 77.0 \\
     Unicorn-T$^{*}$~\cite{yan2022towards} && 65.3 & 73.1 & 68.7 && 79.0 & 82.0 & 77.4 \\
    \system-T && 65.9 & 73.5 & 69.3 && 80.2 & 82.5 & 77.7 \\
    Unicorn-ConvL~\cite{yan2022towards} && 68.5 & 76.6 & 74.1 && 83.0 & 86.4 &  82.2\\
    \system-L && \textcolor{red}{69.1} & \textcolor{red}{77.3} & \textcolor{red}{75.4} && \textcolor{red}{83.4} & \textcolor{red}{86.7} & \textcolor{red}{82.3} \\
    \bottomrule
  \end{tabular*}
 \caption{Comparisons with the SOT-specific trackers and unified trackers on LaSOT~\cite{fan2019lasot} and TrackingNet~\cite{muller2018trackingnet}. }
\label{tab:sot}
\vspace{-0.25in}
\end{table}

\noindent \textbf{Inference.} During inference, we implement the memory bank mentioned in Sec.~\ref{subsec:tracker} as a First-In-First-Out (FIFO) queue. Note that for the instance tracking tasks, we store the identity embedding of the box whose IoU with the ground truth box is highest in the memory for the first frame, and always reserve it during memory update. The memory size is set to 64, 64, 32, and 3 for SOT, VOS, MOT(S), and VIS. 

Next, we report the performance of \system on various tracking tasks. For each benchmark, the best performance of the task-specific method is noted in \textcolor{blue}{blue}, while the best result of the unified model is noted in \textcolor{red}{red}.

\subsection{Evaluation on the Instance Tracking Tasks}

\subsubsection{Single Object Tracking}
In Table~\ref{tab:sot}, we compare \system with both SOT-specific and unified tracking methods on two large-scale SOT benchmarks: LaSOT~\cite{fan2019lasot} which contains 1,120 training sequences and 280 testing sequences, and TrackingNet~\cite{muller2018trackingnet} which includes 30k sequences in the training set and 511 sequences in the testing set. Success (Suc), precision (P), and normalized precision (P$_{norm}$) are reported. We can see that using Swin-Tiny as backbone, \system outperforms Unicorn~\cite{yan2022towards} by 0.4\% and 0.5\% in terms of P$_{norm}$ on these two datasets, respectively. When a larger backbone is adopted, we maintain the advantage over the unified models and achieve competitive results compared to the SOT-specific models.

\begin{table}[t]
\centering
  \renewcommand\arraystretch{1.1}
  \setlength{\tabcolsep}{0.7pt} % let TeX compute the intercolumn space
  \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lcc | cccc | cccc @{}}
    \toprule
    \small
    \multirow{2}*{\textbf{Method}} & \multirow{2}*{\textbf{Mem}} && \multicolumn{3}{c}{\textbf{DAVIS16}} && \multicolumn{3}{c}{\textbf{DAVIS17}}  \\
    ~ & ~ && $\mathcal{J\&F}$ & $\mathcal{J}$ & $\mathcal{F}$ && $\mathcal{J\&F}$ & $\mathcal{J}$ & $\mathcal{F}$ \\
    \midrule
    %FAVOS~\cite{cheng2018fast} & \XSolidBrush && 81.0 & 82.4 & 79.5 && 58.2 & 54.6 & 61.8 \\
    %OSMN~\cite{maninis2018video} & \XSolidBrush && 73.5 & 74.0 & 72.9 && 54.8 & 52.5 & 57.1 \\
    VideoMatch~\cite{hu2018videomatch} & \XSolidBrush && - & 81.0 & - && 56.5 & - & - \\
    RANet~\cite{wang2019ranet} & \XSolidBrush && 85.5 & 85.5 & 85.4 && 65.7 & 63.2 & 68.2 \\
    FRTM~\cite{robinson2020learning} & \Checkmark && 83.5 & 83.6 & 83.4 && 76.7 & 73.9 & 79.6 \\
    LWL~\cite{bhat2020learning} & \Checkmark && - &  - & - && 81.6 & 79.1 & 84.1 \\
    STM~\cite{oh2019video} & \Checkmark && 89.3 & 88.7 & 89.9 && 81.8 & 79.2 & 84.3 \\
    CFBI~\cite{yang2020collaborative} & \Checkmark && 89.4 & 88.3 & 90.5 && 81.9 & 79.1 & 84.6 \\
    HMMN~\cite{seong2021hierarchical} & \Checkmark && 90.8 & 89.6 & 92.0 && 84.7 & 81.9 & 87.5 \\
    STCN~\cite{cheng2021stcn} & \Checkmark && 91.6 & 90.8 & 92.5 &&  85.4 & 82.2 & 88.6 \\
    XMem~\cite{cheng2022xmem} & \Checkmark && 92.0 & 90.7 & 93.2 && 87.7 & 84.0 & 91.4 \\
    ISVOS~\cite{wang2022look} & \Checkmark && \textcolor{blue}{92.8} & \textcolor{blue}{91.8} & \textcolor{blue}{93.8} && \textcolor{blue}{88.2} & \textcolor{blue}{84.5} & \textcolor{blue}{91.9} \\
    \midrule
    SiamMask~\cite{wang2019fast} & \XSolidBrush && 69.8 & 71.7 & 67.8 && 56.4 & 54.3 & 58.5 \\
    D3S~\cite{lukezic2020d3s} & \XSolidBrush && 74.0 & 75.4 & 72.6 && 60.8 & 57.8 & 63.8 \\
    Siam R-CNN~\cite{voigtlaender2020siam} & \XSolidBrush && - & - & - && 70.6 & 66.1 & 75.0 \\
    \midrule
    UniTrack~\cite{wang2021different} & \Checkmark && - & - & - && - & 58.4 & - \\
    Unicorn-T$^{*}$~\cite{yan2022towards} & \XSolidBrush && 83.2 & 83.0 & 83.4 && 64.5 & 62.7 & 66.3 \\
    \system-T & \Checkmark && 84.7 & 84.1 & 85.3 && 66.2 & 64.9 & 67.5 \\
    Unicorn-ConvL~\cite{yan2022towards} & \XSolidBrush && 87.4 & 86.5 & 88.2 && 69.2 & 65.2 & 73.2 \\
    \system-L & \Checkmark && \textcolor{red}{88.5} & \textcolor{red}{87.3} & \textcolor{red}{89.7} && \textcolor{red}{71.0} & \textcolor{red}{66.8} & \textcolor{red}{75.2} \\
    \bottomrule
  \end{tabular*}
 \caption{Comparisons with the VOS-specific trackers and unified trackers on DAVIS 2016 and 2017 val~\cite{pont20172017}. \textbf{Mem} indicates whether a memory bank is maintained to utilize long-term information. }
\label{tab:vos}
\end{table}

\subsubsection{Video Object Segmentation}

Compared with SOT, VOS is a per-pixel classification problem that requires the model to capture more fine-grained information. We evaluate \system on both single-object VOS benchmark, DAVIS 2016~\cite{pont20172017}, and its multi-object extension DAVIS 2017. Popular metrics including the mean Jaccard $\mathcal{J}$ index, the mean boundary $\mathcal{F}$ score, and the average of them $\mathcal{J\&F}$ are used. Note that the results of several multi-task instance trackers, \eg, SiamMask~\cite{wang2019fast}, D3S~\cite{lukezic2020d3s}, and Siam R-CNN~\cite{voigtlaender2020siam}, are also reported for comparison. The results in Table~\ref{tab:vos} show that \system beats its multi-task counterparts and other unified tracking models by a large margin on both datasets. In terms of $\mathcal{J\&F}$, we surpass Unicorn~\cite{yan2022towards} by 1.1\% and 1.8\% on DAVIS16 and 17, respectively. Although there still exists some gap between us and the task-specific VOS methods, \system could solve multiple tracking tasks simultaneously and therefore enjoys higher flexibility.


\begin{table}[t]
\centering
  \renewcommand\arraystretch{1.1}
  \small
  \setlength{\tabcolsep}{0.5pt} % let TeX compute the intercolumn space
  \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lc | ccccccc @{}}
    \toprule
    \small
    \textbf{Method} && MOTA$\uparrow$ & IDF1$\uparrow$ & HOTA$\uparrow$ & FP$\downarrow$ & FN$\downarrow$ & IDs$\downarrow$ \\
    \midrule
    TransCenter~\cite{xu2021transcenter} && 73.2 & 62.2 & 54.5 & 23112 & 123738 & 4614 \\
    FairMOT~\cite{zhang2021fairmot} && 73.7 & 72.3 & 59.3 & 27507 & 117477 & 3303 \\
    RelationTrack~\cite{yu2022relationtrack} && 73.8 & 74.7 & 61.0 & 27999 & 118623 & 1374 \\
    CSTrack~\cite{liang2022rethinking} && 74.9 & 72.6 & 59.3 & 23847 & 114303 & 3567 \\
    TransTrack~\cite{sun2020transtrack} && 75.2 & 63.5 & 54.1 & 50157 & 86442 & 3603 \\
    SiamMOT~\cite{liang2022one} && 76.3 & 72.3 & - & - & - & - \\
    CorrTracker~\cite{wang2021multiple} && 76.5 & 73.6 & 60.7 & 29808 & 99510 & 3369 \\
    TransMOT~\cite{chu2023transmot} &&  76.7 & 75.1 & 61.7 & 36231 & 93150 & 2346 \\
    MAATrack~\cite{stadler2022modelling} && 79.4 & 75.9 & 62.0 & 37320 & 77661 & 1452 \\
    OCSORT~\cite{cao2022observation} && 78.0 & 77.5 & 63.2 & \textcolor{blue}{15129} & 107055 & 1950 \\
    StrongSORT++~\cite{du2022strongsort} && 79.6 & 79.5 & 64.4 & 27876 & 86205 & 1194 \\
    ByteTrack~\cite{zhang2022bytetrack} && 80.3 & 77.3 & 63.1 & 25491 & \textcolor{blue}{83721} & 2196 \\
    Bot-SORT~\cite{aharon2022bot} && \textcolor{blue}{80.5} & \textcolor{blue}{80.2} & \textcolor{blue}{65.0} & 22521 & 86037 & \textcolor{blue}{1212} \\
    \midrule
    Unicorn-T$^{*}$~\cite{yan2022towards} && 73.1 & 73.3 & 60.1 & \textcolor{red}{28434} & 121245 & 2094 \\
    \system-T && 76.9 & 74.0 & 61.0 & 30216 & 97914 & 2367 \\
    Unicorn-ConvL~\cite{yan2022towards} && 77.2 & 75.5 & 61.7 & 50087 & \textcolor{red}{73349} & 5379 \\
    \system-L && \textcolor{red}{79.1} & \textcolor{red}{75.6} & \textcolor{red}{62.3} & 29247 & 87192 & \textcolor{red}{1968} \\
    \bottomrule
  \end{tabular*}
 \caption{Comparisons with the MOT-specific trackers and unified trackers on  MOT17~\cite{milan2016mot16} test set. }
\label{tab:mot}
\end{table}


\subsection{Evaluation on Category Tracking Tasks}

\subsubsection{Multiple Object Tracking}
We perform multiple object tracking on the most popular dataset, MOT17~\cite{milan2016mot16}, which focuses on pedestrian tracking in crowded scenes. MOT17 includes 7 sequences in the training set and 7 sequences in the test set. Six representative metrics are reported for quantitative comparison, including Multiple-Object Tracking Accuracy (MOTA), Identity F1 Score (IDF1), False Positives (FP), False Negatives (FN), and Identity Switches (IDS). The results in Table~\ref{tab:mot} show that \system achieves 79.1\% and 75.6\% in terms of MOTA and IDF1, surpassing Unicorn~\cite{yan2022towards} by 1.9\% and 0.1\%, respectively. Note that most state-of-the-art MOT-specific tracking models, \eg, ByteTrack~\cite{zhang2022bytetrack} additionally adopt Cityperson~\cite{zhang2017citypersons} and ETHZ~\cite{ess2008mobile} for training, we believe the performance of our model will be further boosted with these additional training data.

\subsubsection{Multiple Object Tracking and Segmentation}

To increase the challenge of multiple object tracking, MOTS20~\cite{voigtlaender2019mots} extends the MOT17~\cite{milan2016mot16} dataset with additional pixel-wise annotations. We report the results of \system on it, including sMOTSA, Identity F1 Score (IDF1), False Positives (FP), False Negatives (FN), and Identity Switches (IDS). Note that sMOTSA differs from MOTA in that it is based on the mask overlap. We can observe from Table~\ref{tab:mots} that \system outperforms PointTrackV2~\cite{xu2021segment} and Unicorn~\cite{yan2022towards} by 5.2\% and 2.2\% in terms of sMOTSA, which demonstrates the effectiveness of our method on this task.

\begin{table}[t]
\centering
  \renewcommand\arraystretch{1.1}
  \small
  \setlength{\tabcolsep}{0.pt} % let TeX compute the intercolumn space
  \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lc | ccccccc @{}}
    \toprule
    \textbf{Method} && sMOTSA$\uparrow$ & IDF1$\uparrow$ & FP$\downarrow$ & FN$\downarrow$ & IDs$\downarrow$ \\
    \midrule
    Track R-CNN~\cite{voigtlaender2019mots} &&  40.6 & 42.4 & 1261 & 12641 & 567 \\
    TraDeS~\cite{wu2021track} &&  50.8 & 58.7 & 1474 & 9169 & 492 \\
    TrackFormer~\cite{meinhardt2022trackformer} && 54.9 & 63.6 & 2233 & 7195 & 278\\
    PointTrackV2~\cite{xu2021segment} && \textcolor{blue}{62.3} & \textcolor{blue}{42.9} & \textcolor{blue}{963} & \textcolor{blue}{5993} & \textcolor{blue}{541} \\
    \midrule
    Unicorn-T$^{*}$~\cite{yan2022towards} && 61.5 & 60.8 & 2675 & 6509 & 466 \\
    \system-T && 62.2 & 62.5 & 1394 & 5805 & 411 \\
    Unicorn-ConvL~\cite{yan2022towards} && 65.3 & 65.9 & 1364 & \textcolor{red}{4452} & 398 \\
    \system-L && \textcolor{red}{67.5} & \textcolor{red}{69.2} & \textcolor{red}{780} & 5204 & \textcolor{red}{215} \\
    \bottomrule
  \end{tabular*}
 \caption{Comparisons with the MOT-specific trackers and unified trackers on  MOT17~\cite{milan2016mot16} test set. }
\label{tab:mots}
\end{table}


\begin{table}[t]
\centering
  \renewcommand\arraystretch{1.1}
  \setlength{\tabcolsep}{0.pt} % let TeX compute the intercolumn space
  \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lc | cccccc @{}}
    \toprule
    \small
    \textbf{Method} && mAP & AP50 & AP75 & AR1 & AR10 \\
    \midrule
    MaskTrack R-CNN~\cite{yang2019video} && 31.8 & 53.0 & 33.6 & 33.2 & 37.6 \\
    CrossVIS~\cite{yang2021crossover} && 36.6 & 57.3 & 39.7 & 36.0 & 42.0 \\
    PCAN~\cite{ke2021prototypical} &&  37.6 & 57.2 & 41.3 & 37.2 & 43.9 \\
    STEm-Seg~\cite{athar2020stem} && 34.6 & 55.8 & 37.9 & 34.4 & 41.6 \\
    VisTR~\cite{wang2021end} && 40.1 & 64.0 & 45.0 & 38.3 & 44.9 \\
    MaskProp~\cite{bertasius2020classifying} && 42.5 & - & 45.6 & - & - \\
    Propose-Reduce~\cite{lin2021video} && 43.8 & 65.5 & 47.4 & 43.0 & 53.2 \\
    IFC~\cite{hwang2021video} && 44.6 & 69.2 & 49.5 & 44.0 & 52.1 \\
    SeqFormer-Res101~\cite{wu2022seqformer} && 49.0 & 71.1 & 55.7 & 46.8 & 56.9 \\
    IDOL-Res101~\cite{wu2022defense} && 50.1 & 73.1 & 56.1 & 47.0 & 57.9 \\ 
    \midrule
    \system-T && 58.8 & 82.4 & 64.9 & 52.4 & 65.5 \\
    SeqFormer-L~\cite{wu2022seqformer} && 59.3 & 82.1 & 66.4 & 51.7 & 64.4 \\
    IDOL-L$^{*}$~\cite{wu2022defense} && \textcolor{blue}{63.0} & \textcolor{blue}{87.0} & \textcolor{blue}{69.7} & \textcolor{blue}{55.0} & \textcolor{blue}{67.9} \\
    \textcolor{gray}{IDOL-L}~\cite{wu2022defense} && \textcolor{gray}{64.3} & \textcolor{gray}{87.5} & \textcolor{gray}{71.0} & \textcolor{gray}{55.6} & \textcolor{gray}{69.1} \\
    \system-L && \textcolor{red}{63.9} & \textcolor{red}{88.0} & \textcolor{red}{70.1} & \textcolor{red}{55.1} & \textcolor{red}{68.5} \\
    \bottomrule
  \end{tabular*}
 \caption{Comparisons with the VIS-specific trackers on YouTube-VIS~\cite{yang2019video} 2019 val set. Currently, there are no unified tracking models that support the VIS task. }
\label{tab:vis}
\end{table}

\subsubsection{Video Instance Segmentation}

VIS shares the same goal with MOTS but covers more categories and more open scenarios. We evaluate \system on YTVIS 2019~\cite{yang2019video}, which contains 2,238 training, 302 validation, and 343 test high-resolution video clips collected from YouTube. Since existing unified tracking models cannot support the VIS task, we only compare \system with the VIS-specific models. The standard metrics such as AP, AP50, AP75, AR1, and AR10 are reported in Table~\ref{tab:vis} for quantitative comparison. Note that we reproduce IDOL-L~\cite{wu2022defense} using the officially released code and report both the reproduced results (IDOL-L$^{*}$) and the results in their paper (\textcolor{gray}{IDOL-L}). We can see that \system outperforms the existing methods by a clear margin on all the metrics, \eg, 0.9\% and 4.6\% in terms of mAP.

\subsection{Discussion and Analysis}
\noindent \textbf{Joint training $v.s.$ independent training.} \system is jointly trained on various tracking datasets by alternatively feeding the batched data from different datasets during training. To verify the effects of joint training on the task unification, we also train our model on different tasks separately to obtain several task-specific models. The comparison results on several representative datasets between joint training (Our-Joint), separate training (Ours-Sep), and Unicorn~\cite{yan2022towards}, are reported in Table~\ref{tab:joint}. We can see that joint training achieves consistently better results than both separate training and Unicorn on all the tasks. We hypothesize this is because we achieve a greater unification across different tasks, and training on the data from various sources could improve the generalization ability of our model. In addition, the comparison of Frames-Per-Second (FPS) also shows that \system enjoys a prominent advantage over Unicorn in terms of inference efficiency, \eg, 20.9 $vs$ 41.7 using Swin-Tiny as backbone.
 
\begin{table}[!ht]
\centering
  \renewcommand\arraystretch{1.1}
  \setlength{\tabcolsep}{0.pt} % let TeX compute the intercolumn space
  \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lc | ccccccc @{}}
    \toprule
    \small
    \multirow{2}*{\textbf{Method}} && \textbf{TrNet} & \textbf{D17} & \multicolumn{2}{c}{\textbf{MOT17}} & \textbf{YT19} & \multirow{2}*{FPS} \\
    ~ && P$_{norm}$ & $\mathcal{J\&F}$ & MOTA & IDF1 & mAP & ~ \\
    \midrule
    Unicorn-T$^{*}$~\cite{yan2022towards} && 82.0 & 62.7 & 71.7 & 73.4 & - & 20.9 \\
    \midrule
    Ours-Sep && 82.0 & 65.7 & 73.1 & 75.0 & 58.3 & 41.7 \\
    Ours-Joint && \textbf{82.5} & \textbf{66.2} & \textbf{73.2} & \textbf{75.4} & \textbf{58.8} & 41.7 \\
    \bottomrule
  \end{tabular*}
 \caption{The performance on various tracking tasks with and without the proposed RFE module. TrNet: TrackingNet, D17: DAVIS 2017, YT19: YTVIS 2019. }
\label{tab:joint}
\end{table}


\begin{figure*}[t]
  \centering
   \includegraphics[width=\linewidth]{./visualization.pdf}
   \vspace{-0.25in}
   \caption{Visualizations of tracking results predicted by \system on various tracking tasks. }
   \label{fig:visualization}
\end{figure*}

\noindent \textbf{Impact of the RFE Module.} RFE supplements the detector with appearance priors by modeling the correlation between target objects (or the reference frame) and the tracking frame with cross-attention. To evaluate its contribution, we remove it from \system and conduct experiments on various tasks. Without the RFE module, the P$_{norm}$ on TrackingNet and the MOTA on MOT17 decrease by 1.6\% and 0.8\%, respectively. The performance degradation validates that the RFE module effectively improves the performance of our model.

\begin{table}[!ht]
\centering
  \renewcommand\arraystretch{1.1}
  \setlength{\tabcolsep}{0.pt} % let TeX compute the intercolumn space
  \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lc | ccccc @{}}
    \toprule
    \small
    \multirow{2}*{\textbf{Method}} && \textbf{TrNet} & \textbf{D17} & \multicolumn{2}{c}{\textbf{MOT17}} & \textbf{YT19} \\
    ~ && P$_{norm}$ & $\mathcal{J\&F}$ & MOTA & IDF1 & mAP \\
    \midrule
    w/o RFE && 80.9 & 64.7 & 72.4 & 74.6 & 58.5 \\
    w/ RFE (Ours) && \textbf{82.5} & \textbf{66.2} & \textbf{73.2} & \textbf{75.4} & \textbf{58.8} \\
    \bottomrule
  \end{tabular*}
 \caption{The performance on various tracking tasks with and without the proposed RFE module. TrNet: TrackingNet, D17: DAVIS 2017, YT19: YTVIS 2019. }
\label{tab:rfe}
\vspace{-0.05in}
\end{table}

\noindent We also visualize the feature maps before and after the RFE module in Figure~\ref{fig:rfe}. The results demonstrate that the target objects in the enhanced features are more distinguishable, with which our detector could localize the target objects more accurately.

\begin{figure}[!ht]
  \centering
   \includegraphics[width=\linewidth]{./rfe.pdf}
   \vspace{-0.25in}
   \caption{Visualizations of the reference frame, backbone features, enhanced features, and the tracking results.}
   \label{fig:rfe}
\end{figure}

\vspace{0.05in}
\noindent \textbf{Visualizations.} We visualize the tracking results of \system on different tasks in Figure~\ref{fig:visualization}. For the instance tracking tasks, the target objects may belong to any classes, \eg the luminous corner of a stone in the 1st row, which requires the model to possess strong detection and association ability. While for the category tracking tasks, the dramatic movements of target objects  (5th row) and the severe occlusions (the 3rd row) both pose great challenges to the robustness of the model. The superior performance of \system on both types of tasks fully proves its effectiveness.


%-------------------------------------------------------------------------
\section{Conclusion}
\label{sec:conclusion}
This paper presented \system, a unified Deformable DETR-based tracking model that addresses both instance tracking tasks (\ie, SOT and VOS) and category tracking tasks (\ie, MOT, MOTS, and VIS), with a fully shared network architecture, model weights, and inference pipeline. Incorporating the advantages of the dominant solutions in the above two types of tasks, we further introduced a \textbf{tracking-with-detection} paradigm, where tracking supplements detection with appearance priors to locate the targets more accurately and detection provides tracking with candidate boxes for the association. Extensive experiments conducted on a wide variety of tracking benchmarks demonstrate the effectiveness of the proposed method. 

Although \system significantly outperforms Unicorn~\cite{yan2022towards} on the VOS task, there is still a gap between the task-specific models. We believe the reason lies in that they widely adopt high-resolution spatial-temporal memory for dense matching, whereas we only exploit compact query-based memory. In the future, we will explore the combination of both types of memory for tracking to further improve the performance on the VOS task.

%-------------------------------------------------------------------------
\clearpage

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
