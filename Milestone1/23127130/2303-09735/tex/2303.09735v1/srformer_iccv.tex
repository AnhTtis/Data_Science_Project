\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
% \usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\usepackage[pagebackref=false,breaklinks,colorlinks,linkcolor=BrickRed,citecolor=RoyalBlue]{hyperref}
\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{5789} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
% add from arxiv
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{verbatim}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{overpic}
\usepackage[dvipsnames,table,xcdraw]{xcolor}

\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\figref}[1]{Fig.~\ref{#1}}
\newcommand{\tabref}[1]{Tab.~\ref{#1}}
\newcommand{\eqnref}[1]{Eqn.~(\ref{#1})}
\newcommand{\secref}[1]{Sec.~\ref{#1}}
\newcommand{\myPara}[1]{\vspace{5pt}\noindent\textbf{#1}}
\newcommand{\myAppendex}[1]{\vspace{5pt}\noindent\textbf{\Large{#1}}}
\newcommand{\sArt}{state-of-the-art~}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\graphicspath{{./figs/}}
\newcommand{\para}[1]{\noindent\textbf{#1}}
\usepackage{makecell}
\newcommand{\highlight}[1]{\textbf{\textcolor{MidnightBlue}{#1}}}
\def\cc{\cellcolor[HTML]{EFEFEF}}
\newcommand{\addFig}[1]{}
\newcommand{\addFigs}[1]{}
\usepackage{adjustbox}
% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\usepackage{caption}
\usepackage{subcaption}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
% add from arxiv

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
% \usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
% \usepackage[pagebackref=false,breaklinks,colorlinks,linkcolor=BrickRed,citecolor=RoyalBlue]{hyperref}
%

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi
\newcommand{\nameofmethod}{SRFormer}
\begin{document}

%%%%%%%%% TITLE
\title{\nameofmethod{}: Permuted Self-Attention for Single Image Super-Resolution}

\author{Yupeng Zhou$^1$ \quad Zhen Li$^1$ \quad Chun-Le Guo$^1$ \quad Song Bai$^2$ \quad Ming-Ming Cheng$^1$ \quad Qibin Hou$^1$\thanks{Corresponding author.}\\ \\
$^1$TMCC, School of Computer Science, Nankai University\\
$^2$ByteDance, Singapore\\
{\tt\small https://github.com/HVision-NKU/SRFormer}
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\begin{abstract}
Previous works have shown that increasing the window size for Transformer-based image super-resolution models (e.g., SwinIR) can significantly improve the model performance but the  computation overhead is also considerable.
%
In this paper, we present \nameofmethod{}, a simple but novel method that can enjoy the benefit of large window self-attention but introduces even less computational burden.
%
The core of our \nameofmethod{} is the permuted self-attention (PSA), which strikes an appropriate balance between the channel and spatial information 
for  self-attention.
%
Our PSA is simple and can be easily applied to existing super-resolution networks based on window self-attention.
%
Without any bells and whistles, we show that our \nameofmethod{} achieves
a 33.86dB PSNR score on the Urban100 dataset, which is 0.46dB higher than that of
SwinIR but uses fewer parameters and computations.
%
We hope our simple and effective approach can serve as a useful tool
for future research in super-resolution model design.



% Exploring efficient and effective super resolution algorithms is a hot research direction in computer vision. 
%
% In this paper, we investigate how to more efficiently take advantage of
% the local self-attention to generate higher-quality super-resolution images.
% %
% Enlarging the window size of the local self-attention in SwinIR has been shown an effective way to achieve the state-of-the-art performance but the computational burden is also considerable.
% %
% We reexamine the design principle of self-attention and propose to 

% the local self-attention by shrinking the channel information
% convey the spatial information
% to the channel dimension

% We renovate the 
%
% Previous works have shown that enlarging the window size of
% the local self-attention used in SwinIR is  improve the performance
% but the computational burden are also considerable.
% %
% To reduce the computational cost due to the utilization of large windows,
% we attempt to shrink the channel information and propose to convey the spatial information
% to the channel dimension to guarantee that the self-attention is performed in a large window.
% %
% Our goal is to test the limits of what a local window self-attention can achieve with no
% increase of the computations.
% %
% Based on the improved self-attention, we advance the SwinIR model and present a new network, termed \nameofmethod{}, for image super resolution.
% %
% Experiments show that



% design principles of self-attention for Super Resolution.Although self-attention has dominated many fields, there is still a lack of self-attention design backbone networks incorporating domain knowledge in the field of image super-resolution.We 
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

Single image super-resolution (SR) aims to restore a high-quality image from its degraded low-resolution version.
%
Exploring efficient and effective super-resolution algorithms has been a hot research topic in computer vision, which have a variety of applications~\cite{jo2018deep,wang2019edvr,anwar2020deep}. 
%
Since the pioneer works~\cite{dong2014learning,kim2016accurate,zhang2018learning,ledig2017photo,shi2016real,lim2017enhanced}, CNN-based methods have been the mainstream for image super-resolution for a long time.
%
These methods mostly take advantage of  residual learning~\cite{tai2017image,ledig2017photo,lim2017enhanced,kim2016accurate,zhang2021plug,li2018multi}, dense connections~\cite{wang2018esrgan,zhang2018residual,tong2017image}, or channel attention~\cite{zhang2018image,yang2021image} to construct network architectures,
making great contributions to the development of super-resolution models.

\begin{figure}
    \centering
    \scriptsize
    \includegraphics[width=\linewidth]{figures/teaser.pdf}
    \put(-28., 41){\cite{liang2021swinir}}
    % SRFormer
    \put(-200, 48){$\mathrm{WS}: 12\times12$}
    \put(-200, 38){$\mathrm{PSNR}: 33.08$}
    \put(-190, 95){$\mathrm{WS}: 16\times16$}
    \put(-190, 85){$\mathrm{PSNR}: 33.26$}
    \put(-136, 130){$\mathrm{WS}: 24\times24$}
    \put(-136, 120){$\mathrm{PSNR}: 33.51$}
    % SwinIR
    \put(-142.5, 50){$\mathrm{WS}: 8\times8$}
    \put(-142.5, 40){$\mathrm{PSNR}: 33.09$}
    \put(-82.5, 83){$\mathrm{WS}: 12\times12$}
    \put(-82.5, 73){$\mathrm{PSNR}: 33.28$}
    \put(-64, 132){$\mathrm{WS}: 16\times16$}
    \put(-64, 122){$\mathrm{PSNR}: 33.40$}
    
    \caption{Performance comparison between SwinIR and our \nameofmethod{} when training 200k iterations with different window sizes (WS) for 200k iterations. Our \nameofmethod{} enjoys a large window size of $24\times24$ with even fewer computations but higher PSNR scores.}
    \label{fig:teaser}
     \vspace{-4mm}
\end{figure}

Despite the success made by CNN-based models in super-resolution, recent works~\cite{chen2021pre,liang2021swinir,zhang2022swinfir,zhang2022efficient}
have shown that Transformer-based models perform better.
%
They observe that the ability of self-attention to build pairwise relationships is a more efficient way to produce high-quality super-resolution images than convolutions. One  typical work among them should be SwinIR~\cite{liang2021swinir} which introduces Swin Transformer~\cite{liu2021swin} to image super-resolution, greatly improving
the state-of-the-art CNN-based models on various benchmarks.
%
Later, a variety of works, such as SwinFIR~\cite{zhang2022swinfir}, ELAN~\cite{zhang2022efficient}, and HAT~\cite{chen2022activating}, further develop SwinIR and use Transformers to design different network architectures for SR.

% (need rewrite)Balance of the model's performance, parameters, flops, and GPU memory usage is the key to designing an excellent super-resolution model. Previous work usually only considers the first three, but we believe that GPU memory usage is also an important factor in evaluating models.
% As far as we know, there is still a lack of comprehensive research on the micro design of self-attention in image super-resolution. This makes it often difficult to investigate how self-attention helps images recover information and how to design self-attention networks to maximize information recovery from low-resolution images.


The aforementioned methods reveal that properly enlarging the windows for the shifted window self-attention in SwinIR can result in clear performance gain (see Figure~\ref{fig:teaser}).
%
However, the computational burden is also an important issue as the window size goes larger. 
%
In addition, Transformer-based methods utilize self-attention and require networks of larger channel numbers compared to previous CNN-based methods~\cite{zhang2018image,zhang2018residual,kim2016accurate}.
%
To explore efficient and effective super-resolution algorithms, a straightforward question should be: How would the performance go if we reduce the channel number and meanwhile increase the window size?

Motivated by the question mentioned above, in this paper, we present permuted self-attention (PSA), an efficient way to build pairwise relationships within large windows (e.g., $24\times24$).
%
% We observe that given the input feature with $C$ channels (e.g., 180 in SwinIR),
% We observe that when computing the attention maps shrinking the channel numbers
% of the key and value yields no performance drop.
%
The intention is to enable more pixels to get involved in the attention map computation and at the same time introduce no extra computational burden.
%
To this end, we propose to shrink the channel dimensions of the key and value matrices
and adopt a permutation operation to convey part of the spatial information into the channel dimension.
%
In this way, despite the channel reduction, there is no loss of spatial information, and each attention head is also allowed to
keep a proper number of channels to produce expressive attention maps~\cite{touvron2021going}.
%
% In addition, the self-attention can be done within a large window 
% with no extra computational burden.
%
In addition, we also improve the original feed-forward network (FFN) by
adding a depth-wise convolution between the two linear layers, which we found
helps in high-frequency component recovery.

Given the proposed PSA, we construct a new network for SR, termed \nameofmethod{}.
%
We evaluate our \nameofmethod{} on five widely-used datasets.
%
Benefiting from the proposed PSA, our \nameofmethod{} can clearly improve its performance on almost all five datasets.
%
Notably, for $\times2$ SR, our \nameofmethod{} trained on only the DIV2K dataset~\cite{lim2017enhanced} achieves a 33.86 PSNR score on the challenging Urban100 dataset~\cite{huang2015single}.
%
This result is much higher than those of
the recent SwinIR (33.40) and ELAN (33.44).
%
A similar phenomenon can also be observed when evaluating on the $\times3$ and $\times4$ SR tasks.
%
In addition, we perform experiments using a light version of our \nameofmethod{}.
%
Compared to previous lightweight SR models, our method also achieves better performance on all benchmarks.

% The comparison results of our \nameofmethod{} with the state-of-the-art methods
% shows that our model
%
To sum up, our contributions can be summarized as follows:
\begin{itemize}[leftmargin=*]
    \item We propose a novel permuted self-attention for image super-resolution, which can enjoy large-window self-attention by transferring spatial information into the channel dimension. By leveraging it, we are the first to implement  24x24 large window attention mechanism at an acceptable time complexity in SR.
    % \vspace{3pt}
    \item We build a new transformer-based super-resolution network, dubbed \nameofmethod{}, based on the proposed PSA and an improved FFN from the frequency perspective (ConvFFN). Our \nameofmethod{} achieves \sArt performance in  classical, lightweight, and real-world image SR tasks.
\end{itemize}






\section{Related Work}

In this section, we briefly review the literature on image super-resolution.
%
We first describe CNN-based methods and then transition to the recent popular
Transformer-based models.

\subsection{CNN-Based Image Super-Resolution}

Since SRCNN\cite{dong2014learning} first  introduced CNN into image super-resolution (SR), a large number of CNN-based  SR models have emerged.
%
DRCN~\cite{kim2016deeply} and DRRN~\cite{tai2017image} introduce recursive convolutional networks to increase the depth of the network without increasing the parameters.
%
Some early CNN-based methods~\cite{tai2017memnet,dong2014learning,kim2016deeply,tai2017image} attempt to interpolate the low-resolution (LR) as input, which results in a computationally expensive feature extraction.
%
To accelerate the SR inference process, FSRCNN~\cite{dong2016accelerating} extracts features at the LR scale and conducts an upsampling operation at the end of the network.
%
This pipeline with  pixel shuffle upsampling~\cite{shi2016real} has been widely used in later works~\cite{zhang2022efficient,zhang2018image,liang2021swinir}.
%
LapSRN~\cite{lai2017deep} and DBPN~\cite{haris2018deep} perform upsampling during extracting feature  to learn the correlation between LR and HR.
%
% ~\cite{hu2019meta,wang2021learning,zhou2022deep} propose different upsampling methods.
%
There are also some works~\cite{ledig2017photo,wang2018esrgan,zhang2019ranksrgan,wang2021real} that use GAN~\cite{goodfellow2014generative} to generate realistic textures in reconstruction.
%
MemNet~\cite{tai2017memnet}, RDN~\cite{zhang2018residual}, and HAN~\cite{niu2020single}  efficiently aggregate the intermediate features  to enhance the quality of the reconstructed images.
%
Non-Local attention~\cite{wang2018non} has also been extensively explored in SR to better model the long-range dependencies.
%
Methods of this type include CS-NL~\cite{mei2020image}, NLSA~\cite{mei2021image}, SAN~\cite{dai2019second}, IGNN~\cite{zhou2020cross}, etc.
%
% Recently, transformer-based methods\cite{yang2020learning,chen2021pre,liang2021swinir,zhang2022efficient} have demonstrated strong performance in SR tasks. 


% Since SRCNN firstly  introduced CNN into image super-resolution (SR), a large number of CNN-based  SR models have emerged.
% Considering upsampling is a key step in  mapping from low-resolution to a high-resolution,
% ~\cite{dong2016accelerating,haris2018deep,lai2017deep} investigate about when to upsample during high-resolution image inference and ~\cite{shi2016real,hu2019meta,wang2021learning,zhou2022deep} propose different upsampling methods.~\cite{ledig2017photo,wang2018esrgan,zhang2019ranksrgan,wang2021real} use GAN~\cite{goodfellow2014generative} to generate realistic textures in reconstruction.
% MemNet~\cite{tai2017memnet}, RDN~\cite{zhang2018residual}, and HAN~\cite{niu2020single}  efficiently aggregate and utilize  intermediate features  to enhance reconstruction. Non-Local attention~\cite{wang2018non} has also been extensively explored in SR to better model the long-range dependencies, including CS-NL~\cite{mei2020image}, NLSA~\cite{mei2021image}, SAN~\cite{dai2019second} and IGNN~\cite{zhou2020cross}.

% Recently, transformer-based SR methods\cite{yang2020learning,chen2021pre,liang2021swinir,zhang2022efficient} have demonstrated strong performance. IPT~\cite{chen2021pre}is a large pre-trained model based on Transfromer encoder and decoder structure.
%  SwinIR~\cite{liang2021swinir} uses Swin Transformer encoder~\cite{liu2021swin} for feature extraction, which reduces the amount of calculation and parameters of the model and achieves extremely powerful performance. ELAN~\cite{zhang2022efficient} simplify the architecture of SwinIR and use different window sizes to exploit the correlation between long-range pixels,but it barely improves reconstruction performance. Different from them, Our \nameofmethod{} allows each Transformer
% block to build pairwise correlations within large windows with even less computation.

% FSRCNN  uses  a transposed convolution at the tail of the nerwork for upsampling, which reduces the computational complexity of feature extraction. ESPCN proposed Sub-Pixel Convolutional upsampling, which is currently widely used by mainstream models.LapSRN and DBPN employ progressive upsampling and Iterative up and downsampling, respectively, to help super-resolution reconstruction. EDSR and RCAN extend the depth of the network based on residual connections.
%  MemNet, RDN, and HAN aggregate intermediate features to enhance reconstruction. CS-NL, NLSA, and SAN use non-local attention to better model the long-range dependencies.


% IPT\cite{chen2021pre} is a large model based on Transfromer encoder and decoder structure, pre-trained on imagenet.
% Based on the residual in residual structure\cite{zhang2018image}, SwinIR uses Swin Transformer\cite{liu2021swin} for feature extraction, which reduces the amount of calculation and parameters of the model and achieves extremely powerful performance.ELAN,



\begin{figure*}
    \centering
    \small
    \setlength{\abovecaptionskip}{5pt}
    \includegraphics[width=\linewidth]{figures/arch.pdf}
    \caption{Overall architecture of \nameofmethod{}. The pixel embedding module is a $3\times3$ convolution to map the input image to feature space. The HR image reconstruction module contains a $3\times3$ convolution and a pixel shuffle operation to reconstruct the high-resolution image. The middle feature encoding part has $N$ PAB groups, followed by a $3\times3$ convolution.}
    \label{fig:arch}
   \vspace{-4mm}
\end{figure*}

\subsection{Vision Transformers}

Transformers recently have shown great potential in a variety of vision tasks,
including image classification~\cite{dosovitskiy2020image,touvron2020training,Yuan_2021_ICCV,wang2021pyramid,yuan2022volo}, object detection~\cite{carion2020end,sun2021rethinking,fang2021you}, semantic segmentation~\cite{xie2021segformer,zheng2021rethinking,strudel2021segmenter},
image  restoration~\cite{zamir2022restormer,liang2021swinir,chen2021pre}, etc.
%
Among these, the most typical work should be Vision Transformer (ViT)~\cite{dosovitskiy2020image} which proves Transformers can perform better
than convolutional neural networks on feature encoding.
%
% shows the great potential of Transformers for processing large-scale
% data in image classification and it performs not well on small-sized dataset.
%
% DeiT~\cite{touvron2020training} and T2T-ViT~\cite{Yuan_2021_ICCV,yuan2022volo} advances the training recipe of the original ViT
% and show that with the reliance on the ImageNet-1k dataset and using strong
% data augmentation methods, ViTs without the support of large-scale data can still
% behave well.
%
The application of Transformers in low-level vision mainly includes two categories: generation~\cite{jiang2021transgan,lee2021vitgan,zhang2022styleswin,deng2022stytr2} and restoration.
%
Further, the restoration tasks can also be divided into two categories: video restoration~\cite{lu2022video,liu2021fuseformer,ren2022dlformer,liu2022learning,geng2022rstt} and image restoration~\cite{chen2021pre,zamir2022restormer,wang2022uformer,guo2022image}. 
% Difference with high-level tasks, Transfromer-based models applied to image restoration tasks~\cite{chen2021pre,zamir2022restormer,wang2022uformer,guo2022image}, such as image super-resolution, image denoise,  dehaze, deblur and derain, 

As an important task in image restoration, image super-resolution needs to preserve the structural information of the input, which is a great challenge for Transformer-based model design.
%
IPT~\cite{chen2021pre} is a large pre-trained model based on the Transformer encoder and decoder structure and has been applied to super-resolution, denoising, and deraining.
%
% The input is split into a sequence of image patches in order to reduce the number of calculations but it blocks a wide range of information interaction.
% Based on U-Net structure, Uformer~\cite{wang2022uformer} and Restormer~\cite{zamir2022restormer} introduce local self-attention and transposed attention, respectively, and achieve \sArt performance in denoising, deblurring and deraining.
% SwinIR introducing Swin Transfromer to image restoration and keep feature scale invariant during feature extraction, which is a common design  in image super-resolution such as~\cite{zhang2018image,tai2017memnet,mei2021image,zhang2018residual}.
Based on the Swin Transformer encoder~\cite{liu2021swin}, SwinIR~\cite{liang2021swinir}  performs self-attention on an $8\times8$ local window in feature extraction and achieves extremely powerful performance.
% without a massive increase in  computations compared to previous CNN-based methods.
%
ELAN~\cite{zhang2022efficient} simplifies the architecture of SwinIR and uses self-attention computed in different window sizes to collect the correlations between long-range pixels.
% but it barely improves the reconstruction performance. 

Our \nameofmethod{} is also based on Transformer.
%
Different from the aforementioned methods that directly leverage self-attention to build models, our \nameofmethod{} mainly aims at the self-attention itself.
%
Our intention is to study how to compute self-attention in a large window to improve the performance of SR models without increasing the parameters and computational cost.
% block to build pairwise correlations within a larger windows 




% Transformers recently have shown great potentials in a variety of vision tasks,
% including image classification~\cite{dosovitskiy2020image,touvron2020training,Yuan_2021_ICCV,wang2021pyramid}, object detection~\cite{carion2020end,sun2021rethinking,fang2021you}, semantic segmentation~\cite{xie2021segformer,zheng2021rethinking,strudel2021segmenter},
% image restoration~\cite{zamir2022restormer,liang2021swinir,chen2021pre}, etc.
% %
% % originally designed for natural language processing tasks~\cite{devlin2018bert,vaswani2017attention}, have been widely used in
% % visual recognition.
% %
% Among these, the most typical work should be Vision Transformer (ViT)~\cite{dosovitskiy2020image} that proves Transformers can perform better
% than convolutional neural networks on ImageNet classification~\cite{deng2009imagenet} when large-scale datasets are available.
% %
% % shows the great potential of Transformers for processing large-scale
% % data in image classification and it performs not well on small-sized dataset.
% %
% DeiT~\cite{touvron2020training} and T2T-ViT~\cite{Yuan_2021_ICCV,yuan2022volo} advances the training recipe of the original ViT
% and show that with the reliance on the ImageNet-1k dataset and using strong
% data augmentation methods, ViTs without the support of large-scale data can still
% behave well.
% %

% Motivated by the success of pyramid architecture in ConvNets, some works~\cite{heo2021rethinking,liu2021swin,wang2021pyramid,yang2021focal,dong2022cswin,li2022mvitv2} design pyramid structures
% using transformers to take advantage of multi-scale features.
% %
% Some works~\cite{chen2021crossvit,han2021transformer,wu2021cvt,vaswani2021scaling,guo2022cmt,dai2021coatnet,Yuan_2021_ICCV,han2021demystifying,hassani2022neighborhood} propose to introduce local dependencies into ViTs
% and showing great performance in visual recognition.
% %
% Besides,there are also some works~\cite{zhou2021deepvit,zhai2021scaling,touvron2021going,liu2022swinv2,yuan2022volo,jiang2021all} exploring the scaling capability of ViTs.






% \subsection{Transformers in Low-Level Vision}


%% Set5\cite{bevilacqua2012low}, Set14\cite{zeyde2010single}, BSD100\cite{martin2001database}, Urban100\cite{huang2015single} and Manga109\cite{matsui2017sketch}




\section{Method}
%The vanilla SwinIR~\cite{liang2021swinir} uses window multi-head attention mechanism (W-MSA)~\cite{liu2021swin} to infer high-resolution images, achieving state-of-the-art performance in super resolution.
%
%Later, HAT~\cite{chen2022activating} shows that simply enlarging the window size from $8\times8$ to $16\times16$ in window self-attention computation can largely boost the model performance but the computational overhead is also considerable.
%
%Based on SwinIR, we also conduct experiments with window sizes of 8, 12, and 16.
%The conclusion is that within a certain range, the performance of the model is enhanced with the enlargement of the window size. But there is a fact that the cost of attention computation is positively related to the number of points participating in attention computation. 

% In this section, we first present the overall architecture of the proposed \nameofmethod{}. Then, we describe the proposed permuted self-attention in detail. Finally, we discuss another two ways to achieve large-window self-attention.

% It can perform large window attention with even less computational burden. After that, we will introduce  four other large-window self-attention variants designed by us and compare them with permuted self-attention.



\subsection{Overall Architecture}

The overall architecture of our \nameofmethod{} is shown in \figref{fig:arch}, consisting of three parts: a pixel embedding layer $G_{P}$, a feature encoder $G_{E}$, and a high-resolution image reconstruction layer $G_{R}$. 
%
Following previous works~\cite{liang2021swinir,zhang2022efficient}, the pixel embedding layer $G_{P}$ is a single $3\times3$ convolution that transforms the low-resolution RGB image $I \in \mathbb{R}^{H \times W \times 3}$ to feature embeddings $F_{P} \in \mathbb{R}^{H \times W \times C}$.
%
$F_{P}$ will then be sent into the feature encoder $G_E$ with a hierarchical structure.
%
It consists of $N$ permuted self-attention groups, each of which is with $M$ permuted self-attention blocks followed by a $3\times3$ convolution. 
%
A $3\times3$ convolution is added at the end of the feature encoder, yielding
$F_E$. 
%
The summation results of $F_E$ and $F_{P}$ are fed into $G_R$ for high-resolution image reconstruction, which contains a $3\times3$ convolution and a sub-pixel convolutional layer~\cite{shi2016real} to reconstruct high-resolution images.
%
% Feeding shallow feature  $F_{s}$ into the function, we can get Deep feature $F_d$:
% $$F_d=H_{D F}\left(F_s\right)$$
%
% Then we add shallow feature $F_{s}$ and deep features $F_{d}$ to mix as input to the reconstruction module $H_{R E C}$:
% $$
% I_{HQ}=H_{R E C}\left( F_{s} + F_{d} \right)
% $$
% where $I_{HQ}$ denotes high-resolution image reconstructed by our network.
%
% For $G_R$, we use the sub-pixel convolutional layer to reconstruct high-resolution images.
%
We compute the L1 loss between the high-resolution reconstructed image and ground-truth HR image to optimize our \nameofmethod{}.

\begin{figure*}
    \centering
    \small
    \setlength{\abovecaptionskip}{0pt}
    \includegraphics[width=0.95\linewidth]{figures/psa.pdf}
    \caption{Comparison between (a) self-attention and (b) our proposed permuted self-attention. To avoid  spatial information loss, we propose to reduce the channel numbers and transfer the spatial information to the channel dimension.}
    \label{fig:psa_comp}
    \vspace{-4mm}
\end{figure*}

\subsection{Permuted Self-Attention Block}\label{sec:PSA}

The core of our \nameofmethod{} is the permuted self-attention block (PAB), which consists of a permuted self-attention (PSA) layer and a convolutional feed-forward network (ConvFFN).
%
% The PSA module uses permuted self-attention to perform an efficient large-window attention operation. The local MLP module aggregates local information and adds high-frequency information from the frequency domain perspective.

\myPara{Permuted self-attention.} As shown in  \figref{fig:psa_comp}(b),  given an input feature map $\mathbf{X}_{in} \in \mathbb{R}^{H \times W \times C}$ and a tokens reduction factor $r$, we first split $\mathbf{X}_{in}$ into $N$ non-overlapping square windows  $\mathbf{X} \in \mathbb{R}^{NS^2 \times C}$, where $S$ is the side length of each window.
%
Then, we use three linear layers $L_Q,L_K,L_V$ to get $\mathbf{Q} $, $\mathbf{K}$, and   $\mathbf{V}$:
\begin{equation}
\mathbf{Q},\mathbf{K},\mathbf{V} = L_Q(\mathbf{X}),L_K(\mathbf{X}), L_V(\mathbf{X}).
\end{equation}
Here, $\mathbf{Q}$ keeps the same channel dimension to $\mathbf{X}$ while $L_K$ and $L_V$ compress the channel dimension to $C/r^2$, yielding $\mathbf{K} \in \mathbb{R}^{NS^2 \times C/r^2}$  and   $\mathbf{V} \in \mathbb{R}^{NS^2  \times C /r^2}$. 
%
After that, to enable more tokens to get involved in the self-attention calculation and avoid the increase of the computational cost, we propose to permute the spatial tokens in $\mathbf{K}$ and $\mathbf{V}$ to the channel dimension, attaining permuted tokens $\mathbf{K}_{p} \in \mathbb{R}^{NS^2/r^2 \times C}$ and  $\mathbf{V}_{p} \in \mathbb{R}^{NS^2 /r^2 \times C}$. 

We use $\mathbf{Q}$ and the shrunken $\mathbf{K}_{p}$ and $\mathbf{V}_{p}$ to perform the self-attention operation.
%
In this way, the window size for $\mathbf{K}_{p}$ and $\mathbf{V}_{p}$
will be reduced to $\frac{S}{r}\times\frac{S}{r}$ but their channel dimension is still unchanged to guarantee the expressiveness of the attention map generated by each attention head~\cite{touvron2021going}.
%
% Since the window size of $\mathbf{Q}$ is larger than that of $\mathbf{K}_{p}$, we adopt an aligned relative position embedding $\text{ARPE} \in \mathbb{R}^{S^2 \times S^2 /r^2}$ when computing the attention scores.
% %
% For each position in $\mathbf{Q}$, we first find the same position in  $\mathbf{K} $ , and then find the position after permute in $\mathbf{k}_{p}$, So that a mapping from $\mathbf{Q}$ to $\mathbf{K}_{p}$ can be established.Using it, we can map each position from  $\mathbf{Q}$ to $\mathbf{K}_{p}$ to calculate align relative position bias $\text{ARPE}$
%
The formulation of the proposed PSA can be written as follows:
%
\begin{equation}
\operatorname{PSA}(\mathbf{Q}, \mathbf{K}_{p}, \mathbf{V}_{p})=\operatorname{Softmax}\left(\frac{\mathbf{Q}  \mathbf{K}_{p}^{T}}{\sqrt{d_{k}}}+\mathbf{B}\right)  \mathbf{V}_{p},
\end{equation}
where $\mathbf{B}$ is an aligned relative position embedding that can be attained by interpolating the original one defined in~\cite{liu2021swin} since the window size of $\mathbf{Q}$ does not match that of $\mathbf{K}_{p}$.
%
$\sqrt{d_{k}}$ is a scalar as defined in~\cite{dosovitskiy2020image}.
%
% we adopt a align relative position bias $\mathbf{ARPB} \in \mathbb{R}^{W^2}  \times \mathbb{R}^{W^2 /r^2} $. For each position in  $\mathbf{Q}$, we first find the same position in  $\mathbf{K} $ , and then find the position after permute in $\mathbf{k}_{p}$, So that a mapping from $\mathbf{Q}$ to $\mathbf{K}_{p}$ can be established. We map each position from  $\mathbf{Q}$ to $\mathbf{K}_{p}$ to calculate align relative position bias $\mathbf{ARPB}$. 
% As $\mathbf{Q}$ and $\mathbf{K}_{p},\mathbf{V}_{p}$ have the same dimensions, 
%
Note that the above equation can easily be converted to the multi-head version by splitting the channels into multiple groups.
\begin{figure}[!htbp]
    \vspace{-5pt}
    \centering
    \includegraphics[width=\linewidth]{sub_picture/Power/convFN.pdf}
    \vspace{-22pt}
    \caption{Power spectrum of the intermediate feature maps produced by our \nameofmethod{} with FFN and ConvFFN. Lines in darker  color correspond to features from deeper layers.}
    \label{fig:power}
    \vspace{-5pt}
\end{figure}  % BSL and Path
Our PSA transfers the spatial information to the channel dimension.
%
It ensures the following two key design principles: i) We do not downsample the tokens first as done in~\cite{xie2021segformer,wang2021pyramid} but allow each token to  participate in the self-attention computation independently. This enables more representative attention maps.
We will discuss more variants of our PSA in \secref{sec:variants} and show  more results in our experiment section.
ii) In contrast to the original self-attention illustrated in \figref{fig:psa_comp}(a), PSA can be conducted in a large window (e.g., $24\times24$) using even fewer computations than SwinIR with $8\times8$ window while attaining better performance. 
% Calculates the dense relationship between a wide range of tokens to avoid short-sightedness.

\myPara{ConvFFN.} Previous works have demonstrated that self-attention can be viewed as a low-pass filter~\cite{parkvision,wang2022anti}.
%
To better restore high-frequency information, a $3\times3$ convolution is often added at the end of each group of Transformers as done in SwinIR~\cite{liang2021swinir}.
%
Different from SwinIR, in our PAB, we propose to add a local depthwise convolution branch
between the two linear layers of the FFN block to assist in encoding more details.
%
We name the new block as ConvFFN.
%
% as it has been shown that self-attention is a low-pass filter~\cite{wang2022anti}.
% The original FFN in SwinIR is composed of two linear layers to encode the channel information.
%
% In our PAB, inspired by~\cite{xie2021segformer}, we add a local depthwise convolution
% between the two linear layers to assist in encoding more details as it has been shown that self-attention is a low-pass filter~\cite{wang2022anti}.
%
We empirically found that such an operation increases nearly no computations but can
compensate for the loss of high-frequency information caused by self-attention shown in \figref{fig:power}. 
%
We simply calculate the power spectrum of the feature maps produced by our \nameofmethod{} with FFN and ConvFFN.
%
By comparing the two figures, we can see that ConvFFN can clearly increase high-frequency information, and hence yields better results as listed in \tabref{tab:window_size}.
%
% More analysis can be found in our supplementary materials.

% We redesign the MLP module based on two principles: 1)Since Permuted self-attention can efficiently achieve larger scale self-attention, we also want it to take into account some very local information meanwhile, such as  $3 \times 3$ or $5 \times 5$ spatial area. 2) Since the self-attention operation is proved to be a low-pass filter~\cite{wang2022anti}, maybe itâ€˜s worthwhile to use high-pass filters   to compensate for the loss of high-frequency information caused by self-attention operation.

% By visualizing the power spectrum of intermediate features during the whole inference process of SwinIR, we find that each $3 \times 3$ convolution in the tail of the Residual Swin Transformer Block is particularly important for high-frequency information improvement, (Details are in the appendix.) Therefore, adding a convolution branch to the feed-forward network after permuted self-attention may also be useful. Considering that simply adding  $3 \times 3$  convolution kernels will increase lots of parameters and computation, we balance various factors and redesign feed-forward network by adding a high-frequency branch  based $5 \times 5$ depth-wise convolution. The ablation study show that it helps our model utilize local information and high-frequency information augmentation.

\subsection{Large-Window Self-Attention Variants} \label{sec:variants}

% In this subsection, we describe another two large-window self-attention variants, discuss the differences between PSA and them, and show the advantage of the proposed PSA over them.

% There are also other ways to enlarge the window size but do not increase the parameters and computational cost.
%
To  provide guidance for the design of large-window self-attention and demonstrate the advantage of our PSA, here, we introduce another two large-window self-attention variants.
%
The quantitative comparisons and  analysis can be found in our experiment section.

% , in this subsection, we introduce another three large-window self-attention variants.
% and compare them with the proposed PSA. Through the discussions with PSA and different variants, we hope to find out why our PSA works efficient and powerful. 

% The large-windows self-attention variants we design must efficiently compute attention over a larger window.Since the  Linear projection before and after QKV computation will not  
%
% We intend to share

% The vanilla SwinIR~\cite{liang2021swinir} uses window multi-head attention mechanism (W-MSA)~\cite{liu2021swin} to infer high-resolution images, achieving state-of-the-art performance in super resolution. But W-MSA computes local attention within a window, for SwinIR~\cite{liang2021swinir}, the window size fo W-MSA is $8\times8$,which limits the inference for large textures.
% %
% % Many works has shown the  
% % Later, HAT~\cite{chen2022activating} shows that simply enlarging the window size from $8\times8$ to $16\times16$ in window self-attention computation can largely boost the model performance but the computational overhead is also considerable.
% %
% Based on SwinIR, we conduct experiments with window sizes of 8, 12, and 16.
% The conclusion is that within a certain range, the performance of the model is enhanced with the enlargement of the window size, but the computations of W-MSA has also increased dramatically.In this case, we hope to explore how to design a self-attention module that does not increase the amount of computation and has a larger receptive field. \textbf{Except for permuted self-attention, we also designed other three variants: i) Spatial downsampling of the self-attention window. ii)Bottleneck attention mechanism. iii)Sampling attention. }
% % But there is a fact that the cost of attention computation is positively related to the number of points participating in attention computation. 

% % In this work,we hope to explore how to design a self-attention module that does not increase the amount of computation and has a larger receptive field. \textbf{Expect for permuted self-attention, we also designed other three variants: 
% i) Spatial downsampling. ii)Bottleneck attention. iii) Random attention.
% %i)Simple reduce the dim of the channel to balance Flops. 
% To better illustrate our work, we now give the computational complexity of W-MSA. Specify the window size as $S$, a $
% H \times W \times C
% $ low-resolution feature map was first split into $
% \frac{H}{S} \times \frac{W}{S}
% $ windows, then the attention among pixels was computed in each window. The computational cost of  W-MSA can be represented by the following functions:

% \begin{equation}
%     \text{O(W-MSA)}=\underbrace{3HWC^{2}}_{\text{QKV}}+\underbrace{2HWS^{2} C}_{\textrm{Attention}}+\underbrace{HWC^{2}}_{\text{Proj}}
% \end{equation}

\myPara{Token Reduction.} The first way to introduce large-window self-attention and avoid the increase in computational cost is to reduce the number of tokens as done in~\cite{xie2021segformer}.
%
Let $r$ and $S$ be a reduction factor and the window size.
%
Given an input $\mathbf{X}\in \mathbb{R}^{NS^2 \times C}$, we can adopt a depthwise convolutional function with kernel size $r \times r$ and stride $r$ to reduce the token numbers of $\mathbf{K}$ and $\mathbf{V}$ in each window to $(\frac{S}{r})^2$, yielding
$\mathbf{Q}_r \in \mathbb{R}^{NS^2 \times C}$ and $\mathbf{K}_r \in \mathbb{R}^{NS^2/r^2 \times C}$.
%
$\mathbf{Q}_r$ and $\mathbf{K}_r$ are used to compute the attention scores $\mathbf{A} \in \mathbb{R}^{{S^2} \times {S^2/r^2} }$.
%
Computing the matrix multiplication between $\mathbf{A}$ and  $\mathbf{V}$ yields
the output with the same number of tokens to $\mathbf{X}$.
% then $M \times V$ will recover the tokens number from $W^2$ to ensure the size of feature map  unchanged.
% $F_{\theta}$ can be any downsampling function, such as max-pooling, average pooling, or a
% convolution layer.We choose a $r \times r$ depth-wise convolution kernel with stride $ r $ as function $F_{\theta}$,make the convolution kernel learn a downsampling function  from the data.

% \myPara{Bottleneck attention.} Inspired by  bottleneck designed in ResNet~\cite{he2016deep}, the second way introduce efficient  large-window self-attention by decompose attention scores $\mathbf{A}  \in \mathbb{R}^{{S^2} \times {S^2}}$ into two submatrices:
% \begin{equation}
%     \mathbf{A} = \mathbf{A_1}\mathbf{A_2}, 
% \end{equation}
% where $\mathbf{A_1}  \in \mathbb{R}^{{S^2} \times {S^2/r^2}}$ and $\mathbf{A_2}  \in \mathbb{R}^{{S^2/r^2} \times {S^2}}$.  By computing $\mathbf{A_1}$ and $\mathbf{A_2}$ separately, we can reduce the amount of computation by $1-2/r^2$.  Given a token $\mathbf{X} \in \mathbb{R}^{N \times S^2 \times C}$, we can get original size $\mathbf{Q,K,V} \in \mathbb{R}^{N \times S^2 \times C}$ and  bottleneck feature $\mathbf{I} \in \mathbb{R}^{N \times S^2/r^2 \times C}$ reduced by a depthwise convolutional function. Then $\mathbf{A_1}$ , $\mathbf{A_2}$ and the output are compute as follows:




% \begin{equation}
% \begin{aligned}
% \mathbf{A_{1}} &=\mathbf{Q} \mathbf{I} \\
% \mathbf{A_{2}} &=\mathbf{I} \mathbf{K} \\
% \text { Attention}\left(\mathbf{Q,K,V}\right)  &=\mathbf{A_{1}} \mathbf{A_{2}} \mathbf{V}=\mathbf{A_{1}}\left(\mathbf{A_{2}} \mathbf{V}\right)
% \end{aligned}
% \end{equation}

% we reduce the token numbers to get an intermediate layer $\mathbf{X}_{I} \in \mathbb{R}^{N \times S^2/r^2 \times C}$,  to the attention computation to avoid too many tokens involved in one attention calculation. We use a tokens reduction function $F_{\theta}$ mentioned above to get a an intermediate layer$X_{\theta} \in R^{w^2/r^2} \times R^c $  from input feature maps $X \in R^{w^2} \times R^c $  , and use $X_{\theta}$  as a bottleneck attention layer to operator twice attention calculations.

\myPara{Token Sampling.} The second way to achieve large-window self-attention is to randomly sample $T^2~(0 \le T \le S)$ tokens from each window according to a given sampling ratio $t$ for the key $\mathbf{K}$ and value $\mathbf{V}$.
%
Given the input $\mathbf{X}\in \mathbb{R}^{NS^2 \times C}$, $\mathbf{Q}$ shares the same shape with $\mathbf{X}$ but the shapes of $\mathbf{K}$ and $\mathbf{V}$
are reduced to $NT^2 \times C$.
%
In this way, as long as $T$ is fixed, the computational cost increases linearly as the window size gets larger.
%
A drawback of token sampling is that randomly selecting a portion of tokens loses structural information of the scene content, which is essentially needed for image super-resolution.
%
We will show more numerical results in our experiment section.

% Then we can use the attention mechanism between sample tokens and original tokens to enlarge the receptive field of W-MSA. Sampling Attention performs badly because random sampling loses structural information of feature maps, which is especially needed for image super-resolution.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ablation window size
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table*}[tp]
\setlength\tabcolsep{3.7pt}
\renewcommand{\arraystretch}{0.8}
% \scriptsize
\small
\centering
\setlength{\belowcaptionskip}{0cm}   
% \resizebox{\linewidth}{!}{
\begin{tabular}{lccccccccccccc}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{Window size} & \multirow{2}{*}{Params} & \multirow{2}{*}{MACs}  &  \multicolumn{2}{l}{  \makebox[0.10\textwidth][c]{SET5~\cite{bevilacqua2012low}} } & \multicolumn{2}{l}{\makebox[0.10\textwidth][c]{SET14~\cite{zeyde2010single}}} & \multicolumn{2}{l}{ \makebox[0.10\textwidth][c]{B100~\cite{martin2001database}}} & \multicolumn{2}{l}{\makebox[0.10\textwidth][c]{Urban100~\cite{huang2015single}}} & \multicolumn{2}{l}{\makebox[0.10\textwidth][c]{Manga109~\cite{matsui2017sketch}}} \\ 
\cmidrule(lr){5-6}\cmidrule(lr){7-8}\cmidrule(lr){9-10}\cmidrule(lr){11-12} \cmidrule(lr){13-14}
& & & &\makecell{PSNR}  &\makecell{SSIM} &\makecell{PSNR}  &\makecell{SSIM}&\makecell{PSNR}  &\makecell{SSIM}  &\makecell{PSNR}  &\makecell{SSIM}&\makecell{PSNR}  &\makecell{SSIM}     \\ \midrule
\multirow{3}{*}{$ \rm \textbf{SwinIR}$~\cite{liang2021swinir}}& $8\times8$ &11.75M&2868G&38.24&0.9615&33.94&0.9212&32.39&0.9023&33.09&0.9373&39.34&0.9784\\
& $12\times12$ &11.82M&3107G&38.30&0.9617&34.04&0.9220&32.42&0.9026&33.28&0.9381&39.44&0.9788\\
& $16\times16$ &11.91M&3441G&38.32&0.9618&34.00&0.9212&32.44&0.9030&33.40&0.9394&39.53&0.9791
\\ \midrule
\multirow{3}{*}{\makecell{$\rm \textbf{\nameofmethod{}}$ w/o \\ $\textbf{ConvFFN}$}} 
& $12\times12$  &9.97M&2381G&38.23&0.9615&34.00&0.9216&32.37&0.9023&32.99&0.9367&39.30&0.9786 \\
&  $16\times16$ &9.99M&2465G&38.25&0.9616&33.98&0.9209&32.38&0.9022&33.09&0.9371&39.42&0.9789  \\
&  $24\times24$ &10.06M&2703G&38.30&0.9618&34.08&0.9225&32.43&0.9030&33.38&0.9397&39.44&0.9786    \\ \midrule
\multirow{3}{*}{$\rm \textbf{\nameofmethod{}}$}
& $12\times12$  &10.31M&2419G&38.22&0.9614&34.08&0.9220&32.38&0.9025&33.08&0.9372&39.13&0.9780 \\
&  $16\times16$ &10.33M&2502G&38.31&0.9617&34.10&0.9217&32.43&0.9026&33.26&0.9385&39.36&0.9785 \\
&  $24\times24$  &10.40M&2741G&38.33&0.9618&34.13&0.9228&32.44&0.9030&33.51&0.9405&39.49&0.9788    \\
 \bottomrule % Swin-B-22k
%&  \highlight{0.884} & \highlight{0.936}& 0.830&\highlight{0.033}&\highlight{0.856}&\highlight{0.926}&0.758&\highlight{0.025}&\highlight{0.873}&\highlight{0.931}&\highlight{0.823}&\highlight{0.044} \\ \midrule % Swin-B-22k
\end{tabular}
\vspace{-8pt}
\caption{
% Comparison of our method with other \sArt methods on three benchmarks in terms of S-measure, adaptive E-measure, weight F-measure and mean absolute error. 
% The best two results are highlighted in {\color{BrickRed}{red}}.
%  '-P': PVTv2~\cite{wang2022pvt}, '-S': Swin-Transformer~\cite{liu2021swin}. 
%%DP Fan
 Ablation study on the window size. We report results on the original SwinIR, \nameofmethod{} without ConvFFN, and our full \nameofmethod{}. Note that the parameters and MACs of \nameofmethod{} with $24\times24$ window are fewer than SwinIR with $8\times8$ window. Larger windows can result in better performance.
}
\label{tab:window_size}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ablation window size
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ablation mlp size
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}[tp]
\setlength\tabcolsep{5.7pt}
\renewcommand{\arraystretch}{0.9}
% \scriptsize
\small
\centering
\setlength{\belowcaptionskip}{0cm}   
% \resizebox{\linewidth}{!}{
\begin{tabular}{cccccccccccccc}
\toprule
 \multirow{2}{*}{ConvFFN}  &  \multicolumn{2}{l}{\makebox[0.10\textwidth][c]{Urban100~\cite{huang2015single}}} & \multicolumn{2}{l}{\makebox[0.10\textwidth][c]{Manga109~\cite{matsui2017sketch}}} \\ 
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
& \makecell{PSNR}  &\makecell{SSIM} &\makecell{PSNR}  &\makecell{SSIM}  \\ \midrule
  w/o Depth-wise Conv &33.38&0.9397&39.44&0.9786\\
 $3\times3$ Depth-wise Conv &33.42&0.9398&39.34&0.9787\\
$5\times5$ Depth-wise Conv &\highlight{33.51}&\highlight{0.9405}&\highlight{39.49}&\highlight{0.9788}\\
 \bottomrule % Swin-B-22k
%&  \highlight{0.884} & \highlight{0.936}& 0.830&\highlight{0.033}&\highlight{0.856}&\highlight{0.926}&0.758&\highlight{0.025}&\highlight{0.873}&\highlight{0.931}&\highlight{0.823}&\highlight{0.044} \\ \midrule % Swin-B-22k
\end{tabular}
\vspace{-8pt}
\caption{
% Comparison of our method with other \sArt methods on three benchmarks in terms of S-measure, adaptive E-measure, weight F-measure and mean absolute error. 
% The best two results are highlighted in {\color{BrickRed}{red}}.
%  '-P': PVTv2~\cite{wang2022pvt}, '-S': Swin-Transformer~\cite{liu2021swin}. 
%%DP Fan
 Ablation study on ConvFFN for $\times$2 SR. From the results on Urban100 and Manga109, we can see that using $5\times5$ depthwise convolution yields the best results. This indicates that local details are also essential for Transformer-based models.
}
\label{tab:depth-wise}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ablation large-windows  Large-Window Attention Variants
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[tp]
\setlength\tabcolsep{4.5pt}
\renewcommand{\arraystretch}{0.9}
% \scriptsize
\small
\centering
\setlength{\belowcaptionskip}{0cm}   
% \resizebox{\linewidth}{!}{
\begin{tabular}{ccccccc}
\toprule
Method  & Params & MACs & $S$ & $r$ & PSNR & SSIM  \\ 
\midrule
% & & &\makecell{PSNR}  &\makecell{SSIM} &\makecell{PSNR}  &\makecell{SSIM}    \\ \midrule
 SwinIR~\cite{liang2021swinir}& 11.75M&2868G& 8 &	- &33.09& 	0.9373
\\ \midrule
Token Reduction & 11,78M & 2471G & 16 & 2 &	33.09  &	0.9372     \\
Token Reduction & 11.85M & 2709G & 24 & 2 &	33.24  &	0.9387     \\  \midrule
% \makecell{\rm \textbf{Bottleneck} \\ \textbf{Attention}}& 11.08M &	2695K  & 32.84 & 0.935 & 39.32 & 0.9783   \\\midrule
Token Sampling &11.91M&2465G& 16 & 2 & 32.38&0.9312   \\
Token Sampling &12.18M&2703G& 24 & 2 & 32.34&0.9305   \\  \midrule
PSA & 9.99M &	2465G	& 16 & 2 & 33.09 &	0.9371	\\
PSA & 10.06M&	2703G	& 24 & 2 &33.38 &	0.9397	\\ \bottomrule % Swin-B-22k
%&  \highlight{0.884} & \highlight{0.936}& 0.830&\highlight{0.033}&\highlight{0.856}&\highlight{0.926}&0.758&\highlight{0.025}&\highlight{0.873}&\highlight{0.931}&\highlight{0.823}&\highlight{0.044} \\ \midrule % Swin-B-22k
\end{tabular}
\vspace{-8pt}
\caption{
% Comparison of our method with other \sArt methods on three benchmarks in terms of S-measure, adaptive E-measure, weight F-measure and mean absolute error. 
% The best two results are highlighted in {\color{BrickRed}{red}}.
%  '-P': PVTv2~\cite{wang2022pvt}, '-S': Swin-Transformer~\cite{liu2021swin}. 
%%DP Fan
 $\times2$ SR performance comparison among SwinIR~\cite{liang2021swinir}, our proposed PSA, and the two variants on Urban100~\cite{huang2015single}. The results reported here are based on the best model trained on DIV2K for 200k iterations. For token sampling, $r=S/T$. PSA performs better than another two variants.
}
\vspace{-5pt}
\label{tab:large_window_variant}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ablation mlp size
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%To demonstrate our model's scalability, We  train \nameofmethod{-light} and compare it with a list of state-of-the-art light-weight SR methods: CARN~\cite{ahn2018fast}, IMDN~\cite{hui2019lightweight}, LAPAR-A~\cite{li2020lapar}, LatticeNet~\cite{luo2020latticenet}, ESRT~\cite{lu2021efficient}, SwinIR-light~\cite{liang2021swinir} and ELAN~\cite{zhang2022efficient}. The quantitative comparisons of Lightweight image SR is shown on table 3.

\section{Experiments}
In this section, we conduct experiments on both the classical, lightweight, and real-world image SR tasks, compare our \nameofmethod{} with existing state-of-the-art methods, and do ablation analysis of the proposed method.

\subsection{Experimental Setup}
\myPara{Datasets and Evaluation.}
The choice of training datasets keeps the same as the comparison models. In classical image SR, we use DIV2K~\cite{lim2017enhanced} and DF2K (DIV2K~\cite{lim2017enhanced} + Flickr2K~\cite{timofte2017ntire}) to train two versions \nameofmethod{}. 
%
In lightweight image SR, we use DIV2K~\cite{lim2017enhanced} to train our \nameofmethod{}-light.
%
In real-world SR, We use DF2K and OST~\cite{wang2018recovering}. 
%
For testing, we mainly evaluate our method on five benchmark datasets, including Set5~\cite{bevilacqua2012low}, Set14~\cite{zeyde2010single}, BSD100~\cite{martin2001database}, Urban100~\cite{huang2015single}, and Manga109~\cite{matsui2017sketch}. Self-ensemble strategy is introduced to further improve performance, named \nameofmethod{+}.
The experimental results are evaluated in terms of PSNR and SSIM values, which are calculated on the Y channel from the YCbCr space.


\myPara{Implementation Details.} In the classical image SR task, we set the PAB group number, PAB number, channel number, and attention head number to 6, 6, 180, and 6, respectively.
%
% The window size $S$ and reduction factor $r$ are set to 24 and 2. 
%
When training on DIV2K~\cite{lim2017enhanced}, the patch size, window size $S$, and reduction factor $r$ are set to $48\times48$, 24, and 2, respectively. 
%
When training on DF2K~\cite{lim2017enhanced,timofte2017ntire}, they are $64\times64$, 22, and 2, respectively.
%
For the lightweight image SR task, we set the PAB group number, PAB number, channel number, windows size $S$, reduction factor $r$, and attention head number to 4, 6, 60, 16,  2, and 6, respectively.
%
The training patch size we use is $64\times 64$. 
%
% For a fair comparison, we use the same training settings as SwinIR without any tricks.
We randomly rotate images by $90^{\circ}$, $180^{\circ}$, or $270^{\circ}$ and randomly flip images horizontally for data augmentation.
We adopt the Adam~\cite{kingma2014adam} optimizer with $\beta_1 = 0.9$ and  $\beta_2 = 0.99$ to train the model for 500k iterations.  The initial learning rate is set as $2\times10^{-4}$ and is reduced by half at the $\rm \left\{{250k,400k,450k,475k}\right\}$-th iterations.

\subsection{Ablation Study}

\myPara{Impact of window size in PSA.} Permuted self-attention provides an efficient and effective way to enlarge window size.
%
To investigate the impact of different window sizes on model performance, we conduct three group experiments and report in Table \ref{tab:window_size}.
%
The first group is the vanilla SwinIR~\cite{liang2021swinir} with $8 \times 8$, $12\times12$, and $16\times16$ window sizes.
%
In the second group, we do not use the ConvFFN but only the PSA in our \nameofmethod{} and set the window size to $12\times12$, $16\times16$, and $24\times24$, respectively, to observe the performance difference.
%
In the third group, we use our full \nameofmethod{} with $12\times12$, $16\times16$, and $24\times24$ as window size to explore the performance change.
%
The  results show that a larger window size yields better performance improvement for all three groups of experiments.
%
% PSA can significantly improve the performance of SwinIR, and reduce its parameters and Flops.
%
In addition, the parameters and MACs of our \nameofmethod{} with $24\times24$ window  are even fewer than the original SwinIR with $8\times8$ window.
%
To balance the performance and MACs, we set window size as $24\times24$  in \nameofmethod{} and $16\times16$  in \nameofmethod{-light}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!htbp]
% 	\captionsetup{font=small}
	\centering
	\scriptsize
	
	\newcommand{\h}{0.105}
	\newcommand{\wa}{0.12}
	\newcommand{\wb}{0.16}
	\newcommand{\g}{-0.7mm}
	\renewcommand{\tabcolsep}{1.8pt}
	\renewcommand{\arraystretch}{1}
	\resizebox{1.00\linewidth}{!} {
 		\begin{tabular}{c}
				\normalsize
			\newcommand{\name}{figures/classicSR/urban73/}
			\renewcommand{\h}{0.086}
			\newcommand{\w}{0.180}
			\begin{tabular}{cc}
			\normalsize
				\begin{adjustbox}{valign=t}
					\begin{tabular}{c}
						\includegraphics[height=0.205\textwidth, width=0.374\textwidth]{\name rectangle.png}
						\\
						Urban100 ($4\times$): img\_073
					\end{tabular}
				\end{adjustbox}
				\begin{adjustbox}{valign=t}
					\begin{tabular}{ccccc}
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name HR} \hspace{\g} &
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name Bicuble} \hspace{\g} &
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name EDSR} \hspace{\g} &
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name RCAN} \hspace{\g} &
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name RDN} \hspace{\g} 
						\\
						HR \hspace{\g} &
						Bicubic \hspace{\g} &
						EDSR~\cite{lim2017enhanced} \hspace{\g} &
						RCAN~\cite{zhang2018image} \hspace{\g} &
						RDN~\cite{zhang2018residual} \hspace{\g}
						\\
						\vspace{-4.0mm}
						\\
						
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name IGNN} \hspace{\g} &
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name NLSA} \hspace{\g} &
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name ipt} \hspace{\g} &
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name DF2K_s64w8_SwinIR}
						\hspace{\g} &		
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name SRformer} \hspace{\g} 
						\\ 
						
						IGNN~\cite{zhou2020cross}  \hspace{\g} &
						NLSA~\cite{zhou2020cross}  \hspace{\g} &
						IPT~\cite{chen2021pre} \hspace{\g} &
						SwinIR~\cite{liang2021swinir}
						& \textbf{\nameofmethod{}} (ours)
						\\
					\end{tabular}
				\end{adjustbox}
			\end{tabular} \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\newcommand{\name}{figures/classicSR/urban92/}
			\renewcommand{\h}{0.086}
			\newcommand{\w}{0.180}
			\begin{tabular}{cc}
			\normalsize
				\begin{adjustbox}{valign=t}
					\begin{tabular}{c}
						\includegraphics[height=0.205\textwidth, width=0.374\textwidth]{\name rectangle.png}
						\\
						Urban100 ($4\times$): img\_092
					\end{tabular}
				\end{adjustbox}
				\begin{adjustbox}{valign=t}
					\begin{tabular}{ccccc}
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name HR} \hspace{\g} &
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name Bicuble} \hspace{\g} &
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name EDSR} \hspace{\g} &
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name RCAN} \hspace{\g} &
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name RDN} \hspace{\g} 
						\\
						HR \hspace{\g} &
						Bicubic \hspace{\g} &
						EDSR~\cite{lim2017enhanced} \hspace{\g} &
						RCAN~\cite{zhang2018image} \hspace{\g} &
						RDN~\cite{zhang2018residual} \hspace{\g}
						\\
						\vspace{-4.0mm}
						\\
						
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name IGNN} \hspace{\g} &
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name NLSA} \hspace{\g} &
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name ipt} \hspace{\g} &
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name DF2K_s64w8_SwinIR}
						\hspace{\g} &		
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name SRformer} \hspace{\g} 
						\\ 
						
						IGNN~\cite{zhou2020cross}  \hspace{\g} &
						NLSA~\cite{zhou2020cross}  \hspace{\g} &
						IPT~\cite{chen2021pre} \hspace{\g} &
						SwinIR~\cite{liang2021swinir}
						& \textbf{\nameofmethod{}} (ours)
						\\
					\end{tabular}
				\end{adjustbox}
			\end{tabular} \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			% \newcommand{\name}{figures/classicSR/manga100/}
			% \renewcommand{\h}{0.086}
			% \newcommand{\w}{0.180}
			% \begin{tabular}{cc}
			% \normalsize
			% 	\begin{adjustbox}{valign=t}
			% 		\begin{tabular}{c}
			% 			\includegraphics[height=0.205\textwidth, width=0.374\textwidth]{\name rectangle.png}
			% 			\\
			% 			Manga109 ($4\times$): UltraEleven
			% 		\end{tabular}
			% 	\end{adjustbox}
			% 	\begin{adjustbox}{valign=t}
			% 		\begin{tabular}{cccccc}
			% 			\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name HR} \hspace{\g} &
			% 			\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name Bicuble} \hspace{\g} &
			% 			\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name EDSR} \hspace{\g} &
			% 			\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name RCAN} \hspace{\g} &
			% 			\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name RDN} \hspace{\g} 
			% 			\\
			% 			HR \hspace{\g} &
			% 			Bicubic \hspace{\g} &
			% 			EDSR~\cite{lim2017enhanced} \hspace{\g} &
			% 			RCAN~\cite{zhang2018image} \hspace{\g} &
			% 			RDN~\cite{zhang2018residual} \hspace{\g}
			% 			\\
			% 			\vspace{-4.0mm}
			% 			\\
						
			% 			\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name IGNN} \hspace{\g} &
			% 			\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name NLSA} \hspace{\g} &
			% 			\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name ipt} \hspace{\g} &
			% 			\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name DF2K_s64w8_SwinIR}
			% 			\hspace{\g} &		
			% 			\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name SRformer} \hspace{\g} 
			% 			\\ 
						
			% 			IGNN~\cite{zhou2020cross}  \hspace{\g} &
			% 			NLSA~\cite{zhou2020cross}  \hspace{\g} &
			% 			IPT~\cite{chen2021pre} \hspace{\g} &
			% 			SwinIR~\cite{liang2021swinir}
			% 			& \textbf{\nameofmethod{}} (ours)
			% 			\\
			% 		\end{tabular}
			% 	\end{adjustbox}
			% \end{tabular}
 		 \end{tabular}
	}\vspace{-2mm}
	\caption{Qualitative comparison with recent state-of-the-art  \textbf{classical image SR}  methods on the $\times 4$ SR task. }
	\label{fig:sr_visual}
	\vspace{-10pt}
\end{figure*}

\myPara{Impact of kernel size of ConvFFN.}
We introduce ConvFFN in \secref{sec:PSA}, which aims to encode more local information without increasing too many computations.
%
In order to explore which kernel size can bring the best performance improvement, 
we attempt to use $3\times3$ depth-wise convolution and $5\times5$ depth-wise convolution and report the results in Table \ref{tab:depth-wise}. 
%
Given that the depth-wise convolution has little effect on the number of parameters and MACs, we do not list them in the table.
%
Obviously, $5\times5$ depth-wise convolution leads to the best results.
%
Thus, we use $5\times5$ depth-wise convolution in our ConvFFN.

\myPara{Large-window self-attention variants.} In \secref{sec:variants}, we introduce another two large-window self-attention variants.
%
We summarize the results in Table~\ref{tab:large_window_variant}.
%
Though token reduction can slightly improve SwinIR when using a large window,
the number of parameters does not decrease  and the performance gain is lower than ours. We argue that it is because directly applying downsampling operations to the key and value results in spatial information loss.
%
For token sampling, the performance is even worse than the original SwinIR.
%
We believe the reason is that dropping out some tokens severely breaks the image content structure.


% We now give the quantitative comparison of large-window attention variants in Table~\ref{tab:large_window_variant}.
%
% We choose SwinIR as backbone and replace W-MSA with the attention variants mention above. 
%
% Among five models , permuted self-attention achieve the best performance on all five testsets with the smallest  parameters and the third smallest Flops.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% classical weight
%In classical image SR task,we compare our \nameofmethod{} with  EDSR \cite{lim2017enhanced}, RCAN \cite{zhang2018image}, RDN\cite{zhang2018residual},SRFBN\cite{li2019feedback}, RRDB\cite{wang2018esrgan}, DBPN\cite{haris2018deep},SAN\cite{dai2019second}, IGNN\cite{zhou2020cross}, HAN\cite{niu2020single}, NLSA\cite{mei2021image} ,SwinIR\cite{liang2021swinir} and ELAN\cite{zhang2022efficient}, The quantitative comparisons of classical image SR is shown on table 2.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[tp]
\setlength\tabcolsep{8.5pt}
\renewcommand{\arraystretch}{0.84}
\scriptsize
% \footnotesize
% \tiny
\centering
\setlength{\belowcaptionskip}{0cm}   
% \resizebox{\linewidth}{!}{
\begin{tabular}{crccccccccccc}
\toprule
& \multirow{2}{*}{Method}  & \multirow{2}{*}{\makecell{Training \\ Dataset}} &  \multicolumn{2}{c}{  SET5~\cite{bevilacqua2012low} } & \multicolumn{2}{c}{SET14~\cite{zeyde2010single}} & \multicolumn{2}{c}{ B100~\cite{martin2001database}} & \multicolumn{2}{c}{Urban100~\cite{huang2015single}} & \multicolumn{2}{c}{Manga109~\cite{matsui2017sketch}} \\ 
\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11} \cmidrule(lr){12-13} 
    & & &\makecell{PSNR}  &\makecell{SSIM} &\makecell{PSNR}  &\makecell{SSIM}&\makecell{PSNR}  &\makecell{SSIM}  &\makecell{PSNR}  &\makecell{SSIM}&\makecell{PSNR}  &\makecell{SSIM}     \\ \midrule
% \multicolumn{11}{l}{\emph{$\times 2$ SR}}\\ \midrule
\multirow{11}{*}{\rotatebox{90}{$\times 2$ SR}} 
& $\rm \textbf {EDSR}$~\cite{lim2017enhanced}& DIV2K  &38.11&0.9602&33.92&0.9195&32.32&0.9013&32.93&0.9351&39.10&0.9773   \\
& $\rm \textbf{RCAN}$~\cite{zhang2018image} & DIV2K  & 38.27&0.9614&34.12&0.9216&32.41&0.9027&33.34&0.9384&39.44&0.9786   \\
% & $\rm \textbf{RDN}$~\cite{zhang2018residual}& DIV2K &38.24&0.9614&34.01&0.9212&32.34&0.9017&32.89&0.9353&39.18&0.9780\\
% & $\rm \textbf{SRFBN}$~\cite{li2019feedback}& DIV2K & & &38.11&0.9609&33.82&0.9196&32.29&0.9010&32.62&0.9328&39.08&0.9779\\
& $\rm \textbf{SAN}$~\cite{dai2019second} & DIV2K &38.31&0.9620&34.07&0.9213&32.42&0.9028&33.10&0.9370&39.32&0.9792    \\
& $\rm \textbf{IGNN}$~\cite{zhou2020cross}& DIV2K &38.24&0.9613&34.07&0.9217&32.41&0.9025&33.23&0.9383&39.35&0.9786\\
& $\rm \textbf{HAN}$~\cite{niu2020single}& DIV2K &38.27&0.9614&34.16&0.9217&32.41&0.9027&33.35&0.9385&39.46&0.9785\\
& $\rm \textbf{NLSA}$~\cite{mei2021image}& DIV2K  &38.34&0.9618&34.08&0.9231&32.43&0.9027&33.42&0.9394&39.59&0.9789   \\
& $\rm \textbf{SwinIR}$~\cite{liang2021swinir}& DIV2K  &38.35&0.9620&34.14&0.9227&32.44&0.9030&33.40&0.9393&39.60&0.9792 \\
& $\rm \textbf{ELAN}$~\cite{zhang2022efficient}& DIV2K &38.36&0.9620&34.20&0.9228&32.45&0.9030&33.44&0.9391&39.62&\highlight{0.9793} \\ 
& \textbf{\nameofmethod{ (ours)}}& DIV2K & \highlight{38.45}&\highlight{0.9622}&\highlight{34.21}&\highlight{0.9236}&\highlight{32.51}&\highlight{0.9038}&\highlight{33.86}&\highlight{0.9426}&\highlight{39.69}&0.9786 \\ 
 \cmidrule(lr){2-13}
% & $\rm \textbf{SRFBN}$~\cite{li2019feedback}& DF2K &38.11&0.9609&33.82&0.9196&32.29&0.9010&32.62&0.9328&39.08&0.9779\\
& $\rm \textbf{IPT}$~\cite{chen2021pre}& ImageNet  &38.37&-&34.43&-&32.48&-&33.76&-&-&-\\
& $\rm \textbf{SwinIR}$~\cite{liang2021swinir}& DF2K &38.42&0.9623&34.46&0.9250&32.53&0.9041&33.81&0.9427&39.92&0.9797\\
& $\rm \textbf{EDT}$~\cite{li2021efficient}& DF2K &38.45&0.9624&\underline{34.57}&\underline{0.9258}&32.52&0.9041&33.80&0.9425&39.93&0.9800\\
& \textbf{\nameofmethod{ (ours)}} & DF2K & \underline{38.51}&\underline{0.9627}&34.44&0.9253&\underline{32.57}&\underline{0.9046}&\underline{34.09}&\underline{0.9449}&\underline{40.07}&\underline{0.9802} \\ 
& \textbf{\nameofmethod{+ (ours)}} & DF2K & \highlight{38.58}&\highlight{0.9628}&\highlight{34.60}&\highlight{0.9262}&\highlight{32.61}&\highlight{0.9050}&\highlight{34.29}&\highlight{0.9457}&\highlight{40.19}&\highlight{0.9805} \\ \midrule%è°ƒå¥½äº†æ”¾ä¸Šæ¥ä¸ç„¶å°±æ”¹æˆåŠ å¤šå…¶ä»–Transformeræ–¹æ³•
%%%%%%%%%%%%%%%%%%%%%x3 SR
% \multicolumn{11}{l}{\emph{$\times 3$ SR}}\\ \midrule
\multirow{11}{*}{\rotatebox{90}{$\times 3$ SR}} 
& $ \rm \textbf {EDSR}$~\cite{lim2017enhanced}& DIV2K  &34.65&0.9280&30.52&0.8462&29.25&0.8093&28.80&0.8653&34.17&0.9476  \\
& $\rm \textbf{RCAN}$~\cite{zhang2018image} & DIV2K &34.74&0.9299&30.65&0.8482&29.32&0.8111&29.09&0.8702&34.44&0.9499 \\
% &  $\rm \textbf{RDN}$~\cite{zhang2018residual}& DIV2K  &34.71&0.9296&30.57&0.8468&29.26&0.8093&28.80&0.8653&34.13&0.9484\\
% & $\rm \textbf{SRFBN}$~\cite{li2019feedback}& DIV2K & & &34.70&0.9292&30.51&0.8461&29.24&0.8084&28.73&0.8641&34.18&0.9481\\
& $\rm \textbf{SAN}$~\cite{dai2019second} & DIV2K  &34.75&0.9300&30.59&0.8476&29.33&0.8112&28.93&0.8671&34.30&0.9494   \\
& $\rm \textbf{IGNN}$~\cite{zhou2020cross}& DIV2K  &34.72&0.9298&30.66&0.8484&29.31&0.8105&29.03&0.8696&34.39&0.9496\\
& $\rm \textbf{HAN}$~\cite{niu2020single}& DIV2K &34.75&0.9299&30.67&0.8483&29.32&0.8110&29.10&0.8705&34.48&0.9500\\
& $\rm \textbf{NLSA}$~\cite{mei2021image}& DIV2K  &34.85&0.9306&30.70&0.8485&29.34&0.8117&29.25&0.8726&34.57&0.9508   \\
& $\rm \textbf{SwinIR}$~\cite{liang2021swinir}& DIV2K &34.89&0.9312&30.77&0.8503&29.37&0.8124&29.29&0.8744&34.74&0.9518 \\
& $\rm \textbf{ELAN}$~\cite{zhang2022efficient}& DIV2K &34.90&0.9313&30.80&0.8504&29.38&0.8124&29.32&0.8745&34.73&0.9517 \\ 
& \textbf{\nameofmethod{ (ours)}}& DIV2K &  \highlight{34.94}&\highlight{0.9318}&\highlight{30.81}&\highlight{0.8518}&\highlight{29.41}&\highlight{0.8142}&\highlight{29.52}&\highlight{0.8786}&\highlight{34.78}&\highlight{0.9524} \\  \cmidrule(lr){2-13}
% & $\rm \textbf{SRFBN}$~\cite{li2019feedback}& DF2K &34.70&0.9292&30.51&0.8461&29.24&0.8084&28.73&0.8641&34.18&0.9481\\
& $\rm \textbf{IPT}$~\cite{chen2021pre}& ImageNet &34.81&-&30.85&-&29.38&-&29.49&-&-&-\\
& $\rm \textbf{SwinIR}$~\cite{liang2021swinir}& DF2K  &34.97&0.9318&30.93&0.8534&29.46&0.8145&29.75&0.8826&35.12&0.9537\\
& $\rm \textbf{EDT}$~\cite{li2021efficient}& DF2K  &34.97&0.9316&30.89&0.8527&29.44&0.8142&29.72&0.8814&35.13&0.9534\\
& \textbf{\nameofmethod{ (ours)}} & DF2K  &  \underline{35.02}&\underline{0.9323}&\underline{30.94}&\underline{0.8540}&\underline{29.48}&\underline{0.8156}&\underline{30.04}&\underline{0.8865}&\underline{35.26}&\underline{0.9543}\\
& \textbf{\nameofmethod{+ (ours)}} & DF2K  &  \highlight{35.08}&\highlight{0.9327}&\highlight{31.04}&\highlight{0.8551}&\highlight{29.53}&\highlight{0.8162}&\highlight{30.21}&\highlight{0.8884}&\highlight{35.45}&\highlight{0.9550}\\ \midrule%è°ƒå¥½äº†æ”¾ä¸Šæ¥ä¸ç„¶å°±æ”¹æˆåŠ å¤šå…¶ä»–Transformeræ–¹æ³•
%%%%%%%%%%%%%%%%%%%%%x4 SR
% \multicolumn{11}{l}{\emph{$\times 4$ SR}}\\ \midrule
\multirow{11}{*}{\rotatebox{90}{$\times 4$ SR}} 
& $ \rm \textbf {EDSR}$~\cite{lim2017enhanced}& DIV2K  &32.46&0.8968&28.80&0.7876&27.71&0.7420&26.64&0.8033&31.02&0.9148 \\
& $\rm \textbf{RCAN}$~\cite{zhang2018image}& DIV2K  &32.63&0.9002&28.87&0.7889&27.77&0.7436&26.82&0.8087&31.22&0.9173 \\
% & $\rm \textbf{RDN}$~\cite{zhang2018residual}& DIV2K  &32.47&0.8990&28.81&0.7871&27.72&0.7419&26.61&0.8028&31.00&0.9151\\
% & $\rm \textbf{SRFBN}_{2019}$~\cite{li2019feedback}& DIV2K & & &32.47&0.8983&28.81&0.7868&27.72&0.7409&26.60&0.8015&31.15&0.9160\\
& $\rm \textbf{SAN}$~\cite{dai2019second} & DIV2K  &32.64&0.9003&28.92&0.7888&27.78&0.7436&26.79&0.8068&31.18&0.9169   \\
& $\rm \textbf{IGNN}$~\cite{zhou2020cross}& DIV2K  &32.57&0.8998&28.85&0.7891&27.77&0.7434&26.84&0.8090&31.28&0.9182\\
& $\rm \textbf{HAN}$~\cite{niu2020single}& DIV2K  &32.64&0.9002&28.90&0.7890&27.80&0.7442&26.85&0.8094&31.42&0.9177\\
& $\rm \textbf{NLSA}$~\cite{mei2021image}& DIV2K &32.59&0.9000&28.87&0.7891&27.78&0.7444&26.96&0.8109&31.27&0.9184   \\
& $\rm \textbf{SwinIR}$~\cite{liang2021swinir}& DIV2K &32.72&0.9021&28.94&0.7914&27.83&0.7459&27.07&0.8164&31.67&0.9226 \\
& $\rm \textbf{ELAN}$~\cite{zhang2022efficient}& DIV2K &32.75&0.9022&28.96&0.7914&27.83&0.7459&27.13&0.8167&31.68&0.9226 \\ 
& \textbf{\nameofmethod{ (ours)}}& DIV2K & \highlight{32.81}&\highlight{0.9029}&\highlight{29.01}&\highlight{0.7919}&\highlight{27.85}&\highlight{0.7472}&\highlight{27.20}&\highlight{0.8189}&\highlight{31.75}&\highlight{0.9237}\\  \cmidrule(lr){2-13}
% & $\rm \textbf{DBPN}_{2018}$~\cite{haris2018deep}& DF2K & & &32.47&0.8980&28.82&0.7860&27.72&0.7400&26.38&0.7946&30.91&0.9137\\
% & $\rm \textbf{SRFBN}$~\cite{li2019feedback}& DF2K &32.47&0.8983&28.81&0.7868&27.72&0.7409&26.60&0.8015&31.15&0.9160\\
& $\rm \textbf{IPT}$~\cite{chen2021pre}& ImageNet &32.64&-&29.01&-&27.82&-&27.26&-&-&-\\
& $\rm \textbf{SwinIR}$~\cite{liang2021swinir}& DF2K &32.92&\underline{0.9044}&\underline{29.09}&0.7950&27.92&0.7489&27.45&0.8254&32.03&0.9260\\
& $\rm \textbf{EDT}$~\cite{li2021efficient}& DF2K &32.82&0.9031&\underline{29.09}&0.7939&27.91&0.7483&27.46&0.8246&32.03&0.9254\\
&\textbf{\nameofmethod{ (ours)}}& DF2K & \underline{32.93}&0.9041&29.08&\underline{0.7953}&\underline{27.94}&\underline{0.7502}&\underline{27.68}&\underline{0.8311}&\underline{32.21}&\underline{0.9271}\\
&\textbf{\nameofmethod{+ (ours)}}& DF2K & \highlight{33.09}&\highlight{0.9053}&\highlight{29.19}&\highlight{0.7965}&\highlight{28.00}&\highlight{0.7511}&\highlight{27.85}&\highlight{0.8338}&\highlight{32.44}&\highlight{0.9287}\\
\bottomrule % Swin-B-22k
%&  \highlight{0.884} & \highlight{0.936}& 0.830&\highlight{0.033}&\highlight{0.856}&\highlight{0.926}&0.758&\highlight{0.025}&\highlight{0.873}&\highlight{0.931}&\highlight{0.823}&\highlight{0.044} \\ \midrule % Swin-B-22k
\end{tabular}
\vspace{-8pt}
\caption{
% Comparison of our method with other \sArt methods on three benchmarks in terms of S-measure, adaptive E-measure, weight F-measure and mean absolute error. 
% The best two results are highlighted in {\color{BrickRed}{red}}.
%  '-P': PVTv2~\cite{wang2022pvt}, '-S': Swin-Transformer~\cite{liu2021swin}. 
%%DP Fan
Quantitative comparison of our \nameofmethod{} with recent \sArt \textbf{ {classical image SR}} methods  on five benchmark datasets. For a fair comparison, the parameters and MACs of \nameofmethod{} are  \textbf{lower than} SwinIR (See supplementary
material for details).
The best performance is highlighted and the second is underlined. 
}
\label{tab:classical_SR}
\vspace{-5pt}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% classical weight
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{table*}[t]
% 		\scriptsize
% 		%\small
% 		\vspace{-2mm}
% 		\begin{center}
% 			\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
% 				\hline
% 				Method & EDSR & RCAN & SRFBN & HAN & IGNN & NLSA & SwiIR & SRFormer (ours) \\
% 				\hline
% 				\hline
% 				Params (M) & 43.09 & 15.59 & 3.63 & 16.07 & 7.16 & 11.90 & 11.87 \\
% 				\hline
% 				Mult-Adds (G) & 1,286 & 407 & 498 & 420 & 103,640 & 336 & 392 \\
% 				\hline
% 				PSNR on Urban100 (dB) & 26.64 & 26.82 & 26.60 & 26.85 & 27.22 & 27.45 & 27.54 & 27.77\\
% 				\hline
% 				PSNR on Manga109 (dB) & 31.02 & 31.22 & 31.15 & 31.42 & 31.43 & 32.03 & 32.13 & 32.31\\
% 				\hline
% 			\end{tabular}
% 			\vspace{-2mm}
% 			\caption{Model size comparisons ($\times$4 SR). Output size is 3$\times$ 1080 $\times $720 for Mult-Adds calculation.}
% 			\label{table:model_size}
% 		\end{center}
% 		\vspace{-8mm}
% 	\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ablation window size
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{table*}[tp]
% \setlength\tabcolsep{3.7pt}
% \renewcommand{\arraystretch}{0.9}
% % \scriptsize
% \small
% \centering
% \setlength{\belowcaptionskip}{0cm}   
% % \resizebox{\linewidth}{!}{
% \begin{tabular}{lcccccccccccc}
% \toprule
% Scale   & \multicolumn{4}{l}{  \makebox[0.20\textwidth][c]{$\times2$} } & \multicolumn{4}{l}{  \makebox[0.20\textwidth][c]{$\times3$} } &\multicolumn{4}{l}{  \makebox[0.20\textwidth][c]{$\times4$} }\\\cmidrule(lr){2-5}\cmidrule(lr){6-9}\cmidrule(lr){10-13}
% Training Dataset&  \multicolumn{2}{l}{  \makebox[0.10\textwidth][c]{DIV2K} } & \multicolumn{2}{l}{\makebox[0.10\textwidth][c]{DF2K}} & \multicolumn{2}{l}{ \makebox[0.10\textwidth][c]{DIV2K}} & \multicolumn{2}{l}{\makebox[0.10\textwidth][c]{DF2K}} & \multicolumn{2}{l}{\makebox[0.10\textwidth][c]{DIV2K}} & \multicolumn{2}{l}{\makebox[0.10\textwidth][c]{DF2K}} \\ 
% \cmidrule(lr){2-3} \cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11} \cmidrule(lr){12-13} 
% Method   & \makecell{PSNR}  &\makecell{SSIM} &\makecell{PSNR}  &\makecell{SSIM}&\makecell{PSNR}  &\makecell{SSIM}  &\makecell{PSNR}  &\makecell{SSIM}&\makecell{PSNR}  &\makecell{SSIM}&\makecell{PSNR}  &\makecell{SSIM}     \\ \midrule
% $ \rm \textbf{SwinIR}$~\cite{liang2021swinir} &36.77&0.9500&36.85&0.9507&32.97&0.8965&33.10&0.8986&30.94&0.8492&31.08&0.8523\\
% $ \rm \textbf{\nameofmethod{}}$ & \textbf{36.88}&\textbf{0.9508}&\textbf{36.95}&\textbf{0.9512}&\textbf{33.03}&\textbf{0.8979}&\textbf{33.18}&\textbf{0.8994}&\textbf{30.97}&\textbf{0.8503}&\textbf{31.16}&\textbf{0.8535}\\
%  \bottomrule % Swin-B-22k
% %&  \highlight{0.884} & \highlight{0.936}& 0.830&\highlight{0.033}&\highlight{0.856}&\highlight{0.926}&0.758&\highlight{0.025}&\highlight{0.873}&\highlight{0.931}&\highlight{0.823}&\highlight{0.044} \\ \midrule % Swin-B-22k
% \end{tabular}
% \vspace{-8pt}
% \caption{
% % Comparison of our method with other \sArt methods on three benchmarks in terms of S-measure, adaptive E-measure, weight F-measure and mean absolute error. 
% % The best two results are highlighted in {\color{BrickRed}{red}}.
% %  '-P': PVTv2~\cite{wang2022pvt}, '-S': Swin-Transformer~\cite{liu2021swin}. 
% %%DP Fan
%  Ablation study on the window size. We report results on the original SwinIR, \nameofmethod{} without ConvFFN, and our full \nameofmethod{}. Note that the parameters and FLOPs of \nameofmethod{} with $24\times24$ window are less than SwinIR with $8\times8$ window.
% }
% \label{tab:main_performance}
% \end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ablation window size
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % ablation window size
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{table*}[tp]
% \setlength\tabcolsep{3.7pt}
% \renewcommand{\arraystretch}{0.9}
% % \scriptsize
% \small
% \centering
% \setlength{\belowcaptionskip}{0cm}   
% % \resizebox{\linewidth}{!}{
% \begin{tabular}{lcccccccccccccc}
% \toprule
% Scale  & & & \multicolumn{4}{l}{  \makebox[0.20\textwidth][c]{$\times2$} } & \multicolumn{4}{l}{  \makebox[0.20\textwidth][c]{$\times3$} } &\multicolumn{4}{l}{  \makebox[0.20\textwidth][c]{$\times4$} }\\\cmidrule(lr){4-7}\cmidrule(lr){8-11}\cmidrule(lr){12-15}
% Training Dataset& && \multicolumn{2}{l}{  \makebox[0.10\textwidth][c]{DIV2K} } & \multicolumn{2}{l}{\makebox[0.10\textwidth][c]{DF2K}} & \multicolumn{2}{l}{ \makebox[0.10\textwidth][c]{DIV2K}} & \multicolumn{2}{l}{\makebox[0.10\textwidth][c]{DF2K}} & \multicolumn{2}{l}{\makebox[0.10\textwidth][c]{DIV2K}} & \multicolumn{2}{l}{\makebox[0.10\textwidth][c]{DIV2K}} \\ 
% \cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11} \cmidrule(lr){12-13} \cmidrule(lr){14-15}
% Method & Params & Flops & \makecell{PSNR}  &\makecell{SSIM} &\makecell{PSNR}  &\makecell{SSIM}&\makecell{PSNR}  &\makecell{SSIM}  &\makecell{PSNR}  &\makecell{SSIM}&\makecell{PSNR}  &\makecell{SSIM}&\makecell{PSNR}  &\makecell{SSIM}     \\ \midrule
% $ \rm \textbf{SwinIR}$~\cite{liang2021swinir}& & &36.77&0.9500&36.85&0.9507&32.97&0.8965&33.10&0.8986&30.94&0.8492&31.08&0.8523\\
% $ \rm \textbf{\nameofmethod{}}$& & & \textbf{36.88}&\textbf{0.9508}&\textbf{36.95}&\textbf{0.9512}&\textbf{33.03}&\textbf{0.8979}&\textbf{33.18}&\textbf{0.8994}&\textbf{30.97}&\textbf{0.8503}&\textbf{31.16}&\textbf{0.8535}\\
%  \bottomrule % Swin-B-22k
% %&  \highlight{0.884} & \highlight{0.936}& 0.830&\highlight{0.033}&\highlight{0.856}&\highlight{0.926}&0.758&\highlight{0.025}&\highlight{0.873}&\highlight{0.931}&\highlight{0.823}&\highlight{0.044} \\ \midrule % Swin-B-22k
% \end{tabular}
% \vspace{-8pt}
% \caption{
% % Comparison of our method with other \sArt methods on three benchmarks in terms of S-measure, adaptive E-measure, weight F-measure and mean absolute error. 
% % The best two results are highlighted in {\color{BrickRed}{red}}.
% %  '-P': PVTv2~\cite{wang2022pvt}, '-S': Swin-Transformer~\cite{liu2021swin}. 
% %%DP Fan
%  Ablation study on the window size. We report results on the original SwinIR, \nameofmethod{} without ConvFFN, and our full \nameofmethod{}. Note that the parameters and FLOPs of \nameofmethod{} with $24\times24$ window are less than SwinIR with $8\times8$ window.
% }
% \label{tab:main_performance}
% \end{table*}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % ablation window size
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% light weight
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[tp]
\setlength\tabcolsep{6pt}
\renewcommand{\arraystretch}{0.8}
 \scriptsize
%\footnotesize
\centering
% \setlength{\belowcaptionskip}{0cm}   
% \resizebox{\linewidth}{!}{
\begin{tabular}{crccccccccccccc}
\toprule
& \multirow{2}{*}{Method}  & \multirow{2}{*}{\makecell{Training \\ Dataset}} & \multirow{2}{*}{Params} & \multirow{2}{*}{MACs}  &  \multicolumn{2}{c}{  SET5~\cite{bevilacqua2012low} } & \multicolumn{2}{c}{SET14~\cite{zeyde2010single}} & \multicolumn{2}{c}{ B100~\cite{martin2001database}} & \multicolumn{2}{c}{Urban100~\cite{huang2015single}} & \multicolumn{2}{c}{Manga109~\cite{matsui2017sketch}} \\ 
\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11} \cmidrule(lr){12-13} \cmidrule(lr){14-15}
 & &   & & &\makecell{PSNR}  &\makecell{SSIM} &\makecell{PSNR}  &\makecell{SSIM}&\makecell{PSNR}  &\makecell{SSIM}  &\makecell{PSNR}  &\makecell{SSIM}&\makecell{PSNR}  &\makecell{SSIM}     \\ \midrule
% \multicolumn{11}{l}{\emph{$\times 2$ SR}}\\ \midrule
\multirow{9}{*}{\rotatebox{90}{$\times 2$ SR}} 
& $\rm \textbf{EDSR-baseline}$~\cite{lim2017enhanced} & DIV2K & 1370K &316G &37.99&0.9604&33.57&0.9175&32.16&0.8994&31.98&0.9272&38.54&0.9769 \\
& $\rm \textbf{CARN}$~\cite{ahn2018fast}& DIV2K & 1592K & 222.8G &37.76&0.9590&33.52&0.9166&32.09&0.8978&31.92&0.9256&38.36&0.9765 \\
& $\rm \textbf{IMDN}$~\cite{hui2019lightweight}& DIV2K & 694K & 158.8G &38.00&0.9605&33.63&0.9177&32.19&0.8996&32.17&0.9283&38.88&0.9774\\
& $\rm \textbf{LAPAR-A}$~\cite{li2020lapar}& DF2K & 548K  & 171G &38.01&0.9605&33.62&0.9183&32.19&0.8999&32.10&0.9283&38.67&0.9772\\
& $\rm \textbf{LatticeNet}$~\cite{luo2020latticenet}& DIV2K & 756K & 169.5G &38.15&0.9610&33.78&0.9193&32.25&0.9005&32.43&0.9302&-&- \\
& $\rm \textbf{ESRT}$~\cite{lu2021efficient}& DIV2K & 751K & - &38.03&0.9600&33.75&0.9184&32.25&0.9001&32.58&0.9318&39.12&0.9774\\
& $\rm \textbf{SwinIR-light}$~\cite{liang2021swinir}& DIV2K &910K & 244G  &38.14&0.9611&33.86&0.9206&32.31&0.9012&32.76&0.9340&39.12&0.9783 \\
& $\rm \textbf{ELAN}$~\cite{zhang2022efficient}& DIV2K &621K &203G &38.17&0.9611&\highlight{33.94}&0.9207&32.30&0.9012&32.76&0.9340&39.11&0.9782 \\ 
& \textbf{\nameofmethod{-light}} & DIV2K & 853K & 236G & \highlight{38.23}&\highlight{0.9613}&\highlight{33.94}&\highlight{0.9209}&\highlight{32.36}&\highlight{0.9019}&\highlight{32.91}&\highlight{0.9353}&\highlight{39.28}&\highlight{0.9785} \\ \midrule%è°ƒå¥½äº†æ”¾ä¸Šæ¥ä¸ç„¶å°±æ”¹æˆåŠ å¤šå…¶ä»–Transformeræ–¹æ³•
%%%%%%%%%%%%%%%%%%%%%x3 SR
% \multicolumn{11}{l}{\emph{$\times 3$ SR}}\\ \midrule
\multirow{9}{*}{\rotatebox{90}{$\times 3$ SR}} 
& $\rm \textbf{EDSR-baseline}$~\cite{lim2017enhanced} & DIV2K & 1555K & 160G&34.37&0.9270&30.28&0.8417&29.09&0.8052&28.15&0.8527&33.45&0.9439 \\
& $\rm \textbf{CARN}$~\cite{ahn2018fast}& DIV2K & 1592K & 118.8G &34.29&0.9255&30.29&0.8407&29.06&0.8034&28.06&0.8493&33.50&0.9440 \\
& $\rm \textbf{IMDN}$~\cite{hui2019lightweight}& DIV2K & 703K & 71.5G &34.36&0.9270&30.32&0.8417&29.09&0.8046&28.17&0.8519&33.61&0.9445\\
& $\rm \textbf{LAPAR-A}$~\cite{li2020lapar}& DF2K &594K &114G &34.36&0.9267&30.34&0.8421&29.11&0.8054&28.15&0.8523&33.51&0.9441\\
& $\rm \textbf{LatticeNet}$~\cite{luo2020latticenet}& DIV2K & 765K & 76.3G  &34.53&0.9281&30.39&0.8424&29.15&0.8059&28.33&0.8538&-&-  \\
& $\rm \textbf{ESRT}$~\cite{lu2021efficient}& DIV2K & 751K & - &34.42&0.9268&30.43&0.8433&29.15&0.8063&28.46&0.8574&33.95&0.9455\\
& $\rm \textbf{SwinIR-light}$~\cite{liang2021swinir}& DIV2K &918K & 111G &34.62&0.9289&30.54&0.8463&29.20&0.8082&28.66&0.8624&33.98&0.9478 \\
& $\rm \textbf{ELAN}$~\cite{zhang2022efficient}& DIV2K &629K &90.1G &34.61&0.9288&30.55&0.8463&29.21&0.8081&28.69&0.8624&34.00&0.9478 \\ 
& \textbf{\nameofmethod{-light}} & DIV2K & 861K & 105G & \highlight{34.67}&\highlight{0.9296}&\highlight{30.57}&\highlight{0.8469}&\highlight{29.26}&\highlight{0.8099}&\highlight{28.81}&\highlight{0.8655}&\highlight{34.19}&\highlight{0.9489} \\ \midrule%è°ƒå¥½äº†æ”¾ä¸Šæ¥ä¸ç„¶å°±æ”¹æˆåŠ å¤šå…¶ä»–Transformeræ–¹æ³•
%%%%%%%%%%%%%%%%%%%%%x4 SR
% \multicolumn{11}{l}{\emph{$\times 4$ SR}}\\ \midrule
\multirow{9}{*}{\rotatebox{90}{$\times 4$ SR}} 
& $\rm \textbf{EDSR-baseline}$~\cite{lim2017enhanced} & DIV2K & 1518K&114G &32.09&0.8938&28.58&0.7813&27.57&0.7357&26.04&0.7849&30.35&0.9067 \\
& $\rm \textbf{CARN}$~\cite{ahn2018fast}& DIV2K & 1592K & 90.9G &32.13&0.8937&28.60&0.7806&27.58&0.7349&26.07&0.7837&30.47&0.9084 \\
& $\rm \textbf{IMDN}$~\cite{hui2019lightweight}& DIV2K & 715K & 40.9G &32.21&0.8948&28.58&0.7811&27.56&0.7353&26.04&0.7838&30.45&0.9075\\
& $\rm \textbf{LAPAR-A}$~\cite{li2020lapar}& DF2K &659K & 94G &32.15&0.8944&28.61&0.7818&27.61&0.7366&26.14&0.7871&30.42&0.9074\\
& $\rm \textbf{LatticeNet}$~\cite{luo2020latticenet}& DIV2K & 777K & 43.6G &32.30&0.8962&28.68&0.7830&27.62&0.7367&26.25&0.7873&-&-  \\
& $\rm \textbf{ESRT}$~\cite{lu2021efficient}& DIV2K & 751K & - &32.19&0.8947&28.69&0.7833&27.69&0.7379&26.39&0.7962&30.75&0.9100\\
& $\rm \textbf{SwinIR-light}$~\cite{liang2021swinir}& DIV2K &930K & 63.6G &32.44&0.8976&28.77&0.7858&27.69&0.7406&26.47&0.7980&30.92&0.9151 \\
& $\rm \textbf{ELAN}$~\cite{zhang2022efficient}& DIV2K &640K &54.1G &32.43&0.8975&28.78&0.7858&27.69&0.7406&26.54&0.7982&30.92&0.9150 \\ 
& \textbf{\nameofmethod{-light}}& DIV2K & 873K & 62.8G & \highlight{32.51}&\highlight{0.8988}&\highlight{28.82}&\highlight{0.7872}&\highlight{27.73}&\highlight{0.7422}&\highlight{26.67}&\highlight{0.8032}&\highlight{31.17}&\highlight{0.9165}\\ \bottomrule % Swin-B-22k
%&  \highlight{0.884} & \highlight{0.936}& 0.830&\highlight{0.033}&\highlight{0.856}&\highlight{0.926}&0.758&\highlight{0.025}&\highlight{0.873}&\highlight{0.931}&\highlight{0.823}&\highlight{0.044} \\ \midrule % Swin-B-22k
\end{tabular}
\vspace{-8pt}
\caption{
% Comparison of our method with other \sArt methods on three benchmarks in terms of S-measure, adaptive E-measure, weight F-measure and mean absolute error. 
% The best two results are highlighted in {\color{BrickRed}{red}}.
%  '-P': PVTv2~\cite{wang2022pvt}, '-S': Swin-Transformer~\cite{liu2021swin}. 
%%DP Fan
Quantitative comparison of our \nameofmethod{-light} with recent \sArt \textbf{ lightweight image SR} methods on five benchmark datasets.
The best performance among all the model is highlighted.
}
\label{tab:light_weight_SR}
\vspace{-15pt}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% light weight
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\subsection{Classical Image Super-Resolution}

For the classical image SR task, we compare our \nameofmethod{} with a series of \sArt CNN-based and Transformer-based SR methods: RCAN~\cite{zhang2018image}, RDN~\cite{zhang2018residual}, SAN~\cite{dai2019second}, IGNN~\cite{zhou2020cross}, HAN~\cite{niu2020single}, NLSA~\cite{mei2021image}, IPT~\cite{chen2021pre}, SwinIR~\cite{liang2021swinir}, EDT~\cite{li2021efficient}, and ELAN~\cite{zhang2022efficient}. 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!htbp]
% 	\captionsetup{font=smaller}
	
	\centering
	\scriptsize
	
	\newcommand{\h}{0.105}
	\newcommand{\wa}{0.12}
	\newcommand{\wb}{0.16}
	\newcommand{\g}{-0.7mm}
	\renewcommand{\tabcolsep}{1.8pt}
	\renewcommand{\arraystretch}{1}
	\resizebox{1.00\linewidth}{!} {
 		\begin{tabular}{c}
			\normalsize
			\newcommand{\name}{figures/lightSR/Urban24/}
			\renewcommand{\h}{0.086}
			\newcommand{\w}{0.180}
			\begin{tabular}{cc}
			\normalsize
				\begin{adjustbox}{valign=t}
					\begin{tabular}{c}
						\includegraphics[height=0.205\textwidth, width=0.374\textwidth]{\name rectangle.png}
						\\
						Urban100 ($4\times$): img\_024
					\end{tabular}
				\end{adjustbox}
				\begin{adjustbox}{valign=t}
					\begin{tabular}{cccccc}
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name HR} \hspace{\g} &
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name Bicuble} \hspace{\g} &
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name CARN} \hspace{\g} &
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name IDN} \hspace{\g} &
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name IMDN} \hspace{\g} 
						\\
						HR \hspace{\g} &
						Bicubic \hspace{\g} &
						CARN~\cite{ahn2018fast} \hspace{\g} &
						IDN~\cite{hui2018fast} \hspace{\g} &
						IMDN~\cite{hui2019lightweight} \hspace{\g}
						\\
						\vspace{-4.0mm}
						\\
						
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name EDSR-baseline} \hspace{\g} &
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name LAPAR} \hspace{\g} &
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name latticenet4x} \hspace{\g} &
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name swinir-light}
						\hspace{\g} &		
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name SRformer} \hspace{\g} 
						\\ 
						
						EDSR-baseline~\cite{lim2017enhanced}  \hspace{\g} &
						LAPAR-A~\cite{li2020lapar} \hspace{\g} &
						LatticeNet \cite{luo2020latticenet}&
						SwinIR-light~\cite{liang2021swinir}
						& \textbf{\nameofmethod{}-light} (ours)
						\\
					\end{tabular}
				\end{adjustbox}
			\end{tabular} \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 			\newcommand{\name}{figures/lightSR/Urban67/}
% 			\renewcommand{\h}{0.086}
% 			\newcommand{\w}{0.180}
% 			\begin{tabular}{cc}
% 			\normalsize
% 				\begin{adjustbox}{valign=t}
% 					\begin{tabular}{c}
% 						\includegraphics[height=0.205\textwidth, width=0.374\textwidth]{\name rectangle.png}
% 						\\
% 						Urban100 ($4\times$): img\_067
% 					\end{tabular}
% 				\end{adjustbox}
%                 \begin{adjustbox}{valign=t}
% 					\begin{tabular}{cccccc}
% 						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name HR} \hspace{\g} &
% 						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name Bicuble} \hspace{\g} &
% 						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name CARN} \hspace{\g} &
% 						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name IDN} \hspace{\g} &
% 						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name IMDN} \hspace{\g} 
% 						\\
% 						HR \hspace{\g} &
% 						Bicubic \hspace{\g} &
% 						CARN~\cite{ahn2018fast} \hspace{\g} &
% 						IDN~\cite{hui2018fast} \hspace{\g} &
% 						IMDN~\cite{hui2019lightweight} \hspace{\g}
% 						\\
% 						\vspace{-4.0mm}
% 						\\
						
% 						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name EDSR-baseline} \hspace{\g} &
% 						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name LAPAR} \hspace{\g} &
% 						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name latticenet4x} \hspace{\g} &
% 						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name swinir-light}
% 						\hspace{\g} &		
% 						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name SRformer} \hspace{\g} 
% 						\\ 
						
% 						EDSR-baseline~\cite{lim2017enhanced}  \hspace{\g} &
% 						LAPAR-A~\cite{li2020lapar} \hspace{\g} &
% 						LatticeNet \cite{luo2020latticenet}&
% 						SwinIR-light~\cite{liang2021swinir}
% 						& \textbf{\nameofmethod{}-light} (ours)
% 						\\
% 					\end{tabular}
% 				\end{adjustbox}
% 			\end{tabular} \\	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\newcommand{\name}{figures/lightSR/B100_92/}
			\renewcommand{\h}{0.086}
			\newcommand{\w}{0.180}
			\begin{tabular}{cc}
			\normalsize
				\begin{adjustbox}{valign=t}
					\begin{tabular}{c}
						\includegraphics[height=0.205\textwidth, width=0.374\textwidth]{\name rectangle.png}
						\\
						B100 ($4\times$): img\_78004
					\end{tabular}
				\end{adjustbox}
                \begin{adjustbox}{valign=t}
					\begin{tabular}{cccccc}
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name HR} \hspace{\g} &
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name Bicuble} \hspace{\g} &
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name CARN} \hspace{\g} &
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name IDN} \hspace{\g} &
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name IMDN} \hspace{\g} 
						\\
						HR \hspace{\g} &
						Bicubic \hspace{\g} &
						CARN~\cite{ahn2018fast} \hspace{\g} &
						IDN~\cite{hui2018fast} \hspace{\g} &
						IMDN~\cite{hui2019lightweight} \hspace{\g}
						\\
						\vspace{-4.0mm}
						\\
						
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name EDSR-baseline} \hspace{\g} &
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name LAPAR} \hspace{\g} &
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name latticenet4x} \hspace{\g} &
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name swinir-light}
						\hspace{\g} &		
						\includegraphics[height=\h \textwidth, width=\w \textwidth]{\name SRformer} \hspace{\g} 
						\\ 
						
						EDSR-baseline~\cite{lim2017enhanced}  \hspace{\g} &
						LAPAR-A~\cite{li2020lapar} \hspace{\g} &
						LatticeNet \cite{luo2020latticenet}&
						SwinIR-light~\cite{liang2021swinir}
						& \textbf{\nameofmethod{}-light} (ours)
						\\
					\end{tabular}
				\end{adjustbox}
			\end{tabular} \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 		\end{tabular}
	}\vspace{-2mm}
	\caption{Qualitative comparison of our \nameofmethod{-light} with recent state-of-the-art \textbf{lightweight image SR} methods for the $\times 4$ SR task. For each example, our \nameofmethod{-light} can restore the structures and details better than other methods.}
	\label{fig:lightsr_visual}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!htbp]
    \vspace{-10pt}
    \centering
    \setlength{\abovecaptionskip}{0pt}
    \includegraphics[width=0.975\linewidth]{sub_picture/realworld/realworld.pdf}
    \small
    \put(-65., 8){\nameofmethod{} (ours)}
    \put(-125., 8){SwinIR~\cite{liang2021swinir}}
    \put(-210., 8){Real-ESRGAN~\cite{wang2021real}}
    \put(-273., 8){BSRGAN~\cite{KaiZhang2021DesigningAP}}
    \put(-335., 8){RealSR~\cite{ji2020real}}
    \put(-407., 8){ESRGAN~\cite{wang2018esrgan}}
    \put(-455., 8){LR}
    % \vspace{-4mm}
    \vspace{-5pt}
    \caption{Qualitative comparisons with recent state-of-the-art  methods on the $\times 4$ \textbf{real-world image SR} task. }
    \label{fig:real_sr}
    \vspace{-15pt}
\end{figure*}  % BSL and Path

\myPara{Quantitative comparison.} The quantitative comparison of the methods for classical image SR is shown in Table \ref{tab:classical_SR}. For a fair comparison, the number of parameters and MACs of \nameofmethod{} are lower than SwinIR\cite{liang2021swinir}, see supplementary material for details.
%
It can be clearly seen that \nameofmethod{} achieves the best performance on almost all five benchmark datasets for all scale factors.
%
Since calculating self-attention within large windows can allow more information to be aggregated over a large area, our \nameofmethod{} performs much better on the high-resolution test set, such as Urban100 and Manga109.
%
Especially, for the $\times2$ SR training with DIV2K, our \nameofmethod{} achieves a 33.86dB PSNR score on the Urban100 dataset, which is 0.46dB higher than SwinIR but uses fewer parameters and computations. The performance boost gets even bigger when introducing ensemble strategy as \nameofmethod{+}. 
%
The above strongly supports that our \nameofmethod{} is effective and efficient.

\myPara{Qualitative comparison.} We show qualitative comparisons with other methods in Figure \ref{fig:sr_visual}. 
%
From the first example of Figure \ref{fig:sr_visual}, one can clearly observe that \nameofmethod{} can restore crisp and detailed textures and edges.
%
In contrast, other models restore blurred or low-quality textures.
%
For the second example, our \nameofmethod{} is the only model that clearly restores every letter. 
%
The qualitative comparison shows that our \nameofmethod{} can restore much better high-resolution images from the low-resolution ones.


\subsection{Lightweight Image Super-Resolution}
To demonstrate our model's scalability and further proof of \nameofmethod{}'s efficiency and effectiveness, we train \nameofmethod{-light} and compare it with a list of \sArt lightweight SR methods: EDSR-baseline~\cite{lim2017enhanced}, CARN~\cite{ahn2018fast}, IMDN~\cite{hui2019lightweight}, LAPAR-A~\cite{li2020lapar}, LatticeNet~\cite{luo2020latticenet}, ESRT~\cite{lu2021efficient}, SwinIR-light~\cite{liang2021swinir}, and ELAN~\cite{zhang2022efficient}.

\myPara{Quantitative comparison.} The quantitative comparisons of lightweight image SR models are shown in Table~\ref{fig:lightsr_visual}. Following previous works~\cite{luo2020latticenet,ahn2018fast}, we report the MACs 
%(Multiply-Accumulate Operations)%
by upscaling a low-resolution image to  $1280 \times 720$ resolution on all scales.
%
We can see that our \nameofmethod{}-light achieves the best performance on all five benchmark datasets for all scale factors. Our model outperforms SwinIR-light~\cite{liang2021swinir}  by up to $0.20$ dB PSNR scores on the Urban100 dataset and $0.25$ dB PSNR scores on the Manga109 dataset with even fewer parameters and MACs.
%
The results indicate that despite the simplicity, our permuted self-attention is a more effective way to encode spatial information.

\myPara{Qualitative comparison.} We compare our \nameofmethod{} with \sArt lightweight image SR models for qualitative comparisons  in Figure~\ref{fig:lightsr_visual}.
%
Notably, for all examples in Figure \ref{fig:lightsr_visual}, \nameofmethod{-light} is the only model that can restore the main structures with less blurring and artifacts.
%
This strongly demonstrates that the light version of \nameofmethod{} also performs better for restoring edges and textures compared to other methods.

\subsection{Real-World Image Super-Resolution}
Since the ultimate goal of image SR  is to deal with rich real-world degradation and produce visually  pleasing images, we  follow SwinIR~\cite{liang2021swinir} to  retrain our \nameofmethod{} by using the same degradation model as BSRGAN~\cite{KaiZhang2021DesigningAP} and show results in \figref{fig:real_sr}. \nameofmethod{} still produces more realistic and visually pleasing textures without artifacts when facing with real-world images, which demonstrates the robustness of our method. 

\begin{figure}[!htbp]
    \vspace{-5pt}
    \centering
    \scriptsize
    
    \includegraphics[width=\linewidth]{sub_picture/LAM/LAM_simple8.pdf}
    % SRFormer
    % \put(-200, 48){$\mathrm{WS}: 12\times12$}
    % \put(-200, 38){$\mathrm{PSNR}: 33.08$}
    \put(-230, 96){HR Image}
    \put(-160, 96){LAM Attribution}
    \put(-80, 96){Area of Contribution}
    \put(-133, -7){\nameofmethod{}}
    \put(-185, -7){SwinIR~\cite{liang2021swinir}}
    \put(-90, -7){SwinIR~\cite{liang2021swinir}}
    \put(-40, -7){\nameofmethod{}}

    \vspace{-7pt}
    \caption{    \small LAM results of  SwinIR~\cite{liang2021swinir} and \nameofmethod{} on  multiple challenging examples. We can see that \nameofmethod{} can perform SR reconstruction  based on a particularly wide range of pixels. }
    \label{fig:lam}
    \vspace{-15pt}
\end{figure}


%%%%%%%%% BODY TEXT
\subsection{LAM Comparison}
To observe the range of utilized pixels for SR reconstruction, we compare our model with SwinIR using LAM~\cite{gu2021interpreting} shown in \figref{fig:lam}. 
%Given a specified region in the SR image, LAM analyzes the contributions of each pixel in the input image to reconstruct this region. 
Based on the extremely large attention window, \nameofmethod{} infers SR images with a significantly wider range of pixels than SwinIR~\cite{liang2021swinir}. The experimental results are strongly consistent with our motivation and demonstrate the superiority of our method from the interpretability perspective.

% \subsection{other Experiments}

% \textbf{LAM} 
% jpeg,denoise need 1600k iters,in the end



\section{Conclusion}
% \textbf{Design principle1: Larger self-attention window can achieve more excellent performance. }Built on Swin Transfomer blocks, SwinIR uses local window attention to calculate the correlation between pixels in the same window, showing dominance in image super-resolution tasks. Our experiments have confirmed that a larger window size can achieve more excellent performance because more pixels will be involved in the self-attention calculation process. However,  large window self-attention squarely increases the FLOPs, and quartet increases the GPU memory consumption.

In this paper, we propose PSA, an efficient self-attention mechanism which can efficiently build pairwise correlations within large windows.
%
Based on our PSA, we design a simple yet effective Transformer-based model for single image super-resolution, called \nameofmethod{}.
%
Due to the extremely large attention window and  high-frequency information enhancement, \nameofmethod{} achieves \sArt performance on classical, lightweight, and real-world SR task.
%On both the classical SR task and lightweight SR task, we achieve \sArt performance On five widely-used benchmark datasets and also produces more realistic textures on real-world task.%
%
We hope our permuted self-attention can be a  paradigm of large window self-attention and serve as a useful tool for future research in super-resolution model design.





{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}