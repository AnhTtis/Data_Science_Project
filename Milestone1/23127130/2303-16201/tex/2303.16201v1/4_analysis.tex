
\begin{figure*}[!ht]
  \centering
  \includegraphics[width=\linewidth]{imgs/Warped.pdf}
  \caption{\textbf{Dense warping} from a source image (top row) to a target image (second row). We warp all foreground pixels (highlighted by a red overlay in the source image). Our methods produce dense and semantically more meaningful warps from the source to the target.}
  \label{fig:warp}
  \vspace{-1em}
\end{figure*}

\section{Experiments}
\label{sec:experiments}

We evaluate our method on several real-world in-the-wild image collections of both rigid and non-rigid object categories. For all datasets, we use a fixed set of hyperparameters (provided in the appendix) unless specified otherwise. 
%and train our model on a single 16GB GPU for a fixed 20,000 iterations (batch size of 20). 

%In \S\ref{sec:qual_consistent}, we visualize the canonical space mapping learned by the model. In \S\ref{sec:qual_dense}, we evaluate the dense correspondence learned by the model qualitatively.
% In \S\ref{sec:quant_pair}, we show the performance of our approach using standard quantitative metrics on datasets where ground truth keypoint annotations are available. In \S\ref{sec:quant_consistent}, we propose a new metric to evaluate the consistency of keypoint predictions as we propagate keypoints over multiple images in a sequence.

\myparagraph{Datasets.}
\textbf{SPair-71k}~\citep{min2019spair} consists of 1,800 images from 18 
%PASCAL VOC~\citep{everingham2015pascal} 
categories. We optimize over image collections derived from the SPair-71k test set for each category independently and report results
%We train a model on the test images for each category independently ($\sim$25 images each) using no ground truth annotations. 
 on each individual category, as well as aggregate results over all 18 categories. In case of \textbf{PF-Willow}~\citep{ham2016}, we consider all 4 categories of the dataset containing $\sim$30 images. 
% We also report results on the test set of the first three individual categories of 
\textbf{CUB-200}~\citep{wah2011caltech} datasets consists of over 200 fine-grained categories. We optimized our model on the test sets of first 3 categories of the dataset, consisting of 15-20 images each.
%contains about 12K images from 200 bird categories. We report our results on 
We also show qualitative results on 4 objects from \textbf{SAMURAI} dataset~\citep{boss2022samurai}.
%We don't provide quantitative results on the SAMURAI data since the groundtruth keypoints annotations are unavailable.

%to demonstrate the effectiveness of our approach for collections where only few images are available. SAMURAI consists of images of objects taken under a variety of background and lighting conditions. While the images are taken in less `in-the-wild' conditions as compared to SPair, objects in SAMURAI are relatively uncommon (and not present in datasets used to train large-scale SSL models).


\subsection{Canonical Space Alignment}
\label{sec:qual_consistent}

% A convenient 
One simple way to visualize the alignment of an image set when mapped to the canonical space  $\G$
% mapping of the image set 
is to define a colormap over the canonical grid and color the image pixels according to their mapped location in the canonical grid. 
In \cref{fig:canon}, the first row for each collection contains sample input images. The second row shows discrete parts obtained via parts co-segmentation using \citep{amir2021deep}. 
While these parts are also consistent across the image set, our canonical space mapping (third row) can be seen as a dense and continuous co-segmentation. We show the results for six datasets: CUB-200 birds; Dogs, Cats, and Train from SPair-71k; and Robot and Shoe from SAMURAI. 
The colormap used for the canonical space is provided in the supplement.
We observe that our method can find dense correspondences across highly varying poses, backgrounds, and lighting.
It also maps common parts of objects in a dataset to nearby regions of the canonical space.
This is evident in \cref{fig:canon} where, for instance, the faces of different cats are colored similarly.
% always 
%While our method can reason about semantically similar parts of objects, it struggles with left-right ambiguity in symmetric objects. This is partially an artifact of the part consistency loss (\cref{eq:parts}). Since the parts obtained using part co-segmentation do not disambiguate between left and right parts (hands, legs, \etc), these errors are propagated to our method as well.
% A simple way to deal with this ambiguity is to estimate the total variation (TV) loss of the output of alignment network for both original and flipped image, and use the one with minimum TV loss.

\subsection{Visualizing Dense Correspondences}
\label{sec:qual_dense}
We can also find dense correspondences between a pair of images $\Ia$ and $\Ib$ using our framework. Recall that $\A$ outputs canonical space coordinates $\cca$ and $\ccb$ for each pixel location $\pa$ and $\pb$.
%Since our mapping is unconstrained, it is not trivially invertible.
In order to warp the source image $\Ia$ to a target image $\Ib$, for every foreground pixel $\pa_i$ in $\Ia$, we need to find its nearest neighbor among the set of points in $\pb$ in the canonical space:
%%%%%%
% \begin{align*}
    % \Iatob(\pa_i) \approx \arg \min_j \parallel \cca_i - \ccb_j \parallel^2
% \end{align*}
%%%%%%
We perform this action for all the foreground pixels in the source image, and splat according to the nearest neighbor mapping to get our desired warped image. 
\cref{fig:warp} shows qualitative results 
% of our method in the last row 
for 10 different datasets. The top row is the source image with foreground mask highlighted. The second row is the target image. We show results for two other pairwise image optimization approaches, and then our method in the last row. NBB~\citep{aberman2018neural} computes nearest neighbors using VGG-19 and applies a Moving Least Squares (MLS) optimization~\citep{schaefer2006image} to compute a dense flow from source to target. While the flow computed via MLS is smooth, it usually does not respect semantic correspondences, as evident in the figure. 
We extend their technique to use DINO features as well.
With DINO and the nearest neighbor approach (DVD)~\citep{amir2021deep}, the semantic correspondences are arguably better, but since this approach relies on the output of a Vision Transformer (ViT), which has lower resolution than the image, it produces a sparse flow. Our method produces both dense and consistent flow between an image pair.

\begin{table*}[t]
\centering
% \footnotesize
% \renewcommand{\arraystretch}{1.1}
% \renewcommand{\tabcolsep}{2pt}
\caption{\textbf{Evaluation on SPair-71k.} Per-class and average PCK@$0.10$ on test split. Highest PCK among \textit{weakly supervised} methods in bold, second highest underlined. Scores marked with ($\star$) means the paper uses a fixed image from the test set as canonical image. Our method is competitive against other weak supervised approaches and often outperforms them.}
% \vspace{-0.5em}
\label{tab:spair_pck}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}clccccccccccccccccccc@{}}
% \begin{tabular}{*{5}{>{\rowfonttype}clccccccccccccccccccc}<{\rowfont{}}}
\toprule
\textbf{Supervision} & \textbf{Method}  & Aero & Bike & Bird & Boat &Bottle& Bus  & Car  & Cat  & Chair& Cow  & Dog  & Horse& Motor&Person& Plant& Sheep & Train & TV & \textbf{All}\\  
\midrule
\multirow{1}{3.2cm}{\centering{Strong Supervision}}
% & HPF~\citep{min2019hyperpixel}         & 25.2 & 18.9 & 52.1 & 15.7 & 38.0 & 22.8 & 19.1 & 52.9 & 17.9 & 33.0 & 32.8 & 20.6 & 24.4 & 27.9 & 21.1 & 15.9 & 31.5 & 35.6 & 28.2\\
% & SCOT~\citep{liu2020semantic}          & 34.9 & 20.7 & 63.8 & 21.1 & 43.5 & 27.3 & 21.3 & 63.1 & 20.0 & 42.9 & 42.5 & 31.1 & 29.8 & 35.0 & 27.7 & 24.4 & 48.4 & 40.8 & 35.6\\
% & DHPF~\citep{min2020learning}          & 38.4 & 23.8 & 68.3 & 18.9 & 42.6 & 27.9 & 20.1 & 61.6 & 22.0 & 46.9 & 46.1 & 33.5 & 27.6 & 40.1 & 27.6 & 28.1 & 49.5 & 46.5 & 37.3\\
% & PMD~\citep{li2021probabilistic}       & 38.5 & 23.7 & 60.3 & 18.1 & 42.7 & 39.3 & 27.6 & 60.6 & 14.0 & 54.0 & 41.8 & 34.6 & 27.0 & 25.2 & 22.1 & 29.9 & 70.1 & 42.8 & 37.4\\
% & MMNet~\citep{zhao2021multi}           & 43.5 & 27.0 & 62.4 & 27.3 & 40.1 & 50.1 & 37.5 & 60.0 & 21.0 & 56.3 & 50.3 & 41.3 & 30.9 & 19.2 & 30.1 & 33.2 & 64.2 & 43.6 & 40.9\\
% & CHM~\citep{min2021convolutional}      & 49.6 & 29.3 & 68.7 & 29.7 & 45.3 & 48.4 & 39.5 & 64.9 & 20.3 & 60.5 & 56.1 & 46.0 & 33.8 & 44.3 & 38.9 & 31.4 & 72.2 & 55.5 & 46.3\\
% & CATs~\citep{cho2021cats}              & 52.0 & 34.7 & 72.2 & 34.3 & 49.9 & 57.5 & 43.6 & 66.5 & 24.4 & 63.2 & 56.5 & 52.0 & 42.6 & 41.7 & 43.0 & 33.6 & 72.6 & 58.0 & 49.9\\
% & PMNC~\citep{lee2021patchmatch}        & 54.1 & 35.9 & 74.9 & 36.5 & 42.1 & 48.8 & 40.0 & 72.6 & 21.1 & 67.6 & 58.1 & 50.5 & 40.1 & 54.1 & 43.3 & 35.7 & 74.5 & 59.9 & 50.4\\
& SCorrSAN~\citep{huang2022learning}    & 57.1 & 40.3 & 78.3 & 38.1 & 51.8 & 57.8 & 47.1 & 67.9 & 25.2 & 71.3 & 63.9 & 49.3 & 45.3 & 49.8 & 48.8 & 40.3 & 77.7 & 69.7 & 55.3\\
\rowfont{\color{Black}}
\multirow{1}{3.2cm}{\centering{GAN supervision}}
& GANgealing~\citep{peebles2022gan}     &    - & 37.5 &    - &    - &    - &    - &    - & 67.0 &    - &    - & 23.1 &    - &    - &    - &    - &    - &    - & 57.9 &    -\\
\hline
\multirow{7}{3.2cm}{\centering{Weak supervision \\ (train/test)}}
& CNNGeo~\citep{rocco2017convolutional} & 23.4 & 16.7 & 40.2 & 14.3 & 36.4 & 27.7 & 26.0 & 32.7 & 12.7 & 27.4 & 22.8 & 13.7 & 20.9 & 21.0 & 17.5 & 10.2 & 30.8 & 34.1 & 20.6\\
& A2Net~\citep{seo2018attentive}        & 22.6 & 18.5 & 42.0 & 16.4 & 37.9 & \textbf{30.8} & 26.5 & 35.6 & 13.3 & 29.6 & 24.3 & 16.0 & 21.6 & 22.8 & \textbf{20.5} & 13.5 & 31.4 & 36.5 & 22.3\\
& WeakAlign~\citep{rocco2018end}        & 22.2 & 17.6 & 41.9 & 15.1 & 38.1 & 27.4 & 27.2 & 31.8 & 12.8 & 26.8 & 22.6 & 14.2 & 20.0 & 22.2 & 17.9 & 10.4 & 32.2 & 35.1 & 20.9\\
& NCNet~\citep{rocco2018neighbourhood}  & 17.9 & 12.2 & 32.1 & 11.7 & 29.0 & 19.9 & 16.1 & 39.2 &  9.9 & 23.9 & 18.8 & 15.7 & 17.4 & 15.9 & 14.8 &  9.6 & 24.2 & 31.1 & 20.1\\
& SFNet~\citep{lee2019sfnet}            & 26.9 & 17.2 & 45.5 & 14.7 &  \underline{38.0} & 22.2 & 16.4 & \textbf{55.3} & 13.5 & 33.4 & 27.5 & 17.7 & 20.8 & 21.1 & 16.6 & 15.6 & 32.2 & 35.9 & 26.3 \\
& PMD~\citep{li2021probabilistic}       & 26.2 & 18.5 & 48.6 & 15.3 & \textbf{38.0} & 21.7 & 17.3 & 51.6 & 13.7 & 34.3 & 25.4 & 18.0 & 20.0 & 24.9 & 15.7 & 16.3 & 31.4 & \textbf{38.1} & 26.5\\
& PSCNet-SE~\citep{jeon2021pyramidal}   & 28.3 & 17.7 & 45.1 & 15.1 & 37.5 &  \underline{30.1} & \underline{27.5} & 47.4 & 14.6 & 32.5 & 26.4 & 17.7 & 24.9 & 24.5 & \underline{19.9} & 16.9 & 34.2 & \underline{37.9} & 27.0\\
\hline
\multirow{5}{3.2cm}{\centering{Weak supervision \\ (test-time optimization)}}
& VGG+MLS~\citep{aberman2018neural}     & 29.5 & 22.7 & 61.9 &  \textbf{26.5} & 20.6 & 25.4 & 14.1 & 23.7 & 14.2 & 27.6 & 30.0 & 29.1	& \underline{24.7} & 27.4 &	19.1 & 19.3 & 24.4 & 22.6 & 27.4\\
& DINO+MLS~\citep{aberman2018neural,caron2020unsupervised}& 49.7 & 20.9 & 63.9 & 19.1 & 32.5 & 27.6 & 22.4 & 48.9 & 14.0 & 36.9 & 39.0 & \underline{30.1} & 21.7 & \underline{41.1} & 17.1 & 18.1 & \underline{35.9} & 21.4  & 31.1\\
& DINO+NN~\citep{amir2021deep}          & \underline{57.2} &  24.1 &  \underline{67.4} & 24.5 & 26.8 & 29.0 & 27.1 & 52.1 &  \underline{15.7} &  \underline{42.4} &  \underline{43.3} & 30.1  & 23.2 & 40.7 & 16.6 & \underline{24.1} & 31.0 & 24.9 & \underline{33.3}\\
& NeuCongeal~\citep{ofri2023neural}& -    & \textbf{29.1$^\star$}    &    - &    - &    - &    - &    - & 53.3    &    - &    - & 35.2   &    - &    - &    - &    - &    - &    - & -    &    -\\
& \backronym~(Ours)                    & \textbf{57.9} & \underline{25.2} & \textbf{68.1} &  \underline{24.7}	& 35.4 & 28.4 & \textbf{30.9} &  \underline{54.8}	& \textbf{21.6} & \textbf{45.0} &	\textbf{47.2} & \textbf{39.9} & \textbf{26.2} & \textbf{48.8} &	14.5 & \textbf{24.5} & \textbf{49.0} & 24.6 & \textbf{36.9}\\

\bottomrule
\end{tabular}}
\vspace{-1em}
\end{table*}

% \begin{table}[!ht]
% \centering
%     \caption{\textbf{Evaluation on CUB-200 and PF-Willow.} PCK for three CUB categories. We outperform other approaches at both coarse precision ($\alphapck$=0.10) and fine precision ($\alphapck$=0.01).}
%     \vspace{-0.8em}
%     \label{tab:cub_pck}
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{ lcc|cc|cc } 
%     \toprule
%      & \multicolumn{2}{c|}{CUB-001} & \multicolumn{2}{c|}{CUB-002} & \multicolumn{2}{c}{CUB-003} \\
%      \midrule
%     \textbf{Method} & PCK@$0.1$ & PCK@$0.01$ & PCK@$0.1$ & PCK@$0.01$ & PCK@$0.1$ & PCK@$0.01$  \\
%     \midrule
%     NBB~\citep{aberman2018neural} & 22.1 & 0.3 & 21.2 & 2.1  & 34.2 & \textbf{10.2} \\
%     DVD~\citep{amir2021deep}      & 66.8 & 7.8 & 68.5 & 8.2  & 69.5 & 6.9 \\
%     \backronym~(Ours)             & \textbf{71.8} & \textbf{9.6} & \textbf{78.6} & \textbf{10.9} & \textbf{77.4} & 9.9 \\
%     \bottomrule
%     \end{tabular}
%     }
%     \vspace{-1.2em}
% \end{table}

\begin{table}[!ht]
\centering
    \caption{\textbf{CUB-200 and PF-Willow.} PCK@$0.10$ for three CUB categories and four PF-Willow categories.}
    % \vspace{-0.8em}
    \label{tab:cub_pck}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ lcc|cc } 
    \toprule
     & \multicolumn{2}{c|}{CUB-200 (3 categories)} & \multicolumn{2}{c}{PF-Willow (4 categories)} \\
     \midrule
    \textbf{Method} & PCK@$0.1$ & PCK@$0.05$ & PCK@$0.1$ & PCK@$0.05$  \\
    \midrule
    PMD~\citep{li2021probabilistic}        & - & - & 74.7 & 40.3 \\
    PSCNet-SE~\citep{jeon2021pyramidal}    & - & - & 75.1 & 42.6  \\
    VGG+MLS~\citep{aberman2018neural}      & 25.8 & 18.3 & 63.2 & 41.2   \\
    DINO+MLS~\citep{aberman2018neural,caron2020unsupervised} & 67.0 & 52.0 & 66.5 & 45.0 \\
    DINO+NN~\citep{amir2021deep}               & 68.3 & 52.8 & 60.1 & 40.1 \\
    \backronym~(Ours)                      & \textbf{75.9} & \textbf{57.9} & \textbf{76.3} & \textbf{53.0} \\
    \bottomrule
    \end{tabular}
    }
    \vspace{-1.2em}
\end{table}

\subsection{Pairwise Correspondence}
\label{sec:quant_pair}
\myparagraph{Metric.}
For evaluating accuracy of pairwise correspondence, we use the PCK metric~\citep{yang2012articulated} (percentage of correct keypoints) on the SPair, CUB, and PF-Willow datasets.
%We report PCK computed across all pair of images in all datasets, unless mentioned otherwise. 

%While most existing works show the result at $\alphapck=0.10$, which corresponds to roughly $20$ pixels (for a bounding box of size $200 \times 200$ pixels), we also report results for our method at higher precision thresholds.

\myparagraph{Baselines.}
We categorize prior works based on the supervision used: 
(1) \emph{Strong supervision} methods utilize human-annotated keypoints to learn pairwise image correspondence and achieve the best performance (on average). We include the numbers from a recent work~\citep{huang2022learning} for reference purposes.
(2) \emph{GAN supervision} methods like~\citep{peebles2022gan} use a category-specific GAN pre-trained with large external datasets. While this method works well, it is restricted to only the categories for which large datasets are available and GAN training is feasible.
(3) \emph{Weak supervision} methods use category-level supervision (\ie they assume that given pair/collection of images are from same category). They often resort to fine-tuning a large ImageNet~\citep{deng2009imagenet} pre-trained network using a self-supervised loss function (\eg with synthetic transformations) and optionally use additional information such as foreground masks or matching image pairs for training.
%Some works, such as GANgealing~\citep{peebles2022gan} use large external datasets for training GAN models that allow them to sample from highly diverse category-specific images.
%Most importantly, these works follow a train/test setting where they (usually) fine-tune  on the SPair-71k train set and evaluate on SPair-71k test. 
Some of these works follow a \emph{train/test} setting, where the network is fine-tuned on a separate set of training images.  Note that in our work, we train a much smaller network from scratch instead of fine-tuning a large network. Some approaches (including ours) directly perform \emph{test-time optimization} without additional training data or annotations.

%Note that these works are not directly comparable to our method which trains a much smaller network from scratch on SPair-71k test per object category and does not aim for generalization but rather for consistent dense correspondences. For the sake of completion (and lack of other closely related works), we include some of these recent weakly-supervised works in \cref{tab:spair_pck}.
%(3) \emph{Weak supervision (test-time optimization)} refers to the approaches which simply optimize for test images without additional training data or annotations. Our method is closest to these works. 

NBB \cite{aberman2018neural} optimizes a flow from one image to another using mutual nearest neighbors as control points~\citep{schaefer2006image}. While~\cite{aberman2018neural} shows the results by computing nearest neighbors from a VGG network, we further extend their work to utilize a DINO network. \cite{amir2021deep} simply computes nearest neighbors in DINO feature space. 
A concurrent work, Neural Congealing~\cite{ofri2023neural}, is closest to our work, in that they also perform test-time training using a canonical atlas. 
However, for objects with large deformations (such as in SPair-71k), they need to apply category specific accommodations (for instance, fixing the atlas for bicycle category). 
Our canonical grid allows for large deformations and is learned in all cases with a fixed set of hyperparameters.
We obtain scores for other models from their respective papers (whenever available) or from~\citep{huang2022learning}. 
Scores for~\cite{aberman2018neural,amir2021deep,caron2020unsupervised} are computed using official code. 
The official code of~\cite{ofri2023neural} did not converge on several objects in our experiments, hence we report the quantitative results from the paper.

\myparagraph{Discussion.} \cref{tab:spair_pck} shows PCK@$0.1$ for all SPair-71k~\citep{min2019spair} categories.
It is evident that having groundtruth keypoint annotations
%(and/or large datasets)
during training is highly beneficial; approaches that lack keypoints 
% available
during training lag behind.
%and have difficulty generalizing to unseen images. Test-time optimization approaches, such as ours, attempt to bridge the gap between these two approaches.
We also observe that for categories with rigid objects (or less extreme deformations) such as `Bottle' or `Bus', weakly supervised approaches attain a similar performance as ours. However, in the objects with extreme variations such as animals/birds, our method outperforms other baselines. In our experiments, we observed that per-category hyperparameters can increase PCK performance further by $\sim 2\%$. This strategy is similar to Neural Congealing~\cite{ofri2023neural} where specific accommodations are made per category (\eg tailored training regime for bicycle). However we report our numbers with a fixed set of hyperparameters for consistency.

\cref{tab:cub_pck} shows average results for the first 3 categories of the CUB dataset, and 4 categories of the PF-Willow dataset. Note that PF-Willow is % a relatively `easier' dataset as 
an easier dataset
compared to SPair-71k since it consists of rigid objects with little variation. 
Our method has performance similar to PSCNet-SE~\cite{jeon2021pyramidal} when we compute PCK using threshold $\alphapck=0.1$ (which corresponds to a $\sim$20-pixels margin of error). However at higher precision ($\alphapck=0.05$), our method provides much larger gains compared to the baselines.

%Note that our work combines NBB~\citep{aberman2018neural} and DVD~\citep{amir2021deep} to derive pseudo-groundtruth keypoint pairs for self-supervision. 
%Since these pseudo-groundtruth keypoints need not align with the annotated keypoints (used for testing), our optimization scheme indeed performs worse than the simpler DVD approach in some categories. 
%However, we outperform DVD on average for 18 SPair classes. Our method also achieves higher PCK than NBB and DVD for all three fine-grained CUB categories (refer \cref{tab:cub_pck}).

% \begin{figure}[!t]
%   \centering
%   \includegraphics[width=\linewidth]{imgs/precise_pck.pdf}
%   \caption{PCK at varying values of $\alphapck$.}
%   \label{fig:precise_pck}
%   \vspace{-.7em}
% \end{figure}

%In addition to PCK@$0.1$, we also report finer grained PCK metrics varying $\alphapck$ between 0.01 and 0.1 to better evaluate dense correspondence accuracy. \cref{fig:precise_pck} shows the performance for our model on two SPair and one CUB categories (full results available in the supplement). While the performance of all methods drops drastically at smaller thresholds, our approach performs slightly better than DVD. \cref{tab:cub_pck} shows the PCK results for both $\alphapck=0.01$ and $\alphapck=0.10$ on the CUB dataset.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{imgs/cycle_pck.pdf}
%   \vspace{-1.2em}
  \caption{\footnotesize \textbf{Image Set Correspondence.} $\kcycle$ at varying $\alphapck$ (higher is better). Our method outperforms DVD baseline at both large and small values of $\alphapck$.}
  \label{fig:cycle_pck}
%   \vspace{-1.2em}
\end{figure}

\subsection{Image Set Correspondence}
\label{sec:quant_consistent}
Our goal in this work is to recover dense and \textit{consistent} correspondences. A shortcoming of the PCK metric is that it is only computed between image pairs. %In our experiments, we observed that for all methods, 
However, the errors in keypoint prediction tend to accumulate when transferring keypoints over a sequence of images. To address this limitation, we propose a new metric to measure consistency across multiple images, called $\kcycle$. 
Given a set of $k$ images $\{\I^1, \I^2, \dots, \I^k\}$ and an annotated keypoint in the first image $\p^1$ visible in \textbf{each} of the $k$ images, we propagate $\p$ from $\I^1 \rightarrow \I^2, \I^2 \rightarrow \I^3,\dots, \I^{k-1} \rightarrow \I^{k}, \I^{k} \rightarrow \I^{1}$ and get the corresponding predictions $\p^{1\rightarrow2}, \p^{1\rightarrow2\rightarrow3},\dots$ and so on. As before, $\p^{1\rightarrow \dots \rightarrow j}$ is considered to be predicted correctly if it is within a threshold $\alphapck\cdot\max{(H_\bbox, W_\bbox)}$ of the ground truth keypoints $\hat{\p}^{1\rightarrow \dots \rightarrow j}$. We sum up all the correct predictions and plot scores at different values of $\alphapck$ in \cref{fig:cycle_pck}. 
We choose $k=4$ for all experiments (with additional results for other values of $k$ provided in the supplement). Note that since the number of possible permutations of $k$-length sequences can be very large, we randomly sample 200 sequences in our experiments.

\cref{fig:cycle_pck} shows that our method significantly outperforms the DINO+NN baseline for both small and large values of $\alphapck$ across all datasets
% consistently 
(complete results in the supplemental material). We attribute this result to having a consistent canonical space across the image collection that 
% doesn't allow 
prevents errors in keypoint transfer from accumulating to large values. 

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=\linewidth]{imgs/edit.pdf}
%   \caption{\footnotesize \textbf{Edit Propagation.} We demonstrate edit propagation applications using our model for `Cow' category of SPair-71k and `Firetruck' from SAMURAI. \backronym allows us to propagate edits from a random image to other images in the dataset.}
%   \label{fig:edit}
%   \vspace{-1.2em}
% \end{figure}

% \subsection{Edit Propagation}
% \label{sec:edit}
% \backronym can enable a number of image editing applications. In \cref{fig:edit}, we show results of a model trained on `Cow' category of SPair-71k and `Firetruck' from SAMURAI. We added a small edit on the nose of the cow, and propagated it to other images of the dataset. ASIC is able to propagate the edits despite of changes in the object appearance, occlusion, \etc. We make a similar observation in the case of `Firetruck'. Here edit applied to the window of firetruck was reliable transferred to other images.

% \begin{table}[!ht]
% \centering
%     \caption{Consistent Correspondence Results PCK$(\kcycle, \alphapck)$.}
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{ lccccccc } 
%     \toprule 
%     \textbf{Dataset} & \textbf{Method} & $\mathbf{(2, 0.1)}$ & $\mathbf{(3, 0.1)}$ & $\mathbf{(4, 0.1)}$ & $\mathbf{(2, 0.01)}$ & $\mathbf{(3, 0.01)}$ & $\mathbf{(4, 0.01)}$ \\
%     \hline
%     \multirow{2}{*}{Bicyle} & DVD++~\citep{amir2021deep} & 65.3 & 47.1 & 38.3 & 14.6 & 5.6 & 2.0 \\
%                             & \backronym~ (Ours)                       & 92.8 & 88.9 & 92.0 & 59.3 & 43.3 & 40.9  \\
%     \hline
%     \multirow{2}{*}{Cat} & DVD++~\citep{amir2021deep} & 76.3 & 70.7 & 70.1 & 18.4 & 9.1 & 8.4 \\
%                          & \backronym~ (Ours)                       & 84.7 & 84.3 & 84.2 & 54.2 & 42.7 & 33.9  \\
%     \hline
%     \multirow{2}{*}{Dog} & DVD++~\citep{amir2021deep} & 79.2 & 65.0 & 66.7 & 16.8 & 7.2 & 8.4 \\
%                          & \backronym~ (Ours)                       & 91.5 & 90.0 & 87.9 & 47.3 & 31.5 & 27.4  \\
%     \hline
%     \multirow{2}{*}{TV} & DVD++~\citep{amir2021deep} & 52.7 & 31.6 & 23.4 & 8.8 & 4.8 & 2.2 \\
%                         & \backronym~ (Ours)                       & 83.9 & 73.2 & 71.9 & 30.6 & 15.1 & 12.1  \\
%     \hline
%     \multirow{2}{*}{CUB1} & DVD++~\citep{amir2021deep} & 84.3 & 70.7 & 63.9 & 21.8 & 8.7 & 7.2 \\
%                           & \backronym~ (Ours)                       & 94.1 & 94.4 & 94.1 & 58.3 & 51.5 & 48.8  \\
%     \hline
%     \multirow{2}{*}{CUB2} & DVD++~\citep{amir2021deep} & 79.1 & 67.4 & 62.4 & 13.9 & 8.3 & 5.6 \\
%                           & \backronym~ (Ours)                       & 94.6 & 94.0 & 94.8 & 57.6 & 53.0 & 44.9  \\
%     \hline
%     \multirow{2}{*}{CUB3} & DVD++~\citep{amir2021deep} & 80.8 & 68.6 & 63.9 & 17.4 & 9.4 & 5.6 \\
%                           & \backronym~ (Ours)                       & 94.8 & 94.8 & 92.9 & 62.9 & 56.2 & 49.7  \\
%     \bottomrule
%     \end{tabular}
%     }
% \end{table}

% \myparagraph{Improving SSL backbone.}


\subsection{Ablations}
\label{sec:ablation}
We perform an ablation study on our various proposed losses proposed, summarized in \cref{tab:ablation}. We report average PCK@$0.10$ results for first 3 categories of the CUB-200 dataset and all 4 categories of the PF-Willow dataset. As expected, the keypoint loss $\mathcal{L}_\text{KP}$ plays the most important role in our overall framework. We also found the total variation regularization $\mathcal{L}_\text{TV}$ to be crucial for network training convergence. $\mathcal{L}_\text{Equi}$ is necessary for learning dense correspondence. Finally, $\mathcal{L}_\text{Recon}$ and $\mathcal{L}_\text{Parts}$ provide comparatively small improvements.

%As expected, too high or too low of the temperature values deteriorate the PCK performance. At high values of temperature, the model tends to ignore the pseudo correspondences and at lower values, the model starts overfitting on the the pseudo correspondences. We observe a similar trend for the coefficients of other loss functions $\mathcal{L}_\text{Equi}$ , $\mathcal{L}_\text{Parts}$, and $\mathcal{L}_\text{Recon}$. While our method works out of the box for a fixed set of hyperparameters for all the categories, to achieve the best results for image sets, we perform a hyperparameter search over different hyperparameters using $\kcycle$ (for pseudo correspondences) as the criterion.

\begin{table}[!ht]
\centering
    % \footnotesize
    \caption{\textbf{Ablation Study.} Average PCK@$0.10$ (for 3 and 4 categories respectively) in CUB and PF-Willow datasets.}
    \vspace{-1em}
    \label{tab:ablation}
    \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{ lcc } 
    \toprule
    \textbf{Ablation} & CUB-200 & PF-Willow \\
                      & (3 categories) & (4 categories) \\
    \midrule
    % Temperature in $\mathcal{L}_\text{KP}$ ($\tau=0.1$)    & 66.8 & 31.4 \\
    % Temperature in $\mathcal{L}_\text{KP}$ ($\tau=0.5$)    & 70.7 & 38.4 \\
    % Temperature in $\mathcal{L}_\text{KP}$ ($\tau=1.0$)    & 58.7 & 30.8 \\
    % Coefficient of $\mathcal{L}_\text{Equi}$  ($\lambda=0$)  & 65.2 & 31.2 \\
    % Coefficient of $\mathcal{L}_\text{Equi}$  ($\lambda=1$)  & 70.7 & 38.4 \\
    % Coefficient of $\mathcal{L}_\text{Equi}$  ($\lambda=10$) & 68.3 & 35.5 \\
    % Coefficient of $\mathcal{L}_\text{Parts}$  ($\lambda=0$)  & 68.1 & 30.3 \\
    % Coefficient of $\mathcal{L}_\text{Parts}$  ($\lambda=1$)  & 70.7 & 30.1 \\
    % Coefficient of $\mathcal{L}_\text{Parts}$  ($\lambda=10$) & 68.9 & 38.4 \\
    % Coefficient of $\mathcal{L}_\text{Recon}$ ($\lambda=0$)  & 69.3 & 32.5\\
    % Coefficient of $\mathcal{L}_\text{Recon}$ ($\lambda=1$)  & 70.7 & 38.4 \\
    % Coefficient of $\mathcal{L}_\text{Recon}$ ($\lambda=10$) & 67.2 & 37.2\\
    % Complete objective (with $\tau=0.5$) & 70.7 & 38.4 \\
    Complete objective & 75.9 & 76.3 \\
    %Lower temperature ($\tau=0.1$)  & 66.8  & 31.4 \\
    %Higher temperature ($\tau=1.0$) & 58.7 & 30.8 \\
    No  $\mathcal{L}_\text{KP}$     & 22.8 & 36.2 \\
    No  $\mathcal{L}_\text{TV}$     & 43.9 & 40.4 \\
    No  $\mathcal{L}_\text{Equi}$   & 64.8 & 65.6 \\
    No  $\mathcal{L}_\text{Recon}$  & 73.3 & 74.2 \\
    No  $\mathcal{L}_\text{Parts}$  & 73.6 & 73.5 \\
    \bottomrule
    \end{tabular}
    }
    % \vspace{-1.2em}
\end{table}


\begin{figure}[!ht]
 \centering
%  \vspace{-1em}
 \includegraphics[width=\linewidth,trim={0 0 0 0}, clip]{imgs/Challenges.pdf}
 \vspace{-1em}
 \caption{\footnotesize \textbf{Limitations.} Top row shows that our model can map left part of object in source to right part of object in target when object is symmetric. Bottom row shows that our model fails for very large out-of-plane rotations.}
 \label{fig:challenges}
 \vspace{-1em}
\end{figure}
\subsection{Limitations}
\noindent\textbf{Left-right ambiguity:}
One shortcoming of our approach is that it cannot differentiate well between left and right parts well for symmetric objects. We attribute this problem to the SSL models being invariant to left-right flips during their training. The top row of \cref{fig:challenges} shows that our model matches the left part of the cow torso in the source image to the right part of the torso in the target image (note left part of cow is not visible in the target image). Some heuristics used in prior works, such as flipping the source and target images and picking a combination that provides minimum total variation loss, could be used in our work as well. For brevity, we provide results without this heuristic.
\noindent\textbf{Large shape changes:}
Our model doesn't handle large viewpoint changes well, especially when there are few intermediate viewpoints. In the bottom row of \cref{fig:challenges}, we see that model is unable to warp the source cow image to target image even for the co-visible portions.
