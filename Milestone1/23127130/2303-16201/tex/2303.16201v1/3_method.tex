\section{\backronym~Framework}

\begin{figure*}
 \centering
 \includegraphics[width=\linewidth,trim={0 0 0 0}, clip]{imgs/wild-imposer-v3-Combined.pdf}
 \caption{\footnotesize \textbf{\backronym Architecure.} The alignment network $\A$ predicts canonical space coordinates for all pixels for all input images. Images can be reconstructed using a differentiable warp from the canonical space. In order to align semantically similar pixels from different images to the same location in the canonical space, we propose two primary loss functions $\loss_\text{KP}$ and $\loss_\text{Recon}$. Please refer to the \cref{sec:arch} for more details.}
 \label{fig:arch}
 \vspace{-0.5cm}
\end{figure*}



Given a collection of images of an object or an object category, our goal is to assign corresponding pixels in all the images to a unique location in a canonical space. By doing so, we can use this learned canonical space as an intermediary when mapping pixels from one image to any another image in the collection while guaranteeing global consistency. The absence of ground truth annotations, small size of the datasets we consider ($\sim$10 - 30 images) and the presence of occlusions and variations in shape, texture, viewpoint, and background lighting all serve to make this task highly challenging. We introduce a simple yet robust framework with a novel self-supervised contrastive loss function over image pairs, as well as auxiliary regularization losses on this learned canonical space to find consistent dense correspondences across the collection.

\subsection{Obtaining Pseudo-correspondences}
\label{sec:preprocessing}


Prior work has shown that deep features extracted from large pre-trained networks contain useful local semantic information~\citep{caron2020unsupervised,choudhury2021unsupervised,hung2019scops}. In this work, we use DINO~\citep{caron2020unsupervised} to extract these local semantic features. Note that these features are only extracted for obtaining pseudo-correspondences only once and are not used during the training.
Given a pair of images $\Ia$ and $\Ib$, we obtain feature maps $\Fa$ and $\Fb$ using DINO. Here $\Fa = \{\fa_i\}$ and $\Fb = \{\fb_i\}$ represents the sets of feature vectors $\f \in \mathbb{R}^\text{d}$ for all spatial locations $\p_i=(x,y) \in \mathbb{R}^2$. In practice, we obtain these feature maps at a coarser resolution, but for brevity, we do not introduce new notations for low-resolution feature maps. 
We define our pseudo keypoint correspondences, between the two images $\Ia$ and $\Ib$ as all pairs of locations of feature vectors that are mutual nearest neighbors, \ie,
%%%%%%%%
\begin{equation*}
    \{(\pa_i, \pb_j)\ |\ \big(\text{NN}(\fa_i, \Fb)=\fb_j\big) \ \wedge\  \big(\text{NN}(\fb_j, \Fa)=\fa_i\big)\}
\end{equation*}
%%%%%%%%
where $\text{NN}(\fa_i, \Fb)$ corresponds to the nearest neighbor of the normalized feature vector $\fa_i$ in the set of feature vectors $\Fb$.
The mutual nearest neighbors are usually noisy and sparse, and they serve as pseudo-correspondences for training our alignment network which we discuss next.


\subsection{Architecture}
\label{sec:arch}
\cref{fig:arch} gives the high-level overview of the framework. Formally, we are given an image collection consisting of $N$ images $\{\I^k\}_{k=1}^N$. We want to train an alignment network $\A$ that 
takes a single image as input at a time and outputs $\C = \A(\I)$, the canonical space coordinate map, of the image. The canonical space coordinate map $\C$ has the same spatial dimensions as the input $H \times W$ and contains $(u,v)$ coordinates in the shared canonical grid for that location. We parameterize this alignment network $\A: \mathbb{R}^{H \times W \times 3} \rightarrow \mathbb{R}^{H \times W \times 2}$ with a fully convolutional U-Net~\citep{ronneberger2015u} trained from scratch for the collection. Each pixel location $\p = (x, y)$ of this map consists of a 2-dimensional $\cc = (u, v)$ coordinate.

These coordinates corresponds to a location in a learned canonical grid $\G \in \mathbb{R}^{H' \times W' \times 4}$. The canonical grid $\G$ is also two-dimensional but can have arbitrary height and width $H' \times W'$, and is shared by all the images in the collection.
Each location in canonical grid $\G$ stores an $(r,g,b,\alpha)$ value which corresponds to colors $(r,g,b)$ and a probability $\alpha$ that this location corresponds to a foreground pixel in the image. The original image, and a foreground visibility mask can now be reconstructed using this shared canonical grid $\G$, canonical space mapping $\C$, and a differentiable warp operator commonly used in spatial transformer networks~\citep{jaderberg2015spatial}.
For the mapping to be meaningful, we want semantically similar points from different images to map to the same location in the canonical space. In the next section, we describe the training loss we devised to this end. %in order to learn a joint alignment of the image collection.

\subsection{Training Objectives}
\label{sec:training}
\noindent\textbf{Sparse pseudo-correspondence consistency.} The central goal of our framework is to ensure that semantically similar points in the images are aligned in the canonical space.  Recall from the \cref{sec:preprocessing}, that we pre-compute the pseudo-correspondences between all pairs of images using mutual nearest neighbors in the SSL feature space. Since SSL models are not trained for the task of correspondence, the pseudo-correspondences are noisy and sparse. Our first loss term is targeted at improving the accuracy of the correspondences by jointly aligning them for all pairwise combinations of images in our collection.
Formally, given an image pair $(\Ia, \Ib)$, we denote all the $K$ pseudo-correspondences in the pair by $\{\pa_i, \pb_i\}_{i=1}^K$. We apply the alignment network $\A$ to $\Ia$ and $\Ib$ independently to obtain the canonical space coordinates for each pixel in the pair, which we denote by $\{\cca_i, \ccb_i\}_{i=1}^K$. We want to map each keypoint location in $\Ia$ as close as possible to its counterpart in $\Ib$, while pushing it away from the mapping of other keypoints in $\Ib$. To achieve this, we define our first loss function $\loss_\text{KP}$ as
%%%%%%%%
\begin{align}
    \loss_\text{KP} &= -\sum_{i=1}^{K}{\log{\frac{\exp(-\parallel\cca_i - \ccb_i\parallel^2/\tau)}{\sum_{j=1}^{K}{\exp(-\parallel\cca_i - \ccb_j\parallel^2}/\tau)}}}
\end{align}
%%%%%%%%
where $\tau$ is a hyperparameter and is fixed to $1.0$ in all our experiments. $\loss_\text{KP}$ plays the key role in improving the accuracy of pseudo-correspondences jointly for all images in our collection. However, the number of pseudo-correspondences is still very small (typically 100-300 for an image pair) as compared to the number of pixel locations. Hence, this loss is sparse and we need to add extra regularization terms in order to learn dense alignment, that we will discuss next.

\myparagraph{Geometric transformation equivariance.}
In order to make our learned mapping dense, we introduce a geometric equivariance regularization term in our loss function. We apply a random synthetic geometric transformation $\T$ to a given image $\I$. Since the output of the alignment network $\A$ learns the canonical space coordinates for each location of input image, we can apply the same geometric transformation $\T$ to $\A(\I)$,  and enforce an equivariance loss as follows
%%%%%%%%
\begin{align}
    \loss_\text{Equi} &= \parallel\T(\A(\I)) - \A(\T(\I))\parallel
\end{align}
%%%%%%%%
where $\T$ is the geometric transformation. We choose thin plate spline (TPS) transformations~\citep{duchon1977splines} in our work, commonly used for image warps. $\loss_\text{KP}$ and $\loss_\text{Equi}$ serve as the two primary loss functions for the image set alignment problem, serving the purpose of making the pseudo-correspondences accurate and dense respectively. To further aid the training, we also propose the following auxiliary regularizations.

\myparagraph{Total variation regularization.}
In order to encourage smooth mappings from from each image to the canonical space, we add a total variation (TV) regularization to the computed mapping $\C$. We found TV loss to be crucial to mitigate degenerate solutions (see \cref{sec:ablation}):
%%%%%%%%
\begin{gather}
   \loss_\text{TV} = \loss_\text{Huber}(\Delta_x (\C-\Id)) + \loss_\text{Huber}(\Delta_y (\C-\Id))
\end{gather}
%%%%%%%%
where $\Id$ is the identity mapping (\ie each pixel $(x, y)$ in the image gets mapped to $(x,y)$ in the canonical space), $\Delta_x$ and $\Delta_y$ denote the partial derivatives under finite differences \wrt $x$ and $y$ dimensions, and $\loss_\text{Huber}$ denotes the Huber loss~\cite{huber1992robust}.

\begin{figure*}[!ht]
  \centering
  \vspace{-2em}
  \includegraphics[width=\linewidth]{imgs/Canonical.pdf}
  \vspace{-2.5em}
  \caption{\textbf{Visualizing canonical space alignment.} 
  For each dataset, the top row shows sample images from the dataset (composed of 10-30 images each). 
  The middle row shows part co-segmentations computed by DVD~\citep{amir2021deep}. DVD computes a coarse, discrete set of parts across the dataset. 
  The bottom row shows the continuous canonical space mapping computed by our method. Our canonical space mapping is 
  % simultaneously 
  smooth and consistent across the images for each dataset/collection.}
  \vspace{-1em}
  \label{fig:canon}
\end{figure*}

\myparagraph{Reconstruction loss.} All the loss terms so far are computed on the canonical coordinates given by $\A$ and does not use the canonical grid $\G$. Recall that $\G$ is of the size  $H' \times W' \times 4$, and allows us to reconstruct each image as well as a foreground visibility mask via a differentiable warp operator ($\W$)~\citep{jaderberg2015spatial} such that $\Irec, \Mrec=\W(\G, \C)$.
This allows us to compute a per image reconstruction loss using the original and reconstructed images. However a simple $L_1$ or $L_2$ loss will not suffice since $\G$ is shared for all the images in the collection and these images may come from wildly different backgrounds and lighting conditions.
Furthermore, the two images might contain two different instances of the same object class, and may have different textures, shapes, and viewpoints.
We instead minimize the perceptual (LPIPS) loss~\citep{zhang2018unreasonable} which measures a perceptual patch level similarity between images.
For the reconstructed mask, we compute pixel-wise binary cross entropy (BCE) loss using the image foregrounds obtained with co-segmentation~\cite{amir2021deep}.
%%%%%%%%
\begin{gather}
   \Irec, \Mrec = \W(\G, \C) \nonumber \\
   \loss_\text{Recon} = \text{LPIPS}(\I, \Irec) + \text{BCE}(\M, \Mrec)
\end{gather}

\noindent\textbf{Consistent part alignment.}
For our final auxiliary loss, we obtain  part co-segmentation maps from the images~\citep{amir2021deep,choudhury2021unsupervised} by clustering deep ViT features into $S$ semantic parts, then running GrabCut~\citep{rother2004grabcut} to smoothen the part boundaries. Our hypothesis is that semantically similar parts in the images should get mapped to similar location in $\G$, 
Formally, for each image $\I \in \mathbb{R}^{H\times W\times 3}$, we obtain semantic part masks as a binary matrix, $\textbf{P} \in \mathbb{R}^{H\times W \times S}$. Since we want a part across the image set to map to a compact location in the canonical space, we minimize the variance of the canonical space coordinates for all pixels belonging to a part:
%%%%%%%%
\begin{align}
% \vspace{-2cm}
\label{eq:parts}
    \loss_\text{Parts} &= \sum_{s=1}^S \frac{1}{N_s}\sum_i \parallel \cc_i^s - \mathbb{E}(\cc^s) \parallel^2
% \vspace{-2cm}
\end{align}
%%%%%%%%
where $N_s$ is the number of pixels belonging to the part, $\cc_i^s$ is the canonical coordinates of $i^\text{th}$ pixel location belonging to the $s^\text{th}$ part, and $\mathbb{E}(\cc_i^s)$ is the centroid of the $s^\text{th}$ part.
We fix the number of parts to 8 in all our experiments. Alternately, the number of parts can be computed using the elbow method~\citep{thorndike1953belongs} at the expense of additional compute. 
