% \vspace{-10mm}
\section{Introduction}
\label{sec:intro}
\vspace{-1.5mm}

% Problem and its importance
Given an image of a car, we as humans, can easily map corresponding pixels between this car and an arbitrary collection of car images. Our visual system is able to achieve this (rather impressive) feat using a multitude of cues - low level photometric consistency, high level visual grouping and our priors on cars as an object category (shape, pose, materials, illumination etc.). The above is also true for an image of a ``never-before-seen" object (as opposed to a common object category such as cars) where humans demonstrate surprisingly robust generalization despite lacking an object or category specific priors~\citep{biederman1987recognition}. These correspondences in turn inform downstream inferences about the object such as shape, affordances, and more. In this work, we tackle this problem of ``low-shot dense correspondence" -- \ie given only a small in-the-wild image collection ($\sim$10--30 images) of an object or object category, we recover dense and consistent correspondences across the entire collection.

Prior works addressing this problem of dense alignment in ``in-the-wild" image collections assume availability of annotated keypoint matches and image pairs~\citep{min2019hyperpixel,cho2021cats}, a mesh of the object~\citep{kulkarni2020articulation,zhang2021ners}, or a very large collection of images of the object~\citep{peebles2022gan,mu2022coordgan}. These assumptions often do not hold for the long-tail of objects that exist in real world imagery. This long-tail is unavoidable; no matter how many new images we annotate, we will keep uncovering new and rare categories of objects. In our work, we show that it is possible to achieve dense correspondence of small in-the-wild image collections without any manual annotations by leveraging the power of large self-supervised vision models.
Aligning these image sets can be useful for a wide range of applications such as edit propagation for images and videos, as well as downstream problems such as pose and shape estimation. 

In the presence of a limited number of samples and a high-dimensional search space, dense correspondence and joint alignment of an image set is a challenging optimization problem. We draw inspiration from classical image alignment methods~\citep{szeliski2007image,learned2005data} where images are warped (or congealed) to a consistent canonical pose before classification using simple transformations, as well as recent works on per-image-set optimization~\citep{ulyanov2018deep,mildenhall2021nerf,kasten2021layered}, where the inductive model biases coupled with additional regularization allows for learning a good solution with self-supervision. Our framework, dubbed \backronym, consists of a small image-to-image network which predicts a dense per-pixel mapping from the image to a two-dimensional canonical grid.  This canonical grid is parameterized as a multi-channel learned embedding and stores an RGB color along with an alpha value
indicating whether the location represents the object or the background. We devise a novel contrastive loss 
function to ensure that semantic keypoints from different images map to a consistent location in canonical space.

The key contribution of our work is to exploit noisy and sparse pseudo-correspondences between a pair of images and extend them to learn consistent dense correspondences across the image collection. These pseudo-correspondences can be obtained using any of the large self-supervised learning (SSL) models~\citep{chen2020simple, caron2020unsupervised, chen2020exploring, misra2020self, he2020momentum, grill2020bootstrap, radford2021learning} which learn without explicit labels on large internet-scale data. 
In order to make them accurate, we enforce pair-wise consistency across the image collection with an alignment network and a self-supervised keypoint consistency loss. Further, we introduce additional regularization via equivariance and reconstruction terms to get dense correspondences across collection. 

\cref{fig:teaser} demonstrates the dense and consistent mapping learned by our model for two image sets. We also evaluate our method on 18 image categories in SPair-71k~\citep{min2019spair} dataset, 4 categories in PF-Willow~\citep{ham2016}, 3 fine-grained categories in the CUB~\citep{wah2011caltech}, as well as 5 collections in SAMURAI~\citep{boss2022samurai} datasets and show that \backronym is competitive against unsupervised keypoint correspondence approaches, and often outperforms them. An additional advantage of learning a joint canonical mapping is that our method suffers significantly less drift when propagating keypoints on a sequence of images (instead of just a single image pair). In order to evaluate the keypoint consistency over a sequence of $k$ images, we propose a new metric $\kcycle$ (or k-cycle PCK) in \cref{sec:quant_consistent}
and show that our method outperforms existing methods by over 20\% at both the low and high precision settings. In summary, our contributions are as follows:
\begin{itemize}[leftmargin=*,itemsep=0em]
\item We introduce a test-time optimization technique to recover consistent dense correspondence maps over a small collection of in-the-wild images.
\item We design a novel loss function and several regularization terms to encourage mapping to be consistent across multiple images in a given collection.
\item We perform extensive quantitative and qualitative evaluations on 4 different datasets (spanning 30 object categories) to show that our method is competitive with the unsupervised methods, often outperforming them.
\item We propose a novel metric $\kcycle$ to evaluate consistency of keypoint propagation over a sequence of images, which is not captured by traditional metrics such as PCK.
\end{itemize}

