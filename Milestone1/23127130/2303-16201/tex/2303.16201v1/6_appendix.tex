\section{Implementation Details}

\subsection{$\A$ Architecture}

We use a small U-Net architecture~\citep{ronneberger2015u} to represent $\A$ consisting of four downscaling fully convolutional blocks and four upscaling fully convolutional blocks. Output of downscaling blocks is concatenated to the upscaling blocks, as is typical in U-Nets. The size and parameter details of each of the blocks is provided in \Cref{tab:unet}.

Output of the final layer has two channels predicting the $x$ and $y$ canonical space coordinates for each pixel. The canonical grid is a learned embedding of dimension $256\times256\times4$. We fixed the learning rate for the network to $0.001$ and train the entire network end to end for $20,000$ iterations with a batch size of 20 on a single GPU.

\begin{table}[!htb]
    \centering
    \footnotesize
    \caption{U-Net architecture for $\A$ - It is a fully convolutional architecture, consisting of an Input block, Output block, four Up blocks, and four Down blocks. Each block consists of Up, Down, and DoubleConv Layers as shown below. Each DoubleConv, Up, and Down blocks is parameterized by number of input and output channels \ie, $(C_{in}, C_{out})$. Each Conv2D is represented by $(C_{in}, C_{out}, \text{kernel}, \text{stride}, \text{pad})$. BN stands for batch normalization layer and has $C_{out}$ parameters. ReLU are Rectified Linear Units without any parameters.}
    \begin{tabular}{@{}l|lc@{}}
    \toprule
    Blocks & Layers & Output Size\\
    \midrule
    
    Input  & DoubleConv $(3, 32)$    & $32 \times 128 \times 128$  \\
    Down-1 & Down       $(32, 64)$   & $64 \times 64 \times 64$  \\
    Down-2 & Down       $(64, 128)$  & $128 \times 32 \times 32$ \\
    Down-3 & Down       $(128, 256)$ & $256 \times 16 \times 16$  \\
    Down-4 & Down       $(256, 512)$ & $512 \times 8 \times 8$  \\
    Up-1   & Up         $(512, 128)$ & $256 \times 16 \times 16$ \\
    Up-2   & Up         $(256, 64)$ & $128 \times 32 \times 32$  \\
    Up-3   & Up         $(128, 32)$  & $64 \times 64 \times 64$ \\
    Up-4   & Up         $(64, 32)$   & $32 \times 128 \times 128$\\
    Output & DoubleConv $(32, 4)$    & $4 \times 256 \times 256$ \\
    \midrule
    DoubleConv & $\begin{array}{@{}l}  \text{Conv2D}(C_{in}, C_{out}, 3, 1, 1) \\ \text{BN}(C_{out}) \\ \text{ReLU}  \\   \text{Conv2D}(C_{in}, C_{out}, 3, 1, 1)\\ \text{BN}(C_{out}) \\ \text{ReLU} \end{array}$ &\\
    \midrule
    Down & $\begin{array}{@{}l}  \text{MaxPool2D}(2) \\ \text{DoubleConv}(C_{in}, C_{out}) \end{array}$ &\\
    \midrule
    Up & $\begin{array}{@{}l}  \text{BilinearUpsample}(2) \\ \text{DoubleConv}(C_{in}, C_{out}) \end{array}$ &\\
    % $\#$ Parameters & 888,192  \\
    \bottomrule
    \end{tabular}
    %}
    \label{tab:unet}
\end{table}

\subsection{Canonical grid $\G$}
The canonical grid $\G$ consists of a simple $256 \times 256\times 4$ feature grid which is learned during the training with the same learning rate and optimizer as the alignment network $\A$. Each location in $\G$ stores an $(r,g,b,\alpha)$ value which corresponds to colors $(r,g,b)$ and a probability $\alpha$ that this location corresponds to a foreground pixel in the image.

\subsection{Loss terms}

Recall that our overall objective function comprises 5 different loss terms of which $\loss_\text{KP}$, $\loss_\text{Equi}$, and $\loss_\text{TV}$ are applied to canonical space coordinates $\C$ and update only the parameters of alignment network $\A$ (and not the canonical grid $\G$). $\loss_\text{Recon}$ and $\loss_\text{Parts}$ can backpropagate gradients to both the alignment network and the canonical grid. In all our experiments, on all 4 datasets and their respective categories, we use the same set of weight coefficients (except for the ablation study in Section 4, where we make the coefficients zero one at a time). We set of the coefficients for different loss terms as following: $\lambda_\text{KP}=10$, $\lambda_\text{Equi}=1$, $\lambda_\text{TV}=9000$, $\lambda_\text{Recon}=1$, and $\lambda_\text{Parts}=10$. We observed that our framework is robust to the choice of hyperparameters. We can further increase the PCK performance by setting per-category hyperparameters, however, per-category (or per-collection) tuning is not ideal for scaling the model to a large number of image collections. Hence, we choose to report all our numbers with a fixed set of hyperparameters. % for consistency.

\subsection{Choice of SSL for pseudo-correspondences.} In our experiments, we obtain initial set of pseudo-correspondences by finding mutual nearest neighbors from frozen DINO (ViT-S/8) network. Note that DINO is not trained or fine-tuned in our experiments. Our alignment network $\A$, which is much smaller than DINO, is trained from scratch. This is also in contrast with other weakly supervised techniques such as PMD which uses ResNet-101 / VGG-16 ($> 40\text{M}$ params). We observe that performance of our framework can be improved further by using better pseudo-correspondences. \cref{tab:ablation_bb} shows ASIC results when obtaining pseudo-correspondences from 3 different ViT architectures.

\begin{table}[!ht]
\centering
    % \vspace{-0.8em}
    \caption{Pseudo-correspondences Ablation on CUB-001}
    % \vspace{-0.8em}
    \label{tab:ablation_bb}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ lcc|cc } 
    \toprule
    \multirow{2}{*}{\textbf{Architecture}} & \multirow{2}{*}{\# params} & ImageNet Top-1 & DVD & ASIC (ours) \\
    &  & (Accuracy) & PCK@$0.1$ & PCK@$0.1$  \\
    \midrule
    ViT-S/16        & 21M & 77.0 & 59.8 & \textbf{63.7} \\
    ViT-B/8         & 85M & 80.1 & 66.4 & \textbf{74.9} \\
    ViT-S/8 (paper) & 21M & 79.7 & 66.8 & \textbf{71.8} \\
    \bottomrule
    \end{tabular}
    }
    % \vspace{-1.2em}
\end{table}


\section{Visualizing the Canonical Space}
Recall that in Section 4 of the paper, we showed the canonical space mapping for various datasets learned by our model.
Here we provide further details of the canonical space mapping.
Specifically we first show the region in 2D space where each point in the image is getting mapped in \Cref{subsec:colormap}.
Next we show the RGB grid that is learned by our model.


\begin{figure}[!ht]
 \centering
 \includegraphics[width=\linewidth,trim={0 0 0 0}, clip]{imgs/Colormap.pdf}
 \caption{\footnotesize \textbf{Colormap for canonical space visualization.} We use the colormap shown in the top row to represent the canonical space. Based on the canonical space coordinates predicted by our model for each pixel, we copy (or more precisely splat) the colors from the canonical space colormap to the original image. Each row shows the mapping learned by the model for different datasets.}
 \label{fig:colormap}
\end{figure}

\subsection{With colormap}
\label{subsec:colormap}
First, we reproduce the results from Section 4 here, along with the colormap of canonical space used to visualize them in \Cref{fig:colormap}. Note that we train a different model for each dataset. The figure shows that the semantically similar parts of objects get mapped to nearby location in the canonical space. Our model is able to learn a smooth mapping for each object.

\subsection{With learned RGB Grid}
Our method also learns an RGB grid. \Cref{fig:grid} shows the grid learned for 4 different datasets. We observe that while our grid is not interpretable, there are distinct patterns that emerge for each dataset. Specifically, one can observe wheel-like shapes in the bicycle grid, and a cube in the train canonical grid. We attribute the weak interpretability of the learned grid to the large variability in the challenging in-the-wild images, where images may consist of different instances of an object category in very different poses, articulations, shapes, textures, background, and lighting. The collections we used are also very small (5-25 images). Further while our alignment network ensures that the pseudo-correspondences across images land at the same location in the canonical space, nearby points within the same image can still map to far away locations in the canonical space.
Making the grid more interpretable could be useful for better understanding of the model's capabilities and limitations. For instance, in the case of GANgealing~\cite{peebles2022gan}, training a GAN on a large dataset of cats ($\sim$1.5M images), they are able to learn a canonical atlas which looks like face of a cat. This allows them to use canonical atlas as the template for image editing and edit propagation templates (although, a limitation of this approach is that it doesn't allow editing any parts other than the face of a cat).

\begin{figure}[!ht]
 \centering
 \includegraphics[width=\linewidth,trim={0 0 0 0}, clip]{imgs/LearnedGrid.pdf}
 \caption{Sample images from the dataset in the left four columns, followed by the mean image of the dataset. The last column shows the joint canonical grid learned by the model.}
 \label{fig:grid}
\end{figure}


\section{Results on different k-values and all datasets for $\kcycle$}

We share the results for of $k \in \{2,3,4\}$ and plot $\kcycle$ for all the datasets (with groundtruth keypoint annotations) we considered in our experiments. \Cref{fig:2cycle,fig:3cycle,fig:4cycle} show the comparison between our method and DVD. Note that DVD is also referred to as DINO + NN (where NN stands for nearest neighbors) in the main paper to clarify the strategy used to find the correspondences. Our method consistently outperforms the baseline, at both small and large values of $\alphapck$ (which corresponds to the coarse and fine precision or accuracy of the transfer).


\begin{figure*}[!ht]
 \centering
 \begin{minipage}{\textwidth} 
 \includegraphics[width=\linewidth,trim={0 0 0 0}, clip]{imgs/2_cycle_pck.pdf}
 \caption{$\mathbf{2}\cycle$ for three CUB-200 categories and 18 SPair-71k categories (test split)}
 \label{fig:2cycle}
 \vspace{3em}
 \end{minipage}

\begin{minipage}{\textwidth} 
\includegraphics[width=\linewidth,trim={0 0 0 0}, clip]{imgs/3_cycle_pck.pdf}
 \caption{ $\mathbf{3}\cycle$ for three CUB-200 categories and 18 SPair-71k categories (test split) }
 \label{fig:3cycle}
 \vspace{3em}
\end{minipage}

\begin{minipage}{\textwidth} 
 \centering
 \includegraphics[width=\linewidth,trim={0 0 0 0}, clip]{imgs/4_cycle_pck.pdf}
 \caption{$\mathbf{4}\cycle$ for three CUB-200 categories and 18 SPair-71k categories (test split)}
 \label{fig:4cycle}
 \vspace{3em}
 \end{minipage}
\end{figure*}
