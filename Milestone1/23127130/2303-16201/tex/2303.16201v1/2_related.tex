\section{Related Work}
\label{sec:related}

\noindent \textbf{Correspondence between image pairs.}
Keypoint matching or correspondence between images is one of the oldest 
tasks in computer vision. Some very early works focused on finding dense optical flow~\citep{horn1981determining,black1993framework,beauchemin1995computation} between pairs of consecutive images in videos via a variational framework to optimize flow based on pixel intensities. 
Sparse keypoint matching, e.g., using SIFT descriptors~\citep{lowe2004sift}, also gained importance due to applications in tracking~\citep{lucas1981iterative,tomasi1991detection} and structure from motion (SfM)~\citep{frahm2010building,agarwal2011building,schonberger2016structure}. 
SIFT Flow~\citep{liu2010sift} proposed the idea of using SIFT descriptors for dense alignment between image pairs. 
Initial deep learning based correspondence works~\citep{choy2016universal,kim2017fcss,detone2018superpoint} replaced SIFT with deep features. With the availability of labeled datasets, a number of works have performed end-to-end matching with deep networks~\citep{min2019hyperpixel,liu2020semantic,li2021probabilistic,jiang2021cotr,sun2021loftr,sarlin2020superglue,rocco2018neighbourhood,li2020correspondence,zhao2021multi,min2021convolutional,lee2021patchmatch,huang2022learning}. However, a shortcoming of these aforementioned works is that they usually require large labeled datasets, and often fail to generalize on unseen objects or scenes.

\myparagraph{Joint alignment of image sets.}
The concept of a canonical image has long been used for the task of object detection via template matching~\citep{gavrila1998multi,ioffe2001probabilistic}. 
Learned-Miller~\etal~\citep{huang2007unsupervised,learned2005data} formalized the task of jointly aligning a set of images (i.e., congealing them) by continuously warping each image (\eg via affine transformations) to minimize the entropy distribution of the image set. 
\cite{huang2012learning} use deep features from multiple resolutions in place of hand-crafted features. 
GANgealing~\citep{peebles2022gan} extended this idea by constraining the canonical image to be the output of a pre-trained StyleGAN~\citep{karras2021alias,goodfellow2020generative}. 
In a similar vein, CoordGAN~\citep{mu2022coordgan} trains a structure-texture disentangled GAN with a canonical coordinate frame as input. 
Both of these works attempt to solve a similar tasks as ours, but are limited by data-hungry GAN training. Some works exploits 3D shape as a means for consistent dense correspondences across image collections~\citep{kulkarni2020articulation, kulkarni2019csm, cmrKanazawa18,yao2022lassie} but require access to additional signals such as category specific 3D templates, segmentation masks or keypoint correspondences. In contrast, our work attempts to learn dense correspondences in a low-shot setting where GAN training is infeasible and in the absence of additional training signals. As mentioned before, we do so primarily by leveraging large pre-trained SSL models as our source of semantic priors on general imagery.

\myparagraph{Self-supervised correspondence discovery.}
To overcome the lack of large datasets with ground-truth correspondence, recent work seeks to combine the idea of distilling deep features from a network trained with self-supervision on large-scale image datasets. Some of these works optimize for proxy losses computed with known  transformations~\citep{seo2018attentive, novotny2018self, truong2021warp,thewlis2017unsupervised,aygun2022demystifying,thewlis2019unsupervised,jeon2018parn,rocco2018end,kim2019semantic}. Like these methods, we also train our network to be equivariant to synthetic geometric transformations. However, a key difference is that we also train with pseudo-correspondences `across' real images, which allows the method to generalize better and build a consistent mapping across the given image collection.

Deep Matching Prior ~\citep{hong2021deep}
and Neural Best Buddies~\citep{aberman2018neural} optimize for only a single pair of images to match deep features of one image to another. More recently PSCNet~\cite{jeon2021pyramidal} and Neural Congealing~\citep{ofri2023neural} train large networks for simultaneously matching deep features for image pairs by learning a flow from image to image, and image to canonical space respectively. However, these methods have limited flexibility in the deformation space and do not generalize well to out of plane rotations present in datasets such as SPair-71k. We allow our model to map different image regions arbitrarily to different parts of the canonical space. In \cref{tab:spair_pck}, we show that this allows us to generate more accurate correspondences and generalize to more object categories.
