\documentclass[10pt]{article} % For LaTeX2e
%\usepackage{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

% START - Packages that did not come with the TMLR template
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{algcompatible}
\usepackage{float}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
% END - Packages that did not come with the TMLR template

\usepackage{hyperref}
\usepackage{url}


\title{Positive Unlabeled Learning Selected Not At Random \mbox{(PULSNAR):} class proportion estimation when the SCAR assumption does not hold}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{\name Praveen Kumar \email pkumar81@unm.edu \\
      \addr Department of Internal Medicine\\
      University of New Mexico, Albuquerque, NM, USA
      \AND
      \name Christophe G. Lambert \email cglambert@salud.unm.edu \\
      \addr Department of Internal Medicine\\
      University of New Mexico, Albuquerque, NM, USA
      }

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{MM}  % Insert correct month for camera-ready version
\def\year{YYYY} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version


\begin{document}


\maketitle

\begin{abstract}
Positive and Unlabeled (PU) learning is a type of semi-supervised binary classification where the machine learning algorithm differentiates between a set of positive instances (labeled) and a set of both positive and negative instances (unlabeled). PU learning has broad applications in settings where confirmed negatives are unavailable or difficult to obtain, and there is value in discovering positives among the unlabeled (e.g., viable drugs among untested compounds). Most PU learning algorithms make the \emph{selected completely at random} (SCAR) assumption, namely that positives are selected independently of their features. However, in many real-world applications, such as healthcare, positives are not SCAR (e.g., severe cases are more likely to be diagnosed), leading to a poor estimate of the proportion, $\alpha$, of positives among unlabeled examples and poor model calibration, resulting in an uncertain decision threshold for selecting positives. PU learning algorithms can estimate $\alpha$ or the probability of an individual unlabeled instance being positive or both. We propose two PU learning algorithms to estimate $\alpha$, calculate calibrated probabilities for PU instances, and improve classification metrics: i) PULSCAR (positive unlabeled learning selected completely at random), and ii) PULSNAR (positive unlabeled learning selected not at random). PULSNAR uses a divide-and-conquer approach that creates and solves several SCAR-like sub-problems using PULSCAR. In our experiments, PULSNAR outperformed state-of-the-art approaches on both synthetic and real-world benchmark datasets.
\end{abstract}

\section{Introduction}
\label{introduction}

In a standard binary supervised classification problem, the classifier (e.g., decision trees, support vector machines, etc.) is given training instances $\mathcal{X}$ with features $x$ and their labels $y=0$ (negative) or $y=1$ (positive). The classifier learns a model $f :  \mathcal{X} \rightarrow 0, 1$, which classifies an unlabeled instance as positive or negative based on $x$. It is often challenging, expensive, and even impossible to annotate large datasets in real-world applications \cite{pulsnar_1}, and frequently only positive instances are labeled. Unlabeled instances with their features can be classified via positive and unlabeled (PU) learning \cite{pulsnar_1, pulsnar_2}. Some of the PU learning literature focuses on improving classification metrics, and others focus on the problem of estimating the fraction, $\alpha$, of positives among the unlabeled instances. Although this work focuses on the latter, calibration and enhancing classification performance are also addressed.

PU learning problems abound in many domains \cite{pulsnar_50}. For instance, in electronic healthcare records, the lack of a diagnosis code doesn't confirm a patient's negative disease status, as negatives are not routinely recorded, making traditional supervised learning impractical. Much medical literature is dedicated to estimating disease incidence and prevalence but contends with incomplete medical assessment and recording. The potential to assess disease incidence without costly in-person assessment or chart reviews could have substantial public health benefits. In market research, one typically has a modest set of positives, say of customers or buyers of a product, has a set of attributes over both the positives and a large population of unlabeled people of size $N$, and wishes to establish the size of the addressable market, $\alpha N$.

The majority of PU learning algorithms use the \emph{selected completely at random} (SCAR) assumption, which states that the labeled positive examples are randomly selected from the universe of positives. That is, the labeling probability of any positive instance is constant \cite{pulsnar_2}. This assumption may fail in real-world applications. For example, in email spam detection, positive instances labeled from an earlier time period could differ from later spam due to adaptive adversaries. 

Although some PU learning algorithms have shown promising performance on different machine learning (ML) benchmark SCAR datasets, the development of PU learning algorithms to estimate the extent of undercoding in large and highly imbalanced \emph{selected not at random} (SNAR) real-world data remains an active research area. Class imbalance in a PU setting generally means the number of unlabeled instances is large compared to the labeled positive examples. Also, current PU learning approaches have rarely explored how to calculate well-calibrated probabilities for PU examples in SCAR and SNAR settings. In addition, few PU algorithms have been assessed when $\alpha$ is small ($\leq 5\%$), where performance is expected to suffer.

In this paper, we propose a PU learning approach to estimate $\alpha$ when positives are SCAR or SNAR, and evaluating its performance in simulated and real data. We assess the performance with class imbalance in both modest and large datasets and over a rigorous $\alpha$ range. Our contributions are summarized as follows:
\begin{enumerate}
    \itemsep0em
    \item We propose PULSCAR, a PU learning algorithm for estimating $\alpha$ when the SCAR assumption holds. It uses kernel density estimates of the positive and unlabeled distributions of ML probabilities to estimate $\alpha$. The algorithm employs the beta distribution to estimate density and introduces an objective function whose derivative maximum provides a rapid, robust estimate of $\alpha$.
    \item We propose PULSNAR, a PU learning algorithm for estimating $\alpha$ when the positives are SNAR, that uses a novel clustering approach to divide the positives into several subsets that can have separate $\alpha$ estimates versus the unlabeled. These sub-problems are more SCAR-like and are solved with PULSCAR.
    \item We propose methods to calibrate the probabilities of PU examples to their true (unknown) labels and improve the classification performance in SCAR and SNAR settings.
\end{enumerate}


\section{Related work}
\label{sectionRelatedWork}
Early PU learning methods \cite{pulsnar_12, pulsnar_13, pulsnar_14} generally followed a two-step heuristic: i) identify strong negative examples from the unlabeled set, and then ii) apply an ML algorithm to given positive and identified negative examples. In contrast, \cite{pulsnar_15} extracted high-quality positive and negative examples from the unlabeled set and then applied classifiers to those data. Some recent work iteratively identifies better negatives \cite{pulsnar_47}, or combines negative-unlabeled learning with unlabeled-unlabeled learning \cite{pulsnar_44}. 

The following studies in PU learning focused on estimating the proportion of positives among the unlabeled examples with/without the PU classifier. These studies predominantly centered around the SCAR assumption. \cite{pulsnar_2} introduced the SCAR assumption and proposed a PU method to estimate the mixture proportion under the SCAR assumption. By partially matching the class-conditional density of the positive class to the input density under Pearson divergence minimization, \cite{pulsnar_20} estimated the mixture coefficient. \cite{pulsnar_21} proposed a nonparametric class prior estimation technique, AlphaMax, using two-component mixture models. The kernel embedding approaches KM1 and KM2 \cite{pulsnar_22} showed that the algorithm for mixture proportion estimation converges to the true prior under certain assumptions. Estimating the class prior through decision tree induction (TICE) \cite{pulsnar_23} provides a lower bound for label frequency under the SCAR assumption. Using the SCAR assumption, DEDPUL \cite{pulsnar_24} estimates $\alpha$ by applying a compute-intensive EM-algorithm to probability densities; the method also returns uncalibrated probabilities.

The following studies employed different approaches to learn a classifier from PU data. \cite{pulsnar_16} converts PU data learning into a noisy learning problem by designating all unlabeled instances as negatives. They employ a linear function to learn from these noisy examples using weighted logistic regression. Confident learning (CL) \cite{pulsnar_25} combines the principle of pruning noisy data, probabilistic thresholds to estimate noise, and sample ranking. Multi-Positive and Unlabeled Learning \cite{pulsnar_26} extends PU learning to multi-class labels. Oversampling the minority class \cite{pulsnar_27,pulsnar_28} or undersampling the majority class are not well-suited approaches for PU data due to contamination in the unlabeled set; \cite{pulsnar_31} uses a re-weighting strategy for imbalanced PU learning. 

Recent studies have focused on labeling/selection bias to address the SCAR assumption not holding. \cite{pulsnar_42, pulsnar_43} used propensity scores to address labeling bias and improve classification. Using the propensity score, based on a subset of features, as the labeling probability for positive examples, \cite{pulsnar_42} reduced the Selected At Random (SAR) problem into the SCAR problem to learn a classification model in the PU setting. The ``Labeling Bias Estimation'' approach was proposed by \cite{pulsnar_48} to label the data by establishing the relationship among the feature variables, ground-truth labels, and labeling conditions.

\iffalse
\subsection{Old related work}
Early PU learning \cite{pulsnar_12, pulsnar_13, pulsnar_14} generally followed a two-step heuristic: i) identify strong negative examples from the unlabeled set, and then ii) apply an ML algorithm to given positive and identified negative examples. Some recent work iteratively identifies better negatives \cite{pulsnar_47}, or combines negative-unlabeled learning with unlabeled-unlabeled learning \cite{pulsnar_44}. Instead of extracting only strong negative examples from the unlabeled set, \cite{pulsnar_15} extracted high-quality positive and negative examples from the unlabeled set and then applied classifiers to those data. \cite{pulsnar_16, pulsnar_17} assigned weights to the unlabeled examples to train a classifier. \cite{pulsnar_2} introduced the SCAR assumption. By partially matching the class-conditional density of the positive class to the input density under Pearson divergence, \cite{pulsnar_20} estimated the class prior. \cite{pulsnar_21} proposed a nonparametric class prior estimation technique, AlphaMax, using two-component mixture models. The kernel embedding approaches KM1 and KM2 \cite{pulsnar_22} showed that the algorithm for mixture proportion estimation converges to the true prior under certain assumptions. Estimating the class prior through decision tree induction \cite{pulsnar_23} provides a lower bound for label frequency under the SCAR assumption. DEDPUL \cite{pulsnar_24} assumes SCAR and uses probability densities to estimate $\alpha$ with a compute-intensive EM-algorithm; the method does not produce calibrated probabilities. Confident learning (CL) \cite{pulsnar_25} combines the principle of pruning noisy data, probabilistic thresholds to estimate noise, and sample ranking. Multi-Positive and Unlabeled Learning \cite{pulsnar_26} extends PU learning to multi-class labels. Oversampling the minority class \cite{pulsnar_27,pulsnar_28} or undersampling the majority class are not well-suited approaches for PU data due to contamination in the unlabeled set; \cite{pulsnar_31} uses a re-weighting strategy for imbalanced PU learning. Recent studies have focused on labeling/selection bias to address the SCAR assumption not holding. \cite{pulsnar_42, pulsnar_43} used propensity scores to address labeling bias and improve classification. Using the propensity score, based on a subset of features, as the labeling probability for positive examples, \cite{pulsnar_42} reduced the Selected At Random (SAR) problem into the SCAR problem to learn a classification model in the PU setting. The ``Labeling Bias Estimation'' approach was proposed by \cite{pulsnar_48} to label the data by establishing the relationship among the feature variables, ground-truth labels, and labeling conditions.
\fi

\section{Problem Formulation and Algorithms}
\label{sectionProblemFormulation}
In this section, we explain: i) the SCAR and SNAR assumptions, ii) our PULSCAR algorithm for SCAR data and PULSNAR algorithm for SNAR data, iii) bandwidth estimation techniques, and iv) method to find the number of clusters in the labeled positive set. Our method to calibrate probabilities and enhance classification performance using PULSCAR/PULSNAR is in Appendix \ref{appendix1} and \ref{appendix2}, respectively.  

\subsection{SCAR assumption and SNAR assumption}
In PU learning, a positive or unlabeled example can be represented as a triplet ($x, y, s$) where ``$x$'' is a vector of attributes, ``$y$'' the actual class, and ``$s$'' a binary variable representing whether or not the example is labeled. If an example is labeled ($s=1$), it belongs to the positive class ($y=1$) i.e., $p(y=1|s=1)=1$. If unlabeled ($s=0$), it can belong to either class. Since only positive examples are labeled, $p(s=1|x, y=0)=0$ \cite{pulsnar_2}. Under the SCAR assumption, a labeled positive is an independent and identically distributed (i.i.d) example from the positive distribution, i.e., positives are selected independently of their attributes. Therefore, $p(s=1|x, y=1) = p(s=1|y=1)$ \cite{pulsnar_2}.

For a given dataset, $p(s=1|y=1)$ is a constant and is the fraction of labeled positives. If $|P|$ is the number of labeled positives, $|U|$ is the number of unlabeled examples, and $\alpha$ is the unknown fraction of positives in the unlabeled set, then 
\begin{gather}
p(s=1) = \frac{|P|}{|P|+|U|} \quad \text{and} \quad p(y=1) = \frac{|P|+\alpha |U|}{|P|+|U|} \nonumber \\
p(s=1|y=1) = \frac{p(y=1|s=1)p(s=1)}{p(y=1)} = \frac{p(s=1)}{p(y=1)} \quad \text{, since $p(y=1|s=1) = 1$} \nonumber\\
= \frac{|P|}{|P|+\alpha |U|}  \text{, which is a constant.}
\end{gather}

On the contrary, under the SNAR assumption, the probability that a positive example is labeled is not independent of its attributes. Stated formally, the assumption is that
$p(s=1|x, y=1) \neq p(s=1|y=1)$ i.e. $p(s=1|x, y=1)$ is not a constant, which can be proved by Bayes' rule (Appendix \ref{appendix0}). 

The SCAR assumption can hold when both labeled and unlabeled positives: a) are not subclass mixtures, sharing similar attributes; b) belong to \emph{k} subclasses ($1 \dots k$), with equal subclass proportions in both positive and unlabeled sets. Intra-subclass examples will have similar attributes, whereas the inter-subclass examples may not have similar attributes. E.g., in patients positive for diabetes, type 1 patients will be in one subclass, and type 2 patients will be in another. The SCAR assumption can fail when labeled and unlabeled positives are from \emph{k} subclasses, and the proportion of those subclasses is different in positive and unlabeled sets. Suppose both positive and unlabeled sets have subclass 1 and subclass 2 positives, and in the positive set, their ratio is 30:70. If the ratio is also 30:70 in the unlabeled set, the SCAR assumption will hold. If it was different, say 80:20, the SCAR assumption would not hold.

\subsubsection{PU data assumptions}
Positive and unlabeled examples in PU data can be either from a single source or two independent sources. In the \emph{single-training-set scenario (one-sample)}, the positive and unlabeled examples are selected from one dataset, and that dataset is an independent and identically distributed (i.i.d.) sample from the actual distribution. In the \emph{case-control scenario (two-sample)}, positive and unlabeled examples are assumed to have originated from two independent datasets, and the unlabeled dataset is an i.i.d. sample from the actual distribution \cite{pulsnar_2, pulsnar_3}.

Our PU algorithms involve running a machine learning model (PU classifier) on a set of combined positive and unlabeled instances, regardless of whether it is a one-sample or two-sample scenario. The goal is to obtain machine learning-predicted probabilities for all instances. Subsequently, we determine $\alpha$, the fraction of positives among the unlabeled examples. However, our algorithms make certain assumptions regarding the PU data, which include the following: $1)$ Positive examples have correct labels, i.e., no negative example is marked as positive. Only the unlabeled set has a mix of positives and negatives; $2)$ The unlabeled positive instances have counterparts (examples with similar features) in the labeled positive set.

As mentioned in the study by \cite{pulsnar_52}, the one-sample scenario is a special case of learning from noisy labels where only negative data are contaminated. Thus, in the one-sample scenario, unlabeled data can be regarded as negative-labeled data contaminated by positive data. Our first assumption satisfies the criteria for the one-sample scenario. Additionally, according to \cite{pulsnar_52} the unlabeled data of the case-control scenario can be made from positive and unlabeled data of the one-sample scenario. Our second assumption allows us to extend the applicability of our approaches to the two-sample scenario. Our approaches will underestimate $\alpha$ in both one-sample and two-sample scenarios if the second assumption does not hold.

\begin{figure}[ht]
\centering
\includegraphics[width=0.75\columnwidth]{allFigures/densityErrorPlot.png} 
\caption{\textbf{PULSCAR algorithm visual intuition}. PULSCAR finds the smallest $\alpha$ such that $f_u(x) - \alpha  f_p(x)$ is everywhere positive in [0 \dots 1]. A) Kernel density estimates for simulated data with $\alpha=10\%$ positives in the unlabeled set -- estimated negative density (blue) nearly equals the ground truth (green). B) Overweighting the positive density by $\alpha=15\%$ results in the estimated negative density (blue), $f_u(x) - \alpha f_p(x)$ dropping below zero. C) Underweighting the positive density by $\alpha=5\%$ results in the estimated negative density (blue) being higher than the ground truth (green). D) Objective function with estimated $\alpha=10.68\%$ selected where the finite-differences estimate of the slope is largest -- very close to ground truth $\alpha=10\%$.}
\label{fig:densityerrorplot}
\end{figure}

\subsection{Positive and Unlabeled Learning Selected Completely At Random (PULSCAR) Algorithm}

Given any ML algorithm, $\mathcal{A}(x)$, that generates [0\dots1] probabilities for the data based on covariates $x$, let $f_p(x)$, $f_n(x)$, and $f_u(x)$ be probability density functions (PDFs) corresponding to the probability distribution of positives, negatives, and unlabeled respectively. Let $\alpha$ be the unknown proportion of positives in the unlabeled, then 

\begin{align}\label{obj_func_prop}
 &f_u(x) = \alpha f_p(x) + (1-\alpha) f_n(x) \text{,}\quad \text{ using the law of total probability} && \nonumber \\ 
 &\Rightarrow 1 = \frac{\alpha f_p(x)}{f_u(x)} + \frac{(1-\alpha) f_n(x)}{f_u(x)} && \nonumber \\ 
 &\Rightarrow \frac{\alpha f_p(x)}{f_u(x)} = 1 - \frac{(1-\alpha) f_n(x)}{f_u(x)} && \nonumber \\ 
 &\Rightarrow 0 \leq \frac{\alpha f_p(x)}{f_u(x)} \leq 1, \text{ since } 0  \leq \alpha \leq 1 \text{ and }  f_n(x) \leq f_u(x) && \nonumber \\
 &\Rightarrow 0 \leq \alpha f_p(x) \leq f_u(x)
\end{align} 

From property \ref{obj_func_prop}, a key observation is that $\alpha f_p(x)$ should not exceed $f_u(x)$ anywhere, allowing one to place an upper bound on $\alpha$. 

PULSCAR estimates $\alpha$ by finding the value $\alpha$ where the following objective function maximally changes:

\begin{equation}\label{eq:obj_func}
    f(\alpha) = log(|\min(f_u(x) - \alpha f_p(x))|+ \epsilon) \text{, where } 
 \epsilon = |\min(f_p(x))| \text{ if } \min(f_p(x)) \neq 0 \text{, else } \epsilon = 10^{-10}
\end{equation}


Property \ref{obj_func_prop}, $\alpha f_p(x) \leq f_u(x)$, guided the reasoning behind the design choice of the objective function. The intuition behind the objective function is that $|\min(f_u(x) - \alpha f_p(x))|$ approaches zero at the point where $\alpha f_p(x)$ equals $f_u(x)$, see Figure \ref{fig:densityerrorplot}D. When we take the logarithm of $|\min(f_u(x) - \alpha f_p(x))|$, the resulting value tends toward $-\infty$ as $|\min(f_u(x) - \alpha f_p(x))|$ approaches zero. Consequently, when $|\min(f_u(x) - \alpha f_p(x))|$ is not zero, there is a maximum change in the value of $\log(|\min(f_u(x) - \alpha f_p(x))|)$ (from $-\infty$ to some value). That is why we locate the point where $\alpha f_p(x)$ equals $f_u(x)$ to place an upper bound on $\alpha$. The reason we use the logarithm is it steeply approaches $-\infty$ as $|\min(f_u(x) - \alpha f_p(x))|$ approaches zero. Adding $\epsilon$ to prevent $log(0)$ and using finite differences to find the max change in slope gives a robust estimator that is resilient to noise. The objective function may not be convex; if multiple points with the same maximal change occur, we take the one closest to zero as the $\alpha$ estimate. This approach eliminates the need for implementing an iterative solver technique, accounting in part for the speed of our algorithm. 

We use beta kernel density estimates on ML-predicted class 1 probabilities of positives and unlabeled to estimate $f_p(x)$ and $f_u(x)$. We use a finite difference approximation of the slope of $f(\alpha)$ to find its maximum. The value of $\alpha$ can also be determined visually by plotting the objective function (Figure \ref{fig:densityerrorplot}D); the sharp inflection point in the plot represents the value of $\alpha$. Algorithm \ref{alg:pulscar} shows the pseudocode of the PULSCAR algorithm to estimate $\alpha$ using the objective function based on probability densities. Algorithm \ref{alg:bandwidthestimate} is a subroutine to compute the beta kernel bandwidth. Full source code for our algorithms is provided as a supplemental document.


\begin{minipage}[t]{.48\linewidth}
\begin{algorithm}[H]
    \caption{PULSCAR Algorithm}
    \label{alg:pulscar}
    \textbf{Input}: X ($X_p \cup X_u$), y ($y_p \cup y_u$), n\_bins \\
    %\textbf{Parameter}: p, labels, bin\_method \\
    \textbf{Output}: estimated $\alpha$
    \begin{algorithmic}[1] %[1] enables line numbers
    \STATE predicted\_probabilities (p) $\leftarrow$ $\mathcal{A}(X,y)$
    \STATE p0 $\leftarrow$ p[y == 0]
    \STATE p1 $\leftarrow$ p[y == 1]
    \STATE estimation\_range $\leftarrow$ [0, 0.0001, 0.0002, ..., 1.0]
    \STATE bw $\leftarrow$ estimate\_bandwidth\_pu(p, n\_bins)
    \STATE $D_u$ $\leftarrow$ beta\_kernel(p0, bw, n\_bins)
    \STATE $D_p$ $\leftarrow$ beta\_kernel(p1, bw, n\_bins)
    \STATE $\epsilon \leftarrow |\min(D_p)|$
    \IF{$\epsilon$ = 0}
        \STATE $\epsilon \leftarrow 10^{-10}$
    \ENDIF
    \STATE len $\leftarrow$ length(estimation\_range)
    \STATE selected\_range $\leftarrow$ estimation\_range[2:len]
    \STATE $\alpha \leftarrow$ estimation\_range
    \STATE f($\alpha$) $\leftarrow \log(|\min(D_u - \alpha  D_p)| + \epsilon$)
    \STATE d $\leftarrow$ f'($\alpha$)
    \STATE i $\leftarrow$ where the value of d changes maximally 
    \STATE \textbf{return} selected\_range[i]
    \end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{.48\linewidth}
\vspace{0pt}
\begin{algorithm}[H]
    \caption{estimate\_bandwidth\_pu}
    \label{alg:bandwidthestimate}
    \textbf{Input}: predicted\_probabilities, n\_bins \\
    %\textbf{Parameter}: p, labels, bin\_method \\
    \textbf{Output}: bandwidth
    \begin{algorithmic}[1] %[1] enables line numbers
    \STATE preds $\leftarrow$ predicted\_probabilities
    \STATE bw $\in$ [0.001, 0.5]
    \STATE $D_{hist}$ $\leftarrow$ histogram(preds, n\_bins, 
    \STATEx density=True)
    \STATE $D_{beta}$ $\leftarrow$ beta\_kernel(preds, bw, n\-bins)
    \STATE \textbf{return} optimize(MeanSquaredError($D_{hist}$, 
    \STATEx $D_{beta}$))
    \end{algorithmic}
    \end{algorithm}
\end{minipage}


\subsection{Kernel Bandwidth estimation}\label{KernelBandwidthestimation}
A beta kernel estimator is used to create a smooth density estimate of both the positive and unlabeled ML probabilities, generating distributions over [$0 \dots1$], free of the problematic boundary biases of kernels (e.g. Gaussian) whose range extends outside that interval, adopting the approach of \cite{pulsnar_38}. Another problem with (faster) Gaussian kernel density implementations is that they often use polynomial approximations that can generate negative values in regions of low support, dramatically distorting $\alpha$ estimates which require non-negative probability distribution estimates. The beta PDF is as follows \cite{pulsnar_41}:
\begin{equation}
h(x, a, b) = \frac{\Gamma(a+b) x^{a-1} (1-x)^{b-1}}{\Gamma(a) \Gamma(b)},
\end{equation}
for x $\in$ [0,1], where $\Gamma$ is the gamma function, $a=1+\frac{z}{bw}$ and $b=1+\frac{1-z}{bw}$, with $z$ the bin location, and $bw$ the bandwidth.

Kernel bandwidth selection can also significantly influence $\alpha$ estimates: too narrow of a bandwidth can result in outliers driving poor estimates, and too wide of a bandwidth prevents distinguishing between distributions. We use a histogram bin count heuristic to generate a histogram density, then optimize the beta distribution bandwidth to best fit that histogram density.

\subsubsection{Bin count} Our implementation supports 4 well-known methods to determine the number of histogram bins: square root, Sturges' rule, Rice's rule, Scott's rule, and Freedmanâ€“Diaconis (FD) rule \cite{pulsnar_35}.

\subsubsection{Bandwidth estimation} We compute a histogram density using a bin count heuristic and a beta kernel density estimate at those bin centers using the ML probabilities of both the positive and unlabeled examples. We find the global minimum of the mean squared error (MSE) between the histogram and beta kernel densities using the scipy \textit{differential\_evolution()} optimizer \cite{pulsnar_41}, solving for the best bandwidth in the range [0.001...0.5]. That bandwidth is chosen for kernel density estimation in the PULSCAR algorithm. All experiments herein use MSE as the error metric, butthe Jensen-Shannon distance may also be employed.

\subsection{Positive and Unlabeled Learning Selected Not At Random (PULSNAR) Algorithm}
We propose a new PU learning algorithm (PULSNAR) to estimate the  $\alpha$ in SNAR data, i.e., labeled positives are not selected completely at random. PULSNAR uses a divide-and-conquer strategy for the SNAR data. It converts a SNAR problem into several sub-problems using an unsupervised learning method (clustering), each of which better approximates the SCAR assumption holding; then applies the PULSCAR algorithm to those sub-problems. The final alpha is computed by summing the alpha returned by the PULSCAR algorithm for each cluster.
\begin{gather}
 %\alpha = (|U| \alpha_1 +  |U| \alpha_2 + ... +  |U| \alpha_c)/|U|   \nonumber\\
 \alpha = \alpha_1 + \alpha_2 + ... + \alpha_c,  \quad \text{$c=$ number of clusters}
\end{gather}




Figure \ref{fig:PULSNAR_flowchart} visualizes the PULSNAR algorithm, and Algorithm \ref{alg:pulsnar} provides its pseudocode.


\subsubsection{Clustering rationale} 
Suppose both positive and unlabeled sets contain positives from $k$ subclasses ($1\dots k$). With selection bias (SNAR), the subclass proportions will differ between the sets, and thus the PDF of the labeled positives cannot be scaled by a uniform $\alpha$ to estimate positives among the unlabeled. The smallest subclass would drive an $\alpha$ underestimate with PULSCAR. To address this, we apply clustering to the labeled positives to split them into $c$ clusters. Clustering separates subclasses of positives, and if the assumption that subclass membership drives selection bias holds, PU data comprising examples from one cluster and the unlabeled set 
will approximate the SCAR assumption. Applying PULSCAR to each cluster of positives versus the unlabeled results in better estimates of the proportions of similar unlabeled positives (Figure \ref{fig:PULSNAR_flowchart}).

\subsubsection{Determining the number of clusters in the positive set}
We build an XGBoost \cite{pulsnar_5} model on all positive and unlabeled examples to determine the important features and their \emph{gain} scores. A \emph{gain} score measures the magnitude of the feature's contribution to the model. We select all labeled positives and then cluster them on those features scaled by their corresponding \emph{gain} score, using scikit\_learn's Gaussian mixture model (GMM) method. To establish the number of clusters (n\_components), we iterate n\_components over $1\ldots m$ (e.g., $m$=25)  and compute the Bayesian information criterion (BIC)\cite{pulsnar_6} for each clustering model. We use max\_iter=250, and covariance\_type=``full''. The other parameters are used with their default values. We implemented the ``Knee Point Detection in BIC'' algorithm, explained in \cite{pulsnar_36},  to find the number of clusters in the labeled positives.

\subsection{Calculating calibrated probabilities}
An approach to calibrate the ML-predicted probabilities of positive and unlabeled examples in the SCAR and SNAR data is explained in Appendix \ref{appendix1}.
\subsection{Improving classification performance}
An approach to improving PULSCAR and PULSNAR classification, based on flipping the highest probability $\alpha |U|$ unlabeled examples to 1, is explained in Appendix \ref{appendix2}.



\section{Experiments}
\label{sectionExperiments}
We evaluated our proposed PU learning algorithms in terms of $\alpha$ estimates, six classification performance metrics, and probability calibration. We used real-world ML benchmark datasets and synthetic data for our experiments. For real-world data, we used Bank \cite{pulsnar_7} and KDD Cup 2004 particle physics \cite{pulsnar_8} datasets as SCAR data and Statlog (Shuttle) \cite{pulsnar_9} and Firewall datasets \cite{pulsnar_10} as SNAR data. The synthetic (SCAR and SNAR) datasets were generated using the scikit-learn function \emph{make\_classification()} \cite{pulsnar_11}. We used XGBoost as a binary classifier in our proposed algorithms. To train the classifier on the imbalanced data, we used \emph{scale\_pos\_weight} parameter of XGBoost to scale the weight of the labeled positive examples by the factor $s=\frac{|U|}{|P|}$. We observed that if the ratio, $s$, of the majority class to the minority class is less than 50, handling class imbalance can be achieved by setting the \emph{scale\_pos\_weight} parameter of XGBoost to $s$. We also compared our methods with five recently published methods for PU learning: KM1 and KM2 \cite{pulsnar_22}, TICE \cite{pulsnar_23}, DEDPUL \cite{pulsnar_24} and CleanLab \cite{pulsnar_25}. KM1, KM2, and TICE algorithms were not scalable and failed to execute on large datasets, so we used smaller synthetic datasets to compare our method with these methods. We compared PULSNAR with only DEDPUL on large synthetic datasets (Appendix \ref{appendix3}). Also, \cite{pulsnar_24} previously demonstrated that DEDPUL outperformed KM and TICE algorithms on several UCI (University of California Irvine) ML benchmark and synthetic datasets.

\begin{minipage}[t]{.48\linewidth}
\vspace{0pt}
\begin{algorithm}[H]
    \caption{PULSNAR Algorithm}
    \label{alg:pulsnar}
    \textbf{Input}: X ($X_p \cup X_u$), y ($y_p \cup y_u$), n\_bins \\
    \textbf{Output}: estimated $\alpha$
    \begin{algorithmic}[1] %[1] enables line numbers
    \STATE feature\_importance ($v_1...v_k$), 
    \STATEx imp\_features ($x_1...x_k$) $\leftarrow$ $\mathcal{A}(X,y)$ 
    \STATE $x'_1...x'_k$ $\leftarrow$ $x_1 v_1...x_k v_k$
    \STATE $X'_p$ $\leftarrow$ $X_p$[$x'_1...x'_k$]
    \STATE clusters $s_1...s_c$ $\leftarrow$ GMM($X'_p$)
    \STATE $\alpha$ $\leftarrow$ 0
    \FOR{c in $s_1...s_c$}
        \STATE X' $\leftarrow$ $X_p[c] \cup X_u$
        \STATE y' $\leftarrow$ $y_p[c] \cup y_u$
        \STATE $\alpha$ $\leftarrow$ $\alpha$ + PULSCAR(X', y', n\_bins)
    \ENDFOR
    \STATE \textbf{return} $\alpha$
    \end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{.48\linewidth}
\vspace{0pt}
%\begin{figure}[H]
\includegraphics[width=0.80\columnwidth]{allFigures/PULSNARFlowchart.png}
%\end{figure}
\end{minipage}

\begin{figure}[H]
\caption{\textbf{Schematic of PULSNAR algorithm}. An ML model is trained and tested with 5-fold CV on all positive and unlabeled examples. The model covariates are scaled by their importance value. Positives are divided into c clusters using the scaled important covariates. c ML models are trained and tested with 5-fold CV on the records from a cluster and all unlabeled records. We estimate the proportions ($\alpha_1...\alpha_c$) of each subtype of positives in the unlabeled examples using PULSCAR. The sum of those estimates gives the overall fraction of positives in the unlabeled set. P = positive set, U = Unlabeled set.}
\label{fig:PULSNAR_flowchart}
\end{figure}



\subsection{Synthetic data}
We generated SCAR and SNAR PU datasets with different fractions of positives (1\%, 5\%, 10\%, 20\%, 30\%, 40\%, and 50\%) among the unlabeled examples to test the effectiveness of our proposed algorithms. For each fraction, we generated 40 datasets using sklearn's
\emph{make\_classification()} function with random seeds 0-39. The \emph{class\_sep} parameter of the function was used to specify the separability of data classes. Values nearer to 1.0 make the classification task easier; we used class\_sep=0.3 to create difficult classification problems. 

\subsubsection{SCAR data}
The datasets contained 2,000 positives (class 1) and 6,000 unlabeled (class 0) examples with 50 continuous features. The unlabeled set comprised $k\%$ positive examples with labels flipped to 0 and $(100-k)\%$ negative examples. 

\subsubsection{SNAR data}
We generated datasets with 6 labels (0-5), defining `0' as negative and 1-5 as positive subclasses. These datasets contained 2,000 positives (400 from each positive subclass) and 6,000 unlabeled examples with 50 continuous features. The unlabeled set comprised k\% positive examples with labels (1-5) flipped to 0 and (100-k)\% negative examples. The unlabeled positives were markedly SNAR, with the 5 subclasses comprising 1/31, 2/31, 4/31, 8/31, and 16/31 of the unlabeled positives. (e.g., in the unlabeled set with 20\% positives, negative: 4,800, label 1 positive: 39, label 2 positive: 77, label 3 positive: 155, label 4 positive: 310, label 5 positive: 619).

\subsection{SCAR ML Benchmark Datasets}
\subsubsection{UCI Bank dataset}
The dataset has 45,211 records (class 1: 5,289, class 0: 39,922) with 16 features. This dataset is a good example of data with class imbalance and mixed features. Since the features contain both numerical and categorical values, they were one-hot encoded \cite{pulsnar_49} using the scikit-learn function \emph{OneHotEncoder()} \cite{pulsnar_11}. The encoder derives the categories based on the unique values in each feature, resulting in 9,541 features. The ML classifier was applied to the encoded features.

\subsubsection{KDD Cup 2004 Particle Physics dataset}
The dataset contains two types of particles generated in high-energy collider experiments; 50,000 examples (class 1: 24,861, class 0: 25,139) with 78 numerical attributes. This dataset is a good example of balanced data.

In both datasets, class 1 records were used as positive, and class 0 records were used as unlabeled for the ML model. To add k\% positive examples to the unlabeled set, the labels of $m$ randomly selected positive records were flipped from 1 to 0, where $m = \frac{k |U|}{100-k}$.

\subsection{SNAR ML Benchmark Datasets}
\subsubsection{UCI Statlog (Shuttle) Dataset}
The dataset contains 43,500 records (class 1: 34,108, class 2: 37, class 3: 132, class 4: 6,748, class 5: 2,458, class 6: 6, class 7: 11) with 9 numerical attributes. This dataset is an example of data with multiclass and class imbalance. We used class 1 as unlabeled examples and the rest of the records as subclasses of positive examples for the ML model (positive: 9,392, unlabeled: 34,108).

\subsubsection{UCI Firewall dataset}
It is a multiclass dataset containing 65,532 records (`allow': 37,640, `deny': 14,987, `drop': 12,851, `reset-both': 54) with 12 numerical attributes. Class `allow' was used as unlabeled examples, and the others (`deny', `drop', `reset-both') were used as subclasses of positive examples for the ML model (positive: 27,892, unlabeled: 37,640). 

In both datasets, the majority of positives are from two classes (\emph{shuttle: class 4, 5; firewall: `deny', `drop'}). So, to add $k\%$ positive examples to the unlabeled set, we randomly selected some examples from the minor positive classes and the remaining examples from two major positive classes in equal proportion. Thus, the proportion of positives in the positive set differed from the unlabeled set.

\subsection{Estimation of fraction of positives among unlabeled examples}
We applied the PULSCAR algorithm to both SCAR and SNAR data, and the PULSNAR algorithm only to SNAR data, to estimate $\alpha$.

\subsubsection{Using the PULSCAR algorithm}
To find the 95\% confidence interval (CI) on estimation, we ran XGBoost with 5-fold cross-validation (CV) for 40 random instances of each dataset generated (or selected from benchmark data) using 40 random seeds. Each iteration's class 1 predicted probabilities of positives and unlabeled were used to calculate the value of $\alpha$.

\subsubsection{Using the PULSNAR algorithm}
The labeled positives were divided into \emph{c} clusters to get homogeneous subclasses of labeled positives. The XGBoost ML models were trained and tested with 5-fold CV on data from each cluster and all unlabeled records. For each cluster, $\alpha$ was estimated by applying the PULSCAR method to class 1 predicted probabilities of positives from the cluster and all unlabeled examples. The overall proportion was calculated by summing the estimated $\alpha$ for each cluster. To compute the 95\% CI on the estimation, PULSNAR was repeated 40 times on data generated/selected using 40 random seeds.


\section{Results}
\label{sectionResults}
\subsection{Synthetic datasets}
Figure \ref{fig:synthetic_scar_non_scar_data} shows the $\alpha$ estimated by PU learning algorithms for synthetic datasets. Appendix Figure \ref{fig:synthetic_scar_non_scar_data_err} shows the difference between the mean $\alpha$ estimated by PU learning algorithms and the true fractions for synthetic datasets. TICE overestimated $\alpha$ for all fractions in both SCAR and SNAR datasets. For SCAR datasets, only PULSCAR returned close estimates for all fractions; DEDPUL overestimated for 1\%; KM1 and KM2 underestimated for 50\%; CleanLab underestimated for larger $\alpha$ (10-50\%). For SNAR datasets, only PULSNAR's estimates were close to the true $\alpha$; other algorithms overestimated/underestimated for larger $\alpha$ (30-50\%). Figure \ref{fig:dedpulVSpulsnar} in Appendix \ref{appendix3} shows the $\alpha$ estimated by DEDPUL and PULSNAR on large SNAR datasets with different class imbalances. As the class imbalance increased, the performance of DEDPUL dropped, especially for larger fractions. The estimated $\alpha$ by the PULSNAR method was close to the true $\alpha$ for all fractions and sample sizes.

\begin{figure}[H]
\centering
\includegraphics[width=0.99\columnwidth]{allFigures/syntheticScarNonScarData1.png} 
\caption{\textbf{KM1, KM2, TICE, CleanLab, DEDPUL, PULSCAR, and PULSNAR evaluated on SCAR and SNAR synthetic datasets}. The bar represents the mean value of the estimated $\alpha$, with 95\% confidence intervals for estimated $\alpha$.}
\label{fig:synthetic_scar_non_scar_data}
\end{figure}

\subsection{ML Benchmark datasets}
\subsubsection{SCAR data}
Figure \ref{fig:uci_scar_data} shows the $\alpha$ estimated by PU learning algorithms for the KDD Cup 2004 particle physics and UCI bank datasets. For KDD Cup, estimates by PULSCAR and DEDPUL were close to the true answers for all fractions; TICE overestimated for all fractions; CleanLab overestimated for 1-30\%. For Bank, only PULSCAR returned correct estimates for all fractions; other algorithms overestimated for all fractions.

\subsubsection{SNAR data}
Figure \ref{fig:uci_non_scar_data} shows the $\alpha$ estimated by PU learning algorithms for UCI Shuttle and UCI Firewall datasets. For the  Shuttle dataset, only PULSNAR's estimates were close to the true fractions; other algorithms either overestimated or underestimated. For the Firewall dataset, TICE overestimated, and CleanLab underestimated for all fractions; PULSNAR's estimates were within $\pm 20\%$ of the true $\alpha$; DEDPUL and PULSCAR underestimated for 40\%.





\begin{minipage}[t]{.48\linewidth}
\begin{figure}[H]
\includegraphics[width=0.99\columnwidth]{allFigures/uciScarData.png}
\caption{\textbf{TICE, CleanLab, DEDPUL, and PULSCAR evaluated on SCAR KDD cup 2004 particle physics and UCI Bank datasets}. The bar represents the mean value of the estimated $\alpha$, with 95\% confidence intervals for estimated $\alpha$. KM1 and KM2 failed to execute on both datasets. As TICE was taking several hours to finish one iteration on the bank dataset, the mean $\alpha$ was computed using 5 iterations, and the standard error was set to 0.}
\label{fig:uci_scar_data}
\end{figure}
\end{minipage}
\hfill
\begin{minipage}[t]{.48\linewidth}
\begin{figure}[H]
\includegraphics[width=0.99\columnwidth]{allFigures/uciNonScarData.png} 
\caption{\textbf{KM1, KM2, TICE, CleanLab, DEDPUL, PULSCAR, and PULSNAR evaluated on SNAR UCI Shuttle and Firewall datasets}. The bar represents the mean value of the estimated $\alpha$, with 95\% confidence intervals for estimated $\alpha$. KM1 and KM2 failed to execute on the Firewall dataset. As KM1 and KM2 were taking several hours to finish one iteration on the Shuttle dataset, the mean $\alpha$ was computed using 5 iterations, and the standard error was set to 0.}
\label{fig:uci_non_scar_data}
\end{figure}
\end{minipage}

\subsection{Probability calibration}
Appendix \ref{appendix1results} shows the calibration curves generated using the unblinded labels and isotonically calibrated probabilities of positive and unlabeled examples or only unlabeled examples in the SCAR and SNAR data.

\subsection{Classification performance metrics}
Appendix \ref{appendix2results} shows substantial improvement in 6 classification performance metrics when applying PULSCAR and PULSNAR versus XGBoost alone.

\section{Discussion and Conclusion}
\label{sectionConclusion}
This paper presented novel PU learning algorithms to estimate the proportion of positives among unlabeled examples in both SCAR and SNAR data with/without class imbalance. Preliminary work (not shown) suggests PULSNAR $\alpha$ estimation is robust to overestimating the number of clusters in SNAR data. Our synthetic experiments were run on difficult classification tasks with low separability. For SNAR data, with true $\alpha=1\%$, when we increased \textit{class\_sep} from 0.3 to 0.5 the PULSNAR $\alpha$ estimate improved from $1.6\%$ (Figure \ref{fig:synthetic_scar_non_scar_data}) to 0.98\% (data not shown).  Experimentally, we showed that our proposed methods outperformed state-of-art methods on synthetic and real-world SCAR and SNAR datasets. PU learning methods based on the SCAR assumption generally give poor $\alpha$ estimates on SNAR data. We demonstrated that after applying PULSCAR/PULSNAR, classifier performance, including calibration, improved significantly. Since the SCAR assumption often does not hold in real-world data, better $\alpha$ estimates in a SNAR setting open up new horizons in PU Learning.

Our experimentation showed that the KM1, KM2, and TICE algorithms exhibited scalability issues and could not process large datasets with high dimensions. This observation aligns with the findings of \cite{pulsnar_51}, who noted the underperformance of these techniques in high-dimensional scenarios and scalability issues with large datasets. While we evaluated PULSCAR/PULSCAR against these methods using moderately sized datasets, it is plausible that their inherent limitations with data size and high dimension contributed to inaccurate $\alpha$ estimates for some of our test datasets. The CleanLab method is not explicitly designed for PU problems but is primarily developed for noisy label problems. This could be a potential explanation for its poor effectiveness when applied to PU scenarios.

We posit two reasons why PULSCAR outperformed DEDPUL in some experiments: a) The version of PULSCAR presented here uses the Beta distribution, outperforming earlier prototypes that used the Gaussian distribution. This may account for PULSCAR's superiority over DEDPUL, which uses the Gaussian kernel for density estimation. The problems with boundary biases of the Gaussian (as well as the dangers of polynomial approximations to the Gaussian) are described in section \ref{KernelBandwidthestimation}, which are addressed using the Beta. b) In addition, we believe our robust approach to density-based $\alpha$ estimation using Equation \ref{eq:obj_func} may have more robust convergence properties than the EM algorithm used by DEDPUL.

\section{Limitations}
Our approach counts on knowing whether the data are SCAR or SNAR because the knee-point cluster determination approach may produce $>1$ clusters on SCAR data containing just one type of positive in both positive and unlabeled sets. This will result in PULSNAR overestimating $\alpha$ as two near-identical positive types cannot be distinguished and get counted more than once. 

\bibliography{PULSNAR_paper}
\bibliographystyle{tmlr}


\section*{Appendix}
\appendix

\section{Proof: positives are not independent of their attributes under the SNAR Assumption}
\label{appendix0}

Under the SNAR assumption, the probability that a positive example is labeled is not independent of its attributes. Stated formally, the assumption is that
$p(s=1|x, y=1) \neq p(s=1|y=1)$ i.e. $p(s=1|x, y=1)$ is not a constant.

\textbf{Proof:}
\begin{equation} 
\begin{split}
p(s=1|x, y=1) & = p(y=1|(s=1|x))p(s=1|x) \nonumber \\
 & = p(y=1|(s=1|x)) \frac{p(x|s=1)p(s=1)}{p(x)} \nonumber 
 \text{ , using Bayes' rule} \\
 & = \frac{p(x|s=1)p(s=1)}{p(x)} \nonumber
 \text{ , since $p(y=1|(s=1|x)) = 1$ } \\
 & = \text{a function of $x$.}
\end{split}
\end{equation}

\clearpage
\section{Errors in \texorpdfstring{$\alpha$}{alpha} estimation with synthetic SCAR and SNAR data}

To further emphasize the magnitude and direction of the $\alpha$ estimate errors in Figure \ref{fig:synthetic_scar_non_scar_data}, we show the difference between the estimated and true values.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\columnwidth]{allFigures/syntheticScarNonScarData_err.png} 
\caption{\textbf{KM1, KM2, TICE, CleanLab, DEDPUL, PULSCAR, and PULSNAR evaluated on SCAR and SNAR synthetic datasets}. The bar represents the difference between the mean value of the estimated $\alpha$ and the true fraction. Bar above the line y=0 represents overestimation and bar below the line y=0 represents underestimation.}
\label{fig:synthetic_scar_non_scar_data_err}
\end{figure}

\section{Probability calibration}
\label{appendix1}
\subsection{Algorithm for calibrating probabilities}
Algorithm \ref{alg:calibprobs} shows the complete pseudocode to calibrate the machine learning (ML) model predicted probabilities. Once $\alpha$ is known, we seek to transform the original class 1 probabilities so that their sum is equal to $\alpha |U|$ among the unlabeled or $|P| + \alpha |U|$ among positive and unlabeled, and that they are well-calibrated. Our approach is to probabilistically flip $\alpha |U|$ labels of unlabeled to positive (from 0 to 1) in such a way as to match the PDF of labeled positives across 100 equispaced bins over $[0\ldots1]$, then fit a logistic or isotonic regression model on those labels versus the probabilities to generate the transformed probabilities. To determine the number of unlabeled examples that need to be flipped in each bin, we compute the normalized histogram density, $D\_hist$, for the labeled positives with 100 bins and then multiply $\alpha |U|$ with $D\_hist$.

The unlabeled examples are also divided into 100 bins based on their predicted probabilities. Starting from the bin with the highest probability(p=1), we randomly select \emph{k} examples and flip their labels from 0 to 1, where \emph{k} is the number of unlabeled examples that need to be flipped in the bin. If the number of records ($n_1$) that need to be flipped in a bin is more than the number of records ($n_2$) present in the bin, the difference ($n_1-n_2$) is added to the number of records to be flipped in the next bin, resulting in $\alpha |U|$ flips.

After flipping the labels of $\alpha |U|$ unlabeled examples from 0 to 1, we fit an isotonic or sigmoid regression model on the ML-predicted class 1 probabilities with the updated labels to obtain calibrated probabilities.

The above calibration approach applies to both SCAR and SNAR data. For the SNAR data, the PULSNAR algorithm divides labeled positive examples into \emph{k} clusters and estimates the $\alpha$ for each cluster. For each cluster, the ML-predicted class 1 probabilities of the examples (positives from the cluster and all unlabeled examples or only unlabeled examples) are calibrated using the estimated $\alpha$ for the cluster. Since, for each cluster, PULSNAR uses all unlabeled examples, each unlabeled example has \emph{k} ML-predicted/calibrated probabilities. The final ML-predicted/calibrated probability of an unlabeled example is calculated using the following Equation \ref{eq:combined_probs}:

\begin{equation}\label{eq:combined_probs}
p = 1 - (1-p_1)(1-p_2)\dots(1-p_k)
\end{equation} 
where $p_k$ is the probability of an unlabeled example from cluster \emph{k}.




\subsection{Experiments and Results}
\label{appendix1results}
We used synthetic SCAR and SNAR datasets and KDD Cup SCAR dataset to test our calibration algorithm. 

\textbf{SCAR datasets: }After estimating the $\alpha$ using the PULSCAR algorithm, we applied Algorithm \ref{alg:calibprobs} to calibrate the ML-predicted probabilities. To calculate the calibrated probabilities for both positive and unlabeled (PU) examples, we applied isotonic regression to the ML-predicted class 1 probabilities of PU examples with labels of positives and updated labels of unlabeled (of which $\alpha|U|$ were flipped per Algorithm \ref{alg:calibprobs}). We applied isotonic regression to the unlabeled's predicted probabilities with their updated labels to calculate the calibrated probabilities only for the unlabeled.

\textbf{SNAR datasets: } Using the PULSNAR algorithm, the labeled positive examples were divided into \emph{k} clusters. For each cluster, after estimating the $\alpha$, Algorithm \ref{alg:calibprobs} was used to calibrate the ML-predicted probabilities. To calculate the calibrated probabilities for positives from a cluster and all unlabeled examples, we applied isotonic regression to their ML-predicted class 1 probabilities with labels of positives from the cluster and updated labels of unlabeled (of which $\alpha_j |U|$ were flipped for cluster $j=1\dots k$, see Algorithm \ref{alg:calibprobs}). We applied isotonic regression to the unlabeled's predicted probabilities with their updated labels to calculate the calibrated probabilities only for the unlabeled. Thus, each unlabeled example had \emph{k} calibrated probabilities. We computed the final calibrated probability for each unlabeled example using Formula \ref{eq:combined_probs}.


Figures \ref{fig:PU_scar_syn_pu_data_calibration}, \ref{fig:PU_scar_syn_u_data_calibration}, \ref{fig:PU_snar_syn_pu_data_calibration}, \ref{fig:PU_snar_syn_u_data_calibration}, \ref{fig:PU_scar_kdd_pu_data_calibration} and \ref{fig:PU_scar_kdd_u_data_calibration} show the calibration curves generated using the unblinded labels and isotonically calibrated (red)/ uncalibrated (blue) probabilities. When both positive and unlabeled examples were used to calculate calibrated probabilities, the calibration curve followed the y=x line (well-calibrated probabilities). When only unlabeled examples were used, the calibration curve for 1\% did not follow the y=x line, presumably due to the ML model being biased toward negatives, given the small $\alpha$. Also, the calibration curves for the SCAR data followed the y=x line more closely than the calibration curves for the SNAR data. It is due to the fact that the final probability of an unlabeled example in the SNAR data is computed using its \emph{k} probabilities from \emph{k} clusters. So, a poor probability estimate from even one cluster can influence the final probability of an unlabeled example. 

%% Algorithm to calibrate probabilities
\begin{algorithm}
\caption{calibrate\_probabilities}
\label{alg:calibprobs}
\textbf{Input}: predicted\_probs, labels, n\_bins, calibration\_method, calibration\_data, $\alpha$ \\
%\textbf{Parameter}: p, labels, bin\_method \\
\textbf{Output}: calibrated\_probs
\begin{algorithmic}[1] %[1] enables line numbers
\STATE p0 $\leftarrow$ predicted\_probs[labels == 0]
\STATE p1 $\leftarrow$ predicted\_probs[labels == 1]
\STATE y0 $\leftarrow$ labels[labels == 0]
\STATE y1 $\leftarrow$ labels[labels == 1]
\STATE $D_{hist}$ $\leftarrow$ histogram(p1, n\_bins, density=True)
\STATE unlab\_pos\_count\_in\_bin $\leftarrow$ $\alpha$  $|p0|$ $D_{hist}$
\STATE p0\_bins $\leftarrow$ split unlabeled examples into n\_bins using p0
\FOR{k $\leftarrow$ [n\_bins $\dots$ 1]}
    \STATE $n_1$ $\leftarrow$ unlab\_pos\_count\_in\_bin[k]
    \STATE $n_2$ $\leftarrow$ p0\_bins[k]
    \IF{$n_1 > n_2$}
        \STATE $\hat{y0}$ $\leftarrow$ flip labels (y0) of $n_2$ examples from 0 to 1 in bin k
        \STATE unlab\_pos\_count\_in\_bin[k-1] $\leftarrow$ unlab\_pos\_count\_in\_bin[k-1] + ($n_1-n_2$)
    \ELSE 
        \STATE $\hat{y0}$ $\leftarrow$ flip labels (y0) of random $n_1$ examples from 0 to 1 in bin k
    \ENDIF
\ENDFOR
\IF{calibration\_data == `PU'}
    \STATE p, y $\leftarrow$ $p1 \cup p0$, $y1 \cup \hat{y0}$
\ELSIF{calibration\_data == `U'}
    \STATE p, y $\leftarrow$ p0, $\hat{y0}$
\ENDIF
\IF{calibration\_method is `sigmoid'}
    \STATE $\hat{p}$ $\leftarrow$ LogisticRegression(p, y)
\ELSIF{calibration\_method is `isotonic'}
    \STATE $\hat{p}$ $\leftarrow$ IsotonicRegression(p, y)
\ENDIF
\STATE \textbf{return} $\hat{p}$
\end{algorithmic}
\end{algorithm}

\begin{figure}
\centering
\includegraphics[width=0.80\columnwidth]{allFigures/PUScarSynPuDataCalibration.png}
\caption{\textbf{Calibration curves for Synthetic SCAR datasets (both positive and unlabeled examples)}. Synthetic datasets were generated with different fractions of positives (1\%, 5\%, 10\%, 20\%, 30\%, and 50\%) among the unlabeled examples. class\_sep=0.3, number of attributes=100, $|P| = 5,000$ and $|U| = 50,000$. Calibration curves were generated using both positive and unlabeled examples (Uncalibrated probabilities - blue, calibrated probabilities - red).}
\label{fig:PU_scar_syn_pu_data_calibration}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.80\columnwidth]{allFigures/PUScarSynUDataCalibration.png}
\caption{\textbf{Calibration curves for Synthetic SCAR datasets (only unlabeled examples)}. Synthetic datasets were generated with different fractions of positives (1\%, 5\%, 10\%, 20\%, 30\%, and 50\%) among the unlabeled examples. class\_sep=0.3, number of attributes=100, $|P| = 5,000$ and $|U| = 50,000$. Calibration curves were generated using only unlabeled examples (Uncalibrated probabilities - blue, calibrated probabilities - red).}
\label{fig:PU_scar_syn_u_data_calibration}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.80\columnwidth]{allFigures/PUSnarSynPuDataCalibration.png}
\caption{\textbf{Calibration curves for Synthetic SNAR datasets (both positive and unlabeled examples)}. Synthetic datasets were generated with different fractions of positives (1\%, 5\%, 10\%, 20\%, 30\%, and 50\%) among the unlabeled examples. class\_sep=0.3, number of attributes=100, number of positive subclasses=5, $|P|$ = 20,000 (4,000 from each subclass) and $|U|$ = 50,000. Calibration curves were generated using both positive and unlabeled examples (Uncalibrated probabilities - blue, calibrated probabilities - red).}
\label{fig:PU_snar_syn_pu_data_calibration}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.80\columnwidth]{allFigures/PUSnarSynUDataCalibration.png}
\caption{\textbf{Calibration curves for Synthetic SNAR datasets (only unlabeled examples)}. Synthetic datasets were generated with different fractions of positives (1\%, 5\%, 10\%, 20\%, 30\%, and 50\%) among the unlabeled examples. class\_sep=0.3, number of attributes=100, number of positive subclasses=5, $|P|$ = 20,000 (4,000 from each subclass) and $|U|$ = 50,000. Calibration curves were generated using only unlabeled examples (Uncalibrated probabilities - blue, calibrated probabilities - red).}
\label{fig:PU_snar_syn_u_data_calibration}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.80\columnwidth]{allFigures/PUScarKddPuDataCalibration.png}
\caption{\textbf{Calibration curves for SCAR KDD Cup 2004 particle physics dataset (both positive and unlabeled examples)}. Unlabeled sets contained 1\%, 5\%, 10\%, 20\%, 30\%, and 40\% positive examples. Calibration curves were generated using both positive and unlabeled examples (Uncalibrated probabilities - blue, calibrated probabilities - red).}
\label{fig:PU_scar_kdd_pu_data_calibration}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.80\columnwidth]{allFigures/PUScarKddUDataCalibration.png}
\caption{\textbf{Calibration curves for SCAR KDD Cup 2004 particle physics dataset (only unlabeled examples)}. Unlabeled sets contained 1\%, 5\%, 10\%, 20\%, 30\%, and 40\% positive examples. Calibration curves were generated using only unlabeled examples (Uncalibrated probabilities - blue, calibrated probabilities - red).}
\label{fig:PU_scar_kdd_u_data_calibration}
\end{figure}



\clearpage
\section{Improving classification performance with PULSCAR and PULSNAR}
\label{appendix2}
\subsection{Algorithm for improving classification}
Algorithm \ref{alg:classification_metrics} shows the complete pseudocode to improve classification performance with PULSCAR and PULSNAR. The algorithm returns the following six classification metrics: \emph{Accuracy, AUC-ROC, Brier score (BS), F1, Matthew's correlation coefficient (MCC)}, and \emph{Average precision score (APS)}. The approach to enhancing the classification performance is as follows: 

\textbf{Using PULSCAR: } After estimating the $\alpha$, the class 1 predicted probabilities of only unlabeled examples are calibrated using Algorithm \ref{alg:calibprobs}. The calibrated probabilities of the unlabeled examples are sorted in descending order, and the labels of top $\alpha |U|$ unlabeled examples with the highest calibrated probabilities are flipped from 0 to 1 (probable positives). We then train and test an ML classifier (XGBoost) with 5-fold CV using the labeled positives, probable positives, and the remaining unlabeled examples. The classification performance metrics are calculated using the ML predictions and the true labels of the data. 

\textbf{Using PULSNAR: } The PULSNAR algorithm divides the labeled positive examples into \emph{k} clusters. For each cluster, after estimating $\alpha_j$ for $j$ in $1\dots k$, the class 1 predicted probabilities of only unlabeled examples are calibrated using Algorithm \ref{alg:calibprobs}. Since each unlabeled example has \emph{k} calibrated probabilities, we compute the final calibrated probability for each unlabeled example using the Formula \ref{eq:combined_probs}. The final $\alpha$ is calculated by summing the $\alpha_j$ values over the $k$ clusters. The final calibrated probabilities of the unlabeled examples are sorted in descending order, and the labels of top $\alpha |U|$ unlabeled examples with the highest calibrated probabilities are flipped from 0 to 1 (probable positives). We then train and test an ML classifier (XGBoost) with 5-fold CV using the labeled positives, probable positives, and the remaining unlabeled examples. The classification performance metrics are calculated using the ML predictions and the true labels of the data. 


\begin{algorithm}
    \caption{calculate\_classification\_metrics}
    \label{alg:classification_metrics}
    \textbf{Input}: X ($X_p \cup X_u$), y ($y_p \cup y_u$), y\_true, bin\_method, n\_bins, predicted\_probabilities, $\alpha$ \\
    \textbf{Output}: classification\_metrics (accuracy, roc auc, brier score, f1, Matthew's correlation coefficient, average precision) 
\begin{algorithmic}[1] %[1] enables line numbers
    \STATE p $\leftarrow$ predicted\_probabilities
    \STATE $\hat{p}$ $\leftarrow$ calibrate\_probabilities(p, y, n\_bins, calibration\_method, `U', $\alpha$)
    \STATE sort $\hat{p}$ in descending order
    \STATE $\hat{y_u}$ $\leftarrow$ flip labels of top $\alpha |U|$ unlabeled examples with highest $\hat{p}$
    \STATE y $\leftarrow$ $y_p \cup \hat{y_u}$
    \STATE predicted\_probabilities (p) $\leftarrow$ $\mathcal{A}(X,y)$
    \STATE \textbf{return} accuracy(p, y\_true), auc(p, y\_true), bs(p, y\_true), f1(p, y\_true), mcc(p, y\_true), aps(p, y\_true)
\end{algorithmic}    
\end{algorithm}

\subsection{Experiments and Results}
\label{appendix2results}
We applied Algorithm \ref{alg:classification_metrics} to synthetic SCAR and SNAR datasets to get the performance metrics for the XGBoost model with PULSCAR and PULSNAR, respectively. The classification performance metrics were also calculated without applying the PULSCAR or PULSNAR algorithm, in order to determine the improvement in the classification performance of the model. The experiment was repeated 40 times by selecting different train and test sets using 40 random seeds to compute the 95\% confidence interval (CI) for the metrics. 

Figures \ref{fig:scar_syn_classification_metrics} and \ref{fig:snar_syn_classification_metrics} show the classification performance of the XGBoost model with/without the PULSCAR or PULSNAR algorithm on synthetic SCAR and SNAR data, respectively. The classification performance using PULSCAR or PULSNAR increased significantly over XGBoost alone. As the proportion of positives among the unlabeled examples increased, the performance of the model without PULSCAR or PULSNAR (blue) worsened significantly more than when using PULSCAR or PULSNAR.


\begin{figure}
\centering
\includegraphics[width=0.85\columnwidth]{allFigures/scarSynClassificationMetricsLinePlot.png}
\caption{\textbf{Classification performance of XGBoost model on synthetic SCAR datasets with and without the PULSCAR algorithm}. Synthetic datasets were generated with different fractions of positives (1\%, 5\%, 10\%, 20\%, 30\%, 40\%, and 50\%) among the unlabeled examples. class\_sep=0.3, number of attributes=100, $|P| = 5,000$ and $|U| = 50,000$. \emph{``no PULSCAR''} (blue): XGBoost model was trained and tested with 5-fold CV on the given data; the classification metrics were calculated using the model predictions and true labels. \emph{``PULSCAR''} (red): PULSCAR algorithm was used to find the proportion of positives among unlabeled examples ($\alpha$); using $\alpha$, probable positives were identified; XGBoost model was trained and tested with 5-fold CV on labeled positives, probable positives, and the remaining unlabeled examples; classification metrics were calculated using the model predictions and true labels. The error bars represent 95\% CIs for the performance metrics.}
\label{fig:scar_syn_classification_metrics}
\end{figure}


\begin{figure}
\centering
\includegraphics[width=0.85\columnwidth]{allFigures/snarSynClassificationMetricsLinePlot.png}
\caption{\textbf{Classification performance of XGBoost model on synthetic SNAR datasets with and without the PULSNAR algorithm}. Synthetic datasets were generated with different fractions of positives (1\%, 5\%, 10\%, 20\%, 30\%, 40\%, and 50\%) among the unlabeled examples. class\_sep=0.3, number of attributes=100, number of positive subclasses=5, $|P|$ = 20,000 (4,000 from each subclass) and $|U|$ = 50,000. \emph{``no PULSNAR''} (blue): XGBoost model was trained and tested with 5-fold CV on the given data; the classification metrics were calculated using the model predictions and true labels. \emph{``PULSNAR''} (red): PULSNAR algorithm was used to find the proportion of positives among unlabeled examples ($\alpha$); using $\alpha$, probable positives were identified; XGBoost model was trained and tested with 5-fold CV on labeled positives, probable positives, and the remaining unlabeled examples; classification metrics were calculated using the model predictions and true labels. The error bars represent 95\% CIs for the performance metrics.}
\label{fig:snar_syn_classification_metrics}
\end{figure}

\clearpage
\section{DEDPUL vs. PULSNAR: \texorpdfstring{$\alpha$}{alpha} estimation}
\label{appendix3}
Public implementations of the PU learning methods KM1, KM2, and TICE were not scalable; they either failed to execute or would have taken weeks to run the multiple iterations required to obtain confidence estimates for large datasets. We thus could not compare our method with KM1, KM2, and TICE on large datasets and used only DEDPUL for comparison. Importantly, it was previously demonstrated that the DEDPUL method outperformed these three methods on several UCI ML benchmark and synthetic datasets \cite{pulsnar_24}. 

We compared our algorithm with DEDPUL on synthetic SNAR datasets with different fractions (1\%, 5\%, 10\%, 20\%, 30\%, 40\%, and 50\%) of positives among unlabeled examples. In our experiments, we observed that class imbalance (ratio of majority class to minority class) could affect the $\alpha$ estimates. So, we used 4 different sample sizes: 1) positive: 5,000 and unlabeled: 5,000; 2) positive: 5,000 and unlabeled: 25,000; 3) positive: 5,000 and unlabeled: 50,000; 4) positive: 5,000 and unlabeled: 100,000. For each sample size and fraction, we generated 20 datasets using sklearn's \emph{make\_classification()} method with random seeds 0-19 to compute 95\% CI. We used class\_sep=0.3 for each dataset to create difficult classification problems. All datasets were generated with 100 attributes and 6 labels (0-5), defining `0' as negative and 1-5 as positive subclasses. The positive set contained 1000 examples from each positive subclass in all datasets. The unlabeled set comprised k\% positive examples with labels (1-5) flipped to 0 and (100-k)\% negative examples. The unlabeled positives were markedly SNAR, with the 5 subclasses comprising 1/31, 2/31, 4/31, 8/31, and 16/31 of the unlabeled positives. 

Figure \ref{fig:dedpulVSpulsnar} shows the $\alpha$ estimates by DEDPUL and PULSNAR on synthetic SNAR data. For smaller true fractions (1\%, 5\%, 10\%), DEDPUL returned close $\alpha$ estimates, but for larger fractions (20\%, 30\%, 40\%, and 50\%), it underestimated $\alpha$. Also, as the class imbalance increased, the performance of DEDPUL dropped, especially for larger true fractions. The estimated $\alpha$ by the PULSNAR method was close to the true $\alpha$ for all fractions and sample sizes. 

\begin{figure}
\centering
\includegraphics[width=0.90\columnwidth]{allFigures/dedpulVSpulsnar.png}
\caption{\textbf{PULSNAR and DEDPUL evaluated on synthetic SNAR datasets}. The bar represents the mean value of the estimated $\alpha$, with 95\% CI for estimated $\alpha$.}
\label{fig:dedpulVSpulsnar}
\end{figure}



\end{document}
