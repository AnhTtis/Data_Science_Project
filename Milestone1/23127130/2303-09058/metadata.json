{
    "arxiv_id": "2303.09058",
    "paper_title": "SVDE: Scalable Value-Decomposition Exploration for Cooperative Multi-Agent Reinforcement Learning",
    "authors": [
        "Shuhan Qi",
        "Shuhao Zhang",
        "Qiang Wang",
        "Jiajia Zhang",
        "Jing Xiao",
        "Xuan Wang"
    ],
    "submission_date": "2023-03-16",
    "revised_dates": [
        "2023-03-17"
    ],
    "latest_version": 1,
    "categories": [
        "cs.AI"
    ],
    "abstract": "Value-decomposition methods, which reduce the difficulty of a multi-agent system by decomposing the joint state-action space into local observation-action spaces, have become popular in cooperative multi-agent reinforcement learning (MARL). However, value-decomposition methods still have the problems of tremendous sample consumption for training and lack of active exploration. In this paper, we propose a scalable value-decomposition exploration (SVDE) method, which includes a scalable training mechanism, intrinsic reward design, and explorative experience replay. The scalable training mechanism asynchronously decouples strategy learning with environmental interaction, so as to accelerate sample generation in a MapReduce manner. For the problem of lack of exploration, an intrinsic reward design and explorative experience replay are proposed, so as to enhance exploration to produce diverse samples and filter non-novel samples, respectively. Empirically, our method achieves the best performance on almost all maps compared to other popular algorithms in a set of StarCraft II micromanagement games. A data-efficiency experiment also shows the acceleration of SVDE for sample collection and policy convergence, and we demonstrate the effectiveness of factors in SVDE through a set of ablation experiments.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.09058v1"
    ],
    "publication_venue": "13 pages, 9 figures"
}