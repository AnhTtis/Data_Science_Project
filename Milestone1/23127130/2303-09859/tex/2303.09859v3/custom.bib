% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@misc{Wikiextractor2015,
  author = {Giusepppe Attardi},
  title = {WikiExtractor},
  year = {2015},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/attardi/wikiextractor}}
}

@misc{https://doi.org/10.48550/arxiv.1810.12885,
  doi = {10.48550/ARXIV.1810.12885},
  
  url = {https://arxiv.org/abs/1810.12885},
  
  author = {Zhang, Sheng and Liu, Xiaodong and Liu, Jingjing and Gao, Jianfeng and Duh, Kevin and Van Durme, Benjamin},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{
You2020Large,
title={Large Batch Optimization for Deep Learning: Training BERT in 76 minutes},
author={Yang You and Jing Li and Sashank Reddi and Jonathan Hseu and Sanjiv Kumar and Srinadh Bhojanapalli and Xiaodan Song and James Demmel and Kurt Keutzer and Cho-Jui Hsieh},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=Syx4wnEtvH}
}

@inproceedings{
loshchilov2018decoupled,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

@ARTICLE{jelinek1976,

  author={Jelinek, F.},

  journal={Proceedings of the IEEE}, 

  title={Continuous speech recognition by statistical methods}, 

  year={1976},

  volume={64},

  number={4},

  pages={532-556},

  doi={10.1109/PROC.1976.10159}}

@article{copa,
author = {Roemmele, Melissa and Bejan, Cosmin and Gordon, Andrew},
year = {2011},
month = {01},
pages = {},
title = {Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning.},
journal = {AAAI Spring Symposium - Technical Report}
}

@inproceedings{silveira14gold,
  year = {2014},
  author = {Natalia Silveira and Timothy Dozat and Marie-Catherine de
	  Marneffe and Samuel Bowman and Miriam Connor and John Bauer and
	  Christopher D. Manning},
  title = {A Gold Standard Dependency Corpus for {E}nglish},
  booktitle = {Proceedings of the Ninth International Conference on Language
    Resources and Evaluation (LREC-2014)}
}


@misc{https://doi.org/10.35111/xmhb-2b84,
  doi = {10.35111/XMHB-2B84},
  url = {https://catalog.ldc.upenn.edu/LDC2013T19},
  author = {{Weischedel,  Ralph} and {Palmer,  Martha} and {Marcus,  Mitchell} and {Hovy,  Eduard} and {Pradhan,  Sameer} and {Ramshaw,  Lance} and {Xue,  Nianwen} and {Taylor,  Ann} and {Kaufman,  Jeff} and {Franchini,  Michelle} and {El-Bachouti,  Mohammed} and {Belvin,  Robert} and {Houston,  Ann}},
  title = {OntoNotes Release 5.0},
  publisher = {Linguistic Data Consortium},
  year = {2013}
}

@article{MATTHEWS1975442,
title = {Comparison of the predicted and observed secondary structure of T4 phage lysozyme},
journal = {Biochimica et Biophysica Acta (BBA) - Protein Structure},
volume = {405},
number = {2},
pages = {442-451},
year = {1975},
issn = {0005-2795},
doi = {https://doi.org/10.1016/0005-2795(75)90109-9},
url = {https://www.sciencedirect.com/science/article/pii/0005279575901099},
author = {B.W. Matthews},
abstract = {Predictions of the secondary structure of T4 phage lysozyme, made by a number of investigators on the basis of the amino acid sequence, are compared with the structure of the protein determined experimentally by X-ray crystallography. Within the amino terminal half of the molecule the locations of helices predicted by a number of methods agree moderately well with the observed structure, however within the carboxyl half of the molecule the overall agreement is poor. For eleven different helix predictions, the coefficients giving the correlation between prediction and observation range from 0.14 to 0.42. The accuracy of the predictions for both β-sheet regions and for turns are generally lower than for the helices, and in a number of instances the agreement between prediction and observation is no better than would be expected for a random selection of residues. The structural predictions for T4 phage lysozyme are much less successful than was the case for adenylate kinase (Schulz et al. (1974) Nature 250, 140–142). No one method of prediction is clearly superior to all others, and although empirical predictions based on larger numbers of known protein structure tend to be more accurate than those based on a limited sample, the improvement in accuracy is not dramatic, suggesting that the accuracy of current empirical predictive methods will not be substantially increased simply by the inclusion of more data from additional protein structure determinations.}
}

@article{rte2,
author = {Bar-Haim, Roy and Dagan, Ido and Dolan, Bill and Ferro, Lisa and Giampiccolo, Danilo},
year = {2006},
month = {01},
pages = {},
title = {The second PASCAL recognising textual entailment challenge},
journal = {Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment}
}

@article{de_Marneffe_Simons_Tonhauser_2019, title={The CommitmentBank: Investigating projection in naturally occurring discourse}, volume={23}, url={https://ojs.ub.uni-konstanz.de/sub/index.php/sub/article/view/601}, DOI={10.18148/sub/2019.v23i2.601}, abstractNote={&lt;p&gt;This paper describes a new resource, the CommitmentBank, developed for the empirical investigation of the projection of finite clausal complements. A clausal complement is said to project when its content is understood as a commitment of the speaker even though the clause occurs under the scope of an entailment canceling operator such as negation or a question. The study of projection is therefore part of the study of commitments expressed by speakers to non-asserted sentence content. The content of clausal complements has been a central case for the study of projection, as there is a long-standing claim that clause-taking predicates fall into two classes—factives and nonfactives—distinguished on the basis of whether the contents of their complements project. This claim identifies the embedding predicate as the primary determinant of the projection behavior of these contents. The CommitmentBank is a corpus of naturally occurring discourses whose final sentence contains a clause-embedding predicate under an entailment canceling operator. In this paper, we describe the CommitmentBank and present initial results of analyses designed to evaluate the factive/nonfactive distinction and to investigate additional factors which affect the projectivity of clausal complements.&lt;/p&#38;gt;}, number={2}, journal={Proceedings of Sinn und Bedeutung}, author={de Marneffe, Marie-Catherine and Simons, Mandy and Tonhauser, Judith}, year={2019}, month={Jul.}, pages={107-124} }

@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@misc{https://doi.org/10.48550/arxiv.2112.11446,
  doi = {10.48550/ARXIV.2112.11446},
  
  url = {https://arxiv.org/abs/2112.11446},
  
  author = {Rae, Jack W. and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and Rutherford, Eliza and Hennigan, Tom and Menick, Jacob and Cassirer, Albin and Powell, Richard and Driessche, George van den and Hendricks, Lisa Anne and Rauh, Maribeth and Huang, Po-Sen and Glaese, Amelia and Welbl, Johannes and Dathathri, Sumanth and Huang, Saffron and Uesato, Jonathan and Mellor, John and Higgins, Irina and Creswell, Antonia and McAleese, Nat and Wu, Amy and Elsen, Erich and Jayakumar, Siddhant and Buchatskaya, Elena and Budden, David and Sutherland, Esme and Simonyan, Karen and Paganini, Michela and Sifre, Laurent and Martens, Lena and Li, Xiang Lorraine and Kuncoro, Adhiguna and Nematzadeh, Aida and Gribovskaya, Elena and Donato, Domenic and Lazaridou, Angeliki and Mensch, Arthur and Lespiau, Jean-Baptiste and Tsimpoukelli, Maria and Grigorev, Nikolai and Fritz, Doug and Sottiaux, Thibault and Pajarskas, Mantas and Pohlen, Toby and Gong, Zhitao and Toyama, Daniel and d'Autume, Cyprien de Masson and Li, Yujia and Terzi, Tayfun and Mikulik, Vladimir and Babuschkin, Igor and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Jones, Chris and Bradbury, James and Johnson, Matthew and Hechtman, Blake and Weidinger, Laura and Gabriel, Iason and Isaac, William and Lockhart, Ed and Osindero, Simon and Rimell, Laura and Dyer, Chris and Vinyals, Oriol and Ayoub, Kareem and Stanway, Jeff and Bennett, Lorrayne and Hassabis, Demis and Kavukcuoglu, Koray and Irving, Geoffrey},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Scaling Language Models: Methods, Analysis \&; Insights from Training Gopher},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.2203.15556,
  doi = {10.48550/ARXIV.2203.15556},
  
  url = {https://arxiv.org/abs/2203.15556},
  
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and Driessche, George van den and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Training Compute-Optimal Large Language Models},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@INPROCEEDINGS{Bentivogli09thefifth,
    author = {Luisa Bentivogli and Ido Dagan and Hoa Trang Dang and Danilo Giampiccolo and Bernardo Magnini},
    title = {The Fifth PASCAL Recognizing Textual Entailment Challenge},
    booktitle = {In Proc Text Analysis Conference (TAC’09},
    year = {2009}
}

@InProceedings{10.1007/11736790_9,
author="Dagan, Ido
and Glickman, Oren
and Magnini, Bernardo",
editor="Qui{\~{n}}onero-Candela, Joaquin
and Dagan, Ido
and Magnini, Bernardo
and d'Alch{\'e}-Buc, Florence",
title="The PASCAL Recognising Textual Entailment Challenge",
booktitle="Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="177--190",
abstract="This paper describes the PASCAL Network of Excellence first Recognising Textual Entailment (RTE-1) Challenge benchmark. The RTE task is defined as recognizing, given two text fragments, whether the meaning of one text can be inferred (entailed) from the other. This application-independent task is suggested as capturing major inferences about the variability of semantic expression which are commonly needed across multiple applications. The Challenge has raised noticeable attention in the research community, attracting 17 submissions from diverse groups, suggesting the generic relevance of the task.",
isbn="978-3-540-33428-6"
}


@inproceedings{bender2021dangers,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages={610--623},
  year={2021}
}

@misc{bnc2007british,
  title={British {N}ational {C}orpus},
  author={BNC Consortium},
  year={2007},
  publisher={Oxford Text Archive}
}


@inproceedings{
he2021deberta,
title={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},
author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=XPZIaotutsD}
}

@misc{https://doi.org/10.48550/arxiv.2204.06644,
  doi = {10.48550/ARXIV.2204.06644},
  
  url = {https://arxiv.org/abs/2204.06644},
  
  author = {Bajaj, Payal and Xiong, Chenyan and Ke, Guolin and Liu, Xiaodong and He, Di and Tiwary, Saurabh and Liu, Tie-Yan and Bennett, Paul and Song, Xia and Gao, Jianfeng},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {METRO: Efficient Denoising Pretraining of Large Scale Autoencoding Language Models with Model Generated Signals},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{NEURIPS2019_4496bf24,
 author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems},
 url = {https://proceedings.neurips.cc/paper/2019/file/4496bf24afe7fab6f046bf4923da8de6-Paper.pdf},
 volume = {32},
 year = {2019}
}


@article{Hendrycks2016,
  title={Gaussian Error Linear Units (GELUs)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}

@inproceedings{NEURIPS2020_ff4dfdf5,
 author = {Levine, Yoav and Wies, Noam and Sharir, Or and Bata, Hofit and Shashua, Amnon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {22640--22651},
 publisher = {Curran Associates, Inc.},
 title = {Limits to Depth Efficiencies of Self-Attention},
 url = {https://proceedings.neurips.cc/paper/2020/file/ff4dfdf5904e920ce52b48c1cef97829-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{10.1162/coli_a_00422,
    author = {Belinkov, Yonatan},
    title = "{Probing Classifiers: Promises, Shortcomings, and Advances}",
    journal = {Computational Linguistics},
    volume = {48},
    number = {1},
    pages = {207-219},
    year = {2022},
    month = {04},
    abstract = "{Probing classifiers have emerged as one of the prominent methodologies for interpreting and analyzing deep neural network models of natural language processing. The basic idea is simple—a classifier is trained to predict some linguistic property from a model’s representations—and has been used to examine a wide variety of models and properties. However, recent studies have demonstrated various methodological limitations of this approach. This squib critically reviews the probing classifiers framework, highlighting their promises, shortcomings, and advances.}",
    issn = {0891-2017},
    doi = {10.1162/coli_a_00422},
    url = {https://doi.org/10.1162/coli\_a\_00422},
    eprint = {https://direct.mit.edu/coli/article-pdf/48/1/207/2006605/coli\_a\_00422.pdf},
}





@article{DBLP:journals/corr/abs-2002-05202,
  author    = {Noam Shazeer},
  title     = {{GLU} Variants Improve Transformer},
  journal   = {CoRR},
  volume    = {abs/2002.05202},
  year      = {2020},
  url       = {https://arxiv.org/abs/2002.05202},
  eprinttype = {arXiv},
  eprint    = {2002.05202},
  timestamp = {Fri, 14 Feb 2020 12:07:41 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2002-05202.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{
shleifer2022normformer,
title={NormFormer: Improved Transformer Pretraining with Extra Normalization},
author={Sam Shleifer and Myle Ott},
year={2022},
url={https://openreview.net/forum?id=GMYWzWztDx5}
}

@inproceedings{
Lan2020ALBERT,
title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=H1eA7AEtvS}
}

@article{JMLR:v21:20-074,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@inproceedings{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{
tenney2018what,
title={What do you learn from context? Probing for sentence structure in contextualized word representations},
author={Ian Tenney and Patrick Xia and Berlin Chen and Alex Wang and Adam Poliak and R Thomas McCoy and Najoung Kim and Benjamin Van Durme and Sam Bowman and Dipanjan Das and Ellie Pavlick},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=SJzSgnRcKX},
}

@inproceedings{clark2020electra,
  title = {{ELECTRA}: Pre-training Text Encoders as Discriminators Rather Than Generators},
  author = {Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
  booktitle = {ICLR},
  year = {2020},
  url = {https://openreview.net/pdf?id=r1xMH1BtvB}
}

@article{10.1162/tacl_a_00321,
    author = {Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey, Anhad and Peng, Wei and Wang, Sheng-Fu and Bowman, Samuel R.},
    title = "{BLiMP: The Benchmark of Linguistic Minimal Pairs for English}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {8},
    pages = {377-392},
    year = {2020},
    month = {07},
    abstract = "{We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP),1 a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairs—that is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4\\%. We evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We find that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00321},
    url = {https://doi.org/10.1162/tacl\_a\_00321},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00321/1923697/tacl\_a\_00321.pdf},
}

@inproceedings{
levine2021pmimasking,
title={{\{}PMI{\}}-Masking: Principled masking of correlated spans},
author={Yoav Levine and Barak Lenz and Opher Lieber and Omri Abend and Kevin Leyton-Brown and Moshe Tennenholtz and Yoav Shoham},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=3Aoft6NWFej}
}

@article{DBLP:journals/corr/abs-1907-11692,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692}
}


@article{nguyen2019,
	author = {Nguyen, Dat Quoc  and Verspoor, Karin},
	year = "2019",
	title = {End-to-end neural relation extraction using deep biaffine attention},
	journal = {Advances in Information Retrieval},
	pages = "729--738",
	isbn = "978-3-030-15712-8",
	publisher = {Springer International Publishing}
}

@misc{peng2021sparse,
      title={Sparse Fuzzy Attention for Structured Sentiment Analysis}, 
      author={Letian Peng and Zuchao Li and Hai Zhao},
      year={2021},
      eprint={2109.06719},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{ZHANG201956,
title = "End-to-end neural opinion extraction with a transition-based model",
journal = "Information Systems",
volume = "80",
pages = "56 - 63",
year = "2019",
issn = "0306-4379",
doi = "https://doi.org/10.1016/j.is.2018.09.006",
url = "http://www.sciencedirect.com/science/article/pii/S0306437918301182",
author = "Meishan Zhang and Qiansheng Wang and Guohong Fu",
keywords = "Opinion extraction, End-to-end, Transition-based system",
abstract = "Fine-grained opinion extraction has received increasing interests in the natural language processing community. It usually involves several subtasks. Recently, joint methods and neural models have been investigated by several studies, achieving promising performance by using graph-based models such as conditional random field. In this work, we propose a novel end-to-end neural model alternatively for joint opinion extraction, by using a transition-based framework. First, we exploit multi-layer bi-directional long short term memory (LSTM) networks to encode the input sentences, and then decode incrementally based on partial output results dominated by a transition system. We use global normalization and beam search for training and decoding. Experiments on a standard benchmark show that the proposed end-to-end model can achieve competitive results compared with the state-of-the-art neural models of opinion extraction."
}

@inproceedings{barnes-etal-2021-structured,
    title = "Structured Sentiment Analysis as Dependency Graph Parsing",
    author = "Barnes, Jeremy  and
      Kurtz, Robin  and
      Oepen, Stephan  and
      {\O}vrelid, Lilja  and
      Velldal, Erik",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.263",
    doi = "10.18653/v1/2021.acl-long.263",
    pages = "3387--3402",
    abstract = "Structured sentiment analysis attempts to extract full opinion tuples from a text, but over time this task has been subdivided into smaller and smaller sub-tasks, e.g., target extraction or targeted polarity classification. We argue that this division has become counterproductive and propose a new unified framework to remedy the situation. We cast the structured sentiment problem as dependency graph parsing, where the nodes are spans of sentiment holders, targets and expressions, and the arcs are the relations between them. We perform experiments on five datasets in four languages (English, Norwegian, Basque, and Catalan) and show that this approach leads to strong improvements over state-of-the-art baselines. Our analysis shows that refining the sentiment graphs with syntactic dependency information further improves results.",
}

@inproceedings{maehlum-etal-2021-negation,
    title = "Negation in {N}orwegian: an annotated dataset",
    author = "M{\ae}hlum, Petter  and
      Barnes, Jeremy  and
      Kurtz, Robin  and
      {\O}vrelid, Lilja  and
      Velldal, Erik",
    booktitle = "Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)",
    month = may # " 31--2 " # jun,
    year = "2021",
    address = "Reykjavik, Iceland (Online)",
    publisher = {Link{\"o}ping University Electronic Press, Sweden},
    url = "https://aclanthology.org/2021.nodalida-main.30",
    pages = "299--308",
    abstract = "This paper introduces NorecNeg {--} the first annotated dataset of negation for Norwegian. Negation cues and their in-sentence scopes have been annotated across more than 11K sentences spanning more than 400 documents for a subset of the Norwegian Review Corpus (NoReC). In addition to providing in-depth discussion of the annotation guidelines, we also present a first set of benchmark results based on a graph-parsing approach.",
}

@article{Wiebe2005,
    author = {Wiebe, Janyce
              and Wilson, Theresa
              and Cardie, Claire},
    journal = {Language Resources and Evaluation},
    number = {2-3},
    pages = {165--210},
    title = {Annotating Expressions of Opinions and Emotions in Language},
    volume = {39},
    year = {2005}
}

@inbook { WheredidweGoWrongARetrospectiveLookattheBritishNationalCorpus,
      author = "Lou Burnard",
      title = "Where did we Go Wrong? A Retrospective Look at the British National Corpus",
      booktitle = "",
      year = "2002",
      publisher = "Brill",
      address = "Leiden, The Netherlands",
      isbn = "9789004334236",
      doi = "https://doi.org/10.1163/9789004334236_007",
      pages=      "51 - 70",
      url = "https://brill.com/view/book/edcoll/9789004334236/B9789004334236-s007.xml"
}

@book { TeachingandLearningbyDoingCorpusAnalysis,
      author = "Bernhard Kettemann and Georg Marko",
      title = "Teaching and Learning by Doing Corpus Analysis: Proceedings of the Fourth International Conference on Teaching and Language Corpora, Graz 19-24 July, 2000",
      year = "2016",
      publisher = "Brill",
      address = "Leiden, The Netherlands",
      isbn = "978-90-04-33423-6",
      doi = "https://doi.org/10.1163/9789004334236",
      url = "https://brill.com/view/title/30090"
}

@misc{peng2019knowing,
      title={Knowing What, How and Why: A Near Complete Solution for Aspect-based Sentiment Analysis}, 
      author={Haiyun Peng and Lu Xu and Lidong Bing and Fei Huang and Wei Lu and Luo Si},
      year={2019},
      eprint={1911.01616},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{hosseini-etal-2021-understanding,
    title = "Understanding by Understanding Not: Modeling Negation in Language Models",
    author = "Hosseini, Arian  and
      Reddy, Siva  and
      Bahdanau, Dzmitry  and
      Hjelm, R Devon  and
      Sordoni, Alessandro  and
      Courville, Aaron",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.102",
    doi = "10.18653/v1/2021.naacl-main.102",
    pages = "1301--1312",
    abstract = "Negation is a core construction in natural language. Despite being very successful on many tasks, state-of-the-art pre-trained language models often handle negation incorrectly. To improve language models in this regard, we propose to augment the language modeling objective with an unlikelihood objective that is based on negated generic sentences from a raw text corpus. By training BERT with the resulting combined objective we reduce the mean top 1 error rate to 4{\%} on the negated LAMA dataset. We also see some improvements on the negated NLI benchmarks.",
}

@inproceedings{dozat2016deep,
  author    = {Timothy Dozat and
               Christopher D. Manning},
  title     = {Deep Biaffine Attention for Neural Dependency Parsing},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=Hk95PK9le},
  timestamp = {Thu, 25 Jul 2019 14:25:56 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/DozatM17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{chelba14_interspeech,
  author={Ciprian Chelba and Tomas Mikolov and Mike Schuster and Qi Ge and Thorsten Brants and Phillipp Koehn and Tony Robinson},
  title={{One billion word benchmark for measuring progress in statistical language modeling}},
  year=2014,
  booktitle={Proc. Interspeech 2014},
  pages={2635--2639},
  doi={10.21437/Interspeech.2014-564}
}

@inproceedings{NEURIPS2019_dc6a7e65,
 author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
 url = {https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf},
 volume = {32},
 year = {2019}
}


@inproceedings{NEURIPS2020_1457c0d6,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}



@INPROCEEDINGS{7410368,

  author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},

  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 

  title={Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books}, 

  year={2015},

  volume={},

  number={},

  pages={19-27},

  doi={10.1109/ICCV.2015.11}}


@misc{https://doi.org/10.48550/arxiv.1609.08144,
  doi = {10.48550/ARXIV.1609.08144},
  
  url = {https://arxiv.org/abs/1609.08144},
  
  author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, Łukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{NIPS2017_3f5ee243,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{loshchilov2017decoupled,
  author    = {Ilya Loshchilov and
               Frank Hutter},
  title     = {Decoupled Weight Decay Regularization},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
               New Orleans, LA, USA, May 6-9, 2019},
  publisher = {OpenReview.net},
  year      = {2019},
  url       = {https://openreview.net/forum?id=Bkg6RiCqY7},
  timestamp = {Thu, 25 Jul 2019 14:26:04 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/LoshchilovH19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
zhang2021revisiting,
title={Revisiting Few-sample {\{}BERT{\}} Fine-tuning},
author={Tianyi Zhang and Felix Wu and Arzoo Katiyar and Kilian Q Weinberger and Yoav Artzi},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=cO1IH43yUF}
}

@inproceedings{chen2018gradnorm,
  title={Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks},
  author={Chen, Zhao and Badrinarayanan, Vijay and Lee, Chen-Yu and Rabinovich, Andrew},
  booktitle={International Conference on Machine Learning},
  pages={794--803},
  year={2018}
}

@inproceedings{berg-kirkpatrick2012empirical,
  title = {An {{Empirical Investigation}} of {{Statistical Significance}} in {{NLP}}},
  booktitle = {Proceedings of the 2012 {{Joint Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and {{Computational Natural Language Learning}}},
  author = {{Berg-Kirkpatrick}, Taylor and Burkett, David and Klein, Dan},
  year = {2012},
  pages = {995--1005},
  publisher = {{Association for Computational Linguistics}},
  address = {{Jeju Island, Korea}}
}

@inproceedings{barnes-etal-2022-semeval,
    title = "{S}em{E}val-2022 Task 10: Structured Sentiment Analysis",
    author = "Barnes, Jeremy and
              Oberl{\"a}nder, Laura Ana Maria and
              Troiano, Enrica and
              Kutuzov, Andrey and
              Buchmann, Jan and
              Agerri, Rodrigo and
              {\O}vrelid, Lilja  and
              Velldal, Erik",
    booktitle = "Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)",
    month = july,
    year = "2022",
    address = "Seattle",
    publisher = "Association for Computational Linguistics"
}