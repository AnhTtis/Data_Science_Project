\input{iccv2023AuthorKit/resources/tabels/open_seg}
\section{Experiment}
\subsection{Experimental Setup}
\noindent
\textbf{Datasets and Settings.}
In our experiments, we jointly pre-train on two types of data, including panoptic segmentation and object detection. For panoptic segmentation, we use 
COCO2017~\cite{lin2014microsoft} with segmentation annotations (around 110k images). For object detection, we use Objects365~\cite{shao2019objects365} (660k images for v1 and 1700k images for v2). We use Objects365v1 for training and ablating our tiny model and Objects365v2 only for training our large model. We evaluate our models on all tasks covered by pretraining, including semantic, instance, panoptic segmentation, and object detection. In particular, we benchmark on more than 60 datasets covering a wide range of domains on zero-shot segmentation and detection. 
\smallskip

\noindent
% \textbf{Training Details.}
% We build on Mask DINO~\cite{li2022mask} to implement our model. Mask DINO is a unified detection and segmentation framework that simultaneously predicts box and mask. Mask DINO consists of a backbone, a multi-layer Transformer encoder, and a multi-layer Transformer decoder. The encoder process the image features from the backbone and produces multi-scale features, which the decoder will query to decode mask and box.  \textit{For training simplicity, we only use online mask assistance without offline mask assistance by default}.
% \\
\textbf{Implementation Details.}
We build on Mask DINO~\cite{li2022mask} to implement our model. Mask DINO is a unified detection and segmentation framework which simultaneously predicts box and mask. We follow~\cite{li2022mask} to use 300 latent queries and nine decoder layers for thing categories in instance segmentation and add 100 panoptic queries for stuff categories. For the visual backbone, we adopt pretrained Swin-T/L~\cite{liu2021swin} by default. We also use Focal-T~\cite{yang2022focal} in our ablation studies following~\cite{zou2022generalized}. For the language backbone, we adopt the pretrained base model in UniCL~\cite{yang2022unified}. Particularly, our model only uses these pretrained backbones and does not use other image-text pairs or grounding data for pretraining~\cite{zou2022generalized,li2022grounded}. 
During pretraining, we set a minibatch for segmentation to $32$ and detection to $64$, and the image resolution is $1024\times 1024$ for both segmentation and detection. During fine-tuning, we use $512\times 1024$ for Cityscapes~\cite{cordts2015cityscapes} and $640\times 640$ for ADE20K~\cite{zhou2017scene} by default. Following the balanced sampling strategy in~\cite{yang2022unified, zou2022generalized}, the segmentation data are always sampled for a consistent number of epochs, regardless of the total number of detection data. We use AdamW~\cite{loshchilov2017decoupled} as the optimizer. We pre-train our model on the joint dataset for 30 epochs. The learning rate is set to $0.0001$, which is decayed at 0.9 and 0.95 fractions of the total number of steps by 10. \textit{Unless otherwise specified, we use online mask assistance during our pretraining by default}.
% We follow Mask DINO to use deformable attention~\cite{zhu2020deformable} in both the Transformer encoder and decoder. \\
% \textbf{End-to-end training. }Our model can be end-to-end pretrained with both detection and segmentation data. After the joint pretraining, \ourmodel{} can directly produce zero-shot detection and segmentation on open-vocabulary datasets with one suit of weights. Third, training and inference difference.

\subsection{Open-Vocabulary Benchmarking}
After pretraining our \ourmodel{} on COCO and Objects365, we evaluate it on a wide range of datasets in a zero-shot manner. Following~\cite{zou2022generalized}, we cover six commonly used segmentation datasets, including indoor scenes (ADE20K~\cite{zhou2017scene}), outdoor scenes (Cityscapes~\cite{cordts2015cityscapes}), and driving scenes (BDD100K~\cite{yu2018bdd100k}). In addition, we evaluate both segmentation and detection performance on LVIS~\cite{gupta2019lvis}. We report PQ, mask AP, and mIoU for panoptic, instance, and semantic segmentation, respectively, and use box AP for detection. The results are shown in Table~\ref{tab:open_seg}. 

\input{iccv2023AuthorKit/resources/tabels/task_transfer}
\input{iccv2023AuthorKit/resources/tabels/seginw}
% \noindent
% \textbf{Comparison with strong baseline.} We build a strong baseline \ourmodel{} w/o DET that includes all our technical improvements but not using detection data for joint training. The results show that after adding the detection data, the open segmentation performance is significantly boosted, especially in mask AP and PQ. This indicates that the instance-level representation learned from detection supervision is seamlessly transferred to pixel-level segmentation.
% \\
% \noindent
% \textbf{Comparison with state-of-the-art methods.} 
We first compare with previous works on segmentation tasks. Overall, our model achieves significantly better performance on instance segmentation and comparable performance for panoptic and semantic segmentation. Compared with state-of-the-art methods ODISE~\cite{xu2023open} and X-Decoder~\cite{zou2022generalized}, \ourmodel{} achieves \textbf{1.1} and \textbf{1.9} mask AP improvements on ADE20K, respectively. This gap is even larger on Cityscapes and LVIS. Our \ourmodel{} outperforms X-Decoder by \textbf{10.2} and \textbf{8.3} mask AP with a tiny and large model on Cityscapes, respectively. On LVIS, we evaluate the mask AP with the released X-Decoder tiny model and the comparison shows \textbf{9.8} mask AP improvement with our \ourmodel{} tiny model. These results indicate that the proposed joint learning method can effectively transfer the instance-level knowledge in detection data for instance segmentation. Compared with instance segmentation, both panoptic and semantic segmentation requires the segmentation of background stuff, which is fully absent in the detection data. Despite that, our \ourmodel{} still outperforms X-Decoder for panoptic segmentation on 3 out of 4 datasets (except for ADE20K), and achieves comparable semantic segmentation performance. The results on three segmentation tasks suggest that detection data significantly benefit the instance-level understanding while image-text pairs mainly augment the semantic understanding for semantic segmentation.
% At last, we note that X-Decoder uses a large amount of image-text pairs (4M) and complex captioning/referring losses, while our \ourmodel{} only leverages detection supervision in a simpler way.
In addition to segmentation, \ourmodel{} also produces reasonably good detection performance. Compared with GLIP, our \ourmodel{}~(T) outperforms GLIP~(T) (setting A) for zero-shot detection on LVIS (\textbf{21.8}~\textit{v.s.}~18.5), where both only use Objects365 as the pretraining detection dataset. \emph{At last, we highlight that our model is the first one that can be pretrained with segmentation and detection data jointly and perform zero-shot transfer to both tasks.}
% In Table~\ref{tab:open_seg}, we directly evaluate the pretrained \ourmodel{} on various open segmentation datasets. With this pretrained weights, we 
\input{iccv2023AuthorKit/resources/tabels/odinw}
\subsection{Direct and Task-Specific Transfer}
After pretraining, our model can be directly transferred to downstream segmentation and detection tasks. In Table~\ref{tab:task_transfer}, we compare our model with both closed-set and open-vocabulary methods. Remarkably, our model achieves SOTA performance \textbf{59.5} PQ for COCO panoptic segmentation \textit{without any further fine-tuning}. After data-specific fine-tuning, \ourmodel{} establishes a new SOTA on ADE20K panoptic~(\textbf{53.7} PQ) and instance segmentation~(\textbf{42.6} AP) when trained with $1280\times 1280$ image size. In addition, we also achieve a new SOTA on Cityscapes instance segmentation~(\textbf{48.5} AP). These results indicate the jointly pretrained open-vocabulary model can also be well transferred to closed-set detection and segmentation.
% swint final model (fine-tune)
% mask dino swint-> ade; cityscapes; coco
% mask dino swinL
% \subsection{Segmentation in the Wild}
% To further explore the generalization ability of \ourmodel{}, we evaluate our model on more domain-specific datasets. We do a zero-shot evaluation of our model on the Segmentation in the Wild (SeginW) benchmark~\cite{zou2022generalized}, which includes 25 datasets. This benchmark focuses on instance segmentation, so we report the average and median mAP of all the datasets following the common practice. The results shown in Table~\ref{tab:seginw} indicate that adding detection supervision significantly improves the segmentation performance by more than \textbf{10} AP under the same setting.

\subsection{Segmentation and Detection in the Wild}

To investigate the generalization ability of \ourmodel{} for segmentation, we evaluate our model on more domain-specific datasets. We conduct a zero-shot evaluation of our model on the Segmentation in the Wild (SeginW) benchmark~\cite{zou2022generalized}, which includes 25 datasets. As this benchmark focuses on instance segmentation, we report the average and median mAP of all the datasets following the common practice. The results in Table~\ref{tab:seginw} indicate that combining detection supervision significantly improves the segmentation performance by more than \textbf{10} AP under the same setting.

To further study the object detection ability of \ourmodel{}, we follow GLIP~\cite{li2022grounded} to evaluate  detection performance on Object Detection in the Wild (ODinW) benchmark. It collects over 35 datasets and is closer to real-world scenarios. We report the average and median mAP of all 35 datasets. With the jointly pretrained model weights, we directly evaluate this challenging benchmark in a zero-shot manner.  Under the same setting that only uses Objects365 as the detection data for training, our tiny model outperforms GLIP-T (setting A) by $2.8$ AP in average.
% \subsection{Metrics}

\input{iccv2023AuthorKit/resources/tabels/ablation_coco}
\input{iccv2023AuthorKit/resources/tabels/ablation_annotation}
% \input{iccv2023AuthorKit/resources/tabels/ablation_teacher_matching}
\subsection{Ablation}
\noindent
\textbf{Ablation on our basic components.} We first ablate on our basic model to verify whether each basic component works as expected. We evaluate our model on the COCO closed-set panoptic segmentation task. The first two rows in Table~\ref{tab:ablation:coco} show that our model can achieve a comparable closed-set panoptic segmentation performance as MaskDINO using the same ResNet-50 backbone. It is also shown in the last four rows that our framework can significantly improve the segmentation performance by combining tasks and utilizing detection data across different backbones.
\\
\textbf{Ablation on the offline mask supervision.}
As we discussed earlier, the proposed conditioned mask decoding can be used either in online or offline manner. Here, to investigate the effectiveness of offline setting, we generate pseudo annotations with our large models and then use them to tune the tiny models. Concretely, we first generate pseudo masks on Objects365 conditioned on boxes with our \ourmodel{}~(L) model. Given the masks, we conduct experiments on two models, including the open-vocabulary model (\ourmodel{}) and the closed-set model (MaskDINO) by using the pseudo masks for supervision. As shown in Table~\ref{tab:ablation:offline anno}, when evaluating our models, we find that the mask AP and box AP are significantly improved.
When conducting experiments on MaskDINO, we perform object detection for the setting without pseudo annotation and instance segmentation for the setting with pseudo annotations during pretraining. After pretraining, we do a zero-shot evaluation on COCO. We also use the pretrained model for fine-tuning. The results in Table~\ref{tab:ablation:offline anno closedset} indicate that in both settings, pseudo-annotations improve performance. It is also shown that the box AP can also be improved accordingly with extra mask annotations. In addition, we are the first to report zero-shot segmentation performance on COCO which is comparable with the full-shot performance of other methods.
\\
\noindent
\textbf{Ablation on decoupled decoding and online mask assistance.} In Table~\ref{tab:ablation:query split}, we conduct experiments to show the effectiveness of our proposed components by removing them one at a time. In the second row, after removing online mask assistance, the instance mask and box performance on the open-segmentation dataset ADE20K drops by 0.6 AP and 0.5 AP, respectively. In the third row, when we remove the decoupled decoding, the performance of the mask and box is further impacted by a large margin (-1.1 AP and -2.3 AP for ADE mask and box). Both results suggest that the proposed techniques help to mitigate the gap between segmentation and detection data.
% decoupled foreground and background decoding. Table~\ref{tab:ablation:query split} shows that our decoupled foreground and background decoding achieve performance gain on both settings with Focal-T~\cite{yang2022focal} and Swin-T as the backbone. The improvement is especially significant in mask AP and box AP. 
% \\
% \textit{We present visualizations in the Appendix.}
% \Feng{merge the two ablations into Effectiveness of Each Component}
% \textbf{Ablation of the online mask matching.}
% To evaluate the effectiveness of conditioned masks generated with GT boxes and labels, we conduct ablation study on two commonly used open vocabulary segmentation datasets ADE20K and Cityscapes. The results shows that \ourmodel{} with conditioned mask matching achieve better performance on both datasets.\\
% \noindent

\input{iccv2023AuthorKit/resources/tabels/annotation_maskdino}
\input{iccv2023AuthorKit/resources/tabels/ablation_split}




% \input{iccv2023AuthorKit/resources/tabels/metrics_ov}

% 1. only coco result
%     1. 有没有o365 gain
%     2. reproduce mask dino
    
% 2. task split/discrepancy
% 3. annotation
% 4. metric
% 5. analysis
%     1. good/bad case
%     2. box condition/ visualize/ demo
    
% 6. zero-shot referring evaluation

    