\documentclass[10pt,twocolumn,letterpaper]{article}

% \usepackage{iccv}
% \usepackage{times}
% \usepackage{epsfig}
% \usepackage{graphicx}
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{adjustbox}
% \usepackage{hhline}

% \usepackage{siunitx}
% \usepackage{cite}
% \usepackage{ctable}
% \usepackage{hhline}
% \usepackage{booktabs}
% \usepackage{subcaption}
% \usepackage{csquotes}
\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{color}

\usepackage{multirow}
% \usepackage{float}
\usepackage{stfloats}

\usepackage{amsthm}
\usepackage{comment}
\usepackage{listings}
\usepackage{algorithm}


% Include other packages here, before hyperref.
\usepackage{siunitx}
\usepackage{cite}
\usepackage{ctable}
\usepackage{hhline}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{csquotes}
\usepackage{adjustbox}
% \usepackage{tabularray}
\usepackage{tabu}
\usepackage{enumitem}
\usepackage{tabularx}
\usepackage[normalem]{ulem}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{booktabs}
\usepackage{tablefootnote}
\input{table_style_lib.tex}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission
\usepackage{color, colortbl}
\definecolor{Gray}{gray}{0.9}
\definecolor{Red}{RGB}{230, 57, 70}
\def\iccvPaperID{6690} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% \newcommand{\ourmodel}{UniSeg}
\newcommand{\ourmodel}{\textit{OpenSeeD}}
\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}
\newcommand{\Hao}[1]{\textcolor{red}{[Hao: #1]}}
\newcommand{\Feng}[1]{\textcolor{blue}{[Feng: #1]}}
\newcommand{\Jianwei}[1]{\textcolor{purple}{[Jianwei: #1]}}
\newcommand{\xyz}[1]{\textcolor{orange}{[Xyz: #1]}}

\newcommand{\tl}[1]{\multicolumn{1}{l}{#1}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{A Simple Framework for Open-Vocabulary Segmentation and Detection}

\author{\textbf{ ~Hao Zhang$^{1,2}$\thanks{Equal contribution. List in random.}, Feng Li$^{1,2*}$, ~Xueyan Zou$^{4}$, ~Shilong Liu$^{2,5}$, ~Chunyuan Li$^{3}$} \\ \textbf{~Jianfeng Gao$^{3}$, ~Jianwei Yang$^{3}$\thanks{Equal advisory contribution.}, ~Lei Zhang$^{2\dag}$\hspace{1.mm}} \\
$^1$The Hong Kong University of Science and Technology. \\
$^2$International Digital Economy Academy (IDEA). \\
$^3$Microsoft Research at Redmond. \\
$^4$University of Wisconsin-Madison. \\
$^5$Dept. of CST., BNRist Center, Institute for AI, Tsinghua University. 
% $^5$The Hong Kong University of Science and Technology (Guangzhou).\\
% \texttt{\{hzhangcx, fliay\}@connect.ust.hk} \\
% \texttt{\{liusl20\}@mails.tsinghua.edu.cn} \\
% \texttt{\{eeli.hongyang\}@mail.scut.edu} \\
% \texttt{\{leizhang\}@idea.edu.cn} \\
% \texttt{\{ni\}@ust.hk} \\
% \and
\\
\centerline{\tt\tiny  
\{hzhangcx, fliay\}@connect.ust.hk
 \ \{xueyan\}@cs.wisc.edu 
 \ \{liusl20\}@mails.tsinghua.edu.cn
 \ \{jianwyan,jfgao,chunyl\}@microsoft.com
 \ \{leizhang\}@idea.edu.cn
}
}
\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\vspace{-3.3em}
\begin{center}
    \centering
    \includegraphics[width=0.98\textwidth]{iccv2023AuthorKit/resources/imgs/introfig_v3.pdf}  
    % \includegraphics[width=0.98\textwidth]{iccv2023AuthorKit/resources/imgs/introfig_v3_jw.pdf}
\vspace{0.2em}
\captionof{figure}{Upper row: visualizations of \ourmodel{} for open-vocabulary instance segmentation and detection, panoptic and semantic segmentation, instance segmentation in the wild, and conditioned segmentation given referring box location and concept. Lower row: Our \ourmodel{} model outperforms previous state-of-the-art methods (listed below each gray bar) on eight benchmarks in zero-shot and task-specific transfer settings.
% Right: Our \ourmodel{} model outperforms previous state-of-the-art methods (listed below each gray bar) on eight benchmarks in zero-shot and task-specific transfer settings.
}
\label{fig:intro}
\vspace{0.5em}
\end{center}%
}]

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
\let\thefootnote\relax\footnotetext{$^*$Equal contribution. List in random.}
\let\thefootnote\relax\footnotetext{$^\dag$Equal advisory contribution.}
   We present \ourmodel{}, a simple \textbf{Open}-vocabulary \textbf{Se}gm\textbf{e}ntation and \textbf{D}etection framework that jointly learns from different segmentation and detection datasets. 
   To bridge the gap of vocabulary and annotation granularity, we first introduce a pre-trained text encoder to encode all the visual concepts in two tasks and learn a common semantic space for them. This gives us reasonably good results compared with the counterparts trained on segmentation task only. To further reconcile them, we locate two discrepancies: $i$) task discrepancy -- segmentation requires extracting masks for both foreground objects and background stuff, while detection merely cares about the former; $ii$) data discrepancy -- box and mask annotations are with different spatial granularity, and thus not directly interchangeable. To address these issues, we propose a decoupled decoding to reduce the interference between foreground/background and a conditioned mask decoding to assist in generating masks for given boxes. To this end, we develop a simple encoder-decoder model encompassing all three techniques and train it jointly on COCO and \let\thefootnote\relax\footnotetext{This work is developed during an internship at IDEA.}
   \noindent Objects365. After pre-training, our model exhibits competitive or stronger zero-shot transferability for both segmentation and detection. Specifically, \ourmodel{} beats the state-of-the-art method for open-vocabulary instance and panoptic segmentation across 5 datasets, and outperforms previous work for open-vocabulary detection on LVIS and ODinW under similar settings. When transferred to specific tasks, our model achieves new SoTA for panoptic segmentation on COCO and ADE20K, and instance segmentation on ADE20K and Cityscapes. The lower row in Fig.~\ref{fig:intro} shows a comparison of the performance of \ourmodel{} and previous SoTA methods. Finally, we note that \ourmodel{} is the first to explore the potential of joint training on segmentation and detection, and hope it can be received as a strong baseline for developing a single model for both tasks in open world. Code will be released at \url{https://github.com/IDEA-Research/OpenSeeD}.
   
\end{abstract}
\input{iccv2023AuthorKit/resources/imgs/teaser}
%%%%%%%%% BODY TEXT
\input{iccv2023AuthorKit/resources/sections/intro}

\input{iccv2023AuthorKit/resources/sections/related_work}

\input{iccv2023AuthorKit/resources/sections/method}

\input{iccv2023AuthorKit/resources/sections/experiments}

\input{iccv2023AuthorKit/resources/sections/conclusion}

%-------------------------------------------------------------------------



{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}