\documentclass[10pt,twocolumn,letterpaper]{article}

% \usepackage{iccv}
% \usepackage{times}
% \usepackage{epsfig}
% \usepackage{graphicx}
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{adjustbox}
% \usepackage{hhline}

% \usepackage{siunitx}
% \usepackage{cite}
% \usepackage{ctable}
% \usepackage{hhline}
% \usepackage{booktabs}
% \usepackage{subcaption}
% \usepackage{csquotes}
\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tablefootnote}
\usepackage{color}

\usepackage{multirow}
% \usepackage{float}
\usepackage{stfloats}

\usepackage{amsthm}
\usepackage{comment}
\usepackage{listings}
\usepackage{algorithm}


% Include other packages here, before hyperref.
\usepackage{siunitx}
\usepackage{cite}
\usepackage{ctable}
\usepackage{hhline}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{csquotes}
\usepackage{adjustbox}
\usepackage{tabularray}
\usepackage{tabu}
\usepackage{tabularx}
\usepackage[normalem]{ulem}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{booktabs}
\input{table_style_lib.tex}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% \iccvfinalcopy % *** Uncomment this line for the final submission
\usepackage{color, colortbl}
\definecolor{Gray}{gray}{0.9}
\definecolor{Red}{RGB}{230, 57, 70}
\def\iccvPaperID{6690} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% \newcommand{\ourmodel}{UniSeg}
\newcommand{\ourmodel}{\textit{OpenSeeD}}
\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}
\newcommand{\Hao}[1]{\textcolor{red}{[Hao: #1]}}
\newcommand{\Feng}[1]{\textcolor{blue}{[Feng: #1]}}
\newcommand{\Jianwei}[1]{\textcolor{purple}{[Jianwei: #1]}}
\newcommand{\xyz}[1]{\textcolor{orange}{[Xyz: #1]}}

\newcommand{\tl}[1]{\multicolumn{1}{l}{#1}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{---Supplementary Materials---\\  A Simple Framework for Open-Vocabulary Segmentation and Detection}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


\appendix
\section*{Overview}
This supplementary material presents more details and additional results not included in the main paper due to page limitation. The list of items included are:

\begin{itemize}
    \item Correction of typos in Sec.~\ref{typo}.
    \item More experimental results in Sec.~\ref{exp}.
    \item Visualization of the predictions of \ourmodel{} in Sec.~\ref{vis}.
    \item More implementation details in Sec.~\ref{imp}.
\end{itemize}
\section{Typo Correction}
\label{typo}
 There are two typos in the paper. We are sorry if they affect your reading. We will fix them in the next version.\\
 \begin{enumerate}
 \item There is a typo of duplicate sentences ``Mask2Former and MaskDINO treat ... handling foreground and background." in ``Comparison with previous works " in section 3.2. 
 \item There is a typo in Row 3 in Table 8. The correct version should be Table~\ref{tab:ablation:offline anno closedset}.
 \end{enumerate}
 \setcounter{table}{9}
\input{iccv2023AuthorKit/resources/tabels/offline_closedset_fixed}

\section{More Experimental Results}
\label{exp}
\subsection{SwinL 4-scale results}
Our Swin-L results in Table 2 adopts 5 scales of image features. In order to compare with other methods more thoroughly, we show the performance of our model with Swin-L and 4 scales of image features in Table~\ref{tab_supp:open_seg_swinl}.
% a few warmup epochs trained on COCO panoptic segmentation which harms the results. 
% Therefore, we report new results in Table~\ref{tab_supp:open_seg_swinl}.
\input{iccv2023AuthorKit/resources/tabels/openseg_swinL_update}
\subsection{Offline Mask Guidance Ablation with SwinT}
In order to be coherent with Table 2, we also show the ablation of offline mask assistance in Table~\ref{tab_supp:ablation: offline anno swint}, which verifies the effectiveness of our pseudo-annotations.
\input{iccv2023AuthorKit/resources/tabels/ablation_annotation_swint}
\section{Visualization}
In this section, we show a visualization of \ourmodel{} for open-vocabulary segmentation on ADE20K and Objects365 we also show the conditioned segmentation ability of \ourmodel{}. Note that all experiments here utilize the model jointly trained on COCO panoptic segmentation and Objects365 detection without fine-tuning.
\label{vis}
% \subsection{Segmentation in O365 Seen Categories}
\subsection{Three open-vocabulary segmentation tasks on ADE20K}
\input{iccv2023AuthorKit/resources/imgs/supp_basic_seg}
In Fig.~\ref{fig:supp basic seg}, we show a visualization of \ourmodel{} for open-vocabulary instance segmentation and detection, panoptic and semantic segmentation on ADE20K dataset without finetuning.
\subsection{Segmentation in Objects365 Categories}
\input{iccv2023AuthorKit/resources/imgs/supp_o365_unseen}
In Fig.~\ref{fig:supp o365 unseen}, we show the instance segmentation on Objects365 where unseen concepts are listed under each image. Unseen concepts are the categories that do not exist in COCO, which means they are not trained with segmentation annotations. Our model can segment instances from the unseen categories well although it is only trained with detection task on these categories.
\subsection{Conditioned Segmentation}
\input{iccv2023AuthorKit/resources/imgs/supp_cond_ade}
In Fig.~\ref{fig:supp cond ade}, we show the conditioned segmentation ability of \ourmodel{}. When we give different conditioned boxes and text, we can obtain the corresponding masks.
\section{More Implementation Details}
\label{imp}
\input{iccv2023AuthorKit/resources/imgs/cond_matching}
\textbf{Online Conditioned Mask as the Guidance. }
% to Assist Detection Training
%put online and offline together to strength
% \Feng{direct match box with gt in query selection; evaluate on other open-seg datasets with swinL/online teacher} 
% \Feng{reason only use it for matching: 1. label assignment: lift the classification score of real tgt and lower the false positives with bad mask performance; 2. assumption: matching consistency.}
% According to Table~\ref{tab:box2mask}, our conditioned masks have much higher mask AP than the masks predicted without condition. 
\noindent 
% Our model is trained with both detection and segmentation, where GT masks are missing in the detection task in matching and training loss. 
Given that our model can generate reasonably good masks with GT boxes as the condition, we seek to 
% generate masks for the detection data and
use them to better align detection with segmentation. A straightforward way is directly using masks for supervision on the fly. This requires high mask quality during the whole training phase, which however is not true, especially in the early training stage. 
% while our mask quality is inferior to GT (shown in Table~\ref{tab:box2mask}) \Jianwei{do we have GT numbers in Table 1?}, especially in the early training stage. 
Therefore, we alternatively use the generated mask as the additional guidance to find the matched foreground queries with the GT concept and box in detection data. % to align the label assignment for detection and segmentation. 
As shown in Fig.~\ref{fig:cond matching}~(a), detection during training fully ignores the predicted mask quality when finding the matched foreground queries, which is different from segmentation in Fig.~\ref{fig:cond matching}~(b). This ignorance may lead to a biased matching toward box quality for our model which needs to produce high-quality boxes and masks simultaneously. Therefore, we use the generated masks from GT boxes and labels as the guidance for better label assignment, as shown in Fig.~\ref{fig:cond matching}~(c). 
% do use mask to assign predicts to targets, which disregards mask quality. 
% Therefore, in our joint training model, when a detection image has two predictions with similar box quality but different mask predictions, it will assign the target to the one with higher box quality.  If the assigned one has low mask quality, this label assignment will penalize the high-quality mask prediction by lowering its classification score. \Feng{Described not so clear here.} 
% With the conditioned masks from GT boxes and labels, we use the generated masks for better label assignment, as shown in Fig.~\ref{fig:cond matching}~(c). 
\\
\textbf{Offline Conditioned Mask as the Supervision. } Another way to better use the generated mask is to train two models. For example, we can train a large conditioned mask decoding model first and then use it to generate pseudo masks for all the detection data. Afterward, we can train the second model on the annotated detection dataset with mask supervision. However, though our model can generate fairly good masks for novel categories, these masks still has inferior quality compared to the trained categories. Considering that 85 categories in Objects365~\cite{shao2019objects365} are in the COCO foreground category set, we treat the generated annotations on these categories as \textit{golden annotations} because they are trained with mask annotations on COCO. Other annotations are \textit{coarse annotations}. We adopt different strategies for the two types of annotations, where \textit{golden annotations} are used for mask supervision while \textit{coarse annotations} are only used for matching (similar to our online guidance). 
% \Feng{Complete this paragraph.}
% \noindent\textbf{SwinL Setting. }For models that use SwinL backbone, we use five feature scales.
% \\
% \textbf{Finetune Setting. }
% \\
% \subsection{Detection}
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}
\end{document}