\section{Introduction}
Developing vision systems that can be transferable to novel concepts or domains has emerged as an important research topic in the community. In the light of strong zero-shot transferability demonstrated in the seminal work CLIP~\cite{radford2021learning}, a number of researchers have attempted to build advanced open-vocabulary models by leveraging large-scale image-text pairs for fine-grained vision tasks like detection~\cite{gu2021open,li2022grounded,zhong2022regionclip,gao2022open,kuo2022f} and segmentation~\cite{ding2022open,huynh2022open,xu2023side,zhou2021denseclip}.

Arguably, core vision tasks like detection and segmentation are fairly distinct in their vocabulary sizes and spatial granularities of supervision, as illustrated in Fig.~\ref{fig:teaser}~(a). For example, the commonly used public detection dataset Objects365~\cite{shao2019objects365} contains box annotations for 365 concepts in around 1.7M images, while mask annotations in COCO~\cite{lin2014microsoft} cover merely 133 categories in 0.1M images. Previous works have explored different ways of leveraging a large amount of image-text data for open-vocabulary detection or segmentation, such as distilling the visual-semantic representations from multi-modal foundation models~\cite{gu2021open,zhong2022regionclip}, designing fine-grained or augmented contrastive learning methods~\cite{mukhoti2022open} or utilizing pseudo-labeling techniques ~\cite{li2022grounded,zhang2022glipv2}. To the best of our knowledge, most (if not all) of them focused on how to improve the performance for either detection or segmentation. Moreover, transferring weak image-level supervision to fine-grained tasks usually requires sophisticated designs to mitigate the huge granularity gap and is vulnerable to noises in image-text pairs. This leads to a natural question: \emph{can we bridge detection and segmentation that are cleaner and have a closer gap to attain a good open-vocabulary model for both?}



Taking one step back, marrying detection and segmentation had been previously explored in two main ways. On one hand, Mask R-CNN~\cite{he2017mask} is one of the first works that proposed to jointly learn detection and instance segmentation on COCO. On the other hand, it is shown that detection models pre-trained on Objects365 can be feasibly transferred for COCO panoptic segmentation~\cite{li2022mask}. However, as depicted in Fig.~\ref{fig:teaser}~(b), the former method requires the model to be trained on the same dataset containing aligned box and mask annotations, while the latter method follows pre-train-then-fine-tune protocol, leading to two separate closed-set models. In this work, we are the first to propose jointly learning from detection and segmentation data, and more importantly serving an open-vocabulary model for both tasks (Fig.~\ref{fig:teaser}~(b) bottom). 
Achieving this goal requires answering two critical questions: $i$) how to transfer the semantic knowledge across detection and segmentation data; $ii$) how to bridge the gap between box and mask supervision. First, the vocabulary shares commons but also bear substantial differences between the two tasks. We need to accommodate the two vocabularies and further go beyond towards \textit{open} vocabulary. Second, semantic and panoptic segmentation tasks require segmenting not only foreground objects (things like ``dog'' and ``cat''.) but also background concepts (stuff like ``sky'' and ``building''), while detection task solely cares about foreground objects. Third, box supervision by nature is coarser than mask supervision. We can convert masks into boxes but hardly vice versa.

To the end, we propose \ourmodel{}, a simple encoder-decoder framework to reconcile the two tasks by mitigating the aforementioned problems. Concretely, we first exploit a single text encoder to encode all concepts occurring in the data and train our model to align the visual tokens with the semantics in a common space. Second, we explicitly divide the object queries in the decoder into two sub-types: foreground and background queries, where the first group is responsible for foreground objects from both segmentation and detection while the second group is only for background stuffs in segmentation. Third, we introduce conditioned mask decoding which learns to decode masks from ground-truth boxes from segmentation data and generates the mask assistant for detection data. 
% Based on the generated masks, we operate two approaches, namely online mask assistance and offline mask assistance to mitigate the data gap between segmentation and detection. 
% Because the inputs are all ground-truth boxes, the generated masks are with relatively high quality as we will show in Sec.~\ref{}\Jianwei{point to visualizations}. Afterwards, 
As a result, our \ourmodel{} is able to learn from separate detection and segmentation data seamlessly and achieves outstanding or competitive zero-shot and transfer performance across various tasks/datasets. Fig~\ref{fig:intro} shows a visualization of our model on instance, panoptic and semantic segmentation tasks. It also shows the segmentation results on datasets that largely differ from our training data such as the SeginW datasets and demonstrates the conditioned segmentation ability of \ourmodel{}. \textit{Given the encouraging results, we hope our work can contribute as the first strong baseline for developing a single open-vocabulary model for both tasks}.

\input{iccv2023AuthorKit/resources/imgs/teaser}

% baseline for open-vocabulary seg-
% mentation in three aspects covering model, data and met-
% rics. First, we propose a strong baseline model that can
% leverage both pixel-level and region-level annotations and
% achieves significant improvement on open-vocabulary seg-
% mentation over previous methods. Second, we generate
% pseudo-annotations with our strongest model to extend Ob-
% ject365 to a segmentation dataset. We also show that our
% pseudo-annotations can improve the performance of tiny
% models when used as training labels. Finally, we propose a
% set of metrics for open-vocabulary segmentation which take
% the similarity between classes into consideration.

\noindent
\textbf{Contributions}. To summarize, our main contributions are:
\begin{itemize}[nolistsep]
    \item We are the first to present a strong baseline model that can jointly learn from detection and segmentation data towards an open-vocabulary model for both tasks.
    \item We locate the discrepancies in two tasks/datasets and propose separate techniques including shared semantic space, decoupled decoding, and conditioned mask assistance to mitigate the issues.
    % We note the gap between the detection and segmentation task and propose a new decoder encompassing a decoupled decoding and 
    % an online teacher and separate foreground/background decoding to mitigate the aforementioned issues.
    \item By jointly training our model on segmentation and detection data, we achieve new state-of-the-art segmentation performance for zero-shot and task transfer across a variety of datasets, and competitive performance for zero-shot object detection.
\end{itemize}