\section{Method}
\input{iccv2023AuthorKit/resources/imgs/model_framework}
% Our baseline encompasses a strong baseline model, a series of open-vocabulary metrics, and a segmentation dataset labeled by our model conditioned on boxes.
% \subsection{Model}
% 1.  generate mask from box
%     bridge the gap between mask and box
%     gt box generate mask; box serve as anchor to guide mask;

%     only match
    
% 2. panoptic query
%     bridge the gap between stuff and thing
%     text encoder for vocabulary unification

% 3. architecture design

% 4. one-stage/joint training; inference
Given segmentation and detection datasets, \ourmodel{} is aimed at learning an open-vocabulary model for both tasks. Formally, let $\mathcal{D}_m = \{{I}_i, (\mathbf{c}_i, \mathbf{m}_i)\}_{i=1}^{M}$ denote the segmentation dataset of size $M$ and $\mathcal{D}_b = \{I_j, (\mathbf{c}_j, \mathbf{b}_j)\}_{j=1}^{N}$ the detection dataset of size $N$, where $\mathbf{c}$ are the visual concepts in an image, and $\mathbf{m}$ and $\mathbf{b}$ the corresponding masks and boxes, respectively. Suppose $\mathcal{V}=\{c_1,...c_K\}$ be the vocabulary of unique $K$ visual concepts appearing in $\mathcal{D}_m$ and $\mathcal{D}_b$. The goal of \ourmodel{} is learning to detect \textit{and} segment visual concepts in $\mathcal{V}$ and beyond.
% by learning a joint semantic space and bridging the region-level and pixel-level supervisions. 

To achieve the goal, we exploit a general encoder-decoder design and employ a text encoder for our \ourmodel{}, as shown in Fig.~\ref{fig:framework}. Our model takes as input an image $I$ and the vocabulary $\mathcal{V}$ and output a set of predictions including masks $\mathbf{P^m}$, boxes $\mathbf{P^b}$, and classification scores $\mathbf{P^c}$. As a whole, $\langle\mathbf{P^m}, \mathbf{P^b}, \mathbf{P^c}\rangle=\mathsf{OpenSeeD}(I,\mathcal{V})$. More specifically, our model consists of one image encoder $\mathsf{Enc_I}$, one text encoder $\mathsf{Enc_T}$, and one decoder $\mathsf{Dec}$. Given an image $I$ and the vocabulary $\mathcal{V}$, we first encode them by $\mathsf{Enc_I}$ and $\mathsf{Enc_T}$, respectively: 
\begin{equation}
    \mathbf{O}=\mathsf{Enc_I}(I), \mathbf{T}=\mathsf{Enc_T}(\mathcal{V})
\end{equation}
where the image features $\mathbf{O}\in \mathcal{R}^{H \times W \times C}$, 
% queries $\mathbf{Q} \in \mathcal{R}^{L \times C}$ to do the corresponding detection and segmentation tasks. 
and the text features $\mathbf{T}= \{t_1, t_2, ..., t_K\}$. Afterward, the decoder takes $L$ queries $\mathbf{Q} \in \mathcal{R}^{L \times C}$ as inputs and cross-attends the image features to get outputs:
\begin{equation}
\begin{aligned}
    \langle\mathbf{P^m}, \mathbf{P^b}, \mathbf{P^s}\rangle &= \mathsf{Dec}\left(\mathbf{Q}; \mathbf{O}\right) \\
    \mathbf{P^c}&=\mathsf{Sim}(\mathbf{P^s}, \mathbf{T})
    \end{aligned}
    \label{EQ:uniseg_overall}
\end{equation}
where $\mathbf{P^s}$ is the decoded semantics. The visual-semantic matching scores $\mathbf{P^c}$ is derived from $\mathbf{Sim}(\mathbf{P^s}, \mathbf{T})$ by calculating the similarity scores between $\mathbf{P^s}$ and $\mathbf{T}$, which is used to compute the loss during training and predict the category during inference.


% In Sec.~\ref{sec:architecture}, we detail the architecture and training scheme. 

% Given three types of queries as inputs, we decode the masks $\mathbf{P^m}$, boxes $\mathbf{P^b}$, and semantics $\mathbf{P^s}$ correspondingly by:
% \begin{equation}
% \begin{aligned}
%     \langle\mathbf{P^m}, \mathbf{P^b}, \mathbf{P^s}\rangle &= \mathbf{Dec}\left(\mathbf{Q^f, Q^b, Q^d}; \mathbf{O}\right) \\
%     \mathbf{P^c}&=\mathbf{Sim}(\mathbf{P^s}, \mathbf{T})
%     \end{aligned}
%     \label{EQ:uniseg_overall}
% \end{equation}
% where the classification logits  $\mathbf{P^c}$ is derived from $\mathbf{Sim}(\mathbf{P^s}, \mathbf{T})$ by calculating the similarity matrix between $\mathbf{P^s}$ and $\mathbf{T}$, which is then used to compute the loss during training or predict the box/mask category during inference.


% and select the top-ranked text token as the predicted category for each semantic token.

% Based on the above basic model design, our \ourmodel{} can learn from the union of two datasets and promote a common semantic space through a single text encoder. As such, simply pretraining our model on the combination of segmentation and detection datasets already gives us reasonably good open-vocabulary performance across the board. 
% As we discussed earlier, there are still two intrinsic discrepancies between the two tasks. First, semantic or panoptic segmentation cares about not only the foreground instances but also the background stuff, which differs from detection which merely localizes foreground objects. Therefore, completely sharing the queries for both tasks leads to clear conflict and further hurts the performance. Second, it is intuitive that a good box prediction can usually induce a good mask and vice versa. Training the box and mask head with detection and segmentation data separately hinders the synergy of spatial supervision from the two datasets. With these in mind, we propose two techniques to bridge the discrepancies and elaborate below.

% All detection and segmentation data share the same text encoder to learn a shared vocabulary space. 
% In addition, different from many previous works~\cite{li2022grounded, kamath2021mdetr, chen2022unified} that fuse image and text features in the encoder, our model is a dual-tower structure, which fully decouples the text and image encoder.
% This simple design is efficient and tractable for different modalities, which makes our model compatible  with other data formats (i.e, image-text pairs) for future usage.
\subsection{Basic Loss Formulation}
In this basic formula, we attempt to reconcile the two tasks by promoting a shared visual-semantic space without touching other issues. For multiple tasks and datasets, our loss function can be written as follows.
\begin{equation}
\footnotesize
\begin{aligned}
    \mathcal{L}_{all}&=\sum_{{I}, (\mathbf{c}, \mathbf{m})   \in \mathcal{D}_m} 
    \overbrace{\left(\mathcal{L}_{m}(\mathbf{P^m},\mathbf{m})+
    \mathcal{L}_{b}(\mathbf{P^b},\hat{\mathbf{b}})+
    \mathcal{L}_{c}(\mathbf{P^c},\mathbf{c})\right)}^{\substack{\text{Segmentation loss}}}\\
    &+\sum_{{I}, (\mathbf{c}, \mathbf{b}) \in \mathcal{D}_b}
    \underbrace{\left(\mathcal{L}_{b}(\mathbf{P^b},\mathbf{b})+
    \mathcal{L}_{c}(\mathbf{P^c},\mathbf{c})\right)}_{\substack{\text{Detection loss}}}\\
\end{aligned}
\label{Eq:base}
\end{equation}
% where $\langle\mathbf{P^m}, \mathbf{P^b}, \mathbf{P^s}\rangle=\mathbf{F_m}(I,\mathcal{V})$. 
For clarity, we omit the weight for each loss term. Note that for the segmentation task, we can derive accurate boxes $\hat{\mathbf{b}}$ from masks $\mathbf{m}$ and use them to compute the box loss as in term $\mathcal{L}_{b}(\mathbf{P^b},\hat{\mathbf{b}})$.
% $D_p$ and $D_d$ denotes the panoptic segmentation data and detection data and $(x, y)$ is a pair of input image and annotations. $L_{mask}(x,y)$,$L_{cls}(x,y)$ and $L_{box}(x,y)$ are mask, class and box losses, respectively.
By summing over all the terms, our model can achieve a reasonably good open-vocabulary performance. Furthermore, it can be pre-trained end-to-end with detection and segmentation data, allowing it to perform open-vocabulary segmentation and detection using a single set of weights.

Despite building a strong baseline, we must consider the intrinsic discrepancies between the two tasks, as previously discussed. Semantic and panoptic segmentation require the recognition of both foreground and background, while detection focuses solely on localizing foreground objects. As a result, using the same queries for both tasks creates conflicts that can significantly degrade performance. Additionally, good box predictions are typically indicative of good masks, and vice versa. Separately training the box and mask head on detection and segmentation data obstructs the synergy of spatial supervision from both datasets. 
% With these in mind, we propose two techniques to bridge the discrepancies and elaborate below. 

To address the aforementioned discrepancies, we introduce a new decoder design for our \ourmodel{}. We divide the queries $\mathbf{Q}$ into three types: $L_f$ foreground queries $\mathbf{Q_f}$, $L_b$ background queries $\mathbf{Q_b}$ and $L_d$ conditioned queries $\mathbf{Q_d}$, and propose query-specific computations for each type. In the following, we will describe how we decouple the foreground and background decoding to address the task discrepancy in Sec.~\ref{sec:task_discrepancy}, and employ the conditioned mask decoding to tackle the data discrepancy in Sec.~\ref{sec:data_discrepancy}. 
% Third, training and inference differences.
\subsection{Bridge Task Gap: Decoupled Foreground and Background Decoding}\label{sec:task_discrepancy}
% Our model broadly incorporates generic segmentation (instance/semantic/panoptic) and detection tasks. By nature these tasks focus on different types of semantics. For example, instance segmentation and detection focus on instance-level foreground objects, while panoptic segmentation also cares about category-level background stuff. 
% Considering prominent data in our training model is detection data that only contains  foreground instances, a plain way of combining them causes severe interference across tasks. 
\input{iccv2023AuthorKit/resources/imgs/query_interaction}
% Without loss of generality, we define the visual concepts in instance segmentation and detection as the foreground while those stuff categories in panoptic segmentation as the background. To mitigate the task discrepancy, we perform foreground and background decoding with foreground queries $\mathbf{Q^f}$ and background queries $\mathbf{Q^b}$, respectively. Concretely, regarding the two query types, our decoder predicts two sets of outputs: $\langle\mathbf{P}^{m}, \mathbf{P}^{b}, \mathbf{P}^s \rangle^f$ and $\langle\mathbf{P}^{m}, \mathbf{P}^{b}, \mathbf{P}^s \rangle^b$. Then we perform two independent Hungarian Matching to match these two sets of outputs to the foreground and background GTs in the datasets, as shown in Fig.~\ref{fig:interaction_assignment}~(a). As a result, the segmentation data is used to train both foreground and background decoding while detection data only supervises foreground decoding. Therefore, our loss becomes.
Without loss of generality, we have defined the visual concepts that appear in instance segmentation and detection as foreground, while the stuff categories in panoptic segmentation are considered background. To mitigate the task discrepancy, we perform foreground and background decoding with foreground queries $\mathbf{Q_f}$ and background queries $\mathbf{Q_b}$, respectively. Specifically, for these two query types, our decoder predicts two sets of outputs: $\langle\mathbf{P}^{m}_f, \mathbf{P}^{b}_f, \mathbf{P}^c_f \rangle$ and $\langle\mathbf{P}^{m}_b, \mathbf{P}^{b}_b, \mathbf{P}^c_b \rangle$. We also divide the ground truths in segmentation dataset into two groups: $(\mathbf{c}_f, \mathbf{m}_f)$ and $(\mathbf{c}_b, \mathbf{m}_b)$, and then perform two independent Hungarian Matching processes for these two sets correspondingly, as shown in Fig.\ref{fig:interaction_assignment}~(a). Consequently, both foreground and background decoding are used for segmentation, while only foreground decoding is used for detection. As a result, our basic loss function in Eq.~\eqref{Eq:base} is reformulated to:
% \begin{equation}
% \begin{aligned}
%     L_{all}&=\sum_{{I}, (\mathbf{c}, \mathbf{m})   \in \mathcal{D}_m} 
%     L_{m}(\mathbf{P^m},\mathbf{m})+
%     L_{b}(\mathbf{P^b},\mathbf{m})+
%     L_{c}(\mathbf{P^c},\mathbf{c})\\
%     &+\sum_{{I}, (\mathbf{c}, \mathbf{b}) \in \mathcal{D}_b}
%     L_{b}(\mathbf{P^b},\mathbf{b})+
%     L_{c}(\mathbf{P^c},\mathbf{c})\\
%     &=\textcolor{red}{\sum_{{I}, (\mathbf{c}, \mathbf{m})}\sum_{(c_i,m_i) \in \mathbf{fg_I}} L_{m}(\mathbf{P^m_f},\mathbf{m_i})+
%     L_{b}(\mathbf{P^b_f},\mathbf{m_i})+
%     L_{c}(\mathbf{P^c_f},\mathbf{c_i})}\\
%     &+\textcolor{red}{\sum_{{I}, (\mathbf{c}, \mathbf{m})}\sum_{(c_i,m_i) \in \mathbf{bg_I}} L_{m}(\mathbf{P^m_b},\mathbf{m_i})+L_{b}(\mathbf{P^b_b},\mathbf{m_i})+
%     L_{c}(\mathbf{P^c_b},\mathbf{c_i})}\\
%     &+\sum_{{I}, (\mathbf{c}, \mathbf{b})}\sum_{(c_i,b_i) \in \mathbf{fg_I}} L_{b}(\mathbf{P^b_f},\mathbf{b_i})+
%     L_{c}(\mathbf{P^c_f},\mathbf{c_i})\\
% \end{aligned}
% \label{decouple}
% \end{equation}
\begin{equation}
\footnotesize
\begin{aligned}
    % L_{all}&=\sum_{{I}, (\mathbf{c}, \mathbf{m})   \in \mathcal{D}_m} 
    % L_{m}(\mathbf{P^m},\mathbf{m})+
    % L_{b}(\mathbf{P^b},\mathbf{m})+
    % L_{c}(\mathbf{P^c},\mathbf{c})\\
    % &+\sum_{{I}, (\mathbf{c}, \mathbf{b}) \in \mathcal{D}_b}
    % L_{b}(\mathbf{P^b},\mathbf{b})+
    % L_{c}(\mathbf{P^c},\mathbf{c})\\
    \mathcal{L}_{all}&=\sum_{{I}, (\mathbf{c}, \mathbf{m}) \in \mathcal{D}_m} \overbrace{\left(\mathcal{L}_{m}(\mathbf{P^m_f},\mathbf{m}_f)+
    \mathcal{L}_{b}(\mathbf{P^b_f},\hat{\mathbf{b}}_f)+
    \mathcal{L}_{c}(\mathbf{P^c_f},\mathbf{c}_f)\right)}^{\substack{\text{Segmentation loss for foreground}}} \\
    &+\overbrace{\left(\mathcal{L}_{m}(\mathbf{P^m_b},\mathbf{m}_b)+L_{b}(\mathbf{P^b_b},\hat{\mathbf{b}}_b)+
    \mathcal{L}_{c}(\mathbf{P^c_b},\mathbf{c}_b)\right)}^{\substack{\text{Segmentation loss for background}}} \\
    &+\sum_{{I}, (\mathbf{c}, \mathbf{b}) \in \mathcal{D}_b} \underbrace{\left(\mathcal{L}_{b}({\mathbf{P^b_f}},\mathbf{b})+
    \mathcal{L}_{c}({\mathbf{P^c_f}},\mathbf{c})\right)}_{\substack{\text{Detection loss for foreground}}}\\
\end{aligned}
\label{decouple}
\end{equation}
% L_{all}&=\sum_{(x,y) in D_p}\left(  L_{mask}(x,y)+L_{cls}(x,y)+L_{box}(x,y)\right)\\&+\sum_{(x,y) in D_d}\left(  L_{cls}(x,y)+L_{box}(x,y)\right)\\
    % &=\sum_{(x,y_p) in D_p}\textcolor{red}{\sum_{y_i \in C_{fg}}\left(  L_{mask}(x,y_i)+L_{cls}(x,y_i)+L_{box}(x,y_i)\right)}\\&+\textcolor{red}{\sum_{y_j \in C_{bg}}\left(  L_{mask}(x,y_j)+L_{cls}(x,y_j)+L_{box}(x,y_j)\right)}\\
    % &+\sum_{(x,y_d) in D_d}\sum_{y_i \in C_{fg}}\left(  L_{cls}(x,y_i)+L_{box}(x,y_i)\right)
% where $\mathbf{fg_I}$ and $\mathbf{bg_I}$ are the set of foreground and background annotations of the image $I$. For the segmentation task we have $(\mathbf{c},\mathbf{m})=\mathbf{fg_I}\cup \mathbf{bg_I}$ and for the detection task, we have $(\mathbf{c},\mathbf{b})=\mathbf{fg_I}$.  
% We mark the terms corresponding to our proposed decoupled decoding in red font.
% Note that background matching is not used for detection data. 
where $\hat{\mathbf{b}}_f$ and $\hat{\mathbf{b}}_b$ are derived from ${\mathbf{m}}_f$ and ${\mathbf{m}}_b$, respectively. Based on such explicit decoupling, our model maximizes the cooperation of foreground supervision from both detection and segmentation datasets and significantly reduces the interference between foreground and background categories. Though decoupled, we note that these two types of queries share the same decoder and interact with each other with self-attention, as shown in Fig.~\ref{fig:interaction_assignment}~(b). 
% Considering prominent data in our training model is detection data that only contains  foreground instances, a plain way of combining them causes severe interference across tasks. 
Below we explain how the foreground and background queries are determined.
% In addition,  $\mathbf{Q^f}$ and  $\mathbf{Q^b}$ can interact with each other .



% To incorporate detection task in open-vocabulary segmentation, we have to bridge the gap between the two types of datasets. Detection datasets usually contain only foreground categories while segmentation datasets contain both foreground and background categories. To make our model suitable for both tasks, we decouple foreground and background decoding by using different queries. 

\noindent
\textbf{Language-guided foreground query selection}. Open-vocabulary setting differs from the conventional closed-set setting in that a model is required to localize a large number of foreground objects far beyond the training vocabulary. However, the fact is that our decoder contains a limited number of foreground queries (a few hundred typically), making it hardly handle all possible concepts in the image. 
\input{iccv2023AuthorKit/resources/imgs/compare_match}
To address this issue, we propose a method called \textit{language-guided foreground query selection} to adaptively select queries with respect to given text concepts as shown in Fig.~\ref{fig:framework} left part. 
% and use them to initialize the foreground queries for the decoder. 
Given the image features $\mathbf{O}$ and text features $\mathbf{T}$, we employ a lightweight module to predict the box and score for each feature: 
% train each encoder token to predict a box and a mask, and compute a similarity matrix between the encoder tokens $\mathbf{O}$ and text features $\mathbf{T}$.
% \begin{equation}
% \begin{aligned}
%     % \langle\mathbf{E^m}, \mathbf{E^b}\rangle &= \mathbf{Dec}\left(\mathbf{O}\right) \\
%     \mathbf{E^b} &= \mathbf{Dec}\left(\mathbf{O}\right) \\
%     \mathbf{E^c}&=\mathbf{Sim}(\mathbf{O}, \mathbf{T}) \\
%     \mathbf{Ind}&=\mathbf{TopK}\left(\mathbf{E^c}\right) \\
%     \mathbf{Q^f_c}&=\mathbf{Sel}\left(\mathbf{O}; \mathbf{Ind}\right), \mathbf{Q^f_b}&=\mathbf{Sel}\left(\mathbf{E^b}; \mathbf{Ind}\right) \\
%     \end{aligned}
%     \label{EQ:uniseg_overall}
% \end{equation}
\begin{equation}
    % \langle\mathbf{E^m}, \mathbf{E^b}\rangle &= \mathbf{Dec}\left(\mathbf{O}\right) \\
    \mathbf{E}^b = \mathsf{Head}\left(\mathbf{O}\right),    \mathbf{E}^c =\mathsf{Sim}(\mathbf{O}, \mathbf{T}) \\
    \label{EQ:uniseg_overall}
\end{equation}
where $\mathsf{Head}$ is the box head. Then we select $L_f$ top-ranked entries from $\mathbf{E}^b$ and $\mathbf{O}$ according to the scores in $\mathbf{E}^c$.
% where $\mathbf{Topk}$ is a function to get the top-ranked $k$ indices from a matrix, $\mathbf{Ind}$ is the selected index, and 
% $\mathbf{Sel(M; Ind)}$ is a function to get the values of a matrix $\mathbf{M}$ concerning the index $\mathbf{Ind}$, and $\mathbf{Q^f_c}$ and $\mathbf{Q^f_b}$ 
These selected $L_f$ image features and boxes are then fed to the decoder as the foreground queries (blue squares in Fig.~\ref{fig:framework}). By selecting only the text-related tokens as decoder queries, we mitigate the problem of decoding irrelevant semantics and provide better query initialization. Such an adaptive way of proposing foreground queries enables our model to effectively transfer to novel vocabulary during test scenarios.
% It is infeasible for the model to enumerate all categories to find the visual concepts in each image. How to bridge the gap between infinite semantic space and limited model capability becomes an important problem. 
% X-Decoder does not work on this. It simply set the object queries as learnable queries, leading to its relatively low performance on datasets that have large domain gaps with their training data such as SeginW. GLIP alleviates this problem by feeding the text as input and letting the model do an early fusion of the image and text. This technic counters the purpose of open-vocabulary detection because it assumes that the user knows the concept they want, which is similar to a closed-set problem. Our solution is a language-guided query selection. 

\noindent
% \textbf{Learnable background query}. We use learnable query embeddings for our background queries for two reasons. Firstly, query selection does not work well because the selected reference points often extend outside of large and non-convex background regions. Secondly, background stuff has a much smaller number of categories, such as 'sky' and 'grass', and learnable queries are sufficient to handle them. By using learnable query embeddings, our model can effectively handle a wide range of background stuff categories without being limited by the predefined set of queries.
\textbf{Learnable background queries}. Different from foreground queries, we use learnable query embeddings for our background queries for two reasons. Firstly, query selection does not work well because the selected reference points often extend beyond large and non-convex background regions, leading to suboptimal results. Secondly, background stuff has a relatively smaller number of categories than the foreground, and a single image typically contains a few different stuffs (\textit{e.g.}, ``sky'', ``building''). As a result, using learnable queries for our model can sufficiently and effectively handle background stuff categories and generalize well to open-vocabulary settings. The background queries are marked by green squares in Fig.~\ref{fig:framework}.
% , where there may be a wide variety of background categories.
\input{iccv2023AuthorKit/resources/imgs/cond_seg}
\\
\textbf{Comparison with previous works}. 
% One may notice that the proposed decoupled decoding resemble 
In Fig.~\ref{fig:match compare}, we show a comparison between our approach and others on handling foreground and background. 
% Mask2Former~\cite{cheng2022masked} and MaskDINO~\cite{li2022mask} treat foreground and background equally when conducting panoptic segmentation, which leads to suboptimal mask AP for foreground things compared with the mask AP when they do segmentation on foreground classes only (instance segmentation). Panoptic Segformer~\cite{li2022panoptic} separates foreground and background queries, but their background queries have fixed semantics, where each query corresponds to a pre-defined background category, which cannot handle open-vocabulary categories. In contrast, our foreground queries are proposed through a language-guided selection mechanism, and our background queries are fully learnable, which gets rid of the restriction of a pre-defined vocabulary. Fig.\ref{fig:match compare}, we present a comparison between our approach and others for handling foreground and background. 
Mask2Former\cite{cheng2022masked} and MaskDINO~\cite{li2022mask} treat foreground and background equally when conducting panoptic segmentation, resulting in suboptimal mask average precision (AP) for foreground objects compared to the same model solely trained on foreground classes (instance segmentation). Panoptic Segformer~\cite{li2022panoptic} separates foreground and background queries, but their background queries have fixed semantics, with each query corresponding to a pre-defined background category, limiting their ability to handle open-vocabulary categories. In contrast, our approach proposes foreground queries through a language-guided selection mechanism, and our background queries are fully learnable, eliminating the restrictions of a predefined vocabulary.
% They are matched with GT instances through bipartite matching during training. 
\subsection{Bridge Data Gap: Conditioned Mask Decoding}
\label{sec:data_discrepancy}
Our ultimate goal is to bridge the data gap by using a single loss function to train multiple tasks, resulting in the following loss function:
\begin{equation}
\footnotesize
\begin{aligned}
    \mathcal{L}_{all}&=\sum_{{I}, (\mathbf{c}, \mathbf{m}, \mathbf{b})   \in \mathcal{D}} 
    \mathcal{L}_{m}(\mathbf{P^m},\mathbf{m})+
    \mathcal{L}_{b}(\mathbf{P^b},\mathbf{b})+
    \mathcal{L}_{c}(\mathbf{P^c},\mathbf{c})
\end{aligned}
\label{Eq:unified}
\end{equation}
Here, $\mathcal{D}$ represents the union of segmentation and detection datasets. However, the loss function requires mask annotations for detection data and box annotations for segmentation data, leading to a discrepancy in the granularity of spatial supervision between the two tasks. % From the viewpoint of data format, box and instance-level mask are twin annotations with different granularity. 
% Namely, a mask is a fine-grained annotation that tightly covers an object, 
% with many points (form a polygon), 
% while a box is a coarse-grained annotation encircling an object with only four coordinates. 
As we discussed earlier, we can easily convert an object mask $m$ to a box $\hat{b}$, which augments the original segmentation data $\mathcal{D}_m=\{I_i, (\mathbf{c}_i, \mathbf{m}_i)\}_{i=1}^{M}$ into $\hat{\mathcal{D}}_m=\{I_i, (\mathbf{c}_i, \mathbf{m}_i, \hat{\mathbf{b}}_i)\}_{i=1}^{M}$. For detection data $\mathcal{D}_b$, however, we are only given coarse location (box) and category. Then an interesting question comes -- \textit{can we obtain its mask given these priors?} 
% is transferring the region-level and pixel-level supervision across two tasks. is transferring region-level semantics from detection to pixel-level segmentation. 
% \\
% \textbf{Mapping from GT Box\&Label to Mask. }From the viewpoint of data format, box and instance-level mask are twin annotations with different granularity. Namely, a mask is a fine-grained annotation that tightly covers an object, 
% with many points (form a polygon), 
% while a box is a coarse-grained annotation encircling an object with only four coordinates. Therefore, given a mask as the fine-grained annotation, we can easily convert it to a box. For each object in detection data, we are given its coarse location (box) and category. The interesting question is, \textit{can we decode its mask given these priors?} 
% Different from weakly-supervised segmentation models that only use box annotation to generate mask, 
\input{iccv2023AuthorKit/resources/tabels/box2mask}
To address this problem, we resort to the segmentation data which contains rich mappings from label\&box to mask, \textit{i.e.}, $(c, b) \rightarrow m$ and propose conditioned mask decoding to learn the mappings as shown in Fig.~\ref{fig:framework} right-most part. Given the ground-truth concepts and boxes, $(\mathbf{c}, \mathbf{b})$, we employ the decoder to decode the mask:
\begin{equation}
    \mathbf{P^m} = \mathsf{Dec}\left((\mathbf{t}, \mathbf{b}); \mathbf{O}\right)
    \label{Eq:mask_decoding}
\end{equation}
where $t$ is the text features extracted for the concepts. Based on Eq.~\eqref{Eq:mask_decoding}, the question becomes, \textit{can we learn from segmentation data a good mapping which generalizes well to detection data with different categories?}
\\
\textbf{Mapping Hypothesis Verification}. To answer the question, we conduct a pilot study. 
% Following our current design,
We train a model which learns to decode masks conditioned on GT concepts and boxes on COCO~\cite{chen2015microsoftcoco}, 
% where the GT box serves as box query and GT label (language) encoded by the language encoder serves as the content query. 
and then evaluate the conditioned decoding performance on ADE20K~\cite{zhou2017scene}. The results are shown in Table~\ref{tab:box2mask}. Comparing the top two rows, we can find mask decoding conditioned on the GT concept and box significantly improves the quality (mask AP from 8.6 to 46.4), which even reaches a similar level to COCO (46.4 \textit{v.s.} 53.2). These results indicate that our learned mask decoding generalizes well to a new dataset with novel categories. To further verify, we visualize decoded masks in Fig.~\ref{fig:decoded_mask}. 

\underline{\textit{Interactive Segmentation}}. 
% The conditioned training  empowers \ourmodel{} with the capability of conditioned inference. 
The above study implies a new interface of image segmentation. Apart from segmenting an image from scratch, users can give a hint about the object location by drawing a box (click four points), and our \ourmodel{} can generate its mask with fairly high quality. This capacity can potentially help accelerate the annotation of segmentation data, especially for those with boxes. 
% for other detection datasets by taking the GT box as the condition, 
We leave a comprehensive study on this as future work.
% \Feng{TODO: maybe show some visualization here.}
\\
\textbf{Conditioned Mask Decoding Training. }
% \Feng{new capability: Conditioned Interface}
Based on the verified hypothesis, we add all the GT boxes and labels as the conditioned queries to simultaneously learn foreground/background decoding and conditioned mask decoding, as shown in Fig.~\ref{fig:framework}. 
% \Feng{Do we need to compare with Mask DINO denoising training?} \Jianwei{We may discuss this in the end of this section, similar to Sec.3.1} 
It unifies all our tasks and enables \ourmodel{} to learn more generalized conditioned decoding in the joint semantic space. Based on this, we can literally derive the pseudo masks $\hat{\mathbf{m}}$ for object detection data and obtain augmented $\hat{\mathcal{D}}_b=\{I_i, (\mathbf{c}_i, \hat{\mathbf{m}_i}, {\mathbf{b}}_i)\}_{i=1}^{N}$. 
% Thus far, we managed to fill the data gap between the two tasks. Eq.~\eqref{Eq:unified} can be used to train our model from both data seamlessly. 
Below we elaborate on how these pseudo masks are used for training.
% In what follows, we will explain two different ways of leveraging this augmented detection dataset.
% More specifically, for segmentation data, the model will have mask supervision to learn the conditioned decoding task. For detection data, the model can transfer the learned conditioned decoding and directly generate a mask for each conditioned object.
% \Feng{why match: 1. online: mask quality; one-stage; -> detach noise, use it for label assignment; 2. offline: seen and unseen conditioned mask AP gap in ADE20K; bad ADE conditioned performance in the early stage}
% \textbf{Conditioned Interface for High-Quality Segmentation. }The conditioned training  empowers \ourmodel{} with the capability of conditioned inference. Apart from directly segmenting an image, the user can also give a coarse location of the object with a box (click four points), and \ourmodel{} can generate its mask with high quality. This design can potentially help annotate segmentation for other detection datasets by taking the GT box as the condition, which we leave as a future study.

\noindent\textbf{Conditioned Mask Generation to Guide Detection Data. } The trained conditioned mask decoding component can also be used to assist detection data as segmentation guidance. We propose two methods to utilize the generated mask to guide our model training, \textit{Online Mask Assistance} and \textit{Offline Mask Assistance}. For \textit{Online Assistance}, we only train one model and generate the masks on the fly. Instead of directly using the generated masks as mask supervision, we use the masks to assist in matching predictions and GT instances because the mask quality is not strong enough for supervision. especially in the early stage (shown in Tab.~\ref{tab:box2mask} third row). As for \textit{Offline Assistance}, we train our model with conditioned mask decoding until convergence and generate mask annotations for detection data. The annotated dataset can be used to train a segmentation model. Considering detection data only has instance-level annotations, the generated masks are expected to improve instance segmentation in both cases. More details about these two methods are discussed in the Appendix.
% \input{iccv2023AuthorKit/resources/imgs/cond_matching}
% \textbf{Online Conditioned Mask as the Guidance. }
% % to Assist Detection Training
% %put online and offline together to strength
% % \Feng{direct match box with gt in query selection; evaluate on other open-seg datasets with swinL/online teacher} 
% % \Feng{reason only use it for matching: 1. label assignment: lift the classification score of real tgt and lower the false positives with bad mask performance; 2. assumption: matching consistency.}
% % According to Table~\ref{tab:box2mask}, our conditioned masks have much higher mask AP than the masks predicted without condition. 
% \noindent 
% % Our model is trained with both detection and segmentation, where GT masks are missing in the detection task in matching and training loss. 
% Given that our model can generate reasonably good masks with GT boxes as the condition, we seek to 
% % generate masks for the detection data and
% use them to better align detection with segmentation. A straightforward way is directly using masks for supervision on the fly. This requires high mask quality during the whole training phase, which however is not true, especially in the early training stage. 
% % while our mask quality is inferior to GT (shown in Table~\ref{tab:box2mask}) \Jianwei{do we have GT numbers in Table 1?}, especially in the early training stage. 
% Therefore, we alternatively use the generated mask as the additional guidance to find the matched foreground queries with the GT concept and box in detection data. % to align the label assignment for detection and segmentation. 
% As shown in Fig.~\ref{fig:cond matching}~(a), detection during training fully ignores the predicted mask quality when finding the matched foreground queries, which is different from segmentation in Fig.~\ref{fig:cond matching}~(b). This ignorance may lead to a biased matching toward box quality for our model which needs to produce high-quality boxes and masks simultaneously. Therefore, we use the generated masks from GT boxes and labels as the guidance for better label assignment, as shown in Fig.~\ref{fig:cond matching}~(c). 
% % do use mask to assign predicts to targets, which disregards mask quality. 
% % Therefore, in our joint training model, when a detection image has two predictions with similar box quality but different mask predictions, it will assign the target to the one with higher box quality.  If the assigned one has low mask quality, this label assignment will penalize the high-quality mask prediction by lowering its classification score. \Feng{Described not so clear here.} 
% % With the conditioned masks from GT boxes and labels, we use the generated masks for better label assignment, as shown in Fig.~\ref{fig:cond matching}~(c). 
% \\
% \textbf{Offline Conditioned Mask as the Supervision. } Another way to better use the generated mask is to train two models. For example, we can train a large conditioned mask decoding model first and then use it to generate pseudo masks for all the detection data. Afterward, we can train the second model on the annotated detection dataset with mask supervision. However, though our model can generate fairly good masks for novel categories, these masks still has inferior quality compared to the trained categories, as shown in Fig.~\ref{fig:decoded_mask} \Feng{To add conditioned mask on novel categories}. Considering that XX categories in Objects365~\cite{shao2019objects365} have similar semantics as the COCO categories, we treat the generated annotations on these categories as \textit{golden annotations}. Other annotations are \textit{coarse annotations}. We adopt different strategies for the two types of annotations, where \textit{golden annotations} are used for mask supervision while \textit{coarse annotations} is only used for matching (similar to our online guidance). \Feng{Complete this paragraph.}
\\
\noindent
\textbf{Comparison with Denoising Training}. Compared with models~\cite{li2022dn, zhang2022dino, li2022mask} using denoising training (DN), \textit{conditioned mask decoding} differs in two aspects. First, their design choices are different. DN adds noise to the GT boxes and labels for reconstruction, but our model learns to generate masks conditioned on the GT priors. Second, their design purposes are different. DN is designed to accelerate training convergence~(understanding), while our method aims to generate masks for detection data~(generation).
% Considering that the mask AP on seen
% However, the generated masks are not good enough for supervision, so we  Our method is to train the conditioned mask decoding together with the foreground and background decoding.
% \Feng{1. how to say Bridge}

% \Feng{2. cityscapes finetune}

% \Feng{3. coco finetune}

% \Feng{3. SwinL result}


% \Feng{Move to implememtation details.training details}
% \subsection{Architecture}
% \label{sec:architecture}
% We build on Mask DINO~\cite{li2022mask} to implement our model. Mask DINO is a unified detection and segmentation framework that simultaneously predicts box and mask. Mask DINO consists of a backbone, a multi-layer Transformer encoder, and a multi-layer Transformer decoder. The encoder process the image features from the backbone and produces multi-scale features, which the decoder will query to decode mask and box.  We follow Mask DINO to use deformable attention~\cite{zhu2020deformable} in both the Transformer encoder and decoder. \\
% \textbf{End-to-end training. }Our model can be end-to-end pre-trained with both detection and segmentation data. After the joint pre-training, \ourmodel{} can directly produce zero-shot detection and segmentation on open-vocabulary datasets with one suit of weights. Third, training and inference difference.


% \subsection{Dataset}
% \Feng{move this section to 3.2}

% \Feng{query type/matching guidance with pesudo code}
