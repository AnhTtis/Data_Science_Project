\section{Conclusion}
We have presented \ourmodel{}, a simple {open}-vocabulary {se}gm{e}ntation and {d}etection framework, which jointly learns from different segmentation and detection datasets with a single model. To bridge the task gap between foreground objects and background stuff, we propose a decoupled decoding method with language-guided foreground query selection. We also jointly train a conditioned mask decoding task, which provides an interactive segmentation interface during inference and helps bridge data gap for detection data during training. The result indicates our unified model significantly improves open-segmentation while keeping a reasonable detection performance. The jointly pre-trained model can also be seamlessly transferred to improve close-vocabulary performance. 
% We hope \ourmodel{} can be recevied as a strong baseline for open-vocabulary models.
\\
\textbf{Limitations}. In this work, we aim at exploring the potential of training an open-vocabulary model for both segmentation and detection. \ourmodel{} does not utilize either referring/grounding data or large-scale image-text pairs to further enrich our training data and semantic coverage. We leave a grander joint training to future work. 