\section{Results}
\subsection{Within-Dataset Testing}
Table \ref{tab:within_dataset} shows the results for models trained and tested on subject-disjoint partitions from the same datasets. For PURE and UBFC we achieve MAE lower than 1 bpm, performing better or on par with all traditional and supervised learning approaches. For PURE, our approach gives the lowest MAE and a Pearson $r$ of nearly 1. Performance drops on DDPM due to the overall difficulty of the dataset. \ourapproach outperforms contrastive approaches, only being surpassed by supervised deep learning models.

In comparison to other unsupervised methods, Contrast-Phys~\cite{Sun_2022_ECCV} gives the most competitive performance on all but DDPM. Note that our approach gives the lowest MAE on all datasets, but has higher RMSE. We believe this is due their use of harmonic removal as a post-processing step when estimating the pulse rate, which is not described in \cite{Sun_2022_ECCV}, but can be found in their publicly available code. 

\subsection{Cross-Dataset Testing}
\input{tables/cross-dataset}
We perform cross-dataset testing to analyze whether the approach is robust to changes to the lighting, camera sensor, pulse rate distribution, and motion. Table \ref{tab:cross_dataset} shows the results for \ourapproach and supervised training with the same architecture. We find that the performance is similar for the supervised and unsupervised approaches when transferring to different data sources. Training on PURE exclusively gives relatively poor results when transferring to UBFC-rPPG and DDPM, due to the low pulse rate variability within PURE samples and lack of movement. Training on DDPM gives the best results overall, since the dataset is the largest and captures larger subjects' movements compared to other datasets.

\subsection{Training with CelebV-HQ Videos}\label{sec:celebv_training}
Given the abundance of face videos publicly available online, we trained a model on the CelebV-HQ dataset~\cite{zhu2022celebvhq}. After processing the available videos with MediaPipe and resampling to 30 fps, our unlabeled dataset consisted of 34,029 videos. We trained the model for 23 epochs and manually stopped training due to a plateau in the validation loss.
Unfortunately, we found that the model could not converge to the true blood volume pulse. We attribute the failure to poor video quality from compression. Although the videos were downloaded with the highest available quality, they have likely been compressed multiple times, removing the pulse signal entirely. See Appendix \ref{sec:app_celebv} for a quantitative assessment of rPPG quality using a baseline approach.
%The MAE in bpm for UBFC-rPPG, PURE, and DDPM was 19.22, 24.83, and 27.41, respectively.

\subsection{Training with HKBU-MARs Videos}
The HKBU-MARs dataset~\cite{Liu_2016_CVPRW} was designed for face presentation attack detection, but we trained models on the ``real'' video sessions in the dataset. The bottom rows in Table \ref{tab:cross_dataset} show the results for training on HKBU-MARs, then testing on the benchmark rPPG datasets. Training on HKBU-MARs gives better results when testing on UBFC-rPPG and PURE than all training sets except DDPM, which is an order of magnitude larger. {\bf To our knowledge, this is the first succesful experiment showing that non-rPPG videos can be used to train robust rPPG models, even if they do not have ground-truth pulse labels}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/waveforms.png}
    \caption{Within-dataset waveform predictions on all baseline datasets from end-to-end unsupervised models over an 8-second window. The model predictions are remarkably periodic without any form of filtering. Note that phase is not considered during training, so each model learns its own phase shift.}
    \label{fig:preds}
\end{figure}

\subsection{Ablation Study on Losses}
\input{tables/loss_ablation}
We trained models using all combinations of loss components to analyze their contributions. Table \ref{tab:loss_ablation} shows the results for training and testing on UBFC-rPPG. The bandwidth loss is the most critical for discovering the true blood volume pulse, while the sparsity and variance losses do not learn the desired signal by themselves. Surprisingly, combining the bandwidth loss with just one of the sparsity or variance losses gives worse performance than just the bandwidth loss. However, when combining all three components, the model achieves impressive results.