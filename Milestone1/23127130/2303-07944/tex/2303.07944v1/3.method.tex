\section{Method}
We first formulate the general setup for signal regression from video. A video sample $x_i \in \mathbb{R}^{T\times W\times H\times C}$ sampled from a dataset $\mathcal{D}$ consists of $T$ images of size $W\times H$ pixels across $C$ channels, captured uniformly over time.
State-of-the-art methods offer models $f$ that regress a waveform $\mathbb{R}^T \ni y_i = f(x_i)$ of the same length as the video.
Recently, the task has been effectively modeled end-to-end with the models $f$ being spatiotemporal neural networks~\cite{Yu2019,Lee_ECCV_2020,Speth_CVIU_2021,Lu_2021,Yu_2022_CVPR}. While most previous works are supervised and minimize the loss to a contact pulse measurement, we perform non-contrastive learning using only the model's estimated waveform.

The key realization is that we can place strong priors on the estimated pulse regarding its bandwidth and periodicity.
Observed signals outside the desired frequency range are pollutants, so penalizing the model for carrying them through the forward pass results in invariances to such noisy visual features.
We find that it is surprisingly easy to impose the desired constraints in the frequency domain.
Thus, all waveforms are transformed into their discrete Fourier components with the FFT before computing all losses in our approach, $F = \mathrm{FFT}(y)$. The following sections describe the loss functions and augmentations used during training.

\input{3.1.losses}
\input{3.2.augmentations}
