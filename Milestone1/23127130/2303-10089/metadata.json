{
    "arxiv_id": "2303.10089",
    "paper_title": "LP-SLAM: Language-Perceptive RGB-D SLAM system based on Large Language Model",
    "authors": [
        "Weiyi Zhang",
        "Yushi Guo",
        "Liting Niu",
        "Peijun Li",
        "Chun Zhang",
        "Zeyu Wan",
        "Jiaxiang Yan",
        "Fasih Ud Din Farrukh",
        "Debing Zhang"
    ],
    "submission_date": "2023-03-17",
    "revised_dates": [
        "2023-03-20"
    ],
    "latest_version": 1,
    "categories": [
        "cs.RO"
    ],
    "abstract": "Simultaneous localization and mapping (SLAM) is a critical technology that enables autonomous robots to be aware of their surrounding environment. With the development of deep learning, SLAM systems can achieve a higher level of perception of the environment, including the semantic and text levels. However, current works are limited in their ability to achieve a natural-language level of perception of the world. To address this limitation, we propose LP-SLAM, the first language-perceptive SLAM system that leverages large language models (LLMs). LP-SLAM has two major features: (a) it can detect text in the scene and determine whether it represents a landmark to be stored during the tracking and mapping phase, and (b) it can understand natural language input from humans and provide guidance based on the generated map. We illustrated three usages of the LLM in the system including text cluster, landmark judgment, and natural language navigation. Our proposed system represents an advancement in the field of LLMs based SLAM and opens up new possibilities for autonomous robots to interact with their environment in a more natural and intuitive way.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.10089v1"
    ],
    "publication_venue": "12 pages, 16 figures"
}