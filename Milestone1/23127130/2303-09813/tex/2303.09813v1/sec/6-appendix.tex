\onecolumn
\section{Appendix}

In this supplementary material, we start by giving details about evaluation metrics in \secref{Evaluation Metrics}, and datesets in \secref{Datesets Details}.
In \secref{Visualization}, some qualitative visualizations about four benchmarks and our synthetic dataset are displayed.



\subsection{Evaluation Metrics}
\label{Evaluation Metrics}
\subsubsection{Saliency Segmentation Metrics}
Here we define three metrics used for evaluating saliency segmentation performance:
\begin{itemize}
    \item \textbf{Accuracy (Acc)} measures pixel-wise accuracy using ground-truth masks $\mathcal{G}\in\{0,1\}^{H \times W}$ and binary predictions $\mathcal{M}\in\{0,1\}^{H \times W}$.
    \begin{equation}
        \text{Acc}=\frac{1}{HW}\sum_{i=1}^H\sum_{j=1}^W\mathbb{I}({\mathcal{G}_{ij}=\mathcal{M}_{ij}}),
    \end{equation}
    where $\mathbb{I(\cdot)}$ is the indicator function.
    \item \textbf{Intersection-over-union (IoU)} is the overlapped size divided by the total size of the foreground regions from $\mathcal{G}$ and $\mathcal{M}$.
    \begin{equation}
        \text{IoU}=\frac{|\mathcal{M} \cap \mathcal{G}|}{|\mathcal{M} \cup \mathcal{G}|}.
    \end{equation}
    \item \textbf{maximal-$F_\beta$ (max$F_\beta$)} is the maximum score of $F_\beta$ among masks binarised using different thresholds. Given binarized mask $\mathcal{M}$ and ground-truth $\mathcal{G}$, $F_\beta$ is defined as:
    \begin{equation}
        F_\beta = \frac{(1+\beta^2)\text{Precision}\times\text{Recall}}{\beta^2\text{Precision}+\text{Recall}},
    \end{equation}
    where $\text{Precision}=\frac{tp}{tp+fp}$ and $\text{Recall}=\frac{tp}{tp+fn}$. $tp,fp,fn$ represent true-positive, false-positive and false-negative respectively. $\beta^2$ denotes weight. We set $\beta^2=0.3$ in our experiments following~\cite{voynov2021object, melas2021finding, tokencut, selfmask}.
\end{itemize}

\subsubsection{Single Object Localization Metrics}
We report performance using \textit{CorLoc} metric following~\cite{tokencut}. CorLoc considers a predicted bounding box to be correct if the intersection over union (IoU) score between this box and one of the ground-truth bounding boxes is greater than $0.5$.

\subsubsection{Geometry Metrics}
In \tabref{tab:shape} we use three metrics to measure the dataset's geometry statistics. Here we provide the implementation details of the three metrics: shape complexity (SC), polygon length (PL) and shape diversity (SD).

Following ~\cite{li2022bigdatasetgan}, we use OpenCV's \texttt{findContours} function with \texttt{RETR\_EXTERNAL} and \texttt{CHAIN\_APPROX\_SIMPLE} flag to extract a simplified polygon for each mask. Then we normalize the polygon by $p_i=(p_i-p_{min})/(p_{max}-p_{min})$. This operation normalizes the polygon to a unit square in both horizontal and vertical directions. $p_{min}$ and $p_{max}$ are the minimum and maximum coordinates among the set of points. We further apply Douglas-Peucker algorithm~\cite{douglas1973algorithms} with a threshold of $0.01$ to simplify the polygon. After that, we define:
\begin{itemize}
    \item \textbf{Shape Complexity (SC)} is the number of points in the normalized and simplified polygon.
    \item \textbf{Polygon Length (PL)} is defined as the total length of the polygon.
    \item \textbf{Shape Diversity (SD)} is the average mean of pair-wise Chamfer distance~\cite{fan2017point} over all dataset. Chamfer distance is:
\begin{equation}
    d_{\text{CD}}(S_1,S_2)=\sum_{p\in S_1}\min_{q\in S_2}\|p-q\|_2^2+\sum_{q\in S_2}\min_{p\in S_1}\|p-q\|_2^2,
\end{equation}
where $S_1$ and $S_2$ are two sets of points corresponding to different polygons.
\end{itemize} 

\subsection{Datesets Details}
\label{Datesets Details}
Here we present details of all benchmarks used in our experiments:
\begin{itemize}
    \item \textbf{ECSSD} (Extended Complex Scene Saliency Dataset)~\cite{ecssd} consists of 1,000 real-world images of complex scenes. 
    \item \textbf{DUT-OMRON}~\cite{dut_omron} contains 5,168 high quality images with very challenging scenarios. 
    \item \textbf{DUTS}~\cite{duts} contains 10,553 training images (DUTS-TR) collected from the ImageNet~\cite{imagenet} DET training/val sets, and 5,019 test images (DUTS-TE) collected from the ImageNet DET test set and the SUN~\cite{sun} dataset. Following previous works~\cite{shen2022learning, tokencut, selfmask}, the performance is reported only on DUTS-TE.
    \item \textbf{CUB} (Caltech-UCSD Birds-200-2011)~\cite{cub} contains 11,788 images and segmentation masks of 200 subcategories belonging to birds. We follow~\cite{chen2019unsupervised, voynov2021object, melas2021finding}  but only use the 1,000 images for the test subset from splits provided by~\cite{chen2019unsupervised}.
    \item \textbf{VOC07}~\cite{pascal-voc-2007} and \textbf{VOC12}~\cite{pascal-voc-2012} correspond to the training and validation set of PASCAL VOC07 and PASCAL VOC12. VOC07 and VOC12 contains 5,011 and 11,540 images respectively which belong to 20 categories.
    \item \textbf{COCO20K} contains 19,817 randomly chosen images from the COCO2014 dataset~\cite{Lin2014cocodataset}. It is used as a benchmark in~\cite{Vo20rOSD} for a large scale evaluation.
\end{itemize}





\clearpage
\subsection{Visualization}
\label{Visualization}
In this section, 
we first present qualitative visualizations of CUB~\cite{cub} on AttentionCut (\figref{fig:cub}), then visualizations of ECSSD~\cite{ecssd} (\figref{fig:ecssd}), DUTS-TE~\cite{duts} (\figref{fig:duts-te}), DUT-OMRON~\cite{dut_omron} (\figref{fig:duts-omron}) on both AttentionCut and \methodName, respectively. 
\figref{fig:synthetic} shows our synthetic dataset. 
Note that \ding{172}, \ding{173}, \ding{174}, \ding{177} are the same meaning as in \tabref{tab:attentioncut}. 
\ding{172}: only $\mathcal{A}_c$;
\ding{173}: only $r(p)$;
\ding{174}: $\mathcal{A}_c$ with $r(p)$, \ie, $\phi(p)$;
\ding{177}: $\phi(p)$ with $\psi(p,q)$, \ie, AttentionCut.



\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.33\textwidth]{sec/image/supp_vis7.png}
    \includegraphics[width=0.33\textwidth]{sec/image/supp_vis8.png}
    \includegraphics[width=0.33\textwidth]{sec/image/supp_vis9.png}
    \caption{\textbf{Qualitative Results of AttentionCut on CUB.} 
    The columns from left to right are input image, ground-truth mask and \ding{172}, \ding{173}, \ding{174}, \ding{177} are the same as defined in \tabref{tab:attentioncut} (\ding{172}: only $\mathcal{A}_c$; \ding{173}: only $r(p)$; \ding{174}: $\mathcal{A}_c$ with $r(p)$, \ie, $\phi(p)$; \ding{177}: $\phi(p)$ with $\psi(p,q)$, \ie, AttentionCut).
    }
    \label{fig:cub}
\end{figure*}


\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.48\textwidth]{sec/image/supp_vis5.png}
    \includegraphics[width=0.48\textwidth]{sec/image/supp_vis6.png}
    \caption{\textbf{Qualitative Results on ECSSD.} 
    First three columns are input image, ground-truth mask and reconstructed image by diffusion inversion.  
    \ding{172}, \ding{173}, \ding{174}, \ding{177} are the same as defined in \tabref{tab:attentioncut} (\ding{172}: only $\mathcal{A}_c$; \ding{173}: only $r(p)$; \ding{174}: $\mathcal{A}_c$ with $r(p)$, \ie, $\phi(p)$; \ding{177}: $\phi(p)$ with $\psi(p,q)$, \ie, AttentionCut). The last column is the prediction of \methodName.
    } 
    \label{fig:ecssd}
\end{figure*}


\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.48\textwidth]{sec/image/supp_vis1.png}
    \includegraphics[width=0.48\textwidth]{sec/image/supp_vis2.png}
    \caption{\textbf{Qualitative Results on DUTS-TE.} 
    First three columns are input image, ground-truth mask and reconstructed image by diffusion inversion.  
    \ding{172}, \ding{173}, \ding{174}, \ding{177} are the same as defined in \tabref{tab:attentioncut} (\ding{172}: only $\mathcal{A}_c$; \ding{173}: only $r(p)$; \ding{174}: $\mathcal{A}_c$ with $r(p)$, \ie, $\phi(p)$; \ding{177}: $\phi(p)$ with $\psi(p,q)$, \ie, AttentionCut). The last column is the prediction of \methodName.
    } 
    \label{fig:duts-te}
\end{figure*}



\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.48\textwidth]{sec/image/supp_vis3.png}
    \includegraphics[width=0.48\textwidth]{sec/image/supp_vis4.png}
    \caption{\textbf{Qualitative Results on DUT-OMRON.} 
    First three columns are input image, ground-truth mask and reconstructed image by diffusion inversion.  
    \ding{172}, \ding{173}, \ding{174}, \ding{177} are the same as defined in \tabref{tab:attentioncut} (\ding{172}: only $\mathcal{A}_c$; \ding{173}: only $r(p)$; \ding{174}: $\mathcal{A}_c$ with $r(p)$, \ie, $\phi(p)$; \ding{177}: $\phi(p)$ with $\psi(p,q)$, \ie, AttentionCut). The last column is the prediction of \methodName.
    } 
    \label{fig:duts-omron}
\end{figure*}





\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.98\textwidth]{sec/image/supp_grid_gen.png}
    \caption{\textbf{Synthetic Image-mask Pairs.} With \textit{zero} human annotated required, \methodName can generate ``infinite'' realistic and diverse images together with impressive masks. Random samples are shown here.} 
    \label{fig:synthetic}
\end{figure*}


\twocolumn

