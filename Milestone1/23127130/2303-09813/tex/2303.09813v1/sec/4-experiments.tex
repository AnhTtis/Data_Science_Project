\section{Experiments}
\subsection{Experimental Setup}
{\noindent \bf Datasets \& Evaluations.}
For unsupervised saliency segmentation, we evaluate on three standard benchmarks: ECSSD~\cite{ecssd}, DUTS~\cite{duts} and DUT-OMRON~\cite{dut_omron}. We also use CUB~\cite{cub} to compare with some generative-based segmentation models~\cite{chen2019unsupervised, voynov2021object, melas2021finding}.
For metrics, we report pixel-wise accuracy (Acc), intersection-over-union (IoU), and max$F_\beta$ for $\beta^2$ to $0.3$ following conventions~\cite{voynov2021object, melas2021finding, tokencut, selfmask}. 


For unsupervised single object localization, we evaluate on VOC07~\cite{pascal-voc-2007}, VOC12~\cite{pascal-voc-2012} and COCO20K~\cite{Lin2014cocodataset,Vo20rOSD}. We evaluate using correct localization (CorLoc)~\cite{tokencut}, \ie, the percentage of images, where the IoU $> 0.5$ of a predicted single bounding box with at least one of the ground truth. 


\input{sec/table/data_statistics}

\vspace{0.2cm}
{\noindent \bf Implementation Details.}
We adopt the publicly released \texttt{sd-v1-4.ckpt} of Stable Diffusion\footnote{https://huggingface.co/CompVis/stable-diffusion-v-1-4-original} for image generation, and it remains frozen throughout. 
We set image resolution $512\times 512$, timesteps $T=40$, channel num $C=4$, sample frequency $f=8$ and ddim eta is $0.0$. 
For AttentionCut, we set $\lambda_{\phi}=0.16$, $\lambda_{\psi}=2.5$ and $\lambda=0.1$.
Our synthetic dataset contains about $50,000$ image-mask pairs.
We use a three-layer FCN as segment decoder, and set $\text{lr}=0.001$ on Adam~\cite{kingma2014adam} for optimization. The batch size is set to $10$.





\subsection{Comparison with the State-of-the-art}
{\noindent \bf Unsupervised Saliency Segmentation.} 
\tabref{tab:main_seg} compares for unsupervised object segmentation.
\methodName
reached a new SOTA, and largely improves AttentionCut by 12.5\%, 9.6\%, 12.1\% in IoU after training on our synthetic dataset, which proves the value of synthesis data.

Besides, we also compared with some GAN-based unsupervised object segmentation methods on CUB benchmark, as shown in \tabref{table:bird}.
Our \methodName, utilizing diffusion model, can largely outperform all GAN-based method even without the need of training on the synthetic data.





\vspace{0.1cm}
{\noindent \bf Unsupervised Single Object Localization.}
Given the predicted segmentation mask from our model, we convert it to a bounding box by first connecting components, then choosing the tight outline of the largest components from the top, bottom, left and right sides. As shown in \tabref{tab:main_det}, our method reaches new state-of-the-art on all three benchmarks.  











\subsection{Synthesized Data Analysis}
This section provides a thorough analysis of our synthesized dataset. The results show that with sufficient data scale, our dataset is a reliable simulation of the real world.

\subsubsection{Data Statistics}
We compare our synthetic dataset to a real dataset DUTS-TR~\cite{duts}. For most key properties, statistics show that our synthetic dataset has a similar distribution to DUTS-TR.

\vspace{0.2cm}
{\noindent \bf Color Contrast.}
As shown in~\figref{fig:color}, our dataset has almost the same distribution of color contrast as DUTS-TR, which can make the model easy to transfer to the real world.

\vspace{0.2cm}
{\noindent \bf Object Size.}
Defining object size as the ratio of foreground pixels to full image pixels, \figref{fig:size} shows the comparison.
Compared to DUTS-TR, 
our synthetic data has a broader scale of salient objects (object sizes ranging from $0.1$ to $0.5$), which is suitable for training the object discovery task.


\vspace{0.2cm}
{\noindent \bf Center Bias.}
\figref{fig:center_bias} draws the scatter plot for each object using bounding box centers. 
In comparison with object-centric DUTS-TR, a more diverse center distribution contains more hard samples, and can improve generalizability of the model.

\vspace{0.2cm}
{\noindent \bf Geometry Statistics.}
\tabref{tab:shape} shows shape complexity (SC), polygon length (PL) and shape diversity (SD) of our dataset, which are close with real DUTS-TR.
Following~\cite{li2022bigdatasetgan}, we convert masks into polygons and define SC as vertice number, PL as perimeter. SD is defined as averaging pairwise Chamfer distance between two polygons.

\subsubsection{Training Performance}
We train two typical segmentation pipeline on different scale of synthetic dataset as well as real ones. Synthetic data is a replacement for real data with sufficient scale.

\vspace{0.2cm}
{\noindent \bf Compared with Real Dataset.}
Ideally, a well-established dataset should be capable of training on arbitrary architectures. To reveal such ability, we compare performances of training on our synthetic dataset and DUTS-TR. Specifically, we deal with saliency segmentation in an end-to-end manner. We select two widely used segmentation architectures, UNet~\cite{ronneberger2015u} and DeepLabV3~\cite{Chen2017RethinkingAC} as representatives. \textbf{Note that}, training on real data makes use of both image and \textit{ground truth} annotations, which can be seen as \textit{fully supervised} in this scenario.

\input{sec/table/train_on_syn}
\tabref{tab:train_syn} shows experimental results. The gap between synthetic and real data can be observed on the same data scale. However, performance could be boosted by adding more synthetic data. With an increase of $10\times$ in scale, the model is trained to be comparable with that on real data. Considering the infinite generating capability of diffusion models, we come to the conclusion that our synthetic data is a viable alternative to real ones.












\vspace{0.2cm}
{\noindent \bf Data Scale.}
~\figref{fig:scale} answers for ``how much synthetic data is enough for training?'' We increase the data scale from $1k$ to $100k$ and report IoU on DUTS-TE, and find there's a decrease marginal effect as scale increases. We keep the scale to $50k$, for its good trade-off between synthesizing cost and performance.

\input{sec/table/scale}




\subsection{Diffusion Features Analysis}
\input{sec/table/diffusion_feature}

To show the superiority of pre-trained diffusion features, we compare them with some discriminative pre-training methods (DINO~\cite{dino}, MoCo~\cite{moco}, CLIP~\cite{clip}). 
During training, we freeze all pre-trained models as feature extractor, attaching a same segment decoder for mask prediction.



\vspace{0.2cm}
{\noindent \bf Diffusion Pre-training \textit{vs.} Discriminative Pre-training.}
\tabref{tab:diffusion_feature} shows the privilege of diffusion features, compared with discriminative ones. Noticing that pixel-wise reconstruction is the most informative pre-training task among the three, it's not surprising to have such results. 
Although both diffusion and CLIP are pre-trained using text-image pairs,
discriminative pre-training like image-caption alignment focuses mainly on global features and loses detail.



\subsection{Ablation Study}



\vspace{0.2cm}
{\noindent \bf Mask Generation Methods.}
Besides our AttentionCut, \tabref{tab:raw_cut} also compares with other training-free segmentation methods.
They are usually used in RGB space.
Here, we apply them to diffusion features with minor modifications.
Overall, AttentionCut far outperforms these methods.

\input{sec/table/raw_cut}
\input{sec/table/attentioncut}

\vspace{0.2cm}
{\noindent \bf AttentionCut Components.}
\tabref{tab:attentioncut} ablates the AttentionCut formulation to show their effectiveness. $\mathcal{A}_c$ alone (\ding{172}) provides a reasonable mask prediction and can be enhanced by $r(p)$ (\ding{173},\,\ding{174}).
Both semantic and spatial coherence improve results (\ding{175},\,\ding{176}).
The benefits of two coherences are addable, and combining them all performs best (\ding{177}).


 

















\vspace{0.2cm}
{\noindent \bf Effectiveness of CLIP-classifiable Prior.}
\input{sec/table/classification_prior}
~\tabref{tab:class_prior} ablates the CLIP-classifiable prior. Under \textit{w/o prior} setting, the category label is replaced by an empty string.
It shows AttentionCut heavily relies on this prior, but
DiffusionSeg is robust.
As AttentionCut is based on attention maps, it is sensitive to the given label. 
However, in DiffusionSeg, the network is trained, potentially having the ability to understand diffusion features \textit{without} CLIP.



























