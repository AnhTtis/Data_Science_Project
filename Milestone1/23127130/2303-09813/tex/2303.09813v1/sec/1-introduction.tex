
\section{Introduction} 
\label{intro}

To date in the literature, large-scale pre-trained models, \ie, foundation models, have swept the CV 
domain for their remarkable progress. 
One general trend is pre-training then application, \ie, given a large corpus of data, first optimizes large-scale models to learn valuable prior knowledge about practical scenarios; then extracts some specific knowledge from pre-trained models for various downstream tasks. 




Specifically, existing foundation models can be grouped into two branches, namely, discriminative (\eg, MoCo~\cite{moco}, DINO~\cite{dino}, CLIP~\cite{clip}) and generative (\eg, MAE~\cite{mae}, Diffusion~\cite{sohl2015deep, ddpm}). 
The two branches have their own advantages. 
\textit{Discriminative-based models} are trained to align images within the same class or with corresponding captions, thus they are aware of ``what'' the object is, \ie, better at \textit{high-level semantic tasks}, \eg, classification and retrieval. 
While \textit{generative-based models} are trained to capture both low-level visual knowledge (textures, edges, structures) and high-level semantic relations, thus they are aware of ``what'' and ``where'' the object is, \ie, better at \textit{pixel-level processing tasks}, \eg, reconstruction and segmentation. 
In terms of applications to downstream tasks, these two foundation models have large gaps. 
Discriminative-based models have been explored for both discriminative and generative tasks, \eg, detection~\cite{zareian2021open}, segmentation~\cite{ma2022open}, image synthesis~\cite{wang2022clip} and video understanding~\cite{ju2022prompting,ju2022distilling}. However, since discriminative pre-training focuses more on high-level semantics, it is difficult to deal with dense prediction tasks well. While generative pre-training, with both low-level and high-level visual knowledge, is now still stuck in the limited applications of low-level tasks, \eg, image generation~\cite{nichol2021glide}, colorization~\cite{saharia2022palette}, visual inpainting~\cite{esser2021imagebart}.




Hence, a novel question naturally raises: \textit{is generative-based pre-training also or even more valuable for the mainstream discriminative tasks?}
This paper makes a step towards positively answering the question, \ie, we adopt popular diffusion models to solve object discovery, \ie, saliency segmentation and object localization. The \textit{unsupervised} setting is explored to clearly evaluate the effectiveness.


To adapt pre-trained diffusion models for downstream tasks, a vanilla idea is to directly use the feature inside the model. However, it is infeasible, as there are considerable gaps lying across diffusion models and discriminative object discovery. 
(1) The structural difference between generative and discriminative models limits the direct transfer, \ie, diffusion turns noise into random images, while object discovery finds masks from given images. 
(2) Lacking explicitly labeled data significantly limits training performance of downstream tasks, especially for unsupervision.



In this paper, we design one novel synthesis-exploitation framework, containing two-stage strategies to tackle the above two issues respectively.
Specifically, the first synthesis stage is designed to tackle the issue of insufficient labeled data. We propose novel training-free  AttentionCut to obtain masks during synthesizing sufficient images. 
Images are synthesized using text-to-image diffusion model with random noise and category as inputs. 
Masks are generated leveraging cross- and self- attention in this diffusion model. 
As shown in Fig.~\ref{fig:teaser}, these synthetic images are realistic with accurate mask, which is impressive and demonstrates the quality. 
The second exploitation stage is proposed to bridge the structural gap. We combine inversion technique with diffusion models, to deterministically map the given image back to diffusion features. 
This allows the diffusion model to be regarded as a universal knowledge extractor, which can be directly used by any downstream architecture.
Results show the strong capabilities of this knowledge and training a lightweight decoder can unify the utilization of diffusion pre-training and object discovery.



On six standard benchmarks, namely, ECSSD, DUTS, DUT-OMRON for segmentation, while VOC07, VOC12, COCO20K for detection, our method significntly outperforms existing state-of-the-art methods. We also conduct extensive ablation studies to reveal the effectiveness of each component, both quantitatively and qualitatively. 

To sum up, our contributions lie three fold:

$\bullet$ We pioneer the early exploration in adapting free pixel-level knowledge from pre-trained diffusion models to facilitate unsupervised object discovery;

\vspace{0.05cm}
$\bullet$ We design a novel synthesis-exploitation framework that  explicitly extracts knowledge through data synthesis and leverages implicit knowledge by diffusion inversion;


\vspace{0.05cm}
$\bullet$ We conduct extensive experiments and ablations to reveal the significance of adapting diffusion knowledge and our superior performance on six public benchmarks.


















































