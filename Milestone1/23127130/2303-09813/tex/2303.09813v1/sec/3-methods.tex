\section{Methods}



This paper aims to utilize pre-trained diffusion generation models for downstream tasks by proposing a two-stage synthesis-exploitation framework. 
In \secref{Problem Formulation}, we start by describing the preliminary. 
In \secref{Synthesizing Labeled Data}, we detail the synthesis stage to generate sufficient labeled data. 
In \secref{Diffusion Features and Synthetic Data Supervision}, we detail the exploitation stage to close the structural gap between generative models and discriminative tasks.










\subsection{Preliminary and Overview}
\label{Problem Formulation}
{\noindent \bf Problem Definition.} Object Discovery (OD), \ie, saliency segmentation and object localization, as a fundamental and typical discriminative task, is studied in this paper. 
Concretely, object discovery aims to train one pixel-level segmentation model $\Phi_{\mathrm{OD}}$ that partitions one image $\mathcal{I}$ into two disjoint groups, namely, foreground and background.
\vspace{-0.5em}
\begin{equation}
    \mathcal{M}_{\mathrm{seg}} = \Phi_{\mathrm{OD}}(\mathcal{I}) \in\{0,1\}^{H \times W \times 1}, \  \mathcal{I} \in \mathbb{R}^{H \times W \times 3}, 
\vspace{-0.5em}
\end{equation}
where $\mathcal{M}_{\mathrm{seg}}$ refers to the binary segmentation mask. 

Here, to clearly evaluate the effectiveness of our method, we focus on the strict \textit{unsupervised} setting, {\em i.e.}, the model is trained \textit{without} any manually annotated data. 


\vspace{0.1cm}
{\noindent \bf Motivation.} 
\hspace{1pt} This paper aims to exploit pixel-level visual knowledge from pre-trained diffusion generation models, for downstream discriminative tasks, \eg, OD. To achieve this goal, we design a novel synthesis-exploitation framework (\figref{fig:framework}). Specifically, at the synthesis stage, we explicitly construct one free (infinite-size) discriminative synthetic dataset, to obtain sufficient labeled samples. At the exploitation stage, we enable diffusion to be compatible with OD tasks, by extracting implicit diffusion features, and training one discovery decoder with the synthetic dataset.

























\vspace{0.2cm}
{\noindent \bf Diffusion}~\cite{sohl2015deep,ddpm} is one recently popular generative idea, containing forward and reverse processes. 
The \textit{forward process} is a Markov chain where noise is gradually added to the data.
The \textit{reverse process} is a denoising procedure that can be decomposed into a linear combination of a noisy image $\boldsymbol{x}_t$ and a noise approximator $\epsilon_\theta(\cdot)$. $t=1,\dots,T$ refers to the denoising timesteps.
The key to diffusion models is to learn the function $\epsilon_\theta(\cdot)$, typically using a UNet~\cite{ronneberger2015u}.


Particularly, we build on a variant of the text-to-image diffusion model, namely, Stable Diffusion~\cite{ldm}. During the synthesis process, it's sampled by iteratively denoising $\boldsymbol{x}_t$ conditioned on the input text prompt $y$ for timestep $t=1,\dots,T$. The conditional denoising UNet $\boldsymbol\epsilon_\theta(\boldsymbol{x}_t, t, y)$ stacks layers of self- and cross-attentions. 
$y$ is first encoded to text embeddings by a pre-trained text encoder, then text embeddings are mapped to intermediate layers as $K$ and $V$ via the attention mechanism, and the noisy image $\boldsymbol{x}_t$ is mapped as $Q$. 
For step $t$ and layer $l$, we call cross-attention as $\mathcal{A}_c^{t,l}$, self-attention as $\mathcal{A}_s^{t,l}$, and intermediate features as $\mathcal{F}^{t,l}$.
{\bf Note that}, this paper freezes Stable Diffusion pre-trained on LAION-5B~\cite{schuhmann2022laion} (5 billion image-text pairs), as a knowledge provider. This diffusion model involves both low-level object details and high-level class semantics, enabling us to achieve unsupervised object discovery.

































\subsection{Synthesis Stage:  Free Data Generation}
\label{Synthesizing Labeled Data}
As illustrated in \figref{fig:framework} (1), this stage aims to synthesize large and free image-mask pairs through Stable Diffusion, solving the lack of labeled training data under unsupervised settings. 
We detail image synthesis in \secref{sec:Image Generation}, and mask generation in \secref{synthetic mask generation}.
 





\vspace{-0.25cm}
\subsubsection{Image Generation}
\label{sec:Image Generation}
For one pre-trained text-to-image Stable Diffusion~\cite{ldm}, we here freeze it, then generate images through inputting random Gaussian noise and class text prompts. Class names are sampled from ImageNet~\cite{imagenet}. 



For text input, a simple way is to simply use class names, but this may limit diversity and cause bottlenecks for downstream tasks. 
Hence, to adaptively generate various text prompts for each class, we interact with ChatGPT~\cite{chatGPT}. 
For example, we ask ChatGPT to list prompts about ``aeroplane'', then it could give some generative-style prompts like: ``\textit{A aeroplane soaring through a vibrant sunset sky, fluffy clouds, warm lighting, viewed from a low angle, realistic style.}'' 
The generated prompts introduce richer context, thus can better unleash the potential of the Stable Diffusion to synthesis high-fidelity, more diverse images. One noise reduction strategy is also applied following~\cite{he2022synthetic}.


\vspace{-0.25cm}
\subsubsection{Mask Generation}
\label{synthetic mask generation}




















Here, we generate high-quality masks by leveraging attentions in pre-trained diffusion models as clues, following two non-trivial observations.
(1) Cross-attention $\mathcal{A}_c$ indicates locality between the conditioning text and noisy image, thus $\mathcal{A}_c$ can coarsely describe \textit{objectness}.
(2) Self-attention $\mathcal{A}_s$ inside one image indicates pairwise semantic similarity between pixels, thus $\mathcal{A}_s$ could roughly describe \textit{coherence}.
Inspired by these, we propose AttentionCut, a training-free strategy to generate masks guided by attention maps.







\vspace{0.2cm}
{\noindent \bf Preparations.}
We first extract $\mathcal{A}_c$ and $\mathcal{A}_s$ at the position of category token in the prompt sentence, then aggregate different resolutions and timesteps considering multi-scale objects and avoiding focus shift during diffusion.
Formally,
\vspace{-0.5em}
\begin{equation}
    \mathcal{A}_c=\frac{1}{kT}\sum_{l=1}^{k}\sum_{t=0}^{T-1} \mathcal{A}_c^{t,l}; \
    \mathcal{A}_s=\frac{1}{LT}\sum_{l=1}^{L}\sum_{t=0}^{T-1} \mathcal{A}_s^{t,l}, 
    \vspace{-0.5em}
\end{equation}
where $t=T-1, \dots, 0$ is for each reverse step and $l = 1, \dots, L$ is for intermediate layers. $\mathcal{A}_c$ is averaged among the top-$k$ of the standard variation from all $\mathcal{A}_c^l$, while $\mathcal{A}_s$ is averaged among all layers and time steps.


\vspace{0.2cm}
{\noindent \bf Objectness.}
Intuitively, the pixel-level cross-attention $\mathcal{A}_c$ under a specific category can roughly be seen as segmentation masks, as it indicates how likely a pixel belongs to the category. However, in practice we found $\mathcal{A}_c$ is sparse and inattentive near the boundary, which can seriously damage segmentation results. 
To handle this issue, we improve $\mathcal{A}_c$ by strengthening the edge area with the self-attention $\mathcal{A}_s$. It indicates semantic connectivity, \ie, how semantically two pixels belong to one group. 
Specifically, we first randomly select a set of initial seeds $\mathcal{B}$ from the boundary of the binary mask $\left[\mathcal{A}_c>\tau\right]$. 
Then each selected seed $b \in \mathcal{B}$ can expand as a confidence map $\mathcal{A}_s(b, \cdot)$, which is the self-attention between $b$ and other pixels, indicating weights of the boundary area. 
We assume $\mathcal{A}_s(\cdot, b)=\mathcal{A}_s(b, \cdot)$, as $\mathcal{A}_s$ is symmetric theoretically.
For pixel $p$, these maps are averaged as a refined map $r(p)$, to reinforce the boundary pixels: 
\vspace{-0.5em}
\begin{equation}
    r(p) = 1/{|\mathcal{B}|}\cdot \sum\nolimits_{b \in {B}} \mathcal{A}_s(p,b).
\vspace{-0.5em}
\end{equation}

Combining cross-attention $\mathcal{A}_c$ and the refined map $r(p)$ with a balance weight $\lambda_\phi$, the pixel-level objectness $\phi$ are:
\vspace{-0.5em}
\begin{equation}
    \phi(p) = \left\{
    \begin{aligned}
        -\log(\mathcal{A}_c(p)+\lambda_\phi r(p)),\  \text{if }p \in\text{foreground},\\
        \log(1-\mathcal{A}_c(p)-\lambda_\phi r(p)),\  \text{if }p\in\text{background},
    \end{aligned}
    \right.
    \vspace{-0.5em}
\end{equation}
where $\mathcal{A}_c(p)$ is the cross-attention at pixel $p$.


\vspace{0.2cm}
{\noindent \bf Inner Coherence.}
With only objectness, we found that the masks tend to lose local information, for example, irregular corners, mis-segmented holes, or jagged contours.
This can be solved by taking local consistency into account, \ie, how likely two neighboring pixels belong to one group.
Here we design an inner coherence term that can help to enforce continuity, proximity and smoothness of segments belonging to the same object, and penalize those who deviate.




The proposed inner coherence consists of two parts: semantic and spatial.
As mentioned above, $\mathcal{A}_s$ can indicate semantic coherence, as self-attention is calculated in semantic feature space.
Spatial coherence is designed to indicate pixels pairwise distance in both RGB and Euclidian space.
This coherence is obtained by absorbing the form of geodesic distance on the surface of image intensity, then by negative exponential transformation.
The inner coherence $\psi$ can be formalized as:
\vspace{-0.5em}
\begin{equation}
    \begin{aligned}
        \psi(p,q) &= \mathcal{A}_s(p,q) + \lambda_\psi e^{-\mathcal{D}(p,q)},\\
        \mathcal{D}(p,q) &= \min_{P}\int_0^1\|\nabla I\left(P(s)\right)\cdot v(s)\|ds,
    \end{aligned}
    \label{eq:psi}
    \vspace{-0.5em}
\end{equation}
where for pixel $p$ and $q$, $\mathcal{A}_s(p,q)$ is the self-attention and $\mathcal{D}(p,q)$ is the geodesic distance;
$P$ is an arbitrary path from $p$ to $q$ parameterized by $s\in[0,1]$;
$v(s)$ denotes the unit vector $P'(s)/\|P'(s)\|$ that is tangent to the path direction; $I(\cdot)$ is image RGB intensity.






\vspace{0.2cm}
{\noindent \bf Calculating Mask.}
Given objectness and inner coherence, we define an energy function $E$ for each potential mask $\mathcal{M}$:
\vspace{-0.5em}
\begin{equation}
E(\mathcal{M}) = \sum\nolimits_p\phi(p)+\lambda\sum\nolimits_{\mathcal{M}(p)\neq\mathcal{M}(q)}\psi(p,q),
\vspace{-0.5em}
\end{equation}
where $\lambda$ denotes the weight between $\phi$ and $\psi$; $\mathcal{M}(\cdot)\in\{0,1\}$ means the pixel in this mask. The binary mask $\mathcal{M}$ is generated by minimizing $E(\mathcal{M})$, \ie, use Ford-Fulkerson algorithm~\cite{ford1956maximal} to find a minimum cut in the image graph. And after further post-processing and denoising~\cite{barron2016fast, zhang2021datasetgan,li2022bigdatasetgan}, we can obtain the final synthetic mask (see \figref{fig:teaser} Right for some examples).









\vspace{0.2cm}
{\noindent \bf Discussion.}
Compared with other training-free mask generation methods like NCut~\cite{shi2000normalized} and K-means~\cite{lloyd1982},
they only consider pairwise similarly, thus cannot decide fore/background for each partition. 
Compared with DenseCRF~\cite{NIPS2011_beda24c1},
AttentionCut has well-designed objectness and inner coherence terms, which is more suitable for diffusion models and guarantees convergence.
In \tabref{tab:raw_cut}, we have conducted experiments to validate the superiority of AttentionCut.



\subsection{Exploitation Stage: Diffusion Knowledge }
\label{Diffusion Features and Synthetic Data Supervision}


This stage aims to bridge the architectural gap between pre-trained diffusion models and discriminative tasks, \eg, object discovery. 
As shown in \figref{fig:framework} (2), we achieve this in two steps: in \secref{Extracting Diffusion Features}, we treat diffusion models as a universal feature extractor to distill explicit visual knowledge; in \secref{Segment Decoder}, we feed diffusion features into one flexible decoder, and train with ``infinite'' synthetic data.









\subsubsection{Extracting Diffusion Knowledge}
\label{Extracting Diffusion Features}






For diffusion models, they are fed with noise and text to output synthesis images; while for object discovery models, they are fed with images to output pixel-level masks. Such an architectural gap blocks direct feature extraction from diffusion. 
To solve this, given one image, we are required to find the corresponding input noise of diffusion models under some conditioning text, then features can be extracted through diffusion reverse process.
To get input noise, we combine diffusion inversion~\cite{ddim}
with the conditional UNet. To get the conditioning text, we simply classify images by CLIP~\cite{clip}.











\vspace{0.2cm}
{\noindent \bf Diffusion Inversion and Feature Extraction.} 
Given pre-trained diffusion models, we here inverse one image back to its corresponding noise under the conditioning text. 
This diffusion inversion can be seen as a special forward process.





One trivial solution is to use the typical DDPM~\cite{ddpm}. Although it can yield latent variables (\ie, noise) through the forward process, 
these variables are stochastic and cannot reconstruct the image through the reverse process.
So it is not suitable for feature extraction.
Inspired by DDIM~\cite{ddim}, we modify each step by combining it with conditional denoising UNet $\boldsymbol\epsilon_\theta(\boldsymbol{x}_t, t, y)$ in Stable Diffusion, making the forward/reverse non-Markovian to enjoy deterministic. 
Now the forward/reverse process for each step is:
\vspace{-0.5em}
\begin{equation}
\resizebox{0.90\linewidth}{!}{$
\begin{aligned}
    &\boldsymbol{x}_{t+1}=\sqrt{\alpha_{t+1}} \boldsymbol{f}_\theta\left(\boldsymbol{x}_t, t, y\right)+\sqrt{1-\alpha_{t+1}} \boldsymbol{\epsilon}_\theta\left(\boldsymbol{x}_t, t, y\right),
    \\
    &\boldsymbol{x}_{t-1}=\sqrt{\alpha_{t-1}} \boldsymbol{f}_\theta\left(\boldsymbol{x}_t, t,y\right)+\sqrt{1-\alpha_{t-1}} \boldsymbol{\epsilon}_\theta\left(\boldsymbol{x}_t, t,y\right),
    \end{aligned}$
    }
    \label{equ: deterministic reverse}
    \vspace{-0.5em}
\end{equation}
where $\boldsymbol{f}_\theta\left(\boldsymbol{x}_t, t, y\right)=\left({\boldsymbol{x}_t-\sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}_\theta\left(\boldsymbol{x}_t, t, y\right)}\right)\,/\, {\sqrt{\bar{\alpha}_t}}$, $\alpha_t=1-\beta_t$, $\bar{\alpha}_t=\prod_{s=1}^t\left(1-\beta_s\right)$, $\beta_t$ is a variance schedule. $y$ denotes the conditional text, and $t$ means timesteps.

After diffusion inversion, to get the corresponding noise, features $\mathcal{F}^{t,l}$ can be extracted from $\boldsymbol\epsilon_\theta(\boldsymbol{x}_t, t, y)$ during each reverse step $t=T-1, \dots, 0$ and intermediate layer $l = 1, \dots, L$.
To cover long range and multi-level features of multi-scale objects, they are aggregated in all time steps:
\vspace{-0.5em}
\begin{equation}
\mathcal{F}^l=1/T\cdot\sum\nolimits_{t=0}^{T-1} \mathcal{F}^{t,l}.
\vspace{-0.5em}
\end{equation}
In practice, we choose the output of the ``SpatialTransformer'' block in Stable Diffusion, where $L=6$ with resolutions $16 \times 16$, $32 \times 32$, and $64 \times 64$, two of each.


\input{sec/table/main_table}


\vspace{0.2cm}
{\noindent \bf CLIP-classifiable Prior.}
Notice that in \equref{equ: deterministic reverse}, the diffusion inversion should be done under some conditional text $y$.
We choose $y$ to be the CLIP-classified category of the input image, because of the following observations:
(1) humans take pictures by naturally framing an object of interest near the center of the image~\cite{judd2009learning} (center prior); 
(2) most background regions can be easily connected to image boundaries, while difficult for object regions~\cite{wei2012geodesic} (background prior); 
(3) CLIP is pre-trained on a large corpus of web-curated data, and most of which is human-token images with saliency objects~\cite{clip} (source prior). 
It is easy to classify images with the center and background priors, and the source prior enables us to classify using CLIP~\cite{clip}.
We summarize this as \textit{CLIP-classifiable prior}.

In practice, we choose the label set in ImageNet~\cite{imagenet}, and combine semantically similar classes, \eg, poodle and Chihuahua as dogs, etc.
Besides, multiple prompt templates are used, \eg, ``A photo of \{category\}'' to boost performance.



\subsubsection{Segment Decoder}
\label{Segment Decoder}
To enable diffusion compatible with object discovery, we here propose two options for preference. 
One is to attach a flexible decoder to the pre-trained diffusion models, and train using the synthesised data to achieve object discovery. This option costs many parameters and rich training data, bringing superior performance, and we denote it as {\textit{DiffusionSeg}} in  {\tabref{table:main_all}}. 
The other is to extract cross- and self-attention during diffusion inversion, and generate pseudo-masks using AttentionCut in \secref{synthetic mask generation}.
Such an option costs no trainable parameters and data, thus showing faster inference speeds, and we call it {\textit{AttentionCut}} in \tabref{table:main_all}. 

 
















\subsection{Discussion}
This paper uses pre-trained diffusion models for unsupervised object discovery.
Comparing with discriminative pre-training~\cite{lost,tokencut,selfmask}, generative pre-training has additional pixel-level understanding, which is more suitable for object discovery. 
Compared with MAE-style~\cite{mae} generative pre-training, which learns reconstruction representations to help object discovery, diffusion models show a clear advantage, \ie, synthesis abundant data, which is valuable to improve performance (see \tabref{tab:train_syn} and \figref{fig:scale}). 
Comparing with GANs in image synthesizing, diffusion models have significant advantages in higher sample quality and diversity, more stability and robustness~\cite{dhariwal2021diffusion}.
Compared to a few early GAN-based works that struggle to synthesise  mask with manual annotations~\cite{zhang2021datasetgan,li2022bigdatasetgan}, diffusion model can obtain mask using AttentionCut, without manually labeling.













