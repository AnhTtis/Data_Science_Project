{
    "arxiv_id": "2303.09813",
    "paper_title": "DiffusionSeg: Adapting Diffusion Towards Unsupervised Object Discovery",
    "authors": [
        "Chaofan Ma",
        "Yuhuan Yang",
        "Chen Ju",
        "Fei Zhang",
        "Jinxiang Liu",
        "Yu Wang",
        "Ya Zhang",
        "Yanfeng Wang"
    ],
    "submission_date": "2023-03-17",
    "revised_dates": [
        "2023-03-20"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV",
        "cs.AI"
    ],
    "abstract": "Learning from a large corpus of data, pre-trained models have achieved impressive progress nowadays. As popular generative pre-training, diffusion models capture both low-level visual knowledge and high-level semantic relations. In this paper, we propose to exploit such knowledgeable diffusion models for mainstream discriminative tasks, i.e., unsupervised object discovery: saliency segmentation and object localization. However, the challenges exist as there is one structural difference between generative and discriminative models, which limits the direct use. Besides, the lack of explicitly labeled data significantly limits performance in unsupervised settings. To tackle these issues, we introduce DiffusionSeg, one novel synthesis-exploitation framework containing two-stage strategies. To alleviate data insufficiency, we synthesize abundant images, and propose a novel training-free AttentionCut to obtain masks in the first synthesis stage. In the second exploitation stage, to bridge the structural gap, we use the inversion technique, to map the given image back to diffusion features. These features can be directly used by downstream architectures. Extensive experiments and ablation studies demonstrate the superiority of adapting diffusion for unsupervised object discovery.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.09813v1"
    ],
    "publication_venue": null
}