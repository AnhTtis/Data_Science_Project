@Article{mdcgen,
         author={F. Iglesias and T.             Zseby and D. Ferreira},
         title={{MDCG}en: Multidimensional Dataset Generator for Clustering},
         journal={Journal of Classification},
         volume={36},
         pages={599-618},
         year={2019}}
         
@article{Haar,
        author={F. Mezzadri},
        title={How to generate random matrices from the classical compact groups},
        journal={Notices of the American Mathematical Society},
        pages = {592-604},
        volume = {54(5)},
        year={2007}}

@Book{ggplot,
    author = {Hadley Wickham},
    title = {ggplot2: Elegant Graphics for Data Analysis},
    publisher = {Springer-Verlag New York},
    year = {2016},
    url = {https://ggplot2.tidyverse.org},
}

@article{Ball,
    author = {K. Ball},
    title = "{A lower bound for the optimal density of lattice packings}",
    journal = {International Mathematics Research Notices},
    volume = {1992},
    number = {10},
    pages = {217-221},
    year = {1992},
    eprint = {https://academic.oup.com/imrn/article-pdf/1992/10/217/6767935/1992-10-217.pdf},
}

@inproceedings{kmeans,
 Author = {MacQueen, J.},
 Title = {Some methods for classification and analysis of multivariate observations},
 Year = {1967},
 Language = {English},
 booktitle = {Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability},
 series = {University of California, Berkeley},
 pages={281-297},
 year={1967},
 Keywords = {62H30},
 zbMATH = {3340881},
 Zbl = {0214.46201}
}

@book{ESL,
  title={The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition},
  author={Hastie, T. and Tibshirani, R. and Friedman, J.},
  lccn={2008941148},
  series={Springer Series in Statistics},
  year={2009},
  publisher={Springer New York}
}


@article{bahadur,
 abstract = {Linear procedures for classifying an observation as coming from one of two multivariate normal distributions are studied in the case that the two distributions differ both in mean vectors and covariance matrices. We find the class of admissible linear procedures, which is the minimal complete class of linear procedures. It is shown how to construct the linear procedure which minimizes one probability of misclassification given the other and how to obtain the minimax linear procedure; Bayes linear procedures are also discussed.},
 author = {T. W. Anderson and R. R. Bahadur},
 journal = {The Annals of Mathematical Statistics},
 number = {2},
 pages = {420--431},
 publisher = {Institute of Mathematical Statistics},
 title = {Classification into two Multivariate Normal Distributions with Different Covariance Matrices},
 volume = {33},
 year = {1962}
}

@inproceedings{ami,
author = {Vinh, Nguyen Xuan and Epps, Julien and Bailey, James},
title = {Information Theoretic Measures for Clusterings Comparison: Is a Correction for Chance Necessary?},
publisher = {ACM},
year = {2009},
abstract = {Information theoretic based measures form a fundamental class of similarity measures for comparing clusterings, beside the class of pair-counting based and set-matching based measures. In this paper, we discuss the necessity of correction for chance for information theoretic based measures for clusterings comparison. We observe that the baseline for such measures, i.e. average value between random partitions of a data set, does not take on a constant value, and tends to have larger variation when the ratio between the number of data points and the number of clusters is small. This effect is similar in some other non-information theoretic based measures such as the well-known Rand Index. Assuming a hypergeometric model of randomness, we derive the analytical formula for the expected mutual information value between a pair of clusterings, and then propose the adjusted version for several popular information theoretic based measures. Some examples are given to demonstrate the need and usefulness of the adjusted measures.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {1073–1080},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

         
@Article{silhouette,
        author={P. Rousseeuw},
        title={Silhouettes: a graphical aid to the interpretation and validation of cluster analysis},
        journal={Journal of Computational and Applied Mathematics},
        volume={20},
        pages={53-65},
        year={1987}}
        
@Article{matplotlib,
  Author    = {J. D. Hunter},
  Title     = {Matplotlib: A {2D} graphics environment},
  Journal   = {Computing in Science \& Engineering},
  Volume    = {9},
  Number    = {3},
  Pages     = {90-95},
  abstract  = {Matplotlib is a 2D graphics package used for Python for
  application development, interactive scripting, and publication-quality
  image generation across user interfaces and operating systems.},
  publisher = {IEEE COMPUTER SOC},
  year      = 2007
}

@ARTICLE{scipy,
  author  = {Virtanen, P. and Gommers, R. and Oliphant, T. E. and
            Haberland, M. and Reddy, T. and Cournapeau, D. and
            Burovski, E. and Peterson, P. and Weckesser, W. and
            Bright, J. and {van der Walt}, S. J. and
            Brett, M. and Wilson, J. and Millman, K. J. and
            Mayorov, N. and Nelson, A. R. J. and Jones, E. and
            Kern, R. and Larson, E. and Carey, C. J. and
            Polat, I. and Feng, Y. and Moore, E. W. and
            {VanderPlas}, J. and Laxalde, D. and Perktold, J. and
            Cimrman, R. and Henriksen, I. and Quintero, E. A. and
            Harris, C. R. and Archibald, A. M. and
            Ribeiro, A. H. and Pedregosa, F. and
            {van Mulbregt}, P. and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261-272}
}

@Article{         numpy,
 title         = {Array programming with {NumPy}},
 author        = {C. R. Harris and K. J. Millman and S. J.
                 van der Walt and R. Gommers and P. Virtanen and D.
                 Cournapeau and E. Wieser and J. Taylor and S.
                 Berg and N. J. Smith and R. Kern and M. Picus
                 and S. Hoyer and M. H. van Kerkwijk and M.
                 Brett and A. Haldane and J. Fern{\'{a}}ndez del
                 R{\'{i}}o and M. Wiebe and P. Peterson and P.
                 G{\'{e}}rard-Marchant and K. Sheppard and T. Reddy and
                 W. Weckesser and H. Abbasi and C. Gohlke and
                 T. E. Oliphant},
 year          = {2020},
 journal       = {Nature},
 volume        = {585},
 number        = {7825},
 pages         = {357-362},
 publisher     = {Springer Science and Business Media {LLC}}
}
         
@techreport{peizaiane06,
  author={Y. Pei and O. Zaïane},
  title={A synthetic data generator for clustering and outlier analysis},
  group={Computing Science Department},
  institution={University of Alberta, Edmonton},
  year={2006}}

@inproceedings{hawks,
    author={C. Shand and R. Allmendinger and J. Handl and A. Webb and J. Keane},
    booktitle = {Proceedings of the Genetic and Evolutionary Computation                   Conference},
    title = {Evolving Controllably Difficult Datasets for Clustering},
    series={GECCO '19},
    year={2019},
    location={Prague, Czech Republic},
    publisher={ACM},
    pages={463-471}}

@article{qiujoe06sep,
  title={Separation index and partial membership for clustering},
  author={W. Qiu and H. Joe},
  journal={Computational Statistics \& Data Analysis},
  year={2006},
  volume={50},
  pages={585-603}
}

@Article{qiujoe06,
        author={W. Qiu and H. Joe},
        title={Generation of Random Clusters with Specified Degree of Separation},
        journal={Journal of Classification},
        volume={23},
        pages={315–334},
        year={2006}}

@inproceedings{handl05,
  author={J. Handl and J. Knowles},
  booktitle={Proceedings of the IEEE Congress on Evolutionary Computation}, 
  title={Improvements to the scalability of multiobjective clustering}, 
  year={2005},
  series={CEC '05},
  location={Edinburgh, UK},
  publisher={IEEE},
  pages={2372-2379}}
  
@article{elki,
  author    = {E. Schubert and
               A. Zimek},
  title     = {{ELKI:} {A} large open-source library for data analysis - {ELKI} Release
               0.7.5 ``{H}eidelberg''},
  journal   = {CoRR},
  volume    = {abs/1902.03616},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.03616},
  eprinttype = {arXiv},
  eprint    = {1902.03616},
  timestamp = {Sat, 23 Jan 2021 01:11:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-03616.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@Book{pearl:88,
  author = 	 {Judea Pearl},
  title = 	 {Probabilistic {R}easoning in {I}ntelligent {S}ystems: 
		  {N}etworks of {P}lausible {I}nference},
  publisher = 	 {Morgan Kaufman Publishers},
  year = 	 {1988},
  address = 	 {San Mateo, CA}
}

@article{
PopAlign,
author = {Sisi Chen  and Paul Rivaud  and Jong H. Park  and Tiffany Tsou  and Emeric Charles  and John R. Haliburton  and Flavia Pichiorri  and Matt Thomson },
title = {Dissecting heterogeneous cell populations across drug and disease conditions with PopAlign},
journal = {Proceedings of the National Academy of Sciences},
volume = {117},
number = {46},
pages = {28784-28794},
year = {2020},
doi = {10.1073/pnas.2005990117},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2005990117},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2005990117},
abstract = {Many physiological processes are driven by changes across heterogeneous populations of cells. However, we currently lack a conceptual framework for comparing single-cell transcriptional data collected from populations as they respond to perturbations. Here, we develop a framework, called PopAlign, that models a cell population as a probability distribution in gene-expression space. Individual subpopulations are represented as local densities that are statistically aligned across samples. The models present an integrated representation that allows comparisons at multiple scales, from gene-expression programs to cell state to population structure. The models are memory-efficient and can be scaled across hundreds of samples, which we demonstrate using public data and data from human immune cells responding to drugs and disease. Single-cell measurement techniques can now probe gene expression in heterogeneous cell populations from the human body across a range of environmental and physiological conditions. However, new mathematical and computational methods are required to represent and analyze gene-expression changes that occur in complex mixtures of single cells as they respond to signals, drugs, or disease states. Here, we introduce a mathematical modeling platform, PopAlign, that automatically identifies subpopulations of cells within a heterogeneous mixture and tracks gene-expression and cell-abundance changes across subpopulations by constructing and comparing probabilistic models. Probabilistic models provide a low-error, compressed representation of single-cell data that enables efficient large-scale computations. We apply PopAlign to analyze the impact of 40 different immunomodulatory compounds on a heterogeneous population of donor-derived human immune cells as well as patient-specific disease signatures in multiple myeloma. PopAlign scales to comparisons involving tens to hundreds of samples, enabling large-scale studies of natural and engineered cell populations as they respond to drugs, signals, or physiological change.}}

@misc{RenOODforLLMs,
      title={Out-of-Distribution Detection and Selective Generation for Conditional Language Models}, 
      author={Jie Ren and Jiaming Luo and Yao Zhao and Kundan Krishna and Mohammad Saleh and Balaji Lakshminarayanan and Peter J. Liu},
      year={2023},
      eprint={2209.15558},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2209.15558}, 
}

@inproceedings{UniversalLMClassification,
    title = "Universal Language Model Fine-tuning for Text Classification",
    author = "Howard, Jeremy  and
      Ruder, Sebastian",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1031",
    doi = "10.18653/v1/P18-1031",
    pages = "328--339",
    abstract = "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24{\%} on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",
}

@inproceedings{DBSCAN,
author = {Ester, Martin and Kriegel, Hans-Peter and Sander, J\"{o}rg and Xu, Xiaowei},
title = {A density-based algorithm for discovering clusters in large spatial databases with noise},
year = {1996},
publisher = {AAAI Press},
abstract = {Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of DBSCAN using synthetic data and real data of the SEQUOIA 2000 benchmark. The results of our experiments demonstrate that (1) DBSCAN is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm CLAR-ANS, and that (2) DBSCAN outperforms CLARANS by a factor of more than 100 in terms of efficiency.},
booktitle = {Proceedings of the Second International Conference on Knowledge Discovery and Data Mining},
pages = {226–231},
numpages = {6},
keywords = {handling nlj4-275oise, efficiency on large spatial databases, clustering algorithms, arbitrary shape of clusters},
location = {Portland, Oregon},
series = {KDD'96}
}

@inproceedings{AzariaMitchell2023,
    title = "The Internal State of an {LLM} Knows When It{'}s Lying",
    author = "Azaria, Amos  and
      Mitchell, Tom",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.68",
    doi = "10.18653/v1/2023.findings-emnlp.68",
    pages = "967--976",
    abstract = "While Large Language Models (LLMs) have shown exceptional performance in various tasks, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. In this paper, we provide evidence that the LLM{'}s internal state can be used to reveal the truthfulness of statements. This includes both statements provided to the LLM, and statements that the LLM itself generates. Our approach is to train a classifier that outputs the probability that a statement is truthful, based on the hidden layer activations of the LLM as it reads or generates the statement. Experiments demonstrate that given a set of test sentences, of which half are true and half false, our trained classifier achieves an average of 71{\%} to 83{\%} accuracy labeling which sentences are true versus false, depending on the LLM base model. Furthermore, we explore the relationship between our classifier{'}s performance and approaches based on the probability assigned to the sentence by the LLM. We show that while LLM-assigned sentence probability is related to sentence truthfulness, this probability is also dependent on sentence length and the frequencies of words in the sentence, resulting in our trained classifier providing a more reliable approach to detecting truthfulness, highlighting its potential to enhance the reliability of LLM-generated content and its practical applicability in real-world scenarios.",
}

@article{HennigTrueClusters,
title = {What are the true clusters?},
journal = {Pattern Recognition Letters},
volume = {64},
pages = {53-62},
year = {2015},
note = {Philosophical Aspects of Pattern Recognition},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2015.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167865515001269},
author = {Christian Hennig},
keywords = {Constructivism, Active scientific realism, Natural kinds, Categorization, Mixture models, Variable selection},
abstract = {Constructivist philosophy and Hasok Chang’s active scientific realism are used to argue that the idea of “truth” in cluster analysis depends on the context and the clustering aims. Different characteristics of clusterings are required in different situations. Researchers should be explicit about on what requirements and what idea of “true clusters” their research is based, because clustering becomes scientific not through uniqueness but through transparent and open communication. The idea of “natural kinds” is a human construct, but it highlights the human experience that the reality outside the observer’s control seems to make certain distinctions between categories inevitable. Various desirable characteristics of clusterings and various approaches to define a context-dependent truth are listed, and I discuss what impact these ideas can have on the comparison of clustering methods, and the choice of a clustering methods and related decisions in practice.}
}

@article{McCormackClassificationReview,
 ISSN = {00359238},
 URL = {http://www.jstor.org/stable/2344237},
 abstract = {The summarization of large quantities of multivariate data by clusters, undefined a priori, is increasingly practiced, often irrelevantly and unjustifiably. This paper attempts to survey the burgeoning bibliography, restricting itself to published, freely available, references of known provenance. A plethora of definitions of similarity and of cluster are presented. The principles, but not details of implementation, of the many empirical classification techniques currently in use are discussed, and limitations and short-comings in their development and practice are pointed out. Methods based on well-defined mathematical formulations of the problem are emphasized, and other ways of summarizing data are suggested as alternatives to classification. The growing tendency to regard numerical taxonomy as a satisfactory alternative to clear thinking is condemned.},
 author = {R. M. Cormack},
 journal = {Journal of the Royal Statistical Society. Series A (General)},
 number = {3},
 pages = {321--367},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {A Review of Classification},
 urldate = {2024-09-13},
 volume = {134},
 year = {1971}
}

@article{Milligan1980,
  author    = {Glenn W. Milligan},
  title     = {An examination of the effect of six types of error perturbation on fifteen clustering algorithms},
  journal   = {Psychometrika},
  year      = {1980},
  volume    = {45},
  number    = {3},
  pages     = {325--342},
  doi       = {10.1007/BF02293907},
  url       = {https://doi.org/10.1007/BF02293907},
  abstract  = {An evaluation of several clustering methods was conducted. Artificial clusters which exhibited the properties of internal cohesion and external isolation were constructed. The true cluster structure was subsequently hidden by six types of error-perturbation. The results indicated that the hierarchical methods were differentially sensitive to the type of error perturbation. In addition, generally poor recovery performance was obtained when random seed points were used to start the K-means algorithms. However, two alternative starting procedures for the nonhierarchical methods produced greatly enhanced cluster recovery and were found to be robust with respect to all of the types of error examined.},
  issn      = {1860-0980}
}

@ARTICLE{Milligan1983,
  author={Milligan, Glenn W. and Soon, S. C. and Sokol, Lisa M.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={The Effect of Cluster Size, Dimensionality, and the Number of Clusters on Recovery of True Cluster Structure}, 
  year={1983},
  volume={PAMI-5},
  number={1},
  pages={40-47},
  keywords={Clustering algorithms;Statistics;Monte Carlo methods;Clustering methods;Size measurement;Error analysis;Algorithm design and analysis;Character generation;Extraterrestrial measurements;Cluster recovery;clustering validation;error perturbation procedures;external criterion statistics;Monte Carlo methods},
  doi={10.1109/TPAMI.1983.4767342}}

@article{Steinley2008,
  author    = {Douglas Steinley and Michael J. Brusco},
  title     = {Selection of Variables in Cluster Analysis: An Empirical Comparison of Eight Procedures},
  journal   = {Psychometrika},
  year      = {2008},
  volume    = {73},
  number    = {1},
  pages     = {125--144},
  doi       = {10.1007/s11336-007-9019-y},
  url       = {https://doi.org/10.1007/s11336-007-9019-y},
  abstract  = {Eight different variable selection techniques for model-based and non-model-based clustering are evaluated across a wide range of cluster structures. It is shown that several methods have difficulties when non-informative variables (i.e., random noise) are included in the model. Furthermore, the distribution of the random noise greatly impacts the performance of nearly all of the variable selection procedures. Overall, a variable selection technique based on a variance-to-range weighting procedure coupled with the largest decreases in within-cluster sums of squares error performed the best. On the other hand, variable selection methods used in conjunction with finite mixture models performed the worst.},
  issn      = {1860-0980}
}

@article{Steinley2011,
  author    = {Douglas Steinley and Michael J. Brusco},
  title     = {Evaluating mixture modeling for clustering: recommendations and cautions},
  journal   = {Psychological Methods},
  year      = {2011},
  volume    = {16},
  number    = {1},
  pages     = {63--79},
  doi       = {10.1037/a0022673},
  issn      = {1939-1463},
  abstract  = {This article provides a large-scale investigation into several of the properties of mixture-model clustering techniques (also referred to as latent class cluster analysis, latent profile analysis, model-based clustering, probabilistic clustering, Bayesian classification, unsupervised learning, and finite mixture models). Focus is given to the multivariate normal distribution, and 9 separate decompositions of the covariance matrix are investigated. Comparisons are made with K-means clustering in 3 detailed Monte Carlo studies. Mixture-model clustering techniques performed best when the covariance structure and number of clusters were known, but performance degraded as this information became unknown.},
  pmid      = {21319900},
  grant     = {K25AA017456-02/AA/NIAAA NIH HHS/United States}
}

@article{WhitePaper,
author = {Van Mechelen, Iven and Boulesteix, Anne-Laure and Dangl, Rainer and Dean, Nema and Hennig, Christian and Leisch, Friedrich and Steinley, Douglas and Warrens, Matthijs J.},
title = {A white paper on good research practices in benchmarking: The case of cluster analysis},
journal = {WIREs Data Mining and Knowledge Discovery},
volume = {13},
number = {6},
pages = {e1511},
keywords = {conceptual underpinnings, foundational recommendations, method comparison},
doi = {https://doi.org/10.1002/widm.1511},
url = {https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/widm.1511},
eprint = {https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1511},
abstract = {Abstract To achieve scientific progress in terms of building a cumulative body of knowledge, careful attention to benchmarking is of the utmost importance, requiring that proposals of new methods are extensively and carefully compared with their best predecessors, and existing methods subjected to neutral comparison studies. Answers to benchmarking questions should be evidence-based, with the relevant evidence being collected through well-thought-out procedures, in reproducible and replicable ways. In the present paper, we review good research practices in benchmarking from the perspective of the area of cluster analysis. Discussion is given to the theoretical, conceptual underpinnings of benchmarking based on simulated and empirical data in this context. Subsequently, the practicalities of how to address benchmarking questions in clustering are dealt with, and foundational recommendations are made based on existing literature. This article is categorized under: Fundamental Concepts of Data and Knowledge > Data Concepts Fundamental Concepts of Data and Knowledge > Key Design Issues in Data Mining Technologies > Structure Discovery and Clustering},
year = {2023}
}

@inproceedings{GPT3Paper,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{GPT4Report,
      title={{GPT-4} {T}echnical {R}eport}, 
      author={OpenAI},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@article{AMIPaper,
  author  = {Nguyen Xuan Vinh and Julien Epps and James Bailey},
  title   = {Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance},
  journal = {Journal of Machine Learning Research},
  year    = {2010},
  volume  = {11},
  number  = {95},
  pages   = {2837--2854},
  url     = {http://jmlr.org/papers/v11/vinh10a.html}
}

@article{oclus,
  title={{OCLUS}: An Analytic Method for Generating Clusters with Known Overlap},
  author={Douglas L. Steinley and Robert Henson},
  journal={Journal of Classification},
  year={2005},
  volume={22},
  pages={221-250},
  url={https://api.semanticscholar.org/CorpusID:21871230}
}

@article{TSNEPaper,
  author  = {Laurens van der Maaten and Geoffrey Hinton},
  title   = {Visualizing Data using {t-SNE}},
  journal = {Journal of Machine Learning Research},
  year    = {2008},
  volume  = {9},
  number  = {86},
  pages   = {2579--2605},
  url     = {http://jmlr.org/papers/v9/vandermaaten08a.html}
}

@book{BlumHD,
  title={Foundations of Data Science},
  author={Blum, A. and Hopcroft, J. and Kannan, R.},
  isbn={9781108485067},
  lccn={2019038133},
  url={https://books.google.com/books?id=koHCDwAAQBAJ},
  year={2020},
  publisher={Cambridge University Press}
}

@article{hdbscan, 
    doi = {10.21105/joss.00205}, 
    url = {https://doi.org/10.21105/joss.00205}, 
    year = {2017}, 
    publisher = {The Open Journal}, 
    volume = {2}, 
    number = {11}, 
    pages = {205}, 
    author = {Leland McInnes and John Healy and Steve Astels}, 
    title = {{HDBSCAN}: Hierarchical density based clustering}, 
    journal = {Journal of Open Source Software} 
}

@inproceedings{dbscanrevisited,
author = {Gan, Junhao and Tao, Yufei},
title = {{DBSCAN} {R}evisited: {M}is-{C}laim, {U}n-{F}ixability, and {A}pproximation},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2737792},
doi = {10.1145/2723372.2737792},
abstract = {DBSCAN is a popular method for clustering multi-dimensional objects. Just as notable as the method's vast success is the research community's quest for its efficient computation. The original KDD'96 paper claimed an algorithm with O(n log n) running time, where n is the number of objects. Unfortunately, this is a mis-claim; and that algorithm actually requires O(n2) time. There has been a fix in 2D space, where a genuine O(n log n)-time algorithm has been found. Looking for a fix for dimensionality d ≥ 3 is currently an important open problem.In this paper, we prove that for d ≥ 3, the DBSCAN problem requires Ω(n4/3) time to solve, unless very significant breakthroughs---ones widely believed to be impossible---could be made in theoretical computer science. This (i) explains why the community's search for fixing the aforementioned mis-claim has been futile for d ≥ 3, and (ii) indicates (sadly) that all DBSCAN algorithms must be intolerably slow even on moderately large n in practice. Surprisingly, we show that the running time can be dramatically brought down to O(n) in expectation regardless of the dimensionality d, as soon as slight inaccuracy in the clustering results is permitted. We formalize our findings into the new notion of ρ-approximate DBSCAN, which we believe should replace DBSCAN on big data due to the latter's computational intractability.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {519–530},
numpages = {12},
keywords = {algorithm, dbscan, density-based clustering},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}

@article{dbscanrevisitedrevisited,
author = {Schubert, Erich and Sander, J\"{o}rg and Ester, Martin and Kriegel, Hans Peter and Xu, Xiaowei},
title = {{DBSCAN} {R}evisited, {R}evisited: {W}hy and {H}ow You {S}hould ({S}till) Use {DBSCAN}},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {3},
issn = {0362-5915},
url = {https://doi.org/10.1145/3068335},
doi = {10.1145/3068335},
abstract = {At SIGMOD 2015, an article was presented with the title “DBSCAN Revisited: Mis-Claim, Un-Fixability, and Approximation” that won the conference’s best paper award. In this technical correspondence, we want to point out some inaccuracies in the way DBSCAN was represented, and why the criticism should have been directed at the assumption about the performance of spatial index structures such as R-trees and not at an algorithm that can use such indexes. We will also discuss the relationship of DBSCAN performance and the indexability of the dataset, and discuss some heuristics for choosing appropriate DBSCAN parameters. Some indicators of bad parameters will be proposed to help guide future users of this algorithm in choosing parameters such as to obtain both meaningful results and good performance. In new experiments, we show that the new SIGMOD 2015 methods do not appear to offer practical benefits if the DBSCAN parameters are well chosen and thus they are primarily of theoretical interest. In conclusion, the original DBSCAN algorithm with effective indexes and reasonably chosen parameter values performs competitively compared to the method proposed by Gan and Tao.},
journal = {ACM Trans. Database Syst.},
month = {jul},
articleno = {19},
numpages = {21},
keywords = {range-search complexity, density-based clustering, DBSCAN}
}

@article{dbscanheuristics,
  author    = {J{\"o}rg Sander and Martin Ester and Hans-Peter Kriegel and Xiaowei Xu},
  title     = {Density-Based Clustering in Spatial Databases: The Algorithm {GDBSCAN} and Its Applications},
  journal   = {Data Mining and Knowledge Discovery},
  year      = {1998},
  volume    = {2},
  number    = {2},
  pages     = {169--194},
  doi       = {10.1023/A:1009745219419},
  url       = {https://doi.org/10.1023/A:1009745219419},
  issn      = {1573-756X}
}

@article{ARIPaper,
  author    = {Lawrence Hubert and Phipps Arabie},
  title     = {Comparing partitions},
  journal   = {Journal of Classification},
  year      = {1985},
  volume    = {2},
  number    = {1},
  pages     = {193--218},
  doi       = {10.1007/BF01908075},
  url       = {https://doi.org/10.1007/BF01908075},
  issn      = {1432-1343}
}

@inproceedings{Milligan1996,
    author = {Glenn W. Milligan},
    title = {Clustering Validation: Results and Implications for Applied Analyses},
    year = {1996},
    booktitle = {Clustering and Classification},
    chapter = {},
    pages = {341-375},
    doi = {10.1142/9789812832153_0010},
    URL = {https://www.worldscientific.com/doi/abs/10.1142/9789812832153_0010},
    eprint = {https://www.worldscientific.com/doi/pdf/10.1142/9789812832153_0010},
    abstract = { Abstract The following sections are included: Introduction Overview of the Steps in a Cluster Analysis Basis for Recommendations Fundamental Clustering Problems The Heuristic Approach Strategies for Evaluating Clustering Procedures The Monte Carlo Process Validation Results and Recommendations Selection of Entities to Cluster Selection of Variables to Cluster Decisions Concerning Variable Standardization Selection of a Similarity or Dissimilarity Measure Selection of a Clustering Method Determining the Number of Clusters Interpretation, Testing, and Replication Conclusion References }
}

@article{gapstat,
author = {Tibshirani, Robert and Walther, Guenther and Hastie, Trevor},
title = {Estimating the number of clusters in a data set via the gap statistic},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
volume = {63},
number = {2},
pages = {411-423},
keywords = {Clustering, Groups, Hierarchy, Uniform distribution},
doi = {https://doi.org/10.1111/1467-9868.00293},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/1467-9868.00293},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9868.00293},
abstract = {We propose a method (the ‘gap statistic’) for estimating the number of clusters (groups) in a set of data. The technique uses the output of any clustering algorithm (e.g. K-means or hierarchical), comparing the change in within-cluster dispersion with that expected under an appropriate reference null distribution. Some theory is developed for the proposal and a simulation study shows that the gap statistic usually outperforms other methods that have been proposed in the literature.},
year = {2001}
}

@article{compareOCLUSandQJ,
  title     = {Empirical evaluation of OCLUS and GenRandomClust algorithms of generating cluster structures},
  author    = {Korzeniewski, Jerzy},
  journal   = {Statistics in Transition new series},
  volume    = {14},
  number    = {3},
  pages     = {487--494},
  year      = {2013},
  doi       = {10.59170/stattrans-2013-030},
  url       = {https://doi.org/10.59170/stattrans-2013-030},
  note      = {Published online: 2 September 2013}
}

@article{milligan1985,
  title     = {An examination of procedures for determining the number of clusters in a data set},
  author    = {Milligan, Glenn W. and Cooper, Martha C.},
  journal   = {Psychometrika},
  volume    = {50},
  number    = {2},
  pages     = {159--179},
  year      = {1985},
  doi       = {10.1007/BF02294245},
  url       = {https://doi.org/10.1007/BF02294245}
}

@InProceedings{HighDim,
author="Aggarwal, Charu C.
and Hinneburg, Alexander
and Keim, Daniel A.",
editor="Van den Bussche, Jan
and Vianu, Victor",
title="On the Surprising Behavior of Distance Metrics in High Dimensional Space",
booktitle="Database Theory --- ICDT 2001",
year="2001",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="420--434",
abstract="In recent years, the effect of the curse of high dimensionality has been studied in great detail on several problems such as clustering, nearest neighbor search, and indexing. In high dimensional space the data becomes sparse, and traditional indexing and algorithmic techniques fail from a effciency and/or effectiveness perspective. Recent research results show that in high dimensional space, the concept of proximity, distance or nearest neighbor may not even be qualitatively meaningful. In this paper, we view the dimensionality curse from the point of view of the distance metrics which are used to measure the similarity between objects. We specifically examine the behavior of the commonly used Lknorm and show that the problem of meaningfulness in high dimensionality is sensitive to the value of k. For example, this means that the Manhattan distance metric L(1norm) is consistently more preferable than the Euclidean distance metric L(2norm) for high dimensional data mining applications. Using the intuition derived from our analysis, we introduce and examine a natural extension of the Lknorm to fractional distance metrics. We show that the fractional distance metric provides more meaningful results both from the theoretical and empirical perspective. The results show that fractional distance metrics can significantly improve the effectiveness of standard clustering algorithms such as the k-means algorithm.",
isbn="978-3-540-44503-6"
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@inproceedings{SpectralNg,
 author = {Ng, Andrew and Jordan, Michael and Weiss, Yair},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 pages = {},
 publisher = {MIT Press},
 title = {On Spectral Clustering: Analysis and an algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/file/801272ee79cfde7fa5960571fee36b9b-Paper.pdf},
 volume = {14},
 year = {2001}
}

@article{SpectralShiMalik,
  title={Normalized cuts and image segmentation},
  author={Jianbo Shi and Jitendra Malik},
  journal={Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  year={1997},
  pages={731-737},
  url={https://api.semanticscholar.org/CorpusID:14848918}
}

@article{em,
author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
title = {Maximum Likelihood from Incomplete Data Via the {EM} Algorithm},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
volume = {39},
number = {1},
pages = {1-22},
keywords = {maximum likelihood, incomplete data, em algorithm, posterior mode},
doi = {https://doi.org/10.1111/j.2517-6161.1977.tb01600.x},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1977.tb01600.x},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1977.tb01600.x},
abstract = {Summary A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
year = {1977}
}

@Article{DirectionalCoClustering,
  author={Aghiles Salah and Mohamed Nadif},
  title={{Directional co-clustering}},
  journal={Advances in Data Analysis and Classification},
  year=2019,
  volume={13},
  number={3},
  pages={591-620},
  month={September},
  keywords={Co-clustering; Directional data; von Mises-Fisher distribution; EM algorithm; Document clustering},
  doi={10.1007/s11634-018-0323-4},
  abstract={ Co-clustering addresses the problem of simultaneous clustering of both dimensions of a data matrix. When dealing with high dimensional sparse data, co-clustering turns out to be more beneficial than one-sided clustering even if one is interested in clustering along one dimension only. Aside from being high dimensional and sparse, some datasets, such as document-term matrices, exhibit directional characteristics, and the $$L_2$$ L 2 normalization of such data, so that it lies on the surface of a unit hypersphere, is useful. Popular co-clustering assumptions such as Gaussian or Multinomial are inadequate for this type of data. In this paper, we extend the scope of co-clustering to directional data. We present Diagonal Block Mixture of Von Mises–Fisher distributions (dbmovMFs), a co-clustering model which is well suited for directional data lying on a unit hypersphere. By setting the estimate of the model parameters under the maximum likelihood (ML) and classification ML approaches, we develop a class of EM algorithms for estimating dbmovMFs from data. Extensive experiments, on several real-world datasets, confirm the advantage of our approach and demonstrate the effectiveness of our algorithms.},
  url={https://ideas.repec.org/a/spr/advdac/v13y2019i3d10.1007_s11634-018-0323-4.html}
}

@article{ClusteringVMF,
  author  = {Arindam Banerjee and Inderjit S. Dhillon and Joydeep Ghosh and Suvrit Sra},
  title   = {Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions},
  journal = {Journal of Machine Learning Research},
  year    = {2005},
  volume  = {6},
  number  = {46},
  pages   = {1345--1382},
  url     = {http://jmlr.org/papers/v6/banerjee05a.html}
}

@misc{openai_api,
  author       = {OpenAI},
  title        = {{OpenAI} {API}},
  howpublished = {\url{https://platform.openai.com/}},
  year         = {2023},
  note         = {Accessed: 2024-09-18}
}

@inproceedings{subclugen,
author = {Beer, Anna and Schüler, Nadine-Sarah and Seidl, Thomas},
year = {2019},
month = {01},
pages = {},
title = {A Generator for Subspace Clusters}
}

@inproceedings{weighttying,
  author       = {Hakan Inan and
                  Khashayar Khosravi and
                  Richard Socher},
  title        = {Tying Word Vectors and Word Classifiers: {A} Loss Framework for Language
                  Modeling},
  booktitle    = {5th International Conference on Learning Representations, {ICLR} 2017,
                  Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher    = {OpenReview.net},
  year         = {2017},
  url          = {https://openreview.net/forum?id=r1aPbsFle},
  timestamp    = {Thu, 25 Jul 2019 14:26:02 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/InanKS17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@Inbook{lecunbackprop,
author="LeCun, Yann A.
and Bottou, L{\'e}on
and Orr, Genevieve B.
and M{\"u}ller, Klaus-Robert",
editor="Montavon, Gr{\'e}goire
and Orr, Genevi{\`e}ve B.
and M{\"u}ller, Klaus-Robert",
title="Efficient BackProp",
bookTitle="Neural Networks: Tricks of the Trade: Second Edition",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="9--48",
abstract="The convergence of back-propagation learning is analyzed so as to explain common phenomenon observed by practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposed in serious technical publications. This paper gives some of those tricks, and offers explanations of why they work.",
isbn="978-3-642-35289-8",
doi="10.1007/978-3-642-35289-8_3",
url="https://doi.org/10.1007/978-3-642-35289-8_3"
}

@article{layernorm,
  title={Layer Normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}







