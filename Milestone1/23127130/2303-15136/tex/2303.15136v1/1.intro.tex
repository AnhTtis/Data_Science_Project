\section{Introduction}\label{sec:intro}
High energy nuclear physics and machine learning, though being seemingly disparate, their interplay has already begun to emerge and been yielding promising results over the past decade. This review aims to introduce to the community the current status and report an overview of applying machine learning for high energy nuclear physics studies. From different aspects, we will present how scientific questions involved in this field can be tackled or assisted with the state-of-the-art techniques.

\subsection{Background}\label{subsec:bg}

The study of nuclear physics focuses on comprehending the nature of nuclear matter, its properties under various conditions, its building blocks, and the fundamental interactions that govern them. The behavior of nuclear matter is fundamentally governed by the strong interaction, described by quantum chromodynamics (QCD)~\cite{Gross:2022hyw}.
While nucleons serve as the main degrees of freedom for traditional low energy nuclear physics, high energy nuclear physics (HENP) cares more about QCD matter in extreme conditions, where the basic degrees of freedom are typically quarks and gluons.

A primary goal of HENP is to understand how nuclear matter behaves under extreme conditions, which remains an unresolved area of study~\cite{Yagi:2005yb, Wang:2016opj, Fukushima:2020yzx,Shuryak:2004pry}. Intuitively, when matter is subjected to extreme conditions, its basic constituents can be revealed. For example, when nuclear matter is heated to high temperatures, mesons and resonances are gradually excited out, and at some point causing hadrons or nucleons to \textit{overlap} (the same can happen when nuclear matter is compressed to high densities) thus their constituent quarks and gluons can move freely inside a larger area. The induced state of matter, known as quark-gluon plasma (QGP)~\cite{Rafelski:2019twp}, is a deconfined state in which quarks and gluons are the basic degrees of freedom and strongly interacting. The existence of such primordial state of matter is believed to have occurred in the early universe after the Big-Bang, and may also exist in the dense interior of neutron stars~\cite{Luo:2022mtp}.
The first-principle lattice QCD simulations predict that normal nuclear matter in the form of a dilute hadronic gas can undergo a crossover transition to the strongly-interacting, deconfined state of QGP at high temperature and low baryon chemical potentials, where chiral symmetry is also restored~\cite{Ding:2015ona, Karsch:2022opd, Aarts:2023vsf}. 

However, the situation at high baryon chemical potential is not well understood due to the fermionic sign problem that impedes direct lattice QCD simulations~\cite{Nagata:2021ugx}. The structures of the QCD phase diagram and the properties of matter in these regions have to be studied through effective model calculations and experimental searches. The formation and study of hot and dense QGP can be accomplished through relativistic heavy ion collision (HIC) experiments~\cite{Busza:2018rrf}, which act as a heating and compression machine to QCD matter to reach extreme conditions~\cite{Bzdak:2019pkr}. However, the collision of heavy nuclei creates a rapidly evolving and dynamic process with many entangled and not-fully understood physics factors~\cite{Luo:2022mtp}. Decoding the physics properties related to the early-formed QGP from the final measurements of heavy ion collisions therefore remains a challenging task.
In general, both theoretical calculations and experimental facilities are crucial for advancing the field of HENP, which is anyhow becoming increasingly intricate and producing ever-larger amounts of data from sources such as detectors in experiments e.g., Relativistic Heavy Ion Collider (RHIC) at BNL and Large Hadron Collider (LHC) at CERN, etc., and model simulations such as relativistic hydrodynamics, transport cascades, and lattice QCD simulations. Nevertheless, performing the involved calculations, analyzing the involved data, and decode underlying physics from confronting theory with data,i  are routinely demanding due to factors such as high-dimensionality, background contamination, and potentially entangled physical influences. 

Artificial Intelligence (AI) has grown rapidly in popularity with the goal of imparting intelligence to machines. The field of computer science, dating back to the 1960s, has experienced a revival in recent years with the development of modern learning algorithms, advanced computing power, and the vast amount of available data~\cite{Hogg:1987nn,AIreview:2021}. Machine learning (ML), an essential part of AI, is viewed as a modern computational paradigm that gives machines the ability to learn without being explicitly programmed. It enables computers to go beyond being instructed by humans and learn how to perform specific tasks on their own. This novel paradigm, particularly deep learning (DL)~\cite{lecun2015deep}, a subfield of ML that uses deep neural networks for hierarchical representation, has shown successful applications~\cite{sarker2021deep} in computer vision(CV), natural language processing(NLP), game AI, and drug discovery. It also demonstrates great promise in transforming scientific research~\cite{osti_1604756}. It allows for automatically recognizing patterns hidden in large, complex datasets, and has proven to be particularly effective in handling intricate structures and non-linear correlations that are beyond conventional analysis~\cite{osti_1604756}. 

Therefore, a variety of scientific tasks can be aided from these new computational methodologies, such as extracting knowledge from measurements or calculations, improving simulations, detecting anomalies or discovering new phenomena. 
The deployment of machine learning in various scientific disciplines has become progressively ubiquitous.
It is believed that machine learning has the potential to greatly enhance physics research as well (see e.g., ~\cite{Mehta:2018dln} for pedagogical introduction). With its flexibility in tackling general computational physics tasks, machine learning is actively being explored to support physics studies~\cite{Carleo:2019ptp}. Conversely, many concepts and foundations in machine learning can be traced back to physics, such as the Boltzmann machine and variational generative algorithms, which have close ties to statistical physics~\cite{Mezard:2009ipa}. The synergy between these fields will not only benefit physics, but also deepen our understanding of the underlying working mechanisms of machine learning~\cite{Bahri:2020smd} or even intelligence.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp!]
    \centering
    \includegraphics[width = 0.85\textwidth]{figures/fig_1-1-1_ML4HENP.pdf}
    \caption{Schematic QCD phase diagram and the applications of machine learning in three domains, i.e., Lattice QCD, Heavy-Ion Collisions and Neutron Stars.
    \label{fig:ML4HENP}}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

High energy nuclear physics can greatly benefit from machine learning techniques due to its computational and data-intensive nature. In recent years, the intersection of machine learning and HENP has shown great potential and unconventional possibilities~\cite{Bedaque:2021bja,Boehnlein:2021eym}. HENP has long been at the forefront of big data analysis, using techniques such as neural networks and statistical learning~\cite{Humpert:1990rw,Gyulassy:1990et,Bass:1993vx,Bass:1996ez}, but recent advancements in deep learning have brought new developments to the crossing of these two fields. Deep learning has improved the ability to handle large amounts of high-dimensional data, extended the feasibility of going beyond human-crafted heuristics, and generated new possibilities such as fast and effective simulation, sensitive observable design, and new physics detection (see a living review~\cite{Feickert:2021ajf} and a collection of datasets in high energy physics~\cite{Benato:2021olt}). This growing and influential research area deserves dedicated recognition and further study to release the full potential of such novel computational paradigms for high energy nuclear physics.


The use of machine learning in high energy nuclear physics encompasses a wide range of topics and tasks, from experimental data analysis with normal discriminative algorithms, to speeding up various simulations with generative models. We will review them systemically from three sectors, as shown in Fig.~\ref{fig:ML4HENP}, i.e., heavy-ion collisions in Sec.~\ref{sec:hic}, lattice QCD in Sec.~\ref{sec:lat} and neutron stars in Sec.~\ref{sec:astro}. Besides, the involved studies are quite often with multi-scale and high-dimensional computations, and requiring customized ML methods that address the specific physics constraints and characteristics, e.g., physical differential equations, corresponding symmetries and conservation laws. This leads to the development of unique techniques for science automation and physics exploration with ML paradigm. A refined summary of some new and advanced developments and their applications in high energy nuclear physics will be discussed in Sec.~\ref{sec:new}. This review, although unavoidably being limited and with bias in terms of content selection, aims to offer a practical introduction to readers interested in exploring the rapidly developing field of applying machine learning into high energy nuclear physics. A general overview of the ongoing activities in this field will be covered. In the remainder of this section, we will briefly introduce the basics of machine learning for those unfamiliar with the terminology. These information and related techniques can be revisited when delving into the subsequent physics research discussions.


\subsection{Machine Learning in a Nutshell}\label{subsec:ML}
Before physics parts, we would like to provide a comprehensive but concise introduction to the basic concepts of machine learning, with a focus on deep learning and its current developments. The concepts and techniques touched here will be revisited later in this review. Although it can not cover all aspects of machine learning~\footnote{The missing parts include but are not limited to dimensionality reduction, clustering, linear/Logistic regressions, (support vector machines)SVMs, and ensemble learning.}, the main objective is to introduce the advanced deep learning approaches. For a deeper understanding of machine learning, we recommend reading books~\cite{Bishop2006,Goodfellow2016}, or recent reviews for its practical applications in physics~\cite{Mehta:2018dln,Boehnlein:2021eym,Dawid:2022fga,Shanahan:2022ifi}.

\subsubsection{Bayesian Inference}\label{subsubsec:bi}
From a probabilistic perspective, one can infer the probability distribution of random variables (A) from limited trials/observations over its consequences (B), based on the Bayes' theorem~\cite{Murphy2012},
\begin{equation}
    P(A \mid B)=\frac{P(B \mid A) P(A)}{P(B)}, 
    \label{eq:bayesian}
\end{equation}
where in scientific applications, $A$ often represents parameters of theoretical models and $B$ refers to experimental observations. The Bayes formula is used to update or revise our belief/knowledge of a theoretical model in terms of $A$ using new experimental observations $B$. It involves the calculation of the posterior probability distribution $P(A \mid B)$, which takes into account the prior probability distribution $P(A)$ and the conditional probability $P(B \mid A)$, known as the likelihood. Uncertainties in the experimental observations $B$ will impact the posterior distribution of $A$ via likelihood, causing it to become wider. The posterior distribution $P(A \mid B)$ is a probability density function over A, which can be characterized by its mean, peak value, and variance. One way to determine the most likely value of $A$ is by finding the group of parameter combinations $A=a$ that maximizes the posterior distribution $P(A \mid B)$, known as the maximum a posteriori estimation (MAP). This process allows us to locate the peak of the posterior distribution.

The variance of posterior $P(A\mid B)$ can be used to determine the uncertainty or confidence level of the theoretical model. If $P(A\mid B)$ is a narrow distribution with a sharp peak, its variance is small, indicating that the data are decisive and useful in constraining the model with small uncertainty. The marginal distribution $P(B)=\sum_a P(B \mid A=a) P(A=a)$ is called evidence, which provides a normalization factor by traversing the space of the theoretical model. For an n-dimensional parameter space $A=\left(A_1, A_2, \cdots, A_n \right)$, if the random variable has $m$ possible choices in each dimension, one must repeat the theoretical calculations $m^n$ times to get the normalization factor $P(B)$. This procedure encounters the challenge of \textit{curse of dimensionality} for high-dimensional parameter spaces. Fortunately, $P(B)$ is not needed using Markov Chain Monte Carlo (MCMC) methods, which work for an unnormalized posterior distribution as follows,
\begin{equation}
    P(A \mid B) \propto P(B \mid A) P(A), 
    \label{eq:bayesian_unnorm}
\end{equation}
The way MCMC methods work for Eq.~\eqref{eq:bayesian_unnorm} is by importance sampling using random walk~\cite{garnett_bayesoptbook_2023}. Starting from any position $A = a_0$ in the parameter space, walk to the next position using a random step size $\epsilon$ sampled from a uniform or normal distribution, following the Markov Principle that each update only depends on its current position, $a_{n+1} = a_{n} + \epsilon$. The trajectory of one random walk in the parameter space form a set $\{ a_n \}$ for $n=0, 1, 2\cdots$ whose density estimation produces a probability distribution. This probability distribution does not equal to $P(A|B)$ without any constraints to the process of random walk. The MCMC algorithms set an acceptance-rejection probability to each update such that $A=a$ values in the parameter space are visited more frequently with larger $P(A=a|B)$ than smaller ones to form a stationary distribution that is close to $P(A|B)$. E.g., in Metropolis-Hastings algorithm which is one kind of MCMC algorithm, the acceptance-rejection ratio is set to be ${\rm min}(1, \alpha)$ where,
\begin{align}
\alpha = \frac{P(A=a_{n+1}|B) q(a_{n} | a_{n+1})}{P(A=a_{n}|B) q(a_{n+1-} | a_{n})} =  \frac{P(A=a_{n+1}|B)}{P(A=a_{n}|B)}
\label{eq:rejection_rate}
\end{align}
where $q(a_{n+1} | a_{n})$ is called proposal function that can be a uniform distribution or a normal distribution centered at $a_n$. The proposal function is symmetric in Metropolis-Hastings algorithm, which can be eliminated in Eq.~\eqref{eq:rejection_rate}.
This is got from the detailed balance condition with $P(A=a_{n+1}|B) q(a_{n} | a_{n+1}) = \alpha P(A=a_{n}|B) q(a_{n+1} | a_{n})$. In practice, one can sample $a_{n+1}$ from a normal distribution $\mathcal{N}(a_n, \sigma)$ centered at $a_n$ with standard deviation $\sigma$. The parameter $\sigma$ can be adjusted adaptively for efficient sampling.

\textit{\textbf{Gaussian Processes}}--- Gaussian process(GP) is a probabilistic model that can be used for regression or classification~\cite{garnett_bayesoptbook_2023,NIPS1995_7cce53cf}.Formally, as a type of stochastic process, a GP dictates a probability distribution over functions, $f(\bm{x})\sim \mathcal{GP}(m(\bm{x}),k(\bm{x},\bm{x}'))$, which is fully specified by a mean function $m(\bm{x})$ (usually chosen to be zero) and a covariance function $k(\bm{x},\bm{x}')$ (measures the similarity between two input values). The GP is defined as a collection of random variables (e.g., the evaluations of its sample function at different inputs $\{f(\bm{x}_i)\}$), with its any finite set following a joint Gaussian (i.e., multivariate normal) distribution, $f(\bm{x}_1),...,f(\bm{x}_N)\sim\mathcal{N}(\bm{\mu},\bm{K})$ where $\bm{\mu}_i=m(\bm{x}_i)$ and $\bm{K}_{ij}=k(\bm{x}_i,\bm{x}_j)$. 

For a regression task one can use GP to construct a non-parametric prior of the to be fitted function. Per observing a training set $\bm{X}=\{\bm{x}_i\}^N_{i=1}$ with its corresponding targets $Y=\{y_i\}^N_{i=1}$, one can update the prior by conditioning on the observations, which essentially restrict the functions from the GP to be in line with the training data. Specifically, one cares the prediction $Y^{*}$ over new inputs $\bm{X}^{*}$, which together with the training targets $Y$ have the joint prior distribution, 
$$
\begin{bmatrix}
 Y\\
Y^{*}
\end{bmatrix} \sim \mathcal{N}(\bm{0},
\begin{bmatrix}
      \bm{K}(\bm{X},\bm{X}) & \bm{K}(\bm{X}, \bm{X}^{*}) \\
      \bm{K}(\bm{X}^{*},\bm{X}) & \bm{K}(\bm{X}^{*},\bm{X}^{*})
\end{bmatrix} ),
$$
and the conditioning of this joint Gaussian prior on the training data results in the posterior for the prediction,
$$\mathrm{p}(Y^{*}|\bm{X}^{*},\bm{X},Y)=\mathcal{N}(\bar{Y}^{*},\mathrm{cov}(Y^{*})),$$ with $\bar{Y}^{*}=\bm{K}(\bm{X}^{*},\bm{X})\bm{K}(\bm{X},\bm{X})^{-1}Y$ and $\mathrm{cov}(Y^{*})=\bm{K}(\bm{X}^{*},\bm{X}^{*})-\bm{K}(\bm{X}^{*},\bm{X})\bm{K}(\bm{X},\bm{X})^{-1}\bm{K}(\bm{X},\bm{X}^{*})$.

In practice, choosing the appropriate kernel function $K$ for a GP is an important step in the modeling process. It determines the shape of the covariance function and thus the overall structure of the model. The choice of kernel function depends on the specific problem at hand, as well as any prior knowledge or assumptions about the structure of the underlying function. Some common kernel functions contain: \textit{linear kernel}, $ k(x, x') = x^T x',$ where $x$ and $x'$ are vectors representing the input values; \textit{polynomial kernel}, $ k(x, x') = (x^T x' + c)^d$, where $c$ is a constant, and $d$ is the degree of the polynomial; \textit{squared exponential (SE)} or \textit{Radial Basis Function (RBF)} kernel~\cite{Genton:2002ker}. This is a popular choice for regression problems, as it can capture complex, non-linear relationships between inputs and outputs. The RBF kernel is defined as, 
$$ k(x, x') = \sigma_f^2 \exp\left(-\frac{1}{2l^2}(x - x')^2\right),$$
where $\sigma_f^2$ is the variance and $l$ is the length scale, which determines the smoothness of the function. Besides, there are many other choices such as \textit{Mat\'ern kernel}~\cite{Genton:2002ker} kernel for capturing a variety of functional shapes. Many other examples can be found in the book~\cite{Genton:2002ker}.  Once a kernel has been chosen, the parameters of the kernel (such as $\sigma_f^2$, $l$, and $d$ in the above examples) can be estimated from the data using a variety of optimization methods.



\subsubsection{Deep Learning}\label{subsubsec:dl}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp!]
    \centering
    \includegraphics[width = 0.72\textwidth]{figures/fig_1-2-1_NNs.pdf}
    \caption{A demonstration of the common neural network structures that will be covered in this review. The Deep Neural Networks (DNNs), AutoEncoders (AEs) with "bottleneck latent" layers, Convolutional Neural Networks (CNNs) utilizing convolutional operations, and Graph Neural Networks (GNNs) capable of processing graph-type inputs.
    \label{fig:NNs}}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Deep learning, as a modern branch of machine learning~\cite{lecun2015deep,schmidhuber2015deep} usually involving deep neural network usage, provides tools for automatically extracting patterns from the currently available massive multi-fidelity observation data. The \textbf{enormous data} makes it possible to establish a reliable mapping between input features and desired targets with deep models. \textbf{Deep models} are usually constructed using artificial neural networks(ANNs) which are originally designed to imitate functions of human brain~\cite{mcculloch1943logical}, further proved having ability to represent arbitrary complicated continuous function~\cite{cybenko1989approximation,hornik1991approximation}. In an over-simplified picture, deep neural networks(DNNs) are a type of compound function which has multilayer nesting structures: $f_\theta(x) = z^l(\cdots z^2(z^1(x))), z^i(z^{i-1}) = \sigma(\omega^i z^{i-1} + b^i)$, where $x$ is input data defined in a set of samples $\mathbb{X}$, $l$ labels the number of layers and $i$ is the corresponding index, and $z^0 \equiv x $. $\sigma(\cdot)$ is a non-linear activation function and $\{\omega,b\}$ are weights and bias respectively. Weights and bias are all trainable parameters, they could be abbreviated as $\{\theta\}$. Some common neural network structures are shown in Figure.~\ref{fig:NNs}. They will be mentioned tirelessly in the remainder of this review.


Given a well-definite \textbf{Loss function} $\mathcal{L}(f_\theta(x))$, these trainable parameters can be tuned in an optimization problem,
\begin{equation}
    \mathbf{\theta} = \mathop{argmin}\limits_{\theta}\mathcal{L}(f_\theta(\mathbb{X}))=\mathop{argmin}\limits_{\theta}\sum_j^m\mathcal{L}(f_\theta(x^{(j)})),
    \label{eq:1:op}
\end{equation}
where $j$ labels the index of a sample. If one can collect corresponding labels $y\in \mathbb{Y}$ for input $x$, the optimization process becomes a \textit{Supervised Learning} task. Common Loss functions contain: Mean Squared Error(MSE), Mean Absolute Error(MAE) and Cross Entropy, etc.


In fact, one can understand the above optimization from a \textit{probabilistic perspective}.
\begin{equation}
    \mathbf{\theta} = \mathop{argmax}\limits_{\theta}p_\theta(\mathbb{X})=\mathop{argmax}\limits_{\theta}\prod_j^m p_\theta(x^{(j)}),
\end{equation}
which is the \textit{maximum likelihood estimation(MLE)} for parameters $\{\theta\}$ given the data-set $\mathbb{X}$. From such a perspective, training a deep model becomes approaching the conditional probability distribution $p_\text{data}(y|x)$. Moreover, it can naturally extend to the \textit{Unsupervised Learning} task, which is approaching the underlying distribution $p_\text{data}(x)$ of data without labels. A useful trick is to maximize the log-likelihood, $\mathop{log}(p_\theta)$, converting the product into a summation. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[!hbpt]
\centering
\begin{tabular}{l |l | l }
\hline\hline
    Metrics & Formula & Probabilistic Description \\
\hline
MSE & 
    $(f_\theta(x) - y)^2$&
    Gaussian\\
MAE & 
    $|f_\theta(x) - y|$&
    Laplace\\
Binary Cross-Entropy & 
    $-\left[ y \mathop{log} f_\theta(x) + (1 - y) \mathop{log} (1-f_\theta(x)) \right]$ &
    Bernoulli\\
\hline\hline
\end{tabular}
\caption{Common Loss functions and their probabilistic descriptions.}
\label{tab:metrics}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Before introducing the optimization algorithm, one should be aware of the \textit{generalization ability} for deep models. Intuitively speaking, when training data is not enough to represent the underlying distribution $p_\text{data}(x)$ or the deep model is over-represented in the training data set, one will observe that metrics perform better in training than the testing data-set. That is \textit{over-fitting}. On the contrary, it is called \textit{under-fitting}. \textit{Under-fitting} can be easily overcame by increasing complexity of deep models. However, \textit{over-fitting} is thorny because it's from a \textbf{trade-off} between model flexibility and accuracy. Adding priors to the parameters is a conventional and efficient strategy to avoid the possible \textit{over-fitting}. From a Bayesian perspective, it makes the MLE to the \textit{maximum a posterior (MAP)} estimation,
\begin{equation}
    \mathbf{\theta}_\text{MAP} = \mathop{argmax}\limits_{\theta}p_\theta(\mathbb{X}|\theta)=\mathop{argmax}\limits_{\theta}\mathop{log}p(\mathbb{X}|\theta)+\mathop{log}p(\theta),
    \label{eq:1:op-map}
\end{equation}
where the last term $\mathop{log}p(\theta)$ is the prior distribution of parameters. The Gaussian and Laplace priors correspond to $L_2$ and $L_1$ regularization, respectively~\cite{Goodfellow2016}.

From an optimization perspective, once the loss function and model are given, one can choose many different methods to train the model. However, the \textbf{gradient-based optimizer} within the differentiable programming paradigm has been proven successful in deep models~\cite{Lecun1998,lecun2015deep,schmidhuber2015deep}. In fact, almost all current deep learning packages are adopting gradient-based methods, which are implemented in \textit{\textbf{automatic differentiation(AD)}} frameworks. AD is different from either the symbolic differentiation or the numerical differentiation~\cite{baydin2018automatic}. Its backbone is the chain rule, which can be programmed in a standard computation with the calculation of derivatives. Lots of AD libraries have been developed in the past several years, rendering easy access to AD usage for researchers, such as Tensorflow~\cite{AbaBar16Tensorflow}, PyTorch~\cite{NEURIPS2019_9015} and JAX~\cite{2019arXiv191204232S}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[ht!]
    \centering
    \includegraphics[width = 0.45\textwidth]{figures/fig_1-2-2_ADsample.pdf}
    \caption{Computational graph of a logistic least squares regression in a vanilla net. 
    \label{fig:adsample}}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 In a simplified example shown in Fig.~\ref{fig:adsample}, the computation of a logistic least squares regression consists of a series of differentiable operations. The forward mode is indicated by black arrows with derivatives. When calculating $\mathcal{L}$ from the input $x$, the corresponding derivatives can be evaluated by applying the chain rule simultaneously. With this intuitive example, one could understand the \textbf{back-propagation(BP) algorithm}~\cite{schmidhuber2015deep,Goodfellow2016} clearly. A generalized BP algorithm corresponds to the reverse mode of AD, which propagates derivatives backward from a given output. The adjoint is defined as $\bar{v}_i = \frac{\partial \mathcal{L}}{\partial v_i}$, which reflects how the output will change with respect to changes of intermediate variables $v_i$. The forward and reverse calculations can be demonstrated as follows,
\begin{align}
\begin{split}
    &z=w x+b,\quad y=\sigma(z),\quad \mathcal{L}=\frac{1}{2}(y-\hat{y})^{2},\\
    &\overline{\mathcal{L}}=1,\quad \bar{y}=y-\hat{y},\quad \bar{z}=\bar{y} \sigma^{\prime}(z),\quad \bar{w}=\bar{z} x,\quad \bar{b}=\bar{z}.
\end{split}
\end{align}
The first line is the forward process and the second line indicates the reverse mode. Given a target $\hat{y}$, one can define a proper loss function $\mathcal{L}(y,\hat{y})$ and fine-tune the parameters $\mathbf{\theta} = (\omega,b)$ with the gradient $\partial_{\mathbf{\theta}}\mathcal{L}$, which is the gradient-based optimization introduced before. The BP is crucial for training a DNN because the derivatives can be used to optimize a high dimensional parameter set also layer by layer. In most deep learning tasks, models are trained to approximate a $\mathbb{R}^N\rightarrow \mathbb{R}^1$ mapping. In such a case, the reverse mode of AD holds dominant advantages in the gradient-based optimization compared with the forward mode or the numerical differentiation, see more details in Ref~\cite{baydin2018automatic}.

In the optimization problem defined in \eqref{eq:1:op} or \eqref{eq:1:op-map}, a well-adopted \textbf{gradient-based optimizer} is the \textit{stochastic gradient descent(SGD)}~\cite{Lecun1998}. This algorithm updates parameters $\{\theta\}$ through subtracting the gradients derived from the BP algorithm,
\begin{equation}
    \mathbf{\theta}_{t+1} = \mathbf{\theta}_t - \eta\, \partial_{\mathbf{\theta}}\mathcal{L}_t,
\end{equation}
where $\eta$ is the \textit{learning rate} and $t$ indicates the $t$-th time repetition (\textit{epoch}) for the training data. In each epoch, the \textit{SGD} updates parameters on batches randomly selected from the training data set. It results in the stochastic updates of parameter $\mathbf{\theta}$  on loss function landscape. This stochasticity has been shown necessary for escaping possible local minima~\cite{Bottou1999}. Besides, we introduce a modified \textit{SGD} algorithm which is also widely used, i.e., \textit{Adam}~\cite{Adam2015}. It is designed to combine adaptive estimates of lower-order moments with the stochastic optimization for improving the stability.
As a practical example, the optimizer implemented in our following discussions can be expressed as,
\begin{align}
    \theta_{t+1} &=  \theta_{t} - \eta \frac{\hat{m}}{\sqrt{\hat{v}} + \xi},\\
    \hat{m} &\equiv m_{t+1} = \frac{\beta_1}{1-\beta_1} m_{t} + \partial_\theta\mathcal{L}_{t},\\
    \hat{v} &\equiv v_{t+1} = \frac{\beta_2}{1-\beta_2} v_{t} + (\partial_\theta\mathcal{L}_{t})^2,
\end{align}
where $\xi$ is a small enough scalar for preventing divergence and $\beta_1, \beta_2$ are the forgetting factors($0.9, 0.99$ in default setting) for momentum term $\hat{m}$ and its weight $\hat{v}$. 


Before entering the next section, it's helpful to introduce one specific architecture of neural networks, the AutoEncoder(AE)~\cite{tschannen2018recent} shown in Figure~\ref{fig:NNs}. It consists of an encoder and a decoder. The encoder maps the input data $x$ to a typically smaller dimensional variable in a latent space, $z=f_{\phi}(x)$, while the decoder tries to convert the latent variable $z$ back to the data, $\tilde{x}=g_{\theta}(z)$. The two components can be designed as two neural networks. The AE can learn a compressed representation of the input data that captures its most important features, while minimizing the dissimilarity between input and output, to which the mean-squared error $L(x,\tilde{x})=\Sigma_i(x_i-\tilde{x}_i)^2$ serves heuristically well as the reconstruction loss. This architecture can be used for manifesting the data in an interpretable latent space, which has been proven a successful paradigm in physics~\cite{Iten:2020dpc}. Similarly, principal component analysis (PCA) can also serve as a linear method for dimensionality reduction and feature extraction~\cite{ladjal2019pca}. PCA computes the covariance matrix $ C = X^T X  $ and its eigenvectors and eigenvalues, where  $X$ is the centered data matrix in a $d$-dimensional feature space. These eigenvectors are termed as the principal components. Sorting the principal components in descending order of eigenvalues, the first $k$ components with the largest eigenvalues are used to form the $k$-dimensional representation of a sample $x_i$ by projecting it onto the first $k$ principal components using $z_i = V_k^T (x_i - \mu)$. Here, $V_k$ is the matrix of the first $k$ eigenvectors of $C$. Later, many works introduced in this review involve applications of PCA, e.g., in Sections~\ref{outlier}, \ref{flow_hic}, and \ref{sec_phases_obs}.


\subsubsection{Generative Models}\label{subsubsec:gm}

In general, two typical tasks can be cast for categorizing deep learning algorithms: discriminative modeling and generative modeling. From probabilistic perspectives, discriminative modelling aim at capturing a conditional probability, $p(y|x)$, to be able to predict associated properties or class identities ($y$) of given object ($x$), while generative modelling target at learning and sampling from a joint probability (quite often lies in a high dimensional space), $p(x,y)$, to facilitate the generation of new data samples following the same desired statistics represented by the training data or specified distribution. There are several excellent tutorials~\cite{Mehta:2018dln, Wang2018GenerativeMF} tailored to physics research, which can be resorted to for details. In physics we often encounter such tasks like for statistical many-body system or lattice QFT simulation we use Markov Chain Monte Carlo (MCMC) to sample new configurations and further estimate the system's physical observables.

Currently, many generative models were developed from ML community with profound influence from and into physics. Basically, most (but not all) of the generative models at present are with the maximum likelihood principle as common guidance. Specifically, generative models can be viewed as parametric models, $p_{\theta}(x)$, to approach the probability distribution $p(x)$ of a system to which we may have or not have collected data. The Kullback-Leibler (KL) divergence (also called relative entropy), measuring the dissimilarity between the two distribution, can serve the objective for this task, 
\begin{equation}
    \mathcal{D}_{KL}(p(x)||p_{\theta}(x))=\int p(x)\log\frac{p(x)}{p_{\theta}(x)}dx, 
    \label{eq:KL_divergence}
\end{equation}
the minimization of which under
given observational data for the system, $\mathcal{D}=\{x\}$, is equivalent to minimization of the negative log-likelihood (NLL),
\begin{equation}
    \mathcal{L}=\frac{1}{|\mathcal{D}|}\sum_{x\in\mathcal{D}}\log p_{\theta}(x), 
    \label{eq:NLL}
\end{equation}
thus the maximum likelihood principle.

In the following, we will briefly introduce several of the popular generative models with deep neural networks involved, include variational autoencoder (VAE), generative adversarial networks (GAN), autoregressive model and normalizing flow (NF). 


\textbf{Variantional autoencoder (VAE)}~\cite{2013arXiv1312.6114K} added probabilistic spins into classic autoencoder, with which variational inference can be performed under latent variable framework to the data distribution learning. For classic autoencoder (AE), the basic idea is learning to reconstruct the input data going through a bottleneck structured pipeline, where the bottleneck hidden layer describes the \textit{code} to more efficiently represent the data also called latent variables usually with lower dimensional. The transformation from input data $x$ to the latent code $z$ is called encoder, and the left half in the network that reconstruct from the latent code to data manifold in output is called decoder or generator as Figure~\ref{fig:vae} shown.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[ht!]
    \centering
    \includegraphics[width = 0.55\textwidth]{figures/fig_1-2-4_VAE.pdf}
    \caption{A demonstration of VAE whose latent layers output the parameters of prior $p_0(z)$.
    \label{fig:vae}}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In VAE the latent variable per generation convenient is assumed to follow a simple distribution called prior $p_0(z)$ such as a multivariate Gaussian distribution whose parameters, i.e., means $\mathbf{\mu}$ and standard deviations $\mathbf{\sigma}$, are from outputs of the encoder. The decoder consequently provides a generative model through the trainable conditional probability $p_{\theta}(x|z)$. However, the introduction of latent variable makes the whole data generation distribution intractable since the potentially high dimensional integration, $p_{\theta}(x)=\int p_{\theta}(x|z)p_0(z)dz$. This thus also results in the intractable encoder probability or the posterior distribution $p(z|x)=p_{\theta}(x|z)p_0(z)/p_{\theta}(x)$. To facilitate the MLE or equivalently minimizing the NLL on the training set, VAE employs variational approach. Basically VAE introduced a parameterized model $q_{\phi}(z|x)$ (modelled by a neural network, termed as encoder) to approximate the posterior for latent variable $p(z|x)$, naturally the KL divergence between $q_{\phi}(z|x)$ and $p(z|x)$, $\mathcal{D}_{KL}(q_{\phi}(z|x)||p(z|x)$, can be invoked for the training objective which derives the variational lower bound (also called evidence lower bound, ELBO) for the likelihood as the cornerstone for VAE:
\begin{equation}
    \mathcal{L}= \mathbb{E}_{q_{\phi}(z|x)}[\log p_{\theta}(x|z) + \log p_0(z) - \log q_{\phi}(z|x)]\le\log p_{\theta}(x), 
    \label{eq:ELBO}
\end{equation}
where
\begin{equation}
\mathbb{E}_{p(x)}[A(x)]=\int dx \,p(x) A(x), \quad \int dx\,p(x)=1\,,
\end{equation}
denotes the expectation value over the normalized probability distribution $p(x)$.


\textbf{Generative adversarial network (GAN)}~\cite{2014arXiv1406.2661G} also belongs to latent variable generative model, and introduces an adversarial training strategy to optimize the generator together with a discriminator (later soon will be explained). Basically the GAN framework contains two non-linear differentiable function, both of which being represented by adaptive neural networks. The first is the generator $G$, which maps random latent variable $z$ from a prior distribution $p(z)$ (usually multivariate uniform or Gaussian) to the target data space, $\tilde{x}=G(z)$, which induces an implicit synthetic distribution $p_G(x)$ that per training to be pushed to the target distribution $p_{true}(x)$. The second one is named as the discriminator $D(x)$ with a single scalar output for each data (faked or real) sample, that tries to distinguish between real data $x$ and generated data $\hat{x}$ by training to output $D(x)=1$ and $D(\tilde{x})=0$. These two networks will be trained in turn to improve their respective abilities going against each other, hence mimicking a two-player min-max game (also called zero-sum game) where their corresponding loss function sum to zero, $\mathcal{L}_G + \mathcal{L}_D =0$. Upon optimization the respective parameters $\theta_G$ and $\theta_D$ in
the GAN will converge to the \textit{Nash equilibrium} state,
\begin{equation}
\theta_{G,D}^{*}=\arg \min\limits_{\theta_G} \max\limits_{\theta_D}(-\mathcal{L}_D(\theta_G,\theta_D))\,.
\label{nash-eq}
\end{equation}
where the generator excels in synthesizing samples that the discriminator cannot differentiate anymore from real ones, therefore the data distribution has been captured by the generator after training. The original GAN uses the loss function
\begin{equation}
\mathcal{L}_D=-\mathbb{E}_{x\sim p_{\rm true}}[\log D(x)] - \mathbb{E}_{z\sim p_{\rm prior}}[\log(1-D(G(z)))]\,,
\label{gan_l1}
\end{equation}
and we used
\begin{equation}
\mathbb{E}_{z\sim p_{\rm prior}}[A(G(z))] = \mathbb{E}_{\hat x \sim p_G} [A(\hat x)]\,.
\end{equation}
Mathematically it's proved that the training of GAN is equivalent to minimizing the Jensenâ€“Shannon(JS) divergence generalized from the KL divergences, 
\begin{equation}
    \mathcal{D}_{JS}(p_{real}||p_G)=\frac{1}{2}(\mathcal{D}_{KL}(p_{real}||p_{mix})+\mathcal{D}_{KL}(p_G||p_{mix})), 
    \label{eq:JS}
\end{equation}
with $p_{mix}=(p_{real}+p_G)/2$. Thus, the GAN belongs to implicit MLE based generative model as well.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp!]
    \centering
    \includegraphics[width = 0.9\textwidth]{figures/fig_1-2-5_GAN.pdf}
    \caption{A demonstration of GAN, consisting of the generator and discriminator for generating and recognizing data.
    \label{fig:gan}}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
To add more flexible conditional-control and enhance the training stability, later a multitude of advanced development for GAN have been proposed, like ACGAN~\cite{2016arXiv161009585O}, Wasserstein-GAN~\cite{2017arXiv170107875A}, improved Wasserstein-GAN~\cite{2017arXiv170400028G}. The most important difference of Wasserstein-GAN to the original GAN lies in the optimization object, which turns to the Wasserstein-distance (also called Earth Mover distance) being superior over the JS divergence in vanila GAN.



\textbf{Autoregressive model}~\cite{2015arXiv150203509G,2016arXiv160106759V}, as an explicit MLE based generative model, invokes the chain rule for decomposing a joint probability into a series of conditionals
\begin{equation}
    p_{\theta}(x)=\prod_{i}^{N}p_{\theta}(x_i|x_1,x_2,...,x_{i-1})
\label{auto_prob}
\end{equation}
to model the data likelihood $p_{\theta}(x)$. Autoregressive model usually use neural networks to represent each of the involved conditional probability, the collection of which as a whole can be viewed as one network with triangular weight parameter matrix for simple fully connected network case (for cases using CNN or RNN the integral weight matrix is masked accordingly), in order to respect the autoregressive properties specified by the probability decomposition (Eq.~\eqref{auto_prob}). In other words, it's designed such that each output element of the autoregressive network is independent of those input elements with later index inside a predetermined order. Such construction for the generative model with Eq.~\eqref{auto_prob} also renders efficient sampling from the joint distribution $p_{\theta}(x)$ by sequentially sampling from each conditional probabilities.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[ht!]
    \centering
    \includegraphics[width = 0.4\textwidth]{figures/fig_1-2-3_AR.pdf}
    \caption{A neural network with autoregressive property, i.e., the value of outputs (red solid circle) at a given order only depends on its past inputs (green solid circles) and bias (black solid circle).
    \label{fig:autore}}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

It was first designed into time-series data generation, like the recurrent neural network (RNN) in priciple also holds such autoregressive property. For structured systems it was further developed to employ the convolutional layer and correspondingly the PixelCNN~\cite{2016arXiv160605328V,2017arXiv170105517S} structure is constructed to fulfil the autoregressive transformation. Once the parameterization (through network) for Eq.~\eqref{auto_prob} is given then explicit MLE can be performed by minimizing the forward KL divergence $\mathcal{D}_{KL}(p_{real}||p_{\theta})$ if there are collected samples available. Otherwise, with known unnormalized distribution one can also minimize the backward KL divergence $\mathcal{D}_{KL}(p_{\theta}||p_{real})$ by sampling to estimate the involved entropy term.

\textbf{Normalizing flow (NF)}~\cite{2015arXiv150505770J, 2019arXiv190809257K} puts more efforts in performing explicitly the maximum likelihood estimation (MLE), through introducing a bijective, invertible and differentiable transformation $f_{\theta}$ (parameterized with networks) between a simple latent space $z$ and the complex data space $x=f_{\theta}(z)$. The essential idea lies in the change of variable theorem which manifests the conservation of probability in connecting the latent space and target data space, 
\begin{equation}
    p_{\theta}(x)=p(z)|\det(\frac{\partial z}{\partial x})|=p(f_{\theta}^{-1}(x))|\det(\mathcal{J}_{f_{\theta}^{-1}})|,
    \label{eq:change-of-variable}
\end{equation}
through which the likelihood can be evaluated and maximized for optimizing the transformation thus the generator $f_{\theta}(z)=x$. Special network structures are usually adopted which render accessible and practically computable Jacobian determinant evaluations. like the NICE (Non-linear Independent Components Estimation)~\cite{2014arXiv1410.8516D} or Real NVP (Real-valued Non-Volume Preserving)~\cite{2016arXiv160508803D} both with triangular Jacobian matrix. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp!]
    \centering
    \includegraphics[width = 0.75\textwidth]{figures/fig_1-2-6_flow.pdf}
    \caption{Sketch of a normalizing flow model, deforming a simple distribution $p(\mathbf{z}_0)$ to a complex one $q(\mathbf{x})$ step by step. 
    \label{fig:1.2.3:flow}}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Basically, NICE or Real NVP composes a sequence of simple affine coupling layers (represented by DNN) which split the sample (e.g., image, or field configuration as discussed in Sec.~\ref{sec:3:flow_based}) into two subsets and transform one subset variables conditioned on the other, thus the transformation is with triangular Jacobian matrix, whose determinant can be efficiently evaluated. Specifically, the $i^{\text{th}}$ affine coupling layer in NICE~\cite{2014arXiv1410.8516D} reads
\begin{equation}
\left\{
\begin{aligned}
    z^{i}_{1:k} &= z^{i-1}_{1:k} \\
    z^{i}_{k+1:N} &= z^{i-1}_{k+1:N}  + m_{\theta}^i(z^{i-1}_{1:k}),
\end{aligned}
\right.
\label{flow_nice}
\end{equation}
with $z^i$ the output of the $i^{\text{th}}$ affine coupling layer (thus $z^0 = z\sim p(z)$ and final output $z^{L}=x\sim p(x)$), and $N$ the dimensionality or the number of variables in one sample. The above coupling layer operation can be easily inverted,
\begin{equation}
\left\{
\begin{aligned}
    z^{i-1}_{1:k} &= z^{i}_{1:k} \\
    z^{i-1}_{k+1:N} &= z^{i}_{k+1:N} - m_{\theta}^i(z^{i}_{1:k}),
\end{aligned}
\right.
\end{equation}
The neural network representation come into play for the mapping construction of the translation function ($m_{\theta}: \mathbb{R}^{k}\rightarrow \mathbb{R}^{N-k}$), or in Real-NVP for the involved scaling and translation functions (as will be introduced in Sec.~\ref{sec:3:flow_based}). The non-linearity or complexity introduced by the network would not hurt the accessibility for the Jacobian determinant, which is the trace for the lower triangular matrix and in the case of NICE it is just unity.
Usually, several such affine coupling layers are composed together to construct the normalizing flow transformation $f_{\theta}(z)$.


\subsubsection{Physics-motivated New Developments}
In the above, we reviewed the deep-learning techniques in detail.
Deep learning is a powerful tool that can be broadly applied to different aspects of high-energy nuclear physics. 
If physics knowledge is encoded in the set-up of the learning process, one can further improve the efficiency, even applicability, of deep learning. 
In the history of its development, several concepts have been raised following this philosophy in deep learning --- 
\textit{physics-inspired}, \textit{physics-informed}, and \textit{physics-driven} deep learning.

The concept of \textit{physics-inspired} machine learning is lacking of clear definition yet loosely summarized in Ref.~\cite{Ahmad:2020kdd}. It basically describes a paradigm that transfer ideas originating in physics into machine learning. Historically, many concepts from statistical physics deeply influence the foundations of machine learning, e.g., information entropy, information bottleneck and energy-based model, see a more systematic introduction in Ref.~\cite{sompolinsky1988statistical,mezard2009information}. In many modern applications, such as Restricted Boltzmann Machines(RBM)~\footnote{It consists of two layers of nodes: a visible layer, which represents the input, and a hidden layer, which models the underlying interactions. The nodes in the two layers are linked by weighted connections, and the RBM defines a probability distribution over the visible units given the hidden units and vice versa. The RBMs have been used to construct many-body quantum states~\cite{2017Sci...355..602C}(mentioned in Sec.~\ref{sec_phases_obs}), also see more advanced researches in Ref.~\cite{Carrasquilla:2020mas,Jia:2021qaa} about recent developments in Neural Network Quantum State (NNQS).}~\cite{ackley1985learning,fischer2012introduction}, Geometric Learning~\cite{Bronstein:2016thv,Bronstein:2021mdi} and Diffusion Models~\footnote{The Diffusion Model is a cutting-edge development in deep learning. It consists of a forward diffusion process and a reverse generation process. The forward diffusion process incrementally adds Gaussian noise to the samples in multiple small steps until it reaches a purely normal distribution. After training, generation can be initiated from a normal distribution by repeatedly applying the learned inverse denoising transformation until the sample is in the data space. See a recent review~\cite{Yang2022DiffusionMA}.}~\cite{ho:2020denoising,song2021scorebased}, the physics origins are explicit. The backbone of modern physics, symmetry, is also widely manifested in the design of neural network architecture~\cite{Goodfellow2016,Mattheakis:2019tyi,Kicki:2021so}, e.g., translation invariance in CNNs~\cite{zhang1988shift,Goodfellow2016}, 
Group Equivariant Convolutional Networks~\cite{pmlr-v48-cohenc16}, permutation invariance in Point Nets~\cite{qi2016pointnet}, and rotational invariance in E3NN~\cite{e3nn_paper}, and gauge invariance in the partition function of a lattice field theory~\cite{Albergo:2019eim, Kanwar:2020xzo, Albergo:2021vyo}.


Physics knowledge is implemented in \textit{physics-informed} deep learning~\cite{Raissi:2017zsi,2021NatRP...3..422K} in a different manner. Some physics properties cannot be encoded in the network architecture explicitly. Instead, one can include extra terms in the loss function to ``punish'' the violation of the desired properties. Let us take a dynamical system evolving in time as an example. One can train a network with the loss function, including the violation of the observation and the underlying equation of motion. A successfully trained network would be able to predict the observables at any given time without solving the differential evolution equation. Compared to \textit{physics-inspired} DL, \textit{physics-informed} ones take the physics knowledge as a soft constraint, as one can hardly guarantee the loss term to be exactly zero.
More introduction can be found in recent review~\cite{2021NatRP...3..422K}. 

Last but not least, \textit{physics-driven} deep learning is purposed to train differentiable problems. The Back-Propagation method discussed in Sec.~\ref{subsubsec:dl} is designed to optimize the parameters in a network so that its output matches the desired observables. In some problems, the observables are not naively the network output but some function (or functional) of it. If such a function or functional relation is differentiable, one may incorporate their derivatives with the Back-Propagation method such that parameters can be efficiently optimized using gradient based methods.
See also Ref.~\cite{2021arXiv210905237T} as a comprehensive introduction.

Generally speaking, physics-driven machine learning implements more physics information into the training and therefore requires less amount of data, compared to the other two approaches. Additionally, the three approaches are not exclusive from each other. One can simultaneously apply more than one technique in above from different aspects on one problem. A concrete example, which attempts to reconstruct the nuclear matter equation of state from the neutron star mass and radius measurements, will be given in Sec.~\ref{sec:new}.

\subsection{Outline}
The organization of this review is summarized in Fig.~\ref{fig:1:Outline}. We cover ML applications in 
Heavy-ion collisions (Sec.~\ref{sec:hic}),  Lattice QCD (Sec.~\ref{sec:lat}) and Neutron star equation of states (Sec.~\ref{sec:astro}). Finally, in Sec.~\ref{sec:new}, we also demonstrate some new advanced developments.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=1.0\textwidth]{figures/fig_1-3-1_outline.png}
\caption{Outline of this review.}\label{fig:1:Outline}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%