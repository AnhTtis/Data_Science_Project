\section{Heavy-Ion Collisions}\label{sec:hic}

\subsection{Overview and Challenges for HICs}\label{sub:hic_method}

Relativistic heavy-ion collisions (HICs) create an extreme environment for QCD matter in the laboratory~\cite{BRAHMS:2004adc, PHENIX:2004vcz, PHOBOS:2004zne, STAR:2005gfr, Schukraft:2011cz, Steinberg:2011qq, Wyslouch:2011zz}, e.g., at RHIC and LHC (also see Ref.~\cite{Baym:2001in} for a history background retrospect), with the highest temperature~\cite{Muller:2012zq, Braun-Munzinger:2015hba, Muller:2006ee}, the strongest magnetic field~\cite{STAR:2018gyt, Muller:2018ibh}, and fastest rotational angular velocity on Earth~\cite{STAR:2017ckg}. In this extreme environment, hadronic matter is expected to be deconfined to form a strongly interacting and free-roaming quark-gluon plasma (QGP), which is the same matter that was created in the early universe $10^{-6}$s after the \textbf{Big Bang}~\cite{Yagi:2005yb, Wang:2016opj, Fukushima:2020yzx}.

The goals of the HIC physics include (but not limited to),
\begin{itemize}
\item \textbf{Look for evidence of deconfinement.} E.g., the strong collective flow~\cite{Teaney:2000cw}, the enhancement of strange particles~\cite{Rafelski:1982pu}, the suppression of high-$p_T$ particles explained by the jet-medium interaction in QGP~\cite{Wang:1992qdg} and the suppression of heavy quarkonium~\cite{Matsui:1986dk}.
\item \textbf{Study the QCD phase structure.} Such as, what is the nature of the phase transition between QGP and a hadron resonance gas (HRG) with equal proportions of matter and antimatter? What is the type of phase transition between QGP and normal nuclear matter with a finite baryon number? If there are crossover and first-order phase transitions in the diagram, is there a critical endpoint in between? What is the experimental evidence for the critical phenomena? See e.g.,~\cite{Gupta:2011wh, Luo:2017faz}.
\item \textbf{Study the equation of state (EoS) of QGP}, e.g., the relations between local pressure, local energy density, entropy density, and local temperature. Will the EoS provided by lattice QCD lead to a reasonable dynamical evolution of QGP that can describe the momentum distribution of final state particles in HICs? See, e.g.,~\cite{Shuryak:2004cy, Bazavov:2009zn, Borsanyi:2016ksw} for more details. 
\item \textbf{Study the sensitivity of the physical observables to the physical parameters in the collisions.} For example, what are the effects of shear and bulk viscosity of QGP? What are the effects of the freeze-out temperature and the hadronic cascade? What are the effects of initial nuclear structure and gluon saturation? One may refer to~\cite{Bernhard:2018hnz, Bernhard:2019bmu, JETSCAPE:2020shq, Nijs:2020ors} and the references therein.
\item \textbf{Study the jet-medium interaction and in-medium effects for heavy bound states.} E.g., the shape and structure of the jet shower due to the jet-medium interaction, the sensitivity of the medium response to the underlying EoS of QGP, and the heavy quark potential inside the medium. Refer to~\cite{Wang:2004dn, Vitev:2002pf, JET:2013cls, Rapp:2018qla} for an overview.
\item \textbf{Search for CP violation and rotation/spin polarization (Chirality and Vorticity in QCD)} in the strong interaction. coupling between the classical, collective orbital angular momentum and the spin, an intrinsic quantum property, of a single-hadron. 
\end{itemize}

\subsubsection{``Standard Model'' of Simulating HICs}\label{sub:hic_method:standard_model}
To describe the whole process of a heavy ion collision, one has to construct hybrid models with different physics at different stages of the collision~\cite{Yagi:2005yb, Busza:2018rrf, Lappi:2016gmk, Elfner:2022iae}. Here we briefly summarize the state-of-the-art modeling for each stage. In the pre-collision state, a \textit{Monte Carlo model} is used to determine the positions of the nucleons inside a 3-dimensional nucleus, to determine the collision patterns between two nuclei, e.g., the impact parameter of a collision, the orientation of the deformed nucleus, which can lead to more complex tip-tip or body-body collisions. During the collision, \textit{color-glass condensate and saturation models} are used to account for the physics of special relativity and vacuum fluctuations in the nucleus to calculate the fluctuating local entropy deposition in the overlap region. After the formation of locally equilibrated QGPs, \textit{relativistic hydrodynamics} is used to describe the dynamical evolution of the local temperature and fluid velocity in the expanding QGP. Hadrons form at the boundaries of the QGP and will interact with each other via \textit{transport models}. In parallel to the dynamical evolution of the soft(low energy) particles in HIC, the hard(high energy) partons with extremely high energy and momentum will pass through the QGP and interact with the thermal partons in the QGP. The physics is described by QCD, where partons will split and collide. The differential cross-sections are usually provided by pQCD calculations in leading order. 
    
\emph{\textbf{Monte Carlo models for the initial nuclear structure}} ---
According to the charge distribution of the nucleons in the nucleus, the density distribution of the nucleons is modeled using a deformed Woods--Saxon function~\cite{Woods:1954zz, Kahana:1969zz},
\begin{align}
\rho(r, \theta, \phi) = \frac{\rho_0}{e^{(r - R_0(1 + \beta_2 Y_{20}(\theta) + \beta_4 Y_{40}(\theta)))/a} + 1},
\end{align}
where $\rho_0$ is the nucleon density inside the nucleus, $R_0$ is the radius, $a$ is the diffusiveness, $\beta_2$ and $\beta_4$ are two deformation factors whose values determine the shape of the nucleus, e.g., whether it is prolate or oblate.
For $^{208}$Pb, whose proton number $82$ and neutron number $126$ are both magic numbers, the deformation parameters $\beta_2=\beta_4=0$, its shape is a perfect sphere~\cite{10.2307/1758208,Loizides:2017ack}. Other heavy-ionic nuclei like Au, U, Cu, O, Xe, Ru, and Zr are deformed to varying degrees.

In Monte Carlo simulations, $A$ nucleons are first sampled from the above deformed Woods--Saxon distribution for each nucleus. For each pair of the sampled nuclei, the probability $P(b) = 2b / R^2$ is used to sample the impact parameter $b$, where $R$ is the maximum distance between two nucleons for overlap. In this procedure, each nucleon is sampled independently, without taking into account the nucleon-nucleon correlation, the clustering effect, and the difference between the proton and neutron distributions. These can be taken into account in specific studies.

\emph{\textbf{Color Glass Condensate}} ---
Due to special relativity (Lorentz contraction and dilation effect, specifically), at extremely high energy with respect to the lab frame, the shape of the nucleus is compressed along the beam direction and the lifetime of quantum fluctuations in the nucleus is extended~\cite{Mueller:1989st, McLerran:1993ka, McLerran:1993ni, McLerran:1994vd, Lappi:2006fp, Gelis:2010nm}. As a result, virtual quark-antiquark pairs and gluons live long enough to participate in high-energy collisions.
As the collision energy increases, the gluons of typical longitudinal momentum correspond to a small momentum fraction($x$) of the incoming nucleons. Noting the small-$x$ region of parton distribution e.g., from Deep Inelastic Scatterings(DISc), the dominant contribution is from gluons, and the number of gluons in the projectile seen by the target increases with the collision energy. To leading order, the energy-momentum after the collision is given by
\begin{align}
T^{\mu\nu} = {1\over 4} g^{\mu\nu} F^{\alpha\beta}F_{\alpha\beta} - F^{\mu\alpha}F^{\nu}_{\beta},
\end{align}
where $F^{\mu\nu}$ is the field strength of the classical retarded color field $A^{\mu}$ described by the classical Yang--Mills equation,
\begin{align}
[D_{\mu}, F^{\mu\nu}] = I^{\nu}
\end{align}
where $I^{\nu}= \delta^{\nu+}\rho_1 + \delta^{\nu -} \rho_2$ is the external current associated with the fast-moving partons in the projectile with density $\rho_1$ and in the target with density $\rho_2$.

The IPGlasma model is a successful attempt is to describe the HIC initial condition by solving the classical Yang--Mills equation for gluons radiated from color sources~\cite{Schenke:2012wb, Schenke:2012hg}. Solving the field equation is, however, computationally expensive. One may adopt phenomenological models, such as the Trento Monte Carlo model~\cite{Moreland:2014oya}
\begin{align}
s({\bf x_T}) = \left( \frac{T_A^p({\bf x_T}) + T_B^p({\bf x_T})}{2}\right)^{1/p}.
\end{align}
which generates the initial condition of entropy density($s$) as a $p$-powered average of the thickness functions ($T_A$ and $T_B$), where $T_A$($T_B$) is the nuclear matter in projectile(target) with the longitudinal direction integrated. $p$ is a dimensionless parameter that can be tuned. Also note that such CGC inspired initial condition to HICs is also phenomenologically interesting by itself~\cite{Xu:2014ega,Stoecker:2015zea,Stocker:2015nka,Zhou:2017zql}.
From a Bayesian global fit~\cite{Bernhard:2016tnd}, it has been found that the anisotropy of entropy deposition in the transverse plane of IPGlasma can be approximated by choosing $p \approx 0$.

\emph{\textbf{Relativistic Hydrodynamics}} ---
\label{sec:hydro}
The dynamical evolution of created fireball (QGP and HRG in local equilibrium) from HICs can be described by relativistic hydrodynamic equations,
\begin{align}
\nabla_{\mu} T^{\mu\nu}  = 0, \quad\;
\nabla_{\mu} J^{\mu} = 0,
\end{align}   
where $\nabla_{\mu}$ represents the covariant derivatives, $T^{\mu\nu} = (\varepsilon + P + \Pi) u^{\mu}u^{\nu} - (P + \Pi) g^{\mu\nu} + \pi^{\mu\nu}$ is the energy-momentum tensor of hot nuclear matter, with $\varepsilon$ the local energy density,  $P$ the local pressure given by the equation of state $P = P(\varepsilon, \mu_B)$, $\Pi$ the bulk viscosity, $u^{\mu}$ the fluid four-velocity satisfying $u^2=1$, $\pi^{\mu\nu}$ the shear viscous tensor, $g^{\mu\nu}$ the metric tensor. 
$J^{\mu} = n u^{\mu} + v^{\mu}$ is the charge current, where $n$ is the net charge density,
$v^{\mu}$ is the diffusion of the net charge, e.g., the net baryon diffusion.  
This set of equations is solved together with the Israel--Steward equations~\cite{Israel:1979wp} for $\Pi$, $\pi^{\mu\nu}$ and the baryon diffusion current $v^{\mu}$. 

One merit of relativistic hydrodynamics is that it encodes the equation of state provided by lattice QCD calculations.  Meanwhile, relativistic hydrodynamics is a complex dynamical evolution that involves many fundamental properties of hot QCD matter, e.g., shear viscosity over entropy density $\eta/s$, bulk viscosity over entropy density $\zeta /s$, baryon diffusion parameter $k_B$, the initial time $\tau_0$, the freeze-out temperature $T_f$. With the accessible experimental results on HIC, one may determine these fundamental properties by performing Bayesian analysis in a global fitting~\cite{Bernhard:2018hnz, Bernhard:2019bmu, JETSCAPE:2020shq, Nijs:2020ors}. Hydrodynamics provide the complete evolutionary history of the soft partons, e.g., the energy density, the pressure, and the temperature at any given space-time point $(t, x, y, z)$. This information is valuable because it provides not only the spectra and momentum anisotropy of soft hadrons but also the background information for jet quenching and the production of direct photons and dileptons. In this way, relativistic hydrodynamics provides multiple ways to study the properties of QGP.


There are many different implementations of the relativistic hydrodynamics, either in 2+1D or in 3+1D~\cite{Kolb:2003dz, Muronga:2001zk, Hirano:2005xf, Chaudhuri:2006jd, Romatschke:2007jx, Dusling:2007gi, Song:2007ux, Du:2019obx, Inghirami:2016iru, Okamoto:2017ukz, Nijs:2021clz, Florkowski:2010cf, Strickland:2012bc, Schenke:2010nt, Ryu:2015vwa, Shen:2017bsr, Bazow:2013ifa, Shen:2014vra, Pierog:2013ria, Sakai:2020pjw, Karpenko:2013wva, Pang:2018zzo, Yin:2015fca, Hattori:2022hyo, Guo:2019mgh, Shi:2020htn}. 
2+1D hydro assumes the fluid to be boost invariant along the beam direction, whereas 3+1D hydro does not. The former is a good approximation for HIC phenomena in mid rapidity. Typically simulating one HIC event in 3+1D hydrodynamics with finite shear viscosity takes 40 to 60 times longer than simulating one event in 2+1D hydro. It is thus more convenient to accumulate a large amount of data using 2+1D hydro, which will also be beneficial for machine learning studies. For example, in the Bayesian analysis~\cite{Bernhard:2016tnd} that requires millions of events, VISH2+1 hydrodynamic model is used to generate data with event-by-event fluctuating initial conditions. 

\emph{\textbf{Transport models for the hadronic cascade}} ---
The particle-yield ratios between different hadrons are well described by the statistical model, assuming that the particles emitted from the freeze-out hypersurface obey the Fermi--Dirac distribution for baryons and the Bose--Einstein distribution for mesons, with mass $m_i$ and chemical potential $\mu_i$,
\begin{equation}
{1 \over 2\pi}{d N \over dY p_T dp_T d\phi} = {\rm dof} \int {d^3 p \over (2\pi)^3}f(p\cdot u, T),
\label{eq:freezeout}
\end{equation}
where ${\rm dof} = 2 \times {\rm spin} + 1$ is the spin degeneracy, $f$ is the distribution function given by,
\begin{equation}
 f = {1 \over e^{p\cdot u/T} \pm 1},
\end{equation}
where $\pm$ stands for baryons and mesons respectively. In hybrid models, one can thus sample hadrons using the above distribution function in the comoving frame of each fluid cell with local temperature $T$ and fluid velocity $u^{\mu}$ on the freeze-out hyper surface. 
    
The hadrons from the freeze-out hyper surface can go through two different processes. Immediately after particleisation, the hadron density is still very high and the many-body interactions between hadrons are still too strong to be described by transport models. At this stage, relativistic hydrodynamics still works well. At a later stage, hadrons become diluted, and their interactions should be described by hadronic cascade models such as UrQMD~\cite{Bleicher:1999xi} and SMASH~\cite{Weil:2016zrk}. 
    
\emph{\textbf{Jet quenching in QGP}} --- There have been tremendous efforts in developing theoretical tools to model the interactions between energetic partons and thermal partons in QGP and simulate them in phenomenological studies ~\cite{Gyulassy:1993hr, Wang:1994fx, Baier:1996sk, Baier:1994bd, Baier:1996kr, Zakharov:1996fv, Wiedemann:2000za, Guo:2000nz, Wang:2001ifa, Zhang:2003yn, Schafer:2007xh, He:2015pra, Cao:2016gvr, Casalderrey-Solana:2014bpa, Shi:2018izg, Cao:2020wlm, Qin:2015srf, Majumder:2010qh, Armesto:2011ht, Qin:2009bk, Jeon:2003gi, Noronha-Hostler:2016eow, Andres:2016iys, Bianchi:2017wpt, Chien:2015vja, Andres:2019eus, Yazdi:2022bru, Shi:2022rja}. While these models take different assumptions in the derivation, here we take the linear Boltzmann transport equations~\cite{He:2015pra} as an example to demonstrate the theoretical framework,
\begin{equation}
p_i \cdot \partial f_i=\int \sum_{i,j,k} \prod_{a=i, j, k} \frac{1}{(2 \pi)^3 }\frac{d^3 p_a}{2 E_a}\left(f_k f_l-f_i f_j\right)\left|\mathcal{M}_{i j \rightarrow k l}\right|^2 \frac{\gamma_j}{2} S_2(\hat{s}, \hat{t}, \hat{u})(2 \pi)^4 \delta^4\left(p_i+p_j-p_k-p_l\right)+\text { inelastic },
\label{eq:LBT}
\end{equation}
where $f_i$ is the distribution function for the hard parton $i$ whose initial position is sampled from the distribution of binary collisions between nucleons and whose four-momentum is provided by the Pythia Monte Carlo model~\cite{Sjostrand:2019zhc}. The right-hand side includes the gain term $f_k f_l$ and the loss term $-f_i f_j$ due to the collisions of hard partons with thermal partons. The scattering amplitude $\mathcal{M}_{i j \rightarrow k l}$ is given by tree-level pQCD calculations. $\gamma_j$ is the spin and colour degeneracy of the thermal parton $j$. $S_2$ is a control factor to eliminate the collinear divergence, which is given by
\begin{equation}
S_2(\hat{s}, \hat{t}, \hat{u}) = \theta(-\hat{s} + \mu_D^2 < \hat{t} < -\mu_D^2) \theta(\hat{s} > 2 \mu_D^2),
\label{eq:s2_collinear}
\end{equation}
where $\hat{s}, \hat{t}, \hat{u}$ are three Mandelstam variables and $m_D = {\sqrt{6} \over 2} g T$ is the Debye screening mass for gluons and light quarks. 

The last inelastic term accounts for the gluon radiation induced by elastic scattering. The gluon radiation rate $\Gamma_a^{\text {inel}}$ is taken from higher-twist calculations,
\begin{align}
 \frac{d \Gamma_a^{\text {inel}}}{d z d k_{\perp}^2}=\frac{6 \alpha_{\mathrm{s}} P_a(z) k_{\perp}^4}{\pi\left(k_{\perp}^2+z^2 m^2\right)^4} \frac{p \cdot u}{p_0} \hat{q}_a(x) \sin ^2 \frac{\tau-\tau_i}{2 \tau_f}
\label{eq:gluon_radiation}
\end{align}
where $z$ is the energy fraction of the emitted gluon with respect to the hard parton $a$, $k_{\perp}$ is the transverse momentum of the emitted gluon, $\alpha_{\mathrm{s}}=\frac{g^2}{4\pi}$ is the coupling constant, $P_a(z)$ is the parton splitting function, $\hat{q}_a(x)$ is the transverse momentum transfer per unit length due to elastic scattering. The $\sin$ function encodes the quantum interference between gluons emitted at different time. The $\tau_i$ is the production time of the parent parton $a$ and $\tau_f = 2 p_0 z(1-z) / (k_{\perp}^2+z^2 m^2)$ is the formation time of the emitted gluon.

All simulation techniques and packages mentioned above are developed by different research groups, which makes it crucial to arrange them in a consistent and organized manner for a systematic phenomenological study in HIC. Thus, 
the JETSCAPE topical collaboration is formed, aiming to provide a comprehensive framework that implements the state-of-the-art simulation packages for every stage, so that soft and hard physics can be studied consistently. For jet-quenching phenomena in particular, the JETSCAPE framework construct a multi-stage jet-medium interaction model that takes into account different energy and virtuality scales of hard partons in the jet shower compared to the medium~\cite{JETSCAPE:2017eso}. 
For example, JETSCAPE uses MATTER~\cite{Kordell:2017hmi} to simulate the parton splitting at early times when the parton $a$ is highly virtual, it uses LBT~\cite{He:2015pra}, MARTINI~\cite{Schenke:2009gb}, or AdS/CFT~\cite{Pablos:2017csi} to simulate the interactions between medium and hard partons with low virtuality.
In this way, the JETSCAPE Monte Carlo model provides an integration of models for the production of simulating data in heavy ion collisions. 
	
\subsubsection{HIC Challenges}
\label{hic_challenges}
Theoretical simulations of high-energy heavy ion collisions (HIC) and experiments at RHIC and LHC have generated a huge amount of data. Unlike $e^+ + e^-$ collisions and $p+p$ collisions, HIC produces thousands of final-state hadrons at the highest energies of RHIC and LHC, in every single collision event of Au+Au and Pb+Pb. These produced hadrons contain both soft and hard particles, with the soft particles coming mainly from the freeze-out of the QGP while the hard particles from jet fragmentation or heavy quarkonium decay. 
    
The data is routinely compressed to low-dimensional representations in physics space for data-model comparison, e.g., the charged multiplicity as a function of pseudo-rapidity, the $p_T$ spectra, the anisotropic flow coefficients, the di-hadron correlation, etc. However, the initial state, the QCD matter EoS, the QGP properties such as shear and bulk viscosity are all coupled to final state observables intricately. For example, both the shear viscosity and the freezing temperature will change the slope of the $p_T$ spectra. Traditional data analysis techniques encounter difficulties in determining a physical property using the final state observables, as its value will change adaptively with the values of other model parameters, as shown in Fig.~\ref{fig:Bass}.

    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp!]
    \centering
    \includegraphics[width = 0.6\textwidth]{figures/entangled_features.png}
    \caption{The entanglement between different model parameters and physical observables in heavy ion collisions. Taken from Ref.~\cite{Bass2017} with permission.
    \label{fig:Bass}}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    
Data analysis in high-energy HICs is a typical \textit{inverse problem}\footnote{See Sec.~\ref{sec:5:inverse} for more discussions on inverse problems.}: given all the different known physics factors, well-established standard computational models such as 3+1 D hydrodynamics can mimic the collision dynamics and obtain corresponding final state information, but given only limited and cross-impacted final state measurements, how to extract knowledge about the early time physics happened from the intricate entanglement influence, it is a very non-trivial inverse inference task. The lifetime of the formed QGP in high-energy HICs is only about $10^{-23}$ seconds, which is too short (also too small) to be resolved. What can be determined experimentally is the four-momentum of the final state hadrons or their decay products, but what we are interested in is the initial state and the properties of the QGP early in the collision evolution. It is unknown whether the physical information will survive the violent expansion and leave an imprint in the final state due to entropy production and memory decay induced by information loss. It is also unknown whether this is an ill-defined inverse problem, where different parameter combinations may lead to degenerate output (final-state information).
    
In recent years, two techniques behave promising for extracting physical information from exotic final state particles of high-energy HICs. One method is the Bayesian analysis, where all available experimental data can be used together to determine multiple model parameters simultaneously through global fitting~\cite{Bass2017, Bernhard:2016tnd, Bernhard:2018hnz, Bernhard:2019bmu, JETSCAPE:2020shq, Nijs:2020ors}. The other method is deep learning, which can search for observables that are sensitive to only one physical property\cite{Pang:2016vdc,Steinheimer:2019iso,Benato:2021olt}. Deep learning is currently the best pattern recognition method that can extract features and feature combinations from high-dimensional data and map them to specific physical properties.

	
\subsection{Initial States and Collision Geometry}
Since HIC challenges can be viewed as a type of inverse problems, it is natural to ask whether the initial state of HIC can be extracted from the momentum distribution of the final state hadrons. Due to entropy production and information loss, there is no guarantee that the initial state is recoverable from the final state particles' information. However, we know for sure that some of the initial state information survives the complex dynamical evolution of strongly coupled matter and is present in the exotic particles of the final state. For example, the momentum anisotropies have a strong correlation with the geometric eccentricity of the initial state as well as with the impact parameter. It is also known that the deformation of the nuclear structure leads to intrinsic collision patterns related to the distributions of the final state charge multiplicity and momentum anisotropy. It would also be interesting to know whether other initial state fluctuations as well as correlations are transformed into correlations of final state particles in momentum space. E.g., the neutron skin, the nucleon-nucleon correlation, the $\alpha$ clusters in heavy nuclei~\cite{He:2021uko} and the gluon saturation at relativistic energies.

	
\subsubsection{Impact Parameter Determination}\label{hic_b}
If one wants to extract one number at the initial state from the final state particles using machine learning, the first one to try is the impact parameter, which is the transverse distance between two colliding nuclei. It is essential to know the impact parameter $b$ for determining the event geometry and further analysis, e.g., the volume estimation in fluctuation analysis. However, we have no direct control or measurement over $b$ in experiments. Usually, final state observables such as charged multiplicity are used to define centrality classes based upon models such as (Monte-Carlo)-Glauber simulation~\cite{Miller:2007ri}, through which one can get only a likely distribution of $b$ for a given centrality class. Here the centrality classes are usually specified from percentiles of those final state observables, and further guide the grouping of events but in a rough manner.
This impact parameter is so important that recently RHIC spent several million on the detector to improve the precision of impact parameter determination~\cite{Kagamaster:2020oon}. ML algorithms can provide a useful tool in discriminating initial conditions for HICs from the final state accessible information.

\emph{\textbf{Early Attempts with ML to Determine Impact Parameter}} ---
Many early attempts with the usage of ML techniques for determining the impact parameter mainly resort to simple algorithms, e.g., feed-forward neural network using conventional observables~\cite{David:1994qc,Bass:1996ez,Haddad:1996xw} or support vector machine (SVM)~\cite{De_Sanctis_2009} or Bayesian inference with also K-means clustering~\cite{Li:2022mni}. Later, working directly on the two-dimensional transverse momentum and rapidity spectra of final state particles, the DNN, Light Gradient Boosting Machine (LightGBM) and the Convolutional Neural Networks (CNN) algorithms were employed to estimate the impact parameter at intermediate energies~\cite{Li:2020qqn, Li:2021plq, Zhang:2021zxd} with UrQMD provide the simulated events, and then also on realistic cases including detector responses of the S$\pi$RIT Time Projection Chamber into the simulation events~\cite{Tsang:2021rku}. Compared to conventional means, these ML-based methods show better performance in estimating the impact parameter, especially giving rise to the ability in recognizing the central collision events. Such strategy for impact parameter estimation using DNN and CNN was also discussed for Au+Au collisions at $\sqrt{s_{NN}}=200$ GeV~\cite{Xiang:2021ssj} using final state energy spectrum in $(p_x, p_y)$ space as input. With data simulated from AMPT model, the trained CNN gives good prediction accuracy for impact parameter with a mean absolute error about 0.4 fm for $2<b<12.5 fm$, while for central and peripheral collisions the performance gets worse. For HICs at LHC energies, the Gradient Boosted Decision Trees (GBDTs) were used~\cite{Mallick:2021wop} for impact parameter regression in Pb+Pb collisions at $\sqrt{s_{NN}}=5.03$ TeV with charged-particle multiplicity ($\langle dN_{ch}/d\eta\rangle$, $\langle N_{ch}^{TS}\rangle$) and mean transverse momentum ($\langle p_T\rangle$) as the input features, meanwhile, the transverse spherocity is obtained which characterize in two limits the hard and soft events\footnote{Transverse spherocity is defined for unit vector $\hat{\mathbf{n}}$ which minimizes the ratio $S_0=\frac{\pi^2}{4}(\frac{\sum_i\vec{p_{Ti}}\times \hat{\mathbf{n}} }{\sum_i p_{Ti}})^2$.}. In Ref.~\cite{Saha:2022skj}, besides the determination of impact parameter, two other quantities in characterizing the initial geometry--eccentricity and participant eccentricity, were also included as targets within ML-based regressions, where k-NearestNeighbors (kNN), ExtraTrees Regressor(ET) and the Random Forest Regressor(RF) models were employed based on the transverse momentum spectra as input features, model dependencies and generalizability of the trained were also discussed.  

\emph{\textbf{End-to-End b-meter with PointCloud Network}} ---
The detector's record in HIC has an inherent point cloud structure (as will also be discussed in Sec.~\ref{sec:2:eos:pcn}), which is defined as a collection of points as an unordered list with their record attributes, e.g., the position, charge, or momentum of particles. Such point cloud format in principle should hold the permutation invariance. Being specially developed, the PointNet provides an appropriate structure handling point cloud dataset and is meanwhile invariant under the ordering of the points (i.e., particles). Therefore, for HICs study, the PointNet-based models open up the possibility to work directly on the detector readout for physics exploration using pattern recognition strategy in the big data sense. As introduced in above, an accurate estimation of impact parameters on an event-by-event basis is non-trivial, much less the demand in working with detector output (hits or tracks) directly even before the particle identification. This actually forms an inverse problem, where the task of determining the initial impact parameter given purely detector output for the final state individual event is implicit by itself. In Ref.~\cite{OmanaKuttan:2020brq,OmanaKuttan:2021axp}, an end-to-end\footnote{Here \textbf{end-to-end} means the inference is performed on direct detector output without much preprocessing for the data.} impact parameter meter is devised with PointNet-based deep learning models and demonstrated for the Compressed Baryonic Matter (CBM) experiment. 
    
As is under construction within the FAIR program at GSI, CBM aims at studying the properties of strongly compressed nuclear matter through heavy ion collisions with beam energies ranging from 2 to 10$A$ GeV. The key feature of CBM experiment is that it will have high event rate and trigger rate rendering rare particle detection and high statistic evaluation for some observables (e.g., higher order fluctuations or correlations), which however calls for fast real-time analysis in selecting events from the flooding stream of data produced from the collision experiment. To this end, thus being able to work with directly the detector output, Ref.~\cite{OmanaKuttan:2020brq, OmanaKuttan:2021axp} adopted a supervised training strategy to construct an end-to-end impact parameter meter using PointNet, where the training data is prepared from UrQMD followed by the CBM detector simulation using CbmRoot. As input to the PointNet-based impact parameter estimator, each event is represented with particle hits or tracks information in point cloud format. One PointNet-based model consisting of two joint alignment networks as shown in Fig.~\ref{fig:pointnet} is constructed to capture the inverse mapping from the detector output of CBM to impact parameter and was trained supervisedly with training data simulated from UrQMD+CbmRoot. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp!]
    \centering
    \includegraphics[width = 0.8\textwidth]{figures/pointnet.jpg}
    \caption{General structure of joint alignment network which induce transformations on input or features inside the PointNet. Taken from Ref.~\cite{OmanaKuttan:2020brq} with permission.
    \label{fig:pointnet}}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 As seen from Fig.~\ref{fig:pointnet}, the adopted 1-D convolutions with kernels of size 1 across point features for individual points (particles) ensures the operations to be order invariant, after which symmetric function like global Average or Max pooling is used to aggregate all global features for further processing (usually use dense layers), thus preserves the order invariance for the model. Note that the used 1-D CNN is equivalent to applying a shared dense layer operation to lift the feature space of each particle to a high-dimensional transformed feature space.  As demonstrated in Ref.~\cite{OmanaKuttan:2020brq}, such PointNet-based model provides fast and accurate, end-to-end and event-by-event impact parameter determination, showing around $0.5 fm$ mean squared error. By using such a trained model, It is promising to access event-by-event centrality estimation from the hits record for CBM experiment.

\subsubsection{Unsupervised Centrality Outlier Detection}\label{outlier}
Though there is a well-established ``standard model'' for HICs simulations, due to the inaccessible first principle calculation to the collisional dynamics, new or not well-understood physics may be lacking in the models, e.g., the critical phenomenon. From the experimental point of view, in HICs the new and interesting physics might be hidden in rare events and/or rare particles and also their inter-correlations, such as higher-order cumulants of particle multiplicity distributions. In addressing such rare probes, \textit{large event rate} is scheduled especially for the new experiments, like CBM or PANDA at FAIR, the RHIC beam energy scan, and the ALICE experiment at CERN. Efficient online event selection is then in urgent need, which for example could pop out events potentially containing different characteristics or statistics as compared to the background. Experimentally the disability in this issue may induce artifacts for the interpretation of the observable, such as an imperfect centrality determination or detector malfunction's contamination induced different event types within a bulk background, manifested as two-bump distribution in the proton number distribution (which could also be induced by critical fluctuation physically), is discussed~\cite{Bzdak:2018uhv} to be able to explain the STAR observed deviation~\cite{Luo:2015ewa} for net-proton multiplicity distribution from simple binomial in Au+Au collisions at $\sqrt{s_{NN}}=7.7$ GeV. Otherwise, this deviation may also signal a critical endpoint in the QCD phase diagram.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!hbtp]\centering
\includegraphics[width=0.43\textwidth]{figures/reconstruction_loss-eps-converted-to.pdf}
\includegraphics[width=0.44\textwidth]{figures/ROC_curve-eps-converted-to.pdf}
\caption{Taken from Refs.~\cite{Thaprasop:2020mzp}. (left) the histogram of the RE in Eq.~\eqref{eq:re} for PCA using 2 PC; (right) the ROC curves for best models of each type: PAC, AEN-FC, AEN-CNN.\label{fig:outlier}}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In such a context, outlier detection which in ML community has been well-developed is proposed to detect anomaly event in HICs~\cite{Thaprasop:2020mzp}. As an exploratory study in testing the methodology, Ref.~\cite{Thaprasop:2020mzp} generated an ensemble of mixed events with the majority to be central collisions (with impact parameter $b=3$ fm) and a few portions to be peripheral ($b=7$ fm) using UrQMD transport model, with the ratio of the two centrality classes set to be able to give the anomalous proton number distribution observed from STAR. The task is unsupervisedly select the outlier--peripheral collision events out of the background--large number of central events. Each event is represented by the 2D-histograms of transverse momentum ($p_x, p_y$) with normalization performed to eliminate the easy characteristic of total number of particles per event in distinguishing the centrality classes. Two kinds of algorithms were discussed for this outlier detection, principal component analysis (PCA) and autoencoder network (AEN), both can achieve dimensionality reduction over the training data-set while the output (for PCA i.e., the inverse transformation) largely reproduce the input. In the reduced feature space (e.g., with just 2 or 3 dimensions), the feasible visualization provides the most simple way for data clustering and outlier identification, which however is limited by the expressibility of the used feature space. Another way to realize outlier detection with arbitrary latent feature space is utilizing the \textit{reconstruction error} (RE) of each event to guide the selection, e.g., the mean-squared error (see Fig.~\ref{fig:outlier} for its histograms and performance in identifying outlier), 
\begin{equation}
RE(x)=\frac{1}{N}\sqrt{\sum^N_{i=1} (x^{rec}_i-x_i)^2},
    \label{eq:re}
\end{equation}
where $x^{rec}_i$ is the $i^{th}$ component of the reconstructed event for $x$. Since the learning algorithms (PCA or AEN) is trained to reproduce the input event through reduced latent space on data-set with majority to be background types, the RE reflecting the reconstruction loss is expected to be different between two types of events if they possess distinct characteristics or statistics, thus provides also a promising indicator for anomaly identifier. It is found that the PCA with five principal components gives better outlier detection performance than complex AEN in the considered task~\cite{Thaprasop:2020mzp}.
 

\subsubsection{Nuclear Structure Inference}\label{hic_nuclear_structure}
The momentum distribution of final state hadrons produced in heavy ion collisions has a clear dependence on the initial nuclear structure, which provides an opportunity to determine the initial nuclear structure using the final state hadrons in heavy ion collisions~\cite{Bally:2022vgo}. The momentum anisotropy as a function of the charged particle multiplicity is quite different for Pb+Pb and U+U collisions, where $^{208}{\rm Pb}$ is a double magic nucleus whose shape is close to a perfect sphere, while $^{238}{\rm U}$ has a shape close to a prolate watermelon. As a result, the collision geometry is much more complex in ${\rm U}+{\rm U}$ collisions than in ${\rm Pb}+{\rm Pb}$ collisions. Of course, nuclear structure is not limited to shape deformations. Other nuclear structures include the neutron skin~\cite{PREX:2021umo}, which reveals the difference between the distribution of protons and neutrons in the nucleus, the $\alpha$ cluster, which is crucial for the light nucleus, and the nucleon-nucleon correlation.
A deep residual neural network is trained to predict the initial nuclear deformation~\cite{Pang:2019aqb} parameters $\beta_2$ and $\beta_4$. The network was found to obtain the absolute values of $\beta_2$ and $\beta_4$ using distributions of geometric eccentricity and total entropy. 
Deep learning is also used to classify whether there is $\alpha$ cluster structure in $^{12}{\rm C}$ and $^{16}{\rm O}$ using AMPT simulations of collisions between light nuclei \cite{Bailey2021,He:2021uko}.
    
One difficulty in determining the nuclear structure using high-energy heavy ion collisions is caused by the effect of special relativity. On one hand, the nuclear structure information along the beam direction is strongly destroyed due to Lorentz contraction. On the other hand, fluctuations of sea quarks and gluons will participate in the collision due to time dilation. The size of the nucleons inside the nucleus may increase due to the parton cloud surrounding the nucleons. This will definitely change the initial fluctuations in the overlapped region of the colliding nucleus, e.g., the sizes of the hot spots in the initial entropy-density distribution. Fortunately, the Trento Monte Carlo model has taken this change into account for the initial condition.

Another difficulty in determining nuclear structure from high-energy heavy ion collisions is due to the evolutionary processes that link the initial state to the final observed particles. Such processes include pre-hydro evolution, hydrodynamic expansion, and hadron scattering, where many transport coefficients are undetermined. It has been proposed~\cite{Jia:2021oyt} that such a subtlety might be avoided in a contrast experiment for isobar systems --- a pair of nuclei that have the same mass number but different electric charge, and therefore different nucleon structures. The ratio of the final state observables between the isobar collisions is expected to be sensitive only to the nucleon structure and not to the transport parameters. The natural question then arises as to whether it is possible to determine the nucleon structures of the isobar pair by focusing only on the ratio of the final state observables. In Ref.~\cite{Cheng:2023ciy}, the authors attempted to answer this question by taking the Monte Carlo Glauber model as an ``emulator'' to map the nuclear structure onto the final state particle distribution, and performing Bayesian inference of the nuclear structure parameters from different combinations of ``mock data''. The authors found that it was not possible to constrain the structure for both nuclei simultaneously if only the ratios of the multiplicity distribution and the elliptical, triangular and radial flows were fitted. However, the authors found that simultaneous reconstruction is plausible if one includes the multiplicity distributions for both collision systems. Such a pioneering investigation paves the way for further nuclear structure studies in HICs using Bayesian inference based on more computationally expensive but realistic models for the final state particle distribution.

\subsection{QCD Phase Diagram}
\label{qcd_phase_hic}

\subsubsection{Bayesian Analysis of QCD EoS at $\mu_{B}=0$}
Although the Lattice QCD predicts that the transition between QGP and HRG is a smooth crossover in the high temperature and zero baryon chemical potential region, there is no clear evidence from experimental data produced at RHIC and LHC. Ref.~\cite{Pratt:2015zsa} uses Bayesian analysis to extract the speed of sound square $c_s^2$ as a function of temperature, with the help of relativistic hydrodynamic simulations and experimental data. The $c_s^2 = {dP / d\epsilon}$ is a direct measure of QCD EoS, comparing $c_s^2(T)$ from data with that given by Lattice QCD provides direct evidence of a smooth crossover.

 \label{sec:2:eos_bayes}     
	\begin{figure}[htbp!]
    \centering
    \includegraphics[width = 0.62\textwidth]{figures/priorvpost50.png}
    \caption{The prior and the posterior of the speed of sound square $c_s^2$ of hot nuclear matter using Bayesian analysis from Ref.~\cite{Pratt:2015zsa}.
    \label{fig:bayes_eos}}
\end{figure}

The $c_s^2$ is parameterized as a function of energy density in the following,
\begin{equation}
c_s^2(\epsilon) =  c_s^2(\epsilon_h) + \left({1\over 3} -  c_s^2(\epsilon_h) \right) {X_0 x + x^2 \over X_0 x + x^2 + X'^2 }
\label{eq:cs2_vs_T}
\end{equation}
where $X_0 = \sqrt{12} R X' c_s(\epsilon_h)$, $x \equiv \ln \frac{\epsilon}{\epsilon_h}$, $\epsilon_h$ is the energy density at $T=165$ MeV, $R$ and $X'$ are the two parameters in the EoS to be determined. Randomly choosing $R$ and $X'$ from the range $-0.9 < R < 2$ and $0.5<X'<5$ generate the unconstrained EoS that varies in a large region between $c_s^2=0.05$ and $c_s^2=0.33$, as shown in Fig.~\ref{fig:bayes_eos}-a. This corresponds to the 
a priori distribution of $c_s^2$ parameters together with other 12 parameters in the model $P(\theta)$. 

The likelihood between experimental data and relativistic hydrodynamic outputs is given by,
\begin{align}
    P(D | \theta) = \Pi_i \exp\left( -(z_i(\theta)-z_{ i,{\rm exp}})^2 / 2\right)
\end{align}
where $D$ represent the experimental observables, $z_i(\theta)$ and $z_{i, {\rm exp}}$ are the principle components of $D$ from model outputs and the experimental data correspondingly. In Bayesian analysis, MCMC is used to sample $\theta$ from the posterior distribution of parameters $P(\theta | D)\propto P(D | \theta) P(\theta)$ to generate a batch of $c_s^2(T)$ curves. The EoS constrained by data is in good agreement with Lattice QCD calculations for $T>150$ MeV. 

Note that recently in Ref.~\cite{OmanaKuttan:2022aml} the Bayesian method has been successfully applied in constraining the density dependence of the QCD EoS for dense nuclear matter(at high $\mu_{B}$) based on low energy HICs experimental data. Specifically, the mean transverse kinetic energy and integrated elliptic flow of protons from HICs in the beam energy range $\sqrt{s_{NN}}=2\sim 10$ GeV are taken as the evidence for the inference. Up to around 4 times nuclear saturation density ($4\rho_{0}$) the EoS is extracted from the analysis which describes other observables (the directed flow $v_1$ and the $p_T$ dependent elliptic flow $v_2$).



\subsubsection{Identify QCD Phase Transitions using CNN}

Lattice QCD predicts that the transition between QGP and hadron resonance gas (HRG) at high temperature and zero baryon chemical potential is a smooth crossover~\cite{Ding:2015ona}. It also produces the QCD EoS, which describes the pressure as a function of energy density. Using Bayesian analysis, the QCD EoS can be parameterized and used in relativistic hydrodynamic simulations of the HIC to extract the QCD EoS at the highest RHIC and LHC energies. The extracted EoS is in agreement with lattice QCD calculations~\cite{Pratt:2015zsa}.
It is conjectured that at high baryon chemical potentials, the transition between QGP and HRG is a first order phase transition. The endpoint of the first-order phase transition close to the crossover is called the critical endpoint. Tremendous efforts have been made to look for this critical endpoint (CEP) or region, including also the deployment of machine learning techniques. 

Fig.~\ref{fig:phase_diag_eos} shows two different transition regions between QGP and hadron resonance gas (HRG) in the QCD phase diagram, the crossover region and the first order phase transition region. Different phase transitions lead to different equation of states  (EOS). 
For crossover EOS, the pressure as a function of energy density(the blue dashed curve) is smooth in the region between QGP phase at high energy density and HRG at low energy density. For first order phase transition (the red solid line), the pressure as a function of energy density has a plateau between QGP phase and HRG phase. As a result, the pressure gradient is zero in this region. Notice that the main driven force of fireball expansion is the pressure gradient. Difference of the pressure gradient in the phase transition regions between two EOS will lead to different evolution histories, which may encode the information of phase transition to the final state particles. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp!]
    \centering
    \includegraphics[width = 0.7\textwidth]{figures/qcd_phase_eos.jpg}
    \caption{A schematic chart for QCD transition in the QCD phase diagram and the associated equation of state.
    \label{fig:phase_diag_eos}}
\end{figure}

Fig.~\ref{fig:cnn_eos}-(a) compares the energy density distributions in the transverse plane for 4 time snapshots, for two different EOSs in CLVisc relativistic hydrodynamic simulations. The evolution histories are visually different and can be easily distinguished by human. The energy density distributions using a EOS with crossover as shown in the first row is much smoother than that shown in the second row for a first order phase transition. However, the QGP will convert to hadrons through particlization and hadronic cascade, as shown in Eq.~\ref{eq:freezeout}. 
What have been detected in experiment are the final state particles in momentum space. It is verified in ~\cite{Pang:2016vdc} that the event-by-event distributions of traditional observables for 2 different EOSs almost overlap. The shape of the event-by-event distribution are also sensitive to the initial condition, the shear viscosity and the freeze out temperature. It seems that there is no way to identify the used EOS from the spectra of a single collision event.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp!]
    \centering
    \includegraphics[width = \textwidth,trim=0 4cm 0 5cm, clip]{figures/dl_eos_ppnp.jpg}
    \caption{A schematic chart for QCD transition binary classification with CNN using final particle spectra from HIC as input.
    \label{fig:cnn_eos}}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Ref.~\cite{Pang:2016vdc} gives the first exploratory study of using deep learning techniques to directly connect the raw final state information from the HIC experiment to the bulk properties of this QCD phase transition type encoded in the EoS. Inspired by the success of image recognition in computer vision, a deep convolutional neural network(CNN) is employed to learn and to directly represent the potentially existing inverse mapping from the final state information to the early time QCD matter bulk properties. Specifically, the CNN is used to classify two phase transition regions using simulated data from the parallelized relativistic hydrodynamics on GPU~\cite{Pang:2016vdc, Pang:2019int, Zhou:2018hsl}, with different EoS and phase transition types embedded in. 

Fig.~\ref{fig:cnn_eos}-(c) shows the schematic flow chart of the CNN. The input to the CNN is the the $(p_T, \phi)$ distribution of final state pions simulated by the CLVisc hydrodynamic model, with fluctuating initial conditions and different values of shear viscosity. The two dimensional spectra has 15 different $p_T$ and 48 different azimuthal angle $\phi$ which serves as input images with $15\times 48$ pixels. The output of the CNN have 2 neurons, representing the probability of the crossover and the first order phase transition, given by softmax activation as shown in Eq.~\ref{eq:eos_2class},
\begin{align}
    \hat{y} = \begin{bmatrix}
      p_{\rm crossover} \\
      p_{\rm 1st\ order}
    \end{bmatrix} = \begin{bmatrix}
      {e^{z_1} \over e^{z_1} + e^{z_2}} \\
      {e^{z_2} \over e^{z_1} + e^{z_2}}
    \end{bmatrix} 
    \label{eq:eos_2class}
\end{align}
where $\hat{y}$ is the discrete probability density distribution predicted by the network. 
$z_1$ and $z_2$ are the values of the last two neurons before the softmax activation. Multi-layer CNN and MLP are used to extracts correlations between particles in different momentum space in the input image, that can be used to make a final decision in the output layer.

The true label for crossover is $y=\left[y_1, y_2\right]^T=\left[1, 0\right]^T$, indicating that $p_{\rm crossover} = 1$ and $p_{\rm 1st\ order}=0$. The true label for 1st order phase transition is thus $y=\left[y_1, y_2\right]^T=\left[0, 1\right]^T$. The loss function contains a cross entropy loss between 2 distributions and the $l_2$ regularization term,
\begin{align}
    l(\theta) = - {1 \over m} \sum_{i=1}^m \left(y^i_1 \log p^i_{\rm crossover} + y^i_2 \log p^i_{\rm 1st\ order}\right) + {\lambda \over 2}||\theta||_2^2 
\end{align}
where $\theta$ represents all the trainable parameters such as the matrix elements in the convolution kernels, the weights and bias in the MLP. $m$ is the mini-batch size. The index $i$ represents the $i$-th sample in the mini-batch. The $||\theta||_2 = \sqrt{\sum_k \theta_k^2}$ is the $l_2$-norm used to constrain the magnitude of model parameters. The $\lambda$ is a small number set manually. After trained, the network generalizes well to data generated with another relativistic hydrodynamic model (iEBE-VISHNU~\cite{Shen:2014vra}), or to data generated with CLVisc with different initial conditions. In average, the prediction accuracy achieves $93\%$. 
 
It is worth noting that, the demonstration in Ref.~\cite{Pang:2016vdc} of using deep CNN to identify the QCD transition types from the final pion's spectra is only on the level of pure hydrodynamic evolution, with its superb classification accuracy in the testing stage clearly indicating that: the early time transition information especially its types within hydrodynamics (mimicking HICs) evolution can survive to the final state. The constructed inverse mapping by the trained deep CNN also shows robustness to different initial fluctuations and shear viscosities. In modeling heavy ion collision with more realistic consideration, hybrid simulations combining hydrodynamics together with after-burner hadronic cascade transport become more appropriate. Accordingly, for HICs this strategy of using supervised learning to capture the inverse mapping from final accessible information to early time desired physics could be further deepened.

Indeed, later, the same method is used to classify the EoS using final state hadrons sampled from the freeze-out hypersurface and passing through the hadronic cascade via UrQMD~\cite{Du:2019civ}. Both the stochastic particlization being followed by hadronic rescattering and the resonance decay effects are taken into account in producing the final state pion spectra $\rho(p_T,\phi)$. Initially, it was found that the accuracy for CNN with event-by-event spectra is significantly lower than that using smooth particle spectra in pure hydrodynamic case. This performance decline manifests the concealing over the fingerprint of QCD transition inside final state, by the fluctuations due to finite particles and resonance decay. A scenario with event-fine-averaged spectra as input is investigated, where the performance is showed to be greatly improved by feeding such averaged spectra with 30 events within the same fine centrality bin into the deep CNN. See Fig.~\ref{hybrid_performance} for a performance comparison from Refs.~\cite{Du:2019civ,Du:2020poe}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{figure}[hbpt!]
\centering
\includegraphics[width=0.48\textwidth]{figures/hybrid_performance.pdf} 
\caption{Taken from ~\cite{Du:2019civ}. Comparison between the validation accuracy in all the different sub-scenarios studied. The green star depicts the pure hydrodynamic result~\cite{Pang:2016vdc}. The orange square, the purple triangle and the red filled circle symbols depict the results for the 30-events-fine-averaged, cascade-coarse-grained and event-by-event spectra, respectively, in different switching temperatures.}
\label{hybrid_performance}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Learning Stochastic Process with QCD Phase Transition}
The aforementioned EoS identification assumes that the phase transition process involved in the entire HICs evolution take place in equilibrium states. However, the actual collisional evolution dynamics, especially the phase transition process involved, should be a non-equilibrium evolutionary behavior.
It is nontrivial to uncover the phase transition and involved dynamical information from a stochastically evolving dynamical system. Refs.~\cite{Jiang:2021gsw,Wang:2021yjw} generalized the idea of identifying transition type from HICs final state further, to recognize the phase order and extract the dynamical parameters in a stochastic dynamical process would happen in HICs. The general thermodynamics and phase behavior of QCD can be reasonably approximated by a linear sigma model~\cite{Nahrgang:2011mg}, the effective potential of which describes the crossover transition at small chemical potentials and the first-order phase transition at large chemical potentials. As simplified modelling to the phase transition processes in HICs, the Langevin equation is adopted to describe the semi-classical evolution of the long wavelength mode of the sigma field, 
\begin{equation}
\partial ^{\mu }\partial _{\mu }\sigma \left( t,x\right) +\eta \partial
_{t}\sigma \left( t,x\right) +\frac{\delta V_{eff}\left( \sigma \right) }{
\delta \sigma }=\xi \left( t,x\right),
\label{eq:langevin}
\end{equation}
with the effective potential $V_{eff}$ controlling the type of phase transition in the stochastic process. The damping coefficient $\eta$, and the noise term $\xi \left( t,x\right)$, follow the fluctuation-dissipation theorem. As a simplified description for HICs, Hubble-like decaying temperature field and constant baryon chemical potential are assumed for the heat bath. A Gaussian-type spatial noise with different overall strengths is adopted for $\xi$ to inject fluctuations associated with the phase transition.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!hbtp]\centering
\includegraphics[width=0.33\textwidth]{figures/CNNsLangevin.pdf}
\includegraphics[width=0.34\textwidth]{figures/testlossacc-eps-converted-to.pdf}
\includegraphics[width=0.31\textwidth]{figures/test_rev-eps-converted-to.pdf}
\caption{Taken from Refs.~\cite{Jiang:2021gsw,Wang:2021yjw}. (left) the deep CNN for phase transition identification and damping coefficient regression; (middle) phase order classification accuracy in the testing stage with different spatial noise magnitude B; (right) the regression performance for damping coefficient during training and comparison to ground truth.\label{fig:langevin}}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Then a deep CNN, as shown in the left panel of Fig.~\ref{fig:langevin}, is devised to detect possible QCD phase transition order and predict the damping coefficient from the recorded $\sigma$ field spatio-temporal configurations in later stage of the non-equilibrium evolution. It is shown that the trained CNN is able to identify the encoded phase transition to be cross-over or first-order based on the final stage stochastic evolution of the field, which works even on different magnitude of noise, see middle panel of Fig.~\ref{fig:langevin}. For the damping coefficient prediction, the network gives a performance with on average $R^2=0.97$ in the scenario of first-order phase transition, as shown in the right panel of Fig.~\ref{fig:langevin}.

\subsubsection{Point Cloud Network for Spinodal Clumping Identification}
\label{sec:2:eos:pcn}
Several follow-up studies used convolutional neural networks (CNNs) to map final particle spectra to EoS types within hadronic transport models~\cite{Kvasiuk:2020izb,Sergeev:2020fir,Wang:2020tgb} or nuclear symmetry energy~\cite{Wang:2021xbb}. The CNN is the state-of-the-art for image recognition, so the momenta of the final state particles have to be converted into images. This is done by density estimation in $(p_x, p_y)$ space or $(p_t, \phi)$ space using 2-dimensional histograms. In both cases, there is a loss of information just from this pre-processing to the final state. As also been introduced in Sec.~\ref{hic_b}, the natural data structure of particles in momentum space is a \textit{point cloud}, where each particle is a point in momentum and feature space. The \textit{PointCloud network} is used to classify two first-order phase transitions with Maxwell and spinodal constructions~\cite{Steinheimer:2019iso}. With the latter construction, QGP evolves into many blobs that hadronize separately. However, it is rather  difficult  to distinguish the equation-of-state type from the final state hadrons in momentum space. Another point cloud neural network is trained using reconstructed tracks of final state hadrons given by the CBM detector simulations~\cite{OmanaKuttan:2020btb}. It was found that the performance decreases when considering realistic detector acceptance cuts and resolution. On the other hand, the performance improves when multiple events are combined. 

Shown in Fig.~\ref{fig:pointcloud} is a  simple point  cloud  network used classify  two  different  equation  of state. The point cloud network uses an MLP to transform the momentum and species information of each particle into a high-dimensional feature space (128 dimensions in this example). This MLP is shared by all particles in the cloud, so it is also called a 1-dimensional CNN.  Notice  that  CNN is not a specific technique  for image  processing,  any operations having  the  properties of local connection and weight sharing can be called CNN. Afterwards, a permutation-symmetric operations is used here to extract the multi-particle correlations hidden in the point cloud. A commonly used  permutation-symmetric operation  is the Global Max Pooling that is able to extract the boundaries of this point cloud in high-dimensional feature space, as shown below,
\begin{align}
f_i = max(\{f_{ij}\}, {\rm along}=j)
\end{align}
where $\{ f_{ij} \}$ is the feature matrix with $i$ denoting the i-th feature, $j$ representing the j-th particle and $f_{ij} = {\rm MLP}_i(\vec{p}_j)$, $f_i$ is the maximum value of the i-th feature among all the particles in the cloud,  extracted by the Global Max Pooling. Note that other permutation-symmetric operations can be used here to replace the global max-pooling. For example, the widely used Global Average Pooling is shown below, 
\begin{align}
f_i = {1 \over N}\sum_{j=1}^N f_{ij}
\end{align}
where $N$ is the number of particles in the cloud.

\begin{figure}[htbp!]
\centering
\includegraphics[width = 0.8\textwidth]{figures/point_cloud_network_structure.jpeg}
\caption{A simple  point cloud network used  to  classify two kinds  of  first  order  phase  transitions.
\label{fig:pointcloud}}
\end{figure}

In principle, the point cloud network trained with 100 particles can be applied to a point cloud  with 1000 or more particles,  as the MLP is  shared by all particles and the permutation-symmetric operation is not sensitive to the number of particles. The number of particles may be different for different particle clouds produced in different HIC events. However, implementing a deep neural network with varying number of input particles during training meet technical difficulties, because a mini-batch of input data are required to have the same shape for GPU parallel acceleration.  Padding is usually used to produce the same number of particles in each cloud. There are different ways of padding, e.g., a list of virtual particles whose properties are all zeros, or particles from other events can be added which have no correlation with particles in the current event. To eliminate the effect of padding particles, another property can be added to each particle, called a "mask". The masks are equal to 1 for real particles in the cloud, but 0 for padding particles. Using these masks, the padding particles will not participate in the global max(average) pooling.

\subsubsection{Dynamical Edge Convolution Network for Critical Similarity}
There are rich critical phenomena near the critical endpoint, such as fluctuation enhancement and the emergence of self-similarity. The self-similarity in momentum space is modelled by Critical Monte Carlo (CMC) and encoded in the interparticle distances in momentum space. If self-similarity is present in only a few percent of all final state hadrons, the signal-to-background ratio is small. Traditionally, intermittency analysis is used to search for self-similarity in momentum space. However, this method fails at the current experimental $p_T$ resolution of about $0.1$ GeV/$c$. A point cloud network and a dynamical edge convolution neural network are used to identify events with interparticle similarity and to label correlated particles~\cite{Huang:2021iux}. 
The dynamical edge convolution network succeeds with a test accuracy of $92\%$ on events with $5\%$ signal particles. The method developed here appears to be powerful in searching for multi-particle correlations that are sensitive to specific physics.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp!]
\centering
\includegraphics[width = 0.9\textwidth]{figures/DynamicalEdgeCNN_Structure_PPNP.jpg}
\caption{A schematic diagram of critical self-similarity search with a dynamical edge convolutional neural network using the HIC particle cloud as input. Taken from~\cite{Huang:2021iux}.
\label{fig:decnn}}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Fig.~\ref{fig:decnn} shows the structure of the dynamical edge convolution neural network used to search for multi-particle correlations in the particle cloud.
The input to the network is a particle cloud produced in heavy ion collisions, with multiple features for each particle. The output of the network has two branches, one for classification and the other for tagging. The dynamic edge convolutional neural network extracts correlations between multiple particles to build a map between the input and output. Each input particle can have the four momenta, the charge, the baryon number, or the strange number as properties. In practice, the four momentum is usually normalized to the maximum energy of all particles in the cloud. Other quantum numbers can also be normalized to values between $-1$ and $1$.

In the dynamic edge convolution network, the k nearest neighbors of each particle are used to capture the multi-particle correlation.
In the first layer, the distances between different particles in coordinate or momentum space are computed to explicitly determine the neighbors. 
A fully connected neural network (MLP) is used to compute the correlation between the central particle and one of its neighbors, using the following formula
\begin{align}
    e_{ij} = {\rm MLP}({p}_i, p_j)
\end{align}
where $e_{ij}$ is the edge feature between particle $i$ and particle $j$, the $p_i$ represent the input features of particle $i$. 
This MLP network is shared by each pair of particles to produce edge features. 
The name "edge convolution" comes from the fact that the MLP is locally connected and its weights are shared among all edges.
After the first edge convolution, there is a point cloud network that transforms the input of each particle together with its neighbors into a high-dimensional latent space, whose input is a concatenation of the central particle and its k edge features,
\begin{align}
    f_i = [p_i, e_{i1}, e_{i2}, ..., e_{ik}].
\end{align}
Using the features $f_i$ in the high-dimensional latent space, the edge convolution can be applied again and again. As a result, the k nearest neighbors of each particle are determined by the distances between different particles in feature space afterwards. In other words, particles can be close in feature space but far apart in momentum space, allowing the network to capture long-range multi-particle correlation, which is critical for some specific classification or tagging tasks.

\subsubsection{Active Learning for Unstable Regions in QCD EoS}

Active learning is a sub-field of machine learning which can get higher accuracy using fewer labels \cite{Burr2009}.
This is achieved by allowing the machine to choose data from which it learns.
The motivation of using active learning is that labels may be difficult to obtain in some tasks (expensive or time consuming) and
the machine can learn better on some instances than on others.  
Active learning starts with a small labeled data set for supervised training. The trained model is used to make predictions about samples from a large unlabeled pool, as Figure~\ref{fig:aclearn} shows. If the network is uncertain about a sample, e.g., the sample is predicted to be stable with probability $51\%$ and unstable with probability $49\%$, it is labeled and added to the labeled dataset for further supervised training. Labels can be made by human in the loop or by computer programs. The key of active learning is to propose samples for labelling, the first method is to use the uncertainty criterion as mentioned above, which is the entropy of the prediction,
\begin{align}
    s = - \sum_i p_i \log p_i
\end{align}
where $p_i$ is the predicted probability that the sample is in the $i$th category. 
The entropy $s$ is known to be maximized if the predicted distribution is uniform.
The entropy criterion is similar to the margin criterion $m = p_1 - p_2$,
where $p_1$ and $p_2$ are the first and the second most probable category labels for input $x$, predicted by the pretrained network.
There are other criterions based on which the machines make their proposal.
For example, four neural networks can be trained in parallel and vote on samples from unsupervised pool.
If their judgements disagree the most on one sample, that sample will be voted for labelling.
Another criterion is the gradient which quantifies how much the machine will learn from this instance.
The negative gradients $-\partial l / \partial \theta$ have dependence on the input,
it is thus possible to choose input that leads to the largest change to the network for labelling.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp!]
    \centering
    \includegraphics[width = 0.65\textwidth]{figures/ActiveLearningNetworkStructure.png}
    \caption{A schematic diagram of active learning.
    \label{fig:aclearn}}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Ref.~\cite{mroczek2022} uses active learning to detect thermodynamically unstable and acausal regions in QCD EoS. 
Lattice QCD fails to produce the QCD EoS around the critical endpoint region due to the sign problem (it will be discussed in the next section). The Beam Energy Scan Theory (BEST) collaboration~\cite{An:2021wof} constructs a QCD EoS by mapping the 3D Ising model and the lattice QCD EoS. However, the shape and position of the critical region depend on several parameters. Some parameter combinations lead to unstable and acausal QCD EoS that should be discarded in the future model to data comparisons, as the hybrid model of HIC is computationally expensive and sensitive to acausal EoS in the medium.

The pressure is a sum of the contributions from Lattice QCD and the 3D Ising model as demonstrated in~\cite{An:2021wof}, 
\begin{align}
P(T, \mu) = P_{\rm lattice}(T, \mu) + P_{\rm Ising}(T, \mu)
\end{align}
where the pressure from Lattice QCD calculations at non-zero baryon chemical potential $\mu$ is given by the Tylor expansion around $\mu=0$,
\begin{align}
{P_{\rm lattice}(T, \mu) \over T^4 } =  \sum_n c_n(T) ({\mu \over T})^n
\end{align}
where the coefficients are $c_n(T) = {1\over n!} \chi^B_n(T) = {1 \over n!}{\partial^n P/T^4 \over \partial (\mu/T)^n }$,
the $\chi_n^B$ is the $n$th order baryon susceptibility.
Using the pressure provided above, the complete thermal dynamic relations are given below,  
\begin{align}
{n_B(T, \mu) \over T^3} &= {1 \over T^3}\left({\partial P(T, \mu) \over \partial \mu}\right)_{T} \\
{s(T, \mu) \over T^3} &= {1 \over T^3}\left({\partial P(T, \mu) \over \partial T}\right)_{\mu} \\
{\epsilon(T, \mu) \over T^4} &= {s(T, \mu) \over T^3} - {P(T, \mu) \over T^4 } + {\mu \over T}{n_B \over T^3}
\end{align}

There are $4$ parameters in total, whose combinations determine the size and the shape of critical endpoint in QCD EoS,
as well as the stability and the causality of the derived EoS,
\begin{align}
\left(\mu, \alpha_{\text {diff }}, w, \rho\right) \mapsto P\left(T, \mu\right) \mapsto\{\rm acceptable, unstable, acausal \}
\end{align}
where stability requires the positivity of the energy density $\epsilon$, the pressure $P$, the entropy density $s$, the net baryon density $n_B$,
the second order baryon susceptibility $\chi_2^B$ and the heat capacity $\left({\partial s \over \partial T}\right)_{n_B}$. 
The causality requires that the speed of sound square satisfies $0 \le c_s^2 \le 1$.
Any parameter combinations that lead to unstable or acausal EoS will be discarded.

\subsection{Dynamical Properties of QCD Matter}

\subsubsection{Shear and Bulk Viscosity}
\label{transport}
Relativistic hydrodynamic simulations indicate that the QGP produced in HIC is a strongly coupled plasma~\cite{Heinz:2005bw, Romatschke:2007mq, Song:2010mg} whose shear viscosity over the entropy density ratio $\eta/s$ is close to the universal limit ${1\over 4\pi}$ by AdS/CFT calculations~\cite{Buchel:2003tz}. Both $\eta/s$ and $\zeta/s$ strongly influence the collective expansion of hot nuclear matter~\cite{Teaney:2003kp}, leaving clear signals in the momentum anisotropy of final state hadrons produced in HIC. However, to quantitatively determine the values of $\eta/s$ and $\zeta/s$ from the final state hadrons, one faces the difficult inverse problem described above. There are two difficulties, the entanglement between different model parameters and the temperature dependence of $\eta/s(T)$ and $\zeta/s(T)$. Ref.~\cite{Niemi:2015qia} shows that relativistic hydrodynamic simulations with different fluctuating initial conditions can all describe data, with different effective shear viscosity. For example, using ``IPGlasma'' initial condition in MUSIC hydrodynamic model, the simulation describes the event by event fluctuations of $v_n$,
the mean $v_n$ and the $v_n(p_T)$ with constant $\eta/s=0.12$ at RHIC energy and $\eta/s=0.2$ at LHC energy~\cite{Gale:2012rq}. Using ``MC-KLN'' initial condition in VISHNU hydrodynamic model, the simulation describes the charged multiplicity, 
the $p_T$ spectra as well as the elliptic flow at both RHIC and LHC energy, with constant shear viscosity to entropy ratio $\eta/s=0.16$~\cite{Song:2013qma}. Using NLO improved EKRT initial condition in viscous hydrodynamics, it was observed that both the
 constant $\eta/s=0.2$ and a temperature dependent $\eta/s(T)$ give equal good overall fitting to RHIC and LHC data. The used linear parametrization is $0.12 < \eta/s < 0.12 + (0.18/320)(T /\text{MeV} - 180) $ for temperature above 180 MeV in the QGP phase and $\eta/s(T) = 0.12 - (0.20/80)(T /\text{MeV} - 180)$ for temperature below 180 MeV in the hadron resonance gas phase. To quantitatively constrain the temperature-dependent shear and bulk viscosity, a global analysis on all available RHIC and LHC data using Bayesian parameter estimation is required~\cite{Novak:2013bqa,Pratt:2015zsa,Bernhard:2016tnd, Bernhard:2019bmu,JETSCAPE:2020shq, JETSCAPE:2020mzn,Nijs:2020ors, Nijs:2020roc}.

Ref.~\cite{Bernhard:2016tnd, Bernhard:2019bmu} use Trento + IEBE-VishNew + UrQMD hybrid model to do a global fitting to the charged multiplicity, the transverse momentum spectra, the centrality dependence of the elliptic flow and the triangular flow, which helps to constrain both the parameters in the initial condition and the temperature-dependent shear and bulk viscosity. The study shows that the extracted initial condition agrees with the gluon saturation model, and there is a clear signal of non-zero bulk viscosity.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp!]
    \centering
    \includegraphics[width = 0.95\textwidth]{figures/posterior.pdf}
    \caption{The posterior distribution of nine model parameters for initial condition and transport coefficients using Bayesian analysis. Taken from~\cite{Bernhard:2016tnd}.
    \label{fig:bayes}}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
A set of nine model parameters in total are constrained using Bayesian analysis, whose posterior distributions are given in Fig.~\ref{fig:bayes}. These parameters include:
\begin{enumerate}
    \item Norm, the overall normalization factor determining the initial total entropy that is responsible for the multiplicity of final state hadrons;
    \item $p$, the entropy deposition parameter, which is used to mimic different kinds of initial entropy production, from MC-Glauber to saturation-based models;
    \item $k$, multiplicity fluctuation shape parameter, which determines the multiplicity distribution in minimum-bias proton+proton collisions;
    \item $w$, Gaussian nuclear width, which determines the size of nucleons during collision;
    \item $(\eta/s)_{\rm hrg}$, a constant parameter for the value of $\eta/s$ in hadron resonance gas phase;
    \item $(\eta/s)_{\rm min}$, the minimum $\eta/s$ at the critical temperature $T_c=154$ MeV;
    \item $(\eta/s)_{\rm slope}$, the slope of $\eta/s(T)$ above the critical temperature $T_c$;
    \item $\zeta/s$ norm, one overall normalization factor for the given $\zeta/s(T)$ function;
    \item $T_{\rm switch}$, the particlization temperature.
\end{enumerate}

These parameters form a nine-dimensional parameter space. Each point in this space represents one combination of model parameters. Without constraining, values of these parameters vary in a given range provided by physical a priori. According to Bayesian parameter estimation, un-normalized posterior distributions of these parameters are given by $P(\theta | D) \propto P(D |\theta) P(\theta)$, where the experimental data $D$ used in the Bayes formula is composed of $dN/dy$, mean transverse momentum $\langle p_T \rangle$ for $\pi^{\pm}$, $K^{\pm}$, $p\bar{p}$ at mid-rapidity and two-particle cumulants $v_n\{2\}$ with $n=2$, $3$, $4$ of charged hadrons, at various different centrality classes. The likelihood $P(D |\theta) $ is computed using the model output and experimental data. The prior $P(\theta)$ is usually chosen to be a uniform distribution. It shall be worth mentioning that the computationally expensive HIC simulations are run for a finite number of parameter sets and then emulated by a Gaussian process (see Sec.~\ref{sec:hic:emulator} for more details).

The diagonal subplots in Fig.~\ref{fig:bayes} show the marginal distributions of these 9 parameters. The posterior distribution in red color is constrained using charged particles, while the distribution in blue color is constrained using identified particles. For many parameters, the optimal values differ using these two groups of experimental data. However, the marginal distributions of $p$ and $\zeta/s$ norm have narrow peaks whose locations agree with each other using two groups of data. The results indicate that the initial condition agrees with saturation-based models, and the bulk viscosity is clearly non-zero. 

Follow up Bayesian Inferences of transport parameters are performed by the JETSCAPE collaboration~\cite{JETSCAPE:2020shq, JETSCAPE:2020mzn} and Nijs et al~\cite{Nijs:2020ors, Nijs:2020roc}. These independent researches take different experimental results as evidence and focus on different parameters. The former focused on $p_T$ averaged observables in both RHIC and LHC energies, whereas the latter looked into the $p_T$ differential ones in LHC collisions. A clear tension is observed in the bulk viscosity, which reveals the model dependence in the parameter extraction and calls for further investigation.

Bayesian Inference provides a statistically systematic way to make full use of the low-dimensional observables\footnote{i.e., preprocessed and projected from original detector raw record to expert designed quantities like elliptic flow or $p_T$ spectra within rapidity cut and centrality bin.} provided by experiment as well as the physical prior, where the uncertainty of the inference can be properly estimated with the experimental error and theoretical modelling error being able to be incorporated. However, most of the Bayesian analysis in HICs use handcrafted parametrizations for many of the inference targets (e.g., EoS or shear viscosity's temperature dependence) thus the final results may be limited or dependent on the setup and also priors. The other concern lies in the probable information loss of using only low-dimensional processed ``observations'' instead of the original high-dimensional raw record. A naive pushing forward for Bayesian inference to, e.g., event-by-event quantity analysis would hugely increase the computational demanding and time. Direct mapping capturing by deep learning or combining deep generative models into traditional Bayesian Inference are thus necessary alternatives.

\subsubsection{Jet Energy Loss in Hot QGP}\label{hic_jet}
    
As energetic partons pass through the hot QGP, they lose energy through elastic scattering and inelastic gluon radiation. The energy loss of the jet thus serves as a good probe of the properties of the medium. Bayesian analysis is used to study the temperature and momentum dependence of the heavy quark diffusion parameter~\cite{Xu:2017obm}, the jet quenching or transverse diffusion coefficient $\hat{q}$~\cite{Soltz:2019aea} and the jet energy loss distributions~\cite{He:2018gks}. Bayesian analysis typically assumes a parameterized function whose parameters are determined by maximum a posteriori (MAP) estimation. Recently, the information field has been used to generate random functions that can be used as a model for Bayesian analysis to obtain the temperature-dependent $\hat{q}(T)/T^3$ using both RHIC and LHC data~\cite{Xie:2022ght}. The information field provides a non-parametric representation of the unknown function, which can eliminate the unnecessary long-range correlations in the prior. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp!]
    \centering
    \includegraphics[width = 0.6\textwidth]{figures/MCMC_InclusiveJet.pdf}
    \caption{The extracted jet energy loss distribution and the computed $R_{AA}$ as compared with Linear Boltzmann Transport model calculations. Taken from~\cite{He:2018gks}.
    \label{fig:jet_bayes}}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Fig.~\ref{fig:jet_bayes} shows the application of Bayesian parameter estimation in determining the jet energy loss distribution~\cite{He:2018gks}. The jet cross-section is approximated by the convolution between p+p jet cross-section and the jet energy loss distribution function, where the latter is parameterized using a Gamma distribution function,
\begin{align}
    W_{AA}(x) = {\alpha^{\alpha} x^{\alpha - 1} e^{-\alpha x} \over \Gamma(\alpha)},
\end{align}
where $x= \Delta p_T / \langle \Delta p_T \rangle$, $\alpha$ can be interpreted as the average number of jet-medium scatterings that take energy out of the jet cone. The energy per scattering is thus $\langle p_T \rangle / \alpha$ with the mean transverse momentum loss as a function of $p_T$ defined as follows,
\begin{align}
   \langle \Delta p_T \rangle (p_T) = \beta\, p_T^{\gamma}\, \log p_T.
\end{align}

Note that the function form of the averaged energy loss is motivated by theoretical calculations~\cite{He:2015pra}. There are thus three parameters $(\alpha, \beta, \gamma)$ in the modification factor $R_{AA}$ to be constrained using the Bayesian analysis. The first row of Fig.~\ref{fig:jet_bayes} demonstrate that the $R_{AA}$ calculated from the constrained jet energy loss distribution is in good agreement with experimental data in Pb+Pb 2.76 and 5.02 TeV collisions. The second and the third row compare the mean  $p_T$ loss and the energy loss distribution $W_{AA}(x)$, as compared with Monte Carlo simulations using the Linear Boltzmann Transport model. The extracted value of $\alpha$ is small, indicating a large transverse momentum loss per scattering.
The results suggest that the observed jet quenching is mainly caused by a few out-of-cone scatterings.

\subsubsection{Jet Classification and Jet  Tomography}
Various studies have  been carried  out  to study jets in particle physics\cite{Larkoski:2017jix,Louppe:2017ipp,Dreyer:2021hhr,deLima:2021fwm,Romero:2021qlf,Konar:2021zdg,Karagiorgi:2021ngt,Nguyen:2021xnq,Luchmann:2022iih,Gong:2022lye,Bedeschi:2022rnj,Qu:2022mxj,CMS:2022wjj,Cal:2022fnm,Cranmer:2021gdt,Rossi:2023qvf} and heavy ion collisions \cite{Chien:2018rgm,Du:2021qwv,Apolinario:2021olp,Du:2020pmp, Du:2021pqa,Yang:2022yfr}, please refer \cite{Feickert:2021ajf} for a more complete list. The machine learning based jet-flavor and event classifications may find their applications in the future electron-ion collider\cite{Lee:2022kdn}. The machine learning-assisted jet-flavor tagging, such as the u-d, ud-s, uds-c binary classification may be important to determine the collinear and transverse momentum dependent PDFs~\cite{Arratia2020CharmJA}. The  ${\rm qq/q\bar{q}-g}$ classification and the direct .vs. resolved photon classification may provide additional constraints to the parton-in-photon PDFs. The classifications of e+P and e+A collisions may help to find observables that are sensitive to the cold nuclear effect. It is also proposed that the charm and anti-charm tagging in di-jet events can help to constrain the gluon TMD and the gluon Sivers function~\cite{Arrington:2021yeb}. Jet substructures are proposed to be able to constrain the gluon PDF~\cite{Caletti:2021ysv}.

Ref.~\cite{Apolinario:2021olp} uses deep learning to discriminate the vacuum-like jet (p+p collisions) and the jet in medium (Pb+Pb collisions). This helps to identify the effect of jet quenching in the presence of QGP in Pb+Pb collisions. The lost energy as well as the charged particle distributions inside a jet cone can be used as input to a deep neural network to predict the initial jet energy and their production positions~\cite{Du:2020pmp, Du:2021pqa, Yang:2022yfr}. The machine learning assisted jet tomography can be used to select jet events with similar initial energy or production locations, for a  more detailed differential study.

The machine learning  assisted jet tomography is employed to study Mach cones produced in high energy heavy ion  collisions \cite{Yang:2022yfr}. Mach cones  are  expected  to form in the smallest nuclear liquid droplet when  energetic partons traverse through QGP and  deposite   energy  and momentum  in  QGP \cite{Baumgardt:1975qv,Rischke:1990jy,Casalderrey-Solana:2004fdk,Satarov:2005mv,Dremin:2005an,Koch:2005sx,Ma:2006mz,Gubser:2007ga,Betz:2008js,Neufeld:2008dx,Torrieri:2008aqg,Qin:2009uh,Roy:2009cc}. The speed of hard partons is close to the  speed of the  light,  which is  much larger than the speed of sound $c_S$ of QGP. There  is a simple formula that relates the  half-angle $\theta$ of Mach cone to the speed of sound $c_s$ and the nuclear equation of  state (pressure  as a  function of energy  density),
\begin{align}
    {d  P  \over d \epsilon}  =   c_s^2  = \sin^2 \theta 
\end{align}
where $P$ is  the  local  pressure and $\epsilon$ is the  local energy density of hot  nuclear  matter. The Mach cone  angle thus provides a direct probe of the  QCD  EoS,  if its  effects on  the final state  hadrons are  well understood.

In practice, the Mach cones are difficult to find because the jets are generated at different positions and travel in different directions \cite{Satarov:2005mv}. The shape of  the Mach  cones will be affected by  both  the  tempperature gradients  and the strong collective flow  \cite{Tachibana:2015qxa}. As  shown  in  Fig.~\ref{fig:machcones}, the medium response is stronger for jets that travel a longer path length (jets initiated from $y=-3.0$) than those travel a shorter path length (jets initiated from $y=3.0$) in QGP. On the other hand, the shape of the Mach cone and the associated diffusion wake is deformed depending on the initial jet production position and the collective flow on the jet trajectory. For jet initiated from $x=-3, y=-3$, the medium response is stronger for the right wave front than its left branch. This is caused by the increased temperature and parton density in the central region of QGP, which leads to increased number of scatterings between jet parton and thermal partons. The diffusion wake is also affected by the strong radial flow which pushes the medium response outwards. One can imagine that the effect of medium response, the signals of Mach cone and the diffusion wake will all be amplified if it is possible to determine the initial jet production positions for event selection.

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=13.0cm]{figures/machcones.pdf}
    \setlength{\belowcaptionskip}{-0.cm}
    \caption{The shape of Mach cones for jets that are initiated at different positions and travel in the same positive-y direction. Reproduced from Ref.~\cite{Yang:2022yfr}.}
    \label{fig:machcones}
\end{figure}

Authors in Ref.~\cite{Yang:2022yfr} use a point cloud network to determine the initial jet production positions, which helps to select jets with similar path lengths and directions that is shown to be able to amplify the signal of 3D mach cones. The network in this study uses two different kinds of inputs, one is the point cloud in momentum space for particles with $p_T>2$ GeV, the other is the global information of $\gamma$-trigger and the full jet. These features are manipulated differently. The particle cloud goes through a point cloud network and global max pooling, while the global information is processed using a simple MLP. Two inputs are concatenated to make the final decision. The neural network output two real numbers to represent the initial jet production locations. This is a regression task that is solved using supervised learning. The training data is generated using LBT and CLVisc. After trained, the network is able to predict the initial jet production positions approximately.

The machine learning assisted jet tomography is used to select events for jets initiated from the same region. Fig.~\ref{fig:jet_vs_pos} shows the angular correlation between the final state hadrons and the jet, for jet partons initiated from 4 different regions. The magnitudes of the angular correlation in sub-figures (f) and (h) are stronger than (e) and (g), as the path lengths in these events are selected to be larger. The suppression of hadrons in the opposite direction of jets is caused by the diffusion wake. Constraining initial jets to the left half of the QGP fireball will introduce asymmetric angular correlation, which reflects the effect of radial flow to the diffusion wake. Observing these features in experimental data using machine-learning assisted jet tomography will provide direct evidence of the existence of Mach cones and diffusion wake. 

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/dndphi_vs_jetpos.pdf}
    \setlength{\belowcaptionskip}{-0.cm}
    \caption{The hadron-jet angular correlation for selected jet events that are initiated from different regions in the transverse plane. Reproduced from Ref.~\cite{Yang:2022yfr}.}
    \label{fig:jet_vs_pos}
\end{figure}


\subsubsection{Chiral Magnetic Effect Detection}\label{hic_cme}

The chiral magnetic effect (CME) is a phenomenon that occurs in chiral matter, where the combination of a chiral anomaly and a magnetic field leads to the separation of electric charges along the direction of the magnetic field~\cite{Fukushima:2008xe}. The CME is important in the study of the quark-gluon plasma (QGP) created in HIC experiments~\cite{Kharzeev:2013ffa}. Recently, researchers have been trying to search for this effect in HICs~\cite{Kharzeev:2015znc, Zhao:2019hta, Li:2020dwr}, but the traditional methods (e.g., $\gamma$-correlator) have proven to introduce large amounts of background contamination, e.g., the elliptic flow, global polarization and indeterminate background noises. Zhao et al. have developed a \textit{CME-meter} using a deep convolutional neural network (CNN), to analyze the entire final-state hadronic spectrum as big data and reveal the distinctive signatures of CME.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp!]
    \centering
    \includegraphics[width=12.0cm]{figures/fig_2-7-1_CME.pdf}
    \setlength{\belowcaptionskip}{-0.cm}
    \caption{The convolutional neural network architecture with $\pi^+$ and $\pi^-$ spectra $\rho^{\pm}(p_T, \phi)$ as input. Reproduced from Ref.~\cite{Zhao:2021yjo}.}
    \label{fig:CNN}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

They trained the meter using data prepared from a multiphase transport model(AMPT)~\cite{Lin:2004en, Ma:2011uma, Jin:2018fwq} and found that it was accurate at identifying the CME-related charge separation in the final-state pion spectra. In data preparation, for CME events, the y-components of momenta of a fraction of downward moving light quarks and their corresponding anti-quarks were switched to upward moving direction, with the CS fraction, $f$, defining the events as ``no CS'' (labeled as ``0'') for those with $f=0$, and ``CS'' (labeled as ``1'') for those with $f>0$. Each event was represented as two-dimensional transverse momentum and azimuthal angle spectra of charged pions in the final state, $\rho_{\pi}(p_T,\phi)$. The training set includes multiple collision beam energies and centralities to ensure diversity in the dataset.  With two different charge separation fractions as super parameters of the meter (5\% and 10\%), the meter is trained to recognize the CME signal under supervision. The sketch of the framework is shown in Fig.~\ref{fig:CNN} and the detailed architecture can be found in Ref.~\cite{Zhao:2021yjo}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!hbtp]\centering
    \includegraphics[width=0.38\textwidth]{figures/fig_2-7-2_Acc_cen_0327-eps-converted-to.pdf}
    \includegraphics[width=0.38\textwidth]{figures/fig_2-7-3_Acc_eng_0327-eps-converted-to.pdf}
\caption{Taken from Refs.~\cite{Zhao:2021yjo} with permission. (left) The test accuracy of models on different data-sets containing mixed collision energies along with centralities; (right) the test accuracy of models on different data-sets containing mixed centralities along with collision energies.\label{fig:acc}}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The well-trained CNN models are denoted as (0\%+5\%) and (0\%+10\%), respectively, in Fig.~\ref{fig:acc}. Their ability to recognize the CS signal is also shown in the figure. Although there is a discrepancy between the two models, they both display robust performance under varying collision conditions, such as different $\sqrt{s_{NN}}$ and centralities. This indicates that the CS signals are not completely lost or contaminated during the collision dynamics, and can still be detected by our network-based CME-meter. In addition to its accuracy and robustness in handling collision conditions, it is noteworthy that the well-trained machine, embedded with the knowledge of CME in the final state of HICs, can extrapolate this pattern to different charge separation fractions, collision systems, and even different simulation model, i.e., Anomalous-Viscous Fluid Dynamics(AVFD)~\cite{Shi:2017cpu, Shi:2018sah, Shi:2019wzi}. The well-trained machine also provides a powerful tool for quantifying the CME and is insensitive to the backgrounds dominated by elliptic flow ($v_2$) and local charge conservation(LCC), compared to the conventional $\gamma$-correlator. 

To implement the trained CME-meter in real experiments, the first step would be to reconstruct the reaction plane of each collision event and form averaged events as input for the meter. In general, reconstructing the reaction plane requires measuring correlations among final state particles, which is subject to finite resolution and background effects. However, the well-trained CME-meter is still capable of recognizing the CS signals even with restricted event plane reconstruction. As shown in Fig.~\ref{fig:CNN}, the network output consists of two nodes, which can be naturally interpreted as the probability of the meter recognizing a given input spectrum as a CME event ($P_1$) or a non-CME event ($P_0=1-P_1$). From a hypothesis test perspective, a characteristic distribution of $\max{P_1(\phi)}$ can be obtained, which can be used as a criterion for determining the existence of CME in HICs. Further details on deploying the CME-meter on single event measurements can be found in Ref.~\cite{Zhao:2021yjo}. Finally, DeepDream, a method used to visualize the patterns learned by convolutional neural networks (CNNs), is applied as a validation test using $P_1$ to detect the CME. It helps us uncover the hidden physical knowledge in the well-trained machine, including charge conservation and specific charge distributions.


\subsubsection{Anisotropic Flow Analysis}
\label{flow_hic}

One of the signals for QGP production in HIC is the strong collective flow~\cite{Shuryak:2004cy}. In semi-central and peripheral collisions, the geometric eccentricity of the initial state is transformed into the momentum anisotropy of final state hadrons, through the strong collective expansion. The azimuthal angular distribution $dN/d\phi$ in momentum space can be Fourier decomposed to get the famous elliptic flow $v_2$, which is the second-order coefficient in the decomposition. The number-of-constituent-quark scaling of $v_2$ for identified particles in HIC is strong evidence that the underlying degree of freedom is partons. It is mysterious that the triangular flow which is sensitive to initial state fluctuations is discovered much later than the elliptic flow. Later, higher orders are also discovered and non-linear correlations are found between $v_4$ and $v_2$, as well as between $v_5$, $v_2$, and $v_3$. The correlations between different orders of harmonic flows (at different $p_T$) are used as new observables to constrain the properties of QGP.


\begin{figure}[htbp!]
    \centering
    \includegraphics[width=\textwidth]{figures/PCA3A.pdf}
    \setlength{\belowcaptionskip}{-0.cm}
    \caption{The anisotropic flow harmonics from principal component analysis as compared with traditional Fourier decomposition method. Taken from ~\cite{Liu:2019jxg} with permission.}
    \label{fig:pca_flow}
\end{figure}

Ref.~\cite{Liu:2019jxg} uses principal component analysis (PCA) to get the first several principal components of the single-particle distribution. As shown in Fig.~\ref{fig:pca_flow}, the $v_2'$ and $v_3'$ got from PCA are the same as $v_2$ and $v_3$ using Fourier decomposition, but higher order harmonic flows with $n\ge 4$ deviate from the Fourier decomposition method a lot. The correlations between different harmonic flows are reduced while the Pearson correlation between $v_n'$ and the initial geometric eccentricity is enhanced, as compared with the traditional method. The results imply that the relativistic hydrodynamics might not be that non-linear which can couple different modes. The PCA rediscovers anisotropic flows to different orders. Compared with the flow analysis history, the importance of triangular flow emerge automatically in PCA as it is the next most important component.  

Ref.~\cite{Mallick:2021wop} uses a deep neural network to learn the elliptic flow from the final state particle kinetic information. This is a supervised learning which uses elliptic flow calculated from the event-plane method as labels. After training using minimum bias data from Pb+Pb $5.02$ TeV collisions, the network succeeds in computing elliptic flows at various other beam energies and centrality classes. This may provide a way to replace time-consuming data analysis tasks using machine learning tools. Deep neural networks have the capability to compute critical physical observables such as anisotropic flow in heavy ion collisions, which suggests its potential for revealing new physical observables that are sensitive to a specific QGP property within the complex hybrid model. The machine-learned physical observable could be correlations among different traditional observables, or non-linear correlations between particles from different regions in the momentum space. The objective can be accomplished through supervised learning, which involves training a deep neural network to map the physical property of interest to the final state hadrons.


\subsection{Fast Simulations for HICs}
Simulations are a vital, albeit highly computationally demanding, element in linking experimental results with theoretical discoveries. This is especially true for the field of HENP, specifically in studying heavy ion collision physics.  As mentioned in Sec.~\ref{sec:hydro}, relativistic hydrodynamics simulations have been shown crucial in modeling the heavy ion collisions, which thus serve as a powerful tool for studying the properties and evolution of the strongly interacting QCD matter under extreme conditions in the context of HICs. The deciphering of the driven physics from HICs requires a huge amount of collision event simulations to confront the experimental measurements. However, traditional numerical methods for HICs simulations are computational resources demanded and time-consuming, especially for a large amount of events simulations.

\subsubsection{Efficient Emulator with Transfer Learning}\label{sec:hic:emulator}
As introduced in Sec.~\ref{transport}, when performing Bayesian inference to estimate the likely model parameters, the MCMC procedures which are needed to sample the posterior distribution for the to-be-inferred parameters would require huge model simulations to explore the parameter space. In practice, a model emulator or surrogate is necessary to largely reduce the computational demand in enabling the MCMC for inference. In general, a model emulator, usually constructed with machine learning, gives a fast map from any arbitrary point in multidimensional parameter space to the physical model predictions on the desired observables with uncertainties associated. In the context of QGP properties constraints in HICs or other parameter estimation for computationally intensive models~\cite{higdon2015bayesian, higdon2008computer}, Gaussian Process (GP) emulators have been shown successful and taken as the standard practice for a decade. Notwithstanding, the training of a reliable emulator including preparation of training data is still highly computationally intensive, especially when different collision systems need to be considered jointly for the inference. Since each individual system needs a separate emulator for facilitating the global Bayesian inference.

By realizing that different collision systems share common physics are thus have related physical observables, Ref.~\cite{Liyanage:2022byj} applied transfer learning to build an efficient emulator (or target task, $f_T(\bold{x})$) from emulator (for source task, $f_S(\bold{x})$) trained from limited existing training data simulated for a different situation. Specifically, the Kennedy--O'Hagan (KO) model is applied to model the discrepancy function between different nuclear collision systems or dynamical simulations, i.e., the systematic difference between target and source systems is modeled with one independent GP prior,
\begin{equation}
f_T(\bold{x}) = \rho f_S(\bold{x}) + \delta(\bold{x}), \quad \delta(\bold{x})\sim GP\{\mu_{\delta}, k^{SE}_{\delta}(\cdot,\cdot)\},\quad f_S(\bold{x})\sim GP\{\mu_{S}, k^{SE}_S(\cdot, \cdot)\},
\label{eq:transfer}
\end{equation}
where the squared-exponential kernels are used with different variance and length-scale parameters for the source emulator and the discrepancy function.
With a collection of training data consisting of enough (suppose size to be $m$) model simulations for the source system ($\{\mathbf{X}_S, \mathbf{y_S} \}$) and a much smaller sized (suppose size to be $n\ll m$) simulations for the target system ($\{\mathbf{X}_T, \mathbf{y}_T \}$), the posterior distribution for the target system $f_T$ at any given different parameter $\mathbf{x}_{\rm new}$ can be derived as
\begin{equation}
[f_T(\mathbf{x}_{\rm new})|\mathbf{y}_S, \mathbf{y}_T] \sim \mathcal{N}(\mu^{*}_T(\mathbf{x}_{\rm new}),\sigma^{2*}_T(\mathbf{x}_{\rm new})),
\label{eq:transfer_posterior}
\end{equation}
\begin{equation}
\mu^{*}_T(\mathbf{x}_{\rm new})=\rho\mu_S + \mu_{\delta} + \mathbf{k}^{\top}\mathbf{\Sigma}^{-1}\left(
  \begin{bmatrix}
    \mathbf{y}_S\\
    \mathbf{y}_T
  \end{bmatrix} - 
  \begin{bmatrix}
    \mu_S\mathbf{1}_{m}\\
    (\rho\mu_S+\mu_\delta)\mathbf{1}_{n}
  \end{bmatrix} \right),
\label{eq:transfer_posterior_mean}
\end{equation}
\begin{equation}
{\sigma^2_T}^*(\mathbf{x}_{\rm  new}) = \rho^2\mathbf{k}_S(\mathbf{x}_{\rm new},\mathbf{x}_{\rm new})+\mathbf{k}_\delta(\mathbf{x}_{\rm new},\mathbf{x}_{\rm new}) - \mathbf{k}_{\rm new}^\top \mathbf{\Sigma}^{-1}\mathbf{k}_{\rm new},,
\label{eq:transfer_posterior_variance}
\end{equation}
where $\mathbf{k}_{\rm new}=[\mathbf{k}_{\rm new}^S,\mathbf{k}_{\rm new}^T]=[[k(\mathbf{x}_{\rm new},\mathbf{x}^S_i)]_{i=1}^{m}, [k(\mathbf{x}_{\rm new},\mathbf{x}^T_j)]_{j=1}^{n}]$ and 
\begin{align*}
    \mathbf{\Sigma} &= 
    \begin{bmatrix}
        \mathbf{K}_S(\mathbf{X}_S)+\gamma^2_S\mathbf{I}_{m} & \rho\mathbf{K}_S(\mathbf{X}_S,\mathbf{X}_T)\\
        \rho\mathbf{K}_S(\mathbf{X}_S,\mathbf{X}_T)^T & \rho^2\mathbf{K}_S(\mathbf{X}_T)+\mathbf{K}_\delta(\mathbf{X}_T)+\gamma^2_T\mathbf{I}_{n}
    \end{bmatrix}.
\end{align*}\
These thus serve the transfer learning emulator model with $\mu^{*}_T$ providing the prediction and $\sigma^{2*}_T$ quantifying the uncertainty, which by construction and in spirit transfers the captured knowledge from an easily accessed or already existed source term emulator to a different target system.

\subsubsection{Accelerating Hydrodynamic Simulations}
Fast simulation via machine learning for heavy ion collision modeling with relativistic hydrodynamics is discussed in Ref.~\cite{Huang:2018fzn}. A stacked U-Net (sU-net) is designed and trained to capture the non-linear mapping from the initial state to final state profiles of the fluid field in the special case of being with ideal EoS ($p=e/3$), zero viscosity, zero charge densities and a fixed spatiotemporal discretization. The initial and final energy-momentum tensor profiles from hydrodynamics, $T^{\tau\tau}(x,y), T^{\tau x}(x,y), T^{\tau y}(x,y)$, are taken as the input and output for the deep neural network. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp!]
    \centering
    \includegraphics[width = 0.75\textwidth]{figures/sUnet.pdf}
    \caption{Taken from ~\cite{Huang:2018fzn} with permission. The stacked U-net structure for predicting flow field in the relativistic hydrodynamic simulation of heavy ion collisions.
    \label{fig:sunet}}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As a variation of the autoencoder structure with short-cut residual connections, the constructed sU-net architecture has been shown to be powerful for image synthesis or object segmentation, with the ability to identify local patterns. In the context of HICs, with simulation from hydrodynamic non-linear partial differential equations, sU-net, as shown in Fig.~\ref{fig:sunet}, was demonstrated to be able to predict well the final energy density and flow velocity profiles given any arbitrary initial profile from an initial condition model. Different initial conditional models were implemented for the generalizability test of the trained sU-Net, including MC-KLN, AMPT, and TRENTo with vanishing transverse flow-velocity, which were used for training the MC-Glauber initial condition. For physics interests in investigating the deformation and inhomogeneity of the created QGP medium, the eccentricity coefficients associated with the final energy density profiles from the sU-Net prediction were calculated and found to reproduce the results from VISH2+1 hydro simulation well.
Note in Ref.~\cite{Taradiy:2021pxd} demonstrated on simple non-relativistic hydrodynamics, one single DNN was shown to be able to predict the fluid dynamics in extrapolation manner on both initial hydro profile and simulation duration. Another promising strategy not fully explored for HICs yet may come from a physics-informed neural network, which has been demonstrated for fluid dynamics~\cite{2021AcMSn..37.1727C, RAO2020207, BAI2022114740} prediction in applied physics. It is also interesting to explore the inverse inference through such trained PINN for the relativistic Hydrodynamics simulations.



\subsection{Summary}\label{sub:hic_outlook}

In this chapter, the ``standard model'' of high energy heavy ion collisions is reviewed. It is a hybrid model of gluon saturation, relativistic fluid dynamics for QGP expansion, hadronic transport, and the jet-medium interaction. The model is used to generate a vast amount of data that have been used in Bayesian analysis to constrain the QCD EoS at high energy, the initial state entropy deposition, the temperature-dependent shear and bulk viscosity and the jet energy loss coefficients/distributions. The data are also used in PCA or deep learning to rediscover physical observables, which verifies the representation power of machine learning.
It is worthy noting that, besides the researches mentioned above, the determination of other bulk thermodynamics, like the entropy production~\cite{Habashy:2021qku}, or related particle ratios~\cite{Rahman:2022tfq} and particles multiplicity~\cite{Habashy:2021poi} have been very recently investigated with DNN or Bayesian approach~\cite{Yousefnia:2021cup}.
The deep neural network is used in supervised learning to build mappings between the initial nuclear structure, the QCD EoS, the CME, the jet flavors, and the jet production positions to the final state hadrons.
As an outlook, machine learning tools can be used as fast emulators of HIC simulations. The active learning algorithm can be used to propose important regions of the parameter space to be simulated with hybrid models to increase data generation efficiency. After training to build mappings between data and model parameters, the network $f(x, \theta)$ serves as a new physical observable, whose interpretation may inspire both experimentalists and theorists. 
