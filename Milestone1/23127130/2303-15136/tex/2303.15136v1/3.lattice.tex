\section{Lattice QCD}\label{sec:lat}
As the fundamental theory for describing the strong interaction, QCD is challenging to be solved due to its characteristic asymptotic freedom, which embarrasses perturbative treatment at low energy scales~\cite{Gross:2022hyw}. Therefore, non-perturbative methods for investigating nuclear matter in the context of strong interactions are required. Lattice field theory plays a significant role in the ab-initio computation\footnote{Fuctional methods e.g., functional renormalization group (fRG) and Dyson-Schwinger (D-S) equations form the other first principle approaches. See Ref.~\cite{Fischer:2018sdj} for a review.} of general many-body systems~\cite{Wilson:1974sk}. In this approach, the system's partition function or path-integrals are discretized on a lattice of Euclidean spacetime, and field configurations or paths can be generated using importance sampling for further evaluation of physical observables.

Typically, Markov Chain Monte Carlo (MCMC) related algorithms such as the Hybrid Monte Carlo (HMC) algorithm~\cite{Duane:1987de} are used to generate ensembles of configurations in lattice calculations~\cite{Knechtli:2017sna}, and have succeeded in giving important results e.g., for the QCD phase structure~\cite{Fukushima:2010bq,Philipsen:2012nu,Ding:2015ona,Guenther:2020jwe, Karsch:2022opd}. See also Ref.~\cite{Ratti:2018ksb} for a recent review. However, the sequential nature and diffusive update involved in these Monte Carlo algorithms seriously hampers the sampling efficiency when moving to larger and finer lattices due to the critical slowing down(CSD)~\cite{Wolff:1989wq}. The computation becomes even more challenging for evaluating real-time dynamical properties. Recent advances in machine learning techniques may provide a promising way to overcome, or at least mitigate, the computational barriers involved in lattice QFT and further QCD studies~\cite{Boyda:2022nmh}. From the methodology point of view, many of the algorithmic developments were firstly explored on simple many-body physics or QFT systems but with the potential to be generalized to QCD study. Therefore, the discussion in this chapter is with generality into lattice field theory.

\subsection{Overview and Challenges in Lattice Field Theory}
\label{sec:lattice_cha}
In general, Lattice QFT provides a non-perturbative solution of the path integral for the field system on a discretized Euclidean spacetime lattice. This allows the expectation value of an observable, $\mathcal{O}(\phi)$, to be represented by the discretized action,
\begin{equation}
    \langle\mathcal{O}\rangle=
    \frac{1}{Z}\int\mathcal{D}\phi\,\mathcal{O}(\phi)e^{-S(\phi)},
    \label{eq:obs}
\end{equation}
with $Z=\int\mathcal{D}\phi e^{-S(\phi)}$ the partition function, and $\int\mathcal{D}\phi$ the integration over all possible configurations of the discretized quantum field $\phi$. All the dynamical and interaction information of the fields are encoded in the action $S(\phi)$. The specific numerical evaluation of Eq.~\ref{eq:obs} relies on sampling configurations following the action dictated probability distribution 
\begin{equation}
    p(\phi)=\frac{1}{Z}e^{-S(\phi)}.
    \label{eq:target}
\end{equation}

Several obstacles or computational tasks for lattice field theory calculations are briefly introduced in following, to which we will later accordingly review the current progresses in utilizing ML techniques for a rescue.

\begin{itemize}
\item{\textbf{Critical slowing down}}

Monte Carlo (MC) simulation can provide unbiased sampling for lattice field theory, which has been proven to be ergodic and asymptotically exact to approach the target distribution $p(\phi)$ under detailed balance condition for the involved proposal probability~\cite{krauth2006statistical}.
Within a classical MCMC framework, the configuration sampling process typically involves proposing a new configuration from the previous configuration on the chain for further acceptance or rejection judgment, based upon evaluation of a specific target probability using the proposed and also the previous field configurations (e.g., within Metropolis-Hastings algorithm~\cite{Metropolis:1953eos,Hastings:1970mcs}). In these diffusive updates based sampling approaches, the proposal for new field configuration is usually made by local perturbation of the previous one or heat bath updates, which thus is inefficient in drawing independent configurations. Improvements can be realized by HMC which relies on evolving the configuration $\phi(x)$ jointly with a “conjugate momentum” $\pi(x)$ under a classical Hamiltonian dynamics. Practically the computational cost involved are due to the strong autocorrelation of local updates, or the single local update itself is computationally expensive. The autocorrelation time is expected to scale as $\tau\sim\xi^z$, where $\xi$ is the correlation length that diverges around the critical point or approaching the continuum limit, and $z$ is the dynamical critical exponents which are $\approx2$ for standard local update algorithms~\cite{Wolff:1989wq}. Correspondingly, the induced severe inefficiency of sampling is called \textit{critical slowing down} (CSD). Being related, in the context of lattice field theories with well-defined topological behavior like QCD, \textit{topological freezing} can happen where topological observables usually have exponential scaling $\tau\sim e^{z\xi}$~\cite{DelDebbio:2004xh,Schaefer:2010hu}. CSD and topological freezing form the main barrier of lattice QFT computations.

\item{\textbf{Observables measurements and physics analysis}}

With ensembles of field configurations sampled from the desired distribution $p(\phi)$, correlation functions and further different physical observables can be evaluated, during which there'll be intensive operations on high-dimensional tensors (field configurations). Physics is thus also to be analyzed such as thermodynamics or phases transition (order parameter) of the system, different real-time dynamical properties, e.g., the reconstruction of spectral function or parton distribution functions (PDF) which form ill-conditioned inverse problem. 

\item{\textbf{Sign problem}}

The sign problem in lattice QCD calculations refers to the fact that the functional integral defined in the partition function is not positive definite in many cases (e.g., at finite density or for Minkowski-time dynamical physics), which makes it difficult to utilize standard Monte Carlo methods~\cite{Troyer:2004ge}. In lattice QCD, the partition function is defined as a functional integral over all possible configurations of quarks and gluons on a discrete four-dimensional lattice in the form~\cite{Aarts:2015tyj,Nagata:2021ugx},
\begin{equation}
    Z = \int D[A] D[\psi] D[\bar{\psi}] e^{-S[A,\psi,\bar{\psi}])},
\end{equation}
where $S$ is the QCD action, $A$ is the gauge field, and $\psi$ and $\bar{\psi}$ are the quark and antiquark fields, respectively. 
In finite-density QCD, the nonzero chemical potential($\mu$) makes the fermion determinant complex within the partition function, $Z = \int D[A] \text{exp}(-S_\text{YM})\text{det}M(\mu)$, where $S_\text{YM}$ is the Yang-Mills action. The problem is that the exponent in the integrand, $-S$, is not always positive, which leads to the non-positive definite integral. It induces the ill-defined probability when sampling configurations in standard Monte Carlo techniques. Even if some methods can bypass the predicament technically, they will inevitably encounter the highly fluctuating phase factors which result in exponential growth of computational cost as the volume increases~\cite{Berger:2019odf,Alexandru:2020wrj}.


\end{itemize}

\subsection{Field Configuration Generation}

For lattice QFT study, it's expected that the generation of field configurations constructs the most expensive computation, because of the usually needed Monte Carlo simulation on Markov chains for the involved high dimensional distribution's sampling, especially with also the need to push approach continuum limit for the simulation.
In fact, for a general many-body system study within Monte Carlo simulation, much effort has been put into making a clever proposal or global update during the MCMC procedures to reduce the autocorrelation time, the increase of which usually hinders efficient sampling for MCMC. Besides the critical slowing down as introduced in Sec.~\ref{sec:lattice_cha}, which is a crucial barrier when pushing the lattice calculation to the continuum limit, the other challenge facing conventional MCMC simulation is sampling from multimodal distributions. It's not trivial to traverse regions between the widely separated modes of the target distribution using update-based samplers.~\cite{DelDebbio:2004xh,Hasenbusch:2017fsd}. This high-dimensional sampling and efficient proposal design task is also popular in the machine learning community, which motivated the rapid development of generative models~\cite{Wang2018GenerativeMF}. It has been shown from both general classical/quantum many-body statistical physics study and lattice QFT research that the modern generative algorithmic development from AI/ML can bring in improved efficiency and support for traditional Monte Carlo computation on configuration generation for physical system.

\subsubsection{GAN-based Algorithms}
As introduced in Sec.~\ref{subsubsec:gm}, the Generative Adversarial Network(GAN), is a deep generative model to realize implicit MLE for distribution learning through adversarial training. This generative model has been recently explored for many-body statistical systems (e.g., for Ising model in~\cite{2017arXiv171004987L}) and also for the configuration generation of quantum field systems.
In Refs.~\cite{Zhou:2018ill,Zhou:2021vza,Zhou:2020yna}, the GAN is used for configuration generation of complex scalar $\phi^4$ field at non-zero chemical potential in the context of lattice QFT study. In Sec.~\ref{sec_phases_obs} about \textit{\textbf{Regression in QFT}} sector, this QFT system under worldline formalism was also introduced for regression exploration via deep learning. New configuration generation via Wasserstein-GAN scheme is also explored~\cite{Zhou:2018ill} at the task of producing uncorrelated configurations fulfilling the physical distribution, with training ensemble of configurations prepared from worm algorithm. Since the considered field system under the dualization approach should satisfy an important local divergence-type constraint reflecting flux conservation, the effectiveness of the generative algorithms, in terms of whether the generated configurations are physical or not, is verified by checking this divergence-type constraint condition for training. Surprisingly, as shown in the left of Fig.~\ref{fig:scalar_gan}, it is found that this highly implicit physical constraint condition is realized in a converging manner (more and more together with training epochs) for configurations generated from the trained generator within GAN, without explicit guidance of this constraint. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp!]
  \centering
  \includegraphics[width = 0.41\textwidth]{figures/div_per_site.png}
  \includegraphics[width = 0.45\textwidth]{figures/prob_den_gan_phi2.png}
  \caption{(left) The degree of divergence satisfaction for cofigurations from trained GAN generator; (right) the probability density distribution for the squared field $\phi^2$ from GAN and from Monte-Carlo simulations at fixed chemical potential. Taken from Ref.~\cite{Zhou:2018ill}.}
  \label{fig:scalar_gan}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Also, the distribution with respect to normal physical observables from the generated samples was found to agree well with the MCMC evaluation. And here, for the considered complex scalar field system, it even agrees reasonably well over the multimodally distributed number density and field square (see the right of Fig.~\ref{fig:scalar_gan}), which is not trivial for conventional sampling approaches without special treatment. Moreover, a conditional generative network, cGAN, is proposed in~\cite{Zhou:2018ill} to generalize the configuration generation ability of the generator to be dependent/conditioned on physical observables (specifically the number density $n$ was used for demonstration) and go beyond the distribution that it was trained on. Ref.~\cite{Singha:2021nht} latter applied the conditional GAN in lattice Gross Neveu model to mitigate the critical slowing down when approaching the critical region.

This strategy with the conditional GAN was also used for the spin configurations generation of two-dimensional XY model~\cite{2021ScPP...11...43S}, with several different architectures proposed including also one specific output distribution entropy maximization regulator to mitigate the mode-collapse issue. As the non-local defining feature for the involved topological phase transition, the vortices distribution is also specially scrutinized besides other relevant observables (e.g., magnetization and energy). With the trained GAN, Ref.~\cite{2021ScPP...11...43S} further proposed the GAN fidelity in terms of the discriminator network output which is shown to be able to detect the phase transition unsupervisedly.

The above demonstration, especially the consistency between the distribution from the GAN and the desired one, actually indicate that the trained GAN here can be taken as good enough proposal on a Markov Chain when one wants to guarantee the ergodicity and detailed balance properties for the sampling process, thus make sure the correct physics can be estimated in converging manner. This indeed is further demonstrated in Ref.~\cite{Pawlowski:2018qxs} on scalar field theory together with Hamiltonian Monte Carlo (HMC) approach. 
Specifically, the trained GAN is proposed to serve as overrelaxation (i.e., configuration space exploration that keeps the action unchanged, $\delta S=0$) procedure within action-based MCMC sampling algorithm. Start from some initial configuration $\phi$ after a number of HMC steps, the gradient flow is performed on the latent variable $z$ to achieve the GAN overrelaxation proposal $G(z')$, 
\begin{equation}
    z'(\tau+\epsilon)=z'(\tau)-\epsilon\frac{\partial (S[G(z)]-S[\phi])^2}{\partial z}, 
    \label{eq:gflow_gan}
\end{equation}
where $\epsilon$ is the learning rate and $\tau$ the training epochs. In Metropolis steps, proposal configuration with $\delta S=0$ (w.r.t. the last configuration) is accepted automatically given the transition probability for proposal is symmetric. Heuristic arguments are provided in~\cite{Pawlowski:2018qxs} on this issue for GAN's selection probability $P(G(z')|\phi)$. With demonstration on 2d scalar field theory, under such GAN overrelaxation method the autocorrelation time has been shown to be largely reduced. 

Note that the GAN-based approach in general requires a training data set to be prepared from conventional sampling means, which hampers its ability to assist the field configuration generation purely from the physically known action or Hamiltonian. Also, though it can be taken as proposal in generating independent and physically promising configurations, the vanilla GAN has no evaluation to the likelihood thus the sample probability. More potential can be unlocked for GAN-based approach if the explicit likelihood estimation can be added inside the adversarial learning in the future. In contrast, the methods introduced in the following can render the self-training just from physical action or Hamiltonian, with also the sample probability to be accessible.

\subsubsection{Self-Learning through Effective Action}

\emph{\textbf{Self-Learning Monte Carlo}} --- 
Self-Learning Monte Carlo (SLMC) method is a general-purposed numerical algorithms for configuration generation of many-body system based upon machine learning. The development of SLMC originally is within classical spin systems\cite{2017PhRvB..95d1101L} and shows clear improvement in curing the critical slowing down problem. It works also to other general quantum models that are concerned in condensed matter physics and quantum chemistry.~\cite{PhysRevB.102.041124}. The basic idea is to learn a tunable and effective Hamiltonian or Action that can be associated with a more efficient update algorithm (such as cluster-update or other global moving ways to propose uncorrelated configurations), with which, the Metropolis-Hasting test using the real action can turn it to be exact sampler for the system.

As detailed in Ref.~\cite{2017PhRvB..95d1101L} on the example of a statistical spin model, the SLMC consists a learning phase and exploration phase within actual MC simulation in general: (1) a conventional MC simulation under local update can be performed to produce a series of configurations with its weight (energy or action evaluation) also being known; (2) based on these training data an effective Hamiltonian $H_{eff}$ can be learned, to which one can use global update for a faster simulation (e.g., when only two-particle interactions are contained in $H_{eff}$); (3) the learned $H_{eff}$ can be used to make proposal in actual MC simulation and (4) perform Metropolis Hasting test with the original Hamiltonian to correct (reject or accept). See Fig.~\ref{fig:SLMC} for a schematic illustration of SLMC in a simple spin model.
\begin{figure}[!hbtp]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/SLMC.pdf}
    \caption{(From Ref.~\cite{2017PhRvB..95d1101L})Schematic illustration of learning process (top panel) and simulating process (bottom panel) in SLMC. \label{fig:SLMC}}
\end{figure}
The training over $H_{eff}$ can also be self-improved by iteratively reinforce training $H_{eff}$ with generated configuration from the last step self-learning update with $H_{eff}$. Basically, a more efficient MC update model can be obtained from the SLMC. It's demonstrated that the SLMC indeed largely reduced the autocorrelation time $\tau$ especially near the phase transition, and gives around 10 to 20-fold speedup on the considered 2D generalized Ising model.
It's worthy noting that similar ideas by training and simulating Restricted Boltzmann Machines (RBM)~\cite{10.1162/089976602760128018} were adopted in Refs.~\cite{PhysRevB.95.035105,2016PhRvB..94p5134T,PhysRevE.96.051301,2019PhRvE.100d3301P} for proposing efficient Monte Carlo updates, which is not limited to Trotter decomposition for fermionic system and renders faster proposal since the used Gibbs sampling in RBM.


This self-learning MC strategy was further pushed forward into more realistic fermionic systems where Trotter decomposition is employed~\cite{2017PhRvB..95x1104L, 2017PhRvB..96d1119X} and for continuous time Monte Carlo simulation~\cite{2017PhRvB..96p1102N}.  Deep neural network techniques are also additionally introduced into the effective action or Hamiltonian's construction in Ref.~\cite{2018PhRvB..97t5140S, PhysRevB.101.115111,2019PhRvB.100d5153S}, which can further increase the flexibility of the effective action and improve the following acceptance rate in the Monte Carlo sampling stage. 

The SLMC for non-abelian SU(2) gauge theory with dynamical staggered fermions at zero and finite temperature was developed in Ref.~\cite{Nagai:2020jar}.
Ref.~\cite{Tomiya:2021ywc} devised gauge covariant neural network for 4-dimensional non-abelian gauge field theory, and adopted such network to construct effective action within HMC to achieve self-learning HMC scheme, with demonstration on the case of two color QCD including un-rooted staggered fermion.

\emph{\textbf{Action parameter regression}} ---
In tackling the issue of CSD for lattice field theory study, with promise, multiscale methods are proposed to overcome the CSD by refining ensemble of configurations at a coarse-scale. Such methods require \textbf{action matching} across different scales via renormalization group (RG), then the sampling at coarse-scale level can already render the approach to continuum limit physics evaluation together with a cheap re-thermalization with the original fine action. The key challenge involved is parametric regression for identifying the proper action parameters that best describe physics at coarse-scale from an ensemble of configurations generated at finer scale. It's thus proposed to use deep neural networks to tackle this action matching regression task\cite{Shanahan:2018vcv}, where the coarsened ensemble of SU(2) gauge field configurations are taken as input and the required action parameters being the output. It's worth noting that the mismatch from the regression can be corrected by re-thermalization steps in the finer scale with the corresponding action.

The simple fully-connected neural network was tried first and found to appear successful in validation, which however fails to generalize to different parameter cases or even new Hybrid Monte Carlo (HMC) simulation streams of the same parameters as of the training set. The failure of such naive neural network is argued in Ref.\cite{Shanahan:2018vcv} to be induced by the lacking of symmetries of the gauge field configurations, and further proposed a customized symmetry-preserving network to reduce effective degrees of freedom for the task. The embedding of the symmetries is designed by featuring an initial preprocessing layer to yield possible symmetry-invariant quantities as input to following fully-connected layers.  This gives accurate parameter regression and successful generalization for even ensembles not distinguishable to principal component analysis (PCA). It thus provides a solution to the action matching.

\subsubsection{Variational Autoregressive Network}
\label{van}

The key object for a general many-body system is the free energy, which contains all the information about the system in principle. From a probabilistic point of view, the usual MCMC approach basically is using importance sampling to implicitly approach the free energy, which is an intractable high dimensional integration. However, the direct evaluation of the free energy is not possible for naive classical MCMC approach (note that there are variants of MCMC developed to be able to approximately assess the free energy, which is computationally expensive). As another alternative strategy based on variational point of view, the \textit{mean field approach} or related information transfer algorithms can be adopted for the free energy estimation, which then basically perform the minimization over the variational free energy. In probabilistic language this is equivalent to the minimization over the reverse (or backward) KL divergence between the variational distribution $q_{\theta}(\phi)$ and the target distribution in Eq.~\ref{eq:target} or in statistical physics $p(\phi)=e^{-\beta H(\phi)}/Z$ (where $\beta$ is the inverse temperature, $H(\phi)$ the Hamiltonian of the system, and $Z$ is the partition function),
\begin{equation}
    \mathcal{D}_{KL}(q_{\theta}(\phi)||p(\phi))=\int\mathcal{D}[\phi] q_{\theta}(\phi)\log\frac{q_{\theta}(\phi)}{p(\phi)}=\beta(F_{q}-F), 
    \label{eq:KL_van}
\end{equation}
with the variational free energy defined as
\begin{equation}
    \beta F_q=\mathbb{E}_{\phi\sim q_{\theta}}[\beta H(\phi)+\log q_{\theta}(\phi)], 
    \label{eq:var_free}
\end{equation}
Because of the non-negativity for KL divergence ($\mathcal{D}_{KL}\ge0$, also known as \textit{Gibbs-Bogoliubov-Feynman inequality}~\cite{PhysRev.54.918,doi:10.1063/1.1704383,PhysRevLett.22.631}), the true free energy $F$ is upper bounded by the variational free energy $F_q$, and the equality happens when the variational distribution $q_{\theta}$ really reaches the target distribution $p$ exactly.  Note that in machine learning community as proposed in the beginning as general density estimation method, the autoregressive network usually uses the (forward) KL divergence between data empirical distribution (e.g., constructed with training data) and the variational distribution, $\mathcal{D}_{KL}(p_{data}||q_{\theta})$. For many-body physics or QFT study, the proposal to use the reverse KL divergence together with the easily sampleable variational ansatz makes it possible for \textbf{self-training} starting solely from the unnormalized target distribution\footnote{i.e., the training is performed with sampled generated from the variational distribution, instead of collected samples from the true distribution in advance}, e.g., knowing the action or Hamiltonian for the physical system in equilibrium.

Despite its popularity and success, the \textit{mean field calculation} is quite often limited by the assumed \textit{variational ansatz} (e.g., factorized), especially when the system is with strong correlations between their degrees of freedom, thus most of the time it is valid only in high temperature cases or when the system's topological structure fulfills the requirement from mean field approximation. Here the autoregressive model naturally stands out, because both the direct sampling and tractable likelihood evaluation (which is desired in evaluating the variational free energy) can be simultaneously realized. Meanwhile, when the autoregressive model is constructed as variational ansatz, the variational free energy (Eq.~\ref{eq:var_free}), which serves as the loss function, can be estimated unbiasedly and stochastically by drawing ensembles of samples from the sequential stochastic process specified by the autoregressive model (see Eq.~\ref{auto_prob} as introduced in Sec.~\ref{subsubsec:gm}). Thus, in every optimization iteration, using data sampled from the autoregressive model it suffices to perform \textbf{self-training} on the model.

Ref.~\cite{2019PhRvL.122h0602W} for the first time proposed to introduce neural network to give a more powerful yet tractable variational ansatz, taking advantage of the strong representational ability of neural networks due to the universal approximation theorem. To keep the evaluation of the variational free energy tractable, one needs to design the network such that the variational distribution represented by the network to be accessible and efficiently computable. Accordingly, the autoregressive networks were adopted by the authors to decompose the joint probability over all lattices to a product of conditional probabilities, and parametrized each conditional with neural networks\footnote{Note that to represent the conditional probability, the output of neural networks are intepreted as essential parameters of the distribution. More explaination about autoregressive network can refer to Sec.~\ref{subsubsec:gm}}. The idea got directly demonstrated on a simple 2D ferromagnetic Ising system, also generalized to the PixelCNN structure with convolutional layer included to respect the locality and the translational symmetry of the system. Compared to conventional MCMC evaluation, this variational autoregressive network can also give evaluation of the free energy by giving its well minimized upper bound. This strategy of using autoregressive probabilistic models was later combined with quantum circuits, and extended the variatiOnal quantum eigensolver (VQE) to investigate thermal properties and excitations of quantum lattice model, termed as $\beta$-VQE~\cite{Liu_2021}. This quantum classical hybrid algorithm was also applied to Schwinger model at finite temperature and density~\cite{Tomiya:2022chr}, with large volume limit evaluated and continuum limit taken in obtaining the phase diagram.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp!]
    \centering
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=7.cm]{figures/XY_can.pdf}
        \caption{The energy per site of 2D XY model ($L=16$) from CANs, CANs+IS and MCMC. The upper insert shows the number of vortex pairs density with inverse temperature. Taken from ~\cite{Wang:2020hji}.}
        \label{fig:xy_en_can}
    \end{minipage}
    \hspace{0.5cm}
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=9cm]{figures/vortex_prob.pdf}
        \caption{Probabilities analysis with CANs and corresponding vortices for a random 2D XY model configuration sampled from well-trained CANs at $\beta = 1.0$. Taken from ~\cite{Wang:2020hji}.}
        \label{fig:xy_vor_can}
    \end{minipage}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Authors in Ref.~\cite{Wang:2020hji} further generalized the above method to general many-body system with continuous variables, where the probability interpretation of the introduced autoregressive is devised to be a mixture beta distribution, instead of a Bernoulli distribution for Ising spins. This newly extended continuous-mixture autoregressive network (denoted as CAN in the paper) is well demonstrated on the 2D XY model which exhibits non-trivial topological KT phase transition. The thermodynamics of the systems were shown to be captured successfully (see Fig.~\ref{fig:xy_en_can}), and the underlying emergent degree of freedom--vertex--is also found to be rediscovered by this CAN method. Furthermore, it is found that the trained CAN network can automatically give rise the vortices' distribution for any random given XY spin configurations with its conditional probability components output from plaquette (see Fig.~\ref{fig:xy_vor_can}), directly indicate that the network captured the underlying emergent physics about this many-body system. Note that this CAN method can capture the O(2) symmetry for 2D XY model, which is equivalent to global U(1).

Ref.~\cite{2021PhRvR...3d2024W} proposed symmetry-enforcing updates within MCMC with autoregressive neural network as global update proposer, since the system action or Hamiltonian remain invariant under specific symmetry operations (e.g., translation and reflections as considered in Ref.~\cite{2021PhRvR...3d2024W}). This introduced symmetry operation largely reduced the ergodicity problem from those exponentially suppressed configurations (i.e., those are with exponentially smaller $q_{\theta}(\phi)$ compared to $p(\phi)$ however will hardly influence the variational free energy evaluation). Additionally, a neural cluster update scheme is devised in Ref.~\cite{2021PhRvR...3d2024W} utilizing the decomposition structure of autoregressive model, which can lower the autocorrelation time by setting only a subset of the lattice to be changed instead of the whole in each Monte Carlo step.


\subsubsection{Flow-based Variational Learning}
\label{sec:3:flow_based}
Being similar to the above-mentioned variational autoregressive network models (see Sec.~\ref{van}), the flow-based models construct the variational ansatz using normalizing flows (NF) for the target distribution, to further approach and learn the desired distribution via minimizing the variational free energy. Again, technically, it is minimizing the reverse mode of KL divergence between the flow constructed variational distribution and the target one.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp!]
  \centering
  \includegraphics[width = 0.6\textwidth]{figures/fig_3-normalizing_flow.pdf}
  \caption{A schematic demonstration for the normalizing flow of the Real NVP, in which the yellow circles represent affine transformations represented by the neural networks.}
  \label{fig:3:normflow}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Though introduced already in Sec.~\ref{subsubsec:gm}, we briefly explain the NF idea in the context of field configuration generation for the sake of terminology consistency in following. Generally speaking, NF construct a flexible and tractable probability distribution by providing a generative bijective transformation between a naive prior distribution $p_u(u)$ (e.g., multivariate Gaussians) and the variational distribution $q_{\theta}(\phi)$ (to approach the target distribution), $\phi=f_{\theta}(u)\sim q_{\theta}(\phi)$, which is parametrized by neural networks with trainable parameters denoted as $\{\theta\}$ and specially designed to keep the transformation invertible. Through probability conservation (i.e., change of variable theorem for probability distribution) one easily get the connection between the variational distribution and prior, $q_{\theta}(\phi)=p_u(u)|\det\frac{\partial f_{\theta}(u)}{\partial u}|^{-1}$. With the prior probability easily evaluable and the Jacobian determinant $J_f$ computable, one can calculate the variational probability $q_{\theta}(\phi)$ on any samples to allow for further minimization on the varitional free energy. Thus the key recipe in constructing feasible flow model is to make the network represented transformation $f_{\theta}$ to be invertible, differentiable and composable. As the most common flow-based model, Real NVP structure (see Sec.~\ref{subsubsec:gm} for details) are often applied, which constructs the transformation $f_{\theta}$ by composing a series of affine coupling layers (see its schematic diagram in Fig.~\ref{fig:3:normflow}) each looks like:
\begin{equation}
\left\{
\begin{aligned}
    \phi^{i}_{1:k} &= \phi^{i-1}_{1:k} \\
    \phi^{i}_{k+1:N} &= \phi^{i-1}_{k+1:N} \odot e^{s_{\theta}^i(\phi^{i-1}_{1:k})} + t_{\theta}^i(\phi^{i-1}_{1:k}),
\end{aligned}
\right.
\end{equation}
with $\phi^i$ the output of the $i^{\text{th}}$ affine coupling layer (thus $\phi^0 = u\sim p_u(u)$ and final output $\phi^{L}=\phi\sim p(\phi)$), $k$ the separation point of the configuration variables into two subsets, and $N$ the number of variables in one configuration. The scaling and translation functions can be parameterized by DNNs as $s_{\theta}: \mathbb{R}^{k}\rightarrow \mathbb{R}^{N-k}$ and $t_{\theta}: \mathbb{R}^{k}\rightarrow \mathbb{R}^{N-k}$. The Jacabian determinant can be directly evaluated as well to be $(\det J_{T}^i) = \Pi_j^{N-k}e^{s_{\theta}^i(X_{1:k})_j}$. Also, the above coupling layer can be easily inverted due to the splitting,
\begin{equation}
\left\{
\begin{aligned}
    \phi^{i-1}_{1:k} &= \phi^{i}_{1:k} \\
    \phi^{i-1}_{k+1:N} &= (\phi^{i}_{k+1:N} - t_{\theta}^i(\phi^{i}_{1:k}))\odot e^{-s_{\theta}^i(\phi^{i}_{1:k})}.
\end{aligned}
\right.
\end{equation}

With the above-mentioned flows, i.e., a series of bijective transformation layers one actually has a parametric model with explicit probability for each sample, $q_{\theta}(\phi)$, and with tunable parameters denoted as $\theta$. Then similar to autoregressive models, the reverse KL divergence in Eq.~\ref{eq:KL_van} can be applied to guide the optimization of $q_{\theta}(\phi)$ in approaching the target distribution of $p(\phi)$ in Eq.~\ref{eq:target},
\begin{equation}   \theta^{*}=\arg\min_{\theta}\mathcal{D}_{KL}(q_{\theta}(\phi)||p(\phi))=\arg\min_{\theta}[\int\mathcal{D}\phi q_{\theta}(\phi)(S(\phi)+\log q_{\theta}(\phi)) + \log Z ], 
    \label{eq:trainingKL}
\end{equation}



Ref.~\cite{Albergo:2019eim} delivered the first application of flow-based model to lattice field theory simulations\footnote{Earlier similar flow-based model was developed and demonstrated on the two-dimensional Ising model in its continuous dual version, with the novel concept of neural network based variational Renormalization Group approach\cite{2018PhRvL.121z0601L} proposed.}. Taking the two-dimensional $\phi^4$ field theory as example, this work showed that the flow-based sampler offers significant advantages over traditional sampling algorithms (local Metropolis sampling and HMC considered for benchmark), with the autocorrelation times found to be systematically reduced when combining the flow sampler as proposal inside the Markov Chain. The combination of the trained flow sampler and MCMC (e.g., Metropolis-Hastings) guarantee the asymptotic exactness of the sampling, since the Metropolis-Hastings acceptance/rejection step just provide a corrector to assure the sampling distribution to approach exactly the target distribution.
This method is usually dubbed as \textit{flow-based MCMC} in literature, of which one highlight is that the critical slowing down issue associated to the Markov Chain sampling is eliminated since the independent generation of samples from the well-trained flow model, and the cost correspondingly is shifted to the up-front training expense for the generative model.
Note that when calculating observables ($\langle\mathcal{O}(\phi) \rangle$), besides the stochastic MCMC correction method, reweighting (i.e., importance sampling) can also be applied which just assign a weight $w(\phi)=p(\phi)/q_{\theta}(\phi)$ to each sample $\phi$ in computing the observables' expectation. A detailed introduction to \textit{flow-based MCMC} on scalar field with code written in \textit{PyTorch} can refer to Ref.~\cite{Albergo:2021vyo}. We also summarize in Tab.~\ref{tab:flow} some related applications of this strategy onto different systems.
In Ref.~\cite{Nicoli:2020njz}, a simpler normalizing flow construction with Non-linear Independent Component Estimation (NICE, see Eq.~\ref{flow_nice}) is adopted for lattice simulation of 2-D real scalar field theory. In this work, the $\mathbb{Z}_2$-invariance is explicitly introduced by constraining the network to be with \textit{tanh} non-linearity and vanishing biases to ensure the transformation from the flow to be odd function, $f_{\theta}(-u)=-f_{\theta}(u)$. Interestingly, this work also proposed a direct estimator for the \textit{free energy} ($\hat{F}=-T\ln\hat{Z}$)\footnote{The temperature $T=\frac{1}{a N_T}$ where $a$ is the lattice spacing and $N_T$ is the number of lattice points in temporal direction.} from the trained flow model through Monte-Carlo approximation on the partition function,
\begin{equation}
\hat{Z}=\frac{1}{N}\sum_{\phi_i\sim q_{\theta}}[e^{-S(\phi_i)}/q_{\theta}(\phi_i)] ,
\label{eq:eff_free}
\end{equation}
and it shows conceptual appeal since it bypasses the cumbersome integration error accumulation in conventional MCMC-based estimation for the partition function. With the free energy well estimated, other thermodynamic observables can be naturally obtained through taking derivative of the free energy, and be further corrected by the importance sampling~\cite{Muller:2019nis}. Such capability from flow-based model is mainly attributed to their explicit likelihood estimation ability, which is also shared by variational autoregressive models like introduced in Sec.~\ref{van}.

A conditional normalizing flow (c-NF) model was trained in Ref.~\cite{Singha:2023cql} on samples pre-generated from HMC in noncritical region of the theory\footnote{Note that being different from the variational strategy of flow-based MCMC method, this work trains the flow on existing samples, thus using forward KL divergence as the loss function.}. Such trained c-NF model is able to interpolate or extrapolate the dependence of configuration generation on the parameter and thus is applied to critical region (in i.e., phase transition situation or continuum limit seeking). Though the interpolation or extrapolation of the trained c-NF model in critical region for configuration generation may have bias, it can be corrected efficiently with few e.g., Metroplolis-Hasting steps because of the tractable probability evaluation for each of the generated (``proposed'') configurations. Since the shared flow-model across the whole parameter space exlude the conditioning on them, it provides an economical way to reduce the cost of performing flow-based model for phase diagram exploration of the theory.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[htbp!]
\caption{Normalizing Flows applied to different physical system in terms of lattice study}
\label{tab:flow}
\centering
\begin{tabular}{ccccccc}
\hline
systems\quad & Ising \quad & Scalar \quad & U(1) gauge \quad & SU(N) gauge \quad & Yukawa  \quad & SU(3) gauge \\ \hline
dimensionality\quad & 2D \quad & 2D \quad & 2D \quad & 2D \quad & 2D  \quad & 2D \\ \hline
fermions\quad & no \quad & no \quad & no \quad & no \quad & Staggered  \quad & 2 flavors \\ \hline
Ref.  \quad & \cite{2018PhRvL.121z0601L} \quad & \cite{Albergo:2019eim,DelDebbio:2021qwf,Hu:2019nea} \quad & \cite{Kanwar:2020xzo,Foreman:2021ixr} \quad & \cite{Boyda:2020hsi} \quad & \cite{Albergo:2021bna} \quad & \cite{Abbott:2022zhs} \\ \hline
%$\sigma_{R_{\rm iso}}$ \quad & 2.52\% \quad & 2.05\% \quad & 2.03\% \quad & 1.87\% \quad & 2.25\% \quad & 2.64\% \\ \hline
\end{tabular}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

It is worth noting that, such normalizing flow (with networks) built diffeomorphic transformations $f_{\theta}$ between a naive simply distributed ``prior field'', $u$, and the physical field configurations, $\phi$, resembles the \textit{trivializing maps} approach suggested by L\"uscher in Ref.~\cite{Luscher:2009eq}, under which actually one is constructing for the system an effective action,
\begin{equation} 
S_{\rm{eff}}[u]=S[f_{\theta}(u)] - \log\det J_{f_{\theta}}(u). 
    \label{eq:flow_eff_action}
\end{equation}
that decouples the field variables by using the transformation whose Jacobian hopefully cancel out the interaction part in the original action, such that easier sampling can be realized for the (inversely) transformed new system $u=f_{\theta}^{-1}(\phi)\sim e^{-S_{\rm{eff}}(u)}$. Thus, one can in general view NF as neural network parametrisations for trivializing maps. By noting such perspective and taking the trained flow model inverse as ab approximation of the trivializing map, Ref.~\cite{Foreman:2021ljl} and Ref.~\cite{Albandea:2023wgd} performed HMC on the flow transformed system $u$ for a 2-dimensional U(1) gauge theory and $\phi^4$ scalar field theory, respectively. Then to the HMC resulted Markov chain $\{u_i\}^N_{i=1}\sim e^{-S_{\rm{eff}}(u)}$ which is with shorter autocorrelation times, the application of the inverse flow transformation just gives the recovered field configuration samples, $\{f_{\theta}(u_i)\}^N_{i=1}=\{\phi_i\}^N_{i=1}\sim e^{-S(\phi)}$. It was also demonstrated for the 2-dimensional $U(1)$ theory that such deep learning assisted HMC (DLHMC)~\cite{Foreman:2021ixr, Foreman:2021ljl} allows mix between modes of different topological charge sectors. Note that performing HMC on the inversely transformed system (can also be called the latent space for the original system) was proposed also by Ref.~\cite{2018PhRvL.121z0601L} on Ising system and showed enhanced efficiency, and later Ref.~\cite{Hu:2019nea} adopted similar approach of the neural RG to complex $\phi^4$ field theory with interpretation as automatic construction of exact holographic mapping of AdS-CFT.

The splitting operation in Real NVP (i.e., partitioning the field configuration into two parts before entering each flow coupling layer, as Fig.~\ref{fig:3:normflow} shown.) may constrain the expressibility of the flow induced field transformation $f_{\theta}$, such as in terms of the symmetry embedding. Ref.~\cite{Gerdes:2022eve} introduced continuous normalizing flow to define this invertible map, $f_{\theta}: u\to\phi$, as the solution to neural ODE~\cite{2018arXiv180607366C} with a fixed time $\mathcal{T}$ ($x$ indicate the lattice cites):
\begin{equation}
\frac{d\phi(t)_x}{dt}=g_{\theta}(\phi(t),t)_x,\quad \text{with} \quad u\equiv\phi(0), \quad \phi\equiv\phi(\mathcal{T}),
\label{eq:flow_ode}
\end{equation}
where $g_{\theta}$ is a neural network represented vector field, to which symmetries can be built in more easily. The probability of the generated configuration $phi$ follows the solution from a second ODE~\cite{2018arXiv180607366C}:
\begin{equation}
\frac{d\log p(\phi(t))}{dt}=-(\nabla_{\phi}\cdot g_{\theta})(\phi(t),t), \quad \text{with} \quad p(\phi(0))\equiv p_u(u), \quad p(\phi(\mathcal{T}))\equiv q_{\theta}(\phi),
\label{eq:flow_ode_prob}
\end{equation}
Again, taking the two-dimensional scalar field theory as the testing ground, authors in Ref.~\cite{Gerdes:2022eve} proposed a vector field for the neural ODE inspired by Fourier expansion, $g_{\theta}(\phi(t),t)_x=\sum_{y,a,f}W_{xyaf}K(t)_a\sin(\omega_f\phi(t)_y)$, with $\omega_f$ the learnable frequencies and $W$ the learnable weight tensor and $K(t)_a$ the first several terms of Fourier expansion on the interval $[0,\mathcal{T}]$. With such vector field, the required symmetry and also the internal Z2 ($\phi\to -\phi$) symmetry of the scalar $\phi^4$ theory can be easily satisfied. Compared to Real NVP, such continuous flow method shows quite enhanced sampling efficiency: the effective sample size (ESS)\footnote{The effective sample size, ESS, can be computed as $\text{ESS}=\frac{[N^{-1}\sum^N_{i=1}p(\phi_i)/q_{\theta}(\phi_i)]^2}{N^{-1}\sum^N_{i=1}[p(\phi_i)/q_{\theta}(\phi_i)]^2}$ with $p(\phi)$ the target distribution and $q_{\theta}(\phi)$ the flow-sampler induced distribution} increases from $1\%$ to $91\%$ for $32\times 32$ lattice size.

For target distributions with multimodal structure, it is well known that the usual (local) update-based sampling methods, such as MCMC, face the challenge of traversing regions between different modes, e.g., Higgs modes/double well potentials. ``Freezing'' may probably happen when modes are very widely separated that the sampler tend to collapse to only one or few modes.~\cite{DelDebbio:2004xh,Hasenbusch:2017fsd}. It was pointed out~\cite{Hackett:2021idh} that the flow-based MCMC encounters such difficulties as well, due to the tendency of ``mode collapse'' from the original version of flow model. Then several different trials were performed in Ref.~\cite{Hackett:2021idh} to construct and train flow models in sampling multimodal distributional in lattice field theory with example on the $\mathbb{Z}_2$-broken phase of real scalar field theory, including data preparation in mixture models and data-augmented forwards KL divergence training, or adiabatic retraining with flow-distance regularization. But these methods either need prior knowledge for the mode structure to be provided, or difficult to really control the multimodal sampling with action parameters adjustments. Recently one interesting development~\cite{Chen:2022ytr} related is to introduce the Fourier transformation layer into the flow construction and showed promising in solving multimodal distribution sampling problem, details can refer to Sec.~\ref{sec_phy_manifest} (Note that later very recently, similar ideas but named as \textit{power spectral density layer} for flow models is introduced in Ref.~\cite{Komijani:2023fzy} and applied in scalar $\phi^4$ field).

For lattice gauge fields, new architectures of flow-based models have recently been developed to preserve the relevant local symmetries. A gauge invariant flow model is designed for sampling configurations for a $\mathrm{U}(1)$ gauge theory in Ref.~\cite{Kanwar:2020xzo}. See Sec.~\ref{flow_symmetry} for more details. Soon this kind of flow-based scheme was developed further to system with $SU(N)$ links~\cite{Boyda:2020hsi} and also to fermionic system as shown in Ref.~\cite{Albergo:2021bna,Abbott:2022zhs}. In the lattice community, it is now under progressing in translating these developments into real QCD simulations as reported in the recent proceeding~\cite{Abbott:2022hkm}. Though being promising, note however that, such flow-based methods currently are with training costs growing very fast as approaching to the continuum limit, as pointed out recently~\cite{DelDebbio:2021qwf, Abbott:2022zsh, Komijani:2023fzy}. Thus further efforts are needed in improving the approach e.g., incorporating transfer learning strategy.


\subsection{Observables Analysis for QFT}
In lattice QFT simulations, after the field configuration generation, usually the other big portion of the computations lie in the estimation of physical observables from the generated ensembles of field configurations or the accessible correlation functions, for examples, the thermodynamics evaluation, and real-time physics extraction. Some problem especially for real-time physics reconstruction will often encounter ill-posedness from an inverse problem perspective, such as spectral functions extraction, the parton distribution functions computation and other related transport coefficient analysis based upon Euclidean correlators from lattice calculation. ML and DL techniques has now been explored for tackling these problems in recent years.

\subsubsection{Thermodynamics and Phases}
\label{sec_phases_obs}

Thermodynamic states of matter, especially the accompanying phase transition, is one very important and extensively observed phenomenon in various many-body physics systems. From whatever theoretical (i.e., analytical or numerical) or experimental point of view, the exploration of phase diagrams forms a long-standing physics focus.
Usually estimators for physical quantities calculated on numerically generated samples (e.g., from MCMC) are constructed with close relation to phase related indicator such as order parameter. It is however not always easy to identify some physically important states with such estimators, like for topological phase. In this context, the data-driven machine learning well suits the scope of phase identification due to its relevant strengths in recognizing hidden patterns and correlations from complex data, and both supervised and unsupervised learning techniques have been explored for such a task~\cite{Dawid:2022fga}.

\emph{\textbf{Supervised Phase Classification}} ---
The very beginning of introducing deep learning methods into thermodynamics/phase identification in general many-body systems can be dated back to Ref~\cite{2017NatPh..13..431C}, where it was firmly demonstrated on simple Ising systems that deep neural networks can be trained to recognize phases and phase transitions solely from the raw configurations sampled from Monte Carlo simulations\footnote{This strategy is dubbed as ``supervised'' because labeled phases from domain/prior knowledge is relied on.}. This indicates the feasibility of using ML algorithms to identify order parameters of the many-body system per supervised training. A lot of further explorations along this strategy follows up, including also quantum phases identification for fermionic system~\cite{2017NatSR...7.8823B}. See Fig.~\ref{fig:cnn_phase} for a general schematic view of such supervised phase classification network.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp!]
  \centering
  \includegraphics[width = 0.8\textwidth]{figures/cnn_phase.pdf}
  \caption{A typical convolutional neural network for phase binary classification. Taken from Ref.~\cite{2017NatSR...7.8823B} with permission.}
  \label{fig:cnn_phase}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

By training a convolutional neural network (CNN) to capture the correlation between the inverse temperature and the 2D Ising spin configurations, Ref.~\cite{Tanaka:2016rtu} found that the trained CNN can locate characteristic features for the phase transition of the system with scrutinization of the filters (weight parameters $W$) from the network. Accordingly, a neural network assisted order parameter is defined to give rise to critical temperature estimation in this work. Ref.~\cite{Li:2021yet} generalized the phase identification with deep learning on Ising system to 3-dimensional case (employed 3D CNN on 3D Ising model), also discussed the average magnetization and energy regression, and the second-order and first-order phase transition learning. Being of special interest, it is expected that the QCD critical region have similar critical behaviors as in 3D Ising model because of their same universality.

\emph{\textbf{Usupervised Phase Clustering}} ---
Focusing on the phase classification task which is indeed one important physical goal in many-body physics, a lot of the machine learning approach based studies rely on prior knowledge of the order parameters of the system and ensembles of microscopic configurations of the physical system to be prepared, i.e., supervised training with correctly labelled ``configuration-phase'' large data set is needed as mentioned in above. However, phase labels for ensembles of field configuration are routinely not accessible, especially for a newly studied system. From machine learning, there is also unsupervised learning strategies which would find out the underlying important patterns in the collected data by itself and might be related to phase information in the context of phase identification, without knowing prior knowledge about the phase's information under consideration. 

The very first unsupervised learning application to phase transition recognition is proposed by Ref.~\cite{2016PhRvB..94s5105W} where neither the order parameter, thus the presence of a phase transition, nor the location of the critical point need to be provided. Specifically on the example of 2D classical Ising model, the principal component analysis (PCA) is adopted to extract the most significant components for the collected configurations of the system, then the projection of the spin configurations onto the first two principal components just automatically split into clusters matching well with the corresponding physical phases (see left of Fig.~\ref{fig:pca_ising}). The proposed simple PCA phase exploration approach was also shown successful in the Ising model with a conserved order parameter. In Ref.~\cite{2017PhRvE..95f2122H} this simple unsupervised learning techniques is also extended to different physical models, including the square and triangular-lattice Ising models, the Blume--Capel model, the biquadratic-exchange spin-1 Ising (BSI) model and the 2D XY model. It got confirmed that the extracted principal components can allow exploration of symmetry-breaking induced phase structure, can also help to identify the phase transition type and locating the transition points. 

However, the naive PCA analysis by definition is limited to linear transformations, thus not suited for more complicated transitions with non-linear patterns. Like it is shown in Ref.~\cite{2017PhRvE..95f2122H} that the vorticity structure in the BSI model and XY model can not be captured from raw spin configurations by the PCA, and thus motivated to embark on nonlinear unsupervised machine learning algorithms.
Ref.~\cite{2017NatPh..13..435V} proposed a \textit{confusion scheme} which does not depend on labelled data and therefore can be taken as a generic tool to investigate upexplored phases of matter. Basically, the neural network is trained via deliberately by hand -“labelled” data for confusion purpose, then the performance of the trained network was found to give a universal \textit{W-shape} as a function of the guessed critical point for the parameter (e.g., chemical potential or temperature). This interesting idea got successfully demonstrated on Kitaev chain for topological phase transition, the classical Ising system for thermal phase transition and also the quantum many-body-localization transition in a disordered Random-field Heisenberg chain. However, this framework failed when applied on 2D XY model (with original unprocessed spin configurations as input) which shows unconventional topological phase transition, as demonstrated in Ref.~\cite{2018PhRvB..97d5207B}. It is found that significant feature engineering on the spin configurations is needed for the correct phase classification, which is closely related to the underlying vortex patterns of the KT transition.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp!]
  \centering
  \includegraphics[width = 0.42\textwidth]{figures/ising_pca.pdf}
  \hspace{1cm}
  \includegraphics[width = 0.3\textwidth]{figures/Ising_latentB.png}
  \caption{Taken from Ref.~\cite{2016PhRvB..94s5105W} (left) and ~\cite{2017PhRvE..96b2140W} (right).}
  \label{fig:pca_ising}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As another popular unsupervised learning method, deep generative models such as the variational autoencoder (VAE) and generative adversarial networks (GAN), were also applied to investigate phase transition detection of physical systems by learning latent parameters of the original configurations. In Ref.~\cite{2017PhRvE..96b2140W} the authors studied the phase transition with PCA and VAE on states of 2D Ising model (see right of Fig.~\ref{fig:pca_ising}) and 3D XY model. It was found that the unsupervisedly learned latent representation of the spin configurations are clustered by itself and correspond to the known order parameters of the system under investigation. Also, the reconstruction loss from the training can serve as an universal identifier for phase transition. In Ref~\cite{2020NatSR..1013047W} this strategy was further extended to explore crossover region identification to reveal deeper understanding of the latent space, which is achieved by studying the response of the learned latent variable mappings of the Ising configurations along with external non-vanishing magnetic fields and temperatures. 

\emph{\textbf{Unsupervised Anomaly Detection}} ---
With a different strategy, DNN autoencoder (AE) is also deployed by performing anomaly detection to explore phase diagrams of quantum many-body systems~\cite{2020PhRvL.125q0603K} in an automated and completely unsupervised manner. Intuitively, the anomaly detection can be realized by utilizing the reconstruction loss from the well-trained AE, as introduced in Sec.~\ref{subsubsec:dl} and also applied for outlier detection in HICs in Sec.~\ref{outlier}, to single out newly confronted data those are showing larger reconstruction error compared to training classes. Thus, by investigating the loss map of the trained AE on different parameter space of a physical system (with input could be full state vector or entanglement spectrum or correlations for the system), one can possibly map out the phase diagram without physical a priori knowledge for example of the order parameter. This anomaly detection scheme got tested on the extended Bose-Hubbard Model, which shows a rich phase diagram. Aside from the several standard phases, the method also reveals a new phase showing unexpected properties on the phase diagram. 

Under a similar scheme, the generative adversarial network (GAN) is trained in Ref.~\cite{Contessi:2021mrn} as the anomaly detector to identify gapless-to-gapped phase transition in different one-dimensional models. Specifically, the detection of the elusive Berezinskii--Kosterlitz--Thouless (BKT) phase transitions in the XYZ spin chain, the Bose--Hubbard model and the generalized two-component Bose-Hubbard model (all at zero temperature) is demonstrated~\cite{Contessi:2021mrn} with entanglement spectrum (measuring the degree of quantum correlation among sub-portions of the system) as dataset.

\emph{\textbf{Interpretable Learning}} --- 
Though phases of matter are shown to be detectable through supervised or unsupervised learning strategies, it's not clear yet in physics interpretation what the learning algorithm captured from such classification tasks. Some early works explored under the supervised kernel framework of support vector machine (SVM)\footnote{Briefly speaking, in the course of binary classification given training set $(\mathbf{x}^{(k)},y^{(k)}\in\{\pm 1 \})$, the SVM aims at determining a decision boundary, $\mathbf{\omega} \cdot \mathbf{x} +b=0$, a hyperplane with parameters $\mathbf{\omega}$ and $b$ to separate data into two classes. To clearly separate the data, a margin without any training data contained is inserted, which is with the boundaries defined as $\mathbf{\omega}\cdot\mathbf{x}+b=\pm 1$, and the margin width $2/||\omega||$ is expected to be maximized in generating the best separation of data. $\mathbf{\omega}\cdot\mathbf{x}+b=d(\mathbf{x})$ provide the \textbf{decision function}.} to give interpretable decision functions and further physical discriminators from the trained machines~\cite{2017PhRvB..96t5146P,2019PhRvB..99j4410L,2019PhRvB..99f0404G}.
Ref.~\cite{Wetzel:2017ooo} proposed a \textit{correlation probing neural network} to reveal the fact that the learned decision functions originate from physical quantities. It is also shown that a full explicit expression of the learned decision function can be reconstructed, from which one can further extract the quantities to facilitate the network's decision-making in classifying phases. The proposed procedure get demonstrated first on Ising model, where it digged out the magnetization and energy per spin as the decision support of the trained network. Then on SU(2) lattice gauge theory, a whole ML pipeline combining PCA and the \textit{correlation probing neural network} is constructed to examine the deconfinement phase transition related. The PCA with the average reconstruction loss serving as a universal phase transition identifier is shown to be able to capture the phase indicator, though the involved Polyakov loop is a non-linear order parameter in SU(2) gauge theory. This enables the awareness of an existing phase transition unsupervisedly. Then the \textit{correlation probing network} is trained to correctly predict phases and further construct the explicit expression of the decision function, to which it is found that the decision is based upon the Polyakov loop as a non-local and non-linear order parameter.

In many of the ML based explorations for physics study, the more complicated algorithms though might be with better performance yet quite often lacking transparency and interpretability, especially when one seeks for new physical insight or comprehension of the learned representations from the data. In  
Ref.\cite{Blucher:2020mjt} it was proposed to adopt ``explainable AI'' techniques--specifically the layer-wise relevance propagation (LRP) method--to identify relevant features that influence the trained algorithms towards or against the particular recognition result. The work takes the (2+1) dimensional scalar Yukawa theory as demonstration context, which displays an interesting phase structure with two broken phases (ferromagnetic denoted as FM and antiferromagnetic denoted as AFM) separated by a symmetric paramagnetic phase (PM). These can be indicated by the normal magnetisation and the staggered magnetisation. With both the field configurations and preprocessed physical observables prepared, several machine learning models were trained to infer the action parameters (the hopping parameter $\kappa$ is taken in this work) from the known observables (labelled as approach A) or solely from the raw field configurations (labelled as approach B). It should be noted that, the action parameter learning in this work is taken as just a pretext task to reveal the underlying phase structure or related physical insights. This is achieved by the adopted LRP to propagate the initially assigned relevance on the output to input layer by layer.

\emph{\textbf{Observables' Regression in QFT}} ---
The application of deep neural network for regressive tasks in lattice quantum field theoretical setting was explored in
Ref~\cite{Zhou:2018ill} to unravel the dynamical information related to phase transition and physical observables. In Ref~\cite{Zhou:2018ill} the authors considered a complex massive scalar field with quartic coupling $\lambda$ in $(1+1)$-dimensional Euclidean space-time at nonzero temperature, and a finite chemical potential $\mu$ is introduced to control the charge density fluctuation, which also makes the action complex. The worldline formalism is taken for simulating the field configurations and circumvent the sign problem involved. Correspondingly, this $1+1$-d charged $\phi^4$ field is fully represented by 4 interger-valued dual variables: $k_1$, $k_2$ and $l_1$, $l_2$. Observables like number density and field square can also be calculated with the re-expressed partition function under the dualization approach. 
The ``silver blaze'' behavior is expected for this system at low temperature and low chemical potential $\mu$, that the particle density gets suppressed (with a mass gap) until some threshold value $\mu>\mu_{th}$ then enter the condensate region and increase considerably. Though for 2-dimensional systems there is no real phase transition related to symmetry breaking, one can refer to this pronounced change in density as a transition to condensation as treated in Ref.~\cite{Zhou:2018ill}. With the configurations training set prepared, two regressive tasks were attempted: phase identification and physical observable regression. 

For the phase identification,  a convolutional neural network (CNN) was devised to perform the phase binary classification task using the field configurations as input. Purposely, the training set is prepared to consist only two ensembles of configurations with one well above and one well below the transition point ($\mu_{th}$), while the testing set is constructed with many ensembles of configurations at different chemical potential values sit in between the two chemical potential values of training ensembles. The network output is interpreted as \textit{condensation probability}, $P(\phi)$, for each inputted configuration. Strong correlations were observed between the network output and number density or field square, without any specific supervision on the role of these observables in distinguishing the phases. The ensemble average of the network predicted condensation probability, $\langle P(\phi) \rangle$ serves as an accurate \textit{phase classifier}, with its first non-vanishing point well indicating the transition threshold value. It is worthy to note that such concept of interpreting the network output $P$ as an observable was also extended to undergo histogram reweighting~\cite{Bachtis:2020dmf}\footnote{Specifically, the reweighting in terms of inverse temperature is performed for 2-dimensional Ising system as $\langle P\rangle_{\beta}=\frac{\sum^N_i P_{\sigma_{i=1}}\exp(-(\beta-\beta_0)H(\sigma_i))}{\sum^N_{i=1}\exp(-(\beta-\beta_0)H(\sigma_i))}$, with $\beta_0$ the place of inverse temperature where MCMC measurements for $P$ is given.} to construct an effective order parameter and perform scaling analysis (see also combination with transfer learning in studying unknown phase transitions~\cite{Bachtis:2020ajb}. Ref.\cite{Zhou:2018ill} further tried to reduce the input features of this phase classification task, and found that with even restricted training input e.g., only $l$ or even only $k_1$ variables, the network still can well distinguish the two phases. Note that the ``order parameter''--number density of the system, $n$, is given by the sum of $k_2$ variables for this field system (See left panel of Fig.~\ref{fig:scalar_regress}). This thus indicate that the network have captured hidden structures in $k_2$ variables, though it ($k_2$) conventionally is not able to distinguish the low-density and high-density phases.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp!]
  \centering
  \includegraphics[width = 0.41\textwidth]{figures/condprob_avg.pdf}
  \includegraphics[width = 0.42\textwidth]{figures/regress_phi.pdf}
  \caption{(left) Ensemble average of condensation probability in testing stage along with chemical potential; (right) Comparison of the network predicted squared field $\phi^2$ and the true values at different chemical potentials. Taken from Ref.~\cite{Zhou:2018ill} with permission.}
  \label{fig:scalar_regress}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The same supervised training paradigm was used for the observables (number density $n$ and field square $\phi^2$)\footnote{Specifically the number density is given by $n=\frac{1}{N_1 N_2}\sum_x k_2(x)$ and the squared field is given by $|\phi|^2=\frac{1}{N_1 N_2}\sum_x\frac{W[s(k,l;x)+2]}{W[s(k,l;x)]}$ where the weight $W[s]=\int_0^{\infty}r^{s+1}e^{-(4+m^2)^2-\lambda r^4dr}$ and $s(k,l;x)=\sum_{\nu}[|k_{\nu}(x)| + |k_{\nu}(x-\hat{\nu})| + 2(l_{\nu}(x)+l_{\nu}(x-\hat{\nu}))]$.} regression task, with only the network output layer adapted to linear activation, and the loss function for training changed from cross entropy (for classification) to MSE. The same two ensembles of configurations were taken as the training set, with which the regression network is trained and then tested on previously unseen configurations at also different chemical potential showing different number density and field square. The regression performance was found to be successful over a broad range of chemical potentials, as shown in the right panel of Fig.~\ref{fig:scalar_regress}. While for the number density regression the needed pattern may seem simple, the field square is a highly non-linear function of the high-dimensional input (i.e., field configurations with $4\times N_1 \times N_2$=8000 entries). Thus, the good predictive ability of the network in regressing field square is non-trivial and impressive. Similar findings in the context of lattice Yang-Mills theories (SU(2) and SU(3)) about the transferability of the neural network learned regression function to a different parameter space are also reported in Ref.~\cite{Boyda:2020nfh}, where the Polyakov loop as gauge invariant deconfinement order parameter is the prediction target. 
For similar tasks investigated in Ref.~\cite{Zhou:2018ill}, authors in Ref.~\cite{Bulusu:2021rqz} further explored the influences of translational equivariance satisfication of the used network structure on the regression performance and generalization capabilities.
Note also there's earlier trial~\cite{Yoon:2018krb} with traditional machine learning technique, specifically a boosted decision tree (BDT) regression algorithm, to reduce the computational cost of evaluating lattice QCD observables, by means of predicting observable from simpler and less compute-intensive observables' evaluation those are correlated with the target observable. 

\emph{\textbf{Enhanced Regression with Symmetries Embedded Networks}} --- 
Interactions for physics systems always respect some symmetries, which possess fundamental importance to physics theories across all scales nowadays. The incorporation of the symmetries for the system into the analyzing procedures like machine learning architectures has been proven to be beneficial in improving the performance of the algorithms. One such popular simple example is the convolutional neural network (CNN), which is good at pattern recognition for image-like data structure because of the satisfied global translational equivariance (as manifested in the sharing of convolutional kernels). This concept has now been extended to yield up group equivariant CNNs (G-CNN)~\cite{2016arXiv160207576C}, where more general symmetries including rotations and reflections are discussed, with also local symmetry e.g., for data on curved manifolds~\cite{2019arXiv190204615C}. See also a recent snowmass white paper~\cite{Bogatskiy:2022hub} for a report about symmetry group equivariant architectures across physics studies.
In the context of quantum field theory, symmetries provide important constraints on the action and thus are essential for lattice field theory study, their proper consideration is also crucial in obtaining meaningful results in lattice simulations.

As the fundamental theory for strong interactions that guide high energy nuclear physics phenomenon, QCD is a (non-abelian) gauge theory that the Lagrangian should be invariant under local symmetry transformations that form symmetry group SU(3). Being relevant, there's lattice gauge equivariant (LGE) CNNs being proposed recently~\cite{Favoni:2020reg}. Consider a SU($N_c$) Yang-Mills theory on a lattice $\Lambda$ and discretized in terms of links variables (parallel transporters) $U_{\mu}(x)=\exp[-igA^{\mu}(x+a\hat{\mu}/2)]$,  the gauge links are transformed by group elements $\Omega_x$ as 
\begin{equation}
U_\mu(x) \to \tilde{U}_\mu(x) = \Omega(x) {U}_\mu(x) \Omega^\dagger(x+\hat{\mu}),
\label{eq:u_gauge}
\end{equation}
with $\Omega:\mathbb{R}^4\to$SU($N_c$) gauge transformations of the gauge fields. Taking the Wilson action as approximation for the Yang-Mills theory with coupling $g$,
\begin{equation}
S_W[U]=\frac{2}{g^2}\sum_{x\in\Lambda}\sum_{\mu<\nu} \mathrm{Re} \mathrm{Tr}[\mathbbm{1}-U_{\mu\nu}(x)],
\label{eq:wilson_action}
\end{equation}
with $U_{\mu\nu}(x)$ the plaquette ($1\times1$ Wilson loop),
\begin{equation}
U_{\mu\nu}(x)=U_{\mu}(x)U_{\nu}(x+\mu)U_{-\mu}(x+\mu+\nu)U_{-\nu}(x+\nu),
\label{eq:plaquette}
\end{equation}
and transform under gauge transformation locally as $U_{\mu\nu}(x)\to\Omega(x)U_{\mu\nu}(x)\Omega^{\dagger}(x)$.To construct lattice gauge equivariant network architectures, Ref.~\cite{Favoni:2020reg} devised several elementary layers to explicitly respect the gauge symmetry. One essential starting point is processing the input fields to be tuples $(\mathcal{U,W})$, where $\mathcal{U}=\{U_{\mu}(x) \}$ the set of links of the configuration and $\mathcal{W}=\{ W_i(x) \}$ with $W_i(x)\in\mathbb{C}^{N_c\times N_c}$ a set of locally transforming complex matrices like the plaquettes is used as example ($W_i(x)\to\Omega(x)W_i(x)\Omega^{\dagger}(x)$, note that Polyakov loops can also be included as stated in Ref.~\cite{Favoni:2020reg}). Then two gauge equivariant operations are introduced to act on the tuple data, $(\mathcal{U,W})$, one is performing convolutions named as LGE convolution (L-Convs) and the other is LGE bilinear layer shortly named as L-Bilin, both leaving the gauge links variables unchanged while modifying only the $\mathcal{W}$ part in covariant manner. Specifically, the L-Convs generalizes the normal convolutional operation to account for the parallel transport under geodesics to meet the requirement of gauge equivariance,
\begin{equation}
W^{'}_i(x)=\sum_{j,\mu,k}\omega_{i,j,\mu,k}U_{k\cdot \mu}(x)W_j(x+k\cdot \mu)U^{\dagger}_{k\cdot \mu}(x),
\label{eq:l_convs}
\end{equation}
with the kernel weights $\omega_{i,j,\mu,k}\in\mathbb{C}$ and $1\le i\le N_{ch,out}, 1\le j\le N_{ch,in}, 0\le\mu\le D, -K\le k\le K$ where $K$ specifies the kernel size and $N_{ch}$ the feature map channel number. Such L-Convs operation combines data at different lattice sites with parallel transport well taken into account. The L-Bilin layer on the other hand is acting on a single lattice site, which combines two input tuples with bilinear product (note again the $\mathcal{U}$ part are the same after operation):
\begin{equation}
W^{''}_i(x)=\sum_{j,k}\alpha_{i,j,k}W_j(x)W^{'}_j(x),
\label{eq:l_bilin}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp!]
  \centering
  \includegraphics[width = 0.8\textwidth]{figures/L-CNNs.pdf}
  \caption{A generic lattice gauge equivariant CNN as from Ref.~\cite{Favoni:2020reg} with permission.}
  \label{fig:lge_cnn}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
with the trainable weights $\alpha_{i,j,k}\in\mathbb{C}$ and $1\le i\le N_{out}, 1\le j\le N_{in,1}, 1\le k\le N_{in,2}$. Besides L-Convs and L-Bilin, LGE activation function and LGE Trace layer, plaquettes and Polyakov loops calculation layers (pre-processing layer to prepare the $\mathcal{W}$ information) are also proposed as elementary component to construct the gauge equivariant L-CNNs for lattice gauge field configuration treatment. See Fig.~\ref{fig:lge_cnn} for a typical L-CNNs comprising the above proposed LGE layers as from Ref.~\cite{Favoni:2020reg}. It is further demonstrated that such specially devised L-CNNs surpasses traditional CNN models in regression of gauge invariant observables. 


\subsubsection{Variational Neural-Network Quantum States}
\label{vnqs}
Many of the previous studies shown in above rely on existed or pre-prepared ensembles of configurations for the physical system, until Ref.~\cite{2017Sci...355..602C} first introduced artificial neural networks to represent the wave function and presents a stochastic reinforcement learning scheme for solving the many-body problem without prior knowledge of exact samples. This gives state-of-the-art accurate description of both the ground-state and time-dependent quantum states for given Hamiltonian $\mathcal{H}$ in several prototypical spin systems indlucing 1 and 2-d transverse-field Ising (IFI) and anti-feromagnetic Heisenberg (AFT) models. The corresponding wave function represented by a neural network which maps from the N discrete-valued degrees of freedom set $\mathcal{S}= (\mathcal{S}_1,\mathcal{S}_2, ..., \mathcal{S}_N)$ to the complex phase and amplitude information, $\Psi(\mathcal{S})$ (taking the Restricted Boltzmann machine, RBM\footnote{Note in Ref.~\cite{2018PhRvB..97h5104C} it was demonstrated that the RBM possess equivalence to tensor network states that widely used in quantum many-body physics}, with M hidden spin variables $h_i=\{\pm 1\}$ for example), 
\begin{equation}
\Psi_M(\mathcal{S};,\mathcal{W})=\Sigma_{h_i}e^{\Sigma_j a_j \mathcal{S}_j + \Sigma_i b_i h_i + \Sigma_{ij}W_{ij}h_i\mathcal{S}_j},
    \label{eq:nqs}
\end{equation}
is termed as neural-network quantum states (NQS) with the trainable parameters $\mathcal{W}=\{a_i, b_i, W_{ij}\}$. Such RBM representation is formally equivalent to a two-layers feed-forward neural network with special activation functions, i.e., $z^1(x)=\log\cosh(x)$,$z^2(x)=\exp(x)$.
Through minimization of the energy expectation $E(\mathcal{W})=\langle\Psi_M|\mathcal{H}|\Psi_M\rangle/\langle \Psi_M|\Psi_M \rangle$, the network parameters $\mathcal{W}$ can be optimized via variational Monte Carlo (VMC) sampling. It's shown that this proposed scheme can accurately evaluate the ground state energy in the TFI and AFH examples. Later an extension for this approach to calculate excited states was also introduced with both RBM and deeper fully connected neural networks~\cite{2018PhRvL.121p7204C}.

For dynamical properties of the many-body state which is upon solution of the time-dependent Schr\"odinger equation, the NQS also works with extension of the network parameters to be complex-valued and time-dependent $\mathcal{W}(t)$~\cite{2017Sci...355..602C}. Accordingly, per the Dirac-Frenkel time-dependent variational principle, the network parameter at each time $t$ can be trained taking the variational residuals as the objective function,
\begin{equation}
R(t:\dot{\mathcal{W}}(t))=dist(\partial_t\Psi(\mathcal{W}(t)),-i\mathcal{H}\Psi),
    \label{eq:nqs_t}
\end{equation}
which is achieved stochastically by a time-dependent VMC method. On both TFI and AFH models, this time-dependent NQS scheme is demonstrated to capture with high accuracy the unitary dynamics induced by quantum quenches. There are further developments along using NQS for many-body physics studies, see Refs.~\cite{Noormandipour:2020dqp,2020PhRvL.124b0503S,2021PhRvR...3d2024W} and Refs.~\cite{2021PRXQ....2d0201C,2021arXiv210111099C}.

\subsubsection{Real-Time Dynamics Analysis}\label{subsubsec:realt}

Within modern theoretical physics, the dynamics of strongly correlated systems holds the central role for many pressing research problems, e.g., the hadronic spectrum/behaviors at zero temperature or immersed inside a thermal medium~\cite{Asakawa:2000tr,Rothkopf:2022fyo}, the non-equilibrium evolution and transport properties for the created QGP in heavy ion collisions~\cite{Rothkopf:2019ipj,Zhao:2020jqu}, the understanding for parton distribution functions of nucleons and nuclei~\cite{Ji:2020ect,Constantinou:2020pek,Candido:2023nnb}. The computation of these real-time physics is often noncompliance to perturbative analysis, thus calls for nonperturbative treatment such as lattice QFT simulations. These first principle Monte-Carlo based simulations are usually performed in Euclidean space-time (after a Wick rotation $t\to i t\equiv\tau$) and provides only Euclidean correlators.
Accessing real-time physics from imaginary-time correlation's “measurements” in quantum Monte-Carlo or lattice QFT simulation generally forms ill-conditioned inverse problems. Spectral representation forms a bridge to approach the real-time information of the dynamics from the Euclidean correlators. Also, quite often the relevant physics can be decoded directly from the spectral functions, like transport coefficients or in-medium hadronic behaviors~\cite{Asakawa:2000tr}.

\emph{\textbf{Spectral Function Reconstruction}} ---
The involved spectral reconstruction problem can in general be cast from a Fredholm equation of the first kind, $g(t)=\int_a^b K(t, s)\rho(s)ds$, with the aim of rebuilding the function $\rho(s)$ given the kernel function $K(t, s)$ and limited numerical evaluation on $g(t)$. Once only a finite set of evaluation data with non-vanishing uncertainties are possible for $g(t)$, the inverse transformation of the above convolution becomes ill-conditioned (see Ref.~\cite{Shi:2022yqw} and Sec.~\ref{sec:5:inverse} for more details). Basically, one can expand the convolution kernel (as a linear operator) by basis functions in a Hilbert space, within which it's shown in Refs.~\cite{J_G_McWhirter_1978} and \cite{Shi:2022yqw} respectively that the Laplace transformation kernel, $K(t,s)=e^{-st}$, and K\"allen-Lehmann (KL) kernel, $K(t,s)=s(s^2+t^2)/\pi$, possess arbitrarily small eigenvalues thus correspond to eigenfunctions being able to induce negligible changes for the integral result of $g(t)$. Consequently, for the inversion operator these eigenfunctions, termed as null-modes, are related to arbitrarily large eigenvalues and will bring about numerically unstable inversion from noisy $g(t)$ to $\rho(s)$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp!]
  \centering
  \includegraphics[width = 0.7\textwidth]{figures/spectral_examples.pdf}
  \caption{Spectral functions differed by null-modes (left) and their corresponding K\"allen--Lehmann correlation functions (right). The insert figure shows the differences-in-propagator caused by null-modes. Taken from~\cite{Wang:2021jou} with permission.}
  \label{fig:spectral_samples}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In the context of QFTs the involved target function $\rho(s)$ is the spectral functions, and the integral $g(t)$ corresponds to correlator which can be measured from lattice calculations. To break the degeneracy for facilitating the inversion related, different regulator terms indicating prior domain knowledge have been proposed over the past several decades, such as Tikhonov regularization(L2 regularization)~\cite{Tikhonov1943OnTS,tikhonov1995numerical}, sparse modeling approach(L1 regularization)~\cite{Otsuki:2017sma,Itou:2020azb}, maximum entropy method (MEM)~\cite{JARRELL1996133,Asakawa:2000tr} and related Bayesian Reconstruction (BR) method~\cite{Burnier:2013nla}.

Recently there are also studies incorporating machine learning methods into solving this ill-posed problem, mainly in supervised manner under data-driven paradigm, including also unsupervised learning based trials. 
As the early stage ML application, in Ref.~\cite{2016arXiv161204895A} the kernel riedge regression (KRR) and kernel quantile regression (KQR) models are adopted to invert the Fredholm integral of the first kind. Through the preparation of training database and also the restriction on basis functions and kernel involved, a regularization is naturally provided by such projected regression treatment to tame the ill-conditioned inversion.

Ref.~\cite{2018PhRvB..98x5101Y} firstly introduced deep convolutional neural network (CNN) and variants of stochastic gradient descent optimizer into the spectral reconstruction from imaginary time Green's function, which is domain-knowledge-free as distinct from Ref.~\cite{2016arXiv161204895A}. Being demonstrated on a Mott-Hubbard insulator and metallic spectrum, the deep CNN gives good reconstruction performance superior to the classical MEM method. It's also found that the usage of CNN structure achieved better reconstruction than fully connected neural network structure. A similar strategy was taken in Ref.~\cite{PhysRevLett.124.056401} with the principal component analysis (PCA) also introduced to reduce the dimensionality of the QMC simulated imaginary time correlation function. On a prototypical problem of quantum harmonic oscillator linearly coupled to an ideal heat bath, which is physically more relevant scenario, Ref.~\cite{PhysRevLett.124.056401} demonstrated that the deep neural network with PCA processed input can outperform the MEM approach in reconstructing the spectral from the single particle fermionic Green's function, especially while the noise level increases for the data. Ref.~\cite{Kades:2019wtd} further pushed such data-driven supervised approach to QFT context, with the K\"allen--Lehmann (KL) spectral representation considered. The database is prepared in the form of combination of Breit--Wigner peaks, $\rho^{BW}(\omega)=4A\Gamma\omega/((M^2+\Gamma^2-\omega^2)^2+4\Gamma^2\omega^2)$. Then in representing the network output -- spectral function, two schemes are studied: one is with parameters of the Breit--Wiger peaks inside the spectral function and the other is with list of discretised data points of the spectral directly. The reconstruction performance is found to be at least comparable and again surpassing classical methods at large noise cases. Being inspired from the well-designed Shannon--Jaynes entropy term in regularizing the ill-posed inverse problem, Ref.~\cite{Chen:2021giw} proposed a novel framework called SVAE based on the variational autoencoder (VAE) together with the entropy term “S” included in the loss function to reconstruct spectral functions from Euclidean correlators. A Gaussian mixture model is adopted to generate the spectral function database, while physically motivated spectral corresponded correlators are prepared for the test. Realistic noise level of lattice QCD data is implemented in both the training and testing data, and it was found that the trained SVAE in most of the cases gives comparable reconstruction quality as to MEM, and in cases with sharp spectral peaks with fewer data points for the correlator, SVAE shows superior results than MEM.

The above-mentioned studies more or less all rely on training data set preparation to regularize the inverse problem, thus may have dependency on the specific kinds of training data as well. There are also studies taking unsupervised learning strategy to perform the inversion directly, like Ref.~\cite{Zhou:2021bvw} adopt the radial basis function network (RBFN) to represent the spectral, basically approximating the spectral as linearly combined radial basis function (RBF),
\begin{equation}
  \rho(\omega)=\sum_{j=1}^N w_{j}\phi(\omega-m_j),\label{eq:linear_summation}
\end{equation}
with $\phi$ the active RBF unit at adjustable center $m_j$ and $w_j$ the trainable weight. In discretized form, Eq.~\ref{eq:linear_summation} can be rewritten in matrix format $\rho=\Phi\,W$, with which the K\"allen-Lehmann spectral representation integral becomes
\begin{equation}\label{eq:RBFMatForm}
  G_i=\sum_{j=1}^{M} \sum_{k=1}^N K_{ij}\Phi_{jk}w_k\equiv \sum_{k=1}^{M} \tilde{K}_{ik}w_k, \ \ \ i=1,\cdots,\widehat{N}
\end{equation}
with $\tilde{K}$ an irreversible $\hat{N}\times M$ matrix. By setting $M=N$ then the truncated singular value decomposition (TSVD) method can be naturally applied to solve $w_j$. As compared to supervised learning applications, this method is fast in training and also free of over-fitting issue. Compared to traditional methods, RBFN resulted in better spectral reconstruction, especially about the low frequency part which matters to the transport coefficients extraction in Kubo formula.

As another alternative representation, Gaussian Processes (GP) are incorporated within Bayesian inference procedure for reconstructing 2+1 flavor QCD ghost and gluon spectral~\cite{Horak:2021syv}. GP in general defines a probability distribution over a functional characterized by the chosen kernel function, 
\begin{align}
\rho(\omega)\sim \mathcal{GP}(\mu(\omega),C(\omega, \omega')),
\end{align}
where $\mu(\omega)$ is the mean function usually set as zeros, and $C(\omega,\omega')$ is the covariance dictated by the kernel function. Ref.~\cite{Horak:2021syv} used the radial basis function (RBF) kernel. Actually, it's proved that GP is equivalent to infinite wide neural network. In this sense, the spectral representation in Ref.~\cite{Horak:2021syv} somehow is limiting push for the one in Ref.~\cite{Zhou:2021bvw} since the RBF activation usage. One distinctive point is that Ref.~\cite{Horak:2021syv} plug such GP represented spectral priors into the Bayesian framework to construct the likelihood of the ghost and gluon spectral. The corresponding reconstruction of the spectral function for ghost and gluons shows similar peak structure as to fRG reconstruction of the Yang--Mills propagator.

In Refs.~\cite{Wang:2021jou,Wang:2021cqw, Shi:2022yqw} a different approach based upon automatic differentiation and general deep neural network representation [$\rho(\omega)=NN(\omega)$] has been devised (see Fig.~\ref{fig:5:inverse:ad} for the flow chart), which belongs to unsupervised category as well thus avoided the over-fitting issue and does not rely on training data preparation in advance. Since its general tackling of inverse problem as a strategy, we summarized them in Sec.~\ref{sec:5:inverse}, refer there for more technical details and explanations on the corresponding results.

\emph{\textbf{In-medium Heavy Quark Potential}} ---
Another interesting and important real-time physics in the context of high energy nuclear physics lies in the in-medium effects of hard probes, for example the jets or heavy quarkonium (bound states of heavy quark and its anti-quark). Regarding as a smoking gun for the creation of QGP, heavy quarkonium has been intensively studied both theoretically~\cite{Chen:2012gg, Zhao:2010nk,Zhou:2014kka,Zhao:2020jqu,Rothkopf:2019ipj} and experimentally~\cite{CMS:2011all,CMS:2012gvv}. One central focus is to understand the in-medium heavy quark interaction, the computation of which represents a big challenge for non-perturbative strong interaction calculations. Because of the large mass and small relative velocities for the inter quarks inside the bound state, non-relativistic treatment of it is allowed, and also the color electric interactions inside will be dominant. It has been long expected that color screening will weaken the heavy quark interaction, in analogy with Debye screening phenomenon for QED. Additionally,  studies from both the hard thermal loop (HTL) calculations~\cite{Laine:2006ns,Beraudo:2007ky} and the recent effective field theory approach e.g., pNRQCD calculations~\cite{Brambilla:2008cx,Brambilla:2010vq} point out that a non-vanishing imaginary part for the heavy quark interaction will appear in the QCD medium beyond just screening effect. For a complete understanding, the non-perturbative framework like lattice QCD is called for. In the past decade, there are studies for real-time heavy quark interaction based upon lattice QCD calculations, mainly with a Bayesian technique~\cite{Rothkopf:2011db,Burnier:2014ssa,Burnier:2015tda} used for spectral functions of the thermal Wilson loop. On the other hand, quantification of the in-medium Bottomonium masses and thermal widths are released from the very recent lattice QCD studies~\cite{Larsen:2019bwy,Larsen:2019zqv,Larsen:2020rjk}. Empirically, it'd be interesting to know if one in-medium heavy quark potential $V(T,r)$ under a QM potential picture can accommodate these in-medium properties from lattice studies, since this can not be answered yet from field theoretic point of view.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!hbtp]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/hq_flow_chart.pdf}
    \caption{Flow chart of in-medium heavy quark empirical potential reconstruction from LQCD measurements of mass and thermal width. Taken from Ref.~\cite{Shi:2021qri} with permission.\label{fig:hq_flow_chart}}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Ref.~\cite{Shi:2021qri} accordingly devised a DNN-based method to infer the in-medium heavy quark interaction starting from the lattice QCD released in-medium properties for Bottomonium. The DNN is introduced to parametrize the temperature and inter-quark distance dependent complex potential $V(T,r)$ in a model independent manner, and coupled to the Schr\:odinger equation to give rise to the bound state mass and thermal width. By comparing to the lattice QCD data correspondingly, the $\chi_2$ can be evaluated serving as the loss function to optimize the DNN representation. Perturbative response analysis is performed to derive the gradient of the loss function with respect to network parameters which involves naturally the Feynmann-Hellman theorem, with which the optimization can be done using stochastic gradient descent algorithms. Fig.~\ref{fig:hq_flow_chart} displays the flow chart of this DNN-based automatic differentiation inference for heavy quark potential. The uncertainties of the reconstruction can be properly evaluated via Bayesian inference, which take into account both the aleatoric and epistemic uncertainty by construction. For more details, see Sec.~\ref{sec:5:inverse}.

\emph{\textbf{Parton Distribution Function Reconstruction}} ---
The parton distribution function (PDF) is a fundamental property that reveals the hadron inner structure, which specifically depicts the probability distribution of the momentum fraction carried by the constituent quarks and gluons inside the hadron. It can be measured in high energy deep inelastic scattering experiments and also computed in theoretical calculations, such as lattice field theory. One may refer to~\cite{Lin:2017snn, Forte:2020yip} for an overall review of recent developments on the study of PDF. There have been some pioneer efforts to employ deep learning techniques to help to extract the parton distribution function in a nucleon, where the NNPDF approach~\cite{Forte:2002fg,Ball:2009mk,Ball:2010de,Ball:2012cx,NNPDF:2014otw,Bertone:2017tyb} for global QCD analyses has been systematically applied in determining the PDF in proton and also the fragmentation functions (FFs) in the light-hadron. With neural network parametrization for the PDF, NNPDF performs the global QCD fit based on high energy collider experimental data and higher order perturbative calculation in QCD and QED/Electroweak theory. This methodology was also implemented for the determination of the nuclear parton distribution function (nPDF)~\cite{AbdulKhalek:2019mzd,AbdulKhalek:2020yuc,AbdulKhalek:2022fyi} with the $\chi^2$ minimization achieved via stochastic gradient descent. 

In the practice of lattice QCD calculation, a useful intermediate quantity is the Ioffe-time distribution --- the Fourier transformation of the PDF, $Q_\mathrm{Ioffe}(\lambda;\mu) = \int_{-1}^{1} dx\, e^{ix\lambda}\, q_\mathrm{PDF}(x;\mu)$, where $\mu$ is the energy scale, $x$ the momentum fraction, and $\lambda$ the Ioffe-time. Meanwhile, the observables that can be computed in lattice QCD calculation can be expressed as a convolution of the Ioffe-time distribution. Therefore, if $Q_\mathrm{Ioffe}(\lambda;\mu)$ is obtained from lattice QCD calculation, one can then perform the inverse Fourier transformation and compute the PDF.
Ref.~\cite{Karpie:2019eiq} reconstructed the Ioffe time distribution using two approaches, i) a Bayesian Inference reconstruction and ii) DNN representation. In the latter, the network parameters are updated according to a generic algorithm, which takes random walks in the network parameter space and select the configurations that reduce the difference between the desired data and the reconstructed ones.
In Ref.~\cite{Gao:2022iex}, the authors implemented DNN to represent $Q_\mathrm{Ioffe}(\lambda;\mu)$ and applied the gradient driven update method that is described in Sec.~\ref{sec:5:inverse} to optimize the network parameters. The new method significantly increases the efficiency and accuracy of the Ioffe-time distribution reconstruction.
More applications can be found in Refs.~\cite{Forte:2002fg, Forte:2002us, Zhang:2019qiq, DelDebbio:2020rgv, DelDebbio:2021whr}.

\subsection{Sign Problem}
\label{sec:4:sign}
There have been many efforts to find ways to overcome the sign problem in lattice QCD, but it remains a challenging and active area of research (see recent reviews in Ref.~\cite{Berger:2019odf,Alexandru:2020wrj,Nagata:2021ugx}). The most head-on method can be summarized as “statistical approach”, which aims to improve the statistic directly. \textit{Reweighting} the observable with phase factors~\cite{Ferrenberg:1988yz} and representing the actions with \textit{density-of-states}~\cite{Wang:2000fzi} are two practical attempts. The alternative head-on methods contain, e.g., Taylor expansions to $\mu/T$ around zero chemical potential~\cite{Allton:2002zi,Borsanyi:2015axp}, analytic continuation from imaginary chemical potential~\cite{deForcrand:2002hgr,deForcrand:2009zkb}. The other branch of approaches is the “new variables” method. It attempts to tackle the problem by redefining a new set of variables to reformulate the complex action. \textit{Dualization} is one successful example, in which the dual variables can afford a representation of the partition function in terms of positive quantities~\cite{Rossi:1984cv,Berger:2019odf}. To handle with the action on the complex plane properly, people have developed complex Langevin methods and integration contour deformations. The former originates from stochastic quantization~\cite{Parisi:1980ys}, processing the complex action with two coupled Langevin dynamics~\cite{Aarts:2013uxa,Attanasio:2020spv}. The latter should rely on the \textit{thimble method}, and the latest advances in using machine learning techniques focus on this approach~\cite{Alexandru:2020wrj}. Both the complex Langevin and Lefschetz thimbles methods rely on a complexification of the field degrees of freedom to move the integration over paths into the complex plane.

The key idea of the \textit{thimble method} is to continuously deform the integration contour for the path integral from the original real fields ($\in\mathbb{R}^{n}$) into an N-dimensional real manifold $\mathcal{M}$ immersed in the complexified field space ($\in\mathbb{C}^n$), on which the dramatic phase fluctuations induced by complex actions can be suppressed or even removed~\cite{1997CPL...270..382R}. Earlier attempts chose the manifold $\mathcal{M}$ as the set of Lefshetz thimbles resembling the high-dimensional generalization of the “steepest descent direction” or “stationary phase path”, and the induced integrand turns out to be real and positive up to an overall phase over the thimble~\cite{Cristoforetti:2012su,Cristoforetti:2013wha}. The reason is that the imaginary part of the action, $S_I[\phi]$, becomes locally constant and the real part, $S_R[\phi]$, is located as close as possible to the “saddle point”, which constructs the best landscape to perform stochastic evaluations of the path integrals. This further inspired the “generalized thimble method” ~\cite{Alexandru:2015sua,Nishimura:2017vav}, where the integration contour is deformed to a manifold $\mathcal{M}_T$ chosen as the evolution results of the so-called holomorphic gradient flow equation by a fixed flow time T starting from the original integration region,
\begin{equation}
    \frac{d\phi}{dt} = \frac{\overline{\partial S}}{\partial \phi},
\end{equation}
where $S$ is a generic Euclidean action and the bar indicates the complex conjugation. The time $t$ is an auxiliary variable, which denotes the evolution of the equation. Any flowed configuration $\phi(T)$ (which collectively forms the manifold $\mathcal{M}_T$) is uniquely corresponding to one original configuration, $\phi(t=0)=\zeta\in\mathbb{R}^n$ thus defines a one-to-one mapping $\tilde{\phi}(\zeta)=\phi(T)$. Starting from the initial point $\phi(0)\equiv \zeta $, as the flow time increases, $\tilde{\phi}(\zeta)\rightarrow\phi(T)$, the set of fields eventually approaches the right combination of thimbles which equals the original integral. 


\subsubsection{NN-based Manifold}
To approach the proper thimbles for mitigating the sign problem in larger systems, large-enough flow time is necessary which would induce large computational consumption, especially for evaluating the required Jacobian. In Ref.~\cite{Alexandru:2017czx}, to avoid directly solving the gradient flow equation and Jacobian evluation, the authors first proposed to train a feed-forward neural network to approximate the thimble (or the generalized manifold), accordingly the network can be termed as “\textit{learnifold}” in approaching the flowed manifold by predicting its corresponding imaginary part based upon a real configuration input $\phi_R$,
\begin{equation}
    \tilde{\phi}(\phi_R) = \phi_R + i \tilde{f}_\theta(\phi_R),
\end{equation}
where the function $\tilde{f}_\theta(\cdot)$ is represented by a neural network. Compared with the standard generalized thimble method, the \textit{learnifold} has inputs of the real part manifold and thus results in large Jacobian in practice, also the gaps between integral contributing regions (thimbles) are smaller which renders easy exploration of all relevant regions of integration. It can practically help to solve the problem of the time-consuming flow evolution and multi-modal search in Monte Carlo sampling. Translational symmetries can also be inserted for the \textit{learnifold} network. The authors validated the method in a 1+1 dimensional Thirring model with Wilson fermions on sizable lattices. 

This method has been further applied into the Hubbard model by Rodekamp et al.~\cite{2021PhRvB.103l5153W}. However, the standard(real-valued) neural networks still suffer from the computational effort, mainly because of severe volume scaling of the Jacobian determinant. Recently, the authors have developed complex-valued neural networks to instead learn the mapping from the integration manifold to the target manifold directly~\cite{2022PhRvB.106l5139R,Rodekamp:2022ylw}, $\tilde{\phi}(\phi) = \tilde{f}_\theta(\phi)$. Meanwhile, given the affine coupling layers, the Jacobian can be evaluated high efficiently in this novel architecture, i.e., reducing the scaling of the Jacobian determinant from a general cubic scaling down to a linear scaling in the volume. This method has been demonstrated in systems of different sizes.

\subsubsection{Normalizing Flow for Complex Actions}

Recall the flow-based model introduced in Sec.~\ref{sec:3:flow_based}, one will immediately find that it has similarities with the contour deformation. The main idea of a normalizing flow is to construct an isomorphic deformation on smooth manifolds. Although it cannot tackle the complex action naturally, it can be conceivably generalized, defining an integration contour along which the sign problem may be mitigated. In works of Lawrence and Yamauchi.~\cite{Lawrence:2021izu,Yamauchi:2021kpo}, they discussed the conditions for the existence of a manifold that can exactly solve the sign problem, which sets the requirement for building complex normalizing flows. The authors demonstrated the effectiveness numerically across a range of couplings for the Schwinger--Keldysh sign problem associated with a real scalar field in 1+1 dimensions. Manifolds that approximately solve the sign problem should be available in many physical systems, as the authors suggested in Ref.~\cite{Lawrence:2022afv}. 
In addition, in a recent work~\cite{Pawlowski:2022rdn}, Pawlowski and Urban have proposed to compute the density with the normalizing flow directly, which is the core of the \textit{density-of-states} approach for touching sign problems. They validated the method in a two-component scalar field theory, in which an imaginary external field breaks O(2) symmetry explicitly.\footnote{Note in condensed matter physics, there are earlier trials using automatic differentiation to optimize a sufficiently general Hubbard--Stranovichi transformation on fermionic system to mitigate the sign problem~\cite{Wan:2020lff}, which is in line with flow-based strategies in terms of field transformation optimization.}

\subsubsection{Path Optimization Method}

In Ref.~\cite{Mori:2017pne,Mori:2017nwj}, Mori et al. first proposed the path optimization method. This new approach addresses the sign problem as an optimization problem of the integration path. They utilized a cost function to specify the path in the complex plane and adjust it to minimize a cost function that represents the degree of weight cancellation,
\begin{equation}
    F[\phi(t)] = \frac{1}{2}\int dt |e^{i\theta(t)} - e^{i\theta_0}|^2 |J(\phi(t))e^{-S[\phi(t)]}|,
\end{equation}
where $\theta(t)$ is the complex phase of the parameterized integrand $J(\phi(t))e^{-S[\phi(t)]}$, and $\theta_0$ is the complex phase of the original integrand. The original partition function becomes $Z= \int_\mathcal{C}\mathcal{D}t J[\phi(t)]\text{exp}\{-S[\phi(t)]\}$. This method eliminates the need for solving the gradient flow found in the Lefschetz-thimble method. Instead, the construction of the integration-path contour becomes an optimization problem that can be solved using various efficient methods, e.g., gradient-based algorithms. This method has been successfully extended to e.g., 2D complex $\lambda\,\phi^4$ theory~\cite{Mori:2017nwj}, the Polyakov-loop extended Nambu--Jona-Lasinio model~\cite{Kashiwa:2019lkv,Kashiwa:2018vxr}, the 0 + 1 dimensional Bose gas~\cite{Bursa:2018ykf}, the 0 + 1 dimensional QCD~\cite{Mori:2019tux}, as well as 
SU(N) lattice gauge theory~\cite{Detmold:2021ulb}.

Simultaneously, in Ref.~\cite{Alexandru:2018fqp,Alexandru:2018ddf}, Alexandru et al. also investigated the path optimization method by parameterizing the manifold with neural networks $\mathcal{M}_\theta$. They showed the results first for the 1+1 dimensional Thirring model with Wilson fermions on lattice sizes up to $40\times10$. Then they demonstrated the performance in the 2+1 dimensional Thirring model~\cite{Alexandru:2018ddf}. The recent progress of the path optimization method can be found in the review~\cite{Alexandru:2020wrj}.

Namekawa et al. further investigated the efficiency of gauge-invariant inputs~\cite{Namekawa:2021nzu} and gauge-covariant neural networks approximating integral path~\cite{Namekawa:2022liz} for the path optimization method. The motivation is the path optimization method with a completely gauge-fixed link-variable input can tame the sign problem in a simple gauge theory, but does not work well when the gauge degrees of freedom remain. To overcome this problem, 
the authors proposed to employ a gauge-invariant input, such as a plaquette, or a gauge-covariant neural network, which is composed of the Stout-like smearing for representing the modified integral path. The efficiency is evaluated in the two-dimensional U(1) gauge theory with a complex coupling. The average phase factor is significantly enhanced by the path optimization with the plaquette or gauge-covariant neural network, indicating good control of the sign problem. Furthermore, another improvement of dropping the Jacobian during the learning process, which reduces the numerical cost of the Jacobian calculation from $O(N^3)$ to $O(1)$, where $N$ means the number of degrees of freedom of the theory.  Although a slight increase of the statistical error will emerge with the approximation, this practical strategy with invariant/covariant designs will push the path optimization towards solving complicated gauge theories.

\subsection{Summary}
This chapter introduces first some computational challenges in lattice field theory, the non-perturbative approach in tackling QCD calculation in understanding extreme nuclear matter properties. Then from three different sectors: field configuration generation, lattice data and physics analysis, and sign problem, we summarize some relevant explorations and present the status of using machine-/deep-learning techniques to facilitate the lattice field study. Many recent advanced developments are also discussed, including the symmetry embedding into learning algorithm, physics priors incorporation via automatic differentiation to solve inverse problems involved in real-time physics extraction, flow-based field configuration generation and series of novel trials in applying deep neural networks for mitigating the sign problems in lattice simulation.