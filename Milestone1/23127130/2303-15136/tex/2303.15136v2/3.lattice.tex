\section{Lattice QCD}\label{sec:lat}
As the fundamental theory for describing the strong interaction, QCD is challenging to be solved due to its characteristic asymptotic freedom, which embarrasses perturbative treatment at low energy scales~\cite{Gross:2022hyw}. Therefore, non-perturbative methods are needed to study nuclear matter in the context of strong interactions. Lattice field theory plays an important role in ab-initio computation\footnote{Functional methods e.g., functional renormalization group (fRG) and Dyson-Schwinger (D--S) equations form the other first-principles approaches. See Ref.~\cite{Fischer:2018sdj} for a review.} of general many-body systems~\cite{Wilson:1974sk}. In this approach, the system's partition function or path integrals are discretized on a lattice of Euclidean spacetime, and field configurations or paths can be generated using importance sampling for further evaluation of physical observables.

Generally, algorithms related to Markov Chain Monte Carlo (MCMC), such as the Hybrid Monte Carlo (HMC) algorithm~\cite{Duane:1987de}, are employed to generate ensembles of configurations in lattice calculations~\cite{Knechtli:2017sna}, and have succeeded in providing important results, e.g., for the QCD phase structure~\cite{Fukushima:2010bq,Philipsen:2012nu,Ding:2015ona,Guenther:2020jwe, Karsch:2022opd}. See also Ref.~\cite{Ratti:2018ksb} for a recent review. However, the inherent sequential nature and diffusive update involved in these Monte Carlo algorithms seriously hamper the sampling efficiency for larger and finer lattices due to the phenomenon of critical slowing down(CSD)~\cite{Wolff:1989wq}. The computation becomes even more challenging for evaluating real-time dynamical properties. Recent strides in machine learning techniques may provide a promising way to either circumvent or alleviate these computational obstacles involved in lattice QFT and subsequent QCD studies~\cite{Boyda:2022nmh}. From a methodological point of view, many of the algorithmic advancements were initially explored on simple many-body physics or QFT systems. Yet, they hold the potential for broader application to QCD studies. Therefore, this chapter discusses the topic with a generalized perspective on lattice field theory.%the discussion in this chapter is with generality into lattice field theory.

\subsection{Overview and Challenges in Lattice Field Theory}
\label{sec:lattice_cha}
In general, Lattice QFT offers a non-perturbative approach to solving the path integral for the field system on a discretized Euclidean spacetime lattice. This allows the expectation value of an observable, $\mathcal{O}(\phi)$, to be represented by the discretized action,
\begin{equation}
    \langle\mathcal{O}\rangle=
    \frac{1}{Z}\int\mathcal{D}\phi\,\mathcal{O}(\phi)e^{-S(\phi)}.
    \label{eq:obs}
\end{equation}
Here $Z=\int\mathcal{D}\phi e^{-S(\phi)}$ is the partition function, while $\int\mathcal{D}\phi$ signifies the integration over all possible configurations of the discretized quantum field $\phi$. All the dynamical and interaction information of the fields are encoded in the action $S(\phi)$. The specific numerical evaluation of Eq.~\ref{eq:obs} hinges on sampling configurations following the action-dictated probability distribution 
\begin{equation}
    p(\phi)=\frac{1}{Z}e^{-S(\phi)}.
    \label{eq:target}
\end{equation}

We briefly introduce several challenges and computational tasks associated with lattice field theory calculations in the following. Subsequently, we will review the current advancements in leveraging machine learning techniques to address these challenges.

\begin{itemize}
\item{\textbf{Critical slowing down}}

Monte Carlo (MC) simulation can provide unbiased sampling for lattice field theory, which has been proven to be ergodic and asymptotically exact to approach the target distribution $p(\phi)$ under detailed balance condition for the involved proposal probability~\cite{krauth2006statistical}.
Within a classical MCMC framework, the process of configuration sampling typically involves proposing a new configuration based on the preceding one. This proposal then undergoes a subsequent judgment for acceptance or rejection, with the decision often made by evaluating a specific probability using both the proposed and the previous field configurations --- as exemplified by the Metropolis-Hastings algorithm~\cite{Metropolis:1953am,Hastings:1970mcs}. In these diffusive updates based sampling approaches, the proposal for new field configuration is usually made by local perturbation of the previous one or heat bath updates, which thus is inefficient in drawing independent configurations. Improvements can be realized by HMC which relies on evolving the configuration $\phi(x)$ jointly with a “conjugate momentum” $\pi(x)$ under classical Hamiltonian dynamics. Practically the computational costs involved are due to the strong autocorrelation of local updates, or the single local update itself is computationally expensive. The autocorrelation time is expected to scale as $\tau\sim\xi^z$, where $\xi$ is the correlation length that diverges around the critical point or approaching the continuum limit, and $z$ is the dynamical critical exponents which are $\approx2$ for standard local update algorithms~\cite{Wolff:1989wq}. Correspondingly, the induced severe inefficiency of sampling is called \textit{critical slowing down} (CSD). Being related, in the context of lattice field theories with well-defined topological behavior like QCD, \textit{topological freezing} can happen where topological observables usually have exponential scaling $\tau\sim e^{z\xi}$~\cite{DelDebbio:2004xh,Schaefer:2010hu}. CSD and topological freezing form the main barrier of lattice QFT computations.

\item{\textbf{Observables measurements and physics analysis}}

With ensembles of field configurations sampled from the desired distribution $p(\phi)$, one can evaluate correlation functions and further various physical observables. This process involves intensive operations on field configurations which are high-dimensional tensors. Furthermore, the physics underlying the studied system must be analyzed, including thermodynamics, phase transitions (characterized by the order parameter), % Physics is thus also to be analyzed such as thermodynamics or phases transition (order parameter) of the system, 
and also different real-time dynamical properties, e.g., the reconstruction of spectral function or parton distribution functions (PDF) represent a particularly challenging, ill-conditioned inverse problem. 

\item{\textbf{Sign problem}}

The sign problem in lattice QCD calculations refers to the fact that the functional integral defined in the partition function is not positive definite in many cases (e.g., at finite density or for Minkowski-time dynamical physics), which complicates the application of standard Monte Carlo methods~\cite{Troyer:2004ge}. In lattice QCD, the partition function is expressed as a functional integral over all possible configurations of quarks and gluons on a discrete four-dimensional lattice in the form~\cite{Aarts:2015tyj,Nagata:2021ugx},
\begin{equation}
    Z = \int D[A] D[\psi] D[\bar{\psi}] e^{-S[A,\psi,\bar{\psi}])},
\end{equation}
where $S$ is the QCD action, $A$ is the gauge field, and $\psi$ and $\bar{\psi}$ are the quark and antiquark fields, respectively. 
In finite-density QCD, the nonzero chemical potential($\mu$) renders the fermion determinant complex within the partition function, $Z = \int D[A] \text{exp}(-S_\text{YM})\text{det}M(\mu)$, where $S_\text{YM}$ is the Yang-Mills action. A primary concern arises since the exponent in the integrand, $-S$, is not always positive, which leads to the non-positive definite integral. It induces the ill-defined probability when sampling configurations in standard Monte Carlo techniques. Though certain methods might technically navigate around the predicament, they will inevitably encounter the highly fluctuating phase factors which result in an exponential growth of computational cost as the system's volume expands~\cite{Berger:2019odf,Alexandru:2020wrj}.


\end{itemize}

\subsection{Field Configuration Generation}

In lattice QFT research, generating field configurations is often the most computationally demanding task, primarily because of the typically required Monte Carlo simulation on Markov chains. These simulations tackle the sampling of high dimensional distribution distributions, especially as they strive to approach the continuum limit.
In fact, for general many-body system studies that use Monte Carlo simulation, considerable efforts have been invested in crafting intelligent proposals or global updates during the MCMC procedures. The goal here is to reduce the autocorrelation time, which, when increased, tends to impede the efficiency of MCMC sampling. Besides the critical slowing down as introduced in Sec.~\ref{sec:lattice_cha}, which is a crucial barrier when pushing the lattice calculation to the continuum limit, the other challenge faced by conventional MCMC simulations is sampling from multimodal distributions. Navigating between widely separated modes of the target distribution using update-based samplers is far from straightforward~\cite{DelDebbio:2004xh,Hasenbusch:2017fsd}. This challenge of high-dimensional sampling and the quest for efficient proposal design is also a prominent theme in the machine learning community. It has spurred the rapid advancement of generative models~\cite{Wang2018GenerativeMF}. Research in both general classical/quantum many-body statistical physics studies and lattice QFT has demonstrated that modern generative algorithmic development stemming from AI/ML can offer improved efficiency, and serve as a valuable complement to traditional Monte Carlo methods, especially in the context of generating configurations for physical systems.

\subsubsection{GAN-based Algorithms}
As introduced in Sec.~\ref{subsubsec:gm}, the Generative Adversarial Network(GAN), is a deep generative model to realize implicit MLE for distribution learning through adversarial training. This generative model has recently been explored for many-body statistical systems (e.g., for Ising model in~\cite{2017arXiv171004987L}) and also for the configuration generation of quantum field systems.
In Refs.~\cite{Zhou:2018ill,Zhou:2021vza,Zhou:2020yna}, the GAN is used for the configuration generation of complex scalar $\phi^4$ field at non-zero chemical potential in the context of lattice QFT study. In Sec.~\ref{sec_phases_obs} about \textit{\textbf{Regression in QFT}} sector, this QFT system under worldline formalism was also introduced for regression exploration via deep learning. New configuration generation via Wasserstein-GAN scheme is also explored~\cite{Zhou:2018ill} in the task of generating uncorrelated configurations satisfying the physical distribution, with training ensembles of configurations prepared by the worm algorithm. Since the considered field system under the dualization approach should satisfy an important local divergence-type constraint reflecting flux conservation, the effectiveness of the generative algorithms, in terms of whether the generated configurations are physical or not, is verified by checking this divergence-type constraint condition for training. Surprisingly, as shown in the left part of Fig.~\ref{fig:scalar_gan}, it is found that this highly implicit physical constraint condition is realized in a converging manner (more and more together with training epochs) for configurations generated by the trained generator within GAN, without explicit guidance of this constraint. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp!]
  \centering
  \includegraphics[width = 0.41\textwidth]{figures/div_per_site.png}
  \includegraphics[width = 0.45\textwidth]{figures/prob_den_gan_phi2.png}
  \caption{(left) The degree of divergence satisfaction for configurations from trained GAN generator; (right) the probability density distribution for the squared field $\phi^2$ from GAN and from Monte-Carlo simulations at a fixed chemical potential. Taken from Ref.~\cite{Zhou:2018ill}.}
  \label{fig:scalar_gan}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Also, the distribution with respect to normal physical observables from the generated samples was found to agree well with the MCMC evaluation. And here, for the considered complex scalar field system, it even agrees reasonably well over the multimodally distributed number density and field square (see the right side of Fig.~\ref{fig:scalar_gan}), which is not trivial for conventional sampling approaches without special treatment. Furthermore, a conditional generative network, cGAN, is proposed in~\cite{Zhou:2018ill} to generalize the configuration generation ability of the generator to be dependent/conditional on physical observables (specifically the number density $n$ was used for demonstration) and go beyond the distribution on which it was trained. Ref.~\cite{Singha:2021nht} later applied the conditional GAN in the lattice Gross Neveu model to mitigate the critical slowing down when approaching the critical region.

This strategy with the conditional GAN has also been applied to the spin configuration generation of the two-dimensional XY model~\cite{2021ScPP...11...43S}, with several different architectures proposed including one specific output distribution entropy maximization regulator to mitigate the mode-collapse problem. As the non-local defining feature for the involved topological phase transition, the vortices distribution is also especially scrutinized besides other relevant observables (e.g., magnetization and energy). With the trained GAN, Ref.~\cite{2021ScPP...11...43S} further proposed the GAN fidelity in terms of the discriminator network output which is shown to be able to detect the phase transition unsupervisedly.

The above demonstration, especially the consistency between the distribution from the GAN and the desired one, actually indicates that the trained GAN here can be taken as a good enough proposal on a Markov Chain when one wants to guarantee the ergodicity and detailed balance properties for the sampling process, thus make sure the correct physics can be estimated in converging manner. This indeed is further demonstrated in Ref.~\cite{Pawlowski:2018qxs} on scalar field theory together with the Hamiltonian Monte Carlo (HMC) approach. 
Specifically, the trained GAN is proposed to serve as overrelaxation (i.e., configuration space exploration that keeps the action unchanged, $\delta S=0$) procedure within the action-based MCMC sampling algorithm. Starting from some initial configuration $\phi$ after a number of HMC steps, the gradient flow is performed on the latent variable $z$ to achieve the GAN overrelaxation proposal $G(z')$, 
\begin{equation}
    z'(\tau+\epsilon)=z'(\tau)-\epsilon\frac{\partial (S[G(z)]-S[\phi])^2}{\partial z}, 
    \label{eq:gflow_gan}
\end{equation}
where $\epsilon$ is the learning rate and $\tau$ the training epochs. In Metropolis steps, proposal configuration with $\delta S=0$ (w.r.t. the last configuration) is accepted automatically given the transition probability for the proposal is symmetric. Heuristic arguments are provided in~\cite{Pawlowski:2018qxs} on this issue for GAN's selection probability $P(G(z')|\phi)$. With demonstration on 2d scalar field theory, under such GAN overrelaxation method, the autocorrelation time has been shown to be largely reduced. 

Note that the GAN-based approach in general requires a training data set to be prepared from conventional sampling means, which hampers its ability to assist the field configuration generation purely from the physically known action or Hamiltonian. Also, though it can be taken as a proposal in generating independent and physically promising configurations, the vanilla GAN has no evaluation of the likelihood and thus the sample probability. More potential can be unlocked for the GAN-based approach if the explicit likelihood estimation can be added inside the adversarial learning in the future. In contrast, the methods introduced in the following can render the self-training just from physical action or Hamiltonian, with also the sample probability to be accessible.

\subsubsection{Self-Learning through Effective Action}

\emph{\textbf{Self-Learning Monte Carlo}} --- 
The self-learning Monte Carlo (SLMC) method is a general-purpose numerical algorithm for configuration generation of many-body systems based on machine learning. The development of SLMC originally is within classical spin systems\cite{2017PhRvB..95d1101L} and shows clear improvement in curing the critical slowing down problem. It works also with other general quantum models that are concerned with condensed matter physics and quantum chemistry.~\cite{PhysRevB.102.041124}. The basic idea is to learn a tunable and effective Hamiltonian or Action that can be associated with a more efficient update algorithm (such as cluster-update or other global moving ways to propose uncorrelated configurations), with which, the Metropolis-Hasting test using the real action can turn it to be an exact sampler for the system.

As detailed in Ref.~\cite{2017PhRvB..95d1101L} on the example of a statistical spin model, the SLMC consists of a learning phase and exploration phase within actual MC simulation in general: (1) a conventional MC simulation under local update can be performed to produce a series of configurations with its weight (energy or action evaluation) also being known; (2) based on these training data an effective Hamiltonian $H_{eff}$ can be learned, to which one can use a global update for a faster simulation (e.g. when only two-particle interactions are contained in $H_{eff}$); (3) the learned $H_{eff}$ can be used to make proposal in actual MC simulation and (4) perform Metropolis Hasting test with the original Hamiltonian to correct (reject or accept). See Fig.~\ref{fig:SLMC} for a schematic illustration of SLMC in a simple spin model.
\begin{figure}[!hbtp]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/SLMC.pdf}
    \caption{(From Ref.~\cite{2017PhRvB..95d1101L})Schematic illustration of learning process (top panel) and simulating process (bottom panel) in SLMC. \label{fig:SLMC}}
\end{figure}
The training over $H_{eff}$ can also be self-improved by iteratively reinforcing training $H_{eff}$ with generated configuration from the last step self-learning update with $H_{eff}$. Basically, a more efficient MC update model can be obtained from the SLMC. It's demonstrated that the SLMC indeed largely reduced the autocorrelation time $\tau$ especially near the phase transition, and gives around 10 to 20-fold speedup on the considered 2D generalized Ising model.
It's worth noting that similar ideas by training and simulating Restricted Boltzmann Machines (RBM)~\cite{10.1162/089976602760128018} were adopted in Refs.~\cite{PhysRevB.95.035105,2016PhRvB..94p5134T,Wang:2017mzw,2019PhRvE.100d3301P} for proposing efficient Monte Carlo updates, which is not limited to Trotter decomposition for fermionic system and renders faster proposal since the used Gibbs sampling in RBM.


This self-learning MC strategy was further pushed forward into more realistic fermionic systems where Trotter decomposition is employed~\cite{Liu:2016zmg, 2017PhRvB..96d1119X} and for continuous time Monte Carlo simulation~\cite{2017PhRvB..96p1102N}.  Deep neural network techniques are also additionally introduced into the effective action or Hamiltonian's construction in Ref.~\cite{2018PhRvB..97t5140S, Nagai:2018sav,2019PhRvB.100d5153S}, which can further increase the flexibility of the effective action and improve the following acceptance rate in the Monte Carlo sampling stage. 

The SLMC for non-abelian SU(2) gauge theory with dynamical staggered fermions at zero and finite temperature was developed in Ref.~\cite{Nagai:2020jar}.
Ref.~\cite{Tomiya:2021ywc} devised a gauge covariant neural network for 4-dimensional non-abelian gauge field theory, and adopted such network to construct effective action within HMC to achieve self-learning HMC scheme, with demonstration on the case of two color QCD including un-rooted staggered fermion.

\emph{\textbf{Action parameter regression}} ---
In tackling the issue of CSD for lattice field theory study, with promise, multiscale methods are proposed to overcome the CSD by refining ensemble of configurations at a coarse scale. Such methods require \textbf{action matching} across different scales via renormalization group (RG), then the sampling at coarse-scale level can already render the approach to continuum limit physics evaluation together with a cheap re-thermalization with the original fine action. The key challenge involved is parametric regression for identifying the proper action parameters that best describe physics at coarse scale from an ensemble of configurations generated at a finer scale. It's thus proposed to use deep neural networks to tackle this action-matching regression task\cite{Shanahan:2018vcv}, where the coarsened ensemble of SU(2) gauge field configurations are taken as input and the required action parameters are the output. It's worth noting that the mismatch from the regression can be corrected by re-thermalization steps on the finer scale with the corresponding action.

The simple fully-connected neural network was tried first and found to appear successful in validation, which however fails to generalize to different parameter cases or even new Hybrid Monte Carlo (HMC) simulation streams of the same parameters as of the training set. The failure of such naive neural network is argued in Ref.\cite{Shanahan:2018vcv} to be induced by the lack of symmetries of the gauge field configurations, and further proposed a customized symmetry-preserving network to reduce effective degrees of freedom for the task. The embedding of the symmetries is designed by featuring an initial preprocessing layer to yield possible symmetry-invariant quantities as input to the following fully-connected layers.  This gives accurate parameter regression and successful generalization for even ensembles not distinguishable from principal component analysis (PCA). It thus provides a solution to the action matching.

\subsubsection{Variational Autoregressive Network}
\label{van}

The key object for a general many-body system is the free energy, which contains all the information about the system in principle. From a probabilistic point of view, the usual MCMC approach basically uses importance sampling to implicitly approach the free energy, which is an intractable high dimensional integration. However, the direct evaluation of the free energy is not possible for the naive classical MCMC approach (note that there are variants of MCMC algorithms developed to be able to approximately assess the free energy, which is computationally expensive). As another alternative strategy based on variational point of view, the \textit{mean field approach} or related information transfer algorithms can be adopted for the free energy estimation, which then basically performs the minimization over the variational free energy. In probabilistic language this is equivalent to the minimization over the reverse (or backward) KL divergence between the variational distribution $q_{\theta}(\phi)$ and the target distribution in Eq.~\ref{eq:target} or in statistical physics $p(\phi)=e^{-\beta H(\phi)}/Z$ (where $\beta$ is the inverse temperature, $H(\phi)$ the Hamiltonian of the system, and $Z$ is the partition function),
\begin{equation}
    \mathcal{D}_{KL}(q_{\theta}(\phi)||p(\phi))=\int\mathcal{D}[\phi] q_{\theta}(\phi)\log\frac{q_{\theta}(\phi)}{p(\phi)}=\beta(F_{q}-F), 
    \label{eq:KL_van}
\end{equation}
with the variational free energy defined as
\begin{equation}
    \beta F_q=\mathbb{E}_{\phi\sim q_{\theta}}[\beta H(\phi)+\log q_{\theta}(\phi)], 
    \label{eq:var_free}
\end{equation}
Because of the non-negativity for KL divergence ($\mathcal{D}_{KL}\ge0$, also known as \textit{Gibbs-Bogoliubov-Feynman inequality}~\cite{Peierls:1938zz, 10.1063/1.1704383, PhysRevLett.22.631}), the true free energy $F$ is upper bounded by the variational free energy $F_q$, and the equality happens when the variational distribution $q_{\theta}$ really reaches the target distribution $p$ exactly.  Note that in machine learning community as proposed in the beginning as general density estimation method, the autoregressive network usually uses the (forward) KL divergence between data empirical distribution (e.g., constructed with training data) and the variational distribution, $\mathcal{D}_{KL}(p_{data}||q_{\theta})$. For many-body physics or QFT study, the proposal to use the reverse KL divergence together with the easily sampleable variational ansatz makes it possible for \textbf{self-training} starting solely from the unnormalized target distribution\footnote{i.e., the training is performed with sampled generated from the variational distribution, instead of collected samples from the true distribution in advance}, e.g., knowing the action or Hamiltonian for the physical system in equilibrium.

Despite its popularity and success, the \textit{mean field calculation} is quite often limited by the assumed \textit{variational ansatz} (e.g., factorized), especially when the system is with strong correlations between their degrees of freedom, thus most of the time it is valid only in high temperature cases or when the system's topological structure fulfills the requirement from mean field approximation. Here the autoregressive model naturally stands out, because both the direct sampling and tractable likelihood evaluation (which is desired in evaluating the variational free energy) can be simultaneously realized. Meanwhile, when the autoregressive model is constructed as variational ansatz, the variational free energy (Eq.~\ref{eq:var_free}), which serves as the loss function, can be estimated unbiasedly and stochastically by drawing ensembles of samples from the sequential stochastic process specified by the autoregressive model (see Eq.~\ref{auto_prob} as introduced in Sec.~\ref{subsubsec:gm}). Thus, in every optimization iteration, using data sampled from the autoregressive model it suffices to perform \textbf{self-training} on the model.

Ref.~\cite{2019PhRvL.122h0602W} for the first time proposed to introduce the neural network to give a more powerful yet tractable variational ansatz, taking advantage of the strong representational ability of neural networks due to the universal approximation theorem. To keep the evaluation of the variational free energy tractable, one needs to design the network such that the variational distribution represented by the network is accessible and efficiently computable. Accordingly, the autoregressive networks were adopted by the authors to decompose the joint probability over all lattices to a product of conditional probabilities, and parametrized each conditional with neural networks\footnote{Note that to represent the conditional probability, the output of neural networks are interpreted as essential parameters of the distribution. More explanation about autoregressive networks can refer to Sec.~\ref{subsubsec:gm}}. The idea was directly demonstrated on a simple 2D ferromagnetic Ising system, also generalized to the PixelCNN structure with convolutional layers included to respect the locality and the translational symmetry of the system. Compared to conventional MCMC evaluation, this variational autoregressive network can also give evaluation of the free energy by giving its well-minimized upper bound. This strategy of using autoregressive probabilistic models was later combined with quantum circuits, and extended the variational quantum eigensolver (VQE) to investigate thermal properties and excitations of quantum lattice model, termed as $\beta$-VQE~\cite{Liu:2021bst}. This quantum-classical hybrid algorithm was also applied to the Schwinger model at finite temperature and density~\cite{Tomiya:2022chr}, with a large volume limit evaluated and a continuum limit took in obtaining the phase diagram.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp!]
    \centering
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=7.cm]{figures/XY_can.pdf}
        \caption{The energy per site of 2D XY model ($L=16$) from CANs, CANs+IS and MCMC. The upper inset shows the number of vortex pairs density with inverse temperature. Taken from ~\cite{Wang:2020hji}.}
        \label{fig:xy_en_can}
    \end{minipage}
    \hspace{0.5cm}
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=9cm]{figures/vortex_prob.pdf}
        \caption{Probabilities analysis with CANs and corresponding vortices for a random 2D XY model configuration sampled from well-trained CANs at $\beta = 1.0$. Taken from ~\cite{Wang:2020hji}.}
        \label{fig:xy_vor_can}
    \end{minipage}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Authors in Ref.~\cite{Wang:2020hji} further generalized the above method to a general many-body system with continuous variables, where the probability interpretation of the introduced autoregressive is devised to be a mixture beta distribution, instead of a Bernoulli distribution for Ising spins. This newly extended continuous-mixture autoregressive network (denoted as CAN in the paper) is well demonstrated on the 2D XY model which exhibits non-trivial topological KT phase transition. The thermodynamics of the systems were shown to be captured successfully (see Fig.~\ref{fig:xy_en_can}), and the underlying emergent degree of freedom--vertex--is also found to be rediscovered by this CAN method. Furthermore, it is found that the trained CAN network can automatically give rise to the vortices' distribution for any random given XY spin configurations with its conditional probability components output from plaquette (see Fig.~\ref{fig:xy_vor_can}), directly indicating that the network captured the underlying emergent physics about this many-body system. Note that this CAN method can capture the O(2) symmetry for the 2D XY model, which is equivalent to global U(1).

Ref.~\cite{Wu:2021tfb} proposed symmetry-enforcing updates within MCMC with autoregressive neural network as global update proposer, since the system action or Hamiltonian remain invariant under specific symmetry operations (e.g., translation and reflections as considered in Ref.~\cite{Wu:2021tfb}). This introduced symmetry operation largely reduced the ergodicity problem from those exponentially suppressed configurations (i.e., those with exponentially smaller $q_{\theta}(\phi)$ compared to $p(\phi)$, however, will hardly influence the variational free energy evaluation). Additionally, a neural cluster update scheme is devised in Ref.~\cite{Wu:2021tfb} utilizing the decomposition structure of autoregressive model, which can lower the autocorrelation time by setting only a subset of the lattice to be changed instead of the whole in each Monte Carlo step.


\subsubsection{Flow-based Variational Learning}
\label{sec:3:flow_based}
Analogous to the above-mentioned variational autoregressive network models (refer to Sec.~\ref{van}), flow-based models employ normalizing flows (NF) to construct the variational ansatz for the target distribution, with the primary objective of approximating and learning the desired distribution by minimizing the variational free energy. Again, technically, this process involves minimizing the reverse mode of Kullback-Leibler (KL) divergence between the variational distribution constructed by the flow-based model and the target distribution.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp!]
  \centering
  \includegraphics[width = 0.6\textwidth]{figures/fig_3-normalizing_flow.pdf}
  \caption{A schematic demonstration for the normalizing flow of the Real NVP, in which the yellow circles represent affine transformations represented by the neural networks.}
  \label{fig:3:normflow}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Though previously introduced in Sec.~\ref{subsubsec:gm}, we provide a brief explanation of the normalizing flow (NF) applications in the context of field configuration generation for the sake of terminology consistency in the following discussion. Generally, NF establishes a flexible and tractable probability distribution by facilitating a generative bijective transformation between a naive prior distribution $p_u(u)$ (e.g., multivariate Gaussians) and the variational distribution $q_{\theta}(\phi)$ (aimed at approaching the target distribution), $\phi=f_{\theta}(u)\sim q_{\theta}(\phi)$. This transformation is parametrized by neural networks with trainable parameters denoted as $\{\theta\}$, and is specifically designed to possess invertibility. Through the principle of probability conservation (i.e., the change of variable theorem for the probability distribution), one can easily derive the connection between the variational distribution and prior as $q_{\theta}(\phi)=p_u(u)|\det\frac{\partial f_{\theta}(u)}{\partial u}|^{-1}$. Given that the prior probability is easily evaluable and the Jacobian determinant $J_f$ is computable, one can calculate the variational probability $q_{\theta}(\phi)$ on any samples, allowing for further minimization of the variational free energy. Therefore, the key recipe in constructing a viable flow model is ensuring that the network represented transformation $f_{\theta}$ is invertible, differentiable, and composable. As the most common flow-based model, Real NVP structure (see Sec.~\ref{subsubsec:gm} for details) is frequently employed, which constructs the transformation $f_{\theta}$ by composing a series of affine coupling layers (see its schematic diagram in Fig.~\ref{fig:3:normflow}) each structured as follows:
\begin{equation}
\left\{
\begin{aligned}
    \phi^{i}_{1:k} &= \phi^{i-1}_{1:k} \\
    \phi^{i}_{k+1:N} &= \phi^{i-1}_{k+1:N} \odot e^{s_{\theta}^i(\phi^{i-1}_{1:k})} + t_{\theta}^i(\phi^{i-1}_{1:k}),
\end{aligned}
\right.
\end{equation}
with $\phi^i$ the output of the $i^{\text{th}}$ affine coupling layer (thus $\phi^0 = u\sim p_u(u)$ and final output $\phi^{L}=\phi\sim p(\phi)$), $k$ the separation point of the configuration variables into two subsets, and $N$ the number of variables in one configuration. The scaling and translation functions can be parameterized by DNNs as $s_{\theta}: \mathbb{R}^{k}\rightarrow \mathbb{R}^{N-k}$ and $t_{\theta}: \mathbb{R}^{k}\rightarrow \mathbb{R}^{N-k}$. The Jacobian determinant can also be directly evaluated to be $(\det J_{T}^i) = \Pi_j^{N-k}e^{s_{\theta}^i(X_{1:k})_j}$. Additionally, due to the splitting, the above coupling layer can be easily inverted,
\begin{equation}
\left\{
\begin{aligned}
    \phi^{i-1}_{1:k} &= \phi^{i}_{1:k} \\
    \phi^{i-1}_{k+1:N} &= (\phi^{i}_{k+1:N} - t_{\theta}^i(\phi^{i}_{1:k}))\odot e^{-s_{\theta}^i(\phi^{i}_{1:k})}.
\end{aligned}
\right.
\end{equation}

With the above-mentioned flows, i.e., a series of bijective transformation layers, one effectively attains a parametric model with an explicit probability for each sample, $q_{\theta}(\phi)$, and with tunable parameters denoted as $\theta$. Analogous to autoregressive models, the reverse KL divergence expressed in Eq.~\ref{eq:KL_van} can be employed to steer the optimization of $q_{\theta}(\phi)$ towards approaching the target distribution $p(\phi)$ in Eq.~\ref{eq:target},
\begin{equation}   \theta^{*}=\arg\min_{\theta}\mathcal{D}_{KL}(q_{\theta}(\phi)||p(\phi))=\arg\min_{\theta}[\int\mathcal{D}\phi q_{\theta}(\phi)(S(\phi)+\log q_{\theta}(\phi)) + \log Z ], 
    \label{eq:trainingKL}
\end{equation}

Ref.~\cite{Albergo:2019eim} presented the first application of the flow-based model to lattice quantum field theory simulations\footnote{An earlier similar flow-based model was developed and demonstrated on the two-dimensional Ising model in its continuous dual version, with the novel concept of neural network based variational Renormalization Group approach proposed\cite{2018PhRvL.121z0601L} proposed.}. Taking the two-dimensional $\phi^4$ field theory as an example, this study showed that the flow-based sampler confers substantial advantages over conventional sampling algorithms (with local Metropolis sampling and HMC considered for benchmarking), as evidenced by the systematic reduction in autocorrelation times when deploying the flow sampler as a proposal within the Markov Chain. The incorporation of the trained flow sampler with MCMC (e.g., Metropolis-Hastings) guarantees the asymptotic exactness of the sampling, as the Metropolis-Hastings acceptance/rejection step serves as a corrector to ensure that the sampling distribution precisely converges to the target distribution.
This method is commonly referred to as \textit{flow-based MCMC} in the literature. One of its notable attributes is the alleviation of the critical slowing down issue associated with the Markov Chain sampling, as the samples generated from the flow model are uncorrelated. Consequently, the associated cost is transitioned to the up-front training expenditure for the flow generative model.
It should be noted that in calculating observables ($\langle\mathcal{O}(\phi) \rangle$), besides the stochastic MCMC correction method, reweighting (i.e., importance sampling) can also be employed, which assigns a weight $w(\phi)=p(\phi)/q_{\theta}(\phi)$ to each sample $\phi$ when computing the expectation of the observables. A detailed introduction to \textit{flow-based MCMC} for the scalar field with code written in \textit{PyTorch} can be found in Ref.~\cite{Albergo:2021vyo}. We also summarize in Tab.~\ref{tab:flow} some related applications of this strategy to different many-body physics systems.
In Ref.~\cite{Nicoli:2020njz}, a simpler normalizing flow construction with Non-linear Independent Component Estimation (NICE, see Eq.~\ref{flow_nice}) is adopted for lattice simulation of 2-D real scalar field theory. In this study, the $\mathbb{Z}_2$-invariance is explicitly introduced by constraining the network to be with \textit{tanh} non-linearity and vanishing biases, which ensures that the transformation derived from the flow is an odd function, $f_{\theta}(-u)=-f_{\theta}(u)$. Interestingly, this work also proposed a direct estimator for the \textit{free energy} ($\hat{F}=-T\ln\hat{Z}$)\footnote{The temperature $T=\frac{1}{a N_T}$ where $a$ is the lattice spacing and $N_T$ is the number of lattice points in the temporal direction.} from the trained flow model via a Monte-Carlo approximation on the partition function,
\begin{equation}
\hat{Z}=\frac{1}{N}\sum_{\phi_i\sim q_{\theta}}[e^{-S(\phi_i)}/q_{\theta}(\phi_i)] .
\label{eq:eff_free}
\end{equation}
This approach is conceptually appealing as it circumvents the cumbersome integration error accumulation inherent in conventional MCMC-based estimations for the partition function. Once the free energy is well estimated, other thermodynamic observables can be naturally obtained by taking the derivative of the free energy, and can also be further refined by the importance sampling~\cite{Muller:2019nis}. The capability exhibited by flow-based models primarily stems from their explicit likelihood estimation ability, a feature also shared by variational autoregressive models as introduced in Sec.~\ref{van}.

In Ref.~\cite{Singha:2023cql}, a conditional normalizing flow (c-NF) model was trained on samples pre-generated from HMC in the non-critical region of the theory\footnote{Note that being different from the variational strategy of flow-based MCMC method, this work trains the flow on existing samples, thus using forward KL divergence as the loss function.}. Such a trained c-NF model is able to interpolate or extrapolate the dependency of configuration generation on the parameter, thereby being applicable to the critical regions (i.e., phase transition situation or continuum limit seeking). Although the interpolation or extrapolation of the trained c-NF model in a critical region for configuration generation may exhibit bias, it can be efficiently corrected with a few e.g., Metroplolis-Hasting steps due to the tractable probability evaluation for each generated (or ``proposed'') configurations. Since the flow model is shared across the entire parameter space except for the conditioning on them, it presents a cost-effective means to reduce the expenditure involved in employing a flow-based model for the phase diagram exploration of the theory.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[htbp!]
\caption{Normalizing Flows applied to different physical systems in terms of lattice study}
\label{tab:flow}
\centering
\begin{tabular}{ccccccc}
\hline
systems\quad & Ising \quad & Scalar \quad & U(1) gauge \quad & SU(N) gauge \quad & Yukawa  \quad & SU(3) gauge \\ \hline
dimensionality\quad & 2D \quad & 2D \quad & 2D \quad & 2D \quad & 2D  \quad & 2D \\ \hline
fermions\quad & no \quad & no \quad & no \quad & no \quad & Staggered  \quad & 2 flavors \\ \hline
Ref.  \quad & \cite{2018PhRvL.121z0601L} \quad & \cite{Albergo:2019eim,DelDebbio:2021qwf,Hu:2019nea} \quad & \cite{Kanwar:2020xzo,Foreman:2021ixr} \quad & \cite{Boyda:2020hsi} \quad & \cite{Albergo:2021bna} \quad & \cite{Abbott:2022zhs} \\ \hline
%$\sigma_{R_{\rm iso}}$ \quad & 2.52\% \quad & 2.05\% \quad & 2.03\% \quad & 1.87\% \quad & 2.25\% \quad & 2.64\% \\ \hline
\end{tabular}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

It is worth noting that, such NF-build (with networks) diffeomorphic transformations $f_{\theta}$ between a naive simply distributed ``prior field'', $u$, and the physical field configurations, $\phi$, resembles the \textit{trivializing maps} approach suggested by L\"uscher in Ref.~\cite{Luscher:2009eq}, under which one actually is constructing for the system an effective action,
\begin{equation} 
S_{\rm{eff}}[u]=S[f_{\theta}(u)] - \log\det J_{f_{\theta}}(u). 
    \label{eq:flow_eff_action}
\end{equation}
that decouples the field variables by using the transformation whose Jacobian hopefully cancels out the interaction part in the original action, such that easier sampling can be realized for the (inversely) transformed new system $u=f_{\theta}^{-1}(\phi)\sim e^{-S_{\rm{eff}}(u)}$. Thus, one can in general view NF as neural network parametrizations for trivializing maps. By noting such perspective and taking the trained flow model inverse as an approximation of the trivializing map, Ref.~\cite{Foreman:2021ljl} and Ref.~\cite{Albandea:2023wgd} performed HMC on the flow transformed system $u$ for a 2-dimensional U(1) gauge theory and $\phi^4$ scalar field theory, respectively. Then to the HMC induced Markov chain $\{u_i\}^N_{i=1}\sim e^{-S_{\rm{eff}}(u)}$ which is with shorter autocorrelation times, the application of the inverse flow transformation just gives the recovered field configuration samples, $\{f_{\theta}(u_i)\}^N_{i=1}=\{\phi_i\}^N_{i=1}\sim e^{-S(\phi)}$. It was also demonstrated for the 2-dimensional $U(1)$ theory that such deep learning assisted HMC (DLHMC)~\cite{Foreman:2021ixr, Foreman:2021ljl} allows mix between modes of different topological charge sectors. Note that performing HMC on the inversely transformed system (can also be called the latent space for the original system) was proposed also by Ref.~\cite{2018PhRvL.121z0601L} on Ising system and showed enhanced efficiency, and later Ref.~\cite{Hu:2019nea} adopted similar approach of the neural RG to complex $\phi^4$ field theory with interpretation as automatic construction of exact holographic mapping of AdS-CFT.

The splitting operation in Real NVP (i.e., partitioning the field configuration into two parts before entering each flow coupling layer, as Fig.~\ref{fig:3:normflow} shown.) may constrain the expressibility of the flow-induced field transformation $f_{\theta}$, such as in terms of the symmetry embedding. Ref.~\cite{Gerdes:2022eve} introduced continuous normalizing flow to define this invertible map, $f_{\theta}: u\to\phi$, as the solution to neural ODE~\cite{2018arXiv180607366C} with a fixed time $\mathcal{T}$ ($x$ indicate the lattice cites):
\begin{equation}
\frac{d\phi(t)_x}{dt}=g_{\theta}(\phi(t),t)_x,\quad \text{with} \quad u\equiv\phi(0), \quad \phi\equiv\phi(\mathcal{T}),
\label{eq:flow_ode}
\end{equation}
where $g_{\theta}$ is a neural network represented vector field, to which symmetries can be built in more easily. The probability of the generated configuration $\phi$ follows the solution from a second ODE~\cite{2018arXiv180607366C}:
\begin{equation}
\frac{d\log p(\phi(t))}{dt}=-(\nabla_{\phi}\cdot g_{\theta})(\phi(t),t), \quad \text{with} \quad p(\phi(0))\equiv p_u(u), \quad p(\phi(\mathcal{T}))\equiv q_{\theta}(\phi),
\label{eq:flow_ode_prob}
\end{equation}
Again, taking the two-dimensional scalar field theory as the testing ground, authors in Ref.~\cite{Gerdes:2022eve} proposed a vector field for the neural ODE inspired by Fourier expansion, $g_{\theta}(\phi(t),t)_x=\sum_{y,a,f}W_{xyaf}K(t)_a\sin(\omega_f\phi(t)_y)$, with $\omega_f$ the learnable frequencies and $W$ the learnable weight tensor and $K(t)_a$ the first several terms of Fourier expansion on the interval $[0,\mathcal{T}]$. With such a vector field, the required symmetry and also the internal Z2 ($\phi\to -\phi$) symmetry of the scalar $\phi^4$ theory can be easily satisfied. Compared to Real NVP, such continuous flow method shows quite enhanced sampling efficiency: the effective sample size (ESS)\footnote{The effective sample size, ESS, can be computed as $\text{ESS}=\frac{[N^{-1}\sum^N_{i=1}p(\phi_i)/q_{\theta}(\phi_i)]^2}{N^{-1}\sum^N_{i=1}[p(\phi_i)/q_{\theta}(\phi_i)]^2}$ with $p(\phi)$ the target distribution and $q_{\theta}(\phi)$ the flow-sampler induced distribution} increases from $1\%$ to $91\%$ for $32\times 32$ lattice size.

For target distributions with multimodal structure, it is well known that the usual (local) update-based sampling methods, such as MCMC, face the challenge of traversing regions between different modes, e.g., Higgs modes/double-well potentials. ``Freezing'' may probably happen when modes are so widely separated that the sampler tends to collapse to only one or few modes~\cite{DelDebbio:2004xh,Hasenbusch:2017fsd}. It was pointed out~\cite{Hackett:2021idh} that the flow-based MCMC encounters such difficulties as well, due to the tendency of ``mode collapse'' from the original version of the flow model. Then several trials were performed in Ref.~\cite{Hackett:2021idh} to construct and train flow models in sampling multimodal distributional in lattice field theory with the example of the $\mathbb{Z}_2$-broken phase of real scalar field theory, including data preparation in mixture models and data-augmented forwards KL divergence training, or adiabatic retraining with flow-distance regularization. But these methods either need prior knowledge for the mode structure to be provided, or are difficult to really control the multimodal sampling with action parameters adjustments. Recently one interesting development~\cite{Chen:2022ytr} related is to introduce the Fourier transformation layer into the flow construction and showed promising in solving multimodal distribution sampling problem, details can refer to Sec.~\ref{sec_phy_manifest} (Note that later very recently, similar ideas named as \textit{power spectral density layer} for flow models is introduced in Ref.~\cite{Komijani:2023fzy} and applied to scalar $\phi^4$ field theory).

For lattice gauge fields, new architectures of flow-based models have recently been developed to preserve the relevant local symmetries. A gauge invariant flow model is designed for sampling configurations for a $\mathrm{U}(1)$ gauge theory in Ref.~\cite{Kanwar:2020xzo}. See Sec.~\ref{flow_symmetry} for more details. Soon this kind of flow-based scheme was developed further to system with $SU(N)$ links~\cite{Boyda:2020hsi} and also to fermionic system as shown in Ref.~\cite{Albergo:2021bna,Abbott:2022zhs}. In the lattice community, it is now under progress in translating these developments into real QCD simulations, as reported in the recent proceeding~\cite{Abbott:2022hkm}. Though promising, note however that, such flow-based methods currently are with training costs growing very fast as approaching the continuum limit, as pointed out recently~\cite{DelDebbio:2021qwf, Abbott:2022zsh, Komijani:2023fzy}. Thus, further efforts are needed to improve the approach, e.g., incorporating a transfer learning strategy.


\subsection{Observables Analysis for QFT}
In lattice QFT simulations, after the field configuration generation, usually the other big portion of the computations lie in the estimation of physical observables from the generated ensembles of field configurations or the accessible correlation functions, for example, the thermodynamics evaluation, and real-time physics extraction. Some problems especially for real-time physics reconstruction will often encounter ill-posedness from an inverse problem perspective, such as spectral functions extraction, the parton distribution functions computation, and other related transport coefficient analysis based upon Euclidean correlators from lattice calculation. ML and DL techniques have now been explored for tackling these problems in recent years.

\subsubsection{Thermodynamics and Phases}
\label{sec_phases_obs}

%Thermodynamic states of matter, especially the accompanying phase transition, is one very important and extensively observed phenomenon in various many-body physics systems. 
The study of thermodynamic states of matter, particularly the associated phase transitions, constitutes a critical and extensively researched phenomenon in various many-body physics systems.
Viewed from both theoretical (analytical or numerical) and experimental perspectives, the exploration of phase diagrams has long been a focal point in physics.
Typically, estimators for physical quantities calculated on numerically sampled configurations (e.g., from MCMC) are constructed with close relation to phase indicative parameters such as order parameters. However, identifying certain physically important states, like topological phases, using these estimators is not always straightforward.
%It is however not always easy to identify some physically important states with such estimators, like for the topological phase. 
In this context, data-driven machine learning is exceptionally well-suited for phase identification, which is largely due to its relevant strengths in discerning hidden patterns and correlations from complex datasets. Both supervised and unsupervised learning techniques have been explored for this specific task~\cite{Dawid:2022fga}.

\emph{\textbf{Supervised Phase Classification}} ---
The initial incorporation of deep learning into thermodynamics and phase identification within general many-body systems can be traced back to Ref~\cite{2017NatPh..13..431C}. In this foundational work, it was convincingly demonstrated, using simple Ising systems, 
%The very beginning of introducing deep learning methods into thermodynamics/phase identification in general many-body systems can be dated back to Ref~\cite{2017NatPh..13..431C}, where it was firmly demonstrated on simple Ising systems 
that deep neural networks could be trained to recognize phases and phase transitions solely based on raw configurations sampled from Monte Carlo simulations\footnote{This strategy is dubbed as ``supervised'' since it relies on labeled configurations from domain or prior knowledge}. This pivotal work attests to the potential for leveraging machine learning algorithms to identify order parameters of the many-body system through supervised training. Subsequent research has expanded upon this strategy, including applications in the identification of quantum phases in fermionic system~\cite{2017NatSR...7.8823B}. See Fig.~\ref{fig:cnn_phase} for a general schematic view of such a supervised phase classification network.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp!]
  \centering
  \includegraphics[width = 0.8\textwidth]{figures/cnn_phase.pdf}
  \caption{A typical convolutional neural network for phase binary classification. Taken from Ref.~\cite{2017NatSR...7.8823B} with permission.}
  \label{fig:cnn_phase}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

By training a convolutional neural network (CNN) to capture the correlation between the inverse temperature and the 2D Ising spin configurations, Ref.~\cite{Tanaka:2016rtu} found that the trained CNN can isolate characteristic features associated with the phase transition of the system with scrutinization of the filters (weight parameters $W$) of the network. Accordingly, a neural network assisted order parameter is defined to give rise to critical temperature estimation in this work. Ref.~\cite{Li:2021yet} generalized the phase identification with deep learning on the Ising system to 3-dimensional case (employed 3D CNN on 3D Ising model), also discussed the average magnetization and energy regression, and the second-order and first-order phase transition learning. Being of special interest, it is expected that the QCD critical region has similar critical behaviors as in the 3D Ising model because of their shared universality.

\emph{\textbf{Unsupervised Phase Clustering}} ---
Focusing on the task of phase classification, which holds significant importance in many-body physics studies, numerous studies grounded in machine learning methods hinge on prior knowledge of the system's order parameters, along with the preparation of ensembles of microscopic configurations of the physical system. In other words, a supervised training strategy necessitating a substantial dataset of correctly labeled configurations-phase pairs is often required, as previously mentioned.% i.e., supervised training with correctly labeled ``configuration-phase'' large data set is needed as mentioned in above.
However, labels of phase classes for ensembles of field configuration are routinely a hurdle, especially in the investigation of newly studied systems. On a promising note, the domain of machine learning offers also unsupervised learning strategies. These strategies are capable of autonomously discerning crucial patterns within the amassed data, which could potentially correlate with phase information in the context of phase identification. Such an unsupervised approach to phase identification is particularly advantageous, as it does not require prior knowledge regarding the phases under scrutiny.% without knowing prior knowledge about the phase's information under consideration. 

The pioneering application of unsupervised learning to phase transition recognition was introduced in Ref.~\cite{2016PhRvB..94s5105W}, where neither the knowledge of the order parameter, indicating the presence of a phase transition, nor the location of the critical point was necessitated. Specifically illustrated through the 2D classical Ising model, Principal Component Analysis (PCA) was employed to extract the most significant components for the collected configurations of the system, and then the projection of the spin configurations onto the first two principal components just automatically split into clusters matching well with the corresponding physical phases (see left of Fig.~\ref{fig:pca_ising}). The proposed simple PCA phase exploration approach also exhibited success in analyzing the Ising model with a conserved order parameter. Extending this rudimentary unsupervised learning phase identification approach, Ref.~\cite{2017PhRvE..95f2122H} applied it to various physical models, including the square and triangular-lattice Ising models, the Blume--Capel model, the biquadratic-exchange spin-1 Ising (BSI) model and the 2D XY model. The study affirmed that the extracted principal components could facilitate the exploration of symmetry-breaking induced phase structure, besides aiding in identifying the phase transition type and locating the transition points. 

However, the naive PCA analysis has inherent limitations due to the involved linear transformations, rendering it unsuitable for deciphering more intricate transitions characterized by non-linear patterns. As illustrated in Ref.~\cite{2017PhRvE..95f2122H}, the vorticity structure in the BSI model and XY model could not be captured from raw spin configurations by the PCA, thus motivating the exploration of nonlinear unsupervised machine learning algorithms.
Ref.~\cite{2017NatPh..13..435V} proposed a \textit{confusion scheme}, which does not depend on labeled data and therefore can be taken as a generic tool to investigate unexplored phases of matter. Basically, the neural network is trained on deliberately by hand -“labeled” data for confusion purposes, then the performance of the trained network was found to give a universal \textit{W-shape} as a function of the guessed critical point for the parameter (e.g., chemical potential or temperature). This interesting idea was successfully demonstrated on Kitaev chain for topological phase transition, the classical Ising system for thermal phase transition, and also the quantum many-body-localization transition in a disordered Random-field Heisenberg chain. However, this framework failed when applied to 2D XY model (with original unprocessed spin configurations as input) which shows unconventional topological phase transition, as demonstrated in Ref.~\cite{2018PhRvB..97d5207B}. It is found that significant feature engineering on the spin configurations is needed for the correct phase classification, which is closely related to the underlying vortex patterns of the KT transition.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp!]
  \centering
  \includegraphics[width = 0.42\textwidth]{figures/ising_pca.pdf}
  \hspace{1cm}
  \includegraphics[width = 0.3\textwidth]{figures/Ising_latentB.png}
  \caption{Taken from Ref.~\cite{2016PhRvB..94s5105W} (left) and ~\cite{2017PhRvE..96b2140W} (right).}
  \label{fig:pca_ising}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As another popular unsupervised learning method, deep generative models such as the variational autoencoder (VAE) and generative adversarial networks (GAN), were also applied to investigate phase transition detection of physical systems by learning latent parameters of the original configurations. In Ref.~\cite{2017PhRvE..96b2140W} the authors studied the phase transition with PCA and VAE on states of the 2D Ising model (see right of Fig.~\ref{fig:pca_ising}) and 3D XY model. It was found that the unsupervisedly learned latent representations of the spin configurations are clustered automatically and correspond to the known order parameters of the system under investigation. Also, the reconstruction loss from the training can serve as an universal identifier for phase transition. In Ref~\cite{2020NatSR..1013047W} this strategy was further extended to explore crossover region identification to reveal a deeper understanding of the latent space, which is achieved by studying the response of the learned latent variable mappings of the Ising configurations along with external non-vanishing magnetic fields and temperatures. 

\emph{\textbf{Unsupervised Anomaly Detection}} ---
With a different strategy, DNN autoencoder (AE) is also deployed by performing anomaly detection to explore phase diagrams of quantum many-body systems~\cite{2020PhRvL.125q0603K} in an automated and completely unsupervised manner. Intuitively, the anomaly detection can be realized by utilizing the reconstruction loss from the well-trained AE, as introduced in Sec.~\ref{subsubsec:dl} and also applied for outlier detection in HICs in Sec.~\ref{outlier}, to single out newly confronted data those are showing larger reconstruction error compared to training classes. Thus, by investigating the loss map of the trained autoencoder on different points in the parameters space of a physical system (with input could be full state vector, entanglement spectrum, or correlations for the system), one can possibly map out the phase diagram without physical a priori knowledge for example of the order parameter. This anomaly detection scheme was tested on the extended Bose-Hubbard Model, which shows a rich phase diagram. Aside from the several standard phases, the method also reveals a new phase showing unexpected properties on the phase diagram. 

Under a similar scheme, the generative adversarial network (GAN) is trained in Ref.~\cite{Contessi:2021mrn} as the anomaly detector to identify gapless-to-gapped phase transition in different one-dimensional models. Specifically, the detection of the elusive Berezinskii--Kosterlitz--Thouless (BKT) phase transitions in the XYZ spin chain, the Bose--Hubbard model and the generalized two-component Bose-Hubbard model (all at zero temperature) is demonstrated~\cite{Contessi:2021mrn} with entanglement spectrum (measuring the degree of quantum correlation among sub-portions of the system) as dataset.

\emph{\textbf{Interpretable Learning}} --- 
Though phases of matter are shown to be detectable through supervised or unsupervised learning strategies, it's not clear yet in physics interpretation what the learning algorithm captured from such classification tasks. Some early works explored under the supervised kernel framework of support vector machine (SVM)\footnote{Briefly speaking, in the course of binary classification given training set $(\mathbf{x}^{(k)},y^{(k)}\in\{\pm 1 \})$, the SVM aims at determining a decision boundary, $\mathbf{\omega} \cdot \mathbf{x} +b=0$, a hyperplane with parameters $\mathbf{\omega}$ and $b$ to separate data into two classes. To clearly separate the data, a margin without any training data contained is inserted, which is with the boundaries defined as $\mathbf{\omega}\cdot\mathbf{x}+b=\pm 1$, and the margin width $2/||\omega||$ is expected to be maximized in generating the best separation of data. $\mathbf{\omega}\cdot\mathbf{x}+b=d(\mathbf{x})$ provide the \textbf{decision function}.} to give interpretable decision functions and further physical discriminators from the trained machines~\cite{2017PhRvB..96t5146P,2019PhRvB..99j4410L,2019PhRvB..99f0404G}.
Ref.~\cite{Wetzel:2017ooo} proposed a \textit{correlation probing neural network} to reveal the fact that the learned decision functions originate from physical quantities. It is also shown that a full explicit expression of the learned decision function can be reconstructed, from which one can further extract the quantities to facilitate the network's decision-making in classifying phases. The proposed procedure is demonstrated first on Ising model, where it dug out the magnetization and energy per spin as the decision support of the trained network. Then on SU(2) lattice gauge theory, a whole ML pipeline combining PCA and the \textit{correlation probing neural network} is constructed to examine the deconfinement phase transition related. The PCA with the average reconstruction loss serving as a universal phase transition identifier is shown to be able to capture the phase indicator, though the involved Polyakov loop is a non-linear order parameter in SU(2) gauge theory. This enables the awareness of an existing phase transition unsupervisedly. Then the \textit{correlation probing network} is trained to correctly predict phases and further construct the explicit expression of the decision function, to which it is found that the decision is based upon the Polyakov loop as a non-local and non-linear order parameter.

In many of the ML-based explorations for physics study, the more complicated algorithms though might be with better performance yet quite often lacking transparency and interpretability, especially when one seeks for new physical insight or comprehension of the learned representations from the data. In  
Ref.\cite{Blucher:2020mjt} it was proposed to adopt ``explainable AI'' techniques--specifically the layer-wise relevance propagation (LRP) method--to identify relevant features that influence the trained algorithms towards or against the particular recognition result. The work takes the (2+1) dimensional scalar Yukawa theory as a demonstration context, which displays an interesting phase structure with two broken phases (ferromagnetic denoted as FM, and antiferromagnetic denoted as AFM) separated by a symmetric paramagnetic phase (PM). These can be indicated by the normal magnetization and the staggered magnetization. With both the field configurations and preprocessed physical observables prepared, several machine learning models were trained to infer the action parameters (the hopping parameter $\kappa$ is taken in this work) from the known observables (labeled as approach A) or solely from the raw field configurations (labeled as approach B). It should be noted that the action parameter learning in this work is taken as just a pretext task to reveal the underlying phase structure or related physical insights. This is achieved by the adopted LRP to propagate the initially assigned relevance on the output to input layer by layer.

\emph{\textbf{Observables' Regression in QFT}} ---
The application of deep neural networks for regressive tasks in lattice quantum field theoretical setting was explored in
Ref~\cite{Zhou:2018ill} to unravel the dynamical information related to phase transition and physical observables. In Ref~\cite{Zhou:2018ill} the authors considered a complex massive scalar field with quartic coupling $\lambda$ in $(1+1)$-dimensional Euclidean space-time at nonzero temperature, and a finite chemical potential $\mu$ is introduced to control the charge density fluctuation, which also makes the action complex. The worldline formalism is taken for simulating the field configurations and circumventing the sign problem involved. Correspondingly, this $1+1$-d charged $\phi^4$ field is fully represented by 4 integer-valued dual variables: $k_1$, $k_2$ and $l_1$, $l_2$. Observables like number density and field square can also be calculated with the re-expressed partition function under the dualization approach. 
The ``silver blaze'' behavior is expected for this system at low temperature and low chemical potential $\mu$, that the particle density gets suppressed (with a mass gap) until some threshold value $\mu>\mu_{th}$ then enter the condensate region and increase considerably. Though for 2-dimensional systems there is no real phase transition related to symmetry breaking, one can refer to this pronounced change in density as a transition to condensation as treated in Ref.~\cite{Zhou:2018ill}. With the configurations training set prepared, two regressive tasks were attempted: phase identification and physical observable regression. 

For the phase identification,  a convolutional neural network (CNN) was devised to perform the phase binary classification task using the field configurations as input. Purposely, the training set is prepared to consist of only two ensembles of configurations with one well above and one well below the transition point ($\mu_{th}$), while the testing set is constructed with many ensembles of configurations at different chemical potential values sit in
between the two chemical potential values of training ensembles. The network output is interpreted as \textit{condensation probability}, $P(\phi)$, for each inputted configuration. Strong correlations were observed between the network output and number density or field square, without any specific supervision on the role of these observables in distinguishing the phases. The ensemble average of the network predicted condensation probability, $\langle P(\phi) \rangle$ serves as an accurate \textit{phase classifier}, with its first non-vanishing point well indicating the transition threshold value. It is worthy to note that such concept of interpreting the network output $P$ as an observable was also extended to undergo histogram reweighting \cite{Bachtis:2020dmf}\footnote{Specifically, the reweighting in terms of inverse temperature is performed for 2-dimensional Ising system as $\langle P\rangle_{\beta}=\frac{\sum^N_i P_{\sigma_{i=1}}\exp(-(\beta-\beta_0)H(\sigma_i))}{\sum^N_{i=1}\exp(-(\beta-\beta_0)H(\sigma_i))}$, with $\beta_0$ the place of inverse temperature where MCMC measurements for $P$ is given.} to construct an effective order parameter and perform scaling analysis (see also combination with transfer learning in studying unknown phase transitions \cite{Bachtis:2020ajb}). Ref.~\cite{Zhou:2018ill} further tried to reduce the input features of this phase classification task and found that with even restricted training input e.g.,~only $l$ or even only $k_1$ variables, the network still can well distinguish the two phases. Note that the ``order parameter'' --- number density of the system, $n$, is given by the sum of $k_2$ variables for this field system (See left panel of Fig.~\ref{fig:scalar_regress}). This thus indicates that the network has captured hidden structures in $k_2$ variables, though it ($k_2$) conventionally is not able to distinguish the low-density and high-density phases.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp!]
  \centering
  \includegraphics[width = 0.41\textwidth]{figures/condprob_avg.pdf}
  \includegraphics[width = 0.42\textwidth]{figures/regress_phi.pdf}
  \caption{(left) Ensemble average of condensation probability in testing stage along with chemical potential; (right) Comparison of the network predicted squared field $\phi^2$ and the true values at different chemical potentials. Taken from Ref.~\cite{Zhou:2018ill} with permission.}
  \label{fig:scalar_regress}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The same supervised training paradigm was used for the observables (number density $n$ and field square $\phi^2$)\footnote{Specifically the number density is given by $n=\frac{1}{N_1 N_2}\sum_x k_2(x)$ and the squared field is given by $|\phi|^2=\frac{1}{N_1 N_2}\sum_x\frac{W[s(k,l;x)+2]}{W[s(k,l;x)]}$ where the weight $W[s]=\int_0^{\infty}r^{s+1}e^{-(4+m^2)^2-\lambda r^4dr}$ and $s(k,l;x)=\sum_{\nu}[|k_{\nu}(x)| + |k_{\nu}(x-\hat{\nu})| + 2(l_{\nu}(x)+l_{\nu}(x-\hat{\nu}))]$.} regression task, with only the network output layer adapted to linear activation, and the loss function for training changed from cross entropy (for classification) to MSE. The same two ensembles of configurations were taken as the training set, with which the regression network is trained and then tested on previously unseen configurations at also different chemical potentials showing different number densities and field square. The regression performance was found to be successful over a broad range of chemical potentials, as shown in the right panel of Fig.~\ref{fig:scalar_regress}. While for the number density regression, the needed pattern may seem simple, the field square is a highly non-linear function of the high-dimensional input (i.e., field configurations with $4\times N_1 \times N_2$=8000 entries). Thus, the good predictive ability of the network in regressing field squares is non-trivial and impressive. Similar findings in the context of lattice Yang-Mills theories (SU(2) and SU(3)) about the transferability of the neural network learned regression function to a different parameter space are also reported in Ref.~\cite{Boyda:2020nfh}, where the Polyakov loop as gauge invariant deconfinement order parameter is the prediction target. 
For similar tasks investigated in Ref.~\cite{Zhou:2018ill}, authors in Ref.~\cite{Bulusu:2021rqz} further explored the influences of translational equivariance satisfaction of the used network structure on the regression performance and generalization capabilities.
Note also there's earlier trial~\cite{Yoon:2018krb} with traditional machine learning technique, specifically a boosted decision tree (BDT) regression algorithm, to reduce the computational cost of evaluating lattice QCD observables, by means of predicting observables from simpler and less compute-intensive observables' evaluation those are correlated with the target observable. 

\emph{\textbf{Enhanced Regression with Symmetries Embedded Networks}} --- 
Interactions for physics systems always respect some symmetries, which possess fundamental importance to physics theories across all scales nowadays. The incorporation of the symmetries for the system into the analyzing procedures like machine learning architectures has been proven to be beneficial in improving the performance of the algorithms. One such popular simple example is the convolutional neural network (CNN), which is good at pattern recognition for image-like data structure because of the satisfied global translational equivariance (as manifested in the sharing of convolutional kernels). This concept has now been extended to yield up group equivariant CNNs (G-CNN)~\cite{2016arXiv160207576C}, where more general symmetries including rotations and reflections are discussed, with also local symmetry e.g., for data on curved manifolds~\cite{2019arXiv190204615C}. See also a recent snowmass white paper~\cite{Bogatskiy:2022hub} for a report about symmetry group equivariant architectures across physics studies.
In the context of quantum field theory, symmetries provide important constraints on the action and thus are essential for lattice field theory study, their proper consideration is also crucial in obtaining meaningful results in lattice simulations.

As the fundamental theory for strong interactions that guide high-energy nuclear physics phenomenon, QCD is a (non-abelian) gauge theory that the Lagrangian should be invariant under local symmetry transformations that form symmetry group SU(3). Being relevant, there are lattice gauge equivariant (LGE) CNNs being proposed recently~\cite{Favoni:2020reg}. Consider a SU($N_c$) Yang-Mills theory on a lattice $\Lambda$ and discretized in terms of links variables (parallel transporters) $U_{\mu}(x)=\exp[-igA^{\mu}(x+a\hat{\mu}/2)]$,  the gauge links are transformed by group elements $\Omega_x$ as 
\begin{equation}
U_\mu(x) \to \tilde{U}_\mu(x) = \Omega(x) {U}_\mu(x) \Omega^\dagger(x+\hat{\mu}),
\label{eq:u_gauge}
\end{equation}
with $\Omega:\mathbb{R}^4\to$SU($N_c$) gauge transformations of the gauge fields. Taking the Wilson action as an approximation for the Yang-Mills theory with coupling $g$,
\begin{equation}
S_W[U]=\frac{2}{g^2}\sum_{x\in\Lambda}\sum_{\mu<\nu} \mathrm{Re} \mathrm{Tr}[\mathbbm{1}-U_{\mu\nu}(x)],
\label{eq:wilson_action}
\end{equation}
with $U_{\mu\nu}(x)$ the plaquette ($1\times1$ Wilson loop),
\begin{equation}
U_{\mu\nu}(x)=U_{\mu}(x)U_{\nu}(x+\mu)U_{-\mu}(x+\mu+\nu)U_{-\nu}(x+\nu),
\label{eq:plaquette}
\end{equation}
and transform under gauge transformation locally as $U_{\mu\nu}(x)\to\Omega(x)U_{\mu\nu}(x)\Omega^{\dagger}(x)$.To construct lattice gauge equivariant network architectures, Ref.~\cite{Favoni:2020reg} devised several elementary layers to explicitly respect the gauge symmetry. One essential starting point is processing the input fields to be tuples $(\mathcal{U,W})$, where $\mathcal{U}=\{U_{\mu}(x) \}$ the set of links of the configuration and $\mathcal{W}=\{ W_i(x) \}$ with $W_i(x)\in\mathbb{C}^{N_c\times N_c}$ a set of locally transforming complex matrices like the plaquettes is used as example ($W_i(x)\to\Omega(x)W_i(x)\Omega^{\dagger}(x)$, note that Polyakov loops can also be included as stated in Ref.~\cite{Favoni:2020reg}). Then two gauge equivariant operations are introduced to act on the tuple data, $(\mathcal{U,W})$, one is performing convolutions named as LGE convolution (L-Convs) and the other is LGE bilinear layer shortly named as L-Bilin, both leaving the gauge links variables unchanged while modifying only the $\mathcal{W}$ part in a covariant manner. Specifically, the L-Convs generalizes the normal convolutional operation to account for the parallel transport under geodesics to meet the requirement of gauge equivariance,
\begin{equation}
W^{'}_i(x)=\sum_{j,\mu,k}\omega_{i,j,\mu,k}U_{k\cdot \mu}(x)W_j(x+k\cdot \mu)U^{\dagger}_{k\cdot \mu}(x),
\label{eq:l_convs}
\end{equation}
with the kernel weights $\omega_{i,j,\mu,k}\in\mathbb{C}$ and $1\le i\le N_{ch,out}, 1\le j\le N_{ch,in}, 0\le\mu\le D, -K\le k\le K$ where $K$ specifies the kernel size and $N_{ch}$ the feature map channel number. Such L-Convs operation combines data at different lattice sites with parallel transport well taken into account. The L-Bilin layer on the other hand is acting on a single lattice site, which combines two input tuples with bilinear product (note again the $\mathcal{U}$ part are the same after operation):
\begin{equation}
W^{''}_i(x)=\sum_{j,k}\alpha_{i,j,k}W_j(x)W^{'}_j(x),
\label{eq:l_bilin}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp!]
  \centering
  \includegraphics[width = 0.8\textwidth]{figures/L-CNNs.pdf}
  \caption{A generic lattice gauge equivariant CNN as from Ref.~\cite{Favoni:2020reg} with permission.}
  \label{fig:lge_cnn}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
with the trainable weights $\alpha_{i,j,k}\in\mathbb{C}$ and $1\le i\le N_{out}, 1\le j\le N_{in,1}, 1\le k\le N_{in,2}$. Besides L-Convs and L-Bilin, LGE activation function and LGE Trace layer, plaquettes and Polyakov loops calculation layers (pre-processing layer to prepare the $\mathcal{W}$ information) are also proposed as elementary components to construct the gauge equivariant L-CNNs for lattice gauge field configuration treatment. See Fig.~\ref{fig:lge_cnn} for typical L-CNNs comprising the above-proposed LGE layers as from Ref.~\cite{Favoni:2020reg}. It is further demonstrated that such specially devised L-CNNs surpass traditional CNN models in the regression of gauge invariant observables. Recently, the L-CNN architecture has been extended to include additional global symmetries such
as rotations and reflections~\cite{Aronsson:2023rli}.


\subsubsection{Variational Neural-Network Quantum States}
\label{vnqs}
Many of the previous studies shown above rely on existing or pre-prepared ensembles of configurations for the physical system, up until Ref.~\cite{2017Sci...355..602C} pioneered the utilization of artificial neural networks to represent the wave function and presented a stochastic reinforcement learning scheme for solving the many-body problem without prior knowledge of exact samples. This gives a state-of-the-art accurate description of both the ground-state and time-dependent quantum states for a given Hamiltonian $\mathcal{H}$ across several prototypical spin systems, including 1 and 2-dimensional transverse-field Ising (IFI) and antiferromagnetic Heisenberg (AFT) models. The corresponding wave function, represented by a neural network, is basically a mapping from the N discrete-valued degrees of freedom set $\mathcal{S}= (\mathcal{S}_1,\mathcal{S}_2, ..., \mathcal{S}_N)$ to the complex phase and amplitude information, $\Psi(\mathcal{S})$ (taking the Restricted Boltzmann machine, RBM\footnote{Note in Ref.~\cite{2018PhRvB..97h5104C} it was demonstrated that the RBM possess holds an equivalence to tensor network states, which are widely used in quantum many-body physics}, with M hidden spin variables $h_i=\{\pm 1\}$ for example), 
\begin{equation}
\Psi_M(\mathcal{S};,\mathcal{W})=\Sigma_{h_i}e^{\Sigma_j a_j \mathcal{S}_j + \Sigma_i b_i h_i + \Sigma_{ij}W_{ij}h_i\mathcal{S}_j},
    \label{eq:nqs}
\end{equation}
is termed as neural-network quantum states (NQS) with the trainable parameters $\mathcal{W}=\{a_i, b_i, W_{ij}\}$. Such RBM representation is formally equivalent to a two-layer feed-forward neural network with special activation functions, i.e., $z^1(x)=\log\cosh(x)$,$z^2(x)=\exp(x)$.
Through the minimization of the energy expectation $E(\mathcal{W})=\langle\Psi_M|\mathcal{H}|\Psi_M\rangle/\langle \Psi_M|\Psi_M \rangle$, the network parameters $\mathcal{W}$ can be optimized via variational Monte Carlo (VMC) sampling. It has been shown that this proposed scheme can accurately evaluate the ground state energy in the TFI and AFH examples. Later, an extension of this approach to calculate excited states was also introduced, with both RBM and deeper fully connected neural networks~\cite{2018PhRvL.121p7204C}.

For the dynamical properties of the many-body state, which are elucidated upon solving the time-dependent Schr\"odinger equation, the NQS framework remains efficacious with extension of the network parameters to be complex-valued and time-dependent, $\mathcal{W}(t)$~\cite{2017Sci...355..602C}. Accordingly, per the Dirac-Frenkel time-dependent variational principle, the network parameter at each time $t$ can be trained, where the variational residuals are taken as the objective function,
\begin{equation}
R(t:\dot{\mathcal{W}}(t))=dist(\partial_t\Psi(\mathcal{W}(t)),-i\mathcal{H}\Psi),
    \label{eq:nqs_t}
\end{equation}
a feat attainable stochastically via a time-dependent VMC method. On both TFI and AFH models, this time-dependent NQS scheme has been demonstrated to capture with high precision the unitary dynamics induced by quantum quenches. There are further developments in using NQS for many-body physics studies, see Refs.~\cite{Noormandipour:2020dqp,2020PhRvL.124b0503S,Wu:2021tfb} and Refs.~\cite{Carrasquilla:2021zlj,2021arXiv210111099C}.

\subsubsection{Real-Time Dynamics Analysis}\label{subsubsec:realt}

Within modern theoretical physics, the dynamics of strongly correlated systems hold the central role for many pressing research problems, e.g., the hadronic spectrum/behaviors at zero temperature or immersed inside a thermal medium~\cite{Asakawa:2000tr,Rothkopf:2022fyo}, the non-equilibrium evolution and transport properties for the created QGP in heavy ion collisions~\cite{Rothkopf:2019ipj,Zhao:2020jqu}, the understanding for parton distribution functions of nucleons and nuclei~\cite{Ji:2020ect,Constantinou:2020pek,Candido:2023nnb}. The computation of these real-time physics is often noncompliant with perturbative analysis, thus calling for nonperturbative treatment such as lattice QFT simulations. These first principle Monte-Carlo-based simulations are usually performed in Euclidean space-time (after a Wick rotation $t\to i t\equiv\tau$) and provide only Euclidean correlators.
Accessing real-time physics from imaginary-time correlation's “measurements” in quantum Monte Carlo or lattice QFT simulation generally forms ill-conditioned inverse problems. Spectral representation forms a bridge to approach the real-time information of the dynamics from the Euclidean correlators. Also, quite often the relevant physics can be decoded directly from the spectral functions, like transport coefficients or in-medium hadronic behaviors~\cite{Asakawa:2000tr}.

\emph{\textbf{Spectral Function Reconstruction}} ---
The involved spectral reconstruction problem can, in general, be cast from a Fredholm equation of the first kind, $g(t)=\int_a^b K(t, s)\rho(s)ds$, with the aim of rebuilding the function $\rho(s)$ given the kernel function $K(t, s)$ and limited numerical evaluation on $g(t)$. Once only a finite set of evaluation data with non-vanishing uncertainties are possible for $g(t)$, the inverse transformation of the above convolution becomes ill-conditioned (see Ref.~\cite{Shi:2022yqw} and Sec.~\ref{sec:5:inverse} for more details). Basically, one can expand the convolution kernel (as a linear operator) by basis functions in a Hilbert space, within which it's shown in Refs.~\cite{J_G_McWhirter_1978} and \cite{Shi:2022yqw} respectively that the Laplace transformation kernel, $K(t,s)=e^{-st}$, and K\"allen--Lehmann (KL) kernel, $K(t,s)=s(s^2+t^2)/\pi$, possess arbitrarily small eigenvalues thus correspond to eigenfunctions being able to induce negligible changes for the integral result of $g(t)$. Consequently, for the inversion operator these eigenfunctions, termed null-modes, are related to arbitrarily large eigenvalues and will bring about numerically unstable inversion from noisy $g(t)$ to $\rho(s)$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp!]
  \centering
  \includegraphics[width = 0.7\textwidth]{figures/spectral_examples.pdf}
  \caption{Spectral functions differed by null-modes (left) and their corresponding K\"allen--Lehmann correlation functions (right). The inset figure shows the differences-in-propagator caused by null modes. Taken from~\cite{Wang:2021jou} with permission.}
  \label{fig:spectral_samples}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In the context of QFTs the involved target function, $\rho(s)$, is the spectral function, and the integral $g(t)$ corresponds to the correlator which can be measured from lattice calculations. To break the degeneracy for facilitating the inversion related, different regulator terms indicating prior domain knowledge have been proposed over the past several decades, such as Tikhonov regularization(L2 regularization)~\cite{Tikhonov1943OnTS,tikhonov1995numerical}, sparse modeling approach(L1 regularization)~\cite{Otsuki:2017sma,Itou:2020azb}, maximum entropy method (MEM)~\cite{Jarrell:1996rrw,Asakawa:2000tr} and related Bayesian Reconstruction (BR) method~\cite{Burnier:2013nla}.

Recent endeavors have integrated machine learning (ML) techniques to unravel this ill-posed problem, predominantly through a supervised, data-driven paradigm, albeit with some ventures into unsupervised learning realms. 
As the early stage ML application, in Ref.~\cite{2016arXiv161204895A}, both the kernel ridge regression (KRR) and kernel quantile regression (KQR) models were harnessed to invert the Fredholm integral of the first kind. Through the preparation of the training database, coupled with the restriction on basis functions and kernels involved, a regularization is naturally provided by such projected regression treatment, thus taming the ill-conditioned inversion.

Ref.~\cite{2018PhRvB..98x5101Y} firstly introduced deep convolutional neural network (CNN) and variants of stochastic gradient descent optimizer into the spectral reconstruction from imaginary time Green's function, which is domain-knowledge-free as distinct from Ref.~\cite{2016arXiv161204895A}. Being demonstrated on a Mott-Hubbard insulator and metallic spectrum, the deep CNN gives good reconstruction performance superior to the classical MEM method. It's also found that the usage of CNN structure achieved better reconstruction than a fully connected neural network structure. A comparable strategy, delineated in Ref.~\cite{PhysRevLett.124.056401}, incorporated the principal component analysis (PCA) to reduce the dimensionality of the QMC simulated imaginary time correlation function. Within a prototypical problem of quantum harmonic oscillator linearly coupled to an ideal heat bath--which presents a more physically pertinent scenario--Ref.~\cite{PhysRevLett.124.056401} showcased that a deep neural network, with PCA processed input, outperforms the maximum entropy method (MEM) in reconstructing the spectral function from the single particle fermionic Green's function, particularly as the data noise level increases. Ref.~\cite{Kades:2019wtd} extended such a data-driven supervised approach into QFT domain, with the K\"allen--Lehmann (KL) spectral representation considered. The database is prepared in the form of a combination of Breit--Wigner peaks, $\rho^{BW}(\omega)=4A\Gamma\omega/((M^2+\Gamma^2-\omega^2)^2+4\Gamma^2\omega^2)$. In rendering the network output -- spectral function, two schemes were examined: one is with parameters of the Breit--Wigner peaks inside the spectral function and the other is with a list of discretized data points of the spectral directly. The performance of reconstruction was found to be at least on par and in numerous instances surpassing classical methods particularly in large noise cases. 

Drawing inspiration from the adeptly crafted Shannon--Jaynes entropy term in regularizing this ill-posed inverse problem, Ref.~\cite{Chen:2021giw} proposed a novel framework dubbed SVAE based on the variational autoencoder (VAE) together with an incorporated entropy term “S” within the loss function during reconstructing spectral functions from Euclidean correlators. A Gaussian mixture model was employed to construct the spectral function database, while physically motivated spectral corresponded correlators were curated for testing. Realistic noise levels of lattice QCD data were infused both in training and testing datasets. It was discerned that the trained SVAE, in most of the cases, offered reconstruction quality comparable to MEM, and in cases with sharp spectral peaks with fewer data points for the correlator, SVAE demonstrated superior results in comparison to MEM.

The aforementioned studies predominantly hinge on training data set preparation to regularize the inverse problem, and, as a corollary, may exhibit dependency on the specific kinds of training data utilized. Diverging from this, certain studies have embraced unsupervised learning strategies to execute the inversion directly. For instance, Ref.~\cite{Zhou:2021bvw} employed the radial basis function network (RBFN) to represent the spectral, essentially approximating the spectral as a linear combination of radial basis functions (RBF),
\begin{equation}
  \rho(\omega)=\sum_{j=1}^N w_{j}\phi(\omega-m_j),\label{eq:linear_summation}
\end{equation}
with $\phi$ the active RBF unit at an adjustable center $m_j$, and $w_j$ the trainable weight. Upon discretization, Eq.~\ref{eq:linear_summation} can be rewritten into a matrix format $\rho=\Phi\,W$, whereby the K\"allen--Lehmann spectral representation integral becomes
\begin{equation}\label{eq:RBFMatForm}
  G_i=\sum_{j=1}^{M} \sum_{k=1}^N K_{ij}\Phi_{jk}w_k\equiv \sum_{k=1}^{M} \tilde{K}_{ik}w_k, \ \ \ i=1,\cdots,\widehat{N}
\end{equation}
where $\tilde{K}$ manifests as an irreversible $\hat{N}\times M$ matrix. By equating $M$ and $N$, the truncated singular value decomposition (TSVD) method can be seamlessly applied to deduce $w_j$. Contrasted with supervised learning endeavors, this methodology is fast in training and also free of over-fitting issues. Compared to traditional methods, RBFN resulted in better spectral reconstruction, especially for the low-frequency part which is pivotal for the extraction of transport coefficients in the Kubo formula.

As an alternate representation, Gaussian Processes (GP) are incorporated within the Bayesian inference procedure to reconstruct the 2+1 flavor QCD ghost and gluon spectral functions~\cite{Horak:2021syv}. Generally, a GP defines a probability distribution over functions, characterized by a selected kernel function, as expressed in,
\begin{align}
\rho(\omega)\sim \mathcal{GP}(\mu(\omega),C(\omega, \omega')),
\end{align}
where $\mu(\omega)$ is the mean function, usually set as zeros, and $C(\omega,\omega')$ denotes the covariance dictated by the kernel function. Ref.~\cite{Horak:2021syv} used the radial basis function (RBF) kernel. Actually, it has been proven that a GP is equivalent to an infinitely wide neural network. In this sense, the spectral representation in Ref.~\cite{Horak:2021syv} provides a nuanced expansion upon the one in Ref.~\cite{Zhou:2021bvw} since the utilization of the RBF activation.A distinctive facet of Ref.~\cite{Horak:2021syv}is the integration of GP-represented spectral priors within the Bayesian framework to construct the likelihood of the ghost and gluon spectral. The corresponding reconstruction of the spectral function for ghosts and gluons shows a similar peak structure as to the fRG reconstruction of the Yang-Mills propagator, highlighting a convergence in understanding across different methodological approaches.

In Refs.~\cite{Wang:2021jou,Wang:2021cqw, Shi:2022yqw} a novel approach pivoted on automatic differentiation and a general deep neural network representation [$\rho(\omega)=NN(\omega)$] has been devised (refer to Fig.~\ref{fig:5:inverse:ad} for the flow chart), categorizing it within the unsupervised learning domain as well, thus circumventing the over-fitting issue and negating the necessity for training data preparation in advance. Given its general strategy towards addressing inverse problems, we summarized them in Sec.~\ref{sec:5:inverse}, refer there for an elaborated discourse on the technical intricacies and insights garnered from the corresponding results.

\emph{\textbf{In-medium Heavy Quark Potential}} ---
Another intriguing and important real-time physics in the context of high-energy nuclear physics delves into the in-medium effects on hard probes, for example, the jets or heavy quarkonium (the bound states of heavy quark and its anti-quark). Being regarded as a smoking gun indicative of Quark--Gluon Plasma (QGP) formation, heavy quarkonium has been intensively studied both theoretically~\cite{Chen:2012gg, Zhao:2010nk, Zhou:2014kka, Zhao:2020jqu, Rothkopf:2019ipj} and experimentally~\cite{CMS:2011all, CMS:2012gvv}. A cardinal endeavor lies in comprehending the in-medium heavy quark interaction, the computation of which represents a big challenge for non-perturbative strong interaction calculations. Due to the large mass and small relative velocities for the inter quarks inside the bound state, a non-relativistic treatment of them is permissible, and also the color electric interactions inside will be dominant. It has been long expected that color screening will attenuate the heavy quark interaction, analogous to the Debye screening phenomenon observed in Quantum Electrodynamics (QED). Additionally, studies from both the hard thermal loop (HTL) calculations~\cite{Laine:2006ns, Beraudo:2007ky} and the recent effective field theory approach, e.g., pNRQCD calculations~\cite{Brambilla:2008cx, Brambilla:2010vq}, underscore the emergence of a non-zero imaginary part for the heavy quark interaction beyond mere screening effect within the QCD medium. To attain a comprehensive understanding, a non-perturbative framework like lattice QCD is warranted. Over the past decade, examinations centered on the real-time heavy quark interaction based upon lattice QCD calculations have been undertaken, mainly employing a Bayesian reconstruction technique~\cite{Rothkopf:2011db, Burnier:2014ssa, Burnier:2015tda} for spectral functions of the thermal Wilson loop. On the other hand, quantification of the in-medium Bottomonium masses and thermal widths have been released through the very recent lattice QCD studies~\cite{Larsen:2019bwy, Larsen:2019zqv, Larsen:2020rjk}. Empirically, it proffers an intriguing inquiry to discern whether an in-medium heavy quark potential $V(T,r)$ under a Quantum Mechanical potential picture can accommodate these in-medium properties unveiled in lattice studies, a query yet to be addressed from a field-theoretic point of view.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!hbtp]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/hq_flow_chart.pdf}
    \caption{Flow chart of in-medium heavy quark empirical potential reconstruction from LQCD measurements of mass and thermal width. Taken from Ref.~\cite{Shi:2021qri} with permission.\label{fig:hq_flow_chart}}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Ref.~\cite{Shi:2021qri} adeptly devised a DNN-based method to infer the in-medium heavy quark interaction, starting from the lattice QCD unveiled in-medium properties for Bottomonium. The DNN is introduced to parametrize the temperature and inter-quark distance-dependent complex potential $V(T,r)$ in a model-independent fashion, and is coupled to the Schr\:odinger equation to give rise to the bound state mass and thermal width. By comparing to the lattice QCD data, the corresponding $\chi^2$ can be evaluated and serve as the loss function to optimize the DNN representation. Perturbation response analysis is performed to derive the gradient of the loss function with respect to network parameters which involves naturally the Feynmann--Hellmann theorem, with which the optimization can be done using stochastic gradient descent algorithms. Fig.~\ref{fig:hq_flow_chart} displays the flow chart of this DNN-based automatic differentiation inference for heavy quark potential. The uncertainties of the reconstruction can be properly evaluated via Bayesian inference, which takes into account both the aleatoric and epistemic uncertainty per evaluation of the posterior distribution. For a more in-depth discourse, see Sec.~\ref{sec:5:inverse}.

\emph{\textbf{Parton Distribution Function Reconstruction}} ---
The Parton Distribution Function (PDF) is a fundamental property that elucidates the internal structure of hadrons, which specifically portrays the probability distribution of the momentum fraction carried by the constituent quarks and gluons inside the hadron. It can be measured in high energy deep inelastic scattering experiments and also computed in theoretical calculations, such as lattice field theory. One may refer to~\cite{Lin:2017snn, Forte:2020yip} for an overall review of recent developments in the study of PDF. There have been some pioneer efforts to employ deep learning techniques to help extract the parton distribution function in a nucleon, where the NNPDF approach~\cite{Forte:2002fg,Ball:2009mk,Ball:2010de,Ball:2012cx,NNPDF:2014otw,Bertone:2017tyb} which conducts global QCD analyses has been systematically applied in determining the PDF in proton and also the fragmentation functions (FFs) in the light-hadron. With neural network parametrization for the PDF, NNPDF performs the global QCD fit based on high-energy collider experimental data and higher-order perturbative calculation in QCD and QED/Electroweak theory. This adept methodology has also been extended to the determination of the nuclear parton distribution function (nPDF)~\cite{AbdulKhalek:2019mzd,AbdulKhalek:2020yuc,AbdulKhalek:2022fyi} with the $\chi^2$ minimization achieved via stochastic gradient descent. 

In the practice of lattice QCD calculation, a useful intermediate quantity is the Ioffe-time distribution --- the Fourier transformation of the PDF, $Q_\mathrm{Ioffe}(\lambda;\mu) = \int_{-1}^{1} dx\, e^{ix\lambda}\, q_\mathrm{PDF}(x;\mu)$, where $\mu$ is the energy scale, $x$ denotes the momentum fraction, and $\lambda$ represents the Ioffe-time. Meanwhile, the observables that can be computed in lattice QCD calculation can be expressed as a convolution of the Ioffe-time distribution. Therefore, if $Q_\mathrm{Ioffe}(\lambda;\mu)$ is obtained from lattice QCD calculation, one can then perform the inverse Fourier transformation and compute the PDF.
Ref.~\cite{Karpie:2019eiq} reconstructed the Ioffe time distribution using two approaches, i) a Bayesian Inference reconstruction and ii) a DNN representation. In the latter, the network parameters are updated according to a generic algorithm, which takes random walks in the network parameter space and selects the configurations that reduce the difference between the desired data and the reconstructed ones.
The study in Ref.~\cite{Gao:2022iex} further accentuates this framework by employing a DNN to represent $Q_\mathrm{Ioffe}(\lambda;\mu)$ and executing the gradient-driven update method (elucidated in Sec.~\ref{sec:5:inverse}) to optimize the network parameters. The new method significantly increases both the efficiency and accuracy of the Ioffe-time distribution reconstruction.
More applications can be found in Refs.~\cite{Forte:2002fg, Forte:2002us, Zhang:2019qiq, DelDebbio:2020rgv, DelDebbio:2021whr}.

\subsection{Sign Problem}
\label{sec:4:sign}
Many efforts have been made to find ways to surmount the sign problem in lattice QCD, yet it persists as a challenging and active area of research (see recent reviews in Ref.~\cite{Berger:2019odf,Alexandru:2020wrj,Nagata:2021ugx}). The most direct method can be summarized as the ``statistical approach", which attempts to enhance the statistics directly in tackling the problem. \textit{Reweighting} the observable with phase factors~\cite{Ferrenberg:1988yz} and representing the actions with \textit{density-of-states}~\cite{Wang:2000fzi} are two practical attempts. Moreover, direct methods such as Taylor expansions to $\mu/T$ around zero chemical potential~\cite{Allton:2002zi,Borsanyi:2015axp}, and analytic continuation from imaginary chemical potential~\cite{deForcrand:2002hgr,deForcrand:2009zkb}, are other viable strategies.

The alternative approach to solving the problem is through the ``new variable" method. This entails redefining a new set of variables to reformulate the complex action. One successful example is \textit{Dualization}, which uses dual variables to represent the partition function in terms of positive quantities~\cite{Rossi:1984cv,Berger:2019odf}, thereby bypassing the complexities associated with the sign problem. To properly handle the action on the complex plane, researchers have developed complex Langevin methods and integration contour deformations. The former approach originates from stochastic quantization~\cite{Parisi:1980ys} and involves processing the complex action with two coupled Langevin dynamics~\cite{Aarts:2013uxa,Attanasio:2020spv}. The latter approach should rely on the thimble method, with the latest advancements in machine learning techniques focusing on this approach~\cite{Alexandru:2020wrj}. Both the complex Langevin and Lefschetz thimbles methods utilize complexification of the field degrees of freedom to shift the path integration into the complex plane.

The key idea of the \textit{thimble method} is to continuously deform the integration contour for the path integral from the original real fields ($\in\mathbb{R}^{n}$) into an N-dimensional real manifold $\mathcal{M}$ embedded in the complexified field space ($\in\mathbb{C}^n$), on which the dramatic phase fluctuations induced by complex actions can be mitigated or even removed~\cite{1997CPL...270..382R}. Earlier attempts designated the manifold $\mathcal{M}$ as the set of Lefshetz thimbles, resembling the high-dimensional generalization of the ``steepest descent direction" or ``stationary phase path". Consequently, the resulting integrand turns out to be real and positive up to a total phase over the thimble~\cite{Cristoforetti:2012su,Cristoforetti:2013wha}. The reason is that the imaginary part of the action, $S_I[\phi]$, morphs into a locally constant entity, and the real part, $S_R[\phi]$, is as close as possible to the ``saddle point", thus constructing the best landscape to perform stochastic evaluations of the path integrals. This conceptual framework further inspired the ``generalized thimble method" ~\cite{Alexandru:2015sua,Nishimura:2017vav}, wherein the integration contour is deformed to a manifold $\mathcal{M}_T$ chosen as the evolution results of the so-called holomorphic gradient flow equation over a fixed flow time T, starting from the original integration domain,
\begin{equation}
    \frac{d\phi}{dt} = \frac{\overline{\partial S}}{\partial \phi},
\end{equation}
where $S$ is a generic Euclidean action, and the bar indicates the complex conjugation. The time $t$ is an auxiliary variable denoting the evolution of the equation. Each flowed configuration $\phi(T)$ (collectively constituting the manifold $\mathcal{M}_T$) uniquely corresponds to an original configuration, $\phi(t=0)=\zeta\in\mathbb{R}^n$, thereby establishing a one-to-one mapping $\tilde{\phi}(\zeta)=\phi(T)$. Starting from the initial point $\phi(0)\equiv \zeta$, as the flow time increases, $\tilde{\phi}(\zeta)\rightarrow\phi(T)$, the ensemble of fields eventually approaches the right combination of thimbles, aligning with the original integral. 


\subsubsection{NN-based Manifold}
To address the identification of the appropriate thimbles for alleviating the sign problem in larger systems, a significant flow time is required, which would lead to substantial computational consumption, particularly when assessing the required Jacobian. In Ref.~\cite{Alexandru:2017czx}, the authors proposed using a feed-forward neural network to approximate the thimble (or the generalized manifold) instead of directly solving the gradient flow equation and Jacobian evaluation. They referred to this network as "learnifold" since it predicts the imaginary part of the flowed manifold based on the real configuration input $\phi_R$,
\begin{equation}
    \tilde{\phi}(\phi_R) = \phi_R + i \tilde{f}_\theta(\phi_R),
\end{equation}

where the function $\tilde{f}_\theta(\cdot)$ is represented by a neural network. The \textit{learnifold} method, in contrast to the standard generalized thimble method, uses inputs from the real part manifold, resulting in a large Jacobian in practice. Additionally, the gaps between the integral contributing regions (thimbles) are smaller, making it easier to explore all relevant regions of integration objectively. This method can effectively address the issues of time-consuming flow evolution and multi-modal search in Monte Carlo sampling. The \textit{learnifold} network can incorporate translational symmetries. The authors tested the approach on sizable lattices in a 1+1 dimensional Thirring model with Wilson fermions and validated its effectiveness. 

This approach has been extended to the Hubbard model by Rodekamp et al.~\cite{Wynen:2020uzx}. Nevertheless, conventional (real-valued) neural networks continue to face computational challenges, primarily due to the extensive volume scaling of the Jacobian determinant. Recently, researchers have developed advanced neural networks that can learn the mapping from the integration manifold to the target manifold directly, utilizing complex values~\cite{Rodekamp:2022xpf, Rodekamp:2022ylw}, $\tilde{\phi}(\phi) = \tilde{f}_\theta(\phi)$. In this novel architecture, the Jacobian can be evaluated with high efficiency, due to the use of affine coupling layers. This results in a reduction of the scaling of the Jacobian determinant from a general cubic scaling to a linear scaling in volume. The method has been demonstrated in systems of different sizes.

\subsubsection{Normalizing Flow for Complex Actions}
Recalling the flow-based model introduced in Sec.~\ref{sec:3:flow_based}, one can observe similarities with contour deformation. The main idea of a normalizing flow is to construct an isomorphic deformation on smooth manifolds. While it is incapable of naturally dealing with complex actions, a generalization can be conceived by defining an integration contour to help alleviate the sign problem. In their works, Lawrence and Yamauchi~\cite{Lawrence:2021izu,Yamauchi:2021kpo} discuss the necessary conditions for the existence of a manifold that can solve the sign problem exactly, which establishes the requirement for constructing complex normalizing flows. The authors demonstrate the effectiveness of the manifold numerically over a range of couplings for the Schwinger--Keldysh sign problem associated with a real scalar field in 1+1 dimensions. Manifolds that can approximately solve the sign problem may be found in various physical systems, as indicated by Lawrence et al. in Ref.~\cite{Lawrence:2022afv}. 
In addition, in a recent paper~\cite{Pawlowski:2022rdn}, Pawlowski and Urban proposed to compute the density directly with the normalizing flow, which is the core of the \textit{density-of-states} approach for tackling sign problems. They validated the method in a two-component complex scalar field theory in which an imaginary external field explicitly breaks O(2) symmetry.\footnote{Note that in condensed matter physics, there are earlier trials using automatic differentiation to optimize a sufficiently general Hubbard--Stratonovich transformation on fermionic system to mitigate the sign problem~\cite{Wan:2020lff}, aligning with flow-based strategies in terms of field transformation optimization.}

\subsubsection{Path Optimization Method}

In Ref.~\cite{Mori:2017pne,Mori:2017nwj}, the authors first proposed the path optimization method. This new approach addresses the sign problem as an optimization problem of the integration path. They utilized a cost function to specify the path in the complex plane and adjusted it to minimize the cost function that represents the degree of weight cancellation,
\begin{equation}
    F[\phi(t)] = \frac{1}{2}\int dt |e^{i\theta(t)} - e^{i\theta_0}|^2 |J(\phi(t))e^{-S[\phi(t)]}|,
\end{equation}
where $\theta(t)$ is the complex phase of the parameterized integrand $J(\phi(t))e^{-S[\phi(t)]}$, and $\theta_0$ is the complex phase of the original integrand. The original partition function becomes $Z= \int_\mathcal{C}\mathcal{D}t J[\phi(t)]\text{exp}\{-S[\phi(t)]\}$. This method eliminates the need for solving the gradient flow found in the Lefschetz-thimble method. Instead, the construction of the integration-path contour becomes an optimization problem that can be solved using various efficient methods, e.g., gradient-based algorithms. This method has been successfully extended to e.g., 2D complex $\lambda\,\phi^4$ theory~\cite{Mori:2017nwj}, the Polyakov-loop extended Nambu--Jona--Lasinio model~\cite{Kashiwa:2019lkv,Kashiwa:2018vxr}, the 0+1 dimensional Bose gas~\cite{Bursa:2018ykf}, the 0+1 dimensional QCD~\cite{Mori:2019tux}, as well as SU(N) lattice gauge theory~\cite{Detmold:2021ulb}.

Simultaneously, in Ref.~\cite{Alexandru:2018fqp,Alexandru:2018ddf}, Alexandru et al. also investigated the path optimization method by parameterizing the manifold with neural networks $\mathcal{M}_\theta$. They first showed the results for the 1+1 dimensional Thirring model with Wilson fermions on lattice sizes up to $40\times10$. Then, they demonstrated the performance in the 2+1 dimensional Thirring model~\cite{Alexandru:2018ddf}. The recent progress of the path optimization method can be found in the review~\cite{Alexandru:2020wrj}.

Namekawa et al. further investigated the efficiency of gauge-invariant inputs~\cite{Namekawa:2021nzu} and gauge-covariant neural networks approximating the integral path~\cite{Namekawa:2022liz} for the path optimization method. The motivation is that the path optimization method with a completely gauge-fixed link-variable input can tame the sign problem in a simple gauge theory, but does not work well when the gauge degrees of freedom remain. To overcome this problem, the authors proposed employing a gauge-invariant input, such as a plaquette, or a gauge-covariant neural network, which is composed of the Stout-like smearing for representing the modified integral path. The efficiency is evaluated in the two-dimensional U(1) gauge theory with a complex coupling. The average phase factor is significantly enhanced by the path optimization with the plaquette or gauge-covariant neural network, indicating good control of the sign problem. Furthermore, another improvement is dropping the Jacobian during the learning process, which reduces the numerical cost of the Jacobian calculation from $O(N^3)$ to $O(1)$, where $N$ is the number of degrees of freedom of the theory.  Although a slight increase in the statistical error will emerge with the approximation, this practical strategy with invariant/covariant designs will push the path optimization toward solving complicated gauge theories.

\subsection{Summary}
The intricate landscape of lattice field theory presents a unique set of computational challenges, 
%This chapter first introduces the unique set of computational challenges encountered in lattice field theory, 
notably in the endeavor to undertake non-perturbative QCD calculations pivotal for understanding the properties of extreme nuclear matter. This chapter delineates these challenges and encapsulates significant explorations in three paramount sectors: %We then summarize relevant explorations from three sectors: 
field configuration generation, lattice data and physics analysis, and the sign problems, with an emphasis on the burgeoning role of the machine and deep learning techniques in propelling lattice field studies forward. %Around these three facets, we present the current state of using machine/deep-learning techniques to facilitate lattice field study. 
Many recent advanced developments are also discussed in this chapter, such as the symmetry embedding into the learning algorithms, the integration of physics priors through automatic differentiation in tackling inverse problems involved in real-time physics extraction, flow-based quantum field configuration generation, and a series of novel trials in applying deep neural networks for mitigating the sign problems in lattice simulation.