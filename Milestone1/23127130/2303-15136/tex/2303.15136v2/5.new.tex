\section{Advanced Developments --- Physics Meets ML}\label{sec:new}

In the previous sections, we have reviewed how deep learning can be broadly applied to diverse aspects of high-energy nuclear physics, serving as a powerful tool for addressing specific problems abstracted from physical domains.
As we also noted, both accumulating large amounts of data rapidly and collecting small amounts of physical observations slowly pose challenges to readily usable deep learning techniques. Indeed, one needs to further enhance the efficiency, if not the applicability, of deep learning by incorporating physics knowledge into the learning process. In the development of deep learning, several attempts have been made to introduce physics knowledge, including:
\textit{Physics-inspired}~\cite{Ahmad:2020kdd, sompolinsky1988statistical, mezard2009information} deep learning, where physics properties such as symmetry or conservation laws are implemented in the architecture of the network and strictly enforced. For properties or rules that cannot be directly encoded in the network setup, one can adopt the \textit{physics-informed}~\cite{2021NatRP...3..422K} approach and include a loss function to penalize their violation. Finally, the \textit{physics-driven}~\cite{2021arXiv210905237T} approach extends the backpropagation procedure to train network parameters when the output of a network is related to observables through a differentiable function or functional mapping.


The EOS reconstruction from Neutron Star mass and radius observations that was discussed in Sec.~\ref{sec:astro} is a good example of how these three approaches can be simultaneously implemented in different aspects of a particular problem. 
One may reconstruct the nuclear matter equation of state, i.e., energy density as a function of pressure $\varepsilon(P)$, from the observation of mass and radius for neutron stars. One may adopt a neural network to represent the inverse of the speed of sound, $c_s^{-2}(P)\equiv \mathrm{d}\varepsilon(P)/\mathrm{d}P$. Then the causality requirement can be fulfilled by setting the activation function of the output layer to be $\sigma(z) = 1+e^z$. Such procedure encodes the physics constraint in the setup, and therefore it is a \textit{physics-inspired} procedure. Then prior knowledge from chiral Effective Field Theory(perturbative QCD calculation) can be implemented as a term in the loss function to constrain the low(high)-pressure sector of the equation of state, which fits the definition of \textit{physics-informed} deep learning. Finally, \textit{physics-driven} deep learning would be reflected by the variation analysis of the TOV equation ---  the functional mapping between $\varepsilon(P)$ and the mass-radius relation --- in which the functional derivative of mass and radius with respect to arbitrary change of the equation of state can be computed to guide the optimization of the network parameter in gradient-based training.

In this section, we focus on some works that have been briefly mentioned earlier in the previous sections and expand on their advanced developments of method which implement physics knowledge and/or constraints into the machine learning task. 

\subsection{Manifesting Physics Properties in NNs}
\label{sec_phy_manifest}
An illustration of physics-inspired deep learning is the integration of physics properties, such as symmetry, into the architecture of a neural network. For further information on this topic, readers can refer to references~\cite{Goodfellow2016, Mattheakis:2019tyi, Kicki:2021so}.
Many special neural network architectures have been designed to incorporate specific physics properties. For example, Convolutional Neural Networks (CNNs) are known to be invariant under coordinate translation~\cite{zhang1988shift,Goodfellow2016}. The E3NN architecture~\cite{e3nn_paper} is designed to preserve rotational invariance, while Group Equivariant Convolutional Networks~\cite{pmlr-v48-cohenc16}, Point Nets~\cite{qi2016pointnet} with permutation invariance, and Lorentz group equivariant Neural Networks~\cite{Bogatskiy:2020} are other examples.

In the field of nuclear and particle physics, implementing symmetry properties has been applied to reduce the computational complexity of machine learning tasks. This includes point cloud representations for collider event classifications~\cite{Onyisi:2022hdh}, embedding guage symmetry in normalizing flow for lattice guage field calculations~\cite{Kanwar:2020xzo}, Feynman path generation using Fourier-flow-based models~\cite{Chen:2022ytr} and the use of CNNs to approximate the inversion of the renormalization group transformation in quantum field theory~\cite{Bachtis:2021eww}. The last three examples will be discussed in more detail in the remainder of the current subsection.

\subsubsection{Gauge Symmetry in Normalizing Flow}
\label{flow_symmetry}
As introduced in Section~\ref{sec:3:flow_based}, the flow-based generative machine learning methods offer a promising solution to addressing the critical slowing down(CSD) in lattice simulations. Specifically, in the context of high-energy nuclear physics (HENP) studies of quantum chromodynamics (QCD), lattice gauge field simulations require consideration of special local symmetries. In recent years, learning architectures that encode the relevant symmetries in lattice field theory have been developed~\footnote{See also the end of Section~\ref{sec_phases_obs} where the incorporation of symmetries into neural networks is discussed for a regression task in lattice gauge theory studies.}.

In Ref.~\cite{Kanwar:2020xzo}, a gauge invariant flow model was developed to sample configurations in a $\mathrm{U}(1)$ gauge theory. On a lattice, the field configuration can be represented by the gauge variable $U_\mu(x)$, which connects the neighboring sites $x$ and $x+\hat{\mu}$. For a theory with $N_d$ spacetime dimensions (i.e. $\mu = 1, 2, \cdots, N_d$), the variable $U(x)$ has $G$ degrees of freedom (e.g. color). The authors defined the lattice volume as $V$, and the field configurations have dimensions of $G^{N_d V}$. The theory is invariant under a gauge transformation,
\begin{align}
    U_\mu(x) \to \tilde{U}_\mu(x) = \Omega(x) {U}_\mu(x) \Omega^\dagger(x+\hat{\mu}),
\end{align}
where the space-time-dependent $G\times G$ matrices, $\Omega$, act on the intrinsic degrees of freedom.
Therefore, the physical distribution shall also be invariant, i.e., 
\begin{align}
    p(U) = p(\tilde{U}).
    \label{eq:5:gauge_invariance}
\end{align}
In comparison to learning symmetries across a training dataset, the gauge invariant flow model efficiently encodes the invariance of distribution. One may use the normalizing flow method introduced in Sec.~\ref{subsubsec:gm}, in which the invertible coupling layers ($g$) are bijective mappings between a $G^{N_d V}$-dimensional manifold and itself. Equation~\eqref{eq:5:gauge_invariance} will be fulfilled if two conditions are met: 1) the prior distribution ($\mathcal{P}$) is symmetrical and 2) each coupling layer commutes with the gauge transformations. While the first criterion can be easily satisfied by setting $\mathcal{P}$ as a uniform distribution, the second requires a specific design for the coupling layer.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!hbtp]
    \centering
    \includegraphics[width=0.6\textwidth]{fig_5-1_2003-06413}
    \caption{Demonstration of gauge invariant normalizing flow. Taken from~\protect{\cite{Kanwar:2020xzo}} with permission. \label{fig:5:gauge_inv}}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In~\cite{Kanwar:2020xzo}, each $g$ only acts on some of the gauge links, and one may divide the $G^{N_d V}$-dimensional manifold into two subsets, $U^A$ and $U^B$. All elements in the latter are invariant under $g$: $\forall U\in U^B$, $g(U)=U$; whereas a link in the former ($U^i \in U^A$) is mapped to
\begin{align}
    U'^i \equiv g(U^i) = h(U^i S^i|\boldsymbol{\Pi}_i) (S^{i})^{\dagger},
\end{align} 
in which $S^i$ is a product of links such that $U^i S^i$ forms a loop ending at the starting point. It belongs to $U^B$ to ensure invertibility, and its explicit form can be determined when a theory is specified. $h$ is a $G$-to-$G$ dimensional invertible kernel explicitly parametrized by a set of gauge invariant quantities ($\boldsymbol{\Pi}_i$) constructed from the elements of $U^B$, and satisfies,
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{align}
    h(W X W^\dagger) = W h(X) W^\dagger, \qquad\forall X, W \in G,
\end{align}
so that $g$ commutes with the gauge transformation:
\begin{align}
\begin{split}
    U'^i \to \tilde{U}'^i 
=\;&
    h\Big(\Omega(x) U^i S^i \Omega^\dagger(x)\Big) \; \Big(\Omega(x+\hat{\mu}) S^{i} \Omega^\dagger(x)\Big)^{\dagger} 
\\=\;&
    \Omega(x)  h(U^i S^i) \Omega^\dagger(x)\; \Omega(x) (S^{i})^{\dagger} \Omega^\dagger(x+\hat{\mu})
\\=\;&
    \Omega(x) U'^i \Omega^\dagger(x+\hat{\mu}).
\end{split}
\end{align}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Taking a $\mathrm{U}(1)$ gauge field in $1+1$ dimension as an application example, to which the lattice discretization for this field can be approximated through the Wilson action, defined as:
\begin{align}
S(U) = -\beta \sum_x \mathrm{Re} P(x),
\end{align}
where $P(x)=U_0(x) U_1(x+\hat{0}) U_0^\dagger(x+\hat{1}) U_1^\dagger(x)$ represents the plaquette at $x$. The choice of $S_i$ is depicted in Fig.~\ref{fig:5:gauge_inv}, with $U_i S_i$ representing $1\times1$ loops adjacent to each $U_i$. The elements of $U^A$ are sparse enough such that updates do not overlap with one another. With the aforementioned construction of gauge-equivariant coupling layers, a flow-based MCMC scheme was employed on this 1+1D $\mathrm{U}(1)$ gauge theory at a fixed lattice size ($L=16$). This resulted in more efficient calculations for topological quantities compared to traditional HMC and Heat Bath algorithms~\cite{Kanwar:2020xzo}.


\subsubsection{Inverse Renormalization Group Transformation} 
In addition to Normalizing Flow method~\cite{Albergo:2019eim, Kanwar:2020xzo, Albergo:2021vyo} which independently samples field configurations in each round, another approach to addressing the CSD using physics-inspired deep learning is the implementation of Convolutional Neural Networks (CNNs) in solving inversion of renormalization group (RG) transformations. This has been independently studied in the context of quantum field theory~\cite{Bachtis:2021eww} and spin systems~\cite{Shiina:2021pqe}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!hbtp]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/rg.png}
    \caption{Taken from Ref.~\cite{Bachtis:2021eww} with permission. Demonstration of the inverse RG training with transposed convolution (TC).\label{fig:5:rg}}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

At the critical point, a thermal system exhibits divergence of the correlation length and  therefore there is no length scale remaining in it. As a result, the expectation values of intensive observables are invariant under arbitrary scale transformations. This property is attributed to the theory of RG, which is widely used to study thermodynamics near the critical point. In the typical application of RG, one reduces the size of a system by coarse-graining the configurations of a thermal ensemble, comparing the thermal quantities before and after the operation. The number of RG steps is limited by the original finite size of the system, as the operations always result in a reduction in size. However, one needs to approach the critical behaviors in a large-size limit, which is routinely hindered by computational resources. It has been noted that the coarse-graining operation in real space can be treated as a convolution operation with step $2$ and filter size $2 \times 2$. The authors of~\cite{Bachtis:2021eww} and~\cite{Shiina:2021pqe} supervisedly train a transposed convolution or upsampling layer with filter size $2 \times 2$ to approximate the inverse operation of RG\footnote{See Ref.~\cite{2016arXiv160307285D} for details on the involved different convolution arithmetics}. Fig.~\ref{fig:5:rg} gives an illustration of such training using data generated from standard RG procedures. With this tool, it is able to generate large-size configurations from small-size ones with an infinite number of inverse RG steps. One can then accurately determine the location of the critical point and study the critical exponents in its vicinity. 



\subsubsection{Fourier-flow Model Generating Feynman Paths}
Besides embedding symmetries and approximating physical processes with neural networks, it is helpful to deploy the machine learning algorithms in a more efficient physics representation. In Ref.~\cite{Chen:2022ytr}, the authors proposed a Fourier flow model(FFM) to simulate the Feynman propagator and generate paths for quantum systems (see Fig.~\ref{fig:5:ffm}). The Fourier transformation is introduced to approach a Matsubara representation in order to preserve the physics condition for the system. The Euclidean action defined in a discrete time, $S_\text{E}[x_n]$, is converted into the Fourier space as, $S_\text{E}[X_k]$. The Fourier modes, $X_k$, represent multi-level correlations in coordinate space. Time-reversal symmetry requires that certain boundary conditions be imposed on the discretized paths ${x_n}$. They include invariance under translation (${n\rightarrow n+1}$), inversion (${n\rightarrow -n}$), and periodicity (${n\to n+N}$), where $n$ is the index of the site. These conditions are naturally satisfied in Fourier space. The path generator of FFM is validated on the harmonic and anharmonic oscillators as a demonstration. The latter is set as a multimode system in a double-well potential without an analytic solution. The ground-state wave function and low-lying energy levels are accurately estimated.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!hbtp]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/fig_5-2-1_FFM.pdf}
    \caption{Demonstration of the Fourier flow model. The discrete Fourier transformation(DFT) and inverse discrete Fourier transformation(iDFT) are inserted before the input and after the output of a normalizing flow model respectively.\label{fig:5:ffm}}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

It should be noted that the kinetic term of action, $m(\Delta x/\Delta t)^2$ which correlates in coordinate space, will disentangle in frequency space as, $m|X_k|^2$(see details in Ref.~\cite{Chen:2022ytr}). As a matter of fact, the Feynman path describes the field in 0+1 dimensional field theory. The Fourier transformation converts the dynamical fields to the inverse of the two-point correlation function in frequency space (also known as the inverse power spectral density as discussed in Ref.~\cite{Komijani:2023fzy}). It can preserve the dynamical information at least at mean-field level~\cite{Georges:1996zz}. This method provides a novel way to manifest physics properties, such as time-reversal symmetry and dynamical mean-field theory, in the design of flow-based models.

\subsection{Fusing Physics Models into NNs}\label{sec:5:phy_mod}

Another type of physics-inspired machine learning leverages the mathematical similarity between a specific physics model and a neural network architecture, enabling the application of existing deep learning frameworks to solve physics problems with ease. Some pioneer attempts~\cite{Hashimoto:2018ftp,Hashimoto:2019bih,Hu:2019nea,Hashimoto:2021ihd} have been made in the direction of anti-de Sitter/conformal field theory(AdS-CFT) correspondence. The AdS-CFT correspondence~\cite{Maldacena:1997re,Gubser:1998bc,Witten:1998qj} is a renowned holographic relation between $d$-dimensional Quantum Field Theories and $(d+1)$-dimensional gravity. It has been widely applied in solving problems for strong coupling quantum fields. In the limit of large number of colors ($N_c$), the generating functional of the CFT boundary and the action of the gravity bulk are related by the Gubser--Klebanov--Polyakov--Witten relation,
\begin{align}
    Z_\mathrm{QFT}[J] = \exp(-S_\mathrm{gravity}[\phi]),
\end{align}
where $\phi(x,\eta)$ is the bulk field that satisfies the boundary condition $\phi(x,\eta=0) = J(x)$. It has been argued~\cite{Hashimoto:2019bih} that AdS/CFT correspondence as a deep Boltzmann machine~\footnote{It has also been found that the existence of the exact mapping between the deep Boltzmann machine and the renormalization group~\cite{mehta2014exact}.}. The Boltzmann machines are network models that give a probabilistic distribution of variables $v_i$ that defined as,
\begin{align}
    P(v_i) = \exp\Big(-\sum_i a_i v_i - \sum_{i,j} w_{ij} v_i v_j\Big),
\end{align}
with $a_i$ and $w_{ij}$ being network parameters. The deep Boltzmann machines further include hidden variables to enhance the expression ability, 
\begin{align}
    P(v_i) = \sum_{h_i^{(k)}} \exp\Big(-\sum_{i,j} w_{ij}^{(0)} v_i h^{(1)}_j - \sum_{k=1}^{N-1}\sum_{i,j} w_{i,j}^{(k)} h^{(k)}_i h^{(k+1)}_j\Big),
\end{align}
When comparing with AdS/CFT correspondence, the variables of interest are the quantum fields $J$, represented as $v_i$, and the bulk field $\phi$ corresponds to the hidden variables $h^{(k)}_{j}$. The bulk field is governed by its equation of motion, which can be modeled through the setup of a deep neural network in the deep Boltzmann machine, as claimed in~\cite{Hashimoto:2018ftp}. For instance, in the $(d+1)$-dimensional space-time with the metric,
\begin{align}
    \mathrm{d}s^2 = -f(\eta) \mathrm{d}t^2 +\mathrm{d}\eta^2
    + g(\eta)(\mathrm{d}x_1^2 + \cdots + \mathrm{d}x_{d-1}^2),
\end{align}
with $\eta$ being the holographic direction, a scalar field theory is defined by the action,
\begin{align}
    S = \frac{1}{2}\int \mathrm{d}^{d+1} x \, \sqrt{|g|} \Big(g^{\mu\nu}(\partial_\mu \phi) (\partial_\nu \phi) + m^2 \phi^2 + \frac{\lambda}{2} \phi^4 \Big),
\end{align}
where $\sqrt{|g|} = \sqrt{f(\eta) g^{d-1}(\eta)}$ is the spacetime volume factor. The classical equation of motion for a homogeneous, constant field ($\phi$) and its canonical momentum ($\pi$) reads
\begin{align}
\begin{split}
    \partial_\eta \pi + h(\eta) \pi - m^2 \phi - \lambda \,\phi^3 = 0\,,
\qquad
    \partial_\eta \phi = \pi\,.
\end{split}
\label{eq:5:adscft}
\end{align}
where $h(\eta) \equiv \partial_\eta \ln\sqrt{|g|}$. It has been claimed in~\cite{Hashimoto:2018ftp} that numerically solving \eqref{eq:5:adscft} with Euler's method is equivalent to a DNN with width $w=2$ at each layer, see Fig.~\ref{fig:5:adscft}. Each layer corresponds to an Euler iteration step. The weights at the $n^\mathrm{th}$ layer are set to be 
\begin{align}
W^{(n)} = 
\left(\begin{array}{cc}
    m^2\delta\eta & 1-h(\eta^{(n)})\Delta\eta\\
    1 & \Delta\eta
\end{array}\right),
\end{align}
whereas biases are set to zero. Linear activation is set for $\phi$ and non-linear one is set for $\pi$ to implement the interaction term ($\lambda \phi^3$).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!hbtp]
    \centering
    \includegraphics[width=0.5\textwidth]{fig_5-2_1802-08313}
    \caption{Demonstration of solving Eq.~\protect{\eqref{eq:5:adscft}} with DNN. Taken from~\protect{\cite{Hashimoto:2018ftp}} with permission. \label{fig:5:adscft}}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In Ref.~\cite{Hashimoto:2018ftp}, the authors aim to determine the function $h(\eta)$ given the values of $\pi$ and $\phi$ at the $\eta=\infty$ and $\eta=0$ limits. As a mock test, the authors prepared a set of data by solving Eq.~\eqref{eq:5:adscft} exactly with a known $h(\eta)$. The parameters of the network [$h(\eta^{(n)})$] were then trained to match the relationship between the large and small boundary values in the holographic direction. The results showed that the reconstructed metric agreed well with the true values, with a deviation of about $30\%$ in the near-horizon (small $\eta$) region.


The procedure has been further improved by parameterizing $h(\eta)$ by polynomials~\cite{Hashimoto:2020jug}. The loss function and parameter training procedure of the neural ODE~\cite{2018arXiv180607366C} are borrowed to learn the polynomial coefficients in $h(\eta)$. Then, the authors trained the machine from lattice QCD results of the quark mass condensate and obtained results that are qualitatively consistent with the temperature dependence of the confinement and the Debye-screening behavior.

\subsection{Solving Complex Inverse Problems}\label{sec:5:inverse}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[!hbpt]
\centering
\begin{tabular}{l |l | l | l}
\hline\hline
    problem & quantity of interest $\mathcal{Q}(x)$ & observable $\mathcal{O}_y$ & relation \\
\hline
Sec.~\ref{subsubsec:realt} & 
    in-medium potential $V(r)$&
    energy spectrum $\{E_n\}$&
    Schr\"odinger equation~\eqref{eq:5:schroedinger}\\
Sec.~\ref{subsec:mr} & 
    equation of state $\varepsilon(P)$&
    mass and radius $\{M_i, R_i\}$&
    TOV equation~\eqref{eq:4:tov}\\
Sec.~\ref{subsubsec:realt} & 
    spectral function $\rho(\omega)$&
    Euclidean correlator $D(k)$&
    K\"allen--Lehmann convolution~\eqref{eq:5:corr_D}\\
\hline\hline
\end{tabular}
\caption{Complex inverse problems: quantities of interest, observables, and the relation between them.}
\label{tab:inverse_example}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We finally review how deep learning helps in solving inverse problems~\cite{Zhou:2023tvv}.
Nuclear physics, and more broadly, physics, presents numerous challenging inverse problems. In these problems, the forward problem is straightforward, but its inversion is not. For instance, consider a quantum system described by the  Schr\"odinger equation with a potential model, if the interaction potential is known, the microscopic properties such as the energy levels and corresponding wave functions can easily be predicted. However, for certain systems like mesons (bound states of quarks and anti-quarks), the energy spectrum can be experimentally measured, but the effective interaction potential remains unclear. In this case, extracting the interaction potential from the given energy spectrum, which is the inverse problem of solving the Schr\"odinger equation, poses a significant challenge, yet remains an important practical issue. Deep learning has been found suitable for solving such inverse problems, examples include reconstructions of heavy quark interaction potential~\cite{Shi:2021qri}, spectral function~\cite{Kades:2019wtd,Chen:2021giw,Zhou:2021bvw,Wang:2021jou,Wang:2021cqw,Shi:2022yqw,Horak:2021syv}, parton distribution function~\cite{DelDebbio:2007ee,Ball:2010de,Ball:2011gg,Ball:2011uy,Nocera:2014gqa,Gao:2022iex}, nuclear matter equation of state~\cite{Fujimoto:2017cdo,Fujimoto:2019hxv,Fujimoto:2021zas,Soma:2022qnv,Soma:2022vbb}, and effective parton mass in a finite-temperature QCD medium~\cite{Li:2022ozl}.


Such inverse problems share similar characteristics. The unknown functions of interest, denoted as $\mathcal{Q}(x)$, are continuous, while the observables which can be either continuous functions or discrete variables, are functionals of $\mathcal{Q}(x)$, i.e. $\mathcal{O}_y = \mathcal{F}_y[\mathcal{Q}(x)]$, where the subscript $y$ labels the continuous argument or discrete index of the observables. A summary of the quantities of interest, observables, and their relationship for the inverse problems discussed in this subsection can be found in Table~\ref{tab:inverse_example}.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!hbpt]
    \centering
    \includegraphics[width=0.9\textwidth]{fig_5-3_flow_chart}
    \caption{Different approaches of solving inverse problems.}
    \label{fig:5:inverse}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Generally speaking, there are three different approaches to solving such inverse problems. They are summarized in Fig.~\ref{fig:5:inverse} and listed as follows.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{itemize}
\item[(i)]
    The first approach is to parameterize or discretize $\mathcal{Q}(x)$, prepare a sufficiently large ensemble of different $\mathcal{Q}$'s, compute their corresponding $\mathcal{O}$'s from the feasible forward modelling and exploit machine learning techniques, such as regression or classification algorithms e.g., with deep neural networks(DNNs) to learn an inverse mapping from the set of $\{\mathcal{O},\mathcal{Q}\}$ pairs. Such an approach falls in the category of direct supervised learning. Physics priors are implicitly manifested inside the training data collection. This approach has been demonstrated in studies discussed in, e.g., Sections~\ref{hic_b}, \ref{hic_cme}, \ref{qcd_phase_hic}, \ref{sec_phases_obs}, and \ref{subsubsec:dlim}.
\item[(ii)]
    The second approach is to parameterize the quantity of interest by polynomials, neural networks or Gaussian Process(GP), $\mathcal{Q}(x)=\mathcal{Q}(x|\boldsymbol{\theta})$, update its parameters according to statistical approach (such as Markov-Chain Monte Carlo within Bayesian Inference) or Heuristic Algorithm (Generic Algorithm), in order to minimize the uncertainty-weighted difference between the target observable and the ones corresponding to $\mathcal{Q}(x|\boldsymbol{\theta})$,
\begin{align}
    \chi^2 = \sum_y \Big(\frac{\mathcal{F}_y[\mathcal{Q}_\text{NN}(x|\boldsymbol{\theta})] - \mathcal{O}_y}{\Delta\mathcal{O}_y} \Big)^2,
    \label{eq:5:chisq}
\end{align}
where $\Delta\mathcal{O}_y$ is the uncertainty of $\mathcal{O}_y$ and $\sum_y \to \int \mathrm{d}y$ for continuous arguments. Then being integrated into the Bayesian formula one can evaluate the posterior distribution for the target. See Sections~\ref{hic_nuclear_structure}, \ref{hic_jet}, and \ref{subsubsec:infer} for typical examples adopted this approach.
\item[(iii)]
    The third approach is similar to the second one, but to update parameters of neural networks according to a gradient-based method,
\begin{align}
    \frac{1}{2}\nabla_{\boldsymbol{\theta}}\chi^2 = \sum_y \frac{\mathcal{F}_y[\mathcal{Q}_\text{NN}(x|\boldsymbol{\theta})] - \mathcal{O}_y}{(\Delta\mathcal{O}_y)^2} 
    \int \mathrm{d}x \frac{\delta\mathcal{F}_y[\mathcal{Q}(x)]}{\delta \mathcal{Q}(x)}\bigg|_{\mathcal{Q}(x)=\mathcal{Q}_\text{NN}(x|\boldsymbol{\theta})} \nabla_{\boldsymbol{\theta}}\mathcal{Q}_\text{NN}(x|\boldsymbol{\theta}),
    \label{eq:5:chisq_grad}
\end{align}
with $\frac{\delta\mathcal{F}_y[\mathcal{Q}(x)]}{\delta \mathcal{Q}(x)}$ being the functional derivative of $\mathcal{F}_y$ with respect to $\mathcal{Q}(x)$. Intuitively, when the chi-square function reaches its minimum, the gradients vanish by definition, $\frac{1}{2}\nabla_{\boldsymbol{\theta}}\chi^2=0$, the parameters stop being updated and achieve the optimal point. This approach also allows the fusion of physical model or simulation into the optimization procedure with differentiable programming strategy. See Sections~\ref{subsubsec:realt}, \ref{subsubsec:ad}, and \ref{sec:5:phy_mod} for examples using this approach, and in the following we will have details explanations for three typical cases.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Compared to Approach(ii), the physics-driven Approach(iii) directs the parameters to the minimum of $\chi^2$, rather than random walks. Therefore, it is more efficient for networks with numerous parameters. To apply the third method, the differentiability of $\mathcal{F}_y[\mathcal{Q}(x)]$ is a prerequisite, as shown in Eq.~\eqref{eq:5:chisq_grad}. In most cases, $\frac{\delta\mathcal{F}_y[\mathcal{Q}(x)]}{\delta \mathcal{Q}(x)}$ can be obtained either through analytical variation analysis or the modern Auto Differentiation framework(e.g., PyTorch, Tensorflow and Paddle); however, in some specific cases, the mapping from $\mathcal{Q}(x)$ to $\mathcal{O}_y$ may be implicit, requiring non-trivial physics knowledge and/or mathematical derivation to compute the functional derivative. In the remainder of this subsection, we provide several examples to highlight the differences between these approaches.

In Sec.~\ref{sec:astro}, we reviewed a few examples of inferring nuclear matter equation of state(EOS) from the neutron star mass-radius relation. In Refs.~\cite{Soma:2022qnv, Soma:2022vbb}, the authors follow Approach (iii) to implement a Neural Network to represent the EOS and unsupervisedly optimize the network parameter in order to fit the observation data, whereas Refs.~\cite{Fujimoto:2017cdo, Fujimoto:2019hxv, Fujimoto:2021zas} supervisedly train a network to represent the inverse mapping from mass-radius observations to the EOS.
The latter fit into Approach (i) and are natural applications of DL. However, one should carefully prepare the training data set to avoid bias. In this particular case, one should also be careful in ensuring that the sequence of different observations does not alter the result EOS. Also, reliable uncertainty quantification is challenging~\cite{Fujimoto:2021zas}. The aforementioned challenges can be addressed through unsupervised training. In this approach, a DNN is used as an unbiased parametrization of the EoS and fed into a Bayesian analysis. This enables the extraction of not only the mean value, but also the associated uncertainty.


\emph{\textbf{Inverse TOV equation}} ---
The success of unsupervised training lies in designing a physics-driven training process that guides the iteration of parameters towards maximizing likelihood. This can be achieved through the use of automatic differentiation (AD) as discussed in Sec.~\ref{subsec:mr}, or through analytical derivation. Despite the highly non-linear nature of the TOV equation~\eqref{eq:4:tov} with respect to the EoS, it is still possible to analyze the linear response of observables out of TOV equations to small changes in the EoS.

For the sake of convenience, we reformulate Eq.~\eqref{eq:4:tov} by changing the independent variable from the radius ($r$) to the logarithmic of pressure, $\ln(P/P_\text{bnd})$. $P_\text{bnd}$ denotes the boundary pressure, and it is small enough such that $M$ and $R$ are insensitive to it. Also, we consider the argument of $v\equiv r^3$ rather than $r$ to avoid numerical divergence. The TOV equation thus becomes,
\begin{align}
\begin{split}
\frac{\mathrm{d}v}{\mathrm{d}\ln \frac{P}{P_\text{bnd}}} =\;&
   - \mathcal{K}_v(P,\varepsilon,v,m)  \,, \\
\frac{\mathrm{d}m}{\mathrm{d}\ln \frac{P}{P_\text{bnd}}} =\;&
   - \mathcal{K}_m(P,\varepsilon,v,m)\,,
\end{split}\label{eq:5:tov}
\end{align}
where 
\begin{align}
\begin{split}
\mathcal{K}_v(P,\varepsilon,v,m) \equiv \;&
    \frac{3(v^{\frac{1}{3}} - 2\,m)}{(m/v+4\pi P)(1+\varepsilon/P)} \,,\\
\mathcal{K}_m(P,\varepsilon,v,m) \equiv \;&
    \frac{4\pi(v^{\frac{1}{3}} - 2\,m)\varepsilon}{(m/v+4\pi P)(1+\varepsilon/P)} \,.
\end{split}
\end{align}
For later convenience, we define $\mathcal{K}_{X,Y} \equiv
\frac{\partial \mathcal{K}_X}{\partial Y}$ for $X\in\{v,m\}$ and $Y\in\{P,\varepsilon,v,m\}$. Given a small, Dirac-$\delta$ function-like perturbation in the EoS, $\varepsilon(P) \to \varepsilon(P) + \lambda\, \delta(P-P')$, the change of $v$ and $m$, denoted as $\Delta v(P|P')$ and $\Delta m(P|P')$, are given by the linear variation of Eq.~\eqref{eq:5:tov},
\begin{align}
\begin{split}
0 =\;& \frac{\mathrm{d}\Delta X(P|P')}{\mathrm{d} \ln  \frac{P}{P_\text{bnd}}}  
      + \mathcal{K}_{X,v} \Delta v(P|P')
      + \mathcal{K}_{X,m} \Delta m(P|P')
      + \lambda\, \mathcal{K}_{X,\varepsilon} \delta(P-P')\,.
\end{split}\label{eq:5:tov_derivative}
\end{align}
It is obvious that both $\Delta v(P|P')$ and $\Delta m(P|P')$ are linearly depending on the perturbation parameter $\lambda$.
By solving Eq.~\eqref{eq:5:tov_derivative} with parameter $\lambda=1$ and argument $\ln \frac{P}{P_\text{bnd}}$ going from $\ln \frac{P_c}{P_\text{bnd}}$ to unity, the functional derivative can be computed as
\begin{align}
\begin{split}
    \frac{\delta M(P_c)}{\delta \varepsilon(P')} =\;& \Delta m(P|P'=P_\text{bnd})\,,\\
    \frac{\delta R(P_c)}{\delta \varepsilon(P')} =\;& \Big(\Delta v(P|P'=P_\text{bnd})\Big)^{1/3}\,.
\end{split}
\end{align}
Consequently, we demonstrate that it is possible to calculate the linear response of a Neutron Star's mass and radius to an infinitesimal perturbation in energy density, thereby supporting the numerical automatic differentiation method outlined in Sec.~\ref{sec:astro}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!hbt]
    \centering
    \includegraphics[width=0.35\textwidth]{fig_5-3-1_potential.pdf}
    \includegraphics[width=0.3\textwidth]{fig_5-3-1_mass.pdf}
    \includegraphics[width=0.3\textwidth]{fig_5-3-1_width.pdf}
    \caption{(Left)Real (blue) and imaginary (red) part of interaction potentials versus temperature and quark-antiquark distance extracted via DNNs.
    (Middle and Right) In-medium mass shifts with respect to the vacuum mass (middle) and the thermal widths (right) of different bottomonium states obtained from fits to lattice QCD results of Ref.~\cite{Larsen:2019zqv} (lines and shaded bands) using weak-coupling motivated functional forms~\cite{Lafferty:2019jpr} (open symbols) and DNN based optimization (solid symbols). The points are shifted horizontally for better visualization. $\Upsilon(1S)$, $\chi_{b_0}(1P)$, $\Upsilon(2S)$, $\chi_{b_0}(2P)$ and $\Upsilon(3S)$ states are represented by red circles, orange pluses, green squares, blue crosses and purple diamonds, respectively. Figures from Ref.~\cite{Shi:2021qri} with permission.}
    \label{fig:5:inverse:schroedinger}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\emph{\textbf{Inverse Schr\"odinger equation}}---
Another example of solving inverse problem with physics-driven learning is the reconstruction of finite-temperature interaction potential from the lattice QCD simulation of Bottomonia masses and thermal widths in the Quark-Gluon Plasma(QGP). 
The QGP is a new state of matter formed in high energy nuclear collisions and is composed of color-deconfined quarks and gluons, which is in contrast to the low-temperature phase where quarks and gluons are confined within hadrons. The suppression of heavy quarkonium production rates, which are the bound states of a heavy quark and its anti-quark, is evidence of QGP formation in high energy nuclear collisions~\cite{Matsui:1986dk}. Due to the very large mass and small velocity of heavy quarks, one is allowed to employ the Schr\"odinger equation with an effective finite-temperature potential and study the properties of bottomonium in medium~\cite{Satz:2005hx,Guo:2012hx},
\begin{equation}
-\frac{\nabla^2}{2m_\mu} \psi_n(r) + V(r) \psi_n(r) = E_n \psi_n(r) \,,
\label{eq:5:schroedinger}
\end{equation}
where the reduced mass is half of the $b$-quark mass, $m_\mu=m_b/2$, $\psi_n$ the relative wavefuntion, and $E_n$ the energy. At finite temperature, the interaction potential becomes complex~\cite{Laine:2006ns},  $V(T,r) = V_R(T,r) + i \cdot V_I(T,r)$, where the imaginary part emerges due to the Landau damping effect and transition between color-singlet and octet and vanishing in the vacuum. Accordingly, the energy eigenvalues are complex, with the real and imaginary parts correspond to the mass, $\mathrm{Re}[E_n] = m-2m_b$, and width, $\mathrm{Im}[E_n] = -\Gamma$, respectively.

In Ref.~\cite{Shi:2021qri}, it was found that the recent lattice QCD calculation of Bottomonia mass and width (Refs.~\cite{Larsen:2019bwy,Larsen:2019zqv,Larsen:2020rjk}) contradicts the weak-coupling motivated functional forms~\cite{Lafferty:2019jpr} (shown as open symbols in Fig.~\ref{fig:5:inverse:schroedinger}). To address this, the authors developed a model-independent approach for extracting the finite-temperature complex potential directly from the lattice QCD calculation. The complex-valued potential, $\mathcal{Q} \equiv {V_R(r,T), V_I(r,T)}$, is the quantity of interest, while the observables, the masses and widths for various bound states at different temperatures, $\mathcal{O}_y \equiv {m_n(T_j), \Gamma_n(T_j)}$, are obtained by solving the eigenvalue problem of a differential equation. This makes the application of general techniques in physics-driven deep learning challenging in computing the parameter gradients in Eq.~\eqref{eq:5:chisq_grad}.

To compute the functional derivative, one needs to know how the complex-valued energy eigenvalues response to an arbitrary perturbation in potential. Since we only consider small perturbations, the changes of energy eigenvalues are given by the Hellmann--Feynmann theorem in quantum mechanics, $\delta E_n = \int |\psi_n(r)|^2 \delta V(r) \mathrm{d}r$. In Ref.~\cite{Shi:2021qri}, the temperature and distance dependence of the real and imaginary potentials are expressed by the DNNs,
\begin{align}
    V_R(r,T) = V_{R,\mathrm{DNN}}(r,T|\boldsymbol{\theta}_R),
\quad
    V_I(r,T) = V_{R,\mathrm{DNN}}(r,T|\boldsymbol{\theta}_I),
\end{align}
with the network parameters being trained according to the parameter gradients~\eqref{eq:5:chisq_grad} with functional derivatives,
\begin{align}
\begin{split}
 \frac{\delta m_n}{\delta V_R(r)} =&\; -\frac{\delta \Gamma_n}{\delta V_I(r)} = |\psi_n(r)|^2 \,, \\
 \frac{\delta m_n}{\delta V_I(r)} =&\; \frac{\delta \Gamma_n}{\delta V_R(r)} = 0\,.
\end{split}
\end{align}

The wavefunctions $\psi_n(r)$ are obtained by solving the eigenvalue problem. The validity of this method has been confirmed through a closure test with known potentials. The authors began by using a given formula for complex-valued potentials and solving the Schr\"odinger equation at six different temperatures to generate a set of pseudo-data, which consists of the mass and width for five eigenstates. This choice of pseudo-data was consistent with the data obtained from lattice QCD calculations~\cite{Larsen:2019zqv}. The set of pseudo-data was then used to train the parameters of a DNN, and the resulting $V_{R,\mathrm{DNN}}$ and $V_{I,\mathrm{DNN}}$ were found to be in consistent with the ground-truth. The validated method is then applied to the masses and thermal widths computed from lattice QCD calculation. With the optimized parameter sets for $\boldsymbol{\theta}_R$ and $\boldsymbol{\theta}_I$, the real and imaginary potentials are shown in Fig.~\ref{fig:5:inverse:schroedinger} (left), and the corresponding mass shifts and thermal widths are shown by the solid symbols in Fig.~\ref{fig:5:inverse:schroedinger} (left and middle).


\emph{\textbf{Tackling ill-posed inverse problems}} --- In practice, when solving ill-posed inverse problems, one must introduce additional rules to better control the input function and eliminate degeneracy. This is demonstrated through the example of reconstructing spectral functions, which cannot be directly computed in non-perturbative Monte Carlo calculations such as lattice QCD. Instead, they must be inferred from limited sets of correlation data~\cite{Asakawa:2000tr}. The K"allen--Lehmann (KL) correlation functions are among the commonly studied observables,
\begin{align}
D(k) = \;& 
    \int_0^\infty \frac{1}{\pi}
    \frac{\omega \,\mathrm{d}\omega}{\omega^2 + k^2} 
    \rho(\omega),
    \label{eq:5:corr_D}
\end{align}
which is a linear transformation mapping a continuous real function to another continuous real function, and the arguments of both the input and output functions are defined in the real axis. Ref.~\cite{Shi:2022yqw} analytically solved the eigenvalue problem and found the eigenvalues to be $1/(2\cosh\frac{\pi s}{2})$. Here, $s \in \mathbb{R}$ is a continuous parameter that labels the eigenstates. The eigenvalues can be arbitrarily close to zero for large $s$, and the eigenvalues of the inverse operation, which are the inverses of those of the KL convolution, can be arbitrarily large. In the inverse operation, a small noise in the realistic numerical calculation of $D$ will be magnified to a large deviation in $\rho$ as shown in Fig.~\ref{fig:spectral_samples} of Sec.~\ref{subsubsec:realt}. Therefore, inverse KL convolution is ill-posed.

Common methods for reconstructing spectral functions include the Tikhonov (TK) regulator~\cite{Tikhonov1943OnTS,tikhonov1995numerical}, the Maximum Entropy Method (MEM) that employs the Shannon--Jaynes entropy~\cite{Narayan:1986wj,Jarrell:1996rrw}, and the Bayesian Reconstruction (BR) method~\cite{Burnier:2013nla}. In references~\cite{Wang:2021jou,Wang:2021cqw,Shi:2022yqw}, the spectral function $\rho(\omega)$ is either formulated as a one-dimensional input and one-dimensional output deep neural network (DNN) that approximates the function (\texttt{NN-P2P} in Figure~\ref{fig:5:inverse:ad}), or as a unity-input and $N_\omega$-dimensional output DNN that represents the value of $\rho$ at $N_\omega$ points in $\omega$ (\texttt{NN} in Figure~\ref{fig:5:inverse:ad}). It has been shown~\cite{Shi:2022yqw} that both representations provide non-local regulators and lead to a unique solution of $\rho(\omega)$. To optimize the parameters of network representations $\{\boldsymbol{\theta}\}$ with loss function, the authors implemented gradient-based algorithms. It derives as,
\begin{align}
    \nabla_{\boldsymbol{\theta}} \mathcal{L} =
    \sum_{j,i}
    K(k_j,\omega_i)
    \frac{\partial \mathcal{L}}{\partial D(k_j)}
    \nabla_{\boldsymbol{\theta}} \rho_i,
\end{align}
where $K(k,\omega) = \frac{1}{\pi}\frac{\omega}{\omega^2 + k^2}$, and $\nabla_{\boldsymbol{\theta}} \rho_i$ is computed by the standard back-propagation  method The reconstruction error is propagated to parameters of the neural network, combined with gradients derived from automatic differentiation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!hbt]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/fig_5-3-2_spectrum.pdf}
    \caption{Automatic differential framework to reconstruct spectral from observations. (a) \texttt{NN}. Neural networks have outputs as a list representation of spectrum $\rho_i(\omega_i)$. (b) \texttt{NN-P2P}. Neural networks have input and output nodes as $(\omega_i,\rho_i)$ pairwise.}
    \label{fig:5:inverse:ad}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As a numerical demonstration, the spectral function formed with two Breit--Wigner peaks is choesen as the ground truth,
\begin{align}\label{eq.breit_wigner}
    \rho(\omega) = \sum_{n=1}^{2} \frac{4 A_n \Gamma_n \omega}{\left(M_n^{2}+\Gamma_n^{2}-\omega^{2}\right)^{2}+4 \Gamma_n^{2} \omega^{2}},
\end{align}
with $A_1= 0.8$, $A_2 = 1.0$, $\Gamma_1 = \Gamma_2 = 0.5$~GeV, $M_1= 2.0$~GeV, $M_2 =5.0$~GeV, and compute the corresponding KL correlation functions $D(k_i)$ at $k_i = i\times\Delta k$, with $i=1,2,\cdots,100$, and  $\Delta k=0.2$~GeV. To investigate the effects of noise in a realistic situation, mock data were prepared with random noise on the correlation function, i.e. $\mathrm{D}_i^\text{noisy} = D(k_i) + n_{i}$. See Refs.~\cite{Asakawa:2000tr,Shi:2022yqw} for the detailed setup. Then, for points $\omega_a = a\times \Delta\omega$, with $a=1, 2, \cdots, 500$, and $\Delta\omega=0.04$~GeV, the spectral functions $\rho(\omega)$ were reconstructed. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!hbtp]\centering
\includegraphics[width=0.32\textwidth]{figures/fig_5-3-3_eigen_real.pdf}
\includegraphics[width=0.32\textwidth]{figures/fig_5-3-3_eigen_cos.pdf}
\includegraphics[width=0.32\textwidth]{figures/fig_5-3-3_eigen_sin.pdf}
\caption{Spectral functions using different reconstruction methods (upper panels) and their corresponding KL correlation functions (lower panels) in the generalized coordinate space (left) and generalized momentum space (middle and right). 
Black curves are for the ground truth using Breit--Wigner spectral functions. Numerically reconstructed functions using \texttt{NN}, \texttt{NN-P2P}, and MEM using $N_\text{basis}=100$ basis are represented by red, blue, and green curves, respectively. Figures reproduced from Ref.~\cite{Wang:2021cqw, Shi:2022yqw} with permission.\label{fig:5:inverse:comp}}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In Figure~\ref{fig:5:inverse:comp}, the spectral functions and their respective correlation functions in both the generalized coordinate and momentum spaces are displayed for three methods: \texttt{NN} and \texttt{NN-P2P} representations, and the widely used Maximum Entropy Method (MEM). To ensure the stability of the MEM results, the number of basis functions, $N_\text{basis}=N_k$, has been set to 100. The \texttt{NN} architecture includes three hidden layers with a width of 64, with the input layer consisting of a single constant node set to unity. The output layer contains $N_\omega$ nodes. For the \texttt{NN-P2P} architecture, the input and hidden layers are unchanged, but the output layer has only one node. All activation functions before the output have been selected as \texttt{ELU}. Although the behavior of $\rho(\omega)$ may vary significantly in the generalized coordinate space, the generalized momentum $\widetilde{\rho}_\pm(s)$ exhibits similar behavior for values of $s$ less than or equal to three, as indicated by the vertical dashed lines in Fig.~\ref{fig:5:inverse:comp}. This highlights the fundamental challenge in the reconstruction problem. Regardless of the reconstruction method used, one can always accurately recover the low-frequency modes of $\widetilde{\rho}$, but the high-frequency modes are prone to being polluted by noise or numerical inaccuracies in the correlation functions, making them nearly impossible to achieve. On the other hand, all $D$ and $\widetilde{D}$ values are nearly equal, which is ensured through the $\chi^2$-fitting.

Thus, it should be noted that all these methods are not guaranteed to provide the correct inversion function when $D$ is of finite precision. Meanwhile, the ``biases'' introduced in the supervised training data set become helpful in solving ill-posed inverse problems if unwanted biases are carefully avoided. A natural way to implement the physical regulators can be provided by Approach (i). This is demonstrated in studies that employ DNNs to learn the inverse mapping, such as~\cite{Kades:2019wtd, 2018PhRvB..98x5101Y, 2020PhRvL.124e6401F, PhysRevLett.124.056401, Chen:2021giw}. In these works, an ensemble of $\rho$'s that follow the physical properties is generated and the corresponding $D$'s are computed. DNNs are then trained to represent the inverse mapping from $D$ to $\rho$. The trained networks become automatically regularized inverse functions with embedded prior knowledge, but one should be careful about the risk of introducing unwanted biases in the training data.

\subsection{Summary}
In this section, we reviewed recent advancements in embedding more physical prior knowledge into machine learning methods to enhance the efficiency and applicability of physics exploration. We illustrated these physics-inspired, informed, and driven deep learning approaches with concrete examples of lattice field configuration sampling, inverse renormalization group transformation, Feynman path generation, and the network realization of AdS-CFT calculation. Additionally, we presented complex inverse problems such as nuclear equation of state reconstruction from neutron star masses and radii, interaction potential extraction from the Schr\"odinger equation energy spectrum, and spectral function reconstruction from Euclidean correlators. The examples presented in this section demonstrated significant advantages of using these advanced developments in ML/DL for solving physics problems. We hope that these examples will inspire further work in the near future.