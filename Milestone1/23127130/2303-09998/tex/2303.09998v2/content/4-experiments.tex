
\vspace{-3mm}
\section{Experiments}



% Please add the following required packages to your document preamble:
% \usepackage{multirow}


% \begin{table*}[htbp]
% \centering
% \begin{tabular}{c|c|cc|cc|c}
% \toprule
%   \multirow{ 2}{*}{Method}  & RGB & \multicolumn{2}{c|}{Future semantic seg. }  & \multicolumn{2}{c}{Future instance seg.} \\
%   &  Resolution & short (IoU) & long (IoU) &  
%  VPQ (short) & VPQ (long) \\
% \midrule
% FIERY\cite{fiery}      & 224$\times$480          & 59.4      & 36.7     & 50.2      & 29.9     \\
% StretchBEV \cite{stretchbev} & 224$\times$480          & 55.5      & 37.1     & 46.0      & 29.0     \\
% ST-P3\cite{stp3}      & 224$\times$480          & -         & 38.9     & -         & 32.1     \\
% BEVerse \cite{beverse}   & 256$\times$704          & 60.3      & 38.7     & 52.2      & 33.3     \\
% \midrule
% Ours       & 224$\times$480          & \textbf{64.7}      & \textbf{41.9}     & \textbf{56.7}      & \textbf{36.9}   \\
% \bottomrule
% \end{tabular}
% \caption{\textbf{Predcition results on nuScenes\cite{caesar2020nuscenes} validation set.} Intersection-over-Union (IoU) is used for future semantic segmentation and Video Panoptic Quality (VPQ) for future instance segmentation. Results are reported under two settings: short ($30m \times 30m$) range and long ($100m \times 100m$) range.}
% \label{tab:prediction_result}
% \end{table*}




% \begin{table}[htbp]
% \centering
% \begin{tabular}{c|c|c|ccc}
% \toprule
% Temporal model & fps & IoU                & VPQ  & VRQ  & VSQ  \\
% \midrule
% $\text{MotionNet}^\dag$  \cite{wu2020motionnet}     &     & 35.4 &     30.6     &    43.1      &   71.1       \\
% $\text{FIERY}^\dag$ \cite{fiery}         &     & 38.3 &      32.1    &     45.4     & 70.7         \\
% $\text{BEVerse}^\dag$  \cite{beverse}      &     & 40.2 &        34.0  &     48.0     &    70.9      \\
% \midrule
% Ours           &     & \textbf{41.9} &      \textbf{36.9}    &     \textbf{51.5}     &   \textbf{72.6}      \\
% \bottomrule
% \end{tabular}
% \caption{\textbf{Ablation for prediction model.} $\dag$: We use MotionNet\cite{wu2020motionnet}, FIERY\cite{fiery} and BEVerse\cite{beverse} to replace our prediction model for comparison, and the BEV encoder and task heads are the same.}
% \label{tab:temporal_ablation}
% \end{table}

% \begin{table}[htbp]
% \centering
% \begin{tabular}{c|c|c|ccc}
% \toprule
% Temporal model & fps & IoU                & VPQ   \\
% \midrule
% Ours + MotionNet \cite{wu2020motionnet}     &     & 35.4 &     30.6     &         \\
% Ours + FIERY \cite{fiery}         &     & 38.3 &      32.1    &            \\
% Ours + BEVerse  \cite{beverse}      &     & 38.8 &        32.4  &        \\
% \midrule
% Ours           &     & \textbf{41.9} &      \textbf{36.9}    &    \\
% \bottomrule
% \end{tabular}
% \caption{Temporal model ablation. IoU, vpq  long}
% \label{tab:temporal_ablation}
% \end{table}



% \begin{table*}[htbp]
% \centering
% \begin{tabular}{cccc|cc|cc}
% \toprule
% \multirow{2}{*}{Warp Sync} & \multirow{2}{*}{Pose Sync} & \multirow{2}{*}{Time Query} & \multirow{2}{*}{HDmap Enhanced} &  \multicolumn{2}{c|}{Future semantic seg. }  & \multicolumn{2}{c}{Future instance seg.}            \\ 
% % \cline{5-8} 
% &       &       &       & \multicolumn{1}{c|}{short} & long & \multicolumn{1}{c|}{short} & long  \\ \midrule
% \checkmark &    &    &    & \multicolumn{1}{c|}{58.7}  & 38.4 & \multicolumn{1}{c|}{50.6}  & 31.8  \\
% \checkmark  & & \checkmark   &     & \multicolumn{1}{c|}{60.3}  & 38.6 & \multicolumn{1}{c|}{52.2}  & 33.4  \\
% \checkmark  &  & \checkmark  & \checkmark  & \multicolumn{1}{c|}{62.0}  & 40.7 & \multicolumn{1}{c|}{53.2} & 34.3 \\
% & \checkmark    &   &   & \multicolumn{1}{c|}{63.1}  & 40.4 & \multicolumn{1}{c|}{54.6}  & 35.3  \\
% & \checkmark  & \checkmark   &  & \multicolumn{1}{c|}{63.8}  & 41.1 & \multicolumn{1}{c|}{55.8}  & 35.8  \\
% & \checkmark  & \checkmark  & \checkmark    & \multicolumn{1}{c|}{\textbf{64.7}}  & \textbf{41.9} & \multicolumn{1}{c|}{\textbf{56.7}}  & \textbf{36.9}  \\ \bottomrule
% \end{tabular}
% \caption{Module ablation. IoU, vpq}
% \label{tab:module_ablation}
% \end{table*}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}

\begin{table*}[ht]
\centering
\begin{tabular}{c|cccc|cc|cc}
\toprule
 &  &  & &  & \multicolumn{2}{c|}{Future semantic seg. }  & \multicolumn{2}{c}{Future instance seg.}  \\
Exp. & Warp. & Sync. & SLQ & SPE &Short (IoU) &  Long (IoU) &  Short (VPQ) &  Long (VPQ) \\
\hline
\hline
% 1   & \checkmark                 &                 &                                                             &                                                                  & 60.0              & 38.5             & 50.9              & 32.0             \\
% 2   & \checkmark                 &                 & \checkmark                                                           &                                                                  & 60.3              & 38.6             & 52.2              & 33.4             \\
1   & \checkmark                 &                 &                                                             &                                                                  & 58.7              & 38.4             & 50.6              & 31.8             \\
2   & \checkmark                 &                 & \checkmark                                                           &                                                                  & 60.8              & 38.6             & 52.4              & 33.4             \\
3   &\checkmark                 &                 & \checkmark                                                           & \checkmark                                                                & 62.0              & 40.7             & 53.2              & 34.3             \\
\hline
\hline
4   &                   &\checkmark               &                                                             &                                                                  & 63.0              & 40.8             & 54.1              & 34.3              \\
5   &                   & \checkmark               & \checkmark                                                          &                                                                  & 63.8              & 41.1              & 55.8              & 35.8              \\
6   &                   &\checkmark               & \checkmark                                                           & \checkmark                                                               & \textbf{64.7}              & \textbf{41.9}             & \textbf{56.7}              & \textbf{36.9}             \\
\bottomrule
\end{tabular}
\caption{\textbf{Ablation of our proposed architecture.} Ablation results for our PoseSync BEV Encoder (Sync.), the learnable future queries, and the spatial embedding are presented. Exp. 1-3 use the traditional warping methods to align temporal BEV features. Separate learnable queries (SLQ) represent using separate learnable future queries instead of utilizing the same query with temporal positional encoding. Spatial positional embedding (SPE) represents using spatial scene representations in future prediction queries.}
\vspace{-4mm}
\label{tab:module_ablation}
\end{table*}

% \begin{table*}[ht]
% \centering
% \begin{tabular}{c|ccc|cc|cc}
% \toprule
%  &  &  &  & \multicolumn{2}{c|}{Future semantic seg. }  & \multicolumn{2}{c}{Future instance seg.}  \\
% Exp. & Warp. & Sync. & SPE &Short (IoU) &  Long (IoU) &  Short (VPQ) &  Long (VPQ) \\
% \hline
% \hline
% 1   & \checkmark                 &                                                                          &                                                                  & 60.3              & 38.6             & 52.2              & 33.4             \\
% 2   &\checkmark                 &                                                                        & \checkmark                                                                & 62.0              & 40.7             & 53.2              & 34.3             \\
% \hline
% \hline
% 3   &                   & \checkmark                                                                        &                                                                  & 63.8              & 41.1              & 55.8              & 35.8              \\
% 4   &                   &\checkmark                                                                       & \checkmark                                                               & \textbf{64.7}              & \textbf{41.9}             & \textbf{56.7}              & \textbf{36.9}             \\
% \bottomrule
% \end{tabular}
% \caption{\textbf{Ablation of our proposed architecture.} Ablation results for our PoseSync BEV Encoder (Sync.) and the spatial embedding are presented. Exp. 1 \& 2  use the traditional warping methods to align temporal BEV features. Spatial positional embedding (SPE) represents utilizing spatial scene representations in future prediction queries.}
% \vspace{-2mm}
% \label{tab:module_ablation}
% \end{table*}



\begin{table}[ht]
\centering
\begin{tabular}{c|c|ccc}
\toprule
Temporal model                                                   & IoU                            & VPQ                            & VRQ                            & VSQ                            \\ \hline
\hline
$\text{MotionNet}^{\dag}$~\cite{wu2020motionnet}   & 35.4                           & 30.6                           & 43.1                           & 71.1                           \\
$\text{FIERY}^{\dag}$~\cite{fiery}               & 38.3                           & 32.1                           & 45.4                           & 70.7                           \\
$\text{BEVerse}^{\dag}$~\cite{beverse}           & 40.2                           & 34.0                           & 48.0                           & 70.9                           \\ \hline
TBP-Former                                                             & \textbf{41.9} & \textbf{36.9} & \textbf{51.5} & \textbf{72.6} \\ \hline
\end{tabular}
\caption{\textbf{Ablation for the prediction model.} $\dag$: We use MotionNet, FIERY and BEVerse to replace our prediction model for comparison, and the BEV encoder and task heads are the same. Besides IoU and VPQ, we also use Video Recognition Quality (VRQ) and Video Segmentation quality (VSQ) for evaluation. }
\label{tab:temporal_ablation}
\vspace{-1mm}
\end{table}



\begin{table}[ht]
\centering
\begin{tabular}{cc|cc|cc}
\toprule
% \multirow{2}{*}{Cam aug} & \multirow{2}{*}{BEV aug}
\multicolumn{2}{c|}{Augmentation} & \multicolumn{2}{c|}{Perception}                   & \multicolumn{2}{c}{Prediction}           \\ 
% \cline{3-6} 
  Cam  &  BEV       & \multicolumn{1}{c}{Veh.} & Ped. & \multicolumn{1}{c}{IoU} & VPQ\\ 
\hline
\hline
&      & \multicolumn{1}{c}{45.0}  & 17.7 & \multicolumn{1}{c}{40.5}     & 34.4     \\
\checkmark  &   & \multicolumn{1}{c}{44.8} & 18.5           & \multicolumn{1}{c}{40.9}     & 35.3     \\
& \checkmark   & \multicolumn{1}{c}{45.3} & 18.6 & \multicolumn{1}{c}{41.4}     & 35.6     \\
\checkmark  & \checkmark & \multicolumn{1}{c}{\textbf{46.2}}  & \textbf{18.6}       & \multicolumn{1}{c}{\textbf{41.9}}  & \textbf{36.9} \\ \bottomrule
\end{tabular}
\caption{\textbf{Ablation for data augmentation strategies.} Perception of Vehicles and Pedestrians with different data augmentation strategies are evaluated on segmentation IoU. Prediction results are evaluated on segmentation IoU and Video Panoptic Quality. }
\label{tab:aug_ablation}
    \vspace{-4mm}
\end{table}


\subsection{Dataset and settings}
We use nuScenes~\cite{caesar2020nuscenes} datasets to evaluate our approach. NuScenes contains 1000 scenes, each of which has 20 seconds annotated at 2Hz. In nuScenes, the images are captured by 6 cameras with a small overlap in the field of view, which guarantees the cameras cover the full 360Â° field of view. For model input, raw camera images with the size of $900 \times 1600$ are resized and cropped to a resolution of $224 \times 480$. We follow the training and evaluating settings used in previous methods~\cite{fiery, stp3, stretchbev, beverse} for fair comparisons, which use 1.0 second past states and current state to predict 2.0 seconds of the future states. It corresponds to predicting 4 future frames based on 3 observed frames. The 
size of the generated BEV grid map is $200 \times 200$. Each grid has a range of $0.5m \times 0.5m$, which means the perception and prediction range is $100m \times 100m$.

For training, we use AdamW~\cite{loshchilov2017decoupled} with a weight decay $0.01$ to optimize the models. The learning rate is initialized as $10^{-4}$ and decays with a cosine annealing scheduler~\cite{loshchilov2016sgdr}. All models are trained on 4 NVIDIA A100 GPUs for $10$ epochs. 
% Compared to many recent works, we use much fewer computational resources.

\subsection{Metrics}
Following previous works~\cite{fiery, beverse, stp3, stretchbev}, we mainly use two metrics for evaluation. 
The first is Intersection over Union (IoU), which measures the quality of segmentation at each frame. The second is Video Panoptic Quality (VPQ), which is used to measure the consistency of the detected instances over time and the accuracy of the segmentation. 
The formula is shown below:
\begin{equation*}
 \vspace{-3mm}
\begin{aligned}
\textrm{VPQ}=\sum^{H}_{t=0} \frac{\sum_{(p_t, q_t) \in TP_t}\textrm{IoU}(p_t, q_t)}{\left| TP_t \right| + \frac{1}{2} \left| FP_t \right| + \frac{1}{2} \left| FN_t \right|}
\end{aligned}
 % \vspace{-1mm}
\end{equation*}
 % \vspace{-2mm}
where $H$ is the sequence length, $TP_t$ represents the set of true positives, $FP_t$ represents the set of false positives and $FN_t$ represents the set of false negtives at timestamp $t$.


\vspace{-1mm}
\subsection{PnP results}
\vspace{-1mm}

\textbf{Perception and Prediction.}
Table~\ref{tab:prediction_result} compares TBP-Former with other methods of perception and prediction task based on multi-view cameras. We see that \textrm{i}) we achieve state-of-the-art performance and exceed previous methods by a large margin. \textrm{ii}) Even though BEVerse has larger RGB resolutions, TBP-Former still surpasses their performance on IoU by \textbf{7.3\%/8.3\%} for short/long settings, respectively. TBP-Former also improves the VPQ by \textbf{12.1\%/10.8\%}. \textrm{iii}) Apart from the performance improvement, TBF-Former also has a larger FPS compared to other methods. Its inference speed is 25\% faster than BEVerse's.

% We improve more in the short distance setting. The reason is that distant objects are small in the raw images and difficult to extract features from. 


Fig.~\ref{fig:result} shows the visualization results of our proposed method. We see that \textrm{i}) almost all the objects are detected correctly except for those occluded ones. \textrm{ii}) TBP-Former is capable of capturing the motion information in past frames and precisely predicting the vehicles' trajectories by occupancy and flow. Compared with FIERY~\cite{fiery}, TBP-Former is closer to the ground truth. \textrm{iii}) TBP-Former does a better job than FIERY when predicting vehicles' turning.


% \begin{table}[tbh!]
% \centering
% \begin{tabular}{c|c|cc}
% \toprule
% Method    & Temp. & Veh. IoU                      & Ped. IoU \\
% \hline
% \hline
% VED    \cite{ved}                      &        &  23.3 & 11.9      \\
% VPN  \cite{vpn}                        &         & 28.2 & 10.3      \\
% PON    \cite{pon}                      &         &  27.9 & 13.9      \\
% LSS    \cite{lift-splat-shoot}                      &         &  34.6 & 15.0      \\
% CVT  \cite{cvt}                        &         & 36.0                         & -          \\
% Image2Map \cite{saha2022translating}  &         & 40.2                         & -          \\
% BEVFormer \cite{bevformer}                    &         & 44.4                         & -          \\
% IVMP    \cite{wang2021learning}                     & \checkmark        & 36.8                        & 17.4      \\
% FIERY  \cite{fiery}         & \checkmark        & 38.2                         & 17.2      \\
% ST-P3      \cite{stp3}                  & \checkmark        & 40.1                        & 14.5      \\
% % *BEVerse                     & 1        &      44.46                        &   \textcolor{red}{todo}          \\
% \midrule
% TBP-Former \textit{static}            &         & 44.8                        & 17.2      \\
% TBP-Former    & \checkmark        & \textbf{46.2}                        & \textbf{18.6}     \\
% \bottomrule
% \end{tabular}
% \caption{\textbf{Perception results on nuScenes\cite{caesar2020nuscenes} validation set.} Results of vehicles and pedestrians are compared by segmentation IoU. Temp.: whether temporal information is involved. }
% \label{tab:perception_result}
% \end{table}






\textbf{Perception Only.} 
Table~\ref{tab:perception_result} compares the results of plenty of state-of-the-art methods on perception (segmentation) task.  We see that our static model, which does not contain temporal information, can achieve 44.8 and 17.2 IoU of vehicles and pedestrians. With the input of temporal sequences, the performance improves further since auxiliary information is provided for better perception. The state-of-the-art results prove the effectiveness of the novel design of our Pose-synchronized BEV encoder.


% They are state-of-the-art results as a consequence of the novel design of our Pose-synchronized BEV encoder. With the input of temporal sequences, the performance improves further since auxiliary information is provided for better perception. 
% \begin{table}[]
% \centering
% \footnotesize
% \begin{tabular}{c|ccccc}
% \toprule
% Method                       & Temporal & Vehicle & Pedestrian & Drivable & Lane    \\
% \midrule
% VED                          & 0        & 23.28       & 11.93      & 60.82    & 16.74   \\
% VPN                          & 0        & 28.17       & 10.26      & 65.97    & 17.05   \\
% PON                          & 0        & 27.91       & 13.93      & 63.05    & 17.19   \\
% LSS                          & 0        & 34.61       & 15.02      & 72.23    & 19.98   \\
% IVMP                         & 0        & 36.76       & 17.38      & 74.70    & 20.94   \\
% Translating & 0        & 40.2        &            & 80.5     &         \\
% FIERY         & 1        & 38.2        & 17.15      & 71.97    & 33.58   \\
% ST-P3                        & 1        & 40.10       & 14.48      & 75.97    & 40.2    \\
% BEVFormer                    & 0        & 44.4        & -          & 77.6     & -(19.8) \\
% BEVFormer-T                  & 1        & 46.7        & -          & 77.5     & -(23.9) \\
% \midrule
% Ours                         & 0        & 44.91       & 17.24      & 73.77    & 37.08   \\
% Ours-T                       & 1        & TBD         &            &          &        

% \bottomrule
% \end{tabular}
% \vspace{-3mm}
% \caption{Perception results. Segmentation IoU}
% \label{tab:perception_result}
% \vspace{-3mm}
% \end{table}


\begin{figure*}[t]
    \centering
    \includegraphics[scale=0.60]{content/figure/result.pdf}
    \caption{Demonstration of our results compared with FIERY and Ground Truth. 
    % From left to right, this figure shows input images, results of FIERY, results of ours and ground truth. 
    Different vehicles are assigned with different colors in order to make a distinction. The darker parts represent the perception of the current frame, and the lighter parts represent the prediction of the vehicles in future frames. The visualization is based on the predicted occupancy and flow. }
    \label{fig:result}
    \vspace{-3mm}
\end{figure*} 

\vspace{-2mm}
\subsection{Ablation}
\vspace{-1mm}

\textbf{Effectiveness of PoseSync View Projection.} 
The Exp.~1\&4,~2\&5,~3\&6 in Table~\ref{tab:module_ablation} compare the proposed PoseSync View Projection and the existing feature warping methods. We see that the proposed method always achieves better performance when other settings remain the same. The reasons are that: \textrm{i}) PoseSync View Projection based on Deformable Attention can guarantee the precise correspondence between BEV grids and image features. \textrm{ii}) Our projection method can alleviate distortion and our-of-range issues when synchronizing sequential BEV features. 

% \textbf{Effectiveness of spatial and temporal priors for queries.} Exp.~2\&3\&5\&6 in Table~\ref{tab:module_ablation} validate the effectiveness of two priors for future queries. We see that both priors facilitate the temporal model to learn multi-scale spatial and temporal information. The reasons are that: \textrm{i}) the predicted high dimensional map features provide the model with useful geographic information. It allows the future BEV queries to perceive their surroundings and scenes better. \textrm{ii}) Positional embedding along the time dimension provides the temporal ordering information. 


\textbf{Effectiveness of the designed future queries.} 
In Exp.~1\&3 in Table~\ref{tab:module_ablation}, we utilize the identical query with temporal positional encoding for future queries. Exp.~2\&4 in Table~\ref{tab:module_ablation} demonstrate that using separate learnable embedding for future queries can achieve better performance.
Exp.~3\&6 in Table~\ref{tab:module_ablation} validate the efficacy of the proposed spatial priors for future queries. The generated high-dimensional map features provide the prediction model with useful geographic information. The additional spatial information can aid the prediction and lead to better scene forecasting. 


\textbf{Effectiveness of STPT.} Table~\ref{tab:temporal_ablation} compares STPT with popular CNN-based~\cite{wu2020motionnet} and RNN-based~\cite{fiery, beverse} methods. We keep all the settings the same except for temporal modeling. To be specific, the size of input images ($224 \times 480$), image backbones and BEV feature extractor are the same. And then we plug their temporal models into our architecture. We see that \textrm{i}) STPT model performs better in all four metrics, including semantic segmentation IoU and three instance segmentation metrics from the video prediction area.  \textrm{ii}) Our reproduced temporal models achieve higher performance than the original implements. This further validates the effectiveness and power of our BEV feature extractor.


\textbf{Data Augmentation.} We perform both image-view and BEV augmentations. The image-view augmentations include random scaling, rotation and flip of the input images. The BEV augmentations include similar operations on both BEV representations and corresponding ground truth labels. Table~\ref{tab:aug_ablation} compares the results of different data augmentation strategies. We see that \textrm{i}) both augmentation methods improve the performance when used separately. \textrm{ii}) The combination of two methods works better than any single approach. Introducing data augmentation strategies is beneficial to the model's robustness and generalization ability.
% Hence introducing data augmentation strategies can effectively expand the data volume, which is beneficial to the model's robustness and generalization ability.

 % \vspace{-1mm}


% \begin{table*}[]
% \centering
% \begin{tabular}{cccc|cccc}
% \toprule
% Warp Sync & Pose Sync & Time Query & Hdmap enhanced & IoU short & IoU long & VPQ short & VPQ long \\
% \midrule
% \checkmark         &           &            &                & 58.7     & 38.4    & 50.6     & 31.8    \\
% \checkmark         &           & \checkmark          &                & 59.5      & 38.9    & 51.7     & 32.5    \\
% \checkmark         &           & \checkmark          & \checkmark              &           &          &           &          \\
%           & \checkmark         &            &                &           &          &           &          \\
%           & \checkmark         & \checkmark          &                & 63.8     & 41.1     & 55.8     & 35.8     \\
%           & \checkmark         & \checkmark          & \checkmark              & 64.7     & 41.9    & 56.7     & 36.9    \\
% \bottomrule
% \end{tabular}
% \caption{Module ablation. IoU, vpq}
% \label{tab:module_ablation}
% \end{table*}


% \begin{table*}[]
% \centering
% \begin{tabular}{cc|cc|cc}
% \toprule
% Cam aug & BEV aug & Vehicle iou             & Pedestrian & IoU long & VPQ long \\
% \midrule
%         &         & &            &          &          \\
% \checkmark       &         &  &            &          &          \\
%         & \checkmark       &  &            &          &          \\
% \checkmark       & \checkmark       &  &            &          &       \\
% \bottomrule
% \end{tabular}
% \caption{Augmentation ablation. IoU, vpq}
% \label{tab:aug_ablation}
% \end{table*}


% \begin{figure*}[t]
%     \centering
%     \includegraphics[scale=0.6]{content/figure/result.pdf}
%     \caption{Demonstration of our results compared with FIERY and Ground Truth. From left to right, this figure shows input images, results of FIERY, results of ours and ground truth. Different vehicles are assigned with different colors in order to make a distinction. The darker part represents the perception of the current frame, and the lighter part represents the prediction of the vehicles in future frames. The visualization is based on the predicted occupancy and flow. }
%     \label{fig:result}
% \end{figure*} 