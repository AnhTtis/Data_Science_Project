\begin{figure*}[ht]
    \centering
    \includegraphics[scale=0.27]{content/figure/overview_new.png}
    \caption{An overview of TBP-Former architecture. Taking consecutive surrounding camera images as inputs, TBP-Former first generates image-space features and uses the PoseSync BEV Encoder to map front-view features to BEV features in a shared and synchronized BEV space. Then the BEV features from multiple frames are processed by the Spatial-Temporal Pyramid Transformer to extract BEV spatial-temporal features and predict future BEV states in order. In this process, high-level scene representations are generated from the last frame BEV feature as spatial priors to guide the prediction. Finally, the well-predicted future states are sent to decoder heads for joint perception and prediction tasks.}
    \label{fig:overview}
     \vspace{-4mm}
\end{figure*} 

\vspace{-2mm}
\section{Related Work}


\subsection{Joint Perception and Prediction}

As the two core system modules of autonomous driving, how to conduct perception and prediction tasks jointly has received a lot of attention. Traditional approaches ~\cite{casas2020spagnn, casas2018intentnet, liang2020pnpnet, luo2018faf, phillips2021deep} formulate this joint task as a trajectory prediction problem that relies on the perception outputs of 3D object detection and tracking. The dependency on intermediate results tends to accumulate errors and lacks the capacity to perceive unknown objects~\cite{wu2020motionnet, wong2020identifying}. 
Subsequently, instance-free methods~\cite{wu2020motionnet, lee2020pillarflow, luo2021pillarmotion, filatov2020any, schreiber2021dynamic, sadat2020perceive} that predict dense future semantic occupancy and flow has become a growing trend to simplify the understanding of dynamic scenes. 
Also, several recent works~\cite{beverse, stp3, fiery} explore joint perception and prediction in the form of dense occupancy and flow using only surrounding camera input. 

In many previous works~\cite{phillips2021deep, sadat2020perceive, casas2018intentnet, liang2020pnpnet}, raster HD (high-definition) maps play an important role as input of the frameworks. HD maps can provide strong priors to guide the predicted results to follow the traffic lanes. However, in practice, HD maps are laborious and costly to produce and require frequent maintenance. Instead of using off-the-peg HD maps, we follow the philosophy of \cite{li2021hdmapnet, casas2021mp3, chen20203d} in predicting online HD maps but propose to learn high-level scene geometry representations from real-time sensor inputs and take these representations as priors for the prediction task.

\subsection{BEV Representations}
% Forming BEV representations has attracted immense interest in the field of autonomous driving. 
BEV representations provide a unified and physical-interpretable way to represent the rich information of road, moving objects and occlusion in a traffic scene, which can be easily utilized for downstream tasks such as motion prediction, planning and control, etc. 
For camera-based methods, how to solve the problem of projecting features from perspective view to BEV is a major challenge. 
Some learnable methods use MLP~\cite{vpn, pon, li2021hdmapnet} or transformer network ~\cite{cvt, bevsegformer} to implicitly reason the relationship between two different views.
LSS~\cite{lift-splat-shoot} proposes the approach of predicting depth distribution per pixel on 2D features, then ‘lifting’ the 2D features according to the corresponding depth distribution to BEV space. Numerous works, aiming at tasks of BEV perception~\cite{bevdet, m2bev, Where2comm:22}, motion prediction~\cite{fiery, stp3, beverse, stretchbev}, lidar-camera fusion~\cite{bevfusion}, etc., follow this form to generate BEV representations.
Also, some methods~\cite{simplebev, bevformer, persformer} explicitly establish the correspondence from BEV location to image-view pixel using homography between image and BEV plane and achieve attractive performance in diverse tasks. 
% \cite{simplebev} simply averages features from all projected image locations, while \cite{bevformer, persformer} uses deformable attention to aggregate image features.

However, when dealing with temporal information, most methods~\cite{fiery, stp3, beverse, bevformer, stretchbev} warp history BEV representations according to the variation of ego poses. Due to the pre-defined fixed range and size of the BEV grid, rotation and translation operations may cause distortion and out-of-range problems when aligning history BEV maps to current ego coordinates. Though~\cite{qin2022uniformer} introduces a similar operation to us to integrate historical information into the current frame, the design of their model is unable to predict future states. To alleviate these issues, we propose a PoseSync BEV Encoder module based on deformable attention to generate pose-synchronized BEV representations from temporally consecutive image-view input.
% \Note{sc: this part is too long and the title is strange. what is the connection to our work?}



\begin{figure*}[ht]
    \centering
    \includegraphics[scale=0.26]{content/figure/fig_encoder.png}
    \caption{The PoseSync BEV Encoder (A) takes front-view features and camera poses as input and then maps to BEV space. The core to generate BEV features in a synchronized way is the Pose-Aware Cross Attention. Its cross-view attention mechanism is depicted in (B), where front-view features from different frames of a dynamic vehicle are projected into a uniform BEV space.}
    \label{fig:PoseSync BEV Encoder}
     \vspace{-4mm}
\end{figure*} 

\subsection{Spatial-Temporal Modeling}

In the BEV prediction field, how to design a temporal model to aggregate spatial-temporal information is a critical problem. Existing modeling methods can be classified into three categories: RNN-based, CNN-based and transformer-based. RNN-based methods~\cite{fiery, stp3, beverse, stretchbev, sadat2020perceive, CMPNMMP:20} utilize recurrent models such as LSTM~\cite{lstm}, GRU~\cite{gru} to predict the future latent states. Though the recurrent model is powerful to model temporal relationships, it is time-consuming for constraints in the parallelization of computation. Besides, some CNN-based methods~\cite{wu2020motionnet, wang2022sti, luo2018faf, casas2021mp3} concatenate BEV features in the time dimension and take advantage of 3D convolution to extract spatial-temporal features. 

Due to the great power of transformer~\cite{transformer} in sequence modeling, it has shown promise in many temporal modeling tasks such as trajectory prediction~\cite{girgis2021latent, ngiam2021scenetransformer, xu2022GroupNet}, object tracking~\cite{li2022time3d}, video prediction~\cite{gupta2022maskvit, rakhimov2020latent, weissenborn2019scaling}, video interpolation~\cite{lu2022video, geng2022rstt, shi2022video}, etc. For BEV perception, \cite{bevformer, liu2022petrv2} utilize self-attention to model temporal information from multiple frames to boost perception task. \cite{li2022time3d} leverage self-attention to aggregate spatial information and cross-attention to exploit affinities among sequence frames.
To explore the capacity of transformer model in BEV spatial-temporal modeling, we propose a novel Spatial-Temporal Pyramid Transformer (STPT) architecture with future queries for BEV spatial-temporal features extraction and BEV future states prediction.

