\vspace{-2mm}
\section{Introduction}
\begin{figure}[t]
    \centering
    \includegraphics[scale=0.32]{content/figure/fig1_2.png}
    \caption{Two major challenges in vision-based perception and prediction are (a) how to avoid distortion and deficiency when aggregating features across time and camera views; and (b) how to achieve spatial-temporal feature learning for prediction. Our Pose-Synchronized BEV Encoder can precisely map the visual features into synchronized BEV space, and Spatial-Temporal Pyramid Transformer extracts feature at multiple scales.}
    \label{fig:warp_problem}
\end{figure} 
% Traditional autonomous driving systems always adopt a cascade paradigm, where the whole system can be decomposed into a series of cascaded modules according to their functionalities, including perception, tracking, prediction, planning, and control (cite some). The output result of each module is used as the input of the next module. Separate modules with clear objectives make the overall system more interpretable. However, in such a cascade system, the uncertainty of results is hard to transfer effectively, and errors caused by one module affect all subsequent tasks and are difficult to correct, which harms robustness and may result in serious errors. Therefore, plenty of works (cite some) investigate how to design a unified system to integrate multiple serial tasks.

As one of the most fascinating engineering projects, autonomous driving has been an aspiration for many researchers and engineers for decades. Although significant progress has been made, it is still an open question in designing a practical solution to achieve the goal of full self-driving. A traditional and common solution consists of a sequential stack of perception, prediction, planning, and control. Despite the idea of divide-and-conquer having achieved tremendous success in developing software systems, a long stack could cause cascading failures in an autonomous system. Recently, there is a trend to combine multiple parts in an autonomous system to be a joint module, cutting down the stack. For example,~\cite{liang2020pnpnet, shah2020liranet} consider joint perception and prediction and~\cite{sadat2020perceive, casas2021mp3} explore joint prediction and planning. This work focuses on joint perception and prediction.

The task of joint perception and prediction (PnP) aims to predict the current and future states of the surrounding environment with the input of multi-frame raw sensor data. The output current and future states would directly serve as the input for motion planning.
Recently, many PnP methods are proposed based on diverse sensor input choices. For example,~\cite{liang2020pnpnet,luo2018faf,casas2018intentnet} take multi-frame LiDAR point clouds as input and achieve encouraging 3D detection and trajectory prediction performances simultaneously. Recently, the rapid development of vision-centric methods offers a new possibility to provide a cheaper and easy-to-deploy solution for PnP. For instance,~\cite{fiery, stretchbev, stp3} only uses RGB images collected by multiple cameras to build PnP systems. Meanwhile, without precise 3D measurements, vision-centric PnP is more technically challenging. Therefore, this work aims to advance this direction.



% Many early works (cite some) detect bounding boxes first and then predict future trajectories at the instance level. Another formation (cite some) is to express the scene as BEV semantic grid maps and flow of occupancy grids. This bounding-box-free formation consists of sufficient information for the dynamic 3D environment and maintains interpretability and differentiability for downstream planning and control tasks \cite{casas2021mp3, sadat2020perceive}. 
% Meanwhile, PnP could output in various ways, including bounding boxes and occupancy grid maps (OGM). For example,~\cite{TBD} ****; and~\cite{TBD} ****. In comparison,  bounding box-based solutions emphasize recognizing the category foreground objects, which are more commonly adopted; while OGM-based solutions prone to reflect the existence and occupancy of objects, which is less mature.




The core of vision-centric PnP is to learn appropriate spatial-temporal feature representations from temporal image sequences. It is a crux and difficult from three aspects. First, since the input and the output of vision-centric PnP are supported in camera front-view (FV) and bird's-eye-view (BEV) respectively, one has to deal with distortion issues during geometric transformation between two views. Second, when the vehicle is moving, the view of the image input is time-varying and it is thus nontrivial to precisely map visual features across time into a shared and synchronized space. Third, since information in temporal image sequences is sufficiently rich for humans to accurately perceive the environment, we need a powerful learning model to comprehensively exploit spatial-temporal features.

To tackle these issues, previous works on vision-centric PnP consider diverse strategies. For example,~\cite{fiery, beverse} follows the method in~\cite{lift-splat-shoot} to map FV features to BEV features, then synchronizes BEV features across time via rigid transformation, and finally uses a recurrent network to exploit spatial-temporal features. However, due to the image discretization nature and depth estimation uncertainty, simply relying on rigid geometric transformations would cause inevitable distortion; see Fig.~\ref{fig:warp_problem}. Some other work~\cite{wang2021learning} transforms the pseudo feature point cloud to current ego coordinates and then pools the pseudo-lidar to BEV features; however, this approach encounters deficiency due to the limited sensing range in perception. Meanwhile, many works~\cite{fiery, beverse, stp3} simply employ recurrent neural networks to learn the temporal features from multiple BEV representations, which is hard to comprehensively extract spatial-temporal features.

To promote more reliable and comprehensive feature learning across views and time, we propose the temporal bird's-eye-view pyramid transformer (TBP-Former) for vision-centric PnP. The proposed TBP-Former includes two key innovations: i) pose-synchronized BEV encoder, which leverages a pose-aware cross-attention mechanism to directly map a raw image input with any camera pose at any time to the corresponding feature map in a shared and synchronized BEV space; and ii) spatial-temporal pyramid transformer, which leverages a pyramid architecture with Swin-transformer~\cite{liu2021swin} blocks to learn comprehensive spatial-temporal features from sequential BEV maps at multiple scales and predict future BEV states with a set of future queries equipped with spatial priors.
% spatial and temporal priors.
% {\HC Based on the multi-head decoder, the proposed TBP-Former deals with the tasks of perception and prediction. }


Compared to previous works, the proposed TBP-Former brings benefits from two aspects. First, previous works~\cite{fiery, beverse, stp3, bevformer} consider FV-to-BEV transformation and temporal synchronization as two separate steps, each of which could bring distortion due to discrete depth estimation and rigid transformation; while we merge them into one step and leverage both geometric transformation and attention-based learning ability to achieve spatial-temporal synchronization. Second, previous works~\cite{fiery, wu2020motionnet} use RNNs or 3D convolutions to learn spatial-temporal features; while we leverage a powerful pyramid transformer architecture to comprehensively capture spatial-temporal features, which makes prediction more effective.

%  based on well synchronized BEV feature maps, 

% {\HC We conduct expensive experiments ***}


To summarize, the main contributions of our work are:
\begin{itemize}
    \item To tackle the distortion issues in mapping temporal image sequences to a synchronized BEV space, we propose a pose-synchronized BEV encoder (PoseSync BEV Encoder) based on cross-view attention mechanism to extract quality temporal BEV features.
    \item We propose a novel Spatial-Temporal Pyramid Transformer (STPT) to extract multi-scale spatial-temporal features from sequential BEV maps and predict future BEV states according to well-elaborated future queries integrated with spatial priors.
    \item Overall, we propose TBP-Former, a vision-based joint perception and prediction framework for autonomous driving.
    TBP-Former achieves state-of-the-art performance on nuScenes~\cite{caesar2020nuscenes} dataset for the vision-based prediction task. Extensive experiments show that both PoseSync BEV Encoder and STPT contribute greatly to the performance. Due to the decoupling property of the framework, both proposed modules can be easily utilized as alternative modules in any vision-based BEV prediction framework.
\end{itemize}

