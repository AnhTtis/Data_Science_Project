
% \begin{figure*}[h]
%     \centering
%     \includegraphics[scale=0.45]{content/figure/fig_encoder.png}
%     \caption{Encoder (\textcolor{red}{TODO: image size, split subfigure})}
%     \label{fig:PoseSync BEV Encoder}
% \end{figure*} 




\section{Methodology}




\subsection{Overview Architecture}
The overall architecture of the proposed TBP-Former is illustrated in Fig.~\ref{fig:overview}. It takes the input of multi-view images with the corresponding camera poses at consecutive $T$ timestamps. The final output includes BEV map segmentation for current scene understanding and occupancy flow for motion prediction. The whole  TBP-Former can be decoupled into three parts: (i) pose-synchronized BEV encoder, which maps raw image sequences into feature maps in a spatial-temporal-synchronized BEV space; (ii) spatial-temporal pyramid transformer, which achieves comprehensive feature learning at multiple spatial and temporal scales; and (iii) a multi-head decoder, which takes the spatial and temporal features to achieve scene understanding and motion prediction. We will elaborate on each part in the following subsections.


% \begin{figure}[t]
%   \centering
%   \begin{subfigure}{0.95\linewidth}
%     % \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \includegraphics[scale=0.37]{content/figure/fig_encoder_1.png}
%     \caption{Network architecture of PoseSync BEV Encoder.}
%     \label{fig:encoder_overview}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.95\linewidth}
%     % \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \includegraphics[scale=0.38]{content/figure/fig_encoder_2.png}
%     \caption{Illustration of the cross-view attention mechanism.}
%     \label{fig:encoder_illustration}
%   \end{subfigure}
%   \caption{The PoseSync BEV Encoder (a) takes front-view features and camera poses as input and then maps to BEV space. The core to generate BEV features in a synchronized way is the Pose-Aware Cross Attention. Its cross-view attention mechanism is depicted in (b), where front-view features from different frames of a dynamic vehicle are projected into a uniform BEV space.}
%   \label{fig:encoder}
% \end{figure}

\subsection{Pose-Synchronized BEV Encoder}
\label{sec3.2}
Given images collected at multiple time stamps and from various camera poses, we aim to generate the corresponding feature maps in a shared and synchronized BEV space. Different from many previous works that synchronize spatial and temporal information in two separate steps, the proposed pose-synchronized BEV encoder leverage both geometric prior and learning ability to achieve one-step synchronization, alleviating distortion effects. Following the previous transformer-based method~\cite{bevformer}, this encoder adopts a transformer architecture whose core is a novel cross-view attention operation.

% {\HC In previous works \cite{bevformer, stp3, fiery, stretchbev, beverse} that capture spatial-temporal information in the BEV space, BEV representations from consecutive timestamps need to be extracted and then warped to a unified coordinate, which can easily cause distortion and out-of-range problems because of the movement of the vehicle.}\Note{sc: hard to understand}. To tackle this issue, we propose a pose-synchronized encoder, which generates the BEV feature map supported in a unified space for an input image collected at any time from any pose.

\textbf{Front-view feature map.}
Let  $\mathcal{X} = \{ \mathbf{X}_i^{(-t)} \}^{N, T}_{i=1, t= 0}$ be the input multi-frame multi-view images, where $N$ is the number of cameras, $T$ is the number of historical timestamps and $\mathbf{X}_i^{(-t)} \in \mathbb{R}^{ H \times W \times 3}$ is the RBG image captured by the $i$th camera at historical time stamp $t$. Note that each front-view image is associated with a different camera pose. Let $\mathcal{S}_{i}^{(-t)} = \{(u_{i}^{(-t)}, v_{i}^{(-t)})\}_{1,1}^{H,W}$ be the pixel indices of the $i$th camera's front-view space, whose image size is $H \times W$. We feed each RBG image $\mathbf{X}_i^{(-t)}$ into a shared backbone network (our implementation uses ResNet-101~\cite{resnet}) and obtain the corresponding front-view feature map $\mathbf{F}_i^{(-t)} \in \mathbb{R}^{ H^{\prime} \times W^{\prime} \times C}$ with $C$ the channel number, which is also supported on the front-view space $\mathcal{S}_{i}^{(-t)}$.

\textbf{BEV queries.}
Let $\mathcal{S}_{\rm BEV} = \{(x,y)\}_{x=1,y=1}^{X,Y}$ be the BEV grid indices, reflecting the  $X \times Y$ BEV grid space based on the vehicle-ego pose at the current timestamp. Note that $\mathcal{S}_{\rm BEV}$ is the only BEV space we work with in this paper. Let  $\mathbf{Q} \in \mathbb{R}^{ X \times Y \times C}$  be the trainable BEV queries whose element $\mathbf{Q}_{x,y} \in \mathbb{R}^C$ is a $C$-dimensional query feature at the $(x,y)$th geo-location in the BEV space $\mathcal{S}_{\rm BEV}$. We use $\mathbf{Q}$ as the input to query from the front-view feature map $\mathbf{F}_i^{(-t)}$ to produce the corresponding BEV feature map.


\textbf{Cross-view attention.} 
As the key operation in the pose-synchronized BEV encoder, the proposed cross-view attention constructs a feature map in the BEV space $\mathcal{S}_{\rm BEV}$ by absorbing information from the corresponding pixels in the front-view feature map.

Let $\mathcal{P}_i^{(-t)}: \mathcal{S}_{\rm BEV} \times \mathcal{Z} \rightarrow  \mathcal{S}_{i}^{(-t)}$ be a project operation that maps a BEV index with a specific height index to a pixel index in the $i$th camera's front view at historical timestamp $t$; that is,
\begin{equation*}
    (u_{i}^{(-t)}, v_{i}^{(-t)} ) = \mathcal{P}_i^{(-t)} \Big( (x,y,z) \Big),
\end{equation*}
where $z \in \mathcal{Z} = \{1,\cdots, Z\}$. The project operation $\mathcal{P}_i^{(-t)}$ builds the geometric relationship between the BEV and a front-view. The implementation of $\mathcal{P}_i^{(-t)}$ works as 
\begin{equation*}
\begin{aligned}
z^{(-t)}_{i} \cdot \begin{bmatrix}
    u^{(-t)}_{i} \\[4pt]
    v^{(-t)}_{i} \\[4pt]
    1
\end{bmatrix} =  \mathcal{T}_{\mathcal{S}_{\rm BEV} \rightarrow  \mathcal{S}_{i}^{(-t)}} \cdot
\begin{bmatrix}
    x \\
    y \\
    z \\
    1
\end{bmatrix},
\end{aligned}
\label{eq:projection}
\end{equation*}
where $\mathcal{T}_{\mathcal{S}_{\rm BEV} \rightarrow  \mathcal{S}_{i}^{(-t)}} \in \mathbb{R}^{3 \times 4}$ is a transformation matrix that can be calculated by camera's intrinsic/ extrinsic parameters and vehicle-ego pose.

Based on the project operation, the cross-view attention can trace visual features in the front view through a BEV index. Let $\mathbf{B}_i^{(-t)} \in \mathbb{R}^{ X \times Y \times C}$ be the BEV feature map associated with the RBG image $\mathbf{X}_i^{(-t)}$. The $(x,y)$th element of the BEV feature map is obtained as
% \begin{equation}
% \label{eq:cross_view_attention}
% (\mathbf{B}_i^{(-t)})_{x,y} \ = \ \sum_{z} f_{\rm DA} \left( \mathbf{Q}_{x,y}, \mathcal{F}^{(-t)}_{\mathcal{N}_{(x,y,z)}} \right),
% \end{equation}
\begin{equation}
\label{eq:cross_view_attention}
(\mathbf{B}_i^{(-t)})_{x,y} \ = \ \sum_{z} f_{\rm DA} \left( \mathbf{Q}_{x,y}, \mathcal{P}_i^{(-t)}(x,y,z), \mathbf{F}_i^{(-t)} \right),
\end{equation}
where, $f_{\rm DA}(\cdot)$ represents the deformable attention operation~\cite{deformabledetr}. It allows BEV query $Q_{x,y}$ only to interact with the front-view feature $\mathbf{F}_i^{(-t)}$ within its regions of interest, which is sampled around the reference point calculated by $\mathcal{P}_i^{(-t)}$. 
Since one BEV index might lead to multiple pixel indices in the front-view image because of various height possibilities in the 3D space. We thus sum over all possible heights in~\eqref{eq:cross_view_attention}.  To further aggregate BEV feature maps across all the $N$ camera views, we simply take the average; that is, the BEV feature map at historical timestamp $t$ is $\mathbf{B}^{(-t)} = \frac{1}{N} \sum_i \mathbf{B}_i^{(-t)}$.
%  $\mathcal{F}^{(-t)}_{\mathcal{N}_{(x,y,z)}} = \{ (\mathbf{F}_i^{(-t)})_{u,v} | d\left( (u,v) - \mathcal{P}_i^{(-t)}((x,y,z)) \right) \leq \tau \} $ is the collection of front-view features of all the neighboring pixels corresponding to the BEV index $(x,y)$ with $\tau$ the neighboring radius
% where $DA$ represents deformable attention\cite{deformabledetr} operation. It allows BEV query $Q_{h,w}$ only to interact with features $\mathcal{F}_{T-t}$ within its regions of interest, which is sampled around the reference point $(u^{T-t}_{ij}, v^{T-t}_{ij})$.
Note that all the front-view features across time and from multiple cameras are synchronized into the same BEV space in one step~\eqref{eq:cross_view_attention}, leading to less information distortion or deficiency issues.
% ; and ii) the cross-view attention~\eqref{eq:cross_view_attention} allows the BEV query $\mathbf{Q}_{{x},{y}}$  to interact with the front-view features $\mathbf{F}_i^{(-t)}$ only within its region of interest. 

We can successively apply the cross-view attention followed by feed forward networks and normalization layers for multiple times. Finally, we order BEV feature map at multiple timestamps and obtain a temporal BEV feature map $\mathcal{B} = [ \mathbf{B}^{(0)}, \mathbf{B}^{(-1)}, ..., \mathbf{B}^{(-T)} ] \in \mathbb{R}^{(T+1) \times X \times Y \times C} $.


% and $(x^{\prime}, y^{\prime})$ as its real-world coordinates in ego coordinate. For each query, $Z$ points of various heights are sampled and create a set of 3D reference points $\{(x^{\prime}, y^{\prime}, z^{\prime}_j), j=1, \dots, Z\}$.


% In order to generate BEV grid features under $T$ vehicle-ego coordinates, we need to utilize the physical correspondence between BEV space and perspective view at different timestamps, see (cite fig 2.b). Take timestamp $T-t$ as an example, let $(u^{(T-t)}_{i}, v^{(T-t)}_{i})$ be the corresponding pixel position in $i$-th view at $T-t$ time to the reference point $(x, y, z)$.
% Denote $\textbf{T}_{img_i}^{(T-t)} \in \mathbb{R}^{3 \times 4}$ as the transformation matrix from global coordinates to image homogeneous coordinates of $i$-th view at timestamp $T-t$, which can be calculated by camera's intrinsic and extrinsic parameters. $\textbf{T}_{ego}^{(T)}$ is the transformation matrix from ego coordinates to global coordinates at timestamp $t$.
% The mathematical relationship between $(x,y,z)$ and $(u^{(T-t)}_{i}, v^{(T-t)}_{i})$ is
% % \begin{equation}
% % \begin{aligned}
% % z^{T-t}_{ij} [ u^{T-t}_{ij}, v^{T-t}_{ij}, 1 ]^T  = \textbf{T}^{img_i}_{T-t} \cdot \textbf{T}^{ego}_{T} \cdot [ x^{\prime},y^{\prime},z^{\prime}_j, 1 ]^T
% % \end{aligned}
% % \label{eq:projection}
% % \end{equation}


% \begin{equation}
% \begin{aligned}
% z^{(T-t)}_{i} \cdot \begin{bmatrix}
%     u^{(T-t)}_{i} \\[4pt]
%     v^{(T-t)}_{i} \\[4pt]
%     1
% \end{bmatrix} = \textbf{T}_{img_i}^{(T-t)} \cdot \textbf{T}_{ego}^{(T)} \cdot
% \begin{bmatrix}
%     x \\
%     y \\
%     z \\
%     1
% \end{bmatrix}
% \end{aligned}
% \label{eq:projection}
% \end{equation}

% Based on equation \ref{eq:projection}, we can define a pose-sync view projection function $\mathcal{P}_i^{(T \rightarrow T-t)}$. Its functionality is to map 3D reference point $(x,y,z)$ in $T$-th vehicle-ego coordinates to $i$-th image view at $T-t$ timestamp and get the corresponding position $(u^{(T-t)}_{i}, v^{(T-t)}_{i})$ in cartesian coordinates, which can be written as

% \begin{equation}
% \begin{aligned}
% \vspace{-3mm}
% (u^{(T-t)}_{i}, v^{(T-t)}_{i}) = \mathcal{P}_i^{(T \rightarrow T-t)}(x,y,z)
% \end{aligned}
% \end{equation}


% % \begin{equation}
% % \begin{aligned}
% % \text{PCAttn}(Q_{h,w}, \mathcal{F}_{T-t}) = \sum_{i=1}^{N} \sum_{j=1}^{Z} \text{DA}(Q_{h,w}, (u^{T-t}_{ij}, v^{T-t}_{ij}), \mathcal{F}^i_{T-t})
% % \end{aligned}
% % \end{equation}

% % \begin{equation}
% % \mathbf{A}_{\mathrm{x},\mathrm{y}}^{(T-t)} = \sum_{i, z} f_{\rm DA} \left( \mathbf{Q}_{\mathrm{x},\mathrm{y}}, \mathbf{F}_i^{(T-t)}, \mathcal{P}_i^{(T \rightarrow T-t)}(x,y,z) \right)
% % \end{equation}


\subsection{Spatial-Temporal Pyramid Transformer}
\label{sec3.3}

\begin{table*}[ht]
\centering
\begin{tabular}{c|c|cc|cc|c}
\toprule
\multirow{2}{*}{Method}                       & RGB            & \multicolumn{2}{c|}{Future semantic seg.}                       & \multicolumn{2}{c|}{Future instance seg.}                       & \multirow{2}{*}{~~~FPS~~~} \\
& Resolution     & IoU (Short)& IoU (Long)  & VPQ (Short)             & VPQ (Long) &     \\ 
\hline
\hline
FIERY\cite{fiery}            & 224$\times$480 & 59.4                           & 36.7                           & 50.2                           & 29.9                           & 1.56                                 \\
StretchBEV \cite{stretchbev} & 224$\times$480 & 55.5                           & 37.1                           & 46.0                           & 29.0                           & 1.56                                 \\
ST-P3\cite{stp3}             & 224$\times$480 & -                              & 38.9                           & -                              & 32.1                           & 1.43                                 \\
BEVerse \cite{beverse}       & 256$\times$704 & 60.3                           & 38.7                           & 52.2                           & 33.3                           & 1.96                                 \\ \midrule
TBP-Former                                          & 224$\times$480 & \textbf{64.7} & \textbf{41.9} & \textbf{56.7} & \textbf{36.9} & \textbf{2.44}                                 \\ \bottomrule
\end{tabular}
\caption{\textbf{Prediction results on nuScenes~\cite{caesar2020nuscenes} validation set.} Intersection-over-Union (IoU) is used for future semantic segmentation and Video Panoptic Quality (VPQ) for future instance segmentation. Results are reported under two settings: short ($30m \times 30m$) range and long ($100m \times 100m$) range. Frame Per Second (FPS) means the inverse of inference time. All methods are tested under the same settings on a single NVIDIA A100. Our TBP-Former achieves SOTA performance and is still more computationally efficient than other methods.}
\label{tab:prediction_result}
\end{table*}



\begin{figure}[t]
    \centering
    \includegraphics[scale=0.38]{content/figure/fig4.png}
    \caption{The network architecture of Spatial-Temporal Pyramid Transformer (STPT). Each encoder layer consists of an optional convolutional block for downsampling and Swin Transformer Blocks, while each decoder layer contains Swin Transformer Blocks and a deconvolutional block for upsampling. In the decoding process, we pre-define a set of future queries to represent future BEV states and query the features from encoders. }
    \label{fig:temporal_model}
\end{figure} 



We further propose a novel spatial-temporal pyramid transformer (STPT) to learn spatial-temporal features more comprehensively and produce the future BEV states. The detailed structure of STPT is depicted in Fig.~\ref{fig:temporal_model}.


\textbf{Temporal BEV pyramid feature learning.} We encode the input temporal BEV feature map $\mathcal{B}$ with four hierarchical layers. Each encoder layer is composed of an optional convolution layer with stride 2 to downsample the features and a swin transformer encoder, which is a stack of Swin Transformer blocks~\cite{liu2021swin}. In our implementation, the window size of Swin-T blocks is set as $(4, 4)$. We can then obtain multi-scale spatial-temporal features $\mathcal{B}_s \in \mathbb{R}^{(T+1) \times \frac{X}{2^s} \times \frac{Y}{2^s}  \times C}, ~s=0,1,2,3$.

\textbf{Future BEV queries.}
Future BEV queries are defined to represent future BEV states and query the generated multi-scale spatial-temporal features. There is a set of future queries $\{\mathbf{Q}^{(t)}\}, ~t=0, \dots, T^{\prime}$, where $\mathbf{Q}^{(t)}$ has the same sptial dimension $\frac{X}{8} \times \frac{Y}{8}$ as $\mathcal{B}_3$.
The queries are equipped with both spatial and temporal priors. A map feature generator is applied to generate high-dimensional features from $\mathbf{B}^{(T)}$ for scene geometry information. To be specific, the same structure and parameters of the hdmap decoder head (excluding the last linear layer) are reused. The extracted map feature is added to all future queries for spatial information prior. 
On the other hand, an additional time embedding is added to queries along the time dimension to distinguish the time variation of predicted BEV states.

\textbf{Future BEV state prediction.} The decoding process contains corresponding four hierarchical layers as the encoding process. Unlike the encoding process, $\{\mathbf{Q}^{(t)}\}$ is used as the query input of Swin-T block and performs cross attention with the encoded features $\mathbf{E}_3$. After the first decoding layer, the output of each layer is used as the query input of next layer. Similar to encoding layers, the deconvolution layer is optionally applied to upsample the decoded features. The simplified process can be written as 

$$ \mathcal{D}_s = \left\{ 
\begin{array}{rcl}
\begin{aligned}
 &\mathrm{SwinT}(\mathcal{B}_3, \{\mathbf{Q}^{(t)}\}), & s =3   \\
 & \mathrm{SwinT}(\mathcal{B}_s, \mathrm{DeConv}(\mathbf{D}_{s+1})), & s =0, 1, 2 \\
\end{aligned}
\end{array} \right. $$
where $\mathcal{D}_s  \in  \mathbb{R}^{(T'+1)  \times \frac{X}{2^s} \times \frac{Y}{2^s} \times C}, ~s = 0,1,2,3$ are decoded features.
The future temporal BEV feature map at the $0$th scale is a temporal sequence of final predicted BEV states; that is, $\mathcal{D}_0 =  [\mathbf{B}^{(0)}_*,\mathbf{B}^{(1)}_*,..., \mathbf{B}^{(T')}_* ]$, where $\mathbf{B}^{(t)}_* \in \mathbb{R}^{X \times Y \times C}$ is the BEV state at future time stamp $t$.

\begin{table}[t]
\centering
\begin{tabular}{c|c|cc}
\toprule
Method    & Temp. & Veh. IoU                      & Ped. IoU \\
\hline
\hline
VED    \cite{ved}                      &        &  23.3 & 11.9      \\
VPN  \cite{vpn}                        &         & 28.2 & 10.3      \\
PON    \cite{pon}                      &         &  27.9 & 13.9      \\
LSS    \cite{lift-splat-shoot}                      &         &  34.6 & 15.0      \\
CVT  \cite{cvt}                        &         & 36.0                         & -          \\
Image2Map \cite{saha2022translating}  &         & 40.2                         & -          \\
BEVFormer \cite{bevformer}                    &         & 44.4                         & -          \\
IVMP    \cite{wang2021learning}                     & \checkmark        & 36.8                        & 17.4      \\
FIERY  \cite{fiery}         & \checkmark        & 38.2                         & 17.2      \\
ST-P3      \cite{stp3}                  & \checkmark        & 40.1                        & 14.5      \\
% *BEVerse                     & 1        &      44.46                        &   \textcolor{red}{todo}          \\
\midrule
TBP-Former \textit{static}            &         & 44.8                        & 17.2      \\
TBP-Former    & \checkmark        & \textbf{46.2}                        & \textbf{18.6}     \\
\bottomrule
\end{tabular}
\caption{\textbf{Perception results on nuScenes~\cite{caesar2020nuscenes} validation set.} Results of vehicles and pedestrians are compared by segmentation IoU. Temp.indicates whether temporal information is involved.}
\label{tab:perception_result}
\end{table}



\subsection{Multi-head decoder}
\label{sec3.4}

The future temporal BEV feature map is fed into the multi-task decoder heads to generate various outputs for dynamic scene understanding, see Fig.~\ref{fig:result}. We follow the output setting in~\cite{fiery} that predicts BEV semantic segmentation, instance center, instance offset, and future flow for joint perception and prediction. Meanwhile, we set up an additional HD map decoder head to predict basic traffic scene elements including drivable areas and lanes. The map decoder head can not only provide scene information for subsequent planning and control modules but also give guidance to the prediction process, see sec.~\ref{sec3.3}.



