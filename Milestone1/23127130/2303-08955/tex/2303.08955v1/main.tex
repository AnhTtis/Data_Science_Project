
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{orcidlink}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\newcommand{\SMART}{S.M.A.R.T. }

\begin{document}

\title{Large-scale End-of-Life Prediction of Hard Disks in Distributed Datacenters}

\author{\IEEEauthorblockN{Rohan Mohapatra}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{San Jos\'e State University}\\
San Jose, CA, USA \\
rohan.mohapatra@sjsu.edu
}
\and
\IEEEauthorblockN{Saptarshi Sengupta}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{San Jos\'e State University}\\
San Jose, CA, USA \\
saptarshi.sengupta@sjsu.edu}
}




\author{\IEEEauthorblockN{Rohan Mohapatra \orcidlink{0000-0003-1654-7994}\IEEEauthorrefmark{1} , Austin Coursey \orcidlink{0000-0003-1774-6442}\IEEEauthorrefmark{2} and
Saptarshi Sengupta \orcidlink{0000-0003-1114-343X}\IEEEauthorrefmark{1}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}Department of Computer Science, San Jos\'e State University, San Jos\'e, CA, USA \\
\IEEEauthorrefmark{2}Department of Computer Science, Vanderbilt University, Nashville, TN, USA \\
Email: \IEEEauthorrefmark{1}rohan.mohapatra@sjsu.edu,
\IEEEauthorrefmark{2}austin.c.coursey@vanderbilt.edu, \IEEEauthorrefmark{1}saptarshi.sengupta@sjsu.edu}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\IEEEoverridecommandlockouts
% \IEEEpubid{\makebox[\columnwidth]{XXX-X-XXXX-XXXX-X/XX/\$XX.XX~\copyright2023 IEEE \hfill} \hspace{\columnsep}\makebox[\columnwidth]{ }}
\maketitle


\maketitle

\begin{abstract}
% Data centers process large amounts of data, the use of inexpensive hard-disks aided this. To mitigate the risk of failures, cloud storage providers prematurely replace hard-disks before they crash or fail. Sometimes, hard-disks fail, causing reliability issues that can hamper high-volume storage facilities. By understanding the remaining useful life (RUL) of Hard-disks, we can predict the failure day and replace it at the right time, ensuring maximum utilization whilst reducing operations costs. In this paper, we contrast the use of all health statistics against extracting useful attributes from severely skewed health statistics data. Past works suggest the use of LSTMs as an excellent approach to predict RUL. In this work, we also present an encoder-decoder LSTM model to predict RUL. The context gained from understanding health statistics sequences can predict an output sequence of the number of days remaining for a hard-disk. The model is trained and tested across all the years of data to build a robust model. We also tackle the problem of generalizability by using the trained LSTM on other hard-disk models.

On a daily basis, data centers process huge volumes of data backed by the proliferation of inexpensive hard disks. Data stored in these disks serve a range of critical functional needs from financial, and healthcare to aerospace. As such, premature disk failure and consequent loss of data can be catastrophic. To mitigate the risk of failures, cloud storage providers perform condition-based monitoring and replace hard disks before they fail. By estimating the remaining useful life of hard disk drives, one can predict the time-to-failure of a particular device and replace it at the right time, ensuring maximum utilization whilst reducing operational costs. In this work, large-scale predictive analyses are performed using severely skewed health statistics data by incorporating customized feature engineering and a suite of sequence learners. Past work suggests using LSTMs as an excellent approach to predicting remaining useful life. To this end, we present an encoder-decoder LSTM model where the context gained from understanding health statistics sequences aid in predicting an output sequence of the number of days remaining before a disk potentially fails. The models developed in this work are trained and tested across an exhaustive set of all of the 10 years of S.M.A.R.T. health data in circulation from Backblaze and on a wide variety of disk instances. It closes the knowledge gap on what full-scale training achieves on thousands of devices and advances the state-of-the-art by providing tangible metrics for evaluation and generalization for practitioners looking to extend their workflow to all years of health data in circulation across disk manufacturers. The encoder-decoder LSTM posted an RMSE of 0.83 on an exhaustive set while being able to generalize competitively over the other Seagate family hard drives.
\end{abstract}

\begin{IEEEkeywords}
Failure Prediction, Remaining Useful Life, Long Short-Term Memory, Hard Drive Health, Encoder-Decoder Models, S.M.A.R.T.
\end{IEEEkeywords}

\section{Introduction}
Data centers offer a broad range of data-related services. To support this, the dependability of hard disk drives (HDD) is of utmost importance. Data-center outages are plagued by HDD failure events \cite{hdd-errors}. Accurate prediction of an HDD's life span allows timely mitigation measures that prevent data loss and improve data center reliability. Data centers often rely on S.M.A.R.T. (Self-Monitoring Analysis and Reporting Technology) logs to understand how the HDD is performing. As seen by \cite{biderectionallstm}, Backblaze reported an annualized failure rate of 1.37\%. The above AFR  means that, on average, 1.37\% of the hard drives in their data centers fail each year. Working on prediction when hard drives fail is beneficial because it allows for proactive maintenance and management of hard drives. This can help to reduce downtime, minimize the risk of data loss, and ultimately save time and money. Additionally, it can help to identify patterns in hard drive failures, which can be used to inform future purchasing decisions and improve overall data storage strategies. We are able to identify significant correlations and patterns of failure, as well as produce helpful forecasts of future failures. Knowing when a drive will retire could enable early intervention, such as repair or replacement before failure occurs. 

% In this paper, ...

In Section \ref{section:lit}, we go over related work in the field and point out areas of interest and issues that we try to solve. Section \ref{section:data} provides a comprehensive framework to deal with datasets of this scale. It introduces a data pipeline, analysis of the failures of different Seagate models, and pre-processing techniques to improve the use of raw health statistics. Section \ref{section:lstm} talks about the rationale behind using an encoder-decoder LSTM and presents a hypothesis on the working of the proposed model. Section \ref{section:experiments} divulges into the different configurations and experimentation done with the dataset. In this section, we also analyze the generalizability of the proposed model and provide metrics to gain insights. Lastly, Section \ref{section:future} looks at potential avenues of the research setting this paper as a baseline.

\textbf{Contributions:} The contributions of this research paper are significant in several ways.

\begin{enumerate}
    \item \textit{\textbf{A encoder-decoder LSTM for RUL prediction:}} We propose an LSTM architecture based on encoder-decoder LSTMs. Encoder-decoder LSTM is novel in capturing context in a time series dataset because it is capable of learning complex temporal patterns and dependencies between input sequences and their corresponding output sequences. In contrast to traditional LSTM models that process input sequences one step at a time, the encoder-decoder LSTM processes the entire input sequence at once and generates the output sequence in a one-shot manner. This network is an important contribution to the paper and sheds light on how we extend the end-of-life sequences for more online learning. This type of network has not been implemented before, and we look to understand how this sequence can help mitigate the risk of a failing hard drive.
    \item \textit{\textbf{Investigative analysis on the data:}}The Backblaze dataset is very large, and as such, it cannot be used directly. In this context, we focused on analyzing Seagate models from 2013-2022. To analyze the data, a pipeline with PostgreSQL was proposed to preprocess and filter the Seagate models with significant failures. XGBoosting was employed for feature selection to identify the essential features in predicting hard drive failures. Interpolation was utilized to fill in the missing data in the dataset. This investigative analysis sanitizes the input which further is fed into the network.  
    \item \textit{\textbf{Generalizability of the network on the Seagate family:}} An important contribution to this paper. Many papers do not talk about the possibility of being able to predict end-of-life sequences for hard drive models other than one in focus. We present metrics across the Seagate family and show how well the model is able to generalize.

\end{enumerate}
% Firstly, it presents a review of the existing literature on the topic, synthesizing and evaluating previous studies and theories to provide a clear understanding of the research problem. Secondly, it introduces an approach with encoder-decoder LSTMs to predict end-of-life sequences of the number of days remaining before a disk fails. Thirdly, the paper presents a thorough investigation and experimentation on 10 years of data and insights on the generalizability of the proposed model on a number of models in the Seagate family. Finally, the paper directs future work on attention-based mechanisms, threat modeling to increase robustness, and an ensemble approach. 

\section{Related Work}
\label{section:lit}
In the industry, S.M.A.R.T. has become a standard for monitoring and failure warning technology for hard disk drives \cite{SMART}. However, how many computer systems today enable or read the \SMART logs is unknown. This isn't the case for Data-centers that use threshold testing algorithms to detect drive failures \cite{thresholdtestingalgo}. Classical approaches include using an SVM to classify failures \cite{svm} and Bayesian-based prediction of failures \cite{bayesian}. To make forecasts, \cite{hmmhdd} observed S.M.A.R.T. characteristics as time series and use Hidden Markov Models (HMMs) and Hidden Semi-Markov Models (HSMMs) to construct models for "good" and "failed" drives. This increasing tendency in attribute values (or their rates of change) over time indicates that some attributes are about to fail. Researchers have recently begun investigating the use of machine learning methods, such as Long Short-Term Memory (LSTM) networks, to forecast hard drive failures based on a study of hard drive sensor data incorporating S.M.A.R.T characteristics. This serves as a prelude to the use of Recurrent networks and LSTMs to detect drives that may fail and predict RUL \cite{biderectionallstm, hddpaper4, hddpaper2, stackedlstm, BASAK2021101283}.

A lot of research has been done by using LSTMs to forecast hard drive failures. Variants of LSTMs have been tried to find the best RMSE. Lima et al.\cite{stackedlstm} use a stacked LSTM to predict the remaining useful life of a hard drive. The use of a stacked LSTM is motivated by the network's ability to handle complicated temporal relationships in data. This is essential for forecasting hard drive failures because hard drive failure is often a gradual process that can be affected by a variety of variables over time. Coursey et al. \cite{biderectionallstm} explore the possibility of using Bidirectional Long Short-Term Memory (Bi-LSTM). Bi-LSTM is especially useful for describing temporal relationships in time-series data. Bi-LSTM networks extend the conventional LSTM design by processing input sequences in both forward and backward directions, enabling them to record both past and future context in data. The temporal relationships between S.M.A.R.T. characteristics in the case of hard disk drive failure forecast can be complicated and non-linear, and it is essential to consider both past and future circumstances when making statements about an HDD's residual usable life (RUL). The model can successfully encapsulate these dependencies and make accurate forecasts with the use of Bi-LSTM networks.

\section{Understanding the Data}
\label{section:data}
The hard drive dataset is provided by Backblaze, it contains logs recorded daily at a data center. The data is collected from 2013-2022 and contains information like the date, model, serial number, S.M.A.R.T. features, and if the hard drive has failed. In this paper, we only consider Seagate Models, pre-process and sanitize the data before we feed it to our network. Every day, every hard drive logs the following metrics:
\begin{enumerate}
    \item The timestamp of the log.
    \item The serial number and model of the hard drive.
    \item Failure information, 0 if the hard drive has not failed and 1 otherwise.
    \item S.M.A.R.T. features: All S.M.A.R.T. features are included in the dataset. While some of them are empty or zero. We use a feature selection technique to filter out important features.
\end{enumerate}

Seagate is one of the major manufacturers of hard drives and has been collecting data on hard drive failures for many years. This means that there is a large amount of data available on Seagate hard drives for researchers to use in studies of reliability and predicted remaining useful life (RUL). In addition, Seagate has a reputation for producing hard drives with consistent performance characteristics, which can make the data collected from Seagate drives more reliable for use in RUL prediction studies than data from other manufacturers. Even if we pivot to another manufacturer, Backblaze over the years has collected more \SMART logs for Seagate than for any other manufacturer. Because of this, our analysis on RUL predictions is focused on Seagate hard drives.

\subsection{Data Pipeline: Processing Data}
The Backblaze dataset is very large and reading it to memory is inviable. An easy way to extract data based on years for a given hard-disk model warranted the need to use a database. We processed the data for each model into a separate logical layer using the pipeline shown in Fig. \ref{fig:datapipeline}. An HDD model was selected which was used to filter. The filtered data was then parallelized based on the year as Backblaze reports the health statistics every year. Each year was then parallelized and pushed to a locally managed PostgreSQL server.

\begin{figure}[h!]
\includegraphics[width=\columnwidth]{DataPipeline.drawio.eps}
\caption{Data pipeline to process HDD data}
\label{fig:datapipeline}
\end{figure}

\subsection{Extracting HDD Models with Failures}
The next task of understanding the data is to figure out the HDD models that fail the most. Using a modified data pipeline from Fig, \ref{fig:datapipeline}, we queried for the number of unique failures of all Seagate models. Table \ref{table:failures} shows the Seagate models and the number of unique failures from 2013-2022.


\begin{table}[htbp]
\begin{center}
\caption{Unique failures of Seagate HDD Models from 2013 to 2022}
\begin{tabular}{|c|c|}

\textbf{Seagate HDD Model}             & \textbf{Number of unique failures} \\ \hline \hline
ST4000DM000                            & 4934                               \\ \hline
ST12000NM0007                          & 2010                               \\ \hline
ST3000DM001                            & 1708                               \\ \hline
ST8000NM0055                           & 1101                               \\ \hline
ST8000DM002                            & 731                                \\ \hline
ST12000NM0008                          & 679                                \\ \hline
ST31500541AS                           & 397                                \\ \hline
ST31500341AS                           & 216                                \\ \hline
\end{tabular}
\label{table:failures}  
\end{center}
\end{table}

\subsection{Feature Selection using XGBoost}
The dataset contains a lot of S.M.A.R.T features. As part of our pre-processing and visualization, we realized that working with all the features could be a viable approach. But as we looked at different models, there are very few overlapping features. Another thing that we realized was that many S.M.A.R.T. features were either zeros or did not vary with time. This led us to believe that a feature selection algorithm can find the important features. XGBoosting was used to select the features and we present a comparison of training with full features and training with important features. 

\begin{figure}[h!]

\includegraphics[width=\columnwidth]{destination_path.eps}
\caption{Important features}
\label{fig:impfeat}
\end{figure}

XGBoost \cite{xgboost}, or Extreme Gradient Boosting, is a powerful algorithm that is often used for feature extraction in machine learning. It is particularly good for this task because it can handle high-dimensional feature spaces as in the case of the hard disk dataset and is able to automatically select the most important features. XGBoost works by iteratively adding decision trees to a model, with each new tree trying to correct the errors made by the previous trees. The algorithm places more emphasis on misclassified data points, which makes it robust against outliers and noise. Additionally, XGBoost allows for the computation of feature importance scores, which can be used to rank features and select the most relevant ones for a particular task. It uses a built-in feature importance metric that allows it to rank the input features based on how much they contribute to the model's performance which in this case, by intuition, provides relevant \SMART features.

Some of the important features that we discovered are tabulated in Table \ref{table:smart}. While there are numerous \SMART features that provide information about a hard drive's health, only a subset of them is commonly associated with potential drive failure as shown in Fig. \ref{fig:impfeat}. These specific \SMART features provide insight into various aspects of a hard drive's health, such as the frequency of raw read errors, reallocated sectors, start-stop cycles, spin-up time, power-on hours, and temperature fluctuations. 

These factors can all contribute to the gradual degradation of a hard drive's mechanical components, ultimately leading to drive failure. For instance, an excessively high raw read error rate, which indicates errors encountered while reading data from a hard drive's platters, can be an early warning sign of drive failure. Similarly, an increased number of reallocated sectors suggests that a hard drive is struggling to maintain the integrity of stored data, which can also lead to a hard drive failure. 
% \begin{enumerate}
%     \item S.M.A.R.T 1 
%     \item S.M.A.R.T 4
%     \item S.M.A.R.T 5 
%     \item S.M.A.R.T 7 
%     \item S.M.A.R.T 9
%     \item S.M.A.R.T 188 
%     \item S.M.A.R.T 190
%     \item S.M.A.R.T 241
%     \item S.M.A.R.T 242
% \end{enumerate}

\begin{table*}[h]
\centering
\caption{Selected \SMART features and their context related to hard drives \cite{backblaze2014}}
\begin{tabular}{|l|p{10cm}|}

\textbf{\SMART Feature} & \textbf{Meaning} \\
\hline
\hline
\SMART 1 & Raw Read Error Rate: The total number of errors the hard drive encounters when reading data from a disk. \\
\hline
\SMART 4 & Start/Stop Count: The total number of times the hard drive is powered on and off. \\
\hline
\SMART 5 & Reallocated Sector Count: The total number of sectors that have been remapped due to read errors. \\
\hline
\SMART 7 & Seek Error Rate: The total number of errors that occur when the hard drive tries to position the read/write head over the correct sector on the disk. \\
\hline
\SMART 9 & Power-On Hours: The total number of hours the hard drive has been powered on. \\
\hline
\SMART 188 & Command Timeout: The total number of times that the drive did not respond to a command sent by the computer's operating system. \\
\hline
\SMART 190 & Airflow Temperature Celsius: The temperature of the hard drive's airflow, measured in Celsius. \\
\hline
\SMART 192 & Power-Off Retract Count: The total number of times the read/write head is retracted due to power loss or other reasons. \\
\hline
\SMART 193 & Load/Unload Cycle Count: The total number of times the head is loaded and unloaded. \\
\hline
\SMART 194 & Temperature Celsius: The temperature of the hard drive, measured in Celsius. \\
\hline
\SMART 197 & Current Pending Sector Count: The total number of unstable sectors that the hard drive is attempting to read. \\
\hline
\SMART 198 & Offline Uncorrectable Sector Count: The total number of sectors that cannot be corrected using hardware error correction. \\
\hline
\SMART 199 & UDMA CRC Error Count: The total number of errors that occur when data is transferred between the hard drive and the computer's memory. \\
\hline
\SMART 241 & Total LBAs Written: The total number of sectors written to the hard drive. \\
\hline
\SMART 242 & Total LBAs Read: The total number of sectors read from the hard drive. \\
\hline
\end{tabular}
\label{table:smart}
\end{table*}


\begin{figure}[h!]
\includegraphics[width=\columnwidth]{allvimp.eps}
\caption{Training and Validation RMSE on Full features vs Important Features discovered by XGBoosting}
\label{fig:fullvimp}
\end{figure}

Fig. \ref{fig:fullvimp} demonstrates that using important features reaches a lower RMSE. One thing to note is that we build this figure based on 2022 Data, which is then extended to the use of full data.

\subsection{Missing Data}
A known issue that has been carefully ignored with the Backblaze dataset is the missing S.M.A.R.T. parameters before February 2014. Santo et al. \cite{hddpaper4} conclusively indicate that 70 \% of the S.M.A.R.T. parameters had not been collected for the Seagate model in focus during this time. As we try to reason whether the data before this time is meaningful or not, we train on the data with and without the missing data (gaps filled by interpolation) to come to a conclusion. 

When there are spaces in the data, a frequent method in data analysis is to fill them with interpolation. Interpolation can be used in time series analysis to approximate missing values at a particular time point using the values of nearby time points. Linear interpolation is the most basic technique for estimating missing values by making a straight line between two known values on either side of the missing value.

Because the time series has a linear tendency (see Fig. \ref{fig:lineartendency}), we use linear interpolation to replace the empty gap. As noticed from Fig. \ref{fig:interpolate}, the interpolation indeed works and gives a substantial extra year to extract meaningful predictions. Lower RMSE is also an indication that data points lying between 2013 and February 2014 are significant and cannot be simply ignored.

\begin{figure}[htbp]
\centering
\begin{subfigure}[b]{0.55\textwidth}
        \includegraphics[width=0.85\columnwidth]{lineartendency.eps}
   \caption{An example of \SMART 241 feature, this figure \\ demonstrates
    the linear tendency and the motivation to use \\linear interpolation}
   
   \label{fig:lineartendency} 
\end{subfigure}

\begin{subfigure}[b]{0.55\textwidth}
   \includegraphics[width=0.85\columnwidth]{interpolate.eps}
   \caption{RMSE trend when interpolation is used to \\fill the missing gaps}
   \label{fig:interpolate}
\end{subfigure}
\end{figure}

% \begin{figure}[htbp]
% \includegraphics[width=\columnwidth]{interpolate.eps}
% \caption{RMSE trend when interpolation is used to fill the missing gaps}
% \label{fig:interpolate}
% \end{figure}

\subsection{Data Scaling}
Historically, data scaling is performed using a minimum maximum scaler that clamps the data between 0 and 1 \cite{biderectionallstm}. This would work best and provide a lower RMSE value. But as we observed, the data isn't exactly between a small range which can translate to being normalized between 0-1. Such a scaling would most likely lead to information loss. Table \ref{table:datacharac} shows the minimum and maximum ranges of the \SMART features. Maximum values of the order of $10^{14}$ would mean that the range is big and cannot be scaled to [0, 1] without potential information loss. Adopting the normalization technique proposed by Backblaze \cite{beach_2021}, we normalized and scaled the data between [0, 255] and we believe this provides more information for the model to learn and predict without bias. 

\begin{table*}[htbp]
\centering
\caption{This table shows the characteristics of the important features. It displays the minimum, maximum, mean, median, and standard deviation of the raw values of the \SMART features}
\label{table:datacharac}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{S.M.A.R.T. Feature Name} & \textbf{Minimum}    & \textbf{Maximum}     & \textbf{Mean}        & \textbf{Median}      & \textbf{Standard Deviation} \\ \hline
S.M.A.R.T. 1                     & $0$                 & $2.4 \times 10^{8}$  & $1.2 \times 10^{8}$  & $1.2 \times 10^{8}$  & $7 \times 10^{7}$           \\ \hline
S.M.A.R.T. 4                     & $1$                 & $2.7 \times 10^{3}$  & $11$                 & $9$                  & $17$                        \\ \hline
S.M.A.R.T. 5                     & $0$                 & $6.5 \times 10^{4}$  & $1.5 \times 10^{2}$  & $0$                  & $1.9 \times 10^{3}$         \\ \hline
S.M.A.R.T. 7                     & $0$                 & $2.8 \times 10^{14}$ & $1.4 \times 10^{11}$ & $4.7 \times 10^{8}$  & $5.6 \times 10^{12}$        \\ \hline
S.M.A.R.T. 9                     & $0$                 & $6.7 \times 10^{4}$  & $2.5 \times 10^{4}$  & $2.3 \times 10^{4}$  & $1.6 \times 10^{4}$         \\ \hline
S.M.A.R.T. 188                   & $0$                 & $1 \times 10^{13}$   & $1.2 \times 10^{9}$  & $0$                  & $1 \times 10^{11}$          \\ \hline
S.M.A.R.T. 190                   & $13$                & $1.4 \times 10^{2}$  & $25$                 & $24$                 & $4.9$                       \\ \hline
S.M.A.R.T. 192                   & $0$                 & $1.2 \times 10^{4}$  & $2.1$                & $0$                  & $26$                        \\ \hline
S.M.A.R.T. 193                   & $2$                 & $1.6 \times 10^{6}$  & $2.9 \times 10^{4}$  & $1.2 \times 10^{4}$  & $4.8 \times 10^{4}$         \\ \hline
S.M.A.R.T. 194                   & $13$                & $1.4 \times 10^{2}$  & $25$                 & $24$                 & $4.9$                       \\ \hline
S.M.A.R.T. 197                   & $0$                 & $6.4 \times 10^{4}$  & $25$                 & $0$                  & $6.6 \times 10^{2}$         \\ \hline
S.M.A.R.T. 198                   & $0$                 & $6.4 \times 10^{4}$  & $24$                 & $0$                  & $6.5 \times 10^{2}$         \\ \hline
S.M.A.R.T. 199                   & $0$                 & $5.4 \times 10^{3}$  & $3.6$                & $0$                  & $1.1 \times 10^{2}$         \\ \hline
S.M.A.R.T. 241                   & $0$                 & $8 \times 10^{10}$   & $3.6 \times 10^{10}$ & $3 \times 10^{10}$   & $1.8 \times 10^{10}$        \\ \hline
S.M.A.R.T. 242                   & $1.2 \times 10^{3}$ & $1.6 \times 10^{12}$ & $1.7 \times 10^{11}$ & $1.5 \times 10^{11}$ & $1.3 \times 10^{11}$        \\ \hline
\end{tabular}
\end{table*}

% \begin{figure}[h!]
% \includegraphics[width=\columnwidth]{fulldata_difference_between_scaling_2.png}
% \caption{Training Log RMSE with scaling between [0, 1] and [0, 255]}
% \label{fig:scaling}
% \end{figure}

% As you can see from Fig. \ref{fig:scaling}, the data scaling between [0, 1] reported an RMSE of 0 quite fast. This indicates that the model was easily able to learn the features. This can be deceptive if the range of values is high and it is scaled down between [0, 1] which could lead to information loss.

\begin{figure*}[h!]
 \centering
\includegraphics[width=\textwidth]{windows_1.eps}
\caption{Training RMSE with varying window sizes}
\label{fig:window}
\end{figure*}

\section{The Machine Learning Approach}
\label{section:lstm}
The Remaining useful life prediction extends to a time series forecasting problem. In order to make forecasts and guide strategic decision-making, time series forecasting involves studying time series data using statistics and modeling. Recently, Machine Learning approaches combined with neural networks have been extensively used for time series forecasting \cite{HEWAMALAGE2021388}. Through time, LSTMs have gained popularity for such problems. The Long Short-Term Memory network (LSTM)  is a type of recurrent neural network used in deep learning because it can handle sequence dependence efficiently and successfully. 

A typical LSTM block contains 3 gates, \textit{input gate} ($i_t$), \textit{forget gate} ($f_t$) and \textit{output gate} ($o_t$). LSTM also contains a \textit{memory cell} ($C_t$) introduced as a measure to counter the problem of vanishing/exploding gradient.  

\begin{equation}
f_t = \sigma_g(W_f h_{t-1} + U_f x_t + b_f) 
\end{equation}

\begin{equation}
i_t = \sigma_g(W_i h_{t-1} + U_i x_t + b_i) 
\end{equation}

\begin{equation}
\tilde{C}t = \tanh(W_C h{t-1} + U_C x_t + b_C) 
\end{equation}

\begin{equation}
C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}t 
\end{equation}

\begin{equation}
o_t = \sigma_g(W_o h{t-1} + U_o x_t + b_o) 
\end{equation}

\begin{equation}
h_t = o_t \odot \tanh(C_t)
\end{equation}

where:
\begin{enumerate}
\item $x_t$ is the input at time $t$
\item $h_{t-1}$ is the hidden state at time $t-1$
\item $f_t$ is the forget gate output at time $t$
\item $i_t$ is the input gate output at time $t$
\item $\tilde{C}_t$ is the candidate cell state at time $t$
\item $C_t$ is the cell state at time $t$
\item $o_t$ is the output gate output at time $t$
\item $h_t$ is the hidden state output at time $t$
\end{enumerate}


We employ an advanced LSTM architecture called Encoder-Decoder LSTM \cite{NIPS2014_a14ac55a} or Seq2Seq LSTM for RUL prediction.
\subsection{Encoder-Decoder LSTM (Sequence to Sequence LSTM)}
LSTM networks are recurrent neural networks that are capable of recording temporal relationships in time series data. They are made up of cells and hidden states that enable the accounting of long and short-term memory impacts. A big improvement in language-to-language translation was achieved by Encoder-Decoder LSTM \cite{cho-etal-2014-learning}. The Encoder-Decoder (ED) structure employs two LSTM networks in the encoder and decoder portions. This framework enables the model to manage a variety of input and output time steps, which can be helpful for ahead-of-time forecasting. In this model, the encoder layer only emits the last cell's hidden state, which is then used as input for each LSTM cell in the decoder layer, allowing information from the input series to be collected at each time step. In comparison to the regular LSTM, structure improves long-term dependencies for extended time step forecasts. This research used five extra dense layers after the LSTM decoder layer to improve the decoding of sequence output at each time increment. 

\begin{figure}[h!]
\includegraphics[width=1\columnwidth]{LSTMED.eps}
\caption{The architecture of an encoder-decoder LSTM with an LSTM cell in focus }
\label{fig:architecture}
\end{figure}

Building upon the existing knowledge, one could leverage the latent space encoding of past observations of S.M.A.R.T. to incorporate more contextual information for improved RUL forecasting. This can be achieved by first encoding the S.M.A.R.T values from X days into a latent space using an encoder, then utilizing the encoded past as a "context" in the decoder to improve RUL predictions. The decoder could be modified to take the encoded past as input in addition to the temporal data, thereby allowing for a richer representation of past patterns to inform future predictions. By incorporating this additional information, the model may be better equipped to handle more complex and nuanced patterns in the data, potentially leading to more accurate RUL forecasts. This also opens the avenue to look at attention mechanisms to further enhance the training and build upon this model. Fig. \ref{fig:end-dec} describes the idea and the approach that the rest of the paper takes while building the model.

\begin{figure}[h!]
\includegraphics[width=\columnwidth]{Encoder-Decoder.eps}
\caption{The architecture with the inputs and outputs customized for the problem}
\label{fig:end-dec}
\end{figure}

% \begin{figure*}[htbp]
%  \centering
% \includegraphics[height=0.65\textwidth]{RUL_4.eps}
% \caption{Predicted vs. Expected RUL on a test set for the models mentioned in Table \ref{table:configurations} }
% \label{fig:Ng2}
% \end{figure*}

\begin{figure*}[htbp]
\centering
\begin{subfigure}[b]{\textwidth}
        \includegraphics[width=\columnwidth]{RULL_3_25.eps}
   \caption{Experiments run with a single encoder-decoder with varying LSTM units}
\end{subfigure}
\\~\\
\begin{subfigure}[b]{\textwidth}
   \includegraphics[width=\columnwidth]{RULL_3_d4.eps}
   % [width=0.85\columnwidth]
   \caption{Experiments run with encoder-decoder LSTM with multiple layers}
\end{subfigure}
\caption{Predicted vs. Expected RUL on a test set for the models mentioned in Table \ref{table:configurations} }
\label{fig:Ng2}
\end{figure*}


\begin{figure*}[htbp]
 \centering
\includegraphics[width=0.85\textwidth]{generalize.eps}
\caption{Predicted vs Expected RUL on different models of the Seagate family }
\label{fig:generalize}
\end{figure*}


\section{Experiments}
\label{section:experiments}
In the above section, we explain the data pre-processing stages and pipeline. Deep neural networks can be strenuous while training large amounts of data. The perfect batch size and techniques like early stopping \cite{Prechelt1996EarlySW} can help train LSTMs with little or no overfitting. We build multiple Encoder-Decoder LSTMs and analysis the impact on the data. In this section, we break it into different subsections where we first analyze thoroughly a subset of data and then extend it to the complete dataset. 

For the analysis of a subset of the data, we look at 2022 Q1 and Q2 data, with the most failing model (ST4000DM000). Typically, an LSTM model requires data to be in the shape of [samples x timesteps x features]. \cite{biderectionallstm} describe in brief what each of the parameters means.

\subsection{Analysis on 2022 Q1 \& Q2 Data}
To understand how an ED-based LSTM understands the trends of hard-drive sequences, we conduct an analysis of a subset of data to gain insights that can be extended to the larger set. To evaluate the patterns and trends in the data, we selected 2022 Q1 and Q2 data. The primary goal of the analysis was to find connections and relationships between variables, spot patterns in the data, and investigate possible causes and effects of the observed trends. These insights may aid in the identification of areas of interest for further study or the formulation of possible answers to issues within the dataset. We discovered many fundamental problems, such as data scaling, window size configuration, and how to create sequences to conduct useful analysis on the entire dataset. We discovered that ED-LSTMs can comprehend time series data by learning patterns, and dependencies across time, and retrieving the context encoded in the encoder to predict the RUL sequences.

\subsection{Varying window sizes on 2022 Q1 \& Q2 Data}
\cite{biderectionallstm, hddpaper1, hddpaper2} explore the time-series variation of the S.M.A.R.T. features across a single-sized window. By exploring different window sizes or time steps, an analysis of how the sequence dependence affects the training can be done. We look at different time steps of 5, 10, 15, 20, 25, and 30. 

To make it robust and analyze it better, we run the model with cross-validation evaluation metrics. Our metrics to find the perfect time-step was looking at how each cross-fold varies with training RMSE. In Fig. \ref{fig:window}, window size 25 is a perfect time-step that provides variability across multiple training sessions along with a large enough window to predict remaining useful life. 5 or 10 time-step is a very small window to predict whilst 30 is too extreme on the variability of training. 


\subsection{Different Encoder-Decoder LSTM models}

In this subsection, we investigate the impact of different configurations of Long Short-Term Memory (LSTM) units on the performance of encoder-decoder models. Specifically, we explore six different configurations described in Table \ref{table:configurations}, varying the number of LSTM units in the Encoder and Decoder layers, as well as the number of layers in the Encoder and Decoder. These configurations include 50 LSTM units in both Encoder and Decoder, 100 LSTM units in both, and 200 LSTM units in both. This configuration is a shallower network with just one layer of each kind. Additionally, we investigate the impact of using a single-layer Encoder and multi-layer Decoder, a multi-layer Encoder and single-layer Decoder, and a multi-layer Encoder and multi-layer Decoder. We hypothesize that increasing the number of LSTM units in the Encoder and Decoder layers will improve the model's ability to capture complex relationships between input and output sequences.  To evaluate the performance of these configurations, we ran these models on the Seagate model ST4000DM000. On investigation, we noticed the RMSE values to be lower for the multi-layer configurations but that does not translate to accurate predictions as depicted in Fig. \ref{fig:Ng2}. This points to the conclusion that lower LSTM units with a shallower configuration work best. In the later sections, we confine the experiments to a single-layer encoder and decoder with 50 LSTM units.

% \begin{figure}[htbp]
% \centering
% \begin{subfigure}[b]{0.55\textwidth}
%         \includegraphics[height=6cm]{modelss_1.eps}
%    \caption{}
   
%    \label{fig:Ng1} 
% \end{subfigure}

% \begin{subfigure}[b]{0.55\textwidth}
%    \includegraphics[width=0.88\columnwidth]{RUL_2.eps}
%    \caption{}
%    \label{fig:Ng2}
% \end{subfigure}

\begin{figure}
    \centering
     \includegraphics[width=\columnwidth]{modelss.eps}
    \caption{RMSE training curve for the different kinds of Encoder-Decoder models, this figure shows the robustness of using a shallower network compared to a deeper network.}
    \label{fig:Ng1}
\end{figure}


\begin{table}[htbp]
\centering
\caption{Configurations of the encoder-decoder models used}
\label{table:configurations}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{\begin{tabular}[c]{@{}c@{}}Serial \\ Number\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Number of LSTM \\ Units per layer\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Number of \\ Encoder Layers\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Number of \\ Decoder Layers\end{tabular}} \\ 
\hline
\hline
1 & 50 & 1 & 1 \\ \hline
2 & 100 & 1 & 1 \\ \hline
3 & 200 & 1 & 1 \\ \hline
4 & 100 & 1 & 3 \\ \hline
5 & 100 & 3 & 1 \\ \hline
6 & 100 & 2 & 2 \\ \hline
\end{tabular}
\end{table}

We tabulate the RMSE scores for the different configurations presented in Table \ref{table:configurations}. The below table gives an overview of how the 50 single-layer encoder and decoder model performs the best with lower RMSE values and with [0, 255] scaling. 

\begin{table}[!htbp]
\centering
\caption{RMSE scores tabulated for the different configurations of the models used. The serial numbers of the model can be referenced from Table \ref{table:configurations}. }
\label{tab:diffmodelsresult}
\begin{tabular}{|c|ccc|}
\hline
\textbf{Model serial number} & \multicolumn{3}{c|}{\textbf{Log RMSE}}                                                                                    \\ \hline
\textit{\textbf{}}           & \multicolumn{1}{c|}{\textit{\textbf{Train}}} & \multicolumn{1}{c|}{\textit{\textbf{Validation}}} & \textit{\textbf{Test}} \\ \hline
\hline
1                            & \multicolumn{1}{c|}{\textbf{0.83}}                    & \multicolumn{1}{c|}{\textbf{0.75}}                         & \textbf{0.86 }                  \\ \hline
2                            & \multicolumn{1}{c|}{1.42}                    & \multicolumn{1}{c|}{1.41}                         & 1.17                   \\ \hline
3                            & \multicolumn{1}{c|}{1.93}                    & \multicolumn{1}{c|}{1.55}                         & 4.92                   \\ \hline
4                            & \multicolumn{1}{c|}{1.36}                    & \multicolumn{1}{c|}{2.51}                         & 1.49                   \\ \hline
5                            & \multicolumn{1}{c|}{2.05}                    & \multicolumn{1}{c|}{2.04}                         & 2.34                   \\ \hline
6                            & \multicolumn{1}{c|}{1.39}                    & \multicolumn{1}{c|}{2.11}                         & 2.10                   \\ \hline
\end{tabular}
\end{table}

\subsection{Generalizability of the proposed model}
Machine learning models need to be able to perform well on new data that they have not previously encountered, which is why generalizability is a crucial concept in this field. A model that possesses strong generalizability can accurately predict outcomes on unseen data. The primary goal of machine learning training is to learn patterns and correlations in the data that can be extended to new data. It is essential to note, however, that obtaining excellent generalizability is not always simple, and can be affected by a number of variables such as the quality and amount of training data, the intricacy of the model, and the nature of the issue being addressed.

\begin{table}[htbp]
\centering
\caption{Metrics (R2 Score and RMSE) for different Seagate Models to depict generalizability}
\label{table:metrics}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Seagate HDD Model} & \textbf{R2 Score} & \textbf{RMSE} \\ \hline
ST4000DM000                & 0.98              & 1.044         \\ \hline
ST12000NM0007              & 0.87              & 2.59          \\ \hline
ST3000DM001                & 0.51              & 5.7           \\ \hline
ST8000NM0055               & 0.6               & 4.52          \\ \hline
ST8000DM002                & 0.68              & 4.08          \\ \hline
ST12000NM0008              & 0.62              & 4.49          \\ \hline
ST31500541AS               & 0.66              & 4.22          \\ \hline
ST31500341AS               & 0.58              & 4.92          \\ \hline
\end{tabular}
\end{table}

In this case, the generalizability is challenged on how the model trained a specific Seagate model behaves on other models of the Seagate family. We train the Encoder-Decoder LSTM model on ST4000DM000 and test the same model on the other Seagate models mentioned in Table \ref{table:failures}. We observe that at a certain point, and as the number of failures occurring in the Seagate model decreases, the generalizability suffers. We observe higher RMSE and lower R2 scores for those particular models. These observations are tabulated in Table \ref{table:metrics}. With the two models ST4000DM000 and ST12000NM0007
having failures of 4934 and 2010 respectively, the predicted RUL (seen in Fig. \ref{fig:generalize} is close to the expected values). As we move to the other Seagate models, with decreasing failures. The model suffers in its ability to generalize. The predicted RUL is off from the expected RUL.

\section{Future Work}
\label{section:future}
% // Rephrase to make it better

Several topics for future study in hard drive failure prediction using machine learning can be investigated. One avenue of investigation could be to broaden the models' generalizability beyond the Seagate hard drive family. This would entail testing current models against data from other makers and potentially retraining the models with the new data to improve performance.

Furthermore, investigating the possibility of offline and online training for hard drive failure prediction models could be a fascinating field of study. This would entail looking into how the models work when taught offline and then put on peripheral devices for online training, or when learned using federated learning methods. Another field of investigation could be attention-based LSTMs for hard drive failure forecasts. This would entail examining the efficacy of attention processes in capturing pertinent features and enhancing the models' general performance.

Future studies could also look into the stability of LSTM models when working with threat actor networks. This would entail running the models against data containing attack scenarios and evaluating their success in such scenarios. There may be opportunities to leverage recent advancements in explainable AI to provide interpretable insights into the factors driving hard drive failures. By using techniques such as attention visualization or sensitivity analysis, it may be possible to identify specific features or time intervals that are particularly predictive of failure. This could lead to a better understanding of the underlying mechanisms of hard drive failure and more effective maintenance strategies.

Lastly, future research could look into the use of ensemble networks to forecast hard drive failures from other makers. This would entail training numerous models on data from various makers and then combining their forecasts to improve total performance.

\section{Conclusion}
This paper employs large-scale predictive analyses using customized feature engineering and sequence learners to develop an encoder-decoder LSTM model that predicts the number of days remaining before a disk potentially fails. The models developed in this work were trained and tested on 10 years of \SMART health data from Backblaze, and the results show that the encoder-decoder LSTM approach outperforms in terms of generalizability over other Seagate family hard drives. The paper presents tangible metrics for evaluation and generalization for practitioners looking to extend their workflow to all years of health data in circulation across disk manufacturers. Overall, the paper contributes to closing the knowledge gap on what full-scale training achieves on thousands of devices and advances the state-of-the-art in predicting the end-of-life of hard disk drives.

// Needs work


\section*{Acknowledgment}
The research reported in this publication was supported by the Division of Research and Innovation at San Jos\'e State University under Award Number 23-UGA-08-044. The content is solely the responsibility of the author(s) and does not necessarily represent the official views of San Jos\'e State University.

\bibliographystyle{IEEEtran} % We choose the "plain" reference style
\bibliography{main}


\end{document}
