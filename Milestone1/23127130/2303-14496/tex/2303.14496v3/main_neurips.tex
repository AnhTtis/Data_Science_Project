\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
    \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023

% \usepackage{natbib}
% ready for submission
% \usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
    \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
% \usepackage{hyperref}
\usepackage{xcolor}
\definecolor{myblue}{rgb}{0.0,0.18,0.65}
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false,citecolor=myblue,linkcolor=myblue]{hyperref}       % hyperlinks
% \usepackage[hidelinks]{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[final]{neurips_2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}


%\usepackage[tight]{subfigure}
\usepackage{graphicx}
\usepackage{appendix}


\usepackage{mdwlist}
\usepackage{xspace}

\usepackage{color}
\usepackage{mathrsfs}

\RequirePackage{algorithm}
\RequirePackage{algorithmic}


\usepackage{booktabs}
\usepackage{comment}
%\usepackage{geometry}

\usepackage{multirow}
\usepackage[normalem]{ulem}


\newcommand{\ind}{\mathbb{I}}

% Functions using mathrm
\renewcommand{\dim}{\mathrm{dim}}
\newcommand{\OPT}{\textup{\textsf{OPT}}}
\newcommand{\range}{\mathcal{range}}
\newcommand{\sign}{\textup{\textsf{sign}}}
\newcommand{\sgn}{\textup{\textsf{sign}}}
\newcommand{\diag}{\textsf{Diag}}
\newcommand{\ber}{\textup{\textsf{Ber}}}
\newcommand{\err}{\mathrm{err}}
\newcommand{\adv}{\mathrm{adv}}
\newcommand{\nat}{\mathrm{nat}}
\newcommand{\greedy}{\mathrm{greedy}}
\newcommand{\opt}{\mathrm{opt}}
\newcommand{\abstain}{\mathrm{abstain}}
\newcommand{\gen}{(\frac{\nu}{12})}
\newcommand{\error}{\mathrm{err}}
\newcommand{\hinge}{\mathrm{hinge}}
\newcommand{\minimax}{\mathrm{minimax}}
\newcommand{\boundary}{\mathrm{DB}}
\newcommand{\erf}{\mathrm{erf}}
\newcommand{\ERM}{\mathrm{ERM}}
\newcommand{\Appendix}[1]{the full version for}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

%\newcommand{\fix}{\marginpar{FIX}}
%\newcommand{\new}{\marginpar{NEW}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{claim}{Claim}
\newtheorem{fact}{Fact}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{condition}{Condition}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
% \renewcommand{\a}{\mathbf{a}}
% \renewcommand{\b}{\mathbf{b}}
% \renewcommand{\c}{\mathbf{c}}
% \newcommand{\e}{\mathbf{e}}
% \newcommand{\g}{\mathbf{g}}
\renewcommand{\u}{\bm{u}}
% \renewcommand{\v}{\mathbf{v}}
\newcommand{\w}{\bm{w}}
\newcommand{\x}{\bm{x}}
\newcommand{\y}{\bm{y}}
% \newcommand{\z}{\mathbf{z}}
% \newcommand{\A}{\mathbf{A}}
% \newcommand{\B}{\mathbf{B}}
% \newcommand{\C}{\mathbf{C}}
% \newcommand{\D}{\mathbf{D}}
% \newcommand{\E}{\mathbf{E}}
% \newcommand{\F}{\mathbf{F}}
% \newcommand{\G}{\mathbf{G}}
% \renewcommand{\H}{\mathbf{H}}
% \newcommand{\I}{\mathbf{I}}
% \newcommand{\K}{\mathcal{K}}
% \renewcommand{\L}{\mathbf{L}}
% \newcommand{\M}{\mathbf{M}}
% \newcommand{\N}{\mathcal{N}}
% \renewcommand{\P}{\mathcal{P}}
% \newcommand{\Q}{\mathbf{Q}}
\newcommand{\R}{\mathbb{R}}
% \renewcommand{\S}{\mathbf{S}}
% \newcommand{\T}{\mathbf{T}}
% \newcommand{\U}{\mathbf{U}}
% \newcommand{\V}{\mathbf{V}}
% \newcommand{\W}{\mathbf{W}}
% \newcommand{\X}{\mathbf{X}}
% \newcommand{\Y}{\mathbf{Y}}
% \newcommand{\Z}{\mathbf{Z}}
\newcommand{\rank}{\textup{\textsf{rank}}}
\newcommand{\orthc}{\mathbf{orth}_c}
\newcommand{\orthr}{\mathbf{orth}_r}
\newcommand{\bLambda}{\mathbf{\Lambda}}
\newcommand{\RS}{\mathcal{R}}
\newcommand{\0}{\mathbf{0}}
\newcommand{\1}{\mathbf{1}}
\renewcommand{\comment}[1]{}
\newcommand{\red}[1]{{\color{red}#1}}
% \usepackage[dvipsnames]{xcolor}
% \newcommand{\blue}[1]{{\color{blue}#1}}
\newcommand{\blue}[1]{{\color{black}#1}}
\newcommand{\cready}[1]{{\color{black}#1}}
\newcommand{\arxiv}[1]{{\color{black}#1}}


\newcommand{\tr}{\textsf{tr}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cZ}{\mathcal{Z}}
\newcommand{\bbB}{\mathbb{B}}
\newcommand{\bbI}{\mathbb{I}}
\newcommand{\bbE}{\mathbb{E}}
\newcommand{\bbS}{\mathbb{S}}
\newcommand{\Pro}{\text{Pro}}
\newcommand{\imperceptible}{\mathsf{imperceptible}}
\newcommand{\dist}{\mathsf{dist}}
\newcommand{\spann}{\mathsf{span}}
\newcommand{\vol}{\mathsf{vol}}
\newcommand{\Null}{\mathsf{null}}
\newcommand{\Area}{\mathsf{Area}}

\definecolor{colorY}{rgb}{0.7 , 0.7 , 0.2}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\newenvironment{proofoutline}{\noindent{\emph{Proof Sketch. }}}{\hfill$\square$\medskip}


\renewcommand{\baselinestretch}{0.98}
\linepenalty=1000

\title{Learning with Explanation Constraints}


% \author{%
  % David S.~Hippocampus\thanks{Use footnote for providing further information
  %   about author (webpage, alternative address)---\emph{not} for acknowledging
  %   funding agencies.} \\
  % Department of Computer Science\\
  % Cranberry-Lemon University\\
  % Pittsburgh, PA 15213 \\
  % \texttt{hippo@cs.cranberry-lemon.edu} \\

  % \author{Rattana Pukdee$^{1}$\footnote{Equal contribution} , Dylan Sam$^{1*}$, J. Zico Kolter$^{1,2}$,\\ Maria-Florina Balcan$^{1}$, Pradeep Ravikumar$^{1}$ \\
  %   \quad\\
  %   $^{1}$Carnegie Mellon University\\
  %   $^{2}$Bosch Center for AI
  %   }
\author{
    Rattana Pukdee\thanks{Equal contribution} \\
    Carnegie Mellon University \\
    \texttt{rpukdee@cs.cmu.edu}
    \And 
    Dylan Sam$^{*}$ \\
    Carnegie Mellon University \\
    \texttt{dylansam@andrew.cmu.edu}
    \And
    J. Zico Kolter\\
    Carnegie Mellon University\\
    Bosch Center for AI \\
    \texttt{zkolter@cs.cmu.edu}
     \And Maria-Florina Balcan \\
     Carnegie Mellon University\\
     \texttt{ninamf@cs.cmu.edu}
     \And Pradeep Ravikumar \\
     Carnegie Mellon University\\
     \texttt{pkr@cs.cmu.edu}
  }
  
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
% }




\begin{document}

\maketitle

\begin{abstract}

As larger deep learning models are hard to interpret, there has been a recent focus on generating explanations of these black-box models. 
In contrast, we may have apriori explanations of how models should behave. In this paper, we formalize this notion as learning from \emph{explanation constraints} and provide a learning theoretic framework to analyze how such explanations can improve the learning of our models. 
One may naturally ask, ``When would these explanations be helpful?"
Our first key contribution addresses this question via a class of models that satisfies these explanation constraints in expectation over new data. We provide a characterization of the benefits of these models (in terms of the reduction of their Rademacher complexities) for a canonical class of explanations given by gradient information in the settings of both linear models and two layer neural networks. In addition, we provide an algorithmic solution for our framework, via a variational approximation that achieves better performance and satisfies these constraints more frequently, when compared to simpler augmented Lagrangian methods to incorporate these explanations. We demonstrate the benefits of our approach over a large array of synthetic and real-world experiments.

\end{abstract}

\section{Introduction}

There has been a considerable recent focus on generating explanations of complex black-box models so that humans may better understand their decisions. \cready{These can take the form of feature importance \citep{ribeiro2016should, smilkov2017smoothgrad}, counterfactuals \citep{ribeiro2016should, smilkov2017smoothgrad}, influential training samples \citep{koh2017understanding,yeh2018representer}, etc.} But what if humans were able to provide explanations for how these models should behave? We are interested in the question of how to learn models given such apriori explanations.
% This can take many forms such as feature importance \citep{ribeiro2016should, smilkov2017smoothgrad, lundberg2017unified,sundararajan2017axiomatic}, high level concepts \citep{kim2018interpretability,yeh2020completeness}, counterfactual examples \citep{wachter2017counterfactual,goyal2019counterfactual, mothilal2020explaining}, robustness of gradients \citep{wicker2022robust}, or influential training samples~\citep{koh2017understanding,yeh2018representer}.
\blue{A recent line of work incorporates explanations as a regularizer, penalizing models that do not exhibit apriori given explanations \citep{ross2017right, rieger2020interpretations, ismail2021improving, stacey2022supervising}. For example, \citet{rieger2020interpretations} penalize the feature importance of spurious patches on a skin-cancer classification task. These methods lead to models that inherently satisfy ``desirable'' properties and, thus, are more trustworthy. In addition, some of these empirical results suggest that constraining models via explanations also leads to higher accuracy and robustness to changing test environments. However, there is no theoretical analysis to explain this phenomenon.} 

\cready{
We note that such explanations can arise from domain experts and domain knowledge, but also other large ``teacher'' models that might have been developed for related tasks. An attractive facet of the latter is that we can automatically generate model-based explanations given unlabeled data points. For instance, we can use segmentation models to select the background pixels of images solely on unlabeled data, which we can use in our model training. We 
 thus view incorporating explanation constraints from such teacher models as a form of knowledge distillation into our student models \cite{hinton2015distilling}. 


% However, using the teacher model solely to generate pseudolabels may not be the most efficient approach. Some models, like segmentation models, can provide useful explanations but cannot generate pseudolabels due to their different output spaces. For neural network-based models, using logits instead of discrete labels, as suggested by \citet{hinton2015distilling}, is a better approach. In general, we can see incorporating explanations involves using complex explanation functionals to regularize the model.
}


% Simply using the teacher model to generate pseudolabels is not the most efficient way to use the information from the teacher model \citep{hinton2015distilling}.
% Moreover, there are models that can provide useful explanations but cannot generate pseudolabels such as segmentation models that have a different output spaces. For the specific case of neural network-based models, \citet{hinton2015distilling} suggested using logits rather than discrete labels. As such, incorporating explanations in general can be thought of using complex explanation functionals instead of pre-softmax logits.

% As \citet{hinton2015distilling} showed, merely using teacher models to generate pseudolabels on unlabeled data does not efficiently use information from the teacher model. For the specific case of neural network-based models, \citet{hinton2015distilling} suggested using logits rather than discrete labels. As such, incorporating explanations can be thought of using complex explanation functionals instead of pre-softmax logits. 

% We also note that some models such as segmentation models can provide useful explanations but cannot generate pseudolabels since they have a different output space.

% There has been a considerable recent focus on generating explanations of complex black-box models so that humans may better understand their decisions. But what if humans were able to provide explanations for how these models should behave? We are interested in the question of how to learn models given such apriori explanations. We note that such explanations can arise from domain experts and domain knowledge, but also other large ``teacher'' models that might have been developed for related tasks. An attractive facet of the latter is that we can automatically generate such model based explanations given unlabeled data points.  We could thus view incorporating explanation constraints generated by such teacher models as a form of distillation of knowledge from these teacher models into our student models. As \citet{hinton2015distilling} showed, merely using teacher models to generate pseudo-labels on unlabeled data does not efficiently use information from the teacher model. For the specific case of neural network based models, \citet{hinton2015distilling} suggested using the logits rather than discrete labels. We can view our framework as going further in using more complex explanation functionals than pre-softmax logits of the model. We also note that some models such as segmentation models can provide useful explanations but cannot generate pseudolabels since they have a different output space.

% \sout{We believe that learning from explanations is a natural characterization for training machine learning models as it matches how humans learn. For example, we learn math much better and more efficiently from a (good) teacher, who can explain the underlying principles and rules.} 
% \blue{A recent line of work incorporate explanations as a regularizer, penalizing models that do not exhibit apriori given explanations \citep{ross2017right, rieger2020interpretations, ismail2021improving, stacey2022supervising}. For example, \citet{rieger2020interpretations} penalize the feature importance of spurious patches on a skin-cancer classification task. These methods lead to models that inherently satisfy ``desirable'' properties and, thus, are more trustworthy. In addition, some of these empirical results suggest that constraining models via explanations also leads to higher accuracy and robustness to changing test environments. However, there is no theoretical analysis to explain this phenomenon.} 

% \sout{As labeled examples are provided by domain experts, we can also ask them to provide explanations for their decisions. While this indeed requires effort from the domain expert, these explanations can significantly improve the standard learning process, reducing the required number of labeled data. }

In this paper, we provide an analytical framework for learning from explanations \blue{to reason when and how explanations can improve the model performance}. We first provide a mathematical framework for model constraints given explanations. Casting explanations as functionals $g$ that take in a model $h$ and input $x$ (as is standard in explainable AI), we can represent domain knowledge of how models should behave as constraints on the values of such explanations. We can leverage these to then solve a constrained ERM problem where we additionally constrain the model to satisfy these explanation constraints.
% \sout{From an analysis standpoint, this poses challenges as these constraints are random; the explanations and constraints are provided on randomly sampled inputs. To handle these stochastic constraints, we draw from classical approaches in stochastic programming \citep{kall1994stochastic, birge2011introduction}. In particular, we formalize the class of what we term \emph{EPAC} models, or models that satisfy the explanation constraints (in expectation) up to some slack with high probability. Here, the probability is with respect to the randomness of the models themselves.}
\cready{Since the explanations and constraints are provided on randomly sampled inputs, these constraints are random.  Nevertheless, via standard statistical learning theoretic arguments \citep{valiant1984theory}, any model that satisfies the set of explanation constraints on the finite sample can be shown to satisfy the constraints in expectation up to some slack with high probability. In our work, \arxiv{we term a model that satisfies explanations constraints in expectation, an \textit{CE model} (see Definition \ref{def:EPAC}).} Then, we can capture the benefit of learning with explanation constraints by analyzing the generalization capabilities of this class of \arxiv{CE models} \blue{(Theorem \ref{thm: generalization bound})}. This analysis builds off of a learning theoretic framework for semi-supervised learning of \cready{Balcan and Blum} \citep{balcan2005pac, balcan2010discriminative}. 
\cready{We remark that if the explanation constraints are arbitrary, it is not possible to reason if a model satisfies the constraints in expectation based on a finite sample. We provide a detailed discussion on when this argument is possible in Appendix \ref{section: EPAC},\ref{appendix: EPAC learnable}}. \cready{In addition, we note that our work also has a connection with classical approaches in stochastic programming \citep{kall1994stochastic, birge2011introduction} and is worth investigating this relationship further.}}

% \cready{We draw from classical approaches in stochastic programming \citep{kall1994stochastic, birge2011introduction} to handle this technical challenge.}

% \cready{ (up) Our work has connection to stochastic programming and is worth investigating. also rephrase this thing about EPAC learnability}} 
% \sout{The high-level idea is that any model that satisfies the set of explanation constraints on the finite sample can be shown, via standard statistical learning theoretic arguments \citep{valiant1984theory}, to satisfy the constraints in expectation up to some slack with high probability.} 


\begin{figure*}[t]
\centering
\vspace{-2mm}
\includegraphics[width=\textwidth]{figs/bean_combine.pdf}
\vspace{-2mm}
\caption{A restricted hypothesis class $\cH_{\phi, \alpha}$ (left). Our algorithmic solution to solve a proposed variational objective in Section \ref{sec:variational_algo} (right).}
\vspace{-7mm}
\end{figure*}

Another key contribution of our work is concretely analyzing this framework for a canonical class of explanation constraints given by gradient information for linear models \blue{(Theorem \ref{theorem:bound_linear})} and two layer neural networks \blue{(Theorem \ref{theorem:bound_2layer_gradient})}. We focus on gradient constraints as we can represent many different notions of explanations, such as feature importance and ignoring spurious features. These corollaries clearly illustrate that restricting the hypothesis class via explanation constraints can lead to fewer required labeled data. \blue{Our results also provide a quantitative measure of the benefits of the explanation constraints in terms of the number of labeled data.} We also discuss when learning these explanation constraints makes sense or is possible (i.e., with a finite generalization bound). 
We note that our framework allows for the explanations to be noisy, and not fully satisfied by even the Bayes optimal classifier. Why then would incorporating explanation constraints help? As our analysis shows, this is by reducing the estimation error (variance) by constraining the hypothesis class, at the expense of approximation error (bias). We defer the question of how to explicitly denoise noisy explanations to future work.


Now that we have provided a learning theoretic framework for these explanation constraints, we next consider the algorithmic question: how do we solve for these explanation-constrained models? In general, these constraints are not necessarily well-behaved and are difficult to optimize. \blue{One can use augmented Lagrangian approaches \citep{ross2017right, fioretto2021lagrangian}, or simply regularized versions of our constrained problems \citep{rieger2020interpretations} (which however do not in general solve the constrained problems for non-convex parameterizations but is more computationally tractable). }
% We discuss additional related work in Appendix \ref{appendix: related work}.} 
% \sout{Our first algorithmic ingredient is the use of surrogate explanation losses that quantify how well a model satisfies an explanation constraint. Our second algorithmic ingredient relates to the fact that constrained model estimation is much less scalable in general than unconstrained estimation. One can use augmented Lagrangian approaches \citep{ross2017right, fioretto2021lagrangian}, or simply regularized versions of our constrained problems \citep{rieger2020interpretations} (which however do not in general solve the constrained problems for non-convex parameterizations). However, even these pose challenges for complex models and increasingly complex explanations (where even simple instances of the latter can involve the model's Jacobian). }
% In particular, note that computing gradients with respect to model parameters would entail differentiating through explanation surrogate loss and the explanation functional.
% , but this does not necessarily match the analytical framework from above, and potentially requires more complicated techniques in optimization regularization strength \citep{balcan2010discriminative}. 
We draw from seminal work in posterior regularization \citep{ganchev2010posterior}, which has also been studied in the capacity of model distillation \citep{hu2016harnessing}, to provide a variational objective. \blue{Our objective is composed of two terms; supervised empirical risk and the discrepancy between the current model and the class of \arxiv{CE models.} The optimal solution of our objective is also the optimal solution of the constrained problem which is consistent with our theoretical analysis. Our objective naturally incorporates unlabeled data and provides a simple way to control the trade-off between explanation constraints and the supervised loss (Section \ref{sec:variational_algo}). We propose a tractable algorithm that iteratively trains a model on the supervised data, and then approximately projects this learnt model onto the class of} \arxiv{CE models.}
Finally, we provide an extensive array of experiments that capture the benefits of learning from explanation constraints. These experiments also demonstrate that the variational approach improves over simpler augmented Lagrangian approaches and can lead to models that indeed satisfy explanations more frequently.

% \sout{We propose a tractable algorithm that iteratively trains a model on the supervised data, and then approximately projects this learnt model onto the set of those hypotheses that satisfy the explanation constraints. Finally, we provide an extensive array of experiments that capture the benefits of learning from explanation constraints. These experiments clearly illustrate our generalization bounds and also reveal fundamental tradeoffs about the design of explanation constraints. These experiments also demonstrate that the variational approach improves over simpler augmented Lagrangian approaches and can lead to models that indeed satisfy explanations more frequently.}



% \subsection{Related work}
% There is also prior work proposing learning objectives that incorporate rules into deep neural networks \citep{hu2016harnessing, fioretto2021lagrangian, seo2021controlling}. While \citep{hu2016harnessing} also leverages variational objectives, their method specifically concerns itself with logic rules and over probability distributions using KL divergence projections. 
% On the contrary, our approach handles more general forms of explanations and that naturally conforms to our theoretical framework. \blue{We refer additional related work to Appendix \ref{appendix: related work}.}


\section{Related Work}
\label{sec:relatedwork}

\cready{\textbf{Explainable AI.}} Recent advances in deep learning have led to models that achieve high performance but which are also highly complex \citep{lecun2015deep, goodfellow2016deep}. Understanding these complex models is crucial for safe and reliable deployments of these systems in the real-world. One approach to improve our understanding of a model is through explanations. This can take many forms such as feature importance \citep{ribeiro2016should, smilkov2017smoothgrad, lundberg2017unified,sundararajan2017axiomatic}, high level concepts \citep{kim2018interpretability,yeh2020completeness}, counterfactual examples \citep{wachter2017counterfactual,goyal2019counterfactual, mothilal2020explaining}, robustness of gradients \citep{wicker2022robust}, or influential training samples~\citep{koh2017understanding,yeh2018representer}.
% Once we generate explanations, human expert may decide whether the model is correct or not. These explanations can help us detect bugs [cite model debugging], provide a suggestion [counterfactual example] and build our trust with a model. Connecting explaination with use cases is an active line of research \cite{chen2022interpretable}. 

In contrast to generating post-hoc explanations of a given model, we aim to learn models given apriori explanations. There has been some recent work along such lines.  \citet{koh2020concept, zarlenga2022concept} incorporates explanations within the model architecture by requiring a conceptual bottleneck layer. \citet{ross2017right, rieger2020interpretations,ismail2021improving, stacey2022supervising}  use explanations to modify the learning procedure for any class of models: they incorporate explanations as a regularizer, penalizing models that do not exhibit apriori given explanations; \citet{ross2017right} penalize input gradients, while \citet{rieger2020interpretations} penalize a Contextual Decomposition score \citep{murdoch2018beyond}. Some of these suggest that constraining models via explanations leads to higher accuracies and more robustness to spurious correlation, but do not provide analytical guarantees. On the theoretical front, \citet{li2020learning} show that models that are easier to explain locally also generalize well. However, \citet{bilodeau2022impossibility} show that common feature attribution methods without additional assumptions on the learning algorithm or data distribution do no better than random guessing at inferring counterfactual model behavior.

\cready{\textbf{Learning Theory.}} Our contribution is to provide an analytical framework for learning from explanations that quantify the benefits of explanation constraints.   Our analysis is closely related to the framework of learning with side information. \citet{balcan2010discriminative} shows how unlabeled data can help in semi-supervised learning through a notion of compatibility between the data and the target model. This work studies classical notions of side information (e.g., margin, smoothness, and co-training). Subsequent papers have adapted this learning theoretic framework to study the benefits of representation learning \citep{garg2020functional} and transformation invariance \citep{shao2022a}. On the contrary, our paper focuses on the more recent notion of explanations. Rather than focus on the benefits of unlabeled data, we characterize the quality of different explanations. \cready{We highlight that constraints here are stochastic, as they depend on data points which differs from deterministic constraints that have been considered in existing literature, such as constraints on the norm of weights (i.e., L2 regularization). }


% provide concrete examples for explanations given by gradient information and noisy classifiers.
% studied in the context of semi-supervised learning . The seminal work studies the case when the side information is in the form of margin, smoothness and co-training. Subsequent papers have adapted this framework to study representation learning \citep{garg2020functional} and transformation invariance \citep{shao2022a}.

% In our paper, we focus on the recent notion of explanations.


% After this work, subsequent papers have adapted this framework to handle particular inductive biases, which we consider as specific instantiations of our setting. This includes \citep{shao2022a}, which studies this for transformation invariance, and \citep{garg2020functional} for representation learning.  


% We also remark that our framework is quite general and can handle constraints beyond those considered in the explainable AI community. For example, this is an immediate generalization of the framework for handling side information in semi-supervised learning \citep{balcan2010discriminative}, including notions of smoothness \citep{xiaojin2002learning, pukdee2022label}, transformation invariance \citep{shao2022a}, and representation learning \citep{garg2020functional}. It can also analyze cases of other real-world constraints, including that of physics rules \citep{NEURIPS2018_842424a1}.




 % However, there haven't been any theoretical guarantee on why and how explanation constraints can help us. Recent theoretical works have been focusing on axiomatic approach to explanations to propose explanations with different desirable property \citep{yeh2019fidelity}[cite chih kuan/ pradeep/ cite ameet and gregory plumb's paper] since it is known that many well-known explanations can have bad properties [cite adebayo]; saliency map of an untrained model looks similar to the trained one. There have been works connecting learning theory with explanation ML suggesting that  In this work, we hope to quantify the benefits of learning explanation constraints through the lense of learning theory.


% We leverage variational characterizations to scalably learn complex models under explanation constraints. 
\cready{\textbf{Self-Training.}} Our work can also be connected to the self-training literature \citep{chapelle2009semi,xie2020self,wei2020theoretical, frei2022self}, where we could view our variational objective as comprising a regularized (potentially simpler) teacher model that encodes these explanation constraints into a student model. Our variational objective (where we use simpler teacher models) is also related to distillation, which has also been studied in terms of gradients \citep{czarnecki2017sobolev}.










\section{Learning from Explanation Constraints}

Let $\cX$ be the instance space and $\cY$ be the label space. We focus on binary classification where $\cY = \{-1,1\}$, but which can be naturally generalized.  Let $\cD$ be the joint data distribution over $(\cX, \cY)$ and $\cD_\cX$ the marginal distribution over $\cX$. For any classifier $h : \cX \to \cY$, we are interested in its classification error $\err(h): = \Pr_{(x,y) \sim D}(h(x) \neq y)$, though one could also use other losses to define classification error. Our goal is to learn a classifier with small error from a family of functions $\cH$. \cready{In this work, we use the words model and classifier interchangeably}. Now, we formalize local explanations as functionals that take in a model and a test input, and output a vector:
%
\begin{definition}
[Explanations]
Given an instance space $\cX$,  model hypothesis class $\cH$, and an explanation functional $g: \cH \times \cX \to \R^r $, we say $g(h,x)$ is an explanation of $h$ on point $x$ induced by $g$. 
\end{definition}

 For simplicity, we consider the setting when $g$ takes a single data point and model as input, but this can be naturally extended to multiple data points and models.
We can combine these explanations with prior knowledge on how explanations should look like at sample points in term of constraints.
\begin{definition}
[Explanation Constraint Set]
For any instance space $\cX$, hypothesis class $\cH$, an explanation functional $g:\cH \times \cX \to \R^r$, and a family of constraint sets $\{C(x) \subseteq \R^r \mid x \in \cX\}$, we say that $h \in \cH$ satisfies the explanation constraints with respect to $C$ iff:
\begin{equation*}
    g(h,x) \in C(x), \; \forall x \in \cX.
\end{equation*}


\end{definition}
In our definition, $C(x)$ represents values that we believe our explanations should take at a point $x$. For example, ``an input gradient of a feature 1 must be larger than feature 2'' can be represented by $g(h,x) = \nabla_x h(x)$ and $C(x) = \{(x_1,\dots, x_d) \in \R^d \mid x_1 > x_2\}$.  In practice, human annotators will be able to provide the constraint set $C(x')$ for a random sample $k$ data points $S_E = \{ x'_1,\dots, x'_k\}$ drawn i.i.d. from $\cD_\cX$. We then say that any $h \in \cH$ $S_E$-satisfies the explanation constraints with respect to $C$ iff $g(h,x) \in C(x), \; \forall x \in S_E$. We note that the constraints depends on random samples $x'_i$ and therefore \emph{are random}. To tackle this challenge, we can draw from the standard learning theoretic arguments to reason about probably approximately satisfying the constraints in expectation. Before doing so, we first consider the notion of explanation surrogate losses, which will allow us to generalize the setup above to a form that is amenable to practical estimators.

\begin{definition}
(Explanation surrogate loss)
An explanation surrogate loss $\phi: \cH \times \cX \to \R$ quantifies how well a model $h$ satisfies the explanation constraint $g(h,x) \in C(x)$. For any $h \in \cH, x \in \cX$:
\begin{enumerate}
    \item $\phi(h,x) \geq 0$.
    \item If $g(h,x) \in C(x)$ then $\phi(h,x) = 0$.
\end{enumerate}
\end{definition}
For example, we could define $\phi(h, x) = 1\{g(h,x) \in C(x)\}.$ Given such a surrogate loss, we can substitute the explanation constraint that $g(h,x) \in C(x)$ with the surrogate $\phi(h,x) \le 0$.
We now have the machinery to formalize how to reason about the random explanation constraints given a random set of inputs. First, denote the expected explanation loss as $\phi(h,\cD) := \mathbb{E}_{x \sim \cD}[\phi(h,x)]$. We are interested in models that satisfy the explanation constraints up to some slack $\tau$ (i.e. approximately) in expectation. \cready{We define a learnability condition of this explanation surrogate loss as \arxiv{EPAC (Explanation Probably Approximately Correct ) learnability.}
}

\cready{
\begin{definition}
    [\arxiv{EPAC learnability}] For any $\delta \in (0,1), \tau > 0$, the sample complexity of $(\delta, \tau)$ - \arxiv{EPAC learning} of $\cH$ with respect to a surrogate loss $\phi$, denoted $m(\tau, \delta; \cH, \phi)$ is defined as the smallest $m\in \mathbb{N}$ for which there exists a learning rule $\cA$ such that every data distribution $\cD_\cX$ over $\cX$, with probability at least $1-\delta$ over $S\sim \cD^m$,
$$
\phi(\cA(S), \cD) \leq \inf_{h \in \cH} \phi(h, \cD) + \tau.    
$$
If no such $m$ exists, define $m(\tau, \delta; \cH, \phi) = \infty$. We say that $\cH$ is \arxiv{EPAC} learnable in the agnostic setting with respect to a surrogate loss $\phi$ if  $\; \forall \delta \in (0,1), \tau > 0$,  $m(\tau, \delta; \cH, \phi)$ is finite.

\arxiv{Furthermore, for a constant $\tau$, we denote any model $h\in \cH$ with $\tau$-Approximately Correct Explanation where $\phi(h,\cD) \le \tau$, with a $\tau$ - CE models. We define the class of \arxiv{$\tau$ - CE models} as }
\begin{equation}
\cH_{\phi,\cD,\tau} = \{h \in \cH \;:\; \phi(h,\cD)\le \tau\}. \label{def:EPAC}
\end{equation}
\end{definition}}

\cready{We simply use $\cH_{\phi, \tau}$ to denote this \cready{class of \arxiv{CE models}}}. From natural statistical learning theoretic arguments, a model that satisfies the random constraints in $S_E$ might also be a \arxiv{CE} model.  \blue{

\begin{proposition}\label{prop:epac-bound}
Suppose a model $h$ $S_E$-satisfies the explanation constraints then
$$\phi(h,\cD_\cX) \le 2R_k(\cG) + \sqrt{\frac{\ln(4/\delta)}{2k}},$$
 with probability at least $1 - \delta$, when $k = |S_E|$ and  $\cG = \{\phi(h, \cdot)\mid  h \in \cH\}$. 
\end{proposition}

}

We use $R_k(\cdot)$ to denote Rademacher complexity; please see Appendix~\ref{appendix: rademacher complexity} where we review this and related concepts. Note that even when $h$ satisfies the constraints exactly on $S$, we can only guarantee a bound on the expected surrogate loss $\phi(h,\cD_\cX)$.We can achieve a bound similar to that in Proposition~\ref{prop:epac-bound} via a single and simpler constraint on the empirical expectation $\phi(h,S_E) = \frac{1}{|S_E|}\sum_{x \in S_E} \phi(h, x)$. We can then extend the above proposition to show that if $\phi(h,S_E) \le \tau$, then $\phi(h,\cD_\cX) \le \tau + 2R_k(\cG) + \sqrt{\frac{\ln(4/\delta)}{2k}},$
 with probability at least $1 - \delta$. Another advantage of such a constraint is that the explanation constraints could be noisy, or it may be difficult to satisfy them exactly, so $\tau$ also serves as a slack. The class $\cG$ contains all surrogate losses of any $h\in \cH$. Depending on the explanation constraints, $\cG$ can be extremely large. \cready{
 We remark that the surrogate loss $\phi$ allows us to reason about satisfying an explanation constraint on a new data point and in expectation. However, for many constraints, $\phi$ does not have a closed-form or is unknown on an unseen data point. The question of which types of explanation constraints are generalizable may be of independent interest, and we further discuss this in Appendix \ref{section: EPAC} and provide further examples of learnable constraints in Appendix \ref{appendix: EPAC learnable}. }

% \subsection{Learning Objective}



\noindent\textbf{EPAC-ERM Objective.}
Let us next discuss \emph{combining} the two sources of information: the explanation constraints that we set up in the previous section, together with the usual set of labeled training samples $S = \{(x_1, y_1), \dots, (x_n, y_n) \}$ drawn i.i.d. from $\cD$  that informs the empirical risk. Combining these, we get what we call EPAC-ERM objective:
\begin{align}
    \min_{h \in \cH} \frac{1}{n}\sum_{i=1}^n \ell(h, x_i,y_i) \;
    \text{ s.t. } \;  \frac{1}{k}\sum_{i=1}^k \phi(h, x'_i) \leq \tau.
\label{eq:epac-erm}
\end{align}
\cready{We provide a learnability condition for a model that achieve both low average error and surrogate loss in Appendix \ref{appendix: EPAC-PAC}.}

\subsection{Generalization Bound}

% Under this formalization of explanations constraints, we provide a performance guarantee of the minimizer of the mentioned learning objective. At a high level, our analysis will start by only selecting classifiers that has low empirical explanation surrogate loss, and then perform standard supervised learning with the remaining set of classifiers. This follows the analysis from prior work on semi-supervised learning \citep{balcan2010discriminative}. 
% \red{main difference is blacan blum10 also note this in term of unlabeled error, only have a function in term of incompatibility, assume that the compatibility is something intrinsic to the underlying classifier}




% One main challenge in analyzing the benefits of explanations is that they are distribution-dependent.
% We can consider the simple case when the area that we have zero incompatibility $\phi(g(h,x),x)$ is the support of any $\operatorname{supp}(g(\cdot, x)) \subseteq \{a \mid \phi(a,x) = 0\}$ , so adding this constraint does not lead to any benefit. 
% For example, consider learning a linear separator that passes through the origin when data are drawn from the surface of a unit sphere (may not be drawn uniformly). In this case, gradient constraints tell us information on the direction of the optimal linear separator. 
% Given an interval that contains the optimal gradient, one may argue that a narrower interval is a better explanation constraint as we have restricted to a smaller concept class. However, we remark that the benefit of this explanation highly depends on the distribution of data. If all of our data naturally lies in that region restricted by an explanation, then we may gain no benefit from this explanation. 
% This has been noted in prior work that VC dimension analysis that does not take the data distribution into account lead to no performance gain \cite{balcan2010discriminative}. The prior work, instead, utilized techniques from Annealed VC Entropy \citep{Vayatis1999DistributionDependentVB} to deal with this issue. On the contrary, we provide distribution-dependent bounds through the Rademacher complexity in this work. 
% \begin{figure}[ht]
%     \centering
%     \includegraphics[width = 0.7\columnwidth]{pics/data-dependent-exp.jpeg}
%     \caption{ An interval of gradient can be useful (left) and not useful (right) depending on the data distribution.}
%     \label{fig:benefit of an explanation}
% \end{figure}
% In this case, we may want to learn a concept $h$ that satisfy $E$ at all point $x$ that is
% \begin{equation*}
    % g(h,x) = g(h^*,x).
% \end{equation*}
% We hope that such concept $h$ will also be close to the optimal $h^*$. 
% We define a satisfiablity of an explanation $E$ on a concept $h$ as
% \begin{definition}
% [Insatisfiability of $h$ on $E$] For a concept $h$ and an explanation $E(g,C)$ , we define the unsatisfiability as
% \begin{equation*}
%     \operatorname{UNSAT}(h, E(g,c) ) = \Pr_x(g(h,x) \in C(x))
% \end{equation*}
% \end{definition}

% \begin{definition}
% [Distance between 2 concepts] We define a distance between $h_1, h_2 \in \cH$ as
% \begin{equation*}
%     d(h_1,h_2) = \Pr_{x\in\cX}(h_1(x)\neq h_2(x)).
% \end{equation*}
% \end{definition}
% We define a distance between 2 concepts under a $g$ below.
% \begin{definition}
% [Empirical distance between 2 concepts under $g$] Let $S_x = \{x_1, ..., x_m \}$ be the set of examples drawn from space $\cX$. Then, we can define the empirical distance between two concepts with respect to the function $g$ as:
% \begin{equation*}
%     \widehat{d}_g(h_1,h_2) = \sum_{i=1}^m \left(g(h_1,x_i) - g(h_2,x_i)\right)^2
% \end{equation*}
% \end{definition}
% First, we define the restriction of our hypothesis space $\cH$ that has  explanation loss at most $\tau$.

We assume that we are in a doubly agnostic setting. Firstly, we are agnostic in the usual sense that there need be no classifier in the hypothesis class $\cH$ that perfectly labels $(x,y)$; instead, we hope to achieve the best error rate in the hypothesis class, $h^* = \arg\min_{h \in \cH} \err_\cD(h)$. Secondly, we are also agnostic with respect to the explanations, so that the optimal classifier $h^*$ may not satisfy the explanation constraints exactly, so that it incurs nonzero surrogate explanation loss $ \phi(h^*, D) > 0$. 
% In our analysis, we first select classifiers that have lower empirical surrogate loss than a threshold $t$, then perform supervised learning with the remaining classifiers.

\begin{theorem}
[Generalization Bound for Agnostic Setting]
\label{thm: generalization bound}
Consider a hypothesis class $\cH$, distribution $\cD$, and explanation loss $\phi$. Let $S = \{(x_1, y_1), \dots, (x_n, y_n) \}$ be drawn i.i.d. from $\cD$ and $S_E = \{ x'_1,\dots, x'_k\}$ drawn i.i.d. from $\cD_\cX$. With probability at least $1-\delta$, for $h \in \cH$ that minimizes empirical risk $\err_S(h)$ and has $\phi(h, S_E) \leq \tau$, we have
\begin{equation*}
    \err_D(h) \leq  \err_D(h^*_{\tau - \varepsilon_k}) + 2R_n(\cH_{\phi, \tau + \varepsilon_k}) + 2\sqrt{\frac{\ln(4/\delta)}{2n}},
\end{equation*}
% when 
% \begin{equation*}
%     h_t^* = \arg\min_{h \in \cH_{\phi, t}} \err_\cD(h)
% \end{equation*}
% and
\begin{equation*}
    \varepsilon_k =  2R_k(\cG) + \sqrt{\frac{\ln(4/\delta)}{2k}},
\end{equation*}
when $\cG = \{\phi(h, x) \mid  h \in \cH, x \in \cX\}$ and $h_\tau^* = \arg\min_{h \in \cH_{\phi, \tau}} \err_\cD(h)$.
\label{thm: generalization bound agnostic}
\end{theorem}
\begin{proof}
The proof largely follows the arguments in \citet{balcan2010discriminative}, but we use Rademacher complexity-based deviation bounds instead of VC-entropy. We defer the full proof to  Appendix \ref{appx:generalization_bound_agnostic}.
\end{proof}
Our bound suggests that these constraints help with our learning by shrinking the hypothesis class $\cH$ to $\cH_{\phi, \tau+ \varepsilon_k}$, reducing the required sample complexity. However, there is also a trade-off between reduction and accuracy. In our bound, we compare against the best classifier $h^*_{\tau-\varepsilon_k} \in \cH_{\phi,\tau-\varepsilon_k}$ instead of $h^*$. Since we may have $\phi(h^*,\cD) > 0$, if $\tau$ is too small, we may reduce $\cH$ to a hypothesis class that does not contain any good classifiers.
% Since $\phi(h^*,\cD)$ can be greater than $0$, when $t$ is too small, we could reduce $\cH$ to a hypothesis class that does not contain any good classifiers and $\err_\cD(h^*_t) \gg \err_\cD(h^*)$, leading to a weaker guarantee. On the other hand, if $t$ is large enough that $h^* \in \cH_{\phi,t}$ then $h^*_{t - \varepsilon_k} = h^*$ and $\err_\cD(h^*) = \err_\cD(h^*_t)$, the restricted hypothesis class can remain quite large.
% In practice, we only need to select $t$ that is small enough while the hypothesis class $\cH_{\phi,t - \varepsilon_k}$ still contains a good classifier.
 Recall that the generalization bound for standard supervised learning --- in the absence of explanation constraints --- is given by
\begin{equation*}
    \err_D(h) \leq  \err_D(h^*) + 2R_n(\cH) + 2\sqrt{\frac{\ln(2/\delta)}{2n}}.
\end{equation*}

We can see the difference between this upper bound and  the upper bound in Theorem \ref{thm: generalization bound agnostic} here as a possible notion of the goodness of an explanation constraint. We further discuss this in Appendix \ref{appendix: goodness of an explanation}.



% However, when the number of labeled examples $n$ is small, the Rademacher complexity of $R_n(\cH_{\phi, t})$ can be much smaller than the complexity of $R_n(\cH)$ and this leads to significant benefits from explanation constraints.

% One limitation of our definition of goodness is that it is difficult to analytically compute the Rademacher complexity.

\section{Gradient Explanations for Particular Hypothesis Classes}
In this section, we further quantify the usefulness of explanation constraints on different concrete examples and characterize the Rademacher complexity of the restricted hypothesis classes. In particular, we consider an explanation constraint of a constraint on the input gradient. For example, we may want our model's gradient to be close to that of some $h' \in \cH$. This translates to $g(h, x) = \nabla_x h(x)$ and $C(x) = \{ x \in \R^d \: | \: \lVert x - \nabla_x h'(x) \rVert \leq \tau \}$ for some $\tau > 0$. 
% In addition,  we consider a class of linear models and two layer neural networks with bounded weights. Since knowing an exact gradient is enough to identify these models (Appendix \ref{appendix:2nn_algo}), we allow the difference between gradients up to some slack $\tau$.


\subsection{Gradient Explanations for Linear Models}
%  Let $\cD_\cX$ be a uniform distribution over a unit sphere in $\R^d$. Consider a hypothesis class of linear separators that pass through an origin $\cH = \{h | h(x) = \operatorname{sign}( w_h^\top x)\}$ and we want to learn $h^* \in \cH$. We consider explanation constraints in term of gradient information. For any $h\in \cH$, input gradient is given by
%  \begin{equation*}
%     \nabla_x (w^\top_h x) = w_h.
% \end{equation*}
% We can see that knowing an exact gradient of a data point is enough to identify $h$. Therefore, we further assume that we only have an access to a close approximation of a true gradient $w_{h'}$ with 
% \begin{equation*}
%     \theta(w_{h'},w_{h^*}) \leq \tau
% \end{equation*}
% when $\theta(a,b)$ be an angle between two vectors $a, b$.  Assume that $\tau$ is known, we define our explanation loss as
% \begin{equation*}
%     \phi(h,x) =  \theta(w_h, w_{h'}).
% \end{equation*}



We now consider the case of a uniform distribution on a sphere, and we use the symmetry of this distribution to derive an upper bound on the Rademacher complexity (full proof to Appendix \ref{appendix: main theory linear}).

\begin{theorem}
[Rademacher complexity of linear models with a gradient constraint, uniform distribution on a sphere]
\label{theorem:bound_linear}
Let $\cD_\cX$ be a uniform distribution on a unit sphere in $\R^d$, let $\cH = \{h: x \mapsto \langle w_h, x \rangle \mid w_h \in \R^d, ||w_h||_2 \leq B\}$ be a class of linear models with weights bounded by a constant $B$. Let $\phi(h,x) = \theta(w_h, w_{h'})$ be a surrogate loss where $\theta(u,v)$ is an angle between $u,v$. We have
\begin{align*}
    R_n(\cH_{\phi, \tau}) & \leq \frac{B}{\sqrt{n}} \left(\sin(\tau) \cdot p + \frac{1 - p}{2} \right),
\end{align*}
where $p = \erf\left( \frac{\sqrt{d}\sin(\tau)}{\sqrt{2}}\right)$
and $\erf(x) = \frac{2}{\sqrt{\pi}}\int_0^x e^{-t^2} dt$ is the standard error function.
\end{theorem}
% \begin{proof}
%     We defer the proof to Appendix \ref{appendix: main theory linear}
% \end{proof}
The standard upper bound on the Rademacher complexity of linear models is $\frac{B}{\sqrt{n}}$. Our bound has a nice interpretation; we shrink our bound by a factor of $(\frac{1 - p}{2} + \sin(\tau) p)$. We remark that  $d$ increases, we observe that $p \to 1$, so the term $\sin(\tau) p$ dominates this factor. As a consequence, we get that our bound is now scaled by $\sin(\tau) \approx \tau$ and the the Rademacher complexity scales down by a factor of $\tau$. This implies that given $n$ labeled data, to achieve a fast rate $\cO(\frac{1}{n})$, we need $\tau$ to be as good as $O(\frac{1}{\sqrt{n}})$.

% \red{change to alpha?}
\begin{figure}
\vspace{-8mm}
\centering
\includegraphics[width=0.65\columnwidth]
{figs/2NNS_m1_clean2.pdf}
% \vspace{-1mm}
\caption{Visualization of the piecewise constant function of $\nabla_x h(x) - \nabla_x h'(x)$ when $h$ is a two layer NNs with 1 node. Background colors represent regions with non-zero value. }
\vspace{-3mm}
\label{fig: simplify 2NN 1}
\end{figure}
\subsection{Gradient Explanations for Two Layer Neural Networks}

% \subsection{Gradient Explanations for Two layer neural networks}
% We now analyze the case of gradient explanations for two layer neural networks, with $m$ hidden nodes.

\begin{theorem}
[Rademacher complexity of two layer neural networks ($m$ hidden nodes) with a gradient constraint]
\label{theorem:bound_2layer_gradient} 
Let $\cX$ be an instance space and $\cD_{\cX}$ be a distribution over $\cX$ with a large enough support. Let $\cH = \{h : x \mapsto \sum_{j=1}^m w_j \sigma(u_j^\top x) | w_j \in \R, u_j \in \R^d, \sum_{j=1}^m |w_j|  \leq B, \lVert u_j \rVert_2 = 1 \}$ be a class of two layer neural networks with a ReLU activation function and bounded weight. Assume that there exists some constant $C > 0$ such that $\bbE_{x \sim \cD_{\cX}} [ \lVert x \rVert_2^2 ] \leq C^2$. Consider an explanation loss given by 
\begin{align*}
    \phi(h, x) = & \lVert \nabla_x h(x) - \nabla_x h'(x) \rVert_2 +  \infty \cdot 1 \{ \lVert \nabla_x h(x) - \nabla_x h'(x) \rVert > \tau \} 
\end{align*} 
for some $\tau > 0$. Then, we have that $R_n ( \cH_{\phi, \tau}) \leq  \frac{3 \tau m C}{\sqrt{n}}.$
\end{theorem}
\begin{proof}
(Sketch)
     The key ingredient is to identify the impact of the gradient constraint and the form of class $\cH_{\phi, \tau}$. We provide an idea when we have  $m=1$ node. We write $h(x) = w \sigma(u^\top x)$ and $h'(x) = w'\sigma(u'^\top x)$. Note that
    $\nabla_x h(x) - \nabla_x h'(x) = wu 1\{u^\top x > 0\} - w'u' 1\{(u')^\top x > 0\}
    $
    is a piecewise constant function (Figure \ref{fig: simplify 2NN 1}). Assume that the probability mass of each region is non-negative, our gradient constraint implies that the norm of each region cannot be larger than $\tau$. 
    \begin{enumerate}
        \item If $u,u'$ have different directions, we have 4 regions in $\nabla_x h(x) - \nabla_x h'(x)$ and can conclude that $|w| < \tau, |w'| < \tau$.
        \item If $u = u'$ have the same direction, we only have 2 regions in $\nabla_x h(x) - \nabla_x h'(x)$ and can conclude that $\lVert wu - w'u'\rVert = |w - w'| < \tau$.
    \end{enumerate}
The gradient constraint enforces a model to have the same node boundary $(u = u')$ with a small weight difference $|w-w'| < \tau$ or that node would have a small weight $|w| < \tau$. This finding allows us to determine the restricted class $\cH_{\phi,\tau}$, and we can use this to bound the Rademacher complexity accordingly. For full details, see Appendix \ref{appx: main theory nn}.
\end{proof}
We compare this with the standard Rademacher complexity of a two layer neural network \citep{ma2022notes},
$$ R_n(\cH) \leq \frac{2BC}{\sqrt{n}}.$$
We can do better than this standard bound if $\tau <  \frac{2B}{3m}$. One interpretation for this is that we have a budget at most $\tau$ to change the weight of each node and for total $m$ nodes, we can change the weight by at most $\tau m$. We compare this to $B$ which is an upper bound on the total weight  $\sum_{j=1}^m |w_j| \leq B$. Therefore, we can do better than a standard bound when we can change the weight by at most two thirds of the average weight $\frac{2B}{3m}$ for each node.
% \textcolor{red}{ADD COMPARISON}
% \red{add assumption on the distribution of x, discuss the strongness of the surrogate loss in this section}
We would like to point out that our bound does not depend on the distribution $\cD$ because we choose a specific explanation loss that guarantees that the gradient constraint holds almost everywhere. Extending to a weaker loss such as $\phi(h,x) = \lVert \nabla_x h(x) - \nabla_x h'(x) \rVert$ is a future research direction. In contrast, our result for linear models uses a weaker explanation loss and depends on $\cD$ (Theorem \ref{theorem:linear_distribution_free}).
We also assume that there exists $x$ with a positive probability density at any partition created by $\nabla_x h(x)$. This is not a strong assumption, and it holds for any distribution where the support is the $\mathbb{R}^d$, e.g., Gaussian distributions.



% \begin{theorem}
% Assume $\cD$ is a uniform distribution over a unit sphere in $\R^d$. Let $\cH$ be a class of linear separators that pass through an origin. Let $\phi(h,x) = \theta(w_h, w_{h'})$ be an explanation loss. Then we have
% \begin{equation*}
%     R_n(\cH_{\phi,\tau}) = \cO(\sqrt{\frac{\tau}{n}}). 
% \end{equation*}
% \end{theorem}
% \begin{proof}
% Recall that
% \begin{equation*}
%     \cH_{\phi, \tau} = \{ 
%     h\mid h(x) = \operatorname{sign}(w_h^\top x), \theta(w_h,w_h') \leq \tau\}.
% \end{equation*}

% We define a disagreement region of $\cH_{\phi,\tau}$ as the set of points that there exists two classifiers in $\cH_{\phi,\tau}$ that disagree.
% \begin{equation*}
%     \operatorname{DIS}(\cH_{\phi,\tau}) = \{x \in \cX \mid \exists h_1,h_2 \in \cH_{\phi,\tau}, h_1(x) \neq h_2(x) \}.
% \end{equation*}
% We define an agreement region as the complement of the disagreement region
% \begin{equation*}
%     \operatorname{Agree}(\cH_{\phi,\tau}) = \cX \setminus \operatorname{DIS}(\cH_{\phi,\tau}).
% \end{equation*}
% \begin{figure}[ht]
%     \centering
%     \includegraphics[width = 1\columnwidth]{figs/linear_case.jpeg}
%     \caption{\red{change pic to one wedge}.}
%     % \label{}
% \end{figure}
% The disagreement region is known for a class of linear separator and a uniform distribution over a unit sphere \cite{?}.
% \begin{equation*}
%     \operatorname{DIS}(\cH_{\phi,\tau}) = \{x \in \cX \mid \frac{\pi}{2} - \tau \leq \theta(x, w_{h'}) \leq \frac{\pi}{2} + \tau \}.
% \end{equation*}
% \red{Rademacher complexity with |.| or without }
% The key observation is that 
%  the empirical Rademacher complexity of $\cH_{\phi,\tau}$ only depends on the points that lie in the disagreement region $\operatorname{DIS}(\cH_{\phi,\tau})$. 
% For a set of samples $S = \{x_1,\dots, x_n\}$, the empirical Rademacher complexity is given by
% \begin{align*}
% R_S(\cH_{\phi,\tau})=
%     \mathbb{E}_\sigma[\frac{1}{n} \sup_{h \in \cH_{\phi,\tau}}( \sum_{x_j \in \operatorname{DIS}(\cH_{\phi,\tau}) } h(x_j)\sigma_j)]
% \end{align*}
% because for points that lie in $\operatorname{Agree}(\cH_{\phi,\tau})$, any $h \in \cH_{\phi,\tau}$ will take the same value and the expectation of $\sup_{h \in \cH_{\phi,\tau}}h(x_i)\sigma_i$ over Rademacher distribution will be zero. Furthermore, in our case of linear separators over a uniform distribution over a unit sphere, the empirical Rademacher complexity of $\cH_{\phi,\tau}$ over $m$ samples from $\operatorname{DIS}(\cH_{\phi,\tau})$ equals to the Rademacher complexity of $\cH$ over $m$ random samples from $\cD$,
% \begin{equation*}
%     R_{S \sim \operatorname{DIS}(\cH_{\phi,\tau})^m}(\cH_{\phi,\tau}) = R_{S \sim \cD^m}(\cH).
% \end{equation*}



% Informally, the disagreement region contains points with angle at most $\tau$ from the normal vector $w_{h'}$, this take a wedge shape (Figure ).  Our restriced hypothesis class $\cH_{\phi,\tau}$ contains any linear separators with a decision boundary that cut through this wedge and can linearly separate any two points in the disagreement region. By geometry, fitting random labels on $m$ points in this wedge with $\cH_{\phi,\tau}$ has the same complexity as fitting random labels on $m$ points on the unit sphere with $\cH$.
% \red{add absolute to the rademacher complexity definition then half sphere = full sphere}

% Formally, when $\tau < \frac{\pi}{2}$, there is a bijective map $\rho_1:\bbS^{d-1} \to \operatorname{DIS}(\cH_{\phi,\tau})$ between a unit sphere and the disagreement region and a bijective map $\rho_2: \cH \to \cH_{\phi,\tau}$ between the original hypothesis class to the restricted hypothesis class such that for any points $S = \{x_1,\dots, x_m\}$ drawn i.i.d. from $\cD$ and $h \in \cH$ 

% \begin{equation*}
%     \sum_{i=1}^m h(x_i)\sigma_i = \sum_{i=1}^m \rho_2(h)(\rho_1(x_i))\sigma_i.
% \end{equation*}
% Intiutively, $\rho_1$ shrink the whole unit sphere into the disagreement region wedge by rotation (?) and $\rho_2$ shrink $\cH$ to $\cH_{\phi,\tau}$ by rotating the normal vector (?)(Figure ). In addtion, the distribution $\rho_1(\cD)$ is also a uniform distribution over the disagreement region. With the existence of $\rho_1,\rho_2$, we can conclude that the empirical Rademacher complexity of $\cH_{\phi,\tau}$ over $m$ samples from $\operatorname{DIS}(\cH_{\phi,\tau})$ equals to the Rademacher complexity of $\cH$ over $m$ random samples from $\cD$.

% Let $R_{n}(\cH)$ be a Rademacher complexity of $\cH$ over a distribution $\cD$ then we can write
% \begin{align*}
%     R_{n}(\cH_{\phi, \tau}) &= \mathbb{E}_{S \sim \cD^n}[R_S(\cH_{\phi, \tau})]\\
%     &= \mathbb{E}_{S \sim \cD^n}[\mathbb{E}_{S \sim \cD^n}[R_S(\cH_{\phi, \tau}) \mid |S \cap \operatorname{DIS}(\cH_{\phi,\tau})| = m]]\\
%     &= \mathbb{E}_{S \sim \cD^n}[\mathbb{E}_{S \sim \cD^n}[\frac{m}{n}R_m(\cH) \mid |S \cap \operatorname{DIS}(\cH_{\phi,\tau})| = m]]\\
%     &= \frac{1}{n}\sum_{k = 0}^n kR_k(\cH)\Pr(|S \cap \operatorname{DIS}(\cH_{\phi,\tau})| = k).
% \end{align*}
% Let $|S \cap \operatorname{DIS}(\cH_{\phi,\tau})|$ is the number of points in $S$ that lies in the disagreement region which follows a Binomial distribution $\operatorname{Bin}(n,p)$ when $p = \Pr(\operatorname{DIS}(\cH_{\phi,\tau})) = \frac{\tau}{\pi}$. Therefore,
% \begin{equation*}
%     R_{n}(\cH_{\phi, \tau})  = \frac{1}{n}\sum_{k = 0}^n kR_k(\cH) {n \choose k} \frac{\tau}{\pi}^k (1 - \frac{\tau}{\pi})^{n-k}. 
% \end{equation*}
% It is known that $R_n(\cH) = \cO(\frac{1}{\sqrt{n}})$ (see Appendix ?). With the Normal approximation to the Binomial distribution ,we have
% \begin{equation*}
%     R_{n}(\cH_{\phi, \tau})  = \cO (\frac{\bbE_X[\sqrt{X}]}{n})
% \end{equation*}
% when $X \sim \cN(\frac{n\tau}{\pi}, \frac{n\tau}{\pi}(1 - \frac{\tau}{\pi}))$. Since, $\bbE_X[\sqrt{X}] = \cO(\sqrt{\frac{n\tau}{\pi}})$. We have
% \begin{equation*}
%     R_{n}(\cH_{\phi, \tau}) = \cO (\sqrt{\frac{\tau}{n}}).
% \end{equation*}

    
% \end{proof}

% We note that the complexity of the original linear separator is given by $\mathcal{O}(\frac{1}{\sqrt{n}})$. This implies that in order to reap a full benefit from the explanation, we need the angle between our noisy gradient and the true gradient $w_{h^*}$, $\tau$ to be smaller that $o(1)$. For example, if $\tau = \frac{1}{n}$ then we have $R_n(H_{\phi,\tau}) = \mathcal{O}(\frac{1}{n})$.





% \subsection{A Noisy Classifier for Linear Classifier}

% In addition, for a linear model, this gradient information is equivalent to having access to a noisy classifier $h'(x) = \operatorname{sign}(w_{h'}^\top x)$. In our previous analysis, we define $\phi(h,x) = \theta(w_h, w_{h'})$ as an angle between the normal vector of a given linear classifier $h$ and $h'$. We can also define $$\phi(h,x) = 1[h(x) \neq h'(x)]$$ as a disagreement between $h$ and $h'$. Then the restricted hypothesis class is given by any linear separator that different from $h'$ by probability at most $\tau$. However, these two notions is equivalent since we can bound the disagreement between any two linear separators with the angle between the corresponding normal vector $\Pr(h(x) \neq h'(x)) \leq C\theta(w_h,w_{h'})$ (\cite{?}).




% \subsection{Physics Explanations}

% We can also analyze explanations given by physics rules or simulators \citep{NEURIPS2018_842424a1, seo2021controlling}. We can consider the hypothesis class $\cH = \{ f_1 \circ f_2 \circ f_3 | f_1, f_3 \text{ are linear}, f_2 \text{ is a two layer neural network} \}$. The hypothesis class of classifiers that use information from physics rules or simulators is given by $\cH_p = \{f_1 \circ p \circ f_3 | f_1, f_3 \text{ are linear} \}$ where $p$ is the particular physics simulator. While this is a simplification of incorporating implicit physics rules or a physics simulator, we note that this still captures the intuition of a classifier learning information as inputs for physics reasoning and then using the outputs to make a decision. We also note that this formulation assumes that we can capture the information represented by a physics simulator or physics rules in a two layer neural network.

% \textcolor{red}{TODO: Generalization Bound}

% We note that there are no big differences in learning these approaches as previous methods are differentiable approximations or capture this information implicitly. As a result, we can optimize these methods as normal (i.e., through SGD). 


% \section{Questions}

% \begin{enumerate}
%     \item For a linear model, gradient information is equivalent to providing an interval for each weight (see 5.2 in Tengyu's notes).
%     \item When $d = 2$, give a closed form goodness of an explanation for a gradient information for a linear separator when the data is drawn uniformly on a sphere.
%     \item When $d > 2$, give a closed form goodness of a gradient information.
%     \item (Showing that gradient can be helpful)If we know the exact gradient of a neural network, would it be possible to recover the two layer neural network with $m$ nodes and how many example we need to do so ? - Both
%     \item Can we formalize what a physics rule looks like? What hypothesis class / data?
%     \item Do we need to address transformation invariance? (Read NeurIPS paper first) - \textcolor{red}{Dylan: It seems like the existing NeurIPS paper handles this pretty well. The paper defines specific notions of VC dimension that take into account translation invariance (which is defined by partitioning data into their respective groups of translations). The paper handles multiple cases and derives an algorithm that achieves sample complexity on the order of their notion of VC dimension (without knowing if the setting is realizable, agnostic, or if the translations help).}
%     \item Handle noisy classifiers (imperfect explanation case) - the same for the linear case... Can we handle for more complex case (two layer NN ...) - later when the noisy classifier does not come from $\cH$ (improper learning).
%     \item Information gain bound.
% \end{enumerate}

% \section{Matching Explanations without Unlabeled Data}

% \pagebreak

% \section{Algorithmic Results on Learning with Explanation Constraints}

%a constraint that we want our concept $h$ to satisfy. To analyze the impacts of incorporating explanations in our classifiers, we build off of existing work \citep{balcan2010discriminative}, which provides a PAC-style framework for semi-supervised learning. This paper analyzes classical notions of compatibility with a distribution, including structural risk minimization, margin, and smoothness. In our work, we are interested in and motivated by a different type of compatibility: that of satisfying a particular, known explanation. We now formally define our notion of an explanation $E$. 
% as a function $g(h,x)$ on a concept $h$ and a data point $x$ and a compatibility function $\ell_E(g(h, x))$, which tells us how desirable a given hypothesis element is.

% \begin{definition}
% [Explanation]
% % For a bounded function $g:\cH \times \cX \to \R$ and a set $C(x) \subseteq \R$ for all $x\in \cX$, we call $E(g,C)$ as explanation induced by $g,C$ if for a concept $h \in \cH$ and a data point $x\in \cX$, when $g(h,x) \in C(x)$ then we say that $h$ satisfy $E$ at a point $x$. Otherwise, $h$ does not satisfy $E$ at $x$.
% Let $E(g, \phi)$ be an explanation, where $g: \cH \times \cX \to \R$ and $\phi: \cH \times \cX \to \R$. In our notation, $g$ represents a particular function of interest of an element $h \in \cH$, and $\phi$ represents a notion of compatibility, or how we believe a hypothesis element should behave on a particular data. 
% % We let $\ell_E(h, D) = \E_{x \in D}[C(g(h, x))]$.
% For an explanation $E(g,\phi)$, and a classifier $h \in \cH$ and a data point $x \in \cX$, an incompatibility of $E$ on $h$ is given by $\phi(g(h,x),x)$ and we also overload the notation,
% \begin{equation*}
%     \phi(h,\cD) = \mathbb{E}_{x \in \cD}[\phi(g(h,x),x)].
% \end{equation*}
% \end{definition}

% We can use this definition to capture many different types of explanations and forms of inductive bias that are frequently used in practice. This formulation is equally as expressible as existing work \citep{balcan2010discriminative}, and we assume that our explanation is interested in a particular function $g$ of $h$ and $x$. We now will discuss the function of interest $g$ for different type of explanations that are used in practice in Table \ref{table: examples of explanation}. In addition, we have a separate notion of compatibility $\phi$ that control the strength and quality of an explanation. We can see $\phi$ as a generalization of a surrogate loss for 0-1 loss where we impose small loss on area where we believe our ideal model should be at. We provide examples of $\phi$ for different situations in Figure \ref{fig:phi example}. We note that a more restrictive $\phi$ where we only have a small area with zero loss can lead to a stronger explanation but at the same time this may increase the probability of being wrong and ruling out an optimal classifier.



% \begin{enumerate}
    % \item Gradient: $\nabla h(x) \in C \subset \R$. (The gradient w.r.t. the first feature must be positive ($ g(x) = \nabla_{x_1} h(x)$, $C = (0, \infty)$))
    % \item Scope of output: $h(x) \in C \subset \R$. 
    % \item Noisy classifier: $h(x) = \lambda(x) \in \R$
    % \item Relationship between two points (need to generalize more): $h(x) - h(y) \in C \subset \R$
% \end{enumerate}\begin{table}[]


% \begin{figure}[th]
%     \centering
%     \includegraphics[width = 0.7\textwidth]{pics/phi.jpeg}
%     \caption{Choice of $\phi$ for different scenarios}
%     \label{fig:phi example}
% \end{figure}

% \red{to-do}
% \begin{enumerate}
    % \item add a learning theory setup, hypothesis class $\cH$ bla bla
    % \item polish examples
% \end{enumerate}

% What is a good explanation? 
% \begin{definition}
% [Goodness of an explanation]
% For a hypothesis class $\cH$, the goodness of an explanation $E(g,C)$ is given by
% \begin{equation*}
%     \Pr(\err(h) \leq \varepsilon | g(h,x) \in C)
% \end{equation*}
% \end{definition}

% How well an explanation can explain a concept $h$
% \begin{definition}
% [Satisfaction of an explanation]
% The amount some hypothesis $h$ satisfies $E$ is given by $$\E_{x \in supp(g)}[ \1 \{ g(x, h) \in C \}]$$
% \end{definition}

\section{Algorithms for Learning from Explanation Constraints}\label{sec:variational_algo}

Although we have analyzed learning with explanation constraints, algorithms to solve this constrained optimization problem are non-trivial. 
% We propose a variational approach to solve this constrained optimization problem, which has underlying connections to self-training \citep{wei2020theoretical, frei2022self}. 
In this setting, we assume that we have access to $n$ labeled data $\{(x_i,y_i)\}_{i=1}^n$, $m$ unlabeled data $\{x_{n+i}\}_{i=1}^m$, and $k$ data with explanations $\{(x_{n+m+i}, \phi(\cdot, x_{n+m+i}))\}_{i=1}^{k}$. \blue{We argue that in many cases, $n$ labeled data are the most expensive to annotate. The $k$ data points with explanations also have non-trivial cost; they require an expert to provide the annotated explanation or provide a surrogate loss $\phi$. If the surrogate loss is specified then we can evaluate it on any unlabeled data, otherwise these data points with explanations could be expensive. On the other hand, the $m$ data points can cheaply be obtained as they are completely unlabeled.}  We now consider existing approaches to incorporate this explanation information.

\noindent\textbf{EPAC-ERM:} 
Recall our EPAC-ERM objective from \eqref{eq:epac-erm}:
    \begin{equation*}
        \min_{h \in \cH} \frac{1}{n} \sum_{i=1}^n 1\{h(x_i) \neq y_i\} \; \text{ s.t. } \; \frac{1}{k}\sum_{j=n+m+1}^{n+m+k}\phi(h, x_j ) \leq \tau
    \end{equation*}
for some constant $\tau$. This constraint in general requires more complex optimization techniques (e.g., running multiple iterations and comparing values of $\tau$) to solve algorithmically. We could also consider the case where $\tau = 0$, which would entail the hypotheses satisfy the explanation constraints exactly, which however is in general too strong a constraint with noisy explanations.

% \noindent\textbf{Constrained optimization:}
% One could of course also consider solving the fully constrained ERM objective:
% \begin{equation*}
%         \min_{h \in \cH} \frac{1}{n} \sum_{i=1}^n 1\{h(x_i) \neq y_i\} \; \text{s.t. } \phi(h_j, x_j) = 0, \forall j \geq n+m+1
% \end{equation*}
% This does not apply well to gradient-based methods and is difficult to optimize for deep networks.

\noindent\textbf{Augmented Lagrangian objectives:}
\begin{equation*}
    \min_{h \in \cH} \frac{1}{n} \sum_{i=1}^n 1[h(x_i) \neq y_i] + \frac{\lambda}{k}\sum_{j=n+m+1}^{n+m+k}\phi(h, x_j )
\end{equation*}
As is done in prior work \citep{rieger2020interpretations}, we can consider an augmented Lagrangian objective. 
% However, this does not exactly fit into our analytical framework. Also, tuning the hyperparameter $\lambda$ is difficult, as the two components of the loss function may lie in different spaces. 
% While these approaches are viable, they do not necessarily scale well to larger (deep) models. 
A crucial caveat with this approach is that the explanation surrogate loss is in general a much more complicated functional of the hypothesis than the empirical risk. 
\cready{For instance, it might involve the gradient of the hypothesis when we use gradient-based explanations. Computing the gradients of such a surrogate loss can be more expensive compared to the gradients of the empirical risk. 
For instance, in our experiments, computing the gradients of the surrogate loss that involves input gradients is 2.5 times slower than that of the empirical risk.
With the above objective, however, we need to compute the same number of gradients of both the explanation surrogate loss and the empirical risk. These computational difficulties have arguably made incorporating explanation constraints not as popular as they could be.}
% \begin{align*}
%     % \min_{f\in \mathcal{H}} \text{err}(f) + d(f, \mathcal{H}_E)
%     \min_{h \in \cH} (1 - \tau) \displaystyle \mathop{\mathbb{E}}_{(x, y) \sim \cD} \left[\ell(h(x),y)\right] +
%     \tau \inf_{\substack{f \in \cH_{\phi, t}}} \displaystyle \mathop{\mathbb{E}}_{\substack{x \sim \cD_{\cX}, \\ \tilde{y}|x \sim f}} \left[ \ell(h(x),\tilde{y})\right],
% \end{align*}

\subsection{Variational Method}

To alleviate these aforementioned computational difficulties, we propose a \textit{new} variational objective 
{\small \begin{align*}
    % \min_{f\in \mathcal{H}} \text{err}(f) + d(f, \mathcal{H}_E)
    \min_{h \in \cH} (1 - \lambda) \displaystyle \mathop{\mathbb{E}}_{(x, y) \sim \cD} \left[\ell(h(x),y)\right] +
    \lambda \inf_{\substack{f \in \cH_{\phi, \tau}}} \displaystyle \mathop{\mathbb{E}}_{x \sim \cD_{\cX}} \left[ \ell(h(x),f(x))\right],
\end{align*}}where $\ell$ is some loss function and $t \geq 0$ is some threshold. \blue{The first term is the standard expected risk of $h$ while the second term can be viewed as a projection distance between $h$ and \arxiv{$\tau$-CE models. }
% The distance is a risk of $h$ given that the label $y$ is labeled by $f$ e.g. pseudo labels given by the EPAC model that is closest to $h$. We note that both terms are in the same space which makes it easier to tune the parameter $\lambda$.
It can be seen that the optimal solution of \textbf{EPAC-ERM} would also be an optimal solution of our proposed variational objective.} The advantage of this formulation however is that it decouples the standard expected risk component from the surrogate risk component. This allows us to solve this objective with the following iterative technique, drawing inspiration from prior work in posterior regularization \citep{ganchev2010posterior, hu2016harnessing}. More specifically, let $h_t$ be the learned model at time $t$ and at each timestep $t$,
\begin{enumerate}
\item We project  $h_t$ to the class of  \arxiv{$\tau$-CE models. } \begin{align*}
f_{t+1, \phi} = & \argmin_{h \in \cH} \frac{1}{m} \sum_{i=n+1}^{n+m} \ell(h(x_i),h_t(x_i))  \quad + \lambda \max\left(0,\, \frac{1}{k} \sum_{i=n+m+1}^{n+m+k} \phi(h, x_i) - \tau\right).\end{align*} 
    The first term is the difference between $h_t$ and $f$ on unlabeled data. The second term is the surrogate loss, which we want to be smaller than $t$.  $\eta$ is a regularization hyperparameter.
\item We calculate $h_{t+1}$ that minimizes the empirical risk of labeled data and matches pseudolabels from $f_{t+1, \phi}$
\begin{align*}
    h_{t+1, \phi}  = & \argmin_{h \in \cH} \frac{1}{n} \sum_{i=1}^n \ell(h(x_i),y_i)  + \frac{1}{m} \sum_{i=n+1}^{n+m} \ell(h(x_i), f_{t+1, \phi}(x_i)).\end{align*}
    \blue{Here, the discrepancy between $h$ and  $f_{t+1,\phi}$ is evaluated on the unlabeled data $\{x_j\}_{j=n+1}^{n+m}$.}
\end{enumerate}

The advantage of this decoupling is that we could use a differing number of gradient steps and learning rates for the projection step that involves the complicated surrogate loss when compared to the  empirical risk minimization step. Secondly, we can simplify the projection iterate computation by replacing $\cH_{\phi, \tau}$ with a simpler class of teacher models $\cF_{\phi,\tau}$ for greater efficiency. Thus, the decoupled approach to solving the EPAC-ERM objective is in general more computationally convenient.


We initialize this procedure with some model $h_0$. 
% fix the current student model iterate $h_{t,\phi}$, and learn the explanation-regularized teacher function $f_{t+1, \phi}$ (that aims to project $h_{t,\phi}$ onto the set of explanation constrained models); and then fix that to obtain the next iterate $h_{t+1,\phi}$ of the student model that aims to match the outputs of $f_{t+1, \phi}$ on \textit{unlabeled data} in addition to the labeled samples:
% \begin{align*}
% f_{t+1, \phi} = & \argmin_{h \in \cH} \frac{1}{m} \sum_{i=n+1}^{n+m} \ell(h(x_i),h_t(x_i))   + \lambda \max\left(0,\, \frac{1}{k} \sum_{i=n+m+1}^{n+m+k} \phi(h, x_i) - \tau\right) \\
% h_{t+1, \phi}  = & \argmin_{h \in \cH} \frac{1}{n} \sum_{i=1}^n \ell(h(x_i),y_i)   + \frac{1}{m} \sum_{i=n+1}^{n+m} \ell(h(x_i), f_{t+1, \phi}(x_i)),
% \end{align*}
% given some initialization $h_0$. 
% This scales nicely to training deeper models as we can efficiently model this projection onto our class of EPAC models through learning $\hat{f}$. 
% In essence, our algorithm iteratively learns an  and trains a student model $h_{t+1, \phi}$ . In turn, our student model is then projected o in order to train the teacher $f$. 
\blue{
We remark that could see this as a constraint regularized self-training where  $h_t$ is a student model and $f_t$ is a teacher model. At each timestep, we project a student model to the closest teacher model that satisfies the constraint. The next student model then learns from both labeled data and pseudo labels from the teacher model. In the standard self-training, we do not have any constraint and we have $f_t = h_t$.  }
% One primary benefit of this approach as it makes use of this $m$ unlabeled data; as mentioned before, for particular types of hypothesis classes and explanations, the sample complexity for learning our class of EPAC models can be exponentially large (in $k$). Our variational objective allows us to reduce this explanation sample complexity through regularized self-training over unlabeled data. On the contrary, standard supervised and Lagrangian approaches do not use this information.


\section{Experiments} \label{experiments}

We provide both synthetic and real-world experiments to support our theoretical results and clearly illustrate interesting tradeoffs of incorporating explanations. In our experiments, we compare our method against 3 baselines: (1) a standard supervised learning approach, (2) a simple Lagrangian-regularized method (that directly penalizes the surrogate loss $\phi$), and 
% \sout{(3) a self-training approach that propagates its own predictions over a set of unlabeled data} 
\blue{(3) self-training, which propagates the predictions of (1) and matches them on unlabeled data}. We remark that (2) captures the essence of the method in \citet{ross2017right}, except there is no $\ell_2$ regularization term.

Our experiments demonstrate that the proposed variational approach is preferable to simple Lagrangian methods and other supervised methods in many cases. \blue{In particular, the variational approach leads to a higher accuracy under limited labeled data settings. In addition, our method leads to models that satisfy the explanation constraints much more frequently than other baselines.} 
\blue{We also compare to a Lagrangian-regularized + self-training baseline (first, we use the model (2) to generate pseudolabels for unlabeled data and then train a new model on both labeled and unlabeled data) in Appendix \ref{appx:added_baseline}. We remark that this baseline isnt a standard method in practice and does not fit nicely into a theoretical framework, although it seems to be the most natural approach to using unlabeled data in this procedure.}
More extensive ablations are deferred to Appendix \ref{appx:ablation}, and code to replicate our experiments will be released with the full paper.

\subsection{Regression Task with Exact Gradient Information}

\begin{figure}[t]
    \centering
    \vspace{-10mm}
    \includegraphics[width=0.48\columnwidth]{figs/v_linear.pdf}
    \vspace{-4mm}
    \caption{Comparison of MSE on regressing a linear model. Results are averaged over 5 seeds. $m = 1000, k=20$. } 
    \vspace{-3mm}
    \label{fig:linear_mse}
\end{figure}

\begin{figure*}[t]
    \centering
    \vspace{-2mm}
    \includegraphics[width=0.48\columnwidth]{figs/v_nn.pdf}
    \includegraphics[width=0.48\columnwidth]{figs/v_constraints.pdf}
    \vspace{-2mm}
    \caption{Comparison of MSE on regressing a two layer neural network (left) and $\ell_2$ distance over input gradients as we vary the amount of labeled data $n$ (right). Left is task performance and right is explanation constraint satisfcation. Results are averaged over 5 seeds. $m = 1000, k=20$.} 
    \vspace{-3mm}
    \label{fig:2layer_mse}
\end{figure*}

% \begin{figure*}[t]
%     \centering
%     \vspace{-2mm}
%     \includegraphics[width=0.49\columnwidth]{figs/noise/v_nn_ep0.5.pdf}
%     \includegraphics[width=0.49\columnwidth]{figs/noise/v_nn_ep2.pdf}
%     \vspace{-2mm}
%     \caption{Comparison of MSE on regressing a two layer neural network with explanations of noisy gradients. $m = 1000, k=20, T = 10$. Results are averaged over 5 seeds.} 
%     \vspace{-2mm}
%     \label{fig:noisy_nn}
% \end{figure*}

\begin{figure*}[t]
    \centering
    \vspace{-4mm}
    \includegraphics[width=0.48\columnwidth]{figs/v_youtube.pdf}
    \includegraphics[width=0.48\columnwidth]{figs/v_yelp.pdf}
    \vspace{-2mm}
    \caption{Comparison of accuracy on the YouTube (left) and the Yelp (right) datasets. Here, we let $m = 500, k = 150, T = 2, \tau = 0.0$. Results are averaged over 40 seeds.}
    \label{fig:youtube}
    \vspace{-7mm}
\end{figure*}

In our synthetic experiments, we focus on a regression task where we try to learn some model contained in our hypothesis class. Our data is given by $\cX = \R^d$, and we try to learn a target function $h^*: \cX \to \R$. Our data distribution is given by $X \sim \cN(0, \sigma^2 I)$, where $I$ is a $d \times d$ identity matrix. We generate $h^*$ by randomly initializing a model in the specific hypothesis class $\cH$. We assume that we have $n$ labeled data, $m$ unlabeled data, and $k$ data with explanations.


We first present a synthetic experiment for learning with a perfect explanation, meaning that $\phi(h^*, S) = 0$. We consider the case where we have the \textit{exact} gradient of $h^*$. Here, let $\cH$ be a linear classifier and note that the exact gradient gives us the slope of the linear model, and we only need to learn the bias term. Incorporating these explanation indeed helps as both methods that include explanation constraints (Lagrangian and ours) perform much better (Figure \ref{fig:linear_mse}).


We also demonstrate incorporating this information for two layer neural networks. We observe a clear difference between the simpler Lagrangian approach and our variational objective (Figure \ref{fig:2layer_mse} - left). Our method is clearly the best in the setting with limited labeled data and matches the performance of the strong self-training baseline with sufficient labeled data. We note that this is somewhat expected, as these constraints primarily help in the setting with limited labeled data; with enough labeled data, standard PAC bounds suffice for strong performance. 

We also analyze how strongly the approaches enforce these explanation constraints on new data points that are seen at test time (Figure \ref{fig:2layer_mse} - right) for two layer NNs. We observe that our variational objective approaches have input gradients that more closely match the ground-truth target network's input gradients. This demonstrates that, in the case of two layer NNs with gradient explanations, our approach best achieves both good performance and satisfying the constraints. Standard self-training achieves similar performance in terms of MSE but has no notion of satisfying the explanation constraints. The Lagrangian method does not achieve the same level of satisfying these explanations as it is unable to generalize and satisfy these constraints on new data. 
% We provide more experiments in Appendix \ref{fig:ablation_k_constraint} to compare the performance of our variational method and the Lagrangian method over varying values of $k$. 

% \begin{figure*}[t]
%     \centering
%     \vspace{-2mm}
%     \includegraphics[width=0.9\columnwidth]{figs/rebuttal/v_nn_rebuttal.pdf}
%     \includegraphics[width=0.9\columnwidth]{figs/rebuttal/v_constraints_rebuttal.pdf}
%     \vspace{-2mm}
%     \blue{\caption{Comparison of MSE on regressing a two layer neural network (left). Comparison of $\ell_2$ distance over input gradients on the test data as we vary the amount of labeled data $n$ (right). Results are averaged over 20 seeds. $m = 1000, k=20$.}}
%     \vspace{-3mm}
%     \label{fig:2layer_mse_rebuttal}
% \end{figure*}

% \begin{figure}[t]
%     \centering
%     \vspace{-2mm}
%     \includegraphics[width=0.9\columnwidth]{figs/rebuttal/v_nn_rebuttal_little_n.pdf}
%     \vspace{-2mm}
%     \blue{\caption{Same setting as Figure \ref{fig:2layer_mse_rebuttal} over smaller values of $n$.}}
%     \vspace{-4mm}
%     \label{fig:2layer_mse_rebuttal}
% \end{figure}

\subsection{Tasks with Imperfect Explanations}

Assuming access to perfect explanations may be unrealistic in practice, so we present experiments when our explanations are imperfect. We present classification tasks (Figure \ref{fig:youtube}) from a weak supervision benchmark \citep{zhang2021wrench}. In this setting, we obtain explanations through the approximate gradients of a single weak labeler, as is done in \citep{sam2022losses}. \blue{More explicitly, weak labelers are heuristics designed by domain experts; one example is functions that check for the presence of particular words in a sentence (e.g., checking for the word ``delicious" in a Yelp comment, which would indicate positive sentiment). We can then access gradient information from such weak labelers, which gives us a notion of feature importance about particular features in our data. We note that these examples of gradient information are rather \textit{easy} to obtain, as we only need domain experts to specify simple heuristic functions for a particular task. Once given these functions, we can apply them easily over unlabeled data without requiring any example-level annotations.}
% \blue{The parameter  $T$ is the number of timestep in the iterative method and $\tau$ is the threshold on the surrogate loss $\phi$. }


\cready{We observe that our variational objective achieves better performance than all other baseline approaches on the majority of settings defined by the number of labeled data.} We remark that the explanation in this dataset is a noisy gradient explanation along two feature dimensions, yet this still improves upon methods that do not incorporate this explanation constraint. Indeed, our method outperforms the Lagrangian approach, showing the benefits of iterative rounds of self-training over the unlabeled data. In addition to our real-world experiments, we present synthetic experiments with noisy gradients in Appendix \ref{appx:noise}.

% In addition to our real-world experiments, we present synthetic experiments in the same regression setting as above. To generate an imperfect explanation, we use $\nabla_x h^*(x) + \epsilon$, where $\epsilon \sim \cN(0, \sigma^2)$. In our experiments, we add fixed, randomly sampled noise for each of our $k$ data annotated with explanations. We provide results for a regression task on 2-layer neural networks in Figure \ref{fig:noisy_nn} and under additional levels of noise in Appendix \ref{appx:noise}. This reveals that our method tolerates noisy explanations far better than the Lagrangian approach. Our method also performs comparably to the methods that do not use noisy explanations.


\section{Discussion}

Our work proposes a new learning theoretic framework that provides insight into how apriori explanations of desired model behavior can benefit the standard machine learning pipeline. The statistical benefits of explanations arise from constraining the hypothesis class: explanation samples serve to better estimate the population explanation constraint, which constrains the hypothesis class. This is to be contrasted with the statistical benefit of labeled samples, which serve to get a better estimate of the population risk. We provide instantiations of our analysis for the canonical class of gradient explanations, which captures many explanations in terms of  feature importance. It would be of interest to provide corollaries for other types of explanations in future work. As mentioned before, the generality of our framework has larger implications towards incorporating constraints that are not considered as ``standard'' explanations. For example, this work can be leveraged to incorporate more general notions of side information and inductive biases. \blue{We also discuss the societal impacts of our approach in Appendix \ref{appendix: social impacts}}. As a whole, our paper supports using further information (e.g., explanation constraints) in the standard learning setting.

% \blue{
% \section{Additional Experiments}

% Here, we present additional experiments from the rebuttal process in Figure \ref{fig:2layer_mse_rebuttal}. We compared against an additional baseline to further assess from where the benefits of the variational approach arise. We compare against a Lagrangian-penalized model, which then uses a round of self-training to use the available unlabeled data. We note that this approach is not currently standard practice, but is perhaps the most natural existing method to combine explanation constraints and unlabeled data. We observe that this has a similar accuracy to our variational approach, which suggests that the primary cause of the accuracy improvement of our method over the standard Lagrangian penalty is indeed due to using this unlabeled data. 

% However, we note that another main benefit of our variational approach is that it satisfies constraints on \textit{new} data much more frequently than the Lagrangian baseline and the new Lagrangian + Self-training baseline (Figure \ref{fig:2layer_mse_rebuttal}, right). As these explanation constraints are desirable behavior, this can lead to more trustworthy or reliable models. 
% }

% \section{Handling Multiple Explanations}

% \section{Training a Model via Explanation}

% \clearpage

\cready{
\section{Acknowledgements}
This work was supported in part by  DARPA under cooperative agreement HR00112020003, FA8750-23-2-1015, ONR grant N00014-23-1-2368, NSF grant IIS-1909816, a  Bloomberg Data Science PhD fellowship and funding from Bosch Center for Artificial Intelligence and the ARCS Foundation.

}


\bibliographystyle{abbrvnat}
\bibliography{main_neurips.bib}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\input{appendix.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}