% \section{Related Work}
% \label{sec:relatedwork}


% Recent advances in deep learning have led to models that achieve high performance but which are also highly complex \citep{lecun2015deep, goodfellow2016deep}. Understanding these complex models is crucial for safe and reliable deployments of these systems in the real-world. One approach to improve our understanding of a model is through explanations. This can take many forms such as feature importance \citep{ribeiro2016should, smilkov2017smoothgrad, lundberg2017unified,sundararajan2017axiomatic}, high level concepts \citep{kim2018interpretability,yeh2020completeness}, counterfactual examples \citep{wachter2017counterfactual,goyal2019counterfactual, mothilal2020explaining}, robustness of gradients \citep{wicker2022robust}, or influential training samples~\citep{koh2017understanding,yeh2018representer}.
% % Once we generate explanations, human expert may decide whether the model is correct or not. These explanations can help us detect bugs [cite model debugging], provide a suggestion [counterfactual example] and build our trust with a model. Connecting explaination with use cases is an active line of research \cite{chen2022interpretable}. 

% In contrast to generating post-hoc explanations of a given model, we aim to learn models given apriori explanations. There has been some recent work along such lines.  \citet{koh2020concept, zarlenga2022concept} incorporates explanations within the model architecture by requiring a conceptual bottleneck layer. \citet{ross2017right, rieger2020interpretations,ismail2021improving, stacey2022supervising}  use explanations to modify the learning procedure for any class of models: they incorporate explanations as a regularizer, penalizing models that do not exhibit apriori given explanations; \citet{ross2017right} penalize input gradients, while \citet{rieger2020interpretations} penalize a Contextual Decomposition score \citep{murdoch2018beyond}. Some of these suggest that constraining models via explanations leads to higher accuracies and more robustness to spurious correlation, but do not provide analytical guarantees. On the theoretical front, \citet{li2020learning} show that models that are easier to explain locally also generalize well. However, \citet{bilodeau2022impossibility} show that common feature attribution methods without additional assumptions on the learning algorithm or data distribution do no better than random guessing at inferring counterfactual model behavior.

% Our contribution is to provide an analytical framework for learning from explanations that quantifies the benefits of explanation constraints. Our analysis is closely related to the framework of learning with side information. \citet{balcan2010discriminative} shows how unlabeled data can help in semi-supervised learning through a notion of compatibility between the data and the target model. This seminal work studies classical notions of side information (e.g., margin, smoothness, and co-training). 
% Subsequent papers have adapted this learning theoretic framework to study the benefits of representation learning \citep{garg2020functional} and transformation invariance \citep{shao2022a}. On the contrary, our paper focuses on the more recent notion of explanations. Rather than focus on the benefits of unlabeled data, we characterize the quality of different explanations.


% % provide concrete examples for explanations given by gradient information and noisy classifiers.
% % studied in the context of semi-supervised learning . The seminal work studies the case when the side information is in the form of margin, smoothness and co-training. Subsequent papers have adapted this framework to study representation learning \citep{garg2020functional} and transformation invariance \citep{shao2022a}.

% % In our paper, we focus on the recent notion of explanations.


% % After this work, subsequent papers have adapted this framework to handle particular inductive biases, which we consider as specific instantiations of our setting. This includes \citep{shao2022a}, which studies this for transformation invariance, and \citep{garg2020functional} for representation learning.  


% % We also remark that our framework is quite general and can handle constraints beyond those considered in the explainable AI community. For example, this is an immediate generalization of the framework for handling side information in semi-supervised learning \citep{balcan2010discriminative}, including notions of smoothness \citep{xiaojin2002learning, pukdee2022label}, transformation invariance \citep{shao2022a}, and representation learning \citep{garg2020functional}. It can also analyze cases of other real-world constraints, including that of physics rules \citep{NEURIPS2018_842424a1}.




%  % However, there haven't been any theoretical guarantee on why and how explanation constraints can help us. Recent theoretical works have been focusing on axiomatic approach to explanations to propose explanations with different desirable property \citep{yeh2019fidelity}[cite chih kuan/ pradeep/ cite ameet and gregory plumb's paper] since it is known that many well-known explanations can have bad properties [cite adebayo]; saliency map of an untrained model looks similar to the trained one. There have been works connecting learning theory with explanation ML suggesting that  In this work, we hope to quantify the benefits of learning explanation constraints through the lense of learning theory.


% % We leverage variational characterizations to scalably learn complex models under explanation constraints. 
% Our work can also be connected to the self-training literature \citep{chapelle2009semi,xie2020self,wei2020theoretical, frei2022self}, where we could view our variational objective as comprising a regularized (potentially simpler) teacher model that encodes these explanation constraints into a student model. Our variational objective (where we use simpler teacher models) is also related to distillation, which has also been studiend in terms of gradients \citep{czarnecki2017sobolev}.








\section{Uniform Convergence via Rademacher Complexity}\label{appendix: rademacher complexity}
A standard tool for providing performance guarantees of supervised learning problems is a generalization bound via uniform convergence. We will first define the Rademacher complexity and its corresponding generalization bound.

\begin{definition} Let $\cF$ be a family of functions mapping $\cX \to \R$. Let $S = \{x_1, \dots, x_m\}$ be a set of examples drawn i.i.d. from a distribution $D_{\cX}$. Then, the empirical Rademacher complexity of $\cF$ is defined as
\begin{equation*}
    {R}_S(\cF) = \displaystyle \mathop{\mathbb{E}}_{\sigma}\left[\sup_{f\in\cF}\left(\frac{1}{m}\sum_{i=1}^m \sigma_if(x_i) \right)\right]
\end{equation*}
where $\sigma_1,\dots,\sigma_m$ are independent random variables uniformly chosen from $\{-1,1\}.$
\end{definition}

\begin{definition}
Let $\cF$ be a family of functions mapping $\cX \to \R$. Then, the Rademacher complexity of $\cF$ is defined as
\begin{equation*}
    R_n(\cF) = \displaystyle \mathop{\mathbb{E}}_{S \sim \cD_\cX^n}\left[R_S(\cF) \right]. 
\end{equation*}
The Rademacher complexity is the expectation of the empirical Rademacher complexity, over $n$ samples drawn i.i.d. from the distribution $\cD_\cX$.
\end{definition}
\begin{theorem}
[Rademacher-based uniform convergence]
Let $D_{\cX}$ be a distribution over $\cX$, and $\cF$ a family of functions mapping $\cX \to [0,1]$. Let $S = \{x_1, \dots, x_n\}$ be a set of samples drawn i.i.d. from $D_{\cX}$, then with probability at least $1 - \delta$ over our draw $S$,
\begin{equation*}
    |\bbE_{\cD}[f(x)] - \hat{\bbE}_S[f(x)]| \leq   2R_n(\cF) + \sqrt{\frac{\ln(2/\delta)}{2n}}.
\end{equation*}
This holds for every function $f \in \cF$, and $\hat{\bbE}_S[f(x)]$ is expectation over a uniform distribution over $S$.
\end{theorem}

This bound on the empirical Rademacher complexity leads to the standard generalization bound for supervised learning.

\begin{theorem}
For a binary classification setting when $y \in \{\pm 1\}$ with a zero-one loss, for $\cH \subset \{h: \cX \to \{-1,1\}\}$ be a family of binary classifiers,  let $S=\{(x_{1},y_{1}), \dots, (x_{n},y_{n})\}$ is drawn i.i.d. from $D$ then with probability at least $1 - \delta$, we have
\begin{equation*}
    |\err_\cD(h) - \widehat{\err_S}(h)| \leq  R_n(\cH) + \sqrt{\frac{\ln(2/\delta)}{2n}},
\end{equation*}
for every $h \in \cH$ when
\begin{equation*}
    \err_\cD(h) = \Pr_{(x,y) \sim\cD}(h(x) \neq y)
\end{equation*}
and 
\begin{equation*}
    \widehat{\err}_S(h) = \frac{1}{n} \sum_{i=1}^n 1[h(x_i) \neq y_i] 
\end{equation*}
is the empirical error on $S$.
\end{theorem}
For a linear model with a bounded weights in $\ell_2$ norm, the Rademacher complexity is $\cO(\frac{1}{\sqrt{n}})$. 
We refer to the proof from \citet{ma2022notes} for this result.
\begin{theorem}
[Rademacher complexity of a linear model (\citep{ma2022notes})] 
\label{theorem:unconstrained_linear}
Let $\cX$ be an instance space in $\R^d$, let  $\cD_\cX$ be a distribution on $\cX$, let $\cH = \{h: x \to \langle w_h, x \rangle \mid w_h \in \R^d, ||w_h||_2 \leq B\}$ be a class of linear model with weights bounded by some constant $B>0$ in $\ell_2$ norm. Assume that there exists a constant $C>0$ such that $\bbE_{x \sim \cD_\cX}[||x||_2^2] \leq C^2$. For any $S=\{x_{1}, \dots, x_{n}\}$ is drawn i.i.d. from $\cD_\cX$, we have
$$
R_S(\cH) \leq \frac{B}{n}\sqrt{\sum_{i=1}^n||x_i||_2^2}
$$
and
$$
R_n(\cH) \leq \frac{BC}{\sqrt{n}}.
$$
\end{theorem}
% \begin{proof}
%     For a set of samples $S$, we have
%     \begin{align*}
%         R_S(\cH) &= \frac{1}{n} \bbE_{\sigma}\left[\sup_{h \in \cH} \sum_{i=1}^n h(x_i)\sigma_i\right]\\
%         &= \frac{1}{n} \bbE_{\sigma}\left[\sup_{\lVert w_h\rVert_2 \leq B } \sum_{i=1}^n \langle w_h, x_i \rangle\sigma_i\right]\\
%         &= \frac{1}{n} \bbE_{\sigma}\left[\sup_{\lVert w_h\rVert_2 \leq B }  \langle w_h, \sum_{i=1}^n x_i\sigma_i \rangle\right]\\
%         &= \frac{B}{n} \bbE_{\sigma}\left[ \left\lVert \sum_{i=1}^n x_i\sigma_i \right\rVert_2\right] \quad \quad \quad \quad \left(\sup_{\lVert w_h \rVert \leq B} \langle w_h, v \rangle = B\lVert v \rVert \right)\\
%         &= \frac{B}{n} \sqrt{\bbE_{\sigma}\left[ \left\lVert \sum_{i=1}^n x_i\sigma_i \right\rVert_2^2\right]} \quad \quad \quad (\text{Jensen's inequality})\\
%         &= \frac{B}{n} \sqrt{\bbE_{\sigma}\left[  \sum_{i=1}^n \lVert x_i\rVert_2^2\sigma_i^2 + 2\sum_{1 \leq i < j \leq n} \langle x_i\sigma_i, x_j\sigma_j \rangle\right]} \quad \quad \quad ( \sigma_i,\sigma_j\text{ are independent and } \bbE[\sigma_i] = 0)\\
%         &= \frac{B}{n} \sqrt{ \sum_{i=1}^n \lVert x_i\rVert_2^2}.
%     \end{align*}
% Taking its expectation, we have
% $$
%     R_n(\cH) = \bbE[R_S(\cH)] \leq \frac{B}{n}\bbE\left[\sqrt{ \sum_{i=1}^n \lVert x_i\rVert_2^2}\right] \leq \frac{B}{n}\sqrt{\bbE\left[ \sum_{i=1}^n \lVert x_i\rVert_2^2\right]} \leq \frac{BC}{n}.
% $$
% Here, we again use Jensen's inequality to derive the second inequality.
% \end{proof}


Many of our proofs require the usage of Talgrand's lemma, which we now present.
\begin{lemma} 
[Talgrand's Lemma \citep{ledoux1991probability}]\label{lemma:talgrand}
Let $\phi: \R \to \R$ be a $k$-Lipschitz function. Then for a hypothesis class $\cH = \{ h: \R^d \to \R \}$, we have that
$$ R_S(\phi \circ \cH) \leq k R_s(\cH)$$
where $\phi \circ \cH = \{f: z \mapsto \phi(h(z)) | h \in \cH \}$. \end{lemma}

\section{Generalizable Constraints} \label{section: EPAC}



We know that constraints $C(x)$ capture human knowledge about how explanations at a point $x$ should behave. For any constraints $C(x)$ that are known apriori for all $x\in \cX$, we can evaluate whether a model satisfies the constraints at a point $x\in \cX$. This motivates us to discuss the ability of models to generalize from any finite samples $S_E$ to satisfy these constraints over $\cX$ with high probability. Having access to $C(x)$ is equivalent to knowing how models should behave over \textit{all} possible data points in terms of explanations, which may be too strong of an assumption. Nevertheless, many forms of human knowledge can be represented by a closed-form function $C(x)$. For example,
\begin{enumerate}
    \item An explanation has to take value in a fixed range can be represented by $C(x) = \Pi_{i=1}^r[a_i,b_i], \forall x\in \cX.$
    \item An explanation has to stay in a ball around $x$ can be represented by $C(x) = \{u \in  \R^d \mid ||u-x||_2 \leq r \}$.
    \item An explanation has to stay in a rectangle around $\frac{x}{3}$ can be represented by $C(x) = \{u \in \R^d \mid \frac{x_i}{3} - a_i \leq u_i \leq \frac{x_i}{3} + b_i, i = 1,\dots, d\}$.
\end{enumerate}
\begin{figure}[ht]
    \centering
    \includegraphics[width = 0.6\columnwidth]{figs/eml_explanation_type.pdf}
    \caption{Illustration of examples of explanation constraints, given from some learnable class $C(x)$.}
    % \label{}
\end{figure}

In this case, there always exists a surrogate loss that represents the explanation constraints $C(x)$; for example, we can set  $\phi(h,x) = 1\{g(h,x) \in C(x)\}$. On the other hand, directly specifying explanation constraints through a surrogate loss would also imply that $C(x)$ is known apriori for all $x\in \cX$.  The task of generalization to satisfy the constraint on unseen data is well-defined in this setting. Furthermore, if  a surrogate loss $\phi$ is specified, then we can evaluate $\phi(h,x)$ on any unlabeled data point without the need for human annotators which is a desirable property.

On the other hand, we usually do not have knowledge over all data points $x\in \cX$; rather, we may only know these explanation constraints over a random sample of $k$ data points $S_E = \{x'_1,\dots, x'_k\}$. If we do not know the constraint set $C(x)$, it is unclear what satisfying the constraint at an unseen data point $x$ means. 
% What does it mean by explanation constraints generalization from $S_E$ to $\cX$, if we also don't have access to the rest of $C(x)$? 
Indeed, without additional assumptions, it may not make sense to think about generalization. For example, if there is no relationship between $C(x)$ for different values of $x$, then it is not possible to infer about $C(x)$ from $C(x'_i)$ for $i = 1,\dots, k$. In this case, we could define 
\begin{equation*}
    \phi(h,x) = 1\{g(h,x) \in C(x)\}1\{x \in S_E\},
\end{equation*}
where we are only interested in satisfying these explanation constraints over the finite sample $S_E$. For other data points, we have $\phi(h,x) = 0$. This guarantees that any model with low empirical explanation loss would also achieve loss expected explanation loss, although this does not have any particular implication on any notion of generalization to new constraints. Regardless, we note that our explanation constraints still reduce the size of the hypothesis class from $\cH$ to $\cH_{\phi,\tau}$, leading to an improvement in sample complexity. 

The more interesting setting, however, is when we make an additional assumption that the true (unknown) surrogate loss $\phi$ exists and, during training, we only have access to instances of this surrogate loss evaluated on the sample $\phi(\cdot, x'_i)$. We can apply a uniform convergence argument to achieve
\[ \phi(h,\cD_\cX) \le \phi(h,S_E) + 2R_k(\cG) + \sqrt{\frac{\ln(4/\delta)}{2k}}\]
with probability at least $1 - \delta$ over $S_E$, drawn i.i.d. from $\cD_\cX$ and $\cG = \{\phi(h, \cdot)|h \in \cH\}$,  $k = |S_E|$. Although the complexity term $R_k(\cG)$ is unknown (since $\phi$ is unknown), we can upper bound this by the complexity of a class of functions $\Phi$ (e.g., neural networks) that is large enough to well-approximate any $\phi(h,\cdot) \in \cG$, meaning that $R_k(\cG) \leq R_k(\Phi)$. Comparing to the former case when $C(x)$ is known for all $x\in \cX$ apriori, the generalization bound has a term that increases from $R_k(\cG)$ to $R_k(\Phi)$, which may require more explanation-annotated data to guarantee generalization to new data points. We note that the simpler constraints lead to a simpler surrogate loss, which in turn implies a less complex upper bound $\Phi$. This means that simpler constraints are easier to learn.

Nonetheless, this is a more realistic setting when explanation constraints are hard to acquire and we do not have the constraints for all data points in $\cX$. For example, \citet{ross2017right} considers an image classification task on MNIST, and imposes an explanation constraint in terms of penalizing the input gradient of the background of images. In essence, the idea is that the background should be less important than the foreground for the classification task. In general, this constraint does not have a closed-form expression, and we do not even have access to the constraint for unseen data points. However, if we assume that a surrogate loss $\phi(h,\cdot)$ can be well-approximated by two layer neural networks, then our generalization bound allows us to reason about the ability of  model to generalize and ignore background features on new data. 






% $C$ is a member of some learnable class $\cC$, which now allows us to think about the ability of a model to generalize to \textit{new} explanation constraints. In particular, consider a zero-one surrogate loss
% \begin{equation*}
%     \phi_C(h,x) = 1\{g(h,x) \in C(x)\}.
% \end{equation*}
% The loss $\phi_C$ belongs to a class $\cG_\cC=\{\phi_C(h, \cdot)|h \in \cH, C \in \cC \}$. By a standard uniform convergence argument,  with probability at least $1 - \delta$ over $S_E$ drawn i.i.d. from $\cD_\cX$,
% \[ \phi_C(h,\cD_\cX) \le \phi_C(h,S_E) + 2R_k(\cG_\cC) + \sqrt{\frac{\ln(4/\delta)}{2k}}\]


% ; how many explanation constraints we need to see so that the learned model would also satisfies the constraints on unseen data points. In this particular examples, this means that the model would rely on the background of digits for the classification task.
% We note that an application of active learning to effectively select points to provide explanation constraints is an interesting direction for future work.
\section{Goodness of an explanation constraint}
\label{appendix: goodness of an explanation}
\begin{definition}
[Goodness of an explanation constraint] For a hypothesis class $\cH$, a distribution $\cD$ and an explanation loss $\phi$, the goodness of $\phi$ with respect to a threshold $\tau$ and $n$ labeled examples is:
\begin{equation*}
    G_{n,\tau}(\phi, \cH) = (R_n(\cH) - R_n(\cH_{\phi, \tau})) + (\err_\cD(h^*) - \err_\cD(h^*_t))
\end{equation*}
\begin{equation*}
    h^* = \arg\min_{h \in \cH} \err_\cD(h), \quad h^*_\tau = \arg\min_{h \in \cH_{\phi,\tau}}\err_\cD(h).
\end{equation*}
\end{definition}
 Here, we assume access to infinite explanation data so that $\varepsilon_k \to 0$. The goodness depends on the number of labeled examples $n$ and a threshold $t$. In our definition, a good explanation constraint leads to a reduction in the complexity of $\cH$ while still containing a classifier with low error. This suggests that the benefits from explanation constraints exhibit diminishing returns as $n$ becomes large. In fact, as $n \to \infty$, we have $R_n(\cH) \to 0, R_n(\cH_{\phi,\tau}) \to 0$ which implies  $G_n(\phi, \cH) \to \err_\cD(h^*) - \err_\cD(h^*_\tau) \leq 0$. On the other hand, explanation constraints help when $n$ is small. For $t$ large enough, we expect $\err_\cD(h^*) - \err_\cD(h^*_\tau)$ to be small, so that our notion of goodness is dominated by the first term: $R_n(\cH) - R_n(\cH_{\phi, \tau})$, which has the simple interpretation of reduction in model complexity.

\section{Examples for Generalizable constraints}
\label{appendix: EPAC learnable}

In this section, we look at the Rademacher complexity of $\cG$  for different explanation constraints to characterize how many samples with explanation constraints are required in order to generalize to satisfying the explanation constraints on unseen data. We remark that this is a different notion of sample complexity; these unlabeled data require annotations of explanation constraints, not standard labels. In practice, this can be easier and less expertise might be necessary if define the surrogate loss $\phi$ directly. First, we analyze the case where our explanation is given by the gradient of a linear model.
\begin{proposition}[Learning a gradient constraint for  linear models]
Let $\cD$ be a distribution over $\R^d$. Let $\cH = \{h: x \mapsto \langle w_h, x \rangle \mid w_h \in \mathbb{R}^d, \lVert w_h \rVert_2 \leq B\}$ be a class of linear models that pass through the origin. Let $\phi(h,x) = \theta(w_h, w_{h'})$ be a surrogate explanation loss. Let $\cG = \{\phi(h,\cdot) \mid h \in \cH\}$, then we have
\begin{equation*}
    R_n(\cG) \leq  \frac{\pi}{2\sqrt{m}}. 
\end{equation*}
\end{proposition}
\begin{proof}
    For a linear separator, $\phi(h,\cdot)$ is a constant function over $\cX$. The Rademacher complexity is given by
    \begin{align*}
        R_n(\cG) &= \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{\phi(h,\cdot) \in\cG}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i\phi(h,x_i) \right)\right] \right]\\
        &= \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{h \in \cH}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i \right)\theta(w_h, w_{h'})\right] \right]\\
        &= \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\left(\frac{1}{m}\sum_{i=1}^m \sigma_i \right)\sup_{h \in \cH}\theta(w_h, w_{h'})\right] \right] \\
         &= \frac{\pi}{2}\displaystyle \displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\left|\frac{1}{m}\sum_{i=1}^m \sigma_i \right|\right]  \\
        &\leq  \frac{\pi}{2\sqrt{m}}.
    \end{align*}
\end{proof}

We  compare this with the Rademacher complexity of linear models  which is given by $R_m(\cH) \leq \frac{B}{\sqrt{m}}$. The upper bound does not depend on the upper bound on the weight $B$. In practice, we know that the gradient of a linear model is constant for any data point. This implies that knowing a gradient of a single point is enough to identify the gradient of the linear model. 

% First learning to restrict out class to $\cH_{\phi,\tau}$ extremely helpful for the performance and sample complexity, as is evident in Figure \ref{fig:linear_mse}.

% This makes sense as the input gradient for a linear separator is fixed, and knowing a gradient of a single point is enough to identify the true linear separator. 

% Therefore, in this case, $\cG$ actually has a significantly smaller Rademacher complexity. First learning to restrict our class to $\cG$ is 

% extremely helpful for the performance and sample complexity, as is evident in Figure \ref{fig:linear_mse}.

We consider another type of explanation constraint that is given by a noisy model. Here, we could observe either a noisy classifier and noisy regressor, and the constraint could be given by having similar outputs to this noisy model. This is reminiscent of learning with noisy labels \citep{natarajan2013learning} or weak supervision \citep{ratner2016data, ratner2017snorkel, pukdee2022label}. In this case, our explanation $g$ is simply the hypothesis element $h$ itself, and our constraint is on the values that $h(x)$ can take. We first analyze this in the classification setting.

\begin{proposition}[Learning a constraint given by a noisy classifier]
Let $\cD$ be a distribution over $\R^d$. Consider a binary classification task with $\cY = \{-1,1\}$. Let $\cH$ be a hypothesis class. Let $\phi(h,x) = 1[h(x) \neq h'(x)]$ be a surrogate explanation loss. Let $\cG = \{\phi(h,\cdot) \mid h \in \cH\}$, then we have
\begin{equation*}
    R_n(\cG) = \frac{1}{2}R_n(\cH). 
\end{equation*}
\end{proposition}
\begin{proof}
        \begin{align*}
        R_n(\cG) &= \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{\phi(h,\cdot) \in\cG}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i\phi(h,x_i) \right)\right] \right]\\
        &= \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{h \in \cH}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i (\frac{1-h(x)h'(x)}{2})\right)\right] \right]\\
        &= \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{h \in \cH}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i (\frac{h(x)h'(x)}{2})\right)\right] \right]\\
        &= \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{h \in \cH}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i (\frac{h(x)}{2})\right)\right] \right]\\
        &= \frac{1}{2}R_n(\cH).
        \end{align*}
\end{proof}

Here, to learn the restriction of $\cG$ is on the same order of $R_n(\cH)$. For a given noisy regressor, we observe slightly different upper bound.

\begin{proposition}[Learning a constraint given by a noisy regressor]
Let $\cD$ be a distribution over $\R^d$. Consider a regression task with $\cY = \R$. Let $\cH$ be a hypothesis class that $\forall h \in \cH, -h \in \cH$. Let $\phi(h,x) = |h(x) - h'(x)|$ be a surrogate explanation loss. Let $\cG = \{\phi(h,\cdot) \mid h \in \cH\}$, then we have

\begin{equation*}
    R_n(\cG) \leq 2R_n(\cH).
\end{equation*}

\end{proposition}

\begin{proof}
        \begin{align*}
        R_n(\cG) &= \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{\phi(h,\cdot) \in\cG}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i\phi(h,x_i) \right)\right] \right]\\
        &= \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{h \in \cH}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i |h(x_i) - h'(x_i)|\right)\right] \right]\\ 
        & = \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{h \in \cH}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i \max(0, h(x_i) - h'(x_i)) + \frac{1}{m}\sum_{i=1}^m  \sigma_i \max(0, h'(x_i) - h(x_i)) \right)\right] \right]\\
        % & \leq \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{h \in \cH}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i |h(x)| + \sigma_i | h'(x)|\right)\right] \right]\\
        & \leq \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{h \in \cH}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i \max(0, h(x_i) - h'(x_i)) \right) \right] \right] + \quad \\
        & \quad \quad \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[ \sup_{h\in\cH} \left( \frac{1}{m}\sum_{i=1}^m  \sigma_i \max(0, h'(x_i) - h(x_i)) \right)\right] \right] \\
        & \leq \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{h \in \cH}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i (h(x_i) - h'(x_i)) \right) \right] \right] +  \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[ \sup_{h\in\cH} \left( \frac{1}{m}\sum_{i=1}^m  \sigma_i ( h'(x_i) - h(x_i)) \right)\right] \right],
        \end{align*}
where in the last line, we apply Talgrand's lemma \ref{lemma:talgrand} and note that the max function $\max(0, h(x))$ is 1-Lipschitz; in the third line, we note that we break up the supremum as both terms by definition of the $\max$ function are non-negative. Then, noting that we do not optimize over $h'(x)$, we further simplify this as
\begin{align*}
    R_n(\cG) & \leq \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{h \in \cH}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i h(x_i) \right) \right] \right]+  \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[ \sup_{h\in\cH} \left( \frac{1}{m}\sum_{i=1}^m  \sigma_i ( - h(x_i)) \right)\right] \right] \\
    & \leq 2 R_n(\cH).
\end{align*}\end{proof}
% As mentioned before, knowing apriori the target function $f'$ might be too strong. Therefore, we also consider the case when the constraints are from a learnable class of noisy regressors, $\cC$. Here, we note that $C(x) \in \cC$ is \textit{not known}, and we must determine $C(x)$ from unlabeled data that are annotated with explanations. This requires some assumptions about the distribution $\cD$. 

As mentioned before, knowing apriori surrogate loss $\phi$ might be too strong. In practice, we may only have access to the instances $\phi(\cdot, x_i)$ on a set of samples $S = \{x_1,\dots, x_k\}$. We also consider the case when $\phi(h,x) = |h(x) - h'(x)|$ when $h'$ is unknown and $h'$ belongs to a learnable class $\cC$. 

\begin{proposition}[Learning a constraint given by a noisy regressor from some learnable class $\cC$]
Assume $\cD$ is a distribution over  $\R^d$. Let $\cH$ and $\cD$ be hypothesis classes.   Let $\phi_{h'}(h,x) = |h(x) - h'(x)|$ be a surrogate explanation loss of a constraint corresponding to $h'$. Let $\cG_\cC=\{\phi_{h'}(h, \cdot)|h \in \cH, h' \in \cC \}$, then we have
% \begin{equation*}
%     R_n(\cG) = \frac{B}{\sqrt{n}} \left( 1 + \sin(\tau) \cdot p + \frac{1 - p}{2} \right),
% \end{equation*}

\begin{equation*}
    R_n(\cG_\cC) \leq 2 R_n(\cH) + 2R_n(\cC).
\end{equation*}

\end{proposition}

\begin{proof}
    \begin{align*}
        R_n(\cG_\cC) &= \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{\phi(h,\cdot) \in\cG_\cC}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i\phi(h,x_i) \right)\right] \right]\\
        &= \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{\substack{h \in \cH, \\  h' \in \cC}}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i |h(x_i) - h'(x_i)|\right)\right] \right]\\
        & \leq \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{\substack{h \in \cH, \\  h' \in \cC}}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i \max(0, h(x_i) - h'(x_i)) \right) \right] \right] \quad + \\
        & \quad \quad \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[ \sup_{\substack{h \in \cH, \\  h' \in \cC}}\left( \frac{1}{m}\sum_{i=1}^m  \sigma_i \max(0, h'(x_i) - h(x_i)) \right)\right] \right]\\
        & \leq \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{\substack{h \in \cH, \\  h' \in \cC}}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i (h(x_i) - h'(x_i)) \right) \right] \right] \quad + \\
        & \quad \quad \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[ \sup_{\substack{h \in \cH, \\  h' \in \cC}}\left( \frac{1}{m}\sum_{i=1}^m  \sigma_i ( h'(x_i) - h(x_i)) \right)\right] \right]
    \end{align*}
    where the lasts line again holds by an application of Talgrand's lemma. In this case, we indeed are optimizing over $h'$, so we get that
    \begin{align*}
        R_n(\cG_\cC) & \leq 2 \cdot \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{h \in \cH}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i (h(x_i) ) \right) \right] \right] + 2\cdot \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{h' \in \cC}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i (h'(x_i)) \right) \right] \right] \\
        & = 2 R_n(\cH) + 2R_n(\cC).
    \end{align*}
\end{proof}
We remark that while this value is much larger than that of $R_n(\cH)$, we only need information about $\phi(h, x)$ and \emph{not} the true label. Therefore, in many cases, this is preferable and not as expensive to learn. 

% In addition, when $\cD$ is uniform over a unit sphere, we can invoke our result in Theorem \ref{theorem:bound_linear} to get the result that
%     \begin{align*}
%         R_n(\cG_\cC) & \leq 2\frac{B}{\sqrt{n}} + 2\frac{B}{\sqrt{n}} \left( \sin(\tau) p + \frac{1 - p}{2} \right),
%     \end{align*}
%     where $$ p = \erf\left( \frac{\sqrt{d} \sin(\tau)}{\sqrt{2}}\right).$$ 

% \begin{proof}
%     \begin{align*}
%         R_n(\cG) &= \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{\phi(h,\cdot) \in\cG}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i\phi(h,x_i) \right)\right] \right]\\
%         &= \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{\substack{h \in \cH, \\ h' | \theta(w_h', w_{h^*}) \leq \tau}}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i |h(x) - h'(x)|\right)\right] \right]\\
%         & \leq \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{h \in \cH}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i h(x)\right) + \sup_{h' | \theta(w_h', w_h^*) \leq \tau} \left( \sum_{i=1}^m \sigma_i h'(x) \right)\right] \right]\\
%         &= R_n(\cH) + R_n(\cH'),
%     \end{align*}
%     where $\cH'$ is the class of linear models constrained to have gradient with angle $\theta(w_h, w_h^*) \leq \tau$. As $\cD$ is uniform over a unit sphere, we can invoke our result in Theorem \ref{theorem:bound_linear} to get the result that
%     \begin{align*}
%         R_n(\cG) & = \frac{B}{\sqrt{n}} + \frac{B}{\sqrt{n}} \left( \sin(\tau) p + \frac{1 - p}{2} \right).
%     \end{align*} \end{proof}

% \begin{equation*}
%     R_n(\cG) \leq \frac{2B}{\sqrt{n}} \left( 1 + \sin(\tau) \cdot p + \frac{1 - p}{2} \right),
% \end{equation*}

% where 
% $$ p = \erf\left( \frac{\sqrt{d} \sin(\tau)}{\sqrt{2}}\right)$$



\section{Proof of Theorem \ref{thm: generalization bound agnostic}}\label{appx:generalization_bound_agnostic}

We consider the agnostic setting of Theorem \ref{thm: generalization bound agnostic}. Here, we have two notions of deviations; one is deviation in a model's ability to satisfy explanations, and the other is a model's ability to generalize to correctly produce the target function.
\begin{proof}
    From Rademacher-based uniform convergence, for any $h \in \cH$, with probability at least $1 - \delta/2$ over $S_E$
\begin{equation*}
    |\phi(h, \cD) - \phi(h, S_E)| \leq 2R_k(\cG) + \sqrt{\frac{\ln(4/\delta)}{2k}} = \varepsilon_k
\end{equation*}
 Therefore, with probability at least $1 - \delta/2$, for any $h \in \cH_{\phi,t - \varepsilon_k}$  we also have $\phi(h, S_E) \leq t $ and for any $h$ with $\phi(h, S_E) \leq t$, we have $h \in \cH_{\phi, t + \varepsilon_k}$. In addition, by a uniform convergence bound, with probability at least $1 - \delta / 2$, for any $h \in \cH_{\phi, t + \varepsilon_k}$
\begin{align*}
    |\err_\cD(h) - \err_S(h)| &\leq R_n(\cH_{\phi, t + \varepsilon_k}) + \sqrt{\frac{\ln(4/\delta)}{2n}}.
\end{align*}
Now, let $h'$ be the minimizer of $\err_S(h)$ given that $\phi(h, S_E) \leq t$. By previous results, with probability $1 - \delta$, we have $h' \in \cH_{\phi, t + \varepsilon_k}$ and 
\begin{align*}
    \err_\cD(h') &\leq \err_S(h') +  R_n(\cH_{\phi, t + \varepsilon_k}) + \sqrt{\frac{\ln(4/\delta)}{2n}}\\
    &\leq \err_S(h^*_{t-\varepsilon_k}) +  R_n(\cH_{\phi, t + \varepsilon_k}) + \sqrt{\frac{\ln(4/\delta)}{2n}} \\
    &\leq \err_\cD(h^*_{t-\varepsilon_k}) + 2 R_n(\cH_{\phi, t + \varepsilon_k}) + 2\sqrt{\frac{\ln(4/\delta)}{2n}}.
\end{align*}
\end{proof}

\cready{
\section{EPAC-PAC learnability}
\label{appendix: EPAC-PAC}
We note that in our definition of EPAC learnability, we are only concerned with whether a model achieves a lower surrogate loss than $\tau$. However, the objective of minimizing the EPAC-ERM objective is to achieve both low average error and low surrogate loss. We characterize this property as EPAC-PAC learnability.

\begin{definition}
    [EPAC-PAC learnability] For any $\delta \in (0,1), \tau > 0$, the sample complexity of $(\delta, \tau, \gamma)$ - EPAC learning of $\cH$ with respect to a surrogate loss $\phi$, denoted $m(\delta, \tau, \gamma; \cH, \phi)$ is defined as the smallest $m\in \mathbb{N}$ for which there exists a learning rule $\cA$ such that every data distribution $\cD_\cX$ over $\cX$, with probability at least $1-\delta$ over $S\sim \cD^m$,
$$
\phi(\cA(S), \cD) \leq \inf_{h \in \cH} \phi(h, \cD) + \tau    
$$
and
$$
\err_\cD(\cA(S)) \leq \inf_{h \in \cH} \err_\cD(h) + \gamma.
$$
If no such $m$ exists, define $m(\delta, \tau, \gamma; \cH, \phi) = \infty$. We say that $\cH$ is EPAC-PAC learnable in the agnostic setting with respect to a surrogate loss $\phi$ if $\forall \delta \in (0,1), \tau > 0$,  $m(\delta, \tau, \gamma; \cH, \phi)$ is finite.
\end{definition}

}
\section{A Generalization Bound in the Realizable Setting}
\label{appendix: realizable setting}
In this section, we assume that we are in the doubly realizable \cite{balcan2010discriminative} setting where there exists $h^*\in \cH$ such that $\error_{\cD}(h^*) = 0$ and $\phi(h^*, \cD) = 0$. The optimal classifier $h^*$ lies in $\cH$ and also achieve zero expected explanation loss. In this case, we want to output a hypothesis $h$ that achieve both zero empirical risk and empirical explanation risk.

\begin{theorem}
[Generalization bound for the doubly realizable setting]
For a hypothesis class $\cH$, a distribution $\cD$ and an explanation loss $\phi$. Assume that there exists $h^* \in \cH$ that $\err_\cD(h^*) = 0$ and $\phi(h^*, \cD) = 0$. Let  $S = \{(x_1, y_1), \dots, (x_n, y_n) \}$ is drawn i.i.d. from $\cD$ and $S_E = \{ x'_1,\dots, x'_k\}$ drawn i.i.d. from $\cD_\cX$. With probability at least $1-\delta$, for any $h \in \cH$ that $\err_S(h) = 0$ and $\phi(h, S_E) = 0$, we have
\begin{equation*}
    \err_D(h) \leq R_n(\cH_{\phi, \varepsilon_k}) + \sqrt{\frac{\ln(2/\delta)}{2n}}
\end{equation*}
when 
\begin{equation*}
    \varepsilon_k = 2R_k(\cG) + \sqrt{\frac{\ln(2/\delta)}{2k}}
\end{equation*}
when $\cG = \{\phi(h, x) \mid  h \in \cH, x \in \cX\}$.

\end{theorem}
\begin{proof}
We first consider only classifiers than has low empirical explanation loss and then perform standard supervised learning. From Rademacher-based uniform convergence, for any $h \in \cH$, with probability at least $1 - \delta/2$ over $S_E$
\begin{equation*}
    \phi(h, \cD) \leq \phi(h, S_E)  + 2R_k(\cG) + \sqrt{\frac{\ln(2/\delta)}{2k}}
\end{equation*}
when $\cG = \{\phi(h, x) \mid  h \in \cH, x \in \cX\}$. Therefore, for any $h \in \cH$ with $\phi(h, S_E) = 0$, we have $h \in \cH_{\phi, \varepsilon_k}$ with probability at least $1 - \delta/2$. Now, we can apply the uniform convergence on $\cH_{\phi, \varepsilon_k}$. For any $h \in \cH_{\phi, \varepsilon_k}$ with $\err_S(h) = 0$, with probability at least $1 - \delta/2$, we have
\begin{equation*}
    \err_\cD(h) \leq  R_n(\cH_{\phi, \varepsilon_k}) + \sqrt{\frac{\ln(2/\delta)}{2n}}.
\end{equation*}
Therefore, for $h \in \cH$ that $\phi(h, S_E) = 0, \err_S(h) = 0$, we have our desired guarantee.
\end{proof}

We remark that, since our result relies on the underlying techniques of the Rademacher complexity, our result is on the order of $O(\frac{1}{\sqrt{n}})$. In the (doubly) realizable setting, this is somewhat loose, and more complicated techniques are required to produce tighter bounds. We leave this as an interesting direction for future work.

\section{Rademacher Complexity of Linear Models with a Gradient Constraint}\label{appendix: main theory linear}
We calculate the empirical Rademacher complexity of a linear model under a gradient constraint. 
\begin{figure}[ht]
    \hspace*{1cm}
    \includegraphics[width = 0.6\columnwidth]{figs/linear_f_v2.pdf}
    \caption{Illustration of different value of a function $f(v)$.}
    \label{fig: function f(v)}
\end{figure}
\begin{theorem}
[Empirical Rademacher complexity of linear models with a gradient constraint]
\label{theorem:linear_distribution_free}
Let $\cX$ be an instance space in $\R^d$, let  $\cD_\cX$ be a distribution on $\cX$, let $\cH = \{h: x \to \langle w_h, x \rangle \mid w_h \in \R^d, ||w_h||_2 \leq B\}$ be a class of linear model with weights bounded by some constant $B>0$ in $\ell_2$ norm. Assume that there exists a constant $C>0$ such that $\bbE_{x \sim \cD_\cX}[||x||_2^2] \leq C^2$. Assume that we have an explanation constraint in terms of gradient constraint; we want the gradient of our linear model to be close to the gradient of some linear model $h'$. Let $\phi(h,x) = \theta(w_h, w_{h'})$ be an explanation surrogate loss when $\theta(u,v)$ is an angle between $u,v$. For any $S=\{x_{1}, \dots, x_{n}\}$ is drawn i.i.d. from $\cD_\cX$, we have
$$
R_S(\cH_{\phi,\tau}) = \frac{B}{n} \bbE_{\sigma}\left[\lVert v \rVert f(v)\right].
$$
when $v = \sum_{i=1}^n x_i\sigma_i$ and

\begin{equation*}
    f(v) = \begin{cases}
        1 & \text{ when } \theta(v,w') \leq \tau \\
        \cos(\theta(v,w') - \tau) & \text{ when } \tau \leq \theta(v,w') \leq \frac{\pi}{2} + \tau\\
        0 & \text{ when } \theta(v,w') \geq \frac{\pi}{2} + \tau.
    \end{cases}
\end{equation*}
\end{theorem}
% \begin{proof}
%     (Sketch)
%      Recall that $\cH_{\phi,\tau} = \{h: x \to \langle w_h, x \rangle \mid w_h \in \R^d, ||w_h||_2 \leq B, \theta(w_h, w_{h'}) \leq \tau\}$. For a set of sample $S$, the empirical Rademacher complexity of $\cH_{\phi,\tau}$ is given by
%         \begin{align*}
%         R_S(\cH_{\phi,\tau}) &= \frac{1}{n} \bbE_{\sigma}\left[\sup_{h \in \cH_{\phi,\tau}} \sum_{i=1}^n h(x_i)\sigma_i\right]\\
%         &= \frac{1}{n} \bbE_{\sigma}\left[\sup_{\substack{\lVert w_h\rVert_2 \leq B\\ \theta(w_h, w_{h'}) \leq \tau}} \sum_{i=1}^n \langle w_h, x_i \rangle\sigma_i\right]\\
%         &= \frac{1}{n} \bbE_{\sigma}\left[\sup_{\substack{\lVert w_h\rVert_2 \leq B\\ \theta(w_h, w_{h'}) \leq \tau}}  \langle w_h, \sum_{i=1}^n x_i\sigma_i \rangle\right].
%     \end{align*}
%  For a vector $w'\in \R^d$ with $\lVert w'\rVert_2 = 1$, and a vector $v\in \R^d$, we will claim the following, 
% \begin{enumerate}
%     \item If $\theta(v, w') \leq \tau$, we have
%     $$
% \sup_{\substack{\lVert w\rVert_2 \leq B\\ \theta(w,w') \leq \tau}}  \langle w, v \rangle = B\lVert v \rVert.
% $$
% \item If $\frac{\pi}{2} + \tau \leq \theta(v, w') \leq \pi$, we have
%     $$
% \sup_{\substack{\lVert w\rVert_2 \leq B\\ \theta(w,w') \leq \tau}}  \langle w, v \rangle = 0.
% $$
% \item If $\tau \leq \theta(v, w') \leq \frac{\pi}{2} + \tau $, we have
% $$
% \sup_{\substack{\lVert w\rVert_2 \leq B\\ \theta(w,w') \leq \tau}}  \langle w, v \rangle = B\lVert v\rVert \cos(\theta(v, w') - \tau).
% $$
% \end{enumerate}
% We can use this to calculate the empirical Rademacher complexity of $\cH_{\phi,\tau}$. For full proof, we refer to Appendix \ref{appendix: main theory linear}.


% \end{proof}
\begin{figure}[h!]
\centering
    \includegraphics[width = 0.6\columnwidth]{figs/linear_2_cases.pdf}
    \caption{Benefits of an explanation constraint also depend on the data distribution. We represent data points $x_i$ with red squares (Left). The possible regions for $v = \sum_{i=1}^n x_i\sigma_i$ are the shaded areas (Right). When the data is highly correlated with $w'$, $v$ would lie in a region where $f(v)$ is large (Top) and this implies that the gradient constraints provide less benefits. On the other hand, when the data is almost orthogonal to $w'$, $v$ would lie in a region with a small value of $f(v)$ (Bottom) which leads to more benefits from the gradient constraints}. 
    \label{fig: linear case benefit}
\end{figure}

\noindent For the proof, we refer to Appendix \ref{appendix: main theory linear} for full proof. We compare this with the standard bound on linear models which is given by
$$
R_S(\cH) = \frac{B}{n}\bbE_\sigma[\lVert v \rVert].
$$
The benefits of the explanation constraints depend on the underlying data distribution; in the case of linear models with a gradient constraint, this depends on an angle between $v = \sum_{i=1}^n x_i \sigma_i$ and $w'$. The explanation constraint reduces the term inside the expectation by a factor of $f(v)$ depending on $\theta(v,w')$.  When $\theta(v,w') \leq \tau$ then $f(v) = 1$ which implies that there is no reduction. The value of $f(v)$ decreases as the angle between $\theta(v,w')$ increases and reaches $f(v) = 0$ when $\theta(v,w') \geq \frac{\pi}{2} + \tau$. When the data is concentrated around the area of $w'$, the possible regions for $v$ would be close to $w'$ or $-w'$  (Figure \ref{fig: linear case benefit} (Top)). The value of $f(v)$ in this region would be either $1$ or $0$ and the reduction would be $\frac{1}{2}$ on average. In essence, this means that the gradient constraint of being close to $w'$ does not actually tell us much information beyond the information from the data distribution. On the other hand, when the data points are nearly orthogonal to $w'$, the possible regions for $v$ would lead to a small $f(v)$ (Figure \ref{fig: linear case benefit} (Bottom)). This can lead to a large reduction in complexity. Intuitively, when the data is nearly orthogonal to $w'$, there are many valid linear models including those not close in angle to $w'$. The constraints allows us to effectively shrink down the class of linear models that are close to $w'$.

% This implies that many linear models, including those not close in angle to $w'$ are valid. In this setting, the gradient information indeed restricts the $\cH$ 


% As mentioned before, our restrictions may not be beneficial if the underlying data distribution is already concentrated about this restricted class of hypothesis. The bound above gives us a result that depends on the given linear model $w'$. Recall that $v = \sum_{i=1}^n x_i \sigma_i$, when $\theta(v, w') \leq \tau$, $v$ is highly correlated with $w'$. In essence, this means that the data concentrated around the area of $w'$, and the gradient constraint of being close to $w'$ does not actually tell us much information (Figure \ref{fig: linear case benefit} (Top)). This is illustrated by our bound not changing here, remaining as a factor of $\lVert v \rVert$. 

% However, in the case when $\theta(v, w') \geq \tau$, we observe that the data is concentrated in regions other than near $w'$. This implies that many linear models, including those not close in angle to $w'$ are valid. In this setting, the gradient information indeed restricts the $\cH$ effectively (Figure \ref{fig: linear case benefit} (Bottom)). This is manifested in our bound, now on the order of $\cos(\theta(v, w') - \tau) \cdot \lVert v \rVert$. We remark that $\cos(\theta(v, w') - \tau)$ is the angle between $v$ and a linear model that is within angle $\tau$ of $w$. As this increases (to the value of $\frac{\pi}{2}$), we have a smaller upper bound.




\begin{proof}
(Proof of Theorem \ref{theorem:linear_distribution_free})
    Recall that $\cH_{\phi,\tau} = \{h: x \to \langle w_h, x \rangle \mid w_h \in \R^d, ||w_h||_2 \leq B, \theta(w_h, w_{h'}) \leq \tau\}$. For a set of sample $S$, the empirical Rademacher complexity of $\cH_{\phi,\tau}$ is given by
        \begin{align*}
        R_S(\cH_{\phi,\tau}) &= \frac{1}{n} \bbE_{\sigma}\left[\sup_{h \in \cH_{\phi,\tau}} \sum_{i=1}^n h(x_i)\sigma_i\right]\\
        &= \frac{1}{n} \bbE_{\sigma}\left[\sup_{\substack{\lVert w_h\rVert_2 \leq B\\ \theta(w_h, w_{h'}) \leq \tau}} \sum_{i=1}^n \langle w_h, x_i \rangle\sigma_i\right]\\
        &= \frac{1}{n} \bbE_{\sigma}\left[\sup_{\substack{\lVert w_h\rVert_2 \leq B\\ \theta(w_h, w_{h'}) \leq \tau}}  \langle w_h, \sum_{i=1}^n x_i\sigma_i \rangle\right].
    \end{align*}
 For a vector $w'\in \R^d$ with $\lVert w'\rVert_2 = 1$, and a vector $v\in \R^d$, we will claim the following, 
\begin{enumerate}
    \item If $\theta(v, w') \leq \tau$, we have
    $$
\sup_{\substack{\lVert w\rVert_2 \leq B\\ \theta(w,w') \leq \tau}}  \langle w, v \rangle = B\lVert v \rVert.
$$
\item If $\frac{\pi}{2} + \tau \leq \theta(v, w') \leq \pi$, we have
    $$
\sup_{\substack{\lVert w\rVert_2 \leq B\\ \theta(w,w') \leq \tau}}  \langle w, v \rangle = 0.
$$
\item If $\tau \leq \theta(v, w') \leq \frac{\pi}{2} + \tau $, we have
$$
\sup_{\substack{\lVert w\rVert_2 \leq B\\ \theta(w,w') \leq \tau}}  \langle w, v \rangle = B\lVert v\rVert \cos(\theta(v, w') - \tau)
$$
% $$
% \sup_{\substack{\lVert w\rVert_2 \leq B\\ \theta(w,w') \leq \tau}}  \langle w, v \rangle = B(\sin (\tau) \sqrt{\lVert v\rVert_2^2 - \langle v, w'\rangle^2)} + \cos (\tau) \langle v, w'\rangle).
% $$


\end{enumerate}

For the first claim, we can see that if $\theta(v,w') \leq \tau$, we can pick $w = \frac{Bv}{\lVert v \rVert}$ and achieve the optimum value. For the second claim, we use the fact that $\theta(\cdot, \cdot)$ satisfies a triangle inequality and for any $w$ that $\theta(w,w') \leq \tau$, we have
\begin{align*}
    \theta(v,w) + \theta(w,w') &\geq \theta(v,w')\\
    \theta(v,w) \geq \theta(v,w') - \theta(w,w')\\
    \theta(v,w) \geq \frac{\pi}{2} + \tau - \tau = \frac{\pi}{2}.
\end{align*}
This implies that for any $w$ that $\theta(w,w') \leq \tau$, we have
$\langle w, v \rangle = \lVert w \rVert \lVert v \rVert \cos(\theta(v,w)) \leq 0$ and the supremum is given by $0$ where we can set $\lVert w \rVert = 0$. For the third claim, we know that $\langle w, v \rangle$ is maximum when the angle between $v,w$ is the smallest. From the triangle inequality above, we must have $\theta(w,w') = \tau$ to be the largest possible value so that we have the smallest lower bound $\theta(v,w) \geq \theta(v,w') - \theta(w,w')$. In addition, the inequality holds when $v,w',w$ lie on the same plane. Since we do not have further restrictions on $w$, there exists such $w$ and we have 
$$
\sup_{\substack{\lVert w\rVert_2 \leq B\\ \theta(w,w') \leq \tau}}  \langle w, v \rangle = B\lVert v\rVert \cos(\theta(v, w') - \tau)
$$
as required. One can calculate a closed form formula for $w$ by solving a quadratic equation. Let $w = \frac{B\Tilde{w}}{\lVert \Tilde{w} \rVert}$ when $\Tilde{w} = v + \lambda w'$ for some constant $\lambda > 0$ such that $\theta(w, w') = \tau$. With this we have an equation
\begin{align*}
    \frac{\langle \Tilde{w}, w'\rangle}{\lVert \Tilde{w} \rVert} &= \cos(\tau)\\
    \frac{\langle v + \lambda w', w'\rangle}{\lVert v + \lambda w' \rVert} &= \cos(\tau)\\
\end{align*}
Let $\mu = \langle v, w'\rangle$, solving for $\lambda$, we have
\begin{align*}
    \frac{\mu + \lambda}{\sqrt{\lVert v \rVert^2 + 2\lambda\mu + \lambda^2}} &= \cos(\tau)\\
    \mu^2 + 2\mu\lambda + \lambda^2 &= \cos^2(\tau)(\lVert v \rVert^2 + 2\lambda\mu + \lambda^2)\\
    \sin^2(\tau)\lambda^2 + 2\sin^2(\tau)\mu\lambda + \mu^2 - \cos^2(\tau)\lVert v \rVert^2 &= 0\\
     \lambda^2 + 2\mu\lambda + \frac{\mu^2}{\sin^2(\tau)} - \cot^2(\tau)\lVert v \rVert^2 &= 0\\
\end{align*}
Solve this quadratic equation, we have
$$
\lambda = -\mu \pm \cot(\tau)\sqrt{\lVert v \rVert^2 - \mu^2}.
$$
Since $\lambda > 0$, we have $\lambda = -\mu + \cot(\tau)\sqrt{\lVert v \rVert^2 - \mu^2}$. We have

\begin{align*}
    \Tilde{w} &= v + \lambda w'\\
    &= v+ (-\mu + \cot(\tau)\sqrt{\lVert v \rVert^2 - \mu^2})w'\\
    &= v - \langle v, w' \rangle w' + cot(\tau)w'\sqrt{\lVert v \rVert^2 - \mu^2}.
\end{align*}


With these claims, we have
\begin{align*}
    R_S(\cH_{\phi, \tau}) &= \frac{1}{n} \bbE_{\sigma}\left[\sup_{\substack{\lVert w_h\rVert_2 \leq B\\ \theta(w_h, w_{h'}) \leq \tau}}  \langle w_h, \sum_{i=1}^n x_i\sigma_i \rangle\right]\\
    &= \frac{B}{n} \bbE_{\sigma}\left[\lVert v \rVert 1\{\theta(v,w') \leq \tau\} + \lVert v \rVert 1\{\tau \leq \theta(v,w') \leq \frac{\pi}{2} + \tau \} \cos(\theta(v,w') - \tau)\right]\\
    &= \frac{B}{n} \bbE_{\sigma}\left[\lVert v \rVert f(v)\right].
\end{align*}

\end{proof}

\begin{theorem}
[Rademacher complexity of linear models with gradient constraint, uniform distribution on a sphere]
Let $\cX$ be an instance space in $\R^d$, let  $\cD_\cX$ be a uniform distribution on a unit sphere in $\R^d$, let $\cH = \{h: x \to \langle w_h, x \rangle \mid w_h \in \R^d, ||w_h||_2 \leq B\}$ be a class of linear model with weights bounded by some constant $B>0$ in $\ell_2$ norm. Assume that there exists a constant $C>0$ such that $\bbE_{x \sim \cD_\cX}[||x||_2^2] \leq C^2$. Assume that we have an explanation constraint in terms of gradient constraint; we want the gradient of our linear model to be close to the gradient of some linear model $h'$. Let $\phi(h,x) = \theta(w_h, w_{h'})$ be an explanation surrogate loss when $\theta(u,v)$ is an angle between $u,v$. We have 
\begin{align*}
    R_n(\cH_{\phi, \tau}) & = \frac{B}{\sqrt{n}} \left(\sin(\tau) \cdot p + \frac{1 - p}{2} \right),
\end{align*}
where
$$ p = \erf\left( \frac{\sqrt{d}\sin(\tau)}{\sqrt{2}}\right).$$
\end{theorem}
\begin{proof}
    From Theorem \ref{theorem:linear_distribution_free}, we have that
    \begin{align*}
        R_n(\cH_{\phi,\tau}) &= \bbE[R_S(\cH_{\phi,\tau})]\\
        &= \frac{B}{n} \bbE_\cD\left[\bbE_{\sigma}\left[\lVert v \rVert 1\{\theta(v,w') \leq \tau\} + \lVert v \rVert 1\{\tau \leq \theta(v,w') \leq \frac{\pi}{2} + \tau \} \cos(\theta(v,w') - \tau)\right]\right]\\
        &= \frac{B}{n} \bbE_\cD\left[\bbE_{\sigma}\left[\lVert v \rVert 1\{\theta(v,w') \leq \frac{\pi}{2} - \tau \} + \lVert v \rVert 1\{\frac{\pi}{2} - \tau \leq \theta(v,w') \leq \frac{\pi}{2} + \tau \} \cos(\theta(v,w') - \tau)\right]\right]
    \end{align*}
when $v = \sum_{i=1}^n x_i\sigma_i$. Because $x_i$ is drawn uniformly from a unit sphere, in expectation $\theta(v,w')$ has a uniform distribution over $[0,\pi]$ and the distribution $\lVert v \rVert$ for a fixed value of $\theta(v,w')$ are the same for all $\theta(v,w')\in [0,\pi]$. From Trigonometry, we note that
\begin{align*}
    \cos(\frac{\pi}{2} - 2\tau + a) + \cos(\frac{\pi}{2} - a) = \sin(2\tau - a) + \sin(a) \leq 2\sin(\tau).
\end{align*}
By the symmetry property and the uniformity of the distribution of $\theta(v,w')$ and $\lVert v \rVert$.
\small
\begin{align*}
    &\bbE_\cD\left[\bbE_{\sigma}\left[\lVert v \rVert 1\{\frac{\pi}{2} - \tau \leq \theta(v,w') \leq \frac{\pi}{2} + \tau \} \cos(\theta(v,w') - \tau)\right]\right]\\ 
    &= \bbE_\cD\left[\bbE_{\sigma}\left[\lVert v \rVert 1\{0 \leq \theta(v,w') \leq 2\tau \} \cos(\frac{\pi}{2} + \theta(v,w') - \tau)\right]\right]\\ 
    &= \bbE_\cD\left[\bbE_{\sigma}\left[\lVert v \rVert( 1\{0 \leq \theta(v,w') \leq \tau \} \cos(\frac{\pi}{2} + \theta(v,w') - \tau) + 1\{\tau \leq \theta(v,w') \leq 2\tau \} \cos(\frac{\pi}{2} + \theta(v,w') - \tau)) \right]\right]\\ 
        &= \bbE_\cD\left[\bbE_{\sigma}\left[\lVert v \rVert( 1\{0 \leq \theta(v,w') \leq \tau \} \cos(\frac{\pi}{2} + \theta(v,w') - \tau) + 1\{0 \leq 2\tau -\theta(v,w') \leq \tau \} \cos(\frac{\pi}{2} - (2\tau -  \theta(v,w')) )) \right]\right]\\ 
            &= \bbE_\cD\left[\bbE_{\sigma}\left[\lVert v \rVert( 1\{0 \leq \theta(v,w') \leq \tau \} \cos(\frac{\pi}{2} + \theta(v,w') - \tau) + 1\{0 \leq \Tilde{\theta}(v,w') \leq \tau \} \cos(\frac{\pi}{2} - \Tilde{\theta}(v,w') )) \right]\right]\\ 
        &= \bbE_\cD\left[\bbE_{\sigma}\left[\lVert v \rVert( 1\{0 \leq \theta(v,w') \leq \tau \} \cos(\frac{\pi}{2} + \theta(v,w') - \tau) + 1\{0 \leq \theta(v,w') \leq \tau \} \cos(\frac{\pi}{2} - \theta(v,w') )) \right]\right]\\ 
    &\leq
    \bbE_\cD\left[\bbE_{\sigma}\left[\lVert v \rVert 1\{\frac{\pi}{2} - \tau \leq \theta(v,w') \leq \frac{\pi}{2} + \tau \} \sin( \tau)\right]\right]
\end{align*}
\normalsize
when $\Tilde{\theta}(v,w') = \frac{\pi}{2} - \theta(v,w') $. We have
\begin{align*}
    R_n(\cH_{\phi,\tau}) &\leq \frac{B}{n}\bbE_\cD\left[\bbE_{\sigma}\left[ \lVert v \rVert 1\{\theta(v,w') \leq \frac{\pi}{2} - \tau \} + \lVert v \rVert 1\{\frac{\pi}{2} - \tau \leq \theta(v,w') \leq \frac{\pi}{2} + \tau \} \sin( \tau)\right]\right]\\
    &= \frac{B}{n}\bbE_\cD\left[\bbE_{\sigma}\left[ \lVert v \rVert\right]\right] ( \Pr(\theta(v,w') \leq \frac{\pi}{2} - \tau) + \Pr(\frac{\pi}{2} - \tau \leq \theta(v,w') \leq \frac{\pi}{2} + \tau)\sin(\tau))
\end{align*}
The last equation follows from the symmetry and uniformity properties. We can bound the first expectation
\begin{align*}
    \bbE_\cD[\bbE_\sigma \lVert v \rVert]] &= \bbE_\cD[\bbE_\sigma \lVert \sum_{i=1}^n x_i\sigma_i \rVert]]\\
    &\leq \bbE_\cD[\sqrt{\bbE_\sigma \lVert \sum_{i=1}^n x_i\sigma_i \rVert^2]}]\\
    &= \bbE_\cD[\sqrt{\bbE_\sigma \sum_{i=1}^n 
    \lVert x_i \rVert^2 \sigma_i^2 ]}]\\
    &\leq C\sqrt{n}.
\end{align*}
Next, we can simply note that, since our data is distributed over a unit sphere, each data has norm no greater than 1. Therefore, we know that $C = 1$ is indeed an upper bound on $\bbE_{x \sim \cD_\cX}[||x||_2^2]$. For the probability term, we note that in expectation $v$ has the same distribution as a random vector $u$ drawn uniformly from a unit sphere. We let this be some probability $p$:
\begin{align*}
 p = \Pr\left(\frac{\pi}{2} - \tau \leq \theta(v,w') \leq \frac{\pi}{2} + \tau \right) = \Pr \left(|\langle u, w' \rangle| \leq \sin(\tau)\right).
\end{align*}
We know that the projection $\langle u, w' \rangle \sim \cN(0, \frac{1}{d})$. 
% By a standard tail bound
% \begin{align*}
    % \Pr(|\langle u, w' \rangle| \leq \sin(\tau)) &= 1 -  \Pr(|\langle u, w' \rangle| \geq \sin(\tau)) \geq 1 - 2\exp(-\frac{\sin^2(\tau)}{2}).
% \end{align*}
Then, we have that $|\langle u, w' \rangle |$ is given by a Folded Normal Distribution, which has a CDF given by
\begin{align*}
    \Pr\left(|\langle u, w' \rangle | \leq \sin(\tau) \right) & = \frac{1}{2} \left[ \erf\left( \frac{ \sqrt{d} \sin(\tau)}{\sqrt{2}}\right) + \erf\left( \frac{\sqrt{d}\sin(\tau)}{\sqrt{2}}\right)  \right] \\
    & = \erf\left( \frac{\sqrt{d}\sin(\tau)}{\sqrt{2}}\right).
\end{align*}

We then observe that
\begin{align*}
    \Pr \left( \theta(v, w') \leq \frac{\pi}{2} - \tau \right) & = \frac{1}{2} \left( 1 -  \Pr \left( \frac{\pi}{2} - \tau \leq \theta(v, w') \leq \frac{\pi}{2} + \tau \right) \right)  \\
    & = \frac{1 - p}{2}
\end{align*}

Plugging this in yields the following bound
\begin{align*}
    R_n(\cH_{\phi, \tau}) & = \frac{B}{\sqrt{n}} \left(\sin(\tau) \cdot p + \frac{1 - p}{2} \right),
\end{align*}

where
$$ p = \erf\left( \frac{\sqrt{d}\sin(\tau)}{\sqrt{2}}\right).$$

\end{proof}


\section{Rademacher Complexity for Two Layer Neural Networks with a Gradient Constraint}\label{appx: main theory nn}

Here, we present the full proof of the generalization bound for two layer neural networks with gradient explanations. In our proof, we use two results from \citet{ma2022notes}. One result is a technical lemma, and the other is a bound on the Rademacher complexity of two layer neural networks. 

\begin{lemma}\label{lemma:TM_note}
    Consider a set $S = \{x_1, ..., x_n \}$ and a hypothesis class $\cF \subset \{f : \R^d \to \R\}$. If 
    $$ \sup_{f \in \cF} \sum_{i=1}^n f(x_i) \sigma_i \geq 0 \; \text{ for any } \; \sigma_i \in \{ \pm 1\}, i = 1, ..., n,$$
    then, we have that
    $$ \bbE_\sigma \left[ \sup_{f\in\cF} | \sum_{i=1}^n f(x_i) \sigma_i | \right] \leq 2 \bbE_{\sigma} \left[ \sup_{f \in \cF} \sum_{i=1}^n f(x_i) \sigma_i \right].$$\vspace{2mm}
    
\end{lemma}

\begin{theorem}
[Rademacher complexity for two layer neural networks \cite{ma2022notes}]\label{theorem:bound_2NNs}
Let $\cX$ be an instance space and $\cD_{\cX}$ be a distribution over $\cX$. Let $\cH = \{h : x \mapsto \sum_{i=1}^m w_i \sigma(u_i^\top x) | w_i \in \R, u_i \in \R^d, \sum_{i=1}^m |w_i| \lVert u_i \rVert_2 \leq B \}$ be a class of two layer neural networks with $m$ hidden nodes with a ReLU activation function $\sigma(x) = \max(0, x)$. Assume that there exists some constant $C > 0$ such that $\bbE_{x \sim \cD_{\cX}} [\lVert x \rVert_2^2 ] \leq C^2$. Then, for any $S=\{x_{1}, \dots, x_{n}\}$ is drawn i.i.d. from $\cD_\cX$, we have that
$$
R_S(\cH) \leq \frac{2B}{n}\sqrt{\sum_{i=1}^n||x_i||_2^2}
$$
and
$$ R_n(\cH) \leq \frac{2 B C}{\sqrt{n}}.$$
\end{theorem}

We defer interested readers to \cite{ma2022notes} for the full proof of this result. Here, the only requirement of the data distribution is that $\bbE_{x \sim \cD_{\cX}} [\lVert x \rVert_2^2 ] \leq C^2$. We now present our result in the setting of two layer neural networks with one hidden node $m = 1$ to provide clearer intuition for the overall proof.
\begin{theorem}
[Rademacher complexity for two layer neural networks ($m=1$) with gradient constraints]
Let $\cX$ be an instance space and $\cD_{\cX}$ be a distribution over $\cX$. Let $\cH = \{h : x \mapsto w\sigma(u^\top x) | w \in \R, u \in \R^d, |w| \leq B, \lVert u \rVert = 1 \}$. Without loss of generality, we assume that $\lVert u \rVert = 1$.  Assume that there exists some constant $C > 0$ such that $\bbE_{x \sim \cD_{\cX}} [ \lVert x \rVert_2^2 ] \leq C^2$. Our explanation constraint is given by a constraint on the gradient of our models, where we want the gradient of our learnt model to be close to a particular target function $h' \in \cH$. Let this be represented by an explanation loss given by $$\phi(h, x) = \lVert \nabla_x h(x) - \nabla_x h'(x) \rVert_2 + \infty \cdot 1 \{ \lVert \nabla_x h(x) - \nabla_x h'(x) \rVert > \tau \} $$ for some $\tau > 0$. Let $h'(x) = w'\sigma((u')^\top x)$ the target function, then we have
\begin{align*}
 R_n ( \cH_{\phi, \tau}) \leq  \frac{\tau C}{\sqrt{n}} & \quad \quad \text{ if } |w'|  > \tau, \\
 R_n ( \cH_{\phi, \tau}) \leq  \frac{3 \tau C}{\sqrt{n}} & \quad \quad \text{ if } |w'|  \leq  \tau.
\end{align*}
\end{theorem}
\begin {proof}
Our choice of $\phi(h, x)$ guarantees that, for any $h \in \cH_{\phi, \tau}$, we have that $ \lVert \nabla_x h(x) - \nabla_x h'(x) \rVert \leq \tau$ almost everywhere. We note that for $h(x) = w \sigma(u^\top x)$, the gradient is given by $\nabla_x h(x) = wu 1\{u^\top x > 0 \}$, which is a piecewise constant function over two regions (i.e., $u^\top x > 0, u^\top x \leq 0)$, captured by Figure \ref{fig:piecewise_constant}.

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{figs/2NNS_m1.pdf}
\label{fig:piecewise_constant}
\caption{Visualization of the piecewise constant function of $\nabla_x h(x) - \nabla_x h'(x)$ over 4 regions.}
\end{figure}
We now consider $\nabla_x h(x) - \nabla_x h'(x)$, and we have 3 possible cases.

\textbf{Case 1:} $\theta(u, u') > 0$ \\
This implies that the boundaries of $\nabla_x(h)$ and  $\nabla_x h'(x)$ are different. Then, we have that $\nabla_x h(x) - \nabla_x h'(x)$ is a piecewise constant function with 4 regions, taking on values
% \begin{enumerate}
%     \item  \makebox[3cm]{$wu - w'u'$} when $u^\top x > 0, (u')^\top x > 0$
%     \item  \makebox[3cm]{$wu$} when $u^\top x > 0, (u')^\top x < 0$
%     \item  \makebox[3cm]{$- w'u'$} when $u^\top x < 0, (u')^\top x > 0$
%     \item  \makebox[3cm]{$0$} when $u^\top x < 0, (u')^\top x < 0$.
% \end{enumerate}
\begin{equation*}
    \nabla_x h(x) - \nabla_x h'(x) = \begin{cases}
        wu - w'u' & \text{ when } u^\top x > 0, (u')^\top x > 0 \\
        wu & \text{ when } u^\top x > 0, (u')^\top x < 0 \\
        -w'u' & \text{ when } u^\top x < 0, (u')^\top x > 0 \\
        0 & \text{ when } u^\top x < 0, (u')^\top x < 0
    \end{cases}
\end{equation*}

If we assume that each region has probability mass greater than 0 then our constraint $\lVert \nabla_x h(x) - \nabla_x h'(x) \rVert_2 \leq \tau$ implies that $|w| = |w| \lVert u \rVert \leq \tau, |w'| =  |w'| \lVert u' \rVert \leq \tau, \lVert wu - w'u'\rVert \leq \tau$.

\textbf{Case 2:} $\theta(u, u') = 0$ \\
This implies that the boundary of $\nabla_x h(x)$ and $\nabla_x h'(x)$ are the same. Then, $\nabla_x h(x) - \nabla_x h'(x)$ is a piecewise constant over two regions
% \begin{enumerate}
%     \item  \makebox[3cm]{$wu - w'u'$} when $u^\top x > 0$
%     \item  \makebox[3cm]{$0$} when $u^\top x < 0$.
% \end{enumerate}

\begin{equation*}
    \nabla_x h(x) - \nabla_x h'(x) = \begin{cases}
        wu - w'u' & \text{ when } u^\top x > 0 \\
        0 & \text{ when } u^\top x < 0
    \end{cases}
\end{equation*}

This gives us that $|w-w'| = \lVert wu - w'u' \rVert \leq \tau$.

\textbf{Case 3:} $\theta(u, u') = \pi$ \\
Here, we have that the decision boundaries of $\nabla_x h(x)$ and $\nabla_x h'(x)$ are the same but the gradients are non-zero on different sides. Then, $\nabla_x h(x) - \nabla_x h'(x)$ is a piecewise constant on two regions
% \begin{enumerate}
%     \item  \makebox[3cm]{$wu$} when $u^\top x > 0$
%     \item  \makebox[3cm]{$w'u'$} when $u^\top x < 0$.
% \end{enumerate}

\begin{equation*}
    \nabla_x h(x) - \nabla_x h'(x) = \begin{cases}
        wu & \text{ when } u^\top x > 0 \\
        -w'u' & \text{ when } u^\top x < 0
    \end{cases}
\end{equation*}

This gives us that $|w| \leq \tau$ and $|w'| \leq \tau$.

These different cases tell us that the constraint $\lVert \nabla_x h(x) - \nabla_x h'(x) \rVert \leq \tau$ reduces $\cH$ into a class of models follows either

\begin{enumerate}
    \item $u = u'$ and $|w-w'| < \tau$.
    \item $u \neq u'$ and $|w| < \tau$. However, this case only possible when $|w'| < \tau$.
\end{enumerate}
If $|w'| > \tau$, we know that we must only have the first case.
Now, we can calculate the Rademacher complexity of our restricted class $\cH_{\phi, \tau}$. We will again do this in separate cases.
% \begin{enumerate}
%     \item $\theta(u, u') = 0$, the model $h$ must have the same boundary as $h'$ and furthermore
%     \begin{align*}
%         \lVert wu - w'u' \rVert_2 & = \lVert w \lVert u \rVert \overline{u} - w' \lVert u' \rVert \overline{u'} \rVert_2 & (\lVert \overline{u} \rVert_2 = \lVert \overline{u'}\rVert_2 = 1)\\
%         & = \lVert(w \lVert u \rVert - w' \lVert u' \rVert) \overline{u} \rVert_2 & (\overline{u} = \overline{u'}) \\
%         & = | w \lVert u \rVert - w' \lVert u' \rVert | < \tau.
%     \end{align*}
%     Then,  only the weight $w$ of hypothesis $h$ can differ from that of $h'$ by at most $\tau$. 
%     \item $|w| \lVert u \rVert < \tau$, the model $h$ must have weights with norm smaller than $\tau$. However, this is only possible when $\tau$ is larger than the norm of the weights of $h' (|w'| \lVert u' \rVert < \tau)$.

%     If $h'$ has weights with larger norm than $\tau$, we know that by (1) that the restricted class $\cH$ must only contain models with the same decision boundary ($u'$) as $h'$. 
% \end{enumerate}


\textbf{Case 1:} $|w'| > \tau$ \\
For any $h \in \cH_{\phi, \tau}$, we have that $u = u'$ and $|w - w'| < \tau$.
For a sample $S = \{x_1, ..., x_n\}$, 
\begin{align*}
    R_s(\cH_{\phi, \tau}) & = \frac{1}{n} \bbE_{\sigma} \left[ \sup_{h \in \cH_{\phi, \tau}} \sum_{i=1}^n h(x_i)\sigma_i \right] \\
    & = \frac{1}{n} \bbE_{\sigma} \left[ \sup_{w} \sum_{i=1}^n w \sigma((u')^\top x_i) \sigma_i \right] & (\text{ as } u = u') \\
    & = \frac{1}{n} \bbE_{\sigma} \left[ \sup_{w} w \left(\sum_{i=1}^n \sigma((u')^\top x_i) \sigma_i \right) \right].
\end{align*}

Since, $|w-w'| < \tau$,
$$
w' - \tau < w < w' + \tau
$$

Then, we can compute the supremum over $w$ as

$$ w = \begin{cases}
    w' - \tau & \text{ if } \left( \sum_{i=1}^n \sigma( (u')^\top x_i) \sigma_i \right) < 0\\
    w' + \tau & \text{ if } \left( \sum_{i=1}^n \sigma( (u')^\top x_i) \sigma_i \right) \geq 0
\end{cases}$$

Therefore, we have
$$ \sup_{w} w \left( \sum_{i=1}^n \sigma((u')^\top x_i) \sigma_i \right) = \left( w' + \tau \operatorname{sign} \left(\sum_{i=1}^n \sigma( (u')^\top x_i) \sigma_i \right) \right) \cdot \left(\sum_{i=1}^n \sigma((u')^\top x_i) \sigma_i \right).$$

Now, we can calculate the Rademacher complexity as 
\begin{align*}
    R_S(\cH_{\phi, \tau}) & = \frac{1}{n} \bbE_{\sigma} \left[ w' \left( \sum_{i=1}^n \sigma ((u')^\top x_i) \sigma_i \right) + \tau | \sum_{i=1}^n \sigma( (u')^\top x_i) \sigma_i | \right] \\
    & = \frac{\tau}{n} \bbE_\sigma \left[ |\sum_{i=1}^n \sigma( (u')^\top x_i) \sigma_i |\right]  \\
    & \leq \frac{\tau}{n} \sqrt{ \bbE_\sigma \left[ \lVert \sum_{i=1}^n \sigma( (u')^\top x_i) \sigma_i \rVert^2 \right]} \hspace{42mm} (\text{Jensen's inequality})\\
    & = \frac{\tau}{n} \sqrt{ \bbE_\sigma \left[ \sum_{i=1}^n \sigma( (u')^\top x_i)^2 \sigma_i^2 \right]} \hspace{8mm} (\text{since } \sigma_i, \sigma_j \text{ are independent with mean 0})\\
    & \leq \frac{\tau}{n} \sqrt{\sum_{i=1}^n ((u')^\top x_i)^2}\\
    & \leq \frac{\tau}{n} \sqrt{\sum_{i=1}^n \lVert x_i \rVert^2 }.
\end{align*}
Combining this with the fact that $\bbE \left[\lVert x \rVert^2 \right] \leq C^2,$ we have
\begin{align*}
    R_n (\cH_{\phi, \tau}) &= \bbE[R_S(\cH_{\phi, \tau})] \\
    &\leq \frac{\tau}{n}\bbE[\sqrt{\sum_{i=1}^n \lVert x_i \rVert^2 }]\\
    &\leq \frac{\tau}{n}\sqrt{\bbE[\sum_{i=1}^n \lVert x_i \rVert^2 ]} \quad (\text{Jensen's inequality})\\
    &\leq \frac{\tau C}{\sqrt{n}}.
\end{align*}

\textbf{Case 2:} $|w'| \lVert u' \rVert < \tau$. \\
We know that $\cH_{\phi, \tau} = \cH_{\phi, \tau}^{(1)} \bigcup \cH_{\phi, \tau}^{(2)}$ when
\begin{align*}
    \cH_{\phi, \tau}^{(1)} & = \{ h \in \cH | h: x \to w \sigma(u^\top x), u = u', |w -w' | < \tau \}\\
    \cH_{\phi, \tau}^{(2)} & = \{ h \in \cH | h: x \to w \sigma(u^\top x), \lVert u \rVert = 1, u \neq u', |w| < \tau \}
\end{align*}

% From \textbf{Case 1}, we have
% $$
% R_S(\cH_{\phi, \tau}^{(1)}) \leq \frac{\tau}{n}\sqrt{\sum_{i=1}^n \lVert x_i \rVert^2 }.
% $$
% \begin{align*}
%      \sum_{i=1}^n h(x_i)\sigma_i &\leq w' \left( \sum_{i=1}^n \sigma ((u')^\top x_i) \sigma_i \right) + \frac{\tau}{\lVert u' \rVert} | \sum_{i=1}^n \sigma( (u')^\top x_i) \sigma_i |\\
%      &\leq w' \left( \sum_{i=1}^n \sigma ((u')^\top x_i) \sigma_i \right) + \tau\sup_{\lVert u \rVert = 1} | \sum_{i=1}^n \sigma( (u)^\top x_i) \sigma_i |
% \end{align*}
% Now, we can see that 


% consider $h \in \cH_{\phi, \tau}^{(2)}$, we have
% \begin{align*}
%     \sum_{i=1}^n h(x_i)\sigma_i &=   \sum_{i=1}^n w \sigma(u^\top x_i) \sigma_i  \\
%     &\leq |w|\lVert u \rVert \sup_{\lVert u \rVert = 1} | \sum_{i=1}^n \sigma( (u)^\top x_i) \sigma_i |\\
%     &\leq \tau \sup_{\lVert u \rVert = 1} | \sum_{i=1}^n \sigma( (u)^\top x_i) \sigma_i |.
% \end{align*}
% The last line holds from $|w|\lVert u \rVert$ for $h \in \cH_{\phi, \tau}^{(2)}$. Therefore, we have
We have
\begin{align*}
    R_S(\cH_{\phi, \tau}) & = \frac{1}{n} \bbE_\sigma \left[ \sup_{h \in \cH_{\phi,\tau}} \sum_{i=1}^n h(x_i)\sigma_i \right] \\
    & \leq \frac{1}{n} \bbE_\sigma \left[  \sup_{h \in \cH_{\phi,\tau}^{(1)}} \sum_{i=1}^n h(x_i)\sigma_i + \sup_{h \in \cH_{\phi,\tau}^{(2)}} \sum_{i=1}^n h(x_i)\sigma_i\right] \\
    & = R_S(\cH_{\phi, \tau}^{(1)}) + R_S(\cH_{\phi, \tau}^{(2)})
\end{align*}

The second line holds as $\sup_{x \in A \cup B} f(x) \leq \sup_{x \in A} f(x) + \sup_{x \in B}f(x)$ when $\sup_{x \in A} f(x) \geq 0$ and $\sup_{x \in B}f(x) \geq 0$. We know that both of these supremums be greater than zero, as we can recover the value of 0 with $w = 0$.
From \textbf{Case 1}, we know that
$$R_n(\cH_{\phi, \tau}^{(1)}) \leq \frac{\tau C}{\sqrt{n}}.$$
We also note that $\cH_{\phi, \tau}^{(2)}$ is a class of two layer neural networks with weights with norms bounded by $\tau$. From Theorem \ref{theorem:bound_2NNs}, we have that 
$$R_n(\cH_{\phi, \tau}^{(2)}) \leq \frac{2\tau C}{\sqrt{n}}.$$
Therefore, in \textbf{Case 2},

$$
R_n(\cH_{\phi, \tau}) \leq \frac{3\tau C}{\sqrt{n}}.$$

as required.
\end{proof}
Now, we consider in the general setting (i.e., no restriction on $m$). 

\begin{theorem}
[Rademacher complexity for two layer neural networks with gradient constraints ]
Let $\cX$ be an instance space and $\cD_{\cX}$ be a distribution over $\cX$ with a large enough support. Let $\cH = \{h : x \mapsto \sum_{j=1}^m w_j \sigma(u_j^\top x) | w_j \in \R, u_j \in \R^d, \lVert u_j \rVert_2 = 1, \sum_{j=1}^m |w_j| \leq B \}$. Assume that there exists some constant $C > 0$ such that $\bbE_{x \sim \cD_{\cX}} [ \lVert x \rVert_2^2 ] \leq C^2$. Our explanation constraint is given by a constraint on the gradient of our models, where we want the gradient of our learnt model to be close to a particular target function $h' \in \cH$. Let this be represented by an explanation loss given by $$\phi(h, x) = \lVert \nabla_x h(x) - \nabla_x h'(x) \rVert_2 + \infty \cdot 1 \{ \lVert \nabla_x h(x) - \nabla_x h'(x) \rVert > \tau \} $$ for some $\tau > 0$. Then, we have that

\begin{align*}
     R_n ( \cH_{\phi, \tau}) \leq  \frac{3\tau m C}{\sqrt{n}}.
\end{align*}
To be precise, 
\begin{align*}
     R_n ( \cH_{\phi, \tau}) \leq  \frac{(2m + q)\tau C}{\sqrt{n}}.
\end{align*}
when $q$ is the number of node $j$ of $h'$ such that $|w'_j| < \tau $.


\end{theorem}


We note that this result indeed depends on the number of hidden dimensions $m$; however, we note that in the general case (Theorem \ref{theorem:bound_2NNs}), the value of $B$ is $O(m)$ as it is a sum over the values of each hidden node.  We now present the proof for the more general version of our theorem.

\begin{proof}
For simplicity, we first assume that any $h \in \cH$ has that $\lVert u_j \rVert = 1, \forall j$. Consider $h \in \cH$, we write $h = \sum_{j=1}^m w_j' \sigma( (u_j')^\top x)$ and let $h'(x) = \sum_{j=1}^m w_j' \sigma( (u_j')^\top x)$ be a function for our gradient constraint. The gradient of a hypothesis $h$ is given by
$$ \nabla_x h(x) = \sum_{j=1}^m w_j u_j \cdot 1\{ u_j^\top x > 0 \},$$
which is a piecewise constant function over at most $2^m$ regions. Then, we consider that
$$ \nabla_x h(x) - \nabla_x h'(x) = \sum_{j=1}^m w_j u_j \cdot 1 \{u_j^\top x > 0\} - \sum_{j=1}^m w_j' u_j' \cdot 1\{(u_j')^\top x > 0 \}, $$
which is a piecewise constant function over at most $2^{2m}$ regions. We again make an assumption that each of these regions has a non-zero probability mass. Our choice of $\phi(h, x)$ guarantees that the norm of the gradient in each region is less than $\tau$.  Similar to the case with $m = 1$, we will show that the gradient constraint leads to a class of functions with the same decision boundary or neural networks that have weights with a small norm. 

Assume that among $u_1, ..., u_m$ there are $k$ vectors that have the same direction as $u_1', ..., u_m'$. Without loss of generality, let $u_j = u_j'$ for $j = 1, ..., k$. In this case, we have that $\nabla_x h(x) - \nabla_x h'(x)$ is a piecewise function over $2^{2m-k}$ regions. As each region has non-zero probability mass, for each $j \in \{1, ..., k\}$, we know that $\exists x$ such that 
$$ u_j^\top x = (u_j')^\top x > 0 , \quad \quad  u_i^\top x < 0 \text{ for } i \neq j , \quad \quad (u_i')^\top x < 0 \text{ for } i \neq j.$$

In other words, we can observe a data point from each region that uniquely defines the value of a particular $w_j, u_j$. In this case, we have that
\begin{align*}
    \nabla_x h(x) - \nabla_x h'(x) & = w_j u_j - w_j' u_j' \\
                                   & = (w_j - w_j') u_j'.
\end{align*}    

From our gradient constraint, we know that $||\nabla_x h(x) - \nabla_x h'(x)|| \leq \tau, \forall x$, which implies that $|w_j - w_j'| \leq \tau$ for $j = 1, ..., k$. 

On the other hand, for the remaining $j = k + 1, ..., m$, we know that there exists $x$ such that
$$ u_j^\top x > 0 , \quad \quad u_i^\top x < 0  \text{ for } i \neq j, \quad \quad (u_i')^\top x < 0 \text{ for } i = 1, ..., m. $$
Then, we have that $\nabla_x h(x) = w_j u_j$, and our constraint implies that $|w_j| \lVert u_j \rVert = |w_j| \leq \tau$. Similarly, we have that $|w_j'| \lVert u_j' \rVert = |w_j'| < \tau, $ for $j = k+1, ..., m$. We can conclude that $\cH_{\phi, \tau}$ is a class of two layer neural networks with $m$ hidden nodes (assuming $\lVert u_i \rVert$ = 1) that for each node $w_j \sigma (u_j^\top x)$ satisfies

\begin{enumerate}
    \item There exists $l \in [m]$ that $u_j = u_l'$ and $|w_j - w_l'| < \tau$.
    \item $|w_j| < \tau$
\end{enumerate}

% \begin{enumerate}
%     \item A node $w_j \sigma(u_j^\top x)$ that has the same boundary as the node $w'_l \sigma( (u_l')^\top x)$ of $h'(x)$ such that $u_j = u_l'$ and that $w_j$ differs from $w_l'$ by at most $\tau$. 
%     \item A node $w_j \sigma(u_j^\top x)$ that has a weight with small norm. In other words, it satisfies that $|w_j| \lVert u_j \rVert = |w_j| \leq \tau$. 
% \end{enumerate}

We further note that for a node $w_l' \sigma((u_l')^\top x)$ in $h'(x)$ that has that a high weight $|w_l'| > \tau$, there must be a node $w_j \sigma(u_j^\top x)$  in $h$ with the same boundary $u_j = u_l$. Otherwise, there is a contradiction with $|w_l'| < \tau$ for all nodes in $h'$ without a node in $h$ with the same boundary. We can utilize this characterization of the restricted class $\cH_{\phi,\tau}$ to bound the Rademacher complexity of the class. Let
$$
\cH' = \{h: x \mapsto \sum_{j=1}^m w_j' \sigma((u'_j)^\top x)a_j \mid a_j \in \{0,1\} \text{ and for } j \text{ that } |w_j'| > \tau, a_j = 1  \}.
$$
This is a class of two layer neural networks with at most $m$ nodes such that each node is from $h'$. We also have a condition that if the weight of the $j$-th node in $h'$ is greater than $\tau$, the $j$-th node must be present in any member of this class. Let
$$
\cH^{(\tau)} = \{h: x \mapsto \sum_{j=1}^m w_j \sigma((u_j)^\top x)a_j \mid w_j \in \R, u_j \in \R^d, |w_j| < \tau, \lVert u_j \rVert = 1\}.
$$
be a class of two layer neural networks with $m$ nodes such that the weight of each node is at most $\tau$. We claim that for any $h \in \cH_{\phi,\tau}$ there exists $h_1 \in \cH', h_2 \in \cH^{(\tau)}$ that $h = h_1 + h_2.$ For any $h\in \cH_{\phi,\tau}$, let $p_h: [m] \to [m]\cup \{0\}$ be a function that match a node in $h$ with the node with the same boundary in $h'$. Formally,
\begin{equation*}
    p_h(j) = \begin{cases}
        l & \text{ when } u_j = u_l' \\
        0 & \text{ otherwise}.
    \end{cases}
\end{equation*}
The function $p_h$ maps $j$ to $0$ if there is no node in $h'$ with the same boundary. Let $w'_0 = 0, u'_0 = [0,\dots, 0]$, we can write
\begin{align*}
    h(x) &= \sum_{j=1}^m w_j \sigma (u_j^\top x) \\
    &= \sum_{j=1}^m w_j \sigma (u_j^\top x) - w'_{p_h(j)}\sigma((u')_{p_h(j)}^\top x) + w'_{p_h(j)}\sigma((u')_{p_h(j)}^\top x)\\
    &= \underbrace{\sum_{p_h(j) \neq 0} (w_j - w'_{p_h(j)})\sigma((u')_{p_h(j)}^\top x) + \sum_{p_h(j) = 0} w_j \sigma (u_j^\top x)}_{\in \cH^{(\tau)}} + \underbrace{\sum_{p(j) \neq 0} w_{p_h(j)}'\sigma((u')_{p_h(j)}^\top x)}_{\in \cH'}.
\end{align*}
The first term is a member of $\cH^{(\tau)}$ because we know that $|w_j - w'_{p(j)}| < \tau$ or $|w_j| < \tau$. The second term is also a member of $\cH'$ since for any $l$ that $|w'_l| > \tau$, there exists $j$ that $p_h(j) = l$. Therefore, we can write $h$ in terms of a sum between a member of $\cH'$ and $\cH^{(\tau)}$. This implies that
$$
R_n(\cH_{\phi,\tau}) \leq R_n(\cH') + R_n(\cH^{(\tau)}).
$$
From Theorem \ref{theorem:bound_2NNs}, we have that 
$$R_n(\cH_{\phi, \tau}^{(\tau)}) \leq \frac{2\tau m C}{\sqrt{n}}.$$ Now, we will calculate the Rademacher complexity of $\cH'$. For $S = \{x_1,\dots, x_n\}$,
\begin{align*}
    R_S(\cH') &= \frac{1}{n} \bbE_\sigma \left[ \sup_{h \in \cH'} \sum_{i=1}^n h(x_i) \sigma_i \right] \\
    &= \frac{1}{n} \bbE_\sigma \left[ \sup_{h \in \cH'} \sum_{i=1}^n (\sum_{j=1}^m w_j' \sigma((u'_j)^\top x_i)a_j )\sigma_i \right]\\
    &= \frac{1}{n} \bbE_\sigma \left[ \sup_{h \in \cH'} \sum_{i=1}^n (\sum_{|w'_j| < \tau} w_j' \sigma((u'_j)^\top x_i)a_j + \sum_{|w'_j| > \tau} w_j' \sigma((u'_j)^\top x_i))\sigma_i \right]\\
    &= \frac{1}{n} \bbE_\sigma \left[ \sup_{a_j\in \{0,1\}} \sum_{i=1}^n \sum_{|w'_j| < \tau} w_j' \sigma((u'_j)^\top x_i)a_j \sigma_i \right]\\
    &= \frac{1}{n} \bbE_\sigma \left[ \sup_{a_j\in \{0,1\}}  \sum_{|w'_j| < \tau} a_j(w_j' \sum_{i=1}^n\sigma((u'_j)^\top x_i)\sigma_i) \right].\\
\end{align*}
To achieve the supremum, if $w_j' \sum_{i=1}^n\sigma((u'_j)^\top x_i)\sigma_i > 0$ we need to set $a_j = 1$, otherwise, we need to set $a_j = 0$. Therefore,
\begin{align*}
    R_S(\cH') &=\frac{1}{n} \bbE_\sigma \left[ \sup_{a_j\in \{0,1\}}  \sum_{|w'_j| < \tau} a_j(w_j' \sum_{i=1}^n\sigma((u'_j)^\top x_i)\sigma_i) \right]\\
    &= \frac{1}{n} \bbE_\sigma \left[  \sum_{|w'_j| < \tau} \sigma(w_j' \sum_{i=1}^n\sigma((u'_j)^\top x_i)\sigma_i) \right]\\
    &= \frac{1}{2n} \bbE_\sigma \left[  \sum_{|w'_j| < \tau} (w_j' \sum_{i=1}^n\sigma((u'_j)^\top x_i)\sigma_i)  + |w_j' \sum_{i=1}^n\sigma((u'_j)^\top x_i)\sigma_i|\right] & (\sigma(x) = \frac{x + |x|}{2})\\
    &= \frac{1}{2n} \bbE_\sigma \left[  \sum_{|w'_j| < \tau}  |w_j' \sum_{i=1}^n\sigma((u'_j)^\top x_i)\sigma_i|\right] \\
    &\leq \frac{1}{2n} \left(\sum_{|w'_j| < \tau} |w_j'| \right)\bbE_\sigma \left[ \sup_{\lVert u \rVert = 1} | \sum_{i=1}^n\sigma(u^\top x_i)\sigma_i|\right] \\
    &\leq \frac{1}{n} \left(\sum_{|w'_j| < \tau} |w_j'| \right)\bbE_\sigma \left[ \sup_{\lVert u \rVert = 1}  \sum_{i=1}^n\sigma(u^\top x_i)\sigma_i\right] &(\text{Lemma } \ref{lemma:TM_note})\\
    &\leq  \left(\sum_{|w'_j| < \tau} |w_j'| \right)\underbrace{\bbE_\sigma \left[\frac{1}{n} \sup_{\lVert u \rVert = 1}  \sum_{i=1}^nu^\top x_i\sigma_i\right]}_{\text{Empirical Rademacher complexity of a linear model}} &(\text{Talagrand's Lemma}).\\
\end{align*}
From Theorem \ref{theorem:bound_linear}, we can conclude that
$$
R_n(\cH') \leq \sum_{|w'_j| < \tau} |w_j'|\frac{C}{\sqrt{n}} \leq  \frac{q\tau C}{\sqrt{n}}\leq \frac{m\tau C}{\sqrt{n}}
$$
when $q$ is the number of nodes $j$ of $h'$ such that $|w'_j| < \tau $. Therefore,

$$
R_n(\cH') \leq \frac{(2m + q)\tau C}{\sqrt{n}} \leq \frac{3m\tau C}{\sqrt{n}}.
$$

%  We consider 
% \begin{align*}
%     \cH_{\phi, \tau}^{(1)} & = \{ h \in \cH_{\phi,\tau} | \exists j,l, u_j = u'_l \}\\
%     \cH_{\phi, \tau}^{(2)} & = \{ h \in \cH_{\phi,\tau} | \forall j,l, u_j \neq u'_l \}.
% \end{align*}
% $\cH_{\phi, \tau}^{(1)}$ is a class of neural network that has at least one node with the same boundary as some node of $h'$.
% For any $h \in \cH_{\phi, \tau}^{(1)}$ with $k > 0$ nodes that share a boundary as a node in $h'$ (we refer to these as $u_j = u_j'$ for $j = 1, 2, ..., k)$, consider that 
% \begin{align*}
%     \sum_{i=1}^n h(x_i) \sigma_i & = \sum_{i=1}^n \left( \sum_{j=1}^m w_j \sigma (u_j^\top x_i)\sigma_i \right) \\
%     & = \sum_{j=1}^m w_j \left( \sum_{i=1}^n \sigma(u_j^\top x_i) \sigma_i \right) \\
%     & = \sum_{j=1}^k w_j \left( \sum_{i=1}^n \sigma( (u_j')^\top x_i) \sigma_i \right) + \sum_{j=k+1}^m w_j \left( \sum_{i=1}^n \sigma( u_j^\top x_i) \sigma_i \right)
% \end{align*}

% We can bound these first two terms in the same fashion as done for the case when $m = 1$. The first term $\sum_{i=1}^n \sigma( (u_j)'^\top x_i) \sigma_i$ is fixed and $|w_j - w_j'| < \tau$. Then, we can let $v_j = \sum_{i=1}^n \sigma( (u_j')^\top x_i) \sigma_i$, and we observe that
% \begin{align*}
%     \sum_{j=1}^k w_j v_j & \leq \sum_{j=1}^k (w_j' + \tau \cdot \operatorname{sign}(v_j)) v_j \\
%     & = \sum_{j=1}^k w_j' v_j + \tau |v_j | \\
%     & \leq \sum_{j=1}^k w_j' v_j + k\tau \sup_{\lVert u \rVert = 1} | \sum_{i=1}^n \sigma( u^\top x_i) \sigma_i|
% \end{align*}

% For the second term, we use the fact that $|w_j| < \tau$ must be small. This has that
% \begin{align*}
%     \sum_{j=k+1}^m w_j \left( \sum_{i=1}^n \sigma (u_j^\top x) \sigma_i)\right) & \leq \left( \sum_{j=k+1}^m |w_j| \right) \max_{k+1 \leq l \leq m} | \sum_{i=1}^n \sigma( u_l^\top x_i)\sigma_i| \\
%     & \leq \left( \sum_{j=k+1}^m |w_j| \right) \sup_{\lVert u\rVert} |\sum_{i=1}^n \sigma(u^\top x_i) \sigma_i| \\
%     & \leq \tau (m-k) \sup_{\lVert u \rVert = 1} | \sum_{i=1}^n \sigma (u^\top x_i) \sigma_i|
% \end{align*}
% Combining these two terms, we have that
% \begin{align*}
%     \sum_{i=1}^n h(x_i) \sigma_i \leq \sum_{j=1}^k w_j' \left( \sum_{i=1}^n \sigma( (u_j')^\top x_i) \sigma_i \right) + \tau m \sup_{\lVert u \rVert = 1} | \sum_{i=1}^n \sigma(u^\top x_i) \sigma_i |
% \end{align*}
% This also implies that 
% \begin{align*}
%     R_S(\cH_{\phi, \tau}^{(1)}) & = \frac{1}{n} \bbE_\sigma \left[ \sup_{h \in \cH_{\phi, \tau}^{(1)}} \sum_{i=1}^n h(x_i) \sigma_i \right] \\
%     & \leq \frac{1}{n} \bbE_\sigma \left[ \sum_{i=1}^n \left( \sum_{j=1}^n w_j' \sigma( (u_j')^\top x_i) \sigma_i + \tau m \sup_{\lVert u \rVert = 1} |\sum_{i=1}^n \sigma(u^\top x_i) \sigma_i| \right)  \right] \\
%     & = \frac{\tau m }{n} \bbE_\sigma \left[ \sup_{\lVert u \rVert = 1} | \sum_{i=1}^n \sigma(u^\top x_i) \sigma_i | \right] \\
%     & \leq \frac{2 \tau m }{n} \bbE_\sigma \left[ \sup_{\lVert u \rVert = 1} \sum_{i=1}^n \sigma( u^\top x_i) \sigma_i \right] \\
%     &\leq  2 \tau m R_S(\cH'),
% \end{align*}
% where the second to last line holds from Lemma \ref{lemma:TM_note}, the last line holds from Talgrand's Lemma, and $\cH' = \{h : x \mapsto u^\top x \; | \; \lVert u \rVert = 1 \}$. This implies that
% \begin{align*}
%     R_n(\cH_{\phi, \tau}^{(1)}) & \leq 2\tau m R_n(\cH') \\
%                             & \leq \frac{2\tau m C}{\sqrt{n}}
% \end{align*}    
% where $R_n(\cH')$ is simply the Rademacher complexity of a linear model (Theorem \ref{theorem:unconstrained_linear}). If there exists $j \in [m]$ such than $|w'_j| > \tau$, we know that there must be some node of $h$ that has the same direction as $u'_j$. Therefore, $\cH_{\phi, \tau}^{(2)} = \emptyset$ and $R_S(\cH_{\phi, \tau}) = R_S(\cH_{\phi, \tau}^{(1)})$. We have
% \begin{align*}
%     R_n(\cH_{\phi, \tau}) \leq \frac{2\tau m C}{\sqrt{n}}.
% \end{align*}    


% On the other hand, if there is no such $j$, $\cH_{\phi, \tau}^{(2)} \neq \emptyset$. We note that $R_S(\cH_{\phi, \tau}^{(2)})$ is a class of two layer neural networks with weights with bounded norm by $\tau m$ since for each $j$, we must have $|w_j| < \tau$. From Theorem \ref{theorem:bound_2NNs}, we have that 
% $$R_n(\cH_{\phi, \tau}^{(2)}) \leq \frac{2\tau m C}{\sqrt{n}}.$$ Finally, we can conclude that
% \begin{align*}
%     R_n(\cH_{\phi, \tau}) = R_n(\cH_{\phi, \tau}^{(2)} \cup R_n(\cH_{\phi, \tau})^{(1)}) \leq R_n(\cH_{\phi, \tau}^{(1)}) + R_n(\cH_{\phi, \tau}^{(2)}) \leq  \frac{4\tau m C}{\sqrt{n}}
% \end{align*}
\end{proof}
A tighter bound is given by $\frac{(2m + q)\tau C}{\sqrt{n}}$ when $q$ is the number of $w'_j$ that $|w'_j| < \tau$. As $\tau \to 0$, we also have $q\to 0$. This implies that we have an upper bound of $\frac{2m\tau C}{\sqrt{n}}$ if $\tau$ is small enough. When comparing this to the original bound $\frac{2BC}{\sqrt{n}}$, we can do much better if $\tau \ll \frac{B}{m}$. We would like to point out that our bound does not depend on the distribution $\cD$ because we choose a strong explanation loss
$$\phi(h, x) = \lVert \nabla_x h(x) - \nabla_x h'(x) \rVert_2 + \infty \cdot 1 \{ \lVert \nabla_x h(x) - \nabla_x h'(x) \rVert > \tau \} $$
which guarantees that $\lVert \nabla_x h(x) - \nabla_x h'(x) \rVert_2 \leq \tau$ almost everywhere. We also assume that we are in a high-dimensional setting $d \gg m$ and there exists $x$ with a positive probability density at any partition created by $\nabla_x h(x)$. 

\section{Algorithmic Results for Two Layer Neural Networks with a Gradient Constraint}\label{appendix:2nn_algo}

Now that we have provided generalization bounds for the restricted class of two layer neural networks, we also present an algorithm that can identify the parameters of a two layer neural network (up to a permutation of the weights). In practice, we might solve this via our variational objective or other simpler regularized techniques. However, we also provide a theoretical result for the required amount of data (given some assumptions about the data distribution) and runtime for an algorithm to exactly recover the parameters of these networks under gradient constraints.

We again know that the gradient of two layer neural networks with ReLU activations can be written as
\begin{equation*}
    \nabla_x f_{w, U}(x) = \sum_{i=1}^m w_i u_i \cdot 1\{u_i^T x > 0 \},
\end{equation*}

where we consider $||u_i|| = 1$. Therefore, an exact gradient constraint given of the form of pairs $(x, \nabla_x f(x))$ produces a system of equations. 
\begin{proposition}
    If the values of $u_i$'s are known, we can identify the parameters $w_i$ with exactly $m$ fixed samples. 
\end{proposition}
\begin{proof}
    We can select $m$ datapoints, which each achieve value 1 for the indicator value in the gradient of the two layer neural network. This would give us $m$ equations, which each are of the form 
    $$\nabla_x f_{w, U}(x_i) = w_i u_i.$$
    Therefore, we can easily solve for the values of $w_i$, given that $u_i$ is known. 
\end{proof}

To make this more general, we now consider the case where $u_i$'s are not known but are at least linearly independent.

\begin{proposition}
    Let the $u_i$'s be linearly independent. Assume that each region of the data (when partitioned by the values of $u_i$) has non-trivial support $> p$. Then, with probability $1 - \delta$, we can identify the parameters $w_i, u_i$ with $O\left(2^m + \frac{m + \log(\frac{1}{\delta})}{\log(\frac{1}{1-p})}\right)$ data points and in $O(2^{2m})$ time.
\end{proposition}
\begin{proof}
    Let us partition $\mathcal{X}$ into regions satisfying unique values of the binary vector $(1\{u_1^T x > 0 \}, ..., 1\{u_m^T x > 0 \})$, which by our assumption each have at least some probability mass $p$. First, we calculate the probability that we observe one data point with an explanation from each region in this partition. This is equivalent to sampling from a multinomial distribution with probabilities ($p_1, ..., p_{2^m})$, where $p_i \geq p, \forall i$. Then,
    \begin{align*}
        \Pr(\text{observe all regions in $n$ draws}) & =  1 - \Pr(\exists i \text{ s.t. we do not observe region $i$} ) \\
        & = 1 - 2^m(1 - p)^n.
    \end{align*}
    Setting this as no less than $1 - \delta$ leads to that $n \geq \frac{m + \log(\frac{1}{\delta})}{\log(\frac{1}{1 - p})}$.
    
    Given $O(2^m + \frac{m + \log(\frac{1}{\delta})}{\log(\frac{1}{1-p})})$ pairs of data and gradients, we will observe at least one pair from each region of the partition. Then, identifying the values of $u_i$'s and $w_i$'s is equivalent to identifying the datapoints that correspond to a value of the binary vector where only one indicator value is 1. These values can be identified in $O(2^{3m})$ time; the algorithm is given in Algorithm \ref{algo_2NN}. These results demonstrate that we can indeed learn the parameters (up to a permutation) of a two layer neural network given exact gradient information. 
\end{proof}

% \section{Stuff}

% \begin{definition}
% [Truthful explanation] For a concept class $\cH$, a data distribution $\cD$ that there exists $h^* \in \cH$ with $\error_{\cD}(h^*) = 0$. An explanation $E(g,\phi)$ is truthful for learning from a concept class $\cH$ and a dataset $\cD$ if
% \begin{equation*}
%     \phi(h^*, D) = 0.
% \end{equation*}
% \end{definition}

\begin{algorithm*}[t]
    \caption{Algorithm for identifying parameters of a two layer neural network, given exact gradient constraints}\label{alg:algo_2NN}
    
    \begin{algorithmic}[1]
    \STATE \textbf{Input:} We are given $M = \{ \sum_{x \in C} x | C \in \cP(\{x_1, ..., x_m \})\}$, with $\{x_1, ..., x_m \}$ linearly independent
    \STATE \textbf{Output:} The set of basis elements $\{x_1, ..., x_m \}$
    \FUNCTION{}
        \STATE $B = \{\}$, $S = \{ \}$ \hfill  \COMMENT{Set for basis vectors and set for a current sum of at least 2 elements}
        \FOR{$x \in M$}
            \IF {$x \in S$} 
                \STATE pass
            \ELSE
                \STATE $B = B \cup \{ x \}$
                \IF {$|B| = 2$}
                    \STATE $S = \{ y_1 + y_2\}$, where $B = \{y_1, y_2 \}$
                \ELSE
                    \STATE $S = S \cup \{y + x | y \in S\}$ 
                    \hfill \COMMENT{Updating sums from adding $x$}
                \ENDIF
                \STATE $O = B \cap S$ \hfill \COMMENT{Computing overlap between current basis and sums}
                \STATE $B = B \setminus O$ \hfill \COMMENT{Removing elements contained in pairwise span}
                \STATE $S = \{y - y_o | y \in S, y_o \in O \}$ \hfill \COMMENT{Updating sums $S$ from removing set $O$}
            \ENDIF
        \ENDFOR
        \STATE \text{\textbf{return}} $B$
    \ENDFUNCTION
    \end{algorithmic}
\end{algorithm*}

\subsection{Algorithm for Identifying Regions} \label{algo_2NN}

We first note that identifying the parameters $u_i$'s and $w_i$'s of a two layer neural network is equivalent to identifying the values $\{x_1, ..., x_m \}$ from the set $\{ \sum_{x \in C} x | C \in \cP(\{x_1, ..., x_m \})\}$, where $\cP$ denotes the power set. We also assume that $x_1, ..., x_m$ are linearly independent, so we cannot create $x_i$ from any linear combination of $x_j$'s with $j \neq i$. Then, we can identify the set $\{x_1, ..., x_m \}$ as in Algorithm \ref{alg:algo_2NN}. This algorithm runs in $O(2^{3m})$ time as it iterates through each point in $M$ and computes the overlapping set $O$ and resulting updated sum $S$, which takes $O(2^{2m})$ time. From the resulting set $B$, we can exactly compute values $u_i$ and $w_i$ up to a permutation.

\section{Additional Synthetic Experiments} \label{appx:synth}

We now present additional synthetic experiments that demonstrate the performance of our approach under settings with imperfect explanations and compare the benefits of using \textit{different types} of explanations.

\subsection{Variational Objective is Better with Noisy Gradient Explanations} \label{appx:noise}

Here, we present the remainder of the results from the synthetic regression task of \ref{fig:noisy_nn}, under more settings of noise $\epsilon$ added to the gradient explanation.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figs/noise/v_nn_ep0.0001.pdf}
    \includegraphics[width=0.48\textwidth]{figs/noise/v_nn_ep0.1.pdf}
    \includegraphics[width=0.48\textwidth]{figs/noise/v_nn_ep0.5.pdf}
    \includegraphics[width=0.48\textwidth]{figs/noise/v_nn_ep1.pdf}
    \includegraphics[width=0.48\textwidth]{figs/noise/v_nn_ep2.pdf}
    \caption{Comparison of MSE on regressing a two layer neural network with explanations of noisy gradients. $m = 1000, k=20, \lambda = 10.$ For the iterative methods, $T = 10$. Results are averaged over 5 seeds.} 
    \label{fig:noisy_exp_appx}
\end{figure*}

Again, we observe that our method does better than that of the Lagrangian approach and the self-training method. Under high levels of noise, the Lagrangian method does poorly. On the contrary, our method is resistant to this noise and also outperforms self-training significantly in settings with limited labeled data. 

\subsection{Comparing Different Types of Explanations}

Here, we present synthetic results to compare using different types of explanation constraints. We focus on comparing noisy gradients as before, as well as noisy classifiers, which are used in the setting of weak supervision \citep{ratner2016data}. Here, we generate our noisy classifiers as $h^*(x) + \epsilon$, where $\epsilon \sim \cN(0, \sigma^2)$. We omit the results of self-training as it does not use any explanations, and we keep the supervised method as a baseline. Here, $t = 0.25$.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figs/compare/v_nn_noisy_classifier_0.0001.pdf}
    \includegraphics[width=0.48\textwidth]{figs/compare/v_nn_noisy_classifier_0.1.pdf}
    \includegraphics[width=0.48\textwidth]{figs/compare/v_nn_noisy_classifier_0.5.pdf}
    \includegraphics[width=0.48\textwidth]{figs/compare/v_nn_noisy_gradient_0.0001.pdf}
    \includegraphics[width=0.48\textwidth]{figs/compare/v_nn_noisy_gradient_0.1.pdf}
    \includegraphics[width=0.48\textwidth]{figs/compare/v_nn_noisy_gradient_0.5.pdf}
    % \vspace{-5mm}
    \caption{Comparison of MSE on regressing a two layer neural network with explanations as a noisy classifier (top) and noisy gradients (bottom). $m = 1000, k=20.$ For the iterative methods, $T = 10$. Results are averaged over 5 seeds. $\epsilon$ represents the variance of the noise added to the noisy classifier or noisy gradient.} 
    \label{fig:noisy_nn_full}
\end{figure*}

We observe different trends in performance as we vary the amount of noise in the noisy gradient or noisy classifier explanations. With any amount of noise and sufficient regularization ($\lambda$), this influences the overall performance of the methods that incorporate constraints. With few labeled data, using noisy classifiers helps outperform standard supervised learning. With a larger amount of labeled data, this leads to no benefits (if not worse performance of the Lagrangian approach). However, with the noisy gradient, under small amounts of noise, the restricted class of hypothesis will still capture solutions with low error. Therefore, in this case, we observe that the Lagrangian approach outperforms standard supervised learning in the case with few labeled data and matches it with sufficient labeled data. Our method outperforms or matches both methods across all settings.

We consider another noisy setting, where noise has been added to the weights of a copy of the target two layer neural network. Here, we compare how this information impacts learning from the direct outputs (noisy classifier) or the gradients (noisy gradients) of that noisy copy (Figure \ref{fig:noisy_weights_nn}). 

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figs/noisy_weights/v_nn_noisy_weights_classifier_0.3.pdf}
    \includegraphics[width=0.48\textwidth]{figs/noisy_weights/v_nn_noisy_weights_classifier_0.5.pdf}
    \includegraphics[width=0.48\textwidth]{figs/noisy_weights/v_nn_noisy_weights_classifier_0.7.pdf}
    \includegraphics[width=0.48\textwidth]{figs/noisy_weights/v_nn_noisy_weights_gradient_0.3.pdf}
    \includegraphics[width=0.48\textwidth]{figs/noisy_weights/v_nn_noisy_weights_gradient_0.5.pdf}
    \includegraphics[width=0.48\textwidth]{figs/noisy_weights/v_nn_noisy_weights_gradient_0.7.pdf}
    % \vspace{-5mm}
    \caption{Comparison of MSE on regressing a two layer neural network with explanations as a noisy classifier (top) and noisy gradients (bottom). $m = 1000, k=20.$ For the iterative methods, $T = 10$. Results are averaged over 5 seeds. $\epsilon$ represents the variance of the noise added to the noisy classifier or noisy gradient.} 
    \label{fig:noisy_weights_nn}
\end{figure*}

\clearpage

\section{Additional Baselines}
\label{appx:added_baseline}

We compare against an additional baseline of a Lagrangian-regularized model + self-training on unlabeled data. We again note that this is not a standard method in practice and does not naturally fit into a theoretical framework, although we present it to compare against a method that uses both explanations and unlabeled data. 

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figs/v_linear.pdf}
    \includegraphics[width=0.48\textwidth]{figs/v_nn.pdf}
    \includegraphics[width=0.48\textwidth]{figs/v_youtube.pdf}
    \includegraphics[width=0.48\textwidth]{figs/v_yelp.pdf}
    \caption{Comparison of MSE on regressing a linear model (top left) and two layer neural network (top right) with gradient explanations. $m = 1000, k=20.$ For the iterative methods, $T = 2$. Results are averaged over 5 seeds.  Comparison of classification accuracy on the YouTube dataset (bottom left) and the Yelp dataset (bottom right). $m = 500, k = 150$. Results are averaged over 40 seeds.} 
    \label{fig:new_baseline}
\end{figure*}

We observe that our method outperforms this baseline (Figure \ref{fig:new_baseline}), again especially in the settings with limited labeled data. We observe that although this new method indeed satisfies constraints, when performing only a single round of self-training, it no longer satisfies these constraints as much. Thus, this supports the use of our method to perform multiple rounds of projections onto a set of EPAC models.

\section{Experimental Details}

For all of our synthetic and real-world experiments, we use values of $m = 1000, k = 20, T = 3, \tau = 0, \lambda=1$, unless otherwise noted. For our synthetic experiments, we use $d = 100, \sigma^2 = 5$. Our two layer neural networks have hidden dimensions of size 10. They are trained with a learning rate of 0.01 for 50 epochs. We evaluate all networks on a (synthetic) test set of size 2000.

For our real-world data, our two layer neural networks have a hidden dimension of size 10 and are trained with a learning rate of 0.1 (YouTube) and 0.1 (Yelp) for 10 epochs. $\lambda = 0.01$ and gradient values computed by the smoothed approximation in \citep{sam2022losses} has $c = 1$. Test splits are used as follows from the YouTube and Yelp datasets in the WRENCH benchmark \citep{zhang2021wrench}.

We choose the initialization of our variational algorithm $h_0$ as the standard supervised model, trained using gradient descent.

\clearpage


\section{Ablations} \label{appx:ablation}

We also perform ablation studies in the same regression setting as Section \ref{experiments}. We vary parameters that determine either the experimental setting or hyperparameters of our algorithms. 

\subsection{Number of Explanation-annotated Data}
First, we vary the value of $k$ to illustrate the benefits of our method over the existing baselines. 

\begin{figure*}[h]
    \centering
    \vspace{-4mm}
    \includegraphics[width=0.48\textwidth]{figs/explain_data/v_nn_k20.pdf}
    \includegraphics[width=0.48\textwidth]{figs/explain_data/v_nn_k40.pdf}
    \includegraphics[width=0.48\textwidth]{figs/explain_data/v_nn_k60.pdf}
    \includegraphics[width=0.48\textwidth]{figs/explain_data/v_nn_k80.pdf}
    \includegraphics[width=0.48\textwidth]{figs/explain_data/v_nn_k100.pdf}
    \includegraphics[width=0.48\textwidth]{figs/explain_data/v_nn_k200.pdf}
    \vspace{-2mm}   
    \caption{Comparison of MSE on regressing a two layer neural network over different amounts of explanation-annotated data $k$. $m = 1000.$ For the iterative methods, $T = 10$. Results are averaged over 5 seeds.} 
    \label{fig:ablation_k}
\end{figure*}

We observe that our variational approach performs much better than a simple augmented Lagrangian method, which in turn does better than supervised learning with sufficiently large values of $k$. Our approach is always better than the standard supervised approach. 

We also provide results for how well these methods satisfy these explanations over varying values of $k$. 

% \begin{figure*}[h]
%     \centering
%     \vspace{-4mm}
%     \includegraphics[width=0.48\textwidth]{figs/explain_data_constraint/v_nn_k20.pdf}
%     \includegraphics[width=0.48\textwidth]{figs/explain_data_constraint/v_nn_k50.pdf}
%     \vspace{-4mm}
%     % \includegraphics[width=0.49\textwidth]{figs/explain_data_constraint/v_nn_k40.pdf}
%     % \includegraphics[width=0.49\textwidth]{figs/explain_data_constraint/v_nn_k60.pdf}
%     % \includegraphics[width=0.49\textwidth]{figs/explain_data_constraint/v_nn_k80.pdf}
%     \includegraphics[width=0.48\textwidth]{figs/explain_data_constraint/v_nn_k100.pdf}
%     \includegraphics[width=0.48\textwidth]{figs/explain_data_constraint/v_nn_k200.pdf}
%     \vspace{-4mm}
%     \includegraphics[width=0.48\textwidth]{figs/explain_data_constraint/v_nn_k500.pdf}
%     \includegraphics[width=0.48\textwidth]{figs/explain_data_constraint/v_nn_k1000.pdf}
%     % \vspace{-2mm}   
%     \caption{Comparison of Input Gradient Distance when regressing a two layer neural network over different values of $k$. $m = 1000, T = 10.$ Results are averaged over 5 seeds.} 
%     \vspace{-3mm}
%     \label{fig:ablation_k_constraint}
% \end{figure*}

\subsection{Simpler Teacher Models Can Maintain Good Performance}

As noted before, we can use \textit{simpler} teacher models to be regularized into the explanation-constrained subspace. This can lead to overall easier optimization problems, and we synthetically verify the impacts on the overall performance. In this experimental setup, we are regressing a two layer neural network with a hidden dimension size of 100, which is much larger than in our other synthetic experiments. Here, we vary over simpler teacher models by changing their hidden dimension size.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.49\textwidth]{figs/v_nn_hidden.pdf}
    \vspace{-3mm}
    \caption{Comparison of MSE on regressing a two layer neural network over simpler teacher models (hidden dimension). Here, $k = 20, m = 1000, T = 10$. Results are averaged over 5 seeds.} 
    \vspace{-4mm}
    \label{fig:ablation_teacher}
\end{figure}

We observe no major differences as we shrink the hidden dimension size by a small amount. For significantly smaller hidden dimensions (e.g., 2 or 4), we observe a large drop in performance as these simpler teachers can no longer fit the approximate projection onto our class of EPAC models accurately. However, slightly smaller networks (e.g., 6, 8) can fit this projection as well, if not better in some cases. This is a useful finding, meaning that our teacher can be a \textit{smaller model} and get comparable results, showing that this simpler teacher can help with scalability without much or any drop in performance.

\subsection{Number of Unlabeled Data}

As a main benefit of our approach is the ability to incorporate large amounts of unlabeled data, we provide a study as we vary the amount of unlabeled data $m$ that is available. When varying the amount of unlabeled data, we observe that the performance of self-training and our variational objective improves at similar rates. 

\begin{figure*}[h]
    \centering
    \vspace{-2mm}
    % \includegraphics[width=0.48\textwidth]{figs/unlabeled_data/v_nn_m50.pdf}
    \includegraphics[width=0.48\textwidth]{figs/unlabeled_data/v_nn_m100.pdf}
    \includegraphics[width=0.48\textwidth]{figs/unlabeled_data/v_nn_m250.pdf}
    \vspace{-2mm}
    \includegraphics[width=0.48\textwidth]{figs/unlabeled_data/v_nn_m500.pdf}
    \includegraphics[width=0.48\textwidth]{figs/unlabeled_data/v_nn_m1000.pdf}
    % \vspace{-2mm}
    \caption{Comparison of MSE on regressing a two layer neural network over different values of $m$. $k = 20, T = 10.$ Results are averaged over 5 seeds.} 
    \vspace{-2mm}
    \label{fig:ablation_m}
\end{figure*}

\clearpage

\subsection{Data Dimension}

We also provide ablations as we vary the underlying data dimension $d$. As we increase the dimension $d$, we observe that the methods seem to achieve similar performance, due to the difficulty in modeling the high-dimensional data. Also, here gradient information is much harder to incorporate, as the input gradient itself is $d$-dimensional, so we do not see as much of a benefit of our approach as $d$ grows. 

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figs/data_dimension/v_nn_d25.pdf}
    \includegraphics[width=0.48\textwidth]{figs/data_dimension/v_nn_d50.pdf}
    \includegraphics[width=0.48\textwidth]{figs/data_dimension/v_nn_d75.pdf}
    \includegraphics[width=0.48\textwidth]{figs/data_dimension/v_nn_d100.pdf}
    \caption{Comparison of MSE on regressing a two layer neural network over different underlying data dimensions $d$. $m = 1000, k = 20.$ For the iterative methods, $T = 10$. Results are averaged over 5 seeds.} 
    \label{fig:ablation_d}
\end{figure*}

\clearpage

\subsection{Hyperparameters}

First, we compare the different approaches over different values of regularization ($\lambda)$ towards satisfying the explanation constraints. Here, we compare the augmented Lagrangian approach, the self-training approach, and our variational approach.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figs/regularization/v_nn_lamb0.1.pdf}
    \includegraphics[width=0.48\textwidth]{figs/regularization/v_nn_lamb1.pdf}
    \includegraphics[width=0.48\textwidth]{figs/regularization/v_nn_lamb10.pdf}
    \includegraphics[width=0.48\textwidth]{figs/regularization/v_nn_lamb100.pdf}
    \caption{Comparison of MSE on regressing a two layer neural network over different values of $\lambda$. $m = 1000, k = 20.$ For the iterative methods, $T = 10$. Results are averaged over 5 seeds.} 
    \label{fig:ablation_hyp}
\end{figure*}

We observe that there is not a significant trend as we change the value of $\lambda$ across the different methods. Since we know that our explanation is perfect (our restricted EPAC class contains the target classifier), increasing the value of $\lambda$ should help, until this constraint is met.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figs/v_nn_T.pdf}
    \includegraphics[width=0.48\textwidth]{figs/v_nn_t.pdf}
    \caption{Comparison of MSE on regressing a two layer neural network over different values of $T$ (left) and $\tau$ (right) in our variational approach. $m = 1000, k = 20, \tau = 10, T = 10$, unless noted otherwise. Results are averaged over 5 seeds.} 
    \label{fig:ablation_ts}
\end{figure*}

Next, we compare different hyperparameter settings for our variational approach. Here, we analyze trends as we vary the values of $T$ (number of iterations) and $\tau$ (threshold before adding hinge penalty). We note that the value of $\tau$ does not significantly impact the performance of our method while increasing values of $T$ seems to generally benefit performance on this task.


\blue{
\section{Social Impacts}
\label{appendix: social impacts}
While our proposed method has the potential to improve performance and efficiency in a variety of applications, our method could introduce new biases or reinforce existing biases in the data used to train the model. For example, if our explanations constraints are poorly specified and reflect biased behavior, this could lead to inaccurate or discriminatory predictions, which could have negative impacts on individuals or groups that are already marginalized. Therefore, it is important to note that these explanation constraints must be properly analyzed and specified to exhibit the desired behavior of our model.
}