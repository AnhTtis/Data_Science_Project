\section{Uniform Convergence via Rademacher Complexity}
A standard tool for providing performance guarantees of supervised learning problems is a generalization bound via uniform convergence. We will first define the Rademacher complexity and its corresponding generalization bound.

\begin{definition} Let $\cF$ be a family of functions mapping $\cX \to \R$. Let $S = \{x_1, \dots, x_m\}$ be a set of examples drawn i.i.d. from a distribution $D_{\cX}$. Then, the empirical Rademacher complexity of $\cF$ is defined as
\begin{equation*}
    {R}_S(\cF) = \displaystyle \mathop{\mathbb{E}}_{\sigma}\left[\sup_{f\in\cF}\left(\frac{1}{m}\sum_{i=1}^m \sigma_if(x_i) \right)\right]
\end{equation*}
where $\sigma_1,\dots,\sigma_m$ are independent random variables uniformly chosen from $\{-1,1\}.$
\end{definition}

\begin{definition}
Let $\cF$ be a family of functions mapping $\cX \to \R$. Then, the Rademacher complexity of $\cF$ is defined as
\begin{equation*}
    R_n(\cF) = \displaystyle \mathop{\mathbb{E}}_{S \sim \cD_\cX^n}\left[R_S(\cF) \right]. 
\end{equation*}
The Rademacher complexity is the expectation of the empirical Rademacher complexity, over $n$ samples drawn i.i.d. from the distribution $\cD_\cX$.
\end{definition}
\begin{theorem}
[Rademacher-based uniform convergence]
Let $D_{\cX}$ be a distribution over $\cX$, and $\cF$ a family of functions mapping $\cX \to [0,1]$. Let $S = \{x_1, \dots, x_n\}$ be a set of samples drawn i.i.d. from $D_{\cX}$, then with probability at least $1 - \delta$ over our draw $S$,
\begin{equation*}
    |\bbE_{\cD}[f(x)] - \hat{\bbE}_S[f(x)]| \leq   2R_n(\cF) + \sqrt{\frac{\ln(2/\delta)}{2n}}.
\end{equation*}
This holds for every function $f \in \cF$, and $\hat{\bbE}_S[f(x)]$ is expectation over a uniform distribution over $S$.
\end{theorem}

This bound on the empirical Rademacher complexity leads to the standard generalization bound for supervised learning.

\begin{theorem}
For a binary classification setting when $y \in \{\pm 1\}$ with a zero-one loss, for $\cH \subset \{h: \cX \to \{-1,1\}\}$ be a family of binary classifiers,  let $S=\{(x_{1},y_{1}), \dots, (x_{n},y_{n})\}$ is drawn i.i.d. from $D$ then with probability at least $1 - \delta$, we have
\begin{equation*}
    |\err_\cD(h) - \widehat{\err_S}(h)| \leq  R_n(\cH) + \sqrt{\frac{\ln(2/\delta)}{2n}},
\end{equation*}
for every $h \in \cH$ when
\begin{equation*}
    \err_\cD(h) = \Pr_{(x,y) \sim\cD}(h(x) \neq y)
\end{equation*}
and 
\begin{equation*}
    \widehat{\err}_S(h) = \frac{1}{n} \sum_{i=1}^n 1[h(x_i) \neq y_i] 
\end{equation*}
is the empirical error on $S$.
\end{theorem}
For a linear model with a bounded weights in $\ell_2$ norm, the Rademacher complexity is $\cO(\frac{1}{\sqrt{n}})$. 
We refer to the proof from \citet{ma2022notes} for this result.
\begin{theorem}
[Rademacher complexity of a linear model (\citep{ma2022notes})] 
\label{theorem:unconstrained_linear}
Let $\cX$ be an instance space in $\R^d$, let  $\cD_\cX$ be a distribution on $\cX$, let $\cH = \{h: x \to \langle w_h, x \rangle \mid w_h \in \R^d, ||w_h||_2 \leq B\}$ be a class of linear model with weights bounded by some constant $B>0$ in $\ell_2$ norm. Assume that there exists a constant $C>0$ such that $\bbE_{x \sim \cD_\cX}[||x||_2^2] \leq C^2$. For any $S=\{x_{1}, \dots, x_{n}\}$ is drawn i.i.d. from $\cD_\cX$, we have
$$
R_S(\cH) \leq \frac{B}{n}\sqrt{\sum_{i=1}^n||x_i||_2^2}
$$
and
$$
R_n(\cH) \leq \frac{BC}{\sqrt{n}}.
$$
\end{theorem}
% \begin{proof}
%     For a set of samples $S$, we have
%     \begin{align*}
%         R_S(\cH) &= \frac{1}{n} \bbE_{\sigma}\left[\sup_{h \in \cH} \sum_{i=1}^n h(x_i)\sigma_i\right]\\
%         &= \frac{1}{n} \bbE_{\sigma}\left[\sup_{\lVert w_h\rVert_2 \leq B } \sum_{i=1}^n \langle w_h, x_i \rangle\sigma_i\right]\\
%         &= \frac{1}{n} \bbE_{\sigma}\left[\sup_{\lVert w_h\rVert_2 \leq B }  \langle w_h, \sum_{i=1}^n x_i\sigma_i \rangle\right]\\
%         &= \frac{B}{n} \bbE_{\sigma}\left[ \left\lVert \sum_{i=1}^n x_i\sigma_i \right\rVert_2\right] \quad \quad \quad \quad \left(\sup_{\lVert w_h \rVert \leq B} \langle w_h, v \rangle = B\lVert v \rVert \right)\\
%         &= \frac{B}{n} \sqrt{\bbE_{\sigma}\left[ \left\lVert \sum_{i=1}^n x_i\sigma_i \right\rVert_2^2\right]} \quad \quad \quad (\text{Jensen's inequality})\\
%         &= \frac{B}{n} \sqrt{\bbE_{\sigma}\left[  \sum_{i=1}^n \lVert x_i\rVert_2^2\sigma_i^2 + 2\sum_{1 \leq i < j \leq n} \langle x_i\sigma_i, x_j\sigma_j \rangle\right]} \quad \quad \quad ( \sigma_i,\sigma_j\text{ are independent and } \bbE[\sigma_i] = 0)\\
%         &= \frac{B}{n} \sqrt{ \sum_{i=1}^n \lVert x_i\rVert_2^2}.
%     \end{align*}
% Taking its expectation, we have
% $$
%     R_n(\cH) = \bbE[R_S(\cH)] \leq \frac{B}{n}\bbE\left[\sqrt{ \sum_{i=1}^n \lVert x_i\rVert_2^2}\right] \leq \frac{B}{n}\sqrt{\bbE\left[ \sum_{i=1}^n \lVert x_i\rVert_2^2\right]} \leq \frac{BC}{n}.
% $$
% Here, we again use Jensen's inequality to derive the second inequality.
% \end{proof}


Many of our proofs require the usage of Talgrand's lemma, which we now present.
\begin{lemma} 
[Talgrand's Lemma \citep{ledoux1991probability}]\label{lemma:talgrand}
Let $\phi: \R \to \R$ be a $k$-Lipschitz function. Then for a hypothesis class $\cH = \{ h: \R^d \to \R \}$, we have that
$$ R_S(\phi \circ \cH) \leq k R_s(\cH)$$
where $\phi \circ \cH = \{f: z \mapsto \phi(h(z)) | h \in \cH \}$. \end{lemma}
\section{Examples for EPAC learnable constraints}
\label{appendix: EPAC learnable}

In this section, we look at the Rademacher complexity of $\cG$  for different explanation constraints to characterize how many samples with explanation constraints are required in order to generalize to satisfying the explanation constraints on unseen data. We remark that this is a different notion of sample complexity; these unlabeled data require annotations of explanation constraints, not standard labels. In practice, this can be easier and less expertise might be necessary if define the surrogate loss $\phi$ directly. First, we analyze the case where our explanation is given by the gradient of a linear model.
\begin{proposition}[Learning a gradient constraint for  linear models]
Let $\cD$ be a distribution over $\R^d$. Let $\cH = \{h: x \mapsto \langle w_h, x \rangle \mid w_h \in \mathbb{R}^d, \lVert w_h \rVert_2 \leq B\}$ be a class of linear models that pass through the origin. Let $\phi(h,x) = \theta(w_h, w_{h'})$ be a surrogate explanation loss. Let $\cG = \{\phi(h,\cdot) \mid h \in \cH\}$, then we have
\begin{equation*}
    R_n(\cG) \leq  \frac{\pi}{2\sqrt{m}}. 
\end{equation*}
\end{proposition}
\begin{proof}
    For a linear separator, $\phi(h,\cdot)$ is a constant function over $\cX$. The Rademacher complexity is given by
    \begin{align*}
        R_n(\cG) &= \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{\phi(h,\cdot) \in\cG}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i\phi(h,x_i) \right)\right] \right]\\
        &= \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{h \in \cH}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i \right)\theta(w_h, w_{h'})\right] \right]\\
        &= \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\left(\frac{1}{m}\sum_{i=1}^m \sigma_i \right)\sup_{h \in \cH}\theta(w_h, w_{h'})\right] \right] \\
         &= \frac{\pi}{2}\displaystyle \displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\left|\frac{1}{m}\sum_{i=1}^m \sigma_i \right|\right]  \\
        &\leq  \frac{\pi}{2\sqrt{m}}.
    \end{align*}
\end{proof}

We  compare this with the Rademacher complexity of linear models  which is given by $R_m(\cH) \leq \frac{B}{\sqrt{m}}$. The upper bound does not depend on the upper bound on the weight $B$. In practice, we know that the gradient of a linear model is constant for any data point. This implies that knowing a gradient of a single point is enough to identify the gradient of the linear model. 

% First learning to restrict out class to $\cH_{\phi,\tau}$ extremely helpful for the performance and sample complexity, as is evident in Figure \ref{fig:linear_mse}.

% This makes sense as the input gradient for a linear separator is fixed, and knowing a gradient of a single point is enough to identify the true linear separator. 

% Therefore, in this case, $\cG$ actually has a significantly smaller Rademacher complexity. First learning to restrict our class to $\cG$ is 

% extremely helpful for the performance and sample complexity, as is evident in Figure \ref{fig:linear_mse}.

We consider another type of explanation constraint that is given by a noisy model. Here, we could observe either a noisy classifier and noisy regressor, and the constraint could be given by having similar outputs to this noisy model. This is reminiscent of learning with noisy labels \citep{natarajan2013learning} or weak supervision \citep{ratner2016data, ratner2017snorkel, pukdee2022label}. In this case, our explanation $g$ is simply the hypothesis element $h$ itself, and our constraint is on the values that $h(x)$ can take. We first analyze this in the classification setting.

\begin{proposition}[Learning a constraint given by a noisy classifier]
Let $\cD$ be a distribution over $\R^d$. Consider a binary classification task with $\cY = \{-1,1\}$. Let $\cH$ be a hypothesis class. Let $\phi(h,x) = 1[h(x) \neq h'(x)]$ be a surrogate explanation loss. Let $\cG = \{\phi(h,\cdot) \mid h \in \cH\}$, then we have
\begin{equation*}
    R_n(\cG) = \frac{1}{2}R_n(\cH). 
\end{equation*}
\end{proposition}
\begin{proof}
        \begin{align*}
        R_n(\cG) &= \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{\phi(h,\cdot) \in\cG}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i\phi(h,x_i) \right)\right] \right]\\
        &= \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{h \in \cH}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i (\frac{1-h(x)h'(x)}{2})\right)\right] \right]\\
        &= \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{h \in \cH}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i (\frac{h(x)h'(x)}{2})\right)\right] \right]\\
        &= \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{h \in \cH}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i (\frac{h(x)}{2})\right)\right] \right]\\
        &= \frac{1}{2}R_n(\cH).
        \end{align*}
\end{proof}

Here, to learn the restriction of $\cG$ is on the same order of $R_n(\cH)$. For a given noisy regressor, we observe slightly different upper bound.

\begin{proposition}[Learning a constraint given by a noisy regressor]
Let $\cD$ be a distribution over $\R^d$. Consider a regression task with $\cY = \R$. Let $\cH$ be a hypothesis class that $\forall h \in \cH, -h \in \cH$. Let $\phi(h,x) = |h(x) - h'(x)|$ be a surrogate explanation loss. Let $\cG = \{\phi(h,\cdot) \mid h \in \cH\}$, then we have

\begin{equation*}
    R_n(\cG) \leq 2R_n(\cH).
\end{equation*}

\end{proposition}

\begin{proof}
        \begin{align*}
        R_n(\cG) &= \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{\phi(h,\cdot) \in\cG}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i\phi(h,x_i) \right)\right] \right]\\
        &= \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{h \in \cH}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i |h(x_i) - h'(x_i)|\right)\right] \right]\\ 
        & = \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{h \in \cH}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i \max(0, h(x_i) - h'(x_i)) + \frac{1}{m}\sum_{i=1}^m  \sigma_i \max(0, h'(x_i) - h(x_i)) \right)\right] \right]\\
        % & \leq \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{h \in \cH}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i |h(x)| + \sigma_i | h'(x)|\right)\right] \right]\\
        & \leq \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{h \in \cH}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i \max(0, h(x_i) - h'(x_i)) \right) \right] \right] + \quad \\
        & \quad \quad \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[ \sup_{h\in\cH} \left( \frac{1}{m}\sum_{i=1}^m  \sigma_i \max(0, h'(x_i) - h(x_i)) \right)\right] \right] \\
        & \leq \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{h \in \cH}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i (h(x_i) - h'(x_i)) \right) \right] \right] +  \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[ \sup_{h\in\cH} \left( \frac{1}{m}\sum_{i=1}^m  \sigma_i ( h'(x_i) - h(x_i)) \right)\right] \right],
        \end{align*}
where in the last line, we apply Talgrand's lemma \ref{lemma:talgrand} and note that the max function $\max(0, h(x))$ is 1-Lipschitz; in the third line, we note that we break up the supremum as both terms by definition of the $\max$ function are non-negative. Then, noting that we do not optimize over $h'(x)$, we further simplify this as
\begin{align*}
    R_n(\cG) & \leq \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{h \in \cH}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i h(x_i) \right) \right] \right]+  \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[ \sup_{h\in\cH} \left( \frac{1}{m}\sum_{i=1}^m  \sigma_i ( - h(x_i)) \right)\right] \right] \\
    & \leq 2 R_n(\cH).
\end{align*}\end{proof}
% As mentioned before, knowing apriori the target function $f'$ might be too strong. Therefore, we also consider the case when the constraints are from a learnable class of noisy regressors, $\cC$. Here, we note that $C(x) \in \cC$ is \textit{not known}, and we must determine $C(x)$ from unlabeled data that are annotated with explanations. This requires some assumptions about the distribution $\cD$. 

As mentioned before, knowing apriori surrogate loss $\phi$ might be too strong. In practice, we may only have access to the instances $\phi(\cdot, x_i)$ on a set of samples $S = \{x_1,\dots, x_k\}$. We also consider the case when $\phi(h,x) = |h(x) - h'(x)|$ when $h'$ is unknown and $h'$ belongs to a learnable class $\cC$. 

\begin{proposition}[Learning a constraint given by a noisy regressor from some learnable class $\cC$]
Assume $\cD$ is a distribution over  $\R^d$. Let $\cH$ and $\cD$ be hypothesis classes.   Let $\phi_{h'}(h,x) = |h(x) - h'(x)|$ be a surrogate explanation loss of a constraint corresponding to $h'$. Let $\cG_\cC=\{\phi_{h'}(h, \cdot)|h \in \cH, h' \in \cC \}$, then we have
% \begin{equation*}
%     R_n(\cG) = \frac{B}{\sqrt{n}} \left( 1 + \sin(\tau) \cdot p + \frac{1 - p}{2} \right),
% \end{equation*}

\begin{equation*}
    R_n(\cG_\cC) \leq 2 R_n(\cH) + 2R_n(\cC).
\end{equation*}

\end{proposition}

\begin{proof}
    \begin{align*}
        R_n(\cG_\cC) &= \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{\phi(h,\cdot) \in\cG_\cC}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i\phi(h,x_i) \right)\right] \right]\\
        &= \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{\substack{h \in \cH, \\  h' \in \cC}}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i |h(x_i) - h'(x_i)|\right)\right] \right]\\
        & \leq \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{\substack{h \in \cH, \\  h' \in \cC}}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i \max(0, h(x_i) - h'(x_i)) \right) \right] \right] \quad + \\
        & \quad \quad \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[ \sup_{\substack{h \in \cH, \\  h' \in \cC}}\left( \frac{1}{m}\sum_{i=1}^m  \sigma_i \max(0, h'(x_i) - h(x_i)) \right)\right] \right]\\
        & \leq \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{\substack{h \in \cH, \\  h' \in \cC}}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i (h(x_i) - h'(x_i)) \right) \right] \right] \quad + \\
        & \quad \quad \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[ \sup_{\substack{h \in \cH, \\  h' \in \cC}}\left( \frac{1}{m}\sum_{i=1}^m  \sigma_i ( h'(x_i) - h(x_i)) \right)\right] \right]
    \end{align*}
    where the lasts line again holds by an application of Talgrand's lemma. In this case, we indeed are optimizing over $h'$, so we get that
    \begin{align*}
        R_n(\cG_\cC) & \leq 2 \cdot \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{h \in \cH}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i (h(x_i) ) \right) \right] \right] + 2\cdot \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{h' \in \cC}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i (h'(x_i)) \right) \right] \right] \\
        & = 2 R_n(\cH) + 2R_n(\cC).
    \end{align*}
\end{proof}
We remark that while this value is much larger than that of $R_n(\cH)$, we only need information about $\phi(h, x)$ and \emph{not} the true label. Therefore, in many cases, this is preferable and not as expensive to learn. 

% In addition, when $\cD$ is uniform over a unit sphere, we can invoke our result in Theorem \ref{theorem:bound_linear} to get the result that
%     \begin{align*}
%         R_n(\cG_\cC) & \leq 2\frac{B}{\sqrt{n}} + 2\frac{B}{\sqrt{n}} \left( \sin(\tau) p + \frac{1 - p}{2} \right),
%     \end{align*}
%     where $$ p = \erf\left( \frac{\sqrt{d} \sin(\tau)}{\sqrt{2}}\right).$$ 

% \begin{proof}
%     \begin{align*}
%         R_n(\cG) &= \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{\phi(h,\cdot) \in\cG}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i\phi(h,x_i) \right)\right] \right]\\
%         &= \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{\substack{h \in \cH, \\ h' | \theta(w_h', w_{h^*}) \leq \tau}}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i |h(x) - h'(x)|\right)\right] \right]\\
%         & \leq \displaystyle \mathop{\mathbb{E}}_{x \sim D}\left[\displaystyle \mathop {\mathbb{E}}_{\sigma}\left[\sup_{h \in \cH}\left(\frac{1}{m}\sum_{i=1}^m \sigma_i h(x)\right) + \sup_{h' | \theta(w_h', w_h^*) \leq \tau} \left( \sum_{i=1}^m \sigma_i h'(x) \right)\right] \right]\\
%         &= R_n(\cH) + R_n(\cH'),
%     \end{align*}
%     where $\cH'$ is the class of linear models constrained to have gradient with angle $\theta(w_h, w_h^*) \leq \tau$. As $\cD$ is uniform over a unit sphere, we can invoke our result in Theorem \ref{theorem:bound_linear} to get the result that
%     \begin{align*}
%         R_n(\cG) & = \frac{B}{\sqrt{n}} + \frac{B}{\sqrt{n}} \left( \sin(\tau) p + \frac{1 - p}{2} \right).
%     \end{align*} \end{proof}

% \begin{equation*}
%     R_n(\cG) \leq \frac{2B}{\sqrt{n}} \left( 1 + \sin(\tau) \cdot p + \frac{1 - p}{2} \right),
% \end{equation*}

% where 
% $$ p = \erf\left( \frac{\sqrt{d} \sin(\tau)}{\sqrt{2}}\right)$$



\section{Proof of Theorem \ref{thm: generalization bound agnostic}}\label{appx:generalization_bound_agnostic}

We consider the agnostic setting of Theorem \ref{thm: generalization bound agnostic}. Here, we have two notions of deviations; one is deviation in a model's ability to satisfy explanations, and the other is a model's ability to generalize to correctly produce the target function.
\begin{proof}
    From Rademacher-based uniform convergence, for any $h \in \cH$, with probability at least $1 - \delta/2$ over $S_E$
\begin{equation*}
    |\phi(h, \cD) - \phi(h, S_E)| \leq 2R_k(\cG) + \sqrt{\frac{\ln(4/\delta)}{2k}} = \varepsilon_k
\end{equation*}
 Therefore, with probability at least $1 - \delta/2$, for any $h \in \cH_{\phi,t - \varepsilon_k}$  we also have $\phi(h, S_E) \leq t $ and for any $h$ with $\phi(h, S_E) \leq t$, we have $h \in \cH_{\phi, t + \varepsilon_k}$. In addition, by a uniform convergence bound, with probability at least $1 - \delta / 2$, for any $h \in \cH_{\phi, t + \varepsilon_k}$
\begin{align*}
    |\err_\cD(h) - \err_S(h)| &\leq R_n(\cH_{\phi, t + \varepsilon_k}) + \sqrt{\frac{\ln(4/\delta)}{2n}}.
\end{align*}
Now, let $h'$ be the minimizer of $\err_S(h)$ given that $\phi(h, S_E) \leq t$. By previous results, with probability $1 - \delta$, we have $h' \in \cH_{\phi, t + \varepsilon_k}$ and 
\begin{align*}
    \err_\cD(h') &\leq \err_S(h') +  R_n(\cH_{\phi, t + \varepsilon_k}) + \sqrt{\frac{\ln(4/\delta)}{2n}}\\
    &\leq \err_S(h^*_{t-\varepsilon_k}) +  R_n(\cH_{\phi, t + \varepsilon_k}) + \sqrt{\frac{\ln(4/\delta)}{2n}} \\
    &\leq \err_\cD(h^*_{t-\varepsilon_k}) + 2 R_n(\cH_{\phi, t + \varepsilon_k}) + 2\sqrt{\frac{\ln(4/\delta)}{2n}}.
\end{align*}
\end{proof}


\section{A Generalization Bound in the Realizable Setting}
\label{appendix: realizable setting}
In this section, we assume that we are in the doubly realizable \cite{balcan2010discriminative} setting where there exists $h^*\in \cH$ such that $\error_{\cD}(h^*) = 0$ and $\phi(h^*, \cD) = 0$. The optimal classifier $h^*$ lies in $\cH$ and also achieve zero expected explanation loss. In this case, we want to output a hypothesis $h$ that achieve both zero empirical risk and empirical explanation risk.

\begin{theorem}
[Generalization bound for the doubly realizable setting]
For a hypothesis class $\cH$, a distribution $\cD$ and an explanation loss $\phi$. Assume that there exists $h^* \in \cH$ that $\err_\cD(h^*) = 0$ and $\phi(h^*, \cD) = 0$. Let  $S = \{(x_1, y_1), \dots, (x_n, y_n) \}$ is drawn i.i.d. from $\cD$ and $S_E = \{ x'_1,\dots, x'_k\}$ drawn i.i.d. from $\cD_\cX$. With probability at least $1-\delta$, for any $h \in \cH$ that $\err_S(h) = 0$ and $\phi(h, S_E) = 0$, we have
\begin{equation*}
    \err_D(h) \leq R_n(\cH_{\phi, \varepsilon_k}) + \sqrt{\frac{\ln(2/\delta)}{2n}}
\end{equation*}
when 
\begin{equation*}
    \varepsilon_k = 2R_k(\cG) + \sqrt{\frac{\ln(2/\delta)}{2k}}
\end{equation*}
when $\cG = \{\phi(h, x) \mid  h \in \cH, x \in \cX\}$.

\end{theorem}
\begin{proof}
We first consider only classifiers than has low empirical explanation loss and then perform standard supervised learning. From Rademacher-based uniform convergence, for any $h \in \cH$, with probability at least $1 - \delta/2$ over $S_E$
\begin{equation*}
    \phi(h, \cD) \leq \phi(h, S_E)  + 2R_k(\cG) + \sqrt{\frac{\ln(2/\delta)}{2k}}
\end{equation*}
when $\cG = \{\phi(h, x) \mid  h \in \cH, x \in \cX\}$. Therefore, for any $h \in \cH$ with $\phi(h, S_E) = 0$, we have $h \in \cH_{\phi, \varepsilon_k}$ with probability at least $1 - \delta/2$. Now, we can apply the uniform convergence on $\cH_{\phi, \varepsilon_k}$. For any $h \in \cH_{\phi, \varepsilon_k}$ with $\err_S(h) = 0$, with probability at least $1 - \delta/2$, we have
\begin{equation*}
    \err_\cD(h) \leq  R_n(\cH_{\phi, \varepsilon_k}) + \sqrt{\frac{\ln(2/\delta)}{2n}}.
\end{equation*}
Therefore, for $h \in \cH$ that $\phi(h, S_E) = 0, \err_S(h) = 0$, we have our desired guarantee.
\end{proof}

We remark that, since our result relies on the underlying techniques of the Rademacher complexity, our result is on the order of $O(\frac{1}{\sqrt{n}})$. In the (doubly) realizable setting, this is somewhat loose, and more complicated techniques are required to produce tighter bounds. We leave this as an interesting direction for future work.

\section{Rademacher Complexity of Linear Models with a Gradient Constraint}\label{appendix: main theory linear}

\begin{proof}
(Proof of Theorem \ref{theorem:linear_distribution_free})
    Recall that $\cH_{\phi,\tau} = \{h: x \to \langle w_h, x \rangle \mid w_h \in \R^d, ||w_h||_2 \leq B, \theta(w_h, w_{h'}) \leq \tau\}$. For a set of sample $S$, the empirical Rademacher complexity of $\cH_{\phi,\tau}$ is given by
        \begin{align*}
        R_S(\cH_{\phi,\tau}) &= \frac{1}{n} \bbE_{\sigma}\left[\sup_{h \in \cH_{\phi,\tau}} \sum_{i=1}^n h(x_i)\sigma_i\right]\\
        &= \frac{1}{n} \bbE_{\sigma}\left[\sup_{\substack{\lVert w_h\rVert_2 \leq B\\ \theta(w_h, w_{h'}) \leq \tau}} \sum_{i=1}^n \langle w_h, x_i \rangle\sigma_i\right]\\
        &= \frac{1}{n} \bbE_{\sigma}\left[\sup_{\substack{\lVert w_h\rVert_2 \leq B\\ \theta(w_h, w_{h'}) \leq \tau}}  \langle w_h, \sum_{i=1}^n x_i\sigma_i \rangle\right].
    \end{align*}
 For a vector $w'\in \R^d$ with $\lVert w'\rVert_2 = 1$, and a vector $v\in \R^d$, we will claim the following, 
\begin{enumerate}
    \item If $\theta(v, w') \leq \tau$, we have
    $$
\sup_{\substack{\lVert w\rVert_2 \leq B\\ \theta(w,w') \leq \tau}}  \langle w, v \rangle = B\lVert v \rVert.
$$
\item If $\frac{\pi}{2} + \tau \leq \theta(v, w') \leq \pi$, we have
    $$
\sup_{\substack{\lVert w\rVert_2 \leq B\\ \theta(w,w') \leq \tau}}  \langle w, v \rangle = 0.
$$
\item If $\tau \leq \theta(v, w') \leq \frac{\pi}{2} + \tau $, we have
$$
\sup_{\substack{\lVert w\rVert_2 \leq B\\ \theta(w,w') \leq \tau}}  \langle w, v \rangle = B\lVert v\rVert \cos(\theta(v, w') - \tau)
$$
% $$
% \sup_{\substack{\lVert w\rVert_2 \leq B\\ \theta(w,w') \leq \tau}}  \langle w, v \rangle = B(\sin (\tau) \sqrt{\lVert v\rVert_2^2 - \langle v, w'\rangle^2)} + \cos (\tau) \langle v, w'\rangle).
% $$


\end{enumerate}

For the first claim, we can see that if $\theta(v,w') \leq \tau$, we can pick $w = \frac{Bv}{\lVert v \rVert}$ and achieve the optimum value. For the second claim, we use the fact that $\theta(\cdot, \cdot)$ satisfies a triangle inequality and for any $w$ that $\theta(w,w') \leq \tau$, we have
\begin{align*}
    \theta(v,w) + \theta(w,w') &\geq \theta(v,w')\\
    \theta(v,w) \geq \theta(v,w') - \theta(w,w')\\
    \theta(v,w) \geq \frac{\pi}{2} + \tau - \tau = \frac{\pi}{2}.
\end{align*}
This implies that for any $w$ that $\theta(w,w') \leq \tau$, we have
$\langle w, v \rangle = \lVert w \rVert \lVert v \rVert \cos(\theta(v,w)) \leq 0$ and the supremum is given by $0$ where we can set $\lVert w \rVert = 0$. For the third claim, we know that $\langle w, v \rangle$ is maximum when the angle between $v,w$ is the smallest. From the triangle inequality above, we must have $\theta(w,w') = \tau$ to be the largest possible value so that we have the smallest lower bound $\theta(v,w) \geq \theta(v,w') - \theta(w,w')$. In addition, the inequality holds when $v,w',w$ lie on the same plane. Since we do not have further restrictions on $w$, there exists such $w$ and we have 
$$
\sup_{\substack{\lVert w\rVert_2 \leq B\\ \theta(w,w') \leq \tau}}  \langle w, v \rangle = B\lVert v\rVert \cos(\theta(v, w') - \tau)
$$
as required. One can calculate a closed form formula for $w$ by solving a quadratic equation. Let $w = \frac{B\Tilde{w}}{\lVert \Tilde{w} \rVert}$ when $\Tilde{w} = v + \lambda w'$ for some constant $\lambda > 0$ such that $\theta(w, w') = \tau$. With this we have an equation
\begin{align*}
    \frac{\langle \Tilde{w}, w'\rangle}{\lVert \Tilde{w} \rVert} &= \cos(\tau)\\
    \frac{\langle v + \lambda w', w'\rangle}{\lVert v + \lambda w' \rVert} &= \cos(\tau)\\
\end{align*}
Let $\mu = \langle v, w'\rangle$, solving for $\lambda$, we have
\begin{align*}
    \frac{\mu + \lambda}{\sqrt{\lVert v \rVert^2 + 2\lambda\mu + \lambda^2}} &= \cos(\tau)\\
    \mu^2 + 2\mu\lambda + \lambda^2 &= \cos^2(\tau)(\lVert v \rVert^2 + 2\lambda\mu + \lambda^2)\\
    \sin^2(\tau)\lambda^2 + 2\sin^2(\tau)\mu\lambda + \mu^2 - \cos^2(\tau)\lVert v \rVert^2 &= 0\\
     \lambda^2 + 2\mu\lambda + \frac{\mu^2}{\sin^2(\tau)} - \cot^2(\tau)\lVert v \rVert^2 &= 0\\
\end{align*}
Solve this quadratic equation, we have
$$
\lambda = -\mu \pm \cot(\tau)\sqrt{\lVert v \rVert^2 - \mu^2}.
$$
Since $\lambda > 0$, we have $\lambda = -\mu + \cot(\tau)\sqrt{\lVert v \rVert^2 - \mu^2}$. We have

\begin{align*}
    \Tilde{w} &= v + \lambda w'\\
    &= v+ (-\mu + \cot(\tau)\sqrt{\lVert v \rVert^2 - \mu^2})w'\\
    &= v - \langle v, w' \rangle w' + cot(\tau)w'\sqrt{\lVert v \rVert^2 - \mu^2}.
\end{align*}


With these claims, we have
\begin{align*}
    R_S(\cH_{\phi, \tau}) &= \frac{1}{n} \bbE_{\sigma}\left[\sup_{\substack{\lVert w_h\rVert_2 \leq B\\ \theta(w_h, w_{h'}) \leq \tau}}  \langle w_h, \sum_{i=1}^n x_i\sigma_i \rangle\right]\\
    &= \frac{B}{n} \bbE_{\sigma}\left[\lVert v \rVert 1\{\theta(v,w') \leq \tau\} + \lVert v \rVert 1\{\tau \leq \theta(v,w') \leq \frac{\pi}{2} + \tau \} \cos(\theta(v,w') - \tau)\right]\\
    &= \frac{B}{n} \bbE_{\sigma}\left[\lVert v \rVert f(v)\right].
\end{align*}

\end{proof}

\begin{theorem}
[Rademacher complexity of linear models with gradient constraint, uniform distribution on a sphere]
Let $\cX$ be an instance space in $\R^d$, let  $\cD_\cX$ be a uniform distribution on a unit sphere in $\R^d$, let $\cH = \{h: x \to \langle w_h, x \rangle \mid w_h \in \R^d, ||w_h||_2 \leq B\}$ be a class of linear model with weights bounded by some constant $B>0$ in $\ell_2$ norm. Assume that there exists a constant $C>0$ such that $\bbE_{x \sim \cD_\cX}[||x||_2^2] \leq C^2$. Assume that we have an explanation constraint in terms of gradient constraint; we want the gradient of our linear model to be close to the gradient of some linear model $h'$. Let $\phi(h,x) = \theta(w_h, w_{h'})$ be an explanation surrogate loss when $\theta(u,v)$ is an angle between $u,v$. We have 
\begin{align*}
    R_n(\cH_{\phi, \tau}) & = \frac{B}{\sqrt{n}} \left(\sin(\tau) \cdot p + \frac{1 - p}{2} \right),
\end{align*}
where
$$ p = \erf\left( \frac{\sqrt{d}\sin(\tau)}{\sqrt{2}}\right).$$
\end{theorem}
\begin{proof}
    From Theorem \ref{theorem:linear_distribution_free}, we have that
    \begin{align*}
        R_n(\cH_{\phi,\tau}) &= \bbE[R_S(\cH_{\phi,\tau})]\\
        &= \frac{B}{n} \bbE_\cD\left[\bbE_{\sigma}\left[\lVert v \rVert 1\{\theta(v,w') \leq \tau\} + \lVert v \rVert 1\{\tau \leq \theta(v,w') \leq \frac{\pi}{2} + \tau \} \cos(\theta(v,w') - \tau)\right]\right]\\
        &= \frac{B}{n} \bbE_\cD\left[\bbE_{\sigma}\left[\lVert v \rVert 1\{\theta(v,w') \leq \frac{\pi}{2} - \tau \} + \lVert v \rVert 1\{\frac{\pi}{2} - \tau \leq \theta(v,w') \leq \frac{\pi}{2} + \tau \} \cos(\theta(v,w') - \tau)\right]\right]
    \end{align*}
when $v = \sum_{i=1}^n x_i\sigma_i$. Because $x_i$ is drawn uniformly from a unit sphere, in expectation $\theta(v,w')$ has a uniform distribution over $[0,\pi]$ and the distribution $\lVert v \rVert$ for a fixed value of $\theta(v,w')$ are the same for all $\theta(v,w')\in [0,\pi]$. From Trigonometry, we note that
\begin{align*}
    \cos(\frac{\pi}{2} - 2\tau + a) + \cos(\frac{\pi}{2} - a) = \sin(2\tau - a) + \sin(a) \leq 2\sin(\tau).
\end{align*}
By the symmetry property and the uniformity of the distribution of $\theta(v,w')$ and $\lVert v \rVert$.
\small
\begin{align*}
    &\bbE_\cD\left[\bbE_{\sigma}\left[\lVert v \rVert 1\{\frac{\pi}{2} - \tau \leq \theta(v,w') \leq \frac{\pi}{2} + \tau \} \cos(\theta(v,w') - \tau)\right]\right]\\ 
    &= \bbE_\cD\left[\bbE_{\sigma}\left[\lVert v \rVert 1\{0 \leq \theta(v,w') \leq 2\tau \} \cos(\frac{\pi}{2} + \theta(v,w') - \tau)\right]\right]\\ 
    &= \bbE_\cD\left[\bbE_{\sigma}\left[\lVert v \rVert( 1\{0 \leq \theta(v,w') \leq \tau \} \cos(\frac{\pi}{2} + \theta(v,w') - \tau) + 1\{\tau \leq \theta(v,w') \leq 2\tau \} \cos(\frac{\pi}{2} + \theta(v,w') - \tau)) \right]\right]\\ 
        &= \bbE_\cD\left[\bbE_{\sigma}\left[\lVert v \rVert( 1\{0 \leq \theta(v,w') \leq \tau \} \cos(\frac{\pi}{2} + \theta(v,w') - \tau) + 1\{0 \leq 2\tau -\theta(v,w') \leq \tau \} \cos(\frac{\pi}{2} - (2\tau -  \theta(v,w')) )) \right]\right]\\ 
            &= \bbE_\cD\left[\bbE_{\sigma}\left[\lVert v \rVert( 1\{0 \leq \theta(v,w') \leq \tau \} \cos(\frac{\pi}{2} + \theta(v,w') - \tau) + 1\{0 \leq \Tilde{\theta}(v,w') \leq \tau \} \cos(\frac{\pi}{2} - \Tilde{\theta}(v,w') )) \right]\right]\\ 
        &= \bbE_\cD\left[\bbE_{\sigma}\left[\lVert v \rVert( 1\{0 \leq \theta(v,w') \leq \tau \} \cos(\frac{\pi}{2} + \theta(v,w') - \tau) + 1\{0 \leq \theta(v,w') \leq \tau \} \cos(\frac{\pi}{2} - \theta(v,w') )) \right]\right]\\ 
    &\leq
    \bbE_\cD\left[\bbE_{\sigma}\left[\lVert v \rVert 1\{\frac{\pi}{2} - \tau \leq \theta(v,w') \leq \frac{\pi}{2} + \tau \} \sin( \tau)\right]\right]
\end{align*}
\normalsize
when $\Tilde{\theta}(v,w') = \frac{\pi}{2} - \theta(v,w') $. We have
\begin{align*}
    R_n(\cH_{\phi,\tau}) &\leq \frac{B}{n}\bbE_\cD\left[\bbE_{\sigma}\left[ \lVert v \rVert 1\{\theta(v,w') \leq \frac{\pi}{2} - \tau \} + \lVert v \rVert 1\{\frac{\pi}{2} - \tau \leq \theta(v,w') \leq \frac{\pi}{2} + \tau \} \sin( \tau)\right]\right]\\
    &= \frac{B}{n}\bbE_\cD\left[\bbE_{\sigma}\left[ \lVert v \rVert\right]\right] ( \Pr(\theta(v,w') \leq \frac{\pi}{2} - \tau) + \Pr(\frac{\pi}{2} - \tau \leq \theta(v,w') \leq \frac{\pi}{2} + \tau)\sin(\tau))
\end{align*}
The last equation follows from the symmetry and uniformity properties. We can bound the first expectation
\begin{align*}
    \bbE_\cD[\bbE_\sigma \lVert v \rVert]] &= \bbE_\cD[\bbE_\sigma \lVert \sum_{i=1}^n x_i\sigma_i \rVert]]\\
    &\leq \bbE_\cD[\sqrt{\bbE_\sigma \lVert \sum_{i=1}^n x_i\sigma_i \rVert^2]}]\\
    &= \bbE_\cD[\sqrt{\bbE_\sigma \sum_{i=1}^n 
    \lVert x_i \rVert^2 \sigma_i^2 ]}]\\
    &\leq C\sqrt{n}.
\end{align*}
Next, we can simply note that, since our data is distributed over a unit sphere, each data has norm no greater than 1. Therefore, we know that $C = 1$ is indeed an upper bound on $\bbE_{x \sim \cD_\cX}[||x||_2^2]$. For the probability term, we note that in expectation $v$ has the same distribution as a random vector $u$ drawn uniformly from a unit sphere. We let this be some probability $p$:
\begin{align*}
 p = \Pr\left(\frac{\pi}{2} - \tau \leq \theta(v,w') \leq \frac{\pi}{2} + \tau \right) = \Pr \left(|\langle u, w' \rangle| \leq \sin(\tau)\right).
\end{align*}
We know that the projection $\langle u, w' \rangle \sim \cN(0, \frac{1}{d})$. 
% By a standard tail bound
% \begin{align*}
    % \Pr(|\langle u, w' \rangle| \leq \sin(\tau)) &= 1 -  \Pr(|\langle u, w' \rangle| \geq \sin(\tau)) \geq 1 - 2\exp(-\frac{\sin^2(\tau)}{2}).
% \end{align*}
Then, we have that $|\langle u, w' \rangle |$ is given by a Folded Normal Distribution, which has a CDF given by
\begin{align*}
    \Pr\left(|\langle u, w' \rangle | \leq \sin(\tau) \right) & = \frac{1}{2} \left[ \erf\left( \frac{ \sqrt{d} \sin(\tau)}{\sqrt{2}}\right) + \erf\left( \frac{\sqrt{d}\sin(\tau)}{\sqrt{2}}\right)  \right] \\
    & = \erf\left( \frac{\sqrt{d}\sin(\tau)}{\sqrt{2}}\right).
\end{align*}

We then observe that
\begin{align*}
    \Pr \left( \theta(v, w') \leq \frac{\pi}{2} - \tau \right) & = \frac{1}{2} \left( 1 -  \Pr \left( \frac{\pi}{2} - \tau \leq \theta(v, w') \leq \frac{\pi}{2} + \tau \right) \right)  \\
    & = \frac{1 - p}{2}
\end{align*}

Plugging this in yields the following bound
\begin{align*}
    R_n(\cH_{\phi, \tau}) & = \frac{B}{\sqrt{n}} \left(\sin(\tau) \cdot p + \frac{1 - p}{2} \right),
\end{align*}

where
$$ p = \erf\left( \frac{\sqrt{d}\sin(\tau)}{\sqrt{2}}\right).$$

\end{proof}


\section{Rademacher Complexity for Two Layer Neural Networks with a Gradient Constraint}\label{appx: main theory nn}

Here, we present the full proof of the generalization bound for two layer neural networks with gradient explanations. In our proof, we use two results from \citet{ma2022notes}. One result is a technical lemma, and the other is a bound on the Rademacher complexity of two layer neural networks. 

\begin{lemma}\label{lemma:TM_note}
    Consider a set $S = \{x_1, ..., x_n \}$ and a hypothesis class $\cF \subset \{f : \R^d \to \R\}$. If 
    $$ \sup_{f \in \cF} \sum_{i=1}^n f(x_i) \sigma_i \geq 0 \; \text{ for any } \; \sigma_i \in \{ \pm 1\}, i = 1, ..., n,$$
    then, we have that
    $$ \bbE_\sigma \left[ \sup_{f\in\cF} | \sum_{i=1}^n f(x_i) \sigma_i | \right] \leq 2 \bbE_{\sigma} \left[ \sup_{f \in \cF} \sum_{i=1}^n f(x_i) \sigma_i \right].$$\vspace{2mm}
    
\end{lemma}

\begin{theorem}
[Rademacher complexity for two layer neural networks \cite{ma2022notes}]\label{theorem:bound_2NNs}
Let $\cX$ be an instance space and $\cD_{\cX}$ be a distribution over $\cX$. Let $\cH = \{h : x \mapsto \sum_{i=1}^m w_i \sigma(u_i^\top x) | w_i \in \R, u_i \in \R^d, \sum_{i=1}^m |w_i| \lVert u_i \rVert_2 \leq B \}$ be a class of two layer neural networks with $m$ hidden nodes with a ReLU activation function $\sigma(x) = \max(0, x)$. Assume that there exists some constant $C > 0$ such that $\bbE_{x \sim \cD_{\cX}} [\lVert x \rVert_2^2 ] \leq C^2$. Then, for any $S=\{x_{1}, \dots, x_{n}\}$ is drawn i.i.d. from $\cD_\cX$, we have that
$$
R_S(\cH) \leq \frac{2B}{n}\sqrt{\sum_{i=1}^n||x_i||_2^2}
$$
and
$$ R_n(\cH) \leq \frac{2 B C}{\sqrt{n}}.$$
\end{theorem}

We defer interested readers to \cite{ma2022notes} for the full proof of this result. Here, the only requirement of the data distribution is that $\bbE_{x \sim \cD_{\cX}} [\lVert x \rVert_2^2 ] \leq C^2$. We now present our result in the setting of two layer neural networks with one hidden node $m = 1$ to provide clearer intuition for the overall proof.
\begin{theorem}
[Rademacher complexity for two layer neural networks ($m=1$) with gradient constraints]
Let $\cX$ be an instance space and $\cD_{\cX}$ be a distribution over $\cX$. Let $\cH = \{h : x \mapsto w\sigma(u^\top x) | w \in \R, u \in \R^d, |w| \leq B, \lVert u \rVert = 1 \}$. Without loss of generality, we assume that $\lVert u \rVert = 1$.  Assume that there exists some constant $C > 0$ such that $\bbE_{x \sim \cD_{\cX}} [ \lVert x \rVert_2^2 ] \leq C^2$. Our explanation constraint is given by a constraint on the gradient of our models, where we want the gradient of our learnt model to be close to a particular target function $h' \in \cH$. Let this be represented by an explanation loss given by $$\phi(h, x) = \lVert \nabla_x h(x) - \nabla_x h'(x) \rVert_2 + \infty \cdot 1 \{ \lVert \nabla_x h(x) - \nabla_x h'(x) \rVert > \tau \} $$ for some $\tau > 0$. Let $h'(x) = w'\sigma((u')^\top x)$ the target function, then we have
\begin{align*}
 R_n ( \cH_{\phi, \tau}) \leq  \frac{\tau C}{\sqrt{n}} & \quad \quad \text{ if } |w'|  > \tau, \\
 R_n ( \cH_{\phi, \tau}) \leq  \frac{3 \tau C}{\sqrt{n}} & \quad \quad \text{ if } |w'|  \leq  \tau.
\end{align*}
\end{theorem}
\begin {proof}
Our choice of $\phi(h, x)$ guarantees that, for any $h \in \cH_{\phi, \tau}$, we have that $ \lVert \nabla_x h(x) - \nabla_x h'(x) \rVert \leq \tau$ almost everywhere. We note that for $h(x) = w \sigma(u^\top x)$, the gradient is given by $\nabla_x h(x) = wu 1\{u^\top x > 0 \}$, which is a piecewise constant function over two regions (i.e., $u^\top x > 0, u^\top x \leq 0)$, captured by Figure \ref{fig:piecewise_constant}.

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{figs/2NNS_m1.pdf}
\label{fig:piecewise_constant}
\caption{Visualization of the piecewise constant function of $\nabla_x h(x) - \nabla_x h'(x)$ over 4 regions.}
\end{figure}
We now consider $\nabla_x h(x) - \nabla_x h'(x)$, and we have 3 possible cases.

\textbf{Case 1:} $\theta(u, u') > 0$ \\
This implies that the boundaries of $\nabla_x(h)$ and  $\nabla_x h'(x)$ are different. Then, we have that $\nabla_x h(x) - \nabla_x h'(x)$ is a piecewise constant function with 4 regions, taking on values
% \begin{enumerate}
%     \item  \makebox[3cm]{$wu - w'u'$} when $u^\top x > 0, (u')^\top x > 0$
%     \item  \makebox[3cm]{$wu$} when $u^\top x > 0, (u')^\top x < 0$
%     \item  \makebox[3cm]{$- w'u'$} when $u^\top x < 0, (u')^\top x > 0$
%     \item  \makebox[3cm]{$0$} when $u^\top x < 0, (u')^\top x < 0$.
% \end{enumerate}
\begin{equation*}
    \nabla_x h(x) - \nabla_x h'(x) = \begin{cases}
        wu - w'u' & \text{ when } u^\top x > 0, (u')^\top x > 0 \\
        wu & \text{ when } u^\top x > 0, (u')^\top x < 0 \\
        -w'u' & \text{ when } u^\top x < 0, (u')^\top x > 0 \\
        0 & \text{ when } u^\top x < 0, (u')^\top x < 0
    \end{cases}
\end{equation*}

If we assume that each region has probability mass greater than 0 then our constraint $\lVert \nabla_x h(x) - \nabla_x h'(x) \rVert_2 \leq \tau$ implies that $|w| = |w| \lVert u \rVert \leq \tau, |w'| =  |w'| \lVert u' \rVert \leq \tau, \lVert wu - w'u'\rVert \leq \tau$.

\textbf{Case 2:} $\theta(u, u') = 0$ \\
This implies that the boundary of $\nabla_x h(x)$ and $\nabla_x h'(x)$ are the same. Then, $\nabla_x h(x) - \nabla_x h'(x)$ is a piecewise constant over two regions
% \begin{enumerate}
%     \item  \makebox[3cm]{$wu - w'u'$} when $u^\top x > 0$
%     \item  \makebox[3cm]{$0$} when $u^\top x < 0$.
% \end{enumerate}

\begin{equation*}
    \nabla_x h(x) - \nabla_x h'(x) = \begin{cases}
        wu - w'u' & \text{ when } u^\top x > 0 \\
        0 & \text{ when } u^\top x < 0
    \end{cases}
\end{equation*}

This gives us that $|w-w'| = \lVert wu - w'u' \rVert \leq \tau$.

\textbf{Case 3:} $\theta(u, u') = \pi$ \\
Here, we have that the decision boundaries of $\nabla_x h(x)$ and $\nabla_x h'(x)$ are the same but the gradients are non-zero on different sides. Then, $\nabla_x h(x) - \nabla_x h'(x)$ is a piecewise constant on two regions
% \begin{enumerate}
%     \item  \makebox[3cm]{$wu$} when $u^\top x > 0$
%     \item  \makebox[3cm]{$w'u'$} when $u^\top x < 0$.
% \end{enumerate}

\begin{equation*}
    \nabla_x h(x) - \nabla_x h'(x) = \begin{cases}
        wu & \text{ when } u^\top x > 0 \\
        -w'u' & \text{ when } u^\top x < 0
    \end{cases}
\end{equation*}

This gives us that $|w| \leq \tau$ and $|w'| \leq \tau$.

These different cases tell us that the constraint $\lVert \nabla_x h(x) - \nabla_x h'(x) \rVert \leq \tau$ reduces $\cH$ into a class of models follows either

\begin{enumerate}
    \item $u = u'$ and $|w-w'| < \tau$.
    \item $u \neq u'$ and $|w| < \tau$. However, this case only possible when $|w'| < \tau$.
\end{enumerate}
If $|w'| > \tau$, we know that we must only have the first case.
Now, we can calculate the Rademacher complexity of our restricted class $\cH_{\phi, \tau}$. We will again do this in separate cases.
% \begin{enumerate}
%     \item $\theta(u, u') = 0$, the model $h$ must have the same boundary as $h'$ and furthermore
%     \begin{align*}
%         \lVert wu - w'u' \rVert_2 & = \lVert w \lVert u \rVert \overline{u} - w' \lVert u' \rVert \overline{u'} \rVert_2 & (\lVert \overline{u} \rVert_2 = \lVert \overline{u'}\rVert_2 = 1)\\
%         & = \lVert(w \lVert u \rVert - w' \lVert u' \rVert) \overline{u} \rVert_2 & (\overline{u} = \overline{u'}) \\
%         & = | w \lVert u \rVert - w' \lVert u' \rVert | < \tau.
%     \end{align*}
%     Then,  only the weight $w$ of hypothesis $h$ can differ from that of $h'$ by at most $\tau$. 
%     \item $|w| \lVert u \rVert < \tau$, the model $h$ must have weights with norm smaller than $\tau$. However, this is only possible when $\tau$ is larger than the norm of the weights of $h' (|w'| \lVert u' \rVert < \tau)$.

%     If $h'$ has weights with larger norm than $\tau$, we know that by (1) that the restricted class $\cH$ must only contain models with the same decision boundary ($u'$) as $h'$. 
% \end{enumerate}


\textbf{Case 1:} $|w'| > \tau$ \\
For any $h \in \cH_{\phi, \tau}$, we have that $u = u'$ and $|w - w'| < \tau$.
For a sample $S = \{x_1, ..., x_n\}$, 
\begin{align*}
    R_s(\cH_{\phi, \tau}) & = \frac{1}{n} \bbE_{\sigma} \left[ \sup_{h \in \cH_{\phi, \tau}} \sum_{i=1}^n h(x_i)\sigma_i \right] \\
    & = \frac{1}{n} \bbE_{\sigma} \left[ \sup_{w} \sum_{i=1}^n w \sigma((u')^\top x_i) \sigma_i \right] & (\text{ as } u = u') \\
    & = \frac{1}{n} \bbE_{\sigma} \left[ \sup_{w} w \left(\sum_{i=1}^n \sigma((u')^\top x_i) \sigma_i \right) \right].
\end{align*}

Since, $|w-w'| < \tau$,
$$
w' - \tau < w < w' + \tau
$$

Then, we can compute the supremum over $w$ as

$$ w = \begin{cases}
    w' - \tau & \text{ if } \left( \sum_{i=1}^n \sigma( (u')^\top x_i) \sigma_i \right) < 0\\
    w' + \tau & \text{ if } \left( \sum_{i=1}^n \sigma( (u')^\top x_i) \sigma_i \right) \geq 0
\end{cases}$$

Therefore, we have
$$ \sup_{w} w \left( \sum_{i=1}^n \sigma((u')^\top x_i) \sigma_i \right) = \left( w' + \tau \operatorname{sign} \left(\sum_{i=1}^n \sigma( (u')^\top x_i) \sigma_i \right) \right) \cdot \left(\sum_{i=1}^n \sigma((u')^\top x_i) \sigma_i \right).$$

Now, we can calculate the Rademacher complexity as 
\begin{align*}
    R_S(\cH_{\phi, \tau}) & = \frac{1}{n} \bbE_{\sigma} \left[ w' \left( \sum_{i=1}^n \sigma ((u')^\top x_i) \sigma_i \right) + \tau | \sum_{i=1}^n \sigma( (u')^\top x_i) \sigma_i | \right] \\
    & = \frac{\tau}{n} \bbE_\sigma \left[ |\sum_{i=1}^n \sigma( (u')^\top x_i) \sigma_i |\right]  \\
    & \leq \frac{\tau}{n} \sqrt{ \bbE_\sigma \left[ \lVert \sum_{i=1}^n \sigma( (u')^\top x_i) \sigma_i \rVert^2 \right]} \hspace{42mm} (\text{Jensen's inequality})\\
    & = \frac{\tau}{n} \sqrt{ \bbE_\sigma \left[ \sum_{i=1}^n \sigma( (u')^\top x_i)^2 \sigma_i^2 \right]} \hspace{8mm} (\text{since } \sigma_i, \sigma_j \text{ are independent with mean 0})\\
    & \leq \frac{\tau}{n} \sqrt{\sum_{i=1}^n ((u')^\top x_i)^2}\\
    & \leq \frac{\tau}{n} \sqrt{\sum_{i=1}^n \lVert x_i \rVert^2 }.
\end{align*}
Combining this with the fact that $\bbE \left[\lVert x \rVert^2 \right] \leq C^2,$ we have
\begin{align*}
    R_n (\cH_{\phi, \tau}) &= \bbE[R_S(\cH_{\phi, \tau})] \\
    &\leq \frac{\tau}{n}\bbE[\sqrt{\sum_{i=1}^n \lVert x_i \rVert^2 }]\\
    &\leq \frac{\tau}{n}\sqrt{\bbE[\sum_{i=1}^n \lVert x_i \rVert^2 ]} \quad (\text{Jensen's inequality})\\
    &\leq \frac{\tau C}{\sqrt{n}}.
\end{align*}

\textbf{Case 2:} $|w'| \lVert u' \rVert < \tau$. \\
We know that $\cH_{\phi, \tau} = \cH_{\phi, \tau}^{(1)} \bigcup \cH_{\phi, \tau}^{(2)}$ when
\begin{align*}
    \cH_{\phi, \tau}^{(1)} & = \{ h \in \cH | h: x \to w \sigma(u^\top x), u = u', |w -w' | < \tau \}\\
    \cH_{\phi, \tau}^{(2)} & = \{ h \in \cH | h: x \to w \sigma(u^\top x), \lVert u \rVert = 1, u \neq u', |w| < \tau \}
\end{align*}

% From \textbf{Case 1}, we have
% $$
% R_S(\cH_{\phi, \tau}^{(1)}) \leq \frac{\tau}{n}\sqrt{\sum_{i=1}^n \lVert x_i \rVert^2 }.
% $$
% \begin{align*}
%      \sum_{i=1}^n h(x_i)\sigma_i &\leq w' \left( \sum_{i=1}^n \sigma ((u')^\top x_i) \sigma_i \right) + \frac{\tau}{\lVert u' \rVert} | \sum_{i=1}^n \sigma( (u')^\top x_i) \sigma_i |\\
%      &\leq w' \left( \sum_{i=1}^n \sigma ((u')^\top x_i) \sigma_i \right) + \tau\sup_{\lVert u \rVert = 1} | \sum_{i=1}^n \sigma( (u)^\top x_i) \sigma_i |
% \end{align*}
% Now, we can see that 


% consider $h \in \cH_{\phi, \tau}^{(2)}$, we have
% \begin{align*}
%     \sum_{i=1}^n h(x_i)\sigma_i &=   \sum_{i=1}^n w \sigma(u^\top x_i) \sigma_i  \\
%     &\leq |w|\lVert u \rVert \sup_{\lVert u \rVert = 1} | \sum_{i=1}^n \sigma( (u)^\top x_i) \sigma_i |\\
%     &\leq \tau \sup_{\lVert u \rVert = 1} | \sum_{i=1}^n \sigma( (u)^\top x_i) \sigma_i |.
% \end{align*}
% The last line holds from $|w|\lVert u \rVert$ for $h \in \cH_{\phi, \tau}^{(2)}$. Therefore, we have
We have
\begin{align*}
    R_S(\cH_{\phi, \tau}) & = \frac{1}{n} \bbE_\sigma \left[ \sup_{h \in \cH_{\phi,\tau}} \sum_{i=1}^n h(x_i)\sigma_i \right] \\
    & \leq \frac{1}{n} \bbE_\sigma \left[  \sup_{h \in \cH_{\phi,\tau}^{(1)}} \sum_{i=1}^n h(x_i)\sigma_i + \sup_{h \in \cH_{\phi,\tau}^{(2)}} \sum_{i=1}^n h(x_i)\sigma_i\right] \\
    & = R_S(\cH_{\phi, \tau}^{(1)}) + R_S(\cH_{\phi, \tau}^{(2)})
\end{align*}

The second line holds as $\sup_{x \in A \cup B} f(x) \leq \sup_{x \in A} f(x) + \sup_{x \in B}f(x)$ when $\sup_{x \in A} f(x) \geq 0$ and $\sup_{x \in B}f(x) \geq 0$. We know that both of these supremums be greater than zero, as we can recover the value of 0 with $w = 0$.
From \textbf{Case 1}, we know that
$$R_n(\cH_{\phi, \tau}^{(1)}) \leq \frac{\tau C}{\sqrt{n}}.$$
We also note that $\cH_{\phi, \tau}^{(2)}$ is a class of two layer neural networks with weights with norms bounded by $\tau$. From Theorem \ref{theorem:bound_2NNs}, we have that 
$$R_n(\cH_{\phi, \tau}^{(2)}) \leq \frac{2\tau C}{\sqrt{n}}.$$
Therefore, in \textbf{Case 2},

$$
R_n(\cH_{\phi, \tau}) \leq \frac{3\tau C}{\sqrt{n}}.$$

as required.
\end{proof}
Now, we consider in the general setting (i.e., no restriction on $m$). 

\begin{theorem}
[Rademacher complexity for two layer neural networks with gradient constraints ]
Let $\cX$ be an instance space and $\cD_{\cX}$ be a distribution over $\cX$ with a large enough support. Let $\cH = \{h : x \mapsto \sum_{j=1}^m w_j \sigma(u_j^\top x) | w_j \in \R, u_j \in \R^d, \lVert u_j \rVert_2 = 1, \sum_{j=1}^m |w_j| \leq B \}$. Assume that there exists some constant $C > 0$ such that $\bbE_{x \sim \cD_{\cX}} [ \lVert x \rVert_2^2 ] \leq C^2$. Our explanation constraint is given by a constraint on the gradient of our models, where we want the gradient of our learnt model to be close to a particular target function $h' \in \cH$. Let this be represented by an explanation loss given by $$\phi(h, x) = \lVert \nabla_x h(x) - \nabla_x h'(x) \rVert_2 + \infty \cdot 1 \{ \lVert \nabla_x h(x) - \nabla_x h'(x) \rVert > \tau \} $$ for some $\tau > 0$. Then, we have that

\begin{align*}
     R_n ( \cH_{\phi, \tau}) \leq  \frac{3\tau m C}{\sqrt{n}}.
\end{align*}
To be precise, 
\begin{align*}
     R_n ( \cH_{\phi, \tau}) \leq  \frac{(2m + q)\tau C}{\sqrt{n}}.
\end{align*}
when $q$ is the number of node $j$ of $h'$ such that $|w'_j| < \tau $.


\end{theorem}


We note that this result indeed depends on the number of hidden dimensions $m$; however, we note that in the general case (Theorem \ref{theorem:bound_2NNs}), the value of $B$ is $O(m)$ as it is a sum over the values of each hidden node.  We now present the proof for the more general version of our theorem.

\begin{proof}
For simplicity, we first assume that any $h \in \cH$ has that $\lVert u_j \rVert = 1, \forall j$. Consider $h \in \cH$, we write $h = \sum_{j=1}^m w_j' \sigma( (u_j')^\top x)$ and let $h'(x) = \sum_{j=1}^m w_j' \sigma( (u_j')^\top x)$ be a function for our gradient constraint. The gradient of a hypothesis $h$ is given by
$$ \nabla_x h(x) = \sum_{j=1}^m w_j u_j \cdot 1\{ u_j^\top x > 0 \},$$
which is a piecewise constant function over at most $2^m$ regions. Then, we consider that
$$ \nabla_x h(x) - \nabla_x h'(x) = \sum_{j=1}^m w_j u_j \cdot 1 \{u_j^\top x > 0\} - \sum_{j=1}^m w_j' u_j' \cdot 1\{(u_j')^\top x > 0 \}, $$
which is a piecewise constant function over at most $2^{2m}$ regions. We again make an assumption that each of these regions has a non-zero probability mass. Our choice of $\phi(h, x)$ guarantees that the norm of the gradient in each region is less than $\tau$.  Similar to the case with $m = 1$, we will show that the gradient constraint leads to a class of functions with the same decision boundary or neural networks that have weights with a small norm. 

Assume that among $u_1, ..., u_m$ there are $k$ vectors that have the same direction as $u_1', ..., u_m'$. Without loss of generality, let $u_j = u_j'$ for $j = 1, ..., k$. In this case, we have that $\nabla_x h(x) - \nabla_x h'(x)$ is a piecewise function over $2^{2m-k}$ regions. As each region has non-zero probability mass, for each $j \in \{1, ..., k\}$, we know that $\exists x$ such that 
$$ u_j^\top x = (u_j')^\top x > 0 , \quad \quad  u_i^\top x < 0 \text{ for } i \neq j , \quad \quad (u_i')^\top x < 0 \text{ for } i \neq j.$$

In other words, we can observe a data point from each region that uniquely defines the value of a particular $w_j, u_j$. In this case, we have that
\begin{align*}
    \nabla_x h(x) - \nabla_x h'(x) & = w_j u_j - w_j' u_j' \\
                                   & = (w_j - w_j') u_j'.
\end{align*}    

From our gradient constraint, we know that $||\nabla_x h(x) - \nabla_x h'(x)|| \leq \tau, \forall x$, which implies that $|w_j - w_j'| \leq \tau$ for $j = 1, ..., k$. 

On the other hand, for the remaining $j = k + 1, ..., m$, we know that there exists $x$ such that
$$ u_j^\top x > 0 , \quad \quad u_i^\top x < 0  \text{ for } i \neq j, \quad \quad (u_i')^\top x < 0 \text{ for } i = 1, ..., m. $$
Then, we have that $\nabla_x h(x) = w_j u_j$, and our constraint implies that $|w_j| \lVert u_j \rVert = |w_j| \leq \tau$. Similarly, we have that $|w_j'| \lVert u_j' \rVert = |w_j'| < \tau, $ for $j = k+1, ..., m$. We can conclude that $\cH_{\phi, \tau}$ is a class of two layer neural networks with $m$ hidden nodes (assuming $\lVert u_i \rVert$ = 1) that for each node $w_j \sigma (u_j^\top x)$ satisfies

\begin{enumerate}
    \item There exists $l \in [m]$ that $u_j = u_l'$ and $|w_j - w_l'| < \tau$.
    \item $|w_j| < \tau$
\end{enumerate}

% \begin{enumerate}
%     \item A node $w_j \sigma(u_j^\top x)$ that has the same boundary as the node $w'_l \sigma( (u_l')^\top x)$ of $h'(x)$ such that $u_j = u_l'$ and that $w_j$ differs from $w_l'$ by at most $\tau$. 
%     \item A node $w_j \sigma(u_j^\top x)$ that has a weight with small norm. In other words, it satisfies that $|w_j| \lVert u_j \rVert = |w_j| \leq \tau$. 
% \end{enumerate}

We further note that for a node $w_l' \sigma((u_l')^\top x)$ in $h'(x)$ that has that a high weight $|w_l'| > \tau$, there must be a node $w_j \sigma(u_j^\top x)$  in $h$ with the same boundary $u_j = u_l$. Otherwise, there is a contradiction with $|w_l'| < \tau$ for all nodes in $h'$ without a node in $h$ with the same boundary. We can utilize this characterization of the restricted class $\cH_{\phi,\tau}$ to bound the Rademacher complexity of the class. Let
$$
\cH' = \{h: x \mapsto \sum_{j=1}^m w_j' \sigma((u'_j)^\top x)a_j \mid a_j \in \{0,1\} \text{ and for } j \text{ that } |w_j'| > \tau, a_j = 1  \}.
$$
This is a class of two layer neural networks with at most $m$ nodes such that each node is from $h'$. We also have a condition that if the weight of the $j$-th node in $h'$ is greater than $\tau$, the $j$-th node must be present in any member of this class. Let
$$
\cH^{(\tau)} = \{h: x \mapsto \sum_{j=1}^m w_j \sigma((u_j)^\top x)a_j \mid w_j \in \R, u_j \in \R^d, |w_j| < \tau, \lVert u_j \rVert = 1\}.
$$
be a class of two layer neural networks with $m$ nodes such that the weight of each node is at most $\tau$. We claim that for any $h \in \cH_{\phi,\tau}$ there exists $h_1 \in \cH', h_2 \in \cH^{(\tau)}$ that $h = h_1 + h_2.$ For any $h\in \cH_{\phi,\tau}$, let $p_h: [m] \to [m]\cup \{0\}$ be a function that match a node in $h$ with the node with the same boundary in $h'$. Formally,
\begin{equation*}
    p_h(j) = \begin{cases}
        l & \text{ when } u_j = u_l' \\
        0 & \text{ otherwise}.
    \end{cases}
\end{equation*}
The function $p_h$ maps $j$ to $0$ if there is no node in $h'$ with the same boundary. Let $w'_0 = 0, u'_0 = [0,\dots, 0]$, we can write
\begin{align*}
    h(x) &= \sum_{j=1}^m w_j \sigma (u_j^\top x) \\
    &= \sum_{j=1}^m w_j \sigma (u_j^\top x) - w'_{p_h(j)}\sigma((u')_{p_h(j)}^\top x) + w'_{p_h(j)}\sigma((u')_{p_h(j)}^\top x)\\
    &= \underbrace{\sum_{p_h(j) \neq 0} (w_j - w'_{p_h(j)})\sigma((u')_{p_h(j)}^\top x) + \sum_{p_h(j) = 0} w_j \sigma (u_j^\top x)}_{\in \cH^{(\tau)}} + \underbrace{\sum_{p(j) \neq 0} w_{p_h(j)}'\sigma((u')_{p_h(j)}^\top x)}_{\in \cH'}.
\end{align*}
The first term is a member of $\cH^{(\tau)}$ because we know that $|w_j - w'_{p(j)}| < \tau$ or $|w_j| < \tau$. The second term is also a member of $\cH'$ since for any $l$ that $|w'_l| > \tau$, there exists $j$ that $p_h(j) = l$. Therefore, we can write $h$ in terms of a sum between a member of $\cH'$ and $\cH^{(\tau)}$. This implies that
$$
R_n(\cH_{\phi,\tau}) \leq R_n(\cH') + R_n(\cH^{(\tau)}).
$$
From Theorem \ref{theorem:bound_2NNs}, we have that 
$$R_n(\cH_{\phi, \tau}^{(\tau)}) \leq \frac{2\tau m C}{\sqrt{n}}.$$ Now, we will calculate the Rademacher complexity of $\cH'$. For $S = \{x_1,\dots, x_n\}$,
\begin{align*}
    R_S(\cH') &= \frac{1}{n} \bbE_\sigma \left[ \sup_{h \in \cH'} \sum_{i=1}^n h(x_i) \sigma_i \right] \\
    &= \frac{1}{n} \bbE_\sigma \left[ \sup_{h \in \cH'} \sum_{i=1}^n (\sum_{j=1}^m w_j' \sigma((u'_j)^\top x_i)a_j )\sigma_i \right]\\
    &= \frac{1}{n} \bbE_\sigma \left[ \sup_{h \in \cH'} \sum_{i=1}^n (\sum_{|w'_j| < \tau} w_j' \sigma((u'_j)^\top x_i)a_j + \sum_{|w'_j| > \tau} w_j' \sigma((u'_j)^\top x_i))\sigma_i \right]\\
    &= \frac{1}{n} \bbE_\sigma \left[ \sup_{a_j\in \{0,1\}} \sum_{i=1}^n \sum_{|w'_j| < \tau} w_j' \sigma((u'_j)^\top x_i)a_j \sigma_i \right]\\
    &= \frac{1}{n} \bbE_\sigma \left[ \sup_{a_j\in \{0,1\}}  \sum_{|w'_j| < \tau} a_j(w_j' \sum_{i=1}^n\sigma((u'_j)^\top x_i)\sigma_i) \right].\\
\end{align*}
To achieve the supremum, if $w_j' \sum_{i=1}^n\sigma((u'_j)^\top x_i)\sigma_i > 0$ we need to set $a_j = 1$, otherwise, we need to set $a_j = 0$. Therefore,
\begin{align*}
    R_S(\cH') &=\frac{1}{n} \bbE_\sigma \left[ \sup_{a_j\in \{0,1\}}  \sum_{|w'_j| < \tau} a_j(w_j' \sum_{i=1}^n\sigma((u'_j)^\top x_i)\sigma_i) \right]\\
    &= \frac{1}{n} \bbE_\sigma \left[  \sum_{|w'_j| < \tau} \sigma(w_j' \sum_{i=1}^n\sigma((u'_j)^\top x_i)\sigma_i) \right]\\
    &= \frac{1}{2n} \bbE_\sigma \left[  \sum_{|w'_j| < \tau} (w_j' \sum_{i=1}^n\sigma((u'_j)^\top x_i)\sigma_i)  + |w_j' \sum_{i=1}^n\sigma((u'_j)^\top x_i)\sigma_i|\right] & (\sigma(x) = \frac{x + |x|}{2})\\
    &= \frac{1}{2n} \bbE_\sigma \left[  \sum_{|w'_j| < \tau}  |w_j' \sum_{i=1}^n\sigma((u'_j)^\top x_i)\sigma_i|\right] \\
    &\leq \frac{1}{2n} \left(\sum_{|w'_j| < \tau} |w_j'| \right)\bbE_\sigma \left[ \sup_{\lVert u \rVert = 1} | \sum_{i=1}^n\sigma(u^\top x_i)\sigma_i|\right] \\
    &\leq \frac{1}{n} \left(\sum_{|w'_j| < \tau} |w_j'| \right)\bbE_\sigma \left[ \sup_{\lVert u \rVert = 1}  \sum_{i=1}^n\sigma(u^\top x_i)\sigma_i\right] &(\text{Lemma } \ref{lemma:TM_note})\\
    &\leq  \left(\sum_{|w'_j| < \tau} |w_j'| \right)\underbrace{\bbE_\sigma \left[\frac{1}{n} \sup_{\lVert u \rVert = 1}  \sum_{i=1}^nu^\top x_i\sigma_i\right]}_{\text{Empirical Rademacher complexity of a linear model}} &(\text{Talagrand's Lemma}).\\
\end{align*}
From Theorem \ref{theorem:bound_linear}, we can conclude that
$$
R_n(\cH') \leq \sum_{|w'_j| < \tau} |w_j'|\frac{C}{\sqrt{n}} \leq  \frac{q\tau C}{\sqrt{n}}\leq \frac{m\tau C}{\sqrt{n}}
$$
when $q$ is the number of nodes $j$ of $h'$ such that $|w'_j| < \tau $. Therefore,

$$
R_n(\cH') \leq \frac{(2m + q)\tau C}{\sqrt{n}} \leq \frac{3m\tau C}{\sqrt{n}}.
$$

%  We consider 
% \begin{align*}
%     \cH_{\phi, \tau}^{(1)} & = \{ h \in \cH_{\phi,\tau} | \exists j,l, u_j = u'_l \}\\
%     \cH_{\phi, \tau}^{(2)} & = \{ h \in \cH_{\phi,\tau} | \forall j,l, u_j \neq u'_l \}.
% \end{align*}
% $\cH_{\phi, \tau}^{(1)}$ is a class of neural network that has at least one node with the same boundary as some node of $h'$.
% For any $h \in \cH_{\phi, \tau}^{(1)}$ with $k > 0$ nodes that share a boundary as a node in $h'$ (we refer to these as $u_j = u_j'$ for $j = 1, 2, ..., k)$, consider that 
% \begin{align*}
%     \sum_{i=1}^n h(x_i) \sigma_i & = \sum_{i=1}^n \left( \sum_{j=1}^m w_j \sigma (u_j^\top x_i)\sigma_i \right) \\
%     & = \sum_{j=1}^m w_j \left( \sum_{i=1}^n \sigma(u_j^\top x_i) \sigma_i \right) \\
%     & = \sum_{j=1}^k w_j \left( \sum_{i=1}^n \sigma( (u_j')^\top x_i) \sigma_i \right) + \sum_{j=k+1}^m w_j \left( \sum_{i=1}^n \sigma( u_j^\top x_i) \sigma_i \right)
% \end{align*}

% We can bound these first two terms in the same fashion as done for the case when $m = 1$. The first term $\sum_{i=1}^n \sigma( (u_j)'^\top x_i) \sigma_i$ is fixed and $|w_j - w_j'| < \tau$. Then, we can let $v_j = \sum_{i=1}^n \sigma( (u_j')^\top x_i) \sigma_i$, and we observe that
% \begin{align*}
%     \sum_{j=1}^k w_j v_j & \leq \sum_{j=1}^k (w_j' + \tau \cdot \operatorname{sign}(v_j)) v_j \\
%     & = \sum_{j=1}^k w_j' v_j + \tau |v_j | \\
%     & \leq \sum_{j=1}^k w_j' v_j + k\tau \sup_{\lVert u \rVert = 1} | \sum_{i=1}^n \sigma( u^\top x_i) \sigma_i|
% \end{align*}

% For the second term, we use the fact that $|w_j| < \tau$ must be small. This has that
% \begin{align*}
%     \sum_{j=k+1}^m w_j \left( \sum_{i=1}^n \sigma (u_j^\top x) \sigma_i)\right) & \leq \left( \sum_{j=k+1}^m |w_j| \right) \max_{k+1 \leq l \leq m} | \sum_{i=1}^n \sigma( u_l^\top x_i)\sigma_i| \\
%     & \leq \left( \sum_{j=k+1}^m |w_j| \right) \sup_{\lVert u\rVert} |\sum_{i=1}^n \sigma(u^\top x_i) \sigma_i| \\
%     & \leq \tau (m-k) \sup_{\lVert u \rVert = 1} | \sum_{i=1}^n \sigma (u^\top x_i) \sigma_i|
% \end{align*}
% Combining these two terms, we have that
% \begin{align*}
%     \sum_{i=1}^n h(x_i) \sigma_i \leq \sum_{j=1}^k w_j' \left( \sum_{i=1}^n \sigma( (u_j')^\top x_i) \sigma_i \right) + \tau m \sup_{\lVert u \rVert = 1} | \sum_{i=1}^n \sigma(u^\top x_i) \sigma_i |
% \end{align*}
% This also implies that 
% \begin{align*}
%     R_S(\cH_{\phi, \tau}^{(1)}) & = \frac{1}{n} \bbE_\sigma \left[ \sup_{h \in \cH_{\phi, \tau}^{(1)}} \sum_{i=1}^n h(x_i) \sigma_i \right] \\
%     & \leq \frac{1}{n} \bbE_\sigma \left[ \sum_{i=1}^n \left( \sum_{j=1}^n w_j' \sigma( (u_j')^\top x_i) \sigma_i + \tau m \sup_{\lVert u \rVert = 1} |\sum_{i=1}^n \sigma(u^\top x_i) \sigma_i| \right)  \right] \\
%     & = \frac{\tau m }{n} \bbE_\sigma \left[ \sup_{\lVert u \rVert = 1} | \sum_{i=1}^n \sigma(u^\top x_i) \sigma_i | \right] \\
%     & \leq \frac{2 \tau m }{n} \bbE_\sigma \left[ \sup_{\lVert u \rVert = 1} \sum_{i=1}^n \sigma( u^\top x_i) \sigma_i \right] \\
%     &\leq  2 \tau m R_S(\cH'),
% \end{align*}
% where the second to last line holds from Lemma \ref{lemma:TM_note}, the last line holds from Talgrand's Lemma, and $\cH' = \{h : x \mapsto u^\top x \; | \; \lVert u \rVert = 1 \}$. This implies that
% \begin{align*}
%     R_n(\cH_{\phi, \tau}^{(1)}) & \leq 2\tau m R_n(\cH') \\
%                             & \leq \frac{2\tau m C}{\sqrt{n}}
% \end{align*}    
% where $R_n(\cH')$ is simply the Rademacher complexity of a linear model (Theorem \ref{theorem:unconstrained_linear}). If there exists $j \in [m]$ such than $|w'_j| > \tau$, we know that there must be some node of $h$ that has the same direction as $u'_j$. Therefore, $\cH_{\phi, \tau}^{(2)} = \emptyset$ and $R_S(\cH_{\phi, \tau}) = R_S(\cH_{\phi, \tau}^{(1)})$. We have
% \begin{align*}
%     R_n(\cH_{\phi, \tau}) \leq \frac{2\tau m C}{\sqrt{n}}.
% \end{align*}    


% On the other hand, if there is no such $j$, $\cH_{\phi, \tau}^{(2)} \neq \emptyset$. We note that $R_S(\cH_{\phi, \tau}^{(2)})$ is a class of two layer neural networks with weights with bounded norm by $\tau m$ since for each $j$, we must have $|w_j| < \tau$. From Theorem \ref{theorem:bound_2NNs}, we have that 
% $$R_n(\cH_{\phi, \tau}^{(2)}) \leq \frac{2\tau m C}{\sqrt{n}}.$$ Finally, we can conclude that
% \begin{align*}
%     R_n(\cH_{\phi, \tau}) = R_n(\cH_{\phi, \tau}^{(2)} \cup R_n(\cH_{\phi, \tau})^{(1)}) \leq R_n(\cH_{\phi, \tau}^{(1)}) + R_n(\cH_{\phi, \tau}^{(2)}) \leq  \frac{4\tau m C}{\sqrt{n}}
% \end{align*}
\end{proof}
A tighter bound is given by $\frac{(2m + q)\tau C}{\sqrt{n}}$ when $q$ is the number of $w'_j$ that $|w'_j| < \tau$. As $\tau \to 0$, we also have $q\to 0$. This implies that we have an upper bound of $\frac{2m\tau C}{\sqrt{n}}$ if $\tau$ is small enough. When comparing this to the original bound $\frac{2BC}{\sqrt{n}}$, we can do much better if $\tau \ll \frac{B}{m}$. We would like to point out that our bound does not depend on the distribution $\cD$ because we choose a strong explanation loss
$$\phi(h, x) = \lVert \nabla_x h(x) - \nabla_x h'(x) \rVert_2 + \infty \cdot 1 \{ \lVert \nabla_x h(x) - \nabla_x h'(x) \rVert > \tau \} $$
which guarantees that $\lVert \nabla_x h(x) - \nabla_x h'(x) \rVert_2 \leq \tau$ almost everywhere. We also assume that we are in a high-dimensional setting $d \gg m$ and there exists $x$ with a positive probability density at any partition created by $\nabla_x h(x)$. 

\section{Algorithmic Results for Two Layer Neural Networks with a Gradient Constraint}\label{appendix:2nn_algo}

Now that we have provided generalization bounds for the restricted class of two layer neural networks, we also present an algorithm that can identify the parameters of a two layer neural network (up to a permutation of the weights). In practice, we might solve this via our variational objective or other simpler regularized techniques. However, we also provide a theoretical result for the required amount of data (given some assumptions about the data distribution) and runtime for an algorithm to exactly recover the parameters of these networks under gradient constraints.

We again know that the gradient of two layer neural networks with ReLU activations can be written as
\begin{equation*}
    \nabla_x f_{w, U}(x) = \sum_{i=1}^m w_i u_i \cdot 1\{u_i^T x > 0 \},
\end{equation*}

where we consider $||u_i|| = 1$. Therefore, an exact gradient constraint given of the form of pairs $(x, \nabla_x f(x))$ produces a system of equations. 
\begin{proposition}
    If the values of $u_i$'s are known, we can identify the parameters $w_i$ with exactly $m$ fixed samples. 
\end{proposition}
\begin{proof}
    We can select $m$ datapoints, which each achieve value 1 for the indicator value in the gradient of the two layer neural network. This would give us $m$ equations, which each are of the form 
    $$\nabla_x f_{w, U}(x_i) = w_i u_i.$$
    Therefore, we can easily solve for the values of $w_i$, given that $u_i$ is known. 
\end{proof}

To make this more general, we now consider the case where $u_i$'s are not known but are at least linearly independent.

\begin{proposition}
    Let the $u_i$'s be linearly independent. Assume that each region of the data (when partitioned by the values of $u_i$) has non-trivial support $> p$. Then, with probability $1 - \delta$, we can identify the parameters $w_i, u_i$ with $O\left(2^m + \frac{m + \log(\frac{1}{\delta})}{\log(\frac{1}{1-p})}\right)$ data points and in $O(2^{2m})$ time.
\end{proposition}
\begin{proof}
    Let us partition $\mathcal{X}$ into regions satisfying unique values of the binary vector $(1\{u_1^T x > 0 \}, ..., 1\{u_m^T x > 0 \})$, which by our assumption each have at least some probability mass $p$. First, we calculate the probability that we observe one data point with an explanation from each region in this partition. This is equivalent to sampling from a multinomial distribution with probabilities ($p_1, ..., p_{2^m})$, where $p_i \geq p, \forall i$. Then,
    \begin{align*}
        \Pr(\text{observe all regions in $n$ draws}) & =  1 - \Pr(\exists i \text{ s.t. we do not observe region $i$} ) \\
        & = 1 - 2^m(1 - p)^n.
    \end{align*}
    Setting this as no less than $1 - \delta$ leads to that $n \geq \frac{m + \log(\frac{1}{\delta})}{\log(\frac{1}{1 - p})}$.
    
    Given $O(2^m + \frac{m + \log(\frac{1}{\delta})}{\log(\frac{1}{1-p})})$ pairs of data and gradients, we will observe at least one pair from each region of the partition. Then, identifying the values of $u_i$'s and $w_i$'s is equivalent to identifying the datapoints that correspond to a value of the binary vector where only one indicator value is 1. These values can be identified in $O(2^{3m})$ time; the algorithm is given in Algorithm \ref{algo_2NN}. These results demonstrate that we can indeed learn the parameters (up to a permutation) of a two layer neural network given exact gradient information. 
\end{proof}

% \section{Stuff}

% \begin{definition}
% [Truthful explanation] For a concept class $\cH$, a data distribution $\cD$ that there exists $h^* \in \cH$ with $\error_{\cD}(h^*) = 0$. An explanation $E(g,\phi)$ is truthful for learning from a concept class $\cH$ and a dataset $\cD$ if
% \begin{equation*}
%     \phi(h^*, D) = 0.
% \end{equation*}
% \end{definition}

\begin{algorithm*}[t]
    \caption{Algorithm for identifying parameters of a two layer neural network, given exact gradient constraints}\label{alg:algo_2NN}
    
    \begin{algorithmic}[1]
    \STATE \textbf{Input:} We are given $M = \{ \sum_{x \in C} x | C \in \cP(\{x_1, ..., x_m \})\}$, with $\{x_1, ..., x_m \}$ linearly independent
    \STATE \textbf{Output:} The set of basis elements $\{x_1, ..., x_m \}$
    \FUNCTION{}
        \STATE $B = \{\}$, $S = \{ \}$ \hfill  \COMMENT{Set for basis vectors and set for a current sum of at least 2 elements}
        \FOR{$x \in M$}
            \IF {$x \in S$} 
                \STATE pass
            \ELSE
                \STATE $B = B \cup \{ x \}$
                \IF {$|B| = 2$}
                    \STATE $S = \{ y_1 + y_2\}$, where $B = \{y_1, y_2 \}$
                \ELSE
                    \STATE $S = S \cup \{y + x | y \in S\}$ 
                    \hfill \COMMENT{Updating sums from adding $x$}
                \ENDIF
                \STATE $O = B \cap S$ \hfill \COMMENT{Computing overlap between current basis and sums}
                \STATE $B = B \setminus O$ \hfill \COMMENT{Removing elements contained in pairwise span}
                \STATE $S = \{y - y_o | y \in S, y_o \in O \}$ \hfill \COMMENT{Updating sums $S$ from removing set $O$}
            \ENDIF
        \ENDFOR
        \STATE \text{\textbf{return}} $B$
    \ENDFUNCTION
    \end{algorithmic}
\end{algorithm*}

\subsection{Algorithm for Identifying Regions} \label{algo_2NN}

We first note that identifying the parameters $u_i$'s and $w_i$'s of a two layer neural network is equivalent to identifying the values $\{x_1, ..., x_m \}$ from the set $\{ \sum_{x \in C} x | C \in \cP(\{x_1, ..., x_m \})\}$, where $\cP$ denotes the power set. We also assume that $x_1, ..., x_m$ are linearly independent, so we cannot create $x_i$ from any linear combination of $x_j$'s with $j \neq i$. Then, we can identify the set $\{x_1, ..., x_m \}$ as in Algorithm \ref{alg:algo_2NN}. This algorithm runs in $O(2^{3m})$ time as it iterates through each point in $M$ and computes the overlapping set $O$ and resulting updated sum $S$, which takes $O(2^{2m})$ time. From the resulting set $B$, we can exactly compute values $u_i$ and $w_i$ up to a permutation.

\section{Additional Synthetic Experiments} \label{appx:synth}

We now present additional synthetic experiments that demonstrate the performance of our approach under settings with imperfect explanations and compare the benefits of using \textit{different types} of explanations.

\subsection{Variational Objective is Better with Noisy Gradient Explanations} \label{appx:noise}

Here, we present the remainder of the results from the synthetic regression task of \ref{fig:noisy_nn}, under more settings of noise $\epsilon$ added to the gradient explanation.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figs/noise/v_nn_ep0.0001.pdf}
    \includegraphics[width=0.48\textwidth]{figs/noise/v_nn_ep0.1.pdf}
    \includegraphics[width=0.48\textwidth]{figs/noise/v_nn_ep0.5.pdf}
    \includegraphics[width=0.48\textwidth]{figs/noise/v_nn_ep1.pdf}
    \includegraphics[width=0.48\textwidth]{figs/noise/v_nn_ep2.pdf}
    \caption{Comparison of MSE on regressing a two layer neural network with explanations of noisy gradients. $m = 1000, k=20, \lambda = 10.$ For the iterative methods, $T = 10$. Results are averaged over 5 seeds.} 
    \label{fig:noisy_exp_appx}
\end{figure*}

Again, we observe that our method does better than that of the Lagrangian approach and the self-training method. Under high levels of noise, the Lagrangian method does poorly. On the contrary, our method is resistant to this noise and also outperforms self-training significantly in settings with limited labeled data. 

\subsection{Comparing Different Types of Explanations}

Here, we present synthetic results to compare using different types of explanation constraints. We focus on comparing noisy gradients as before, as well as noisy classifiers, which are used in the setting of weak supervision \citep{ratner2016data}. Here, we generate our noisy classifiers as $h^*(x) + \epsilon$, where $\epsilon \sim \cN(0, \sigma^2)$. We omit the results of self-training as it does not use any explanations, and we keep the supervised method as a baseline. Here, $t = 0.25$.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figs/compare/v_nn_noisy_classifier_0.0001.pdf}
    \includegraphics[width=0.48\textwidth]{figs/compare/v_nn_noisy_classifier_0.1.pdf}
    \includegraphics[width=0.48\textwidth]{figs/compare/v_nn_noisy_classifier_0.5.pdf}
    \includegraphics[width=0.48\textwidth]{figs/compare/v_nn_noisy_gradient_0.0001.pdf}
    \includegraphics[width=0.48\textwidth]{figs/compare/v_nn_noisy_gradient_0.1.pdf}
    \includegraphics[width=0.48\textwidth]{figs/compare/v_nn_noisy_gradient_0.5.pdf}
    \vspace{-5mm}
    \caption{Comparison of MSE on regressing a two layer neural network with explanations as a noisy classifier (top) and noisy gradients (bottom). $m = 1000, k=20.$ For the iterative methods, $T = 10$. Results are averaged over 5 seeds. $\epsilon$ represents the variance of the noise added to the noisy classifier or noisy gradient.} 
    \label{fig:noisy_nn_full}
\end{figure*}

We observe different trends in performance as we vary the amount of noise in the noisy gradient or noisy classifier explanations. With any amount of noise and sufficient regularization ($\lambda$), this influences the overall performance of the methods that incorporate constraints. With few labeled data, using noisy classifiers helps outperform standard supervised learning. With a larger amount of labeled data, this leads to no benefits (if not worse performance of the Lagrangian approach). However, with the noisy gradient, under small amounts of noise, the restricted class of hypothesis will still capture solutions with low error. Therefore, in this case, we observe that the Lagrangian approach outperforms standard supervised learning in the case with few labeled data and matches it with sufficient labeled data. Our method outperforms or matches both methods across all settings.

We consider another noisy setting, where noise has been added to the weights of a copy of the target two layer neural network. Here, we compare how this information impacts learning from the direct outputs (noisy classifier) or the gradients (noisy gradients) of that noisy copy (Figure \ref{fig:noisy_weights_nn}). 

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figs/noisy_weights/v_nn_noisy_weights_classifier_0.3.pdf}
    \includegraphics[width=0.48\textwidth]{figs/noisy_weights/v_nn_noisy_weights_classifier_0.5.pdf}
    \includegraphics[width=0.48\textwidth]{figs/noisy_weights/v_nn_noisy_weights_classifier_0.7.pdf}
    \includegraphics[width=0.48\textwidth]{figs/noisy_weights/v_nn_noisy_weights_gradient_0.3.pdf}
    \includegraphics[width=0.48\textwidth]{figs/noisy_weights/v_nn_noisy_weights_gradient_0.5.pdf}
    \includegraphics[width=0.48\textwidth]{figs/noisy_weights/v_nn_noisy_weights_gradient_0.7.pdf}
    \vspace{-5mm}
    \caption{Comparison of MSE on regressing a two layer neural network with explanations as a noisy classifier (top) and noisy gradients (bottom). $m = 1000, k=20.$ For the iterative methods, $T = 10$. Results are averaged over 5 seeds. $\epsilon$ represents the variance of the noise added to the noisy classifier or noisy gradient.} 
    \label{fig:noisy_weights_nn}
\end{figure*}

\section{Experimental Details}

For all of our synthetic and real-world experiments, we use values of $m = 1000, k = 20, T = 3, \tau = 0, \lambda=1$, unless otherwise noted. For our synthetic experiments, we use $d = 100, \sigma^2 = 5$. Our two layer neural networks have hidden dimensions of size 10. They are trained with a learning rate of 0.01 for 50 epochs. We evaluate all networks on a (synthetic) test set of size 2000.

For our real-world data, our two layer neural networks have a hidden dimension of size 10 and are trained with a learning rate of 0.1 (YouTube) and 0.1 (Yelp) for 10 epochs. $\lambda = 0.01$ and gradient values computed by the smoothed approximation in \citep{sam2022losses} has $c = 1$. Test splits are used as follows from the YouTube and Yelp datasets in the WRENCH benchmark \citep{zhang2021wrench}.

We choose the initialization of our variational algorithm $h_0$ as the standard supervised model, trained using gradient descent.

\clearpage


\section{Ablations} \label{appx:ablation}

We also perform ablation studies in the same regression setting as Section \ref{experiments}. We vary parameters that determine either the experimental setting or hyperparameters of our algorithms. 

\subsection{Number of Explanation-annotated Data}
First, we vary the value of $k$ to illustrate the benefits of our method over the existing baselines. 

\begin{figure*}[h]
    \centering
    \vspace{-4mm}
    \includegraphics[width=0.48\textwidth]{figs/explain_data/v_nn_k20.pdf}
    \includegraphics[width=0.48\textwidth]{figs/explain_data/v_nn_k40.pdf}
    \includegraphics[width=0.48\textwidth]{figs/explain_data/v_nn_k60.pdf}
    \includegraphics[width=0.48\textwidth]{figs/explain_data/v_nn_k80.pdf}
    \includegraphics[width=0.48\textwidth]{figs/explain_data/v_nn_k100.pdf}
    \includegraphics[width=0.48\textwidth]{figs/explain_data/v_nn_k200.pdf}
    \vspace{-2mm}   
    \caption{Comparison of MSE on regressing a two layer neural network over different amounts of explanation-annotated data $k$. $m = 1000.$ For the iterative methods, $T = 10$. Results are averaged over 5 seeds.} 
    \label{fig:ablation_k}
\end{figure*}

We observe that our variational approach performs much better than a simple augmented Lagrangian method, which in turn does better than supervised learning with sufficiently large values of $k$. Our approach is always better than the standard supervised approach. 

We also provide results for how well these methods satisfy these explanations over varying values of $k$. 

\begin{figure*}[h]
    \centering
    \vspace{-4mm}
    \includegraphics[width=0.48\textwidth]{figs/explain_data_constraint/v_nn_k20.pdf}
    \includegraphics[width=0.48\textwidth]{figs/explain_data_constraint/v_nn_k50.pdf}
    \vspace{-4mm}
    % \includegraphics[width=0.49\textwidth]{figs/explain_data_constraint/v_nn_k40.pdf}
    % \includegraphics[width=0.49\textwidth]{figs/explain_data_constraint/v_nn_k60.pdf}
    % \includegraphics[width=0.49\textwidth]{figs/explain_data_constraint/v_nn_k80.pdf}
    \includegraphics[width=0.48\textwidth]{figs/explain_data_constraint/v_nn_k100.pdf}
    \includegraphics[width=0.48\textwidth]{figs/explain_data_constraint/v_nn_k200.pdf}
    \vspace{-4mm}
    \includegraphics[width=0.48\textwidth]{figs/explain_data_constraint/v_nn_k500.pdf}
    \includegraphics[width=0.48\textwidth]{figs/explain_data_constraint/v_nn_k1000.pdf}
    % \vspace{-2mm}   
    \caption{Comparison of Input Gradient Distance when regressing a two layer neural network over different values of $k$. $m = 1000, T = 10.$ Results are averaged over 5 seeds.} 
    \vspace{-3mm}
    \label{fig:ablation_k_constraint}
\end{figure*}

\subsection{Simpler Teacher Models Can Maintain Good Performance}

As noted before, we can use \textit{simpler} teacher models to be regularized into the explanation-constrained subspace. This can lead to overall easier optimization problems, and we synthetically verify the impacts on the overall performance. In this experimental setup, we are regressing a two layer neural network with a hidden dimension size of 100, which is much larger than in our other synthetic experiments. Here, we vary over simpler teacher models by changing their hidden dimension size.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.49\textwidth]{figs/v_nn_hidden.pdf}
    \vspace{-3mm}
    \caption{Comparison of MSE on regressing a two layer neural network over simpler teacher models (hidden dimension). Here, $k = 20, m = 1000, T = 10$. Results are averaged over 5 seeds.} 
    \vspace{-4mm}
    \label{fig:ablation_teacher}
\end{figure}

We observe no major differences as we shrink the hidden dimension size by a small amount. For significantly smaller hidden dimensions (e.g., 2 or 4), we observe a large drop in performance as these simpler teachers can no longer fit the approximate projection onto our class of EPAC models accurately. However, slightly smaller networks (e.g., 6, 8) can fit this projection as well, if not better in some cases. This is a useful finding, meaning that our teacher can be a \textit{smaller model} and get comparable results, showing that this simpler teacher can help with scalability without much or any drop in performance.

\subsection{Number of Unlabeled Data}

As a main benefit of our approach is the ability to incorporate large amounts of unlabeled data, we provide a study as we vary the amount of unlabeled data $m$ that is available. When varying the amount of unlabeled data, we observe that the performance of self-training and our variational objective improves at similar rates. 

\begin{figure*}[h]
    \centering
    \vspace{-2mm}
    % \includegraphics[width=0.48\textwidth]{figs/unlabeled_data/v_nn_m50.pdf}
    \includegraphics[width=0.48\textwidth]{figs/unlabeled_data/v_nn_m100.pdf}
    \includegraphics[width=0.48\textwidth]{figs/unlabeled_data/v_nn_m250.pdf}
    \vspace{-4mm}
    \includegraphics[width=0.48\textwidth]{figs/unlabeled_data/v_nn_m500.pdf}
    \includegraphics[width=0.48\textwidth]{figs/unlabeled_data/v_nn_m1000.pdf}
    % \vspace{-2mm}
    \caption{Comparison of MSE on regressing a two layer neural network over different values of $m$. $k = 20, T = 10.$ Results are averaged over 5 seeds.} 
    \vspace{-2mm}
    \label{fig:ablation_m}
\end{figure*}

\clearpage

\subsection{Data Dimension}

We also provide ablations as we vary the underlying data dimension $d$. As we increase the dimension $d$, we observe that the methods seem to achieve similar performance, due to the difficulty in modeling the high-dimensional data. Also, here gradient information is much harder to incorporate, as the input gradient itself is $d$-dimensional, so we do not see as much of a benefit of our approach as $d$ grows. 

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figs/data_dimension/v_nn_d25.pdf}
    \includegraphics[width=0.48\textwidth]{figs/data_dimension/v_nn_d50.pdf}
    \includegraphics[width=0.48\textwidth]{figs/data_dimension/v_nn_d75.pdf}
    \includegraphics[width=0.48\textwidth]{figs/data_dimension/v_nn_d100.pdf}
    \caption{Comparison of MSE on regressing a two layer neural network over different underlying data dimensions $d$. $m = 1000, k = 20.$ For the iterative methods, $T = 10$. Results are averaged over 5 seeds.} 
    \label{fig:ablation_d}
\end{figure*}

\clearpage

\subsection{Hyperparameters}

First, we compare the different approaches over different values of regularization ($\lambda)$ towards satisfying the explanation constraints. Here, we compare the augmented Lagrangian approach, the self-training approach, and our variational approach.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figs/regularization/v_nn_lamb0.1.pdf}
    \includegraphics[width=0.48\textwidth]{figs/regularization/v_nn_lamb1.pdf}
    \includegraphics[width=0.48\textwidth]{figs/regularization/v_nn_lamb10.pdf}
    \includegraphics[width=0.48\textwidth]{figs/regularization/v_nn_lamb100.pdf}
    \caption{Comparison of MSE on regressing a two layer neural network over different values of $\lambda$. $m = 1000, k = 20.$ For the iterative methods, $T = 10$. Results are averaged over 5 seeds.} 
    \label{fig:ablation_hyp}
\end{figure*}

We observe that there is not a significant trend as we change the value of $\lambda$ across the different methods. Since we know that our explanation is perfect (our restricted EPAC class contains the target classifier), increasing the value of $\lambda$ should help, until this constraint is met.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figs/v_nn_T.pdf}
    \includegraphics[width=0.48\textwidth]{figs/v_nn_t.pdf}
    \caption{Comparison of MSE on regressing a two layer neural network over different values of $T$ (left) and $\tau$ (right) in our variational approach. $m = 1000, k = 20, \tau = 10, T = 10$, unless noted otherwise. Results are averaged over 5 seeds.} 
    \label{fig:ablation_ts}
\end{figure*}

Next, we compare different hyperparameter settings for our variational approach. Here, we analyze trends as we vary the values of $T$ (number of iterations) and $\tau$ (threshold before adding hinge penalty). We note that the value of $\tau$ does not significantly impact the performance of our method while increasing values of $T$ seems to generally benefit performance on this task.


