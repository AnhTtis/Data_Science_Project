
\documentclass[11pt]{article} % For LaTeX2e
\usepackage[a4paper, margin = 1in]{geometry}

\usepackage[hidelinks]{hyperref}
\usepackage{url}
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage[sort&compress,square,comma,authoryear]{natbib}
% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}


%\usepackage[tight]{subfigure}
\usepackage{graphicx}
\usepackage{appendix}

%\usepackage[algo2e,ruled,vlined]{algorithm2e}
%\setlength{\Algomargin}{-0.05em}
\usepackage{mdwlist}
\usepackage{xspace}
%\usepackage{enumitem}
\usepackage{color}
\usepackage{mathrsfs}

\RequirePackage{algorithm}
\RequirePackage{algorithmic}


\usepackage{booktabs}
\usepackage{comment}
\usepackage{geometry}

\usepackage{multirow}

\newcommand{\ind}{\mathbb{I}}

% Functions using mathrm
\renewcommand{\dim}{\mathrm{dim}}
\newcommand{\OPT}{\textup{\textsf{OPT}}}
\newcommand{\range}{\mathcal{range}}
\newcommand{\sign}{\textup{\textsf{sign}}}
\newcommand{\sgn}{\textup{\textsf{sign}}}
\newcommand{\diag}{\textsf{Diag}}
\newcommand{\ber}{\textup{\textsf{Ber}}}
\newcommand{\err}{\mathrm{err}}
\newcommand{\adv}{\mathrm{adv}}
\newcommand{\nat}{\mathrm{nat}}
\newcommand{\greedy}{\mathrm{greedy}}
\newcommand{\opt}{\mathrm{opt}}
\newcommand{\abstain}{\mathrm{abstain}}
\newcommand{\gen}{(\frac{\nu}{12})}
\newcommand{\error}{\mathrm{err}}
\newcommand{\hinge}{\mathrm{hinge}}
\newcommand{\minimax}{\mathrm{minimax}}
\newcommand{\boundary}{\mathrm{DB}}
\newcommand{\erf}{\mathrm{erf}}
\newcommand{\ERM}{\mathrm{ERM}}
\newcommand{\Appendix}[1]{the full version for}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

%\newcommand{\fix}{\marginpar{FIX}}
%\newcommand{\new}{\marginpar{NEW}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{claim}{Claim}
\newtheorem{fact}{Fact}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{condition}{Condition}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\renewcommand{\a}{\mathbf{a}}
\renewcommand{\b}{\mathbf{b}}
\renewcommand{\c}{\mathbf{c}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\g}{\mathbf{g}}
\renewcommand{\u}{\bm{u}}
\renewcommand{\v}{\mathbf{v}}
\newcommand{\w}{\bm{w}}
\newcommand{\x}{\bm{x}}
\newcommand{\y}{\bm{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\C}{\mathbf{C}}
\newcommand{\D}{\mathbf{D}}
% \newcommand{\E}{\mathbf{E}}
\newcommand{\F}{\mathbf{F}}
\newcommand{\G}{\mathbf{G}}
\renewcommand{\H}{\mathbf{H}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\K}{\mathcal{K}}
\renewcommand{\L}{\mathbf{L}}
\newcommand{\M}{\mathbf{M}}
\newcommand{\N}{\mathcal{N}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\S}{\mathbf{S}}
\newcommand{\T}{\mathbf{T}}
\newcommand{\U}{\mathbf{U}}
\newcommand{\V}{\mathbf{V}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\rank}{\textup{\textsf{rank}}}
\newcommand{\orthc}{\mathbf{orth}_c}
\newcommand{\orthr}{\mathbf{orth}_r}
\newcommand{\bLambda}{\mathbf{\Lambda}}
\newcommand{\RS}{\mathcal{R}}
\newcommand{\0}{\mathbf{0}}

\newcommand{\1}{\mathbf{1}}
\renewcommand{\comment}[1]{}
\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\blue}[1]{{\color{blue}#1}}
\newcommand{\tr}{\textsf{tr}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cZ}{\mathcal{Z}}
\newcommand{\bbB}{\mathbb{B}}
\newcommand{\bbI}{\mathbb{I}}
\newcommand{\bbE}{\mathbb{E}}
\newcommand{\bbS}{\mathbb{S}}
\newcommand{\Pro}{\text{Pro}}
\newcommand{\imperceptible}{\mathsf{imperceptible}}
\newcommand{\dist}{\mathsf{dist}}
\newcommand{\spann}{\mathsf{span}}
\newcommand{\vol}{\mathsf{vol}}
\newcommand{\Null}{\mathsf{null}}
\newcommand{\Area}{\mathsf{Area}}

\definecolor{colorY}{rgb}{0.7 , 0.7 , 0.2}

\newenvironment{proofoutline}{\noindent{\emph{Proof Sketch. }}}{\hfill$\square$\medskip}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\linepenalty=1000


\title{\Huge{Learning with Explanation Constraints}}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Rattana Pukdee$^{1}$\footnote{Equal contribution} , Dylan Sam$^{1*}$, J. Zico Kolter$^{1,2}$,\\ Maria-Florina Balcan$^{1}$, Pradeep Ravikumar$^{1}$ \\
\quad\\
$^{1}$Carnegie Mellon University\\
$^{2}$Bosch Center for AI
% Pittsburgh, PA 15213, USA \\
% \texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
}
\date{}
% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

% \newcommand{\fix}{\marginpar{FIX}}
% \newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle


\begin{abstract}

While supervised learning assumes the presence of labeled data, we may have prior information about how models should behave. In this paper, we formalize this notion as learning from \emph{explanation constraints} and provide a learning theoretic framework to analyze how such explanations can improve the learning of our models. For what models would explanations be helpful? Our first key contribution addresses this question via the definition of what we call EPAC models (models that satisfy these constraints in expectation over new data), and we analyze this class of models using standard learning theoretic tools. Our second key contribution is to characterize these restrictions (in terms of their Rademacher complexities) for a canonical class of explanations given by gradient information for linear models and two layer neural networks. Finally, we provide an algorithmic solution for our framework, via a variational approximation that achieves better performance and satisfies these constraints more frequently, when compared to simpler augmented Lagrangian methods to incorporate these explanations. We demonstrate the benefits of our approach over a large array of synthetic and real-world experiments.

\end{abstract}

\section{Introduction}

% Supervised learning is studied in classical PAC learning theory \citep{valiant1984theory}, where the goal is to learn a concept $h\in \cH$ from a set of labeled examples. In practice, we usually have more information than labeled examples to train machine learning models (e.g., unlabeled examples or domain knowledge). In practice, machine learning models benefit from incorporating such inductive biases in developing new training techniques, such as structured risk minimization \citep{shawe1998structural}, self-supervised learning \citep{chen2020simple}, etc. Prior works have provided learning theoretic frameworks to capture the benefits of these alternative sources of information, particularly in semi-supervised settings \citep{balcan2010discriminative} and in settings with pretrained representations \citep{garg2020functional}. 
There has been a considerable recent focus on generating explanations of complex black-box models so that humans may better understand their decisions. But what if humans were able to provide explanations for how these models should behave? We are interested in the question of how to learn models given such apriori explanations. We believe that learning from explanations is a natural characterization for training machine learning models as it matches how humans learn. For example, we learn math much better and more efficiently from a (good) teacher, who can explain the underlying principles and rules. As labeled examples are provided by domain experts, we can also ask them to provide explanations for their decisions. While this indeed requires effort from the domain expert, these explanations can significantly improve the standard learning process, reducing the required number of labeled data. 

% In this paper, we provide an analytical framework for learning from explanations. We consider local explanations functionals $g$, which take in the hypothesis/model $h$, input $x$, and outputs the explanation $g(h,x)$. Suppose we have domain knowledge that consists of constraints on such explanations. We can leverage these to then solve a constrained ERM problem where we additionally constrain the model/hypothesis to satisfy these explanation constraints. From an analysis standpoint, this poses challenges since these constraints are random (since the explanations and their constraints are provided on randomly sampled inputs).
% To handle these stochastic constraints, we draw from classical approaches in stochastic programming \citep{kall1994stochastic, birge2011introduction}. In particular, we formalize the class of what we term \emph{EPAC} models, which satisfies the explanation constraints in expectation up to some slack with high probability, where the probability is with respect to the randomness of the models themselves. The high level idea is that any model that satisfies the set of explanation constraints on the finite sample can, via standard statistical learning theoretic arguments, can be shown to satisfy the constraints in expectation up to some slack with high probability. Before doing so, we first restate these explanation constraints in terms of surrogate losses $\phi(h,x)$ that quantify the extent to which the model/hypothesis satisfies any constraints. As one natural notion, drawing from classical work on surrogates of zero-one binary classification losses, we could use binary classification surrogate losses as upper bounds on the zero-one indicator function for whether the explanation is not satisfied. We note that in certain contexts, it might even be more natural to directly specify the explanation constraint using such a surrogate loss.

In this paper, we provide an analytical framework for learning from explanations. We first provide a mathematical framework for model constraints given explanations. Casting explanations as functionals $g$ that take in a model $h$ and input $x$, we can represent domain knowledge of how models should behave as constraints on the values of such explanations. We can leverage these to then solve a constrained ERM problem where we additionally constrain the model to satisfy these explanation constraints. From an analysis standpoint, this poses challenges as these constraints are random; the explanations and constraints are provided on randomly sampled inputs. To handle these stochastic constraints, we draw from classical approaches in stochastic programming \citep{kall1994stochastic, birge2011introduction}. In particular, we formalize the class of what we term \emph{EPAC} models, or models that satisfy the explanation constraints (in expectation) up to some slack with high probability. Here, the probability is with respect to the randomness of the models themselves. The high level idea is that any model that satisfies the set of explanation constraints on the finite sample can be shown, via standard statistical learning theoretic arguments \citep{valiant1984theory}, to satisfy the constraints in expectation up to some slack with high probability. 
% Before doing so, we first restate these explanation constraints in terms of surrogate losses $\phi(h,x)$ that quantify the extent to which the model/hypothesis satisfies any constraints. As one natural notion, drawing from classical work on surrogates of zero-one binary classification losses, we could use binary classification surrogate losses as upper bounds on the zero-one indicator function for whether the explanation is not satisfied. We note that in certain contexts, it might even be more natural to directly specify the explanation constraint using such a surrogate loss.
Then, we can capture the benefit of learning with explanation constraints by analyzing the generalization capabilities of this restricted class of EPAC models. This analysis builds off of a learning theoretic framework for semi-supervised learning \citep{balcan2010discriminative}. 

\begin{figure*}[t]
\centering
\vspace{-2mm}
\includegraphics[width=0.98\textwidth]{figs/bean_combine.pdf}
\vspace{-4mm}
\caption{A restricted hypothesis class $\cH_{\phi, \tau}$ (left). Our algorithmic solution to solve a proposed variational objective in Section \ref{sec:variational_algo} (right).}
\vspace{-3mm}
\end{figure*}

Another key contribution of our work is concretely analyzing this framework for a canonical class of explanation constraints given by gradient information for linear models and two layer neural networks. We focus on gradient constraints as we can represent many different notions of explanations, such as feature importance and ignoring background/spurious features as a (noisy) gradient constraint. These corollaries clearly illustrate that restricting the hypothesis class via explanation constraints can lead to fewer required labeled data. We also discuss when learning these explanation constraints makes sense or is possible (i.e., with a finite generalization bound). 

Now that we have provided a learning theoretic framework for these explanation constraints, we next consider the algorithmic question: how do we solve for these explanation-constrained models? In general, these constraints are not necessarily well-behaved and are difficult to optimize. We draw from seminal work in posterior regularization \citep{ganchev2010posterior}, which has also been studied in the capacity of model distillation \citep{hu2016harnessing}, to provide a variational objective. Our first algorithmic ingredient is the use of surrogate explanation losses that quantify how well a model satisfies an explanation constraint. Our second algorithmic ingredient relates to the fact that constrained model estimation is much less scalable in general than unconstrained estimation. One can use augmented Lagrangian approaches \citep{ross2017right, fioretto2021lagrangian}, or simply regularized versions of our constrained problems \citep{rieger2020interpretations} (which however do not in general solve the constrained problems for non-convex parameterizations). However, even these pose challenges for complex models and increasingly complex explanations (where even simple instances of the latter can involve the model's Jacobian). 
% In particular, note that computing gradients with respect to model parameters would entail differentiating through explanation surrogate loss and the explanation functional.
% , but this does not necessarily match the analytical framework from above, and potentially requires more complicated techniques in optimization regularization strength \citep{balcan2010discriminative}. 

We propose a tractable alternative via a variational objective that iteratively trains a model on the supervised data, and then approximately projects this learnt model onto the set of those hypotheses that satisfy the explanation constraints. Finally, we provide an extensive array of experiments that capture the benefits of learning from explanation constraints. These experiments clearly illustrate our generalization bounds and also reveal fundamental tradeoffs about the design of explanation constraints. These experiments also demonstrate that the variational approach improves over simpler augmented Lagrangian approaches and can lead to models that indeed satisfy explanations more frequently.



\subsection{Related Work}


Recent advances in deep learning have led to models that achieve high performance but which are also highly complex \citep{lecun2015deep, goodfellow2016deep}. Understanding these complex models is crucial for safe and reliable deployments of these systems in the real-world. One approach to improve our understanding of a model is through explanations. This can take many forms such as feature importance \citep{ribeiro2016should, smilkov2017smoothgrad, lundberg2017unified,sundararajan2017axiomatic}, high level concepts \citep{kim2018interpretability,yeh2020completeness}, counterfactual examples \citep{wachter2017counterfactual,goyal2019counterfactual, mothilal2020explaining}, or influential training samples~\citep{koh2017understanding,yeh2018representer}.
% Once we generate explanations, human expert may decide whether the model is correct or not. These explanations can help us detect bugs [cite model debugging], provide a suggestion [counterfactual example] and build our trust with a model. Connecting explaination with use cases is an active line of research \cite{chen2022interpretable}. 

In contrast to generating post-hoc explanations of a given model, we aim to learn models given apriori explanations. There has been some recent work along such lines.  \citet{koh2020concept, zarlenga2022concept} incorporates explanations within the model architecture by requiring a conceptual bottleneck layer. \citet{ross2017right, rieger2020interpretations}  use explanations to modify the learning procedure for any class of models: they incorporate explanations as a regularizer, penalizing models that do not exhibit apriori given explanations; \citet{ross2017right} penalize input gradients, while \citet{rieger2020interpretations} penalize a Contextual Decomposition score \citep{murdoch2018beyond}. Some of these suggest that constraining models via explanations leads to higher accuracies and more robustness to spurious correlation, but do not provide analytical guarantees. On the theoretical front, \citet{li2020learning} show that models that are easier to explain locally also generalize well. However, \citet{bilodeau2022impossibility} show that common feature attribution methods without additional assumptions on the learning algorithm or data distribution do no better than random guessing at inferring counterfactual model behavior.

Our contribution is to provide an analytical framework for learning from explanations that quantifies the benefits of explanation constraints. Our analysis is closely related to the framework of learning with side information. \citet{balcan2010discriminative} shows how unlabeled data can help in semi-supervised learning through a notion of compatibility between the data and the target model. This seminal work studies classical notions of side information (e.g., margin, smoothness, and co-training). 
Subsequent papers have adapted this learning theoretic framework to study the benefits of representation learning \citep{garg2020functional} and transformation invariance \citep{shao2022a}. On the contrary, our paper focuses on the more recent notion of explanations. Rather than focus on the benefits of unlabeled data, we characterize the quality of different explanations.


% provide concrete examples for explanations given by gradient information and noisy classifiers.
% studied in the context of semi-supervised learning . The seminal work studies the case when the side information is in the form of margin, smoothness and co-training. Subsequent papers have adapted this framework to study representation learning \citep{garg2020functional} and transformation invariance \citep{shao2022a}.

% In our paper, we focus on the recent notion of explanations.


% After this work, subsequent papers have adapted this framework to handle particular inductive biases, which we consider as specific instantiations of our setting. This includes \citep{shao2022a}, which studies this for transformation invariance, and \citep{garg2020functional} for representation learning.  


% We also remark that our framework is quite general and can handle constraints beyond those considered in the explainable AI community. For example, this is an immediate generalization of the framework for handling side information in semi-supervised learning \citep{balcan2010discriminative}, including notions of smoothness \citep{xiaojin2002learning, pukdee2022label}, transformation invariance \citep{shao2022a}, and representation learning \citep{garg2020functional}. It can also analyze cases of other real-world constraints, including that of physics rules \citep{NEURIPS2018_842424a1}.




 % However, there haven't been any theoretical guarantee on why and how explanation constraints can help us. Recent theoretical works have been focusing on axiomatic approach to explanations to propose explanations with different desirable property \citep{yeh2019fidelity}[cite chih kuan/ pradeep/ cite ameet and gregory plumb's paper] since it is known that many well-known explanations can have bad properties [cite adebayo]; saliency map of an untrained model looks similar to the trained one. There have been works connecting learning theory with explanation ML suggesting that  In this work, we hope to quantify the benefits of learning explanation constraints through the lense of learning theory.

There is also prior work proposing learning objectives that incorporate rules into deep neural networks \citep{hu2016harnessing, fioretto2021lagrangian, seo2021controlling}. While \citep{hu2016harnessing} also leverages variational objectives, their method specifically concerns itself with logic rules and over probability distributions using KL divergence projections. 
% On the contrary, our approach handles more general forms of explanations and provides a much more flexible approach in projecting a model to an explanation-constrained subspace. 
On the contrary, our approach handles more general forms of explanations and that naturally conforms to our theoretical framework. 
% We leverage variational characterizations to scalably learn complex models under explanation constraints. 
Our work can also be connected to the self-training literature \citep{chapelle2009semi,xie2020self,wei2020theoretical, frei2022self}, where we could view our variational objective as comprising a regularized (potentially simpler) teacher model that encodes these explanation constraints into a student model.


% \textbf{Semi-supervised Learning Theory}

% Seminal work has provided a learning theoretic framework for analyzing various inductive biases from semi-supervised learning \citep{balcan2010discriminative}. We draw inspiration from this work, and extend this more generally to the notion of explanation constraints. These common inductive biases (e.g., max-margin, regularization, smoothness) all fall within our notion of an explanation and, thus, fit into our learning framework. This approach also provides more classical algorihtmic approaches to solving this method, which does not transfer well to the training of more complex architectures, such as deep neural networks. 

% After this work, subsequent papers have adapted this framework to handle particular inductive biases, which we consider as specific instantiations of our setting. This includes \citep{shao2022a}, which studies this for transformation invariance, and \citep{garg2020functional} for representation learning.  


% \textbf{Rules in Deep Learning}



% \textbf{Types of Explanations}

% \begin{enumerate}
%     \item Gradient: Many existing works \citep{ross2017right, sam2022losses} have studied the benefits of using input gradient information (i.e., how models make decisions) in the training of models.
%     \item Physics: Other approaches incorporate ground truth rules borrowed from Physics \citep{} to improve the approaches of neural networks. This can again be captured in our framework for explanation constraints. 
%     \item Invariances
%     \item Weak Supervision
% \end{enumerate}



% \begin{table*}[t]
% \centering
% \begin{tabular}{p{0.2\linewidth} | p{0.15\linewidth} |  p{0.65\linewidth}}
% \toprule
% Example & Explanation $g$  & Description \\ \midrule
% Gradient & {$\nabla_x h(x)$}    & {In many instances, we may know information about how particular classifiers should make decisions. This can be translated to constraints or knowledge about the target function's gradients. This has been introduced in existing work \citep{ross2017right, rieger2020interpretations}. In many cases, knowledge of the exact gradient is too strong; instead, we may know smoothed or noisy versions of the gradient \citep{, sam2022losses}} \\ \midrule

% Noisy Classifier &{$h(x)$}  & {We are also frequently given noisy laels from a classifier that can also abstain from making predictions, which occurs in settings such as weakly supervised learning \citep{snorkel}. Here, the label is provided by hand-engineered logic rules or by pretrained models.}  \\ \midrule

% Shapley Values & & \\
% \bottomrule
% % Smoothness &{$d(h(x_1), h(x_2))$}    &  {Another common form of inductive bias is that networks should behave similarly on nearby points. This has been conceptualized in semi-supervised learning by methods, including label propagation \citep{xiaojin2002learning, pukdee2022label} and graph neural networks \citep{welling2016semi}.}\\ \midrule
% % Transformation Invariance & {$ d(h(T(x)), h(x))$ }  & {Approaches in computer vision have trained networks to be invariant to some transformation $T$ over inputs. This is a common approach in self-supervised \citep{chen2020simple, foster2020improving}, and this can be thought of a soft constraint on our hypothesis class to classifiers that exhibit these invariance properties. } \\ \midrule
% % Pretrained representation & {$d(h_{\operatorname{pre}(x)}, h(x))$} &{When our classfier is in term of $f \circ h$ when $h$ is a representation function. In transfer learning, we can initialize a representation function $h$ with $h_{\operatorname{pre}}$, a representation function learned on a different task.}\\ \midrule
% % Physics Rules & ??? & Existing work in deep learning has integrated physics rules to improve and correct the decision-making of deep learning systems. One particular work integrates a differentiable physics simulator into a deep learning architecture \citep{NEURIPS2018_842424a1}.   \\
% \end{tabular}
% \caption{Examples of explanations}
% \label{table: examples of explanation}
% \end{table*}

% \section{Preliminaries}





\section{Learning from Explanation Constraints}

Let $\cX$ be the instance space and $\cY$ be the label space. We focus on binary classification where $\cY = \{-1,1\}$, but which can be naturally generalized.  Let $\cD$ be the joint data distribution over $(\cX, \cY)$ and $\cD_\cX$ the marginal distribution over $\cX$. For any classifier $h : \cX \to \cY$, we are interested in its classification error $\err(h): = \Pr_{(x,y) \sim D}(h(x) \neq y)$, though one could also use other losses to define classification error. Our goal is to learn a classifier with small error from a family of functions $\cH$. We draw from the explainable machine learning literature, and formalize (what are known as local) explanations as functionals that take in a model and test input, and output a vector:
%
\begin{definition}
[Explanations]
Given an instance space $\cX$,  model hypothesis class $\cH$, and an explanation functional $g: \cH \times \cX \to \R^r $, we say $g(h,x)$ is an explanation of $h$ on point $x$ induced by $g$. 
\end{definition}

In our notation, $g$ represents a functional of interest on a classifier $h \in \cH$. For simplicity, we consider the setting when $g$ takes a single data point and model as input, but this can be naturally extended to multiple data points and models.
In practice, we need to combine these explanations with additional human knowledge on how explanations at particular sample points should look like. This can naturally be expressed in the form of explanation constraints.
\begin{definition}
[Explanation Constraint Set]
For any instance space $\cX$, hypothesis class $\cH$, an explanation functional $g:\cH \times \cX \to \R^r$, and a family of constraint sets $\{C(x) \subseteq \R^r \mid x \in \cX\}$, we say that $h \in \cH$ satisfies the explanation constraints with respect to $C$ iff:
\begin{equation*}
    g(h,x) \in C(x), \; \forall x \in \cX.
\end{equation*}


\end{definition}
In our definition, $C(x)$ represents values that we believe our explanations should take at a point $x$. For example, ``an input gradient of a feature 1 must be larger than feature 2'' can be represented by $g(h,x) = \nabla_x h(x)$ and $C(x) = \{(x_1,\dots, x_d) \in \R^d \mid x_1 > x_2\}$. Note that we might not have access to this constraint set $C(x)$ for each of the inputs $x \in \cX$. In practice, human annotators will be able to provide such explanation constraints for a random sample of say $k$ data points $S_E = \{ x'_1,\dots, x'_k\}$, which we assume will be drawn i.i.d. from $\cD_\cX$. We then say that any $h \in \cH$ $S_E$-satisfies the explanation constraints with respect to $C$ iff:
\begin{equation*}
    g(h,x) \in C(x), \; \forall x \in S_E.
\end{equation*}
It can be seen that the constraints \emph{are random} since samples $x'_i \in S_E$ are drawn i.i.d. from $\cD_\cX$. Thus, even if $h$ 
$S_E$-satisfies the explanation constraints with respect to $C$, it likely does not hold that $h$ fully satisfies the explanation constraints with respect to $C$ i.e. for all inputs $x \in \cX$. Here, we can draw from stochastic programming as well as standard learning theoretic arguments to reason about probably approximately satisfying the constraints in expectation. Before doing so we wish to first consider the notion of explanation surrogate losses, which will allow us to generalize the setup above to a form that is amenable to practical estimators.

\begin{definition}
(Explanation surrogate loss)
An explanation surrogate loss $\phi: \cH \times \cX \to \R$ quantifies how well a model $h$ satisfies the explanation constraint $g(h,x) \in C(x)$, so that for any $h \in \cH, x \in \cX$:
\begin{enumerate}
    \item $\phi(h,x) \geq 0$.
    \item If $g(h,x) \in C(x)$ then $\phi(h,x) = 0$.
\end{enumerate}
\end{definition}
For example, we could define
\begin{equation*}
    \phi(h, x) = 1\{g(h,x) \in C(x)\}.
\end{equation*}

Given such a surrogate loss, we can substitute the explanation constraint that $g(h,x) \in C(x)$ with the surrogate $\phi(h,x) \le 0$.
We now have the machinery to formalize how to reason about the random explanation constraints given a random set of inputs. Consider the class of models that satisfy the explanation constraints with respect to $C$, as mediated by the explanation surrogate loss $\phi$: 
\[\cH_\phi = \{h \in \cH \;:\; \phi(h,x) \le 0, \; \forall x \in \cX\}.\]
And those models that only satisfy the explanation constraints on $S_E$:
\[\cH_{\phi,S_E} = \{h \in \cH \;:\; \phi(h,x) \le 0, \; \forall x \in S_E\}.\]
How do we compare $\cH_{\phi,S_E}$ to $\cH_\phi$? Towards addressing this question, consider the expected explanation loss:
\begin{equation*}
    \phi(h,\cD) := \mathbb{E}_{x \sim \cD}[\phi(h,x)].
\end{equation*}
We can then define the class:
\[\cH_{\phi,\cD,\tau} = \{h \in \cH \;:\; \phi(h,\cD)\le \tau\}.\]
It can be seen that this consists of models that satisfy the explanation constraints up to some slack $\tau$ (i.e. approximately) in expectation. At times, we may suppress the dependence on the data distribution in the notation above, and simply use $\cH_{\phi, \tau}$ to denote this restricted class.
%
We can see than for any $\tau_1 < \tau_2$ we have $\cH_{\phi, \tau_1} \subseteq \cH_{\phi, \tau_2}$ and $\cH_{\phi, \infty} = \cH$ is the original concept class. We further refine this to the class of what we term EPAC models (Explanation constraints Probably Approximately Correct).

\begin{definition}
[EPAC model]
We say that h is a $\tau$ - EPAC model w.r.t. data distribution $\cD$ and a surrogate loss $\phi$ if  $\phi(h,\cD) \le \tau$.
\end{definition}

\begin{definition}
    [EPAC learnability] For any $\delta \in (0,1), \tau > 0$, the sample complexity of $(\delta, \tau)$ - EPAC learning of $\cH$ with respect to a surrogate loss $\phi$, denoted $\mathcal{M}(\tau, \delta; \cH, \phi)$ is defined as the smallest $m\in \mathbb{N}$ for which there exists a learning rule $\cA$ such that every data distribution $\cD_\cX$ over $\cX$, with probability at least $1-\delta$ over $S\sim \cD^m$,
$$
\phi(\cA(S), \cD) \leq \inf_{h \in \cH} \phi(h, \cD) + \tau.    
$$
If no such $m$ exists, define $\mathcal{M}(\tau, \delta; \cH, \phi) = \infty$. We say that $\cH$ is EPAC learnable in the agnostic setting with respect to a surrogate loss $\phi$ if $\forall \delta \in (0,1), \tau > 0$,  $\mathcal{M}(\tau, \delta; \cH, \phi)$ is finite.
\end{definition}

% \begin{definition}
% We say that h is an $(\tau,\delta)$ EPAC model w.r.t. data distribution $\cD$ iff, with probability at least $(1-  \delta)$ (where the probability is with respect to internal randomization in $h$), $\phi(h,\cD) \le \tau$.
% \end{definition}
One might wonder if a model that satisfies the random constraints in $S_E$ might also be an EPAC model. In the proposition below, we use natural statistical learning theoretic arguments to show that that indeed is the case.

\begin{proposition}
Suppose a model $h$ $S_E$-satisfies the explanation constraints so that $h \in \cH_{\phi,S_E,\tau}$. Then, with probability at least $1 - \delta$:
$$\phi(h,\cD_\cX) \le \tau + 2R_k(\cG) + \sqrt{\frac{\ln(4/\delta)}{2k}},$$
when $k = |S_E|$ and  $\cG = \{\phi(h, \cdot)\mid  h \in \cH\}$. 
\end{proposition}
The class $\cG$ contains all surrogate losses of any $h\in \cH$. Depending on the explanation constraints, $\cG$ can be extremely large. The question of which types of  explanation constraints are EPAC learnable might be of independent interest, and we further discuss this in Section \ref{section: EPAC}  and provide further examples in Appendix \ref{appendix: EPAC learnable}. \\

% \subsection{Learning Objective}


\noindent\textbf{EPAC-ERM Objective.}
Let us next discuss \emph{combining} the two sources of information: the explanation constraints that we set up in the previous section, together with the usual set of labeled training samples $S = \{(x_1, y_1), \dots, (x_n, y_n) \}$ drawn i.i.d. from $\cD$  that informs the empirical risk. Combining these, we get what we call EPAC-ERM objective:
\begin{align}
    \min_{h \in \cH} \frac{1}{n}\sum_{i=1}^n \ell(h, x_i,y_i) \;
    \text{ s.t. } \;  \frac{1}{k}\sum_{i=1}^k \phi(h, x'_i) \leq \tau.
\label{eq:epac-erm}
\end{align}


\subsection{Generalization Bound}

% Under this formalization of explanations constraints, we provide a performance guarantee of the minimizer of the mentioned learning objective. At a high level, our analysis will start by only selecting classifiers that has low empirical explanation surrogate loss, and then perform standard supervised learning with the remaining set of classifiers. This follows the analysis from prior work on semi-supervised learning \citep{balcan2010discriminative}. 
% \red{main difference is blacan blum10 also note this in term of unlabeled error, only have a function in term of incompatibility, assume that the compatibility is something intrinsic to the underlying classifier}




% One main challenge in analyzing the benefits of explanations is that they are distribution-dependent.
% We can consider the simple case when the area that we have zero incompatibility $\phi(g(h,x),x)$ is the support of any $\operatorname{supp}(g(\cdot, x)) \subseteq \{a \mid \phi(a,x) = 0\}$ , so adding this constraint does not lead to any benefit. 
% For example, consider learning a linear separator that passes through the origin when data are drawn from the surface of a unit sphere (may not be drawn uniformly). In this case, gradient constraints tell us information on the direction of the optimal linear separator. 
% Given an interval that contains the optimal gradient, one may argue that a narrower interval is a better explanation constraint as we have restricted to a smaller concept class. However, we remark that the benefit of this explanation highly depends on the distribution of data. If all of our data naturally lies in that region restricted by an explanation, then we may gain no benefit from this explanation. 
% This has been noted in prior work that VC dimension analysis that does not take the data distribution into account lead to no performance gain \cite{balcan2010discriminative}. The prior work, instead, utilized techniques from Annealed VC Entropy \citep{Vayatis1999DistributionDependentVB} to deal with this issue. On the contrary, we provide distribution-dependent bounds through the Rademacher complexity in this work. 
% \begin{figure}[ht]
%     \centering
%     \includegraphics[width = 0.7\columnwidth]{pics/data-dependent-exp.jpeg}
%     \caption{ An interval of gradient can be useful (left) and not useful (right) depending on the data distribution.}
%     \label{fig:benefit of an explanation}
% \end{figure}
% In this case, we may want to learn a concept $h$ that satisfy $E$ at all point $x$ that is
% \begin{equation*}
    % g(h,x) = g(h^*,x).
% \end{equation*}
% We hope that such concept $h$ will also be close to the optimal $h^*$. 
% We define a satisfiablity of an explanation $E$ on a concept $h$ as
% \begin{definition}
% [Insatisfiability of $h$ on $E$] For a concept $h$ and an explanation $E(g,C)$ , we define the unsatisfiability as
% \begin{equation*}
%     \operatorname{UNSAT}(h, E(g,c) ) = \Pr_x(g(h,x) \in C(x))
% \end{equation*}
% \end{definition}

% \begin{definition}
% [Distance between 2 concepts] We define a distance between $h_1, h_2 \in \cH$ as
% \begin{equation*}
%     d(h_1,h_2) = \Pr_{x\in\cX}(h_1(x)\neq h_2(x)).
% \end{equation*}
% \end{definition}
% We define a distance between 2 concepts under a $g$ below.
% \begin{definition}
% [Empirical distance between 2 concepts under $g$] Let $S_x = \{x_1, ..., x_m \}$ be the set of examples drawn from space $\cX$. Then, we can define the empirical distance between two concepts with respect to the function $g$ as:
% \begin{equation*}
%     \widehat{d}_g(h_1,h_2) = \sum_{i=1}^m \left(g(h_1,x_i) - g(h_2,x_i)\right)^2
% \end{equation*}
% \end{definition}
% First, we define the restriction of our hypothesis space $\cH$ that has  explanation loss at most $\tau$.

We assume that we are in a doubly agnostic setting. This means that there is no classifier in the hypothesis class $\cH$ that perfectly labels $(x,y)$; instead, we hope to achieve the best error rate in the hypothesis class, $h^* = \arg\min_{h \in \cH} \err_\cD(h)$. This also assumes that $h^*$ may have $ \phi(h^*, D) > 0$ or nonzero surrogate explanation loss. In our analysis, we start by selecting classifiers that have lower empirical explanation risk than a threshold $t$, then perform a standard supervised learning with the remaining set of classifiers.

\begin{theorem}
[Generalization Bound for Agnostic Setting]
Consider a hypothesis class $\cH$, distribution $\cD$, and explanation loss $\phi$. Let $S = \{(x_1, y_1), \dots, (x_n, y_n) \}$ be drawn i.i.d. from $\cD$ and $S_E = \{ x'_1,\dots, x'_k\}$ drawn i.i.d. from $\cD_\cX$. With probability at least $1-\delta$, for $h \in \cH$ that minimizes empirical risk $\err_S(h)$ and has $\phi(h, S_E) \leq \tau$, we have
\begin{equation*}
    \err_D(h) \leq  \err_D(h^*_{\tau - \varepsilon_k}) + 2R_n(\cH_{\phi, \tau + \varepsilon_k}) + 2\sqrt{\frac{\ln(4/\delta)}{2n}},
\end{equation*}
% when 
% \begin{equation*}
%     h_t^* = \arg\min_{h \in \cH_{\phi, t}} \err_\cD(h)
% \end{equation*}
% and
\begin{equation*}
    \varepsilon_k =  2R_k(\cG) + \sqrt{\frac{\ln(4/\delta)}{2k}},
\end{equation*}
when $\cG = \{\phi(h, x) \mid  h \in \cH, x \in \cX\}$ and $h_\tau^* = \arg\min_{h \in \cH_{\phi, \tau}} \err_\cD(h)$. 
\label{thm: generalization bound agnostic}
\end{theorem}
\begin{proof}
The proof largely follows the arguments in \citet{balcan2005pac, balcan2010discriminative}, but we use Rademacher complexity instead of VC-entropy. We defer the full proof to  Appendix \ref{appx:generalization_bound_agnostic}.
\end{proof}
Our bound suggests that these constraints help with our learning by shrinking the hypothesis class $\cH$ to $\cH_{\phi, \tau+ \varepsilon_k}$, reducing the required sample complexity. We can see that a smaller threshold $\tau$ leads to a more restricted hypothesis class and, thus, a larger reduction. However, there is also a trade-off between reduction and accuracy. In our bound, we compare against the best classifier $h^*_{\tau-\varepsilon_k} \in \cH_{\phi,\tau-\varepsilon_k}$ instead of $h^*$. Since we may have $\phi(h^*,\cD) > 0$, if $\tau$ is too small, we may reduce $\cH$ to a hypothesis class that does not contain any good classifiers.
% Since $\phi(h^*,\cD)$ can be greater than $0$, when $t$ is too small, we could reduce $\cH$ to a hypothesis class that does not contain any good classifiers and $\err_\cD(h^*_t) \gg \err_\cD(h^*)$, leading to a weaker guarantee. On the other hand, if $t$ is large enough that $h^* \in \cH_{\phi,t}$ then $h^*_{t - \varepsilon_k} = h^*$ and $\err_\cD(h^*) = \err_\cD(h^*_t)$, the restricted hypothesis class can remain quite large.
% In practice, we only need to select $t$ that is small enough while the hypothesis class $\cH_{\phi,t - \varepsilon_k}$ still contains a good classifier.
 Recall that the generalization bound for standard supervised learning --- in the absence of explanation constraints --- is given by
\begin{equation*}
    \err_D(h) \leq  \err_D(h^*) + 2R_n(\cH) + 2\sqrt{\frac{\ln(2/\delta)}{2n}}.
\end{equation*}

We can see the difference between this upper bound and  the upper bound in Theorem \ref{thm: generalization bound agnostic} here as a possible notion of the goodness of an explanation constraint.

\begin{definition}
[Goodness of an explanation constraint] For a hypothesis class $\cH$, a distribution $\cD$ and an explanation loss $\phi$, the goodness of $\phi$ with respect to a threshold $\tau$ and $n$ labeled examples is given by
\begin{equation*}
    G_{n,\tau}(\phi, \cH) = (R_n(\cH) - R_n(\cH_{\phi, \tau})) + (\err_\cD(h^*) - \err_\cD(h^*_t))
\end{equation*}
\begin{equation*}
    h^* = \arg\min_{h \in \cH} \err_\cD(h), \quad h^*_\tau = \arg\min_{h \in \cH_{\phi,\tau}}\err_\cD(h).
\end{equation*}
\end{definition}
 Here, we assume access to infinite explanation data so that $\varepsilon_k \to 0$. The goodness depends on the number of labeled examples $n$ and a threshold $t$. In our definition, a good explanation constraint leads to a reduction in the complexity of $\cH$ while still containing a classifier with low error. This suggests that the benefits from explanation constraints exhibit diminishing returns as $n$ becomes large. In fact, as $n \to \infty$, we have $R_n(\cH) \to 0, R_n(\cH_{\phi,\tau}) \to 0$ which implies  $G_n(\phi, \cH) \to \err_\cD(h^*) - \err_\cD(h^*_\tau) \leq 0$. On the other hand, explanation constraints help when $n$ is small. For $t$ large enough, we expect $\err_\cD(h^*) - \err_\cD(h^*_\tau)$ to be small, so that our notion of goodness is dominated by the first term: $R_n(\cH) - R_n(\cH_{\phi, \tau})$, which has the simple interpretation of reduction in model complexity.

% However, when the number of labeled examples $n$ is small, the Rademacher complexity of $R_n(\cH_{\phi, t})$ can be much smaller than the complexity of $R_n(\cH)$ and this leads to significant benefits from explanation constraints.

% One limitation of our definition of goodness is that it is difficult to analytically compute the Rademacher complexity.
\section{EPAC Learnable Constraints} \label{section: EPAC}



We know that constraints $C(x)$ capture human knowledge about how explanations at a point $x$ should behave. For any constraints $C(x)$ that are known apriori for all $x\in \cX$, we can evaluate whether a model satisfies the constraints at a point $x\in \cX$. This motivates us to discuss the ability of models to generalize from any finite samples $S_E$ to satisfy these constraints over $\cX$ with high probability. Having access to $C(x)$ is equivalent to knowing how models should behave over \textit{all} possible data points in terms of explanations, which may be too strong of an assumption. Nevertheless, many forms of human knowledge can be represented by a closed-form function $C(x)$. For example,
\begin{enumerate}
    \item An explanation has to take value in a fixed range can be represented by $C(x) = \Pi_{i=1}^r[a_i,b_i], \forall x\in \cX.$
    \item An explanation has to stay in a ball around $x$ can be represented by $C(x) = \{u \in  \R^d \mid ||u-x||_2 \leq r \}$.
    \item An explanation has to stay in a rectangle around $\frac{x}{3}$ can be represented by $C(x) = \{u \in \R^d \mid \frac{x_i}{3} - a_i \leq u_i \leq \frac{x_i}{3} + b_i, i = 1,\dots, d\}$.
\end{enumerate}
\begin{figure}[ht]
    \centering
    \includegraphics[width = 0.6\columnwidth]{figs/eml_explanation_type.pdf}
    \caption{Illustration of examples of explanation constraints, given from some learnable class $C(x)$.}
    % \label{}
\end{figure}

In this case, there always exists a surrogate loss that represents the explanation constraints $C(x)$; for example, we can set  $\phi(h,x) = 1\{g(h,x) \in C(x)\}$. On the other hand, directly specifying explanation constraints through a surrogate loss would also imply that $C(x)$ is known apriori for all $x\in \cX$.  The task of generalization to satisfy the constraint on unseen data is well-defined in this setting. Furthermore, if  a surrogate loss $\phi$ is specified, then we can evaluate $\phi(h,x)$ on any unlabeled data point without the need for human annotators which is a desirable property.

On the other hand, we usually do not have knowledge over all data points $x\in \cX$; rather, we may only know these explanation constraints over a random sample of $k$ data points $S_E = \{x'_1,\dots, x'_k\}$. If we do not know the constraint set $C(x)$, it is unclear what satisfying the constraint at an unseen data point $x$ means. 
% What does it mean by explanation constraints generalization from $S_E$ to $\cX$, if we also don't have access to the rest of $C(x)$? 
Indeed, without additional assumptions, it may not make sense to think about generalization. For example, if there is no relationship between $C(x)$ for different values of $x$, then it is not possible to infer about $C(x)$ from $C(x'_i)$ for $i = 1,\dots, k$. In this case, we could define 
\begin{equation*}
    \phi(h,x) = 1\{g(h,x) \in C(x)\}1\{x \in S_E\},
\end{equation*}
where we are only interested in satisfying these explanation constraints over the finite sample $S_E$. For other data points, we have $\phi(h,x) = 0$. This guarantees that any model with low empirical explanation loss would also achieve loss expected explanation loss, although this does not have any particular implication on any notion of generalization to new constraints. Regardless, we note that our explanation constraints still reduce the size of the hypothesis class from $\cH$ to $\cH_{\phi,\tau}$, leading to an improvement in sample complexity. 

The more interesting setting, however, is when we make an additional assumption that the true (unknown) surrogate loss $\phi$ exists and, during training, we only have access to instances of this surrogate loss evaluated on the sample $\phi(\cdot, x'_i)$. We can apply a uniform convergence argument to achieve
\[ \phi(h,\cD_\cX) \le \phi(h,S_E) + 2R_k(\cG) + \sqrt{\frac{\ln(4/\delta)}{2k}}\]
with probability at least $1 - \delta$ over $S_E$, drawn i.i.d. from $\cD_\cX$ and $\cG = \{\phi(h, \cdot)|h \in \cH\}$,  $k = |S_E|$. Although the complexity term $R_k(\cG)$ is unknown (since $\phi$ is unknown), we can upper bound this by the complexity of a class of functions $\Phi$ (e.g., neural networks) that is large enough to well-approximate any $\phi(h,\cdot) \in \cG$, meaning that $R_k(\cG) \leq R_k(\Phi)$. Comparing to the former case when $C(x)$ is known for all $x\in \cX$ apriori, the generalization bound has a term that increases from $R_k(\cG)$ to $R_k(\Phi)$, which may require more explanation-annotated data to guarantee generalization to new data points. We note that the simpler constraints lead to a simpler surrogate loss, which in turn implies a less complex upper bound $\Phi$. This means that simpler constraints are easier to learn.

Nonetheless, this is a more realistic setting when explanation constraints are hard to acquire and we do not have the constraints for all data points in $\cX$. For example, \citet{ross2017right} considers an image classification task on MNIST, and imposes an explanation constraint in terms of penalizing the input gradient of the background of images. In essence, the idea is that the background should be less important than the foreground for the classification task. In general, this constraint does not have a closed-form expression, and we do not even have access to the constraint for unseen data points. However, if we assume that a surrogate loss $\phi(h,\cdot)$ can be well-approximated by two layer neural networks, then our generalization bound allows us to reason about the ability of  model to generalize and ignore background features on new data. 






% $C$ is a member of some learnable class $\cC$, which now allows us to think about the ability of a model to generalize to \textit{new} explanation constraints. In particular, consider a zero-one surrogate loss
% \begin{equation*}
%     \phi_C(h,x) = 1\{g(h,x) \in C(x)\}.
% \end{equation*}
% The loss $\phi_C$ belongs to a class $\cG_\cC=\{\phi_C(h, \cdot)|h \in \cH, C \in \cC \}$. By a standard uniform convergence argument,  with probability at least $1 - \delta$ over $S_E$ drawn i.i.d. from $\cD_\cX$,
% \[ \phi_C(h,\cD_\cX) \le \phi_C(h,S_E) + 2R_k(\cG_\cC) + \sqrt{\frac{\ln(4/\delta)}{2k}}\]


% ; how many explanation constraints we need to see so that the learned model would also satisfies the constraints on unseen data points. In this particular examples, this means that the model would rely on the background of digits for the classification task.
% We note that an application of active learning to effectively select points to provide explanation constraints is an interesting direction for future work.
\section{Gradient Explanations for Particular Hypothesis Classes}
In this section, we further quantify the usefulness of explanation constraints on different concrete examples and characterize the Rademacher complexity of the restricted hypothesis classes. In particular, we consider an explanation constraint of a constraint on the input gradient. For example, we may want our model's gradient to be close to that of some $h' \in \cH$. This translates to $g(h, x) = \nabla_x h(x)$ and $C(x) = \{ x \in \R^d \: | \: \lVert x - \nabla_x h'(x) \rVert \leq \tau \}$ for some $\tau > 0$. 
% In addition,  we consider a class of linear models and two layer neural networks with bounded weights. Since knowing an exact gradient is enough to identify these models (Appendix \ref{appendix:2nn_algo}), we allow the difference between gradients up to some slack $\tau$.


\subsection{Gradient Explanations for Linear Models}
%  Let $\cD_\cX$ be a uniform distribution over a unit sphere in $\R^d$. Consider a hypothesis class of linear separators that pass through an origin $\cH = \{h | h(x) = \operatorname{sign}( w_h^\top x)\}$ and we want to learn $h^* \in \cH$. We consider explanation constraints in term of gradient information. For any $h\in \cH$, input gradient is given by
%  \begin{equation*}
%     \nabla_x (w^\top_h x) = w_h.
% \end{equation*}
% We can see that knowing an exact gradient of a data point is enough to identify $h$. Therefore, we further assume that we only have an access to a close approximation of a true gradient $w_{h'}$ with 
% \begin{equation*}
%     \theta(w_{h'},w_{h^*}) \leq \tau
% \end{equation*}
% when $\theta(a,b)$ be an angle between two vectors $a, b$.  Assume that $\tau$ is known, we define our explanation loss as
% \begin{equation*}
%     \phi(h,x) =  \theta(w_h, w_{h'}).
% \end{equation*}


We calculate the empirical Rademacher complexity of a linear model under a gradient constraint. 
\begin{figure}[ht]
    \hspace*{1cm}
    \includegraphics[width = 0.6\columnwidth]{figs/linear_f_v2.pdf}
    \caption{Illustration of different value of a function $f(v)$.}
    \label{fig: function f(v)}
\end{figure}
\begin{theorem}
[Empirical Rademacher complexity of linear models with a gradient constraint]
\label{theorem:linear_distribution_free}
Let $\cX$ be an instance space in $\R^d$, let  $\cD_\cX$ be a distribution on $\cX$, let $\cH = \{h: x \to \langle w_h, x \rangle \mid w_h \in \R^d, ||w_h||_2 \leq B\}$ be a class of linear model with weights bounded by some constant $B>0$ in $\ell_2$ norm. Assume that there exists a constant $C>0$ such that $\bbE_{x \sim \cD_\cX}[||x||_2^2] \leq C^2$. Assume that we have an explanation constraint in terms of gradient constraint; we want the gradient of our linear model to be close to the gradient of some linear model $h'$. Let $\phi(h,x) = \theta(w_h, w_{h'})$ be an explanation surrogate loss when $\theta(u,v)$ is an angle between $u,v$. For any $S=\{x_{1}, \dots, x_{n}\}$ is drawn i.i.d. from $\cD_\cX$, we have
$$
R_S(\cH_{\phi,\tau}) = \frac{B}{n} \bbE_{\sigma}\left[\lVert v \rVert f(v)\right].
$$
when $v = \sum_{i=1}^n x_i\sigma_i$ and

\begin{equation*}
    f(v) = \begin{cases}
        1 & \text{ when } \theta(v,w') \leq \tau \\
        \cos(\theta(v,w') - \tau) & \text{ when } \tau \leq \theta(v,w') \leq \frac{\pi}{2} + \tau\\
        0 & \text{ when } \theta(v,w') \geq \frac{\pi}{2} + \tau.
    \end{cases}
\end{equation*}
\end{theorem}
% \begin{proof}
%     (Sketch)
%      Recall that $\cH_{\phi,\tau} = \{h: x \to \langle w_h, x \rangle \mid w_h \in \R^d, ||w_h||_2 \leq B, \theta(w_h, w_{h'}) \leq \tau\}$. For a set of sample $S$, the empirical Rademacher complexity of $\cH_{\phi,\tau}$ is given by
%         \begin{align*}
%         R_S(\cH_{\phi,\tau}) &= \frac{1}{n} \bbE_{\sigma}\left[\sup_{h \in \cH_{\phi,\tau}} \sum_{i=1}^n h(x_i)\sigma_i\right]\\
%         &= \frac{1}{n} \bbE_{\sigma}\left[\sup_{\substack{\lVert w_h\rVert_2 \leq B\\ \theta(w_h, w_{h'}) \leq \tau}} \sum_{i=1}^n \langle w_h, x_i \rangle\sigma_i\right]\\
%         &= \frac{1}{n} \bbE_{\sigma}\left[\sup_{\substack{\lVert w_h\rVert_2 \leq B\\ \theta(w_h, w_{h'}) \leq \tau}}  \langle w_h, \sum_{i=1}^n x_i\sigma_i \rangle\right].
%     \end{align*}
%  For a vector $w'\in \R^d$ with $\lVert w'\rVert_2 = 1$, and a vector $v\in \R^d$, we will claim the following, 
% \begin{enumerate}
%     \item If $\theta(v, w') \leq \tau$, we have
%     $$
% \sup_{\substack{\lVert w\rVert_2 \leq B\\ \theta(w,w') \leq \tau}}  \langle w, v \rangle = B\lVert v \rVert.
% $$
% \item If $\frac{\pi}{2} + \tau \leq \theta(v, w') \leq \pi$, we have
%     $$
% \sup_{\substack{\lVert w\rVert_2 \leq B\\ \theta(w,w') \leq \tau}}  \langle w, v \rangle = 0.
% $$
% \item If $\tau \leq \theta(v, w') \leq \frac{\pi}{2} + \tau $, we have
% $$
% \sup_{\substack{\lVert w\rVert_2 \leq B\\ \theta(w,w') \leq \tau}}  \langle w, v \rangle = B\lVert v\rVert \cos(\theta(v, w') - \tau).
% $$
% \end{enumerate}
% We can use this to calculate the empirical Rademacher complexity of $\cH_{\phi,\tau}$. For full proof, we refer to Appendix \ref{appendix: main theory linear}.


% \end{proof}
\begin{figure}[h!]
\centering
    \includegraphics[width = 0.6\columnwidth]{figs/linear_2_cases.pdf}
    \caption{Benefits of an explanation constraint also depend on the data distribution. We represent data points $x_i$ with red squares (Left). The possible regions for $v = \sum_{i=1}^n x_i\sigma_i$ are the shaded areas (Right). When the data is highly correlated with $w'$, $v$ would lie in a region where $f(v)$ is large (Top) and this implies that the gradient constraints provide less benefits. On the other hand, when the data is almost orthogonal to $w'$, $v$ would lie in a region with a small value of $f(v)$ (Bottom) which leads to more benefits from the gradient constraints}. 
    \label{fig: linear case benefit}
\end{figure}

\noindent For the proof, we refer to Appendix \ref{appendix: main theory linear} for full proof. We compare this with the standard bound on linear models which is given by
$$
R_S(\cH) = \frac{B}{n}\bbE_\sigma[\lVert v \rVert].
$$
The benefits of the explanation constraints depend on the underlying data distribution; in the case of linear models with a gradient constraint, this depends on an angle between $v = \sum_{i=1}^n x_i \sigma_i$ and $w'$. The explanation constraint reduces the term inside the expectation by a factor of $f(v)$ depending on $\theta(v,w')$.  When $\theta(v,w') \leq \tau$ then $f(v) = 1$ which implies that there is no reduction. The value of $f(v)$ decreases as the angle between $\theta(v,w')$ increases and reaches $f(v) = 0$ when $\theta(v,w') \geq \frac{\pi}{2} + \tau$. When the data is concentrated around the area of $w'$, the possible regions for $v$ would be close to $w'$ or $-w'$  (Figure \ref{fig: linear case benefit} (Top)). The value of $f(v)$ in this region would be either $1$ or $0$ and the reduction would be $\frac{1}{2}$ on average. In essence, this means that the gradient constraint of being close to $w'$ does not actually tell us much information beyond the information from the data distribution. On the other hand, when the data points are nearly orthogonal to $w'$, the possible regions for $v$ would lead to a small $f(v)$ (Figure \ref{fig: linear case benefit} (Bottom)). This can lead to a large reduction in complexity. Intuitively, when the data is nearly orthogonal to $w'$, there are many valid linear models including those not close in angle to $w'$. The constraints allows us to effectively shrink down the class of linear models that are close to $w'$.

% This implies that many linear models, including those not close in angle to $w'$ are valid. In this setting, the gradient information indeed restricts the $\cH$ 


% As mentioned before, our restrictions may not be beneficial if the underlying data distribution is already concentrated about this restricted class of hypothesis. The bound above gives us a result that depends on the given linear model $w'$. Recall that $v = \sum_{i=1}^n x_i \sigma_i$, when $\theta(v, w') \leq \tau$, $v$ is highly correlated with $w'$. In essence, this means that the data concentrated around the area of $w'$, and the gradient constraint of being close to $w'$ does not actually tell us much information (Figure \ref{fig: linear case benefit} (Top)). This is illustrated by our bound not changing here, remaining as a factor of $\lVert v \rVert$. 

% However, in the case when $\theta(v, w') \geq \tau$, we observe that the data is concentrated in regions other than near $w'$. This implies that many linear models, including those not close in angle to $w'$ are valid. In this setting, the gradient information indeed restricts the $\cH$ effectively (Figure \ref{fig: linear case benefit} (Bottom)). This is manifested in our bound, now on the order of $\cos(\theta(v, w') - \tau) \cdot \lVert v \rVert$. We remark that $\cos(\theta(v, w') - \tau)$ is the angle between $v$ and a linear model that is within angle $\tau$ of $w$. As this increases (to the value of $\frac{\pi}{2}$), we have a smaller upper bound.




We now consider the case of a uniform distribution on a sphere to make these benefits more concrete. We utilize the symmetry of the uniform distribution over a sphere to derive an upper bound on the Rademacher complexity.



\begin{theorem}
[Rademacher complexity of linear models with a gradient constraint, uniform distribution on a sphere]
\label{theorem:bound_linear}
Let $\cD_\cX$ be a uniform distribution on a unit sphere in $\R^d$, let $\cH = \{h: x \mapsto \langle w_h, x \rangle \mid w_h \in \R^d, ||w_h||_2 \leq B\}$ be a class of linear models with weights bounded by a constant $B$. Let $\phi(h,x) = \theta(w_h, w_{h'})$ be a surrogate loss where $\theta(u,v)$ is an angle between $u,v$. We have
\begin{align*}
    R_n(\cH_{\phi, \tau}) & \leq \frac{B}{\sqrt{n}} \left(\sin(\tau) \cdot p + \frac{1 - p}{2} \right),
\end{align*}
where $p = \erf\left( \frac{\sqrt{d}\sin(\tau)}{\sqrt{2}}\right)$
and $\erf(x) = \frac{2}{\sqrt{\pi}}\int_0^x e^{-t^2} dt$ is the standard error function.
\end{theorem}
\begin{proof}
    We defer the proof to Appendix \ref{appendix: main theory linear}.
\end{proof}
The standard upper bound on the Rademacher complexity of linear models is $\frac{B}{\sqrt{n}}$. Our bound has a nice interpretation; we shrink our bound by a factor of $(\frac{1 - p}{2} + \sin(\tau) p)$. We remark that  $d$ increases, we observe that $p \to 1$, so the term $\sin(\tau) p$ dominates this factor. As a consequence, we get that our bound is now scaled by $\sin(\tau) \approx \tau$ and the the Rademacher complexity scales down by a factor of $\tau$. This implies that given $n$ labeled data, to achieve a fast rate $\cO(\frac{1}{n})$, we need $\tau$ to be as good as $O(\frac{1}{\sqrt{n}})$.

% \red{change to alpha?}

\subsection{Gradient Explanations for Two Layer Neural Networks}

% \subsection{Gradient Explanations for Two layer neural networks}
% We now analyze the case of gradient explanations for two layer neural networks, with $m$ hidden nodes.

\begin{theorem}
[Rademacher complexity of two layer neural networks ($m$ hidden nodes) with a gradient constraint]
\label{theorem:bound_2layer_gradient} 
Let $\cX$ be an instance space and $\cD_{\cX}$ be a distribution over $\cX$ with a large enough support. Let $\cH = \{h : x \mapsto \sum_{j=1}^m w_j \sigma(u_j^\top x) | w_j \in \R, u_j \in \R^d, \sum_{j=1}^m |w_j|  \leq B, \lVert u_j \rVert_2 = 1 \}$ be a class of two layer neural networks with a ReLU activation function and bounded weight. Assume that there exists some constant $C > 0$ such that $\bbE_{x \sim \cD_{\cX}} [ \lVert x \rVert_2^2 ] \leq C^2$. Consider an explanation loss given by 
\begin{align*}
    \phi(h, x) = & \lVert \nabla_x h(x) - \nabla_x h'(x) \rVert_2 + \\
    & \quad \infty \cdot 1 \{ \lVert \nabla_x h(x) - \nabla_x h'(x) \rVert > \tau \} 
\end{align*} 
for some $\tau > 0$. Then, we have that
$$R_n ( \cH_{\phi, \tau}) \leq  \frac{3 \tau m C}{\sqrt{n}}$$
\end{theorem}
\begin{proof}
(Sketch)
     The key ingredient is to identify the impact of the gradient constraint and the form of class $\cH_{\phi, \tau}$. We provide an idea when we have  $m=1$ node. We write $h(x) = w \sigma(u^\top x)$ and $h'(x) = w'\sigma(u'^\top x)$. Note that
    $$
    \nabla_x h(x) - \nabla_x h'(x) = wu 1\{u^\top x > 0\} - w'u' 1\{(u')^\top x > 0\}
    $$
    is a piecewise constant function (Figure \ref{fig: simplify 2NN 1}). Assume that the probability mass of each region is non-negative, our gradient constraint implies that the norm of each region cannot be larger than $\tau$. 
    \begin{enumerate}
        \item If $u,u'$ have different directions, we have 4 regions in $\nabla_x h(x) - \nabla_x h'(x)$ and can conclude that $|w| < \tau, |w'| < \tau$.
        \item If $u = u'$ have the same direction, we only have 2 regions in $\nabla_x h(x) - \nabla_x h'(x)$ and can conclude that $\lVert wu - w'u'\rVert = |w - w'| < \tau$.
    \end{enumerate}
\begin{figure}
\centering
\includegraphics[width=0.65\columnwidth]{figs/2NNS_m1_clean2.pdf}
\caption{Visualization of the piecewise constant function of $\nabla_x h(x) - \nabla_x h'(x)$ when $h$ is a two layer NNs with 1 node. Background colors represent regions with non-zero value. }
\vspace{-2mm}
\label{fig: simplify 2NN 1}
\end{figure}
The gradient constraint enforces a model to have the same node boundary $(u = u')$ with a small weight difference $|w-w'| < \tau$ or that node would have a small weight $|w| < \tau$. This finding allows us to determine the restricted class $\cH_{\phi,\tau}$, and we can use this to bound the Rademacher complexity accordingly. For full details, see Appendix \ref{appx: main theory nn}.
\end{proof}
We compare this with the standard Rademacher complexity of a two layer neural network \citep{ma2022notes}
$$ R_n(\cH) \leq \frac{2BC}{\sqrt{n}}.$$
We can do better than this standard bound if $\tau <  \frac{2B}{3m}$. One interpretation for this is that we have a budget at most $\tau$ to change the weight of each node and for total $m$ nodes, we can change the weight by at most $\tau m$. We compare this to $B$ which is an upper bound on the total weight  $\sum_{j=1}^m |w_j| \leq B$. Therefore, we can do better than a standard bound when we can change the weight by at most two thirds of the average weight $\frac{2B}{3m}$ for each node.
% \textcolor{red}{ADD COMPARISON}
% \red{add assumption on the distribution of x, discuss the strongness of the surrogate loss in this section}

We would like to point out that our bound does not depend on the distribution $\cD$ because we choose a strong explanation loss that guarantees that the gradient constraint holds almost everywhere. Extending to a weaker loss such as $\phi(h,x) = \lVert \nabla_x h(x) - \nabla_x h'(x) \rVert$ is a future research direction. We also assume that there exists $x$ with a positive probability density at any partition created by $\nabla_x h(x)$. In contrast, our result for linear models uses a weaker explanation loss and depends on $\cD$ (see Theorem \ref{theorem:linear_distribution_free}).

% \begin{theorem}
% Assume $\cD$ is a uniform distribution over a unit sphere in $\R^d$. Let $\cH$ be a class of linear separators that pass through an origin. Let $\phi(h,x) = \theta(w_h, w_{h'})$ be an explanation loss. Then we have
% \begin{equation*}
%     R_n(\cH_{\phi,\tau}) = \cO(\sqrt{\frac{\tau}{n}}). 
% \end{equation*}
% \end{theorem}
% \begin{proof}
% Recall that
% \begin{equation*}
%     \cH_{\phi, \tau} = \{ 
%     h\mid h(x) = \operatorname{sign}(w_h^\top x), \theta(w_h,w_h') \leq \tau\}.
% \end{equation*}

% We define a disagreement region of $\cH_{\phi,\tau}$ as the set of points that there exists two classifiers in $\cH_{\phi,\tau}$ that disagree.
% \begin{equation*}
%     \operatorname{DIS}(\cH_{\phi,\tau}) = \{x \in \cX \mid \exists h_1,h_2 \in \cH_{\phi,\tau}, h_1(x) \neq h_2(x) \}.
% \end{equation*}
% We define an agreement region as the complement of the disagreement region
% \begin{equation*}
%     \operatorname{Agree}(\cH_{\phi,\tau}) = \cX \setminus \operatorname{DIS}(\cH_{\phi,\tau}).
% \end{equation*}
% \begin{figure}[ht]
%     \centering
%     \includegraphics[width = 1\columnwidth]{figs/linear_case.jpeg}
%     \caption{\red{change pic to one wedge}.}
%     % \label{}
% \end{figure}
% The disagreement region is known for a class of linear separator and a uniform distribution over a unit sphere \cite{?}.
% \begin{equation*}
%     \operatorname{DIS}(\cH_{\phi,\tau}) = \{x \in \cX \mid \frac{\pi}{2} - \tau \leq \theta(x, w_{h'}) \leq \frac{\pi}{2} + \tau \}.
% \end{equation*}
% \red{Rademacher complexity with |.| or without }
% The key observation is that 
%  the empirical Rademacher complexity of $\cH_{\phi,\tau}$ only depends on the points that lie in the disagreement region $\operatorname{DIS}(\cH_{\phi,\tau})$. 
% For a set of samples $S = \{x_1,\dots, x_n\}$, the empirical Rademacher complexity is given by
% \begin{align*}
% R_S(\cH_{\phi,\tau})=
%     \mathbb{E}_\sigma[\frac{1}{n} \sup_{h \in \cH_{\phi,\tau}}( \sum_{x_j \in \operatorname{DIS}(\cH_{\phi,\tau}) } h(x_j)\sigma_j)]
% \end{align*}
% because for points that lie in $\operatorname{Agree}(\cH_{\phi,\tau})$, any $h \in \cH_{\phi,\tau}$ will take the same value and the expectation of $\sup_{h \in \cH_{\phi,\tau}}h(x_i)\sigma_i$ over Rademacher distribution will be zero. Furthermore, in our case of linear separators over a uniform distribution over a unit sphere, the empirical Rademacher complexity of $\cH_{\phi,\tau}$ over $m$ samples from $\operatorname{DIS}(\cH_{\phi,\tau})$ equals to the Rademacher complexity of $\cH$ over $m$ random samples from $\cD$,
% \begin{equation*}
%     R_{S \sim \operatorname{DIS}(\cH_{\phi,\tau})^m}(\cH_{\phi,\tau}) = R_{S \sim \cD^m}(\cH).
% \end{equation*}



% Informally, the disagreement region contains points with angle at most $\tau$ from the normal vector $w_{h'}$, this take a wedge shape (Figure ).  Our restriced hypothesis class $\cH_{\phi,\tau}$ contains any linear separators with a decision boundary that cut through this wedge and can linearly separate any two points in the disagreement region. By geometry, fitting random labels on $m$ points in this wedge with $\cH_{\phi,\tau}$ has the same complexity as fitting random labels on $m$ points on the unit sphere with $\cH$.
% \red{add absolute to the rademacher complexity definition then half sphere = full sphere}

% Formally, when $\tau < \frac{\pi}{2}$, there is a bijective map $\rho_1:\bbS^{d-1} \to \operatorname{DIS}(\cH_{\phi,\tau})$ between a unit sphere and the disagreement region and a bijective map $\rho_2: \cH \to \cH_{\phi,\tau}$ between the original hypothesis class to the restricted hypothesis class such that for any points $S = \{x_1,\dots, x_m\}$ drawn i.i.d. from $\cD$ and $h \in \cH$ 

% \begin{equation*}
%     \sum_{i=1}^m h(x_i)\sigma_i = \sum_{i=1}^m \rho_2(h)(\rho_1(x_i))\sigma_i.
% \end{equation*}
% Intiutively, $\rho_1$ shrink the whole unit sphere into the disagreement region wedge by rotation (?) and $\rho_2$ shrink $\cH$ to $\cH_{\phi,\tau}$ by rotating the normal vector (?)(Figure ). In addtion, the distribution $\rho_1(\cD)$ is also a uniform distribution over the disagreement region. With the existence of $\rho_1,\rho_2$, we can conclude that the empirical Rademacher complexity of $\cH_{\phi,\tau}$ over $m$ samples from $\operatorname{DIS}(\cH_{\phi,\tau})$ equals to the Rademacher complexity of $\cH$ over $m$ random samples from $\cD$.

% Let $R_{n}(\cH)$ be a Rademacher complexity of $\cH$ over a distribution $\cD$ then we can write
% \begin{align*}
%     R_{n}(\cH_{\phi, \tau}) &= \mathbb{E}_{S \sim \cD^n}[R_S(\cH_{\phi, \tau})]\\
%     &= \mathbb{E}_{S \sim \cD^n}[\mathbb{E}_{S \sim \cD^n}[R_S(\cH_{\phi, \tau}) \mid |S \cap \operatorname{DIS}(\cH_{\phi,\tau})| = m]]\\
%     &= \mathbb{E}_{S \sim \cD^n}[\mathbb{E}_{S \sim \cD^n}[\frac{m}{n}R_m(\cH) \mid |S \cap \operatorname{DIS}(\cH_{\phi,\tau})| = m]]\\
%     &= \frac{1}{n}\sum_{k = 0}^n kR_k(\cH)\Pr(|S \cap \operatorname{DIS}(\cH_{\phi,\tau})| = k).
% \end{align*}
% Let $|S \cap \operatorname{DIS}(\cH_{\phi,\tau})|$ is the number of points in $S$ that lies in the disagreement region which follows a Binomial distribution $\operatorname{Bin}(n,p)$ when $p = \Pr(\operatorname{DIS}(\cH_{\phi,\tau})) = \frac{\tau}{\pi}$. Therefore,
% \begin{equation*}
%     R_{n}(\cH_{\phi, \tau})  = \frac{1}{n}\sum_{k = 0}^n kR_k(\cH) {n \choose k} \frac{\tau}{\pi}^k (1 - \frac{\tau}{\pi})^{n-k}. 
% \end{equation*}
% It is known that $R_n(\cH) = \cO(\frac{1}{\sqrt{n}})$ (see Appendix ?). With the Normal approximation to the Binomial distribution ,we have
% \begin{equation*}
%     R_{n}(\cH_{\phi, \tau})  = \cO (\frac{\bbE_X[\sqrt{X}]}{n})
% \end{equation*}
% when $X \sim \cN(\frac{n\tau}{\pi}, \frac{n\tau}{\pi}(1 - \frac{\tau}{\pi}))$. Since, $\bbE_X[\sqrt{X}] = \cO(\sqrt{\frac{n\tau}{\pi}})$. We have
% \begin{equation*}
%     R_{n}(\cH_{\phi, \tau}) = \cO (\sqrt{\frac{\tau}{n}}).
% \end{equation*}

    
% \end{proof}

% We note that the complexity of the original linear separator is given by $\mathcal{O}(\frac{1}{\sqrt{n}})$. This implies that in order to reap a full benefit from the explanation, we need the angle between our noisy gradient and the true gradient $w_{h^*}$, $\tau$ to be smaller that $o(1)$. For example, if $\tau = \frac{1}{n}$ then we have $R_n(H_{\phi,\tau}) = \mathcal{O}(\frac{1}{n})$.





% \subsection{A Noisy Classifier for Linear Classifier}

% In addition, for a linear model, this gradient information is equivalent to having access to a noisy classifier $h'(x) = \operatorname{sign}(w_{h'}^\top x)$. In our previous analysis, we define $\phi(h,x) = \theta(w_h, w_{h'})$ as an angle between the normal vector of a given linear classifier $h$ and $h'$. We can also define $$\phi(h,x) = 1[h(x) \neq h'(x)]$$ as a disagreement between $h$ and $h'$. Then the restricted hypothesis class is given by any linear separator that different from $h'$ by probability at most $\tau$. However, these two notions is equivalent since we can bound the disagreement between any two linear separators with the angle between the corresponding normal vector $\Pr(h(x) \neq h'(x)) \leq C\theta(w_h,w_{h'})$ (\cite{?}).




% \subsection{Physics Explanations}

% We can also analyze explanations given by physics rules or simulators \citep{NEURIPS2018_842424a1, seo2021controlling}. We can consider the hypothesis class $\cH = \{ f_1 \circ f_2 \circ f_3 | f_1, f_3 \text{ are linear}, f_2 \text{ is a two layer neural network} \}$. The hypothesis class of classifiers that use information from physics rules or simulators is given by $\cH_p = \{f_1 \circ p \circ f_3 | f_1, f_3 \text{ are linear} \}$ where $p$ is the particular physics simulator. While this is a simplification of incorporating implicit physics rules or a physics simulator, we note that this still captures the intuition of a classifier learning information as inputs for physics reasoning and then using the outputs to make a decision. We also note that this formulation assumes that we can capture the information represented by a physics simulator or physics rules in a two layer neural network.

% \textcolor{red}{TODO: Generalization Bound}

% We note that there are no big differences in learning these approaches as previous methods are differentiable approximations or capture this information implicitly. As a result, we can optimize these methods as normal (i.e., through SGD). 


% \section{Questions}

% \begin{enumerate}
%     \item For a linear model, gradient information is equivalent to providing an interval for each weight (see 5.2 in Tengyu's notes).
%     \item When $d = 2$, give a closed form goodness of an explanation for a gradient information for a linear separator when the data is drawn uniformly on a sphere.
%     \item When $d > 2$, give a closed form goodness of a gradient information.
%     \item (Showing that gradient can be helpful)If we know the exact gradient of a neural network, would it be possible to recover the two layer neural network with $m$ nodes and how many example we need to do so ? - Both
%     \item Can we formalize what a physics rule looks like? What hypothesis class / data?
%     \item Do we need to address transformation invariance? (Read NeurIPS paper first) - \textcolor{red}{Dylan: It seems like the existing NeurIPS paper handles this pretty well. The paper defines specific notions of VC dimension that take into account translation invariance (which is defined by partitioning data into their respective groups of translations). The paper handles multiple cases and derives an algorithm that achieves sample complexity on the order of their notion of VC dimension (without knowing if the setting is realizable, agnostic, or if the translations help).}
%     \item Handle noisy classifiers (imperfect explanation case) - the same for the linear case... Can we handle for more complex case (two layer NN ...) - later when the noisy classifier does not come from $\cH$ (improper learning).
%     \item Information gain bound.
% \end{enumerate}

% \section{Matching Explanations without Unlabeled Data}

% \pagebreak

% \section{Algorithmic Results on Learning with Explanation Constraints}

%a constraint that we want our concept $h$ to satisfy. To analyze the impacts of incorporating explanations in our classifiers, we build off of existing work \citep{balcan2010discriminative}, which provides a PAC-style framework for semi-supervised learning. This paper analyzes classical notions of compatibility with a distribution, including structural risk minimization, margin, and smoothness. In our work, we are interested in and motivated by a different type of compatibility: that of satisfying a particular, known explanation. We now formally define our notion of an explanation $E$. 
% as a function $g(h,x)$ on a concept $h$ and a data point $x$ and a compatibility function $\ell_E(g(h, x))$, which tells us how desirable a given hypothesis element is.

% \begin{definition}
% [Explanation]
% % For a bounded function $g:\cH \times \cX \to \R$ and a set $C(x) \subseteq \R$ for all $x\in \cX$, we call $E(g,C)$ as explanation induced by $g,C$ if for a concept $h \in \cH$ and a data point $x\in \cX$, when $g(h,x) \in C(x)$ then we say that $h$ satisfy $E$ at a point $x$. Otherwise, $h$ does not satisfy $E$ at $x$.
% Let $E(g, \phi)$ be an explanation, where $g: \cH \times \cX \to \R$ and $\phi: \cH \times \cX \to \R$. In our notation, $g$ represents a particular function of interest of an element $h \in \cH$, and $\phi$ represents a notion of compatibility, or how we believe a hypothesis element should behave on a particular data. 
% % We let $\ell_E(h, D) = \E_{x \in D}[C(g(h, x))]$.
% For an explanation $E(g,\phi)$, and a classifier $h \in \cH$ and a data point $x \in \cX$, an incompatibility of $E$ on $h$ is given by $\phi(g(h,x),x)$ and we also overload the notation,
% \begin{equation*}
%     \phi(h,\cD) = \mathbb{E}_{x \in \cD}[\phi(g(h,x),x)].
% \end{equation*}
% \end{definition}

% We can use this definition to capture many different types of explanations and forms of inductive bias that are frequently used in practice. This formulation is equally as expressible as existing work \citep{balcan2010discriminative}, and we assume that our explanation is interested in a particular function $g$ of $h$ and $x$. We now will discuss the function of interest $g$ for different type of explanations that are used in practice in Table \ref{table: examples of explanation}. In addition, we have a separate notion of compatibility $\phi$ that control the strength and quality of an explanation. We can see $\phi$ as a generalization of a surrogate loss for 0-1 loss where we impose small loss on area where we believe our ideal model should be at. We provide examples of $\phi$ for different situations in Figure \ref{fig:phi example}. We note that a more restrictive $\phi$ where we only have a small area with zero loss can lead to a stronger explanation but at the same time this may increase the probability of being wrong and ruling out an optimal classifier.



% \begin{enumerate}
    % \item Gradient: $\nabla h(x) \in C \subset \R$. (The gradient w.r.t. the first feature must be positive ($ g(x) = \nabla_{x_1} h(x)$, $C = (0, \infty)$))
    % \item Scope of output: $h(x) \in C \subset \R$. 
    % \item Noisy classifier: $h(x) = \lambda(x) \in \R$
    % \item Relationship between two points (need to generalize more): $h(x) - h(y) \in C \subset \R$
% \end{enumerate}\begin{table}[]


% \begin{figure}[th]
%     \centering
%     \includegraphics[width = 0.7\textwidth]{pics/phi.jpeg}
%     \caption{Choice of $\phi$ for different scenarios}
%     \label{fig:phi example}
% \end{figure}

% \red{to-do}
% \begin{enumerate}
    % \item add a learning theory setup, hypothesis class $\cH$ bla bla
    % \item polish examples
% \end{enumerate}

% What is a good explanation? 
% \begin{definition}
% [Goodness of an explanation]
% For a hypothesis class $\cH$, the goodness of an explanation $E(g,C)$ is given by
% \begin{equation*}
%     \Pr(\err(h) \leq \varepsilon | g(h,x) \in C)
% \end{equation*}
% \end{definition}

% How well an explanation can explain a concept $h$
% \begin{definition}
% [Satisfaction of an explanation]
% The amount some hypothesis $h$ satisfies $E$ is given by $$\E_{x \in supp(g)}[ \1 \{ g(x, h) \in C \}]$$
% \end{definition}

\section{Algorithms for Learning from Explanation Constraints}\label{sec:variational_algo}



Although we have analyzed learning with explanation constraints, algorithms to solve this constrained optimization problem are non-trivial. 
% We propose a variational approach to solve this constrained optimization problem, which has underlying connections to self-training \citep{wei2020theoretical, frei2022self}. 
In this setting, we assume that we have access to $n$ labeled data $\{(x_i,y_i)\}_{i=1}^n$, $m$ unlabeled data $\{x_{n+i}\}_{i=1}^m$, and $k$ data with explanations $\{(x_{n+m+i}, \phi(\cdot, x_{n+m+i}))\}_{i=1}^{k}$. We argue that in many cases, $n$ labeled data are the most expensive to annotate. The $k$ data points with explanations also have non-trivial cost; they require an expert to provide the annotated explanation or provide a surrogate loss $\phi$. If the surrogate loss is specified then we can evaluate it on any unlabeled data, otherwise these data points with explanations could be expensive. On the other hand, the $m$ data points can cheaply be obtained as they are completely unlabeled.  We now consider existing approaches to incorporate this explanation information into a machine learning pipeline.\\

\noindent\textbf{EPAC-ERM:} 
Recall our EPAC-ERM objective from \eqref{eq:epac-erm}:
    \begin{equation*}
        \min_{h \in \cH} \frac{1}{n} \sum_{i=1}^n 1\{h(x_i) \neq y_i\} \; \text{ s.t. } \; \frac{1}{k}\sum_{j=n+m+1}^{n+m+k}\phi(h, x_j ) \leq \tau
    \end{equation*}
for some constant $\tau$. This constraint in general requires more complex optimization techniques (e.g., running multiple iterations and comparing values of $\tau$) to solve algorithmically. \\

\noindent\textbf{Constrained optimization:}
One could of course also consider solving the fully constrained ERM objective:
\begin{equation*}
        \min_{h \in \cH} \frac{1}{n} \sum_{i=1}^n 1\{h(x_i) \neq y_i\} \; \text{s.t. } \phi(h_j, x_j) = 0, \forall j \geq n+m+1
\end{equation*}
This objective does not apply well to gradient-based methods and thus is difficult to optimize for deep networks.\\

\noindent\textbf{Augmented Lagrangian objectives:}
\begin{equation*}
    \min_{h \in \cH} \frac{1}{n} \sum_{i=1}^n 1[h(x_i) \neq y_i] + \frac{\lambda}{k}\sum_{j=n+m+1}^{n+m+k}\phi(h, x_j )
\end{equation*}
As is done in prior work \citep{rieger2020interpretations}, we can consider an augmented Lagrangian objective. However, this does not exactly fit into our analytical framework. Also, tuning the hyperparameter $\lambda$ is difficult, as the two components of the loss function may lie in different spaces. While these approaches are viable, they do not necessarily scale well to larger (deep) models. \\
% \begin{align*}
%     % \min_{f\in \mathcal{H}} \text{err}(f) + d(f, \mathcal{H}_E)
%     \min_{h \in \cH} (1 - \tau) \displaystyle \mathop{\mathbb{E}}_{(x, y) \sim \cD} \left[\ell(h(x),y)\right] +
%     \tau \inf_{\substack{f \in \cH_{\phi, t}}} \displaystyle \mathop{\mathbb{E}}_{\substack{x \sim \cD_{\cX}, \\ \tilde{y}|x \sim f}} \left[ \ell(h(x),\tilde{y})\right],
% \end{align*}

\noindent \textbf{Variational objective: } We propose a \textit{new} variational objective
{\small
\begin{align*}
    % \min_{f\in \mathcal{H}} \text{err}(f) + d(f, \mathcal{H}_E)
    \min_{h \in \cH} (1 - \lambda) \displaystyle \mathop{\mathbb{E}}_{(x, y) \sim \cD} \left[\ell(h(x),y)\right] +
    \lambda \inf_{\substack{f \in \cH_{\phi, \tau}}} \displaystyle \mathop{\mathbb{E}}_{x \sim \cD_{\cX}} \left[ \ell(h(x),f(x))\right],
\end{align*}
}
where $\ell$ is some loss function and $t \geq 0$ is some threshold. The first term is a risk while the second term is the distance between $h$ and a class of models $f$ that are $\tau$-EPAC. The distance is a risk of $h$ given that the label $y$ is labeled by $f$ e.g. pseudo labels given by the EPAC model that is closest to $h$. We note that both terms are in the same space which makes it easier to tune the parameter $\lambda$. It is clear that the optimal solution of \textbf{EPAC-ERM} would also be an optimal solution of our proposed variational objective. We approximate this objective with the following iterative technique, drawing inspiration from prior work in posterior regularization \citep{ganchev2010posterior, hu2016harnessing}. More specifically, let $h_t$ be the learned model at time $t$ and at each timestep $t$,
\begin{enumerate}
\item We project  $h_t$ to the class of  $\tau$-EPAC model $\cH_{\phi,\tau}$, denoted as $f_{t+1, \phi}$,
$$f_{t+1, \phi} =  \argmin_{h \in \cH} \frac{1}{m} \sum_{i=n+1}^{n+m} \ell(h(x_i),h_t(x_i))   + \eta \max\left(0,\, \frac{1}{k} \sum_{i=n+m+1}^{n+m+k} \phi(h, x_i) - \tau\right). $$ The first term is the difference between $h_t$ and $f$ on unlabeled data and the second term is the expected surrogate loss and we want this to be smaller than $t$.  Also, $\eta$ is a regularization parameter.
\item We calculate $h_{t+1}$ that minimizes the empirical risk of labeled data and matches the pseudo labels given by  $f_{t+1, \phi}$
$$
h_{t+1, \phi}  =  \argmin_{h \in \cH} \frac{1}{n} \sum_{i=1}^n \ell(h(x_i),y_i)   + \frac{1}{m} \sum_{i=n+1}^{n+m} \ell(h(x_i), f_{t+1, \phi}(x_i)).$$
\end{enumerate}

We initialize this procedure with some model $h_0$. 
% fix the current student model iterate $h_{t,\phi}$, and learn the explanation-regularized teacher function $f_{t+1, \phi}$ (that aims to project $h_{t,\phi}$ onto the set of explanation constrained models); and then fix that to obtain the next iterate $h_{t+1,\phi}$ of the student model that aims to match the outputs of $f_{t+1, \phi}$ on \textit{unlabeled data} in addition to the labeled samples:
% \begin{align*}
% f_{t+1, \phi} = & \argmin_{h \in \cH} \frac{1}{m} \sum_{i=n+1}^{n+m} \ell(h(x_i),h_t(x_i))   + \lambda \max\left(0,\, \frac{1}{k} \sum_{i=n+m+1}^{n+m+k} \phi(h, x_i) - \tau\right) \\
% h_{t+1, \phi}  = & \argmin_{h \in \cH} \frac{1}{n} \sum_{i=1}^n \ell(h(x_i),y_i)   + \frac{1}{m} \sum_{i=n+1}^{n+m} \ell(h(x_i), f_{t+1, \phi}(x_i)),
% \end{align*}
% given some initialization $h_0$. 
% This scales nicely to training deeper models as we can efficiently model this projection onto our class of EPAC models through learning $\hat{f}$. 
% In essence, our algorithm iteratively learns an  and trains a student model $h_{t+1, \phi}$ . In turn, our student model is then projected o in order to train the teacher $f$. 
We remark that could see this as a constraint regularized self-training where  $h_t$ is a student model and $f_t$ is a teacher model. At each timestep, we project a student model to the closest teacher model that satisfies the constraint. The next student model then learns from both labeled data and pseudo labels from the teacher model. In the standard self-training, we do not have any constraint and we have $f_t = h_t$.  We also note that we can replace $\cH_{\phi, \tau}$ with a simpler class of teacher models $\cF_{\phi,\tau}$ for greater efficiency. 
% One primary benefit of this approach as it makes use of this $m$ unlabeled data; as mentioned before, for particular types of hypothesis classes and explanations, the sample complexity for learning our class of EPAC models can be exponentially large (in $k$). Our variational objective allows us to reduce this explanation sample complexity through regularized self-training over unlabeled data. On the contrary, standard supervised and Lagrangian approaches do not use this information.


\section{Experiments} \label{experiments}

We provide both synthetic and real-world experiments to support our theoretical results and clearly illustrate interesting tradeoffs of incorporating explanations. In our experiments, we compare our method against 3 baselines: (1) a standard supervised learning approach, (2) a simple Lagrangian-regularized method (that directly penalizes the surrogate loss $\phi$), and (3) a self-training approach that propagates its own predictions over a set of unlabeled data. These experiments also demonstrate that our new variational approach is preferable to simple Lagrangian methods and other supervised methods in many cases. More extensive ablations are deferred to Appendix \ref{appx:ablation}, and code to replicate our experiments will be released publicly with the full paper.

\subsection{Regression Task with Exact Gradient Information}

\begin{figure}[t]
    \centering
    \vspace{-3mm}
    \includegraphics[width=0.48\columnwidth]{figs/v_linear.pdf}
    \vspace{-4mm}
    \caption{Comparison of MSE on regressing a linear model. Results are averaged over 5 seeds. $m = 1000, k=20$. } 
    \vspace{-2mm}
    \label{fig:linear_mse}
\end{figure}

\begin{figure*}[t]
    \centering
    \vspace{-2mm}
    \includegraphics[width=0.48\columnwidth]{figs/v_nn.pdf}
    \includegraphics[width=0.48\columnwidth]{figs/v_constraints.pdf}
    \vspace{-2mm}
    \caption{Comparison of MSE on regressing a two layer neural network (left). Comparison of $\ell_2$ distance over input gradients on the test data as we vary the amount of labeled data $n$ (right). Results are averaged over 5 seeds. $m = 1000, k=20$.} 
    \vspace{-2mm}
    \label{fig:2layer_mse}
\end{figure*}


In our synthetic experiments, we focus on a regression task where we try to learn some model contained in our hypothesis class. Our data is given by $\cX = \R^d$, and we try to learn a target function $h^*: \cX \to \R$. Our data distribution is given by $X \sim \cN(0, \sigma^2 I)$, where $I$ is a $d \times d$ identity matrix. We generate $h^*$ by randomly initializing a model in the specific hypothesis class $\cH$. We assume that we have $n$ labeled data, $m$ unlabeled data, and $k$ data with explanations.


We first present a synthetic experiment for learning with a perfect explanation, meaning that $\phi(h^*, S) = 0$. We consider the case where we have the \textit{exact} gradient of $h^*$. Here, let $\cH$ be a linear classifier and note that the exact gradient gives us the slope of the linear model, and we only need to learn the bias term. Incorporating these explanation indeed helps as both methods that include explanation constraints (Lagrangian and ours) perform much better (Figure \ref{fig:linear_mse}).


We also demonstrate incorporating this information for two layer neural networks. We observe a clear difference between the simpler Lagrangian approach and our variational objective (Figure \ref{fig:2layer_mse} - left). Our method is clearly the best in the setting with limited labeled data and matches the performance of the strong self-training baseline with sufficient labeled data. We note that this is somewhat expected, as these constraints primarily help in the setting with limited labeled data; with enough labeled data, standard PAC bounds suffice for strong performance. 

We also analyze how strongly the approaches enforce these explanation constraints on new data points that are seen at test time (Figure \ref{fig:2layer_mse} - right) for two layer NNs. We observe that our variational objective approaches have input gradients that more closely match the ground-truth target network's input gradients. This demonstrates that, in the case of two layer NNs with gradient explanations, our approach best achieves both good performance and satisfying the constraints. Standard self-training achieves similar performance in terms of MSE but has no notion of satisfying the explanation constraints. The Lagrangian method does not achieve the same level of satisfying these explanations as it is unable to generalize and satisfy these constraints on new data. 
% We provide more experiments in Appendix \ref{fig:ablation_k_constraint} to compare the performance of our variational method and the Lagrangian method over varying values of $k$. 

\subsection{Tasks with Imperfect Explanations}

\begin{figure*}[t]
    \centering
    \vspace{-2mm}
    \includegraphics[width=0.48\columnwidth]{figs/noise/v_nn_ep0.5.pdf}
    \includegraphics[width=0.48\columnwidth]{figs/noise/v_nn_ep2.pdf}
    \vspace{-2mm}
    \caption{Comparison of MSE on regressing a two layer neural network with explanations of noisy gradients. $m = 1000, k=20, T = 10$. Results are averaged over 5 seeds.} 
    % \vspace{-3mm}
    \label{fig:noisy_nn}
\end{figure*}

\begin{figure*}[t]
    \centering
    \vspace{-4mm}
    \includegraphics[width=0.48\columnwidth]{figs/v_youtube.pdf}
    \includegraphics[width=0.48\columnwidth]{figs/v_yelp.pdf}
    \vspace{-4mm}
    \caption{Comparison of accuracy on the YouTube (left) and the Yelp (right) datasets. Here, we let $m = 1500, k = 150, T = 2, \tau = 1$. Results are averaged over 5 seeds.}
    \label{fig:youtube}
    \vspace{-2mm}
\end{figure*}

Assuming access to perfect explanations may be unrealistic in practice, so we present experiments when our explanations are imperfect. We present classification tasks (Figure \ref{fig:youtube}) from a weak supervision benchmark \citep{zhang2021wrench}. In this setting, we obtain explanations through the approximate gradients of a single weak labeler, as is done in \citep{sam2022losses}. We note that this differs from the standard setting of the benchmark, as we assume access to some labeled data and only use gradient information as our explanations (and not the noisy classifiers). The parameter  $T$ is the number of timestep in the iterative method and $\tau$ is the threshold on the surrogate loss $\phi$. 

We observe that our variational objective achieves better performance than all other baseline approaches, across varying amounts of labeled data. We remark that the explanation in this dataset is a noisy gradient explanation along two feature dimensions, yet this still improves upon methods that do not incorporate this explanation constraint. Indeed, our method outperforms the Lagrangian approach, showing the benefits of iterative rounds of self-training over the unlabeled data. 

In addition to our real-world experiments, we present synthetic experiments in the same regression setting as above. To generate an imperfect explanation, we use $\nabla_x h^*(x) + \epsilon$, where $\epsilon \sim \cN(0, \sigma^2)$. In our experiments, we add fixed, randomly sampled noise for each of our $k$ data annotated with explanations. We provide results for a regression task on 2-layer neural networks in Figure \ref{fig:noisy_nn} and under additional levels of noise in Appendix \ref{appx:noise}. This reveals that our method tolerates noisy explanations far better than the Lagrangian approach. Our method also performs comparably to the methods that do not use noisy explanations.

\section{Discussion}

Our work proposes a new learning theoretic framework that provides insight into how apriori explanations of desired model behavior can benefit the standard machine learning pipeline. We provide instantiations of our analysis for the canonical class of gradient explanations, which captures many explanations in terms of  feature importance. It would be of interest to provide corollaries for other types of explanations in future work. As mentioned before, the generality of our framework has larger implications towards incorporating constraints that are not considered as ``standard'' explanations. For example, this work can be leveraged to incorporate more general notions of side information and inductive biases. As a whole, our paper supports using further information (e.g., explanation constraints) in the standard learning setting.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\bibliography{arxiv}
\bibliographystyle{bibstyle}

\newpage

\appendix
\input{appendix.tex}

\end{document}
