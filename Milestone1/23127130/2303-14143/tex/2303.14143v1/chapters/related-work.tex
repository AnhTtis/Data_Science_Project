This section provides a high-level overview of LLMs and their applications before situating our work with related efforts in context-aware smart spaces.

\textbf{Large language models (LLMs)} have gained significant attention in recent years due to their impressive performance on a wide range of natural language processing tasks. In 2018, Devlin proposed BERT, a language representation model that uses Bidirectional Encoder Representations from Transformers~\cite{devlin2018bert} and can be fine-tuned for a variety of NLP tasks, such as text classification and sentiment analysis. In the same year, OpenAI proposed GPT (Generative Pre-trained Transformer)~\cite{radford2018improving}. Both models use a transformer architecture~\cite{vaswani2017attention} that was pre-trained on a massive corpus of text data, including books, articles, and websites. The resulting models demonstrate impressive results on a wide range of natural language processing tasks, including language translation, text generation, and the ability to translate natural language descriptions into program implementations.
 
Following the success of the transformer-based model, subsequent studies have explored ways to improve and expand the model's performance. In 2019, Radford et al., published an updated version of GPT and called GPT-2 \cite{radford2019language}. Building on the success of GPT-2, Brown et al. released GPT-3 in 2020 \cite{gpt3}. After that, in 2023, GPT-4 was introduced. It is currently one of the largest and most powerful language models, with more than 1 trillion parameters \cite{gpt_4}. At time of writing, access to GPT-4 is limited---we therefore base our study on GPT-3.

Two popular approaches exist for adapting task-agnostic LLMs to new applications: \emph{prompt engineering} and \emph{fine-tuning}. Prompt engineering refers to the process of designing a task-specific prompt or template that guides the model to produce relevant outputs for a particular task~\cite{zhao2021calibrate}. These prompts generally contain instructions to the model written in natural language---e.g., ``explain the following passage of text''. Fine-tuning, on the other hand, involves directly training the model on a new task by providing task-specific examples~\cite{radford2019language}. The key advantage of prompt engineering over fine-tuning is that it does not require task-specific data---we therefore adopt that approach here. Within the realm of prompt engineering, there are \emph{zero-shot} and \emph{few-shot} learning approaches. Zero-shot approaches provide the model with a single prompt containing instructions and task-specific information; few-shot approaches provide examples to the model of correct input/output pairs. We focus on zero-shot learning.

\textbf{Context-aware spaces} leverage sensor information, user data (including past behaviors and preferences), and device state to influence system actions toward meeting user needs~\cite{6177682}. The notion of ``context-awareness'' in this sense has roots in research on ambient intelligence~\cite{cook2009ambient}---that is, the development of built environments that sense and adapt to users. A concrete example of this concept is a home that leverages contextual information to improve energy efficiency~\cite{jahn2010energy, geraldo2019energy}. In an early paper, Yamazaki suggested that smart homes should go beyond automation and instead integrate expressive interfaces between the user and system~\cite{yamazaki06}, a goal that is partially realized in smart assistants~\cite{purington2017alexa}, but with limited ability to adapt to more complex user commands~\cite{kiseleva2016understanding}. Ample prior work has approached the issue of context-awareness using task-specific models~\cite{qolomany2019leveraging, kabir2015machine, machorro2020hems, liang2018unsupervised}. While these methods can achieve high performance given ample task-specific data, we believe that the high zero-shot performance of LLMs could hint at better generalizability without a need for training data. However, we are aware of no work to-date that has explored the use of task-agnostic LLMs for deeper contextual reasoning in smart environments. This motivates our feasibility study.