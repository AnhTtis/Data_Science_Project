An exciting prospect of smart homes at their advent was the potential to reduce user burden by providing seamless, unobtrusive, and ``smart'' interfaces to everyday devices. While smart assistants have improved significantly over the years with respect to speech recognition~\cite{swarup2019improving, raju2019scalable} and user satisfaction~\cite{purington2017alexa, lopatovska2019talk}, a central challenge remains: how can these assistants be made to respond appropriately to ambiguous user commands that may be influenced by context or are otherwise impossible for system developers to anticipate beforehand? An example of such a command might be a user preparing their home to entertain for guests, who asks their smart assistant to ``get ready for a party''. The hope is that the assistant---if it is truly smart---might be able to help by inferring the meaning of the statement and determining how to change the state of available devices in response: perhaps to start up the user's party playlist on a smart speaker and change their smart lights to a festive color scheme. In practice, however, such a request is beyond the capacity of current smart home systems. Google Home will sadly admit: ``I'm sorry, I didn't understand.''

In this paper, we are motivated by the observation that large language models (LLMs) like OpenAI's GPT-3~\cite{brown2020language} have shown an impressive ability to generalize to new tasks with high zero-shot performance, as well as the capacity to infer meaning behind semantically complex or abstract statements~\cite{liu2021gpt}. We thus ask the question: can this powerful capacity for cross-domain contextual reasoning be applied to practical smart home applications? 

To explore this question, we carry out a feasibility study that places GPT-3 in control of a smart home. We evaluate GPT-3's ability to provide high-quality responses to user commands of varying ambiguity given only a simple prompt and a data structure containing information about devices that it can control. Our results demonstrate that LLMs like GPT can infer the meaning behind ambiguous user commands like ``get ready for a party'' or ``I am tired and I want to sleep'' and respond with properly-formatted data describing courses of action, enabling more intuitive control of smart devices. We furthermore build a proof-of-concept implementation that puts GPT-3 in control of real devices, showing LLM-driven command inference and action planning can function in practice \emph{with no fine-tuning or task-specific training required}. Motivated by our results, we propose future work that can further leverage the power of LLMs toward building smarter smart home applications.

Our key contributions are as follows:

\begin{itemize}
    \item An experimental setup and study results that show LLMs can infer meaning behind abstract user commands like ``I am tired and I have to work'' and, in response, quickly and appropriately change the state of the smart devices available in the home, \emph{with no task-specific training}.
    \item An implementation that puts a GPT model in control of real devices, showing that it can intuitively respond to a variety of commands. When told to ``set up for a party'', it responds by turning on a stereo and configuring a group of Hue lights to loop through a festive set of colors; given the command ``I'm leaving'', it turns off all available devices. We trigger these actions by inputting the LLM's response \emph{directly into smart device APIs}.
    \item Analytical results that suggest responses are variable in quality, dependent on both the devices available and the nature of the user's command. In essence, further system design is necessary to manage the LLM's tendency to ``not know what it doesn't know'' in order to produce consistently high-quality responses.
\end{itemize}

The following describes the structure of this paper. Section~\ref{related-work} situates our work with related research. Section~\ref{approach} describes the experimental setup that we use to demonstrate the feasibility of LLMs as smart home controllers. Section~\ref{evaluation} presents the results of our exploratory study, while Section~\ref{implementation} demonstrates a proof-of-concept implementation. Section~\ref{limitations} offers avenues for future work. Section~\ref{conclusion} concludes.