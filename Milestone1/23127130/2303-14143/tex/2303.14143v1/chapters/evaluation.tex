This section describes the results of our feasibility study using the experimental setup described in the previous section. Our evaluations address two high-level questions:

\begin{enumerate}
    \item \textbf{How good are the agent's responses?} We measure the quality of the agent's responses, in the sense that they include courses of action that can reasonably be thought to meet the user's request and can be easily machine-parsed and executed.
    \item \textbf{How timely are the agent's responses?} We also measure the round-trip response latency. This hints at how feasible a practical system is with respect to user experience and responsiveness.
\end{enumerate}

To better understand the system from these two perspectives, we design scenarios of increasing complexity and ambiguity of \emph{context} and \emph{command}. This captures the intuition that (1)~different smart homes can have different complexity of context, from an apartment with a few smart lights to a large home with many devices and (2)~different user commands can have different levels of ambiguity, from direct commands like ``turn on the light'' to wholly ambiguous statements like ``I am tired''. Evaluating agent responses under these circumstances allows us to identify the failure modes of LLM-driven smart home control given increasingly challenging prompts with respect to both the context and the nature of the command. 

We use three contexts of increasing complexity, as follows:

\begin{itemize}
    \item \textbf{Simple:} Describes a home with a bedroom and living room that have one and two lights, respectively, all initially off. Lights can either be on or off but have no other state (e.g. color).
    \item \textbf{Medium:} Same as above, but adds red, green, and blue color state to each of the lights, with expected values in the range [0, 255].
    \item \textbf{Complex:} Same as above, but adds a TV with on/off and volume state to the bedroom, as well as a TV and smart speaker to the living room (each also with on/off and volume state).
\end{itemize}

Each of these contexts is expressed in the schema described in Section~\ref{approach}. We combine these contexts with three user prompts of increasing ambiguity, as follows:

\begin{itemize}
    \item \textbf{Direct:} {\it ``Turn on the light.''} This command is simple since it directly expresses a state change, as well as a relevant device. Existing home assistants can easily respond to this type of command.
    \item \textbf{Indirect:} {\it ``Get ready for a party.''} This command is more ambiguous since it expresses a desired state change, but provides no information about which devices are relevant.
    \item \textbf{Ambiguous:} {\it ``I am tired.''} This command is completely ambiguous since it expresses neither a state change, nor which devices might be relevant.
\end{itemize}

We run our tests with each possible combination of these three contexts and commands (9 total), each for 10 trials. We save the agent's response for each trial in a human-readable format, then perform manual rating to measure the quality of the responses. Our process for rating the quality of responses is based on a binary label, where each is assigned one of the following labels based on its quality:

\begin{itemize}
    \item \textbf{Poor (0):} ``The changes to the devices do not at all reflect the intent behind the user command, or the response is malformed/garbled.''
    \item \textbf{Good (1):} ``The changes to the devices are reasonable for the command. You can imagine \emph{someone} being satisfied with the result, even if it is somewhat subjective (e.g., based on different personal preferences).''
\end{itemize}

Three researchers independently reviewed all responses and assigned them a label. We report the aggregate score for each trial as the average across all assigned scores. We also note the average latency for each trial---this includes both the network transmission time of the request, as well as the inference time taken by the model. Since this time is subject to network conditions and API demand, it should be taken as a rough estimate rather than a concrete benchmark. Our results are summarized in Table~\ref{tab:results}.%\cj{Can you think about a different way to present this information? It's a lot to process and at a glance I'm not immediately sure what is ``good''}

\textbf{Response time is a function of context complexity.} With respect to latency, we can see that responses generally arrive on the order of seconds, meaning that a practical system could feasibly leverage an LLM for ambiguous command inference and action planning without significant detriments to user experience or responsiveness. For direct commands, a 2 to 3 second response time may be too long---future system designs could thus leverage the LLM only for commands that require it. This may entail a hybrid of rule-based inference for common commands, along with LLM inference for less familiar commands. It is also worth noting that as the context increases in complexity, the response latency also increases---this motivates future work to develop methods for filtering context prior to prompting the agent so that only the most relevant information is provided. 

\textbf{Response quality is a function of context and command ambiguity.} With respect to response quality, we find that the LLM approach provides good responses given the same direct and simple commands that current home assistants are able to service. Note, however, that unlike existing home assistants, the LLM approach utilizes a much simpler system architecture that performs command inference and action planning in the same pass. These results are consistent given increasing degrees of context complexity, suggesting that the model was not overwhelmed by the growth in the decision space that comes with adding new devices and possible state changes. On the contrary, the model provides \emph{better} responses when given more context that might be relevant to the user's command, as is apparent when comparing the low response quality of the Simple/Indirect and Medium/Indirect experiments against the Complex/Indirect experiment. Upon inspection of the responses, the reason for this is clear: given minimal context and a subjective command like ``get ready for a party'', the LLM simply makes up a response---specifically, it turns on all the lights in the house, to include the bedroom. When we add a speaker and a television to the context, the model now has more relevant knobs to turn, and produces a higher quality response. The tendency to make something up when the answer is unknown or requires more context is an open problem and motivates the development of application-specific methods to mediate between the user and the model. 

For the most ambiguous command (``I am tired''), we note that the model delivers poor responses regardless of context. In all but a few cases, the LLM simply turns on all of the home's lights. The exception is in a Medium/Ambiguous trial, where it only turns on the bedroom lamp, perhaps to help the user prepare for bed. This is to be expected: an individual's intent and preference in this case are highly subjective (are they, e.g., tired and ready for bed or tired but they have a pressing deadline?) and the LLM ultimately cannot read the user's mind. However, since the LLM does not ``know what it doesn't know'', it does not ask for clarification or go with the safest choice, which is likely to do nothing. Instead, it makes something up.

Since our previous results suggested more context is often beneficial, we dig deeper to see if we can help it make a better choice. We amend the vague command ``I am tired'' to offer hints at the user's intent:

\begin{itemize}
    \item \textbf{Ambiguous*:} ``I am tired and I need to work.''
    \item \textbf{Ambiguous**:} ``I am tired and I want to sleep.''
\end{itemize}

Provided this added hint at the user's context, the response quality improves significantly. For Ambiguous*, the model consistently responds by turning on the living room lights while leaving all other devices off; for Ambiguous**, the model turns on only the user's bedside lamp and, in some cases, reduces the volume on their speaker and TV. Note that although the amended statement includes additional context, it still requires the model to infer meaning in a way that more rigid or rule-based approaches cannot.

\begin{table}[t!]
  \centering
  \begin{tabular}{|l | l | l | l|} \hline
    \textbf{Context} & \textbf{Command} & \textbf{Avg. Quality} & \textbf{Avg Latency (sec)} \\ \hline
    Simple  &  Direct & 1.00  & 2.42 \\
      &  Indirect &  0.67 & 2.31 \\
      &  Ambiguous & 0.00 & 2.22 \\ \hline \hline
    Medium  &  Direct & 1.00 & 4.56 \\
      &  Indirect & 0.63 & 4.70 \\
      &  Ambiguous & 0.17 & 4.97 \\ \hline \hline
    Complex  &  Direct & 1.00 & 7.90 \\
      &  Indirect & 1.00 & 7.25 \\
      &  Ambiguous & 0.00 & 7.04 \\ \hline \hline \hline
    Complex & Ambiguous* & 1.00 & 7.49 \\
     & Ambiguous** & 1.00 & 8.09 \\
    \hline
  \end{tabular}
  \caption{Results for experiments given various combinations of different context complexity and command ambiguity. Higher quality responses suggest the model produced a course of action that would be desirable for an end user (e.g., turning on the bedroom light when receiving the command ``I am tired and I want to sleep''). Lower latency suggests better system responsiveness.}
  \label{tab:results}
\end{table}