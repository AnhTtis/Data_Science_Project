\subsection{Related Work}
This paper is related to a large body of research, which can be classified into three main fields: multi-objective optimization problems, approximation variants of known solution concepts, and algorithms for finding optimal leximin solutions.

In general multi-objective\footnote{Multi-objective is also called \emph{multi-criteria} (for example in~\cite{ehrgott_multicriteria_2005}).} optimization problems, finding a leximin\footnote{Leximin is also called \emph{Max-Min fairness} (for example in~\cite{nace_max-min_2008}), \emph{Lexicographic Min-Max} (for example in~\cite{Ogryczak_2006}), \emph{Lexicographic max-ordering} (for example in~\cite{ehrgott_multicriteria_2005}) and \emph{Leximax} (for example in~\cite{henzinger_et_al_FORC_2022}).} optimal solution is quite common goal~\cite{ehrgott_multicriteria_2005}, which is still an open challenge.
Studies on this topic are usually focused on a specific problem and leverages its special characteristics --- the structure of the \textit{feasible region} and \textit{objective-functions} that describe the concrete problem at hand.
In this paper, we focus on the widely studied domain of resource allocation problems~\cite{moulin2004fair}.
In that context, as leximin maximization is an extension of egalitarian welfare maximization, it is usually mentioned when fairness is desired.

There are cases where a leximin optimal solution can be calculated in polynomial time, for example in: fair allocation of divisible items~\cite{willson}, giveaway lotteries~\cite{arbiv_fair_2022}, portioning with ordinal preferences~\cite{ airiau_portioning_2019}, cake sharing~\cite{bei_truthful_2022}, multi-commodity ﬂow networks~\cite{nace_max-min_2008}, and location problems~\cite{Ogryczak_1997_loc}.
However, even when algorithms are theoretically polynomial, they can still be inaccurate in practice, for example due to numeric round-off errors.


In other cases, calculating a leximin optimal solution is NP-hard, for example in: representative
cohort selection~\cite{henzinger_et_al_FORC_2022}, fair combinatorial auctions~\cite{bouveret_computing_2009}, package upgrade-ability~\cite{cabral2022sat}, allocating papers to referees~\cite{garg_assigning_2010, lian_conference_2018}, and stochastic allocations of indivisible goods (Section \ref{sec:app} in this paper).
However, to our knowledge, 
studies of this kind typically suggest non-polynomial algorithms and heuristics for solving small instances of the general problem and empirically evaluate their efficiency, rather than suggesting polynomial-time approximation algorithms.

\iffalse % EREL: Removed for now - to complete later
Another approach to approximate leximin (and in fact, to approximate other solution concepts) is to suggest ad-hoc variants that are meaningful for the given problem.
% For example, \textcite{danna_2017_upward}
For example, Danna et al.~\cite{danna_2017_upward}
suggest a leximin relaxation for the problem of multicommodity flow. 
An instance of the problem is described by a graph with capacity constraints, and $n$ tuples $(s_i,t_i,P_i)$ such that $s_i$ is the source node, $t_i$ is the target node, and $P_i$ is a subset of paths between $s_i$ and $t_i$; the objective of commodity $i$ is the sum over the flow value of all paths.
\eden{TODO}
% ($\sum_{p \in P_i} f_i(p)$).
% In this problem, the feasible region is described by a graph with capacity constraints, and the objectives are flow values of the commodities.
Their relaxation says that a multicommodity flow is approximately optimal\footnote{aa} if for any $i \in [n]$, the $i$-th smallest utility cannot be increase if the \emph{current} flow routing for the smaller or equal commodities is fixed. 
Their definition can be adapted to other problems, for example, in fair division, we can say that an allocation is approximately optimal if for any $i \in [n]$, the $i$-th smallest utility cannot be increase if the items allocated to the agents with smaller or equal utility is fixed. 
a leximin optimal solution\footnote{The leximin optimal solutions are referred in their paper as max-min fair allocations.} is one which for each $k$
\textcite{garg_assigning_2010} considered the problem of allocating papers to referees for peer review. They model the solution space of assignments using integer variables, then relax those variables so that they can use a linear programming algorithm to find a fractional assignment and eventually round it.
The rounding technique should fit the specific problem.
\eden{TODO}
\erel{This is not clear - can you add more details?}
Although it makes a lot of sense, in this paper, we aim to suggest a definition that is as general as possible. 
\fi

% WLU
% \eden{maybe to combined the following two paragraphs and add the explanation about the weights?~\cite{yager_analytic_1997} how is the following?}\eden{changed from here: }
Another approach to leximin optimization is to represent the leximin ordering by an aggregation function. 
Such a function takes a utility vector and returns a number, such that a solution is leximin-preferred over another if-and-only-if its aggregate number is higher.
Finding such a function will of course reduce the problem to solving only one single-objective optimization problem.
Unfortunately, it is known that no aggregate function can represent the leximin ordering in \emph{all} problems~\cite{moulin2004fair,Ogryczak2004TelecommunicationsND}.
%
Still, there are interesting cases in which such functions can be found. 
For example, Yager~\cite{yager_ordered_1988} suggested that the \emph{ordered weighted averaging (OWA)} technique can be used when there is a lower bound on the difference between any two possible utilities. 
However, it is unclear how (and whether) approximating the aggregate function would translate to approximating leximin.
%
% Further... maybe to say somthing about the (mixed-integer linear program) algotirhm...? 
%
% \eden{instead of:
% \textit{In~\cite{sherali1983preemptive} it is proved that, for any linear lexicographic optimization problem, there exists a set of weights such that the weighted average of the objectives is maximized if and only if the solution is lexicographically optimal; however, they mention that finding this set of weights for each particular problem is not practical.
% %
% There is a great deal of research in which the goal is to find functions that are as close to the ideal as possible (for example,~\cite{yager_ordered_1988} in which \emph{ordered weighted averaging (OWA)} technique was introduced).
% % also: (for example in~\cite{yager_ordered_1988,yager_analytic_1997,lian_conference_2018}). 
% Unfortunately, it is known that no aggregate function can represent the leximin ordering in \emph{all} problems~\cite{moulin2004fair,Ogryczak2004TelecommunicationsND}.}}
% % \eden{ moulin2004fair at page 72}. \eden{ Ogryczak2004TelecommunicationsND at page 3}.

\iffalse % EREL: Removed from now. To complete later.
\eden{todo:}
Since we aim for a definition that in the absence of errors would be identical to the original definition, we neglect this approach. 
\erel{It is not entirely clear how the last sentence follows from the previous one.}
\fi

% general definition
To the best of our knowledge, other general approximations of leximin exist but they are less common.
They are usually mentioned in the context of robustness or noise (e.g.~\cite{henzinger_et_al_FORC_2022,kalai_lexicographic_2012}) and lack characteristics that we emphasize within the context of errors.
% \eden{todo}
% ; therefore, we also consider approximate variants of related solution concepts alongside it.
% The following are some examples.
%--------
% Kalaı et al.~\cite{kalai_lexicographic_2012} presented a new preference relation that aims to relax the leximin order.
% \erel{Is it the same as our second example above? If not, I suggest to give more details and an example, as this is a "competitor" of our approach --- an alternative relaxation to leximin.}\eden{It is not the same, their definition is more complex. I have a version with more details but I wasn't sure how to explain it without notations (which we haven't provided yet).}
% \eden{Their preference relation can be viewed as follows (it is presented in our context of utility maximization instead of their context of cost \emph{minimization}):
% an alternative $y$ is $\alpha$-preferred over an alternative $x$ if  there exists an integer $k \in [n]$ such that the element-wise difference between the $(k-1)$ smallest values achieved by $y$ and $x$ is at most $\alpha$, and the $k$'th smallest value achieved by $y$ is higher by more than $\alpha$ than the $k$'th smallest value achieved by $x$, that is:
% \begin{align*}
%     \forall j < k \colon &|\valBy{j}{y} - \valBy{j}{x}| \leq \alpha\\
%     & \valBy{k}{y} > \valBy{k}{x} + \alpha
% \end{align*}}
% However, the leximin optimal solution is not a maximum element according to this order.
% Several approximation variants of leximin were presented by \textcite{henzinger_et_al_FORC_2022}, some of them a 
 % This might make sense in the context of robustness (which they investigated), but not in our case.
%
%~\cite{aziz_2022} 
% % EREL: Note you can use \textcite{...} to automatically add the authors' names to the citation.
% presented a relaxation of the \emph{Maximin Share (MMS)} solution concept, according to which a solution is approximately optimal if each agent gets at least some fraction of the value it would have gotten from an optimal solution.
% \erel{Do you mean this maximin share? \url{https://en.wikipedia.org/wiki/Maximin-share} It was introduced much before Aziz et al. And I do not think it is a relaxation of leximin, so it is not relevant.}
% \eden{I tried to say that Aziz et al. introduce an \emph{approximation} for maximin share and explain why the same \emph{idea} cannot be used to approximate leximin. would it be better to remove it?}
% However, this approach would not work in the context of leximin, since allowing one value to be smaller might increase the following optimal values.
% \textcite{aumann_pareto_2010} proposed a way to quantify the \emph{distance} from Pareto efficiency.
% In fact, their approach is very similar to ours as they defined a new preference relation and built the approximation definition upon it
% % Their approach is actually very similar in spirit to our approach as they also defined a new \eden{stronger} preference relation and the approximation definition is build on top of that, i.e., an approximately optimal solution is one on which no other solution is dominates.

%Henzinger et al
Most similar to our work is the recent paper by Henzinger et al.~\cite{henzinger_et_al_FORC_2022}.
This paper presents several approximation variants of leximin for the case of \emph{additive} errors in the single-objective problems.
Their motivation is different than ours --- they use approximation as a method to improve efficiency and ensure robustness to noise. However, one of their definitions,  (\emph{$\epsilon$-tradeoff Leximax}) fits our motivation of achieving the best possible leximin-approximation in the presence of errors.
In fact, our approximation definition can be viewed as a generalization of their definition to include both multiplicative and additive errors.
It should also be noted that the authors mention multiplicative approximation in the their Future Work Section.
% \eden{I removed the quote (as  was suggested by one of the reviewers)}
% :\begin{quote}
% \textit{In another direction, our approximation notions all reason about allowing for additive
% amounts of error. However, considering what notions, especially those in line with $\epsilon$-significant
% recursive leximax, might arise from multiplicative error could be a useful direction to explore.}
% \end{quote}


\iffalse % EREL: We already mentioned this in the introduction - no need to repeat.
Given our definition (described in Section \ref{sec:approx-leximin-def}), our second goal is an algorithm that in the absence of errors would return a leximin optimal solution, while in the presence of errors would return an appropriate approximation.
However, existing leximin optimization algorithms may return solutions that are very far from the optimum when errors are present.
For instance, as the leximin concept considers only the order of outcomes rather than identities, a very common idea in leximin optimization is \emph{saturation}.
According to this idea, at each point, the algorithm fixes the values of the objectives that cannot be further improved, i.e., are saturated.
Indeed, different versions of the same algorithm, which is based on this idea, have been independently proposed many times, for example in~\cite{Ogryczak2004TelecommunicationsND, willson,airiau_portioning_2019,nace_max-min_2008}\footnote{The algorithm is fully described in Appendix \ref{sec:saturation-algorithm}.}.
In this case, errors in the single-objective solver might cause this algorithm to never end.
\eden{should we include the example? if so, where?}
\erel{If we describe the algorithm in Appendix \ref{sec:saturation-algorithm}, then we should present the examples afterwards in the same appendix.
But maybe it is better to describe both the algorithm and the example at the intro, after the examples for the definitions.}
\fi
