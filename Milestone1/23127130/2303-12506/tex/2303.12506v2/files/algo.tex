\displayEcai{\newpage}
\section{Approximation Algorithm}\label{sec:algo-short}
We now present an algorithm for computing a leximin approximation. 
The algorithm is an adaptation of one of the algorithms of Ogryczak
and {\'{S}}liwi{\'{n}}ski \cite{Ogryczak_2006} for finding exact leximin optimal solutions. 
% \eden{is the following more clear? the comment: \textit{It appears to say that the output of a single-objective function is approximately optimal for (some) multi-objective problem.}}
%
% We prove that, given an $(\multApprox,\additiveApprox)$-approximation oracle for the single-objective problem, the algorithm outputs an $\left(\frac{\multApprox^2}{1-\multApprox + \multApprox^2}, \frac{\additiveApprox}{1-\multApprox +\multApprox^2}\right)$-leximin-approximation for the multi-objective problem.

\subsection{Preliminary: exact leximin-optimal solution}
Following the definition of leximin, the core algorithm for finding a leximin optimal solution is iterative, wherein one first maximizes the least objective function, then the second, and so forth. 
In each iteration, 
$t=1,\ldots,n$, 
% $1 \leq t \leq n$,
it looks for the value that maximizes the $t$-th smallest objective, $z_t$, given that for any $i < t$ the $i$-th smallest objective is at least $z_i$ (the value that was computed in the $i$-th iteration).
The core, single-objective optimization problem is thus:
\begin{align}
 \max &&&\ztVar{x}   \tag{\progBasic}\label{eq:basic-OP}\\
        s.t. &&& (\text{\progBasic.1}) \Hquad x \in S \nonumber\\
              &&& (\text{\progBasic.2}) \Hquad \valBy{\ell}{x}\geq z_{\ell} & \forXinY{\ell}{t-1} \nonumber \\
               &&& (\text{\progBasic.3}) \Hquad \valBy{t}{x} \geq \ztVar{x} \nonumber   
\end{align} 
where the variables are the scalar $\ztVar{x}$ and the vector $x$, whereas $z_1, \ldots z_{t-1}$ are constants (computed in previous iterations).

Suppose we are given a procedure $\textsf{OP}(z_1,\ldots,z_{t-1})$, which, given  $z_1,\ldots,z_{t-1},$ outputs $(x,z_t)$ that is the exact optimal solution to \eqref{eq:basic-OP}.  
Then, the \emph{leximin} optimal solution is obtained by iterating this process for $t=1,\ldots,n$, as described in Algorithm \ref{alg:basic-ordered-Outcomes}.
\displayEcai{
The algorithm first maximizes the smallest objective $\valBy{1}{x}$, and puts the result in $z_1$.
Then 
it maximizes the second-smallest objective $\valBy{2}{x}$,
subject to $\valBy{1}{x}$ being at least $z_1$, and puts the result in $z_2$; and so on.
}
\begin{algorithm}[!tbp]
\caption{The Ordered Outcomes Algorithm}
\label{alg:basic-ordered-Outcomes}
\begin{algorithmic}[1] %[1] enables line numbers
\FOR{$t=1$ to n}
\STATE \( %\begin{align*}
(x_t,z_t)\leftarrow \textsf{OP}(z_1,\ldots,z_{t-1})
\) %\end{align*} 
\ENDFOR
% \STATE \textbf{return} {$x_n$ (with objective values $z_1,\ldots,z_n$)}.
% the above it true only for the exact case 
\STATE \textbf{return} {$x_n$ (with objective values $f_1(x_n),\ldots,f_n(x_n)$)}.
\end{algorithmic}
\end{algorithm}

Since constraints (\progBasic.2) and (\progBasic.3) are not linear  with respect to the objective-functions, it is difficult to solve the program \eqref{eq:basic-OP} as is. 
\cite{Ogryczak_2006} suggests a way to ``linearize`` the program in two steps. 
%
% two additional problems that can be solved within Algorithm \ref{alg:basic-ordered-Outcomes}, instead of \eqref{eq:basic-OP}, such that it will still output a leximin optimal solution.
%
% \eden{I removed the algorithm run description}
\displayEcai{

}
% In the first step,
First,
we replace \eqref{eq:basic-OP} with a the following program, that considers sums instead of individual values (where again the variables are $\ztVar{x}$ and $x$):
% , and $z_1, \ldots , z_{t-1}$ are constants):  
\begin{align*}
\max &&&\ztVar{x} \tag{\progSums}\label{eq:sums-OP}\\
s.t. &&& (\text{\progSums.1}) \quad x \in S \\
&&& (\text{\progSums.2}) \quad \sum_{i \in F'} f_i(x) \geq \sum_{i=1}^{|F'|}  z_i && \forall F' \subseteq [n], \Hquad |F'| < t \\
&&& (\text{\progSums.3}) \quad \sum_{i \in F'} f_i(x) \geq \sum_{i=1}^{t}  z_i  && \forall F' \subseteq [n], \Hquad |F'| = t
\end{align*}
Here, constraints (\progBasic.2) and (\progBasic.3) are replaced with constraints (\progSums.2) and (\progSums.3), respectively. 
Constraint (\progSums.2) says that for any $\ell<t$, the sum of any $\ell$ objectives is at least the sum of the first $\ell$ constants $z_i$
(equivalently: the sum of the smallest $\ell$ objectives is at least the sum of the first $\ell$ constants $z_i$\footnote{A formal proof of this claim is given in Appendix \ref{sub:equivalence-P2-P3}}). 
Similarly, (\progSums.3) says that the sum of any $t$ objectives (equivalently: the sum of the smallest $t$ objectives) is at least the sum of the first $t-1$ constants  $z_i$, plus the variable $z_t$.
% \eden{I removed the algorithm run description}

\displayEcai{
Suppose \eqref{eq:basic-OP}
is replaced with \eqref{eq:sums-OP}
in Algorithm\ref{alg:basic-ordered-Outcomes}.
Then, in the first iteration, the algorithm still maximizes the smallest objective $\valBy{1}{x}$, and puts the result in $z_1$.
In the second iteration, it maximizes the \emph{difference} between the sum of the two smallest objectives $\valBy{1}{x}+\valBy{2}{x}$ and $z_1$,
subject to $\valBy{1}{x}$ being at least $z_1$, and puts the result in $z_2$.
Since $z_1$ is the maximum value of $\valBy{1}{x}$, being at least $z_1$ becomes being exactly $z_1$, which means that, as 
 was for \eqref{eq:basic-OP}, the algorithm actually maximizes $\valBy{2}{x}$, subject to $\valBy{1}{x}$ being at least $z_1$.
\eden{is it clear enough?}
%
% Notice that, as $z_1$ is constant, this difference is maximized when the sum $\valBy{1}{x}+\valBy{2}{x}$ is.
% Since constraint (\progSums.2) ensures that $\valBy{1}{x}$ obtains its maximum value $z_1$, this occurs when $\valBy{2}{x}$ is maximized, subject to $\valBy{1}{x}$ being at least $z_1$.
% Also, as $\valBy{1}{x}$ is exactly $z_1$, it is cleat that the difference describes the   
% Accordingly, as before, $z_2$ is the maximum value for $\valBy{2}{x}$ subject to  $\valBy{1}{x}$ being at least $z_1$.
%
% or:
% Notice that as $z_1$ is the the highest value for $\valBy{1}{x}$, actually, as before, $z_2$ is the maximum value for $\valBy{2}{x}$ subject to  $\valBy{1}{x}$ being at least $z_1$.
%
%
% In the third iteration, 
% the algorithm maximizes the difference between the sum $\sum_{i=1}^3\valBy{i}{x}$ and the sum $z_1 + z_2$,
% subject to $\valBy{1}{x}$ being at least $z_1$ and $\valBy{1}{x}+\valBy{2}{x}$ being at least $z_1+z_2$; and puts the result in $z_3$.
% Similarly to the previous iteration, it can be concluded that it actually maximizes $\valBy{3}{x}$, subject to $\valBy{2}{x}$ being at least $z_2$ and $\valBy{1}{x}$ being at least $z_1$ (as if \eqref{eq:basic-OP} was used).
% And so on.
Similarly for any iteration $1 <t\leq n$, as the sum $\sum_{i=1}^{\ell} z_i$ is the maximum value of $\sum_{i=1}^{\ell} \valBy{i}{x}$ for all $1\leq \ell < t$, it can be concluded that the algorithm actually maximizes $\valBy{t}{x}$, subject to $\valBy{\ell}{x}$ being at least $z_{\ell}$ for $1\leq \ell < t$ (as if \eqref{eq:basic-OP} was used).
Accordingly, the algorithm still finds a leximin-optimal solution.
}

% Then, in the first iteration, the algorithm still maximizes the smallest objective $\valBy{1}{x}$, and puts the result in $z_1$.
% In the second iteration, it maximizes the sum of the two smallest objectives $\valBy{1}{x}+\valBy{2}{x}$,
% subject to $\valBy{1}{x}$ being at least $z_1$, and puts the result in $z_2$.
% In the third iteration, 
% it maximizes $\valBy{1}{x}+\valBy{2}{x}+\valBy{3}{x}$,
% subject to $\valBy{1}{x}+\valBy{2}{x}$ being at least $z_1+z_2$, and puts the result in $z_3$; and so on.
% It is easy to see that the algorithm still finds the leximin-optimal solution.

%---option 2
% In the second iteration, it maximizes the sum of the two smallest objectives $\valBy{1}{x}+\valBy{2}{x}$,
% subject to $\valBy{1}{x}$ being at least $z_1$, and puts the difference ($\valBy{1}{x}+\valBy{2}{x} - z_1$) in $z_2$.
% In the third iteration, 
% it maximizes $\valBy{1}{x}+\valBy{2}{x}+\valBy{3}{x}$,
% subject to $\valBy{1}{x}$ being at least $z_1$ and $\valBy{1}{x}+\valBy{2}{x}$ being at least $z_1+z_2$, and puts the difference ($\valBy{1}{x}+\valBy{2}{x}+\valBy{3}{x} - z_1 - z_2$) in $z_3$; and so on.

While \eqref{eq:sums-OP} is linear with respect to the objective-functions, it has an exponential number of constraints.
To overcome this challenge, auxiliary variables were used in the second problem ($y_{\ell}$ and $m_{\ell,j}$ for all $1 \leq \ell \leq t$ and $1 \leq  j\leq n$):
\begin{align}
\max &&& \ztVar{x} \tag{\progLinear}\label{eq:vsums-OP} \\
s.t. &&& (\text{\progLinear.1}) \Hquad x \in S \nonumber  \\
                    &&& (\text{\progLinear.2}) \Hquad \ell y_{\ell} - \sum_{j=1}^n m_{\ell,j}\geq \sum_{i=1}^{\ell}  z_i && \forXinY{\ell}{t-1} \nonumber \\
                    &&& (\text{\progLinear.3}) \Hquad t y_t - \sum_{j=1}^{n} m_{t,j} \geq \sum_{i=1}^{t}  z_i  \nonumber \\
                    &&& (\text{\progLinear.4}) \Hquad m_{\ell,j} \geq y_{\ell} - f_j(x)  && \forXinY{\ell}{t},\Hquad \forXinY{j}{n} \nonumber \\
                    &&& (\text{\progLinear.5}) \Hquad m_{\ell,j} \geq 0  && \forXinY{\ell}{t},\Hquad \forXinY{j}{n} \nonumber
\end{align}
% This is a polynomial-sized problem.
% \eden{we can also use $1 \leq \ell \leq t,\Hquad 1 \leq j  \leq n$ but $\ell = 1, \ldots,t,\Hquad j = 1, \ldots,n$ is too ling}
% \eden{should probably change the name P3 if we no longer have P2}
The importance of the problems \eqref{eq:sums-OP} and \eqref{eq:vsums-OP} for leximin is shown by the following theorem (that combines  Theorem 4 in \cite{Ogryczak2004TelecommunicationsND} and Theorem 1 in \cite{Ogryczak_2006}):
\begin{theorem*}
% \label{lemma:alg-1-can-use-sums-exact}
If Algorithm \ref{alg:basic-ordered-Outcomes} is applied with a solver for \eqref{eq:sums-OP} or \eqref{eq:vsums-OP}\footnote{If the algorithm uses a solver for \eqref{eq:vsums-OP}, it takes only the assignment of the variables $x$ and $z_t$
, ignoring the auxiliary variables.}  
(instead of for \eqref{eq:basic-OP}), the algorithm still outputs a leximin-optimal solution. 
\end{theorem*}
% \noindent For completeness, Appendix \ref{sec:algo-sec-proofs} provides an alternative proof of this lemma. \eden{to add it to the section}
%--------
% Our main result (Theorem \ref{th:main})  extends their theorem from exact leximin-optimal solutions to $(\alpha,\epsilon)$-approximate leximin solutions for every $\alpha \in(0,1]$ and $\epsilon\geq 0$.
We shall later see that our main result (Theorem \ref{th:main}) extends and implies their theorem. 


\subsection{Using an approximate solver}
Now we assume that, instead of an exact solver in Algorithm \ref{alg:basic-ordered-Outcomes}, we only have an approximate solver. 
In this case, the constants $z_1,\ldots,z_{t-1}$ are only approximately-optimal solutions for the previous iterations.
It is easy to see that if \textsf{OP} is an $(\multApprox,\additiveApprox)$-approximation algorithm
% \footnote{See Section \ref{sec:preliminaries} for the  formal definition of approximation algorithm.} 
to \eqref{eq:basic-OP}, then Algorithm \ref{alg:basic-ordered-Outcomes} outputs an $(\multApprox,\additiveApprox)$-leximin-approximation\footnote{A formal proof is given in Appendix \ref{sub:approximate-P1}}.
% \footnote{See Section \ref{sec:approx-leximin-def} for the formal definition of approximately leximin-optimal solution.}
%
% (a formal proof is given in Appendix \ref{sub:approximate-P1}).

In contrast, for \eqref{eq:sums-OP} and \eqref{eq:vsums-OP}, we shall see that  Algorithm \ref{alg:basic-ordered-Outcomes} may output a solution that is \emph{not} an $(\multApprox,\additiveApprox)$-leximin-approximation.
However, we will prove that it is not too far from that --- in this case, the output is always an $\left(\frac{\multApprox^2}{1-\multApprox + \multApprox^2}, \frac{\additiveApprox}{1-\multApprox +\multApprox^2}\right)$-leximin-approximation.


In order to demonstrate both claims more clearly, we start by proving that the problems \eqref{eq:sums-OP} and \eqref{eq:vsums-OP} are \emph{equivalent} in the following sense:
\begin{lemma}
\label{lem:equivalence}
    Let $1 \leq t \leq n$ and let $z_1, \ldots z_{t-1} \in \mathbb{R}$.
    Then, $(x, z_t)$ is feasible for \eqref{eq:sums-OP} if and only if there exist $y_{\ell}$ and $m_{\ell,j}$ for $1 \leq \ell \leq t$ and $1 \leq j \leq n$ such that $\left(x, z_t, (y_1, \ldots, y_t), (m_{1,1}, \ldots m_{t,n})\right)$ is feasible for \eqref{eq:vsums-OP}.
\end{lemma}
The proof  is provided  in Appendix \ref{sub:equivalence-P2-P3}.
\displayEcai{

}
Since both  \eqref{eq:sums-OP} and \eqref{eq:vsums-OP} have the same objective function ($\max z_t$), the lemma implies that $(x,z_t)$ is an $(\alpha,\epsilon)$-approximate solution for \eqref{eq:sums-OP} if and only if $(x,z_t)$ is a part of an $(\alpha,\epsilon)$-approximate solution for \eqref{eq:vsums-OP}.
Thus, it is sufficient to prove the theorems for only one of the problems. 
We will prove them for \eqref{eq:sums-OP}.

\begin{theorem}
There exist $\multApprox\in (0,1]$, $\additiveApprox \geq 0$ and \textsf{OP} that is an $(\multApprox,\additiveApprox)$-approximation procedure to \eqref{eq:sums-OP}, such that if Algorithm \ref{alg:basic-ordered-Outcomes} is applied with this procedure, it might return a solution that is not 
an $(\multApprox,\additiveApprox)$-leximin-approximation.
\end{theorem}
\begin{proof}
% in order to show that Algorithm \ref{alg:basic-ordered-Outcomes} might return a solution that is \emph{not} approximately-optimal, 
Consider the following multi-objective optimization problem with $n=2$:
\begin{align*}
    \max \quad &\{f_1(x) := x_1, f_2(x) := x_2\} \\
    s.t. \quad  & (1.1) \Hquad x_1 \leq 100, \quad (1.2) \Hquad x_1 + x_2 \leq 200, \quad (1.3) \Hquad x \in \mathbb{R}^2_{+}
\end{align*}
In the corresponding \eqref{eq:sums-OP}, constraint (\progSums.1) will be replaced with constraints (1.1)-(1.3).
The following is a possible run of the algorithm with \textsf{OP} that is a $(0.9,0)$-approximate solver.
% for $\multApprox=0.9$ and $\additiveApprox = 0$. 
\displayComsoc{
In iteration $t=1$, condition (\progSums.2) is empty, and the optimal value of $z_1$ is $100$, so \textsf{OP} may output $z_1=0.9\cdot 100 = 90$.
In iteration $t=2$, given  $z_1=90$, 
condition (\progSums.2) says that each of $x_1$ and $x_2$ must be at least $90$;
the optimal value of $z_2$ under these constraints is $110$, so \textsf{OP} may output $z_2=99$, for example with  $x_1=x_2=94.5$.  
Since $n=2$, the algorithm ends 
and returns the solution $(94.5,94.5)$.
}
\displayEcai{
\begin{itemize}
    \item In iteration $t=1$, condition (\progSums.2) is empty, and the optimal value of $z_1$ is $100$, so \textsf{OP} may output $z_1=0.9\cdot 100 = 90$.  
    \item
    In iteration $t=2$, given  $z_1=90$, 
condition (\progSums.2) says that each of $x_1$ and $x_2$ must be at least $90$;
the optimal value of $z_2$ under these constraints is $110$, so \textsf{OP} may output $z_2=99$, for example with  $x_1=x_2=94.5$.  
\item Since $n=2$, the algorithm ends 
and returns the solution $(94.5,94.5)$.
\end{itemize}
}
But $(x_1,x_2) = (94.5,105.5)$ is also a feasible solution, and it is $(0.9,0)$-leximin-preferred
% $(94.5,105.5) \alphaBetaPreferredParams{0.9}{0} (94.5,94.5)$,
since $105.4>\frac{1}{0.9}\cdot 94.5 = 105$.
% $94.5/105.4 \approx 0.896 < 0.9$.
Hence, the returned solution is \emph{not} a $(0.9,0)$-leximin-approximation.
%-------------------------------
% with P-linear
% In the corresponding \eqref{eq:vsums-OP}, constraint (1) will be replaced with constraints (1.1)-(1.3).
% The following is a possible run of the algorithm with \textsf{OP} that is a $(\multApprox,\additiveApprox)$-approximate solver for $\multApprox=0.9$ and $\additiveApprox = 0$\footnote{The objective functions in the example are linear, and therefore, \eqref{eq:vsums-OP} becomes a linear program and the optimal values can also be calculated using any LP solver (such as CVXPY \cite{diamond2016cvxpy}).}. 
% % Notice that for simplicity, we use an example in which $\additiveError=0$; however, this also allows us to demonstrate that even when there is only a multiplicative error, the algorithm behaves differently.
% %
% %
% % \eden{in this version it's hard to give intuition, maybe to say that it can be verified with any LP solver like cvxpy}
% In iteration $t=1$, the optimal value of $z_1$ is $100$, so \textsf{OP} may output $z_1=0.9\cdot 100 = 90$. 
% In iteration $t=2$, given  $z_1=90$,
% the optimal value of $z_2$ is $110$, so \textsf{OP} may output $z_2=99$, for example with  $x_i= y_i = 94.5$ for $i=1,2$, and $m_{\ell,j} = 0$ for $\ell = 1,2$ and $j =1,2$.
% Since $n=2$, the algorithm ends 
% and returns this solution with utilities $(94.5,94.5)$.
% But, taking $x_1 = 94.5, x_2 = 105.5,z_2 = X, y_1 = 94.5, y_2 = 105.5, m_{1,1}= m_{1,2} = m_{2,2} = 0, m_{2,1} = 11$ gives a feasible solution with utilities $(94.5,105.5)$, which is $(0.9,0)$-preferred over the returned solution.
% Therefore, by definition, the returned solution is \emph{not} $(0.9,0)$-approximately-optimal.
% \erel{Maybe: try to show why (P1) is different.}
\end{proof}

Note that, while the above solution is not a $(0.9,0)$-leximin-approximation, it is for $\multApprox = 0.896$.
Our main theorem below shows that this is not a coincidence:
using an approximate solver to \eqref{eq:sums-OP} or \eqref{eq:vsums-OP} in Algorithm \ref{alg:basic-ordered-Outcomes} guarantees a non-trivial leximin approximation.

\begin{theorem}\label{th:main}
Let $\multApprox\in (0,1]$, $\additiveApprox \geq 0$, and \textsf{OP} be an $(\multApprox,\additiveApprox)$-approximation procedure to \eqref{eq:sums-OP} or \eqref{eq:vsums-OP}. Then Algorithm \ref{alg:basic-ordered-Outcomes} outputs an $\left(\frac{\multApprox^2}{1-\multApprox + \multApprox^2}, \frac{\additiveApprox}{1-\multApprox +\multApprox^2}\right)$-leximin-approximation.  
\end{theorem}
% When applied to the above example, Theorem \ref{th:main} guarantees an $(\frac{81}{91},0)\approx (0.89,0)$-leximin-approximation.
For the above example, it guarantees an $(\frac{81}{91},0)\approx (0.89,0)$-leximin-approximation.

A complete proof of Theorem \ref{th:main} is given in Appendix \ref{sub:th:main}.
Here we provide a high level overview of the main steps.
\displayEcai{

}
% First, we note that the set of constraints (\progSums.2) is equivalent to the $t-1$ constraints
% $\sum_{i=1}^{\ell} \valBy{i}{x} \geq \sum_{i=1}^{\ell} z_i$ for all $\ell<t$.
% This is because putting a lower bound on \emph{all} sums of $\ell$ objectives is equivalent to putting the same lower bound on the sum of the $\ell$ \emph{smallest} objectives.
% Similarly,
% the set of constraints (\progSums.3) is equivalent to the single constraint
% $\sum_{i=1}^{t} \valBy{i}{x} \geq \sum_{i=1}^{t} z_i$.
%
%Next, 
%
% \eden{The proof explanation is a little less detailed now}
First, we note that the value of the variable $z_t$ is completely determined by the variable $x$. 
This is because the program aims to maximize $z_t$ that appears only in constraint (\progSums.3), which is equivalent to $z_t\leq \sum_{i=1}^{t} \valBy{i}{x} - \sum_{i=1}^{t-1}  z_i$.
Thus, this constraint will always hold with equality.
% , so we will have $z_t = \sum_{i=1}^{t} \valBy{i}{x} - \sum_{i=1}^{t-1}  z_i$.
\displayEcai{

}
Next, 
% we denote the returned solution by $\retSol$ and
we show that the returned solution, $\retSol$, is feasible to all single-objective
% optimization 
problems that were solved during the algorithm run. 
This allows us to relate the objective values attained by $\retSol$ and the $z_i$ values.
\displayEcai{
Since the solver used in iteration $t$ is $(\alpha,\epsilon)$-approximately-optimal, it follows that the objective value attained by $\retSol$
is at most $\frac{1}{\alpha}(z_t+\epsilon)$, where $z_t$ is the approximation obtained to that problem.}
% EDEN: this claim is not true, this means that $x$ will have value higher than optimal
%
% Further, we show that, for any iteration $t$, the optimal value of the problem that was solved in iteration $t$ can be related to the objective value obtained by $\retSol$ in this problem.
% Also, we show that we can relate  the optimal value of the problem that was solved in iteration $t$ and the approximation obtained for this problem during the algorithm run, $z_t$.
\displayEcai{

}
We then assume for contradiction that $\retSol$ is \emph{not} a leximin approximation as claimed in the theorem.
By definition, there exits a solution $y \in S$ and an integer $1 \leq k \leq n$ such that $\valBy{i}{y} \geq \valBy{i}{\retSol}$ for any $i<k$, while $\valBy{k}{y}$ is $\left(\frac{\multApprox^2}{1-\multApprox + \multApprox^2}, \frac{\additiveApprox}{1-\multApprox +\multApprox^2}\right)$-preferred\footnote{See Section \ref{sub:our-def} for formal definition.} over $\valBy{k}{\retSol}$.
Accordingly, we prove that $y$ is feasible to the program that was solved in the $k$-th iteration, and that its objective value in this problem is higher than the optimal value $z_t^*$, which is a contradiction.
% As this is a contradiction, we get that $\retSol$ is a leximin approximation as required.

% \eden{TODO: to try to add a proof sketch}

\displayComsoc{Theorem \ref{th:main} implies that if \textsf{OP} has only a multiplicative error ($\additiveApprox = 0$), the returned solution will also have only a multiplicative error, and if \textsf{OP} has only an additive error ($\multApprox = 1$), the returned solution will also have only the same additive error $\additiveError$.}
\displayEcai{Theorem \ref{th:main} implies that if \textsf{OP} only has a multiplicative error ($\additiveApprox = 0$), the solution returned by Algorithm \ref{alg:basic-ordered-Outcomes} will only have a multiplicative error as well, and if \textsf{OP} only has an additive error ($\multApprox = 1$), the solution returned by Algorithm \ref{alg:basic-ordered-Outcomes} will have only the same additive error $\additiveError$.}


\subsection{Using a randomized solver}
Next, we assume that the solver is not only approximate but  also \emph{randomized} --- it always returns a feasible solution to the single-objective problem, but only 
with probability $p \in [0,1]$ it is also approximately-optimal.
As Algorithm \ref{alg:basic-ordered-Outcomes} activates the solver $n$ times overall, assuming the success events of different activations are independent, there is a probability of $p^n$ that
the solver returns an approximately-optimal solution in every iteration and so, Algorithm \ref{alg:basic-ordered-Outcomes} performs as in the previous subsection.
 This leads to the following conclusion:
\begin{corollary}\label{corollary:main-with-probability}
Let $\multApprox\in (0,1]$, $\additiveError \geq 0$, $p \in (0,1]$, and \textsf{OP} be a \emph{$p$-randomized} $(\multApprox, \additiveError)$-approximation procedure to \eqref{eq:sums-OP} or \eqref{eq:vsums-OP}. Then Algorithm \ref{alg:basic-ordered-Outcomes} outputs an $\left(\frac{\multApprox^2}{1-\multApprox + \multApprox^2}, \frac{\additiveApprox}{1-\multApprox +\multApprox^2}\right)$-leximin-approximation with probability $p^n$.
\end{corollary}

Notice that, since the procedure \textsf{OP} always returns a feasible solution to the single-objective problem, Algorithm \ref{alg:basic-ordered-Outcomes} always returns a feasible solution as well.

The following section applies such a solver to obtain a leximin approximation to the problem of stochastic allocations of indivisible goods w.h.p.

