
\section{Approximation Algorithm}\label{sec:algo-short}
We now present an algorithm for computing an approximately-optimal leximin solution. The algorithm is an adaptation of one of the algorithms of \textcite{Ogryczak_2006} for finding exact leximin solutions. 

% \eden{yonatan}    
% Following the \emph{definition} of leximin, the core \emph{algorithm} for finding a leximin optimal solution is interative, wherein one first maximizes the least objective function, then the second, and so forth. 
% Specifically, donating by $z_i$ the $i$-th smallest objective value, in each iteration, $t$, one seeks to maximize $z_t$, given the already calculated optimal values for $z_1,\ldots,z_{t-1}$.  The core, single-valued, optimization problem is thus:
%
% \eden{erel} 
% Following the definition of leximin, the core algorithm for finding a leximin optimal solution is iterative, wherein one first maximizes the least objective function, then the second, and so forth. 
% Denote by $z_t$ the value of the $i$-th smallest objective in the leximin-optimal solution, where $t\in\{1,\ldots,n\}$. Suppose we have already computed the optimal values $z_1,\ldots, z_{t-1}$. \eden{I'm not sure about it.. since we use the $z_t$ value later as the \textbf{approximation} value I think it could be a bit confusing}
% Then,  $z_t$ is the solution to the following single-objective optimization problem (where the variables are the scalar $z$ and the vector $x$):

\eden{how is this? I tried to combine the two versions}
Following the definition of leximin, the core algorithm for finding a leximin optimal solution is iterative, wherein one first maximizes the least objective function, then the second, and so forth. 
In each iteration, $t=1,\ldots,n$, it looks for the value that maximizes the $t$-th smallest objective, $z_t$, given that for any $i < t$ the $i$-th smallest objective is at least $z_i$ (the value that was computed in the $i$-th iteration).
The core, single-objective optimization problem is thus:
\begin{align}
 \max \quad &\ztVar{x}  \;\;
        s.t. &\quad  & (1) \quad x \in S  \tag{P1}\label{eq:basic-OP}\\
              &     & & (2) \quad \valBy{\ell}{x}\geq z_{\ell} & \ell = 1,\ldots,t-1\nonumber \\
               &    & & (3) \quad \valBy{t}{x} \geq \ztVar{x} \nonumber   
\end{align} 
where the variables are the scalar $\ztVar{x}$ and the vector $x$, and $z_1, \ldots z_{t-1}$ are constants.

Suppose we are given a procedure $\textsf{OP}(z_1,\ldots,z_{t-1})$, which, given  $z_1,\ldots,z_{t-1},$ outputs $(x,z)$ that is the exact optimal solution to \eqref{eq:basic-OP}.  
Then, the \emph{leximin} optimal solution is obtained by iterating this process for $t=1,\ldots,n$, as described in Algorithm \ref{alg:basic-ordered-Outcomes}.
\begin{algorithm}[!tbp]
\caption{The Ordered Outcomes Algorithm}
\label{alg:basic-ordered-Outcomes}
\begin{algorithmic}[1] %[1] enables line numbers
\FOR{$t=1$ to n}
\STATE \( %\begin{align*}
(x_t,z_t)\leftarrow \textsf{OP}(z_1,\ldots,z_{t-1})
\) %\end{align*} 
\ENDFOR
\STATE return $(z_1,\ldots,z_n)$ 
\end{algorithmic}
\end{algorithm}

However, it might be difficult to solve the problem \eqref{eq:basic-OP} as is, since constraints (2) and (3) are not linear even with respect to the objective-functions. 
 Thus, \cite{Ogryczak_2006} suggest using a variant of the problem that considers sums instead of individual values, which described as follows (similarly, the variables are $\ztVar{x}$ and $x$, and $z_1, \ldots z_{t-1}$ are constants):  
\begin{align*}
    \max \quad &\ztVar{x} \tag{P2}\label{eq:sums-OP}\\
        s.t. \quad  & (1) \quad x \in S\\
                    & (\hat{2}) \quad \sum_{f_i \in F'} f_i(x) \geq \sum_{i=1}^{|F'|}  z_i && \forall F' \subseteq \allObjFunc, |F'| < t \\
                    & (\hat{3}) \quad \sum_{f_i \in F'} f_i(x) \geq \sum_{i=1}^{t}  z_i  && \forall F' \subseteq \allObjFunc, |F'| = t
\end{align*}
Here, the constraints $(2)$ and $(3)$ are replaced with constraints $(\hat{2})$ and $(\hat{3})$, respectively. 
Constraint $(\hat{2})$ says that for any $\ell<t$, the sum of any $\ell$ objective functions is at least the sum of the first $\ell$ $z_i$'s. 
Similarly, $(\hat{3})$ says that the sum of any $t$  objective functions is at least the sum of the $t$ first $z_i$'s.
The program \eqref{eq:sums-OP} is important in the context of leximin since (\cite{Ogryczak2004TelecommunicationsND} Theorem 4):
\begin{lemma}\label{lemma:alg-1-can-use-sums-exact}
    In Algorithm \ref{alg:basic-ordered-Outcomes}, a solver for \eqref{eq:sums-OP} can be used (instead of for \eqref{eq:basic-OP}), and the algorithm will still output a leximin optimal solution. 
\end{lemma}
\noindent For the completeness of this paper, in Appendix \ref{sec:equivalent-proofs}, we provide an alternative proof of this claim.



Alas, while \eqref{eq:sums-OP} is linear with respect to the objective-functions, it has an exponential number of constraints. 
To overcome this challenge, \cite{Ogryczak_2006} employ auxiliary variables ($y_{\ell}$ and $m_{\ell,j}$ for all $\ell \in [t]$ and $ j\in [n]$) to obtain a polynomial-sized program that can be used in the same way as \eqref{eq:sums-OP} (\cite{Ogryczak_2006} Theorem 1):
% (where the variables are $\ztVar{x}$, $x$, and $y_{\ell}$ and $m_{\ell,j}$ for all $\ell \in [t]$ and $ j\in [n]$; and $z_1, \ldots z_{t-1}$ are constants)
\begin{align}
    \max \quad &z_t\tag{P3}\label{eq:vsums-OP}\\
        s.t. \quad  & (1) \quad x \in S \nonumber\\
                    & (2) \quad \ell y_{\ell} - \sum_{j=1}^n m_{\ell,j}\geq \sum_{i=1}^{\ell}  z_i && \ell = 1, \ldots,t-1 \nonumber \\
                    & (3) \quad t y_t - \sum_{j=1}^{n} m_{t,j} \geq \sum_{i=1}^{t}  z_i  \nonumber \\
                    & (4) \quad m_{\ell,j} \geq y_{\ell} - f_j(x)  && \ell = 1, \ldots,t,\Hquad j = 1, \ldots,n \nonumber \\
                    & (5) \quad m_{\ell,j} \geq 0  && \ell = 1, \ldots,t,\Hquad j = 1, \ldots,n \nonumber
\end{align}
In Appendix \ref{sec:equivalent-proofs}, we prove a stronger claim:
\begin{lemma}
    The programs \eqref{eq:sums-OP} and \eqref{eq:vsums-OP} are equivalent\footnote{Recall that two programs are considered \emph{equivalent} if they have the same set of feasible solutions (Section \ref{sec:preliminaries}).}.
\end{lemma}
\noindent When programs are equivalent, a solver for one can be used as a solver for the other (Lemma \ref{}). 
Together with Lemma \ref{lemma:alg-1-can-use-sums-exact} this means that a solver for \eqref{eq:vsums-OP} can also be used in Algorithm \ref{alg:basic-ordered-Outcomes} to obtain a leximin optimal solution. 

Notice that none of these claims asserts that \eqref{eq:basic-OP} is equivalent to either \eqref{eq:sums-OP} or \eqref{eq:vsums-OP}; in fact, we will show that  \eqref{eq:basic-OP} is not equivalent.
In practice, the reason that solvers for these programs can be used interchangeably inside algorithm \ref{alg:basic-ordered-Outcomes} is that they all have the same optimal value when the constants $z_1,\ldots,z_{t-1}$ are the \emph{optimal} values of the previous iterations $1, \ldots, (t-1)$ respectively.

% All of the above is for exact solvers.  
The question is what happens when we consider approximation procedures? 
That is, when the constants $z_1,\ldots,z_{t-1}$ are \emph{approximations} obtained in the previous iterations.
It is easy to see that if \textsf{OP} is a $(\multError, \additiveError)$-approximation algorithm\footnote{See Section \ref{sec:preliminaries} for $(\multError, \additiveError)$-approximation algorithm formal definition.} to \eqref{eq:basic-OP}, then Algorithm \ref{alg:basic-ordered-Outcomes} outputs a $(\multError, \additiveError)$-approximately-optimal leximin solution\footnote{See Section \ref{sec:approx-leximin-def} for $(\multError, \additiveError)$-approximately-optimal leximin solution formal definition.} (formal proof in Appendix \ref{sec:equivalent-proofs}). 
% \erel{ To say this, we first have to define the notion of "approximation algorithm". It is known, but we still have to define it formally. Maybe in the "preliminaries" section. } 
However, if \textsf{OP} approximates \eqref{eq:sums-OP}, or the equivalent \eqref{eq:vsums-OP}, the algorithm may behave differently.
In the following example, \textsf{OP} is a $(\multError, \additiveError)$-approximation procedure for \eqref{eq:sums-OP}, and Algorithm \ref{alg:basic-ordered-Outcomes} outputs a solution that is \emph{not} $(\multError, \additiveError)$-approximately-optimal.
Consider the following multi-objective optimization problem with $n=2$:
\begin{align*}
    \max \quad &\{f_1(x) := x_1, f_2(x) := x_2\} \\
    s.t. \quad  & x_1 \leq 100\\
                & x_1 + x_2 \leq 200\\
                & x \in \mathbb{R}^2_{+}
\end{align*}
Accordingly, the problem \eqref{eq:vsums-OP} becomes:
\begin{align*}
    \max \quad &z_t \\
    s.t. \quad  & (1.1) \quad  x_1 \leq 100\\
                & (1.2) \quad  x_1 + x_2 \leq 200\\
                & (1.3) \quad  x \in \mathbb{R}^2_{+}\\
                & (\hat{2}) \quad \sum_{f_i \in F'} f_i(x) \geq \sum_{i=1}^{|F'|}  z_i && \forall F' \subseteq \allObjFunc, |F'| < t \\
                & (\hat{3}) \quad \sum_{f_i \in F'} f_i(x) \geq \sum_{i=1}^{t}  z_i  && \forall F' \subseteq \allObjFunc, |F'| = t
\end{align*}
The following is a possible run of the algorithm with \textsf{OP} that is a $(\multError, \additiveError)$-approximate solver for $\multError=0.1$ and $\additiveError = 0$. 
Notice that for simplicity, we use an example in which $\additiveError=0$; however, this also allows us to demonstrate that even when there is only a multiplicative error, the algorithm behaves differently.
In the first iteration, $t=1$, the optimal value of $z_1$ is $100$, so \textsf{OP} may output $z_1=90$.  
Then, in the second iteration, $t=2$, given that $z_1=90$, the optimal value of $z_2$ is $110$, so \textsf{OP} may output $z_2=99$, which is obtained (for example) with  $x_1=x_2=94.5$.  
\erel{Since the objectives are the $x_i$, how can the objective be $99$ and the $x_i$ be $94.5$?}\eden{is it more clear now?}
Since $n=2$, the algorithm ends after this iteration and returns the solution $(94.5,94.5)$.
But, $(x_1,x_2) = (94.5,105.5)$ is also a feasible solution, and $(94.5,105.5) \alphaBetaPreferredParams{0.1}{0} (94.5,94.5)$.
Therefore, by definition, the returned solution is \emph{not} $(0.1,0)$-approximately-optimal.
% \eden{this is not exactly accurate, we are not talking about solution that are approximation of one another, but about the relation. maybe something like "But, $(94.5,105.5)$ is also a feasible solution, and since $(94.5,105.5)  \leximinPreferred_{\frac{1}{0.9}} (94.5,94.5)$, by definition  $(94.5,94.5)$ is not a $0.9$-approximation"}
% \erel{This example does not seem very tight. Can you play with the numbers to get a tighter upper bound on the approximation ratio?}

 %On the other hand, \eqref{eq:vsums-OP} is the most solvable variant of the problem.  
So, using an approximate solver to \eqref{eq:sums-OP} or \eqref{eq:vsums-OP}, what guarantee can we provide for the overall resulting solution? 

\begin{theorem}\label{th:main}
Let $\multError\in [0,1)$, $\additiveError \geq 0$, and \textsf{OP} be a $(\multError, \additiveError)$-approximation procedure to \eqref{eq:vsums-OP}. Then Algorithm \ref{alg:basic-ordered-Outcomes} outputs a $\left(\frac{1}{1-\multError +\multError^2}\multError, \frac{1-\multError^2}{1-\multError +\multError^2}\additiveError\right)$-approximately-optimal leximin solution.  
\end{theorem}

% \eden{should we explain somewhere about the consequences on the cases where only multiplicative or additive error are present? if so, where?}
\noindent Notice that this result also implies that if \textsf{OP} only has a multiplicative error $\multError$, the solution returned by Algorithm \ref{alg:basic-ordered-Outcomes} will only have a multiplicative error of $\frac{\multError}{1-\multError +\multError^2}$, and if \textsf{OP} only has an additive error $\additiveError$, the solution returned by Algorithm \ref{alg:basic-ordered-Outcomes} will have only the same additive error $\additiveError$.

The remainder of this section is dedicated to proving the theorem.
To this end, we use another equivalent representation of \eqref{eq:sums-OP} (proof in Appendix \ref{sec:equivalent-proofs}), which was also introduced by \cite{Ogryczak_2006}. 
% \erel{This variant should move upwards, between P1 and P2.}
% (here also, the variables are $\ztVar{x}$ and $x$, and $z_1, \ldots z_{t-1}$ are constants)
\begin{align*}
    \max \quad &z_t \tag{P2-compact}\label{eq:compact-OP}\\
        s.t. \quad  & (1) \quad x \in S\\
                    & (\Tilde{2}) \quad \sum_{i=1}^{\ell} \valBy{i}{x} \geq \sum_{i=1}^{\ell}  z_i && \ell = 1,\ldots, t-1 \nonumber\\
                    & (\Tilde{3}) \quad \sum_{i=1}^{t} \valBy{i}{x} \geq \sum_{i=1}^{t}  z_i
\end{align*}
In this program, constraints $(\hat{2})$ and $(\hat{3})$ are replaced by  $(\Tilde{2})$ and $(\Tilde{3})$, respectively.  
In  $(\hat{2})$, for each $\ell$, one gives a lower bound on the sum for \emph{any} set of $\ell$ objective functions.  
In $(\Tilde{2})$, on the other hand, one only considers the sum of the $\ell$ \emph{smallest} such values.  
% However, since the constraints set the same lower bound on this sum, the constraints are equivalent.  
Similarly for $(\hat{3})$ and $(\Tilde{3})$.  
Recall that when two programs are equivalent, a solver, either exact or approximate, for one can be used as a solver, with the same level of accuracy, for the other (Lemma \ref{}). 
Therefore, as \eqref{eq:compact-OP} is equivalent to \eqref{eq:sums-OP}, which, in turn, is equivalent to \eqref{eq:vsums-OP}, in proving the theorem we may assume that \textsf{OP} is an approximation procedure for \eqref{eq:compact-OP}.  
This will simplify the proofs.

\erel{*** I do not understand. We say that P1 and P2 are equivalent with an exact solver, but not with an approximate solver. 
Here, we claim that P3 and P2-compact are equivalent, but this is true only with an exact solver. Don't we have to prove that they are equivalent also with an approximate solver? ***}

We denote $\retSol := x_n$ = the solution $x$ attained at the last iteration ($t=n$) of the algorithm. 

Following are some observations regarding the set of feasible solutions in each iteration, their objective values, and the solution $\retSol$ that will be useful later on.
First, notice that any solution $x \in S$ that satisfies constraint $(\Tilde{2})$ of \eqref{eq:compact-OP} is feasible to this problem.
This is because any solution $x \in S$ can satisfy constraint $(\Tilde{3})$ with a small enough assignment to the variable $z_t$. \eden{I'm not sure how to explain it....}
\begin{observation}\label{obs:feasi-and-constraint2}
Any solution $x \in S$ that satisfies constraint $(\Tilde{2})$ is feasible to \eqref{eq:compact-OP}.
\end{observation}

Since $\retSol$ is a feasible solution of \eqref{eq:compact-OP} in iteration $n$, and as each
iteration only adds new constraints to $(\Tilde{2})$, it follows that $\retSol$ is also a feasible solution of \eqref{eq:compact-OP} in any iteration $1 \leq t\leq n$. 
\begin{observation}\label{obs:retSol-solves-any-t}
$\retSol$ is a feasible solution of \eqref{eq:compact-OP} in any iteration $1 \leq t\leq n$.
\end{observation}

Now, consider the problem \eqref{eq:compact-OP} that was solved in iteration $t$.
Here, $z_t$ is a \emph{variable} and $z_1, \ldots z_{t-1}$ are constants.
The objective of this problem is $(\max z_t)$ and the only constraint that includes the variable $z_t$ is constraint $(\Tilde{3})$.
Therefore, rearranging it to $\sum_{i=1}^{t} \valBy{i}{x} - \sum_{i=1}^{t-1}  z_i\geq z_t$, allows us to conclude that the objective value is determined by the left side of this inequality (as $z_k$ is maximized when the inequality turns to equality).
\begin{observation}\label{obs:obj-value}
The objective value obtained by a feasible solution $x$ to the problem \eqref{eq:compact-OP} that was solved in iteration $t$ is $\sum_{i=1}^{t} \valBy{i}{x} - \sum_{i=1}^{t-1}  z_i$.
\end{observation}

Lastly, as the value obtained as a $(\multError, \additiveError)$-approximation for this problem is the \emph{constant} $z_t$, the optimal value is at most $\frac{1}{1-\multError} (z_t+\additiveError)$. 
Consequently, the objective value of any feasible solution is at most this value.
Since $\retSol$ is feasible for any iteration $t$ (Observation \ref{obs:retSol-solves-any-t}) and since its objective is $\sum_{i=1}^t \valBy{i}{\retSol} - \sum_{i=1}^{t-1} z_i$ (Observation \ref{obs:obj-value}), we can conclude:

\begin{observation}\label{obs:obj-xt-to-zt}
    The objective value obtained by $\retSol$ to the problem \eqref{eq:compact-OP} that was solved in iteration $t$ is at most $\frac{1}{1-\multError} (z_t+\additiveError)$. That is:
    \begin{align*}
        \sum_{i=1}^t \valBy{i}{\retSol} - \sum_{i=1}^{t-1} z_i \leq \frac{1}{1-\multError} \left(z_t+\additiveError \right)
    \end{align*}
\end{observation}

% This conclusion also implies that for any $1 \leq t \leq n$, the solution $(x_t, z_t)$ that that was outputted for \eqref{eq:compact-OP} in iteration $t$, satisfies constraint $(\Tilde{3})$ as equality. That is:
% \begin{observation}\label{obs:equality-xt-zt}
% For any $1 \leq t \leq n$,  $\sum_{i=1}^{t} \valBy{i}{x_t} = \sum_{i=1}^{t}  z_i$.
% \end{observation}



%%%
% OVERALL EXPLANATION
%We first establish our motivation by demonstrating, in Lemma \ref{lemma:not-beta-approx}, that $\retSol$ is not necessarily a $1-\multError$ leximin approximation.\eden{wasn't sure were to put lemma \ref{lemma:not-beta-approx}, before or after}
% However, we then prove it is actually not that far off, it is a $1-2\multError$ approximation. 
We start by Lemma \ref{lemma:retSol-approx-all-t} \eden{To myself: to do}
Then, Lemmas \ref{lemma:mult-fk-lower-bound}-\ref{lemma:mult-special-z_k-upper-bound} establish a relationship between the $k$-th least objective value at 
% the returned solution ($\retSol$)
$\retSol$
% solution  returned by Algorithm \ref{alg:basic-ordered-Outcomes} ($\retSol$) 
and the value $z_k$. 
% In particular, the main conclusion is that the first is at least $\frac{1-2\multError}{1-\multError}$ of the second.
Theorem \ref{th:main} then uses this relation to prove that the existence of another solution that would be $\frac{1}{1-2\multError}$-preferred over $\retSol$ would lead to a contradiction.

% LEMMAS.
% BLAH BLAH.

\begin{lemma}\label{lemma:beta-vk}
    Let $1 \leq k \leq n$. Then:
    \begin{align*}
        \multError \valBy{k}{\retSol} \geq \left(\sum_{i=1}^k \valBy{i}{\retSol} - \sum_{i=1}^k z_i\right) -\multError \left(\sum_{i=1}^{k-1} \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i\right) -\additiveError
    \end{align*}
\end{lemma}

\begin{proof}
    The claim follows from Observation \ref{obs:obj-xt-to-zt}:
    \begin{align*}
         &\sum_{i=1}^k \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i \leq \frac{1}{1-\multError} \left(z_k + \additiveError \right)\\
         &\Rightarrow z_k +\additiveError \geq (1-\multError) \left(\sum_{i=1}^{k} \valBy{i}{\retSol} - \sum_{i=1}^{k-1}  z_i\right)\\
        &\Rightarrow z_k +\additiveError\geq \left(\sum_{i=1}^{k} \valBy{i}{\retSol} - \sum_{i=1}^{k-1}  z_i\right) - \multError \left(\sum_{i=1}^{k} \valBy{i}{\retSol} - \sum_{i=1}^{k-1}  z_i\right)\\
        &\Rightarrow \multError \valBy{k}{\retSol} \geq \left(\sum_{i=1}^k \valBy{i}{\retSol} - \sum_{i=1}^k z_i\right) -\multError \left(\sum_{i=1}^{k-1} \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i\right) -\additiveError
    \end{align*}
\end{proof}


\begin{lemma}\label{lemma:beta-sums-to-diff}
    Let $1 \leq k \leq n$. Then:
    \begin{align*}
        \sum_{i=1}^k \multError^{i} \valBy{k-i+1}{\retSol} \geq \sum_{i=1}^k \valBy{i}{\retSol} - \sum_{i=1}^{k} z_i -\additiveError
    \end{align*}
\end{lemma}

\begin{proof}
    The proof is by induction on $k$.
    For $k=1$ the claim follows directly from Lemma \ref{lemma:beta-vk}.
    Assuming that the claim is true for $1,\ldots k-1$, then it is true also for $k$:
    \begin{align*}
        &\sum_{i=1}^k \multError^{i} \valBy{k-i+1}{\retSol} = \multError \valBy{k}{\retSol} + \sum_{i=2}^k \multError^{i} \valBy{k-i+1}{\retSol}\\
        &= \multError \valBy{k}{\retSol} + \sum_{i=1}^{k-1} \multError^{i+1} \valBy{k-(i+1)+1}{\retSol} \\
        &= \multError \valBy{k}{\retSol} + \multError \sum_{i=1}^{k-1} \multError^{i} \valBy{(k-1) -i+1}{\retSol}\\
        &= \multError \valBy{k}{\retSol} + \multError \left(\sum_{i=1}^{k-1} \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i\right) && \text{Induction assumption}\\
        &\geq \left(\sum_{i=1}^k \valBy{i}{\retSol} - \sum_{i=1}^k z_i\right) -\multError \left(\sum_{i=1}^{k-1} \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i\right)-\additiveError  \\
        & \quad +  \multError \left(\sum_{i=1}^{k-1} \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i\right) && \text{Lemma \ref{lemma:beta-vk}} \\
        &\geq \sum_{i=1}^k \valBy{i}{\retSol} - \sum_{i=1}^{k} z_i -\additiveError
    \end{align*}
\end{proof}


\begin{lemma}\label{lemma:fk-to-all}
    Let $1 \leq k \leq n$. Then:
    \begin{align*}
        \frac{1-\multError +\multError^2(1-\multError^{n})}{1-\multError} \valBy{k}{\retSol} \geq \valBy{k}{\retSol} + \multError\left(\sum_{i=1}^{k-1}\valBy{i}{\retSol} - \sum_{i=1}^{k-1}z_i - \additiveError \right)
    \end{align*}
\end{lemma}

\begin{proof}
First, notice that since $k \geq (k-1)-i+1$ for any $1\leq i \leq k$ and as the function $\valBy{i}$ represents the $i$-th smallest objective value, also:
    \begin{align}\label{eq:increase-by-obj-size}
        \forall 1\leq i \leq k \colon \quad \valBy{k}{\retSol} \geq \valBy{(k-1)-i+1}{\retSol}
    \end{align}
    In addition, consider the geometric series with a first element $1$, a ratio $\multError$, and a length $k-1$. 
    As $\multError \neq 1$, its sum can be described as:
    \begin{align}\label{eq:geometric-series-beta}
        \sum_{i=1}^{k-1} \multError^{i-1} = \frac{1-\multError^k}{1-\multError}
    \end{align}
    
    Now, the claim can be concluded as follows:
    \begin{align*}
        &\frac{1-\multError +\multError^2(1-\multError^{n})}{1-\multError} \valBy{k}{\retSol} = \left(1 + \frac{\multError^2(1-\multError^k)}{1-\multError}\right) \valBy{k}{\retSol} \\
        &= \valBy{k}{\retSol} + \multError^2 \left(\frac{1-\multError^k}{1-\multError} \valBy{k}{\retSol} \right)\\
        & = \valBy{k}{\retSol} + \multError^2 \left(\sum_{i=1}^{k-1} \multError^{i-1} \valBy{k}{\retSol} \right) && \text{Equation \ref{eq:geometric-series-beta}}\\
        & \geq \valBy{k}{\retSol} + \multError^2 \left(\sum_{i=1}^{k-1} \multError^{i-1} \valBy{(k-1)-i+1}{\retSol} \right) && \text{Equation \ref{eq:increase-by-obj-size}}\\
        &= \valBy{k}{\retSol} + \multError \sum_{i=1}^{k-1} \multError^{i} \valBy{(k-1)-i+1}{\retSol} \\
        &\geq \valBy{k}{\retSol} + \multError\left(\sum_{i=1}^{k-1}\valBy{i}{\retSol} - \sum_{i=1}^{k-1}z_i - \additiveError \right) && \text{Lemma \ref{lemma:beta-sums-to-diff}}
\end{align*}
\end{proof}



%------
% thm.

We are now ready to prove the Theorem \ref{th:main}.
\begin{proof}[Proof of Theorem \ref{th:main}]
\eden{should decide whether to keep this part}
Recall that the claim is that $\retSol$ is a $\left(\frac{1}{1-\multError +\multError^2}\multError, \frac{1-\multError^2}{1-\multError +\multError^2}\additiveError\right)$-approximation.
However, we prove a slightly stronger result: the solution $\retSol$ is $\left(\Delta^{mult}_n \multError, \Delta^{add}_n\additiveError\right)$ approximately-optimal, where:
\begin{align*}
    &\Delta^{mult}_n=\frac{1-\multError+\multError(1-\multError^n)}{1-\multError +\multError^2(1-\multError^n)}, &&\Delta^{add}_n = \frac{1-\multError^2}{1-\multError + \multError^2(1-\multError^n)}
\end{align*}
% $\Delta^{mult}_n=\frac{1-\multError+\multError(1-\multError^n)}{1-\multError +\multError^2(1-\multError^n)}$ and $\Delta^{add}_n = \frac{1-\multError^2}{1-\multError + \multError^2(1-\multError^n)}$.
Since $\multError < 1$, when the numbers of objectives, $n$, increases and tends to infinity, the value of $\multError^n$ tends to zero, and therefore $\Delta^{mult}_n$ tends to $\frac{1-\multError+\multError}{1-\multError +\multError^2} = \frac{1}{1-\multError +\multError^2}$ and $\Delta^{add}_n$ tends to $\frac{1-\multError^2}{1-\multError +\multError^2}$.
Specifically, since $\lambda_n < \frac{1}{1-\multError +\multError^2}$, proving this claim implies the original one (by Lemma \ref{lemma:beta1-beta2-approx}), while, at the same time, gives us a tighter bound on the approximation ratio with respect to $n$.

We first prove the following equation that will be helpful later:
\begin{align}\label{equ:mu}
\frac{1}{1 - \lambda_n \multError } = \frac{1-\multError +\multError^2(1-\multError^{n})}{(1-\multError)^2}
\end{align}
This is true because:
\begin{align*}
    & 1 - \lambda_n \multError =  1 - \frac{1-\multError+\multError(1-\multError^n)}{1-\multError +\multError^2(1-\multError^n)}\multError && \text{definition of $\mu$} \\
    & = \frac{1-\multError +\multError^2(1-\multError^n) - \multError(1-\multError)- \multError^2(1-\multError^n)}{1-\multError +\multError^2(1-\multError^{n})}\\
    & = \frac{(1-\multError)^2}{1-\multError +\multError^2(1-\multError^{n})}
    \end{align*}
    and therefore,
    \begin{align*}
        & \frac{1}{1 - \lambda_n \multError } = \frac{1-\multError +\multError^2(1-\multError^{n})}{(1-\multError)^2}
    \end{align*}
    In addition, we prove the following equation that will be useful as well:
    \begin{align}\label{eq:additive-error}
        \frac{1}{1 - \lambda_n \multError }\cdot\frac{1-\multError^2}{1-\multError + \multError^2(1-\multError^n)} = \frac{1+\beta}{1-\beta}
    \end{align}
    The reason for this is that:
    \begin{align*}
        &\frac{1}{1 - \lambda_n \multError }\cdot\frac{1-\multError^2}{1-\multError + \multError^2(1-\multError^n)} \\
        &=\frac{1-\multError +\multError^2(1-\multError^{n})}{(1-\multError)^2} \cdot\frac{(1-\multError)(1+\multError)}{1-\multError + \multError^2(1-\multError^n)}&& \text{Equation \ref{equ:mu}}\\
        &= \frac{1+\beta}{1-\beta}
    \end{align*}

    Now, suppose by contradiction that $\retSol$ is not $\left(\lambda_n \multError, (1+\multError)\additiveError\right)$-approximately-optimal.
    By definition, this means there exists a solution $y \in S$  that is $\left(\lambda_n \multError, (1+\multError)\additiveError\right)$-preferred over it.
    That is, there exists an integer $1 \leq k \leq n$ such that:
    \begin{align*}
        \forall j < k \colon &\valBy{j}{y} \geq \valBy{j}{\retSol};\\
        & \valBy{k}{y} > \frac{1}{(1 - \lambda_n \multError)} \left(\valBy{k}{\retSol} + (1+\multError)\additiveError \right).
    \end{align*}

    Since $\retSol$ was obtained in \eqref{eq:compact-OP} that was solved in the last iteration $n$, it is clear that $\sum_{i=1}^k \valBy{i}{\retSol} \geq \sum_{i=1}^{k} z_i$ (by constraint $(\Tilde{2})$ if $k<n$ and $(\Tilde{3})$ otherwise).
    Which implies:
    \begin{align}\label{eq:fk-to-zk}
        \sum_{i=1}^k \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i \geq z_k
    \end{align}

    Now, consider \eqref{eq:compact-OP} that was solved in iteration $t$.
    By Observation \ref{obs:retSol-solves-any-t}, $\retSol$ is feasible to this problem.
    As $y$'s $(k-1)$ least objective values are at least as high as those of $\retSol$, it is easy to conclude the $y$ satisfies constraint $(\Tilde{2})$ of this problem; since, for any $\ell < k$:
    \begin{align*}
        \sum_{i=1}^{\ell} \valBy{i}{y} \geq\sum_{i=1}^{\ell} \valBy{i}{\retSol} \geq \sum_{i=1}^{\ell} z_i
    \end{align*}
    Therefore, by Observation \ref{obs:feasi-and-constraint2}, $y$ is also feasible to this problem.
    
    However, the approximation obtained for this problem during the algorithm run is $z_k$, and so, the optimal value is at most $\frac{1}{(1-\multError)}\left(z_k+\additiveError\right)$.
    But, as we shall see, the objective $y$ yields in this problem, $\sum_{i=1}^k \valBy{i}{y} - \sum_{i=1}^{k-1} z_i$ (by Observation \ref{obs:obj-value}), is higher than this value, which is of course a contradiction:
    \begin{align*}
        &\sum_{i=1}^k \valBy{i}{y} - \sum_{i=1}^{k-1} z_i=\sum_{i=1}^{k-1} \valBy{i}{y} - \sum_{i=1}^{k-1} z_i + \valBy{k}{y}\\
        &\geq \sum_{i=1}^{k-1} \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i + \valBy{k}{y} && \text{$y$'s definition for $j<k$}\\
        &> \sum_{i=1}^{k-1} \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i + \frac{1}{(1 - \lambda_n \multError)} \left(\valBy{k}{\retSol} + (1+\multError)\additiveError \right) && \text{$y$'s definition for $k$}\\
        &= \sum_{i=1}^{k-1} \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i + \frac{1-\multError +\multError^2(1-\multError^{n})}{(1-\multError)^2} \valBy{k}{\retSol}\emark{+\frac{1+\multError}{1-\multError}\additiveError} && \text{equitation \ref{equ:mu}}\\
        &\geq\sum_{i=1}^{k-1} \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i + \frac{1}{1-\multError} \valBy{k}{\retSol} + \frac{\multError}{1-\multError}\left(\sum_{i=1}^{k-1}\valBy{i}{\retSol} - \sum_{i=1}^{k-1}z_i\emark{-\additiveError}\right) \emark{+\frac{1+\multError}{1-\multError}\additiveError} && \text{Lemma \ref{lemma:fk-to-all}}\\
        & = \frac{1}{1-\multError} \left(\sum_{i=1}^k \valBy{k}{\retSol} - \sum_{i=1}^{k-1}z_i\right) \emark{+\frac{1}{1-\multError}\additiveError} \geq \frac{1}{1-\multError} z_k \emark{+\frac{1}{1-\multError}\additiveError} && \text{equation \ref{eq:fk-to-zk}}
    \end{align*}
\end{proof}


\iffalse
Finally, to complete the picture, the following is an example that shows that the algorithm need not output a $(1-\multError)$ approximate leximin.
Consider the following example with $n=2$:
    \begin{align*}
        \max \quad &\{f_1(x) := x_1, f_2(x) := x_2\} \\
        s.t. \quad  & x_1 \leq 100\\
                    & x_1 + x_2 \leq 200\\
                    & x \in \mathbb{R}^2_{+}
    \end{align*}
    Consider a run of the algorithm with \textsf{OP} that is a $(1-\multError)$ approximate solver, for $\multError=0.1$. 
    In the first iteration, the optimal value of $z_1$ is $100$, so \textsf{OP} may output $z_1=90$.  
    Then, in the second iteration, given $z_1=90$, the optimal value of $z_2$ is $110$, so \textsf{OP} may output $z_2=99$, which is obtained with  $x_1=x_2=94.5$.  
    \erel{Since the objectives are the $x_i$, how can the objective be $99$ and the $x_i$ be $94.5$?}
    But, $(x_1,x_2) = (94.5,105.5)$ is also a feasible solution, and $(94.5,94.5)$ is \emph{not} a $0.9$ approximation of $(94.5,105.5)$.
    \eden{this is not exactly accurate, we are not talking about solution that are approximation of one another, but about the relation. maybe something like "But, $(94.5,105.5)$ is also a feasible solution, and since $(94.5,105.5) \leximinPreferred_{\frac{1}{0.9}} (94.5,94.5)$, by definition 
    $(94.5,94.5)$ is not a $0.9$-approximation"}
\erel{This example does not seem very tight. Can you play with the numbers to get a tighter upper bound on the approximation ratio?}
\fi
