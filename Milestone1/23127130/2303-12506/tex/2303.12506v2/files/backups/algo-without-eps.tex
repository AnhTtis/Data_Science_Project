
\section{Approximation Algorithm}\label{sec:algo-short}
We now present an algorithm for computing a multiplicative approximate leximin \eden{optimal?}. The algorithm is an adaptation of one of the algorithms of \textcite{Ogryczak_2006} for finding exact Leximin solutions. 

% \eden{yonatan}
% Following the \emph{definition} of Leximin, the core \emph{algorithm} for finding a Leximin optimal solution is interative, wherein one first maximizes the least objective function, then the second, and so forth. 
% Specifically, donating by $z_i$ the $i$-th smallest objective value, in each iteration, $t$, one seeks to maximize $z_t$, given the already calculated optimal values for $z_1,\ldots,z_{t-1}$.  The core, single-valued, optimization problem is thus:
%
% \eden{erel} 
% Following the definition of leximin, the core algorithm for finding a leximin optimal solution is iterative, wherein one first maximizes the least objective function, then the second, and so forth. 
% Denote by $z_t$ the value of the $i$-th smallest objective in the leximin-optimal solution, where $t\in\{1,\ldots,n\}$. Suppose we have already computed the optimal values $z_1,\ldots, z_{t-1}$. \eden{I'm not sure about it.. since we use the $z_t$ value later as the \textbf{approximation} value I think it could be a bit confusing}
% Then,  $z_t$ is the solution to the following single-objective optimization problem (where the variables are the scalar $z$ and the vector $x$):

\eden{how is this? I tried to combine the two versions}
Following the definition of Leximin, the core algorithm for finding a Leximin optimal solution is iterative, wherein one first maximizes the least objective function, then the second, and so forth. 
In each iteration, $t=1,\ldots,n$, it looks for the value that maximizes the $t$-th smallest objective, $z_t$, given that for any $i < t$ the $i$-th smallest objective is at least $z_i$ (the value that was computed in the $i$-th iteration).
The core, single-objective optimization problem is thus:
\begin{align}
 \max \quad &\ztVar{x}  \;\;
        s.t. &\quad  & (1) \quad x \in S  \tag{P1}\label{eq:basic-OP}\\
              &     & & (2) \quad \valBy{\ell}{x}\geq z_{\ell} & \ell = 1,\ldots,t-1\nonumber \\
               &    & & (3) \quad \valBy{t}{x} \geq \ztVar{x} \nonumber   
\end{align} 
where the variables are the scalar $\ztVar{x}$ and the vector $x$, and $z_1, \ldots z_{t-1}$ are constants.

Suppose we are given a procedure $\textsf{OP}(z_1,\ldots,z_{t-1})$, which, given  $z_1,\ldots,z_{t-1},$ outputs $(x,z)$ that is the exact optimal solution to \eqref{eq:basic-OP}.  
Then, the \emph{leximin} optimal solution is obtained by iterating this process for $t=1,\ldots,n$, as described in Algorithm \ref{alg:basic-ordered-Outcomes}.
\begin{algorithm}[!tbp]
\caption{The Ordered Outcomes Algorithm}
\label{alg:basic-ordered-Outcomes}
\begin{algorithmic}[1] %[1] enables line numbers
\FOR{$t=1$ to n}
\STATE \( %\begin{align*}
(x_t,z_t)\leftarrow \textsf{OP}(z_1,\ldots,z_{t-1})
\) %\end{align*} 
\ENDFOR
\STATE return $(z_1,\ldots,z_n)$ 
\end{algorithmic}
\end{algorithm}

However, it might be difficult to solve the problem \eqref{eq:basic-OP} as is, since constraints (2) and (3) are not linear even with respect to the objective-functions. 
\erel{I suggest to move P2-Compact here: it is a natural step between P1 and P2.}
\eden{}
 Thus, \cite{Ogryczak_2006} suggest using a variant of the problem that considers sums instead of individual values, which described as follows (similarly, the variables are $\ztVar{x}$ and $x$, and $z_1, \ldots z_{t-1}$ are constants):  
% \eden{maybe we should say it is not a linear program \textbf{with respect to the objective functions}?}
% \erel{I agree. Since the objective functions themselves may be non-linear.}
\begin{align*}
    \max \quad &\ztVar{x} \tag{P2}\label{eq:sums-OP}\\
        s.t. \quad  & (1) \quad x \in S\\
                    & (\hat{2}) \quad \sum_{f_i \in F'} f_i(x) \geq \sum_{i=1}^{|F'|}  z_i && \forall F' \subseteq \allObjFunc, |F'| < t \\
                    & (\hat{3}) \quad \sum_{f_i \in F'} f_i(x) \geq \sum_{i=1}^{t}  z_i  && \forall F' \subseteq \allObjFunc, |F'| = t
\end{align*}
Here, the constraints $(2)$ and $(3)$ are replaced with constraints $(\hat{2})$ and $(\hat{3})$, respectively. 
Constraint $(\hat{2})$ says that for any $\ell<t$, the sum of any $\ell$ objective functions is at least the sum of the first $\ell$ $z_i$'s. Similarly, $(\hat{3})$ says that the sum of any $t$  objective functions is at least the $t$ first $z_i$'s.  So, since we seek to maximize $z_t$, the optimal solution to \eqref{eq:sums-OP} and \eqref{eq:basic-OP} are provably identical (as in both $z_i$ is the $i$-th least objective value, see \cite{Ogryczak_2006} for a formal proof \eden{I'm actually not sure which of the papers to cite as I didn't find a formal proof (only the claim in given in \cite{Ogryczak_2006} and in \cite{Ogryczak2004TelecommunicationsND} Theorem 4)}). As such, in Algorithm \ref{alg:basic-ordered-Outcomes}, a solver for \eqref{eq:sums-OP} can be used (instead of for \eqref{eq:basic-OP}), and the algorithm will still output a Leximin optimal solution. 

Alas, while \eqref{eq:sums-OP} is linear with respect to the objective-functions, it has an exponential number of constraints. 
To overcome this problem, \cite{Ogryczak_2006} employ auxiliary variables ($y_{\ell}$ and $m_{\ell,j}$ for all $\ell \in [t]$ and $ j\in [n]$) to obtain a polynomial-sized program that is provably equivalent to \eqref{eq:sums-OP}:\eden{To myself: should explain "equivalent" in more detail, maybe to add the alternative proof}
% (where the variables are $\ztVar{x}$, $x$, and $y_{\ell}$ and $m_{\ell,j}$ for all $\ell \in [t]$ and $ j\in [n]$; and $z_1, \ldots z_{t-1}$ are constants)
\begin{align}
    \max \quad &z_t\tag{P3}\label{eq:vsums-OP}\\
        s.t. \quad  & (1) \quad x \in S \nonumber\\
                    & (2) \quad \ell y_{\ell} - \sum_{j=1}^n m_{\ell,j}\geq \sum_{i=1}^{\ell}  z_i && \ell = 1, \ldots,t-1 \nonumber \\
                    & (3) \quad t y_t - \sum_{j=1}^{n} m_{t,j} \geq \sum_{i=1}^{t}  z_i  \nonumber \\
                    & (4) \quad m_{\ell,j} \geq y_{\ell} - f_j(x)  && \ell = 1, \ldots,t,\Hquad j = 1, \ldots,n \nonumber \\
                    & (5) \quad m_{\ell,j} \geq 0  && \ell = 1, \ldots,t,\Hquad j = 1, \ldots,n \nonumber
\end{align}
So, in Algorithm \ref{alg:basic-ordered-Outcomes} using a solver for \eqref{eq:vsums-OP}, the algorithm will output a Leximin optimal solution. 

All of the above is for exact solutions.  
The question is what happens when we consider approximation procedures? It is easy to see that if \textsf{OP} is a $(1-\beta)$ approximation algorithm to \eqref{eq:basic-OP}, then Algorithm \ref{alg:basic-ordered-Outcomes} outputs a $(1-\beta)$ approximation of the Leximin. 
\erel{
To say this, we first have to define the notion of "approximation algorithm". It is known, but we still have to define it formally. Maybe in the "preliminaries" section.
} 
However, for approximations, \eqref{eq:sums-OP}, and the equivalent \eqref{eq:vsums-OP}, are no longer equivalent to \eqref{eq:basic-OP}. Indeed, the following example shows that if \textsf{OP} is a $(1-\beta)$ approximation procedure for \eqref{eq:vsums-OP}, then Algorithm \ref{alg:basic-ordered-Outcomes} may output a solution that is \emph{not} a $1-\beta$ approximation to the leximin.
% \erel{Maybe it is better to move the example to here, to motivate our proof.}
Consider the following multi-objective optimization problem with $n=2$:
\begin{align*}
    \max \quad &\{f_1(x) := x_1, f_2(x) := x_2\} \\
    s.t. \quad  & x_1 \leq 100\\
                & x_1 + x_2 \leq 200\\
                & x \in \mathbb{R}^2_{+}
\end{align*}
Accordingly, the problem \eqref{eq:vsums-OP} becomes:
\begin{align*}
    \max \quad &z_t \\
    s.t. \quad  & (1.1) \quad  x_1 \leq 100\\
                & (1.2) \quad  x_1 + x_2 \leq 200\\
                & (1.3) \quad  x \in \mathbb{R}^2_{+}\\
                & (\hat{2}) \quad \sum_{f_i \in F'} f_i(x) \geq \sum_{i=1}^{|F'|}  z_i && \forall F' \subseteq \allObjFunc, |F'| < t \\
                & (\hat{3}) \quad \sum_{f_i \in F'} f_i(x) \geq \sum_{i=1}^{t}  z_i  && \forall F' \subseteq \allObjFunc, |F'| = t
\end{align*}
The following is a possible run of the algorithm with \textsf{OP} that is a $(1-\beta)$ approximate solver for $\beta=0.1$. 
In the first iteration, $t=1$, the optimal value of $z_1$ is $100$, so \textsf{OP} may output $z_1=90$.  
Then, in the second iteration, $t=2$, given that $z_1=90$, the optimal value of $z_2$ is $110$, so \textsf{OP} may output $z_2=99$, which is obtained (for example) with  $x_1=x_2=94.5$.  
\erel{Since the objectives are the $x_i$, how can the objective be $99$ and the $x_i$ be $94.5$?}\eden{is it more clear now?}
Since $n=2$, the algorithm ends after this iteration and returns this solution $(94.5,94.5)$.
But, $(x_1,x_2) = (94.5,105.5)$ is also a feasible solution, and $(94.5,105.5)\xPreferred{\frac{1}{0.9}}(94.5,94.5)$.
Therefore, by definition, the returned solution $(94.5,94.5)$ is \emph{not} a $0.9$ approximately-optimal solution.
% \eden{this is not exactly accurate, we are not talking about solution that are approximation of one another, but about the relation. maybe something like "But, $(94.5,105.5)$ is also a feasible solution, and since $(94.5,105.5)  \leximinPreferred_{\frac{1}{0.9}} (94.5,94.5)$, by definition  $(94.5,94.5)$ is not a $0.9$-approximation"}
\erel{This example does not seem very tight. Can you play with the numbers to get a tighter upper bound on the approximation ratio?}

 %On the other hand, \eqref{eq:vsums-OP} is the most solvable variant of the problem.  
So, using an approximate solver to \eqref{eq:vsums-OP}, what guarantee can we provide for the overall resulting solution? 
\begin{theorem}
\label{th:main}
Let $\beta<1/2$, and \textsf{OP} be a procedure that outputs a $(1-\beta)$ approximation to \eqref{eq:vsums-OP}. Then Algorithm \ref{alg:basic-ordered-Outcomes} outputs a $(1-\frac{\beta}{1-\beta +\beta^2})$-approximate Leximin solution.  
\end{theorem}
\eden{Alternative:}
\begin{theorem}
\label{th:main}
Let $\beta<1/2$, and \textsf{OP} be a procedure that outputs a $(1-\beta)$ approximation to \eqref{eq:vsums-OP}. Then Algorithm \ref{alg:basic-ordered-Outcomes} outputs a $(1-\beta-\frac{\beta^2(1-\beta)}{1-\beta+\beta^2})$-approximate Leximin solution.  
\end{theorem}
\eden{To myself: need to check whether the assumption $\beta< 1/2$ can be removed}

\eden{maybe we should also include the proof of the additive variant somewhere? if so. where?}
\erel{
Since the proof is not explicitly mentioned anywhere, I think it is good to include a proof for comparison. Maybe in an appendix.
}
\eden{if we want to do so we should also describe the additive variant definition, maybe to put them both in the appendix? I added appendix \ref{sec:additive}}
The remainder of this section is dedicated to proving the theorem.
To this end, we use another equivalent representation of \eqref{eq:sums-OP}, which was also introduced by \cite{Ogryczak_2006}. 
\erel{This variant should move upwards, between P1 and P2.}
% (here also, the variables are $\ztVar{x}$ and $x$, and $z_1, \ldots z_{t-1}$ are constants)
\begin{align*}
    \max \quad &z_t \tag{P2-compact}\label{eq:compact-OP}\\
        s.t. \quad  & (1) \quad x \in S\\
                    & (\Tilde{2}) \quad \sum_{i=1}^{\ell} \valBy{i}{x} \geq \sum_{i=1}^{\ell}  z_i && \ell = 1,\ldots, t-1 \nonumber\\
                    & (\Tilde{3}) \quad \sum_{i=1}^{t} \valBy{i}{x} \geq \sum_{i=1}^{t}  z_i
\end{align*}
In this program, constraints $(\hat{2})$ and $(\hat{3})$ are replaced by  $(\Tilde{2})$ and $(\Tilde{3})$, respectively.  In  $(\hat{2})$, for each $\ell$, one gives a lower bound on the sum for \emph{any} set of $\ell$ objective functions.  In $(\Tilde{2})$, on the other hand, one only considers the sum of the $\ell$ \emph{smallest} such values.  However, since the constraints set the same lower bound on this sum, the constraints are equivalent.  Similarly for $(\hat{3})$ and $(\Tilde{3})$.  So, \eqref{eq:compact-OP} and \eqref{eq:sums-OP} are equivalent, which, in turn, is equivalent to \eqref{eq:vsums-OP}.  So, in proving the theorem we may assume that \textsf{OP} is an approximation procedure for \eqref{eq:compact-OP}.  This will simplify the proofs.

\erel{*** I do not understand. We say that P1 and P2 are equivalent with an exact solver, but not with an approximate solver. 
Here, we claim that P3 and P2-compact are equivalent, but this is true only with an exact solver. Don't we have to prove that they are equivalent also with an approximate solver? ***}

We denote $\retSol := x_n$ = the solution $x$ attained at the last iteration ($t=n$) of the algorithm. 

Following are some observations regarding the problem \eqref{eq:compact-OP} and the solution $\retSol$ that will be useful later.
First, notice that any solution $x \in S$ that satisfies constraint $(\Tilde{2})$ of \eqref{eq:compact-OP} is feasible to this problem.
This is because any solution $x \in S$ can satisfy constraint $(\Tilde{3})$ with a small enough assignment to the variable $z_t$. \eden{I'm not sure how to explain it....}
\begin{observation}\label{obs:feasi-and-constraint2}
Any solution $x \in S$ that satisfies constraint $(\Tilde{2})$ is feasible to \eqref{eq:compact-OP}.
\end{observation}

Since $\retSol$ is a feasible solution of \eqref{eq:compact-OP} in iteration $n$, and as each
iteration only adds new constraints to $(\Tilde{2})$, it follows that $\retSol$ is also a feasible solution of \eqref{eq:compact-OP} in any iteration $1 \leq t\leq n$. 
\begin{observation}\label{obs:retSol-solves-any-t}
$\retSol$ is a feasible solution of \eqref{eq:compact-OP} in any iteration $1 \leq t\leq n$.
\end{observation}

Now, consider the problem \eqref{eq:compact-OP} that was solved in iteration $t$.
Here, $z_t$ is a \emph{variable} and $z_1, \ldots z_{t-1}$ are constants.
The objective of this problem is $(\max z_t)$ and the only constraint that includes the variable $z_t$ is constraint $(\Tilde{3})$.
Therefore, rearranging it to $\sum_{i=1}^{t} \valBy{i}{x} - \sum_{i=1}^{t-1}  z_i\geq z_t$, allows us to conclude that the objective value is determined by the left side of this inequality (as $z_k$ is maximized when the inequality turns to equality).
\begin{observation}\label{obs:obj-value}
The objective value obtained by a feasible solution $x$ to the problem \eqref{eq:compact-OP} that was solved in iteration $t$ is $\sum_{i=1}^{t} \valBy{i}{x} - \sum_{i=1}^{t-1}  z_i$.
\end{observation}

Lastly, since the value obtained as a $(1 - \beta)$ approximation for this problem is the \emph{constant} $z_t$, the objective value of any feasible solution is at most $\frac{1}{1-\beta} z_t$.
It is particularly true for $\retSol$ since it is feasible for any iteration $t$ (Observation \ref{obs:retSol-solves-any-t}).
Since its objective is $\sum_{i=1}^t \valBy{i}{\retSol} - \sum_{i=1}^{t-1} z_i$ (Observation \ref{obs:obj-value}) we can conclude:

\begin{observation}\label{obs:obj-xt-to-zt}
    The objective value obtained by $\retSol$ to the problem \eqref{eq:compact-OP} that was solved in iteration $t$ is at most $\frac{1}{1-\beta} z_t$. That is:
    \begin{align*}
        \sum_{i=1}^t \valBy{i}{\retSol} - \sum_{i=1}^{t-1} z_i \leq \frac{1}{1-\beta} z_t
    \end{align*}
\end{observation}

% This conclusion also implies that for any $1 \leq t \leq n$, the solution $(x_t, z_t)$ that that was outputted for \eqref{eq:compact-OP} in iteration $t$, satisfies constraint $(\Tilde{3})$ as equality. That is:
% \begin{observation}\label{obs:equality-xt-zt}
% For any $1 \leq t \leq n$,  $\sum_{i=1}^{t} \valBy{i}{x_t} = \sum_{i=1}^{t}  z_i$.
% \end{observation}



%%%
% OVERALL EXPLANATION
%We first establish our motivation by demonstrating, in Lemma \ref{lemma:not-beta-approx}, that $\retSol$ is not necessarily a $1-\beta$ Leximin approximation.\eden{wasn't sure were to put lemma \ref{lemma:not-beta-approx}, before or after}
% However, we then prove it is actually not that far off, it is a $1-2\beta$ approximation. 
We start by Lemma \ref{lemma:retSol-approx-all-t} \eden{To myself: to do}
Then, Lemmas \ref{lemma:mult-fk-lower-bound}-\ref{lemma:mult-special-z_k-upper-bound} establish a relationship between the $k$-th least objective value at 
% the returned solution ($\retSol$)
$\retSol$
% solution  returned by Algorithm \ref{alg:basic-ordered-Outcomes} ($\retSol$) 
and the value $z_k$. 
% In particular, the main conclusion is that the first is at least $\frac{1-2\beta}{1-\beta}$ of the second.
Theorem \ref{th:main} then uses this relation to prove that the existence of another solution that would be $\frac{1}{1-2\beta}$-preferred over $\retSol$ would lead to a contradiction.

% LEMMAS.
% BLAH BLAH.

\begin{lemma}\label{lemma:beta-vk}
    Let $1 \leq k \leq n$. Then:
    \begin{align*}
        \beta \valBy{k}{\retSol} \geq \biggl(\sum_{i=1}^k \valBy{i}{\retSol} - \sum_{i=1}^k z_i\biggl) -\beta \biggl(\sum_{i=1}^{k-1} \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i\biggl) \emark{-\epsilon}
    \end{align*}
\end{lemma}

\begin{proof}
    The claim follows from Observation \ref{obs:obj-xt-to-zt}:
    \begin{align*}
         &\sum_{i=1}^k \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i \leq \frac{1}{1-\beta} z_k \emark{+\frac{1}{1-\beta}\epsilon}\\
         &\Rightarrow z_k \geq (1-\beta) \biggl(\sum_{i=1}^{k} \valBy{i}{\retSol} - \sum_{i=1}^{k-1}  z_i\biggl)\\
        &\Rightarrow z_k \geq \biggl(\sum_{i=1}^{k} \valBy{i}{\retSol} - \sum_{i=1}^{k-1}  z_i\biggl) - \beta \biggl(\sum_{i=1}^{k} \valBy{i}{\retSol} - \sum_{i=1}^{k-1}  z_i\biggl)\\
        &\Rightarrow \beta \valBy{k}{\retSol} \geq \biggl(\sum_{i=1}^k \valBy{i}{\retSol} - \sum_{i=1}^k z_i\biggl) -\beta \biggl(\sum_{i=1}^{k-1} \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i\biggl)
    \end{align*}
\end{proof}


\begin{lemma}\label{lemma:beta-sums-to-diff}
    Let $1 \leq k \leq n$. Then:
    \begin{align*}
        \sum_{i=1}^k \beta^{i} \valBy{k-i+1}{\retSol} \geq \sum_{i=1}^k \valBy{i}{\retSol} - \sum_{i=1}^{k} z_i
    \end{align*}
\end{lemma}

\begin{proof}
    The proof is by induction on $k$.
    For $k=1$ the claim follows directly from Lemma \ref{lemma:beta-vk}.
    Assuming that the claim is true for $1,\ldots k-1$, then it is true also for $k$:
    \begin{align*}
        &\sum_{i=1}^k \beta^{i} \valBy{k-i+1}{\retSol} = \beta \valBy{k}{\retSol} + \sum_{i=2}^k \beta^{i} \valBy{k-i+1}{\retSol}\\
        &= \beta \valBy{k}{\retSol} + \sum_{i=1}^{k-1} \beta^{i+1} \valBy{k-(i+1)+1}{\retSol} \\
        &= \beta \valBy{k}{\retSol} + \beta \sum_{i=1}^{k-1} \beta^{i} \valBy{(k-1) -i+1}{\retSol}\\
        &= \beta \valBy{k}{\retSol} + \beta \biggl(\sum_{i=1}^{k-1} \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i\biggl) && \text{induction assumption}\\
        &\geq \biggl(\sum_{i=1}^k \valBy{i}{\retSol} - \sum_{i=1}^k z_i\biggl) -\beta \biggl(\sum_{i=1}^{k-1} \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i\biggl)  && \text{Lemma \ref{lemma:beta-vk}} \\
        & \quad +  \beta \biggl(\sum_{i=1}^{k-1} \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i\biggl)\\
        &\geq \sum_{i=1}^k \valBy{i}{\retSol} - \sum_{i=1}^{k} z_i
    \end{align*}
\end{proof}


\begin{lemma}\label{lemma:fk-to-all}
    Let $1 \leq k \leq n$. Then:
    \begin{align*}
        \frac{1-\beta +\beta^2(1-\beta^{n})}{1-\beta} \valBy{k}{\retSol} \geq \valBy{k}{\retSol} + \beta\biggl(\sum_{i=1}^{k-1}\valBy{i}{\retSol} - \sum_{i=1}^{k-1}z_i\biggl)
    \end{align*}
\end{lemma}

\begin{proof}
First, notice that since $k \geq (k-1)-i+1$ for any $1\leq i \leq k$ and as the function $\valBy{i}$ represents the $i$-th smallest objective value, also:
    \begin{align}\label{eq:increase-by-obj-size}
        \forall 1\leq i \leq k \colon \quad \valBy{k}{\retSol} \geq \valBy{(k-1)-i+1}{\retSol}
    \end{align}
    In addition, consider the geometric series with a first element $1$, a ratio $\beta$, and a length $k-1$. 
    As $\beta \neq 1$, its sum can be described as:
    \begin{align}\label{eq:geometric-series-beta}
        \sum_{i=1}^{k-1} \beta^{i-1} = \frac{1-\beta^k}{1-\beta}
    \end{align}
    
    Now, the claim can be concluded as follows:
    \begin{align*}
        &\frac{1-\beta +\beta^2(1-\beta^{n})}{1-\beta} \valBy{k}{\retSol} = \biggl(1 + \frac{\beta^2(1-\beta^k)}{1-\beta}\biggl) \valBy{k}{\retSol} \\
        &= \valBy{k}{\retSol} + \beta^2 \biggl(\frac{1-\beta^k}{1-\beta} \valBy{k}{\retSol} \biggl)\\
        & = \valBy{k}{\retSol} + \beta^2 \biggl(\sum_{i=1}^{k-1} \beta^{i-1} \valBy{k}{\retSol} \biggl) && \text{equation \ref{eq:geometric-series-beta}}\\
        & \geq \valBy{k}{\retSol} + \beta^2 \biggl(\sum_{i=1}^{k-1} \beta^{i-1} \valBy{(k-1)-i+1}{\retSol} \biggl) && \text{equation \ref{eq:increase-by-obj-size}}\\
        &= \valBy{k}{\retSol} + \beta \sum_{i=1}^{k-1} \beta^{i} \valBy{(k-1)-i+1}{\retSol} \\
        &\geq \valBy{k}{\retSol} + \beta\biggl(\sum_{i=1}^{k-1}\valBy{i}{\retSol} - \sum_{i=1}^{k-1}z_i\biggl) && \text{Lemma \ref{lemma:beta-sums-to-diff}}
\end{align*}
\end{proof}



%------
% thm.

We are now ready to prove the Theorem \ref{th:main}.
\begin{proof}[Proof of Theorem \ref{th:main}]
\eden{should decide whether to keep this part}
We prove a slightly stronger result: the returned solution is a $(1 - \mu \beta)$ approximately-optimal, where $\mu=\frac{1-\beta+\beta(1-\beta^n)}{1-\beta +\beta^2(1-\beta^n)}$.
Notice that since $\beta < 1$, when the numbers of objective $n$ increases and tends to infinity the value of $\beta^n$ tends to zero and therefore $\mu$ tends to $\frac{1-\beta+\beta}{1-\beta +\beta^2} = \frac{1}{1-\beta +\beta^2}$.
Specifically, since $\mu < \frac{1}{1-\beta +\beta^2}$, proving that $\retSol$ is $(1 - \mu \beta)$ approximately-optimal also proves that it is $(1 - \frac{1}{1-\beta +\beta^2} \beta)$ approximately-optimal (by Lemma \ref{lemma:beta1-beta2-approx}).
This gives us a tighter bound on the approximation ratio with respect to $n$ and, at the same time, implies the original claim.

We first prove the following equation that will be helpful later:
\begin{align}\label{equ:mu}
\frac{1}{1 - \mu \beta } = \frac{1-\beta +\beta^2(1-\beta^{n})}{(1-\beta)^2}
\end{align}
This is true because:
\begin{align*}
    & 1 - \mu \beta =  1 - \frac{1-\beta+\beta(1-\beta^n)}{1-\beta +\beta^2(1-\beta^n)}\beta && \text{definition of $\mu$} \\
    & = \frac{1-\beta +\beta^2(1-\beta^n) - \beta(1-\beta)- \beta^2(1-\beta^n)}{1-\beta +\beta^2(1-\beta^{n})}\\
    & = \frac{(1-\beta)^2}{1-\beta +\beta^2(1-\beta^{n})}
\end{align*}
and therefore,
\begin{align*}
    & \frac{1}{1 - \mu \beta } = \frac{1-\beta +\beta^2(1-\beta^{n})}{(1-\beta)^2}
\end{align*}

    Now, suppose by contradiction that $\retSol$ is not a $(1 - \mu \beta)$-approximate Leximin solution.
    By definition, this means there exists a solution $y \in S$  that is $\frac{1}{(1 - \mu\beta)}$-preferred over it.
    That is, there exists an integer $1 \leq k \leq n$ such that:
    \begin{align*}
        \forall j < k \colon &\valBy{j}{y} \geq \valBy{j}{\retSol};\\
        & \valBy{k}{y} > \frac{1}{(1 - \mu\beta)} \cdot \valBy{k}{\retSol}.
    \end{align*}

    Recall that $\retSol$ was obtained in \eqref{eq:compact-OP} that was solved in the last iteration $n$.
    Therefore, $\sum_{i=1}^k \valBy{i}{\retSol} \geq \sum_{i=1}^{k} z_i$ (by constraint $(\Tilde{2})$ if $k<n$ and $(\Tilde{3})$ otherwise).
    Which implies:
    \begin{align}\label{eq:fk-to-zk}
        \sum_{i=1}^k \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i \geq z_k
    \end{align}

    Now, consider \eqref{eq:compact-OP} that was solved in iteration $t$.
    By Observation \ref{obs:retSol-solves-any-t}, $\retSol$ is feasible to this problem.
    As $y$'s $(k-1)$ least objective values are at least as high as those of $\retSol$, it is easy to conclude that $y$ satisfies constraint $(\Tilde{2})$ of this problem.
    Therefore, by Observation \ref{obs:feasi-and-constraint2}, $y$ is also feasible.
    
    However, the approximation obtained for this problem during the algorithm run is $z_k$, and so, the optimal value is at most $\frac{1}{(1-\beta)}z_k$.
    But, as we shall see, the objective $y$ yields in this problem, $\sum_{i=1}^k \valBy{i}{y} - \sum_{i=1}^{k-1} z_i$ (by Observation \ref{obs:obj-value}), is higher than this value, which is of course a contradiction:
    \begin{align*}
        &\sum_{i=1}^k \valBy{i}{y} - \sum_{i=1}^{k-1} z_i=\sum_{i=1}^{k-1} \valBy{i}{y} - \sum_{i=1}^{k-1} z_i + \valBy{k}{y}\\
        &\geq \sum_{i=1}^{k-1} \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i + \valBy{k}{y} && \text{$y$'s definition for $j<k$}\\
        &> \sum_{i=1}^{k-1} \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i + \frac{1}{(1 - \mu\beta)} \valBy{k}{\retSol} && \text{$y$'s definition for $k$}\\
        &= \sum_{i=1}^{k-1} \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i + \frac{1-\beta +\beta^2(1-\beta^{n})}{(1-\beta)^2} \valBy{k}{\retSol} && \text{equitation \ref{equ:mu}}\\
        &\geq\sum_{i=1}^{k-1} \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i + \frac{1}{1-\beta} \valBy{k}{\retSol} + \frac{\beta}{1-\beta}\biggl(\sum_{i=1}^{k-1}\valBy{i}{\retSol} - \sum_{i=1}^{k-1}z_i\biggl) && \text{Lemma \ref{lemma:fk-to-all}}\\
        & = \frac{1}{1-\beta} \biggl(\sum_{i=1}^k \valBy{k}{\retSol} - \sum_{i=1}^{k-1}z_i\biggl)  \geq \frac{1}{1-\beta} z_k && \text{equation \ref{eq:fk-to-zk}}
    \end{align*}
\end{proof}


\iffalse
Finally, to complete the picture, the following is an example that shows that the algorithm need not output a $(1-\beta)$ approximate Leximin.
Consider the following example with $n=2$:
    \begin{align*}
        \max \quad &\{f_1(x) := x_1, f_2(x) := x_2\} \\
        s.t. \quad  & x_1 \leq 100\\
                    & x_1 + x_2 \leq 200\\
                    & x \in \mathbb{R}^2_{+}
    \end{align*}
    Consider a run of the algorithm with \textsf{OP} that is a $(1-\beta)$ approximate solver, for $\beta=0.1$. 
    In the first iteration, the optimal value of $z_1$ is $100$, so \textsf{OP} may output $z_1=90$.  
    Then, in the second iteration, given $z_1=90$, the optimal value of $z_2$ is $110$, so \textsf{OP} may output $z_2=99$, which is obtained with  $x_1=x_2=94.5$.  
    \erel{Since the objectives are the $x_i$, how can the objective be $99$ and the $x_i$ be $94.5$?}
    But, $(x_1,x_2) = (94.5,105.5)$ is also a feasible solution, and $(94.5,94.5)$ is \emph{not} a $0.9$ approximation of $(94.5,105.5)$.
    \eden{this is not exactly accurate, we are not talking about solution that are approximation of one another, but about the relation. maybe something like "But, $(94.5,105.5)$ is also a feasible solution, and since $(94.5,105.5) \leximinPreferred_{\frac{1}{0.9}} (94.5,94.5)$, by definition 
    $(94.5,94.5)$ is not a $0.9$-approximation"}
\erel{This example does not seem very tight. Can you play with the numbers to get a tighter upper bound on the approximation ratio?}
\fi
