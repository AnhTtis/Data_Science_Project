
\section{Approximation Algorithm}\label{sec:algo-short}
We now present an algorithm for computing a multiplicative approximate leximin \eden{optimal?}. The algorithm is an adaptation of one of the algorithms of \textcite{Ogryczak_2006} for finding exact Leximin solutions. 

% \eden{yonatan}
% Following the \emph{definition} of Leximin, the core \emph{algorithm} for finding a Leximin optimal solution is interative, wherein one first maximizes the least objective function, then the second, and so forth. 
% Specifically, donating by $z_i$ the $i$-th smallest objective value, in each iteration, $t$, one seeks to maximize $z_t$, given the already calculated optimal values for $z_1,\ldots,z_{t-1}$.  The core, single-valued, optimization problem is thus:
%
% \eden{erel} 
% Following the definition of leximin, the core algorithm for finding a leximin optimal solution is iterative, wherein one first maximizes the least objective function, then the second, and so forth. 
% Denote by $z_t$ the value of the $i$-th smallest objective in the leximin-optimal solution, where $t\in\{1,\ldots,n\}$. Suppose we have already computed the optimal values $z_1,\ldots, z_{t-1}$. \eden{I'm not sure about it.. since we use the $z_t$ value later as the \textbf{approximation} value I think it could be a bit confusing}
% Then,  $z_t$ is the solution to the following single-objective optimization problem (where the variables are the scalar $z$ and the vector $x$):

\eden{how is this? I tried to combine the two versions}
Following the definition of Leximin, the core algorithm for finding a Leximin optimal solution is iterative, wherein one first maximizes the least objective function, then the second, and so forth. 
In each iteration, $t=1,\ldots,n$, it looks for the value that maximizes the $t$-th smallest objective, $z_t$, given that for any $i < t$ the $i$-th smallest objective is at least $z_i$ (the value that was computed in the $i$-th iteration).
The core, single-objective optimization problem is thus:
\begin{align}
 \max \quad &\ztVar{x}  \;\;
        s.t. &\quad  & (1) \quad x \in S  \tag{P1}\label{eq:basic-OP}\\
              &     & & (2) \quad \valBy{\ell}{x}\geq z_{\ell} & \ell = 1,\ldots,t-1\nonumber \\
               &    & & (3) \quad \valBy{t}{x} \geq \ztVar{x} \nonumber   
\end{align} 
where the variables are the scalar $\ztVar{x}$ and the vector $x$, and $z_1, \ldots z_{t-1}$ are constants.

Suppose we are given a procedure $\textsf{OP}(z_1,\ldots,z_{t-1})$, which, given  $z_1,\ldots,z_{t-1},$ outputs $(x,z)$ that is the exact optimal solution to \eqref{eq:basic-OP}.  
Then, the \emph{leximin} optimal solution is obtained by iterating this process for $t=1,\ldots,n$, as described in Algorithm \ref{alg:basic-ordered-Outcomes}.
\begin{algorithm}[!tbp]
\caption{The Ordered Outcomes Algorithm}
\label{alg:basic-ordered-Outcomes}
\begin{algorithmic}[1] %[1] enables line numbers
\FOR{$t=1$ to n}
\STATE \( %\begin{align*}
(x_t,z_t)\leftarrow \textsf{OP}(z_1,\ldots,z_{t-1})
\) %\end{align*} 
\ENDFOR
\STATE return $(z_1,\ldots,z_n)$ 
\end{algorithmic}
\end{algorithm}

However, it might be difficult to solve the problem \eqref{eq:basic-OP} as is, since constraints (2) and (3) are not linear even with respect to the objective-functions. 
\erel{I suggest to move P2-Compact here: it is a natural step between P1 and P2.}
\eden{}
 Thus, \cite{Ogryczak_2006} suggest using a variant of the problem that considers sums instead of individual values, which described as follows (similarly, the variables are $\ztVar{x}$ and $x$, and $z_1, \ldots z_{t-1}$ are constants):  
% \eden{maybe we should say it is not a linear program \textbf{with respect to the objective functions}?}
% \erel{I agree. Since the objective functions themselves may be non-linear.}
\begin{align*}
    \max \quad &\ztVar{x} \tag{P2}\label{eq:sums-OP}\\
        s.t. \quad  & (1) \quad x \in S\\
                    & (\hat{2}) \quad \sum_{f_i \in F'} f_i(x) \geq \sum_{i=1}^{|F'|}  z_i && \forall F' \subseteq \allObjFunc, |F'| < t \\
                    & (\hat{3}) \quad \sum_{f_i \in F'} f_i(x) \geq \sum_{i=1}^{t}  z_i  && \forall F' \subseteq \allObjFunc, |F'| = t
\end{align*}
Here, the constraints $(2)$ and $(3)$ are replaced with constraints $(\hat{2})$ and $(\hat{3})$, respectively. 
Constraint $(\hat{2})$ says that for any $\ell<t$, the sum of any $\ell$ objective functions is at least the sum of the first $\ell$ $z_i$'s. Similarly, $(\hat{3})$ says that the sum of any $t$  objective functions is at least the $t$ first $z_i$'s.  So, since we seek to maximize $z_t$, the optimal solution to \eqref{eq:sums-OP} and \eqref{eq:basic-OP} are provably identical (as in both $z_i$ is the $i$-th least objective value, see \cite{Ogryczak_2006} for a formal proof \eden{I'm actually not sure which of the papers to cite as I didn't find a formal proof (only the claim in given in \cite{Ogryczak_2006} and in \cite{Ogryczak2004TelecommunicationsND} Theorem 4)}). As such, in Algorithm \ref{alg:basic-ordered-Outcomes}, a solver for \eqref{eq:sums-OP} can be used (instead of for \eqref{eq:basic-OP}), and the algorithm will still output a Leximin optimal solution. 

Alas, while \eqref{eq:sums-OP} is linear with respect to the objective-functions, it has an exponential number of constraints. 
To overcome this problem, \cite{Ogryczak_2006} employ auxiliary variables ($y_{\ell}$ and $m_{\ell,j}$ for all $\ell \in [t]$ and $ j\in [n]$) to obtain a polynomial-sized program that is provably equivalent to \eqref{eq:sums-OP}:\eden{To myself: should explain "equivalent" in more detail, maybe to add the alternative proof}
% (where the variables are $\ztVar{x}$, $x$, and $y_{\ell}$ and $m_{\ell,j}$ for all $\ell \in [t]$ and $ j\in [n]$; and $z_1, \ldots z_{t-1}$ are constants)
\begin{align}
    \max \quad &z_t\tag{P3}\label{eq:vsums-OP}\\
        s.t. \quad  & (1) \quad x \in S \nonumber\\
                    & (2) \quad \ell y_{\ell} - \sum_{j=1}^n m_{\ell,j}\geq \sum_{i=1}^{\ell}  z_i && \ell = 1, \ldots,t-1 \nonumber \\
                    & (3) \quad t y_t - \sum_{j=1}^{n} m_{t,j} \geq \sum_{i=1}^{t}  z_i  \nonumber \\
                    & (4) \quad m_{\ell,j} \geq y_{\ell} - f_j(x)  && \ell = 1, \ldots,t,\Hquad j = 1, \ldots,n \nonumber \\
                    & (5) \quad m_{\ell,j} \geq 0  && \ell = 1, \ldots,t,\Hquad j = 1, \ldots,n \nonumber
\end{align}
So, in Algorithm \ref{alg:basic-ordered-Outcomes} using a solver for \eqref{eq:vsums-OP}, the algorithm will output a Leximin optimal solution. 

All of the above is for exact solutions.  
The question is what happens when we consider approximation procedures? It is easy to see that if \textsf{OP} is a $(1-\beta)$ approximation algorithm to \eqref{eq:basic-OP}, then Algorithm \ref{alg:basic-ordered-Outcomes} outputs a $(1-\beta)$ approximation of the Leximin. 
\erel{
To say this, we first have to define the notion of "approximation algorithm". It is known, but we still have to define it formally. Maybe in the "preliminaries" section.
} 
However, for approximations, \eqref{eq:sums-OP}, and the equivalent \eqref{eq:vsums-OP}, are no longer equivalent to \eqref{eq:basic-OP}. Indeed, the following example shows that if \textsf{OP} is a $(1-\beta)$ approximation procedure for \eqref{eq:vsums-OP}, then Algorithm \ref{alg:basic-ordered-Outcomes} may output a solution that is \emph{not} a $1-\beta$ approximation to the Leximin.
% \erel{Maybe it is better to move the example to here, to motivate our proof.}
Consider the following multi-objective optimization problem with $n=2$:
\begin{align*}
    \max \quad &\{f_1(x) := x_1, f_2(x) := x_2\} \\
    s.t. \quad  & x_1 \leq 100\\
                & x_1 + x_2 \leq 200\\
                & x \in \mathbb{R}^2_{+}
\end{align*}
Accordingly, the problem \eqref{eq:vsums-OP} becomes:
\begin{align*}
    \max \quad &z_t \\
    s.t. \quad  & (1.1) \quad  x_1 \leq 100\\
                & (1.2) \quad  x_1 + x_2 \leq 200\\
                & (1.3) \quad  x \in \mathbb{R}^2_{+}\\
                & (\hat{2}) \quad \sum_{f_i \in F'} f_i(x) \geq \sum_{i=1}^{|F'|}  z_i && \forall F' \subseteq \allObjFunc, |F'| < t \\
                & (\hat{3}) \quad \sum_{f_i \in F'} f_i(x) \geq \sum_{i=1}^{t}  z_i  && \forall F' \subseteq \allObjFunc, |F'| = t
\end{align*}
The following is a possible run of the algorithm with \textsf{OP} that is a $(1-\beta)$ approximate solver for $\beta=0.1$. 
In the first iteration, $t=1$, the optimal value of $z_1$ is $100$, so \textsf{OP} may output $z_1=90$.  
Then, in the second iteration, $t=2$, given that $z_1=90$, the optimal value of $z_2$ is $110$, so \textsf{OP} may output $z_2=99$, which is obtained (for example) with  $x_1=x_2=94.5$.  
\erel{Since the objectives are the $x_i$, how can the objective be $99$ and the $x_i$ be $94.5$?}\eden{is it more clear now?}
Since $n=2$, the algorithm ends after this iteration and returns this solution $(94.5,94.5)$.
But, $(x_1,x_2) = (94.5,105.5)$ is also a feasible solution, and $(94.5,105.5)\xPreferred{\frac{1}{0.9}}(94.5,94.5)$.
Therefore, by definition, the returned solution $(94.5,94.5)$ is \emph{not} a $0.9$ approximately-optimal solution.
% \eden{this is not exactly accurate, we are not talking about solution that are approximation of one another, but about the relation. maybe something like "But, $(94.5,105.5)$ is also a feasible solution, and since $(94.5,105.5)  \leximinPreferred_{\frac{1}{0.9}} (94.5,94.5)$, by definition  $(94.5,94.5)$ is not a $0.9$-approximation"}
\erel{This example does not seem very tight. Can you play with the numbers to get a tighter upper bound on the approximation ratio?}

 %On the other hand, \eqref{eq:vsums-OP} is the most solvable variant of the problem.  
So, using an approximate solver to \eqref{eq:vsums-OP}, what guarantee can we provide for the overall resulting solution? 
\begin{theorem}
\label{th:main}
Let $\beta<1/2$, and \textsf{OP} be a procedure that outputs a $(1-\beta)$ approximation to \eqref{eq:vsums-OP}. Then Algorithm \ref{alg:basic-ordered-Outcomes} outputs a $(1-2\beta)$-approximate Leximin solution.  
\end{theorem}

\eden{maybe we should also include the proof of the additive variant somewhere? if so. where?}
\erel{
Since the proof is not explicitly mentioned anywhere, I think it is good to include a proof for comparison. Maybe in an appendix.
}
\eden{if we want to do so we should also describe the additive variant definition, maybe to put them both in the appendix? I added appendix \ref{sec:additive}}
The remainder of this section is dedicated to proving the theorem.
To this end, we use another equivalent representation of \eqref{eq:sums-OP}, which was also introduced by \cite{Ogryczak_2006}. 
\erel{This variant should move upwards, between P1 and P2.}
% (here also, the variables are $\ztVar{x}$ and $x$, and $z_1, \ldots z_{t-1}$ are constants)
\begin{align*}
    \max \quad &z_t \tag{P2-compact}\label{eq:compact-OP}\\
        s.t. \quad  & (1) \quad x \in S\\
                    & (\Tilde{2}) \quad \sum_{i=1}^{\ell} \valBy{i}{x} \geq \sum_{i=1}^{\ell}  z_i && \ell = 1,\ldots, t-1 \nonumber\\
                    & (\Tilde{3}) \quad \sum_{i=1}^{t} \valBy{i}{x} \geq \sum_{i=1}^{t}  z_i
\end{align*}
In this program, constraints $(\hat{2})$ and $(\hat{3})$ are replaced by  $(\Tilde{2})$ and $(\Tilde{3})$, respectively.  In  $(\hat{2})$, for each $\ell$, one gives a lower bound on the sum for \emph{any} set of $\ell$ objective functions.  In $(\Tilde{2})$, on the other hand, one only considers the sum of the $\ell$ \emph{smallest} such values.  However, since the constraints set the same lower bound on this sum, the constraints are equivalent.  Similarly for $(\hat{3})$ and $(\Tilde{3})$.  So, \eqref{eq:compact-OP} and \eqref{eq:sums-OP} are equivalent, which, in turn, is equivalent to \eqref{eq:vsums-OP}.  So, in proving the theorem we may assume that \textsf{OP} is an approximation procedure for \eqref{eq:compact-OP}.  This will simplify the proofs.

\erel{*** I do not understand. We say that P1 and P2 are equivalent with an exact solver, but not with an approximate solver. 
Here, we claim that P3 and P2-compact are equivalent, but this is true only with an exact solver. Don't we have to prove that they are equivalent also with an approximate solver? ***}

We denote $\retSol := x_n$ = the solution $x$ attained at the last iteration ($t=n$) of the algorithm.
Also, the optimal value of \eqref{eq:compact-OP} in any iteration $1 \leq t\leq n$ is denoted by $\sOptT{t}$. 

Following are some observations regarding the problem \eqref{eq:compact-OP} and the solution $\retSol$ that will be useful later.
First, notice that any solution $x \in S$ that satisfies constraint $(\Tilde{2})$ of \eqref{eq:compact-OP} is feasible to this problem.
This is because any solution $x \in S$ can satisfy constraint $(\Tilde{3})$ with a small enough assignment to the variable $z_t$. \eden{I'm not sure how to explain it....}
\begin{observation}\label{obs:feasi-and-constraint2}
Any solution $x \in S$ that satisfies constraint $(\Tilde{2})$ is feasible to \eqref{eq:compact-OP}.
\end{observation}

Since $\retSol$ is a feasible solution of \eqref{eq:compact-OP} in iteration $n$, and as each
iteration only adds new constraints to $(\Tilde{2})$, it follows that $\retSol$ is also a feasible solution of \eqref{eq:compact-OP} in any iteration $1 \leq t\leq n$. 
\begin{observation}\label{obs:retSol-solves-any-t}
$\retSol$ is a feasible solution of \eqref{eq:compact-OP} in any iteration $1 \leq t\leq n$.
\end{observation}

Now, consider the problem \eqref{eq:compact-OP} that was solved in iteration $t$.
Here, $z_t$ is a \emph{variable} and $z_1, \ldots z_{t-1}$ are constants.
The objective of this problem is $(\max z_t)$ and the only constraint that includes the variable $z_t$ is constraint $(\Tilde{3})$.
Therefore, rearranging it to $\sum_{i=1}^{t} \valBy{i}{x} - \sum_{i=1}^{t-1}  z_i\geq z_t$, allows us to conclude that the objective value is determined by the left side of this inequality (as $z_k$ is maximized when the inequality turns to equality).
\begin{observation}\label{obs:obj-value}
The objective value obtained by a feasible solution $x$ to the problem \eqref{eq:compact-OP} that was solved in iteration $t$ is $\sum_{i=1}^{t} \valBy{i}{x} - \sum_{i=1}^{t-1}  z_i$.
\end{observation}

This conclusion also implies that for any $1 \leq t \leq n$, the solution $(x_t, z_t)$ that that was outputted for \eqref{eq:compact-OP} in iteration $t$, satisfies constraint $(\Tilde{3})$ as equality. That is:
\begin{observation}\label{obs:equality-xt-zt}
For any $1 \leq t \leq n$,  $\sum_{i=1}^{t} \valBy{i}{x_t} = \sum_{i=1}^{t}  z_i$.
\end{observation}



%%%
% OVERALL EXPLANATION
%We first establish our motivation by demonstrating, in Lemma \ref{lemma:not-beta-approx}, that $\retSol$ is not necessarily a $1-\beta$ Leximin approximation.\eden{wasn't sure were to put lemma \ref{lemma:not-beta-approx}, before or after}
% However, we then prove it is actually not that far off, it is a $1-2\beta$ approximation. 
We start by Lemma \ref{lemma:retSol-approx-all-t} \eden{To myself: to do}
Then, Lemmas \ref{lemma:mult-fk-lower-bound}-\ref{lemma:mult-special-z_k-upper-bound} establish a relationship between the $k$-th least objective value at 
% the returned solution ($\retSol$)
$\retSol$
% solution  returned by Algorithm \ref{alg:basic-ordered-Outcomes} ($\retSol$) 
and the value $z_k$. 
% In particular, the main conclusion is that the first is at least $\frac{1-2\beta}{1-\beta}$ of the second.
Theorem \ref{th:main} then uses this relation to prove that the existence of another solution that would be $\frac{1}{1-2\beta}$-preferred over $\retSol$ would lead to a contradiction.

% LEMMAS.
% BLAH BLAH.

 

\begin{lemma}\label{lemma:retSol-approx-all-t}
    Let $1 \leq t \leq n$. Then, the objective value obtained by $\retSol$ to the problem \eqref{eq:compact-OP} that was solved in iteration $t$ is a $(1-\beta)$ approximation.
\end{lemma}

\begin{proof}
    First, Observation \ref{obs:retSol-solves-any-t} ensures that $\retSol$ is a feasible solution for \eqref{eq:compact-OP} that was solved in  iteration $t$. 
    
    In addition, as $\retSol$ is a feasible solution of \eqref{eq:compact-OP} that was solved in iteration $n$, it is easy to see that $\sum_{i=1}^t \valBy{i}{\retSol} \geq \sum_{i=1}^{t} z_i$ (by constraint $(\Tilde{2})$ if $t<n$ and $(\Tilde{3})$ otherwise).
    By Observation \ref{obs:equality-xt-zt}, the solution $(x_t,z_t)$ obtained in iteration $t$ satisfies $\sum_{i=1}^t \valBy{i}{x_t} = \sum_{i=1}^{t} z_i$.
    We can conclude that:
    \begin{align}\label{equ:retSol-higher-than-xt}
        \sum_{i=1}^t \valBy{i}{\retSol} \geq \sum_{i=1}^t \valBy{i}{x_t}
    \end{align}
    The same Observation implies that $\sum_{i=1}^{t} \valBy{i}{x_t} - \sum_{i=1}^{t-1}  z_i = z_t$.
    However, $z_t$ is a $(1-\beta)$ approximation of $\sOptT{t}$, and so:
    \begin{align}\label{equ:xt-and-opt-t}
         \sum_{i=1}^{t} \valBy{i}{x_t} - \sum_{i=1}^{t-1}  z_i = z_t \geq (1-\beta) \sOptT{t}
     \end{align}
     
    Lastly, as Observation \ref{obs:obj-value} says that the objective values obtained by $\retSol$ to the problem \eqref{eq:compact-OP} that was solved in iteration $t$ is $\sum_{i=1}^{t} \valBy{i}{\retSol} - \sum_{i=1}^{t-1}  z_i$, we can conclude that this value is also a $(1-\beta)$ approximation of $\sOptT{t}$:
    \begin{align*}
        \sum_{i=1}^{t} \valBy{i}{\retSol} - \sum_{i=1}^{t-1}  z_i &\geq \sum_{i=1}^{t} \valBy{i}{x_t} - \sum_{i=1}^{t-1}  z_i && \text{equation \ref{equ:retSol-higher-than-xt}}\\
        &\geq (1-\beta) \sOptT{t} && \text{equation \ref{equ:xt-and-opt-t}}
    \end{align*}
\end{proof}




\begin{lemma}\label{lemma:mult-fk-lower-bound}
    For any $1 < k \leq n \colon \quad \valBy{k}{\retSol} \geq (1-\beta)\sOptT{k} - \frac{\beta}{1-\beta} z_{k-1}$.
\end{lemma}

\begin{proof}
    First, by Observation \ref{obs:retSol-solves-any-t}, $\retSol$ is also a feasible solution of \eqref{eq:compact-OP} in any iteration $t\in [n]$.
    By Observation \ref{obs:obj-value} the objective value that $\retSol$ yields in the problem solved in iteration $t$ is $\sum_{i=1}^t \valBy{i}{\retSol} - \sum_{i=1}^{t-1} z_i$.
    However, during the algorithm run, the value obtained as a $(1 - \beta)$ approximation for this problem is $z_t$.
    This implies that the objective value of any feasible solution, in particular of $\retSol$, is at most $\frac{1}{1-\beta} z_t$.
    Therefore: 
    \begin{align}\label{equ:mult-other-sol-to-OP-upper-bound}
        \sum_{i=1}^t \valBy{i}{\retSol} - \sum_{i=1}^{t-1} z_i \leq \frac{1}{1-\beta} z_t.
    \end{align}

    Now, let $1 < k \leq n$. 
    By Observation \ref{obs:equality-xt-zt}, the solution $(x_k,z_k)$ obtained in iteration $k$ satisfies  $\sum_{i=1}^k \valBy{i}{x_k} = \sum_{i=1}^{k} z_i$.
    On the one hand, as the value $z_k$ is a $(1-\beta)$ approximation of $\sOptT{k}$, this implies the following:
    \begin{align*}
        \sum_{i=1}^k \valBy{i}{x_k} - \sum_{i=1}^{k-1} z_i = z_k \geq (1-\beta) \sOptT{k}
    \end{align*}
    On the other hand, it is easy to see that $\sum_{i=1}^k \valBy{i}{\retSol} \geq \sum_{i=1}^{k} z_i$ (by constraint $(\Tilde{2})$ if $k<n$ and $(\Tilde{3})$ otherwise), which also means that $\sum_{i=1}^k \valBy{i}{\retSol} \geq \sum_{i=1}^{k} \valBy{i}{x_t}$. 
    The following can be concluded:
    \begin{align}
        \sum_{i=1}^k \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i \geq \sum_{i=1}^k \valBy{i}{x_k} - \sum_{i=1}^{k-1} z_i = z_k \geq (1-\beta) \sOptT{k}
    \end{align}
    % Notice that by Observation \ref{obs:obj-value} this means that $\retSol$ also yields a $(1-\beta)$ approximation for $\sOptT{k}$.
    
    Now, the claim can concluded by isolating $\valBy{k}{\retSol}$:
    \begin{align*}
        \valBy{k}{\retSol} &\geq \sum_{i=1}^k z_i - \sum_{i=1}^{k-1} \valBy{i}{\retSol} \\
        & = z_k + z_{k-1} - \Bigr[\sum_{i=1}^{k-1} \valBy{i}{\retSol} - \sum_{i=1}^{k-2} z_i \Bigr] \\
        & \geq z_k + z_{k-1} - \frac{1}{1-\beta}z_{k-1}  && \text{(by equation \eqref{equ:mult-other-sol-to-OP-upper-bound})}
        \\
        & = z_k - \frac{\beta}{1-\beta} z_{k-1}
    \end{align*}
\end{proof}


% ----- lemma 2 
\begin{lemma}\label{lemma:mult-zk-upper-bound}
    For any $1 < k \leq n \colon \quad z_k \leq \sum_{i=1}^k \biggr[\frac{\beta}{1-\beta}\biggr]^{i-1} \valBy{k-i+1}{\retSol}$.
\end{lemma}


\begin{proof}
    The proof is by induction on $k$.
    For $k=2$, applying Lemma \ref{lemma:mult-fk-lower-bound} and rearranging it results in $z_2 \leq \valBy{2}{\retSol} + \frac{\beta}{1-\beta} z_{1}$. 
    However, constraint $(\Tilde{2})$ for $\ell = 1$ in the problem solved in iteration $n$ ensures that $\valBy{1}{\retSol} \geq z_1$.
    % \erel{In iteration $t=1$? In what constraint?}
    Accordingly, the claim can easily concluded:
    \begin{align*}
        z_2 &\leq \valBy{2}{\retSol} + \frac{\beta}{1-\beta} z_{1} \leq \valBy{2}{\retSol} + \frac{\beta}{1-\beta} \valBy{1}{\retSol} = \sum_{i=1}^2 \biggr[\frac{\beta}{1-\beta}\biggr]^{i-1} \valBy{2-i+1}{\retSol}
    \end{align*}
    Next, assuming that the claim holds for $2, \dots, k-1$, by rearranging Lemma \ref{lemma:mult-fk-lower-bound}, it follows that it also holds for $k$:
    \begin{align*}
        z_k &\leq \valBy{k}{\retSol} + \frac{\beta}{1-\beta} z_{k-1}\\
        & \leq \valBy{k}{\retSol} + \frac{\beta}{1-\beta} \sum_{i=1}^{k-1} \biggr[\frac{\beta}{1-\beta}\biggr]^{i-1} \valBy{k-1-i+1}{\retSol} && \text{induction assumption}\\
        & \leq \valBy{k}{\retSol} + \sum_{i=1}^{k-1} \biggr[\frac{\beta}{1-\beta}\biggr]^{(i+1)-1} \valBy{k-(i+1)+1}{\retSol}\\
        & = \valBy{k}{\retSol} + \sum_{i=2}^{k} \biggr[\frac{\beta}{1-\beta}\biggr]^{i-1} \valBy{k-i+1}{\retSol} && \text{index change (sum)}\\
        & = \sum_{i=1}^{k} \biggr[\frac{\beta}{1-\beta}\biggr]^{i-1} \valBy{k-i+1}{\retSol}
    \end{align*}
\end{proof}


% ---------- lemma 3 
\begin{lemma}\label{lemma:mult-special-z_k-upper-bound}
    For any $1 \leq k \leq n \colon \quad 
        \frac{1-\beta}{1 - 2\beta} \Biggl(1 - \biggr[\frac{\beta}{1-\beta}\biggr]^k\Biggl) \valBy{k}{\retSol} \geq z_k$
\end{lemma}

% \eden{we should decide if we include the following: \textit{"We prove a slightly stronger result: the approximation ratio is at most $\phi\cdot \beta$, where $\phi=$..."} I'm not sure where to put it....}

\begin{proof}
    Let $1 \leq k \leq n$. 
    First, observe that the expression $\sum_{i=1}^k \Bigr[\frac{\beta}{1-\beta}\Bigr]^{i-1}$ describes the sum of the geometric series with a first element $1$, a ratio $\frac{\beta}{1-\beta}$, and a length $k$. 
    As $\beta >1/2$, the ratio $\frac{\beta}{1-\beta} \neq 1$.
    Therefore, we can conclude the following:
    \begin{align*}
        \sum_{i=1}^k \Bigr[\frac{\beta}{1-\beta}\Bigr]^{i-1} = \frac{\biggr[\frac{\beta}{1-\beta}\biggr]^k - 1}{\frac{\beta}{1-\beta}-1}= \frac{1-\beta}{2\beta - 1} \Biggl(\biggr[\frac{\beta}{1-\beta}\biggr]^k - 1\Biggl) = \frac{1-\beta}{1 - 2\beta} \Biggl(1 - \biggr[\frac{\beta}{1-\beta}\biggr]^k\Biggl)
    \end{align*}
    
    In addition, since $k-i+1 \leq k$ for any $1\leq i \leq k$, also $\valBy{k-i+1}{\retSol} \leq \valBy{k}{\retSol}$.
    Accordingly, the claim can be concluded from Lemma \ref{lemma:mult-zk-upper-bound}:
    \begin{align*}
        z_k &\leq \sum_{i=1}^k \biggr[\frac{\beta}{1-\beta}\biggr]^{i-1} \valBy{k-i+1}{\retSol} \leq \sum_{i=1}^k \biggr[\frac{\beta}{1-\beta}\biggr]^{i-1} \valBy{k}{\retSol} \\
        &\leq \frac{1-\beta}{1 - 2\beta} \Biggl(1 - \biggr[\frac{\beta}{1-\beta}\biggr]^k\Biggl) \valBy{k}{\retSol} 
    \end{align*}
\end{proof}

%------
% thm.

We are now ready to prove the Theorem \ref{th:main}.
\begin{proof}[Proof of Theorem \ref{th:main}]
We prove a slightly stronger result: the returned solution is a $(1 - \mu \beta)$ approximately-optimal, where $\mu=\biggl(2- \frac{1}{\beta} \biggr[\frac{\beta}{1-\beta}\biggr]^n\biggl)/\biggl(1-\biggr[\frac{\beta}{1-\beta}\biggr]^n\biggl)$.
Notice that since $\biggr[\frac{\beta}{1-\beta}\biggr] < 1$, when the numbers of objective $n$ increases and tends to infinity the value of $ \biggr[\frac{\beta}{1-\beta}\biggr]^n$ tends to zero and therefore $\mu$ tends to $2$.\eden{is this clear enough?}
Specifically, since $\mu < 2$, by applying Lemma \ref{lemma:beta1-beta2-approx}, we get that if a solution is $(1 - \mu \beta)$ approximately-optimal then it is also $(1 - 2 \beta)$ approximately-optimal.
Therefore, proving that $\retSol$ is $(1 - \mu \beta)$ approximately-optimal  gives us a tighter bound on the approximation ratio with respect to $n$, while at the same time implies the original claim ---  $\retSol$ is $(1 - 2 \beta)$ approximately-optimal regardless of the number of objectives.
% $\phi=\frac{2- \beta \biggr[\frac{\beta}{1-\beta}\biggr]^k}{1-\biggr[\frac{\beta}{1-\beta}\biggr]^k}$.
% 

We first prove the following equation that will be helpful later:
\begin{align}\label{equ:mu}
\frac{1}{1 - \mu \beta } = \frac{1}{1 - 2\beta} \Biggl(1 - \biggr[\frac{\beta}{1-\beta}\biggr]^k\Biggl)
\end{align}
This is true because:
\begin{align*}
    \mu &= \biggl(2- \frac{1}{\beta} \biggr[\frac{\beta}{1-\beta}\biggr]^n\biggl)/\biggl(1-\biggr[\frac{\beta}{1-\beta}\biggr]^n\biggl)  && \text{definition of $\mu$}\\
    & = \biggl(2- \frac{\beta^{n-1}}{(1-\beta)^n}\biggl)/\biggl(1-\frac{\beta^n}{(1-\beta)^n}\biggl) = \frac{2(1-\beta)^n - \beta^{n-1}}{(1-\beta)^n - \beta^{n}}
\end{align*}
and therefore,
\begin{align*}
    \frac{1}{1 - \mu \beta } &= \frac{1}{1 - \frac{2\beta(1-\beta)^n - \beta^{n}}{(1-\beta)^n - \beta^{n}}} = 
    \frac{1}{\frac{(1-\beta)^n - \beta^{n} - 2\beta(1-\beta)^n + \beta^{n}}{(1-\beta)^n - \beta^{n}}} \\
    & = \frac{(1-\beta)^n - \beta^{n}}{ (1 - 2\beta)(1-\beta)^n}= \frac{1}{1 - 2\beta} \Biggl(1 - \biggr[\frac{\beta}{1-\beta}\biggr]^n\Biggl)
\end{align*}

Now, suppose for contradiction that $\retSol$ is not a $(1 - \mu \beta)$-approximate Leximin solution.
    By definition, this means there exists a solution $y \in S$  that is $\frac{1}{(1 - \mu\beta)}$-preferred over it.
    That is, there exists an integer $1 \leq k \leq n$ such that:
    \begin{align*}
        \forall j < k \colon &\valBy{j}{y} \geq \valBy{j}{\retSol};\\
        & \valBy{k}{y} > \frac{1}{(1 - \mu\beta)} \cdot \valBy{k}{\retSol}.
    \end{align*}
    
    As mentioned before, since $\retSol$ is a feasible solution to the optimization problem that was solved in iteration $n$, it is also feasible for the problem that was solved in iteration $k$ (as constraints were only added).
    As $y$'s $k$ least objective values are at least as those of $x$, this implies that $y$ 
    % satisfies all the constraints of this problem \eden{should we say something about constraint $(\Tilde{3})$? I'm mot sure how to explain it} 
    is feasible for this problem as well. 
    Since the objective of this problem is $(\max z_k)$ and the variable $z_k$ is included only in constraint $(\Tilde{3})$, the objective value of $y$ is determined by $\sum_{i=1}^k \valBy{i}{y} - \sum_{i=1}^{k-1} z_i$.
    However, the approximation obtained for this problem during the algorithm run is $z_k$. 
    This implies that the optimal value value for this problem is at most $\frac{1}{(1-\beta)}z_k$.
    But, as we shall see, the objective of $y$ in this problem is higher than the optimal value, which is of course a contradiction.
    \begin{align*}
        &\sum_{i=1}^k \valBy{i}{y} - \sum_{i=1}^{k-1} z_i\\
        &=  \valBy{k}{y} + \Bigr[\sum_{i=1}^{k-1} \valBy{i}{y} - \sum_{i=1}^{k-1} z_i\Bigr] \\
        & \geq  \valBy{k}{y} +0&& \text{constraint ($\Tilde{2}$) for } \ell=k-1\\
        &> \frac{1}{(1 - \mu\beta)} \cdot \valBy{k}{\retSol} && \text{$y$'s definition for } k\\
        & = \frac{1}{1 - 2\beta} \Biggl(1 - \biggr[\frac{\beta}{1-\beta}\biggr]^n\Biggl) \valBy{k}{\retSol} && \text{equation \ref{equ:mu}}\\
        &\geq\frac{1}{1 - 2\beta} \Biggl(1 - \biggr[\frac{\beta}{1-\beta}\biggr]^k\Biggl) \valBy{k}{\retSol} && k \leq n \\
        & = \frac{1}{1-\beta} \biggr[\frac{1-\beta}{1 - 2\beta} \Biggl(1 - \biggr[\frac{\beta}{1-\beta}\biggr]^k\Biggl) \valBy{k}{\retSol}\biggr]\\
        &\geq \frac{1}{1-\beta} z_k && \text{Lemma \ref{lemma:mult-special-z_k-upper-bound}}
    \end{align*}
\end{proof}
\iffalse
Finally, to complete the picture, the following is an example that shows that the algorithm need not output a $(1-\beta)$ approximate Leximin.
Consider the following example with $n=2$:
    \begin{align*}
        \max \quad &\{f_1(x) := x_1, f_2(x) := x_2\} \\
        s.t. \quad  & x_1 \leq 100\\
                    & x_1 + x_2 \leq 200\\
                    & x \in \mathbb{R}^2_{+}
    \end{align*}
    Consider a run of the algorithm with \textsf{OP} that is a $(1-\beta)$ approximate solver, for $\beta=0.1$. 
    In the first iteration, the optimal value of $z_1$ is $100$, so \textsf{OP} may output $z_1=90$.  
    Then, in the second iteration, given $z_1=90$, the optimal value of $z_2$ is $110$, so \textsf{OP} may output $z_2=99$, which is obtained with  $x_1=x_2=94.5$.  
    \erel{Since the objectives are the $x_i$, how can the objective be $99$ and the $x_i$ be $94.5$?}
    But, $(x_1,x_2) = (94.5,105.5)$ is also a feasible solution, and $(94.5,94.5)$ is \emph{not} a $0.9$ approximation of $(94.5,105.5)$.
    \eden{this is not exactly accurate, we are not talking about solution that are approximation of one another, but about the relation. maybe something like "But, $(94.5,105.5)$ is also a feasible solution, and since $(94.5,105.5) \leximinPreferred_{\frac{1}{0.9}} (94.5,94.5)$, by definition 
    $(94.5,94.5)$ is not a $0.9$-approximation"}
\erel{This example does not seem very tight. Can you play with the numbers to get a tighter upper bound on the approximation ratio?}
\fi

