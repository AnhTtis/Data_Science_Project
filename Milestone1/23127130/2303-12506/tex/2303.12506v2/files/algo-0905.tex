\section{Approximation Algorithm}\label{sec:algo-short}
We now present an algorithm for computing an approximately leximin-optimal solution. 
The algorithm is an adaptation of one of the algorithms of Ogryczak
and {\'{S}}liwi{\'{n}}ski \cite{Ogryczak_2006} for finding exact leximin solutions. 
We prove that given a $(\multApprox,\additiveApprox)$-approximation algorithm for the single-objective problem, the output is a $\left(\frac{\multApprox^2}{1-\multApprox + \multApprox^2}, \frac{\additiveApprox}{1-\multApprox +\multApprox^2}\right)$-approximation for the multi-objective problem.

\subsection{Preliminary: exact leximin-optimal solution}
Following the definition of leximin, the core algorithm for finding a leximin optimal solution is iterative, wherein one first maximizes the least objective function, then the second, and so forth. 
In each iteration, $t=1,\ldots,n$, it looks for the value that maximizes the $t$-th smallest objective, $z_t$, given that for any $i < t$ the $i$-th smallest objective is at least $z_i$ (the value that was computed in the $i$-th iteration).
The core, single-objective optimization problem is thus:
\begin{align}
 \max &&&\ztVar{x}   \tag{\progBasic}\label{eq:basic-OP}\\
        s.t. &&& (\text{\progBasic.1}) \Hquad x \in S \nonumber\\
              &&& (\text{\progBasic.2}) \Hquad \valBy{\ell}{x}\geq z_{\ell} & \forall \ell \in [t-1] \nonumber \\
               &&& (\text{\progBasic.3}) \Hquad \valBy{t}{x} \geq \ztVar{x} \nonumber   
\end{align} 
where the variables are the scalar $\ztVar{x}$ and the vector $x$, whereas $z_1, \ldots z_{t-1}$ are constants (computed in previous iterations).

Suppose we are given a procedure $\textsf{OP}(z_1,\ldots,z_{t-1})$, which, given  $z_1,\ldots,z_{t-1},$ outputs $(x,z_t)$ that is the exact optimal solution to \eqref{eq:basic-OP}.  
Then, the \emph{leximin} optimal solution is obtained by iterating this process for $t=1,\ldots,n$, as described in Algorithm \ref{alg:basic-ordered-Outcomes}.
\begin{algorithm}[!tbp]
\caption{The Ordered Outcomes Algorithm}
\label{alg:basic-ordered-Outcomes}
\begin{algorithmic}[1] %[1] enables line numbers
\FOR{$t=1$ to n}
\STATE \( %\begin{align*}
(x_t,z_t)\leftarrow \textsf{OP}(z_1,\ldots,z_{t-1})
\) %\end{align*} 
\ENDFOR
% \STATE \textbf{return} {$x_n$ (with objective values $z_1,\ldots,z_n$)}.
% the above it true only for the exact case 
\STATE \textbf{return} {$x_n$ (with objective values $f_1(x_n),\ldots,f_n(x_n)$)}.
\end{algorithmic}
\end{algorithm}

However, it might be difficult to solve the problem \eqref{eq:basic-OP} as is, since constraints (\progBasic.2) and (\progBasic.3) are not linear even with respect to the objective-functions. 
 Thus, \cite{Ogryczak_2006} suggest the following optimization problem with the auxiliary variables $y_{\ell}$ and $m_{\ell,j}$ for $\ell = 1, \ldots, t$ and $j = 1, \ldots, n$:
% (where the variables are $\ztVar{x}$, $x$, and $y_{\ell}$ and $m_{\ell,j}$ for all $\ell \in [t]$ and $ j\in [n]$; and $z_1, \ldots z_{t-1}$ are constants)
\begin{align}
\max &&& \ztVar{x} \tag{\progLinear}\label{eq:vsums-OP} \\
s.t. &&& (\text{\progLinear.1}) \Hquad x \in S \nonumber  \\
                    &&& (\text{\progLinear.2}) \Hquad \ell y_{\ell} - \sum_{j=1}^n m_{\ell,j}\geq \sum_{i=1}^{\ell}  z_i && \forall \ell \in [t-1] \nonumber \\
                    &&& (\text{\progLinear.3}) \Hquad t y_t - \sum_{j=1}^{n} m_{t,j} \geq \sum_{i=1}^{t}  z_i  \nonumber \\
                    &&& (\text{\progLinear.4}) \Hquad m_{\ell,j} \geq y_{\ell} - f_j(x)  && \forall \ell \in [t],\Hquad \forall j  \in [n] \nonumber \\
                    &&& (\text{\progLinear.5}) \Hquad m_{\ell,j} \geq 0  && \forall \ell \in [t],\Hquad \forall j  \in [n] \nonumber
\end{align}
% This is a polynomial-sized problem.
% \eden{we can also use $1 \leq \ell \leq t,\Hquad 1 \leq j  \leq n$ but $\ell = 1, \ldots,t,\Hquad j = 1, \ldots,n$ is too ling}
% \eden{should probably change the name P3 if we no longer have P2}
The importance of problem \eqref{eq:vsums-OP} for leximin is shown by the following theorem (Theorem 1 in \cite{Ogryczak_2006})):
\begin{theorem*}
% \label{lemma:alg-1-can-use-sums-exact}
If Algorithm \ref{alg:basic-ordered-Outcomes} is applied with a solver for \eqref{eq:vsums-OP}\footnote{Actually, the algorithm uses a solver for \eqref{eq:vsums-OP} and then takes only the assignment of the variables $x$ and $z_t$, ignoring the auxiliary variables.}  (instead of for \eqref{eq:basic-OP}), the algorithm still outputs a leximin-optimal solution. 
\end{theorem*}
% \noindent For completeness, Appendix \ref{sec:algo-sec-proofs} provides an alternative proof of this lemma. \eden{to add it to the section}
%--------
% Our main result (Theorem \ref{th:main})  extends their theorem from exact leximin-optimal solutions to $(\alpha,\epsilon)$-approximate leximin solutions for every $\alpha \in(0,1]$ and $\epsilon\geq 0$.
We shall later see that our main result (Theorem \ref{th:main}) extends and implies their theorem. 



\subsection{Using an approximate solver}
Now we assume that, instead of an exact solver in Algorithm \ref{alg:basic-ordered-Outcomes}, we only have an approximate solver. 
In this case, the constants $z_1,\ldots,z_{t-1}$ are only approximately-optimal solutions for the previous iterations.
It is easy to see that if \textsf{OP} is a $(\multApprox,\additiveApprox)$-approximation algorithm\footnote{See Section \ref{sec:preliminaries} for the  formal definition of approximation algorithm.} to \eqref{eq:basic-OP}, then Algorithm \ref{alg:basic-ordered-Outcomes} outputs a $(\multApprox,\additiveApprox)$-approximately leximin-optimal solution\footnote{See Section \ref{sec:approx-leximin-def} for the formal definition of approximately leximin-optimal solution.} (a formal proof is given in Appendix \ref{sub:approximate-P1}).

However, we shall now see that if \textsf{OP} 
is a $(\multApprox,\additiveApprox)$-approximation algorithm for \eqref{eq:vsums-OP}, Algorithm \ref{alg:basic-ordered-Outcomes} may output a solution that is \emph{not} $(\multApprox,\additiveApprox)$-approximate leximin-optimal.
Then, we will prove that 
if \textsf{OP} 
is a $(\multApprox,\additiveApprox)$-approximation algorithm for \eqref{eq:vsums-OP}, Algorithm \ref{alg:basic-ordered-Outcomes} always outputs a solution that is  $\left(\frac{\multApprox^2}{1-\multApprox + \multApprox^2}, \frac{\additiveApprox}{1-\multApprox +\multApprox^2}\right)$-approximate leximin-optimal.


In order to demonstrate both claims more clearly, we will use the following optimization problem, which was also introduced by \cite{Ogryczak_2006}:
\begin{align*}
    \max &&& z_t \tag{\progCompact}\label{eq:compact-OP} \\
    s.t. &&& (\text{\progCompact.1}) \Hquad x \in S\nonumber\\
                    &&& (\text{\progCompact.2}) \Hquad \sum_{i=1}^{\ell} \valBy{i}{x} \geq \sum_{i=1}^{\ell}  z_i && \forall \ell \in [t-1] \nonumber\\
                    &&& (\text{\progCompact.3}) \Hquad \sum_{i=1}^{t} \valBy{i}{x} \geq \sum_{i=1}^{t}  z_i
\end{align*}
Here, constraint (\progCompact.2) says that for any $\ell<t$, the sum of the \emph{smallest}  $\ell$ objective functions is at least the sum of the first $\ell$ constants $z_i$'s. 
Similarly, (\progCompact.3) says that the sum of the \emph{smallest} $t$  objective functions is at least the sum of the first $t-1$ constants  $z_i$'s, plus the variable $z_t$.
In Appendix \ref{sub:equivalence-P2-P3} we prove 
that the problems \eqref{eq:vsums-OP} and \eqref{eq:compact-OP} are \emph{equivalent} in the following sense:
\begin{lemma}
\label{lem:equivalence}
    Let $t \in [n]$ and let $z_1, \ldots z_{t-1} \in \mathbb{R}$.
    Then, $(x, z_t)$ is feasible for \eqref{eq:compact-OP} if and only if there exist $y_{\ell}$ and $m_{\ell,j}$ for $\ell \in [t]$ and $j \in [n]$ such that $\left(x, z_t, (y_1, \ldots, y_t), (m_{1,1}, \ldots m_{t,n})\right)$ is feasible for \eqref{eq:vsums-OP}.
\end{lemma}
Since both  \eqref{eq:compact-OP} and \eqref{eq:vsums-OP} have the same objective function ($\max z_t$), the lemma implies that $(x,z_t)$ is an $(\alpha,\epsilon)$-approximate solution for \eqref{eq:compact-OP} if and only if $(x,z_t)$ is a part of an $(\alpha,\epsilon)$-approximate solution for \eqref{eq:vsums-OP}.

%  \begin{lemma}
%      Let $t \in [n]$, $z_1, \ldots z_{t-1} \in \mathbb{R}$, $0 <\multApprox \leq 1$ and $\additiveApprox \geq 0$.
%      Then, $(x, z_t)$ is an $(\multApprox, \additiveApprox)$-approximate solution to \eqref{eq:compact-OP} if and only if there exist $y_{\ell}$ and $m_{\ell,j}$ for $\ell \in [t]$ and $j \in [n]$ such that $\left(x, z_t, (y_1, \ldots, y_t), (m_{1,1}, \ldots m_{t,n})\right)$ is an $(\multApprox, \additiveApprox)$-approximate solution to \eqref{eq:vsums-OP}.
% \end{lemma}
%-------------------
% \begin{lemma}
%     Let $A$ be the set of approximate solutions $(x,z_t)$ that might return by an $(\multApprox, \additiveApprox)$-approximate solver for \eqref{eq:vsums-OP}. 
%     Them, the set $A$ also describes set of approximate solutions $(x,z_t)$ that might return by an $(\multApprox, \additiveApprox)$-approximate solver for \eqref{eq:compact-OP}.
% \end{lemma}
%----------------
% \begin{lemma}\label{lemma:sums-eq-to-aux}
% Let $t \in [n]$ and let 
% $z_1,\ldots,z_{t-1} \in \mathbb{R}$.
% There exists a bijection mapping each solution $(x,z_t)$ to \eqref{eq:sums-OP}, to a unique solution to \eqref{eq:vsums-OP} with the same $x$ and $z_t$.
% \end{lemma}

% \begin{lemma}\label{lemma:op3-to-comp}
%     For any $t \in [n]$ and any constants $z_1, \ldots z_{t-1} \in \mathbb{R}$, let $A$ be the set of $(x,z_t)$ that might be returned by an $(\multApprox, \additiveApprox)$-approximate solver for \eqref{eq:vsums-OP}. 
%     Then, $(x,z_t) \in A$  if and only if it is an $(\multApprox, \additiveApprox)$-approximation for \eqref{eq:compact-OP}.
% \end{lemma}
% Since the objective value in both problems is determined by $z_t$, \eqref{eq:compact-OP} can be used to identify optimal and approximate solutions to \eqref{eq:vsums-OP}.

\begin{theorem}
There exist $\multApprox\in (0,1]$, $\additiveApprox \geq 0$ and \textsf{OP} that is an $(\multApprox,\additiveApprox)$-approximation procedure to \eqref{eq:vsums-OP}, such that if Algorithm \ref{alg:basic-ordered-Outcomes} is applied with this procedure, it might return a solution that is not 
an $(\multApprox,\additiveApprox)$-approximate leximin-optimal.  
\end{theorem}
\begin{proof}
% in order to show that Algorithm \ref{alg:basic-ordered-Outcomes} might return a solution that is \emph{not} approximately-optimal, 
Based on Lemma \ref{lem:equivalence}, we can prove the theorem for \eqref{eq:compact-OP}.
Consider the following multi-objective optimization problem with $n=2$:
\begin{align*}
    \max \quad &\{f_1(x) := x_1, f_2(x) := x_2\} \\
    s.t. \quad  & (1.1) \Hquad x_1 \leq 100, \quad (1.2) \Hquad x_1 + x_2 \leq 200, \quad (1.3) \Hquad x \in \mathbb{R}^2_{+}
\end{align*}
In the corresponding \eqref{eq:compact-OP}, constraint (\progCompact.1) will be replaced with constraints (1.1)-(1.3).

The following is a possible run of the algorithm with \textsf{OP} that is a $(\multApprox,\additiveApprox)$-approximate solver for $\multApprox=0.9$ and $\additiveApprox = 0$. 
In iteration $t=1$, condition (\progCompact.2) is empty, and the optimal value of $z_1$ is $100$, so \textsf{OP} may output $z_1=0.9\cdot 100 = 90$.  
In iteration $t=2$, given  $z_1=90$, 
condition (\progCompact.2) says that both $x_1$ and $x_2$ must be at least $90$;
the optimal value of $z_2$ under these constraints is $110$, so \textsf{OP} may output $z_2=99$, for example with  $x_1=x_2=94.5$.  
Since $n=2$, the algorithm ends 
and returns the solution $(94.5,94.5)$.
But, $(x_1,x_2) = (94.5,105.5)$ is also a feasible solution, and $(94.5,105.5) \alphaBetaPreferredParams{0.9}{0} (94.5,94.5)$.
Therefore, by definition, the returned solution is \emph{not} $(0.9,0)$-approximately leximin-optimal.

However, according to our main result (Theorem \ref{th:main}), it is not too far from that --- it is a $(\frac{81}{91},0)\approx (0.89,0)$-approximately leximin-optimal.


%-------------------------------
% with P-linear
% In the corresponding \eqref{eq:vsums-OP}, constraint (1) will be replaced with constraints (1.1)-(1.3).
% The following is a possible run of the algorithm with \textsf{OP} that is a $(\multApprox,\additiveApprox)$-approximate solver for $\multApprox=0.9$ and $\additiveApprox = 0$\footnote{The objective functions in the example are linear, and therefore, \eqref{eq:vsums-OP} becomes a linear program and the optimal values can also be calculated using any LP solver (such as CVXPY \cite{diamond2016cvxpy}).}. 
% % Notice that for simplicity, we use an example in which $\additiveError=0$; however, this also allows us to demonstrate that even when there is only a multiplicative error, the algorithm behaves differently.
% %
% %
% % \eden{in this version it's hard to give intuition, maybe to say that it can be verified with any LP solver like cvxpy}
% In iteration $t=1$, the optimal value of $z_1$ is $100$, so \textsf{OP} may output $z_1=0.9\cdot 100 = 90$. 
% In iteration $t=2$, given  $z_1=90$,
% the optimal value of $z_2$ is $110$, so \textsf{OP} may output $z_2=99$, for example with  $x_i= y_i = 94.5$ for $i=1,2$, and $m_{\ell,j} = 0$ for $\ell = 1,2$ and $j =1,2$.
% Since $n=2$, the algorithm ends 
% and returns this solution with utilities $(94.5,94.5)$.
% But, taking $x_1 = 94.5, x_2 = 105.5,z_2 = X, y_1 = 94.5, y_2 = 105.5, m_{1,1}= m_{1,2} = m_{2,2} = 0, m_{2,1} = 11$ gives a feasible solution with utilities $(94.5,105.5)$, which is $(0.9,0)$-preferred over the returned solution.
% Therefore, by definition, the returned solution is \emph{not} $(0.9,0)$-approximately-optimal.

\erel{Maybe: try to show why (P1) is different.}
\end{proof}

Nonetheless, using an approximate solver to \eqref{eq:vsums-OP}, we can guarantee a non-trivial approximation to the leximin-optimal solution.

\begin{theorem}\label{th:main}
Let $\multApprox\in (0,1]$, $\additiveApprox \geq 0$, and \textsf{OP} be a $(\multApprox,\additiveApprox)$-approximation procedure to \eqref{eq:vsums-OP}. Then Algorithm \ref{alg:basic-ordered-Outcomes} outputs a $\left(\frac{\multApprox^2}{1-\multApprox + \multApprox^2}, \frac{\additiveApprox}{1-\multApprox +\multApprox^2}\right)$-approximately leximin-optimal leximin solution.  
\end{theorem}

The proof is given in Appendix \ref{sub:th:main} due to space restrictions.


\noindent Notice that this result also implies that if \textsf{OP} only has a multiplicative error ($\additiveApprox = 0$), the solution returned by Algorithm \ref{alg:basic-ordered-Outcomes} will only have a multiplicative error as well, and if \textsf{OP} only has an additive error ($\multApprox = 1$), the solution returned by Algorithm \ref{alg:basic-ordered-Outcomes} will have only the same additive error $\additiveError$.


\subsection{Using a randomized solver}
Next, we assume that the solver for OP is not only approximate but  also \emph{randomized}.
Specifically, we assume that we have a solver that always returns a feasible solution to the single-objective problem, and 
with probability $p \in [0,1]$ it is also approximately-optimal.
As Algorithm \ref{alg:basic-ordered-Outcomes} activates the \textsf{OP} solver $n$ times overall, assuming the success events of different activations are independent, there is a probability of $p^n$ that
\textsf{OP} returns an approximately-optimal solution in every iteration, so Algorithm \ref{alg:basic-ordered-Outcomes} performs as in the previous subsection.
 This leads to the following conclusion:
\begin{corollary}\label{corollary:main-with-probability}
Let $\multApprox\in (0,1]$, $\additiveError \geq 0$, and \textsf{OP} be a \emph{$p$-randomized} $(\multApprox, \additiveError)$-approximation procedure to \eqref{eq:vsums-OP}. Then Algorithm \ref{alg:basic-ordered-Outcomes} outputs a $\left(\frac{\multApprox^2}{1-\multApprox + \multApprox^2}, \frac{\additiveApprox}{1-\multApprox +\multApprox^2}\right)$-approximately leximin-optimal solution with probability $p^n$.
\end{corollary}

Notice that, since the procedure \textsf{OP} always returns a feasible solution to the single-objective problem, Algorithm \ref{alg:basic-ordered-Outcomes} always returns a feasible solution as well.

