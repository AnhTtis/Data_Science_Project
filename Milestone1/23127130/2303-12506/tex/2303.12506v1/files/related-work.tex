\subsection{Related Work}
This paper is related to a large body of research, which can be classified into three main fields: multi-objective optimization problems, approximation variants of known solution concepts, and algorithms for finding optimal leximin solutions.

In general multi-objective\footnote{Multi-objective is also called \emph{multi-criteria} (for example in \cite{ehrgott_multicriteria_2005}).} optimization problems, finding a leximin\footnote{Leximin is also called \emph{Max-Min fairness} (for example in \cite{nace_max-min_2008}), \emph{Lexicographic Min-Max} (for example in \cite{Ogryczak_2006}), \emph{Lexicographic max-ordering} (for example in \cite{ehrgott_multicriteria_2005}) and \emph{Leximax} (for example in \cite{henzinger_et_al_FORC_2022}).} optimal solution is quite common goal \cite{ehrgott_multicriteria_2005}, which is still an open challenge.
Studies on this topic are usually focused on a specific problem and leverages its special characteristics --- the structure of the \textit{feasible region} and \textit{objective-functions} that describe the concrete problem at hand.
In this paper, we focus on the widely studied domain of resource allocation problems \cite{moulin2004fair}.
In that context, as leximin maximization is an extension of egalitarian welfare maximization, it is usually mentioned when fairness is desired.

There are cases where a leximin optimal solution can be calculated in polynomial time, for example in: fair allocation of divisible items \cite{willson}, giveaway lotteries \cite{arbiv_fair_2022}, portioning with ordinal preferences\cite{ airiau_portioning_2019}, cake sharing \cite{bei_truthful_2022}, multi-commodity ﬂow networks \cite{nace_max-min_2008}, and location problems \cite{Ogryczak_1997_loc}.
However, even when algorithms are theoretically polynomial, they can still be inaccurate in practice, for example due to numeric round-off errors.
% \eden{maybe this is a better place for the example that the saturation algorithm never ends?}
% \erel{Related Work usually does not contain detailed examples. I suggest to keep the example in the intro, or move to an appendix.}


In other cases, calculating a leximin optimal solution is NP-hard, for example in: representative
cohort selection \cite{henzinger_et_al_FORC_2022}, fair combinatorial auctions \cite{bouveret_computing_2009}, package upgrade-ability \cite{cabral2022sat}, allocating papers to referees \cite{garg_assigning_2010, lian_conference_2018}, and stochastic allocations of indivisible goods (Section \ref{sec:app} in this paper).
% \erel{Can you add more details? In what cases is it polynomial and in what cases it is NP-hard?}
% \eden{is this better?}
However, to our knowledge, 
% approximate leximin in the context of NP-hard problems usually refers in an empirically context (e.g.~in \cite{bouveret_computing_2009,cabral2022sat}).
% S
studies of this kind typically suggest non-polynomial algorithms and heuristics for solving small instances of the general problem and empirically evaluate their efficiency, rather than suggesting polynomial-time approximation algorithms.
%
% \eden{to myself: need to look on these papers again}
% \erel{I do not understand what you mean by "empirical estimate". Can you add more details on these papers? 
% I thought that they do a heuristic search for an optimal solution.}
% \eden{the first evaluate the algorithm in a 10 min experiment in which they count how many instances the algorithm has managed to solve}

\iffalse % EREL: Removed for now - to complete later
Another approach to approximate leximin (and in fact, to approximate other solution concepts) is to suggest ad-hoc variants that are meaningful for the given problem, for example \cite{garg_assigning_2010, danna_2017_upward}.
\erel{This is not clear - can you add more details?}
Although it makes a lot of sense, in this paper, we aim to suggest a definition that is as general as possible. 
\fi

% WLU
Another approach to leximin optimization is to approximate the optimality of a solution with a single number, that is, find an aggregation function that takes a utility vector and returns a number such that if a solution is leximin-preferred if and only if its aggregate number is higher.
Finding such a function will of course reduce the problem to solving only one single-objective optimization problem.
In \cite{sherali1983preemptive} it is proved that, for any linear lexicographic optimization problem, there exists a set of weights such that the weighted average of the objectives is maximized if and only if the solution is lexicographically optimal; however, they mention that finding this set of weights for each particular problem is not practical.

There is a great deal of research in which the goal is to find functions that are as close to the ideal as possible  (for example, \cite{yager_ordered_1988} in which \emph{ordered weighted averaging (OWA)} technique was introduced).
% also: (for example in \cite{yager_ordered_1988,yager_analytic_1997,lian_conference_2018}). 
Unfortunately, it is known that no aggregate function can represent the leximin ordering in \emph{all} problems \cite{moulin2004fair,Ogryczak2004TelecommunicationsND}.
% \eden{ moulin2004fair at page 72}. \eden{ Ogryczak2004TelecommunicationsND at page 3}.

\iffalse % EREL: Removed from now. To complete later.
Since we aim for a definition that in the absence of errors would be identical to the original definition, we neglect this approach. 
\erel{It is not entirely clear how the last sentence follows from the previous one.}
\fi

% general definition
To the best of our knowledge, other general approximations of leximin exist but they are less common.
They are usually mentioned in the context of robustness or noise (e.g.~\cite{kalai_lexicographic_2012,henzinger_et_al_FORC_2022}) and lack characteristics that we emphasize within the context of error.
% ; therefore, we also consider approximate variants of related solution concepts alongside it.
% The following are some examples.
%--------
% Kalaı et al. \cite{kalai_lexicographic_2012} presented a new preference relation that aims to relax the leximin order.
% \erel{Is it the same as our second example above? If not, I suggest to give more details and an example, as this is a "competitor" of our approach --- an alternative relaxation to leximin.}\eden{It is not the same, their definition is more complex. I have a version with more details but I wasn't sure how to explain it without notations (which we haven't provided yet).}
% \eden{Their preference relation can be viewed as follows (it is presented in our context of utility maximization instead of their context of cost \emph{minimization}):
% an alternative $y$ is $\alpha$-preferred over an alternative $x$ if  there exists an integer $k \in [n]$ such that the element-wise difference between the $(k-1)$ smallest values achieved by $y$ and $x$ is at most $\alpha$, and the $k$'th smallest value achieved by $y$ is higher by more than $\alpha$ than the $k$'th smallest value achieved by $x$, that is:
% \begin{align*}
%     \forall j < k \colon &|\valBy{j}{y} - \valBy{j}{x}| \leq \alpha\\
%     & \valBy{k}{y} > \valBy{k}{x} + \alpha
% \end{align*}}
% However, the leximin optimal solution is not a maximum element according to this order.
% Several approximation variants of leximin were presented by \textcite{henzinger_et_al_FORC_2022}, some of them a 
 % This might make sense in the context of robustness (which they investigated), but not in our case.
%
% \cite{aziz_2022} 
% % EREL: Note you can use \textcite{...} to automatically add the authors' names to the citation.
% presented a relaxation of the \emph{Maximin Share (MMS)} solution concept, according to which a solution is approximately optimal if each agent gets at least some fraction of the value it would have gotten from an optimal solution.
% \erel{Do you mean this maximin share? \url{https://en.wikipedia.org/wiki/Maximin-share} It was introduced much before Aziz et al. And I do not think it is a relaxation of leximin, so it is not relevant.}
% \eden{I tried to say that Aziz et al. introduce an \emph{approximation} for maximin share and explain why the same \emph{idea} cannot be used to approximate leximin. would it be better to remove it?}
% However, this approach would not work in the context of leximin, since allowing one value to be smaller might increase the following optimal values.
% \textcite{aumann_pareto_2010} proposed a way to quantify the \emph{distance} from Pareto efficiency.
% In fact, their approach is very similar to ours as they defined a new preference relation and built the approximation definition upon it
% % Their approach is actually very similar in spirit to our approach as they also defined a new \eden{stronger} preference relation and the approximation definition is build on top of that, i.e., an approximately optimal solution is one on which no other solution is dominates.

%Henzinger et al
Most similar to our work is the recent paper by \textcite{henzinger_et_al_FORC_2022}.
This paper presents several approximation variants of leximin for the case of \emph{additive} errors in the single-objective problems.
Their motivation is different than ours --- they use approximation as a method to improve efficiency and ensure robustness to noise. However, one of their definitions,  (\emph{$\epsilon$-tradeoff Leximax}) fits our motivation of achieving the best possible leximin-approximation in the presence of errors.
% \erel{It is worthwhile to quote the passage in their Future Work, in which they mention multiplicative approximation.}
In fact, our approximation definition can be viewed as a generalization of their definition to include both multiplicative and additive errors.
It should also be noted that the authors mention multiplicative approximation in the their Future Work section:
\begin{quote}
\textit{In another direction, our approximation notions all reason about allowing for additive
amounts of error. However, considering what notions, especially those in line with $\epsilon$-significant
recursive leximax, might arise from multiplicative error could be a useful direction to explore.}
\end{quote}


\iffalse % EREL: We already mentioned this in the introduction - no need to repeat.
Given our definition (described in Section \ref{sec:approx-leximin-def}), our second goal is an algorithm that in the absence of errors would return a leximin optimal solution, while in the presence of errors would return an appropriate approximation.
However, existing leximin optimization algorithms may return solutions that are very far from the optimum when errors are present.
For instance, as the leximin concept considers only the order of outcomes rather than identities, a very common idea in leximin optimization is \emph{saturation}.
According to this idea, at each point, the algorithm fixes the values of the objectives that cannot be further improved, i.e., are saturated.
Indeed, different versions of the same algorithm, which is based on this idea, have been independently proposed many times, for example in \cite{Ogryczak2004TelecommunicationsND, willson,airiau_portioning_2019,nace_max-min_2008}\footnote{The algorithm is fully described in Appendix \ref{sec:saturation-algorithm}.}.
In this case, errors in the single-objective solver might cause this algorithm to never end.
\eden{should we include the example? if so, where?}
\erel{If we describe the algorithm in Appendix \ref{sec:saturation-algorithm}, then we should present the examples afterwards in the same appendix.
But maybe it is better to describe both the algorithm and the example at the intro, after the examples for the definitions.}
\fi

% EREL: Moved to "Contributions" 