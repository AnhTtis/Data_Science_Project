\section{Proof of Theorem \ref{th:main}}\label{sec:algo-sec-proofs}
\eden{should probably change the title}

This section is dedicated to proving Theorem \ref{th:main}.
To this end, we use another equivalent representation of \eqref{eq:sums-OP}, which was also introduced by \cite{Ogryczak_2006} 
(we provide the proof of equivalence in Appendix \ref{sec:equivalent-proofs}). 
\erel{Can't we just use it directly instead of P2?}
% (here also, the variables are $\ztVar{x}$ and $x$, and $z_1, \ldots z_{t-1}$ are constants)
\begin{align*}
    \max \quad &z_t \tag{P2-compact}\label{eq:compact-OP} \;\;
        s.t. &\quad  & (1) \quad x \in S\\
                    &&& (\Tilde{2}) \quad \sum_{i=1}^{\ell} \valBy{i}{x} \geq \sum_{i=1}^{\ell}  z_i && \ell = 1,\ldots, t-1 \nonumber\\
                    &&& (\Tilde{3}) \quad \sum_{i=1}^{t} \valBy{i}{x} \geq \sum_{i=1}^{t}  z_i
\end{align*}
In this problem, constraints $(\hat{2})$ and $(\hat{3})$ are replaced by  $(\Tilde{2})$ and $(\Tilde{3})$, respectively.  
The difference is that
$(\hat{2})$ gives, for each $\ell$, a lower bound on the sum for \emph{any} set of $\ell$ objective functions; whereas $(\Tilde{2})$ only considers the sum of the $\ell$ \emph{smallest} such values.  
% However, since the constraints set the same lower bound on this sum, the constraints are equivalent.  
Similarly for $(\hat{3})$ and $(\Tilde{3})$. 
Since  the problems are equivalent, a solver, either exact or approximate, for one can be used as a solver, with the same level of accuracy, for the other (Lemma \ref{lemma:solver-equivalent-prob}). 
Therefore, as \eqref{eq:compact-OP} is equivalent to \eqref{eq:sums-OP}, which, in turn, is equivalent to \eqref{eq:vsums-OP}, in proving the theorem we may assume that \textsf{OP} is an approximation procedure for \eqref{eq:compact-OP}.  
This will simplify the proofs. \eden{I added the line from the comment back, isn't it important to explain why we need this representation?}

% \erel{*** I do not understand. We say that P1 and P2 are equivalent with an exact solver, but not with an approximate solver. Here, we claim that P3 and P2-compact are equivalent, but this is true only with an exact solver. Don't we have to prove that they are equivalent also with an approximate solver? ***}

We denote $\retSol := x_n$ = the solution $x$ attained at the last iteration ($t=n$) of the algorithm. 

Following are some observations regarding the set of feasible solutions in each iteration, their objective values, and the solution $\retSol$ that will be useful later on.

% For any constants $z_1,\ldots, z_{t-1}$,
% any vector $x \in S$ that satisfies constraint $(\Tilde{2})$ of \eqref{eq:compact-OP} 
% is feasible to this problem.
% This is because any solution $x \in S$ can satisfy constraint $(\Tilde{3})$ with a small enough assignment to the variable $z_t$. \eden{I'm not sure how to explain it....}
\begin{observation}\label{obs:feasi-and-constraint2}
For any constants $z_1,\ldots, z_{t-1}$,
any vector $x \in S$ that satisfies constraint $(\Tilde{2})$ of \eqref{eq:compact-OP} 
can be a part of a feasible solution $(x,z_t)$ for any $z_t \leq \sum_{i=1}^{t} \valBy{i}{x} - \sum_{i=1}^{t-1} z_i$.
\end{observation}

Since $\retSol$ is a feasible solution of \eqref{eq:compact-OP} in iteration $n$, and as each
iteration only adds new constraints to $(\Tilde{2})$, it follows that $\retSol$ is also a feasible solution of \eqref{eq:compact-OP} in any iteration $1 \leq t\leq n$. 
\begin{observation}\label{obs:retSol-solves-any-t}
$\retSol$ is a feasible solution of \eqref{eq:compact-OP} in any iteration $1 \leq t\leq n$.
\end{observation}

Now, consider the problem \eqref{eq:compact-OP} that was solved in iteration $t$.
Here, $z_t$ is a \emph{variable} and $z_1, \ldots z_{t-1}$ are constants.
The objective of this problem is $\max z_t$, and the only constraint that includes the variable $z_t$ is  $(\Tilde{3})$.
Therefore, rearranging it to $\sum_{i=1}^{t} \valBy{i}{x} - \sum_{i=1}^{t-1}  z_i\geq z_t$, allows us to conclude that the objective value is determined by the left side of this inequality (as $z_t$ is maximized when the inequality turns to equality).
\begin{observation}\label{obs:obj-value}
The objective value obtained by a feasible solution $x$ to the problem \eqref{eq:compact-OP} that was solved in iteration $t$ is $\sum_{i=1}^{t} \valBy{i}{x} - \sum_{i=1}^{t-1}  z_i$.
\end{observation}

Lastly, as the value obtained as a $(\multApprox, \additiveApprox)$-approximation for this problem is the \emph{constant} $z_t$, the optimal value is at most $\frac{1}{\multApprox} (z_t+\additiveError)$. 
Consequently, the objective value of any feasible solution is at most this value.
Since $\retSol$ is feasible for any iteration $t$ (Observation \ref{obs:retSol-solves-any-t}) and since its objective is $\sum_{i=1}^t \valBy{i}{\retSol} - \sum_{i=1}^{t-1} z_i$ (Observation \ref{obs:obj-value}), we can conclude:

\begin{observation}\label{obs:obj-xt-to-zt}
    The objective value obtained by $\retSol$ to the problem \eqref{eq:compact-OP} that was solved in iteration $t$ is at most $\frac{1}{\multApprox} (z_t+\additiveError)$. That is:
    \begin{align*}
        \sum_{i=1}^t \valBy{i}{\retSol} - \sum_{i=1}^{t-1} z_i \leq \frac{1}{\multApprox} \left(z_t+\additiveError \right).
    \end{align*}
\end{observation}

% This conclusion also implies that for any $1 \leq t \leq n$, the solution $(x_t, z_t)$ that that was outputted for \eqref{eq:compact-OP} in iteration $t$, satisfies constraint $(\Tilde{3})$ as equality. That is:
% \begin{observation}\label{obs:equality-xt-zt}
% For any $1 \leq t \leq n$,  $\sum_{i=1}^{t} \valBy{i}{x_t} = \sum_{i=1}^{t}  z_i$.
% \end{observation}



%%%
% OVERALL EXPLANATION 
We start with Lemmas \ref{lemma:beta-vk}-\ref{lemma:fk-to-all}, which establish a relationship between the $k$-th least objective value obtained by $\retSol$ 
% ($\valBy{k}{\retSol}$) 
and the difference between the sum of the $(k-1)$ least objective values obtained by $\retSol$ and the sum of the $(k-1)$ first $z_i$ values.
% ($\sum_{i=1}^{k-1}\valBy{k}{\retSol} - \sum_{i=1}^{k-1}z_i$). 
Theorem \ref{th:main} then uses this relation to prove that the existence of another solution that would be $\left(\frac{\multApprox^2}{1-\multApprox + \multApprox^2}, \frac{\multApprox(2-\multApprox)\additiveApprox}{1-\multApprox +\multApprox^2}\right)$-preferred over $\retSol$ would lead to a contradiction.

For clarity, throughout the proofs, we denote the multiplicative error factor by $\multError = 1-\multApprox$.

% LEMMAS.
% BLAH BLAH.

\begin{lemma}\label{lemma:beta-vk}
    For all $k\in[n]$, 
    \begin{align*}
        \multError \valBy{k}{\retSol} \geq \left(\sum_{i=1}^k \valBy{i}{\retSol} - \sum_{i=1}^k z_i\right) -\multError \left(\sum_{i=1}^{k-1} \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i\right) -\additiveError
    \end{align*}
\end{lemma}

\begin{proof}
By Observation \ref{obs:obj-xt-to-zt},
    \begin{align*}
         &\sum_{i=1}^k \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i \leq \frac{1}{\multApprox} \left(z_k + \additiveError \right) = \frac{1}{1-\multError} \left(z_k + \additiveError \right)\\
         &\Rightarrow z_k +\additiveError \geq (1-\multError) \left(\sum_{i=1}^{k} \valBy{i}{\retSol} - \sum_{i=1}^{k-1}  z_i\right)\\
        &\Rightarrow z_k +\additiveError\geq \left(\sum_{i=1}^{k} \valBy{i}{\retSol} - \sum_{i=1}^{k-1}  z_i\right) - \multError \left(\sum_{i=1}^{k} \valBy{i}{\retSol} - \sum_{i=1}^{k-1}  z_i\right)\\
        &\Rightarrow \multError \valBy{k}{\retSol} \geq \left(\sum_{i=1}^k \valBy{i}{\retSol} - \sum_{i=1}^k z_i\right) -\multError \left(\sum_{i=1}^{k-1} \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i\right) -\additiveError.
        \qedhere
    \end{align*}
\end{proof}


\begin{lemma}\label{lemma:beta-sums-to-diff}
    For all $k\in[n]$, 
    \begin{align*}
        \sum_{i=1}^k \multError^{i} \valBy{k-i+1}{\retSol} \geq \sum_{i=1}^k \valBy{i}{\retSol} - \sum_{i=1}^{k} z_i -\additiveError
    \end{align*}
\end{lemma}

\begin{proof}
    The proof is by induction on $k$.
    For $k=1$ the claim follows directly from Lemma \ref{lemma:beta-vk}.
    Assuming the claim is true for $1,\ldots k-1$, we show it is true for $k$:
    \begin{align*}
        &\sum_{i=1}^k \multError^{i} \valBy{k-i+1}{\retSol} = \multError \valBy{k}{\retSol} + \sum_{i=2}^k \multError^{i} \valBy{k-i+1}{\retSol}\\
        &= \multError \valBy{k}{\retSol} + \sum_{i=1}^{k-1} \multError^{i+1} \valBy{k-(i+1)+1}{\retSol} \\
        &= \multError \valBy{k}{\retSol} + \multError \sum_{i=1}^{k-1} \multError^{i} \valBy{(k-1) -i+1}{\retSol}\\
        &= \multError \valBy{k}{\retSol} + \multError \left(\sum_{i=1}^{k-1} \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i\right) && \text{(by induction assumption)}\\
        &\geq \left(\sum_{i=1}^k \valBy{i}{\retSol} - \sum_{i=1}^k z_i\right) -\multError \left(\sum_{i=1}^{k-1} \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i\right)-\additiveError  \\
        & \quad +  \multError \left(\sum_{i=1}^{k-1} \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i\right) && \text{(by Lemma \ref{lemma:beta-vk})} \\
        &= \sum_{i=1}^k \valBy{i}{\retSol} - \sum_{i=1}^{k} z_i -\additiveError.
        \qed
    \end{align*}
\end{proof}


\begin{lemma}\label{lemma:fk-to-all}
    For all $1<k \leq n$, 
    \begin{align*}
        \frac{\multError}{1-\multError} \valBy{k}{\retSol} \geq \sum_{i=1}^{k-1}\valBy{i}{\retSol} - \sum_{i=1}^{k-1}z_i - \additiveError
    \end{align*}
\end{lemma}

\begin{proof}
    First, notice that since $k \geq (k-1)-i+1$ for any $1\leq i \leq k$ and as the function $\valBy{i}$ represents the $i$-th smallest objective value, also:
    \begin{align}\label{eq:increase-by-obj-size}
        \forall 1\leq i \leq k \colon \quad \valBy{k}{\retSol} \geq \valBy{(k-1)-i+1}{\retSol}
    \end{align}
    In addition, consider the geometric series with a first element $1$, a ratio $\multError$, and a length $(k-1)$. 
    As $\multError < 1$, its sum can be bounded in the following way:
    \begin{align}\label{eq:geometric-series-beta}
        \sum_{i=1}^{k-1} \multError^{i-1} = \frac{1-\multError^{k-1}}{1-\multError} < \lim_{k \to \infty}\frac{1-\multError^{k-1}}{1-\multError} = \frac{1}{1-\multError}
    \end{align}
    
    Now, the claim can be concluded as follows:
    \begin{align*}
        & \frac{\multError}{1-\multError}\valBy{k}{\retSol} = \multError \left(\frac{1}{1-\multError} \valBy{k}{\retSol} \right)\\
        & > \multError \left(\sum_{i=1}^{k-1} \multError^{i-1} \valBy{k}{\retSol} \right) && \text{(by Equation \eqref{eq:geometric-series-beta})}\\
        & \geq  \multError \left(\sum_{i=1}^{k-1} \multError^{i-1} \valBy{(k-1)-i+1}{\retSol} \right) && \text{(by Equation \eqref{eq:increase-by-obj-size})}\\
        &= \sum_{i=1}^{k-1} \multError^{i} \valBy{(k-1)-i+1}{\retSol} \\
        &\geq \sum_{i=1}^{k-1}\valBy{i}{\retSol} - \sum_{i=1}^{k-1}z_i - \additiveError && \text{(by Lemma \ref{lemma:beta-sums-to-diff})}
\end{align*}
\erel{Formally, Lemma \ref{lemma:beta-sums-to-diff} is for $k\geq 1$, and we apply it for $k-1$, which might be $0$.}\eden{I tried to fixed it, is it better?}
\end{proof}



%------
% thm.

We are now ready to prove the Theorem \ref{th:main}.
\begin{proof}[Proof of Theorem \ref{th:main}]
% \eden{I'm not sure if we should write again about the claim with $\multApprox$}
Recall that the claim is that $\retSol$ is a $\left(\frac{\multApprox^2}{1-\multApprox + \multApprox^2}, \frac{\multApprox(2-\multApprox)\additiveApprox}{1-\multApprox +\multApprox^2}\right)$-approximation.

For brevity, we define the following constants:
\begin{align*}
    \Delta^{mult} = \frac{\multApprox}{1-\multApprox + \multApprox^2}, \quad  \Delta^{add} = \frac{\multApprox(2-\multApprox)}{1-\multApprox +\multApprox^2}
\end{align*}
Accordingly, we need to prove that $\retSol$ is a $\left(\Delta^{mult} \cdot \multApprox, \Delta^{add}\cdot\additiveApprox\right)$-approximation.

We prove the following equation, that will be helpful later:
\begin{align}\label{equ:mu}
\frac{1}{\Delta^{mult} \cdot \multApprox} = \frac{1-\multError +\multError^2}{(1-\multError)^2}
\end{align}
This is true because
\begin{align*}
    &\Delta^{mult} \cdot \multApprox =   \frac{\multApprox^2}{1-\multApprox + \multApprox^2} && \text{(Definition of $\Delta^{mult}$)} \\
    &= \frac{(1-\multError)^2}{\multError +(1-\multError)^2} = \frac{(1-\multError)^2}{1-\multError +\multError^2} &&\text{(since $\multApprox = 1-\multError$)}\\
    & \Rightarrow \frac{1}{\Delta^{mult} \cdot \multApprox} = \frac{1-\multError +\multError^2}{(1-\multError)^2}
    \end{align*}
    Another equation that will be useful later is:
    \begin{align}\label{eq:additive-error}
        \frac{\Delta^{add}}{\Delta^{mult}\cdot \multApprox}  = \frac{1+\multError}{1-\multError}.
    \end{align}
    The reason for this is that
    \begin{align*}
        &\frac{\Delta^{add}}{\Delta^{mult}\cdot \multApprox} =\frac{1-\multApprox + \multApprox^2}{\multApprox^2} \cdot \frac{\multApprox(2-\multApprox)}{1-\multApprox +\multApprox^2}&& \text{(Definitions of $\Delta^{mult}$ and $\Delta^{add}$)}\\
        &=\frac{\multApprox(2-\multApprox)}{\multApprox^2} = \frac{(1-\multError)(1 + \multError)}{(1-\multError)^2} =\frac{1+\multError}{1-\multError}  &&\text{(since $\multApprox = 1-\multError$)}
    \end{align*}

    Now, suppose by contradiction that $\retSol$ is \emph{not} $\left(\Delta^{mult} \cdot \multApprox, \Delta^{add}\cdot\additiveApprox\right)$-approximately-optimal.
    By definition, this means there exists a solution $y \in S$  that is $\left(\Delta^{mult} \cdot \multApprox, \Delta^{add}\cdot\additiveApprox\right)$-preferred over it.
    That is, there exists an integer $1 \leq k \leq n$ such that:
    \begin{align*}
        \forall j < k \colon &\valBy{j}{y} \geq \valBy{j}{\retSol};\\
        & \valBy{k}{y} > \frac{1}{\Delta^{mult} \cdot\multApprox} \left(\valBy{k}{\retSol} + \Delta^{add} \cdot\additiveError \right).
    \end{align*}

    Since $\retSol$ was obtained in \eqref{eq:compact-OP} that was solved in the last iteration $n$, it is clear that $\sum_{i=1}^k \valBy{i}{\retSol} \geq \sum_{i=1}^{k} z_i$ (by constraint $(\Tilde{2})$ if $k<n$ and $(\Tilde{3})$ otherwise).
    Which implies:
    \begin{align}\label{eq:fk-to-zk}
        \sum_{i=1}^k \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i \geq z_k
    \end{align}

    Now, consider \eqref{eq:compact-OP} that was solved in iteration $k$.
    By Observation \ref{obs:retSol-solves-any-t}, $\retSol$ is feasible to this problem.
    As the $(k-1)$ smallest objective values of $y$ are at least as high as those of $\retSol$, it is easy to conclude that $y$ also satisfies constraints $(\Tilde{2})$ of this problem; since, for any $\ell < k$:
    \begin{align*}
        \sum_{i=1}^{\ell} \valBy{i}{y} \geq\sum_{i=1}^{\ell} \valBy{i}{\retSol} \geq \sum_{i=1}^{\ell} z_i
    \end{align*}
    Therefore, by Observation \ref{obs:feasi-and-constraint2}, $y$ is also feasible to this problem. 

    If $k=1$, the objective value $y$ in this problem is $\valBy{1}{y}$ (Observation \ref{obs:obj-value}).
    In addition, $\valBy{1}{\retSol} \geq z_1$ by equation \ref{eq:fk-to-zk}. As $\Delta^{mult}\geq 0$ and $\Delta^{add}\geq 0$, it follows that:
    \begin{align*}
        \valBy{1}{y}> \frac{1}{\Delta^{mult} \cdot\multApprox} \left(\valBy{1}{\retSol} + \Delta^{add} \cdot\additiveError \right)\geq \frac{1}{\multApprox} \left(z_1 + \additiveError \right)
    \end{align*}
    But, $z_1$ was obtained as an approximation for this problem, therefore the optimal value is at most $\frac{1}{\multApprox}\left(z_1 + \additiveError \right)$ --- a contradiction.

    
    Otherwise, $k>1$, we shall now see that in this case $y$ also satisfies the following:
    \begin{align}\label{eq:yk-to-sum}
        \valBy{k}{y} > \frac{1}{1-\multError} \valBy{k}{\retSol} + \frac{\multError}{1-\multError}\sum_{i=1}^{k-1}\valBy{i}{\retSol} - \frac{\multError}{1-\multError} \sum_{i=1}^{k-1}z_i  +\frac{1}{1-\multError}\cdot\additiveError
    \end{align}
    this is true because
    \begin{align*}
        &\valBy{k}{y} > \frac{1}{ \Delta^{mult} \cdot\multApprox} \left(\valBy{k}{\retSol} + \Delta^{add}\cdot \additiveError \right) && \text{(Definition of $y$ for $k$)}\\
        &= \frac{1-\multError +\multError^2}{(1-\multError)^2} \valBy{k}{\retSol}+ \frac{\Delta^{add}}{\Delta^{mult} \multApprox}\cdot\additiveError && \text{(by Equation \ref{equ:mu})}\\
        &= \frac{1-\multError +\multError^2}{(1-\multError)^2} \valBy{k}{\retSol}+ \frac{1+\multError}{1-\multError}\cdot\additiveError && \text{(by Equation \ref{eq:additive-error})} \erel{???}\\
        &\geq\frac{1}{1-\multError} \valBy{k}{\retSol} + \frac{\multError}{1-\multError}\left(\sum_{i=1}^{k-1}\valBy{i}{\retSol} - \sum_{i=1}^{k-1}z_i-\additiveError\right) +\frac{1+\multError}{1-\multError}\cdot\additiveError && \text{(by Lemma \ref{lemma:fk-to-all} for $k>1$)}\\
        & = \frac{1}{1-\multError} \valBy{k}{\retSol} +\frac{\multError}{1-\multError}\sum_{i=1}^{k-1}\valBy{i}{\retSol} - \frac{\multError}{1-\multError} \sum_{i=1}^{k-1}z_i +\frac{1}{1-\multError}\cdot\additiveError &&\erel{???}\text{\eden{is it more clear?}}
    \end{align*}    
    
    We compute the objective value of $y$, which is $\sum_{i=1}^k \valBy{i}{y} - \sum_{i=1}^{k-1} z_i$ (by Observation \ref{obs:obj-value}):  
    \begin{align*}
        &\sum_{i=1}^k \valBy{i}{y} - \sum_{i=1}^{k-1} z_i=\sum_{i=1}^{k-1} \valBy{i}{y} - \sum_{i=1}^{k-1} z_i + \valBy{k}{y}\\
        &\geq \sum_{i=1}^{k-1} \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i + \valBy{k}{y} && \text{(Definition of $y$ for $j<k$)}\\
        &> \sum_{i=1}^{k-1} \valBy{i}{\retSol} - \sum_{i=1}^{k-1} z_i + \frac{1}{1-\multError} \valBy{k}{\retSol} \\
        & \quad + \frac{\multError}{1-\multError}\sum_{i=1}^{k-1}\valBy{i}{\retSol} - \frac{\multError}{1-\multError}\sum_{i=1}^{k-1}z_i +\frac{1}{1-\multError}\cdot\additiveError && \text{(by Equation \ref{eq:yk-to-sum})}\\
        & = \frac{1}{1-\multError} \left(\sum_{i=1}^k \valBy{k}{\retSol} - \sum_{i=1}^{k-1}z_i + \additiveError\right) &&\text{(since  $1+\frac{\multError}{1-\multError} = \frac{1}{1-\multError}$)}\erel{???}\text{\eden{is it more clear?}}
        \\
        &\geq \frac{1}{1-\multError} \left(z_k +\additiveError\right) && \text{(by Equation \ref{eq:fk-to-zk}) }
    \end{align*}
    \eden{I'm not sure why to comment the lines, shouldn't we explain why it is a contradiction?how is the following?}
    % \emark{However, the approximately-optimal solution obtained for this problem during the algorithm run is $z_k$, so the optimal value is at most $\frac{1}{(1-\multError)}\left(z_k+\additiveError\right)$.
    % But, as we shall see, the objective $y$ yields in this problem, $\sum_{i=1}^k \valBy{i}{y} - \sum_{i=1}^{k-1} z_i$ (by Observation \ref{obs:obj-value}), is higher than this value, which is of course a contradiction:}
    However, the approximately-optimal value obtained for this problem during the algorithm run is $z_k$, so the optimal value is at most $\frac{1}{(1-\multError)}\left(z_k+\additiveError\right)$, which is, again, a contradiction.
    
\end{proof}
