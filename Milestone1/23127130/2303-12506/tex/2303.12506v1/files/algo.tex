\section{Approximation Algorithm}\label{sec:algo-short}
We now present an algorithm for computing an approximately-optimal leximin solution. 
The algorithm is an adaptation of one of the algorithms of \textcite{Ogryczak_2006} for finding exact leximin solutions. 
We prove that given a $(\multApprox,\additiveApprox)$-approximation algorithm for the single-objective problem, the output is a $\left(\frac{\multApprox^2}{1-\multApprox + \multApprox^2}, \frac{\multApprox(2-\multApprox)\additiveApprox}{1-\multApprox +\multApprox^2}\right)$-approximation for the multi-objective problem.


% \eden{maybe to write the main result here}

% \eden{yonatan}    
% Following the \emph{definition} of leximin, the core \emph{algorithm} for finding a leximin optimal solution is interative, wherein one first maximizes the least objective function, then the second, and so forth. 
% Specifically, donating by $z_i$ the $i$-th smallest objective value, in each iteration, $t$, one seeks to maximize $z_t$, given the already calculated optimal values for $z_1,\ldots,z_{t-1}$.  The core, single-valued, optimization problem is thus:
%
% \eden{erel} 
% Following the definition of leximin, the core algorithm for finding a leximin optimal solution is iterative, wherein one first maximizes the least objective function, then the second, and so forth. 
% Denote by $z_t$ the value of the $i$-th smallest objective in the leximin-optimal solution, where $t\in\{1,\ldots,n\}$. Suppose we have already computed the optimal values $z_1,\ldots, z_{t-1}$. \eden{I'm not sure about it.. since we use the $z_t$ value later as the \textbf{approximation} value I think it could be a bit confusing}
% Then,  $z_t$ is the solution to the following single-objective optimization problem (where the variables are the scalar $z$ and the vector $x$):

% \eden{how is this? I tried to combine the two versions}
\subsection{Preliminary: exact leximin-optimal solution}
Following the definition of leximin, the core algorithm for finding a leximin optimal solution is iterative, wherein one first maximizes the least objective function, then the second, and so forth. 
In each iteration, $t=1,\ldots,n$, it looks for the value that maximizes the $t$-th smallest objective, $z_t$, given that for any $i < t$ the $i$-th smallest objective is at least $z_i$ (the value that was computed in the $i$-th iteration).
The core, single-objective optimization problem is thus:
\begin{align}
 \max \quad &\ztVar{x}  \;\;
        s.t. &\quad  & (1) \quad x \in S  \tag{P1}\label{eq:basic-OP}\\
              &     & & (2) \quad \valBy{\ell}{x}\geq z_{\ell} & \ell = 1,\ldots,t-1\nonumber \\
               &    & & (3) \quad \valBy{t}{x} \geq \ztVar{x} \nonumber   
\end{align} 
where the variables are the scalar $\ztVar{x}$ and the vector $x$, whereas $z_1, \ldots z_{t-1}$ are constants (computed in previous iterations).

Suppose we are given a procedure $\textsf{OP}(z_1,\ldots,z_{t-1})$, which, given  $z_1,\ldots,z_{t-1},$ outputs $(x,z_t)$ that is the exact optimal solution to \eqref{eq:basic-OP}.  
Then, the \emph{leximin} optimal solution is obtained by iterating this process for $t=1,\ldots,n$, as described in Algorithm \ref{alg:basic-ordered-Outcomes}.
\begin{algorithm}[!tbp]
\caption{The Ordered Outcomes Algorithm}
\label{alg:basic-ordered-Outcomes}
\begin{algorithmic}[1] %[1] enables line numbers
\FOR{$t=1$ to n}
\STATE \( %\begin{align*}
(x_t,z_t)\leftarrow \textsf{OP}(z_1,\ldots,z_{t-1})
\) %\end{align*} 
\ENDFOR
\STATE \textbf{return} {$x_n$ (with objective values $z_1,\ldots,z_n$)}.
\end{algorithmic}
\end{algorithm}

However, it might be difficult to solve the problem \eqref{eq:basic-OP} as is, since constraints (2) and (3) are not linear even with respect to the objective-functions. 
 Thus, \cite{Ogryczak_2006} suggest using a variant of the problem that considers sums instead of individual values (where again the variables are $\ztVar{x}$ and $x$, whereas $z_1, \ldots z_{t-1}$ are constants):  
\begin{align*}
\max \quad &\ztVar{x}  \;\;
s.t. &\quad  & (1) \quad x \in S  \tag{P2}\label{eq:sums-OP}\\
&&& (\hat{2}) \quad \sum_{i \in F'} f_i(x) \geq \sum_{i=1}^{|F'|}  z_i && \forall F' \subseteq [n], |F'| < t \\
&&& (\hat{3}) \quad \sum_{i \in F'} f_i(x) \geq \sum_{i=1}^{t}  z_i  && \forall F' \subseteq [n], |F'| = t
\end{align*}
Here, the constraints $(2)$ and $(3)$ are replaced with constraints $(\hat{2})$ and $(\hat{3})$, respectively. 
Constraint $(\hat{2})$ says that for any $\ell<t$, the sum of any $\ell$ objective functions is at least the sum of the first $\ell$ constants $z_i$'s. 
Similarly, $(\hat{3})$ says that the sum of any $t$  objective functions is at least the sum of the first $t-1$ constants  $z_i$'s, plus the variable $z_t$.
The importance of problem \eqref{eq:sums-OP} in for leximin is shown by the following lemma (Theorem 4 in \cite{Ogryczak2004TelecommunicationsND}):
\begin{lemma}\label{lemma:alg-1-can-use-sums-exact}
If Algorithm \ref{alg:basic-ordered-Outcomes} is applied with a solver for \eqref{eq:sums-OP}  (instead of for \eqref{eq:basic-OP}), the algorithm  still outputs a leximin-optimal solution. 
\end{lemma}
\noindent For completeness, Appendix \ref{sec:equivalent-proofs}  provides an alternative proof of this lemma.

While \eqref{eq:sums-OP} is linear with respect to the objective-functions, it has an exponential number of constraints. 
To overcome this challenge, \cite{Ogryczak_2006} employ auxiliary variables ($y_{\ell}$ and $m_{\ell,j}$ for all $\ell \in [t]$ and $ j\in [n]$) to obtain a polynomial-sized problem that can be used in the same way as \eqref{eq:sums-OP} (Theorem 1 in \cite{Ogryczak_2006}):
% (where the variables are $\ztVar{x}$, $x$, and $y_{\ell}$ and $m_{\ell,j}$ for all $\ell \in [t]$ and $ j\in [n]$; and $z_1, \ldots z_{t-1}$ are constants)
\begin{align}
 \max \quad &\ztVar{x}  \;\;
        s.t. &\quad  & (1) \quad x \in S  \tag{P3}\label{eq:vsums-OP}\\
                    &&& (2) \quad \ell y_{\ell} - \sum_{j=1}^n m_{\ell,j}\geq \sum_{i=1}^{\ell}  z_i && \ell = 1, \ldots,t-1 \nonumber \\
                    &&& (3) \quad t y_t - \sum_{j=1}^{n} m_{t,j} \geq \sum_{i=1}^{t}  z_i  \nonumber \\
                    &&& (4) \quad m_{\ell,j} \geq y_{\ell} - f_j(x)  && \ell = 1, \ldots,t,\Hquad j = 1, \ldots,n \nonumber \\
                    &&& (5) \quad m_{\ell,j} \geq 0  && \ell = 1, \ldots,t,\Hquad j = 1, \ldots,n \nonumber
\end{align}
In Appendix \ref{sec:equivalent-proofs}, we prove a stronger claim:
\begin{lemma}\label{lemma:sums-eq-to-aux}
For every $t\geq 1$ and every fixed constants 
$z_1,\ldots,z_{t-1}$, the problems \eqref{eq:sums-OP} and \eqref{eq:vsums-OP} are \emph{equivalent}
% : they have the same set of feasible solutions and the same objective function.
\end{lemma}
\noindent When problems are equivalent, a solver for one can be used as a solver for the  (Lemma \ref{lemma:solver-equivalent-prob}). 
Together with Lemma \ref{lemma:alg-1-can-use-sums-exact} this means that a solver for \eqref{eq:vsums-OP} can also be used in Algorithm \ref{alg:basic-ordered-Outcomes} to obtain a leximin optimal solution. 

Notice that \eqref{eq:basic-OP} is \emph{not} equivalent to \eqref{eq:sums-OP} or \eqref{eq:vsums-OP}, as will be shown later.
The reason that solvers for these problems can be used interchangeably in Algorithm \ref{alg:basic-ordered-Outcomes} is that they all have the same optimal value when the constants $z_1,\ldots,z_{t-1}$ are the \emph{optimal} values of the previous iterations $1, \ldots, (t-1)$ respectively.

% All of the above is for exact solvers.  
% The question is what happens when we consider approximation procedures? 
\subsection{Using an approximate solver}
Now we assume that, instead of an exact solver for 
OP in 
Algorithm \ref{alg:basic-ordered-Outcomes}, we only have an approximate solver. In this case, the constants $z_1,\ldots,z_{t-1}$ are only approximately-optimal solutions for the previous iterations.
It is easy to see that if \textsf{OP} is a $(\multApprox,\additiveApprox)$-approximation algorithm\footnote{See Section \ref{sec:preliminaries} for $(\multApprox,\additiveApprox)$-approximation algorithm formal definition.} to \eqref{eq:basic-OP}, then Algorithm \ref{alg:basic-ordered-Outcomes} outputs a $(\multApprox,\additiveApprox)$-approximately-optimal leximin solution\footnote{See Section \ref{sec:approx-leximin-def} for $(\multApprox,\additiveApprox)$-approximately-optimal leximin solution formal definition.} (a formal proof is given in Appendix \ref{sec:equivalent-proofs}). 
However, if \textsf{OP} 
is a $(\multApprox,\additiveApprox)$-approximation for
\eqref{eq:sums-OP}, or the equivalent \eqref{eq:vsums-OP}, the algorithm may 
% behave differently.
output a solution that is \emph{not} $(\multApprox,\additiveApprox)$-approximately-optimal.
As an example,
Consider the following multi-objective optimization problem with $n=2$:
\begin{align*}
    \max \quad &\{f_1(x) := x_1, f_2(x) := x_2\} \\
    s.t. \quad  & (1) \Hquad x_1 \leq 100, \quad (2) \Hquad x_1 + x_2 \leq 200, \quad (3) \Hquad x \in \mathbb{R}^2_{+}
\end{align*}
The corresponding problem \eqref{eq:sums-OP} is
\begin{align*}
\max \quad &\ztVar{x}  \;\;
s.t. &\quad  & (1.1) \quad x_1 \leq 100\\
&&& (1.2) \quad x_1 + x_2 \leq 200\\
&&&(1.3) \quad x \in \mathbb{R}^2_{+}\\
&&& (\hat{2}) \quad 
\sum_{i \in F'} x_i \geq \sum_{i=1}^{|F'|}  z_i && \forall F' \subseteq [2], |F'| < t \\
&&& (\hat{3}) \quad 
\sum_{i \in F'} x_i \geq \sum_{i=1}^{t}  z_i  && \forall F' \subseteq [2], |F'| = t
\end{align*}
The following is a possible run of the algorithm with \textsf{OP} that is a $(\multApprox,\additiveApprox)$-approximate solver for $\multApprox=0.9$ and $\additiveApprox = 0$. 
% Notice that for simplicity, we use an example in which $\additiveError=0$; however, this also allows us to demonstrate that even when there is only a multiplicative error, the algorithm behaves differently.
In iteration $t=1$, condition $\hat{2}$ is empty, and the optimal value of $z_1$ is $100$, so \textsf{OP} may output $z_1=0.9\cdot 100 = 90$.  
In iteration $t=2$, given  $z_1=90$, 
condition $\hat{2}$ says that both $x_1$ and $x_2$ must be at least $90$;
the optimal value of $z_2$ under these constraints is $110$, so \textsf{OP} may output $z_2=99$, for example with  $x_1=x_2=94.5$.  
% \erel{Since the objectives are the $x_i$, how can the objective be $99$ and the $x_i$ be $94.5$?}\eden{is it more clear now?}
Since $n=2$, the algorithm ends 
% after this iteration 
and returns the solution $(94.5,94.5)$.
But, $(x_1,x_2) = (94.5,105.5)$ is also a feasible solution, and $(94.5,105.5) \alphaBetaPreferredParams{0.9}{0} (94.5,94.5)$.
Therefore, by definition, the returned solution is \emph{not} $(0.9,0)$-approximately-optimal.
% \eden{this is not exactly accurate, we are not talking about solution that are approximation of one another, but about the relation. maybe something like "But, $(94.5,105.5)$ is also a feasible solution, and since $(94.5,105.5)  \leximinPreferred_{\frac{1}{0.9}} (94.5,94.5)$, by definition  $(94.5,94.5)$ is not a $0.9$-approximation"}
% \erel{This example does not seem very tight. Can you play with the numbers to get a tighter upper bound on the approximation ratio?}

 %On the other hand, \eqref{eq:vsums-OP} is the most solvable variant of the problem.  
Nonetheless, using an approximate solver to \eqref{eq:sums-OP} or \eqref{eq:vsums-OP}, we can guarantee a non-trivial approximation to the leximin-optimal solution.

\begin{theorem}\label{th:main}
Let $\multApprox\in (0,1]$, $\additiveApprox \geq 0$, and \textsf{OP} be a $(\multApprox,\additiveApprox)$-approximation procedure to \eqref{eq:vsums-OP}. Then Algorithm \ref{alg:basic-ordered-Outcomes} outputs a $\left(\frac{\multApprox^2}{1-\multApprox + \multApprox^2}, \frac{\multApprox(2-\multApprox)\additiveApprox}{1-\multApprox +\multApprox^2}\right)$-approximately-optimal leximin solution.  
\end{theorem}

The proof is given in Appendix \ref{sec:algo-sec-proofs} due to space restrictions.


\noindent Notice that this result also implies that if \textsf{OP} only has a multiplicative error ($\additiveApprox = 0$), the solution returned by Algorithm \ref{alg:basic-ordered-Outcomes} will only have a multiplicative error as well, and if \textsf{OP} only has an additive error ($\multApprox = 1$), the solution returned by Algorithm \ref{alg:basic-ordered-Outcomes} will have only the same additive error $\additiveError$.


\subsection{Using a randomized solver}
Next, we assume that the solver for OP is not only approximate but  also \emph{randomized}.
Specifically, we assume that we have a solver that always returns a feasible solution to the single-objective problem, and 
with probability $p \in [0,1]$ it is also approximately-optimal.
As Algorithm \ref{alg:basic-ordered-Outcomes} activates the \textsf{OP} solver $n$ times overall, assuming the success events of different activations are independent, there is a probability of $p^n$ that
\textsf{OP} returns an approximately-optimal solution in every iteration, so Algorithm \ref{alg:basic-ordered-Outcomes} performs as in the previous subsection.
 This leads to the following conclusion:
\begin{corollary}\label{corollary:main-with-probability}
Let $\multApprox\in (0,1]$, $\additiveError \geq 0$, and \textsf{OP} be a \emph{$p$-randomized} $(\multApprox, \additiveError)$-approximation procedure to \eqref{eq:vsums-OP}. Then Algorithm \ref{alg:basic-ordered-Outcomes} outputs a $\left(\frac{\multApprox^2}{1-\multApprox + \multApprox^2}, \frac{\multApprox(2-\multApprox)\additiveApprox}{1-\multApprox +\multApprox^2}\right)$-approximately-optimal leximin solution with probability $p^n$.
\end{corollary}

Notice that, since the procedure \textsf{OP} always returns a feasible solution to the single-objective problem, Algorithm \ref{alg:basic-ordered-Outcomes} always returns a feasible solution as well.

% Finally, to complete the picture, 