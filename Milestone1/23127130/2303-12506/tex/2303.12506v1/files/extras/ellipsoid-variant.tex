
\section{Ellipsoid Method Variant for Approximation}\label{sec:mult-variant-ellipsoid}
This Appendix describes a variant of the ellipsoid method that can be used to approximate  LPs that cannot be solved directly due to a large number of variables.
% It requires an approximate separation oracle for the dual program.
The method combines techniques presented in \cite{grotschel_geometric_1993,grotschel_ellipsoid_1981,karmarkar_efficient_1982}.

\subsection{Using Approximate Separation Oracles (multiple error)}
Our goal is to solve the following linear program (the primal):
\begin{align}
\tag{P}
\begin{split}
\min \quad &c^T \cdot x \\
s.t. \quad &A \cdot x \geq b, \quad x\geq 0;
\end{split}
\end{align}
We assume that (P) has a small number of constraints, but may have a huge number of variables, so we cannot solve (P) directly. We consider its \emph{dual}:
\begin{align}
\tag{D}
\begin{split}
\max \quad & b^T \cdot y \\
s.t. \quad &A^T \cdot y \leq c,\quad y\geq 0.
\end{split}
\end{align}
Assume that both problems have optimal solutions and denote the optimal solutions of (P) and (D) by $x^{*}$ and $y^{*}$ respectively. By the strong duality theorem:
\begin{align}
    c^T \cdot x^{*} = b^T \cdot y^{*}
\end{align}

While (D) has a small number of variables, it has a huge number of constraints, so
we cannot solve it directly either. 
In this Appendix, we show that it can be approximately using the following tool:

\begin{definition}
An \emph{approximate separation oracle} with multiplicative error (MASO) for the dual LP is an efficient function parameterized by a constant $\multError \geq 0$.
Given a vector $y$  it returns one of the following two answers:
\begin{enumerate}
\item "$y$ is infeasible". In this case, is returns a violated constraint, that is, a row $a_i^T \in A^T$ such that $a_i^T  y > c_i$.
\item "$y$ is \emph{approximately feasible}". 
That means that $A^T y \leq (1+\multError) \cdot c$
\end{enumerate}

\end{definition}
Given the MASO, we apply the ellipsoid method as follows (this is just a sketch
to illustrate the way we use the MASO; it omits some technical details):
\begin{itemize}
    \item Let $E_0$ be a large ellipsoid, that contains the entire feasible region, that is, all $y \geq 0$ for which $A^T y \leq c$.

    \item For $k = 0,1,\dots, K$ (where $K$ is a fixed constant, as will be explained later):
    \begin{itemize}
        \item Let $y_k$ be the centroid of ellipsoid $E_k$.
        
        \item Run the MASO on $y_k$.
        
        \item If the MASO returns "$y_k$ is infeasible" and a violated constraint $a_i^T$, then make a \emph{feasibility cut} --- keep in $E_{k+1}$ only those $y \in E_k$ for which $a_i^T y \leq c_i$.
        
        \item If the MASO returns "$y$ is approximately feasible", then make an \emph{optimality cut} --- keep in $E_{k+1}$ only those $y \in E_k$ for which $b^T y \geq b^T y_k$.
    \end{itemize}
    
    \item From the set $y_0, y_1, \dots, y_K$, choose the point with the highest $b^T \cdot y_k$ among all the approximately-feasible points.
\end{itemize}
Since both cuts are through the center of the ellipsoid, the ellipsoid dilates by a factor of at least $\frac{1}{r}$ at each iteration, where $r > 1$ is some constant (see \cite{grotschel_ellipsoid_1981} for computation of $r$). Therefore, by choosing $K := \log_2 r \cdot L$, where $L$ is the
number of bits in the binary representation of the input, the last ellipsoid $E_K$ is so small that all points in it can be considered equal (up to the accuracy of the binary representation).


The solution $y'$ returned by the above algorithm satisfies the following two conditions:
\begin{equation} \label{mult:y-star-is-approximetly-feasible}
     A^T y' \leq (1+\multError)\cdot c
\end{equation}
\begin{equation} \label{mult:y-star-obj-geq-opt}
     b^T y' \geq b^T y^{*}
\end{equation}
Inequality \ref{mult:y-star-is-approximetly-feasible} holds since, by definition, $y'$ is approximately-feasible.

To prove \ref{mult:y-star-obj-geq-opt}, suppose by contradiction that $b^T y^{*} > b^T y'$. 
Since $y^{*}$ is feasible for (D), it is in the initial ellipsoid. 
It remains in the ellipsoid throughout the algorithm: it is removed neither by a feasibility cut (since it is
feasible), nor by an optimality cut (since its value is at least as large as all values used for optimality cuts).
Therefore, it remains in the final ellipsoid, and it is chosen as the highest-valued feasible point rather than $y'$ --- a contradiction.

Now, we construct a reduced version of (D), where there are only at most $K$ constraints --- only the constraints used to make feasibility cuts.
Denote the reduced constraints by $A_{red}^T \cdot y \leq c_{red}$, where $A_{red}^T$ is a matrix containing a subset of at most $K$ rows of of $A^T$, and $c_{red}$ is a vector containing the corresponding subset of the elements of $c$. The reduced-dual LP is:
\begin{equation}
\tag{RD}
\begin{split}
\max  \quad & b^T y \\
s.t. \quad & A_{red}^T \cdot y \leq c_{red}, \quad y\geq 0
\end{split}
\end{equation}
Notice that it has the same number of variables as the program (D). Further, if we had run this ellipsoid method variant on (RD) (instead of (D)), then the result would have been exactly the same --- $y'$.
Therefore, (\ref{mult:y-star-obj-geq-opt}) holds for the (RD) too:
\begin{equation} \label{mult:y-star-to-y-redopt}
    b^T y' \geq b^T y^{*}_{red}
\end{equation}
where $y^{*}_{red}$ is the optimal value of (RD).


As $A_{red}^T$ contains a subset of at most $K$ rows of $A^T$, the matrix $A_{red}$ contains a subset of \emph{columns} of $A$.
Therefore, the dual of (RD) has only at most $K$ variables, which are those who correspond to the remaining columns of $A$:
\begin{equation}
	\tag{RP}
    \begin{split}
     \min \quad &c_{red}^T \cdot x_{red} \\
            s.t. \quad &A_{red} \cdot x_{red} \geq b, \quad x_{red}\geq 0
    \end{split}
\end{equation}
%  reduced-primal
%\er{Note that $A_{red}$ is a matrix with the same number of rows as $A$, but only at most $K$ columns.}
Since (RP) has a polynomial number of variables  and constraints, it can be solved exactly by any LP solver (not necessarily the ellipsoid method).
Denote the optimal solution by $x^{*}_{red}$. 

Let $x'$ be a vector which describes an assignment to the variables of (P), in which all variables that exist in (RP) have the same value as in $x^{*}_{red}$, and all other variables are set to $0$.
It follows that $A \cdot x' = A_{red} \cdot x^{*}_{red}$, therefore, since $x^{*}_{red}$ is feasible to RD, also $x'$ is a feasible solution to (P).
\erel{
In second reading, I think this should be made more formal.
Let $x'$ be a solution to (P), in which all variables that exist in (RP) have the same value as in $x^{*}_{red}$, and all other variables are set to 0.
We have to prove that 
(1) $x'$ is feasible for (P);
(2) $c^T x' \leq (1+\epsilon)\cdot c^T\cdot x^{*}$.
}
\eden{better?}
Similarly, $c^T \cdot x' = c^T_{red} \cdot x^{*}_{red}$.
We shall now see that this implies that the objective obtained by $x'$ approximates the objective obtained by $x^{*}$:
\begin{align*} 
&c^T \cdot x' = c^T_{red} \cdot x^{*}_{red} \\
&=  b^T \cdot y^{*}_{red} & \text{(by strong duality for the reduced LPs)} \\
                     &\leq  b^T\cdot y' & \text{(By (\ref{mult:y-star-to-y-redopt}))}\\
                     &\leq  (A \cdot x^{*})^T y' & \text{(definition of (P))} \\
                     &=  (x^{*})^T (A^T\cdot y') & \text{(properties of transpose and associativity of multiplication)} \\
                     &\leq  (x^{*})^T ((1+\multError)\cdot c) & \text{(by \ref{mult:y-star-is-approximetly-feasible})} \\
                     & = (1+\multError) \cdot (c^T x^{*}) & \text{(properties of transpose)}
\end{align*}
So, $x'$ ($x^{*}_{red}$ with all missing variables set to $0$) is an approximate solution to the primal LP (P) --- as required.

\subsection{Using Half-Randomized Approximate Separation Oracles}
Here, we allow the oracle to also be \emph{half-randomized}, that is, when it says that a solution is infeasible, it is always correct; however, when it says that a solution is approximately feasible, it is only correct with some probability $p \in [0,1]$.

Since the ellipsoid method variant is iterative, and since the oracle calls are independent, there is a probability $p^T$ that the oracle answers correctly in each iteration, and so, the overall process performs as before. 
We shall now explain why, using a half-randomized oracle, this ellipsoid method variant \emph{always} returns a feasible solution to the primal (even if the oracle was incorrect).

First, notice that the oracle is always correct when it determines that a solution is infeasible.
In addition, the construction of RD is only depended by these set of constraints.
Therefore, by the same arguments, $x'$ would still be a feasible solution to P (but not necessarily with an approximately-optimal objective value).

This means that given a half-randomized approximate separation oracle for the dual with error $\multError$ and success probability $p$, this ellipsoid method variant can be used as a randomized approximation algorithm for the primal with the same error and success probability $p^I$ (where $I$ is an upper bound on the number of iteration of the method on the given input).