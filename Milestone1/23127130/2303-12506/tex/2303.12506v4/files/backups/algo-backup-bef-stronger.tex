
\section{Approximation Algorithm}\label{sec:algo-short}
We now present an algorithm for computing a multiplicative approximate leximin \eden{optimal?}. The algorithm is an adaptation of one of the algorithms of \textcite{Ogryczak_2006} for finding exact Leximin solutions. 
% Accordingly, the bulk of the effort is in proving that the approximate version of the algorithm indeed produces the desired approximate Leximin.

Following the definition of leximin, the core algorithm for finding a leximin optimal solution is iterative, wherein one first maximizes the least objective function, then the second, and so forth. 
Denote by $z_t$ the value of the $i$-th smallest objective in the leximin-optimal solution, where $t\in\{1,\ldots,n\}$. Suppose we have already computed the optimal values $z_1,\ldots, z_{t-1}$. \eden{I'm not sure about it.. since we use the $z_t$ value later as the \textbf{approximation} value I think it could be a bit confusing}
Then,  $z_t$ is the solution to the following single-objective optimization problem (where the variables are the scalar $z$ and the vector $x$):
\eden{need to check if changing $z_t$ to $z$ effect the proofs}
%, in each iteration, $t$, one seeks to maximize $z_t$, given the already calculated optimal values for $z_1,\ldots,z_{t-1}$.  The core, single-valued, optimization problem is thus:
\begin{align}
 \max \quad &z  \;\;
        s.t. &\quad  & (1) \quad x \in S  \tag{P1}\label{eq:basic-OP}\\
              &     & & (2) \quad \valBy{\ell}{x}\geq z_{\ell} & \ell = 1,\ldots,t-1\nonumber \\
               &    & & (3) \quad \valBy{t}{x} \geq z \nonumber   
\end{align} 
Suppose we are given a procedure $\textsf{OP}(z_1,\ldots,z_{t-1})$, which, given  $z_1,\ldots,z_{t-1},$ outputs $(x,z)$ that is the exact optimal solution to \eqref{eq:basic-OP}.  Then, the Leximin optimal solution is obtained by iterating this process for $t=1,\ldots,n$, as described in Algorithm \ref{alg:basic-ordered-Outcomes}.
\begin{algorithm}[!tbp]
\caption{The Ordered Outcomes Algorithm}
\label{alg:basic-ordered-Outcomes}
\begin{algorithmic}[1] %[1] enables line numbers
\FOR{$t=1$ to n}
\STATE \( %\begin{align*}
(x_t,z_t)\leftarrow \textsf{OP}(z_1,\ldots,z_{t-1})
\) %\end{align*} 
\ENDFOR
\STATE return $(z_1,\ldots,z_n)$ 
\end{algorithmic}
\end{algorithm}

However, as is, constraints (2) and (3) of problem \eqref{eq:basic-OP} are not linear even with respect to the objective-functions.
Therefore, it might be difficult to solve. 
\erel{I suggest to move P2-Compact here: it is a natural step between P1 and P2.}
\eden{}
 Thus, \cite{Ogryczak_2006} suggest using a variant of the problem that considers sums instead of individual values, as follows (where the variables are the scalar $z$ and the vector $x$):  
% \eden{maybe we should say it is not a linear program \textbf{with respect to the objective functions}?}
% \erel{I agree. Since the objective functions themselves may be non-linear.}
\begin{align*}
    \max \quad &z_t \tag{P2}\label{eq:sums-OP}\\
        s.t. \quad  & (1) \quad x \in S\\
                    & (\hat{2}) \quad \sum_{f_i \in F'} f_i(x) \geq \sum_{i=1}^{|F'|}  z_i && \forall F' \subseteq \allObjFunc, |F'| < t \\
                    & (\hat{3}) \quad \sum_{f_i \in F'} f_i(x) \geq \sum_{i=1}^{t}  z_i  && \forall F' \subseteq \allObjFunc, |F'| = t
\end{align*}
Here, the constraints $(2)$ and $(3)$ are replaced with constraints $(\hat{2})$ and $(\hat{3})$, respectively. Constraint $(\hat{2})$ says that for any $\ell<t$, the sum of any $\ell$ objective functions is at least the sum of the first $\ell$ $z_i$'s. Similarly, $(\hat{3})$ says that the sum of any $t$  objective functions is at least the $t$ first $z_i$'s.  So, since we seek to maximize $z_t$, the optimal solution to \eqref{eq:sums-OP} and \eqref{eq:basic-OP} are provably identical (as in both $z_i$ is the $i$-th least objective value, see \cite{Ogryczak_2006} for a formal proof \eden{I'm actually not sure which of the papers to cite as I didn't find a formal proof (only the claim in given in \cite{Ogryczak_2006} and in \cite{Ogryczak2004TelecommunicationsND} Theorem 4)}). As such, in Algorithm \ref{alg:basic-ordered-Outcomes}, a solver for \eqref{eq:sums-OP} can be used (instead of for \eqref{eq:basic-OP}), and the algorithm will still output a Leximin optimal solution. 

Alas, while \eqref{eq:sums-OP} is linear with respect to the objective-functions, it has an exponential number of constraints. 
To overcome this problem, \cite{Ogryczak_2006} employ auxiliary variables ($y_{\ell}$ and $m_{\ell,j}$ for all $\ell \in [t]$ and $ j\in [n]$) to obtain a polynomial-sized program that is provably equivalent to \eqref{eq:sums-OP}:\eden{To myself: should explain "equivalent" in more detail, maybe to add the alternative proof}
\begin{align}
    \max \quad &z_t\tag{P3}\label{eq:vsums-OP}\\
        s.t. \quad  & (1) \quad x \in S \nonumber\\
                    & (2) \quad \ell y_{\ell} - \sum_{j=1}^n m_{\ell,j}\geq \sum_{i=1}^{\ell}  z_i && \ell = 1, \ldots,t-1 \nonumber \\
                    & (3) \quad t y_t - \sum_{j=1}^{n} m_{t,j} \geq \sum_{i=1}^{t}  z_i  \nonumber \\
                    & (4) \quad m_{\ell,j} \geq y_{\ell} - f_j(x)  && \ell = 1, \ldots,t,\Hquad j = 1, \ldots,n \nonumber \\
                    & (5) \quad m_{\ell,j} \geq 0  && \ell = 1, \ldots,t,\Hquad j = 1, \ldots,n \nonumber
\end{align}
So, in Algorithm \ref{alg:basic-ordered-Outcomes} using a solver for \eqref{eq:vsums-OP}, the algorithm will output a Leximin optimal solution. 

All of the above is for exact solutions.  The question is what happens when we consider approximation procedures? It is easy to see that if \textsf{OP} is a $(1-\beta)$ approximation algorithm to \eqref{eq:basic-OP}, then Algorithm \ref{alg:basic-ordered-Outcomes} outputs a $(1-\beta)$ approximation of the Leximin. 
\erel{
To say this, we first have to define the notion of "approximation algorithm". It is known, but we still have to define it formally. Maybe in the "preliminaries" section.
} 
However, for approximations, \eqref{eq:sums-OP}, and the equivalent \eqref{eq:vsums-OP}, are no longer equivalent to \eqref{eq:basic-OP}. Indeed, the following example shows that if \textsf{OP} is a $(1-\beta)$ approximation procedure for \eqref{eq:vsums-OP}, then Algorithm \ref{alg:basic-ordered-Outcomes} may output a solution that is \emph{not} a $1-\beta$ approximation to the Leximin.
\erel{Maybe it is better to move the example to here, to motivate our proof.}
Consider the following multi-objective optimization problem with $n=2$:
\begin{align*}
    \max \quad &\{f_1(x) := x_1, f_2(x) := x_2\} \\
    s.t. \quad  & x_1 \leq 100\\
                & x_1 + x_2 \leq 200\\
                & x \in \mathbb{R}^2_{+}
\end{align*}
According to these settings, the problem \eqref{eq:vsums-OP} becomes:
\begin{align*}
    \max \quad &z_t \\
    s.t. \quad  & (1.1) \quad  x_1 \leq 100\\
                & (1.2) \quad  x_1 + x_2 \leq 200\\
                & (1.3) \quad  x \in \mathbb{R}^2_{+}\\
                & (\hat{2}) \quad \sum_{f_i \in F'} f_i(x) \geq \sum_{i=1}^{|F'|}  z_i && \forall F' \subseteq \allObjFunc, |F'| < t \\
                & (\hat{3}) \quad \sum_{f_i \in F'} f_i(x) \geq \sum_{i=1}^{t}  z_i  && \forall F' \subseteq \allObjFunc, |F'| = t
\end{align*}
The following is a possible run of the algorithm with \textsf{OP} that is a $(1-\beta)$ approximate solver, for $\beta=0.1$. 
In the first iteration, $t=1$, the optimal value of $z_1$ is $100$, so \textsf{OP} may output $z_1=90$.  
Then, in the second iteration, $t=2$, given that $z_1=90$, the optimal value of $z_2$ is $110$, so \textsf{OP} may output $z_2=99$, which is obtained with  $x_1=x_2=94.5$.  
\erel{Since the objectives are the $x_i$, how can the objective be $99$ and the $x_i$ be $94.5$?}\eden{is it more clear now?}
But, $(x_1,x_2) = (94.5,105.5)$ is also a feasible solution, and $(94.5,105.5)\xPreferred{\frac{1}{0.9}}(94.5,94.5)$.
Therefore, by definition, the returned solution $(94.5,94.5)$ is \emph{not} a $0.9$ approximately-optimal solution.
% \eden{this is not exactly accurate, we are not talking about solution that are approximation of one another, but about the relation. maybe something like "But, $(94.5,105.5)$ is also a feasible solution, and since $(94.5,105.5)  \leximinPreferred_{\frac{1}{0.9}} (94.5,94.5)$, by definition  $(94.5,94.5)$ is not a $0.9$-approximation"}
\erel{This example does not seem very tight. Can you play with the numbers to get a tighter upper bound on the approximation ratio?}

 %On the other hand, \eqref{eq:vsums-OP} is the most solvable variant of the problem.  
So, using an approximate solver to \eqref{eq:vsums-OP}, what guarantee can we provide for the overall resulting solution? 
\begin{theorem}
\label{th:main}
Let $\beta<1/2$, and \textsf{OP} be a procedure that outputs a $(1-\beta)$ approximation to \eqref{eq:vsums-OP}. Then Algorithm \ref{alg:basic-ordered-Outcomes} outputs a $(1-2\beta)$-approximate Leximin solution.  
\end{theorem}

\eden{maybe we should also include the proof of the additive variant somewhere? if so. where?}
\erel{
Since the proof is not explicitly mentioned anywhere, I think it is good to include a proof for comparison. Maybe in an appendix.
}
\eden{if we want to do so we should also describe the additive variant definition, maybe to put them both in the appendix? I added appendix \ref{sec:additive}}
The remainder of this section is dedicated to proving the theorem.
To this end, we introduce yet another equivalent representation of \eqref{eq:sums-OP}. 
\erel{This variant should move upwards, between P1 and P2.}
\begin{align*}
    \max \quad &z_t \tag{P2-compact}\label{eq:compact-OP}\\
        s.t. \quad  & (1) \quad x \in S\\
                    & (\Tilde{2}) \quad \sum_{i=1}^{\ell} \valBy{i}{x} \geq \sum_{i=1}^{\ell}  z_i && \ell = 1,\ldots, t-1 \nonumber\\
                    & (\Tilde{3}) \quad \sum_{i=1}^{t} \valBy{i}{x} \geq \sum_{i=1}^{t}  z_i
\end{align*}
In this program, constraints $(\hat{2})$ and $(\hat{3})$ are replaced by  $(\Tilde{2})$ and $(\Tilde{3})$, respectively.  In  $(\hat{2})$, for each $\ell$, one gives a lower bound on the sum for \emph{any} set of $\ell$ objective functions.  In $(\Tilde{2})$, on the other hand, one only considers the sum of the $\ell$ \emph{smallest} such values.  However, since the constraints set the same lower bound on this sum, the constraints are equivalent.  Similarly for $(\hat{3})$ and $(\Tilde{3})$.  So, \eqref{eq:compact-OP} and \eqref{eq:sums-OP} are equivalent, which, in turn, is equivalent to \eqref{eq:vsums-OP}.  So, in proving the theorem we may assume that \textsf{OP} is an approximation procedure for \eqref{eq:compact-OP}.  This will simplify the proofs.

\erel{*** I do not understand. We say that P1 and P2 are equivalent with an exact solver, but not with an approximate solver. 
Here, we claim that P3 and P2-compact are equivalent, but this is true only with an exact solver. Don't we have to prove that they are equivalent also with an approximate solver? ***}

\er{We denote $\retSol := x_n$ = the solution $x$ attained at the last iteration ($t=n$) of the algorithm.}

% OVERALL EXPLANATION
%We first establish our motivation by demonstrating, in Lemma \ref{lemma:not-beta-approx}, that $\retSol$ is not necessarily a $1-\beta$ Leximin approximation.\eden{wasn't sure were to put lemma \ref{lemma:not-beta-approx}, before or after}
% However, we then prove it is actually not that far off, it is a $1-2\beta$ approximation. 
Lemmas \ref{lemma:mult-fk-lower-bound}-\ref{lemma:mult-special-z_k-upper-bound} establish a relationship between the $k$-th least objective value at 
% the returned solution ($\retSol$)
$\retSol$
% solution  returned by Algorithm \ref{alg:basic-ordered-Outcomes} ($\retSol$) 
and the value $z_k$. 
In particular, the main conclusion is that the first is at least $\frac{1-2\beta}{1-\beta}$ of the second.
Theorem \ref{th:main} then uses this lower bound to prove that the existence of another solution that would be $\frac{1}{1-2\beta}$-preferred over $\retSol$ would lead to a contradiction.

% LEMMAS.
% BLAH BLAH.


\begin{lemma}\label{lemma:mult-fk-lower-bound}
    For any $1 < k \leq n \colon \quad \valBy{k}{\retSol} \geq z_k - \frac{\beta}{1-\beta} z_{k-1}$.
\end{lemma}

\begin{proof}
    Since $\retSol$ is a feasible solution of \eqref{eq:compact-OP} in iteration $n$, it is also a feasible solution of \eqref{eq:compact-OP} in any iteration $t\leq n$, as each iteration only adds new constraints. 
    Consider the problem solved in iteration $t$.
    Since its objective is $(\max z_t)$ and the only constraint that includes the variable $z_t$ is constraint $(\Tilde{3})$ (i.e., $\sum_{i=1}^{t} \valBy{i}{x} \geq \sum_{i=1}^{t}  z_i$), the objective value that $\retSol$ yields in this problem is: 
    \begin{align}\label{eq:obj-retSol-in-prob-t}
        \sum_{i=1}^t \valBy{i}{\retSol} - \sum_{i=1}^{t-1} z_i
    \end{align}
    However, during the algorithm run, the value obtained as a $(1 - \beta)$ approximation for this problem is \eden{the constant} $z_t$.
    This implies that the objective value of any feasible solution is at most $\frac{1}{1-\beta} z_t$.
    As $\retSol$ is feasible, equation \ref{eq:obj-retSol-in-prob-t} allows us to conclude the following:
    \begin{align}\label{equ:mult-other-sol-to-OP-upper-bound}
    	\sum_{i=1}^t \valBy{i}{\retSol} - \sum_{i=1}^{t-1} z_i \leq \frac{1}{1-\beta} z_t.
    \end{align}

    Now, let $1 < k \leq n$. 
    By considering the problem solved in the last iteration $n$, it is easy to see that $\sum_{i=1}^t \valBy{i}{\retSol} \geq \sum_{i=1}^{t} z_i$ (by constraint $(\Tilde{2})$ if $k<n$ and $(\Tilde{3})$ otherwise).
 % \erel{These constraints are not in P2-Compact.   What program do you refer to?}).
    As a result, the claim can concluded by isolating $\valBy{k}{\retSol}$:
    \begin{align*}
        \valBy{k}{\retSol} &\geq \sum_{i=1}^k z_i - \sum_{i=1}^{k-1} \valBy{i}{\retSol} \\
        & = z_k + z_{k-1} - \Bigr[\sum_{i=1}^{k-1} \valBy{i}{\retSol} - \sum_{i=1}^{k-2} z_i \Bigr] \geq \\
        & \geq z_k + z_{k-1} - \frac{1}{1-\beta}z_{k-1}  && \text{(by equation \eqref{equ:mult-other-sol-to-OP-upper-bound})}
        \\
        & = z_k - \frac{\beta}{1-\beta} z_{k-1}
    \end{align*}
\end{proof}


% ----- lemma 2 
\begin{lemma}\label{lemma:mult-zk-upper-bound}
    For any $1 < k \leq n \colon \quad z_k \leq \sum_{i=1}^k \biggr[\frac{\beta}{1-\beta}\biggr]^{i-1} \valBy{k-i+1}{\retSol}$.
\end{lemma}


\begin{proof}
    The proof is by induction on $k$.
    For $k=2$, applying Lemma \ref{lemma:mult-fk-lower-bound} and rearranging it results in $z_2 \leq \valBy{2}{\retSol} + \frac{\beta}{1-\beta} z_{1}$. 
    However, constraint $(\Tilde{2})$ for $\ell = 1$ in the problem solved in iteration $n$ ensures that $\valBy{1}{\retSol} \geq z_1$.
    % \erel{In iteration $t=1$? In what constraint?}
    Accordingly, the claim can easily concluded:
    \begin{align*}
        z_2 &\leq \valBy{2}{\retSol} + \frac{\beta}{1-\beta} z_{1} \leq \valBy{2}{\retSol} + \frac{\beta}{1-\beta} \valBy{1}{\retSol} = \sum_{i=1}^2 \biggr[\frac{\beta}{1-\beta}\biggr]^{i-1} \valBy{2-i+1}{\retSol}
    \end{align*}
    Next, assuming that the claim holds for $2, \dots, k-1$, by rearranging Lemma \ref{lemma:mult-fk-lower-bound}, it follows that it also holds for $k$:
    \begin{align*}
        z_k &\leq \valBy{k}{\retSol} + \frac{\beta}{1-\beta} z_{k-1}\\
        & \leq \valBy{k}{\retSol} + \frac{\beta}{1-\beta} \sum_{i=1}^{k-1} \biggr[\frac{\beta}{1-\beta}\biggr]^{i-1} \valBy{k-1-i+1}{\retSol} && \text{induction assumption}\\
        & \leq \valBy{k}{\retSol} + \sum_{i=1}^{k-1} \biggr[\frac{\beta}{1-\beta}\biggr]^{(i+1)-1} \valBy{k-(i+1)+1}{\retSol}\\
        & = \valBy{k}{\retSol} + \sum_{i=2}^{k} \biggr[\frac{\beta}{1-\beta}\biggr]^{i-1} \valBy{k-i+1}{\retSol} && \text{index change (sum)}\\
        & = \sum_{i=1}^{k} \biggr[\frac{\beta}{1-\beta}\biggr]^{i-1} \valBy{k-i+1}{\retSol}
    \end{align*}
\end{proof}


% ---------- lemma 3 
\begin{lemma}\label{lemma:mult-special-z_k-upper-bound}
    For any $1 \leq k \leq n \colon \quad 
        \frac{1-\beta}{1-2\beta} \cdot \valBy{k}{\retSol} \geq z_k$
\end{lemma}

\eden{we should decide if we include the following: \textit{"We prove a slightly stronger result: the approximation ratio is at most $\phi\cdot \beta$, where $\phi=$..."} I'm not sure where to put it....}

\begin{proof}
    Let $1 \leq k \leq n$. 
    If $k=1$ the claim is trivial since $\valBy{1}{\retSol} \geq z_1$ (direct constraint) and $\frac{1-\beta}{1-2\beta} \geq 1$ (as $\beta \geq 0$).
    For $k>1$, however, as $\beta < 1/2$ implies that $0 \leq \frac{\beta}{1-\beta} < 1$, by considering the sum of the infinite geometric series with this ratio and a first element $1$, we get that:
    \begin{align*}
        \sum_{i=1}^k \Bigr[\frac{\beta}{1-\beta}\Bigr]^{i-1} &< \lim_{k \rightarrow \infty} \sum_{i=1}^k \Bigr[\frac{\beta}{1-\beta}\Bigr]^{i-1} =  \frac{1}{1-\Bigr[\frac{\beta}{1-\beta}\Bigr]} = \frac{1-\beta}{1-2\beta}
    \end{align*}
    Therefore, as $k-i+1 \leq k$ for any $1\leq i \leq k$, the claim can be concluded from Lemma \ref{lemma:mult-zk-upper-bound}:
    \begin{align*}
        z_k \leq \sum_{i=1}^k \biggr[\frac{\beta}{1-\beta}\biggr]^{i-1} \valBy{k-i+1}{\retSol} \leq \sum_{i=1}^k \biggr[\frac{\beta}{1-\beta}\biggr]^{i-1} \valBy{k}{\retSol} < \frac{1-\beta}{1-2\beta} \valBy{k}{\retSol} 
    \end{align*}
\end{proof}

%------
% thm.

We are now ready to prove the Theorem \ref{th:main}.
\begin{proof}[Proof of Theorem \ref{th:main}]
Suppose for contradiction that $\retSol$ is not a $(1 - 2\beta)$-approximate Leximin solution.
    By definition, this means there exists a solution $y$  that is $\frac{1}{(1 - 2\beta)}$-preferred over it.
    That is, there exists an integer $1 \leq k \leq n$ such that:
    \begin{align*}
        \forall j < k \colon &\valBy{j}{y} \geq \valBy{j}{\retSol};\\
        & \valBy{k}{y} > \frac{1}{(1 - 2\beta)} \cdot \valBy{k}{\retSol}.
    \end{align*}
    Since $\retSol$ is a solution to the optimization problem that was solved in iteration $k$, and as $y$'s $k$ least objective values are higher, $y$ is also a solution to this problem. 
    But, as we shall see, the objective value of $y$ in this problem is higher than the optimal value, which is of course a contradiction:
    \begin{align*}
        &\sum_{i=1}^k \valBy{i}{y} - \sum_{i=1}^{k-1} z_i && \text{objective isolating \erel{What is "objective isolating"?} }\\
        &=  \valBy{k}{y} + \Bigr[\sum_{i=1}^{k-1} \valBy{i}{y} - \sum_{i=1}^{k-1} z_i\Bigr] \\
        & \geq  \valBy{k}{y} +0&& \text{constraint (2) for } \ell=k-1\\
        &> \frac{1}{(1 - 2\beta)} \cdot \valBy{k}{\retSol} && \text{$y$'s definition for } k\\
        & = \frac{1}{1-\beta} \biggr[\frac{1 - \beta}{(1 - 2\beta)} \cdot \valBy{k}{\retSol}\biggr] \\
        &\geq \frac{1}{1-\beta} z_k && \text{Lemma \ref{lemma:mult-special-z_k-upper-bound}}
    \end{align*}
\end{proof}
\iffalse
Finally, to complete the picture, the following is an example that shows that the algorithm need not output a $(1-\beta)$ approximate Leximin.
Consider the following example with $n=2$:
    \begin{align*}
        \max \quad &\{f_1(x) := x_1, f_2(x) := x_2\} \\
        s.t. \quad  & x_1 \leq 100\\
                    & x_1 + x_2 \leq 200\\
                    & x \in \mathbb{R}^2_{+}
    \end{align*}
    Consider a run of the algorithm with \textsf{OP} that is a $(1-\beta)$ approximate solver, for $\beta=0.1$. 
    In the first iteration, the optimal value of $z_1$ is $100$, so \textsf{OP} may output $z_1=90$.  
    Then, in the second iteration, given $z_1=90$, the optimal value of $z_2$ is $110$, so \textsf{OP} may output $z_2=99$, which is obtained with  $x_1=x_2=94.5$.  
    \erel{Since the objectives are the $x_i$, how can the objective be $99$ and the $x_i$ be $94.5$?}
    But, $(x_1,x_2) = (94.5,105.5)$ is also a feasible solution, and $(94.5,94.5)$ is \emph{not} a $0.9$ approximation of $(94.5,105.5)$.
    \eden{this is not exactly accurate, we are not talking about solution that are approximation of one another, but about the relation. maybe something like "But, $(94.5,105.5)$ is also a feasible solution, and since $(94.5,105.5) \leximinPreferred_{\frac{1}{0.9}} (94.5,94.5)$, by definition 
    $(94.5,94.5)$ is not a $0.9$-approximation"}
\erel{This example does not seem very tight. Can you play with the numbers to get a tighter upper bound on the approximation ratio?}
\fi

