%%%%%%%% ICML 2023 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
 \usepackage{relsize}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2023}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{dsfont}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\newcommand\alphat{\Tilde{\alpha}}
\newcommand\XS{\boldsymbol{X}_S}
\newcommand\Xtest{\boldsymbol{X}_{n+1}}
\newcommand\X{\boldsymbol{X}}
\newcommand\xs{\boldsymbol{x}_S}
\newcommand\XSb{\boldsymbol{X}_{\bar{S}}}
\newcommand\xsb{\boldsymbol{x}_{\bar{S}}}
\newcommand\x{\boldsymbol{x}}
\newcommand\Xd{\boldsymbol{X}^\diamond}
\newcommand\hC{\widehat{C}}
\newcommand\cov{\text{cov}}
\newcommand\Z{\boldsymbol{Z}}
\newcommand\hV{\widehat{V}}
\newcommand{\labs}{\left\lvert}
\newcommand{\rabs}{\right\rvert}
\newcommand{\defeq}{\overset{\mathrm{def}}{=}}
\newcommand{\as}{\underset{n \to +\infty}{\overset{a.s}{\longrightarrow}}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\newcommand{\indep}{\perp \!\!\! \perp}


% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Adaptive Conformal Prediction by Reweighting Nonconformity Score}

\begin{document}

\twocolumn[
\icmltitle{Adaptive Conformal Prediction by Reweighting Nonconformity Score}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Salim I. Amoukou}{yyy,sch}
\icmlauthor{Nicolas J.B Brunel}{comp,yyy}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{University Paris Saclay}
\icmlaffiliation{sch}{Stellantis}
\icmlaffiliation{comp}{Quantmetry}

\icmlcorrespondingauthor{Salim I. Amoukou}{salim.ibrahim-amoukou@universite-paris-saclay.fr}
% \icmlcorrespondingauthor{Nicolas J.B Brunel}{nicolas.brunel@ensiie.fr}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Despite attractive theoretical guarantees and practical successes, Predictive Interval (PI) given by Conformal Prediction (CP) may not reflect the uncertainty of a given model. This limitation arises from CP methods using a constant correction for all test points, disregarding their individual uncertainties, to ensure coverage properties. To address this issue, we propose using a Quantile Regression Forest (QRF) to learn the distribution of nonconformity scores and utilizing the QRF's weights to assign more importance to samples with residuals similar to the test point. This approach results in PI lengths that are more aligned with the model's uncertainty. In addition, 
the weights learnt by the QRF provide a partition of the features space, allowing for more efficient computations and improved adaptiveness of the PI through groupwise conformalization. Our approach enjoys an assumption-free finite sample marginal and training-conditional coverage, and under suitable assumptions, it also ensures conditional coverage. Our methods work for any nonconformity score and are available as a \href{https://github.com/salimamoukou/ACPI}{Python package}. We conduct experiments on simulated and real-world data that demonstrate significant improvements compared to existing methods.
% Despite attractive theoretical guarantees and impressive practical successes, Predictive Intervals (PI) built within the Conformal Prediction (CP) framework can over or underestimate the PI at a new test point. In the case of regression, the PI is not informative and relevant about the reliability of a predictive model $\widehat{\mu}$ in the different regions of the features space.  For this reason, we propose to take into account the variability of the nonconformity scores in the split CP constructions, by learning the distribution of the estimated residuals $\widehat{\epsilon_i}$ with a Quantile Regression Forest (QRF) $\widehat{F}_{\widehat{\epsilon}}$. The used QRF has several advantages over existing methods based on kernel estimators, e.g. we can handle high-dimensional data and categorical/continuous covariates. In addition, the weights learnt by the QRF provide a partition of the features space which allows more efficient computations and better adaptiveness by groupwise conformalization. The resulting PI have varying and shorter lengths with respect to the prediction error of the test point for fixed $\widehat{\mu},\widehat{F}_{\widehat{\epsilon}}$.  We demonstrate that our construction enjoys an assumption-free finite sample marginal coverage and prove a so-called "training-conditional coverage" which ensures that most draws of the calibration data set result in accurate PI on future test points. Finally, we show that we achieve conditional coverage under suitable assumptions.
% Our methods are available as a Python package, and we provide experimental results on both simulated and real-world data to show its robustness and effectiveness compared to existing methods.
\end{abstract}

\section{Motivations}
Machine learning techniques offer single point predictions, such as mean estimates for regression and class labels for classification, without providing any indication of uncertainty or reliability. This can be a major concern in high-stakes applications where precision is vital.

Consider a training set $\mathcal{D}_m = \{\Z_i\}_{i=1}^{m}$ with $\Z_i = (\X_i, Y_i) \in \mathcal{X} \times \mathbb{R}$ drawn exchangeably from $P= P_{\X} P_{Y \vert \X}$, and an algorithm $\mathcal{A}$ that gives mean or quantile estimate $\widehat{\mu}(\cdot)=\mathcal{A}(\mathcal{D}_m)$. We consider the problem of constructing a predictive set $\hC(\cdot)$ for the unseen response $Y_{n+1}$ given a new feature $\Xtest$. Conformal Prediction is a universal framework that constructs a prediction interval $\widehat{C}(\Xtest)$ that covert $Y_{n+1}$ with finite-sample (non-asymptotic) coverage guarantee without assumption on $P$ and $\widehat{\mu}$. Conformal Prediction methods can be broadly divided into two categories: those that involve retraining the model multiple times, such as using full conformal \citep{vovk2005algorithmic} or jackknife methods \citep{barber2021predictive}, and those that use sample splitting, known as split conformal methods \citep{Papadopoulos2002InductiveCM, Lei2016DistributionFreePI}. The latter is more computationally feasible at the cost of splitting the data. Throughout this paper, we consider the split-conformal approach.

The foundation of the PI of the CP framework is the nonconformity score $\hV(\X, Y)$ that represents the error of the model $\widehat{\mu}$ on $\Z=(\X, Y)$. Given a calibration set $\mathcal{D}_n = \{ \Z_i\}_{i=1}^{n}$ independent of the training set $\mathcal{D}_m = \{ \Z_i\}_{i=1}^{m}$, and the nonconformity scores $\hV_i := \hV(\X_i, Y_i)$ for all $i \in \mathcal{D}_n$, the PI of $\Xtest$ given by the split conformal methods (split-CP) is:
\begin{equation} \label{eq:predictive_set}
    \small \widehat{C}(\Xtest)= \big\{y\in \mathbb{R}:  \hV(\Xtest,y)\leq \mathcal{Q}\big(1-\alpha; \; \widehat{F}_{n+1}(r)\big) \big\}
\end{equation}
where $\mathcal{Q}(1-\alpha; \;F)$ denotes the $(1-\alpha)$-quantile of any cumulative distribution function (c.d.f) $F$, and 
\begin{equation}
    \small \widehat{F}_{n+1}(r) = \sum_{i=1}^n \frac{1}{n+1} \mathds{1}_{\hV_i\leq r} + \frac{1}{n+1} \mathds{1}_{\infty \leq r}
    \label{eq:Simplecdf}
\end{equation}
$\widehat{F}_{n+1}(r)$ is the empirical c.d.f of the samples $\hV_{1:n} \cup \infty$. By exchangeability of the joint distribution of $(\Z_1, \dots, \Z_n, \Z_{n+1})$, we can show that the PI has marginal coverage:
\begin{align} \label{eq:MarginalCoverage}
    P^{n+1}\big(Y_{n+1} \in \hC(\Xtest)\big) \geq 1 - \alpha.
\end{align} $P^{n+1}$ denotes that the probability is taken with respect to $(\Z_1, \dots, \Z_{n+1})$ and $\alpha \in (0, 1)$ is a predefined miscoverage rate. However, despite the marginal guarantees, split-CP cannot represent the variability of the model's uncertainty given $\Xtest$. Indeed, it constructs the PI (eq. \ref{eq:predictive_set}) of future test points $\Xtest$ through the uniform distribution over the calibration residuals $\widehat{F}_{n+1}(v)$ (eq. \ref{eq:Simplecdf}) that treat all the calibration residuals as the same regardless of $\Xtest$. To better illustrate the issue, consider an example where the true distribution of $Y$ is homoskedastic, meaning that $Y = \mu(\X) + \epsilon$, where $\X$ and $\epsilon$ are independent.  In this case, the true residuals of the calibration samples $V_i:= V(\X_i, Y_i) = |Y_i - \mu(\X_i)| = |\epsilon|$ are independent of $\X_i$ and  $V_i \sim |\epsilon|$ for $i \in \mathcal{D}_n$. Hence, we have $F_{V}(v) = F_{V|\X=\x}(v)$. However, in practice, we only have the estimated residuals, $\hV_i:= \hV(\X_i, Y_i) = |Y_i - \widehat{\mu}(\X_i)| = |\mu(\X_i) - \widehat{\mu}(\X_i) + \epsilon|$, which do depend on $\X_i$ as the accuracy of $\widehat{\mu}$ can vary for different $\X_i$. For example, if $\X_i$ is in a region with a large amount of data, $\widehat{\mu}$ is likely to be more accurate, while in a region with a small amount of data, $\widehat{\mu}$ is likely to be less accurate. In contrast of the true residual, the conditional law of the estimated residuals $\hV |\X = \x$ is not equal to the marginal law of $\hV$, thus using the latter $F_{\hV}(v)$ as in split-CP to construct the PI of $\x$ may produce under/over coverage PI as $\mathcal{Q}\big(1-\alpha;\; F_{\hV}(v)\big)$ may be greater or lower than $\mathcal{Q}\big(1-\alpha;\; F_{\hV|\X = \x}(v)\big)$.

Our goal is to construct Prediction Intervals (PIs) with valid coverage for the model of interest $\widehat{\mu}$, while adjusting the width of the intervals to help visualize and understand the source of uncertainty of the model $\widehat{\mu}$. In fact, the split-CP uses a constant correction term $\mathcal{Q}\big(1-\alpha; \; \widehat{F}_{n+1}(v)\big)$ for all test samples, while we aim to have adaptive correction term that depends on the specific test observation $\Xtest$. To achieve this, we propose to directly model the conditional distribution of the nonconformity scores computed on the calibration dataset $\hV_i$ by re-weighting the distribution $\widehat{F}_{n+1}(r)$ in order to favor the residuals $\hV_i$ closer to the residual of $\X_{n+1}$. We also aim to give PI with stronger coverage guarantee. Indeed, in practical applications, what is of interest is the coverage rate on future test points based on a given calibration set. However, the marginal coverage in equation \ref{eq:MarginalCoverage} does not address this issue. It only bounds the coverage rate on average over all possible sets of calibration and test observations. In contrast, training-conditional coverage ensures that with  probability $1-\delta$ over the calibration samples $\mathcal{D}_n$, the resulting coverage on future test observation is still above $1 - \alpha$. Formally,
\begin{align*}
    P^n \Bigg( P\Big(Y_{n+1} \in \hC(X_{n+1})  \;|\;\mathcal{D}_n\Big) \geq 1 - \alpha \Bigg) \geq 1 - \delta.
\end{align*}
This style of guarantee is also known as  “Probably Approximately Correct” (PAC) predictive interval \cite{valiant1984theory}. Despite the importance of training-conditional coverage in practice, only a few methods have been proven to achieve it. \cite{vovk2012conditional} was the first to establish this result for split conformal methods, and recently \cite{bian2022training} has shown that the K-fold CV+ method also achieves it. However, no analogous results are currently known for other CP methods, such as jackknife+ \citep{barber2021predictive} and full-conformal \citep{vovk2005algorithmic}. Therefore, we propose a further calibration step such that our proposed adaptive PI also achieves training-conditional coverage.

There is another area of research that focuses on developing CP procedures for conditional coverage $\mathbb{P}(Y_{n+1} \in \widehat{C}(X_{n+1}) \; \vert \; \X_{n+1}=\x ) \geq 1-\alpha $. It is well known that obtaining nontrivial finite sample and distribution-free conditional coverage is not possible without making assumptions about the underlying distribution \citep{Lei2014DistributionfreePB, vovk2012conditional}. Consequently, we prove under suitable assumptions that our methods also achieve asymptotic conditional coverage.

\section{Related works and contributions}



%It is important to note that the problem of over/under-estimating the PI of each point $\Xtest$ also arises when the variable $Y$ is heteroscedastic, i.e., $Y = \mu(\X) + \epsilon(\X)$. In this case, both the true and the estimated residuals depend on $\X$. While various techniques exist to address heteroscedasticity of $Y$, such as Conformal Quantile Regression (CQR) \citep{romano2019conformalized} or locally adaptative split conformal prediction \citep{papadopoulos2008normalized}, they do not fully address our problem.

% simplest way of constructing  PI that satisfies \ref{eq:marginal_coverage} is the split/inductive conformal prediction. This method gives $C(\Xtest) = \Big\{v \leq \mathcal{Q}\big(\{\hV_i\}_{i \in \mathcal{D}_n}, 1 - \alpha\big)\Big\}$ where $\mathcal{Q}\big(\{\hV_1, \dots, \hV_n\}, 1 - \alpha\big)\Big\}$ is 



% $\mathcal{Q}()$
% Here, we assume that the model is homoscedastic, i.e., $Y = \mu(\X) + \epsilon$, $\X \indep \epsilon$ to distinguish from the usual narrative. Indeed, the classic narrative is based on the exchangeability of the true residuals of the calibration samples $V_1, \dots, V_n$ with $V_i = |Y_i - \mu(\X_i)|$ which are i.i.d. drawn from $V_i \sim \epsilon$ that do not depend on $\X$. However, in practice, we only have the estimated residuals $\hV_1, \dots, \hV_i$ with $\hV_i = |Y_i - \widehat{\mu}(\X)| = |\mu(\X) - \widehat{\mu}(\X) + \epsilon|$ and most methods \textcolor{blue}{[split conformal, jackknife, etc.]} treat them equally as we would have used the true residual $V_1, \dots, V_n$. Even if it does not harm the marginal coverage (\ref{eq:marginal_coverage}) as both the true/estimated residuals are exchangeable, it does affect the length of the intervals. Indeed, the estimated residuals $\hV_i = \hV(\X_i)$ depend on $\X$ as $\widehat{\mu}$ can be more/less accurate for different $\X_i$, then used the estimated residual equally tends to produce under/over-coverage in low/high uncertainty region.

% Note that the best (symmetric) interval we can construct using $\widehat{\mu}(X_{n+1})$ with the nonconformity score $V(\cdot)$ is $C^\star(X_{n+1}) = \Big[\widehat{\mu}(X_{n+1}) - \widehat{q}_{1-\alpha},\; \widehat{\mu}(X_{n+1}) + \widehat{q}_{1-\alpha}\Big]$ with $\widehat{q}_{1-\alpha}$ is the $(1 - \alpha)$-quantile of the $\mathcal{L}(|Y - \widehat{\mu}(\X)| \; \big| \X = \X_{n+1})$.

For the sake of simplicity, we use the absolute residual as the nonconformity score,  $\small \hV_i := \hV(\X_i, Y_i) = |Y_i - \widehat{\mu}(\X_i)|$, without loss of generality.  Therefore, the best (symmetric) PI that can be constructed with  $\widehat{\mu}(\cdot)$ and the score $\hV(\cdot)$ is $\small C^\star(\X_{n+1}) = \Big[\widehat{\mu}(\X_{n+1}) - q^\star_{1-\alpha}(\Xtest),\; \widehat{\mu}(\X_{n+1}) + q^\star_{1-\alpha}(\Xtest)\Big]$ where $\small q^\star_{1-\alpha}(\Xtest)$ is the $(1 - \alpha)$-quantile of $\small F_{\hV_{n+1} | \Xtest}(v)$.  To construct adaptive PIs, we propose focusing on the estimated residuals of the calibration samples $\{ \hV_i\}_{i \in \mathcal{D}_n}$, and approximate the distribution of $\hV | \X = \x$ or identify the stable regions $R_l$ where $Var(\hV(\X, Y) \;|\;\X \in R_l) \approx 0$, which would allow us to isolate the regions where there is high/low uncertainty of the model.

 % Recently, \cite{guanlocalizer} and \cite{han2022split} have proposed localized conformal prediction (LCP) and split localized conformal prediction (SLCP), respectively, which uses kernel smoothing or Nadaraya-Watson (NW) estimator \citep{nadaraya1964estimating} to approximate the conditional CDF of $\hV | \X = \x$ in order to construct an adaptive PI. The main difference between these two methods is that the NW estimator is learned on the training data $\mathcal{D}_m$ for SLCP, $\widehat{F}_h(v | \X = \x) = \sum_{i \in \mathcal{D}_m} w_h(\X_i, x) \mathds{1}_{v \leq \hV_i}$, and the calibration data $\mathcal {D}_n$ for LCP, $\widehat{F}_h(v | \X = \x) = \sum_{i \in \mathcal{D}_n} w_h(\X_i, x) \mathds{1}_{v \leq \hV_i}$, where $w_h(\X_i, x)$ is defined as:

Recently, \cite{guanlocalizer} proposed localized conformal prediction (LCP) and \cite{han2022split} proposed split localized conformal prediction (SLCP) which uses kernel-based weights $w_h(\X_i, x)$ or Nadaraya-Watson (NW) estimator \citep{nadaraya1964estimating} to approximate the conditional c.d.f of $\hV | \X = \x$ in order to construct an adaptive PI. Both methods differ in how they learn the NW estimator, SLCP uses the training data $\mathcal{D}_m$ to learn the estimator $\widehat{F}^{(S)}_h(r | \X = \x) = \sum_{i \in \mathcal{D}_m} w_h(\x, \X_i) \mathds{1}_{\hV_i \leq r}$, while LCP uses the calibration data $\mathcal{D}_n$ to learn the estimator $\widehat{F}^{(L)}_h(r | \X = \x) = \sum_{i \in \mathcal{D}_n} w_h(\x, \X_i) \mathds{1}_{\hV_i \leq r}$. The calibration step of these two methods is also different. The PI of $\Xtest$ given by SLCP is:\begin{align*}
   \small  C^{S}(\Xtest) = \Big[\widehat{\mu}(\Xtest) \pm \mathcal{Q}\big(1-\alpha; \; \widehat{F}^{(S)}_h(r |\Xtest) + \widehat{Q} \Big]
\end{align*}
where $\widehat{Q}$ is the split conformal correction term to achieve marginal coverage. On the other hand, LCP does not use split-CP but instead adapts the threshold $\alphat = 1 - \alpha$ in $\small \mathcal{Q}\big(1-\alpha;\; \widehat{F}^{(S)}_h(r | \X = \Xtest)\big)$ to achieve the marginal coverage. Specifically, LCP constructs the predictive interval (PI) for a new point $\Xtest$ as follows: 
\begin{align*}
    \small C^{L}(\Xtest) = \Big[\widehat{\mu}(\Xtest) \pm \mathcal{Q}\big(\alphat; \; \widehat{F}^{(L)}_h(r | \X = \Xtest)\big)\Big]
\end{align*}
where $\alphat$ is chosen to achieve the marginal coverage.
However, while both Localized Conformal Prediction (LCP) and Split Localized Conformal Prediction (SLCP) address the problem and guarantee marginal coverage, they have some limitations. A main limitation is that they are based on kernel methods, which are known to be limited in high dimensions due to the curse of dimensionality. Additionally, choosing the appropriate kernel width can be challenging and it can be difficult to define kernels that handle both categorical and continuous variables.


Another limitation of SLCP is that it learns $\widehat{F}^{(S)}_h(r | \X = \x)$ on the training data $\mathcal{D}_m$, which may result in overfitting and thus the calibration step using split-CP may produce large intervals to attain the marginal coverage. In contrast, LCP learns $\widehat{F}^{(L)}_h(r | \X = \x)$ on the calibration data $\mathcal{D}_n$, but the calibration step that consists of finding the adaptive $\alphat$ is computationally costly.

In this work, we propose to replace the Nadaraya-Watson (NW) estimator with the Quantile Regression Forest (QRF) algorithm \citep{meinshausen2006quantile} to estimate the distribution $\hV| \X = \x$ and use the LCP approach to calibrate the PI. The QRF algorithm is an adaptation of the Random Forest (RF) algorithm \citep{breiman1984classification} that can be seen as an adaptive neighborhood procedure \citep{lin2006random}. It estimates the conditional c.d.f of $\hV | \X = \x$  as $\widehat{F}(v | \X = \x) = \sum_{i} w_n(\x, \X_i) \mathds{1}_{\hV_i \leq v}$ where the weights correspond to the average number of times where $\X_i$ falls in the same leaves of the RF as $\x$.  Unlike kernel-based methods, the weights given by the RF depend on both $\X_i$ and $\hV_i$ due to the splits. We called this approach LCP-RF.

This estimator has several advantages over the NW estimator. Firstly, it is known to perform well in practice, even in high dimensions. It can handle both categorical and continuous variables. Additionally, it has interesting theoretical properties in high dimensions; under certain assumptions, it can be shown to be consistent and to adapt to the intrinsic dimension \citep{klusowski2021universal, scornet2015consistency}. As seen in Figure \ref{fig:toy_demo1}, the competitors LCP and SLCP fail to perform even on a very simple with 1 active and 20 noise features, while our method benefits from the power of the Random Forest algorithm on tabular data \citep{grinsztajn2022tree}.

\begin{figure*}[ht] 
\centering
\begin{tabular}{ccc}
\includegraphics[width=0.32\textwidth]{figures/toy_example_lcp.png} &
\includegraphics[width=0.32\textwidth]{figures/toy_example_slcp.png} &
\includegraphics[width=0.32\textwidth]{figures/toy_example_lcp_rf.png} \\
\end{tabular}
\caption{Predictive interval at level $1-\alpha$ of SLCP, LCP and LCP-RF of a random Forest fitted on toy model $(\X, Y), \X \in [0, 7]^{21}$ and the target is defined as $Y = \sin(X_1)^2 + 0.1 + 0.6 \times \epsilon \times \sin(2X_1)$ with $\epsilon \sim \mathcal{N}(0, 1)$, and $X_i \sim \mathcal{U}(0, 7)$ for all $i \in[21].$}\label{fig:toy_demo1}
\end{figure*}

Additionally, we show that the learned weights of the RF can be use to create a partition of the input space or to create groups/clusters. This allows for a more efficient computation of the LCP calibration and also allows for groupwise conformalization to give a more adaptive PI.

% To summarize, we propose methods that address the issue of over/under-estimating predictive intervals (PI) when using conformal prediction methods by considering the variability of the nonconformity score. The Quantile Regression Forest (QRF) algorithm is used to estimate the distribution of the estimated residuals, which has several advantages over existing methods (LCP and SLCP) that used Nadaraya-Watson, such as handling high-dimensional data and categorical/continuous covariates. Additionally, the learned weights of the QRF can be used to create a partition of the input space, allowing for more efficient computation of the PI and for groupwise conformalization to give a more adaptive PI.
In practice, it is often desirable to have a stronger coverage guarantee than marginal coverage. Consequently, we propose a further calibration step such that our PI satisfies training-conditional coverage. We also show that it achieves conditional coverage guarantee under suitable assumptions.


% our proposed methods have not only assumption-free finite sample marginal coverage but also training-conditional coverage which is of particular importance in practice as highlighted by \cite{bian2022training}. This ensures that with most given calibration data $\mathcal{D}_n$, the coverage rate of the predictive interval computed using $\mathcal{D}_n$ will be at least as high as the specified significance level ($1 - \alpha$), regardless of the specific test data $\Xtest$ used. More formally, there exists $\alpha, \delta \in (0, 0.05]$ s.t.

% \begin{align}
%     P^n \Bigg( P\Big(Y_{n+1} \in C(X_{n+1})  \;|\;\mathcal{D}_n\Big) \geq 1 - \alpha \Bigg) \geq 1 - \delta
% \end{align}
% where P denotes that the probability is with respect to the test point $Z_{n+1} = (\Xtest, Y_{n+1})$. 

An active area of research involves using a better nonconformity score to provide an adaptive prediction interval considering the variability of $Y|\X=\x$. Several methods have been proposed such as Conformal Quantile Regression (CQR) \citep{romano2019conformalized}, which uses score functions based on estimated quantiles, Locally Adaptive Split Conformal methods \citep{papadopoulos2008normalized} which use a scaled residual, and \cite{izbicki20a} proposed using the estimated conditional density as the conformity score. These methods incorporate different nonconformity scores $\hV(\cdot)$ that are better suited for handling the variability of $Y$. However, the extracted residuals $\hV_i$ of these nonconformity scores still depend on and vary according to $\X_{n+1}$. These methods are not competing with the LCP-RF approach as LCP-RF can be applied to them to improve their PIs.

% Various methods have been proposed to construct more adaptive Prediction Intervals by using better nonconformity scores. Conformal Quantile Regression (CQR) and Locally Adaptive Split Conformal methods are designed to handle the heterogeneity of Y by using score functions based on estimated quantiles instead of mean values, and by using a scaled residual, respectively. These methods incorporate different nonconformity scores $V(\cdot)$ that are better suited for handling heteroscedasticity. However, the extracted residuals $\hV_i$ of these nonconformity scores still depend and vary according to $\X_{n+1}$ and may not represent the variability of the uncertainty of the model. Indeed, these methods are not competing with the LCP-RF approach as we can still apply it on top of them.

The main contributions of this paper are: (1) Developing an adaptive PI that better represents the uncertainty of a given model $\widehat{\mu}$ by using a QRF to learn the conditional distribution of the residuals $\hV(\X, Y)$, and utilizing the LCP framework to calibrate the resulting PI for marginal coverage, (2) Introducing a calibration step to achieve training-conditional coverage, (3) Exploiting the structure of the weights of the QRF to create groups for more adaptive PI and efficient computation through groupwise conformalization, (4) Showing that our methods achieve asymptotic conditional coverage under suitable conditions, (5) Demonstrating through simulations and real-world datasets that our methods outperform competitors LCP and SLCP, and providing a Python package for the methods: \href{https://github.com/salimamoukou/ACPI}{github.com/salimamoukou/ACPI}.

% \begin{enumerate}
%     \item To have an adaptive PI that better represents the uncertainty of a given model $\widehat{\mu}$, we propose to learn the conditional distribution of the residuals of $V(\X, Y, \widehat{\mu})$ using a Quantile Regression Forest. Then, we used the LCP framework to calibrate the resulting PI for marginal coverage.
%     \item We propose a further calibration step to achieve training-conditional coverage.
    
%     \item We exploit the structure of the weights of the QRF to create a partition of the input space or to create groups/clusters. This allows for more efficient computation of the LCP calibration step and also allows for groupwise conformalization to give a more adaptive PI.

%     \item We show that our methods achieve asymptotical conditional coverage under suitable conditions.

%     \item We demonstrate empirically that our methods outperform the competitor LCP and SLCP by a significant margin on simulated and real-world datasets. Hence, the LCP-RF can be used to improve the PI of any CP methods. We provide a Python package that implements the methods.
% \end{enumerate}

\section{Random Forest Localizer}

In this section, we present the Random Forest Localizer for constructing adaptive PI that depends on the test point $\Xtest$. The approach utilizes the learned weights of the RF and assigns higher weights to calibration samples that have residuals $\hV_i$ similar to $\hV_{n+1}$. This is based on the RF algorithm's ability to partition the input space by recursively splitting the data, resulting in similar observations with respect to the target variable within each leaf node of the trees. The basic idea of the tree of the RF is to partition the input space into cell (leaf node) $R_l$ such that $\small Var(\hV(\X, Y) \;|\;\X \in R_l) \approx 0$. The corresponding weight of each calibration sample for $\Xtest$ are determined by the number of times it appears in the leaves of the trees where $\Xtest$ falls.

Random Forest (RF) is grown as an ensemble of $k$ trees, based on random node and split point selection based on the CART algorithm \citep{breiman1984classification}. The algorithm works as follows. For each tree, $a_n$ data points are drawn at random with replacement from the original data set of size $n$; then, at each cell of every tree, a split is chosen by maximizing the CART-criterion; finally, the construction of every tree is stopped when the total number of cells in the tree reaches the value $t_n$. The trees are then averaged to gives the prediction of the forest. The Random Forest estimator can also be seen as an adaptive neighborhood procedure \citep{lin2006random}. Let assume we have trained the RF on $\mathcal{D}_n$, then for every instance $\boldsymbol{x}$, the observations in $\mathcal{D}_n$ are  weighted by $w_n(\boldsymbol{x}, \X_i)$, $i=1, \dots, n$. Therefore, the prediction of Random Forests and the weights can be rewritten as $m_{k, n}(\boldsymbol{x}, \Theta_{1:k}, \mathcal{D}_n) = \sum_{i=1}^{n} w_n(\boldsymbol{x}, \X_i) Y_{i}$, with 
\begin{align*}
   \small w_n(\boldsymbol{x}, \X_i) =  \sum_{l=1}^{k}  \frac{B_n(\X_i; \Theta_l) \; \mathds{1}_{\X_{i} \in  A_n(\boldsymbol{x}; \;\Theta_l)}}{k\times N_n(\boldsymbol{x};\; \Theta_l)}
\end{align*}where $\Theta_{1:k} = \{\Theta_l, l=1, \dots, k\}$ are independent random vectors that represent the bootstrap samples and the splitting candidate variables, $A_n(\boldsymbol{x}; \;\Theta_l)$ is the tree cell (leaf) containing $\boldsymbol{x}$, $N_n(\boldsymbol{x};\; \Theta_l)$ is the number of bootstrap elements  that fall into $A_n(\boldsymbol{x}; \;\Theta_l)$, and $B_n(\X_i; \Theta_l)$ is the bootstrap component i.e. the number of times that the observation has been chosen from the original data.

Random Forests can be used to estimate more complex quantities, such as cumulative hazard function \citep{ishwaran2008random}, treatment effect \citep{wager2017estimation}, and conditional density \citep{du2021wasserstein}. Quantile Regression Forests, proposed by \cite{meinshausen2006quantile}, use the same weights $w_n(\boldsymbol{x}, \X_i)$ as Random Forests to approximate the c.d.f $F(y |\x)$ as:\begin{equation}
    \small \widehat{F}(y |\boldsymbol{x}) = \sum_{i=1}^{n} w_n(\boldsymbol{x}, \X_i) \mathds{1}_{Y_i \leq y} \label{eq:estimator_boostrap_quantile}
\end{equation}
\paragraph{Localized Random Forest:} To approximate the estimated residuals $\hV | \X = \x$, we propose to fit a Quantile Regression Forest $\widehat{p}(r| \X = \x)$  on the calibration data $\widehat{\mathcal{D}}_n = \{(\X_i, \hV_i)\}_{i=1}^n$, and $\widehat{p}(r | \X = \x)$ is defined as:
\begin{align} \label{eq:qrf}
    \widehat{p}(r|\x) & = \sum_{i=1}^{n + 1} w_n(\boldsymbol{x}, \X_i) \mathds{1}_{\hV_{i} \leq r}
\end{align}It's worth noting that this estimator is slightly different from \ref{eq:estimator_boostrap_quantile}, as it includes the observation $\Xtest$ in the weighted sum. We will see later that this addition would be essential to prove the marginal coverage property of our method.

Using the estimator (\ref{eq:qrf}), a natural PI for $\hV_{n+1}$ is:
\begin{align} \label{eq:ci_v0}
    C_V(\X_{n+1}) = \big\{v: v \leq \mathcal{Q}\big(1 - \alpha;\; \widehat{p}(v|\Xtest)\big) \big\}
\end{align} The question at hand is whether the PI $C_V(\Xtest)$ defined in (\ref{eq:ci_v0}) satisfies marginal coverage. If $w_n(\Xtest, \X_i) = \frac{1}{n+1}$, we have $\mathcal{Q}\big(1 - \alpha;\; \widehat{p}(v|\Xtest) \big) = \hV_{(\lceil (1-\alpha)(n + 1) \rceil)}$ and thanks to the quantile lemma and exchangeability of the $\hV_i$, we have the marginal coverage. However, If $\widehat{p}(v | \Xtest)$ gives non-equal weights to the calibration samples, it is no longer the case. Recent methods have been proposed by \cite{tibshirani2019conformal} and \cite{barber2022conformal} that achieve marginal coverage when using reweighting. However, these methods cannot be applied to calibrate our PI, as they work under different assumptions. The method introduced by \cite{barber2022conformal} assumes that the weights do not depend on the data, while the method proposed by \cite{tibshirani2019conformal} handles data-dependent weights but assumes a covariate shift, where the training and test data have different input distributions but the same conditional distribution $Y | \X$.


To calibrate our PI, we use the Localized Conformal Prediction (LCP) framework to select an appropriate level $\alphat$ to  the quantile used in the PI (\ref{eq:ci_v0}) to ensure marginal coverage at level $1-\alpha$. Hence,  the PI becomes
\begin{align} \label{eq:pi_a}
    C_V(X_{n+1}) = \{v: v \leq \mathcal{Q}\big(\alphat;\; \widehat{p}(v|\Xtest)\big) \}.
\end{align}

% In this section, we  provide an overview of the LCP framework using the Random Forest localizer for completeness. We also describe our calibration method, which ensures training-conditional coverage, and how we use the weights of the RF to speed up the LCP calibration process and create more adaptable prediction intervals.

\section{LCP-RF}
In this section, we give a comprehensive overview of the LCP framework of \cite{guanlocalizer} with the Random Forest localizer for completeness. Additionally, we describe our calibration approach that guarantees training-conditional coverage, and how we leverage the weights of the RF to improve the LCP calibration process and produce more adaptive prediction intervals. All proofs of theorems and lemmas are in the appendix.

We denote $\small \mathcal{F}_i(\cdot) = \widehat{p}(r | \X_i) = \sum_{j=1}^{n+1} w_n(\X_i, \X_j) \mathds{1}_{\hV_j \leq r}= \sum_{j=1}^{n} w_n(\X_i, \X_j) \mathds{1}_{\hV_j \leq r} + w_n(\X_i, \X_{n+1}) \mathds{1}_{\hV_{n+1} \leq r}$ as the estimated distribution of $\hV$ given $\X_i$ given by the Quantile Regression Forest. As $\hV_{n+1}$ is not observed and we need to consider the possible values of $\hV_{n+1}$  for  constructing the PI, we introduce the additional notations  $\mathcal{F}_i^v$ for the c.d.f when $\hV_{n+1} = v$ if $v$ is finite, and  $\mathcal{F} = \mathcal{F}_{n+1}^{\infty}$ if $\hV_{n+1} = +\infty$. 


%We define $\mathcal{F}^v_i = \mathcal{F}_i({V_{n+1}=v})$ where we set $\hV_{n+1} = V$ with $V \in \overline{\mathbb{R}}$, and $\mathcal{F} = \mathcal{F}_{n+1}^{\infty} = \widehat{p}(v | \X_{n+1}) = \sum_{j=1}^{n+1} w_n(\X_{n+1}, \X_j) \mathds{1}_{\hV_j \leq V}$ where $\hV_{n+1} = +\infty$.


%For ease of presentation, we denote $\espi\mathcal{F}_i^{v_{n+1}} = \widehat{p}(\epsilon | \X_i) = \sum_{j=1}^{n+1} w_n(\X_i, \X_j) \mathds{1}_{\hV_j \leq v}= \sum_{j=1}^{n} w_n(\X_i, \X_j) \mathds{1}_{\hV_j \leq \epsilon} + w_n(\X_i, \X_{n+1}) \mathds{1}_{v_{n+1} \leq \epsilon}$ as the estimated distribution of $\hV$ given $\X_i$ given by the Quantile Regression Forest. We define $\mathcal{F}^v_i = \mathcal{F}_i({V_{n+1}=v})$ where we set $\hV_{n+1} = V$ with $V \in \overline{\mathbb{R}}$, and $\mathcal{F} = \mathcal{F}_{n+1}^{\infty} = \widehat{p}(v | \X_{n+1}) = \sum_{j=1}^{n+1} w_n(\X_{n+1}, \X_j) \mathds{1}_{\hV_j \leq V}$ where $\hV_{n+1} = +\infty$.
\subsection{Localized Conformal Prediction \cite{guanlocalizer}}
% we restate the results of Guan...
The following lemma, which is the cornerstone of the LCP framework, shows how to achieve marginal coverage by properly selecting the level $\alphat$ of the quantile of the localizer.
\begin{lemma} \label{theorem:lcp}
    Let $\alphat$ be the smallest value in $\small \Gamma = \Big\{ \sum_{j=1}^{k} w_n(\X_i, \X_j): i=1, \dots, n;\; k=1, \dots, n\Big\}$ s.t.
    \begin{align}
        \small \sum_{i=1}^{n+1}\frac{1}{n+1} \mathds{1}_{\hV_i \leq \mathcal{Q}(\alphat;\;  \mathcal{F}_i )} \geq 1 - \alpha,
    \end{align}
\end{lemma}
then $\small \mathbb{P}\Bigl\{\hV_{n+1} \leq \mathcal{Q}(\alphat; \; \mathcal{F}_{n+1})\Bigl\} \geq 1-\alpha$, or equivalently $\small \mathbb{P}\Bigl\{\hV_{n+1} \leq \mathcal{Q}(\alphat; \; \mathcal{F})\Bigl\} \geq 1-\alpha$.

It is important to keep in mind that $\alphat$ and $\mathcal{F}_{n+1}$ depends on $\small\widehat{\mathcal{D}}_{n} = \Big\{\widehat{Z}_1, \dots, \widehat{Z}_{n} \Big\}$ and $\widehat{Z}_{n+1} = (\Xtest, \hV_{n+1})$, but we will not specify them for ease of reading.

Now, we can use Lemma \ref{theorem:lcp} to test $H_0: \hV_{n+1} = v$ for each $v \in \overline{\mathbb{R}}$ under exchangeability, then invert the test to construct the PI. $C_V(\Xtest)$ consists of all values $v$ that are not rejected by the test.  The resulting PI has marginal coverage as shown in the following Theorem.
\begin{theorem} \label{lemma:lcp}
    Let $\hV(.)$ be a fixed nonconformity score. At $\hV_{n+1} = v$, let define $\alphat$ that depends on $\widehat{\mathcal{D}}_n$ and $(\Xtest, v)$ to be the smallest value $\alphat \in \Gamma$ such that\begin{align} \label{eq:lcp}
        \small \sum_{i=1}^{n + 1} \frac{1}{n+1} \mathds{1}_{\hV_i \leq \mathcal{Q}(\alphat; \; \mathcal{F}^v_i) } \geq 1 - \alpha.
    \end{align}
    Set $\small C_V(V_{n+1}) = \Big\{ v: v \leq \mathcal{Q}\Big(\alphat; \; \mathcal{F}\Big)\Big\}$ and $\small C(Y_{n+1}) = \Big\{ y: \hV(\Xtest, y) \leq \mathcal{Q}\Big(\alphat; \; \mathcal{F}\Big)\Big\}$, then by construction Lemma \ref{theorem:lcp} gives $\small \mathbb{P}\big(Y_{n+1} \in C(\Xtest)\big) = \mathbb{P}\big(\hV_{n+1} \in C_V(\Xtest)\big) = \mathbb{P}\big(\hV_{n+1} \leq \mathcal{Q}\big(\alphat; \; \mathcal{F}\big)\big) \geq 1 - \alpha$.
\end{theorem}
At this point, the LCP  method is not practical as it requires computing $\alphat$ for every possible value of $v \in \overline{\mathbb{R}}$ in order to construct the prediction interval (PI). This process can be extremely time-consuming and computationally intensive. 


However, \cite{guanlocalizer} show that the computation of $C_V(\Xtest)$ can be done efficiently thanks to its interesting properties. Indeed, If $v$ is accepted in $C_V(\Xtest)$, all $v^\prime \leq v$ is also accepted. Thus, we just need to find the largest accepted value $v^\star$. Additionally, as $\mathcal{Q}(\alphat; \;\mathcal{F})$ is non-decreasing in both $\alphat$ and $v$, and piece-wise constant in $\alphat$, with value changes only occurring at different $\hV_i$, it can be proven that the largest value is attained by one of the estimated residuals $\hV_{k^\star}$ with $k^\star \in [n]$. Therefore, the closure $\bar{C}_V(\Xtest)$ of $C_V(\Xtest)$ is given by $\small\bar{C}_V(\Xtest)= \Big\{v: v \leq \hV_{k^\star} \Big\}$ for some $k^\star \in [n]$. The following Lemma shows how to find $V_{k^\star}$.


% \section{LCP-RF} 

% \paragraph{Practical computation of $C_V(\Xtest)$:}
% The interval $C_V(\Xtest)$ has some nice properties that permit its computation efficiently. One of the main properties is: 

% \begin{itemize}
%     \item If $v^\star$ is accepted in $C_V(\Xtest)$ then all $v \leq v^\star$ is also accepted.
%     \item $\mathcal{Q}(\alphat; \; \mathcal{F}(V))$ is non-decreasing on $\alphat$ and $V$, piece-wise constant on $\alphat$, and with value changing only at at different $V_i$.
%     \item The closure of $C_V(\Xtest)$ is of the form $\bar{C}_V(\Xtest)= \Big\{v: v \leq \bar{V}_{k^\star} \Big\}$ with $k^\star \in [n]$.
% \end{itemize}
\begin{lemma} \label{eq:lemma_sk}
    We denote $\hV_{(1)}, \dots, \hV_{(n)}$ the order statistics of the residuals of the calibration samples $\widehat{\mathcal{D}}_n$, and set $\hV_{(n+1)} = +\infty$, and $\tilde{\theta}_k = \sum_{i=1}^{n} w_n(\Xtest, \X_{i}) \mathds{1}_{\hV_{i} < \hV_{(k)}}$. Let  $k^\star \in \{1, \dots, n+1\}$ the largest index s.t.\begin{align} \label{eq:sk}
        \small S(k) := \sum_{i=1}^{n}\frac{1}{n+1} \mathds{1}_{\hV_i \leq \mathcal{Q}\big(\tilde{\theta}_{k^\star};\; \mathcal{F}_i^{\hV_{(k^\star)}}\big)} < \alpha.
    \end{align}Then,  $\small \bar{C}_V(\Xtest)= \Big\{v: v \leq \hV_{(k^\star)} \Big\}$ is the closure of $C_V(\Xtest)$.
\end{lemma} \cite{guanlocalizer} also proposed an algorithm that computed $S(k)$ in $\mathcal{O}(n \log(n))$ time. The description of the algorithm can be found in the original paper.

%\subsection{Beyond the LCP framework}
\subsection{Training-Conditional coverage for LCP-RF}

 Here, we consider training-conditional coverage or PAC predictive interval guarantees for the LCP-RF. Let's consider the coverage rate given a calibration set $\mathcal{D}_n$ as 
 \begin{align*}
     \cov(\mathcal{D}_n) = P(\hV_{n+1} \in C_V(\Xtest) \; \vert \; \mathcal{D}_n)
 \end{align*}
 where the probability is taken with respect to the test point $(\Xtest, \hV_{n+1})$. The PAC predictive interval ensures that for most draws of the calibration samples $\mathcal{D}_n \sim P^n$, we have $\cov(\mathcal{D}_n) \geq 1 - \alpha$. Formally, $\exists \delta$ s.t. $P^n\left(\cov(\mathcal{D}_n) \geq 1 - \alpha \right) \geq 1 - \delta$.
 

We use a two-step approach to ensure training-conditional coverage for the LCP-RF. First, we use a portion of the calibration samples to ensure marginal coverage by applying the LCP-RF. Next, we use a separate portion of the calibration samples to learn a correction term, which is then added to the LCP-RF approach to ensure training-conditional coverage. This approach is similar to the one used in \citep{kivaranovic2020adaptive}.

We split the calibration set $\widehat{\mathcal{D}}_n$ into two sets $\small \widehat{\mathcal{D}}^i_{n_i} = \Big\{(\X^i_1, \hV^i_1), \dots, (\X^i_{n_i}, \hV^i_{n_i}) \Big\}$ for $\small i=1, 2$  with $\small n_1 + n_2=n$. We train the Quantile Regression Forest on $\widehat{\mathcal{D}}^1_{n_1}$, and compute PI for the observations in the second set $\widehat{\mathcal{D}}^2_{n_2}$ using the LCP-RF. The PI of each $i \in \widehat{\mathcal{D}}^2_{n_2}$ is $\small C_V(\X^2_i) = \Big\{v: v \leq \mathcal{Q}(\alphat(\X^2_i);\; \mathcal{F}^{2, \infty}_i) \Big\}$, where $\alphat(\X^2_i)$ is the adapted level $\alphat$ to have marginal coverage if $\X^2_i$ is the test point and $\small\mathcal{F}^{2, \infty}_i = \sum_{j=1}^{n_1 + 1} w_n(\X^2_i, \X^1_j) \mathds{1}_{\hV^1_j \leq V}$ is the estimated residual distribution learn on $\mathcal{D}^1_{n_1}$ evalued on $\X^2_i$ where we set $\X^1_{n_1 + 1} = \X^2_i$ and $\hV^1_{n_1 + 1} = +\infty$. The following lemma shows that we can correct the corresponding $\alphat(\X^2_i)$ by adding a correction term $\widehat{\alpha}$ to ensure PAC coverage.
\begin{theorem}\label{theo:pac_interval}
    Let $\small \epsilon>0$,  $\alpha - \epsilon >0$ and $\small \widehat{\alpha} \in T = \{\alpha_0=0, \alpha_1,\dots, \alpha_K=\alpha\}$  s.t.
    \begin{align}
        \small \sum_{i=1}^{n_2} \frac{1}{n_2} \mathds{1}_{\hV^{2}_{i} \leq  \mathcal{Q}\left( \alphat(\X^{2}_{i}) + \widehat{\alpha};\; \mathcal{F}^{2, \infty}_{i}\right)} \geq 1 - \alpha
    \end{align}
    Then, we have  $\small P^{n_1}\Bigg\{ \cov(\mathcal{D}_{n_1}) \geq 1 - \alpha - \epsilon \Bigg\} \geq 1 - \delta$ with $\delta=K \exp(-2n_2\epsilon^2)$ and $\small \cov(\mathcal{D}_{n_1}) = P \Big\{ \hV_{n+1} \leq \mathcal{Q}\Big(\alphat(\X_{n + 1}) + \widehat{\alpha};\; \mathcal{F}^{\infty}_{n+1}\Big)\;\Big| D_{n_1}\Big\}$.
\end{theorem}
\textbf{Remark:} This result is valid under the i.i.d assumption and not under exchangeability as the other results of the paper. We suggest choosing a grid $T \subset [0, \alpha]$ as we observe in practice that $\alphat(\X_{n + 1}) \approx 1 - \alpha$, but the idea remains the same, which is to choose a grid in order to go from $\alphat(\X_{n + 1})$ to 1. Additionally, as $\alphat(\X_{n + 1}) + \widehat{\alpha}$ may be above $1$, we define $\alphat(\X_{n + 1}) + \widehat{\alpha} := \alphat(\X_{n + 1}) + \widehat{\alpha} \lor 1$.

%\subsection{Properties of the Random Forest weights and its implications on LCP} 
\subsection{Clustering using the weights of LCP-RF}

In this section, we analyze the weights of the Random Forest Localizer and show that it offers several benefits compared to traditional kernel-based localizers. These benefits include faster computation of PIs and more adaptive PIs. One key difference between the RF localizer and kernel-based localizers is that the RF localizer's weights are sparse, meaning that they have many zero coefficients. For a given test point $\Xtest$, if $w_n(\Xtest, \X_i) = 0$, then the function $\mathcal{F}_i $ does not depend on the value of $\hV_{n+1}$. Thus, it may not be necessary to use  $\mathcal{F}_i$ in eq. \ref{eq:sk} for the marginal calibration.

The weights defined by the Random Forest Localizer have a structure that can be utilized to group similar observations together before applying the calibration steps. Indeed, we can view the weights of the RF on the calibration set as a transition matrix or a weighted adjacency matrix $G$ where $ \small G(\X_i, \X_j) = w_n(\X_i, \X_j)$ and $\small \forall j \in [n], \sum_{i=1}^{n} w_n(\X_j, \X_i) = \sum_{i=1}^{n} w_n(\X_i, \X_j) = 1.$


% \begin{itemize}
%     \item If $w_n(\X_i, \X_j) > 0$, $\exists k$ s.t $\X_i, \X_j \in A_k$ i.e., there exist a leaf $k$ of the forest that contains $\X_i, \X_j$.
%     \item If $w_n(\X_i, \X_j) = 0$, $\forall k$, we have  $\X_i, \X_j \notin A_k$ i.e., there exist no leaf $k$ in the forest that contains both $\X_i, \X_j$
% \end{itemize}
% One implication of these facts is when we are searching for the smallest $\alphat$ that satisfies Eq. \ref{eq:lcp} for $\Xtest$ we may use observations $i$ and $\mathcal{F}_i$ that do not used $\Xtest$ if $w_n(\X_i, \Xtest) = 0$. So we may not want to use $\mathcal{F}_i \indep \hV_{n+1}$ in Eq \ref{eq:lcp}.




To exploit this structure, we propose to group observations that are connected to each other, and separate observations that are not connected. This can be done by considering the connected components of the graph represented by the matrix $G$. Assume that $G$ has $L$ connected components represented by the disjoint sets of vertices $G_1, \dots, G_L$, defined such that for any $\X_i, \X_j \in G_l$, there is a path from $\X_i$ to $\X_j$, and they are connected to no other vertices outside the vertex in $G_l$. This leads to the existence of a partition of the input space $R_1, \dots, R_L \in \mathcal{X}$, where $\forall k, l \in [L], R_l \cap R_q = \varnothing$, and for all $\X_i \in R_p, X_j \in R_q$, we have $w_n(\X_i, \X_j) = 0$. The regions $R_i$ is defined as $\small R_i = \Big\{\x\in \mathbb{R}^d: \exists \X \in G_i, w_n(\x, \X) > 0 \text{ and } \forall \X^\prime \in G_k, k\neq l, \; w_n(\x, \X^\prime)=0 \Big\}$. By definition of the weights, we can also define $R_i$ using the leaves of the RF
\begin{align} \label{eq:ri}
    \small R_i = \bigcup_{\X_i \in G_i}\Big[  \cup_{l=1}^{k} A_n(\X_i, \Theta_l)\Big].
\end{align}
This shows that the $R_i$ are connected space. Hence, we can apply the conformalization steps separately on each group and use only the observations that are connected to the test point. By using the conformalization by group, we reduce the computation of $S(k)$ in Lemma \ref{eq:lemma_sk} needed for the computation of the PI from $\mathcal{O}(n \log(n))$ to $\mathcal{O}(|\textbf{R}(\Xtest)| \log(|\textbf{R}(\Xtest)|))$, since we only use the observations in the group of $\Xtest$ in the calibration step. This results in a more accurate and efficient PI. In addition, no coverage guarantees are lost as the $R_i$ forms a partition. We prove the marginal coverage of the group-wise LCP-RF in Appendix \ref{sup:group_marginal_coverage}.

In some cases, the graph may have a single connected component. Consequently, we propose to regroup calibration observations by (non-overlapping) communities using the weights $w$. This involves grouping the nodes (calibration samples) of the graph into communities such that nodes within the same community are strongly connected to each other and weakly connected to nodes in other groups. 

Various methods exist for detecting communities in graphs, such as hierarchical clustering, spectral clustering, random walk, label propagation, and modularity maximization. A comprehensive overview of these methods can be found in  \citep{Schaeffer2007SurveyGC}. Nonetheless, it is challenging to determine the most suitable approach as the selection depends on the particular problem and characteristics of the graph. In our experiments, we found that the popular Louvain-Leiden \citep{traag2019louvain} method coupled with Markov Stability \citep{delvenne2010stability} is effective in detecting communities of the learned weights $w$ of the Random Forest. However, any clustering method can be used depending on the specific application and dataset. 


Let's assume a graph-clustering algorithm $C$ that returns L disjoint clusters $C(\mathcal{D}_n) = \{ C_1, \dots, C_L \}$. Note that contrary to connected components we can have $\X \in C_i, \; \X^\prime \in C_j$ and $w_n(\X, \X^\prime) \neq 0$, therefore it's more difficult to define the associated regions $R_1, \dots R_L$ that form a partition of $\mathcal{X}$ s.t. for any $\X \in C_i$, then $\X \in R_i$. 

We define $R_i$ as the set of points $\x$ that assigns the highest weights to the observations in cluster $C_i$. As $w$ can be interpreted as a transition matrix, it represents the set of $\X$ such that $\small \widehat{p}(\X \in C_i) > \widehat{p}(\X \in C_k), k\neq i$, where the probability is computed using the weights of the forest. Formally, $R_i$ can be represented as
\begin{align*}
    \small R_i = \Big\{ \x \in \mathbb{R}^d: \sum_{j \in C_l} w_n(\x, \X_j) > \sum_{j \in C_k} w_n(\x, \X_j),\; k\neq i\Big\}
\end{align*}
However, we also need to define another set for observations that are "undecidable" i.e., belong to several groups at the same time. We define this set as $\small \bar{R} =\Big\{ \x \in \mathbb{R}^d: \exists k, l \in [L], \sum_{j \in C_l} w_n(\x, \X_j) = \sum_{j \in C_k} w_n(\x, \X_j)\Big\}$.

Finally, we get marginal/PAC coverage as we do with the connected components case by applying the calibration step conditionally on the groups $R_1, \dots, R_L$ and $\bar{R}$.


\section{Asymptotic conditional coverage}

We study the conditional coverage of LCP-RF. It is widely recognized that obtaining meaningful distribution-free conditional coverage is impossible without making further assumptions \citep{Lei2014DistributionfreePB, vovk2012conditional}. Below, we demonstrate the asymptotic conditional coverage of LCP-RF while making weaker assumptions than LCP.

\begin{assumption} \label{prop:assym1}
$\forall x \in \mathbb{R}^d$, the conditional cumulative distribution function $F(r | X=x)$ is continuous.
\end{assumption}
Assumption \ref{prop:assym1} is necessary to get uniform convergence of the RF estimator.
\begin{assumption} \label{prop:assym2}
For $l \in [k]$, we assume that the variation of the conditional cumulative distribution function within any cell goes to $0$.
\begin{equation*}
\small \forall x \in \mathbb{R}^d, \forall r \in \mathbb{R}, \sup_{\boldsymbol{z} \in A_n(\boldsymbol{x}; \;\Theta_l)} |F(r | \boldsymbol{\boldsymbol{z}}) - F(r|\boldsymbol{x})| \overset{a.s}{\to} 0  
\end{equation*}
\end{assumption}
Assumption \ref{prop:assym2} allows for control of the approximation error of the RF estimator. \cite{scornet2015consistency} show that this is true when the data come from additive regression models \citep{additiveStone}, and \cite{elie2020random} show that it holds for a more general class, such as product functions or sums of product functions. This result also applies to all regression functions, with a slightly modified version of RF, where there are at least a fraction $\gamma$ observations in child nodes and the number of splitting candidate variables is set to 1 at each node with a small probability. Therefore, we do not need to assume that $F(r|.)$ is Lipschitz, as is the case for LCP \cite{guanlocalizer}, which is a much stronger assumption.

\begin{assumption} \label{prop:assym3}
Let $k$ and $\small N_n(\boldsymbol{x};\; \Theta_l)$ (number of bootstrap observations in a leaf node), then there exists $k = \mathcal{O}(n^\alpha)$, with $\alpha > 0$, and $\forall \x \in \mathbb{R}^d$, $\small N_n(\boldsymbol{x};\; \Theta_l) = \Omega\footnote{$\small f(n) = \Omega(g(n)) \iff \exists k > 0, \exists n_0 >0  \; \forall n \geq n_0, \; |f(n)| \geq |g(n)|.$ }(\sqrt{n} (ln (n))^\beta)$, with $\beta > 1$ a.s. 
\end{assumption}
Assumption \ref{prop:assym3} allows us to control the estimation error and means that the cells should contain a sufficiently large number of points so that averaging among the observations is effective. It can be enforced by adjusting the hyperparameters of the RF.

Under these assumptions, we prove that the selected $\alphat(v)$ when $\hV_{n+1} = v$ given by the LCP-RF converges to $1-\alpha$,  and the resulting PI achieves the target level $1-\alpha$.
\begin{theorem} \label{theo:lcp_cond}
    Let $\alphat(v)$ and $C_V(\Xtest)$ define as in Theorem \ref{lemma:lcp}. Under assumptions \ref{prop:assym1}-\ref{prop:assym3}, for all $\epsilon > 0$, we have $\small\lim_{n \rightarrow \infty} P\Big( \hV_{n+1} \in C_V(\Xtest) \; |\;  \Xtest\Big) = 1-\alpha$ and $\small \lim_{n \rightarrow \infty} P\Big( \max_v|\alphat(v) - (1-\alpha)| < \epsilon\; |\;  \Xtest\Big) = 1$.
\end{theorem}


\section{Experiments}

\begin{figure*}[ht!]
\centering
\subfigure[sim (lengths)]{\includegraphics[width=0.19\textwidth]{figures/sim_1_lengths2.png}\label{fig:lengths_sim}}
\subfigure[bio (lengths)]{\includegraphics[width=0.19\textwidth]{figures/bio_lengths2.png} \label{fig:bio_lengths}}
\subfigure[meps19 (lengths)]{\includegraphics[width=0.19\textwidth]{figures/meps_19_lengths3.png}}
\subfigure[bike (lengths)]{\includegraphics[width=0.19\textwidth]{figures/bike_lengths2.png}}
% \subfigure[concrete (lengths)]{\includegraphics[width=0.19\textwidth]{figures/lengths_concrete.png}}
\subfigure[star (lengths)]{\includegraphics[width=0.19\textwidth]{figures/star_lengths2.png}\label{fig:star_lengths}}
\subfigure[sim ($\small err_{n+1}$)]{\includegraphics[width=0.19\textwidth]{figures/sim_1_residuals2.png}\label{fig:residuals_sim}}
\subfigure[bio ($\small\widehat{err}_{n+1}$)]{\includegraphics[width=0.19\textwidth]{figures/bio_residuals2.png}\label{fig:bio_residuals}}
\subfigure[meps19 ($\small\widehat{err}_{n+1}$)]{\includegraphics[width=0.19\textwidth]{figures/meps_19_residuals3.png}}
\subfigure[bike ($\small\widehat{err}_{n+1}$)]{\includegraphics[width=0.19\textwidth]{figures/bike_residuals2.png}}
% \subfigure[concrete (errors)] {\includegraphics[width=0.19\textwidth]{figures/residuals_concrete.png}}
\subfigure[star ($\small\widehat{err}_{n+1}$)]{\includegraphics[width=0.19\textwidth]{figures/star_residuals2.png}\label{fig:star_residuals}} 
\caption{PI lengths and errors of the different methods. The training-conditional coverages are at the top of the figure.}
\label{fig:realdata_results}
\end{figure*}
We evaluate the performance of our proposed methods: LCP-RF (Random Forest Localizer with marginal and training-conditional calibration), LCP-RF-G (LCP-RF with groupwise calibration) and QRF-TC (Random Forest Localizer with only training-conditional calibration) against their competitors SPLIT (split-CP), SLCP and LCP. We used the original implementation of SLCP and LCP that can be found in \href{https://github.com/aaronhan223/SLCP}{github.com/aaronhan223/SLCP} and \href{https://github.com/LeyingGuan/LCP}{github.com/LeyingGuan/LCP} respectively and tuned the kernel widths as described in their respective papers. We test the methods on simulated data with heterogeneous $Y$ and 4 real-world datasets from UCI \citep{uci}: physicochemical properties of protein tertiary structure (bio, n=45730, p=10),  medical expenditure
panel survey number 19 (meps19, n=15781, p=141), bike sharing (bike, n=10886, p=12) and Tennessee’s student teacher achievement ratio (star, n=11598, p=48). We split each dataset into a training $(40\%)$ - calibration $(40\%)$ - test $(20\%)$ sets and compute the PI at level $1-\alpha=0.9$ on the test sets.
% \begin{figure*}[ht]
% \centering
% \subfigure[bio]{\includegraphics[width=0.19\textwidth]{figures/bio_residuals.png}}
% \subfigure[meps19]{\includegraphics[width=0.19\textwidth]{figures/meps_19_residuals.png}}
% \subfigure[bike]{\includegraphics[width=0.19\textwidth]{figures/bike_residuals.png}}
% \subfigure[concrete] {\includegraphics[width=0.19\textwidth]{figures/residuals_concrete.png}}
% \subfigure[star]{\includegraphics[width=0.19\textwidth]{figures/star_residuals.png}}
% \label{fig:toy_oracle}
% \caption{Difference between the PI and the true residuals of the different methods}
% \end{figure*}

We consider two nonconformity scores: mean score $\small \hV(\X, Y) = |Y - \widehat{\mu}(\X)|$ where $\small \widehat{\mu}$ is mean estimate, and quantile score $\small \hV^Q(\X, Y)=\max\big\{\widehat{q}_{\alpha/2}(\X)-Y, Y-\widehat{q}_{1-\alpha/2}(\X)\big\}$ where $\{\widehat{q}_{\alpha/2}, \widehat{q}_{1-\alpha/2} \}$ are quantile estimates. We use a Random Forest as the mean estimate $\widehat{\mu}$ in our experiments. We leave the analysis of different models and the quantile score for the appendix, due to space limitations. We denote $\small C^{m}(\Xtest) = [\widehat{\mu}(\Xtest) \pm q^{m}(\Xtest)]$ the PI of each method $m$, and the oracle PI as $\small C^{\star}(\Xtest) = [\widehat{\mu}(\Xtest) \pm q^\star(\Xtest)]$ where $\small q^\star(\Xtest) = \mathcal{Q}\big(1-\alpha; \; F_{\hV_{n+1}| \Xtest})\big)$.

The simulated data (sim) is defined as: $\X \in [0, 1]^{50}$, $\X_i \sim \mathcal{U}([0, 1])$ for all $i \in [50]$ and $\small Y = \X_1 + \epsilon \times \X_1/(1 +\X_1)$ where $\small \epsilon \sim \mathcal{N}(0, 1)$. In figure \ref{fig:residuals_sim}, we compute the absolute relative distance between the PI of each method and the oracle PI as $\small err_{n+1} = |q^m(\Xtest) - q^\star(\Xtest)|/q^\star(\Xtest)$ showing that our methods are much closer to the oracle PI than its competitors. SLCP and SPLIT are close, but they are less accurate than LCP. Figure \ref{fig:lengths_sim} shows that most methods provide training-conditional coverage or empirical coverage over the test points at nearly $90\%$. Our methods give varied intervals while the others have almost constant intervals. We present further results on simulated data in the appendix that demonstrate the same trend.
% \begin{figure}[ht!]
% \centering
% \subfigure[Lengths] {\includegraphics[width=0.19\textwidth]{figures/sim_1_lengths.png}\label{fig:lengths_toy}}
% \subfigure[AR w.r.t oracle PI]{\includegraphics[width=0.19\textwidth]{figures/sim_2_residuals.png}\label{fig:residuals_toy}}
% % \caption{AR distribution with the oracle PI}
% \end{figure}

The analysis of real-world data is more challenging because we don't have the oracle PI. To evaluate the effectiveness of the methods, we compare the length of the PI $q^m(\Xtest)$ to the true error of the model $\small \hV_{n+1} = |Y_{n+1} - \widehat{\mu}(\Xtest)|$.  Indeed, a larger error of the model should result in a larger PI. Note that if $Y_{n+1}|\Xtest$ does not vary too much then $\small \hV_{n+1} \approx q^\star(\Xtest)$. We denote $\small \widehat{err}_{n+1} = |q^m(\Xtest) - \hV_{n+1}|/\hV_{n+1}$ as the model's fidelity errors. 

Figure \ref{fig:realdata_results} summarizes the results on the 4 real-world datasets. Starting with average coverage (top of the figure), most methods have empirical coverage at nearly exact nominal levels for all datasets. Our methods are slightly lower, which could be explained by the sample splitting used for the PAC interval calibration. Indeed, the bound in theorem \ref{theo:pac_interval} depends on the size of the data and as we split the calibration set in two we lose a bit in statistical efficiency.

The figures on top (\ref{fig:bio_lengths}-\ref{fig:star_lengths}) represent the distribution of the lengths of the PI and the last 4 at the bottom (\ref{fig:bio_residuals}-\ref{fig:star_residuals}) represent the distribution of the fidelity errors of the model $\widehat{err}_{n+1}$. Overall, our methods outperform the others by a significant margin in terms of model's uncertainty fidelity and the adaptiveness of lengths. SLCP does not provide any significant improvement over standard split-CP. This may be due to the fact that it learns the localizer on the residuals of the training set, which may not be representative of the residuals of the calibration and thus may overfit.

While LCP-RF-G and QRF-TC are faster than LCP-RF, their performance are similar. In these datasets, we suspect that the RF localizer is so accurate that it is difficult to distinguish between the Groupwise LCP-RF and the LCP-RF. To demonstrate the significance of the groups defined by the RF weights, we apply the split-CP method by group (SPLIT-G) on the meps19 dataset. Figure \ref{fig:split_g} shows that by using the groups defined by the RF, we are able to improve the PI of split-CP. This illustrates the ability of our methods to enhance the performance of any CP method.
\begin{figure}[ht!]
\centering
\subfigure[lengths] {\includegraphics[width=0.19\textwidth]{figures/meps_19_lengths2.png}\label{fig:split_g_lengths}}
\subfigure[$\widehat{err}_{n+1}$]{\includegraphics[width=0.19\textwidth]{figures/meps_19_residuals2.png}\label{fig:split_g_residuals}}
\caption{Comparisons of SPLIT-G and the others methods} \label{fig:split_g}
\end{figure}
\section{Conclusion}

Our reweighting strategy based on a Random Forest can improve the PI computed using any nonconformity score. This results in more adaptive PI with marginal, training-conditional, and conditional coverage, making Conformal Predictive Intervals more similar to those produced by traditional statistics. This may ease their interpretation in terms of risks and give a clearer relationship between the length of the PI and the uncertainties of a given model $\widehat{\mu}$, thereby allowing for a better understanding of the limitations of $\widehat{\mu}$. %The extension to challenging settings such as distribution shift is a promising.  

\bibliography{example_paper}
\bibliographystyle{icml2023}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Proofs of section 4.1: Localized Conformal Prediction}
The Lemma 4.1, which is the cornerstone of the LCP framework, shows how to achieve marginal coverage by properly selecting the level $\alphat$ of the quantile of the localizer.

\begin{lemma} \label{suptheorem:lcp}
    Let $\alphat$ be the smallest value in $\Gamma = \Big\{ \sum_{j=1}^{k} w_n(\X_i, \X_j): i=1, \dots, n;\; k=1, \dots, n\Big\}$ such that 
    \begin{align}
        \sum_{i=1}^{n+1}\frac{1}{n+1} \mathds{1}_{\hV_i \leq \mathcal{Q}(\alphat;\;  \mathcal{F}_i )} \geq 1 - \alpha,
    \end{align}
\end{lemma}

then $\mathbb{P}\Bigl\{\hV_{n+1} \leq \mathcal{Q}(\alphat; \; \mathcal{F}_{n+1})\Bigl\} \geq 1-\alpha$, or equivalently $\mathbb{P}\Bigl\{\hV_{n+1} \leq \mathcal{Q}(\alphat; \; \mathcal{F})\Bigl\} \geq 1-\alpha$.

It is important to keep in mind that $\alphat$ and $\mathcal{F}_{n+1}$ depends on $\widehat{\mathcal{D}}_{n} = \Big\{\widehat{Z}_1, \dots, \widehat{Z}_{n} \Big\}$ and $(\Xtest, \hV_{n+1})$ where $\widehat{Z}_i = (\X_i, \hV_i)$, but we will not specify them for ease of reading.

\begin{proof}
Let define the event $E_{n+1} = \Big\{\widehat{Z}_1 = z_1, \dots, \widehat{Z}_{n+1} = z_{n+1}\Big\}$, then the exchangeability of the residuals implies that $\hV_{n+1}|E_{n+1}$ is uniform on the set  $\Big\{z_1, \dots,z_{n+1}\Big\}$.
    \begin{align*}
        \mathbb{P}\Big\{ \hV_{n+1} \leq \mathcal{Q}\left(\alphat;\; \mathcal{F}_{n+1}\right) \;\Big| E_{n+1}\Big\} & = \sum_{i=1}^{n+1} P(Z_{n+1} = z_i \;|\; E_{n+1}) \mathds{1}_{v_i \leq \mathcal{Q}(\alphat; \; \mathcal{F}_i) } \\
        & = \sum_{i=1}^{n+1} \frac{1}{n+1} \mathds{1}_{v_i \leq \mathcal{Q}(\alphat; \; \mathcal{F}_i) } \geq 1 - \alpha.
    \end{align*}

By marginalizing over the event $E_{n+1}$, we have $\mathbb{P}\Bigl\{\hV_{n+1} \leq \mathcal{Q}(\alphat; \; \mathcal{F}_{n+1})\Bigl\} \geq 1-\alpha$. Additionally, we can remove the dependence on the unknown residuals $\hV_{n+1}$ using the well-known fact that $\hV_{n+1} \leq \mathcal{Q}(\alphat; \; \mathcal{F}_{n+1}) \iff \hV_{n+1} \leq \mathcal{Q}(\alphat; \; \mathcal{F})$ (see [Lemma 1, \citep{tibshirani2019conformal}] or [Lemma A.1, \citep{guanlocalizer}] for proof). Thus, we also have  $\mathbb{P}\Bigl\{\hV_{n+1} \leq \mathcal{Q}(\alphat; \; \mathcal{F})\Bigl\} \geq 1-\alpha$.
\end{proof}

We refer to the original paper \cite{guanlocalizer} for the proof of Theorem 4.2 and Lemma 4.3.


\section{Proof of section 4.2: Training-conditional of LCP-RF}
In this section, we prove Theorem 4.4 that shows how to correct the LCP-RF approach to have training-conditional coverage.

\begin{theorem}
    Let $\epsilon>0$,  $\alpha - \epsilon >0$ and $\widehat{\alpha} \in T = \{\alpha_0=0, \alpha_1,\dots, \alpha_K=\alpha\}$  s.t.
    \begin{align}
        \sum_{i=1}^{n_2} \frac{1}{n_2} \mathds{1}_{\hV^{2}_{i} \leq  \mathcal{Q}\left( \alphat(\X^{2}_{i}) + \widehat{\alpha};\; \mathcal{F}^{2, \infty}_{i}\right)} \geq 1 - \alpha
    \end{align}

    Then, we have training-conditional coverage with $\delta=K \exp(-2n_2\epsilon^2)$
\begin{align*}
    P^{n_1}\Bigg\{ P \Big\{ \hV_{n+1} \leq \mathcal{Q}\left(\alphat(\X_{n + 1}) + \widehat{\alpha};\; \mathcal{F}_{n+1}^{\infty}\right)\;\Big| D_{n_1}\Big\} \geq 1 - \alpha - \epsilon \Bigg\} \geq 1 - \delta
\end{align*}
\end{theorem}

\textbf{Remark:} We suggest chosing a grid $T \subset [0, \alpha]$ as we observe in practice that $\alphat(\X_{n + 1}) \approx 1 - \alpha$, but the idea remains the same, which is to choose a grid in order to go from $\alphat(\X_{n + 1})$ to 1. Additionally, as $\alphat(\X_{n + 1}) + \widehat{\alpha}$ may be above $1$, we define $\alphat(\X_{n + 1}) + \widehat{\alpha} := \max(\alphat(\X_{n + 1}) + \widehat{\alpha}, 1)$.

\begin{proof}
    Recall that here $\alphat$ and $\mathcal{F}^{\infty}_{n+1}$ is a function of $\widehat{\mathcal{D}}_{n_1} = \Big\{\widehat{Z}_1, \dots, \widehat{Z}_{n_1} \Big\}$ and $\Xtest$ as the RF has been trained on $\widehat{\mathcal{D}}_{n_1}$, but we will not specify $\widehat{\mathcal{D}}_{n_1}$ for ease of reading.
    \begin{align*}
        &P^{n_1}\Bigg\{ P\Big\{ \hV_{n+1} \leq \mathcal{Q}\Big(\alphat(\Xtest) + \widehat{\alpha};\; \mathcal{F}_{n+1}^{\infty}\Big)\;\Big| D_{n_1}\Big\}  \leq 1 - \alpha - \epsilon \Bigg\} \\
        & \leq P^{n_1}\Bigg\{ P\Big\{ \hV_{n+1} \leq \mathcal{Q}\Big(\alphat(\Xtest) + \widehat{\alpha};\; \mathcal{F}_{n+1}^{\infty}\Big)\;\Big| D_{n_1}\Big\}  \leq \sum_{i=1}^{n_2} \frac{1}{n_2} \mathds{1}_{\hV^{2}_{i} \leq  \mathcal{Q}\left( \alphat(\X^{2}_{i}) + \widehat{\alpha};\; \mathcal{F}^{2, \infty}_{i}\right)} - \epsilon \Bigg\} \\
        & = \mathbb{E} \Bigg[ P^{n_1}\Bigg\{ P\Big\{ \hV_{n+1} \leq \mathcal{Q}\Big(\alphat(\Xtest) + \widehat{\alpha};\; \mathcal{F}_{n+1}^{\infty}\Big)\;\Big| D_{n_1}\Big\}  \leq \sum_{i=1}^{n_2} \frac{1}{n_2} \mathds{1}_{\hV^{2}_{i} \leq  \mathcal{Q}\left( \alphat(\X^{2}_{i}) + \widehat{\alpha};\; \mathcal{F}^{2, \infty}_{i}\right)} - \epsilon \Bigg\} \Bigg| \mathcal{D}_{n_1}\Bigg\}  \Bigg] \\
        & \leq \sum_{\alpha \in T} \mathbb{E} \Bigg[ P^{n_1}\Bigg\{ P\Big\{ \hV_{n+1} \leq \mathcal{Q}\Big(\alphat(\Xtest) + \alpha;\; \mathcal{F}_{n+1}^{\infty}\Big)\;\Big| D_{n_1}\Big\}  \leq \sum_{i=1}^{n_2} \frac{1}{n_2} \mathds{1}_{\hV^{2}_{i} \leq  \mathcal{Q}\left( \alphat(\X^{2}_{i}) + \alpha;\; \mathcal{F}^{2, \infty}_{i}\right)} - \epsilon \Bigg\} \Bigg| \mathcal{D}_{n_1}\Bigg\}  \Bigg] \\
    \end{align*}
    Note that conditionally on $\mathcal{D}_{n_1}$, $\sum_{i=1}^{n_2} \frac{1}{n_2} \mathds{1}_{\hV^{2}_{i} \leq  \mathcal{Q}\left( \alphat(\X^{2}_{i}) + \alpha;\; \mathcal{F}^{2, \infty}_{i}\right)}$ is the average of $n_2$ bernoulli-trial with mean $P\Big\{ \hV_{n+1} \leq \mathcal{Q}\Big(\alphat(\Xtest) + \widehat{\alpha};\; \mathcal{F}_{n+1}^{\infty}\Big)\;\Big| D_{n_1}\Big\}$, therefore we can bound the conditional probability by using Hoeffding's inequality. Finally, we have

    \begin{align*}
        & P^{n_1}\Bigg\{ P\Big\{ \hV_{n+1} \leq \mathcal{Q}\Big(\alphat(\Xtest) + \widehat{\alpha};\; \mathcal{F}_{n+1}^{\infty}\Big)\;\Big| D_{n_1}\Big\}  \leq 1 - \alpha - \epsilon \Bigg\} \\
        & \leq \sum_{\alpha \in T} \mathbb{E} \Bigg[ P^{n_1}\Bigg\{ P\Big\{ \hV_{n+1} \leq \mathcal{Q}\Big(\alphat(\Xtest) + \alpha;\; \mathcal{F}_{n+1}^{\infty}\Big)\;\Big| D_{n_1}\Big\}  \leq \sum_{i=1}^{n_2} \frac{1}{n_2} \mathds{1}_{\hV^{2}_{i} \leq  \mathcal{Q}\left( \alphat(\X^{2}_{i}) + \alpha;\; \mathcal{F}^{2, \infty}_{i}\right)} - \epsilon \Bigg\} \Bigg| \mathcal{D}_{n_1}\Bigg\}  \Bigg] \\
        & \leq K \exp(-2 \epsilon^2 n_2)
    \end{align*}
\end{proof}


\section{Proof of Section 4.3: Marginal Coverage of groupwise LCP-RF} \label{sup:group_marginal_coverage}
Here, we show that there is no loss in coverage guarantee when conformalizing by groups. We demonstrate the case of marginal coverage, the groupwise training-conditional is obtained in a similar way.
\begin{theorem}
 Given a partition of the calibration data $\mathcal{D}_n$ in $G_1, \dots, G_L$ and their associated regions $\textbf{R}= \{R_1, \dots, R_L\}$ defined by the weighted adjacency matrix $w_n(\X_i, \X_j)$ of the RF, we denote $R(\X) \in \textbf{R}$ the region where $\X$ falls. Let $\hV(.)$ be a fixed score function.  At $\hV_{n+1} = V$, let define $\alphat(V, R(\Xtest), \Xtest)$ to be the smallest value $ \alphat \in \Gamma$ such that

    \begin{align} \label{eq:lcp}
        \mathlarger{\sum}_{i \in R(\Xtest)} \frac{1}{|\textbf{R}(\Xtest)|+1} \mathds{1}_{\hV_i \leq \mathcal{Q}(\alphat; \; \mathcal{F}^v_i)}  \geq 1 - \alpha.
    \end{align}

    Set $C_V(V_{n+1}) = \Big\{ v: v \leq \mathcal{Q}\Big(\alphat(V, R(\Xtest), \Xtest); \; \mathcal{F}(V)\Big)\Big\}$ and $C(V_{n+1}) = \Big\{ y: \hV(\Xtest, y) \leq \mathcal{Q}\Big(\alphat(V, \mathcal{D}_n, \Xtest); \; \mathcal{F}(V)\Big)\Big\}$, then $P^{n+1}\big(\hV_{n+1} \in C_V(\Xtest)\big) \geq 1 - \alpha \text{ and } P^{n+1}\big(Y_{n+1} \in C(\Xtest)\big) \geq 1 - \alpha$.
\end{theorem}

\begin{proof}
    \begin{align*}
        P^{n+1}\big(\hV_{n+1} \in C_V(\Xtest) \big) & = P^{n+1}\Big( \hV_{n+1} \leq \mathcal{Q} \big(\alphat(\hV_{n+1}, R(\Xtest), \Xtest); \; \mathcal{F}(\hV_{n+1}) \big)\Big) \\
        & = \sum_{l=1}^{L} P(R_l) \; P^{n+1}\Big( \hV_{n+1} \leq \mathcal{Q} \big(\alphat(\hV_{n+1}, R(\Xtest), \Xtest); \; \mathcal{F}(\hV_{n+1}) \big) \;\big|\; \Xtest \in R_l\Big) \\
        & \geq \sum_{l=1}^{L} p(R_l) (1 - \alpha) \\
        & \geq 1 - \alpha
    \end{align*}
\end{proof}

\section{Proof of Section 5: Asymptotic conditional coverage}
Here, we prove the asymptotic conditional coverage of the LCP-RF approach or Theorem 5.4. Our primary contribution is Lemma \ref{lemma:born}, which enables us to control the weights of the RF and, subsequently, to proceed with \cite{guanlocalizer}'s proof.

\begin{theorem} \label{theo:lcp_cond}
    Let $\alphat(v)$ and $C_V(\Xtest)$ define as in Theorem \ref{lemma:lcp}. Under assumptions \ref{prop:assym1}-\ref{prop:assym3}, for all $\epsilon > 0$, we have $\small\lim_{n \rightarrow \infty} P^{n+1}\Big( \hV_{n+1} \in C_V(\Xtest) \; |\;  \Xtest\Big) = 1-\alpha$ and $\small lim_{n \rightarrow \infty} P^{n+1}\Big( \max_v|\alphat(v) - (1-\alpha)| < \epsilon\; |\;  \Xtest\Big) = 1$.
\end{theorem}

The bootstrap step in Random Forest makes its theoretical analysis difficult, which is why it has been replaced by subsampling without replacement in most studies that investigate the asymptotic properties of Random Forests \citep{scornet2015consistency, wager2017estimation, goehry2020random}. To circumvent this issue, we will use Honest Forest as a theoretical surrogate. Honest Forest is a variation of random forest that is simpler to analyze, and \cite{elie2020random} have shown that asymptotically, the original forest and the honest forest are close a.s., thus we can extend the results from the Honest Forest to the original forest.

The main idea is to use a second independent sample $\mathcal{D}_n^\diamond$. We assume that we have a honest forest \citep{wager2017estimation} of the Quantile Regression Forest $\widehat{F}^\diamond(v | \X = \x, \Theta_1, \dots, \Theta_k, \mathcal{D}_n, \mathcal{D}_n^{\diamond})$, which is a random forest that is grown using $\mathcal{D}_n$, but uses another sample $\mathcal{D}_n^{\diamond}$ (independent of $\mathcal{D}_n$ and $\Theta$) to estimate the weights and the prediction. The Honest QRF is defined as:
\begin{equation*}
    F^{\diamond}(r | \X = \x, \Theta_1, \dots, \Theta_k, \mathcal{D}_n,  \mathcal{D}_n^{\diamond}) = \sum_{i=1}^{n+1}  w_n(\x, \X_j^\diamond)) \mathds{1}_{V^{\diamond i} \leq r}  
\end{equation*}
where $\Xtest^\diamond = \Xtest$ and
\begin{equation*}
      w_n(\x, \X_j^\diamond) = \frac{1}{k} \sum_{l=1}^{k}  \frac{\mathds{1}_{\X_i^\diamond \in  A_{n}(\x; \Theta_l, \mathcal{D}_n)}}{N_{n}(\x; \Theta_l, \mathcal{D}_n, \mathcal{D}_n^\diamond)},    
\end{equation*}
and $N^\diamond(A_{n}(\x; \Theta_l)) =N_{n}(\boldsymbol{x}; \Theta_l, \mathcal{D}_n, \mathcal{D}_n^\diamond)$ is the number of observation of $\mathcal{D}_n^\diamond = \{(X_1^\diamond, \hV_1^\diamond) \dots, (X_n^\diamond, \hV_n^\diamond) \}\cup(\Xtest, \hV_{n+1})$ that fall in $A_{n}(\x; \Theta_l, \mathcal{D}_n)$. To ease the notations, we do not write $\Theta_1, \dots, \Theta_k, \mathcal{D}_n,  \mathcal{D}_n^{\diamond}$ if not necessary e.g. we write $\widehat{F}^{\diamond}(r | \x)$ instead of $ \widehat{F}^{\diamond}(r | \X = \x, \Theta_1, \dots, \Theta_k, \mathcal{D}_n,  \mathcal{D}_n^{\diamond})$.


The following Lemma is the key element to prove Theorem \ref{theo:lcp_cond} for Random Forest Localizer. 

\begin{lemma} \label{lemma:born}
    Let define $R_i = \sum_{j = 1, j \neq i}^{n} w_n(\X_i^\diamond, \X_j^\diamond) \big( \mathds{1}_{\hV_j^\diamond < \hV_i} - F(\hV_i | \X^\diamond_j)\big)$ and $I_i = \sum_{j = 1, j \neq i}^{n} w_n(\X_i^\diamond, \X_j^\diamond) F(\hV_i | \X^\diamond_j)$ for all $i=1, \dots, n$, then for any $\epsilon >0$, under assumptions \ref{prop:assym1}-\ref{prop:assym3}, we have
    \begin{align}
    & P(|R_i|> \epsilon)  \leq  2(1 + 24 k (n+1)^{2p})\exp \left({\frac{K \ln(n)^{\beta}}{576 \sqrt{n}} - \frac{ \epsilon K \ln(n)^{\beta}}{24}}\right) \\
    & I_i \in \Big[F(\hV_i|\X_i) - v(n) - \frac{2 k}{K \sqrt{n} \ln(n)^\beta}, \quad F(\hV_i|\X_i) + v(n)+\frac{2 k}{K \sqrt{n} \ln(n)^\beta}\Big]\\
\end{align}
where $v(n)$ is a sequence so that $v(n) \xrightarrow[n \rightarrow \infty]{} 0$.

    \begin{proof}
        First, let rewrite $R_i$ as $R_i = \sum_{j = 1, j \neq i}^{n} w_n(\X_i^\diamond, \X_j^\diamond) \big( \mathds{1}_{\hV_j^\diamond < \hV_i} - F(\hV_i | \X_j^\diamond)\big) = \sum_{j = 1, j \neq i}^{n} w_n(\X_i^\diamond, \X_j^\diamond) H_j^\diamond$  where $H_j^\diamond$ is bounded by $1$ and $E[H_j^\diamond | \X_j^\diamond] = 0$. Then, let $\epsilon>0$

        \begin{align*}
    P(R_i > \epsilon) & \leq e^{-t\epsilon} \; \mathbb{E}[e^{t R_i}] \\
    & \leq e^{-t\epsilon} \; \mathbb{E}\left[ \prod_{j=1}^{n}  \mathbb{E} \left[e^{t   w_n(\X_i^\diamond, \X_j^\diamond) H_j^\diamond} | \Theta_1, \dots, \Theta_k, \mathcal{D}_n, \X^\diamond_i \dots, \X^\diamond_{n}, \Xtest \right] \right] \\ 
    & \leq  e^{-t\epsilon} \; \mathbb{E}\left[ \prod_{i=j}^{n}  e^{\frac{t^2}{2} w_n(\X_i^\diamond, \X_j^\diamond)^2} \right]
\end{align*}
The last inequality comes from the fact that  $w_n(\X_i^\diamond, \X_j^\diamond)$ is a constant given $\Theta_1, \dots, \Theta_k, \mathcal{D}_n, \X^\diamond_i \dots, \X^\diamond_{n}, \Xtest$, and as $H_j^\diamond$ is bounded by 1 with $E[H_j^\diamond | \X_j^\diamond] = 0$, we used the following inequality: If $|X| \leq 1$ a.s and $\mathbb{E}[X] = 0$, then $\mathbb{E}[e^{t X}] \leq \mathbb{E}[e^{\frac{t^2}{2}}]$.

By using assumption \ref{prop:assym3}, let $K > 0$ be such that $\forall l \in [k]$, $N(A_{n}(\X_i; \Theta_l)) \geq \frac{K \sqrt{n} \ln(n)^\beta}{2}$ a.s., then we have $\Gamma(l) = \{ N^\diamond(A_{n}(\X_i; \Theta_l)) \leq \frac{K \sqrt{n} \ln(n)^\beta}{2}\} \subset \{ |N(A_{n}(\X_i; \Theta_l))-N^\diamond(A_{n}(\X_i; \Theta_l))| \geq \frac{K \sqrt{n} \ln(n)^\beta}{2} \}$. Thus, using Lemma \ref{lemma:vapnik_boostrap}, we have that $\mathbb{P}(\Gamma(l)) \leq 24(n+1)^{2 p}\exp(-\frac{-K^2 (\ln(n)^{2\beta})}{1152})$. 

We have
\begin{align*}
    \sum_{j=1}^{n} w_n(\X_i^\diamond, \X_j^\diamond)^2 & = 
    \sum_{j=1}^{n}  \frac{w_n(\X_i^\diamond, \X_j^\diamond)}{k} \left( \sum_{l=1}^{k} \frac{\mathds{1}_{\X_j^\diamond \in A_{n}(\X_i; \Theta_l, \mathcal{D}_m)}}{N^\diamond(A_{n}(\X_i; \Theta_l))} (\mathds{1}_{\{ \Gamma(l)\}} + \mathds{1}_{\{ \Gamma(l)^c\}})\right) \\
    & \leq \sum_{j=1}^{n}  w_n(\X_i^\diamond, \X_j^\diamond)^2 \left( \frac{2}{K \sqrt{n} \ln(n)^\beta} + \frac{1}{k} \sum_{l=1}^{k} \mathds{1}_{\X_j^\diamond \in A_{n}(\X_i; \Theta_l, \mathcal{D}_m)} \mathds{1}_{\{ \Gamma(l)\}}\right)
\end{align*}

So that, \begin{align*}
    P(R_i > \epsilon) & \leq \exp(-t\epsilon + \frac{t^2}{K \sqrt{n} \ln(n)^\beta}) \mathbb{E}\left[\exp\left(\frac{t^2}{2} \mathds{1}_{\cup_{l=1}^{k} \Gamma(l)}\right)\right] \\
    & \leq \exp(-t\epsilon + \frac{t^2}{K \sqrt{n} \ln(n)^\beta}) \times \left( 1 + e^{\frac{t^2}{2}} \sum_{l=1}^{k} \mathbb{P}(\Gamma(l)) \right) \\
    & \leq \exp(-t\epsilon + \frac{t^2}{K \sqrt{n} \ln(n)^\beta}) \times \left( 1 + 24 k (n+1)^{2p}\exp \left({\frac{t^2}{2} - \frac{K^2 \ln(n)^{2\beta}}{1152}}\right)  \right)
\end{align*}
Taking $t^2 = \frac{K^2 \ln(n)^{2\beta}}{576}$ leads to 
\begin{align*}
    P(R_i> \epsilon) & \leq  (1 + 24 k (n+1)^{2p})\exp \left({\frac{K \ln(n)^{\beta}}{576 \sqrt{n}} - \frac{ \epsilon K \ln(n)^{\beta}}{24}}\right) 
\end{align*}

We obtain the same bound for $\mathbb{P}(R_i \leq -\epsilon) = \mathbb{P}(-R_i > \epsilon)$, then by using assumption \ref{prop:assym2}, item 1., $k = \mathcal{O}(n^\alpha)$ so that the right term is finite, we conclude by Borel cantelli that $|W_n|$ goes to 0 a.s and 

\begin{align*}
    P(|R_i|> \epsilon) & \leq  2(1 + 24 k (n+1)^{2p})\exp \left({\frac{K \ln(n)^{\beta}}{576 \sqrt{n}} - \frac{ \epsilon K \ln(n)^{\beta}}{24}}\right) 
\end{align*}

Now, we consider $I_i$. By assumption \ref{prop:assym2}, we have $\forall x \in \mathbb{R}^d, \forall y \in \mathbb{R}, \sup_{\boldsymbol{z} \in A_n(\boldsymbol{x}; \;\Theta_l)} |F(r | \boldsymbol{\boldsymbol{z}}) - F(r|\boldsymbol{x})| \overset{a.s}{\to} 0$ then we can assume that there exists a sequence $v(n) \rightarrow 0$ s.t.  
\begin{align}
    \forall x \in \mathbb{R}^d, \forall y \in \mathbb{R}, \sup_{\boldsymbol{z} \in A_n(\boldsymbol{x}; \;\Theta_l)} |F(r | \boldsymbol{\boldsymbol{z}}) - F(r|\boldsymbol{x})| \leq v(n)
\end{align}

Therefore, 

\begin{align*}
    |I_i - F(\hV_i | \X_i)| & = \big|\sum_{j = 1, j \neq i}^{n} w_n(\X_i^\diamond, \X_j^\diamond)\big( F(\hV_i | \X_j^\diamond) - F(\hV_i | \X_i) \big) - w_n(\X_i, \Xtest) F(\hV_i | \X_j^\diamond)\big|\\
    & \leq \sum_{j = 1, j \neq i}^{n} w_n(\X_i^\diamond, \X_j^\diamond) \big|F(\hV_i | \X_j^\diamond) - F(\hV_i | \X_i)\big| + w_n(\X_i, \Xtest) \\
    & \leq \sum_{j = 1, j \neq i}^{n} w_n(\X_i^\diamond, \X_j^\diamond)  \sup_{\boldsymbol{z} \in A_n(\X_i; \Theta_l, \mathcal{D}_n)}\big|F(\hV_i | \boldsymbol{z}) - F(\hV_i | \X_i)\big| + \frac{2 k}{K \sqrt{n} \ln(n)^\beta} \\
    & \leq v(n) + \frac{2 k}{K \sqrt{n} \ln(n)^\beta}
\end{align*}    
We use the fact that by assumption \ref{prop:assym2}, we can lower bound the weights of the forest since $N(A_{n}(\X_i; \Theta_l)) \geq \frac{K \sqrt{n} \ln(n)^\beta}{2}$, we have $ w_n(\X_i, \Xtest) \leq \frac{2 k}{K \sqrt{n} \ln(n)^\beta}$.
\end{proof}
\end{lemma}

The following Lemma \ref{lemma:vapnik_boostrap} allows for control of the weights of the Honest Forest.

\begin{lemma} \label{lemma:vapnik_boostrap}
Consider $\mathcal{D}_n, \mathcal{D}_n^\diamond,$ two independent datasets of independent n samples of $(\boldsymbol{X}, Y)$. Build a tree using $\mathcal{D}_n$ with bootstrap and bagging procedure driven by $\Theta$. As before,  $N(A_{n}(\x; \Theta_l))$ is the number of bootstrap observations of $\mathcal{D}_n$ that fall into $A_{n}(\x; \Theta_l, \mathcal{D}_n)$ and $N^\diamond(A_{n}(\x; \Theta_l))$ is the number of observations of  $\mathcal{D}_n^\diamond$ that fall into $A_{n}(\x; \Theta_l, \mathcal{D}_n)$. Then:
\begin{align}
    \forall \epsilon >0, \quad \mathbb{P}\left(\labs N(A_{n}(\x; \Theta_l)) - N^\diamond(A_{n}(\x; \Theta_l)) \rabs > \epsilon\right) \leq 24(n+1)^{2p}e^{-\epsilon^2/288n}
\end{align}
\end{lemma}

See the proof in \citep{elie2020random}, Lemma 5.3.


\subsection{Proof of theorem \ref{theo:lcp_cond}}

As in \cite{guanlocalizer}, we first prove that $\alphat(v) \rightarrow 1 - \alpha$ and then show that the resulting PI has coverage rate that achieves the desired level $1 - \alpha$ for any $v$.

\begin{proof}
Let define $R_i = \sum_{j = 1, j \neq i}^{n} w_n(\X_i^\diamond, \X_j^\diamond) \big( \mathds{1}_{\hV_j^\diamond < \hV_i} - F(\hV_i | \X^\diamond_j)\big)$ and $I_i = \sum_{j = 1, j \neq i}^{n} w_n(\X_i^\diamond, \X_j^\diamond) F(\hV_i | \X^\diamond_j)$ for all $i=1, \dots, n$, and 
    \begin{align*}
        J_i(v, \alphat) := \big\{ \hV_i \leq \mathcal{Q}(\alphat; \; \mathcal{F}^v_i) \big\} = \big\{ \alphat > \sum_{j \leq n: \hV_j < V_i} w_n(\Xd_i,\Xd_j) + w_n(\Xd_i, \Xd_{n+1})\mathds{1}_{v < \hV_i}\big\}
    \end{align*}
which is the event in the sum of equation \ref{eq:lcp} of Theorem \ref{lemma:lcp}. Let consider the lower term in $J_i(v, \alphat)$, then we have
\begin{align} \label{eq:ji_born}
     R_i + I_i - w_n(\Xd_i, \Xd_{n+1}) \leq R_i + I_i \leq \sum_{j \leq n: \hV_j < V_i} w_n(\Xd_i,\Xd_j) + w_n(\Xd_i, \Xd_{n+1})\mathds{1}_{v < \hV_i}  \leq R_i + I_i + w_n(\Xd_i, \Xd_{n+1})
\end{align}

Let $\epsilon > 0$, and denote $ G = \big\{i \in \{1, \dots, n\}: |R_i| \leq \epsilon \big\}$. By lemma \ref{lemma:born}, we have $ I_i \in \Big[F(\hV_i|\X_i) - v(n) - \frac{2 k}{K \sqrt{n} \ln(n)^\beta}, \quad F(\hV_i|\X_i) + v(n)+\frac{2 k}{K \sqrt{n} \ln(n)^\beta}\Big]$. Using the upper bound of equation \ref{eq:ji_born}, for any $i \in G$, we have
\begin{align}
    J^{down}_i(\alphat) := \big\{\alphat >  F(\hV_i|\X_i) + \epsilon + v(n) + \frac{4 k}{K \sqrt{n} \ln(n)^\beta}\big\} \subseteq J_i(v, \alphat) 
\end{align}
and similary with the lower bound of equation \ref{eq:ji_born}, we have 

\begin{align}
    J^{up}_i(\alphat) := \big\{\alphat >  F(\hV_i|\X_i) - \epsilon - v(n) - \frac{4 k}{K \sqrt{n} \ln(n)^\beta}\big\} \supseteq J_i(v, \alphat) 
\end{align}
Hence, we can upper and lower bound the left side of the equation in \ref{lemma:lcp} using $J^{up}_i(\alphat)$ and $J^{down}_i(\alphat)$.

\begin{align}
    & \frac{1}{n+1} \sum_{i=1}^{n+1} J_i(v, \alphat) \leq \frac{1}{n+1} + \frac{1}{n+1} \sum_{i \in G}  J^{up}_i(\alphat) + \frac{|\Bar{G}|}{n+1} \label{eq:1}\\
    & \frac{1}{n+1} \sum_{i=1}^{n+1} J_i(v, \alphat) \geq \frac{1}{n+1} \sum_{i \in G}  J^{down}_i(\alphat)\label{eq:2} 
\end{align}

$W_i = F(\hV_i | \X_i)$ is an i.i.d. uniform distribution as $\hV | \X_i$ is a continuous random variable. Therefore, on the event $\{ |\Bar{G}| = 0 \}$, if $\alphat$ satistfy the marginal coverage of equation \ref{eq:lcp} of Theorem \ref{lemma:lcp}, then 

\begin{itemize}
    \item By equation \ref{eq:1}, we have

    \begin{align}
        \frac{1}{n+1}\Big(1 + \sum_{i=1}^{n} J^{up}_i(\alphat)\Big) \geq 1 - \alpha \implies \alphat \geq \mathcal{Q}\Big( \frac{n+1}{n}(1 - \alpha) - \frac{1}{n}; \; \frac{1}{n} \sum_{i=1}^{n} W_i\Big) - \epsilon - v(n) - \frac{4 k}{K \sqrt{n} \ln(n)^\beta}
    \end{align}
    The implication cames from the fact that $\frac{1}{n+1}\Big(1 + \sum_{i=1}^{n} J^{up}_i(\alphat)\Big) \geq 1 - \alpha$ implies that at lest $\lceil (n+1)(1-\alpha)\rceil - 1$ of the $J^{up}_i(\alphat) := \big\{\alphat >  F(\hV_i|\X_i) - \epsilon - v(n) - \frac{4 k}{K \sqrt{n} \ln(n)^\beta}\big\}$ is true. Assume that $J^{up}_i(\alphat)$ is true, then replacing $W_i = F(\hV_i|\X_i)$ by the order statistics $W_{(\lceil (n+1)(1-\alpha)\rceil -1)}$ should also satisfy the condition by definition. Note that $ \mathcal{Q}\Big( \frac{n+1}{n}(1 - \alpha) - \frac{1}{n}; \; \frac{1}{n} \sum_{i=1}^{n} W_i\Big) = W_{(\lceil (n+1)(1-\alpha)\rceil -1)}$.
    \item Similary, with equation \ref{eq:2}, we have

    \begin{align}
        \frac{1}{n+1}\sum_{i=1}^{n} J^{down}_i(\alphat) \geq 1 - \alpha \implies \alphat \geq \mathcal{Q}\Big( \frac{n+1}{n}(1 - \alpha); \; \frac{1}{n} \sum_{i=1}^{n} W_i\Big) + \epsilon + v(n) + \frac{4 k}{K \sqrt{n} \ln(n)^\beta}
    \end{align}
\end{itemize}
The maximal deviation between two weights of the forest is $\frac{4 k}{K \sqrt{n} \ln(n)^\beta}$, then there exists $C$ s.t

\begin{align}
    \alphat \leq \mathcal{Q}\Big( \frac{n+1}{n}(1 - \alpha); \; \frac{1}{n} \sum_{i=1}^{n} W_i\Big) + \epsilon + v(n) + \frac{4 k C}{K \sqrt{n} \ln(n)^\beta}
\end{align}
Therefore, on the event $\{ |\Bar{G}| = 0\}$, we have
\begin{align}
   \mathcal{Q}\Big( \frac{n+1}{n}(1 - \alpha) - \frac{1}{n}; \; \frac{1}{n} \sum_{i=1}^{n} W_i\Big) - \epsilon - v(n) - \frac{4 k}{K \sqrt{n} \ln(n)^\beta} \leq  \alphat \leq \mathcal{Q}\Big( \frac{n+1}{n}(1 - \alpha); \; \frac{1}{n} \sum_{i=1}^{n} W_i\Big) + \epsilon + v(n) + \frac{4 k C}{K \sqrt{n} \ln(n)^\beta}
\end{align}

In addition, let $\epsilon^\prime >0$ and consider the event $H =\Big\{ \sup_t |\mathcal{Q}(t;\; \sum_{i=1}^{n} W_i) - t| \leq \epsilon^\prime\Big\}$, on this even we have:


\begin{align} 
   (1 - \alpha) + \frac{1}{n}(1 - \alpha) - \frac{1}{n}  - \epsilon^\prime - \epsilon - v(n) - \frac{4 kC}{K \sqrt{n} \ln(n)^\beta} \leq  \alphat \leq (1 - \alpha) + \frac{1}{n}(1 - \alpha) + \epsilon^\prime + \epsilon + v(n) + \frac{4 k C}{K \sqrt{n} \ln(n)^\beta}
\end{align}
Then, there exists C and $\epsilon$ s.t.
\begin{align}\label{eq:816}
   (1 - \alpha) - \epsilon - v(n) - \frac{4 k C}{K \sqrt{n} \ln(n)^\beta} \leq  \alphat \leq (1-\alpha)  + \epsilon + v(n) + \frac{4 k C}{K \sqrt{n} \ln(n)^\beta}
\end{align}
We can simplify the equation \ref{eq:816} as
\begin{align}
    |\alphat - (1-\alpha)| \leq \epsilon + v(n) + \frac{4 k C}{K \sqrt{n} \ln(n)^\beta}
\end{align}
Finally, we have

\begin{align*}
 P\Big( |\alphat - (1-\alpha)| > \epsilon + v(n) + \frac{4 k C}{K \sqrt{n} \ln(n)^\beta}\big) & \leq P(\Bar{G}) + P(\Bar{H}) 
 % & \leq 2 \exp(-2n\epsilon^\prime) + P(\exists i \in \{1,\dots, n\}: |R_i| > \epsilon) \\
 % & \leq 2 \exp(-2n\epsilon^\prime) + n \times  2(1 + 24 k (n+1)^{2p})\exp \left({\frac{K \ln(n)^{\beta}}{576 \sqrt{n}} - \frac{ \epsilon K \ln(n)^{\beta}}{24}}\right)\\
 % & \rightarrow 0
\end{align*}
Using DKW inequality \citep{massart1990tight} for $\Bar{H}$, and union bound for $\Bar{G}$, we have
\begin{align*}
    & P(\Bar{H}) = P(\sup_t |\mathcal{Q}(t;\; \frac{1}{n}\sum_{i=1}^{n} W_i) - t| > \epsilon^\prime) \leq 2 \exp(-2n\epsilon^{\prime 2}) \\
    & P(\Bar{G}) = P(\exists i \in \{1,\dots, n\}: |R_i| > \epsilon)  \leq n \times  2(1 + 24 k (n+1)^{2p})\exp \left({\frac{K \ln(n)^{\beta}}{576 \sqrt{n}} - \frac{ \epsilon K \ln(n)^{\beta}}{24}}\right)
\end{align*}
Consequently, we have $P\Big( |\alphat - (1 - \alpha)| > \epsilon + v(n) + \frac{4 k C}{K \sqrt{n} \ln(n)^\beta}\big) \xrightarrow[n \rightarrow \infty]{} 0$ with $\epsilon + v(n) + \frac{4 k C}{K \sqrt{n} \ln(n)^\beta} \xrightarrow[n \rightarrow \infty]{} 0$ which conclude our proof. 

Now, let's prove that $\lim_{n \rightarrow \infty} P^{n+1}\Big( \hV_{n+1} \in C_V(\Xtest) \; |\;  \Xtest\Big) = 1-\alpha$. 

By definition, we have 
\begin{align} \label{eq:3}
\hV_{n+1} \leq \mathcal{Q}(\alphat; \; \mathcal{F}) \iff \sum_{i=1}^n w_n(\Xtest, \Xd_i) \mathds{1}_{\hV_i < \hV_{n+1}} = I_{n+1} + R_{n+1} < \alphat.
\end{align}

Define $G = \{|R_{n+1}| \leq \epsilon \}$ with $\epsilon=\frac{1}{n}$. On the event $G$, we can lower and upper bound the left side of equation \ref{eq:3} using Lemma \ref{lemma:born} as above, then we have: 

\begin{align}
    & I_{n+1} + R_{n+1} \leq F(\hV_{n+1}|\Xtest) + v(n) + \frac{4 k C}{K \sqrt{n} \ln(n)^\beta} + \epsilon \\
    & I_{n+1} + R_{n+1} \geq F(\hV_{n+1}|\Xtest) - v(n) - \frac{4 k C}{K \sqrt{n} \ln(n)^\beta} - \epsilon
\end{align}

Since $F(\hV_{n+1}|\Xtest)$ is an i.i.d uniform distribution, and $P(\Bar{G}) \rightarrow 0$, we have
\begin{align}
    & P(I_{n+1} + R_{n+1} < \alpha_t) \leq \alphat + v(n) + \frac{4 k C}{K \sqrt{n} \ln(n)^\beta} + \epsilon + P(\Bar{G}) \rightarrow \alphat\\
    & P(I_{n+1} + R_{n+1} < \alpha_t) \geq \alphat - v(n) - \frac{4 k C}{K \sqrt{n} \ln(n)^\beta} - \epsilon \rightarrow \alphat
\end{align}

As we have shown that $\alphat \rightarrow 1 - \alpha$ for any $v$ in probability, the LCP-RF achieve the asymptotic conditional coverage at level $1 - \alpha$.
\end{proof}

\section{Additional experiments}
The github repo for reproducing the results is: \href{https://github.com/salimamoukou/ACPI}{github.com/salimamoukou/ACPI}.
\subsection{Real-world datasets}
In this section we present additional experiments on real-world datasets. First, we show the lengths and residuals of the PI when $\widehat{\mu}$ is a linear model with $\hV(\X, Y) = |Y - \widehat{\mu}(\X)|$ on star and bike datasets.

\begin{figure*}[ht!]
\centering
\subfigure[bike (lengths)]{\includegraphics[width=0.3\textwidth]{figures/bike_linear_lenghts.png}\label{fig:bike_lengths_lin}}
\subfigure[bike ($\widehat{err}_{n+1}$)]{\includegraphics[width=0.3\textwidth]{figures/bike_linear_residuals.png} \label{fig:bike_residual_lin}}
\end{figure*}

Now, we compute the experiment above using quantile score $\small V(\X, Y, \{\widehat{q}_{\alpha/2}, \widehat{q}_{1-\alpha/2} \})=\max\big(\widehat{q}_{\alpha/2}(\X)-Y, Y-\widehat{q}_{1-\alpha/2}(\X)\big)$. We first estimate $\{\widehat{q}_{\alpha/2}, \widehat{q}_{1-\alpha/2} \}$ using quantile linear regression \citep{chernozhukov2010quantile}, then we use Quantile Regression Forest. Note that in this case, split-CP corresponds to Conformalized Quantile Regression \citep{romano2019conformalized}.  

\begin{figure*}[ht!]
\centering
\subfigure[star (lengths) - QLR]{\includegraphics[width=0.22\textwidth]{figures/quantile_star_linear_lengths.png}\label{fig:star_lql}}
\subfigure[star (residuals) - QLR]{\includegraphics[width=0.22\textwidth]{figures/quantile_star_linear_residuals.png} \label{fig:star_lqr}}
\subfigure[bike (lengths) - QLR]{\includegraphics[width=0.22\textwidth]{figures/quantile_bike_linear_lengths.png}}
\subfigure[bike (residuals) - QLR]{\includegraphics[width=0.22\textwidth]{figures/quantile_bike_linear_residuals.png}}
% \subfigure[concrete (lengths)]{\includegraphics[width=0.19\textwidth]{figures/lengths_concrete.png}}
\caption{Lengths and errors distribution of quantile score using Quantile Linear Regression (QLR)}
\end{figure*}

We also compute the quantile score using Quantile Regression Forest in the figure below.

\begin{figure*}[ht!]
\centering
\subfigure[star (lengths) - QRF]{\includegraphics[width=0.22\textwidth]{figures/quantile_star_random_forest_lengths.png}\label{fig:star_lql}}
\subfigure[star (residuals) - QRF]{\includegraphics[width=0.22\textwidth]{figures/quantile_star_random_forest_residuals.png} \label{fig:star_lqr}}
\subfigure[bike (lengths) - QRF]{\includegraphics[width=0.22\textwidth]{figures/quantile_bike_random_forest_lengths.png}}
\subfigure[bike (residuals) - QRF]{\includegraphics[width=0.22\textwidth]{figures/quantile_bike_random_forest_residuals.png}}
% \subfigure[concrete (lengths)]{\includegraphics[width=0.19\textwidth]{figures/lengths_concrete.png}}
\caption{Lengths and errors distribution of quantile score using Quantile Random Forest (QRF)}
\end{figure*}
All these figures show that the Random Forest Localizer performs much better than the other methods. 

% \subsection{Simulated datasets}
% Here, we show an additional example on 1-dimensional simulated data where $Y = 2 \sin(X)^2 + 0.1 + 0.15\times X \times \mathcal{N}(0, 1)$. We use mean score with a Random Forest.

% \begin{figure*}[ht!]
% \centering
% \subfigure[]{\includegraphics[width=0.3\textwidth]{figures/SLCP_random_forest_simulation_2.pdf} \label{fig:star_lqr}}
% \subfigure[]{\includegraphics[width=0.3\textwidth]{figures/LCP_random_forest_simulation_2.png}}
% \subfigure[]{\includegraphics[width=0.3\textwidth]{figures/LCP-RF_random_forest_simulation_2.pdf}}
% \subfigure[]{\includegraphics[width=0.3\textwidth]{figures/LCP-RF-Group_random_forest_simulation_2.pdf}}
% \subfigure[]{\includegraphics[width=0.3\textwidth]{figures/QRF_train_random_forest_simulation_2.pdf}\label{fig:star_lql}}
% \caption{Lengths and errors distribution of quantile score using Quantile Random Forest (QRF)}
% \end{figure*}
\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
