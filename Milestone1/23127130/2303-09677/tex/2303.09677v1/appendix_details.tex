\section{Implementation and Practical Tips}
\label{suppl:details}

\subsection{Data augmentation and optimization hyper-parameters}
\paragraph{Handcrafted data augmentations.} We used the VISSL library~\citep{goyal2021vissl} which relies on Torchvision transformations\footnote{\url{https://pytorch.org/vision/stable/transforms.html}}, and considered the following data augmentation strategies:
\begin{itemize}
    \item \underline{Hflip}: Random horizontal flipping applied with 50\% probability.
    \item \underline{RRCrop}: Random resized cropping. The location of the crop is sampled uniformly based on the sampled crop size, which is in the range (0.08, 1). When adopted in our experiments, we apply it to all images in a batch.
    \item \underline{RAug} = RandAugment + Color Jittering (CJ) + Random erasing (RE): When enabled,  RandAugment~\citep{cubuk_randaugment_2020-2} is applied to all images with magnitude $9.0\pm0.5$ and increasing distortion severity for higher magnitude values. At each iteration RandAugment randomly chooses two types of distortions. CJ distorts brightness, contrast, saturation, and hue, each with probability 0.4. RE is applied with probability 0.25, erasing a rectangle of size sampled from (0.02, 0.33).
    \item \underline{CutMixUp}. CutMix~\citep{yun_cutmix_2019} and MixUp~\citep{zhang_mixup_2017} are never applied simultaneously; there is a 0.5 probability of choosing one or the other at each iteration. Note that Mixup is applied 80\% of the time when selected. As previously mentioned, we adopt label smoothing of 0.1 to ease convergence when using CutMixUp.  
\end{itemize}

\paragraph{Optimization hyper-parameters.} In Table~\ref{suppl:tab:in1k_training}, we list the optimization hyper-parameters explored for supervised training on ImageNet. Note that for all supervised experiments, we optimized the multi-class cross-entropy loss.

\begin{table}[ht]
    \caption{Training hyper-parameters for supervised training on ImageNet.}
    \label{suppl:tab:in1k_training}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{@{}lcccccc@{}}
    \toprule
    Model & Optimizer & Epochs & Learning rate (LR) & LR scheduler & LR scaling & Weight decay \\%& Hflip & RRCrop & RAug & Col. Jit. & RErase & CutMixUp \\
    \midrule
    ResNet-50 & Mom. SGD%
    & 105 & \{(5, 2, 1)e-1, (5, 1)e-2, 5e-3\} & Step(30,60,90,100) & Lin-256 & \{(1, 5)e-5, (1, 5)e-4, 1e-3\} \\%& .5 & (0.08, 1.0) & / & / & / & / \\ 
     ResNet(-101, -152, 50W2) & Mom. SGD%
    & 105 & 1e-1 & Step(30,60,90,100) & Lin-256 & 1e-4 \\%& .5 & (0.08, 1.0) & / & / & / & / \\ 
    DeiT-B & AdamW%
   & 100/300 & 1/5e-4 & Lin + Cosine & Lin-512 & 1e-1/5e-2 \\%& .5 & (0.08, 1.0) & magn. (9, .5) & .4 & .25, (0.02, 0.33) & .5, CM(.8) MU(1.) \\
    \bottomrule
    \end{tabular}
    }
\end{table}

\subsection{Implementation details}

\paragraph{Fixing the number of IC-GAN-augmented datapoints.} Variable batch size can cause unexpected breaks in GPU-accelerated computations, mostly due to GPU memory pre-allocation. To avoid this phenomenon, we fix the number of \icgan-augmented images in a batch to be \texttt{ceil(batch\_size * p\_G)}.

\paragraph{Computational overhead of \ours.} Adding \ours to the training recipe requires some additional space and time for the \icgan generation. For instance, in terms of space, $\sim$11GB of a single GPU memory are required to generate a batch of 64 images at $256\times 256$ resolution. In terms of time, we noticed that training ResNets with \ours and $p_G=1.0$ doubles the training time, whereas for $p_G=0.5$ the time requirement increases by roughly 50\%. However, we did not take advantage of half-precision computations nor of any other inference-only trick like \texttt{jit} scripting in PyTorch. We hypothesize that exploring such optimizations might significantly reduce the computational overhead of \ours.

\paragraph{Pre-computing dataset embeddings.} The \icgan generation step requires feature representation of the conditioning images. In order to reduce the computation needed during the training, we compute the embeddings of the entire training dataset in advance, and store them in an array which is loaded into memory at the beginning of the training. 

\paragraph{Hardware used for experiments.} For most of the experiments, we performed distributed training using cluster nodes with 8 Nvidia V100 GPUs with 32GB memory. We changed the number of nodes based on the training model and the desired batch size -- e.g., 1 node for ResNets, 4 for DeiT-B, and 8 for SwAV.












