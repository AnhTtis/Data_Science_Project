\section{Conclusions}


We have studied the potential of Instance-Conditioned GAN (\icgan) as a data augmentation technique in state-of-the-art training recipes for visual representation learning. 
Specifically, we have presented \ours, a data augmentation module which leverages the generations of \allicgan and integrates them seamlessly with standard handcrafted data augmentation recipes. 
We have validated \ours in the context of image classification, leveraging supervised learning with ResNets~\citep{he_deep_2016} and DeiT-B~\citep{touvron_training_2021}, as well as self-supervised learning with SwAV~\citep{caron_unsupervised_2020}. The results of this validation have unveiled a beneficial impact of \ours, especially for higher capacity networks and when coupling \allicgan augmentations with soft hand-crafted augmentation strategies, suggesting \ours may act as an implicit regularizer for the models. Additionally, we have found that the representations learned when training models with \ours are more robust when transferred to unseen datasets and more invariant across variations in instance and viewpoint, as a byproduct of augmenting the dataset with generated images obtained with \allicgan. 
Moreover, with a per-class stratification of the results, we have discovered a correlation between per-class performance and generated per-class image quality. These findings hint at two future directions to improve the effect of \ours: increasing generation quality for the classes which are poorly modeled by \allicgan and to tune the \allicgan augmentations per class. 
Finally, in the case of more aggressive data augmentation techniques for which \ours does not provide an improvement over the baselines, such as CutMixUp or multi-crop, we hypothesize that those augmentation recipes already result in strong image variations, and consequently, combining those with \allicgan generations out-of-the-box through \ours may cause an over-regularization of the training. 


To conclude, we have shown that current state-of-the-art large capacity models can be improved using instance conditioned generative models such as \icgan in conjunction with hand-crafted data augmentation techniques. 
We further hypothesize that by boosting the quality and diversity of instance conditioned samples, models may eventually stop relying on hand-crafted data augmentation techniques altogether, and instead move towards completely data-driven augmentation schemes to obtain infinitely many realistic augmented samples.\looseness-1
