\section{Methodology}
\label{sec:method}

\subsection{Review of Instance-Conditioned GAN (IC-GAN)}
\label{ssec:met:bg}

Instance-conditioned GAN (IC-GAN)~\citep{casanova_instance-conditioned_2021} is a  conditional generative model that synthesizes high quality and diverse images that resemble an input image used to condition the model. The key idea of \icgan is to model the data distribution as a mixture of overlapping and fine-grained data clusters, defined by each datapoint -- or ``instance'' -- and the set of its nearest neighbors. Training \icgan requires access to a dataset $\mathcal{D} = \{\x_i\}_{i=1}^N$ with $N$ datapoints and a pre-trained feature extractor $E_\phi$ parameterized by $\phi$. The pre-trained feature extractor is used to extract embedded representations $\mathbf{h}_i = E_\phi(\x_i)$. Next, a set of nearest neighbors $\mathcal{A}_i$, with cardinality $k$, is computed using the cosine similarity in the embedded representation space. The \icgan generator network, $G_\psi$, parameterized by $\psi$, takes as input an embedded representation $\mathbf{h}$ together with a Gaussian noise vector $\z \sim \mathcal{N}(0,I)$, and generates a synthetic image $\tilde\x = G_\psi(\z, \mathbf{h})$. IC-GAN is trained using a standard adversarial game between a generator $G_\psi$ and a discriminator $D_\omega$, parameterized by $\omega$, as follows:
\begin{equation}
\min_{\psi}\max_{\omega} \;  
\mathbb{E}_{\x_i\sim p(\x), \x_j\sim \mathcal{A}_i} \left[\ln D_\omega(\x_j, \mathbf{h}_i)\right] \; +
\; \mathbb{E}_{ \x_i\sim p(\x),\z \sim \mathcal{N}(0,I) } \left[\ln(1-D_\omega(G_\psi(\z, \mathbf{h}_i), \mathbf{h}_i))\right].
\end{equation}
The discriminator $D_\omega$ attempts to distinguish between real samples in $\mathcal{A}_i$ and the generated samples, while the generator $G_\psi$ tries to fool the discriminator by generating realistic images following the distribution of the nearest neighbor samples in $\mathcal{A}_i$. In the class-conditional version of \icgan, referred to as \ccicgan, a class label $y$ is used as an extra input conditioning for the generator, such that $\tilde\x = G_\psi(\z, \mathbf{h}, y)$; this enables control over the generations given both a class label and an input image.


\subsection{Data augmentation with \icgan}
\label{ssec:met:da_icg}

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{figures/DA-ICGAN.pdf}
    \caption{\ours integration scheme to train a model $F_\theta$. For each image $\x \in \D$ we apply \ours, with probability $p_G$. When an image is \icgan-augmented, the  representation $\h$ of the image is used as input to the generator, together with the Gaussian noise $\z$. 
    The generated image $\tilde\x$ undergoes an additional sequence of handcrafted data augmentations, $\tilde T$. When \ours is not applied, the standard handcrafted data augmentation, $T$, is applied to $\x$ to produce $\x'$. In the case of multi-view SSL training (orange branch), an additional view, $\x''$, is obtained by independently applying $T$ to the original image.
    }
    \label{fig:method}
\end{figure}

\paragraph{Data augmentation notation.} We define a data augmentation recipe as a transformation, $T$, of a datapoint, $\x_i \in \D$, to produce a perturbed version  $\x_i' = T(\x_i)$ of it. 
The data augmentation mapping $T : \Rimg \rightarrow \Rimg$ is usually composed of multiple single transformation functions defined in the same domain, $\tau : \Rimg \rightarrow \Rimg$, $T = \tau_1 \circ \tau_2 \circ ... \circ \tau_t$. Each function $\tau$ corresponds to a specific augmentation of the input such as zooming or color jittering, and is applied with a probability  $p_\tau$. Moreover, $\tau$ can be modified with other hyper-parameters $\lambda_\tau$ that are augmentation-specific -- e.g. magnitude of  zooming, or intensity of color distortion.

\paragraph{\ours.} We introduce a new data augmentation, \ours, that leverages a pre-trained \icgan generator model and can be used in conjunction with other data augmentation techniques to train a neural network. \ours is applied before any other data augmentation technique and is regulated by a hyper-parameter $p_G$ controlling a percentage of datapoints to be augmented. When a datapoint $\x_i$ is \icgan-augmented, it is substituted by the model sample $\tilde\x_i = G_\psi(\z, E_\phi(\x_i))$, with $\z$ a Gaussian noise vector. $\tilde\x_i$ may then be further transformed with a sequence of subsequent transformations $\tilde T = \tau_1 \circ \tau_2 \circ ... \circ \tau_{\tilde t}$. 
Note that $\tilde T$ might differ from the sequence of transformations $T$ applied when $\x_i$ is not \icgan-augmented. We depict this scenario in Figure \ref{fig:method}. Moreover, we use the {\em truncation trick} \citep{marchesi_megapixel_2017} and introduce a second hyper-parameter, $\sigma$, to control the variance of the latent variable $\z$. During \icgan training truncation is not applied, and  $\z$ is sampled from the unit Gaussian distribution. \ours augmentations can be applied to both supervised and self-supervised representation learning off-the-shelf, see section \ref{sec:exps} for details.\looseness-1 %

