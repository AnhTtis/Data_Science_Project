\section{Additional Results}
\label{suppl:results}


\subsection{DeiT-B per-class analysis}

Figure~\ref{suppl:fig:per_class_deit} assesses the impact of \allicgan's generation quality on the per class performance of the DeiT-B model. The exclusive use of generated samples to train DeiT-B leads to rather a low top-1 accuracy of $\sim$48\% and $\sim$51\%, when using \icgan and \ccicgan respectively.\looseness-1

Following section~\ref{sssec:class}, Figures~\ref{suppl:fig:per_class_deit} (a--b) show the per-class FID of \allicgan as a function of per-class top-1 accuracy of the vanilla baseline and the \ours models. We observe similar trends as for the ResNet-152 models, -- i.e., \ours tends to exhibit higher accuracy for classes with lower (better) FID values, and lower accuracy for classes with higher FID values, suggesting that using image generations of poorly modeled classes hurts the performance of DeiT-B. Figures~\ref{suppl:fig:per_class_deit} (c--d) highlight that the low accuracies of the model trained with generated data can be partially explained by the NN corruption.



\begin{figure}[ht]
\centering
\begin{subfigure}[b]{.43\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/corr_icg_baseline_fid.pdf}
    \caption{FID -- DeiT-B, \icgan}
\end{subfigure}
\begin{subfigure}[b]{.43\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/corr_ccicg_baseline_fid.pdf}
    \caption{FID -- DeiT-B, \ccicgan}
\end{subfigure}
\begin{subfigure}[b]{.43\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/corr_icg_baseline_purity.pdf}
    \caption{NN corruption -- DeiT-B, \icgan}
\end{subfigure}
\begin{subfigure}[b]{.43\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/corr_ccicg_baseline_purity.pdf}
    \caption{NN corruption -- DeiT-B, \ccicgan}
\end{subfigure}
\caption{
Impact of \allicgan's generation quality on per-class performance. (a-b) Per-class FID as a function of per-class top-1 accuracy of the vanilla and \ours models. We observe that higher quality \allicgan generations tend to result in improved performances. (c-d) Per-class NN corruption as a function of per-class top-1 accuracy of the vanilla and \ours models. We observe that less corrupted classes tend to result in improved performances. ImageNet validation results are shown for the DeiT-B model trained with horizontal flips, random crops, and RandAugment. We limited the FID colormap interval to 250 to aid interpretability, while we observed FID values up to 500 for certain classes.}
\label{suppl:fig:per_class_deit}
\end{figure}



\subsection{Avoiding \ours on low quality classes}
In this analysis, we try to exploit the observed correlation between per-class accuracy and per-class FID, i.e., samples quality (see Section~\ref{sssec:class}), by restricting the use of \ours only to classes that have an acceptable quality. We set an FID threshold of 150, under which we consider a class to have acceptable quality as the visual inspection of classes with FID >= 150 reveals either very poor image quality or mode-collapse (as shown in Figure~\ref{fig:mode_collapse}); this threshold value is distant around 1.5$\sigma$ and 3$\sigma$ from the average per-class FID computed on \icgan and \ccicgan samples, respectively. For this experiment, we train ResNet-152 with an augmentation recipe composed by HFlip and RRCrop applied to all classes, and \ours applied to FID-filtered classes. We report the results in Table~\ref{suppl:tab:fid_filtered_rn152}.

\begin{table}[ht]
\centering
\caption{ImageNet classification accuracy of ResNet-152 when using \ours indistinctly on all classes vs. augmenting only classes with FID < 150. For each column of results we report the mean top-1 accuracy computed over the indicated set of classes.}
\label{suppl:tab:fid_filtered_rn152}
\resizebox{.9\textwidth}{!}{%
\begin{tabular}{@{}lllccc@{}}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{DA base} & \multirow{2}{*}{\ours} & \multicolumn{3}{c}{Top-1 accuracy} \\
 & & & all classes & classes w/ FID < 150 & classes w/ FID >= 150 \\ \midrule
\multirow{2}{*}{ResNet-152} & \multirow{2}{*}{HFlip + RRCrop} & w/ \icgan & 77.71 & 77.44 & 80.44 \\
 & & w/ FID-filtered \icgan & \textbf{77.94} & \textbf{77.56} & \textbf{81.67} \\ \midrule
\multirow{2}{*}{ResNet-152} & \multirow{2}{*}{HFlip + RRCrop} & w/ \ccicgan & 77.96 & 77.92 & 80.29 \\
 & & w/ FID-filtered \ccicgan & 77.96 & 77.92 & \textbf{80.71} \\ \bottomrule
\end{tabular}%
}
\end{table}

Overall, we obtain, on average, a slightly better top-1 accuracy, +0.2\%p, which can be stratified into +1.2\%p considering the classes with FID >= 150 and +0.1\%p on the remaining classes. From these results we can observe that skipping the use of \ours on poorly modeled classes increases the performances on such classes, while not harming the performance on the others.





