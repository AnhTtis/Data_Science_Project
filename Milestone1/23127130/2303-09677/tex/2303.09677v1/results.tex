

\section{Experimental Evaluation}
\label{sec:results}

In this section, we first present the results obtained in the supervised setting using ResNets and DeiT: in-distribution evaluation on IN (Section~\ref{sssec:ind}); classification results on robustness benchmarks (Section~\ref{sssec:ood}); invariance of learned representations (Section~\ref{sssec:inv}); stratified per-class analysis (Section~\ref{sssec:class}); and sensitivity and ablation studies (Section~\ref{sssec:ablation}). Secondly, we show the SSL results of SwAV on IN (Section~\ref{ssec:ssl_res}).

\subsection{Supervised ImageNet training}
\label{ssec:sup_res}

\subsubsection{In-distribution evaluation}
\label{sssec:ind}

We start by analyzing the impact of \ours when used in addition to several hand-crafted data augmentation recipes for ResNet-50, ResNet-101, ResNet-152, ResNet-50W2, and DeiT-B. In Figure~\ref{fig:top1_in1k}, we report the top-1 accuracy on the IN validation set for the models under study. 

\begin{figure}
\centering
\begin{subfigure}[b]{\textwidth}
    \centering
    \includegraphics[width=.35\textwidth]{figures/top1_acc_deit_legend.pdf}
\end{subfigure}
\begin{subfigure}[b]{.17\textwidth}
    \centering
    \includegraphics[height=4.5cm]{figures/top1_acc_rn50.pdf}
    \caption{ResNet-50}
    \label{fig:rn50_top1_in1k}
\end{subfigure}
\hfill
\begin{subfigure}[b]{.17\textwidth}
    \centering
    \includegraphics[height=4.5cm]{figures/top1_acc_rn101.pdf}
    \caption{ResNet-101}
    \label{fig:rn101_top1_in1k}
\end{subfigure}
\hfill
\begin{subfigure}[b]{.17\textwidth}
    \centering
    \includegraphics[height=4.5cm]{figures/top1_acc_rn152.pdf}
    \caption{ResNet-152}
    \label{fig:rn152_top1_in1k}
\end{subfigure}
\hfill
\begin{subfigure}[b]{.17\textwidth}
    \centering
    \includegraphics[height=4.5cm]{figures/top1_acc_rnw2.pdf}
    \caption{ResNet-50W2}
    \label{fig:rnw2_top1_in1k}
\end{subfigure}
\hfill
\begin{subfigure}[b]{.29\textwidth}
    \centering
    \includegraphics[height=4.5cm]{figures/top1_acc_deit.pdf}
    \caption{DeiT-B}
    \label{fig:deit_top1_in1k}
\end{subfigure}
    \caption{Impact of \ours when coupled with different data augmentation (DA) recipes for training on IN. Hand-crafted DA techniques are: Hflip --random horizontal flips--, RRCrop --random resized crops--, RAug --RandAugment\citep{cubuk_randaugment_2020-2}--, and CutMixUp~\citep{yun_cutmix_2019, zhang_mixup_2017}. DA techniques are added from left to right, with the right-most column combining all possible DA strategies; i.e, +RRCrop applies RRCrop on top of Hflip.}

    \label{fig:top1_in1k}    
\end{figure}

When training ResNets (see Figures \ref{fig:rn50_top1_in1k}-\ref{fig:rnw2_top1_in1k}), coupling \ours with random horizontal flips (Hflip) results in an overall accuracy boost of 0.5-1.7\%p when using \icgan and 0.3-1.6\%p when using \ccicgan. However, when pairing \ours with random horizontal flips and crops (+RRCrop), we observe an accuracy decrease of 0.1-0.4\%p for ResNet-50 and 0.4-0.8\%p for ResNet-101, while for the bigger capacity ResNets the accuracy is boosted by 0.4-0.7\%p for ResNet-152 and 0.2\%p for ResNet-50W2 with either \icgan or \ccicgan. These results show that \ours is beneficial for higher capacity networks, which might be able to capture the higher image diversity induced by more aggressive DA recipes. These observations align with those of~\cite{kolesnikov_big_2020}, who showed that extremely large dataset sizes may be detrimental to low-capacity networks such as ResNet-50 and ResNet-101, and those of~\cite{steiner_how_2021}, who showed that hand-crafted DA strategies applied on large-scale datasets can also result in performance drops.\looseness-1
 
When training DeiT-B (see Figure \ref{fig:deit_top1_in1k}), the largest model considered --with $\sim4\times$ as many parameters as the ResNet-50-- and which usually needs aggressive regularization strategies~\citep{steiner_how_2021, touvron_deit_2022}, we observe that \ours paired with random horizontal flips (Hflip), random crops (+RRCrop) and RandAugment (+RAug) provides a remarkable accuracy boost of 8.0/7.2\%p, 2.9/2.3\%p, 2.0/2.3\%p for \icgan and \ccicgan, respectively, when compared to only using the hand-crafted DA recipes. However, when extending the recipe by adding more aggressive DA such as CutMixUp, the combination with \ours results in a slight decrease of -0.5\%p and -1.0\%p in accuracy for \ccicgan and \icgan respectively.\looseness-1

Overall, \ours boosts the top-1 accuracy when paired with most of the hand-crafted DA recipes studied and with larger ResNet models, showcasing the promising application of \ours as a DA tool. We hypothesize that \ours acts as an implicit regularizer and as such, when paired with the most aggressive DA recipes for smaller ResNet models and DeiT-B, does not lead to an accuracy improvement, possibly due to an over-regularization of the models. Moreover, we argue that state-of-the-art training recipes with hand-crafted DA strategies have been carefully tuned and, therefore, simply adding \ours into the mix without careful tuning of training and hand-crafted DA hyper-parameters or the optimization strategy might explain the decrease in accuracy for these recipes.



\subsubsection{Robustness evaluation}
\label{sssec:ood}
We present results on six additional datasets: Fake-IN, Fake-IN\textsubscript{CC}, IN-A, IN-R, IN-Real and ObjectNet, to test the robustness of our models. We consider the ResNet-50 model for its ubiquitous use in the literature, as well as the high capacity models ResNet-152 and DeiT-B for their high performance. Results are reported in Table~\ref{tab:ood}.\looseness-1

\begin{table}
\centering
\caption{Robustness evaluation. Top-1 accuracy for ResNet-50, ResNet-152 and DeiT-B, trained on IN and evaluated on: IN-Real (IN-ReaL), Fake-IN, FAKE-IN\textsubscript{CC}, IN-A(IN-A), IN-R (In-R) and ObjectNet. IN (in distribution) results are reported for reference.}
\label{tab:ood}
\resizebox{.8\textwidth}{!}{%
\begin{tabular}{@{}cllccccccc@{}}
\toprule
& & & & \multicolumn{6}{c}{robustness benchmarks}\\
\cmidrule(lr){5-10}
Model & DA base & \ours & IN & IN-ReaL & Fake-IN & Fake-IN\textsubscript{CC} & IN-A & IN-R & ObjectNet \\ \midrule
\multirow{6}{*}{ResNet-50} & \multirow{3}{*}{Hflip} & / & 70.75 & 74.18 & 33.11 & 55.00 & 2.07 & \textbf{23.07}  & \textbf{33.33} \\
 &  & w/ \icgan & \textbf{71.43} & 74.38 & \textbf{39.53} & \textbf{58.06} & 0.97 & 21.46 & 31.93 \\
 &  & w/ \ccicgan & 71.25 & \textbf{74.63} & 33.38 & 57.70 & \textbf{2.23} & 22.47  & \textbf{33.32} \\ \cmidrule(l){2-10} 
 & \multirow{3}{*}{+ RRCrop} & / & \textbf{76.29} & \textbf{77.52} & 37.55 & 61.87 & \textbf{0.61} & \textbf{23.28}  & \textbf{34.67} \\
 &  & w/  \icgan & 76.21 & 77.23 & \textbf{40.65} & 63.14 & 0.45 & 22.99 & 34.45 \\
 &  & w/  \ccicgan & 75.91 & 77.21 & 38.55 & \textbf{65.06} & \textbf{0.60} & 22.70 & 33.52 \\ \midrule
 \multirow{6}{*}{ResNet-152} & \multirow{3}{*}{Hflip} & / & 71.42 & 73.90 & 34.85 & 58.61 & 1.68 & 23.02  & 33.44 \\
 &  & w/ \icgan & \textbf{73.06} & \textbf{75.29} & \textbf{38.28} & 60.52 & \textbf{2.31} & 24.24  & \textbf{35.44} \\
 &  & w/ \ccicgan & 73.02 & 75.09 & 34.39 & \textbf{65.02} & 2.08 & \textbf{24.93}  & 35.28 \\ \cmidrule(l){2-10} 
 & \multirow{3}{*}{+ RRCrop} & / & 77.27 & 78.17 & 37.88 & 63.64 & 1.56 & 24.68 & 35.51 \\
 &  & w/  \icgan & 77.71 & \textbf{78.90} & \textbf{40.56} & 64.11 & 2.32 & \textbf{26.03}  & 38.16 \\
 &  & w/  \ccicgan & \textbf{77.97} & 78.87 & 37.90 & \textbf{66.50} & \textbf{2.57} & 25.96  & \textbf{38.27} \\ \midrule
\multirow{9}{*}{DeiT-B} & \multirow{3}{*}{Hflip + RRCrop} & / & 69.47 & 70.46 & 35.78 & 59.38 & 1.85 & 15.82  & 20.36 \\
 &  & w/ \icgan & \textbf{72.35}  & \textbf{73.59} & \textbf{43.79} & 62.63 & 1.92 & 18.36 & \textbf{24.49} \\
 &  & w/ \ccicgan & 71.79  & 72.74 & 36.41 & \textbf{75.35} & \textbf{2.12} & \textbf{18.74} & 23.78 \\ \cmidrule(l){2-10} 
 & \multirow{3}{*}{+ RAug} & / & 75.28 & 75.56 & 34.53 & 59.75 & 4.36 & 24.18 & 27.31 \\
 &  & w/ \icgan & 76.74 & 77.37 & \textbf{42.80} & 64.49 & 4.34 & 25.11  & \textbf{31.85} \\
 &  & w/ \ccicgan & \textbf{77.02} & \textbf{77.53} & 37.07 & \textbf{75.71} & \textbf{4.86} & \textbf{26.37}  & 31.49 \\ \cmidrule(l){2-10} 
 & \multirow{3}{*}{+ CutMixUp} & / & \textbf{81.19} & \textbf{81.23} & 37.87 & 66.90 & \textbf{11.95} & 31.63  & \textbf{40.56} \\
 &  & w/ \icgan & 80.16 & 80.84 & \textbf{41.78} & 67.33 & 11.14 & 30.85 & 38.59 \\
 &  & w/ \ccicgan & 80.65 & 80.97 & 38.23 & \textbf{76.41} & 11.70 & \textbf{32.23} & 38.58 \\ \bottomrule
\end{tabular}%
}
\end{table}


On Fake-IN and Fake-IN\textsubscript{CC}, datasets composed of generated images obtained with \icgan and \ccicgan respectively, we make two observations. First, the decrease in accuracy of the vanilla ResNets and DeiT-B on these datasets with respect to their IN accuracy highlights a considerable data distribution shift between IN and both Fake-IN and Fake-IN\textsubscript{CC}. Moreover, the accuracy on Fake-IN is significantly lower than on Fake-IN\textsubscript{CC}, as one may expect given the higher generation quality and label preservation of \ccicgan. Secondly, the use of \icgan and \ccicgan provides significant boosts on the respective Fake-IN and Fake-IN\textsubscript{CC} datasets, highlighting the increased robustness of the models trained with \ours while remaining competitive on IN.\looseness-1

On IN-A and IN-R, \ours outperforms the vanilla baselines in most of the settings explored, while especially increasing robustness for larger models -- i.e., ResNet-152 and DeiT-B. However, we notice a better impact of \ccicgan compared to \icgan in 6/7 cases for IN-A and in 5/7 for IN-R, which overturns the results on IN validation where \icgan is better in most cases. This might be explained by the fact that despite being less diverse, \ccicgan generations are more likely to depict the correct class; during training the lower sample diversity reduces the regularization effect providing lower in-distribution gains.  

Finally, on ObjectNet and IN-ReaL, we observe similar trends to those in IN: ResNet-152 and DeiT-B with horizontal flips, random crops and random augment benefit from \ours, leading to an increase in accuracy. This evidences that the improvements that \ours provides in-distribution to high capacity models transfer well when considering a more correct IN labeling, such as the one of IN-ReaL, and more importantly when classifying different objects with several viewpoints and backgrounds, such as those in ObjectNet.\looseness-1

Overall, this robustness evaluation confirms a positive impact of \ours for high-capacity models --already benefiting on in-distribution data--, suggesting that they learn more robust representations which may transfer to unseen datasets. In particular, the generations of \allicgan appear to increase the robustness of the trained models by presenting them with slightly different characteristics from in-distribution images.\looseness-1 %



\subsubsection{Feature invariances}
\label{sssec:inv}

We study the invariance of the learned representations of the ResNet-152 model--our best-performing ResNet model--, to assess whether the \ours's performance boosts could be attributed to more robust learned representations. In particular, we evaluate representation invariances to {\em instance},  {\em viewpoint}, {\em occlusion}, and {\em illumination}, in terms of the top-25 RIS scores. Results are reported in Table~\ref{tab:invariances}.\looseness-1

\begin{table}
\caption{Top-25 Representation Invariance Score (RIS) of the learned representations, evaluated on ImageNet (IN), Pascal3D (P3D), GOT-10K (GOT), and ALOI datasets. $\uparrow$ top-25 RIS means $\uparrow$ invariance.\looseness-1}
\label{tab:invariances}
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{@{}cllcccccc@{}}
\toprule

\multirow{2}{*}{Model} & \multirow{2}{*}{DA base} & \multirow{2}{*}{\ours} %
& P3D & P3D & GOT & ALOI & ALOI & ALOI \\
 &  & %
 & Instance & Inst. + View. & Occlusion & Viewpoint & IllumColor & IllumDir \\ \midrule
\multirow{6}{*}{ResNet-152} & \multirow{3}{*}{Hflip} & / %
& 57.05 & 61.15 & \textbf{73.12} & \textbf{81.23} & \textbf{98.91} & \textbf{90.11}  \\
 &  & w/  \icgan %
 & \textbf{58.64} & 61.63 & 70.63 & 78.19 & \textbf{98.90} & 87.27 \\
 &  & w/ \ccicgan %
 & 58.58 & \textbf{62.02} & 71.09 & 78.68 & 98.65 & 87.53 \\ \cmidrule(l){2-9} 
 & \multirow{3}{*}{+ RRCrop} & / %
 & 59.74 & 62.86 & 74.06 & 83.53 & \textbf{99.67} & 90.00 \\
 &  & w/  \icgan %
 & \textbf{62.22} & \textbf{65.87} & \textbf{74.37} & 83.89 & 99.63 & \textbf{91.31} \\
 &  & w/  \ccicgan %
 & 62.01 & 64.93 & 74.36 & \textbf{84.06} & 99.65 & 91.23 \\ 

\bottomrule
\end{tabular}
}
\end{table}

We measure the invariance to {\em instance} on P3D. We observe that \ours always induces a higher RIS. We argue that this result might be expected by considering the ability of \allicgan of populating the neighborhood of each datapoint, i.e., instance. 

Next, we quantify the invariance to {\em viewpoint} using both P3D -- {\em instance + viewpoint} -- and ALOI. In this case, we also notice that \ours generally induces more consistent representations, except when combined with horizontal flips and evaluated on ALOI. Our explanation for the generally higher viewpoint invariance is that \allicgan samples depict slightly different viewpoints of the object present in the conditioning image -- see visual examples in Appendix~\ref{suppl:visual}.

Finally, by looking at the {\em occlusion} invariance on GOT, and the {\em illumination color} and {\em direction} invariance on ALOI, we observe mixed results: in some cases \ours slightly increases the RIS while in some other cases \ours slightly decreases it. This result is perhaps unsurprising as none of these invariances is directly targeted by \ours; and the slight increases observed in some cases could be a side-effect of the larger diversity given by \ours -- e.g., higher occlusion invariance might be due to erroneous generations not containing the object class.\looseness-1

Overall, the invariance analysis highlights that \ours, by leveraging the diversity of the  neighborhood, can be useful not only to regularize the model and achieve better classification accuracy, but also to provide more consistent feature representations across variations of {\em instance} and {\em viewpoint}. Guaranteeing such invariances is likely to lead to a better transferability/robustness of the representations -- as shown in Section~\ref{sssec:ood}.    







         


\subsubsection{Per-class analysis}
\label{sssec:class}

To further characterize the impact of \ours, we perform a more in-depth analysis by stratifying the ResNet-152 results per class. We compare the per-class FID of \icgan and \ccicgan, as well as their NN corruption, with the top-1 accuracy per class of a vanilla model -- trained without \ours --  and a model trained only with generated samples -- i.e., using \ours with $p_G = 1.0$ -- with the goal of better understanding the impact of \allicgan's generations on the model's performance. Results are reported in Figure~\ref{fig:correlation_fid}. Note that the exclusive use of generated samples leads to rather low top-1 accuracy: $\sim$43\% and $\sim$46\% for ResNet-152 %
when using \icgan and \ccicgan respectively.\looseness-1

\begin{figure}
\centering
\begin{subfigure}[b]{.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/corr_rn152_icg_baseline_fid.pdf}
    \caption{FID -- ResNet-152, \icgan}
    \label{fig:fid_icgan_rn152}
\end{subfigure}
\centering
\begin{subfigure}[b]{.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/corr_rn152_ccicg_baseline_fid.pdf}
    \caption{FID -- ResNet-152, \ccicgan}
    \label{fig:fid_ccicgan_rn152}
\end{subfigure}
\hfill
\begin{subfigure}[b]{.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/corr_rn152_icg_baseline_purity.pdf}
    \caption{NN corruption -- ResNet-152, \icgan}
    \label{fig:nn_icgan_rn152}
\end{subfigure}
\begin{subfigure}[b]{.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/corr_rn152_ccicg_baseline_purity.pdf}
    \caption{NN corruption -- ResNet-152, \ccicgan}
    \label{fig:nn_ccicgan_rn152}
\end{subfigure}

\caption{Impact of \allicgan's generation quality on per-class performance. (a-b) Per-class FID as a function of per-class top-1 accuracy of the vanilla and \ours models. We observe that higher quality \allicgan generations tend to result in improved performances. (c-d) Per-class NN corruption as a function of per-class top-1 accuarcy of the vanilla and \ours models. We observe that less corrupted classes tend to result in improved performances. ImageNet validation results shown for the ResNet-152 model trained with horizontal flips and random crops.
We limited FID colormap interval to 250 to aid interpretability, while we observed FID values up to 500 for certain classes.}
\label{fig:correlation_fid}
\end{figure}

\input{fig_perclass_fid}

Figures~\ref{fig:fid_icgan_rn152} and~\ref{fig:fid_ccicgan_rn152} present the per-class FID of \allicgan as a function of per-class top-1 accuracy of the vanilla baseline and the \ours models. We observe that \ours tends to exhibit higher accuracy for classes with lower FID values, and lower accuracy for classes with higher FID values overall. In particular, classes for which generated images have good quality and diversity (e.g., $\sim$ 50 FID or lower) tend to achieve high top-1 accuracy for both the vanilla model and the one trained with only generated data. Conversely, when the FID of a class is high, its per-class accuracy oftentimes drops for the \allicgan-trained models, whereas the vanilla model remains performant. Perhaps unsurprisingly, this evidences that leveraging image generations of poorly modeled classes to train the ResNet-152 model hurts the performance. Moreover, we note that there are more classes with very high FID ($\sim$ 200 or higher) for \icgan than \ccicgan. Intuitively, this could be explained by the fact that \ccicgan uses labels to condition the model and appears to be less prone to mode collapse (see Figure~\ref{fig:mode_collapse}). %

We additionally observe in Figures~\ref{fig:nn_icgan_rn152} and~\ref{fig:nn_ccicgan_rn152} that the low accuracies of the model trained with generated data can be partially explained by the NN corruption: classes with less corrupted neighborhoods tend to exhibit higher top-1 accuracies than the more corrupted ones. However, we observe some specific cases of classes with low corruption which result in very low accuracy when considering the model trained with all generated samples (see the bottom-right corner of the plots). This could be explained by the mode collapse that \allicgan experience, as we see that those same classes generally have very high FID ($>$ 200) in Figures~\ref{fig:fid_icgan_rn152} and~\ref{fig:fid_ccicgan_rn152}.\looseness-1

In this analysis, we shed some light on the problematic \allicgan modeling of certain classes. 
We believe that computing stratified results for generative models might be a good practice to be adopted by the community, as also supported by \cite{ravuri_classification_2019}. Nevertheless, the observed positive correlation between high classification accuracy and \allicgan's generation quality --studied through the lens of per-class FID and NN corruption-- constitutes a promising result to improve the effectiveness of \ours. To this end, we ran an additional experiment where we avoid applying \ours on classes having very high FID (>= 150), i.e., where \allicgan has very low generation quality. We report the results in Appendix~\ref{suppl:results}. Notably, the impact of leveraging \ours could be potentially improved by increasing the generation quality of the \allicgan's poorly modeled classes. These findings improve upon those of \cite{ravuri_classification_2019}, where a pre-trained BigGAN showed little to no correlation between FID and classification accuracy in a similar setting, strengthening the position of instance-conditioned models such as \allicgan. We observe similar trends for DeiT-B (see Appendix~\ref{suppl:results}).









\subsubsection{Sensitivity and ablation studies}
\label{sssec:ablation}


\paragraph{Probability $p_G$.} We study the impact of the probability of applying \ours, $p_G$, in Figure~\ref{fig:prob}. We consider our best ResNet model as well as DeiT-B. We further include the study on ResNet-50 as a sanity check to validate our previous over-regularization hypothesis. When coupling \ours with horizontal flip to train the ResNet-50 model (Figure~\ref{fig:rn50_top1_prob_in1k}), we observe that a probability of $p_G=0.1$ and $p_G=0.5$ achieve the best results for \ccicgan and \icgan, respectively. However, when adding random crops to the recipe, ResNet-50 no longer benefits from \ours and obtains the best results for $p_G=0$, highlighting the potential over-regularization suffered by low-capacity models as discussed in section~\ref{sssec:ind}. When it comes to ResNet-152 (Figure~\ref{fig:rn152_top1_prob_in1k}), we observe that the overall accuracy increases until achieving its peak value for some $p_G$ and then starts decreasing. 
More precisely, \ccicgan shows optimal $p_G$ for lower values, 0.1 and 0.3, whereas \icgan benefits from the higher probability values 0.3 and 0.5. Note that in both cases, \ours coupled with random horizontal flips and crops requires lower $p_G$ values than \ours coupled with horizontal flips only, emphasizing the benefit of \ours especially when leveraging soft augmentation strategies. For DeiT-B architecture (Figure~\ref{fig:deit_top1_prob_in1k}), we note that increasing $p_G$ values mostly result in better accuracy when using all DA recipes except the strongest one containing CutMixUp. This trend might be due to the higher capacity of the DeiT-B model that combined with the lower architectural inductive bias -- i.e., no convolution -- requires stronger regularization on IN. 
This shows the benefits of using \ours to regularize  training, especially for architectures prone to overfitting, which require higher $p_G$ values.

\begin{figure}
\centering
\begin{subfigure}[b]{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/top1_acc_deit_prob_legend.pdf}
\end{subfigure}
\hfill
\begin{subfigure}[b]{.32\textwidth}
    \centering
    \includegraphics[height=4.5cm]{figures/top1_acc_rn50_prob.pdf}
    \caption{ResNet-50}
    \label{fig:rn50_top1_prob_in1k}
    \end{subfigure}
    \hfill
\begin{subfigure}[b]{.32\textwidth}
    \centering
    \includegraphics[height=4.5cm]{figures/top1_acc_rn152_prob.pdf}
    \caption{ResNet-152}
    \label{fig:rn152_top1_prob_in1k}
\end{subfigure}
\hfill
\begin{subfigure}[b]{.32\textwidth}
    \centering
    \includegraphics[height=4.5cm]{figures/top1_acc_deit_prob.pdf}
    \caption{DeiT-B}
    \label{fig:deit_top1_prob_in1k}
\end{subfigure}
\caption{Sensitivity study for the probability $p_G$ of applying \ours. The missing datapoint in red curves of panel (c) are due to trainings not converging.}
\vspace{-0.4cm}
\label{fig:prob}
\end{figure}

\begin{wraptable}{ri}{9cm}
\vspace{-.5cm}
\caption{\small CutMixUp ablation when training DeiT-B with Hflip+RRCrop+RAug and \ours. All models trained with a label smoothing of 0.1. *: Failed to converge.
}
\vspace{-0.2cm}
\label{tab:cmu_ablation}
\centering
\resizebox{.7\linewidth}{!}{%
\begin{tabular}{@{}ccccc@{}}
\toprule
\icgan & \ccicgan & CutMix & MixUp & Top-1 \\ 
\midrule
\xmarkg & \xmarkg & \cmarkg & \cmarkg & 81.2 \\
\xmarkg & \xmarkg & \cmarkg & \xmark & 81.7 \\
\xmarkg & \xmarkg & \xmark & \cmarkg & 80.3 \\
\xmarkg & \xmarkg & \xmark & \xmark & 77.4 \\ 
\midrule
\cmark & \xmarkg & \cmarkg & \cmarkg & 80.2 \\
\cmark & \xmarkg & \cmarkg & \xmark & 78.5 \\
\cmark & \xmarkg & \xmark & \cmarkg & * \\
\cmark & \xmarkg & \xmark & \xmark & 78.2 \\
\midrule
\xmarkg & \cmark & \cmarkg & \cmarkg & 80.7 \\
\xmarkg & \cmark & \cmarkg & \xmark & 78.2 \\
\xmarkg & \cmark & \xmark & \cmarkg & * \\
\xmarkg & \cmark & \xmark & \xmark & 78.1 \\
\bottomrule
\end{tabular}
}
\vspace{-.5cm}
\end{wraptable}
\vspace{-0.2cm}
\paragraph{CutMixUp components.} In Table~\ref{tab:cmu_ablation} we present an ablation of the two components in CutMixUp -- CutMix and MixUp -- when training DeiT-B models with and without \ours. To facilitate convergence when using CutMix and/or MixUp, we follow~\cite{touvron_deit_2022} and perform label smoothing. Moreover, we leverage the {\em soft labels} introduced in section~\ref{ssec:sup_res} for all experiments using \ours. We observe that removing CutMix results in a decrease in accuracy of 0.9\%p, whereas removing MixUP increases the performance by +0.5\%p. Removing both CutMix and MixUp results in the lowest accuracy. When considering \ours, we observe that results are consistently better than those of the baseline model when not using CutMix nor MixUp. Leveraging CutMixUp appears to be beneficial but the induced accuracy gains remain lower than those experienced by the baseline model, suggesting that additional tuning of CutMixUp might be required when coupled with \ours.\looseness-1

\subsection{Self-supervised ImageNet training}
\label{ssec:ssl_res}





Given that \icgan can be trained without labels, and can synthesize images without the need of class categories, we employ it to extend multi-view SSL pre-training on IN \citep{he_momentum_2020, chen_simple_2020, caron_unsupervised_2020} by integrating \ours into the standard hand-crafted DA recipes. In Table \ref{tab:ssl} we report the accuracy scores obtained on the IN validation set when testing the pre-trained SSL methods via linear classifier evaluation. %

\begin{wraptable}{r}{.6\linewidth}
\vspace{-.5cm}
\caption{SwAV accuracy on ImageNet validation using different image sources and DA methods. For each view (view1 and view2) in the multi-view SSL setup, the \textit{image source} can be: \textit{Original} - real image from the dataset, or \textit{NN} - a randomly sampled neighbor (k-NN with k=50). \textit{\ours} is applied on top of the image source. \textit{RRC}:  RandomResizedCrop + ColorDistortion + Gaussian Blurring. RRC can produce a \textit{single}-crop or \textit{multi}-crop. 
 }
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{@{}ccc|cc|c@{}}
\toprule
\multicolumn{3}{c|}{View1} & \multicolumn{2}{c|}{View2} & \multirow{2}{*}{Top-1} \\%& \multirow{2}{*}{TOP-5} \\
Image source & \ours & RRC & Image source & RRC &  \\ \midrule
Original & \xmarkg & single & Original & single & 67.96 \\%& 88.58 \\
Original & \cmark & - & Original & single & 68.90 \\%& 89.12 \\+0.94 & +0.54 \\
NN & \xmarkg & single & Original & single & \textbf{70.06} \\%& \textbf{90.10} \\%+2.64 & +1.52 \\

\midrule
Original & \xmarkg & single & Original & multi & 73.64 \\%& \textbf{91.74} \\
Original & \cmark & single & Original & multi & 71.72 \\% & 90.62 \\-1.92 & -1.12 \\
NN & \xmarkg & single & Original & multi & \textbf{73.73} \\%& 91.62 \\74.13 & 91.84 \\%+0.49 & +0.10 \\
\bottomrule
\end{tabular}
}
\vspace{-.5cm}
\label{tab:ssl}
\end{wraptable}
 
We observe a clear difference between the two SwAV settings: (i) with single-crop, and (ii) with multi-crop. In the former case, the use of \ours boosts the top-1 classification accuracy of the linear evaluation probe by $\sim$1\%p. On the contrary, when \ours is combined with multi-crop RRC (crops with different sizes and zooms of the original image), we observe a detrimental effect, with roughly a 2\%p accuracy drop. It is worth noting that the multi-crop transformation already results in significant variations from the original image, in some cases even changing its semantics. Hence, combining \ours and multi-crop might result in extreme augmentation diversity (see example reported in Appendix \ref{suppl:visual}).
Moreover, we recall that the SwAV model has a ResNet-50 backbone, whose capacity might be too low to capture such a large image diversity, as discussed in the supervised IN training in Section~\ref{ssec:sup_res}. A further confirmation of this hypothesis may come from the results of SwAV-NN, which are positive for single-crop, with a $\sim$ 2\%p increase, while remaining on-par for multi-crop (+0.1\%); using real image neighbors instead of \icgan-generated ones (last row of Table \ref{tab:ssl}) does not increase the diversity in the training distribution, requiring less model capacity. Moreover, a non-negligible role might be played by the quality of \allicgan generations that for some instances (or classes) might be poor or semantically far from the conditioning, as observed in Section~\ref{sssec:class} and visually shown in Figure \ref{fig:icg_example}.\looseness-1 


Overall, this empirical analysis reveals that \ours improves SSL training only when single-crop augmentation is adopted, while the use of stronger hand-crafted DA (multi-crop) on top of \icgan-generated images is detrimental. We point out that in NNCLR~\citep{dwibedi_little_2021-1} no gains from the combination of the multi-crop augmentation with the neighbor-based augmentation were found, and our SwAV-NN experiments confirm this finding.

\begin{figure}
\centering
\begin{subfigure}[b]{.3\textwidth}
    \centering
    {\footnotesize Conditioning}\hspace{.8cm}{\footnotesize Gen. sample}
    \includegraphics[width=.48\textwidth]{figures/icgan_examples/img_15_class_tiger_beetle}
    \includegraphics[width=.48\textwidth]{figures/icgan_examples/img_16_class_tiger_beetle}
    \caption{Tiger beetle}
\end{subfigure}
\hfill
\begin{subfigure}[b]{.3\textwidth}
    \centering
    {\footnotesize Conditioning}\hspace{.8cm}{\footnotesize Gen. sample}
    \includegraphics[width=.48\textwidth]{figures/icgan_examples/img_45_class_water_tower_crop}
    \includegraphics[width=.48\textwidth]{figures/icgan_examples/img_47_class_water_tower}
    \caption{Water tower}
\end{subfigure}
\hfill
\begin{subfigure}[b]{.3\textwidth}
    \centering
    {\footnotesize Conditioning}\hspace{.8cm}{\footnotesize Gen. sample}
    \includegraphics[width=.48\textwidth]{figures/icgan_examples/img_25_class_Siberian_husky}
    \includegraphics[width=.48\textwidth]{figures/icgan_examples/img_26_class_Siberian_husky}
    \caption{Siberian husky}
\end{subfigure}
    \caption{Example of three \icgan generations, one (a) qualitatively and semantically correct, one (b) semantically incorrect, and one (c) with poor quality. Left image in each pair is the conditioning, where the red square shows the central crop actually used by \icgan.}
    \label{fig:icg_example}
\end{figure}











































