\section{Instance-Conditioned GAN (IC-GAN)}
\label{sec:bg}


Instance-conditioned GAN (IC-GAN) \citep{casanova_instance-conditioned_2021} is a recent generative model able to generate possibly infinite synthetic but realistic images which resemble a given conditioning image. The key idea of IC-GAN is to learn to model the local distribution surrounding the training images, in order to generate novel images that are the ability to learn the local distribution surrounding conditioning image, so that newly generated images (should) lie in this neighborhood. given a certain condition should be synthetic neighbors of the same. Additionally, the instance conditioning can be coupled with a class label (if available) to better control the semantics of the generation. In this case, the model is referred to as class-conditional IC-GAN.

\paragraph{Inference with IC-GAN}  The IC-GAN generator network, $G_\psi: \R^d  \rightarrow \Rimg$, produces a new synthetic image, $\tilde\x \in \Rimg$, taking as input an embedded representation $\h \in \R^d$ of the conditioning image $\x \in \Rimg$ together with a Gaussian noise vector $\z \in \R^d$: 

\begin{equation}
\begin{aligned}
    \h &= E_\phi(\x), \, \z\sim\mathcal{N}(0,\sigma) \\
    \tilde\x &= G_\psi(\z, \h)
\end{aligned}
\end{equation}

The embedding $\h$ is obtained with a feature extractor, $E_\phi: \Rimg \rightarrow \R^d$, that is previously learned in an unsupervised or self-supervised manner. The value of $\sigma$, which is fixed to the identity matrix $I$ during IC-GAN training, at inference time, can be truncated in order to reduce the variance of generation ({\em truncation trick}, \cite{marchesi_megapixel_2017}). This trick helps to avoid the generation of artifact images, which being in the tail of the Gaussian are more complex to be modeled by the network.

For class-conditional IC-GAN, considering a class label $c \in C$ ($C$ is the set of classes present in the IC-GAN training dataset), a new image is generated as: $\tilde\x = G_\psi(\z, \x, c).$ Note that the class label provided at inference time is arbitrary and might differ from the actual class of $\x$. Moreover, since in this setting labels are available, $E_\phi$ can also be trained in a supervised manner.

\paragraph{Training} IC-GAN is trained using a standard adversarial game between the generator $G_\psi$ and the discriminator $D_\omega$. More precisely, $D_\omega$ is trained to predict whether the $i$-th input sample is real  or synthetic given the same conditioning $\h_i$ fed to $G_\psi$. For this step, real images are uniformly sampled from the neighborhood, $\mathcal{A}_i$, of $\h_i$, computed by means of cosine similarity $k$-nn. In this way, to fool $D_\omega$, $G_\psi$ is induced to generate images that are similar to the conditioning image $\x_i$ and its neighbors $\x_j \in \mathcal{A}_i, j \in \{1, ..., k\}$.
