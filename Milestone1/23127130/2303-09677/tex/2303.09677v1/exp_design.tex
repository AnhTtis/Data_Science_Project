\section{Experimental Setup}
\label{sec:exps}

In our empirical analysis, we investigate the effectiveness of \ours in supervised and self-supervised representation learning. In the following subsections, we describe the experimental details of both scenarios.  

\subsection{Models, metrics, and datasets}

\paragraph{Models.} For supervised learning, we train ResNets~\citep{he_deep_2016} with different depths: 50, 101 and 152 layers, and widths: ResNet-50 twice wider (ResNet-50W2)~\citep{zagoruyko_wide_2016-1}, and DeiT-B~\citep{touvron_training_2021}. For self-supervised learning, we train the SwAV~\citep{caron_unsupervised_2020} model with a ResNet-50 backbone. For \ours, we employ two pre-trained generative models on ImageNet: \icgan and \ccicgan, both using the BigGAN~\citep{brock_large_2019} backbone architecture. \icgan conditions the generation process on instance feature representations, obtained with a pre-trained SwAV model\footnote{\url{https://dl.fbaipublicfiles.com/deepcluster/swav_800ep_pretrain.pth.tar}}, while \ccicgan conditions the generation process on both the instance representation obtained with a ResNet-50 trained for classification\footnote{\url{https://download.pytorch.org/models/resnet50-19c8e357.pth}} and a class label. %
Unless specified otherwise, our models use the default \icgan and \ccicgan configuration from \cite{casanova_instance-conditioned_2021}: neighborhood size of $k$=50 and $256\times256$ image resolution, trained using only horizontal flips as data augmentation\footnote{\url{https://github.com/facebookresearch/ic_gan}}. To guarantee a better quality of generations we set truncation $\sigma = 0.8$ and 1.0 for \icgan and \ccicgan respectively.
For simplicity, we will use the term \allicgan to refer to both pre-trained models hereinafter.

\paragraph{Datasets.} We train all the considered models from scratch on ImageNet \underline{(IN)}~\citep{deng09cvpr} and test them on the IN validation set. Additionally, in the supervised learning case, models are tested for robustness on a plethora of datasets, including \underline{Fake-IN}: containing 50K generated images obtained by conditioning the \icgan model on the IN validation set; \underline{Fake-IN\textsubscript{CC}}: containing 50K images generated with the \ccicgan conditioned on the IN validation set\footnote{To avoid as much as possible unrealistic generations in creating Fake-IN and Fake-IN\textsubscript{CC}, for each IN image we generate a set of 20 samples, from which we chose the one most similar (cosine similarity) to the conditioning image in the feature space.}; IN-Adversarial \underline{(IN-A)}~\citep{hendrycks_natural_2021}: composed of ResNet's adversarial examples present in IN\footnote{Although IN-A contains samples from only 200 out of the 1000 classes of IN, we compute the results without restricting the predictions to those 200 classes.}; IN-Rendition \underline{(IN-R)}~\citep{hendrycks_many_2021}: containing stylized images such as cartoons and paintings belonging to IN classes; \underline{IN-ReaL}~\citep{beyer_are_2020}: a relabeled version of the IN validation with multiple labels per image; and \underline{ObjectNet}~\citep{barbu_objectnet_2019}: containing object-centric images specifically selected to increase variance in viewpoint, background and rotation w.r.t. IN\footnote{The class mapping from ObjectNet to IN is one-to-multi -- i.e., one class is mapped to one or more classes of IN. We consider predictions pointing to any of the mapped classes as correct.}. We also consider the following datasets to study invariances in the learned representations: \underline{IN validation set} to analyze {\em instance+viewpoint} invariances; Pascal-3D+ \underline{(P3D)}~\citep{xiang_beyond_2014}, composed of $\sim$36K images from 12 categories to measure {\em instance}, and {\em instance+viewpoint} invariances; \underline{GOT}~\citep{huang_got-10k_2019}, 10K video clips with a single moving object to measure invariance to object {\em occlusion}; and \underline{ALOI}~\citep{geusebroek_amsterdam_2005}, single-object images from 1K object categories with plain dark background to measure invariance w.r.t.\ {\em viewpoint} (72 viewpoints per object), {\em illumination color} (12 color variations per object), and {\em illumination direction} (24 directions per object).

\paragraph{Metrics.} We quantify performance for classification tasks as the top-1 accuracy on a given dataset. Moreover, we analyze invariances of the learned representations by using the top-25 Representation Invariance Score (RIS) proposed by \cite{purushwalkam_demystifying_2020-1}. In particular, given a class $y$, we sample a set of object images $\mathcal{T}$ by applying a transformation $\tau$ with different parameters $\lambda_\tau$ such that $\mathcal{T} = \{\tau(x, \lambda_\tau) | \forall \lambda_\tau\}$. We then compute the mean invariance on the transformation $\tau$ of all the objects belonging to $y$ as the average firing rate of the (top-25) most frequently activating neurons/features in the learned representations. We follow the recipe suggested in \cite{purushwalkam_demystifying_2020-1} and compute the top-25 RIS only for ResNets models, extracting the learned representations from the last ResNet block ($2048$-$d$ vectors).

\paragraph{Per-class metrics.} We further stratify the results by providing class-wise accuracies and correlating them with the quality of the generated images obtained with \allicgan. We quantify the quality and diversity of generations using the Fr√©chet Inception Distance (FID)~\citep{heusel17nips}. We compute per-class FID by using the training samples of each class both as the reference and as the conditioning to generate the same number of synthetic images. We also measure a particular characteristic of the \icgan model: the \textit{NN corruption}, which measures the percentage of images in each datapoint's neighborhood that has a different class than the datapoint itself; this metric is averaged for all datapoints in a given class to obtain per-class NN corruption.


\subsection{Training recipes}
In this subsection, we define the training recipe for each model in both supervised and self-supervised learning; we describe which data augmentation techniques are used, how \ours is integrated and the hyper-parameters used to train the models.

\paragraph{Model selection.} In all settings, hyper-parameter search was performed with a restricted grid-search for the learning rate, weight decay, number of epochs, and \ours probability $p_G$, selecting the model with the best accuracy on IN validation.

\subsubsection{Supervised learning} 

For the ResNet models, we follow the standard procedure in Torchvision\footnote{See the \texttt{IMAGENET1K\_V1} recipe at \url{https://github.com/pytorch/vision/tree/main/references/classification}.} and apply random horizontal flips \underline{(Hflip)} with 50\% probability as well as random resized crops \underline{(RRCrop)}~\citep{krizhevsky_imagenet_2012}. We train ResNet models for 105 epochs, following the setup in VISSL~\citep{goyal2021vissl}. For DeiT-B we follow the experimental setup from~\cite{touvron_training_2021}, whose data augmentation recipe is composed of Hflip, RRCrop, RandAugment~\citep{cubuk_randaugment_2020-2}, as well as color jittering (CJ) and random erasing (RE)~\citep{zhong_random_2020} ---we refer to RandAugment + CJ + RE as \underline{RAug}---, and typical combinations of CutMix~\citep{yun_cutmix_2019} and MixUp~\citep{zhang_mixup_2017}, namely \underline{CutMixUp}. 
DeiT models are trained for the standard 300 epochs~\citep{touvron_training_2021} except when using only Hflip or RRCrop as data augmentation, where we reduce the training time to 100 epochs to mitigate overfitting. Both ResNets and DeiT-B are trained with default hyper-parameters; despite performing a small grid search, better hyper-parameters were not found. Additional details can be found in Appendix~\ref{suppl:details}.

\paragraph{Soft labels.} We note that the classes in the neighborhoods used to train the
\allicgan models are not homogeneous: a neighborhood, computed via cosine similarity between embedded images in a feature space, might contain images depicting different classes. Therefore, \allicgan samples are likely to follow the class distribution in the conditioning instance neighborhood, generating images that may mismatch with the class label from the conditioning image. To account for this mismatch when using \allicgan samples for training, we employ \textit{soft labels}, which are soft class membership distributions corresponding to each instance-specific neighborhood class distribution. More formally, considering the $i$-th datapoint, its $k$-size neighborhood in the feature space, $\mathcal{A}_i$, and its class label $y_i \in C$ one-hot encoded with the vector $\y_i$, we compute its soft label as:
\begin{equation}
    \y^\mathrm{soft}_i = \frac{1}{k} \sum_{j \in \mathcal{A}_i}{\y_j}, \quad \textrm{with} \quad \y_j \in \{0,1\}^C  \quad \textrm{and} \quad \sum_{c} {\y_{j,c} = 1}.
\end{equation}

\subsubsection{Self-supervised learning} 

We devise a straightforward use of \ours for \textit{multi-view} SSL approaches. Although we chose SwAV~\citep{caron_unsupervised_2020} to perform our experiments, \ours could also be applied to other state-of-the-art methods for \textit{multi-view} SSL like MoCov3~\citep{chen_empirical_2021}, SimCLRv2~\citep{chen_big_2020}, DINO~\citep{caron_emerging_2021} or BYOL~\citep{grill_bootstrap_2020-1}. In this family of approaches, two or more views of the same instance are needed in order to learn meaningful representations. 
These methods construct multi-view positive pairs $(\x_i', \x_i'')^+$ from an image $\x_i$ by applying two independently sampled transformations to obtain $\x'_i=T(\x_i)$ and similarly for $\x_i''$ (see orange branch in Figure~\ref{fig:method}).
To integrate \ours in such pipelines as  an alternative form of  data augmentation, we replace $\x_i'$ with a generated image $\tilde \x_i'$ with probability $p_G$. To this end, we sample an image $\tilde \x_i$ from \icgan conditioned on $\x_i$, and apply further hand-crafted data augmentations $\tilde T$ to obtain $\tilde \x_i' = \tilde T (\tilde\x_i)$.

\paragraph{SwAV pre-training and evaluation.} We follow the SwAV pre-training recipe proposed in \cite{caron_unsupervised_2020}. This recipe comprises the use of random horizontal flipping, random crops, color distortion, and Gaussian blurring for the creation of each image view. In particular, we investigate two augmentation recipes, differing in the use of the {\em multi-crop} augmentation~\citep{caron_unsupervised_2020} or the absence thereof. 
The multi-crop technique augments positive pairs $(\x_i', \x_i'')^+$ with multiple other views obtained from smaller crops of the original image: $(\x_i', \x_i'', \x_i^{\mathrm{small}'''}, \x_i^{\mathrm{small}''''}, ...)^+$. In all experiments, we pre-train SwAV for 200 epochs using the hyper-parameter settings of \cite{caron_unsupervised_2020}. 
To evaluate the learned representation we freeze the ResNet-50 SwAV-backbone and substitute the SSL SwAV head with a linear classification head, which we train supervised on IN validation set for 28 epochs with Momentum SGD and step learning rate scheduler --following the VISSL setup~\citep{goyal2021vissl}.          

\paragraph{Neighborhood augmented SwAV.} To further evaluate the impact of \ours in SSL, we devise an additional baseline, denoted as SwAV-NN, that uses real image neighbors as augmented samples instead of \icgan generations: $(\x_j', \x_i'')^+,\, \x_j \in \mathcal{A}_i$. SwAV-NN is inspired by NNCLR~\citep{dwibedi_little_2021-1}, with the main difference that neighbor images are computed off-line on the whole dataset rather than online using a subset (queue) of the dataset. The nearest neighbors are computed using cosine similarity in the same representation space used for \icgan training. With a probability $p_G$, each image in a batch is paired with a uniformly sampled neighbor in each corresponding neighborhood.
