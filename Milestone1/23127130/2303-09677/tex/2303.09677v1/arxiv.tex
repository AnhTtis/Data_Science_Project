\documentclass[10pt]{article} %
\usepackage[preprint]{tmlr}


\definecolor{citecolor}{HTML}{0071bc}
\usepackage[breaklinks=true,bookmarks=false,colorlinks,bookmarks=false, citecolor=citecolor]{hyperref} 
\usepackage{url}


\PassOptionsToPackage{numbers}{natbib}
\usepackage{array, multirow, tablefootnote}
\PassOptionsToPackage{dvipsnames,table}{xcolor}
\usepackage[bottom]{footmisc}
\usepackage{adjustbox}
\usepackage{amsmath, amsfonts, bm}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{pifont}
\usepackage{booktabs}
\usepackage{xspace}
\usepackage{wrapfig}
\usepackage{placeins}
\usepackage[normalem]{ulem}

\let\oldcite\cite
\renewcommand{\cite}[1]{\mbox{\oldcite{#1}}}


\newcommand{\pietro}[1]{\noindent\textcolor{teal}{PA: #1}\\}
\def\ara#1{{\color{blue} Ar: #1}}
\def\jkb#1{{\color{magenta} Jkb: #1}}
\def\ars#1{{\color{olive} ARS: #1}}

\def \ours {{DA\textsubscript{IC-GAN}}\xspace}
\def \ccicgan {{CC-IC-GAN}\xspace}
\def \icgan {{IC-GAN}\xspace}
\def \allicgan {{(CC-)IC-GAN}\xspace}


\newcommand{\todo}{\noindent\textcolor{red}{\% TODO} \textcolor{red}}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\cmarkg}{\textcolor{gray}{\ding{51}}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\xmarkg}{\textcolor{gray}{\ding{55}}}%


\newcommand*{\x}{\mathbf{x}}
\newcommand*{\y}{\mathbf{y}}
\newcommand*{\z}{\mathbf{z}}
\newcommand*{\h}{\mathbf{h}}
\renewcommand*{\L}{\mathcal{L}}
\newcommand*{\D}{\mathcal{D}}
\newcommand*{\R}{\mathbb{R}}
\newcommand*{\Rimg}{\R^{3 \times H \times W}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\topk}{top_{K}}
\DeclareMathOperator*{\argtopk}{arg\,\topk}

\title{Instance-Conditioned GAN Data Augmentation\\for Representation Learning}


\author{\name Pietro Astolfi\footnotemark[1] \footnotemark[2] \textsuperscript{1},
Arantxa Casanova\footnotemark[1] \textsuperscript{1,2,5},
Jakob Verbeek\textsuperscript{1},
Pascal Vincent\textsuperscript{1,2,3,4},\\
Adriana Romero-Soriano\textsuperscript{1,2,6},
Michal Drozdzal\textsuperscript{1}\\
\addr 
\textsuperscript{1}Meta AI, 
\textsuperscript{2}Mila, Quebec AI Institute,
\textsuperscript{3}Université de Montréal,
\textsuperscript{4}CIFAR,\\
\textsuperscript{5}École Polytechnique de Montréal,
\textsuperscript{6}McGill University
\\
\email \footnotemark[2] pietroastolfi@meta.com \\
\footnotemark[1] Contributed equally
      }


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{MM}  %
\def\year{YYYY} %
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} %


\begin{document}


\maketitle

\begin{abstract}



Data augmentation has become a crucial component to train state-of-the-art visual representation models. However, handcrafting combinations of transformations that lead to improved performances is a laborious task, which can result in visually unrealistic samples. To overcome these limitations, recent works have explored the use of generative models as learnable data augmentation tools, showing promising results in narrow application domains, e.g., few-shot learning and low-data medical imaging. In this paper, we introduce a data augmentation module, called \ours, which leverages instance-conditioned GAN generations and can be used off-the-shelf in conjunction with most state-of-the-art training recipes. We showcase the benefits of \ours by plugging it out-of-the-box into the supervised training of ResNets and DeiT models on the ImageNet dataset, and achieving accuracy boosts up to between 1\%p and 2\%p with the highest capacity models. Moreover, the learnt representations are shown to be more robust than the baselines when transferred to a handful of out-of-distribution datasets, and exhibit increased invariance to variations of instance and viewpoints. We additionally couple \ours with a self-supervised training recipe and show that we can also achieve an improvement of 1\%p in accuracy in some settings. %
With this work, we strengthen the evidence on the potential of learnable data augmentations to improve visual representation learning, paving the road towards non-handcrafted augmentations in model training.

\end{abstract}

\input{introduction}
\input{rel_works}
\input{method}
\input{exp_design}
\input{results}
\input{conclusions}

\bibliography{tmlr}
\bibliographystyle{tmlr}

\appendix
\input{appendix_details}
\input{appendix_results}
\input{appendix_visual}


\end{document}
