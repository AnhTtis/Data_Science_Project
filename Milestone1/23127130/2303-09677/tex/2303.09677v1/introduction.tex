\section{Introduction}
\label{sec:intro}

Recently, deep learning models have been shown to achieve astonishing results across a plethora of computer vision tasks when trained on \emph{very large} datasets of hundreds of millions datapoints \citep{alayrac_flamingo_2022, gafni22arxiv, goyal_vision_2022, radford_learning_2021, ramesh_hierarchical_2022, saharia_photorealistic_2022, zhang_opt_2022}. 
Oftentimes, however,  large datasets are not available, limiting the performance of deep learning models. To overcome this limitation, researchers explored ways of artificially increasing the size of the training data by transforming the input images via \emph{handcrafted} data augmentations.
These augmentation techniques consist of heuristics involving different types of image distortion~\citep{shorten_survey_2019}, including random erasing \citep{devries_improved_2017, zhong_random_2020}, and image mixing \citep{yun_cutmix_2019, zhang_mixup_2017}. 
It is important to note that all current state-of-the-art representation learning models seem to benefit from such complex data augmentation recipes as they help regularizing models -- e.g., vision transformers trained with supervision~\citep{dosovitskiy_image_2021, touvron_training_2021, steiner_how_2021, touvron_deit_2022} and models trained with self-supervision~\citep{chen_simple_2020, he_momentum_2020, caron_unsupervised_2020, caron_emerging_2021, grill_bootstrap_2020-1}. However, coming up with data augmentation recipes is laborious and the augmented images, despite being helpful, often look unrealistic, see first five images in Figure~\ref{fig:da_comparison}. Such a lack of realism is a sub-optimal effect of these heuristic data augmentation strategies, which turns out to be even detrimental when larger training datasets are available~\citep{steiner_how_2021} and no strong regularization is needed.\looseness-1

\input{fig_da_comparison}

To this end, researchers have tried to move away from training exclusively on real dataset samples and their corresponding hand-crafted augmentations, and have instead explored increasing the dataset sizes with generative model samples~\citep{frid-adar_gan-based_2018, bowles_gan_2018,ravuri_classification_2019,zhang_datasetgan_2021-2,li_semantic_2021}. 
A generative model can  potentially provide infinitely many synthetic image samples; however, the quality and diversity of the generated samples is usually a limiting factor, resulting in moderate gains for specific tasks like image segmentation~\citep{zhang_datasetgan_2021-2,li_semantic_2021} and in poor performance in standard image classification benchmarks~\citep{ravuri_classification_2019}. With the advent of photorealistic image samples obtained with generative adversarial networks (GAN)~\citep{goodfellow_generative_2014}, researchers have explored the use of GAN-based data augmentation~\citep{antoniou_data_2018, tritrong_repurposing_2021, mao_generative_2021, wang_regularizing_2021}. However, none of these approaches has shown improvement when applied to large-scale datasets, such as ImageNet~\citep{deng09cvpr}, in most cases due to a lossy and computationally intensive GAN-inversion step.











In this paper, we study the use of Instance-Conditioned GAN (IC-GAN)~\citep{casanova_instance-conditioned_2021}, a generative model that, conditioned on an image, generates samples that are semantically similar to the conditioning image.
Thus, we propose to leverage \icgan to generate plausible augmentations of each available datapoint and design a module, called \ours, that can be coupled off-the-shelf with most supervised and self-supervised data augmentation strategies and training procedures.
We validate the proposed approach by training supervised image classification models of increasing capacity on the ImageNet dataset and evaluating them in distribution and out-of-distribution. Our results highlight the benefits of leveraging \ours, by outperforming strong baselines when considering high-capacity models, and by achieving robust representations exhibiting increased invariance to viewpoint and instance. We further couple \ours with a self-supervised learning model and show that we can also boost its performance in some settings.

Overall, the contributions of this work can be summarized as follows:
\begin{itemize}

    \item We introduce \ours, a data augmentation module that combines IC-GAN with handcrafted data augmentation techniques and that can be plugged off-the-shelf into most supervised and self-supervised training procedures.\looseness-1
    
    \item We find that using \ours in the supervised training scenario is beneficial for high-capacity networks, e.g., ResNet-152, ResNet-50W2, and DeIT-B, boosting in-distribution performance and robustness to out-of-distribution when combined with traditional data augmentations like random crops and RandAugment. %

    \item We extensively explore \oursâ€™s impact on the learned representations, we discover an interesting correlation between per-class FID and classification accuracy, and report promising results in the self-supervised training of SwAV when not used in combination with multi-crop.\looseness-1


    

    
    

\end{itemize}




 






