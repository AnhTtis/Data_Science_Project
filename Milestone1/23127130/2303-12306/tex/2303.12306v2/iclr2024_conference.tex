
\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

% \usepackage{hyperref}
\usepackage{url}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
% \usepackage[dvipsnames]{xcolor}         % colors
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{pifont}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{natbib}
% \setcitestyle{numbers,square,comma}
\usepackage{wrapfig}
\usepackage[normalem]{ulem}
\usepackage{color}
\usepackage{xcolor}
\usepackage{makecell}
\usepackage{array}

\title{Understanding Expressivity of GNN in Rule Learning}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\iclrfinalcopy

\author{Haiquan Qiu$^1$, Yongqi Zhang$^2$, Yong Li$^1$, Quanming Yao$^1$\footnotemark[1] \\
$^1$Department of Electronic Engineering, Tsinghua University\\
$^2$The Hong Kong University of Science and Technology (Guangzhou) \\
\url{qyaoaa@tsinghua.edu.cn} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{observation}[theorem]{Observation}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem*{remark*}{Remark}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Quanming Yao is the corresponding author.}
\renewcommand{\thefootnote}{\arabic{footnote}}

\begin{abstract}
Rule learning is critical to improving knowledge graph (KG) reasoning due to their ability to provide logical and interpretable explanations.
Recently, Graph Neural Networks (GNNs) with tail entity scoring achieve the state-of-the-art performance on KG reasoning.
However, 
the theoretical understandings for
these GNNs are either lacking or focusing on single-relational graphs,
leaving what the kind of rules these GNNs can learn an open problem.
We propose to fill the above gap in this paper.
Specifically, GNNs with tail entity scoring are unified into a common framework. Then, we analyze their expressivity by formally describing the rule structures they can learn and theoretically demonstrating their superiority.
These results further inspire us to propose a novel labeling strategy 
to learn more rules in KG reasoning.
Experimental results are consistent with our theoretical findings and verify the effectiveness of our proposed method.
The code is publicly available at \url{https://github.com/LARS-research/Rule-learning-expressivity}.
\end{abstract}


\section{Introduction}

A knowledge graph~(KG)~\citep{battaglia2018relational, ji2021survey}
is a type of graph where edges represent multiple types of relationships between entities. 
These relationships can be of different types, such as friend, spouse, coworker, or parent-child, and each type of relationship is represented by a separate edge. 
By encapsulating the interactions among
entities, KGs provide a way for machines to understand and
process complex information.
KG reasoning refers to the task of deducing new facts from the existing facts in
KG.
This task is important because it helps in many real-world applications, such as recommendation systems~\citep{cao2019unifying} and drug discovery~\citep{Mohamed2019DiscoveringPD}.


With the success of graph neural networks (GNNs) in modeling graph-structured data, 
GNNs have been developed for KG reasoning in recent years.
Classical methods such as R-GCN~\citep{schlichtkrull2018modeling} and CompGCN~\citep{vashishth2019composition} are proposed for
KG reasoning by aggregating the representations of two end entities of a triplet.
And they are known to fail to distinguish the structural role of different neighbors.
GraIL~\citep{teru2020inductive} and RED-GNN~\citep{zhang2022knowledge} tackle this problem by encoding the subgraph around the target triplet. GraIL predicts a new triplet using the subgraph representations, while RED-GNN employs dynamic programming for efficient subgraph encoding.
Motivated by the effectiveness of heuristic metrics over paths between a link, NBFNet~\citep{zhu2021neural} proposes a neural network based on Bellman-Ford algorithm for KG reasoning.
AdaProp~\citep{zhang2023adaprop} and
A$^\star$Net~\citep{zhu2022scalable} enhance 
the scalability of RED-GNN and NBFNet respectively 
by selecting crucial nodes and edges iteratively.
Among these methods, NBFNet, RED-GNN and their variants score a triplet with its tail entity representation and achieve state-of-the-art (SOTA) performance on KG reasoning.
However, 
these methods are motivated by different heuristics, e.g., Bellman-Ford algorithm and enclosing subgraph encoding, which make the understanding of their effectiveness for KG reasoning difficult.


In this paper, inspired by the importance of rule learning in KG reasoning,
we propose to study expressivity of SOTA GNNs for KG reasoning by analyzing the kind of rules they can learn.
First, we unify SOTA GNNs for KG reasoning into 
a common framework called QL-GNN, based on the observation that they score a triplet with its tail entity representation and essentially extract rule structures from subgraphs with same pattern. Then, we analyze the logical expressivity of QL-GNN to study its ability of learning rule structures.
The analysis helps us reveal the underneath theoretical reasons that contribute to the empirical success of QL-GNN, elucidating their effectiveness over classical methods.
Specifically,
our analysis is based on the formal description of rule structures in graph,
which differs from previous analysis that relies on graph isomorphism testing~\citep{xu2018powerful,zhang2021labeling} and focuses on the expressivity of distinguishing various rules. 
The new analysis tool allows us to understand the rules learned by QL-GNN and reveals the maximum expressivity that QL-GNN can generalize through training.
Based on the new theory, we also uncover the deficiencies of QL-GNN in learning rule structures and we propose EL-GNN based on labeling trick as an improvement upon QL-GNN to improve its learning ability.
In summary, our paper has the following contributions:


\begin{itemize}[leftmargin=*]
\item Our work unifies state-of-the-art GNNs for KG reasoning into a common framework named QL-GNN, 
and analyzes their logical expressivity to study their ability of learning rule structures, explaining their superior performance over classical methods.

\item The logical expressivity of QL-GNN demonstrates its capability in learning a particular class of rule structures. Consequently, based on further theoretical analysis, we introduce EL-GNN, a novel GNN designed to learn rule structures that are beyond the learning capacity of QL-GNN.


\item Synthetic datasets are generated to evaluate the expressivity of various GNNs, whose experimental results are consistent with our theory. Also, results of the proposed labeling method show improved performance on real datasets.


\end{itemize}




\begin{figure*}[t]
	\centering
	\includegraphics[width=1.0\textwidth]{img/mainfig.pdf}
	\caption{The existence of a triplet in KG is determined by the corresponding rule structure. 
		We investigates the kind of rule structures can be learned by SOTA GNNs for KG reasoning (i.e., QL-GNN), 
		and proposes EL-GNN, which can learn more rule structures compared to QL-GNN.}
	\label{fig:overview}
\end{figure*}



\section{A common framework for the state-of-the-art methods}\label{sec:cat}


To study the state-of-the-art GNNs for KG reasoning, we find that they (e.g., RED-GNN and NBFNet) essentially learn rule structures from GNN's tail entity representation which encodes subgraphs
with the same pattern, 
i.e., a subgraph with the query entity as the source node and the tail entity as the sink node. 
Based on this observation, we are motivated to derive a common framework for these SOTA methods 
and analyze their ability of learning rule structures with the derived framework.


Given a query $(h,R,?)$, the labeling trick of query entity $h$ ensures the SOTA methods to extract rules from a graph with the same pattern because it makes the query entity distinguishable among all entities in graph. Therefore, we unify NBFNet, RED-GNN and their variants 
to a common framework called Query Labeling (QL) GNN (see correspondence in Appdendix~\ref{app:sec:ql-gnn}).
For a query $(h, R, ?)$, QL-GNN first applies labeling trick by assigning special initial representation $\mathbf{e}_{h}^{(0)}$ to entity $h$, which make the query entity distinguishable from other entities.
Base on these initial features, QL-GNN aggregates entity representations with a $L$-layer message passing neural network (MPNN) for each candidate $t\in \mathcal V$.
MPNN's last layer representation of entity $t$ in QL-GNN is denoted as $\mathbf{e}_t^{(L)}[h]$ indicating its dependency on query entity $h$.
Finally, QL-GNN scores new facts $(h, R, t)$ with tail entity representation $\mathbf{e}_t^{(L)}[h]$.
For example, NBFNet uses the score function $s(h,R,t)=\text{FFN}(\mathbf{e}_t^{(L)}[h])$ for new triplet $(h,R,t)$ where $\text{FFN}(\cdot)$ denotes a feed-forward neural network.

Even RED-GNN, NBFNet and their variant may take the different MPNNs to calculate $\mathbf{e}_t^{(L)}[h]$, without loss of generality, their MPNNs can take the following form in QL-GNN (omit $[h]$ for simplicity):
\begin{align}
	\label{eq:GNN}
	\mathbf{e}_v^{(k)}
	=
	\delta 
	\Big( 
	\mathbf{e}_v^{(k-1)}, \phi
	\left(
	\big\{
	\{ \psi(\mathbf{e}_u^{(k-1)}, R) | 
	u \in \mathcal{N}_R(v), R\in \mathcal{R} 
	\}
	\big\}
	\right) 
	\Big),
\end{align}
where $\delta$ and $\phi$ are combination and aggregation functions respectively, 
$\psi$ is the message function encoding the relation $R$ and entity $u$ neighboring to $v$, 
$\{\{\cdots\}\}$ is a multiset, 
and 
$\mathcal{N}_R(v)$ is the neighboring entity set $\{ u | (u,R,v) \in \mathcal{E} \}$.


\section{Expressivity of QL-GNN}\label{sec:gnn-exp}



{In this section, we explore the logical expressivity of QL-GNN to analyze the types of rule structures QL-GNN can learn.} {First, we provide the logic to describe rules in KGs.}
Then, we analyze {logical expressivity} of QL-GNN using Theorem~\ref{theorem:QL} and Corollary~\ref{coro:QL}, formally demonstrating the kind of rule structures it can learn. Finally, we compare QL-GNN with classical methods and highlight its superior expressivity in KG reasoning.


\subsection{{Expressivity analysis with logic of rule structures}}\label{ssec:formal-description-of-rule-structures}


From previous works of rule mining on KG~\citep{yang2017differentiable,sadeghian2019drum}, rule structures are usually described as a formula in first-order logic.
We also follow this way to formally describe the rule structures in KG. Therefore, we have the following correspondence between the elements in rule structures and logic:

\begin{itemize}[leftmargin=*]
  \item Variable: variables denoted with lowercase italic letters $x,y,z$ represent entities in a KG;
  \item Unary predicate: unary predicate $P_i(x)$ is corresponding to the entity property $P_i$ in a KG, e.g., $\text{red}(x)$ denotes the color of an entity $x$ is red;
  \item Binary predicate: binary predicate $R_j(x,y)$ is corresponding to the relation $R_j$ in a KG, e.g., $\text{father}(x,y)$ denotes $x$ is the father of $y$;
  \item Constant: constant denoted with lowercase letters $\mathsf{h},\mathsf{c}$ with serif typestyle is the unique identifier of some entity in a KG.
\end{itemize}

Except from the above elements, the quantifier $\exists$ expresses the existence of entities satisfying a condition, $\forall$ expresses universal quantification, and $\exists^{\geq N}$ represents the existence of at least $N$ entities satisfying a condition. The logical connective $\wedge$ denotes conjunction, $\vee$ denotes disjunction, and $\top$ and $\bot$ represent true and false, respectively. Using these symbols, rule structures can be represented by describing their elements directly. 
For example, $C_{3}(x, y):= \exists z_1 z_2, R_1(x, z_1) \wedge R_2(z_1, z_2) \wedge R_3(z_2, y)$ in Figure~\ref{fig:example} describes a chain-like structure between $x$ and $y$ with three relations $R_1, R_2, R_3$.
Rule structures can be represented using the rule formula $R(x,y)$, and the existence of a rule structure for the triplet $(h,R,t)$ is equivalent to the satisfaction of the rule formula $R(x,y)$ at the entity pair $(h,t)$.
{In this paper, 
	\textbf{logical expressivity} of GNN is a measurement of the ability of GNN to learn logical formulas and is defined as the set of logical formulas that GNN can learn.
Therefore, since rule structures can be described by logical formulas, the logical expressivity of QL-GNN can determine their ability to learn rule structures in KG reasoning.}



\subsection{What kind of rule structures can QL-GNN learn?}

In this section, we analyze the logical expressivity of QL-GNN regarding what kind of rule structure it can learn.
Given a query $(h,R,?)$, 
we first have the following proposition about the rule formula describing a rule structure.

\begin{proposition}
\label{prop:QL}
The rule structure for query $(h,R,?)$ can be described with rule formula $R(x,y)$ or rule formula $R(\mathsf{h},x)$
\footnote{The rule formula $R(\mathsf{h}, x)$ is equivalent to $\exists z R(z,x)\wedge P_h(z)$ where $P_h(x)$ denotes the assignment of constant $\mathsf{h}$ to $x$ and is called constant predicate in our paper.}
where $\mathsf{h}$ is the logical constant assigned to query entity $h$.
\end{proposition}

QL-GNN applies labeling trick to the query entity $h$, which can be equivalently seen as assigning constant $\mathsf{h}$ to query entity $h$\footnote{The initial representation of an entity should be unique among all entities to be regarded as constant in logic. The initial representation assigned to query entity are indeed unique in NBFNet, RED-GNN and their variants.}.With Proposition~\ref{prop:QL} (proven in Appendix~\ref{app:sec:rule-analysis}), {the logical expressivity of QL-GNN can be analyzed by the types of rule formula $R(\mathsf{h}, x)$ it can learn.}
In this case, the rule structure of triplet $(h,R,t)$ exists if and only if the logical formula $R(\mathsf{h},x)$ is satisfied at entity $t$.


\subsubsection{Expressivity of QL-GNN}
Before presenting the {logical} expressivity of QL-GNN, we start by explaining how QL-GNN learns the rule formula $R(\mathsf{h}, x)$. Following the definition in \citet{barcelo2020logical}, we treat $R(\mathsf{h}, x)$ as a binary classifier. When given a candidate tail entity $t$, if the triplet $(h,R,t)$ exists in a KG, the binary classifier $R(\mathsf{h}, x)$ should output true; otherwise, it should output false.
If QL-GNN can learn the rule formula $R(\mathsf{h}, x)$, it implies that QL-GNN can estimate binary classifier $R(\mathsf{h}, x)$.
Consequently, if the rule formula $R(\mathsf{h}, x)$ is satisfied at entity $t$, the representation $\mathbf{e}_t^{(L)}[h]$ is mapped to a high probability value, indicating the existence of triplet $(h,R,t)$ in KG.
Conversely, when the rule formula is not satisfied at $t$, $\mathbf{e}_t^{(L)}[h]$ is mapped to a low probability value, indicating the absence of the triplet.

The rule structures that QL-GNN can learn are described by a family of logic called graded modal logic (CML)~\citep{de2000note,otto2019graded}.
CML is defined by recursion with the base elements $\top, \bot$, all unary predicates $P_i(x)$, and the recursion rule: if $\varphi(x), \varphi_1(x), \varphi_2(x)$ are formulas in CML, $\neg \varphi(x), \varphi_1(x) \wedge \varphi_2(x), \exists^{\geq N}y \left( R(y, x) \wedge \varphi(y) \right)$ are also formulas in CML. Since QL-GNN introduces a constant $\mathsf{h}$ to the query entity $h$, we use the notation $\text{CML}[G, \mathsf{h}]$ to denote the CML recursively built from base elements in $G$ and constant $\mathsf{h}$ (equivalent to constant predicate $P_h(x)$).
Then, the following theorem and corollary show the expressivity of QL-GNN for KG reasoning.

\begin{theorem}[Logical expressivity of QL-GNN]\label{theorem:QL}
For KG reasoning, given a query $(h, R, ?)$, 
a rule formula $R(\mathsf{h}, x)$ is learned by QL-GNN if and only if $R(\mathsf{h}, x)$ is a formula in $\text{CML}[G, \mathsf{h}]$.
\end{theorem}


\begin{corollary}\label{coro:QL}
The rule structures learned by QL-GNN can be constructed with the recursion:
\vspace{-8px}
\begin{itemize}[leftmargin=*]
  \item \textbf{Base case:} all unary predicates $P_i(x)$ can be learned by QL-GNN; the constant predicate $P_h(x)$ can be learned by QL-GNN;
  
  \vspace{-8px}
  \item \textbf{Recursion rule:} if the rule structures $R_1(\mathsf{h}, x), R_2(\mathsf{h}, x), R(\mathsf{h}, y)$ are learned by QL-GNN, $R_1(\mathsf{h}, x) \wedge R_2(\mathsf{h}, y)$, $\exists^{\geq N}y \left( R_i(y, x) \wedge R(\mathsf{h}, y) \right)$ are learned by QL-GNN.
\end{itemize}
\end{corollary}


Theorem~\ref{theorem:QL} (proved in Appendix~\ref{app:proof}) provides the {logical} expressivity of QL-GNN with rule formula $R(\mathsf{h}, x)$ in $\text{CML}[G,\mathsf{h}]$, which shows that querying labeling transforms $R(x,y)$ to $R(\mathsf{h}, x)$ and enable QL-GNN to learn the corresponding rule structure.
To gain a concrete understanding of the rule structures learned by QL-GNN, Corollary~\ref{coro:QL} provides the recursive definition for these rule structures.
{
Note that Theorem~\ref{theorem:QL} cannot be directly applied to analyze the expressivity of QL-GNN 
when learning more than one rule structures.
The ability of learning more than one rule structures relates to the capacity of QL-GNN, which we take as a future direction.
Theorem~\ref{theorem:QL} also reveals the maximum expressivity that QL-GNN can generalize through training, and its proof also provides some insights about the design QL-GNN with better generalization
(more discussions are provided in Appendix~\ref{app:sec:QL-generalization}).
}
Besides, 
our results in this section can be 
reduced to single relational-graph by restricting the relation type to a single relation type, 
and we give these results as corollaries in Appendix~\ref{app:sec:single}.

\subsubsection{Examples}\label{ssec:example}

We analyze several rule structures and their corresponding rule formulas in Figure~\ref{fig:example} as illustrative examples, 
demonstrating the application of our theory in analyzing the rule structures that QL-GNN can learn.
The real examples of these rule structures are shown in Figure~\ref{fig:overview}.
{In Appdendix~\ref{app:sec:rule-analysis}, we have detailed analysis of rule structures discussed in the paper and present some rules from real datasets.}



Chain-like rules, e.g., $C_3(x,y)$ in Figure~\ref{fig:example}, are basic rule structures investigated in many previous works~\citep{sadeghian2019drum,teru2020inductive,zhu2021neural}.
QL-GNN assigns constant $\mathsf{h}$ to query entity $h$, 
thus triplets with relation $C_3$ can be predicted by learning the rule formula $C_3(\mathsf{h}, x)$.
$C_3(\mathsf{h}, x)$ are formulas in $\text{CML}[G,\mathsf{h}]$ and can be recursively defined with rules in Corollary~\ref{coro:QL} (proven in Corollary~\ref{app:corollary:chain}). 
Therefore, our theory gives a general proof of QL-GNN's ability to learn chain-like structures.

\begin{wrapfigure}{f}{0.65\columnwidth}
	\centering
%    \vspace{-20px}
	\includegraphics[width=.65\columnwidth]{img/sec4.pdf}
   \vspace{-20px}
	\caption{Example of rule structures and their corresponding rule formulas QL-GNN can learn.}
   \vspace{-10px}
	\label{fig:example}
\end{wrapfigure}

The second type of rule structure $I_1(\mathsf{h}, x)$ in Figure~\ref{fig:example} is composed of a chain-like structure from query entity to tail entity
along with additional
entity $z_2$
connected to the chain.
$I_1(\mathsf{h}, x)$ are formulas in $\text{CML}[G,\mathsf{h}]$ and can be defined with recursive rules in Corollary~\ref{coro:QL} (proven in Corollary~\ref{app:corollary:inductive_1}), which indicates that $I_1(\mathsf{h}, x)$ can be learned by QL-GNN.
These structures are important in KG reasoning because the entity connected to the chain can bring extra information about property of the entity it connected to (see examples of rule in Appendix~\ref{app:sec:rule-analysis}).






\subsection{Comparison with classical methods}

Classical methods such as R-GCN and CompGCN perform KG reasoning by first applying MPNN (\ref{eq:GNN}) to compute the entity representations $\mathbf{e}_v^{(L)}, v\in\mathcal{V}$ and then scoring the triplet $(h, R, t)$ by $s(h,R,t)=\text{Agg}(\mathbf{e}_h^{(L)}, \mathbf{e}_t^{(L)})$ with aggregation function $\text{Agg}(\cdot,\cdot)$. For simplicity, we take CompGCN as an example to analyze the expressivity of the classical methods on learning rule structures. 


Since CompGCN scores a triplet using its query and tail entity representations without applying labeling trick, 
the rule structures learned by CompGCN should be in the form of $R(x,y)$.
In CompGCN, the query and tail entities' representations encode different subgraphs. 
However, the joint subgraph they represent may not necessarily be connected. 
This suggests that the rule structures learned by CompGCN are non-structural, indicating there is no path between its query and tail entities except for relation $R$. This observation is proven with the following theorem.
\begin{theorem}[{Logical expressivity of CompGCN}]
   \label{theorem:EA}
   For KG reasoning, 
   CompGCN can learn the rule formula
   $R(x, y)=f_R\left(\{\varphi(x)\}, \{\varphi^\prime(y)\}\right)$
   where $f_R$ is a formula involving sub-formulas from $\{\varphi(x)\}$ and $\{\varphi^\prime(y)\}$ which are the sets of formulas in $\text{CML}[G]$.
\end{theorem}

{
\begin{remark*}
	Theorem~\ref{theorem:EA} indicates that representations of two end entities encoding two formulas respectively, and these two formulas are independent. Thus, the rule structures learned by CompGCN should be two disconnected subgraphs surrounding the query and tail entities respectively.
\end{remark*}
}
Similar to Theorem~\ref{theorem:QL}, CompGCN learns rule formula $R(x, y)$ by treating it as a binary classifier.
In a KG, the binary classifier $R(x,y)$ should output true if the triplet $(h,R,t)$ exists; otherwise, it should output false.
If CompGCN can learn the rule formula $R(x,y)$, it implies that it can estimate the binary classifier $R(x, y)$.
Consequently, if the rule formula $R(x, y)$ is (not) satisfied at entity pair $(h,t)$, the score $s(h,R,t)$ is a high (low) value, indicating the existence (absence) of triplet $(h,R,t)$.


Theorem~\ref{theorem:EA} (proven in Appendix~\ref{app:proof}) shows that CompGCN can only learn rule formula $R(x,y)$ for non-structural rules. 
One important type of relation in this category is the similarity between two entities (experiments in Appendix~\ref{app:exp:EA-GNN}), like $\texttt{same\_color}(x,y)$ indicating entities with the same color.
However, structural rules are more commonly observed in KG reasoning~\citep{lavrac1994inductive,sadeghian2019drum,srinivasan2019equivalence}.
{
Since Theorem~\ref{theorem:EA} indicates CompGCN fails to learn connected rule structures that are not independent, the structural rules in Figure~\ref{fig:example} cannot be learned by CompGCN.
}
Such a comparison shows why QL-GNN is more efficient than classical methods, e.g., R-GCN and CompGCN, in real applications.
Compared with previous work on single-relational graphs, \citet{zhang2021labeling} shows CompGCN cannot distinguish many non-isomorphic links, while our paper derives expressivity of CompGCN for learning rule structures.


\section{Entity Labeling GNN based on rule formula transformation}\label{sec:complex-rule}


QL-GNN is proven to be able to learn the class of rule structures defined in Corollary~\ref{coro:QL}.
For rule structures outside this class, we try to learn them with a novel labeling trick based on QL-GNN.
Our general idea is to transform the rule structures outside this class into the rule structures in this class by adding constants to the graph.
The following proposition and corollary show how to add constants to a rule structure so that it can be described by formulas in CML and how to apply labeling trick to make it learnable for QL-GNN.


\begin{proposition}\label{proposition:label}
Let $R(\mathsf{h}, x)$ describe a single-connected rule structure $\mathsf{G}$ in $G$. 
If we assign constants $\mathsf{c}_1, \mathsf{c}_2, \cdots, \mathsf{c}_k$ to all $k$ entities with out-degree larger than one in $\mathsf{G}$, the rule structure $\mathsf{G}$ can be described with a new rule formula $R'(\mathsf{h}, x)$ in $\text{CML}[G,\mathsf{h},\mathsf{c}_1, \mathsf{c}_2, \cdots, \mathsf{c}_k]$.
\end{proposition}

\begin{corollary}\label{coro:label}
Applying labeling trick with unique initial representations to entities assigned with constants $\mathsf{c}_1, \mathsf{c}_2, \cdots, \mathsf{c}_k$ in Proposition~\ref{proposition:label}, the rule structure $\mathsf{G}$ can be learned by QL-GNN.
\end{corollary}


For instance, in Figure~\ref{fig:sec5}, the rule structure $U$ cannot be distinguished from the rule structure $T$ by recursive definition in Corollary~\ref{coro:QL}, thus cannot be learned by QL-GNN.
In this example, 
Proposition~\ref{proposition:label} suggests assigning constant $\mathsf{c}$ to the entity colored with gray in Figure~\ref{fig:sec5}, then a new rule formula
\begin{align*}
U'(\mathsf{h}, x) := R_1(\mathsf{h}, \mathsf{c}) \wedge        \big( \exists z_2, z_3, R_2(\mathsf{c}, z_2) \wedge R_4(z_2, x) \wedge R_3(\mathsf{c}, z_3) \wedge R_5(z_3, x) \big)  
\end{align*}
in $\text{CML}[G, \mathsf{h}, \mathsf{c}]$ (Corollary~\ref{app:corollary:labeling}) can describe the rule structure of $U$. 
Therefore, the rule structure of $U$ can be learned with $U'(\mathsf{h}, x)$ by QL-GNN with constant $\mathsf{c}$ and cannot be learned by classical methods and vanilla QL-GNN.



\begin{algorithm}[h]
	\caption{Entity Labeling}
	\begin{algorithmic}[1]
		\REQUIRE query $(h, R, ?)$, knowledge graph $G$, degree threshold $d$.
		\STATE compute the out-degree $d_v$ of each entity $v$ in $G$;
		\FOR{entity $v$ in $G$}\label{step:labeling_start}
			\IF{$d_v > d$}
				\STATE assign a unique representation $\mathbf{e}_v^{(0)}$ to entity $v$;
			\ENDIF
		\ENDFOR\label{step:labeling_end}
		\STATE assign initial representation $\mathbf{e}_h^{(0)}$ to the query entity $h$;\label{step:query_labeling}
	\STATE \textbf{Return:} initial representation of all entities.
	\end{algorithmic}
	\label{alg:labeling}
\end{algorithm}

\begin{wrapfigure}{f}{0.65\columnwidth}
   \centering
   \vspace{-15px}
   \includegraphics[width=.65\columnwidth]{img/sec5.pdf}
   \vspace{-20px}
   \caption{Two rule structures cannot be distinguished by QL-GNN.}
   \label{fig:sec5}
   \vspace{-10px}
\end{wrapfigure}

Based on Corollary~\ref{coro:label},
we need apply labeling trick to entities other than the query entities in QL-GNN to learn the rule structures outside the scope of Corollary~\ref{coro:QL}.
The new method is called Entity-Labeling (EL) GNN shown in Algorithm~\ref{alg:labeling} and is different from QL-GNN in assigning constants to all the entities with out-degree larger than $d$.
We choose the degree threshold $d$ as a hyperparameter because a small $d$ (such as $1$) will introduce too many constants to KG, which impedes the generalization of GNN~\citep{abboud2020surprising} (see an explanation from logical perspective in Appendix~\ref{app:sec:many-features}).
In fact, a smaller $d$ makes GNN learn the rule formulas with many constants and results bad generalization,
while a larger $d$ may not be able to transform indistinguishable rules into formulas in CML.
As a result, the degree threshold $d$ should be tuned to balance the expressivity and generalization of GNN.
Same as the constant $\mathsf{h}$
in QL-GNN,
we add a unique initial representation $\mathbf{e}_v^{(0)}$ for entities $v$ whose out-degree $d_v>d$ in steps~3-5.
For the query entity $h$, 
we assign it 
with a unique initial representation $\mathbf{e}_h^{(0)}$ in step~7.
{
In Algorithm~\ref{alg:labeling}, it can be seen that the additional time of EL-GNN comes from traversing all entities in the graph. The additional time complexity is linear with respect to the number of entities, which is negligible compared to QL-GNN.
}
For convenience, GNN initialized with EL algorithm is denoted as EL-GNN (e.g., EL-NBFNet) in our paper.


\paragraph{Discussion}

In Figure~\ref{fig:overview}, 
we visually compare the expressivity of QL-GNN and EL-GNN.
Classical methods, e.g., R-GCN and CompGCN, are not compared here because they can solely learn non-structural rules which are not commonly-seen in real applications.
QL-GNN, e.g., NBFNet and RED-GNN, excels at learning rule structures described by formula $R(\mathsf{h}, x)$ in $\text{CML}[G, \mathsf{h}]$. 
The proposed EL-GNN, encompassing QL-GNN as a special case, can learn rule structures described by formula $R(\mathsf{h},x)$ in $\text{CML}[G, \mathsf{h}, \mathsf{c}_1, \cdots, \mathsf{c}_k]$ which has a larger description scope than $\text{CML}[G, \mathsf{h}]$.






\section{Related Works}


\subsection{Expressivity of Graph Neural Network~(GNN)}
\label{sec:rel:gnn}

GNN~\citep{kipf2016semi,gilmer2017neural} has shown good performance on a wide range of tasks involving graph-structured data, thus many existing works try to analyze the expressivity of GNNs.
Most of these works analyze the expressivity of GNNs
from the perspective of graph isomorphism testing.
A well-known result~\citep{xu2018powerful} shows that the expressivity of vanilla GNN
is limited to 
WL test and the result is extended to KG by \citet{barcelo2022weisfeiler}.
To improve the expressivity of GNNs, 
most of the existing works either design GNNs motivated by high-order WL test~\citep{morris2019weisfeiler,morris2020weisfeiler,barcelo2022weisfeiler} 
or apply special initial representations~\citep{abboud2020surprising,you2021identity,sato2021random,zhang2021labeling}.
Except for using graph isomorphism testing, 
\citet{barcelo2020logical} analyze the {logical expressivity of GNNs} and identify that the logical rules from graded modal logic can be learned by vanilla GNN.
However, their analysis is limited to node classification on the single-relational graph.
Except from the expressivity of vanilla GNN, \citet{tena2022explainable} propose monotonic GNN whose prediction can be explained by symbolical rules in Datalog and the expressivity of monotonic GNN is further analyzed in \citet{cucala2023correspondence}.


Regarding the expressivity of GNNs for link prediction, \citet{srinivasan2019equivalence} demonstrate that GNNs' structural node representations alone are insufficient for accurate link prediction. To overcome this limitation, they introduce a method that incorporates Monte Carlo samples of node embeddings obtained from network embedding techniques instead of relying solely on GNNs.
However, \citet{zhang2021labeling} discovered that by leveraging the labeling trick in GNNs, 
it is indeed possible to learn structural link representations for effective link prediction. 
This finding provides reassurance regarding the viability of GNNs for this task.
Nonetheless, their analysis is confined to single-relational graphs, and their conclusions are limited to the fact that the labeling trick enables distinct representations for some non-isomorphic links, which other approaches cannot achieve.
In this paper, we delve into the analysis of GNNs' {logical expressivity to study their ability of learning rule structures}. By doing so, we aim to gain a comprehensive understanding of the rule structures that SOTA GNNs can learn in graphs. 
Our analysis encompasses both single-relational graph and KGs, thus broadening the applicability of our findings.

A concurrent work by \citet{huang2023theory} analyzes the expressivity of GNNs for NBFNet (a kind of QL-GNN in our paper) with conditional MPNN while our work unifies state-ot-the-art GNNs into QL-GNN and analyzes the expressivity from a different perspective focusing on the understanding of relationship between labeling trick and constants in logic.

\subsection{Knowledge graph reasoning}
\label{sec:rel:kgr}

KG reasoning is the task to predict new facts based on the known facts in a KG $G=(\mathcal{V}, \mathcal{E}, \mathcal{R})$ where $\mathcal{V},\mathcal{E},\mathcal{R}$ are sets of entities, edges and relation types in the graph respectively.
The facts (or edges, links) are typically expressed as triplets in the form of $(h,R,t)$,
where the head entity $h$ and tail entity $t$ are related with the relation type $R$.
KG reasoning can be modeled as the process of predicting the tail entity $t$ of a query in the form $(h,R,?)$ where $h$ is called the query entity in our paper.
The head prediction $(?,R,t)$ can be transformed into tail prediction $(t,R^{-1},?)$ with inverse relation $R^{-1}$. 
Thus, we focus on tail prediction in this paper.

Embedding-based methods like TransE~\citep{bordes2013translating}, ComplEx~\citep{trouillon2016complex}, RotatE~\citep{sun2019rotate}, 
and QuatE~\citep{zhang2019quaternion} have been developed for KG reasoning. 
They learn embeddings for entities and relations,
and predict facts by aggregating their representations.
To capture local evidence within graphs, Neural LP~\citep{yang2017differentiable} and DRUM~\citep{sadeghian2019drum} 
learn logical rules based on predefined chain-like structures.
However, apart from chain-like rules, these methods failed to learn more complex structures in KG~\citep{hamilton2018embedding,ren2019query2box}. GNNs have also been used for KG reasoning, 
such as R-GCN~\citep{schlichtkrull2018modeling} and CompGCN~\citep{vashishth2019composition}, 
which aggregate entity and relation representations to calculate scores for new facts.
However, these methods struggle to differentiate between the structural roles of different neighbors~\citep{srinivasan2019equivalence,zhang2021labeling}. 
GraIL~\citep{teru2020inductive} addresses this by extracting enclosing subgraphs to predict new facts, 
while RED-GNN~\citep{zhang2022knowledge} employs dynamic programming for efficient subgraph extraction and predicts new facts based on the tail entity representation.
To extract relevant structures from graph, AdaProp~\citep{zhang2023adaprop} improves RED-GNN by employing adaptive propagation to filter out irrelevant entities and retain promising targets.
Motivated by the effectiveness of heuristic path-based metrics for link prediction, 
NBFNet~\citep{zhu2021neural} proposes a neural network aligned with Bellman-Ford algorithm for KG reasoning.
\citet{zhu2022scalable} propose A$^\star$Net to learn a priority function to select important nodes and edges at each iteration.
Specifically, AdaProp and A$^\star$Net are variants of RED-GNN and NBFNet, respectively, designed to enhance their scalability.
Among these methods, RED-GNN, NBFNet, AdaProp, and A$^\star$Net achieve state-of-the-art performance on KG reasoning.



\section{Experiment}

In this section, we validate our theoretical findings from Section~\ref{sec:gnn-exp} and showcase the efficacy of our proposed EL-GNN (Section~\ref{sec:complex-rule}) on synthetic and real datasets through experiments. All experiments were implemented in Python using PyTorch and executed on A100 GPUs with 80GB memory.






\subsection{Experiments on synthetic datasets}\label{ssec:synthetic-exp}





We generate six KGs based on rule structures in Figure~\ref{fig:example}, \ref{fig:sec5}, \ref{app:fig:more-rules} to validate our theory on expressivity and verify the improved performance of EL-GNN.
These rule structures are either analyzed in the previous sections, or representative for evaluating GNN's ability for learning rule structures. 
We evaluate R-GCN, CompGCN, RED-GNN, NBFNet, EL-RED-GNN, and EL-NBFNet (using RED-GNN/NBFNet as backbone with Algorithm~\ref{alg:labeling}).
Our evaluation metric is prediction \textit{Accuracy} which measures how well a rule structure is learned. We report testing accuracy of classical methods, QL-GNN, and EL-GNN on six synthetic graphs. Hyperparameters for all methods are automatically tuned with Ray~\citep{liaw2018tune} based on the validation accuracy.

\begin{table}[h]
	\vspace{-15px}
	\centering
	\caption{Accuracy on synthetic data.}
	\small
	\begin{tabular}{c|c|c|c|c|c|c|c}
		\toprule
		Method & Method & $C_3$ & $C_4$ & $I_1$ & $I_2$ & $T$ & $U$ \\ \midrule
		\multirow{2}{*}{Classical}&R-GCN         &      0.016      & 0.031  & 0.044 &    0.024   &          0.067            &        0.014        \\ 
		&CompGCN         &    0.016        &  0.021 & 0.053 &    0.039   &             0.067         &      0.027           \\ \midrule
		\multirow{2}{*}{QL-GNN} & RED-GNN          &  1.0       & 1.0  & 1.0 &   1.0   &   1.0      &     0.405         \\  
		& NBFNet          & 1.0        &  1.0 & 1.0 &  1.0  & 1.0                  &   0.541                   \\   \midrule
		\multirow{2}{*}{EL-GNN} & EL-RED-GNN &  1.0         &  1.0  & 1.0 &  1.0    &         1.0             &          0.797            \\ 
		& EL-NBFNet          & 1.0        & 1.0  &  1.0 &   1.0     & 1.0       &       0.838           \\ \bottomrule
	\end{tabular}\label{tab:syn-data}
	\vspace{-15px}
\end{table}

\paragraph{Dataset generation}
Given a target relation, there are three steps to generate a dataset:
(1) rule structure generation: generate specific rule structures according to their definition; (2) noisy triplets generation: generate noisy triplets to avoid GNN from learning naive rule structures; (3) missing triplets completion: generate missing triplets based on the target rule structure because the noisy triplets generation step could add triplets satisfying the target rule structure.
We use triplets generated from rule structure and noisy triplets generation steps as known triplets in graph. Triplets with the target relation are separated into training, validation, and testing sets. 
Our experimental setting differs slightly from previous works as all GNNs in the experiments only perform message passing on the known triplets in the graph.
This setup is reasonable and allows for evaluating the performance of GNNs in learning rule structures because the presence of a triplet can be determined based on the known triplets in the graph, following the rule structure generation process.



\paragraph{Results}


Table~\ref{tab:syn-data} presents the testing accuracy of classical GNN methods, QL-GNN, and EL-GNN on six synthetic datasets (denoted as $C_3, C_4, I_1, I_2, T,$ and $U$) generated from their corresponding rule structures. The experimental results support our theory. CompGCN performs poorly on all six datasets, as it fails to learn the underlying rule structures discussed in examples of Section~\ref{sec:gnn-exp} (refer to Section~\ref{app:exp:EA-GNN} for experiments of CompGCN). QL-GNN achieves perfect predictions (100\% accuracy) for triplets with relations $C_l, I_i,$ and $T$, successfully learning the corresponding rule formulas from $\text{CML}[G, \mathsf{h}]$. EL-GNN demonstrates improved expressivity, as evidenced by its performance on dataset $U$, aligning with the analysis in Section~\ref{sec:complex-rule}. Furthermore, EL-GNN effectively learns rule formulas $C(\mathsf{h}, x)$ and $I(\mathsf{h}, x)$, validating its expressivity.

\begin{wrapfigure}{f}{0.42\columnwidth}
	\centering
	\vspace{-30px}
	\includegraphics[width=0.39\textwidth]{img/outdegree.pdf}
	\vspace{-10px}
	\caption{Accuracy versus out-degree $d$ of EL-GNN on the dataset with relation $U$.}
	\label{fig:out-degree}
	\vspace{-30px}
\end{wrapfigure}


Furthermore, we demonstrate the impact of the degree threshold $d$ on EL-GNN with dataset $U$. 
The testing accuracy in Figure~\ref{fig:out-degree} reveals that an excessively small or large out-degree $d$ hinders the performance of EL-GNN. 
Therefore, it is important to empirically fine-tune the hyperparameter $d$.
{To test the robustness of QL-GNN and EL-GNN in learning rules with incomplete structures, we randomly remove triplets in the training set to evaluate the accuracy of learning rule structures. The results can be found in Appendix~\ref{app:exp:missing}.}


\subsection{Experiments on real datasets} \label{ssec:real-exp}

In this section, we follow the standard setup as \citet{zhu2021neural} to test EL-GNN's effectiveness on four real datasets: Family~\citep{kok2007statistical}, Kinship~\citep{hinton1986learning}, UMLS~\citep{kok2007statistical}, WN18RR~\citep{dettmers2017convolutional}, {and FB15k-237~\citep{toutanova-chen-2015-observed}}. 
For a fair comparison, we evaluate EL-NBFNet and EL-RED-GNN (applying EL to NBFNet and RED-GNN) using the same hyperparameters as NBFNet and RED-GNN and handcrafted $d$. 
We compare it with embedding-based methods (RotatE, QuatE), rule-based methods (Neural LP, DRUM), and GNN-based methods (CompGCN, NBFNet, RED-GNN). 
To evaluate performance, 
we provide testing accuracy 
and standard deviation obtained from three repetitions for thorough evaluation.


In Table~\ref{tab:real_result}, we present our experimental findings. 
The results first show that NBFNet and RED-GNN (QL-GNN) outperform CompGCN. 
Furthermore, the proposed EL algorithm improves the accuracy of RED-GNN and NBFNet on real datasets. 
However, the degree of improvement varies across datasets due to the number and variations of rule types, and the quality of missing triplets in training sets.
{More experimental results, e.g., time cost and more performance metrics, are in Appendix~\ref{app:exp:real}.
}

\begin{table*}[ht]
	\centering
	\vspace{-15px}
	\caption{Accuracy and standard deviation on real datasets. The best (and comparable best) results are in ``\textbf{bold}'',
			the second (and comparable second) best are \underline{underlined}.}
	\label{tab:real_result}
	% \renewcommand{\arraystretch}{1.05}
	\setlength\tabcolsep{4pt}
	\footnotesize
	\begin{tabular}{c|c|c|c|c|c|c}
		\toprule
		        \multirow{1}{*}{Method Class}         & \multirow{1}{*}{Methods} & \multicolumn{1}{c|}{Family} & \multicolumn{1}{c|}{Kinship} & \multicolumn{1}{c|}{UMLS} & \multicolumn{1}{c|}{WN18RR} & \multicolumn{1}{c}{FB15k-237} \\ \cmidrule{1-7}
		\multirow{2}{*}{\makecell{Embedding-\\based}} &          RotatE          &         0.865±0.004         &         0.704±0.002          &        0.860±0.003        &         0.427±0.003         &       {0.240±0.001}       \\
		                                              &          QuatE           &         0.897±0.001         &         0.311±0.003          &        0.907±0.002        &         0.441±0.002         &       {0.255±0.004}       \\ \midrule
		         \multirow{2}{*}{Rule-based}          &        Neural LP         &         0.872±0.002         &         0.481±0.006          &        0.630±0.001        &         0.369±0.003         &       {0.190±0.002}       \\
		                                              &           DRUM           &         0.880±0.003         &         0.459±0.005          &        0.676±0.004        &         0.424±0.002         &       {0.252±0.003}       \\ \midrule
		     \multirow{5}{*}[-0.8ex]{GNN-based}       &         CompGCN          &         0.883±0.001         &         0.751±0.003          &        0.867±0.002        &         0.443±0.001         &       {0.265±0.001}       \\
		                                              &         RED-GNN          &    \textbf{0.988±0.002}     &   \underline{0.820±0.003}    &        \underline{0.946±0.001}        &   \underline{0.502±0.001}   &       {0.284±0.002}       \\
		                                              &          NBFNet          &   \underline{0.977±0.001}   &   \underline{0.819±0.002}    &        \underline{0.946±0.002}        &         0.496±0.002         & \underline{{0.320±0.001}} \\ \cmidrule{2-7}
		                                              &        EL-RED-GNN        &    \textbf{0.990±0.002}     &     \textbf{0.839±0.001}     &  \textbf{0.952±0.003}  &    \textbf{0.504±0.001}     & \underline{{0.322±0.002}} \\
		                                              &        EL-NBFNet         &    \textbf{0.985±0.001}     &     \textbf{0.842±0.003}     &   \textbf{0.953±0.002}    &   \underline{0.501±0.003}   &  \textbf{0.332±0.001}   \\ \midrule
	\end{tabular}
	 \vspace{-15px}
\end{table*}




\section{Conclusion}

In this paper, we analyze the expressivity of the state-of-the-art GNNs for learning rules in KG reasoning, 
explaining their superior performance over classical methods. 
Our analysis sheds light on the rule structures that GNNs can learn. Additionally, our theory motivates an effective labeling method to improve GNN's expressivity. Moving forward, we will extend our analysis to GNNs with general labeling trick and try to extract explainable rule structures from trained GNN.
Limitations and impacts are discussed in Appendix~\ref{app:sec:limitation}.

\section*{Acknowledgments}
Q. Yao was in part supported by
National Key Research and Development Program of China under Grant 2023YFB2903904
and
NSFC (No. 92270106).


\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}

\newpage
\appendix
\onecolumn

\section{Rule analysis}\label{app:sec:rule-analysis}

We first give a simple proof for Proposition~\ref{prop:QL}.
\begin{proof}[proof of Proposition~\ref{prop:QL}]
   Since $R(\mathsf{h},x)$ is equivalent to $\exists z R(z,x)\wedge P_h(z)$, where $P_h(z)$ is the constant predicate only satisfied at entity $h$. Because $R(z,x)$ can describe the rule structure of $(h,R,?)$, $\exists z R(z,x)\wedge P_h(z)$ can describe the rule structure of $(h,R,?)$ as well. 
\end{proof}

We use the notation $G, v \models P_i$ ($G, v \nvDash P_i$) to represent that the unary predicate $P_i(x)$ is (not) satisfied at entity $v$.
\begin{definition}[Definition of graded modal logic]\label{app:def:CML}
	\label{def:gralogic}
	A formula in graded modal logic of KG $G$ is recursively defined as follows:
	\begin{enumerate}
	\item  
	If $\varphi(x) = \top$, $G, v \models \varphi$ if $v$ is an entity in KG;
	
	\item If $\varphi(x) = P_c(x)$, $G, v \models \varphi$ if and only if $v$ has the property $P_c$ or can be uniquely identified by constant $\mathsf{c}$;
	
	\item If $\varphi(x) = \varphi_1(x) \wedge \varphi_2(x)$, $G, v \models \varphi$ if and only if $G, v \models \varphi_1$ and $G, v \models \varphi_2$;
	
	\item If $\varphi(x) = \neg\phi(x)$, $G, v \models \varphi$ if and only if $G, v \nvDash \phi$;
	
	\item If $\varphi(x) = \exists^{\geq N}y, R_j(y, x)\wedge \phi(y)$, $G, v \models \varphi$ if and only if the set of entities $\{u | u\in \mathcal{N}_{R_j}(v) \text{ and } G, u \models \phi \}$ has cardinality at least $N$.
	\end{enumerate}
\end{definition}

\begin{corollary}\label{app:corollary:chain}
	$C_3(\mathsf{h}, x)$ are formulas in $\text{CML}[G, \mathsf{h}]$.
\end{corollary}
\begin{proof}
	$C_3(\mathsf{h}, x)$ is a formula in $\text{CML}[G, \mathsf{h}]$ as it can be recursively defined as follows
	\begin{align*}
		\varphi_1(x) &= P_h (x), \\
		\varphi_2 (x) &= \exists y, R_1 (y, x) \wedge \varphi_1(y), \\
		\varphi_3 (x) &= \exists y, R_2 (y, x) \wedge \varphi_2(y), \\
		C_3(\mathsf{h}, x) &= \exists y, R_3 (y, x) \wedge \varphi_3(y).
	\end{align*}
\end{proof}

\begin{corollary}\label{app:corollary:inductive_1}
	$I_1(\mathsf{h}, x)$ is a formula in $\text{CML}[G, \mathsf{h}]$.
\end{corollary}
\begin{proof}
	$I_1(\mathsf{h}, x)$ is a formula in $\text{CML}[G, \mathsf{h}]$ as it can be recursively defined as follows
	\begin{align*}
		\varphi_1(x) &= P_h (x), \\
		\varphi_2 (x) &= \exists y, R_1 (y, x) \wedge \varphi_1(y), \\
		\varphi_s (x) &= \exists y, R_3(y, x) \wedge \top, \\
		\varphi_3 (x) &= \varphi_s(x) \wedge \varphi_2(x), \\
		I_1(\mathsf{h}, x) &= \exists y, R_2 (y, x) \wedge \varphi_3(y).
	\end{align*}
\end{proof}


\begin{corollary}\label{app:corollary:T}
	$T(\mathsf{h}, x)$ is a formula in $\text{CML}[G, \mathsf{h}]$.
\end{corollary}
\begin{proof}
	By Corollary~\ref{app:corollary:chain}, $C^{\prime}_{3}(\mathsf{h}, x):= \exists z_1 z_2, R_1(\mathsf{h}, z_1) \wedge R_2(z_1, z_2) \wedge R_4(z_2, x)$ and $C_{3}^{\star}(\mathsf{h}, x):= \exists z_1 z_2, R_1(\mathsf{h}, z_1) \wedge R_3(z_1, z_2) \wedge R_5(z_2, x)$ are formulas in $\text{CML}[G, \mathsf{h}]$.
	Thus $T(\mathsf{h}, x) = C^{\prime}_{3}(\mathsf{h}, x) \wedge C_{3}^{\star}(\mathsf{h}, x)$ is a formula in $\text{CML}[G, \mathsf{h}]$.
\end{proof}

\begin{corollary}\label{app:corollary:labeling}
	$U'(\mathsf{h}, x)$ is a formula in $\text{CML}[G, \mathsf{h}, \mathsf{c}]$.
\end{corollary}
\begin{proof}
	$U'(\mathsf{h}, x)$ is a formula in $\text{CML}[G, \mathsf{h}, \mathsf{c}]$ as it can be recursively defined as follows
	\begin{align*}
		\varphi_1(x) &= P_h(x), \varphi_c(x) = P_c(x), \\
		\varphi_2(x) &= \exists y, R_1(y, x) \wedge \varphi_1(y), \\
		\varphi_3(x) &= \varphi_2(x) \wedge \varphi_c(x), \\
		\varphi'_4(x) &= \exists y, R_2(y, x) \wedge \varphi_3(y), \\
		\varphi'_5(x) &= \exists y, R_4(y, x) \wedge \varphi'_4(y), \\
		\varphi''_4(x) &= \exists y, R_3(y, x) \wedge \varphi_3(y), \\
		\varphi''_5(x) &= \exists y, R_5(y, x) \wedge \varphi''_4(y), \\
		U'(\mathsf{h}, x) &= \varphi'_5(x) \wedge \varphi''_5(x)
	\end{align*}
	where the constant $\mathsf{c}$ ensures that there is only one entity satisfied for unary predicate $\varphi_3(x)$.
\end{proof}



\paragraph{Example of rules} We can find some relations in reality corresponding to rules in Figure~\ref{fig:example}. Here are two examples of $C_3$ and $I_1$:
\begin{itemize}
	\item Relation \texttt{nationality} ($C_3$): $\text{Einstein}\rightarrow_{\text{born\_in}}\text{Ulm} \rightarrow_{\text{hometown\_of}}\text{Born}\rightarrow_{nationality}\text{Germany}$;
	\item Relation \texttt{father} ($I_1$): $\text{A}\rightarrow_{\text{spouse}}\text{B}\rightarrow_{\text{parent}}\text{C}$ and $\text{D}\rightarrow_{\text{sisterhood}}\text{B}$.
\end{itemize}

{
\paragraph{Rule structures in real datasets}\label{app:exp:real-rules}
To show that the expressivity is meaningful in our paper, we select three rule structures from Family and FB15k-237 in Figure~\ref{fig:real-rules} to show the existence of rule structures in real datasets. With the definition of CML, the rule structure in Figure~\ref{fig:real-rules}(a) is not a formula in CML and rule structures in Figure~\ref{fig:real-rules}(b) and \ref{fig:real-rules}(c) are formulas in CML. The real rules shows that rules defined by CML is common in real-world datasets and the rules beyond CML also exist, which highlights the importance of our work.

\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\textwidth]{img/real-rule.pdf}
	\vspace{-15px}
	\caption{Some rule structures in real datasets. The rule structure (a) is from Family dataset and is not a rule formula in $\text{CML}[G, \mathsf{h}]$, which cannot not be learned by QL-GNN. The rule structures (b) and (c) are from FB15k-237 dataset and are rule formulas in $\text{CML}[G, \mathsf{h}]$, which can be learned by QL-GNN.}
	\label{fig:real-rules}
\end{figure}

}

\paragraph{Summary}
Here we give Table~\ref{tab:case} to illustrate the correspondence between GNNs for KG reasoning, rule structures, and theories presented in our paper.
\begin{table*}[h]
	\centering
	\caption{Whether GNNs investigated in our paper can learn the rule formulas in Figure~\ref{fig:example} and \ref{fig:sec5} and the exemplar methods of these GNNs. \cmark (\xmark) mean the corresponding GNN can(not) lean the rule formula.}
	\small
	%	\renewcommand{\arraystretch}{0.9}
	%\setlength\tabcolsep{2pt}
	\begin{tabular}{c|cccc|c|c}
		\toprule
		Rule formula     & $C_3(\mathsf{h},x)$ & $I_1(\mathsf{h},x)$ & $T(\mathsf{h},x)$ & {$U(\mathsf{h},x)$} & Theoretical result & Exemplar Methods \\ \midrule
		Classical              & \xmark              & \xmark        & \xmark         & \xmark     & Theorem~\ref{theorem:EA} & R-GCN, CompGCN       \\  
		QL-GNN               & \cmark              & \cmark    & \cmark             & \xmark      &  Theorem~\ref{theorem:QL} & NBFNet, RED-GNN     \\ 
		EL-GNN & {\cmark}           & \cmark       & \cmark            & \cmark    &    Proposition~\ref{proposition:label} & EL-NBFNet/RED-GNN     \\ \bottomrule
	\end{tabular}
	\label{tab:case}
	\vspace{-10px}
\end{table*} 



\section{Relation between QL-GNN and NBFNet/RED-GNN}\label{app:sec:ql-gnn}
In this part, we show that NBFNet and RED-GNN are special cases of QL-GNN in Table~\ref{tab:QL-NBF} and \ref{tab:QL-RED} respectively.
\begin{table}[h]
   \caption{NBFNet is a special case of QL-GNN.}
   \label{tab:QL-NBF}
   \centering
   \small
   \begin{tabular}{c|c}
   \toprule
   & NBFNet \\
   \midrule
   Query representation     & Relation embedding     \\
   Non-query representation & 0  \\
   MPNN                     &     $\textsc{Aggregate}\left(\left\{\textsc{Message}\left(\bm{h}^{(t-1)}_x,   {\bm{w}}_q(x, r, v)\right) \middle\vert (x, r, v)   \in \gE(v)\right\} \cup \left\{\bm{h}^{(0)}_v\right\}\right)$    \\
   Triplet score            & Feed-forward network \\
   \bottomrule
   \end{tabular}
\end{table}

\begin{table}[h]
   \caption{RED-GNN is a special case of QL-GNN.}
   \label{tab:QL-RED}
   \centering
   \small
   \begin{tabular}{c|c}
   \toprule
      & RED-GNN   \\
                              \midrule
   Query representation    & 0    \\
   Non-query representation & NULL    \\
   MPNN   &   $\delta\Big(\sum\nolimits_{\{e_s,r\} : (e_s, r, e)\in \mathcal E_{e_q}^\ell}  \varphi\big({\bm h}^{\ell-1}_{e_q,e_s}, \bm h_r^\ell\big)\Big)$   \\
   Triplet score  & Linear transformation \\
   \bottomrule
   \end{tabular}
\end{table}
\section{Proof}\label{app:proof}

We use the notation $G, (h,t) \models R_j$ ($G, (h,t) \nvDash R_j$) to denote $R_j(x,y)$ is (not) satisfied at $h,t$.

\subsection{Base theorem: what kind of logical formulas can MPNN backbone for KG learn?}\label{app:what-kind-of-logic-can-MPNN-lean}

In this section, we analyze the expressivity of MPNN backbone (\ref{eq:GNN}) for learning logical formulas in KG.
This section is the extension of \citet{barcelo2020logical} to KG.

In a KG $G=(\mathcal{V}, \mathcal{E}, \mathcal{R})$,
MPNN with $L$ layers is a type of neural network that applies graph $G$ 
and initial entity representation $\mathbf{e}_v^{(0)}$ to learn the representations $\mathbf{e}_v^{(L)}, v\in \mathcal{V}$. 
MPNN employs message-passing mechanisms~\citep{gilmer2017neural} to propagate information between entities in graph.
The $k$-th layer of MPNN updates the entity representation via the following message-passing formula
\begin{align*}
	\mathbf{e}_v^{(k)}
	=
	\delta 
	\Big( 
	\mathbf{e}_v^{(k-1)}, \phi\left(\{\{ \psi(\mathbf{e}_u^{(k-1)}, R) | 
	u \in \mathcal{N}_R(v), R\in \mathcal{R} \}\}\right) 
	\Big),
\end{align*}
where $\delta$ and $\phi$ are combination and aggregation functions respectively, 
$\psi$ is the message function encoding the relation $R$ and entity $u$ neighboring to $v$, 
$\{\{\cdots\}\}$ is a multiset, 
and 
$\mathcal{N}_R(v)$ is the neighboring entity set $\{ u | (u,R,v) \in \mathcal{E} \}$.

To understand how MPNN can learn logical formulas, we regard logical formula $\varphi(x)$ as a binary classifier indicating whether $\varphi(x)$ is satisfied at entity $x$.
Then, we commence with the following definition.
\begin{definition}
   A MPNN captures a logical formula $\varphi(x)$ if and only if given any graph $G$, the MPNN representation can be mapped to a binary value, where \texttt{True} indicates that $\varphi(x)$ satisfies on entity $x$, while \texttt{False} does not satisfy.
\end{definition}

According to the above definition, MPNN can learn logical formula in KG by encoding whether these logical formulas is satisfied in the representation of the corresponding entity.
For example, 
% \footnote{$\surd$ +yq+: two verbs in this sentence}
if MPNN can learn a logical formula $\varphi(x)$, it implies that $\mathbf{e}_v^{(L)}$ can be mapped to a binary value \texttt{True}/\texttt{False} by a function indicating whether $\varphi(x)$ is satisfied at entity $v$.
% \footnote{$\surd$ +yq+: check this sentence}
Previous work~\citep{barcelo2020logical} has proven that
vanilla GNN for single-relational graph can learn the logical formulas from graded modal logic~\citep{de2000note,otto2019graded} 
(a.k.a., Counting extension of Modal Logic, CML). 
In this section, 
we will present a similar theory of MPNN for KG.

The insight of MPNN's ability to learn formulas in CML lies in the alignment between certain CML formulas and the message-passing mechanism, which also holds for KG. Specifically, $\exists^{\geq N}y \left( R_j(y, x) \wedge \varphi(y) \right)$ is the formula aligned with MPNN's message-passing mechanism and allows to check the property of neighbor $y$ of entity variable $x$.
We use notation $\text{CML}[G]$ to denote
CML of a graph $G$.
Then, 
we give the following theorem to find out the kind of logical formula MPNN~(\ref{eq:GNN}) can learn in KG.


\begin{theorem}\label{theorem:main}
In a KG $G$, 
a logical formula $\varphi(x)$ is learned by MPNN (\ref{eq:GNN}) from its representations 
if and only if $\varphi(x)$ is a formula in $\text{CML}[G]$.
\end{theorem}

Our theorem can be viewed as an extension of Theorem 4.2 in \citet{barcelo2020logical} to KG and is the elementary tool for analyzing the expressivity of GNNs for KG reasoning.
The proof of Theorem~\ref{theorem:main} is in Appendix~\ref{app:proof} and employs novel techniques that specifically account for relation types.
Our theorem shows that
CML of KG is the tightest subclass of logic that MPNN can learn.
Similarly, our theorem is about the ability to implicitly learn logical formulas by MPNN rather than explicitly extracting them.


\subsection{Proof of Theorem~\ref{theorem:main}}

The backward direction of Theorem~\ref{theorem:main} is proven by constructing a MPNN that can learn any formula $\varphi(x)$ in CML. 
The forward direction relies on the results from recent theoretical results in \citet{otto2019graded}. 
Our theorem can be seen as an extension of Theorem 4.2 in \citet{barcelo2020logical} to KG. 



We first prove the backward direction of Theorem~\ref{theorem:main}.
\begin{lemma}\label{app:lemma:backward}
    Each formula $\varphi(x)$ in CML can be learned by MPNN~(\ref{eq:GNN}) from its entity representations.
\end{lemma}
\begin{proof}
    Let $\varphi(x)$ be a formula in CML. We decompose $\varphi$ into a series of sub-formulas $\text{sub}[\varphi]=(\varphi_1, \varphi_2, \cdots, \varphi_L)$ where $\varphi_k$ is a sub-formula of $\varphi_\ell$ if $k \leq \ell$ and $\varphi = \varphi_L$. 
	Assume the MPNN representation $\mathbf{e}_v^{(i)}\in\mathbb{R}^L, v\in\mathcal{V}, i=1\cdots L$.
	In this proof, the theoretical analysis will based on the following simple choice of (\ref{eq:GNN})
	\begin{align}\label{app:eq:simple-gnn}
		\mathbf{e}_v^{(i)}=\sigma \left( \mathbf{e}_v^{(i-1)}\mathbf{C} + \sum_{j=1}^r\sum_{u\in\mathcal{N}_{R_j}(v)} \mathbf{e}_u^{(i-1)} \mathbf{A}_{R_j} + \mathbf{b} \right)
	\end{align}
	with $\sigma=\min(\max(0,x), 1)$, $\mathbf{A}_{R_j}, \mathbf{C}\in\mathbb{R}^{L\times L}$ and $\mathbf{b}\in\mathbb{R}^L$.
	The entries of the $\ell$-th columns of $\mathbf{A}_{R_j}, \mathbf{C}$, and $\mathbf{b}$ depend on the sub-formulas of $\varphi$ as follows:
    \begin{itemize}
        \item \textbf{Case 0.} if $\varphi_\ell(x)=P_\ell(x)$ where $P_
		\ell$ is a unary predicate, $\mathbf{C}_{\ell \ell}=1$;
        \item \textbf{Case 1.} if $\varphi_\ell(x)=\varphi_j(x) \wedge \varphi_k(x)$, $\mathbf{C}_{j\ell}=\mathbf{C}_{k\ell}=1$ and $\mathbf{b}_{\ell}=-1$;
        \item \textbf{Case 2.} if $\varphi_\ell = \neg \varphi_{k}(x)$, $\mathbf{C}_{k\ell}=-1$ and $\mathbf{b}_\ell=1$;
        \item \textbf{Case 3.} if $\varphi_\ell(x)=\exists^{\geq N}y \left( R_j(y,x) \wedge \varphi_k(y) \right)$, $\left( \mathbf{A}_{R_j} \right)_{k\ell} = 1$ and $\mathbf{b}_\ell = -N + 1$.
    \end{itemize}
    with all the other values set to 0.

    Before the proof, for every entity $v\in\mathcal{V}$, the initial representation $\mathbf{e}_v^{(0)}=(t_1, t_2, \cdots, t_n)$ has $t_\ell=1$ if the sub-formula $\varphi_\ell = P_\ell(x)$ is satisfied at $v$, and $t_\ell=0$ otherwise.


    Let $G=(\mathcal{V},\mathcal{E}, \mathcal{R})$ be a KG. 
	We next prove that for every $\varphi_\ell \in \text{sub}[\varphi]$ and every entity $v\in\mathcal{V}$ it holds that
    \begin{equation*}
        \left( \mathbf{e}_v^{(i)} \right)_\ell = 1 \quad
        \text{if} \quad G, v \models \varphi_\ell,\quad \text{and} \quad  \left( \mathbf{e}_v^{(i)} \right)_\ell = 0 \quad \text{otherwise},
    \end{equation*}
	for every $\ell \leq i \leq L$.

	
	Now, we prove this by induction of the number of formulas in $\varphi$.

    \textbf{Base case:} One sub-formula in $\varphi$. In this case, the formula is an atomic predicate $\varphi=\varphi_\ell(x)=P_\ell(x)$. 
	Because $\mathbf{C}_{\ell \ell}=1$ and $(\mathbf{e}_v^{(0)})_\ell=1, (\mathbf{e}_v^{(0)})_i=0, i\neq \ell$, we have $(\mathbf{e}_v^{(1)})_\ell=1$ if $G, v \models \varphi_\ell$ and $(\mathbf{e}_v^{(1)})_\ell=0$ otherwise.
	For $i\geq 1$, $\mathbf{e}_v^{(i)}$ satisfies the same property.

    \textbf{Induction Hypothesis:} $k$ sub-formulas in $\varphi$ with $k < \ell$. Assume $\left(\mathbf{e}_v^{(i)} \right)_{k}=1$ if $G, v \models \varphi_k$ and $\left(\mathbf{e}_v^{(i)} \right)_{k}=0$ otherwise for $k \leq i \leq L$.

	\textbf{Proof:} $\ell$ sub-formulas in $\varphi$. Let $i\geq \ell$. 
    Case 1-3 should be considered. 

    Case 1. Let $\varphi_\ell(x)=\varphi_j(x) \wedge \varphi_k(x)$. Then $\mathbf{C}_{j\ell}=\mathbf{C}_{k\ell}=1$ and $\mathbf{b}_{\ell}=-1$.
	Then we have
	\begin{align*}
		(\mathbf{e}_v^{(i)})_\ell = \sigma \left( (\mathbf{e}_v^{(i-1)})_j + (\mathbf{e}_v^{(i-1)})_k - 1 \right).
	\end{align*}
	By the induction hypothesis, $(\mathbf{e}_v^{(i-1)})_j=1$ if only if $G, v \models \varphi_j$ and $(\mathbf{e}_v^{(i-1)})_j=0$ otherwise.
	Similarly, $(\mathbf{e}_v^{(i-1)})_k=1$ if and only if $G, v \models \varphi_k$ and $(\mathbf{e}_v^{(i-1)})_k=0$ otherwise.
	Then we have $(\mathbf{e}_v^{(i)})_\ell=1$ if and only if $(\mathbf{e}_v^{(i-1)})_j + (\mathbf{e}_v^{(i-1)})_k - 1 \geq 1$, which means $(\mathbf{e}_v^{(i-1)})_j=1$ and $(\mathbf{e}_v^{(i-1)})_k=1$.
	Then $(\mathbf{e}_v^{(i)})_\ell=1$ if and only if $G, v \models \varphi_j$ and $G, v \models \varphi_k$, i.e., $G, v \models \varphi_\ell$, and $(\mathbf{e}_v^{(i)})_\ell=0$ otherwise.


	Case 2. Let $\varphi_\ell(x)=\neg \varphi_k(x)$. Because of $\mathbf{C}_{k\ell}=-1$ and $\mathbf{b}_\ell=1$, we have
	\begin{align*}
		(\mathbf{e}_v^{(i)})_\ell = \sigma \left( -(\mathbf{e}_v^{(i-1)})_k+1 \right).
	\end{align*}
	By the induction hypothesis, $(\mathbf{e}_v^{(i-1)})_k=1$ if and only if $G, v \models \varphi_k$ and $(\mathbf{e}_v^{(i-1)})_k=0$ otherwise.
	Then we have $(\mathbf{e}_v^{(i)})_\ell=1$ if and only if $-(\mathbf{e}_v^{(i-1)})_k + 1 \geq 1$, which means $(\mathbf{e}_v^{(i-1)})_k=0$.
	Because $(\mathbf{e}_v^{(i-1)})_k=0$ if and only if $G, v \nvDash \varphi_k$,
	we have $(\mathbf{e}_v^{(i)})_\ell=1$ if and only if $G, v \nvDash \varphi_k$, i.e., $G, v \models \varphi_\ell$, and $(\mathbf{e}_v^{(i)})_\ell=0$ otherwise.


	Case 3. Let $\varphi_\ell(x)=\exists^{\geq N}y \left( R_j(y,x) \wedge \varphi_k(y) \right)$. Because of $\left( \mathbf{A}_{R_j} \right)_{k\ell} = 1$ and $\mathbf{b}_\ell = -N + 1$, we have
	\begin{align*}
		(\mathbf{e}_v^{(i)})_\ell = \sigma \left( \sum_{u\in\mathcal{N}_{R_j}(v)} (\mathbf{e}_u^{(i-1)})_k - N + 1 \right).
	\end{align*}
	By the induction hypothesis, $(\mathbf{e}_u^{(i-1)})_k=1$ if and only if $G, u \models \varphi_k$ and $(\mathbf{e}_u^{(i-1)})_k=0$ otherwise.
	Let $m = |\{ u | u \in \mathcal{N}_{R_j}(v) \text{ and } G, u \models \varphi_k \}|$.
	Then we have $(\mathbf{e}_v^{(i)})_\ell=1$ if and only if $\sum_{u\in\mathcal{N}_{R_j}(v)} (\mathbf{e}_u^{(i-1)})_k - N + 1 \geq 1$, which means $m \geq N$.
	Because $G, u \models \varphi_k$, $u$ is connected to $v$ with relation $R_j$, and $m\geq N$, we have $(\mathbf{e}_v^{(i)})_\ell=1$ if and only if $G, v \models \varphi_\ell$ and $(\mathbf{e}_v^{(i)})_\ell=0$ otherwise.


	To learn a logical formula $\varphi(x)$, we only apply a linear classifier to $\mathbf{e}_v^{(L)}, v\in\mathcal{V}$ to extract the component of $\mathbf{e}_v^{(L)}$ corresponding to $\varphi$.
	If $G, v \models \varphi$, the value of the corresponding extracted component is 1.



\end{proof}

Next, we prove the forward direction of Theorem~\ref{theorem:main}.
\begin{theorem}\label{app:theorem:forward}
    A formula $\varphi(x)$ is learned by MPNN~(\ref{eq:GNN}) if it can be expressed as a formula in CML.
\end{theorem}

To prove Theorem~\ref{app:theorem:forward}, we introduce Definition~\ref{app:def:unravelling}, Lemma~\ref{app:lemma:WL-unr}, Theorem~\ref{app:theorem:otto}, and Lemma~\ref{app:lemma:forward-contrary-proposition}.
\begin{definition}[Unraveling tree]\label{app:def:unravelling}
    Let $G$ be a KG, $v$ be entity in $G$, and $L\in\mathbb{N}$. The unravelling of $v$ in $G$ at depth $L$, denoted  by $\text{Unr}_{G}^L (v)$, is a tree composed of
    \begin{itemize}
        \item a node $(v, R_1, u_1, \cdots, R_i, u_i)$ for each path $(v, R_1, u_1, \cdots, R_i, u_i)$ in $G$ with $i\leq L$,
        \item an edge $R_i$ between $(v, R_1, u_1, \cdots, R_{i-1}, u_{i-1})$ and $(v, R_1, u_1, \cdots, R_i, u_i)$ when $(u_{i}, R_i, u_{i-1})$ is a triplet in $G$ (assume $u_0$ is $v$), and
        \item each node $(v, R_1, u_1, \cdots, R_i, u_i)$ has the same properties as $u_i$ in $G$.
    \end{itemize}
\end{definition}

\begin{lemma}\label{app:lemma:WL-unr}
    Let $G$ and $G'$ be two KGs, $v$ and $v'$ be two entities in $G$ and $G'$ respectively. Then for every $L \in \mathbb{N}$, the RWL test~\citep{barcelo2022weisfeiler} assigns the same color/hash to $v$ and $v'$ at round $L$ if and only if there is an isomorphism between $\text{Unr}_{G}^L(v)$ and $\text{Unr}_{G'}^L(v')$ sending $v$ to $v'$.
\end{lemma}
\begin{proof}
    \textbf{Base Case:} When $L=1$, the result is obvious.

    \textbf{Induction Hypothesis:} Relational WL (RWL) test assigns the same color to $v$ and $v'$ at round $L-1$ if and only if there is an isomorphism between $\text{Unr}_{G}^{L-1}(v)$ and $\text{Unr}_{G'}^{L-1}(v')$ sending $v$ to $v'$.

    \textbf{Proof:} In the $L$-th round,

    $\bullet$ Prove ``same color $\Rightarrow$ isomorphism''.

    \begin{align*}
        c^{L}(v) =& \text{hash}(c^{L-1}(v), \big\{\big\{ (c^{L-1}(u), R_i) | u \in \mathcal{N}_{R_i}(v), i=1,\cdots,r \big\} \big\}), \\
        c^{L}(v') =& \text{hash}(c^{L-1}(v'), \big\{\big\{ (c^{L-1}(u'), R_i) | u \in \mathcal{N}_{R_i}(v'), i=1,\cdots,r \big\} \big\}).
    \end{align*}
    Because $c^L(v)=c^L(v')$, we have $c^{L-1}(v)=c^{L-1}(v')$, and there exists an entity pair $(u, u'), u \in \mathcal{N}_{R_i}(v), u' \in \mathcal{N}_{R_i}(v')$ that
    \begin{align*}
        (c^{L-1}(u), R_i) = (c^{L-1}(u'), R_i).
    \end{align*}
    Then we have $c^{L-1}(u)=c^{L-1}(u')$. According to induction hypothesis, we have $\text{Unr}_{G}^{L-1}(u) \cong  \text{Unr}_{G'}^{L-1}(u')$. Also, because the edge connecting entity pair $(v, u)$ and $(v', u')$ is $R_i$, so there is an isomorphism between $\text{Unr}_{G}^{L}(v)$ and $\text{Unr}_{G'}^{L}(v')$ sending $v$ to $v'$.

    $\bullet$ Prove ``isomorphism $\Rightarrow$ same color''.

    Because there exists an isomorphism $\pi$ between $\text{Unr}_{G}^{L}(v)$ and $\text{Unr}_{G'}^{L}(v')$ sending $v$ to $v'$, assume $\pi$ is an bijective between the neighbors of $v$ and $v'$, e.g, $ u \in \mathcal{N}_{R_i}(v), u' \in \mathcal{N}_{R_i}(v') $ and $u_i'=\pi(u_i)$, the relation between entity pair $(u, v)$ and $(u', v')$ is $R_i$. 
    
Next we prove $c^{L-1}(u)=c^{L-1}(u')$.
Because $\text{Unr}_{G}^{L}(v)$ and $\text{Unr}_{G}^{L}(v')$ are isomorphism, and $\pi$ maps $u\in \mathcal{N}_{R_i}(v)$ to $u' \in \mathcal{N}_{R_i}(v')$, for the left tree with $L-1$ depth, i.e., $\text{Unr}_{G}^{L-1}(u)$ and $\text{Unr}_{G'}^{L-1}(u')$, $\pi$ can be the isomorphism mapping between $\text{Unr}_{G}^{L-1}(u)$ and $\text{Unr}_{G'}^{L-1}(u')$. According to induction hypothesis, we have $c^{L-1}(u)=c^{L-1}(u')$. Because $\text{Unr}_{G}^{L}(v) \cong \text{Unr}_{G'}^{L}(v')$, we also have $\text{Unr}_{G}^{L-1}(u) \cong \text{Unr}_{G'}^{L-1}(u')$ which means $c^{L-1}(u)=c^{L-1}(u')$. After running RWL test, we have $c^L(v)=c^L(v')$.
\end{proof}

\begin{theorem}\label{app:theorem:otto}
    Let $\varphi(x)$ be a unary formula in the formal description of graph $G$ in Section~\ref{ssec:formal-description-of-rule-structures}. If $\varphi(x)$ is not equivalent to a formula in CML, there exist two KGs $G$ and $G'$ and two entities $v$ in $G$ and $v'$ in $G'$ such that $\text{Unr}_{G}^L(v) \cong \text{Unr}_{G'}^L(v')$ for every $L\in\mathbb{N}$ and such that $G, v \models \varphi$ but $G', v' \nvDash \varphi$.
\end{theorem}
\begin{proof}
    The theorem follows directly from Theorem 2.2 in \citet{otto2019graded}. 
    Because $G, v \sim_{\#} G', v'$ and $\text{Unr}_{G}^L(v) \cong \text{Unr}_{G'}^L(v')$ are equivalent with the definition of counting bisimulation (i.e., notation $\sim_{\#}$).
\end{proof}

\begin{lemma}\label{app:lemma:forward-contrary-proposition}
    If a formula $\varphi(x)$ is not equivalent to any formula in CML,
    there is no MPNN~(\ref{eq:GNN}) that can learn $\varphi(x)$.
\end{lemma}
\begin{proof}
    Assume for a contradiction that there exists a MPNN that can learn $\varphi(x)$.
    Since $\varphi(x)$ is not equivalent to any formula in CML, with Theorem~\ref{app:theorem:otto}, there exists two KGs $G$ and $G'$ and two entities $v$ in $G$ and $v'$ in $G'$ such that $\text{Unr}_{G}^L(v) \cong \text{Unr}_{G'}^L(v')$ for every $L\in\mathbb{N}$ and such that $G, v \models \varphi$ and $G', v' \nvDash \varphi$. 
    By Lemma~\ref{app:lemma:WL-unr}, because $\text{Unr}_{G}^L(v) \cong \text{Unr}_{G'}^L(v')$ for every $L\in\mathbb{N}$, we have $\mathbf{e}_{v}^{(L)}=\mathbf{e}_{v'}^{(L)}$. But this contradicts the assumption that MPNN is supposed to learn $\varphi(x)$.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{app:theorem:forward}]
	Theorem can be obtained directly from Lemma~\ref{app:lemma:forward-contrary-proposition}.
\end{proof}


\begin{proof}[Proof of Theorem~\ref{theorem:main}]
	Theorem can be obtained directly by combining Lemma~\ref{app:lemma:backward} and Theorem~\ref{app:theorem:forward}.
\end{proof}

The following two remarks intuitively explain why MPNN can learn formulas in CML.
\begin{remark}
	Theorem~\ref{theorem:main} applies to both $\text{CML}[G]$ and $\text{CML}[G,\mathsf{c}_1, \mathsf{c}_2, \cdots,\mathsf{c}_k]$.
	The atomic unary predicate $P_i(x)$ in CML of graph $G$ is learned by the initial representations $\mathbf{e}_v^{(0)}, v\in\mathcal{V}$, which can be achieved by assigning special vectors to $\mathbf{e}_v^{(0)}, v\in\mathcal{V}$.
	In particular, the constant predicate $P_c(x)$ in $\text{CML}[G,\mathsf{c}]$ is learned by assigning a unique vector (e.g., one-hot vector for different entities)
	as the initial representation of the entity with unique identifier $\mathsf{c}$.
	The other sub-formulas $\neg\varphi(x), \varphi_1(x) \wedge \varphi_2(x)$ in Definition~\ref{app:def:CML} can be learned by continuous logical operations~\citep{arakelyan2020complex} which are independent of message-passing mechanisms. 
\end{remark}

\begin{remark}
	Assume the $(i-1)$-th layer representations $\mathbf{e}_v^{(i-1)},v\in\mathcal{V}$ can learn the formula $\varphi(x)$ in CML, the $i$-th layer representations $\mathbf{e}_v^{(i)}, v\in\mathcal{V}$ of MPNN can learn $\exists^{\geq N} y, R_j(y,x) \wedge \varphi(y)$ with specific aggregation function in (\ref{eq:GNN}) because $\mathbf{e}_v^{(i)}, v\in\mathcal{V}$ can aggregate the logical formulas in the one-hop neighbor representation $\mathbf{e}_v^{(i-1)}, v\in\mathcal{V}$ (i.e., $\varphi(x)$) with message-passing mechanisms.
\end{remark}


The following remark clarifies the scope of Theorem~\ref{theorem:main} and \ref{theorem:QL}.
\begin{remark}
	The positive results for our theorem (e.g., a MPNN variant can learn a logical formula) hold for MPNNs powerful than the MPNN we construct in (\ref{app:eq:simple-gnn}), while our negative results (e.g., a MPNN variant cannot learn a logical formula) hold for any general MPNNs~(\ref{eq:GNN}). 
	Hence, the backward direction remains valid irrespective of the aggregate and combine operators under consideration. This limitation is inherent to the MPNN architecture represented by (\ref{eq:GNN}) and not specific to the chosen representation update functions. On the other hand, the forward direction holds for MPNNs that are more powerful than (\ref{app:eq:simple-gnn}).
\end{remark}


\subsection{Proof of Theorem~\ref{theorem:QL}}
\begin{definition}
   QL-GNN learns a rule formula $R(\mathsf{h}, x)$ if and only if given any graph $G$, the QL-GNN's score of a new triplet $(h,R,t)$ can be mapped to a binary value, where \texttt{True} indicates that $R(\mathsf{h}, x)$ satisfies on entity $t$, while \texttt{False} does not satisfy.
\end{definition}

\begin{proof}
	We set the KG as $G$ and restrict the unary formulas in $\text{CML}[G,\mathsf{h}]$ to the form of $R(\mathsf{h}, x)$.
	This theorem is directly obtained by Theorem~\ref{theorem:main} because constant $h$ can be equivalently transformed to constant predicate $P_h(x)$.
\end{proof}

\begin{proof}[Proof of Corollary~\ref{coro:QL}]
\textbf{Base case:}
Since the unary predicate can be encoded into the initial representation of the entity according to Section~\ref{app:what-kind-of-logic-can-MPNN-lean}. Then the base case is obvious.

\textbf{Recursion rule:}
Since the rule structures $R(\mathsf{h}, x), R_1(\mathsf{h}, x), R_2(\mathsf{h}, x)$ are unary predicate and can be learned by QL-GNN,
they are formulas in $\text{CML}[G, \mathsf{h}]$.
According to recursive definition of CML,
$R_1(\mathsf{h}, x) \wedge R_2(\mathsf{h}, y)$, $\exists^{\geq N}y \left( R_i(y, x) \wedge R(\mathsf{h}, y) \right)$ are also formulas in $\text{CML}[G, \mathsf{h}]$, therefore can be learned by QL-GNN.

\end{proof}

\subsection{Proof of Theorem~\ref{theorem:EA}}

\begin{definition}
   CompGCN learns a rule formula $R(x,y)$ if and only if given any graph $G$, the QL-GNN's score of a new triplet $(h,R,t)$ can be mapped to a binary value, where \texttt{True} indicates that $R(x,y)$ satisfies on entity pair $(h,t)$, while \texttt{False} does not satisfy.
\end{definition}

\begin{proof}
	According to Theorem~\ref{theorem:main}, the MPNN representation $\mathbf{e}_v^{(L)}$ can represent the formulas in $\text{CML}[G]$.
	Assume $\varphi_1(x)$ and $\varphi_2(y)$ can be represented by the MPNN representation $\mathbf{e}_v^{(L)},v\in\mathcal{V}$ and there exists two functions $g_1$ and $g_2$ that can extract the logical formulas from $\mathbf{e}_v^{(L)}$, i.e., $g_i(\mathbf{e}_v^{(L)})=1$ if $G, v \models \varphi_i$ and $g_i(\mathbf{e}_v^{(L)})=0$ if $G, v \nvDash \varphi_i$ for $i=1,2$.
	We show how the following two logical operators can be learned by $s(h,R,t)$ for candidate triplet $(h,R,t)$: 
	\begin{itemize}
		\item Conjunction: $\varphi_1(x) \wedge \varphi_2(y)$. The conjunction of $\varphi_1(x), \varphi_2(y)$ can be learned with function $s(h,R,t)=g_1(\mathbf{e}_h^{(L)})\cdot g_2(\mathbf{e}_t^{(L)})$.
		
		\item Negation: $\neg \varphi_1(x)$. The negation of $\varphi_1(x)$ can be learned with function $s(h,R,t)=1-g_1(\mathbf{e}_h^{(L)})$.
	\end{itemize}
	The disjunction $\vee$ can be obtained by $\neg (\neg \varphi_1(x) \wedge \neg \varphi_2(y))$.
	More complex formula involving sub-formulas from $\{\varphi(x)\}$ and $\{\varphi^\prime(y)\}$ can be learned by combining the score functions above.	
\end{proof}

\subsection{Proof of Proposition~\ref{proposition:label}}

\begin{lemma}\label{app:lemma:label}
	Assume $\varphi(x)$ describes a single-connected rule structure $\mathsf{G}$ in a KG.
	If assign constant to entities with out-degree large than 1 in the KG, the structure $\mathsf{G}$ can be described with formula $\varphi'(x)$ in CML of KG with assigned constants.
\end{lemma}
\begin{proof}
	According to Theorem~\ref{app:theorem:otto}, assume $\varphi'(x)$ with assigned constants is not equivalent to a formula in CML, there should exist two rule structures $\mathsf{G}, \mathsf{G}'$ in KG $G, G'$, and entity $v$ in $\mathsf{G}$ and entity $v'$ in $\mathsf{G}'$ such that $\text{Unr}_{\mathsf{G}}^L(v) \cong \text{Unr}_{\mathsf{G}'}^L(v')$ for every $L\in\mathbb{N}$ and such that $\mathsf{G}, v \models \varphi'$ but $\mathsf{G}', v' \nvDash \varphi'$.

	Since each entity in $\mathsf{G}$ ($\mathsf{G}'$) with out-degree larger than 1 is assigned with a constant, the rule structure $\mathsf{G}$ ($\mathsf{G}'$) can be uniquely recovered from its unravelling tree $\text{Unr}_{\mathsf{G}}^L(v)$ ($\text{Unr}_{\mathsf{G}'}^L(v)$) for sufficient large $L$.
	Therefore, if $\text{Unr}_{\mathsf{G}}^L(v) \cong \text{Unr}_{\mathsf{G}'}^L(v')$ for every $L\in\mathbb{N}$, the corresponding rule structures $\mathsf{G}$ and $\mathsf{G}'$ should be isomorphism too, which means $\mathsf{G}, v \models \varphi'$ and $\mathsf{G}', v' \models \varphi'$.
	Thus, $\varphi'(x)$ must be a formula in CML.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{proposition:label}]
	The theorem holds by restricting the unary formula to the form of $R(\mathsf{h}, x)$ on Lemma~\ref{app:lemma:label}. 
\end{proof}

\begin{proof}[Proof of Corollary~\ref{coro:label}]
   By converting new constants $\mathsf{c}_1,\mathsf{c}_2,\cdots,\mathsf{c}_k$ to constant predicates $P_{c_1}(x),P_{c_2}(x),\cdots,P_{c_k}(x)$, the corollary holds by using Theorem~\ref{theorem:QL}.
\end{proof}

\section{Experiments}

\subsection{More rule structures in synthetic datasets}\label{app:sec:more-rules}

In Section~\ref{ssec:synthetic-exp}, we also include the following rule structures in the synthetic datasets, i.e., $C_4$ and $I_2$ in Figure~\ref{app:fig:more-rules}, for experiments.
$C_4$ and $I_2$ are both formulas from $\text{CML}[G, \mathsf{h}]$.
The proof of $C_4$ is similar to the proof of $C_3$ in Corollary~\ref{app:corollary:chain}.
The proof of $I_2$ is similar to that of $I_1$ and is in Corollary~\ref{app:corollary:inductive_2}.

\begin{figure*}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{img/more-rules.pdf}
	\vspace{-15px}
	\caption{In the synthetic experiments, we also compare the performance of various GNNs on the synthetic datasets generated from $C_4$ and $I_2$.
	}
	\label{app:fig:more-rules}
\end{figure*}



\begin{corollary}\label{app:corollary:inductive_2}
	$I_2(\mathsf{h}, x)$ is a formula in $\text{CML}[G, \mathsf{h}]$.
\end{corollary}
\begin{proof}
	$I_2(\mathsf{h}, x)$ is a formula in $\text{CML}[G, \mathsf{h}]$ as it can be recursively defined as follows
	\begin{align*}
		\varphi_1(x) &= P_h (x), \\
		\varphi_2 (x) &= \exists y, R_1 (y, x) \wedge \varphi_1(y), \\
		\varphi_3 (x) &= \exists y, R_2 (y, x) \wedge \varphi_2(y), \\
		\varphi_s (x) &= \exists^{\geq 2}y, R_4(y, x) \wedge \top, \\
		\varphi_4 (x) &= \varphi_s(x) \wedge \varphi_3(x), \\
		I_2(\mathsf{h}, x) &= \exists y, R_3 (y, x) \wedge \varphi_4(y).
	\end{align*}
\end{proof}


\subsection{Experiments for CompGCN}\label{app:exp:EA-GNN}

The classical framework of KG reasoning is inadequate for assessing the expressivity of CompGCN because the query $(h,R,?)$ assumes that certain logical formula ${ \varphi(x) }$ are satisfied at the head entity $h$ by default. In order to validate the expressivity of CompGCN, it is necessary to predict all missing triplets directly based on entity representations without relying on the query $(h,R,?)$. To accomplish this, we create a new dataset called $S$ that adheres to the rule formula $S(x,y)=\varphi^\star(x) \wedge \varphi^\star(y)$, where the logical formula is defined as:

\begin{equation*}
\varphi^\star(x) = \exists y R_1(x,y) \wedge \left(\exists x R_2(y,x) \wedge (\exists y R_3(x,y))\right).
\end{equation*}

Here, $\varphi^\star(x)$ is represented with parameter reusing (reusing $x$ and $y$) and is a formula in CML. Therefore, the formula $S(x,y)$ takes the form of $R(x,y)=f_R(\{\varphi(x)\}, \{ \varphi^\prime (y) \})$ and can be learned by CompGCN, as indicated by Theorem~\ref{theorem:EA}. To validate our theorem, we generate a synthetic dataset $S$ using the same steps outlined in Section~\ref{ssec:synthetic-exp}, following the rule $S(x,y)$. We then train CompGCN on dataset $S$. The experimental results demonstrate that CompGCN effectively learns the rule formula $S(x,y)$ with 100\% accuracy. Comparing it with QL-GNN is unnecessary since the latter is specifically designed for KG reasoning setting involving the query $(h,R,?)$.



\subsection{Statistics of synthetic datasets}
\begin{table}[H]
	\centering
	\caption{Statistics of the synthetic datasets.}
   \small
	\begin{tabular}{c|ccccccc}
	\toprule
	Datasets    & $C_3$ & $C_4$ & $I_1$ & $I_2$ & $T$  & $U$  & $S$ \\ \midrule
	known triplets & 1514  & 2013  & 843   & 1546  & 2242 & 2840 & 320 \\ \midrule
	training    & 1358  & 2265  & 304   & 674   & 83   & 396  & 583 \\ \midrule
	validation  & 86    & 143   & 20    & 43    & 6    & 26   & 37  \\ \midrule
	testing     & 254   & 424   & 57    & 126   & 15   & 183  & 109 \\ \bottomrule
	\end{tabular}
\end{table}

{
\subsection{Results on synthetic with missing triplets}\label{app:exp:missing}

We randomly remove 5\%, 10\%, and 20\% edges from synthetic datasets to test the robustness of QL-GNN and EL-GNN for rule structures learning. The results of QL-GNN and EL-GNN are shown in Table~\ref{tab:missing-QL} and \ref{tab:missing-EL} respectively. The results show that the completeness of rule structure correlates strongly with the performance of QL-GNN and EL-GNN.
}
\begin{table}[h]
	\centering
	\caption{The accuracy of QL-GNN on synthetic datasets with missing triplets.}
	\small
	\label{tab:missing-QL}
	\begin{tabular}{c|cccccc}
		\toprule
		Triplet missing ratio	 & $C_3$  & $C_4$  & $I_1$  & $I_2$  & $T$     & $U$     \\ \midrule
	5\%  & 0.899 & 0.866 & 0.760 & 0.783 & 0.556 & 0.329 \\
	10\% & 0.837 & 0.718 & 0.667 & 0.685 & 0.133 & 0.279 \\
	20\% & 0.523 & 0.465 & 0.532 & 0.468 & 0.111 & 0.162 \\ \bottomrule
	\end{tabular}
\end{table}

\begin{table}[h]
	\centering
	\caption{The accuracy of EL-GNN on synthetic datasets with missing triplets.}
	\small
	\label{tab:missing-EL}
	\begin{tabular}{c|cccccc}
		\toprule
		Triplet missing ratio & $C_3$  & $C_4$  & $I_1$  & $I_2$  & $T$     & $U$     \\ \midrule
	5\%  & 0.878 & 0.807 & 0.842 & 0.857 & 0.244 & 0.5   \\
	10\% & 0.766 & 0.674 & 0.725 & 0.661 & 0.222 & 0.347 \\
	20\% & 0.499 & 0.405 & 0.637 & 0.458 & 0.111 & 0.257 \\ \bottomrule
	\end{tabular}
\end{table}

\subsection{More experimental details on real datasets}\label{app:exp:real}
\paragraph{MRR and Hit@10}
Here we supplement MRR and Hit@10 of NBFNet and EL-NBFNet on real datasets in Table~\ref{tab:real-mrr}.
The improvement of EL-NBFNet on MRR and Hit@10 is not as significant as that on Accuracy because the EL-NBFNet is designed for exactly learning rule formulas and only Accuracy can be guaranteed to be improved. 
\begin{table}[h]
   \caption{MRR and Hit@10 of NBFNet and EL-NBFNet on real datasets.}
   \centering
   \small
   \setlength\tabcolsep{4pt}
   \label{tab:real-mrr}
   \begin{tabular}{c|cccccccccc}
   \toprule
    &
     \multicolumn{2}{c}{Family} &
     \multicolumn{2}{c}{Kinship} &
     \multicolumn{2}{c}{UMLS} &
     \multicolumn{2}{c}{WN18RR} &
	 \multicolumn{2}{c}{FB15k-237} \\ \midrule
             & MRR   & Hit@10 & MRR   & Hit@10 & MRR   & Hit@10 & MRR   & Hit@10 & MRR & Hit@10 \\ \midrule
   NBFNet    & 0.983 & 0.993  & 0.900 & 0.997  & 0.970 & 0.997  & 0.548 & 0.657 & 0.415 & 0.599 \\
   EL-NBFNet & 0.990 & 0.991  & 0.905 & 0.996  & 0.975 & 0.993  & 0.562 & 0.669 & 0.424 & 0.607 \\ \bottomrule
   \end{tabular}
\end{table}

\paragraph{Different hyperparameters of $d$}
We have observed that a larger or smaller $d$ does not necessarily lead to better performance in Figure~\ref{fig:out-degree}.
For real datasets, we also observed similar phenomenon in Table~\ref{tab:real-d}.
For real datasets, we uses $d=5, 30, 100, 100, 300$ for Family, Kinship, UMLS, WN18RR, and FB15k-237, respectively.
\begin{table}[h]
   \caption{The accuracy of EL-NBFNet on UMLS with different $d$.}
   \centering
   \label{tab:real-d}
   \begin{tabular}{ccccc}
   \toprule
   $d=0$ & $d=50$ & $d=100$ & $d=150$ & NBFNet \\ \midrule
   0.948 & 0.958  & 0.963   & 0.961   & 0.951  \\ \bottomrule
   \end{tabular}
\end{table}

{
\paragraph{Time cost of EL-NBFNet} In Table~\ref{tab:time-cost}, we show the time cost of EL-NBFNet and NBFNet on real datasets. The time cost is measured by seconds of testing phase. The results show that EL-NBFNet is slightly slower than NBFNet. The reason is that EL-NBFNet needs to traverse all entities on KG to assign constants to entities with out-degree larger than degree threshold $d$.
\begin{table}[H]
    \centering
	\caption{Time cost (seconds of testing) of EL-NBFNet on real datasets.}
	\label{tab:time-cost}
    \begin{tabular}{cccccc}
    \toprule
        Methods & Family & Kinship & UMLS & WN18RR & FB15k-237 \\  \midrule
        EL-NBFNet & 270.3 & 14.0 & 6.7 & 35.6 & 20.1 \\ 
        NBFNet & 269.6 & 13.5 & 6.4 & 34.3 & 19.8 \\ \bottomrule
    \end{tabular}
\end{table}

}
\section{Theory of GNNs for single-relational link prediction}\label{app:sec:single}

Our theory of KG reasoning can be easily extended to the single-relational link prediction. The following two corollaries are the extensions of Theorem~\ref{theorem:QL} and Theorem~\ref{theorem:EA} to the single-relational link prediction, respectively.


\begin{corollary}[Theorem~\ref{theorem:QL} on single-relational link prediction]\label{app:corollary:QL-single}
	For single-relational link prediction, given a query $(h, R, ?)$, 
	a rule formula $R(\mathsf{h}, x)$ is learned by QL-GNN if and only if $R(\mathsf{h}, x)$ is a formula in $\text{CML}[G,\mathsf{h}]$.
\end{corollary}

\begin{corollary}[Theorem~\ref{theorem:EA} on single-relational link prediction]\label{app:corollary:EA-single}
   For single-relational link prediction, 
   CompGCN can learn the rule formula 
   $R(x, y)=f_R\left(\{\varphi(x)\}, \{\varphi^\prime(y)\}\right)$
   where $f_R$ is a logical formula involving sub-formulas from $\{\varphi(x)\}$ and $\{\varphi^\prime(y)\}$ which are the sets of formulas in $\text{CML}[G]$ that can be learned by GNN~(\ref{eq:GNN}).
\end{corollary}   

Corollary \ref{app:corollary:QL-single} and \ref{app:corollary:EA-single} can be directly proven by restricting the logic of KG to single-relational graph, which means there is only one binary predicate in logic of graph.

{

\section{Understanding generalization based on expressivity}\label{app:sec:generalization}

\subsection{Understanding expressivity vs. generalization}\label{app:sec:QL-generalization}
In this section, we provide some insights on the relation between expressivity and generalization.
Expressivity in deep learning pertains to a model's capacity to accurately represent information, whereas the ability of a model to achieve this level of expressivity depends on its generalization.
Considering generalization requires not only contemplating the model design but also assessing whether the training algorithm can enable the model to achieve its expressivity.
The experiments in this paper can also show this relation about expressivity and generalization from two perspective: (1) The experimental results of QL-GNN shows that its expressivity can be achieved with classical deep learning training strategies; (2) In the development of deep learning, a consensus is that more expressivity often leads to better generalization. The experimental results of EL-GNN verify this consensus.

In addition, our theory can provide some insights on model design with better generalization.
Based on the constructive proof of Lemma~\ref{app:lemma:backward}, if QL-GNN can learn a rule formula $R(\mathsf{h}, x)$ with $L$ recursive definition, QL-GNN can learn $R(\mathsf{h}, x)$ with layers and hidden dimensions no less than $L$. 
Assuming learning $r$ relations with QL-GNN and numbers of recursive definition for these relations are $L_1, L_2, \cdots, L_r$ respectively, QL-GNN can learn these relations with layers no more than $max_i L_i$ and hidden dimensions no more than $\sum L_i$.
Since these bounds are nearly worst-case scenarios, both the dimensions and layers can be further optimized. 
Also, in the constructive proof of Lemma~\ref{app:lemma:backward}, the aggregation function is summation, and it is difficult for mean and max/min aggregation function to capture sub-formula $\exists^{\geq N}y \left( R_i(y, x) \wedge R(\mathsf{h}, y) \right)$.
From the perspective of rule learning, QL-GNN extracts structural information at each layer. Therefore, to learn rule structures, QL-GNN needs an activation function with compression capability for information extraction from inputs. Empirically, QL-GNN with identify activation function fails to learn with rules in synthetic dataset. 

Moreover, because our theory cannot help understand generalization related to network training, the dependence to hyperparameters of network training, e.g., the number of training examples, graph size, number of entities, cannot be revealed from our theory.


\subsection{Why assigning lots of constants hurts generalization?}\label{app:sec:many-features}

}
We take the relation $C_3$ as an example to show why assigning lots of constants hurts generalization from logical perspective.
We add two different constants $\mathsf{c}_1$ and $\mathsf{c}_2$ to the rule formula $C_3(h,x)$, which results two different rule formulas $C_3^\prime(\mathsf{h},y)=\exists z_1 R_1(\mathsf{h},z_1)\wedge R_2(z_1,\mathsf{c}_1)\wedge R_3(\mathsf{c}_1,x)$ and $C_3^\star(\mathsf{h},y)=\exists z_1 R_1(\mathsf{h},z_1)\wedge R_2(z_1,\mathsf{c}_2)\wedge R_3(\mathsf{c}_2,x)$.
Predicting new triplets for relation $C_3$ can now be achieved by learning the rule formulas $C_3(\mathsf{h},x), C_3^\prime(\mathsf{h},x)$, or $C_3^\star(\mathsf{h},x)$. Among these rule formulas, $C_3(\mathsf{h},x)$ is the rule with the best generalization, while $C_3^\prime(\mathsf{h},x)$ and $C_3^\star(\mathsf{h},x)$ require the rule structure to pass through the entities with identifiers of constants $\mathsf{c}_1$ and $\mathsf{c}_2$, respectively. Thus, when adding constants, maintaining performance requires the network to learn both rule formulas $C_3^\prime(\mathsf{h},x), C_3^\star(\mathsf{h},x)$ simultaneously which may potentially require a network with larger capacity. Even EL-GNN is unnecessary to learn $C_3^\prime(\mathsf{h},x), C_3^\star(\mathsf{h},x)$ since $C_3(\mathsf{h}, x)$ is learnable, EL-GNN cannot avoid learning rules with more than one constant in it when the rules are out of CML.


\section{Limitations and Impacts}\label{app:sec:limitation}
Our work offers a fresh perspective on understanding GNN's expressivity in KG reasoning.
Unlike most existing studies that focus on distinguishing ability, we analyze GNN's expressivity based solely on its ability to learn rule structures.
Our work has the potential to inspire further studies. For instance, our theory analyzes GNN's ability to learn a single relation, but in practice, GNNs are often applied to learn multiple relations. Therefore, determining the number of relations that GNNs can effectively learn for KG reasoning remains an interesting problem that can help determine the size of GNNs. Furthermore, while our experiments are conducted on synthetic datasets without missing triplets, real datasets are incomplete (e.g., missing triplets in testing sets). Thus, understanding the expressivity of GNNs for KG reasoning on incomplete datasets remains an important challenge.

\end{document}
