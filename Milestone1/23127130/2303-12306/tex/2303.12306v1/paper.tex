%%%%%%%% ICML 2023 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
\pdfoutput=1
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}
% \usepackage{ulem}
\usepackage{enumitem}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2023}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{pifont}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{bm,color}
\usepackage{multirow}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Logical Expressiveness of GNN for KG Reasoning}

\begin{document}

\twocolumn[
\icmltitle{Logical Expressiveness of Graph Neural Network \\ 
for Knowledge Graph Reasoning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Haiquan Qiu}{thu}
\icmlauthor{Yongqi Zhang}{4p}
\icmlauthor{Yong Li}{thu}
\icmlauthor{Quanming Yao}{thu}
\end{icmlauthorlist}

\icmlaffiliation{thu}{Department of Electronic Engineering, Tsinghua University, Beijing, China}
\icmlaffiliation{4p}{4Paradigm Inc., Beijing, China}

\icmlcorrespondingauthor{Quanming Yao}{first1.last1@xxx.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Graph Neural Networks (GNNs) have been recently
introduced to learn from knowledge graph (KG) and achieved state-of-the-art performance
in KG reasoning.
However, 
a theoretical certification for their good empirical performance is still absent. 
Besides, while logic in KG is important for inductive and interpretable inference, 
existing GNN-based methods are just designed to fit data distributions
with limited knowledge of their logical expressiveness.
We propose to fill the above gap in this paper. 
Specifically, we theoretically analyze GNN from logical expressiveness and find out what kind of logical rules can be captured from KG. 
Our results first
show that GNN can capture logical rules from graded modal logic, 
providing a new theoretical tool for analyzing the expressiveness of GNN for KG reasoning; 
and a query labeling trick makes it easier for GNN to capture logical rules,
explaining why SOTA methods are mainly based on labeling trick. 
Finally, 
insights from our theory motivate the development of an entity labeling method for capturing difficult logical rules.
Experimental results are consistent with our theoretical results and verify the effectiveness of our proposed method.
\end{abstract}

\section{Introduction}

Knowledge graph (KG) is graph-structured data that connects artificial intelligence with human knowledge 
by representing real-world entities and their relationships in a structured way. 
By encapsulating the interactions among entities, 
KGs provide a way for machines to understand and process complex information. 
In this paper, we focus on KG reasoning which deduces new facts from the existing facts in KG. 
KG reasoning has led to the widespread use of KGs in a variety of applications, 
e.g., drug discovery~\cite{Mohamed2019DiscoveringPD}, 
personalized recommendation~\cite{cao2019unifying}, 
and question answering~\cite{abujabal2018never}, etc.

There are many methods proposed for KG reasoning over the last decade, which can be grouped into three categories: 
embedding-based methods~\cite{bordes2013translating,trouillon2016complex}, 
rule-based methods~\cite{richardson2006markov,yang2017differentiable,sadeghian2019drum}, 
and GNN-based methods~\cite{schlichtkrull2018modeling,zhu2021neural,zhang2022knowledge}.
Among them, GNN-based methods have superior performance over the other methods due to its ability of automatically extracting important features from graphs. 
According to \citet{zhang2021labeling}, 
GNN methods for KG reasoning can be mainly divided into two categories: 
entity-representation-aggregation GNN~\cite{schlichtkrull2018modeling,vashishth2019composition} 
and labeling-trick GNN~\cite{teru2020inductive,zhu2021neural,zhang2022knowledge}.
However, the good empirical performance of GNNs is not well understood theoretically.
While there has been some progress in understanding the expressiveness~\cite{xu2018powerful,li2020distance,zhang2021labeling} of GNN to justify its good performance,
most of these works are based on either strong assumptions (e.g., isomorphism nodes or links in graph~\cite{xu2018powerful,zhang2021labeling}) or toy examples (e.g., regular graph~\cite{li2020distance}).
Therefore, it is difficult to analyze GNN for KG reasoning with the results from these works.

Meanwhile, 
logic in KG is a powerful tool for both inductive and interpretable KG reasoning~\cite{lavrac1994inductive,richardson2006markov,yang2017differentiable,sadeghian2019drum,qu2019probabilistic}.
With 
entities and relationships represented by logic, 
it is possible to reason and draw conclusions based on existing facts in KG
and bring knowledge to machines.
Rule-based methods for KG reasoning are based on logic, which 
either utilize predefined rules~\cite{lavrac1994inductive,richardson2006markov,qu2019probabilistic} 
or learn a logical rule based on predefined structures~\cite{yang2017differentiable,sadeghian2019drum}.
Although logic is a powerful tool for KG reasoning, most existing GNN-based methods do not take it into account,
and they instead rely on techniques from graph isomorphism testing to improve expressiveness. 
However, 
the analysis based on graph isomorphism testing is not well-suited for fine-grained rule structure analysis of GNN for KG reasoning 
since it relies on automorphism links or regular graphs. 
On the other hand, due to the close connection between logic and KG reasoning, analyzing the expressiveness of GNN from the logical perspective could provide valuable tools for evaluating existing GNNs for KG reasoning and improving their empirical performance.


In this paper, we fill the gap between GNN and logic in KG 
by theoretically analyzing logical expressiveness of GNN for KG reasoning.
We aim to find out 
\textit{what kind of logical rules can be captured by GNN for KG reasoning}.
The analysis helps better understand existing methods and
motivates future design directions on GNN for KG reasoning.


First, we show that logical rules from graded modal logic~\cite{de2000note,otto2019graded}
can be encoded into GNN representations, 
which serves as the fundamental 
theory for analyzing the logical expressiveness of GNN for KG reasoning.
Then, we analyze the logical expressiveness of two categories of GNNs for KG reasoning.
For \textbf{E}ntity-\textbf{R}epresentation-\textbf{A}ggregation GNN (ERA-GNN), 
our theory shows that it has limited logical expressiveness 
because
the logical rules, where head and tail entities are connected in their rule structures, 
cannot be captured.
For labeling-trick GNN, we identify an important subclass of GNN 
that assigns label to the query entity.
This subclass of GNN is called \textbf{Q}uery-\textbf{L}abeling GNN (QL-GNN) in our paper,
which includes existing state-of-the-art methods, 
e.g., 
NBFNet~\cite{zhu2021neural} and RED-GNN~\cite{zhang2022knowledge}. 
Our theory shows that the query labeling trick in QL-GNN makes it easier to capture logical rules and provides insights about why NBFNet and RED-GNN are particularly effective at KG reasoning.
Finally, for rules that cannot be captured by QL-GNN, 
we propose an entity labeling method to transform these rules to be capturable with special initial features of GNN.
In summary, our paper has the following contributions:

\begin{itemize}[leftmargin=*]
\item Our work is the first to introduce logical expressiveness into 
GNNs for KG reasoning, 
which 
provides new theoretical tools for analyzing their expressiveness.

\item We answer the question of what kind of logical rules can be captured by two classes of GNNs respectively,
and give insights into why 
NBFNet and RED-GNN are good at KG reasoning.

\item 
Our theory provides a way for transforming difficult logical rules to be capturable. Based on our theory, we propose an entity labeling method to improve the performance of learning difficult logical rules.
\item 
	Synthetic datasets are generated to evaluate various GNNs, whose experimental results are consistent with our theory. 
	Also, results of the proposed labeling method show improved performance on both synthetic and real datasets.

\end{itemize}







\section{Related Works}

\subsection{Graph Neural Network~(GNN)}
\label{sec:rel:gnn}

Let $G=(\mathcal{V}, \mathcal{E}, \mathcal{R})$ be a heterogeneous graph 
where $\mathcal{V},\mathcal{E},\mathcal{R}$ are sets of nodes, edges and relations of graph respectively,
and $\mathbf{e}_v^0$ be node feature vectors for $v \in \mathcal{V}$. 
Graph neural network (GNN)~\cite{kipf2016semi,gilmer2017neural} 
with $L$ layers is a type of neural network that uses graph $G$ 
and node features $\mathbf{e}_v^0$ to learn node representations $\mathbf{e}_v^{L}, v\in \mathcal{V}$. 
GNN uses message-passing mechanisms to propagate information between nodes in a graph.
The $k$-th layer of GNN updates the node representation via the following message-passing formula
\begin{align}
\label{eq:GNN}
\mathbf{e}_v^{k}
=
\delta 
\Big( 
\mathbf{e}_v^{k-1}, \phi\left(\{\{ \psi(\mathbf{e}_u^{k-1}, R), 
u \in \mathcal{N}_R(v) \}\}\right) 
\Big),
\end{align}
where $\delta$ and $\phi$ are combination and aggregation functions respectively, 
$\psi$ is the message function encoding the relation $R$ and node $u$ neighboring to $v$, 
$\{\{\cdots\}\}$ is a multiset, 
and 
$\mathcal{N}_R(v)$ is the entity set $\{ u | (u,R,v) \in \mathcal{F} \}$.

Since GNN has shown good performance on a wide range of tasks involving graph-structured data, many existing work try to analyze the expressiveness of GNN.
Most of these works analyze the expressiveness with tools from graph isomorphism testing.
A well-known result~\cite{xu2018powerful} shows that the expressiveness of vanilla GNN
is limited to Weisfeiler-Lehman~(WL) test.
To improve the expressiveness of GNNs, 
most of the existing works either utilize GNNs motivated by high-order WL test~\cite{morris2019weisfeiler,morris2020weisfeiler} 
or apply special initial features to improve the expressiveness of GNN~\cite{abboud2020surprising,you2021identity,sato2021random,zhang2021labeling}.

Except for using graph isomorphism testing, 
\citet{barcelo2020logical} analyzes the expressiveness of GNN with 
logic and identifies that the logical rules from graded modal logic can be captured by vanilla GNN.
However, their analysis is limited to node classification on the homogenous graph.

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.98\textwidth]{img/fig1.pdf}
	\vspace{-15px}
	\caption{Examples of three relations investigated in our paper. ``Logical rule'' shows the logical formula of the corresponding relation, i.e., new facts about a relation in the figure can be predicted by the corresponding logical rules.
	``Rule structure'' is the topology structure in KG which can help deduce new facts about the corresponding relation.
	``Example'' shows the instantiation of rule structure in the figure and corresponding new fact. 
	}
	\label{fig:example-relation}
\end{figure*}

\subsection{GNN for knowledge graph reasoning}
\label{sec:rel:kgr}


Knowledge graph~(KG) reasoning  uses KG,
i.e., $\mathcal{K}=(\mathcal{V}, \mathcal{F}, \mathcal{R})$
where $\mathcal{V}, \mathcal{F}, \mathcal{R}$ are the sets of entities, triplets and relations respectively, 
to predict new facts based on known ones.
Facts in KG are represented as interactions or relationships between entities.
These interactions are typically expressed as triplets in the form of $(h, R, t)$, 
where the head entity $h$ and tail entity $t$ are related with the relation $R$.
KG reasoning can be modeled as the process of predicting the tail entity $t$ of a query in the form $(h,R,?)$.
Existing methods for KG reasoning can be roughly divided into three classes: 
embedding-based methods~\cite{bordes2013translating}, 
rule-based methods~\cite{lavrac1994inductive,sadeghian2019drum}, and
GNN-based methods~\cite{schlichtkrull2018modeling,zhu2021neural}.
Among the above methods, GNN-based ones achieve the state-of-the-art performance.

GNN-based methods can be divided into two classes,
i.e., Entity-Representation-Aggregation GNN (ERA-GNN)~\cite{schlichtkrull2018modeling} 
and 
Labeling-Trick GNN (LT-GNN)~\cite{teru2020inductive,zhu2021neural}.
ERA-GNN first applies GNN to transform the initial features of each entity to get the entity representation $\mathbf{e}_v^L,v\in\mathcal{V}$ 
and then utilizes representations $\mathbf{e}_h^L, \mathbf{e}_t^L$ to calculate the score of potential new facts $(h, R, t)$.
Many GNN methods for KG reasoning belong to ERA-GNN,
such as RGCN~\cite{schlichtkrull2018modeling} and CompGCN~\cite{vashishth2019composition}. 
For example, CompGCN belongs to ERA-GNN since it calculates the score of a triplet with 
$s(h, R, t)=-\| \mathbf{e}_h^L+ \mathbf{e}_R- \mathbf{e}_t^L \|$
with relation embedding $\mathbf{e}_R$.

LT-GNN first assigns 
special initial features to entities in KG by the labeling trick, 
then applies GNN to get the entity representation $\mathbf{e}_v^L,v\in\mathcal{V}$ which is used to calculate the score for each candidate triplet.
For example, GraIL~\cite{teru2020inductive} belongs to LT-GNN because the entity features is initialized with Distance Encoding~\cite{zhang2018link}; 
the state-of-the-art GNN-based methods, 
e.g., NBFNet~\cite{zhu2021neural} and RED-GNN~\cite{zhang2022knowledge}, 
belong to GNN with partial labeling (a subclass of LT-GNN), 
which only assigns special initial feature to the query entity.
LT-GNN achieves the state-of-the-art performance in KG reasoning.
However,
there still lacks a theoretical certification for their good empirical performance.



\section{Logic of Knowledge Graph}
\label{sec:logic-of-KG}

Motivated by previous works on graph theory and logic~\cite{cai1992optimal,barcelo2020logical}, 
we consider the first-order logic with counting (FOC)
which is an extension of first-order logic by adding the counting quantifier $\exists^{\geq N}$.

FOC of KG
represents and reasons about KG using a formal logic language,
where the entities in KG are represented as logical variables, and the relations are represented as 
binary predicates over these variables.
The formulas in FOC of KG can be used to express constraints on the KG structure 
and to reason about KG with logical inference.


First, 
we introduce how entities and relations are modeled in FOC of KG.
To model entities in KG, we use lowercase italic letters $x, y, z$ to denote logical variables 
which can represent an entity or a class of entities in KG.
To describe property of entities in KG, 
the unary predicate $P_i(x)$ is introduced.
For example, $\texttt{red}(x)$ is a unary predicate indicating if an entity is colored with red.
In addition, lowercase letters $\mathsf{h}, \mathsf{c}$ 
with sans serif typestyle are used to denote constants in KG which are unique identifiers of entities.
A constant $\mathsf{c}$ can be transformed into a unary predicate $P_{c}(x)$ which is called \textit{constant predicate} in our paper.
In this way, if a constant predicate $P_{c}(x)$ is satisfied at an entity $v$, 
it means that $v$ can be uniquely identified by the constant $\mathsf{c}$.


To model relations in KG, binary predicates $R_j(x, y)$ 
are introduced to logically model the relation $R_j$ between entities. 
Therefore, an equivalent logical formula of relation $R_j$ can help to predict new facts in KG reasoning.
If the binary predicate $R_j(x, y)$ is satisfied at two entities $h,t$, 
there must exist a relation $R_j$ between entities $h, t$, i.e., $(h, R_j, t)\in\mathcal{F}$.
For example, a binary predicate $\texttt{father}(x,y)$ satisfied at two entities $h,t$ shows that $h$ is the father of $t$.
Other symbols in FOC of KG include 
logical connectives $\wedge, \neg$ denoting conjunction and negation respectively, 
symbol $\top$ representing \texttt{True}, 
universal quantifier $\forall$ representing ``for all", 
and counting quantifier $\exists^{\geq N}$ expressing the statements about the number of objects satisfying a given condition.




In the above definition, the assignment of constants is optional in FOC of KG.
To discriminate FOC with or without constants,
we denote
$\text{FOC}[\mathcal{K}]$ as FOC of $\mathcal{K}$ without assignment of constants, and $\text{FOC}[\mathcal{K},\mathsf{c}_1, \mathsf{c}_2, \cdots,\mathsf{c}_k]$ as FOC of $\mathcal{K}$ assigned with constants $\mathsf{c}_1, \mathsf{c}_2, \cdots,\mathsf{c}_k$.
Constants play an important role in our analysis because they can be combined with predicates.
When combined with a constant $\mathsf{c}$, 
$P_i(\mathsf{c})$ and $R_j(\mathsf{c}, x)$ are equivalent to logical formulas 
$\exists y, P_i(y) \wedge P_c(y)$ and $\exists y, R_j(y, x)\wedge P_c(y)$ respectively. 
Given a query $(h,R_j,?)$ in KG reasoning, new facts with relation $R_j$ can be predicted with the equivalent logical formula 
of $R_j(x,y)$ or $R_j(\mathsf{h}, x)$.
For example, the new fact $(h,C,t)$ with relation $C$ in 
Figure~\ref{fig:example-relation}
can be deduced from the logical rule $C(x,y)$ which is a formula in $\text{FOC}[\mathcal{K}]$, 
or from the logical rule $C(\mathsf{h}, x)$ which is a formula in $\text{FOC}[\mathcal{K},\mathsf{h}]$. 
If the triplet $(h,C,t)$ exists in KG, 
we should have $C(x, y)$ satisfied at $(h,t)$ or $C(\mathsf{h}, x)$ satisfied at $t$.


Compared with previous work~\cite{barcelo2020logical} on homogenous graphs, 
FOC of KG can be seen as FOC defined on heterogeneous graphs, 
and we consider constants in FOC of KG which play an important role in the logical expressiveness of GNN for KG reasoning.


\section{Logical Analysis of GNN Representation}
\label{sec:representation-logic}


Before giving the logical results for KG reasoning, 
we first analyze the logical expressiveness of entity representations obtained by GNN since GNN for KG reasoning predicts new facts based on the GNN entity representations.
We will find out what kind of logical rules can be captured
\footnote{A logical formula $\phi(x)$ is captured by GNN representation means that the GNN representation contains information about whether $\phi(x)$ is satisfied at the entities in KG.
}
by entities'
representations from GNNs, i.e., find out what kind of logical rules can be implicitly encoded into the entities' representations.


As GNN is not a universal approximator~\cite{xu2018powerful,xu2019can,barcelo2020logical},
some formulas in FOC cannot be captured by GNNs.
For the node classification on homogenous graph, 
\citet{barcelo2020logical} identifies a subclass of FOC called graded modal logic~\cite{de2000note,otto2019graded} which can be captured by GNN.
Thus, for KG (heterogeneous graph), our logical analysis will also focus on graded modal logic of KG which is the 
Counting extension of Modal Logic (CML). 
The formulas in CML of KG can be recursively defined in the following sub-formulas 
\begin{align}
\begin{split}
\top, 
\;
&
P_i(x),
\;
\neg \varphi(x), 
\;
\varphi_1(x) \wedge \varphi_2(x), 
\\
&
\exists^{\geq N}y \left( R_j(y, x) \wedge \varphi(y) \right),
\end{split}
\label{eq:CML}
\end{align}
where $\top$ is satisfied at all entities, $P_i(x)$ is atomic unary predicate, $\varphi(x), \varphi_1(x), \varphi_2(x)$ are sub-formulas in CML and $\exists^{\geq N}y \left( R_j(y, x) \wedge \varphi(y) \right)$ allows to check the property of neighbor $y$ of entity variable $x$.

Since CML is a subset of FOC, we have similar notations to represent CML of KG, 
namely,
$\text{CML}[\mathcal{K}]$ denotes
CML of a KG $\mathcal{K}$ without constant, 
and $\text{CML}[\mathcal{K},\mathsf{c}_1, \mathsf{c}_2, \cdots,\mathsf{c}_k]$ denotes 
CML of a KG $\mathcal{K}$ with constants $\mathsf{c}_1, \mathsf{c}_2, \cdots,\mathsf{c}_k (k\geq 1)$.
Then, 
we give the following theorem to connect GNN representation and formulas in CML.




\begin{theorem}\label{theorem:main}
In a knowledge graph, 
a logical rule $\varphi(x)$ is captured by the representations of GNN in Eq.\eqref{eq:GNN} 
on the knowledge graph 
if and only if the logical rule $\varphi(x)$ is a formula in CML of the knowledge graph.
\end{theorem}

The proof of Theorem~\ref{theorem:main} is in Appendix~\ref{app:proof}.
Our theorem can be seen as an extension of Theorem 4.2 in \citet{barcelo2020logical} to 
KG (heterogeneous graph).
Similarly, our theorem talks about the ability to implicitly capture logical rules by the GNN representations rather than explicitly extracting rules, which is future work.  
It shows that
CML of KG is the tightest subclass of FOC that GNN can capture.
The following two remarks intuitively explain how GNN can capture formulas in CML.

\begin{remark}
	Theorem~\ref{theorem:main} applies to both $\text{CML}[\mathcal{K}]$ and $\text{CML}[\mathcal{K},\mathsf{c}_1, \mathsf{c}_2, \cdots,\mathsf{c}_k]$.
	The atomic unary predicate $P_i(x)$ in CML of KG is captured by the initial features $\mathbf{e}_v^0, v\in\mathcal{V}$, which can be achieved by assigning special vectors to $\mathbf{e}_v^0, v\in\mathcal{V}$.
	In particular, the constant predicate $P_c(x)$ in $\text{CML}[\mathcal{K},\mathsf{c}]$ is captured by assigning a unique vector (e.g., one-hot vector for different entities)
	as the initial feature of the entity identified by $\mathsf{c}$.
	The other sub-formulas $\neg\varphi(x), \varphi_1(x) \wedge \varphi_2(x)$ in \eqref{eq:CML} can be captured by continuous logical operations~\cite{arakelyan2020complex} which are independent of message-passing mechanisms. 
\end{remark}

\begin{remark}
	Assume the $(i-1)$-th layer representations $\mathbf{e}_v^{i-1},v\in\mathcal{V}$ can capture the formula $\varphi(x)$ in CML, the $i$-th layer representations $\mathbf{e}_v^i, v\in\mathcal{V}$ of GNN can capture $\exists^{\geq N} y, R_j(y,x) \wedge \varphi(y)$ with specific aggregation function in \eqref{eq:GNN} because $\mathbf{e}_v^{i}, v\in\mathcal{V}$ can aggregate the logical rules in the one-hop neighbor representation $\mathbf{e}_v^{i-1}, v\in\mathcal{V}$ (i.e., $\varphi(x)$) with message-passing mechanisms.
\end{remark}


Next, we will use Theorem~\ref{theorem:main} to analyze the logical expressiveness of various GNN methods for
KG reasoning.

\begin{table*}[t]
	\centering
	\caption{This table shows whether the GNNs investigated in our paper can capture the logical rules in Figure~\ref{fig:example-relation} and the exemplar methods of these GNNs. \cmark\ means the corresponding GNN can capture the logical rule, thus can help predict new facts involving corresponding relation. \xmark\ means the corresponding GNN cannot capture the logical rule.}
	\begin{tabular}{c|cc|c|cc|c}
		\toprule
		Relation type        & \multicolumn{2}{c|}{$C$}                            & $I$               & \multicolumn{2}{c|}{$U$} & \multirow{2}{*}[-0.8ex]{Exemplar Method}                                    \\ \cmidrule{1-6}
		Logical rule    & {$C(x,y)$} & $C(\mathsf{h},x)$ & $I(\mathsf{h},x)$ & {$U(\mathsf{h},x)$} & $U'(\mathsf{h},x)$ & \\ \midrule
		ERA-GNN         &{\xmark}     & \xmark              & \xmark            &     {\xmark}            & \xmark     & R-GCN,CompGCN        \\  
		QL-GNN          & {\xmark}     & \cmark              & \cmark            &    {\xmark}            & \xmark      &  NBFNet,RED-GNN     \\ 
		EL-GNN & {\xmark}     & \cmark              & \cmark            &    {\xmark}            & \cmark    &    Ours     \\ \bottomrule
	\end{tabular}
	\label{tab:case}
\end{table*}


\section{Logical expressiveness of GNN for KG Reasoning}\label{sec:logic-exp}


Here,
we analyze the logical expressiveness of 
two main types of GNN-based KG reasoning methods,
i.e., ERA-GNN in Section~\ref{ssec:EA} and QL-GNN in Section~\ref{ssec:QL}.
In Section~\ref{ssec:case}, 
we present some logical rules in 
Figure~\ref{fig:example-relation} to analyze the expressiveness of ERA-GNN and QL-GNN.


\subsection{Logical expressiveness of ERA-GNN}
\label{ssec:EA}



Given a KG, 
ERA-GNN (e.g., R-GCN, CompGCN) first applies \eqref{eq:GNN} to compute the entity representations $\mathbf{e}_v^L, v\in\mathcal{V}$. 
Then for a query $(h, R, ?)$, 
the score $s(h,R,t)$ for candidate $(h, R, t)$ with each $t\in\mathcal V$
is calculated by aggregating the entity representations $\mathbf{e}_h^L, \mathbf{e}_t^L$.
Representative functions for entity aggregation are TransE~\cite{bordes2013translating} and ComplEx~\cite{trouillon2016complex}, etc.
For example, CompGCN can use the score function $s(h, R, t)=-\| \mathbf{e}_h^L+ \mathbf{e}_R- \mathbf{e}_t^L \|$ with relation embedding $\mathbf{e}_R$.
If the triplet $(h, R, t)$ is in KG, 
the value of $s(h, R, t)$ is optimized to be larger than the non-existing triplets.

From Theorem~\ref{theorem:main}, the entity representation $\mathbf{e}_v^L$ can capture some formula $\phi(x)$ in 
$\text{CML}[\mathcal{K}]$ at $v$.
In KG reasoning, if ERA-GNN can capture the logical rule $R(x,y)$, 
the score $s(h, R, t)$
should be able to determine whether $R(x,y)$ is satisfied at $h,t$ for candidate triplet $(h, R, t)$.
Thus, when calculating the score, the arithmetic operation between $\mathbf{e}_h^L$ and $\mathbf{e}_t^L$ represents some logical operations connecting the formulas in CML at entities $h$ and $t$.
With the above analysis, we have the following theorem of the logical expressiveness for ERA-GNN.

\begin{theorem}[Logical expressiveness of ERA-GNN]
\label{theorem:EA}
For KG reasoning, 
ERA-GNN can capture the logical rule 
$R(x, y)=f_R\left(\{\phi_i(x)\}, \{\psi_j(y)\}\right)$
where $f_R$ is a logical formula involving sub-formulas from $\{\phi_i(x)\}$ and $\{\psi_j(y)\}$ which are the sets of formulas in $\text{CML}[\mathcal{K}]$ that can be captured by GNN representations.
\end{theorem}


Theorem~\ref{theorem:EA} (proved in Appendix~\ref{app:proof})
indicates that ERA-GNN is only capable of learning logical rules $R(x,y)$ where $x$ and $y$ are independent. 
An important class of relation belonging to this kind of logical rule describes the similarity between two entities, 
e.g., $\texttt{same\_color}(x,y)$ describing entities with the same color.
However, relations involving dependent head and tail entities are more commonly seen in KG reasoning~\cite{lavrac1994inductive,sadeghian2019drum}.
For example, the relations shown in Figure~\ref{fig:example-relation} involve dependent head and tail entities and cannot be captured by ERA-GNN due to Theorem~\ref{theorem:EA}. 
Previous work on homogenous graph~\cite{zhang2021labeling} says that there are many non-isomorphic links that ERA-GNN cannot distinguish while our paper provides a fine-grained analysis for heterogeneous graph (KG) which specifies which kind of logical rules can be captured by ERA-GNN. 


\subsection{Logical expressiveness of QL-GNN}\label{ssec:QL}

Given a query $(h, R, ?)$, 
QL-GNN (a subclass of LT-GNN, e.g., NBFNet and RED-GNN)
assigns label to the query entity $h$,
aggregates entity representations $\mathbf{e}_t^L[\mathsf{h}]$ for each candidate $t\in \mathcal V$
depending on $h$,
and scores a candidate triplet $(h, R, t)$ with tail entity representation $\mathbf{e}_t^L[\mathsf{h}]$.
For example, NBFNet uses the score function $s(h,R,t)=\text{FFN}(\mathbf{e}_t^L[\mathsf{h}])$ where $\text{FFN}(\cdot)$ denotes a feed-forward neural network.


From the logical perspective, 
the label assigned to query entity is denoted as constant $\mathsf{h}$ or equivalently converted to a constant predicate $P_h(x)$ in KG.
In QL-GNN, the assigned constant is transformed into a unique initial feature $\mathbf{e}_h^0$ to indicate that $h$ is the head entity of the query.
Then, QL-GNN applies GNN~\eqref{eq:GNN} to get the representations $\mathbf{e}_t^L[\mathsf{h}],t\in\mathcal{V}$.
With the constant $\mathsf{h}$, 
the logical rule behind $(h, R, ?)$ becomes a rule in the form of $R(\mathsf{h}, x)$. 
Then, we obtain the following theorem immediately from Theorem~\ref{theorem:main}.


\begin{theorem}[Logical expressiveness of QL-GNN]\label{theorem:QL}
For KG reasoning, given query $(h, R, ?)$, 
a logical rule $R(\mathsf{h}, x)$ is captured by QL-GNN if and only if the logical rule $R(\mathsf{h}, x)$ is a formula in $\text{CML}[\mathcal{K},\mathsf{h}]$.
\end{theorem}


From Theorem~\ref{theorem:QL}(proved in Appendix~\ref{app:proof}), QL-GNN transforms logical rules $R(x,y)$ to a rule $R(\mathsf{h}, x)$ in $\text{CML}[\mathcal{K}, \mathsf{h}]$
by introducing the constant $\mathsf{h}$.
Then based on Theorem~\ref{theorem:main},
the common logical rules, where $x$ and $y$ are dependent on each other,
can be captured by QL-GNN with constant $\mathsf{h}$ but cannot be captured by ERA-GNN,
meaning that QL-GNN is more powerful than ERA-GNN.



The analysis for GNN-based methods with general labeling trick goes beyond our theoretical tools.
For example, for a candidate triplet $(h,R,t)$, Distance Encoding~\cite{zhang2018link} in GraIL is equivalent to assigning constants $\mathsf{h}, \mathsf{t}$ to entities $h,t$.
Thus, GraIL actually learns a sentence $R(\mathsf{h}, \mathsf{t})$ in $\text{FOC}[\mathcal{K},\mathsf{h}, \mathsf{t}]$.
However,
our analysis shows that query labeling is an important subclass of labeling trick for KG reasoning.
In Section~\ref{sec:complex-rule},
our work provides a tool for fine-grained analysis (compared to isomorphism) 
of the 
structures of KG reasoned by QL-GNN
and gives insights on how to assign entity labels with labeling trick.


\subsection{Case analysis}\label{ssec:case}

Here, 
we use 
representative logical rules in Figure~\ref{fig:example-relation} to investigate the logical expressiveness of GNN for KG reasoning and summarize the results of case analysis in 
Table~\ref{tab:case}.




The chain-like rule is a basic logical rule investigated in many previous works~\cite{sadeghian2019drum,teru2020inductive,zhu2021neural}.
With Theorem~\ref{theorem:EA}, $C(x, y)$ in Figure~\ref{fig:example-relation}(a) cannot be decomposed into two sets of logical formulas involving $x$ and $y$ separately, 
and thus cannot be captured by ERA-GNN.
On the other hand,
QL-GNN assigns a constant $\mathsf{h}$ to the query entity $h$, thus the new facts about relation $C$ can be predicted by learning the logical rule $C(\mathsf{h}, x)$ in Figure~\ref{fig:example-relation}(a).
With Theorem~\ref{theorem:QL}, $C(\mathsf{h}, x)$ is a formula in $\text{CML}[\mathcal{K},\mathsf{h}]$ (Corollary~\ref{app:corollary:chain} in Appendix~\ref{app:sec:rule-analysis}) and thus can be captured by QL-GNN.
Hence, our theory gives a general theoretical tool to determine if a GNN-based method can capture the chain-like rule.

\begin{figure}[t]
	\centering
	\includegraphics[width=.95\columnwidth]{img/overview_rel.pdf}
	\caption{The division of different relations $R$ between two entities $h,t$ on KG. 
		It shows the scope of relations that can be handled by various GNN methods investigated in our paper.}
	\label{fig:overview}
\end{figure}

The second logical rule $I(\mathsf{h}, x)$ in Figure~\ref{fig:example-relation}(b) is composed of a chain-like structure along with two additional
entities with variable $z_3$ connected to an entity with variable $z_2$ and is a formula in $\text{CML}[\mathcal{K},\mathsf{h}]$ (Corollary~\ref{app:corollary:inductive} in Appendix~\ref{app:sec:rule-analysis}), and thus can be captured by QL-GNN with Theorem~\ref{theorem:QL}.
$I(\mathsf{h}, x)$ has never been analyzed in any previous work, and we find that similar structures play an important role in the 
inductive KG reasoning, i.e., KG reasoning task with unseen entities in the inference.
$I(\mathsf{h}, x)$ can improve the performance of inductive KG reasoning because two edges with relation $R_4$ are connected to an entity with variable $z_2$ and can help infer the entity's property. 
In fact, using edges to infer the entity property is a common idea in inductive graph learning tasks, e.g., \citet{galkin2022inductive} learns representations of unseen entities based on the edges that are connected to it. 
The result in our paper is the first to provide a theoretical tool to analyze why some GNN-based methods (e.g., NBFNet, RED-GNN) are good at inductive KG reasoning.



In summary, we visualize the analysis of ERA-GNN and QL-GNN in 
Figure~\ref{fig:overview}.
As we can see, ERA-GNN (in blue),
such as R-GCN and CompGCN, 
can only handle relations with independent head and tail entities which are described with the logical rule in the form of $R(x,y)=f(\{\phi_i(x)\}, \{\psi_j(y)\})$ with $\phi_i(x),\psi_j(y)$ in 
$\text{CML}[\mathcal{K}]$, 
while QL-GNN (in green), 
such as NBFNet and RED-GNN, 
can learn the relations with dependent head and tail entities which can be described with formulas in the form of $R(\mathsf{h}, x)$ in $\text{CML}[\mathcal{K}, \mathsf{h}]$.

\section{A Labeling Method for Difficult Rules}\label{sec:complex-rule}

The logical expressiveness in Section~\ref{ssec:QL} shows that QL-GNN can capture the logical rule $R(\mathsf{h}, x)$ in $\text{CML}[\mathcal{K}, \mathsf{h}]$ for KG reasoning, e.g., $C(\mathsf{h}, x)$ and $I(\mathsf{h},x)$.
However, it is well-known that CML has limited expressiveness for describing graphs.
Specifically, any formula in CML cannot distinguish the counting bisimilar graph structures~\cite{blackburn_rijke_venema_2001,otto2019graded}, which means the counting bisimilar graph structures cannot be distinguished by QL-GNN.
For example, the rule structure of relation $U$ in 
Figure~\ref{fig:example-relation}(c) and the rule structure in Figure~\ref{fig:bisimilar} are counting bisimilar, thus can not be distinguished by QL-GNN.
Furthermore, the logical rule $U(\mathsf{h}, x)$ is a formula in $\text{FOC}[\mathcal{K}, \mathsf{h}]$ 
rather than $\text{CML}[\mathcal{K}, \mathsf{h}]$,
meaning that $U(\mathsf{h}, x)$ cannot be captured by QL-GNN based on Theorem~\ref{theorem:QL}.
To resolve this limitation, 
we propose a labeling method to improve the empirical performance of QL-GNN when learning difficult rules.


\begin{figure}[ht]
	\centering

	\includegraphics[width=.7\columnwidth]{img/bisimilar.pdf}
	\caption{An exemplar rule structure that is counting bisimilar 
		to the rule structure of relation $U$ in Figure~\ref{fig:example-relation}(c).}
	\label{fig:bisimilar}
\end{figure}

	


The motivation behind our method is to transform difficult rules
into formulas in CML by adding constants to KG.
Basically, 
assigning constants to entities with an out-degree large than one is enough to achieve this purpose
due to the following theorem.

\begin{theorem}\label{theorem:label}
	Assume $R(\mathsf{h}, x)$ in $\text{FOC}[\mathcal{K},\mathsf{h}]$ describes a single-connected rule structure
	$\mathsf{G}$ in $\mathcal{K}$.
	If we assign constants $\mathsf{c}_1, \mathsf{c}_2, \cdots, \mathsf{c}_k$ to entities with out-degree larger than 1 in $\mathcal{K}$, the rule structure $\mathsf{G}$ can be described with $R'(\mathsf{h}, x)$ in $\text{CML}[\mathcal{K},\mathsf{h},\mathsf{c}_1, \mathsf{c}_2, \cdots, \mathsf{c}_k]$.
\end{theorem}

With Theorem~\ref{theorem:label}, when encoding the constants 
into the initial features of GNN, 
the logical rule $R'(\mathsf{h},x)$ can be captured by QL-GNN along with the assigned constants due to Theorem~\ref{theorem:main}.
For example, 
when assigning constant $\mathsf{c}$ to 
the entity colored with gray in the rule structure of relation $U$
in Figure~\ref{fig:example-relation}(c), a new logical rule
\begin{align*}
	U'(\mathsf{h}, x) :=& R_1(\mathsf{h}, \mathsf{c}) \wedge        \big( \exists z_2, z_3, R_2(\mathsf{c}, z_2) \\
	 &\wedge R_4(z_2, x) \wedge R_3(\mathsf{c}, z_3) \wedge R_5(z_3, x) \big)    
\end{align*}
in $\text{CML}[\mathcal{K}, \mathsf{h}, \mathsf{c}]$ (Corollary~\ref{app:corollary:labeling} in Appendix~\ref{app:sec:rule-analysis}) can describe the rule structure in Figure~\ref{fig:example-relation}(c). 
Thus the logical rule $U'(\mathsf{h}, x)$ can be captured by QL-GNN with constant $\mathsf{c}$ and cannot be captured by ERA-GNN and vanilla QL-GNN as shown in Table~\ref{tab:case}.


However, assigning constants to all the entities with an out-degree larger than one 
will introduce lots of constants in KG, 
impeding the generalization of GNN~\cite{abboud2020surprising}.
Instead, we propose a heuristic labeling method called \textbf{E}ntity \textbf{L}abeling GNN (EL-GNN) in 
Algorithm~\ref{alg:labeling} 
that assigns constants to all the entities with out-degree larger than $d$, 
whose value is a hyper-parameter to be tuned.
A smaller $d$ makes GNN learn the logical rules with many constants with bad generalization,
while a larger $d$ may not be able to transform difficult logical rules into formulas in CML.
Same as the constant $\mathsf{h}$
in QL-GNN,
we add a unique feature $\mathbf{e}_v^0$ for entities $v$ whose out-degree $d_v>d$ in steps~2-6.
For the query entity $h$, 
we assign it 
with a unique initial feature $\mathbf{e}_h^0$ in step~7.





\begin{algorithm}[ht]
	\caption{Entity Labeling GNN (EL-GNN)}
	\begin{algorithmic}[1]
%		\small
		\REQUIRE query $(h, R, ?)$, knowledge graph $\mathcal{K}$, degree threshold $d$.
		\STATE compute the out-degree $d_v$ of each entity $v$ in $\mathcal{K}$;
		\FOR{entity $v$ in $\mathcal{K}$}\label{step:labeling_start}
			\IF{$d_v > d$}
				\STATE assign a unique feature $\mathbf{e}_v^0$ to entity $v$;
			\ENDIF
		\ENDFOR\label{step:labeling_end}
		\STATE assign initial feature $\mathbf{e}_h^0$ to the query entity $h$;\label{step:query_labeling}
	\STATE \textbf{Return:} initial feature of all entities
	\end{algorithmic}
	\label{alg:labeling}
\end{algorithm}

In summary, EL-GNN can learn relations with dependent head and tail entities 
which can be described with formulas in the form of $R(\mathsf{h},x)$ in $\text{CML}[\mathcal{K}, \mathsf{h}, \mathsf{c}_1, \cdots, \mathsf{c}_k]$,
including QL-GNN as a special case as shown in Figure~\ref{fig:overview}
(green and yellow ellipses).



\section{Experiment}

In this section, we conduct experiments on synthetic datasets to validate our theoretical findings in Section~\ref{sec:logic-exp} and demonstrate the effectiveness of the proposed EL-GNN on both synthetic and real datasets.
All the experiments are written in Python with PyTorch run on RTX 3090 GPUs with 24GB memory.


\begin{table}[ht]
	% \vspace{-8px}
	\centering
	\setlength\tabcolsep{8pt}
	\caption{Hit@1 on synthetic data.}
	\begin{tabular}{c|c|c|c}
		\toprule
		& $C$ & $I$ & $U$ \\ \midrule
		CompGCN         &    0.016                  &       0.039               &      0.027           \\ \midrule
		NBFNet          & 1.0                  & 1.0                  &   0.541                   \\   \midrule
		EL-GNN &    1.0                  &         1.0             &  0.838                    \\ \bottomrule
	\end{tabular}\label{tab:syn-data}
	% \vspace{-5px}
\end{table}

\begin{table*}[t]
    \centering
    \caption{Results of KG reasoning on real datasets. The best result is in ``\textbf{bold}''.}
    \label{tab:real_result}
	\renewcommand{\arraystretch}{1.05}
	\setlength\tabcolsep{2pt}
	\begin{tabular}{c|c|ccc|ccc|ccc|ccc}
		\toprule
		\multirow{2}{*}[-0.8ex]{Method Class} & \multirow{2}{*}[-0.8ex]{Methods} & \multicolumn{3}{c|}{Family}                                                                  & \multicolumn{3}{c|}{Kinship}                                                                 & \multicolumn{3}{c|}{ULMS}                                                                    & \multicolumn{3}{c}{WN18RR}                                                                  \\ \cmidrule{3-14} 
								 &  & {MRR}   &  {Hit@1}   & Hit@10  &  {MRR}   &  {Hit@1}   & Hit@10  &  {MRR}   &  {Hit@1}   & Hit@10  &  {MRR}   &  {Hit@1}   & Hit@10  \\ \midrule
		\multirow{2}{*}{Embedding-based} & RotatE & 0.921 & 0.866 & 0.988 & 0.807 &  0.703 & 0.975 & 0.925 & 0.863 & 0.993 &  0.477 & 0.428 & 0.571 \\
		& QuatE & 0.941 & 0.896 & 0.991 & 0.479 & 0.310 & 0.831 & 0.944 & 0.905 & 0.993 & 0.480 & 0.440 & 0.551 \\
		\midrule
		\multirow{2}{*}{Rule-based} & Neural LP & 0.924 & 0.871 & 0.994 & 0.620 & 0.480 & 0.910 &  0.745 & 0.627 & 0.918 & 0.435 & 0.371 & 0.566 \\
		& DRUM & 0.934 & 0.881 & \textbf{0.996} & 0.61 & 0.460 & 0.910 & 0.813 & 0.674 & 0.976 & 0.486 & 0.425 & 0.586 \\
		\midrule
		\multirow{3}{*}[-0.8ex]{GNN-based} & CompGCN & 0.933 & 0.883 & 0.991 & 0.840 & 0.751 & 0.979 & 0.927 & 0.867 & \textbf{0.994} & 0.479 & 0.443 & 0.546 \\
		& NBFNet                   &  \textbf{0.989} &  {0.987} & 0.989 &  {0.887} &  {0.806} & \textbf{0.995} &  {0.948} & 0.920 & {0.992} &  {0.551} &  {0.497} & 0.666 \\ \cmidrule{2-14}
		& EL-GNN          &  \textbf{0.989} &  \textbf{0.988} & {0.990} &  \textbf{0.888} &  \textbf{0.815} & 0.990 &  \textbf{0.957} &  \textbf{0.937} & 0.990 &  \textbf{0.556} &  \textbf{0.501} & \textbf{0.670} \\ \midrule
		\end{tabular}
	% \vspace{-3px}
\end{table*}


\subsection{Experiments on synthetic datasets}

We generate three KGs based on the relations in Figure~\ref{fig:example-relation} to validate our theory on logical expressiveness and verify that EL-GNN can improve the empirical performance of KG reasoning.
In this section, we use CompGCN, NBFNet and 
NBFNet with Algorithm~\ref{alg:labeling} to evaluate ERA-GNN, QL-GNN and EL-GNN, respectively.
Similar to \citet{sadeghian2019drum,zhu2021neural}, 
we use the filtered ranking metric Hit@1 for evaluation. 
This metric measures the proportion of correctly predicting the target tail entity,
with larger values indicating better performance.
We report the testing Hit@1 performance of ERA-GNN, QL-GNN and EL-GNN 
on three synthetic KGs.
The hyperparameters of all methods in this section are automatically tuned with the hyperparameter tuning library Ray~\cite{liaw2018tune}
by validation Hit@1 performance.


\paragraph{Dataset generation}
Given a target relation, there are three steps to generate a dataset:
\begin{enumerate}
	\item rule structure generation: generate specific rule structures according to those in Figure~\ref{fig:example-relation};
	\item noisy structure generation: generate noisy structures
	to avoid GNN from learning naive logical rules;
	\item triplet completion: generate missing triplets
	according to the target rule structures.
\end{enumerate}
The triplets generated from target rule structures and noisy structures are used as known facts in KG, 
and the triplets with target relation are separated as training, validation and testing sets. 



\paragraph{Results}
The empirical Hit@1 results of ERA-GNN, QL-GNN and EL-GNN on the three synthetic datasets,
which are generated according to the relations in Figure~\ref{fig:example-relation},
are reported in Table~\ref{tab:syn-data}.
For simplicity, we denote these three datasets as $C$, $I$ and $U$
according to the type of rule structures.
The experimental results here are consistent with our theory.
First, ERA-GNN shows poor results in all the three datasets 
since ERA-GNN cannot capture any logical rules behind the relations of these synthetic datasets 
as discussed in Section~\ref{ssec:EA}.
Second, facts with relations $C$ and $I$ are perfectly predicted by QL-GNN with $\text{Hit@1}=1.0$
since QL-GNN can capture the logical rules $C(\mathsf{h}, x)$ and $I(\mathsf{h}, x)$ as discussed in Section~\ref{ssec:case}.
Third, we observe that EL-GNN can indeed improve the empirical performance of learning the difficult logical rule from the experimental results on dataset $U$,
which is consistent with our analysis in Section~\ref{sec:complex-rule}.
Finally,
EL-GNN can also well capture the logical rules $C(\mathsf{h}, x)$ and $I(\mathsf{h}, x)$,
verifying its logical expressiveness.


In addition, 
we show the influence of degree threshold value $d$ on EL-GNN
with dataset $U$.
The testing Hit@1 performance plotted in Figure~\ref{fig:out-degree} shows that 
the out-degree $d$ with a too small or too large value impedes the generalization of EL-GNN.
Hence, the hyperparameter $d$ should be empirically tuned according to the validation performance.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\columnwidth]{img/h1_d.pdf}
	\vspace{-5px}
	\caption{Hit@1 versus out-degree $d$ of EL-GNN on the dataset with relation $U$. This figure shows that a too small or too large out-degree $d$ impedes the generalization of EL-GNN.}
		\label{fig:out-degree}
\end{figure}

\subsection{Experiments on real datasets} 
We perform experiments to verify the effectiveness of EL-GNN on four real datasets: 
Family~\cite{kok2007statistical}, Kinship~\cite{hinton1986learning}, UMLS~\cite{kok2007statistical} and WN18RR~\cite{dettmers2017convolutional}.
For a fair comparison, 
we evaluate EL-GNN (NBFNet with Algorithm~\ref{alg:labeling}) 
with the same set of hyperparameters as NBFNet and handcrafted $d$.
The experiment involves embedding-based methods (RotatE~\cite{sun2019rotate}, QuatE~\cite{zhang2019quaternion}), rule-based methods (Neural LP~\cite{yang2017differentiable}, DRUM~\cite{sadeghian2019drum}) and GNN-based methods (CompGCN~\cite{vashishth2019composition}, NBFNet~\cite{zhu2021neural}) for comparison.
To evaluate the performance, we use the filtered ranking metrics MRR (Mean Reciprocal Rank), Hit@1 and Hit@10 which are commonly used for evaluating models for KG reasoning~\cite{sadeghian2019drum,zhang2022knowledge}.
Larger values for these metrics indicate better performance. 


We report the testing MRR, Hit@1 and Hit@10~\cite{bordes2013translating} in Table~\ref{tab:real_result}.
Our experimental results demonstrate that GNN-based methods, particularly NBFNet (QL-GNN), outperform both embedding-based and rule-based methods. 
Additionally, our proposed EL-GNN improves the empirical performance of KG reasoning on real datasets. 
However, the improvement performance varies on different datasets as the difficult rules in different datasets are distinct.




\section{Conclusion}
In this paper, we analyze the logical expressiveness of GNN for KG reasoning. 
Our paper answers the question of what kind of logical rules can be captured by GNN in KG reasoning, which provides insights into why NBFNet and RED-GNN are good at inductive KG reasoning.
Furthermore, our theory motivates an effective labeling method 
for improving GNN's empirical performance 
in learning difficult logical rules.
In addition, the results in our paper can be directly extended to link prediction on the heterogeneous graph because KG is equivalent to the heterogeneous graph.
As a future work, we will extend the logical expressiveness to GNNs with general labeling trick, apply our theory and proposed method for scientific KG reasoning,
and try to extract explainable logical rules from the parameters of GNN.



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \cleardoublepage

\bibliography{bib}
\bibliographystyle{icml2023}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Related work}

GNN-based methods for KG reasoning have been introduced in Section~\ref{sec:rel:kgr}. In the appendix, we will introduce the other two classes of methods for KG reasoning, i.e., embedding-based methods and rule-based methods.

Embedding-based methods, such as TransE~\cite{bordes2013translating}, ComplEx~\cite{trouillon2016complex}, RotatE~\cite{sun2019rotate} and QuatE~\cite{zhang2019quaternion}, directly calculate the score based on the embeddings $\mathbf{e}_h, \mathbf{e}_R, \mathbf{e}_t$ of the query entity $h$, query relation $R$ and candidate entity $t$. 
For example, the score function of TransE is $s(h, R, t)=-\| \mathbf{e}_h+ \mathbf{e}_R- \mathbf{e}_t \|$.


Rule-based methods are generally divided into two main categories for incorporating logic into KG reasoning: rule-composition methods and rule-mining methods.
Rule-composition methods involve the combination of a set of predefined logical rules to deduce new facts based on known ones in KG. 
These methods are often based on first-order logic, and they can be effective at making precise and accurate inferences. 
Classical rule-composition methods include ILP~\cite{lavrac1994inductive} and MLN~\cite{richardson2006markov}.
Rule-mining methods, on the other hand, involve the use of machine learning to learn the logical rule of some relations in KG based on given rule structures and make inferences based on the learned logical rule. 
Classical rule-mining methods include Neural LP~\cite{yang2017differentiable}, DRUM~\cite{sadeghian2019drum}.
These methods are powerful for discovering new logical rules in a KG and enable machines to reason in a way similar to humans in KG.

In summary, GNN-based methods achieve state-of-the-art performance in KG reasoning but lack a theoretical certification for their good empirical performance.
While embedding-based methods are effective for KG reasoning, they are not suitable for inductive KG reasoning and lack a theoretical foundation for their good performance.
Rule-based methods are effective for solving inductive KG reasoning and offer good interpretability,
but the predefined rules or rule structures that they used to predict new facts can be time-consuming and difficult to obtain. 
In addition, the predefined rules and rule structures may not be flexible enough to capture the full complexity of the relationships in KG, which can limit the accuracy and effectiveness of the rule-based methods.


\section{Rule analysis}\label{app:sec:rule-analysis}

We use the notation $\mathcal{K}, v \models P_i$ ($\mathcal{K}, v \nvDash P_i$) to represent that the unary predicate $P_i(x)$ is (not) satisfied at entity $v$.
\begin{definition}[Definition of graded modal logic]\label{app:def:CML}
	\label{def:gralogic}
	A formula in graded modal logic of knowledge graph $\mathcal{K}$ is recursively defined as follows:
	\begin{enumerate}
	\item  
	If $\varphi(x) = \top$, $\mathcal{K}, v \models \varphi$ if $v$ is an entity in knowledge graph;
	
	\item If $\varphi(x) = P_c(x)$, $\mathcal{K}, v \models \varphi$ if and only if $v$ has the property $c$ or can be uniquely identified by constant $\mathsf{c}$;
	
	\item If $\varphi(x) = \varphi_1(x) \wedge \varphi_2(x)$, $\mathcal{K}, v \models \varphi$ if and only if $\mathcal{K}, v \models \varphi_1$ and $\mathcal{K}, v \models \varphi_2$;
	
	\item If $\varphi(x) = \neg\phi(x)$, $\mathcal{K}, v \models \varphi$ if and only if $\mathcal{K}, v \nvDash \phi$;
	
	\item If $\varphi(x) = \exists^{\geq N}y, R_j(y, x)\wedge \phi(y)$, $\mathcal{K}, v \models \varphi$ if and only if the set of entities $\{u | u\in \mathcal{N}_{R_j}(v) \text{ and } \mathcal{K}, u \models \phi \}$ has cardinality at least $N$.
	\end{enumerate}
\end{definition}

\begin{corollary}\label{app:corollary:chain}
	$C(\mathsf{h}, x)$ is a formula in $\text{CML}[\mathcal{K}, \mathsf{h}]$.
\end{corollary}
\begin{proof}
	$C(\mathsf{h}, x)$ is a formula in $\text{CML}[\mathcal{K}, \mathsf{h}]$ as it can be recursively defined as follows
	\begin{align*}
		\varphi_1(x) &= P_h (x), \\
		\varphi_2 (x) &= \exists y, R_1 (y, x) \wedge \varphi_1(y), \\
		\varphi_3 (x) &= \exists y, R_2 (y, x) \wedge \varphi_2(y), \\
		C(\mathsf{h}, x) &= \exists y, R_3 (y, x) \wedge \varphi_3(y).
	\end{align*}
\end{proof}

\begin{corollary}\label{app:corollary:inductive}
	$I(\mathsf{h}, x)$ is a formula in $\text{CML}[\mathcal{K}, \mathsf{h}]$.
\end{corollary}
\begin{proof}
	$I(\mathsf{h}, x)$ is a formula in $\text{CML}[\mathcal{K}, \mathsf{h}]$ as it can be recursively defined as follows
	\begin{align*}
		\varphi_1(x) &= P_h (x), \\
		\varphi_2 (x) &= \exists y, R_1 (y, x) \wedge \varphi_1(y), \\
		\varphi_3 (x) &= \exists y, R_2 (y, x) \wedge \varphi_2(y), \\
		\varphi_s (x) &= \exists^{\geq 2}y, R_4(y, x) \wedge \top, \\
		\varphi_4 (x) &= \varphi_s(x) \wedge \varphi_3(x), \\
		I(\mathsf{h}, x) &= \exists y, R_3 (y, x) \wedge \varphi_4(y).
	\end{align*}
\end{proof}

\begin{corollary}\label{app:corollary:labeling}
	$U'(\mathsf{h}, x)$ is a formula in $\text{CML}[\mathcal{K}, \mathsf{h}, \mathsf{c}]$.
\end{corollary}
\begin{proof}
	$U'(\mathsf{h}, x)$ is a formula in $\text{CML}[\mathcal{K}, \mathsf{h}, \mathsf{c}]$ as it can be recursively defined as follows
	\begin{align*}
		\varphi_1(x) &= P_h(x), \varphi_c(x) = P_c(x), \\
		\varphi_2(x) &= \exists y, R_1(y, x) \wedge \varphi_1(y), \\
		\varphi_3(x) &= \varphi_2(x) \wedge \varphi_c(x), \\
		\varphi'_4(x) &= \exists y, R_2(y, x) \wedge \varphi_3(y), \\
		\varphi'_5(x) &= \exists y, R_4(y, x) \wedge \varphi'_4(y), \\
		\varphi''_4(x) &= \exists y, R_3(y, x) \wedge \varphi_3(y), \\
		\varphi''_5(x) &= \exists y, R_5(y, x) \wedge \varphi''_4(y), \\
		U'(\mathsf{h}, x) &= \varphi'_5(x) \wedge \varphi''_5(x)
	\end{align*}
	where the constant $\mathsf{c}$ ensures that there is only one entity satisfied for unary predicate $\varphi_3(x)$.
\end{proof}

\section{Proof}\label{app:proof}

We use the notation $\mathcal{K}, (h,t) \models R_j$ ($\mathcal{K}, (h,t) \nvDash R_j$) to denote $R_j(x,y)$ is (not) satisfied at $h,t$.
\subsection{Proof of Theorem~\ref{theorem:main}}

The backward direction of Theorem~\ref{theorem:main} is proved by constructing a GNN that can capture any formula $\varphi(x)$ in CML. 
The forward direction relies on the results from recent theoretical results in \citet{otto2019graded}. 
Our theorem can be seen as an extension of Theorem 4.2 in \citet{barcelo2020logical} to heterogeneous graph (KG). 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We first prove the backward direction of Theorem~\ref{theorem:main}.
\begin{lemma}\label{app:lemma:backward}
    Each formula $\varphi(x)$ in CML can be captured by GNN~\eqref{eq:GNN} from its representations.
\end{lemma}
\begin{proof}
    Let $\varphi(x)$ be a formula in CML. We decompose $\varphi$ into a series of sub-formulas $\text{sub}[\varphi]=(\varphi_1, \varphi_2, \cdots, \varphi_L)$ where $\varphi_k$ is a sub-formula of $\varphi_\ell$ if $k \leq \ell$ and $\varphi = \varphi_L$. 
	Assume the GNN representation $\mathbf{e}_v^i\in\mathbb{R}^L, v\in\mathcal{V}, i=1\cdots L$.
	In our paper, the theoretical analysis will based on the following simple choice of \eqref{eq:GNN}\footnote{Results for our paper hold for \eqref{eq:GNN} that is at least as powerful as \eqref{app:eq:simple-gnn}, while negative results for \eqref{app:eq:simple-gnn} holds for general \eqref{eq:GNN}.}
	\begin{align}\label{app:eq:simple-gnn}
		\mathbf{e}_v^{i}=\sigma \left( \mathbf{e}_v^{i-1}\mathbf{C} + \sum_{j=1}^r\sum_{u\in\mathcal{N}_{R_j}(v)} \mathbf{e}_u^{i-1} \mathbf{A}_{R_j} + \mathbf{b} \right)
	\end{align}
	with $\sigma=\min(\max(0,x), 1)$, $\mathbf{A}_{R_j}, \mathbf{C}\in\mathbb{R}^{L\times L}$ and $\mathbf{b}\in\mathbb{R}^L$.
	The entries of the $\ell$-th columns of $\mathbf{A}_{R_j}, \mathbf{C}$, and $\mathbf{b}$ depend on the sub-formulas of $\varphi$ as follows:
    \begin{itemize}
        \item \textbf{Case 0.} if $\varphi_\ell(x)=P_\ell(x)$ where $P_
		\ell$ is a unary predicate, $\mathbf{C}_{\ell \ell}=1$;
        \item \textbf{Case 1.} if $\varphi_\ell(x)=\varphi_j(x) \wedge \varphi_k(x)$, $\mathbf{C}_{j\ell}=\mathbf{C}_{k\ell}=1$ and $\mathbf{b}_{\ell}=-1$;
        \item \textbf{Case 2.} if $\varphi_\ell = \neg \varphi_{k}(x)$, $\mathbf{C}_{k\ell}=-1$ and $\mathbf{b}_\ell=1$;
        \item \textbf{Case 3.} if $\varphi_\ell(x)=\exists^{\geq N}y \left( R_j(y,x) \wedge \varphi_k(y) \right)$, $\left( \mathbf{A}_{R_j} \right)_{k\ell} = 1$ and $\mathbf{b}_\ell = -N + 1$.
    \end{itemize}
    with all the other values set to 0.


    Before the proof, for every entity $v\in\mathcal{V}$, the initial feature $\mathbf{e}_v^0=(t_1, t_2, \cdots, t_n)$ has $t_\ell=1$ if the sub-formula $\varphi_\ell = P_\ell(x)$ is satisfied at $v$, and $t_\ell=0$ otherwise.


    Let $\mathcal{K}=(\mathcal{V}, \mathcal{F}, \mathcal{R})$ be a knowledge graph. 
	We next prove that for every $\varphi_\ell \in \text{sub}[\varphi]$ and every entity $v\in\mathcal{V}$ it holds that
    \begin{equation*}
        \left( \mathbf{e}_v^i \right)_\ell = 1 \quad
        \text{if} \quad \mathcal{K}, v \models \varphi_\ell,\quad \text{and} \quad  \left( \mathbf{e}_v^i \right)_\ell = 0 \quad \text{otherwise},
    \end{equation*}
	for every $\ell \leq i \leq L$.

	
	Now, we prove this by induction of the number of formulas in $\varphi$.

    \textbf{Base case:} One sub-formula in $\varphi$. In this case, the formula is an atomic predicate $\varphi=\varphi_\ell(x)=P_\ell(x)$. 
	Because $\mathbf{C}_{\ell \ell}=1$ and $(\mathbf{e}_v^0)_\ell=1, (\mathbf{e}_v^0)_i=0, i\neq \ell$, we have $(\mathbf{e}_v^1)_\ell=1$ if $\mathcal{K}, v \models \varphi_\ell$ and $(\mathbf{e}_v^1)_\ell=0$ otherwise.
	For $i\geq 1$, $\mathbf{e}_v^i$ satisfies the same property.

    \textbf{Induction Hypothesis:} $k$ sub-formulas in $\varphi$ with $k < \ell$. Assume $\left(\mathbf{e}_v^i \right)_{k}=1$ if $\mathcal{K}, v \models \varphi_k$ and $\left(\mathbf{e}_v^i \right)_{k}=0$ otherwise for $k \leq i \leq L$.

	\textbf{Proof:} $\ell$ sub-formulas in $\varphi$. Let $i\geq \ell$. 
    Case 1-3 should be considered. 

    Case 1. Let $\varphi_\ell(x)=\varphi_j(x) \wedge \varphi_k(x)$. Then $\mathbf{C}_{j\ell}=\mathbf{C}_{k\ell}=1$ and $\mathbf{b}_{\ell}=-1$.
	Then we have
	\begin{align*}
		(\mathbf{e}_v^i)_\ell = \sigma \left( (\mathbf{e}_v^{i-1})_j + (\mathbf{e}_v^{i-1})_k - 1 \right).
	\end{align*}
	By the induction hypothesis, $(\mathbf{e}_v^{i-1})_j=1$ if only if $\mathcal{K}, v \models \varphi_j$ and $(\mathbf{e}_v^{i-1})_j=0$ otherwise.
	Similarly, $(\mathbf{e}_v^{i-1})_k=1$ if and only if $\mathcal{K}, v \models \varphi_k$ and $(\mathbf{e}_v^{i-1})_k=0$ otherwise.
	Then we have $(\mathbf{e}_v^i)_\ell=1$ if and only if $(\mathbf{e}_v^{i-1})_j + \mathbf{e}_v^{i-1})_k - 1 \geq 1$, which means $(\mathbf{e}_v^{i-1})_j=1$ and $(\mathbf{e}_v^{i-1})_k=1$.
	Then $(\mathbf{e}_v^i)_\ell=1$ if and only if $\mathcal{K}, v \models \varphi_j$ and $\mathcal{K}, v \models \varphi_k$, i.e., $\mathcal{K}, v \models \varphi_\ell$, and $(\mathbf{e}_v^i)_\ell=0$ otherwise.


	Case 2. Let $\varphi_\ell(x)=\neg \varphi_k(x)$. Because of $\mathbf{C}_{k\ell}=-1$ and $\mathbf{b}_\ell=1$, we have
	\begin{align*}
		(\mathbf{e}_v^i)_\ell = \sigma \left( -(\mathbf{e}_v^{i-1})_k+1 \right).
	\end{align*}
	By the induction hypothesis, $(\mathbf{e}_v^{i-1})_k=1$ if and only if $\mathcal{K}, v \models \varphi_k$ and $(\mathbf{e}_v^{i-1})_k=0$ otherwise.
	Then we have $(\mathbf{e}_v^i)_\ell=1$ if and only if $-(\mathbf{e}_v^{i-1})_k + 1 \geq 1$, which means $(\mathbf{e}_v^{i-1})_k=0$.
	Because $(\mathbf{e}_v^{i-1})_k=0$ if and only if $\mathcal{K}, v \nvDash \varphi_k$,
	we have $(\mathbf{e}_v^i)_\ell=1$ if and only if $\mathcal{K}, v \nvDash \varphi_k$, i.e., $\mathcal{K}, v \models \varphi_\ell$, and $(\mathbf{e}_v^i)_\ell=0$ otherwise.


	Case 3. Let $\varphi_\ell(x)=\exists^{\geq N}y \left( R_j(y,x) \wedge \varphi_k(y) \right)$. Because of $\left( \mathbf{A}_{R_j} \right)_{k\ell} = 1$ and $\mathbf{b}_\ell = -N + 1$, we have
	\begin{align*}
		(\mathbf{e}_v^i)_\ell = \sigma \left( \sum_{u\in\mathcal{N}_{R_j}(v)} (\mathbf{e}_u^{i-1})_k - N + 1 \right).
	\end{align*}
	By the induction hypothesis, $(\mathbf{e}_u^{i-1})_k=1$ if and only if $\mathcal{K}, u \models \varphi_k$ and $(\mathbf{e}_u^{i-1})_k=0$ otherwise.
	Let $m = |\{ u | u \in \mathcal{N}_{R_j}(v) \text{ and } \mathcal{K}, u \models \varphi_k \}|$.
	Then we have $(\mathbf{e}_v^i)_\ell=1$ if and only if $\sum_{u\in\mathcal{N}_{R_j}(v)} (\mathbf{e}_u^{i-1})_k - N + 1 \geq 1$, which means $m \geq N$.
	Because $\mathcal{K}, u \models \varphi_k$, $u$ is connected to $v$ with relation $R_j$, and $m\geq N$, we have $(\mathbf{e}_v^i)_\ell=1$ if and only if $\mathcal{K}, v \models \varphi_\ell$ and $(\mathbf{e}_v^i)_\ell=0$ otherwise.


	To capture a logical rule $\varphi(x)$, we only apply a linear classifier to $\mathbf{e}_v^L, v\in\mathcal{V}$ to extract the component of $\mathbf{e}_v^L$ corresponding to $\varphi$.
	If $\mathcal{K}, v \models \varphi$, the value of the corresponding extracted component is 1.



\end{proof}

Next, we prove the forward direction of Theorem~\ref{theorem:main}.
\begin{theorem}\label{app:theorem:forward}
    A formula $\varphi(x)$ is captured by GNN~\eqref{eq:GNN} if it can be expressed as a formula in CML.
\end{theorem}

To prove Theorem~\ref{app:theorem:forward}, we introduce Definition~\ref{app:def:unravelling}, Lemma~\ref{app:lemma:WL-unr}, Theorem~\ref{app:theorem:otto}, and Lemma~\ref{app:lemma:forward-contrary-proposition}.
\begin{definition}[Unraveling tree]\label{app:def:unravelling}
    Let $\mathcal{K}$ be a knowledge graph, $v$ be entity in $\mathcal{K}$, and $L\in\mathbb{N}$. The unravelling of $v$ in $\mathcal{K}$ at depth $L$, denoted  by $\text{Unr}_{\mathcal{K}}^L (v)$, is a tree composed of
    \begin{itemize}
        \item a node $(v, R_1, u_1, \cdots, R_i, u_i)$ for each path $(v, R_1, u_1, \cdots, R_i, u_i)$ in $\mathcal{K}$ with $i\leq L$,
        \item an edge $R_i$ between $(v, R_1, u_1, \cdots, R_{i-1}, u_{i-1})$ and $(v, R_1, u_1, \cdots, R_i, u_i)$ when $(u_{i}, R_i, u_{i-1})$ is a triplet in $\mathcal{K}$ (assume $u_0$ is $v$), and
        \item each node $(v, R_1, u_1, \cdots, R_i, u_i)$ has the same properties as $u_i$ in $\mathcal{K}$.
    \end{itemize}
\end{definition}

\begin{lemma}\label{app:lemma:WL-unr}
    Let $\mathcal{K}$ and $\mathcal{K}'$ be two knowledge graphs, $v$ and $v'$ be two entities in $\mathcal{K}$ and $\mathcal{K}'$ respectively. Then for every $L \in \mathbb{N}$, the WL test assigns the same color/hash to $v$ and $v'$ at round $L$ if and only if there is an isomorphism between $\text{Unr}_{\mathcal{K}}^L(v)$ and $\text{Unr}_{\mathcal{K}'}^L(v')$ sending $v$ to $v'$.
\end{lemma}
\begin{proof}
    \textbf{Base Case:} When $L=1$, the result is obvious.

    \textbf{Induction Hypothesis:} WL test assigns the same color to $v$ and $v'$ at round $L-1$ if and only if there is an isomorphism between $\text{Unr}_{\mathcal{K}}^{L-1}(v)$ and $\text{Unr}_{\mathcal{K}'}^{L-1}(v')$ sending $v$ to $v'$.

    \textbf{Proof:} In the $L$-th round,

    $\bullet$ Prove ``same color $\Rightarrow$ isomorphism''.

    \begin{align*}
        c^{L}(v) =& \text{hash}(c^{L-1}(v), \big\{\big\{ (c^{L-1}(u), R_i) | u \in \mathcal{N}_{R_i}(v), i=1,\cdots,r \big\} \big\}), \\
        c^{L}(v') =& \text{hash}(c^{L-1}(v'), \big\{\big\{ (c^{L-1}(u'), R_i) | u \in \mathcal{N}_{R_i}(v'), i=1,\cdots,r \big\} \big\}).
    \end{align*}
    Because $c^L(v)=c^L(v')$, we have $c^{L-1}(v)=c^{L-1}(v')$, and there exists an entity pair $(u, u'), u \in \mathcal{N}_{R_i}(v), u' \in \mathcal{N}_{R_i}(v')$ that
    \begin{align*}
        (c^{L-1}(u), R_i) = (c^{L-1}(u'), R_i).
    \end{align*}
    Then we have $c^{L-1}(u)=c^{L-1}(u')$. According to induction hypothesis, we have $\text{Unr}_{\mathcal{K}}^{L-1}(u) \cong  \text{Unr}_{\mathcal{K}'}^{L-1}(u')$. Also, because the edge connecting entity pair $(v, u)$ and $(v', u')$ is $R_i$, so there is an isomorphism between $\text{Unr}_{\mathcal{K}}^{L}(v)$ and $\text{Unr}_{\mathcal{K}'}^{L}(v')$ sending $v$ to $v'$.

    $\bullet$ Prove ``isomorphism $\Rightarrow$ same color''.

    Because there exists an isomorphism $\pi$ between $\text{Unr}_{\mathcal{K}}^{L}(v)$ and $\text{Unr}_{\mathcal{K}'}^{L}(v')$ sending $v$ to $v'$, assume $\pi$ is an bijective between the neighbors of $v$ and $v'$, e.g, $ u \in \mathcal{N}_{R_i}(v), u' \in \mathcal{N}_{R_i}(v') $ and $u_i'=\pi(u_i)$, the relation between entity pair $(u, v)$ and $(u', v')$ is $R_i$. 
    
Next we prove $c^{L-1}(u)=c^{L-1}(u')$.
Because $\text{Unr}_{\mathcal{K}}^{L}(v)$ and $\text{Unr}_{\mathcal{K}}^{L}(v')$ are isomorphism, and $\pi$ maps $u\in \mathcal{N}_{R_i}(v)$ to $u' \in \mathcal{N}_{R_i}(v')$, for the left tree with $L-1$ depth, i.e., $\text{Unr}_{\mathcal{K}}^{L-1}(u)$ and $\text{Unr}_{\mathcal{K}'}^{L-1}(u')$, $\pi$ can be the isomorphism mapping between $\text{Unr}_{\mathcal{K}}^{L-1}(u)$ and $\text{Unr}_{\mathcal{K}'}^{L-1}(u')$. According to induction hypothesis, we have $c^{L-1}(u)=c^{L-1}(u')$. Because $\text{Unr}_{\mathcal{K}}^{L}(v) \cong \text{Unr}_{\mathcal{K}'}^{L}(v')$, we also have $\text{Unr}_{\mathcal{K}}^{L-1}(u) \cong \text{Unr}_{\mathcal{K}'}^{L-1}(u')$ which means $c^{L-1}(u)=c^{L-1}(u')$. After running WL test, we have $c^L(v)=c^L(v')$.
\end{proof}

\begin{theorem}\label{app:theorem:otto}
    Let $\varphi(x)$ be a unary formula in FOC of KG. If $\varphi(x)$ is not equivalent to a formula in CML, there exist two knowledge graphs $\mathcal{K}$ and $\mathcal{K}'$ and two entities $v$ in $\mathcal{K}$ and $v'$ in $\mathcal{K}'$ such that $\text{Unr}_{\mathcal{K}}^L(v) \cong \text{Unr}_{\mathcal{K}'}^L(v')$ for every $L\in\mathbb{N}$ and such that $\mathcal{K}, v \models \varphi$ but $\mathcal{K}', v' \nvDash \varphi$.
\end{theorem}
\begin{proof}
    The theorem follows directly from Theorem 2.2 in \citet{otto2019graded}. 
    Because $\mathcal{K}, v \sim_{\#} \mathcal{K}', v'$ and $\text{Unr}_{\mathcal{K}}^L(v) \cong \text{Unr}_{\mathcal{K}'}^L(v')$ are equivalent with the definition of counting bisimulation (i.e., notation $\sim_{\#}$).
\end{proof}

\begin{lemma}\label{app:lemma:forward-contrary-proposition}
    If a formula $\varphi(x)$ is not equivalent to any formula in CML,
    there is no GNN~\eqref{eq:GNN} that can capture $\varphi(x)$.
\end{lemma}
\begin{proof}
    Assume for a contradiction that there exists a GNN that can captures $\varphi(x)$.
    Since $\varphi(x)$ is not equivalent to any formula in CML, with Theorem~\ref{app:theorem:otto}, there exists two knowledge graphs $\mathcal{K}$ and $\mathcal{K}'$ and two entities $v$ in $\mathcal{K}$ and $v'$ in $\mathcal{K}'$ such that $\text{Unr}_{\mathcal{K}}^L(v) \cong \text{Unr}_{\mathcal{K}'}^L(v')$ for every $L\in\mathbb{N}$ and such that $\mathcal{K}, v \models \varphi$ and $\mathcal{K}', v' \nvDash \varphi$. 
    By Lemma~\ref{app:lemma:WL-unr}, because $\text{Unr}_{\mathcal{K}}^L(v) \cong \text{Unr}_{\mathcal{K}'}^L(v')$ for every $L\in\mathbb{N}$, we have $\mathbf{e}_{v}^L=\mathbf{e}_{v'}^L$. But this contradicts the assumption that GNN is supposed to capture $\varphi(x)$.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{app:theorem:forward}]
	Theorem can be obtained directly from Lemma~\ref{app:lemma:forward-contrary-proposition}.
\end{proof}


\begin{proof}[Proof of Theorem~\ref{theorem:main}]
	Theorem can be obtained directly by combining Lemma~\ref{app:lemma:backward} and Theorem~\ref{app:theorem:forward}.
\end{proof}

\subsection{Proof of Theorem~\ref{theorem:EA}}
\begin{proof}
	According to Theorem~\ref{theorem:main}, the GNN representation $\mathbf{e}_v^L$ can capture the formulas in $\text{CML}[\mathcal{K}]$.
	Assume $\varphi_1(x)$ and $\varphi_2(y)$ can be captured by the GNN representation $\mathbf{e}_v^L,v\in\mathcal{V}$ and there exists two functions $g_1$ and $g_2$ that can extract the logical rules from $\mathbf{e}_v^L$, i.e., $g_i(\mathbf{e}_v^L)=1$ if $\mathcal{K}, v \models \varphi_i$ and $g_i(\mathbf{e}_v^L)=0$ if $\mathcal{K}, v \nvDash \varphi_i$ for $i=1,2$.
	We show how the following two logical operators can be captured by $s(h,R,t)$ for candidate triplet $(h,R,t)$: 
	\begin{itemize}
		\item Conjunction: $\varphi_1(x) \wedge \varphi_2(y)$. The conjunction of $\varphi_1(x), \varphi_2(y)$ can be captured with function $s(h,R,t)=g_1(\mathbf{e}_h^L)\cdot g_2(\mathbf{e}_t^L)$.
		
		\item Negation: $\neg \varphi_1(x)$. The negation of $\varphi_1(x)$ can be captured with function $s(h,R,t)=1-g_1(\mathbf{e}_h^L)$.
	\end{itemize}
	The disjunction $\vee$ can be obtained by $\neg (\neg \varphi_1(x) \wedge \neg \varphi_2(y))$.
	More complex formula involving sub-formulas from $\{\phi_i(x)\}$ and $\{\psi_j(y)\}$ can be captured by combining the score functions above.	
\end{proof}
\subsection{Proof of Theorem~\ref{theorem:QL}}
\begin{proof}
	We set the knowledge graph as $\mathcal{K}$ and restrict the unary formulas in $\text{CML}[\mathcal{K},\mathsf{h}]$ to the form of $R(\mathsf{h}, x)$.
	This theorem is directly obtained by Theorem~\ref{theorem:main}.
\end{proof}

\subsection{Proof of Theorem~\ref{theorem:label}}

\begin{lemma}\label{app:lemma:label}
	Assume $\varphi(x)$ describes a single-connected rule structure $\mathsf{G}$ in a knowledge graph.
	If assign constant to entities with out-degree large than 1 in the knowledge graph, the structure $\mathsf{G}$ can be described with formula $\varphi'(x)$ in CML of knowledge graph with assigned constants.
\end{lemma}
\begin{proof}
	According to Theorem~\ref{app:theorem:otto}, assume $\varphi'(x)$ with assigned constants is not equivalent to a formula in CML, there should exist two rule structures $\mathsf{G}, \mathsf{G}'$ in knowledge graphs $\mathcal{K}, \mathcal{K}'$, and entity $v$ in $\mathsf{G}$ and entity $v'$ in $\mathsf{G}'$ such that $\text{Unr}_{\mathsf{G}}^L(v) \cong \text{Unr}_{\mathsf{G}'}^L(v')$ for every $L\in\mathbb{N}$ and such that $\mathsf{G}, v \models \varphi'$ but $\mathsf{G}', v' \nvDash \varphi'$.

	Since each entity in $\mathsf{G}$ ($\mathsf{G}'$) with out-degree larger than 1 is assigned with a constant, the rule structure $\mathsf{G}$ ($\mathsf{G}'$) can be uniquely recovered from its unravelling tree $\text{Unr}_{\mathsf{G}}^L(v)$ ($\text{Unr}_{\mathsf{G}'}^L(v)$) for sufficient large $L$.
	Therefore, if $\text{Unr}_{\mathsf{G}}^L(v) \cong \text{Unr}_{\mathsf{G}'}^L(v')$ for every $L\in\mathbb{N}$, the corresponding rule structures $\mathsf{G}$ and $\mathsf{G}'$ should be isomorphism too, which means $\mathsf{G}, v \models \varphi'$ and $\mathsf{G}', v' \models \varphi'$.
	Thus, $\varphi'(x)$ must be a formula in CML.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{theorem:label}]
	The theorem holds by restricting the unary formula to the form of $R(\mathsf{h}, x)$ on Lemma~\ref{app:lemma:label}. 
\end{proof}





\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
