We first show in this section an assessment of \glspl{NN} at run-time that highlights their methodological advantages compared to conventional solvers. We then perform a comprehensive analysis of the required training phase and the effect of physics regularisation.

\subsection{\glspl{NN} at run-time - opportunities for accuracy and computational cost}\label{subsec:NN_run_time}

The primary motivation for the use of \gls{NN}-based solution approaches is their extremely fast evaluation. \Cref{fig:runtime_aspects} shows the run-time for different prediction times. The \glspl{NN} return the value of the states at prediction time $t$ between 10 and 1'000 times faster than the conventional solvers depending on three factors: the prediction time, the power system size, and the solver/\gls{NN} settings.

\begin{figure}[!th]
    \centering
    \includegraphics{timing_results.pdf}
    \caption{Run-time as a function of the prediction time $t$ for \glspl{NN} of different size and a conventional solver with varied tolerance settings $\epsilon$. Tests for the 11-bus and 39-bus system with a disturbance $P_i = \SI{6.09}{\pu}$.}
    \label{fig:runtime_aspects}
\end{figure}

First, for \glspl{NN} the run-time is independent of the prediction time as the prediction only requires a single evaluation of the \gls{NN}. In contrast, the conventional solver's run-time increases with larger prediction times as more internal time steps are required. Second, the power system size strongly affects the conventional solver's run-time as shown by the increase when moving from the 11-bus to the 39-bus system. For the \gls{NN}, it causes only a negligible change in run-time as only the last layer of the \gls{NN} changes in size according to the number of states of the system, see \labelcref{eq:NN_output}. Third, the \say{solver settings} play an important role; for conventional solvers, the internal tolerance setting $\epsilon$ governs its evaluation speed, while for the \gls{NN} the size, i.e., its number of layers $K$ and number of neurons per layer $N_K$, determine the run-time.

\begin{figure}[!th]
    \centering
    \includegraphics{accuracy_run_time.pdf}
    \caption{Evaluation of run-time and accuracy for the 11-bus system for varied solver tolerances $\epsilon$ and \gls{NN} sizes ($K$ layers and $N_K$ neurons per layers). Point-wise evaluation for 10 disturbance sizes $P_7$ and prediction time as in \cref{fig:runtime_aspects}.}
    \label{fig:runtime_accuracy}
\end{figure}

\Cref{fig:runtime_accuracy} sets the above results in relation to the achieved accuracy. The points represent different disturbance sizes and prediction times and the accuracy is measured as the associated loss. If a solver yielded points in the lower left corner of the plot, it could be called an ideal solver -- fast and accurate. Conventional solvers can be very accurate when the internal tolerance $\epsilon$ is set low enough, but at the price of being slower to evaluate. Allowing larger tolerances accelerates the solution process slightly at the expense of less accurate solutions. However, this trade-off is limited by the numerical stability of the used scheme; for too high tolerances the results would be considered as non-converged. In case of \glspl{NN}, their superior speed is weighed against less accurate solutions. The accuracy of \glspl{NN} is not only controlled by their size but also, very importantly, by the training process. The achievable accuracy is therefore determined before run-time, in contrast to the tolerance of a conventional solver, which is set at run-time. As a final remark related to \Cref{fig:runtime_accuracy}, we need to highlight that while less adjustable, in contrast to conventional solvers, \glspl{NN} do not face issues of numerical stability as their evaluation is a single and explicit function call.

We lastly want to show how the accuracy, here expressed as the maximum absolute error across all voltage angle states $\max \text{AE}_\delta$ for better intuition, relates to the \gls{NN} size and the power system size. The boxplots in \cref{fig:accuracy_max_errors} represent the evaluation of 20 \glspl{NN} with the same training setup but with different random initialisations of their parameters. We observe that deeper and wider \glspl{NN} usually perform better on this metric. However, the largest \glspl{NN} for the 11-bus system ($N_K=128$ and $K=4$ or $K=5$) show a larger variation than the smaller \gls{NN} which means that the initialisation of the \glspl{NN} affect their performance on the test dataset. This arises in models with a large representational capacity, loosely speaking models with many parameters, hence multiple parameter sets can lead to a low training loss but not all of them generalise well, i.e., have low error on the test dataset. The other, at first sight counter-intuitive, observation is that the 39-bus system performs better on the metric than the smaller 11-bus system. This can be attributed to the complexity of the target function, i.e., of the dynamic responses. The 11-bus system exhibits faster and more intricate dynamics for the presented cases, hence, it is more difficult to approximate their evolution. We could therefore achieve the same level accuracy for the 39-bus system with a smaller \gls{NN} than for the 11-bus system. In terms of run-time, this would mean that the 39-bus system could be faster to evaluate than the 11-bus system. This characteristic of \glspl{NN} effectively overcomes the relationship seen for conventional solvers that larger systems cause longer run-times\footnote{Of course, this relationship breaks when the necessary time step sizes of the conventional solvers differ significantly.} as we have seen in \cref{fig:runtime_aspects}.    

\begin{figure}[!th]
    \centering
    \includegraphics{boxplots_accuray_NN_size.pdf}
    \caption{Maximum absolute error of angle $\delta$ on the test dataset for the 11-bus and 39-bus system with varying \gls{NN} sizes, i.e., number of layers $K$ and neurons per layer $N_K$.}
    \label{fig:accuracy_max_errors}
\end{figure}

\subsection{\glspl{NN} at training time - a trade-off between accuracy and computational cost}\label{subsec:NN_training}

The benefits of \glspl{NN} compared to conventional solvers at run-time become possible by shifting the computational burden to the \gls{NN} training stage, i.e., the pre-computation of the solution. In this stage, we examine the trade-off between accuracy and the computational cost of the training. This trade-off is influenced by several factors; here, we consider 1) the used training dataset, 2) the type of regularisation, and 3) the optimisation algorithm.
\begin{table}[!th]
    \footnotesize
    \centering
    \renewcommand{\arraystretch}{1.1}
    \caption{Overview of the scenarios with different training datasets.}
    \begin{tabular}{ccccc}
    \toprule
        \thead{Scenario\\\quad} & \thead{Time\\increment $\Delta t$} & \thead{Power disturbance\\increment $\Delta P$} & \thead{Dataset\\size $|\mathcal{D}|$} & \thead{Dataset\\creation cost}\\\midrule
        A & \SI{2}{\second} &\SI{2}{\pu} & 66 & \SI{0.413}{\second}\\
        B & \SI{1}{\second} &\SI{2}{\pu} & 126 & \SI{0.412}{\second}\\
        C & \SI{2}{\second} &\SI{1}{\pu} & 121 & \SI{0.812}{\second}\\
        D & \SI{1}{\second} &\SI{1}{\pu} & 231 & \SI{0.814}{\second}\\
        E & \SI{0.2}{\second} &\SI{0.2}{\pu} & 5151 & \SI{3.880}{\second}\\
        \bottomrule
    \end{tabular}
    %}
    \label{tbl:dataset_scenarios}
\end{table}
To investigate the influence of the training dataset and the regularisation, we use the 11-bus system with a \gls{NN} of size $K=5$ and $N_K=32$. We consider five scenarios as shown in \cref{tbl:dataset_scenarios} with different numbers of data points $|\mathcal{D}|$ and the three \say{flavours} of \glspl{NN} which we introduced in \Cref{sec:methodology}: vanilla \gls{NN}, dtNN, \gls{PINN}. The datasets are created by sampling with different increments of time $\Delta t$ and the power disturbance $\Delta P$. As expected, more data points incur a higher dataset creation cost, however, it also depends what \say{kind} of additional data points we generate. When we halve the time increment $\Delta t$, e.g., from scenario A to B or from scenario C to D, the dataset generation cost remains approximately the same. However, this does not hold if we halve the power increment $\Delta P$. When simulating a certain trajectory, it is basically free to evaluate additional points, i.e., reduce $\Delta t$, since interpolation schemes can be used for intermediate points. In contrast, any additional trajectory that needs to be simulated adds to the total cost. Similarly for \say{free}, we can obtain the necessary values for the dtNN regularisation as this only requires the evaluation of the right hand side in \eqref{eq:dynamical_system}. The \gls{PINN} regularisation also incurs only negligible dataset generation cost, as it is a mere sampling of the collocation points $|\mathcal{D}_f|$, here 5151, without the need for any simulation. Therefore, the additional regularisation come at no or negligible cost compared to generating more data points unless they lie on trajectories that are evaluated anyways.

\Cref{fig:scenario_characteristics_accuracy} shows the resulting $\max \text{AE}_\delta$ across 20 training runs with different initialisations of the \gls{NN} parameters. Unsurprisingly, the error metric improves with more data points, i.e., from scenario A to E, and additional regularisation, i.e., from a vanilla \gls{NN} to a dtNN and a \gls{PINN}. In scenario E, which has the largest dataset, all three network types perform on a similar level, whereas \glspl{PINN} otherwise clearly deliver the best performance. Furthermore, the performance becomes more consistent, i.e., less variance, towards scenario E. A very sensitive issue is the point when to stop the training process to prevent over-fitting. In this study, we use the best validation loss as the indicator to determine the \say{best epoch} and \cref{fig:scenario_characteristics_epochs} shows the results. \glspl{PINN} consistently train for more epochs and only for scenario E the three NN types train for approximately the same number of epochs. 
\begin{figure}[!ht]
    \centering
    \subfloat[Accuracy on the test dataset \dataset{Test}]{\includegraphics{boxplots_delta_AE_error.pdf}\label{fig:scenario_characteristics_accuracy}}
    \subfloat[Epoch with lowest $\mathcal{L}_x(\dataset{Validation})$]{\includegraphics{boxplots_best_epoch.pdf}\label{fig:scenario_characteristics_epochs}}
    \caption{Training characteristic for different scenarios (for scenario definition, see \Cref{tbl:dataset_scenarios})}
    \label{fig:scenario_characteristics}
\end{figure}
In \cref{fig:validation_loss_epochs} we plot the validation loss over the training epoch. In scenario D, we can clearly see, that while the vanilla \glspl{NN} and the dtNNs do not improve much further after about 100 epochs, \glspl{PINN} still see a significant improvement in terms of accuracy. From around this point onward, the physics-based loss $\mathcal{L}_f$ drives the optimisations, the other training loss terms are already very small. This behaviour partly stems from the fade-in of $\mathcal{L}_f$ but also from the fact that \lossx{} and \lossdt{} are based on much smaller datasets except for scenario E, in which the improvement of the accuracy progresses at similar speeds for all three \gls{NN} types.
\begin{figure}[!bh]
    \centering
    \includegraphics{validation_loss_epochs.pdf}
    \caption{Validation loss as a function of trained epochs. The shadings signify the range from 20 randomly initialised runs.}
    \label{fig:validation_loss_epochs}
\end{figure}
\glspl{PINN} offer us therefore the ability to achieve accuracy improvements for more epochs but we can also terminate them early if the achieved accuracy is sufficient to reduce the computational burden. By multiplying the number of epochs with the computational cost per epoch, we can estimate the total computational cost of the training. The vanilla NN and dtNN required about \SI{0.17}{\second} and \SI{0.18}{\second} per epoch for scenarios A-D and \SI{0.40}{\second} and \SI{0.43}{\second} for scenario E while the \gls{PINN} constantly needs \SI{0.66}{\second} per epoch due to the collocation points. These numbers are very implementation and setup dependent, but show the trend that \glspl{PINN} have higher cost per epoch due to the computation of \lossf{} while the dtNN is only slightly more expensive than a vanilla NN. The total up-front cost, comprised of data generation cost and training cost, has then to be evaluated against the desired accuracy to find an efficient setup. This trade-off is again very dependent on the case study. For the 39-bus system the dataset generation cost is 2.5 times more while the cost per epoch only increases by a few percentage points.

\begin{figure}[!th]
    \centering
    \subfloat[Distribution of $\mathcal{L}_x(\dataset{Test})$ as a function of the prediction time $t$]{\includegraphics{error_characteristics_time.pdf} \label{fig:error_characteristics_time}}\\
    \subfloat[Distribution of $\mathcal{L}_x(\dataset{Test})$ as a function of the power disturbance size $P_7$]{\includegraphics{error_characteristics_power.pdf}}
    \caption{Distribution of $\mathcal{L}_x(\dataset{Test})$ for different \gls{NN} flavours (vanilla NN, dtNN, PINN) and scenarios (A,C,E). The shaded areas correspond to 100\%, 80\%, 50\% of the errors and the black line represents the median. (for the definition of scenarios A, C, E, see \Cref{tbl:dataset_scenarios})}
    \label{fig:error_characteristics}
\end{figure}

\Cref{fig:accuracy_max_errors,fig:scenario_characteristics_accuracy} displayed the maximum absolute errors on the test dataset as the accuracy metric, which is a critical metric of any solution approach. However, the accuracy of a \glspl{NN} must also be seen as a distribution across data points as it is visible in \cref{fig:runtime_accuracy}. We, therefore, show in \cref{fig:error_characteristics} the resulting distribution of the loss values as a function of the two input variables, i.e., the prediction time $t$ and the disturbance size $\Delta P_i$, for scenarios A, C, and E and the \gls{NN} types. The plots clearly show that for a majority of points in the test dataset, the predictions are much more accurate than the maximum values. This is true in particular around the data points. These are clearly visible in scenario A by the \say{indents}. The panel for the dtNN and scenario C in \cref{fig:error_characteristics_time} shows an extreme case where the prediction at the available data points is very accurate but the interpolation in between produces high errors. In comparison with the vanilla \gls{NN}, the additional regularisation of the dtNN leads to a more unbalanced error distribution. In contrast, the \gls{PINN} shows overall higher levels of accuracy but also more balanced error distributions thanks to the evaluation of the collocation points. We observe two more trends: Smaller prediction times are associated with higher errors due to the faster dynamics; and secondly, larger disturbances tend to show larger errors as they include larger variations of the output variables. These results show the importance of the dataset and the regularisation on the overall characteristics of a \gls{NN}-based predictor, which must be considered for assessing the trade-off between training time and accuracy.

We lastly touch upon the effect of the optimiser on the training process, in this case the \gls{LBFGS} algorithm. The hyper-parameters of the algorithm strongly influence the required training time but also the achieved accuracy which is shown in \cref{fig:lbfgs}. The points represent the outcome from  random hyper-parameter settings and they clearly show a strong relationship between training time and accuracy. Furthermore, the optimiser's internal tolerance setting (coloured) strongly influences this relationship.
\begin{figure}[!h]
    \centering
    \includegraphics{lbfgs_impact.pdf}
    \caption{Influence of hyper-parameters of the \gls{LBFGS}-optimiser on the trade-off between training time and achieved accuracy. The tolerance level of the optimiser has a large influence as shown by the coloured clusters of points.}
    \label{fig:lbfgs}
\end{figure}



