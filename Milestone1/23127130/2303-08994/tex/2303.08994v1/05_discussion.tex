The results in the previous Section illustrate how \gls{NN}-based approaches for solving \glspl{DAE} offer a number of advantages at run-time: 10 to 1'000 times faster evaluation speed, no issues of numerical instability, and, in contrast to conventional solvers, their solution time does not increase with a growing power system size. These properties come at the cost of training the \glspl{NN}. To assess an overall benefit in terms of computational time we, therefore, have to consider the total cost as the sum of up-front cost $C_{\text{up-front}}$ for the dataset generation and training and the run-time cost $C_{\text{run-time}}$ per evaluation $n$:
\begin{align}
    C_{\text{total}} = C_{\text{up-front}} + C_{\text{run-time}} \cdot n. \label{eq:total_cost}
\end{align}
\Cref{fig:total_cost} shows a graphical representation of \labelcref{eq:total_cost} for conventional solvers and \glspl{NN}. It is clear, that \gls{NN}-based approaches need to pass a critical number of evaluations $n_{\text{critical}}$ to be useful in terms of overall cost, unless other considerations like numerical stability or real-time applicability outweigh the cost consideration. The results in \cref{subsec:NN_run_time,subsec:NN_training} discussed the various \say{settings} that affect run and training time - in \cref{fig:total_cost} they would correspond to the dashed lines. For conventional solvers, changing these settings affects the slope, whereas for \glspl{NN} they mostly impact the y-intercept, i.e., $C_{\text{up-front}}$; in either case, as expected, a different \say{setting} will change $n_{\text{critical}}$. Hence, the decision for using \gls{NN}-based methods largely hinges around whether we expect sufficiently many evaluations $n$. Here, it is important to point out that the \gls{NN} will be trained for a specific problem setup and a change in the setup, e.g., another network configuration, requires a new training process. In this aspect of \say{flexibility}, conventional solvers have an important advantage over \gls{NN}-based approaches. 
\begin{figure}[!th]
    \centering
    \includegraphics{computation_cost.pdf}
    \caption{Total cost of different approaches in dependence of the number of evaluations.}
    \label{fig:total_cost}
\end{figure}

Addressing this lack of flexibility is of paramount importance for adopting \glspl{NN}-based simulation methods and we see three routes forward for this challenge: 1) Reducing the up-front cost $C_{\text{up-front}}$ by tailoring for example the learning algorithms, the used \gls{NN} architectures, and regularisation schemes to the applications; this can largely be seen in the context of actively controlling the trade-off between accuracy and training time. 2) Finding use cases with large $n$, i.e., highly repetitive tasks. 3) Designing hybrid setups -- similar to \gls{SAS}-based methods -- in which repetitive sub-problems are solved by \glspl{NN} and conventional solvers handle computations that require a lot of flexibility.
