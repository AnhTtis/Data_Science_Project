% This section briefly introduces the test cases (\cref{subsec:test_cases}) and the implementation of the NN training (\cref{subsec:NN_setup}).
This section introduces the test cases and the details of the NN training.

\subsection{Power system - Kundur 11-bus and IEEE 39-bus system}\label{subsec:test_cases}

As a study setup, we investigate the dynamic response of a power system to a load disturbance. We use a second order model to represent each of the generators in the system. The update equation \labelcref{eq:dynamical_system} formulates for generator buses as
\begin{align}
    \begin{bmatrix} 1 & 0 \\ 0 & 2 H_i \omega_0 \end{bmatrix} \frac{d}{dt} \begin{bmatrix} \delta_{i} \\ \Delta \omega_i \end{bmatrix} &= \begin{bmatrix}
    \Delta \omega_i \\
    P_{mech,i} - D_i\Delta \omega_i + P_{e,i}
    \end{bmatrix}
    \intertext{and for load buses as}
    \begin{bmatrix}d_i \omega_0\end{bmatrix} \frac{d}{dt} \begin{bmatrix}
    \delta_{i}
    \end{bmatrix} &= \begin{bmatrix}
    P_{mech,i} + P_{e,i}
    \end{bmatrix}
\end{align}
where $P_{mech,i} = P_{set, i} + P_{dist, i}$ at bus $i$, with $P_{set, i}$ representing the power setpoint and $P_{dist, i}$ the disturbance . The states $\bm{x}$ are the bus voltage angle $\delta_i$ and the frequency deviation $\Delta \omega_i$ for generator buses, and the bus voltage angle $\delta_i$ for the load buses. The buses are linked through the active power flows in the network defined by the admittance matrix $\Bar{\bm{Y}}_{bus}$ and the vector of complex voltages $\Bar{\bm{V}} = \bm{V}_{m} e^{j\bm{\delta}}$, where the vector $\bm{V}_m$ collects the voltage magnitudes and $\bm{\delta}$ the bus voltage angles:
\begin{align}
    \bm{P}_{e} &= \Re \left(\Bar{\bm{V}} (\Bar{\bm{Y}}_{bus} \Bar{\bm{V}})^*\right).
\end{align}
The $*$ indicates the complex conjugate and $P_{e,i}$ corresponds to the $i$-th entry of vector $\bm{P}_{e}$, i.e., the active power balance at bus $i$. 
In \cref{sec:results}, we demonstrate the methodology on the Kundur 2-area system (11 buses, 4 generators) and the IEEE 39-bus test system (39 buses, 10 generators). For both systems we are using the base power of $\SI{100}{MVA}$ and $\omega_0 = \SI{60}{\hertz}$. The network parameters and set-points stem from the case description of Kundur \cite[p.~813]{kundur_power_1994} and the IEEE 39-bus test case in Matpower \cite{zimmerman_matpower_2011}. The values for the inertia of the generators $H_i$ are [6.5, 6.5, 6.175, 6.175] \si{\pu} for the 11-bus case and [500.0, 30.3, 35.8, 38.6, 26.0, 34.8, 26.4, 24.3, 34.5, 42.0] \si{\pu} for the 39-bus case. The damping factor was set to $D_i = 0.05 \frac{\omega_0}{P_{set,i}}$ in both cases and for the loads to $d_i = 1.0 \frac{P_{set,i}}{\omega_0}$ and $d_i = 0.2 \frac{P_{set,i}}{\omega_0}$ respectively.

\subsection{NN training implementation}\label{subsec:NN_setup}

The entire workflow is implemented in Python 3.8 and available under \cite{stiasny_publicly_2022}. When we use the conventional numerical approaches to carry out the time-domain simulations for this system, the dynamical system is simulated using the Assimulo package \cite{andersson_assimulo_2015} which implements various solution methods for systems of \glspl{DAE}. The training process utilises PyTorch \cite{paszke_pytorch_2019} for the learning process and WandB \cite{biewald_experiment_2020} for monitoring and processing the workflow. The implementation builds on \cite{stiasny_closing_2022} for the steps of the workflow.

All datasets comprise the simulated response of the system over a period of \SI{20}{\second} to a disturbance. The tested disturbance is the step response to an instantaneous loss of load $|P_{dist,i}|$ at bus $i$ with a magnitude between \SI{0}{\pu} and \SI{10}{\pu}, where $i=7$ for the 11-bus system and $i=20$ for the 39-bus system. We record these data in increments of $\Delta t$ and $\Delta P$. The test dataset \dataset{test} which shall serve as a ground truth uses $\Delta t = \SI{0.05}{\second}$ and $\Delta P = \SI{0.05}{\pu}$, resulting in $|\dataset{test}| = 401 \times 201 = 80601$ points. This large size of \dataset{test} is chosen as it densely covers the input domain, hence, we obtain a reliable estimate of the maximum absolute error \labelcref{eq:max_abs_error}. For the training datasets \dataset{train} used in \cref{subsec:NN_training} we create datasets with $\Delta t \in [\num{0.2}, \num{1.0}, \num{2.0}]\si{\second}$ and $\Delta P \in [\num{0.2}, \num{1.0}, \num{2.0}]\si{\pu}$. The validation datasets \dataset{validation} for those scenarios are offset by $\frac{\Delta t}{2}$ and $\frac{\Delta P}{2}$.

For the scalings \scalelossx{} in \eqref{eq:loss_data}, we calculate the average standard deviation $\sigma$ across all voltage angle differences $\delta_{ij}$\footnote{The training process benefits from using the voltage angle difference $\delta_{ij} = \delta_i - \delta_j$, where $j$ indicates a reference bus, as the output of the \gls{NN}. The prediction becomes easier as the occurring drift in the dataset with respect to the variable $t$ is significantly reduced.} and all frequency deviations $\Delta \omega_i$, here the relevant groups of states $\mathcal{S}$:
\begin{align}
    \scalelossx{} &= \frac{1}{|\mathcal{S}|}\sum_{i \in \mathcal{S}} \sigma (x_i(\dataset{}))
\end{align}
Thereby, we aim for equal levels of error within all $\delta_{ij}$ and $\Delta \omega_i$ states and account for the difference in magnitude between them.  \scalelossdt{} and \scalelossf{} are all set to 1.0 to avoid adding further hyperparameters, more elaborate choices based on system analysis or the database are conceivable. During training and testing \scalelossx{} is based on \dataset{train} and \dataset{test} respectively.

The regularisation weights \weightlossdt{} and \weightlossf{} are hyperparameters. For the latter, we incorporate a fade-in dependent on the current epoch $E$ :
\begin{align}
    \weightlossf{}(E) = \min{\left(\weightlossfmax{}; \weightlossfzero{} \; 10^{E/E'}\right)},
\end{align}
where \weightlossfmax{} is the maximum and \weightlossfzero{} the initial regularisation weight and $E'$ determines the ``speed'' of the fade-in. The fade-in causes that \lossx{} and \lossdt{} are first minimised and then \lossf{} helps for ``fine-tuning'' and better generalisation. We apply the \gls{LBFGS} algorithm implemented in PyTorch in the training process, a standard optimiser for PINNs as \cite{cuomo_scientific_2022} reviews. The set of hyperparameters comprises $K$, $N_K$, \weightlossdt{}, \weightlossfmax{}, \weightlossfzero{} $E'$, and additional \gls{LBFGS} parameters. \Cref{tbl:hyperparameters_accuracy} reports the used hyperparameters for the scenarios (letters A-E) in \cref{subsec:NN_training}, \cref{subsec:NN_run_time} is based on scenario E for vanilla NNs.
\input{hyperparameter_table}

The hyperbolic tangent ($\tanh{}$) is selected as the activation function $\sigma$ as it is continuously differentiable. We initialise the \gls{NN} weights and biases with samples from the distribution described in \cite{glorot_understanding_2010} and achieve different initial values by altering the seed of the random number generator. All training and timing was performed on the \gls{HPC} cluster at the \gls{DTU} with nodes of 2xIntel Xeon Processor 2650v4 (12 core, 2.20GHz) and 256 GB memory of which we used 4 cores per training run.
