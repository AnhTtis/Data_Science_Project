@misc{nellikkath2023physics,
      title={Physics Informed Neural Networks for Phase Locked Loop Transient Stability Assessment}, 
      author={Rahul Nellikkath and Andreas Venzke and Mohammad Kazem Bakhshizadeh and Ilgiz Murzakhanov and Spyros Chatzivasileiadis},
      year={2023},
      eprint={2303.12116},
      archivePrefix={arXiv},
      primaryClass={eess.SY}
}


@misc{stiasny_solving_2023,
	title = {Solving {Differential}-{Algebraic} {Equations} in {Power} {Systems} {Dynamics} with {Neural} {Networks} and {Spatial} {Decomposition}},
	copyright = {Creative Commons Attribution 4.0 International},
	abstract = {The dynamics of the power system are described by a system of differential-algebraic equations. Time-domain simulations are used to understand the evolution of the system dynamics. These simulations can be computationally expensive due to the stiffness of the system which requires the use of finely discretized time-steps. By increasing the allowable time-step size, we aim to accelerate such simulations. In this paper, we use the observation that even though the individual components are described using both algebraic and differential equations, their coupling only involves algebraic equations. Following this observation, we use Neural Networks (NNs) to approximate the components' state evolution, leading to fast, accurate, and numerically stable approximators, which enable larger time-steps. To account for effects of the network on the components and vice-versa, the NNs take the temporal evolution of the coupling algebraic variables as an input for their prediction. We initially estimate this temporal evolution and then update it in an iterative fashion using the Newton-Raphson algorithm. The involved Jacobian matrix is calculated with Automatic Differentiation and its size depends only on the network size but not on the component dynamics. We demonstrate this NN-based simulator on the IEEE 9-bus test case with 3 generators.},
	author = {Stiasny, Jochen and Chatzivasileiadis, Spyros and Zhang, Baosen},
	year = {2023},
	note = {arXiv:2303.10256 [eess.SY]},
	keywords = {FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Mathematics, Machine Learning (cs.LG), Numerical Analysis (math.NA), Systems and Control (eess.SY)},
}

@article{huang_applications_2023,
	title = {Applications of {Physics}-{Informed} {Neural} {Networks} in {Power} {Systems} - {A} {Review}},
	volume = {38},
	issn = {0885-8950, 1558-0679},
	url = {https://ieeexplore.ieee.org/document/9743327/},
	doi = {10.1109/TPWRS.2022.3162473},
	number = {1},
	urldate = {2023-06-21},
	journal = {IEEE Transactions on Power Systems},
	author = {Huang, Bin and Wang, Jianhui},
	month = jan,
	year = {2023},
	keywords = {Data models, Deep learning, Mathematical models, Neural networks, Optimization, Physics, Training, Training data, first principle, neural networks, physics-informed neural networks, smart grids},
	pages = {572--588},
}

@article{linka_bayesian_2022,
	title = {Bayesian {Physics} {Informed} {Neural} {Networks} for real-world nonlinear dynamical systems},
	volume = {402},
	issn = {00457825},
	doi = {10.1016/j.cma.2022.115346},
	language = {en},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Linka, Kevin and Schäfer, Amelie and Meng, Xuhui and Zou, Zongren and Karniadakis, George Em and Kuhl, Ellen},
	month = dec,
	year = {2022},
	pages = {115346},
}

@inproceedings{li_machine-learning-based_2020,
	address = {Tempe, AZ, USA},
	title = {Machine-{Learning}-{Based} {Online} {Transient} {Analysis} via {Iterative} {Computation} of {Generator} {Dynamics}},
	isbn = {978-1-72816-127-3},
	doi = {10.1109/SmartGridComm47815.2020.9302975},
	abstract = {Transient analysis is vital to the planning and operation of electric power systems. Traditional transient analysis utilizes numerical methods to solve the differential-algebraic equations (DAEs) to compute the trajectories of quantities in the grid. For this, various numerical integration methods have been developed and used for decades. On the other hand, solving the DAEs for a relatively large system such as power grids is computationally intensive and is particularly challenging to perform online. In this paper, a novel machine learning (ML) based approach is proposed and developed to predict post-contingency trajectories of a generator in the time domain. The training data are generated by using an off-line simulation platform considering random disturbance occurrences and clearing times. As a proof-of-concept study, the proposed ML-based approach is applied to a single generator. A Long Short Term Memory (LSTM) network representation of the selected generator is successfully trained to capture the dependencies of its dynamics across a sufﬁciently long time span. In the online assessment stage, the LSTM network predicts the entire post-contingency transient trajectories given initial conditions of the power system triggered by system changes due to fault scenarios. Numerical experiments in the New York/New England 16-machine 86-bus power system show that the trained LSTM network accurately predicts the generator’s transient trajectories. Compared to existing numerical integration methods, the post-disturbance trajectories of generator’s dynamic states are computed much faster using the trained predictor, offering great promises for signiﬁcantly accelerating both ofﬂine and online transient studies.},
	language = {en},
	urldate = {2021-04-25},
	booktitle = {2020 {IEEE} {International} {Conference} on {Communications}, {Control}, and {Computing} {Technologies} for {Smart} {Grids} ({SmartGridComm})},
	publisher = {IEEE},
	author = {Li, Jiaming and Yue, Meng and Zhao, Yue and Lin, Guang},
	year = {2020},
	keywords = {Generators, Power system dynamics, Predictive models, Smart grids, Training, Trajectory, Transient analysis},
	pages = {1--6},
}

@article{aristidou_dynamic_2014,
	title = {Dynamic {Simulation} of {Large}-{Scale} {Power} {Systems} {Using} a {Parallel} {Schur}-{Complement}-{Based} {Decomposition} {Method}},
	volume = {25},
	issn = {1045-9219},
	url = {http://ieeexplore.ieee.org/document/6619387/},
	doi = {10.1109/TPDS.2013.252},
	number = {10},
	urldate = {2023-01-13},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Aristidou, Petros and Fabozzi, Davide and Van Cutsem, Thierry},
	month = oct,
	year = {2014},
	pages = {2561--2570},
}

@book{lin_mathematics_1988,
	title = {Mathematics {Applied} to {Deterministic} {Problems} in the {Natural} {Sciences}},
	isbn = {978-0-89871-229-2 978-1-61197-134-7},
	url = {http://epubs.siam.org/doi/book/10.1137/1.9781611971347},
	language = {en},
	urldate = {2023-06-21},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Lin, C. C. and Segel, L. A.},
	month = jan,
	year = {1988},
	doi = {10.1137/1.9781611971347},
}

@article{caruana_multitask_1997,
	title = {Multitask {Learning}},
	volume = {28},
	issn = {08856125},
	url = {http://link.springer.com/10.1023/A:1007379606734},
	doi = {10.1023/A:1007379606734},
	number = {1},
	urldate = {2023-06-22},
	journal = {Machine Learning},
	author = {Caruana, Rich},
	year = {1997},
	pages = {41--75},
}

@misc{noauthor_paris_2018,
	title = {The {Paris} {Agreement} - {Publication} {\textbar} {UNFCCC}},
	url = {https://unfccc.int/documents/184656},
	urldate = {2023-06-27},
	month = nov,
	year = {2018},
}

@misc{noauthor_world_2022,
	title = {World {Energy} {Transitions} {Outlook} 2022},
	url = {https://www.irena.org/Digital-Report/World-Energy-Transitions-Outlook-2022#page-0},
	abstract = {How hydrogen could impact geopolitics of energy transformation, disrupt global trade and bilateral energy relations},
	language = {en},
	urldate = {2023-06-27},
	year = {2022},
}

@misc{noauthor_pscad_nodate,
	title = {{PSCAD}},
	url = {https://www.pscad.com},
	urldate = {2023-06-30},
	publisher = {Manitoba Hydro International Ltd.},
}

@misc{noauthor_powerfactory_nodate,
	title = {{PowerFactory}},
	url = {https://www.digsilent.de/en/},
	urldate = {2023-06-30},
	publisher = {DIgSILENT},
}

@phdthesis{kidger_neural_2022,
	address = {Oxford, United Kingdom},
	title = {On {Neural} {Differential} {Equations}},
	abstract = {The conjoining of dynamical systems and deep learning has become a topic of great interest. In particular, neural differential equations (NDEs) demonstrate that neural networks and differential equation are two sides of the same coin. Traditional parameterised differential equations are a special case. Many popular neural network architectures, such as residual networks and recurrent networks, are discretisations. NDEs are suitable for tackling generative problems, dynamical systems, and time series (particularly in physics, finance, ...) and are thus of interest to both modern machine learning and traditional mathematical modelling. NDEs offer high-capacity function approximation, strong priors on model space, the ability to handle irregular data, memory efficiency, and a wealth of available theory on both sides. This doctoral thesis provides an in-depth survey of the field. Topics include: neural ordinary differential equations (e.g. for hybrid neural/mechanistic modelling of physical systems); neural controlled differential equations (e.g. for learning functions of irregular time series); and neural stochastic differential equations (e.g. to produce generative models capable of representing complex stochastic dynamics, or sampling from complex high-dimensional distributions). Further topics include: numerical methods for NDEs (e.g. reversible differential equations solvers, backpropagation through differential equations, Brownian reconstruction); symbolic regression for dynamical systems (e.g. via regularised evolution); and deep implicit models (e.g. deep equilibrium models, differentiable optimisation). We anticipate this thesis will be of interest to anyone interested in the marriage of deep learning with dynamical systems, and hope it will provide a useful reference for the current state of the art.},
	school = {University of Oxford},
	author = {Kidger, Patrick},
	year = {2022},
	note = {arXiv: 2202.02435 [cs.LG]},
	keywords = {Computer Science - Machine Learning, Mathematics - Classical Analysis and ODEs, Mathematics - Dynamical Systems, Mathematics - Numerical Analysis, Statistics - Machine Learning},
}

@article{raissi_physics-informed_2018,
	title = {Physics-informed neural networks: {A} deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
	volume = {378},
	issn = {0021-9991},
	shorttitle = {Physics-informed neural networks},
	doi = {10.1016/j.jcp.2018.10.045},
	abstract = {The U.S. Department of Energy's Office of Scientific and Technical Information},
	language = {English},
	number = {C},
	urldate = {2021-04-22},
	journal = {Journal of Computational Physics},
	author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
	month = nov,
	year = {2018},
}

@inproceedings{misyris_physics-informed_2020,
	address = {Montreal, QC, Canada},
	title = {Physics-{Informed} {Neural} {Networks} for {Power} {Systems}},
	isbn = {978-1-72815-508-1},
	doi = {10.1109/PESGM41954.2020.9282004},
	urldate = {2021-06-25},
	booktitle = {2020 {IEEE} {Power} \& {Energy} {Society} {General} {Meeting} ({PESGM})},
	publisher = {IEEE},
	author = {Misyris, George S. and Venzke, Andreas and Chatzivasileiadis, Spyros},
	year = {2020},
	pages = {1--5},
}

@article{cuomo_scientific_2022,
	title = {Scientific {Machine} {Learning} {Through} {Physics}–{Informed} {Neural} {Networks}: {Where} we are and {What}’s {Next}},
	volume = {92},
	issn = {1573-7691},
	shorttitle = {Scientific {Machine} {Learning} {Through} {Physics}–{Informed} {Neural} {Networks}},
	doi = {10.1007/s10915-022-01939-z},
	abstract = {Physics-Informed Neural Networks (PINN) are neural networks (NNs) that encode model equations, like Partial Differential Equations (PDE), as a component of the neural network itself. PINNs are nowadays used to solve PDEs, fractional equations, integral-differential equations, and stochastic PDEs. This novel methodology has arisen as a multi-task learning framework in which a NN must fit observed data while reducing a PDE residual. This article provides a comprehensive review of the literature on PINNs: while the primary goal of the study was to characterize these networks and their related advantages and disadvantages. The review also attempts to incorporate publications on a broader range of collocation-based physics informed neural networks, which stars form the vanilla PINN, as well as many other variants, such as physics-constrained neural networks (PCNN), variational hp-VPINN, and conservative PINN (CPINN). The study indicates that most research has focused on customizing the PINN through different activation functions, gradient optimization techniques, neural network structures, and loss function structures. Despite the wide range of applications for which PINNs have been used, by demonstrating their ability to be more feasible in some contexts than classical numerical techniques like Finite Element Method (FEM), advancements are still possible, most notably theoretical issues that remain unresolved.},
	language = {en},
	number = {3},
	urldate = {2023-02-16},
	journal = {Journal of Scientific Computing},
	author = {Cuomo, Salvatore and Di Cola, Vincenzo Schiano and Giampaolo, Fabio and Rozza, Gianluigi and Raissi, Maziar and Piccialli, Francesco},
	month = jul,
	year = {2022},
	keywords = {Deep Neural Networks, Nonlinear equations, Numerical methods, Partial Differential Equations, Physics–Informed Neural Networks, Scientific Machine Learning, Uncertainty},
	pages = {88},
}

@article{willard_integrating_2023,
	title = {Integrating {Scientific} {Knowledge} with {Machine} {Learning} for {Engineering} and {Environmental} {Systems}},
	volume = {55},
	issn = {0360-0300, 1557-7341},
	doi = {10.1145/3514228},
	abstract = {There is a growing consensus that solutions to complex science and engineering problems require novel methodologies that are able to integrate traditional physics-based modeling approaches with state-of-the-art machine learning (ML) techniques. This article provides a structured overview of such techniques. Application-centric objective areas for which these approaches have been applied are summarized, and then classes of methodologies used to construct physics-guided ML models and hybrid physics-ML frameworks are described. We then provide a taxonomy of these existing techniques, which uncovers knowledge gaps and potential crossovers of methods between disciplines that can serve as ideas for future research.},
	language = {en},
	number = {4},
	journal = {ACM Computing Surveys},
	author = {Willard, Jared and Jia, Xiaowei and Xu, Shaoming and Steinbach, Michael and Kumar, Vipin},
	month = apr,
	year = {2023},
	pages = {1--37},
}

@inproceedings{cranmer_lagrangian_2020,
	title = {Lagrangian {Neural} {Networks}},
	url = {https://openreview.net/forum?id=iE8tFa4Nq},
	abstract = {Accurate models of the world are built upon notions of its underlying symmetries. In physics, these symmetries correspond to conservation laws, such as for energy and momentum. Yet even though neural network models see increasing use in the physical sciences, they struggle to learn these symmetries. In this paper, we propose Lagrangian Neural Networks (LNNs), which can parameterize arbitrary Lagrangians using neural networks. In contrast to models that learn Hamiltonians, LNNs do not require canonical coordinates, and thus perform well in situations where canonical momenta are unknown or difficult to compute. Unlike previous approaches, our method does not restrict the functional form of learned energies and will produce energy-conserving models for a variety of tasks. We test our approach on a double pendulum and a relativistic particle, demonstrating energy conservation where a baseline approach incurs dissipation and modeling relativity without canonical coordinates where a Hamiltonian approach fails.},
	language = {en},
	author = {Cranmer, Miles and Greydanus, Sam and Hoyer, Stephan and Battaglia, Peter and Spergel, David and Ho, Shirley},
	year = {2020},
}

@article{rodriguez_einns_2023,
	title = {{EINNs}: {Epidemiologically}-{Informed} {Neural} {Networks}},
	volume = {37},
	issn = {2374-3468, 2159-5399},
	shorttitle = {{EINNs}},
	doi = {10.1609/aaai.v37i12.26690},
	abstract = {We introduce EINNs, a framework crafted for epidemic forecasting that builds upon the theoretical grounds provided by mechanistic models as well as the data-driven expressibility afforded by AI models, and their capabilities to ingest heterogeneous information. Although neural forecasting models have been successful in multiple tasks, predictions well-correlated with epidemic trends and long-term predictions remain open challenges. Epidemiological ODE models contain mechanisms that can guide us in these two tasks; however, they have limited capability of ingesting data sources and modeling composite signals. Thus, we propose to leverage work in physics-informed neural networks to learn latent epidemic dynamics and transfer relevant knowledge to another neural network which ingests multiple data sources and has more appropriate inductive bias. In contrast with previous work, we do not assume the observability of complete dynamics and do not need to numerically solve the ODE equations during training. Our thorough experiments on all US states and HHS regions for COVID-19 and influenza forecasting showcase the clear benefits of our approach in both short-term and long-term forecasting as well as in learning the mechanistic dynamics over other non-trivial alternatives.},
	number = {12},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Rodríguez, Alexander and Cui, Jiaming and Ramakrishnan, Naren and Adhikari, Bijaya and Prakash, B. Aditya},
	month = jun,
	year = {2023},
	pages = {14453--14460},
}

@inproceedings{majumdar_deepepisolver_2023,
	title = {{DeepEpiSolver}: {Unravelling} {Inverse} {Problems} in {Covid}, {HIV}, {Ebola} and {Disease} {Transmission}},
	shorttitle = {{DeepEpiSolver}},
	url = {https://openreview.net/forum?id=U5AiUCEDhy},
	abstract = {The spread of many infectious diseases is modeled using variants of the SIR compartmental model, which is a coupled differential equation. The coefficients of the SIR model determine the spread trajectories of disease, on whose basis proactive measures can be taken. Hence, the coefficient estimates must be both fast and accurate. Shaier et al. in {\textbackslash}cite\{DBLP:journals/corr/abs-2110-05445\} used Physics Informed Neural Networks (PINNs) to estimate the parameters of the SIR model. There are two drawbacks to this approach. First, the training time for PINNs is high, with certain diseases taking close to 90 hrs to train. Second, PINNs don't generalize for a new SIDR trajectory, and learning its corresponding SIR parameters requires retraining the PINN from scratch. In this work, we aim to eliminate both of these drawbacks. We generate a dataset between the parameters of ODE and the spread trajectories by solving the forward problem for a large distribution of parameters using the LSODA algorithm. We then use a neural network to learn the mapping between spread trajectories and coefficients of SIDR in an offline manner. This allows us to learn the parameters of a new spread trajectory without having to retrain, enabling generalization at test time. We observe a speed-up of 3-4 orders of magnitude with accuracy comparable to that of PINNs for 11 highly infectious diseases. Further finetuning of neural network inferred ODE coefficients using PINN further leads to 2-3 orders improvement of estimated coefficients.},
	language = {en},
	author = {Majumdar, Ritam and Karande, Shirish and Vig, Lovekesh},
	year = {2023},
}

@article{ji_stiff-pinn_2021,
	title = {Stiff-{PINN}: {Physics}-{Informed} {Neural} {Network} for {Stiff} {Chemical} {Kinetics}},
	volume = {125},
	issn = {1089-5639, 1520-5215},
	shorttitle = {Stiff-{PINN}},
	doi = {10.1021/acs.jpca.1c05102},
	abstract = {Recently developed physics-informed neural network (PINN) has achieved success in many science and engineering disciplines by encoding physics laws into the loss functions of the neural network, such that the network not only conforms to the measurements, initial and boundary conditions but also satisfies the governing equations. This work first investigates the performance of PINN in solving stiff chemical kinetic problems with governing equations of stiff ordinary differential equations (ODEs). The results elucidate the challenges of utilizing PINN in stiff ODE systems. Consequently, we employ Quasi-Steady-State-Assumptions (QSSA) to reduce the stiffness of the ODE systems, and the PINN then can be successfully applied to the converted non/mild-stiff systems. Therefore, the results suggest that stiffness could be the major reason for the failure of the regular PINN in the studied stiff chemical kinetic systems. The developed Stiff-PINN approach that utilizes QSSA to enable PINN to solve stiff chemical kinetics shall open the possibility of applying PINN to various reaction-diffusion systems involving stiff dynamics.},
	number = {36},
	journal = {The Journal of Physical Chemistry A},
	author = {Ji, Weiqi and Qiu, Weilun and Shi, Zhiyu and Pan, Shaowu and Deng, Sili},
	month = sep,
	year = {2021},
	keywords = {Mathematics - Numerical Analysis, Physics - Chemical Physics, Physics - Computational Physics},
	pages = {8098--8106},
}

@article{innes_flux_2018,
	title = {Flux: {Elegant} machine learning with {Julia}},
	volume = {3},
	issn = {2475-9066},
	shorttitle = {Flux},
	url = {http://joss.theoj.org/papers/10.21105/joss.00602},
	doi = {10.21105/joss.00602},
	number = {25},
	journal = {Journal of Open Source Software},
	author = {Innes, Mike},
	month = may,
	year = {2018},
	pages = {602},
}

@misc{innesFashionableModellingFlux2018,
	title = {Fashionable {Modelling} with {Flux}},
	abstract = {Machine learning as a discipline has seen an incredible surge of interest in recent years due in large part to a perfect storm of new theory, superior tooling, renewed interest in its capabilities. We present in this paper a framework named Flux that shows how further refinement of the core ideas of machine learning, built upon the foundation of the Julia programming language, can yield an environment that is simple, easily modifiable, and performant. We detail the fundamental principles of Flux as a framework for differentiable programming, give examples of models that are implemented within Flux to display many of the language and framework-level features that contribute to its ease of use and high productivity, display internal compiler techniques used to enable the acceleration and performance that lies at the heart of Flux, and finally give an overview of the larger ecosystem that Flux fits inside of.},
	publisher = {arXiv},
	author = {Innes, Michael and Saba, Elliot and Fischer, Keno and Gandhi, Dhairya and Rudilosso, Marco Concetto and Joy, Neethu Mariya and Karmali, Tejan and Pal, Avik and Shah, Viral},
	month = nov,
	year = {2018},
	note = {arXiv:1811.01457 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages},
}

@inproceedings{zhang_understanding_2017,
	title = {Understanding deep learning requires rethinking generalization},
	url = {https://openreview.net/forum?id=Sy8gdB9xx},
	abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
	language = {en},
	author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	year = {2017},
}

@misc{bronstein_geometric_2021,
	title = {Geometric {Deep} {Learning}: {Grids}, {Groups}, {Graphs}, {Geodesics}, and {Gauges}},
	shorttitle = {Geometric {Deep} {Learning}},
	abstract = {The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.},
	author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veličković, Petar},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.13478 [cs.LG]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computational Geometry, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{glorot_understanding_2010,
	title = {Understanding the difficulty of training deep feedforward neural networks},
	url = {https://proceedings.mlr.press/v9/glorot10a.html},
	abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
	language = {en},
	booktitle = {Proceedings of the {Thirteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {JMLR Workshop and Conference Proceedings},
	author = {Glorot, Xavier and Bengio, Yoshua},
	year = {2010},
	note = {ISSN: 1938-7228},
	pages = {249--256},
}

@inproceedings{keskar_large-batch_2017,
	title = {On {Large}-{Batch} {Training} for {Deep} {Learning}: {Generalization} {Gap} and {Sharp} {Minima}},
	shorttitle = {On {Large}-{Batch} {Training} for {Deep} {Learning}},
	url = {https://openreview.net/forum?id=H1oyRlYgg},
	abstract = {The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say \$32\$--\$512\$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions---and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
	language = {en},
	author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
	year = {2017},
}

@misc{wang_respecting_2022,
	title = {Respecting causality is all you need for training physics-informed neural networks},
	abstract = {While the popularity of physics-informed neural networks (PINNs) is steadily rising, to this date PINNs have not been successful in simulating dynamical systems whose solution exhibits multi-scale, chaotic or turbulent behavior. In this work we attribute this shortcoming to the inability of existing PINNs formulations to respect the spatio-temporal causal structure that is inherent to the evolution of physical systems. We argue that this is a fundamental limitation and a key source of error that can ultimately steer PINN models to converge towards erroneous solutions. We address this pathology by proposing a simple re-formulation of PINNs loss functions that can explicitly account for physical causality during model training. We demonstrate that this simple modification alone is enough to introduce significant accuracy improvements, as well as a practical quantitative mechanism for assessing the convergence of a PINNs model. We provide state-of-the-art numerical results across a series of benchmarks for which existing PINNs formulations fail, including the chaotic Lorenz system, the Kuramoto-Sivashinsky equation in the chaotic regime, and the Navier-Stokes equations in the turbulent regime. To the best of our knowledge, this is the first time that PINNs have been successful in simulating such systems, introducing new opportunities for their applicability to problems of industrial complexity.},
	author = {Wang, Sifan and Sankaran, Shyam and Perdikaris, Paris},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.07404 [cs.LG]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Nonlinear Sciences - Chaotic Dynamics, PINNs, Physics - Fluid Dynamics, Statistics - Machine Learning},
}

@misc{doumeche_convergence_2023,
	title = {Convergence and error analysis of {PINNs}},
	abstract = {Physics-informed neural networks (PINNs) are a promising approach that combines the power of neural networks with the interpretability of physical modeling. PINNs have shown good practical performance in solving partial differential equations (PDEs) and in hybrid modeling scenarios, where physical models enhance data-driven approaches. However, it is essential to establish their theoretical properties in order to fully understand their capabilities and limitations. In this study, we highlight that classical training of PINNs can suffer from systematic overfitting. This problem can be addressed by adding a ridge regularization to the empirical risk, which ensures that the resulting estimator is risk-consistent for both linear and nonlinear PDE systems. However, the strong convergence of PINNs to a solution satisfying the physical constraints requires a more involved analysis using tools from functional analysis and calculus of variations. In particular, for linear PDE systems, an implementable Sobolev-type regularization allows to reconstruct a solution that not only achieves statistical accuracy but also maintains consistency with the underlying physics.},
	author = {Doumèche, Nathan and Biau, Gérard and Boyer, Claire},
	month = may,
	year = {2023},
	note = {arXiv: 2305.01240 [math.ST]},
	keywords = {FOS: Mathematics, Statistics Theory (math.ST)},
}

@misc{hao_physics-informed_2023,
	title = {Physics-{Informed} {Machine} {Learning}: {A} {Survey} on {Problems}, {Methods} and {Applications}},
	shorttitle = {Physics-{Informed} {Machine} {Learning}},
	abstract = {Recent advances of data-driven machine learning have revolutionized ﬁelds like computer vision, reinforcement learning, and many scientiﬁc and engineering domains. In many real-world and scientiﬁc problems, systems that generate data are governed by physical laws. Recent work shows that it provides potential beneﬁts for machine learning models by incorporating the physical prior and collected data, which makes the intersection of machine learning and physics become a prevailing paradigm. By integrating the data and mathematical physics models seamlessly, it can guide the machine learning model towards solutions that are physically plausible, improving accuracy and efﬁciency even in uncertain and high-dimensional contexts. In this survey, we present this learning paradigm called Physics-Informed Machine Learning (PIML) which is to build a model that leverages empirical data and available physical prior knowledge to improve performance on a set of tasks that involve a physical mechanism. We systematically review the recent development of physics-informed machine learning from three perspectives of machine learning tasks, representation of physical prior, and methods for incorporating physical prior. We also propose several important open research problems based on the current trends in the ﬁeld. We argue that encoding different forms of physical prior into model architectures, optimizers, inference algorithms, and signiﬁcant domain-speciﬁc applications like inverse engineering design and robotic control is far from being fully explored in the ﬁeld of physics-informed machine learning. We believe that the interdisciplinary research of physics-informed machine learning will signiﬁcantly propel research progress, foster the creation of more effective machine learning models, and also offer invaluable assistance in addressing long-standing problems in related disciplines.},
	language = {en},
	urldate = {2023-04-26},
	publisher = {arXiv},
	author = {Hao, Zhongkai and Liu, Songming and Zhang, Yichi and Ying, Chengyang and Feng, Yao and Su, Hang and Zhu, Jun},
	month = mar,
	year = {2023},
	note = {arXiv:2211.08064 [cs, math]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Mathematics - Numerical Analysis},
}

@misc{faroughi_physics-guided_2023,
	title = {Physics-{Guided}, {Physics}-{Informed}, and {Physics}-{Encoded} {Neural} {Networks} in {Scientific} {Computing}},
	abstract = {Recent breakthroughs in computing power have made it feasible to use machine learning and deep learning to advance scientiﬁc computing in many ﬁelds, including ﬂuid mechanics, solid mechanics, materials science, etc. Neural networks, in particular, play a central role in this hybridization. Due to their intrinsic architecture, conventional neural networks cannot be successfully trained and scoped when data is sparse, which is the case in many scientiﬁc and engineering domains. Nonetheless, neural networks provide a solid foundation to respect physics-driven or knowledge-based constraints during training. Generally speaking, there are three distinct neural network frameworks to enforce the underlying physics: (i) physics-guided neural networks (PgNNs), (ii) physics-informed neural networks (PiNNs), and (iii) physics-encoded neural networks (PeNNs). These methods provide distinct advantages for accelerating the numerical modeling of complex multiscale multiphysics phenomena. In addition, the recent developments in neural operators (NOs) add another dimension to these new simulation paradigms, especially when the real-time prediction of complex multi-physics systems is required. All these models also come with their own unique drawbacks and limitations that call for further fundamental research. This study aims to present a review of the four neural network frameworks (i.e., PgNNs, PiNNs, PeNNs, and NOs) used in scientiﬁc computing research. The state-of-the-art architectures and their applications are reviewed, limitations are discussed, and future research opportunities in terms of improving algorithms, considering causalities, expanding applications, and coupling scientiﬁc and deep learning solvers are presented. This critical review provides researchers and engineers with a solid starting point to comprehend how to integrate diﬀerent layers of physics into neural networks.},
	language = {en},
	publisher = {arXiv},
	author = {Faroughi, Salah A. and Pawar, Nikhil and Fernandes, Celio and Raissi, Maziar and Das, Subasish and Kalantari, Nima K. and Mahjour, Seyed Kourosh},
	month = feb,
	year = {2023},
	note = {arXiv:2211.07377 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{chang_antisymmetricrnn_2018,
	title = {{AntisymmetricRNN}: {A} {Dynamical} {System} {View} on {Recurrent} {Neural} {Networks}},
	shorttitle = {{AntisymmetricRNN}},
	url = {https://openreview.net/forum?id=ryxepo0cFX},
	abstract = {Recurrent neural networks have gained widespread use in modeling sequential data. Learning long-term dependencies using these models remains difficult though, due to exploding or vanishing gradients. In this paper, we draw connections between recurrent networks and ordinary differential equations. A special form of recurrent networks called the AntisymmetricRNN is proposed under this theoretical framework, which is able to capture long-term dependencies thanks to the stability property of its underlying differential equation. Existing approaches to improving RNN trainability often incur significant computation overhead. In comparison, AntisymmetricRNN achieves the same goal by design. We showcase the advantage of this new architecture through extensive simulations and experiments. AntisymmetricRNN exhibits much more predictable dynamics. It outperforms regular LSTM models on tasks requiring long-term memory and matches the performance on tasks where short-term dependencies dominate despite being much simpler.},
	language = {en},
	author = {Chang, Bo and Chen, Minmin and Haber, Eldad and Chi, Ed H.},
	year = {2018},
}

@inproceedings{tallec_can_2018,
	title = {Can recurrent neural networks warp time?},
	url = {https://openreview.net/forum?id=72xO7jUzvCs},
	abstract = {Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use ad hoc gating mechanisms. Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues.},
	language = {en},
	author = {Tallec, Corentin and Ollivier, Yann},
	year = {2018},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{li_physics-informed_2023,
	title = {Physics-{Informed} {Neural} {Operator} for {Learning} {Partial} {Differential} {Equations}},
	abstract = {In this paper, we propose physics-informed neural operators (PINO) that uses available data and/or physics constraints to learn the solution operator of a family of parametric Partial Differential Equation (PDE). This hybrid approach allows PINO to overcome the limitations of purely data-driven and physics-based methods. For instance, data-driven methods fail to learn when data is of limited quantity and/or quality, and physics-based approaches fail to optimize on challenging PDE constraints. By combining both data and PDE constraints, PINO overcomes all these challenges. Additionally, a unique property that PINO enjoys over other hybrid learning methods is its ability to incorporate data and PDE constraints at different resolutions. This allows us to combine coarse-resolution data, which is inexpensive to obtain from numerical solvers, with higher resolution PDE constraints, and the resulting PINO has no degradation in accuracy even on high-resolution test instances. This discretization-invariance property in PINO is due to neural-operator framework which learns mappings between function spaces and allows evaluation at different resolutions without the need for re-training. Moreover, PINO succeeds in the purely physics setting, where no data is available, while other approaches such as the Physics-Informed Neural Network (PINN) fail due to optimization challenges, e.g. in multi-scale dynamic systems such as Kolmogorov ﬂows. This is because PINO learns the solution operator by optimizing PDE constraints on multiple instances while PINN optimizes PDE constraints of a single PDE instance. Further, in PINO, we incorporate the Fourier neural operator (FNO) architecture which achieves orders-of-magnitude speedup over numerical solvers and also allows us to compute explicit gradients on function spaces efﬁciently.},
	language = {en},
	publisher = {arXiv},
	author = {Li, Zongyi and Zheng, Hongkai and Kovachki, Nikola and Jin, David and Chen, Haoxuan and Liu, Burigede and Azizzadenesheli, Kamyar and Anandkumar, Anima},
	month = apr,
	year = {2023},
	note = {arXiv: 2111.03794 [cs.LG]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
}

@inproceedings{gunasekar_implicit_2018,
	title = {Implicit {Bias} of {Gradient} {Descent} on {Linear} {Convolutional} {Networks}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/0e98aeeb54acf612b9eb4e48a269814c-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nati},
	year = {2018},
}

@inproceedings{chen_neural_2018,
	title = {Neural {Ordinary} {Differential} {Equations}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf},
	abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
	year = {2018},
}

@article{kovachki_universal_2021,
	title = {On {Universal} {Approximation} and {Error} {Bounds} for {Fourier} {Neural} {Operators}},
	volume = {22},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v22/21-0806.html},
	abstract = {Fourier neural operators (FNOs) have recently been proposed as an effective framework for learning operators that map between infinite-dimensional spaces. We prove that FNOs are universal, in the sense that they can approximate any continuous operator to desired accuracy. Moreover, we suggest a mechanism by which FNOs can approximate operators associated with PDEs efficiently. Explicit error bounds are derived to show that the size of the FNO, approximating operators associated with a Darcy type elliptic PDE and with the incompressible Navier-Stokes equations of fluid dynamics, only increases sub (log)-linearly in terms of the reciprocal of the error. Thus, FNOs are shown to efficiently approximate operators arising in a large class of PDEs.},
	number = {290},
	journal = {Journal of Machine Learning Research},
	author = {Kovachki, Nikola and Lanthaler, Samuel and Mishra, Siddhartha},
	year = {2021},
	pages = {1--76},
}

@misc{krishnapriyan_learning_2022,
	title = {Learning continuous models for continuous physics},
	abstract = {Dynamical systems that evolve continuously over time are ubiquitous throughout science and engineering. Machine learning (ML) provides data-driven approaches to model and predict the dynamics of such systems. A core issue with this approach is that ML models are typically trained on discrete data, using ML methodologies that are not aware of underlying continuity properties, which results in models that often do not capture the underlying continuous dynamics of a system of interest. As a result, these ML models are of limited use for for many scientific and engineering applications. To address this challenge, we develop a convergence test based on numerical analysis theory. Our test verifies whether a model has learned a function that accurately approximates a system's underlying continuous dynamics. Models that fail this test fail to capture relevant dynamics, rendering them of limited utility for many scientific prediction tasks; while models that pass this test enable both better interpolation and better extrapolation in multiple ways. Our results illustrate how principled numerical analysis methods can be coupled with existing ML training/testing methodologies to validate models for science and engineering applications.},
	author = {Krishnapriyan, Aditi S. and Queiruga, Alejandro F. and Erichson, N. Benjamin and Mahoney, Michael W.},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.08494 [cs.LG]},
	keywords = {Computer Science - Machine Learning, PINNs},
}

@misc{phan_composable_2019,
	title = {Composable {Effects} for {Flexible} and {Accelerated} {Probabilistic} {Programming} in {NumPyro}},
	abstract = {NumPyro is a lightweight library that provides an alternate NumPy backend to the Pyro probabilistic programming language with the same modeling interface, language primitives and effect handling abstractions. Effect handlers allow Pyro's modeling API to be extended to NumPyro despite its being built atop a fundamentally different JAX-based functional backend. In this work, we demonstrate the power of composing Pyro's effect handlers with the program transformations that enable hardware acceleration, automatic differentiation, and vectorization in JAX. In particular, NumPyro provides an iterative formulation of the No-U-Turn Sampler (NUTS) that can be end-to-end JIT compiled, yielding an implementation that is much faster than existing alternatives in both the small and large dataset regimes.},
	publisher = {arXiv},
	author = {Phan, Du and Pradhan, Neeraj and Jankowiak, Martin},
	month = dec,
	year = {2019},
	note = {arXiv:1912.11554 [stat.ML]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Programming Languages, G.3, I.2.5, I.2.5, G.3, Statistics - Machine Learning},
}

@misc{perez_adaptive_2023,
	title = {Adaptive weighting of {Bayesian} physics informed neural networks for multitask and multiscale forward and inverse problems},
	copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
	doi = {10.48550/ARXIV.2302.12697},
	abstract = {In this paper, we present a novel methodology for automatic adaptive weighting of Bayesian Physics-Informed Neural Networks (BPINNs), and we demonstrate that this makes it possible to robustly address multi-objective and multi-scale problems. BPINNs are a popular framework for data assimilation, combining the constraints of Uncertainty Quantification (UQ) and Partial Differential Equation (PDE). The relative weights of the BPINN target distribution terms are directly related to the inherent uncertainty in the respective learning tasks. Yet, they are usually manually set a-priori, that can lead to pathological behavior, stability concerns, and to conflicts between tasks which are obstacles that have deterred the use of BPINNs for inverse problems with multi-scale dynamics. The present weighting strategy automatically tunes the weights by considering the multi-task nature of target posterior distribution. We show that this remedies the failure modes of BPINNs and provides efficient exploration of the optimal Pareto front. This leads to better convergence and stability of BPINN training while reducing sampling bias. The determined weights moreover carry information about task uncertainties, reflecting noise levels in the data and adequacy of the PDE model. We demonstrate this in numerical experiments in Sobolev training, and compare them to analytically \$ε\$-optimal baseline, and in a multi-scale Lokta-Volterra inverse problem. We eventually apply this framework to an inpainting task and an inverse problem, involving latent field recovery for incompressible flow in complex geometries.},
	author = {Perez, Sarah and Maddu, Suryanarayana and Sbalzarini, Ivo F. and Poncet, Philippe},
	month = feb,
	year = {2023},
	note = {arXiv: 2302.12697 [physics.comp-ph]},
	keywords = {Computational Physics (physics.comp-ph), Data Analysis, Statistics and Probability (physics.data-an), FOS: Physical sciences},
}

@misc{rackauckas_universal_2021,
	title = {Universal {Differential} {Equations} for {Scientific} {Machine} {Learning}},
	abstract = {In the context of science, the well-known adage "a picture is worth a thousand words" might well be "a model is worth a thousand datasets." In this manuscript we introduce the SciML software ecosystem as a tool for mixing the information of physical laws and scientific models with data-driven machine learning approaches. We describe a mathematical object, which we denote universal differential equations (UDEs), as the unifying framework connecting the ecosystem. We show how a wide variety of applications, from automatically discovering biological mechanisms to solving high-dimensional Hamilton-Jacobi-Bellman equations, can be phrased and efficiently handled through the UDE formalism and its tooling. We demonstrate the generality of the software tooling to handle stochasticity, delays, and implicit constraints. This funnels the wide variety of SciML applications into a core set of training mechanisms which are highly optimized, stabilized for stiff equations, and compatible with distributed parallelism and GPU accelerators.},
	author = {Rackauckas, Christopher and Ma, Yingbo and Martensen, Julius and Warner, Collin and Zubov, Kirill and Supekar, Rohit and Skinner, Dominic and Ramadhan, Ali and Edelman, Alan},
	month = nov,
	year = {2021},
	note = {arXiv: 2001.04385 [cs.LG]},
	keywords = {Computer Science - Machine Learning, Mathematics - Dynamical Systems, Quantitative Biology - Quantitative Methods, Statistics - Machine Learning},
}

@misc{cui_predicting_2021,
	title = {Predicting {Power} {System} {Dynamics} and {Transients}: {A} {Frequency} {Domain} {Approach}},
	shorttitle = {Predicting {Power} {System} {Dynamics} and {Transients}},
	abstract = {The dynamics of power grids are governed by a large number of nonlinear ordinary differential equations (ODEs). To safely operate the system, operators need to check that the states described by this set of ODEs stay within prescribed limits after various faults. Limited by the size and stiffness of the ODEs, current numerical integration techniques are often too slow to be useful in real-time or large-scale resource allocation problems. In addition, detailed system parameters are often not exactly known. Machine learning approaches have been proposed to reduce the computational efforts, but existing methods generally suffer from overfitting and failures to predict unstable behaviors. This paper proposes a novel framework for power system dynamic predictions by learning in the frequency domain. The intuition is that although the system behavior is complex in the time domain, there are relatively few dominate modes in the frequency domain. Therefore, we learn to predict by constructing neural networks with Fourier transform and filtering layers. System topology and fault information are encoded by taking a multi-dimensional Fourier transform, allowing us to leverage the fact that the trajectories are sparse both in time and spatial (across different buses) frequencies. We show that the proposed approach does not need detailed system parameters, speeds up prediction computations by orders of magnitude and is highly accurate for different fault types.},
	publisher = {arXiv},
	author = {Cui, Wenqi and Yang, Weiwei and Zhang, Baosen},
	month = nov,
	year = {2021},
	note = {arXiv: 2111.01103 [eess.SY]},
	keywords = {Electrical Engineering and Systems Science - Systems and Control},
}

@misc{moya_approximating_2023,
	title = {On {Approximating} the {Dynamic} {Response} of {Synchronous} {Generators} via {Operator} {Learning}: {A} {Step} {Towards} {Building} {Deep} {Operator}-based {Power} {Grid} {Simulators}},
	shorttitle = {On {Approximating} the {Dynamic} {Response} of {Synchronous} {Generators} via {Operator} {Learning}},
	abstract = {This paper designs an Operator Learning framework to approximate the dynamic response of synchronous generators. One can use such a framework to (i) design a neural-based generator model that can interact with a numerical simulator of the rest of the power grid or (ii) shadow the generator’s transient response. To this end, we design a data-driven Deep Operator Network (DeepONet) that approximates the generators’ inﬁnite-dimensional solution operator. Then, we develop a DeepONet-based numerical scheme to simulate a given generator’s dynamic response over a short/medium-term horizon. The proposed numerical scheme recursively employs the trained DeepONet to simulate the response for a given multi-dimensional input, which describes the interaction between the generator and the rest of the system. Furthermore, we develop a residual DeepONet numerical scheme that incorporates information from mathematical models of synchronous generators. We accompany this residual DeepONet scheme with an estimate for the prediction’s cumulative error. We also design a data aggregation (DAgger) strategy that allows (i) employing supervised learning to train the proposed DeepONets and (ii) ﬁne-tuning the DeepONet using aggregated training data that the DeepONet is likely to encounter during interactive simulations with other grid components. Finally, as a proof of concept, we demonstrate that the proposed DeepONet frameworks can eﬀectively approximate the transient model of a synchronous generator.},
	language = {en},
	publisher = {arXiv},
	author = {Moya, Christian and Lin, Guang and Zhao, Tianqiao and Yue, Meng},
	month = jan,
	year = {2023},
	note = {arXiv:2301.12538 [cs.LG]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Dynamical Systems},
}

@article{pagnier_toward_2022,
	title = {Toward {Model} {Reduction} for {Power} {System} {Transients} {With} {Physics}-{Informed} {PDE}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3183336},
	journal = {IEEE Access},
	author = {Pagnier, Laurent and Fritzsch, Julian and Jacquod, Philippe and Chertkov, Michael},
	year = {2022},
	pages = {65118--65125},
}

@misc{chen_torchdiffeq_2018,
	title = {torchdiffeq},
	url = {https://github.com/rtqichen/torchdiffeq},
	abstract = {Differentiable ODE solvers with full GPU support and O(1)-memory backpropagation. - torchdiffeq/torchdiffeq at master · rtqichen/torchdiffeq},
	author = {Chen, Ricky T. Q.},
	year = {2018},
}

@article{rackauckas_differentialequationsjl_2017,
	title = {{DifferentialEquations}.jl – {A} {Performant} and {Feature}-{Rich} {Ecosystem} for {Solving} {Differential} {Equations} in {Julia}},
	volume = {5},
	issn = {2049-9647},
	doi = {10.5334/jors.151},
	number = {1},
	journal = {Journal of Open Research Software},
	author = {Rackauckas, Christopher and Nie, Qing},
	month = may,
	year = {2017},
	pages = {15},
}

@misc{noauthor_formal_nodate,
	title = {Formal {Verification} of {Deep} {Neural} {Networks}: {Theory} and {Practice} - {Tutorial}},
	url = {https://neural-network-verification.com/},
	urldate = {2023-06-27},
}

@misc{noauthor_pytorch_nodate,
	title = {{PyTorch} {Lightning}},
	url = {https://www.pytorchlightning.ai},
	abstract = {The ultimate PyTorch research framework. Scale your models, without the boilerplate.},
	urldate = {2023-06-27},
	publisher = {Lightning AI et al.},
}

@inproceedings{de_ryck_generic_2022,
	title = {Generic bounds on the approximation error for physics-informed (and) operator learning},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/46f0114c06524debc60ef2a72769f7a9-Paper-Conference.pdf},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {De Ryck, Tim and Mishra, Siddhartha},
	editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
	year = {2022},
	pages = {10945--10958},
}

@book{strogatz_nonlinear_2015,
	address = {Boulder, CO},
	edition = {Second edition},
	title = {Nonlinear dynamics and chaos: with applications to physics, biology, chemistry, and engineering},
	isbn = {978-0-8133-4910-7},
	shorttitle = {Nonlinear dynamics and chaos},
	publisher = {Westview Press, a member of the Perseus Books Group},
	author = {Strogatz, Steven H.},
	year = {2015},
	note = {OCLC: ocn842877119},
	keywords = {Chaotic behavior in systems, Dynamics, Nonlinear theories},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	volume = {12},
	issn = {1533-7928},
	shorttitle = {Scikit-learn},
	url = {http://jmlr.org/papers/v12/pedregosa11a.html},
	abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
	number = {85},
	journal = {Journal of Machine Learning Research},
	author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Édouard},
	year = {2011},
	pages = {2825--2830},
}

@misc{noauthor_hugging_nodate,
	title = {Hugging {Face} – {The} {AI} community building the future.},
	url = {https://huggingface.co/},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
}

@misc{biewald_experiment_2020,
	title = {Experiment {Tracking} with {Weights} and {Biases}},
	url = {https://www.wandb.com/},
	abstract = {WandB is a central dashboard to keep track of your hyperparameters, system metrics, and predictions so you can compare models live, and share your findings.},
	author = {Biewald, Lukas},
	year = {2020},
	note = {Software available from wandb.com},
}

@article{paleyes_challenges_2023,
	title = {Challenges in {Deploying} {Machine} {Learning}: {A} {Survey} of {Case} {Studies}},
	volume = {55},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Challenges in {Deploying} {Machine} {Learning}},
	doi = {10.1145/3533378},
	abstract = {In recent years, machine learning has transitioned from a field of academic research interest to a field capable of solving real-world business problems. However, the deployment of machine learning models in production systems can present a number of issues and concerns. This survey reviews published reports of deploying machine learning solutions in a variety of use cases, industries, and applications and extracts practical considerations corresponding to stages of the machine learning deployment workflow. By mapping found challenges to the steps of the machine learning deployment workflow, we show that practitioners face issues at each stage of the deployment process. The goal of this article is to lay out a research agenda to explore approaches addressing these challenges.},
	language = {en},
	number = {6},
	journal = {ACM Computing Surveys},
	author = {Paleyes, Andrei and Urma, Raoul-Gabriel and Lawrence, Neil D.},
	month = jul,
	year = {2023},
	pages = {1--29},
}

@article{yu_veridical_2020,
	title = {Veridical data science},
	volume = {117},
	copyright = {Copyright © 2020 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
	issn = {0027-8424, 1091-6490},
	doi = {10.1073/pnas.1901326117},
	abstract = {Building and expanding on principles of statistics, machine learning, and scientific inquiry, we propose the predictability, computability, and stability (PCS) framework for veridical data science. Our framework, composed of both a workflow and documentation, aims to provide responsible, reliable, reproducible, and transparent results across the data science life cycle. The PCS workflow uses predictability as a reality check and considers the importance of computation in data collection/storage and algorithm design. It augments predictability and computability with an overarching stability principle. Stability expands on statistical uncertainty considerations to assess how human judgment calls impact data results through data and model/algorithm perturbations. As part of the PCS workflow, we develop PCS inference procedures, namely PCS perturbation intervals and PCS hypothesis testing, to investigate the stability of data results relative to problem formulation, data cleaning, modeling decisions, and interpretations. We illustrate PCS inference through neuroscience and genomics projects of our own and others. Moreover, we demonstrate its favorable performance over existing methods in terms of receiver operating characteristic (ROC) curves in high-dimensional, sparse linear model simulations, including a wide range of misspecified models. Finally, we propose PCS documentation based on R Markdown or Jupyter Notebook, with publicly available, reproducible codes and narratives to back up human choices made throughout an analysis. The PCS workflow and documentation are demonstrated in a genomics case study available on Zenodo.},
	language = {en},
	number = {8},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Yu, Bin and Kumbier, Karl},
	month = feb,
	year = {2020},
	pmid = {32054788},
	note = {Publisher: National Academy of Sciences
Section: Physical Sciences},
	keywords = {computation, data science, prediction, stability},
	pages = {3920--3929},
}

@article{silver_mastering_2016,
	title = {Mastering the game of {Go} with deep neural networks and tree search},
	volume = {529},
	issn = {0028-0836, 1476-4687},
	doi = {10.1038/nature16961},
	language = {en},
	number = {7587},
	journal = {Nature},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	month = jan,
	year = {2016},
	pages = {484--489},
}

@inproceedings{weckesser_investigation_2012,
	address = {Berlin, Germany},
	title = {Investigation of the adaptability of transient stability assessment methods to real-time operation},
	isbn = {978-1-4673-2597-4 978-1-4673-2595-0 978-1-4673-2596-7},
	doi = {10.1109/ISGTEurope.2012.6465835},
	booktitle = {2012 3rd {IEEE} {PES} {Innovative} {Smart} {Grid} {Technologies} {Europe} ({ISGT} {Europe})},
	publisher = {IEEE},
	author = {Weckesser, Tilman and Johannsson, Hjortur and Sommer, Stefan and Ostergaard, Jacob},
	month = oct,
	year = {2012},
	pages = {1--9},
}

@book{scott_algorithms_2023,
	address = {Cham},
	series = {Nečas {Center} {Series}},
	title = {Algorithms for {Sparse} {Linear} {Systems}},
	isbn = {978-3-031-25819-0 978-3-031-25820-6},
	language = {en},
	publisher = {Springer International Publishing},
	author = {Scott, Jennifer and Tůma, Miroslav},
	year = {2023},
	doi = {10.1007/978-3-031-25820-6},
}

@techreport{dembart_power_1977,
	title = {Power system dynamic analysis: {Phase} {I}. {Final} report},
	shorttitle = {Power system dynamic analysis},
	language = {en},
	number = {EPRI-EL-484, 7296152},
	author = {Dembart, B. and Erisman, A. M. and Cate, E. G. and Epton, M. A. and Dommel, H.},
	year = {1977},
	doi = {10.2172/7296152},
}

@article{baker_emulating_2022,
	title = {Emulating {AC} {OPF} {Solvers} {With} {Neural} {Networks}},
	volume = {37},
	issn = {0885-8950, 1558-0679},
	doi = {10.1109/TPWRS.2022.3195097},
	number = {6},
	journal = {IEEE Transactions on Power Systems},
	author = {Baker, Kyri},
	month = nov,
	year = {2022},
	pages = {4950--4953},
}

@article{donon_neural_2020,
	title = {Neural networks for power flow: {Graph} neural solver},
	volume = {189},
	issn = {0378-7796},
	shorttitle = {Neural networks for power flow},
	doi = {10.1016/j.epsr.2020.106547},
	abstract = {Recent trends in power systems and those envisioned for the next few decades push Transmission System Operators to develop probabilistic approaches to risk estimation. However, current methods to solve AC power flows are too slow to fully attain this objective. Thus we propose a novel artificial neural network architecture that achieves a more suitable balance between computational speed and accuracy in this context. Improving on our previous work on Graph Neural Solver for Power System [1], our architecture is based on Graph Neural Networks and allows for fast and parallel computations. It learns to perform a power flow computation by directly minimizing the violation of Kirchhoff’s law at each bus during training. Unlike previous approaches, our graph neural solver learns by itself and does not try to imitate the output of a Newton-Raphson solver. It is robust to variations of injections, power grid topology, and line characteristics. We experimentally demonstrate the viability of our approach on standard IEEE power grids (case9, case14, case30 and case118) both in terms of accuracy and computational time.},
	language = {en},
	journal = {Electric Power Systems Research},
	author = {Donon, Balthazar and Clément, Rémy and Donnot, Benjamin and Marot, Antoine and Guyon, Isabelle and Schoenauer, Marc},
	month = dec,
	year = {2020},
	keywords = {Artificial neural networks, Graph neural networks, Graph neural solver, Power flow, Solver},
	pages = {106547},
}

@book{anderson_power_2003,
	address = {Piscataway, N.J},
	edition = {2nd ed},
	series = {{IEEE} {Press} power engineering series},
	title = {Power system control and stability},
	isbn = {978-0-471-23862-1},
	language = {en},
	publisher = {IEEE Press ; Wiley-Interscience},
	author = {Anderson, P. M. and Fouad, A. A.},
	year = {2003},
	keywords = {Control, Electric power system stability, Electric power systems, Power system modelling},
}

@article{moya_deeponet-grid-uq_2023,
	title = {{DeepONet}-grid-{UQ}: {A} trustworthy deep operator framework for predicting the power grid’s post-fault trajectories},
	volume = {535},
	issn = {09252312},
	shorttitle = {{DeepONet}-grid-{UQ}},
	doi = {10.1016/j.neucom.2023.03.015},
	language = {en},
	journal = {Neurocomputing},
	author = {Moya, Christian and Zhang, Shiqi and Lin, Guang and Yue, Meng},
	month = may,
	year = {2023},
	pages = {166--182},
}

@article{adomian_review_1988,
	title = {A review of the decomposition method in applied mathematics},
	volume = {135},
	issn = {0022247X},
	doi = {10.1016/0022-247X(88)90170-9},
	language = {en},
	number = {2},
	journal = {Journal of Mathematical Analysis and Applications},
	author = {Adomian, G},
	month = nov,
	year = {1988},
	pages = {501--544},
}

@article{petzold_differentialalgebraic_1982,
	title = {Differential/{Algebraic} {Equations} are not {ODE}’ {\textless}span style="font-variant:small-caps;"{\textgreater}s{\textless}/span{\textgreater}},
	volume = {3},
	issn = {0196-5204, 2168-3417},
	shorttitle = {Differential/{Algebraic} {Equations} are not {ODE}’ {\textless}span style="font-variant},
	doi = {10.1137/0903023},
	language = {en},
	number = {3},
	journal = {SIAM Journal on Scientific and Statistical Computing},
	author = {Petzold, Linda},
	month = sep,
	year = {1982},
	pages = {367--384},
}

@article{gear_simultaneous_1971,
	title = {Simultaneous {Numerical} {Solution} of {Differential}-{Algebraic} {Equations}},
	volume = {18},
	issn = {0018-9324},
	doi = {10.1109/TCT.1971.1083221},
	number = {1},
	journal = {IEEE Transactions on Circuit Theory},
	author = {Gear, C.},
	year = {1971},
	pages = {89--95},
}

@article{dommel_digital_1969,
	title = {Digital {Computer} {Solution} of {Electromagnetic} {Transients} in {Single}-and {Multiphase} {Networks}},
	volume = {PAS-88},
	issn = {0018-9510},
	doi = {10.1109/TPAS.1969.292459},
	number = {4},
	journal = {IEEE Transactions on Power Apparatus and Systems},
	author = {Dommel, Hermann},
	month = apr,
	year = {1969},
	pages = {388--399},
}

@article{fasel_ensemble-sindy_2022,
	title = {Ensemble-{SINDy}: {Robust} sparse model discovery in the low-data, high-noise limit, with active learning and control},
	volume = {478},
	shorttitle = {Ensemble-{SINDy}},
	doi = {10.1098/rspa.2021.0904},
	abstract = {Sparse model identiﬁcation enables the discovery of nonlinear dynamical systems purely from data; however, this approach is sensitive to noise, especially in the low-data limit. In this work, we leverage the statistical approach of bootstrap aggregating (bagging) to robustify the sparse identiﬁcation of nonlinear dynamics (SINDy) algorithm. First, an ensemble of SINDy models is identiﬁed from subsets of limited and noisy data. The aggregate model statistics are then used to produce inclusion probabilities of the candidate functions, which enables uncertainty quantiﬁcation and probabilistic forecasts. We apply this ensemble-SINDy (E-SINDy) algorithm to several synthetic and real-world data sets and demonstrate substantial improvements to the accuracy and robustness of model discovery from extremely noisy and limited data. For example, E-SINDy uncovers partial differential equations models from data with more than twice as much measurement noise as has been previously reported. Similarly, ESINDy learns the Lotka Volterra dynamics from remarkably limited data of yearly lynx and hare pelts collected from 1900-1920. E-SINDy is computationally efﬁcient, with similar scaling as standard SINDy. Finally, we show that ensemble statistics from E-SINDy can be exploited for active learning and improved model predictive control.},
	language = {en},
	number = {2260},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Fasel, Urban and Kutz, J. Nathan and Brunton, Bingni W. and Brunton, Steven L.},
	month = apr,
	year = {2022},
	keywords = {Mathematics - Dynamical Systems, Mathematics - Numerical Analysis, Mathematics - Optimization and Control},
	pages = {20210904},
}

@article{brunton_discovering_2016,
	title = {Discovering governing equations from data by sparse identification of nonlinear dynamical systems},
	volume = {113},
	issn = {0027-8424, 1091-6490},
	doi = {10.1073/pnas.1517384113},
	abstract = {Significance
            Understanding dynamic constraints and balances in nature has facilitated rapid development of knowledge and enabled technology, including aircraft, combustion engines, satellites, and electrical power. This work develops a novel framework to discover governing equations underlying a dynamical system simply from data measurements, leveraging advances in sparsity techniques and machine learning. The resulting models are parsimonious, balancing model complexity with descriptive ability while avoiding overfitting. There are many critical data-driven problems, such as understanding cognition from neural recordings, inferring climate patterns, determining stability of financial markets, predicting and suppressing the spread of disease, and controlling turbulence for greener transportation and energy. With abundant data and elusive laws, data-driven discovery of dynamics will continue to play an important role in these efforts.
          , 
            Extracting governing equations from data is a central challenge in many diverse areas of science and engineering. Data are abundant whereas models often remain elusive, as in climate science, neuroscience, ecology, finance, and epidemiology, to name only a few examples. In this work, we combine sparsity-promoting techniques and machine learning with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data. This results in parsimonious models that balance accuracy with model complexity to avoid overfitting. We demonstrate the algorithm on a wide range of problems, from simple canonical systems, including linear and nonlinear oscillators and the chaotic Lorenz system, to the fluid vortex shedding behind an obstacle. The fluid example illustrates the ability of this method to discover the underlying dynamics of a system that took experts in the community nearly 30 years to resolve. We also show that this method generalizes to parameterized systems and systems that are time-varying or have external forcing.},
	language = {en},
	number = {15},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Brunton, Steven L. and Proctor, Joshua L. and Kutz, J. Nathan},
	month = apr,
	year = {2016},
	keywords = {NeurIPS 2021 Workshop: ML for Physics and Physics for ML},
	pages = {3932--3937},
}

@article{papamarkou_challenges_2022,
	title = {Challenges in {Markov} {Chain} {Monte} {Carlo} for {Bayesian} {Neural} {Networks}},
	volume = {37},
	issn = {0883-4237},
	doi = {10.1214/21-STS840},
	number = {3},
	journal = {Statistical Science},
	author = {Papamarkou, Theodore and Hinkle, Jacob and Young, M. Todd and Womble, David},
	month = aug,
	year = {2022},
}

@inproceedings{blundell_weight_2015,
	title = {Weight {Uncertainty} in {Neural} {Network}},
	url = {https://proceedings.mlr.press/v37/blundell15.html},
	abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
	language = {en},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	month = jun,
	year = {2015},
	note = {ISSN: 1938-7228},
	pages = {1613--1622},
}

@article{mackay_practical_1992,
	title = {A {Practical} {Bayesian} {Framework} for {Backpropagation} {Networks}},
	volume = {4},
	issn = {0899-7667, 1530-888X},
	doi = {10.1162/neco.1992.4.3.448},
	abstract = {A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible (1) objective comparisons between solutions using alternative network architectures, (2) objective stopping rules for network pruning or growing procedures, (3) objective choice of magnitude and type of weight decay terms or additive regularizers (for penalizing large weights, etc.), (4) a measure of the effective number of well-determined parameters in a model, (5) quantified estimates of the error bars on network parameters and on network output, and (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian "evidence" automatically embodies "Occam's razor," penalizing overflexible and overcomplex models. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well matched to a problem, a good correlation between generalization ability and the Bayesian evidence is obtained.},
	language = {en},
	number = {3},
	journal = {Neural Computation},
	author = {MacKay, David J. C.},
	month = may,
	year = {1992},
	pages = {448--472},
}

@article{mackay_probable_1995,
	title = {Probable networks and plausible predictions — a review of practical {Bayesian} methods for supervised neural networks},
	volume = {6},
	issn = {0954-898X, 1361-6536},
	doi = {10.1088/0954-898X_6_3_011},
	language = {en},
	number = {3},
	journal = {Network: Computation in Neural Systems},
	author = {Mackay, David J C},
	month = jan,
	year = {1995},
	pages = {469--505},
}

@article{hoffman_stochastic_2013,
	title = {Stochastic {Variational} {Inference}},
	volume = {14},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v14/hoffman13a.html},
	abstract = {We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.},
	number = {40},
	journal = {Journal of Machine Learning Research},
	author = {Hoffman, Matthew D. and Blei, David M. and Wang, Chong and Paisley, John},
	year = {2013},
	pages = {1303--1347},
}

@article{bingham_pyro_2019,
	title = {Pyro: {Deep} {Universal} {Probabilistic} {Programming}},
	volume = {20},
	issn = {1533-7928},
	shorttitle = {Pyro},
	url = {http://jmlr.org/papers/v20/18-403.html},
	abstract = {Pyro is a probabilistic programming language built on Python as a platform for developing advanced probabilistic models in AI research. To scale to large data sets and high-dimensional models, Pyro uses stochastic variational inference algorithms and probability distributions built on top of PyTorch, a modern GPU-accelerated deep learning framework. To accommodate complex or model-specific algorithmic behavior, Pyro leverages Poutine, a library of composable building blocks for modifying the behavior of probabilistic programs.},
	number = {28},
	journal = {Journal of Machine Learning Research},
	author = {Bingham, Eli and Chen, Jonathan P. and Jankowiak, Martin and Obermeyer, Fritz and Pradhan, Neeraj and Karaletsos, Theofanis and Singh, Rohit and Szerlip, Paul and Horsfall, Paul and Goodman, Noah D.},
	year = {2019},
	pages = {1--6},
}

@article{wang_long-time_2023,
	title = {Long-time integration of parametric evolution equations with physics-informed {DeepONets}},
	volume = {475},
	issn = {00219991},
	doi = {10.1016/j.jcp.2022.111855},
	language = {en},
	journal = {Journal of Computational Physics},
	author = {Wang, Sifan and Perdikaris, Paris},
	month = feb,
	year = {2023},
	pages = {111855},
}

@article{brunton_modern_2022,
	title = {Modern {Koopman} {Theory} for {Dynamical} {Systems}},
	volume = {64},
	issn = {0036-1445, 1095-7200},
	doi = {10.1137/21M1401243},
	language = {en},
	number = {2},
	journal = {SIAM Review},
	author = {Brunton, Steven L. and Budišić, Marko and Kaiser, Eurika and Kutz, J. Nathan},
	month = may,
	year = {2022},
	pages = {229--340},
}

@article{koopman_hamiltonian_1931,
	title = {Hamiltonian {Systems} and {Transformation} in {Hilbert} {Space}},
	volume = {17},
	issn = {0027-8424, 1091-6490},
	doi = {10.1073/pnas.17.5.315},
	language = {en},
	number = {5},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Koopman, B. O.},
	month = may,
	year = {1931},
	pages = {315--318},
}

@article{shin_convergence_2020,
	title = {On the convergence of physics informed neural networks for linear second-order elliptic and parabolic type {PDEs}},
	volume = {28},
	issn = {1815-2406, 1991-7120},
	doi = {10.4208/cicp.OA-2020-0193},
	abstract = {Physics informed neural networks (PINNs) are deep learning based techniques for solving partial differential equations (PDEs) encounted in computational science and engineering. Guided by data and physical laws, PINNs find a neural network that approximates the solution to a system of PDEs. Such a neural network is obtained by minimizing a loss function in which any prior knowledge of PDEs and data are encoded. Despite its remarkable empirical success in one, two or three dimensional problems, there is little theoretical justification for PINNs. As the number of data grows, PINNs generate a sequence of minimizers which correspond to a sequence of neural networks. We want to answer the question: Does the sequence of minimizers converge to the solution to the PDE? We consider two classes of PDEs: linear second-order elliptic and parabolic. By adapting the Schauder approach and the maximum principle, we show that the sequence of minimizers strongly converges to the PDE solution in \$C{\textasciicircum}0\$. Furthermore, we show that if each minimizer satisfies the initial/boundary conditions, the convergence mode becomes \$H{\textasciicircum}1\$. Computational examples are provided to illustrate our theoretical findings. To the best of our knowledge, this is the first theoretical work that shows the consistency of PINNs.},
	number = {5},
	journal = {Communications in Computational Physics},
	author = {Shin, Yeonjong and Darbon, Jerome and Karniadakis, George Em},
	month = jun,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	pages = {2042--2074},
}

@article{chen_approximations_1993,
	title = {Approximations of continuous functionals by neural networks with application to dynamic systems},
	volume = {4},
	issn = {10459227},
	doi = {10.1109/72.286886},
	number = {6},
	journal = {IEEE Transactions on Neural Networks},
	author = {Chen, T. and Chen, H.},
	month = nov,
	year = {1993},
	pages = {910--918},
}

@article{chen_universal_1995,
	title = {Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems},
	volume = {6},
	issn = {10459227},
	doi = {10.1109/72.392253},
	number = {4},
	journal = {IEEE Transactions on Neural Networks},
	author = {Chen, Tianping and Chen, Hong},
	month = jul,
	year = {1995},
	pages = {911--917},
}

@article{kovachki_neural_2023,
	title = {Neural {Operator}: {Learning} {Maps} {Between} {Function} {Spaces} {With} {Applications} to {PDEs}},
	volume = {24},
	issn = {1533-7928},
	shorttitle = {Neural {Operator}},
	url = {http://jmlr.org/papers/v24/21-1524.html},
	abstract = {The classical development of neural networks has primarily focused on learning mappings between finite dimensional Euclidean spaces or finite sets. We propose a generalization of neural networks to learn operators, termed neural operators, that map between infinite dimensional function spaces. We formulate the neural operator as a composition of linear integral operators and nonlinear activation functions. We prove a universal approximation theorem for our proposed neural operator, showing that it can approximate any given nonlinear continuous operator. The proposed neural operators are also discretization-invariant, i.e., they share the same model parameters among different discretization of the underlying function spaces. Furthermore, we introduce four classes of efficient parameterization, viz., graph neural operators, multi-pole graph neural operators, low-rank neural operators, and Fourier neural operators. An important application for neural operators is learning surrogate maps for the solution operators of partial differential equations (PDEs). We consider standard PDEs such as the Burgers, Darcy subsurface flow, and the Navier-Stokes equations, and show that the proposed neural operators have superior performance compared to existing machine learning based methodologies, while being several orders of magnitude faster than conventional PDE solvers.},
	number = {89},
	journal = {Journal of Machine Learning Research},
	author = {Kovachki, Nikola and Li, Zongyi and Liu, Burigede and Azizzadenesheli, Kamyar and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	year = {2023},
	pages = {1--97},
}

@inproceedings{li_fourier_2021,
	title = {Fourier {Neural} {Operator} for {Parametric} {Partial} {Differential} {Equations}},
	url = {https://openreview.net/forum?id=c8P9NQVtmnO},
	abstract = {The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and Navier-Stokes equation. The Fourier neural operator is the first ML-based method to successfully model turbulent flows with zero-shot super-resolution. It is up to three orders of magnitude faster compared to traditional PDE solvers. Additionally, it achieves superior accuracy compared to previous learning-based solvers under fixed resolution.},
	language = {en},
	author = {Li, Zongyi and Kovachki, Nikola Borislavov and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	year = {2021},
}

@article{lu_learning_2021,
	title = {Learning nonlinear operators via {DeepONet} based on the universal approximation theorem of operators},
	volume = {3},
	issn = {2522-5839},
	doi = {10.1038/s42256-021-00302-5},
	language = {en},
	number = {3},
	journal = {Nature Machine Intelligence},
	author = {Lu, Lu and Jin, Pengzhan and Pang, Guofei and Zhang, Zhongqiang and Karniadakis, George Em},
	month = mar,
	year = {2021},
	pages = {218--229},
}

@inproceedings{kingma_adam_2015,
	address = {San Diego},
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	booktitle = {{arXiv}:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	year = {2015},
	keywords = {Computer Science - Machine Learning},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667, 1530-888X},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	language = {en},
	number = {8},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	year = {1997},
	pages = {1735--1780},
}

@article{legaard_constructing_2023,
	title = {Constructing {Neural} {Network} {Based} {Models} for {Simulating} {Dynamical} {Systems}},
	volume = {55},
	issn = {0360-0300, 1557-7341},
	doi = {10.1145/3567591},
	abstract = {Dynamical systems see widespread use in natural sciences like physics, biology, and chemistry, as well as engineering disciplines such as circuit analysis, computational fluid dynamics, and control. For simple systems, the differential equations governing the dynamics can be derived by applying fundamental physical laws. However, for more complex systems, this approach becomes exceedingly difficult. Data-driven modeling is an alternative paradigm that seeks to learn an approximation of the dynamics of a system using observations of the true system. In recent years, there has been an increased interest in applying data-driven modeling techniques to solve a wide range of problems in physics and engineering. This article provides a survey of the different ways to construct models of dynamical systems using neural networks. In addition to the basic overview, we review the related literature and outline the most significant challenges from numerical simulations that this modeling paradigm must overcome. Based on the reviewed literature and identified challenges, we provide a discussion on promising research areas.},
	language = {en},
	number = {11},
	journal = {ACM Computing Surveys},
	author = {Legaard, Christian and Schranz, Thomas and Schweiger, Gerald and Drgoňa, Ján and Falay, Basak and Gomes, Cláudio and Iosifidis, Alexandros and Abkar, Mahdi and Larsen, Peter},
	year = {2023},
	pages = {1--34},
}

@article{wang_understanding_2021,
	title = {Understanding and {Mitigating} {Gradient} {Flow} {Pathologies} in {Physics}-{Informed} {Neural} {Networks}},
	volume = {43},
	issn = {1064-8275, 1095-7197},
	doi = {10.1137/20M1318043},
	language = {en},
	number = {5},
	journal = {SIAM Journal on Scientific Computing},
	author = {Wang, Sifan and Teng, Yujun and Perdikaris, Paris},
	month = jan,
	year = {2021},
	pages = {A3055--A3081},
}

@misc{bradbury_jax_2018,
	title = {{JAX}: composable transformations of {Python}+{NumPy} programs},
	copyright = {Apache-2.0},
	shorttitle = {{JAX}},
	url = {https://github.com/google/jax},
	abstract = {Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more},
	publisher = {Google},
	author = {Bradbury, James and Frostig, Roy and Hawkins, Peter and Johnson, Matthew James and Leary, Chris and Maclaurin, Dougal and Necula, George and Paszke, Adam and VanderPlas, Jake and Wanderman-Milne, Skye and Zhang, Qiao},
	year = {2018},
	keywords = {jax},
}

@inproceedings{paszke_pytorch_2019,
	title = {{PyTorch}: {An} imperative style, high-performance deep learning library},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and dAlché-Buc, F. and Fox, E. and Garnett, R.},
	year = {2019},
}

@techreport{baker_workshop_2019,
	title = {Workshop {Report} on {Basic} {Research} {Needs} for {Scientific} {Machine} {Learning}: {Core} {Technologies} for {Artificial} {Intelligence}},
	shorttitle = {Workshop {Report} on {Basic} {Research} {Needs} for {Scientific} {Machine} {Learning}},
	language = {en},
	number = {1478744},
	author = {Baker, Nathan and Alexander, Frank and Bremer, Timo and Hagberg, Aric and Kevrekidis, Yannis and Najm, Habib and Parashar, Manish and Patra, Abani and Sethian, James and Wild, Stefan and Willcox, Karen and Lee, Steven},
	month = feb,
	year = {2019},
	doi = {10.2172/1478744},
}

@inproceedings{nakkiran_deep_2020,
	title = {Deep {Double} {Descent}: {Where} {Bigger} {Models} and {More} {Data} {Hurt}},
	shorttitle = {Deep {Double} {Descent}},
	url = {https://openreview.net/forum?id=B1g5sA4twr},
	abstract = {We demonstrate, and characterize, realistic settings where bigger models are worse, and more data hurts.},
	language = {en},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	year = {2020},
}

@article{abadi_tensorflow_2016,
	title = {{TensorFlow}: {Large}-{Scale} {Machine} {Learning} on {Heterogeneous} {Distributed} {Systems}},
	shorttitle = {{TensorFlow}},
	abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
	journal = {arXiv:1603.04467 [cs]},
	author = {Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.04467},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Machine learning},
}

@incollection{guckenheimer_numerical_2002,
	title = {Numerical {Analysis} of {Dynamical} {Systems}},
	volume = {2},
	isbn = {978-0-444-50168-4},
	language = {en},
	booktitle = {Handbook of {Dynamical} {Systems}},
	publisher = {Elsevier},
	author = {Guckenheimer, John},
	year = {2002},
	doi = {10.1016/S1874-575X(02)80029-7},
	keywords = {ODE Theory},
	pages = {345--390},
}

@misc{kolter_10-414714_2022,
	address = {CMU},
	title = {10-414/714: {Deep} {Learning} {Systems}},
	url = {https://dlsyscourse.org/lectures/},
	abstract = {10-414/714: Deep Learning Systems},
	language = {en},
	urldate = {2023-05-20},
	author = {Kolter, J. Zico and Chen, Tianqi},
	year = {2022},
	note = {Lectrure notes},
}

@article{weng_multiscale_2022,
	title = {Multiscale {Physics}-{Informed} {Neural} {Networks} for {Stiff} {Chemical} {Kinetics}},
	volume = {126},
	issn = {1089-5639, 1520-5215},
	doi = {10.1021/acs.jpca.2c06513},
	language = {en},
	number = {45},
	journal = {The Journal of Physical Chemistry A},
	author = {Weng, Yuting and Zhou, Dezhi},
	month = nov,
	year = {2022},
	pages = {8534--8543},
}

@misc{antonelo_physics-informed_2022,
	title = {Physics-{Informed} {Neural} {Nets} for {Control} of {Dynamical} {Systems}},
	abstract = {Physics-informed neural networks (PINNs) impose known physical laws into the learning of deep neural networks, making sure they respect the physics of the process while decreasing the demand of labeled data. For systems represented by Ordinary Differential Equations (ODEs), the conventional PINN has a continuous time input variable and outputs the solution of the corresponding ODE. In their original form, PINNs do not allow control inputs, neither can they simulate for variable long-range intervals without serious degradation in their predictions. In this context, this work presents a new framework called Physics-Informed Neural Nets for Control (PINC), which proposes a novel PINN-based architecture that is amenable to control problems and able to simulate for longer-range time horizons that are not fixed beforehand, making it a very flexible framework when compared to traditional PINNs. Furthermore, this long-range time simulation of differential equations is faster than numerical methods since it relies only on signal propagation through the network, making it less computationally costly and, thus, a better alternative for simulation of models in Model Predictive Control. We showcase our proposal in the control of two nonlinear dynamic systems: the Van der Pol oscillator and the four-tank system.},
	publisher = {arXiv},
	author = {Antonelo, Eric Aislan and Camponogara, Eduardo and Seman, Laio Oriel and de Souza, Eduardo Rehbein and Jordanou, Jean P. and Hubner, Jomi F.},
	month = may,
	year = {2022},
	note = {arXiv:2104.02556 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{greydanus_hamiltonian_2019,
	title = {Hamiltonian neural networks},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/26cd8ecadce0d4efd6cc8a8725cbd1f8-Paper.pdf},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Greydanus, Samuel and Dzamba, Misko and Yosinski, Jason},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and dAlché-Buc, F. and Fox, E. and Garnett, R.},
	year = {2019},
}

@book{coddington_theory_1955,
	title = {Theory of ordinary differential equations},
	language = {eng},
	publisher = {New York, McGraw-Hill},
	author = {Coddington, Earl A. and Levinson, Norman},
	year = {1955},
	keywords = {Differential equations},
}

@book{iserles_first_2008,
	edition = {2},
	title = {A {First} {Course} in the {Numerical} {Analysis} of {Differential} {Equations}},
	isbn = {978-0-521-73490-5},
	abstract = {Numerical analysis presents different faces to the world. For mathematicians it is a bona fide mathematical theory with an applicable flavour. For scientists and engineers it is a practical, applied subject, part of the standard repertoire of modelling techniques. For computer scientists it is a theory on the interplay of computer architecture and algorithms for real-number calculations. The tension between these standpoints is the driving force of this book, which presents a rigorous account of the fundamentals of numerical analysis of both ordinary and partial differential equations. The exposition maintains a balance between theoretical, algorithmic and applied aspects. This new edition has been extensively updated, and includes new chapters on emerging subject areas: geometric numerical integration, spectral methods and conjugate gradients. Other topics covered include multistep and Runge-Kutta methods; finite difference and finite elements techniques for the Poisson equation; and a variety of algorithms to solve large, sparse algebraic systems.},
	publisher = {Cambridge University Press},
	author = {Iserles, Arieh},
	month = nov,
	year = {2008},
	doi = {10.1017/CBO9780511995569},
}

@book{pai_energy_1989,
	address = {Boston, MA},
	title = {Energy {Function} {Analysis} for {Power} {System} {Stability}},
	isbn = {978-1-4613-1635-0},
	language = {English},
	publisher = {Springer US},
	author = {Pai, M. A},
	year = {1989},
	note = {OCLC: 852790803},
}

@article{scarselli_graph_2009,
	title = {The {Graph} {Neural} {Network} {Model}},
	volume = {20},
	issn = {1045-9227, 1941-0093},
	doi = {10.1109/TNN.2008.2005605},
	number = {1},
	journal = {IEEE Transactions on Neural Networks},
	author = {Scarselli, F. and Gori, M. and {Ah Chung Tsoi} and Hagenbuchner, M. and Monfardini, G.},
	month = jan,
	year = {2009},
	pages = {61--80},
}

@incollection{rumelhart_learning_1988,
	title = {Learning {Internal} {Representations} by {Error} {Propagation}},
	isbn = {978-1-4832-1446-7},
	language = {en},
	booktitle = {Readings in {Cognitive} {Science}},
	publisher = {Elsevier},
	author = {Rumelhart, D.E. and Hinton, G.E. and Williams, R.J.},
	year = {1988},
	doi = {10.1016/B978-1-4832-1446-7.50035-2},
	pages = {399--421},
}

@article{lecun_backpropagation_1989,
	title = {Backpropagation {Applied} to {Handwritten} {Zip} {Code} {Recognition}},
	volume = {1},
	issn = {0899-7667, 1530-888X},
	doi = {10.1162/neco.1989.1.4.541},
	abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
	language = {en},
	number = {4},
	journal = {Neural Computation},
	author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
	month = dec,
	year = {1989},
	pages = {541--551},
}

@article{hornik_multilayer_1989,
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	issn = {08936080},
	doi = {10.1016/0893-6080(89)90020-8},
	language = {en},
	number = {5},
	journal = {Neural Networks},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	month = jan,
	year = {1989},
	pages = {359--366},
}

@article{stuart_numerical_1994,
	title = {Numerical analysis of dynamical systems},
	volume = {3},
	issn = {0962-4929, 1474-0508},
	doi = {10.1017/S0962492900002488},
	abstract = {This article reviews the application of various notions from the theory of dynamical systems to the analysis of numerical approximation of initial value problems over long-time intervals. Standard error estimates comparing individual trajectories are of no direct use in this context since the error constant typically grows like the exponential of the time interval under consideration.
            Instead of comparing trajectories, the effect of discretization on various sets which are invariant under the evolution of the underlying differential equation is studied. Such invariant sets are crucial in determining long-time dynamics. The particular invariant sets which are studied are equilibrium points, together with their unstable manifolds and local phase portraits, periodic solutions, quasi-periodic solutions and strange attractors.
            Particular attention is paid to the development of a unified theory and to the development of an existence theory for invariant sets of the underlying differential equation which may be used directly to construct an analogous existence theory (and hence a simple approximation theory) for the numerical method.},
	language = {en},
	journal = {Acta Numerica},
	author = {Stuart, Andrew M.},
	month = jan,
	year = {1994},
	pages = {467--572},
}

@book{stuart_dynamical_1998,
	address = {Cambridge, United Kingdom},
	edition = {1. paperback ed},
	series = {Cambridge monographs on applied and computational mathematics},
	title = {Dynamical systems and numerical analysis},
	isbn = {978-0-521-64563-8},
	language = {eng},
	number = {2},
	publisher = {Cambridge University Press},
	author = {Stuart, Andrew M. and Humphries, A. R.},
	year = {1998},
	keywords = {ODE Theory},
}

@book{guckenheimer_nonlinear_1983,
	address = {New York, NY},
	series = {Applied {Mathematical} {Sciences}},
	title = {Nonlinear {Oscillations}, {Dynamical} {Systems}, and {Bifurcations} of {Vector} {Fields}},
	volume = {42},
	isbn = {978-1-4612-7020-1},
	publisher = {Springer New York},
	author = {Guckenheimer, John and Holmes, Philip},
	year = {1983},
	doi = {10.1007/978-1-4612-1140-2},
	keywords = {ODE Theory},
}

@article{yang_b-pinns_2021,
	title = {B-{PINNs}: {Bayesian} physics-informed neural networks for forward and inverse {PDE} problems with noisy data},
	volume = {425},
	issn = {00219991},
	shorttitle = {B-{PINNs}},
	doi = {10.1016/j.jcp.2020.109913},
	language = {en},
	journal = {Journal of Computational Physics},
	author = {Yang, Liu and Meng, Xuhui and Karniadakis, George Em},
	month = jan,
	year = {2021},
	pages = {109913},
}

@article{gusmao_kinetics-informed_2023,
	title = {Kinetics-informed neural networks},
	volume = {417},
	issn = {09205861},
	doi = {10.1016/j.cattod.2022.04.002},
	language = {en},
	journal = {Catalysis Today},
	author = {Gusmão, Gabriel S. and Retnanto, Adhika P. and Cunha, Shashwati C. Da and Medford, Andrew J.},
	month = may,
	year = {2023},
	pages = {113701},
}

@article{de_florio_physics-informed_2022,
	title = {Physics-informed neural networks and functional interpolation for stiff chemical kinetics},
	volume = {32},
	issn = {1054-1500, 1089-7682},
	doi = {10.1063/5.0086649},
	abstract = {This work presents a recently developed approach based on physics-informed neural networks (PINNs) for the solution of initial value problems (IVPs), focusing on stiff chemical kinetic problems with governing equations of stiff ordinary differential equations (ODEs). The framework developed by the authors combines PINNs with the theory of functional connections and extreme learning machines in the so-called extreme theory of functional connections (X-TFC). While regular PINN methodologies appear to fail in solving stiff systems of ODEs easily, we show how our method, with a single-layer neural network (NN) is efficient and robust to solve such challenging problems without using artifacts to reduce the stiffness of problems. The accuracy of X-TFC is tested against several state-of-the-art methods, showing its performance both in terms of computational time and accuracy. A rigorous upper bound on the generalization error of X-TFC frameworks in learning the solutions of IVPs for ODEs is provided here for the first time. A significant advantage of this framework is its flexibility to adapt to various problems with minimal changes in coding. Also, once the NN is trained, it gives us an analytical representation of the solution at any desired instant in time outside the initial discretization. Learning stiff ODEs opens up possibilities of using X-TFC in applications with large time ranges, such as chemical dynamics in energy conversion, nuclear dynamics systems, life sciences, and environmental engineering.},
	language = {en},
	number = {6},
	journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	author = {De Florio, Mario and Schiassi, Enrico and Furfaro, Roberto},
	month = jun,
	year = {2022},
	pages = {063107},
}

@article{roehrl_modeling_2020,
	title = {Modeling {System} {Dynamics} with {Physics}-{Informed} {Neural} {Networks} {Based} on {Lagrangian} {Mechanics}},
	volume = {53},
	issn = {24058963},
	doi = {10.1016/j.ifacol.2020.12.2182},
	language = {en},
	number = {2},
	journal = {IFAC-PapersOnLine},
	author = {Roehrl, Manuel A. and Runkler, Thomas A. and Brandtstetter, Veronika and Tokic, Michel and Obermayer, Stefan},
	year = {2020},
	pages = {9195--9200},
}

@article{berkhahn_physics-informed_2022,
	title = {A physics-informed neural network to model {COVID}-19 infection and hospitalization scenarios},
	volume = {2022},
	issn = {2731-4235},
	doi = {10.1186/s13662-022-03733-5},
	abstract = {Abstract
            In this paper, we replace the standard numerical approach of estimating parameters in a mathematical model using numerical solvers for differential equations with a physics-informed neural network (PINN). This neural network requires a sequence of time instances as direct input of the network and the numbers of susceptibles, vaccinated, infected, hospitalized, and recovered individuals per time instance to learn certain parameters of the underlying model, which are used for the loss calculations.
            The established model is an extended susceptible-infected-recovered (SIR) model in which the transitions between disease-related population groups, called compartments, and the physical laws of epidemic transmission dynamics are expressed by a system of ordinary differential equations (ODEs). The system of ODEs and its time derivative are included in the residual loss function of the PINN in addition to the data error between the current network output and the time series data of the compartment sizes. Further, we illustrate how this PINN approach can also be used for differential equation-based models such as the proposed extended SIR model, called SVIHR model.
            In a validation process, we compare the performance of the PINN with results obtained with the numerical technique of non-standard finite differences (NSFD) in generating future COVID-19 scenarios based on the parameters identified by the PINN. The used training data set covers the time between the outbreak of the pandemic in Germany and the last week of the year 2021.
            We obtain a two-step or hybrid approach, as the PINN is then used to generate a future COVID-19 outbreak scenario describing a possibly next pandemic wave. The week at which the prediction starts is chosen in mid-April 2022.},
	language = {en},
	number = {1},
	journal = {Advances in Continuous and Discrete Models},
	author = {Berkhahn, Sarah and Ehrhardt, Matthias},
	month = oct,
	year = {2022},
	pages = {61},
}

@article{long_identification_2021,
	title = {Identification and prediction of time-varying parameters of {COVID}-19 model: a data-driven deep learning approach},
	volume = {98},
	issn = {0020-7160, 1029-0265},
	shorttitle = {Identification and prediction of time-varying parameters of {COVID}-19 model},
	doi = {10.1080/00207160.2021.1929942},
	language = {en},
	number = {8},
	journal = {International Journal of Computer Mathematics},
	author = {Long, Jie and Khaliq, A. Q. M. and Furati, K. M.},
	month = aug,
	year = {2021},
	pages = {1617--1632},
}

@article{grimm_estimating_2021,
	title = {Estimating the time-dependent contact rate of {SIR} and {SEIR} models in mathematical epidemiology using physics-informed neural networks},
	volume = {56},
	doi = {10.1553/etna_vol56s1},
	language = {en},
	journal = {ETNA - Electronic Transactions on Numerical Analysis},
	author = {Grimm, Viktor and Heinlein, Alexander and Klawonn, Axel and Lanser, Martin and Weber, Janine},
	year = {2021},
	pages = {1--27},
}

@article{duchesne_recent_2020,
	title = {Recent {Developments} in {Machine} {Learning} for {Energy} {Systems} {Reliability} {Management}},
	volume = {108},
	issn = {0018-9219, 1558-2256},
	doi = {10.1109/JPROC.2020.2988715},
	number = {9},
	journal = {Proceedings of the IEEE},
	author = {Duchesne, Laurine and Karangelos, Efthymios and Wehenkel, Louis},
	month = sep,
	year = {2020},
	pages = {1656--1676},
}

@article{konstantelos_implementation_2017,
	title = {Implementation of a {Massively} {Parallel} {Dynamic} {Security} {Assessment} {Platform} for {Large}-{Scale} {Grids}},
	volume = {8},
	issn = {1949-3053, 1949-3061},
	doi = {10.1109/TSG.2016.2606888},
	number = {3},
	journal = {IEEE Transactions on Smart Grid},
	author = {Konstantelos, Ioannis and Jamgotchian, Geoffroy and Tindemans, Simon H. and Duchesne, Philippe and Cole, Stijn and Merckx, Christian and Strbac, Goran and Panciatici, Patrick},
	month = may,
	year = {2017},
	pages = {1417--1426},
}

@article{park_examination_2021,
	title = {Examination of {Semi}-{Analytical} {Solution} {Methods} in the {Coarse} {Operator} of {Parareal} {Algorithm} for {Power} {System} {Simulation}},
	volume = {36},
	issn = {0885-8950, 1558-0679},
	doi = {10.1109/TPWRS.2021.3069136},
	number = {6},
	journal = {IEEE Transactions on Power Systems},
	author = {Park, Byungkwon and Sun, Kai and Dimitrovski, Aleksandar and Liu, Yang and Simunovic, Srdjan},
	month = nov,
	year = {2021},
	pages = {5068--5080},
}

@article{cui_hybrid_2021,
	title = {Hybrid {Symbolic}-{Numeric} {Framework} for {Power} {System} {Modeling} and {Analysis}},
	volume = {36},
	issn = {0885-8950, 1558-0679},
	doi = {10.1109/TPWRS.2020.3017019},
	number = {2},
	journal = {IEEE Transactions on Power Systems},
	author = {Cui, Hantao and Li, Fangxing and Tomsovic, Kevin},
	month = mar,
	year = {2021},
	pages = {1373--1384},
}

@article{milano_open_2005,
	title = {An {Open} {Source} {Power} {System} {Analysis} {Toolbox}},
	volume = {20},
	issn = {0885-8950},
	doi = {10.1109/TPWRS.2005.851911},
	language = {en},
	number = {3},
	journal = {IEEE Transactions on Power Systems},
	author = {Milano, F.},
	month = aug,
	year = {2005},
	pages = {1199--1206},
}

@book{suli_introduction_2003,
	address = {Cambridge ; New York},
	title = {An introduction to numerical analysis},
	isbn = {978-0-521-81026-5},
	publisher = {Cambridge University Press},
	author = {Süli, Endre and Mayers, D. F.},
	year = {2003},
	note = {OCLC: ocm50525488},
	keywords = {Numerical analysis},
}

@article{yasuda_c-e_2022,
	title = {C-{E} (curtailment – {Energy} share) map: {An} objective and quantitative measure to evaluate wind and solar curtailment},
	volume = {160},
	issn = {13640321},
	shorttitle = {C-{E} (curtailment – {Energy} share) map},
	doi = {10.1016/j.rser.2022.112212},
	language = {en},
	journal = {Renewable and Sustainable Energy Reviews},
	author = {Yasuda, Yoh and Bird, Lori and Carlini, Enrico Maria and Eriksen, Peter Børre and Estanqueiro, Ana and Flynn, Damian and Fraile, Daniel and Gómez Lázaro, Emilio and Martín-Martínez, Sergio and Hayashi, Daisuke and Holttinen, Hannele and Lew, Debra and McCam, John and Menemenlis, Nickie and Miranda, Raul and Orths, Antje and Smith, J. Charles and Taibi, Emanuele and Vrana, Til Kristian},
	month = may,
	year = {2022},
	pages = {112212},
}

@article{hatziargyriou_definition_2021,
	title = {Definition and {Classification} of {Power} {System} {Stability} – {Revisited} \& {Extended}},
	volume = {36},
	doi = {10.1109/TPWRS.2020.3041774},
	abstract = {Since the publication of the original paper on power system stability definitions in 2004, the dynamic behavior of power systems has gradually changed due to the increasing penetration of converter interfaced generation technologies, loads, and transmission devices. In recognition of this change, a Task Force was established in 2016 to re-examine and extend, where appropriate, the classic definitions and classifications of the basic stability terms to incorporate the effects of fast-response power electronic devices. This paper based on an IEEE PES report summarizes the major results of the work of the Task Force and presents extended definitions and classification of power system stability.},
	number = {4},
	journal = {IEEE Transactions on Power Systems},
	author = {Hatziargyriou, Nikos and Milanovic, Jovica and Rahmann, Claudia and Ajjarapu, Venkataramana and Canizares, Claudio and Erlich, Istvan and Hill, David and Hiskens, Ian and Kamwa, Innocent and Pal, Bikash and Pourbeik, Pouyan and Sanchez-Gasca, Juan and Stankovic, Aleksandar and Van Cutsem, Thierry and Vittal, Vijay and Vournas, Costas},
	month = jul,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Power Systems},
	keywords = {Control systems, Converter-driven stability, Phase locked loops, Power electronics, Power system stability, Stability criteria, electric resonance stability, frequency stability, power system stability, small-signal stability, transient stability, voltage stability},
	pages = {3271--3281},
}

@article{kundur_definition_2004,
	title = {Definition and classification of power system stability {IEEE}/{CIGRE} joint task force on stability terms and definitions},
	volume = {19},
	doi = {10.1109/TPWRS.2004.825981},
	abstract = {The problem of defining and classifying power system stability has been addressed by several previous CIGRE and IEEE Task Force reports. These earlier efforts, however, do not completely reflect current industry needs, experiences and understanding. In particular, the definitions are not precise and the classifications do not encompass all practical instability scenarios. This report developed by a Task Force, set up jointly by the CIGRE Study Committee 38 and the IEEE Power System Dynamic Performance Committee, addresses the issue of stability definition and classification in power systems from a fundamental viewpoint and closely examines the practical ramifications. The report aims to define power system stability more precisely, provide a systematic basis for its classification, and discuss linkages to related issues such as power system reliability and security.},
	number = {3},
	journal = {IEEE Transactions on Power Systems},
	author = {Kundur, P. and Paserba, J. and Ajjarapu, V. and Andersson, G. and Bose, A. and Canizares, C. and Hatziargyriou, N. and Hill, D. and Stankovic, A. and Taylor, C. and Van Cutsem, T. and Vittal, V.},
	month = aug,
	year = {2004},
	note = {Conference Name: IEEE Transactions on Power Systems},
	keywords = {Power system reliability, Power system security, Power system stability},
	pages = {1387--1401},
}

@misc{noauthor_fluxjl_nodate,
	title = {Flux.jl},
	url = {https://fluxml.ai/fluxml.ai},
	abstract = {The elegant machine learning library},
	language = {en},
	urldate = {2023-06-26},
}

@book{luenberger_linear_2008,
	address = {New York, NY},
	edition = {3rd ed},
	series = {International series in operations research and management science},
	title = {Linear and nonlinear programming},
	isbn = {978-0-387-74502-2},
	publisher = {Springer},
	author = {Luenberger, David G. and Ye, Yinyu},
	year = {2008},
	keywords = {Linear programming, Nonlinear programming},
}

@article{winkler_high-resolution_2023,
	title = {High-{Resolution} {Spectra} of {Supernova} {Remnants} in {M83}},
	volume = {943},
	issn = {0004-637X, 1538-4357},
	url = {http://arxiv.org/abs/2212.00097},
	doi = {10.3847/1538-4357/aca7f9},
	abstract = {In order to better characterize the rich supernova remnant (SNR) population of M83 (NGC 5236), we have obtained high-resolution (about 85 km/s) spectra of 119 of the SNRs and SNR candidates in M83 with Gemini/GMOS, as well as new spectra of the young SNRs B12-174a and SN1957D. Most of the SNRs and SNR candidates have [S II]:H\{{\textbackslash}alpha\} ratios that exceed 0.4. Combining these results with earlier studies we have carried out with MUSE and at lower spectroscopic resolution with GMOS, we have confirmed a total of 238 emission nebulae to be SNRs on the basis of their [S II]:H\{{\textbackslash}alpha\} ratios, about half of which have emission lines that show velocity broadening greater than 100 km/s, providing a kinematic confirmation that they are SNRs and not H II regions. Looking at the entire sample, we find a strong correlation between velocity widths and the line ratios of [O I]\{{\textbackslash}lambda\}6300:H\{{\textbackslash}alpha\}, [N II]\{{\textbackslash}lambda\}6584:H\{{\textbackslash}alpha\} and [S II]\{{\textbackslash}lambda\}\{{\textbackslash}lambda\}6716,6731:H\{{\textbackslash}alpha\}. The density-sensitive [S II]\{{\textbackslash}lambda\}6716:\{{\textbackslash}lambda\}6731 line ratio is strongly correlated with SNR diameter, but not with the velocity width. We discuss these results in the context of previously published shock models.},
	number = {1},
	urldate = {2023-06-25},
	journal = {The Astrophysical Journal},
	author = {Winkler, P. Frank and Long, Knox S. and Blair, William P. and Points, Sean D.},
	month = jan,
	year = {2023},
	note = {arXiv:2212.0097 null},
	keywords = {Astrophysics - Astrophysics of Galaxies, Astrophysics - High Energy Astrophysical Phenomena, Astrophysics - Solar and Stellar Astrophysics},
	pages = {15},
}

@article{illman_patients_2000,
	title = {Patients' voices grow louder in {Great} {Britain}},
	volume = {92},
	issn = {0027-8874},
	doi = {10.1093/jnci/92.17.1373-a},
	language = {eng},
	number = {17},
	journal = {Journal of the National Cancer Institute},
	author = {Illman, J.},
	month = sep,
	year = {2000},
	pmid = {10974067},
	keywords = {Anecdotes as Topic, Health Policy, Humans, Male, National Health Programs, Neoplasms, Patient Advocacy, Physician-Patient Relations, Prostate-Specific Antigen, Prostatic Neoplasms, Public Opinion, State Medicine, United Kingdom, United States},
	pages = {1373--1375},
}

@article{kaiser_impact_1999,
	title = {The impact of the interviewer-interviewee relationship on subjective quality of life ratings in schizophrenia patients},
	volume = {45},
	issn = {0020-7640},
	doi = {10.1177/002076409904500408},
	abstract = {Subjective quality of life (SQOL) ratings are usually based on interviews. This study examined in which way patients' ratings differ depending on whom they are interviewed by. SQOL was assessed in 78 schizophrenia patients in an out patient clinic and in sheltered living arrangements. Using patients randomly allocated to two interview situations: one group was interviewed by external researchers, the other group by their case managers. On average, more favourable ratings were elicited by case managers. Some of the differences were statistically significant and substantial in size. Yet, opposing differences were also found regarding some life domains in one group. It may be concluded that a significant impact of the interviewer-interviewee relationship on SQOL ratings may exist, but that it is not consistent, unidirectional and uniform regarding life domains and across different settings and samples.},
	language = {eng},
	number = {4},
	journal = {The International Journal of Social Psychiatry},
	author = {Kaiser, W. and Priebe, S.},
	year = {1999},
	pmid = {10689613},
	keywords = {Adult, Analysis of Variance, Female, Humans, Interview, Psychological, Male, Mental Health Services, Patient Satisfaction, Professional-Patient Relations, Quality of Life, Schizophrenia, Surveys and Questionnaires},
	pages = {292--301},
}

@article{rohrhofer_role_2022,
	title = {On the {Role} of {Fixed} {Points} of {Dynamical} {Systems} in {Training} {Physics}-{Informed} {Neural} {Networks}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2203.13648},
	doi = {10.48550/ARXIV.2203.13648},
	abstract = {This paper empirically studies commonly observed training difficulties of Physics-Informed Neural Networks (PINNs) on dynamical systems. Our results indicate that fixed points which are inherent to these systems play a key role in the optimization of the in PINNs embedded physics loss function. We observe that the loss landscape exhibits local optima that are shaped by the presence of fixed points. We find that these local optima contribute to the complexity of the physics loss optimization which can explain common training difficulties and resulting nonphysical predictions. Under certain settings, e.g., initial conditions close to fixed points or long simulations times, we show that those optima can even become better than that of the desired solution.},
	urldate = {2023-06-25},
	author = {Rohrhofer, Franz M. and Posch, Stefan and Gößnitzer, Clemens and Geiger, Bernhard C.},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@article{nathasarma_physics-informed_2023,
	title = {Physics-{Informed} {Long}-{Short}-{Term} {Memory} {Neural} {Network} for {Parameters} {Estimation} of {Nonlinear} {Systems}},
	issn = {0093-9994, 1939-9367},
	url = {https://ieeexplore.ieee.org/document/10138438/},
	doi = {10.1109/TIA.2023.3280896},
	urldate = {2023-06-25},
	journal = {IEEE Transactions on Industry Applications},
	author = {Nathasarma, Rahash and Roy, Binoy Krishna},
	year = {2023},
	pages = {1--9},
}

@article{anantharaman_accelerating_2020,
	title = {Accelerating {Simulation} of {Stiff} {Nonlinear} {Systems} using {Continuous}-{Time} {Echo} {State} {Networks}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2010.04004},
	doi = {10.48550/ARXIV.2010.04004},
	abstract = {Modern design, control, and optimization often requires simulation of highly nonlinear models, leading to prohibitive computational costs. These costs can be amortized by evaluating a cheap surrogate of the full model. Here we present a general data-driven method, the continuous-time echo state network (CTESN), for generating surrogates of nonlinear ordinary differential equations with dynamics at widely separated timescales. We empirically demonstrate near-constant time performance using our CTESNs on a physically motivated scalable model of a heating system whose full execution time increases exponentially, while maintaining relative error of within 0.2 \%. We also show that our model captures fast transients as well as slow dynamics effectively, while other techniques such as physics informed neural networks have difficulties trying to train and predict the highly nonlinear behavior of these models.},
	urldate = {2023-06-25},
	author = {Anantharaman, Ranjan and Ma, Yingbo and Gowda, Shashi and Laughman, Chris and Shah, Viral and Edelman, Alan and Rackauckas, Chris},
	year = {2020},
	note = {Publisher: arXiv
Version Number: 6},
	keywords = {Dynamical Systems (math.DS), FOS: Computer and information sciences, FOS: Mathematics, Machine Learning (cs.LG)},
}

@incollection{nguyen_systems_2023,
	address = {New York, NY},
	title = {Systems {Biology}: {Identifiability} {Analysis} and {Parameter} {Identification} via {Systems}-{Biology}-{Informed} {Neural} {Networks}},
	volume = {2634},
	isbn = {978-1-07-163007-5 978-1-07-163008-2},
	shorttitle = {Systems {Biology}},
	url = {https://link.springer.com/10.1007/978-1-0716-3008-2_4},
	language = {en},
	urldate = {2023-06-25},
	booktitle = {Computational {Modeling} of {Signaling} {Networks}},
	publisher = {Springer US},
	author = {Daneker, Mitchell and Zhang, Zhen and Karniadakis, George Em and Lu, Lu},
	editor = {Nguyen, Lan K.},
	year = {2023},
	doi = {10.1007/978-1-0716-3008-2_4},
	note = {Series Title: Methods in Molecular Biology},
	pages = {87--105},
}

@article{fabiani_parsimonious_2023,
	title = {Parsimonious physics-informed random projection neural networks for initial value problems of {ODEs} and index-1 {DAEs}},
	volume = {33},
	issn = {1054-1500, 1089-7682},
	url = {https://pubs.aip.org/aip/cha/article/2878586},
	doi = {10.1063/5.0135903},
	abstract = {We present a numerical method based on random projections with Gaussian kernels and physics-informed neural networks for the numerical solution of initial value problems (IVPs) of nonlinear stiff ordinary differential equations (ODEs) and index-1 differential algebraic equations (DAEs), which may also arise from spatial discretization of partial differential equations (PDEs). The internal weights are fixed to ones while the unknown weights between the hidden and output layer are computed with Newton’s iterations using the Moore–Penrose pseudo-inverse for low to medium scale and sparse QR decomposition with [Formula: see text] regularization for medium- to large-scale systems. Building on previous works on random projections, we also prove its approximation accuracy. To deal with stiffness and sharp gradients, we propose an adaptive step-size scheme and address a continuation method for providing good initial guesses for Newton iterations. The “optimal” bounds of the uniform distribution from which the values of the shape parameters of the Gaussian kernels are sampled and the number of basis functions are “parsimoniously” chosen based on bias-variance trade-off decomposition. To assess the performance of the scheme in terms of both numerical approximation accuracy and computational cost, we used eight benchmark problems (three index-1 DAEs problems, and five stiff ODEs problems including the Hindmarsh–Rose neuronal model of chaotic dynamics and the Allen–Cahn phase-field PDE). The efficiency of the scheme was compared against two stiff ODEs/DAEs solvers, namely, ode15s and ode23t solvers of the MATLAB ODE suite as well as against deep learning as implemented in the DeepXDE library for scientific machine learning and physics-informed learning for the solution of the Lotka–Volterra ODEs included in the demos of the library. A software/toolbox in Matlab (that we call RanDiffNet) with demos is also provided.},
	language = {en},
	number = {4},
	urldate = {2023-06-25},
	journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	author = {Fabiani, Gianluca and Galaris, Evangelos and Russo, Lucia and Siettos, Constantinos},
	month = apr,
	year = {2023},
	pages = {043128},
}

@misc{gusmao_maximum-likelihood_2023,
	title = {Maximum-likelihood {Estimators} in {Physics}-{Informed} {Neural} {Networks} for {High}-dimensional {Inverse} {Problems}},
	url = {http://arxiv.org/abs/2304.05991},
	abstract = {Physics-informed neural networks (PINNs) have proven a suitable mathematical scaffold for solving inverse ordinary (ODE) and partial differential equations (PDE). Typical inverse PINNs are formulated as soft-constrained multi-objective optimization problems with several hyperparameters. In this work, we demonstrate that inverse PINNs can be framed in terms of maximum-likelihood estimators (MLE) to allow explicit error propagation from interpolation to the physical model space through Taylor expansion, without the need of hyperparameter tuning. We explore its application to high-dimensional coupled ODEs constrained by differential algebraic equations that are common in transient chemical and biological kinetics. Furthermore, we show that singular-value decomposition (SVD) of the ODE coupling matrices (reaction stoichiometry matrix) provides reduced uncorrelated subspaces in which PINNs solutions can be represented and over which residuals can be projected. Finally, SVD bases serve as preconditioners for the inversion of covariance matrices in this hyperparameter-free robust application of MLE to ``kinetics-informed neural networks''.},
	urldate = {2023-06-25},
	publisher = {arXiv},
	author = {Gusmão, Gabriel S. and Medford, Andrew J.},
	month = apr,
	year = {2023},
	note = {arXiv:2304.05991 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Dynamical Systems, Mathematics - Numerical Analysis},
}

@misc{desai_one-shot_2022,
	title = {One-{Shot} {Transfer} {Learning} of {Physics}-{Informed} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2110.11286},
	abstract = {Solving differential equations efficiently and accurately sits at the heart of progress in many areas of scientific research, from classical dynamical systems to quantum mechanics. There is a surge of interest in using Physics-Informed Neural Networks (PINNs) to tackle such problems as they provide numerous benefits over traditional numerical approaches. Despite their potential benefits for solving differential equations, transfer learning has been under explored. In this study, we present a general framework for transfer learning PINNs that results in one-shot inference for linear systems of both ordinary and partial differential equations. This means that highly accurate solutions to many unknown differential equations can be obtained instantaneously without retraining an entire network. We demonstrate the efficacy of the proposed deep learning approach by solving several real-world problems, such as first- and second-order linear ordinary equations, the Poisson equation, and the time-dependent Schrodinger complex-value partial differential equation.},
	urldate = {2023-06-25},
	publisher = {arXiv},
	author = {Desai, Shaan and Mattheakis, Marios and Joy, Hayden and Protopapas, Pavlos and Roberts, Stephen},
	month = jul,
	year = {2022},
	note = {arXiv:2110.11286 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics},
}

@misc{flamant_solving_2020,
	title = {Solving {Differential} {Equations} {Using} {Neural} {Network} {Solution} {Bundles}},
	url = {http://arxiv.org/abs/2006.14372},
	abstract = {The time evolution of dynamical systems is frequently described by ordinary differential equations (ODEs), which must be solved for given initial conditions. Most standard approaches numerically integrate ODEs producing a single solution whose values are computed at discrete times. When many varied solutions with different initial conditions to the ODE are required, the computational cost can become significant. We propose that a neural network be used as a solution bundle, a collection of solutions to an ODE for various initial states and system parameters. The neural network solution bundle is trained with an unsupervised loss that does not require any prior knowledge of the sought solutions, and the resulting object is differentiable in initial conditions and system parameters. The solution bundle exhibits fast, parallelizable evaluation of the system state, facilitating the use of Bayesian inference for parameter estimation in real dynamical systems.},
	urldate = {2023-06-25},
	publisher = {arXiv},
	author = {Flamant, Cedric and Protopapas, Pavlos and Sondak, David},
	month = jun,
	year = {2020},
	note = {arXiv:2006.14372 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics},
}

@article{dissanayake_neural-network-based_1994,
	title = {Neural-network-based approximations for solving partial differential equations},
	volume = {10},
	issn = {10698299},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/cnm.1640100303},
	doi = {10.1002/cnm.1640100303},
	language = {en},
	number = {3},
	urldate = {2023-06-25},
	journal = {Communications in Numerical Methods in Engineering},
	author = {Dissanayake, M. W. M. G. and Phan-Thien, N.},
	month = mar,
	year = {1994},
	pages = {195--201},
}

@article{mattheakis_hamiltonian_2022,
	title = {Hamiltonian neural networks for solving equations of motion},
	volume = {105},
	issn = {2470-0045, 2470-0053},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.105.065305},
	doi = {10.1103/PhysRevE.105.065305},
	language = {en},
	number = {6},
	urldate = {2023-06-25},
	journal = {Physical Review E},
	author = {Mattheakis, Marios and Sondak, David and Dogra, Akshunna S. and Protopapas, Pavlos},
	month = jun,
	year = {2022},
	pages = {065305},
}

@misc{liu_residual-based_2023,
	title = {Residual-based error bound for physics-informed neural networks},
	url = {http://arxiv.org/abs/2306.03786},
	abstract = {Neural networks are universal approximators and are studied for their use in solving differential equations. However, a major criticism is the lack of error bounds for obtained solutions. This paper proposes a technique to rigorously evaluate the error bound of Physics-Informed Neural Networks (PINNs) on most linear ordinary differential equations (ODEs), certain nonlinear ODEs, and first-order linear partial differential equations (PDEs). The error bound is based purely on equation structure and residual information and does not depend on assumptions of how well the networks are trained. We propose algorithms that bound the error efficiently. Some proposed algorithms provide tighter bounds than others at the cost of longer run time.},
	urldate = {2023-06-25},
	publisher = {arXiv},
	author = {Liu, Shuheng and Huang, Xiyue and Protopapas, Pavlos},
	month = jun,
	year = {2023},
	note = {arXiv:2306.03786 [cs, math]},
	keywords = {Computer Science - Computational Engineering, Finance, and Science, Mathematics - Numerical Analysis},
}

@article{heldmann_pinn_2023,
	title = {{PINN} training using biobjective optimization: {The} trade-off between data loss and residual loss},
	volume = {488},
	issn = {00219991},
	shorttitle = {{PINN} training using biobjective optimization},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999123003066},
	doi = {10.1016/j.jcp.2023.112211},
	language = {en},
	urldate = {2023-06-25},
	journal = {Journal of Computational Physics},
	author = {Heldmann, Fabian and Berkhahn, Sarah and Ehrhardt, Matthias and Klamroth, Kathrin},
	month = sep,
	year = {2023},
	pages = {112211},
}

@article{prantikos_physics-informed_2022,
	title = {Physics-{Informed} {Neural} {Network} {Solution} of {Point} {Kinetics} {Equations} for a {Nuclear} {Reactor} {Digital} {Twin}},
	volume = {15},
	issn = {1996-1073},
	url = {https://www.mdpi.com/1996-1073/15/20/7697},
	doi = {10.3390/en15207697},
	abstract = {A digital twin (DT) for nuclear reactor monitoring can be implemented using either a differential equations-based physics model or a data-driven machine learning model. The challenge of a physics-model-based DT consists of achieving sufficient model fidelity to represent a complex experimental system, whereas the challenge of a data-driven DT consists of extensive training requirements and a potential lack of predictive ability. We investigate the performance of a hybrid approach, which is based on physics-informed neural networks (PINNs) that encode fundamental physical laws into the loss function of the neural network. We develop a PINN model to solve the point kinetic equations (PKEs), which are time-dependent, stiff, nonlinear, ordinary differential equations that constitute a nuclear reactor reduced-order model under the approximation of ignoring spatial dependence of the neutron flux. The PINN model solution of PKEs is developed to monitor the start-up transient of Purdue University Reactor Number One (PUR-1) using experimental parameters for the reactivity feedback schedule and the neutron source. The results demonstrate strong agreement between the PINN solution and finite difference numerical solution of PKEs. We investigate PINNs performance in both data interpolation and extrapolation. For the test cases considered, the extrapolation errors are comparable to those of interpolation predictions. Extrapolation accuracy decreases with increasing time interval.},
	language = {en},
	number = {20},
	urldate = {2023-06-25},
	journal = {Energies},
	author = {Prantikos, Konstantinos and Tsoukalas, Lefteri H. and Heifetz, Alexander},
	month = oct,
	year = {2022},
	pages = {7697},
}

@misc{chang_multi-level_2018,
	title = {Multi-level {Residual} {Networks} from {Dynamical} {Systems} {View}},
	url = {http://arxiv.org/abs/1710.10348},
	abstract = {Deep residual networks (ResNets) and their variants are widely used in many computer vision applications and natural language processing tasks. However, the theoretical principles for designing and training ResNets are still not fully understood. Recently, several points of view have emerged to try to interpret ResNet theoretically, such as unraveled view, unrolled iterative estimation and dynamical systems view. In this paper, we adopt the dynamical systems point of view, and analyze the lesioning properties of ResNet both theoretically and experimentally. Based on these analyses, we additionally propose a novel method for accelerating ResNet training. We apply the proposed method to train ResNets and Wide ResNets for three image classiﬁcation benchmarks, reducing training time by more than 40\% with superior or on-par accuracy.},
	language = {en},
	urldate = {2023-06-25},
	publisher = {arXiv},
	author = {Chang, Bo and Meng, Lili and Haber, Eldad and Tung, Frederick and Begert, David},
	month = feb,
	year = {2018},
	note = {arXiv:1710.10348 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	copyright = {1986 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	language = {en},
	number = {6088},
	urldate = {2023-06-25},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	note = {Number: 6088
Publisher: Nature Publishing Group},
	keywords = {Humanities and Social Sciences, Science, multidisciplinary},
	pages = {533--536},
}

@book{brooks_handbook_2011,
	address = {New York},
	title = {Handbook of {Markov} {Chain} {Monte} {Carlo}},
	isbn = {978-0-429-13850-8},
	abstract = {Since their popularization in the 1990s, Markov chain Monte Carlo (MCMC) methods have revolutionized statistical computing and have had an especially profound impact on the practice of Bayesian statistics. Furthermore, MCMC methods have enabled the development and use of intricate models in an astonishing array of disciplines as diverse as fisherie},
	publisher = {Chapman and Hall/CRC},
	editor = {Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li, },
	month = may,
	year = {2011},
	doi = {10.1201/b10905},
}

@book{raiffa_applied_2000,
	address = {New York},
	edition = {Wiley classics library ed},
	series = {Wiley classics library},
	title = {Applied statistical decision theory},
	isbn = {978-0-471-38349-9},
	publisher = {Wiley},
	author = {Raiffa, Howard and Schlaifer, Robert},
	year = {2000},
	keywords = {Statistical decision},
}

@article{stone_applied_1961,
	title = {Applied {Statistical} {Decision} {Theory}},
	volume = {3},
	issn = {00401706},
	url = {https://www.jstor.org/stable/1266736?origin=crossref},
	doi = {10.2307/1266736},
	number = {3},
	urldate = {2023-06-24},
	journal = {Technometrics},
	author = {Stone, M. and Raiffa, H. and Schlaifer, R.},
	month = aug,
	year = {1961},
	pages = {439},
}

@misc{noauthor_differentialequationsjl_2023,
	title = {{DifferentialEquations}.jl},
	url = {https://github.com/SciML/DifferentialEquations.jl},
	abstract = {Multi-language suite for high-performance solvers of differential equations and scientific machine learning (SciML) components. Ordinary differential equations (ODEs), stochastic differential equations (SDEs), delay differential equations (DDEs), differential-algebraic equations (DAEs), and more in Julia.},
	urldate = {2023-06-24},
	publisher = {SciML Open Source Scientific Machine Learning},
	month = jun,
	year = {2023},
	note = {original-date: 2016-05-11T05:13:39Z},
	keywords = {dae, dde, delay-differential-equations, differential-algebraic-equations, differential-equations, differentialequations, dynamical-systems, julia, neural-differential-equations, ode, partial-differential-equations, pde, python, r, scientific-machine-learning, sciml, sde, spde, stochastic-differential-equations, stochastic-processes},
}

@article{box_science_1976,
	title = {Science and {Statistics}},
	volume = {71},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1976.10480949},
	doi = {10.1080/01621459.1976.10480949},
	language = {en},
	number = {356},
	urldate = {2023-06-22},
	journal = {Journal of the American Statistical Association},
	author = {Box, George E. P.},
	month = dec,
	year = {1976},
	pages = {791--799},
}

@article{tran_solving_2023,
	title = {Solving {Differential}-{Algebraic} {Equations} in {Power} {Systems} {Dynamics} with {Quantum} {Computing}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2306.01961},
	doi = {10.48550/ARXIV.2306.01961},
	abstract = {Power system dynamics are generally modeled by high dimensional nonlinear differential-algebraic equations due to a large number of generators, loads, and transmission lines. Thus, its computational complexity grows exponentially with the system size. In this paper, we aim to evaluate the alternative computing approach, particularly the use of quantum computing algorithms to solve the power system dynamics. Leveraging a symbolic programming framework, we convert the power system dynamics' DAEs into an equivalent set of ordinary differential equations (ODEs). Their data can be encoded into quantum computers via amplitude encoding. The system's nonlinearity is captured by Taylor polynomial expansion and the quantum state tensor whereas state variables can be updated by a quantum linear equation solver. Our results show that quantum computing can solve the dynamics of the power system with high accuracy whereas its complexity is polynomial in the logarithm of the system dimension.},
	urldate = {2023-06-22},
	author = {Tran, Huynh T. T. and Nguyen, Hieu T. and Vu, Long Thanh and Ojetola, Samuel T.},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {FOS: Physical sciences, Quantum Physics (quant-ph)},
}

@article{kobyzev_normalizing_2021,
	title = {Normalizing {Flows}: {An} {Introduction} and {Review} of {Current} {Methods}},
	volume = {43},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {Normalizing {Flows}},
	url = {http://arxiv.org/abs/1908.09257},
	doi = {10.1109/TPAMI.2020.2992934},
	abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efﬁcient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
	language = {en},
	number = {11},
	urldate = {2023-06-15},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Kobyzev, Ivan and Prince, Simon J. D. and Brubaker, Marcus A.},
	month = nov,
	year = {2021},
	note = {arXiv:1908.09257 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {3964--3979},
}

@misc{goswami_learning_2023,
	title = {Learning stiff chemical kinetics using extended deep neural operators},
	url = {http://arxiv.org/abs/2302.12645},
	abstract = {We utilize neural operators to learn the solution propagator for the challenging chemical kinetics equation. Specifically, we apply the deep operator network (DeepONet) along with its extensions, such as the autoencoder-based DeepONet and the newly proposed Partition-of-Unity (PoU-) DeepONet to study a range of examples, including the ROBERS problem with three species, the POLLU problem with 25 species, pure kinetics of the syngas skeletal model for \$CO/H\_2\$ burning, which contains 11 species and 21 reactions and finally, a temporally developing planar \$CO/H\_2\$ jet flame (turbulent flame) using the same syngas mechanism. We have demonstrated the advantages of the proposed approach through these numerical examples. Specifically, to train the DeepONet for the syngas model, we solve the skeletal kinetic model for different initial conditions. In the first case, we parametrize the initial conditions based on equivalence ratios and initial temperature values. In the second case, we perform a direct numerical simulation of a two-dimensional temporally developing \$CO/H\_2\$ jet flame. Then, we initialize the kinetic model by the thermochemical states visited by a subset of grid points at different time snapshots. Stiff problems are computationally expensive to solve with traditional stiff solvers. Thus, this work aims to develop a neural operator-based surrogate model to solve stiff chemical kinetics. The operator, once trained offline, can accurately integrate the thermochemical state for arbitrarily large time advancements, leading to significant computational gains compared to stiff integration schemes.},
	urldate = {2023-05-02},
	publisher = {arXiv},
	author = {Goswami, Somdatta and Jagtap, Ameya D. and Babaee, Hessam and Susi, Bryan T. and Karniadakis, George Em},
	month = feb,
	year = {2023},
	note = {arXiv:2302.12645 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Chemical Physics},
}

@article{qin_data_2019,
	title = {Data driven governing equations approximation using deep neural networks},
	volume = {395},
	issn = {00219991},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999119304504},
	doi = {10.1016/j.jcp.2019.06.042},
	language = {en},
	urldate = {2023-06-14},
	journal = {Journal of Computational Physics},
	author = {Qin, Tong and Wu, Kailiang and Xiu, Dongbin},
	month = oct,
	year = {2019},
	pages = {620--635},
}

@misc{goswami_physics-informed_2022,
	title = {Physics-{Informed} {Deep} {Neural} {Operator} {Networks}},
	url = {http://arxiv.org/abs/2207.05748},
	abstract = {Standard neural networks can approximate general nonlinear operators, represented either explicitly by a combination of mathematical operators, e.g., in an advection-diﬀusion-reaction partial diﬀerential equation, or simply as a black box, e.g., a system-of-systems. The ﬁrst neural operator was the Deep Operator Network (DeepONet), proposed in 2019 based on rigorous approximation theory. Since then, a few other less general operators have been published, e.g., based on graph neural networks or Fourier transforms. For black box systems, training of neural operators is data-driven only but if the governing equations are known they can be incorporated into the loss function during training to develop physics-informed neural operators. Neural operators can be used as surrogates in design problems, uncertainty quantiﬁcation, autonomous systems, and almost in any application requiring real-time inference. Moreover, independently pre-trained DeepONets can be used as components of a complex multi-physics system by coupling them together with relatively light training. Here, we present a review of DeepONet, the Fourier neural operator, and the graph neural operator, as well as appropriate extensions with feature expansions, and highlight their usefulness in diverse applications in computational mechanics, including porous media, ﬂuid mechanics, and solid mechanics.},
	language = {en},
	urldate = {2023-06-15},
	publisher = {arXiv},
	author = {Goswami, Somdatta and Bora, Aniruddha and Yu, Yue and Karniadakis, George Em},
	month = jul,
	year = {2022},
	note = {arXiv:2207.05748 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
}

@misc{noauthor_math_nodate,
	title = {Math {207A}, {Fall} 2018},
	url = {https://www.math.ucdavis.edu/~hunter/m207a_18/m207a_18.html},
	urldate = {2023-06-08},
}

@article{teschl_ordinary_nodate,
	title = {Ordinary {Differential} {Equations} and {Dynamical} {Systems}},
	language = {en},
	author = {Teschl, Gerald},
}

@article{rackauckas_confederated_2019,
	title = {Confederated modular differential equation {APIs} for accelerated algorithm development and benchmarking},
	volume = {132},
	issn = {09659978},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0965997818310251},
	doi = {10.1016/j.advengsoft.2019.03.009},
	language = {en},
	urldate = {2023-06-08},
	journal = {Advances in Engineering Software},
	author = {Rackauckas, Christopher and Nie, Qing},
	month = jun,
	year = {2019},
	pages = {1--6},
}

@article{johnson_fundamental_nodate,
	title = {The {Fundamental} {Theorem} of {Numerical} {Analysis}},
	language = {en},
	author = {Johnson, Alec},
}

@misc{wadern_dagstuhl_nodate,
	title = {Dagstuhl {Seminar} 22332: {Differential} {Equations} and {Continuous}-{Time} {Deep} {Learning}},
	shorttitle = {Dagstuhl {Seminar} 22332},
	url = {https://www.dagstuhl.de/en/seminars/seminar-calendar/seminar-details/22332},
	language = {en},
	urldate = {2023-06-03},
	author = {Wadern, 66687, Schloss Dagstuhl-Leibniz-Zentrum für Informatik GmbH},
}

@book{brunton_data-driven_2019,
	address = {Cambridge},
	title = {Data-driven science and engineering: machine learning, dynamical systems, and control},
	isbn = {978-1-108-42209-3},
	shorttitle = {Data-driven science and engineering},
	abstract = {"Data-driven discovery is revolutionizing the modelling, prediction, and control of complex systems. This textbook brings together machine learning, engineering mathematics, and mathematical physics to integrate modelling and control of dynamical systems with modern methods in data science. It highlights many of the recent advances in scientific computing that enable data-driven methods to be applied to a diverse range of complex systems, such as turbulence, the brain, climate, epidemiology, finance, robotics, and autonomy. Aimed at advanced undergraduate and beginning graduate students in the engineering and physical sciences, the text presents a range of topics and methods from introductory to state of the art"--},
	publisher = {Cambridge University Press},
	author = {Brunton, Steven L. and Kutz, Jose Nathan},
	year = {2019},
	keywords = {Data processing, Engineering, Mathematical analysis, Science},
}

@misc{noauthor_probable_nodate,
	title = {Probable networks and plausible predictions — a review of practical {Bayesian} methods for supervised neural networks},
	url = {https://www-tandfonline-com.proxy.findit.cvt.dk/doi/epdf/10.1088/0954-898X_6_3_011?needAccess=true&role=button},
	language = {en},
	urldate = {2023-05-30},
	doi = {10.1088/0954-898X_6_3_011},
}

@article{hu_model_2021,
	title = {Model complexity of deep learning: a survey},
	volume = {63},
	issn = {0219-1377, 0219-3116},
	shorttitle = {Model complexity of deep learning},
	url = {https://link.springer.com/10.1007/s10115-021-01605-0},
	doi = {10.1007/s10115-021-01605-0},
	language = {en},
	number = {10},
	urldate = {2023-05-26},
	journal = {Knowledge and Information Systems},
	author = {Hu, Xia and Chu, Lingyang and Pei, Jian and Liu, Weiqing and Bian, Jiang},
	month = oct,
	year = {2021},
	pages = {2585--2619},
}

@inproceedings{vittal_power_1983,
	title = {Power {System} {Transient} {Stability} {Analysis}: {Formulation} as {Nearly} {Hamiltonian} {Systems}},
	shorttitle = {Power {System} {Transient} {Stability} {Analysis}},
	doi = {10.23919/ACC.1983.4788196},
	abstract = {Necessary and sufficient conditions for asymptotic stability of an equilibrium for power systems are established. The authors consider power systems with damping and model these as nearly Hamiltonian systems. Using the invariance principle for ordinary differential equations, a novel method of assessing asymptotic stability is developed. The results shed further light on the mechanism of instability in power systems. An estimate of the domain of attraction of the equilibrium can also be obtained using these results. Furthermore analytical justification is provided for concept of potential energy boundary surface in power systems.},
	booktitle = {1983 {American} {Control} {Conference}},
	author = {Vittal, V. and Michel, A. N. and Fouad, A. A.},
	month = jun,
	year = {1983},
	keywords = {Asymptotic stability, Damping, Differential equations, Hydrogen, Potential energy, Power system analysis computing, Power system modeling, Power system stability, Power system transients, Stability analysis},
	pages = {668--673},
}

@misc{noauthor_mads_2023,
	title = {Mads {Rønne} {Almassalkhi} (@{theEnergyMads}) / {Twitter}},
	url = {https://twitter.com/theEnergyMads},
	abstract = {Professor, scientist, entrepreneur. Father x 3 \& Husband. Prof @uvm\_ee, Chief Sci @PNNLab, Past: Cofounder @ Packetized. @UMichece + @uofcincy alum. B\&R����},
	language = {en},
	urldate = {2023-05-11},
	journal = {Twitter},
	month = may,
	year = {2023},
}

@misc{noauthor_george_2017,
	title = {George {J}. {Pappas} (@pappasg69) / {Twitter}},
	url = {https://twitter.com/pappasg69},
	abstract = {UPS Foundation Professor @Penn, department chair @ESEatPenn, former deputy dean @PennEngineers, former director @GRASPlab},
	language = {en},
	urldate = {2023-05-11},
	journal = {Twitter},
	month = feb,
	year = {2017},
}

@misc{tsiamis_statistical_2023,
	title = {Statistical {Learning} {Theory} for {Control}: {A} {Finite} {Sample} {Perspective}},
	shorttitle = {Statistical {Learning} {Theory} for {Control}},
	url = {http://arxiv.org/abs/2209.05423},
	abstract = {This tutorial survey provides an overview of recent non-asymptotic advances in statistical learning theory as relevant to control and system identification. While there has been substantial progress across all areas of control, the theory is most well-developed when it comes to linear system identification and learning for the linear quadratic regulator, which are the focus of this manuscript. From a theoretical perspective, much of the labor underlying these advances has been in adapting tools from modern high-dimensional statistics and learning theory. While highly relevant to control theorists interested in integrating tools from machine learning, the foundational material has not always been easily accessible. To remedy this, we provide a self-contained presentation of the relevant material, outlining all the key ideas and the technical machinery that underpin recent results. We also present a number of open problems and future directions.},
	language = {en},
	urldate = {2023-05-11},
	publisher = {arXiv},
	author = {Tsiamis, Anastasios and Ziemann, Ingvar and Matni, Nikolai and Pappas, George J.},
	month = apr,
	year = {2023},
	note = {arXiv:2209.05423 [cs, eess, math, stat]},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@book{hase_power_2020,
	address = {Hoboken, NJ},
	title = {Power system dynamics with computer-based modeling and analysis},
	isbn = {978-1-119-48746-3},
	abstract = {A unique combination of theoretical knowledge and practical analysis experience Derived from Yoshihide Hase's Handbook of Power Systems Engineering, 2nd Edition, this book provides readers with everything they need to know about power system dynamics. Presented in three parts, it covers power system theories, computation theories, and how prevailed engineering platforms can be utilized for various engineering works. It features many illustrations based on ETAP to help explain the knowledge within as much as possible.' Recompiling all the chapters from the previous book, Power System Dynamics with Computer Based Modeling and Analysis offers nineteen new and improved content with updated information and all new topics, including two new chapters on circuit analysis which help engineers with non-electrical engineering backgrounds. Topics covered include: Essentials of Electromagnetism; Complex Number Notation (Symbolic Method) and Laplace-transform; Fault Analysis Based on Symmetrical Components; Synchronous Generators; Induction-motor; Transformer; Breaker; Arrester; Overhead-line; Power cable; Steady-State/Transient/Dynamic Stability; Control governor; AVR; Directional Distance Relay and R-X Diagram; Lightning and Switching Surge Phenomena; Insulation Coordination; Harmonics; Power Electronics Applications (Devices, PE-circuit and Control) and more.' -Combines computer modeling of power systems, including analysis techniques, from an engineering consultant's perspective -Uses practical analytical software to help teach how to obtain the relevant data, formulate 'what-if' cases, and convert data analysis into meaningful information -Includes mathematical details of power system analysis and power system dynamics Power System Dynamics with Computer-Based Modeling and Analysis will appeal to all power system engineers as well as engineering and electrical engineering students},
	language = {eng},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Hase, Yoshihide and Khandelwal, Tanuj and Kameda, Kazuyuki},
	year = {2020},
	note = {OCLC: 1090278023},
}

@misc{michalowska_neural_2023,
	title = {Neural {Operator} {Learning} for {Long}-{Time} {Integration} in {Dynamical} {Systems} with {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2303.02243},
	abstract = {Deep neural networks are an attractive alternative for simulating complex dynamical systems, as in comparison to traditional scientific computing methods, they offer reduced computational costs during inference and can be trained directly from observational data. Existing methods, however, cannot extrapolate accurately and are prone to error accumulation in long-time integration. Herein, we address this issue by combining neural operators with recurrent neural networks to construct a novel and effective architecture, resulting in superior accuracy compared to the state-of-the-art. The new hybrid model is based on operator learning while offering a recurrent structure to capture temporal dependencies. The integrated framework is shown to stabilize the solution and reduce error accumulation for both interpolation and extrapolation of the Korteweg-de Vries equation.},
	urldate = {2023-05-02},
	publisher = {arXiv},
	author = {Michałowska, Katarzyna and Goswami, Somdatta and Karniadakis, George Em and Riemer-Sørensen, Signe},
	month = mar,
	year = {2023},
	note = {arXiv:2303.02243 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{vidal_taming_2023,
	title = {Taming hyperparameter tuning in continuous normalizing flows using the {JKO} scheme},
	volume = {13},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-023-31521-y},
	doi = {10.1038/s41598-023-31521-y},
	abstract = {Abstract
            
              A normalizing flow (NF) is a mapping that transforms a chosen probability distribution to a normal distribution. Such flows are a common technique used for data generation and density estimation in machine learning and data science. The density estimate obtained with a NF requires a change of variables formula that involves the computation of the Jacobian determinant of the NF transformation. In order to tractably compute this determinant, continuous normalizing flows (CNF) estimate the mapping and its Jacobian determinant using a neural ODE. Optimal transport (OT) theory has been successfully used to assist in finding CNFs by formulating them as OT problems with a soft penalty for enforcing the standard normal distribution as a target measure. A drawback of OT-based CNFs is the addition of a hyperparameter,
              
                
                  \$\${\textbackslash}alpha \$\$
                  
                    α
                  
                
              
              , that controls the strength of the soft penalty and requires significant tuning. We present JKO-Flow, an algorithm to solve OT-based CNF without the need of tuning
              
                
                  \$\${\textbackslash}alpha \$\$
                  
                    α
                  
                
              
              . This is achieved by integrating the OT CNF framework into a Wasserstein gradient flow framework, also known as the JKO scheme. Instead of tuning
              
                
                  \$\${\textbackslash}alpha \$\$
                  
                    α
                  
                
              
              , we repeatedly solve the optimization problem for a fixed
              
                
                  \$\${\textbackslash}alpha \$\$
                  
                    α
                  
                
              
              effectively performing a JKO update with a time-step
              
                
                  \$\${\textbackslash}alpha \$\$
                  
                    α
                  
                
              
              . Hence we obtain a ”divide and conquer” algorithm by repeatedly solving simpler problems instead of solving a potentially harder problem with large
              
                
                  \$\${\textbackslash}alpha \$\$
                  
                    α
                  
                
              
              .},
	language = {en},
	number = {1},
	urldate = {2023-05-02},
	journal = {Scientific Reports},
	author = {Vidal, Alexander and Wu Fung, Samy and Tenorio, Luis and Osher, Stanley and Nurbekyan, Levon},
	month = mar,
	year = {2023},
	pages = {4501},
}

@inproceedings{rahimi_unsupervised_2006,
	title = {Unsupervised {Regression} with {Applications} to {Nonlinear} {System} {Identification}},
	volume = {19},
	url = {https://papers.nips.cc/paper_files/paper/2006/hash/f0204e1d3ee3e4b05de4e2ddbd39e076-Abstract.html},
	abstract = {We derive a cost functional for estimating the relationship between highdimensional observations and the low-dimensional process that generated them with no input-output examples. Limiting our search to invertible observation functions confers numerous benefits, including a compact representation and no suboptimal local minima. Our approximation algorithms for optimizing this cost functional are fast and give diagnostic bounds on the quality of their solution. Our method can be viewed as a manifold learning algorithm that utilizes a prior on the low-dimensional manifold coordinates. The benefits of taking advantage of such priors in manifold learning and searching for the inverse observation functions in system identification are demonstrated empirically by learning to track moving targets from raw measurements in a sensor network setting and in an RFID tracking experiment.},
	urldate = {2023-04-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Rahimi, Ali and Recht, Ben},
	year = {2006},
}

@misc{duruisseaux_lie_2022,
	title = {Lie {Group} {Forced} {Variational} {Integrator} {Networks} for {Learning} and {Control} of {Robot} {Systems}},
	url = {http://arxiv.org/abs/2211.16006},
	abstract = {Incorporating prior knowledge of physics laws and structural properties of dynamical systems into the design of deep learning architectures has proven to be a powerful technique for improving their computational efficiency and generalization capacity. Learning accurate models of robot dynamics is critical for safe and stable control. Autonomous mobile robots, including wheeled, aerial, and underwater vehicles, can be modeled as controlled Lagrangian or Hamiltonian rigid-body systems evolving on matrix Lie groups. In this paper, we introduce a new structure-preserving deep learning architecture, the Lie group Forced Variational Integrator Network (LieFVIN), capable of learning controlled Lagrangian or Hamiltonian dynamics on Lie groups, either from position-velocity or position-only data. By design, LieFVINs preserve both the Lie group structure on which the dynamics evolve and the symplectic structure underlying the Hamiltonian or Lagrangian systems of interest. The proposed architecture learns surrogate discrete-time flow maps allowing accurate and fast prediction without numerical-integrator, neural-ODE, or adjoint techniques, which are needed for vector fields. Furthermore, the learnt discrete-time dynamics can be utilized with computationally scalable discrete-time (optimal) control strategies.},
	urldate = {2023-04-27},
	publisher = {arXiv},
	author = {Duruisseaux, Valentin and Duong, Thai and Leok, Melvin and Atanasov, Nikolay},
	month = dec,
	year = {2022},
	note = {arXiv:2211.16006 [cs, math]},
	keywords = {68T40, 65P10, 70E60, 68T07, 37J11, Computer Science - Machine Learning, Computer Science - Robotics, I.2.6, I.2.9, Mathematics - Dynamical Systems},
}

@inproceedings{djeumou_neural_2022,
	title = {Neural {Networks} with {Physics}-{Informed} {Architectures} and {Constraints} for {Dynamical} {Systems} {Modeling}},
	url = {https://proceedings.mlr.press/v168/djeumou22a.html},
	abstract = {Effective inclusion of physics-based knowledge into deep neural network models of dynamical systems can greatly improve data efficiency and generalization. Such a priori knowledge might arise from physical principles (e.g., conservation laws) or from the system’s design (e.g., the Jacobian matrix of a robot), even if large portions of the system dynamics remain unknown. We develop a framework to learn dynamics models from trajectory data while incorporating a priori system knowledge as inductive bias. More specifically, the proposed framework uses physics-based side information to inform the structure of the neural network itself, and to place constraints on the values of the outputs and the internal states of the model. It represents the system’s vector field as a composition of known and unknown functions, the latter of which are parametrized by neural networks. The physics-informed constraints are enforced via the augmented Lagrangian method during the model’s training. We experimentally demonstrate the benefits of the proposed approach on a variety of dynamical systems – including a benchmark suite of robotics environments featuring large state spaces, non-linear dynamics, external forces, contact forces, and control inputs. By exploiting a priori system knowledge during training, the proposed approach learns to predict the system dynamics two orders of magnitude more accurately than a baseline approach that does not include prior knowledge, given the same training dataset.},
	language = {en},
	urldate = {2023-04-27},
	booktitle = {Proceedings of {The} 4th {Annual} {Learning} for {Dynamics} and {Control} {Conference}},
	publisher = {PMLR},
	author = {Djeumou, Franck and Neary, Cyrus and Goubault, Eric and Putot, Sylvie and Topcu, Ufuk},
	month = may,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {263--277},
}

@misc{shen_deep_2020,
	title = {Deep {Euler} method: solving {ODEs} by approximating the local truncation error of the {Euler} method},
	shorttitle = {Deep {Euler} method},
	url = {http://arxiv.org/abs/2003.09573},
	abstract = {In this paper, we propose a deep learning-based method, deep Euler method (DEM) to solve ordinary differential equations. DEM significantly improves the accuracy of the Euler method by approximating the local truncation error with deep neural networks which could obtain a high precision solution with a large step size. The deep neural network in DEM is mesh-free during training and shows good generalization in unmeasured regions. DEM could be easily combined with other schemes of numerical methods, such as Runge-Kutta method to obtain better solutions. Furthermore, the error bound and stability of DEM is discussed.},
	urldate = {2023-04-27},
	publisher = {arXiv},
	author = {Shen, Xing and Cheng, Xiaoliang and Liang, Kewei},
	month = mar,
	year = {2020},
	note = {arXiv:2003.09573 [cs, math]},
	keywords = {Mathematics - Numerical Analysis},
}

@misc{galaris_numerical_2021,
	title = {Numerical {Solution} of {Stiff} {ODEs} with {Physics}-{Informed} {RPNNs}},
	url = {http://arxiv.org/abs/2108.01584},
	abstract = {We propose a numerical method based on physics-informed Random Projection Neural Networks for the solution of Initial Value Problems (IVPs) of Ordinary Differential Equations (ODEs) with a focus on stiff problems. We address an Extreme Learning Machine with a single hidden layer with radial basis functions having as widths uniformly distributed random variables, while the values of the weights between the input and the hidden layer are set equal to one. The numerical solution of the IVPs is obtained by constructing a system of nonlinear algebraic equations, which is solved with respect to the output weights by the Gauss-Newton method, using a simple adaptive scheme for adjusting the time interval of integration. To assess its performance, we apply the proposed method for the solution of four benchmark stiff IVPs, namely the Prothero-Robinson, van der Pol, ROBER and HIRES problems. Our method is compared with an adaptive Runge-Kutta method based on the Dormand-Prince pair, and a variable-step variable-order multistep solver based on numerical differentiation formulas, as implemented in the {\textbackslash}texttt\{ode45\} and {\textbackslash}texttt\{ode15s\} MATLAB functions, respectively. We show that the proposed scheme yields good approximation accuracy, thus outperforming {\textbackslash}texttt\{ode45\} and {\textbackslash}texttt\{ode15s\}, especially in the cases where steep gradients arise. Furthermore, the computational times of our approach are comparable with those of the two MATLAB solvers for practical purposes.},
	urldate = {2023-04-27},
	publisher = {arXiv},
	author = {Galaris, Evangelos and Fabiani, Gianluca and Calabrò, Francesco and di Serafino, Daniela and Siettos, Constantinos},
	month = nov,
	year = {2021},
	note = {arXiv:2108.01584 [cs, math]},
	keywords = {65L04, 68T07, 65D12, 60B20, Computer Science - Machine Learning, Mathematics - Numerical Analysis},
}

@article{zhu_convolutional_2023,
	title = {Convolutional neural networks combined with {Runge}–{Kutta} methods},
	volume = {35},
	issn = {0941-0643, 1433-3058},
	url = {https://link.springer.com/10.1007/s00521-022-07785-2},
	doi = {10.1007/s00521-022-07785-2},
	language = {en},
	number = {2},
	urldate = {2023-04-27},
	journal = {Neural Computing and Applications},
	author = {Zhu, Mai and Chang, Bo and Fu, Chong},
	month = jan,
	year = {2023},
	pages = {1629--1643},
}

@article{qiu_accuracy_2023,
	title = {Accuracy and {Architecture} {Studies} of {Residual} {Neural} {Network} {Method} for {Ordinary} {Differential} {Equations}},
	volume = {95},
	issn = {0885-7474, 1573-7691},
	url = {https://link.springer.com/10.1007/s10915-023-02173-x},
	doi = {10.1007/s10915-023-02173-x},
	language = {en},
	number = {2},
	urldate = {2023-04-27},
	journal = {Journal of Scientific Computing},
	author = {Qiu, Changxin and Bendickson, Aaron and Kalyanapu, Joshua and Yan, Jue},
	month = may,
	year = {2023},
	pages = {50},
}

@article{li_solving_2021,
	title = {Solving ordinary differential equations using an optimization technique based on training improved artificial neural networks},
	volume = {25},
	issn = {1432-7643, 1433-7479},
	url = {https://link.springer.com/10.1007/s00500-020-05401-w},
	doi = {10.1007/s00500-020-05401-w},
	language = {en},
	number = {5},
	urldate = {2023-04-27},
	journal = {Soft Computing},
	author = {Li, Shangjie and Wang, Xingang},
	month = mar,
	year = {2021},
	pages = {3713--3723},
}

@misc{muller_space-time_2020,
	title = {On the space-time expressivity of {ResNets}},
	url = {http://arxiv.org/abs/1910.09599},
	abstract = {Residual networks (ResNets) are a deep learning architecture that substantially improved the state of the art performance in certain supervised learning tasks. Since then, they have received continuously growing attention. ResNets have a recursive structure \$x\_\{k+1\} = x\_k + R\_k(x\_k)\$ where \$R\_k\$ is a neural network called a residual block. This structure can be seen as the Euler discretisation of an associated ordinary differential equation (ODE) which is called a neural ODE. Recently, ResNets were proposed as the space-time approximation of ODEs which are not of this neural type. To elaborate this connection we show that by increasing the number of residual blocks as well as their expressivity the solution of an arbitrary ODE can be approximated in space and time simultaneously by deep ReLU ResNets. Further, we derive estimates on the complexity of the residual blocks required to obtain a prescribed accuracy under certain regularity assumptions.},
	urldate = {2023-04-27},
	publisher = {arXiv},
	author = {Müller, Johannes},
	month = feb,
	year = {2020},
	note = {arXiv:1910.09599 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Numerical Analysis, Statistics - Machine Learning},
}

@article{kumar_multilayer_2011,
	title = {Multilayer perceptrons and radial basis function neural network methods for the solution of differential equations: {A} survey},
	volume = {62},
	issn = {08981221},
	shorttitle = {Multilayer perceptrons and radial basis function neural network methods for the solution of differential equations},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0898122111007966},
	doi = {10.1016/j.camwa.2011.09.028},
	language = {en},
	number = {10},
	urldate = {2023-04-27},
	journal = {Computers \& Mathematics with Applications},
	author = {Kumar, Manoj and Yadav, Neha},
	month = nov,
	year = {2011},
	pages = {3796--3811},
}

@article{filici_error_2010,
	title = {Error estimation in the neural network solution of ordinary differential equations},
	volume = {23},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608009002020},
	doi = {10.1016/j.neunet.2009.05.014},
	language = {en},
	number = {5},
	urldate = {2023-04-27},
	journal = {Neural Networks},
	author = {Filici, Cristian},
	month = jun,
	year = {2010},
	pages = {614--617},
}

@article{qin_data-driven_2021,
	title = {Data-{Driven} {Learning} of {Nonautonomous} {Systems}},
	volume = {43},
	issn = {1064-8275, 1095-7197},
	url = {https://epubs.siam.org/doi/10.1137/20M1342859},
	doi = {10.1137/20M1342859},
	language = {en},
	number = {3},
	urldate = {2023-04-27},
	journal = {SIAM Journal on Scientific Computing},
	author = {Qin, Tong and Chen, Zhen and Jakeman, John D. and Xiu, Dongbin},
	month = jan,
	year = {2021},
	pages = {A1607--A1624},
}

@misc{noauthor_data-driven_nodate,
	title = {Data-{Driven} {Learning} of {Nonautonomous} {Systems}},
	url = {https://epubs-siam-org.proxy.findit.cvt.dk/doi/epdf/10.1137/20M1342859},
	language = {en},
	urldate = {2023-04-27},
	doi = {10.1137/20M1342859},
}

@inproceedings{jacot_neural_2018,
	title = {Neural {Tangent} {Kernel}: {Convergence} and {Generalization} in {Neural} {Networks}},
	volume = {31},
	shorttitle = {Neural {Tangent} {Kernel}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/hash/5a4be1fa34e62bb8a6ec6b91d2462f5a-Abstract.html},
	abstract = {At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function (which maps input vectors to output vectors) follows the so-called kernel gradient associated with a new object, which we call the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK.},
	urldate = {2023-04-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
	year = {2018},
}

@book{barenblatt_scaling_1996,
	edition = {1},
	title = {Scaling, {Self}-similarity, and {Intermediate} {Asymptotics}: {Dimensional} {Analysis} and {Intermediate} {Asymptotics}},
	isbn = {978-1-107-05024-2},
	shorttitle = {Scaling, {Self}-similarity, and {Intermediate} {Asymptotics}},
	abstract = {Scaling laws reveal the fundamental property of phenomena, namely self-similarity - repeating in time and/or space - which substantially simplifies the mathematical modelling of the phenomena themselves. This book begins from a non-traditional exposition of dimensional analysis, physical similarity theory, and general theory of scaling phenomena, using classical examples to demonstrate that the onset of scaling is not until the influence of initial and/or boundary conditions has disappeared but when the system is still far from equilibrium. Numerous examples from a diverse range of fields, including theoretical biology, fracture mechanics, atmospheric and oceanic phenomena, and flame propagation, are presented for which the ideas of scaling, intermediate asymptotics, self-similarity, and renormalisation were of decisive value in modelling.},
	publisher = {Cambridge University Press},
	author = {Barenblatt, Grigory Isaakovich},
	month = dec,
	year = {1996},
	doi = {10.1017/CBO9781107050242},
}

@misc{lara_revisiting_2023,
	title = {Revisiting {Power} {Systems} {Time}-domain {Simulation} {Methods} and {Models}},
	url = {http://arxiv.org/abs/2301.10043},
	abstract = {The changing nature of power systems dynamics is challenging present practices related to modeling and study of system-level dynamic behavior. While developing new techniques and models to handle the new modeling requirements, it is also critical to review some of the terminology used to describe existing simulation approaches and the embedded assumptions. This paper provides a first-principles review of the simplifications and transformation commonly used in the formulation of time-domain simulation models. It introduces a taxonomy and classification of time-domain simulation models depending on their frequency bandwidth, network representation, and software availability. Furthermore, it focuses on the fundamental aspects of averaging techniques, and model reduction approaches that result in modeling choices, and discusses the associated challenges and opportunities of applying these methods in systems with large shares of Inverter Based Resources (IBRs). The paper concludes with an illustrative simulation that compares the trajectories of an IBR-dominated system.},
	urldate = {2023-04-20},
	publisher = {arXiv},
	author = {Lara, Jose Daniel and Henriquez-Auba, Rodrigo and Ramasubramanian, Deepak and Dhople, Sairaj and Callaway, Duncan S. and Sanders, Seth},
	month = jan,
	year = {2023},
	note = {arXiv:2301.10043 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Systems and Control},
}

@article{roberts_continuous-time_2022,
	title = {Continuous-time echo state networks for predicting power system dynamics},
	volume = {212},
	issn = {0378-7796},
	url = {https://www.sciencedirect.com/science/article/pii/S0378779622006587},
	doi = {10.1016/j.epsr.2022.108562},
	abstract = {With the growing penetration of converter-interfaced generation in power systems, the dynamical behavior of these systems is rapidly evolving. One of the challenges with converter-interfaced generation is the increased number of equations, as well as the required numerical timestep, involved in simulating these systems. Within this work, we explore the use of continuous-time echo state networks as a means to cheaply, and accurately, predict the dynamic response of power systems subject to a disturbance for varying system parameters. We show an application for predicting frequency dynamics following a loss of generation for varying penetrations of grid-following and grid-forming converters. We demonstrate that, after training on 20 solutions of the full-order system, we achieve a median nadir prediction error of 0.17 mHz with 95\% of all nadir prediction errors within ±4 mHz. We conclude with some discussion on how this approach can be used for parameter sensitivity analysis and within optimization algorithms to rapidly predict the dynamical behavior of the system.},
	language = {en},
	urldate = {2023-04-20},
	journal = {Electric Power Systems Research},
	author = {Roberts, Ciaran and Lara, José Daniel and Henriquez-Auba, Rodrigo and Bossart, Matthew and Anantharaman, Ranjan and Rackauckas, Chris and Hodge, Bri-Mathias and Callaway, Duncan S.},
	month = nov,
	year = {2022},
	keywords = {Data-driven modeling techniques, Electro-magnetic transients, Machine learning, Power system dynamics},
	pages = {108562},
}

@book{mitchell_machine_1997,
	address = {New York},
	series = {{McGraw}-{Hill} series in computer science},
	title = {Machine {Learning}},
	isbn = {978-0-07-042807-2},
	language = {en},
	publisher = {McGraw-Hill},
	author = {Mitchell, Tom M.},
	year = {1997},
	keywords = {Computer algorithms, Machine learning},
}

@book{higham_accuracy_2002,
	address = {Philadelphia},
	edition = {2nd ed},
	title = {Accuracy and stability of numerical algorithms},
	isbn = {978-0-89871-521-7},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Higham, Nicholas J.},
	year = {2002},
	keywords = {Computer algorithms, Data processing, Numerical analysis},
}

@book{ljung_system_1999,
	address = {Upper Saddle River, NJ},
	edition = {2nd ed},
	series = {Prentice {Hall} information and system sciences series},
	title = {System identification: theory for the user},
	isbn = {978-0-13-656695-3},
	shorttitle = {System identification},
	publisher = {Prentice Hall PTR},
	author = {Ljung, Lennart},
	year = {1999},
	keywords = {System identification},
}

@article{lagaris_artificial_1998,
	title = {Artificial neural networks for solving ordinary and partial differential equations},
	volume = {9},
	issn = {10459227},
	doi = {10.1109/72.712178},
	number = {5},
	journal = {IEEE Transactions on Neural Networks},
	author = {Lagaris, I.E. and Likas, A. and Fotiadis, D.I.},
	month = sep,
	year = {1998},
	pages = {987--1000},
}

@article{hoffman_no-u-turn_2014,
	title = {The no-{U}-turn sampler: {Adaptively} setting path lengths in hamiltonian monte carlo},
	volume = {15},
	url = {http://jmlr.org/papers/v15/hoffman14a.html},
	number = {47},
	journal = {Journal of Machine Learning Research},
	author = {Hoffman, Matthew D. and Gelman, Andrew},
	year = {2014},
	pages = {1593--1623},
}

@inproceedings{liu_stein_2016,
	title = {Stein variational gradient descent: {A} general purpose bayesian inference algorithm},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/b3ba8f1bee1238a2f37603d90b58898d-Paper.pdf},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Qiang and Wang, Dilin},
	editor = {Lee, D. and Sugiyama, M. and Luxburg, U. and Guyon, I. and Garnett, R.},
	year = {2016},
}

@article{hullermeier_aleatoric_2021,
	title = {Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods},
	volume = {110},
	issn = {1573-0565},
	shorttitle = {Aleatoric and epistemic uncertainty in machine learning},
	url = {https://doi.org/10.1007/s10994-021-05946-3},
	doi = {10.1007/s10994-021-05946-3},
	abstract = {The notion of uncertainty is of major importance in machine learning and constitutes a key element of machine learning methodology. In line with the statistical tradition, uncertainty has long been perceived as almost synonymous with standard probability and probabilistic predictions. Yet, due to the steadily increasing relevance of machine learning for practical applications and related issues such as safety requirements, new problems and challenges have recently been identified by machine learning scholars, and these problems may call for new methodological developments. In particular, this includes the importance of distinguishing between (at least) two different types of uncertainty, often referred to as aleatoric and epistemic. In this paper, we provide an introduction to the topic of uncertainty in machine learning as well as an overview of attempts so far at handling uncertainty in general and formalizing this distinction in particular.},
	language = {en},
	number = {3},
	urldate = {2023-04-12},
	journal = {Machine Learning},
	author = {Hüllermeier, Eyke and Waegeman, Willem},
	month = mar,
	year = {2021},
	keywords = {Bayesian inference, Calibration, Conformal prediction, Credal sets and classifiers, Deep neural networks, Ensembles, Epistemic uncertainty, Gaussian processes, Generative models, Likelihood-based methods, Probability, Set-valued prediction, Uncertainty, Version space learning},
	pages = {457--506},
}

@article{tsaousoglou_new_2023,
	title = {A {New} {Notion} of {Reserve} for {Power} {Systems} {With} {High} {Penetration} of {Storage} and {Flexible} {Demand}},
	issn = {2771-9626},
	doi = {10.1109/TEMPR.2023.3259028},
	abstract = {Modern power systems face important demand uncertainties due to increasing penetration of behind-the-meter renewable generation. System operators need to account for such uncertainties when solving the unit commitment and economic dispatch problem. The research literature has proposed advanced methods for decision making under uncertainty but, in practice, actual system operators put more trust in the tried-and-true approach of dealing with future uncertainty by committing reserves. In this paper, the unit commitment and economic dispatch problem is formulated for a system with high penetration of storage and the inadequacy of methods based on the traditional notion of reserves is exposed. Namely, in contrast to a generator, a storage unit can provide reserve capacity in a number of timeslots but it cannot provide an analogous reserve activation in all of those timeslots due to the battery's energy being depleted. After discussing two plausible but inadequate approaches, a new, generalized notion of reserves is proposed, which addresses these issues while not abandoning the practical, reserve-based approach for the operator's problem, thus making the best of both worlds. The proposed scheme enables storage units to provide reserves, without putting the system at risk of energy scarcity, which is shown to result in substantial cost savings.},
	journal = {Policy and Regulation IEEE Transactions on Energy Markets},
	author = {Tsaousoglou, Georgios},
	year = {2023},
	note = {Conference Name: Policy and Regulation IEEE Transactions on Energy Markets},
	keywords = {Economic dispatch, Generators, Power systems, Random variables, Real-time systems, Regulation, Standards, Uncertainty, chance constraints, reserves, unit commitment},
	pages = {1--13},
}

@book{gramacy_surrogates_2020,
	address = {Boca Raton},
	title = {Surrogates: {Gaussian} process modeling, design, and optimization for the applied sciences},
	isbn = {978-0-367-81549-3},
	shorttitle = {Surrogates},
	abstract = {"Surrogates: a graduate textbook, or professional handbook, on topics at the interface between machine learning, spatial statistics, computer simulation, meta-modeling (i.e., emulation), design of experiments, and optimization. Experimentation through simulation, "human out-of-the-loop" statistical support (focusing on the science), management of dynamic processes, online and real-time analysis, automation, and practical application are at the forefront. Topics include: Gaussian process (GP) regression for flexible nonparametric and nonlinear modeling. Applications to uncertainty quantification, sensitivity analysis, calibration of computer models to field data, sequential design/active learning and (blackbox/Bayesian) optimization under uncertainty. Advanced topics include treed partitioning, local GP approximation, modeling of simulation experiments (e.g., agent-based models) with coupled nonlinear mean and variance (heteroskedastic) models. Treatment appreciates historical response surface methodology (RSM) and canonical examples, but emphasizes contemporary methods and implementation in R at modern scale. Rmarkdown facilitates a fully reproducible tour, complete with motivation from, application to, and illustration with, compelling real-data examples. Presentation targets numerically competent practitioners in engineering, physical, and biological sciences. Writing is statistical in form, but the subjects are not about statistics. Rather, they're about prediction and synthesis under uncertainty; about visualization and information, design and decision making, computing and clean code"--},
	publisher = {CRC Press, Taylor \& Francis Group},
	author = {Gramacy, Robert B.},
	year = {2020},
	keywords = {Computer simulation, Data processing, Gaussian processes, Mathematical models, R (Computer program language), Regression analysis, Response surfaces (Statistics)},
}

@phdthesis{muller_development_2021,
	address = {Kongens Lyngby},
	title = {Development of {Methods} for {Element}-{Wise} {Assessment} of {Oscillatory} {Rotor} {Angle} {Stability}},
	language = {en},
	school = {Technical University of Denmark},
	author = {Müller, Daniel},
	year = {2021},
}

@article{panciatici_operating_2012,
	title = {Operating in the {Fog}: {Security} {Management} {Under} {Uncertainty}},
	volume = {10},
	issn = {1558-4216},
	shorttitle = {Operating in the {Fog}},
	doi = {10.1109/MPE.2012.2205318},
	abstract = {Over the last ten years, we have heard so often in conferences, seminars, and workshops that the power system will soon be operated very near to its limits that this statement has become a cliché. Unfortunately, it is no longer possible to comply with the classical preventive N-1 security standards during all of the hours in a year. The system is indeed no longer able to survive all single faults without postfault actions. More and more corrective (i.e., postfault) actions are defined and prepared by operators, and the cliché is now a reality, as a matter of fact. To be more precise, it is no longer possible to maintain the N-1 security of the system at all moments by using only preventive actions, and the number of hours during which the system requires corrective actions to be secure is increasing. More and more, new special protection schemes (SPSs) are deployed to implement some of these corrective actions automatically. Devices such as phase-shifting transformers (PSTs) and static var compensators (SVCs) are added in the system to increase its controllability. As a result, the system becomes more and more complex.},
	number = {5},
	journal = {IEEE Power and Energy Magazine},
	author = {Panciatici, Patrick and Bareux, Gabriel and Wehenkel, Louis},
	month = sep,
	year = {2012},
	note = {Conference Name: IEEE Power and Energy Magazine},
	keywords = {Fault diagnosis, Phase shift keying, Power system planning, Power system reliability, Security, Static VAr compensators, Transformers},
	pages = {40--49},
}

@phdthesis{venzke_machine_2020,
	address = {Kongens Lyngby},
	title = {Machine {Learning} and {Convex} {Optimization} for {Secure} {Power} {System} {Operation}},
	language = {en},
	school = {Technical University of Denmark},
	author = {Venzke, Andreas Horst},
	year = {2020},
}

@inproceedings{karangelos_whither_2013,
	title = {Whither probabilistic security management for real-time operation of power systems?},
	doi = {10.1109/IREP.2013.6629408},
	abstract = {This paper investigates the stakes of introducing probabilistic approaches for the management of power system's security. In real-time operation, the aim is to arbitrate in a rational way between preventive and corrective control, while taking into account i) the prior probabilities of contingencies, ii) the possible failure modes of corrective control actions, iii) the socio-economic consequences of service interruptions. This work is a first step towards the construction of a globally coherent decision making framework for security management from long-term system expansion, via mid-term asset management, towards short-term operation planning and real-time operation.},
	booktitle = {2013 {IREP} {Symposium} {Bulk} {Power} {System} {Dynamics} and {Control} - {IX} {Optimization}, {Security} and {Control} of the {Emerging} {Power} {Grid}},
	author = {Karangelos, Efthymios and Panciatici, Patrick and Wehenkel, Louis},
	month = aug,
	year = {2013},
	keywords = {Approximation methods, Decision making, Niobium, Power transmission lines, Probabilistic logic, Real-time systems, Security},
	pages = {1--17},
}

@phdthesis{thams_data-driven_2018,
	address = {Kongens Lyngby},
	title = {Data-{Driven} and {HVDC} {Control} {Methods} to {Enhance} {Power} {System} {Security}},
	language = {en},
	school = {Technical University of Denmark},
	author = {Thams, Florian},
	year = {2018},
}

@phdthesis{halilbasic_convex_2019,
	address = {Kongens Lyngby},
	title = {Convex {Reformulations} of {Security} and {Uncertainty} {Constraints} for {Power} {System} {Optimization}},
	language = {en},
	school = {Technical University of Denmark},
	author = {Halilbašić, Lejla},
	year = {2019},
}

@phdthesis{ratha_market_2022,
	address = {Kongens Lyngby},
	title = {Market {Design} for {Integrated} {Energy} {Systems} of the {Future}},
	school = {Technical University of Denmark},
	author = {Ratha, Anubhav},
	year = {2022},
}

@phdthesis{frolke_market_2023,
	address = {Kongens Lyngby},
	title = {Market {Design} for {Future} {District} {Heating} {Systems}},
	school = {Technical University of Denmark},
	author = {Frölke, Linde},
	year = {2023},
}

@article{yu_gradient-enhanced_2022,
	title = {Gradient-enhanced physics-informed neural networks for forward and inverse {PDE} problems},
	volume = {393},
	issn = {00457825},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0045782522001438},
	doi = {10.1016/j.cma.2022.114823},
	language = {en},
	urldate = {2023-03-06},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Yu, Jeremy and Lu, Lu and Meng, Xuhui and Karniadakis, George Em},
	month = apr,
	year = {2022},
	pages = {114823},
}

@book{milano_power_2010,
	address = {Berlin, Heidelberg},
	series = {Power {Systems}},
	title = {Power {System} {Modelling} and {Scripting}},
	isbn = {978-3-642-13668-9 978-3-642-13669-6},
	language = {en},
	publisher = {Springer Berlin Heidelberg},
	author = {Milano, Federico},
	year = {2010},
	doi = {10.1007/978-3-642-13669-6},
	keywords = {Power system modelling},
}

@book{machowski_power_2008,
	address = {Chichester, U.K},
	edition = {2nd ed},
	title = {Power system dynamics: stability and control},
	isbn = {978-0-470-72558-0},
	shorttitle = {Power system dynamics},
	publisher = {Wiley},
	author = {Machowski, Jan and Bialek, Janusz W. and Bumby, J. R.},
	year = {2008},
	keywords = {Control, Electric power system stability, Electric power systems},
}

@book{campbell_applications_2019,
	address = {Cham},
	series = {Differential-{Algebraic} {Equations} {Forum}},
	title = {Applications of {Differential}-{Algebraic} {Equations}: {Examples} and {Benchmarks}},
	isbn = {978-3-030-03717-8 978-3-030-03718-5},
	shorttitle = {Applications of {Differential}-{Algebraic} {Equations}},
	language = {en},
	urldate = {2023-03-06},
	publisher = {Springer International Publishing},
	editor = {Campbell, Stephen and Ilchmann, Achim and Mehrmann, Volker and Reis, Timo},
	year = {2019},
	doi = {10.1007/978-3-030-03718-5},
}

@book{kunkel_differential-algebraic_2006,
	edition = {1},
	title = {Differential-{Algebraic} {Equations}: {Analysis} and {Numerical} {Solution}},
	isbn = {978-3-03719-017-3 978-3-03719-517-8},
	shorttitle = {Differential-{Algebraic} {Equations}},
	url = {https://ems.press/doi/10.4171/017},
	urldate = {2023-03-06},
	publisher = {EMS Press},
	author = {Kunkel, Peter and Mehrmann, Volker},
	month = feb,
	year = {2006},
	doi = {10.4171/017},
}

@article{schulz_four_2003,
	title = {Four {Lectures} on {Diﬀerential}-{Algebraic} {Equations}},
	abstract = {Diﬀerential-algebraic equations (DAEs) arise in a variety of applications. Therefore their analysis and numerical treatment plays an important role in modern mathematics. This paper gives an introduction to the topic of DAEs. Examples of DAEs are considered showing their importance for practical problems. Several well known index concepts are introduced. In the context of the tractability index existence and uniqueness of solutions for low index linear DAEs is proved. Numerical methods applied to these equations are studied.},
	language = {en},
	author = {Schulz, Steﬀen},
	year = {2003},
}

@article{moya_dae-pinn_2023,
	title = {{DAE}-{PINN}: a physics-informed neural network model for simulating differential algebraic equations with application to power networks},
	volume = {35},
	issn = {0941-0643, 1433-3058},
	shorttitle = {{DAE}-{PINN}},
	doi = {10.1007/s00521-022-07886-y},
	language = {en},
	number = {5},
	journal = {Neural Computing and Applications},
	author = {Moya, Christian and Lin, Guang},
	month = feb,
	year = {2023},
	pages = {3789--3804},
}

@article{duan_power_2017,
	title = {Power {System} {Simulation} {Using} the {Multistage} {Adomian} {Decomposition} {Method}},
	volume = {32},
	issn = {0885-8950, 1558-0679},
	doi = {10.1109/TPWRS.2016.2551688},
	number = {1},
	journal = {IEEE Transactions on Power Systems},
	author = {Duan, Nan and Sun, Kai},
	month = jan,
	year = {2017},
	pages = {430--441},
}

@article{alabert_linear_2006,
	title = {Linear stochastic differential-algebraic equations with constant coefficients},
	volume = {11},
	issn = {1083-589X},
	url = {https://projecteuclid.org/journals/electronic-communications-in-probability/volume-11/issue-none/Linear-stochastic-differential-algebraic-equations-with-constant-coefficients/10.1214/ECP.v11-1236.full},
	doi = {10.1214/ECP.v11-1236},
	number = {none},
	urldate = {2023-03-03},
	journal = {Electronic Communications in Probability},
	author = {Alabert, Aureli and Ferrante, Marco},
	month = jan,
	year = {2006},
}

@misc{chen_continuous-time_2018,
	title = {Continuous-{Time} {Flows} for {Efficient} {Inference} and {Density} {Estimation}},
	url = {http://arxiv.org/abs/1709.01179},
	doi = {10.48550/arXiv.1709.01179},
	abstract = {Two fundamental problems in unsupervised learning are efficient inference for latent-variable models and robust density estimation based on large amounts of unlabeled data. Algorithms for the two tasks, such as normalizing flows and generative adversarial networks (GANs), are often developed independently. In this paper, we propose the concept of \{{\textbackslash}em continuous-time flows\} (CTFs), a family of diffusion-based methods that are able to asymptotically approach a target distribution. Distinct from normalizing flows and GANs, CTFs can be adopted to achieve the above two goals in one framework, with theoretical guarantees. Our framework includes distilling knowledge from a CTF for efficient inference, and learning an explicit energy-based distribution with CTFs for density estimation. Both tasks rely on a new technique for distribution matching within amortized learning. Experiments on various tasks demonstrate promising performance of the proposed CTF framework, compared to related techniques.},
	urldate = {2023-03-02},
	publisher = {arXiv},
	author = {Chen, Changyou and Li, Chunyuan and Chen, Liqun and Wang, Wenlin and Pu, Yunchen and Carin, Lawrence},
	month = aug,
	year = {2018},
	note = {arXiv:1709.01179 [stat]},
	keywords = {Statistics - Machine Learning},
}

@misc{rezende_variational_2016,
	title = {Variational {Inference} with {Normalizing} {Flows}},
	url = {http://arxiv.org/abs/1505.05770},
	doi = {10.48550/arXiv.1505.05770},
	abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
	urldate = {2023-03-02},
	publisher = {arXiv},
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
	month = jun,
	year = {2016},
	note = {arXiv:1505.05770 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology},
}

@misc{noauthor_normalizing_nodate,
	title = {Normalizing {Flows}},
	url = {https://akosiorek.github.io/ml/2018/04/03/norm_flows.html},
	urldate = {2023-03-02},
}

@misc{lozano-duran_information-theoretic_2022,
	title = {Information-theoretic formulation of dynamical systems: causality, modeling, and control},
	shorttitle = {Information-theoretic formulation of dynamical systems},
	url = {http://arxiv.org/abs/2111.09484},
	abstract = {The problems of causality, modeling, and control for chaotic, high-dimensional dynamical systems are formulated in the language of information theory. The central quantity of interest is the Shannon entropy, which measures the amount of information in the states of the system. Within this framework, causality is quantified by the information flux among the variables of interest in the dynamical system. Reduced-order modeling is posed as a problem related to the conservation of information in which models aim at preserving the maximum amount of relevant information from the original system. Similarly, control theory is cast in information-theoretic terms by envisioning the tandem sensor-actuator as a device reducing the unknown information of the state to be controlled. The new formulation is used to address three problems about the causality, modeling, and control of turbulence, which stands as a primary example of a chaotic, high-dimensional dynamical system. The applications include the causality of the energy transfer in the turbulent cascade, subgrid-scale modeling for large-eddy simulation, and flow control for drag reduction in wall-bounded turbulence.},
	language = {en},
	urldate = {2023-03-02},
	publisher = {arXiv},
	author = {Lozano-Durán, Adrián and Arranz, Gonzalo},
	month = may,
	year = {2022},
	note = {arXiv:2111.09484 [nlin, physics:physics]},
	keywords = {Computer Science - Information Theory, Mathematics - Dynamical Systems, Nonlinear Sciences - Chaotic Dynamics, Physics - Fluid Dynamics},
}

@book{hastie_elements_2009,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {The {Elements} of {Statistical} {Learning}},
	isbn = {978-0-387-84857-0 978-0-387-84858-7},
	publisher = {Springer New York},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2009},
	doi = {10.1007/978-0-387-84858-7},
	keywords = {Machine learning},
}

@article{cash_review_2003,
	title = {Review {Paper}. {Efficient} {Numerical} {Methods} for the {Solution} of {Stiff} {Initial}-{Value} {Problems} and {Differential} {Algebraic} {Equations}},
	volume = {459},
	issn = {1364-5021},
	url = {https://www.jstor.org/stable/3560007},
	abstract = {In recent years, after a prolonged period of intense activity, the study of numerical methods for solving stiff initial-value problems for ordinary differential equations and differential algebraic equations has reached a certain maturity. There now exist some excellent codes which are both efficient and reliable for solving these particular classes of problems. In this paper, we sketch some of the main theory which underpins stiff integration methods and we use this to describe, and put into context, some of the best codes currently available. By referencing only codes which have been thoroughly tested and are widely available, our aim is to direct users of numerical software to those codes which they should try initially if faced with the problem of solving ordinary differential equations of this type. An additional feature is that the codes which we propose serve as benchmarks against which any new codes can be evaluated.},
	number = {2032},
	urldate = {2023-02-28},
	journal = {Proceedings: Mathematical, Physical and Engineering Sciences},
	author = {Cash, J. R.},
	year = {2003},
	note = {Publisher: The Royal Society},
	pages = {797--815},
}

@article{xiao_feasibility_2022,
	title = {Feasibility {Study} of {Neural} {ODE} and {DAE} {Modules} for {Power} {System} {Dynamic} {Component} {Modeling}},
	issn = {0885-8950, 1558-0679},
	doi = {10.1109/TPWRS.2022.3194570},
	journal = {IEEE Transactions on Power Systems},
	author = {Xiao, Tannan and Chen, Ying and Huang, Shaowei and He, Tirui and Guan, Huizhe},
	year = {2022},
	pages = {1--13},
}

@article{heinlein_combining_2021,
	title = {Combining machine learning and domain decomposition methods for the solution of partial differential equations—{A} review},
	volume = {44},
	issn = {0936-7195, 1522-2608},
	doi = {10.1002/gamm.202100001},
	language = {en},
	number = {1},
	journal = {GAMM-Mitteilungen},
	author = {Heinlein, Alexander and Klawonn, Axel and Lanser, Martin and Weber, Janine},
	month = mar,
	year = {2021},
}

@article{liu_solving_2020,
	title = {Solving {Power} {System} {Differential} {Algebraic} {Equations} {Using} {Differential} {Transformation}},
	volume = {35},
	issn = {1558-0679},
	doi = {10.1109/TPWRS.2019.2945512},
	abstract = {This paper proposes a novel non-iterative method to solve power system differential algebraic equations (DAEs) using the differential transformation, which is a mathematical tool able to obtain power series coefficients by transformation rules instead of calculating high order derivatives and has proved to be effective in solving state variables of nonlinear differential equations in our previous study. This paper further solves non-state variables, e.g., current injections and bus voltages, directly with a realistic DAE model of power grids. These non-state variables, nonlinearly coupled in network equations, are conventionally solved by numerical methods with time-consuming iterations, but their differential transformations are proved to satisfy formally linear equations in this paper. Thus, a non-iterative algorithm is designed to analytically solve all variables of a power system DAE model with ZIP loads. From test results on a Polish 2383-bus system, the proposed method demonstrates fast and reliable time performance compared to traditional numerical approaches including the implicit trapezoidal rule method and a partitioned scheme using the explicit modified Euler method and Newton Raphson method.},
	number = {3},
	journal = {IEEE Transactions on Power Systems},
	author = {Liu, Yang and Sun, Kai},
	month = may,
	year = {2020},
	keywords = {Computational modeling, Differential algebraic equations, Differential equations, Load modeling, Mathematical model, Numerical models, Power system stability, differential transformation, numerical integration, power system simulation, time domain simulation, transient stability},
	pages = {2289--2299},
}

@article{wang_timepower_2019,
	title = {A {Time}–{Power} {Series}-{Based} {Semi}-{Analytical} {Approach} for {Power} {System} {Simulation}},
	volume = {34},
	issn = {0885-8950, 1558-0679},
	doi = {10.1109/TPWRS.2018.2871425},
	number = {2},
	journal = {IEEE Transactions on Power Systems},
	author = {Wang, Bin and Duan, Nan and Sun, Kai},
	month = mar,
	year = {2019},
	pages = {841--851},
}

@article{dinesha_application_2019,
	title = {Application of {Multi}-{Stage} {Homotopy} {Analysis} {Method} for {Power} {System} {Dynamic} {Simulations}},
	volume = {34},
	issn = {0885-8950, 1558-0679},
	doi = {10.1109/TPWRS.2018.2880605},
	number = {3},
	journal = {IEEE Transactions on Power Systems},
	author = {Dinesha, Disha Lagadamane and Gurrala, Gurunath},
	month = may,
	year = {2019},
	pages = {2251--2260},
}

@article{gurrala_large_2017,
	title = {Large {Multi}-{Machine} {Power} {System} {Simulations} {Using} {Multi}-{Stage} {Adomian} {Decomposition}},
	volume = {32},
	issn = {0885-8950, 1558-0679},
	doi = {10.1109/TPWRS.2017.2655300},
	number = {5},
	journal = {IEEE Transactions on Power Systems},
	author = {Gurrala, Gurunath and Dinesha, Disha Lagadamane and Dimitrovski, Aleksandar and Sreekanth, Pannala and Simunovic, Srdjan and Starke, Michael},
	month = sep,
	year = {2017},
	pages = {3594--3606},
}

@article{ameya_extended_2020,
	title = {Extended {Physics}-{Informed} {Neural} {Networks} ({XPINNs}): {A} {Generalized} {Space}-{Time} {Domain} {Decomposition} {Based} {Deep} {Learning} {Framework} for {Nonlinear} {Partial} {Differential} {Equations}},
	volume = {28},
	issn = {1815-2406, 1991-7120},
	shorttitle = {Extended {Physics}-{Informed} {Neural} {Networks} ({XPINNs})},
	doi = {10.4208/cicp.OA-2020-0164},
	number = {5},
	journal = {Communications in Computational Physics},
	author = {Ameya, D. Jagtap and Karniadakis, George Em},
	month = jun,
	year = {2020},
	pages = {2002--2041},
}

@article{lu_deepxde_2021,
	title = {{DeepXDE}: {A} {Deep} {Learning} {Library} for {Solving} {Differential} {Equations}},
	volume = {63},
	issn = {0036-1445, 1095-7200},
	shorttitle = {{DeepXDE}},
	url = {https://epubs.siam.org/doi/10.1137/19M1274067},
	doi = {10.1137/19M1274067},
	language = {en},
	number = {1},
	urldate = {2023-02-27},
	journal = {SIAM Review},
	author = {Lu, Lu and Meng, Xuhui and Mao, Zhiping and Karniadakis, George Em},
	month = jan,
	year = {2021},
	pages = {208--228},
}

@book{gelman_bayesian_2014,
	address = {Boca Raton},
	edition = {Third edition},
	series = {Chapman \& {Hall}/{CRC} texts in statistical science},
	title = {Bayesian data analysis},
	isbn = {978-1-4398-4095-5},
	abstract = {"Preface This book is intended to have three roles and to serve three associated audiences: an introductory text on Bayesian inference starting from first principles, a graduate text on effective current approaches to Bayesian modeling and computation in statistics and related fields, and a handbook of Bayesian methods in applied statistics for general users of and researchers in applied statistics. Although introductory in its early sections, the book is definitely not elementary in the sense of a first text in statistics. The mathematics used in our book is basic probability and statistics, elementary calculus, and linear algebra. A review of probability notation is given in Chapter 1 along with a more detailed list of topics assumed to have been studied. The practical orientation of the book means that the reader's previous experience in probability, statistics, and linear algebra should ideally have included strong computational components. To write an introductory text alone would leave many readers with only a taste of the conceptual elements but no guidance for venturing into genuine practical applications, beyond those where Bayesian methods agree essentially with standard non-Bayesian analyses. On the other hand, we feel it would be a mistake to present the advanced methods without first introducing the basic concepts from our data-analytic perspective. Furthermore, due to the nature of applied statistics, a text on current Bayesian methodology would be incomplete without a variety of worked examples drawn from real applications. To avoid cluttering the main narrative, there are bibliographic notes at the end of each chapter and references at the end of the book"--},
	publisher = {CRC Press},
	author = {Gelman, Andrew},
	year = {2014},
	keywords = {Bayesian statistical decision theory, MATHEMATICS / Probability \& Statistics / General},
}

@unpublished{noauthor_quadrature_nodate,
	title = {Quadrature notes},
}

@unpublished{frank_numerical_2008,
	title = {Numerical {Modelling} of {Dynamical} {Systems} {Course}},
	url = {https://webspace.science.uu.nl/~frank011/Classes/numwisk/ch7.pdf},
	urldate = {2023-02-22},
	author = {Frank, Jason},
	year = {2008},
}

@article{takeda_neuralnet_2000,
	title = {Neuralnet {Collocation} {Method} for {Solving} {Diﬀerential} {Equations}},
	abstract = {Collocation method for solving diﬀerential equations by using a multilayer neural network is described. Possible applications of the method are investigated by considering the distinctive features of the method. Data assimillation is one of promising application ﬁelds of this method.},
	language = {en},
	author = {Takeda, Tatsuoki and Fukuhara, Makoto and Liaqat, Ali},
	year = {2000},
}

@inproceedings{belkin_overfitting_2018,
	series = {{NIPS}'18},
	title = {Overfitting or perfect fitting? {Risk} bounds for classification and regression rules that interpolate},
	abstract = {Many modern machine learning models are trained to achieve zero or near-zero training error in order to obtain near-optimal (but non-zero) test error. This phenomenon of strong generalization performance for “overﬁtted” / interpolated classiﬁers appears to be ubiquitous in high-dimensional data, having been observed in deep networks, kernel machines, boosting and random forests. Their performance is consistently robust even when the data contain large amounts of label noise.},
	language = {en},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Belkin, Mikhail and Hsu, Daniel J and Mitra, Partha},
	year = {2018},
	pages = {2306--2317},
}

@article{loscalzo_spline_1967,
	title = {Spline {Function} {Approximations} for {Solutions} of {Ordinary} {Differential} {Equations}},
	volume = {4},
	issn = {0036-1429},
	url = {https://www.jstor.org/stable/2949410},
	abstract = {A procedure for obtaining spline function approximations for solutions of the initial value problem in ordinary differential equations is presented. The proposed method with quadratic and cubic splines is shown to be related to the well-known trapezoidal rule and Milne-Simpson method, respectively. The method is shown to be divergent, however, when higher degree spline functions are used.},
	number = {3},
	urldate = {2023-02-22},
	journal = {SIAM Journal on Numerical Analysis},
	author = {Loscalzo, Frank R. and Talbot, Thomas D.},
	year = {1967},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {433--445},
}

@misc{yang_physics-informed_2018,
	title = {Physics-informed deep generative models},
	url = {http://arxiv.org/abs/1812.03511},
	abstract = {We consider the application of deep generative models in propagating uncertainty through complex physical systems. Specifically, we put forth an implicit variational inference formulation that constrains the generative model output to satisfy given physical laws expressed by partial differential equations. Such physics-informed constraints provide a regularization mechanism for effectively training deep probabilistic models for modeling physical systems in which the cost of data acquisition is high and training data-sets are typically small. This provides a scalable framework for characterizing uncertainty in the outputs of physical systems due to randomness in their inputs or noise in their observations. We demonstrate the effectiveness of our approach through a canonical example in transport dynamics.},
	urldate = {2023-02-22},
	publisher = {arXiv},
	author = {Yang, Yibo and Perdikaris, Paris},
	month = dec,
	year = {2018},
	note = {arXiv:1812.03511 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{brink_neural_2021,
	title = {The neural network collocation method for solving partial differential equations},
	volume = {33},
	issn = {0941-0643, 1433-3058},
	url = {https://link.springer.com/10.1007/s00521-020-05340-5},
	doi = {10.1007/s00521-020-05340-5},
	language = {en},
	number = {11},
	urldate = {2023-02-21},
	journal = {Neural Computing and Applications},
	author = {Brink, Adam R. and Najera-Flores, David A. and Martinez, Cari},
	month = jun,
	year = {2021},
	pages = {5591--5608},
}

@article{van_milligen_neural_1995,
	title = {Neural {Network} {Differential} {Equation} and {Plasma} {Equilibrium} {Solver}},
	volume = {75},
	issn = {0031-9007, 1079-7114},
	doi = {10.1103/PhysRevLett.75.3594},
	language = {en},
	number = {20},
	journal = {Physical Review Letters},
	author = {van Milligen, B. Ph. and Tribaldos, V. and Jiménez, J. A.},
	month = nov,
	year = {1995},
	pages = {3594--3597},
}

@book{sauer_power_1998,
	address = {Upper Saddle River, N.J},
	title = {Power system dynamics and stability},
	isbn = {978-0-13-678830-0},
	language = {eng},
	publisher = {Prentice Hall},
	author = {Sauer, Peter W. and Pai, M. A.},
	year = {1998},
	keywords = {ODE},
}

@inproceedings{stiasny_closing_2022,
	address = {Banff, Canada},
	title = {Closing the {Loop}: {A} {Framework} for {Trustworthy} {Machine} {Learning} in {Power} {Systems}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Closing the {Loop}},
	doi = {10.48550/ARXIV.2203.07505},
	abstract = {Deep decarbonization of the energy sector will require massive penetration of stochastic renewable energy resources and an enormous amount of grid asset coordination; this represents a challenging paradigm for the power system operators who are tasked with maintaining grid stability and security in the face of such changes. With its ability to learn from complex datasets and provide predictive solutions on fast timescales, machine learning (ML) is well-posed to help overcome these challenges as power systems transform in the coming decades. In this work, we outline five key challenges (dataset generation, data pre-processing, model training, model assessment, and model embedding) associated with building trustworthy ML models which learn from physics-based simulation data. We then demonstrate how linking together individual modules, each of which overcomes a respective challenge, at sequential stages in the machine learning pipeline can help enhance the overall performance of the training process. In particular, we implement methods that connect different elements of the learning pipeline through feedback, thus "closing the loop" between model training, performance assessments, and re-training. We demonstrate the effectiveness of this framework, its constituent modules, and its feedback connections by learning the N-1 small-signal stability margin associated with a detailed model of a proposed North Sea Wind Power Hub system.},
	booktitle = {Proceedings of the 11th {Bulk} {Power} {Systems} {Dynamics} and {Control} {Symposium} ({IREP} 2022)},
	author = {Stiasny, Jochen and Chevalier, Samuel and Nellikkath, Rahul and Sævarsson, Brynjar and Chatzivasileiadis, Spyros},
	year = {2022},
	keywords = {FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Machine Learning (cs.LG), Systems and Control (eess.SY)},
	pages = {1--21},
}

@book{brenan_numerical_1995,
	title = {Numerical {Solution} of {Initial}-{Value} {Problems} in {Differential}-{Algebraic} {Equations}},
	isbn = {978-0-89871-353-4 978-1-61197-122-4},
	language = {en},
	urldate = {2022-08-19},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Brenan, K. E. and Campbell, S. L. and Petzold, L. R.},
	month = jan,
	year = {1995},
	doi = {10.1137/1.9781611971224},
}

@article{cybenko_approximation_1989,
	title = {Approximation by superpositions of a sigmoidal function},
	volume = {2},
	issn = {0932-4194, 1435-568X},
	doi = {10.1007/BF02551274},
	language = {en},
	number = {4},
	urldate = {2021-06-25},
	journal = {Mathematics of Control, Signals, and Systems},
	author = {Cybenko, G.},
	month = dec,
	year = {1989},
	pages = {303--314},
}

@article{andersson_assimulo_2015,
	title = {Assimulo: {A} unified framework for {ODE} solvers},
	volume = {116},
	issn = {03784754},
	shorttitle = {Assimulo},
	doi = {10.1016/j.matcom.2015.04.007},
	language = {en},
	urldate = {2022-08-19},
	journal = {Mathematics and Computers in Simulation},
	author = {Andersson, Christian and Führer, Claus and Åkesson, Johan},
	month = oct,
	year = {2015},
	pages = {26--43},
}

@article{baydin_automatic_2018,
	title = {Automatic {Differentiation} in {Machine} {Learning}: a {Survey}},
	volume = {18},
	issn = {1533-7928},
	shorttitle = {Automatic {Differentiation} in {Machine} {Learning}},
	number = {153},
	journal = {Journal of Machine Learning Research},
	author = {Baydin, Atilim Gunes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
	year = {2018},
	keywords = {Machine learning},
	pages = {1--43},
}

@article{zimmerman_matpower_2011,
	title = {{MATPOWER}: {Steady}-{State} {Operations}, {Planning}, and {Analysis} {Tools} for {Power} {Systems} {Research} and {Education}},
	volume = {26},
	issn = {0885-8950, 1558-0679},
	shorttitle = {{MATPOWER}},
	doi = {10.1109/TPWRS.2010.2051168},
	number = {1},
	journal = {IEEE Transactions on Power Systems},
	author = {Zimmerman, Ray Daniel and Murillo-Sanchez, Carlos Edmundo and Thomas, Robert John},
	month = feb,
	year = {2011},
	pages = {12--19},
}

@article{karniadakis_physics-informed_2021,
	title = {Physics-informed machine learning},
	volume = {3},
	issn = {2522-5820},
	doi = {10.1038/s42254-021-00314-5},
	language = {en},
	number = {6},
	journal = {Nature Reviews Physics},
	author = {Karniadakis, George Em and Kevrekidis, Ioannis G. and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
	month = jun,
	year = {2021},
	pages = {422--440},
}

@article{stott_power_1979,
	title = {Power system dynamic response calculations},
	volume = {67},
	issn = {1558-2256},
	doi = {10.1109/PROC.1979.11233},
	abstract = {Engineers in the power industry, face the problem that, while stability is increasingly a limiting factor in secure system operation, the simulation of system dynamic response is grossly overburdening on present-day digital computing resources. Each individual response case involves the step-by-step numerical solution in the time domain of perhaps thousands of nonlinear differential-algebraic equations, at a cost of up to several thousand dollars. A high premium is thus to be placed on the use of the most efficient and reliable modern calculation techniques. This paper is a critical tutorial-review of the calculation methods used routinely or investigated for use by the industry. It concentrates on solution concepts and computational techniques rather than on the analysis of the numerical methods. Details of system modeling are only emphasized when they affect the choice of solution method. The paper concludes with a view of the state of the art and a prediction of future directions of development.},
	number = {2},
	journal = {Proceedings of the IEEE},
	author = {Stott, B.},
	month = feb,
	year = {1979},
	keywords = {Computational modeling, Differential equations, Nonlinear dynamical systems, Nonlinear equations, Power engineering and energy, Power engineering computing, Power industry, Power system dynamics, Power system simulation, Power system stability},
	pages = {219--241},
}

@phdthesis{aristidou_time-domain_2015,
	address = {Liège, Belgium},
	title = {Time-domain simulation of large electric power systems using domain-decomposition and parallel processing methods},
	language = {en},
	urldate = {2022-02-05},
	school = {Université de Liège},
	author = {Aristidou, Petros},
	year = {2015},
}

@article{gurrala_parareal_2016,
	title = {Parareal in {Time} for {Fast} {Power} {System} {Dynamic} {Simulations}},
	volume = {31},
	issn = {0885-8950, 1558-0679},
	doi = {10.1109/TPWRS.2015.2434833},
	number = {3},
	journal = {IEEE Transactions on Power Systems},
	author = {Gurrala, Gurunath and Dimitrovski, Aleksandar and Pannala, Sreekanth and Simunovic, Srdjan and Starke, Michael},
	month = may,
	year = {2016},
	pages = {1820--1830},
}

@article{wolpert_no_1997,
	title = {No free lunch theorems for optimization},
	volume = {1},
	issn = {1089778X},
	url = {http://ieeexplore.ieee.org/document/585893/},
	doi = {10.1109/4235.585893},
	number = {1},
	urldate = {2023-02-07},
	journal = {IEEE Transactions on Evolutionary Computation},
	author = {Wolpert, D.H. and Macready, W.G.},
	month = apr,
	year = {1997},
	pages = {67--82},
}

@book{wijckmans_differential_1992,
	address = {Eindhoven},
	series = {{RANA} : reports on applied and numerical analysis},
	title = {Differential algebraic equations},
	publisher = {Eindhoven University of Technology},
	author = {Wijckmans, P.M.E.J.},
	year = {1992},
}

@inproceedings{djeridane_learning_2007,
	address = {Kos},
	title = {A learning theory approach to the computation of reachable sets},
	isbn = {978-3-9524173-8-6},
	url = {https://ieeexplore.ieee.org/document/7068964/},
	doi = {10.23919/ECC.2007.7068964},
	urldate = {2023-02-01},
	booktitle = {2007 {European} {Control} {Conference} ({ECC})},
	publisher = {IEEE},
	author = {Djeridane, Badis and Cruck, Eva and Lygeros, John},
	month = jul,
	year = {2007},
	pages = {2663--2670},
}

@article{filici_neural_2008,
	title = {On a {Neural} {Approximator} to {ODEs}},
	volume = {19},
	issn = {1045-9227},
	url = {http://ieeexplore.ieee.org/document/4459105/},
	doi = {10.1109/TNN.2007.915109},
	number = {3},
	urldate = {2023-02-01},
	journal = {IEEE Transactions on Neural Networks},
	author = {Filici, Cristian},
	month = mar,
	year = {2008},
	pages = {539--543},
}

@inproceedings{fojdl_performance_2008,
	address = {Dayton, OH, USA},
	title = {The {Performance} of {Approximating} {Ordinary} {Differential} {Equations} by {Neural} {Nets}},
	isbn = {978-0-7695-3440-4},
	url = {http://ieeexplore.ieee.org/document/4669809/},
	doi = {10.1109/ICTAI.2008.44},
	urldate = {2023-02-01},
	booktitle = {2008 20th {IEEE} {International} {Conference} on {Tools} with {Artificial} {Intelligence}},
	publisher = {IEEE},
	author = {Fojdl, Josef and Brause, Rüdiger W.},
	month = nov,
	year = {2008},
	pages = {457--464},
}

@book{warner_foundations_1983,
	address = {New York, NY},
	series = {Graduate {Texts} in {Mathematics}},
	title = {Foundations of {Differentiable} {Manifolds} and {Lie} {Groups}},
	volume = {94},
	isbn = {978-1-4419-2820-7 978-1-4757-1799-0},
	url = {http://link.springer.com/10.1007/978-1-4757-1799-0},
	urldate = {2023-02-01},
	publisher = {Springer New York},
	author = {Warner, Frank W.},
	year = {1983},
	doi = {10.1007/978-1-4757-1799-0},
}

@article{li_integrating_nodate,
	title = {Integrating {Learning} and {Physics} based {Computation} for {Fast} {Online} {Transient} {Analysis}},
	abstract = {A novel method that integrates learning and physics based computation is developed for greatly accelerating the simulation of full power system transient trajectories. To solve the dynamic algebraic equations, the method replaces the timeconsuming dynamic computation for generator dynamics with trained predictors, while retaining the time-efficient algebraic computation of solving AC-power flow (PF) for power systems. In particular, a predictor is trained for each generator, and the system trajectories are computed by alternating steps of calling the predictors and solving AC-PF. The proposed method also allows fully parallelizable training strategies and a flexible tradeoff between training time and testing accuracy. Comprehensive evaluations of the proposed method for transient/dynamic contingency analysis of the New York/New England 16-machine 68-bus power systems demonstrate excellent performance and significant acceleration of computation.},
	language = {en},
	author = {Li, Jiaming and Zhao, Yue and Yue, Meng},
}

@book{phillips_how_2010,
	address = {Maidenhead},
	edition = {4. ed., rev. and updated, repr},
	title = {How to get a {PhD}: a handbook for students and their supervisors},
	isbn = {978-0-335-21684-0},
	shorttitle = {How to get a {PhD}},
	language = {en},
	publisher = {Open University Press},
	author = {Phillips, Estelle M. and Pugh, Derek Salman},
	year = {2010},
}

@article{zhang_solving_2020,
	title = {Solving {Ordinary} {Differential} {Equations} {With} {Adaptive} {Differential} {Evolution}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9139206/},
	doi = {10.1109/ACCESS.2020.3008823},
	urldate = {2023-01-31},
	journal = {IEEE Access},
	author = {Zhang, Zijia and Cai, Yaoming and Zhang, Dongfang},
	year = {2020},
	pages = {128908--128922},
}

@article{von_rueden_informed_2023,
	title = {Informed {Machine} {Learning} – {A} {Taxonomy} and {Survey} of {Integrating} {Prior} {Knowledge} into {Learning} {Systems}},
	volume = {35},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2021.3079836},
	abstract = {Despite its great success, machine learning can have its limits when dealing with insufficient training data. A potential solution is the additional integration of prior knowledge into the training process which leads to the notion of informed machine learning. In this paper, we present a structured overview of various approaches in this field. We provide a definition and propose a concept for informed machine learning which illustrates its building blocks and distinguishes it from conventional machine learning. We introduce a taxonomy that serves as a classification framework for informed machine learning approaches. It considers the source of knowledge, its representation, and its integration into the machine learning pipeline. Based on this taxonomy, we survey related research and describe how different knowledge representations such as algebraic equations, logic rules, or simulation results can be used in learning systems. This evaluation of numerous papers on the basis of our taxonomy uncovers key methods in the field of informed machine learning.},
	number = {1},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {von Rueden, Laura and Mayer, Sebastian and Beckh, Katharina and Georgiev, Bogdan and Giesselbach, Sven and Heese, Raoul and Kirsch, Birgit and Pfrommer, Julius and Pick, Annika and Ramamurthy, Rajkumar and Walczak, Michal and Garcke, Jochen and Bauckhage, Christian and Schuecker, Jannis},
	month = jan,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Machine learning, Mathematical model, Pipelines, Systematics, Taxonomy, Training, Training data, expert knowledge, hybrid, informed, neuro-symbolic, prior knowledge, survey, taxonomy},
	pages = {614--633},
}

@article{malek_numerical_2006,
	title = {Numerical solution for high order differential equations using a hybrid neural network—{Optimization} method},
	volume = {183},
	issn = {00963003},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0096300306005583},
	doi = {10.1016/j.amc.2006.05.068},
	language = {en},
	number = {1},
	urldate = {2023-01-31},
	journal = {Applied Mathematics and Computation},
	author = {Malek, A. and Shekari Beidokhti, R.},
	month = dec,
	year = {2006},
	pages = {260--271},
}

@misc{frank_numerical_nodate,
	title = {Numerical {Modelling} of {Dynamical} {Systems}},
	url = {https://webspace.science.uu.nl/~frank011/Classes/numwisk/},
	urldate = {2023-01-25},
	author = {Frank, Jason},
}

@book{hairer_solving_1996,
	address = {Berlin, Heidelberg},
	series = {Springer {Series} in {Computational} {Mathematics}},
	title = {Solving {Ordinary} {Differential} {Equations} {II}},
	volume = {14},
	isbn = {978-3-642-05220-0 978-3-642-05221-7},
	url = {http://link.springer.com/10.1007/978-3-642-05221-7},
	urldate = {2023-01-24},
	publisher = {Springer Berlin Heidelberg},
	author = {Hairer, Ernst and Wanner, Gerhard},
	year = {1996},
	doi = {10.1007/978-3-642-05221-7},
}

@article{higueras_rungekutta_1999,
	title = {Runge–{Kutta} methods for {DAEs}. {A} new approach},
	volume = {111},
	issn = {03770427},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0377042799001314},
	doi = {10.1016/S0377-0427(99)00131-4},
	language = {en},
	number = {1-2},
	urldate = {2023-01-20},
	journal = {Journal of Computational and Applied Mathematics},
	author = {Higueras, Inmaculada and Garcı́a-Celayeta, Berta},
	month = nov,
	year = {1999},
	pages = {49--61},
}

@book{eich-soellner_numerical_1998,
	address = {Wiesbaden},
	series = {European {Consortium} for {Mathematics} in {Industry}},
	title = {Numerical {Methods} in {Multibody} {Dynamics}},
	isbn = {978-3-663-09830-0 978-3-663-09828-7},
	url = {http://link.springer.com/10.1007/978-3-663-09828-7},
	urldate = {2023-01-19},
	publisher = {Vieweg+Teubner Verlag},
	author = {Eich-Soellner, Edda and Führer, Claus},
	editor = {Arkeryd, Leif and Engl, Heinz and Fasano, Antonio and Mattheij, Robert M. M. and Neittaanmäki, Pekka and Neunzert, Helmut},
	year = {1998},
	doi = {10.1007/978-3-663-09828-7},
}

@article{bendtsen_numerical_nodate,
	title = {Numerical {Solution} of {Differential} {Algebraic} {Equations}},
	language = {en},
	author = {Bendtsen, Claus and Thomsen, Per Grove},
}

@article{jalili-marandi_instantaneous_2009,
	title = {Instantaneous {Relaxation}-{Based} {Real}-{Time} {Transient} {Stability} {Simulation}},
	volume = {24},
	issn = {0885-8950, 1558-0679},
	url = {http://ieeexplore.ieee.org/document/4912363/},
	doi = {10.1109/TPWRS.2009.2021210},
	number = {3},
	urldate = {2023-01-18},
	journal = {IEEE Transactions on Power Systems},
	author = {Jalili-Marandi, V. and Dinavahi, V.},
	month = aug,
	year = {2009},
	pages = {1327--1336},
}

@article{liu_two-stage_2016,
	title = {Two-{Stage} {Parallel} {Waveform} {Relaxation} {Method} for {Large}-{Scale} {Power} {System} {Transient} {Stability} {Simulation}},
	volume = {31},
	issn = {0885-8950, 1558-0679},
	url = {http://ieeexplore.ieee.org/document/7035124/},
	doi = {10.1109/TPWRS.2015.2388856},
	number = {1},
	urldate = {2023-01-13},
	journal = {IEEE Transactions on Power Systems},
	author = {Liu, Yunfei and Jiang, Quanyuan},
	month = jan,
	year = {2016},
	pages = {153--162},
}

@article{li_d3m_2020,
	title = {{D3M}: {A} {Deep} {Domain} {Decomposition} {Method} for {Partial} {Differential} {Equations}},
	volume = {8},
	issn = {2169-3536},
	shorttitle = {{D3M}},
	url = {https://ieeexplore.ieee.org/document/8918421/},
	doi = {10.1109/ACCESS.2019.2957200},
	urldate = {2023-01-13},
	journal = {IEEE Access},
	author = {Li, Ke and Tang, Kejun and Wu, Tianfan and Liao, Qifeng},
	year = {2020},
	pages = {5283--5294},
}

@inproceedings{li_deep_2020,
	title = {Deep {Domain} {Decomposition} {Method}: {Elliptic} {Problems}},
	shorttitle = {Deep {Domain} {Decomposition} {Method}},
	url = {https://proceedings.mlr.press/v107/li20a.html},
	abstract = {This paper proposes a deep-learning-based domain decomposition method (DeepDDM), which leverages deep neural networks (DNN) to discretize the subproblems divided by domain decomposition methods (DDM) for solving partial differential equations (PDE).  Using DNN to solve PDE is a physics-informed learning problem with the objective involving two terms, domain term and boundary term, which respectively make the desired solution satisfy the PDE and corresponding boundary conditions. DeepDDM will exchange the subproblem information across the interface in DDM by adjusting the boundary term for solving each subproblem by DNN. Benefiting from the simple implementation and mesh-free strategy of using DNN for PDE, DeepDDM will simplify the implementation of DDM and make DDM more flexible for complex PDE, e.g., those with complex interfaces in the computational domain. This paper will firstly investigate the performance of using DeepDDM for elliptic problems, including a model problem and an interface problem.  The numerical examples demonstrate that DeepDDM exhibits behaviors consistent with conventional DDM: the number of iterations by DeepDDM is independent of network architecture and decreases with increasing overlapping size.  The performance of DeepDDM on elliptic problems will encourage us to further investigate its performance for other kinds of PDE and may provide new insights for improving the PDE solver by deep learning.},
	language = {en},
	urldate = {2023-01-13},
	booktitle = {Proceedings of {The} {First} {Mathematical} and {Scientific} {Machine} {Learning} {Conference}},
	publisher = {PMLR},
	author = {Li, Wuyang and Xiang, Xueshuang and Xu, Yingxiang},
	month = aug,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {269--286},
}

@book{toselli_domain_2005,
	address = {Berlin, Heidelberg},
	series = {Springer {Series} in {Computational} {Mathematics}},
	title = {Domain {Decomposition} {Methods} — {Algorithms} and {Theory}},
	volume = {34},
	isbn = {978-3-540-20696-5 978-3-540-26662-4},
	url = {http://link.springer.com/10.1007/b137868},
	language = {en},
	urldate = {2023-01-13},
	publisher = {Springer Berlin Heidelberg},
	author = {Toselli, Andrea and Widlund, Olof B.},
	year = {2005},
	doi = {10.1007/b137868},
}

@book{pavella_transient_2000,
	address = {Boston, MA},
	title = {Transient {Stability} of {Power} {Systems}},
	isbn = {978-1-4613-6939-4 978-1-4615-4319-0},
	url = {http://link.springer.com/10.1007/978-1-4615-4319-0},
	language = {en},
	urldate = {2023-01-10},
	publisher = {Springer US},
	author = {Pavella, Mania and Ernst, Damien and Ruiz-Vega, Daniel},
	year = {2000},
	doi = {10.1007/978-1-4615-4319-0},
}

@inproceedings{perozzi_deepwalk_2014,
	title = {{DeepWalk}: {Online} {Learning} of {Social} {Representations}},
	shorttitle = {{DeepWalk}},
	url = {http://arxiv.org/abs/1403.6652},
	doi = {10.1145/2623330.2623732},
	abstract = {We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs.},
	language = {en},
	urldate = {2023-01-06},
	booktitle = {Proceedings of the 20th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	author = {Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
	month = aug,
	year = {2014},
	note = {arXiv:1403.6652 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks, H.2.8, I.2.6, I.5.1},
	pages = {701--710},
}

@article{seventikidou_function_nodate,
	title = {Function {Extrapolation} through {Differential} {Equation} {Learning}},
	language = {en},
	author = {Seventikidou, Christina},
}

@article{liu_power_2019,
	title = {Power {System} {Time} {Domain} {Simulation} {Using} a {Differential} {Transformation} {Method}},
	volume = {34},
	issn = {1558-0679},
	doi = {10.1109/TPWRS.2019.2901654},
	abstract = {This paper proposes a novel approach for power system dynamic simulation based on the differential transformation (DT). The DT is introduced to study power systems as high-dimensional nonlinear dynamical systems for the first time, and is able to avoid computations of high-order derivatives with nonlinear differential equations by its transform rules. This paper, first, proposes and proves several new transform rules for generic nonlinear functions that often appear in power system models, and then uses these rules to transform representative power system models such as the synchronous machine model with trigonometric functions and the exciter model with exponential and square root functions. This paper also designs a DT-based simulation scheme that allows significantly prolonged time steps to reduce simulation time compared to a traditional numerical approach. The numerical stability, accuracy, and time performance of the proposed new DT-based simulation approach are compared with widely used numerical methods on the IEEE 39-bus system and Polish 2383-bus system.},
	number = {5},
	journal = {IEEE Transactions on Power Systems},
	author = {Liu, Yang and Sun, Kai and Yao, Rui and Wang, Bin},
	month = sep,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Power Systems},
	keywords = {Adaptation models, Computational modeling, Differential transformation method, Mathematical model, Numerical models, Power system dynamics, Power system stability, Transforms, numerical integration, power system simulation, transient stability},
	pages = {3739--3748},
}

@article{liu_dynamized_2020,
	title = {A {Dynamized} {Power} {Flow} {Method} {Based} on {Differential} {Transformation}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3028060},
	abstract = {This paper proposes a novel method for solving and tracing power flow solutions with changes of a loading parameter. Different from the conventional continuation power flow method, which repeatedly solves static AC power flow equations, the proposed method extends the power flow model into a fictitious dynamic system by adding a differential equation on the loading parameter. As a result, the original solution curve tracing problem is converted to solving the time domain trajectories of the reformulated dynamic system. A non-iterative algorithm based on differential transformation is proposed to analytically solve the aforementioned dynamized model in form of power series of time. This paper proves that the nonlinear power flow equations in the time domain are converted to formally linear equations in the domain of the power series order after the differential transformation, thus avoiding numerical iterations. Case studies on several test systems including a 2383-bus system show the merits of the proposed method.},
	journal = {IEEE Access},
	author = {Liu, Yang and Sun, Kai and Dong, Jiaojiao},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Continuation power flow, Differential equations, Load flow, Loading, Mathematical model, Power system dynamics, Power system stability, Time-domain analysis, differential transformation, dynamized power flow, power flow, power-voltage curve, voltage collapse, voltage stability},
	pages = {182441--182450},
}

@article{elsheikh_equation-based_2015,
	title = {An equation-based algorithmic differentiation technique for differential algebraic equations},
	volume = {281},
	issn = {03770427},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0377042714005822},
	doi = {10.1016/j.cam.2014.12.026},
	language = {en},
	urldate = {2022-12-14},
	journal = {Journal of Computational and Applied Mathematics},
	author = {Elsheikh, Atiyah},
	month = jun,
	year = {2015},
	pages = {135--151},
}

@misc{margossian_efficient_2022,
	title = {Efficient {Automatic} {Differentiation} of {Implicit} {Functions}},
	url = {http://arxiv.org/abs/2112.14217},
	doi = {10.48550/arXiv.2112.14217},
	abstract = {Derivative-based algorithms are ubiquitous in statistics, machine learning, and applied mathematics. Automatic differentiation offers an algorithmic way to efficiently evaluate these derivatives from computer programs that execute relevant functions. Implementing automatic differentiation for programs that incorporate implicit functions, such as the solution to an algebraic or differential equation, however, requires particular care. Contemporary applications typically appeal to either the application of the implicit function theorem or, in certain circumstances, specialized adjoint methods. In this paper we show that both of these approaches can be generalized to any implicit function, although the generalized adjoint method is typically more effective for automatic differentiation. To showcase the relative advantages and limitations of the two methods we demonstrate their application on a suite of common implicit functions.},
	urldate = {2022-12-14},
	publisher = {arXiv},
	author = {Margossian, Charles C. and Betancourt, Michael},
	month = feb,
	year = {2022},
	note = {arXiv:2112.14217 [stat]},
	keywords = {Statistics - Computation},
}

@article{jerosolimski_new_1994,
	title = {A new method for fast calculation of {Jacobian} matrices: automatic differentiation for power system simulation},
	volume = {9},
	issn = {1558-0679},
	shorttitle = {A new method for fast calculation of {Jacobian} matrices},
	doi = {10.1109/59.317672},
	abstract = {Many numerical methods used in power system simulation require the computation of Jacobian matrices. This being particularly true for implicit integration algorithms, and not for explicit ones. These computations often take a significant proportion of the overall CPU time. This paper presents an application of the automatic differentiation method which results in large savings in the computation of Jacobian matrices. An original application of this method is in a software which simulates power systems dynamics. As the program enables the users to introduce their own models, automatic differentiation becomes particularly efficient. In comparison with numerical differentiation, it leads to a saving of 80\% of the time required for the computation of the Jacobian matrices and up to 28\% of the total CPU time. Automatic differentiation is a very efficient method which should be valuable to other power system software, in particular those which offer users the possibility of defining their own models.{\textless}{\textgreater}},
	number = {2},
	journal = {IEEE Transactions on Power Systems},
	author = {Jerosolimski, M. and Levacher, L.},
	month = may,
	year = {1994},
	note = {Conference Name: IEEE Transactions on Power Systems},
	keywords = {Application software, Central Processing Unit, Computational modeling, Cost function, Jacobian matrices, Power system dynamics, Power system modeling, Power system simulation, Power systems, Software systems},
	pages = {700--706},
}

@inproceedings{geng_application_2014,
	title = {Application of automatic differentiation in power system trajectory sensitivity analysis},
	doi = {10.1109/TDC.2014.6863266},
	abstract = {Trajectory sensitivity is one of the most important analysis tools for power system dynamic performance assessment and control. Calculating Jacobian matrix has been identified as one of the most computational-intensive and error-prone algorithmic procedures. Automatic differentiation (AD) is able to efficiently generate derivatives of a specified function without any additional hand coding work. Application and improvement of AD in trajectory sensitivity is investigated in this paper. A novel partial-decoupled AD strategy is developed. The proposed approach enables a module-based design separating dynamic component models from time-domain simulator. Maintainability and flexibility in software architecture can be achieved. Numerical results demonstrate that the application of AD retains numerical accuracy and achieves high computational performance compared with numerical finite differentiation.},
	booktitle = {2014 {IEEE} {PES} {T}\&{D} {Conference} and {Exposition}},
	author = {Geng, Guangchao and Ajjarapu, Venkataramana and Jiang, Quanyuan},
	month = apr,
	year = {2014},
	note = {ISSN: 2160-8563},
	keywords = {Automatic differentiation, Equations, Jacobian matrices, Mathematical model, Power system dynamics, Power system stability, Sensitivity, Trajectory, parallel algorithm, short-term voltage stability, static VAR compensator (SVC), time-domain simulation, trajectory sensitivity, transient stability},
	pages = {1--5},
}

@article{ibsais_role_1997,
	title = {The role of automatic differentiation in power system analysis},
	volume = {12},
	issn = {1558-0679},
	doi = {10.1109/59.589610},
	abstract = {In this paper, automatic differentiation of algorithms is utilized in power system analysis. Automatic differentiation is a computational tool to obtain the derivatives and the value of the function systematically without providing explicit expressions for the derivatives. The method is reliable and produces accurate derivatives instantaneously without the need for hand-coding the derivatives. The method is applied to compute the Jacobian and sensitivities in the continuation power flow. An automatic differentiation package (ADIFOR) that exploits sparsity techniques is used for the application.},
	number = {2},
	journal = {IEEE Transactions on Power Systems},
	author = {Ibsais, A. and Ajjarapu, V.},
	month = may,
	year = {1997},
	note = {Conference Name: IEEE Transactions on Power Systems},
	keywords = {Finite difference methods, Jacobian matrices, Load flow, Packaging, Power engineering and energy, Power engineering computing, Power system analysis computing, Power system reliability, Power system stability, Power system transients},
	pages = {592--597},
}

@book{sussman_functional_2013,
	address = {Cambridge, MA},
	title = {Functional differential geometry},
	isbn = {978-0-262-01934-7},
	language = {en},
	publisher = {The MIT Press},
	author = {Sussman, Gerald Jay and Wisdom, Jack and Farr, Will},
	year = {2013},
	keywords = {Functional differential equations, Geometry, Differential, Mathematical physics},
}

@article{zadkhast_multi-decomposition_2015,
	title = {A {Multi}-{Decomposition} {Approach} for {Accelerated} {Time}-{Domain} {Simulation} of {Transient} {Stability} {Problems}},
	volume = {30},
	issn = {1558-0679},
	doi = {10.1109/TPWRS.2014.2361529},
	abstract = {Time-domain simulation (TDS) is the most accurate and reliable method for solving transient stability problem. This approach relies on the solution of nonlinear differential-algebraic equations (DAEs) using numerical integration, wherein a system of nonlinear equations is solved at each integration time step. Very DisHonest Newton (VDHN) method is one of the fastest available approaches for solving the system of nonlinear equations in the TDS. However, VDHN does not guarantee convergence and its sequential nature makes it difficult to take advantage of available multicore processors. This paper presents a new multi-decomposition approach (MDA) to achieve fast TDS, wherein the nonlinear system of DAEs is decomposed into three linear subsystems. An adaptive scheme is developed for updating the linear subsystems to ensure that approximation remains sufficiently accurate. The linear subsystems can be solved in parallel using multi-processor architecture. The MDA is demonstrated using IEEE 145-bus test system, and the results are verified against a commercial transient stability problem, Powertech Labs' DSATools.},
	number = {5},
	journal = {IEEE Transactions on Power Systems},
	author = {Zadkhast, Sajjad and Jatskevich, Juri and Vaahedi, Ebrahim},
	month = sep,
	year = {2015},
	note = {Conference Name: IEEE Transactions on Power Systems},
	keywords = {Approximation methods, Jacobian matrices, Mathematical model, Multi-dimensional Taylor series, Power system stability, Stability analysis, Transient analysis, parallel computing, time-domain simulation, transient stability},
	pages = {2301--2311},
}

@inproceedings{milano_python-based_2013,
	title = {A python-based software tool for power system analysis},
	doi = {10.1109/PESMG.2013.6672387},
	abstract = {This paper presents a power system analysis tool, called DOME, entirely based on Python scripting language as well as on public domain efficient C and Fortran libraries. The objects of the paper are twofold. First, the paper discusses the features that makes the Python language an adequate tool for research, massive numerical simulations and education. Then the paper describes the architecture of the developed software tool and provides a variety of examples to show the advanced features and the performance of the developed tool.},
	booktitle = {2013 {IEEE} {Power} \& {Energy} {Society} {General} {Meeting}},
	author = {Milano, Federico},
	month = jul,
	year = {2013},
	note = {ISSN: 1932-5517},
	keywords = {Eigenvalues and eigenfunctions, Libraries, Power system stability, Python language, Software tools, Sparse matrices, Stochastic processes, eigenvalue analysis, power flow analysis, scripting, time domain integration},
	pages = {1--5},
}

@inproceedings{donon_graph_2019,
	title = {Graph {Neural} {Solver} for {Power} {Systems}},
	doi = {10.1109/IJCNN.2019.8851855},
	abstract = {We propose a neural network architecture that emulates the behavior of a physics solver that solves electricity differential equations to compute electricity flow in power grids (so-called "load flow"). Load flow computation is a well studied and understood problem, but current methods (based on Newton-Raphson) are slow. With increasing usage expectations of the current infrastructure, it is important to find methods to accelerate computations. One avenue we are pursuing in this paper is to use proxies based on "graph neural networks". In contrast with previous neural network approaches, which could only handle fixed grid topologies, our novel graph-based method, trained on data from power grids of a given size, generalizes to larger or smaller ones. We experimentally demonstrate viability of the method on randomly connected artificial grids of size 30 nodes. We achieve better accuracy than the DC-approximation (a standard benchmark linearizing physical equations) on random power grids whose size range from 10 nodes to 110 nodes. Our neural network learns to solve the load flow problem without overfitting to a specific instance of the problem.},
	booktitle = {2019 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Donon, Balthazar and Donnot, Benjamin and Guyon, Isabelle and Marot, Antoine},
	month = jul,
	year = {2019},
	note = {ISSN: 2161-4407},
	keywords = {Computer architecture, Extremities, Graph Neural Net, Graph Neural Solver, Neural Solver, Neural networks, Power Systems, Power grids, Production, Topology, Transmission line matrix methods},
	pages = {1--8},
}

@article{liao_review_2022,
	title = {A {Review} of {Graph} {Neural} {Networks} and {Their} {Applications} in {Power} {Systems}},
	volume = {10},
	issn = {2196-5420},
	doi = {10.35833/MPCE.2021.000058},
	abstract = {Deep neural networks have revolutionized many machine learning tasks in power systems, ranging from pattern recognition to signal processing. The data in these tasks are typically represented in Euclidean domains. Nevertheless, there is an increasing number of applications in power systems, where data are collected from non-Euclidean domains and represented as graph-structured data with high-dimensional features and interdependency among nodes. The complexity of graph-structured data has brought significant challenges to the existing deep neural networks defined in Euclidean domains. Recently, many publications generalizing deep neural networks for graph-structured data in power systems have emerged. In this paper, a comprehensive overview of graph neural networks (GNNs) in power systems is proposed. Specifically, several classical paradigms of GNN structures, e. g., graph convolutional networks, are summarized. Key applications in power systems such as fault scenario application, time-series prediction, power flow calculation, and data generation are reviewed in detail. Further-more, main issues and some research trends about the applications of GNNs in power systems are discussed.},
	number = {2},
	journal = {Journal of Modern Power Systems and Clean Energy},
	author = {Liao, Wenlong and Bak-Jensen, Birgitte and Pillai, Jayakrishnan Radhakrishna and Wang, Yuelong and Wang, Yusen},
	month = mar,
	year = {2022},
	note = {Conference Name: Journal of Modern Power Systems and Clean Energy},
	keywords = {Convolution, Graph neural networks, Insulators, Load flow, Machine learning, Power systems, Symmetric matrices, Task analysis, artificial intelligence, deep neural network, graph neural network, power system},
	pages = {345--360},
}

@misc{noauthor_physics-informed_nodate,
	title = {Physics-informed geometric deep learning for inference tasks in power systems {\textbar} {Elsevier} {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S0378779622005223?token=12E340B14D425EDA34F9A4838629F1551A0FBF48AD0A5F5BD3DCDDC650563992958527340C0E69652536683AEEAFCF1B&originRegion=us-east-1&originCreation=20221206005448},
	language = {en},
	urldate = {2022-12-06},
	doi = {10.1016/j.epsr.2022.108362},
}

@book{ascher_computer_1998,
	address = {Philadelphia},
	title = {Computer methods for ordinary differential equations and differential-algebraic equations},
	isbn = {978-0-89871-412-8},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Ascher, U. M. and Petzold, Linda Ruth},
	year = {1998},
	keywords = {Data processing, Differential equations, Differential-algebraic equations},
}

@article{biegler_differential-algebraic_nodate,
	title = {Differential-{Algebraic} {Equations} ({DAEs})},
	language = {en},
	author = {Biegler, L T},
	pages = {40},
}

@article{campbell_differential-algebraic_2008,
	title = {Differential-algebraic equations},
	volume = {3},
	issn = {1941-6016},
	url = {http://www.scholarpedia.org/article/Differential-algebraic_equations},
	doi = {10.4249/scholarpedia.2849},
	language = {en},
	number = {8},
	urldate = {2022-12-05},
	journal = {Scholarpedia},
	author = {Campbell, Stephen L. and Linh, Vu Hoang and Petzold, Linda R.},
	month = aug,
	year = {2008},
	pages = {2849},
}

@article{tripathy_-stable_1977,
	title = {A-stable numerical integration method for transmission system transients},
	volume = {96},
	issn = {0018-9510},
	doi = {10.1109/T-PAS.1977.32467},
	abstract = {This paper presents an algorithm for the calculation of electromagnetic transients using the semi-implicit Runge-Kutta method. It is shown that this absolutely stable (A-Stable) integration method is a noniterative technique and therefore advantageous for nonlinear problems. Digital simulation results of switching surges for single-phase and three-phase transformer terminated lines are given in cemparison with field test results. A general discussion of numerical stability of integration methods is also included.},
	number = {4},
	journal = {IEEE Transactions on Power Apparatus and Systems},
	author = {Tripathy, S.C. and Rao, N.D.},
	month = jul,
	year = {1977},
	note = {Conference Name: IEEE Transactions on Power Apparatus and Systems},
	keywords = {Capacitors, Difference equations, Differential equations, Finite wordlength effects, Inductors, Integral equations, Nonlinear systems, Power transmission lines, Stability, Transmission line theory},
	pages = {1399--1407},
}

@misc{kutz_advanced_2020,
	title = {Advanced {Differential} {Equations}: {Asymptotics} \& {Perturbations}},
	shorttitle = {Advanced {Differential} {Equations}},
	url = {http://arxiv.org/abs/2012.14591},
	abstract = {Approximation techniques have been historically important for solving differential equations, both as initial value problems and boundary value problems. The integration of numerical, analytic and perturbation methods and techniques can help produce meaningful approximate solutions for many modern problems in the engineering and physical sciences. An overview of such methods is given here, focusing on the use of perturbation techniques for revealing many key properties and behaviors exhibited in practice across diverse scientiﬁc disciplines.},
	language = {en},
	urldate = {2022-12-01},
	publisher = {arXiv},
	author = {Kutz, J. Nathan},
	month = dec,
	year = {2020},
	note = {arXiv:2012.14591 [nlin]},
	keywords = {Mathematics - Classical Analysis and ODEs, Nonlinear Sciences - Pattern Formation and Solitons},
}

@misc{machine_learning__simulation_adjoint_2021,
	title = {Adjoint {State} {Method} for an {ODE} {\textbar} {Adjoint} {Sensitivity} {Analysis}},
	url = {https://www.youtube.com/watch?v=k6s2G5MZv-I},
	urldate = {2022-12-01},
	author = {{Machine Learning \& Simulation}},
	month = aug,
	year = {2021},
}

@misc{noauthor_amath_nodate,
	title = {{AMATH} 568},
	url = {http://faculty.washington.edu/kutz/am568/am568.html/am568.html},
	urldate = {2022-12-01},
	journal = {Advanced Linear Algebra},
}

@article{sengupta_efficient_2014,
	title = {Efficient gradient computation for dynamical models},
	volume = {98},
	issn = {1053-8119},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811914003097},
	doi = {10.1016/j.neuroimage.2014.04.040},
	abstract = {Data assimilation is a fundamental issue that arises across many scales in neuroscience — ranging from the study of single neurons using single electrode recordings to the interaction of thousands of neurons using fMRI. Data assimilation involves inverting a generative model that can not only explain observed data but also generate predictions. Typically, the model is inverted or fitted using conventional tools of (convex) optimization that invariably extremise some functional — norms, minimum descriptive length, variational free energy, etc. Generally, optimisation rests on evaluating the local gradients of the functional to be optimized. In this paper, we compare three different gradient estimation techniques that could be used for extremising any functional in time — (i) finite differences, (ii) forward sensitivities and a method based on (iii) the adjoint of the dynamical system. We demonstrate that the first-order gradients of a dynamical system, linear or non-linear, can be computed most efficiently using the adjoint method. This is particularly true for systems where the number of parameters is greater than the number of states. For such systems, integrating several sensitivity equations – as required with forward sensitivities – proves to be most expensive, while finite-difference approximations have an intermediate efficiency. In the context of neuroimaging, adjoint based inversion of dynamical causal models (DCMs) can, in principle, enable the study of models with large numbers of nodes and parameters.},
	language = {en},
	urldate = {2022-12-01},
	journal = {NeuroImage},
	author = {Sengupta, B. and Friston, K. J. and Penny, W. D.},
	month = sep,
	year = {2014},
	keywords = {Adjoint methods, Augmented Lagrangian, Dynamic causal modelling, Dynamical systems, Model fitting},
	pages = {521--527},
}

@article{zhang_discrete_2017,
	title = {Discrete {Adjoint} {Sensitivity} {Analysis} of {Hybrid} {Dynamical} {Systems} {With} {Switching}},
	volume = {64},
	issn = {1558-0806},
	doi = {10.1109/TCSI.2017.2651683},
	abstract = {Sensitivity analysis is an important tool for describing power system dynamic behavior in response to parameter variations. It is a central component in preventive and corrective control applications. The existing approaches for sensitivity calculations, namely, finite-difference and forward sensitivity analysis, require a computational effort that increases linearly with the number of sensitivity parameters. In this paper, we investigate, implement, and test a discrete adjoint sensitivity approach whose computational effort is effectively independent of the number of sensitivity parameters. The proposed approach is highly efficient for calculating sensitivities of larger systems and is consistent, within machine precision, with the function whose sensitivity we are seeking. This is an essential feature for use in optimization applications. Moreover, our approach includes a consistent treatment of systems with switching, such as dc exciters, by deriving and implementing the adjoint jump conditions that arise from state-dependent and time-dependent switchings. The accuracy and the computational efficiency of the proposed approach are demonstrated in comparison with the forward sensitivity analysis approach. This paper focuses primarily on the power system dynamics, but the approach is general and can be applied to hybrid dynamical systems in a broader range of fields.},
	number = {5},
	journal = {IEEE Transactions on Circuits and Systems I: Regular Papers},
	author = {Zhang, Hong and Abhyankar, Shrirang and Constantinescu, Emil and Anitescu, Mihai},
	month = may,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Circuits and Systems I: Regular Papers},
	keywords = {Adjoint sensitivity, Mathematical model, Power system dynamics, Power system stability, Sensitivity analysis, Stability analysis, Switches, discrete sensitivity, power system dynamics, trajectory sensitivity analysis (TSA), transient stability},
	pages = {1247--1259},
}

@article{geng_second-order_2019,
	title = {Second-{Order} {Trajectory} {Sensitivity} {Analysis} of {Hybrid} {Systems}},
	volume = {66},
	issn = {1558-0806},
	doi = {10.1109/TCSI.2019.2903196},
	abstract = {Hybrid dynamical systems are characterized by intrinsic coupling between continuous dynamics and discrete events. This paper has adopted a differential-algebraic impulsive switched (DAIS) model to capture such dynamic behavior. For such systems, trajectory sensitivity analysis provides a valuable approach for describing perturbations of system trajectories resulting from small variations in initial conditions and/or uncertain parameters. The first-order sensitivities have been fully described for hybrid system and used in a variety of applications. This paper formulates the differential-algebraic equations (DAE) that govern second-order sensitivities over regions where dynamics are smooth, i.e., away from events. It also establishes the jump conditions that describe the step change in second-order sensitivities at discrete (switching and state reset) events. These results together fully characterize second-order sensitivities for general hybrid dynamical system.},
	number = {5},
	journal = {IEEE Transactions on Circuits and Systems I: Regular Papers},
	author = {Geng, Sijia and Hiskens, Ian A.},
	month = may,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Circuits and Systems I: Regular Papers},
	keywords = {Discrete events, Hybrid dynamical systems, Parameter estimation, Sensitivity analysis, Trajectory, discrete events, parameter uncertainty, second-order sensitivities, trajectory sensitivity analysis},
	pages = {1922--1934},
}

@inproceedings{hiskens_power_2002,
	title = {Power system applications of trajectory sensitivities},
	volume = {2},
	doi = {10.1109/PESW.2002.985199},
	abstract = {Trajectory sensitivities complement time domain simulation in the analysis of large disturbance dynamic behaviour of power systems. They are formed by linearization around a nonlinear, and possibly non-smooth, trajectory. The influence of parameter variations on large disturbance behaviour can be estimated (to first order) from these sensitivities. Large (small) sensitivities indicate that a parameter has a significant (negligible) effect on behaviour. These insights are helpful in analysing the underlying influences on system dynamics, and for assessing the significance of parameter uncertainty. Further, trajectory sensitivities provide gradient information for applications such as parameter estimation, boundary value problems, border-collision bifurcations, and optimal control.},
	booktitle = {2002 {IEEE} {Power} {Engineering} {Society} {Winter} {Meeting}. {Conference} {Proceedings} ({Cat}. {No}.{02CH37309})},
	author = {Hiskens, I.A. and Pai, M.A.},
	month = jan,
	year = {2002},
	keywords = {Analytical models, Boundary value problems, Nonlinear dynamical systems, Parameter estimation, Power system analysis computing, Power system dynamics, Power system simulation, Power systems, Time domain analysis, Uncertain systems},
	pages = {1200--1205 vol.2},
}

@article{humpage_predictorcorrector_1965,
	title = {Predictor–corrector methods of numerical integration in digital-computer analyses of power-system transient stability},
	volume = {112},
	issn = {00203270},
	url = {https://digital-library.theiet.org/content/journals/10.1049/piee.1965.0253},
	doi = {10.1049/piee.1965.0253},
	language = {en},
	number = {8},
	urldate = {2022-12-01},
	journal = {Proceedings of the Institution of Electrical Engineers},
	author = {Humpage, W.D. and Stott, B.},
	year = {1965},
	pages = {1557},
}

@misc{liu_faster_2021,
	title = {Faster than {Real}-{Time} {Simulation}: {Methods}, {Tools}, and {Applications}},
	shorttitle = {Faster than {Real}-{Time} {Simulation}},
	url = {http://arxiv.org/abs/2104.04149},
	abstract = {Real-time simulation enables the understanding of system operating conditions by evaluating simulation models of physical components running synchronized at the real-time wall clock. Leveraging the real-time measurements of comprehensive system models, faster than real-time (FTRT) simulation allows the evaluation of system architectures at speeds faster than realtime. FTRT simulation can assist in predicting the system’s behavior efﬁciently, thus assisting the operation of system processes. Namely, the provided acceleration can be used for improving system scheduling, assessing system vulnerabilities, and predicting system disruptions in real-time systems. The acceleration of simulation times can be achieved by utilizing digital realtime simulators (RTS) and high-performance computing (HPC) architectures. FTRT simulation has been widely used, among others, for the operation, design, and investigation of power system events, building emergency management plans, wildﬁre prediction, etc. In this paper, we review the existing literature on FTRT simulation and its applications in different disciplines, with a particular focus on power systems. We present existing system modeling approaches, simulation tools and computing frameworks, and stress the importance of FTRT accuracy.},
	language = {en},
	urldate = {2022-11-23},
	publisher = {arXiv},
	author = {Liu, XiaoRui and Ospina, Juan and Zografopoulos, Ioannis and Russell, Alonzo and Konstantinou, Charalambos},
	month = apr,
	year = {2021},
	note = {arXiv:2104.04149 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Systems and Control},
}

@article{biazar_solution_2004,
	title = {Solution of the system of ordinary differential equations by {Adomian} decomposition method},
	volume = {147},
	issn = {00963003},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0096300302008068},
	doi = {10.1016/S0096-3003(02)00806-8},
	language = {en},
	number = {3},
	urldate = {2022-11-22},
	journal = {Applied Mathematics and Computation},
	author = {Biazar, J. and Babolian, E. and Islam, R.},
	month = jan,
	year = {2004},
	pages = {713--719},
}

@article{yao_efficient_2020,
	title = {Efficient and {Robust} {Dynamic} {Simulation} of {Power} {Systems} {With} {Holomorphic} {Embedding}},
	volume = {35},
	issn = {0885-8950, 1558-0679},
	url = {https://ieeexplore.ieee.org/document/8809250/},
	doi = {10.1109/TPWRS.2019.2935040},
	number = {2},
	urldate = {2022-11-22},
	journal = {IEEE Transactions on Power Systems},
	author = {Yao, Rui and Liu, Yang and Sun, Kai and Qiu, Feng and Wang, Jianhui},
	month = mar,
	year = {2020},
	pages = {938--949},
}

@article{dorto_comparing_2021,
	title = {Comparing {Different} {Approaches} for {Solving} {Large} {Scale} {Power}-{Flow} {Problems} {With} the {Newton}-{Raphson} {Method}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3072338},
	abstract = {This paper focuses on using the Newton-Raphson method to solve the power-flow problems. Since the most computationally demanding part of the Newton-Raphson method is to solve the linear equations at each iteration, this study investigates different approaches to solve the linear equations on both central processing unit (CPU) and graphical processing unit (GPU). Six different approaches have been developed and evaluated in this paper: two approaches of these run entirely on CPU while other two of these run entirely on GPU, and the remaining two are hybrid approaches that run on both CPU and GPU. All six direct linear solvers use either LU or QR factorization to solve the linear equations. Two different hardware platforms have been used to conduct the experiments. The performance results show that the CPU version with LU factorization gives better performance compared to the GPU version using standard library called cuSOLVER even for the larger power-flow problems. Moreover, it has been proven that the best performance is achieved using a hybrid method where the Jacobian matrix is assembled on GPU, the preprocessing with a sparse high performance linear solver called KLU is performed on the CPU in the first iteration, and the linear equation is factorized on the GPU and solved on the CPU. Maximum speed up in this study is obtained on the largest case with 25000 buses. The hybrid version shows a speedup factor of 9.6 with a NVIDIA P100 GPU while 13.1 with a NVIDIA V100 GPU in comparison with baseline CPU version on an Intel Xeon Gold 6132 CPU.},
	journal = {IEEE Access},
	author = {D’orto, Manolo and Sjöblom, Svante and Chien, Lung Sheng and Axner, Lilit and Gong, Jing},
	year = {2021},
	note = {Conference Name: IEEE Access},
	keywords = {Central Processing Unit, Computational modeling, Graphics processing units, High performance computing, Jacobian matrices, Mathematical model, Newton method, Reactive power, direct solver, parallel algorithms, power engineering computing, power-flow},
	pages = {56604--56615},
}

@article{benidis_deep_2022,
	title = {Deep {Learning} for {Time} {Series} {Forecasting}: {Tutorial} and {Literature} {Survey}},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Deep {Learning} for {Time} {Series} {Forecasting}},
	url = {http://arxiv.org/abs/2004.10240},
	doi = {10.1145/3533382},
	abstract = {Deep learning based forecasting methods have become the methods of choice in many applications of time series prediction or forecasting often outperforming other approaches. Consequently, over the last years, these methods are now ubiquitous in large-scale industrial forecasting applications and have consistently ranked among the best entries in forecasting competitions (e.g., M4 and M5). This practical success has further increased the academic interest to understand and improve deep forecasting methods. In this article we provide an introduction and overview of the ﬁeld: We present important building blocks for deep forecasting in some depth; using these building blocks, we then survey the breadth of the recent deep forecasting literature.},
	language = {en},
	urldate = {2022-10-28},
	journal = {ACM Computing Surveys},
	author = {Benidis, Konstantinos and Rangapuram, Syama Sundar and Flunkert, Valentin and Wang, Yuyang and Maddix, Danielle and Turkmen, Caner and Gasthaus, Jan and Bohlke-Schneider, Michael and Salinas, David and Stella, Lorenzo and Aubet, Francois-Xavier and Callot, Laurent and Januschowski, Tim},
	month = may,
	year = {2022},
	note = {arXiv:2004.10240 [cs, stat]},
	keywords = {A.1, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {3533382},
}

@misc{wang_bridging_2021,
	title = {Bridging {Physics}-based and {Data}-driven modeling for {Learning} {Dynamical} {Systems}},
	url = {http://arxiv.org/abs/2011.10616},
	abstract = {How can we learn a dynamical system to make forecasts, when some variables are unobserved? For instance, in COVID-19, we want to forecast the number of infected and death cases but we do not know the count of susceptible and exposed people. While mechanics compartment models are widely-used in epidemic modeling, data-driven models are emerging for disease forecasting. We ﬁrst formalize the learning of physics-based models as AutoODE, which leverages automatic differentiation to estimate the model parameters. Through a benchmark study on COVID-19 forecasting, we notice that physics-based mechanistic models signiﬁcantly outperform deep learning. Such performance differences highlight the generalization problem in dynamical system learning due to distribution shift. We identify two scenarios where distribution shift can occur: changes in data domain and changes in parameter domain (system dynamics). Through systematic experiments on several dynamical systems, we found that deep learning models fail to forecast well under both scenarios. While much research on distribution shift has focused on changes in data domain, our work calls attention to rethink generalization for learning dynamical systems.},
	language = {en},
	urldate = {2022-10-28},
	publisher = {arXiv},
	author = {Wang, Rui and Maddix, Danielle and Faloutsos, Christos and Wang, Yuyang and Yu, Rose},
	month = apr,
	year = {2021},
	note = {arXiv:2011.10616 [physics, q-bio]},
	keywords = {Computer Science - Machine Learning, Physics - Physics and Society, Quantitative Biology - Populations and Evolution},
}

@article{bouthillier_unreproducible_nodate,
	title = {Unreproducible {Research} is {Reproducible}},
	abstract = {The apparent contradiction in the title is a wordplay on the different meanings attributed to the word reproducible across different scientiﬁc ﬁelds. What we imply is that unreproducible ﬁndings can be built upon reproducible methods. Without denying the importance of facilitating the reproduction of methods, we deem important to reassert that reproduction of ﬁndings is a fundamental step of the scientiﬁc inquiry. We argue that the commendable quest towards easy deterministic reproducibility of methods and numerical results should not have us forget the even more important necessity of ensuring the reproducibility of empirical ﬁndings and conclusions by properly accounting for essential sources of variations. We provide experiments to exemplify the brittleness of current common practice in the evaluation of models in the ﬁeld of deep learning, showing that even if the results could be reproduced, a slightly different experiment would not support the ﬁndings. We hope to help clarify the distinction between exploratory and empirical research in the ﬁeld of deep learning and believe more energy should be devoted to proper empirical research in our community. This work is an attempt to promote the use of more rigorous and diversiﬁed methodologies. It is not an attempt to impose a new methodology and it is not a critique on the nature of exploratory research.},
	language = {en},
	author = {Bouthillier, Xavier and Laurent, César and Vincent, Pascal},
	pages = {10},
}

@article{bouthillier_survey_nodate,
	title = {Survey of machine-learning experimental methods at {NeurIPS2019} and {ICLR2020}},
	abstract = {How do machine-learning researchers run their empirical validation? In the context of a push for improved reproducibility and benchmarking, this question is important to develop new tools for model comparison. This document summarizes a simple survey about experimental procedures, sent to authors of published papers at two leading conferences, NeurIPS 2019 and ICLR 2020. It gives a simple picture of how hyper-parameters are set, how many baselines and datasets are included, or how seeds are used.},
	language = {en},
	author = {Bouthillier, Xavier and Varoquaux, Gaël},
	pages = {12},
}

@article{turner_bayesian_2020,
	title = {Bayesian {Optimization} is {Superior} to {Random} {Search} for {Machine} {Learning} {Hyperparameter} {Tuning}: {Analysis} of the {Black}-{Box} {Optimization} {Challenge} 2020},
	abstract = {This paper presents the results and insights from the black-box optimization (BBO) challenge at NeurIPS 2020 which ran from July–October, 2020. The challenge emphasized the importance of evaluating derivative-free optimizers for tuning the hyperparameters of machine learning models. This was the ﬁrst black-box optimization challenge with a machine learning emphasis. It was based on tuning (validation set) performance of standard machine learning models on real datasets. This competition has widespread impact as black-box optimization (e.g., Bayesian optimization) is relevant for hyperparameter tuning in almost every machine learning project as well as many applications outside of machine learning. The ﬁnal leaderboard was determined using the optimization performance on held-out (hidden) objective functions, where the optimizers ran without human intervention. Baselines were set using the default settings of several open source black-box optimization packages as well as random search.},
	language = {en},
	author = {Turner, Ryan and Eriksson, David and McCourt, Michael and Kiili, Juha and Laaksonen, Eero and Xu, Zhen and Guyon, Isabelle},
	year = {2020},
	pages = {24},
}

@article{burger_implementation_1997,
	title = {{IMPLEMENTATION} {OF} {A} {FAST} {MATRIX} {INVERSION} {METHOD} {IN} {THE} {ELECTRODYNAMIC} {SIMULATION} {PROGRAM}},
	volume = {41},
	copyright = {Copyright (c)},
	issn = {1587-3781},
	url = {https://pp.bme.hu/ee/article/view/4402},
	abstract = {This  paper describes  the imple!llentation  of a  fast  matrix  inversion  algorithm  that can  be 
used  efficiently  in  the electric  network  analysis.
The  ElectroDynamic  Simulation  (EDS)  is  an  IBM  PC  based  computer  program 
for  studying  short  and middle  term  dynamics  of the  electric  power  system.  The electric 
network  is  modelled  by  quasi-stationary  representation.  EDS  has  an  interactive  menu 
system.  The  user  can  change  the operating conditions  like  sending  telecommands  from 
a control center-or initiate simulation of disturbances.  Simulation of  some events requires 
changing  the network matrices.  The admittance matrix Y  can  be modified  directly while 
the  impedance matrix  Z  is  updated  by  inverting  the  new  Y. The  conventional  inversion 
is  very  time  consuming  and  freezes  the simulation  for  unacceptably  long  intervals. 
A  new,  faster method has been  developed which  takes  into account  that  the change 
in  the matrices  due  to  the  events  is  small  and  well-defined.  In  these  cases  the  complete 
inversion  is  not  needed,  the  impedance  matrix  can  be  updated  directly,  by  the  aid  of 
the Sherman-IvIorrison  theorem. The  new  method  has  been  implemented  in  recent  EDS 
versions  and  performs  significantly  faster  than  the conventional  inversion.},
	language = {en},
	number = {1},
	urldate = {2022-10-23},
	journal = {Periodica Polytechnica Electrical Engineering (Archives)},
	author = {Bürger, László},
	year = {1997},
	note = {Number: 1},
	keywords = {matrix  inversion},
	pages = {41--52},
}

@misc{zhang_efficient_2021,
	title = {An {Efficient} {Network} {Solver} for {Dynamic} {Simulation} of {Power} {Systems} {Based} on {Hierarchical} {Inverse} {Computation} and {Modification}},
	url = {http://arxiv.org/abs/2105.10661},
	abstract = {In both power system transient stability and electromagnetic transient (EMT) simulations, up to 90\% of the computational time is devoted to solve the network equations, i.e., a set of linear equations. Traditional approaches are based on sparse LU factorization, which is inherently sequential. In this paper, EMT simulation is considered and an inverse-based network solution is proposed by a hierarchical method for computing and store the approximate inverse of the conductance matrix. The proposed method can also efﬁciently update the inverse by modifying only local sub-matrices to reﬂect changes in the network, e.g., loss of a line. Experiments on a series of simpliﬁed 179-bus Western Interconnection demonstrate the advantages of the proposed methods.},
	language = {en},
	urldate = {2022-10-23},
	publisher = {arXiv},
	author = {Zhang, Lu and Wang, Bin and Sarin, Vivek and Shi, Weiping and Kumar, P. R. and Xie, Le},
	month = jul,
	year = {2021},
	note = {arXiv:2105.10661 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Systems and Control},
}

@article{noauthor_slack_nodate,
	title = {The {Slack} {Bus}: {What} it ({Really}) {Means} and {Why} it ({Really}) {Matters} for {Autonomous} {Energy} {Grids}},
	language = {en},
	pages = {66},
}

@article{dupont_augmented_nodate,
	title = {Augmented {Neural} {ODEs}},
	abstract = {We show that Neural Ordinary Differential Equations (ODEs) learn representations that preserve the topology of the input space and prove that this implies the existence of functions Neural ODEs cannot represent. To address these limitations, we introduce Augmented Neural ODEs which, in addition to being more expressive models, are empirically more stable, generalize better and have a lower computational cost than Neural ODEs.},
	language = {en},
	author = {Dupont, Emilien and Doucet, Arnaud and Teh, Yee Whye},
	pages = {11},
}

@article{olah_feature_2017,
	title = {Feature {Visualization}},
	volume = {2},
	issn = {2476-0757},
	url = {https://distill.pub/2017/feature-visualization},
	doi = {10.23915/distill.00007},
	abstract = {How neural networks build up their understanding of images},
	language = {en},
	number = {11},
	urldate = {2022-10-05},
	journal = {Distill},
	author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
	month = nov,
	year = {2017},
	pages = {e7},
}

@misc{naitzat_topology_2020,
	title = {Topology of deep neural networks},
	url = {http://arxiv.org/abs/2004.06093},
	abstract = {We study how the topology of a data set \$M = M\_a {\textbackslash}cup M\_b {\textbackslash}subseteq {\textbackslash}mathbb\{R\}{\textasciicircum}d\$, representing two classes \$a\$ and \$b\$ in a binary classification problem, changes as it passes through the layers of a well-trained neural network, i.e., with perfect accuracy on training set and near-zero generalization error (\${\textbackslash}approx 0.01{\textbackslash}\%\$). The goal is to shed light on two mysteries in deep neural networks: (i) a nonsmooth activation function like ReLU outperforms a smooth one like hyperbolic tangent; (ii) successful neural network architectures rely on having many layers, even though a shallow network can approximate any function arbitrary well. We performed extensive experiments on the persistent homology of a wide range of point cloud data sets, both real and simulated. The results consistently demonstrate the following: (1) Neural networks operate by changing topology, transforming a topologically complicated data set into a topologically simple one as it passes through the layers. No matter how complicated the topology of \$M\$ we begin with, when passed through a well-trained neural network \$f : {\textbackslash}mathbb\{R\}{\textasciicircum}d {\textbackslash}to {\textbackslash}mathbb\{R\}{\textasciicircum}p\$, there is a vast reduction in the Betti numbers of both components \$M\_a\$ and \$M\_b\$; in fact they nearly always reduce to their lowest possible values: \${\textbackslash}beta\_k{\textbackslash}bigl(f(M\_i){\textbackslash}bigr) = 0\$ for \$k {\textbackslash}ge 1\$ and \${\textbackslash}beta\_0{\textbackslash}bigl(f(M\_i){\textbackslash}bigr) = 1\$, \$i =a, b\$. Furthermore, (2) the reduction in Betti numbers is significantly faster for ReLU activation than hyperbolic tangent activation as the former defines nonhomeomorphic maps that change topology, whereas the latter defines homeomorphic maps that preserve topology. Lastly, (3) shallow and deep networks transform data sets differently -- a shallow network operates mainly through changing geometry and changes topology only in its final layers, a deep one spreads topological changes more evenly across all layers.},
	urldate = {2022-10-05},
	publisher = {arXiv},
	author = {Naitzat, Gregory and Zhitnikov, Andrey and Lim, Lek-Heng},
	month = apr,
	year = {2020},
	note = {arXiv:2004.06093 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, I.2.6, Mathematics - Algebraic Topology, Statistics - Machine Learning},
}

@misc{noauthor_neural_nodate,
	title = {Neural {Networks}, {Manifolds}, and {Topology} -- colah's blog},
	url = {http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/},
	urldate = {2022-10-05},
}

@article{hoffman_neutra-lizing_2019,
	title = {{NeuTra}-lizing {Bad} {Geometry} in {Hamiltonian} {Monte} {Carlo} {Using} {Neural} {Transport}},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	url = {https://arxiv.org/abs/1903.03704},
	doi = {10.48550/ARXIV.1903.03704},
	abstract = {Hamiltonian Monte Carlo is a powerful algorithm for sampling from difficult-to-normalize posterior distributions. However, when the geometry of the posterior is unfavorable, it may take many expensive evaluations of the target distribution and its gradient to converge and mix. We propose neural transport (NeuTra) HMC, a technique for learning to correct this sort of unfavorable geometry using inverse autoregressive flows (IAF), a powerful neural variational inference technique. The IAF is trained to minimize the KL divergence from an isotropic Gaussian to the warped posterior, and then HMC sampling is performed in the warped space. We evaluate NeuTra HMC on a variety of synthetic and real problems, and find that it significantly outperforms vanilla HMC both in time to reach the stationary distribution and asymptotic effective-sample-size rates.},
	urldate = {2022-09-28},
	author = {Hoffman, Matthew and Sountsov, Pavel and Dillon, Joshua V. and Langmore, Ian and Tran, Dustin and Vasudevan, Srinivas},
	year = {2019},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation (stat.CO), FOS: Computer and information sciences, Machine Learning (stat.ML)},
}

@unpublished{noauthor_implicit_nodate,
	title = {Implicit {Runge} {Kutta} methods},
	url = {https://www.epfl.ch/labs/anchp/wp-content/uploads/2018/05/part2-1.pdf},
	urldate = {2022-09-28},
}

@article{noauthor_john_nodate,
	title = {John {Butcher}'s tutorials - {Implicit} {Runge}--{Kutta} methods},
	language = {en},
	pages = {71},
}

@article{de_swart_construction_1997,
	title = {On the construction of error estimators for implicit {Runge}-{Kutta} methods},
	volume = {86},
	issn = {03770427},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0377042797001660},
	doi = {10.1016/S0377-0427(97)00166-0},
	language = {en},
	number = {2},
	urldate = {2022-09-28},
	journal = {Journal of Computational and Applied Mathematics},
	author = {de Swart, Jacques J.B and Söderlind, Gustaf},
	month = dec,
	year = {1997},
	pages = {347--358},
}

@article{sandretto_validated_2016,
	title = {Validated {Explicit} and {Implicit} {Runge}-{Kutta} {Methods}},
	url = {https://hal.archives-ouvertes.fr/hal-01243053},
	language = {en},
	number = {Special issue devoted to material presented at SWIM 2015,},
	journal = {Reliable Computing electronic edition},
	author = {Sandretto, Julien Alexandre Dit and Chapoutot, Alexandre},
	year = {2016},
	note = {hal-01243053},
	pages = {36},
}

@article{shampine_practical_1986,
	title = {Some practical {Runge}-{Kutta} formulas},
	volume = {46},
	issn = {0025-5718, 1088-6842},
	url = {https://www.ams.org/mcom/1986-46-173/S0025-5718-1986-0815836-3/},
	doi = {10.1090/S0025-5718-1986-0815836-3},
	abstract = {A new selection is made of the most practical of the many explicit Runge-Kutta formulas of order 4 which have been proposed. A new formula is considered, formulas are modified to improve their quality and efficiency in agreement with improved understanding of the issues, and formulas are derived which permit interpolation. It is possible to do a lot better than the pair of Fehlberg currently regarded as "best".},
	language = {en},
	number = {173},
	urldate = {2022-09-28},
	journal = {Mathematics of Computation},
	author = {Shampine, Lawrence F.},
	year = {1986},
	pages = {135--150},
}

@article{dormand_family_1980,
	title = {A family of embedded {Runge}-{Kutta} formulae},
	volume = {6},
	issn = {03770427},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0771050X80900133},
	doi = {10.1016/0771-050X(80)90013-3},
	language = {en},
	number = {1},
	urldate = {2022-09-28},
	journal = {Journal of Computational and Applied Mathematics},
	author = {Dormand, J.R. and Prince, P.J.},
	month = mar,
	year = {1980},
	keywords = {Runge-Kutta},
	pages = {19--26},
}

@article{yu_jiang_robust_2013,
	title = {Robust {Adaptive} {Dynamic} {Programming} {With} an {Application} to {Power} {Systems}},
	volume = {24},
	issn = {2162-237X, 2162-2388},
	url = {http://ieeexplore.ieee.org/document/6484168/},
	doi = {10.1109/TNNLS.2013.2249668},
	number = {7},
	urldate = {2022-09-21},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {{Yu Jiang} and {Zhong-Ping Jiang}},
	month = jul,
	year = {2013},
	pages = {1150--1156},
}

@misc{noauthor_deep_nodate,
	title = {Deep {Learning} {Systems} {Course} - {YouTube}},
	url = {https://www.youtube.com/channel/UC3-KIvmiIaZimgXMNt7F99g},
	urldate = {2022-09-18},
}

@misc{lin_accelerated_2021,
	title = {Accelerated replica exchange stochastic gradient {Langevin} diffusion enhanced {Bayesian} {DeepONet} for solving noisy parametric {PDEs}},
	url = {http://arxiv.org/abs/2111.02484},
	abstract = {The Deep Operator Networks (DeepONet) is a fundamentally diﬀerent class of neural networks that we train to approximate nonlinear operators, including the solution operator of parametric partial diﬀerential equations (PDE). DeepONets have shown remarkable approximation and generalization capabilities even when trained with relatively small datasets. However, the performance of DeepONets deteriorates when the training data is polluted with noise, a scenario that occurs very often in practice. To enable DeepONets training with noisy data, we propose using the Bayesian framework of replica-exchange Langevin diﬀusion. Such a framework uses two particles, one for exploring and another for exploiting the loss function landscape of DeepONets. We show that the proposed framework’s exploration and exploitation capabilities enable (1) improved training convergence for DeepONets in noisy scenarios and (2) attaching an uncertainty estimate for the predicted solutions of parametric PDEs. In addition, we show that replica-exchange Langeving Diﬀusion (remarkably) also improves the DeepONet’s mean prediction accuracy in noisy scenarios compared with vanilla DeepONets trained with state-of-the-art gradient-based optimization algorithms (e.g., Adam). To reduce the potentially high computational cost of replica, in this work, we propose an accelerated training framework for replica-exchange Langevin diﬀusion that exploits the neural network architecture of DeepONets to reduce its computational cost up to 25\% without compromising the proposed framework’s performance. Finally, we illustrate the eﬀectiveness of the proposed Bayesian framework using a series of experiments on four parametric PDE problems.},
	language = {en},
	urldate = {2022-09-11},
	publisher = {arXiv},
	author = {Lin, Guang and Moya, Christian and Zhang, Zecheng},
	month = nov,
	year = {2021},
	note = {arXiv:2111.02484 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
}

@misc{zhang_multiauto-deeponet_2022,
	title = {{MultiAuto}-{DeepONet}: {A} {Multi}-resolution {Autoencoder} {DeepONet} for {Nonlinear} {Dimension} {Reduction}, {Uncertainty} {Quantification} and {Operator} {Learning} of {Forward} and {Inverse} {Stochastic} {Problems}},
	shorttitle = {{MultiAuto}-{DeepONet}},
	url = {http://arxiv.org/abs/2204.03193},
	abstract = {A new data-driven method for operator learning of stochastic diﬀerential equations(SDE) is proposed in this paper. The central goal is to solve forward and inverse stochastic problems more eﬀectively using limited data. Deep operator network(DeepONet) has been proposed recently for operator learning. Compared to other neural networks to learn functions, it aims at the problem of learning nonlinear operators. However, it can be challenging by using the original model to learn nonlinear operators for highdimensional stochastic problems. We propose a new multi-resolution autoencoder DeepONet model referred to as MultiAuto-DeepONet to deal with this diﬃculty with the aid of convolutional autoencoder. The encoder part of the network is designed to reduce the dimensionality as well as discover the hidden features of high-dimensional stochastic inputs. The decoder is designed to have a special structure, i.e. in the form of DeepONet. The ﬁrst DeepONet in decoder is designed to reconstruct the input function involving randomness while the second one is used to approximate the solution of desired equations. Those two DeepONets has a common branch net and two independent trunk nets. This architecture enables us to deal with multi-resolution inputs naturally. By adding L1 regularization to our network, we found the outputs from the branch net and two trunk nets all have sparse structures. This reduces the number of trainable parameters in the neural network thus making the model more eﬃcient. Finally, we conduct several numerical experiments to illustrate the eﬀectiveness of our proposed MultiAuto-DeepONet model with uncertainty quantiﬁcation.},
	language = {en},
	urldate = {2022-09-11},
	publisher = {arXiv},
	author = {Zhang, Jiahao and Zhang, Shiqi and Lin, Guang},
	month = apr,
	year = {2022},
	note = {arXiv:2204.03193 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{haugdal_open_2021,
	title = {An {Open} {Source} {Power} {System} {Simulator} in {Python} for {Efficient} {Prototyping} of {WAMPAC} {Applications}},
	url = {http://arxiv.org/abs/2101.02937},
	abstract = {An open source software package for performing dynamic RMS simulation of small to medium-sized power systems is presented, written entirely in the Python programming language. The main objective is to facilitate fast prototyping of new wide area monitoring, control and protection applications for the future power system by enabling seamless integration with other tools available for Python in the open source community, e.g. for signal processing, artificial intelligence, communication protocols etc. The focus is thus transparency and expandability rather than computational efficiency and performance. The main purpose of this paper, besides presenting the code and some results, is to share interesting experiences with the power system community, and thus stimulate wider use and further development. Two interesting conclusions at the current stage of development are as follows: First, the simulation code is fast enough to emulate real-time simulation for small and medium-size grids with a time step of 5 ms, and allows for interactive feedback from the user during the simulation. Second, the simulation code can be uploaded to an online Python interpreter, edited, run and shared with anyone with a compatible internet browser. Based on this, we believe that the presented simulation code could be a valuable tool, both for researchers in early stages of prototyping real-time applications, and in the educational setting, for students developing intuition for concepts and phenomena through real-time interaction with a running power system model.},
	urldate = {2022-09-05},
	publisher = {arXiv},
	author = {Haugdal, Hallvar and Uhlen, Kjetil},
	month = jan,
	year = {2021},
	note = {arXiv:2101.02937 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Systems and Control},
}

@article{fortnow_computational_nodate,
	title = {The {Computational} {Complexity} {Column}},
	language = {en},
	author = {Fortnow, Lance},
	pages = {39},
}

@misc{stiasny_publicly_2022,
	title = {Publicly available implementation},
	url = {https://github.com/jbesty},
	abstract = {jbesty has 6 repositories available. Follow their code on GitHub.},
	language = {en},
	urldate = {2022-08-22},
	journal = {GitHub},
	author = {Stiasny, Jochen},
	year = {2022},
}

@misc{dtu_computing_center_dtu_2022,
	title = {{DTU} {Computing} {Center} resources},
	isbn = {10.48714/DTU.HPC.0001},
	url = {https://doi.org/10.48714/DTU.HPC.0001},
	urldate = {2022-08-19},
	publisher = {Technical University of Denmark},
	author = {DTU Computing Center},
	year = {2022},
}

@misc{daw_rethinking_2022,
	title = {Rethinking the {Importance} of {Sampling} in {Physics}-informed {Neural} {Networks}},
	url = {http://arxiv.org/abs/2207.02338},
	abstract = {Physics-informed neural networks (PINNs) have emerged as a powerful tool for solving partial differential equations (PDEs) in a variety of domains. While previous research in PINNs has mainly focused on constructing and balancing loss functions during training to avoid poor minima, the effect of sampling collocation points on the performance of PINNs has largely been overlooked. In this work, we find that the performance of PINNs can vary significantly with different sampling strategies, and using a fixed set of collocation points can be quite detrimental to the convergence of PINNs to the correct solution. In particular, (1) we hypothesize that training of PINNs rely on successful "propagation" of solution from initial and/or boundary condition points to interior points, and PINNs with poor sampling strategies can get stuck at trivial solutions if there are {\textbackslash}textit\{propagation failures\}. (2) We demonstrate that propagation failures are characterized by highly imbalanced PDE residual fields where very high residuals are observed over very narrow regions. (3) To mitigate propagation failure, we propose a novel {\textbackslash}textit\{evolutionary sampling\} (Evo) method that can incrementally accumulate collocation points in regions of high PDE residuals. We further provide an extension of Evo to respect the principle of causality while solving time-dependent PDEs. We empirically demonstrate the efficacy and efficiency of our proposed methods in a variety of PDE problems.},
	urldate = {2022-08-08},
	publisher = {arXiv},
	author = {Daw, Arka and Bu, Jie and Wang, Sifan and Perdikaris, Paris and Karpatne, Anuj},
	month = jul,
	year = {2022},
	note = {arXiv:2207.02338 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{susuki_nonlinear_2011,
	title = {Nonlinear {Koopman} {Modes} and {Coherency} {Identification} of {Coupled} {Swing} {Dynamics}},
	volume = {26},
	issn = {0885-8950, 1558-0679},
	url = {http://ieeexplore.ieee.org/document/5713215/},
	doi = {10.1109/TPWRS.2010.2103369},
	number = {4},
	urldate = {2022-08-08},
	journal = {IEEE Transactions on Power Systems},
	author = {Susuki, Yoshihiko and Mezic, Igor},
	month = nov,
	year = {2011},
	pages = {1894--1904},
}

@misc{noauthor_minds_nodate,
	title = {{MINDS} {Winter} {School} and {Workshop} 2022 – {Mathematical} {Institute} for {Data} {Science}},
	url = {https://www.minds.jhu.edu/2022-tripods-winter-school-workshop-on-interplay-between-machine-learning-and-dynamical-systems/},
	urldate = {2022-08-08},
}

@misc{mohammadian_gradient-enhanced_2022,
	title = {Gradient-{Enhanced} {Physics}-{Informed} {Neural} {Networks} for {Power} {Systems} {Operational} {Support}},
	url = {http://arxiv.org/abs/2206.10579},
	abstract = {The application of deep learning methods to speed up the resolution of challenging power flow problems has recently shown very encouraging results. However, power system dynamics are not snap-shot, steady-state operations. These dynamics must be considered to ensure that the optimal solutions provided by these models adhere to practical dynamical constraints, avoiding frequency fluctuations and grid instabilities. Unfortunately, dynamic system models based on ordinary or partial differential equations are frequently unsuitable for direct application in control or state estimates due to their high computational costs. To address these challenges, this paper introduces a machine learning method to approximate the behavior of power systems dynamics in near real time. The proposed framework is based on gradient-enhanced physics-informed neural networks (gPINNs) and encodes the underlying physical laws governing power systems. A key characteristic of the proposed gPINN is its ability to train without the need of generating expensive training data. The paper illustrates the potential of the proposed approach in both forward and inverse problems in a single-machine infinite bus system for predicting rotor angles and frequency, and uncertain parameters such as inertia and damping to showcase its potential for a range of power systems applications.},
	urldate = {2022-07-12},
	publisher = {arXiv},
	author = {Mohammadian, Mostafa and Baker, Kyri and Fioretto, Ferdinando},
	month = jun,
	year = {2022},
	note = {arXiv:2206.10579 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control},
}

@phdthesis{donon_deep_2022,
	title = {Deep statistical solvers \& power systems applications},
	language = {English},
	author = {Donon, Balthazar},
	month = may,
	year = {2022},
}

@article{machlev_explainable_2022,
	title = {Explainable {Artificial} {Intelligence} ({XAI}) techniques for energy and power systems: {Review}, challenges and opportunities},
	volume = {9},
	issn = {26665468},
	shorttitle = {Explainable {Artificial} {Intelligence} ({XAI}) techniques for energy and power systems},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2666546822000246},
	doi = {10.1016/j.egyai.2022.100169},
	language = {en},
	urldate = {2022-06-16},
	journal = {Energy and AI},
	author = {Machlev, R. and Heistrene, L. and Perl, M. and Levy, K.Y. and Belikov, J. and Mannor, S. and Levron, Y.},
	month = aug,
	year = {2022},
	pages = {100169},
}

@article{duvenaud_automatic_nodate,
	title = {Automatic {Model} {Construction}  with {Gaussian} {Processes}},
	language = {en},
	author = {Duvenaud, David Kristjanson},
	pages = {157},
}

@book{molnar_interpretable_nodate,
	title = {Interpretable {Machine} {Learning}},
	url = {https://christophm.github.io/interpretable-ml-book/},
	abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.},
	urldate = {2022-06-13},
	author = {Molnar, Christoph},
}

@incollection{holzinger_general_2022,
	address = {Cham},
	title = {General {Pitfalls} of {Model}-{Agnostic} {Interpretation} {Methods} for {Machine} {Learning} {Models}},
	volume = {13200},
	isbn = {978-3-031-04082-5 978-3-031-04083-2},
	url = {https://link.springer.com/10.1007/978-3-031-04083-2_4},
	abstract = {Abstract
            An increasing number of model-agnostic interpretation techniques for machine learning (ML) models such as partial dependence plots (PDP), permutation feature importance (PFI) and Shapley values provide insightful model interpretations, but can lead to wrong conclusions if applied incorrectly. We highlight many general pitfalls of ML model interpretation, such as using interpretation techniques in the wrong context, interpreting models that do not generalize well, ignoring feature dependencies, interactions, uncertainty estimates and issues in high-dimensional settings, or making unjustified causal interpretations, and illustrate them with examples. We focus on pitfalls for global methods that describe the average model behavior, but many pitfalls also apply to local methods that explain individual predictions. Our paper addresses ML practitioners by raising awareness of pitfalls and identifying solutions for correct model interpretation, but also addresses ML researchers by discussing open issues for further research.},
	language = {en},
	urldate = {2022-06-13},
	booktitle = {{xxAI} - {Beyond} {Explainable} {AI}},
	publisher = {Springer International Publishing},
	author = {Molnar, Christoph and König, Gunnar and Herbinger, Julia and Freiesleben, Timo and Dandl, Susanne and Scholbeck, Christian A. and Casalicchio, Giuseppe and Grosse-Wentrup, Moritz and Bischl, Bernd},
	editor = {Holzinger, Andreas and Goebel, Randy and Fong, Ruth and Moon, Taesup and Müller, Klaus-Robert and Samek, Wojciech},
	year = {2022},
	doi = {10.1007/978-3-031-04083-2_4},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {39--68},
}

@inproceedings{lundberg_unified_2017,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'17},
	title = {A unified approach to interpreting model predictions},
	isbn = {978-1-5108-6096-4},
	abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
	urldate = {2022-06-13},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Lundberg, Scott M. and Lee, Su-In},
	month = dec,
	year = {2017},
	pages = {4768--4777},
}

@book{rasmussen_gaussian_2008,
	address = {Cambridge, Mass.},
	edition = {3. print},
	series = {Adaptive computation and machine learning},
	title = {Gaussian processes for machine learning},
	isbn = {978-0-262-18253-9},
	language = {eng},
	publisher = {MIT Press},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
	year = {2008},
}

@article{cremer_machine-learning_2021,
	title = {A machine-learning based probabilistic perspective on dynamic security assessment},
	volume = {128},
	issn = {01420615},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0142061520307547},
	doi = {10.1016/j.ijepes.2020.106571},
	language = {en},
	urldate = {2022-05-22},
	journal = {International Journal of Electrical Power \& Energy Systems},
	author = {Cremer, Jochen L. and Strbac, Goran},
	month = jun,
	year = {2021},
	pages = {106571},
}

@inproceedings{tibshirani_conformal_2019,
	title = {Conformal {Prediction} {Under} {Covariate} {Shift}},
	volume = {32},
	url = {https://papers.nips.cc/paper/2019/hash/8fb21ee7a2207526da55a679f0332de2-Abstract.html},
	abstract = {We extend conformal prediction methodology beyond the case of exchangeable data. In particular, we show that a weighted version of conformal prediction can be used to compute distribution-free prediction intervals for problems in which the test and training covariate distributions differ, but the likelihood ratio between the two distributions is known---or, in practice, can be estimated accurately from a set of unlabeled data (test covariate points). Our weighted extension of conformal prediction also applies more broadly, to settings in which the data satisfies a certain weighted notion of exchangeability.  We discuss other potential applications of our new conformal methodology, including latent variable and missing data problems.},
	urldate = {2022-05-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tibshirani, Ryan J and Foygel Barber, Rina and Candes, Emmanuel and Ramdas, Aaditya},
	year = {2019},
}

@book{vovk_algorithmic_2005,
	address = {New York},
	title = {Algorithmic learning in a random world},
	isbn = {978-0-387-00152-4 978-0-387-25061-8},
	publisher = {Springer},
	author = {Vovk, Vladimir and Gammerman, A. and Shafer, Glenn},
	year = {2005},
	keywords = {Algorithms, Prediction theory, Stochastic processes},
}

@misc{noauthor_integral_2022,
	title = {Integral equation model using {Adomian} decomposition},
	copyright = {MIT},
	url = {https://github.com/epirecipes/sir-julia/blob/5005a44d1604c537573e9da52b7424cfd842c1aa/markdown/adomian/adomian.md},
	abstract = {Various implementations of the classical SIR model in Julia},
	urldate = {2022-05-02},
	publisher = {epirecipes},
	month = may,
	year = {2022},
	note = {original-date: 2020-04-24T16:43:58Z},
}

@article{wazwaz_comparison_1998,
	title = {A comparison between {Adomian} decomposition method and {Taylor} series method in the series solutions},
	volume = {97},
	issn = {00963003},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0096300397101278},
	doi = {10.1016/S0096-3003(97)10127-8},
	language = {en},
	number = {1},
	urldate = {2022-05-02},
	journal = {Applied Mathematics and Computation},
	author = {Wazwaz, Abdul-Majid},
	month = dec,
	year = {1998},
	pages = {37--44},
}

@article{repaci_nonlinear_1990,
	title = {Nonlinear dynamical systems: {On} the accuracy of adomian's decomposition method},
	volume = {3},
	issn = {08939659},
	shorttitle = {Nonlinear dynamical systems},
	url = {https://linkinghub.elsevier.com/retrieve/pii/089396599090042A},
	doi = {10.1016/0893-9659(90)90042-A},
	language = {en},
	number = {4},
	urldate = {2022-05-02},
	journal = {Applied Mathematics Letters},
	author = {Rèpaci, Antonino},
	year = {1990},
	pages = {35--39},
}

@article{hockenberry_evaluation_2004,
	title = {Evaluation of {Uncertainty} in {Dynamic} {Simulations} of {Power} {System} {Models}: {The} {Probabilistic} {Collocation} {Method}},
	volume = {19},
	issn = {0885-8950},
	shorttitle = {Evaluation of {Uncertainty} in {Dynamic} {Simulations} of {Power} {System} {Models}},
	url = {http://ieeexplore.ieee.org/document/1318685/},
	doi = {10.1109/TPWRS.2004.831689},
	language = {en},
	number = {3},
	urldate = {2022-05-02},
	journal = {IEEE Transactions on Power Systems},
	author = {Hockenberry, J.R. and Lesieutre, B.C.},
	month = aug,
	year = {2004},
	pages = {1483--1491},
}

@book{noauthor_1110-2019_2020,
	address = {S.l.},
	title = {1110-2019 - {IEEE} {Guide} for {Synchronous} {Generator} {Modeling} {Practices} and {Parameter} {Verification} with {Applications} in {Power} {System} {Stability} {Analyses}},
	isbn = {978-1-5044-6290-7},
	url = {https://ieeexplore.ieee.org/servlet/opac?punumber=9020272},
	language = {Undetermined},
	urldate = {2022-05-02},
	publisher = {IEEE},
	year = {2020},
	note = {OCLC: 1156330748},
}

@article{dommel_fast_1972,
	title = {Fast {Transient} {Stability} {Solutions}},
	volume = {PAS-91},
	issn = {0018-9510},
	url = {http://ieeexplore.ieee.org/document/4074900/},
	doi = {10.1109/TPAS.1972.293341},
	number = {4},
	urldate = {2022-05-02},
	journal = {IEEE Transactions on Power Apparatus and Systems},
	author = {Dommel, H. W. and Sato, N.},
	month = jul,
	year = {1972},
	pages = {1643--1650},
}

@article{chih-wen_liu_new_2000,
	title = {New methods for computing power system dynamic response for real-time transient stability prediction},
	volume = {47},
	issn = {10577122},
	url = {http://ieeexplore.ieee.org/document/841915/},
	doi = {10.1109/81.841915},
	number = {3},
	urldate = {2022-05-02},
	journal = {IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications},
	author = {{Chih-Wen Liu} and Thorp, J.S.},
	month = mar,
	year = {2000},
	pages = {324--337},
}

@article{moulin_support_2004,
	title = {Support {Vector} {Machines} for {Transient} {Stability} {Analysis} of {Large}-{Scale} {Power} {Systems}},
	volume = {19},
	issn = {0885-8950},
	url = {http://ieeexplore.ieee.org/document/1294987/},
	doi = {10.1109/TPWRS.2004.826018},
	language = {en},
	number = {2},
	urldate = {2022-05-02},
	journal = {IEEE Transactions on Power Systems},
	author = {Moulin, L.S. and daSilva, A.P.A. and El-Sharkawi, M.A. and MarksII, R.J.},
	month = may,
	year = {2004},
	pages = {818--825},
}

@inproceedings{duan_application_2015,
	address = {Eindhoven, Netherlands},
	title = {Application of the adomian decomposition method for semi-analytic solutions of power system differential algebraic equations},
	isbn = {978-1-4799-7693-5},
	url = {http://ieeexplore.ieee.org/document/7232370/},
	doi = {10.1109/PTC.2015.7232370},
	urldate = {2022-05-02},
	booktitle = {2015 {IEEE} {Eindhoven} {PowerTech}},
	publisher = {IEEE},
	author = {Duan, Nan and Sun, Kai},
	month = jun,
	year = {2015},
	pages = {1--6},
}

@article{korda_power_2018,
	title = {Power grid transient stabilization using {Koopman} model predictive control},
	volume = {51},
	issn = {24058963},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2405896318334372},
	doi = {10.1016/j.ifacol.2018.11.718},
	language = {en},
	number = {28},
	urldate = {2022-04-04},
	journal = {IFAC-PapersOnLine},
	author = {Korda, Milan and Susuki, Yoshihiko and Mezić, Igor},
	year = {2018},
	pages = {297--302},
}

@article{lusch_deep_2018,
	title = {Deep learning for universal linear embeddings of nonlinear dynamics},
	volume = {9},
	issn = {2041-1723},
	url = {http://www.nature.com/articles/s41467-018-07210-0},
	doi = {10.1038/s41467-018-07210-0},
	language = {en},
	number = {1},
	urldate = {2022-04-04},
	journal = {Nature Communications},
	author = {Lusch, Bethany and Kutz, J. Nathan and Brunton, Steven L.},
	month = dec,
	year = {2018},
	pages = {4950},
}

@article{williams_datadriven_2015,
	title = {A {Data}–{Driven} {Approximation} of the {Koopman} {Operator}: {Extending} {Dynamic} {Mode} {Decomposition}},
	volume = {25},
	issn = {0938-8974, 1432-1467},
	shorttitle = {A {Data}–{Driven} {Approximation} of the {Koopman} {Operator}},
	url = {http://link.springer.com/10.1007/s00332-015-9258-5},
	doi = {10.1007/s00332-015-9258-5},
	language = {en},
	number = {6},
	urldate = {2022-04-04},
	journal = {Journal of Nonlinear Science},
	author = {Williams, Matthew O. and Kevrekidis, Ioannis G. and Rowley, Clarence W.},
	month = dec,
	year = {2015},
	pages = {1307--1346},
}

@article{noack_recursive_2016,
	title = {Recursive dynamic mode decomposition of transient and post-transient wake flows},
	volume = {809},
	issn = {0022-1120, 1469-7645},
	url = {https://www.cambridge.org/core/product/identifier/S0022112016006789/type/journal_article},
	doi = {10.1017/jfm.2016.678},
	abstract = {A novel data-driven modal decomposition of fluid flow is proposed, comprising key features of proper orthogonal decomposition (POD) and dynamic mode decomposition (DMD). The first mode is the normalized real or imaginary part of the DMD mode that minimizes the time-averaged residual. The
              
                
                  
                  \$N\$
                
              
              th mode is defined recursively in an analogous manner based on the residual of an expansion using the first
              
                
                  
                  \$N-1\$
                
              
              modes. The resulting recursive DMD (RDMD) modes are orthogonal by construction, retain pure frequency content and aim at low residual. Recursive DMD is applied to transient cylinder wake data and is benchmarked against POD and optimized DMD (Chen
              et al.
              ,
              J. Nonlinear Sci.
              , vol. 22, 2012, pp. 887–915) for the same snapshot sequence. Unlike POD modes, RDMD structures are shown to have purer frequency content while retaining a residual of comparable order to POD. In contrast to DMD, with exponentially growing or decaying oscillatory amplitudes, RDMD clearly identifies initial, maximum and final fluctuation levels. Intriguingly, RDMD outperforms both POD and DMD in the limit-cycle resolution from the same snapshots. Robustness of these observations is demonstrated for other parameters of the cylinder wake and for a more complex wake behind three rotating cylinders. Recursive DMD is proposed as an attractive alternative to POD and DMD for empirical Galerkin models, in particular for nonlinear transient dynamics.},
	language = {en},
	urldate = {2022-04-04},
	journal = {Journal of Fluid Mechanics},
	author = {Noack, Bernd R. and Stankiewicz, Witold and Morzyński, Marek and Schmid, Peter J.},
	month = dec,
	year = {2016},
	pages = {843--872},
}

@article{susuki_applied_2016,
	title = {Applied {Koopman} operator theory for power systems technology},
	volume = {7},
	issn = {2185-4106},
	url = {https://www.jstage.jst.go.jp/article/nolta/7/4/7_430/_article},
	doi = {10.1587/nolta.7.430},
	language = {en},
	number = {4},
	urldate = {2022-04-04},
	journal = {Nonlinear Theory and Its Applications, IEICE},
	author = {Susuki, Yoshihiko and Mezic, Igor and Raak, Fredrik and Hikihara, Takashi},
	year = {2016},
	pages = {430--459},
}

@article{hiskens_ieee_nodate,
	title = {{IEEE} {PES} {Task} {Force} on {Benchmark} {Systems} for {Stability} {Controls}},
	abstract = {This report summarizes a study of an IEEE 10-generator, 39-bus system. Three types of analysis were performed: load ﬂow, small disturbance analysis, and dynamic simulation. All analysis was carried out using MATLAB, and this report’s objective is to demonstrate how to use MATLAB to obtain results that are comparable to benchmark results from other analysis methods. Data from other methods may be found on the website www.sel.eesc.usp.br/ieee.},
	language = {en},
	author = {Hiskens, Ian},
	pages = {23},
}

@article{zhou_novel_2017,
	title = {A {Novel} {Method} of {Polynomial} {Approximation} for {Parametric} {Problems} in {Power} {Systems}},
	volume = {32},
	issn = {0885-8950, 1558-0679},
	url = {http://ieeexplore.ieee.org/document/7728091/},
	doi = {10.1109/TPWRS.2016.2623820},
	number = {4},
	urldate = {2022-03-31},
	journal = {IEEE Transactions on Power Systems},
	author = {Zhou, Yongzhi and Wu, Hao and Gu, Chenghong and Song, Yonghua},
	month = jul,
	year = {2017},
	pages = {3298--3307},
}

@article{xia_galerkin_2019,
	title = {A {Galerkin} {Method}-{Based} {Polynomial} {Approximation} for {Parametric} {Problems} in {Power} {System} {Transient} {Analysis}},
	volume = {34},
	issn = {0885-8950, 1558-0679},
	url = {https://ieeexplore.ieee.org/document/8520781/},
	doi = {10.1109/TPWRS.2018.2879367},
	number = {2},
	urldate = {2022-03-31},
	journal = {IEEE Transactions on Power Systems},
	author = {Xia, Bingqing and Wu, Hao and Qiu, Yiwei and Lou, Boliang and Song, Yonghua},
	month = mar,
	year = {2019},
	pages = {1620--1629},
}

@inproceedings{sahoo_learning_2018,
	title = {Learning {Equations} for {Extrapolation} and {Control}},
	url = {https://proceedings.mlr.press/v80/sahoo18a.html},
	abstract = {We present an approach to identify concise equations from data using a shallow neural network approach. In contrast to ordinary black-box regression, this approach allows understanding functional relations and generalizing them from observed data to unseen parts of the parameter space. We show how to extend the class of learnable equations for a recently proposed equation learning network to include divisions, and we improve the learning and model selection strategy to be useful for challenging real-world data. For systems governed by analytical expressions, our method can in many cases identify the true underlying equation and extrapolate to unseen domains. We demonstrate its effectiveness by experiments on a cart-pendulum system, where only 2 random rollouts are required to learn the forward dynamics and successfully achieve the swing-up task.},
	language = {en},
	urldate = {2022-03-23},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Sahoo, Subham and Lampert, Christoph and Martius, Georg},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {4442--4450},
}

@article{zhou_optimal_2021,
	title = {The {Optimal} {Power} {Flow} {Operator}: {Theory} and {Computation}},
	volume = {8},
	issn = {2325-5870, 2372-2533},
	shorttitle = {The {Optimal} {Power} {Flow} {Operator}},
	url = {https://ieeexplore.ieee.org/document/9292090/},
	doi = {10.1109/TCNS.2020.3044258},
	number = {2},
	urldate = {2022-03-23},
	journal = {IEEE Transactions on Control of Network Systems},
	author = {Zhou, Fengyu and Anderson, James and Low, Steven H.},
	month = jun,
	year = {2021},
	pages = {1010--1022},
}

@article{pagnier_embedding_2021,
	title = {Embedding {Power} {Flow} into {Machine} {Learning} for {Parameter} and {State} {Estimation}},
	url = {http://arxiv.org/abs/2103.14251},
	abstract = {Modern state and parameter estimations in power systems consist of two stages: the outer problem of minimizing the mismatch between network observation and prediction over the network parameters, and the inner problem of predicting the system state for given values of the parameters. The standard solution of the combined problem is iterative: (a) set the parameters, e.g. to priors on the power line characteristics, (b) map input observation to prediction of the output, (c) compute the mismatch between predicted and observed output, (d) make a gradient descent step in the space of parameters to minimize the mismatch, and loop back to (a). We show how modern Machine Learning (ML), and specifically training guided by automatic differentiation, allows to resolve the iterative loop more efficiently. Moreover, we extend the scheme to the case of incomplete observations, where Phasor Measurement Units (reporting real and reactive powers, voltage and phase) are available only at the generators (PV buses), while loads (PQ buses) report (via SCADA controls) only active and reactive powers. Considering it from the implementation perspective, our methodology of resolving the parameter and state estimation problem can be viewed as embedding of the Power Flow (PF) solver into the training loop of the Machine Learning framework (PyTorch, in this study). We argue that this embedding can help to resolve high-level optimization problems in power system operations and planning.},
	urldate = {2022-03-23},
	journal = {arXiv:2103.14251 [physics]},
	author = {Pagnier, Laurent and Chertkov, Michael},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.14251},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control, Physics - Physics and Society},
}

@book{sutton_reinforcement_2018,
	address = {Cambridge, Massachusetts},
	edition = {Second edition},
	series = {Adaptive computation and machine learning series},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-03924-6},
	shorttitle = {Reinforcement learning},
	abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
	publisher = {The MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {2018},
	keywords = {Reinforcement learning},
}

@inproceedings{bai_neural_2021,
	title = {Neural {Deep} {Equilibrium} {Solvers}},
	url = {https://openreview.net/forum?id=B0oHOwT5ENL},
	abstract = {A deep equilibrium (DEQ) model abandons traditional depth by solving for the fixed point of a single nonlinear layer \$f\_{\textbackslash}theta\$. This structure enables decoupling the internal structure of the...},
	language = {en},
	urldate = {2022-03-23},
	author = {Bai, Shaojie and Koltun, Vladlen and Kolter, J. Zico},
	month = sep,
	year = {2021},
}

@inproceedings{jiang_assessing_2021,
	title = {Assessing {Generalization} of {SGD} via {Disagreement}},
	url = {https://openreview.net/forum?id=WvOGCEAQhxl},
	abstract = {We empirically show that the test error of deep networks can be estimated by training the same architecture on the same training set but with two different runs of Stochastic Gradient Descent (SGD)...},
	language = {en},
	urldate = {2022-03-23},
	author = {Jiang, Yiding and Nagarajan, Vaishnavh and Baek, Christina and Kolter, J. Zico},
	month = sep,
	year = {2021},
}

@article{kirsch_note_2022,
	title = {A {Note} on "{Assessing} {Generalization} of {SGD} via {Disagreement}"},
	url = {http://arxiv.org/abs/2202.01851},
	abstract = {Jiang et al. (2021) give empirical evidence that the average test error of deep neural networks can be estimated via the prediction disagreement of two separately trained networks. They also provide a theoretical explanation that this 'Generalization Disagreement Equality' follows from the well-calibrated nature of deep ensembles under the notion of a proposed 'class-aggregated calibration'. In this paper we show that the approach suggested might be impractical because a deep ensemble's calibration deteriorates under distribution shift, which is exactly when the coupling of test error and disagreement would be of practical value. We present both theoretical and experimental evidence, re-deriving the theoretical statements using a simple Bayesian perspective and show them to be straightforward and more generic: they apply to any discriminative model -- not only ensembles whose members output one-hot class predictions. The proposed calibration metrics are also equivalent to two metrics introduced by Nixon et al. (2019): 'ACE' and 'SCE'.},
	urldate = {2022-03-23},
	journal = {arXiv:2202.01851 [cs]},
	author = {Kirsch, Andreas and Gal, Yarin},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.01851},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{hyman_kuramoto-sivashinsky_1986,
	title = {The {Kuramoto}-{Sivashinsky} equation: {A} bridge between {PDE}'{S} and dynamical systems},
	volume = {18},
	issn = {01672789},
	shorttitle = {The {Kuramoto}-{Sivashinsky} equation},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0167278986901661},
	doi = {10.1016/0167-2789(86)90166-1},
	language = {en},
	number = {1-3},
	urldate = {2022-03-23},
	journal = {Physica D: Nonlinear Phenomena},
	author = {Hyman, James M. and Nicolaenko, Basil},
	month = jan,
	year = {1986},
	pages = {113--126},
}

@article{dorfler_critical_2011,
	title = {On the {Critical} {Coupling} for {Kuramoto} {Oscillators}},
	volume = {10},
	issn = {1536-0040},
	url = {http://epubs.siam.org/doi/10.1137/10081530X},
	doi = {10.1137/10081530X},
	language = {en},
	number = {3},
	urldate = {2022-03-23},
	journal = {SIAM Journal on Applied Dynamical Systems},
	author = {Dörfler, Florian and Bullo, Francesco},
	month = jan,
	year = {2011},
	pages = {1070--1099},
}

@article{wang_eigenvector_2021,
	title = {On the eigenvector bias of {Fourier} feature networks: {From} regression to solving multi-scale {PDEs} with physics-informed neural networks},
	volume = {384},
	issn = {00457825},
	shorttitle = {On the eigenvector bias of {Fourier} feature networks},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0045782521002759},
	doi = {10.1016/j.cma.2021.113938},
	language = {en},
	urldate = {2022-03-23},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Wang, Sifan and Wang, Hanwen and Perdikaris, Paris},
	month = oct,
	year = {2021},
	pages = {113938},
}

@article{beck_overview_2021,
	title = {An overview on deep learning-based approximation methods for partial differential equations},
	url = {http://arxiv.org/abs/2012.12348},
	abstract = {It is one of the most challenging problems in applied mathematics to approximatively solve high-dimensional partial differential equations (PDEs). Recently, several deep learning-based approximation algorithms for attacking this problem have been proposed and tested numerically on a number of examples of high-dimensional PDEs. This has given rise to a lively field of research in which deep learning-based methods and related Monte Carlo methods are applied to the approximation of high-dimensional PDEs. In this article we offer an introduction to this field of research, we review some of the main ideas of deep learning-based approximation methods for PDEs, we revisit one of the central mathematical results for deep neural network approximations for PDEs, and we provide an overview of the recent literature in this area of research.},
	urldate = {2022-03-23},
	journal = {arXiv:2012.12348 [cs, math]},
	author = {Beck, Christian and Hutzenthaler, Martin and Jentzen, Arnulf and Kuckuck, Benno},
	month = mar,
	year = {2021},
	note = {arXiv: 2012.12348},
	keywords = {65M99 (Primary), 35-02, 65-02, 68T07 (Secondary), Computer Science - Machine Learning, Mathematics - Numerical Analysis},
}

@article{yao_pyhessian_2020,
	title = {{PyHessian}: {Neural} {Networks} {Through} the {Lens} of the {Hessian}},
	shorttitle = {{PyHessian}},
	url = {http://arxiv.org/abs/1912.07145},
	abstract = {We present PYHESSIAN, a new scalable framework that enables fast computation of Hessian (i.e., second-order derivative) information for deep neural networks. PYHESSIAN enables fast computations of the top Hessian eigenvalues, the Hessian trace, and the full Hessian eigenvalue/spectral density, and it supports distributed-memory execution on cloud/supercomputer systems and is available as open source. This general framework can be used to analyze neural network models, including the topology of the loss landscape (i.e., curvature information) to gain insight into the behavior of different models/optimizers. To illustrate this, we analyze the effect of residual connections and Batch Normalization layers on the trainability of neural networks. One recent claim, based on simpler first-order analysis, is that residual connections and Batch Normalization make the loss landscape smoother, thus making it easier for Stochastic Gradient Descent to converge to a good solution. Our extensive analysis shows new finer-scale insights, demonstrating that, while conventional wisdom is sometimes validated, in other cases it is simply incorrect. In particular, we find that Batch Normalization does not necessarily make the loss landscape smoother, especially for shallower networks.},
	urldate = {2022-03-23},
	journal = {arXiv:1912.07145 [cs, math]},
	author = {Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael},
	month = mar,
	year = {2020},
	note = {arXiv: 1912.07145},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
}

@article{wang_when_2022,
	title = {When and why {PINNs} fail to train: {A} neural tangent kernel perspective},
	volume = {449},
	issn = {00219991},
	shorttitle = {When and why {PINNs} fail to train},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S002199912100663X},
	doi = {10.1016/j.jcp.2021.110768},
	language = {en},
	urldate = {2022-03-23},
	journal = {Journal of Computational Physics},
	author = {Wang, Sifan and Yu, Xinling and Perdikaris, Paris},
	month = jan,
	year = {2022},
	pages = {110768},
}

@article{hiskens_inverse_2001,
	title = {Inverse {Problems} in {Power} {Systems}},
	abstract = {Large disturbances in power systems often initiate complex interactions between continuous dynamics and discrete events. Such behaviour can be modeled in a systematic way by a set of diﬀerential-algebraic equations, modiﬁed to incorporate impulse (state reset) action and constraint switching. The paper presents a practical object-oriented approach to implementing the DAIS model. The systematic nature of the DAIS model enables eﬃcient computation of trajectory sensitivities, which in turn facilitate algorithms for solving inverse problems. The paper outlines a number of inverse problems, including parameter uncertainty, parameter estimation, boundary value problems, bordercollision bifurcations, locating critically stable trajectories, and optimal control.},
	language = {en},
	author = {Hiskens, I A},
	year = {2001},
	pages = {10},
}

@misc{poli_torchdyn_nodate,
	title = {{TorchDyn}: {Implicit} {Models} and {Neural} {Numerical} {Methods} in {PyTorch}\vphantom{\{}\}},
	url = {https://github.com/DiffEqML/torchdyn},
	urldate = {2022-03-23},
	author = {Poli, Michael and Massaroli, Stefano and Yamashita, Atsushi and Asama, Hajime and Park, Jinkyoo and Ermon, Stefano},
}

@article{kaheman_automatic_2020,
	title = {Automatic {Differentiation} to {Simultaneously} {Identify} {Nonlinear} {Dynamics} and {Extract} {Noise} {Probability} {Distributions} from {Data}},
	url = {http://arxiv.org/abs/2009.08810},
	abstract = {The sparse identification of nonlinear dynamics (SINDy) is a regression framework for the discovery of parsimonious dynamic models and governing equations from time-series data. As with all system identification methods, noisy measurements compromise the accuracy and robustness of the model discovery procedure. In this work, we develop a variant of the SINDy algorithm that integrates automatic differentiation and recent time-stepping constrained motivated by Rudy et al. for simultaneously (i) denoising the data, (ii) learning and parametrizing the noise probability distribution, and (iii) identifying the underlying parsimonious dynamical system responsible for generating the time-series data. Thus within an integrated optimization framework, noise can be separated from signal, resulting in an architecture that is approximately twice as robust to noise as state-of-the-art methods, handling as much as 40\% noise on a given time-series signal and explicitly parametrizing the noise probability distribution. We demonstrate this approach on several numerical examples, from Lotka-Volterra models to the spatio-temporal Lorenz 96 model. Further, we show the method can identify a diversity of probability distributions including Gaussian, uniform, Gamma, and Rayleigh.},
	urldate = {2022-03-23},
	journal = {arXiv:2009.08810 [cs, eess, math]},
	author = {Kaheman, Kadierdan and Brunton, Steven L. and Kutz, J. Nathan},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.08810},
	keywords = {93B30, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Mathematics - Dynamical Systems},
}

@misc{kidger_diffrax_nodate,
	title = {Diffrax},
	url = {https://docs.kidger.site/diffrax/},
	urldate = {2022-03-23},
	author = {Kidger, Patrick},
}

@inproceedings{garipov_loss_2018,
	title = {Loss {Surfaces}, {Mode} {Connectivity}, and {Fast} {Ensembling} of {DNNs}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/file/be3087e74e9100d4bc4c6268cdbe8456-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry P and Wilson, Andrew G},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
}

@inproceedings{wilson_bayesian_2020,
	title = {Bayesian {Deep} {Learning} and a {Probabilistic} {Perspective} of {Generalization}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/file/322f62469c5e3c7dc3e58f5a4d1ea399-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Wilson, Andrew G and Izmailov, Pavel},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {4697--4708},
}

@article{kalia_learning_2021,
	title = {Learning normal form autoencoders for data-driven discovery of universal,parameter-dependent governing equations},
	url = {http://arxiv.org/abs/2106.05102},
	abstract = {Complex systems manifest a small number of instabilities and bifurcations that are canonical in nature, resulting in universal pattern forming characteristics as a function of some parametric dependence. Such parametric instabilities are mathematically characterized by their universal un-foldings, or normal form dynamics, whereby a parsimonious model can be used to represent the dynamics. Although center manifold theory guarantees the existence of such low-dimensional normal forms, finding them has remained a long standing challenge. In this work, we introduce deep learning autoencoders to discover coordinate transformations that capture the underlying parametric dependence of a dynamical system in terms of its canonical normal form, allowing for a simple representation of the parametric dependence and bifurcation structure. The autoencoder constrains the latent variable to adhere to a given normal form, thus allowing it to learn the appropriate coordinate transformation. We demonstrate the method on a number of example problems, showing that it can capture a diverse set of normal forms associated with Hopf, pitchfork, transcritical and/or saddle node bifurcations. This method shows how normal forms can be leveraged as canonical and universal building blocks in deep learning approaches for model discovery and reduced-order modeling.},
	urldate = {2022-03-17},
	journal = {arXiv:2106.05102 [cs, math]},
	author = {Kalia, Manu and Brunton, Steven L. and Meijer, Hil G. E. and Brune, Christoph and Kutz, J. Nathan},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.05102},
	keywords = {37G05, Computer Science - Machine Learning, Mathematics - Dynamical Systems},
}

@article{bartlett_spectrally-normalized_2017,
	title = {Spectrally-normalized margin bounds for neural networks},
	url = {http://arxiv.org/abs/1706.08498},
	abstract = {This paper presents a margin-based multiclass generalization bound for neural networks that scales with their margin-normalized "spectral complexity": their Lipschitz constant, meaning the product of the spectral norms of the weight matrices, times a certain correction factor. This bound is empirically investigated for a standard AlexNet network trained with SGD on the mnist and cifar10 datasets, with both original and random labels; the bound, the Lipschitz constants, and the excess risks are all in direct correlation, suggesting both that SGD selects predictors whose complexity scales with the difficulty of the learning task, and secondly that the presented bound is sensitive to this complexity.},
	urldate = {2022-03-17},
	journal = {arXiv:1706.08498 [cs, stat]},
	author = {Bartlett, Peter and Foster, Dylan J. and Telgarsky, Matus},
	month = dec,
	year = {2017},
	note = {arXiv: 1706.08498},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{smith_origin_2021,
	title = {On the {Origin} of {Implicit} {Regularization} in {Stochastic} {Gradient} {Descent}},
	url = {http://arxiv.org/abs/2101.12176},
	abstract = {For infinitesimal learning rates, stochastic gradient descent (SGD) follows the path of gradient flow on the full batch loss function. However moderately large learning rates can achieve higher test accuracies, and this generalization benefit is not explained by convergence bounds, since the learning rate which maximizes test accuracy is often larger than the learning rate which minimizes training loss. To interpret this phenomenon we prove that for SGD with random shuffling, the mean SGD iterate also stays close to the path of gradient flow if the learning rate is small and finite, but on a modified loss. This modified loss is composed of the original loss function and an implicit regularizer, which penalizes the norms of the minibatch gradients. Under mild assumptions, when the batch size is small the scale of the implicit regularization term is proportional to the ratio of the learning rate to the batch size. We verify empirically that explicitly including the implicit regularizer in the loss can enhance the test accuracy when the learning rate is small.},
	urldate = {2022-03-17},
	journal = {arXiv:2101.12176 [cs, stat]},
	author = {Smith, Samuel L. and Dherin, Benoit and Barrett, David G. T. and De, Soham},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.12176},
	keywords = {Computer Science - Machine Learning, NeurIPS 2021 Workshop: ML for Physics and Physics for ML, Statistics - Machine Learning},
}

@article{kelly_learning_2020,
	title = {Learning {Differential} {Equations} that are {Easy} to {Solve}},
	url = {http://arxiv.org/abs/2007.04504},
	abstract = {Differential equations parameterized by neural networks become expensive to solve numerically as training progresses. We propose a remedy that encourages learned dynamics to be easier to solve. Specifically, we introduce a differentiable surrogate for the time cost of standard numerical solvers, using higher-order derivatives of solution trajectories. These derivatives are efficient to compute with Taylor-mode automatic differentiation. Optimizing this additional objective trades model performance against the time cost of solving the learned dynamics. We demonstrate our approach by training substantially faster, while nearly as accurate, models in supervised classification, density estimation, and time-series modelling tasks.},
	urldate = {2022-03-17},
	journal = {arXiv:2007.04504 [cs, stat]},
	author = {Kelly, Jacob and Bettencourt, Jesse and Johnson, Matthew James and Duvenaud, David},
	month = oct,
	year = {2020},
	note = {arXiv: 2007.04504},
	keywords = {Computer Science - Machine Learning, NeurIPS 2021 Workshop: ML for Physics and Physics for ML, Statistics - Machine Learning},
}

@article{finlay_how_2020,
	title = {How to train your neural {ODE}: the world of {Jacobian} and kinetic regularization},
	shorttitle = {How to train your neural {ODE}},
	url = {http://arxiv.org/abs/2002.02798},
	abstract = {Training neural ODEs on large datasets has not been tractable due to the necessity of allowing the adaptive numerical ODE solver to refine its step size to very small values. In practice this leads to dynamics equivalent to many hundreds or even thousands of layers. In this paper, we overcome this apparent difficulty by introducing a theoretically-grounded combination of both optimal transport and stability regularizations which encourage neural ODEs to prefer simpler dynamics out of all the dynamics that solve a problem well. Simpler dynamics lead to faster convergence and to fewer discretizations of the solver, considerably decreasing wall-clock time without loss in performance. Our approach allows us to train neural ODE-based generative models to the same performance as the unregularized dynamics, with significant reductions in training time. This brings neural ODEs closer to practical relevance in large-scale applications.},
	urldate = {2022-03-17},
	journal = {arXiv:2002.02798 [cs, stat]},
	author = {Finlay, Chris and Jacobsen, Jörn-Henrik and Nurbekyan, Levon and Oberman, Adam M.},
	month = jun,
	year = {2020},
	note = {arXiv: 2002.02798},
	keywords = {Computer Science - Machine Learning, NeurIPS 2021 Workshop: ML for Physics and Physics for ML, Statistics - Machine Learning},
}

@article{battaglia_relational_2018,
	title = {Relational inductive biases, deep learning, and graph networks},
	url = {http://arxiv.org/abs/1806.01261},
	abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
	urldate = {2022-03-17},
	journal = {arXiv:1806.01261 [cs, stat]},
	author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
	month = oct,
	year = {2018},
	note = {arXiv: 1806.01261},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{battaglia_interaction_2016,
	title = {Interaction {Networks} for {Learning} about {Objects}, {Relations} and {Physics}},
	url = {http://arxiv.org/abs/1612.00222},
	abstract = {Reasoning about objects, relations, and physics is central to human intelligence, and a key goal of artificial intelligence. Here we introduce the interaction network, a model which can reason about how objects in complex systems interact, supporting dynamical predictions, as well as inferences about the abstract properties of the system. Our model takes graphs as input, performs object- and relation-centric reasoning in a way that is analogous to a simulation, and is implemented using deep neural networks. We evaluate its ability to reason about several challenging physical domains: n-body problems, rigid-body collision, and non-rigid dynamics. Our results show it can be trained to accurately simulate the physical trajectories of dozens of objects over thousands of time steps, estimate abstract quantities such as energy, and generalize automatically to systems with different numbers and configurations of objects and relations. Our interaction network implementation is the first general-purpose, learnable physics engine, and a powerful general framework for reasoning about object and relations in a wide variety of complex real-world domains.},
	urldate = {2022-03-17},
	journal = {arXiv:1612.00222 [cs]},
	author = {Battaglia, Peter W. and Pascanu, Razvan and Lai, Matthew and Rezende, Danilo and Kavukcuoglu, Koray},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.00222},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{mattey_novel_2022,
	title = {A novel sequential method to train physics informed neural networks for {Allen} {Cahn} and {Cahn} {Hilliard} equations},
	volume = {390},
	issn = {00457825},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0045782521006939},
	doi = {10.1016/j.cma.2021.114474},
	language = {en},
	urldate = {2022-03-17},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Mattey, Revanth and Ghosh, Susanta},
	month = feb,
	year = {2022},
	keywords = {PINNs},
	pages = {114474},
}

@article{krishnapriyan_characterizing_2021,
	title = {Characterizing possible failure modes in physics-informed neural networks},
	url = {http://arxiv.org/abs/2109.01050},
	abstract = {Recent work in scientific machine learning has developed so-called physics-informed neural network (PINN) models. The typical approach is to incorporate physical domain knowledge as soft constraints on an empirical loss function and use existing machine learning methodologies to train the model. We demonstrate that, while existing PINN methodologies can learn good models for relatively trivial problems, they can easily fail to learn relevant physical phenomena for even slightly more complex problems. In particular, we analyze several distinct situations of widespread physical interest, including learning differential equations with convection, reaction, and diffusion operators. We provide evidence that the soft regularization in PINNs, which involves PDE-based differential operators, can introduce a number of subtle problems, including making the problem more ill-conditioned. Importantly, we show that these possible failure modes are not due to the lack of expressivity in the NN architecture, but that the PINN's setup makes the loss landscape very hard to optimize. We then describe two promising solutions to address these failure modes. The first approach is to use curriculum regularization, where the PINN's loss term starts from a simple PDE regularization, and becomes progressively more complex as the NN gets trained. The second approach is to pose the problem as a sequence-to-sequence learning task, rather than learning to predict the entire space-time at once. Extensive testing shows that we can achieve up to 1-2 orders of magnitude lower error with these methods as compared to regular PINN training.},
	urldate = {2022-03-17},
	journal = {arXiv:2109.01050 [physics]},
	author = {Krishnapriyan, Aditi S. and Gholami, Amir and Zhe, Shandian and Kirby, Robert M. and Mahoney, Michael W.},
	month = nov,
	year = {2021},
	note = {arXiv: 2109.01050},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Numerical Analysis, PINNs, Physics - Computational Physics},
}

@inproceedings{cranmer_discovering_2020,
	title = {Discovering {Symbolic} {Models} from {Deep} {Learning} with {Inductive} {Biases}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/file/c9f2f917078bd2db12f23c3b413d9cba-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Cranmer, Miles and Sanchez Gonzalez, Alvaro and Battaglia, Peter and Xu, Rui and Cranmer, Kyle and Spergel, David and Ho, Shirley},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	keywords = {NeurIPS 2021 Workshop: ML for Physics and Physics for ML},
	pages = {17429--17442},
}

@article{schmidt_distilling_2009,
	title = {Distilling {Free}-{Form} {Natural} {Laws} from {Experimental} {Data}},
	volume = {324},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.1165893},
	doi = {10.1126/science.1165893},
	abstract = {For centuries, scientists have attempted to identify and document analytical laws that underlie physical phenomena in nature. Despite the prevalence of computing power, the process of finding natural laws and their corresponding equations has resisted automation. A key challenge to finding analytic relations automatically is defining algorithmically what makes a correlation in observed data important and insightful. We propose a principle for the identification of nontriviality. We demonstrated this approach by automatically searching motion-tracking data captured from various physical systems, ranging from simple harmonic oscillators to chaotic double-pendula. Without any prior knowledge about physics, kinematics, or geometry, the algorithm discovered Hamiltonians, Lagrangians, and other laws of geometric and momentum conservation. The discovery rate accelerated as laws found for simpler systems were used to bootstrap explanations for more complex systems, gradually uncovering the “alphabet” used to describe those systems.},
	language = {en},
	number = {5923},
	urldate = {2022-03-16},
	journal = {Science},
	author = {Schmidt, Michael and Lipson, Hod},
	month = apr,
	year = {2009},
	keywords = {NeurIPS 2021 Workshop: ML for Physics and Physics for ML},
	pages = {81--85},
}

@article{champion_unified_2020,
	title = {A {Unified} {Sparse} {Optimization} {Framework} to {Learn} {Parsimonious} {Physics}-{Informed} {Models} {From} {Data}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9194760/},
	doi = {10.1109/ACCESS.2020.3023625},
	urldate = {2022-03-16},
	journal = {IEEE Access},
	author = {Champion, Kathleen and Zheng, Peng and Aravkin, Aleksandr Y. and Brunton, Steven L. and Kutz, J. Nathan},
	year = {2020},
	keywords = {NeurIPS 2021 Workshop: ML for Physics and Physics for ML},
	pages = {169259--169271},
}

@article{blum_selection_1997,
	title = {Selection of relevant features and examples in machine learning},
	volume = {97},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370297000635},
	doi = {10.1016/S0004-3702(97)00063-5},
	language = {en},
	number = {1-2},
	urldate = {2022-03-13},
	journal = {Artificial Intelligence},
	author = {Blum, Avrim L. and Langley, Pat},
	month = dec,
	year = {1997},
	pages = {245--271},
}

@article{salcedo-sanz_feature_2018,
	title = {Feature selection in machine learning prediction systems for renewable energy applications},
	volume = {90},
	issn = {13640321},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364032118302168},
	doi = {10.1016/j.rser.2018.04.008},
	language = {en},
	urldate = {2022-03-13},
	journal = {Renewable and Sustainable Energy Reviews},
	author = {Salcedo-Sanz, S. and Cornejo-Bueno, L. and Prieto, L. and Paredes, D. and García-Herrera, R.},
	month = jul,
	year = {2018},
	pages = {728--741},
}

@book{montavon_neural_2012,
	address = {Berlin, Heidelberg},
	edition = {Second Edition},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Neural {Networks}: {Tricks} of the {Trade}},
	volume = {7700},
	isbn = {978-3-642-35288-1 978-3-642-35289-8},
	shorttitle = {Neural {Networks}},
	url = {http://link.springer.com/10.1007/978-3-642-35289-8},
	language = {en},
	urldate = {2022-03-13},
	publisher = {Springer Berlin Heidelberg},
	editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
	year = {2012},
	doi = {10.1007/978-3-642-35289-8},
}

@article{guyon_introduction_nodate,
	title = {An {Introduction} to {Variable} and {Feature} {Selection}},
	abstract = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better deﬁnition of the objective function, feature construction, feature ranking, multivariate feature selection, efﬁcient search methods, and feature validity assessment methods.},
	language = {en},
	author = {Guyon, Isabelle and Elisseeff, Andre},
	pages = {26},
}

@article{roelofs_meta-analysis_nodate,
	title = {A {Meta}-{Analysis} of {Overfitting} in {Machine} {Learning}},
	abstract = {We conduct the ﬁrst large meta-analysis of overﬁtting due to test set reuse in the machine learning community. Our analysis is based on over one hundred machine learning competitions hosted on the Kaggle platform over the course of several years. In each competition, numerous practitioners repeatedly evaluated their progress against a holdout set that forms the basis of a public ranking available throughout the competition. Performance on a separate test set used only once determined the ﬁnal ranking. By systematically comparing the public ranking with the ﬁnal ranking, we assess how much participants adapted to the holdout set over the course of a competition. Our study shows, somewhat surprisingly, little evidence of substantial overﬁtting. These ﬁndings speak to the robustness of the holdout method across different data domains, loss functions, model classes, and human analysts.},
	language = {en},
	author = {Roelofs, Rebecca and Shankar, Vaishaal and Recht, Benjamin and Fridovich-Keil, Sara and Hardt, Moritz and Miller, John and Schmidt, Ludwig},
	pages = {11},
}

@article{lee_deep_2018,
	title = {Deep {Neural} {Networks} as {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/1711.00165},
	abstract = {It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width. This correspondence enables exact Bayesian inference for infinite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network. In this work, we derive the exact equivalence between infinitely wide deep networks and GPs. We further develop a computationally efficient pipeline to compute the covariance function for these GPs. We then use the resulting GPs to perform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10. We observe that trained neural network accuracy approaches that of the corresponding GP with increasing layer width, and that the GP uncertainty is strongly correlated with trained network prediction error. We further find that test performance increases as finite-width trained networks are made wider and more similar to a GP, and thus that GP predictions typically outperform those of finite-width networks. Finally we connect the performance of these GPs to the recent theory of signal propagation in random neural networks.},
	urldate = {2022-02-16},
	journal = {arXiv:1711.00165 [cs, stat]},
	author = {Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S. and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
	month = mar,
	year = {2018},
	note = {arXiv: 1711.00165},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{physics_informed_machine_learning_zico_2021,
	title = {Zico {Kolter} - {Incorporating} physics and decision making into deep learning via implicit layers},
	url = {https://www.youtube.com/watch?v=3K9Et7t1XT8},
	abstract = {Talk starts at 2:57

Prof. Zico Kolter from Carnegie Mellon speaking in the Data-driven methods for science and engineering seminar on January 22, 2021.

For more information including past and upcoming talks, visit: http://www.databookuw.com/seminars/

Sign up for notifications of future talks: https://mailman11.u.washington.edu/ma...

Links:
https://zicokolter.com/
http://implicit-layers-tutorial.org/

Abstract: Despite their wide applicability, deep networks often fail to exactly capture simple "known" features of real-world data sets, such as those governed by physical laws.  In this talk, I'll present methods for integrating hard constraints, such as those associated with decision making, optimization problems, or physical simulation, directly into the predictions of a deep network.  Our tool for achieving this will be the use of so-called implicit layers in deep models: layers that are defined implicitly in terms of conditions we would like them to satisfy, rather than via an explicit computation graph.  I'll discuss how we can use these layers to embed (exact) physical constraints, robust control criteria, and task-based objectives, all within the framework of traditional deep learning.  I will highlight several applications of this work in reinforcement learning, control, energy systems, and other settings, and discuss generalizations and directions for future work in the area.},
	urldate = {2022-02-16},
	author = {{Physics Informed Machine Learning} and Kolter, J. Zico},
	month = jan,
	year = {2021},
}

@misc{brunton_steve_nodate,
	title = {Steve {Brunton}},
	url = {https://www.me.washington.edu/facultyfinder/steve-brunton},
	language = {en},
	urldate = {2022-02-16},
	journal = {Mechanical Engineering},
	author = {Brunton, Steven L.},
}

@misc{heidelbergai_analyzing_2019,
	title = {Analyzing {Inverse} {Problems} in {Natural} {Science} using {Invertible} {Neural} {Networks} {\textbar} {Ullrich} {Köthe}},
	url = {https://www.youtube.com/watch?v=WqP45Iyvd3o},
	abstract = {Heidelberg AI Talk 20th November 2019 {\textbar} Analyzing Inverse Problems in Natural Science using Invertible Neural Networks {\textbar} Ullrich Köthe, Visual Learning Lab, Heidelberg University

https://heidelberg.ai},
	urldate = {2022-02-16},
	author = {{heidelberg.ai} and Köthe, Ullrich},
	month = dec,
	year = {2019},
}

@misc{brunton_machine_nodate,
	title = {Machine {Learning}, {Dynamical} {Systems} and {Control}},
	url = {http://www.databookuw.com/},
	urldate = {2022-02-16},
	author = {Brunton, Steven L. and Kutz, J. Nathan},
}

@article{ardizzone_analyzing_2019,
	title = {Analyzing {Inverse} {Problems} with {Invertible} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1808.04730},
	abstract = {In many tasks, in particular in natural science, the goal is to determine hidden system parameters from a set of measurements. Often, the forward process from parameter- to measurement-space is a well-defined function, whereas the inverse problem is ambiguous: one measurement may map to multiple different sets of parameters. In this setting, the posterior parameter distribution, conditioned on an input measurement, has to be determined. We argue that a particular class of neural networks is well suited for this task -- so-called Invertible Neural Networks (INNs). Although INNs are not new, they have, so far, received little attention in literature. While classical neural networks attempt to solve the ambiguous inverse problem directly, INNs are able to learn it jointly with the well-defined forward process, using additional latent output variables to capture the information otherwise lost. Given a specific measurement and sampled latent variables, the inverse pass of the INN provides a full distribution over parameter space. We verify experimentally, on artificial data and real-world problems from astrophysics and medicine, that INNs are a powerful analysis tool to find multi-modalities in parameter space, to uncover parameter correlations, and to identify unrecoverable parameters.},
	urldate = {2022-02-16},
	journal = {arXiv:1808.04730 [cs, stat]},
	author = {Ardizzone, Lynton and Kruse, Jakob and Wirkert, Sebastian and Rahner, Daniel and Pellegrini, Eric W. and Klessen, Ralf S. and Maier-Hein, Lena and Rother, Carsten and Köthe, Ullrich},
	month = feb,
	year = {2019},
	note = {arXiv: 1808.04730},
	keywords = {68T01, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{noauthor_ml_nodate,
	title = {{ML} for {Physics} and {Physics} for {ML}},
	url = {https://neurips.cc/virtual/2021/tutorial/21896},
	urldate = {2022-02-16},
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2022-02-16},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{lai_structural_2021,
	title = {Structural identification with physics-informed neural ordinary differential equations},
	volume = {508},
	issn = {0022460X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022460X21002686},
	doi = {10.1016/j.jsv.2021.116196},
	language = {en},
	urldate = {2022-02-16},
	journal = {Journal of Sound and Vibration},
	author = {Lai, Zhilu and Mylonas, Charilaos and Nagarajaiah, Satish and Chatzi, Eleni},
	month = sep,
	year = {2021},
	pages = {116196},
}

@misc{zico_kolter_neurips_2020,
	title = {{NeurIPS} 2020 {Tutorial}: {Deep} {Implicit} {Layers}},
	shorttitle = {{NeurIPS} 2020 {Tutorial}},
	url = {https://www.youtube.com/watch?v=MX1RJELWONc},
	abstract = {This is a video recording of our NeurIPS 2020 Tutorial - Deep Implicit Layers: Neural ODEs, Deep Equilibrium Models, and Beyond - by David Duvenaud, Zico Kolter, and Matt Johnson.  Full information about the tutorial, including extensive notes in Colab notebook form, are available on our website: https://implicit-layers-tutorial.org},
	urldate = {2022-02-16},
	author = {{Zico Kolter}},
	month = dec,
	year = {2020},
}

@inproceedings{abreut_semi-analytical_2017,
	address = {Chicago, IL},
	title = {Semi-analytical fault-on trajectory simulation and its application in direct methods},
	isbn = {978-1-5386-2212-4},
	url = {http://ieeexplore.ieee.org/document/8274026/},
	doi = {10.1109/PESGM.2017.8274026},
	urldate = {2022-02-08},
	booktitle = {2017 {IEEE} {Power} \& {Energy} {Society} {General} {Meeting}},
	publisher = {IEEE},
	author = {Abreut, Eric and Wang, Bin and Sun, Kai},
	month = jul,
	year = {2017},
	pages = {1--5},
}

@inproceedings{vrakopoulou_unified_2013,
	address = {Rethymno, Greece},
	title = {A unified analysis of security-constrained {OPF} formulations considering uncertainty, risk, and controllability in single and multi-area systems},
	isbn = {978-1-4799-0199-9},
	url = {http://ieeexplore.ieee.org/document/6629409/},
	doi = {10.1109/IREP.2013.6629409},
	urldate = {2022-02-08},
	booktitle = {2013 {IREP} {Symposium} {Bulk} {Power} {System} {Dynamics} and {Control} - {IX} {Optimization}, {Security} and {Control} of the {Emerging} {Power} {Grid}},
	publisher = {IEEE},
	author = {Vrakopoulou, Maria and Chatzivasileiadis, Spyros and Iggland, Emil and Imhof, Markus and Krause, Thilo and Makela, Olli and Mathieu, Johanna L. and Roald, Line and Wiget, Roger and Andersson, Goran},
	month = aug,
	year = {2013},
	pages = {1--19},
}

@article{athay_practical_1979,
	title = {A {Practical} {Method} for the {Direct} {Analysis} of {Transient} {Stability}},
	volume = {PAS-98},
	issn = {0018-9510},
	url = {http://ieeexplore.ieee.org/document/4113518/},
	doi = {10.1109/TPAS.1979.319407},
	number = {2},
	urldate = {2022-02-08},
	journal = {IEEE Transactions on Power Apparatus and Systems},
	author = {Athay, T. and Podmore, R. and Virmani, S.},
	month = mar,
	year = {1979},
	pages = {573--584},
}

@techreport{bills_-line_1970,
	title = {On-line stability analysis study, {RP} 90-1},
	url = {https://www.osti.gov/biblio/5984031},
	abstract = {The results of an investigation of methods which might be employed for the on-line stability analysis of electric power systems are presented. On-line analysis in this instance does not refer to real-time or faster than real-time analysis, but to the analysis of data supplied primarily from the electric power system by continuous or periodic sensing of state variables. On-line stability analysis will enable the power system dispatcher to be appraised quickly of potentially unstable conditions by utilizing a preselected set of system contingencies and disturbances based on the existing power network configuration and loading. He can then quickly make decisions concerning possible system changes, such as in scheduling generator loading, line or bus switching, equipment shutdowns, etc. Modern control theory techniques, such as the use of Liapunov functions, pattern recognition, and perturbation, were tried and the results of this research are described. Communication rates needed for data to be supplied the On-Line Stability Analysis Program via the state estimator were investigated. The system state estimator and the software package required for on-line stability analysis are discussed, and the use of computer driven CRT's for displaying results is recommended. A follow-on effort for implementing on-line stability analysis is proposed.},
	language = {English},
	number = {NP-2901022},
	urldate = {2022-02-08},
	institution = {North American Rockwell Information Systems Co., Anaheim, CA (USA)},
	author = {Bills, G. W.},
	month = oct,
	year = {1970},
}

@article{dorfler_kron_2013,
	title = {Kron {Reduction} of {Graphs} {With} {Applications} to {Electrical} {Networks}},
	volume = {60},
	issn = {1549-8328, 1558-0806},
	url = {http://ieeexplore.ieee.org/document/6316101/},
	doi = {10.1109/TCSI.2012.2215780},
	number = {1},
	urldate = {2022-02-08},
	journal = {IEEE Transactions on Circuits and Systems I: Regular Papers},
	author = {Dorfler, Florian and Bullo, Francesco},
	month = jan,
	year = {2013},
	pages = {150--163},
}

@article{tomim_parallel_2009,
	title = {Parallel computation of large power system networks using the multi-area {Thévenin} equivalents},
	url = {https://doi.library.ubc.ca/10.14288/1.0067477},
	doi = {10.14288/1.0067477},
	abstract = {The requirements of today's power systems are much different from the ones of the systems of the past. Among others, energy market deregulation, proliferation of independent power producers, unusual power transfers, increased complexity and sensitivity of the equipments demand from power systems operators and planners a thorough understanding of the dynamic behaviour of such systems in order to ensure a stable and reliable energy supply. In this context, on-line Dynamic Security Assessment (DSA) plays a fundamental role in helping operators to predict the security level of future operating conditions that the system may undergo. Amongst the tools that compound DSA is the Transient Stability Assessment (TSA) tools, which aim at determining the dynamic stability margins of present and future operating conditions. The systems employed in on-line TSA, however, are very much simplified versions of the actual systems, due to the time-consuming transient stability simulations that are still at the heart of TSA applications. Therefore, there is an increasing need for improved TSA software, which has the capability of simulating bigger and more complex systems in a shorter lapse of time. In order to achieve such a goal, a reformulation of the Multi-Area Thévenin Equivalents (MATE) algorithm is proposed. The intent of such an algorithm is parallelizing the solution of the large sparse linear systems associated with transient stability simulations and, therefore, speeding up the overall on-line TSA cycle. As part of the developed work, the matrix-based MATE algorithm was re-evaluated from an electric network standpoint, which yielded the network-based MATE presently introduced. In addition, a performance model of the proposed algorithm is developed, from which a theoretical speedup limit of p/2 was deduced, where p is the number of subsystems into which a system is torn apart. Applications of the network-based MATE algorithm onto solving actual power systems (about 2,000 and 15,000 buses) showed the attained speedup to closely follow the predictions made with the formulated performance model, even on a commodity cluster built out of inexpensive out-of-the-shelf computers.},
	language = {en},
	urldate = {2022-02-08},
	author = {Tomim, Marcelo Aroca},
	year = {2009},
	note = {Publisher: University of British Columbia},
}

@book{kundur_power_1994,
	address = {New York},
	series = {The {EPRI} power system engineering series},
	title = {Power system stability and control},
	isbn = {978-0-07-035958-1},
	publisher = {McGraw-Hill},
	author = {Kundur, P. and Balu, Neal J. and Lauby, Mark G.},
	year = {1994},
	keywords = {Control, Electric power system stability, Electric power systems, Power system modelling},
}

@book{bishop_pattern_2006,
	address = {New York},
	series = {Information science and statistics},
	title = {Pattern recognition and machine learning},
	isbn = {978-0-387-31073-2},
	language = {en},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	year = {2006},
	keywords = {Machine learning},
}

@article{pagnier_physics-informed_2021,
	title = {Physics-{Informed} {Graphical} {Neural} {Network} for {Parameter} \& {State} {Estimations} in {Power} {Systems}},
	url = {http://arxiv.org/abs/2102.06349},
	abstract = {Parameter Estimation (PE) and State Estimation (SE) are the most wide-spread tasks in the system engineering. They need to be done automatically, fast and frequently, as measurements arrive. Deep Learning (DL) holds the promise of tackling the challenge, however in so far, as PE and SE in power systems is concerned, (a) DL did not win trust of the system operators because of the lack of the physics of electricity based, interpretations and (b) DL remained illusive in the operational regimes were data is scarce. To address this, we present a hybrid scheme which embeds physics modeling of power systems into Graphical Neural Networks (GNN), therefore empowering system operators with a reliable and explainable real-time predictions which can then be used to control the critical infrastructure. To enable progress towards trustworthy DL for PE and SE, we build a physics-informed method, named Power-GNN, which reconstructs physical, thus interpretable, parameters within Effective Power Flow (EPF) models, such as admittances of effective power lines, and NN parameters, representing implicitly unobserved elements of the system. In our experiments, we test the Power-GNN on different realistic power networks, including these with thousands of loads and hundreds of generators. We show that the Power-GNN outperforms vanilla NN scheme unaware of the EPF physics.},
	language = {en},
	urldate = {2022-01-20},
	journal = {arXiv:2102.06349 [physics]},
	author = {Pagnier, Laurent and Chertkov, Michael},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.06349},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control, Physics - Physics and Society},
}

@inproceedings{betancourt_analysis_2010,
	address = {Sao Paulo, Brazil},
	title = {Analysis of inter-area oscillations in power systems using {Adomian}-{Pade} approximation method},
	isbn = {978-1-4244-8008-1},
	url = {http://ieeexplore.ieee.org/document/5740043/},
	doi = {10.1109/INDUSCON.2010.5740043},
	urldate = {2022-02-05},
	booktitle = {2010 9th {IEEE}/{IAS} {International} {Conference} on {Industry} {Applications} - {INDUSCON} 2010},
	publisher = {IEEE},
	author = {Betancourt, Ramon J. and Marco, A. and Perez, G. and Barocio, E. Emilio and Arroyo, L. Jaime},
	month = nov,
	year = {2010},
	pages = {1--6},
}

@misc{pagnier_pantagruel_2019,
	title = {{PanTaGruEl} - a pan-{European} transmission grid and electricity generation model},
	copyright = {Creative Commons Attribution 4.0 International, Open Access},
	url = {https://zenodo.org/record/2642175},
	doi = {10.5281/ZENODO.2642175},
	abstract = {If you have any questions or comments, please write to laurent.vincent.pagnier@gmail.com. When publishing results based on this data set, please cite: L. Pagnier, P. Jacquod, “Inertia location and slow network modes determine disturbance propagation in large-scale power grids”, PLOS ONE 14(3): e0213550, 2019. PLOS ONE 14(3): e0213550, 2019. and M. Tyloo, L. Pagnier, P. Jacquod, “The Key Player Problem in Complex Oscillator Networks and Electric Power Grids: Resistance Centralities Identify Local Vulnerabilities”, Science Advances 5(11): eaaw8359, 2019. {\textless}strong{\textgreater}Description:{\textless}/strong{\textgreater} PanTaGruEl is a dynamical grid model designed to investigate the propagation of disturbances in the continental European transmission grid. The construction of the model is detailed here. {\textless}strong{\textgreater}Features{\textless}/strong{\textgreater}: Precise distribution of national demands to network buses. Realistic electrical parameters of transmission lines. Merit-Order based economic dispatch of generators. Dynamical parameters of generators and loads for transient stability investigations. {\textless}strong{\textgreater}Files:{\textless}/strong{\textgreater} Data files: Our model is provided in an extended Matpower format and as csv raw data. For more information on Matpower format, see Appendix B of its manual. Script files: {\textless}em{\textgreater}opf\_ex.m {\textless}/em{\textgreater}performs optimal power flow computations for two load configurations.{\textless}br{\textgreater} {\textless}em{\textgreater}spectral\_ex.m{\textless}/em{\textgreater} presents a basic spectral analysis.{\textless}br{\textgreater} {\textless}em{\textgreater}dynamics{\textless}/em{\textgreater}{\textless}em{\textgreater}\_ex.m{\textless}/em{\textgreater} give a minimal example of dynamical simulations. {\textless}strong{\textgreater}Requirements:{\textless}/strong{\textgreater} Our model has been developed for use with Matpower. If you are interested in a port to another language, please contact us. {\textless}strong{\textgreater}Acknowledgement:{\textless}/strong{\textgreater} The authors thank M. Tyloo and K. Van Walstijn for their useful comments and remarks on the model. {\textless}strong{\textgreater}Sources{\textless}/strong{\textgreater}: B. Wiegmans, “GridKit extract of ENTSO-E interactive map”{\textless}br{\textgreater} Global Energy Observatory, “GEO Power plants database”{\textless}br{\textgreater} Siemens, “Power Engineering Guide”},
	urldate = {2022-02-03},
	publisher = {Zenodo},
	author = {Pagnier, Laurent and Jacquod, Philippe},
	month = dec,
	year = {2019},
	keywords = {large transmission grid model, economic dispatch, dynamical simulations},
}

@article{ginoux_differential_2006,
	title = {Differential {Geometry} and {Mechanics} {Applications} to {Chaotic} {Dynamical} {Systems}},
	volume = {16},
	issn = {0218-1274, 1793-6551},
	url = {http://arxiv.org/abs/1408.1711},
	doi = {10.1142/S0218127406015192},
	abstract = {The aim of this article is to highlight the interest to apply Differential Geometry and Mechanics concepts to chaotic dynamical systems study. Thus, the local metric properties of curvature and torsion will directly provide the analytical expression of the slow manifold equation of slow-fast autonomous dynamical systems starting from kinematics variables velocity, acceleration and over-acceleration or jerk. The attractivity of the slow manifold will be characterized thanks to a criterion proposed by Henri Poincar{\textbackslash}'e. Moreover, the specific use of acceleration will make it possible on the one hand to define slow and fast domains of the phase space and on the other hand, to provide an analytical equation of the slow manifold towards which all the trajectories converge. The attractive slow manifold constitutes a part of these dynamical systems attractor. So, in order to propose a description of the geometrical structure of attractor, a new manifold called singular manifold will be introduced. Various applications of this new approach to the models of Van der Pol, cubic-Chua, Lorenz, and Volterra-Gause are proposed.},
	number = {04},
	urldate = {2022-02-02},
	journal = {International Journal of Bifurcation and Chaos},
	author = {Ginoux, Jean-Marc and Rossetto, Bruno},
	month = apr,
	year = {2006},
	note = {arXiv: 1408.1711},
	keywords = {Mathematics - Dynamical Systems},
	pages = {887--910},
}

@book{lai_transient_2011,
	title = {Transient {Chaos}: {Complex} {Dynamics} on {Finite} {Time} {Scales}},
	isbn = {978-1-4419-6987-3},
	shorttitle = {Transient {Chaos}},
	url = {https://doi.org/10.1007/978-1-4419-6987-3},
	abstract = {This book represents the first comprehensive treatment of Transient Chaos. It gives an overview of the subject based on three decades of intensive research. One special emphasis is on applications, and the fact that certain interesting dynamical phenomena can be understood only in the framework of transient chaos. Specific topics treated include basic concepts and characterization of transient chaos, crises, fractal basin boundaries, chaotic scattering, noise-induced chaos, chaotic advections and the spreading of pollutants in fluid flows, quantum chaotic scattering, spatiotemporal chaotic transients and turbulence, controlling transient chaos, and analysis of transient chaotic time series, etc. Materials in the book reflect the most recent advances in the field. Case studies and examples are included in each chapter with relevant experimental evidence wherever appropriate. The book is intended for researchers and graduate students in Physics, Engineering, Applied Mathematics, and Biomedical Sciences. Ying-Cheng Lai is a Professor of Electrical Engineering and Professor of Physics at Arizona State University, USA and a Sixth Century Chair in Electrical Engineering at the University of Aberdeen, UK. Tam??s T??l is a Professor of Physics at E??tv??s University, Budapest, Hungary.},
	language = {English},
	urldate = {2022-02-02},
	author = {Lai, Ying-Cheng and Tél, Tamás},
	year = {2011},
	note = {OCLC: 1120527587},
}

@article{kim_knowledge_2021,
	title = {Knowledge {Integration} into deep learning in dynamical systems: an overview and taxonomy},
	volume = {35},
	issn = {1738-494X, 1976-3824},
	shorttitle = {Knowledge {Integration} into deep learning in dynamical systems},
	url = {https://link.springer.com/10.1007/s12206-021-0342-5},
	doi = {10.1007/s12206-021-0342-5},
	abstract = {Despite the sudden rise of AI, it still leaves a question mark to many newcomers on its widespread adoption as it exhibits a lack of robustness and interpretability. For instance, the insufficient amount of training data usually hinders its performance due to the lack of generalization, and the black box nature of deep neural networks does not allow for a precise explanation behind its mechanism preventing a new scientific discovery. Such limitations have led to the development of several branches of deep learning one of which include physics-informed neural networks that will be covered in the rest of this paper. In this overview, we defined the general concept of informed deep learning followed by an extensive literature survey in the field of dynamical systems. We hope to make a contribution to our mechanical engineering community by conveying knowledge and insights on this emerging field of study through this survey paper.},
	language = {en},
	number = {4},
	urldate = {2022-01-20},
	journal = {Journal of Mechanical Science and Technology},
	author = {Kim, Sung Wook and Kim, Iljeok and Lee, Jonghwan and Lee, Seungchul},
	month = apr,
	year = {2021},
	pages = {1331--1342},
}

@article{pagnier_model_2021,
	title = {Model {Reduction} of {Swing} {Equations} with {Physics} {Informed} {PDE}},
	url = {http://arxiv.org/abs/2110.14066},
	abstract = {This manuscript is the ﬁrst step towards building a robust and efﬁcient model reduction methodology to capture transient dynamics in a transmission level electric power system. Such dynamics is normally modeled on seconds-to-tens-ofseconds time scales by the so-called swing equations, which are ordinary differential equations deﬁned on a spatially discrete model of the power grid. We suggest, following Seymlyen (1974) and Thorpe, Seyler and Phadke (1999), to map the swing equations onto a linear, inhomogeneous Partial Differential Equation (PDE) of parabolic type in two space and one time dimensions with time-independent coefﬁcients and properly deﬁned boundary conditions. The continuous two-dimensional spatial domain is deﬁned by a geographical map of the area served by the power grid, and associated with the PDE coefﬁcients derived from smoothed graph-Laplacian of susceptances, machine inertia and damping. Inhomogeneous source terms represent spatially distributed injection/consumption of power. We illustrate our method on PanTaGruEl (Pan-European Transmission Grid and ELectricity generation model) [1], [2]. We show that, when properly coarse-grained, i.e. with the PDE coefﬁcients and source terms extracted from a spatial convolution procedure of the respective discrete coefﬁcients in the swing equations, the resulting PDE reproduces faithfully and efﬁciently the original swing dynamics. We ﬁnally discuss future extensions of this work, where the presented PDE-based reduced modeling will initialize a physics-informed machine learning approach for realtime modeling, n − 1 feasibility assessment and transient stability analysis of power systems.},
	language = {en},
	urldate = {2022-01-20},
	journal = {arXiv:2110.14066 [physics]},
	author = {Pagnier, Laurent and Chertkov, Michael and Fritzsch, Julian and Jacquod, Philippe},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.14066},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control, Physics - Physics and Society},
}

@article{milano_stability_nodate,
	title = {{STABILITY} {ANALYSIS} {OF} {NONLINEAR} {SYSTEMS} ({EEEN50100})},
	language = {en},
	journal = {Nonlinear Systems},
	author = {Milano, Federico},
	pages = {404},
}

@article{huang_training_2021,
	title = {Training {Certifiably} {Robust} {Neural} {Networks} with {Efficient} {Local} {Lipschitz} {Bounds}},
	url = {http://arxiv.org/abs/2111.01395},
	abstract = {Certiﬁed robustness is a desirable property for deep neural networks in safetycritical applications, and popular training algorithms can certify robustness of a neural network by computing a global bound on its Lipschitz constant. However, such a bound is often loose: it tends to over-regularize the neural network and degrade its natural accuracy. A tighter Lipschitz bound may provide a better tradeoff between natural and certiﬁed accuracy, but is generally hard to compute exactly due to non-convexity of the network. In this work, we propose an efﬁcient and trainable local Lipschitz upper bound by considering the interactions between activation functions (e.g. ReLU) and weight matrices. Speciﬁcally, when computing the induced norm of a weight matrix, we eliminate the corresponding rows and columns where the activation function is guaranteed to be a constant in the neighborhood of each given data point, which provides a provably tighter bound than the global Lipschitz constant of the neural network. Our method can be used as a plug-in module to tighten the Lipschitz bound in many certiﬁable training algorithms. Furthermore, we propose to clip activation functions (e.g., ReLU and MaxMin) with a learnable upper threshold and a sparsity loss to assist the network to achieve an even tighter local Lipschitz bound. Experimentally, we show that our method consistently outperforms state-of-the-art methods in both clean and certiﬁed accuracy on MNIST, CIFAR-10 and TinyImageNet datasets with various network architectures.},
	language = {en},
	urldate = {2021-12-17},
	journal = {arXiv:2111.01395 [cs, stat]},
	author = {Huang, Yujia and Zhang, Huan and Shi, Yuanyuan and Kolter, J. Zico and Anandkumar, Anima},
	month = nov,
	year = {2021},
	note = {arXiv: 2111.01395},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{rosso_grid-forming_2021,
	title = {Grid-{Forming} {Converters}: {Control} {Approaches}, {Grid}-{Synchronization}, and {Future} {Trends}—{A} {Review}},
	volume = {2},
	issn = {2644-1241},
	shorttitle = {Grid-{Forming} {Converters}},
	doi = {10.1109/OJIA.2021.3074028},
	abstract = {In the last decade, the concept of grid-forming (GFM) converters has been introduced for microgrids and islanded power systems. Recently, the concept has been proposed for use in wider interconnected transmission networks, and several control structures have thus been developed, giving rise to discussions about the expected behaviour of such converters. In this paper, an overview of control schemes for GFM converters is provided. By identifying the main subsystems in respect to their functionalities, a generalized control structure is derived and different solutions for each of the main subsystems composing the controller are analyzed and compared. Subsequently, several selected open issues and challenges regarding GFM converters, i. e. angle stability, fault ride-through (FRT) capabilities, and transition from islanded to grid connected mode are discussed. Perspectives on challenges and future trends are lastly shared.},
	journal = {IEEE Open Journal of Industry Applications},
	author = {Rosso, Roberto and Wang, Xiongfei and Liserre, Marco and Lu, Xiaonan and Engelken, Soenke},
	year = {2021},
	note = {Conference Name: IEEE Open Journal of Industry Applications},
	keywords = {Control structure overview, Impedance, Phase locked loops, Power system stability, Reactive power, Stability criteria, Synchronization, Voltage control, grid-following converters, grid-forming converters, power-synchronization},
	pages = {93--109},
}

@article{marwah_parametric_nodate,
	title = {Parametric {Complexity} {Bounds} for {Approximating} {PDEs} with {Neural} {Networks}},
	abstract = {Recent experiments have shown that deep networks can approximate solutions to high-dimensional PDEs, seemingly escaping the curse of dimensionality. However, questions regarding the theoretical basis for such approximations, including the required network size remain open. In this paper, we investigate the representational power of neural networks for approximating solutions to linear elliptic PDEs with Dirichlet boundary conditions. We prove that when a PDE’s coefﬁcients are representable by small neural networks, the parameters required to approximate its solution scale polynomially with the input dimension d and proportionally to the parameter counts of the coefﬁcient networks. To this we end, we develop a proof technique that simulates gradient descent (in an appropriate Hilbert space) by growing a neural network architecture whose iterates each participate as subnetworks in their (slightly larger) successors, and converge to the solution of the PDE. We bound the size of the solution showing a polynomial dependence on d and no dependence on the volume of the domain.},
	language = {en},
	author = {Marwah, Tanya and Lipton, Zachary C and Risteski, Andrej},
	pages = {12},
}

@article{sobbouhi_transient_2021,
	title = {Transient stability prediction of power system; a review on methods, classification and considerations},
	volume = {190},
	issn = {03787796},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0378779620306520},
	doi = {10.1016/j.epsr.2020.106853},
	language = {en},
	urldate = {2021-11-08},
	journal = {Electric Power Systems Research},
	author = {Sobbouhi, Ali Reza and Vahedi, Abolfazl},
	month = jan,
	year = {2021},
	pages = {106853},
}

@article{oluic_methodology_2016,
	title = {Methodology for {Rotor} {Angle} {Transient} {Stability} {Assessment} in {Parameter} {Space}},
	issn = {0885-8950, 1558-0679},
	url = {http://ieeexplore.ieee.org/document/7475910/},
	doi = {10.1109/TPWRS.2016.2571562},
	urldate = {2021-10-11},
	journal = {IEEE Transactions on Power Systems},
	author = {Oluic, Marina and Ghandhari, Mehrdad and Berggren, Bertil},
	year = {2016},
	pages = {1--1},
}

@article{tian_construction_2021,
	title = {Construction of {Multi}-{State} {Transient} {Stability} {Boundary} {Based} on {Broad} {Learning}},
	volume = {36},
	issn = {0885-8950, 1558-0679},
	url = {https://ieeexplore.ieee.org/document/9309335/},
	doi = {10.1109/TPWRS.2020.3047611},
	number = {4},
	urldate = {2021-10-11},
	journal = {IEEE Transactions on Power Systems},
	author = {Tian, Yuan and Wang, Keyou and Oluic, Marina and Ghandhari, Mehrdad and Xu, Jin and Li, Guojie},
	month = jul,
	year = {2021},
	pages = {2906--2917},
}

@article{zhang_online_2017,
	title = {Online {Identification} of {Power} {System} {Equivalent} {Inertia} {Constant}},
	volume = {64},
	issn = {0278-0046, 1557-9948},
	url = {http://ieeexplore.ieee.org/document/7913675/},
	doi = {10.1109/TIE.2017.2698414},
	number = {10},
	urldate = {2021-10-11},
	journal = {IEEE Transactions on Industrial Electronics},
	author = {Zhang, Junbo and Xu, Hanchen},
	month = oct,
	year = {2017},
	pages = {8098--8107},
}

@article{panda_online_2020,
	title = {Online {Estimation} of {System} {Inertia} in a {Power} {Network} {Utilizing} {Synchrophasor} {Measurements}},
	volume = {35},
	issn = {0885-8950, 1558-0679},
	url = {https://ieeexplore.ieee.org/document/8930036/},
	doi = {10.1109/TPWRS.2019.2958603},
	number = {4},
	urldate = {2021-10-11},
	journal = {IEEE Transactions on Power Systems},
	author = {Panda, Rakesh Kumar and Mohapatra, Abheejeet and Srivastava, Suresh Chandra},
	month = jul,
	year = {2020},
	pages = {3122--3132},
}

@book{goodfellow_deep_2016,
	address = {Cambridge, Massachusetts},
	series = {Adaptive computation and machine learning},
	title = {Deep learning},
	isbn = {978-0-262-03561-3},
	publisher = {The MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	keywords = {Machine learning},
}

@article{xu_response-surface-based_2019,
	title = {Response-{Surface}-{Based} {Bayesian} {Inference} for {Power} {System} {Dynamic} {Parameter} {Estimation}},
	volume = {10},
	issn = {1949-3061},
	doi = {10.1109/TSG.2019.2892464},
	abstract = {This paper develops a new response-surface-based Bayesian inference approach for power system dynamic parameter estimation of a decentralized generator using phasor-measurement-unit measurement. The response surface for the decentralized generator model is formulated through a polynomial-chaos-based surrogate. This surrogate allows us to efficiently evaluate the time-consuming dynamic solver at parameter values through a polynomial-based reduced-order representation. In addition, a polynomial-chaos-based analysis of variance is performed to screen out model parameters while ensuring system observability. In dealing with sampling the non-Gaussian posterior distribution for the parameters, the Metropolis-Hastings sampler is adopted. The simulations conducted in the New England system under different system events show that the proposed method can achieve a speedup factor of two orders or magnitude compared with the traditional method while providing full probabilistic distribution of model parameters and achieving the same level of accuracy.},
	number = {6},
	journal = {IEEE Transactions on Smart Grid},
	author = {Xu, Yijun and Huang, Can and Chen, Xiao and Mili, Lamine and Tong, Charles H. and Korkali, Mert and Min, Liang},
	month = nov,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Smart Grid},
	keywords = {Bayes methods, Bayesian inference, Chaos, Dynamic parameter estimation, Generators, Mathematical model, Metropolis-Hastings, PMU, Parameter estimation, Power system dynamics, Response surface methodology, analysis of variance (ANOVA), polynomial chaos expansion (PCE), response surface},
	pages = {5899--5909},
}

@article{brouillon_bayesian_2021,
	title = {Bayesian {Error}-in-{Variables} {Models} for the {Identification} of {Power} {Networks}},
	url = {http://arxiv.org/abs/2107.04480},
	abstract = {The increasing integration of intermittent renewable generation, especially at the distribution level,necessitates advanced planning and optimisation methodologies contingent on the knowledge of thegrid, specifically the admittance matrix capturing the topology and line parameters of an electricnetwork. However, a reliable estimate of the admittance matrix may either be missing or quicklybecome obsolete for temporally varying grids. In this work, we propose a data-driven identificationmethod utilising voltage and current measurements collected from micro-PMUs. More precisely,we first present a maximum likelihood approach and then move towards a Bayesian framework,leveraging the principles of maximum a posteriori estimation. In contrast with most existing con-tributions, our approach not only factors in measurement noise on both voltage and current data,but is also capable of exploiting available a priori information such as sparsity patterns and knownline parameters. Simulations conducted on benchmark cases demonstrate that, compared to otheralgorithms, our method can achieve significantly greater accuracy.},
	urldate = {2021-08-02},
	journal = {arXiv:2107.04480 [cs, eess, stat]},
	author = {Brouillon, Jean-Sébastien and Fabbiani, Emanuele and Nahata, Pulkit and Dörfler, Florian and Ferrari-Trecate, Giancarlo},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.04480},
	keywords = {Electrical Engineering and Systems Science - Systems and Control, Statistics - Machine Learning},
}

@article{vu_lyapunov_2016,
	title = {Lyapunov {Functions} {Family} {Approach} to {Transient} {Stability} {Assessment}},
	volume = {31},
	issn = {0885-8950, 1558-0679},
	url = {http://ieeexplore.ieee.org/document/7106572/},
	doi = {10.1109/TPWRS.2015.2425885},
	number = {2},
	urldate = {2021-07-08},
	journal = {IEEE Transactions on Power Systems},
	author = {Vu, Thanh Long and Turitsyn, Konstantin},
	month = mar,
	year = {2016},
	pages = {1269--1277},
}

@article{singh_learning_2021,
	title = {Learning to {Solve} the {AC}-{OPF} using {Sensitivity}-{Informed} {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2103.14779},
	abstract = {To shift the computational burden from real-time to offline in delay-critical power systems applications, recent works entertain the idea of using a deep neural network (DNN) to predict the solutions of the AC optimal power flow (AC-OPF) once presented load demands. As network topologies may change, training this DNN in a sample-efficient manner becomes a necessity. To improve data efficiency, this work utilizes the fact OPF data are not simple training labels, but constitute the solutions of a parametric optimization problem. We thus advocate training a sensitivity-informed DNN (SI-DNN) to match not only the OPF optimizers, but also their partial derivatives with respect to the OPF parameters (loads). It is shown that the required Jacobian matrices do exist under mild conditions, and can be readily computed from the related primal/dual solutions. The proposed SI-DNN is compatible with a broad range of OPF solvers, including a non-convex quadratically constrained quadratic program (QCQP), its semidefinite program (SDP) relaxation, and MATPOWER; while SI-DNN can be seamlessly integrated in other learning-to-OPF schemes. Numerical tests on three benchmark power systems corroborate the advanced generalization and constraint satisfaction capabilities for the OPF solutions predicted by an SI-DNN over a conventionally trained DNN, especially in low-data setups.},
	urldate = {2021-06-29},
	journal = {arXiv:2103.14779 [math, stat]},
	author = {Singh, Manish K. and Kekatos, Vassilis and Giannakis, Georgios B.},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.14779},
	keywords = {Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{stiasny_transient_2021,
	title = {Transient {Stability} {Analysis} with {Physics}-{Informed} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2106.13638},
	abstract = {Solving the ordinary differential equations that govern the power system is an indispensable part in transient stability analysis. However, the traditionally applied methods either carry a significant computational burden, require model simplifications, or use overly conservative surrogate models. Neural networks can circumvent these limitations but are faced with high demands on the used datasets. Furthermore, they are agnostic to the underlying governing equations. Physics-informed neural network tackle this problem and we explore their advantages and challenges in this paper. We illustrate the findings on the Kundur two-area system and highlight possible pathways forward in developing this method further.},
	urldate = {2021-06-28},
	journal = {arXiv:2106.13638 [cs, eess]},
	author = {Stiasny, Jochen and Misyris, Georgios S. and Chatzivasileiadis, Spyros},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.13638},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control},
}

@article{stiasny_physics-informed_2021,
	title = {Physics-{Informed} {Neural} {Networks} for {Non}-linear {System} {Identification} for {Power} {System} {Dynamics}},
	url = {http://arxiv.org/abs/2004.04026},
	abstract = {Varying power-infeed from converter-based generation units introduces great uncertainty on system parameters such as inertia and damping. As a consequence, system operators face increasing challenges in performing dynamic security assessment and taking real-time control actions. Exploiting the widespread deployment of phasor measurement units (PMUs) and aiming at developing a fast dynamic state and parameter estimation tool, this paper investigates the performance of Physics-Informed Neural Networks (PINN) for discovering the frequency dynamics of future power systems. PINNs have the potential to address challenges such as the stronger non-linearities of low-inertia systems, increased measurement noise, and limited availability of data. The estimator is demonstrated in several test cases using a 4-bus system, and compared with state of the art algorithms, such as the Unscented Kalman Filter (UKF), to assess its performance.},
	urldate = {2021-06-27},
	journal = {arXiv:2004.04026 [cs, eess]},
	author = {Stiasny, Jochen and Misyris, George S. and Chatzivasileiadis, Spyros},
	month = apr,
	year = {2021},
	note = {arXiv: 2004.04026},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control},
}

@article{thams_efficient_2020,
	title = {Efficient {Database} {Generation} for {Data}-{Driven} {Security} {Assessment} of {Power} {Systems}},
	volume = {35},
	issn = {0885-8950, 1558-0679},
	url = {https://ieeexplore.ieee.org/document/8600355/},
	doi = {10.1109/TPWRS.2018.2890769},
	number = {1},
	urldate = {2021-06-25},
	journal = {IEEE Transactions on Power Systems},
	author = {Thams, Florian and Venzke, Andreas and Eriksson, Robert and Chatzivasileiadis, Spyros},
	month = jan,
	year = {2020},
	pages = {30--41},
}

@article{el-abiad_transient_1966,
	title = {Transient {Stability} {Regions} of {Multimachine} {Power} {Systems}},
	volume = {PAS-85},
	issn = {0018-9510},
	url = {http://ieeexplore.ieee.org/document/4073003/},
	doi = {10.1109/TPAS.1966.291554},
	number = {2},
	urldate = {2021-06-25},
	journal = {IEEE Transactions on Power Apparatus and Systems},
	author = {El-abiad, Ahmed and Nagappan, K.},
	month = feb,
	year = {1966},
	pages = {169--179},
}

@article{gless_direct_1966,
	title = {Direct {Method} of {Liapunov} {Applied} to {Transient} {Power} {System} {Stability}},
	volume = {PAS-85},
	issn = {0018-9510},
	url = {http://ieeexplore.ieee.org/document/4073002/},
	doi = {10.1109/TPAS.1966.291553},
	number = {2},
	urldate = {2021-06-25},
	journal = {IEEE Transactions on Power Apparatus and Systems},
	author = {Gless, G.},
	month = feb,
	year = {1966},
	pages = {159--168},
}

@article{zhang_sime_1997,
	title = {{SIME}: {A} hybrid approach to fast transient stability assessment and contingency selection},
	volume = {19},
	issn = {01420615},
	shorttitle = {{SIME}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0142061596000476},
	doi = {10.1016/S0142-0615(96)00047-6},
	language = {en},
	number = {3},
	urldate = {2021-06-25},
	journal = {International Journal of Electrical Power \& Energy Systems},
	author = {Zhang, Y. and Wehenkel, L. and Rousseaux, P. and Pavella, M.},
	month = mar,
	year = {1997},
	pages = {195--208},
}

@article{winter_pushing_2015,
	title = {Pushing the {Limits}: {Europe}'s {New} {Grid}: {Innovative} {Tools} to {Combat} {Transmission} {Bottlenecks} and {Reduced} {Inertia}},
	volume = {13},
	issn = {1540-7977},
	shorttitle = {Pushing the {Limits}},
	url = {http://ieeexplore.ieee.org/document/6998972/},
	doi = {10.1109/MPE.2014.2363534},
	number = {1},
	urldate = {2021-06-22},
	journal = {IEEE Power and Energy Magazine},
	author = {Winter, Wilhelm and Elkington, Katherine and Bareux, Gabriel and Kostevc, Jan},
	month = jan,
	year = {2015},
	pages = {60--74},
}

@article{petra_bayesian_2017,
	title = {A {Bayesian} {Approach} for {Parameter} {Estimation} {With} {Uncertainty} for {Dynamic} {Power} {Systems}},
	volume = {32},
	issn = {0885-8950, 1558-0679},
	url = {http://ieeexplore.ieee.org/document/7736095/},
	doi = {10.1109/TPWRS.2016.2625277},
	number = {4},
	urldate = {2021-06-21},
	journal = {IEEE Transactions on Power Systems},
	author = {Petra, Noemi and Petra, Cosmin G. and Zhang, Zheng and Constantinescu, Emil M. and Anitescu, Mihai},
	month = jul,
	year = {2017},
	pages = {2735--2743},
}

@article{heylen_challenges_2021,
	title = {Challenges and opportunities of inertia estimation and forecasting in low-inertia power systems},
	volume = {147},
	issn = {13640321},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364032121004652},
	doi = {10.1016/j.rser.2021.111176},
	language = {en},
	urldate = {2021-06-21},
	journal = {Renewable and Sustainable Energy Reviews},
	author = {Heylen, Evelyn and Teng, Fei and Strbac, Goran},
	month = sep,
	year = {2021},
	pages = {111176},
}

@article{sun_-line_2019,
	title = {On-line power system inertia calculation using wide area measurements},
	volume = {109},
	issn = {0142-0615},
	url = {https://www.sciencedirect.com/science/article/pii/S0142061518336640},
	doi = {10.1016/j.ijepes.2019.02.013},
	abstract = {Future developments in power systems, e.g. relatively larger generator sets, the virtual power plant and synthetic inertia concept and connection of generation assets over inverters, will cause the system inertia to vary significantly. During system operation, if the inertia of the system is significantly lower than anticipated at the planning stage, then the existing, deterministic protection and control may fail to ensure system stability. Therefore, the ability to accurately determine the inertia of individual system areas, and the system as a whole, online would be very useful. In this paper, an Inertia Calculation Application (ICA), which could be implemented as part of a Wide Area Monitoring Protection and Control scheme, is presented. The necessary wide area measurements must be processed during large disturbances to the active power balance of the system. The ICA has been validated by using computer simulations, under laboratory conditions and by using real-life data recorded by a transmission system operator.},
	language = {en},
	urldate = {2021-06-15},
	journal = {International Journal of Electrical Power \& Energy Systems},
	author = {Sun, M. and Feng, Y. and Wall, P. and Azizi, S. and Yu, J. and Terzija, V.},
	month = jul,
	year = {2019},
	keywords = {Electro-mechanical transient processes, Frequency, Inertia, Nonlinear estimation, Power system measurement, Power system transients, Swing equation, Testing, WAMPAC},
	pages = {325--331},
}

@article{bai_deep_2019,
	title = {Deep {Equilibrium} {Models}},
	url = {http://arxiv.org/abs/1909.01377},
	abstract = {We present a new approach to modeling sequential data: the deep equilibrium model (DEQ). Motivated by an observation that the hidden layers of many existing deep sequence models converge towards some ﬁxed point, we propose the DEQ approach that directly ﬁnds these equilibrium points via root-ﬁnding. Such a method is equivalent to running an inﬁnite depth (weight-tied) feedforward network, but has the notable advantage that we can analytically backpropagate through the equilibrium point using implicit differentiation. Using this approach, training and prediction in these networks require only constant memory, regardless of the effective “depth” of the network. We demonstrate how DEQs can be applied to two state-of-the-art deep sequence models: self-attention transformers and trellis networks. On large-scale language modeling tasks, such as the WikiText-103 benchmark, we show that DEQs 1) often improve performance over these stateof-the-art models (for similar parameter counts); 2) have similar computational requirements to existing models; and 3) vastly reduce memory consumption (often the bottleneck for training large sequence models), demonstrating an up-to 88\% memory reduction in our experiments. The code is available at https://github. com/locuslab/deq.},
	language = {en},
	urldate = {2021-05-18},
	journal = {arXiv:1909.01377 [cs, stat]},
	author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
	month = oct,
	year = {2019},
	note = {arXiv: 1909.01377},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kag_rnns_2019,
	title = {{RNNs} {Evolving} on an {Equilibrium} {Manifold}: {A} {Panacea} for {Vanishing} and {Exploding} {Gradients}?},
	shorttitle = {{RNNs} {Evolving} on an {Equilibrium} {Manifold}},
	url = {http://arxiv.org/abs/1908.08574},
	abstract = {Recurrent neural networks (RNNs) are particularly well-suited for modeling longterm dependencies in sequential data, but are notoriously hard to train because the error backpropagated in time either vanishes or explodes at an exponential rate. While a number of works attempt to mitigate this effect through gated recurrent units, well-chosen parametric constraints, and skip-connections, we develop a novel perspective that seeks to evolve the hidden state on the equilibrium manifold of an ordinary differential equation (ODE). We propose a family of novel RNNs, namely Equilibriated Recurrent Neural Networks (ERNNs) that overcome the gradient decay or explosion effect and lead to recurrent models that evolve on the equilibrium manifold. We show that equilibrium points are stable, leading to fast convergence of the discretized ODE to ﬁxed points. Furthermore, ERNNs account for long-term dependencies, and can efﬁciently recall informative aspects of data from the distant past. We show that ERNNs achieve state-of-the-art accuracy on many challenging data sets with 3-10x speedups, 1.5-3x model size reduction, and with similar prediction cost relative to vanilla RNNs.},
	language = {en},
	urldate = {2021-05-18},
	journal = {arXiv:1908.08574 [cs, stat]},
	author = {Kag, Anil and Zhang, Ziming and Saligrama, Venkatesh},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.08574},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{zhang_equilibrated_2019,
	title = {Equilibrated {Recurrent} {Neural} {Network}: {Neuronal} {Time}-{Delayed} {Self}-{Feedback} {Improves} {Accuracy} and {Stability}},
	shorttitle = {Equilibrated {Recurrent} {Neural} {Network}},
	url = {http://arxiv.org/abs/1903.00755},
	abstract = {We propose a novel Equilibrated Recurrent Neural Network (ERNN) to combat the issues of inaccuracy and instability in conventional RNNs. Drawing upon the concept of autapse in neuroscience, we propose augmenting an RNN with a time-delayed self-feedback loop. Our sole purpose is to modify the dynamics of each internal RNN state and, at any time, enforce it to evolve close to the equilibrium point associated with the input signal at that time. We show that such selffeedback helps stabilize the hidden state transitions leading to fast convergence during training while efﬁciently learning discriminative latent features that result in state-of-the-art results on several benchmark datasets at test-time. We propose a novel inexact Newton method to solve ﬁxedpoint conditions given model parameters for generating the latent features at each hidden state. We prove that our inexact Newton method converges locally with linear rate (under mild conditions). We leverage this result for efﬁcient training of ERNNs based on backpropagation.},
	language = {en},
	urldate = {2021-05-18},
	journal = {arXiv:1903.00755 [cs, stat]},
	author = {Zhang, Ziming and Kag, Anil and Sullivan, Alan and Saligrama, Venkatesh},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.00755},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{winston_monotone_2021,
	title = {Monotone operator equilibrium networks},
	url = {http://arxiv.org/abs/2006.08591},
	abstract = {Implicit-depth models such as Deep Equilibrium Networks have recently been shown to match or exceed the performance of traditional deep networks while being much more memory efﬁcient. However, these models suffer from unstable convergence to a solution and lack guarantees that a solution exists. On the other hand, Neural ODEs, another class of implicit-depth models, do guarantee existence of a unique solution but perform poorly compared with traditional networks. In this paper, we develop a new class of implicit-depth model based on the theory of monotone operators, the Monotone Operator Equilibrium Network (monDEQ). We show the close connection between ﬁnding the equilibrium point of an implicit network and solving a form of monotone operator splitting problem, which admits efﬁcient solvers with guaranteed, stable convergence. We then develop a parameterization of the network which ensures that all operators remain monotone, which guarantees the existence of a unique equilibrium point. Finally, we show how to instantiate several versions of these models, and implement the resulting iterative solvers, for structured linear operators such as multi-scale convolutions. The resulting models vastly outperform the Neural ODE-based models while also being more computationally efﬁcient. Code is available at http://github.com/locuslab/monotone\_op\_net.},
	language = {en},
	urldate = {2021-05-18},
	journal = {arXiv:2006.08591 [cs, stat]},
	author = {Winston, Ezra and Kolter, J. Zico},
	month = may,
	year = {2021},
	note = {arXiv: 2006.08591},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{hu_exploring_2019,
	title = {Exploring {Weight} {Symmetry} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1812.11027},
	abstract = {We propose to impose symmetry in neural network parameters to improve parameter usage and make use of dedicated convolution and matrix multiplication routines. Due to significant reduction in the number of parameters as a result of the symmetry constraints, one would expect a dramatic drop in accuracy. Surprisingly, we show that this is not the case, and, depending on network size, symmetry can have little or no negative effect on network accuracy, especially in deep overparameterized networks. We propose several ways to impose local symmetry in recurrent and convolutional neural networks, and show that our symmetry parameterizations satisfy universal approximation property for single hidden layer networks. We extensively evaluate these parameterizations on CIFAR, ImageNet and language modeling datasets, showing significant benefits from the use of symmetry. For instance, our ResNet-101 with channel-wise symmetry has almost 25\% less parameters and only 0.2\% accuracy loss on ImageNet. Code for our experiments is available at https://github.com/hushell/deep-symmetry},
	language = {en},
	urldate = {2021-05-18},
	journal = {arXiv:1812.11027 [cs, stat]},
	author = {Hu, Xu Shell and Zagoruyko, Sergey and Komodakis, Nikos},
	month = jan,
	year = {2019},
	note = {arXiv: 1812.11027},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{donti_dc3_2021,
	title = {{DC3}: {A} learning method for optimization with hard constraints},
	shorttitle = {{DC3}},
	url = {http://arxiv.org/abs/2104.12225},
	abstract = {Large optimization problems with hard constraints arise in many settings, yet classical solvers are often prohibitively slow, motivating the use of deep networks as cheap “approximate solvers.” Unfortunately, naive deep learning approaches typically cannot enforce the hard constraints of such problems, leading to infeasible solutions. In this work, we present Deep Constraint Completion and Correction (DC3), an algorithm to address this challenge. Speciﬁcally, this method enforces feasibility via a differentiable procedure, which implicitly completes partial solutions to satisfy equality constraints and unrolls gradient-based corrections to satisfy inequality constraints. We demonstrate the effectiveness of DC3 in both synthetic optimization tasks and the real-world setting of AC optimal power ﬂow, where hard constraints encode the physics of the electrical grid. In both cases, DC3 achieves near-optimal objective values while preserving feasibility.},
	language = {en},
	urldate = {2021-05-18},
	journal = {arXiv:2104.12225 [cs, math, stat]},
	author = {Donti, Priya L. and Rolnick, David and Kolter, J. Zico},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.12225},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{chen_efficient_2019,
	title = {{AN} {EFFICIENT} {HOMOTOPY} {TRAINING} {ALGORITHM} {FOR} {NEURAL} {NETWORKS}},
	url = {https://openreview.net/forum?id=B1l6nnEtwr},
	abstract = {We present a Homotopy Training Algorithm (HTA) to solve optimization problems arising from neural networks. The HTA starts with several decoupled systems with low dimensional structure and tracks...},
	language = {en},
	urldate = {2021-05-16},
	author = {Chen, Qipin and Hao, Wenrui},
	month = sep,
	year = {2019},
}

@article{butcher_implicit_1964,
	title = {Implicit {Runge}-{Kutta} processes},
	volume = {18},
	issn = {0025-5718, 1088-6842},
	url = {https://www.ams.org/mcom/1964-18-085/S0025-5718-1964-0159424-9/},
	doi = {10.1090/S0025-5718-1964-0159424-9},
	abstract = {Advancing research. Creating connections.},
	language = {en},
	number = {85},
	urldate = {2021-05-16},
	journal = {Mathematics of Computation},
	author = {Butcher, J. C.},
	year = {1964},
	pages = {50--64},
}

@article{hendriks_linearly_2021,
	title = {Linearly {Constrained} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2002.01600},
	abstract = {We present an approach to modelling and learning vector-valued signals using neural networks that explicitly satisfy known linear operator constraints. To achieve this, the target function is modelled as a linear transformation of an underlying function, which is in turn modelled by a neural network. This transformation is chosen such that any prediction of the target function is guaranteed to satisfy the constraints. The approach is demonstrated on both simulated and real-data examples.},
	urldate = {2021-04-25},
	journal = {arXiv:2002.01600 [physics, stat]},
	author = {Hendriks, Johannes and Jidling, Carl and Wills, Adrian and Schön, Thomas},
	month = apr,
	year = {2021},
	note = {arXiv: 2002.01600},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics, Statistics - Machine Learning},
}

@article{demazy_probabilistic_2020,
	title = {A {Probabilistic} {Reverse} {Power} {Flows} {Scenario} {Analysis} {Framework}},
	volume = {7},
	issn = {2687-7910},
	doi = {10.1109/OAJPE.2020.3032902},
	abstract = {Distributed Energy Resources (DER), mainly residential solar PV, are embedded deep within the power distribution network and their adoption is fast increasing globally. As more customers participate, these power generation units cause Reverse Power Flow (RPF) at the edge of the grid, directed upstream into the network, thus violating one of the traditional design principles for power networks. The effects of a single residential solar PV system is negligible, but as the adoption by end-consumers increases to high percentages, the aggregated effect is no longer negligible and must be considered in the design and configuration of power networks. This article proposes a framework that helps to predict the RPF intensity probability for any given scenario of DER penetration within the distribution network. The considered scenario parameters are the number and location of each residential DERs, their capacity and the daily net-load profiles. Classical simulation-based approach for this is not scalable as it relies on solving the load-flow equations for each individual scenario. The framework leverages machine learning techniques to make fast and precise RPF prediction within the network for each scenario. The framework enables the Distribution Network Service Providers (DNSPs) to assess DERs penetration scenarios at a granular level, derive and localise the RPF risks and assess the respective impacts on the installed assets for network planning purpose. The framework is illustrated with scenario analysis conducted on an IEEE 123 bus system and OpenDSS and shown that it can lead to multiple orders of magnitude savings in computational time while retaining an accuracy of 94\% or above compared to classical brute force simulations.},
	journal = {IEEE Open Access Journal of Power and Energy},
	author = {Demazy, Antonin and Alpcan, Tansu and Mareels, Iven},
	year = {2020},
	note = {Conference Name: IEEE Open Access Journal of Power and Energy},
	keywords = {Computational modeling, DER-rich penetration, Distribution networks, Mathematical model, Power system reliability, Probabilistic logic, Reliability, Reverse power flows, Time series analysis, intermittent energy source, machine learning, power system planning, risk management},
	pages = {524--532},
}

@article{raissi_multistep_2018,
	title = {Multistep {Neural} {Networks} for {Data}-driven {Discovery} of {Nonlinear} {Dynamical} {Systems}},
	url = {http://arxiv.org/abs/1801.01236},
	abstract = {The process of transforming observed data into predictive mathematical models of the physical world has always been paramount in science and engineering. Although data is currently being collected at an ever-increasing pace, devising meaningful models out of such observations in an automated fashion still remains an open problem. In this work, we put forth a machine learning approach for identifying nonlinear dynamical systems from data. Specifically, we blend classical tools from numerical analysis, namely the multi-step time-stepping schemes, with powerful nonlinear function approximators, namely deep neural networks, to distill the mechanisms that govern the evolution of a given data-set. We test the effectiveness of our approach for several benchmark problems involving the identification of complex, nonlinear and chaotic dynamics, and we demonstrate how this allows us to accurately learn the dynamics, forecast future states, and identify basins of attraction. In particular, we study the Lorenz system, the fluid flow behind a cylinder, the Hopf bifurcation, and the Glycoltic oscillator model as an example of complicated nonlinear dynamics typical of biological systems.},
	urldate = {2021-04-25},
	journal = {arXiv:1801.01236 [nlin, physics:physics, stat]},
	author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.01236},
	keywords = {Mathematics - Dynamical Systems, Mathematics - Numerical Analysis, Nonlinear Sciences - Chaotic Dynamics, Physics - Computational Physics, Statistics - Machine Learning},
}

@article{cremer_probabilistic_2020,
	title = {Probabilistic dynamic security assessment for power system control},
	copyright = {Creative Commons Attribution NonCommercial Licence},
	url = {http://spiral.imperial.ac.uk/handle/10044/1/82177},
	doi = {10.25560/82177},
	abstract = {The integration of renewable energy into the power system requires rethinking the operating paradigms. In the future, the operations will be much more volatile and dynamic than in the past. Novel operating approaches are needed that consider these new dynamics, otherwise, investments in redundant grid infrastructure to maintain the security of supply become necessary. In this thesis, machine-learning is investigated in approaches that can operate the power system while assessing and controlling the security of the energy supply.  
 
An approach is proposed that allows learning optimal security rules for operations. The proposed security rules are interpretable, accurate, take low data quality into account and are very fast when applied. Case studies show that security rules are many orders of magnitude faster than current security assessments.  
 
Then, a probabilistic approach is proposed to use machine-learning in combination with current security assessments. This approach allows managing the risks when considering machine-learning in operations. Case studies show that this can reduce computations by 95\%. On the French power system, the practicality and scalability toward multiple contingencies are validated. 
 
Subsequently, security rules are inferred for a control approach. The control approach minimises risk and operating costs at the same time. A case study shows that risks are kept low while operating costs slightly increase in comparison to an oracle. 
 
In the future, four key areas need attention to move these ideas forward. The first area refers to make use of the full information available (physics and operating data) to make these approaches applicable and the second area surrounds the generation of the data required to train these machine-learning-based approaches. The third area is to generalise for changes in the system such as multiple grid topological structures and the final area is to advance the data-driven control approaches.},
	language = {en-US-GB},
	urldate = {2021-04-25},
	author = {Cremer, Jochen Lorenz},
	month = feb,
	year = {2020},
	note = {Accepted: 2020-09-07T08:08:37Z
Publisher: Imperial College London},
}

@article{blechschmidt_three_2021,
	title = {Three {Ways} to {Solve} {Partial} {Differential} {Equations} with {Neural} {Networks} -- {A} {Review}},
	url = {http://arxiv.org/abs/2102.11802},
	abstract = {Neural networks are increasingly used to construct numerical solution methods for partial differential equations. In this expository review, we introduce and contrast three important recent approaches attractive in their simplicity and their suitability for high-dimensional problems: physics-informed neural networks, methods based on the Feynman-Kac formula and methods based on the solution of backward stochastic differential equations. The article is accompanied by a suite of expository software in the form of Jupyter notebooks in which each basic methodology is explained step by step, allowing for a quick assimilation and experimentation. An extensive bibliography summarizes the state of the art.},
	urldate = {2021-04-25},
	journal = {arXiv:2102.11802 [cs, math]},
	author = {Blechschmidt, Jan and Ernst, Oliver G.},
	month = apr,
	year = {2021},
	note = {arXiv: 2102.11802
version: 2},
	keywords = {Mathematics - Numerical Analysis},
}


@inproceedings{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	volume = {32},
	shorttitle = {{PyTorch}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and {et al.}},
	year = {2019},
	pages = {8024--8035},
}
@inproceedings{stiasny_closing_2022,
	address = {Banff, Canada},
	title = {Closing the {Loop}: {A} {Framework} for {Trustworthy} {Machine} {Learning} in {Power} {Systems}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Closing the {Loop}},
	doi = {10.48550/ARXIV.2203.07505},
	booktitle = {Proceedings of the 11th {Bulk} {Power} {Systems} {Dynamics} and {Control} {Symposium} ({IREP} 2022)},
	author = {Stiasny, Jochen and {et al.}},
	year = {2022},
	keywords = {FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Machine Learning (cs.LG), Systems and Control (eess.SY)},
	pages = {1--21},
}
@article{cuomo_scientific_2022,
	title = {Scientific {Machine} {Learning} {Through} {Physics}–{Informed} {Neural} {Networks}: {Where} we are and {What}’s {Next}},
	volume = {92},
	issn = {1573-7691},
	shorttitle = {Scientific {Machine} {Learning} {Through} {Physics}–{Informed} {Neural} {Networks}},
	doi = {10.1007/s10915-022-01939-z},
	language = {en},
	number = {3},
	urldate = {2023-02-16},
	journal = {Journal of Scientific Computing},
	author = {Cuomo, Salvatore and {et al.}},
	month = jul,
	year = {2022},
	keywords = {Deep Neural Networks, Nonlinear equations, Numerical methods, Partial Differential Equations, Physics–Informed Neural Networks, Scientific Machine Learning, Uncertainty},
	pages = {88},
}
@inproceedings{li_machine-learning-based_2020,
	address = {Tempe, AZ, USA},
	title = {Machine-{Learning}-{Based} {Online} {Transient} {Analysis} via {Iterative} {Computation} of {Generator} {Dynamics}},
	isbn = {978-1-72816-127-3},
	doi = {10.1109/SmartGridComm47815.2020.9302975},
	language = {en},
	urldate = {2021-04-25},
	booktitle = {2020 {IEEE} {SmartGridComm}},
	publisher = {IEEE},
	author = {Li, Jiaming and Yue, Meng and Zhao, Yue and Lin, Guang},
	year = {2020},
	pages = {1--6},
}

@article{raissi_physics-informed_2018,
	volume = {378},
	issn = {0021-9991},
	title = {Physics-informed neural networks},
	doi = {10.1016/j.jcp.2018.10.045},
	abstract = {The U.S. Department of Energy's Office of Scientific and Technical Information},
	language = {English},
	number = {C},
	urldate = {2021-04-22},
	journal = {Journal of Computational Physics},
	author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
	month = nov,
	year = {2018},
}
@book{brenan_numerical_1995,
	title = {Numerical {Solution} of {Initial}-{Value} {Problems} in {Differential}-{Algebraic} {Equations}},
	isbn = {978-0-89871-353-4 978-1-61197-122-4},
	language = {en},
	urldate = {2022-08-19},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Brenan, K. E. and Campbell, S. L. and Petzold, L. R.},
	month = jan,
	year = {1995},
}
@article{stott_power_1979,
	title = {Power system dynamic response calculations},
	volume = {67},
	issn = {1558-2256},
	doi = {10.1109/PROC.1979.11233},
	abstract = {Engineers in the power industry, face the problem that, while stability is increasingly a limiting factor in secure system operation, the simulation of system dynamic response is grossly overburdening on present-day digital computing resources. Each individual response case involves the step-by-step numerical solution in the time domain of perhaps thousands of nonlinear differential-algebraic equations, at a cost of up to several thousand dollars. A high premium is thus to be placed on the use of the most efficient and reliable modern calculation techniques. This paper is a critical tutorial-review of the calculation methods used routinely or investigated for use by the industry. It concentrates on solution concepts and computational techniques rather than on the analysis of the numerical methods. Details of system modeling are only emphasized when they affect the choice of solution method. The paper concludes with a view of the state of the art and a prediction of future directions of development.},
	number = {2},
	journal = {Proceedings of the IEEE},
	author = {Stott, B.},
	month = feb,
	year = {1979},
	keywords = {Computational modeling, Differential equations, Nonlinear dynamical systems, Nonlinear equations, Power engineering and energy, Power engineering computing, Power industry, Power system dynamics, Power system simulation, Power system stability},
	pages = {219--241},
}
@book{sauer_power_1998,
	address = {Upper Saddle River, N.J},
	title = {Power system dynamics and stability},
	isbn = {978-0-13-678830-0},
	language = {eng},
	publisher = {Prentice Hall},
	author = {Sauer, Peter W. and Pai, M. A.},
	year = {1998},
	keywords = {ODE},
}
@article{gurrala_parareal_2016,
	title = {Parareal in {Time} for {Fast} {Power} {System} {Dynamic} {Simulations}},
	volume = {31},
	issn = {0885-8950, 1558-0679},
	doi = {10.1109/TPWRS.2015.2434833},
	number = {3},
	journal = {IEEE Transactions on Power Systems},
	author = {Gurrala, Gurunath and {et al.}},
	month = may,
	year = {2016},
	pages = {1820--1830},
}
@article{liu_solving_2020,
	title = {Solving {Power} {System} {Differential} {Algebraic} {Equations} {Using} {Differential} {Transformation}},
	volume = {35},
	issn = {1558-0679},
	doi = {10.1109/TPWRS.2019.2945512},
	number = {3},
	journal = {IEEE Transactions on Power Systems},
	author = {Liu, Yang and Sun, Kai},
	month = may,
	year = {2020},
	keywords = {Computational modeling, Differential algebraic equations, Differential equations, Load modeling, Mathematical model, Numerical models, Power system stability, differential transformation, numerical integration, power system simulation, time domain simulation, transient stability},
	pages = {2289--2299},
}
@phdthesis{aristidou_time-domain_2015,
	address = {Liège, Belgium},
	title = {Time-domain simulation of large electric power systems using domain-decomposition and parallel processing methods},
	language = {en},
	urldate = {2022-02-05},
	school = {Université de Liège},
	author = {Aristidou, Petros},
	year = {2015},
}
@article{lagaris_artificial_1998,
	title = {Artificial neural networks for solving ordinary and partial differential equations},
	volume = {9},
	issn = {10459227},
	doi = {10.1109/72.712178},
	number = {5},
	journal = {IEEE Transactions on Neural Networks},
	author = {Lagaris, I.E. and Likas, A. and Fotiadis, D.I.},
	month = sep,
	year = {1998},
	pages = {987--1000},
}
@article{karniadakis_physics-informed_2021,
	title = {Physics-informed machine learning},
	volume = {3},
	issn = {2522-5820},
	doi = {10.1038/s42254-021-00314-5},
	language = {en},
	number = {6},
	journal = {Nature Reviews Physics},
	author = {Karniadakis, George Em and {et al.}},
	month = jun,
	year = {2021},
	pages = {422--440},
}

@inproceedings{misyris_physics-informed_2020,
	address = {Montreal, QC, Canada},
	title = {Physics-{Informed} {Neural} {Networks} for {Power} {Systems}},
	isbn = {978-1-72815-508-1},
	doi = {10.1109/PESGM41954.2020.9282004},
	urldate = {2021-06-25},
	booktitle = {2020 {IEEE} {Power} \& {Energy} {Society} {General} {Meeting} ({PESGM})},
	publisher = {IEEE},
	author = {Misyris, George S. and Venzke, Andreas and Chatzivasileiadis, Spyros},
	month = aug,
	year = {2020},
	pages = {1--5},
}
@article{gurrala_large_2017,
	title = {Large {Multi}-{Machine} {Power} {System} {Simulations} {Using} {Multi}-{Stage} {Adomian} {Decomposition}},
	volume = {32},
	issn = {0885-8950, 1558-0679},
	doi = {10.1109/TPWRS.2017.2655300},
	number = {5},
	journal = {IEEE Transactions on Power Systems},
	author = {Gurrala, Gurunath and {et al.}},
	month = sep,
	year = {2017},
	pages = {3594--3606},
}

@article{duan_power_2017,
	title = {Power {System} {Simulation} {Using} the {Multistage} {Adomian} {Decomposition} {Method}},
	volume = {32},
	issn = {0885-8950, 1558-0679},
	doi = {10.1109/TPWRS.2016.2551688},
	number = {1},
	journal = {IEEE Transactions on Power Systems},
	author = {Duan, Nan and Sun, Kai},
	month = jan,
	year = {2017},
	pages = {430--441},
}

@article{wang_timepower_2019,
	title = {A {Time}–{Power} {Series}-{Based} {Semi}-{Analytical} {Approach} for {Power} {System} {Simulation}},
	volume = {34},
	issn = {0885-8950, 1558-0679},
	doi = {10.1109/TPWRS.2018.2871425},
	number = {2},
	journal = {IEEE Transactions on Power Systems},
	author = {Wang, Bin and Duan, Nan and Sun, Kai},
	month = mar,
	year = {2019},
	pages = {841--851},
}

@article{moya_dae-pinn_2023,
	title = {{DAE}-{PINN}: a physics-informed neural network model for simulating differential algebraic equations with application to power networks},
	volume = {35},
	issn = {0941-0643, 1433-3058},
	shorttitle = {{DAE}-{PINN}},
	doi = {10.1007/s00521-022-07886-y},
	language = {en},
	number = {5},
	journal = {Neural Computing and Applications},
	author = {Moya, Christian and Lin, Guang},
	month = feb,
	year = {2023},
	pages = {3789--3804},
}


@misc{cui_predicting_2021,
	title = {Predicting {Power} {System} {Dynamics} and {Transients}: {A} {Frequency} {Domain} {Approach}},
	shorttitle = {Predicting {Power} {System} {Dynamics} and {Transients}},
	url = {http://arxiv.org/abs/2111.01103},
	abstract = {The dynamics of power grids are governed by a large number of nonlinear ordinary differential equations (ODEs). To safely operate the system, operators need to check that the states described by this set of ODEs stay within prescribed limits after various faults. Limited by the size and stiffness of the ODEs, current numerical integration techniques are often too slow to be useful in real-time or large-scale resource allocation problems. In addition, detailed system parameters are often not exactly known. Machine learning approaches have been proposed to reduce the computational efforts, but existing methods generally suffer from overfitting and failures to predict unstable behaviors. This paper proposes a novel framework for power system dynamic predictions by learning in the frequency domain. The intuition is that although the system behavior is complex in the time domain, there are relatively few dominate modes in the frequency domain. Therefore, we learn to predict by constructing neural networks with Fourier transform and filtering layers. System topology and fault information are encoded by taking a multi-dimensional Fourier transform, allowing us to leverage the fact that the trajectories are sparse both in time and spatial (across different buses) frequencies. We show that the proposed approach does not need detailed system parameters, speeds up prediction computations by orders of magnitude and is highly accurate for different fault types.},
	urldate = {2022-08-30},
	publisher = {arXiv},
	author = {Cui, Wenqi and Yang, Weiwei and Zhang, Baosen},
	month = nov,
	year = {2021},
	keywords = {Electrical Engineering and Systems Science - Systems and Control},
}


@article{cybenko_approximation_1989,
	title = {Approximation by superpositions of a sigmoidal function},
	volume = {2},
	issn = {0932-4194, 1435-568X},
	doi = {10.1007/BF02551274},
	language = {en},
	number = {4},
	urldate = {2021-06-25},
	journal = {Mathematics of Control, Signals, and Systems},
	author = {Cybenko, G.},
	month = dec,
	year = {1989},
	pages = {303--314},
}

@book{goodfellow_deep_2016,
	address = {Cambridge, Massachusetts},
	series = {Adaptive computation and machine learning},
	title = {Deep learning},
	isbn = {978-0-262-03561-3},
	publisher = {The MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	keywords = {Machine learning},
}

@article{baydin_automatic_2018,
	title = {Automatic {Differentiation} in {Machine} {Learning}: a {Survey}},
	volume = {18},
	issn = {1533-7928},
	shorttitle = {Automatic {Differentiation} in {Machine} {Learning}},
	number = {153},
	journal = {Journal of Machine Learning Research},
	author = {Baydin, Atilim Gunes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
	year = {2018},
	keywords = {Machine learning},
	pages = {1--43},
}
@book{hastie_elements_2009,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {The {Elements} of {Statistical} {Learning}},
	isbn = {978-0-387-84857-0 978-0-387-84858-7},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2009},
	keywords = {Machine learning},
}
@book{kundur_power_1994,
	address = {New York},
	series = {The {EPRI} power system engineering series},
	title = {Power system stability and control},
	isbn = {978-0-07-035958-1},
	publisher = {McGraw-Hill},
	author = {Kundur, P. and Balu, Neal J. and Lauby, Mark G.},
	year = {1994},
	keywords = {Control, Electric power system stability, Electric power systems, Power system modelling},
}
@article{zimmerman_matpower_2011,
	title = {{MATPOWER}: {Steady}-{State} {Operations}, {Planning}, and {Analysis} {Tools} for {Power} {Systems} {Research} and {Education}},
	volume = {26},
	issn = {0885-8950, 1558-0679},
	shorttitle = {{MATPOWER}},
	doi = {10.1109/TPWRS.2010.2051168},
	number = {1},
	journal = {IEEE Transactions on Power Systems},
	author = {Zimmerman, Ray Daniel and Murillo-Sanchez, Carlos Edmundo and Thomas, Robert John},
	month = feb,
	year = {2011},
	pages = {12--19},
}

@misc{stiasny_publicly_2022,
	title = {Publicly available implementation},
	url = {https://github.com/jbesty},
	language = {en},
	urldate = {2022-08-22},
	journal = {GitHub},
	author = {Stiasny, Jochen},
	year = {2023},
}
@misc{dtu_computing_center_dtu_2022,
	title = {{DTU} {Computing} {Center} resources},
	isbn = {10.48714/DTU.HPC.0001},
	url = {https://doi.org/10.48714/DTU.HPC.0001},
	urldate = {2022-08-19},
	publisher = {Technical University of Denmark},
	author = {DTU Computing Center},
	year = {2022},
}
@misc{biewald_experiment_2020,
	title = {Experiment {Tracking} with {Weights} and {Biases}},
	url = {https://www.wandb.com/},
	abstract = {WandB is a central dashboard to keep track of your hyperparameters, system metrics, and predictions so you can compare models live, and share your findings.},
	urldate = {2022-08-19},
	journal = {WandB},
	author = {Biewald, Lukas},
	year = {2020},
	note = {Software available from wandb.com},
}

@article{andersson_assimulo_2015,
	title = {Assimulo: {A} unified framework for {ODE} solvers},
	volume = {116},
	issn = {03784754},
	shorttitle = {Assimulo},
	doi = {10.1016/j.matcom.2015.04.007},
	language = {en},
	urldate = {2022-08-19},
	journal = {Mathematics and Computers in Simulation},
	author = {Andersson, Christian and Führer, Claus and Åkesson, Johan},
	month = oct,
	year = {2015},
	pages = {26--43},
}
