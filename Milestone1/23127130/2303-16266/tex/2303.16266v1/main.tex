% MAX 7 pages + references

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai23}

%
\usepackage{times}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{multirow}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\usepackage{natbib}
\usepackage[normalem]{ulem}
\input{macros}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage{xcolor} 
\newcommand\pawel[1]{{\color{red} [\bf PW: #1]}}
\newcommand\lukasz[1]{{\color{blue} [\bf ŁL: #1]}}

\pdfinfo{
/TemplateVersion (IJCAI.2023.0)
}

% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\begin{document}

\title{Reinforcement learning for optimization of energy trading strategy}

%%% Provide names, affiliations, and email addresses for all authors.

% \author{
% Anonymous Author
% \affiliations
% Anonymous affiliation
% \emails
% anonymous email
% }

% \iffalse
\author{Łukasz Lepak$^1$ \and Paweł Wawrzyński$^2$
\affiliations 
$^1$Warsaw University of Technology \and 
$^2$IDEAS NCBR 
\emails 
$^1$lukasz.lepak.dokt@pw.edu.pl \and $^2$pawel.wawrzynski@ideas-ncbr.pl
}
% \fi

\maketitle

\begin{abstract}
An increasing part of energy is produced from renewable sources by a~large number of small producers. The efficiency of these sources is volatile and, to some extent, random, exacerbating the energy market balance problem. In many countries, that balancing is performed on day-ahead (DA) energy markets. In this paper, we consider automated trading on a~DA energy market by a~medium size prosumer. We model this activity as a Markov Decision Process and formalize a~framework in which a~ready-to-use strategy can be optimized with real-life data. We synthesize parametric trading strategies and optimize them with an~evolutionary algorithm. We also use state-of-the-art reinforcement learning algorithms to optimize a~black-box trading strategy fed with available information from the environment that can impact future prices. \\ 
{\it Keywords:} Reinforcement learning, Automated trading, Energy market
\end{abstract}

\section{Introduction}

In the year 2021, 6.54\% and 3.63\% of global electricity was produced by wind turbines and solar panels, respectively, after these ratios doubled in 5 preceding years \citep{2022ritchie+1}. The power of wind and sunlight reaching the Earth's surface is, to some extent, random. Therefore, while the rise of renewable energy sources presents the prospect of cheap and clean energy, it also exacerbates the problem of balancing power supply and demand. 

In many countries, the main institution that balances volatile electricity supply and demand is a day-ahead energy market. Every day, agents participating in this market place their buy and sell bids separately for every hour between 0 am and 11 pm the next day. Market clearing prices are then designated for each of these hours, and the bids are consequently executed or not, depending on the proposed prices. 

In this paper, we consider an~energy prosumer, who is an agent that (i) consumes electricity, (ii) produces electricity, and (iii) has electricity storage. What is of our interest here is a~strategy for automated trading on a day-ahead energy market on behalf of this agent. 

%In most studies, the problem of automated trading is reduced to the problem of prediction, as buying and selling may be naturally based on predictions: If the price is going to increase, then the action should be to buy, and vice versa. This approach is reasonable in the case of highly liquid markets, where it is always possible to buy/sell at the current market price. However, it does not make sense where there is a~considerable delay between making bids and actual transactions. 
%Therefore, in this paper we take a~different approach and consider placing market bids as a~sequential decision process under uncertainty. We present a~realistic model of such a~process. This model allows to optimize a~trading strategy that is ready for real-life deployment. 
%The contributions of this paper are as follows: 
%\begin{enumerate} 
%\item We model trading on a day-ahead energy market as a~Markov Decision Process (MDP). 
%\item We design a~parametric strategy of automated trading based on the features of this specific MDP. 
%\item We optimize the parameters of the aforementioned strategy in with an evolutionary algorithm and real-life data. The resulting strategy is deployable in the real market. 
%\item We apply reinforcement learning to optimize a~black-box strategy of automated trading. Once again, we use real-life data and produce a~strategy deployable in the real market. 
%\end{enumerate}

In most studies, decision-making in power systems is based solely on the state of this system. We argue that (i) a~useful strategy for operation in the power system needs to be fed with data on the environment and (ii) it needs to be optimized with real-life data. Firstly, reasonable temporal energy allocation needs to be based on the information that makes it possible to anticipate future prices (even if they are not directly predicted). Therefore, the strategy needs to be based on such information. Secondly, the environment that impacts the energy prices (e.g., weather conditions) has its own temporal dynamics that are hardly possible to model but can be replayed from real-life data, which is enough for strategy optimization. 

Based on the above line of thought, this paper contributes as follows: 
\begin{itemize} 
\item We formalize a~framework in which bidding on a day-ahead energy market is a Markov Decision Process, on which behavior can be optimized with real-life data rather than in model-based simulations or with real-life trial-and-error. 
\item We design a~parametric strategy of automated bidding, which is fed with available information that makes it possible to anticipate future prices. 
\item We apply reinforcement learning to optimize the above strategy. 
\end{itemize} 

The rest of the paper is organized as follows: Section~\ref{sec:problem} defines the problem at hand. Section~\ref{sec:related-work} reviews related work. Section~\ref{sec:model} contains the main contribution of this paper: the model of the market, the strategies of automated trading, and the methods of their optimization. Section~\ref{sec:simulations} presents the results of simulation experiments. The last section concludes the paper. 

\section{Problem definition} 
\label{sec:problem} 

\subsection{Day-ahead energy market} 

Details of the day-ahead (DA) energy market are here taken from the Polish market of this kind. When created in 2000, this market was modeled on existing day-ahead energy markets in Western Europe. It is, therefore, typical. 

Every day between 8 am and 10.30 am, an agent participating in the market places a set of bids defined by: (i) [buy or sell] indicator, (ii) price for 1 MWh [PLN], (iii) volume [number of MWh, at least 0.1 MWh], and (iv) an hour of realization [one of 24 between 0 am and 11 pm the next day]. The bids are independent. Based on the bids placed by all agents, the clearing market price for each hour is designated. A~buy bid is accepted when its price is not below the market price for its hour. A~sell bid is accepted when its price is not above the market price for its hour. At each hour of the next day, the agents that realize their sell bids inject the declared volume of electricity into the system and get the market price for it. The agents that realize their buy bids withdraw the declared volume of electricity from the system and pay the market price for it.  

In order to participate in the day-ahead energy market in Poland, every agent has to apply to become a member of this market and pay 2~000 PLN initial fee. Then, market members must pay 2~000 PLN a year to maintain their member status. Each member may choose one of two options of membership. Option 1: A~participant pays a yearly participation fee equal to 50 000 PLN and 0.08 PLN for each MWh traded, making it well-suited for agents with high turnover. Option 2: A~participant pays a yearly participation fee equal to 1 000 PLN and 0.45 PLN for each MWh traded, which may be better for agents making small or occasional bids. %\sout{Day-ahead energy market participants have to pay a yearly participation fee equal to 50 000 PLN (Variant 1) or 1 000 PLN (Variant 2). Also, each participant pays a~small fee proportional to their turnover, equal to 0.08 PLN/MWh (Variant 1) or 0.45 PLN/MWh (Variant 2). } }

\subsection{Prosumer} 

The agent considered here (i) consumes electricity at random but with a~given statistical profile, (ii) produces electricity with means of limited random efficiency, such as a solar panel or wind turbines, (iii) has energy storage with limited capacity and efficiency (it outputs less energy than it inputs). We also assume that the prosumer is large enough to be able to participate in a DA energy market and not large enough for its bids to change the market prices. 

At every hour, the agent may consume, produce, buy and sell some energy. The residual energy is deposited into or taken from the energy storage. If some fraction of the residuum still remains because of the storage being full or empty, this portion is given to or taken from the market operator, and the agent is charged the corresponding penalty fee. 

An example of a~prosumer considered here is a~group (or an aggregator) of households. It cannot be a~single household, though, as the minimum volume of electricity tradeable on the market is 0.1 MWh, which is too much for a~typical single household to consume or produce.

The objective of the prosumer is to maximize its profit (or minimize its costs) by issuing optimal bids on a DA market. Essentially, the agent should buy the energy when its market price is relatively low, keep it in storage, and/or sell it when the market price is relatively high. The agent should also avoid paying penalty fees, thus avoiding having the storage charged or discharged entirely. Note that the problem does not quintessentially change when the prosumer does not produce nor consume electricity because then it becomes a~temporal arbitrator and its profit still non-trivially depends on the strategy of issuing the buy/sell bids. However, if the prosumer does not have the storage, then events at different times are independent of each other, and the objective degenerates to just predicting the prosumer's own production and consumption. 

\section{Related Work} 
\label{sec:related-work} 

\paragraph{Automated trading on the electricity market.} 

%Strategic bidding, energy brokerage (1997, jeden ze starszych) - \citep{lamont1997strategic}
Bidding on a one-day ahead (DA) energy market was presented as an optimization problem by \citet{lamont1997strategic}. 
%Proposed bidding strategies, optimized with genetic algorithm (2001) - \citep{wen2001strategic}
\citet{wen2001strategic} introduced a~catalog of parametric bidding strategies and optimized their parameters with a~genetic algorithm. 
%Bidding strategy, evolutionary programming - \citep{attaviriyanupap2005new}
\citet{attaviriyanupap2005new} proposed other strategies and applied evolutionary programming to optimize them. 
%Mixed integer linear programming, for producers, small test data - \citep{bakirtzis2007electricity}
\citet{bakirtzis2007electricity} analyze selling on a~DA energy market from a~producer point of view and optimize his bidding strategy with mixed integer linear programming. 
%Review of day-ahead bidding strategy models for power producers - \citep{kwon2012optimization}
Bidding strategies applicable by power producers at a~DA energy market were reviewed by \citet{kwon2012optimization}. 
%Microgrid bidding strategy, optimization model proposed, forecasted wind and energy prices - \citep{liu2015bidding}
\citet{liu2015bidding} analyze a microgrid that produces, stores, consumes energy and buys/sells it on a~DA market. The authors use hybrid stochastic/robust optimization and predictions of prices and wind to optimize the bidding strategy. 
%Strategic bidding, virtual power plant - \citep{rahimiyan2015strategic}
\citet{rahimiyan2015strategic} analyze a~microgrid as above, but the strategy they develop also covers bidding on a~real-time (RT) energy market. 
%Stochastic optimization, 1000 prosumers, Iberian market - \citep{iria2017trading}
\citet{iria2017trading} apply stochastic optimization to designate a~strategy of bidding at a DA energy market by an aggregator of prosumers. It is assumed there that the prosumers do not have any batteries but have access to a~real-time (RT) energy market. 
%Clustering + optimization, 1000 prosumers, Iberian market - \citep{iria2019cluster}
\citet{iria2019cluster} further extend the above work with a~clustering of the aggregated prosumers. 
%\citep{prabavathi2015energy} Bidding system for p2p market - \citep{zhang2016bidding}
More general issues related to energy markets, microgrids, and bidding strategies are analyzed in \citep{prabavathi2015energy,zhang2016bidding}. 

Automated trading on an~energy market is a~complex activity that can be modeled as a~parametric transformation of available information into action. The parameters of this transformation can be determined with usual optimization techniques such as evolutionary algorithms. However, as the more complex behavior is expected and the more complex transformation is required, the less effective these techniques become. An~approach specialized for the optimization of complex behavior is reinforcement learning.   

\paragraph{Reinforcement learning and the electricity market.}
%Przeglądowe - \citep{jogunola2020consensus}, \citep{yang2020reinforcement}, \citep{perera2021applications}
With the advent of electricity prosumerism, energy micro-grids, and flexible price-driven energy consumption, there is an increasing need for automated decision-making and control in various activities undertaken by the energy market participants. Strategies for these agents can be optimized with reinforcement learning (RL). Various applications of RL in power systems are reviewed in \citep{jogunola2020consensus,yang2020reinforcement,perera2021applications}. 
%\citep{nanduri2007reinforcement}
\citet{nanduri2007reinforcement} analyze bidding on a DA energy market as a~zero-sum stochastic game played by energy producers willing to exercise their market power and keep their generators productive. RL is used there to optimize their bidding strategy. 
%RL + day-ahead market for EV fleet charging - \citep{vandael2015reinforcement}
\citet{vandael2015reinforcement} analyze bidding on a DA energy market from the point of view of a~flexible buyer (who charges a~fleet of electric vehicles). His strategy is optimized with RL. 
%P2P energy trading with RL - \citep{chen2018indirect}
%Deep Q-Learning on local energy markets (LEM) - \citep{chen2018local}, \citep{jogunola2021trading}
A~number of papers is devoted to peer-to-peer trading with electricity on a~local, event-driven energy market, with RL applied to optimize the behavior of such peers \citep{chen2018indirect,chen2018local,jogunola2021trading,bose2021reinforcement,qiu2021multi}.  
%Arbitrage, real-time energy market - \citep{wang2018energy}
\citet{wang2018energy} use RL to develop a strategy of temporal arbitrage for an agent that operates on a~real-time energy market with an~energy storage. 
%Hour-ahead RL-based model for demand response, a neural network for price prediction - \citep{lu2019demand}
\citet{lu2019demand} use RL and neural price predictions to optimize the scheduling of home appliances of private users. The authors assume that the electricity prices are changing and are known one hour ahead. %LEM - \citep{bose2021reinforcement}
\citet{bose2021reinforcement} analyze a~similar setting in which the users also trade energy with each other.  
%Multi agent RL, P2P day-ahead energy market, distributed RL algorithms - \citep{qiu2021multi} pawel: w tym artykule DA oznacza double-side auction i nie ma tu mowy o day-ahead 
\citet{qiu2021multi} optimize the user strategies in this setting with multi-agent RL. 
%Day-ahead bidding for extending battery life - \citep{dong2021strategic}
\citet{dong2021strategic} use RL to optimize a~strategy of bidding on a~DA energy market by a~battery energy storage system (BESS). The authors address the dynamics of that process only to a~limited extent. Firstly, the criterion of policy optimization is on-day-ahead profit instead of a~long-term profit. Secondly, no information on the environment that could impact future prices is considered, e.g., weather conditions.  

\citet{dong2021strategic} considers simultaneous trading on a~DA and hour-ahead energy markets by an energy storage operator as a~Markov Decision Process. In this MDP, consecutive days are separate episodes, so between-days dynamics of the market are not accounted for. Discrete actions define the parameters of the bids. They are not based on external observations such as weather forecasts. In the current paper, we take into account the between-days dynamics, continuous parameters of the bids, and weather forecasts. These all lead to significantly better performance of our proposed strategy. 

\section{Model}
\label{sec:model} 

\subsection{Markov Decision Process} 

In this section, we model the automated trading on a day-ahead energy market as a~Markov Decision Process (MDP) \citep{2018sutton+1}. This MDP includes the following components: 
\begin{itemize} 
\item Time, $t=1,2,\dots$. Here time instants denote days. 
\item Actions, $\ctrl_t \in \ctrlSpace$. An action is a set of bids in the form 
\Beq
    \ord{ volume, price, type, hour },
\Eeq 
where $type \in \{\Sell, \Buy\}$, $hour\in\{0\text{ am},1\text{ am}, \dots, 11\text{ pm}\}$. 
\item Reward, $r_t \in \real$ is equal to the profit collected during the day. 
\item States of the environment, $\state_t\in\stateSpace$. A state here is a vector that encompasses all the information about the surrounding world that may influence the market prices of electricity and the volume of its production and consumption by the prosumer. Here we divide the coordinates of the state into {\it uncontrollable}, $\state^u_t$, and {\it controllable}, $\state^c_t$, $\state_t = \ord{\state^u_t, \state^c_t}$. The agent does not influence the uncontrollable state coordinates; they may include an~indicator of the day within the week, an~indicator of the month within the year, and weather forecasts. These state coordinates evolve according to a~stationary conditional probability: 
\Beq \label{state^u} 
    \state^u_{t+1} \sim P(\cdot | \state^u_t). 
\Eeq
The controllable state coordinates are directly determined by the actions taken, and the uncontrollable state coordinates, that is
\Beq \label{state^c} 
    \state^c_{t+1} = f(\state^c_t, \ctrl_t, \state^u_t, \state^u_{t+1}), 
\Eeq
where $f$ is known. Here there is only one controllable state coordinate: the storage level. The $f$ function is known because the storage level trivially results from consuming, producing, buying, and selling energy. 
\end{itemize} 
The assumptions that (i) the prosumer is small enough not to impact the market prices, (ii) the uncontrollable states changes according to the stationary rule \eqref{state^u}, (iii) the controllable state evolves according to a~known transition function \eqref{state^c} have the following implication: Based on a~recorder trajectory of uncontrollable states, $(\state^u_t: t=1,\dots,T)$, we can designate a~strategy of selecting actions $\ctrl_t$ based on states $\state_t$ and evaluate this strategy in a~simulation based on $(\state^u_t: t=1,\dots,T)$. This valuation will be an~unbiased estimate of the performance of this strategy deployed in reality. 

Note that the above-defined division of state variables into controllable and uncontrollable is unusual. In a~typical MDP, we assume that the state changes according to 
\Beq 
    \state_{t+1} \sim P_\state(\cdot | \state_t, \ctrl_t), 
\Eeq 
where the conditional probability $P_\state$ is quite difficult to analyze and estimate. Therefore, a~strategy of choosing actions cannot be evaluated with no bias within a~simulation based on~a~model of~$P_\state$.

\subsection{Designing a strategy}

In general, by a~{\it strategy}, $\pi$, we understand a~probability distribution of actions, $\ctrl_t$, conditioned on states, $\state_t$: 
\Beq \label{pi} 
    \ctrl_t \sim \pi(\cdot|\state_t). 
\Eeq 
In some cases, $\pi$ will be a single point distribution, thereby being a~function. 

Let us denote by $l_t\in(0,1)$ the storage level at midnight when the bids defined in action $\ctrl_t$ start to be realized. The action $\ctrl_t$ is selected at 10.30 am on a preceding day. At this moment, $l_t$ is unknown. However, it is known which of the bids placed with $\ctrl_{t-1}$ have been and will be realized. Therefore, $l_t$ can be estimated with a~reasonable accuracy. We will denote this estimate by $\est l_t$. 

\paragraph{Timing-based strategy (Timing).} A simple strategy may be based on an~observation that the market prices are generally low between 0 am - 3 am and high between 5 pm - 8 pm. That leads to actions comprising the following eight bids: 
\Beq \label{pi:timing} 
    \begin{split} 
    & \ord{(\alpha_1 - \alpha_2\est l_t)/4, +\infty, \Buy, 0\text{ am}} \\ 
    & \ord{(\alpha_1 - \alpha_2\est l_t)/4, +\infty, \Buy, 1 \text{ am}} \\ 
    & \ord{(\alpha_1 - \alpha_2\est l_t)/4, +\infty, \Buy, 2 \text{ am}} \\
    & \ord{(\alpha_1 - \alpha_2\est l_t)/4, +\infty, \Buy, 3 \text{ am}} \\ 
    & \ord{(\alpha_1 + \alpha_2\est l_t)/4, 0, \Sell, 5 \text{ pm}} \\ 
    & \ord{(\alpha_1 + \alpha_2\est l_t)/4, 0, \Sell, 6 \text{ pm}} \\
    & \ord{(\alpha_1 + \alpha_2\est l_t)/4, 0, \Sell, 7 \text{ pm}} \\ 
    & \ord{(\alpha_1 + \alpha_2\est l_t)/4, 0, \Sell, 8 \text{ pm}}
    \end{split} 
\Eeq
where $\alpha_1, \alpha_2$ are positive coefficients. The term $\pm\alpha_2\est l_t$ results from the fact that the more we have in the storage, the less we want to buy, and the more we want to sell. The prices ($+\infty$ and $0$) are defined to ensure that the bids will be accepted. 

\paragraph{Opportunistic strategy (Opportunistic).} Another strategy is based on the observation that the prices generally vary, and the best thing to do is to buy when the price is relatively low and sell when it is relatively high while considering the battery level and production capabilities. That leads to the strategy in which, for each hour $h$, there is a pair of bids: 
% \Beq \label{pi:arbiter}
%     \begin{split} 
%     & \ord{\alpha_1, \bar p (\alpha_{2h+4} - \alpha_2\est l_t), \Buy, h} \\ 
%     & \ord{\alpha_1, \bar p (\alpha_{2h+5} + \alpha_3\est l_t), \Sell, h}, 
%     \end{split} 
% \Eeq
\Beq \label{pi:arbiter}
    \begin{split} 
    & \ord{\bar v\exp(\alpha_{4h+5} + \alpha_1 \est l_t), \bar p \exp(\alpha_{4h+7} + \alpha_3\est l_t), \Buy, h} \\ 
    & \ord{\bar v\exp(\alpha_{4h+6} + \alpha_2 \est l_t), \bar p \exp(\alpha_{4h+8} + \alpha_4\est l_t), \Sell, h}, 
    \end{split} 
\Eeq
where $h=0,1,\dots,23$, $\bar p$ is the market energy price averaged over every hour and the preceding 30 days, $\bar v$ is the maximum possible energy generation volume defined by solar and wind installations, and $\alpha_i$ are coefficients. 
%Here, we always try to sell/buy the same quantum $\alpha_1$ of energy but at different prices. These prices are to be optimized with respect to the profit this strategy yields. 
Here we try to sell/buy varied volumes of energy based on our production capabilities and battery level at different prices. These prices and volumes are to be optimized with respect to the profit this strategy yields. 


\subsection{Optimization of strategy} 

Given real data $(\state^u_t: t=1,\dots,T)$, we optimize a~parametric strategy such as \eqref{pi:timing} or \eqref{pi:arbiter} using a~gradient-free optimization method. In this approach, we need to be able to evaluate the strategy for any~given vector of parameters. Here, an evaluation is a~simulation of events over time $t=1,\dots,T$ with the real data, given strategy in use, and calculating the resulting profit. 

\subsection{Black-box strategy and its optimization with reinforcement learning} 

We design a black-box strategy as a~set of 24 pairs of bids 
\Beq \label{pi:black-box} 
    \begin{split}
    & \ord{\bar v \exp(v^B_h), \bar p \exp(y^B_h), \Buy, h} 
    \\
    & \ord{\bar v \exp(v^S_h), \bar p \exp(y^S_h), \Sell, h},
    \end{split} 
\Eeq 
for $h=0\text{ am}, 1\text{ am}, \dots, 11\text{ pm}$. The numbers $v^B_h, y^B_h, v^S_h, y^S_h$ are produced as a~sum of the output of a~zero-mean normal noise, $\xi_t$, and a~neural network, $g$, output: 
\Beq \label{pi:RL} 
    \begin{bmatrix}
    v^B_{0am}  & \dots  & v^B_{11pm} \\ 
    y^B_{0am}  & \dots  & y^B_{11pm} \\ 
    v^S_{0am}  & \dots  & v^S_{11pm} \\
    y^S_{0am}  & \dots  & y^S_{11pm}
    \end{bmatrix}
    = \xi_t + g(\state_t; \theta), 
    \; \xi_t \sim \normal(0, \Sigma). 
\Eeq
The network $g$ is fed with the state $\state_t$ and parameterized by the vector $\theta$ of trained weights. 

The reason to introduce the noise $\xi_t$ into the bids is exploration: By taking different actions under similar circumstances, the trading agent is able to learn to tell good actions from the inferior ones in the current state. 

In order to optimize the strategy \eqref{pi:RL}, we may use any algorithm of on-line reinforcement learning \citep{2001sutton+2} e.g., A2C \citep{2016mnih+many}, PPO \citep{2017schulman+4}, TD3 \citep{fujimoto2018addressing} or SAC \citep{haarnoja2018soft}. A~training consists of a~sequence of simulated trials in which the trajectory of uncontrollable states $(\state^u_t: t=1,\dots,T)$ is just replayed from the data, and the corresponding trajectory of controllable states $(\state^c_t: t=1,\dots,T)$ is designated based on the uncontrollable states, the actions selected and the function $f$ \eqref{state^c}. 

\section{Experimental study} 
\label{sec:simulations}

In this section, we demonstrate the effectiveness of our proposed black-box strategy optimized with reinforcement learning. We compare it to the parametric strategies optimized with the~gradient-free CMA-ES algorithm \citep{hansen2016cma} and the strategy optimized with the FARL algorithm \citep{dong2021strategic}. 

\subsection{Testing environment}

Our experiments are conducted using a~custom environment simulating day-ahead energy market operations (EngMar in short) based on real-life data from the Polish market. This environment allows for customization of various market settings, such as a~bid creation time, a~scale of the bidding prosumer (defined by the number of households), or its solar and wind energy generation capabilities. The environment is based on the OpenAI Gym environment interface, making it compatible with many reinforcement learning libraries, including Stable-Baselines3 \citep{stable-baselines3}, which we use as our source of RL algorithms. 
% The code for the EngMar environment and experiments described below is available at \citep{rl4energytrading}. 

In our experiments, we use real historical data from the following sources:\footnote{We will make both our framework and the data available upon acceptance of the paper.} 
\begin{itemize}
    \item energy prices -- Polish day-ahead energy market (Fixing I).
    \item weather data -- Polish Meteorology and Water Management Institute.
    \item average energy consumption -- Polish Central Statistical Office.
\end{itemize}

As there are no publicly available historical weather forecast datasets for Poland, we generate one by noising actual weather data. For each day in the actual data, we start at 10 am of the previous day. For every hour from 11 am of the previous day to 11 pm of the currently forecasted day, we generate the forecasts as follows:
\Beq
\begin{split}
     & \epsilon_t \sim \normal(0, \frac{\sigma^2}{24}) \\
     & d_t = \sum_{i=1}^{t} \epsilon_t \\
     & x_{t}^{forecast} = x_{t}^{actual} + d_t
\end{split}
\Eeq
where $\sigma$ is an accuracy of a 24-hour forecast, $d_t$ is a deviation for index $t$ and $x_t^{actual}, x_t^{forecast}$ are actual and forecasted weather for index $t$, respectively. For cloudiness, we assume $\sigma = 2$ Oktas, and for wind speed, we assume $\sigma = 1$ m/s. Here, $t = 0$ denotes 10 am of the previous day, and we are interested in $t \in [14, 37]$ - next-day forecasts. Cloudiness forecasts are clipped to be integer values at least zero, while wind speed forecasts are clipped to be at least zero.

We also test the RL agent without the weather forecast data included in the observations. We do this to check if the weather forecasts allow the agent to define better bids, as this information impact future energy production, consumption, and prices. 

Common environment settings used in our experiments are depicted in Table \ref{tab:env-settings}. We set the action scheduling time to match the Polish day-ahead energy market. Battery and solar panel efficiencies reflect the efficiencies of real-life batteries and solar panels. Wind energy and solar energy limits are tuned so that daily energy production in the environment averages around 1 MWh. The number of households is set to 100 to scale the simulation to represent a medium-sized prosumer, an~aggregator, or a small energy generation facility. 

\begin{table}[!h]
\centering
\begin{tabular}{l | r}
Action scheduling time & 10.30 am \\
\hline
Battery capacity & 2 MWh \\
Battery efficiency  & 85\% \\
\hline
Maximum solar energy generation & 0.4 MWh \\
Solar panel efficiency & 20\% \\
Maximum wind energy generation & 0.05 MWh \\
Maximum wind speed for which \\
\quad wind turbines are still operational & 11 m/s \\
Number of households & 100 \\
\end{tabular}
\caption{Parameters of the EngMar environment used for experiments.}
\label{tab:env-settings}
\end{table}

Energy consumption for the given hour ($E_{c}^{h}$) is calculated as follows:
\Beq \label{eq:eng_consumption} 
    E_{c}^{h} = n \cdot E_{c\_avg}^{h} \cdot \left | 1 + \rho \right |
\Eeq
where $E_{c\_avg}^{h}$ is the average energy consumption per one household for the given hour, $n$ is the number of households, and $\rho \sim \normal(0, 0.03)$ allows the resulting energy consumption to differ each day while maintaining the average value. Equation \eqref{eq:eng_consumption} is prepared so that it scales well with the changing number of households. 

Solar energy production for the given hour ($E_{s}^{h}$) is based on cloudiness value from the actual weather data and is calculated as follows:
\Beq \label{eq:solar_gen} 
    E_{s}^{h} = s_{max} \cdot (1 - \frac{c}{8}) \cdot \eta
\Eeq
where $s_{max}$ is the maximum solar energy generation, $c \in \{0, 1, ..., 7, 8\}$ is the cloudiness value in Oktas ($0$ - clear sky, $8$ - heavy overcast) taken from the weather data and $\eta$ is the solar panel efficiency.

Wind energy production for the given hour ($E_{w}^{h}$) is based on the actual wind speed value from the weather data and is calculated as follows:
\Beq \label{eq:wind_gen} 
    E_{w}^{h} = w_{max} \cdot \frac{ws}{ws_{max}} \cdot (ws \leq ws_{max})
\Eeq
where $w_{max}$ is the maximum wind energy generation, $ws$ is the wind speed, $ws_{max}$ is the maximum wind speed for which the wind turbines are still operational, and $(ws \leq ws_{max}) = 1 \text{ when } ws \leq ws_{max} \text{, else } 0$.

During the simulation, it may turn out that the agent has to buy missing energy or sell excess energy immediately. In that case, he is being penalized for such events. Immediate buying is realized for double the current market price, and immediate selling is realized for half the current market price so that the agent has the incentive to better plan his bids instead of relying on instant buys or sells. Also, we do not include market entry and transaction fees, as they are fixed costs independent of the bidding strategy. 

\subsection{Experiments}

\paragraph{Evolutionary algorithm}

The evolutionary algorithm CMA-ES is used to optimize strategies defined by Equations \eqref{pi:timing} and \eqref{pi:arbiter}. It utilizes data from 2016 to 2018 as the training set and data from 2019 as the testing set. 
%\pawel{niepotrzebne} Training is done on a set of intervals from the training set lasting 90 days. These intervals are in the same order for every vector of parameters sampled by the algorithm, so the comparison between them is fair. 
After training, the resulting parameters (mean values) are evaluated on a single testing interval 365 days long. Table \ref{tab:cmaes-settings} presents the parameters used for the CMA-ES algorithm. Customized initialization for parameters $\alpha_{4h+5}, \alpha_{4h+6}$ of Opportunistic strategy as defined in Equation \eqref{pi:arbiter} prevents the initial samples from creating bids with too high volumes, which leads the strategy to the inefficient solution of creating no bids at all. 
\begin{table}[!h]
\centering
\begin{tabular}{l | r}
\multirow{2}{*}{Initial mean value ($\mu$)} & default: $\normal(0, 1)$ \\
&\eqref{pi:arbiter}, $\alpha_{4h+5}, \alpha_{4h+6}$: $\normal(-2, 1)$ \\
Initial sigma ($\sigma$) & 1 \\
\hline
Population size & automatic ($4 + \lfloor 3 \cdot ln(n) \rfloor$) \\ 
Generations & 100 \\
\end{tabular}
\caption{Parameters of the CMA-ES algorithm used for the experiments. $n$ is the number of parameters in the strategy.}
\label{tab:cmaes-settings}
\end{table}


\paragraph{Reinforcement learning}

Reinforcement learning is used to optimize the strategy defined in \eqref{pi:black-box}. It utilizes data from 2016 to the third quarter of 2018 as the training set, data from the fourth quarter of 2018 as the validation set, and data from 2019 as the testing set. The training is done on random intervals from the training set 90 days long, generated randomly. Periodically, evaluation is done on a single validation interval 90 days long. After the training timesteps budget is depleted, the model for which the highest reward on validation interval was achieved is evaluated on the single testing interval 365 days long. We use the A2C algorithm \citep{2016mnih+many} to optimize the strategy of the RL agent. Parameters used for the A2C algorithm are presented in the supplementary material. 

The action space is limited to range the $[-3, 3]$, which allows the agent to define prices and volumes up to $e^3\approx20$ times smaller/larger than the 30-day average price and maximum possible energy generation volume, respectively. 

The observation of the environment's state (117 values) is passed to the agent at bid placing time and contains the following information:
\begin{itemize}
    \item prices of energy at the current day for every hour (24 values) -- these prices result from the bids created the day before; the agent does not know energy prices for the bid currently submitted.
    \item average energy consumption for $n$ households for every hour, $n \cdot E_{c\_avg}^{h}$ (24 values) -- this is statistical data about consumption, the actual consumption data is designated according to \eqref{eq:eng_consumption}.
    \item current relative battery charge (1 value).
    \item estimated relative battery charge at midnight (1 value).
    \item one-hot encoded information about the current month (12 values).
    \item one-hot encoded information about the current day of the week (7 values).
    \item cloudiness and wind speed forecasts for each hour of the next day (48 values).
\end{itemize}

For comparison, we also applied the FARL algorithm \citep{dong2021strategic}, which is a~conceptually different approach to optimize a black-box bidding strategy. We fed FARL with the same training, evaluation, and test data discussed above. Parameters and details of FARL are presented in the supplementary material. 

\subsection{Results}

%. This algorithm *}[h!]
%     \begin{tabular}{c  \includegraphics[width=0.48\textwidth]{images/fig_sample.png} &\!\!\!\!\!
%     \includegraphics[width=0.48\textwidth]{images/fig_sample.png} \\%&\!\!\!\!\!
%     \includegrapics[width=0.48\textwidth]{images/fig_sample.png} &\!\!\!\!\!
%     \includegraphics[width=0.48\textwidth]{images/fig_sample.png}
%     \end{tabular}
%     \caption{Some results.}
%     \label{fig:results}
% \end{figure*}

\begin{figure*}[!htb]
    \centering
    \scalebox{.85}{\includegraphics[width=\textwidth]{images/balance_all.png}}
    \vspace{-1em}
    \caption{Wallet balances achieved on testing data by different strategies. A2Cw denotes the results of the black-box strategy optimized with the A2C algorithm with weather forecasts included in observations, A2Cnw - without weather forecasts. Numbers in the FARL algorithm denote the number of possible discrete actions. }
    \label{fig:balances}
\end{figure*}

\begin{figure*}[!htb]
    \centering
    \begin{tabular}{c c}
    \scalebox{.85}{\includegraphics[width=0.48\textwidth]{images/battery_A2Cw.png}} &\!\!\!\!\!
    \scalebox{.85}{\includegraphics[width=0.48\textwidth]{images/unscheduled_A2Cw.png}} \\%&\!\!\!\!\!
    \scalebox{.85}{\includegraphics[width=0.48\textwidth]{images/amounts_A2Cw.png}} &\!\!\!\!\!
    \scalebox{.85}{\includegraphics[width=0.48\textwidth]{images/prices_A2Cw.png}} \\%&\!\!\!\!\!
    \end{tabular}
    \vspace{-1em}
    \caption{Plots for the black-box strategy trained with the A2C algorithm. {\it Left-top:} Battery level. {\it Right-top:} Unscheduled energy buying/selling. {\it Left-bottom:} Bid volumes. {\it Right-bottom:} Bid prices.}
    \label{fig:a2c}
\end{figure*}

Results of experiments are shown in Figures \ref{fig:balances}, \ref{fig:a2c}, and in the supplementary material. We define the reference balance of the given day as the difference between the energy produced and consumed multiplied by this day's average energy price. We then calculate the sum of these reference balances during the whole simulation. Note that it is moderately difficult to achieve the reference balance. The agent mostly consumes the energy in the evenings, when it is expensive, and produces it when its price is average. Therefore, the agent needs to ensure it buys little energy when it needs it to reach the reference balance.  

Figure \ref{fig:balances} presents balance changes during test simulations, averaged over 5 test runs on different random seeds. Streaks around graphs represent the range of balances achieved by the given strategy. We can see that the black-box strategies optimized with the A2C algorithm achieve the best return on the test simulation, beating our reference balance and the strategies optimized with the CMA-ES and FARL algorithms. Also, the A2C-trained strategy utilizing weather forecasts as part of its observations is able to achieve higher returns than the A2C-trained strategy without those observations. 

In Figures \ref{fig:a2c}, we look into five days from the middle of the testing simulations. For battery levels plots, we show the average relative battery charge with streaks around graphs indicating the range of levels from all testing simulations, while the other plots were taken from a~chosen testing simulation. There is an unscaled market price graph on the bid volumes plot, which allows for easy identification of whether the successful bid was realized when the price was high or low.

It is seen in Figure \ref{fig:a2c} that the strategy trained with RL behaves reasonably: It charges the battery at about 0 am when the prices are low and discharges at about 8 am when they are high. Unscheduled purchases/sells, which are costly, are rare. 

The simple Timing and Opportunistic strategies behave reasonably (we discuss them in more detail in the supplementary material). However, due to their simplicity, they are unable to represent sufficiently complex behavior to respond efficiently to diverse circumstances. 

The bidding strategy developed by the FARL algorithm barely exhibited reasonable behavior. This algorithm is based on {\it Q-Learning} with function approximation applied in a~rather non-standard way: It learns to make a~sequence of $24$ bids, each time only having access to the previous bidding and several other variables. Reasonable bidding based on that information was not possible. 

%The simple Timing strategy based on buying the same amount at night and selling in the afternoon also works reasonably. However, it is not able to adapt sufficiently to the circumstances, hence its performance is noticeably worse than that of A2C. 

%The more elaborate Opportunistic strategy achieves a~better return than its simple Timing counterpart. Its behavior is reasonable: It buys mostly at 0 am (at low prices) and sells mostly at 8 am and 4 pm (at high prices). However, it is still not able to, on average, improves the results over the reference balance. Also, the range of returns of this strategy is noticeably larger than those of other strategies. This strategy usually tries to place selling bids with low prices and high volumes and buying bids with high prices and low volumes. The wide range of returns between runs indicates that the optimization of this strategy is susceptible to getting stuck in local optima. Therefore, it is unlikely that this strategy's globally optimal parameters have been found with the CMA-ES algorithm. 

\subsection{Battery capacity optimization} 

The battery is often the largest part of the prosumer installation cost. Our proposed approach can be readily used to choose the battery from the possible options. It is enough to perform the strategy optimization for each option and compare the incomes with the battery costs. 

Table \ref{tab:a2c-diff-batteries} presents the income gained with the strategy optimized with the A2C algorithm with weather forecasts as input, depending on the battery capacity. A figure with wallet balance changes in testing simulations and additional comments are available in the supplementary material. It is seen that the larger the battery capacity, the larger the income. 

%\lukasz{However, with larger battery capacity, the strategy becomes more speculative, as the agent has more resources to perform bids with higher energy amounts. In some cases, this may lead to losing money on immediate transactions and lowering overall profit. We can see this increased risk when comparing results for capacities 2.0 and 3.0. While the latter has higher mean profit, it also has significantly higher standard deviation, which makes the former safer and still highly profitable option. }

\begin{table}[!h]
\centering
\begin{tabular}{l | r}
Maximum battery capacity (MWh) & Achieved income \\
\hline
1.0 & 39937.62 $\pm$ 1723.72 \\
1.5 & 48245.24 $\pm$ 1182.34 \\
2.0 & 53107.61 $\pm$ 560.78 \\
3.0 & 54770.32 $\pm$ 4760.21 \\
\end{tabular}
\caption{Incomes achieved for different maximum battery capacities with A2C-optimized strategy with weather forecasts. Incomes are averaged over 5 test runes with the same random seeds and are provided with their respective standard deviations.}
\label{tab:a2c-diff-batteries}
\end{table}

\subsection{Discussion} 

The optimal bidding strategy among those analyzed here is based on neural networks trained with reinforcement learning and fed with weather forecasts. Weather impacts the production of energy (e.g., by wind turbines), its consumption (e.g., by air conditioners), and thus its prices. Consequently, optimal bids need to be based on these forecasts. We have tried several RL algorithms. A2C yielded the best performance. The algorithm that was especially disappointing was SAC \citep{haarnoja2018soft}. This algorithm is based on the action-value function with the action (bid parameters in this case) having 96 dimensions. Under these circumstances, the action-value function was impossible to approximate with sufficient accuracy, hence poor performance. 

Parametric strategies with parameters optimized with the CMA-ES evolutionary algorithm behaved worse than those based on neural networks. Even though they were not very complex, their globally optimal parameters were difficult to find for CMA-ES. One can come up with even more elaborate strategies than \eqref{pi:arbiter} (and in fact, we have), but then this strategy will have even more parameters, and their optimal values will be even more difficult to find for any gradient-free optimization algorithm. 

The bidding strategy learned with the FARL algorithm delivered disappointing results, even worse than those achieved with optimized parametric strategies. Its management of available information proved insufficient to effectively map available observations into actions.

\section{Conclusions} 
\label{sec:conclusions} 

In this paper, we proposed a framework for optimization of bidding strategy on a~day-ahead energy market based on simulations and real-life data. We have optimized two parametric strategies with the~state-of-the-art evolutionary algorithm CMA-ES. We have also used reinforcement learning to optimize two strategies that produced bids for this market. One of them was fed weather forecasts, and the other was not. The strategy fed with weather forecasts produced the highest financial return.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% The acknowledgments section is defined using the "acks" environment
%%% (rather than an unnumbered section). The use of this environment 
%%% ensures the proper identification of the section in the article 
%%% metadata as well as the consistent spelling of the heading.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% The next two lines define, first, the bibliography style to be 
%%% applied, and, second, the bibliography file to be used.

\bibliographystyle{named} 
\input{main.bbl}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

