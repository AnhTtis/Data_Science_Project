\section{Related Work}
In this section, we provide an overview of monocular depth estimation, panoptic segmentation, and unsupervised domain adaptation including continual learning.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\parskip=5pt
\noindent\textit{Monocular Depth Estimation:}
Monocular depth estimation is the task of predicting a dense depth map from a single RGB image. While supervised approaches exploit measurements from range sensors to supervise the network predictions~\cite{qiao2021vipdeeplab}, unsupervised methods leverage geometric cues from temporal context~\cite{godard2017unsupervised, zhou2017unsupervised}. Most of the research on unsupervised learning tackles the limitations of the so-called photometric loss function that is usually employed for unsupervised depth learning, e.g., dynamic object handling~\cite{bevsic2022dynamic, casser2019dynamic, li2021dynamic}, occlusion~\cite{godard2019digging}, and abrupt illumination changes~\cite{yang2020d3vo}.
In this work, we leverage Monodepth2~\cite{godard2019digging} for unsupervised depth learning and employ it similarly to \citet{guizilini2021geometric} for the purpose of domain adaptation.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\parskip=5pt
\noindent\textit{Panoptic Segmentation:}
Panoptic segmentation unifies the two tasks of semantic and instance segmentation by fusing the respective targets into a joint output. Furthermore, semantic classes are grouped into ``stuff'' classes, e.g., \textit{road} or \textit{building}, and ``thing'' classes, e.g., \textit{car} or \textit{pedestrian}. In particular, the goal of vision-based panoptic segmentation is to assign a semantic class to every pixel of an image and an additional instance label to each object belonging to the ``thing'' classes. Panoptic segmentation networks usually comprise a joint encoder and separate decoders for each subtask, whose outputs are subsequently merged by a panoptic fusion module.
Existing works can be categorized into bottom-up~\cite{cheng2020panoptic, mohan2022perceiving} and top-down~\cite{gosala2022bird, mohan2022amodal} approaches. Whereas bottom-up methods detect instances in a proposal-free manner from the semantic prediction, top-down methods include an additional proposal generation step. Contradictions to the semantic predictions are then resolved during post-processing.
In this work, we build upon the bottom-up Panoptic-Deeplab~\cite{cheng2020panoptic} with changes to the semantic head according to~\citet{guizilini2021geometric}.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\parskip=5pt
\noindent\textit{Unsupervised Domain Adaptation:}
Domain adaptation aims to bridge the domain gap between a source domain $\mathcal{S}$ used for training and a target domain $\mathcal{T}$ used for inference to mitigate a loss in performance.
An important aspect is whether the performance on the source domain must be maintained, linking domain adaptation to continual learning~(CL)~\cite{lopez2017gradient}, where the objective of a task or the task itself can change over time. A CL system has to adapt to the new target objective while retaining the knowledge to solve the previous task(s), i.e., avoiding catastrophic forgetting. Ideally, the CL system can further achieve positive forward transfer, i.e., improve on future yet untrained tasks given the current task.
In many real-world scenarios ground truth annotations for the target domain are not available, thus requiring unsupervised domain adaptation (UDA) methodology. Offline UDA assumes that abundant target data is accessible. However, in order to guarantee the continuous operation of a robot in new domains, UDA approaches have to work online without previous target data collection.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Offline UDA can leverage both annotated source data and abundant unlabeled target data, enabling learning a given task from $\mathcal{S}$ while simultaneously adapting the network to $\mathcal{T}$.
For depth estimation, DESC~\cite{lopez2020desc} adapts from a synthetic source domain containing RGB images and ground truth depth to a real-world target domain by performing source-to-target style transfer and using a consistency loss between depth predictions from RGB and semantic maps.
GUDA~\cite{guizilini2021geometric} tackles UDA for semantic segmentation using depth estimation as a proxy task. A shared encoder with task-specific heads for depth estimation and semantic segmentation is trained via source supervision. Simultaneously, data from $\mathcal{T}$ is used to update the encoder and depth head in an unsupervised manner. Due to the refined weights of the encoder, the semantic predictions on $\mathcal{T}$ improve as well.
Another common approach for adapting semantic segmentation is cross-domain sampling enabling partial supervision on $\mathcal{T}$. DACS~\cite{tranheden2021dacs} mixes images from $\mathcal{S}$ and $\mathcal{T}$ by copying the pixels of a source image to a target image based on the semantic labels~\cite{olsson2021classmix}. The semantic prediction of the target image is updated with ground truth source labels for the same set of pixels. The network is then jointly trained on annotated source data and the pseudo-labeled mixing data.
Recently, ConfMix~\cite{mattolin2023confmix} proposed a simple yet effective mixing strategy for object detection, where a target image is divided into rectangular image regions. The region with the most confident predictions is then copied onto a source image and the respective ground truth annotations.
Finally, \citet{huang2021cross} propose a UDA method for panoptic segmentation by regularizing complementary features from semantic and instance segmentation.
In this work, we extend the aforementioned mixing strategies to instance-based sampling and explicitly address differing camera parameters.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

During online UDA, samples from $\mathcal{T}$ can only be accessed in a consecutive manner resembling the image stream of a camera. Typically, a network is trained offline via supervision on $\mathcal{S}$ and then adapted to $\mathcal{T}$ during inference time. Such a setup rises two main challenges: first, incoming target samples originate from highly similar scenes and thus drastically reduce the diversity; second, this similarity of consecutive samples leads to a strong overfitting of the model to the scene~\cite{zhang2020online}.
Initial works for online UDA focused on depth learning~\cite{kuznietsov2021comoda, zhang2020online} and visual odometry~\cite{li2020self, voedisch2023continual}, for which unsupervised training schemes are already well established.
Whereas \citet{zhang2020online} propose novel network modules that are adapted via a meta-learning paradigm to mitigate forgetting, CoMoDA~\cite{kuznietsov2021comoda} employs a common CL strategy, i.e., experience replay to combine the online target sample with previously seen samples. Continual SLAM~\cite{voedisch2023continual} also uses unsupervised depth estimation as a proxy task to enhance visual odometry during inference time. Additionally, it demonstrates that incorporating samples from $\mathcal{S}$ and previous target domains $\mathcal{T}_i$ prevents catastrophic forgetting when revisiting domains.
Similar settings involving multiple target domains, which are hence closely related to classical CL, are also addressed for semantic segmentation. CBNA~\cite{klingner2022continual} mixes statistics from $\mathcal{S}$ and $\mathcal{T}$ to update the batch normalization layers and showcases the efficacy of the approach on continually visited target domains.
CoTTA~\cite{wang2022cotta} adapts the entire network without using source data but self-supervision. To tackle error accumulation, it uses an exponential moving average filter and student-teacher consistency when updating the network weights.
Using depth estimation as a proxy task, \citet{kuznietsov2022towards} extend GUDA~\cite{guizilini2021geometric} to online UDA with experience replay and confidence regularization on the semantic predictions.
To the best of our knowledge, we propose the first approach for online continual UDA for joint depth estimation and panoptic segmentation.
