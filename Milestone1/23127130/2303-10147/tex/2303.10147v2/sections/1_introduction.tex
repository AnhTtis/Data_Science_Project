\section{Introduction}

Deploying robots such as autonomous cars in urban scenarios requires a holistic understanding of the environment with a unified perception of semantics, instances, and depth. The joint solution of these tasks enables vision-based methods to generate a 3D semantic reconstruction of the scene, which can be leveraged for downstream applications such as localization or planning. While deep learning-based state-of-the-art approaches perform well when inference is done under similar conditions as used for training, their performance can drastically decrease when the new target domain differs from the source domain, e.g., due to environmental conditions~\cite{valada2016towards}, different sensor parameters~\cite{bevsic2022unsupervised, cheng2022vsseg}. This domain gap poses a great challenge for robotic platforms that are deployed in the open world without prior knowledge about the target domain. Additionally, unlike the source domain where ground truth annotations are generally assumed to be known and can be used for the initial training, such supervision is not applicable to the target domain due to the absence of labels, rendering classical domain adaptation methods unsuitable. Unsupervised domain adaptation attempts to overcome these limitations. However, the vast majority of proposed approaches focuses on sim-to-real domain adaptation mostly in an offline manner~\cite{guizilini2021geometric, lopez2020desc}, i.e., a directed knowledge transfer without the need to avoid catastrophic forgetting and with access to abundant target annotations. Additionally, such works might not consider limitations on a robotic platform, e.g., available compute hardware and limited storage capacity~\cite{kuznietsov2021comoda, voedisch2023continual}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/teaser.pdf}
    \vspace*{-.5cm}
    \caption{Neural networks often perform poorly when deployed on a target domain that differs from the source domain used for training. To close this domain gap, we propose to continuously adapt the network by exploiting online target images. To mitigate catastrophic forgetting and enhance generalizability, we leverage a fixed-size replay buffer allowing the method to revisit data from both the source and target domains.
    }
    \label{fig:teaser}
    \vspace*{-.5cm}
\end{figure}

In this work, we use online continual learning to address these challenges for depth estimation and panoptic segmentation in a multi-task setup. As shown in \reffig{fig:teaser}, we leverage images from an onboard camera to perform online continual learning enhancing performance during inference time. While a naive approach would result in overfitting to the current scene, our method \net mitigates forgetting by leveraging experience replay of both source data and previously seen target images. We combine a classical replay buffer with generative replay in the form of a novel cross-domain mixing strategy allowing us to exploit supervised ideas also for unlabeled target data. We explicitly address the aforementioned hardware limitations by using only a single GPU and restricting the replay buffer to a fixed size.
We demonstrate that \net successfully improves on new target domains without sacrificing performance on previous domains.

The main contributions of this work are as follows:
\begin{enumerate}
    \item We introduce \net, the first online continual learning approach for joint monocular depth estimation and panoptic segmentation.
    \item We propose a novel cross-domain mixing strategy to adapt panoptic segmentation to unlabeled target data.
    \item To address the storage restrictions of robotic platforms, we leverage a fixed-size replay buffer based on rare class sampling and image diversity.
    \item We extensively evaluate \net and compare it to other methods in challenging real-to-real settings.
    \item We release our code and the trained models at \mbox{\url{http://codeps.cs.uni-freiburg.de}}.
\end{enumerate}
