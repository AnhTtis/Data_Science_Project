\section{Experimental Evaluation}

In the following sections, we provide further details on the pretraining step and the datasets that we evaluate on. We present extensive experimental results on the efficiency and efficacy of our proposed approach and include ablation studies on important design choices. Finally, we expand the experimental setup to multi-domain adaptation closely resembling classical continual learning settings.

We follow the evaluation protocol of \citet{zhang2020online}. In detail, we compute the evaluation metrics on the frame of the current timestamp before using the same frame to perform backpropagation and update the model weights. Once \SI{70}{\percent} of a sequence is processed, we calculate the average of the accumulated metrics. Additionally, we report the scores on the remaining \SI{30}{\percent} of the same sequence without further weight updates to analyze the performance of the adapted model. In the tables, we refer to these types of evaluation by protocol~1 and protocol~2, respectively. We further denote the respective parts of a sequence by \textit{adapt} and \textit{eval}.
Unlike \citet{zhang2020online}, we define our task in the context of continual learning. To measure knowledge retention and hence mitigate catastrophic forgetting, we introduce protocol~3 as evaluating the adapted model on the \textit{val} split of the source dataset.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Datasets}
To simulate data from a variety of domains, we employ our method on three datasets, namely Cityscapes~\cite{cordts2016the}, KITTI-360~\cite{liao2022kitti360}, and SemKITTI-DVPS~\cite{behley2019semantickitti}. In particular, we utilize Cityscapes for pre-training and sequences of both KITTI-360 and SemKITTI-DVPS for adaptation. In the supplementary video we further provide qualitative results on in-house data recorded with our robotic platform.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\parskip=3pt
\noindent\textit{Cityscapes:}
The Cityscapes Dataset~\cite{cordts2016the} is a large-scale autonomous driving dataset that was recorded in 50 cities in Germany and bordering regions. It includes RGB images, panoptic annotations, and vehicle metadata. In this work, we utilize the fine panoptic labels to train the semantic and instance heads in a supervised manner. Additionally, we leverage the sequence image data of the left camera to train the depth prediction in an unsupervised fashion. Finally, we compute the depth error metrics using the provided disparity maps.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\parskip=3pt
\noindent\textit{KITTI-360:}
The KITTI-360 Dataset~\cite{liao2022kitti360} is a relatively recently released public dataset for the domain of autonomous driving, which was recorded in the city of Karlsruhe, Germany. It includes both 2D and 3D panoptic annotations for RGB images and LiDAR data. In this work, we predominantly utilize the RGB images to simulate an online image stream of an onboard camera. In particular, we use these images to adapt our network in a self-supervised manner. To compute evaluation metrics, we compare our predictions with the ground truth measurements and annotations of the dataset.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\parskip=3pt
\noindent\textit{SemKITTI-DVPS:}
The SemKITTI-DVPS~\cite{qiao2021vipdeeplab} is based on the odometry benchmark of the KITTI Dataset~\cite{geiger2012are}, which was recorded in Karlsruhe, Germany. We utilize the RGB images to simulate an onboard camera and to adapt our network to the new domain. Furthermore, we compute depth metrics based on the provided projected LiDAR points and the semantic/panoptic metrics using the extension SemanticKITTI~\cite{behley2019semantickitti}.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\parskip=3pt
\noindent\textit{Semantic Labels:}
As the aforementioned datasets use different labeling policies for the semantic annotations, we use the 19 classes of Cityscapes as the reference definition and remap classes of the other datasets. However, certain classes do not exist in the adaptation datasets (\textit{wall}, \textit{traffic light}, \textit{bus}, \textit{train}). For consistency across the datasets, we merge \textit{wall} with \textit{building} and remove the other three classes. Additionally, we merge \textit{motorcycle} and \textit{bicycle} into \textit{two-wheeler} to increase the number of annotated pixels. Consequently, we consider nine ``stuff'' classes and five ``thing'' classes, listed in \reftab{tab:classwise-evaluation}. Note that \textit{sky} is not included in SemKITTI-DVPS due to using LiDAR annotations and hence excluded in the evaluation on this dataset.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}
% \footnotesize
\scriptsize
\centering
\caption{Efficacy of the Network}
\vspace{-0.2cm}
\label{tab:comparison-with-guda}
\setlength\tabcolsep{4.8pt}
\begin{threeparttable}
    \begin{tabular}{l | c | c ccc}
        \toprule
        \textbf{Method} & \textbf{Dataset} & \textbf{mIoU} $\uparrow$ & \textbf{RMSE} $\downarrow$ & \textbf{Abs Rel} $\downarrow$ & $\mathbf{\delta_1}$ $\uparrow$ \\
        \midrule
        GUDA & \multirow{2}{*}{KITTI} & 
                        ---  & 4.42 & 0.11 & 0.88 \\
        \net & & 62.8 & 3.52 & 0.09 & 0.90 \\
        \midrule
        GUDA & \multirow{2}{*}{Cityscapes} & 
                        72.9  & ---   & ---  & ---  \\
        \net & & 72.9 & 10.16 & 0.19 & 0.78 \\
        \bottomrule
    \end{tabular}
    \footnotesize
    Our utilized network is able to reproduce the performance of the baseline method GUDA~\cite{guizilini2021geometric} for both semantic segmentation (mIoU) and depth estimation (RMSE, Abs Rel, $\delta_1$).
    The performance of GUDA is reported by the authors. To evaluate \net on KITTI, we use sequence 08 \textit{eval} of SemKITTI-DVPS.
\end{threeparttable}
\vspace{-0.5cm}
\end{table}

\subsection{Pretraining Protocol}

The initial state of the network weights before adaptation is obtained by initializing the encoders using pretrained weights from the ImageNet dataset, followed by training the entire model on the Cityscapes dataset. In detail, we use the Adam optimizer with a constant learning rate $lr = 0.0001$ and train the entire network for 250 epochs. In our experiments, we compare the performance of our approach to directly training on the target dataset, which can be considered as a theoretical upper limit having full target knowledge. Due to the unbalanced class distribution of KITTI-360, we train a copy of the network in two steps, using the Adam optimizer with $lr = 0.0001$ on sequences \mbox{$00$-$07$}. We train for 45 epochs while ignoring the most common classes \textit{road}, \textit{sidewalk}, \textit{building}, and \textit{vegetation}, followed by 55 epochs including all classes. Similarly, for SemKITTI-DVPS, we train another copy of the network on sequences $00$-$06$, $09$, and $10$ for 30 epochs without the aforementioned classes plus \textit{terrain} and \textit{sky}, which is not included in the dataset, followed by 30 epochs including all classes.
In \reftab{tab:comparison-with-guda}, we demonstrate that our implemented network is able to reproduce the performance of the baseline method GUDA~\cite{guizilini2021geometric}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table*}[t]
% \footnotesize
\scriptsize
\centering
\caption{Adaptation Performance}
\vspace{-0.2cm}
\label{tab:baselines}
% \setlength\tabcolsep{7pt}
\begin{threeparttable}
    \begin{tabular}{l | c | c ccc cc | c ccc cc}
        \toprule
        \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Sequence}} & \multicolumn{6}{c|}{\textbf{Protocol 1}} & \multicolumn{6}{c}{\textbf{Protocol 2}} \\
        & & mIoU $\uparrow$ & PQ $\uparrow$ & SQ $\uparrow$ & RQ $\uparrow$ & RMSE $\downarrow$ & Abs Rel $\downarrow$ & mIoU $\uparrow$ & PQ $\uparrow$ & SQ $\uparrow$ & RQ $\uparrow$ & RMSE $\downarrow$ & Abs Rel $\downarrow$ \\
        \midrule
        % Only source & \multirow{2}{*}{00}
        %             & 51.61 & 39.10 & 72.72 & 50.48 & 6.54 & 0.36 & 49.94 & 35.29 & 72.14 & 45.50 & 6.08 & 0.34 \\
        % \net      & & 53.76 & 40.72 & 72.90 & 52.51 & 5.09 & 0.19 & 52.08 & 36.08 & 72.58 & 46.08 & 4.34 & 0.15 \\
        % \midrule
        % Only source & \multirow{2}{*}{02}
        %             & 45.97 & 31.83 & 67.62 & 41.08 & 6.26 & 0.35 & 46.55 & 30.13 & 65.03 & 39.30 & 6.06 & 0.36 \\
        % \net      & & 46.62 & 32.11 & 67.74 & 41.62 & 4.31 & 0.16 & 47.48 & 30.33 & 65.35 & 39.46 & 3.76 & 0.13 \\
        % \midrule
        % Only source & \multirow{2}{*}{03}
        %             & 46.63 & 28.15 & 57.41 & 35.23 & 8.20 & 0.34 & 52.10 & 28.20 & 56.67 & 35.77 & 7.34 & 0.29 \\
        % \net      & & 47.94 & 29.05 & 58.07 & 36.10 & 8.26 & 0.33 & 52.00 & 31.13 & 61.51 & 39.65 & 6.98 & 0.18 \\
        % \midrule
        % Only source & \multirow{2}{*}{04}
        %             & 45.02 & 29.34 & 65.48 & 38.15 & 6.70 & 0.37 & 45.53 & 30.13 & 79.85 & 38.89 & 6.61 & 0.38 \\
        % \net      & &  \\
        % \midrule
        % Only source & \multirow{2}{*}{05}
        %             & 48.94 & 32.19 & 66.80 & 41.37 & 6.76 & 0.37 & 44.52 & 27.34 & 60.72 & 35.58 & 5.93 & 0.43 \\
        % \net      & &  \\
        % \midrule
        % Only source & \multirow{2}{*}{06}
        %             & 46.03 & 29.88 & 66.58 & 38.42 & 6.09 & 0.39 & 46.28 & 31.79 & 70.47 & 41.40 & 6.12 & 0.37 \\
        % \net      & & \\
        % \midrule
        % Only source & \multirow{2}{*}{07}
        %             & 40.54 & 28.48 & 66.52 & 34.42 & 7.83 & 0.34 & 59.07 & 27.62 & 45.88 & 35.41 & 9.64 & 0.38 \\
        % \net      & & 41.46 & 29.30 & 67.64 & 35.58 & 6.50 & 0.22 & 60.57 & 30.91 & 50.25 & 39.79 & 6.48 & 0.20 \\
        % \midrule
        % Only source & \multirow{2}{*}{09}
        %             & 50.59 & 37.26 & 74.06 & 47.38 & 6.03 & 0.36 & 50.78 & 36.57 & 72.22 & 46.75 & 5.60 & 0.35 \\
        % \net      & & 52.29 & 38.02 & 74.88 & 48.21 & 4.74 & 0.19 & 51.53 & 37.56 & 72.87 & 47.99 & 4.56 & 0.16 \\
        % \midrule
        % Only source & \multirow{2}{*}{10}
        %             & 51.94 & 32.60 & 71.27 & 32.60 & 8.06 & 0.35 & 45.74 & 30.62 & 69.56 & 39.49 & 7.90 & 0.33 \\
        % \net      & & 53.02 & 33.50 & 71.53 & 33.50 & 7.19 & 0.22 & 49.91 & 31.91 & 70.68 & 40.95 & 5.57 & 0.15 \\
                Only source & \multirow{2}{*}{00} & 51.61 & 39.10 & 72.72 & 50.48 & 6.54 & 0.36 & 49.94 & 35.29 & 72.14 & 45.50 & 6.08 & 0.34 \\
        \net      & & \textbf{53.76} & \textbf{40.72} & \textbf{72.90} & \textbf{52.51} & \textbf{5.09} & \textbf{0.19} & \textbf{52.08} & \textbf{36.08} & \textbf{72.58} & \textbf{46.08} & \textbf{4.34} & \textbf{0.15} \\
        \midrule
        Only source & \multirow{2}{*}{02} & 45.97 & 31.83 & 67.62 & 41.08 & 6.26 & 0.35 & 46.55 & 30.13 & 65.03 & 39.30 & 6.06 & 0.36 \\
        \net      & & \textbf{46.62} & \textbf{32.11} & \textbf{67.74} & \textbf{41.62} & \textbf{4.31} & \textbf{0.16} & \textbf{47.48} & \textbf{30.33} & \textbf{65.35} & \textbf{39.46} & \textbf{3.76} & \textbf{0.13} \\
        \midrule
        Only source & \multirow{2}{*}{03} & 46.63 & 28.15 & 57.41 & 35.23 & \textbf{8.20} & 0.34 & \textbf{52.10} & 28.20 & 56.67 & 35.77 & 7.34 & 0.29 \\
        \net      & & \textbf{47.94} & \textbf{29.05} & \textbf{58.07} & \textbf{36.10} & 8.26 & \textbf{0.33} & 52.00 & \textbf{31.13} & \textbf{61.51} & \textbf{39.65} & \textbf{6.98} & \textbf{0.18} \\
        \midrule
        Only source & \multirow{2}{*}{04} & 45.02 & 29.34 & 65.48 & 38.15 & 6.70 & 0.37 & 45.53 & 30.13 & \textbf{70.85} & 38.89 & 6.61 & 0.38 \\
        \net      & & \textbf{45.40} & \textbf{29.78} & \textbf{65.89} & \textbf{38.84} & \textbf{5.00} & \textbf{0.19} & \textbf{45.68} & \textbf{30.63} & 66.18 & \textbf{39.89} & \textbf{4.33} & \textbf{0.17} \\ % seed 15, segments 2
        \midrule
        Only source & \multirow{2}{*}{05} & 48.94 & 32.19 & 66.80 & 41.37 & 6.76 & 0.37 & \textbf{44.52} & \textbf{27.34} & \textbf{60.72} & \textbf{35.58} & 5.93 & 0.43 \\
        \net      & & \textbf{49.26} & \textbf{32.96} & \textbf{66.98} & \textbf{42.40} & \textbf{5.25} & \textbf{0.21} & 43.79 & 26.48 & 60.33 & 34.88 & \textbf{4.68} & \textbf{0.25} \\ % seed 15
        \midrule
        Only source & \multirow{2}{*}{06} & 46.03 & 29.88 & 66.58 & 38.42 & 6.09 & 0.39 & 46.28 & 31.79 & 70.47 & 41.40 & 6.12 & 0.37 \\
        \net      & & \textbf{46.53} & \textbf{30.45} & \textbf{66.66} & \textbf{39.20} & \textbf{4.97} & \textbf{0.22} & \textbf{47.27} & \textbf{31.99} & \textbf{70.74} & \textbf{41.71} & \textbf{4.23} & \textbf{0.18} \\ % seed 15
        \midrule
        Only source & \multirow{2}{*}{07} & 40.54 & 28.48 & 66.52 & 34.42 & 7.83 & 0.34 & 59.07 & 27.62 & 45.88 & 35.41 & 9.64 & 0.38 \\
        \net      & & \textbf{41.46} & \textbf{29.30} & \textbf{67.64} & \textbf{35.58} & \textbf{6.50} & \textbf{0.22} & \textbf{60.57} & \textbf{30.91} & \textbf{50.25} & \textbf{39.79} & \textbf{6.48} & \textbf{0.20} \\
        \midrule
        Only source & \multirow{2}{*}{09} & 50.59 & 37.26 & 74.06 & 47.38 & 6.03 & 0.36 & 50.78 & 36.57 & 72.22 & 46.75 & 5.60 & 0.35 \\
        \net      & & \textbf{52.29} & \textbf{38.02} & \textbf{74.88} & \textbf{48.21} & \textbf{4.74} & \textbf{0.19} & \textbf{51.53} & \textbf{37.56} & \textbf{72.87} & \textbf{47.99} & \textbf{4.56} & \textbf{0.16} \\
        \midrule
        Only source & \multirow{2}{*}{10} & 51.94 & 32.60 & 71.27 & 32.60 & 8.06 & 0.35 & 45.74 & 30.62 & 69.56 & 39.49 & 7.90 & 0.33 \\
        \net      & & \textbf{53.02} & \textbf{33.50} & \textbf{71.53} & \textbf{33.50} & \textbf{7.19} & \textbf{0.22} & \textbf{49.91} & \textbf{31.91} & \textbf{70.68} & \textbf{40.95} & \textbf{5.57} & \textbf{0.15} \\
        \bottomrule
    \end{tabular}
    \footnotesize
    Comparison between our \net and the performance of the same architecture without performing online continual learning on the respective sequence of the KITTI-360 dataset. Thus, ``only source'' refers to the model weights after pretraining on Cityscapes. The listed metrics are mean intersection over union (mIoU) for semantic segmentation; panoptic quality (PQ), segmentation quality (SQ), and recognition quality (RQ) for panoptic segmentation; root mean squared error (RMSE) and absolute relative error (Abs Rel) for monocular depth estimation. Bold values denote the best result on each sequence.
\end{threeparttable}
\vspace*{-.2cm}
\end{table*}

% \begin{table}[t]
% % \footnotesize
% \scriptsize
% \centering
% \caption{Adaptation Performance}
% \vspace{-0.2cm}
% \label{tab:baselines}
% \setlength\tabcolsep{3.5pt}
% \begin{threeparttable}
%     \begin{tabular}{l | c | c ccc cc}
%         \toprule
%         \textbf{Method} & \textbf{Sequence} & \textbf{mIoU} & \textbf{PQ} & \textbf{SQ} & \textbf{RQ} & \textbf{RMSE} & \textbf{Abs Rel} \\
%         \midrule
%         \multicolumn{8}{c}{\textbf{Protocol 1}} \\ 
%         \midrule
%         Only target & \multirow{3}{*}{\makecell{KITTI-360 \\ seq. 09}}
%                       & 68.60 & 51.00 & 82.00 & 59.04 & 3.49 & 0.11 \\
%         Only source & & 50.59 & 37.26 & 74.06 & 47.38 & 6.03 & 0.36 \\
%         \net & & 52.29 & 38.02 & 74.88 & 48.21 & 4.74 & 0.19 \\
%         \midrule
%         Only target & \multirow{3}{*}{\makecell{KITTI-360 \\ seq. 10}}
%                       & 64.65 & 41.91 & 76.68 & 51.48 & 6.13 & 0.15 \\
%         Only source & & 51.94 & 32.60 & 71.27 & 32.60 & 8.06 & 0.35 \\
%         \net & & 53.02 & 33.50 & 71.53 & 33.50 & 7.19 & 0.22 \\
%         \midrule
%         \midrule
%         \multicolumn{8}{c}{\textbf{Protocol 2}} \\ 
%         \midrule
%         Only target & \multirow{3}{*}{\makecell{KITTI-360 \\ seq. 09}}
%                       & 69.90 & 48.84 & 79.36 & 57.49 & 3.18 & 0.10 \\
%         Only source & & 50.78 & 36.57 & 72.22 & 46.75 & 5.60 & 0.35 \\
%         \net & & 51.53 & 37.56 & 72.87 & 47.99 & 4.56 & 0.16 \\
%         \midrule
%         Only target & \multirow{3}{*}{\makecell{KITTI-360 \\ seq. 10}}
%                       & 55.12 & 36.58 & 67.41 & 46.15 & 4.78 & 0.12 \\
%         Only source & & 45.74 & 30.62 & 69.56 & 39.49 & 7.90 & 0.33 \\
%         \net & & 49.91 & 31.91 & 70.68 & 40.95 & 5.57 & 0.15 \\
%         \bottomrule
%     \end{tabular}
%     \footnotesize
%     Comparison with baselines without adaptation evaluated on the \textit{adapt} part (protocol 1) and on the \textit{eval} part (protocol 2) of the specified sequences. ``Only target'' and ``only source'' refer to the domain used for training.
% \end{threeparttable}
% \end{table}

\begin{table*}[t]
% \footnotesize
\scriptsize
\centering
\caption{Continual Learning for Monocular Depth Estimation}
\vspace{-0.2cm}
\label{tab:depth-adaptation}
\setlength\tabcolsep{3.5pt}
\begin{threeparttable}
    \begin{tabular}{l | c | cc ccc | cc ccc | cc ccc}
        \toprule
        \multirow{2}{*}{\textbf{Method}} & \textbf{Batch} & \multicolumn{5}{c|}{\textbf{Protocol 1}} & \multicolumn{5}{c|}{\textbf{Protocol 2}} & \multicolumn{5}{c}{\textbf{Protocol 3}} \\
        & current/target/source & RMSE & Abs Rel & $\delta_1$ & $\delta_2$ & $\delta_3$ & RMSE & Abs Rel & $\delta_1$ & $\delta_2$ & $\delta_3$ & RMSE & Abs Rel & $\delta_1$ & $\delta_2$ & $\delta_3$ \\
        \midrule
        Only target & 0 / 0 / 0 & 6.13 & 0.15 & 0.84 & 0.93 & 0.96 & 4.78 & 0.12 & 0.88 & 0.95 & 0.97 & 12.22 & 0.26 & 0.51 & 0.82 & 0.94 \\
        Only source & 0 / 0 / 0 & 8.06 & 0.35 & 0.43 & 0.77 & 0.91 & 7.90 & 0.33 & 0.44 & 0.77 & 0.93 & \textbf{10.16} & \textbf{0.19} & \textbf{0.78} & \textbf{0.93} & \textbf{0.97} \\
        \midrule
        Online image & 1 / 0 / 0 & 8.33 & 0.27 & 0.64 & 0.84 & 0.93 & 6.06 & 0.33 & 0.46 & 0.73 & 0.90 & 13.72 & 0.57 & 0.30 & 0.50 & 0.68 \\
        Target replay & 1 / 2 / 0 & \textbf{6.35} & \textbf{0.19} & \textbf{0.77} & \textbf{0.91} & \textbf{0.96} & \textbf{5.34} & \textbf{0.15} & \textbf{0.81} & \textbf{0.93} & \textbf{0.97} & 12.48 & 0.44 & 0.34 & 0.68 & 0.88 \\ 
        \rowcolor{Gray}
        \net & 1 / 2 / 2 & \underline{7.19} & \underline{0.22} & \underline{0.73} & \underline{0.89} & \underline{0.94} & \underline{5.57} & \textbf{0.15} & \textbf{0.81} & \textbf{0.93} & \textbf{0.97} & \underline{11.38} & \underline{0.21} & \underline{0.75} & \underline{0.91} & \underline{0.96} \\ 
        \bottomrule
    \end{tabular}
    \footnotesize
    The root mean squared error (RMSE), absolute relative error (Abs Rel) as well as accuracies $\delta_1 = \delta < 1.25$, $\delta_2 = \delta < 1.25^2$, and $\delta_3 = \delta < 1.25^3$, obtained by adapting Cityscapes to sequence 10 of the KITTI-360 dataset. Best results without access to ground truth target data (``only target'') in each category are in \textbf{bold}; second best are \underline{underlined}.
\end{threeparttable}
\vspace*{-.3cm}
\end{table*}

\begin{table*}
% \footnotesize
\scriptsize
\centering
\caption{Continual Learning for Panoptic Segmentation}
\vspace{-0.2cm}
\label{tab:panoptic-adaptation}
\setlength\tabcolsep{3.7pt}
\begin{threeparttable}
    \begin{tabular}{l | c ccc | c ccc | c ccc}
        \toprule
        \multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c|}{\textbf{Protocol 1}} & \multicolumn{4}{c|}{\textbf{Protocol 2}} & \multicolumn{4}{c}{\textbf{Protocol 3}} \\
        & mIoU & PQ & SQ & RQ & mIoU & PQ & SQ & RQ & mIoU & PQ & SQ & RQ \\
        \midrule
        Only target & 64.65 & 41.91 & 76.68 & 51.48 & 55.12 & 36.58 & 67.41 & 46.15 & 46.33 & 28.24 & 70.03 & 36.92 \\
        Only source & 51.94 & 32.60 & 71.27 & 42.44 & 45.74 & 30.62 & 69.56 & 39.49 & \underline{72.87} & 49.19 & \underline{77.45} & 60.40 \\
        \midrule
        GUDA~\cite{guizilini2021geometric} & 45.56 & 29.70 & 70.67 & 39.05 & 47.62 & 31.03 & 64.00 & 40.49 & 66.57 & 44.39 & 75.95 & 55.32 \\        
        DACS~\cite{tranheden2021dacs} & 51.14 & 32.09 & 71.12 & 42.23 & 45.24 & 29.05 & 69.47 & 38.11 & 72.66 & 49.27 & 77.33 & 60.60\\        
        \midrule
        \net (online image) & \textbf{53.22} & \underline{33.46} & \textbf{71.63} & \underline{43.46} & \underline{49.51} & 31.49 & 64.17 & \underline{40.71} & 72.81 & \textbf{49.83} & 77.25 & \textbf{61.49} \\
        \net (random sampling) & 52.36 & 33.24 & \underline{71.60} & 43.25 & 48.78 & \underline{31.50} & \underline{68.83} & 40.56 & 72.05 & 49.11 & 77.18 & 60.52 \\
        \midrule
        \rowcolor{Gray}
        \net & \underline{53.02} & \textbf{33.50} & 71.53 & \textbf{43.62} & \textbf{49.91} & \textbf{31.91} & \textbf{70.68} & \textbf{40.95} & \textbf{72.90} & \underline{49.76} & \textbf{77.49} & \underline{61.22} \\
        \bottomrule
    \end{tabular}
    \footnotesize
    The mean intersection over union (mIoU), panoptic quality (PQ), semantic quality (SQ), and recognition quality (RQ) are obtained by adapting Cityscapes to sequence 10 of the KITTI-360 dataset. Best results without access to ground truth target data (``only target'') in each category are in \textbf{bold}; second best are \underline{underlined}.
\end{threeparttable}
\vspace*{-.3cm}
\end{table*}

\subsection{Online Adaptation}
\label{ssec:exp-online-adaptation}

In this section, we extensively evaluate our proposed \net with respect to both adapting to a new domain and retaining knowledge to mitigate forgetting. In detail, for all presented experiments, we freeze the shared encoder following the study by \citet{mccraith2020monocular}. Based on the ablation study in \refsec{ssec:exp-ablation-study-replay-buffer}, we use a buffer size of 300. For RCS, we follow \citet{hoyer2022daformer} and set $T=0.01$. Updating the EMA model is done with $\alpha = 0.99$.

In \reftab{tab:baselines}, we assess the performance of \net on all sequences of the KITTI-360 dataset and compare it with the baseline method ``only source'', which is also pretrained on Cityscapes but does not perform further adaptation to the target domain $\mathcal{T}$. This approach should be interpreted as a lower performance bound that must be improved.
We demonstrate the key performance metrics of both protocols~1 and 2.
As shown in \reftab{tab:baselines}, \net achieves a performance boost across the board, as measured by the mIoU metric and all depth metrics of protocol~1. We attribute this improvement to the additional supervision signals incorporated into the segmentation head through our mixing strategy and the self-supervised reconstruction loss for depth adaptation. The improvement in semantic segmentation further enhances the panoptic segmentation metrics.
With respect to protocol~2, \net reduces the depth errors on all sequences and improves the performance of semantic and panoptic segmentation on the vast majority of sequences. Note that on sequence~03 the panoptic metrics increase significantly despite the consistent mIoU, which we attribute to the more refined segmentation of objects due to our proposed cross-domain mixing strategy.

For the following experiments, we consider the case of using Cityscapes as the source domain and sequence~10 of \mbox{KITTI-360} as the target domain. In \reffig{fig:qualitative-results}, we illustrate the adaptation progress using unseen validation samples and compare the results to the ground truth. For depth, we visualize predictions generated by the network if it was only trained on $\mathcal{S}$ and $\mathcal{T}$, respectively.
For panoptic segmentation, the progressive adaptation on the target domain is particularly visible on the \textit{sidewalk} and \textit{terrain} image regions, which \net learns to differentiate from the similarly looking classes \textit{road} and \textit{vegetation}. Furthermore, instances become more pronounced, e.g., the cyclist in the right sample. Despite the enhancements on the target domain, \net successfully maintains its performance on the source domain with only minimum decreases in depth estimation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\parskip=5pt
\noindent\textit{Depth Adaptation:}
We present the results for monocular depth estimation in \reftab{tab:depth-adaptation}. The first row ``only target'' shows the theoretical performance on $\mathcal{T}$ (protocols~1 and 2) if the network would have been trained directly on this domain. Note that such a setup is infeasible in the real world when continuous operation must be guaranteed. The second row ``only source'' denotes the performance after pretraining on~$\mathcal{S}$ without performing online continual learning. Comparing the absolute relative error as well as the accuracies $\delta_1$, $\delta_2$, and~$\delta_3$ between these rows reveals the domain gap. Note that the opposite gap can be observed when evaluating on~$\mathcal{S}$ (protocol~3). While continual learning using the current online sample increases the accuracy of protocol~1, it also overfits to the current scene. That is, generalizability to the entire target domain is not achieved as shown by protocol~2. Introducing replay samples from the target buffer overcomes this issue and accounts for online samples of poor quality, improving protocols~1 and 2. However, both of the above result in catastrophic forgetting with respect to $\mathcal{S}$ (protocol~3). The final \net adds additional source replay yielding low errors and high accuracy by compromising on both $\mathcal{S}$ and $\mathcal{T}$.
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/results.pdf}
    \vspace*{-.5cm}
    \caption{Qualitative results for Cityscapes to KITTI-360 adaptation after pretraining on the source, i.e., 0 steps, and after having seen 1,000 and 2,500 frames. As shown in the left column, \net is able to avoid catastrophic forgetting on the source domain. The progressive adaptation on the target domain is particularly visible in the image areas highlighted by yellow boxes. ``Stuff'' classes of similar appearance like \textit{sidewalk} vs. \textit{road} (left image) and \textit{terrain} vs. \textit{vegetation} (right image) can be better distinguished by \net. Furthermore, instances become more pronounced as can be observed for the highlighted car (left image) and the cyclist (right image).
    }
    \label{fig:qualitative-results}
    % \vspace*{-.5cm}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\parskip=5pt
\noindent\textit{Panoptic Adaptation:}
In \reftab{tab:panoptic-adaptation}, we also demonstrate the domain gap between $\mathcal{S}$ and $\mathcal{T}$ for semantic and panoptic segmentation. Similar to depth estimation, both ``only target'' and ``only source'' only perform well on their respective training domain without being able to generalize to the other.
We further evaluate \net by comparing it with two competitive baselines that perform domain adaptation on segmentation tasks: GUDA~\cite{guizilini2021geometric}, which combines semantic segmentation and depth estimation, rendering their task comparable to ours, and DACS~\cite{tranheden2021dacs}, which employs a class-mix strategy for offline domain adaptation of semantic segmentation. To ensure a fair comparison, both baselines are evaluated using the same settings as \net, including diversity sampling-based experience replay. The results in \reftab{tab:panoptic-adaptation} indicate that both approaches lead to a significant performance decrease across all three protocols. GUDA's reliance on self-supervised feature alignment using depth training is not effective in the continual learning setting, as shown in the results. DACS also suffers from a decline in performance, likely due to the strong intervention of its mixing strategy into the pretrained network, which can already produce reasonable predictions on the target domain without adaptation. 

These results imply that traditional approaches from offline sim-to-real adaptation may not perform well in the online continual learning scenario.
To further assess the impact of target replay and our diversity-based buffer sampling, we selectively deactivate both components. Applying the proposed cross-domain mixing strategy results in an improvement in protocol~1. However, similar to depth adaptation, the results are not fully generalizable to the entire target domain, e.g., SQ of protocol~2. Instead of diversity-based sampling, we use random sampling when both creating the source buffer and when updating the target buffer. Compared to \net, the performance heavily degrades demonstrating the efficacy of the sampling method.
Finally, we present the classwise evaluation of the segmentation performance in \reftab{tab:classwise-evaluation}, which demonstrates improvements of \net in the IoU metrics for most classes. In particular, we observe significant enhancements of the \textit{two-wheeler} and \textit{terrain} classes. The latter can also be observed in \reffig{fig:qualitative-results}. In fact, \net outperforms even the model trained directly on the target domain using ground truth supervision for the latter class.
}



\begin{table}
\scriptsize
\centering
\caption{Classwise evaluation}
\vspace{-0.2cm}
\label{tab:classwise-evaluation}
% \setlength\tabcolsep{3.7pt}
\begin{threeparttable}
    \begin{tabular}{c | l | c c c}
        \toprule
        \multicolumn{2}{c|}{\textbf{Class}} & \textbf{Only target} & \textbf{Only source} & \textbf{\net} \\
        \midrule
        \multirow{9}{*}{\begin{sideways} Stuff \end{sideways}}
        & Road          & 93 & 89 & 91 \\
        & Sidewalk      & 40 & 32 & 37 \\
        & Building      & 88 & 85 & 85 \\
        & Fence         & 43 & 14 & 22 \\
        & Pole          & 35 & 29 & 32 \\
        & Traffic sign  & 40 & 35 & 38 \\
        & Vegetation    & 78 & 73 & 75 \\
        & Terrain       & 54 & 21 & 39 \\
        & Sky           & 82 & 79 & 81 \\
        \midrule
        \multirow{5}{*}{\begin{sideways} Thing \end{sideways}}
        & Person        & 47 & 38 & 38 \\
        & Rider         & 47 & 29 & 36 \\
        & Car           & 91 & 83 & 84 \\
        & Truck         &  1 &  4 &  2 \\
        & Two-wheeler   & 33 & 27 & 38 \\
        \midrule
        \multicolumn{2}{c|}{Mean} & 55.1 & 45.7 & 49.9 \\
        \bottomrule
    \end{tabular}
    \footnotesize
    The classwise mIoU is based on protocol 2 in \reftab{tab:panoptic-adaptation}. We compare \net against two baselines that were trained using source (``only source'') or target data (``only target''), respectively. \net provides a significant performance boost of 4.2\% in terms of the mIoU metric.
\end{threeparttable}
\vspace*{-.3cm}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Ablation studies

\subsection{Ablation Study of the Replay Buffer}
\label{ssec:exp-ablation-study-replay-buffer}

We extensively study different sizes of the replay buffer and the effect of diversity sampling as explained in \refsec{ssec:ta-online-adaptation}. We list our results in \reftab{tab:ablation-buffer-size}. Note that an infinite replay buffer contains 2,975 source and a maximum of 2,683 target samples in the employed setting, i.e., adapting from Cityscapes \textit{train} to KITTI-360 using sequence 10 \textit{adapt}. Generally, a larger replay buffer yields higher performance with respect to both adaptation capability and avoiding catastrophic forgetting. Additionally, the proposed diversity sampling using semantic classes for the source and image features for the target samples increases the performance throughout the experiments. However, a greater buffer size increases the required storage posing a challenge for real-world deployment. Based on the presented results, we select a buffer size of 300 with active diversity sampling as for smaller sizes the performance of semantic segmentation on the target domain degrades.

\begin{table*}
% \footnotesize
\scriptsize
\centering
\caption{Ablation Study on the Replay Buffer}
\vspace{-0.2cm}
\label{tab:ablation-buffer-size}
\setlength\tabcolsep{2.7pt}
\begin{threeparttable}
    \begin{tabular}{c | c | c ccc cc | c ccc cc}
        \toprule
        \multirowcell{2}{\textbf{Size}} & \multirowcell{2}{\textbf{Div.}} & \multicolumn{6}{c|}{\textbf{Protocol 2}} & \multicolumn{6}{c}{\textbf{Protocol 3}}  \\
        % & & mIoU & PQ & SQ & RQ & RMSE & Abs Rel & mIoU & PQ & SQ & RQ & RMSE & Abs Rel \\
        & & mIoU $\uparrow$ & PQ $\uparrow$ & SQ $\uparrow$ & RQ $\uparrow$ & RMSE $\downarrow$ & Abs Rel $\downarrow$ & mIoU $\uparrow$ & PQ $\uparrow$ & SQ $\uparrow$ & RQ $\uparrow$ & RMSE $\downarrow$ & Abs Rel $\downarrow$ \\
        \midrule
        $\infty$ &    & 49.15 & 31.95 & 69.08 & 40.96 & 4.94 & 0.15 & 73.25 & 50.37 & 77.77 & 61.87 & 10.76 & 0.21 \\
        \midrule
        1000 &        & 49.11$\pm$0.69 & \underline{31.85}$\pm$0.25 & 66.82$\pm$3.06 & \underline{40.93}$\pm$0.06 & \textbf{5.04}$\pm$0.01 & \textbf{0.14}$\pm$0.00 & 72.84$\pm$0.33 & \underline{49.93}$\pm$0.20 & \underline{77.51}$\pm$0.05 & \underline{61.39}$\pm$0.28 & 11.35$\pm$0.39 & \underline{0.22}$\pm$0.01 \\
        1000 & \cmark & 49.36 & 31.83 & 68.89 & \textbf{41.01} & 5.30 & \underline{0.15} & \textbf{73.50} & \textbf{50.05} & \textbf{77.67} & \textbf{61.48} & 12.06 & 0.23 \\ 
        500 &         & 48.77$\pm$0.39 & 31.54$\pm$0.39 & 67.39$\pm$2.16 & 40.66$\pm$0.54 & \underline{5.20}$\pm$0.20 & \underline{0.15}$\pm$0.00 & 72.38$\pm$0.26 & 49.48$\pm$0.14 & 77.45$\pm$0.16 & 60.90$\pm$0.23 & \textbf{11.14}$\pm$0.54 & \underline{0.22}$\pm$0.01 \\
        500 & \cmark  & \underline{49.56} & 31.83 & 70.11 & 40.96 & 5.55 & 0.16 & 72.78 & 49.68 & 77.39 & 61.10 & \underline{11.30} & \underline{0.22} \\
        300 &         & 48.78$\pm$0.05 & 31.50$\pm$0.19 & \underline{68.83}$\pm$2.31 & 40.56$\pm$0.15 & 5.27$\pm$0.16 & \underline{0.15}$\pm$0.00 & 72.05$\pm$0.25 & 49.11$\pm$0.30 & 77.18$\pm$0.07 & 60.52$\pm$0.35 & \textbf{11.14}$\pm$0.22 & \underline{0.22}$\pm$0.01 \\
        \rowcolor{Gray}
        300 & \cmark  & \textbf{49.91} & \textbf{31.91} & \textbf{70.68} & 40.95 & 5.57 & \underline{0.15} & \underline{72.90} & 49.76 & 77.49 & 61.22 & 11.38 & \textbf{0.21} \\
        100 &         & 48.27$\pm$0.84 & 30.71$\pm$0.41 & 63.95$\pm$0.41 & 39.79$\pm$0.38 & 5.83$\pm$0.15 & 0.16$\pm$0.00 & 69.75$\pm$1.77 & 47.94$\pm$0.95 & 76.66$\pm$0.25 & 59.39$\pm$1.16 & 10.86$\pm$0.56 & \underline{0.22}$\pm$0.02 \\
        100 & \cmark  & 48.40 & 30.85 & 64.07 & 39.95 & 5.31 & 0.16 & 72.35 & 48.81 & 77.16 & 60.25 & 11.71 & \underline{0.22} \\
        25 &          & 46.03$\pm$1.03 & 29.62$\pm$0.37 & 66.10$\pm$2.26 & 38.48$\pm$0.45 & 5.25$\pm$0.26 & \textbf{0.14}$\pm$0.01 & 67.23$\pm$0.85 & 45.90$\pm$0.66 & 75.69$\pm$0.38 & 57.21$\pm$0.76 & 11.81$\pm$0.22 & \underline{0.22}$\pm$0.01 \\
        25 & \cmark   & 46.35 & 29.73 & 63.35 & 38.58 & 5.62 & 0.17 & 68.84 & 46.34 & 76.06 & 57.78 & 12.51 & 0.24 \\
        \bottomrule
    \end{tabular}
    \footnotesize
    The numbers above are obtained by adapting Cityscapes to sequence 10 of the KITTI-360 dataset. Here, an infinite buffer size equals 2,975 source samples and a maximum of 2,683 target samples.
    Note that the effective size is two times the shown value as it refers to both source and target replay. The term ``Div.'' refers to diversity sampling. Where diversity sampling is not used, the same experiment is repeated three times with different random seeds to ensure a statistically reliable measure of performance. The results of these experiments are presented as the mean and standard deviation. Best results in each category are in \textbf{bold}; second best are \underline{underlined}.
\end{threeparttable}
\vspace*{-.3cm}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Continual learning

\begin{table*}
% \footnotesize
\scriptsize
\centering
\caption{Continual Learning on Multiple Domains}
\vspace{-0.2cm}
\label{tab:continual-learning}
\setlength\tabcolsep{2.7pt}
\begin{threeparttable}
    \begin{tabular}{l | c ccc cc | c ccc cc | c ccc cc}
        \toprule
        \textbf{Domain} & \textbf{mIoU} & \textbf{PQ} & \textbf{SQ} & \textbf{RQ} & \textbf{RMSE} & \textbf{Abs Rel} & \textbf{mIoU} & \textbf{PQ} & \textbf{SQ} & \textbf{RQ} & \textbf{RMSE} & \textbf{Abs Rel} & \textbf{mIoU} & \textbf{PQ} & \textbf{SQ} & \textbf{RQ} & \textbf{RMSE} & \textbf{Abs Rel} \\
        \midrule
        & \multicolumn{5}{r}{$\xrightarrow{\hspace*{.5cm}}$ Pretraining on Cityscapes} & \multicolumn{13}{l}{$\xrightarrow{\hspace*{1.8cm}}$ Adaptation on KITTI-360 $\xrightarrow{\hspace*{1.6cm}}$ Adaptation on SemKITTI-DVPS $\xrightarrow{\hspace*{.4cm}}$} \\
        \\[-1.5ex]
        Cityscapes & 72.87 & 49.19 & 77.45 & 60.40 & 10.16 & 0.19 & 72.90 & 49.76 & 77.49 & 61.22 & 11.38 & 0.21 & 72.42 & 48.74 & 77.08 & 60.20 & 10.65 & 0.21 \\
        KITTI-360 seq. 10 & 45.74 & 30.62 & 69.56 & 39.49 & 7.90 & 0.33 & 49.91 & 31.91 & 70.68 & 40.95 & 5.57 & 0.15 & 49.26 & 32.32 & 64.08 & 40.95 & 5.23 & 0.15 \\
        SemKITTI-DVPS seq. 08 & 51.95 & 45.24 & 76.07 & 57.20 & 6.17 & 0.34  & 49.48 & 43.26 & 74.24 & 57.26 & 5.60 & 0.21 & 53.70 & 46.50 & 76.53 & 59.43 & 4.32 & 0.16 \\
        \arrayrulecolor{gray}
        \midrule
        \arrayrulecolor{black}
        & \multicolumn{5}{r}{$\xrightarrow{\hspace*{.5cm}}$ Pretraining on Cityscapes} & \multicolumn{13}{l}{$\xrightarrow{\hspace*{1.5cm}}$ Adaptation on SemKITTI-DVPS $\xrightarrow{\hspace*{1.5cm}}$ Adaptation on KITTI-360 $\xrightarrow{\hspace*{.8cm}}$} \\
        \\[-1.5ex]
        Cityscapes & 72.87 & 49.19 & 77.45 & 60.40 & 10.16 & 0.19 & 72.75 & 49.01 & 77.36 & 60.35 & 10.82 & 0.22 & 72.51 & 48.87 & 76.98 & 60.28 & 11.41 & 0.21 \\
        KITTI-360 seq. 10 & 45.74 & 30.62 & 69.56 & 39.49 & 7.90 & 0.33 & 49.26 & 31.66 & 70.26 & 41.40 & 6.30 & 0.17 & 50.05 & 31.92 & 70.50 & 41.48 & 5.47 & 0.16 \\
        SemKITTI-DVPS seq. 08 & 51.95 & 45.24 & 76.07 & 57.20 & 6.17 & 0.34 & 52.31 & 44.29 & 75.58 & 56.87 & 4.56 & 0.16 & 53.83 & 47.29 & 76.55 & 60.01 & 4.25 & 0.16 \\
        \bottomrule
    \end{tabular}
    \footnotesize
    \net is continually applied to three domains using Cityscapes as the initial source domain and then adapting to KITTI-360 and SemKITTI-DVPS. The listed numbers on the target domains are based on protocol 2.
\end{threeparttable}
\vspace*{-.3cm}
\end{table*}

\subsection{Continual Adaptation}

Finally, we evaluate the performance of \net in the context of multi-domain adaptation, i.e., $\mathcal{S} \to \mathcal{T}_1 \to \mathcal{T}_2$. In particular, we first adapt to sequence 10 of KITTI-360 followed by sequence 08 of SemKITTI-DVPS, then we invert the adaptation order. To analyze forward and backward transfer as defined for continual learning~\cite{lopez2017gradient}, we compute the metrics on the \textit{val} split of the source and the \textit{adapt} parts of the respective target domains. We report the results in \reftab{tab:continual-learning}. Note that we use $\alpha_{\mathcal{S} \shortto \mathcal{T}_1} = 0.9$ and $\alpha_{\mathcal{T}_1 \shortto \mathcal{T}_2} = 0.7$ for updating the EMA model according to \refeqn{eqn:ema} since the network should adapt more strongly when deployed to $\mathcal{T}_2$ due to the larger amount of previously seen data. As shown in the first row of both adaptation orders, \net is able to mitigate catastrophic forgetting with respect to $\mathcal{S}$ maintaining its performance.
We make a similar observation when re-evaluating $\mathcal{T}_1$ after the second adaptation step to $\mathcal{T}_2$. In particular, \net achieves positive backward transfer on SemKITTI-DVPS when adapting to KITTI-360.
On the same adaptation order, we observe positive forward transfer for KITTI-360, i.e., the performance increases although \net was only adapted to SemKITTI-DVPS.

In \reffig{fig:continual-adaptation}, we illustrate the evolution of the performance metrics on SemKITTI-DVPS sequence 08 during adaptation (protocol 1). We compare the error without adaptation to directly adapting to SemKITTI-DVPS versus first adapting to KITTI-360. For both semantic segmentation and depth estimation, it can be clearly observed that the performance improves if more images have been seen. Additionally, adapting first to KITTI-360 results in a large performance increase for both semantic and panoptic segmentation. We account this to the fact that KITTI-360 sequence 10 leads to strongly improved performance, shown in \reftab{tab:continual-learning}, that can be transferred to the SemKITTI-DVPS domain.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/continual_adaptation.pdf}
    \vspace{-.65cm}
    \caption{Evolution of performance metrics on SemKITTI-DVPS sequence~08 during adaptation (protocol 1). The metrics are averaged until the given frame number. The target domains $\mathcal{T}_1$ and $\mathcal{T}_2$ refer to \mbox{SemKITTI-DVPS} and \mbox{KITTI-360}, respectively. It can be seen that there is positive forward transfer when first adapting on $\mathcal{T}_2$.}
    \label{fig:continual-adaptation}
    \vspace*{-.5cm}
\end{figure}





