\section{Technical Approach}

The setting investigated in this work consists of two steps. First, we train a neural network on the source domain~$\mathcal{S}$ partly using ground truth supervision. Second, to close the gap between domains, we continuously adapt the network during inference time on the target domain~$\mathcal{T}$ using a replay buffer and unsupervised training strategies.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/overview.pdf}
    \vspace*{-.5cm}
    \caption{
    Overview of our proposed \net. Unlabeled RGB images from an online camera stream are combined with samples from a replay buffer comprising both annotated source samples and previously seen target images. Cross-domain mixing enables pseudo-supervision on the target domain. The network weights are then updated via backpropagation using the constructed data batch. The additional PoseNet required for unsupervised monocular depth estimation is omitted in this visualization.}
    \label{fig:overview}
    \vspace*{-.5cm}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Network Architecture and Source Domain Pretraining}
\label{ssec:ta-network-pretraining}

In this section, we detail the network architecture and loss functions that we employ during the pretraining stage on the source domain.

{\parskip=5pt
\noindent\textit{Architecture:}
We build our network following a common multi-task design scheme, i.e., using a single backbone followed by task-specific heads. A high-level overview of the network architecture is shown in \reffig{fig:overview}. In detail, we use a ResNet-101~\cite{he2016deep} as the shared encoder for all three tasks including depth prediction, semantic segmentation, and instance segmentation. The depth head follows the design of Monodepth2~\cite{godard2019digging} comprising five consecutive convolutional layers with skip connections to the encoder. Additionally, we include a separate PoseNet consisting of a ResNet-18 encoder and a four-layer CNN to estimate the camera motion between two image frames. For panoptic segmentation, we follow the bottom-up method Panoptic-Deeplab~\cite{cheng2020panoptic}, leveraging separate heads for semantic segmentation and instance segmentation, and slightly modify the semantic head~\cite{guizilini2021geometric}. Specifically, the instance head consists of two sub-heads to predict the center of an object and to associate each pixel of an image to the corresponding object or the background. Combining both the semantic and instance predictions, a panoptic fusion module generates the overall panoptic segmentation map.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\parskip=5pt
\noindent\textit{Source Domain Pretraining:}
During the initial training phase on the source domain, we assume to have access to image sequences as well as ground truth panoptic segmentation annotations. In the following, we briefly describe the respective loss functions that we employ for training the three task-specific heads.
}

We train the depth estimation head using the common methodology of unsupervised training based on the photometric error~\cite{godard2019digging}. In particular, we leverage an image triplet $\{\mathbf{I_{t_0}}, \mathbf{I_{t_1}}, \mathbf{I_{t_2}}\}$ to predict depth $\mathbf{D_{t_1}}$ and camera motion $\mathbf{M_{t_0 \shortto t_1}}$ and  $\mathbf{M_{t_1 \shortto t_2}}$. Afterwards, we compute the photometric error loss $\mathcal{L}^d_\mathit{pe}$ as a weighted sum of the reprojection loss~$\mathcal{L}^d_\mathit{pr}$ and the image smoothness loss~$\mathcal{L}^d_\mathit{sm}$:
\begin{equation}
    \mathcal{L}^d_\mathit{pe} = \lambda_\mathit{pr} \mathcal{L}^d_\mathit{pr} + \lambda_\mathit{sm} \mathcal{L}^d_\mathit{sm}.
    \label{eqn:loss-photometric}
\end{equation}

We train the semantic segmentation head in a supervised manner using the bootstrapped cross-entropy loss with hard pixel mining~$\mathcal{L}^\mathit{sem}_\mathit{bce}$ following Panoptic-Deeplab~\cite{cheng2020panoptic}.

For training the instance segmentation head, we adopt the MSE loss~$\mathcal{L}^\mathit{ins}_\mathit{center}$ for the center head and the L1 loss~$\mathcal{L}^\mathit{ins}_\mathit{offset}$ for the offset head. The total loss to supervise instance segmentation is then computed as a weighted sum:
\begin{equation}
    \mathcal{L}^\mathit{ins}_\mathit{co} = \lambda_\mathit{center} \mathcal{L}^\mathit{ins}_\mathit{center} + \lambda_\mathit{offset} \mathcal{L}^\mathit{ins}_\mathit{offset}.
    \label{eqn:supervised-instance}
\end{equation}
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Online Adaptation}
\label{ssec:ta-online-adaptation}

After the described network has been trained on the source domain $\mathcal{S}$ using the aforementioned losses, we aim to adapt it to the target domain $\mathcal{T}$ in a continuous manner. That is, unlike other works, data from the target domain is revealed frame by frame resembling the online stream of an onboard camera.
As depicted in \reffig{fig:overview}, every adaptation iteration consists of the following steps:
\begin{enumerate}
    \item Construct an update batch by combining online and replay data.
    \item Generate pseudo-labels using the proposed cross-domain mixing strategy.
    \item Perform backpropagation to update the network weights.
    \item Update the replay buffer.
\end{enumerate}
In this section, we first detail the structure of the utilized replay buffer and then propose adaptation schemes for both depth estimation and panoptic segmentation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\parskip=5pt
\noindent\textit{Replay Buffer and Batch Generation:}
Upon receiving a new image taken by the robot's onboard camera, we construct a batch that is used to perform backpropagation on the network weights. In detail, a batch $\mathbf{b_t}$ consists of the current online image $\mathbf{I_t} \in \mathcal{T}$, previously received target images $\mathbf{I_{\mathcal{T}_i}} \in \mathbf{B_\mathcal{T}}$, and fully annotated source samples $\mathbf{I_{\mathcal{S}_i}} \in \mathbf{B_\mathcal{S}}$. Here, $\mathbf{B_\mathcal{T}} \subseteq \mathcal{T}$ and $\mathbf{B_\mathcal{S}} \subseteq \mathcal{S}$ denote the respective replay buffers.
Formally\footnote{To improve readability, we omit in the notation that each image sample includes its two previous frames enabling unsupervised depth estimation.},
\begin{equation}
    \mathbf{b_t} = \{\mathbf{I_{t}}, \mathbf{I_{\mathcal{T}_0}}, \mathbf{I_{\mathcal{T}_1}}, \dots, \mathbf{I_{\mathcal{S}_0}}, \mathbf{I_{\mathcal{S}_1}}, \dots\}.
\end{equation}

By revisiting target images from the past, we increase the diversity in the loss signal on the target domain and hence mitigate overfitting to the current scene. This further accounts for situations in which the current online image suffers from visual artifacts, e.g., overexposure. Similarly, revisiting samples from the source domain addresses the problem of catastrophic forgetting by ensuring that previously acquired knowledge can be preserved. Additionally, the annotations of the source samples enable pseudo-supervision on the target domain by exploiting cross-domain mixing strategies.
For both the target and the source replay, we randomly draw multiple samples from the respective replay buffer and apply augmentation to stabilize the loss. In particular, we perform RGB histogram matching of the source images to the online target image, and all available source samples have to be selected once before repetition is allowed to ensure diverse source supervision.

While previous works~\cite{kuznietsov2021comoda, voedisch2023continual} do not consider limitations on the size of the replay buffer, we explicitly address this challenge to closely resemble the deployment on a robotic platform, where disk storage is an important factor. This poses two questions: First, how to sample from $\mathcal{S}$ to construct the fixed source buffer $\mathbf{B_\mathcal{S}}$ and, second, how to update the dynamic target buffer $\mathbf{B_\mathcal{T}}$? To construct $\mathbf{B_\mathcal{S}}$, we propose a refined version of rare class sampling (RCS)~\cite{hoyer2022daformer}. The frequency $f_c$ of each class $c \in \mathcal{C}$ is calculated based on the number of pixels with class $c$:
\begin{equation}
    f_c = \frac{\sum_{\mathbf{I} \in \mathcal{S}} \sum_p^{H \times W} \mathbb{1}_c(p_{c'})}{|\mathcal{S}| \cdot H \cdot W},
\end{equation}
where $H$ and $W$ denote the height and width of the images in $\mathcal{S}$ and $p_{c'} \in \mathbf{I}$ refers to a pixel with class $c'$. The indicator function is 1 if $c'$ equals $c$ and 0 otherwise. The probability of sampling a class is then given by
\begin{equation}
    P(c) = \frac{e^{ (1 - f_c) / T }}{ \sum_{c' \in \mathcal{C}} e^{ (1 - f_{c'}) / T } },
\end{equation}
with temperature $T$ controlling the smoothness of the distribution, i.e., a smaller $T$ assigns a higher probability to rare classes. In detail, we first sample a class $c \sim P$ and then retrieve all candidate images containing pixels with class~$c$. Instead of taking a random image from these candidates, we sample according to the number of pixels with class~$c$. We repeat both steps $|\mathbf{B_\mathcal{S}}|$ times without selecting the same image more than once. Using RCS ensures that $\mathbf{B_\mathcal{S}}$ contains sufficiently many images with rare classes such that the performance on these classes will further improve during adaptation.

Since $\mathcal{T}$ does not contain annotations and, particularly in the beginning, predictions are not reliable, we cannot use RCS for updating $\mathbf{B_\mathcal{T}}$. Instead, we invert the common methodology of loop closure detection for visual SLAM~\cite{voedisch2023continual}, i.e., the image~$\mathbf{I_t}$ is only added to $\mathbf{B_\mathcal{T}}$ if its cosine similarity with respect to all samples within the buffer is below a threshold.
\begin{equation}
    \text{sim}_{\cos}(\mathbf{I_t}) = \max_{\mathbf{I_{\mathcal{T}_i}} \in \mathbf{B_\mathcal{T}}} \cos \left( \text{feat}(\mathbf{I_t}), \text{feat}(\mathbf{I_{\mathcal{T}_i}}) \right),
\end{equation}
where $\text{feat}(\cdot)$ refers to the image features extracted from the final layer of the shared encoder, which is not adapted. If $\mathbf{B_\mathcal{T}}$ is completely filled, we remove the following image to maximize image diversity:
\begin{equation}
    \argmax_{\mathbf{I_{\mathcal{T}_i}} \in \mathbf{B_\mathcal{T}}} \sum_{\mathbf{I_{\mathcal{T}_j}} \in \mathbf{B_\mathcal{T}}} \cos \left( \text{feat}(\mathbf{I_{\mathcal{T}_i}}), \text{feat}(\mathbf{I_{\mathcal{T}_j}}) \right)
\end{equation}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\parskip=5pt
\noindent\textit{Depth Adaptation:}
To adapt the monocular depth estimation head, we exploit the fact that the photometric error loss (\refeqn{eqn:loss-photometric}) does not require ground truth annotations. Hence, we can directly transfer it to the implemented continual adaptation. In particular, we compute $\mathcal{L}^d_\mathit{pe}$ for the constructed batch~$\mathbf{b_t}$ and average the loss such that each sample contributes by the same amount:
\begin{equation}
    \mathcal{L}^d_\mathit{pe}(\mathbf{b_t}) = \frac{\mathcal{L}^d_\mathit{pe}(\mathbf{I_t}) + \sum_{i} \mathcal{L}^d_\mathit{pe}(\mathbf{I_{\mathcal{T}_i}}) + \sum_{j} \mathcal{L}^d_\mathit{pe}(\mathbf{I_{\mathcal{S}_j}}) }{ |\mathbf{b_t}| }.
\end{equation}

Furthermore, if the predicted camera motion is below a threshold, i.e., the robot is presumably not moving, we do not compute the $\mathcal{L}^d_\mathit{pe}(\mathbf{I_t})$ and subtract 1 from the denominator to avoid adding a bias to the current scene.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/mixup.pdf}
    \vspace{-.6cm}
    \caption{Our proposed cross-domain mixing strategy first transfers the image style from the target to the source sample. Then it augments the target image to match the appearance of the source camera. Finally, a random image patch is copied from the target to the source image. The source annotations are retained and completed by the network's estimate on the copied image patch, combining self-iterative learning with ground truth supervision.}
    \label{fig:panoptic-mixup}
    \vspace*{-.3cm}
\end{figure}

{\parskip=5pt
\noindent\textit{Panoptic Adaptation:}
As described in \refsec{ssec:ta-network-pretraining}, panoptic segmentation is the fused output of a semantic head and an instance head. We observe that the decrease in performance on samples from unseen domains can mostly be attributed to the semantic head, while instance predictions remain stable.
Cross-domain mixing strategies allow leveraging ideas from supervised training to an unsupervised setting, where ground truth annotations are unknown. In \net, we bootstrap annotated source samples and high-confident target predictions to artificially generate pseudo-labels for the target samples in an online fashion to supervise the semantic head. Similar to depth adaptation, we continue to compute $\mathcal{L}^\mathit{sem}_\mathit{bce}$ on $\{\mathbf{I_{\mathcal{S}_0}}, \mathbf{I_{\mathcal{S}_1}}, \dots\}$ to mitigate forgetting, and freeze the instance head.}

We further design a mixing strategy combining pixels of images from both $\mathcal{S}$ and $\mathcal{T}$, that considers multiple factors, which are unique to the online continual learning scenario: (1)~the robust pretraining on a dedicated source dataset, which may result in significant performance degradation on the target dataset if the pre-trained weights are strongly adapted; (2)~the existence of different cameras leading to significant changes in the field-of-view, geometric appearance of objects, resolution, and aspect ratio of the images; and (3)~the continuously evolving visual appearance of street scenes during adaptation.
To address these challenges, our cross-domain mixing approach employs a three-step method to generate the adaptation signal. First, we perform style transfer from the target image~$\mathbf{I_{\mathcal{T}_i}}$ to the source image~$\mathbf{I_{\mathcal{S}_j}}$ by aligning their pixel value histograms, as depicted in \reffig{fig:panoptic-mixup}. This allows supervision with ground truth labels on images that are of similar visual appearance as the target image. Second, we apply a geometric transformation on $\mathbf{I_{\mathcal{T}_i}}$ based on the camera intrinsics of the source and target domains denoted by $\mathbf{K_\mathcal{S}}$ and $\mathbf{K_\mathcal{T}}$, respectively. To this end, we assume a constant depth distribution over $\mathbf{I_{\mathcal{S}_j}}$, lift the pixel values into Euclidean space via inverse camera projection, and project the lifted points back into the camera view of $\mathbf{I_{\mathcal{T}_i}}$ as follows:
\begin{equation}
        \mathbf{I'_{\mathcal{T}}} = \mathbf{I_{\mathcal{T}}}
        \langle \mathbf{K_{\mathcal{T}}} \mathbf{K_{\mathcal{S}}^{-1}} \mathbf{I_\mathcal{S}} \rangle, 
    \label{eqn:geom-augm}
\end{equation}
where $\langle\cdot\rangle$ denotes the bilinear sampling operator.
\refeqn{eqn:geom-augm} results in an adapted target image $\mathbf{I'_\mathcal{T}}$ with an adjusted field of view, resolution, and a geometric appearance of the scene similar to that of $\mathbf{I_\mathcal{S}}$.
The final step in the process involves separating $\mathbf{I'_\mathcal{T}}$  into multiple segments and randomly selecting one of them to be inserted into the style-transferred source image, see \reffig{fig:panoptic-mixup}. To avoid providing a flawed supervision signal caused by geometrically unrealistic images, we only insert a single path. 
Similarly, the ground truth labels of the pixels from $\mathbf{I_\mathcal{S}}$ are retained, and the semantic labels estimated by the network are used to label the inserted patch after intrinsics transformation.
The generated image is then fed into the network and training is performed using the cross-entropy loss and the generated pseudo-labels of the mixed image.
To mitigate the decline in performance commonly associated with self-iterative training on predicted pseudo-labels, often resulting in class collapse, we utilize an exponentially moving average~(EMA) filter for updating the network weights.
In detail, we create a duplicate with network weights $w_\text{EMA}$ of the initial model with weights $w$ and use this so-called EMA model to generate the semantic predictions.
During continual learning, the weights $w$ are updated via backpropagation on $\mathbf{b_t}$. Then, the EMA model is updated as follows:
\begin{equation}
    w_\text{EMA} \leftarrow \alpha \cdot w_\text{EMA} + (1 - \alpha) \cdot w,
    \label{eqn:ema}
\end{equation}
where $\alpha$ denotes the contribution of the EMA model.
