@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

% default given in cvpr template
% @String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
% @String(IJCV = {Int. J. Comput. Vis.})
% @String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
% @String(ICCV= {Int. Conf. Comput. Vis.})
% @String(ECCV= {Eur. Conf. Comput. Vis.})
% @String(NIPS= {Adv. Neural Inform. Process. Syst.})
% @String(ICPR = {Int. Conf. Pattern Recog.})
% @String(BMVC= {Brit. Mach. Vis. Conf.})
% @String(TOG= {ACM Trans. Graph.})
% @String(TIP  = {IEEE Trans. Image Process.})
% @String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
% @String(TMM  = {IEEE Trans. Multimedia})
% @String(ACMMM= {ACM Int. Conf. Multimedia})
% @String(ICME = {Int. Conf. Multimedia and Expo})
% @String(ICASSP=	{ICASSP})
% @String(ICIP = {IEEE Int. Conf. Image Process.})
% @String(ACCV  = {ACCV})
% @String(ICLR = {Int. Conf. Learn. Represent.})
% @String(IJCAI = {IJCAI})
% @String(PR   = {Pattern Recognition})
% @String(AAAI = {AAAI})
% @String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
% @String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

% @String(SPL	= {IEEE Sign. Process. Letters})
% @String(VR   = {Vis. Res.})
% @String(JOV	 = {J. Vis.})
% @String(TVC  = {The Vis. Comput.})
% @String(JCST  = {J. Comput. Sci. Tech.})
% @String(CGF  = {Comput. Graph. Forum})
% @String(CVM = {Computational Visual Media})


% Full text version for consistency 
@String(PAMI = {IEEE Transactions on Pattern Analysis and Machine Intelligence})
@String(IJCV = {International Journal of Computer Vision})
@String(CVPR= {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}) %Proceedings
@String(ICCV= {Proceedings of the IEEE/CVF International Conference on Computer Vision}) %Proceedings
@String(ECCV= {European Conference on Computer Vision }) %Proceedings ?
@String(NIPS= {Advances in Neural Information Processing Systems})
@String(ICPR = {International Conference on Pattern Recognition})
@String(BMVC= {British Machine Vision Conference })
@String(TOG= {ACM Transactions on Graphics })
@String(TIP  = {IEEE Transactions on Image Processing})
@String(TVCG  = {IEEE Transactions on Visualization and Computer Graphics})
@String(TMM  = {IEEE Transactions on Multimedia})
@String(ACMMM= {ACM International Conference on Multimedia})
@String(ICME = {IEEE International Conference on Multimedia & Expo })
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE International Conference on Image Processing})
@String(ACCV  = {ACCV})
@String(ICLR = {International Conference on Learning Representations})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conference on Computer Vision and Pattern Recognition Workshops}) %Proceedings
@String(CSVT = {IEEE Transactions on Circuits and Systems for Video Technology})
@String(ICML = {International Conference on Machine Learning})
@String(WACV = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}) %Proceedings
@String(NAACL = {NAACL})
@String(ICRA = {IEEE International Conference on Robotics and Automation})
@String(TMI = {IEEE Transactions on Medical Imaging})
@String(CVM = {Computational Visual Media})
@string(CVIU = {Computer Vision and Image Understanding})
@String(SPL	= {IEEE Signal Processing Letters})
@String(JMI = {Journal of medical imaging})
@string(EMNLP = {Conference on Empirical Methods in Natural Language Processing})
@String(CoRR = {CoRR})
@String(TCS = {Trends in Cognitive Sciences})
@String(MICCAI = {International Conference on Medical Image Computing and Computer-Assisted Intervention})
@string(ICASSP = {IEEE International Conference on Acoustics, Speech and Signal Processing})
@string(IJCNN = {International Joint Conference on Neural Networks})


@inproceedings{Swin_v2,
  title={Swin transformer v2: Scaling up capacity and resolution},
  author={Liu, Ze and Hu, Han and Lin, Yutong and Yao, Zhuliang and Xie, Zhenda and Wei, Yixuan and Ning, Jia and Cao, Yue and Zhang, Zheng and Dong, Li and others},
  booktitle=CVPR,
  year={2022}
}


@article{MobileNet,
  author    = {Andrew G. Howard and
               Menglong Zhu and
               Bo Chen and
               Dmitry Kalenichenko and
               Weijun Wang and
               Tobias Weyand and
               Marco Andreetto and
               Hartwig Adam},
  title     = {MobileNets: Efficient Convolutional Neural Networks for Mobile Vision
               Applications},
  journal   = CoRR,
  volume    = {abs/1704.04861},
  year      = {2017}
}

@inproceedings{MobileNetV2,
  title={MobileNet-v2: Inverted residuals and linear bottlenecks},
  author={Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  booktitle=CVPR,
  year={2018}
}


@inproceedings{ShuffleNet,
  title={Shufflenet: An extremely efficient convolutional neural network for mobile devices},
  author={Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian},
  booktitle=CVPR,
  year={2018}
}
@inproceedings{MobileNetV3,
  title={Searching for MobileNet-v3},
  author={Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and others},
  booktitle=ICCV,
  year={2019}
}


@inproceedings{espnetv2,
  title={Espnetv2: A light-weight, power efficient, and general purpose convolutional neural network},
  author={Mehta, Sachin and Rastegari, Mohammad and Shapiro, Linda and Hajishirzi, Hannaneh},
  booktitle=CVPR,
  year={2019}
}

@inproceedings{ShuffleNetV2,
  title={Shufflenet v2: Practical guidelines for efficient cnn architecture design},
  author={Ma, Ningning and Zhang, Xiangyu and Zheng, Hai-Tao and Sun, Jian},
  booktitle=ECCV,
  year={2018}
}


@inproceedings{mnasnet,
  title={Mnasnet: Platform-aware neural architecture search for mobile},
  author={Tan, Mingxing and Chen, Bo and Pang, Ruoming and Vasudevan, Vijay and Sandler, Mark and Howard, Andrew and Le, Quoc V},
  booktitle=CVPR,
  year={2019}
}

@inproceedings{Pruning,
  title={Learning Structured Sparsity in Deep Neural Networks},
  author={Wen, Wei and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
  booktitle=NIPS,
  year={2016} 
}

@inproceedings{BitMix,
  title={Bit-Mixer: Mixed-precision networks with runtime bit-width selection}, 
  author={Bulat, Adrian and Tzimiropoulos, Georgios},
  booktitle=ICCV, 
  year={2021},
  }

@inproceedings{DCompress,
  title={Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding},
  author={TSong Han and Huizi Mao and  William J. Dally},
  booktitle=ICLR,
  year={2016} 
}


@inproceedings{Jacob_2018_CVPR,
  title = {Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference},
  author = {Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  booktitle={CVPR}, 
  year={2018},
  }
  

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle = ICCV,
  year={2021}
}
@inproceedings{MobileViT,
  title={Mobilevit: light-weight, general-purpose, and mobile-friendly vision transformer},
  author={Mehta, Sachin and Rastegari, Mohammad},
  booktitle=ICLR,
  year={2022}
}
@article{fastformer,
  title={Fastformer: Additive Attention Can Be All You Need},
  author={Wu, Chuhan and Wu, Fangzhao and Qi, Tao and Huang, Yongfeng},
  journal={arXiv preprint arXiv:2108.09084},
  year={2021}
}
@article{ViTs,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{liu2021efficient,
    title     = {Efficient Training of Visual Transformers with Small Datasets},
    author    = {Liu, Yahui and Sangineto, Enver and Bi, Wei and Sebe, Nicu and Lepri, Bruno and De Nadai, Marco},
    booktitle = NIPS,
    year      = {2021}
}

@inproceedings{deit,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle=ICML,
  year={2021}
}

@inproceedings{Token_Sparse,
  title={DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification},
  author={Rao, Yongming and Zhao, Wenliang and Liu, Benlin and Lu, Jiwen and Zhou, Jie and Hsieh, Cho-Jui},
  booktitle=NIPS,
  year={2021}
}


@inproceedings{ATS,
  title={Adaptive token sampling for efficient vision transformers},
  author={Fayyaz, Mohsen and Abbasi Kouhpayegani, Soroush and Rezaei Jafari, Farnoush 
  and Sommerlade, Eric and Vaezi Joze, Hamid Reza and Pirsiavash, Hamed and Gall, Juergen},
  journal={ECCV},
  year={2022}
}
@inproceedings{EdgeViT,
  title={EdgeViTs: Competing Light-weight CNNs on Mobile Devices with Vision Transformers},
  author={Pan, Junting and Bulat, Adrian and Tan, Fuwen and Zhu, Xiatian and Dudziak, Lukasz and Li, Hongsheng and Tzimiropoulos, Georgios and Martinez, Brais},
  booktitle=ECCV,
  year={2022}
}
@inproceedings{tolstikhin2021mlp,
  title={Mlp-mixer: An all-mlp architecture for vision},
  author={Tolstikhin, Ilya and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and others},
  booktitle = NIPS,
  year={2021}
}
@inproceedings{MetaFormer,
  title={Metaformer is actually what you need for vision},
  author={Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{EfficientFormer,
  title={EfficientFormer: Vision Transformers at MobileNet Speed},
  author={Li, Yanyu and Yuan, Geng and Wen, Yang and Hu, Ju and Evangelidis, Georgios and Tulyakov, Sergey and Wang, Yanzhi and Ren, Jian},
  journal=NIPS,
  year={2022}
}

@inproceedings{MobileFormer,
  title={Mobile-former: Bridging mobilenet and transformer},
  author={Chen, Yinpeng and Dai, Xiyang and Chen, Dongdong and Liu, Mengchen and Dong, Xiaoyi and Yuan, Lu and Liu, Zicheng},
  booktitle=CVPR,
  year={2022}
}


@article{EdgeFormer,
  title={EdgeFormer: Improving Light-weight ConvNets by Learning from Vision Transformers},
  author={Zhang, Haokui and Hu, Wenze and Wang, Xiaoyu},
  journal={arXiv preprint arXiv:2203.03952},
  year={2022}
}

@inproceedings{EdgeNeXt,
    title={EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications},
    author={Muhammad Maaz and Abdelrahman Shaker and Hisham Cholakkal and Salman Khan and Syed Waqas Zamir and Rao Muhammad Anwer and Fahad Shahbaz Khan},
    booktitle={CADL2022},
    year={2022}
}

@article{
    MobileViT2,
    title={Separable Self-attention for Mobile Vision Transformers},
    author={Sachin Mehta and Mohammad Rastegari},
    journal={Transactions on Machine Learning Research},
    year={2023},
}


@inproceedings{Twins,
	title={Twins: Revisiting the Design of Spatial Attention in Vision Transformers},
	author={Xiangxiang Chu and Zhi Tian and Yuqing Wang and Bo Zhang and Haibing Ren and Xiaolin Wei and Huaxia Xia and Chunhua Shen},
	booktitle=NIPS,
	year={2021}
}

@inproceedings{MaxViT,
    title={MaxViT: Multi-Axis Vision Transformer},
    author={Tu, Zhengzhong and Talebi, Hossein and Zhang, Han and Yang, Feng and Milanfar, Peyman and Bovik, Alan and Li, Yinxiao},
    booktitle={ECCV},
    year={2022}
}


@article{LinFormer,
    title={Linformer: Self-Attention with Linear Complexity},
    author={Sinong Wang and Belinda Z. Li and Madian Khabsa and Han Fang and Hao Ma},
    journal={arXiv preprint arXiv:2006.04768},
    year={2020},
}


@inproceedings{ReFormer,
    title       = {Reformer: The Efficient Transformer},
    author      = {Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},
    booktitle   = {International Conference on Learning Representations},
    year        = {2020},
}

@inproceedings{PyramidViT,
  title={Pyramid vision transformer: A versatile backbone for dense prediction without convolutions},
  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
  booktitle={CVPR},
  year={2021}
}

@article{PyramidViT2,
  title={Pvtv2: Improved baselines with pyramid vision transformer},
  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
  journal={Computational Visual Media},
  year={2022},
}
@article{UNETR++,
      title={UNETR++: Delving into Efficient and Accurate 3D Medical Image Segmentation},
      author={Shaker, Abdelrahman and Maaz, Muhammad and Rasheed, Hanoona and Khan, Salman and Yang, Ming-Hsuan and Khan, Fahad Shahbaz},
      journal={arXiv:2212.04497},
      year={2022},
}

@inproceedings{MViT,
  title={Multiscale vision transformers},
  author={Fan, Haoqi and Xiong, Bo and Mangalam, Karttikeya and Li, Yanghao and Yan, Zhicheng and Malik, Jitendra and Feichtenhofer, Christoph},
  booktitle=ICCV,
  year={2021}
}

@inproceedings{TokenLearner,
 title = {TokenLearner: Adaptive Space-Time Tokenization for Videos},
 author = {Ryoo, Michael and Piergiovanni, AJ and Arnab, Anurag and Dehghani, Mostafa and Angelova, Anelia},
 booktitle = NIPS,
 year = {2021}
}
@inproceedings{PiT,
    title={Rethinking Spatial Dimensions of Vision Transformers},
    author={Byeongho Heo and Sangdoo Yun and Dongyoon Han and Sanghyuk Chun and Junsuk Choe and Seong Joon Oh},
    booktitle = {ICCV},
    year={2021},
}
@inproceedings{ConvNeXt,
  title={A ConvNet for the 2020s},
  author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle=CVPR,
  year={2022}
}
@article{ConvNeXtV2,
  title={ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders},
  author={Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon and Saining Xie},
  year={2023},
  journal={arXiv preprint arXiv:2301.00808},
}

@inproceedings{MaskFormer,
  title={Masked-attention Mask Transformer for Universal Image Segmentation},
  author={Bowen Cheng and Ishan Misra and Alexander G. Schwing and Alexander Kirillov and Rohit Girdhar},
  booktitle=CVPR,
  year={2022}
}

@inproceedings{DETR,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle=ECCV,
  year={2020}
}

@inproceedings{zhu2020deformable,
  title={Deformable DETR: Deformable Transformers for End-to-End Object Detection},
  author={Zhu, Xizhou and Su, Weijie and Lu, Lewei and Li, Bin and Wang, Xiaogang and Dai, Jifeng},
  booktitle=ICLR,
  year={2021}
}

@inproceedings{meng2021conditional,
  title={Conditional detr for fast training convergence},
  author={Meng, Depu and Chen, Xiaokang and Fan, Zejia and Zeng, Gang and Li, Houqiang and Yuan, Yuhui and Sun, Lei and Wang, Jingdong},
  booktitle=ICCV,
  year={2021}
}

@inproceedings{li2022dn,
  title={Dn-detr: Accelerate detr training by introducing query denoising},
  author={Li, Feng and Zhang, Hao and Liu, Shilong and Guo, Jian and Ni, Lionel M and Zhang, Lei},
  booktitle=CVPR,
  year={2022}
}

@inproceedings{zhang2022dino,
  title={Dino: Detr with improved denoising anchor boxes for end-to-end object detection},
  author={Zhang, Hao and Li, Feng and Liu, Shilong and Zhang, Lei and Su, Hang and Zhu, Jun and Ni, Lionel and Shum, Harry},
  booktitle=ICLR,
  year={2022}
}

@article{YOLOS,
  title={You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection},
  author={Fang, Yuxin and Liao, Bencheng and Wang, Xinggang and Fang, Jiemin and Qi, Jiyang and Wu, Rui and Niu, Jianwei and Liu, Wenyu},
  journal={arXiv preprint arXiv:2106.00666},
  year={2021}
}
@inproceedings{Swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle = ICCV,
  year={2021}
}
@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle=CVPR,
  year={2016}
}
@inproceedings{PoolFormer,
  title={Metaformer is actually what you need for vision},
  author={Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng},
  booktitle={CVPR},
  year={2022}
}
@inproceedings{pvt,
  title={Pyramid vision transformer: A versatile backbone for dense prediction without convolutions},
  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={568--578},
  year={2021}
}

@inproceedings{
AdamW,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle=ICLR,
year={2019}
}

@inproceedings{RegNet,
  title={Designing Network Design Spaces}, 
  author={Radosavovic, Ilija and Kosaraju, Raj Prateek and Girshick, Ross and He, Kaiming and Doll√°r, Piotr},
  booktitle={CVPR}, 
  year={2020},
  }
  
@inproceedings{PyTorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle=NIPS,
year = {2019}
}
@misc{coreml2021,
  author = {CoreMLTools.},
  title = {Use coremltools to convert models from third-party libraries to Core ML.},
  year = 2021,
  url = {https://coremltools.readme.io/docs},
}

@inproceedings{COCO,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle=ECCV,
  year={2014}
}
@inproceedings{ADE20K,
    title={Scene Parsing through ADE20K Dataset},
    author={Zhou, Bolei and Zhao, Hang and Puig, Xavier and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},
    booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
    year={2017}
}

@inproceedings {FPN,
author = {A. Kirillov and R. Girshick and K. He and P. Dollar},
booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {Panoptic Feature Pyramid Networks},
year = {2019},
}

@inproceedings{Maaz2022Multimodal,
      title={Class-agnostic Object Detection with Multi-modal Transformer},
      author={Maaz, Muhammad and Rasheed, Hanoona and Khan, Salman and Khan, Fahad Shahbaz and Anwer, Rao Muhammad and Yang, Ming-Hsuan},
      booktitle=ECCV,
      year={2022},
}

@inproceedings{zhang2022parcnet,
  title={ParC-Net: Position Aware Circular Convolution with Merits from ConvNets and Transformer},
  author={Zhang, Haokui and Hu, Wenze and Wang, Xiaoyu},
  booktitle=ECCV,
  year={2022}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle=CVPR,
  year={2009}
}

@inproceedings{EfficientNet,
  title = 	 {{E}fficient{N}et: Rethinking Model Scaling for Convolutional Neural Networks},
  author =       {Tan, Mingxing and Le, Quoc},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  year = 	 {2019},
}
@inproceedings{RandAugment,
 author = {Cubuk, Ekin Dogus and Zoph, Barret and Shlens, Jon and Le, Quoc},
 booktitle=NIPS,
 title = {RandAugment: Practical Automated Data Augmentation with a Reduced Search Space},
 year = {2020}
}
@inproceedings{CutMix,
  title={Cutmix: Regularization strategy to train strong classifiers with localizable features},
  author={Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  booktitle=ICCV,
  year={2019}
}
@inproceedings{
Mixup,
title={mixup: Beyond Empirical Risk Minimization},
author={Hongyi Zhang and Moustapha Cisse and Yann N. Dauphin and David Lopez-Paz},
booktitle=ICLR,
year={2018}
}