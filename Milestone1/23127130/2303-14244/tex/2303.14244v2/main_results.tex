\section{Main results}
In this section, we present our main results. We begin with a few preliminary definitions.
\begin{definition}[Condition number]
We denote the condition number of the ground truth matrix $X$ by 
\begin{equation*}
    \kappa 
    :=
    \frac{\norm{X}}{\sigma_{r} \left(X\right)}.
\end{equation*}
\end{definition}
\noindent The second definition concerns the measurement operator $\mathcal{A}$.
\begin{definition}[Restricted Isometry Property \cite{RechtFazel}]
    We say that the measurement operator  $\mathcal{A}: \R^{n_1 \times n_2} \rightarrow \R^m $ has the restricted isometry property (RIP) of order $r$ with constant $\delta >0$, if for all matrices $M \in \R^{n_1 \times n_2}$ of rank at most $r$ it holds that
    \begin{equation*}
        \left(1-\delta\right) \fbnorm{M}^2 \le \twonorm{ \mathcal{A} \left(M\right) }^2 \le \left(1+\delta\right) \fbnorm{M}^2.
    \end{equation*}
\end{definition}
It is well known that if all entries of the measurement matrices $A_i$ are independent (sub-)gaussian random variables
with zero mean and variance $1/m$, 
then the operator $\mathcal{A}$ fulfills the restricted isometry property of order $r$ with constant $\delta >0$
if the number of measurements satisfies $ m \gtrsim \frac{r \left(n_1 + n_2\right) }{\delta^2} $, see \cite{candesplan,vershynin2018high}. With these notations in place, we are now ready to state the main result of this paper.
\begin{theorem}\label{theorem:main}
Let $X\in \R^{n_1 \times n_2}$ be a rectangular matrix of rank $r$
and assume that we are given measurements of the form $ y = \mathcal{A} \left(X\right) $. Furthermore,
assume that the linear measurement operator $\mathcal{A}$ satisfies the restricted isometry property of order $2r+1 $ with constant $ \delta \le \frac{c_1}{\kappa^3 \sqrt{r}} $.
To identify the matrix $X$ we run gradient descent iterates of the form $\left\{ V_t; W_t \right\}_{t \in \mathbb{N}}$ with step size $\mu$ per the updates \eqref{Vupdate} and \eqref{Wupdate} starting from the initialization factors $V_0= \alpha V \in \mathbb{R}^{n_1 \times k}$ and $W_0=\alpha W \in \mathbb{R}^{n_2 \times k} $ for some $k \ge r$.
Here, the matrices $V$ and $W$ have i.i.d. entries with distribution $\mathcal{N} \left(0,1\right)$ and $\alpha >0$ denotes the scale of initialization. Consider $0< \varepsilon <1$ and assume that the step size $\mu$ satisfies
    \begin{equation}\label{main:stepsizeassumption}
        \mu \le  \frac{c_2}{ \kappa^5 \norm{X}} \cdot \frac{1}{ \ln \left( \frac{2 \sqrt{2 \norm{X}} }{  \varepsilon \alpha \left( \sqrt{k} - \sqrt{r-1} \right) } \right) }.
    \end{equation}
Also assume that the scale of initialization $\alpha$ satisfies
    \begin{equation}\label{main:alphabound}
        \alpha \le  
    \frac{ c_3 \sqrt{ \norm{X}  } }{ k^{5}  \left( \max \left\{  n_1 + n_2;k \right\} \right)^2} \left( \frac{ \varepsilon \left( \sqrt{k} -\sqrt{r-1} \right) }{C_1 \kappa^2  \sqrt{\max \left\{  n_1 + n_2; k \right\}} } \right)^{C_2 \kappa }.
    \end{equation}
    Then, 
    after 
    \begin{equation}\label{lemma:tildetbound}
        T
        \lesssim \frac{ \ln \left( \frac{2  \sqrt{2 \norm{X}} }{ \varepsilon \alpha \left( \sqrt{k} - \sqrt{r-1} \right) } \right) }{\mu \sglmin (X)}
    \end{equation}
    iterations, with probability at least $1 -  C_3 \exp \left( -c_4 k \right) + \left(C_4 \varepsilon\right)^{k-r+1} $ it holds that
    \begin{equation}\label{equ:mainresultfinalbound}
    \frac{\norm{V_{T} W_{T}^T - X}}{\norm{X}}
    \lesssim 
    \frac{\alpha^{3/5} }{ \norm{X}^{3/10} }.
    \end{equation}
    Here, $C_1,C_2,C_3,C_4,c_1,c_2,c_3,c_4>0$ are fixed numerical constants.
\end{theorem}
A few comments are in order.

\noindent\textbf{Impact of the scale of initialization on the reconstruction error of $X$:}
    Note that $\norm{V_{T} W_{T}^T - X} $ can be interpreted as the reconstruction error.
    From inequality \eqref{equ:mainresultfinalbound} it follows 
    that this reconstruction error can be made arbitrarily small by choosing the scale of initialization $\alpha$ small enough.
    We note that polynomial dependence of the reconstruction error on $\alpha$ is also observed in our experiments, see Section \ref{sec:experiments}.
   
\noindent\textbf{Overparameterization:}
    Our result holds for any choice of $k$ (the number of columns of the factors $U_t$ and $V_t$), which determines the number of parameters in the training model. Note that in overparameterized models, i.e., $k(n_1+n_2) \gg m $, there may be infinitely many global minimizers of the loss function $\mathcal{L}$ in \eqref{equ:lossdefinition} with arbitrarily large test error. Despite that, our result guarantees that for a sufficiently small random initialization, vanilla gradient descent finds the low-rank solution.
    
\noindent\textbf{Non-overparameterized setting ($k=r$):} In this special case, our result implies that if the measurement operator fulfills the restricted isometry property,
gradient descent with small, random initialization will converge to the ground truth matrix $X$ in
polynomial time. It is known that under the RIP assumption the loss landscape is benign in the
sense that there are no local optima that are not global and all saddles have a direction of negative
curvature. However, such results do not imply that vanilla gradient descent converges quickly, i.e., in polynomial time, to a global optimum, as gradient descent may take exponential time to escape
from saddle points \cite{du2017gradient}.
To the best of our knowledge, this is the first result in the non-overparameterized asymmetric setting $k=r$ which shows the convergence of vanilla gradient descent to the ground truth from a random
initialization using only the restricted isometry property in polynomial time. 
    
    \noindent\textbf{Sample complexity:} If $\mathcal{A}$ is a Gaussian measurement operator, we need
    $m \gtrsim \kappa^{6} r^2 (n_1 +n_2)$ measurements to guarantee that the assumption on the RIP constant $\delta$ holds with high probability.
    Thus, the number of required measurements depends only on the rank of the ground truth matrix $X$, which is $r$, and not on the number of columns $k$ of $V_t$ and $W_t$, which determines the number of parameters of our training model.
    
\noindent\textbf{Step size:}
    The bound on the step size, \eqref{main:stepsizeassumption}, depends logarithmically on the scale of initialization $\alpha$.
    This assumption is necessary for us to show that the imbalance matrix $V_t^TV_t-W_t^TW_t$ remains bounded.
    However, as we will explain in Section \ref{sec:proofoutline}, to achieve this near-optimal dependence of the step size on $\alpha$ we need to conduct a fine-grained analysis of the evolution of this imbalance matrix $ V_t^T V_t -W_t^T W_t$ during training. This intricate analysis goes beyond just controlling the spectral norm of this imbalance matrix. In particular, as also mentioned in the introduction, we also need to show more intricate forms of coupling of the $V_t$ and $W_t$ trajectories. 


