\section{Numerical experiments}\label{sec:experiments}
In this section, we conduct numerical experiments to verify our theoretical results.
%We provide further numerical experiments in Appendix \ref{addexp}.
In our experiments we set $X \in \mathbb{R}^{n_1 \times n_2}$ to be a random matrix with $n_1 = 100$, $n_2 = 50$ and rank $r = 5$.
%and i.i.d. Gaussian entries with distribution $\mathcal{N} \left(0,1\right)$.
Specifically, we generate random matrices $X_1\in \mathbb{R}^{n_1 \times r}$ and $X_2\in \mathbb{R}^{r \times n_2}$ whose entries are drawn from the Gaussian distribution $\mathcal{N} \left(0,1\right)$ and set $X = \frac{X_1X_2}{\norm{X_1X_2}}$.
We use $m = 2000$ random Gaussian measurements.

\subsection{Variations in imbalance with different initialization scales}
In our first experiment, we want to show how the spectral norm of the imbalance term  $\norm{V_t^TV_t-W_t^TW_t}$ evolves during training
for different choices of the scale of initialization $\alpha$.
To this aim, we randomly generate some $V_{rd}$ and $W_{rd}$ using the normal distribution and run gradient descent with $V_0 = \alpha V_{rd}, W_0 = \alpha W_{rd}$ for $\alpha = \{10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}\}$.
We both consider the empirical loss \eqref{equ:lossdefinition} and the population loss 
\begin{equation}
\label{testerr}
   \Loss_{\text{population}} \left(V,W\right) = \frac{1}{2}  \big\Vert X -V W^T \big\Vert_F^2.
\end{equation}
Moreover, we set the step size $\mu = \frac{1}{100\|X\|}$. 
The results are depicted in Figure \ref{fig:balancedness_scaleinit}.

\begin{figure}[t]
    \centering
    \includegraphics[scale=0.6]{Figures/Figures_Balance_vs_initialization.pdf}
    \caption{Evolution of the imbalance term $ \norm{V_t^T V_t - W_t^T W_t} $ with different choices of the scale of initialization $\alpha$.}
    \label{fig:balancedness_scaleinit}
\end{figure}

We observe that in the population case, the imbalance term $\norm{V_t^T V_t - W_t^T W_t} $ stays almost constant during the training.
This is in stark contrast to the empirical scenario, where we observe that the imbalance term grows until it reaches a certain threshold. We like to note that as evident in Figure \ref{fig:balancedness_scaleinit} when the scale of initialization $\alpha$ is chosen sufficiently small, this threshold does not depend on the scale of initialization.
We see that this is different for large initialization ($\alpha=10^{-2}$), but this is to be expected as in this case even in the beginning the imbalance term $\norm{V_t^T V_t - W_t^T W_t} $ is already larger than the threshold in the case of a smaller initialization. 

It is worth noting that the fact that the imbalance term evolves very differently in the population and in the empirical scenario has a huge impact on our theoretical analysis.
In contrast to \cite{ye2021global,ding2021global}, which analyze the population loss scenario, we need a much finer analysis of the imbalance term beyond controlling the spectral norm as stated earlier. For further experiments we refer to Section \ref{sec:threephase}.




\subsection{Change of test and train error during training}
\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[scale=0.55]{Figures/Figures_train_and_test_error_big.pdf}
    \caption{}
    \label{largeinit}
    \end{subfigure}
    \begin{subfigure}[b]{0.42\textwidth}
        \includegraphics[scale=0.55]{Figures/Figures_train_and_test_error_small.pdf}
        \caption{}
        \label{smallinit}
    \end{subfigure}
    %\subfigure[]{\label{largeinit}}
%\subfigure[]{\label{smallinit}\includegraphics[scale=0.55]{Figures/Figures_train_and_test_error_small.pdf}}
    \caption{Depiction of test error $\frac{1}{2}\|V_tW_t^T - X\|^2_F$ and train error $\mathcal{L}(V_t,W_t)$ for (a) large and (b) small $\alpha$ during training.}
    \label{fig:testtrainevolution1}
\end{figure}
% parameters: $n_1 = 100, n_2 = 50, r = 5, k = 40, m = 2000, s = \frac{1}{4\|X\|}$.
In the next two experiments, we want to understand how the test error \eqref{testerr} and the train error $ \mathcal{L} \left(V_t, W_t\right) $ change during training.
For that, we fix the rank of the model to be $k=40$ and we set the step size as $ \mu = \frac{1}{4 \norm{X}} $.

In the first experiment, we compare the evolution of the train and test error for very large and for very small scale of initialization $\alpha$, see Figure \ref{fig:testtrainevolution1}.
For large initialization, depicted in Figure \ref{largeinit}, we observe that the train error converges linearly to $0$, whereas the test error stays roughly constant at a large value. This figure clearly demonstrates that in this large initialization regime the learned model does not generalize well. Choosing a large initialization corresponds to what in the literature is called \textit{lazy training} \cite{chizat2019lazy} which has been extensively studied in the context of neural networks, see e.g., \cite{oymak2020towards,du2019gradient}.
(In the context of matrix sensing with symmetric, positive definite matrices the lazy training regime has been theoretically analyzed in \cite[Theorem 4.2]{oymak2019overparameterized}.) For small initialization, depicted in Figure \ref{smallinit}, which corresponds to the regime studied in this paper, we observe that train and test error evolve very differently.
Indeed, we observe that in this regime both the train and test error decay and thus the solution found by gradient descent does generalize well.

In the next experiment, we want to understand how the relative test error $\frac{\|V_tW_t^T - X\|^2_F}{\|X\|^2_F}$ depends on the scale of initialization $\alpha$. To this aim,
 we run gradient descent until the train error is below $0.5 \times 10^{-9}$ and then we plot the test and train error for several choices of $\alpha$.
The results are depicted in Figure \ref{fig:testtrainevolution2}.
We observe that the train error depends polynomially on the scale of initialization.
This is in line with our main result, Theorem \ref{theorem:main}. Finally, let us note that the last two experiments resemble what has been observed in the symmetric matrix sensing scenario, see, e.g., \cite{stoger2021small}.

\begin{figure}[t]
    \centering
   \includegraphics[scale=0.8]{Figures/Figures_Testerrorvsinitialization.pdf}
  \caption{Relative test error $\frac{ \norm{V_tW_t^T - X}^2_F}{\norm{X}^2_F}$ at the end of training for different scales of initialization $\alpha$.}
 \label{fig:testtrainevolution2}
\end{figure}

\input{addexp.tex}

