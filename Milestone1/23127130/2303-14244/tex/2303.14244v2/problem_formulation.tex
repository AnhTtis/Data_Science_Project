\section{Problem Formulation}
In this paper, we focus on reconstructing a low-rank matrix $X\in \R^{n_1 \times n_2}$ of rank $r$ from  $\left[m\right]:= \left\{ 1, 2, \ldots, m \right\}$ linear measurements of the form
\begin{equation}\label{equ:measurementsequation}
    y_i = \innerproduct{A_i, X}:= \tr\left( A_i X^T \right) ,
\end{equation}
where $ \left\{ A_i\right\}_{i=1}^m \subset \mathbb{R}^{n_1 \times n_2}$ represent known measurement matrices. The linear system in \eqref{equ:measurementsequation} can also be rewritten in the more compact form $y = \Acal \left(X\right)$, where $ \mathcal{A}: \mathbb{R}^{n_1 \times n_2} \rightarrow \mathbb{R}^m$ is a linear measurement operator defined as $\left[ \mathcal{A} \left(Z\right) \right]_i = \innerproduct{A_i, Z} $ and $y\in\mathbb{R}^m$ is a vector consisting of all measurements.

To find the low-rank matrix, we consider the natural factorized loss function 
\begin{equation}\label{equ:lossdefinition}
   \Loss \left(V,W\right) := \frac{1}{2}  \sum_{i=1}^m \left( y_i - \innerproduct{A_i, VW^T} \right)^2=\frac{1}{2} \twonorm{ y - \Acal \left( VW^T\right) }^2,
\end{equation}
where $V \in \R^{n_1 \times k} $ and $ W \in \R^{n_2 \times k} $ are possibly overparameterized factors ($k\ge r$). To minimize this loss we train the factors $V$ and $W$ via gradient descent updates of the form 
\begin{align}
    V_{t+1}=&V_t - \mu \nabla_V \Loss (V_t,W_t)= V_t - \mu \left[ \left(\Acal^* \Acal \right) \left( V_t W_t^T - X \right)  \right] W_t, \label{Vupdate}\\
    W_{t+1}=&W_t - \mu \nabla_W \Loss (V_t,W_t)= W_t - \mu \left[ \left(\Acal^* \Acal \right) \left( V_t W_t^T - X \right)  \right]^T V_t\label{Wupdate}
\end{align}
starting from initial factors $V_0 \in \R^{n_1 \times k} $ and $ W_0 \in \R^{n_2 \times k} $ with step size $\mu >0$. Here, $ \Acal^*$ denotes the adjoint operator of $\Acal$.

\subsection*{Notation}


For any matrix $M \in \mathbb{R}^{d_1 \times d_2} $, we denote its Frobenius norm by $ \norm{M}_F := \sqrt{\text{trace} \left(A^T A\right) } $, its spectral norm by $ \norm{M} $, and its nuclear norm (i.e., the sum of the singular values of $M$) by $ \nucnorm{M}$. Moreover, we denote the singular value decomposition of $ M$ by $ M =  P_{M}  \Sigma_{ M}  Q_{ M}^T$ with
\begin{equation*}
\Sigma_{ M} = \text{diag} \left( \sigma_1 (M),\sigma_2 (M),\ldots,\sigma_{\text{rank} (M)} (M) \right),
\end{equation*}
where
$\sigma_1  (M)\ge \sigma_2 (M) \ge \ldots \ge \sigma_{\text{rank} (M)} (M)>0$
denote the singular values of the matrix $M$. We also use $P_{M,\bot} \in \mathbb{R}^{d_1 \times (d_2 - \text{rank} (M) )} $ to denote a matrix whose columns are orthonormal and orthogonal to the column span of $ P_M $.
The matrix $Q_{M,\bot} \in \mathbb{R}^{d_1 \times (d_2 -\text{rank} (M)  ) }$ is defined analogously. 
The set of symmetric matrices in $ \mathbb{R}^{d \times d} $ is denoted by $ \text{Sym}_{d}$.
Finally, for a symmetric matrix $ S \in \mathbb{R}^{d \times d} $ we denote its eigenvalues by
$\lambda_1 (S) \ge  \lambda_2 (S) \ge \ldots  \ge \lambda_d (S)$. 