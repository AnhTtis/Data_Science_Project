\section{Introduction}


Over the past few years, there has been a significant amount of focus on understanding the optimization and generalization dynamics of \emph{overparameterized} learning problems.
Such overparameterized training, 
which involves training models with more parameters than training data,
is the contemporary learning paradigm for a variety of problems spanning deep neural networks to matrix factorization. Surprisingly, despite the existence of many global optima of the training loss with subpar generalization/prediction capability, these models avoid such \emph{overfitting} when trained via (stochastic) gradient descent \cite{zhang2016understanding}. 

Among others, there seem to be two critical components facilitating this success, putting the gradient updates on a trajectory towards parameters that are not only globally optimal but also generalize well.
The first critical component is the role of small random initialization which seems to guide the gradient trajectory towards low complexity models that tend to generalize better.
For instance in low-rank matrix reconstruction, small random initialization guides the trajectory towards low-rank solutions \cite{gunasekar2018implicit}. 
For neural networks, small random initialization plays a critical role in allowing the neural net to learn good features (a.k.a. \emph{feature learning} regime) facilitating generalization performance far superior to the features of the model learned with large initialization (whose features are essentially frozen at the random initialization a.k.a.~lazy training or NTK regime) \cite{chizat2019lazy}.
The second critical component is that the parameters of the different layers of the model are coupled in intricate ways during the gradient descent trajectory. Understanding these intricate couplings of the trajectory of gradient descent is thus crucial to understand the generalization dynamics during training.


Recently, there has been interesting progress in understanding the role of small random initialization for a variety of problems ranging from positive semi-definite low-rank matrix sensing \cite{ LiMaCOLT18,stoger2021small} to one-hidden layer neural networks in a training regime where only one of the layers plays a dominant role in terms of generalization 
\cite{damian2022neural,mousavi2022neural,bietti2022learning}.
Despite this progress, the intricate couplings of the trajectory are far less understood.
In this paper, we wish to demystify both the role of small random initialization and the intricate coupling with an emphasis on the latter.

We focus on the problem of asymmetric low-rank matrix reconstruction from a few linear measurements in the overparameterized regime where the goal is to reconstruct a matrix from linear measurements of the form 
\begin{equation*}
 y_i 
 = \innerproduct{A_i,X}
 := \tr \left( A_i^T X  \right) \quad \quad \text{for } i \in [m]:= \left\{ 1;2; \ldots; m \right\},
\end{equation*}
Here, $X \in \mathbb{R}^{n_1 \times n_2}$ is the unknown asymmetric low-rank matrix of rank $r$ that we wish to recover and $A_i$ are known measurement matrices.
To recover the unknown matrix $X$, we consider the loss
\begin{equation}\label{equ:lossdefinition}
   \Loss \left(V,W\right) := \frac{1}{2}  \sum_{i=1}^m \left( y_i - \innerproduct{A_i, VW^T} \right)^2,
\end{equation}
and train both $V \in \mathbb{R}^{n_1 \times k} $ and $W \in \mathbb{R}^{n_2 \times k}$ via gradient descent.
In this paper, we are especially interested in the overparameterized scenario, i.e., $ k (n_1 + n_2) \gg m $.
We show that in this setting, starting from small random initialization the gradient descent iterates $V_t$ and $W_t$ follow a trajectory with two implicit properties: (1) an algorithmic regularization property where the iterates show a propensity towards low-rank models despite the overparameterized nature of the factorized model.  (2) a coupling of the trajectory of the two factors via  intricate balancing properties where the factors remain approximately balanced throughout training in various ways.
In this problem, the second property is also crucial to showing the first.
Together these two implicit properties allow us to show that the gradient descent trajectory from small random initialization moves towards solutions that are both globally optimal and generalize well. 


Finally, we would like to emphasize that the coupling of the gradient trajectory of the two factors in the matrix sensing problem is quite intricate, even when compared to related problems such as matrix factorization.
For matrix factorization, one coupling that has been crucial in prior analysis is that the factors remain balanced in the sense that throughout the iterations we have $V_t^TV_t\approx W_t^TW_t$, see, e.g.,~\cite{du2018algorithmic,ye2021global,jiang2022algorithmic}. A similar balancing property does hold in matrix sensing as well. However, the behavior of the imbalance matrix $V_t^TV_t-W_t^TW_t$ is quite different. This contrast is depicted in Figure \ref{fig:balancevsinitialization} where we draw the spectral norm of the imbalance matrix in both cases. This figure demonstrates that imbalancedness increases initially in the matrix sensing problem before tapering off at a constant value. This is in sharp contrast with the matrix factorization problem where the imbalancedness is essentially constant throughout training with a much smaller constant value. As a result, a more careful analysis is required to control the imbalancedness in the matrix sensing problem. Furthermore, the analysis of the matrix sensing problem requires more intricate couplings of the trajectory. In particular, we demonstrate two additional couplings of the trajectory that are crucial for achieving strong generalization and optimization guarantees with a modest number of iterations. The first one is that the imbalance matrix is even smaller in certain ``nuisance" directions. This behavior is depicted in Figure \ref{fig:balancevariant} demonstrating that imbalancedness is orders of magnitude smaller in these nuisance directions. The second form of coupling is an angular form of balancedness demonstrating that an appropriate notion of angle between the imbalance matrix and certain ``signal" directions is sufficiently small across training as depicted in Figure \ref{fig:balanceangle}. In this paper, we show all of these coupling phenomena rigorously. These intricate couplings of the trajectory are crucial to our convergence analysis allowing us to show that despite overparameterization the trajectory of gradient descent leads to solutions with good generalization with fast convergence rates. 

\begin{figure}[t]
\begin{subfigure}[b]{0.3\textwidth}
\includegraphics[width=45mm]{Figures/Figures_Balance_vs_initialization_introduction.pdf}
\caption{}
\label{fig:balancevsinitialization}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\includegraphics[width=45mm]{Figures/Figures_BalanceVariant.pdf}
\caption{}
\label{fig:balancevariant}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\includegraphics[width=45mm]{Figures/Figures_BalanceAngle.pdf}
\caption{}
\label{fig:balanceangle}
\end{subfigure}
\caption{This figure depicts various couplings between the trajectories of the two factors $V_t$ and $W_t$ throughout training. First, (a) depicts the spectral norm of the imbalance matrix $V_t^TV_t-W_t^TW_t$ ($\|V_t^TV_t-W_t^TW_t\|$) for matrix sensing (this paper) and matrix factorization. While the size of imbalancedness remains essentially constant in the matrix factorization problem and is very small, it actually increases in the matrix sensing problem before settling at a small value (albeit orders of magnitude larger than in the matrix factorization case). This shows that the two factors continue to be balanced for matrix sensing but the nature of this balancedness is much more intricate. Second, (b) shows that the size of the imbalance matrix is orders of magnitude smaller in certain ``nuisance'' direction. Such a more refined control of imbalancedness is crucial in our proofs. Finally, in (c) we show that the angle between the imbalance matrix and certain ``signal'' directions are very small during training. This particular coupling of the trajectory also plays a critical role in our proofs.}
\label{allfigs}
%\vspace{-0.5cm}
\end{figure}


