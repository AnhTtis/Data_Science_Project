\section{Related work}



\textbf{Overparameterization in low-rank matrix recovery:} In \cite{gunasekar2017implicit}, the authors observed that in overparameterized low-rank matrix recovery models such as matrix sensing or matrix completion, gradient descent with a small, random initialization converges to a low-rank solution. 
Moreover, in this work it was conjectured, that for sufficiently small, random initialization gradient descent converges to a solution that is (almost) the nuclear norm minimizer, a common heuristic for finding the solution with minimal rank.
However, in  \cite{razin2020implicit} some examples have been constructed where the conjecture in  \cite{gunasekar2017implicit} does not hold.  
In \cite{LiMaCOLT18}, the authors show that small random initialization in symmetric matrix sensing (with a positive definite ground truth matrix) convergences to the ground truth matrix in the special case $k=n_1=n_2=n$.
A more recent paper \cite{stoger2021small} improved this result allowing for an arbitrary overparameterization parameter $k$ and also for an arbitrary scale of initialization $\alpha$.
A key insight in this work was that in the first few iterations, gradient descent exhibits a spectral bias.
A similar observation appeared in \cite{li2021towards}, which argues that gradient flow with sufficiently small initialization can be regarded as a rank-minimization heuristic.
Building upon the framework in \cite{stoger2021small}, this intuition has been made quantitative in \cite{jin2023understanding}.
The results in \cite{stoger2021small} have been generalized to the noisy case in \cite{ding2022validation}. 
In \cite{xu2023power} it was shown that preconditioned gradient descent leads to faster convergence compared to vanilla gradient descent for overparameterized (symmetric) low-rank matrix recovery problems with small random initialization.
We also note that this spectral bias has also been used in some of the algorithms and their corresponding analysis for avoiding strict saddles, e.g.,~see the interesting work in \cite{o2023line}.

Despite all of this interesting work, there has been much less understanding of the asymmetric matrix sensing problem. Very recently, the asymmetric case of the matrix sensing problem has been studied in the limit of the number of samples going to infinity (a.k.a.~population case) which is equivalent to the  matrix factorization problem.
In \cite{du2018algorithmic}, the authors show in this matrix factorization scenario that the factors $V_t$ and $W_t$ stay balanced and their product converges to the ground truth. However, no convergence rate was provided.
In \cite{ye2021global}, the same authors improved this result and showed that for matrix factorization in the non-overparameterized case, i.e., $k=r$, gradient descent from small, random initialization converges to the ground truth matrix with a polynomial amount of iterations.
Building upon \cite{ye2021global}, the paper \cite{jiang2022algorithmic} generalized these results to arbitrary rank $k$ among several other improvements.
However, the proof techniques of these works do not generalize to the finite sample scenario (i.e.~matrix sensing), which is the topic of this paper. The reason is that much more nuanced forms of coupling of the trajectories of $V_t$ and $W_t$ are required for matrix sensing as discussed in Figure \ref{allfigs} and further discussed in the proof and experimental sections.



%\textbf{Overparameterization in low-rank matrix recovery:} 
Let us also comment on several other related works.
In \cite{ding2022flat}, it has been shown that in overparameterized low-rank matrix recovery models, flat minima, measured by the trace of the Hessian, have better generalization properties.
There is also recent work on non-convex subgradient methods in low-rank matrix recovery
when the data is grossly corrupted by noise \cite{ma2021sign,ding2021rank}.
However, in contrast to our result, in these works, the sample complexity scales with $k$ and not with $r$.
In \cite{arora2019implicit} authors focus on low-rank matrix recovery for deeper models, i.e., those which have more than two factors, and show that these models also exhibit a certain low-rank bias.

\textbf{Connection to quadratically reparameterized gradient flow in linear regression:}
It has been shown that in linear regression with quadratically reparameterized gradient flow, respectively gradient descent, with vanishingly small initialization converges to a solution, which corresponds to the $\ell_1$-minimizer, see  \cite{vaskevicius2019implicit,woodworth2020kernel,chou2021more,chou2022non}.
This can be interpreted as the commutative version of the problem studied in this paper.
A key insight in this line of research is that gradient flow on the factorized loss function is equivalent to mirror flow with an appropriately chosen Bregman divergence.
However, due to non-commutativity of the matrix multiplication this equivalence does not hold for the problem studied in this paper, see also \cite{li2022implicit}.
Finally, let us mention that in \cite{wu2021implicit} it has been shown that for low-rank matrix recovery, mirror descent equipped with a suitable Bregman divergence and starting from vanishingly random initialization converges to a solution that is vanishingly close to the nuclear norm minimizer.
However, as mentioned above, unlike in the (commutative) vector case, in the non-commutative matrix case it is unclear to which extent mirror descent connects to gradient descent on the factorized loss function.

\noindent\textbf{(Deep) Linear models and Balancing:} 
A variety of works \cite{bartlett2018gradient,arora2018optimization,arora2018convergence,bah2019learning,chou2020gradient,nguegnang2021convergence,tarmoun2021,Min2021}  studied the convergence of gradient flow and gradient descent for deep linear neural networks of the form
\begin{equation*}
 \underset{W_1, W_2, \ldots, W_N}{\min} \  \sum_{i=1}^m \big\Vert W_N \ldots W_2 W_1 x_i - y_i   \big\Vert^2.
 \end{equation*}
While this model cannot directly be compared with the one studied in this paper, the aforementioned papers also rely on the implicit coupling/balancing effect of the gradient descent algorithm. 

Finally, we note that in \cite{wang2021large} it has been observed that in a low-rank matrix factorization model of the form $ \Vert X-VW^T \Vert_F^2 $ gradient descent with a large step size implicitly balances the factors $V_t$ and $W_t$.
At first glance, this may look like a contradiction with the results presented in this paper.
However, note that \cite{wang2021large} assumes such a large step size that even the loss is not monotonically decreasing in the beginning.
Thus, their work operates in a very different regime (which is sometimes referred to as the "Edge of Stability" \cite{cohen2020gradient}).

\noindent\textbf{Non-convex optimization for low-rank matrix recovery in the non-overparameterized scenario:}
A variety of models in statistics, signal processing, and machine learning can be formulated as low-rank matrix recovery problems such as matrix completion \cite{candes_recht_MC,CandesMatrixComp2}, phase retrieval \cite{candes_strohmer,CESV12}, and blind deconvolution \cite{blind_deconvolution,ling2017blind}. Historically, one approach to solving such problems is via lifting techniques in convex relaxations such as nuclear norm minimization \cite{RechtFazel} which was the subject of intense study. We refer to \cite{davenport2016overview,fuchs2022proof} for an overview. 
However, since lifting increases the number of optimization variables, the nuclear norm minimization approach is computationally less efficient as non-convex approaches using a factorized gradient descent approach.
While the literature is too vast to give a complete overview of non-convex optimization for low-rank matrix recovery, in the following, we try to give an account of how we see our work positioned in this field.
For a more complete overview we refer to \cite{chen_overview}.
In recent years, numerous papers have analyzed gradient-descent based methods in low-rank matrix recovery, e.g., in matrix sensing \cite{tu2016low}, matrix completion \cite{xiaodong_matrixcompletion}, phase retrieval \cite{wirtinger_flow,truncated_wirtinger_flow,chen_implicit_regularization}, and blind deconvolution \cite{ling_blind_deconv,ling_demixing}. However, all of these results rely on spectral initialization.
That is, instead of using a random initialization one uses a carefully designed starting matrix as an initialization which is already close to the ground truth solution.
Moreover, note that many of these papers require adding a specific regularization term to enforce balancedness in the asymmetric scenario. 
An exception is \cite{ma2021balancingfree}, which proves convergence of vanilla gradient from spectral initialization (without adding any additional regularization).

Practitioners often use random initialization as it is model-agnostic.
Subsequently, several papers \cite{landscape_phaseretrieval,ge2016matrix_spurious,zhang2019sharp} have analyzed the loss landscape of non-convex formulations and show that the loss landscapes in these cases are benign in the sense that they do not admit spurious local minima and all saddle points have a direction of strict negative curvature.
In particular, those results imply that specialized solvers such as trust region methods, cubic regularization \cite{nesterov2006cubic, nocedal2006trust}, or noisy (stochastic) gradient-based methods \cite{jin2017escape,ge2015escaping,raginsky2017non,zhang2017hitting} can find the global optimum.
However, they do not explain why methods such as plain vanilla gradient descent can find the global optimum.

More recently, \cite{chen_global_convergence} shows that for phase retrieval, gradient descent using random initialization converges to the global optimum with a near-optimal amount of iterations. Moreover, in \cite{lee2022randomly} it has been shown that for rank-one matrix sensing, alternating least squares converges to the ground truth.
However, our understanding of why random initialization works so well in these settings is still very limited. Indeed, to the best of our knowledge, even in the non-overparameterized case where $k=r$, our work is the first one which shows convergence from a random initialization in the asymmetric scenario. 