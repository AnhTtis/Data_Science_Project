%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%

%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
%%%% Small single column format, used for CIE, CSUR, DTRAP, JACM, JDIQ, JEA, JERIC, JETC, PACMCGIT, TAAS, TACCESS, TACO, TALG, TALLIP (formerly TALIP), TCPS, TDSCI, TEAC, TECS, TELO, THRI, TIIS, TIOT, TISSEC, TIST, TKDD, TMIS, TOCE, TOCHI, TOCL, TOCS, TOCT, TODAES, TODS, TOIS, TOIT, TOMACS, TOMM (formerly TOMCCAP), TOMPECS, TOMS, TOPC, TOPLAS, TOPS, TOS, TOSEM, TOSN, TQC, TRETS, TSAS, TSC, TSLP, TWEB.
% \documentclass[acmsmall]{acmart} 

%%%% Large single column format, used for IMWUT, JOCCH, PACMPL, POMACS, TAP, PACMHCI
% \documentclass[acmlarge,screen]{acmart}

%%%% Large double column format, used for TOG
% \documentclass[acmtog, authorversion]{acmart}

%%%% Generic manuscript mode, required for submission
%%%% and peer review
\documentclass[sigconf]{acmart}
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.
%%
%% \BibTeX command to typeset BibTeX logo in the docs

\settopmatter{printacmref=false}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
%\copyrightyear{2023}

\acmYear{2023}

\setcopyright{none}

\acmConference[HRI 2023 Workshop]{2023 ACM/IEEE International Conference on Human-Robot Interaction}{March 13, 2023}{Stockholm, Sweden}

\acmBooktitle{2023 ACM/IEEE International Conference on Human-Robot Interaction (HRI) Workshop on Advancing HRI Research and Benchmarking Through Open-Source Ecosystems, March 13, 2023, Stockholm, Sweden}

\acmDOI{}

\acmISBN{}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
%\title{HRI Benchmarking Scenarios: Physical, Remote and Simulated Modalities in the SciRoc 2021 Competition}
\title{Preserving HRI Capabilities: Physical, Remote and Simulated Modalities in the SciRoc 2021 Competition}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.


\author{Vincenzo Suriani}
%\authornote{Both authors contributed equally to this research.}
\email{{surname}@diag.uniroma1.it}
\orcid{0000-0003-1199-8358}
\author{Daniele Nardi}
\orcid{0000-0001-6606-200X}

%\author{G.K.M. Tobin}
%\authornotemark[1]
%\email{webmaster@marysville-ohio.com}

\affiliation{
  \institution{Department of Computer, Control, and Management Engineering, Sapienza University of Rome}
  \streetaddress{Via Ariosto 25}
  \city{Rome}
  %\state{Ohio}
  \country{Italy}
  \postcode{00185}
}

%\email{webmaster@marysville-ohio.com}
%\orcid{0000-0003-1199-8358}


%\author{Lun Wang}
%\affiliation{%
  %\institution{The Th{\o}rv{\"a}ld Group}
  %\streetaddress{1 Th{\o}rv{\"a}ld Circle}
  %\city{Hekla}
  %\country{Iceland}
%  }
%\email{larst@affiliation.org}



%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Suriani et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  In the last years, robots are moving out of research laboratories to enter everyday life. Competitions aiming at benchmarking the capabilities of a robot in everyday scenarios are useful to make a step forward in this path. In fact, they foster the development of robust architectures capable of solving issues that might occur during the human-robot coexistence in human-shaped scenarios. One of those competitions is \emph{SciRoc} that, in its second edition, proposed new benchmarking environments. In particular, Episode 1 of \emph{SciRoc 2} proposed three different modalities of participation while preserving the Human-Robot Interaction (HRI), being a fundamental benchmarking functionality.  % and this allowed us to analyse the impact of a similar benchmark system on community. \blue{preserve HRI capabilities...}  
  The \textit{Coffee Shop} environment, used to challenge the participating teams, represented an excellent testbed enabling for the benchmarking of different robotics functionalities, but also an exceptional opportunity for proposing novel solutions to guarantee real human-robot interaction procedures despite the Covid-19 pandemic restrictions. The developed software is publicly released.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
    <concept_id>10003120.10003121.10003124.10010392</concept_id>
       <concept_desc>Human-centered computing~Mixed / augmented reality</concept_desc>
       <concept_significance>100</concept_significance>
       </concept>
   <concept>
       <concept_id>10010520.10010553.10010554</concept_id>
       <concept_desc>Computer systems organization~Robotics</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[100]{Human-centered computing~Mixed / augmented reality}
\ccsdesc[300]{Computer systems organization~Robotics}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Human-Robot-Interaction, Robotics, Benchmarking}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
%\begin{teaserfigure}
%  \includegraphics[width=\textwidth]{sampleteaser}
%  \caption{Seattle Mariners at Spring Training, 2010.}
%  \Description{Enjoying the baseball game from the third-base
%  seats. Ichiro Suzuki preparing to bat.}
%  \label{fig:teaser}
%\end{teaserfigure}

%\received{20 February 2007}
%\received[revised]{12 March 2009}
%\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle


%https://www.robot-manipulation.org/hri-2023

\section{Introduction}
\begin{figure}
    \centering
    \includegraphics[width=1.0\columnwidth]{images/real2Sim.jpeg}
    \caption{E01 \emph{Coffee Shop} in two of the proposed modalities: an interaction with the TIAGo robot during an on-site run in Bologna (upper image), and an execution in the corresponding simulated environment (lower image).}
    \label{fig:intro}
\end{figure}

SciRoc is a European project supporting the European Robotics League (ERL) whose aim is to bring ERL tournaments into smart cities and, up to now, this goal has been pursued through the organization of two SciRoc Challenges: in 2019 in Milton Keynes, UK, and in 2021 in Bologna, Italy. 
%In the SciRoc 2 challenge, five episodes have been proposed as scenarios of the smart city: 
The objective of the Smart Cities Robotics Challenge (SciRoc) as a scientific competition is to provide \emph{Task Benchmarks (TBMs)} to teams, which allow for the measurement of performance, and to develop software and Human-Robot interaction (HRI) solutions. %In the range of the second edition of Sciroc (SciRoc 2), robots were expected to execute tasks in different modalities, i.e., physical, remote, and simulated due to the Covid-19 restrictions. 
%This second edition of the competition 
The second edition of Sciroc (SciRoc 2) 
has been structured in 5 \emph{episodes}\footnote{https://sciroc.org/2021-challenge-description/}, each consisting of a task to be performed through addressing specific research challenges.
%\begin{itemize}
%    \item E01: Coffee Shop, in which the robot assists the staff of a coffee shop to take care of their customers.
%    \item E02: Sign Language Generation/Interpretation, to explore the interaction through sign language with deaf people.
%    \item E03: Shopping Cart, where a robot has to interact with a device designed for humans: the shopping cart.
%    \item E04: Delivery of Emergency Equipment, that benchmarks the delivery of medicines to isolated persons.
%    \item E05: Shopping Pick \& Pack, related to picking and placing of products from a storage container on a designated shelf.
%\end{itemize}
In the Coffee Shop Episode (E01), in which the robot assists the staff of a coffee shop to take care of their customers, the robot is required to recognize and report the status of all tables inside the shop, to take orders from customers and to deliver objects to and from the customers’ tables.

The design of E01 episode of SciRoc 2, paid particular attention to HRI, verbal and non-verbal interaction, navigation, object detection and person detection as \emph{Functional Benchmarks (FBM)}. As compared with SciRoc 1, the new challenge of SciRoc 2 addressed the benchmarking of HRI functionality in simulated and real environments. In fact, 
%the TBMs are focused on the assistance that robot brings to the staff of a coffee shop, reporting the status of the tables, taking orders and delivering them to the customers. 
due to the Covid-19 restrictions, E01 has been proposed in three settings: \textit{Physical, i.e., on-site with a real robot; Remote, i.e., remote with the robot on site; Simulated, i.e., (remote) simulated robot in a Gazebo environment}. Fig. \ref{fig:intro} shows a run in the on-site participation and a run in the simulated participation. 
Six teams participated in E01, using all the modalities offered.  This allowed teams to develop a single platform to test and develop the software for the three proposed settings. The multiple modalities of participation, represent a novelty in this kind of challenge especially considering that HRI is a required functionality: verbal and non-verbal interactions were part of each run of the Episode.
To this end, in all the modalities, special attention has been put in preserving \emph{real} interaction. % has been preserved. 
We believe that our setup pushes forward the boundaries in having remote benchmark facilities to standardize the testing procedures of software. 
%By relying on the results of this challenge, we collected the opinions of the participating teams and evaluated the impact of this kind of benchmarking environments on the robotic community. 

\begin{figure}
    \centering
    \includegraphics[width=0.99\columnwidth]{images/ep1_vinc.png}
    \caption{The simulated modality, executed with real persons, with the virtual robot able to switch between the simulated camera and the external webcam and microphone.}
    \label{fig:sim_with_real}
\end{figure}


\section{Related work}
\label{sec:related_work}
Competitions always played a significant role in the research in robotics, allowing to foster the development and deployment of new technologies and creating standard benchmark environments to evaluate the performances of the research approaches \cite{anderson_baltes_cheng_2011,yanco2015analysis,amigoni2015competitions}. In fact, as compared with a laboratory, the competition setting helps to improve the \emph{robustness} of the application and, hence, the reliability of the performance evaluated. 

The European Robotics League (ERL) approach to benchmarking experiments is based on the definition of two separate, but interconnected, benchmarking modalities: (1) \emph{Functionality Benchmarks (FBMs)}, that evaluate the performance of hardware/software modules dedicated to single, specific functionalities; and (2) \emph{Task benchmarks (TBMs)}, that evaluate the performance of integrated robotic systems executing tasks that need the interaction/composition of different functionalities \cite{fontana2017rockin}. The evaluation of the performances can be carried out at different levels, from the single robotic skills to the functionalities and, finally, to the overall TBMs. %for example: \textbf{ESEMPI QUI, TBM that evaluates, FBM... }. 
Among those, for the present work, we are interested in presenting the modalities that allowed to evaluate the Benchmarking during the SciRoc 2021 Competition. %through three different modalities focusing on the adoption of three different modalities. 


%According to Robot Competition Kick Innovation in cognitive systems and robotics (RoCKIn) \footnote{http://rockinrobotchallenge.eu/benchmarking.php}, TBMs represent an evaluation on task-specific criteria that look at the success and quality of task execution. Robocup@Home competition was also designed on the basis of TBMs in which a set of functional abilities correspond to the target robot technical abilities \cite{van2011robocup}. Robocup@Home tasks are usually performed in a kitchen or living room environment, where a high level of natural interaction is required. 


Before the Covid-19 pandemic, few robotic competitions have been held in \emph{remote participation} or \emph{simulated participation}. For instance, Robocup@Home simulation competition was held in 2013. After the Covid-19 pandemic, all robotic competitions have been transformed in \emph{remote} or \emph{simulated} modality. RoboCup Soccer Standard Platform League has been an example of this. In fact, two events have been organized within this league, during the pandemic: the GORE and the SPL RoboCup 2021 Worldwide. The German Open Replacement Event (GORE) 2021 has been organized as follows: all participants stayed at their premises (except for a few people supporting the organization) and were given remote access to a random subset of robots from the pool of a particular competition site for each game \cite{laue2021let}. The remote modality challenge was successful, sometimes, at the expense of maximum performance. A similar setup allowed the other competition, the RoboCup 2021 Standard Platform League (SPL), to play the matches in a distributed fashion with real robot games and remote participation. In fact, six fields with standard size were installed in different geographical locations. Participating teams sent their code to the local hosts, who were able to run the code on their own robots \cite{stone2021robocup}. 

Open Cloud Robot Table Organization Challenge (OCRTOC) 2020 competition provided a simulation environment to test the benchmarks for robotic grasping and manipulation, focusing on object rearrangement scenarios. In \cite{inamura2013development}, researchers present a SIGVerse simulation platform that enables social and embodied interaction with virtual agents. The virtual agent is asked to perform the clean-up task and the cooperative cooking with TBMs focusing on detection, recognition, manipulation, navigation and HRI. 

Our design of E01 episode of SciRoc 2, introduced \emph{HRI}, through verbal and non-verbal interaction, as an FBM. As compared with SciRoc 1, the new challenge of SciRoc 2 addressed the implementation of HRI in simulated and real environments, using the proposed architecture.%, by developing different modalities: physical, remote and simulated.



\iffalse
\section{The SciRoc 2 challenge}

SciRoc is a European project supporting the ERL whose aim is to bring ERL tournaments into smart cities, it has been an integration of a new challenge to ERL and, up to now, it has been pursued through the organization of two SciRoc Challenges: in 2019 in Milton Keynes, UK, and in 2021 in Bologna, Italy. In the SciRoc 2 challenge, in Bologna, five episodes have been proposed as scenarios of the smart city: 
\begin{itemize}
    \item E01: Coffee Shop, in which the robot assists the staff of a coffee shop to take care of their customers.
    \item E02: Sign Language Generation/Interpretation, to explore the interaction through sign language with deaf people.
    \item E03: Shopping Cart, where a robot has to interact with a device designed for humans: the shopping cart.
    \item E04: Delivery of Emergency Equipment, that benchmarks the delivery of medicines to isolated persons.
    \item E05: Shopping Pick \& Pack, related to picking and placing of products from a storage container on a designated shelf.
\end{itemize}
\fi

\section{Scenario and Procedure of E01}
%\label{sec:ep1}

The scenario of E01 of SciRoc 2 is shown in  Fig. \ref{fig:intro}. The robot assisted the staff of an environment resembling a real coffee shop to take care of their customers. %The robot is required to recognise and report the status of all tables inside the shop, to take orders from customers and to deliver objects to and from the customers’ tables.
Both the real and the simulated environments have been designed with a surface of around 70 square meters, containing 6 tables and a counter. The environments comprised people waiting to be served, customers already served, tables that need to be cleaned, and empty tables ready to receive new customers. The items that customers were able to order are among those typically sold by real coffee shops. Fig. \ref{fig:real} shows a picture of the robot in the real environment, ready to reach the customer to take the order while Fig. \ref{fig:simulated} shows the robot in the starting position of the simulated environment, at the beginning of a run.

Inside the environments of E01, the TIAGo\footnote{\url{https://pal-robotics.com/robots/tiago/}} robot from PAL Robotics has been chosen for both the real and the simulated version of E01. %The TIAGo robot has been equipped with a tray for transporting few small objects, and sensors for people/object perception and navigation. 
In the case of the real environment, a \textit{TIAGo} robot has been provided on site to support remote participation and on-site participation of teams without a robot. Remote participation has been allowed through the delivery of a Docker container to be deployed on site. 

In E01, the procedure is the same for all the modalities. The robot is placed at a starting location near the counter, then, the trial begins. The robot starts navigating around the shop and detects the status of all tables, reporting it to the counter. If a table is clean but with customers present at the table, the robot reaches it to accept an order from that table and, then, reports it to the kitchen. The interaction with customers occurs via spoken language. The robot, then, navigates to the counter to collect the items and deliver them to that table. The robot does not require manipulation. Hence, the items are taken by the customers from the robot tray. On each run, the order provided by the counter has one of the items missing or incorrect and the robot must identify and correct the mistake. A run terminates when the robot has delivered the order to the table. A time limit ends the episode if the robot gets stuck.

%TBMs of E01 include: \textit{people perception}, \textit{object perception}, \textit{navigation}, \textit{spoken language} and \textit{human-robot interaction (HRI)} capabilities. 

%Che risultati abbiamo avuto? Come ha funzionato l'interazione in simulazione? Che possiamo dire sulla performance dei team in HRI? Perché la prossimità la valutiamo solo in reale?
%%%%%%%%

%\begin{figure}
%    \centering
%    \includegraphics[width=1.0\columnwidth]{images/modality_simple.png}
%    \caption{The software architecture of the container provided to the participant of the SciRoc Episode 1.}
%    \label{fig:architecture}
%\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.99\columnwidth]{images/modality_complex.png}
    \caption{A schema of the three modalities highlighting the HRI capabilities. The container provided to the participants of E01 is the same for all the modes. In the three cases, the HRI has been executed with real persons, guaranteeing \textit{verbal (V)} and \textit{non-verbal (NV)} interactions.}
    \label{fig:modalities}
\end{figure}

\iffalse
\section{TBM metrics of E01}
In order to succeed in the execution of E01, it is required to handle a set of known objects and unknown customers in the coffee shop. 
%TBMs of E01 include the following main functionalities: \textit{people perception} and \textit{object perception}, but the robot should have other functionalities including \textit{navigation}, \textit{spoken language} and \textit{human-robot interaction (HRI) capabilities}. 

%\textcolor{red}{Among the main required functionalities, the computer vision part is the most crucial. In fact, the first achievement of the challenge can be realized by reporting correctly the status of all the tables. This required navigation capabilities, person detection and object recognition.
%The person detection has been evaluate assigning scores to the number of currently reportate status. Problem arose in the person detection are related to the environment constraints and the consequents occlusions. In fact, tables have been disposed very closed and people can be easily detected on the wrong table; attendee (\textbf{CHECK}!!) placed right outside of the coffee shop fences can be detected as customers, customers placed in inline chairs required to be percepted with a mutual occlusions.}

The evaluation of TBMs of a robot is based on the concept of \textit{performance classes} employed in the ERL competitions. The performance class of a robot is determined by the number of achievements (or goals) that the robot collects during its execution of the assigned task. Within each class, ranking is defined according to the number of penalties collected by the robots belonging to the class \cite{basiri2019benchmarking}. Hence, TBMs metrics of Episode 1 are determined by \emph{achievements}, \emph{penalties} and \emph{disqualifying behaviors}. \emph{Achievements} are steps that the robot is required to do: during the execution of the benchmark, an achievement is assigned to the team. \emph{Penalties}, things that the robot is required to avoid doing: during the execution of the benchmark, a penalty is assigned to the team. \emph{Disqualifying behaviors}, things that the robot absolutely must not do \cite{fontana2017rockin}. The scoring system of E01 has been designed by the technical committee of SciRoc 2, and it covers all the required functionalities. Hence, we list the achievements of E01 and describe the functionalities covered.
\begin{itemize}
    \item The robot correctly reports all the tables that need serving. (This achievement covers the following functionalities: navigation, object detection, person detection, and verbal interaction.)
    \item The robot correctly reports the status of all the tables. (This achievement covers the following functionalities: navigation, objective detection, person detection, and verbal interaction.)
    \item The robot correctly reports the number of customers for all tables with customers. (This achievement covers the following functionalities: navigation and person detection.)
    \item The robot reaches an unserved table and asks for the order. (This achievement covers the following functionalities: navigation, person detection, object detection, verbal and non-verbal interaction.)
    \item The robot correctly understands (through speech) and communicates the order of the customer to the counter. (This achievement covers the verbal interaction.)
    \item The robot correctly recognizes the wrong or missing item and corrects the order. (This achievement covers the following functionalities: object recognition, verbal and non-verbal interaction.)
    \item The robot delivers the order to the right table. (This achievement covers the navigation functionality.) 
\end{itemize}

In the cases of the \emph{on-site with a real robot} and the \emph{remote participation with a real robot} modalities, even the non-verbal interaction (for instance, proxemics between robot and humans \cite{saunderson2019robots}) is part of the evaluated functionalities.
\fi
%\begin{itemize}
 %   \item The robot uses sign language to greet and ask for the order of a deaf-mute person. (This achievement is optional which covers the following functionalities: sign language recognition and sign language execution.) 
%\end{itemize}



%\subsection{Hardware and Implementation}
%In order to standardize the runs of the participant teams, a base software platform have been provided. 
%The overall software architecture have been designed to be part of unique docker image, capable to take advantage of the robot hardware or the computer's one that hosts the simulation. This docker image, developed with PAL Robotics, have been provided with \textit{ROS Melodic}, the set of TIAGo drivers, a complete navigation stack, a text-to-speech synthesizer, a simulated TIAGo and a virtual environment developed within the Gazebo simulator.
%\subsubsection{On-site Setup and Implementation}

%The On-site procedure required the teams to be in person in Bologna with a shared robot provided by the organizers or using their own robot. The TIAGo used was equipped with an Intel i7 processor. From the organizer's perspective, the map of the environment has been provided and images of the coffee shop objects were delivered to teams some weeks before the competition.  TIAGo robot helped in standardize the code base since his software is entirely based on ROS. This allowed to provide simulated testing environment and to simplify the deployment of teams' software that can run on the robot and on laptops connected to the robot.  

%\subsubsection{Remote On-site Setup and Implementation}
%In the remote on-site fashion the organizers helped in deploying the images during each run of the episode. This time, beyond the map, the organizers had to provide also a set of bags of the objects to recognize and a bag taken with the robot navigating in the environment. In fact, the images were not sufficient for a remote participation and, in order to let the teams to train the classifiers, a bag with images and point cloud of the objects have been prepared. The Docker image provided for the On-Site participation has been successfully used even for this online participation. 

%\subsubsection{Remote Simulated Setup and Implementation}
% Even in the simulated modality, the used container is the same of the other modalities. %Here, the set of functionalities provided has been different and there were some modifications with respect to the real setup. 
% For the participant of the simulated competition, the map has been provided in the container and made available months before the competition. Then, the set of objects have been incrementally provided: first, a small set of objects have been provided. Then, more objects have been provided resembling the ones used in the real competition. For the hardware, the competition has been conducted on a workstation equipped with an Intel core i9-9900K, and RTX 2080 GPU. %To emulate

%\subsection{Evaluating and Scoring}
%Evaluation of the performance of a robot is based on the concept of performance classes employed in the European Robotics League competitions. The performance class of a robot is determined by the number of achievements (or goals) that the robot collects during its execution of the assigned task. Within each class, ranking is defined according to the number of penalties collected by the robots belonging to the class.




\section{SciRoc E01 as a Multiplatform Benchmark System}
\label{sec:cooperation}

The SciRoc 2 competition has been organized and hosted during the Covid-19 pandemic. The effort made to cope with the restrictions caused by the pandemic led to novel technical solutions that are described in this section. We have prepared E01 code base to allow teams to use a single Docker container capable of hosting and run the software in the three modalities proposed for the participation in the challenge, as represented in Fig. \ref{fig:modalities}. This container, developed with PAL Robotics, has been provided with \textit{ROS Melodic}, the set of TIAGo drivers, a complete navigation stack, a text-to-speech synthesizer, a simulated TIAGo and a virtual environment developed within the Gazebo simulator. The use of the TIAGo robot helped in standardizing the code base since its software is entirely based on ROS. This allowed us to implement a simulated testing environment and to simplify the deployment of participating teams' software that can run on the robot and on the laptops connected to the robot.  The whole image is publicly released at the following link: \url{https://gitlab.com/competitions4/sciroc/dockers/-/tree/master}.

\paragraph{SciRoc E01 - On-site Participation}
The on-site procedure required the teams to be in person in Bologna at the competition site, with their own robot or the shared TIAGo robot provided by the organizers. %The TIAGo used was equipped with an Intel i7 processor. 
The map of the environment has been provided and the images of the coffee shop items were delivered to teams some weeks before the competition. 
In order to simplify the deployment of the software on the robot, each team could extend the provided container by adding at least: \textit{object recognition}, \textit{person detection}, \textit{dialog management} and \textit{robot behaviors}. The deployment procedure has been carried on by loading the Docker image on the robot before each run. A picture of a run is shown in Fig. \ref{fig:real}.% , have been conducted on the hardware provided by the organization, by deploying the container, thus relieving the teams from all the complications of robot transportation. In this case, the container is deployed on the robot and no extra hardware was required. 
In this case, verbal and non-verbal interactions occurred totally through the onboard sensors of the robot. 
A timelapse of a run of the episode can be seen at the following link: \url{https://youtu.be/UckQEKtYep8}. 

\begin{figure}
    \centering
    \includegraphics[width=1.0\columnwidth]{images/E-1thPvWYAQkswa_cropped.jpeg}
    \caption{E01, on-site with the TIAGo robot, the counter and a customer in the coffee shop at the beginning of a run, in Bologna.}
    \label{fig:real}
\end{figure}


\paragraph{SciRoc E01 - Remote Participation in the Real Scenario}
For remote participation, a laptop has been prepared to host the containers of the deploying from remote. %The laptop allowed to have a better control of the deployment procedure from remote. When the laptop is connected with the robot, the container joins the ROS master of the TIAGo and publishes on the ROS topics of the robot in order to command the real robot.
%In this case, the role of the organizers is necessary to support the participants. 
For the remote participants, beyond the map, the organizers had to provide also a set of bags of the items to be recognized and a \textit{Rosbag} taken with the robot navigating in the environment. 
%In fact, the images were not sufficient for a remote participation and, in order to let the teams to train the classifiers, a \textit{rosbag} with \textit{RGBD} images of the items have been prepared. %In this case, the interaction with customers is fully performed by the robot and the perception devices (e.g., camera, microphones) are only the ones of the robot.
Even in this case, verbal and non-verbal interactions occurred only with the hardware of the robot.

\paragraph{SciRoc E01 - Simulation} 
%For the remote simulated participation, we developed an environment on Gazebo 
The software of the remote simulated participation has been provided with two graphical tools: a Gazebo environment that resembles the real one, as shown in Fig. \ref{fig:simulated}, and an RViz custom interface developed by Pal Robotics.% to manage the advanced localization system.
%When using the software for the remote simulated participation, the user faces a Gazebo environment that resembles the real one, as shown in Fig. \ref{fig:simulated} and a RViz custom interface developed by Pal Robotics to manage the advanced localization system. 
In this environment, the simulated TIAGo has the same capabilities of the real one and some provided ROS services allow to perform automatically the actions over the items that, in the real world, are performed by the bartender and the customers. 
%In this modality, the map has been provided in the container and made available a few months before the competition. Then, the set of items has been incrementally provided. During the official competition, the runs of this modality have been conducted on a workstation equipped with an Intel core i9-9900K, and an Nvidia RTX 2080 GPU. %To emulate
An important feature of this code base is the possibility to preserve the HRI procedure: by taking advantage of the camera and the microphone of the computer that hosts the simulation, we blended the simulation with the real environment and, the robot in the simulation could interact with the real customers at the competition site. In fact, during the runs of the simulation, the robot was able to use the simulated camera to perform navigation and object detection tasks, but it also used the webcam and the microphone of the host computer to interact with customers, as depicted in Fig. \ref{fig:sim_with_real}. Fig. \ref{fig:remote_run} shows a run of the remote simulated modality at the competition site: the external screen shows the simulated scenario while real customers that can interact with the simulated robot are placed in front of the webcam and microphone of the computer, guaranteeing verbal interactions with real customers. %. Hence, in this modality, verbal interactions with real customers have been guaranteed.

\section{Evaluation}
In E01, HRI was one the TBMs functionalities.
The evaluation of TBMs of a robot is based on the concept of \textit{performance classes} employed in the ERL competitions. The performance class of a robot is determined by the number of achievements that the robot collects during its execution of the assigned task. %The list of achievements and functionalities can be found on the official rulebook \footnote{\url{https://docs.google.com/document/d/1R9ZKSNaFtmM2xDar6Bb2Pjg8LaeZmHwBuKIFKKQAWyI}}. 
In all the proposed modalities, verbal interaction with customers has been an essential achievement in order to complete the challenge. In the cases of the \emph{on-site and remote participation with a real robot}, even the non-verbal interaction (for instance, proxemics between robot and humans%\cite{saunderson2019robots}
) has been part of the evaluated functionalities.
In the simulated setting, the structure of the virtual environment did not allow the disposal of non-verbal interaction. Nevertheless, verbal interaction has been the most challenging functionality. %Despite the efforts to have a strong standardization of the hardware and the software platform, in the simulation,
The choice to have a real HRI procedure between the simulated robot and real persons facing the computer, has given rise to have a declared issues with the models of the microphone and the camera. In fact, teams expressed their need to have a standardized hardware for the HRI procedure.

\begin{figure}
    \centering
    \includegraphics[width=1.0\columnwidth]{images/ScreenshotSim.png}
    \caption{E01 Simulated. The figure represents one of the starting configurations of the simulated episode, running in the Gazebo simulator.}
    \label{fig:simulated}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1.0\columnwidth]{images/remote_cropped.jpeg}
    \caption{E01 in a remote simulated run with real customers, placed in front of the small monitor, interacting with the simulated robot shown in the external screen.}
    \label{fig:remote_run}
\end{figure}

\iffalse
\section{Survey}
After the SciRoc 2 competition, we conducted a post-event survey among the participating teams in order to investigate the differences of TBMs and architectures developed for E01 among the different participating modalities. All members of participating teams were invited to fill out the survey accordingly. The survey has been categorized into two macro-sections: functionalities involved in TBMs and software architecture. The first section aimed at investigating the preferences of the participants on the functionalities. The second one was focused on the software that teams developed on the provided container, analyzing the \emph{portability} on different environments and different robotic platforms. The items of the survey were organized as follows. According to the context (i.e., on-site real context, remote context, remote simulated context) they belong to:\\
\textbf{Evaluation of Functionalities involved in TBMs}
\begin{itemize}
\item Which of the main functionalities evaluated in the real/remote/remote simulated scenario were interesting for your research?
\item Which of the main functionalities do you consider to be challenged most in the real/remote/remote simulated scenario?
\item Which functionalities do you consider to be added in a similar real/remote/remote simulated scenario of future competitions?
\item Which functionalities are ready to operate on a different robot in a real scenario?

\end{itemize}
\noindent
\textbf{Software organization}
\begin{itemize}
\item Do you think that the architecture of your software was difficult to develop in the real/simulated  scenario of E01?
\item Do you think that the architecture of your software is ready to operate on a different robot in the same scenario?
\item Do you think that the architecture of your software is robust enough to operate in a different real/simulated scenario?

\end{itemize}

\noindent
\textbf{Willingness to pay for benchmark facilities}
\noindent Finally, some extra questions have been conducted to supervise the willingness of the participants to pay for testing in a similar facility and the benefits that teams received.

\fi

\iffalse
\section{Results and Discussion}
\label{sec:discussion}
%In the post-event survey, we investigate the differences in TMBs and architectures developed of E01 among the different participating modalities. All participating teams have filled out our survey. In total, we collected 16 answers: 4 answers from JEMARO team and 4 answers from Reply team (on-site participation with a real robot); 3 answers from SocRob (remote participation with a real robot); 1 answer from LASR, 1 answer from GRAVASTARS, and 3 answers from SMART (remote simulated participation). In the survey, the teams were asked the following two sets of questions regarding the benchmarks designed for E01.


%\noindent

%\textcolor{blue}{\emph{Q1: Which of the main functionalities evaluated in Episode 1 were interesting for your research?} 
%Regarding the functionalities, \textit{Object recognition} turned out to be the most interesting one for on-site and simulated participation; \textit{Object recognition and person detection} are the most interesting functionalities for remote participation. %In the simulated participation, \textit{object recognition} has been selected as the most interested functionality.

%\textcolor{blue}{\emph{Q2: Which of the main functionalities do you consider to be challenged most in Episode 1?}} 
%\emph{Object recognition} has been selected also as the most challenged functionality for on-site participation; \emph{object recognition} and \emph{person detection} are the most challenged functionalities for remote participation; while \textit{verbal interaction} is the most challenging functionality for simulated participation. Regarding HRI issues, the teams of on-site participation and the team of remote participation have the similar perspectives on the benchmarks of E01. However, the teams of the remote simulated participation addressed only verbal HRI as the most challenging benchmark among all of them. 


%This approves that the teams of on-site participation and the team of remote participation have the similar perspectives on the benchmarks of E01. The teams of simulated participation have a different perspective. In fact, in the remote simulated participation, HRI, in particular, verbal HRI has been the most challenging functionality. %\blue{i team erano più interessati alla object recognition ma hanno trovato più difficolta con lo Spoken }

%\blue{\emph{Q3: Which functionalities do you consider to be added in a similar scenario of future competitions?}}

%Two directions have been suggested by teams for future editions of this competition. The first one is to add the \textit{person recognition} functionality; this can be very useful to improve the customer experience in the coffee shop, as well as the overall HRI capabilities of a robotic system. The second one is the \textit{object grasping and manipulation} that, in the current set of rules, was not part of the robot capabilities. In a similar scenario, the benchmarking of robot manipulation skills can be challenging due to the wide set of shapes and materials of items involved in the competition. %In fact, reducing the gap between simulation and real environment for a successful manipulation can simplify the development of such robotic platforms.    

%\blue{\emph{Q4: Which functionalities are ready to be deployed on a different platform?}}
Subsequently, we surveyed the portability of the developed software. For the majority of the answers, most of the used and developed functionalities are ready to be deployed on different robotic platforms. In fact, among all the functionalities developed by participants, \textit{person detection, object recognition, verbal interaction }and \textit{navigation capabilities} have been selected as ready to be deployed on different platforms. The adoption of a ROS-based container concurs to standardize the robot functionalities and this allows easier portability of part of the software, as demonstrated by the collected answers.  

We surveyed the quality of the software developed by the participating teams, focusing on the robustness of the architecture.
%\blue{\emph{Q5: Do you think that the architecture of your software was difficult to develop in the real/simulated scenario of E1?}}
Here, we can summarize that the experience with the use of the real robot is a key point: teams that were able to train themselves on the TIAGo robot before the competition had the best performances.

Although teams were distributed on all the available modalities, the presence of the simulated environment has been appreciated even by the teams that competed with real robots. In fact, the simulated environment has proved to be a better tool for debugging the software and a reliable platform to replicate critical scenarios. Then, we investigated the perception awareness of the portability among the different modalities. %As one can well imagine, 
Teams that developed on the real robotic platform expressed an high portability through the simulated environment, while in the opposite case, there is not immediate portability due to some hardware constraints that have to be taken into account. Nevertheless, perception modules have been moved from the simulation to the real setup with no particular effort.

Despite the effort to have a strong standardization of the hardware and the software platform, in the simulation, the choice to have a real HRI procedure between the simulated robot and real persons facing the computer, has given rise to some declared issues with the models of the microphone and the camera. In fact, in the survey, teams expressed their will to have a standardized hardware for the HRI procedure. 

Having a dedicated benchmark facilities with an advanced robotic platform and a custom build scenario can be really expensive and hard to achieve. Hence, at the end of the survey, we asked teams about their willingness to pay for testing on-site or remotely on a real scenario. On this question, we collected 9 negative answers and 5 positive. %In case of positive answer, we also asked a possible price to this service and we collected a unique answer of 20 USD/h.

\fi

%\blue{\emph{Do you think that the architecture of your software is robust enough to operate on a different robot in a real/simulated scenario?}}


%\blue{\emph{Do you think that the architecture of your software is robust enough to operate on a different real/simulated scenario?}}





\section{Conclusions% and Lesson Learned
}
\label{sec:conclusion}


%\begin{figure}
%    \centering
%    \includegraphics[width=1\columnwidth]{images/Winners-announced.png}
%    \caption{The }
%    \label{fig:intro}
%\end{figure}
 
The Episode 1 of the SciRoc 2 competition allowed us 
%In the presented work, we recap the E01 of the SciRoc 2 challenge 
%successful organization and technical setup of the 
%that allowed us 
to introduce new modalities to standardize benchmarks for robotic applications in Human-Robot Interaction scenarios. E01 competition provided to participants a standard benchmarking facility, aiming at reducing the gap between the simulation scenario and the real-world scenario in terms of HRI capabilities. In fact, it allowed the deployment of the developed software on different modalities: remotely simulated, and remotely and in-person on a real benchmarking environment created on purpose at the competition site. In all of them, we preserved real human-robot interactions. For the remote simulated setup, we proposed and adopted an architecture that blends the simulated perceptions and the perceptions coming from the real environment. Despite the worldwide restrictions imposed by the Covid-19 pandemic, the developed architecture allowed the hosting of a competition among six teams that took advantage of all the proposed modalities. The competition has been fully decentralized with teams participating remotely by sending their software in a container and making it interact with the customers in presence.
%Such a setup allowed to conduct complex interaction among robot and humans in a challenging scenario like the virtual and real coffee shops created for this event. 
%After the competition, a post-survey has been dispensed to the teams to evaluate the developed functionalities, the proposed software architecture and the desire of the teams to take advantage of similar benchmark facilities.
%
%From the answers of the survey, %\blue{è emerso che... è stato utile a... ha permesso di... la standardizzazione ha aiutato a...}
%we noticed that simulation cannot resolve problems of the real environments, but it is perceived as an intermediate point to reach a final product. Still, simulation is a fundamental tool that have to walk in parallel with the real scenario, even on a dedicated benchmark environment as the one build for E01 of the SciRoc2 challenge.
%We can summarize that the research conducted in robotics competitions is key for advancing and building effective robotic agents capable to face scenarios of everyday life. Competitions, in fact, represent a good balance between research and engineering of novel solutions that force the research community to develop techniques that can actually be implemented and can interact with the real world.
%The software portability is a key element in the development of benchmark facilities and it represents a necessary contribution for any SimToReal application. Furthermore, it is worth to notice that the preservation of HRI capabilities of a system have to be guaranteed in a benchmarking facilities, even when the deployment is performed from remote or in simulated environments. To this end, our proposed architecture made some further steps in the adaptation and benchmarking of verbal HRI procedures in simulation environments thanks to the possibility to interact with real people.
%
%Having a robot at disposal of the teams, have been very useful and it has been very appreciated. In this respect, we investigate the willingness to provide a benchmark environment for the teams. The majority of the teams are not favorable to pay for having a fixed facilities for software benchmarking. Hence, the operating costs have should not weigh on the developers that are the end users of those benchmarking facilities.
%
%The focus of SciRoc is the interaction among humans, autonomous robots and smart cities. It aims at showcasing to the general public how robots can coexist in a public scenario. 
%Moreover, the SciRoc2 challenge..., E01 is one of the example of standardizing benchmarks and...
%E01 of SciRoc 2 presents the solutions and the results proposed that helps in realizing this kind of interaction even during the restrictions of the Covid-19 Pandemic. 
%The solutions and the results reported in the present work helped in realizing this interaction even during the restrictions of the Covid-19 Pandemic, proposing a novel architecture for executing Human-Robot Interactions with a simulated environment, preserving real interactions with humans. 
This pushes forward the boundaries in having remote benchmark facilities to standardize the testing procedures of software, blending real and simulated environments with a shared set of metrics. 


%The solutions and the results proposed in the present work helped in realizing this interaction even during the restrictions of the COVID-19 Pandemic.

%???The Sciroc 1 episode experience has definitely been an success but, still, there is plenty of room to improve. In particular, as a lesson learned, standardization of the hardware have to follow the one of the software for the simulation environment. The choose of the environment objects should not disadvantage anyone and this can be difficult in a competition that involves teams from all over the world. The remote participation have to be supported by adequate tools, balancing the work on the setup performed by the organizers and the work performed by the participants.  

%We strongly think that the research conducted in robotics competitions is key for advancing and building effective robotic agents capable to face scenarios of everyday life. Competitions, and in particular the one discussed in this work, in fact, represent a good balance between research and engineering of novel solutions that force the research community to develop techniques that can actually be implemented and can interact with the real world.

\begin{acks}
We wish to thank PAL Robotics %and, in particular, Daniel Lopez Puig 
for the technical support in the development of the software architecture used %by the participants of
in this competition.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{main}

\end{document}
\endinput
%%
%% End of file `sample-authordraft.tex'.
