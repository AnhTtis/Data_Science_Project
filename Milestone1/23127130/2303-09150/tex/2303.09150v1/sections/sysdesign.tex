\newcommand{\tokenend}{END}
\section{\system: Optimizing guessability and memorability}
\label{sec:design}
To overcome the limitations of \userpp passphrases and system-generated passphrases, we design \system: an automated passphrases generation framework that can generate memorable as well as hard to guess passphrases. To do so, \system uses a constrained generative process by modifying a generative Markov model.  The constraints are based on approximate memorability and guessability metrics that we define.  



\subsection{Memorability of passphrases}
\label{ssec:memorability} 

\noindent In order to improve memorability of system-generated passphrases, we first have to quantify memorability, and to that end, we use the notion of character error rate (CER), which is the rate of error per character while typing the text. Prior works~\cite{leiva2014representatively, mackenzie2002character, kristensson2012performance} noted that CER is widely accepted as a proxy for memorability. 


Leiva et al.~\cite{leiva2014representatively}, however, tried to obtain memorable sentences which are representative of a corpus and are complete.  On the contrary, we want to ensure generating memorable passphrases which might not be complete and might not be representative of all passphrases users can memorize. Thus, the signals for memorability we want can be very different. %

In order to investigate, we used a dataset of 2,230 sentences, each of which has been annotated with the character error rate (CER) determined from a user survey~\cite{kristensson2012performance}. We calculated various parameters for each phrase in the dataset like frequency of occurrence, out of vocabulary words, the average frequency of occurrence, etc. With these data, we find the statistically significant correlation of each feature concerning CER (using pearson product moment correlation coefficient~\cite{lehmann2005testing}) and found three statically significantly correlated signals. 


\paragraph{Unigram probability ($L_1$) of a phrase.} Calculated as the sum of the log of unigram probabilities of the individual words in the phrase. Phrases with a higher $L_1$ are likely to be more common, consisting of frequent words, and hence, easier to memorize \cite{freq}. In fact, $L_1$ has $p \approx 0$ (very high statistical significance) and $r=-0.83$ (very high negative correlation) with respect to CER.

\paragraph{Bigram probability ($L_2$) of consecutive words.} Similar to $L_1$, $L_2$ is calculated as the sum of the log of bigram probabilities of all the consecutive pairs of words in the phrase. Phrases with a higher $L_2$ are more likely to be closer to the natural language and thus it will be easier for a user to remember which is supported by its high statistical significance ($p \approx 0$) and high negative correlation ($r=-0.84$) with CER. 

\medskip
\noindent\textbf{Standard deviation ($\sdchr$) of the number of characters per word.}  Higher variability in the number of characters per word leads to higher processing effort and cognitive load. This is corroborated by the fact that $\sdchr$ has a very high statistical significance ($p<10^{-4}$) and positively correlated concerning CER ($r=0.25$).

Leiva et. al.~\cite{leiva2014representatively}, only considered unigrams to distinguish memorable sentences. %
However, in our work, we included bigram too as identified by our experiment. Higher bigram probability helps maintain syntactic structure, or the ``lexical distinctiveness'' to increase memorability~\cite{danescu2012you}.


Note that, computation of $L_1$ and $L_2$ requires a large corpus. In this work we use the \wiki dataset (\secref{sec:dataset}) for the purpose. The model was then fitted according to a generalized linear regression  with the above mentioned features. This yielded a good fit ($R^2 = 0.70$) for the CER estimate and we computed it for a passphrase $\pp$ as $\cer$($\pp$) = $\alpha_1\cdot\lprob$($\pp$) + $\alpha_2\cdot\lprobbi$($\pp$) + $\alpha_3\cdot\sdchr$($\pp$)
with $\alpha_{1} = -3.42\times 10^{-2}$, $\alpha_{2} = -6.46\times10^{-3}$ and $\alpha_{3} = 1.19\times10^{-4}$.








\subsection{Guessability of passphrases}
\label{ssec:guessability_old} 

\noindent As discussed in \secref{ssec:secandthreat}, the best way to measure the guessability (i.e., security) of a passphrase is to calculate its guess rank, which is, in fact, the estimated number of tries for an adversary to arrive at the correct passphrase. Recall that higher the guess rank, lower the guessability. To calculate the guess rank of each passphrase, we simulate any cracking algorithm with the support of suitable training data ~\cite{Kelley2012, Bonneau2012}. The cracking algorithm will then have a probability of generation associated with each model. We employ a Monte-Carlo simulation ~\cite{dell2015monte} that uses this probability to calculate the guess rank of the passphrase as discussed below.

In the pre-processing step, we generate $n$ passphrases $\{\beta_1, \ldots, \beta_n\}$ from the target model $\model$ along with their estimated probabilities of generation. We sort the probabilities in an array in  descending order as, $A=[\model(\beta_{1})....\model(\beta_{n})]$, and create the rank array $C$, where the $i$-th element is computed as:
\begin{equation}
    C[i]= 
    \begin{cases}
      \ceil{\frac{1}{n \cdot A[i]}}, & \text{if } i = 0 \\
      \ceil{\sum\limits_{j=1}^{i} \frac{1}{n\cdot A[j]}} = C[i-1] + \ceil{\frac{1}{n \cdot A[i]}},& \text{otherwise } \\
    \end{cases}
\end{equation}

A probability $A[i]$ corresponds to a rank $C[i]$, %
thus we estimate guess rank of a passphrase $\alpha$ with the largest $j$
such that $A[j] > \model(\alpha)$  through binary search. The  guess rank is estimated by taking an weighted average of the values of $C[j-1]$ and  $C[j]$. 

The above process allows us to calculate the guess rank 
from any automated cracking algorithm that can generate passphrases along with their probability of generation, which includes most of the current state of the art cracking algorithms ~\cite{dell2015monte}. But research has shown that a professional attacker using a semi-automated cracking process on a huge corpus of passwords can adapt to the dataset and thus is much more efficient than any known fully automated cracking algorithms \cite{realWorldAccuracies}. This idea can be extrapolated to the domain of passphrases and thus using any single model for estimating the guess rank will overestimate the number of guesses to arrive at the correct passphrases as compared to the number of guesses required by a professional adversary in practice.

To resolve this issue, we use the concept of \textit{min auto}, which has been briefly discussed in \secref{ssec:secandthreat}. Here, we take into consideration multiple models which can be used to estimate the guess rank of passphrases and have been trained in suitable training data. Ur et al.~\cite{realWorldAccuracies} have shown that for each password taking the minimum of all guess ranks estimated by the various models is a reasonable approximation to the actual guess rank needed in practice for passwords. Similarly, we extend the idea to passphrases by taking the minimum of $n-$gram word and character models ($2,3-$gram for words and $4,5,6-$gram for character) trained over a huge corpus of passphrases from the corresponding system to be evaluated. Some system-specific models have also been considered to obtain a more accurate approximation of the guess rank (\secref{evalsec}).
Leveraging this guessability and memorability framework we next examine the dataset \system uses for passphrase-generation.




\subsection{Curating a corpus for quantifying memorability and guessability}
\label{sec:dataset}

\noindent Our metric for measuring memorability in~\secref{ssec:memorability} requires a universal corpus and the \system needs a bigram Markov model for the generation of passphrases. We use Wikipedia data as a corpus of human-generated text data. %

\paragraph{Dataset.}   
We used a recent dump of Wikipedia articles\footnote{https://dumps.wikimedia.org/enwiki/latest/} and  used the Wikimedia Pageview API client to obtain the top 5\% articles based on the page view count, aggregated over the last five years. We then clean the data by removing all tags, URL links, and captions, and used case-folding~\cite{chatterjee2016password}. We also remove words with less than three characters, as well as numeric or alphanumeric words. Our final data contained 8,210 articles with over 29 million total words and more than 455 thousand unique words. We refer to this dataset as \wiki, and use it as a universal corpus for CER estimation throughout the paper and also use this corpus to generate
secure and memorable passphrases from \system.

\paragraph{Training bigram Markov model.} Next we trained a bigram Markov model on \wiki data.  We record all word-bigrams and  their frequency of appearance in the dataset. 
We will refer to this model as $\mkmodel$, %
and the log probabilities of unigram and bigrams in the dataset as $\lprob$ and $\lprobbi$. Thus $\lprob(\w) = \log({f_\w\over \sum_{\w}f_\w})$ and $\lprobbi(\w, w') = \log({f_{\w\w'}\over \sum_{\w''}f_{\w\w''}})$ and all logarithms are over base 10.  Note, we also assume $\mkmodel.\nextstr(\w)$ as a function that returns all the words that appear after $\w$ in the corpus. %

\subsection{Tuning guessability and memorability}
\label{ssec:tuning}

We estimate CER and guess rank using a bigram Markov model %
trained on the \wiki dataset in this work for estimating guess rank for the \system passphrases.

\paragraph{Optimizing memorability.}\label{memopt} Since we are trying to generate memorable passphrases, we focus on the syntactic structure, as well as the usage of simpler words, to help increase the memorability of the passphrase.  We thus introduce the generation probability of
a passphrase, based on the various parameters discussed in~\secref{ssec:memorability}. We express CER for a passphrase as $\cer(\pp) = \alpha_1 \cdot \lprob(\pp) + \alpha_2\cdot\lprobbi(\pp) + \alpha_3 \cdot \sdchr(\pp)$, and to generate memorable passphrases we aim to optimize $\cer(\pp)$ by controlling the parameters it depends on $L_1(\pp), L_2(\pp)$ and $\sdchr(\pp)$.
\par
\paragraph{Optimizing guessability.}\label{guessopt} We use the same \wiki corpus to train a bigram Markov model and also to generate the probability distribution of every unigram and bigram the \system uses for generation of passphrases. We then use the Markov model as one of the algorithms used in the estimation of guess rank of the passphrases generated by \system as shown in Section \ref{ssec:guessability_old}. We use the unigram and bigram probabilities for calculating the $\lprob$ and $\lprobbi$ of passphrases for the estimation of CER.

\subsection{Generative model}
\label{ssec:genmodel}

Once we have curated our corpus, and the metrics have been suitably defined, we start generating passphrases. %
Recall that in our case CER is heavily dependent (and linear combination) of its factors. The equation obtained previously for the CER of a passphrase (using \wiki dataset), helps us have a better estimate to optimize our generated sentences. For ensuring high guess rank too, we try to maintain a trade-off between these two metrics to generate syntactic and secure passphrases using a simple idea: high unigram/bigram probabilities ensure high memorability and low guess rank, so choosing the right words with optimum probability might ensure both memorability and security. 



\paragraph{Generation.} We start generating passphrases based on the current word. In subsequent words, we evaluate the whole support based on thresholds. For every state change, we recheck our CER and guess rank estimates to obtain a reasonable choice of word.  For the successful generation of passphrases, we pass the trained Markov model to the \system along with the desired length of a generation, $L$, to generate passphrases adhering to our constraints as discussed below.

We begin with the start token \start, and the first word appended to the string is from the list of words a sentence begins within the corpus provided that is not a stop word. \textit{Stop words} are a set of commonly used words in any language. Some examples in English - the, are, and, over, etc. We do this to ensure less predictability in our passphrases as there is a high probability of the generated passphrase starting with a stop word otherwise.\par

We use a score function modeled on the observation that an approximate CER can be computed incrementally. That is, given a partially generated passphrase $\pp_i = \w_1\ldots\w_i$, one can compute the intermediate CER value using the following equation.  
\begin{equation*}
  \score(w_1\ldots w_{i}) = \alpha_{1}\lprob(w_{i})+\alpha_{2}\lprobbi(w_{i-1},w_{i})
  +\alpha_{3}\sdchr(w_1\ldots w_{i})
\end{equation*}

Similarly, we also know that the guess rank of the generated passphrases is dependent on the bigram probabilities $\lprobbi$ when the Markov model trained on the \wiki corpus is used as the cracking algorithm. Thus we use the following constraint while generating passphrases: $\score(\w_1\ldots \w_i) \le \thetaone$ 
and $\lprobbi(\w_{i-1}, \w_{i}) \le \thetatwo$, where $\thetaone$ and $\thetatwo$ are two system parameters.\vspace{6pt}





Relaxing thresholds on $thetaone$ and $thetatwo$ a lot will make the quality of generation similar to that of a normal Markov model and tightening them might result in a reduced sample space. Keeping this in mind, the fractions can be tinkered around as per need. Even though they have a common factor, $\lprobbi$, between each other, the optimization can be considered independent. %
Since the flow of a sentence is based on qualitative inspection, the thresholds depend on the corpus itself, and thus, can vary depending on the user's need. For example, a corpus with a higher percentage of rare bigrams will automatically generate less memorable sentences, thus requiring us to relax the upper bound for CER and vice versa. We provide a detailed discussion on trade-off and bound of thresholds on $\thetaone$ and $\thetatwo$ in Appendix~\ref{sec:thresholdtradeoff}. 




\newcommand{\getfirstword}{\mathsf{GetFirstWord}}

\begin{figure}[t]
  \centering
  \fpage{0.45}{
  \fontfamily{cmss}
    \underline{$\getfirstword(\mkmodel):$}\\[2pt]
    $\PW \gets \mkmodel.\nextstr(\start)\setminus \sw$\\
    $\w\genfrom{\lprob}\PW$\\
    return $\w$\\[5pt]
    \underline{$\sysgen(\pwlen, \mkmodel)$:}\\[2pt]
    $\w_1\gets \getfirstword(\mkmodel)$\\
    $i \gets 2$\\
    while $i\le \pwlen$ do\\
    \mytab $\PW' \gets\mkmodel.\nextstr(\w_{i-1})$\\
    \mytab if $i = \pwlen$ then\\
    \mytab \mytab $\PW' \gets \PW' \setminus \sw$ \commentNew{Remove stopwords}\\
    \mytab \commentNew{CER and guess rank constraint}\\ 
    \mytab $\PW' \gets \{\w\in\PW' \given \score\left(\w_1\ldots\w\right) \le \theta_1$ and $\lprobbi(\w_{i-1},\w) \le \thetatwo\}$\\
    \mytab if $i=\pwlen$ then \commentNew{Ends in a end symbol}\\
    \mytab \mytab $\PW' \gets \{\w\in\PW' \given \lastword \in \mkmodel.\nextstr(\w) \}$\\
    \mytab $\w_{i} \getsr \PW'$\\
    \mytab if $\w_{i} = \bot$ then \\
    \mytab \mytab$\w_1 \gets  \getfirstword(\mkmodel)$ \commentNew{No passphrase found; restart} \\
    \mytab \mytab $i \gets 1$\\
    \mytab\ $i\gets i$+$1$\\
  return $\w_1\ldots\w_\pwlen$
}
\caption{The \system algorithm. The algorithm generates a passphrase of length $\pwlen$ given a bigram Markov model $\mkmodel$. Here $\sw$ is a set of stop words, and $\start$ and $\lastword$ are the start and end symbols used in the Markov model. Here $\genfrom{\lprob}\PW$ denotes sampling from the support $\PW$ but according to the probability distribution assigned by unigram probabilities (without log); similarly
$\getsr \PW$ denote sampling uniformly randomly from the elements in $\PW$.
}
\label{fig:algorithm}
\vspace{6pt}
\end{figure}




\subsection{The final algorithm, \system{}}
\label{sec:finalalgo}


We introduce \system{}, a step-by-step approach to generate passphrases, under constraints of memorability and guessability, while preserving its syntax and meaning in ~\figref{fig:algorithm}. The actual implementation is an optimized version of the one shown, where we use $O(\log n)$ time for the sampling of the next word, where $n$ is the number of choices, as opposed to the $O(n)$ shown in ~\figref{fig:algorithm}, with the help of some pre-processing.

The algorithm is greedy, and not necessarily optimal. But, replacing the corpus or changing any of the variables can essentially just be a straight swap with the existing one, based on user preference or need, making the algorithmic approach a generalized version of generating optimized passphrases.

\paragraph{Selecting $\thetaone$ and $\thetatwo$.} We try to ensure that \system{} favors rarer bigrams (low $\thetatwo$) while maintaining a low intermediate CER score (low $\thetaone$). We also note that $\score(\cdot)$ function has $\lprobbi$ in it, so we can bound the value of $\score$ given $\thetatwo$. That is to say, if $\lprobbi(\w_{i-1}, \w_i) \le \thetatwo$, then $\thetaone \ge \score(w_1\ldots w_i) \ge \alpha_1\lprob(\w_i) + \alpha_2\thetatwo + \alpha_3\sdchr(\w_1\ldots\w_i)$, because $\alpha_2$ is negative. Thus, for a given $\thetaone$, the value of $\thetatwo$ can be
bounded. $\sdchr$ and $\lprob$ is at least 0, then $\thetaone \ge \alpha_2\thetatwo$, or $0 \ge \thetatwo \ge {\thetaone\over\alpha_2}$.



We chose $\thetatwo$ to be at 80\% of the minimum possible value of $\lprobbi$ (giving the system a leeway of within 20\% of the minimum value), which is -17.4. This will ensure that the generated passphrases contain rare bigrams and thereby high guess rank.  Setting $\thetatwo= 0.8 \times -17.4$, gives us $\thetaone \le 0.5$, as $\alpha_2=-0.00646$. We use these values for generation in the design of~\system.

Our generation is incremental ensuring the invariant of CER (memorability) and guess rank (guessability). An alternative approach would have been generating the whole passphrase of length $\pwlen$, and then checking if the CER and guess rank constraints are met. Intuitively, we can see that the current approach is much more efficient than the alternative and can generate usable passphrases a lot quicker. %
This is further demonstrated below.

\paragraph{Execution time. }\label{sssec:exectime} To evaluate the performance of the final \system algorithm, we compare it with multiple variations and baselines. The baselines we take into consideration are \dice, \mmap, as well as a basic Markov model trained on the \wiki dataset. We also examine the variation of \system where all rejections (according to constraints of $\thetaone$ and $\thetatwo$) take place after the entire passphrase is generated by a Markov model (\system$_{\text{end}}$). This variation can be understood as a system that can create optimal passphrases for the constraints imposed. For reasonable comparisons, we keep the rest of the system parameters the same for both these versions.


After generating 1000 passphrases of equal length distribution across all the systems taken into consideration, we checked their execution time, which includes the pre-processing time as well. \dice, \mmap, and Markov run in 7.85 seconds, 0.33 seconds and 0.05 seconds, respectively. In comparison, \system takes only 0.08 seconds, which is even comparable to a generic Markov model, and much better due to the guarantees offered by the generated passphrases on their memorability and security. Looking further, the execution of the model that can generate the most optimal passphrases for the set constraints, \system$_{\text{end}}$, takes a very significantly larger 810 seconds to complete.%


