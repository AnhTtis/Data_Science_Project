\section{Camera-Calibrated Model}




%


We've demonstrated that our algorithm can generate simultaneous color holograms in simulation. However, experimental holograms frequently do not match the quality of simulations due to mismatch between the physical system and the model used in optimization (Eqs. \ref{eq:SLM_to_field}, \ref{eq:field_to_intensity}). Therefore, to demonstrate simultaneous color experimentally, we need to calibrate the model to the experimental system. 

To do this, we design a model based on our understanding of the system's physics, but we include several learnable parameters representing unknown elements. To fit the parameters, we capture a dataset of SLM patterns and camera captures and use gradient descent to estimate the learnable parameters based on the dataset. Next we explain the model which is summarized in Fig. \ref{fig:ForwardModel}.

\subsection{Learnable Parameters for Offline Calibration} \label{sec:learnable-model}

\paragraph{Lookup Table} 
A key element in our optimization is $\phi_\lambda$ which converts the digital SLM input into the phase coming off the SLM, and it's important that this function accurately matches the behavior of the real SLM. Many commercial SLMs ship with a lookup-table (LUT) describing $\phi_\lambda$; however, the manufacturer LUT is generally only calibrated at a few discrete wavelengths.  These wavelengths may not be accurate for the source used in the experiment. Therefore, we learn a LUT for each color channel as part of the model. Based on a pre-calibration of the LUT using the approach of \citet{yang2015nonlinear}, we observe the LUT is close to linear; we therefore parameterize the LUT with a linear model to encourage physically realistic solutions.

\paragraph{SLM Crosstalk}
SLMs are usually modeled as having a constant phase over each pixel with sharp transitions at boundaries. However, in LCoS SLMs, 
elastic forces in the LC layer prevent sudden spatial variations, and the electic field that drives the pixels changes gradually over space. As a result, LCoS SLMs suffer from crosstalk, also called field-fringing, in which the phase is blurred \cite{apter2004fringing, moser2019model, persson2012reducing}. We model crosstalk with a convolution on the SLM phase. Combined with our linear LUT described above, we can describe the phase off the SLM as
\begin{align} \label{eq:learned-model-01-LUT}
    \phi_\lambda(s) = k_{\text{xt}} * (a_1 \cdot s + a_2)
\end{align}
where $a_1, a_2$ are the learn parameters of the LUT, and $k_{\text{xt}}$ is a learned $5 \times 5$ convolution kernel representing crosstalk. Separate values of these parameters are learned for each color channel.

\paragraph{Propagation with Higher Diffraction Orders}
The discrete pixel structure on the SLM creates higher diffraction orders that are not modeled well with ASM or Fresnel propagation. A physical aperture at the Fourier plane of the SLM can be used to block higher orders. However, accessing the Fourier plane requires a 4$f$ system, which adds significant size to the optical system, reducing the practicality for head-mounted displays. Therefore, we chose to avoid additional lenses after the SLM and instead account for higher orders in the propagation model.

% 
We adapt the higher-order angular spectrum model (HOASM) of \citet{Gopakumar2021UnfilteredDisplays}. 

The zero order diffraction, $G(f_x, f_y)$, and first order diffraction, $G_{\text{1st order}}$, patterns are propagated with ASM to the plane of interest independently. Then the propagated fields are stacked and passed into a U-net, which combines the zero and first orders and returns the image intensity:
\begin{align} 
f_{\text{ASM}}(G, z) &= \mathcal{F}^{-1}\left\{ G \cdot H_{\text{ASM}}(z)\right\} \label{eq:learned-model-03-propagation} \\ 
I_z &= \text{Unet}\left(f_\text{ASM}(G, z), \: f_\text{ASM}(G_{\text{1st order}}, z)\right), \label{eq:learned-model-04-unet}
\end{align}
where $H_{\text{ASM}}(z)$ is the ASM kernel. The U-Net architecture is detailed in the supplement; a separate U-net for each color is learned from the data. The U-Net helps to address any unmodeled aspects of the system that may affect the final hologram quality such as source polarization, SLM curvature, and beam profiles, and the U-net better models superposition of higher orders, allowing for more accurate compensation in SLM pattern optimization. Figure~\ref{fig:Ablation} compares ASM, HOASM, and our modified version with the U-Net.

\section{Implementation}

\paragraph{Experimental Setup}
Our system starts with a fiber coupled RGB source, collimated with a $\SI{400}{\milli\metre}$ lens. The beam is aligned using two mirrors, passes through a linear polarizer and beamsplitter, reflects off the SLM (Holoeye-2.1-Vis-016), and passes through the beamsplitter a second time before directly hitting the color camera sensor with Bayer filter (FLIR GS3-U3-123S6C). As seen in Fig.~\ref{fig:Setup}, there's no bulky 4$f$ system between the SLM and camera sensor, which allows the setup to be compact, but requires modeling of higher diffraction orders. The camera sensor is on a linear motion stage, enabling a range of propagation distances from $z = \SI{80}{\milli\metre}$ to $z = \SI{130}{\milli\metre}$.    

For our source, we use a superluminescent light emitting diode (SLED, Exalos EXC250011-00) rather than a laser due to its lower coherence, which has been demonstrated to reduce speckle in holographic displays \cite{Deng2017CoherenceDisplays}. Although previous work showed state-of-the-art image quality by modeling the larger bandwidth of the SLED as a summation of coherent sources \cite{Peng2021Speckle-freeCalibration}, we found the computational cost to be prohibitively high for our application due to GPU memory constraints. We achieved sufficient image quality while assuming a fully coherent model, potentially due to the U-net which is capable of simulating the additional blur we expect from a partially coherent source.

\paragraph{Calibration Procedure}
We fit the learned parameters in our model (Eqs. \ref{eq:learned-model-01-LUT} - \ref{eq:learned-model-04-unet}) using a dataset captured on the experimental system. We pre-calculate 882 SLM patterns from a personally collected dataset of images using a naive ASM propagation model. Each SLM pattern is captured in $\SI{5}{\milli\metre}$ increments from $z = \SI{90}{\milli\metre}$ to $\SI{120}{\milli\metre}$, resulting in a total of 6174 paired entries. The raw camera data is debayered and an affine transform is applied to align the image with the SLM (see Supplement for details). Model fitting is implemented in Pytorch using an L1 loss function between the model output and camera capture. To account for the camera color balance, we additionally learn a $3 \times 3$ color calibration matrix from the RGB simulated intensities to the captured color image. We train until convergence, which is typically reached between 50 and 100 epochs (2-3 days on Nvidia A6000 GPU).

\paragraph{Hologram Generation}
After training, we can generate holograms by solving Eq. \ref{eq:loss_func_opponent} using the trained model for $I_{z,\lambda}$, implemented with Pytorch's native auto-differentiation. The SLM pattern, $s$, is constrained to the range where the LUT is valid (for example, 0 - 255); the values outside that range are wrapped after every optimization step.  On the Nvidia A6000 GPU, it takes about two minutes to optimize a 2D hologram.  Computation time for the optimization of a 3D hologram scales proportional to the number of depth planes.