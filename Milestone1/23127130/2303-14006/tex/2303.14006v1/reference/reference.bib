// ML Frameworks
@article{tensorflow2015-whitepaper,
    title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
    author={
        Mart\'{i}n~Abadi and
        Ashish~Agarwal and
        Paul~Barham and
        Eugene~Brevdo and
        Zhifeng~Chen and
        Craig~Citro and
        Greg~S.~Corrado and
        Andy~Davis and
        Jeffrey~Dean and
        Matthieu~Devin and
        Sanjay~Ghemawat and
        Ian~Goodfellow and
        Andrew~Harp and
        Geoffrey~Irving and
        Michael~Isard and
        Yangqing Jia and
        Rafal~Jozefowicz and
        Lukasz~Kaiser and
        Manjunath~Kudlur and
        Josh~Levenberg and
        Dandelion~Man\'{e} and
        Rajat~Monga and
        Sherry~Moore and
        Derek~Murray and
        Chris~Olah and
        Mike~Schuster and
        Jonathon~Shlens and
        Benoit~Steiner and
        Ilya~Sutskever and
        Kunal~Talwar and
        Paul~Tucker and
        Vincent~Vanhoucke and
        Vijay~Vasudevan and
        Fernanda~Vi\'{e}gas and
        Oriol~Vinyals and
        Pete~Warden and
        Martin~Wattenberg and
        Martin~Wicke and
        Yuan~Yu and
        Xiaoqiang~Zheng},
    year={2016},
    journal={arXiv:1603.04467 [cs.DC]}
}

// ML Models
@article{Transformer,
    title={{Attention Is All You Need}},
    author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
    journal={Advances in neural information processing systems},
    volume={30},
    year={2017}
}

@misc{Transformer1T,
    author={Anthony Alford},
    title = {{Google Open-Sources Trillion-Parameter AI Language Model Switch Transformer}},
    url={https://www.infoq.com/news/2021/02/google-trillion-parameter-ai},
    year = {2021}
}

@inproceedings{BERT,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)",
    volume = {1},
    month = jun,
    year = "2019",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186"
}

@inproceedings{GPT3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
 pages = {1877--1901},
 title = {Language Models are Few-Shot Learners},
 volume = {33},
 year = {2020}
}


// ML Systems
@misc{tpuv4ici,
    author={Timothy Prickett Morgan},
    title={{Deep Dive on Google's Exascale TPUv4 AI Systems}},
    url={https://www.nextplatform.com/2022/10/11/deep-dive-on-googles-exascale-tpuv4-ai-systems},
    year={2022}
}

@misc{tpuarch,
    author={Google Cloud},
    title={{System Architecture | Cloud TPU}},
    url={https://cloud.google.com/tpu/docs/system-architecture-tpu-vm},
    year={2022}
}

@article{cloudtpuarch,
    title={A Domain-specific Supercomputer for Training Deep Neural Networks},
    author={Jouppi, Norman P and Yoon, Doe Hyun and Kurian, George and Li, Sheng and Patil, Nishant and Laudon, James and Young, Cliff and Patterson, David},
    journal={Communications of the ACM},
    volume={63},
    number={7},
    pages={67--78},
    year={2020}
}

@misc{dgxa100,
    author={NVIDIA},
    title={{NVIDIA DGX A100: The Universal System for AI Infrastructure}},
    url={https://www.nvidia.com/en-us/data-center/dgx-a100/},
    year={2021}
}

@misc{nvlinkBridge,
    author={NVIDIA},
    title={{NVIDIA NVLink High-Speed GPU Interconnect}},
    url={https://www.nvidia.com/en-us/design-visualization/nvlink-bridges},
    year={2022}
}

@misc{MellanoxSHARP,
    author={NVIDIA},
    title={{NVIDIA Mellanox Scalable Hierarchical Aggregation and Reduction Protocol (SHARP)}},
    url={https://docs.mellanox.com/display/sharpv214},
    year={2020}
}

@misc{NIC400G,
    author={NVIDIA},
    title={{ConnectX SmartNICs}},
    url={https://www.nvidia.com/en-in/networking/ethernet-adapters},
    year={2021}
}

@misc{NIC800G,
    title={{800G Specification}},
    url={https://ethernettechnologyconsortium.org/wp-content/uploads/2020/03/800G-Specification_r1.0.pdf},
    author={{Ethernet Technology Consortium}},
    year={2020}
}

@misc{nvidiadgx,
    title={{NVIDIA DGX Systems}},
    url={https://www.nvidia.com/en-us/data-center/dgx-systems},
}

@misc{h100tensorcore,
    title={{NVIDIA H100 Tensor Core GPU Architecture}},
    howpublished="\url{https://resources.nvidia.com/en-us-tensor-core}",
    note="[Online; accessed 27-October-2022]",
    key="H100"
}

@misc{cerebraswhitepaper,
    title={{Cerebras Systems: Achieving Industry Best AI Performance Through A Systems Approach}},
    author={Cerebras},
    url={https://cerebras.net/wp-content/uploads/2021/04/Cerebras-CS-2-Whitepaper.pdf},
    year=2021
}

@misc{CerebrasKeynote,
 author={Sean Lie},
 title={{Thinking Outside the Die: Architecting the ML Accelerator of the Future}},
 url={https://www.microarch.org/micro54/media/lie-keynote.pdf},
 year={2021}
}

@misc{dojo,
    title={{Tesla Dojo Technology: A Guide to Teslaâ€™s Configurable Floating Point Formats {\&} Arithmetic}},
    author={Tesla},
    url={https://tesla-cdn.thron.com/static/MXMU3S_tesla-dojo-technology_1WDVZN.pdf},
    year=2022
}

@misc{intelpontevecchio,
    author={ServeTheHome},
    title={{Intel Architecture Day 2021 Xe HPC Ponte Vecchio Xe Link}},
    url={https://www.servethehome.com/intel-ponte-vecchio-is-a-spaceship-of-a-gpu/intel-architecture-day-2021-xe-hpc-ponte-vecchio-xe-link},
    year={2021}
}

@misc{programmableswitch,
    title={{DCS800: 6.4T PROGRAMMABLE DATA CENTER SWITCH}},
    howpublished="\url{https://www.edge-core.com/productsInfo.php?id=335}",
    note="[Online; accessed 27-October-2022]",
    key="DCS800"
}

// CXL
@misc{cxl,
    author={{CXL Consortium}},
    title={{Compute Express Link (CXL)}},
    url={https://www.computeexpresslink.org},
}

// Simulators
@article{sst,
    title={The structural simulation toolkit},
    author={Rodrigues, Arun F and Hemmert, K Scott and Barrett, Brian W and Kersey, Chad and Oldfield, Ron and Weston, Marlo and Risen, Rolf and Cook, Jeanine and Rosenfeld, Paul and Cooper-Balis, Elliot and others},
    journal={ACM SIGMETRICS Performance Evaluation Review},
    volume={38},
    number={4},
    pages={37--42},
    year={2011}
}

@article{zsim,
    title={{ZSim}: fast and accurate microarchitectural simulation of thousand-core systems},
    author={Sanchez, Daniel and Kozyrakis, Christos},
    journal={ACM SIGARCH Computer Architecture News},
    volume={41},
    number={3},
    pages={475--486},
    year={2013}
}

@inproceedings{mohammad2017dist,
    title={{dist-gem5: Distributed simulation of computer clusters}},
    author={Mohammad, Alian and Darbaz, Umur and Dozsa, Gabor and Diestelhorst, Stephan and Kim, Daehoon and Kim, Nam Sung},
    booktitle={2017 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
    pages={153--162},
    year={2017}
}

@article{samajdar2018scale,
    title={{SCALE-Sim}: Systolic CNN Accelerator Simulator},
    author={Samajdar, Ananda and Zhu, Yuhao and Whatmough, Paul and Mattina, Matthew and Krishna, Tushar},
    journal={arXiv:1811.02883 [cs.DC]},
    year={2018}
}

// ASTRA-sim
@inproceedings{astrasim,
    title={{ASTRA-SIM}: Enabling SW/HW Co-Design Exploration for Distributed DL Training Platforms},
    author={Rashidi, Saeed and Sridharan, Srinivas and Srinivasan, Sudarshan and Krishna, Tushar},
    booktitle={2020 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
    pages={81--92},
    year={2020}
}
@INPROCEEDINGS{waferScaleGPU,
  author={Pal, Saptadeep and Petrisko, Daniel and Tomei, Matthew and Gupta, Puneet and Iyer, Subramanian S. and Kumar, Rakesh},
  booktitle={2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)}, 
  title={Architecting Waferscale Processors - A GPU Case Study}, 
  year={2019},
  pages={250-263}
}

@misc{packageLessProcessing,
  author={Pal, Saptadeep},
  title = {{Scale-Out Packageless Processing}},
  url={https://nanocad.ee.ucla.edu/wp-content/papercite-data/pdf/phdth11.pdf},
  year = {2021}
}
@article{deepflow,
      title={{DeepFlow}: A Cross-Stack Pathfinding Framework for Distributed AI Systems}, 
      author={Newsha Ardalani and Saptadeep Pal and Puneet Gupta},
      year={2022},
      journal={arXiv:2211.03309 [cs.AR]}
}

@misc{AstraSimGithub,
    title = {{ASTRA-SIM: Enabling SW/HW Co-Design Exploration for Distributed DL Training Platforms}},
    url={https://github.com/astra-sim/astra-sim.git},
    year = {2020}
}

@inproceedings{hotiPaper,
    title={Scalable Distributed Training of Recommendation Models: An ASTRA-SIM + NS3 case-study with TCP/IP transport},
    author={Rashidi, Saeed and Shurpali, Pallavi and Sridharan, Srinivas and Hassani, Naader and Mudigere, Dheevatsa and Nair, Krishnakumar and Smelyanski, Misha and Krishna, Tushar},
    booktitle={2020 IEEE Symposium on High-Performance Interconnects (HOTI)},
    pages={33--42},
    year={2020},
}

@INPROCEEDINGS{niket2009garnet,
  author={Agarwal, Niket and Krishna, Tushar and Peh, Li-Shiuan and Jha, Niraj K.},
  booktitle={2009 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)}, 
  title={{GARNET:} A detailed on-chip network model inside a full-system simulator}, 
  year={2009},
  pages={33-42}
}


// Studies with ASTRA-sim
@inproceedings{themis,
    title={Themis: a network bandwidth-aware collective scheduling policy for distributed training of DL models},
    author={Rashidi, Saeed and Won, William and Srinivasan, Sudarshan and Sridharan, Srinivas and Krishna, Tushar},
    booktitle={Proceedings of the 49th International Symposium on Computer Architecture (ISCA)},
    pages={581--596},
    year={2022}
}

@inproceedings{saeedACE,
    title={{Enabling Compute-Communication Overlap in Distributed Deep Learning Training Platforms}},
    author={Rashidi, Saeed and Denton, Matthew and Sridharan, Srinivas and Srinivasan, Sudarshan and Suresh, Amoghavarsha and Nie, Jade and Krishna, Tushar},
    booktitle={Procedings of the 48th International Symposium on Computer Architecture (ISCA)},
    pages={540--553},
    year={2021}
}

@inproceedings{CCstudies,
    title={Impact of RoCE Congestion Control Policies on Distributed Training of DNNs},
    author={Khan, Tarannum and Rashidi, Saeed and Sridharan, Srinivas and Shurpali, Pallavi and Akella, Aditya and Krishna, Tushar},
    booktitle={2022 IEEE Symposium on High-Performance Interconnects (HOTI)},
    pages={39--48},
    year={2022},
}

@misc{libraarxiv,
    title={{Exploring Multi-dimensional Hierarchical Network Topologies for Efficient Distributed Training of Trillion Parameter DL Models}},
    author={Won, William and Rashidi, Saeed and Srinivasan, Sudarshan and Krishna, Tushar},
    journal={arXiv preprint arXiv:2109.11762},
    year={2021}
}

// Simulating Large-scale Parallel Algorithms
@inproceedings{Culler1993logp,
    title={{LogP: Towards a Realistic Model of Parallel Computation}},
    author={Culler, David and Karp, Richard and Patterson, David and Sahay, Abhijit and Schauser, Klaus Erik and Santos, Eunice and Subramonian, Ramesh and Von Eicken, Thorsten},
    booktitle={Proceedings of the fourth ACM SIGPLAN symposium on Principles and practice of parallel programming},
    pages={1--12},
    year={1993}
}

@article{Alexandrov1997loggp,
    title={{LogGP: Incorporating Long Messages into the LogP Model for Parallel Computation}},
    author={Alexandrov, Albert and Ionescu, Mihai F and Schauser, Klaus E and Scheiman, Chris},
    journal={Journal of parallel and distributed computing},
    volume={44},
    number={1},
    pages={71--79},
    year={1997},
    publisher={Elsevier}
}

@inproceedings{Ino2001loggps,
    title={{LogGPS: A Parallel Computational Model for Synchronization Analysis}},
    author={Ino, Fumihiko and Fujimoto, Noriyuki and Hagihara, Kenichi},
    booktitle={Proceedings of the eighth ACM SIGPLAN symposium on Principles and practices of parallel programming},
    pages={133--142},
    year={2001}
}

@inproceedings{Hoefler2010loggopsim,
  title={{LogGOPSim}: Simulating Large-Scale Applications in the LogGOPS Model},
  author={Hoefler, Torsten and Schneider, Timo and Lumsdaine, Andrew},
  booktitle={Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing (HPDC)},
  pages={597--604},
  year={2010}
}

// Distributed Training Simulator
@article{Degomme2017smpi,
    title={Simulating MPI applications: the SMPI approach},
    author={Degomme, Augustin and Legrand, Arnaud and Markomanolis, George S and Quinson, Martin and Stillwell, Mark and Suter, Fr{\'e}d{\'e}ric},
    journal={IEEE Transactions on Parallel and Distributed Systems},
    volume={28},
    number={8},
    pages={2387--2400},
    year={2017}
}

@inproceedings{Robinson2022DTS,
    title = {{DTS}: A Simulator to Estimate the Training Time of Distributed Deep Neural Networks},
    author = {Robinson, Wilfredo J. and  Esposito, Flavio and  Zuluaga, Maria A.},
    booktitle = {The 30th International Symposium on the Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)},
    year = {2022}
}

// Parameter Servers
@article{dean2012large,
    title={Large Scale Distributed Deep Networks},
    author={Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc'aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and others},
    journal={Proceedings of the 26th Annual Conference on Neural Information Processing Systems (NIPS)},
    year={2012}
}

@inproceedings{jiang2020unified,
    title={{A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters}},
    author={Jiang, Yimin and Zhu, Yibo and Lan, Chang and Yi, Bairen and Cui, Yong and Guo, Chuanxiong},
    booktitle={{Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI)}},
    year={2020}
}

@inproceedings{thangakrishnan2020herring,
    title={{Herring: Rethinking the Parameter Server at Scale for the Cloud}},
    author={Thangakrishnan, Indu and Cavdar, Derya and Karakus, Can and Ghai, Piyush and Selivonchyk, Yauheni and Pruce, Cory},
    booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC)},
    year={2020}
}

@inproceedings{recht2011hogwild,
    title={{Hogwild!: A lock-free approach to parallelizing stochastic gradient descent}},
    author={Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
    journal={Proceedings of the Advances in Neural Information Processing Systems (NIPS)},
    year={2011}
}

// AllReduce
@article{sergeev2018horovod,
    title={{Horovod}: fast and easy distributed deep learning in TensorFlow},
    author={Sergeev, Alexander and Del Balso, Mike},
    journal={arXiv:1802.05799 [cs.LG]},
    year={2018}
}

@article{patarasuk2009bandwidth,
    title={{Bandwidth optimal all-reduce algorithms for clusters of workstations}},
    author={Patarasuk, Pitch and Yuan, Xin},
    journal={Journal of Parallel and Distributed Computing},
    volume={69},
    number={2},
    pages={117--124},
    year={2009},
    publisher={Elsevier}
}

// In-switch Collective Communication
@inproceedings{li2019accelerating,
    title={Accelerating Distributed Reinforcement Learning with In-switch Computing},
    author={Li, Youjie and Liu, Iou-Jen and Yuan, Yifan and Chen, Deming and Schwing, Alexander and Huang, Jian},
    booktitle={Proceedings of the 46th International Symposium on Computer Architecture (ISCA)},
    year={2019}
}

@article{gebara2021network,
    title={In-network Aggregation for Shared Machine Learning Clusters},
    author={Gebara, Nadeen and Ghobadi, Manya and Costa, Paolo},
    journal={Proceedings of the 2021 Machine Learning and Systems (MLSys)},
    year={2021}
}

@inproceedings{sapio2021scaling,
    title={Scaling Distributed Machine Learning with In-Network Aggregation},
    author={Sapio, Amedeo and Canini, Marco and Ho, Chen-Yu and Nelson, Jacob and Kalnis, Panos and Kim, Changhoon and Krishnamurthy, Arvind and Moshref, Masoud and Ports, Dan and Richt{\'a}rik, Peter},
    booktitle={Proceedings of the 18th USENIX Symposium on Networked Systems Design and Implementation (NSDI)},
    year={2021}
}

@inproceedings{de2021flare,
    title={{Flare}: Flexible In-Network Allreduce},
    author={De Sensi, Daniele and Di Girolamo, Salvatore and Ashkboos, Saleh and Li, Shigang and Hoefler, Torsten},
    booktitle={Proceedings of the 2021 International Conference for High Performance Computing, Networking, Storage and Analysis (SC)},
    pages={1--16},
    year={2021}
}

@inproceedings{lao2021atp,
    title={{ATP}: In-network Aggregation for Multi-tenant Learning},
    author={Lao, ChonLam and Le, Yanfang and Mahajan, Kshiteej and Chen, Yixi and Wu, Wenfei and Akella, Aditya and Swift, Michael},
    booktitle={Proceedings of the 18th USENIX Symposium on Networked Systems Design and Implementation (NSDI)},
    year={2021}
}

@inproceedings{rashidi2021enabling,
    title={{Enabling Compute-Communication Overlap in Distributed Deep Learning Training Platforms}},
    author={Rashidi, Saeed and Denton, Matthew and Sridharan, Srinivas and Srinivasan, Sudarshan and Suresh, Amoghavarsha and Nie, Jade and Krishna, Tushar},
    booktitle={Proceedings of the 48th International Symposium on Computer Architecture (ISCA)},
    year={2021}
}

@article{pan2022libra,
    title={{Libra}: In-network Gradient Aggregation for Speeding up Distributed Sparse Deep Training},
    author={Pan, Heng and Cui, Penglai and Jia, Ru and Zhang, Penghao and Zhang, Leilei and Yang, Ye and Wu, Jiahao and Dong, Jianbo and Cao, Zheng and Li, Qiang and Liu, Hongqiang Harry and Laurent, Mathy and Xie, Gaogang},
    journal={arXiv:2205.05243 [cs.NI]},
    year={2022}
}

// Mixture-of-Experts (MoE)
@inproceedings{shazeer2017outrageously,
title={ Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
author={Noam Shazeer and *Azalia Mirhoseini and *Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
booktitle={International Conference on Learning Representations (ICLR)},
year={2017},
}

@inproceedings{lepikhin2020gshard,
  author    = {Dmitry Lepikhin and
               HyoukJoong Lee and
               Yuanzhong Xu and
               Dehao Chen and
               Orhan Firat and
               Yanping Huang and
               Maxim Krikun and
               Noam Shazeer and
               Zhifeng Chen},
  title     = {{GShard}: Scaling Giant Models with Conditional Computation and Automatic
               Sharding},
  booktitle = {Proceedings of the 9th International Conference on Learning Representations (ICLR)},
  year      = {2021}
}

@article{rajbhandari2022deepspeed,
    title={{DeepSpeed-MoE}: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale},
    author={Rajbhandari, Samyam and Li, Conglong and Yao, Zhewei and Zhang, Minjia and Aminabadi, Reza Yazdani and Awan, Ammar Ahmad and Rasley, Jeff and He, Yuxiong},
    journal={arXiv:2201.05596 [cs.LG]},
    year={2022}
}

@article{PytorchDistributedDP,
    title={PyTorch Distributed: Experiences on Accelerating Data Parallel Training},
    author={Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and others},
    journal={arXiv:2006.15704 [cs.DC]},
    year={2020}
}

// Pipeline-parallel
@article{harlap2018pipedream,
    title={{PipeDream}: Fast and Efficient Pipeline Parallel DNN Training},
    author={Harlap, Aaron and Narayanan, Deepak and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil and Ganger, Greg and Gibbons, Phil},
    journal={arXiv:1806.03377 [cs.DC]},
    year={2018}
}

@inproceedings{huang2019gpipe,
    title={{GPipe}: Efficient Training of Giant Neural Networks using Pipeline Parallelism},
    author={Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and others},
    booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
    year={2019}
}

@inproceedings{narayanan2021memory,
    title={{Memory-Efficient Pipeline-Parallel DNN Training}},
    author={Narayanan, Deepak and Phanishayee, Amar and Shi, Kaiyu and Chen, Xie and Zaharia, Matei},
    booktitle={International Conference on Machine Learning},
    pages={7937--7947},
    year={2021},
    organization={PMLR}
}

// 3D-Parallel
@misc{deepspeed,
    author={Rangan Majumder and Junhua Wang},
    title={{DeepSpeed: Extreme-scale model training for everyone}},
    url={https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone},
    year={2020}
}

// Model-Parallel
@article{shoeybi2019megatron,
    title={{Megatron-LM}: Training Multi-Billion Parameter Language Models Using Model Parallelism},
    author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
    journal={arXiv:1909.08053 [cs.CL]},
    year={2019}
}

@inproceedings{3DParallelMegatron,
    title={Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM},
    author={Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others},
    booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC)},
    pages={1--15},
    year={2021}
}

// Fully-sharded Data Parallel
@misc{metafsdp,
    title={{Fully Sharded Data Parallel: faster AI training with fewer GPUs}},
    author={Ott, Myle and Shleifer, Sam and Xu, Min and Goyal, Priya and Duval, Quentin and Caggiano, Vittorio},
    url={https://engineering.fb.com/2021/07/15/open-source/fsdp},
    year={2021}
}

@misc{pytorchfsdp1,
    title={{Introducing PyTorch Fully Sharded Data Parallel (FSDP) API}},
    author={Zhao, Yanli and Varma, Rohan and Huang, Chien-Chin and Li, Shen and Xu, Min and Desmaison, Alban},
    howpublished="\url{https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/}",
    note="[Online; accessed 28-June-2022]",
    key="Introducing PyTorch Fully Sharded Data Parallel (FSDP) API",
    year={2022}
}

@misc{pytorchfsdp2,
    title={{Training a 1 Trillion Parameter Model With PyTorch Fully Sharded Data Parallel on AWS}},
    author={Belevich, Pavel and Zhao, Yanli and Li, Shen and Choi, Jessica and Varma, Rohan and Damania, Pritam and Chauhan, Geeta and Yadav, Mahesh and Aquilanti, Pierre-Yves and Ranganatha, Sundar},
    howpublished="\url{https://medium.com/pytorch/training-a-1-trillion-parameter-model-with-pytorch-fully-sharded-data-parallel-on-aws-3ac13aa96cff}",
    note="[Online; accessed 28-June-2022]",
    key="Training a 1 Trillion Parameter Model With PyTorch Fully Sharded Data Parallel on AWS",
    year={2022}
}

// Parallelism Search
@article{flexflow,
    title={Beyond Data and Model Parallelism for Deep Neural Networks},
    author={Jia, Zhihao and Zaharia, Matei and Aiken, Alex},
    journal={Proceedings of the 2019 Conference on Systems and Machine Learning (SysML)},
    year={2019}
}

@inproceedings{unity,
    title={Unity: Accelerating DNN Training Through Joint Optimization of Algebraic Transformations and Parallelization},
    author={Unger, Colin and Jia, Zhihao and Wu, Wei and Lin, Sina and Baines, Mandeep and Narvaez, Carlos Efrain Quintero and Ramakrishnaiah, Vinay and Prajapati, Nirmal and McCormick, Pat and Mohd-Yusof, Jamaludin and others},
    booktitle={Proceedings of the 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI)},
    pages={267--284},
    year={2022}
}

// ZeRO-family
@inproceedings{rajbhandari2020zero,
    title={{ZeRO}: Memory optimizations Toward Training Trillion Parameter Models},
    author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
    booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC)},
    year={2020}
}

@inproceedings{ren2021zero,
    title={{ZeRO-Offload}: Democratizing Billion-Scale Model Training},
    author={Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong},
    booktitle={2021 USENIX Annual Technical Conference (ATC)},
    year={2021}
}

@inproceedings{rajbhandari2021zero,
    title={{ZeRO-Infinity}: Breaking the GPU Memory Wall for Extreme Scale Deep Learning},
    author={Rajbhandari, Samyam and Ruwase, Olatunji and Rasley, Jeff and Smith, Shaden and He, Yuxiong},
    booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC)},
    year={2021}
}

// Graph-based Execution Engine
@misc{executiongraph,
    author={Louis Feng},
    title={{[PyTorch] Integrate Execution Graph Observer into PyTorch Profiler}},
    url={https://github.com/pytorch/pytorch/pull/75358},
    year={2022}
}

// Collective Communication Basics
@article{rabenseifner,
    title={Optimization of Collective Communication Operations in MPICH},
    author={Thakur, Rajeev and Rabenseifner, Rolf and Gropp, William},
    journal={The International Journal of High Performance Computing Applications},
    volume={19},
    number={1},
    pages={49--66},
    year={2005},
}

@misc{nccl,
    title = {{NVIDIA Collective Communication Library (NCCL)}},
    author = {NVIDIA},
    url = {https://developer.nvidia.com/nccl},
    year = {2017}
}

// Collective Communication Algorithms
@inproceedings{dlCollective,
    title={An In-Network Architecture for Accelerating Shared-Memory Multiprocessor Collectives},
    author={Klenk, Benjamin and Jiang, Nan and Thorson, Greg and Dennison, Larry},
    booktitle={Proceedings of the 47th International Symposium on Computer Architecture (ISCA)},
    pages={996--1009},
    year={2020}
}

@inproceedings{ringCollective,
    title={Collective Communication on Architectures That Support Simultaneous Communication over Multiple Links},
    author={Chan, Ernie and Van De Geijn, Robert and Gropp, William and Thakur, Rajeev},
    booktitle={Proceedings of the 11th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP)},
    pages={2--11},
    year={2006}
}

@article{treeAlg,
    title={Bandwidth optimal all-reduce algorithms for clusters of workstations},
    author={Patarasuk, Pitch and Yuan, Xin},
    journal={Journal of Parallel and Distributed Computing},
    volume={69},
    number={2},
    pages={117--124},
    year={2009}
}

@article{blueconnect,
    title={{BlueConnect}: Decomposing All-Reduce for Deep Learning on Heterogeneous Network Hierarchy},
    author={Cho, Minsik and Finkler, Ulrich and Kung, David and Hunter, Hillery},
    journal={Proceedings of the 2019 Conference on Systems and Machine Learning (SysML)},
    pages={241--251},
    year={2019}
}

// Topology
@inproceedings{dragonfly,
    title={Technology-Driven, Highly-Scalable Dragonfly Topology},
    author={Kim, John and Dally, Wiliam J and Scott, Steve and Abts, Dennis},
    booktitle={Procedings of the 35th International Symposium on Computer Architecture (ISCA)},
    pages={77--88},
    year={2008}
}

@inproceedings{topology_alg_design,
  author    = {Xiang Hou and
               Rui Xu and
               Sheng Ma and
               Qiong Wang and
               Wei Jiang and
               Hongyi Lu},
  title     = {Co-designing the Topology/Algorithm to Accelerate Distributed Training},
  booktitle = {2021 {IEEE} Intl. Conf. on Parallel {\&} Distributed Processing
               with Applications, Big Data {\&} Cloud Computing, Sustainable
               Computing {\&} Communications, Social Computing {\&} Networking
               (ISPA/BDCloud/SocialCom/SustainCom)},
  pages     = {1010--1018},
  year      = {2021}
}

// Memory Modeling
@article{harmony,
    title={{Harmony: Overcoming the Hurdles of GPU Memory Capacity to Train Massive DNN Models on Commodity Servers}},
    author={Li, Youjie and Phanishayee, Amar and Murray, Derek and Tarnawski, Jakub and Kim, Nam Sung},
    journal={arXiv preprint arXiv:2202.01306},
    year={2022}
}

@article{comet,
    title={{COMET}: A Comprehensive Cluster Design Methodology for Distributed Deep Learning Training},
    author={Kadiyala, Divya Kiran and Rashidi, Saeed and Heo, Taekyung and Bambhaniya, Abhimanyu Rajeshkumar and Krishna, Tushar and Daglis, Alexandros},
    journal={arXiv:2211.16648 [cs.DC]},
    year={2022}
}

@inproceedings{shazeer2018adafactor,
    title={{Adafactor: Adaptive Learning Rates with Sublinear Memory Cost}},
    author={Shazeer, Noam and Stern, Mitchell},
    booktitle={International Conference on Machine Learning},
    pages={4596--4604},
    year={2018},
    organization={PMLR}
}

// Disaggregated Memory
@inproceedings{comer1990new,
  author    = {Douglas Comer and
               Jim Griffioen},
  title     = {A New Design for Distributed Systems: The Remote Memory Model},
  booktitle = {Proceedings of the Usenix Summer 1990 Technical Conference},
  pages     = {127--136},
  year      = {1990}
}

@INPROCEEDINGS{iftode1993memory,
  author={Iftode, L. and Li, K. and Petersen, K.},
  booktitle={Digest of Papers. Compcon Spring}, 
  title={Memory servers for multicomputers}, 
  year={1993},
  pages={538-547},
  doi={10.1109/CMPCON.1993.289731}
}


@inproceedings{lim2009disaggregated,
    title={Disaggregated Memory for expansion and Sharing in Blade Servers},
    author={Lim, Kevin and Chang, Jichuan and Mudge, Trevor and Ranganathan, Parthasarathy and Reinhardt, Steven K and Wenisch, Thomas F},
    booktitle={Proceedings of the 36th International Symposium on Computer Architecture (ISCA)},
    year={2009},
}

@inproceedings{gu2017efficient,
    title={Efficient Memory Disaggregation with Infiniswap},
    author={Gu, Juncheng and Lee, Youngmoon and Zhang, Yiwen and Chowdhury, Mosharaf and Shin, Kang G},
    booktitle={Proceedings of the 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI)},
    year={2017}
}

@inproceedings{aguilera2017remote,
    title={Remote memory in the age of fast networks},
    author={Aguilera, Marcos K and Amit, Nadav and Calciu, Irina and Deguillard, Xavier and Gandhi, Jayneel and Subrahmanyam, Pratap and Suresh, Lalith and Tati, Kiran and Venkatasubramanian, Rajesh and Wei, Michael},
    booktitle={Proceedings of the 2017 Symposium on Cloud Computing (SOCC)},
    year={2017}
}

@inproceedings{aguilera2018remote,
    title={{Remote Regions: A Simple Abstraction for Remote Memory}},
    author={Aguilera, Marcos K and Amit, Nadav and Calciu, Irina and Deguillard, Xavier and Gandhi, Jayneel and Novakovic, Stanko and Ramanathan, Arun and Subrahmanyam, Pratap and Suresh, Lalith and Tati, Kiran and others},
    booktitle={USENIX Annual Technical Conference (ATC)},
    year={2018}
}

@inproceedings{shan2018legoos,
  title={LegoOS: A Disseminated, Distributed OS for Hardware Resource Disaggregation},
  author={Shan, Yizhou and Huang, Yutong and Chen, Yilun and Zhang, Yiying},
  booktitle={Proceedings of the 13th Symposium on Operating Systems Design and Implementation (OSDI)},
  pages={69--87},
  year={2018}
}

@inproceedings{ruan2020aifm,
    title={{AIFM}: High-Performance, Application-Integrated Far Memory},
    author={Ruan, Zhenyuan and Schwarzkopf, Malte and Aguilera, Marcos K and Belay, Adam},
    booktitle={Proceedings of the 14th Symposium on Operating Systems Design and Implementation (OSDI)},
    year={2020}
}

@inproceedings{lee2021mind,
    title={{Mind: In-network memory management for disaggregated data centers}},
    author={Lee, Seung-seob and Yu, Yanpeng and Tang, Yupeng and Khandelwal, Anurag and Zhong, Lin and Bhattacharjee, Abhishek},
    booktitle={Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles (SOSP)},
    year={2021}
}

@inproceedings{guo2022clio,
    title={{Clio}: A Hardware-Software Co-Designed Disaggregated Memory System},
    author={Guo, Zhiyuan and Shan, Yizhou and Luo, Xuhao and Huang, Yutong and Zhang, Yiying},
    booktitle={Proceedings of the 27th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
    pages={417--433},
    year={2022}
}

// Survey
@article{distributedSurvey,
    title={{A Survey on Distributed Machine Learning}},
    author={Verbraeken, Joost and Wolting, Matthijs and Katzy, Jonathan and Kloppenburg, Jeroen and Verbelen, Tim and Rellermeyer, Jan S},
    journal={ACM Computing Surveys},
    volume={53},
    number={2},
    pages={1--33},
    year={2020}
}

@misc{nvidiav1002017,
    author={NVIDIA},
    title={{NVIDIA V100 Tensor Core GPU}},
    url={https://www.nvidia.com/en-us/data-center/v100},
    year={2017}
}

@misc{oneCCLDoc,
    author = {Intel},
    title = {{Intel} one{CCL} 2021.1 documentation},
    url = {https://docs.oneapi.io/versions/latest/oneccl/env-variables.html},
    year = {2021}
}

@misc{metaparam,
    author={Meta},
    title={{PARAM}},
    url={https://github.com/facebookresearch/param},
    year={2023}
}