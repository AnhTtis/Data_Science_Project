% \vspace{-1mm}
\section{Motivation}
% \vspace{-2mm}
Even though \astrasim framework has allowed brisk navigation of distributed training search space~\cite{themis, saeedACE}, the tool as-is does not meet the demand to capture more complex target platforms. In this section, we motivate the need to extend the \astrasim toolchain to enable modeling state-of-the-art and futuristic training systems.
%
Specifically, we identify three emerging requirements for \astrasim as shown below.
\begin{itemize}
    \item Ability to model arbitrary parallelisms
    \item Ability to model multi-dimensional hierarchical networks
    \item Ability to model memory systems
\end{itemize}

\subsection{Ability to Model Arbitrary Parallelism Strategies}
% \vspace{-1mm}
One of the major limitations of \astrasim is the limited parallelism support. The original \astrasim cannot support complex parallelization strategies such as pipeline parallelism~\cite{harlap2018pipedream, huang2019gpipe} and 3D parallelism~\cite{deepspeed}. There are two reasons for the limitation. First, \astrasim assumes that all NPUs perform the same operation at the same time. While this assumption saves the engineering overhead in implementing data parallelism and model parallelism, it does not allow pipeline parallelism as it requires executing different operations on each NPU at the same time. Second, parallelization strategies are tightly coupled with the frontend implementation of \astrasim and implemented as separate training loops in the workload layer. Parallelization strategies for distributed training are an active area of research~\cite{metafsdp, dean2012large, sergeev2018horovod, harlap2018pipedream, huang2019gpipe, lepikhin2020gshard}, and sometimes several strategies are jointly applied~\cite{rajbhandari2022deepspeed}. Therefore, to evaluate arbitrary parallelism strategies, it is critical to decouple parallelization strategies from the \astrasim implementation.

\subsection{Ability to Model Multi-dimensional Networks}
% \vspace{-1mm}
From the necessity to distribute and synchronize models and data across devices, large-scale distributed training is usually communication-bound~\cite{sapio2021scaling, dlCollective, li2019accelerating}. Therefore, in order to maximize training performance, state-of-the-art systems mix and match a plethora of networking technologies. This usually ends up in a system having multi-dimensional network topologies with heterogeneous bandwidth configurations~\cite{themis}. As an instance, NVIDIA DGX-A100~\cite{dgxa100} exploits a 2-dimensional network topology whose first dimension is NVIDIA NVLink~\cite{nvlinkBridge} then scaled-out using InfiniBand~\cite{MellanoxSHARP} or Ethernet~\cite{NIC400G, NIC800G} technologies. The Google Cloud TPUv4~\cite{tpuarch} leverages a 3D Torus where each inter-core interconnect runs at 448 Gb/s~\cite{tpuv4ici}.

Although \astrasim can, in principle, target multi-dimensional networks, it only supports a limited set of pre-defined network topologies -- 2D and 3D torus.
In order to study different topologies, one must implement both a new network topology in Garnet and its corresponding topology-aware hierarchical collective algorithm, which significantly drags \astrasim's strength of swift distributed training system modeling and performance analysis.

Therefore, it is necessitated to attach a more powerful network backend to the \astrasim framework for rapid design-space exploration of state-of-the-art and futuristic training platforms. It must define a systematic mechanism to represent arbitrary multi-dimensional network topologies at scale. With such notation, the user cam swiftly represent an arbitrary multi-dimensional networks, instead of manually implementing network topology files and their corresponding collective communication algorithms.

\subsection{Ability to Model Emerging Memory Systems}
As DNN model parameters have to be loaded from and stored back to memory, having an efficient memory system is critical in distributed training. To design an efficient memory system, exploring the memory system design space is essential. However, as the original \astrasim does not have detailed memory models, it limits the opportunity to explore the design space. We find that \astrasim should support the following three features. The first feature is the ability to model local HBM memory. \astrasim should have a local memory model that allows how the performance changes as HBM latency and bandwidth vary. This feature allows system and architecture designers to find the optimal local HBM configuration within the same budget. The second feature is the support for memory disaggregation. It is well known that the limited capacity of GPUs is the major bottleneck in large-model training. Model parallelism~\cite{distributedSurvey} and memory optimizations~\cite{rajbhandari2020zero, ren2021zero, rajbhandari2021zero} have been widely adopted to overcome the limitation. While the proposed solutions have been effective in reducing per-GPU memory footprint, they come with critical limitations such as increased computation and communication time. Memory disaggregation is a fundamental solution to overcome the NPU memory capacity limitation by allowing NPUs to access a larger remote memory pool. Emerging interconnects such as CXL~\cite{cxl} accelerate this trend. \astrasim should be able to answer research questions such as the optimal configurations and design for memory disaggregation. The last feature is in-switch collective communication support. With the introduction of memory disaggregation, network switches are introduced in the memory access path of training systems. Performing collective communication in switches is an attractive option to improve the performance of distributed training by reducing communication time~\cite{li2019accelerating, gebara2021network, sapio2021scaling, lao2021atp, de2021flare, pan2022libra}. To find out the performance benefit and trade-offs of in-switch collective communication, \astrasim should support in-switch collective communication.
