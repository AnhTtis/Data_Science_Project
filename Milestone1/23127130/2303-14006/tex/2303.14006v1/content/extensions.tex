\begin{figure}[!t]
\begin{minipage}{\linewidth}
\begin{lstlisting}
eg = None
if args.eg:
    eg_file = f"{out_file_prefix}_eg.json"
    eg = ExecutionGraphObserver()
    eg.register_callback(eg_file)
    eg.start()
...
if eg:
    eg.stop()
    eg.unregister_callback()
\end{lstlisting}
\captionof{lstlisting}{Execution trace collection example~\cite{metaparam}.}
\label{snippet:param-example}
\end{minipage}
\end{figure}


\section{Extensions to \astrasim}
In this section, we introduce the new features we added to \astrasim and describe how they are implemented.
All extensions are released and publicly accessible in the \astrasim repository\footnote{https://github.com/astra-sim/astra-sim}.

\subsection{Graph-based Execution Engine}
\label{sec:eg}
To support arbitrary parallelization strategies, we replace the frontend of \astrasim with a graph-based execution engine. The graph-based execution engine decouples parallelization strategies from the frontend implementation. As the name implies, the graph-based execution engine works on input graphs. The input graphs encode the execution of ML models and their associated parallelization strategies, which can be generated from ML frameworks such as PyTorch~\cite{executiongraph}, TensorFlow~\cite{tensorflow2015-whitepaper}, and FlexFlow~\cite{flexflow}. Code snippet \ref{snippet:param-example} presents how graphs can be collected with PyTorch. PyTorch offers a seamless option for collecting such graphs, which does not require any modifications to the model. The collected graphs are named execution traces (ETs). ETs are fed into the frontend, and the execution engine is responsible for simulating a distributed training system. ETs encode critical information for simulation such as memory access, computation, and communication. Each operation is modeled as a node, and their dependencies are presented as edges as shown in \autoref{fig:AstraSimOverview}(b). In ETs, parallelization strategies are encoded with dependencies. As each NPU has an independent graph-based execution engine, each NPU can run different operations. The engine consumes nodes one by one, and the dependent nodes become ready to be issued when all of their parent nodes are completed. Nodes are completed after a specific delay, and the delay is determined by the node type and metadata. The execution engine continues the simulation until it consumes all nodes.

We define a common format for execution traces, called \astrasim ET, to avoid implementing ET parsers for all different ET types in \astrasim. Instead, we provide a converter from any ET (e.g., PyTorch ET) to \astrasim ET\footnote{Currently, PyTorch and FlexFlow are supported.}. \astrasim ET has three node types: compute, memory, and communication as presented in \autoref{fig:AstraSimOverview}(b). Each node has metadata that is critical for simulating the operation. Compute nodes have the tensor size and the number of floating point operations to perform computation. \astrasim calculates the number of cycles to perform the operation with an internal roofline model. Memory nodes measure the number of cycles to store or load a tensor. Therefore, the nodes have a tensor size as metadata. Communication nodes encode the communication type (collective communication between NPUs or peer-to-peer communication between a pair of NPUs) and the communication size. This information gets translated into a network delay by the underlying system and network layers of \astrasim.

\subsection{Multi-dimensional Network Representation}\label{subsec:MultiDimNetwork}
% \vspace{-1mm}

\insertFigure{MultiDimNetwork}{
(a) Hierarchical topology building blocks: \ring, \fc, and \switch (b) Multi-dimensional network topologies are created by stacking up network building blocks (c) Multi-dimensional hierarchical topology examples, their shape notations, and corresponding distributed training framework.
}{1}{-1.5em}{-0.5em}

In order for users to quickly target arbitrary multi-dimensional network topologies, it is crucial to design a generic notation to represent such multi-dimensional shapes. In this paper, we propose a taxonomy that constructs a multi-dimensional topology by stacking up network building blocks in a hierarchical manner. \autoref{fig:MultiDimNetwork}(a) shows the three network building blocks utilized in this paper: \ring (R), \fc (FC), and \switch (SW). Ring($k$) connects $k$ NPUs in a ring shape (i.e., two connections per every NPU). FullyConnected($k$), on the other hand, offers all-to-all connectivity among all pairs of NPUs. Finally, Switch($k$) connects all $k$ NPUs using an external switch fabric. We chose these three as the network building blocks as they have corresponding well-known topology-aware collective algorithms as summarized in \autoref{table:TopologyAwareCollective}\footnote{Even if the underlying system uses other topologies, they are logically reduced into one of these building blocks due to the collective communication library~\cite{nccl, oneCCLDoc}. This is a unique feature of DL training platforms.}.

\input{table/TopologyAwareCollective.tex}

Multi-dimensional topologies can be generated by assembling these blocks in an arbitrary hierarchical manner, as glimpsed in \autoref{fig:MultiDimNetwork}(b). A handful of example constructed topologies are shown in \autoref{fig:MultiDimNetwork}(c). Ring(4)\_Ring(2) simply denotes a 2D Torus with 8 NPUs in total, where the first dimension is Ring(4) and two such Dim 1 networks are interconnected using Ring(2) topology. Ring(4)\_Switch(2), on the other hand, has the same Dim 1 but planes are being scaled out using an external switch instead. An example 3D topology from \autoref{fig:MultiDimNetwork}(c) is FC(4)\_FC(2)\_FC(2), a fully-populated DragonFly~\cite{dragonfly} topology with 16 NPUs. Ring(4)\_Ring(2)\_Ring(2) is also shown, where the NPU placement is equivalent but topologies connecting them are substituted to \ring, thereby resulting in a 3D torus instead. The number of network dimensions or the building blocks' order is not restricted, thus arbitrary 4D, 5D, $\cdots$, networks can easily be represented using the same notation. Note that each and every example topologies listed in \autoref{fig:MultiDimNetwork}(c) corresponds to some state-of-the-art distributed training platforms, demonstrating the power of our proposed representation in modeling the design space.

With this representation, designing a multi-dimensional topology-aware collective is straightforward and requires minimal modification. As explained in \autoref{subsec:CollectiveCommunication}, multi-rail hierarchical collective algorithms can be run by iteratively running the basic topology-aware collective algorithm on each dimension. Recall that we deliberately chose network building blocks that have known congestion-free collective algorithms. The corresponding topology-aware collective algorithms are listed in \autoref{table:TopologyAwareCollective}. Consequently, collective communications on any arbitrary multi-dimensional network can be run by running these basic algorithms in order and requires no further modification.

\subsection{Analytical Network Backend}
% \vspace{-1mm}

Supporting arbitrary multi-dimensional network topologies shown in \autoref{subsec:MultiDimNetwork}, we implemented a new analytical network backend\footnote{https://github.com/astra-sim/analytical} and ported it to the \astrasim framework. The following points summarize 
why an analytical equation-based network was sufficient for our purpose:
\squishlist
    \item There is a need for first-order design-space exploration (topology shape and BW) of the target system at scale.
    \item As Garnet is most suitable for modeling network-on-chip targets, it is challenging to easily model arbitrary multi-dimensional network topologies, as discussed in \autoref{subsec:MultiDimNetwork}.
    \item Given the scale (1000s of NPUs) of state-of-the-art and futuristic systems and DL models, cycle-level simulation using Garnet is too slow to be practical.
    \item Multi-dimensional topologies run a topology-aware multi-rail hierarchical collective algorithm, which does not create any network congestion. Thanks to this effect, analytical equation-based modeling shows marginal accuracy change over cycle-accurate simulations, and in fact closely matches real system measurements for a small system, as we show later.
\squishend

\begin{figure}[!t]
\begin{minipage}{\linewidth}
\begin{lstlisting}
sim_schedule(delta, callback)
sim_send(msg_size, dest, callback)
sim_recv(msg_size, src, callback)
\end{lstlisting}
\captionof{lstlisting}{Abstract view of example \astrasim frontend NetworkAPI methods~\cite{hotiPaper}.}
\label{snippet:networkAPI}
\end{minipage}
\end{figure}

In order to model a communication between two NPUs, ASTRA-sim frontend delegates the network backend to simulate such a communication and requests the backend to invoke a callback function to notify the transmission is completed. This protocol is defined in the form of NetworkAPI (\autoref{fig:AstraSimOverview}(c)) methods~\cite{hotiPaper}. Several examples of NetworkAPI methods are shown in Snippet \ref{snippet:networkAPI}. Whenever a communication request, such as \texttt{sim\_send} or \texttt{sim\_recv} is initiated, the analytical network backend leverages a simple equation to estimate the communication delay instead of simulating actual network behaviors:

\insertEqNoNum{
    \text{Time} = (\text{LinkLatency} \times \text{Hops}) + \frac{\text{MessageSize}}{\text{LinkBandwidth}} \label{eq:AnalyticalEq}
}
and simply schedules the callback function to be invoked after this delay, unlike the original Garnet backend which runs packet-level cycle-accurate simulations\footnote{
This approach may have limitations when the network contains non-trivial behaviors, such as network congestion or link oversubscription.
Implementing first-order congestion modeling into the analytical backend is our future work.
}. Modeling communication with serialization and link delay is suitable when the communication size is relatively large to be bandwidth-bound (e.g., DLRM and Transformer-1T has 100MB--1GB collectives). The analytical equation could be amended to consider other effects, such as wire propagation delay, as desired. For example, complex system and network optimizations (such as remote memory management or in-switch collective communication) can be captured by equations (\autoref{sec:memory-models}). 

\insertFigure{RealSystemValidation}{
Analytical network backend validation over real system measurements ranging from 64MB--1.5GB \allreduce collectives.
}{0.95}{-3mm}{-5mm}

\textbf{Validation.}
In order to show the accuracy of the analytical network backend, we constructed two real systems and compared various-sized \allreduce running time. The two real systems leverage NCCL v2.4.6~\cite{nccl} which consist of 4 and 16 NVIDIA V100 GPUs~\cite{nvidiav1002017} using a \ring topology with 150 GB/s NVLink~\cite{nvlinkBridge} among GPUs. \autoref{fig:RealSystemValidation} shows the result. We run 64 MB -- 1.5 GB \allreduce and the results suggest the mean error of simulation over all configurations is 5\%.

\textbf{Speedup.}
In order to measure the simulation time improvement, we run a 1MB \allreduce simulation on a 3D Torus with 64 NPUs (4$\times$4$\times$4). On Garnet-based \astrasim, the simulation took 21.42 minutes to finish. For the same configuration, the analytical backend only spent 1.70 seconds, showing 756$\times$ speedup in simulation runtime. Further, the analytical backend supports a 3D Torus with 4K NPUs (16$\times$16$\times$16) in just 3.14 seconds. Nearly three-orders-of-magnitude speedup proves the capabilities of analytical network backend to profile systems of scale at speed.

\subsection{Memory Models}
\label{sec:memory-models}
We add a memory API to \astrasim to support various memory models as shown in \autoref{fig:AstraSimOverview}(d). The goal of memory API is to model various memory systems ranging from local memory to disaggregated memory. Memory API takes tensor location (local or remote), tensor size, memory bandwidth, and memory system design as arguments and returns the number of cycles to load or store a tensor to a memory system. Tensor size and location are encoded in the metadata of ET nodes, and memory bandwidth and system design are given as system configurations. Memory API supports local memory, remote memory, and in-switch collective communication. Memory API determines the model to run based on the tensor location and system parameters.

\begin{figure}[t!]
    \centering
    \begin{subfigure}{0.4\linewidth}
        \center
        \includegraphics[width=\linewidth]{figure/multi_level_switches.pdf}
        \\
        \caption{Multi-level switches.}
        \vspace{0.5em}
    \end{subfigure}
    \hspace{0.5em}
    \begin{subfigure}{0.4\linewidth}
        \center
        \includegraphics[width=\linewidth]{figure/ring.pdf}
        \\
        \caption{Ring.}
        \vspace{0.5em}
    \end{subfigure}
    \\
    \begin{subfigure}{0.4\linewidth}
        \center
        \includegraphics[width=\linewidth]{figure/mesh.pdf}
        \\
        \caption{Mesh.}
    \end{subfigure}
    \hspace{0.5em}
    \begin{subfigure}{0.4\linewidth}
        \center
        \includegraphics[width=\linewidth]{figure/hierarchical.pdf}
        \\
        \caption{Hierarchical.}
    \end{subfigure}
    \\
    \caption{Various memory pool architectures.}
    \label{fig:memory-pool-architectures}
    \vspace{-2em}
\end{figure}

\circled{1} \textbf{Local Memory Model.} This is a simple memory bandwidth model with memory access latency, tensor size, and memory bandwidth as presented in an equation below. Memory access latency and memory bandwidth are given to \astrasim as system parameters, and payload size is encoded in a memory access node of an ET.

% \vspace{-1.2em}
\begin{gather*}
\label{eq:remote-mem-access-time}
\scalebox{.9}{$
    \begin{aligned}[b]
    &(Memory\ Access\ Time) \\
    &= (Memory\ \ Access\ Latency) \\
    &+ (Tensor\ Size) / (Memory\ Bandwidth)
    \end{aligned}$}
\end{gather*}
% \vspace{-1.2em}

\circled{2} \textbf{Remote Memory Model.} This model has the ability to calculate the data transfer time with a disaggregated memory system. In addition to the default parameters for the local memory model, this model takes the disaggregated memory design as a parameter. A disaggregated memory can take any design such as multi-level switches, rings, mesh, and hierarchical as shown in \autoref{fig:memory-pool-architectures}. Different design choices result in different data transfer times because the load on links and the number of network hops change.

The remote memory model calculates the data transfer time for a given disaggregated memory system for given parameters. For ease of explanation, let's assume a system with a hierarchical disaggregated memory. \autoref{fig:remote-memory-model-illustration} illustrates how the remote memory model works for the hierarchical disaggregated memory. There are multiple nodes in the system, and each node has multiple pairs of CPU and GPU. CPUs and GPUs are hierarchically connected to out-node switches, and the out-node switches are connected to multiple remote memory groups. Remote memory groups collectively work as a shared memory pool for all CPUs and GPUs. Let's assume that there are 16 nodes with 16 pairs of CPU and GPU in each node. In total, there are 256 CPUs and 256 GPUs. Additionally, we assume that there are four out-node switches and eight remote memory groups.

If each GPU wants to load a tensor of size \texttt{W} from the remote memory pool, 256\texttt{W} should be loaded from the remote memory pool. As there are eight remote memory modules, each remote memory module will have 32\texttt{W}. As there are four out-node switches and each remote memory group is connected to all out-node switches, each link has to transfer 8\texttt{W}. The data to transfer on the link between an out-node switch and a node is 4\texttt{W} as each node requires 16\texttt{W} (the number of GPUs in a node) and four out-node switches will transfer the same amount of data. Once the loads on links are determined, they are transferred in a pipelined manner with the chunk size unit. The chunk size is the basic transfer unit of the network. \autoref{fig:pipelined-data-transfer} demonstrates how tensors are transferred in a pipelined manner. The meaning of notations is described as the following equations. The total data transfer time is the sum of the critical path, and the length of a stage is determined by the max of data transfer time (arrows) in the stage.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figure/remote_memory_model_illustration.pdf}
    \caption{Remote memory model illustration.}
    \label{fig:remote-memory-model-illustration}
    \vspace{-1em}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figure/pipelined_data_transfer.pdf}
    \caption{Pipelined data transfer.}
    \label{fig:pipelined-data-transfer}
    \vspace{-1em}
\end{figure}

\vspace{-1.2em}
\begin{gather*}
\scalebox{.7}{$
    \begin{aligned}[b]
    &(Number\ of\ Pipeline\ Stages)\\
    &= ((Tensor\ Size)\times(Number\ of\ GPUs))\\
    &/ (Number\ of\ Remote\ Memory\ Groups)\\
    &/ (Number\ of\ Out{\text -}node\ Switches)\\
    &/ (Chunk\ Size)
    \end{aligned}$}
\end{gather*}
\vspace{-1em}
\begin{gather*}
\scalebox{.7}{$
    \begin{aligned}[b]
    &(TX\_rem2outSW)\\
    &= (Chunk\ Size) / (Mem{\text -}side\ Out{\text -}node\ Fabric\ BW)
    \end{aligned}$}
\end{gather*}
\vspace{-1em}
\begin{gather*}
\scalebox{.7}{$
    \begin{aligned}[b]
    &(TX\_outSW2inSW)\\
    &= ((Number\ of\ Remote\ Memory\ Groups)\times(Chunk\ Size))\\
    &/ ((Number\ of\ Nodes)\times(GPU{\text -}side\ Out{\text -}node\ Fabric\ BW))
    \end{aligned}$}
\end{gather*}
\vspace{-1em}
\begin{gather*}
\scalebox{.7}{$
    \begin{aligned}[b]
    &(TX\_inSW2GPU)\\
    &= ((Num\ of\ Rem\ Mem\ Groups)\times(Num\ of\ Out{\text -}node\ SW)\times(Chunk\ Size))\\
    &/ ((Number\ of\ GPUs)\times(In{\text -}node\ Fabric\ BW))
    \end{aligned}$}
\end{gather*}
\vspace{-1.2em}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figure/in_switch_collective_communication_illustration.pdf}
    \caption{In-switch collective communication illustration.}
    \label{fig:in-switch-collective-communication-illustration}
    \vspace{-1em}
\end{figure}

\insertFigureWide{ResultWaferScale}{
(a) Conventional (multi-dimensional) vs. wafer-scale training time breakdown, with and without greedy collective scheduling (Themis) policy. (b) Conventional (scale-out) vs. wafer-scale (on-chip) scalability analysis. Exposed Comm refers to the communication time that is not hidden behind compute time.
}{1}{-6mm}{-5mm}

\circled{3} \textbf{In-switch Collective Communication.} We support in-switch collective communication with an analytical model. With in-switch collective communication, parameters are gathered while being loaded (All-Gather), and sharded while being stored (Reduce-Scatter). The analytical model for in-switch collective communication is similar to the analytical model for remote memory access. However, the only difference is the data size to transfer for each link as parameters are gathered or scattered. Let's take the same example used for the remote memory model. \autoref{fig:in-switch-collective-communication-illustration} illustrates how in-switch collective communication works. In this figure, we assume that each GPU loads a tensor size of \texttt{W}. As there are 256 GPUs, the total size of tensors to load is 256\texttt{W}. The tensors are sharded into eight remote memory groups, and each remote memory group has 32\texttt{W}. As each remote memory group is connected to four out-node switches, each link transfers 8\texttt{W}. Each out-node switch will have  64\texttt{W} in total because eight remote memory groups transfer 8\texttt{W} for all out-node switches. While receiving the weights, they are gathered. After that, the out-node switches are forwarding 64\texttt{W} to each node. As a result, each in-node switch receives 256\texttt{W}, which is the reconstructed weight. In-node switches are responsible for broadcasting the gathered weights to GPUs. Parameters are transferred in a pipelined manner as shown in the remote memory model. In-switch collective communication changes the equations as below.

% \vspace{-1.2em}
\begin{gather*}
\scalebox{.7}{$
    \begin{aligned}[b]
    &(TX\_rem2outSW)\\
    &= (Chunk\ Size) / (Mem{\text -}side\ Out{\text -}node\ Fabric\ BW)
    \end{aligned}$}
\end{gather*}
\vspace{-1.5em}
\begin{gather*}
\scalebox{.7}{$
    \begin{aligned}[b]
    &(TX\_outSW2inSW)\\
    &= ((Number\ of\ Remote\ Memory\ Groups)\times(Chunk\ Size))\\
    &/ (GPU{\text -}side\ Out{\text -}node\ Fabric\ BW)
    \end{aligned}$}
\end{gather*}
\vspace{-1.5em}
\begin{gather*}
\scalebox{.7}{$
    \begin{aligned}[b]
    &(TX\_inSW2GPU)\\
    &= ((Num\ of\ Rem\ Mem\ Groups)\times(Num\ of\ Out{\text -}node\ SW)\times(Chunk\ Size))\\
    &/ (In{\text -}node\ Fabric\ BW)
    \end{aligned}$}
\end{gather*}
%\vspace{-3em}
