% \vspace{-1mm}
\section{Conclusion}
% \vspace{-2mm}
In this paper, we motivate the need to swiftly model and profile state-of-the-art and emerging training platforms running large DL models. We enhance the capabilities of \astrasim to enable capturing arbitrary parallelization strategies and training loops, supporting multi-dimensional network topologies, and representing complex memory systems. Using the framework, we run a comprehensive end-to-end, full-stack co-design space exploration of distributed training. With the ability to quickly navigate the complex design space of distributed training, this can give meaningful first-order insights to system designers and assist them in building futuristic training platforms at scale.
