\section{Introduction}\label{sec:Introduction}

The rapid growth in computation and memory requirement for Deep Neural Network (DNN) models is far greater than the performance and capacity scale of a single Neural Processing Unit (NPU, such as GPU or TPU). As an example, going from BERT~\cite{BERT} model to GPT-3~\cite{GPT3}, over the course of two years, requires 1800$\times$ more computation to train the model~\cite{CerebrasKeynote}. We have now reached the era of trillion parameter models~\cite{Transformer1T} that require 10's of terabytes of memory and zeta floating point  operations to train a model~\cite{CerebrasKeynote,Transformer1T}. Despite efforts to reduce the overhead of large model training on the workload side~\cite{shazeer2017outrageously}, big model training still remains challenging from the systems perspective~\cite{metafsdp}. Hence, distributed training is an inevitable option to keep up with the pace of increased resource requirements of DNN and Deep Learning (DL) training.

Designing an efficient distributed training system is challenging as there are many design choices such as parallelization strategies, NPU performance, NPU memory bandwidth, network topology, network bandwidth, and scheduling policies. Moreover, these design choices are interdependent, requiring the co-design of hardware and software for training platforms.
%
\astrasim~\cite{astrasim, AstraSimGithub} is an existing open-source infrastructure (originally developed by Georgia Tech, Intel and Meta). \astrasim aims to model the complete SW/HW co-design stack of distributed training systems, shown in \autoref{fig:AstraSimOverview}(a). It captures different aspects of distributed training platforms via three abstraction layers: (i) workload, (ii) system, and (iii) network. The workload layer implements the training loop (i.e., the DNN model, its parallelization strategy - data-parallel, model-parallel, etc., compute/communication ordering). The system layer provides various collective communication algorithm implementations (e.g., All-Reduce, All-to-All) and also manages pipelining and scheduling of communication operations. Finally, the networking layer models the HW/SW components of the network and simulates the traffic issued by the system layer.

\astrasim is a promising tool for exploring the design space of distributed training systems and has been leveraged by several recent works~\cite{themis,saeedACE, comet,hotiPaper,CCstudies,topology_alg_design}.
However, in this work, we identify limitations in \astrasim that restrict it from supporting arbitrary parallelism strategies, networks, and memory models. This comes from the rapidly changing SW/HW landscapes for DNN training as we describe next.

On the software end, there has been a growing interest in new parallelism strategies, both hand-designed such as 3D-parallelism~\cite{3DParallelMegatron,deepspeed}, FSDP~\cite{metafsdp}, ZeRO~\cite{rajbhandari2020zero}, expert parallelism~\cite{rajbhandari2022deepspeed} and discovered~\cite{flexflow, unity}. These strategies enable the training of large models, splitting datasets, parameters and optimizer state, while optimizing for communication~\cite{sapio2021scaling, li2019accelerating}. \astrasim did not have a strong motivation to support arbitrary parallelism when it was proposed as there were a handful of parallelism strategies such as data parallelism~\cite{PytorchDistributedDP}, model parallelism, and hybrid~\cite{shoeybi2019megatron}.

\insertFigureWide{AstraSimOverview}{
Overview of Proposed Infrastructure for Modeling Next-generation Training platforms.
The components extended in \astrasim2.0 from the original \astrasim to model emerging platforms are marked in bold.
}{0.98}{-2mm}{-5mm}

The hardware landscape for distributed training has been evolving rapidly as well. State-of-the-art systems extensively deploy multi-dimensional network topologies with hierarchical bandwidths to interconnect NPUs~\cite{nvidiadgx, dgxa100, tpuarch, cloudtpuarch, intelpontevecchio}. This is because increasing the aggregated network BW per NPU through a single dimension is fundamentally limited by the link technology the network is leveraging (e.g., current NVLink~\cite{nvlinkBridge} offers up to 450 GB/s). Naively scaling out through NIC is also not practical due to engineering limitations such as dollar-cost, power, and thermal problems. Meanwhile, wafer-scale systems~\cite{cerebraswhitepaper, dojo} tackle the communication problem by fabricating NPU chiplets on a large-wafer with low-dimensional, high on-chip networking, then scaling out such wafers using NICs. In order to study these technology-driven network landscapes, there is a need for a mechanism to represent and study arbitrary multi-dimensional topologies at scale, with different shapes and BW configurations.
\astrasim natively uses the Garnet simulator~\cite{niket2009garnet} from gem5 as its network layer, which has limitations in modeling such platforms.

Memory disaggregation, which allows GPUs to access a larger remote memory pool, is another
promising HW solution to overcome the limited GPU memory capacity per node. Although the concept has been studied for several decades~\cite{comer1990new, iftode1993memory, lim2009disaggregated, gu2017efficient, aguilera2017remote, shan2018legoos, ruan2020aifm, guo2022clio}, the network and memory did not support memory disaggregation. Motivated by the need for memory disaggregation, the computing industry is now building a framework with a new network technology, compute express link (CXL)~\cite{cxl}.
As distributed training systems will benefit from disaggregated memory, there is a strong need for exploring this design space.
\astrasim uses a simple BW number to model memory and cannot capture this complex design-space.

In this work, we address the aforementioned limitations of \astrasim and enhance it via three novel features, as shown in \autoref{fig:AstraSimOverview}(b)-(d): (i) arbitrary parallelism support, (ii) hierarchical network support, and (iii) memory model support.
We add arbitrary parallelism support by encoding parallelism strategies as execution traces and developing a parser to translate these into compute and communication tasks with dependencies.
For network support, we developed a taxonomy to define hierarchical topologies and created an analytical model to estimate performance when running a topology-aware collective over the physical topology. For the memory models,
we augment \astrasim
with the ability to model local (e.g., HBM) and networked remote (pooled) memories.

Using these enhancements, we present case studies to deliver key insights about future platforms.
We compared conventional multi-dimensional and wafer-scale systems and found that with appropriate collective scheduling and parallelization strategy designs, conventional systems can match wafer-scale systems' performance, whereas wafer-scale shows up to 2.51$\times$ better collective time when scaled.
We also compared disaggregated memory architectures and found that communication time dominates in training a Mixture-of-Experts (MoE) model, and identify configurations that can hide communication time to provide 4.6x speedup over a baseline Zero-Infinity~\cite{rajbhandari2021zero}.