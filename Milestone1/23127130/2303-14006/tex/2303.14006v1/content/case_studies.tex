\section{Case Studies}
% \vspace{-2mm}
In this section, we run comprehensive case studies showcasing the extended capabilities of ASTRA-sim and provide meaningful insights regarding the design space. For all our experiments, we assumed NPU compute power of 234 TFLOPS observed from the measurements of an A100 GPU~\cite{dgxa100}.

\subsection{Conventional System vs Wafer-scale System}
% \vspace{-1mm}

Wafer-scale systems feature a number of NPUs (on a wafer) connected using low-dimensional but high-BW on-chip (on-wafer) interconnection networks~\cite{packageLessProcessing,cerebraswhitepaper, dojo}. Meanwhile, conventional systems~\cite{dgxa100,cloudtpuarch} have multi-dimensional hierarchical topologies with various networking techniques including on-chip, scale-up, and scale-out (NIC). We compare the two distinct approaches by abstracting these systems. Target experimental topologies with 512 NPUs are summarized in \autoref{table:TopologySetup}. For wafer-scale proxy, we create three 1D topologies with 300, 500, and 600 GB/s on-wafer BW (W-1D), and a 2D topology with 250\_250 GB/s BW (W-2D) to model futuristic wafer systems~\cite{packageLessProcessing,waferScaleGPU}. For conventional systems, we created 3D and 4D topologies (Conv-3D and Conv-4D) using on-chip, scale-up, and scale-out interconnections borrowed from~\cite{dgxa100,nvidiadgx,themis}. Target distributed training workloads and their characteristics are also summarized in \autoref{table:WorkloadSetup}.
\input{table/TopologySetup.tex}

\input{table/WorkloadSetup.tex}

\subsubsection{\textbf{Impact of Scheduling}}
Normalized runtimes of a single 1GB \allreduce as well as real workloads are shown in \autoref{fig:ResultWaferScale}. When a topology is multi-dimensional, complex behaviors like pipelining bubbles or unbalanced network BW result in low BW resource utilization and sub-optimal performance~\cite{themis}. Having only one dimension, \textit{\textbf{W-1D yielded the overall best performance}}. However, if you specifically compare W-1D-350 and Conv-4D (600GBps/NPU), \textit{\textbf{Conv-4D is driving more BW/NPU, showing better performance}} despite being multi-dimensional.
Next, we study the impact of scheduling. Themis is a greedy scheduling policy for collectives that aims to balance the load across multiple dimensions to achieve near-optimal BW utilization~\cite{themis}. W-1D topologies, already being only 1D, show no gain from smart scheduling as shown in \autoref{fig:ResultWaferScale}(a). However, W-2D, Conv-3D, and Conv-4D, being \textit{\textbf{multi-dimensional, heavily benefit from Themis scheduler}}. It is worth noting that for single \allreduce and \dlrm, \textit{\textbf{conventional systems with Themis scheduler shows identical results compared to its corresponding wafer-scale systems with equivalent BW/NPU}}. Considering the complexity and cost to build a system on a single wafer, such results glimpse the possible advantage of using the conventional hierarchical approach in performance-per-cost aspects. Meanwhile, for \gpt and \tlarge, wafer-scale systems still maintained better training time. For hybrid parallelism on conventional systems, MP and DP spans over some (and not every) dimensions and utilize only those BW, whereas for wafer-scale every communication runs on full on-wafer BW. This emphasizes the importance of \textit{\textbf{appropriate parallelization strategies and the need to co-design them with underlying topologies}} for conventional hierarchical systems.

\subsubsection{\textbf{Impact of Scaling using Wafer-scale Systems}}
\input{table/DimMessageSize.tex}
Traditional systems scale the infrastructure by scale-out approach, i.e., attach more nodes to the last-dim NICs. On the contrary, wafer-scale technologies let the framework scale up the system, i.e., increasing the number of NPUs on-chip (Dim 1) while maintaining the number of scale-out nodes equally. Measuring the impact, we take the Conv-4D topology from \autoref{table:TopologySetup}, set the on-chip (i.e., Dim 1) BW to 1,000GB/s to model wafer-scale systems~\cite{packageLessProcessing,waferScaleGPU}, and set it as a baseline. Then, we scale the platform up to 4K nodes and measured the 1GB \allreduce time. The results are shown in \autoref{table:DimMessageSize}. \textit{\textbf{Conventional scale-out increases the Dim 4 (NIC) message size, but the impact was marginal}}, thereby showing identical collective time. \textit{\textbf{Scaling over the wafer, however, significantly increased on-wafer (Dim 1) communication size while dramatically cutting down other dimensions' load}}. As far as the system has enough on-wafer BW, collective time decreases due to such an effect, showing an up to 2.51$\times$ speedup over the corresponding scale-out mechanism. \textit{\textbf{Once the on-wafer dimension becomes the bottleneck, the collective time starts to bounce and increase again}} as can be seen from the 16\_8\_8\_4 system. End-to-end training time breakdown of \gpt and \tlarge is also shown in \autoref{fig:ResultWaferScale}(b), showing the equivalent trend in the end-to-end regime.

\subsection{Comparing Disaggregated Memory Systems}
% \vspace{-1mm}

In this case study, we compare the performance of two disaggregated memory systems: ZeRO-Infinity~\cite{rajbhandari2021zero} and the hierarchical memory pool (\texttt{HierMem}) presented in \autoref{sec:memory-models}. We compare the performance of the disaggregated memory systems because the latest model sizes already exceed the memory capacity of GPUs available in the market. ZeRO-Infinity is chosen as a baseline system as it is proposed as an effective solution to overcome the limited memory capacity. ZeRO-Infinity is a nascent form of memory disaggregation where each GPU can utilize CPU memory and NVMe in addition to its local HBM memory. \autoref{fig:ZeRO_Infinity_system_design} presents the system architecture of ZeRO-Infinity. While ZeRO-Infinity has an advantage in terms of its availability in commodity servers, it does not allow having an arbitrary number of remote memory groups. In other words, it cannot enjoy the major benefit of memory disaggregation, which is cost reduction by eliminating memory underutilization. On the other hand, \texttt{HierMem} can have an arbitrary number of remote memory groups. System parameters for the baseline \texttt{HierMem} configuration are presented in \autoref{tab:case-study-disagg-mem-system-configurations}. The values for the baseline configuration are determined based on the latest GPU performance and network bandwidth of commodity servers.



\insertFigure{ZeRO_Infinity_system_design}{
ZeRO-Infinity system architecture.
}{0.75}{-2mm}{-1mm}

\begin{table}[t!]
    \center
    \caption{Disaggregated memory system configurations}
    \scalebox{0.8}{
    \begin{tabular}{|l|r|r|r|}
    \hline
                                               & \multicolumn{1}{c|}{\textbf{ZeRO-Infinity}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}HierMem\\ (Baseline)\end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}HierMem\\ (Opt)\end{tabular}}} \\ \hline
    \textbf{GPU Peak Perf (TFLOPS)}            & 2048                                        & 2048                                                                                       & 2048                                                                                  \\ \hline
    \textbf{GPU Local HBM BW (GB/sec)}         & 4096                                        & 4096                                                                                       & 4096                                                                                  \\ \hline
    \textbf{In-node Pooled Fabric BW (GB/sec)} & -                                           & 256                                                                                        & \textbf{512}                                                                          \\ \hline
    \textbf{Num of Out-node Switches}          & -                                           & 16                                                                                         & 16                                                                                    \\ \hline
    \textbf{Num of Remote Memory Groups}       & 256                                         & 256                                                                                        & 256                                                                                   \\ \hline
    \textbf{Remote Mem Group BW (GB/sec)}      & 100                                         & 100                                                                                        & \textbf{500}                                                                          \\ \hline
    \end{tabular}}
    \label{tab:case-study-disagg-mem-system-configurations}
    \vspace{-1.3em}
\end{table}



To compare the performance of the systems, we run a training task for a mixture-of-experts (MoE) model with 1 trillion parameters~\cite{rajbhandari2022deepspeed}.
\autoref{fig:execution_time_breakdown_comparison} presents the execution time breakdown.
The execution time of a training task can be broken down into five components: compute time, exposed local memory access time, exposed remote memory access time, exposed communication time, and exposed idle time. The compute time is the total compute time to train a model, and other operations can be hidden behind each other. Non-hidden time of an operation is defined as exposed time. Overall, ZeRO-Infinity performs 0.1\% better than \texttt{HierMem}. Both memory systems present similar performance because they have almost equivalent resources. The small performance drop in \texttt{HierMem} originates from the additional data transfer stages with multi-level switches.

To find a better-performing configuration of \texttt{HierMem}, we explore the design space of \texttt{HierMem} while varying in-node pooled fabric bandwidth and the remote memory group bandwidth. We only sweep these parameters as the exposed communication turns out to be a bottleneck. In-node pooled fabric bandwidth is varied between 256GB/s and 2048GB/s with the unit of 256GB/s, and remote memory group bandwidth is varied between 100GB/s and 500GB/s with the unit of 100GB/s. The found best performance with the least resource provision is shown as \texttt{HierMem(opt)} in \autoref{tab:case-study-disagg-mem-system-configurations} and \autoref{fig:execution_time_breakdown_comparison}. It performs 4.6 times better than the baseline configuration.

\insertFigure{execution_time_breakdown_comparison}{
Runtime breakdown of disaggregated architectures.
}{0.94}{-3mm}{-4mm}