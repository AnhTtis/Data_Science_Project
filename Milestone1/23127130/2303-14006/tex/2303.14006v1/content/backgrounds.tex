\section{Background}

\subsection{Distributed Training}\label{subsec:DistributedTraining}
\circled{1} \textbf{Synchronous/Asynchronous Training.}
When models/data are distributed across NPUs, it is crucial to decide when and how to synchronize such distributed information across them. The asynchronous training approach, as the name suggests, communicates among NPUs in an asynchronous manner. Therefore, asynchronous training suffers from the convergence problem~\cite{saeedACE} and is more complex to implement and maintain~\cite{distributedSurvey}. Therefore, the most common approach is synchronous distributed training. In this mechanism, all nodes work on their own data and synchronize the distributed information altogether before proceeding to the next iteration, usually in the form of collective communications~\cite{astrasim, li2019accelerating}.

\circled{2} \textbf{Parallelization Strategy.}
Each parameter, including model weights and input data, is distributed across NPUs. How each parameter is sharded and distributed is dictated by its ruling parallelization strategy~\cite{astrasim}. The three most pervasive parallelism strategies are: data-parallel (DP), model-parallel (MP), and pipeline-parallel (PP). DP distributes mini-batch across NPUs and synchronizes weight gradients during the backward pass~\cite{astrasim, distributedSurvey}. MP, on the other hand, distributes a model evenly across NPUs and communicates forward activation and input gradients pass~\cite{distributedSurvey}. MP and DP are orthogonal patterns, therefore MP and DP can be used simultaneously, called hybrid-parallel scheme~\cite{shoeybi2019megatron}. PP distributes model layers across nodes and processes micro-batches in a pipelined manner~\cite{huang2019gpipe, harlap2018pipedream}. Other parallelization strategies are also actively being investigated~\cite{rajbhandari2020zero, rajbhandari2021zero, ren2021zero}.

\circled{3} \textbf{Training Loop.}
In addition to parallelization, the order of communication and computation must be clearly defined to execute distributed training. Such computation and communication ordering information is named a training loop~\cite{astrasim}.

% \vspace{-2mm}
\subsection{Collective Communication}\label{subsec:CollectiveCommunication}



\circled{1} \textbf{Collective Communication.}
Depending on the parallelization strategy, models and/or input batches are distributed across NPUs. Therefore, it is unavoidable that devices should communicate and synchronize data, such as forward activation or weight/input gradients~\cite{themis}. This traffic is commonly formulated and processed in the form of collective communications. Some common collective patterns in distributed training are shown in \autoref{fig:CollectiveDefinition}. With a synchronous training approach, the most pervasive collective pattern is \allreduce~\cite{dlCollective}, which could be logically viewed as \reducescatter\ followed by an \allgather.

\circled{2} \textbf{Hierarchical Collective Algorithm.}
There exist several basic topology-aware collective communication algorithms to execute these communication patterns. A handful of examples of basic topology-aware \allreduce collective algorithms include Ring-based~\cite{ringCollective}, Tree-based~\cite{treeAlg}, and Halving-Doubling~\cite{rabenseifner}. However, when the underlying network topology is multi-dimensional, such basic algorithms would not perform optimally as the logical topology each algorithm assumes mismatches the physical one. In order to mitigate such an effect, multi-rail hierarchical collective algorithms have been proposed~\cite{blueconnect}. Using this scheme, in order to run an \allreduce collective on an $N$-dimensional topology:
\squishlist
    \item Run \reducescatter in ascending order from Dim 1, then Dim 2, $\cdots$, up to Dim $N$.
    \item Run \allgather in descending order from Dim $N$, $\cdots$, Dim 2, down to Dim 1.
\squishend

\insertFigure{CollectiveDefinition}{
Definition of \reducescatter, \allgather, \allreduce, and \alltoall collective communication patterns.
}{1}{-5mm}{-4mm}

\subsection{ASTRA-sim}
Vast design choices of distributed training shown in \autoref{subsec:DistributedTraining}, combined with diverse hardware configurations create an enormous SW/HW design space of distributed training as depicted in \autoref{fig:AstraSimOverview}(a). Such enormous design space cannot be solely explored by only leveraging physical systems, especially at scale. Therefore, a simulation-based mechanism to quickly model and profile distributed training platforms is necessary for design-space exploration.
\astrasim~\cite{astrasim} is a distributed training simulation framework to exactly address this demand. \astrasim captures the training configuration explained in \autoref{subsec:DistributedTraining}
Its high-level components are summarized in \autoref{fig:AstraSimOverview}(c).
It codifies the complex SW/HW search space across three abstraction layers. The workload layer lets the user describe and define target DNN models, target parallelization strategies, and training loops.
The system layer implements collective communication algorithms, schedules compute and communication operations, and manages compute-communication overlap. Compute times are fed in via external NPU models~\cite{samajdar2018scale} or real system measurements. Communication times are computed using a network simulator. The default simulator is Garnet~\cite{niket2009garnet} from gem5.
It reports detailed system and network-level behaviors as well as end-to-end training throughput.
