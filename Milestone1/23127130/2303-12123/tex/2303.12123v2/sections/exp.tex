\section{Experiments}

\subsection{Dataset}
We collect a dataset consisting of 80 CBCT dental scans as groundtruth of the 3D oral structure and source images to simulate PX imaging. We divide the model into two groups: 1) 60 cases used for training models based on auto-encoding and adversarial learning, and 2) 20 cases used for inference and validation for all models. The CBCT scan is resized into a size of $288\times256\times160$ using trilinear interpolation to minimize influence brought by imaging machines.


\subsection{PX Imaging Simulation from CBCT}
The moving trajectory of rotation center in PX imaging is fitted by the beta function as:
\begin{equation}
    y = 256 - beta(x/288, 3.6, 3.6) * 100 - 25.
\end{equation}
We split the trajectory curve equally into 576 pieces and assume the radiation rays evenly cross each small curve in angles between $-\pi/4$ and $\pi/4$. Research in \cite{cbct_no_hu_1}\cite{cbct_no_hu_2} show that HU is unreliable in CBCT scans due to variations in gray-scale values for different areas in the scan, especially when the imaging areas have the same density but different relative positions. Therefore, we follow the same method proposed in \cite{PX_generation, oral_3d} during projection to get realistic PX images from CBCT. Then the projection function $f(\cdot)$ in Eq. (\ref{eq:render_function}) can be rewritten into $\hat{f}(\cdot)$:
\begin{equation}
\label{eq:soft_render_discrete}
    \hat{f}(\cdot)=S\cdot\log(\sum_i^{\left\lfloor N_s\left(t_f-t_n\right)\right\rfloor} e^{\frac{V(\mathbf{p_{i}})+C}{S}})-\log{N_s} - C,
\end{equation}
where $C=H(\mu_{air})$. Comparisons among real PX image and simulated images generated by $f(\cdot)$ and $\hat{f}(\cdot)$ can be seen in Fig \ref{fig:px_compare}, where PX images simulated by $\hat{f}(\cdot)$ has a more closer contrast as real images.


\subsection{Hyper-parameters and Network Architecture}
We select $S=1200$ in Equation (\ref{eq:soft_render_discrete}) to distinguish air and soft tissues in HU. The sampling rate $N_s$ for each radiation ray during training follows a uniform distribution in $[0.25, 1,25]$. The level $L$ used in positional encoding is selected to be 16 with the normalizaton of coordinates into $[-1, 1]$. For the multi-head NeXF, we use a 12-layer fully-connected neural network with residual connections and set the number of heads as 160, which is consistent with the CBCT data.


\subsection{Training and Evaluation}
The model is trained for 20k iterations with a batch size of 64. The model is optimized by Adam with a learning rate of 0.0001. We use structural similarity index measure (SSIM) \cite{ssim}, dice coefficient (DC), and peak signal-to-noise ratio (PSNR) to evaluate the reconstruction results. We also use the averaged score proposed in \cite{oral_3d} as the overall metric.

\subsection{Baseline Models}
We compare our method with baseline models that can be grouped into two categories. The first group including GAN \cite{gan}, Oral-3D \cite{oral_3d}, and Res-Encoder \cite{x_to_3d}. These models are trained with the 60 paired simulated X-ray images and CBCT images and learn the prediction of explicit 3D representation with either adversarial learning or auto-encoding. We put our model in the second group with NAF \cite{naf}, another implicit representation with the same framework attenuation coefficients in 3D space.