\section{Introduction}
Radiological 3D reconstruction from limited 2D images has attracted increasing attention with the development of deep generative models in the past few years. Recent works like \cite{oral_3d, x2ct_gan, x_to_3d, 3d_leg} have shown the feasibility of 3D reconstruction from only one or two X-ray images, which provides an alternative solution to 3D imaging where only 2D imaging equipment is available. Due to the low radiation generated by 2D imaging equipment, these methods also bring a new choice in radiological examination for patients who are sensitive to radiation. For example, research in \cite{CT_radiation} shows that the X-ray imaging method could take as much as 200 less radiation than Cone Beam Computed Tomography (CBCT), a fast and low-radiation type of Computed Tomography (CT) and is widely used in dental radiology. Therefore, developing fast and accurate translation models could potentially bring great progress in medical imaging.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{figs/Nexf-oral3d.png}
    \caption{We compare our new model (blue) and Oral-3D (green) in this picture. Oral-3D first learns a back-projection model with paired images to generate a flattened 3D oral structure. Then it deforms the flattened image into a curved shape according to the individual dental arch shape acquired from the patient. In our model, we learn an implicit 3D representation of the oral structure only from the projection information, i.e., projection image and X-ray tube trajectory that is pre-defined by the equipment manufacturer and independent of individuality. After the model is well-trained, the 3D object is reconstructed by inferring the density distribution in 3D space from the implicit representation model and 2D coordinates. 
    }
    \label{fig:oral3d}
\end{figure}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/Nexf-PX-imaging.png}
    \caption{We show the comparison of imaging process of general CT (including CBCT) and PX in this picture. In CT, the X-ray tube and the film moves together around a fixed rotation center for $360$ degrees, where the film receives all X-rays sent from the tube. In PX imaging, the X-ray tube and the film rotates around a moving center, whose trajectory fits the curve of the mandible. Therefore, points that are around and away from the trajectory receive different levels of radiation during the imaging. For example, when the tube and the film moves from A to B in the right picture, the red point is projected twice while the green point is only projected once. This could make the image show more information of the imaging target at the red point over the green point.
    }
    \label{fig:imaging}
\end{figure*}

However, most of these cross-dimension translation models learn to explicitly generate a 3D image by auto-encoding and adversarial learning from paired X-ray images and CT scans. Consequently, the reconstruction quality is sensitive to the diversity and scale of training data. In dental imaging, restoring curved mandibular shapes brings additional challenges as only a single panoramic X-ray (PX) image is available. To solve this problem, recent studies like Oral-3D \cite{oral_3d} and X2Teeth \cite{x2_teeth} utilize individual prior knowledge when training the model, i.e., dental arch shape extracted from buccal images or instance annotations of teeth at the pixel level. Yet these complicated operations could bring conspicuous miss alignment during reconstruction, thus greatly hindering clinical applications in dental examinations. As a comparison, implicit representation models \cite{nerf, hash_nerf} provide a new solution in 3D reconstruction from 2D images. But these models rely on learning from abundant images viewed from various directions, which is hard to apply in radiology due to differences in imaging principles and inflexibility in imaging angles.
 
To address these limitations, we propose a new framework for 3D oral reconstruction from a single 2D panoramic X-ray (PX) image. Different from previous work like Oral-3D, which learns a back projection function to explicitly predict the reconstruction result by learning from paired images and prior knowledge of the individual dental arch shape, our model could learn 3D reconstruction simply from a single X-ray image with the projection settings from the imaging equipment. A comparison between Oral-3D and our method can be seen in Fig. \ref{fig:oral3d}, where only projection data is required during the reconstruction in our method.  

Unlike models in \cite{mednerf, xtransct} that utilize a single X-ray or two orthogonal X-ray images, our method could utilize the rich projection information during a panoramic scan with our advanced architecture. Specifically, we use a deep learning network to learn a mapping function between coordinates and density values of voxels in the 3D space, i.e., Hounsfield Unit (HU). To take advantage of the imaging process in panoramic imaging, we propose a multi-head model that outputs a bunch of voxel values at the same time given a 2D coordinate, which proves to be both efficient and effective over existing implicit representation models. Furthermore, to accommodate the imaging object in radiology, we utilize a dynamic sampling strategy to improve the reconstruction quality by acquiring points along radiation rays in random resolutions. Extensive experiments show that our model could significantly outperform state-of-the-art methods in 3D oral reconstruction both qualitatively and quantitatively. In conclusion, we summarize our contribution as follows:

\begin{itemize}
    \item Different from previous approaches in 3D oral reconstruction, such as Oral-3D\cite{oral_3d} and X2Teeth\cite{x2_teeth}, our model could achieve superior performance without training from any paired data, individual prior knowledge, or annotations.
     
    \item We propose an efficient implicit 3D representation model that maps a 2D coordinate into a bunch of 3D density values. This could reduce the computation complexity from $\mathcal{O}(N^3)$ to $\mathcal{O}(N^2)$ when reconstructing a $N \times N \times N$ object during both training and inference.
    
    \item We also propose a dynamic sampling strategy when sampling points from radiation rays with an adaptive projection method. This could encourage the model for higher reconstruction quality by learning a smooth density distribution in the 3D space .
\end{itemize}


% Various studies, including\cite{x_to_3d}\cite{x2ct_gan}\cite{x2_teeth}\cite{knee_3d}\cite{3d_leg}\cite{oral_3d}, have employed end-to-end deep learning models to translate one or more X-ray images of the imaging target into 3D space. 

%  In the case of single-image reconstruction, the model also relies on prior knowledge of the target shape or pixel-wise annotations to reconstruct an accurate shape of the 3D object. These conditions significantly limit the widespread adoption of such learning-based tomography in clinical applications.


% Compared to learning-based methods that rely on vast amounts of data, recent advances in the field of neural radiance field (NeRF) \cite{nerf}\cite{nerf++} have shown promising results in 3D object reconstruction from a single image in medical imaging. A NeRF model uses a deep neural network to approximate a field function that maps 3D positions into voxel values, and is optimized using paired poses and view images from a camera to reconstruct the 3D object. In medical imaging, the imaging equipment can provide the position of a detector and its captured image, such as an X-ray source and a digital film, making it possible to reconstruct the target 3D object without requiring additional patient data.


% Several recent works demonstrate the versatility of neural radiance field (NeRF) in medical imaging applications. For instance, \cite{naf} utilizes a neural network to predict attenuation coefficients for Cone Beam Computed Tomography (CBCT) reconstruction. Similarly, \cite{mednerf} develops a model that disentangles the shape and depth of surface and internal anatomical structures to create a continuous representation of CT scans from a single X-ray image. In another study, \cite{nerf_ultrasound} applies the NeRF algorithm in 3D ultrasound reconstruction to evaluate spinal curvature measurements. Lastly, \cite{nerf_surgery} presents a framework for stereo 3D reconstruction of deformable tissues from a single viewpoint in robotic surgery.


% However, the existing studies have yet to investigate the application of NeRF in panoramic imaging (PX). PX has become a popular imaging technique in dental healthcare in recent decades due to its fast speed, high accuracy, and low radiation. Similar to (CBCT), which is widely used in orthodontics, the PX imaging process involves the X-ray source and sensors moving simultaneously during scanning. However, unlike CBCT, PX uses focal plane tomography to generate a projection image of the target area. The X-ray source and a moving film rotate horizontally to match the focal curve with the patient's mandible and maxilla shape, as illustrated in Figure~\ref{fig:imaging}. However, the limited projection information in the z dimension presents challenges for 3D reconstruction from panoramic imaging. To address this issue, we propose a framework called Oral-NeXF, based on neural field methods, for 3D oral reconstruction from panoramic imaging. We summarize our key contributions as follows:



% https://iits.dentistry.utoronto.ca/node/765.html
%https://henryscheinequipmentcatalog.com/content-library/panoramic-radiography/
%https://en.wikipedia.org/wiki/Cone_beam_computed_tomography#Bone_density_and_the_Hounsfield_scale