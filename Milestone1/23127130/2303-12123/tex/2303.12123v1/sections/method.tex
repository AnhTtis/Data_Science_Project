\section{Methodologies}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/Nexf-overview.png}
    \caption{This image provides an overview of the Oral-NeXF model.
Starting with radiation rays, we use a dynamic sampler to select projection sample points on each ray with varying sampling rates. Then, we employ a positional encoder and a multi-head neural field model to predict beams of voxel intensities in 3D space. Next, we render the projection pixels adaptively based on the sampling rate. Finally, we calculate the MSE loss between the rendered slice image and the ground-truth projection image to update the parameters of the neural field model.
    }
    \label{fig:model}
\end{figure}

\subsection{Overview}
The Oral-NeXF model is trained in a similar manner to the NeRF model, where paired projection directions and view images are provided as input, as illustrated in Fig.~\ref{fig:model}. In the training process, we generate multiple sample points along each projection ray at various sampling rates. These sampled coordinates are then used as input for a positional encoder to be transformed into high-dimensional embeddings, which are further fed into a multi-head neural field function to predict voxel intensities in 3D space. Finally, the predicted 3D points are adaptively rendered into a pixel value based on the projection direction and sampling rate. The entire model is optimized iteratively using the projection direction provided by the X-ray source and the projection image provided by the X-ray sensor. During inference, the model takes in all the coordinates around the focal plane to reconstruct the target 3D oral structure.


\subsection{Neural X-ray Field}
NeRF has been emerging as a promising method for reconstructing large 3D scenes from 2D images viewed at different angles and positions. Generally, a neural field function $f$ can be taken as a function that maps a 3D coordinate into some physical attributes, e.g., color and density, which can be expressed as:
\begin{equation}
    f_{NeRF}:(\mathbf{p}) \rightarrow (\sigma, c),
\end{equation}
where $\textbf{p}$ is a 5D position coordinate $xyz\theta\phi$, $\sigma$ is the density, and $c$ is the color in RGB space. Given pairs of view data and render images, the model could be optimized by L2 norm between the ground-truth pixel $I_g(\mathbf{o})$ and rendered (projection) pixel $I_r(\textbf{o})$ viewed from the position $\textbf{o}$, which can be denoted as:
\begin{equation}
    Loss = \mathbb{E}_{\mathbf{o}}||I_r(\textbf{o})-I_g(\textbf{o})||_2
\end{equation}
Additionally, there is usually a positional encoder before the neural field function to exploit high-frequency variation of the target object by mapping the input coordinates into a higher embedding space. After training, the model can predict the color and intensity value at any 3D position, thus representing the 3D object implicitly by rendering from any position.

In this paper, we follow a similar framework as NeRF but change the neural field and rendering functions in different forms to accommodate PX imaging. To distinguish NeRF models, we use the term NeXF to specify the field function used in PX imaging. In general, our NeXF only predicts the voxel intensity from the spatial position due to the isotropic feature of X-ray projection. Therefore, the mapping function should be denoted as:
\begin{equation}
    \label{eq:f_nexf}
    f_{NeXF}:(\textbf{p}) \rightarrow (v_p),
\end{equation}
where $v_p$ is the intensity value of a voxel in the 3D space. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/Nexf-multihead.png}
    \caption{In this picture, we present a comparison between the predictions of NeXF and general NeRF models. The NeXF model is specifically designed for PX imaging, where the model uses a single 2D coordinate to predict intensities of voxels at the same positions in axial plane. In contrast, a general NeRF model can only generate a single voxel value given a 3D coordinate input.
    }
    \label{fig:multihed}
\end{figure}

\subsection{Multi-head Prediction}
With the feature of focal plane tomography, pixel intensities in the X-ray sensor are mainly associated with the projection space in the same axial plane, which is quite different from the imaging process of a camera or a CBCT scan. To address these limitations, we use a multi-head neural field function to predict beams of voxel intensities with the same 2D position simultaneously. Consequently, our model can output a slice of the projection image given a radiation ray and the neural field function in (\ref{eq:f_nexf}) can be further expressed as:
\begin{equation}
    \label{eq:f_nexf_detail}
    f_{NeXF}:(x, y) \rightarrow (v_{x, y, 1}, v_{x, y, 2}, \cdots, v_{x, y, n}),
\end{equation}
where $(x, y)$ is a 2D coordinate in the axial plane and $n$ is the height of the X-ray beam. A comparison of multi-head prediction in NeXF and single-head prediction in NeRF can be seen in Fig.~\ref{fig:multihed}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/Nexf-PX-comapre.png}
    \caption{Comparison of different rendering methods in PX imaging. We can see that with soft rendering the generated PX image has a closer contrast with the real PX image (obtained from Internet). The real PX image looks more clear due to the high resolution of the PX machine.
    }
    \label{fig:px_compare}
\end{figure}

\subsection{Soft Rendering}
In this paper, we use CBCT data to simulate the PX imaging and obtain the ground truth of 3D oral structure. However, Hounsfield Units (HU) is unreliable in CBCT scans due to variations in grayscale values for different areas in the scan. This can occur even when these areas have the same density, but different relative positions within the scanned organ \cite{cbct_no_hu_1}\cite{cbct_no_hu_2}. To obtain the PX image with a close distribution with real PX images, we choose to use a different rendering method following the work in \cite{PX_generation}. Therefore, the projection value of a radiation ray that originates from the position $\textbf{o}$ with direction $\textbf{d}$ between $t_n$ and $t_f$ can be expressed as:
\begin{equation}
\label{eq:render_continous}
    I_r(\mathbf{o})=S \cdot \log \int_{t_n}^{t_f} e^{\frac{v(\mathbf{o}+t\cdot\mathbf{d})-C}{S}}dt
\end{equation},
where $v()$ denotes the voxel intensity value in 3D space, S and C are the scaling and bias factors. The selection value for C and S depend on the CBCT data used for simulation. A comparison of the proposed and conventional projection methods can be seen in Fig.~\ref{fig:px_compare}.

\subsection{Dynamic Sampling and Adaptive Rendering}
To obtain a smooth intensity distribution in 3D space, we propose to use a dynamic sampling strategy to acquire points from the radiation ray. Given a projection ray from training data, we acquire points with a random sampling rate $N_s$. For example, the dynamic sampler utilizes different sample rates to get sample points from the red, blue, and purple radiation rays, as shown in the output of the multi-head predictor in Fig.~\ref{fig:model}. Accordingly, the rendering function in (\ref{eq:render_continous}) could be expressed in a discrete form as:
\begin{equation}
\label{eq:render_discrete}
    I_r(\mathbf{d})=S\cdot(\log\sum_i^{\left\lfloor N_s\left(t_f-t_n\right)\right\rfloor} e^{\frac{V\left(t_n+\frac{1}{N_s} \cdot i\right)-C}{S}}-\log{N_s}).
\end{equation}


% Second, Hounsfield Units (HU) is unreliable in CBCT scans due to variations in grayscale values for different areas in the scan. This can occur even when these areas have the same density, but different relative positions within the scanned organ \cite{cbct_no_hu_1}\cite{cbct_no_hu_2}. As a result, one can not simply estimate the X-ray projection by applying the Beerâ€“Lambert law. A comparison of CBCT and PX imaging in different views is shown in Fig. \ref{fig:imaging}. These characteristics have brought great challenges in exploring 3D Oral structure from PX images by NeRF-like models.
% In the absence of accurate HU that could reflect the attenuation rate in physical world, we choose to propose a different model that learns a neural X-ray field (NeXF) for 3D oral reconstruction by PX images. The NeXF model would 