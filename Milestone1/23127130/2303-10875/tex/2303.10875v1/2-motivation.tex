\section{Related Works and Motivation}

\begin{figure}[t]
    \centering
    \includegraphics[width = 1\linewidth]{fig/motivation-pipeline.pdf}
    \caption{(a) Typical GNNs Pipeline with MP paradigm. (b) Accuracy and latency comparison when performing sampled results reuse among different DGCNN layers on ModelNet40~\cite{wu20153d} dataset and RTX3080 platform.}
    \label{fig:motivation-pipeline}
    \vspace{-6pt}
\end{figure}

In combination with previous related works, we introduce several important observations on motivating efficient GNNs explorations for edge computing platforms.

\textbf{Observation \raisebox{-0.1em}{\ding[1.3]{172}}}: Redundant operations bring significant overhead.

Generally, GNN computations follow the \textit{message massing} (MP) paradigm, which consists of \textit{graph sampling}, \textit{aggregation}, and \textit{combination}, as shown in Fig.~\ref{fig:motivation-pipeline}(a). 
By reusing operation results across GNN layers, we observe that redundancy may exist within the MP paradigm.
As illustrated in Fig.~\ref{fig:motivation-pipeline}(b), reusing the sampled results between GNN layers does bring some negligible accuracy loss, but could provide a considerable boost to computational efficiency.
It indicates that redundant operations usually bring significant overhead, which is one of the major obstacles in optimizing computational efficiency.

\textbf{Motivation \raisebox{-0.1em}{\ding[1.3]{182}}}: Decouple the MP paradigm by leveraging fine-grained GNN design space.

The above results demonstrate that building GNNs by stacking generic GNN layers together will inevitably bring redundant operations. 
The most straightforward approach for this problem is optimizing GNN models through large amounts of ablation studies and analysis. 
Typically some redundant sampling operations are eliminated in DGCNN for achieving better efficiency \cite{li2021towards}.
However, these handcrafted approaches heavily rely on design experience and is non-reproducible for different tasks. 
Inspired by the success of designing scalable GNN models by decoupling GNN paradigm \cite{cai2021rethinking, zhang2022pasca}, GNN layers are decoupled into operations for building a fine-grained design space with a operation-wise manner.
With the freedom offered by fine-grained design space, various configurations (i.e. aggregation range, operation order, etc.) could be generated by learning rather than manual laboring.

\textbf{Observation \raisebox{-0.1em}{\ding[1.3]{173}}}: Exploring fine-grained design space is costly.

Compared with layer-wise design space, the fine-grained design space takes operations as candidate options instead of layers, greatly expanding the scope of exploration.
For example, the backbone of DGCNN consists of four GNN layers, and each layer includes three basic operations, i.e. \textit{sample}, \textit{aggregate}, \textit{combine}. Therefore, to cover most of DGCNN architectures, the fine-grained design space must contain at least $12$ positions, $3$ candidate operations, and $N$ functions for each operation (details in Sec.~\ref{sec:design-space}). As a result, there are staggering ${(3N)}^{12}$ options to explore in the design space, which significantly increases the exploration complexity.

\textbf{Motivation \raisebox{-0.1em}{\ding[1.3]{183}}}: Speedup exploration by search space simplification.

The exploration complexity demands a very efficient search strategy to navigate through the huge options.
Intuitively, some studies \cite{zhang2020autoshrink} have proposed to shrink the design space for better search efficiency.
Unfortunately, the boost in efficiency comes with the price of potentially discarding some valued choices.
In reality, the contribution of different layers towards the overall accuracy within a model may vary greatly \cite{sheng2022larger}.
The representational power of front layers usually contributes more to the whole GNNs \cite{tailor2021towards}.
Hence, they simplify the latter parts of DGCNN to obtain better model performance. 
All of these inspire us to improve exploration efficiency by dividing the search space and guiding the search tendencies at different positions.

\begin{figure}[t]
    \centering
    \includegraphics[width = 1\linewidth]{fig/motivation-breakdown.pdf}
    \caption{Execution time breakdown of DGCNN across different platforms.}
    \label{fig:motivation-breakdown}
    \vspace{-6pt}
\end{figure}

\begin{figure*}[ht]
    \centering
    \includegraphics[width = 1\linewidth]{fig/framework-overview.pdf}
    \caption{Overview of the HGNAS framework. Oper. and Func. denotes Operation and Function respectively.}
    \label{fig:framework-overview}
    \vspace{-8pt}
\end{figure*}

\textbf{Observation \raisebox{-0.1em}{\ding[1.3]{174}}}: The same GNN model may behave differently on various computing platforms.

Detailed execution time breakdown of DGCNN is illustrated in Fig.~\ref{fig:motivation-breakdown} for different platforms.
These results are obtained by PyTorch Profiler~\cite{profiler}. 
For Nvidia RTX3080 and Jetson TX2, the \textit{sample} operation occupies the majority of execution time. 
This is because GPUs are better at handling compute-intensive matrix operations, and not so good at memory-intensive graph sampling operations.
For Intel i7-8700K, \textit{aggregate} and \textit{sample} take up most of the execution time, which is caused by a massive number of irregular memory accesses.
As the results indicate, the execution of DGCNN on the above platforms belongs to \texttt{I/O-bound}.
However, on the Raspberry Pi, due to the limited resources, all three phases occupy relatively large proportions of execution time, which makes the execution process also \texttt{compute-bound}.
They demonstrate that the same GNN model may result in various execution characteristics across computing platforms. 

\textbf{Motivation \raisebox{-0.1em}{\ding[1.3]{184}}}: Perceive the hardware sensitivities of GNNs across different computing platforms.

Due to differences in hardware architectures and available resources, GNNs that are computationally efficient on GPUs may not be sufficient on other platforms. 
Moreover, the hybrid execution mode of GNNs, which consists of both memory-intensive and computation-intensive operations, poses great challenges for effectively perceiving GNNs hardware sensitivities~\cite{yan2020characterizing}.
Some works attempt to improve hardware efficiency by analytical estimation for hardware/algorithm co-design~\cite{zhang2021g}. 
However, these approaches introduce obvious accuracy drop and are difficult to extend to other platforms~\cite{benmeziane2021comprehensive}.
Although real-time measurement could really capture hardware behaviors, its tremendous overhead is intolerable for efficient exploration.
Therefore, an efficient and scalable hardware-aware approach is highly demanded for GNNs exploration.
Inspired by \cite{dudziak2020brp}, a GNN-based end-to-end hardware performance predictor is integrated to efficiently and accurately perceive GNNs hardware performance across various platforms.