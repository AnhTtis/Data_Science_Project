\section{HGNAS Framework}


\subsection{Problem Definition and HGNAS Overview}

In this paper, we aim to co-optimize the accuracy and hardware efficiency of GNNs on edge devices.
Given a target edge device $\cal{H}$, and hardware constraints $\cal C$, the multi-objective optimization process of HGNAS can be
formulated as: 
\begin{equation}
 \arg \mathop {\max }\limits_{\{{\cal A}, \cal{H}\} } \left( \alpha  * {acc_{val}}\left({{\cal W}^ * }, {\cal A} \right) - \beta  * lat \left({\cal A}, \cal{H} \right) \right) ,
    \label{equ1}
\end{equation}
\begin{equation}
    \begin{split}
            s.t. \quad & {{\cal W} ^ * } = \arg \mathop {\max }\limits_{\mathcal W}{acc_{train}}({\cal W}, {\cal A}) \\
            & lat ({\cal A}, \cal{H}) < {\cal C}
    \end{split},
    \label{equ2}
\end{equation}
where $\cal W$ denotes the model weights, $acc_{train}$ is the training accuracy, $acc_{val}$ is the validation accuracy, $lat$ is the inference latency on targeted platform $\cal{H}$, $\cal A$ is the GNN architecture candidate, $\alpha$ and $\beta$ are the scaling factors used to adjust the optimize propensity between accuracy and latency.

Fig.~\ref{fig:framework-overview} shows the overview of our HGNAS framework. 
Given a specific task, a target device, the hardware constraints, and the optimizing metrics, HGNAS will first generate a \textit{fine-grained operation-based design space} which consists of the \textit{Function Space} and \textit{Operation Space}.  
HGNAS then constructs a supernet covering the GNN design space to adopt the one-shot exploration approach.
Afterward, HGNAS will explore the hierarchical design spaces based on the proposed \textit{multi-stage hierarchical search strategy}. 
During exploration, the evaluation result of each candidate architecture is determined by both the accuracy on the validation dataset, and the hardware performance on the target device.
The hardware performance of GNNs is provided by the \textit{GNN hardware performance predictor} integrated in HGNAS.
In subsequent sections, we will detail the three main components of HGNAS.

\subsection{Fine-grained Operation-based Design Space}\label{sec:design-space}

\textbf{The GNN supernet.}
To lift the restrictions of the traditional GNN design space, instead of presetting the number of GNN layers, HGNAS builds the design space upon positions for GNN operations to achieve more flexibility.
Specifically, for each position in the supernet, there are four basic operations including \textit{connect}, \textit{aggregate}, \textit{combine}, and \textit{sample}, each containing specific attributes.
Aside from the operations derived from the MP paradigm, the \textit{connect} operation, including direct connection and skip-connection, provides more freedom in GNN model construction.
For point cloud processing applications, the message type attribute of aggregate operations specifies the construction method of the messages to be aggregated.
As shown in Fig.~\ref{fig:framework-overview}, the supernet is organized by stacking all the positions together. 
In practice, supernet training demands that operations within each position must obtain the same hidden dimension length.
HGNAS appends linear transformations to operations incapable of altering hidden dimensions, such as \textit{sample} and \textit{aggregate}, to ensure dimension alignment among operations.
These linear transformations will be disposed of in the finalized architecture to avoid introducing additional overhead.

\begin{table}[t]
  \small
  \centering
  \renewcommand\arraystretch{1.3}
  \caption{The available choices in GNN's supernet.}
    \begin{tabular}{l|l}
    \hline
    \textbf{Operation} & \textbf{Function} \\
    \hline
    Connect & Skip-connect, Identity \\
    \hline
    \multirow{2}[4]{*}{Aggregate} & Aggregator type: sum,  min,  max, mean \\
    \cline{2-2}   & Message type: Source pos, Target pos, Rel pos, \\ & Distance, Source$||$Rel pos, Target$||$Rel pos, Full \\
    \hline
    Combine & $8$, $16$, $32$, $64$, $128$, $256$ \\
    \hline
    Sample & KNN, Random \\
    \hline
    \end{tabular}
  \label{tab:space}
  \vspace{-3pt}
\end{table}

\textbf{The hierarchical design space.}
HGNAS detaches attributes from operations to construct an independent \textit{Function Space} to further decouple the fine-grained design space.
The remainder of design space made up of operation types is then used to assemble the \textit{Operation Space}.
In addition, these two sub-spaces can be explored separately by leveraging the proposed multi-stage hierarchical search strategy (see Sec.~\ref{sec:strategy}) to reduce exploration complexity.
All candidate operations and functions are listed in Tab. \ref{tab:space}.

\subsection{Multi-stage Hierarchical Search Strategy}\label{sec:strategy}

HGNAS divides the search process into two stages corresponding to \textit{Function Space} and \textit{Operation Space}, in order to reduce the exploration complexity of the fine-grained design space as aforementioned in \textbf{Observation \raisebox{-0.1em}{\ding[1.3]{173}}}.
Inspired by \cite{guo2020single}, the search strategy in HGNAS is based on an evolutionary algorithm ($EA$), as illustrated in Alg. \ref{gnnalgo}.
In particular, HGNAS first searches a set of functions for GNN supernet from function space.
After the optimal function set is determined, HGNAS trains the GNN supernet and performs a multi-objective operation search for all the positions.
Finally, the algorithm outputs the top-performing model within the design space.
During supernet training, a random sub-network is generated by sampling operations for each position in the GNN supernet, whose weights $\mathcal W$ are updated via back propagation.
Details of the multi-stage search strategy are presented as follows.

\textbf{Stage 1: Function Search.} 
In this stage, HGNAS aims to find a function setting to maximize the supernet accuracy.
To further improve the exploration efficiency, HGNAS divides $N$ positions in GNN supernet into two halves, and shares a set of functions among the \textit{Upper} half ($0,...,N/2$) and another set among the \textit{Lower} half ($N/2+1,...,N$). 
For a supernet with $12$ positions, through sharing functions among positions in the decoupled design space, HGNAS can reduce the number of exploration candidates from $\bm{4.2 \times {10^{12}}}$ to $\bm{1.7 \times {10^7}}$.
Finally, an optimal function set $\mathbb{F}$ is determined for initializing the supernet ${\cal N}_{super}$. 
Note that fixing $\mathbb{F}$ in this stage will significantly reduce the complexity of subsequent operation searches.


\textbf{Stage 2: Operation Search.}
By pre-training the supernet and performing EA-based searches, HGNAS obtains a set of operations that maximizes the objective function value in the \textit{Operation Space}. 
Benefiting from the one-shot search strategy, supernet training and operation search are divorced to avoid the exorbitant cost of sub-network retraining during the search.
To improve the search efficiency and meet the hardware constraints on the targeted device, HGNAS evaluates the candidate architectures based on the proposed hardware performance predictor (see Sec.~\ref{sec:predictor}) during the search.
Only the architectures that meet the hardware constraints will be further evaluated for the accuracy metric.
Specifically, the objective function during the operation search is formulated as:
\begin{equation}
{{\cal{F}}_{obj}(\cal C)} = \left\{ {\begin{array}{*{20}{c}}
{0 \quad \quad \quad \quad \quad \quad \quad \quad ,  \quad if \quad lat \ge \cal C}\\
{\alpha  * ac{c_{val}} - \beta  * lat ,  \quad if \quad lat< \cal C}
\end{array}} \right.
    \label{equ3}
% \vspace{-5pt}
\end{equation}

\begin{algorithm}[t]
\small
\textbf{Inputs:} population size $P$, hardware constraints $\cal C$, target device $\mathcal{H}$, operation space $\mathbb{S}_{op}$, function space $\mathbb{S}_{f}$, max iteration $T$, number of positions $N$.

\textbf{Outputs:} the best found GNN design $\cal A^*$ for target device $\mathcal{H}$.

Initialize GNN supernet ${\cal N}_{super}$ with $N$ positions and two function sets $upper \leftarrow \mathbb{\O}$, $lower \leftarrow \mathbb{\O}$ \\
\hl{/* Stage 1: Function search */} \\

Assign function set: \parbox[t]{0.5\linewidth}{
${\cal N}_{super}[0, N/2] \leftarrow upper$, \\
${\cal N}_{super}[N/2+1, N] \leftarrow lower$}

\For{$1 \leq t \leq T$}{
$\{upper, lower\} \leftarrow EA(P, {\cal N}_{super}, \mathbb{S}_{f}, obj = max(acc_{val}))$\\
}
Fix function set $\mathbb{F}  \leftarrow \{upper,lower\}$ for ${\cal N}_{super}$\\ 
Re-initialize and pre-train ${\cal N}_{super}(\mathbb{S}_{op}, \mathbb{F})$\\
\hl{/* Stage 2: Operation search */} \\
Initialize operation set $\cal O \leftarrow \mathbb{\O} $\\
\For{$1 \leq t \leq T$}{
    ${\cal O} \leftarrow EA({\cal P}, {\cal N}_{super}, \mathbb{F}, S_{op},
    obj= max({\cal F}_{obj}({\cal C})))$\\
}
\textbf{return} optimal architecture ${\cal A^*} \leftarrow \{{\cal O}, \mathbb{F}\}$

\caption{\small Multi-stage hierarchical search strategy.}
\label{gnnalgo}
\end{algorithm}

\subsection{GNN Hardware Performance Predictor}\label{sec:predictor}

To meet the efficiency requirements on targeted devices, we build a GNN hardware performance predictor that can efficiently learn the relationship between GNN architectures and hardware efficiency.
Specifically, its execution process consists of the following phases: \textit{graph construction}, \textit{node feature generation}, and \textit{inference latency prediction}, as shown in Fig.~\ref{fig:predictor}.

\begin{figure}[t]
    \centering
    \includegraphics[width = 1\linewidth]{fig/predictor.pdf}
    \caption{Latency prediction of a candidate model for the target device.}
    \label{fig:predictor}
    \vspace{-3pt}
\end{figure}

\textbf{Graph construction.} 
During this phase, HGNAS abstracts GNN architectures into directed graphs as the input of the GNN predictor.
The nodes in these architecture graphs represent inputs, outputs, and operations, while the edges represent the dataflow within the GNN architecture. 
In practice, accurate prediction of hardware efficiency requires both candidate model architecture and the graph property of the input dataset, which GNN execution highly depends on.
However, the plain abstraction of the original GNN architectures is too sparse for the predictor to obtain enough structural features, while lacking the necessary information on input data.
Hence, HGNAS introduces a global node connected with all nodes in the graph to improve the graph connectivity.
The input data information is also encoded into the global node for better prediction accuracy.

\textbf{Node feature generation.} 
For an operation node, the node feature consists of the operation type and its corresponding function.
Specifically, HGNAS encodes these two components into a $7$-dimensional and a $9$-dimensional one-hot vector respectively, and concatenates the results to represent the node feature.
For input and output nodes, HGNAS assigns them with a zero vector.
For the global node, HGNAS encodes the input graph data properties (number of nodes, density, etc.) into a $16$-dimensional vector as the global node feature.

\textbf{Latency prediction.} 
To avoid the over-smoothing problem often induced by deeper GNNs on small-scale graphs (i.e., the abstracted architecture graph), the predictor consists of only three GCN layers~\cite{kipf2017semi} and a multi-layer perceptron (MLP).
The inputs of the predictor are information on the target device, adjacency matrix, and node features. 
Specifically, the GCN layers utilize the sum aggregator with hidden dimensions of $256 \times 512 \times 512$.
The three layers in MLP with hidden dimensions of $256 \times 128 \times 1$, are followed by a LeakyReLU function for generating a scalar prediction of latency.
As the architecture graphs normally contain no more than a few dozen of nodes, the overhead brought by latency prediction is mostly negligible.
For instance, the GNN-based predictor can predict the latency of a candidate architecture for a target edge device within milliseconds on the Nvidia RTX3080.