\section{Introduction}

Graph neural networks (GNNs) have achieved state-of-the-art performance in various graph representation scenarios, including node classification \cite{kipf2017semi}, link prediction \cite{zhang2018link}, recommendation \cite{ying2018graph} and 3D representation learning on point clouds \cite{qi2017pointnet++}. 
Due to the powerful feature extraction capabilities on topological structures, GNNs have become a popular strategy for handling point cloud data, such as DGCNN \cite{wang2019dynamic}.
Moreover, with the rising popularity of 3D scanning sensors in edge devices, such as mobiles, unmanned aerial vehicles, etc., it is natural to investigate how to design efficient GNN models for edge applications.

The most challenging issue of GNNs is their hungry demands for hardware resources. 
Especially for resource-constrained edge devices, GNNs usually exhibit low hardware efficiency and \textit{Out-Of-Memory} (OOM) problem, limiting their potential to handle real-world edge applications.
For instance, given a frame of point cloud data, DGCNN performs KNN operations in each GNN layer for graph construction, resulting in poor hardware efficiency on edge platforms.
As shown in Fig. \ref{fig:intro-comparison}, the examined GNN's inference latency and peak memory usage for handling point cloud classification tasks continues increasing rapidly as the number of processed points grows.
Specifically, for the default setting with $1024$ points, DGCNN \cite{wang2019dynamic} needs more than \textbf{4 seconds} to handle a single frame on Raspberry Pi (i.e. about $1/4$ fps, frame per second), which is intolerable for real-time applications.
In addition, processing the involved graphs with more than $1536$ points will cause OOM problems during DGCNN inference.
Therefore, deploying GNNs on edge devices is extremely challenging with resource constraints. 

\begin{figure}[t]
    \centering
    \includegraphics[width = 1\linewidth]{fig/intro-comparison.pdf}
    \caption{Comparison between our approach and DGCNN. Inference latency and peak memory usage on Raspberry Pi are illustrated on the left. Inference speed and memory efficiency improvement across different edge devices are illustrated on the right.}
    \label{fig:intro-comparison}
    \vspace{-3pt}
\end{figure}

Several handcrafted approaches have been proposed to tackle the prohibitive inference cost of GNNs in point cloud processing
\cite{li2021towards, tailor2021towards, zhang2021exploration}. 
Even though they could achieve notable speedups by simplifying the GNN computations, manual optimization is difficult to adapt for different computing platforms, regarding the huge design space and various hardware characteristics \cite{tailor2021towards}.
As a very promising approach, hardware-aware neural architecture search (NAS) could explore optimal architectures automatically according to given objectives, which is independent of human experience and avoids manual labor.
By leveraging this approach, several works have made encouraging progress in developing efficient DNN models on edge devices~\cite{dudziak2020brp, wu2019fbnet}.
More specifically, we aim to exploit hardware-aware NAS for efficient GNNs derived from point cloud applications on edge platforms.


Although several NAS frameworks have been introduced for GNNs \cite{ijcai2020p195, gao2022graphnas++}, most of them have rarely considered the hardware constraints and latency requirements inspired by real-world edge applications.
In this paper, HGNAS is proposed as an efficient hardware-aware graph neural architecture search framework for point cloud applications on edge computing platforms. 
Such a hardware awareness is achieved by integrating hardware efficiency as a partial objective when performing the single-path one-shot evolutionary search.
Given the targeted edge devices, HGNAS automatically explores the optimized graph neural architectures by guaranteeing both accuracy and efficiency under hardware constraints (i.e. inference latency, model size, etc.).
One straightforward approach to achieve hardware awareness is to deploy the generated architecture candidates on the targeted devices and feedback the measured data during exploration.
However, the required tremendous real-time hardware measurement leads to very inefficient exploration process since the cost of edge inference and communication is often unbearable.
As such, a more elegant way to evaluate GNN hardware efficiency for different platforms is warranted.
In addition, the redundant operations within the layer-wise GNN design space and lengthy search times also pose great challenges to the efficient GNNs exploration process.

To address these challenges, our proposed HGNAS framework leverages novel hardware-aware techniques for procuring both desirable GNN performance and search efficiency.
We spot that GNN architectures themselves are also graphs that can be well represented by GNNs.
Such a concept is put into good use by drawing on the idea of \textit{Use GNN to perceive GNNs}.
Therefore, HGNAS can effectively perceive the latency of GNN candidates through a well-polished GNN-based predictor. 
Furthermore, we develop a fine-grained design space composed of basic operations to unleash the potential of GNN computations. 
HGNAS also adopts an efficient multi-stage hierarchical search strategy by dividing the GNN design space, reducing the search time to a few GPU hours.
As shown in Fig.~\ref{fig:intro-comparison}, HGNAS has been proven superior in both latency and peak memory usage across various edge devices. 
Specifically, HGNAS on resource-constrained Jetson TX2 could achieve the same level of inference latency compared with DGCNN on powerful RTX3080, providing $47\times$ (i.e. $350$W vs. $7.5$W) power efficiency improvement without accuracy loss.
In summary, the contributions of this work are listed as follows:
\begin{itemize}
    \item \textbf{Framework.} To the best of our knowledge, HGNAS is the first NAS framework to perform efficient graph neural architecture search for resource-constrained edge devices. HGNAS can automatically explore GNN models with multiple objectives (accuracy, latency, etc.) for targeted platforms.  
    \item \textbf{Hardware awareness.} To the best of our knowledge, HGNAS is also the first work to achieve hardware performance awareness for GNNs across edge devices. The proposed GNN-based predictor can perceive the latency of a given GNN architecture on the targeted platform in milliseconds.
    \item \textbf{Evaluation.} We have conducted extensive experiments on point cloud classification tasks. By deploying HGNAS-designed models on various edge devices, we achieve about $10.6\times$ speedup and $88.2\%$ peak memory reduction with a negligible accuracy loss.
\end{itemize}
