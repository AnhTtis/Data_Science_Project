\section{Further Algorithmic Enhancements}
\label{Some Improvements}

In order to reduce computational costs, we propose two additional
enhancements.
The first one (see Section~\ref{section41}) makes use of the fact that
the SVM is mostly influenced by data points that are close to the separating
hyperplane.
The second one (see Section~\ref{section42}) introduces a rule for
updating $M$ in each iteration of Algorithm~\ref{First version}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Handling Points far From the Hyperplane}
\label{section41}

In Algorithm~\ref{First version}, the number of clusters increases in
each iteration. Hence, the time to solve Problem~\eqref{equation3}
increases from iteration to iteration in general.
Like in the original SVM, the  points closest to the hyperplane
influence the resulting hyperplane more than the other points.
Obviously, eliminating points that do not strongly influence the
hyperplane decreases the size of the problem. Some approaches
to eliminate these points have also been proposed for the original
SVM. For a recent survey, see, e.g., \textcite{reducing}. However,
most of these approaches are heuristics and do not necessarily yield a
feasible point of the problem.

The idea for our setting is the following.
Clusters that are far away from the hyperplane could be omitted as
this  will not change the solution.
The farther a cluster is from the hyperplane in an iteration, the
less likely it is that the cluster will be split or change sides
completely in a future iteration.
Hence, the clusters farthest from the current hyperplane mainly
add information about their side and capacity.
However, in a later iteration, the cluster may become relevant again.
Thus, we need to find a way to discard detailed information on certain
clusters but also a way to reactivate the discarded clusters if necessary.

We propose the following procedure to reduce the amount of clusters
that have to be considered in the current iteration of the algorithm.
If the number of clusters exceeds a
fixed value~$k^+$, we first fix the cluster with the centroid farthest
from the hyperplane as a kind of residual cluster on a side if this
side has points far from the hyperplane.
Second, we discard all clusters in which all points are
farther from the hyperplane than some~$\Delta^t$ and assign them to
the residual cluster on their side of the hyperplane. This way the
cardinality constraint remains valid. Moreover, all formerly discarded
clusters are checked for re-consideration. If a discarded cluster has a
point with a distance to the hyperplane less than $\Delta^t$ or if any
point in the cluster changed the side, the cluster is reactivated.

Let $\bar{S} = (s_{\alpha(1)}, \dotsc, s_{\alpha(d)})^\top$ be the vector of
increasingly sorted values of $S = \{s_1, \dots, s_d\}$ and let
$a \in (0,1)$.
The $a$-quantile of $S$, as proposed by \textcite{Quantile},
is given by
\begin{equation*}
  P_S(a) \define s_{\alpha(q)} +  \frac{s_{\alpha(q)} -
    s_{\alpha(r)}}{q - r} \left((d-1)a - q +1 \right)
\end{equation*}
with
\begin{equation*}
  q \define \max_{i \in [1,d]} \left\{i:\frac{i-1}{d-1} \leq a
  \right\},
  \quad r \define \min_{i \in [1,d]} \left\{i:\frac{i-1}{d-1} \geq a
  \right\}.
\end{equation*}
Given a parameter $\hat{\Delta}^t \in (0,1)$, we choose
$\Delta^t$ in each iteration $t$ according to
\begin{equation}
  \label{delta}
  \Delta^t = P_{D^{t}}(\hat{\Delta}^t)
  \quad \text{with} \quad
  D^{t}_j = \left\vert (\omega^t)^\top c_{j} + b^t \right \vert
  \quad \text{for all } j \in \left[1, k^t\right].
\end{equation}
Note that if in an iteration~$t$, a point in some discarded
cluster changed the side, the vector~$z$ as part of the current
solution does not fit to this change.
This happens when, e.g., $(\omega^{t-1})^\top x^i +b^{t-1} > 0$ and
$(\omega^{t})^\top x^i +b^{t} < 0$ but $z^t_j>0$ with $\mathcal{C}_j$
being the cluster with centroid farthest from the hyperplane on the
positive side.
To avoid that this happens too often, $\hat{\Delta}^{t+1}$ is
increased by a fixed value~$\tilde{\Delta} \in (0,1)$
when there is some point in some discarded cluster that has changed
sides.

Motivated by the above discussions, we add new steps in the
Algorithm~\ref{First version} that can be seen in
Algorithm~\ref{Second version}.
In Step \ref{Secondversion:removed}, if the number of clusters
exceeds $k^+$, clusters far from the hyperplane are discarded.
In Steps~\ref{Secondversion:change1} and~\ref{Secondversion:change2},
clusters discarded with a point that changed sides or that
is closer to the hyperplane than $\Delta^t$ are reactivated.
In Step~\ref{Second version:update-delta}, $\hat{\Delta}^t$ is
updated.

\begin{algorithm}
 \caption{Improved Re-Clustering Method (IRCM)}
  \label{Second version}
  \SetAlgoLined
  \SetKwInput{Input}{Input}\SetKwInOut{Output}{Output}
  \Input{$X\in~\mathbb{R}^{d\times N}$, $y \in \set{-1,1}^n$, $k^1 \in
    \mathbb{N}$, $C_1 >0$, $C_2>0$, $\tau\in\mathbb{N}$,
    $\hat{\Delta}^1 \in (0,1)$, $\tilde{\Delta}\in (0,1)$,
    $\mathcal{G}^1 =\emptyset$, $k^+ \in \mathbb{N}$.}
  Set $t=1$, compute $M^t$ as in \eqref{validM}, cluster $X_u$ in
  $k^1$ clusters using $k$-means, leading to centroids  $c^1, \dots,
  c^{k^1}$ and the numbers $e_1,\dots, e_{k^1}$ of  data points in each
  cluster.
  \\
  Solve  Problem~\eqref{equation3} to compute the hyperplane
  $(\omega^t,b^t)$ as well as $\xi^t, \eta^t, z^t$.\label{step2-2}
  \\  Compute $\Delta^t$ as in \eqref{delta}. \\
  \uIf{$k^t> k^+$}{
    update $\mathcal{G}^{t+1} \ot\mathcal{G}^{t} \cup
    \{\mathcal{C}_j:   \vert (\omega^t){^\top }x^\ell + b^t \vert >
    \Delta^t \; \forall  x^\ell \in \mathcal{C}_j \}$.
    \label{Secondversion:removed}
  }
  \Else{
    set $\mathcal{G}^{t+1} \ot \mathcal{G}^{t}$.
  }
  Set $\mathcal{J}^t\define\{\mathcal{C}_j \in \mathcal{G}^{t}:
  \exists x^\ell \in \mathcal{C}_j :  \sign(  (\omega^t)^\top
  x^\ell + b^t ) \neq \sign( (\omega^{t+1})^\top x^\ell + b^{t+1} ) \}.$ \label{Secondversion:change1}
  \\
  Update $\mathcal{G}^{t+1} \ot \mathcal{G}^{t+1}  \backslash
  (\{\mathcal{C}_j \in \mathcal{G}^{t} : \exists  x^\ell \in
  \mathcal{G}^{t}_j \text{ with }  \vert (\omega^t)^\top x^\ell + b^t
  \vert \leq \Delta^t \}\cup \mathcal{J}^t )$. \label{Secondversion:change2}
  \\
  \uIf{$\mathcal{J}^t \neq \emptyset$}{
    update $\hat{\Delta}^{t+1} \ot \min\{\hat{\Delta}^{t}+
    \tilde{\Delta},1\}$.
    \label{Second version:update-delta}
  }
  \Else{
    set $\hat{\Delta}^{t+1} \ot \hat{\Delta}^{t}$
  }
  Compute $ M^{t+1}$ as in \eqref{BigM 2}.\label{secondversion:updateM}
  \\
  \uIf{$\mathcal{J}^t \neq \emptyset$ or the hyperplane $(\omega^t,b^t)$ cuts a
    cluster}{
  Set $k^{t+1} \ot k^t$. \\
    \For{each cluster that is cut by the hyperplane $(\omega^t,b^t)$}{
      Split the cluster into two new clusters so that neither of the
      two new clusters is cut by the hyperplane~$(\omega^t,b^t)$.      \\
      Update the centroids of the newly created clusters.
      \\
      Set $k^{t+1} \ot k^{t+1} + 1$.}
    Update $t\ot t +1 $ and back to Step \ref{step2-2}.
  }
  \Else{
   Return the hyperplane~$(\omega^t,b^t)$ as well as $\xi^t,\eta^t, z^t$.
  }
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Updating the Big-$M$}\label{section42}
As discussed in Section \ref{sec:miqp-formulation}, $M$ needs to be
sufficiently large.
However, the bigger the $M$, the more likely we face numerical
issues.
As shown in Section~\ref{sec:miqp-formulation}, the smaller the
objective function provided by a feasible point, the smaller
the value of $M$ can be chosen.
Based on that, we update $M$ in each iteration with the aim of
decreasing it.
We do this by adding Step~\ref{secondversion:updateM} in
Algorithm~\ref{Second version} and the next theorem justifies this.

\begin{thm}
  \label{isaupper}
  Consider $X, y, C_1, C_2, \tau$, as well as $c^1,\dots, c^{k^t}$ and
  $e_1, \dots e_{k^t}$ in an iteration~$t$ of Algorithm~\ref{First
    version}.
  Then, the optimal solution $(\bar{\omega}^t,\bar{b}^t, \bar{\xi}^t,
  \bar{\eta}^t, \bar{z}^t)$ of Problem~\eqref{equation3} provides an
  upper bound
  \begin{equation}
    \label{upperf}
    \tilde{f}_t \define \frac{\Vert \bar{\omega}^t \Vert^2}{2} +
    C_1 \sum_{i=1}^n \bar{\xi_i} + C_2(\tilde{\eta}_1
    + \tilde{\eta}_2 ),
  \end{equation}
  with
  \begin{equation}
    \label{ez}
    \tilde{z}_j =
    \begin{cases}
      1, & \text{ if }  \left(\bar{\omega}^{t}\right)^\top \tilde{c}_j +
      \bar{b}^t \geq 0,
      \\
      0, & \text{ otherwise},
    \end{cases}
  \end{equation}
  and
  \begin{equation}
    \label{eeta}
    \tilde{\eta}_1 = \max\left\{0, \tau -  \sum_{j=1}^s
      e_j\tilde{z}_j\right \},
    \quad
    \tilde{\eta}_2 = \max\left\{0,   \sum_{j=1}^s
      e_j\tilde{z}_j - \tau\right \},
  \end{equation}
  for Problem~\eqref{equation3} with $c^1,\dots, c^{k^{t+1}}$ and
  $e_1, \dots e_{k^{t+1}}$ as updated in iteration $t$ with
  \begin{equation}  \label{BigM 2}
    M =  2 \sqrt{2\tilde{f_t}} \max_{i \in [1,N]} \Vert x^i \Vert + 1.
  \end{equation}
\end{thm}
\begin{proof}
  Consider $\tilde{z}$ as given in \eqref{ez} and $\tilde{\eta}_1,
  \tilde{\eta}_2$ as given in \eqref{eeta}.
  We now show  that $(\bar{\omega}^t,\bar{b}^t, \bar{\xi}^t, \tilde{z},
  \tilde{\eta} )$ is a feasible point for Problem
  \eqref{equation3}. Indeed, Constraints~\eqref{constraintx} and
  \eqref{constraints1}--\eqref{constraintz}  are clearly
  satisfied.
  Moreover, $(\bar{\omega}^t,\bar{b}^t, \bar{\xi}^t, \bar{\eta}^t,
  \bar{z}^t)$ provides the objective function value given
  by \eqref{upperf} and
  \begin{equation*}
    \Vert \bar{\omega}^t \Vert \leq \sqrt{2\tilde{f}_t },
    \quad
    \vert \bar{b}^t \vert \leq  \Vert \bar{\omega}^t \Vert  \max_{i \in
      [1,N]} \Vert x^i \Vert +1,
  \end{equation*}
  see the proof of Lemma \ref{theob}.
  This together with $\Vert c^j \Vert \leq \max_{i \in [n+1,N]}\Vert
  x^i \Vert$ implies
  \begin{equation*}
    \left(\bar{\omega}^t\right)^\top  c^j + \bar{b}^t \leq \Vert
    \bar{\omega}^t \Vert \max_{i \in [n+1,N]}\Vert x^i \Vert + \vert
    \bar{b}^t \vert \leq 2 \sqrt{2\tilde{f}_t} \max_{i \in [1,N]}
    \Vert x^i \Vert +1 = M
  \end{equation*}
  and
  \begin{equation*}
  \left(\bar{\omega}^t\right)^\top  c^j + \bar{b}^t \geq -M.
  \end{equation*}
  Hence, Constraints \eqref{constraintc1} and \eqref{constraintc2} are
  satisfied. Since $(\bar{\omega}^t, \bar{b}^t, \bar{\xi}^t, \tilde{z},
  \tilde{\eta})$ is a feasible point for Problem~\eqref{equation3},
  $\tilde{f}_t$ is an upper bound for Problem~\eqref{equation3}.
\end{proof}

Using Theorem~\ref{isaupper}, we can update $M$ in each
iteration of Algorithm~\ref{Second version} as in~\eqref{BigM 2}.
The following theorem establishes that as Algorithm~\ref{First
  version}, Algorithm~\ref{Second version} always terminates after
finitely many iterations.

\begin{thm}
  \label{algo2end}
  The Algorithm~\ref{Second version} terminates after at most $$2m-k^1 +
  \frac{(1-\hat{\Delta}^1)}{\tilde{\Delta}} $$ iterations, where $m$
  is the number of unlabeled data points, $k^1$ is the number of
  initial clusters, and $\hat{\Delta}^1, \tilde{\Delta}$ are inputs of
  Algorithm~\ref{Second version}.
\end{thm}
\begin{proof}
  In Algorithm~\ref{Second version}, the number of iterations can only
  be greater as in Algorithm~\ref{First version} if there is some iteration
  $t$ for which $\mathcal{J}^t \neq \emptyset$ holds but the hyperplane
  does not cut any cluster. At each iteration in which this happens,
  $\hat{\Delta}^t$ is increased and, in the worst case, i.e.,
  \begin{equation*}
    \hat{t} \define  m-k^1 + \frac{(1-\hat{\Delta}^1)}{\tilde{\Delta}},
  \end{equation*}
  we get $\hat{\Delta}^{\hat{t}} = 1$. This implies that for all
  further iterations $t$,
  \begin{equation*}
    \Delta^t = \max_{j \in [1,k^t]} \vert (\omega^t)^\top c^j + b^t\vert
  \end{equation*}
  holds.
  Thus, no cluster is added to the set $\mathcal{G}^t$.
  Since $\vert \mathcal{G}^{\hat{t}} \vert \leq m$ and $\mathcal{J}^t
  \subset \mathcal{G}^{\hat{t}}$, Algorithm~\ref{Second version} can
  only have $m$ more iterations with $\mathcal{J}^t \neq \emptyset$.
  This means that the maximum number of iterations is $2m - k^1 +
  (1-\hat{\Delta}^1)/\tilde{\Delta}$.
\end{proof}

Note that the objective function value obtained by
Algorithm~\ref{Second version} is an upper bound for the objective
function value of Problem~\eqref{equation2}.

\begin{thm}\label{algo2up}
  Let $(\bar{\omega}, \bar{b}, \bar{\xi}, \bar{\eta}, \bar{z})$
  be the point found by Algorithm~\ref{Second version}.
  Then
  \begin{equation*}
    \bar{f} \define \frac{\Vert\bar{\omega} \Vert ^ 2
    }{2} + C_1\sum_{i=1}^n\bar{ \xi_i} +C_2 (\bar{\eta}_1 +
    \bar{\eta}_2)
  \end{equation*}
  is an upper bound of Problem~\eqref{equation2} with $M =
  2\sqrt{2\bar{f}}\max_{i \in [1,N]} \Vert x^i \Vert  + 1$.
\end{thm}
\begin{proof}
  Since Algorithm~\ref{Second version} terminates when no cluster
  changes the side and no cluster is cut by the hyperplane, the proof
is the same as for Theorem~\ref{isaupper}.
\end{proof}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "constrained-svm-preprint"
%%% End:
