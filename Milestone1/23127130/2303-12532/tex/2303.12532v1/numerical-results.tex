\section{Numerical Results}
\label{sec:numerical-results}

In this section, we present and discuss our computational results that
illustrate the benefits of knowing the total amount of each class of
unlabeled data and of using our approaches to speed up the solution
process. We evaluate this on different test sets from the
literature. The test sets  are described in Section
\ref{subsection-test-sets}, while the computational setup is depicted
in Section~\ref{subsection-comp-setup}. The evaluation criteria are
described in Section~\ref{comparsions-SVM} and the numerical results
are discussed in Section~\ref{numericalresults}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Test Sets}
\label{subsection-test-sets}

For the computational analysis of the proposed approaches, we consider
the subset of instances presented by \textcite{Olson2017PMLB} that are
suitable for classification problems and that have at most three
classes. We restrict ourselves to instances of at most three classes
to obtain an overall test set of manageable size.
Repeated instances are removed and instances with missing information
are reduced to the observations without missing information.
If three classes are given in an instance, we transform them into two
classes such that the class with label~1 represents the
positive class, and the other two classes represent the negative
class.
This results in the 97~instances that are listed in Table~\ref{table1}
including their size and their dimension.

To avoid numerical instabilities, we re-scale all data sets as
follows.
For each coordinate $j \in [1,d]$, we compute
\begin{equation*}
  l_j = \min_{i \in [1,N]}\{x^i_j\},
  \quad
  u_j = \max_{i \in [1,N]}\{x^i_j\},
  \quad
  m_j = 0.5 \left(l_j + u_j \right)
\end{equation*}
and shift each coordinate~$j$ of all data points~$x^i$ via $\bar{x}^i_j
= x^i_j - m_j$. If we do this for all data points, they get
centered around the origin. Moreover, if a coordinate~$j$ of the
re-scaled points is still large, i.e.,  if $\tilde{l}_j = l_j - m_j <-
10^{2}$ or $\tilde{u}_j = u_j - m_j > 10^{2}$ holds, it is re-scaled
via
\begin{equation*}
  \tilde{x}^i_j = (\overline{v} - \underline{v} ) \frac{\bar{x}^i_j -
    \tilde{l}_j}{\tilde{u}_j-\tilde{l}_j} +  \overline{v},
\end{equation*}
with $\overline{v} = 10^2$ and $ \underline{v} = -10^{2}$.
The corresponding 29 instances that we re-scaled are marked with an
asterisk in Table~\ref{table1}.
%
\input{instance-table}
%
In our computational study, we want to highlight the importance of
cardinality constraints, especially for the case of non-representative
biased samples. Biased samples occur frequently in non-probability
surveys, which are surveys for which the inclusion process is not
monitored and, hence, the inclusion probabilities are unknown as
well.
Correction methods like inverse inclusion probability weighting are
therefore not applicable. For an insight into inverse inclusion
probability weighting, see \textcite{Skinner2011} and references
therein.

To mimic this situation, we create 5~biased samples with
$\SI{10}{\percent}$ of the data being labeled for each instance.
Different from a simple random sample in which each point has an equal
probability of being chosen as labeled data, in the biased sample, the
labeled data is chosen with probability $\SI{85}{\percent}$ for being
on the positive side of the hyperplane. Then, for each instance, with
a time limit of \SI{3600}{\second}, we apply the approaches listed
in Section~\ref{subsection-comp-setup}.
In Appendix~\ref{sec:num-results-simple-sample}, we also provide
the results under simple random sampling, which produces unbiased
samples. We see that the results form the proposed methods are similar
to the plain SVM in that setting. Hence, besides the additional
computational burden, there is no downside to use the proposed method in
case of an unknown sampling process.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Computational Setup}
\label{subsection-comp-setup}

Our algorithm has been implemented in \codename{Julia}~1.8.5 and we use
\codename{Gurobi}~9.5.2 and
\codename{JuMP} \parencite{DunningHuchetteLubin2017} to solve
Problem~\eqref{l2svm}, \eqref{equation2}, and \eqref{equation3}.  All
computations were executed on the high-performance cluster
``Elwetritsch'', which is part of the ``Alliance of High-Performance
Computing Rheinland-Pfalz'' (AHRP).
We used a single Intel XEON SP 6126 core with \SI{2.6}{\giga\hertz} and
\SI{64}{\giga\byte}~RAM.

For each one of the 485~instances described in
Section~\ref{subsection-test-sets}, the following approaches
are compared:
\begin{enumerate}
\item[(a)] SVM as given in Problem~\eqref{l2svm}, where only labeled
  data are considered;
\item[(b)] CS$^3$VM as given in Problem~\eqref{equation2} with $M$ as given
  in~\eqref{validM};
\item[(c)] IRCM as described in Algorithm~\ref{Second version};
\item[(d)] WIRCM as described in Algorithm~\ref{Scheme version}.
\end{enumerate}
Based on our preliminary experiments, we set the penalty parameters
$C_1 = C_2 = 1$.
For WIRCM, we impose a time limit for solving
Problem~\eqref{equation5} of $T_{\max} = \SI{40}{\second}$.
Moreover, we choose $\gamma = 1.2$ and the maximum number~$B_{\max}$
of unlabeled points that can be fixed as
\begin{equation*}
  B_{\max} =
  \begin{cases}
    0.2m,  & \text{if } m \in  [1,100],\\
    0.25m,  & \text{if } m \in  (100,500],\\
    0.35m, & \text{if } m \in  (500,1000],\\
    0.45m,  & \text{otherwise}.
  \end{cases}
\end{equation*}
Finally, for IRCM and WIRCM, we set $\hat{\Delta}_0 = 0.8$,
$\tilde{\Delta} = 0.1$, $k^+ = 50$, and the initial number of clusters
is set to
\begin{equation*}
  k^1 =
  \begin{cases}
    10, & \text{if } m \in  [1,500],\\
    20, & \text{if } m \in  (500,1000],\\
    50, & \text{otherwise}.
  \end{cases}
\end{equation*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluation Criteria}
\label{comparsions-SVM}

The first evaluation criterion is the run time of CS$^3$VM, IRCM, and
WIRCM.
We do not compare the run time to solve the SVM problem with the
mentioned three approaches because it is a different problem.
The results will help to contextualize other evaluation criteria such
as accuracy and precision.
To compare run times, we use empirical cumulative distribution
functions (ECDFs).
Specifically, for $S$ being a set of solvers (or approaches as above)
and for $P$ being a set of problems, we denote denote by $t_{p,s} \geq
0$ the run time of approach~$s \in S$ applied to problem~$p \in P$
in seconds. If $t_{p,s} > 3600$, we consider problem~$p$ as not being
solved by approach~$s$.
With these notations, the performance profile of approach~$s$ is the
graph of the function~$\gamma_s : [0, \infty) \to [0,1]$ given by
\begin{equation*}
  %\label{perf}
  \gamma_s(\sigma) = \frac{1}{\vert P \vert}\big\vert\left\{p \in P:
    t_{p,s} \leq \sigma \right\}\big \vert.
\end{equation*}
The second evaluation criterion is based on Theorem \ref{upperf},
where we show that the objective function value of the point obtained
by IRCM is an upper bound for CS$^3$VM, and consequently for
Problem~\eqref{equation3} that is solved with WIRCM.
Based on that, we compare how close the objective function values
obtained from CS$^3$VM, IRCM, and WIRCM are to the optimal solution.
To this end, we use ECDFs with
\begin{equation}
  \label{gap}
  t_{p,s} \define \frac{b_{p,s}-f^*_{p}}{f^*_{p}},
\end{equation}
where $f^*_{p}$ is the optimal objective function value of problem~$p$
and $b_{p,s}$ is the objective function value obtained by approach~$s$.

Besides that, for each instance and for each approach described in
Section~\ref{subsection-comp-setup}, after computing the hyperplane
$(\omega, b)$, we classify all points $x^i$ as being on the positive
side if $\omega^\top x^i + b > 0$ and as being on the negative side if
$\omega^\top x^i + b <0$ holds.
For CS$^3$VM and WIRCM, if the hyperplane $(\omega, b)$  satisfies
$\omega^\top x^i + b = 0$ for some unlabeled point $x^i$, we classify
this point as positive or negative depending on the respective binary
variable~$z_i$.
On the other hand, for IRCM, if $\omega^\top x^i + b = 0$  for some
unlabeled point $x^i$, we classify this point as positive or negative
depending on $z_j$ with $j$ so that $x^i \in \mathcal{C}_j$.
For the labeled points in these three approaches and for all points in
the SVM, if  $\omega^\top x^i + b = 0$ holds, we classify the point on the
correct side.
Note that for the cases in which the IRCMs take more than
\SI{3600}{\second} to solve the instance, we use the last hyperplane
found by the algorithm.
If we hit the time limit in \codename{Gurobi} when solving CS$^3$VM
(either standalone or in the final phase of the WIRCM),
we take the best solution found so far.

Knowing the true label of all points, we then distinguish all points in
four categories: true positive (TP) or true negative (TN) if the point is
classified correctly in the positive or negative class, respectively,
as well as false positive (FP) if the point is misclassified in the
positive class and as false negative (FN) if the point is misclassified
in the negative class.
Based on that we compute two classification metrics, for which a
higher value indicates a better classification.
The first one is accuracy ($\AC$).
It measures the proportion of correctly classified points and is given
by
\begin{equation}
  \label{AC}
  \AC \define \frac{\TP + \TN}{\TP + \TN + \FP + \FN} \in [0,1].
\end{equation}
The second metric is precision ($\PR$).
It measures the proportion of correctly classified points among all
positively classified points and is computed by
\begin{equation}
  \label{prec}
  \PR \define \frac{\TP}{\TP + \FP} \in [0,1].
\end{equation}

The main comparison in terms of accuracy and precision is w.r.t.\ the
``true hyperplane'', i.e., the solution of Problem~\eqref{l2svm} on the
complete data with all $N$ points and all labels available. The main
question is how close the accuracy and precision is to the one of the
true hyperplane. Hence, we compute the ratios of the accuracy and
precision according to
\begin{equation}\label{compartrue}
  \widehat{\AC} \define \frac{\AC}{\AC_{\true}},
  \quad
  \widehat{\PR} \define \frac{\PR}{\PR_{\true}},
\end{equation}
where $\AC_{\true}$ and $\PR_{\true}$ are computed as in Equations
\eqref{AC} and \eqref{prec} for the true hyperplane.

We also compare the measures with the SVM method, which only considers
the information of the labeled data. For this purpose, we compute
\begin{equation}
  \label{comparSVM}
  \overline{\AC} \define \frac{\AC-\AC_{\SVM}}{\AC_{\SVM}},
  \quad
  \overline{\PR} \define \frac{\PR-\PR_{\SVM}}{\PR_{\SVM}},
\end{equation}
where $\AC_{\SVM}$ and $\PR_{\SVM}$ are computed as in~\eqref{AC}
and~\eqref{prec} for the SVM hyperplane. To keep the numerical results section concise, we report on recall and the false positive rate in Appendix~\ref{sec:furth-numer-results}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Numerical Results}
\label{numericalresults}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Run Time}
\label{Computation time}

Figure~\ref{perfomancetime} shows the ECDFs for the measured run
times.
It can be seen that the IRCM outperforms both CS$^3$VM and WIRCM.
This shows that the idea to cluster unlabeled data points
significantly decreases the run time.
However, we need to be careful with the interpretation of these run
times since termination of the IRCM does not imply that a globally optimal
point is found, whereas this is guaranteed CS$^3$VM and the WIRCM.
The quality of the points found by the IRCM will be discussed in the
next section.
The figure also clearly indicates that Problem~\eqref{EQproblem1} is
rather challenging:
Even IRCM, which terminates for the most instances within the time
limit (indicated by the gray and dashed vertical line) only does so
for $\SI{57}{\percent}$ of the instances.
Note that the WIRCM has the worst efficiency.
This obviously needs to be the case since due to Step~\ref{Scheme
  version:call-other-alg} of Algorithm~\ref{Scheme version}, its run
time always includes the run time of the IRCM.

\begin{figure}
  \centering
  \includegraphics[width=0.6\textwidth]{figures/PerformanceProfileTime}
  \caption{ECDFs for run time (in seconds).}
  \label{perfomancetime}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Quality of the Obtained Upper Bounds}

\begin{figure}
  \centering
  \includegraphics[width=0.6\textwidth]{figures/PerformanceProfileFunction}
  \caption{ECDFs for the quality of the obtained upper bounds.}
  \label{perfomanceobjectivefunction}
\end{figure}

As discussed in the last section, for some instances none of the three
approaches terminate within the given time limit. This means we do not
obtain the optimal objective function value for these instances, which
we, moreover, can only provably obtain by CS$^3$VM and the WIRCM.
In fact, we have the optimal solution for 179~instances. These are
the baseline instances for Figure~\ref{perfomanceobjectivefunction}, which shows
the ECDFs for the upper bound quality, as defined by using $t_{p,s}$
in~\eqref{gap}.
Note that the IRCM finds an objective function value rather close
to the optimal value (with $t_{ps} \leq 0.2$, see the gray dashed
vertical line) in $\SI{90}{\percent}$ of these instances.
Besides that, the WIRCM outperforms CS$^3$VM in this comparison,
which means using the IRCM as a warm start improves the result.

The consequences of the results so far are the following.
If one is interested in getting a rather good feasible point as
quickly as possible, one should use the IRCM.
If one is able to spend some more run time, one should use the WIRCM.
Hence, both novel methods derived in this paper have their advantage
over just solving the CS$^3$VM with a standard MIQP solver.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Accuracy}

As can be seen in Figure~\ref{ACtru}, the relative
accuracy~$\widehat{\AC}$ (w.r.t.\ the true hyperplane) of CS$^3$VM,
the IRCM, and the WIRCM are closer to one than the relative accuracy
of SVM---especially for the unlabeled data.
That is, our approaches re-produce the classification of the true
hyperplane with higher accuracy than the standard SVM does.
Hence, all newly proposed methods outperform the standard SVM approach
in this setting.
Besides that, the relative accuracy of the SVM is more spread than the
one of the other algorithms, indicating that there is comparable more
variation in the results as compared the results of the three novel approaches.

\begin{figure}
  \centering
  \includegraphics[width=0.495\textwidth]{figures/AccuracybasedontrueALL}
  \includegraphics[width=0.495\textwidth]{figures/AccuracybasedontrueUN}
  \caption{Relative accuracy $\widehat{\AC}$ w.r.t.\ the true
    hyperplane; see~\eqref{compartrue}.
    Left: Comparison for all data points.
    Right: Comparison only for unlabeled data points.}
  \label{ACtru}
\end{figure}

The box in the boxplot depicts the range of the central
\SI{50}{\percent} of the values; \SI{25}{\percent} of the values are
below and \SI{25}{\percent} are above the box. As can be seen in
Figure~\ref{ACSVM}, in almost \SI{75}{\percent} of the  cases,
CS$^3$VM, the IRCM, and the WIRCM have $\overline{\AC}$ values larger
than zero, where zero means same accuracy as the SVM itself. That is,
in general, our methods have greater accuracy than the SVM. Though, some
cases indicate worse $ \overline{\AC} $ for our methods than for the SVM. This
happens because for some instances, the methods (mainly for CS$^3$VM;
see also Figure~\ref{perfomancetime}) do not terminate within the time
limit. Hence, we expect that the number of negative values will
decrease if we increase the time limit.

\begin{figure}
  \centering
  \includegraphics[width=0.495\textwidth]{figures/AccuracybasedonSVMALL}
  \includegraphics[width=0.495\textwidth]{figures/AccuracybasedonSVMUN}
  \caption{Accuracy values $\overline{\AC}$ w.r.t.\ the SVM; see
    \eqref{comparSVM}.
    Left: Comparison for all data points.
    Right: Comparison only for unlabeled data points.}
  \label{ACSVM}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Precision}\label{precisionsec}

As can be seen in Figure~\ref{PRtru}, the SVM's relative precision
$\widehat{\PR}$ is lower than the relative precision of the newly
proposed methods.
This means that the new approaches re-produce the classification of
the true hyperplane with higher precision than the original SVM.
Hence, SVM has more false-positive results.
This happens because the biased sample is more likely to have
positively labeled data and due to having no information about the
unlabeled data, the SVM ends up classifying points on the positive side.
On the other hand, the $\widehat{\PR}$ values of the IRCM and the WIRCM
are less spread than the ones of CS$^3$VM.
The reason most likely is that the CS$^3$VM approach terminates on fewer
instances than the IRCM and the WIRCM.
Moreover, as it was the case for the relative accuracy
$\widehat{\AC}$, the relative precision $\widehat{\PR}$ of the SVM is
more spread than those of the other algorithms.

\begin{figure}
  \centering
  \includegraphics[width=0.495\textwidth]{figures/PrecisionbasedontrueALL}
  \includegraphics[width=0.495\textwidth]{figures/PrecisionbasedontrueUN}
  \caption{Relative precision $\widehat{\PR}$ w.r.t.\ the true
    hyperplane as; see \eqref{compartrue}.
    Left: Comparison for all data points.
    Right: Comparison only for unlabeled data points.}
  \label{PRtru}
\end{figure}

Finally, Figure~\ref{PRSVM} shows that CS$^3$VM, the IRCM, and the
WIRCM have slightly higher $\overline{\PR}$ values than 0, which is the
baseline here that refers to the SVM itself.
This means, our methods are slightly more precise than the SVM.
The negative outliers most likely are due to the same reason as those
for the respective accuracy values.

\begin{figure}
  \centering
  \includegraphics[width=0.495\textwidth]{figures/PrecisionbasedonSVMALL}
  \includegraphics[width=0.495\textwidth]{figures/PrecisionbasedonSVMUN}
  \caption{Precision values $\overline{\PR}$ w.r.t.\ the SVM; see
    \eqref{comparSVM}.
    Left: Comparison for all data points.
    Right: Comparison only for unlabeled data points.}
  \label{PRSVM}
\end{figure}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "constrained-svm-preprint"
%%% End:
