\section{An MIQP Formulation for a Cardinality-Constrained
  Semi-Supervised SVM}
\label{sec:miqp-formulation}

Let $X\in \R^{d \times N}$ be the data matrix with $X_l =
[x^1, \dotsc, x^n]$ being the labeled data and $X_u = [x^{n+1},
  \dotsc, x^N]$ being the unlabeled data. Hence, we have $x^i \in
  \mathbb{R}^d$ for all $i \in [1,N] \define \set{1,\dots, N}$.
We set $m \define N - n$ and $y \in \set{-1,1}^n$ is the vector of
class labels for the labeled data.
When the data is linearly separable, the SVM provides a
hyperplane $(\omega, b)$ that separates the positively and negatively
labeled data.
In the case that the data is not linearly separable, the standard
approach is to use the $\ell_2$-SVM by \textcite{cortes1995support}
given by
%
\begin{varsubequations}{P1}
  \label{l2svm}
  \begin{align}
    \min_{\omega,b,\xi} \quad
    & \frac{\Vert\omega \Vert^2}{2} + C_1 \sum_{i=1}^n \xi_i
    \\
    \st \quad
    & y_i (\omega^\top x^i -b) \geq 1 - \xi_i, \quad i \in [1,n],
    \\
    & \xi_i \geq 0, \quad i \in [1, n].
  \end{align}
\end{varsubequations}
Here and in what follows, $\Vert \cdot \Vert$ denotes the Euclidean norm.
For being able to include unlabeled data in the optimization process,
\textcite{Bennett1998SemiSupervisedSV} propose the semi-supervised SVM
(S$^3$VM).
In many applications, the aggregated information on the labels is
available, e.g., from census data.
In the following, we know the total number~$\tau$ of positive labels
for the unlabeled data from an external source.
We adapt the idea of the S$^3$VM such that we can use $\tau$ as
an additional information in the optimization model.
Our goal is to find optimal parameters $\omega^* \in \R^d$,
$b^* \in \R$, $\xi^* \in \R^n$, and $\eta^* \in \R^2$ that solve the
optimization problem
\begin{varsubequations}{P2}
  \label{EQproblem1}
  \begin{align}
    \min_{\omega,b,\xi,\eta} \quad
    & \frac{\Vert\omega \Vert^2}{2} + C_1 \sum_{i=1}^n
      \xi_i +C_2 (\eta_1 + \eta_2) \label{svmfunction}
    \\
    \st \quad
    & y_i (\omega^\top x^i -b) \geq 1 - \xi_i, \quad i \in
      [1,n], \label{labeledpart}
    \\
    & \tau - \eta_ 1 \leq  \sum_{i=n+1}^N h_{\omega,b}(x^i) \leq \tau +
      \eta_2,
      \label{unlabeled part} \\
    & \xi_i  \geq 0, \quad i \in [1, n] \label{xi1}, \\
    & \eta_1, \eta_2 \geq 0, \label{eta1}
  \end{align}
\end{varsubequations}
where
\begin{equation*}
  h_{\omega,b}(x) =
  \begin{cases}
    1, & \text{if } \omega^\top x + b \geq 0,\\
    0, & \text{otherwise}.
  \end{cases}
\end{equation*}
Note that the objective function in \eqref{svmfunction} is a
compromise between maximizing the distance between the two classes as
well as minimizing the classification error for the label and the
unlabeled data.
The penalty parameters $C_1 > 0 $ and $C_2>0$ aim to control the
importance of the slack variables $\xi$ and $\eta$, respectively.
Constraint~\eqref{labeledpart} enforces on which side of the
hyperplane the labeled data $x^i$ should lie.
Constraint~\eqref{unlabeled part} ensures that we have $\tau$
unlabeled data on the positive side.
Note that, having assigned a very high value to $C_1$ or $C_2$, the
objective function value is dominated by these slack variables.

The function $h_{\omega,b}(\cdot)$ in Constraint~\eqref{unlabeled
  part} is not continuous, which means that Problem~\eqref{EQproblem1}
cannot be easily solved by standard solvers.
A typical way to overcome this problem is to add binary variables to
turn on or off the enforcement of a constraint.
By introducing binary variables $z_i \in \set{0,1}$, $i \in [n+1, N]$,
we can reformulate the optimization Problem~\eqref{EQproblem1} using
the following big-$M$ formulation:
\begin{varsubequations}{P3}
  \label{equation2}
  \begin{align}
    \min_{\omega,b,\xi,\eta, z} \quad
    & \frac{\Vert\omega \Vert^2}{2} + C_1 \sum_{i=1}^n
      \xi_i +C_2 (\eta_1 + \eta_2) \label{2}
    \\
    \st \quad
    & y_i (\omega^\top x^i +b) \geq 1 - \xi_i, \quad i \in
      [1,n],
      \label{3} \\
    &  \omega^\top x^i +b \leq z_i M, \quad i \in [n+1,N],
      \label{4}\\
    &  \omega^\top x^i +b \geq -(1-z_i)M, \quad i \in [n+1,N],
      \label{5}\\
    & \tau - \eta_1 \leq  \sum_{i=n+1}^N z_i \leq \tau + \eta_2,
      \label{6}
    \\
    & \xi_i  \geq 0, \quad i \in [1, n],
      \label{8} \\
    & \eta_1, \eta_2 \geq 0,
      \label{sumeta}\\
    & z_i \in \{0,1\}, \quad i \in [n+1, N],
      \label{10}
  \end{align}
\end{varsubequations}
where $M$ needs to be chosen sufficiently large.
As $z_i$ is binary, Constraints~\eqref{4} and~\eqref{5} lead to
\begin{align*}
  \omega^\top x^i +b > 0 \implies z_i = 1, \quad i \in [n+1,N],\\
  \omega^\top x^i +b < 0 \implies z_i = 0,\quad i \in [n+1,N].
\end{align*}
If $x^i$ lies on the hyperplane, i.e., $\omega^\top x^i +b =0 $,
Constraints~\eqref{4} and~\eqref{5} hold for $z_i = 1$ and $z_i = 0$.
In this case, it can be counted either on
the positive or on the negative side.
Reformulation \eqref{equation2} is a mixed-integer quadratic problem
(MIQP) in which all constraints are linear but the objective function
is quadratic.
We  refer to this problem as CS$^3$VM.
In the big-$M$ formulation, the choice of $M$ is crucial.
If $M$ is too small, the problem can become infeasible or optimal
solutions could be cut off.
If $M$ is chosen too large, the respective continuous relaxations
usually lead to bad lower bounds and solvers may encounter numerical
troubles.
The choice of~$M$ is discussed in the following lemma and theorem.
In Lemma~\ref{theob} we show how~$M$ is related to the objective
function and the given data.
This is then used in Theorem~\ref{theob2} to derive a provably correct
big-$M$.
%
\begin{lem}
  \label{theob}
  Given a feasible point for Problem~\eqref{equation2} with an
  objective function value~$f$, an optimal solution $(\omega^*, b^*,
  \xi^*, \eta^*, z^*)$ of \eqref{equation2} satisfies
  \begin{equation*}
    \label{boundomega}
    \Vert \omega^* \Vert \leq \sqrt{2f}
    \quad\text{and}\quad
    \vert b^* \vert \leq  \Vert \omega^* \Vert  \max_{i \in [1,N]} \Vert
    x^i \Vert + 1
  \end{equation*}
  and, consequently, every optimal solution satisfies \eqref{4} and
  \eqref{5} for
  \begin{equation*}
    \label{HOWM}
    M = 2 \sqrt{2f} \max_{i \in [1,N]} \Vert x^i \Vert + 1.
  \end{equation*}
\end{lem}
\begin{proof}
  Due to optimality, we get
  \begin{equation*}
    \frac{\Vert \omega^* \Vert^2}{2} \leq   \frac{\Vert
      \omega^* \Vert^2}{2} + C_1  \sum_{i = 1}^n
    \xi^*_i + C_2(\eta^*_1 + \eta^*_2)\leq f
    \implies \Vert \omega^* \Vert \leq \sqrt{2f}.
  \end{equation*}
  The second inequality is shown by contradiction.
  To this end, we w.l.o.g.\ assume that $\tilde{b} =
  \Vert \omega^* \Vert  \max_{i \in [1,N]} \Vert x^i \Vert +1 +
  \delta$ is part of an optimal solution for some $\delta > 0$.
  Using the inequality of Cauchy--Schwarz then yields
  \begin{align*}
    (\omega^{*})^\top x^i + \tilde{b}
    & =  (\omega^{*})^\top x^i + \Vert \omega^* \Vert  \max_{j \in
      [1,N]} \Vert x^j \Vert  +1 + \delta
    \\
    & \geq  - \Vert \omega^* \Vert \Vert x^i \Vert + \Vert \omega^*
      \Vert \max_{j  \in [1,N]} \Vert x^j \Vert +1 + \delta
    \\
    & >  1
  \end{align*}
  for all $i \in [1,N]$.
  Hence, for all $i \in [1,n]$ with $y_i = 1$, we get $\tilde{\xi}_i =
  0$ from Constraint~\eqref{3} and the objective function.
  Moreover, for $i \in [1,n]$ with $y_i = -1$, the same reasoning
  implies
  \begin{equation*}
    - (\omega^{*})^\top x^i - \tilde{b}  = 1 - \tilde{\xi}_i \implies
    \tilde{\xi}_i = 2 + (\omega^{*})^\top x^i +  \Vert \omega^* \Vert
    \max_{j  \in [1,N]} \Vert x^j \Vert + \delta.
  \end{equation*}
  Besides that, for the unlabeled data $i\in [n+1,N] $, since $
  (\omega^{*})^\top x^i + \tilde{b} >1$, we get $\tilde{z}_i = 1,$
  which leads to
  \begin{equation*}
    \sum_{i=n+1}^N \tilde{z}_i = m \implies
    \tilde{\eta}_1 = 0, \ \tilde{\eta}_2 = m -\tau.
  \end{equation*}
  This means that the objective function value for the point
  $(\omega^*,\tilde{b}, \tilde{\xi}, \tilde{\eta}, \tilde{z})$ is given by
  \begin{equation*}
    \tilde{f} \define \frac{\Vert \omega^* \Vert^2}{2} + C_1
    \sum_{i : y_i =-1} \left(2 + (\omega^{*})^\top x^i +
      \Vert \omega^* \Vert \max_{j  \in [1,N]} \Vert x^j \Vert + \delta \right)
    + C_2( m -\tau).
  \end{equation*}
  However, if we set  $\bar{b} \define \Vert \omega^* \Vert \max_{i  \in
    [1,N]} \Vert x^i \Vert +1, $ we get
  \begin{equation*}
    (\omega^{*})^\top x^i + \bar{b} \geq 1, \quad i \in [1,N],
  \end{equation*}
  i.e., $z_i = 1$ for all $i \in [n+1,N] $, $\bar{\eta}_1 = 0,$
  $\bar{\eta}_2 = m -\tau$,  and $\bar{\xi}_i = 0 $ for $i$ with $y_i
  = 1$.
  Moreover, for $i \in [1,n]$ with $y_i = -1$, from Constraint
  \eqref{3} we obtain
  \begin{equation*}
    - (\omega^{*})^\top x^i - \tilde{b}  = 1-\bar{\xi_i} \implies
    \bar{\xi_i} = 2 + (\omega^{*})^\top x^i +  \Vert \omega^* \Vert
    \max_{i  \in [1,N]} \Vert x^i \Vert.
  \end{equation*}
  All this implies that the objective function value $\bar{f}$ for the
  point  $(\omega^*,\bar{b}, \bar{\xi}, \bar{\eta}, \bar{z})$
  satisfies
  \begin{equation*}
    \bar{f} \define  \frac{\Vert \omega^* \Vert^2}{2} + C_1
    \sum_{i : y_i =-1}(2 + (\omega^{*})^\top x^i +
    \Vert \omega^* \Vert \max_{j  \in [1,N]} \Vert x^j \Vert ) \ +
    C_2( m -\tau) < \tilde{f},
  \end{equation*}
  which contradicts the assumption that $\tilde{f}$ is optimal.
  Hence,
  \begin{equation*}
    \vert b^* \vert
    \leq \Vert \omega^* \Vert \max_{i  \in [1,N]} \Vert x^i \Vert +1
  \end{equation*}
  holds, which proves the second inequality.
  Note further that
  \begin{equation*}
    (\omega^{*})^\top x^i + b^* \leq \Vert\omega^* \Vert
    \Vert x^i \Vert + \vert b^* \vert \leq  2\sqrt{2f} \max_{j
      \in [1,N]} \Vert x^j \Vert + 1 = M
  \end{equation*}
  and
  \begin{equation*}
    (\omega^{*})^\top x^i + b^* \geq - \Vert\omega^* \Vert
    \Vert x^i \Vert - \vert b^* \vert \geq  - 2\sqrt{2f}
    \max_{j \in [1,N]} \Vert x^j \Vert - 1 = -M
  \end{equation*}
  holds for all $i \in [n+1,N]$.
\end{proof}

We now use the result from the last technical lemma to obtain a
provably correct big-$M$.

\begin{thm}
  \label{theob2}
  A valid big-$M$ for Problem~\eqref{equation2} is given by
  \begin{equation}
    \label{validM}
    M = 2\sqrt{2(2C_1\bar{n} + C_2(m-\tau))}\max_{i \in [1,N]}\Vert x^
    i \Vert +1
  \end{equation}
  with $\bar{n} \define \vert \defset{i \in [1,n]}{ y_i = -1}\vert$.
\end{thm}
\begin{proof}
  Consider the feasible point of \eqref{equation2} given by $\omega = 0 \in \R^d$
  and $b=1$.
  Since \mbox{$\omega^\top x^i + b = 1$} holds for all $i\in[1,N]$,
  Constraint~\eqref{3} implies
  \begin{equation*}
    \xi_i =
    \begin{cases}
      2, & \text{if } y_i = -1 ,\\
      0, & \text{otherwise}.
    \end{cases}
  \end{equation*}
 Moreover, using Constraints~\eqref{4}--\eqref{6} leads to
  \begin{equation*}
    z_i = 1, \ i \in [n+1,N], \quad \eta_1 = 0, \quad \eta_2 = m - \tau,
  \end{equation*}
  which implies that the objective function for the point
  $(\omega, b, \xi, \eta, z)$ is given by
  \begin{equation*}
    f = 0 + 2C_1\bar{n} + C_2(m-\tau).
  \end{equation*}
  Finally, from Lemma~\ref{theob}, we get
  \begin{equation*}
    M = 2\sqrt{2(2C_1\bar{n} + C_2(m-\tau))}\max_{i \in [1,N]}\Vert x^
    i \Vert + 1. \qedhere
  \end{equation*}
\end{proof}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "constrained-svm-preprint"
%%% End:
