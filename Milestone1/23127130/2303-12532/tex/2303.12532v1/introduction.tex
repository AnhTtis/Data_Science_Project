\section{Introduction}
\label{sec:introduction}

Support vector machines (SVMs) are a standard approach for supervised
binary classification \parencite{boser1992,cortes1995support}. The
core idea is to find a separating hyperplane that optimally splits
the feature space in a positive and a negative side according to the
positive and negative labels of the data.

Obtaining labels for all units of interest can be costly. This is
especially the case if one has to do a classic survey to obtain the
labels. In this case, it would be favorable to train the SVM on only
partly labeled data. This yields a semi-supervised learning
setting. \textcite{Bennett1998SemiSupervisedSV}  formulate and solve
the semi-supervised SVM (S$^3$VM) as a mixed-integer linear problem
(MILP). Many strategies for solving S$^3$VM have been proposed in the
following decades such as the transductive approach (TSVM) by
\textcite{TSVM,TSVM2} or manifold regularization (LapSVM) by
\textcite{LAPSVM,LAPSVM2}. Some researchers also consider a balancing
constraint as done in meanS3VM by \textcite{KONTONATSIOS201767} and in
c$^3$SVM by \textcite{Chapelle}.
Moreover, the balancing constraint proposed by \textcite{Chapelle2}
enforces that the proportion of unlabeled and labeled data on both
sides is similar to the proportion given by the labeled data.

In many cases, however, the aggregated information about the number of
positive and negative cases in a population is known from an external
source. For example, in population surveys, there are population
figures from official statistics agencies. As another example, in
some businesses, the total amount of positive labels could be known
but not which customer has a positive or a negative label.
An intuitive example is a supermarket for which the amount of cash payments
is known. However, this information
is not ex-post attributable to the individual customers. We propose to
add this aggregated additional information to the optimization
model by imposing a cardinality constraint on the predicted labels
for the unlabeled data. As will be shown in our numerical experiments,
this improves the accuracy of the classification of the unlabeled
data. Furthermore, the inclusion of such a cardinality constraint is
very useful in the case in which the labeled data
is not a representative sample from the population. When obtaining the
labels from process data or from online surveys, the inclusion
process of the labeled data is generally not known. This is subsumed
under the non-probability sample. In this case, inverse inclusion
probability weighting, as typically done in survey sampling, is not
applicable.
By not controlling the inclusion process, strong over- or
under-coverage of relevant information in the data set is possible and
should be taken into account in the analysis.
Not accounting for possible biases in the data generally leads to
biased results.

We propose a big-$M$-based MIQP to solve the semi-supervised SVM
problem with a cardinality constraint for the unlabeled data.
The cardinality constraint helps to account for biased samples since
the number of positive predictions on the population is bounded by the
constraint.
The computation time for this MIQP grows rapidly with
the number of variables---especially for an increasing number of
integer variables. We develop an algorithm that uses a
clustering-based model reduction to reduce the computation
time. Similar reduction approaches can be found for the classic
SVM using, e.g., fuzzy clustering \parencite{Fuzzy1,Fuzzy2},
clustering-based convex hulls \parencite{CBCH}, and $k$-means
clustering \parencite{Kmeans1,kmeans2}.
We prove the correctness of our iterative clustering method and
further show that it computes feasible points for the original
problem.
Hence, it also delivers proper upper bounds.
Within our iterative approach, we additionally derive a scheme
for updating the required big-$M$ values and present tailored
dimension-reduction as well as warm-starting techniques.

The paper is organized as follows.
In Section~\ref{sec:miqp-formulation}, we describe our optimization
problem and the big-$M$-based MIQP formulation.
Afterward, the clustering-based model reduction technique is presented
in Section~\ref{sec:cluster-unlabeled-points}. There, we also present
our algorithm that combines the model reduction and the MIQP
formulation. In Section~\ref{Some Improvements}, we discuss some
algorithmic improvements such as the handling of data points that are
far away from the hyperplane and the choice of $M$ in the big-$M$
formulation. In Section~\ref{section c2svm3+}, we present how to use
the solution of our algorithm to obtain the solution of the initial
MIQP formulation by fixing some points on the correct side of the hyperplane.
Finally, in Section~\ref{sec:numerical-results}, numerical results are
reported and discussed and we conclude in Section~\ref{sec:conclusion}.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "constrained-svm-preprint"
%%% End:
