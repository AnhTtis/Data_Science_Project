\section{Numerical Results}
\label{sec:numerical-results}

In this section, we present and discuss our computational results that
illustrate the benefits of knowing the total amount of each class of
unlabeled data and of using our approaches to speed up the solution
process. We evaluate this on different test sets from the
literature. The test sets  are described in Section
\ref{subsection-test-sets}, while the computational setup is depicted
in Section~\ref{subsection-comp-setup}. The evaluation criteria are
described in Section~\ref{comparsions-SVM} and the numerical results
are discussed in Section~\ref{numericalresults}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Test Sets}
\label{subsection-test-sets}

For the computational analysis of the proposed approaches, we consider
the subset of instances presented by \textcite{Olson2017PMLB} that are
suitable for classification problems and that have at most three
classes. We restrict ourselves to instances of at most three classes
to obtain an overall test set of manageable size.
Repeated instances are removed and instances with missing information
are reduced to the observations without missing information.
If three classes are given in an instance, we transform them into two
classes such that the class with label~1 represents the
positive class, and the other two classes represent the negative
class.
\rev{This results in a final test set of 97~instances; see
  Table~\ref{table1} in Appendix~\ref{sec:deta-inform-inst}.}

To avoid numerical instabilities, we re-scale all data sets as
follows.
For each coordinate $j \in [1,d]$, we compute
\begin{equation*}
  l_j = \min_{i \in [1,N]}\{x^i_j\},
  \quad
  u_j = \max_{i \in [1,N]}\{x^i_j\},
  \quad
  m_j = 0.5 \left(l_j + u_j \right)
\end{equation*}
and shift each coordinate~$j$ of all data points~$x^i$ via $\bar{x}^i_j
= x^i_j - m_j$. If we do this for all data points, they get
centered around the origin. Moreover, if a coordinate~$j$ of the
re-scaled points is still large, i.e.,  if $\tilde{l}_j = l_j - m_j <-
10^{2}$ or $\tilde{u}_j = u_j - m_j > 10^{2}$ holds, it is re-scaled
via
\begin{equation*}
  \tilde{x}^i_j = (\overline{v} - \underline{v} ) \frac{\bar{x}^i_j -
    \tilde{l}_j}{\tilde{u}_j-\tilde{l}_j} +  \overline{v},
\end{equation*}
with $\overline{v} = 10^2$ and $ \underline{v} = -10^{2}$.
The corresponding 29 instances that we re-scaled are marked with an
asterisk in Table~\ref{table1}.
\rev{Note that we use a linear transformation to scale the
  datasets. Hence, after computing the hyperplane for the
  scaled data, the respective hyperplane for the original data can
  also be computed ex post by applying another suitably chosen linear
  transformation as well.}

In our computational study, we want to highlight the importance of
cardinality constraints, especially for the case of non-representative
biased samples. Biased samples occur frequently in non-probability
surveys, which are surveys for which the inclusion process is not
monitored and, hence, the inclusion probabilities are unknown as
well.
Correction methods like inverse inclusion probability weighting are
therefore not applicable. For an insight into inverse inclusion
probability weighting, see \textcite{Skinner2011} and references
therein.

To mimic this situation, we create 5~biased samples with
$\SI{10}{\percent}$ of the data being labeled for each instance.
Different from a simple random sample in which each point has an equal
probability of being chosen as labeled data, in the biased sample, the
labeled data is chosen with probability $\SI{85}{\percent}$ for being
on the positive side of the hyperplane. Then, for each instance, with
a time limit of \SI{3600}{\second}, we apply the approaches listed
in Section~\ref{subsection-comp-setup}.
In Appendix~\ref{sec:num-results-simple-sample}, we also provide
the results under simple random sampling, which produces unbiased
samples. We see that the results form the proposed methods are similar
to the plain SVM in that setting. Hence, besides the additional
computational burden, there is no downside to use the proposed method in
case of an unknown sampling process.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Computational Setup}
\label{subsection-comp-setup}

Our algorithm has been implemented in \codename{Julia}~1.8.5 and we use
\codename{Gurobi}~9.5.2 and
\codename{JuMP} \parencite{DunningHuchetteLubin2017} to solve
Problem~\eqref{l2svm}, \eqref{equation2}, and \eqref{equation3}.  All
computations were executed on the high-performance cluster
``Elwetritsch'', which is part of the ``Alliance of High-Performance
Computing Rheinland-Pfalz'' (AHRP).
We used a single Intel XEON SP 6126 core with \SI{2.6}{\giga\hertz} and
\SI{64}{\giga\byte}~RAM.

For each one of the 485~instances described in
Section~\ref{subsection-test-sets}, the following approaches
are compared:
\begin{enumerate}
\item[(a)] SVM as given in Problem~\eqref{l2svm}, where only labeled
  data are considered;
\item[(b)] CS$^3$VM as given in Problem~\eqref{equation2} with $M$ as given
  in~\eqref{validM};
\item[(c)] IRCM as described in Algorithm~\ref{Second version};
\item[(d)] WIRCM as described in Algorithm~\ref{Scheme version}.
\end{enumerate}
Based on our preliminary experiments, we set the penalty parameters
$C_1 = C_2 = 1$.
For WIRCM, we impose a time limit for solving
Problem~\eqref{equation5} of $T_{\max} = \SI{40}{\second}$.
Moreover, we choose $\gamma = 1.2$ and the maximum number~$B_{\max}$
of unlabeled points that can be fixed as
\begin{equation*}
  B_{\max} =
  \begin{cases}
    0.2m,  & \text{if } m \in  [1,100],\\
    0.25m,  & \text{if } m \in  (100,500],\\
    0.35m, & \text{if } m \in  (500,1000],\\
    0.45m,  & \text{otherwise}.
  \end{cases}
\end{equation*}
Finally, for IRCM and WIRCM, we set $\hat{\Delta}\rev{^1} = 0.8$,
$\tilde{\Delta} = 0.1$, $k^+ = 50$, and the initial number of clusters
is set to
\begin{equation*}
  k^1 =
  \begin{cases}
    10, & \text{if } m \in  [1,500],\\
    20, & \text{if } m \in  (500,1000],\\
    50, & \text{otherwise}.
  \end{cases}
\end{equation*}
\rev{A more detailed discussion of the choice of hyperparameters is
  given in Appendix~\ref{sec:sensibilty-parameters}.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluation Criteria}
\label{comparsions-SVM}

The first evaluation criterion is the run time of \rev{SVM, }CS$^3$VM,
IRCM, and WIRCM.
The results will help to contextualize other evaluation criteria such
as accuracy and precision.
To compare run times, we use empirical cumulative distribution
functions (ECDFs).
Specifically, for $S$ being a set of solvers (or approaches as above)
and for $P$ being a set of problems, we denote denote by $t_{p,s} \geq
0$ the run time of approach~$s \in S$ applied to problem~$p \in P$
in seconds. If $t_{p,s} > 3600$, we consider problem~$p$ as not being
solved by approach~$s$.
With these notations, the performance profile of approach~$s$ is the
graph of the function~$\gamma_s : [0, \infty) \to [0,1]$ given by
\rev{
\begin{equation}
  \label{perf}
  \gamma_s(\sigma) = \frac{1}{\vert P \vert}\big\vert\left\{p \in P:
    t_{p,s} \leq \sigma \right\}\big \vert.
\end{equation}}%
The second evaluation criterion is based on Theorem \ref{upperf},
where we show that the objective function value of the point obtained
by IRCM is an upper bound for CS$^3$VM, and consequently for
Problem~\eqref{equation3} that is solved with WIRCM.
\rev{Note that SVM also provides a feasible point for CS$^3$VM and,
  consequently, provides an upper bound as well.
  Consider $(\omega,b, \xi)$ the solution of SVM, we compute the
  binary variables $z_i$, $i\in [n+1,N]$ as follows:}
\begin{equation*}
\rev{  z_i =
  \begin{cases}
    1, & \text{if } \omega^\top x^i + b >0,\\
   0, & \text{if } \omega^\top x^i + b <0. \\
  \end{cases}}
\end{equation*}
\rev{If $\omega^\top x^i + b =0$ for some $x^i$, we set}
\begin{equation*}
  \rev{  z_i =
    \begin{cases}
      1, & \text{if } \sum_{j \in [n+1,N] :  \omega^\top x^j + b \neq 0 } z_i \leq \tau ,\\
      0, & \text{otherwise.}
    \end{cases}}
\end{equation*}
\rev{Finally, we set}
$$
\rev{
  \eta_1 = \max\left\{0, \tau -  \sum_{i=n+1}^N
    z_i\right \},
  \quad
  \eta_2 = \max\left\{0,    \sum_{i=n+1}^N
    z_i - \tau\right \},}
$$
\rev{and the objective function value can be computed as}
$$
\rev{ \frac{\Vert\omega \Vert ^ 2  }{2} + C_1  \sum_{i=1}^n
  \xi_i +C_2 (\eta_1 + \eta_2).
}
$$

Based on that, we compare how close the objective function values
obtained from \rev{SVM,} CS$^3$VM, IRCM, and WIRCM are to the optimal
solution.
To this end, we use ECDFs, \rev{for which we replace $t_{p,s}$ by
  $f_{p,s}$ in Equation~\eqref{perf}} with
\begin{equation}
  \label{gap}
  \rev{f}_{p,s} \define \frac{b_{p,s}-f^*_{p}}{f^*_{p}},
\end{equation}
where $f^*_{p}$ is the optimal objective function value of problem~$p$
and $b_{p,s}$ is the objective function value obtained by approach~$s$.

Besides that, for each instance and for each approach described in
Section~\ref{subsection-comp-setup}, after computing the hyperplane
$(\omega, b)$, we classify all points $x^i$ as being on the positive
side if $\omega^\top x^i + b > 0$ and as being on the negative side if
$\omega^\top x^i + b <0$ holds.
For CS$^3$VM and WIRCM, if the hyperplane $(\omega, b)$  satisfies
$\omega^\top x^i + b = 0$ for some unlabeled point $x^i$, we classify
this point as positive or negative depending on the respective binary
variable~$z_i$.
On the other hand, for IRCM, if $\omega^\top x^i + b = 0$  for some
unlabeled point $x^i$, we classify this point as positive or negative
depending on $z_j$ with $j$ so that $x^i \in \mathcal{C}_j$.
For the labeled points in these three approaches and for all points in
the SVM, if  $\omega^\top x^i + b = 0$ holds, we classify the point on the
correct side.
Note that for the cases in which the IRCMs take more than
\SI{3600}{\second} to solve the instance, we use the last hyperplane
found by the algorithm.
If we hit the time limit in \codename{Gurobi} when solving CS$^3$VM
(either standalone or in the final phase of the WIRCM),
we take the best solution found so far.

Knowing the true label of all points, we then distinguish all points in
four categories: true positive (TP) or true negative (TN) if the point is
classified correctly in the positive or negative class, respectively,
as well as false positive (FP) if the point is misclassified in the
positive class and as false negative (FN) if the point is misclassified
in the negative class.
Based on that we compute two classification metrics, for which a
higher value indicates a better classification.
The first one is accuracy ($\AC$).
It measures the proportion of correctly classified points and is given
by
\begin{equation}
  \label{AC}
  \AC \define \frac{\TP + \TN}{\TP + \TN + \FP + \FN} \in [0,1].
\end{equation}
The second metric is precision ($\PR$).
It measures the proportion of correctly classified points among all
positively classified points and is computed by
\begin{equation}
  \label{prec}
  \PR \define \frac{\TP}{\TP + \FP} \in [0,1].
\end{equation}

The main comparison in terms of accuracy and precision is w.r.t.\ the
``true hyperplane'', i.e., the solution of Problem~\eqref{l2svm} on the
complete data with all $N$ points and all labels available. The main
question is how close the accuracy and precision is to the one of the
true hyperplane. Hence, we compute the ratios of the accuracy and
precision according to
\begin{equation}\label{compartrue}
  \widehat{\AC} \define \frac{\AC}{\AC_{\true}},
  \quad
  \widehat{\PR} \define \frac{\PR}{\PR_{\true}},
\end{equation}
where $\AC_{\true}$ and $\PR_{\true}$ are computed as in Equations
\eqref{AC} and \eqref{prec} for the true hyperplane.

We also compare the measures with the SVM method, which only considers
the information of the labeled data. For this purpose, we compute
\begin{equation}
  \label{comparSVM}
  \overline{\AC} \define \frac{\AC-\AC_{\SVM}}{\AC_{\SVM}},
  \quad
  \overline{\PR} \define \frac{\PR-\PR_{\SVM}}{\PR_{\SVM}},
\end{equation}
where $\AC_{\SVM}$ and $\PR_{\SVM}$ are computed as in~\eqref{AC}
and~\eqref{prec} for the SVM hyperplane. To keep the numerical results
section concise, we report on recall and the false positive rate in
Appendix~\ref{sec:furth-numer-results}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Numerical Results}
\label{numericalresults}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Run Time}
\label{Computation time}

Figure~\ref{perfomancetime} shows the ECDFs for the measured run
times.
\rev{Clearly, SVM is the fastest algorithm.
  This is expected as the SVM does not include any binary variables
  related to the unlabeled points, which is in contrast to other
  approaches.}
It can be seen that the IRCM outperforms both CS$^3$VM and WIRCM.
This shows that the idea to cluster unlabeled data points
significantly decreases the run time.
However, we need to be careful with the interpretation of these run
times since termination of \rev{SVM and}  IRCM does not imply that a globally optimal
point is found, whereas this is guaranteed CS$^3$VM and the WIRCM.
The quality of the points found by \rev{ SVM and} IRCM will be discussed in the
next section.
The figure also clearly indicates that Problem~\eqref{EQproblem1} is
rather challenging:
Even IRCM, which terminates for the most instances within the time
limit (indicated by the gray and dashed vertical line) only does so
for $\SI{57}{\percent}$ of the instances.
Note that the WIRCM has the worst efficiency.
This obviously needs to be the case since due to Step~\ref{Scheme
  version:call-other-alg} of Algorithm~\ref{Scheme version}, its run
time always includes the run time of the IRCM.
\rev{To shed some light on the scalability of the approaches,
  we also present a brief analysis of the run times in dependence of
  the number of samples in Appendix~\ref{sec:performancepersize}.}

\begin{figure}
  \centering
  \includegraphics[width=0.6\textwidth]{figures/PerformanceProfileTimewithSVM.pdf}
  \caption{\rev{ECDFs for run time (in seconds).}}
  \label{perfomancetime}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Quality of the Obtained Upper Bounds}

\begin{figure}
  \centering
  \includegraphics[width=0.6\textwidth]{figures/PerformanceProfileFunctionwithSVM.pdf}
  \caption{\rev{ECDFs for the quality of the obtained upper bounds.}}
  \label{perfomanceobjectivefunction}
\end{figure}

As discussed in the last section, for some instances none of the three
approaches \rev{that actually consider the unlabeled data} terminate
within the given time limit. This means we do not
obtain the optimal objective function value for these instances, which
we, moreover, can only provably obtain by CS$^3$VM and the WIRCM.
In fact, we have the optimal solution for 179~instances. These are
the baseline instances for Figure~\ref{perfomanceobjectivefunction},
which shows the ECDFs for the upper bound quality, as defined
in~\eqref{gap}.
\rev{Note that the objective function value obtained by SVM is very
  far from the optimal value, while} the IRCM finds an objective
function value rather close to the optimal value (with $f_{ps} \leq
0.2$, see the gray dashed vertical line) in $\SI{90}{\percent}$ of
these instances.
Besides that, the WIRCM outperforms CS$^3$VM in this comparison,
which means using the IRCM as a warm start improves the result.

The consequences of the results so far are the following.
If one is interested in getting a rather good feasible point as
quickly as possible, one should use the IRCM.
If one is able to spend some more run time, one should use the WIRCM.
Hence, both novel methods derived in this paper have their advantage
over just solving the CS$^3$VM with a standard MIQP solver.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Accuracy}
\label{secaccuracy}

\rev{For some instances, none of the three approaches that
  actually tackle the unlabeled data terminate within the given time
  limit. Hence, our first comparison only considers instances for which
  CS$^3$VM terminates within the time limit.}

\begin{figure}
  \centering
  \includegraphics[width=0.495\textwidth]{figures/ACCURACYALLTRUEVMCS3VMONLYCONVERGE.pdf}
  \includegraphics[width=0.495\textwidth]{figures/ACCURACYUNTRUEVMCS3VMONLYCONVERGE.pdf}
  \caption{\rev{Relative accuracy $\widehat{\AC}$ w.r.t.\ the true
      hyperplane; see~\eqref{compartrue}.
      Only those instances are considered for which CS$^3$VM terminated.
    Left: Comparison for all data points.
    Right: Comparison only for unlabeled data points.}}
  \label{ACtruCSVM}
\end{figure}
\rev{As can be seen in Figure~\ref{ACtruCSVM}, the relative
accuracy~$\widehat{\AC}$ (w.r.t.\ the true hyperplane) of CS$^3$VM,
is closer to 1 than the relative accuracy of SVM---especially for
the unlabeled data.
This means that using the unlabeled points as well as the cardinality
constraint allows to re-produce the classification of the true
hyperplane with higher accuracy than the standard SVM does.
Besides that, the relative accuracy of the SVM is more spread than the
one of the other approaches, indicating that there is comparable more
variation in the results as compared the results of CS$^3$VM. The box
in the boxplot depicts the range of the medium \SI{50}{\percent}
of the values; \SI{25}{\percent} of the values are
below and \SI{25}{\percent} are above the box.}

\begin{figure}
  \centering
  \includegraphics[width=0.495\textwidth]{figures/ACCURACYALLRELATIVESVMCS3VMONLYCONVERGE.pdf}
  \includegraphics[width=0.495\textwidth]{figures/ACCURACYUNRELATIVESVMCS3VMONLYCONVERGE.pdf}
  \caption{\rev{Accuracy values $\overline{\AC}$ w.r.t.\ the SVM; see
    \eqref{comparSVM} only consider the instances that CS$^3$VM terminated.
    Left: Comparison for all data points.
    Right: Comparison only for unlabeled data points.}}
  \label{ACSVMCSVM}
\end{figure}
\rev{Figure~\ref{ACSVMCSVM} shows that, in almost \SI{75}{\percent} of
  the cases, CS$^3$VM, has $\overline{\AC}$ values larger
than zero, where zero means the same accuracy as the SVM itself. In
the others  \SI{25}{\percent} of the cases, the $\overline{\AC}$ of
CS$^3$VM is slightly smaller than SVM.}

\rev{The second comparison considers only those three approaches that
  actually consider the unlabeled data, i.e., CS$^3$VM, IRCM, and
  WIRCM for all instances. As can be seen in Figure~\ref{ACtru3}, even
  though IRCM does not have an optimality guarantee, it has a better
  relative accuracy~$\widehat{\AC}$ than the hyperplane obtained from
  CS$^3$VM within the time limit. Consequently, as the hyperplane
  obtained from IRCM is used as a warm-start in WIRCM, it also has
  better accuracy.}
\begin{figure}
  \centering
  \includegraphics[width=0.495\textwidth]{figures/ACCURACYALLTRUE3METHDOS.pdf}
  \includegraphics[width=0.495\textwidth]{figures/ACCURACYUNTRUE3METHDOS.pdf}
  \caption{\rev{Relative accuracy $\widehat{\AC}$ w.r.t.\ the true
      hyperplane; see~\eqref{compartrue}.
      Left: Comparison for all data points.
      Right: Comparison only for unlabeled data points.}}
  \label{ACtru3}
\end{figure}
\rev{Figure \ref{ACSV3} shows that, in almost \SI{75}{\percent} of the  cases,
CS$^3$VM, the IRCM, and the WIRCM have $\overline{\AC}$ values larger
than zero. That is,
in general, our methods have greater accuracy than the SVM. Though, some
cases indicate worse $\overline{\AC}$ values for our methods than for
the SVM. This happens because for some instances, the methods (mainly
for CS$^3$VM; see also Figure~\ref{perfomancetime}) do not terminate
within the time limit. Hence, we expect that the number of negative
values will decrease if we would increase the time limit.}
\begin{figure}
  \centering
  \includegraphics[width=0.495\textwidth]{figures/ACCURACYALLRELATIVE3METHODS.pdf}
  \includegraphics[width=0.495\textwidth]{figures/ACCURACYUNRELATIVE3METHODS.pdf}
  \caption{\rev{Accuracy values $\overline{\AC}$ w.r.t.\ the SVM; see
    \eqref{comparSVM} consider all instances.
    Left: Comparison for all data points.
    Right: Comparison only for unlabeled data points.}}
  \label{ACSV3}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Precision}
\label{precisionsec}

\rev{We again separate the comparisons as in Section
  \ref{secaccuracy}. Figure \ref{PRtruCSVM} shows that the SVM's
  relative precision $\widehat{\PR}$ is lower than the relative
  precision of CS$^3$VM. This means that CS$^3$VM re-produces the
  classification of the true hyperplane with higher precision than the
  original SVM. Hence, SVM has more false-positive results.
  This happens because the biased sample is more likely to have
  positively labeled data and due to having no information about the
  unlabeled data, the SVM ends up classifying points on the positive
  side. As can be seen in Figure~\ref{PRSVMCSVM}, CS$^3$VM has
  slightly higher $\overline{\PR}$ values than~0, which is the
  baseline here that refers to the SVM itself.
  This means, CS$^3$VM is slightly more precise than the SVM.}

\begin{figure}
  \centering
  \includegraphics[width=0.495\textwidth]{figures/PRECISIONALLTRUEVMCS3VMONLYCONVERGE.pdf}
  \includegraphics[width=0.495\textwidth]{figures/PRECISIONUNTRUEVMCS3VMONLYCONVERGE.pdf}
  \caption{\rev{Relative precision $\widehat{\PR}$ w.r.t.\ the true
      hyperplane as; see \eqref{compartrue}.
      Only those instances are considered for which CS$^3$VM terminated.
      Left: Comparison for all data points.
      Right: Comparison only for unlabeled data points.}}
  \label{PRtruCSVM}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.495\textwidth]{figures/PRECISIONALLRELATIVESVMCS3VMONLYCONVERGE.pdf}
  \includegraphics[width=0.495\textwidth]{figures/PRECISIONUNRELATIVESVMCS3VMONLYCONVERGE.pdf}
  \caption{\rev{Precision values $\overline{\PR}$ w.r.t.\ the SVM; see
      \eqref{comparSVM}.
      Only those instances are considered for which CS$^3$VM terminated.
      Left: Comparison for all data points.
      Right: Comparison only for unlabeled data points.}}
  \label{PRSVMCSVM}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.495\textwidth]{figures/PRECISIONALLTRUE3methods.pdf}
  \includegraphics[width=0.495\textwidth]{figures/PRECISIONUNTRUE3METHODS.pdf}
  \caption{\rev{Relative precision $\widehat{\PR}$ w.r.t.\ the true
      hyperplane as; see \eqref{compartrue}.
      Left: Comparison for all data points.
      Right: Comparison only for unlabeled data points.}}
  \label{PRtru3}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.495\textwidth]{figures/PRECISIONALLRELATIVE3METHODS.pdf}
  \includegraphics[width=0.495\textwidth]{figures/PRECISIONUNRELATIVEALLMETHOD.pdf}
  \caption{\rev{Precision values $\overline{\PR}$ w.r.t.\ the SVM; see
      \eqref{comparSVM}.
      Left: Comparison for all data points.
      Right: Comparison only for unlabeled data points.}}
  \label{PRSVM3}
\end{figure}

\rev{Figure \ref{PRtru3} shows that the $\widehat{\PR}$ values of the
  IRCM and the WIRCM are less spread than the ones of CS$^3$VM.
  The reason most likely is that the CS$^3$VM approach terminates on
  fewer instances than the IRCM and the WIRCM. As can be seen in
  Figure \ref{PRSVM3}, the IRCM and the WIRCM also have slightly
  higher $\overline{\PR}$ values than~0.
  This means that our methods are slightly more precise than the SVM.
  The negative outliers most likely are due to the same reason as
  those for the respective accuracy values.}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "constrained-svm-preprint"
%%% End:
