\section{A Re-Clustering Method for solving CS$^3$VM}
\label{sec:cluster-unlabeled-points}

In Model~\eqref{equation2} of the last section, each binary variable
is related to an unlabeled point. The larger the number of unlabeled
data, the larger the number of binary variables and, hence, the
larger the computational burden to solve Problem~\eqref{equation2}.
To reduce this computational burden, we propose to cluster the
unlabeled data. This way, only one binary variable per cluster is
needed. For every cluster, we use its centroid as its representative
point.
To obtain clusterings, we use minimum sum-of-squares
clustering (MSSC). The MSSC problem is NP-hard; see, e.g.,
\textcite{KNP,KNP2,KNP3}. However, we do not need a globally
optimal solution for the MSSC problem as will be shown below.
Given a number~$k$ of clusters and a matrix $S =[s^1, \dotsc, s^p] \in
\R^{d\times p}$ of given points, the goal of the MSSC is to find
mean vectors $c^j \in \R^d$, $j \in [1, k]$, that solve the problem
\begin{equation*}
  c^* = \argmin_c \ \ell(S,c), \quad c = (c^j)_{j=1,\dots,k},
\end{equation*}
where the loss function $\ell$ is the sum of the squared Euclidean
distances, i.e.,
\begin{equation*}
  \ell(S,c) = \sum_{j=1}^k \sum_{s^i \in \mathcal{C}_j} \Vert s^i - c^j \Vert^2
\end{equation*}
with $\mathcal{C}_j \subset \R^{d}$ being the set of data points that
are assigned to cluster $j$.

We solve this problem heuristically using the $k$-means
algorithm \parencite{Kmeans1a, Kmeans3} for $S = X_u$, i.e.,
we cluster the unlabeled data.
Then, instead of using all unlabeled data as in the last section, we
only use the clusters' centroids $c^1, \dotsc, c^k$ and the numbers
$e_1, \dots, e_k$ of data points in each cluster to obtain the problem
\begin{varsubequations}{P4}
  \label{equation3}
  \begin{align}
    \min_{\omega,b,\xi,\eta, z} \quad
    & \frac{\Vert\omega \Vert ^ 2  }{2} + C_1 \sum_{i=1}^n
      \xi_i +C_2 (\eta_1 + \eta_2)
      \label{svmfunction3} \\
    \st \quad
    & y_i (\omega^\top x^i+b)  \geq 1 - \xi_i,  \quad i \in [1,n],
      \label{constraintx} \\
    & \omega^\top c^j +b \leq z_j M,  \quad   j\in [1,k],
      \label{constraintc1}\\
    & \omega^\top c^j +b \geq -(1-z_j) M, \quad  j \in [1,k],
      \label{constraintc2}\\
    & \tau - \eta_1 \leq   \sum_{j=1}^k e_jz_j \leq \tau + \eta_2,
      \label{constraints1} \\
    & \xi_i   \geq 0, \quad i \in [1, n],
      \label{constraintxi} \\
    & \eta_1, \eta_2 \geq 0,
      \label{constrainteta} \\
    & z_j \in \{0,1\}, \quad j \in [1,k].
      \label{constraintz}
  \end{align}
\end{varsubequations}
A valid big-$M$ is still given by~\eqref{validM} as shown in the next
proposition.
\begin{prop}
  \label{lemma3}
  If $e_j \geq 1$ for all $j \in [1,k]$, a valid big-$M$ for
  Problem~\eqref{equation3} is given by \eqref{validM}.
\end{prop}
\begin{proof}
  The proof follows the same lines as the proofs of Lemma~\ref{theob}
  and Theorem~\ref{theob2} with the additional observation that for all
  $j \in [1,k]$, it holds
  \begin{equation*}
    \Vert c^j \Vert = \frac{ 1 }{e_j} \left\Vert  \sum_{i : x^i\in
        \mathcal{C}_j}x^i\right \Vert \leq \frac{e_j \max_{i \in
        [n+1,N]}\Vert x^i \Vert}{e_j}
    = \max_{i \in [n+1,N]}\Vert x^i \Vert.
    \qedhere
  \end{equation*}
\end{proof}

It can happen that the hyperplane given by $(\omega^*,b^*)$ that
results from the solution of Problem~\eqref{equation3} cuts through
some cluster.
This means that not all data points of the cluster actually lie on the
same side of the hyperplane.
If this happens, the solution of Problem~\eqref{equation3}
does not satisfy the cardinality constraint \eqref{6} of
Problem~\eqref{equation2}. To fix this, we propose an iterative
method that is formally  listed  in Algorithm~\ref{First version}.
\rev{Note that the use the $k$-means algorithm is helpful here as it
  automatically provides the convex hulls of the clusters.
  Hence, it is easy to check if the hyperplane cuts through some
  cluster or not.}

\begin{algorithm}
  \caption{Re-Clustering Method (RCM)}
  \label{First version}
  \SetKwInput{Input}{Input~}
  \Input{$X\in \mathbb{R}^{d\times N}, y \in \set{-1,1}^n, k^1 \in
    \mathbb{N}, C_1 >0, C_2 > 0$, and $\tau \in \mathbb{N}$.}
  Set $t \ot 1$, compute $M^t$ as in~\eqref{validM}, compute a
  clustering of~$X_u$ in $k^1$ many clusters using the $k$-means
  algorithm, and obtain the centroids $c^1, \dotsc, c^{k^1}$ as well as the
  numbers $e_1, \dotsc, e_{k^1}$ of data points in each cluster.\label{first-version:cluster}
  \\
  Solve Problem~\eqref{equation3} to compute the hyperplane
  $(\omega^t,b^t)$ as well as $\xi^t,\eta^t,$ $z^t$.
  \label{first-version:solve-problem}
  \\
  \uIf{the hyperplane $(\omega^t,b^t)$ cuts a cluster}{
    Set $k^{t+1} \ot k^t$. \\
    \For{each cluster that is cut by the hyperplane $(\omega^t,b^t)$}{
      Split the cluster into two new clusters so that neither of the
      two new clusters is cut by the hyperplane~$(\omega^t,b^t)$.
      \label{First version:split-step}
      \\
      Update the centroids of the newly created clusters.
      \\
      Set $k^{t+1} \ot k^{t+1} + 1$.
    }
    Update $t\ot t +1 $ and go to Step~\ref{first-version:solve-problem}.
  }
  \Else{
    Return the hyperplane~$(\omega^t,b^t)$ as well as $\xi^t,\eta^t,$ $z^t$.
  }
\end{algorithm}


If Algorithm~\ref{First version} terminates it holds that all points
in a cluster are on the same side of the final hyperplane.
This implies the cardinality constraint~\eqref{6} is satisfied.
Note that the $k$-means algorithm is only called once to initialize
the clustering.
For all other iterations, we manually split clusters if they are cut
by the hyperplane of the respective iteration and compute the new
centroids directly.

The next theorem establishes that Algorithm~\ref{First version} always
terminates after finitely many iterations.

\begin{thm}
  \label{finite}
  Suppose that $e_j \geq 1$ for all $j\in[1,k^1]$ after
  Step~\ref{first-version:cluster} of Algorithm~\ref{First
    version}. Then, Algorithm~\ref{First version} terminates after at
  most $m-k^1$ iterations, where $m$ is the number of the unlabeled data
  points and $k^1$ is the number of initial clusters.
\end{thm}
\begin{proof}
  Observe that since we cluster $m$ unlabeled points, the maximum
  number of clusters we can obtain is $m$. Besides that, if in an
  iteration~$t$, Algorithm~\ref{First version} does not terminate, at
  least one cluster is split Step~\ref{First version:split-step}.
  Because we start with $k^1$ clusters and since in each iteration, we
  increase the number of clusters at least by one, the maximum
  number of iterations is $m-k^1$.
\end{proof}

Note that the point obtained by Algorithm~\ref{First version} is not
necessarily a minimizer of Problem~\eqref{equation2}.
However, the objective function value of the point obtained by
Algorithm~\ref{First version} is an upper bound for the objective
function value of Problem~\eqref{equation2}.
\begin{thm}
  \label{theoupper}
  Let $(\bar{\omega}, \bar{b}, \bar{\xi}, \bar{\eta}, \bar{z})$ be the
  point returned by Algorithm~\ref{First version}.
  Then, \rev{$(\bar{\omega}, \bar{b}, \bar{\xi}, \bar{\eta}, \bar{z})$
    is feasible for} Problem~\eqref{equation2} with
  \begin{equation*}
    M = 2\sqrt{2\bar{f}}\max_{i~\in[1,N]} \Vert x^i \Vert~+~1
  \end{equation*}
  \rev{and, consequently,
    \begin{equation*}
      \bar{f} \define \frac{\Vert\bar{\omega} \Vert ^ 2
      }{2} + C_1\sum_{i=1}^n\bar{ \xi_i} +C_2 (\bar{\eta}_1 +
      \bar{\eta}_2)
    \end{equation*}
    is an upper bound of Problem~\eqref{equation2}.
  }
\end{thm}
\begin{proof}
  For all clusters $\mathcal{C}_j$, $j\in \set{1, \dotsc, k^t}$, where $t$
  is the final iteration of Algorithm~\ref{First version}, we set
  $\tilde{z}_i = \bar{z}_j$ for all $i$ with $x^i \in
  \mathcal{C}_j$. We now show that $(\bar{\omega}, \bar{b}, \bar{\xi},
  \bar{\eta}, \tilde{z})$ is a feasible point for
  Problem~\eqref{equation2}. Indeed, Constraints~\eqref{3}, \eqref{8},
  \eqref{sumeta}, and~\eqref{10} are clearly fulfilled.
  Furthermore, since
  \begin{equation*}
    \sum_{i \in \mathcal{C}_j} \tilde{z}_i = e_j\bar{z}_j
  \end{equation*}
  for all $j \in [1, k^t]$, using \eqref{constraints1} we get
  \begin{equation*}
    \sum_{i=n+1}^N \tilde{z}_i = \sum_{j=1}^{k^{t}} e_j
    \bar{z}_j \implies  \tau - \bar{\eta}_1 \leq
    \sum_{i=n+1}^N \tilde{z}_i \leq \tau + \bar{\eta}_2
  \end{equation*}
  and Constraint~\eqref{6} is satisfied.
  Besides that,
  \begin{equation}
    \label{normw}
    \frac{\Vert \bar{\omega} \Vert^2}{2} \leq \bar{f}
    \implies
    \Vert \bar{\omega} \Vert \leq \sqrt{2\bar{f}}
  \end{equation}
  holds and as in Lemma~\ref{theob}, we get
  \begin{equation}
    \label{normb}
    \vert \bar{b} \vert
    \leq
    \Vert \bar{\omega} \Vert  \max_{i \in [1,N]} \Vert x^i \Vert +1.
  \end{equation}

  Moreover, by construction, for all $i \in \{n+1, \dots N\}$ with
  $\tilde{z}_i = 1$, $x^i$ belongs to a cluster $\mathcal{C}_j$ such
  that $\bar{\omega}^\top c^j +\bar{b}\geq 0 $. Using the fact that
  all points in $\mathcal{C}_j$ are on the same side of the  hyperplane,
  this side must be the positive one. This fact together with
  \eqref{normw} and \eqref{normb} implies
  \begin{align*}
    -(1- \tilde{z}_i) M = 0\leq \bar{\omega}^\top x^i + \bar{b}
    & \leq \Vert \bar{\omega}\Vert \max_{i \in [1,N]}  \Vert x^i \Vert +
      \vert \bar{b} \vert
    \\
    & \leq 2\sqrt{2\bar{f}}\max_{i \in [1,N]}  \Vert x^i \Vert + 1 = M =
      \tilde{z}_i M.
  \end{align*}
  Similarly, for all $i \in \{n+1, \dots N\}$ with   $\tilde{z}_i = 0$, we get
  \begin{equation*}
    -M = -(1-  \tilde{z}_i) M
    \leq
    \bar{\omega}^\top x^i + \bar{b}
    \leq 0
    = \tilde{z}_i M
  \end{equation*}
  and \eqref{4} as well as \eqref{5} are fulfilled.
  Because $(\bar{\omega}, \bar{b}, \bar{\xi}, \bar{\eta}, \tilde{z})$ is
  a feasible point for Problem~\eqref{equation2}, $\bar{f}$ is an
  upper bound to the Problem~\eqref{equation2}.
\end{proof}
\rev{Note, finally, that since the point obtained from
  Algorithm~\ref{First version} is feasible for
  Problem~\eqref{equation2}, we can use it for warm starting.}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "constrained-svm-preprint"
%%% End:
