% \documentclass[12pt, draftclsnofoot, onecolumn]{IEEEtran}
\documentclass[journal]{IEEEtran}
\usepackage{amssymb}

\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{amsfonts}
\usepackage[dvips]{graphicx}
\usepackage[dvips]{color}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{fancyhdr}
\usepackage{stfloats}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{cite}
\usepackage[bookmarks=false]{}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{mathbbol}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{url}
\usepackage{stfloats}
\usepackage{mathrsfs}
\usepackage{setspace}
\usepackage{booktabs}
\usepackage{latexsym}
\usepackage{url}
\usepackage{stfloats}
\usepackage{mathrsfs}
\theoremstyle{plain}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\usepackage{cite}
\usepackage{setspace}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{makecell}
\usepackage{etoolbox}
\usepackage{diagbox}
\usepackage{caption}
\usepackage{amsthm}
%\usepackage{geometry}
\newtheorem*{remark}{Remark}
%\usepackage{subcaption}
\usepackage{multirow}
\usepackage{array}
\usepackage{colortbl}
\usepackage{bbding}
%\ifCLASSINFOpdf
%\usepackage[pdftex]{graphicx}
%\else
%\fi
\usepackage[tight,footnotesize]{subfigure}
\hyphenation{op-tical net-works semi-conduc-tor}
\usepackage[]{hyperref}
\hypersetup{
	bookmarksnumbered=true,     
	bookmarksopen=true,         
	bookmarksopenlevel=1,       
	colorlinks=true,
	linkcolor=black            
	%pdfstartview=Fit,           
	%pdfpagemode=UseOutlines,    % this is the option you were lookin for
	%pdfpagelayout=TwoPageRight
}
% \usepackage[table]{xcolor}
%\linespread{2.0}
%\makeatletter
%\renewcommand{\maketag@@@}[1]{\hbox{\m@th\normalsize\normalfont#1}}%
%\makeatother
%\renewcommand\citeform[1]{[#1]}
%\renewcommand\citeleft{}
%\renewcommand\citeright{}
%\graphicspath{~/figure}

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}
\begin{document}
%\pagenumbering{gobble}% Remove page numbers (and reset to 1)
\clearpage
%\maketitle
%Title.
% ------
\title{\huge Task-Oriented Explainable Semantic Communication Based on Semantic Triplets}
%
% Single address.
% ---------------

% \author{\IEEEauthorblockN{Chuanhong Liu\IEEEauthorrefmark{1}, Caili Guo\IEEEauthorrefmark{1}\IEEEauthorrefmark{2}, Siyi Wang\IEEEauthorrefmark{3}, Yuze Li\IEEEauthorrefmark{3}, and Dingxin Hu\IEEEauthorrefmark{3}
% \\\IEEEauthorrefmark{1}Beijing Key Laboratory of Network System Architecture and Convergence, School of
% Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing 100876, China
% \\Email:\{2016\_liuchuanhong\}@bupt.edu.cn
% \\\IEEEauthorrefmark{2}Beijing Laboratory of Advanced Information Networks, School of
% Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing 100876, China
% \\Email:guocaili@gmai.com
% \\\IEEEauthorrefmark{3}School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing 100876, China
% \\Email:\{megaman, lyzbupt, hudingxin\}@bupt.edu.cn
% }
% \thanks{ This work was supported by the Fundamental Research Funds for the Central Universities (No.2021XD-A01-1), Beijing Natural Science Foundation (No.4202049) and BUPT Excellent Ph.D. Students Foundation (No.CX2022101).
% }
% }

\author{Chuanhong Liu
% , Caili Guo, \emph{Senior Member}, \emph{IEEE}, Yang Yang and Nan Jiang
\thanks{This work was supported in part by the Fundamental Research Funds for the Central Universities (No.2021XD-A01-1) and in part by BUPT Excellent Ph.D. Students Foundation (No.CX2022101).}
\thanks{Chuanhong Liu is with the Beijing Key Laboratory of Network System Architecture and Convergence, School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing 100876, China (e-mail:2016\_liuchuanhong@bupt.edu.cn)}
% \thanks{Yang Yang is with the Beijing Laboratory of Advanced Information Networks, School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing 100876, China (e-mail:yangyang01@bupt.edu.cn)}
}
%
\maketitle
\pagestyle{headings}
%
\vspace{0cm}
\begin{abstract}
Task-oriented semantic communication has garnered increasing attention due to its ability to reduce the amount of transmitted data without sacrificing task performance. While existing works have focused on designing advanced semantic communication architectures and efficient semantic codecs, challenges such as explainable and efficient semantic representation methods, and knowledge-based semantic compression algorithms have yet to be explored. These challenges have hindered the widespread adoption of semantic communication. To address these challenges, this paper proposes a novel Semantic Communication system based on Semantic Triplets (SCST), in which semantics are represented through explainable semantic triplets. A novel semantic extraction method is proposed to convert transmitted text into semantic triplets, where syntactic dependency analysis is introduced to improve semantic completeness. Furthermore, a semantic filtering method is designed to compress duplicate and task-irrelevant semantic triplets, making use of prior knowledge from the knowledge base. The filtered semantic triplets are then encoded and transmitted wirelessly to complete intelligent tasks at the receiver. To verify the effectiveness of the proposed SCST, it is applied to sentiment analysis and question-answering tasks, with semantic encoders and decoders designed for each task. Experimental results show that the proposed SCST achieves at least 43.5\% and 52\% accuracy gains, respectively, compared to traditional communication method. These results demonstrate that the proposed SCST can significantly improve semantic communication performance.
\end{abstract}

\begin{IEEEkeywords}
	semantic communication, semantic representation, semantic triplets.
\end{IEEEkeywords}

\vspace{-0.3cm}
\section{Introduction}
\label{sec:intro}
Recently, semantic communication has attracted extensive attention from industry and academia\cite{Nine}, which has been identified as one of the core techniques for the sixth generation (6G) of wireless networks\cite{Walid_6G, Chen_distribute, Letaief_Roadmap}. To provide connectivity for intelligent tasks, the goal of communication is no longer the accurate reception of every transmitted bit but to transmit the meaningful content of raw data to accomplish the tasks\cite{framework_Yang, info}. This communication paradigm refers to ``task-oriented" communication, which leads to a truly intelligent system with a significant reduction in data traffic\cite{Qin_survey}.

There are several prior studies on semantic communications based on deep learning. In \cite{Farsad}, the authors designed a joint source-channel coding system for text transmission via a recurrent neural network (RNN), where the system recovers the text directly rather than performing channel and source decoding separately. The authors in \cite{Xie_Deep} proposed a semantic communication system based on Transformer, in which the concept of semantic information was clarified at the sentence level. Based on \cite{Xie_Deep}, the authors in \cite{Xie_lite} further proposed a lite distributed semantic communication system, making the model easier to deploy on the Internet of things (IoT) devices. The work in \cite{Weng1} developed an attention mechanism-based semantic communication system to reconstruct speech signals. Furthermore, considering the speech recognition task, the authors in \cite{Weng2} proposed a semantic communication system to directly recognize the speech signals in text. The work in \cite{Gunduz_JSCC} presented a joint source-channel coding scheme based on convolutional neural networks (CNN) to transmit image data in a wireless channel, which can jointly optimize various modules of the communication system. The authors in \cite{Lee} developed an image classification-oriented semantic communications to improve the classification accuracy, rather than performing image recovering and classification separately. The work in \cite{retrival} proposed two communication schemes to improve the image retrieval accuracy without reconstructing the source image, in which the person or car re-identification is set as the communication task. The authors in \cite{Liu_AIoT} proposed an explainable semantic compression method to compress semantics without performance degradation for task-oriented semantic communication in AI of Things (AIoT). Excepted from single modal data, the authors in \cite{MU-DeepSC1} proposed a multi-users semantic communication system for serving visual question-answering task, in which Long Short Term Memory (LSTM) is used for text transmitter and CNN for image transmitter. Based on \cite{MU-DeepSC1}, the authors in \cite{MU-DeepSC2} proposed a unified framework to support various tasks with multimodal data. In summary, the existing works on semantic communications are focused on the implementation of semantic communication systems and consider all the features extracted by deep neural networks (DNN) as the meaning of the data. 

However, the features extracted via deep learning models are unexplainable and lack human language logic. To solve this problem, the authors in \cite{Jiang_KG, Wang_Attention, Wang_Attention2} proposed to use the knowledge graph instead of features to represent semantics, which can decompose texts into multiple triplets. All of these works aim at recovering the original text at the receiver, rather than implementing a specific task. The entities in such a representation method are well-defined without ambiguity, since knowledge graphs are usually used as generic databases, and the information contained in them is clear and general. However, the textual statements in daily language are dependent on each other, and most of the words are only applicable to a specific context. There exist a large number of words that are ambiguous and have various meanings according to the contexts, which cannot be extracted completely via the existing methods in \cite{Jiang_KG, Wang_Attention, Wang_Attention2}.

\renewcommand\arraystretch{1.3}
\begin{table}[t]
	\normalsize
	\centering
	\caption{TASK PERFORMANCE (ACCURACY) COMPARISON BETWEEN ORIGINAL TEXT AND CORRESPONDING TRIPLETS.}
	\setlength{\abovecaptionskip}{-0.5cm}
	\begin{tabular}{|c|c|c|c|}
		\hline
		Task & Original Text & Triplets & $\Delta$\\
		\hline
		Sentiment Analysis & 0.9493 & 0.9292 & $-$0.0201\\
        \hline
		Question-Answering & 0.9731 & 0.9434 & $-$0.0297\\
        \hline
		Text Classification & 0.8297 & 0.8103 & $-$0.0194\\
		\hline
	\end{tabular}
	\label{tab:intro}
\end{table}

To solve the above problems, we redesign the criteria for triplets extraction and expand the definition of triplets to semantic triplets to ensure the completeness of semantic information. Moreover, we assume that the transceiver are unanimously aware of the communication task, which can be identified in the shared knowledge base, and thus we can make full use of task-relevant prior information to improve communication efficiency. In this paper, we propose a novel task-oriented \textbf{S}emantic \textbf{C}ommunication system based on \textbf{S}emantic \textbf{T}riplets (SCST), which filters the redundant information to reduce the amount of transmitted data. We discover some insights for designing SCST by discussing the following question:\emph{"Whether triplets could provide similar task performance as the original text?"} We conducted a small experiment to verify our hypothesis and presented the results in Table \ref{tab:intro}. It is clear from the results that the triplets generated comparable performance across different tasks, which supports the proposition that triplets can accurately capture text semantics. To our best knowledge, \emph{this is the first work that studies the task-oriented triplets-based semantic communication} The main contributions of this paper are summarized as follows:

\begin{itemize}
	\item[$\bullet$] We propose an SCST that enables users to transmit the semantic triplets to complete intelligent tasks, which emphasizes explainable semantics and fully leverages task-relevant information. The SCST consists of a semantic triplets-based representation module, a semantic encoder that encodes semantic triplets, and a semantic decoder that completes the task at the receiver.
    \item[$\bullet$] The novel semantic triplets-based semantic representation method mainly includes semantic extraction and semantic filtering. The semantic extraction method extracts semantic triplets from the source text to represent its core semantic information, reducing the information redundancy of the transmitted text. To comprehensively extract semantic information from the original text, we propose a complementary semantic triplets extraction module based on syntactic dependency analysis.
    \item[$\bullet$] Moreover, a two-steps semantic filtering method is proposed to compress the extracted semantic triplets, discarding triplets that are semantic-duplicate or task-irrelevant to further reduce the amount of transmitted data and improve semantic communication efficiency.
	\item[$\bullet$] We apply the proposed SCST to the sentiment analysis task and question-answering task to demonstrate the effectiveness and universality. Experiment results show that the proposed SCST can significantly reduce the amount of transmitted data and improve the task performance, compared to the baseline algorithms, especially in low signal-to-noise (SNR) regime.
\end{itemize}

The remainder of this paper is organized as follows. The system model is described in Section II. Section III presents the proposed triplets-based semantic representation method. Section IV applies the proposed SCST to the sentiment analysis task and question-answering task. Experiments results are analyzed in Section V. Finally, Section VI draws some important conclusions.

\emph{Notations}: The boldface letters are used to represent vectors and matrices, and single plain letters denote scalars. Given a vector $\boldsymbol{x}$, $x_i$ represents its $i$th component, $\left\| {\boldsymbol{x}} \right\|$ denotes its Euclidean norm. Given a matrix $\boldsymbol{Y}$, ${\boldsymbol{Y}} \in {\cal{R}^{P \times Q}}$ denotes that matrix $\boldsymbol{Y}$ is a matrix of size ${P \times Q}$. ${{\cal C}{\cal N}}({\boldsymbol{\mu }},{\boldsymbol{\sigma }})$ denotes multivariate circular complex Gaussian distribution with mean vector $\boldsymbol{\mu }$ and co-variance matrix $\boldsymbol{\sigma }$.

%\vspace{-0.2cm}
\section{System Model}
\label{sec:system}
As shown in Fig. \ref{fig:model}, the structure of the proposed SCST consists of a semantic representation module, a semantic codec module, and a channel codec module. Since the principle of the channel codec is the same as that of the conventional communication system, we mainly focus on the implementation of semantic representation and semantic codec. 

The transmitter (e.g. IoT device) aims at gathering data locally and performing an inference task with the assistance of the receiver (e.g. edge server), which is the goal of semantic communication. In particular, the transmitter first extracts the semantic triplets from local raw data and further filters the redundant semantic triplets. Then, the semantics triplets are encoded and transmitted to the receiver in a scheduled manner. Finally, the receiver performs semantic decoding based on the received semantics and returns the result of tasks to the transmitter. The transceiver are equipped with a certain knowledge base to facilitate semantic extraction and filtering, where the knowledge base is further dependent on the target applications. In the following, we detail the process of the proposed SCST.

\begin{figure*}[t]
	\begin{center}
		\includegraphics[width=0.9\linewidth]{system_model_based_on_triplets2.pdf}
	\end{center}
	\caption{System model of the proposed semantic communication system based on semantic triplets.}
	\label{fig:model}
%	\vspace{-0.8cm}
\end{figure*}

As shown in Fig. \ref{fig:model}, the user maps the text, ${\boldsymbol I}$, into complex symbol streams, ${\boldsymbol S}$, and then passes it through the wireless channel with fading and noise. Particularly, we assume that the input of the system is a text, ${\boldsymbol I}{\rm{ = }}\left[ {{i_1},{i_2}, \cdots ,{i_L}} \right]$, where $i_l$ represents the $l$-th word in the text and $L$ is the number of words in ${\boldsymbol I}$. The user first extracts semantic triplets from the transmitted text ${\boldsymbol I}$ to represent its semantics, which can be denoted by
\begin{eqnarray}\label{signalform}
	{\boldsymbol {A}} = {R_{\boldsymbol {T}}}({\boldsymbol {I}}),
\end{eqnarray}
where ${R_{\boldsymbol {T}}}(\cdot)$ denotes the semantic extraction module, and ${\boldsymbol {T}}$ is the oriented task. The output ${\boldsymbol {A}}$ is a series of semantic triplets, which consists of a set of nodes and a set of edges. 

In particular, each node in the semantic triplets is an entity that refers to an object or a concept in the real word. Hereinafter, we define entity $i$ in text ${\boldsymbol I}$ as $en_i$ that consists of a subset of words in text ${\boldsymbol I}$. For example, "\emph{New York City}" and "\emph{the United States}" are two entities that consists of three words in the text "\emph{New York City is a beautiful city, which belongs to the United States}". There are various method such as entity recognition model (NER) can be used to extract the entities in text ${\boldsymbol I}$. Moreover, edges are the \emph{relations} between each pair of entities. Given a pair of extracted entities ($en_i, en_j$), the semantic extractor have to predict the relation $r_{i,j}$ between them. For example, the relation between entity "\emph{New York City}" and entity "\emph{the United States}" can be denoted by "\emph{belongs to}". The relation between two entities can be predicted via some well-established deep neural networks. Based on the extracted entities and the predicted relations, the semantic triplets of text ${\boldsymbol I}$ can be expressed as
\begin{eqnarray}\label{}
{\boldsymbol{A}}{\rm{ = [}}{{\boldsymbol{a}}_{\rm{1}}}{\rm{,}}...{\rm{,}}{{\boldsymbol{a}}_{{k}}}{\rm{,}}...{\rm{,}}{{\boldsymbol{a}}_{{K}}}{\rm{]}},
\end{eqnarray}
where ${{\boldsymbol{a}}_k} = (e{n_{k,i}},{r_{k,ij}},e{n_{k,j}})$ is a semantic triplet and $K$ is the total number of semantic triplets extracted from ${\boldsymbol I}$. Upon examination of the example given,it can be observed that the data size of the semantic triplets extracted is significantly smaller than the data size of the original text ${\boldsymbol I}$. This is mainly due to the compact representation of the triplets, which effectively removes a significant amount of semantic redundancy from the original text. This reduction in data size is advantageous for applications that require efficient transmission and understanding of large amounts of text-based data.

Different from the conventional communication system, in which bits or symbols are treated equally, semantic triplets may have different semantic importance for accomplishing the task, and there may still be some redundancy in the extracted semantic triplets\cite{openie1}. Therefore, we can further filter the extracted semantic triplets according to their contribution to the downstream task. As an illustration, suppose we extract two semantic triplets, namely (\emph{Apples, are, red}) and (\emph{Apples, taste, crisp}), from the text "\emph{Apples are red and taste crisp}". Suppose we only care about the color of the apples and not any other attributes. In this case, we can discard the second triplets without effecting the task-relevant semantics. By retaining only the essential semantic triplets under the guidance of task ${\boldsymbol T}$, the task-irrelevant information is eliminated, which can be especially valuable in scenarios where data transmission is limited by bandwidth or other constraints. The semantic filtering process can be expressed as
\begin{eqnarray}\label{}
	{\boldsymbol{X}} = {C_{\boldsymbol{T}}}({\boldsymbol{A}}),
\end{eqnarray}
where ${\boldsymbol{X}} \subset {\boldsymbol{A}}$, and ${C_{\boldsymbol{T}}}(\cdot)$ denotes the semantic filtering function, which is highly depended on the final task $\boldsymbol{T}$. In summary, semantic filtering offers two major benefits. First, it removes semantic redundancy and reduces the amount of data that needs to be transmitted. Second, it lessens the computation burden and energy consumption of the transmitter, which is significant for lightweight devices (e.g. IoT devices).

The filtered semantic triplets are encoded via a semantic encoder, which can further compress the transmitted data. The implementation details of the semantic encoder are highly related to the downstream tasks ${\boldsymbol T}$ and will be introduced in the specific applications in Section III. The encoded semantics can be denoted by
\begin{eqnarray}\label{}
	{\boldsymbol{M}} = {E_{\boldsymbol{\alpha_T}}}({\boldsymbol{X}}),
\end{eqnarray}
where ${E_{\boldsymbol{\alpha_T}}}(\cdot)$ denotes the semantic encoder network with parameter set ${\boldsymbol{\alpha_T}}$ for accomplishing task ${\boldsymbol T}$.

Next, the features are encoded by the channel encoder to generate symbols for transmission, which can be denoted by
\begin{eqnarray}\label{}
	{\boldsymbol{S}} = {Q_{\boldsymbol{\sigma}}}({\boldsymbol{M}}),
\end{eqnarray}
where $Q_{\boldsymbol{\sigma}}(\cdot)$ denotes the channel encoder network with parameter set ${\boldsymbol{\sigma}}$.

Then, the encoded symbols are transmitted via a wireless channel, and the received signal is expressed as
\begin{eqnarray}\label{}
{\boldsymbol{Y}} = h{\boldsymbol{S}} + \boldsymbol{n},
\end{eqnarray}
where $h$ is the channel covariance coefficient and $\boldsymbol{n}$ is the independent and identically distributed complex Gaussian noise sampled from ${{\cal C}{\cal N}}(0,{\boldsymbol{\sigma }^2\boldsymbol{I}})$. For end-to-end (E2E) training of the semantic transceiver, the channel must allow the backpropagation of parameters, thus physical channel in this paper is modeled by an untrainable neural network.

As shown in Fig. \ref{fig:model}, the receiver includes channel decoder and semantic decoder to recover the transmitted semantics and then complete the final task, respectively. The received symbols are decoded to recover semantics via channel decoder, which can be expressed as
\begin{eqnarray}\label{}
	{{{\boldsymbol{M}}'}} = {Q_{\boldsymbol{\chi }}^{ - 1}}({\boldsymbol{Y}}),
\end{eqnarray}
where ${Q_{\boldsymbol{\chi }}^{ - 1}}(\cdot)$ denotes the channel decoder network with parameter set ${\boldsymbol{\chi }}$.

Finally, the semantic receiver inputs the recovered semantics ${{\boldsymbol{M}}'}$ into semantic decoder to complete the intelligent tasks. Specifically, the output is
\begin{eqnarray}\label{}
	{\boldsymbol{p}} = {E_{\boldsymbol{\mu_T}}^{ - 1}}({\boldsymbol{{{\boldsymbol{M}}'}}}),
\end{eqnarray}
where ${\boldsymbol{p}}$ is the task result, which will be returned to the transmitter and ${E_{\boldsymbol{\mu_T}}^{ - 1}}(\cdot)$ denotes the semantic decoder with the parameter set ${\boldsymbol{\mu_T}}$. 

There are two main challenges in implementing SCST. One of the main challenges is effectively extracting the semantic triplets from the input text. This requires techniques that can identify and disambiguate the semantic relationships between words and phrases in the text, as well as handle the inherent ambiguity and variability in natural language. Another challenge is filtering the extracted triplets to ensure that they are relevant and informative for the downstream task at hand. Different downstream tasks may require different levels of granularity and specificity in the extracted triplets, and it can be challenging to identify the most useful and relevant triplets for a given task. This requires techniques that can filter out irrelevant or noisy triplets based on the downstream task. In the next section, a novel semantic representation method is proposed to solve these two challenges.
% The main challenges of SCST are how to effectively extract the semantic triplets and how to filter the semantic triplets according to the downstream task, which will be detailed in the next section.


%\vspace{-0.1cm}
\section{Proposed Triplets Based Semantic Representation Method}
\label{sec:algorithm}
%\addtolength{\topmargin}{-0.37in}
In this section, we first expand the conventional triplets extraction method to investigate the explainable semantic extraction method. Then, the details of the semantic filtering method are introduced. 

% \subsection{Definition of Semantic Triples}

% We define a semantic triple as the smallest unit for expressing semantics and prove that the form of the triple can retain most of the semantic information through experiments. For the details in the experiment, we chose Scierc as our basic dataset, which is constructed from scientific texts papers where some important entities and relations are labeled. 

% We replicate two models, a relationship extraction model specifically for scientific texts and a text generation model. By calculating the similarity of the original text and the text generated by the semantic triples, we proved that the semantic triples can retain most of the semantic information.

% \begin{figure}[t]
% 	\begin{center}
% 		\includegraphics[width=1\linewidth]{semantic_model.pdf}
% 	\end{center}
% 	\caption{The process of the semantic
% triples information calculation}
% 	\label{fig:model}
% %	\vspace{-0.8cm}
% \end{figure}

% \begin{table}[]
% \begin{tabular}{|l|l|}
% \hline
% article / random text (lower bound)          & 0.52947 \\ \hline
% article / back translated text (upper bound) & 0.91042 \\ \hline
% article / semantic triple spliced text       & 0.81920 \\ \hline

% \end{tabular}
% \end{table}

% There are some differences between semantic triples and traditional knowledge graph triples. Traditional triples are mainly used to build knowledge graphs. The knowledge graphs are usually used as generic databases and the information in them is clear and universally applicable. The entities in traditional triples are only a small percentage of all entities that are well-defined and free from ambiguity.

% However, since the textual statements in daily language are not relatively independent from each other. Most of the sentence contents are not universally applicable and hold only under the conditions of the preceding and following contexts. There exist a large number of noun words that are ambiguous or indirect and refer to other entities. These nouns will not be extracted under the definition of traditional triples so that the semantic information of the sentence cannot be retained completely. Therefore, we reset the criteria for entity selection and expand the definition of semantic triples in order to ensure the integrity of semantic information and we demonstrate through subsequent experiments that this extraction method can retain semantic information more completely.



% \vspace{-0.3cm}
\subsection{Semantic Extraction Method}

\begin{figure}[t]
	\begin{center}
		\includegraphics[width=1\linewidth]{semantic_triples_extraction.pdf}
	\end{center}
	\caption{The process of semantic extraction method.}
	\label{fig:extraction}
%	\vspace{-0.8cm}
\end{figure}

The proposed semantic extraction method is shown in Fig. \ref{fig:extraction}, which mainly consists of an out-of-the-box triplet extraction tool, and a complementary semantic triplets extraction module based on syntactic dependencies analysis. In particular, the out-of-the-box triplet extraction tool that we use in this work is the Open Information Extraction (OpenIE) annotator\cite{openie2}, which is commonly used for triplet extraction in knowledge graphs. However, the extracted results of OpenIE lack abstract and non-generic semantics, which may degrade the performance of semantic communication. To overcome this problem, we further proposed a complementary semantic triplets extraction module based on syntactic dependencies analysis to supplement the missing semantics. 

The complementary semantic triplet extraction module can be divided into four phases: a syntactic dependency analysis phase based on Spacy, a rule-based semantic triplet extraction phase, a lexical check phase, and a referential substitution phase, as shown in the lower part of Fig. \ref{fig:extraction}. In the first phase, we analyze the syntactic dependency of the input sentence based on Spacy to obtain the syntactic structure of the sentence, outputting a tree structure results. The nodes in the tree structure results represent the words in the sentence and the edges connected between words represent the syntactic roles that words play in the sentence. Based on the output of the last phase, we extract triplets with common structures (e.g. subject-verb-object structure, subject-link verb-predicative structure, etc.) according to the predefined rules. For example, in the case of subject-verb-object structure, we first find all of the verbs in the syntactic dependency tree and denote them as the relationship of the extracted triplets. Then, the subject node and object node connected to a specific verb will be expressed as the head entity and tail entity of the triplet, respectively. Moreover, we expand the sub-tree of the extracted node, and then all of its modifiers, which may be abstract and non-generic, will be added into the corresponding triplets to ensure the integrity of semantics. The rule-based semantic triplet extraction phase is illustrated in Fig. \ref{fig:dependency_tree}.

\begin{figure}[t]
	\begin{center}
		\includegraphics[width=1\linewidth]{dependency_tree.pdf}
	\end{center}
	\caption{Illustration of rule-based semantic triplet extraction phase.}
	\label{fig:dependency_tree}
%	\vspace{-0.8cm}
\end{figure}

There may still be some error triplets in the output of the second phase. To ensure the accuracy and standardization of the extracted results, we have to check whether the word in a triplet matches its role in terms of lexicality in the third phase. For example, if a word is extracted as the predicate in the triplet but its lexicality is a noun, it means there is a mismatch between the lexicality and the syntactic role of the word in the triplet, so it will be discarded. So far, we have obtained triplets for the text through syntactic analysis. Whether or not to perform the fourth phase depends on the characteristics of the original text. If there are anonymous entities in the text (e.g. "@entity" in the Children's Book Test dataset), we will execute the referential substitution phase to replace these entities with the corresponding words based on a pre-designed reference table.

Finally, the extracted triplets of the two parts will be combined and the duplicate triplets will be sifted out. Based on the proposed method, the semantics of the raw text, including the abstract and non-generic information, can be fully represented.

% Syntactic dependency analysis is the process of analyzing the input sentence to get the syntactic structure of the sentence. The syntactic structure results are output in a tree-like data structure. The nodes in the tree structure represent the words in the sentence and the line connected between words represent specific syntactic role word plays in the sentence. For example, 'AMOD' means that word act as adverbial modifier in the sentence. Then we do some rule-based extraction. Based on the output syntactic dependency tree, we extract some common structures in the sentence such as subject-verb-object structure and subject-link verb-predicative structure by filtering the specific relationship between words. For example, in the subject-verb-object structure, we take the verb as the relationship of the grammatical triple, and the subject and object as the entities. In order to ensure the integrity of information, and get a more comprehensive syntactic triple, we reduce the original representation of specific subtrees, which means that if a noun word is extracted for the sentence, all of its modifier also will be extracted. To ensure the accuracy and standardization of the extracted results, we also checked the lexicality of the extracted results. For example, if a word is extracted as a predicate, we will check that whether it is a verb or not. We deleted those that did not match the lexicality rules and reduced the verbs in them to the general form. We also replaced some indirect referents. Finally, we combined the extracted results of the two parts and sifted out the duplicate parts as the final output of the semantic triples extracted part.

% \begin{algorithm}[t]
% 	\normalsize
% 	\caption{Parameters Solving Algorithm.}
% 	\begin{algorithmic}[1]
% 		\STATE \textbf{Input:} Initialize parameters ${{\boldsymbol{\zeta }}} = [\zeta _1,\zeta _2,\zeta _3,\zeta _4]$, point set ${{\cal D}}$, step length $\delta$, threshold $L_0$.
% 		\REPEAT		
% 		\FOR {$(o^d,\eta^{d*}) \in {{\cal D}}$}
% 		\STATE Compute ${\eta^d}(o^d) = {\zeta _1}{e^{{\zeta _2}o^d}} + {\zeta _3}{e^{{\zeta _4}o^d}}$.
% 		\ENDFOR
% 		\STATE Compute the loss $L({\boldsymbol{\zeta }}) = \frac{1}{2D}{\sum\limits_{d = 1}^D {\left( {\eta _{}^d({o^d}) - \eta^{d*}} \right)} ^2}$.
% 		\STATE Compute the gradient of ${\boldsymbol{\zeta }}$: $G({\boldsymbol{\zeta }}) = \frac{{\partial L({\boldsymbol{\zeta }})}}{{\partial {\boldsymbol{\zeta }}}}$.
% 		\STATE Update parameters ${\boldsymbol{\zeta }}: = {\boldsymbol{\zeta }} - \delta G({\boldsymbol{\zeta }})$.
% 		\UNTIL {$L({\boldsymbol{\zeta }}) \le {L_0}$}
% 		\STATE \textbf{Output:} ${{\boldsymbol{\zeta }}} = [\zeta _1,\zeta _2,\zeta _3,\zeta _4]$.
% 	\end{algorithmic}
% 	\label{fitting}
% \end{algorithm}

\vspace{-0.3cm}
\subsection{Semantic Filtering Method}
% After the semantic extraction introduced above, the extracted semantic triplets can be huge in quantity, and the total number of words can even exceed the original text. For example, if the transmitted sentence is "jir hubac's script is a gem", there may be two extracted triples, one is ["jir hubac", "has", "script"], and the other is ["jir hubac's script", "is", "gem"]. Obviously, the information covered by the latter triplet is closer to the original text, and the first triplet is semantic-redundant. To deal with this problem, a two-steps semantic filtering method is proposed, in which the first is to filter out triples with redundant semantics, and the second is to discard the semantic triplets that are irrelevant to the intelligent tasks.
During the extraction of semantic triplets, it is possible to generate multiple triplets with similar semantics by freely combining the relations and entities in the sentence. This poses a challenge as the extracted triplets can be extensive in quantity and even surpass the original text. In order to address this issue, a two-step semantic filtering method has been proposed, consisting of filtering out triplets with redundant semantics in the first step, followed by discarding triplets that are irrelevant to the intelligent task in the second step. Table \ref{tab:filtering} presents an example of how the proposed two-step semantic filtering method works.

In the first step of the proposed semantic filtering method, we assume that the relationship between two entities is unique and initialize an empty entity pair set. As each semantic triplet is extracted, we check whether the pair of its head entity and tail entity already exists in the set. If the entity pair is already present in the set, the semantic triplet will be filtered out; otherwise, it will be added to the semantic triplets list. As depicted in Table \ref{tab:filtering}, once the triplet ("China", "capital city", "Beijing") is extracted, the entity pair ("China","Beijing") is added to the entity pair set. Hence, when the triplet ("China", "contain", "Beijing") appears, it will be discarded, as the entity pair ("China","Beijing") already exists in the set. This preliminary processing step helps to significantly reduce the duplicate semantics transmitted.

Furthermore, to further filter the extracted semantic triplets, we can utilize the prior knowledge stored in the knowledge base of the transceiver and discard the semantic triplets that do not contribute to the completion of the task. Taking the examples of sentiment analysis and question-answering tasks, we can illustrate the second step of the semantic filtering method. In sentiment analysis, we can leverage the knowledge that semantic triplets with longer entities and richer relationships tend to include more verbs or adjectives expressing sentiment, making them more useful for analyzing the sentiment of the original text. Therefore, after filtering redundant triplets in the first step, preference can be given to the triplets with longer entities and richer relationships. In contrast, for the question-answering task, we have the prior knowledge that the questions and answers will only involve specific types of entities, and hence unmatched or irrelevant triplets have a higher probability of being filtered. The proposed task-specific filtering method can significantly reduce the amount of transmitted data, by up to 80.04\%, without any compromise in task performance.

After applying the proposed triplet-based semantic representation method, the next step is to encode the semantic triplets and transmit them to perform downstream tasks. This forms the entire SCST system, which will be explained in further detail in the following section.

\begin{table}[t]
	\normalsize
	\vspace{-0.1cm}
	\centering
	\caption{EXAMPLE OF SEMANTIC EXTRACTION AND SEMANTIC FILTERING.}
	\setlength{\abovecaptionskip}{-0.5cm}
        \renewcommand\arraystretch{1.5}
	% \scalebox{0.85}
        \resizebox{\linewidth}{!}{
        \begin{tabular}{|p{3cm}<{\centering}|p{3cm}<{\centering}|p{3cm}<{\centering}|}
            \hline
            \rowcolor{red!10} \multicolumn{3}{|c|}{\textbf{Original Texts}}\\
		\hline
		 \multicolumn{3}{|c|}{Beijing is the capital city of China, and Bob was born in there.}\\
            \hline
		\rowcolor{red!10} \multicolumn{3}{|c|}{\textbf{Extracted Results}}\\
            \hline
            \rowcolor{red!10} \textbf{Subject} & \textbf{Relation} & \textbf{Object}\\
            \hline
             China & capital city & Beijing\\
            \hline
             China & contain & Beijing\\
            \hline
             Bob & born in & Beijing\\
            \hline
		\rowcolor{red!10} \multicolumn{3}{|c|}{\textbf{Filtered Results (Step 1)}}\\
            \hline
            \rowcolor{red!10} \textbf{Subject} & \textbf{Relation} &\textbf{Object}\\
            \hline
             China & capital city & Beijing\\
            \hline
             Bob & born in & Beijing\\
            \hline
        \rowcolor{red!10} \multicolumn{3}{|c|}{\textbf{Filtered Results (Step 2)}}\\
            \hline
            \rowcolor{red!10} \textbf{Subject} & \textbf{Relation} &\textbf{Object}\\
            \hline
             China & capital city & Beijing\\
            \hline
	\end{tabular}}
	\vspace{-0.cm}
	\label{tab:filtering}
\end{table}


% % Please add the following required packages to your document preamble:
% % \usepackage{multirow}
% % \usepackage[table,xcdraw]{xcolor}
% % If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
% \begin{table}[]
% \normalsize
% \vspace{-0.1cm}
% \centering
% \caption{EXAMPLE OF SEMANTIC FILTERING.}
% % \setlength{\abovecaptionskip}{-0.5cm}
% \begin{tabular}{|lll|}
% \hline
% \multicolumn{3}{|l|}{{\color[HTML]{FF0000} }}                                                                                                                          \\
% \multicolumn{3}{|l|}{\multirow{-2}{*}{{\color[HTML]{333333} \textbf{Original Texts}}}}                                                                                        \\ \hline
% \multicolumn{3}{|l|}{\begin{tabular}[c]{@{}l@{}}Beijing is the capital city of China, and \\ Bob was born in there.\end{tabular}}                          \\ \hline
% \multicolumn{3}{|l|}{{\color[HTML]{FF0000} }}                                                                                                                          \\
% \multicolumn{3}{|l|}{\multirow{-2}{*}{{\color[HTML]{333333} \textbf{Extracted Results}}}}                                                                                        \\ \hline
% \multicolumn{1}{|l|}{{\color[HTML]{333333} \textbf{Subject}}} & \multicolumn{1}{l|}{{\color[HTML]{333333} \textbf{Relation}}} & {\color[HTML]{333333} \textbf{Object}} \\ \hline
% \multicolumn{1}{|l|}{China}                                 & \multicolumn{1}{l|}{contains}                            & Beijing                      \\ \hline
% \multicolumn{1}{|l|}{China}                                   & \multicolumn{1}{l|}{capital city}                       & Beijing                      \\ \hline
% \multicolumn{1}{|l|}{Bob}                                   & \multicolumn{1}{l|}{born in}                            & Beijing                              \\ \hline
% \multicolumn{3}{|l|}{\textbf{Filtered Results}}                                                                                                                        \\ \hline
% \multicolumn{1}{|l|}{{\color[HTML]{000000} \textbf{Subject}}} & \multicolumn{1}{l|}{{\color[HTML]{000000} \textbf{Rethe lation}}} & {\color[HTML]{000000} \textbf{Object}} \\ \hline
% \multicolumn{1}{|l|}{China}                                   & \multicolumn{1}{l|}{contains}                       & Beijing                      \\ \hline
% \multicolumn{1}{|l|}{Bob}                                   & \multicolumn{1}{l|}{born in}                            & Beijing                              \\ \hline
% \end{tabular}
% \label{tab:filtering}
% \end{table}

\vspace{-0.2cm}
\section{Proposed SCST and its Implementations}
In this section, we will provide a detailed explanation of how the proposed SCST can be implemented for various downstream tasks. Specifically, we adopt the sentiment analysis\cite{sentiment} and question-answering\cite{QA} as examples, but the SCST can be easily expanded to other intelligent tasks via redesigning the semantic codec. Sentiment analysis and question answering were chosen as examples because they are both common and important tasks in natural language processing (NLP)\cite{nlp} and have different characteristics that can showcase the effectiveness of the proposed semantic communication system. As the implementation of semantic codec highly depends on the downstream tasks, we detail the deep learning-based semantic encoder and decoder for sentiment analysis and question-answering, respectively. Additionally, it is worth noting that all models can be trained in the cloud and then broadcasted to users, making the implementation process more efficient and scalable.

\subsection{Sentiment Analysis}
In this subsection, we will focus on sentiment analysis-oriented SCST. Sentiment analysis is a technique used to analyze subjective texts with various emotions, such as positive or negative, to determine the text's opinions, preferences, and emotional tendencies. Sentiment analysis can be divided into binary classification and multi-classification tasks, in which the latter providing a finer granularity in the division of various emotions. For the purpose of this discussion, we will focus on binary sentiment classification. However, it is worth noting that the proposed semantic communication system can be easily applied to multi-classification tasks as well. 

% Sentiment analysis not only has two categories, but also has multi-category. This multi-category task has a finer granularity in the division of emotions. The sentiment analysis in this paper is for the binary classification task. The dataset used is SST-2 (The Stanford Sentiment Treebank), a single-sentence classification task that includes human annotations of sentences in movie reviews and their emotions. 

\begin{figure*}[t]
	\begin{center}
		\includegraphics[width=1\linewidth]{structure.pdf}
	\end{center}
	\caption{The process of sentiment analysis-oriented SCST.}
	\label{fig:sentiment}
%	\vspace{-0.8cm}
\end{figure*}

The proposed sentiment analysis-oriented SCST is shown in Fig. 3. First, we extract the semantics of the raw text ${\boldsymbol I}$ to obtain the semantic triplets ${\boldsymbol X}$, based on the proposed semantic extraction and filtering methods in Sec. III. The semantic triplets are separated by commas and reorganized into a sentence, which is then input to the encoder network.

Then, we encode the semantic triplets ${\boldsymbol X}$ to convert them into the embedding ${\boldsymbol E}$, which can be represented as
\begin{eqnarray}\label{}
	{\boldsymbol{E}} = {\boldsymbol{E}}_{T} + {\boldsymbol{E}}_{P} + {\boldsymbol{E}}_{S}
\end{eqnarray}
where ${\boldsymbol{E}}_{T}$ is the token embedding, ${\boldsymbol{E}}_{P}$ represents positional embedding, and ${\boldsymbol{E}}_{S}$ denotes the segmentation embedding. The token embedding represents the index of the token in the vocabulary, which is built manually in advance. The positional embedding corresponds to the position of the token in the sentence. Segmentation embedding is to distinguish which sentence the token belongs to, which is effective in the case where there are multiple input sentences.

After obtaining the embedding ${\boldsymbol E}$, the pre-trained Robustly optimized Bidirectional Encoder Representations from Transformers approach (RoBERTa) model\cite{Roberta} is used as the semantic encoder here to further compress the transmitted data. The RoBERTa model adopts a multi-head attention mechanism so that each input ${\boldsymbol{E}}$ can be combined with the context information, capturing more information in the semantic space. The RoBERTa model is pre-trained based on the BooksCorpus (800M words) and English Wikipedia (2,500M words) dataset and masked language modeling (MLM) task, which essentially is a fill-in-the-blank task. During the training, the model randomly masks 15$\%$ of tokens of the input to obtain a deep bidirectional presentation. However, directly masking all of the selected tokens may lead to a mismatch between pre-training and fine-tuning, since the [MASK] token does not appear during fine-tuning. To mitigate this, we replace the $i$-th selected token with the [MASK] token with 80$\%$ probability, a random token with 10$\%$ possibility, and unchanged with 10$\%$ possibility. Then, the final hidden vectors corresponding to the masked tokens will be fed into a softmax layer to predict the original words and the model will be updated under the guidance of cross-entropy loss function. In addition, the same problem may occur during testing, where the data is processed in the same way as pre-training. Based on the pre-trained model, we can obtain the encoded semantics ${\boldsymbol M}$ via 
\begin{eqnarray}\label{}
	{\boldsymbol{M}} = RoBERTa({\boldsymbol{E}})
\end{eqnarray}
The encoded semantics ${\boldsymbol M}$ will be further encoded by the channel encoder and transmitted via the wireless channel.

Considering the sentiment analysis task here, we use the classifier as the semantic decoder, which can be realized via a multi-layer perceptron (MLP). Thus, the sentiment analysis results can be obtained via
\begin{eqnarray}\label{}
	\boldsymbol{p}_l = MLP({\boldsymbol{M}})
\end{eqnarray}
where $\boldsymbol{p}_l$ represents the predicted probability vector of the $l$-th sample.

The cross-entropy loss function is adopted here, which can be expressed as
\begin{eqnarray}\label{}
	{\mathcal{L}_{\rm{SA}}}= \sum\limits_l {-[{\boldsymbol{p}_l}*log({\boldsymbol{q}_l}) + (1-{\boldsymbol{p}_l})*log(1-{\boldsymbol{q}_l})]}
\end{eqnarray}
where ${\boldsymbol q_l}$ represents the label of the $l$-th sample.

\subsection{Question Answering}
Question-answering is a critical component of many human-computer interaction systems, and is used in various fields such as information retrieval, and knowledge management. However, in a question-answering-oriented communication system, the transmitter may need to transmit a large amount of original text to the receiver in order to search for the answer to the question. This can result in a significant transmission burden and may negatively impact the task performance. To address this issue, we can to apply the proposed SCST to the question-answering task, reducing the transmission burden and improving the task performance.

% In Q\&A tasks, the Q\&A model needs to search for answers in the original text for different kinds of questions. Therefore, we can indirectly evaluate how well the semantic triples retains semantic information through comparing the performance of the same model in the original text and the semantic triples spliced text.

\begin{figure}
\centering
\includegraphics[width=1\linewidth]{qa-model.pdf}
\caption{The process of question answering-oriented SCST.}
\label{fig4:qamodel}
\end{figure}

As shown in Fig. \ref{fig4:qamodel}, the proposed question-answering-oriented SCST mainly consists of semantic representation, semantic encoder, and semantic decoder. Similarly to the sentiment analysis, we need to extract and filter the semantic triplets, ${\boldsymbol X}$, from the original text, ${\boldsymbol I}$, based on the methods proposed in Sec. III to maintain the semantics of the source text. Due to the limited computation and storage resources of the transmitter, we have to simplify the structure of the semantic encoder as much as possible to obtain the trade-off between the amount of transmitted data and the resource consumption of the transmitter. Under this consideration, the semantic encoder we used here mainly consists of an embedding layer to map the triplets into embedding vectors based on the vocabulary, which can be optimized in the training process. This enables the proposed semantic communication system to be easily deployed on various lightweight devices. Therefore, the encoded semantics, ${\boldsymbol E}$, can be denoted by
\begin{eqnarray}\label{}
	{\boldsymbol{E}} = \bf{EMB}(\boldsymbol{X})
\end{eqnarray}
where $\bf{EMB}(\cdot)$ denotes the embedding layer.
% As shown in Figure 4, the semantic triplet-based Q\&A task system consists of a semantic triplet extraction module and the text-question-answer model required to complete the Q\&A task. Original text ${\boldsymbol S}$ is input into the semantic extraction module of the system to extract a set of triples ${\boldsymbol T}$ containing the semantic information of the source text. Each triplet in ${\boldsymbol T}$ is fed into the Input module for embedding to obtain a vector representation of each triplet, which is then input into the text-question-answer (TQA) model together with the actual question to obtain the answer. The parameters in TQA model and Input module are continuously optimized with training to improve the accuracy of the system.
% The Input module mainly contains an embedding layer, which maps words into vectors according to the vocabulary size. All word vectors of each triplet jointly contribute to the vector representation of the sentence. 

Similarly to the Sec. IV-A, the encoded semantics ${\boldsymbol E}$ will be further encoded by the channel encoder and transmitted via the wireless channel. After obtaining the received signal, the receiver will decode it via channel decoder to recover the transmitted semantics ${\boldsymbol E'}$. 

As for semantic decoder, we use the structure of a new neural memory model dubbed self-attentive associative memory (SAM)-based Two-memory Model (STM) \cite{SAM} that takes inspiration from the existence of both item and relational memory in the human brain, which currently performs prominently on memory and inference tasks. In STM, the relational memory exists separately from the item memory. To maintain a rich representation of the relationship between items, the relational memory is higher-order than the item memory. That is, the relational memory stores multiple relationships, each of which should be represented by a matrix rather than a scalar or vector. Otherwise, the capacity of the relational memory is downgraded to that of the item memory. Finally, as there are two separate memories, they communicate to enrich the representation of one another. STM applies SAM to transform a second-order (matrix) item memory into a third-order relational representation.

The basic structure of STM model is shown in Fig. \ref{fig4:qamodel}. ${\mathcal M_t^i}$ $\in$ ${\mathcal {\mathbbm{R}}^{d\times d}}$ is a memory unit for items and ${\mathcal M_t^r}$ $\in$ ${\boldsymbol {\mathbbm{R}}^{nq\times d\times d}}$ is for relationships. From a high-level view, at $t$-th step, we use the current input data ${\boldsymbol e_t}$ and the previous state of memories \{${\mathcal M_{t-1}^i}$, ${\mathcal M_{t-1}^r}$\} to produce output ${\boldsymbol a_t}$ and new state of memories \{${\mathcal M_t^i}$, ${\mathcal M_t^r}$\}. 

At each step, the item memory ${\mathcal M_t^i}$ is updated with new input ${\boldsymbol e_t}$ using gating mechanisms. For an input ${\boldsymbol e_t}$, we update the item memory as:
\begin{equation}
\mathcal{M}_t^i=F_t\left(\mathcal{M}_{t-1}^i, e_t\right) \odot \mathcal{M}_{t-1}^i+G_t\left(\mathcal{M}_{t-1}^i, e_t\right) \odot E_t
\end{equation}
where ${\boldsymbol F_t}$ and ${\boldsymbol G_t}$ are forget and input gates, respectively.

The item memory plus the read-out from the relational memory is forwarded to SAM, resulting in a new relational representation to update the relational memory ${\mathcal M_t^r}$ as follow:
\begin{equation}
\mathcal{M}_t^r=\mathcal{M}_{t-1}^r+\alpha_1 \operatorname{SAM}_\theta\left(\mathcal{M}_t^i+\alpha_2 v_t^r \otimes f_2\left(e_t\right)\right)
\end{equation}
where ${\alpha_1}$ and ${\alpha_2}$ are blending hyper-parameters. The input for SAM is a combination of the current item memory ${\mathcal M_t^i}$ and the association between the extracted item from the previous relational memory ${\boldsymbol v_t^r}$ and the current input data ${\boldsymbol e_t}$. Here, ${\boldsymbol v_t^r}$ enhances the relational memory with information from the distant past
\begin{equation}
\boldsymbol v_t^r=\operatorname{softmax}\left(\boldsymbol f_2\left(e_t\right)^{\top}\right) \mathcal{M}_{t-1}^r \boldsymbol f_1\left(e_t\right)
\end{equation}
where ${\boldsymbol f_1}$ and ${\boldsymbol f_2}$ are feed-forward neural networks.

The relational memory transfers its knowledge to the item memory by using high dimensional transformation:
\begin{equation}
\mathcal{M}_t^i=\mathcal{M}_t^i+\alpha_3 \mathcal{B}_1 \circ \mathcal{V}_f \circ \mathcal{M}_t^r
\end{equation}
where $\mathcal{V}_f$ is a function that flattens the first two dimensions of its input tensor, $\mathcal{B}_1$ is a feed-forward neural network that maps $\mathbb{R}^{\left(n_q d\right) \times d} \rightarrow \mathbb{R}^{d \times d}$ and ${\alpha_3}$ is a blending hyper-parameter.

As for the answer to the question, at each time step, we distill the relational memory into an output answer vector $a_t \in \mathbb{R}^{n_o}$. We alternatively flatten and apply high-dimensional transformations as follow:
\begin{equation}
a_t=\mathcal{B}_3 \circ \mathcal{V}_l \circ \mathcal{B}_2 \circ \mathcal{V}_l \circ \mathcal{M}_t^r
\end{equation}
where $\mathcal{V}_l$ is a function that flattens the last two dimensions of its input tensor. $\mathcal{B}_2$ and $\mathcal{B}_3$ are two feed-forward neural networks that map $\mathbb{R}^{n_q \times(d d)} \rightarrow \mathbb{R}^{n_q \times n_r}$ and $\mathbb{R}^{n_q n_r} \rightarrow \mathbb{R}^{n_o}$ , respectively, where ${\boldsymbol n_r}$ is a hyper-parameter.


% % The whole system can be trained in an end-to-end manner, and the loss function is
% % loss function = 


% \vspace{-0.2cm}
\section{Numerical results}
\label{sec:simulation}
In this section, we compare the proposed SCST, including sentiment analysis and QA tasks, with other DNN algorithms and the traditional source coding and channel coding methods over different channels.

\subsection{Experimental Settings}
The adopted dataset for sentiment analysis is the SST2 dataset\cite{SST2}, which is always used to predict sentiment from long movie reviews. The adopted dataset for QA is the bAbI dataset\cite{bAbI}, which consists of 20 subtasks. For each subtask, there are 1000 questions for training, and 1000 for testing. For the baselines, we adopt a task-oriented joint source-channel coding system, a deep learning-based semantic communication system\cite{Xie_Deep}, and the traditional communication system for separate source and channel coding.
\begin{itemize}
    \item[$\bullet$] Error-free Transmission: The full, noiseless texts are delivered to the receiver, which will serve as the upper bound. We label this as "Error\_free" in the simulation figures.
    \item[$\bullet$] Task-oriented DNN-based joint source-channel coding (JSCC): For sentiment analysis task, the joint codec network consists of Bidirectional Long Short-Term Memory (BiLSTM) layers\cite{BiLSTM}. For the QA task, the joint codec network consists of an End-to-end Memory Network (E2EMN)\cite{E2EMN}. Based on task-oriented JSCC, the receiver can directly execute the task based on the received semantic features without reconstructing the original text. We label this benchmark as DeepJSCC in the simulation figures.
    \item[$\bullet$] Deep semantic communication based on Transformer: The text is first transmitted via the DeepSC method\cite{Xie_Deep}, and reconstructed at the receiver. Subsequently, the reconstructed text is input into the downstream task network to obtain the desired results. We label this benchmark as DeepSC-Transformer in the simulation figures.
    \item[$\bullet$] Traditional methods: To perform the source and channel coding separately, we use Huffman coding for source coding, Reed-Solomon (RS) coding for channel coding, and 16-QAM for modulation, and then execute the task based on the recovered text. We label it as "Huffman+RS" in the simulation figures. It's worth mentioning that traditional communication systems are not the only ones used here. The source coding can also choose L-Z coding and other coding methods, while the channel coding can choose turbo coding, LDPC coding, and other coding methods.
 \end{itemize}
 
Accuracy is used to measure the performance of these two tasks. The experiments are performed by the computer with Ubuntu16.04 + CUDA11.0, and the selected deep learning framework is Pytorch. The training parameters are summarized in Table \ref{tab:experiment}.

\begin{table}[t]
	\normalsize
	\vspace{-0.1cm}
	\centering
	\caption{TRAINING PARAMETERS.}
	\setlength{\abovecaptionskip}{-0.5cm}
	\begin{tabular}{ccc}
		\toprule
		Parameter & Sentiment Analysis & Question-Answering \\
		\hline
		Epochs & 5 & 300 \\
		Batch size & 8 & 128\\
		Optimizer & Adam & Adam\\
		Learning rate & $1\times 10^{-5}$ & $6\times 10^{-4}$\\
		Drop & 0.3 & 0.5\\
		\toprule
	\end{tabular}
	\vspace{-0.cm}
	\label{tab:experiment}
\end{table}

\subsection{Experimental Results Analysis}

\begin{figure}
\centering
\includegraphics[width=1\linewidth]{Example_of_the_whole_system.pdf}
\caption{An example of the proposed task-oriented SCST.}
\label{example of the system}
\end{figure}

Fig. \ref{example of the system} shows an example of the proposed task-oriented SCST. In Fig. \ref{example of the system}, the transmitter needs to transmit a text, as shown in Fig. \ref{example of the system}(a), to the receiver to accomplish intelligent tasks. During the communication process, the transmitter first extracts the semantics from the original text, which is represented via some triplets, as shown in Fig. \ref{example of the system}(b). The main entities and their corresponding relations are represented as nodes and edges, respectively. Based on the final task, the transmitter then filter the redundant and task-irrelevant semantic triplets to generate transmitted partial task-relevant semantics. Fig. \ref{example of the system}(c) illustrates the filtered results corresponding to various tasks. From Fig. \ref{example of the system}(b) and Fig. \ref{example of the system}(c), we can see that the selected semantic triplets are variable for different task, since different tasks focus on different semantic information. This means that the proposed SCST can discard task-irrelevant triplets to reduce the amount of transmitted data. At the receiver, the received semantics can be directly used for implementing the intelligent task, as shown in Fig. \ref{example of the system}(d).

\begin{figure*}[t]
\centering
\subfigure[AWGN Channels]{
\includegraphics[width=0.48\linewidth]{SA_AWGN-eps-converted-to.pdf}}
\subfigure[Rayleigh Channels]{
\includegraphics[width=0.48\linewidth]{SA_Rayleigh-eps-converted-to.pdf}}
\caption{Sentiment analysis accuracy versus channel SNRs for AWGN and Rayleigh fading channels.}
\label{fig:SA_AWGN}
\end{figure*}
Fig. \ref{fig:SA_AWGN} shows the relationship between the accuracy of sentiment analysis and the SNR over AWGN and Rayleigh fading channels. As shown in these figures, the accuracy increases with SNR and gradually converges to a certain threshold. This is because higher SNR means a better communication condition and can decrease transmission errors, which consequently increases the task performance at the receiver. It can be observed that the proposed SCST outperforms the DeepJSCC across the entire SNR region. This is because the proposed SCST can extract lossless semantics by using the semantic triplets to represent the semantics of texts. In addition, SCST can further compress semantics based on the final task, which can make full use of the prior information related to the task. Moreover, it can be found that both SCST and DeepJSCC outperform the traditional communication method, especially in low SNR regions. Compared with the traditional communication method, the proposed SCST achieves 43.5\% accuracy gains when SNR equals 5 dB in the AWGN channel. This is because deep learning-based methods take the channel condition into consideration in the training process and have better robustness to channel noise.

\begin{figure*}[t]
\centering
\subfigure[AWGN Channels]{
\includegraphics[width=0.48\linewidth]{QA_AWGN-eps-converted-to.pdf}}
\subfigure[Rayleigh Channels]{
\includegraphics[width=0.48\linewidth]{QA_Rayleigh-eps-converted-to.pdf}}
\caption{Question Answering accuracy versus channel SNRs for AWGN and Rayleigh fading channels.}
\label{fig:QA_AWGN}
\end{figure*}
The accuracy of the QA task versus the SNR over different channels is depicted in Fig. \ref{fig:QA_AWGN}. From these figures, we can observe that the proposed SCST achieves better performance than DeepJSCC and the traditional communication method, and is about to approach the upper bound at high SNR regime. Similarly to Fig. \ref{fig:SA_AWGN}, Fig. \ref{fig:QA_AWGN} demonstrates that the accuracy increases as the SNR due to high SNR decreases the transmission distortion. In addition, the traditional method suffers from the \emph{cliff effect}, which results in a sharp decrease in the performance when the channel condition is worse than a threshold. Moreover, it can also be observed that the proposed SCST harvests significant performance gains compared to the traditional communication method and DeepJSCC. Specifically, the proposed SCST can achieve 52\% and 8.7\% accuracy gains when SNR equals 5 dB in the AWGN channel, respectively, which verifies the effectiveness of the proposed semantic communication system. In addition, the amount of transmitted data of SCST is less than the baselines, this is because compression in SCST is conducted not only in semantic representation but also in semantic encoding. Based on the experimental results in Fig. \ref{fig:SA_AWGN} and Fig. \ref{fig:QA_AWGN}, we can conclude that the proposed SCST achieves the best performance and is general enough to suit various intelligent tasks.

\renewcommand\arraystretch{1.3}
\begin{table*}[ht]
    \normalsize
    \centering
    \caption{PART OF THE RESULTS FOR SENTIMENT ANALYSIS AT 10 DB OVER RAYLEIGH CHANNELS.}
    \begin{tabular}{|c|p{11.5cm}<{\centering}|c|}
    \hline
        \multicolumn{3}{|l|}{\cellcolor[HTML]{F5D9D7}\begin{tabular}[c] {@{}l@{}}\textbf{Original Text: }Reggio falls victim to relying on the very digital technology that he fervently scorns, creating a \\meandering, inarticulate and ultimately disappointing film.\end{tabular}}\\
        \multicolumn{3}{|l|}{\cellcolor[HTML]{F5D9D7}\textbf{Label: }Negative}\\ \hline
        Methods & Recovered Text & Task Result  \\ \hline
        \bf{SCST} & \textbf{} & Negative $\large \color{green}\checkmark$ \\ \hline
        \bf{DeepJSCC} & \textbf{} & Positive {\color{red}{\XSolidBrush}} \\ \hline
        \bf{DeepSC-Transformer} &  & xxx {\color{red}{\XSolidBrush}} \\ \hline
        \bf{Huffman + RS} & \begin{tabular}[l]{@{}l@{}}Reggio falls victim to relying on the very digitad ae renology that he fervently \\scorns, creating a meanderinnrhs dodudeulatetqr sltimaiely disappointing film.\end{tabular} & Negative $\large \color{green}\checkmark$ \\ \hline
        \multicolumn{3}{|l|}{\cellcolor[HTML]{F5D9D7}\begin{tabular}[l]{@{}l@{}}\textbf{Original Text: }for the first two thirds of this sparklingly inventive and artful, always fast and furious tale, kids will go \\happily along for the ride.\end{tabular}}\\
        \multicolumn{3}{|l|}{\cellcolor[HTML]{F5D9D7}\textbf{Label: }Positive}\\ \hline
        Methods & Recovered Text & Task Result  \\ \hline
        \bf{SCST} & \textbf{} & Positive $\large \color{green}\checkmark$ \\ \hline
        \bf{DeepJSCC} & \textbf{} & Negative {\color{red}{\XSolidBrush}} \\ \hline
        \textbf{DeepSC-Transformer} &  & xxx {\color{red}{\XSolidBrush}} \\ \hline
        \textbf{Huffman + RS} & \begin{tabular}[l]{@{}l@{}}for the first two thirds of this sparyocwioor inventive and artful, always fast and \\furious tale, kids will go happily along f e  sbsride\end{tabular} & Negative {\color{red}{\XSolidBrush}} \\ \hline
        \multicolumn{3}{|l|}{\cellcolor[HTML]{F5D9D7}\begin{tabular}[l]{@{}l@{}}\textbf{Original Text: }though the controversial korean filmmaker's latest effort is not for all tastes, it offers gorgeous imagery,\\ effective performances, and an increasingly unsettling sense of foreboding\end{tabular}}\\
        \multicolumn{3}{|l|}{\cellcolor[HTML]{F5D9D7}\textbf{Label: }Positive}\\ \hline
        Methods & Recovered Text & Task Result  \\ \hline
        \textbf{SCST} & \textbf{} & Positive $\large \color{green}\checkmark$ \\ \hline
        \textbf{DeepJSCC} & \textbf{} & Positive $\large \color{green}\checkmark$ \\ \hline
        \textbf{DeepSC-Transformer} & & xxx {\color{red}{\XSolidBrush}} \\ \hline
        \textbf{Huffman + RS} & \begin{tabular}[l]{@{}l@{}}though the controversial korean fymbmhce lc  late tfnna s n  not for all tastes, it\\ offers gorgeous imagery, effective performances, and an increasingly unsettling\\ sense of foreboding\end{tabular} & Negative {\color{red}{\XSolidBrush}}\\ \hline
    \end{tabular}
    \label{results of SA}
\end{table*}

\renewcommand\arraystretch{1.3}
\begin{table*}[ht]
    \normalsize
    \centering
    \caption{PART OF THE RESULTS FOR QUESTION ANSWERING TASK AT 10 DB OVER RAYLEIGH CHANNELS.}
    \begin{tabular}{|c|c|c|}
    \hline
        \multicolumn{3}{|l|}{\cellcolor[HTML]{F5D9D7}\textbf{Original Text: }\emph{The hallway is east of the bathroom. The bedroom is west of the bathroom.}}\\
        \multicolumn{3}{|l|}{\cellcolor[HTML]{F5D9D7}\textbf{Question: }\emph{What is the bathroom east of?}}\\ \hline
        Methods & Recovered Text & Task Result  \\ \hline
        \textbf{SCST} & \textbf{} & bedroom $\large \color{green}\checkmark$ \\ \hline
        \textbf{DeepJSCC} & \textbf{} & hallway {\color{red}{\XSolidBrush}} \\ \hline
        \textbf{DeepSC-Transformer} & the hallway is west of the bathroom. the bathroom is west of the bathroom. & hallway {\color{red}{\XSolidBrush}} \\ \hline
        \textbf{Huffman + RS} & The hallway is east of the bathroom. The bedroom is west of the bathroom. & bedroom $\large \color{green}\checkmark$ \\ \hline
        \multicolumn{3}{|l|}{\cellcolor[HTML]{F5D9D7}\textbf{Original Text: }\emph{The bedroom is west of the kitchen. The hallway is west of the bedroom.}}\\
        \multicolumn{3}{|l|}{\cellcolor[HTML]{F5D9D7}\textbf{Question: }\emph{What is west of the kitchen?}}\\ \hline
        Methods & Recovered Text & Task Result  \\ \hline
        \textbf{SCST} & \textbf{} & bedroom $\large \color{green}\checkmark$ \\ \hline
        \textbf{DeepJSCC} & \textbf{} & bedroom $\large \color{green}\checkmark$ \\ \hline
        \textbf{DeepSC-Transformer} & the bedroom is west of the kitchen. the bedroom is west of the hallway. & bedroom $\large \color{green}\checkmark$ \\ \hline
        \textbf{Huffman + RS} & The bedroohils west of  r  saeechen. The hallway is west of the bedroom. & garden {\color{red}{\XSolidBrush}} \\ \hline
        \multicolumn{3}{|l|}{\cellcolor[HTML]{F5D9D7}\textbf{Original Text: }\emph{The bathroom is north of the garden. The hallway is north of the bathroom.}}\\
        \multicolumn{3}{|l|}{\cellcolor[HTML]{F5D9D7}\textbf{Question: }\emph{What is north of the garden?}}\\ \hline
        Methods & Recovered Text & Task Result  \\ \hline
        \textbf{SCST} & \textbf{} & bathroom $\large \color{green}\checkmark$ \\ \hline
        \textbf{DeepJSCC} & \textbf{} & bathroom $\large \color{green}\checkmark$ \\ \hline
        \textbf{DeepSC-Transformer} & the kitchen is south of the bathroom. the kitchen is south of the bedroom. & bedroom {\color{red}{\XSolidBrush}} \\ \hline
        \textbf{Huffman + RS} & The bathroom is north of the garden. Tdehallway is north of the bathrodi. & office {\color{red}{\XSolidBrush}} \\ \hline
    \end{tabular}
    \label{results of QA}
\end{table*}

The visualized results for the considered tasks including sentimental analysis and question-answering are shown in Table \ref{results of SA} and Table \ref{results of QA}, respectively. Table \ref{results of SA} shows the recovered text and the sentimental analysis results for SCST and baselines at 10 dB over Rayleigh channel. The proposed SCST correctly classifies the sentiment of the text without recovering the original text.  Although the DeepSC method can partially recover the original text, the examples show that it may lose the most crucial semantic information (directly related to the task), making it difficult to successfully perform sentiment analysis. The traditional communication method failed to successfully restore the original text, resulting in task failure. Table \ref{results of QA} shows the recovered text and the question-answering results for SCST and baselines at 10 dB over AWGN channel. Similar to the sentiment analysis task, the proposed SCST correctly answers all questions. The performance of DeepJSCC is second only to SCST, indicating that directly focusing on semantic communication for intelligent tasks can achieve better task performance. This is because the most task-relevant semantic information can be preserved under the guidance of intelligent tasks, albeit at the cost of losing access to the original text. The traditional communications and the DeepSC-based text transmission can only give the right answers to partial questions. The reason is that although the recovered text may suffer from semantic loss, it can still aid the system in guessing and narrowing down the search range for the answer, thereby facilitating the question-answering task to some extent.

\begin{figure}
\centering
\includegraphics[width=1\linewidth]{Transmitted_number_of_symbols.png}
\caption{The number of transmitted symbols comparison between SCST and various baselines.}
\label{transmitted number of symbols}
\end{figure}

The numbers of transmission symbols for different methods and various tasks are compared in Fig. \ref{transmitted number of symbols}. All of the displayed results are the average number of symbols transmitted per sentence computed across the entire dataset. It's noteworthy that all of the deep learning-based methods achieve relatively lower numbers of transmission symbols, demonstrating the powerful compression capability of neural networks and highlighting the advantages of semantic communications. Furthermore, the figure clearly shows that the SCST method transmits fewer symbols compared to DeepJSCC, while transmitting similar numbers of symbols as DeepSC-Transformer. This is attributed to the proposed semantic filtering approach, which can efficiently discard redundant and irrelevant semantics, demonstrating the effectiveness of the proposed semantic representation method. Consequently, we are highly confident that the proposed SCST is suitable for use in scenarios where lower latency is necessary.


\renewcommand\arraystretch{1.5}
\begin{table*}[t]
    \normalsize
    \centering
    \caption{COMPUTATIONAL COMPLEXITY COMPARISON BETWEEN THE PROPOSED SCST AND OTHER COMMUNICATION METHODS.}
    \begin{tabular}{|p{4cm}<{\centering}|p{4cm}<{\centering}|c|}
    \hline
        Task & Method & Computational Complexity (FLOPs) \\ \hline
        \multirow{4}*{Sentiment Analysis} & SCST &  \\ \cline{2-3}
        ~ & DeepJSCC &  \\ \cline{2-3}
        ~ & DeepSC-Transformer & ~ \\ \cline{2-3}
        ~ & Huffman+Turbo & ~ \\ \hline
        \multirow{4}*{Question Answering} & SCST & 4.6 $\times 10^7$ \\ \cline{2-3}
        ~ & DeepJSCC & 4.8 $\times 10^7$ \\ \cline{2-3}
        ~ & DeepSC-Transformer & 8.3 $\times 10^7$ \\ \cline{2-3}
        ~ & Huffman+Turbo & 9.3 $\times 10^7$ \\ \toprule
    \end{tabular}
    \label{Computational Complexity}
\end{table*}

Table \ref{Computational Complexity} compares the computational complexity of various communication methods. As suggested in this table, all of the deep learning-based methods exhibit lower computational complexity than traditional communication methods, with the complexity of SCST capable of being reduced by over 50\%. Besides, both SCST and DeepJSCC have a lower computational complexity than DeepSC-Transformer and traditional method. This is because the former are designed directly towards intelligent tasks, while the latter need to perform tasks after recovering the text, with transmission and understanding being separate. Notably, the proposed SCST can achieve the lowest complexity among all the systems, indicating its suitability for energy-constrained scenarios where low power consumption is essential for transmitting large amounts of data.

\section{Conclusion}
\label{conclusion}
In this paper, we have established the SCST system, a semantic triplets-based communication system that leverages explainable semantic triplets to represent semantics, enabling the system to handle various tasks. We have proposed a novel semantic representation method that comprises of semantic extraction and filtering to transform transmitted data into semantic triplets. The filtering process eliminates redundant or irrelevant semantics based on the specific task requirements. To evaluate the effectiveness of SCST, we have conducted sentiment analysis and question-answering tasks. Simulation results demonstrate that SCST outperforms various benchmarks, particularly in low SNR environments. Consequently, we believe that SCST is a promising candidate for task-oriented semantic communication systems for transmitting text data.
% In this paper, we have established a semantic triplets-based communication system, named SCST, in which semantics is represented via the explainable semantic triplets, and various tasks are considered. Specifically, a novel semantic representation method, consisting of semantic extraction and semantic filtering, is proposed to convert the transmitted data into semantic triplets, where useless and redundant semantics can be filtered according to the final task. In addition, to verify the effectiveness of the SCST, we have conducted SCST for the sentiment analysis task and question-answering task, respectively. The simulation results have demonstrated that the SCST outperforms various benchmarks, especially in low SNR regimes. Therefore, we are highly confident that the proposed SCST is a promising candidate for task-oriented semantic communication systems to transmit text data.

%\begin{appendices}
%\section{$Proof\ of\ Lemma\ 1$}
%\begin{proof}
%*******.
%
%This ends the proof.
%\end{proof}
%\vspace{-0.2cm}
%\end{appendices}
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\nocite{*}\bibliography{stimreference}

\end{document}
