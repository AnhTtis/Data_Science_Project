\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\usepackage[pdftex,
            pdfusetitle,
            pdfauthor={Axel Huebl (orcid.org/0000-0003-1943-7141) et al.},
            pdfsubject={A proceedings paper to an invited talk at the Advanced Accelerator Concepts (AAC) Workshop in its 2022 edition.},
            pdfkeywords={simulation, exascale, particle accelerator, particle-mesh, particle-in-cell, laser-plasma, modeling, HPC},
            hidelinks,  % remove colored boxes
            pdfproducer={LaTeX with hyperref},
            pdfcreator={pdflatex}]{hyperref}

\begin{document}

% shorten author lists
\bstctlcite{aac22:BSTcontrol}

% AAC2022
%   https://www.aac2022.org/proceedings/
%   Page Limit: The proceedings should be limited to five pages
%   talk: https://docs.google.com/presentation/d/1fG_uRHXinXkRUhYjVknecvKF4puXvUmfbew6gQ-NfBU/edit

\title{From Compact Plasma Particle Sources to Advanced Accelerators with Modeling at Exascale\\
\thanks{This research was supported by the Exascale Computing Project (17-SC-20-SC), a joint project of the U.S. Department of Energy's Office of Science and National Nuclear Security Administration, responsible for delivering a capable exascale ecosystem, including software, applications, and hardware technology, to support the nation's exascale computing imperative.
This material is based upon work supported by the CAMPA collaboration, a project of the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research and Office of High Energy Physics, Scientific Discovery through Advanced Computing (SciDAC) program.
This work was supported by the Laboratory Directed Research and Development Program of Lawrence Berkeley National Laboratory under U.S. Department of Energy Contract No. DE-AC02-05CH11231 and by LLNL under Contract DE-AC52-07NA27344.
This research used resources of the Oak Ridge Leadership Computing Facility, which is a DOE Office of Science User Facility supported under Contract DE-AC05-00OR22725, the National Energy Research Scientific Computing Center (NERSC), a U.S. Department of Energy Office of Science User Facility located at Lawrence Berkeley National Laboratory, operated under Contract No. DE-AC02-05CH11231, and the supercomputer Fugaku provided by RIKEN.}
}

% Co-authors based on Title/Scope (accelerator related contribs or research)
\author{\IEEEauthorblockN{%
Axel Huebl\href{https://orcid.org/0000-0003-1943-7141}{\includegraphics[scale=0.19]{orcid.png}},
R{\'e}mi Lehe\href{https://orcid.org/0000-0002-3656-9659}{\includegraphics[scale=0.19]{orcid.png}},
Edoardo Zoni\href{https://orcid.org/0000-0001-5662-4646}{\includegraphics[scale=0.19]{orcid.png}},
Olga Shapoval\href{https://orcid.org/0000-0003-4003-4507}{\includegraphics[scale=0.19]{orcid.png}},\\
Ryan T. Sandberg\href{https://orcid.org/0000-0001-7680-8733}{\includegraphics[scale=0.19]{orcid.png}},
Marco Garten\href{https://orcid.org/0000-0001-6994-2475}{\includegraphics[scale=0.19]{orcid.png}},
Arianna Formenti\href{https://orcid.org/0000-0002-7887-9313}{\includegraphics[scale=0.19]{orcid.png}},\\
Revathi Jambunathan\href{https://orcid.org/0000-0001-9432-2091}{\includegraphics[scale=0.19]{orcid.png}},
Prabhat Kumar\href{https://orcid.org/0000-0002-8454-7497}{\includegraphics[scale=0.19]{orcid.png}},
Kevin Gott\href{https://orcid.org/0000-0003-3244-5525}{\includegraphics[scale=0.19]{orcid.png}},\\
Andrew Myers\href{https://orcid.org/0000-0001-8427-8330}{\includegraphics[scale=0.19]{orcid.png}},
Weiqun Zhang\href{https://orcid.org/0000-0001-8092-1974}{\includegraphics[scale=0.19]{orcid.png}},
Ann Almgren\href{https://orcid.org/0000-0003-2103-312X}{\includegraphics[scale=0.19]{orcid.png}},\\
Chad E. Mitchell\href{https://orcid.org/0000-0002-1986-9852}{\includegraphics[scale=0.19]{orcid.png}},
Ji Qiang,
Jean-Luc Vay\href{https://orcid.org/0000-0002-0040-799X}{\includegraphics[scale=0.19]{orcid.png}}%
}
\IEEEauthorblockA{
\textit{Lawrence Berkeley National Laboratory}\\
Berkeley (CA), USA\\
\{axelhuebl,jlvay\}@lbl.gov}
%
\and
%
\IEEEauthorblockN{%
Alexander Sinn\href{https://orcid.org/0000-0002-4485-971X}{\includegraphics[scale=0.19]{orcid.png}},
Severin Diederichs\href{https://orcid.org/0000-0001-9079-0461}{\includegraphics[scale=0.19]{orcid.png}},\\
Maxence Th{\'e}venet\href{https://orcid.org/0000-0001-7216-2277}{\includegraphics[scale=0.19]{orcid.png}}%
}
\IEEEauthorblockA{\textit{Deutsches Elektronen-Synchrotron (DESY)}\\
Hamburg, Germany}
%
\and
%
\IEEEauthorblockN{%
David Grote\href{https://orcid.org/0000-0002-4057-8582}{\includegraphics[scale=0.19]{orcid.png}}%
}
\IEEEauthorblockA{\textit{Lawrence Livermore National Laboratory}\\
Livermore (CA), USA}
%
\and
%
\IEEEauthorblockN{%
Luca Fedeli\href{https://orcid.org/0000-0002-7215-4178}{\includegraphics[scale=0.19]{orcid.png}},
Thomas Clark\href{https://orcid.org/0000-0002-0141-8674}{\includegraphics[scale=0.19]{orcid.png}},
Neil Za{\"i}m\href{https://orcid.org/0000-0003-0313-4496}{\includegraphics[scale=0.19]{orcid.png}},
Henri Vincenti\href{https://orcid.org/0000-0002-9839-2692}{\includegraphics[scale=0.19]{orcid.png}}%
}
\IEEEauthorblockA{\textit{LIDYL, CEA-Universit{\'e} Paris-Saclay, CEA Saclay}\\
Gif-sur-Yvette, France}
}

\maketitle

\begin{abstract}
Developing complex, reliable advanced accelerators requires a coordinated, extensible, and comprehensive approach in modeling, from source to the end of beam lifetime.
We present highlights in Exascale Computing to scale accelerator modeling software to the requirements set for contemporary science drivers. %in the community, both in the US and internationally.
In particular, we present the first laser-plasma modeling on an exaflop supercomputer using the US DOE Exascale Computing Project WarpX.
%This includes laser-plasma modeling on an exaflop supercomputer using the US DOE Exascale Computing Project WarpX [1-4] as well as progress of PIConGPU in the OLCF Center for Accelerated Application Readiness (CAAR) project for the same machine, and further projects.
%
Leveraging developments for Exascale, the new DOE SCIDAC-5 Consortium for Advanced Modeling of Particle Accelerators (CAMPA) will advance numerical algorithms and accelerate community modeling codes in a cohesive manner: from beam source, over energy boost, transport, injection, storage, to application or interaction.
Such start-to-end modeling will enable the exploration of hybrid accelerators, with conventional and advanced elements, as the next step for advanced accelerator modeling.
Following open community standards, we seed an open ecosystem of codes that can be readily combined with each other and machine learning frameworks.
These will cover ultrafast to ultraprecise modeling for future hybrid accelerator design, even enabling virtual test stands and twins of accelerators that can be used in operations.
\end{abstract}

\begin{IEEEkeywords}
simulation, exascale, particle accelerator, particle-mesh, particle-in-cell, laser-plasma, modeling, HPC
\end{IEEEkeywords}

\section{Introduction}
Research of plasma-based accelerators has achieved significant milestones over the last decade.
Highlights include achieving nearly 8\,GeV electrons in a single-stage source~\cite{Gonsalves2019}, nC-class electron beams~\cite{Couperus2017}, demonstrating plasma-based FELs~\cite{Wang2021,Galletti2022,Labat2023}, reaching stable proton acceleration of ultra-short, nC-class pulses \cite{Hilz2018} and enabling studies into ultrahigh dose rate radiotherapy~\cite{Kroll2022,Bin2022,Geulig2022}.
As the exploratory aspect of the field benefits significantly from the elucidation of fundamental processes through simulations, transitioning from intriguing sources to scalable accelerators requires universally integrated, quantitatively predictive capabilities for design and operations.

\section{Next-Generation, Advanced Particle Accelerator Modeling}
Computational modeling is an established method in particle accelerator research.
Modeling fulfills tasks from exploratory research for new particle accelerator concepts (including beam physics, model building, validation) to design and optimization of accelerator components.
For these tasks, computational model choices need to strike a balance between running with high fidelity (e.g., for detailed physics studies) or high speed (e.g., for ensemble runs).
Figure~\ref{fig:speedfidelity} provides a schematic overview about possible model choices in a simulations workflow.

In advanced particle accelerator modeling, both extremes of modeling choices often benefit from using leadership-scale supercomputers.
High fidelity modeling is a typical capability computing task in high-performance computing (HPC), requiring large portions of leadership-scale supercomputers at %
%
\begin{figure}[htbp]
  \centerline{\includegraphics[width=\columnwidth]{speed-fidelity.eps}}
  \caption{Preference of speed (time-to-solution) or fidelity (accuracy) of results is determined by implemented algorithms and models in codes.
  Faster codes are preferred for design studies and optimization, more accurate codes for model building, validation and exploration.}
  \label{fig:speedfidelity}
\end{figure}%
%
the same time to model a single simulation.
On the other hand, fast modeling of ensembles is a typical capacity computing task in high-throughput computing (HTC).

As a consequence of these needs, the community developed more than one ``type'' of particle accelerator modeling code.
Typical choices include fully-kinetic, electromagnetic particle-in-cell codes as the most accurate (and computationally expensive), electrostatic approximations, as well as (partially) fluid-based models.
Most implemented methods these days are explicit algorithms, which iterate forward along either the independent variable of time $t$ or the reference trajectory $s$ of a beam.
Implicit methods are more common in general plasma physics and their research is beneficial for applications with high accuracy requirements, e.g., for long-time stable modeling with high demands on energy conservation.

Scientific drivers for accelerator and beam physics research were recently described as ``Grand Challenges'' in the 2021 Snowmass HEP community exercise~\cite{NagaitsevChallenges2021}:
order-of-magnitude increases in beam intensity, beam quality and phase space density, beam control and beam prediction directly translate into significant needs for computational resolution (grids and no. of particles modeled) and predictive quality (simulate all the particles, conductors, dark currents, many turns, etc.).
In advanced accelerator modeling, specific drivers that require Exascale-supercomputing resources include staged, wakefield-based acceleration for future particle colliders, compact (X)FEL sources, high-field physics experiments (QED) and novel, ultra-intense proton/ion sources.
Closely related scientific domains that benefit from the same modeling capabilities are in high-energy density laser-plasma physics, fusion-energy science, extreme-field science, astrophysical plasmas, light-source modeling, and applications of compact acceleration sources to medical and industrial applications.

\section{Advanced Particle Accelerator Modeling at Exascale}

%\subsection{A Cambrian Explosion of compute architectures}
% maybe

\subsection{Exascale Computing in the US}

The US DOE Exascale Computing Project (ECP) has set its goal to prepare scientists and computing facilities for supercomputers capable of $10^{18}$ double-precision floating point operations per second (1\,ExaFlop/s).
As part of ECP, developed scientific application software aims to reach a domain-specific figure-of-merit that is $50\times$ higher than at project start 7 years earlier.

Targeting the science case of staged wakefield acceleration, \verb|WarpX|\footnote{\href{https://ecp-warpx.github.io}{Homepage: ecp-warpx.github.io}}~\cite{Vay2018,Vay2021,FedeliHuebl2022} is developed in ECP as the successor to the successful \verb|Warp| code~\cite{Vay2013}.
As a particle-in-cell code, the figure-of-merit of \verb|WarpX| in ECP is its parallel particle-and-cell update rate per second.
Recently, \verb|WarpX| completed the ECP goal and even reached a $500\times$ improvement to pre-ECP, due to significant improvements in both hardware and software~\cite{FedeliHuebl2022}.

\subsection{First Runs on Exascale Machines Achieved}

In 2022, the first reported Exascale machine was revealed at Oak Ridge National Lab, called Frontier.\footnote{\url{https://www.olcf.ornl.gov/frontier/}}
Frontier is deployed as part of ECP and is powered by 9,408 compute nodes, each predominantly computing on four AMD MI250X GPUs per node, each with 2 Graphics Compute Dies.
In July 2022, \verb|WarpX|~\cite{FedeliHuebl2022} ran for the first time on the full scale of this Exascale machine.%
\footnote{Also reported at AAC22: Few days later, the PIConGPU~\cite{Huebl2019} team also measured full-scale OLCF Frontier performance results.}

\begin{figure}[htb]
\centering
%\vspace{-3mm}
\includegraphics[width=\columnwidth]{scaling_double_precision.pdf}
 \caption{
  Weak and strong scaling of WarpX relative to system size.
  Ideal scaling is the grey dotted line at 100\,\% in both graphs.
  Frontier and Perlmutter measurements were taken prior to their system acceptance dates.
  The network hardware of Perlmutter has since been updated from HPE Slingshot 10 to 11.
  Systems: Frontier (OLCF), Fugaku (Riken), Summit (OLCF), Perlmutter (NERSC).
  Published in~\cite{FedeliHuebl2022}.}
  \label{fig:scaling}
%\vspace{-4mm}
\end{figure}

Over the project years of ECP, \verb|WarpX| has been developed with a performance-portable, single-source GPU-CPU programming model~\cite{Myers2021}.
As a consequence, \verb|WarpX| can run on traditional CPU machines and machines that use hardware accelerators, which are as of today GPUs by three different vendors (Nvidia, AMD, Intel).

\begin{figure*}[htb]
\centering
%\vspace{-3mm}
\includegraphics[width=\textwidth]{blast_codes.eps}
 \caption{
  Overview of a toy accelerator complex with beamline elements.
  As more BLAST codes are modernized for GPU-support, mesh-refinement and Exascale, projects evolve into the new Exascale code base~\cite{Huebl2022}.
  Abbreviations: Interaction Point (IP), electrostatic (ES), Free Electron Laser (FEL).}
  \label{fig:exablast}
%\vspace{-4mm}
\end{figure*}

Figure~\ref{fig:scaling} from reference~\cite{FedeliHuebl2022} shows the scalability of \verb|WarpX| on some of the largest supercomputers available today.
Highly relevant for the aforementioned challenges in accelerator modeling is the left plot, weak scaling, which shows the code performance when increasing both computational domain and provided parallel computing power on a supercomputer.
\verb|WarpX| achieves close to the ideal scaling over 4-5 orders of magnitude of system size increase.
The plot to the right, strong scaling, shows the increase in time-to-solution that can be gained by keeping the computational domain constant, but increasing provided parallel computing power.
Due to communication needs in parallel machines, this plot cannot be arbitrarily scaled for complex applications.
Nonetheless, \verb|WarpX| still achieves a remarkable $>50$\,\% efficiency when scaled more than an order of magnitude, before being limited by data communication and GPU/CPU underutilization.

\verb|WarpX| is developed fully in the open, following open science principles~\cite{OpenScienceUNSECO,OpenScienceEU,OpenSource,FreeSoftware} and modern software engineering practices~\cite{LOI_BestPractices}.
Besides running on supercomputers and cloud providers, \verb|WarpX| can also be used on edge and personal computers with Linux, macOS and Windows operating system.
The latter is particularly useful in designing simulation runs in lower dimension and resolution, as well as for development.

\section{Modernizing BLAST for Exascale}

The \verb|WarpX| code is, as was its predecessor \verb|Warp|, part of the BLAST suite of codes.\footnote{\url{https://blast.lbl.gov}}
BLAST, originally standing for ``Berkeley Lab Accelerator Simulation Toolkit'', was renamed in 2021 to ``\textbf{B}eam, P\textbf{l}asma \& \textbf{A}ccelerator \textbf{S}imulation \textbf{T}oolkit''.
This reflects the grown community that spans maintainers, contributors and collaborators from many international institutions, and applies the BLAST codes to a growing number of applications.
For instance, a code primarily developed outside of LBNL is the quasi-static \verb|HiPACE++| code~\cite{Diederichs2022}, which is maintained by DESY.
Another code is \verb|GEMPIX|, developed at IPP Garching for magnetic confinement fusion plasmas.

Leveraging the success and routines of \verb|WarpX|, other existing BLAST codes are being transitioned to Exascale~\cite{Huebl2022} as well.
Figure~\ref{fig:exablast} shows an overview of the new GPU-capable, mesh-refinement (MR)-enabled codes for accelerator modeling and prominent application.
During this transition, existing codes are rewritten from Fortran to C++ for core compute routines and modern data structures.
A common input layer is developed, which is standardized in the Python API \verb|PICMI| (particle-in-cell modeling interface).\footnote{\url{https://picmi-standard.github.io}}
Time-based ($t$) implementations of the codes \verb|Warp| and \verb|IMPACT-T|~\cite{impactt} are combined in the new electromagnetic and electrostatic \verb|WarpX| code.
Beam-dynamics codes along a reference trajectory $s$ as the independent variable, such as \verb|IMPACT-Z|~\cite{impactz} and $s$-based \verb|Warp| modules, are modernized in the new code \verb|ImpactX|~\cite{Huebl2022} and continue to include collective effects.
Common particle-in-cell routines are shared via the Accelerated BLAST Recipes (\verb|ABLASTR|) library, generalizing \verb|WarpX| routines~\cite{Huebl2022}.

The Open Standard for Particle-Mesh Data (\verb|openPMD|) is used in BLAST codes for data compatibility~\cite{openPMDstandard}.
A high-performance, \verb|openPMD| C++ and Python reference implementation, co-developed by LBNL and HZDR/CASUS, is used in massively parallel codes and in data analysis~\cite{openPMDapi}.
Recently, data streaming techniques were developed to transition traditional post-processing scripts to online, massively parallel data analysis workflows, which can be co-located with simulations and enable rapidly prototyping for scientific \textit{in-transit} analysis~\cite{Poeschel2022}.

\section{Seeding a Community Ecosystem}

From experience over the last years developing community standards such as \verb|openPMD|, \verb|PICMI|, the BLAST toolkit, or the \verb|PIConGPU| software stack~\cite{Huebl2019} - open standardization and modular, open source software collaborations emerge as the necessarily efficient way forward.
From such seed projects, the US DOE SCIDAC-5 Consortium for Advanced Modeling of Particle Accelerators (CAMPA)~\cite{CAMPA} will support advancement of numerical algorithms and accelerate community modeling codes in a cohesive manner: from beam source, over energy boost, transport, injection, storage, to application or interaction.

Making the existing particle accelerator modeling ecosystem compatible and interdependent will be beneficial towards the goal of predictive start-to-end modeling~\cite{LOI_ecosystem}.
As a community, we should expect and ready models and codes for the exploration of ``hybrid'' accelerators, with conventional and advanced elements, as the next step for advanced accelerator modeling.
Following open community standards, one can initiate an open ecosystem of codes that can be readily combined with each other and machine learning frameworks~\cite{Huebl2022}, towards enabling virtual test stands and twins of accelerators that can be used in operations.

\section*{Acknowledgment}

%``R. B. G. thanks$\ldots$''.
%Put sponsor acknowledgments in the unnumbered footnote on the first page.
This research used the open-source particle-in-cell code \verb|WarpX| \url{https://github.com/ECP-WarpX/WarpX}, primarily funded by the US DOE Exascale Computing Project.
Primary \verb|WarpX| contributors are with LBNL, LLNL, CEA-LIDYL, SLAC, DESY, CERN, and TAE.
We acknowledge all \verb|WarpX|, \verb|HiPACE++|, \verb|ImpactX| and \verb|openPMD| contributors.
Slides of the plenary presentation are available under~\cite{slides}.

% References
%\bibliographystyle{IEEEtran}
%\bibliography{biblio}

% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\doi}[1]{\href{https://doi.org/#1}{DOI:#1}}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{Gonsalves2019}
A.~J. Gonsalves, K.~Nakamura, J.~Daniels \emph{et~al.},
  ``\BIBforeignlanguage{en}{Petawatt laser guiding and electron beam
  acceleration to 8 {GeV} in a laser-heated capillary discharge waveguide},''
  \emph{\BIBforeignlanguage{en}{Phys. Rev. Lett.}}, vol. 122, no.~8, p. 084801,
  Mar. 2019.  \doi{10.1103/PhysRevLett.122.084801}

\bibitem{Couperus2017}
J.~P. Couperus, R.~Pausch, A.~K{\"o}hler \emph{et~al.},
  ``\BIBforeignlanguage{en}{Demonstration of a beam loaded nanocoulomb-class
  laser wakefield accelerator},'' \emph{\BIBforeignlanguage{en}{Nat. Commun.}},
  vol.~8, no.~1, p. 487, Sep. 2017.  \doi{10.1038/s41467-017-00592-7}

\bibitem{Wang2021}
W.~Wang, K.~Feng, L.~Ke \emph{et~al.}, ``\BIBforeignlanguage{en}{Free-electron
  lasing at 27 nanometres based on a laser wakefield accelerator},''
  \emph{\BIBforeignlanguage{en}{Nature}}, vol. 595, no. 7868, pp. 516--520,
  Jul. 2021.  \doi{10.1038/s41586-021-03678-x}

\bibitem{Galletti2022}
M.~Galletti, D.~Alesini, M.~P. Anania \emph{et~al.}, ``Stable operation of a
  free-electron laser driven by a plasma accelerator,'' \emph{Phys. Rev.
  Lett.}, vol. 129, p. 234801, Nov 2022.  \doi{10.1103/PhysRevLett.129.234801}

\bibitem{Labat2023}
M.~Labat, J.~C. Cabada{\u g}, A.~Ghaith \emph{et~al.},
  ``\BIBforeignlanguage{en}{Seeded free-electron laser driven by a compact
  laser plasma accelerator},'' \emph{\BIBforeignlanguage{en}{Nat. Photonics}},
  vol.~17, no.~2, pp. 150--156, Feb. 2023.  \doi{10.1038/s41566-022-01104-w}

\bibitem{Hilz2018}
P.~Hilz, T.~M. Ostermayr, A.~Huebl \emph{et~al.},
  ``\BIBforeignlanguage{en}{Isolated proton bunch acceleration by a petawatt
  laser pulse},'' \emph{\BIBforeignlanguage{en}{Nat. Commun.}}, vol.~9, no.~1,
  Dec. 2018.  \doi{10.1038/s41467-017-02663-1}

\bibitem{Kroll2022}
F.~Kroll, F.-E. Brack, C.~Bernert \emph{et~al.},
  ``\BIBforeignlanguage{en}{Tumour irradiation in mice with a laser-accelerated
  proton beam},'' \emph{\BIBforeignlanguage{en}{Nat. Phys.}}, vol.~18, no.~3,
  pp. 316--322, Mar. 2022.  \doi{10.1038/s41567-022-01520-3}

\bibitem{Bin2022}
J.~Bin, L.~Obst-Huebl, J.-H. Mao \emph{et~al.}, ``\BIBforeignlanguage{en}{A new
  platform for ultra-high dose rate radiobiological research using the {BELLA}
  {PW} laser proton beamline},'' \emph{\BIBforeignlanguage{en}{Sci. Rep.}},
  vol.~12, no.~1, p. 1484, Jan. 2022.  \doi{10.1038/s41598-022-05181-3}

\bibitem{Geulig2022}
L.~D. Geulig, L.~Obst-Huebl, K.~Nakamura \emph{et~al.}, ``Online charge
  measurement for petawatt laser-driven ion acceleration,'' \emph{Review of
  Scientific Instruments}, vol.~93, no.~10, p. 103301, 2022.
  \doi{10.1063/5.0096423}

\bibitem{NagaitsevChallenges2021}
S.~Nagaitsev, Z.~Huang, J.~Power \emph{et~al.}, ``Accelerator and beam physics
  research goals and opportunities,'' 2021.  \doi{10.48550/ARXIV.2101.04107}

\bibitem{Vay2018}
J.-L. Vay, A.~Almgren, J.~Bell \emph{et~al.},
  ``\BIBforeignlanguage{en}{{Warp-X}: A new exascale computing platform for
  beam--plasma simulations},'' \emph{\BIBforeignlanguage{en}{Nucl. Instrum.
  Methods Phys. Res. A}}, vol. 909, pp. 476--479, Nov. 2018.
  \doi{10.1016/j.nima.2018.01.035}

\bibitem{Vay2021}
J.-L. Vay, A.~Huebl, A.~Almgren \emph{et~al.},
  ``\BIBforeignlanguage{en}{Modeling of a chain of three plasma accelerator
  stages with the {WarpX} electromagnetic {PIC} code on {GPUs}},''
  \emph{\BIBforeignlanguage{en}{Phys. Plasmas}}, vol.~28, no.~2, p. 023105,
  Feb. 2021.  \doi{10.1063/5.0028512}

\bibitem{FedeliHuebl2022}
L.~Fedeli, A.~Huebl, F.~Boillod-Cerneux \emph{et~al.}, ``Pushing the frontier
  in the design of laser-based electron accelerators with groundbreaking
  mesh-refined particle-in-cell simulations on exascale-class supercomputers,''
  in \emph{2022 SC22: International Conference for High Performance Computing,
  Networking, Storage and Analysis (SC) (SC)}.\hskip 1em plus 0.5em minus
  0.4em\relax Los Alamitos, CA, USA: IEEE Computer Society, nov 2022, pp.
  25--36.  \doi{10.1109/SC41404.2022.00008}

\bibitem{Vay2013}
J.-L. Vay, D.~P. Grote, R.~H. Cohen, and A.~Friedman, ``Novel methods in the
  particle-in-cell accelerator code-framework warp,'' \emph{Computational
  Science \& Discovery}, vol.~5, no.~1, p. 014019, dec 2012.
  \doi{10.1088/1749-4699/5/1/014019}

\bibitem{Huebl2019}
A.~Huebl, ``{PIConGPU: Predictive Simulations of Laser-Particle Accelerators
  with Manycore Hardware},'' Ph.D. dissertation, Technische Universitaet
  Dresden, Jul. 2019.  \doi{10.5281/zenodo.3266820}

\bibitem{Myers2021}
A.~Myers, A.~Almgren, L.~D. Amorim \emph{et~al.},
  ``\BIBforeignlanguage{en}{Porting {WarpX} to {GPU-accelerated} platforms},''
  \emph{\BIBforeignlanguage{en}{Parallel Comput.}}, vol. 108, no. 102833, p.
  102833, Dec. 2021.  \doi{10.1016/j.parco.2021.102833}

\bibitem{Huebl2022}
A.~Huebl, R.~Leh{\'e}, C.~Mitchell \emph{et~al.}, ``Next generation
  computational tools for the modeling and design of particle accelerators at
  exascale,'' in \emph{2022 North American Particle Accelerator Conference
  ({NAPAC22})}.\hskip 1em plus 0.5em minus 0.4em\relax JACoW Publishing,
  Geneva, Switzerland, Dec. 2022, invited Oral, {TUYE2}.
  \doi{10.18429/JACoW-NAPAC2022-TUYE2}

\bibitem{OpenScienceUNSECO}
\BIBentryALTinterwordspacing
{United Nations Educational, Scientific and Cultural Organization (UNESCO)},
  ``{Open Science}.''  \url{https://www.unesco.org/en/open-science}
\BIBentrySTDinterwordspacing

\bibitem{OpenScienceEU}
\BIBentryALTinterwordspacing
{FOSTER Plus (European Union funded open project in Horizon 2020 and beyond)},
  ``{Open Science Definition}.''
  \url{https://www.fosteropenscience.eu/foster-taxonomy/open-science-definition}
\BIBentrySTDinterwordspacing

\bibitem{OpenSource}
\BIBentryALTinterwordspacing
{Open Source Initiative}, ``{The Open Source Definition},'' 2004.
  \url{https://opensource.org/docs/osd}
\BIBentrySTDinterwordspacing

\bibitem{FreeSoftware}
\BIBentryALTinterwordspacing
{Free Software Foundation}, ``{The Free Software Definition},'' 1990.
  \url{https://www.gnu.org/philosophy/free-sw.en.html}
\BIBentrySTDinterwordspacing

\bibitem{LOI_BestPractices}
\BIBentryALTinterwordspacing
R.~Lehe, A.~Huebl, J.-L. Vay \emph{et~al.}, ``{Embracing modern software tools
  and user-friendly practices, when distributing scientific codes},''
  \emph{Snowmass21 LOI}, 2020.
  \url{https://www.snowmass21.org/docs/files/summaries/CompF/SNOWMASS21-CompF2_CompF0_Lehe-076.pdf}
\BIBentrySTDinterwordspacing

\bibitem{Diederichs2022}
S.~Diederichs, C.~Benedetti, A.~Huebl \emph{et~al.}, ``Hipace++: A portable, 3d
  quasi-static particle-in-cell code,'' \emph{Computer Physics Communications},
  vol. 278, p. 108421, 2022.  \doi{10.1016/j.cpc.2022.108421}

\bibitem{impactt}
J.~Qiang, S.~Lidia, R.~D. Ryne, and C.~Limborg-Deprey, ``Three-dimensional
  quasistatic model for high brightness beam dynamics simulation,'' \emph{Phys.
  Rev. ST Accel. Beams}, vol.~9, p. 044204, Apr 2006.
  \doi{10.1103/PhysRevSTAB.9.044204}

\bibitem{impactz}
J.~Qiang, R.~Ryne, S.~Habib, and V.~Decyk, ``{An Object-Oriented Parallel
  Particle-in-Cell Code for Beam Dynamics Simulation in Linear Accelerators},''
  \emph{J. Comput. Phys.}, vol. 163, pp. 434--451, Jul. 2000.
  \doi{10.1006/jcph.2000.6570}

\bibitem{openPMDstandard}
\BIBentryALTinterwordspacing
A.~Huebl, R.~Lehe, J.-L. Vay \emph{et~al.}, ``{openPMD: A meta data standard
  for particle and mesh based data},'' https://github.com/openPMD, 2015.
  \url{https://www.openPMD.org}.  \doi{10.5281/zenodo.591699}
\BIBentrySTDinterwordspacing

\bibitem{openPMDapi}
\BIBentryALTinterwordspacing
A.~Huebl, F.~Poeschel, F.~Koller, and J.~Gu, ``{openPMD-api: C++ \& Python API
  for Scientific I/O with openPMD},'' https://github.com/openPMD/openPMD-api,
  2018.  \url{https://openpmd-api.readthedocs.io}.  \doi{10.14278/rodare.27}
\BIBentrySTDinterwordspacing

\bibitem{Poeschel2022}
F.~Poeschel, J.~E, W.~F. Godoy \emph{et~al.}, ``Transitioning from file-based
  {HPC} workflows to streaming data pipelines with {openPMD} and {ADIOS2},'' in
  \emph{Driving Scientific and Engineering Discoveries Through the Integration
  of Experiment, Big Data, and Modeling and Simulation}, J.~Nichols, A.~B.
  Maccabe, J.~Nutaro \emph{et~al.}, Eds.\hskip 1em plus 0.5em minus 0.4em\relax
  Cham: Springer International Publishing, 2022, pp. 99--118.
  \doi{10.1007/978-3-030-96498-6\_6}

\bibitem{CAMPA}
\BIBentryALTinterwordspacing
``{Consortium for Advanced Modeling of Particle Accelerators (CAMPA)}.''
  \url{https://campa.lbl.gov}
\BIBentrySTDinterwordspacing

\bibitem{LOI_ecosystem}
\BIBentryALTinterwordspacing
J.-L. Vay, A.~Huebl, D.~Sagan \emph{et~al.}, ``{A modular community ecosystem
  for multiphysics particle accelerator modeling and design},''
  \emph{Snowmass21 LOI}, 2020.
  \url{https://snowmass21.org/docs/files/summaries/CompF/SNOWMASS21-CompF2_CompF0-AF1_AF0_Vay-070.pdf}
\BIBentrySTDinterwordspacing

\bibitem{slides}
A.~Huebl \emph{et~al.}, ``{From Compact Plasma Particle Sources to Advanced
  Accelerators with Modeling at Exascale},'' Nov. 2022, invited plenary,
  presentation slides.  \doi{10.5281/zenodo.7697402}

\end{thebibliography}

\end{document}
