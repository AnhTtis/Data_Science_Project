\documentclass[12pt]{article}

% Packages for additional functionality
\usepackage[margin=1in]
{geometry} % Set the margin size
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{float}
\usepackage{svg}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{algorithmic}
\usepackage{amsthm}
\usepackage{graphicx} % For including figures
\usepackage{amsmath} % For mathematical symbols and equations
\usepackage{epsf,hyperref}
\usepackage[noadjust]{cite} % For citations and bibliography
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
% Title, author, and date
\title{\textbf{Data driven approach to sparsification of reaction diffusion complex network systems}}
\author{
Abhishek Ajayakumar,\; Soumyendu Raha
}
\begin{document}

\maketitle

\begin{abstract}
% Abstract of your article
Graph sparsification is an area of interest in computer science and applied mathematics. Sparsification of a graph, in general, aims to reduce the number of edges in the network while preserving specific properties of the graph, like cuts and subgraph counts.\textcolor{black}{\;Computing the sparsest cuts of a graph is known to be NP-hard, and sparsification routines exists for generating linear sized sparsifiers in almost quadratic running time $O(n^{2 + \epsilon})$. Consequently, obtaining a sparsifier can be a computationally demanding task and the complexity varies based on the level of sparsity required. In this study, we extend the concept of sparsification to the realm of reaction-diffusion complex systems. We aim to address the challenge of reducing the number of edges in the network while preserving the underlying flow dynamics. To tackle this problem, we adopt a relaxed approach considering only a subset of trajectories.} We map the network sparsification problem to a data assimilation problem on a Reduced Order Model (ROM) space with constraints targeted at preserving the eigenmodes of the Laplacian matrix under perturbations. The Laplacian matrix ($L = D - A$) is the difference between the diagonal matrix of degrees ($D$) and the graph's adjacency matrix ($A$). We propose approximations to the eigenvalues and eigenvectors of the Laplacian matrix subject to perturbations for computational feasibility and include a custom function based on these approximations as a constraint on the data assimilation framework. We demonstrate the extension of our framework to achieve sparsity in parameter sets for Neural Ordinary Differential Equations (neural ODEs). \\
\textbf{Keywords:} Complex systems, Data assimilation, Differential equations, Dimensionality reduction, Sparse matrices, Network theory, Artificial intelligence.
\end{abstract}


\section{Introduction}
% Introduction section
\label{sec:introduction}

 Modern deep learning frameworks, which utilize recurrent neural network decoders and convolutional neural networks, are characterized by a significant number of parameters. Finding redundant edges in such networks and rescaling the weights can prove very useful; one such work is done as in \cite{https://doi.org/10.48550/arxiv.2211.04598}. Another line of thought involves learning differential equations from data (\cite{Raissi_2018}, \cite{Raissi_2018a}, \cite{lu_data}), where the data generation part is rendered accessible by using sparse neural networks. These types of work find applications in areas like fluid dynamics, see \cite{fluidflow_arxiv}. Reducing the number of parameters in neural ODEs (ODENets) which can be used to model many complex systems is another motivation for this work.

We define a graph $G = (V, E, w)$ where $V$, $E$ denote the set of vertices and edges of the graph and ${w}$ denotes the weight function, that is ${w}: V\times V \rightarrow \mathbb{R}$. A sparsifier of a graph aims at reducing the number of edges in the graph such that the new reweighted graph $\bar{G} = (V, \bar{E}, \bar{w})$ achieves a specific objective like preserving quadratic form of the Laplacian matrix, cuts, subgraph counts. E.g., In the case of spectral sparsifiers, we have
\begin{equation*}
\begin{split}
\frac{x^T L x}{(1+\epsilon)} \leq \, x^T \bar{L} x \leq \,(1 + \epsilon) x^T L x \;\; \forall \,x \in \mathbb{R}^n,\;\; \\   \textrm{where}\; \vert V \vert = n,  \epsilon \in (0,1). \\
\end{split}
\end{equation*}
The notion of cut sparsification, which produces weighted subgraphs for which each cut is approximated, was introduced by Karger (\cite{karger_1994},\cite{karger_1999}). The idea involves sampling individual edges of the graph with probability $p$; the resulting sparsifier has size $O(pm)$, where $m = |E|$, the cardinality of the set of edges of the graph. An improved work for producing cut sparsifiers was based on the non-uniform sampling of edges as shown in (\cite{karger_2015},\cite{karger_1996}). There are other types of sparsifiers which preserve subgraph counts, described as in \cite{ahn_guha_mcgregor_2012}. Spectral sparsifiers are a more robust form of sparsifiers compared to cut sparsifiers. They aim to preserve the Laplacian quadratic form, as described in \cite{spielman_teng_2011}. 
  
\par
Spielman and Teng introduced a more advanced form of sparsification known as spectral sparsifiers (\cite{spielman_teng_2011}). Spectral sparsifiers approximate the quadratic form of the graph Laplacian matrix. Such sparsifiers help in solving systems of linear equations (\cite{spielman_teng_2004}, \cite{spielman_teng_2011}). The resulting sparsified graph exhibits an edge count approximated as $O(n \log(n)^c$), where $c$ is a large constant. In their study, Spielman and Srivastava (\cite{spielman_srivastava_2008}) demonstrated that it is possible to construct a sparsifier for any graph with $n$ vertices, containing $O(\frac{n \log(n)}{\epsilon^2}$) edges. Additionally, they introduced a linear algorithm to achieve this objective. Another work by Goyal, Rademacher and Vempala (\cite{goyal_rademacher_vempala_2009}) proposed to create cut sparsifiers from random spanning trees. Batson, Spielman and Srivastava (\cite{batson_spielman_srivastava_2014}) construct sparsifiers with size $O(\frac{n}{\epsilon^2}$) edges in $O(\frac{m n^3}{\epsilon^2 }$) time using a determinstic algorithm. Fung and Harvey devised an algorithm that samples edges based on the inverse of edge connectivity to produce cut sparsifiers (\cite{DBLP:journals/corr/abs-1005-0265}). Additionally they also describe an approach to produce cut sparsifiers from uniformly random spanning trees. A meta-learning-based approach for graph sparsification is discussed in (\cite{metalearning_sparsify}). The use of sparsification routines, as discussed in (\cite{allenzhu2015spectral}), enables the creation of linear-sized sparsifiers in nearly quadratic time. This approach addresses the sparsification problem by treating it as a regret minimization problem.
\par
We present literature that focuses on preserving the dynamic nature of dynamical systems. The paper by Cencetti, Clusella, and Fanelli (\cite{cencetti_clusella_fanelli_2018}) explores methods for altering the network's topology while maintaining its dynamic characteristics intact; the new network obtained was isodynamic to the former. Kumar, Ajayakumar and Raha, in their work (\cite{KUMAR2021100948}), provide mathematical conditions for finding stable bi-partitions of a complex network and also provide a heuristic algorithm for the same. Graph partitioning does not alter the edge weights of graphs; however, it can cause the deletion of edges. This work shows that constrained internal patch dynamics with sufficient network connectivity ensures stable dynamics in subnetworks around its co-existential equilibrium point. 
\par  \textbf{Contributions of our paper:}
Techniques to rewire the edges of the graph while preserving isodynamic behaviour for complex systems have been discussed in \cite{cencetti_clusella_fanelli_2018}. However, eliminating redundant edges of the graph while preserving the isodynamic behaviour of such complex systems by making use of observational data points in reduced dimension using dimensionality reduction techniques like Proper Orthogonal decomposition (POD) are the main contributions of our work. The problem of sparsification is approached as a Data assimilation problem with constraints. Decision variables are the unknown weights of the graph. The objective is to make this vector of weights as sparse as possible. If the perturbed Laplacian matrix has many eigenmodes similar to the eigenmodes of the given network, patterns produced in the new graph would be highly correlated to the patterns in the network (\cite{cencetti_clusella_fanelli_2018}).
Generating eigenvalues and eigenvectors at each iteration of the optimization process is computationally intensive. To overcome this, we propose scalable algorithms which generates approximations to the eigenvalue and eigenvector for the Laplacian matrix subject to perturbations. \\
\par The structure of the article is as follows: we begin by presenting some background material from spectral graph theory. A brief overview of reaction-diffusion systems on graphs is provided in the next section. We briefly introduce the adjoint method for data assimilation, which can be applied to the parameter estimation problem. In the subsequent sections, we present the reduced vector field for the chemical Brusselator reaction-diffusion system on the graph and discuss various performance metrics. Challenges involved in ensuring isodynamic behaviour are presented in the next section, the challenging aspect being getting good estimates of the eigenpair of the perturbed Laplacian matrix iteratively. We explore approximations of the eigenpair and their effectiveness in the next section, followed by formulating the optimization problem and its challenges. We present experimental results on real-world graphs to demonstrate the effectiveness of our approach. In Section \ref{odenetsection}, we show how to produce sparse neural ODENets using the framework for a linear dynamical system.


\section{Background}
\subsection{Laplacian matrix:($L$)}
The Laplacian of an undirected weighted graph $G = (V,E)$, where $|V| = n, |E| = m$ with a weight function $w:V\times V\rightarrow \mathbb{R}$ is given by
\begin{equation*}
  L \left(u,v\right)=\;\left\lbrace \begin{array}{ll}
d_v -w\left(u,v\right) & \mathrm{if}\;u=v,\\
-w\left(u,v\right) & \mathrm{if}\;u\;\mathrm{and}\;v\;\mathrm{are}\;\mathrm{adjacent},\\
0 & \mathrm{otherwise}\ldotp 
\end{array}\right.  
\end{equation*}
Where $d_v = \sum_{u \sim v} w(u,v)$ denotes the degree of vertex $v$.
\subsection{Incidence matrix:($B$)}
The incidence matrix $B_{m \times n}$ with a given orientation of edges of a graph is given by
\begin{equation*}
B\left(e,v\right)=\;\left\lbrace \begin{array}{ll}
1 & \mathrm{if}\;v\;\mathrm{is}\;e^{\prime } s\;\mathrm{head},\\
-1 & \mathrm{if}\;v\;\mathrm{is}\;e^{\prime } s\;\mathrm{tail},\\
0 & \mathrm{otherwise}\ldotp 
\end{array}\right.    
\end{equation*}
The graph Laplacian matrix for a given orientation with weights $\bar{w}_i = \gamma_i w_i,\;i = 1,2,\ldots, m$ can be expressed as
\begin{equation*}
    \begin{split}
    L = B^T W^{1/2} \textrm{diag}(\gamma) W^{1/2} B \quad \textrm{, where } \gamma \in \mathbb{R}^m  \textrm{ represents the multipliers of the weights}, \\    
    \end{split}
\label{multipliers_weight}
\end{equation*}
\textrm{$W$ represents the diagonal matrix of weights $w_i,\; i = 1,2,\ldots,m.$}

\section{Dynamical systems involving graphs}
\label{main-text}
The general form of a reaction-diffusion system is outlined below, as presented in the work of Cencetti et al (\cite{cencetti_clusella_fanelli_2018}). Each node in the network, denoted as $j$, possesses an $\mathrm{m}$-dimensional variable $r_j(t) \in \mathbb{R}^{\mathrm{m}}$ that characterizes its activity at time $t$. The evolution of $r_j$ over time follows a specific set of rules, beginning from an initial state $r_j(0)$ as described below
$$ \frac{dr_j}{dt} = \mathcal{F}(r_j) + K \sum_{k=1}^{N} A_{jk} \mathcal{G}(r_k-r_j) \hspace{5mm} j = 1,2,....N$$
Here $\mathcal{F}$ denotes the reaction component, and the remaining terms account for the diffusion in the graph. $\mathcal{F} : \mathbb{R}^{\mathrm{m}} \rightarrow \mathbb{R}^{\mathrm{m}}$, $\mathcal{G}: \mathbb{R}^{\mathrm{m}} \rightarrow \mathbb{R}^{\mathrm{m}}$. One such Reaction-diffusion system is given by the alternating self-dynamics Brusselator model (Figure \ref{brussdynam}) where we couple the inter-patch dynamics using the graph Laplacian matrix,
\begin{equation}
    \left\lbrace \begin{array}{l}
\dot{x}_i \;=\;a-\left(b+d\right)x_i +c\;x_i^2 y_i -D_x \;\sum_j L {\;}_{\mathrm{ij}} x_j \\
{\dot{y}_i \;=\;\mathrm{b}}x_i -c\;x_{\;i}^2 y_i -D_y \;\sum_j L_{\mathrm{ij}} \;y_j \;\;\;\;\;\;\;
\end{array}\right.
\label{Rd dynamics}
\end{equation}
With $r_i = (x_i, y_i) \in \mathbb{R}^2$. The system as defined by Equation (\ref{Rd dynamics}) has fixed points \[r^{\star} = (x_i,y_i) = (\frac{a}{d}, \frac{bd}{ac}) \, \forall \, i.\]
\begin{figure}[h]
\includegraphics[width=8cm]{./images/brusselator_dynamics.png}
\centering
\captionsetup{justification=centering} 
\caption{Brusselator dynamics}
\label{brussdynam}
\centering
\end{figure}
Our primary objective is to find a graph $\bar{G} = (V,\bar{E}, \bar{w})$ with a new sparse weight function $\bar{w}$ such that on solving Equation (\ref{Rd dynamics}) using the updated graph $\bar{G}$ we get solutions $\bar{x_i} \in B(x_i,\delta_i), \bar{y_i} \in  B(y_i, \Delta_i) \;\forall \;i$, where $\delta_i$ and $\Delta_i$ are relatively small. We will demonstrate the problem on a linear dynamical system involving the Laplacian matrix. The heat kernel matrix involving the Laplacian matrix is given as follows, \\
\begin{equation}
H_t  = e^{-Lt}    
\label{kernel}    
\end{equation}

Differentiating Equation (\ref{kernel}) with respect to time, we get $$\frac{dH_t}{dt} = -L H_t.$$
This system can be considered a system of odes of the form
\begin{equation}
    \hspace{25mm} \frac{d\mathcal{F}(x,t)}{dt} = -L \mathcal{F}(x,t)
    \label{Linear_ode}
\end{equation}
According to the fundamental Theorem of linear systems, we know that the solution to such a system with initial condition $\mathcal{F}(x,0) = f(x)$ is $\mathcal{F}(x,t) = e^{-Lt} f(x)$ and this solution is unique. The underlying quantity which follows ODE described in Equation (\ref{Linear_ode}) could be temperature, chemical reactants, and ecological species. If we use an approximation for $L$, let it be $\bar{L}$ in the above equation with the same initial condition,
the solution turns out to be solutions to the ode of the form
\begin{equation}
\hspace{25mm}\frac{\partial \bar{\mathcal{F}}}{dt} = -\bar{L} \bar{\mathcal{F}} \label{lineardiffusion}   
\end{equation}

$\bar{\mathcal{F}}(x,t) = e^{-\bar{L}t}f(x)$. The error in this substitution at node $x$ for a time $t$ is given by $$H(x,t) = \mathcal{F}(x,t) - \bar{\mathcal{F}}(x,t),$$ $$H(x,t) = (\sum_{k=1}^{\infty} \frac{{L}^k (-t)^k}{k!})f(x) - (\sum_{k=1}^{\infty} \frac{{\bar{L}}^k (-t)^k}{k!})f(x) .$$
\par Given some random initial conditions \{$\mathcal{F}_{0\alpha}\} \in \mathbb{R}^n, \alpha = 1,2 ....,\omega$, one could discretize system as described by Equation (\ref{lineardiffusion}) to obtain solutions at various time points using a numerical scheme. The discretization of the dynamics for the system described by Equation (\ref{lineardiffusion}) is as follows,

\begin{equation}\hspace{5mm}
\bar{\mathcal{F}}^{\alpha}_{q+1} = M(\bar{\mathcal{F}}^{\alpha}_{q},\bar{w})    
\label{forward_eq}
\end{equation} 
where $M:\mathbb{R}^{n} \times \mathbb{R}^m \rightarrow \mathbb{R}^{n}$ 
with $M = (M_{1},M_{2},.....,M_{n})^T, M_{i} = M_{i}(\bar{\mathcal{F}}^{\alpha}_q,\bar{w})$ for $1\leq i \leq n$ and $ 0\leq q \leq S $, $S$ denotes the last time-step, and $\bar{\mathcal{F}}^{\alpha}_{0}  \in \mathbb{R}^n$ 
is the initial condition which is assumed to be known. 
The primary objective would be to obtain a sparse vector of weights $\bar{w}$ which minimizes the error $H$ as explained above, subject to the discrete dynamics shown in Equation (\ref{forward_eq}). 
This forms the core of the data assimilation problem. 
To know more about data assimilation, one could refer to (\cite{lewis_lakshmivarahan_dhall_2009},
\cite{sarkka}, \cite{law_stuart_zygalakis_2015}).
\section{Adjoint method for data assimilation}
Parameter estimation problems have rich literature (\cite{1p}, 
\cite{caracotsios_stewart_1985},
\cite{3p},
\cite{4p}). 
Using Unscented and Extended Kalman filtering-based approaches are not recommended when constraints on parameters are imposed (\cite{EKf_mhe}). We use the Adjoint sensitivity method for the parameter estimation (\cite{lakshmivarahan_lewis_2010}). In this section, we provide a brief overview of this method and how it could be used in parameter estimation. \\
\small\textbf{Statement of the Inverse Problem:} The objective is to estimate the optimal control parameter $\alpha$ that minimizes the cost function $J(\alpha)$ based on a given set of observations  $\{\textbf{$z_k$} \in \mathbb{R}^n \mid 1 \leq k \leq N\}$. These observations represent the true state, and the model state $x_k$ follows a specific relationship described by $x_k = M(x_{k-1},\alpha)$. The state variable $x$ belongs to $\mathbb{R}^n$, denoted as $x = (x_1, x_2, \dots, x_n)$, and $F:\mathbb{R}^n \times \mathbb{R}^p \rightarrow \mathbb{R}^n$ represents a mapping function. Specifically, $F = (F_1, F_2,\dots ,F_n)$ where $F_i = F_i(x, \alpha)$ for $1 \leq i \leq n$ and $\alpha \in$ $\mathbb{R}^p$. \newline
Consider the nonlinear system of the type
\begin{equation}
\frac{d x}{d t} = F(x, \alpha)
\label{IVPprob}
\end{equation}
 with $x(0) = c,$ being the initial condition assumed to be known, $\alpha$ is a system parameter assumed to be unknown. The vector $x(t)$ denotes the state of the system at time t, and $F(x, \alpha)$ represents the vector field at point $x$. If each component of $F_i$ of $F$ is continuously differentiable in $x$, the solution to the initial value problem (IVP) exists and is unique.
\newline \newline
The Equation (\ref{IVPprob}) can be discretized using many schemes (\cite{burden_faires_burden_2016}), and the resulting discrete version of the dynamics can be represented as
\begin{equation*}
x_{k+1} = M(x_k, \alpha)
\label{discretivp}
\end{equation*}

The function $M:\mathbb{R}^n \times \mathbb{R}^p \rightarrow \mathbb{R}^n$ can be represented as $M = (M_1, M_2, \ldots, M_n)$, where each $M_i = M_i(x_k, \alpha)$ for $1 \leq i \leq n$.  \newline \newline

Let us define a function $J:\mathbb{R}^n \rightarrow \mathbb{R}$  as follows, (\cite{lewis_lakshmivarahan_dhall_2009})
\begin{equation*}
    J(\alpha) = \frac{1}{2}\sum_{k=1}^N <(z_k - x_k), (z_k - x_k)>
\end{equation*}
\subsection{Adjoint sensitivity with respect to parameter $\alpha$}
\label{adj_param}
\begin{equation*}
\begin{array}{l}
\delta J(\alpha) \;= \sum_{k=1}^N <\eta_k, \delta x_k> \;\;   \;\text{\; for more details refer \cite{lakshmivarahan_lewis_2010}}\\ \\
\;\;\;\;\;\;\;\;\;\;\;\;= \sum_{k=1}^N <\eta_k, V_k \delta \alpha>\\ \\
\;\;\;\;\;\;\;\;\;\;\;\;\;= <\sum_{k=1}^N V_k^T \eta_k, \delta \alpha>\\ 
\end{array} 
\end{equation*}
\begin{equation}
    \text{where \;} V_k = A_{k-1}V_{k-1} + B_{k-1}
\label{TLS}
\end{equation}
$$A_{k-1} = D_{k-1}(M(x_{k-1},\alpha)) \in \mathbb{R}^{n\times n} \text{\;and\;} B_{k-1} = D_{\alpha}(M(x_{k-1},\alpha)) \in \mathbb{R}^{n\times p}. $$ 
The recursive relation focuses on finding the sensitivity of $J$ with respect to the parameter $\alpha$. Iterating Equation (\ref{TLS}), we get $$ \begin{array}{l}
V_k = \sum_{j = 0}^{k-1} (\prod_{s = j+1}^{k-1} A_s) B_j \\ 
\end{array}$$
The above recursive relation deals with the product of sensitivity matrices.   
\newline
The first variation of $J$ after specific calculations is given by (see \cite{lewis_lakshmivarahan_dhall_2009}) 
\begin{equation}
\delta J =  <\sum_{k=1}^N  \sum_{j = 0}^{k-1}  B_j^T(\prod_{s = k-1}^{j+1} A_s^T) \eta_k, \delta \alpha >, \;\; 
\label{first_var}
\text{where}\;\;\eta_k = x_k - z_k, \\ \prod\,\text{denotes reverse product.}
\end{equation}

From first principles
$$\delta J = < \nabla_{\alpha} J(\alpha), \delta \alpha>$$
Comparing first principles with Equation (\ref{first_var}), we get \[\nabla_{\alpha} J(\alpha) = \sum_{k=1}^N  \sum_{j = 0}^{k-1}  B_j^T(\prod_{s = k-1}^{j+1} A_s^T) \eta_k  \]

\subsubsection{Adjoint method Algorithm}
\label{Adjointalgo}
\begin{enumerate}
    \item Start with $x_0$ and compute the nonlinear trajectory $\{x_k\}_{k=1}^N$ using the model $x_{k+1} = M(x_k, \alpha)$
    \item $\lambda_N = \eta_N$
    \item Compute $\lambda_j = A_j^T \lambda_{j+1} + \bar{\eta_j}$ for $j = N-1 \text{\;to\;} k$, $\bar{\lambda}_k = B_{k-1}^T \lambda_k$, $k = 1 \text{\;to\;} N$, $\bar{\eta}_j = \delta_{j, j_i} \eta_j$ with $\delta_{j,j_i} = 1$ when $j = j_i$ , else 0 
    \item sum = \underline{0}, sum = sum +  $\bar{\lambda}_k$ vectors from $k = 1 \text{\;to\;} N$
    \item $\nabla_{\alpha}J(\alpha) = \text{sum}$
    \item Using a minimization algorithm find the optimal $\alpha^*$ by repeating steps 1 to 5 until convergence
\end{enumerate}



\subsection{Computing the Jacobians $D_{k} \left(M\right)$ and $D_{\alpha} \left(M\right)$
}
This section discusses how to compute the Jacobian matrices described in Section \ref{adj_param}. The numerical solution of an ODE at timestep $j+1$($y_{j+1} \approx x_{j+1}$)  with slope function $\frac{dx}{dt} = f(t,x)$ given $d$ slopes($k_d$), where $x \in \mathbb{R}^n$ and $f: \mathbb{R} \times \mathbb{R}^n \rightarrow \mathbb{R}^n$ is given by $y_{j+1} = y_{j} + b_1 k_{1} + b_2 k_2 + ....+ b_d k_d$, where $k_i = hf(t + c_ih, x + a_{i1}k_1 + a_{i2}k_2 + ... a_{in}k_n)$. See \cite{burden_faires_burden_2016} to learn more about numerical analysis.
\par We demonstrate computing $D_{k}(M)$ using an explicit numerical scheme, particularly the Euler forward method on the Lotka-Volterra model.

$$\left\lbrace \begin{array}{ll}
\begin{array}{l}
\frac{{\mathrm{dx}}_1 }{\mathrm{dt}\;}=\alpha x_1 - \beta x_1 x_2  \\ \\
\frac{{\mathrm{dx}}_2 }{\mathrm{dt}}=\delta x_1 x_2 - \gamma x_2 \\ \\
\end{array} & 
\end{array}\right.$$
When we apply the standard forward Euler scheme, we obtain
$$ x_{k+1} = M(x_k)$$ \\
where $x_k = (x_{1k}, x_{2k}),\; \textbf{M}(x_k) = (M_1(x_k), M_2(x_k))$ where 
$$\begin{array}{l}
M_1 \left(x_k \right)=x_{1k} +\left(\Delta t\right) \left(\alpha\,x_{1k} -\beta\,x_{1k}x_{2k} \right) \\
M_2 \left(x_k \right)=x_{2k} +\left(\Delta t\right)(\delta\;x_{1k} x_{2k} -\gamma\,x_{2k}) 
\end{array} $$
It can be seen that 
$$
D_{k}(M) = A_k = \left\lbrack \begin{array}{cc}
1+\Delta t\;\left(\alpha -\beta \;x_{2k} \right) & -\Delta t\;\beta \;x_{1k} \\
\Delta t\;\delta \;x_{2k}  & 1+\Delta t\;\left(\delta \;x_{1k} -\gamma \;\right)
\end{array}\right\rbrack
$$

\begin{equation*}
D_{\alpha}(M) = B_k =  
\left\lbrack \begin{array}{cccc}
\Delta t x_{1k}  & \Delta t\left(-x_{1k} x_{2k} \right) & 0 & 0\\
0 & 0 & -\Delta tx_{2k}  & \Delta tx_{1k} {\;x}_{2k} 
\end{array}\right\rbrack   
\end{equation*}


\subsection{Using reduced order model for dynamical systems}
\label{ROMappro}
When dealing with graphs of considerable size, the dimensionality of the state vector becomes a concern due to its potential impact on computational efficiency. To address this issue, dimensionality reduction techniques like POD, also known as Karhunenâ€“Lo`{e}ve, can be applied. The work described in \cite{rathinam_petzold_2003} discusses the utilization of POD as a means to alleviate the computational complexity associated with large graph sizes. The procedure requires snapshots of solutions of a dynamical system with a vector field $f$ $\in \mathbb{R}^n$.
\begin{equation}\hspace{5mm}
\dot{x} = f(x,t)
\label{dynamicalPOD}
\end{equation}

Using the snapshot solutions, we could create a matrix $\rho$ of projection $\in \mathbb{R}^{k\times n}$ where k denotes the reduced order dimension and the mean $\bar{x}$, see \cite{rathinam_petzold_2003} for more details. The reduced order model of the system as described by Equation (\ref{dynamicalPOD}) is then given by 

$$
     \dot{z} = \rho f(\rho^T z + \bar{x}, t).
$$
If we are solving an initial value problem with $x(0) =x_0$ , then
the reduced model will have the initial condition $z(0) = z_0$ , where
 
$$
z_0 = \rho(x_0 - \bar{x}).
$$
The reduced order model for system as described by Equation (\ref{lineardiffusion}) given the matrix $\rho$ of projection is 
$$\dot{z} = -\rho \bar{L}\rho^T z - \rho \bar{L} \bar{x} = \psi(z, \bar{w}).$$
The reduced order model for system described by Equation (\ref{Rd dynamics}) is obtained as follows. \\
\textbf{Notation:}
If $p$ and $q$ are vectors, then $p \,\odot \, q$ represents element-wise multiplication, and $p^{\circ n}$ raises every element of the vector $p$ to power n. Let $B$ be a matrix $\in \mathbb{R}^{n\times n}$, then $B(1:2,:)$ indicates the first 2 rows of $B$ and all the columns of the matrix $B$.  \\
In the context of approximating the vector field, we introduce the concept of multipliers ($\bar{\gamma}$), as defined in Section \ref{multipliers_weight}. This concept plays a crucial role in our analysis, $\bar{w} = W \bar{\gamma}$. 
\begin{align*}  
\tag{$f2$}
\label{fa}
 \dot{z}  \qquad & = && \mathbb{\psi}(z,\bar{\gamma})  \\
 & = && \rho \;h(z) -D_x \;\rho(:,1:n)B^T W^{1/2}\mathrm{diag}(\bar{\gamma})W^{1/2}B \nonumber
  (\rho^T(1:n,:)z + \bar{x}(1:n)) \\ \nonumber
 & &&-D_y\; \rho(:,n+1:2n)B^T W^{1/2}\mathrm{diag}(\bar{\gamma})W^{1/2}B \nonumber
 (\rho^T(n+1:2n,:)z+ \bar{x}(n+1:2n))
\end{align*}

\begin{align*}
h(z) = 
\begin{pmatrix} 
a \mathrm{1}_{n\times1} - (b+d)(\rho^T(1:n,:)z + \bar{x}(1:n))  
+ c[(\rho^T(1:n,:)z + \bar{x}(1:n))^{\circ 2} ] \odot \\(\rho^T(n+1:2n,:)z   + \bar{x}(n+1:2n))  \\ \\
b(\rho^T(1:n,:)z + \bar{x}(1:n)) - c((\rho^T(1:n,:)z + \bar{x}(1:n))^{\circ 2})\odot(\rho^T(n+1:2n,:)z \\ + \bar{x}(n+1:2n))  \\ 
\end{pmatrix}   
\end{align*}


\begin{align*}
\hspace{10mm} &\nabla_{z} h_e{\left(z\right)}^T \bigg\vert_{e=1,2,..,n} &&= -\left(b+d\right)\rho^T \left(e,:\right) + c\left({\left(\rho^T \left(e,:\right)z+\bar{x}_e \right)}^2 \right)\rho^T \left(n+e,:\right) \
\\ &  &&
+  2c\left(\rho^T \left(e,:\right)z+\bar{x}e \right) \left(\rho^T \left(n+e,:\right)z +  \bar{x}_{n+e}\right)\rho^T(e,:) 
\\
& \nabla_z {h_f(z)}^T \bigg\vert_{f = n+1,n+2,..,2n} && = b \rho^T(f-n,:)  - c(\rho^T(f-n,:)z + \bar{x}_{f-n})^2 (\rho^T(f,:)) \\
 & &&- \,2c(\rho^T(f-n,:)z + \bar{x}_{f-n})\,(\rho^T(f,:)z + \bar{x}_f)\,\rho^T(f-n,:) \\
 & D_{z}\psi(z,\bar{\gamma}) &&= \rho D_z h(z) -D_x \, \rho(:,1:n)B^T W^{1/2} \text{diag}(\bar{\gamma}) W^{1/2} B \rho^T(1:n,:)  \\   & &&-D_y\,\rho(:,n+1:2n) B^TW^{1/2} \text{diag}(\bar{\gamma})W^{1/2} B \rho^T(n+1:2n,:)  \\
& D_{\bar{\gamma} } \psi (z,\bar{\gamma} ) &&= -D_x \rho (:,1:n)B^T W^{1/2} (\text{diag}(W^{1/2}B\rho^T (1:n,:)z  \\ & && + W^{1/2}B\bar{x} (1:n)))  - D_y \;\rho (:,n+1:2n)B^T W^{1/2}  \\ & &&(\text{diag}(W^{1/2}B\rho^T (n+1:2n,:)z + W^{1/2}B\bar{x} (n+1:2n)))
\end{align*}


\subsection{POD method performance}
Work in \cite{rathinam_petzold_2003} discusses the computational advantages when using POD on the general linear and non-linear dynamical system. We present the program execution times (Figures \ref{randomgraph_rom} and   \ref{realgraph_rom}) when evaluating the vector field as well as the Jacobian matrix-vector product considering the Jacobian with respect to the state and the parameter on the reduced order model of the system as described by Equation (\ref{Rd dynamics}).


\begin{figure}[H]
\begin{subfigure}[t]{0.450\linewidth} \includegraphics[width=\linewidth,height=6cm]{./images/costfunc_real.png}
\caption*{Time taken to evaluate vector field $f$ Equation (\ref{dynamicalPOD}) of regular model vs ROM vector field, see Equation (\ref{fa}).}
\end{subfigure}
\hspace{5mm}
\begin{subfigure}[t]{0.450\linewidth}
\includegraphics[width=\linewidth,height=6cm]{./images/jacobian_statereal.png}
\caption*{  Execution time for $A^T_s x_s $ in ROM and full model with a random vector $x_s$.(see Section \ref{first_var})}
\end{subfigure}
\\
\begin{subfigure}[t]{0.450\linewidth} \includegraphics[width=\linewidth,height=6cm]{./images/net_timereal.png}
\caption*{ Time taken for ROM versus regular model.}
\end{subfigure}
\hspace{5mm}
\begin{subfigure}[t]{0.450\linewidth}
\includegraphics[width=\linewidth,height=6cm]{./images/jacobian_weightsreal.png}
\caption*{Execution time for $B^T_j y_j$ in ROM and full model with a random vector $y_j$.(see Section \ref{first_var})}
\end{subfigure}
\caption{Impact of model reduction techniques for some real world graphs}
\label{realgraph_rom}
\end{figure}

\begin{figure}[H]
\begin{subfigure}[t]{0.450\linewidth} \includegraphics[width=\linewidth,height=6cm]{./images/costfunc_all.png}
\caption*{Time taken to evaluate vector field $f$ Equation \ref{dynamicalPOD} of regular model vs ROM vector field, see Equation (\ref{fa}).}
\end{subfigure}
\hspace{5mm}
\begin{subfigure}[t]{0.450\linewidth}
\includegraphics[width=\linewidth,height=6cm]{./images/jacobian_state.png}
\caption*{  Execution time for $A^T_s x_s $ in ROM and full model with a random vector $x_s$.(see Section \ref{first_var})}
\end{subfigure}
\\
\begin{subfigure}[t]{0.450\linewidth} \includegraphics[width=\linewidth,height=6cm]{./images/net_time.png}
\caption*{ Time taken for ROM versus regular model.}
\end{subfigure}
\hspace{7mm}
\begin{subfigure}[t]{0.450\linewidth}
\includegraphics[width=\linewidth,height=6cm]{./images/jacobian_multipliers.png}
\caption*{Execution time for $B^T_j y_j$ in ROM and full model with a random vector $y$.(see Section \ref{first_var})}
\end{subfigure}
\caption{Impact of model reduction techniques for random graphs}
\label{randomgraph_rom}
\end{figure}





% \begin{figure}[H]
% \begin{minipage}{0.45\textwidth}
% \centering
% \includegraphics[width=\linewidth]{./images/costfunc_all.png}
% {\footnotesize ROM slope function vs regular slope function.}
% \end{minipage}%
% \hspace{5mm}
% \begin{minipage}{0.45\textwidth}
% \centering
% \includegraphics[width=\linewidth]{./images/jacobian_state.png}
% {\footnotesize  Execution time for $A^T_s x_s $ in ROM and full model with a random vector $x_s$.(see section(\ref{first_var}))}
% \end{minipage}

% \vspace{10mm}

% \begin{minipage}{0.45\textwidth}
% \centering
% \includegraphics[width=\linewidth]{./images/jacobian_multipliers.png}
% {\footnotesize Execution time for $B^T_j y_j$ in ROM and full model with a random vector $y$.(see section(\ref{first_var}))}
% \end{minipage}
% \hspace{5mm}
% \begin{minipage}{0.45\textwidth}
% \centering
% \includegraphics[width=\linewidth]{./images/net_time.png}
% {\footnotesize Time taken for ROM versus regular model.}
% \end{minipage}
% \caption{\small Impact of Model Reduction Techniques for real graphs}
% \label{randomgraph_rom}
% \end{figure}

\section{Problem statement}
Considering a subset of trajectories of a complex system on an undirected graph, the objective is to create an edge sparsifier (i.e. graphs with fewer edges than the original graph), and the sparse graph should produce patterns similar to the given subset of trajectories. This problem is posed as a constrained optimization problem where the objective function is made to include a term involving the difference between the observations from the given trajectories projected into a reduced dimension at various time points and the state vectors generated by the reduced order forward model based on the sparsified graph. An $\ell_1$ norm term of the weight multipliers ($\bar{\gamma}$) (Section \ref{multipliers_weight}) is also introduced as a penalty term in the objective function to induce sparsity. 

The new weight vector $\bar{w} = \textrm{diag($w$)} \bar{\gamma}$. We denote the perturbed Laplacian matrix $L^{'} = L + B^T D B, $ where $D = \text{diag(}\bar{w} -w )$. When this perturbed Laplacian matrix has many eigenmodes similar to the eigenmodes of the original network, the resulting patterns in the new graph will be highly correlated with the patterns in the original network. Thus any patterns or behaviours induced in the original network will likely be present in the new graph, see \cite{cencetti_clusella_fanelli_2018}. Computation of eigenmodes of the Laplacian matrix can be computationally intensive, so we propose approximations to determine the eigenmodes of the Laplacian matrix under perturbations, see Section \ref{estimate_eval}. 

 

\subsection{Ensuring isodynamic behaviour}
This section discusses techniques to ensure isodynamic behaviour on the sparse graph and the challenges involved. The dispersion relation connects the stability of the system as described by Equation (\ref{Rd dynamics}) with the Laplacian matrix eigenvalues. A graph with no unstable mode will have (Re$(\lambda) < 0) \, \forall \, \alpha$. The contribution of the $\alpha$-th eigenvalue to the stability of the system is given by the eigenvalues of the following matrix,
$$J_{\alpha} = D_r\mathcal{F}(r^{\star}) - \left\lbrack \begin{array}{cc}
D_x \; & 0\\
0 & D_y 
\end{array}\right\rbrack \lambda {\;}^{(\alpha)} \; $$

\begin{figure}[H]
\centering
\includegraphics[width=10cm]{./images/stable_unstable.png}
\caption{Plot showing the stable and unstable eigenmodes of reaction-diffusion system as described by Equation (\ref{Rd dynamics}) on an Erd\H{o}s-R$\acute{e}$nyi random graph}
\centering
\label{emodes_all}
\end{figure}
Figure \ref{emodes_all} indicates the stable and unstable eigenmodes of a reaction-diffusion system (Eq. \ref{Rd dynamics}) cast on a random graph.
The study in \cite{cencetti_clusella_fanelli_2018} aims at producing a pattern invariant network using techniques like eigenmode randomization and local rewiring. One could use the error function employed in the local rewiring methodology as mentioned in \cite{cencetti_clusella_fanelli_2018} as a constraint to the optimization problem discussed in Section \ref{costfunc} but computing this function iteratively can become computationally expensive. The error function imposed as a constraint will be generally non-smooth. Methods to approach such non-smooth optimization problems using global optimization algorithms like Genetic algorithms are discussed in \cite{nocedalbook}, however mutliple function evaluations can make these methods ineffective. The error function ($\zeta$) introduced below is a modified variant of the one described in \cite{cencetti_clusella_fanelli_2018}.
\[
\zeta = n\zeta_l + \zeta_q
\label{errorfunc}
\]
\[
\zeta_l = \frac{\sum_{i = 1}^{n_p} |\Tilde{\lambda}_{i} - \lambda_{i}|^2}{n_p\;\; \sum_{i = 1}^{n_p} (\lambda_{i})^2}
\]
\[
\zeta_q = \frac{\sum_{i=1}^{n_p} |\langle\Tilde{\phi}_{i}, \phi_{i}\rangle - 1|^2 }{n_p}
\]
Here $n_p$ denotes the number of eigenmodes preserved; we accept the new graph if the error function $\zeta$ is less than a tolerance value.

In the presence of a connected input graph, we impose a minimum connectivity constraint to minimize the number of disconnected components, which, in turn, affects the error function (see Section \ref{errorfunctionestimate}). The intuition is taken from work in \cite{KUMAR2021100948} which states that when the sum of two lowest degrees of the graph component can influence the second smallest eigenvalue of the Laplacian matrix. We control the sum of degrees of the graph by ensuring that the unsigned incidence matrix times the weight vector is greater than a required connectivity level, say $\tau \mathbf{1}_{n\times 1}$ with $\tau$ being a user-defined parameter. We demonstrate this using an example, see Figure \ref{example_connect}.
\begin{figure}[htb]
\centering
\includegraphics[width=10cm]{./images/unsignedincidence.png}
\caption{Figure illustrating on how the connectivity constraint is imposed using the unsigned incidence matrix. We consider an unweighted graph having 4 nodes and the multipliers are denoted by red. Here $Q$ denotes the unsigned incidence matrix, weight vector $w$ = diag($\mathbf{1}_{5\times 1})\gamma$, where $\gamma$ are the multipliers denoted by red. Degree vector $d = Q \gamma$.}
\centering
\label{example_connect}
\end{figure}


\subsubsection{Eigenvalue and eigenvector computation for the 
Laplacian matrix under perturbations}

\begin{lemma} \label{sherman}
(Sherman-Morrison formula). If A is a singular $n \times n$ matrix and x is a vector, then
$$
(A + xx^T)^{-1} = A^{-1} - \frac{A^{-1}xx^TA^{-1}}{1+x^TA^{-1}x}.
$$
\end{lemma}
\begin{lemma}
(matrix determinant lemma) If A is nonsingular and x is a vector, then
$$
\text{det}(A + xx^T) = \text{det}(A)(1+x^TA^{-1}x).
$$
\label{MDL}
\end{lemma}




We present the challenges in finding the eigenpair during each iteration of the optimization procedure. If there is a change to only one of the edges of the graph, then one could use the approximate model described as in \cite{batson_spielman_srivastava_2014} making use of Lemma \ref{sherman} and \ref{MDL} to find the eigenpairs. At iteration $q$ of the algorithm, we will get a new graph with the weights modified according to the scale factor $\bar{\gamma}^q$. We define the matrix $D = \text{diag(}\bar{\gamma}^q - \mathbf{1}_{m \times 1}$) and $\Tilde{\lambda} = \mathrm{diag(} \lambda_1, \lambda_2, \lambda_3,..,\lambda_n)$ denotes the eigenvalues of matrix $L$. We define the new Laplacian matrix as $L^{'} = L + B^T W^{1/2} D W^{1/2}B.$ The characteristic polynomial of matrix $L^{'}$ is given by,
\[
\chi_{L^{'}}(\lambda, D) = \mathrm{det(} L^{'} - \lambda I) = \mathrm{det(} \Phi \, \Tilde{\lambda}\, \Phi^T + B^T W^{1/2} D W^{1/2}B - \lambda I) \; 
\]
Using the property of determinants we get the following
\[
\chi_{L^{'}}(\lambda, D) = \text{det(} \Sigma + U^T D U) \,\, \text{where}\,\, U^T = \Phi^T B^T W^{1/2}
\]
\[
\Sigma = diag(1, \lambda_2 - \lambda,...., \lambda_n - \lambda)
\]
\begin{align*}
 \begin{array}{l}
\text{ $\chi_{L^{'}}(\lambda$, $D$) =  det}\left\lbrack \begin{array}{cccc}
1 & 0 & \ldotp  & 0 \\
0 & u_2^T Du_2 +\lambda_2 -\lambda \; & \ldotp  & u_2^T D u_n   \\
\ldotp  & u_3^T Du_2  & \ldotp  & u_3^T D u_n   \\
\ldotp  & \ldotp  & \ldotp  & \ldotp   \\
0 & u_n^T Du_2  & \ldotp  & u_n^T D u_n + \lambda_n - \lambda  
\end{array}\right\rbrack  \\
\\
\text{$\chi_{L^{'}}(\lambda$, $D$) = det \tag{R}} \bigg( \left\lbrack \begin{array}{ccc}
\mathrm{trace}\left(u_2 {\;u}_2^T \;D\right)+\lambda_2 -\lambda \; & \ldotp  & \ldotp\\
\mathrm{trace}\left(u_3 \;u_2^T \;D\right) & \ldotp  & \ldotp \\
\ldotp  & \ldotp   & \ldotp \\
\mathrm{trace}\left(u_n u_2^T \;D\right) & \ldotp  & \ldotp
\end{array}
\right\rbrack \bigg)
\label{proots}
\end{array}   
\end{align*}


Finding the determinant of matrices with symbolic variables is described in \cite{https://doi.org/10.48550/arxiv.1304.4691}. We need to precompute only the diagonal elements of $u_i u^T_j$ to obtain the expression for the characteristic polynomial since $D$ is a diagonal matrix. Finding all the roots of the polynomial at each iteration and filtering out the highest roots may be more computationally expensive than finding only extreme eigenvalues as in (\cite{lehoucq_sorensen_yang_1998}, \cite{doi:10.1137/S0895479800371529}).
\\
\par
\textbf{Example:}
\begin{figure}[h]
\hspace{30mm}
\centering
\includegraphics[width=3cm]{./images/completegraph.png}
\caption{Complete graph $\mathbb{K}_3$}
\label{k3graph}
\end{figure}
Let us consider the case of a complete graph on 3 vertices $\mathbb{K}_3$ Figure \ref{k3graph}. The Laplacian matrix of this graph is given by 
\[
\;\;\;L = 
\begin{bmatrix}
2 & -1 & -1 \\
-1 & 2 & -1 \\
-1 & -1 & 2 \\
\end{bmatrix}
\]
The eigenvalues of the matrix $L$ are $\lambda_1 = 0, \lambda_2 = 3, \lambda_3 = 3.$ Now let us perturb two of the weights of this graph and let the multiplier vector $\bar{\gamma} = [1.2, 1.4, 1]$. The matrix $D  = \mathrm{diag(\bar{\gamma}} - \mathbf{1}_{3\times1}).$ The matrix $B^T$ is given as follows,
\[
\;\;\;B^T = 
\begin{bmatrix}
1 & 0 & -1 \\
-1 & 1 & 0 \\
0 & -1 & 1 \\
\end{bmatrix}
\]

The expression given by Equation (\ref{proots}) will then become
$p(\lambda) = \mathrm{det(}  \begin{bmatrix}
3.9374 - \lambda & -0.0786 \\
-0.0786& 3.2626 - \lambda 
\end{bmatrix}  )$. $p(\lambda) = \lambda^2 - 7.2 \lambda + 12.84.$ The roots of the polynomial $p$ are given by 3.2536 and 3.9464, which are the eigenvalues of the matrix $L^{'}$.
\\
The effect on eigenvalues of a matrix with a perturbation is studied in \cite{bhatia_2007}. One such significant result in the field of perturbation theory is given below.
\begin{theorem}
(\cite{bhatia_2007}) If A and A + E are $n \times n$ symmetric matrices, then 
$$|\lambda_k (A + E) - \lambda_k (A) |\leq \vert \vert E \vert \vert_2$$
for $k = 1:n$
\end{theorem}
The aforementioned result can be employed to approximate the eigenvalue term in the error function ($\zeta_l$) described in Section \ref{errorfunc}. However, we currently lack a bound for the $\zeta_q$ term, and computing the $\ell_2$ norm of the matrix involves finding the largest eigenvalue, which is computationally expensive.
\\
We propose an approximation for the eigenvalue and the eigenvector for the new Laplacian matrix based on the eigenvalues and eigenvectors of the original Laplacian matrix making use of the method of Rayleigh iteration, which produces an eigenvalue, eigenvector pair. \\
\textbf{Rayleigh Iteration:} $A \in \mathbb{R}^{n\times n}$ symmetric \\
$x_0$ given, $\vert \vert x_0 \vert \vert_2 = 1$
\begin{algorithmic}
\FOR{$i=0,1$ to ...}
\STATE $\mu_k = r(x_k)$ \, $(r(x) = \frac{x^T A x}{x^T x})$ 
\STATE Solve ($A$ - $\mu_k \mathbb{I})z_{k+1} = x_k $ for $z_{k+1}$ 
\STATE $ x_{k+1} = z_{k+1}/\vert\vert z_{k+1} \vert\vert_2 $
\ENDFOR
\end{algorithmic}

\subsubsection{Approximations to eigenpair}
The Rayleigh quotient iteration algorithm is a powerful tool for computing eigenvectors and eigenvalues of symmetric or Hermitian matrices. It has the property of cubical convergence, but this only holds when the initial vector is chosen to be sufficiently close to one of the eigenvectors of the matrix being analyzed. In other words, the algorithm will converge very quickly if the initial vector is chosen carefully. 
From the first step in Rayleigh iteration we get $\mu_0 = \lambda_{i} + \epsilon$, if we consider the $i$-th eigenvector $\phi_{i}$ as the initial point $x_0$. The next step requires solving for $z_1$ in the linear system $(A+E-\mu_0 I) z_1 = x_0$. Conducting the sensitivity of the linear system with a singularity is difficult to determine and is not trivial. The solution to the system $(A - \mu_0 I)z = \phi_{i}$ is given by $-\frac{\phi_{i}}{\epsilon}$. The first iterate using the conjugate gradient method would then be given by $y_{i} = \frac{1}{\epsilon \;}\;\left(\frac{a^T a\;\;E \phi_{i}}{a^T \left(A + E -\lambda_{i \;} I\;-\epsilon I\;\right)a\;\;}-\phi_{i \;} \right)$. We consider this iterate as our eigenvector approximation after normalizing. The proposed approximations to the eigenpair are given below,  

$$(\hat{\lambda}_{i}, \hat{x}_{i}) = \bigg( r(y_{i}), \frac{y_{i}}{\vert\vert y_{i} \vert\vert} \bigg)$$
In the following theorem, we propose bounds on the approximations of the proposed eigenpair.
\begin{theorem}
\label{theorem_estimate}

Let $G$ be an undirected graph with Laplacian matrix $L$ and $L^{'} = L + E$ be the perturbed Laplacian matrix, where $E = B^T D B$ represents the perturbations in the Laplacian matrix. We denote the $i$-th eigenvector estimate as
$\hat{x}_{i} = \frac{y_{i}}{\vert\vert y_{i} \vert \vert_2}$, $y_{i} = \frac{1}{\epsilon \;}\;\left(\frac{a^T a\;\;E \phi_{i}}{a^T \left(A + E -\lambda_{i \;} I\;-\epsilon I\;\right)a\;\;}-\phi_{i \;} \right)$. Then $\vert \vert (L + E - r(y_{i}) I \,) \hat{x}_{i} \vert \vert_2 \leq  \tau  + \vert \vert E \vert \vert_2$, $\epsilon = \phi_{i}^T E  \phi_{i}.$ Where $a = E \phi_{i}$, $\tau = max(\vert \lambda_n - \lambda_{2}(L+E) \vert, \lambda_n(L+E))$. $\lambda_n \geq \lambda_{n-1} \geq...\geq \lambda_2 \geq \lambda_1$ denotes the eigen values of the Laplacian matrix of graph $G$.
\end{theorem}
\begin{proof}

$\vert \vert ((L+E) - \frac{y_{i}^T (L+E)y_{i}}{y_{i}^T y_{i}}I ) \hat{x}_{i} \vert \vert = \vert \vert (\Phi \Lambda \Phi^T - \frac{y_{i}^T (L+E) y_{i}}{y_{i}^T y_{i}}I + E) \hat{x}_{i} \vert \vert $ (since Laplacian matrix is diagonalizable and $L = \Phi \Lambda \Phi^T$).
\\
\[\vert \vert (\Phi \Lambda \Phi^T - \frac{y_{i}^T (L+E) y_{i}}{y_{i}^T y_{i}}I + E) \hat{x}_{i} \vert \vert \leq  \]
\begin{align*}
\vert \vert \Phi (\Lambda - \frac{y_{i}^T (L+E) y_{i}}{y_{i}^T y_{i}}I) \Phi^T + E \vert \vert \,\,  \vert \vert \hat{x}_{i} \vert \vert = \vert \vert \Phi (\Lambda -  \frac{y_{i}^T (L+E) y_{i}}{y_{i}^T y_{i}}I) \Phi^T + E \vert \vert    
\end{align*}

Using the triangular inequality of matrix norms, the above inequality becomes 
$$ \leq 
\vert \vert \Phi (\Lambda - \frac{y_{i}^T (L+E) y_{i}}{y_{i}^T y_{i}}I) \Phi^T \vert \vert + \vert \vert E \vert 
\vert \leq  \tau  + \vert \vert E \vert \vert
$$
It can be observed that $\mathbf{1}^T y_{i} = 0$ and from the definition, we have $\lambda_k(L) = \mathrm{inf_{x \perp P_{k-1}} \,}\frac{x^T L x }{x^T x}$ where $P_{k-1}$ is the subspace generated by eigenvectors of the Laplacian matrix $L$ given by \\ $\mathrm{span(}v_1,v_2,...,v_{k-1}).$ Thus the Fielder value or the second smallest eigenvalue $\lambda_2$ of the Laplacian matrix is given by $\lambda_2 = \mathrm{inf_{x \perp \mathrm{1}}} \frac{x^T L x}{x^T x}$ and the largest eigenvalue $\lambda_n = \text{sup}_x \frac{x^T L x}{x^T x}$. Thus the above inequality becomes 
$$ \leq \tau  + \vert \vert E \vert \vert \mathrm{\,\,,where \,\, \tau = max(\vert \lambda_n - \lambda_{2}(\textit{L+E})\vert, \lambda_n(\textit{L+E}))}
$$
\end{proof}
The error function using the estimate would then be 
$$
\boxed{\bar{\zeta} = n \frac{\sum_{i=1}^{n_p} \vert \hat{\lambda}_{i} - \lambda_{i} \vert^2}{n_p \;\; \sum_{i = 1}^{n_p} (\lambda_{i})^2} + \frac{\sum_{i=1}^{n_p} \vert \langle\hat{x}_{i}, \phi_{i}\rangle - 1 \vert^2 }{n_p}}
\label{estimate_eval}
$$
From (\cite{reddy_trefethen_1990}), the following \textbf{definition} apply to pseudo spectra of matrices. \\
If $A$ is an $N \times N$ matrix and for $\lambda$ to be an $\epsilon-$pseudo-eigenvalue of A is 
$$ \sigma_{N}(\lambda I - A) \leq \epsilon$$ where $\sigma_N$ denotes the smallest singular value. From the above definition, we can see that our estimate $\hat{\lambda}_{i}$ is a $\tau + \vert\vert E \vert\vert_2$-\textbf{pseudo eigenvalue} of \textit{L}. 
\\
We modify the constraint $\zeta \leq \beta_1$ by $\bar{\zeta} \leq k_1 \beta_1$, where $k_1$ and $\beta_1$ are user defined constants. \\
We present numerical experiments on certain graphs, and for every graph, we take the number of eigenmodes to preserve as the largest eigenmodes of the Laplacian matrix of the graph. Random perturbation is given to every edge of the graph, and a comparison is between $\zeta$ and $\bar{\zeta}$ (Table \ref{tab:my_label}).
\par Figures showing the effectiveness of the approximate eigenpair in preserving the eigenvalues and eigenvectors are described by Figures in Section \ref{image-eval1}. 
\newline
\begin{theorem}
Given an undirected graph $G = (V,E,W)$, with the Laplacian matrix $L$, then the matrix $M = L - (pI)$ is not orthogonal $\forall \,p \in \mathbb{R}$ if the minimum degree of the graph $d_{\mathrm{min}}(G) > 2.$
\label{Theo2}
\end{theorem}
\begin{proof}
We prove the above theorem by contradiction. For a matrix $M$ to be orthogonal, we require $M^T M = I$. The eigenvalue decomposition of the matrix $L = \Phi \Lambda \Phi^T$. Applying the definition of the orthogonality of matrices to the matrix $M$, we get $\Phi(\Lambda - pI)^2 \Phi^T = I$. Using the property of similarity of matrices, we can observe that all the eigenvalues of $\Phi(\Lambda - pI)^2 \Phi^T$ should be 1. From the zero eigenvalue of the matrix $L$, $p = \pm 1$. The remaining eigenvalues of the Laplacian matrix should be either 0 or 2 according to this $p$ value, but since the minimum degree of the graph is greater than 2, this cannot be the case as the largest eigenvalue of the Laplacian matrix of the graph will be greater than or equal to the minimum degree ($\lambda_n \geq d_{\text{min}} > 2,\; \lambda_n = \text{sup}_x \frac{x^T L x}{x^T x}$, taking $x = e_1$, where $e_i$ denotes the canonical vector with a 1 in the $i$th coordinate and $0'$s elsewhere).    
\end{proof}
We make use of Theorem \ref{Theo2} in the section below (see Eq. \ref{b}).
\subsubsection*{Points of discontinuity for the function $\bar{\zeta}$}
Discontinuities in $\bar{\zeta}$ can happen due to the following cases, \\
\textbf{Case 1:}\, \[\phi_{i}^T E \phi_{i} = 0. \tag{d.(a)} \label{a}\] 
\\
\textbf{Case 2:}\, \[a^T(L + E - \lambda_{i}I - \epsilon I) a = 0, \text{\;where\;} a = E \phi_{i} \neq \mathbf{0}_{n\times1}. \tag{d.(b)} \label{b} \]  \\
Considering the first case, one possibility is when $ E $ is a rotation matrix. However matrix $E$ is not orthogonal $(\text{det}(E) = 0)$. The other possibility is when $\phi_{i}\, \in$ nullspace($E$). If this happens, then we could see that $(\lambda_{i},\phi_{i})$ is an eigenvalue, eigenvector pair for the new Laplacian matrix. Possible candidates of $\gamma$ satisfying this condition are given by the solution to the under-determined linear system under non-negativity constraints.

\[
 R \gamma = \lambda_{i} \phi_{i}, \, \mathrm{where\,\,} R = B^T W^{1/2} \text{$\text{diag}(W^{1/2}B\phi_{i})$}
\]
\[
\gamma \geq 0
\]
\\
Case 2 occurs when $ a \neq \mathbf{0}_{n\times1}\,\text{and}\, a^T (\Tilde{L} - (\lambda_{i} + \phi_{i}^T E \phi_{i})I) a = 0.$ One possibility is that  $M_{i} = (\Tilde{L} - (\lambda_{i} + \phi_{i}^T E \phi_{i})I)$ needs to be a rotation matrix, but this case cannot happen if we impose a minimum degree constraint as mentioned in Theorem \ref{Theo2}.
Elements in the nullspace of matrix $M_{i}$ under non-negativity constraints are candidates for discontinuity of the function $\bar{\zeta}$. We modify the eigenpair based on the following discontinuities. \\
\[
\label{errorfunctionestimate}
(\tilde{\lambda}_{i}, \tilde{\phi}_{i}) = 
\begin{cases}
  (\lambda_{i}, \phi_{i}), \hspace{30mm} \text{if } E \phi_{i} = \mathbf{0}_{n \times 1}  \\ 
  (\frac{a^T M_{i} a}{a^T a}, \phi_{i}), \hspace{24mm}\text{if } y_{i} = \mathbf{0}_{n \times 1} \text{\;and\;} \\ \hspace{52mm}E \phi_{i} \neq \mathbf{0}_{n \times 1}\\
  (\lambda_{i} + \phi_{i}^T E \phi_{i}, \frac{a}{\vert\vert a \vert \vert}), \hspace{15mm} \text{if } M_{i} a = \mathbf{0}_{n\times 1}, \\ \hspace{45mm}y_{i}, E \phi_{i} \neq \mathbf{0}_{n\times 1} \\ \hspace{45mm}\text{\; and \;} \vert\vert y_{i} \vert\vert < \infty \\
  (\hat{\lambda}_{i}, \hat{x}_{i}), \hspace{30mm}  \text{otherwise} 
\end{cases}
\]
The error function now is modified as 
$$
\bar{\zeta}(\bar{\gamma}) = \frac{n}{n_p \sum_{i = 1}^{n_p} (\lambda^{i})^2} \sum_{i=1}^{n_p} (\tilde{\lambda}_{i} - \lambda_{i})^2 + \frac{1}{n_p} \sum_{i=1}^{n_p} (\langle\tilde{\phi_{i}}, \phi_{i}\rangle - 1)^2 . $$
The summation terms could be computed in parallel using multiple cores of the machine for large graphs. The speedup obtained when evaluating this function in parallel is shown in Table \ref{tab:my_label} and Figure \ref{speedupfigure}.
\subsection*{Gradient of error function $\bar{\zeta}$}
\label{nonlinear_constr_grad}
\begin{align*}
\frac{d \bar{\zeta}}{d \gamma} = \frac{2n}{n_p \sum_{i = 1}^{n_p} (\lambda^{i})^2} \sum_{i = 1}^{n_p}  (\tilde{\lambda}_{i} - \lambda_{i}) \frac{d \tilde{\lambda}_{i}}{d \bar{\gamma}} + \frac{2}{n_p} \sum_{i = 1}^{n_p} (\langle\tilde{\phi}_{i}, \phi_{i}\rangle - 1) \hspace{2mm} (D_{\bar{\gamma}} \tilde{\phi}_{i})^T \phi_{i}   
\end{align*}

\[
\hat{\lambda}_{i} = \frac{y(\bar{\gamma})^T \Tilde{L} y(\bar{\gamma})}{y(\bar{\gamma})^T y(\bar{\gamma})}
\]
\[
\frac{d \hat{\lambda}_{i}}{d \bar{\gamma}} = \frac{y(\bar{\gamma})^T y(\bar{\gamma}) \hspace{2mm} \nabla{y(\bar{\gamma})^T \Tilde{L} y(\bar{\gamma})} - (y(\bar{\gamma})^T \Tilde{L} y(\bar{\gamma})) \; \nabla{y(\bar{\gamma})^T y(\bar{\gamma})}}{(y(\bar{\gamma})^T y(\bar{\gamma}))^2}
\]

\[
\nabla_{\bar{\gamma}}{y(\bar{\gamma})^T \Tilde{L} y(\bar{\gamma})} = 2 D_{\bar{\gamma}} (B_1(\bar{\gamma}))^T B_1(\bar{\gamma}) 
\]
\[
B_1(\bar{\gamma}) = \bar{\gamma}^{1/2} W^{1/2} B y(\bar{\gamma})
\]
\[
\nabla{y(\bar{\gamma})^T y(\bar{\gamma})} = 2 (D_{\bar{\gamma}}y(\bar{\gamma}))^T y(\bar{\gamma})
\]
\[
\hat{x}_{i} = \frac{y(\bar{\gamma})}{\vert\vert y(\bar{\gamma})\vert\vert}, \;\nabla \left( \frac{y_j(\bar{\gamma})}{\vert\vert y \vert\vert}\right) = \left(\frac{\nabla y_j(\bar{\gamma})}{\vert\vert y \vert\vert}  - \frac{y_j(\bar{\gamma})\,D_{\bar{\gamma}}y(\bar{\gamma})^Ty(\bar{\gamma})}{\vert\vert y \vert\vert^3 }\right)
\]
\begin{table}[]
    \centering
    \begin{tabular}{ 
|p{2cm}|p{1cm}|p{0.8cm}|p{1.2cm}|p{1.5cm}|p{1.5cm}|p{1.4cm}|  }
\hline
\textbf{Graph} & n & $n_p$ & m & Relative error $\frac{(\zeta - \bar{\zeta})}{\zeta}$& speedup \;\; (serial) & speedup \;\;(parallel) \\
\hline
complex  & 1408 & 282 & 46326 & 0.0013
  & 1.5532 & 0.9061 
\\
\hline
145bit  & 1002 & 201 & 11251 & 0.0122 &  3.2414 & 0.4946 \\
\hline
192bit  & 2600 & 520 & 35880 & 0.0362 & 13.7567 & 11.9106 \\
\hline
130bit & 584 & 117 & 6058 & 0.0088 & 2.9379 & 0.1888 \\
\hline
Ca-HepPh & 11204 & 2241 & 117619 & 0.0079
 & 27.4243 & 77.1951\\
 \hline
 Ca-HepTh & 8638 & 1728 & 24806 & 0.0341 & 83.6472 & 184.8210 \\
\hline
\end{tabular}
    \caption{Table showing comparison of $\zeta$ and $\bar{\zeta}$ for real world graphs with random perturbation to the edges.}
    \label{tab:my_label}
\end{table}
\label{estimatetable}

\section{Optimization problem}
\label{costfunc}
The sparsification framework we propose involves solving a constrained optimization problem, and the underlying constrained optimization problem is discussed in this section. In the optimization objective function (Eq. \ref{costfuncadjoint}), we strive to promote sparsity in the multipliers by incorporating a term that involves the $\ell_1$ norm. Optimization using $\ell_1$ term generally induces sparsity. The term ($T_1$) in the objective function seeks to minimize the disparity between the observations at different time points, projected onto a reduced dimensional space derived from various trajectories, and the states obtained at these times using the reduced order model with the weights $\bar{w} = \text{diag($w$)} \bar{\bar{\gamma}}$. Here $F^c(t_j)$ represents the observation from trajectory $c$ at time $t_j$. The initial conditions used to get observations $F^c(\cdot)$ will be kept invariant (Eq. \ref{9.c2}). $Q^{'} = Q \,\text{diag($w$)}$, where $Q$ represents the unsigned incidence matrix of the graph (Figure \ref{example_connect}), matrix $Q^{'}$ is used to impose the connectivity constraint (Eq. \ref{9.c3}). $A^c$ contains indices of observations assimilation from trajectory c. $\bar{F}^c(\cdot)$ follows the forward model $M(\cdot)$, the discretization for the reduced order model (Eq. \ref{9.c1}). Constraint $\bar{\zeta} \leq k_1 \beta_1$ tries to minimize perturbations to the largest $n_p$ eigenmodes of the Laplacian matrix (Eq. \ref{C}).  
\begin{equation}
\\
\\
\textbf{minimize} \,\, J(\bar{\gamma}) =  \underbrace{\frac{\sum_{c =1}^{\omega}
      \sum_{j \in A^c} \vert\vert(F^{c}(t_j )  -\bar{F}^{c}(t_j) \;)\vert\vert^2 }{2}}_{T_1}  + \color{blue}{ {\frac{\alpha}{2} \sum_{i=1}^m \vert \bar{\gamma}_i \vert}}     
\label{costfuncadjoint}    
\end{equation}

%\begin{center}
\vspace{-6mm}
\hspace{-20mm}
\begin{alignat*}{2}
\textbf{subject\,\, to} \\
& \tag{9.c1} \label{9.c1} \bar{\mathcal{F}}^{c}_{q+1} = M(\bar{\mathcal{F}}^{c}_{q},\bar{\gamma})\quad &&c = 1,\dots,\omega,\;\;q = 0,1,\dots,T-1,\\ &\bar{F}^c_0 = F^c_0  \tag{9.c2} \label{9.c2}\\
&Q^{'} \bar{\gamma} \geq \tau \quad && \tag{9.c3} \label{9.c3}\\
&\bar{\gamma}_j \geq 0, &&j = 1,2,\dots,m \tag{9.c4} \label{9.c4}\\
&\bar{\zeta}(\bar{\gamma}) \leq k_1\, \beta_1 &&
\tag{9.c5} \label{C}
\end{alignat*}
 \par
 \vspace{5mm}
 The first variation of $J$ is given by $$
 \vspace{-0.5mm}
 \delta J  = \sum_{c= 1}^{\omega} \sum_{j \in A_{c}} < (V_j^c)^T \eta_j^c,\delta\bar{\gamma}> + \textcolor{blue}{ \frac{\alpha}{2} <(\frac{\vert \bar{\gamma}_1 \vert}{\bar{\gamma}_1},..., \frac{\vert \bar{\gamma}_i \vert}{\bar{\gamma}_i},..,\frac{\vert \bar{\gamma}_n \vert}{\bar{\gamma}_n}),\delta \bar{\gamma}>}\,  
 \text{, see Equation (\ref{first_var})}
 $$
 
%  \lipsum
\begin{figure*}
    \centering
    \includegraphics[width=0.8\textwidth]{./images/data_assimfig.png}
    \caption{Data Assimilation approach.}
    \label{datsapproach}
\end{figure*}
% \lipsum
The primary focus of our optimization routine is to minimize the function $J$ (Eq. (\ref{costfuncadjoint})) subject to constraints. The gradient of $J$ is obtained using the adjoint method algorithm as mentioned in (Section \ref{Adjointalgo}). The model $M(\cdot)$ used for forward dynamics represents the discretization for the ROM to reduce computational complexity.
\par If we consider the optimization problem without the non-linear constraint, we can observe that the search space for our objective function described in (Eq. \ref{costfuncadjoint}) can be made closed by using the substitution ($\bar{\gamma} = \gamma^+ - \gamma^-$, $\vert\vert \bar{\gamma} \vert\vert_1 = \mathbf{1}^T \gamma^+  + \mathbf{1}^T \gamma^-$), where $\gamma^+$ represents the positive entries in $\gamma$, 0 elsewhere and $\gamma^-$ represents the negative entries in $\gamma$ and 0 elsewhere. So the objective function and the constraints are continuous. So we have an optimization problem as described below,
\begin{align*}
&\min_{x \in X}  f(x)\
&\text{subject to\;} h_j(x) \leq 0,\;\; j = 1,2,...n.
\end{align*}
Set $X$ is closed where functions $f$ and $h_j$ are continuous. We could use the Barrier methods for optimization as described in (\cite{nocedalbook},\cite{luenberger2021}). The feasibility set $S = X \cap \{x \,|\, h_j(x) \leq 0 \}$. If we generate a decreasing sequence $\epsilon_k$, let us denote $x_k$ as a solution to the problem $\mathrm{min_{x \in S}} \,\,f(x) + \epsilon_k B(x)$. 
Then every limit point of $\{x_k\}$ is a solution to the problem. One selection of barrier function $B(x) = -\sum_{j=1}^n \log(-h_j(x))$(Barrier convergence theorem).
However, because of the discontinuities in the non-linear constraint, convergence in the constrained optimization problem may not be obtained. Heuristic algorithms for global optimization described in \cite{nocedalbook}, like Genetic algorithms, Pattern search algorithms exist however require multiple function evaluations.
\begin{figure}[htb]
\centering
\begin{minipage}{0.5\textwidth}
\includegraphics[width=\linewidth]{./images/speedup.png}
\caption{Speedup obtained for non-linear constraint evaluation.}
\label{speedupfigure}
\end{minipage}%

\end{figure}
Since the computation of the objective function in the optimization problem (Section \ref{costfunc}) is computationally expensive, we do not prefer such methods.
\\
\fbox{
\begin{minipage}{0.97\textwidth}
\textbf{ALGORITHM}\\
\vspace{3pt} % Add some space above the line
\hrule height 1pt % Set the height and thickness of the line
\vspace{3pt} % Add some space below the line
\textbf{INPUT:} 
\begin{align*}
   &   \text{An undirected Graph\;}  G = (V,E,w) \\ 
                            & \alpha - \; \ell_1 \; \text{regularization parameter} \\
                            & \tau_{\vert V \vert \times 1} - \text{connectivity parameter} \\
                            & \epsilon - \text{edge pruning parameter} \\
                            & N - \text{\;number of trajectories taken for POD step} \\
                            & \omega - \text{number of trajectories taken for assimilation} \\
                            & p_1, p_2,\dotsc,p_{\omega} - p_i \text{\;represents the number of points taken from trajectory $i$\;} \text{for assimilation}       \\
                            & k -\text{\;reduced dimension} \\
                            & n_p - \text{number of eigenmodes taken for the non-linear constraint} \\
                            & k_1, \beta_1 - \text{non-linear constraint parameters} \\
                            & F^1_0, F^2_0,....., F^N_0 - \text{initial conditions for $N$ trajectories} 
\end{align*}
\\
\hrule height 1pt
\begin{algorithmic}[] % Start line numbering
\STATE \hspace{-5mm}\textbf{OUTPUT:} Sparse graph $\Tilde{G} = (V,\Tilde{E},\Tilde{w})$
\STATE
\STATE \hrule height 1pt
\STATE \textbf{Procedure:}
\STATE 1. Run the forward reaction-diffusion model as in (Eq. \ref{Rd dynamics}) with initial conditions $F^1_0, F^2_0,....., F^N_0$ and sample $p_i$ number of data points in the interval $[0, T]$, $i = 1,2,\dotsc, \omega$ for assimilation, Sample data points at regular intervals for each trajectory thus obtaining the snapshot matrix containing $N$ trajectories. 
\STATE
\STATE 2. From the snapshot matrix, obtain the matrix $\rho$ of projection and mean $\bar{x}$ as mentioned in (Section \ref{ROMappro}), selecting the $k$ largest eigenvectors of the covariance matrix.
\STATE
\STATE 3. From the $N$ trajectories, select $\omega \leq N$ trajectories for the data assimilation correction part.
%\STATE
\STATE 4. Using the sampled trajectories, run the minimization procedure on the optimization problem discussed in (Section \ref{costfunc}) to obtain the weight multipliers $\bar{\gamma}$.
%\STATE
\STATE 5. On obtaining the multipliers, construct the weights $\bar{w}$ using the multiplier $\bar{\gamma}$, $\bar{w} = \text{diag($w$)} \bar{\gamma}$. Remove weights less than $\epsilon$ to obtain the new weights $w^{'}$. 
%\STATE
\STATE 6. Construct $\Tilde{G}$ using the weights $w^{'}$.
\end{algorithmic}
\textbf{END ALGORITHM}
 \end{minipage}
}
\subsection*{Running time of the procedure}
The time-consuming step in our sparsification procedure is solving a constrained non-linear programming problem (step 4 in algorithm). For every iteration of the optimization procedure, we need to compute the projected vector field (Eq. \ref{fa}) which takes $O(k|V|^2$) operations. Evaluation of the gradient takes $O(k\vert E \vert)$ operations\;(Figure\;\;\ref{costfunccomplexity}).
\par The nonlinear constraint evaluation and the gradient evaluation take $O(|V|^2 n_p)$ and $O(|E|^2 n_p$) operations (Figure \ref{nonlinearcomplexity}) respectively. Article \cite{nocedal} describes the challenges involved in solving a nonlinear constrained optimization problem. We exploit parallelism in the nonlinear constraint evaluation steps, and the speedups are shown in Figure \ref{speedupfigure}.



\begin{figure}[H]
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=0.9\linewidth, height=5cm]{./images/time_costfunc.png} 
\caption{Figure showing the time complexity of cost function.}
\label{costfunccomplexity}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=0.9\linewidth, height=5cm]{./images/nonlinear_complexity.png}
\caption{Figure showing time complexity of the non-linear constraint.}
\label{nonlinearcomplexity}
\end{subfigure}

\caption{Time complexity}
\end{figure}

\section{Results}
The framework developed is applied to several real-world graphs (\cite{nr-aaai15},\cite{snapnets}). The parameters in algorithm described in Section \ref{costfunc} were assigned as follows $\alpha = 16 \log(\vert E \vert)$, $p_1, p_2 $ = 50, $D_x = 0.1$, $D_y = 1.4$, $N = 2$, $\omega = 2$, $k_1 = 1$, $\beta_1 = 10^{-2}$, $\tau = \text{max(2.1,\,}0.1\,d_{\mathrm{min}})\mathbf{1}_{n\times 1}\, \, $,\,$ \epsilon = 10^{-2}$, $k = \mathrm{min(}\lceil\frac{n}{5}\rceil,50)$ and $n_p = \lceil n / 5\rceil$, where $d_{\mathrm{min}}$ denotes the minimum degree of the graph. We include the $n_p$ largest eigenvalues and eigenvectors of the Laplacian matrix in the non-linear constraint evaluation step. The Euler forward method is used for finding the solution at discrete time steps in the adjoint sensitivity step, and the explicit Range-Kutta 4 slope method is used to generate snapshot matrices required for the POD step. The tabulated results on some real-world graphs are mentioned below (Table \ref{Allresults}). The constrained optimization step is solved in MATLAB \cite{MATLAB}, and we are using the interior point method for constrained optimization as described in \cite{nocedal}. The correlation coefficient $R$ is obtained by comparing the grid of solution points of reaction-diffusion system (Eq. \ref{Rd dynamics}) on the original and sparsified graph for a random perturbation to the equilibrium point. We present an illustrative example of our frameworks performance on a 50-node Erd\H{o}s-R$\acute{e}$nyi random graph, as depicted in Figure \ref{randomnode}.  \\
\par Figure \ref{fig:overall} illustrates the comparison between the eigenpairs of the given graph and its corresponding sparsifier in Table \ref{Allresults}.
\begin{table}[H]
\vspace{-8.7mm}
\begin{align*}
\begin{tabular}{
|p{2cm}|p{0.75cm}|p{1cm}|p{1.5cm}|p{1cm}|p{2cm}|  }
\hline
\textbf{Graph} & $n$  & $|E|$ & Number of eigenmodes computed ($n_p$) & $| \vspace{5mm}\tilde{E}|$ & Correlation-coefficient ($R$) \\
\hline 
complex  & 1408 & 46326 & 282 & 39926 & 0.8654\\
\hline 
145bit  & 1002 & 11251 & 201 & 7767 & 0.8737 \\
\hline 
192bit & 2600  & 35880 & 520 & 35880  & 0.9189\\
\hline 
130bit  & 584 & 6058 & 117 & 4350  & 0.8572\\
\hline 
ca-HepTh  & 8638  & 24806 & 1728 & 22709 & 0.9666
\\
\hline
\end{tabular}
\end{align*}
\caption{}
\label{Allresults}
\end{table}

 \label{image-eval1}

\begin{figure}[H]
 \caption*{\textbf{complex }}
  \begin{subfigure}[t]{0.360\linewidth} \includegraphics[width=\linewidth,height=4cm]{./images/complex_1.png}
  \caption*{10.1(a)}
  \end{subfigure}
  \begin{subfigure}[t]{0.320\linewidth}
    \includegraphics[width=\linewidth,height=4cm]{./images/complex_2.png}
    
    \caption*{10.1(b)}
  \end{subfigure}
  \hspace{-3mm}
  \begin{subfigure}[t]{0.320\linewidth}
    \includegraphics[width=\linewidth,height=4cm]{./images/complex_3.png}
    \caption*{10.1(c)} 
  \end{subfigure} 

  \caption*{\textbf{130bit}}
   \begin{subfigure}[t]{0.360\linewidth}
\includegraphics[width=\linewidth,height=4cm]{./images/130bit_1.png}
    \caption*{10.2(a)}   
  \end{subfigure}
  \begin{subfigure}[t]{0.320\linewidth}
    \includegraphics[width=\linewidth,height=4cm]{./images/130bit_2.png}
    
    \caption*{10.2(b)}
  \end{subfigure}
  \hspace{-3mm}
  \begin{subfigure}[t]{0.320\linewidth}
    \includegraphics[width=\linewidth,height=4cm]{./images/130bit_3.png}
    \caption*{10.2(c)} 
  \end{subfigure}

\end{figure}
\addtocounter{figure}{-1}


\begin{figure}[H]
  \caption*{\textbf{145bit}}
   \begin{subfigure}[t]{0.360\linewidth}
\includegraphics[width=\linewidth,height=4cm]{./images/145bit_1.png}
    \caption*{10.3(a)}   
  \end{subfigure}
  \begin{subfigure}[t]{0.320\linewidth}
    \includegraphics[width=\linewidth,height=4cm]{./images/145bit_2.png}
    
    \caption*{10.3(b)}
  \end{subfigure}
  \hspace{-3mm}
  \begin{subfigure}[t]{0.320\linewidth}
    \includegraphics[width=\linewidth,height=4cm]{./images/145bit_3.png}
    \caption*{10.3(c)} 
  \end{subfigure}
\caption*{\textbf{192bit}}
   \begin{subfigure}[t]{0.360\linewidth}
\includegraphics[width=\linewidth,height=4cm]{./images/192bit_1.png}
    \caption*{10.4(a)}    
  \end{subfigure}
  \begin{subfigure}[t]{0.320\linewidth}
    \includegraphics[width=\linewidth,height=4cm]{./images/192bit_2.png}
    
    \caption*{10.4(b)}
  \end{subfigure}
  \hspace{-3mm}
  \begin{subfigure}[t]{0.320\linewidth}
    \includegraphics[width=\linewidth,height=4cm]{./images/192bit_3.png}
    \caption*{10.4(c)}  
  \end{subfigure}
\caption*{\textbf{CA-HepTh}}
   \begin{subfigure}[t]{0.360\linewidth}
\includegraphics[width=\linewidth,height=4cm]{./images/hepth_1.png}
    \caption*{10.5(a)}   
  \end{subfigure}
  \begin{subfigure}[t]{0.320\linewidth}
    \includegraphics[width=\linewidth,height=4cm]{./images/hepth_2.png}
    
    \caption*{10.5(b)}
  \end{subfigure}
  \hspace{-3mm}
  \begin{subfigure}[t]{0.320\linewidth}
    \includegraphics[width=\linewidth,height=4cm]{./images/hepth_3.png}
    \caption*{10.5(c)} 
  \end{subfigure}
  \caption{Figures 10.(1-5)(a) shows the relative changes in the eigenvalues of the Laplacian matrix of the sparsifier($\frac{\tilde{\lambda}_{i} - \lambda_{i} }{\lambda_{i}}$). Figures 10.(1-5)(b) plot showing the first two coordinates of all the eigenvectors of the Laplacian matrix of the graph in the x and y-axis. Figures 10.(1-5)(c) plot showing the first two coordinates of all the eigenvectors of the Laplacian matrix of the sparsifier in the x and y-axis.}
  \label{fig:overall}
\end{figure}
\begin{figure}[htb]
\begin{minipage}[b]{1\linewidth}
\centering
\includegraphics[width=\linewidth]{./images/result.png}
\caption*{}
\end{minipage}
\hspace{0.05\linewidth}
\begin{minipage}[b]{1\linewidth}
\centering
\includegraphics[width=\linewidth]{./images/errorbars.png}
\caption{Figure showing patterns produced on an Erd\H{o}s-R$\acute{e}$nyi 50 node random graph on the left and its sparsifier on the right.}
\label{randomnode}
\end{minipage}

\end{figure}
\section{Sparsifying ODENets}
\label{odenetsection}
Neural ODEs or Neural Ordinary Differential Equations were first introduced by Chen et al. in their 2018 paper "Neural Ordinary Differential Equations" \cite{DBLP:journals/corr/abs-1806-07366}.
The basic idea behind neural ODEs is to parameterize the derivative of the hidden state using a neural network. This allows the network to learn the system's evolution over time rather than just mapping inputs to outputs. In other words, the neural network becomes a continuous function that can be integrated over time to generate predictions.

One of the key advantages of neural ODEs is that they can be used to model systems with complex dynamics, such as those found in physics, biology, and finance. They have been used for various applications, including image recognition, time series prediction, and generative modelling. This section looks at the possibility of our framework in sparsifying such ODENets. In this section, our focus lies in addressing the following question: Can we mimic the dynamics of a dynamical system on a neural ODENet with a collection of snapshots of solutions using a sparse set of parameters by leveraging the adjoint sensitivity framework within a reduced-dimensional setting? \\
Let us consider an example of a Linear dynamical system defined below. 
\begin{equation}
\frac{dx}{dt} = Ax + b,\;\; A\in \mathbb{R}^{6\times6},\; b \in \mathbb{R}^6.
\label{LiDS}    
\end{equation}

For a predefined initial condition $x(0) = x_0$, the above problem will be an IVP. The neural network architecture proposed has one hidden layer with 50 neurons, with the input and output layers having six neurons for our experiment. A nonlinear activation is given at the output layer. The neural network output function proposed is given by $$nn(x(t)) = \sinh[\theta_2\theta_1x(t) + \theta_2 b_1 + b_2]$$
$$\text{\,\,} \theta_1 \in \mathbb{R}^{50\times6}, \theta_2 \in \mathbb{R}^{6\times50}, b_1 \in \mathbb{R}^{50}, b_2 \in \mathbb{R}^6.$$

\subsubsection*{Procedure}
\begin{enumerate}
    \item Generate the projection matrix $\rho \in \mathbb{R}^{2\times 6}$ and mean $\bar{x}\in\mathbb{R}^6$ of projection based on the POD method (Section \ref{ROMappro}) using the snapshot matrix consisting of data points from the linear dynamical system (Eq. \ref{LiDS}).
    \item Sample $p$ data points without replacement from the snapshot matrix for assimilation, denoting this data as $\mathcal{D} = \{ x_i\}_{i = 1,2,\ldots, p}$. Using the projection matrix, project the data into reduced dimension ($\mathcal{D}_{\text{proj}} = \{\rho(x_i - \bar{x})\}_{i = 1,2,\ldots, p}$). The projected neural network output function is given by,
    $$(nn_{\textbf{proj}}(z(t)) = \rho\, \sinh[\theta_2\theta_1\rho^Tz(t) + \theta_2\theta_1\bar{x} + \theta_2b_1 + b_2]$$
    \item For the sake of simplicity, we assume the parameters to be constant over time intervals. The adjoint sensitivity method for data assimilation is used to recover the parameters. The Euler forward method is used for the discretization of the projected neural ODE slope function. The cost function used for obtaining the parameters is given by (Eq. \ref{costfuncadjoint}). For this experiment, we utilized a set of 10 observations obtained from a single trajectory, randomly selected from a pool of 500 timesteps. Additionally, the chosen regularization parameter $\alpha$ was set to 40.


\begin{figure}[H]
  \centering
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth, height = 6.2cm]{./images/neural_ode_complex.png}
    \caption{Error in the solution obtained from neural ODENet compared with the Euler forward method on the linear dynamical system (Eq. \ref{LiDS}).}
    \label{errornode}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.40\linewidth}
    \centering
    \includegraphics[width=\linewidth, height =7cm]{./images/sparsity_pattern.png}
    \caption{Sparsity pattern in the parameter vector of the neural ODENet.}
    \label{sparsitypattern}
  \end{minipage}
\end{figure}
\end{enumerate}
No constraints on the neural ODENet parameters are imposed, so the parameter estimation problem is unconstrained. The difference in the solutions obtained from the neural ODENet and the Euler forward step on the linear dynamical system described by Equation (\ref{LiDS}) is shown in the Figure \ref{errornode}. The number of non-sparse parameters obtained from step 3 is 289. The sparsity pattern of the parameters of the neural ODENet is shown in Figure \ref{sparsitypattern}.    

\section{Conclusion}
\textcolor{black}{We have developed a novel framework that enables efficient sparsification of undirected graphs through a dynamic optimization problem considering a subset of trajectories in a ROM space.} The optimization objective focuses on preserving the patterns in a reduced dimension and giving importance to producing a sparse vector of weights. Our approach delivers good computational performance, with significantly high correlation coefficients obtained when comparing solutions for the sparse and original graphs under random perturbations. Table \ref{tab:my_label} compares the proposed approximations of the eigenpairs (Theorem \ref{theorem_estimate}) with the actual eigenpairs for several real-world graphs under random perturbations to the edges of the graph.
\par The adjoint method cost function on a ROM with an $\ell_1$ norm term produced a sparse vector of weight multipliers, as shown in Table \ref{Allresults}, and the sparse network produced patterns similar to the given network. The framework proved to be also effective in sparsifying ODENets; the sparsity patterns are shown in Figure \ref{sparsitypattern}.
To further improve our approach's computational efficiency, we leveraged multi-core processing for evaluating the error function ($\bar{\zeta}$), resulting in significant speedup (Figure \ref{speedupfigure}) when analyzing large graphs. Corrections done using observations in a ROM space proved to provide good computational performance; Figure \ref{randomgraph_rom} and Figure \ref{realgraph_rom} demonstrate the performance of the POD method for the reaction-diffusion system (Eq. \ref{Rd dynamics}) on both random and real-world graphs.

 
\par  The framework can be generalised to any reaction-diffusion systems involving undirected graphs. However, sparsifying and generating isodynamic patterns in the heterogeneous Kuramoto model as described in \cite{kuramoto_1984} and in other systems like the Ginzgurg-Landau model (\cite{garcÃ­a-morales_krischer_2012}) involving directed graphs are out of scope for this work.

\section*{Acknowledgments}
This work was partially supported by the MATRIX grant MTR/2020/000186 of the Science and Engineering Research Board of India.
\bibliographystyle{plain}
\bibliography{bibfile}
\end{document}
