%%%%%%%% Sample LaTeX input for Complex Systems %%%%%%%%%%% 
% Revision 4, Jun 27, 2018
%
% This is a LaTeX input file  
% Text following % on a particular line is treated as a comment, and 
% ignored by LaTeX.  
% You do not need to type any text that follows a % 
% 
\documentclass{article}
\usepackage{epsf,hyperref}
\usepackage{amssymb,ComplexSystems}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algcompatible}
\usepackage{xcolor}
\usepackage{fancyhdr}
% complex-systems.sty is the macro package for Complex Systems.
% It is available at
% http://www.complex-systems.com/samples/complex-systems.sty
% epsf.sty is the preferred graphics import method

\begin{document}
\title{Data assimilation for sparsification of reaction diffusion systems in a complex network 
% 
% Use \\ to indicate line breaks in titles longer than about 
% 55 characters. 
%
}

\author{\authname{Abhishek Ajayakumar}\,(abhishekajay@iisc.ac.in)\\[2pt] 
% Use \\[2pt] to end the line and add space between author name and affiliation. 
\authadd{Department of Computational and Data Sciences, Indian Institute of Science}\\
\authadd{CV Raman Road}\\
\authadd{Bangalore, Karnataka-560012}\\
\and
% For extra space, precede the second set of authors with \and.
\authname{Soumyendu Raha}\,(raha@iisc.ac.in)\\
% Each author name goes on its own line.
% \authname{Third Author}\\[2pt]
\authadd{Department of Computational and Data Sciences, Indian Institute of Science}\\
\authadd{CV Raman Road}\\
\authadd{Bangalore, Karnataka-560012}
% Do not use a ``.'' at the end of any line in the address. 
}

% The following specifies the running headings 
%
% Each running heading should be less than about 50 characters long. 
% If necessary, give a shortened version of the title. 
%
% Use initials for first and second names. If all author names do not fit, truncate the 
% list and end with ``et al.''.
% \markboth{Complex Systems} 
% {Sample Paper for Complex Systems} 

\maketitle
% End title section

\begin{abstract}
The study focuses on complex networks that are underlying graphs with an embedded dynamical system. We aim to reduce the number of edges in the network while minimizing its impact on network dynamics. We present an algorithmic framework that produces sparse graphs meaning graphs with fewer edges on reaction-diffusion complex systems on undirected graphs. We formulate the sparsification problem as a data assimilation problem on a Reduced order model space(ROM) space along with constraints targeted towards preserving the eigenmodes of the Laplacian matrix under perturbations(L = D - A, where D is the diagonal matrix of degrees and A is the adjacency matrix of the graph). We propose approximations for finding the eigenvalues and eigenvectors of the Laplacian matrix subject to perturbations. We demonstrate the effectiveness of our approach on several real-world graphs.
\end{abstract}

\begin{keywords}
Graph sparsification; Reaction diffusion equation; Complex network; Data Assimilation; Proper Orthogonal Decomposition(POD) 
\end{keywords}

% The text of the paper follows. All of the text should be in the same file. 
% Use separate files for large tabular material and graphics.

\section{Introduction}
\label{intro}
% \label is a hyperlink target for cross-referencing to this section using \ref{intro} (optional).

Graph sparsification is an area of interest in mathematics and computer science. We define a graph $G = (V, E)$ where V and E denote the set of vertices and edges of the graph. A sparsifier of a graph aims at reducing the number of edges in the graph such that the new weighted graph $\bar{G} = (V, \bar{E}, \bar{w})$ achieves a specific objective, where $\bar{w}$ denotes the weight function, that is $w: V\times V \rightarrow \mathbb{R}$. E.g., In the case of spectral sparsifiers, we have

$$ \frac{x^T L x}{(1+\epsilon)} \leq \, x^T \bar{L} x \leq \,(1 + \epsilon) x^T L x \;\; \forall \,x \in \mathbb{R^n},\;\; \textrm{where}\; \vert V \vert = n,  \epsilon \in (0,1).$$

Kargers first introduced the notion of sparsification (\cite{karger_1994}, \cite{karger_1999}), producing graph skeletons which is a form of a cut sparsifier where every cut of the graph is an $\epsilon$ approximation to the original graph. There are other types of sparsifiers which preserve subgraph counts, like as in (\cite{ahn_guha_mcgregor_2012}). There is a much stronger notion of sparsifier than cut sparsifier called spectral sparsifiers where the Laplacian quadratic form is preserved like in (\cite{spielman_teng_2011}).
\par The notion of cut sparsification, which produces weighted subgraphs for which each cut is approximated, was introduced by Karger(\cite{karger_1994},\cite{karger_1999}). The idea involves sampling each edge of the graph with a probability p which results in a sparsifier of size O(pm), where $m = |E|$, the cardinality of the set of edges of the graph. An improved work for producing cut sparsifiers was based on the non-uniform sampling of edges as shown in (\cite{karger_2015},\cite{karger_1996}).
 

\par
Spielman and Teng introduced a much stronger notion of sparsification called spectral sparsifiers (\cite{spielman_teng_2004},\cite{spielman_teng_2011}). Spectral sparsifiers approximate the quadratic form of the graph Laplacian matrix. Such sparsifiers help in solving systems of linear equations(\cite{spielman_teng_2004},\cite{spielman_teng_2011}). The number of edges in the sparsified graph was O($n \log(n)^c$), where c is a large constant. Spielman and Srivastava ($\cite{spielman_srivastava_2008}$) in their work stated that every graph on n vertices has a sparsifier of O($\frac{n \log(n)}{\epsilon^2}$) edges and presented a linear algorithm for the same. Another work by Goyal, Rademacher and Vempala (\cite{goyal_rademacher_vempala_2009}) creates cut sparsifiers from random spanning trees. Batson, Spielman and Srivastava (\cite{batson_spielman_srivastava_2014}) construct sparsifiers with size O($\frac{n}{\epsilon^2}$) edges in O($\frac{m n^3}{\epsilon^2 }$) time using a determinstic algorithm. Fung and Harvey gave an algorithm which samples each edge with probability proportional to the inverse of its edge connectivity to produce cut sparsifiers(\cite{DBLP:journals/corr/abs-1005-0265}, they also describe an approach to produce cut sparsifiers from uniformly random spanning trees. A meta-learning-based approach for graph sparsification is discussed in (\cite{metalearning_sparsify}).
\par
The work in (\cite{cencetti_clusella_fanelli_2018}) discusses techniques to modify the network topology while preserving its dynamic behaviour; the new network obtained was isodynamic to the former. Kumar, Ajayakumar and Raha, in their work (\cite{KUMAR2021100948}), provide mathematical conditions for finding stable bi-partitions of a complex network and also provide a heuristic algorithm for the same. Graph partitioning doesn't alter the edge weights of graphs however can cause the deletion of edges, and it is shown in this work that constrained internal patch dynamics with sufficient network connectivity ensures stable dynamics around its co-existential equilibrium point.
\par  \textbf{Contributions of our paper:}
Techniques to rewire the edges of the graph while preserving isodynamic behaviour for complex systems have been discussed in (\cite{cencetti_clusella_fanelli_2018}). However, eliminating redundant edges of the graph while preserving the isodynamic behaviour of such complex systems using dimensionality reduction techniques like POD are the main contributions of our work. The problem of sparsification is approached as a Data assimilation problem with constraints. Decision variables are the unknown weights of the graph. The objective is to make this vector of weights as sparse as possible. If the perturbed Laplacian matrix has many eigenmodes similar to the eigenmodes of the given network, patterns produced in the new graph would be highly correlated to the patterns in the network(\cite{cencetti_clusella_fanelli_2018}).
Generating eigenvalues and eigenvectors at each iteration of the optimization process is computationally intensive, to overcome this we propose scalable approximations to the eigenvalue and eigenvector for the Laplacian matrix subject to perturbations. \\
\par The paper is structured as follows. We begin with a discussion of the motivation behind our work and provide some background information. We then provide a brief overview of reaction-diffusion systems on graphs. Next, we focus on the use of the Proper Orthogonal Decomposition (POD) method for dynamical systems and briefly introduce the Adjoint method for data assimilation, which can be applied to the parameter estimation problem. We compare the performance of POD in the following section and discuss the challenges involved in ensuring isodynamic behaviour. We also explore approximations of the eigenpair and their effectiveness in the next section, followed by the formulation of the optimization problem and its challenges. Finally, we present some results on real-world graphs to demonstrate the effectiveness of our approach.
\section{Motivation:}
Deep learning frameworks involving recurrent neural network decoders and convolutional neural networks are computationally intensive. Finding redundant edges in such networks and rescaling the weights can prove to be very much useful; one such work is done as in(\cite{https://doi.org/10.48550/arxiv.2211.04598}). Using such sparsified networks can help in reducing training time. Another line of thought involves learning differential equations from data(\cite{Raissi_2018}, \cite{Raissi_2018a}, \cite{lu_data}), where the data generation part is rendered easy by using such sparse neural networks. These types of work find applications in areas like fluid dynamics(\cite{fluidflow_arxiv}).
\section{Background}
\subsection{Laplacian matrix:($L$)}
The Laplacian of an undirected weighted graph G = (V,E), where $|V| = n, |E| = m$ with a weight function $w:V\times V\rightarrow \mathbb{R}$ is given by

$$
L \left(u,v\right)=\;\left\lbrace \begin{array}{ll}
d_v -w\left(u,v\right) & \mathrm{if}\;u=v,\\
-w\left(u,v\right) & \mathrm{if}\;u\;\mathrm{and}\;v\;\mathrm{are}\;\mathrm{adjacent},\\
0 & \mathrm{otherwise}\ldotp 
\end{array}\right.$$
Where $d_v = \sum_{u \sim v} w_{uv}$ denotes the degree of vertex v.
\subsection{Incidence Matrix:(B)}
The incidence matrix $B_{m \times n}$ with a given orientation of edges of graph is given by
$$
B\left(e,v\right)=\;\left\lbrace \begin{array}{ll}
1 & \mathrm{if}\;v\;\mathrm{is}\;e^{\prime } s\;\mathrm{head},\\
-1 & \mathrm{if}\;v\;\mathrm{is}\;e^{\prime } s\;\mathrm{tail},\\
0 & \mathrm{otherwise}\ldotp 
\end{array}\right.
$$
The graph Laplacian matrix for a given orientation can be expressed as
$$
 L = B^T W^{1/2} \textrm{diag}(\gamma) W^{1/2} B \quad \textrm{, where } \gamma \in \mathbb{R}^m \textrm{ represents the multipliers of the weights.}
\label{multipliers_weight}
$$
\section{Dynamical systems involving graphs}
\label{main-text}
A complex network with reaction-diffusion dynamics is shown as in (\cite{cencetti_clusella_fanelli_2018}) where the activity of node j at time t is described by an m-dimensional variable $r_j(t) \in \mathbb{R}^m$, starting from a specific initial state $r_j(0)$, the dynamics of $r_j$ evolves according to
$$ \frac{dr_j}{dt} = \mathcal{F}(r_j) + K \sum_{k=1}^{N} A_{jk} \mathcal{G}(r_k-r_j) \hspace{5mm} j = 1,2,....N$$
One such Reaction-diffusion system is given by the alternating self-dynamics Brusselator model where we couple the inter-patch dynamics using the graph Laplacian matrix,
\begin{equation}
    \left\lbrace \begin{array}{l}
\dot{x}_i \;=\;a-\left(b+d\right)x_i +c\;x_i^2 y_i -D_x \;\sum_j L {\;}_{\mathrm{ij}} x_j \\
{\dot{y}_i \;=\;\mathrm{bx}}_i -c\;x_{\;i}^2 y_i -D_y \;\sum_j L_{\mathrm{ij}} \;y_j \;\;\;\;\;\;\;
\end{array}\right.
\label{Rd dynamics}
\end{equation}
With $r_i = (x_i, y_i) \in \mathbb{R}^2$. The system (\ref{Rd dynamics}) has fixed points $r^{\star} = (x_i,y_i) = (\frac{a}{d}, \frac{bd}{ac}) \, \forall \, i$.
\break
\begin{figure}[h]
\includegraphics[width=8cm]{brusselator_dynamics.png}
\caption{Brusselator dynamics}
\centering
\end{figure}
\break
Our primary objective is to find a graph $\bar{G} = (V,\bar{E}, \bar{w})$ with a new sparse weight function $\bar{w}$ such that on solving (\ref{Rd dynamics}) using the updated graph $\bar{G}$ we get solutions $(\bar{x_i}, \bar{y_i}) \in B(x_i,\delta_i), \;B(y_i, \Delta_i) \;\forall \;i$, where $\delta_i$ and $\Delta_i$ are relatively small. Before approaching this problem, we will demonstrate the problem on a linear dynamical system involving the Laplacian matrix. The heat kernel matrix involving the Laplacian matrix is given as follows, \\
$$
H_t  = e^{-Lt}    
\label{kernel}    
$$

Differentiating (\ref{kernel}) with respect to time, we get $$\frac{dH_t}{dt} = -L H_t.$$
This system can be considered a system of odes of the form
\begin{equation}
    \hspace{25mm} \frac{d\mathcal{F}(x,t)}{dt} = -L \mathcal{F}(x,t)
    \label{Linear_ode}
\end{equation}  
According to the fundamental Theorem of linear systems, we know that the solution to such a system with initial condition $\mathcal{F}(x,0) = f(x)$ is $\mathcal{F}(x,t) = e^{-Lt} f(x)$ and this solution is unique. The underlying quantity which follows ode (\ref{Linear_ode}) could be heat, chemical reactants, ecological species etc. If we use an approximation for L, let it be $\bar{L}$ in the above equation with the same initial condition,
the solution turns out to be solutions to the ode of the form
\begin{equation}
\hspace{25mm}\frac{\partial \bar{\mathcal{F}}}{dt} = -\bar{L} \bar{\mathcal{F}} \label{lineardiffusion}   
\end{equation}

$\bar{\mathcal{F}}(x,t) = e^{-\bar{L}t}f(x)$. The error in this substitution at node x for a time t is given by $$H(x,t) = \mathcal{F}(x,t) - \bar{\mathcal{F}}(x,t),$$ $$T(x,t) = (\bar{L}t + \frac{(\bar{L}t)^2}{2!} + \cdot\cdot\cdot\cdot\cdot ) - (Lt + \frac{(Lt)^2}{2!} + \cdot\cdot\cdot\cdot\cdot ) .$$

\par Given some random initial conditions \{$\mathcal{F}_{0\alpha}\} \in \mathbb{R}^n, \alpha = 1,2 ....,\omega$, one could discretize system (\ref{Linear_ode}) to obtain solutions at various time points using a numerical scheme and the resulting discrete version of the dynamics can be represented as

\begin{equation}\hspace{5mm}
\bar{\mathcal{F}}^{\alpha}_{q+1} = M(\bar{\mathcal{F}}^{\alpha}_{q},\bar{w})    
\end{equation} \label{forward_eq}
where $M:\mathbb{R}^{n} \times \mathbb{R}^m \rightarrow \mathbb{R}^{n}$ 
with $M = (M_{1},M_{2},.....,M_{n})^T, M_{i} = M_{i}(\bar{\mathcal{F}}^{\alpha}_q,\bar{w})$ for $1\leq i \leq n$ and $ 0\leq q \leq S $, and $\bar{\mathcal{F}}^{\alpha}_{0} = \bar{\mathcal{F}}^{\alpha}\;$ where $ \bar{\mathcal{F}}^{\alpha} \in \mathbb{R}^n$ 
is the initial condition. 
The primary objective would be to minimize the error H as explained above, subject to the discrete dynamics (\ref{forward_eq}). 
This forms the core of the data assimilation problem. 
To know more about data assimilation, one could refer to (\cite{lewis_lakshmivarahan_dhall_2009}),
(\cite{sarkka})
or (\cite{law_stuart_zygalakis_2015}).


\subsection{Using Reduced order model for dynamical systems}
\label{ROMappro}
If the graph size is large, the size of the state vector will be large, and to promote computational feasibility, we use dimensionality reduction methods like POD, also known as  Karhunenâ€“Lo\`{e}ve as described in (\cite{rathinam_petzold_2003}). The procedure requires snapshots of solutions of a dynamical system with a vector field $f$ $\in \mathbb{R}^n$.
\begin{equation}\hspace{5mm}
\dot{x} = f(x,t)
\label{dynamicalPOD}
\end{equation}

Using the snapshot solutions, we could create a matrix $\rho$ of projection $\in \mathbb{R}^{k\times n}$ where k denotes the reduced order dimension and the mean $\bar{x}$ (please see \cite{rathinam_petzold_2003}) for more details. The reduced order model of the system (\ref{dynamicalPOD}) is then given by 

$$
     \dot{z} = \rho f(\rho^T z + \bar{x}, t).
$$
Thus if we are solving an initial value problem with  $x(0) = x_0$, then the reduced model one has the initial condition $z(0) = z_0,$ where 
$$
z_0 = \rho(x_0 - \bar{x})
$$
The reduced order model for system (\ref{lineardiffusion}) given the matrix $\rho$ of projection is 
$$\dot{z} = -\rho \bar{L}\rho^T z - \rho \bar{L} \bar{x}  $$
The reduced order model for system (\ref{Rd dynamics}) is obtained as follows. If a and b are vectors then $a \,\odot \, b$ represents element-wise multiplication and $a^{\circ n}$ raises every element of the vector a to power n. We make use of this notation below.

\vspace{3.5mm}
\begin{align*}
\dot{z} &= \mathbb{\psi}(z,\bar{\gamma})\\
&=\rho \;h(z) -D_x \;\rho(:,1:n)B^T W^{1/2}\mathrm{diag}(\bar{\gamma})W^{1/2}B(\rho^T(1:n,:)z+\bar{x}(1:n))\\
&-D_y\;\rho(:,n+1:2n)B^T W^{1/2}\mathrm{diag}(\bar{\gamma})W^{1/2}B(\rho^T(n+1:2n,:)z+\bar{x}(n+1:2n))
\end{align*}

\begin{align*}
h(z) &=\left\lbrack 
\begin{array}{c}
a \mathbb{1}_{n\times1} - (b+d)(\rho^T(1:n,:)z + \bar{x}(1:n))
+ c[(\rho^T(1:n,:)z + \bar{x}(1:n))^{\circ 2} ] \odot (\rho^T(n+1:2n,:)z  \\ + \bar{x}(n+1:2n)) \\
b(\rho^T(1:n,:)z + \hspace{6mm} \bar{x}(1:n)) - c((\rho^T(1:n,:)z + \bar{x}(1:n))^{\circ 2})\odot(\rho^T(n+1:2n,:)z + \bar{x}(n+1:2n))
\end{array}\right\rbrack 
\end{align*}
\begin{align*}
\nabla \;h_e {\left(z\right)}^T \bigg\vert_{e = 1,2,..,n} &=-\left(b+d\right)\;\rho {\;}^T \left(e,:\right)
+c\left({\left(\rho^T \left(e,:\right)z+\bar{x}_e \right)}^2 \right)\rho {\;}^T \left(n+e,:\right) \\ 
& + 2c\left(\rho^T \left(e,:\right)z+\bar{x}_e \right)\left(\rho^T \left(n+e,:\right)z + \bar{x}_{n+e}\;\right)\rho^T(e,:)
\end{align*}

\begin{align*}
\nabla{h_f(z)}^T \bigg\vert_{f = n+1,n+2,..,2n} &= b \rho^T(f-n,:)- c(\rho^T(f-n,:)z + \bar{x}_{f-n})^2 (\rho^T(f,:)) \\
&- \,2c(\rho^T(f-n,:)z + \bar{x}_{f-n})\,(\rho^T(f,:)z + \bar{x}_f)\,\rho^T(f-n,:)
\end{align*}
\begin{align*}
D_{z}\psi(z,\bar{\gamma}) &= \rho D_z h(z)- D_x \, \rho(:,1:n)B^T W^{1/2} \text{diag}(\bar{\gamma}) W^{1/2} B \rho^T(1:n,:)\\
&- D_y\,\rho(:,n+1:2n) B^TW^{1/2} \text{diag}(\bar{\gamma})W^{1/2} B \rho^T(n+1:2n,:) \\
D_{\bar{\gamma} } \psi (z,\bar{\gamma} )&=-D_x \rho (:,1:n)B^T W^{1/2} (\text{diag}(W^{1/2}B\rho^T (1:n,:)z + W^{1/2}B\bar{x} (1:n)))\\
&-D_y \;\rho (:,n+1:2n)B^T W^{1/2} (\text{diag}(W^{1/2}B\rho^T (n+1:2n,:)z + W^{1/2}B\bar{x} (n+1:2n)))
\end{align*}

Parameter estimation problems have a rich literature (\cite{1p}, 
\cite{caracotsios_stewart_1985},
\cite{3p},
\cite{4p}). 
Using Unscented and Extended Kalman filtering-based approaches are not recommended when constraints on parameters are imposed (\cite{EKf_mhe}). We use the Adjoint sensitivity method for the parameter estimation(\cite{lakshmivarahan_lewis_2010}). In the next section, we provide a brief overview of this method on how it could be used in the notion of parameter estimation.

\section{Adjoint Method for data assimilation}

\small\textbf{Statement of the Inverse Problem:} Given the a set of observations \{$\mathbf{x_k}$ $\vert$ $0\leq k\leq N$ \}, the problem is to estimate the control $\alpha$ that minimizes $J(\alpha)$ where $x_k
$ follows the relationship $x_k = M(x_{k-1}, \alpha)$.


Let $k \in \{0,1,2,..S\}$ denote the discrete-time variables respectively. Let $x \in \mathbb{R}^n$ with $x = (x_1,x_2,....,x_n)^T$. Let $F:\mathbb{R}^n \times \mathbb{R}^p \rightarrow \mathbb{R}^n$ such that $F = (F_1,F_2,......,F_n)^T$ where $F_i = F_i(x, \alpha)$ for $1\leq i \leq n,\; \alpha \in \mathbb{R}^p$. \newline
\textbf{Model} Consider the nonlinear system of the type
\begin{equation}
\frac{\partial x}{\partial t} = F(x, \alpha)
\label{IVPprob}
\end{equation}
 with $x(0) = c,$ being the initial condition which is assumed to be known, $\alpha$ is a system parameter assumed to be unknown. The vector $x(t)$ denotes the state of the system at time t, and $F(x, \alpha)$ represents the vector field at point x. If each component of $F_i$ of $F$ is continuously differentiable in x, the solution to the Initial value problem exists and is unique.
\newline \newline
The equation (\ref{IVPprob}) can be discretized using many schemes (\cite{burden_faires_burden_2016}) and the resulting discrete version of the dynamics can be represented as
\begin{equation*}
x_{k+1} = M(x_k, \alpha) 
\label{discretivp}
\end{equation*}
where $M:\mathbb{R}^n \times \mathbb{R}^p \rightarrow \mathbb{R}^n$ with $M = (M_1, M_2,.....,M_n)$, $M_i = M_i(x_k, \alpha)$ for $1 \leq i \leq n$, and $x_0 = c$ is the initial condition with an unknown $\alpha$.  \newline \newline

Let us define a function $J:\mathbb{R}^n \rightarrow \mathbb{R}$  as follows, (\cite{lewis_lakshmivarahan_dhall_2009})
\begin{equation*}
    J(\alpha) = \sum_{k=1}^N <(z_k - x_k), (z_k - x_k)>
\end{equation*}
\subsection{Adjoint sensitivity with respect to parameter $\alpha$:}
\begin{equation*}
\begin{array}{l}
\delta J(\alpha) \;= \sum_{k=1}^N <\eta_k, \delta x_k> \;\;   \;\text{\; for more details refer(\cite{lakshmivarahan_lewis_2010})}\\ \\
\;\;\;\;\;\;\;\;\;\;\;\;= \sum_{k=1}^N <\eta_k, V_k \delta \alpha>\\ \\
\;\;\;\;\;\;\;\;\;\;\;\;\;= <\sum_{k=1}^N V_k^T \eta_k, \delta \alpha>\\ \\
\;\;\;\;\;\;\;\;\;\;\;\;\;\; \text{where \;} V_k = A_{k-1}V_{k-1} + B_{k-1}
\end{array} 
\label{TLS}
\end{equation*}
$$A_{k-1} = D_{k-1}(M(x_{k-1},\alpha)) \in \mathbb{R}^{n\times n} \text{\;and\;} B_{k-1} = D_{\alpha}(M(x_{k-1},\alpha)) $$ 
The recursive relation focuses on finding the sensitivity of J with respect to the parameter $\alpha$. Iterating
(\ref{TLS}) we get $$ \begin{array}{l}
V_k = \sum_{j = 0}^{k-1} (\prod_{s = j+1}^{k-1} A_s) B_j \\ 
\end{array}$$
The above recursive relation deals with the product of sensitivity matrices.   
\newline
The first variation of J after certain calculations (see \cite{lewis_lakshmivarahan_dhall_2009}) is given by 
$$\delta J =  <\sum_{k=1}^N  \sum_{j = 0}^{k-1}  B_j^T(\prod_{s = k-1}^{j+1} A_s^T) \eta_k, \delta \alpha >, \;\; \text{where}\;\;\eta_k = x_k - z_k
$$
\label{first_var}
From first principle
$$\delta J = < \nabla_{\alpha} J(\alpha), \delta \alpha>$$
Comparing first principle with (\ref{first_var}), we get $\nabla_{\alpha} J(\alpha) = \sum_{k=1}^N  \sum_{j = 0}^{k-1}  B_j^T(\prod_{s = k-1}^{j+1} A_s^T) \eta_k$ 


\subsection{Computing the jacobian $D_{j-1} \left(M\right)$
}
The numerical solution of an ODE at timestep j+1($y_{j+1} \approx x_{j+1}$)  with slope function $\frac{dx}{dt} = f(t,x)$ given d slopes($k_d$), where $x \in \mathbb{R}^n$ and $f: \mathbb{R} \times \mathbb{R}^n \rightarrow \mathbb{R}^n$ is given by $y_{j+1} = y_{j} + b_1 k_{1} + b_2 k_2 + ....+ b_d k_d$.
Where $k_i = hf(t + c_ih, x + a_{i1}k_1 + a_{i2}k_2 + ... a_{in}k_n)$. To learn more about numerical analysis, one could refer to (\cite{burden_faires_burden_2016}).
\par We demonstrate computing $D_{j-1}(M)$ using an explicit numerical scheme, particularly the Euler method on the Lotka-Volterra model.

$$\left\lbrace \begin{array}{ll}
\begin{array}{l}
\frac{{\mathrm{dx}}_1 }{\mathrm{dt}\;}=\alpha x_1 - \beta x_1 x_2  \\ \\
\frac{{\mathrm{dx}}_2 }{\mathrm{dt}}=\delta x_1 x_2 - \gamma x_2 \\ \\
\end{array} & 
\end{array}\right.$$
Discretizing it using the standard forward Euler scheme, we obtain
$$ x_{k+1} = M(x_k)$$ \\
where $x_k = (x_{1k}, x_{2k})^T,\; \textbf{M}(x) = (M_1(x), M_2(x))^T$ where 
$$\begin{array}{l}
M_1 \left(x_k \right)=x_{1k} +\left(\Delta t\right) \left(\alpha\,x_{1k} -\beta\,x_{1k}x_{2k} \right) \\
M_2 \left(x_k \right)=x_{2k} +\left(\Delta t\right)(\delta\;x_{1k} x_{2k} -\gamma\,x_{2k}) 
\end{array} $$
It can be seen that 
$$
D_{k}(M) = A_k = \left\lbrack \begin{array}{cc}
1+\Delta t\;\left(\alpha -\beta \;x_{2k} \right) & -\Delta t\;\beta \;x_{1k} \\
\Delta t\;\delta \;x_{2k}  & 1+\Delta t\;\left(\delta \;x_{1k} -\gamma \;\right)
\end{array}\right\rbrack_{2\times2}
$$

\begin{equation*}
D_{\alpha}(M) = B_k =  
\left\lbrack \begin{array}{cccc}
\Delta t\;x_{1k}  & \Delta t\;\left(-x_{1k} x_{2k} \right) & 0 & 0\\
0 & 0 & -\Delta t\;x_{2k}  & \Delta t\;x_{1k} {\;x}_{2k} 
\end{array}\right\rbrack_{2\times4}   
\end{equation*}

\subsection{POD Method Performance}
(\cite{rathinam_petzold_2003}) Discusses the computational advantages when using POD on the general linear and non-linear dynamical system. We present the program execution times when evaluating the numerical solution as well as the jacobian matrix times vector product both with respect to the state and the parameter on the reduced order model of the system (\ref{Rd dynamics}).

\begin{figure}[htb]
\begin{minipage}{0.57\textwidth}
\centering
\includegraphics[width=\linewidth]{costfunc_all.png}
{\footnotesize ROM slope function vs regular slope function.}
\end{minipage}%
\hspace{12mm}
\begin{minipage}{0.57\textwidth}
\centering
\includegraphics[width=\linewidth]{jacobain_state.png}
{\footnotesize Jacobian with respect to state times vector product for ROM and regular model.}
\end{minipage}%
\hspace{2.5mm}
\begin{minipage}{0.57\textwidth}
\centering
\includegraphics[width=\linewidth]{jacobian_multipliers.png}
{\footnotesize Jacobian with respect to parameter times vector product for ROM and regular model.}
\end{minipage}
\hspace{12mm}
\begin{minipage}{0.57\textwidth}
\centering
\includegraphics[width=\linewidth]{net_time.png}
{\footnotesize Time taken for ROM versus regular model.}
\end{minipage}
\caption{\small Plots showing comparison of ROM vs regular model for random graphs}
\label{randomgraph_rom}
\end{figure}


\begin{figure}[htb]
\begin{minipage}{0.57\textwidth}
\centering
\includegraphics[width=\linewidth]{costfunc_real.png}
{\footnotesize ROM slope function vs regular slope function.}
\end{minipage}%
\hspace{12mm}
\begin{minipage}{0.57\textwidth}
\centering
\includegraphics[width=\linewidth]{jacobian_statereal.png}
{\footnotesize Jacobian with respect to state times vector product for ROM and regular model.}
\end{minipage}%
\hspace{2.5mm}
\begin{minipage}{0.57\textwidth}
\centering
\includegraphics[width=\linewidth]{jacobian_weightsreal.png}
{\footnotesize Jacobian with respect to parameter times vector product for ROM and regular model.}
\end{minipage}
\hspace{12mm}
\begin{minipage}{0.57\textwidth}
\centering
\includegraphics[width=\linewidth]{net_timereal.png}
{\footnotesize Time taken for ROM versus regular model.}
\end{minipage}
\caption{\small Plots showing comparison of ROM vs regular model for real world graphs}
\label{realgraph_rom}
\end{figure}
\subsubsection{Adjoint Method Algorithm:}
\label{Adjointalgo}
\begin{enumerate}
    \item Start with $x_0$ and compute the nonlinear trajectory $\{x_k\}_{k=1}^N$ using the model $x_{k+1} = M(x_k, \alpha)$
    \item $\lambda_N = \eta_N$
    \item Compute $\lambda_j = A_j^T \lambda_{j+1} + \bar{\eta_j}$ for j = N-1 to k, $\bar{\lambda}_k = B_{k-1}^T \lambda_k$, k = 1 to N, $\bar{\eta}_j = \delta_{j, j_i} \eta_j$ with $\delta_{j,j_i} = 1$ when $j = j_i$ , else 0 
    \item sum = \underline{0}, sum = sum +  $\bar{\lambda}_k$ vectors from k = 1 to N
    \item $\nabla_{\alpha}J(\alpha) = \text{sum}$
    \item Using a minimization algorithm find the optimal $\alpha^*$ by repeating steps 1 to 5 until convergence
\end{enumerate}
\vspace{15mm}
\section{Problem Statement:}
The underlying objective of this paper presents a framework on undirected graphs to obtain an edge sparsifier having less number of edges than that of the original graph which is also pattern invariant, meaning the sparse graph produces patterns which are similar to the original graph. The problem of rewiring is posed as a constrained optimization problem where we assimilate observations at various time points including a $\ell_2$ norm term in the objective function which indicates how the assimilated observations are deviating from the forward model coupled with $\ell_1$ norm term of the weight multipliers (\ref{multipliers_weight}) to induce sparsity. When the perturbed Laplacian matrix of a network has many eigenmodes that are similar to the eigenmodes of the original network, the resulting patterns in the new graph will be highly correlated with the patterns in the original network. Thus any patterns or behaviours that are induced in the original network are likely to also be present in the new graph. Computation of eigenmodes of the Laplacian matrix can be computationally intensive, so we propose approximations to eigenmodes described as in (\ref{estimate_eval}). The constrained optimization is approached by making use of the interior-point method. (\cite{nocedalbook}) discusses the combinatorial difficulty in solving a non-linear constrained optimization problem.       
\subsection{Sparsification Framework:}

\begin{enumerate}
    \item Get random snapshot solutions of the reaction-diffusion system, like as in (\ref{Rd dynamics}), using a proper numerical scheme with a random initial condition. One choice is random perturbation around the equilibrium point. Sample data points from the trajectory for assimilation as well as for the generation of the POD matrix($\rho$).
 \item Project the solution to the reduced dimension using the matrix $\rho$ (\ref{ROMappro}).
 \item An optimization problem is formulated as shown in (\ref{costfunc}) where the decision variables are the weight multipliers. The assimilated data points in ROM space are included in the objective function to allow permitted variation of state vector for the new network. The $\ell_1$ norm term of weight multipliers are included to induce sparsity of the multipliers. 
\item Using the Adjoint recursive relations on the reduced order model (ROM), we could obtain the objective function's gradient and the expression for the non-linear constraint is given in (\ref{nonlinear_constr_grad}). \item The constrained optimization problem is solved to get the optimal multipliers $\bar{\gamma}^*$ using methods as described in (\cite{nocedal}). The initial conditions for the state will be kept invariant, and the initial multipliers used are $\gamma^{(0)} = \mathbb{1}_{m\times1}$.
 \item Obtain the weight vector $\bar{w}^*$ from the multipliers ($\bar{\gamma}^*$). ($\bar{w}^* = \text{diag(w)} \bar{\gamma}^*$).
    \item Construct the sparse Laplacian matrix using the weight vector. 
\end{enumerate}

We are attempting to sparsify a graph by finding the unknown weight vector, represented as $\bar{w} = \text{diag}(w)\, \bar{\gamma}$. The goal is to make these multipliers sparse. The problem of graph sparsification involves finding the multipliers $\bar{\gamma}$ with the state vector following a reduced order model and the observations projected onto this space. The advantage of using a reduced order model in random graphs and real-world graphs are shown in figures (\ref{randomgraph_rom}) and (\ref{realgraph_rom}). 
In the next section, we discuss techniques to ensure isodynamic behaviour on the sparse graph and the challenges involved.

\subsection{Ensuring isodynamic behaviour}
The resulting sparsified graph should be isodynamic to the original graph. The study in (\cite{cencetti_clusella_fanelli_2018}) aims at producing a pattern invariant network using techniques like eigenmode randomization and local rewiring. The dispersion relation connects the stability of the system as in (\ref{Rd dynamics}) with the laplacian matrix eigenvalues. A graph with no unstable mode will have ( Re$(\lambda) < 0) \, \forall \, \alpha$. The contribution of the $\alpha$-th eigenvalue to the stability of the system is given by the eigenvalues of the following matrix,
$$J_{\alpha} = D_r\mathcal{F}(r^{\star}) - \left\lbrack \begin{array}{cc}
D_x \; & 0\\
0 & D_y 
\end{array}\right\rbrack \lambda {\;}^{(\alpha)} \; $$

\begin{figure}[htb]
\centering
\includegraphics[width=8cm]{stable_unstable.png}
\caption{Plot showing the stable and unstable eigenmodes of reaction-diffusion system(\ref{Rd dynamics}) on an Erdos-Renyi random graph}
\centering
\label{emodes_all}
\end{figure}
The above graph indicates that the reaction-diffusion system embedded in the graph has both stable and unstable modes. The methods mentioned in (\cite{cencetti_clusella_fanelli_2018}) act on less connected nodes of the graph, which corresponds to nodes having lesser connectivity. We pose the problem of rewiring as a constrained optimization problem using the cost function as described below with non-negativity constraints on the weights. Given an input graph which is connected we impose a minimum connectivity constraint so that the resulting graph we obtained is sparse as well as avoids isolated nodes. We ensure a minimum level of connectivity in the sparsified graph with the aim of making the resulting sparsified graph connected because if this condition is not ensured, then the number of zero eigenvalues will be equal to the number of disconnected components in the sparse graph obtained which in turn causes changes in the error function described as in (\cite{cencetti_clusella_fanelli_2018}); we impose this by making sure that the unsigned incidence matrix times the weight vector is greater than a required connectivity level, say $\gamma \mathbb{1}_{n\times 1}$ with gamma being a user-defined parameter. We demonstrate this using an example (\ref{example_connect}).
\begin{figure}[h]
\centering
\includegraphics[width=8cm]{Inc.png}
\caption{Demonstrating method of imposing connectivity constraints using unsigned incidence matrix. We consider an unweighted graph having 4 nodes and the multipliers are denoted by red. Here Q denotes the unsigned incidence matrix, weight vector w = diag($\mathbb{1}_{5\times 1})\gamma$, where $\gamma$ are the multipliers denoted by red. Degree vector $d = Q \gamma$.}
\centering
\label{example_connect}
\end{figure}

\par One could use the error function as employed in the local rewiring methodology as mentioned in (\cite{cencetti_clusella_fanelli_2018}) as a constraint to the optimization problem. In this case, even though the objective function is continuous, the error function imposed as a constraint will be generally non-smooth. Methods to approach such non-smooth optimization problems using global optimization algorithms like Genetic algorithms are discussed in (\cite{nocedalbook}). The error function($\zeta$) is defined as in (\cite{cencetti_clusella_fanelli_2018})
$$
\zeta = n\zeta_l + \zeta_q
\label{errorfunc}
$$
$$
\zeta_l = \frac{\sum_{i = 1}^{n_p} |\Tilde{\lambda}^{\alpha} - \lambda^{\alpha}|^2}{n_p}
$$
$$
\zeta_q = \frac{\sum_{i=1}^{n_p} |<\Tilde{\phi}^{\alpha}, \phi^{\alpha}> - 1|^2 }{n_p}
$$
Here $n_p$ denotes the number of unstable modes preserved; we accept the new graph if the error function $\zeta$ is less than a tolerance value. Computing the constraint $\zeta$ at every iteration of the optimization process is computationally expensive.
\begin{lemma}
(Sherman-Morrison formula). If A is a singular $n \times n$ matrix and v is a vector, then
$$
(A + vv^T)^{-1} = A^{-1} - \frac{A^{-1}vv^TA^{-1}}{1+v^TA^{-1}v}
$$
\end{lemma}

\begin{lemma}
(matrix determinant lemma) If A is nonsingular and v is a vector, then
$$
\text{det}(A + vv^T) = \text{det}(A)(1+v^TA^{-1}v)
$$
\label{MDL}
\end{lemma}
\subsubsection{Eigenvalue and Eigenvector computation for the Laplacian matrix under perturbations:}
We present here the challenge involved in finding the eigenpair during each iteration of the optimization procedure. If there is a change to only one of the edges of the graph, then one could use the approximate model described as in (\cite{batson_spielman_srivastava_2014}) making use of lemma 2 and lemma 1(\ref{MDL}). At iteration z of the algorithm, we will get a new graph with the weights modified according to the scale factor $\beta^z$. Let us define $D = \text{diag(}\beta^z - \mathbb{1}_{m \times 1}$) and $\Tilde{\lambda} = \mathrm{diag(} \lambda_2, \lambda_3,..,\lambda_n)$. We could then define the new graph $L^{'} = L + B^T W^{1/2} D W^{1/2}B.$ The characteristic polynomial is given by,
$$
\chi_{L^{'}}(\lambda, D) = \mathrm{det(} L^{'} - \lambda I) = \mathrm{det(} \phi \, \Tilde{\lambda}\, \phi^T + B^T W^{1/2} D W^{1/2}B - \lambda I) \; 
$$
\text{Using the property of determinants we get the following}
$$
\chi_{L^{'}}(\lambda, D) = \text{det(} \Sigma + U^T D U) \,\, \text{where}\,\, U^T = \phi^T B^T W^{1/2}
$$

$$
\Sigma = diag(1, \lambda_2 - \lambda,...., \lambda_n - \lambda)
$$

$$
\begin{array}{l}
\mathrm{ \chi_{L^{'}}(\lambda, D) =  det\bigg(}\left\lbrack \begin{array}{ccccc}
1 & 0 & \ldotp  & \ldotp  & 0\\
0 & u_2^T D\;u_2 +\lambda_2 -\lambda \; & \ldotp  & \ldotp  & u_2^T \;D\;u_n \\
\ldotp  & u_3^T D\;u_2  & \ldotp  & \ldotp  & u_3^T \;D\;u_n \\
\ldotp  & \ldotp  & \ldotp  & \ldotp  & \ldotp \\
0 & u_n^T \;D\;u_2  & \ldotp  & \ldotp  & u_n^T \;D\;u_n +\lambda_n -\lambda \;
\end{array}\right\rbrack \mathrm{\bigg)}\\
\\
\label{polynomialroots}
\mathrm{\chi_{L^{'}}(\lambda, D) = det\bigg(}\left\lbrack \begin{array}{cccc}
\mathrm{trace}\left(u_2 {\;u}_2^T \;D\right)+\lambda_2 -\lambda \; & \mathrm{trace}\left(u_2 \;u_3^T \;D\;\right) & \ldotp  & \mathrm{trace}\left(u_2 \;u_n^T \;D\right)\\
\mathrm{trace}\left(u_3 \;u_2^T \;D\right) & \mathrm{trace}\left(u_3 \;u_3^T \;D\right)+\lambda_3 -\lambda \; & \ldotp  & \mathrm{trace}\left(u_3 \;u_n^T \;D\right)\\
\ldotp  & \ldotp  & \ldotp  & \ldotp \\
\mathrm{trace}\left(u_n u_2^T \;D\right) & \mathrm{trace}\left(u_n \;u_3^T \;D\right) & \ldotp  & \mathrm{trace}\left(u_n \;u_n^T \;D\right)+\lambda_n -\lambda \;\;
\end{array}\right\rbrack \bigg) 
\end{array}
$$
Here $\lambda$ and D are variables, and finding the determinant of matrices with such variables or symbolic variables is described as in (\cite{https://doi.org/10.48550/arxiv.1304.4691}). We need to precompute only the diagonal elements of $u_i u^T_j$ to obtain the expression for the characteristic polynomial since D is a diagonal matrix. We focus on computing the largest roots of the polynomial. Finding all the roots of the polynomial at each iteration and filtering out the highest roots may be more computationally intensive than finding only extreme eigenvalues as in (\cite{lehoucq_sorensen_yang_1998}, \cite{doi:10.1137/S0895479800371529}).
\\
\par
\textbf{Example:}
\begin{figure}[h]
\hspace{35mm}
\includegraphics[width=3cm]{completegraph.png}
\caption{Complete graph $\mathbb{K}_3$}
\label{k3graph}
\end{figure}
Let us consider the case of a complete graph on 3 vertices $\mathbb{K}_3$ figure (\ref{k3graph}), the Laplacian matrix of this graph is given by 
$$
\;\;\;L = 
\begin{bmatrix}
2 & -1 & -1 \\
-1 & 2 & -1 \\
-1 & -1 & 2 \\
\end{bmatrix}
$$
The eigenvalues of the matrix L are $\lambda_1 = 0, \lambda_2 = 3, \lambda_3 = 3.$ Now let us pertub two of the weights of this graph and let the multiplier vector $\bar{\gamma} = [1.2, 1.4, 1]$. The matrix D will now be $\mathrm{diag(\bar{\gamma}} - \mathbb{1}_{3\times1}).$ The matrix $B^T$ is given as follows,
$$
\;\;\;B^T = 
\begin{bmatrix}
1 & 0 & -1 \\
-1 & 1 & 0 \\
0 & -1 & 1 \\
\end{bmatrix}
$$
The expression  (\ref{polynomialroots}) will then become
$p(\lambda) = \mathrm{det(}  \begin{bmatrix}
3.9374 - \lambda & -0.0786 \\
-0.0786& 3.2626 - \lambda 
\end{bmatrix}  )$. $p(\lambda) = \lambda^2 - 7.2 \lambda + 12.84.$ The roots of the polynomial p are given by 3.2536 and 3.9464 which are the eigenvalues of the matrix $L^{'}$.
\\
\\
The effect on eigenvalues of a matrix with a perturbation is studied in (\cite{bhatia_2007}). One such significant result in the field of perturbation theory is given below.
\begin{theorem}
(\cite{bhatia_2007}) If A and A + E are $n \times n$ symmetric matrices, then 
$$|\lambda_k (A + E) - \lambda_k (A) |\leq \vert \vert E \vert \vert_2$$
for $k = 1:n$
\end{theorem}
The above result could be used to approximate the eigenvalue term in the error function term(\ref{errorfunc}). However, the bound obtained is not tight.
\\
We propose an approximation for the eigenvalue and the eigenvector for the new Laplacian matrix based on the eigenvalues and eigenvectors of the initial Laplacian matrix making use of the method of Rayleigh iteration which produces an eigenvalue, eigenvector pair. \\
\textbf{Rayleigh Iteration:} $A \in \mathbb{R}^{n\times n}$ symmetric \\
$x_0$ given, $\vert \vert x_0 \vert \vert_2 = 1$ 
\begin{algorithmic}
\FOR{$i=0,1$ to ...}
\STATE $\mu_k = r(x_k)$ \, $(r(x) = \frac{x^T A x}{x^T x})$ 
\STATE Solve (A - $\mu_k \mathbb{I})z_{k+1} = x_k $ for $z_{k+1}$ 
\STATE $ x_{k+1} = z_{k+1}/\vert\vert z_{k+1} \vert\vert_2 $
\ENDFOR
\end{algorithmic}
Let us consider the following theorem which will be used later in section 6.2.2.
\begin{theorem}
Given an undirected graph $G = (V,E,W)$, with the Laplacian matrix $L_G$, then the matrix $L_G - (pI)$ is not orthogonal $\forall \,p \in \mathbb{R}$ if the minimum degree of the graph $d_{\mathrm{min}}(G) > 2.$
\label{Theo2}
\end{theorem}

\begin{proof}
We prove the above theorem by contradiction. For a matrix M to be orthogonal we require $M^T M = I$. Let $M = L_G - pI$, then the eigenvalue decomposition of the matrix $L_G = V \Lambda V^T$. Applying the definition of the orthogonality of matrices to the matrix M, we get $V(\Lambda - pI)^2 V^T = I$. This gives the requirement that p = 1, but even if that is the case, the remaining eigenvalues of the graph should be 2 but since the minimum degree of the graph is greater than 2 this cannot be the case(Greshgorin disks theorem \cite{lecture_datascience} ).    
\end{proof}
\subsubsection{Approximations to Eigenpair:}
The Rayleigh quotient iteration algorithm is a powerful tool for computing eigenvectors and eigenvalues of symmetric or Hermitian matrices. It has the property of cubical convergence, but this only holds when the initial vector is chosen to be sufficiently close to one of the eigenvectors of the matrix being analyzed. In other words, the algorithm will converge very quickly if the initial vector is chosen carefully. 
From the first step in Rayleigh iteration we get $\mu_0 = \lambda_{\alpha} + \epsilon$, if we consider the $\alpha$-th eigenvector $v_{\alpha}$ as the initial point $x_0$. The next step requires solving for $z_1$ in the linear system $(A+E-\mu_0 I) z_1 = v_0$. Conducting the sensitivity of the linear system with a singularity is difficult to determine and is not trivial. The solution to the system $(A - \mu_0 )z = v_{\alpha}$ is given by $-\frac{v_{\alpha}}{\epsilon}$. The first iterate using the conjugate gradient method would then be given by $y_{\alpha} = \frac{1}{\epsilon \;}\;\left(\frac{a^T a\;\;E v_{\alpha}}{a^T \left(A + E -\lambda_{\alpha \;} I\;-\epsilon I\;\right)a\;\;}-v_{\alpha \;} \right)$. We consider this iterate as our eigenvector approximation after normalizing. The eigenpair approximation is given below,  

$$(\hat{\lambda}_{\alpha}, \hat{x}_{\alpha}) = \bigg( r(y_{\alpha}), \frac{y_{\alpha}}{\vert\vert y_{\alpha} \vert\vert} \bigg)$$
In the next theorem, we propose bounds on the approximations of the eigenpair we proposed.
\begin{theorem}
\label{theorem_estimate}
Consider the Laplacian matrix of an undirected graph $L^{'} = L + E,\, E = B^T D B$ where $D$ is a diagonal matrix indicating perturbations. If we consider the estimate for the $\alpha-th$ eigenvector to be $\hat{x}_{\alpha} = \frac{y_{\alpha}}{\vert\vert y_{\alpha} \vert \vert_2}$, $y_{\alpha} = \frac{1}{\epsilon \;}\;\left(\frac{a^T a\;\;E v_{\alpha}}{a^T \left(A + E -\lambda_{\alpha \;} I\;-\epsilon I\;\right)a\;\;}-v_{\alpha \;} \right)$. Then $\vert \vert (L + E - r(y_{\alpha}) I \,) \hat{x}_{\alpha} \vert \vert_2 \leq  \tau  + \vert \vert E \vert \vert_2$, $\epsilon = v_{\alpha}^T E v_{\alpha}.$ Where $a = E v_{\alpha}$, $\tau = max(\vert \lambda_n - \lambda_{2}(L+E) \vert, \lambda_n(L+E))$. $\lambda_n \geq \lambda_{n-1} \geq...\geq \lambda_2 \geq \lambda_1$ denotes the eigen values of the Laplacian matrix of graph G.
\end{theorem}

\begin{proof}

$\vert \vert ((L+E) - \frac{y_{\alpha}^T (L+E)y_{\alpha}}{y_{\alpha}^T y_{\alpha}}I ) \hat{x}_{\alpha} \vert \vert = \vert \vert (V \Lambda V^T - \frac{y_{\alpha}^T (L+E) y_{\alpha}}{y_{\alpha}^T y_{\alpha}}I + E) \hat{x}_{\alpha} \vert \vert $ (since Laplacian matrix is diagonalizable and $L = V \Lambda V^T$).
\\
$$\vert \vert (V \Lambda V^T - \frac{y_{\alpha}^T (L+E) y_{\alpha}}{y_{\alpha}^T y_{\alpha}}I + E) \hat{x}_{\alpha} \vert \vert \leq  \vert \vert V (\Lambda - \frac{y_{\alpha}^T (L+E) y_{\alpha}}{y_{\alpha}^T y_{\alpha}}I) V^T + E \vert \vert \,\,  \vert \vert \hat{x}_{\alpha} \vert \vert = \vert \vert V (\Lambda - \frac{y_{\alpha}^T (L+E) y_{\alpha}}{y_{\alpha}^T y_{\alpha}}I) V^T + E \vert \vert $$
Applying the triangular inequality of matrix norms, the above inequality becomes 
$$ \leq 
\vert \vert V (\Lambda - \frac{y_{\alpha}^T (L+E) y_{\alpha}}{y_{\alpha}^T y_{\alpha}}I) V^T \vert \vert + \vert \vert E \vert 
\vert \leq  \tau  + \vert \vert E \vert \vert
$$
It can be observed that $\mathbb{1}^T y_{\alpha} = 0$ and from the definition, we have $\lambda_k(L) = \mathrm{inf_{x \perp P_{k-1}} \,}\frac{x^T L x }{x^T x}$ where $P_{k-1}$ is the subspace generated by eigenvectors of the Laplacian matrix L given by $\mathrm{span(}v_1,v_2,...,v_{k-1}).$ Thus the fielder value or the second smallest eigen value $\lambda_2$ of the Laplacian matrix is given by $\lambda_2 = \mathrm{inf_{x \perp \mathbb{1}}} \frac{x^T L x}{x^T x}$. Thus the above inequality becomes 
$$ \leq \tau  + \vert \vert E \vert \vert \mathrm{\,\,Where \,\, \tau = max(\vert \lambda_n - \lambda_{2}(L+E)\vert, \lambda_n(L+E))}
$$
\end{proof}
The error function using the estimate would then be 
$$
\boxed{\bar{\zeta} = n \frac{\sum_{i=1}^{n_p} \vert \hat{\lambda}^{i} - \lambda^{i} \vert^2}{n_p} + \frac{\sum_{i=1}^{n_p} \vert <\hat{x}^{i}, \phi^{i}> - 1 \vert^2 }{n_p}}
\label{estimate_eval}
$$
From (\cite{reddy_trefethen_1990}), the following \textbf{definition} apply to pseudo spectra of matrices. \\
If A is an $N \times N$ matrix and for $\lambda$ to be an $\epsilon-$pseudo-eigenvalue of A is 
$$ \sigma_{N}(\lambda I - A) \leq \epsilon$$ where $\sigma_N$ denotes the smallest singular value. From the above definition, we can see that our estimate $\hat{\lambda}^{\alpha}$ is a $\tau + \vert\vert E \vert\vert_2$-\textbf{pseudo eigenvalue} of L. 
\\
We modify the constraint $\zeta \leq \beta_1$ by $\bar{\zeta} \leq k_1 \beta_1$, where $k_1$ and $\beta_1$ are user defined constants. \\
We present numerical experiments on certain graphs and for every graph we take the number of eigenmodes to preserve the number of unstable eigenmodes. Random perturbation is given to every edge of the graph and a comparison is between $\zeta$ and $\bar{\zeta}$ (\ref{tab:my_label}). 

\par Figures showing the effectiveness of the approximate eigenpair in preserving the eigenvalues and eigenvectors are described in figures (\ref{image-eval1}). 
\newline
\subsubsection*{Points of discontinuity for the function $\bar{\zeta}$. }
Discontinuities could arise if \\
\textbf{Case 1:}\, $\phi_{\alpha}^T E \phi_{\alpha} = 0$.
\\
\textbf{Case 2:}\, $a^T(L + E - \lambda_{\alpha}I - \epsilon I) a = 0$ where $a = E \phi_{\alpha} \neq \mathbb{0}_{n\times1}$. \\
If we consider the first case, we can see that one possibility is when E is a rotation matrix. However matrix E is not orthogonal $(\text{det}(E) = 0)$. The other possibility is when $\phi_{\alpha}\, \in$ nullspace($E$). If this happens, then we could see that $(\lambda_{\alpha},\phi_{\alpha})$ is an eigenvalue, eigenvector pair for the new Laplacian matrix. Possible candidates of $\gamma$ satisfying this condition are given by the solution to the under-determined linear system under non-negativity constraints.

$$
 R \gamma = \lambda_{\alpha} \phi_{\alpha}, \, \mathrm{where\,\,} R = B^T W^{1/2} \mathrm{diag(W^{1/2}B\phi_{\alpha})}
$$
$$
\gamma \geq 0
$$
\\
Case 2 occurs when $ a \neq \mathbb{0}_{n\times1}\,\text{and}\, a^T (\Tilde{L} - (\lambda_{\alpha} + \phi_{\alpha}^T E \phi_{\alpha})I) a = 0.$ One possibility is that  $M_{\alpha} = (\Tilde{L} - (\lambda_{\alpha} + \phi_{\alpha}^T E \phi_{\alpha})I)$ needs to be a rotation matrix, but this case cannot happen if we impose a minimum degree constraint as mentioned in (\ref{Theo2}).
Elements in the Nullspace of matrix $M_{\alpha}$ under non-negativity constraints are candidates for discontinuity of the function $\bar{\zeta}$. We modify the eigenpair based on the following discontinuities. \\
\[
\label{errorfunctionestimate}
(\tilde{\lambda}_{\alpha}, \tilde{\phi}_{\alpha}) = 
\begin{cases}
  (\lambda_{\alpha}, \phi_{\alpha}), & \text{if } E \phi_{\alpha} = 0_{n \times 1} \\
  (\frac{a^T M_{\alpha} a}{a^T a}, \phi_{\alpha}), & \text{if } y_{\alpha} = 0_{n \times 1} \text{\;and\;} E \phi_{\alpha} \neq 0_{n \times 1}\\
  (\lambda_{\alpha} + \phi_{\alpha}^T E \phi_{\alpha}, \frac{a}{\vert\vert a \vert \vert}), & \text{if } M_{\alpha} a = 0_{n\times 1} \text{\; and\;} y_{\alpha}, E \phi_{\alpha} \neq 0_{n\times 1} \text{\; and \;} \vert\vert y_{\alpha} \vert\vert < \infty \\
  (\hat{\lambda}_{\alpha}, \hat{x}_{\alpha}), &  \text{otherwise} 
\end{cases}
\]
The error function now is modified as 
$$
\bar{\zeta}(\bar{\gamma}) = \frac{n}{n_p} \sum_{\alpha=1}^{n_p} (\tilde{\lambda}_{\alpha} - \lambda_{\alpha})^2 + \frac{1}{n_p} \sum_{\alpha=1}^{n_p} (<\tilde{\phi_{\alpha}}, \phi_{\alpha}> - 1)^2$$
The summation terms could be computed in parallel by making use of multiple cores of the machine for large graphs. Some metrics when evaluating this function in parallel are shown in table (\ref{tab:my_label}) and figure (10).
\pagebreak
\subsection*{Gradient of error function $\bar{\zeta}$}
\label{nonlinear_constr_grad}
$$\frac{d \bar{\zeta}}{d \gamma} = \frac{2n}{n_p} \sum_{\alpha = 1}^{n_p}  (\tilde{\lambda}_{\alpha} - \lambda_{\alpha}) \frac{d \tilde{\lambda}_{\alpha}}{d \gamma} + \frac{2}{n_p} \sum_{\alpha = 1}^{n_p} (<\tilde{\phi}_{\alpha}, \phi_{\alpha}> - 1) \hspace{2mm} (D_{\gamma} \tilde{\phi}_{\alpha})^T \phi_{\alpha}$$

$$
\hat{\lambda}^{\alpha} = \frac{y(\gamma)^T \Tilde{L} y(\gamma)}{y(\gamma)^T y(\gamma)}
$$
$$
\frac{d \hat{\lambda}^{\alpha}}{d \gamma} = \frac{y(\gamma)^T y(\gamma) \hspace{2mm} \nabla{y(\gamma)^T \Tilde{L} y(\gamma)} - (y(\gamma)^T \Tilde{L} y(\gamma)) \; \nabla{y(\gamma)^T y(\gamma)}}{(y(\gamma)^T y(\gamma))^2}
$$

$$
\nabla_{\gamma}{y(\gamma)^T \Tilde{L} y(\gamma)} = 2 D_{\gamma} (B_1(\gamma))^T B_1(\gamma) 
$$
$$
B_1(\gamma) = \gamma^{1/2} W^{1/2} B y(\gamma)
$$
$$
\nabla{y(\gamma)^T y(\gamma)} = 2 (D_{\gamma}y(\gamma))^T y(\gamma)
$$
$$
\hat{x}^{\alpha} = \frac{y(\gamma)}{\vert\vert y(\gamma)\vert\vert}, \;\nabla \left( \frac{y_i(\gamma)}{\vert\vert y \vert\vert}\right) = \left(\frac{\nabla y_i(\gamma)}{\vert\vert y \vert\vert}  - \frac{y_i(\gamma)\,D_{\gamma}y(\gamma)^Ty(\gamma)}{\vert\vert y \vert\vert^3 }\right)
$$
\begin{table}[]
    \centering
    \begin{tabular}{ 
|p{3cm}|p{2cm}|p{2cm}|p{2cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|  }
\hline
\multicolumn{3}{|c|}{Table} && \multicolumn{3}{|c|}{}\\
\hline
\textbf{Graph} & n & $n_p$ & m & Relative error $\frac{(\zeta - \bar{\zeta})}{\zeta}$& speedup \;\; (serial) & speedup \;\;(parallel) \\
\hline
complex  & 1408 & 282 & 46326 & 0.0013
  & 1.5532 & 0.9061 
\\
\hline
145bit  & 1002 & 201 & 11251 & 0.0122 &  3.2414 & 0.4946 \\
\hline
192bit  & 2600 & 520 & 35880 & 0.0362 & 13.7567 & 11.9106 \\
\hline
130bit & 584 & 117 & 6058 & 0.0088 & 2.9379 & 0.1888 \\
\hline
Ca-HepPh & 11204 & 2241 & 117619 & 0.0079
 & 27.4243 & 77.1951\\
 \hline
 Ca-HepTh & 8638 & 1728 & 24806 & 0.0341 & 83.6472 & 184.8210 \\
\hline
\end{tabular}
    \caption{Table showing comparison of $\zeta$ and $\bar{\zeta}$ for real world graphs with random perturbation to the edges.}
    \label{tab:my_label}
\end{table}
\label{estimatetable}


\section{Optimization Problem:}
\label{costfunc}
The task of sparsification can now be viewed as an optimization problem. Ideally, we like to make the multipliers as sparse as possible, so we include a term involving the $\ell_1$ norm of multipliers in our objective function. Optimization using $\ell_1$ term generally induces sparsity. The first term in the objective function minimizes the difference between the observations on a reduced order model space obtained from various trajectories at various time points and on the state obtained at these times using the reduced order model with the weights $\bar{w} = \text{diag(w)} \bar{\gamma}$.  Here $F^c(t_j)$ represents the observation taken from trajectory c at time $t_j$. Problem constants $\alpha, \tau, k_1, \beta_1$ are user defined and $Q^{'} = Q \,\text{diag(w)}$, where Q represents the unsigned incidence matrix of the graph(\ref{example_connect}). $A^c$ contains indices of observations assimilation from trajectory c. $\bar{F}^c$ follows the forward model $M$. The initial conditions which was used to get observations $F^c$ will be kept invariant.
$$
   \textbf{minimize} \,\, J(\bar{\gamma}) = \frac{\sum_{c =1}^{\omega}
      \sum_{j \in A^c} (F^{c}(t_j )  -\bar{F}^{c}(t_j) \;)^T (F^{c}(t_j )  -\bar{F}^{c}(t_j) \;) }{2} \;+\; \color{blue} \frac{\alpha}{2} \sum_{i=1}^m \vert \bar{\gamma}_i \vert 
$$ 
\begin{center}
\;\;sub to    $\bar{\mathcal{F}}^{c}_{q+1} = M(\bar{\mathcal{F}}^{c}_{q},\bar{\gamma})$,\;\;\;c = 1,...,$\omega$, q = 0,1,...,T-1 with $\bar{F}^c_0 = F^c_0$\\
$Q^{'} \bar{\gamma} \geq \tau$ \\
$\bar{\gamma}_j \geq 0$, j = 1,2,...,m \\
$\bar{\zeta}(\bar{\gamma}) \leq k_1\, \beta_1$
\end{center}
 \par The first variation of J is given by $$\delta J  = \sum_{c= 1}^{\omega} \sum_{j \in A_{c}} < (V_j^c)^T \eta_j^c,\delta\bar{\gamma}> + \textcolor{blue}{ \frac{\alpha}{2} <(\frac{\vert \bar{\gamma}_1 \vert}{\bar{\gamma}_1},..., \frac{\vert \bar{\gamma}_i \vert}{\bar{\gamma}_i},..,\frac{\vert \bar{\gamma}_n \vert}{\bar{\gamma}_n}),\delta \bar{\gamma}>}\, \text{(Where}\, V_j\,\text{is defined as in }\ref{first_var}) $$ 
 \begin{figure}[h]
\includegraphics[width=13cm]{pertub_image.png}
\caption{Data Assimilation approach }
\centering
\end{figure}
The primary focus of our optimization routine is to minimize the function J as described above. The gradient of J is obtained using the adjoint method algorithm as mentioned in (\ref{Adjointalgo}). The model used for forward dynamics is made in reduced order to reduce computational complexity.
\par If we consider the optimization problem without the non-linear constraint, we can observe that the search space for our objective function described in (\ref{costfunc}) is closed and the objective function and the constraints are continuous. So we have an optimization problem as described
$$\mathrm{min_{x \in X}} \; f(x) $$
$$\mathrm{sub \; to\;\;} h_j(x) \leq 0\,\; j = 1,2,...n$$
Set X is closed where functions $f$ and $h_j$ are continuous. We could use the Barrier methods for optimization as described in (\cite{nocedalbook},\cite{luenberger2021}). The feasibility set $S = X \cap \{x \,|\, h_j(x) \leq 0 \}$. If we generate a decreasing sequence $\epsilon_k$, let us denote $x_k$ as a solution to the problem $\mathrm{min_{x \in S}} \,\,f(x) + \epsilon_k B(x)$. Then every limit point of $\{x_k\}$ is a solution to the problem. One selection of barrier function $B(x) = -\sum_{j=1}^n \log(-h_j(x))$(Barrier convergence theorem). However because of the discontinuities in the non-linear constraint, we could not guarantee such a convergence. Heuristic algorithms for global optimization (\cite{nocedalbook}) like Genetic algorithms, Pattern search algorithms exist however require multiple function evaluations. Since the computation of the objective function in the optimization problem(\ref{costfunc}) is computationally expensive, we don't prefer such methods. 
\begin{algorithmic}[1] % Start line numbering
\STATE \textbf{ALGORITHM:}
\STATE \textbf{INPUT:} An undirected Graph $G = (V,E,w)$, $\alpha$, $\tau_{\vert V \vert \times 1}$, $\epsilon$, $\omega$, $p_1, p_2,\dotsc,p_N,k$ (dimensionality reduction parameter), $n_p,\,k_1$, $\beta_1$, $F^1_0, F^2_0,....., F^N_0$ and N(number of trajectories)
\STATE \textbf{OUTPUT:} Sparsified graph $\Tilde{G} = (V,\Tilde{E},\Tilde{w})$
\STATE Run the forward reaction-diffusion model as in (\ref{Rd dynamics}) with initial conditions $F^1_0, F^2_0,.....,F^N_0$ and sample $p_i$ number of points in the interval $[0, T]$, $i = 1,2,\dotsc, N$ for assimilation, Sample data points at regular intervals for each trajectory thus obtaining the snapshot matrix containing $N$ trajectories for generating the matrix $\rho$ of projection.
\STATE Obtain the matrix $\rho$ of projection and mean $\bar{x}$ as mentioned in (\ref{ROMappro}), selecting the $k$ largest eigenvectors of the covariance matrix.
\STATE From the $N$ trajectories, select $\omega \leq N$ trajectories for the data assimilation correction part.
\STATE Using the sampled trajectories along with the procedure followed in (\ref{costfunc}), run the minimization procedure to obtain the weight multipliers of the sparsified graph using the $\alpha$, $n_p$, and $\tau$ values.
\STATE On obtaining the multipliers, remove multipliers that are less than $\epsilon$ to obtain the new multiplier $\gamma'$.
Construct the weights $\bar{w}$ using the multiplier $\gamma'$, $\bar{w} = \text{diag(w)} \gamma^{'}$.
\STATE Construct $\Tilde{G}$ using the weights $\bar{w}$.
%\STATE \textbf{OUTPUT:} $\Tilde{G}$
\end{algorithmic}


\subsection*{Running time of the procedure:}
The time-consuming step in our sparsification procedure is the constrained optimization part(step 4) of our algorithm. For every iteration of the optimization procedure, we need to compute the projected slope function involving the Laplacian matrix takes O(k$|V|^2$) operations. Evaluation of the cost function and its gradient takes $O(k\vert E \vert)$.
\par The nonlinear constraint evaluation and the gradient evaluation take O($|V|^2 n_p$) and O($|E|^2 n_p$) operations respectively. (\cite{nocedal}) Describes the challenges involved in solving a nonlinear constrained optimization problem. The algorithm took polynomial time to converge in many Linear programming problems. We exploit parallelism in the nonlinear constraint evaluation steps.
\begin{figure}[htb]
\begin{minipage}{0.5\textwidth}
\centering
\includegraphics[width=\linewidth]{time_costfunc.png}
\caption{Figure showing the time complexity of cost function.}
\end{minipage}%
\hspace{0.5cm}
\begin{minipage}{0.5\textwidth}
\centering
\includegraphics[width=\linewidth]{nonlinear_complexity.png}
\caption{Figure showing time complexity of the non-linear constraint.}
\end{minipage}
\end{figure}

\begin{figure}[h!]
\centering
\begin{minipage}{0.55\textwidth}
\includegraphics[width=\linewidth]{speedup.png}
\caption{Speedup obtained for non-linear constraint evaluation.}
\end{minipage}%
\label{speedup_figure}
\end{figure}
\pagebreak
\section{Results:}
We apply our techniques to a number of real-world graphs (\cite{nr-aaai15},\cite{snapnets}). The parameters were assigned as follows $\alpha = 16 \log(\vert E \vert)$, $p_1, p_2 $ = 50, $D_x = 0.1$, $D_y = 1.4$, N = 2, $\omega = 2$, $k_1 = 1$, $\beta_1 = 10^{-2}$, $\tau = \text{max(2,\,}0.1\,d_{\mathrm{min}})\, \mathbb{1}_{n\times 1}\, $,\,$ \epsilon = 10^{-2}$ and $n_p = n / 5$, where $d_{\mathrm{min}}$ denotes the minimum degree of the graph and we include the $n_p$ largest eigenvalues of the Laplacian matrix in the non-linear constraint evaluation step. The tabulated results on some real-world graphs are mentioned below. All the simulations were made in MATLAB and we are using the interior point method for constrained optimization as described in(\cite{nocedal}). The squared Pearson correlation coefficient is found by computing the average modulus of the solution at each node i\,($|r_i|$) for a random perturbation of the fixed point for the original graph and the sparsified graph.  \\
\par Quality of the approximations proposed on the eigenpair (\ref{errorfunctionestimate}) are shown in figures(11). The approximate eigenvalues seem to be well within the permissible bounds.       
\\
\label{final-result-table}
\begin{align*}
\begin{tabular}{
|p{3cm}|p{1.5cm}|p{2cm}|p{3cm}|p{1.5cm}|p{2cm}|  }
\hline
\multicolumn{3}{|c|}{Table} & \multicolumn{3}{|c|}{}\\
\hline
\textbf{Graph} & n  & $|E|$ & Number of eigenmodes computed ($n_p$) & $|E^{'}|$ & Squared Pearsons correlation-coefficient($R^2$) \\
\hline 
complex  & 1408 & 46326 & 282 & 39926 & 0.74164\\
\hline 
145bit  & 1002 & 11251 & 201 & 7767 & 0.9039 \\
\hline 
192bit & 2600  & 35880 & 520 & 35880  & 0.9091\\
\hline 
130bit  & 584 & 6058 & 117 & 4350  & 0.9123\\
\hline 
ca-HepTh  & 8638  & 24806 & 1728 & 22709 & 0.9626
\\
\hline
\end{tabular}
\end{align*}
 \label{image-eval1}
 \begin{figure}[htbp]
    \centering
    \includegraphics[width=1.4\textwidth]{./images_evals/complex.png}
    {\textbf{complex}
\hspace*{2cm} 11.1(a)\hspace*{5cm}11.1(b)\hspace*{3cm}11.1(c)
}
   
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.4\textwidth]{./images_evals/130bit.png}
    {\textbf{130bit}
\hspace*{2cm} 11.2(a)\hspace*{5cm}11.2(b)\hspace*{3cm}11.2(c)
}
    \label{fig:image2}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.4\textwidth]{./images_evals/145bit.png}
    {\textbf{145bit}
\hspace*{2cm} 11.3(a)\hspace*{5cm}11.3(b)\hspace*{3cm}11.3(c)
}
    \label{fig:image3}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.4\textwidth]{./images_evals/192bit.png}
      {\textbf{192bit}
\hspace*{2cm} 11.4(a)\hspace*{5cm}11.4(b)\hspace*{3cm}11.4(c)
}
    \label{fig:image4}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.4\textwidth]{./images_evals/CA-HepTh.png}
   {\textbf{CA-HepTh}
\hspace*{1cm} 11.5(a)\hspace*{5cm}11.5(b)\hspace*{3cm}11.5(c) \\ 
    
Figures 11.1-5(a) shows the relative changes in the eigenvalues of the Laplacian matrix of the sparsifier($\frac{\tilde{\lambda}^{\alpha} - \lambda^{\alpha} }{\lambda^{\alpha}}$). Figures 11.1-5(b) plot showing the first two coordinates of all the eigenvectors of the Laplacian matrix of the graph in the x and y-axis. Figures 11.1-5(c) plot showing the first two coordinates of all the eigenvectors of the Laplacian matrix of the sparsifier in the x and y-axis.}
    \label{fig:image5}
\end{figure}
\begin{figure}[htbp]
\begin{minipage}[b]{1.4\linewidth}
\centering
\includegraphics[width=\linewidth]{./images_evals/result.png}
{Figure showing an Erdos-Renyi 50 node graph on the left and its sparsifier on the right.}
\end{minipage}
\hspace{0.05\linewidth}
\begin{minipage}[b]{1.4\linewidth}
\centering
\includegraphics[width=\linewidth]{./images_evals/errorbars.png}
{Solution when reaction-diffusion dynamics (\ref{Rd dynamics}) is cast on both the graphs.}
\end{minipage}

\end{figure}
\section*{Acknowledgments}
This work was partially supported by the MATRIX grant MTR/2020/000186 of the Science and Engineering Research Board of India.

\bibliographystyle{plain}
\bibliography{bibfile}

\end{document}
