\section{Kernel Launcher}

In this section, we describe the implementation of \emph{Kernel Launcher}, which is implemented as a C++ library.
Figure~\ref{fig:kernel-launcher} shows how the library is integrated into applications and interacts with Kernel Tuner. 
The remainder of this section explains how Kernel Launcher is utilized within applications, and outlines each of the four steps shown in Figure~\ref{fig:kernel-launcher}. 
First, the user writes the \emph{kernel definition} using Kernel Launcher in C++ to allows kernels to be \emph{captured}. 
Then, Kernel Tuner tunes the kernels, producing wisdom files. These files allow Kernel Launcher to perform \emph{dynamic kernel selection} using \emph{runtime kernel compilation}.

\subsection{Kernel Definition}

To make a CUDA kernel tunable, the programmer defines its specifications using the Kernel Launcher API. 
These specifications consist of three elements: The configuration space, the compilation specifications, and the launch parameters.

For the configuration space, the definition includes the tunable parameters, the allowable values for those parameters, and any constraints on the search space (i.e., boolean expressions).
For the compilation specifications, the programmer must provide details such as the source code, kernel name, compiler flags, template arguments, and preprocessor definitions. 
To launch the kernel, the programmer must specify how the thread block size, the number of thread blocks, and the amount of shared memory are derived from the kernel arguments. 


Once a kernel has been defined, it can be launched using the Kernel Launcher API by providing the kernel arguments in way that is similar to launching a regular kernel in plain CUDA (See Listing~\ref{fig:kernel-launcher-example},Line~\ref{line:launch}).
If the kernel has not yet been tuned, the default values for the tunable parameters are used.

Kernel Launcher thus consolidates the description of the tunable aspects of kernels and the code for launching the kernel, eliminating duplication between the Kernel Tuner script and hos code of application and moving this definition into the source code of the host application. 
%
Previously, with Kernel Tuner, the definition of the tunable kernel and its parameters resided in separate Python files, and the relationship between the problem size and tunable parameters was described in both the Kernel Tuner script and the host application.
%
This distribution of the kernel definition across multiple files led increased maintenance costs since all files need to be kept up to date when changes are made to the kernel source code. 
However, with Kernel Launcher, the kernel definition and its launch code are integrated in the source code of the host application, resulting in significant maintenance cost savings for C++ applications with many tunable kernels.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{fig/kernel-launcher-overview.png}
    \caption{Overview of the designed interaction between the host application, Kernel Launcher, and Kernel Tuner.}\label{fig:kernel-launcher}
\end{figure}

\subsection{Kernel Capturing}
An important concept in Kernel Launcher is the ability to \emph{capture} a kernel launch, which enables storage of all information necessary to execute the kernel, including the kernel definition and possible the input data.
By capturing a kernel, auto-tuning tools can `replay' the exact same kernel launch for different configurations of the tunable parameters. 
%This capture mechanism ensures that the kernel execution uses the same data as was used by the application at runtime.

There are two main advantages of this capturing concept.
First, it removes the need for programmers to have to generate input data when tuning kernels and, instead, making it possible to tune the kernel directly on real-world data extracted from the application. 
This approach can be particularly useful for complex data sets that are difficult to generate off-line.
%Instead, the kernel can be tuned directly on real-world data extracted from the application.
Second, it enables offline tuning instead of having to tune at runtime.
Offline tuning means that more accurate measurements can be obtained since the kernel is executed in isolation.
In contract, runtime tuning could lead to inconsistent measurement if, for example, the GPU also executing other kernels concurrently or the CPU is overloaded.

To capture kernel launches in our implementation, the programmer needs to set the environment variable \verb=KERNEL_LAUNCHER_CAPTURE= to a list of kernel names separated by commas.
It is possible to capture multiple kernels in a single run of the application.


\subsection{Kernel Tuning}

After capturing a kernel launch, the associated kernel can be tuned.
Kernel Launcher includes a command-line script that, in turn, uses Kernel Tuner~\cite{kerneltuner} as the main tool for auto-tuning, although in principle, other GPU auto-tuners, such as KTT~\cite{filipovivc2022using}, could be used as well.

Several command-line options can be provided to the script, such the search strategy and the termination condition. 
The default setting is to tune each kernel for at most 15 minutes using the Bayesian Optimization search strategy~\cite{willemsen2021bayesian}.


\subsection{Wisdom Files}
After the auto-tuner terminates, the best-performing configuration is written to a human-readable file which we referred to as \emph{wisdom files}.
The term ``wisdom'' was originally introduced by FFTW~\cite{frigo1998fftw}, but Kernel Launcher borrows only the terminology and uses a different file format.

For each kernel in the application, a corresponding wisdom file is available that stores the results of all tuning sessions for that specific kernel.
Re-tuning the same kernel multiple times for various problem sizes or GPUs add new results to the existing wisdom file.

A wisdom file consists of a sequence of records.
Each record represents the best-performing configuration found during auto-tuning session for one particular GPU and \emph{problem size}.
The problem size is a multi-dimensional vector that indicates the size of the workload.
The interpretation of the problem size varies depending on the specific problem and is defined as part of the kernel definition.
For example, for matrix multiplication of matrices of sizes $n{\times}m$ and $m{\times}k$, the problem size would be $(n, k, m)$.
In addition to the tuning results, each record includes provenance information associated with the tuning sessions, such as the date, software versions, GPU properties, and the host name.




\subsection{Kernel Selection and Compilation}
During application execution,
Kernel Launcher selects the optimal configuration for each combination of kernel, GPU, and problem size.
The first time a kernel is launched, Kernel Launcher processes the wisdom file for that kernel and selects one record based on the GPU and problem size, using the following heuristic:


\begin{itemize}
\item
If a record exists that matches the GPU and the problem size, that record is chosen.

\item
If no such record exists, the record that matches the GPU and problem size that is closest in Euclidian distance.


\item
If no record exists that matches the current GPU, the record that matches the GPU \emph{architecture} and the problem size is closest is chosen.


\item
If no record exists that matches the current GPU architecture, the record that has the closest problem size is chosen.

\item
If the wisdom file is empty or missing, the default configuration is selected.
\end{itemize}


% TODO: Stijn: NVRTC needs reference?
After selecting a configuration, Kernel Launcher compiles the kernel code at runtime by NVRTC, the NVIDIA runtime compilation library, and loads the compiled code onto the GPU. 
%As part of the run-time compilation, the code for the selected kernel configuration is generated with the values of all tunable parameters inserted in the code as compile-time constants. 
Note that kernel selection and compilation only happens on the first launch of a kernel for a given problem size and on subsequent calls for the same problem size will reuse the compiled instance of the kernel configuration.

\begin{figure}[t!]
\begin{lstlisting}[caption={Example of a kernel definition in Kernel Launcher},label={fig:kernel-launcher-example}]
#include <kernel_launcher.h>

void run(float *c, float *a, float *b, int n) {
    // create builder.
    auto builder = kernel_launcher::KernelBuilder(
        "vector_add", "vector_add.cu");  
    auto block_size = builder.tune("block_size", 
        {32, 64, 128, 256, 1024});
    
    builder
        .problem_size(kl::arg3)
        .template_args(block_size)
        .block_size(block_size);

    // Create kernel
    auto kernel = kernel_launcher::WisdomKernel(
        builder);

    // Launch kernel
    kernel.launch(c, a, b, n);|\label{line:launch}|
}
\end{lstlisting}\vspace{-10pt}
\end{figure}

\subsection{Code Example}


Kernel Launcher is implemented as an easy-to-use C++ library. Figure~\ref{fig:kernel-launcher-example} shows an example of the code needed to integrate a tunable vector addition kernel written in CUDA into a C++ host application using Kernel Launcher. 
First, a \texttt{KernelBuilder} is instantiated that specifies the kernel name and the source file that contains the tunable CUDA kernel code. 
Second, the tunable parameters and valid values are specified. 
Then, the problem size, template arguments and thread block dimensions are specified. Note that these may be specified using the kernel arguments or tunable parameters.
Next, a \texttt{WisdomKernel} object is instantiated, which searchers the wisdom file and prepares the  kernel for runtime compilation. 
Finally, the kernel is launched on line~\ref{line:launch}.
Note that the thread block and grid dimensions are calculated by Kernel Launcher and should not be passed by the user.

% Ben: which commands do I need to type in to tune the kernels to create wisdom files? Can we do this from a Make or CMake file?

