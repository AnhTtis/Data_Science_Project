
\section{Introduction}
% Intro on GPUs
Computer systems used for \emph{High-Performance Computing} (HPC) are becoming more complex~\cite{heldens2020landscape}, making it increasingly more difficult to achieve good performance. 
This complexity is due in part to the widespread use of \emph{Graphics Processing Units} (GPUs). 
For instance, in November 2022, seven of the top ten systems in the TOP500~\cite{top500} achieved their performance solely thanks to GPUs.

% why GPU programming is challenging and requires 
Programming and optimizing applications for GPUs is generally considered to be more challenging than for CPUs~\cite{werkhoven2020lessons}.
One of the key challenges of GPU programming is the need to write GPU-specific functions, called \emph{kernels}, that are executed by millions of threads in parallel.
Deciding how to divide and assign work to threads is critical to achieving optimal performance. 
Additionally, the complex memory hierarchy of GPUs means that the decision on where and how to store data significantly impact performance.
Furthermore, optimizing code for GPUs involves trade-offs: certain code optimization may improve performance in some cases, but could be detrimental in others.
For example, loop unrolling, spatial blocking, prefetching and vectorization can all affect performance in different ways. 
We refer to the work by Hijma et al~\cite{hijma2022} for an extensive overview of GPU code optimizations discovered over the last 14 years.

% introduce Kernel Tuner
Optimizing GPU programs thus requires identifying performance-affecting parameters and tuning these parameters to achieve optimal performance.
Although tuning can be done manually, \emph{auto-tuning} tools can also be used to search for the optimal kernel configuration in an automated way.
%While this tuning could be done manually, \emph{auto-tuning} tools can also be used to search for the optimal kernel configuration in an automated way.
One such tool is \emph{Kernel Tuner}~\cite{kerneltuner}, an easy-to-use tool for tuning GPU code that support many search optimization methods.
With Kernel Tuner, GPU programmers create simple Python scripts that specify the kernel code, the tunable parameters, and the input data.
The tool then finds the best-performing kernel configuration by exploring the optimization search space.

% Introduce perf portability
However, these kernel configurations are often not portable since performance heavily depends on the specific input problem (e.g., dimensions or precision of input data) and the exact GPU architecture.
The kernel configuration that gives optimal performance for one particular workload or GPU may result in poor performance for a different setup.
For example, allowing threads to process more than one item could improve performance for larger datasets, but it limits concurrency for smaller datasets. Similarly, unrolling loops may be preferred for GPUs with large register files, as it maximizes data reuse in registers and increases instruction-level parallelism, but it results in excessive register spilling on GPUs with smaller register files.

In practice, GPU programmers may tune a CUDA kernel for a single GPU and input problem, and then falsely assume that the optimal configuration is sufficiently portable to all possible inputs and GPUs.  
Performance-portable GPU application would require tuning for every possible combination of input data set and GPU an integrating this knowledge into the application.
%achieving optimal performance for GPU applications thus requires tuning for every possible combination of input data set and GPU, complicating the creation of performance-portable GPU applications.
Compile-time kernel selection could be used, but this puts a maintenance burden on the application developers.
%Compile-time kernel selection based on the GPU may be applied, but it is difficult to implement when performance changes drastically for different input problems.
%In this paper, we show that these assumptions lead to suboptimal performance that is sometimes worse than not tuning at all.  % Stijn: Already in bullet list 
Ideally, it should be possible to tune kernels for many different setups and the application should then be able to select the optimal kernel automatically based on the situation at hand.
%However, support for this in current auto-tuning tools is limited and, therefore, difficult to implement.


% In this work
To address this issue, we present \emph{Kernel Launcher}: A C++ library that simplifies the integration of auto-tuning in CUDA applications by performing offline tuning of GPU kernels using Kernel Tuner.
We make the following contributions:
\begin{itemize} %[leftmargin=3pt]
  \item We introduce an API for tunable kernel definitions that unifies the host code to launch the kernel with the description of the kernel's tunable parameters.
  \item We introduce the concept of kernel {\em captures} that automatically export all information required to execute a kernel.
  \item We fully automate the use of Kernel Tuner for CUDA applications by \emph{replaying} the captured launches for different parameter configurations.  % , which can now tune kernels
  \item We demonstrate that only tuning for one dataset or one GPU (even of the same vendor and architecture) is insufficient to achieve optimal performance in all scenarios. Similarly, we show how tuning a kernel for only one particular scenario may result in worse performance compared to not tuning the code at all for other scenarios.
  \item Finally, we show that Kernel Launcher's runtime kernel selection and compilation can be used to create optimal-performance portable CUDA applications. 
\end{itemize}


%This paper is structured as follows.
%First, we provide background information on Kernel Tuner and auto-tuning in general.
%Next, we describe the implementation and features of our library Kernel Launcher.
%Afterwards, we demonstrate the benefits of using Kernel Launcher using X kernels from a real-world computational fluid dynamics code.
%Finally, we present conclusions and ideas for future work.