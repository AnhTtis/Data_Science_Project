\section{Related Work}\label{sec:relatedwork}

We distinguish between two different research topics in the field of auto-tuning research. Some auto-tuning research focuses on (1) auto-tuning compiler-generated code optimizations~\cite{tiwari2009scalable,puschel2005spiral, SRTuner, compilePDEtune}, whereas this paper focuses on (2) software auto-tuning~\cite{li2009note,zhang2012auto}. Ashouri et al.~\cite{ashouri2018survey} wrote an excellent survey on machine-learning methods for compiler-based auto-tuning. 
%
In this paper, we focus on (2) auto-tuning software on the source code level, sometimes called \emph{automated design space exploration}~\cite{nardi2019hypermapper}.
Software auto-tuning is not limited to conservative assumptions that a compiler can make and, as such, allows to automatically optimize computational functions in isolation.
For example, it is possible to automatically select among many different optimization techniques~\cite{hijma2022} or entirely different implementations of the same algorithm.
Software auto-tuning is commonly employed by highly-optimized libraries and CPU applications (e.g., ATLAS~\cite{whaley1998automatically} and FFTW~\cite{frigo2005design}) as well as GPU applications~\cite{grewe2011automatically,li2009note,tomov2010dense,zhang2012auto,mametjanov2012autotuning,vanWerkhoven2014optimizing,sclocco2014auto}.
However, the technology used to tune these applications is application specific. 

Several generic auto-tuning frameworks have been introduced in recent years to
 create a reusable application-independent solution. 
In this way, advances in auto-tuning implementations can be reused for new projects and new results in auto-tuning research no longer have to be implemented in multiple application-specific tuners.
%
%generic auto-tuning frameworks, with a focus on optimization methods
%
\emph{OpenTuner}~\cite{ansel_opentuner_2014} was one of the first generic software auto-tuning frameworks, but with no support for tuning individual GPU kernels. 
\emph{GPTune}~\cite{liu2021gptune} is a tuning framework for MPI-based applications with support for parallel tuning.
\emph{HyperMapper}~\cite{nardi2019hypermapper} is a tuning framework for multi-objective optimization on FPGAs %using user-prior knowledge of FPGAs.
% mention ytopt as well

% GPU auto-tuning frameworks
\emph{CLTune}~\cite{nugteren2015cltune} was the first generic auto-tuning framework with support specifically for tuning OpenCL kernels, it has been used to implement an auto-tuned BLAS library implemented in OpenCL~\cite{nugteren2018clblast}. \emph{Auto-Tuning Framework} (ATF)~\cite{raschTACO} generates auto-tuning search spaces efficiently using a chain-of-trees search space structure.
%
\emph{Kernel Tuning Toolkit} (KTT)~\cite{filipovivc2017autotuning} is a C++ auto-tuning framework explicitly developed  to support online auto-tuning and pipeline tuning, which allows exploring of combinations of tunable parameters over multiple kernels. Online auto-tuning is a powerful technique that requires substantial modification of the host application, but allows applications to be tuned at runtime, during normal execution of the application. Online auto-tuning can be advantageous when performance strongly depends on input data. In this paper, we take an orthogonal approach, where we tune offline, once for each problem, and store the obtained performance results to disk.

In earlier work, we have introduced \emph{Kernel Tuner}~\cite{kerneltuner}, a compile-time auto-tuning framework that supports many different optimization algorithms~\cite{schoonhoven2022benchmarking}, including Bayesian Optimization~\cite{willemsen2021bayesian}. The compile-time auto-tuning approach taken by Kernel Tuner, allows auto-tuned kernels to be integrated into applications with minimal modification to the host application. Since the auto-tuning process happens at compile-time, there are no limitations on the programming language of the host application. 

% papers specifically focusing on run-time kernel selection
Falch and Elster~\cite{falch2017machine} have used machine learning for run-time kernel selection of OpenCL applications to use auto-tuning with to create performance portable applications. A neural network is trained using benchmark data, which is, in turn, used to select parts of the search space that are explored exhaustively. By building on top of Kernel Tuner, Kernel Launcher simply uses the results obtained by one of the optimization strategies implemented in Kernel Tuner.

% definitely need to cite this one https://arxiv.org/pdf/2008.13145.pdf
Lawson~\cite{lawson2020towards,lawson2021performance} uses unsupervised clustering to pre-select a subset of the compiled kernel configurations and uses a classification method to select from the best configuration at runtime. Kernel Launcher uses runtime compilation and, therefore, does not include pre-compiled kernel configurations with its applications, as there is no need to pre-select configurations.

Somewhat related to our framework is CERE~\cite{10.1145/2724717}, a framework for code isolation that can extract ``hotspots'' of an application as isolated code fragments for tuning. 
However, CERE explicitly targets LLVM code generation for CPUs, while Kernel Launcher is aimed at GPU kernels.


%BOAST~\cite{doi:10.1177/1094342017718068}