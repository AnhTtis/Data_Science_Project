    %\documentclass[manuscript]{biometrika}
%\documentclass[11pt, oneside]{biometrika} # FORMAT CHANGE
\documentclass[12pt, hidelinks]{article}
\usepackage[left = 0.8 in, right = 0.8 in, top = 1 in, bottom = 1.25 in]{geometry}              
\geometry{letterpaper}  
%\usepackage{amsmath}

%% Please use the following statements for
%% managing the text and math fonts for your papers:
%\usepackage{times}
%\usepackage[cmbold]{mathtime}
%\usepackage{bm}
%\usepackage{natbib}

%\usepackage[plain,noend]{algorithm2e}
%%%%%%%%%%%%%%%%%%
\usepackage[utf8]{inputenc}
%\usepackage{amsmath, amsthm, amssymb,natbib,xcolor,multicol,url}
\usepackage{amsmath, amsthm, amssymb,xcolor,multicol,url}
\usepackage[sectionbib, round]{natbib}
\usepackage{graphicx,bbm}
\usepackage{booktabs,epstopdf,color}
\usepackage[space]{grffile}
\usepackage{lineno,comment}
\usepackage[plain,noend]{algorithm2e}
\usepackage{multirow}

\usepackage{hyperref}

\usepackage{amssymb}
\usepackage{subfig}

\usepackage{authblk}
%%% mine
% --- defs --- %
\def\m{\mathcal}
\def\mb{\mathbb}
\def\mr{\mathrm}
\def\ms{\mathscr}
\def\ind{\mathbbm{1}}
\def\wt{\widetilde}
\def\wth{\widehat}
%%% mine


\makeatletter
\renewcommand{\algocf@captiontext}[2]{#1\algocf@typo. \AlCapFnt{}#2} % text of caption
\renewcommand{\AlTitleFnt}[1]{#1\unskip}% default definition
\def\@algocf@capt@plain{top}
\renewcommand{\algocf@makecaption}[2]{%
  \addtolength{\hsize}{\algomargin}%
  \sbox\@tempboxa{\algocf@captiontext{#1}{#2}}%
  \ifdim\wd\@tempboxa >\hsize%     % if caption is longer than a line
    \hskip .5\algomargin%
    \parbox[t]{\hsize}{\algocf@captiontext{#1}{#2}}% then caption is not centered
  \else%
    \global\@minipagefalse%
    \hbox to\hsize{\box\@tempboxa}% else caption is centered
  \fi%
  \addtolength{\hsize}{-\algomargin}%
}
\makeatother

%%% User-defined macros should be placed here, but keep them to a minimum.
\def\Bka{{\it Biometrika}}
\def\AIC{\textsc{aic}}
\def\T{{ \mathrm{\scriptscriptstyle T} }}
\def\v{{\varepsilon}}

% -- declared math operators -- %
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\TV}{TV}

% --- defs --- %
\def\m{\mathcal}
\def\mb{\mathbb}
\def\mr{\mathrm}
\def\ms{\mathscr}
\def\ind{\mathbbm{1}}
\def\wt{\widetilde}
\def\wth{\widehat}

\def\T{{\mathrm{\scriptscriptstyle T} }}
\def\I{{\mathrm{\scriptscriptstyle I} }}
\def\D{{\mathrm{D} }}
\def\ELPD{{\mathrm{ELPD} }}
\def\EMM{{\mathrm{EMM} }}
\def\SE{{\mathrm{SE} }}
\def\Var{{\mathrm{Var} }}

\def\MF{{\mathrm{\scriptscriptstyle MF} }}
\def\DY{{\mathrm{\scriptscriptstyle DY} }}
\def\LV{{\mathrm{\scriptscriptstyle LV} }}





% -- new commands -- %
\newcommand{\ypbtodo}[1]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red]{#1}}
\newcommand \bbP{\mathbb{P}}
\newcommand \bbE{\mathbb{E}}
\newcommand{\blds}[1]{\mbox{\scriptsize \boldmath $#1$}}

\newcommand{\be}{\begin{equs}}
\newcommand{\ee}{\end{equs}}


\newcommand{\PX}{\mathscr P_2^r(\m X)}
\newcommand{\PXj}{\mathscr P_2^r(\m X_j)}


% -- theorems etc -- %
%\numberwithin{equation}{section}
%\theoremstyle{plain} # FORMAT CHANGE


\addtolength\topmargin{35pt}
\DeclareMathOperator{\Thetabb}{\mathcal{C}}

\newcommand{\beginsupplement}{%
        \setcounter{lemma}{0}
        \renewcommand{\thelemma}{S\arabic{lemma}}%
        \setcounter{table}{0}
        \renewcommand{\thetable}{S\arabic{table}}%
        \setcounter{figure}{0}
        \renewcommand{\thefigure}{S\arabic{figure}}%
        \setcounter{section}{0}
        \renewcommand{\thesection}{S\arabic{section}}%
        \setcounter{equation}{0}
        %\counterwithin{equation}{subsection}
        \renewcommand{\theequation}{S.\arabic{section}.\arabic{equation}}%
        %\counterwithin{lemma}{section}
        \setcounter{page}{1}
}

\begin{document}

%\jname{Biometrika} # FORMAT CHANGE
%% The year, volume, and number are determined on publication
%\jyear{2017}
%\jvol{103}
%\jnum{1}
%% The \doi{...} and \accessdate commands are used by the production team
%\doi{10.1093/biomet/asm023}
%\accessdate{Advance Access publication on 31 July 2016}

%% These dates are usually set by the production team
%\received{2 January 2017}
%\revised{1 April 2017}
% Keywords command
% -- theorems etc -- %
\numberwithin{equation}{section}
%\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}

\newtheorem{definition}{Definition}

\providecommand{\keywords}[1]
{
  \small	
  \textbf{\textit{Keywords---}} #1
}

%% The left and right page headers are defined here:
%\markboth{Abhisek Chakraborty, Anirban Bhattacharya,  Debdeep Pati}{Robust probabilistic inference via a constrained transport metric}

%% Here are the title, author names and addresses
\title{Robust probabilistic inference via a constrained transport metric}

%\author{Abhisek Chakraborty, Anirban Bhattacharya, Debdeep Pati}
%# FORMAT CHANGE
%\affil{Department of Statistics, Texas A\&M University,\\ College Station, TX 77843, U.S.A. \email{cabhisek@stat.tamu.edu} \email{anirbanb@stat.tamu.edu}
%\email{debdeep@stat.tamu.edu}
%}
\author[1]{Abhisek Chakraborty}
\author[1]{Anirban Bhattacharya}
\author[1]{Debdeep Pati}
\affil[1]{Department of Statistics, Texas A\&M University, 
College Station, TX, USA
}


\maketitle

\begin{abstract}
Flexible Bayesian models are typically constructed using limits of large parametric models with a multitude of parameters that are often uninterpretable.  In this article, we offer a novel alternative by constructing an exponentially tilted empirical likelihood carefully designed to concentrate near a parametric family of distributions of choice with respect to a novel variant of the Wasserstein metric, which is then combined with a prior distribution on model parameters to obtain a robustified posterior. The proposed approach finds applications in a wide variety of robust inference problems, where we intend to perform inference on the parameters associated with the centering distribution in presence of outliers.  Our proposed transport metric enjoys great computational simplicity, exploiting the Sinkhorn regularization for discrete optimal transport problems, and being inherently parallelizable. We demonstrate superior performance of our methodology when compared against state-of-the-art robust Bayesian inference methods. We also demonstrate equivalence of our approach with a nonparametric Bayesian formulation under a suitable asymptotic framework, testifying to its flexibility. The constrained entropy maximization that sits at the heart of our likelihood formulation finds its utility beyond robust Bayesian inference; an illustration is provided in a trustworthy machine learning application. 
%the field of demographic parity in machine learning.
%and entropy based portfolio allocation problems.
\end{abstract}

\begin{keywords}
 \ Algorithmic fairness; Empirical likelihood; Entropy; Non-parametric Bayes;  Robust inference; Wasserstein metric.  %Portfolio allocation;
\end{keywords}

\section{Introduction}
In most modeling exercises, our objective is limited to approximating a few key features of the true data-generating mechanism to ensure interpretable inference. It is often futile, if not misleading, to try to model small-scale and complicated underlying contaminating effects. Thus, the interplay between model adequacy and robustness  are fundamental areas of interest in model-based inference. Robust inferential methods \citep{huber2011robust}
%are regarded as one of the oldest topics 
possess an established and influential literature 
in statistics that has permeated many modern areas of research including differential privacy \citep{Dwork09differentialprivacy, doi:10.1080/01621459.2019.1700130, https://doi.org/10.48550/arxiv.2111.06578}, algorithmic fairness \citep{NEURIPS2020_37d097ca, https://doi.org/10.48550/arxiv.2105.11570}, noise-robust training of deep neural nets \citep{han2018coteaching, 9156647}, sequential decision making \citep{NIPS2010_19f3cd30, doi:10.1080/02331934.2019.1655738}, transfer learning \citep{Shafahi2020Adversarially}, quantification learning \citep{doi:10.1080/01621459.2021.1909599}, to name a few.   Bayesian procedures, however, being almost exclusively model-based,  inevitably fall prey to model mis-specification and/or perturbation of the data-generating mechanism -- an issue that exacerbates as sample size increases \citep{doi:10.1080/01621459.2018.1469995}. Credible intervals obtained from such parametric Bayesian models under model mis-specification may not have the desired asymptotic coverage \citep{kleijn2012bernstein}.
%, thus accentuating the problem significantly. 
Non-parametric Bayes methods are routinely used to guard against such mis-specification, either by enlarging the parameter space to impart flexibility or by taking their limit to construct infinite-dimensional prior distributions \citep{muller2004nonparametric, 10.1214/009053606000000029, 10.2307/24310519}.  

%The question of model adequacy and robustness to mis-specification remains an elusive area of research

%The perennial issue regarding the lack of robustness of most Bayesian methods has led to a proliferation of pseudo-likelihood-based approaches that somewhat address the robustness issues but are routinely criticized for lacking generative model interpretations. To that end, we exercise ardent care to ensure that the proposed framework continues to enjoy the probabilistic interpretation inherent to conventional Bayesian methods.

%\textcolor{blue}{(Should the Hellen, Miller, Dunson ref be Miller \& Dunson? Also, check the Martin Heller paper to see if we are missing any important references regarding pseudo-Bayes methods. Ulrich Mueller's sandwich posterior paper is worth a cite. Perhaps also Grunwald's safe Bayes. Also recall Pierre Jacob et al. had a paper in JRSS(B). Later in the paragraph, we need to cite Schennach.)}
Despite the success of non-parametric Bayes methods over the last few  decades; see \citet{muller2015bayesian} for a comprehensive review; the presence of a large number of non-identifiable parameters can be contentious, %renders their usage superfluous, 
particularly when the interest is solely on simpler population features. For example, in many scientific applications,  standard parametric models are often preferred for convenience and ease of interpretation. This long-standing issue regarding the presence of a large number of uninterpretable parameters in non-parametric Bayes procedures has led to a proliferation of pseudo-likelihood-based approaches \citep{RePEc:eee:econom:v:115:y:2003:i:2:p:293-346, 2008,hooker2012bayesian, sandwitch,JMLR:v18:16-655, safebayes, Bernton_2019, doi:10.1080/01621459.2018.1469995} targeted towards specific parameters of interest. 
%\textcolor{red}{[order citations by year.]}. 
However, these approaches typically lack generative model interpretations making the calibration of the associated dispersion or temperature parameters  challenging \citep{10.1093/biomet/asx010, safebayes}. 

 An empirical likelihood (EL;  \citet{owen2001empirical}) offers an attractive solution which approximates the underlying distribution with a discrete distribution supported at the observed data points and obtains the induced maximum likelihood of the parameter of interest defined through estimating equations, by effectively profiling out the nuisance parameters. Exponentially titled Empirical Likelihood (ETEL) is a variant of this idea that minimizes the Kullback--Leibler  divergence of this discrete distribution with the empirical distribution of the observed data subject to satisfying the estimating equation.  One can import such a likelihood in a Bayesian framework to infer on parameters   \citep{10.2307/30042042, 10.1093/biomet/92.1.31,Chib2018,chib2021bayesian}. Interestingly, posterior credible intervals obtained from such  Bayesian procedure do have the correct frequentest coverage \citep{chib2021bayesian}, thereby effectively eliminating the longstanding criticism associated with parametric Bayesian inference under model mis-specification. 

Our goal here is to develop a flexible Bayesian  semi-parametric procedure that centers around a guessed parametric family $F_\theta$, without having to explicitly model aspects of the underlying data-generating mechanism we are not interested in. The task is similar to developing a robust Bayesian procedure \citep{RePEc:eee:econom:v:115:y:2003:i:2:p:293-346, 2008,hooker2012bayesian, doi:10.1080/01621459.2018.1469995, JMLR:v18:16-655} that allows departures from a parametric model to accommodate outlying observations. One may, alternatively, consider a non-parametric Bayes procedure \citep{Ferguson, teh2010dirichlet, 10.1214/aos/1176342871, 10.1214/aos/1176325623, 10.1214/aos/1024691240} where the parametric guess $F_\theta$ (with density $f_\theta$) assumes the role of the base measure, with the precision parameter controlling the extent of concentration around $F_\theta$. 
\begin{comment}Here,  to perform Bayesian inference on certain parameter $\theta$ of a pre-specified probability distribution $F_\theta$, we propose to formulate a likelihood that allows departure from $F_\theta$ in regions where the observed data do not conform to $F_\theta$.  
\end{comment} 
However, unlike these approaches, we desire our approach to be devoid of nuisance parameters, and that the inference is solely targeted to the parameter of interest while retaining the interpretation of a generative probability model. In a sense these are similar to the goals of EL (or ETEL) where one can simply consider the estimating equation $\mbox{E}[\partial \log f_\theta(X)/ \partial \theta]=0$ to infer about the parameter $\theta$. Observe that such a restriction enforces specific constraints on the moments of the distribution (e.g first moment if $F_\theta$ is Gaussian). Such a moment based constraint is agnostic to the tails of $F_\theta$ which could be significantly affected by outlying observations unless one allows the entire distribution to be constrained to lie in a neighborhood of $F_\theta$.  To that end, we propose a novel adaptation of ETEL by centering the discrete distribution $P$ of the data around $F_\theta$ using a suitable distance metric $\mbox{D}$ that encapsulates a more holistic discrepancy between the two distributions. More specifically,  we restrict $P$ within the neighborhood  $\mbox{D}[P , F_{\theta}]  < \varepsilon$, for some radius  $\varepsilon> 0$. In an inferential task, this framework provides a good balance between modeling flexibility by adaptively tuning $\varepsilon$ and interpretability, since we have the provision to invoke a non-parametric likelihood that concentrates around an  interpretable parametric guess, where the nuisance parameters are profiled out within the ETEL framework.


\begin{comment}
The idea of centering the distribution of the observed data around a pre-specified parametric model is not new.  In fact, the Dirichlet process prior \citep{Ferguson, teh2010dirichlet} in Bayesian non-parametric is exactly designed to achieve this; other related approaches include \citep{10.1214/aos/1176342871, 10.1214/aos/1176325623, 10.1214/aos/1024691240}. But the  traditional non-parametric priors are often accompanied by a large number of un-interpretable nuisance parameters that result in a computational overhead. The proposed approach directly obtain the marginal posterior parameter of interest, and enjoys improved interpretability since the remaining nuisance parameters in the model are marginalised out. Moreover, while there is substantial literature on tuning the concentration parameter of the Dirichlet process mixture model \citep{EscobarWest, IZ, McAuliffe}, it still remains to be a  notoriously difficult task. Our non-parametric Bayes procedure involves a hyper-parameter $\varepsilon$, that too controls concentration around a parametric distribution. But since the nuisance parameters are effectively marginalized out in our approach, we simply adopt a predictive approach  to we devise a data-driven and principled hyper-parameter tuning scheme. Moreover, we demonstrate in  \ref{ssec:npBayes} that a constrained empirical likelihood is asymptotically equivalent to a mixture model, centered at the pre-specified parametric density. Thus we are able to retain the advantages of non-parametric Bayes models while being devoid of nuisance parameters.
\end{comment}

%Notably, since we restrict the data distribution $P$ within the neighborhood  $\mbox{D}[P \,||\ F_{\theta}]  < \varepsilon$,

Naturally, a key ingredient in our proposal is the choice of the  metric $\mbox{D}$ that yields a non-trivial distance between $F_\theta$ and the empirical distribution on the observed data, and at the same time enjoys computational simplicity and straightforward multivariate extension. An equally important task is to have the provision to allow the user to select from relatively wider class of distributions $F_\theta$. Although having a fully flexible $F_\theta$ defeats the purpose of constructing a procedure devoid of nuisance parameters, we choose to work with elliptical mixture model (EMM)s which offers the user a sufficiently large class to choose from.  Because  $F_\theta$ is potentially absolutely continuous with respect to the Lebesgue measure and the empirical distribution on the observed data is discrete, it rules out many standard distances e.g Kullback--Leibler, Hellinger, total variation, $\chi^2$ etc. The $p$-Wasserstein metric \citep{Villani2003TopicsIO, 2019} provides a feasible choice, although it does not allow for a computationally efficient multivariate extension and the search for the optimal coupling becomes challenging. 


To this end, we propose a novel adaptation of the $2$-Wasserstein metric by a {\em restriction} and an 
\emph{augmentation} scheme. In the {\em restriction} scheme, motivated by \citet{delon:hal-02178204}, we assume $F_\theta$ to be an EMM and adapt $\mbox{D}$ by further restricting the coupling measures to  the class of EMMs, which considerably reduce the computational cost and yet encompasses a rich class of
coupling measures. However, this renders the metric to depend only on the variance-covariance matrix of $F_\theta$ which ignores finer comparison in the tails. We address this in the {\em augmentation} scheme, where we augment the coupling measure with a product of univariate couplings. This tantamounts to adding a sum of univariate Wasserstein metrics to our adaptation, which effectively captures tail features. 
Further, the restriction scheme can   exploit a entropic regularization of discrete optimal transport \citep{le2019treesliced, cuturi2013sinkhorn}  that remains expressive, and computationally tenable even in multivariate cases.  Finally, we also developed a data-driven framework for tuning $\varepsilon$
exploiting an interplay of estimated expected log point-wise predictive density (ELPD) and its standard error \citep{JMLR:v17:14-540}. The resulting metric is termed ANDREW with the complete procedure named as $\mbox{D}$-BETEL. 

%Although we have come up with 
Having proposed a seemingly flexible method centering around a parametric family, it is of crucial importance to study to what degree the resulting posterior distribution of $\theta$ obtained from $\mbox{D}$-BETEL deviates from the same obtained from a typical non-parametric Bayes procedure centered around $F_\theta$.  In Section \ref{ssec:npBayes}, we address this by showing that the posterior of $\theta$ obtained from $\mbox{D}$-BETEL is similar to the posterior of $\theta$ obtained from a mixture-of-finite mixtures model \citep{doi:10.1080/01621459.2016.1255636} centered around $F_\theta$ under a suitable asymptotic framework. This is quite satisfying as it indicates that in performing inference on $\theta$, we are able to bypass the caveats associated with a fully non-parametric Bayes model, while still retaining its advantages in offering %full
flexibility regarding the data generation process.  
%, namely, Kullback--Liebler divergence, Hellinger distance, total variation distance, Euclidean distance, $\chi^2$ distance, etc. We can use the Cramer-von-mises metric in a univariate setting, but its extension to multivariate problems is not straightforward. On the other hand, 
%the 2-Wasserstein metric [\cite{Villani2003TopicsIO,2019}] provides an attractive candidate. 
%But, due to its great flexibility and favorable properties Wasserstein metric has consistently featured in recent statistics and machine learning literature \cite{hallin2021multivariate, ramdas2015wasserstein, bernton2019parameter, muzellec2019generalizing}. At the same time, 
%To circumvent computational bottlenecks associated with computing the 2-Wasserstein metric, we propose a constrained transport metric via  entropic regularization of discrete optimal transport [\cite{le2019treesliced, cuturi2013sinkhorn}].  In addition,
%We also developed a data-driven framework for tuning the $\varepsilon$.
%exploiting an interplay of estimated expected log point-wise predictive density (ELPD) and its standard error \cite{JMLR:v17:14-540}.



The distributionally constrained entropic optimization, which forms the backbone of our likelihood formulation, is likely to be useful more broadly. To demonstrate its applicability beyond robust inference, we exploit the idea of re-weighting a distribution to increase proximity to another distribution in the context of demographic parity in algorithmic fairness problems in Section \ref{ssec:algorithmic_fairness}. We envisage more applications of this nature in the trustworthy AI paradigm, and wish to explore them in more details elsewhere.  
%The rest of the article is organised as follows. Section \ref{D_BETEL} introduces the proposed semi-parametric Bayes procedure, formally establish it's generative model interpretations, and device a tailor-made transport metric for ensuing applications. In Section \ref{Robust_Bayes}, we discuss the utility of the proposed methodology in robust Bayesian inference applications. The Section \ref{ssec:algorithmic_fairness}  extend our methodology to applications in demographic parity in algorithmic fairness problems.
%We included an additional adaptation of our framework to entropy based portfolio allocation problems in supplementary material \ref{ssec:po}.
%Finally, we conclude.

%\maketitle
\section{The D-BETEL}\label{D_BETEL}
%\textcolor{blue}{Begin Section 2 (The D-BETEL). Get rid of the subsection header 2.1. Begin by saying that our goal is to develop a flexible Bayesian procedure that centers around a given parametric family $F_\theta$, without having to explicitly model aspects of the underlying distribution we are not interested in. Say that one may view this as a robust Bayesian procedure that allows departures from the parametric model (that is more immune to outlying observations compared to simply fitting the parametric model) or an NP Bayes procedure (where $F_\theta$ assumes the role of the base measure). You will need to flesh this out well. Do not just repeat what I am writing here; it is only a sketch.}
%\textcolor{blue}{(Abhisek: this paragraph seems a continuation of the introduction, and does not seem to say anything new. I am adding some material before the next paragraph so that this section can start from there. You will need to find appropriate places to insert the content of this paragraph into the rest of the section.)}


%However,  such priors are often accompanied by a large number of un-interpretable nuisance parameters that result in a computational overhead. 

%\textcolor{blue}{Go to the next paragraph. Say that our approach is motivated by the construction of the BETEL due to Schennach'05 (since this is a key ref, I would write it as Schennach [58] rather than [58] ). Provide a quick verbal summary of BETEL --- borrow the second line of page 4 to start with. Then, hit two main points: (a) Schennach started with a nonparametric prior on P as a mixture of uniforms, and use limiting arguments to define a BETEL posterior (see section 2.1 of Chib et al. for similar verbiage). (b) Say that the limiting form is available in closed-form and give the form of the BETEL posterior in a displayed equation.}
Let $\{F_\theta: \theta \in \Theta \subseteq \mb R^d\}$ be a parametric family of distributions. In the sequel, we develop a flexible Bayesian semi-parametric procedure that centers around this parametric family while allowing for flexible departures from it. 
Our approach draws inspiration from the Bayesian exponentially tilted empirical likelihood (Bayesian ETEL or BETEL; \citet{10.1093/biomet/92.1.31}) for moment conditional models that are specified by a collection of moment conditions $\mb{E}_{P}\big[g(X,\theta)\big] = 0$, where the expectation  $\mb{E}_{P}$ is taken with respect to the unknown generating distribution $P$,\  $g: \mb{R}^d\times\Theta \rightarrow \mb{R}^r$ is a vector of known functions, $\theta \in \Theta$ is the parameter of interest, and $0$ refers to a vector of $r$ zeros.  
%Given a random sample $x = (x_1,\ldots, x_n)^\T$ from %an unknown data generating distribution 
%$P$, 
Operating under a non-parametric Bayesian framework, \citet{10.1093/biomet/92.1.31} proposed a flexible prior on $P$ with an entropy-maximizing flavor.  
%that prefers  distributions with small support (\textcolor{blue}{What do you mean by `small support'?} \textcolor{red}{borrowed the term from Shennach/ Chib. Could you suggest an alternative?}), and among those sharing common support, prefers distributions with large entropy. 
Under a specific asymptotic regime that allowed analytic marginalization of nuisance parameters describing the generative model, the corresponding {\em marginal} posterior distribution of $\theta$ given a random sample $x = (x_1,\ldots, x_n)^\T$ from $P$ was shown to approach a limiting distribution, called the BETEL posterior, given by
%the BETEL posterior for $\theta$ naturally arises as a limiting case of their non-parametric Bayes procedure and assumes the form:
\begin{equation}\label{eqn:betelpost}
\pi_{\rm MCM}(\theta\mid x_1,\ldots,x_n) \ \propto\ \pi(\theta)\ L_{\rm MCM}(\theta)    
\end{equation} 
where the `likelihood' $L_{\rm MCM}(\theta)$ is called the exponentially tilted empirical likelihood,
\begin{equation}\label{eqn:betel}
  L_{\rm MCM}(\theta) = \bigg\{\prod_{i=1}^n w_{i} :  \argmax_{w}\prod_{i=1}^n w_{i}^{-w_{i}},\ w_i>0,\ \sum_{i=1}^n w_i = 1, \ \sum_{i=1}^n w_i g(x_i, \theta) = 0  \bigg\},
\end{equation} 
and $\pi(\cdot)$ denotes a prior distribution on $\theta$. Here and elsewhere, we use MCM as an acronym for {\em moment condition model}. 
%This result formally demonstrates that the exponentially tilted empirical likelihood  enjoys a well-defined probabilistic interpretation that  justifies its use in Bayesian inference. 

The maximization problem in \eqref{eqn:betel} admits a non-trivial closed-form solution 
%\textcolor{blue}{(Abhisek: I believe this is not true for every $\theta$, it needs to be inside a convex hull; can you be precise?)}
%Moreover, the posterior $\pi_{\rm MCM}(\theta\mid x_1,\ldots,x_n)$ in \eqref{eqn:betelpost}-\eqref{eqn:betel} enjoys a closed form expression:
% \begin{align*}
%     \pi_{\rm MCM}(\theta\mid x_1,\ldots,x_n) \ \propto\ \pi(\theta)\ \prod_{i=1}^n w_{i}^{\star},
% \end{align*}
when the convex hull of $\cup_{i=1}^n g(x_i, \theta)$ contains the origin, leading to $L_{\rm MCM}(\theta) = \prod_{i=1}^n w_i^\star(\theta)$, with  
%\textcolor{blue}{(The equation defining $\lambda$ did not make sense previously. Check if it is correct now.)}
\begin{align*}
    w_{i}^{\star}(\theta) =  \frac{\exp[\lambda(\theta)^{\T}g(x_i, \theta)]}{\sum_{j=1}^n \exp[\lambda(\theta)^{\T}g(x_j, \theta)]}, \quad \lambda(\theta) = \argmin_{\eta} n^{-1} \sum_{i=1}^n \exp[\eta^{\T}g(x_i, \theta)],
\end{align*}
When the convex hull condition is not satisfied, $\pi_{\rm MCM}(\theta\mid x_1,\ldots,x_n)$ is set to zero. 
%and $\pi_{\rm MCM}(\theta\mid x_1,\ldots,x_n) = 0$ otherwise. 
% When the convex hull of $\cup_{i=1}^n g(x_i, \theta)$ indeed contains the origin, the weights $w_{i}^{\star}, \ i = 1,\ldots$ can be simply expressed as
% \begin{align*}
%     w_{i}^{\star}\ \propto\ \frac{\exp[\lambda(\theta)^{\T}g(x_i, \theta)]}{\sum_{i=1}^n \exp[\lambda(\theta)^{\T}g(x_i, \theta)]},\quad\text{and}\quad \lambda(\theta) = \argmin_{\eta} n^{-1} \sum_{i=1}^n \exp[\lambda(\theta)^{\T}g(x_i, \theta)],
% \end{align*}
% and it yields a computationally convenient Bayesian procedure. 
The maximization problem defining $\lambda(\theta)$ is convex, which leads to efficient computation of the ETEL likelihood $L_{\rm MCM}$, and the corresponding BETEL posterior $\pi_{\rm MCM}$ can be sampled using standard MCMC procedures. 
%Importantly, while the BETEL is motivated from a non-parametric Bayes angle, it {\em operationally} avoids a complete probabilistic specification of the data-generating mechanism; the user only needs to specify a prior distribution on the parameter of interest $\theta$.
\citet{Chib2018} significantly contributed towards the theoretical underpinning of BETEL for moment conditional models; proving Bernstein–von Mises (BvM) theorem \& model selection consistency results under model mis-specification; and also numerically displayed its utility in wide-ranging econometric and statistical applications. 

%, and thus reduces the inconsistent estimation associated with model mis-specification.


%\textcolor{blue}{Next paragraph. Say that only a limited class of parametric models are defined by moment conditions, and hence the above scheme does not apply generally for our purpose. Then, say that a key ingredient of our approach is a distance metric D between $F_\theta$ and $\nu_{w, x}$. [Note: D(P, Q) consistently; not D(P||Q) or D(P,  Q). Similarly, be consistent with $\nu_{w, x}$ and not $\nu(w, x)$] You can already foretell a few things about the distance metric. Since $F_\theta$ is typically absolutely conts w.r.t. Lebesgue measure and $\nu$ is discrete, it rules out many standard distances XYZ. A natural choice is transport-based metrics (CITE). Finally, say that we propose a carefully tailored transport metric in Section XYZ that offers a balance between ...}




%nd displayed its utility in  wide-ranging applications in various statistical settings, e.g, generalised linear regression model. Notably, the authors establish the Bernstein–von Mises (BvM) theorem for the corresponding Bayesian ETEL posterior and the model selection consistency of the marginal likelihood, even under model mis-specification.  


The feature of BETEL most relevant to our purpose is that while the BETEL is motivated from a non-parametric Bayesian angle, it {\em operationally} avoids a complete probabilistic specification of the data-generating mechanism; the user only needs to specify a prior distribution on the parameter of interest $\theta$. In a similar spirit, our goal is to avoid a full non-parametric modeling of the data-generating distribution and only place a prior distribution on the (typically low-dimensional) parameter $\theta$ describing the centering model. A direct application of the ETEL to our setup is challenging as moment conditions describing parameters of general parametric models; especially those beyond exponential families; can be quite cumbersome or even unavailable in an analytically tractable form. Instead, our approach is to design a modified likelihood by constraining a weighted empirical distribution of the observed data $\nu_{w,x} :\,= \sum_{i=1}^n w_i\delta_{x_i}$ to be close to the parametric model $F_\theta$ with respect to a statistical metric. Specifically, we propose a likelihood function
\begin{equation}\label{eqn:wbetel}
  L_{\rm DCM}(\theta) :\,= \bigg\{\prod_{i=1}^n w_{i} :  \argmax_{w}\prod_{i=1}^n w_{i}^{-w_{i}},\ w_i>0,\ \sum_{i=1}^n w_i = 1, \  \mbox{D}[F_{\theta}, \nu_{w, x} ]\ \leq  \varepsilon \bigg\}  
\end{equation} 
where $\mbox{D}[\cdot,\cdot ]$ is an appropriate statistical distance, $\varepsilon > 0$ is a concentration parameter which controls fidelity to the centering model, and DCM is an acronym for {\em distributionally constrained model}. With this DCM likelihood, and a prior distribution on the parameter $\theta$, the corresponding posterior distribution is \begin{equation}\label{eqn:wbetelpost}
\pi(\theta\mid x_1,\ldots,x_n) \ \propto\ \pi(\theta)\ L_{\rm DCM}(\theta)    
\end{equation}
We refer to our formulation in  \eqref{eqn:wbetel} -- \eqref{eqn:wbetelpost} as the Bayesian ETEL subject to distributional constraint ($\mbox{D}$-BETEL). We show in Section \ref{ssec:npBayes} that the $\mbox{D}$-BETEL posterior arises organically from a non-parametric Bayes model by marginalization of the nuisance parameters specifying a mixing measure which has a mixture of finite mixtures (MFM; \citet{doi:10.1080/01621459.2016.1255636}) interpretation.   Further, in sub-section \ref{ssec:gen_reg}, the proposed methodology is extended to the regression setup, and detailed empirical study is performed to showcase its efficacy over standard Bayesian methodology and moment conditional models based on maximum likelihood equations.

The idea of centering the distribution of the observed data around a pre-specified parametric model is not new.  In fact, the Dirichlet process prior \citep{Ferguson, teh2010dirichlet} in Bayesian non-parametric is exactly designed to achieve this; other related approaches include \citet{10.1214/aos/1176342871, 10.1214/aos/1176325623, 10.1214/aos/1024691240}. Notably, the  traditional non-parametric priors are often accompanied by a large number of un-interpretable nuisance parameters that result in a computational overhead. On the contrary, $\mbox{D}$-BETEL directly obtain the marginal posterior parameter of interest, and  since the remaining nuisance parameters in the model are marginalised out, it enjoys improved interpretability. Moreover, while there is substantial literature on tuning the concentration parameter of the Dirichlet process mixture model \citep{EscobarWest, IZ, McAuliffe}, it still remains to be a  notoriously difficult task. $\mbox{D}$-BETEL involves a hyper-parameter $\varepsilon$, that too controls concentration around a parametric distribution. Since the nuisance parameters are effectively marginalized out in $\mbox{D}$-BETEL, we simply adopt a predictive approach  to devise a data-driven and principled tuning scheme. Moreover, we demonstrate in  Section \ref{ssec:npBayes} that a constrained empirical likelihood is asymptotically equivalent to a mixture model, centered at the pre-specified parametric density. Thus we are able to retain the advantages of non-parametric Bayes models while being devoid of nuisance parameters.

A key ingredient in our proposal is a  metric $\mbox{D}$ that yields a non-trivial distance between $F_\theta$ and weighted empirical distributions, is computationally convenient, and admits a seamless multivariate extension. Since $F_\theta$ is potentially absolutely continuous with respect to the Lebesgue measure, many standard distances e.g Hellinger, total variation, $\chi^2$ etc. are ruled out. The $p$-Wasserstein metric \citep{Villani2003TopicsIO} provides a feasible choice, however, it too is intractable in many multivariate cases we care about. To that end, assuming $F_\theta$ to be a elliptical mixture model (EMM), we further restrict the coupling measures to a carefully chosen sub-family, which considerably reduce the computational cost and yet encompasses a rich class of
coupling measures. Further, exploiting a entropic regularization of discrete optimal transport \citep{le2019treesliced, cuturi2013sinkhorn},
we propose a carefully tailored Wasserstein metric in Section \ref{ssec:andrew}  that remains expressive, and computationally tenable even in multivariate cases.


Before we move on, we undertake a closer look at the constrained entropy maximization problem at the core of our likelihood formulation in \eqref{eqn:wbetel}. Since
$$
\log \prod_{i=1}^n w_i^{-w_i}= \log n - \sum_{i=1}^n w_i \log (w_i/(1/n))
$$, it is apparent that solving the above  maximization problem is equivalent to finding the probability vector $(w_1,\ldots,w_n)$ that minimizes the Kullback--Leibler  divergence between the probabilities $w_1,\ldots,w_n$ assigned to each sample and the empirical probabilities $1/n,\ldots,1/n$, subject to the distance constraint $\mbox{D}[F_{\theta}, \nu_{w, x} ] <  \varepsilon$. Unlike the case for $L_{\rm MCM}$ \eqref{eqn:betel}, the optimization problem 
%at the heart of 
for $L_{\rm DCM}$ \eqref{eqn:wbetel} does not allow a closed form solution. Fortunately, we can access augmented Lagrangian methods \citep{auglag1, auglag2} and conic solvers \citep{TCFOCS} via the R interface \citep{RCore}  of  constrained non-linear optimization solvers (e.g. $\mbox{NLopt}$; \citet{nlopt} and $\mbox{CVX}$; 
\citet{gb08}). In particular, for fixed $\theta\in\Theta$ and $\varepsilon>0$, we can express the non-linear programming problem in \eqref{eqn:wbetel} in standard form as:
\begin{align*}
&\min_{w \in \m S^{n-1}}\ \sum_{i=1}^n w_i\log w_i, \quad
\text{subject to}\quad \mbox{D}[F_{\theta}, \nu_{w, x} ] \leq \varepsilon ,
\end{align*}
with domain $\mathcal{S}^{n-1} :\,= \{v \in \mb R^n: \ v_i>0,\ i =1,\ldots,n;\ \sum_{i=1}^n v_i = 1\}$ with a non-empty interior.  %$\nu_{w,x} = \sum_{i=1}^n w_i\ \delta_{x_i}$, and $F_{\theta}$ is potentially discrete. 
The associated Lagrangian function $\mathcal{L}: \mb{R}^n\times\mb{R}\to\mb{R}$ is defined as
\begin{align}\label{eqn:dbetel:alt}
  \mathcal{L}(w, \lambda^*) =    \sum_{i=1}^n w_i\log w_i +  \lambda^{\star}\ \mbox{D}^2[F_{\theta} ,\nu_{w, x}], 
\end{align}
where $\lambda^\star$ is the Lagrange multiplier, and the Lagrange dual function $v:\mb{R}\to\mb{R}$ takes the form 
\begin{align}\label{wbetel_dual}
v(\lambda^\star) = \inf_{w\in\mathcal{S}} \mathcal{L}(w, \lambda^\star). 
\end{align}
This dual formulation enables us to access off-the-self augmented Lagrangian based optimization algorithms \citep{auglag1, auglag2} to compute $L_{\rm DCM}$.
%\textcolor{red}{(Abhisek: add a concluding line as to how this dual formulation is used to compute DCM, for example, using the above packages.)}
Moreover, from an application stand-point, the formulation in \eqref{wbetel_dual} may render itself useful in a wide range of problems, where instead of %the exponentially tilted empirical likelihood and a continuous 
the weighted empirical distribution $\nu_{w,x}$ and a parametric guess $F_\theta$, we consider a pair of distributions with at least one of them discrete, and the goal is to ensure that a re-weighted version of the discrete one is close to the other distribution, subject to certain conditions on the weights.
%\textcolor{red}{(not very clear. when you say `any two distributions', what does reweighting mean?)} 
We present one such application in Section \ref{ssec:algorithmic_fairness} 
%and supplement section \ref{ssec:po} respectively; the first 
in the context of ensuring demographic parity in machine learning algorithms.
%, and the second for entropy-based portfolio allocation.


\begin{comment}
%\textcolor{blue}{(add more on how this computation takes place, either here, or within the next two paragraphs.)} 




%In the light of these exciting theoretical developments, it is worthwhile to attempt to expand the horizon of models accessible via Bayesian ETEL \cite{10.1093/biomet/92.1.31, Chib2018}. 
%\textcolor{blue}{(this couple of sentences shows Chib et al. in a negative light. need to put a positive spin.)}
%Our proposed semi-parametric Bayesian procedure with a distance-based constraint brings a versatile and flexible class of models within the Bayesian ETEL framework via ensuring that a weighted empirical distribution of the observed data $\nu_{w,x} = \sum_{i=1}^n w_i\delta_{x_i}$ is close to flexible  parametric guess  $F_{\theta}$ with respect to a statistical metric. %Naturally, a key ingredient in our proposal is a  metric $\mbox{D}$ that yields a non-trivial distance between $F_\theta$ and $\nu_{w, x}$, and at the same time enjoys computational simplicity and seamless multivariate extension. Since  $F_\theta$ is typically absolutely continuous with respect to the Lebesgue measure and $\nu_{w, x}$ is discrete, it rules the possibility to employ many standard distances e.g Kullback--Leibler , Hellinger, total variation, $\chi^2$ etc. The $p$-Wasserstein metric [\cite{Villani2003TopicsIO}] provides a feasible choice, however, it too is intractable in many multivariate cases we care about. To address that, we propose a carefully tailored Wasserstein metric in Section \ref{ssec:andrew}  that remains expressive, and computationally tenable even in multivariate cases.
%\textcolor{blue}{Next paragraph. Broadcast what your proposal is going to look like. Specifically, given a prior $\pi(\cdot)$ on $\theta$, an appropriate metric $\mbox{D}$, and a tolerance parameter $\varepsilon > 0$, say that the proposed D-BETEL posterior takes the form (give 2.2), where $L_{\rm DCM}(\theta)$ is (give 2.1). Give some initial features of the procedure. E.g. the text between current 2.1 - 2.2, also add how the solution to the maximization problem looks like etc. Add the dual formulation here from 2.3 with some edits. First, give it for D-BETEL. Then, comment that this in itself may be a useful thing to look at etc. Delete Section 2.3.}
%With the blueprint delineated in the previous paragraph, we extend the Bayesian ETEL framework beyond moment condition models, specifically when a centering parametric distribution is available \eqref{eqn:wbetelpost} - \eqref{eqn:wbetel}.
%Given a random sample $x = (x_1,\ldots, x_n)^\T$ from an unknown data generating distribution $P$ on $\mathbb{R}^d$, a prior distribution $\pi(\cdot)$ on $\theta$, our posterior distribution takes the form
% \begin{equation}\label{eqn:wbetelpost}
% \pi(\theta\mid x_1,\ldots,x_n) \ \propto\ \pi(\theta)\ L_{\rm DCM}(\theta)    
% \end{equation} 
% where
%is the exponentially tilted empirical likelihood centered around a parametric family $F_{\theta}$ via a statistical distance $\mbox{D}[\cdot \ ,\ \cdot ]$, $\varepsilon > 0$ is a concentration parameter and $\nu_{w,x}$ is a weighted sum of Dirac measures $\nu_{w,x} = \sum_{i=1}^n w_i\delta_{x_i}$.

%\textcolor{blue}{(Abhisek, this paragraph needs to be edited to align with the flow of the previous paragraphs. add details about computation as requested earlier. also, ease into the dual formulation. currently reads rushed. give some more details as to how you arrive at 2.5, what result from Lagrange multipliers you use etc. only then go to talk about extensions.)}
%Thus, to calculate the defined likelihood $L_{\rm DCM}(\theta)$ at a given $\theta\in\Theta$, we find weights $(w_1,\ldots,w_n)$ that minimize the Kullback--Liebler divergence between the probabilities $(w_1,\ldots,w_n)$ assigned to each sample observation and the empirical probabilities $(1/n,\ldots,1/n)$, subject to $(w_1,\ldots,w_n)$ summing to one while satisfying the distance based constraint. We shall refer to our formulation as the Bayesian ETEL subject to the distributional constraint (D-BETEL). 

% Next, we spare a closer look at the constrained entropy maximization problem at the core of our likelihood formulation in \eqref{eqn:wbetel} to ensure better understanding, and identify its potential utilities. For fixed $\theta\in\Theta$ and $\varepsilon>0$, we can express the non-linear programming problem in \eqref{eqn:wbetel} in standard form as:
% \begin{align*}
% &\mbox{minimize}_{w}\ \sum_{i=1}^n w_i\log w_i, \quad
% \text{subject to}\quad \mbox{D}[F_{\theta}, \nu_{w, x} ] \leq \varepsilon ,
% \end{align*}
% with domain $\mathcal{S}= \{w: \ w_i>0,\ i =1,\ldots,n;\ \sum_{i=1}^n w_i = 1\}\subset \mb{R}^n$ with a non-empty interior;  $\nu_{w,x} = \sum_{i=1}^n w_i\ \delta_{x_i}$, and $F_{\theta}$ is potentially discrete. The associated Lagrangian function $\mathcal{L}: \mb{R}^n\times\mb{R}\to\mb{R}$ is defined as
% \begin{align*}
%   \mathcal{L}(w, \lambda^*) =    \sum_{i=1}^n w_i\log w_i +  \lambda^{\star}\ D(F_{\theta} ,\ \nu_{w, x}), 
% \end{align*}
% where $\lambda^*$ is the Lagrange multiplier, and the Lagrange dual function $v:\mb{R}\to\mb{R}$ takes the form 
% \begin{align}\label{wbetel_dual}
% v(\lambda^*) = \inf_{w\in\mathcal{S}} \mathcal{L}(w, \lambda^*).  
% \end{align}
% This formulation in \eqref{wbetel_dual} may render itself useful in a wide range of applications, where instead of the exponentially tilted empirical likelihood and a continuous parametric guess $F_\theta$, we consider any two distributions, and the goal is to ensure that a re-weighted version of one is close to the other subject to certain conditions on the weights.  We present two such interesting applications of the above display, in the context of ensuring demographic parity \ref{ssec:algorithmic_fairness} in machine learning, and entropy-based portfolio allocation \ref{ssec:po}.

\end{comment}

%Since we restrict the data distribution $P$ within the neighborhood  $\mbox{D}[P \,||\ F_{\theta}]  < \varepsilon$, 




%\textcolor{blue}{Create a subsection. Say that we shall now view 2.1-2.2 as a limiting marginal posterior of $\theta$ in an MFM-type model. You can now bring in the second para onwards of Section 3, with minor stylistic edits in sentences to fit the flow from previous paragraphs. The only other change needed will be stating the Theorem with a general metric d -- should be easy.}
Before we put a more specific structure to $\mbox{D}$-BETEL, we shall discuss a key feature of our proposal, that we briefly alluded to earlier, in concrete terms. In particular, we demonstrate that one may view our proposed methodology as a non-parametric Bayes approach based on centering mixture models around a specific parametric family by establishing an intriguing asymptotic equivalence relationship between our framework and a hierarchical setup similar to the mixture of finite mixture (MFM) models \citep{doi:10.1080/01621459.2016.1255636}. Moreover, this enables us to formally identify $\mbox{D}$-BETEL as a generative model - a feature illusive to many existing pseudo-likelihood-based robust Bayesian methods.

%\textcolor{blue}{(The content of this paragraph is mostly fine; we need more supporting references in the first part. Also, we may need to postpone some of the discussion to later in the section. In a methods section, you need to get quickly to your method, and interweave relation with literature rather than a big paragraph at the beginning. Some other comments:
%(i) you cite different refs for the Dirichlet process at various places; be consistent. if you want to cite the original ref, cite the Ferguson (74?) paper. 
%(ii) the line `other related approaches': mention what these are: Polya tree (Lavine; 94) etc. 
%(iii) the point about the concentration parameter of Dirichlet being hard to tune needs rephrasing and references. this is something with a lot of literature behind it, and we need to avoid being dismissive. for example, we may say that while there is an extensive literature, this problem remains difficult etc. also, we need to highlight that unlike other NP Bayes procedures, we directly obtain a marginal posterior of parameter of interest. 
%Btw, this is a common issue in your literature review...sentences need to play more of a balancing act and sound less black-n-white. this is crucial, otherwise refs get angry. )}


\subsection{Non-parametric Bayes interpretation of D-BETEL}\label{ssec:npBayes}
%Contrary to many existing pseudo-likelihood based robust Bayesian methods, our formulation enjoys a concrete probabilistic interpretation.
In the following, we offer a concrete probabilistic justification to $\mbox{D}$-BETEL by building a Bayesian hierarchical generative model centered around $F_\theta$ so that the marginal posterior of $\theta$ converges in distribution to the $\mbox{D}$-BETEL posterior under a limiting environment motivated by \citet{10.1093/biomet/92.1.31}. This result is established in Theorem \ref{th:npbayes_equivalence}. 

In the following, we first describe a generative model for the data points $x_1, \ldots, x_n$ which closely mimics commonly used Bayesian nonparametric methods such as the mixture of finite mixture of Gaussians. The description  proceeds via a probability model for the independent $d$-variate observations $x_i$ conditional on its own set of parameters $\eta_i \in \mb R^d$, i.e., $x_i \mid \eta_i \overset{ind.} \sim f(\cdot \mid \eta_i)$ for $i = 1, \ldots, n$. To impart flexibility, the random effects $\eta_i$ are independently drawn from a common {\em mixing measure}
%, denoted by the {\em mixing measure} 
$P^{(N)}$ defined on $(\mathbb{R}^d, \mathcal{B}(\mathbb{R}^d))$, where $N$ is a positive integer involved in the description of $P^{(N)}$. This renders the marginal density of $x_i \mid P^{(N)}$ to be $\int f(x_i \mid \eta_i) P^{(N)}(d \eta_i)$, independently for $i = 1, \ldots, n$. The mixing distribution $P^{(N)}$ is parameterized 
%is assigned a flexible distribution 
through its associated nuisance parameters 
%$\xi^{\star} = (k,\ b_1,\ldots, b_k,\ \mu_{1},\ldots,\mu_{k})^{\T} \in \mb N \otimes  \{b_1,\ldots, b_k: b_h \in \mathbb{N}, h = 1,\ldots, k; \ \sum_{h=1}^k b_h = N \}\otimes \mb (\mathbb{R}^d)^k$. 
$\xi^\star = \big(k, b, \{\mu_h\}_{h=1}^k\big)$, where $k \in \mb N$, $b = (b_1, \ldots, b_k)$ where each $b_h$ is a positive integer subject to the constraint $\sum_{h=1}^k b_h = N$, and $\mu_h = (\mu_{h,1}, \ldots, \mu_{h, d})^{\T}\in \mb R^d$ for $h = 1, \ldots, k$. We induce a prior distribution on $P^{(N)}$ through a prior distribution on $\xi^\star$. To do so, 
%The prior on $\xi^{\star}$ is denoted $\pi_{\infty, N}(\cdot)$. Hence $P^{(N)}$ becomes a random probability measure, a common terminology in the Bayesian nonparametric literature.  Finally, 
we construct  a joint prior on $(\xi^{\star}, \theta)$ hierarchically by first specifying the marginal prior on the parameter of interest $\theta$, and then the conditional prior of $\xi^{\star} \mid \theta$ in terms of a $\theta$ dependent slice on the support of an unconditional distribution $\pi_{\infty, N}(\cdot)$ for $\xi^\star$. In essence,  $\xi^*$ act as a {\em bridge} between the data and the parameter of interest $\theta$ in the hierarchical formulation. 
%\textcolor{blue}{
This is where our modeling departs from a typical non-parametric Bayes model where $P^{(N)}$ is the object of inference and $\theta$ is viewed as a derived quantity from $P^{(N)}$. Instead, in our framework, $\theta$ retains its own identity and $P^{(N)}$ is viewed as an infinite-dimensional nuisance parameter. In other words, $(P^{(N)}, \theta)$ describes a semi-parametric object for inference, where $P^{(N)}$ is a flexible  probability measure, and $\theta$ is the parameter of interest.
%}


We specify the details for each of these pieces from top down in the sequel. 
\begin{comment}
We posit a flexible random probability measure $P^{(N)}$ in sequel. Given nuisance parameters $\xi^\star$, $P^{(N)}\mid \xi^{\star}$ describes an unknown mixing measure, and we set an unconstrained prior $\pi_{\infty, N}(\cdot)$ on $\xi^{\star}$. 
\end{comment}
First, the distribution $f$ of the data given random effects is chosen to be an appropriate uniform distribution. Specifically, given $\tau > 0$, let
%data generation mechanism is specified by a finite mixture model:
\begin{equation}\label{eqn:npbayes3}
\begin{aligned}
 x_i &\mid \eta_i, P^{(N)}\ \stackrel{\text{ind.}}\sim\ \prod_{l=1}^d \mbox{Uniform}(\eta_{i, l} -  \tau^{-1},\ \eta_{i, l} +  \tau^{-1}), \ i=1,\ldots, n,  \\
\eta_i &\mid P^{(N)} \sim  P^{(N)}.
\end{aligned}
\end{equation}
The uniform kernel is chosen for analytic tractability in ensuing calculations. We expect the main results to hold for more general kernels, albeit with additional technical challenges.
Next, for any set $A \in \mathcal{B}(\mathbb{R}^d)$, define $P^{(N)}(A)$ as  $P^{(N)}(A) = \sum_{h=1}^k \pi_h \delta_{\mu_h}(A)$, where conditional on $k$, the mixture weights $(\pi_1, \ldots, \pi_k)$ are constructed via normalised  counts $(b_1/N,\ldots, b_k/N)$. 
%instead of direct draws from a Dirichlet distribution, commonly used in the finite dimensional version of the Dirichlet process \citep{ishwaran2002dirichlet,ishwaran2002exact} or in the mixture of finite mixtures setup \citep{doi:10.1080/01621459.2016.1255636}.
%Given mixture weights $(b_1/N,\ldots, b_k/N)$, the cluster indicators are drawn independently from $\{1,\ldots, K\}$.
We specify distributions on the pieces to define an {\em unconditional distribution} $\pi_{\infty, N}(\cdot)$ for $\xi^*$, 
\begin{equation}\label{eqn:npbayes1}
\begin{aligned}
&(b_1,\ldots, b_k) \mid k \ \sim \ \mbox{Multinomial}(N; 1/k,\ldots, 1/k)\\
&\mu_h \mid k\ \stackrel{\text{i.i.d.}} \sim \ \mbox{H}^{(N)} , \ h=1,\ldots, k;\quad  k \sim p(k)\equiv \mbox{Geometric}(p);
%& \xi_j \mid \mu_{1:k}, k\ \stackrel{\text{ind.}} \sim\ k^{-1} \sum_{h=1}^k \delta_{\mu_{h}},\ j=1,\ldots, N,\ %N>>k
\end{aligned}
\end{equation}
where  
%$\mu_h = (\mu_{h,1}, \ldots, \mu_{h, d})^{\T}, \ h = 1,\ldots, k$, and 
$\mbox{H}^{(N)}$ is a suitably chosen $d$-dimensional ``base'' distribution; refer to \eqref{eqn:assumptions} for details. Given a draw of $k$, the $k$ atoms $\{\mu_h\}_{h=1}^k$ are drawn 
independently  from $\mbox{H}^{(N)}$, and the count vector $(b_1,\ldots, b_k)$ that yields the mixture weights is drawn from $\mbox{Multinomial}(N; 1/k,\ldots, 1/k)$; instead of  direct draws of the mixture weights from a Dirichlet distribution, commonly used in the finite dimensional version of the Dirichlet process \citep{ishwaran2002dirichlet,ishwaran2002exact}, or in the mixture of finite mixtures setup \citep{doi:10.1080/01621459.2016.1255636}.
The distributional specification $\pi_{\infty, N}$ for $\xi^\star$ 
%probability model in \eqref{eqn:npbayes3} along with 
%prior specification 
in \eqref{eqn:npbayes1} induces a mixture of finite mixtures (MFM; \citet{doi:10.1080/01621459.2016.1255636}) for $P^{(N)}$ given by $P^{(N)} = \sum_{k=1}^\infty p(k) \big[\sum_{h=1}^k (b_h/N) \delta_{\mu_h}\big]$.  
%Here $(P^{(N)}, \theta)$ describes a semi-parametric object for inference, where $P^{(N)}$ is a flexible random probability measure, and $\theta$ is the parameter of interest.

Finally,  we construct a joint prior on $(\xi^{\star}, \theta)$ by first specifying a prior distribution $\pi(\cdot)$ on $\theta$, and then the conditional distribution of  $\xi^* \mid \theta$ by restricting the distribution $\pi_{\infty, N}(\cdot)$ to the slice 
$$
A_{\varepsilon,N}(\theta) :\, = \{\xi^{\star} \,:\, \mbox{D}(P^{(N)}, F_{\theta}) < \varepsilon \}
$$
defined on the support of $\xi^{\star}$, where the metric $\mbox{D}$ and the scalar $\varepsilon > 0$ are as in \eqref{eqn:betel}. Thus, 
\begin{align}\label{eqn:npbayes2}
 \pi_{\varepsilon, N}(\xi^{\star} \mid \theta) \ \propto \ \pi_{\infty, N}(\xi^{\star}) \ 1_{A_{\varepsilon, N}(\theta)}(\xi^{\star}),
\end{align}
that is, given a specific value of $\theta$, only draws from the unconditional prior $\pi_{\infty, N}$ are retained for which $P^{(N)}$  and $F_\theta$ are $\varepsilon$-close under the metric $\mbox{D}$. Figure \ref{fig:cartoon_npbayes} presents a schematic of the hierarchical model in equations \eqref{eqn:npbayes3}--\eqref{eqn:npbayes2}.

Combining the joint prior  $\pi_{\varepsilon,N}(\xi^\star, \theta)$ %on $(\xi^\star, \theta)$  combined 
with the 
%probability model $P^{(N)}(\cdot \mid \xi^\star)$ in
generative model in \eqref{eqn:npbayes3}, one obtains the joint posterior distribution $\pi_{\varepsilon, N}(\theta, \xi^\star\mid x_{1:n})$ of $(\theta, \xi^\star)$. A fully Bayesian analysis of the posterior of $\pi_{\varepsilon, N}(\theta, \xi^\star\mid x_{1:n})$ entails traversing the gigantic parameter space of $(\xi^\star, \theta)$ to simultaneously learn  $(P^{(N)}, \theta)$. Instead, motivated by \citet{10.1093/biomet/92.1.31},
we marginalise $\pi_{\varepsilon, N}(\theta, \xi^\star\mid x_{1:n})$ with respect to nuisance parameters $\xi^\star$ to obtain  the marginal posterior $\pi_{\varepsilon, N}(\theta\mid x_{1:n})$, that enables us to access targeted inference on the parameter of interest $\theta$.  In the remainder of this section, we shall operate in an asymptotic regime motivated by \citet{10.1093/biomet/92.1.31}, where we let the hyperparameters $\tau \equiv \tau(N), p \equiv p(N)$ and the base-measure $H^{(N)}$ to evolve with $N$. Under this environment, we show below that the marginal posterior $\pi_{\varepsilon, N}(\theta\mid x_{1:n})$ converges to \eqref{eqn:wbetelpost} as $N \to \infty$. \\


%\clearpage
\begin{comment}
Our flexible generative model $P^{(N)} \equiv P^{(N)}(\cdot \mid \xi^{\star})$ for the data is characterized by 
%the parameter of interest $\theta$, and 
a collection of nuisance parameters 
$\xi^{\star} = (k,\ \mu_{1:k})^{\T} \in \mb N \otimes \mb R^k$, whose interpretation is provided in the sequel. We construct a joint prior on $(\xi^\star, \theta)$ hierarchically by specifying the marginal prior of $\theta$, and then the conditional prior of $\xi^\star \mid \theta$. The nuisance parameters thus act as a {\em bridge} between the data and the parameter of interest $\theta$ in the hierarchical formulation. We specify the pieces from the ground up below. 

Let $\pi(\cdot)$ denote our prior distribution on $\theta$. The conditional distribution of  $\xi^{\star} \mid \theta$ is constructed by restricting a unconditional prior $\pi_{\infty, N}(\cdot)$ to the slice $$
A_{\varepsilon,N}(\theta) :\, = \{\xi^{\star} \,:\, D(P^{(N)}, F_{\theta}) < \varepsilon \}
$$
defined on the support of $\xi^{\star}$, where the metric $\mbox{D}$ and the scalar $\varepsilon > 0$ are as in \eqref{eqn:betel},
\begin{align}\label{eqn:npbayes2}
 \pi_{\varepsilon, N}(\xi^{\star} \mid \theta) \ \propto \ \pi_{\infty, N}(\xi^{\star}) \ 1_{A_{\varepsilon, N}(\theta)}(\xi^{\star}).
\end{align}
Thus, given a specific value of $\theta$, only draws from the unconditional prior $\pi_{\infty, N}$ are retained for which $P^*$ and and $F_\theta$ are $\varepsilon$-close under the metric $\mbox{D}$. 



%first provide details for the hierarchical model and then 
% establish in Theorem \ref{th:npbayes_equivalence} that under a suitable asymptotic regime motivated by \citep{10.1093/biomet/92.1.31}, the hierarchical model converges to the D-BETEL posterior for any fixed sample size.  To that end, given data $x_1, \ldots, x_n$,
% %arising from an unknown data generation mechanism $P$, 
% %D-BETEL performs inference on $\theta$, implicitly defined through $W_{\rm AR}(P\ || \ F_{\theta}) < \varepsilon$, where $F_{\theta}$ is a parametric family of distributions, and $\varepsilon>0$ is a hyper-parameter. To that end, 
% we construct a flexible generative model $P^{(N)}(\cdot \mid \xi^{\star}, \theta)$, and a prior on $(\xi^{\star},\theta)$ as follows. Here $\theta$ is the model parameter that we wish to infer about (with marginal prior $\pi(\theta)$), and $\xi^{\star} = (k,\ \mu_{1:k},\ \xi_{1:N})^{\T}$ represents a set of nuisance parameters of this hierarchical model. 
We set the unconditional prior $\pi_{\infty, N}(\cdot)$ on $(k, \mu_{1: k})$, as a mixture of finite mixtures (MFM),
\begin{align}%\label{eqn:npbayes1}
%\begin{aligned}
& k \sim p(k)\equiv \mbox{Geometric}(p);\quad
\mu_h \mid k\ \stackrel{\text{i.i.d.}} \sim \ \mbox{H}^{(N)} , \ h=1,\ldots, k;%\notag \\
%& \xi_j \mid \mu_{1:k}, k\ \stackrel{\text{ind.}} \sim\ k^{-1} \sum_{h=1}^k \delta_{\mu_{h}},\ j=1,\ldots, N,\ %N>>k
%\end{aligned}
\end{align}
where  $\mu_h = (\mu_{h,1}, \ldots, \mu_{h, d})^{\T}, \ h = 1,\ldots, k$, and $\mbox{H}^{(N)}$ is a suitably chosen $d$-dimensional ``base'' distribution; refer to \eqref{eqn:assumptions} for details. Given a draw of $k$, the $k$ atoms $\{\mu_h\}_{h=1}^k$ are drawn 
independently  from $\mbox{H}^{(N)}$.
%, and $\xi_j$ is set to $\mu_h$ with probability $1/k$ for $h = 1, \ldots, k$, independently across $j$.   

%The conditional distribution of  $\xi^{\star}$  given $\theta$ is constructed by restricting the unconditional prior $\pi_{\infty, N}(\xi^*)$ to the slice   $A_{\varepsilon,N}(\theta) :\, = \{\xi^{\star} \,:\, D(P^{(N)}\ , \ F_{\theta}) < \varepsilon \}$ defined on the support of $\xi^{\star}$. Intuitively, conditional on $\theta$, only draws from the prior  $\pi_{\infty, N}$ are retained for which $P^*$ and and $F_\theta$ are close, i.e.,  
% \begin{align}\label{eqn:npbayes2}
%  \pi_{\varepsilon, N}(\xi^{\star} \mid \theta) \ \propto \ \pi_{\infty, N}(\xi^{\star}) \ 1_{A_{\varepsilon, N}(\theta)}(\xi^{\star}).
% \end{align}

Equation \eqref{eqn:npbayes2} together with the prior $\pi(\cdot)$ on $\theta$ defines a joint prior on $(\xi^\star, \theta)$, which we again denote by $\pi_{\varepsilon,N}(\xi^\star, \theta)$, with a slight abuse of notation. Finally, the data generation mechanism is completed by
\begin{align}%\label{eqn:npbayes3}
%&z_1,\ldots, z_n  \stackrel{\text{i.i.d.}} \sim\ \mbox{Categorical}\big(1:N\mid 1/N,\ldots,1/N\big);\notag\\
&(b_1,\ldots, b_k) \mid k \ \sim \ \mbox{Multinomial}(N; 1/k,\ldots, 1/k),\notag\\
&z_1,\ldots, z_n\mid b_1, \ldots, b_k  \stackrel{\text{i.i.d.}} \sim\ \mbox{Categorical}\big(1:k\mid b_1/N,\ldots,b_k/N\big),\notag\\
%& x_i \mid z_{1:n},  \xi_{1:N}\ \stackrel{\text{ind.}}\sim\ \prod_{l=1}^d \mbox{Uniform}(\xi_{z_i, l} -  \tau^{-1},\ \xi_{z_i, l} +  \tau^{-1}), \ i=1,\ldots, n,
& x_i \mid z_{1:n},  \mu_{1:k}\ \stackrel{\text{ind.}}\sim\ \prod_{l=1}^d \mbox{Uniform}(\mu_{z_i, l} -  \tau^{-1},\ \mu_{z_i, l} +  \tau^{-1}), \ i=1,\ldots, n.
\end{align}
The probability model $P^{(N)} = \sum_{k=1}^\infty p(k) \sum_{h=1}^k (b_h/N) \delta_{\mu_h}$ together with the prior on $b_{1:k}$ and $\mu_{1:k}$ describe a mixture of finite mixtures (MFM).
Combining the joint prior  $\pi_{\varepsilon,N}(\xi^\star, \theta)$ %on $(\xi^\star, \theta)$  combined 
with the probability model $P^{(N)}(\cdot \mid \xi^\star)$ in \eqref{eqn:npbayes3}, one obtains the joint posterior distribution $\pi_{\varepsilon, N}(\theta, \xi^\star\mid x_{1:n})$ of $(\theta, \xi^\star)$, which upon marginalization with respect to $\xi^\star$ yields the marginal posterior $\pi_{\varepsilon, N}(\theta\mid x_{1:n})$. 
%\textcolor{red}{[what is $P$ here? describe clearly.]}
%Here,  $P(\cdot\mid\xi^{\star})$ is the data generating mechanism, and $\pi(\theta)$ is the prior on $\theta$. 
%For $\varepsilon> 0$, $A_{\varepsilon}(\theta) :\, = \{\xi^{\star} \,:\, W_{\rm AR}(P\ || \ F_{\theta}) < \varepsilon \}$. 
%$\pi_{\varepsilon}(\xi^{\star}\mid\theta)$  denotes the $(\theta,\varepsilon)$ restricted prior on $\xi^{\star}$. 
In the remainder of this section, we shall be working in a special asymptotic regime motivated by \citep{10.1093/biomet/92.1.31}. Under this limiting environment, we show below that the marginal posterior $\pi_{\varepsilon, N}(\theta\mid x_{1:n})$ converges to \ref{eqn:wbetelpost} as $N \to \infty$. \\
%\textcolor{blue}{Insert a cartoon here}\\
%\clearpage
\end{comment}

\begin{figure}
\centering
    %\subfloat%[\centering entropy-based portfolio allocation]
    %{{
    \includegraphics[width=17cm, height = 4cm]{art/figures/cartoon_npbayes.pdf} 
    %}}
    \caption{\emph{\textbf{Left panel.} the pdf $f_{\theta}(x)$ corresponding to the centering distribution $F_{\theta}\equiv 0.6\times\mbox{N}(-1, 0.5^2) + 0.4\times\mbox{N}(1, 0.5^2)$ in black and representative discrete distributions in a $\mbox{D}$-neighborhood (with D chosen as  $\mbox{W}_{\rm AR}$ introduced in Section \ref{ssec:andrew}) of $F_{\theta}$ in red after kernel smoothing;  \textbf{Middle panel.} one particular  $P= \sum_{h=1}^k \pi_h \delta_{\mu_h}$ with $k=20$ in the $\mbox{W}_{\rm AR}$-neighbourhood of $F_{\theta}$ with $W_{\rm AR}^2(P, F_\theta) = 2.5$; \textbf{Right panel.} histogram of a random sample of size $100$ drawn from $f_P(x) = \int f(x \mid \eta) P(d \eta)$ with $\tau = 10^{2}$ in equation \eqref{eqn:npbayes3}}.\label{fig:cartoon_npbayes}}%
    %\quad
    %\subfloat[\centering 5]{{\includegraphics[width=5.5cm]{Toy_WassersteinGF.pdf} }}%
\end{figure}

\noindent \textbf{Assumptions:}
We consider the following constructions on $p=p(N), \tau = \tau(N)$, and the base measure $\mbox{H}^{(N)}$.
First, we introduce sequences $(K_N, L_N) = (N^{\alpha}, N^{\beta}) $ with $\alpha\in \mb{R}^{+}, \beta\in \mb{Z}^{+}, \alpha > \beta$ %such that $K_N, L_N\to\infty$ and $L_N/K_N\to 0$, as $N\to\infty$
. Now, let $\mathcal{X}^N$ be the mid-points of the cells of an uniform $(K_N + 1)(K_N + 1)\ldots(K_N + 1)$ grid on the hyper-cube $\big[-L_N/2, L_N/2 \big]^{d}$, with the total number of grid-points $M_N = (K_N+1)^{d}$ and the grid spacing $2\rho_N = L_N/K_N $. We then set 
\begin{align}\label{eqn:assumptions}
&\mbox{H}^{(N)}\equiv\ \mbox{Uniform}(\mathcal{X}^N), \quad%\notag\\
%&\mbox{H}^{(N,\ k)}(\mu_1,\ldots, \mu_k)\ \propto \ \frac{1}{k^N}\ \frac{N!}{\prod_{h=1}^k b_h!}\  \prod_{h=1}^k \delta\{\mu_{h}\in \mathcal{X}^N\},%\notag\\
\tau(N) = \frac{1}{\rho_N}, \quad  p(N)=1- \frac{1}{AM_N^{N+1}};
\end{align}
for some $A>0$. 
%on $\mu$.
%\textbf{(A-2)} $\mbox{H}_1(\cdot\mid \mu_h)$ is a dirac delta measure at $\mu_h$, $h=1,\ldots, k$, \textbf{(A-3)}  $f(x\mid\mu,\tau^{-1})$ is $\mbox{N}(\mu,\tau^{-1} \mbox{I})$. 

The assumptions above place specific structures on $p=p(N),\ \tau = \tau(N)$ and the base measure $\mbox{H}^{(N)}$ that make aspects of the model-prior increasingly diffuse while also yielding tractable analytic calculation of the marginal posterior of $\theta\mid x_{1:n}$. In particular, under the above construction, $p = p(N)$ converges to $1$, rendering increased penalty on the number of components $k$, and $\tau = \tau(N)$ diverges to $\infty$, allowing the support of the uniform kernel to shrink, as $N \to \infty$.
Moreover, $\mbox{H}^{(N)}$ is a flat  prior on a compact set which expands with $N$. These assumptions share key structural commonalities with the assumptions for the main result in \citet{10.1093/biomet/92.1.31} 
%regarding non-parametric Bayes interpretation of Bayesian ETEL with moment condition models. 
In particular, \citet{10.1093/biomet/92.1.31} also considers sequences $(K_N, L_N)$ such that $K_N, L_N\to\infty$ and $L_N/K_N\to 0$ as $N\to\infty$. We make further specific choices of $K_N, L_N$ in order to simplify the expressions of various posterior quantities in the hierarchical specification (\eqref{eqn:npbayes3}--\eqref{eqn:npbayes1}), and provide rigorous asymptotic results. This ensures improved clarity in the proofs of the next two theorems while maintaining generality of the arguments. 
 

\begin{theorem}\label{th2:lemma2}
Fix sample size $n$. Under the hierarchical specification in \eqref{eqn:npbayes3}--\eqref{eqn:npbayes2} and assumptions in  \eqref{eqn:assumptions}, as $N\to\infty$, the marginal posterior probability $P(k=n\mid x_{1:n})\to 1 $ almost surely.
\end{theorem}
Theorem \ref{th2:lemma2} is an important building block for our main result in Theorem \ref{th:npbayes_equivalence} below. %regarding the equivalence of marginal posterior of $\theta$ under our mixture of the finite mixture model and $\mbox{D}$-BETEL, under our asymptotic regime. 
As noted earlier, Assumption \eqref{eqn:assumptions} imply that under the hierarchical specification in \eqref{eqn:npbayes1}, 
%$\xi_{1:N}$ are simply categorical random variables with 
(a) the number of atoms $\mu_h$s drawn from the base measure $\mbox{H}^{(N)}$ is controlled by the prior on $k$ that strongly encourages small support, i.e smaller number of unique atoms, and  (b) the atoms $(\mu_1,\ldots, \mu_k)$ take values in a uniform grid $\mathcal{X}^N\subset\mb{R}^d$ that expands to $\mb{R}^d$  while getting increasingly dense as $N\to\infty$. Together with \eqref{eqn:npbayes3}--\eqref{eqn:npbayes1}, (a) ensures that the $P(k=n\mid x_{1:n}) = 1$  almost surely and $\eta_{1:n}$ 
%\textcolor{red}{($\mu$ should be $\eta$?)}
collapses on the observed sample as $N\to \infty$, and  (b) ensures that the hierarchical model describes an extremely flexible generative model. The flexible yet discrete nature of $\mathcal{X}^N\subset\mb{R}^d$ critically simplifies the arguments in the proof of Theorem \ref{th:npbayes_equivalence} below.

\begin{theorem}\label{th:npbayes_equivalence}
Fix the concentration parameter $\varepsilon>0$ and sample size $n$.  Suppose $\mbox{H}^{(N)},\ p$ and $\tau$ satisfy the assumptions stated above as $N \to \infty$. Then the marginal posterior $\pi_{\varepsilon, N}(\theta\mid x_{1:n})$ defined after \eqref{eqn:npbayes3}--\eqref{eqn:npbayes2} converges point-wise in $\theta$ to the $\mbox{D}$-BETEL posterior in equation \eqref{eqn:wbetelpost},
\begin{align*}
|\pi_{\varepsilon, N}(\theta\mid x_{1:n}) - \pi(\theta\mid x_{1:n})|\to 0 \quad \text{as}\quad N\to \infty.
\end{align*}
An application of Scheffe's theorem \citep{Resnick} yields, 
\begin{align*}
    \| \pi(\cdot \mid x_{1:n}) - \pi(\cdot \mid x_{1:n}) \|_{\rm TV} :\,= \frac{1}{2} \int_{\theta}|\pi_{\varepsilon, N}(\theta\mid x_{1:n}) - \pi(\theta\mid x_{1:n})| d\theta \to 0 \quad \text{as}\quad N\to \infty.
\end{align*}
\end{theorem}

%\textcolor{blue}{Point out the major differences with Shennach}. 
The proof of Theorems \ref{th2:lemma2} and \ref{th:npbayes_equivalence} and the required results are deferred to Section \ref{ssec:th1th2_proof} of the supplementary document. Instead, we discuss the key takeaway messages that the theorems expose. Perhaps most importantly, Theorem \ref{th:npbayes_equivalence} enables us to formally recognize $\mbox{D}$-BETEL as a limiting non-parametric Bayes posterior. Moreover, the formulation in equations \eqref{eqn:npbayes3}--\eqref{eqn:npbayes2} allows decoupling of the parameter of interest $\theta$ and the potentially infinite-dimensional nuisance parameter $\xi^\star$, tying them together through the slicing. The decoupling enables us explicitly quantify prior information on $\theta$ through prior $\pi(\theta)$ in our implicitly specified models, and the slicing potentially provides an efficient scheme to navigate the enormous $(\xi^{\star},\theta)$ space. We hope that this framework of providing the parameter of interest $\theta$ with its own identity in implicitly specified models and making the nuisance parameter $\xi^{\star}$ dependent on $\theta$ opens up possibilities of developing non-parametric Bayes models tailored to perform inference on specific parameters going forward.

It is also important to point out that the asymptotic regime allows the effect of the prior $P^{(N)}$ to get washed away for any finite $n$ so that the resulting posterior distribution of $P^{(N)}$ converges to the empirical distribution subject to the distributional constraint. As a consequence, the induced posterior distribution on $\theta$ converges to the posterior obtained from the empirical likelihood.  Although in principle, the mixture of finite mixture prior for $P^{(N)}$ can be replaced any other distribution whose effect is allowed to weaken under the asymptotic regime, the MFM construct is a natural choice that allows the $P^{(N)}$ to asymptotically be degenerate at the observed data points.


%\textcolor{blue}{End this section with a discussion comparing your proposal with DP mixture models. Your $\varepsilon$ plays a similar role as the Dirichlet concentration parameter $\alpha$. You may add the hyperparameter tuning stuff here to broadcast that you have a concrete approach to select $\varepsilon$. Also, there is recent work on learning parameters in the base measure of DP (Jeff Miller) -- not a fan of that work since it amounts to solving a deconvolution problem which is statistically hard.}

Another key revelation from the presentation above is the fact that the hyper-parameter $\varepsilon$ bears clear similarity to the concentration parameter in a Dirichlet process \citep{Ferguson, teh2010dirichlet}, as it determines how tightly $\nu_{w,x}$ sits around $F_{\theta}$ with respect to $\mbox{D}$. Since the $\mbox{D}$-BETEL formulation enjoys concrete probabilistic interpretation, we are able to provide a principled guideline for hyper-parameter $\varepsilon$. To that end we recall that, given $x_1,\ldots,x_n$, the Bayesian leave-one-out estimate of out-of-sample predictive fit \citep{JMLR:v17:14-540} is 
\begin{align*}
 \ELPD_{\varepsilon} =   \sum_{i=1}^n \ \log \pi(x_i\mid x_{-i})\quad \text{where}\quad  \pi(x_i\mid x_{-i}) = \int \pi(x_i\mid\theta)\ \pi(\theta\mid x_{-i})\ d\theta   
\end{align*}
is the leave-one-out predictive density given the data without the $i$-th data point, and the corresponding standard error is 
\begin{align*}
\SE[\ELPD_{\varepsilon}] = \sqrt{n}\ \sqrt{\Var[\ \log \pi(x_1\mid x_{-1}),\ldots, \log \pi(x_n\mid x_{-n})\ ]}.    
\end{align*}
 When $\varepsilon$ is too large, the distance-based restriction does not kick in and estimated $\SE[\ELPD_{\varepsilon}]$ is close to $0$. So, we consider a decreasing sequence of $\varepsilon$ values, say $\varepsilon_1,\ldots,\varepsilon_h$, such that $\varepsilon_i > \varepsilon_j \ \forall \ 1 \leq i<j\leq h$. A general strategy to select the sequence is to first consider a grid over powers of $2$ and then use a finer grid in the interval where $\ELPD_{\varepsilon}$ undergoes steep change. Suppose $\varepsilon_{h_0}$ is the largest value of $\varepsilon$ for which the distance-based restriction is active. Then, our estimate of the model parameter $\theta$ is 
\begin{align}\label{eqn:ma}
  \hat{\theta}_{\rm MA} = \sum_{i=h_{0}}^h\kappa_i\hat{\theta}_i ,\quad \text{with}\quad \kappa_i = \frac{\exp(-\ELPD_{\varepsilon_{i}})}{\sum_{j=h_{0}}^h \exp(-\ELPD_{\varepsilon_{j}})}
\end{align}
where $\hat{\theta}_i$ and $\exp(-\ELPD_{\varepsilon_{i}})$ are the parameter estimate and estimated $\ELPD$  at $\varepsilon=\varepsilon_i$ respectively. 
%The lack of valid probabilistic interpretation of Gibbs type  posteriors poses a considerable challenge in  tuning the associated temperature parameter, and it has attracted a proliferation of excellent literature \cite{martin2019, safebayes} in recent past.  
%The basic motivation for using ETEL over a Gibbs type posterior is indeed the probability interpretation that goes with it. We devised  the ELPD based model averaging to tune the hyper parameter $\varepsilon$. 
From the definition of ELPD, we can interpret it as a measure of the extent of unequal weighting of the observations. In the presence of contamination, our approach of selecting the hyper-parameter promotes  unequal weighting of the observations to ensure  -- under weighting of outlying observations, and over-weighting observations around the ``center". This inbuilt mechanism of ensuring immunity against outliers while maintaining  a valid generative model interpretation is what sets our method apart from lot of the existing pseudo-likelihood based approaches. Finally, it is also important to point out that, although $\hat{\theta}_{\rm MA}$ in equation \eqref{eqn:ma} is calculated via an weighted average, in practice $\hat{\theta}_{\rm MA}$ and the associated HPD set typically degenerate to those corresponding to a handful of values of $\varepsilon$. Thus this procedure inherits the generative model interpretation of $\mbox{D}$-BETEL.  

The rest of this section is devoted to completing the specifications in our proposed $\mbox{D}$-BETEL formulation in \eqref{eqn:wbetel}-\eqref{eqn:wbetelpost}. To be precise, we discuss what would be a judicious choice of the centering parametric family $F_{\theta}$, together with the distance metric $\mbox{D}$ between $F_{\theta}$ and $\nu_{w,x}$, that remains computationally feasible across the ensuing applications.


%\textcolor{blue}{Now, create a new Section for ANDREW (current 2.2). The first para of this should be the para surrounding eq. 2.3. Note that our definition of ANDREW is linked with our choice of EMMs as the centering family, and this needs to be clarified. This should be pretty seamless otherwise. Copy the content of 2.2.} 
\subsection{ANDREW: an AugmeNteD \& REstricted  Wasserstein metric}\label{ssec:andrew}
To offer flexibility in the centering mechanism, we seek that the family of centering distributions describe a large and expressive class of models. To that end, we suggest employing Elliptical Mixture Models (EMM) as a general choice of the centering distribution $F_{\theta}$. Given a vector $m \in \mb R^d$, a positive (semi-)definite scale matrix $\Sigma \in \mb R^{d \times d}$, and a generator function $h: (0, \infty) \to (0, \infty)$, the elliptical distribution $\mbox{ED}_{h}(m,\Sigma)$ is defined to be the distribution with characteristic function 
\begin{align}\label{eqn:ch_EMM}
    t\rightarrow\exp(i t^\T \mu)\ h(t^\T\Sigma t), \quad t \in \mathbb{R}^{d}.
\end{align}
Recall that a multivariate Gaussian distribution $\mbox{N}_d(m,\Sigma)$ has characteristic function 
$\exp(i t^\T\mu)\ h(t^\T\Sigma t)$ for $t \in \mb{R}^d$ where $h(z) = \exp(-z/2)$ for $z>0$. Elliptical distributions \citep{muirhead2005aspects} %\textcolor{red}{[cite Muirhead multv. book]}
allow a wider class of positive functions $h$. 
A mixture of such elliptical distributions, $\sum_{k=1}^{K_{0}} s_{0k}\ \mbox{ED}_h(m_k,\Sigma_k)$, provides a flexible tool for statistical modeling \citep{Cambanis1981OnTT, 10.2307/4616956} and probabilistic embedding of complex objects \citep{muzellec2019generalizing, le2019treesliced}. Consequently, elliptical mixture models (EMM) serve as an attractive candidate for our parametric centering family. With this choice, we now discuss the construction of a novel statistical distance $\mbox{D}[\cdot ,\cdot ]$ that is expressive and easy to calculate between the centering elliptical mixture model and the weighted empirical distribution of the observed data.

%\textcolor{blue}{Is this the right reference next line? Also, this line does not make sense. By definition, we need a distance between empirical and $F_\theta$. Commenting it out.}
%The exponentially tilted empirical likelihood (ETEL) subject to a distance-based constraint is a discrete distribution supported on the observed data points \citep{doi:10.1198/016214501750332848}. 
As discussed earlier, we require a statistical distance $\mbox{D}$ that (i) returns a non-trivial distance between a discrete and a continuous distribution. For the ensuing applications we also require that $\mbox{D}$: (ii) allows a straightforward multivariate extension; (iii) is computationally feasible; and (iv) effectively captures the tail behavior of the distributions. The requirement (i) itself rules out the applicability of many popular statistical distances/divergences like the Kullback--Leibler  divergence, Hellinger distance, total variation distance, $\chi^2$ distance, etc. The Cramer--von Mises metric on $\mb R$ satisfies (i), (iv), but its multivariate extension is not immediate. The $p$-Wasserstein metric \citep{Villani2003TopicsIO} satisfies (i), (ii), and (iv), and is an attractive candidate. However, it remains computationally challenging in multivariate examples we care about. This motivates the need for a specialized adaptation of the Wasserstein metric. To that end, we recall some relevant facts about the $p$-Wasserstein metric first.    
\begin{definition}
For $p\geq 1$, the Wasserstein space $\mb{P}_p(\mb{R}^d)$ is defined as the set of probability measures $\mu$ with finite moment of order $p$, i.e $\{\mu \,: \, \int_{\mb{R}^d} \left\lVert x\right\rVert^p d\mu(x) < \infty\}$, where $\left\lVert \ \cdot \ \right\rVert$ is the euclidean norm on $\mb{R}^d$.
\end{definition}
\begin{definition}
For $p_0, p_1 \in \mb{P}_p(\mb{R}^d)$, let $\pi(p_0, p_1) \subset \mb{P}_p(\mb{R}^d\times\mb{R}^d) $ denote the subset of joint probability measures (or {\em couplings}) $\nu$ on $\mb{R}^d\times\mb{R}^d$ with marginal distributions $p_0$ and $p_1$, respectively. Then, the $p$-Wasserstein distance $W_p$ between $p_0$ and $p_1$ is defined as 
\begin{align}\label{eq:W_def}
 W_{p}^{p}(p_0, p_1)
= \inf_{\nu\in \pi(p_0,p_1)} \int_{\mb{R}^d\times\mb{R}^d}\
\left\lVert y_0 - y_1 \right\rVert^p\ d\nu(y_0,y_1).  
\end{align}
\end{definition}

\begin{figure}[!htb]
%\figurebox{1pc}{5pc}{}[art/figures/WAR.pdf]
\includegraphics[width=1\textwidth]{art/figures/WAR.pdf}
\caption{\emph{The augmentation and restriction scheme to construct $R^{\alpha}(p^{\star}_0,p^{\star}_1)$; left: construction of $\big\{\pi(p_0,p_1)\cap \EMM^{\alpha}_{2d}(K_0 K_1)\big\}$ ; right: construction of $\big\{\otimes_{i=1}^d \pi(p_{0i},p_{1i})\big\}$ with $d=2$.} %\textcolor{red}{[the figure needs to go right before/after Def 1]}
}\label{fig:andrew_cartoon}
\end{figure}

In the one-dimensional case,  $W_p$ has a closed-form expression as the $L_p$ distance between the corresponding quantile functions \citep{noauthororeditor}.
However, such closed-form expressions for $d\geq 2$ are unavailable except for a few special cases, and numerical approximations, while available, are computationally expensive. \citet{cuturi2013sinkhorn} demonstrated that regularizing the minimization (or transport) problem \eqref{eq:W_def} with an entropic penalty provides an efficient numerical approach to compute the Wasserstein metric in discrete cases. There also has been a recent line of work \citep{delon:hal-02178204, BionNadal2019OnAW} modifying the 2-Wasserstein metric via restricting the class of coupling measures to a carefully chosen sub-family, which considerably reduces the computational cost and yet encompasses a rich class of coupling measures. In particular, \citet{delon:hal-02178204} proposed a modified Wasserstein metric with $L_2$ cost between two Gaussian mixture models in $\mb{R}^d$ via restricting the class of coupling measures to all possible Gaussian mixtures in $\mb{R}^{2d}$, and derived a computationally convenient discrete formulation for this metric. Moreover, the authors, although without a formal proof,  indicate that the proposed distance could be extended to other types of mixtures if they satisfy a marginal consistency property and an identifiability property. A rigorous proof is presented in Lemma \ref{th1:lemma4}. 
%\textcolor{blue}{(is this a lemma from their paper? then say discussed in their Lemma 9. otherwise, say where this Lemma can be found.)}
Although Gaussian mixtures already describe a rich class of models, if we directly extend their proposal to a more versatile class of models, the proposed metric will be incapable of capturing key differences  of interest, i.e., tail properties, between probability distributions ; please refer to the discussions following the statement of Theorem \ref{th3} for details. 
%\textcolor{blue}{(This is a key line, and needs to be more precise and sharp.)} \textcolor{red}{(Does it serve us well?)}
In what follows, focusing on the class of identifiable elliptical mixture models in equation \eqref{def:indentifiable_emm},  we describe a new and carefully crafted strategy based on augmentation succeeded by restriction in the space of coupling measures that yield a transport metric ANDREW that not only automatically inherits computational tractability as in  \citet{delon:hal-02178204}, but also remains expressive and is capable of accessing improved computational algorithms based on an entropic regularization of the discrete optimal transport. 
%\textcolor{blue}{(good job with this line)} 
In essence, our novel strategy presents a general recipe for devising increasingly expressive transport metrics and describing a corresponding modified class of couplings, of which ANDREW introduced next is a specific example.


With the requirements (i)-(iv) in mind, we 
place the discussion in the previous paragraph in concrete terms and  
propose a modified 2-Wasserstein metric (ANDREW), denoted $W_{\rm AR}$, between elliptical mixture distributions. 
%that remains computationally efficient when one of the distributions is discrete, and that effectively captures finer features of the distributions. \textcolor{blue}{(sounds somewhat repetitive.)}
To that end, we {\em augment} the class of coupling measures $\pi(p_0,p_1)\subset \mb{P}_p(\mb{R}^d\times\mb{R}^d)$ into a class of coupling measures  $\pi(p^{\star}_0,p^{\star}_1)\subset\mb{P}_p(\mb{R}^{2d}\times\mb{R}^{2d})$, and then {\em restrict} $\pi(p^{\star}_0,p^{\star}_1)$ to a carefully chosen sub-class of couplings. We describe the details below. 

\begin{definition}
Let $p_0, p_1 \in \mb{P}_{p}(\mb{R}^d)$ with $p_j = \sum_{k=1}^{K_j} s_{jk} \mbox{ED}_h(m_{jk},\Sigma_{jk}), (j = 0, 1)$. Next, we shall consider an augmentation followed by a restriction scheme as follows:\\
(a) Augmentation:
Define probability distribution $p_{0}^\star \in\mb{P}_2(\mb{R}^{2d})$ as 
\begin{align*}
p_{0}^\star :\,= p_0 \otimes \widetilde{p}_0, \ \text{with}\ \widetilde{p}_0 :\,= p_{01} \otimes \ldots \otimes p_{0d}    
\end{align*}
and $p_{0i}$ the $i$th marginal of $p_0$. Clearly, if $X_0 \sim p_0$, and $\widetilde{X}_0$ independent of $X_0$ is distributed as $\widetilde{p}_0$, then $X_0^{\star} = (X_0, \widetilde{X}_0)^{\T}\sim p_{0}^\star$. Similarly, define $p_1^\star$.
By construction we have
\begin{align*}
 \pi(p^{\star}_0,p^{\star}_1) = \pi(p_0,p_1)\otimes \pi(\widetilde{p}_0,\widetilde{p}_1)  = \pi(p_0,p_1)\otimes \big\{\otimes_{i=1}^d \pi(p_{0i},p_{1i})\big\}.   
\end{align*}
(b) Restriction: 
Suppose
\begin{equation*}
 \EMM_{2d}(K_0,K_1) = \bigg\{ \sum_{k,l} \pi_{kl}\ {\rm ED}_{h}(m_{kl},\Sigma_{kl}): \pi_{kl}\geq 0,\ \sum_{k,l} \pi_{kl} = 1\bigg\}   
\end{equation*}
denote the collection of all ($K_0\times K_1$)-component mixture of identifiable elliptical distributions. Define a subset $\EMM^{\alpha}_{2d}(K_0, K_1)$ of $\EMM_{2d}(K_0,K_1)$ by imposing the entropic restriction 
\begin{equation*}
D_{\rm KL}\big[\Pi\ || \ s_0 s_{1}^{\T}\big]\leq \alpha\ \text{where}\ \Pi = ((\pi_{kl})) \in \mb R^{K_0 \times K_1}
\end{equation*}
is the joint probability matrix of the mixture weights, and $s_0, s_1$ are the respective marginals, i.e., $\Pi 1_{K_1} = s_0, \Pi^{\T}1_{K_0} = s_1$.
Finally, define a collection of couplings $R^{\alpha}(p^{\star}_0,p^{\star}_1) \subset \pi(p^{\star}_0,p^{\star}_1)$ as 
\begin{align*}
R^{\alpha}(p^{\star}_0,p^{\star}_1) = \big\{\pi(p_0,p_1)\cap \EMM^{\alpha}_{2d}(K_0, K_1)\big\}\otimes \big\{\otimes_{i=1}^d \pi(p_{0i},p_{1i})\big\}.   
\end{align*}
Refer to Figure \ref{fig:andrew_cartoon} for a schematic representation of the proposed augmentation and restriction strategy.
With these notations in place, we define ANDREW as
\begin{equation}\label{eqn:andrewdef}
 W_{\rm AR}^{2}(p_0, \ p_1) = \inf_{\nu\in R^{\alpha}(p^{\star}_0,p^{\star}_1)} \mb{E}_{\nu}||X_{0}^{\star} - X_{1}^{\star}||^2. \end{equation}
\end{definition}


%\begin{figure}[!htb]
%\begin{center}
%\includegraphics[width=1\textwidth ]{plots/MW_Why.pdf}
%\includegraphics[width=1\textwidth]{newplots/W_AR.pdf}
%\centering{\caption{\emph{The augmentation and restriction scheme to construct $R^{\alpha}(p^{\star}_0,p^{\star}_1)$; left: construction of $\big\{\pi(p_0,p_1)\cap \EMM^{\alpha}_{2d}(K_0 K_1)\big\}$ ; right: construction of $\big\{\otimes_{i=1}^d \pi(p_{0i},p_{1i})\big\}$ with $d=2$.} %\textcolor{red}{[the figure needs to go right before/after Def 1]}
%}
%}
%\end{center}
%\end{figure}

To cater to our original goal of centering the $\mbox{D}$-BETEL around an EMM, we now present a simplified form of $W_{\rm AR}$ for the case when one of $p_0, p_1$ is discrete.
\begin{theorem}\label{th3}
Suppose $p_{0}\equiv \sum_{k=1}^{K_0} s_{0k} \mathrm{ED}_h(m_{0k},\Sigma_{0k}),\
p_{1}\equiv {\sum_{k=1}^{K_1} s_{1k} \delta_{m_{1k}}}$, and $M = ((||m_{0k}-m_{1l}||^2 )) \in \mathbb{R}^{K_0 \times K_1}$ be the quadratic cost matrix. Then, there exists $\lambda_{\alpha}$ depending on $\alpha$ such that 
% \begin{align*} 
% W_{\rm AR}^2(p_0\ , \ p_1) =
% &\inf_{\pi\in\pi(s_0,s_1)}\bigg[ \big<\Pi, M\big>  - \frac{1}{\lambda_{\alpha}} H(\Pi)\bigg] + \nu_{h}\sum_{k=1}^{K_0} s_{0k} \mathrm{tr}(\Sigma_{0k})+\\ &\sum_{k=1}^d \int_{0}^1 \big(F_{0k}^{-1}(z) - F_{1k}^{-1}(z)\big)^2 dz
% \end{align*}
{\smaller \begin{align*} 
W_{\rm AR}^2(p_0, p_1) =
\inf_{\pi\in\pi(s_0,s_1)}\bigg[ \big<\Pi, M\big>  - \frac{1}{\lambda_{\alpha}} H(\Pi)\bigg] + \nu_{h}\sum_{k=1}^{K_0} s_{0k} \mathrm{tr}(\Sigma_{0k})+ \sum_{k=1}^d \int_{0}^1 \big(F_{0k}^{-1}(z) - F_{1k}^{-1}(z)\big)^2 dz
\end{align*}}
where $\big<\Pi, M\big> = \mathrm{tr}(\Pi^{\T}M)$, $H(\Pi) = -\sum_{k,l} \pi_{kl}\log \pi_{kl}$ and $F_{jk}^{-1}(\cdot)$ is the quantile function of $X_{jk}$.
\end{theorem}

We defer the proof and a cascade of required Lemmas to Section \ref{ssec:th3} of the supplementary document and make some remarks about ANDREW here. Importantly, the expression above is completely tractable and computationally feasible. The entropic regularization term in $W_{\rm AR}$ makes the discrete optimal transport problem strictly convex, and consequently, it can access linear convergence via Sinkhorn’s fixed point iterations \citep{cuturi2013sinkhorn}. On the other hand, traditional simplex or interior-point methods for an $n\times n$ unregularized optimal transport scales at least in $O(n^3\log n)$. Next, $W_{\rm AR}$ effectively captures the tail behavior of the distributions. Had we restricted the class of coupling measures $\pi(p_0,p_1)$ to $\EMM_{2d}(K_0,K_1)$ instead, then
%{\small
\begin{align*}
 \mbox{MW}^2_2(p_0, p_1) :\,=  \inf_{\nu\in \{\pi(p_0,p_1)\cap \EMM_{2d}(K_0 K_1)\}} \mb{E}_{\nu}||X_{0}^{\star} - X_{1}^{\star}||^2  = \inf_{\pi\in\pi(s_0,s_1)} \big<\Pi, M\big> + \nu_h \sum_{k=1}^{K_0} s_{0k} \mbox{tr}(\Sigma_{0k})
 \end{align*}
%}
only depends on first and second-order moments, and fails to capture the tail behavior of the distributions. For example, let $p_{0}\equiv \sum_{k=1}^{K_0} s_{0k} t_{\eta}(m_{0k},\Sigma_{0k}),\ p^{\prime}_{0}\equiv \sum_{k=1}^{K_0} s_{0k} t_{\eta^{\prime}}(m_{0k},\Sigma_{0k}^{\prime}),\ p_{1} \equiv {\sum_{k=1}^{K_1} s_{1k} \delta_{m_{1k}}} $ and set $\eta^{\prime} = \eta/m,\ \Sigma_{0k}^{\prime} = \frac{\eta - 2m}{\eta - 2}\Sigma_{0k}$ for some $ m\in\mb{Z}^{+} $ such that the variances of the multivariate t-distributions $t_{\eta}(m_{0k},\Sigma_{0k})$ and $t_{\eta^{\prime}}(m_{0k},\Sigma_{0k}^{\prime})$ match for $k = 1,\ldots,K_0$. Then $\mbox{MW}^2_2(p_0, p_1) = \mbox{MW}^2_2(p_{0}^{\prime}, p_1)$. Since the expression of $W_{\rm AR}$ additionally involves the marginal quantiles, it is capable of capturing the difference in the tail due to the different d.f. of the $t$. We believe the flexibility and the computational simplicity of our novel Wasserstein metric may render itself useful in many optimal transport-based machine learning applications, beyond what we discuss here; see the discussion section for some specific application domains. 



We now have all the necessary ingredients for $\mbox{D}$-BETEL, and we illustrate the proposed methodology in a number of specific applications. All the examples in the following section use $\mbox{D}$-BETEL in \eqref{eqn:wbetel} with our proposed transport metric ANDREW in \eqref{eqn:andrewdef}.


%\textcolor{blue}{Figure 1 still does not look paper quality to me: the tex rendering is inconsistent, the figure is quite busy as well. Some suggestions: when you define $R^\alpha$, write $R^\alpha = \m C_1 \otimes \m C_2$ and define these individually. Then, you can reuse them in the figure and avoid the large expressions. Also, since you are rendering latex anyway, why not use that for the equality sign (the current one looks ugly). The horizontal arrows might benefit from a different color. Anyway, make the changes in the text, and when the draft is with us, go back to the figure and polish it. The figure caption needs to be more informative as well.}





\section{Robust Bayesian inference}\label{Robust_Bayes}
%\subsection{Hyperparameter tuning}\label{hyperparameter_tuning}
%We provide a principled guideline for hyper-parameter tuning in our set up, which is largely missing in pseudo-likelihood based Robust Bayesian inference approaches.
%The concentration parameter $\varepsilon$, similar to the concentration parameter in a Dirichlet process [\cite{10.2307/24305538}], determines how tightly $\nu_{w,x}$ sits around $F_{\theta}$ with respect to $W_{\rm AR}$. Given $x_1,\ldots,x_n$, the Bayesian leave-one-out estimate of out-of-sample predictive fit [\cite{JMLR:v17:14-540}] is $\ELPD_{\varepsilon} =   \sum_{i=1}^n \ \log \pi(x_i\mid x_{-i})\ \text{where}\  \pi(x_i\mid x_{-i}) = \int \pi(x_i\mid\theta)\ \pi(\theta\mid x_{-i})\ d\theta$ is the leave-one-out predictive density given the data without the $i$th data point, and the corresponding standard error is $
%\SE[\ELPD_{\varepsilon}] = \sqrt{n}\ \sqrt{\Var[\ \log \pi(x_1\mid x_{-1}),\ldots, \log \pi(x_n\mid x_{-n})\ ]}$. When $\varepsilon$ is too large, the distance based restriction does not kick in, and estimated $\SE[\ELPD]$ is close to $0$. So we consider a decreasing sequence of $\varepsilon$ values, say $\varepsilon_1,\ldots,\varepsilon_h$, such that $\varepsilon_i > \varepsilon_j \ \forall \ 1 \leq i<j\leq h$. A general strategy to select the sequence is to first consider a grid over powers of $2$ and then use a finer grid in the interval where $\ELPD_{\varepsilon}$ undergoes steep change. Suppose $\varepsilon_{h_0}$ is the largest value of $\varepsilon$ for which the distance based restriction is active. Then our estimate of the model parameter $\theta$ is 
%\begin{align}
%  \hat{\theta}_{\rm MA} = \frac{\sum_{i = h_{0}}^{h}\exp(-\kappa\ \ELPD_{\varepsilon_{i}})\ \hat{\theta}_i}{\sum_{i = h_{0}}^{h}\exp(-\kappa\ \ELPD_{\varepsilon_{i}})}  
%\end{align}
%where $\kappa = 1$; $\hat{\theta}_i$, and $\ELPD_{i}$ are the parameter estimate and estimated $\ELPD$ value at $\varepsilon_i$ respectively.  With that we now have all the necessary ingredients for D-BETEL. Let us illustrate the proposed methodology in a number of specific applications. All the examples in the following section use D-BETEL in \ref{eqn:wbetel} with our proposed transport metric in \ref{eqn:andrewdef}. 

%\newpage
\subsection{Model based clustering}
Motivated by the model based clustering example in \citet{doi:10.1080/01621459.2018.1469995}; see also \citet{cai}; we generate data from a bivariate skew-normal distribution \citep{SkewNormal} with pdf $f(x) = 2\phi(x) \Phi(\alpha^{\T} x),\ x \in \mb{R}^{2}$, with the two-dimensional skewness parameter $\alpha \neq (0, 0)$ to imitate a situation where the underlying true distribution is bivariate normal in the presence of mild contamination. We wish to demonstrate that $\mbox{D}$-BETEL is resistant to presence of mild perturbations in the data generating mechanism and can adequately describe the above set up with a bivariate normal centering, without resorting to more complex centering distributions.  We shall showcase all our tools in action on this simple example, and skip some of these details in later sections. Throughout this example, for the purposes of model comparison via marginal likelihood, we follow the approach in \citet{doi:10.1198/016214501750332848} to approximate the log marginal density $\log m(x\mid  \mb{M})$ of a model $\mb{M}$ via
\begin{equation*}
\log m(x\mid \mb{M}) = \log f(x\mid \mb{M},\theta^{*}) + \log \pi(\theta^{*}\mid \mb{M}) - \log \pi(\theta^{*}\mid x,\mb{M}), \end{equation*}
where $\log f(x\mid \mb{M},\theta^{*})$  and $\log \pi(\theta^{*}\mid \mb{M})$ are respectively  the log-likelihood and log prior of the model $\mb{M}$ at $\theta^{*}$, preferably  a high-density point. 

We generate data from a bivariate skew-normal distribution with varying value of skewness parameter $\alpha \neq (0, 0)$. We choose sample sizes $n \in \{100,\ 200,\ 300,\ 500\}$; and set $\alpha= (2.5, 2.5)^\T,\, (3.0, 3.0)^\T,\ (3.5, 3.5)^\T$ -- giving us $12$ simulation set-ups in total. First, we compare the following two fully parametric models: (i) $\mb{M}_{1}$, which models the data as independent draws from $\mbox{N}_2(\mu, \Sigma)$, and imposes a diffuse $\mbox{N}_2(0, 10^3 \mbox{I}_2)$ prior on $\mu$ and $\mbox{Wishart}_2(\nu_0, V_0)$ prior on $\Sigma^{-1}$, independently. (ii) $\mb{M}_{2}$, which used a mixture normal data model $\omega\, \mbox{N}_2(\mu_1, \Sigma_{1}) + (1- \omega)\, \mbox{N}_2(\mu_2, \Sigma_{2})$; and imposes independent diffuse $\mbox{N}_2(0, 10^3 \mbox{I}_2)$ priors on $\mu_1$, $\mu_2$, an $\mbox{U}(0, 1)$ prior on $\omega$, and independent $\mbox{Wishart}_2(\nu_0, V_0)$ priors on $\Sigma_{1}^{-1}$ and $\Sigma_{2}^{-1}$. To explore the high-density neighborhoods of the posterior distributions,  we use coordinate-wise Metropolis--Hastings updates. For smaller sample sizes, the simpler model $\mb{M}_{1}$ provides higher marginal likelihood compared to $\mb{M}_{2}$. However, as the sample size grows, the more complex model $\mb{M}_{2}$ predictably starts being preferred; refer to Figure \ref{clustering_results} which plots the posterior model probability of $\mb{M}_{1}$ as a function of sample size.
%\textcolor{blue}{(do we provide any evidence for this?)}
Next, we consider the $\mbox{D}$-BETEL counterparts of $\mb{M}_{1}$ and $\mb{M}_{2}$, which we refer to as $\mb{M}_{1}^{\star}$ and $\mb{M}_{2}^{\star}$ respectively, with $\mb{M}_{1}^{\star}$ using a single normal distribution $\mbox{N}_2(\mu, \Sigma)$ as the centering distribution, and $\mb{M}_{2}^{\star}$ centered around $\omega \mbox{N}_2(\mu_1, \Sigma_{1}) + (1- \omega) \mbox{N}_2(\mu_2, \Sigma_{2})$.
%(i) $\mb{M}_{1}^{\star}$:  $\mbox{D}$-BETEL  centered at $\mbox{N}_2(\mu, \Sigma)$ and (ii) $\mb{M}_{2}^{\star}$: $\mbox{D}$-BETEL  centered at a mixture normal distribution $\omega \mbox{N}_2(\mu_1, \Sigma_{1}) + (1- \omega) \mbox{N}_2(\mu_2, \Sigma_{2})$. 
We use same the prior specification \& MH sampling scheme as before.

First, we showcase our data driven approach to tune the hyper-parameter $\varepsilon$ for both $\mb{M}_{1}^{\star}$ and $\mb{M}_{2}^{\star}$. Figures \ref{fig:hyperparameter_set1} and \ref{fig:hyperparameter_set2} present plots for $\ELPD_{\varepsilon}$, $\mbox{SE}(\ELPD_{\varepsilon})$ and $\kappa$, defined in Section \ref{ssec:npBayes}, as functions of $\log\varepsilon$ for two  particular combination of $(n,\alpha)$ values. We considered a grid of $\varepsilon$ values over powers of $2$ and then use a finer grid in the interval where $\ELPD_{\varepsilon}$ undergoes steep change. For sufficiently large value of $\varepsilon$, the distance based constraint practically becomes inactive, and consequently $\ELPD_{\varepsilon}$ plateaus out  and $\mbox{SE}(\ELPD_{\varepsilon})\downarrow 0$.  Finally, we obtain the $\mbox{D}$-BETEL based parameter estimates $\hat{\theta}_{\rm MA}$ as delineated in Section \ref{ssec:npBayes}.   Although $\hat{\theta}_{\rm MA}$ in equation \eqref{eqn:ma} is calculated via an weighted average, in practice $\hat{\theta}_{\rm MA}$  typically degenerates to  estimates corresponding to a handful of values  of $\varepsilon$, as apparent in the plot of $\kappa$ as a function of $\log \varepsilon$ in Figures  \ref{fig:hyperparameter_set1}, \ref{fig:hyperparameter_set2}. We observe similar pattern for the remaining combinations of $(n,\alpha)$ values, and refrain from presenting them here in order to avoid repetitiveness.

Figure \ref{clustering_results} presents the posterior  probability of selecting the simpler model with only one bivariate normal component under the standard posterior, a fractional posterior with varying temperature parameters, and $\mbox{D}$-BETEL.
For the standard posterior, the posterior probability of selecting the simpler model $\mb{M}_{1}$ drop below $0.5$  as sample size increases. %This provides further empirical evidence towards the recent theoretical results \citep{cai2} asserting the lack of reliability of power posteriors in learning the number of components in finite mixture models, under mild model mis-specification.
On the contrary, $\mbox{D}$-BETEL and fractional posterior \citep{doi:10.1080/01621459.2018.1469995} with small temperature parameter is more resistant towards presence of mild skewness in the data generating mechanism, and still prefer the simpler model  across the sample sizes we considered. However, unless the temperature parameter of the fractional posterior is chosen to be appropriately small,  it cannot reliably estimate the number of components in finite mixture models, under mild model mis-specification \citep{cai2}.

\begin{figure}[!htb]
    \centering
    %\subfloat{{
    \includegraphics[width=18cm, height = 4.5cm]{clustering_plots/choiceofepsilon_n100_alpha3_5.pdf} 
    %}}%
    \caption{\emph{\textbf{Hyper-parameter tuning for model based clustering with sample size $\mathbf{n=100}$, skewness parameter $\mathbf{\alpha = (3.5, 3.5)^\T}.$}  $\ELPD_{\varepsilon}$ gradually plateaus out  and $\mbox{SE}(\ELPD_{\varepsilon})\downarrow 0$ as $\log\varepsilon\uparrow$ for both the models. Consequently, weights $\kappa$ corresponding to a handful of $\varepsilon$ values contribute meaningfully to the weighted sum in $\hat{\theta}_{\rm MA}$ and rest are $\approx 0$. }\label{fig:hyperparameter_set1}}
\end{figure}
\begin{figure}[!htb]
    \centering 
    %\subfloat{{
    \includegraphics[width=18cm, height = 4.5cm]{clustering_plots/choiceofepsilon_n200_alpha2_5.pdf} %}}%
    \caption{\emph{\textbf{Hyper-parameter tuning for model based clustering with sample size $\mathbf{n=200}$, skewness parameter $\mathbf{\alpha = (2.5, 2.5)^\T}.$} $\ELPD_{\varepsilon}$ gradually plateaus out  and $\mbox{SE}(\ELPD_{\varepsilon})\downarrow 0$ as $\log\varepsilon\uparrow$ for both the models. Consequently, weights $\kappa$ corresponding to a handful of $\varepsilon$ values contribute meaningfully to the weighted sum in $\hat{\theta}_{\rm MA}$ and rest are $\approx 0$.}
    \label{fig:hyperparameter_set2}}
\end{figure}

\begin{figure}[!htb]
    \centering
    %\subfloat{{
    \includegraphics[width=17cm, height = 6cm]{SN_Clustering.pdf} 
    %}}%
    \caption{\emph{\textbf{Model based clustering.} We are comparing the Bayes factor for selecting the simpler model via $\mbox{D}$-BETEL, the standard posterior, and the fractional posterior \citep{doi:10.1080/01621459.2018.1469995} with different temperatures, across varying values of skewness parameter $\alpha$ of the generating skew normal distribution and sample sizes. The left panel is for $\alpha= (2.5, 2.5)^\T$, the middle panel is for $\alpha= (3, 3)^\T$, and the right panel is for $\alpha= (3.5, 3.5)^\T$.  Unlike the standard posterior,  $\mbox{D}$-BETEL and fractional posterior with low temperature still prefer the simpler model across the sample sizes.} 
    \label{clustering_results}}
\end{figure}

\subsection{Generalised linear regression}\label{ssec:gen_reg}
Suppose we observe data  $\{(y_i,x_i) \in \mb{R}\times\mb{R}^{d}\}_{i=1}^n$ on a response variable $y$ and  covariates $x$ for $n$ individuals.  In generalised linear regression set up, we model the response by an exponential family distribution:
\begin{equation*}
 f(y_i\mid\theta_i, \phi) = \exp\bigg[\ \frac{y_i \theta_i - b(\theta_i)}{a(\phi)} + c(y_i, \phi)\bigg]   
\end{equation*}
where $a(\cdot),\ b(\cdot),\ c(\cdot)$ are known functions such that $m_i = b^\prime(\theta_i) ,\ \sigma_{i}^2  =\phi b^{\prime\prime}(\theta_i)$ are respectively the mean and variance of the  distribution, and there exists
a one-to-one continuously differentiable link function  $g(\cdot)$ such that $g^{-1}(x_{i}^\T\beta) = b^\prime(\theta_i) $.   The log-likelihood of the parameter of interest $\beta$ is 
\begin{equation*}
  l(\beta\mid x,y)   = \sum_{i=1}^n l_i(\beta\mid x_i, y_i) = \sum_{i=1}^n \bigg[ \frac{y_i \theta_i - b(\theta_i)}{a(\phi)}+ \log c(y_i,\phi)\bigg]  
\end{equation*}
where $\theta_i$ is a function of $m_i$. The corresponding Fisher's score function  $S = (S_{0},\ S_1,..,\ S_d)^\T$:
\begin{equation*}
S_j = \frac{\partial l}{\partial \beta_j}
= \sum_{i = 1}^n \bigg[ \frac{(y_i - m_i)}{a(\phi)} \frac{1}{V_i} \frac{\partial m_i}{\partial \beta_j}  \bigg] = 0 \end{equation*}
with $V_i =  \frac{\partial m_i}{\partial \theta_i} = b^{\prime\prime}(\theta_i)$. For simplicity of exposition, we express $S = \sum_{i = 1}^n \eta_{i}$ where  $\eta_{ij} = \frac{\partial l_i}{\partial \beta_j},\eta_i = (\eta_{i0},\ \eta_{i1}\ ,...,\ \eta_{id})^{\T},\ i = 1,\ldots,n; \ j  =0,\ldots,d$. The score statistic $S$ is asymptotically normal with mean 0 \citep{Haynes2013} and $n_i$ captures the deviation from 0 for the $i$-th observation. With that intuition, to conduct robust Bayesian inference on $\beta$,  we posit $\mbox{D}$-BETEL  on $\{\eta_i\}_{i=1}^n$ with a finite mixture of $(d+1)$-variate normal densities i.e $\sum_{j=1}^K \pi_j \ \mbox{N}(\mu_j, \Sigma_j)$ such that   $\sum_{j=1}^K \pi_j \mu_j = 0$ as our choice for centering parametric guess. 

We generate data from a Poisson random effects model,
\begin{equation*}
 \log(m_i) = \beta_0 + \beta_1 x_i + h_i ; \quad  y_i \sim \mbox{Poisson}(m_i),\ i = 1,\ldots,n,   
\end{equation*}
where $\beta_0 = 5, \ \beta_1 = 1$, \ $x_i\sim \mbox{N}(5, 1)$ and $h_i \sim (1-p) \ \mathbf{1}\{0\} + p \ \mbox{N}(1, 0.1^2)$, in order  to  mimic a situation where a small proportion of outliers are present in the data-set. We place flat $\mbox{N}(0, 100^2)$ priors on $\log\sigma^2_{1}(\beta)$, $\log\sigma^2_{2}(\beta)$, $\beta_0$ and $\beta_1$ and a $\mbox{U}(-1, 1)$ prior on $\rho(\beta)$, independently. We devise a Metropolis--Hastings algorithm to update $g(\beta) = (\log\sigma^2_{1}(\beta),\ \log\sigma^2_{2}(\beta),\ \rho(\beta), \beta_0,\ \beta_1)^\T$ at $(t+1)$th iteration using the 1-step proposal scheme: 
\begin{equation}
 g^{(t+1)}(\beta) \sim \mbox{N}_5\big(g^{(t)}(\beta),\  k\ \nabla g(\hat{\beta}_{m})\ \mbox{I}^{-1}(\hat{\beta}_{m})\ \nabla^{T} g(\hat{\beta}_{m})\big)   
\end{equation}
where $\mbox{I}(\hat{\beta}_m))$ is the Fisher's information matrix evaluated at the maximum likelihood estimator $\hat{\beta}_{m}$ of $\beta$, and $k$ is a tuning parameter. 

In Table \ref{table:glm_1}, we expand on the performance of $\mbox{D}$-BETEL for varying  extent of perturbations in the data generating mechanism with sample size $n=100$, relative to popular practical approaches. In particular, we compare  $\mbox{D}$-BETEL against a standard posterior as well as Bayesian ETEL \citep{Chib2018} with the estimating equations set to  $\mbox{E}[\partial \log l(\beta\mid X, Y)/ \partial \beta]=0$ to infer about the parameter $\beta$. The latter approach can be regarded as a variant of $\mbox{D}$-BETEL, with a stricter moment-type constraint.
From Table \ref{table:glm_1}, we report the $L_1$ error of posterior means, length of the HPD sets and associated coverage probabilities (within braces) for $\mbox{D}$-BETEL and competing approaches. It is evident that $\mbox{D}$-BETEL is more resistant towards presence of outliers when compared with the standard Bayesian and MCM based approaches, across all the sample sizes and proportion of contamination in the data sets that we considered. Also, $\mbox{D}$-BETEL provides slightly wider credible sets compared to the standard posterior based approach, while maintaining high coverage probability. Additional simulation results for $n=250, 500$ are presented in Section \ref{sup:glm} of the supplement.

\begin{table}[!htb]
  \caption{\emph{\textbf{Generalised linear regression (Poisson regression).} Here the \textbf{sample size $n$ is $100$}. We compare standard posterior yielded from the fully parametric model, moment conditional model (MCM) based on the maximum likelihood equations,
  %(\textcolor{blue}{again, say which model}) 
  and $\mbox{D}$-BETEL based parameter estimates over 50 replicated simulations with proportion of outlier $p=0.10, 0.12, 0.15$. %\textcolor{green}{Need to edit} %\textcolor{green}{Should I include $p=0.05,0.02?$} %\textcolor{blue}{Yes, if there is space}. 
$\mbox{D}$-BETEL is more resistant towards presence of outliers all values of $p$ considered, however it provides slightly wider $95\%$ credible sets while maintaining the high coverage probability. Additional simulation results for $n=250, 500$ is presented in the supplement \ref{sup:glm}.}}\label{table:glm_1}
  \centering
  \begin{tabular}{lllllllllll}
    \toprule
    %\multicolumn{2}{c}{Part}                   \\
    \cmidrule(r){1-2}
     &     &      \multicolumn{2}{c}{$\mbox{D}$-BETEL} & \multicolumn{2}{c}{Standard posterior} & \multicolumn{2}{c}{MCM} \\
    \midrule
    p & $\theta$     & $||\theta - \hat{\theta}||_1$     & HPD  & $||\theta - \hat{\theta}||_1$     & HPD   & $||\theta - \hat{\theta}||_1$     & HPD \\
    \midrule
    0.10 & $\beta_0$ & 0.02 & 0.18 (1.00)  & 0.35   & 0.12 (0.20)& 0.41 & 0.22 (0.10) \\
        & $\beta_1$ & 0.01 & 0.02 (1.00)  & 0.07 &0.02 (0.20) &0.06 & 0.04 (0.22)\\
    \midrule
    0.12 & $\beta_0$ &0.02 &0.22 (1.00) &0.31& 0.12 (0.35) & 0.47 & 0.35 (0.14) \\
        & $\beta_1$ & 0.01 &0.04 (1.00) &0.08& 0.02 (0.59) & 0.06 & 0.06 (0.32) \\
    \midrule
    0.15 & $\beta_0$ & 0.06& 0.22 (0.94)  &0.47  & 0.11 (0.00) & 0.54 &0.23 (0.06)\\
        & $\beta_1$ & 0.01 &0.04 (0.94)  &0.06& 0.02 (0.00) & 0.09&0.05 (0.18)\\
    
    \bottomrule
  \end{tabular}
\end{table}

%\clearpage
%, and the second for entropy-based portfolio allocation (deferred to the  supplementary materials in \ref{ssec:po}).
\section{Algorithmic fairness: demographic parity}\label{ssec:algorithmic_fairness}
In this section, we present applications of the dual formulation of D-BETEL presented in equations \eqref{eqn:dbetel:alt}-\eqref{wbetel_dual} in the context of ensuring demographic parity in machine learning algorithms. Machine learning algorithms are increasingly used in critical decisions affecting human lives including but not limited to credit, employment, education, and criminal justice, and hence fairness has emerged as a primary pillar of modern machine learning research in recent years. Discrimination refers to unfavorable treatment of entities due to their membership to certain demographic groups that are determined by the attributes protected by law, called protected attributes. The goal of demographic parity or statistical parity \citep{90450a4b5b49471b8111fc88355f2e7f, gajane2018formalizing} in machine learning is to design algorithms that yield fair inferences devoid of discrimination due to membership to certain demographic groups determined by a protected attribute. 

First, we introduce the mathematical formalization of the notions of demographic parity. To that end,  we assume that $X$ denotes the feature vector used for predictions, $A$ is the protected attribute with two levels $\{S,T\}$, and  $Y$ is the true response. Parity constraints are phrased in terms of the distribution over $(X, A, Y)$. 
Two definitions are in order.
%For the sake of simplicity, we assume that the two groups  $S,T$ determined by two levels of the protected attribute induce a partition of the population $X$.
\begin{definition}[Demographic parity, \citep{90450a4b5b49471b8111fc88355f2e7f}]\label{def:DP}
A predictor $h$  satisfies demographic parity under the distribution over  $(X, A, Y)$ if $h(X)$ is independent of the protected attribute $A$, i.e ,
\begin{align*}
    \mb{P}[h(X) \geq z \mid A = S] = \mb{P}[h(X) \geq z \mid A = T] = \mb{P}[h(X) \geq z ]\ \text{for all} \ z.
\end{align*}
%A predictor $h: (X, A) \to Y$ is said to achieve demographic parity with bias $\varepsilon$ with respect to groups $S,T$ and outcome $O\subset Y$iff 
%\begin{align*}
%   \mid P[h(x_i)\in O\mid a_i\in S] -  P[h(x_j)\in O\mid a_j\in T] \mid \leq \varepsilon. 
%\end{align*}
\end{definition}

\begin{definition}[Demographic parity in expectation, \citep{90450a4b5b49471b8111fc88355f2e7f}
]\label{def:DP_exp}
A predictor $h$  satisfies demographic parity under the distribution over  $(X, A, Y)$ if $h(X)$ is independent of the protected attribute $A$, i.e ,
\begin{align*}
    \mb{E}[h(X) \mid A = S] = \mb{E}[h(X)  \mid A = T] = \mb{E}[h(X) ].
\end{align*}
%A predictor $h: (X, A) \to Y$  is said to achieve demographic parity in expectation with respect to groups $S,T$ iff 
%\begin{align*}
%  \mb{E}[h(x_i)\mid a_i\in S] = \mb{E}[h(x_j)\mid a_j\in T].  
%\end{align*}
\end{definition}
%\textcolor{brown}{
It is perhaps instructive to examine the notion of demographic parity from the viewpoint of causal inference. In particular, \citet{Nabi2018} shows how statistical parity may be unable to reveal an underlying causal effect that is discriminatory. For simplicity in exposition, we concentrate on binary response variables for a while and note down the differences between the notions of demographic parity in fairness and the notion of average treatment (protected attribute) effect in the context of causal inference. Demographic parity merely seeks that the conditional probabilities $ \mb{P}[h(X) = 1 \mid A = S],\ \mb{P}[h(X)=0 \mid A = T]$ are equal. On the other hand, in the potential outcome framework of causal inference, we denote the potential outcome $h(x)$  for an individual as  $h_{(k)}(x)$, had the treatment (protected attribute) $A$ been assigned the value $k\in\{S,T\}$. Then, to ensure that there is no average treatment effect, we seek $\mb{E}[h_{(S)}(X) ]-\mb{E}[h_{(T)}(X)]=0$, or equivalently  $\mb{P}[h_{(S)}(X) = 1]=\mb{P}[h_{(T)}(X) = 1]$. For observational studies, to carry out inference in this potential outcomes framework, it is customary to assume conditional ignorability -- which specifies a set of variables, called confounders, given which the potential outcome becomes independent of the assigned treatment (protected attribute).
%To make this more concrete, let’s assume in our case that the potential outcome $h_{A}$ becomes independent of the assigned value of $A$ given the value of the confounder $E$. Ignorability can be expressed as  $\mb{P}[h_{A = S}(x)\mid E]=\mb{P}[h_{A = T}(x)\mid E]$. 
In essence, ignorability implies that we can express the probability of a potential outcome  conditional on $E$, in terms of probability of the observed outcome conditional on both $E$ and  $C$, i.e $\mb{P}[h_{(k)}(X)\mid E] = \mb{P}[h(X)\mid E, A = k], \ k\in\{S, T\}$. Now, to ensure that there is no average treatment effect, we need
\begin{align*}
    \int\mb{P}[h(X)\mid E, A = S]\ \pi(E) dE = \int\mb{P}[h(X)\mid E, A = T]\ \pi(E) dE.
\end{align*}
However, deciding upon possible confounders $E$ in fairness problems is often not straightforward. Innovative solutions have been proposed in the literature. For example,  \citet{https://doi.org/10.1111/rssa.12613} proposed the construction of an $\Tilde{X}$, a reconstructed version of the data matrix that is orthogonal to the vector of protected attributes with minimal information loss i.e $||X-\Tilde{X}||_{\rm F}$ is minimum subject to the constraint $\langle \Tilde{X}, A\rangle=0$, where $\langle \cdot, \cdot\rangle$ stands for inner product and $||\cdot||_{\rm F}$ denotes the Frobenius norm of matrices. It is straightforward to see that non-linear dependencies on the vector of $A$ could still be present in the transformed matrix $\Tilde{X}$, but  $X-\Tilde{X}$ can serve as a confounder in numerous practical purposes, e.g in models linear in covariates. This presents an interesting alleyway for future inquiry and is well beyond the scope of this article.  For the rest of our presentation, we shall build on the basic notion of statistical/demographic parity.


Although the notions in Definitions  \ref{def:DP} and \ref{def:DP_exp}  coincide when we work with binary responses, the latter may be amenable to simple computational algorithms \citep{e21080741} compared to the general definition. However, the notion of demographic parity in expectation is extremely prohibitive since one cannot control the predictor $h$ over its entire domain, and depending on the application of interest we may be solely interested in controlling the tails of the predictor \citep{yang2019fair}. Taking refuge to our semi-parametric inference framework, we offer a flexible as well as a computationally feasible compromise between the notions in Definitions \ref{def:DP} and \ref{def:DP_exp}. To that end, we introduce the notion of demographic parity in the Wasserstein metric next.
\begin{definition}[Demographic parity in Wasserstein metric]
A predictor $h$ achieves demographic parity in Wasserstein metric  with bias $\varepsilon$, under the distribution over  $(X, A, Y)$  if 
\begin{align*}
  W^{2}_{\rm AR}\big[F_{h_{S}}, F_{h_{T}}\big] \leq \varepsilon,  
\end{align*}
where $F_{h_{k}}$ is the cdf of $h$ under sub-population $k$ i.e $h(X)\mid A = k, k \in\{S,T\}$.
\end{definition}

Next, we shall see demographic parity in the Wasserstein metric in action. Suppose we have data $(x_i, y_i, a_i)\in \mb{R}^d\times\mb{R}\times\{S,T\}$ for  $n$ individuals on $p$-dimensional covariate $x$, univariate continuous response $y$, and levels of the protected attribute $a\in\{S,T\}$.
For the sake of simplicity in exposition, we also assume that $a_i = S,\ i=1,\ldots, n_S$  and $a_i = T,\ i=n_S+ 1,\ldots, n$ where $n = n_S + n_T$. Next, we posit a predictive model
\begin{align*}
    y_i = h(x_i, \theta_{(a_i)}) + e_i, \ e_i \overset{i.i.d}\sim \mbox{N}(0, \sigma^2), \ i =1, \ldots, n,
\end{align*}
where $h$ is potentially non-linear, and $(\theta_{(S)}, \theta_{(T)}) $ is the model parameter of interest to be estimated under the demographic parity constraint  $W^{2}_{\rm AR}\big[F_{h_{S}}, F_{h_{T}}\big] \leq \varepsilon$. In particular, we consider the empirical cdf of $h$ under sub-population $S$,  $F_{h_{S}}= 1/n_S\sum_{i = 1}^{n_S} \delta\{h(x_{iS})\}$; and a weighted empirical cdf of $h$ under sub-population $T$, $F_{h_{T}}= \sum_{i = n_S + 1}^{n} w_{i}\ \delta\{h(x_{iT})\}$. Here $\delta(\cdot)$ is the Dirac delta measure. The goal is to infer about $(\theta_{(S)}, \theta_{(T)}, w) $ ensuring that demographic parity constraint i.e $F_{h_{S}},F_{h_{T}}$ are close  with respect $W^{2}_{\rm AR}$, at the same time the extent of re-weighting in $F_{h_{T}}$ is minimal i.e the entropy $-\sum_{i=n_S + 1}^n w_i\log w_i$ is close to the maximal entropy $\log n_T$.  A related idea in \citet{pmlr-v115-jiang20a} deals with Wasserstein-1 constrained fair classification problems, but our approach of additionally re-weighting the observations offers more flexibility with possible ramifications in studying fairness in mis-specified models.  We achieve our inferential goal via an \emph{in-model} approach, based on the dual formulation of D-BETEL in equations \eqref{eqn:dbetel:alt}-\eqref{wbetel_dual}:
{\smaller
\begin{equation}\label{wgf_simultenous}
  \max_{w,\ \theta_{(S)},\ \theta_{(T)},\ \sigma^2} \bigg\{ - \frac{1}{n_S}\sum_{i=1}^{n_S} l_{i}(\theta_{(S)}\mid x_i) - \sum_{i=n_s + 1}^n w_i\ l_{i}(\theta_{(T)}\mid x_i)  - (1-\lambda^{\star}) W_{\rm AR}^{2}\big[F_{h_{S}}, F_{h_{T}}\big] - \lambda^{\star} \sum_{i=n_s + 1}^{n} w_i\log w_i \bigg\}
  \end{equation}
}
where $\sum_{i=n_s + 1}^{n} w_i = 1$ and $l_{i}(\theta_{(a_i)}\mid x_i) = (y_i - h(x_i, \theta_{(a_i)}))^2/2\sigma^2, \ i = 1,\ldots,n$. For a resulting re-weighting vector $w^{\star} = (w^{\star}_{n_S + 1},\ldots,w^{\star}_{n})^{\prime}$, we can obtain fair prediction at a new $x\in T$ via a weighted kernel density estimate at $x$.

As a competitor to the \emph{in-model} scheme described in \eqref{wgf_simultenous}, we introduce a \emph{two-step} procedure: \\
\textbf{Step 1:} We obtain model parameter estimates by
\begin{equation}\label{wgf_twostep_1}
 (\hat{\theta}_{(S)},\ \hat{\theta}_{(T)} ,\ \hat{\sigma}^2) =  \argmax_{\theta_{(S)},\ \theta_{(T)} ,\ \sigma^2} \bigg\{ - \frac{1}{n_S}\sum_{i=1}^{n_S} l_{i}(\theta_{(S)}\mid x_i) - \frac{1}{n_T}\sum_{i=n_s + 1}^n l_{i}(\theta_{(T)}\mid x_i)   \bigg\}
\end{equation}
followed by a post-processing step at $(\hat{\theta}_{(S)},\ \hat{\theta}_{(T)} ,\ \hat{\sigma}^2)$ to obtain $w^{\star}$  \\
\textbf{Step 2:}
\begin{equation}\label{wgf_twostep_2}
 w^{\star} = \argmax_{w} \big\{ -(1-\lambda^{\star})  \  W_{\rm AR}^{2}\big[F_{h_{S}}, F_{h_{T}}\big] - \lambda^{\star}  \sum_{i=n_s +1}^n w_i\log w_i \big\}.  
\end{equation}
Next, we shall assess the relative performance of the \emph{in-model} scheme in \eqref{wgf_simultenous} and  the \emph{two-step} scheme defined in \eqref{wgf_twostep_1}--\eqref{wgf_twostep_2} under two real data examples.

\begin{figure}[!htb]
    \centering
    \subfloat
    {{\includegraphics[width=7cm, height = 3.5cm]{art/figures/compas_data.pdf}}}%
    \quad
    \subfloat
    {{\includegraphics[width=6cm, height = 3.5cm]{art/Raw_Response.pdf} }}%
    \caption{\emph{\textbf{(a) COMPAS Dataset.} The histograms of raw recidivism score for African and non African-Americans show a clear discrepancy.\label{diag:compas_data} \textbf{(b) Distress Analysis Interview Corpus.} The histograms of raw PHQ-8  for the two biological genders show a clear discrepancy.}\label{diag:daic}} 
\end{figure}
\subsection{COMPAS recidivism  data analysis}
% re-wording needed
In this sub-section, we consider a case study on algorithmic criminal risk assessment. We shall focus here on the popular compas dataset \citep{https://doi.org/10.1111/rssa.12613} that includes detailed information on criminal history for the defendants in Broward County, Florida, freely available from the \href{https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis}{\textcolor{purple}{propublica}} website.  For each individual, several features on criminal history are available, such as the number of past felonies, misdemeanors, and juvenile offenses; additional demographic information includes the sex, age, and ethnic group of each defendant.  We focus on predicting two-year recidivism score $y$ (continuous) as a function of the defendant’s demographic
information except for race and criminal history $x$, while race (categorical) serves as a protected attribute. Algorithms for making such
predictions are routinely used in courtrooms to advise judges, and concerns about the fairness of such
tools with respect to the race of the defendants are raised. Therefore, it is of interest to develop novel methods to produce predictions while avoiding disparate treatment on the basis of the protected attribute race.


\begin{comment}
\begin{figure}[!htb]
    \centering
    %\subfloat[\centering COMPAS Dataset: Recidivism score]
    %{{\includegraphics[width=10cm, height = 6cm]{art/figures/compas_data.pdf} }}%
    %\quad
    %\subfloat
    %[\centering Wasserstein group fariness]
    {{\includegraphics[width=15cm, height = 6cm]{regression_plot_compas.pdf} }}%
    %\quad
    %\subfloat[\centering 5]{{\includegraphics[width=5.5cm]{Toy_WassersteinGF.pdf} }}%
    
    \caption{\emph{\textbf{COMPAS dataset.} \emph{Maximum likelihood estimates of the regression coefficients under both two-step and in model schemes. In the in model scheme the estimates get slightly modified since the regression coefficients and the weights assigned to the data are learned simultaneously.  For details on the in model and two-step approaches, refer to equations \eqref{wgf_simultenous} and \eqref{wgf_twostep_1}-\eqref{wgf_twostep_2} respectively.\textcolor{red}{Remove the plot.}}}\label{diag:daic_reg_plots}}
\end{figure}   
\end{comment}

For simplicity of exposition, we only consider two levels for the protected attribute race, namely, African-American or non-African-American, and consider a sub-sample of the entire data set with $100$ defendants corresponding to each level of the protected attribute. As covariate, for each defendant, we consider demographic information -- sex (binary), age (continuous), marital status (categorical); and criminal status -- legal status (categorical), supervision level (categorical), custody status (categorical). We use linear regression (i.e $h$ is linear in the covariates) as our predictive model of choice; the methodology readily extends to more complicated models. The histograms of raw recidivism score for African-Americans versus non-African-Americans show a clear discrepancy (refer to Figure \ref{diag:compas_data}). We shall assess the relative performance of the \emph{in-model} scheme in \eqref{wgf_simultenous} and \emph{two-step} scheme in \eqref{wgf_twostep_1}--\eqref{wgf_twostep_2} in ensuring demographic parity with respect to the protected attribute race (refer to Figure \ref{diag:compas_plots}). When we fit the predictive model without any fairness constraint, the fitted empirical cumulative distribution functions corresponding to the two sub-populations
%, i.e, African-Americans and non-African-Americans, 
are widely different. Our \emph{in-model} scheme, as well as \emph{two-step} scheme significantly reduce the discrepancy owing to their in-built fairness-based regularization. As expected, the \emph{in-model} scheme provides slightly lower bias since it performs the two-step optimization simultaneously.

\begin{figure}[!htb]
    \centering
    %\subfloat[\centering COMPAS Dataset: Recidivism score]
    %{{\includegraphics[width=10cm, height = 6cm]{art/figures/compas_data.pdf} }}%
    %\quad
    %\subfloat
    %[\centering Wasserstein group fariness]
    {{\includegraphics[width=15cm, height = 4cm]{art/figures/compas_plots.pdf} }}%
    %\quad
    %\subfloat[\centering 5]{{\includegraphics[width=5.5cm]{Toy_WassersteinGF.pdf} }}%
    
    \caption{\emph{\textbf{COMPAS dataset.} Empirical cdfs of  fitted $h$ for the two groups, with  no fairness constraint $( W_{AR} = 0.72)$, fair post-processing $( W_{AR} = 0.05)$, and fair model fitting with $( W_{AR}= 0.02)$ respectively at $\lambda^{\star} = 0$.}\label{diag:compas_plots}}
\end{figure}

\subsection{Distress Analysis Interview Corpus (DAIC)}
The Distress Analysis Interview Corpus (DAIC) is a multi-modal collection of semi-structured clinical interviews, available upon request from the \href{https://dcapswoz.ict.usc.edu/}{\textcolor{purple}{DAIC-WOZ}} website. Designed to simulate standard protocols for identifying people at risk for post-traumatic stress disorder (PTSD) and major depression, these interviews were collected as part of a larger effort to create a computer agent that interviews people and identifies verbal and nonverbal indicators of mental illness. Participants are drawn from two distinct populations living in the Greater Los Angeles metropolitan area – veterans of the U.S. armed forces and from the general public and are coded for depression, PTSD, and anxiety based on accepted psychiatric questionnaires. The corpus contains audio, video, and depth sensor (Microsoft Kinect) recordings of all the interviews, generated logs of the character’s speech and nonverbal behavior events, questionnaire data, and interview transcriptions. For further details on the data set, readers are advised to refer to \citet{gratch-etal-2014-distress}.

\begin{comment}
\begin{figure}[!htb]
    \centering
    %\subfloat[\centering COMPAS Dataset: Recidivism score]
    %{{\includegraphics[width=10cm, height = 6cm]{art/figures/compas_data.pdf} }}%
    %\quad
    %\subfloat
    %[\centering Wasserstein group fariness]
    {{\includegraphics[width=15cm, height = 6cm]{DAIC_regplots.pdf} }}%
    %\quad
    %\subfloat[\centering 5]{{\includegraphics[width=5.5cm]{Toy_WassersteinGF.pdf} }}%
    
    \caption{\emph{\textbf{Distress Analysis Interview Corpus.} \emph{Maximum likelihood estimates of the regression coefficients under both two-step and in model schemes. In the in model scheme the estimates get slightly modified since the regression coefficients and the weights assigned to the data are learned simultaneously.  For details on the in model and two-step approaches, refer to equations \eqref{wgf_simultenous} and \eqref{wgf_twostep_1}-\eqref{wgf_twostep_2} respectively.\textcolor{red}{Remove the plot.}}}\label{diag:daic_reg_plots}}
\end{figure}
\end{comment}
We are particularly interested in the PHQ-8 score that captures the severity of depression. The scores range from $0$ to $27$ with a score from $0-4$ considered none or minimal, $5-9$ mild, $10-14$ moderate, $15-19$ moderately severe, and $20-27$ severe. In this application, we work with this PHQ-8 (continuous response), biological gender (binary protected attribute), and $17$ derived audio features (continuous covariates) corresponding to the $n = 107$ subjects \ref{diag:daic}. The histograms of the PHQ-8 score for two biological genders show a clear discrepancy (refer to Figure \ref{diag:daic}). Therefore, we shall assess the relative performance of the \emph{in-model} scheme in \eqref{wgf_simultenous}) and \emph{two-step} scheme \eqref{wgf_twostep_1}--\eqref{wgf_twostep_2} in ensuring demographic parity with respect to biological gender (refer to Figure \ref{diag:daic_plots}).  As earlier, for the sake of simplicity of exposition, we use linear regression (i.e $h$ is linear in the covariates) as our predictive model of choice.  When we fit the predictive model without any fairness constraint, the fitted empirical cumulative distribution functions corresponding to the two biological genders are widely different. Our \emph{in model} scheme, as well as \emph{two-step} scheme significantly reduce the discrepancy owing to their in-built fairness-based regularization. As noted earlier, the \emph{in model} scheme provides lower bias since it performs the two-step optimization simultaneously.
\begin{figure}[!htb]
    \centering
    %\subfloat[\centering COMPAS Dataset: Recidivism score]
    %{{\includegraphics[width=10cm, height = 6cm]{art/figures/compas_data.pdf} }}%
    %\quad
    %\subfloat
    %[\centering Wasserstein group fariness]
    {{\includegraphics[width=15cm, height = 4cm]{DAIC_plots.pdf} }}%
    %\quad
    %\subfloat[\centering 5]{{\includegraphics[width=5.5cm]{Toy_WassersteinGF.pdf} }}%
    
    \caption{\emph{\textbf{Distress Analysis Interview Corpus.} \emph{Empirical cdfs of  fitted $h$ for the two groups, with  no fairness constraint $( W_{AR} = 19.32)$, fair post-processing $( W_{AR} = 2.24)$, and fair model fitting with $( W_{AR}= 0.79$) respectively at $\lambda^{\star} = 0$. }}\label{diag:daic_plots}}
\end{figure}

\section{Discussion}
%\textcolor{red}{Can you review the discussion?}
%% On all encompassing tool for trust-worthy AI
Generative probabilistic models are immensely popular in applications as they provide a general recipe for statistical inference using the maximum likelihood or Bayesian framework. However, it is also well understood that the resulting inference can crucially depend on the modeling assumptions. 
%One avenue to address such concerns is to build increasingly flexible models with a large number of parameters.  
In this article, we introduced a flexible Bayesian semi-parametric modeling framework $\mbox{D}$-BETEL, and demonstrated it's utility to conduct robust inference under perturbations of the data-generating mechanism. $\mbox{D}$-BETEL is endowed with a fully data-driven hyper parameter tuning scheme, and enjoys a valid generative model interpretation, which is scarce in pseudo-likelihood based robust Bayesian methods. R scripts to reproduce the results presented in the article are available at \href{https://github.com/zovialpapai/D-BETEL}{\textcolor{purple}{zovialpapai/D-BETEL}}.

While semi-parametric in nature, a particularly attractive feature of $\mbox{D}$-BETEL is that the user only needs to specify a plausible family of probability models $F_\theta$ for the data along with a prior distribution for the parameter of interest $\theta$, and does not need to explicitly model departures from the parametric guess as is typical with nonparametric Bayesian techniques; all nuisance parameters are implicitly marginalized out and a marginal posterior for $\theta$ is returned. It remains possible to retrieve a discretized estimate of the generating distribution to allow a more fine-grained analysis of how the data departs from the parameteric guess. The proposed approach is also very general; while we have illustrated its usage for i.i.d. and independent non-i.i.d (i.n.i.d.) setups, extensions to broader classes of dependent data models should be straightforward. Studying theoretical properties of $\mbox{D}$-BETEL, especially second-order properties, is an interesting avenue for future work. 

Our framework can also be extended beyond the traditional statistical modeling setup to mitigate inherent biases in machine learning applications, We have offered an illustration in the context of algorithmic fairness, and we wish to investigate further applications in trustworthy AI, encompassing robustness, fairness \& differential privacy, in the future. We also believe the flexibility and the computational simplicity of our novel Wasserstein metric ANDREW may render itself useful in embedding of complex objects, e.g words, images, as probability distributions \citep{Jebara2004ProbabilityPK, https://doi.org/10.48550/arxiv.1412.6623} which has emerged as a popular application of optimal transport to machine learning problems. 


%We introduced a flexible Bayesian semi-parametric modeling framework $\mbox{D}$-BETEL, endowed with a fully data-driven hyper parameter tuning scheme and demonstrated it's utility to conduct robust inference in presence of slight perturbation of the data generating mechanism. Critically, $\mbox{D}$-BETEL continues to enjoy valid generative model interpretation, which is scarce in pseudo-likelihood robust Bayesian methods. The methodology developed in this article is fairly general, and readily extends to a host of critical inferential tasks. In particular, the constrained entropy maximization framework sitting at the heart of our likelihood formulation is adapted to conduct robust portfolio allocation that significantly generalises existing moment condition based portfolio allocation framework.  
%To the best of our knowledge and understanding, we do not foresee any negative societal consequences of our findings in this article.
%Moreover, it is recognised that the machine learning algorithms, when utilised in critical decision making tasks, often inherit the inherent biases in the training data-sets, the extension of  our robust inference methodology to demographic parity applications  contribute towards alleviating some of these biases. In future, we hope to extend our  framework to build an all inclusive tool for trustworthy AI, encompassing robustness, fairness \& differential privacy. 

%Moreover, the computational simplicity of our novel Wasserstein metric may render itself useful in many optimal transport-based machine learning applications, e.g embedding complex objects in probability spaces.

% Wasserstein metric
%We believe the flexibility and the computational simplicity of our novel Wasserstein metric ANDREW may render itself useful in many optimal transport-based machine learning applications. One of the holy grail of modern machine learning is to compute meaningful low-dimensional embeddings for complex objects, to tackle advanced tasks like inference on texts using word embedding \citep{https://doi.org/10.48550/arxiv.1301.3781, pennington2014glove}, improved image understanding \citep{2be2c4c3e8504383bf53ebeb79f9caf4}, representations of knowledge graphs \citep{vilnis-etal-2018-probabilistic, ijcai2021p278} etc. Until recently, such embeddings have been recovered by seeking embeddings in lower-dimensional Euclidean spaces. However, a recent trend to embed complex objects as probability measures in $\mb{R}^d$  has emerged  \citep{Jebara2004ProbabilityPK, https://doi.org/10.48550/arxiv.1412.6623}. Point embeddings can be regarded as a degenerate case of probabilistic embedding, in which the uncertainty is concentrated on a single point. On the other hand, probability measures can be more spread out, heavy-tailed, skewed or multi-modal, etc., and offer additional flexibility. Naturally, such flexibility can only be exploited by defining a metric or divergence on a (sub-)space of probability measures, and optimal transport serves as an excellent choice owing to its versatility and numerical stability \citep{pmlr-v108-singh20a, Muzellec2018GeneralizingPE}.

% Future theoretical work
%\textcolor{blue}{In numerical studies in section \ref{Robust_Bayes}, we empirically observe that, $\mbox{D}$-BETEL is more resistant towards the presence of outliers compared to standard Bayesian approaches. However the relative advantages wear off, as expected, as the extent of contamination in the data generation mechanism increases.
%It will be interesting to explore if we can show semi-parametric efficiency under a robust framework. There is existing literature on finding minimax rates in $\epsilon$-contamination model: \cite{epsiloncontamination}, which is somewhat related.  This is standalone theoretical problems on their own, and we leave it as avenue for future enquiry.}

%This is the concluding part of the paper.  It is only needed if it contains new material.It  should not repeat the summary or reiterate the contents of the paper.

\section*{Supplemental Document}
An online supplement contains proofs of the theorems stated in the main document along with auxiliary lemmas, and additional numerical results. 
%The R scripts to reproduce the results presented in the article are available in this \href{https://github.com/zovialpapai/D-BETEL}{\textcolor{blue}{GitHub repository}}.
\vspace{-0.2in}
\section*{Acknowledgement} Drs. Bhattacharya and Pati acknowledge NSF DMS-1916371 and NSF DMS-2210689 for partially funding the project. 

\bibliography{paper-ref}
%\bibliographystyle{apalike}
\bibliographystyle{plainnat}
%\setcitestyle{round,aysep={},yysep={;}}
\setcitestyle{notesep={; },round,aysep={},yysep={;}}

%\bibliographystyle{apalike}


\clearpage

\beginsupplement
\begin{center}
\section*{Supplementary material to \\
``Robust probabilistic inference via a constrained transport metric" }\label{SM}
 Abhisek Chakraborty, Anirban Bhattacharya, Debdeep Pati\\
 Department of Statistics, Texas A\&M University, College Station, TX, USA
\end{center}
Section \ref{ssec:th1th2_proof} contains proofs of Theorems \ref{th2:lemma2} and \ref{th:npbayes_equivalence} in Section \ref{ssec:npBayes} in the main document, relating to the non-parametric Bayes interpretation of our semi-parametric methodology D-BETEL. Section \ref{ssec:th3} contains the proof of Theorem \ref{th3} in Section \ref{ssec:andrew} in the main document, concerning the tailor-made transport metric ANDREW. Sections \ref{aux:th1th2} and \ref{aux:th3} record auxiliary results for Theorems \ref{th2:lemma2}-\ref{th:npbayes_equivalence} and Theorem \ref{th3}, respectively. Section \ref{sup:glm} presents additional numerical results on the generalised linear regression example discussed in Section \ref{ssec:gen_reg} in the main document. All bibliographical references can be found in the main document. 
\vspace{0.3in}
%Section \ref{ssec:po} presents an extension of the D-BETEL framework to entropy based portfolio allocation problems, and Section \ref{aux:ssec:po} contain related definitions.


%Further material such as technical details, extended proofs, code, or additional  simulations, figures and examples may appear online, and should be briefly mentioned as Supplementary Material where appropriate.  Please submit any such content as a PDF file along with your paper, entitled `Supplementary material for Title-of-paper'.  After the acknowledgements, include a section `Supplementary material' in your paper, with the sentence `Supplementary material available at \Bka\ online includes $\ldots$', giving a brief indication of what is available.  However it should be possible to read and understand the paper without reading the supplementary material.

%Further instructions will be given when a paper is accepted.
%\section*{Convention}
%Equations defined in this document are numbered (S1), (S2) etc. Equations called as (1.1), (1.2) etc refer to equations defined in the main document. 

\section{Proofs of Theorems \ref{th2:lemma2} and \ref{th:npbayes_equivalence} in the main document}\label{ssec:th1th2_proof}
\subsection{Preliminaries}
Let us first set up some notation to facilitate the proofs of Theorems \ref{th2:lemma2} and \ref{th:npbayes_equivalence}. 
%First, observe that the prior on $(\xi_1, \ldots, \xi_N)$ in equation block \ref{eqn:npbayes1} induces a distribution on the prior for $(b_1,\ldots, b_k)$ where $b_h = \sum_{j=1}^N \ind(\xi_j = \mu_h)$   given by $\mbox{Multinomial}(N;1/k, \ldots, 1/k)$ with probability mass function $N!/(k^N \prod_{h=1}^k b_h!)$. 
First, observe that the prior on $(b_1,\ldots, b_k)$ in equation block \eqref{eqn:npbayes1} is $\mbox{Multinomial}(N;1/k, \ldots, 1/k)$ with probability mass function $N!/(k^N \prod_{h=1}^k b_h!)$.  Next, we re-parameterize $(b_1,\ldots,b_k)\to (w_1,\ldots,w_k)$ and define %introduce the sets, %useful in the proofs of Theorems 1 and 2, of the form
\begin{align}\label{eqn_W_j}
    &\mb{W}_j = \bigg\{w: w_{h} = \frac{b_{h}}{N},\ h=1,\ldots,j;\  b_h \in \mb{Z}^{+},\ \sum_{h=1}^j b_h = N\ \bigg\},\quad j\in\mb{Z}^{+},
\end{align}
where $\mb{Z}^{+}$ is set of all positive integers. 
%Next, to fully set up the premise for the proof of Theorem 1, we introduce the sets of the form
Also, define
\begin{align}\label{eqn_W_j_Tilde}
   &\wt{\mb{W}}_j=\bigg\{w\in\mb{W}_j:\big|w_h - w^{\star}_h\big| < \frac{1}{N^{1-\delta}}, \forall\ h=1,\ldots,j \bigg\}, \quad j\in\mb{Z}^{+},
\end{align}
where $w^{\star}_h, \ h=1,\ldots, j$ is the solution to the $\mbox{D}$-BETEL optimization without the parametric constraint, and we assume that $1/2<\delta<1$. The proof of Theorem \ref{th2:lemma2} involves studying a log ratio of the form
\begin{align}\label{eqn_R_j}
R_j = &\ \log\ \Bigg\{\frac{\prod_{h=1}^j {w^{\star}_{h}}^{-w_{h}^{\star} N + \frac{1}{2}}}{\prod_{h=1}^j w_{h}^{-w_{h} N + \frac{1}{2}}}\Bigg\}
= N\big[ H_{N}(w^{\star}) - H_{N}(w)\big] + \frac{1}{2} \sum_{h=1}^j \log\bigg(\frac{w^{\star}_{h}}{w_h}\bigg), \quad j\in\mb{Z}^{+},
\end{align}
%in the carefully constructed neighbourhood $\wt{\mb{W}}_j$ of $w^{\star}$ and outside it, i.e, 
over $\mb{W}_j\setminus\wt{\mb{W}}_j$, where $H_N(w) = -\sum_{h=1}^j w_h\log w_h$.

Further, %to fully set up the premise for the proof of Theorem 2,  
we follow the definition in \eqref{eqn:andrewdef} to introduce 
\begin{align}\label{eqn_C}
 &C_{\theta,\varepsilon} =
\bigg \{w\in\mb{W}_n:\ \mbox{D}[F_{\theta}, \nu(w,x)]  \leq \varepsilon \bigg\},
%\text{where}\
%&\mb{W}_n = \bigg\{w: w_{h} = \frac{b_{h}}{N},  \sum_{h=1}^n b_h = N, b_h \in \mb{Z}^{+}\bigg\}.
\end{align}
In the constrained $\mbox{D}$-BETEL formulation, for every $\theta\in\Theta$: 
\begin{align}\label{eqn_DBETEL}
  w^{\star}_{1:n}(\theta,\varepsilon) = \argmax_{w\in C_{\theta,\varepsilon}} H_N(w),
\end{align}
and $\varepsilon>0$. With slight abuse of notations, we shall use $w^{\star}_h$ in place of  $w^{\star}_h(\theta,\varepsilon)$.  In places, it is useful to resort to the dual form of the maximization problem above, introduced in equation \eqref{eqn:dbetel:alt} in the main document:
\begin{align}\label{eqn_DBETEL_dual}
    w^{\star}_{1:n}(\theta,\varepsilon) = \argmax_{w\in \mb{W}_n} g(w)\quad \text{where}\quad g(w) = H_N(w) - \lambda_{\star}\ \mbox{D}^2(F_{\theta} ,  \nu(w, x)),
\end{align}
where there exists a $\lambda_{\star}>0$ for every choice of $\varepsilon>0$. The proof of Theorem \ref{th:npbayes_equivalence} hinges on studying the log ratio of the form $R_n = N\big[ H_{N}(w^{\star}) - H_N(w)\big] + \frac{1}{2} \sum_{h=1}^n \log(w^{\star}_{h}/w_h)$, defined in equation \eqref{eqn_R_j}.
%\begin{align}\label{eqn_R}
%R_n = &\ \log\ \Bigg\{\frac{\prod_{h=1}^n {w_{h}{^{\star}}^{-w_{h}^{\star} N + \frac{1}{2}}}}{\prod_{h=1}^n w_{h}^{{-w_{h} N + \frac{1}{2}}}}\Bigg\}
%= N\big[ H_{N}(w^{\star}) - H_N(w)\big] + \frac{1}{2} \sum_{h=1}^n \log\bigg(\frac{w^{\star}_{h}}{w_h}\bigg).
%\end{align}
In order to study the behaviour of $R_n$ in a carefully constructed neighbourhood of $w^{\star}$ and outside it, we introduce the set 
%{\smaller
\begin{align}\label{eqn_C_Tilde}
\wt{C}_{\theta, \varepsilon}=
\bigg\{w\in C_{\theta, \varepsilon}:\  
&\mbox{D}^2[F_{\theta} , \nu(w^{\star}, x) ] - \frac{1}{N^{1-\delta}}\leq \mbox{D}^2[F_{\theta} , \nu(w, x) ],\notag\\
&0\leq H_{N}(w^{\star}) - H_{N}(w)\leq \frac{1}{N^{1-\delta}} \bigg\}
\end{align}
%}
for some $0<\delta<1$. In this article, we focus on the case  $\mbox{D}\equiv W_{\rm AR}$ (refer to equations \eqref{eqn_C}, \eqref{eqn_DBETEL_dual}, \eqref{eqn_C_Tilde}) for the sake the proof of Theorem \ref{th:npbayes_equivalence}. More precisely, we utilize $\mbox{D}\equiv W_{\rm AR}$ in the proofs of Lemmas \ref{th3:lemma1}-\ref{th2_lemma2_asymtotic} that leads to the proof of Theorem \ref{th:npbayes_equivalence}. However, we envision that the proof steps hold true for a more general class of distance metric $\mbox{D}$, of which ANDREW in Section \ref{ssec:andrew} is a special case.

Before we proceed further, we briefly discuss  the intuitions behind the choice of  $\wt{C}_{\theta, \varepsilon}$ in the proof of Theorem \ref{th:npbayes_equivalence}, especially in connection to a similar approach taken in the proof of the main theorem  regarding non-parametric Bayes interpretation of Bayesian exponentially tilted empirical likelihood for moment conditional model in \citet{10.1093/biomet/92.1.31}. Given a random sample $x = (x_1,\ldots, x_n)^\T$ from an unknown data generating distribution $P$ on $\mathbb{R}^d$, the exponentially tilted empirical likelihood for moment conditional model takes the form:
\begin{equation}
  L_{\rm MCM}(\theta) = \bigg\{\prod_{i=1}^n w_{i} :  \argmax_{w}\prod_{i=1}^n w_{i}^{-w_{i}},\ w_i>0,\ \sum_{i=1}^n w_i = 1, \ \sum_{i=1}^n w_i\ u(x_i,\theta)=0  \bigg\},\quad \theta\in\Theta, 
\end{equation} 
where $u:\mathbb{R}^d\times\Theta\to\mathbb{R}$. For the above formulation, a choice of neighbourhood of the form
%{\smaller
\begin{align}\label{C_MCM}
&{C}_{\theta,  \rm MCM}=
\bigg\{w\in \mb{W}_n: \sum_{i=1}^n w_i\ u(x_i, \theta) = 0\bigg\},\notag\\
&\wt{C}_{\theta,  \rm MCM}=
\bigg\{w\in {C}_{\theta,  \rm MCM}: |w_h - w_{h}^{\star}|<\frac{1}{N^{1-\delta}},\ h = 1,\ldots,n \bigg\} ,
\end{align}
%}
where $w^{\star}_{1:n} = \argmax_{w\in C_{\theta, \rm MCM}} H_N(w)$ and $0<\delta<1$, will be apt since 
\begin{align}
     \bigg|\sum_{i=1}^n w_{i}^\star\ u(x_i,\theta) - \sum_{i=1}^n w_i\ u(x_i,\theta)\bigg|\leq \sum_{i=1}^n |w_{i}^\star -  w_{i}||u(x_i,\theta)| \leq \frac{1}{N^{1-\delta}}\sum_{i=1}^n |u(x_i,\theta)| \downarrow 0,
\end{align}
and $H_{N}(w^{\star}) - H_{N}(w)\to 0$ as $ N\to\infty$. Further, $\wt{C}_{\theta,  \rm MCM}$ satisfies two conditions: (i) for every $w\in \wt{C}_{\theta,  \rm MCM}$, $|\prod_{i=1}^n w_i - \prod_{i=1}^n w_{i}^{\star}|\to 0$ as $N\to\infty$, and  (ii) for every $w\in \mbox{W}_j\setminus\wt{C}_{\theta, \rm MCM}$, $N[H_N(w^*) - H_N(w)]> N^{\delta}$.

However, in the proof of our Theorem \ref{th:npbayes_equivalence} for exponentially tilted empirical likelihood with distance based constraints, a simple choice of neighbourhood of $w^{\star}$ like in equation \eqref{C_MCM} poses significant algebraic challenges to analyse the behaviour of $R_n$ in  and outside the neighbourhood. This motivates the choice of the neighborhood of the form
%{\smaller
\begin{align}
\wt{C}_{\theta, \varepsilon}=
\bigg\{w\in C_{\theta, \varepsilon}:
&\mbox{D}^2[F_{\theta} , \nu(w, x) ]- \frac{1}{N^{1-\delta}}\leq \mbox{D}^2[F_{\theta} , \nu(w, x) ],\notag\\
&0\leq H_{N}(w^{\star}) - H_{N}(w)\leq \frac{1}{N^{1-\delta}} \bigg\}.
\end{align}
%}
We focus on $\mbox{D}\equiv W_{\rm AR}$ and demonstrate that $\wt{C}_{\theta, \varepsilon}$ satisfies the two conditions, i.e (i) for every $w\in \wt{C}_{\theta, \varepsilon}$, $|\prod_{i=1}^n w_i - \prod_{i=1}^n w_{i}^{\star}|\to 0$ as $N\to\infty$, and  (ii) for every $w\in {C}_{\theta, \varepsilon}\setminus \wt{C}_{\theta, \varepsilon}$, $N[H_N(w^*) - H_N(w)]> N^{\delta}$. 
The arguments presented in the proofs of Theorem \ref{th:npbayes_equivalence} and corresponding auxiliary results can potentially be extended for other transport metrics.


Now we are in the position to present the proofs of Theorems \ref{th2:lemma2} and \ref{th:npbayes_equivalence}.

\subsection{Proof of Theorem \ref{th2:lemma2}}
\begin{proof}
For the sake of clarity, throughout the proof we shall denote $k$ as $k_N$, in order to recognise it as sequence of random variables indexed by $N$. We shall proceed to prove:
\begin{align}
    P(k_N \neq n \quad \text{infinitely often} \mid x_{1:n}) = 0,
\end{align}
which will follow from the fact that,
\begin{align}\label{1BC_condition}
  \sum_{N=N^{\star}}^{\infty} P(k_N \neq n  \mid x_{1:n}) < \infty,
\end{align}
for a finite $N^{\star} = 1 + \bigg[\big\{\min_{s\neq t} |x_s -x_t|_{\infty}\big\}^{-1/(\alpha-\beta)}\bigg]$, followed by the application of the first Borel--Cantelli Lemma. 

Our general strategy will be to begin with the joint distribution of 
\begin{align*}
  (\xi^{\star}, x_{1:n})=  (k, \ b_1,\ldots,b_k,\ \mu_{1},\ldots, \mu_{k},\ x_{1:n})  
\end{align*}
in equation blocks \eqref{eqn:npbayes3}--\eqref{eqn:npbayes2} and compute the marginal posterior of $P(k_N\mid x_{1:n})$. We shall prove equation \eqref{1BC_condition} in two parts:
$\sum_{N=N^{\star}}^{\infty} P(k_N > n  \mid x_{1:n}) < \infty,\ \text{and}\ \sum_{N=N^{\star}}^{\infty} P(k_N < n  \mid x_{1:n}) < \infty.$  %Here, set $N^{\star}_1 = N^{\star}_2 = N^{\star}$.  

\subsection*{Part 1}
For the proof of the fact that $\sum_{N=N^{\star}}^{\infty} P(k_N >n  \mid x_{1:n}) < \infty$, we consider
\begin{align}\label{eqn:part1}
    P(k_N > n  \mid x_{1:n}) 
    &= \sum_{j=n+1}^{\infty} P(k_N =j  \mid x_{1:n})\notag\\ 
    &= \frac{ \sum_{j=n+1}^{\infty}\ p(j)\ m(x_{1:n}
    \mid k_N = j)}{\sum_{j=1}^{\infty}\ p(j)\ m(x_{1:n}
    \mid k_N = j)}
    \leq \frac{ \sum_{j=n+1}^{\infty}\ p(j)\ m(x_{1:n}
    \mid k_N = j)}{p(n)\ m(x_{1:n}
    \mid k_N = n)}\notag\\ 
   & \leq \bigg\{\frac{\sup_{j>n} m(x_{1:n}
    \mid k_N = j) }{m(x_{1:n}
    \mid k_N = n)}\bigg\} \frac{ \sum_{j=n+1}^{\infty}\ p(j)\ }{p(n)}\notag\\
    &\leq M_{j,n, N}\ \frac{(1-p_N)^{(n-1+1)}}{p_N(1-p_N)^{(n-1)} Q^{-1}_{N,n}}
    \leq  M_{j,n, N}\ \frac{(1-p_N)}{p_N M_{N}^{-N}} \notag\\
    &=   \frac{M_{j,n, N}}{ (AM_{N}^{N+1}-1)M_{N}^{-N}}
    \leq    \frac{M_{j,n, N}}{ AM_{N}-A}
    \leq   \frac{M_{j,n, N}}{AN^{\alpha d}}.
\end{align}
where 
\begin{align}\label{eqn:bounded_ratio_marginallikelihhod}
     M_{j,n, N} = \frac{\sup_{j>n} m(x_{1:n}
    \mid k_N = j) }{m(x_{1:n}
    \mid k_N = n)}.
\end{align}
The second step follows from  the fact that the marginal likelihood of data is a weighted average of marginal likelihood of data given the number of mixture components $k_N$, i.e $m(x_{1:n}) =   \int_{\xi^{\star}} \pi_{\infty,N}(\xi^{\star})P^{(N)}(x_{1:n}\mid \xi^{\star})d\xi^{\star} = \sum_{j=1}^{\infty}\ p(j)\ m(x_{1:n} \mid k_N = j) $. The third and fourth steps are trivial.  The fifth steps follows from the fact that the prior probability attached to any particular SRSWOR of size $n$ from $\mbox{H}^{(N)}$ is $p_N(1-p_N)^{(n-1)} Q^{-1}_{N,n}$, and the prior probability of the event  $k>n$ is $(1-p_N)^{(n-1+1)}$. The sixth step follows from $Q_{N,n}<M_{N}^N$ and $p_N = 1- 1/AM_{N}^{N+1}$. The seventh and the eighth step are trivial, and the last step follows from $M_N = (N^{\alpha} + 1)^d > N^{\alpha d} +1$. Finally, if we can show $M_{j,n, N} \leq K_1$ in equation \eqref{eqn:bounded_ratio_marginallikelihhod} for large enough $N$, first part of the proof would follow from equation \eqref{eqn:part1} since $\sum_{N=1}^{\infty} 1/N^{\alpha d}<\infty$ for $\alpha d > 1$. 

Next, we show that  $M_{j,n, N} \leq K_1$ for some $K_1>0$. To that end, under the hierarchical model in equation \eqref{eqn:npbayes3}--\eqref{eqn:npbayes2} in the main document, the unconstrained posterior of $\xi^{\star}\mid x_{1:n}$ is  
\begin{align}\label{unconstrained_post}
&\pi^{(free)}_{N}(\xi^{\star}\mid x_{1:n})  
 \ \propto \ \pi_{\infty, N}(\xi^{\star})\
P^{(N)}(x_{1:n}\mid \xi^{\star}) \notag\\ 
%& = \bigg\{p(k)\ \prod_{h=1}^k \mbox{H}(\mu_h) \bigg\}\ \bigg\{\prod_{j=1}^N \bigg[\sum_{h=1}^k k^{-1}\ \delta_{\mu_{h}}(\xi_j)  \bigg]\bigg\}\
%\bigg\{\frac{1}{N^n}\prod_{i=1}^n \bigg[\sum_{j=1}^N \mbox{U}_d(x_i\mid\xi_j,\tau^{-1}I) \bigg] \bigg\}\notag\\
& = \bigg\{p(k)\prod_{h=1}^k \mbox{H}^{(N)}(\mu_h) \bigg\} \
\bigg\{\frac{1}{k^N}\ \frac{N!}{\prod_{h=1}^k b_h !} \bigg\}\ 
\bigg\{\prod_{i=1}^n \bigg[\sum_{h=1}^k \frac{b_h}{N} \mbox{U}_d(x_i\mid\eta_{h},\tau^{-1}I) \bigg] \bigg\},
\end{align}
where we use $ \mbox{U}_d(x_i\mid\eta_h,\tau^{-1}I)$ to denote $\prod_{j=1}^d \mbox{Uniform}(\eta_{h, j} -  \tau^{-1},\ \eta_{h, j} +  \tau^{-1})$. From the equation \eqref{unconstrained_post}, we have
\begin{align}\label{unconstrained_marglik}
    &m(x_{1:N}\mid k_N = j)\notag\\ 
    = &\sum_{w_{1:j}}\sum_{\mu_{1:j}}\prod_{h=1}^j \mbox{H}^{(N)}(\mu_h)\ \bigg\{\frac{N!/\prod_{h=1}^j b_h!}{j^N}\bigg\}\prod_{i=1}^n \bigg\{\sum_{h=1}^j w_h \tau^{d}_{N}\ \delta(|x_i - \eta_h|_{\infty} <2\rho_N)\bigg\}\notag\\
    = & \frac{\tau^{d}_{N}}{M_{N}^{j}}\sum_{w_{1:j}}\ \bigg\{\frac{N!/\prod_{h=1}^j b_h!}{j^N}\bigg\}\prod_{i=1}^n  w_i \notag\\
   = & \frac{\tau^{dn}_{N}}{M_{N}^{j}}\bigg[\sum_{w\in \wt{W}_j}\ \bigg\{\frac{N!/\prod_{h=1}^j b_h!}{j^N}\bigg\}\prod_{i=1}^n  w_i + \sum_{w\in W_j\setminus\wt{W}_j}\ \bigg\{\frac{N!/\prod_{h=1}^j b_h!}{j^N}\bigg\}\prod_{i=1}^n  w_i \bigg],
\end{align}
where $\mb{W}_j$ and $\wt{\mb{W}}_j$ are as in equations \eqref{eqn_W_j} and \eqref{eqn_W_j_Tilde}.  The first step holds true since exactly $1$ indicator in the sum  $\sum_{h=1}^j w_h \tau^{d}_{N}\ \delta(|x_i - \eta_h|_{\infty} <2\rho_N)$ corresponding to $x_i, i =1,\ldots,n$ survives as long as  $N\geq N^{\star}$ where it is sufficient to ensure
that
\begin{align}
    \min_{s\neq t} |x_s -x_t|_{\infty} > 2\rho_{N^{\star}} = \frac{1}{(N^{\star})^{\alpha-\beta}}\iff N^{\star} > \big\{\min_{s\neq t} |x_s -x_t|_{\infty}\big\}^{-1/(\alpha-\beta)}.
\end{align} 
Without loss of generality, we assume that only the $i$-th indicator in the sum  $\sum_{h=1}^j w_h \tau^{d}_{N}\ \delta(|x_i - \eta_h|_{\infty} <2\rho_N)$ corresponding to $x_i, i =1,\ldots,n$ survives. Next, we  simplify the equation \eqref{unconstrained_marglik} further, and to that end we note that:

$(i)$ The solution to the $\mbox{D}$-BETEL optimization $w^{\star}_h, \ h=1,\ldots, j$ without the parametric constraint  is $(1/j,\ldots, 1/j)^\T$, and yields
\begin{align}\label{DBETEL_max}
   \bigg\{\frac{N!/\prod_{h=1}^j b^{\star}_h!}{j^N}\bigg\}\prod_{i=1}^n  w_{i}^{\star} = \frac{1}{\sqrt{2\pi}^{j-1}}\frac{j^{-(n + j/2)}}{N^{j-1/2}}.
\end{align}
Further,  for any $w\in\mb{W}_j\setminus\wt{\mb{W}_j}$, we consider the log ratio
\begin{align}\label{eqn:ratio}
R_j = &\ \log\ \Bigg\{\frac{\prod_{h=1}^j {w^{\star}_{h}}^{-w_{h}^{\star} N + \frac{1}{2}}}{\prod_{h=1}^j w_{h}^{-w_{h} N + \frac{1}{2}}}\Bigg\}\notag\\
= &\ N \sum_{h=1}^j (w_h - w^{\star}_h)\log w^{\star}_h + N\sum_{h=1}^j w_h \log\bigg(\frac{w_h}{w^{\star}_{h}}\bigg) + \frac{1}{2} \sum_{h=1}^j \log\bigg(\frac{w^{\star}_{h}}{w_h}\bigg)\notag\\
\geq&\ N\sum_{h=1}^j w_h \log\bigg(\frac{w_h}{w^{\star}_{h}}\bigg)
=\ N\ \mbox{KL}(w\mid\mid w^{\star})
\geq\ N\times 2 \bigg\{|w-w^{\star}|_{\rm TV}\bigg\}^2\notag\\
\geq&\ N\ \bigg\{|w-w^{\star}|_{\rm TV}\bigg\}^2 
= N\ \bigg\{\sup_{h=1,\ldots j} |w_h-w^{\star}_h|\bigg\}^2  \geq N\ \bigg\{\inf_{h=1,\ldots j}
|w_h-w^{\star}_h|\bigg\}^2 \notag\\
\geq& N\ \times \bigg\{\frac{1}{N^{1-\delta}}\bigg\}^2 = N^{\delta^{\star}}.
\end{align}
where $\delta^{\star} = 2\delta - 1 >0$ since $\delta >1/2$   by assumption. The first inequality holds since  $N \sum_{h=1}^j (w_h - w^{\star}_h)\log w^{\star}_h = -N\log j \sum_{h=1}^j(w_h - w^{\star}_h) = 0$, and $\sum_{h=1}^j  \log\big(w^{\star}_{h}/w_h\big)$ is non-negative. The second inequality is due to the Lemma \ref{Pinsker}. Rest is simple algebra.
%$\log(w_h/w^{\star}_{h}) = \log(1 \pm j/N^{1-\delta}) = \mb{O}(1/N^{1-\delta})$  and the  second term is $\mb{O}( N^{\delta})$, 
Finally, the set $\mb{W}_j\setminus\wt{\mb{W}}_j$ can contain at max $N^n$ elements. From the equations \eqref{DBETEL_max}-\eqref{eqn:ratio},  the contributions corresponding to the elements of the set $W_j\setminus\wt{W}_j$ can be bounded as follows:
%{\smaller
\begin{align}\label{term1}
     &0\leq\sum_{w\in W_j\setminus\wt{W}_j}\ \bigg\{\frac{N!/\prod_{h=1}^j b_h!}{j^N}\bigg\}\prod_{i=1}^n  w_i \leq \bigg(\frac{N^{n}}{e^{N^{\delta^{\star}}}}\bigg)\times \frac{j^{-(n + j/2)}}{N^{j-1/2}}  = \mbox{UB}_{j, \rm neglect}.
\end{align}
%}

$(ii)$ For the contributions corresponding to the elements of the set $\wt{W}_j$:
%{\small
\begin{align}\label{term2}
    &h_{j,N}\times \frac{1}{\sqrt{2\pi}^{j-1}} \frac{1}{N^{j-1/2}}\mbox{LB}_{j,N}\leq\sum_{w\in \wt{W}_j}\ \bigg\{\frac{N!/\prod_{h=1}^j b_h!}{j^N}\bigg\}\prod_{i=1}^n  w_i \leq h_{j,N}\times \frac{1}{\sqrt{2\pi}^{j-1}} \frac{1}{N^{j-1/2}} \mbox{UB}_{j},
\end{align}
%}
where
\begin{align*}
   \mbox{LB}_{j,N} = 
   & \bigg[j^{-N}\bigg\{\bigg(\frac{1}{j}-\frac{1}{N^{1-\delta}}\bigg)\bigg(\frac{1}{j}+\frac{1}{N^{1-\delta}}\bigg)\bigg\}^{(-N/2) + (j/2N^{1-\delta})+(j/4)}\bigg]\notag\\
   &\bigg[\bigg\{\bigg(\frac{1}{j}-\frac{1}{N^{1-\delta}}\bigg)^{\min(j/2,n)}\bigg(\frac{1}{j}+\frac{1}{N^{1-\delta}}\bigg)^{n-\min(j/2,n)}\bigg\}\bigg],\notag\\
   \mbox{UB}_{j} =& [j^{- j/2}]\ [j^{-n}],
\end{align*}
and  $h_{j,N}$ is the number of elements in $\wt{W}_j$ that trivially satisfy $(2N^{\delta} +1)^{j-1}\leq h_{j,N} \leq (2N^{\delta} +1)^j$. The step above assumes that $j$ is even, but we can easily do similar calculation for the case where $j$ is odd to obtain expressions that are similar in spirit. The above inequality holds since we simply replace each term in the sum by the minimum and maximum among all the terms in the sum respectively, to obtain the lower and upper bound.

So, we combine $(i)-(ii)$ to obtain the ratio of marginal likelihoods as they appear in the equation \eqref{eqn:bounded_ratio_marginallikelihhod}:
%{\small
\begin{align}
   \mbox{LB}^{\star}_{j, n, N}
   &=\frac{h_{j,N}\ \sqrt{2\pi}^{-(j-n)}\ \mbox{LB}_{j,N}}{h_{n,N}\ (NM_{N})^{j-n}\mbox{UB}_{n} + M_{N}^{j-n} \sqrt{2\pi}^{n-1} N^{j-1/2} \mbox{UB}_{n, \rm neglect}}\notag\\
   &\leq\frac{m(x_{1:N}\mid k_N = j)}{m(x_{1:N}\mid k_N = n)}\notag\\
   %=\frac{h_{j,N}}{h_{n,N}} \frac{\sqrt{2\pi}^{-(j-n)}}{(NM_{N})^{j-n}}\bigg[(n/j)^n\ j^{-j/2}\bigg],
   &\leq \frac{h_{j,N}\ \sqrt{2\pi}^{-(j-n)}\ \mbox{UB}_{j}\ + (\sqrt{2\pi})^{n-1} N^{j-1/2} \mbox{UB}_{j, \rm neglect} }{h_{n,N}\ (NM_{N})^{j-n}\ \mbox{LB}_{N, n}}\
   = \mbox{UB}^{\star}_{j, n, N}.
\end{align}
%}
It is now enough to focus on $\mbox{UB}^{\star}_{j, n, N}$, and obtain two-sided bounds as follows:
\begin{align}\label{eqn:ration_combined}
  &\bigg[\frac{(2N^{\delta} +1)^{j-n-1}}{{N^{(d\alpha +1)}}^{(j-n)}}\sqrt{2\pi}^{-(j-n)}\bigg]\frac{\mbox{UB}_{j}}{\mbox{LB}_{N, n}} + \bigg(\frac{\sqrt{2\pi})^{n-1} N^{n-1/2}}{M_{N}^{j-n}\ h_{n,N}}\bigg)\frac{\mbox{UB}_{j, \rm neglect}}{\mbox{LB}_{N, n}}\notag\\
  &\leq \mbox{UB}^{\star}_{j, n, N}\notag\\
  &\leq\bigg[\frac{(2N^{\delta} +1)^{j-n}}{{N^{(d\alpha +1)}}^{(j-n)}}\sqrt{2\pi}^{-(j-n)}\bigg]\frac{\mbox{UB}_{j}}{\mbox{LB}_{N, n}} +   \bigg(\frac{\sqrt{2\pi})^{n-1} N^{n-1/2}}{M_{N}^{j-n}\ h_{n,N}}\bigg)\frac{\mbox{UB}_{j, \rm neglect}}{\mbox{LB}_{N, n}}
\end{align}
where the terms within the third braces are decreasing functions of $j(\geq n)$. For the sake of simplicity, we obtain a simpler upper bound to the upper-bound of $\mbox{UB}^{\star}_{j, n, N}$ in equation \eqref{eqn:ration_combined}. To that end, first we note that 
\begin{align}\label{eqn:ration_big}
  \frac{\mbox{UB}_{j}}{\mbox{LB}_{N, n}} 
  =T^{(1, \star)}_{N, n}\times T^{(2, \star)}_{N, n} \leq T^{(1)}_{N, n}\times T^{(2)}_{N, n} = T_{N, n}.
\end{align}
where
\begin{align}
  T^{(1, \star)}_{N, n}
  =&\bigg\{\frac{j^{-n}}{(\frac{1}{n}-\frac{1}{N^{1-\delta}})^{n/2}(\frac{1}{n}+\frac{1}{N^{1-\delta}})^{n/2}}\bigg\} 
  \leq  \bigg\{\frac{j^{-n}}{(\frac{1}{n}-\frac{1}{N^{1-\delta}})^{n}}\bigg\}\notag\\
  \leq & \bigg\{\frac{j^{-n}}{n^{-n}}\bigg(1-\frac{n}{N^{1-\delta}}\bigg)^{-n}\bigg\},\quad \text{since}\ j\geq n, \notag\\ 
  \leq&  \bigg\{\bigg(1-\frac{n}{N^{1-\delta}}\bigg)^{-n}\bigg\}  
  = T^{(1)}_{N, n},  
\end{align}
and 
\begin{align}
  T^{(2,\star)}_{N, n}
  =&  \bigg\{\frac{j^{-j/2}}{n^{-N}\{(\frac{1}{n}-\frac{1}{N^{1-\delta}})(\frac{1}{n}+\frac{1}{N^{1-\delta}})\}^{(-N/2) + (n/2N^{1-\delta})+(n/4)}}\bigg\}\notag\\
  = &  \bigg\{\frac{j^{-j/2}}{n^{-N}\{(\frac{1}{n^2}-\frac{1}{N^{2-2\delta}})\}^{(-N/2) + (n/2N^{1-\delta})+(n/4)}}\bigg\}\notag\\
  = &  \bigg\{\frac{j^{-j/2}}{\{(1-\frac{n^2}{N^{2-2\delta}})^{-N/2}\}\ \{(\frac{1}{n^2}-\frac{1}{N^{2-2\delta}})^{n/2N^{1-\delta}}\}\ \{n^{-n/2}(1-\frac{n^2}{N^{2-2\delta}})^{n/4} \}}\bigg\}\notag\\
  \leq &  \bigg\{\bigg(1-\frac{n^2}{N^{2-2\delta}}\bigg)^{N/2}\ \bigg(\frac{1}{n^2}-\frac{1}{N^{2-2\delta}}\bigg)^{-n/2N^{1-\delta}}\ \bigg(1-\frac{n^2}{N^{2-2\delta}}\bigg)^{-n/4}\bigg\},\quad \text{since}\ j\geq n, \notag\\
  \leq &  \bigg\{ \bigg(\frac{1}{n^2}-\frac{1}{N^{2-2\delta}}\bigg)^{-n/2N^{1-\delta}}\ \bigg(1-\frac{n^2}{N^{2-2\delta}}\bigg)^{-n/4}\bigg\}\quad\text{since}\ (1-\frac{n^2}{N^{2-2\delta}})\leq 1, \notag\\
  =& T^{(2)}_{N, n}.    
\end{align}
%The first three steps above are simple algebra, and the fourth step uses the fact that $j\geq n$. The fifth and sixth  steps are again simple algebra, and  the seventh step uses the fact that $j\geq n$. The eighth step uses $(1-\frac{n^2}{N^{2-2\delta}})\leq1$.
The final expression of $T_{N, n}$ in equation \eqref{eqn:ration_big} is devoid of $j(>n)$ and both $T^{(1)}_{N, n}, T^{(2)}_{N, n}$ are decreasing functions of $N$. So,  we are only left to tackle
%{\smaller
\begin{align}\label{eqn:ration_small}
    \bigg(\frac{\sqrt{2\pi})^{n-1} N^{n-1/2}}{M_{N}^{j-n}\ h_{n,N}}\bigg)\frac{\mbox{UB}_{j, \rm neglect}}{\mbox{LB}_{N, n}}
    &\leq \bigg(\frac{\sqrt{2\pi})^{n-1} N^{n-1/2}}{M_{N}^{j-n}\ h_{n,N}}\bigg)\frac{\mbox{UB}_{j, \rm neglect}\times T_{N, n}}{\mbox{UB}_{j}}\notag \\
    &= \frac{(\sqrt{2\pi})^{n-1}}{\ h_{n,N}}\frac{T_{N,n}}{(NM_{N})^{j-n}}\bigg(\frac{N^n}{e^{N^{\delta^{\star}}}}\bigg)\notag\\
    &\leq \frac{(\sqrt{\pi/2})^{n-1}}{\ N^{\delta(n-1)}}\frac{T_{N,n}}{(NM_{N})^{j-n}}\bigg(\frac{N^n}{e^{N^{\delta^{\star}}}}\bigg)\notag\\
    &\leq (\sqrt{\pi/2})^{n-1}\ T_{N,n}\ \bigg(\frac{N^{(1-\delta)n +\delta}}{e^{N^{\delta^{\star}}}}\bigg).
\end{align}
%}
The first step uses the equation \eqref{eqn:ration_big}. The second step is simple algebra. The third step uses $h_{n,N}\geq (2N^{\delta} +1)^{n-1}$. The last step uses $j\geq n$ ,  is devoid of $j$, and is a decreasing function in $N$.  Finally, the equations \eqref{eqn:ration_combined}-\eqref{eqn:ration_small} together complete the proof of the fact that $\sum_{N=N^{\star}}^{\infty} P(k_N > n  \mid x_{1:n}) < \infty$.

\subsection*{Part 2}
Next, we move to the proof of the fact that $\sum_{N=N^{\star}}^{\infty} P(k_N < n  \mid x_{1:n}) < \infty$. Note that, $\min_{h,h^{\prime}} |\mu_h-\mu_{h^{\prime}}|_{\infty}\geq 2\rho_N$, and $\rho_N\to 0$ as $N\to\infty$. If $k_N<n$, then the likelihood  at at least one sample point can be made 0 by choosing $N$ large enough.  For $j<n$, from equation \eqref{eqn:full_post} we have 
\begin{align}
    &m(x_{1:N}\mid k_N = j)\notag\\ 
    =&\sum_{w_{1:j}}\sum_{\mu_{1:j}}\prod_{h=1}^j H(\mu_h)\ \bigg\{\frac{N!/\prod_{h=1}^j b_h!}{j^N}\bigg\}\prod_{i=1}^n \bigg\{\sum_{h=1}^j w_h \tau^{d}_{N}\ \delta(|x_i - \eta_h|_{\infty} <2\rho_N)\bigg\}\notag\\
    = &\ 0,
\end{align}
where $\delta(\cdot)$ is the Dirac delta measure, and $|\cdot|_{\infty}$ denotes the $\mb{L}_{\infty}$ norm. The final step holds true since $n-j$ indicators are exactly 0 for  $N\geq N^{\star}$ where it is sufficient to ensure
that
\begin{align}
    \min_{s\neq t} |x_s -x_t|_{\infty} > 2\rho_{N^{\star}} = \frac{1}{(N^{\star})^{\alpha-\beta}}\iff N^{\star} > \big\{\min_{s\neq t} |x_s -x_t|_{\infty}\big\}^{-1/(\alpha-\beta)}.
\end{align}
Then
\begin{align}\label{eqn:part2}
    P(k_N < n  \mid x_{1:n})
    = \sum_{j=1}^{n-1} P(k_N =j  \mid x_{1:n})
    = \frac{ \sum_{j=1}^{n-1}\ p(j)\ m(x_{1:n}
    \mid k_N = j)}{\sum_{j=1}^{\infty}\ p(j)\ m(x_{1:n}
    \mid k_N = j)} = 0,
\end{align}
for all $N\geq N^{\star}$ and $\sum_{N=N^{\star}}^{\infty} P(k_N < n  \mid x_{1:n}) = 0$. This completes the proof of the theorem.
\end{proof}

\subsection{Proof of Theorem \ref{th:npbayes_equivalence}}

Before we move to our detailed proof the theorem, we shall first look at a brief sketch of our arguments. Our general strategy is to start with the joint distribution of 
%$(\xi^{\star},\ \theta,\ x_{1:n}) = (k,\ \mu_{1:k},\ \xi,\ \theta,\ x_{1:n})$ 
$(\xi^{\star},\ \theta,\ x_{1:n}) = (k, b_{1:k}, \mu_{1:k}, \theta, x_{1:n})$ 
and marginalise out  $\xi^{\star} = (k, b_1,\ldots, b_k,\ \mu_{1},\ldots,\mu_k)$ to compute the posterior of $\theta\mid x_{1:n}$. The proof mainly hinges on three key ideas:
\begin{itemize}
    \item In Theorem \ref{th2:lemma2}, we show that $P(k=n\mid x_{1:n}) \to 1$ \emph{almost surely} as $N\to\infty$. Consequently, we can focus on  
    %$\pi_{\varepsilon, N}(\theta,\ \mu_{1:k},\ \xi \mid x_{1:n}, \ k = n)$,
    $\pi_{\varepsilon, N}(\theta, b_1,\ldots, b_k, \mu_{1},\ldots,\mu_{k} \mid x_{1:n}, \ k = n)$,
    instead of $\pi_{\varepsilon, N}(\theta, b_1,\ldots, b_k, \mu_{1},\ldots,\mu_{k} \mid x_{1:n})$. 
    \item In the assumptions in Section \ref{ssec:npBayes}, we construct a $d$-dimensional uniform grid that expands to entire $\mb{R}^d$ and the length of the sides of the grid cells go to $0$ as $N\to\infty$. This construct makes sure that only one term in the sum over $(\mu_1,\ldots, \mu_k)$ survives for $N$ large enough.
    \item Our proof critically exploits Sterling's approximation of Multinomial probabilities of the form $1/k^N (N!/\prod_{h=1}^k b_h !)$. With the help of Lemma \ref{th2:lemma1}, for $N$ large enough, we can show that  when we consider the sum over $(b_1,\ldots,b_n)$ or equivalently $(w_1,\ldots, w_n)$, only the term(s) corresponding to the weights $(w_1,\ldots, w_n)$
    ``close" to the $\mbox{D}$-BETEL weights survive. We put this idea in rigorous form in equation $\eqref{eqn:continuity}$.
\end{itemize}

\begin{proof}
The proof proceeds in two steps. In step 1, we carry out algebraic simplification of the posterior without resorting to any asymptotic arguments, and in step 2, we invoke the special asymptotic regime to prove the final result.
\subsection*{Step 1 (simplification of the posterior)}
For notational homogeneity, we use $ \mbox{U}_d(x_i\mid\eta_h,\tau^{-1}I)$ to denote $\prod_{j=1}^d \mbox{Uniform}(\eta_{h, j} -  \tau^{-1},\ \eta_{h, j} +  \tau^{-1})$.
Under the hierarchical model in equation \eqref{eqn:npbayes3}--\eqref{eqn:npbayes2} in the main document, the joint posterior of $\theta,\xi^{\star}\mid x_{1:n}$ is  
\begin{align}\label{eqn:full_post}
\ \pi_{\varepsilon, N}(\theta,\xi^{\star}\mid x_{1:n})  
& \ \propto \ \pi(\theta) \ \pi_{\varepsilon, N}(\xi^{\star}\mid\theta)\
P^{(N)}(x_{1:n}\mid \xi^{\star},\theta) \notag\\ 
& = \pi(\theta) \ \Bigg\{\frac{1_{A_{\varepsilon, N}(\theta)}(\xi^{\star}) \ \pi_{\infty, N}(\xi^{\star})}{\int_{\xi^{\star}}1_{A_{\varepsilon, N}(\theta)}(\xi^{\star})\ \pi_{\infty,N}(\xi^{\star})d\xi^{\star}}\Bigg\}\ P^{(N)}(x_{1:n}\mid \xi^{\star},\theta) \notag\\
& = \pi(\theta)\ \Bigg\{\frac{1_{A_{\varepsilon, N}(\theta)}(\xi^{\star}) \ }{\int_{\xi^{\star}}1_{A_{\varepsilon, N}(\theta)}(\xi^{\star})\ \pi_{\infty, N}(\xi^{\star})d\xi^{\star}}\Bigg\}\ \bigg\{p(k)\ \prod_{h=1}^k \mbox{H}^{(N)}(\mu_h) \bigg\} \notag\\
%&\quad\quad\quad \bigg\{\prod_{j=1}^N \bigg[\sum_{h=1}^k k^{-1}\ \delta_{\mu_{h}}(\xi_j)  \bigg]\bigg\}\
%\bigg\{\frac{1}{N^n}\prod_{i=1}^n \bigg[\sum_{j=1}^N \mbox{U}_d(x_i\mid\xi_j,\tau^{-1}I) \bigg] \bigg\},\\
%\end{align}
%Observe that the prior on $(\xi_1, \ldots, \xi_N)$ induces a distribution on the prior for $(b_1,\ldots, b_k)$ where $b_h = \sum_{j=1}^N \ind(\xi_j = \mu_h)$   given by $\mbox{Multinomial}(N;1/k, \ldots, 1/k)$ with probability mass function $N!/(k^N \prod_{h=1}^k b_h!)$. Then, we have
%\begin{align}\label{eqn:full_post}
%\pi_{\varepsilon, N}(\theta,\xi^{\star}\mid x_{1:n}) 
&\ \propto \
 \pi(\theta)\ \Bigg\{\frac{1_{A_{\varepsilon, N}(\theta)}(\xi^{\star}) \ }{\int_{\xi^{\star}}1_{A_{\varepsilon, N}(\theta)}(\xi^{\star})\ \pi_{\infty,N}(\xi^{\star})d\xi^{\star}}\Bigg\}\ \bigg\{p(k)\prod_{h=1}^k \mbox{H}^{(N)}(\mu_h) \bigg\} \notag\\
&\quad\quad\quad\quad \bigg\{\frac{1}{k^N}\ \frac{N!}{\prod_{h=1}^k b_h !} \bigg\}\ 
\bigg\{\prod_{i=1}^n \bigg[\sum_{h=1}^k \frac{b_h}{N} \mbox{U}_d(x_i\mid\eta_{h},\tau^{-1}I) \bigg] \bigg\}. 
\end{align}


%From Lemma \ref{th2:lemma1}) $k=n$ and hence 
%\begin{align} 
%\pi_{\varepsilon, N}(\theta,\xi^{\star}\mid x_{1:n})
%\ \propto \
%& \pi(\theta)\ \frac{ 1_{A_{\varepsilon, N}(\theta)}(\xi^{\star}) \ }{\int_{\xi^{\star}}1_{A_{\varepsilon, N}(\theta)}(\xi^{\star})\ \pi_{\infty,N}(\xi^{\star})d\xi^{\star}}\ \ \bigg\{\prod_{h=1}^n \mbox{H}(\mu_h)\bigg\}\notag\\ 
%&\quad \bigg\{\frac{1}{n^N}\ \frac{N!}{\prod_{h=1}^n b_h !} \bigg\}\ 
%\bigg\{\prod_{i=1}^n \bigg[\sum_{h=1}^n \frac{b_h}{N} \mbox{N}_d(x_i\mid\mu_{h},\tau^{-1}I) \bigg] \bigg\}. 
%\end{align}

Next, use of the first part of Lemma \ref{th2:lemma1_b}, together with  a reparametrization of the posterior by writing $b_h/N=w_h,\ h=1,\ldots,n$ yield,
\begin{align} 
\pi_{\varepsilon, N}(\theta,\xi^{\star}\mid x_{1:n}) \ \propto \
& \pi(\theta)\  \frac{ 1_{A_{\varepsilon, N}(\theta)}(\xi^{\star})\ }{\int_{\xi^{\star}}1_{A_{\varepsilon, N}(\theta)}(\xi^{\star})\ \pi_{\infty,N}(\xi^{\star})d\xi^{\star}}\ \bigg\{p(k)\ \prod_{h=1}^k \mbox{H}^{(N)}(\mu_h) \bigg\}\notag\\
&\quad\quad \bigg\{\frac{U_{k, N, \mathbf{b}}}{V_{k, N}}\prod_{h=1}^k w_{h}^{-w_h N + \frac{1}{2}} \bigg\}\ 
\bigg\{\prod_{i=1}^n \bigg[\sum_{h=1}^k w_h \mbox{U}_d(x_i\mid\eta_{h},\tau^{-1}I) \bigg] \bigg\} 
\end{align}
where $V_{k, N} = k^N N^{k-\frac{1}{2}}\ (\sqrt{2\pi})^{k-1}$ and $e^{\frac{1}{12N+1}-\sum_{h=1}^k\frac{1}{12b_h }}<U_{k, N, \mathbf{b}}<e^{\frac{1}{12N}-\sum_{h=1}^k\frac{1}{12b_h +1}}$. 
%where $C_N = \frac{1}{N^n} \frac{1}{(\sqrt{2\pi N})^{(n-1)}}$. 
Before proceeding to step 2, we introduce some notations that will aid the calculations going forward. Given, $X\mid\xi^{\star}  \sim P^{(N)} :=\sum_{h=1}^n w_h \mbox{U}_d(x\mid\eta_{h},\tau^{-1} I)$, and  
%as for $N\to\infty$, $\tau\to\infty$, we have $\mbox{U}_d(x_h\mid \mu_h,\tau^{-1}I)\to 1_{\mu_h}(x_h), h=1,\ldots,n$. 
for a fixed $\theta\in\Theta$,  $F_{\theta}(\cdot) = \sum_{k=1}^{K_{0}} s_{0k}\ \mbox{ED}_{h}(\cdot\mid m_{0k}, \Sigma_{0k})$,  we can define $C_{\theta,\varepsilon}$ as in equation \eqref{eqn_C}.
%\begin{align*}
% C_{\theta,\varepsilon} =
%\bigg \{w\in\mb{W}_n:\ W_{\rm AR}(P^{(N)}\ , \ F_{\theta})   \leq \varepsilon \bigg\}\ \text{where}\ \mb{W}_n = \bigg\{w: w_{h} = \frac{b_{h}}{N},  \sum_{h=1}^n b_h = N, b_h \in \mb{Z}^{+}\bigg\}.
%\end{align*}
Consequently, we can write
%{\smaller
\begin{align*}
    \int_{\xi^{\star}}1_{A_{\varepsilon, N}(\theta)}(\xi^{\star})\ \pi_{\infty,N}(\xi^{\star})d\xi^{\star} &= \sum_{k} p(k)\sum_{w \in C_{\theta,\varepsilon}}\bigg[\sum_{\mu_1,\ldots,\mu_k} \prod_{h=1}^k\mbox{H}^{(N)}(\mu_h)\  \bigg\{\frac{U_{k, N, \mathbf{b}}}{V_{k, N}}\prod_{h=1}^k w_h^{-w_h N + \frac{1}{2}} \bigg\}\  \bigg].
\end{align*}
%}

Next, in view of Theorem \ref{th2:lemma2}, we work with a partition parameter space 
\begin{align*}
 \{k=n\}\cup \{k\neq n\}, 
\end{align*}
and focus on $\pi_{\varepsilon, N}(\theta,\ b_1,\ldots, b_k, \mu_{1},\ldots,\mu_{k} \mid x_{1:n}, \ k = n) 
\propto$
%{\smaller
\begin{align} 
& \frac{ \pi(\theta) 1\{ C_{\theta,\varepsilon} \}}{\int_{\xi^{\star}}1_{A_{\varepsilon, N}(\theta)}(\xi^{\star})\ \pi_{\infty, N}(\xi^{\star})d\xi^{\star}}\ \bigg\{ \prod_{h=1}^n \mbox{H}^{(N)}(\mu_h)\bigg\}\ \bigg\{\prod_{h=1}^n w_h^{-w_h N + \frac{1}{2}} \bigg\}\ \bigg\{\prod_{h=1}^n  w_h   \bigg\} .
\end{align}
%}
Marginalization of the above with respect to $w_{1:n}$ and $\mu_{1:n}$ leads to 
\begin{align} 
\pi_{\varepsilon,N}(\theta, \mid x_{1:n},\ k = n) \ \propto \ 
& \pi(\theta)\ \frac{N_x^{\star}(\theta)}
{D_x^{\star}(\theta) }
\end{align}
where
\begin{align*} 
&N_x^{\star}(\theta) = \sum_{w \in C_{\theta,\varepsilon}}\bigg[\bigg\{\prod_{h=1}^n \mbox{H}^{(N)}(x_h)\bigg\} \ \bigg\{\prod_{h=1}^n w_{h}^{-w_h N + \frac{1}{2}} \bigg\}\ \bigg\{\prod_{h=1}^n  w_h  \bigg\} \bigg],\\
& D_x^{\star}(\theta) = \sum_{w \in C_{\theta,\varepsilon}}\bigg[\bigg\{\prod_{h=1}^n \mbox{H}^{(N)}(x_h)\bigg\} \  \bigg\{\prod_{h=1}^n w_{h}^{-w_h N + \frac{1}{2}} \bigg\}\bigg]. 
\end{align*}
Only one term in the sum over $\mu_{1},\ldots, \mu_k$ survives since $\mbox{H}^{(N)}(\mu_h)$ is uniform over the mid-points of the grid $\mathcal{X}^N$ with grid length $2\rho_N$ and the components have range $2\rho_{N}$. Next, by the uniformity assumption on $\mbox{H}^{(N)}$, it reduces to
\begin{align}\label{eqn:reduced_post} 
&\pi_{\varepsilon,N}(\theta\mid x_{1:n}, \ k = n) \ \propto \ \pi(\theta)\ \frac{
\sum_{w \in C_{\theta,\varepsilon}} \bigg\{\prod_{h=1}^n w_{h}^{-w_h N + \frac{1}{2}} \bigg\}\ \bigg\{\prod_{h=1}^n  w_h  \bigg\}}
{\sum_{w \in C_{\theta,\varepsilon}}\ \bigg\{\prod_{h=1}^n w_{h}^{-w_h N + \frac{1}{2}} \bigg\} }.
\end{align}


\subsection*{Step 2 (invoking the asymptotic regime)}
We recall that, in the $\mbox{D}$-BETEL formulation, for every $\theta\in\Theta$: 
\begin{align*}
  w^{\star}_{1:n}(\theta,\varepsilon) = \argmax_{w\in C_{\theta,\varepsilon}} H_{N}(w),
\end{align*}
where $H_{N}(w) = -\sum_{h=1}^n w_h\log w_h$, and $\varepsilon>0$. For notational simplicity we shall use $w^{\star}_h$ in place of  $w^{\star}_h(\theta,\varepsilon)$. Note that, $w^{\star}_{1:n}$ is either the global maximizer $(1/n,\ldots,1/n)^{\T}$ of $\prod_{h=1}^n w_{h}^{-w_h}$, or lies  at the boundary of feasible set $C_{\theta,\varepsilon}$ defined in equation \eqref{eqn_C}. In view of that, we shall analyse equation \eqref{eqn:reduced_post} in two cases. In case 1, we can carry out the rest of the analysis as in Theorem \ref{th2:lemma2}. But this case is not of much statistical worth. 
%In places, it is useful to resort to the dual form of the maximization problem above:
%\begin{align*}
%    w^{\star}_{1:n}(\theta,\varepsilon) = \argmax_{w\in \mb{W}} g(w)\quad \text{where}\quad g(w) = H(w) - \lambda_{\star}\ W_{\rm AR}^2(F_{\theta}\ , \ \nu(w, x)),
%\end{align*}
%such that there exists a $\lambda_{\star}>0$ for every choice of $\varepsilon>0$.
In the case 2, our goal is to  study the behaviour of the log ratio $R_n$ defined in equation \eqref{eqn_R_j}. 
%\begin{align}
%R = &\ \log\ \Bigg\{\frac{\prod_{h=1}^n w^{\star}_{h}^{-w_{h}^{\star} N + \frac{1}{2}}}{\prod_{h=1}^n w_{h}^{-w_{h} N + \frac{1}{2}}}\Bigg\}
%= N\big[ H(w^{\star}) - H(w)\big] + \frac{1}{2} \sum_{h=1}^n \log\bigg(\frac{w^{\star}_{h}}{w_h}\bigg).
%\end{align}
In particular, we study the behaviour of $R_n$ in a carefully constructed neighbourhood  $\wt{C}_{\theta, \varepsilon}$ (refer to the equation \eqref{eqn_C_Tilde}) of $w^{\star}$ and outside it i.e in $C_{\theta, \varepsilon}\subset\wt{C}_{\theta, \varepsilon}$.
%, we introduce the set 
%\textcolor{blue}{(some notational redundancy going on below. the way you define $\wt{\mb{W}}$, it is already a subset of $C_{\theta, \varepsilon}$, so you can make that explicit by saying $w \in C_{\theta, \varepsilon}$ in the defintion, and getting rid of $\le \varepsilon$. then, no need to write $\big(\mb{W}\setminus\wt{\mb{W}}\big)\cap  C_{\theta,\varepsilon}$ later on, simply $C_{\theta, \varepsilon} \setminus \wt{\mb{W}}$ suffices. this will also allow you to get rid of the indicators in the sums earlier.)}
%{\smaller
%\begin{align}
%\wt{C}_{\theta, \varepsilon}=
%&\bigg\{w\in C_{\theta, \varepsilon}: W^{2}_{\rm AR}[F_{\theta}\ ,\ \nu(w^{\star}, x) ] - \frac{1}{N^{1-\delta}}\leq W^{2}_{\rm AR}[F_{\theta}\ ,\ \nu(w, x) ],\ 0\leq H(w^{\star}) - H(w)\leq \frac{1}{N^{1-\delta}} \bigg\}
%\end{align}
%}
%for some $0<\delta<1$. 
For any $w\in C_{\theta,\varepsilon}\setminus\wt{C}_{\theta,\varepsilon}$, we have  $R_n >c N^{\delta} - n\log N$ where $c>0$, from the Lemma \ref{th3:lemma1}. Also, note that the set $C_{\theta,\varepsilon}\setminus\wt{C}_{\theta,\varepsilon}$ can contain at max $N^n$ elements.  
 
 Now, we are in a position to return to the posterior in equation \eqref{eqn:reduced_post} for further simplification:
 \begin{align*}
  \pi_{\varepsilon,N}(\theta\mid x_{1:n},\ k = n) \ \propto \pi(\theta)\ \frac{\mbox{N}(\wt{C}_{\theta,\varepsilon}) + \mbox{N}(C_{\theta,\varepsilon}\setminus\wt{C}_{\theta,\varepsilon})}{\mbox{D}(\wt{C}_{\theta,\varepsilon}) + \mbox{D}(C_{\theta,\varepsilon}\setminus\wt{C}_{\theta,\varepsilon})}   
 \end{align*}
 where 
 \begin{align}
  &\mbox{N}(S)  = \sum_{w\in S }  \bigg\{\prod_{h=1}^n w_{h}^{-w_h N + \frac{1}{2}} \bigg\}\ \bigg\{\prod_{h=1}^n  w_h  \bigg\},\ \mbox{D}(S)  = \sum_{w\in S }  \bigg\{\prod_{h=1}^n w_{h}^{-w_h N + \frac{1}{2}} \bigg\},
 \end{align}
with $S=\wt{C}_{\theta,\varepsilon}$ or $C_{\theta,\varepsilon}\setminus\wt{C}_{\theta,\varepsilon}$.
In the remaining of the proof, we shall rigorously demonstrate that both the numerator and the denominator in the above expression are dominated by the terms corresponding to $w\in C_{\theta,\varepsilon}$, and the terms corresponding to $w\in C_{\theta,\varepsilon}\setminus\wt{C}_{\theta,\varepsilon}$ are negligible. For brevity of presentation, we introduce:
\begin{align}
  \mbox{Diff}(S) = \sum_{w\in S}  \bigg\{\prod_{h=1}^n w_{h}^{-w_h N + \frac{1}{2}} \bigg\} \bigg\{\prod_{h=1}^n  w_h - \prod_{h=1}^n  w^{\star}_h  \bigg\}
\end{align}
with $S=\wt{C}_{\theta,\varepsilon}$ or $C_{\theta,\varepsilon}\setminus\wt{C}_{\theta,\varepsilon}$. Next, we focus on the final piece of the proof:
\begin{align}\label{eqn:continuity}
&\Bigg|\frac{\mbox{N}(\wt{C}_{\theta,\varepsilon}) + \mbox{N}(C_{\theta,\varepsilon}\setminus\wt{C}_{\theta,\varepsilon})}{\mbox{D}(\wt{C}_{\theta,\varepsilon}) + \mbox{D}(C_{\theta,\varepsilon}\setminus\wt{C}_{\theta,\varepsilon})}  - \prod_{h=1}^n  w^{\star}_h\Bigg|\ 
=\Bigg|\frac{\mbox{Diff}(\wt{C}_{\theta,\varepsilon}) + \mbox{Diff}(C_{\theta,\varepsilon}\setminus\wt{C}_{\theta,\varepsilon})}
{\mbox{D}(\wt{C}_{\theta,\varepsilon}) + \mbox{D}(C_{\theta,\varepsilon}\setminus\wt{C}_{\theta,\varepsilon}) }\Bigg|\ \notag\\ 
\leq& \Bigg|\frac{\mbox{Diff}(\wt{C}_{\theta,\varepsilon})}{\mbox{D}(\wt{C}_{\theta,\varepsilon}) + \mbox{D}(C_{\theta,\varepsilon}\setminus\wt{C}_{\theta,\varepsilon}) }\Bigg|\ + \Bigg|\frac{\mbox{Diff}(C_{\theta,\varepsilon}\setminus\wt{C}_{\theta,\varepsilon})}{\mbox{D}(\wt{C}_{\theta,\varepsilon}) + \mbox{D}(C_{\theta,\varepsilon}\setminus\wt{C}_{\theta,\varepsilon}) }\Bigg|,\ \quad \text{(triangle inequality)},\notag\\ 
\leq& \Bigg|\frac{\mbox{Diff}(\wt{C}_{\theta,\varepsilon})}{\mbox{D}(\wt{C}_{\theta,\varepsilon}) }\Bigg|\ + \Bigg|\frac{\mbox{Diff}(C_{\theta,\varepsilon}\setminus\wt{C}_{\theta,\varepsilon})}{\mbox{D}(\wt{C}_{\theta,\varepsilon}) }\Bigg|
\leq \Bigg|\frac{\mbox{Diff}(\wt{C}_{\theta,\varepsilon})}{\mbox{D}(\wt{C}_{\theta,\varepsilon}) }\Bigg|\ + \Bigg|\frac{\mbox{Diff}(C_{\theta,\varepsilon}\setminus\wt{C}_{\theta,\varepsilon})}{\prod_{h=1}^n {w_{h}^{\star}}^{-w_{h}^{\star} N + \frac{1}{2}} }\Bigg|.
\end{align}
To argue that the first term $\to 0$ as $N\to\infty$,  it is enough to demonstrate that $\sup_{w\in C_{\theta, \epsilon}}\bigg|\prod_{h=1}^n  w_h - \prod_{h=1}^n  w^{\star}_h  \bigg|\to 0$ as $N\to\infty$ (Lemma \ref{th2_lemma2_asymtotic}). Next we argue that the second term $\to 0$ as $N\to\infty$. The facts that $\bigg|\prod_{h=1}^n  w_h - \prod_{h=1}^n  w^{\star}_h  \bigg|\leq 1/n^n$ trivially, and for any $w\in C_{\theta,\varepsilon}\setminus\wt{C}_{\theta,\varepsilon}$, $R_n >c N^{\delta} - n\log N, c>0$ yield
\begin{align*}
    \Bigg|\frac{\mbox{Diff}(C_{\theta,\varepsilon}\setminus\wt{C}_{\theta,\varepsilon})}{\prod_{h=1}^n {w_{h}^{\star}}^{-w_{h}^{\star} N + \frac{1}{2}} }\Bigg|\leq \frac{1}{n^n}\frac{N^{2n}}{e^{cN^\delta}}
\end{align*}
which is a decreasing sequence in $N$ that $\downarrow 0$ as $N\to \infty$. 


Finally, on application of part two of Lemma \ref{th2:lemma1_b}, Theorem \ref{th2:lemma2}, observation in equation \eqref{eqn:continuity} we have 
%\textcolor{blue}{(this step needs work)}
%Hence,  we have the result.
%Now we claim as $N\to\infty$ (refer to lemma \ref{th2:lemma2}) that the ratio above approximates to $\prod_{h=1}^n \pi_{i}(C_{\theta,\varepsilon})$ where $\pi(C_{\theta,\varepsilon}) = \argmax_{w\in C_{\theta,\varepsilon}} \prod_{h=1}^n w_{h}^{-w_h}$. 
%So we have,
\begin{align*} 
&\pi_{\varepsilon,N}(\theta\mid x_{1:n}) 
\ \propto\ w(\theta)\ \prod_{i=1}^n w_{i}^{\star}(C_{\theta,\varepsilon}).
\end{align*}
The right hand side is precisely the  $\mbox{D}$-BETEL posterior.
\end{proof}

\section{Proof of Theorem \ref{th3} in the main document}\label{ssec:th3}

Here we present the the proof of the Theorem \ref{th3}  in Section \ref{ssec:andrew}\ in the main document and a cascade of auxiliary results.

\begin{proof}
Exploiting the independence between the two blocks of both $X_0^{\star}$ and $X_1^{\star}$ , we have
\begin{align}\label{derive_WAR} 
W_{\rm AR}^2(p_0, p_1)
=& \inf_{\gamma\in  R^{\alpha}(p^{\star}_0,p^{\star}_1)} \mb{E}_{\gamma}\big|\big|X_0^{\star} - X_1^{\star}
\big|\big|^2\notag\\
=& \inf_{\gamma_1\in \pi(p_0,p_1)\cap \EMM^{\alpha}_{2d}(K_0 K_1)} \mb{E}_{\gamma_1}\big|\big|X_0 - X_1\big|\big|^2 +  \inf_{\gamma_2\in \pi(\widetilde{p}_0,\widetilde{p}_1)} \mb{E}_{\gamma_2}\big|\big|\widetilde{X}_0 - \widetilde{X}_1\big|\big|^2\notag\\
=&\inf_{\gamma_1\in \pi(p_0,p_1)\cap \EMM^{\alpha}_{2d}(K_0 K_1)} \mb{E}_{\gamma_1}\big|\big|X_0 - X_1\big|\big|^2 + \sum_{j=1}^d \inf_{\gamma_{2j}\in \pi(p_{0j},p_{1j})} \mb{E}_{\gamma_{2j}}\big(X_{0j} - X_{1j}\big)^2\notag\\
=&\ \inf_{\gamma_1\in \pi(p_0,p_1)\cap \EMM^{\alpha}_{2d}(K_0 K_1)} \mb{E}_{\gamma_1}\big|\big|X_0 - X_1\big|\big|^2 + \sum_{j=1}^d W_2^2(p_{0j},p_{1j})\notag\\
=& \ \inf_{\gamma_1\in \pi(p_0,p_1)\cap \EMM^{\alpha}_{2d}(K_0 K_1)} \mb{E}_{\gamma_1}\big|\big|X_0 - X_1\big|\big|^2 + \sum_{k=1}^d \int_{0}^1 (F_{0k}^{-1}(z) - F_{1k}^{-1}(z))^2 dz 
\end{align}
where $F_{jk}^{-1}$ is quantile function of the univariate random variable $X_{jk},\ j= 0,1;\ k = 1,2,\ldots,d$. Now if we assume
$X_0\sim p_{0}\equiv \sum_{k=1}^{K_0} s_{0k} \mbox{ED}(m_{0k},\Sigma_{0k}),\
X_1\sim p_{1}\equiv \sum_{k=1}^{K_1} s_{1k} \mbox{ED}(m_{1k},\Sigma_{1k})
$,
by Lemma \ref{th1:lemma4}, the expression above reduces to 
\begin{align} 
W_{\rm AR}^2(p_0, p_1) =& \inf_{pi\in\pi^{\alpha}(s_0,s_1)}\bigg[\sum_{k,l} \pi_{kl}\ W_{2}^{2}(\mbox{ED}(m_{0k},\Sigma_{0k}),\mbox{ED}(m_{1l},\Sigma_{1l}))\bigg]\notag\\
&+ \sum_{k=1}^d \int_{0}^1 \big(F_{0k}^{-1}(z) - F_{1k}^{-1}(z)\big)^2 dz
\end{align}
Further if we assume $ p_1 \equiv {\sum_{k=1}^{K_1} s_{1k} \delta_{m_{1k}}} \ $, by Lemma \ref{th1:lemma5}, the expression above becomes 
\begin{align} 
W_{\rm AR}^2(p_0,  p_1) =& \inf_{\pi\in\pi^{\alpha}(s_0,s_1)}\bigg[\sum_{k,l} \pi_{kl}\ ||m_{0k}-m_{1l}||^2\bigg]+ \nu_{h}\sum_{k=1}^{K_0} s_{0k} \mbox{tr}(\Sigma_{0k})\notag\\
&+ \sum_{k=1}^d \int_{0}^1 \big(F_{0k}^{-1}(z) - F_{1k}^{-1}(z)\big)^2 dz
\end{align}
which on application of Lemma \ref{th1:lemma6}, completes the proof. %\textcolor{red}{[label the Lemmas, and call them. don't name them as Lemma 1, Lemma 2 in the label (see document I sent you about typesetting)]}
\end{proof}

\section{Auxiliary results for the proof of Theorems \ref{th2:lemma2} and \ref{th:npbayes_equivalence}}\label{aux:th1th2}
\begin{definition}[Total variation metric, \citet{LevinPeresWilmer2006}]
Consider a measurable space $(\Omega ,{\mathcal {F}})$and probability measures $p$ and $q$ defined on $(\Omega, \mathcal{F})$. The total variation distance between $p$ and $q$ is defined as
$|p-q|_{\rm TV}=\sup _{A\in {\mathcal {F}}}\left|P(A)-Q(A)\right|$.
\end{definition}
When $\Omega$  is countable, the total variation distance is related to the $\mb{L}_1$ norm by the identity 
\begin{align*}
   |p-q|_{\rm TV} = \frac{1}{2}||p-q||_1 = \frac{1}{2}\sum_{\omega\in \Omega}|p(\omega) - q(\omega)|.
\end{align*}

\begin{lemma}[Pinsker's inequality, \citet{alma991023405949705251}]\label{Pinsker}
For any two probability distributions $p,q$ on $(\Omega ,{\mathcal {F}})$,
\begin{align*}
|p-q|_{\rm TV} \leq \sqrt{\frac{1}{2}\ \rm{KL}(p\ ||\ q)}.    
\end{align*}
\end{lemma}

\begin{lemma}\label{th2:lemma1_b}
Under the hierarchical specification in \eqref{eqn:npbayes1}--\eqref{eqn:npbayes3}, the prior on 
%$(\xi_1, \ldots, \xi_N)$ induces a distribution on the prior for
$(b_1,\ldots, b_k)$
%where $b_h = \sum_{j=1}^N \ind(\xi_j = \mu_h)$   
is given by $\rm{Multinomial}(N;1/k, \ldots, 1/k)$ with probability mass function $N!/(k^N \prod_{h=1}^k b_h!)$. Then, if $b_h > 0$ for all $h=1,\ldots,k$,
%{\smaller
\begin{align*}
   \frac{ e^{\frac{1}{12N+1}-\sum_{h=1}^k\frac{1}{12b_h }}}{V_{k,N}}\bigg\{\prod_{h=1}^k  w_{h}^{-w_h N+\frac{1}{2}}\bigg\} \leq \frac{N!}{k^N \prod_{h=1}^k b_h!} \leq \frac{ e^{\frac{1}{12N}-\sum_{h=1}^k\frac{1}{12b_h + 1}}}{V_{k,N}}\bigg\{\prod_{h=1}^k  w_{h}^{-w_h N+\frac{1}{2}}\bigg\}.
\end{align*}
%}
where $w_h = b_h/N,\ h=1,\ldots, k$ and $V_{k,N} = k^N N^{k-\frac{1}{2}}\ (\sqrt{2\pi})^{k-1}$. Also, as $b_h\to \infty,\ h= 1,\ldots, k$,
\begin{align*}
    e^{\frac{1}{12N+1}-\sum_{h=1}^k\frac{1}{12b_h }} \to 1, \quad \text{and}\quad e^{\frac{1}{12N}-\sum_{h=1}^k\frac{1}{12b_h + 1}}\to 1.
\end{align*}
\begin{proof}
Let us recall the Sterling's approximation for factorials, \citep{10.2307/2315957}:
for all $a\geq 1$,
\begin{equation}\label{th2:lemma1}
\sqrt{2\pi} \ a^{a+\frac{1}{2}}\ e^{-a + \frac{1}{12a +1}}  <   a! < \sqrt{2\pi} \ a^{a+\frac{1}{2}}\ e^{-a + \frac{1}{12a}}.
\end{equation}
%Consequently, if $a$ is large enough,
%\begin{align*}
%  a! = \sqrt{2\pi} \ a^{a+\frac{1}{2}}\ e^{-a}. 
%\end{align*}
On repeated use of equation \eqref{th2:lemma1},
\begin{align*}
\frac{\sqrt{2\pi} \ N^{N+\frac{1}{2}}\ e^{-N + \frac{1}{12N+1}}}{k^N \prod_{h=1}^k \sqrt{2\pi} \ b_{h}^{b_h+\frac{1}{2}}\ e^{-b_h + \frac{1}{12b_h}}} \leq \frac{N!}{k^N \prod_{h=1}^k b_h!} \leq \frac{\sqrt{2\pi} \ N^{N+\frac{1}{2}}\ e^{-N + \frac{1}{12N}}}{k^N  \ \prod_{h=1}^k \sqrt{2\pi} \ b_{h}^{b_h+\frac{1}{2}}\ e^{-b_h + \frac{1}{12b_h +1}}}.
\end{align*}
Since $\sum_{h=1}^k b_h = N$, the expression above simplifies to:

\begin{align*}
    \frac{N^{N+\frac{1}{2}}\ e^{ \frac{1}{12N+1}}}{k^N (2\pi)^{k-1} \prod_{h=1}^k   b_{h}^{b_h+\frac{1}{2}}\ e^{\frac{1}{12b_h}}} \leq \frac{N!}{k^N \prod_{h=1}^k b_h!} \leq \frac{N^{N+\frac{1}{2}}\ e^{\frac{1}{12N}}}{k^N (\sqrt{2\pi})^{k-1} \ \prod_{h=1}^k  b_{h}^{b_h+\frac{1}{2}}\ e^{\frac{1}{12b_h +1}}}.
\end{align*}
%{\smaller
Under the re-parametrization $w_h = b_h/N, \ h = 1,\ldots, k$:
\begin{align*}
    &\frac{N^{\frac{1}{2}-k}\ e^{ \frac{1}{12N+1}}}{k^N (2\pi)^{k-1} \prod_{h=1}^k   w_{h}^{w_h N+\frac{1}{2}}\ e^{\frac{1}{12b_h}}} \leq \frac{N!}{k^N \prod_{h=1}^k b_h!} \leq \frac{N^{\frac{1}{2}-k}\ e^{\frac{1}{12N}}}{k^N (\sqrt{2\pi})^{k-1} \ \prod_{h=1}^k  w_{h}^{w_h N+\frac{1}{2}}\ e^{\frac{1}{12b_h +1}}}\\
\iff&\frac{ e^{\frac{1}{12N+1}-\sum_{h=1}^k\frac{1}{12b_h }}}{k^N N^{k-\frac{1}{2}}\ (\sqrt{2\pi})^{k-1}}\bigg\{\prod_{h=1}^k  w_{h}^{-w_h N+\frac{1}{2}}\bigg\} \leq \frac{N!}{k^N \prod_{h=1}^k b_h!} \leq \frac{ e^{\frac{1}{12N}-\sum_{h=1}^k\frac{1}{12b_h + 1}}}{k^N N^{k-\frac{1}{2}}\ (\sqrt{2\pi})^{k-1}}\bigg\{\prod_{h=1}^k  w_{h}^{-w_h N+\frac{1}{2}}\bigg\}.
\end{align*}
%}
Note that, the calculations above do not utilise our  assumptions in Section \ref{ssec:npBayes}.
\end{proof}
\end{lemma}

%\clearpage
%\subsection*{Auxiliary results for the proof of Theorem 3}
\begin{lemma}\label{th3:lemma1}
For every $w\in C_{\theta,\varepsilon}\setminus\wt{C}_{\theta, \varepsilon}$, $R_n > cN^{\delta} - n\log N$ where $c>0$. %\textcolor{blue}{(`We can show that' is redundant.)}
\begin{proof}
%\textcolor{blue}{(Instead of `For some', you should say `Fix'. Also, you mean the set intersected with $C$, right?)}
Fix $w\in C_{\theta,\varepsilon}\setminus\wt{C}_{\theta, \varepsilon}$, by definition of $\wt{C}_{\theta, \varepsilon}$ (refer to equation \eqref{eqn_C_Tilde}) either of the two following displayed equations hold true:
%\textcolor{blue}{(get rid of (a) , (b) etc. Say either of the following two displayed equations is correct. Number them, and say if eq. first is correct, this happens. if eq. second is correct, this happens.)}
\begin{align*}
 &H_{N}(w^{\star}) - H_{N}(w)> 1/N^{1-\delta},\\  
 &  W^{2}_{\rm AR}[F_{\theta} , \nu(w^{\star}, x) ] - 1/N^{1-\delta}> W^{2}_{\rm AR}[F_{\theta} , \nu(w, x) ].
\end{align*}
If the first equation holds true, then 
\begin{align*}
    R_n =  N\big[ H_{N}(w^{\star}) - H_{N}(w)\big] + \frac{1}{2} \sum_{h=1}^n \log\bigg(\frac{w^{\star}_{h}}{w_h}\bigg) >  N^{\delta} - n\log N,
\end{align*}
since $1/N\leq w_{h}^{\star}, w_{h}\leq (N-1)/N$ trivially yields $(1/2) \sum_{h=1}^n \log(w^{\star}_{h}/w_h)\geq - (n/2) \log (N-1)$.
%\textcolor{blue}{(provide more details here regarding what you did with the log ratio term)}, or we have (b) 
Next, suppose the second equation holds true. By definition of $w^{\star}$ in equation \eqref{eqn_DBETEL_dual}, we have 
%\textcolor{blue}{(use of $\implies$ is not recommended.)}
{\footnotesize
\begin{align*}
   &g(w^{\star}) - g(w) > 0 , \notag\\
   =&  H_{N}(w^{\star}) - H_{N}(w) >  \lambda_{\star} \bigg(W_{\rm AR}^2(F_{\theta},  \nu(w^{\star}, x))- W_{\rm AR}^2(F_{\theta} ,  \nu(w, x))\bigg)  >\frac{\lambda_{\star}}{N^{1-\delta}}.
\end{align*}
} 
That yields $R_n > \lambda_{\star} N^{\delta} - n\log N$. Hence, we have the proof.
\end{proof}
\end{lemma}

\begin{lemma}\label{W2_lowerbound}
Fix $\eta>0$. If $W_{\rm AR}^2[\nu(w^{(1)}, x), \nu(w^{(2)}, x)]\leq\eta$, there exists a $\eta^{\prime}$ depended on $\eta$ such that $|w^{(1)}_i - w^{(2)}_i|\leq\eta^{\prime},\ i = 1,\ldots,n$.
\begin{proof}
By definition \ref{derive_WAR}, we have 
\begin{align}
 W_{\rm AR}^2[\nu(w^{(1)}, x), \nu(w^{(2)}, x)]\geq \sum_{j=1}^d \inf_{\gamma_{2j}\in \pi(p_{0j},p_{1j})} \mb{E}_{\gamma_{2j}}\big(X_{0j} - X_{1j}\big)^2.  
\end{align}
Without loss of generality, we assume that $\argmin_{j=1,\ldots,d } \inf_{\gamma_{2j}\in \pi(p_{0j}, p_{1j})} \mb{E}_{\gamma_{2j}}\big(X_{0j} - X_{1j}\big)^2 =1$. That yields $W_{\rm AR}^2[\nu(w^{(1)}, x), \nu(w^{(2)}, x)]\geq d \ [\inf_{\gamma_{21}\in \pi(p_{01}, p_{11})} \mb{E}_{\gamma_{21}}\big(X_{01} - X_{11}\big)^2] $. Next, we focus on
\begin{align}
 &\inf_{\gamma_{21}\in \pi(p_{01}, p_{11})}\mb{E}_{\gamma_{21}}\big(X_{01} - X_{11}\big)^2
 = \inf_{\gamma\in \pi(w^{(1)}, w^{(2)})}\sum_{i,j=1}^n \gamma_{(ij)} (x_{i1} -x_{j1})^2\notag\\
 = &\inf_{\gamma\in \pi(w^{(1)}, w^{(2)})}\bigg\{\sum_{i\neq j} \gamma_{(ij)} (x_{i1} -x_{j1})^2\bigg\}
 \geq\ [\min_{i\neq j}(x_{i1} -x_{j1})^2]\inf_{\gamma\in \pi(w^{(1)}, w^{(2)})}\bigg\{\sum_{i\neq j} \gamma_{(ij)}\bigg\}\notag\\
 =&\ [\min_{i\neq j}(x_{i1} -x_{j1})^2] \inf_{\gamma\in \pi(w^{(1)}, w^{(2)})}\bigg\{ 1-\sum_{i=1}^n \gamma_{(ii)}\bigg\}
 =[\min_{i\neq j}(x_{i1} -x_{j1})^2] \bigg\{ 1-\sum_{i=1}^n \min(w^{(1)}_i, w^{(2)}_i) \bigg\}\notag\\
 \geq&\ \frac{1}{2} [\min_{i\neq j}(x_{i1} -x_{j1})^2] \sum_{i=1}^n |w^{(1)}_i - w^{(2)}_i|, 
\end{align}
where the last step holds since, for any two $a, b\in\mb{R}, |a-b|/2 = (a+b)/2 - \min(a,b) $, and the rest is trivial.  So, we have $(d/2) [\min_{i\neq j}(x_{i1} -x_{j1})^2] \sum_{i=1}^n |w^{(1)}_i - w^{(2)}_i|\ ]\leq \eta $ which yields $ \sum_{i=1}^n |w^{(1)}_i - w^{(2)}_i|\ \leq 2\eta/ [d\times\min_{i\neq j}(x_{i1} -x_{j1})^2]=\eta^{\prime}$. Hence we have the proof.
\end{proof}
\end{lemma}
The arguments in Lemma \ref{W2_lowerbound} can easily be extended for Wasserstein metric with a general cost function.

\begin{lemma}\label{th2_lemma2_asymtotic}
$\sup_{w\in \wt{C}_{\theta, \epsilon}}\big|\prod_{h=1}^n  w_h - \prod_{h=1}^n  w^{\star}_h  \big|\to 0$ as $N\to\infty$. %\textcolor{blue}{Work in progress.}
\begin{proof}
Fix $\theta\in\Theta$ and $\varepsilon>0$. Since for every $w\in \wt{C}_{\theta, \epsilon}$ (refer to equation \eqref{eqn_C_Tilde}), we have $0<W_{\rm AR}^2[F_{\theta}, \nu(w, x)]\leq W_{\rm AR}^2[F_{\theta}, \nu(w^{\star}, x)]\leq \varepsilon$, use of  triangle inequality yields $W_{\rm AR}^2[\nu(w^*, x), \nu(w, x)]\leq 2\varepsilon$. Consequently, by application of Lemma \ref{W2_lowerbound}, we can construct a small rectangle $R$ around $w^\star$ such that $R=\{w: |w_h - w_{h}^{\star}|\le \varepsilon^{\prime},\ h=1,\ldots, n\}$ for an appropriately chosen $\varepsilon^{\prime}>0$.  Further, we assume that no point inside $R$ other than $w^\star$, satisfies $H_{N}(w) = H_{N}(w^\star)$.
Next, we  define a sequence of sets $\mathcal{C}_N = \big\{ w \in R : 0 \le H_{N}(w^*) - H_{N}(w) \le 1/N^{1-\delta}\}$. Clearly, we have $C_{\theta, \epsilon}\subset \mathcal{C}_N$. Denoting $a_{(N)} = \sup_{w \in \mathcal{C}_N} | \prod w_h - \prod w_h^*|$, it is now enough to show that $a_{(N)} \to 0$ as $ N \to \infty$. 

First, we argue that $\mathcal{C}_N\subset R$ is a compact set. To that end, we note that $\mathcal{C}_N\subset R$ is bounded by construction, and it is closed being the intersection of two closed sets -- $R$ and the inverse image of a closed set under the continuous function $H$. So, $\mathcal{C}_N$ is a compact set. Since $\mathcal{C}_N$ is compact and the  function $w\to\prod_{h=1}^n w_h$ is continuous, $a_{(N)}$ is attained at some point $w^{(N)} \in \mathcal{C}_N$. 
%Since $R$ is compact, the sequence ${w^{(N)}}$ will have at least one limit point in $R$. Further,  ${w^{(N)}}$ cannot have a limit point $\neq w^{\star}$ in $R$, since if there is one, we can find a convergent subsequence converging to that limit point, and along that subsequence $H(w^{(N)})$ would converge to some value $\neq H(w^{\star})$. 
%So, if the sequence $w^{(N)}$ is convergent, it must converge to $w^\star$. 

Next, we argue that $\cap_{N=1}^{\infty} \mathcal{C}_N = \{w^{\star}\} $. Here $\mathcal{C}_1\supset \mathcal{C}_2\supset\ldots$ is a nested sequence of non-empty compact sets, and  since $w^\star\in \mathcal{C}_N$ for all $N$ we have $w^{\star}\in \cap_{N=1}^{\infty} \mathcal{C}_N$ . Now, if there is another point $\tilde{w}\in \cap_{N=1}^{\infty} \mathcal{C}_N$, we have $H(\tilde{w}) = H(w^\star)$, which leads to a contradiction. So, $\mathcal{C}_N$ is a nested (decreasing), non-empty sequence of compact sets in $\mathbb{R}^n$, whose intersection is a singleton. 

Next, we argue that $\mbox{diam}(\mathcal{C}_N):= \sup\{ \|w - w^{\prime}\| : w, w^{\prime} \in \mathcal{C}_N)\} \to 0$ as $N\to\infty$. To that end, suppose $\mbox{diam}(\mathcal{C}_N)> r >0$ for all $N$. Then there exists $b^{(N)}, c^{(N)}\in \mathcal{C}_N$ such that $|b^{(N)}- c^{(N)}|> r$ and $b^{(N)}\to b,\ c^{(N)}\to c$ as $N\to\infty$. Consequently, $|b-c|>r$ , and since $\cap_{N=1}^{\infty} \mathcal{C}_N$ is compact $b, c \in \cap_{N=1}^{\infty} \mathcal{C}_N$, this leads to a contradiction. So, $\mbox{diam}(\mathcal{C}_N) \to 0$ as $N\to\infty$.

Finally, since $w^{\star}, w^{(N)}\in \mathcal{C}_N$, we have $|w^{\star}- w^{(N)}|\leq \mbox{diam}(\mathcal{C}_N)$ trivially. Application of Sandwich theorem yields $w^{(N)}\to w^{\star}$ as $N\to\infty$. Hence we have $a_N\to 0$ as $N\to\infty$. 
\end{proof}
\end{lemma}



%\textcolor{blue}{(as noted before, $w\in \wt{\mb{W}}\cap  C_{\theta,\varepsilon}$  is redundant in the statement of the lemma below. also, the statement is unnecessarily complicated. no need to introduce all these notation in the statement, which can simply read $|w_h - w_h^\star| \le \text{blah}$. the notation can come inside the proof.)}

%\textcolor{blue}{(I am a bit concerned about the $\delta_h$s. Note that what we are trying to show has to hold for every $w \in \wt{\mb{W}}$. Not clear to me why all the coordinates of every such $w$ has to fall inside $H_2$, which is what the lemma is essentially claiming. )} \textcolor{red}{I see the point. Here $\Delta_{N, h}$ perhaps needs to be more elaborate. Making an update.
%e.g $\Delta_{N, h} = \sum_{l\in \Upsilon_h} c_l\ N^{\delta_{(l)}}$ where $\sup_{l\in \Upsilon_h} \delta_{(l)} = \delta_h$ and $\Upsilon_h$ is an index set.  
%}
%\begin{lemma}\label{th3:lemma2_a}
%For $w\in \wt{C}_{\theta,\varepsilon}$, $|w_h - w_h^\star|$ is always a decreasing function of $N$ that $\downarrow 0$ as $N\to\infty$.
%\end{lemma}


\section{Auxiliary results for the proof of Theorem \ref{th3}}\label{aux:th3}
%\textcolor{red}{[everything before this repeats what is already in the maid doc. If you need to refer to these facts/definitions, just refer to appropriate section/page number/equation number in the main document. also, the auxiliary results can go at the end of this document.]}
Given two EMMs $p_0$ and $p_1$ on $\mb{R}^d$, the optimal transport plans are usually not an EMM. To avoid this, we can choose to restrict the set of admissible transport plans to the family of  EMMs, and tentatively define a modified 2-Wasserstein metric by
\begin{align}\label{eqn:mwalpha}
\mbox{\rm MW}_{2,\alpha}^{2}(p_0, p_1)
= \inf_{\nu\in \pi(p_0,p_1) \cap \EMM^{\alpha}_{2d}(\infty) }\ \int_{\mb{R}^d\times\mb{R}^d}\
\left\lVert y_0 - y_1 \right\rVert^2\ d\nu(y_0,y_1).
\end{align}

It is worth pointing out that, we briefly discussed about a slight variation of the definition above in Section \ref{ssec:andrew} of the main document given  by 
\begin{align} 
\mbox{\rm MW}_{2}^{2}(p_0, p_1)
= \inf_{\nu\in \pi(p_0,p_1) \cap \EMM_{2d}(\infty) }\ \int_{\mb{R}^d\times\mb{R}^d}\
\left\lVert y_0 - y_1 \right\rVert^2\ d\nu(y_0,y_1).
\end{align}

In this section, it's beneficial to work with \eqref{eqn:mwalpha} as an important building block to introduce our novel Wasserstein metric $W_{\rm AR}^{2}$, which critically involves an additional augmentation scheme  described in Section \ref{ssec:andrew} of the main document. To that end, we first list out some definitions following \citet{10.2307/4616956}.

\begin{definition}\label{def:indentifiable_emm}
Finite mixtures from the location scatter family $\{f_{\zeta, d} : \zeta = (\theta,\mu,\Sigma)\in \mb{A}^d\}$ are called identifiable if a relation of the form
\begin{align*} 
\sum_{j=1}^m \lambda_j f_{\zeta_j, d}(x) = \sum_{j=1}^m \lambda^{\prime}_j f_{\zeta^{\prime}_j, d}(x),\ x\in\mb{R}^d
\end{align*}
where $m$ is a positive integer, $\sum_{j=1}^m \lambda_j = \sum_{j=1}^m \lambda^{\prime}_j = 1$, $\lambda_j, \lambda^{\prime}_j > 0$ for $j = 1, \ldots,m$ implies there exists a permutation $\sigma$ such that $(\lambda_j, \zeta_j) = (\lambda^{\prime}_{\sigma(j)}, \zeta^{\prime}_{\sigma(j)})$ for all $j$.
\end{definition}

\begin{definition}
A function $f_d(\cdot,\theta)$ is called a density generator if it is a non-negative function on $[0,\infty) $ or $(0,\infty) $ such that the spherically symmetric function $f_d(x^\T x,\theta), x\in\mb{R}^d$ integrates to 1.
\end{definition}

\begin{definition}
A function $\phi(u),\ u\geq 0$ is called a characteristic generator in dimension $d\geq 1$ ,if $\phi(t^\T t)$  is the characteristic function of a probability distribution on $\mb{R}^d$. 
\end{definition}
Next, we record a series of sufficient conditions for identifiability of EMMs in terms of density generators and characteristic generators provided in  \citet{10.2307/4616956}. 

\begin{lemma}\label{th1:lemma1}[A sufficient condition for identifiability of EMMs via characteristic generators ]{
Suppose that a parametric family of characteristic generators gives rise to families of elliptical densities $\{f_{\zeta, d} : \zeta = (\theta,\mu,\Sigma)\in \mb{A}^d\}$ in dimension $1\leq d < q$. Suppose there exists a total ordering $\preceq$ on the set $\mb{B}^*$ such that $\beta_1\prec\beta_2$ implies
\begin{align*}
\lim_{u\to \infty}\frac{\phi_{\beta_1}(u)}{\phi_{\beta_2}(u)} = 0.
\end{align*}
}
Then finite mixtures from the class $\{f_{\zeta, d} : \zeta = (\theta,\mu,\Sigma)\in \mb{A}^d\}$ of elliptical distributions in $\mb{R}^d$ are identifiable for each $1\leq d < q$.
\end{lemma}
Family of multivariate t-distribution, symmetric stable law, band-limited densities satisfy the sufficient condition.

\begin{lemma}\label{th1:lemma2}[A sufficient condition for identifiability of EMMs via density generators]{

Let $f_d(\cdot,\theta),\theta\in\Theta$  be a parametric family of density generators for spherically symmetric distributions in $\mb{R}^d$. Let $\mb{C}=\Theta\times(0,\infty)\times\mb{R}$ and let $\gamma_j = (\theta_j,a_j,b_j)\in\mb{C},\ j= 1,2$. Suppose there exists a total ordering $\preceq$ on the set $\mb{C}$ such that $\gamma_1\prec\gamma_2$ implies
\begin{align*} 
\lim_{u\to \infty}\frac{f_d(a_2 u^2 + b_2 u + c_2) ,\theta)}{f_d(a_1 u^2 + b_1 u + c_1) ,\theta)} = 0,\ c_1,c_2\in\mb{R}.
\end{align*}
}
Then finite mixtures from the class $\{f_{\zeta, d} : \zeta = (\theta,\mu,\Sigma)\in \mb{A}^d\}$ of elliptical distributions in $\mb{R}^d$ are identifiable for each $1\leq d < q$.
\end{lemma}
Exponential power distribution, the original Kotz distribution and the multivariate normal law satisfy the above condition. \citet{10.2307/4616956} also provides sufficient conditions for identifiability of location-scatter mixtures for density generators that lacks smoothness at the origin, and Normal scale mixtures.  With these, we now have all the necessary machinery to present the results that leads to the proof of Theorem \ref{th3}.
\begin{lemma}\label{th1:lemma3}{
The optimal transport between two elliptical distributions $ \mbox{\rm ED}_h(a, A)$ and $ \mbox{\rm ED}_{h}(b, B)$ in $\mb{R}^d$ is in the same elliptical family in $\mb{R}^{2d}$. 
}
\end{lemma}
\begin{proof}
Since $X_0\sim p_0\equiv \mbox{\rm ED}_h(a, A)$ and $X_1\sim p_1\equiv \mbox{\rm ED}_h(b, B)$, we know $W_{2}^2(p_0, p_1) = ||a-b||^2 + \nu_h\ \mb{B}(A,B)$ \citep{muzellec2019generalizing}, where $\mb{B}(A,B) = \mbox{tr}[A + B - 2(A^{1/2}BA^{1/2})^{1/2}]$ is the Bures metric \citep{https://doi.org/10.48550/arxiv.1712.01504} between matrices. So, it is enough to show that there exists a coupling belonging to the same  elliptical family in $\mb{R}^{2d}$ with marginals $p_0$ and $p_1$ that incurs the optimal cost $ ||a-b||^2 + \nu_h\ \mb{B}(A,B)$. To that end, consider the construction
\begin{align*} 
& X_0\sim \mbox{ED}_h(a, A)\\
& X_1 = b + A^{-1/2}(A^{1/2}BA^{1/2})^{1/2}A^{1/2}(X_0-a),
\end{align*}
which ensures that $X_1 \sim  \mbox{ED}_{h}(b, B)$. Further, it ensures that $(X_1, X_2)$ is  a coupling belonging to the same  elliptical family in $\mb{R}^{2d}$ which is easy to see from the form characteristic function in equation \eqref{eqn:ch_EMM}.
Denoting $H=A^{-1/2}(A^{1/2}BA^{1/2})^{1/2}A^{1/2}$ and  $D=X_0-X_1=(I - H)X_0 - b + Ha$, we have
\begin{align*} 
\mu_D &= \mb{E}(D) = a-b,\\
\Sigma_D&=\mbox{Var}(D) = \nu_h(I-H)A(I-H)^T\\
&=\nu_h[A+B-A^{-1/2}(A^{1/2}BA^{1/2})^{1/2}A^{1/2}-A^{1/2}(A^{1/2}BA^{1/2})^{1/2}A^{-1/2}].
\end{align*}
Consequently, we have $ 
\mb{E}(||D||^2) = \mb{E}(D^\T D)
= \mu_{D}^\T\mu_{D} +\mbox{tr}(\Sigma_D)
=||a-b||^2 + \nu_h\mb{B}(A,B),
$
which completes the proof.
\end{proof}

%The proof of Lemma 4 & Lemma 5 in this section goes along the lines of similar results in Delony and Desolneuxz, 2019 
%for GMMs.

\begin{lemma}\label{th1:lemma4}[Discrete formulation]{ Suppose we have two EMMs, that satisfy conditions in Lemma \ref{th1:lemma1} and/or Lemma \ref{th1:lemma2}, $p_0 = \sum_{k=1}^{K_0} s_{0k}  \mu_{0k}$ with $\mu_{0k}\sim\mbox{\rm ED}_h(m_{0k}, \Sigma_{0k})$, and $p_1 = \sum_{k=1}^{K_1} s_{1k} \mu_{1k}$ with $\mu_{1k}\sim\mbox{\rm ED}_h(m_{1k}, \Sigma_{1k})$. Then, we have
\begin{align*} 
\mbox{\rm MW}_{2,\alpha}^{2}(p_0, p_1) = \inf_{\pi\in\pi^{\alpha}(s_0,s_1)}\sum_{k,l} \pi_{kl}\ W_{2}^{2}(\mu_{0k}, \mu_{1l})
\end{align*}
}
where $\Pi = ((\pi_{kl}))$ and 
$
  \pi^{\alpha}(s_0,s_1) = \big\{ \Pi: \Pi 1_{K_1} = s_0,\ \Pi^{\T} 1_{K_0} = s_1,\ D_{\rm KL}(\Pi\ ||\ s_0 s_{1}^{\T}) \leq\alpha\big\}$.
\end{lemma}
\begin{proof}
The proof of the Lemma is adapted from Proposition 4 in \citet{delon:hal-02178204}, that proves the result for Gaussian mixture models.  We extend the result for our augmented and restricted class of Elliptical mixture models.

First, we assume $\pi^{\star}$ be a solution to the linear program
\begin{align}\label{th1:lemmma3_lp}
\inf_{\pi\in\pi^{\alpha}(s_0,s_1)}\sum_{k,l} \pi_{kl}\ W_{2}^{2}(\mu_{0k}, \mu_{1l}).    
\end{align}
Next, for every $(k,l)$ we denote the optimal coupling as
\begin{align*} 
\gamma_{kl}= \argmin_{\gamma\in\pi(\mu_{0k},\mu_{1l})} \int_{\mb{R}^d\times\mb{R}^d}\
\left\lVert y_0 - y_1 \right\rVert^2\ d\gamma(y_0,y_1). 
\end{align*}
By Lemma \ref{th1:lemma3}, $\gamma_{kl}$ belongs to the same elliptical family of distributions. Next, we construct $\gamma^{\star}=\sum_{k,l} \pi^{\star}_{kl}\ \gamma_{kl}$ . By construction,   $\gamma^{\star}\in\pi(p_0,p_1) \cap \EMM^{\alpha}_{2d}(K_0 K_1)$ and that trivially yields
\begin{align}\label{th1:lemma3_oneside} 
\sum_{k,l} \pi^{\star}_{kl}\ W_{2}^{2}(\mu_{0k}, \mu_{1l})
&=\int_{\mb{R}^d\times\mb{R}^d}\
\left\lVert y_0 - y_1 \right\rVert^2\ d\gamma^{\star}(y_0,y_1)\notag\\
&\geq \inf_{\gamma\in \pi(p_0,p_1) \cap \EMM^{\alpha}_{2d}(K_0 K_1) }\ \int_{\mb{R}^d\times\mb{R}^d}\
\left\lVert y_0 - y_1 \right\rVert^2\ d\gamma(y_0,y_1).
%&\geq \inf_{\gamma\in \pi(p_0,p_1) \cap \EMM_{2d}(\infty) }\ \int_{\mb{R}^d\times\mb{R}^d}\
%\left\lVert y_0 - y_1 \right\rVert^2\ d\gamma(y_0,y_1).
\end{align}

Next,  for every $\gamma=(\gamma_{(0)},\gamma_{(1)})\in\pi(p_0,p_1) \cap \EMM^{\alpha}_{2d}(K_0 K_1)$, there  exists a $K$ such that $\gamma = \sum_{j=1}^{K} w_j\gamma_j$ where $\gamma_j$ is from the same elliptical family of distribution in $\mb{R}^{2d}$. Further, since $\gamma$ has marginal distributions $p_0$ and $p_1$, we have
\begin{align*}
    &\int_{\mb{R}^d} \gamma d\gamma_0 = \sum_{j=1}^{K} w_j\int_{\mb{R}^d}\gamma_j d\gamma_{(0)} =  \sum_{k=1}^{K_0} s_{0k}  \mu_{0k},\quad \\
    %\text{and}\quad
    &\int_{\mb{R}^d} \gamma d\gamma_1 = \sum_{j=1}^{K} w_j\int_{\mb{R}^d}\gamma_j d\gamma_{(0)} =  \sum_{k=1}^{K_1} s_{1k}  \mu_{1k}.
\end{align*}
By the definition of identifiability of elliptical mixture models in definition \ref{def:indentifiable_emm}, we know that  mixtures must have the same components. Consequently, there exists a $(k,l), 1\leq k\leq K_0, 1\leq l\leq K_1$ such that $\gamma_j\in\pi(\mu_{0l},\mu_{1k})$, and we can express $\gamma = \sum_{k,l}\pi_{kl} \gamma_{kl}$  for some $\pi\in\pi^{\alpha}(s_0,s_1)$. So, we have
\begin{align}\label{th1:lemma3_otherside}
\int_{\mb{R}^d\times\mb{R}^d}\
\left\lVert y_0 - y_1 \right\rVert^2\ d\gamma(y_0,y_1)
\geq \sum_{k,l} \pi_{kl} W_2^{2}(\mu_{0k},\mu_{1l}) 
\geq \sum_{k,l} \pi^{\star}_{kl} W_2^{2}(\mu_{0k},\mu_{1l}),
\end{align}
where the final inequality holds by equation \eqref{th1:lemmma3_lp}.
Equations \eqref{th1:lemma3_oneside}-\eqref{th1:lemma3_otherside} together complete the proof.
\end{proof}

\begin{lemma}\label{th1:lemma5}{
Suppose the conditions in Lemma \ref{th1:lemma1} and/or \ref{th1:lemma2} hold. Suppose we  $p_0 = \sum\limits_{k=1}^{K_0} s_{0k}  \mu_{0k}$ with $\mu_{0k}\sim\mbox{\rm ED}_h(m_{0k}, \Sigma_{0k})$, and $p_1 = \sum\limits_{k=1}^{K_1} s_{1k} \delta_{m_{1k}}$. Let $ p_0^{\prime} = \sum\limits_{k=1}^{K_0} s_{0k}\ \delta_{m_{0k}}$. Then
\begin{align*} 
\mbox{\rm MW}_{2,\alpha}^{2}(p_0, p_1) = W_{2}^{2}( p_0^{\prime}, p_1) + \nu_{h}\sum\limits_{k=1}^{K_0} s_{0k}\ \mbox{\rm tr}(\Sigma_{0k}).
\end{align*}
}
\end{lemma}

\begin{proof}
By Lemma \ref{th1:lemma4},
\begin{align*} 
&\mbox{\rm MW}_{2,\alpha}^{2}(p_0, p_1) = \inf_{\pi\in\pi^{\alpha}(s_0,s_1)}\sum_{k,l} \pi_{kl}\ W_{2}^{2}(\mu_{0k}, \delta_{m_{1l}}) \notag\\
&= \inf_{\pi\in\pi^{\alpha}(s_0,s_1)}\sum_{k,l} \pi_{kl}\big\{ ||m_{0k}-m_{1l}||^2 + \nu_h\ \mbox{tr}(\Sigma_{0k})\big\}
= \bigg[ \inf_{\pi\in\pi^{\alpha}(s_0,s_1)} \big<\Pi, M\big>\bigg] + \nu_{h}\sum\limits_{k=1}^{K_0} s_{0k}\ \mbox{tr}(\Sigma_{0k}),
\end{align*}
and we have the proof.
\end{proof}


\begin{lemma}\label{th1:lemma6}[Entropy regularization of discrete optimal transport]
For every $\alpha\in\mb{R}^{+}$, $\exists\ \lambda_{\alpha}\ > 0$ such that
\begin{align*} 
\inf_{\pi\in\pi^{\alpha}(s_0,s_1)} \big<\Pi, M\big> = \inf_{\pi\in\pi(s_0,s_1)} \bigg[\big<\Pi, M\big> - \frac{1}{\lambda_{\alpha}}H(\Pi)\bigg]
\end{align*}
where $H(\Pi) = -\sum_{k,l} \pi_{kl}\log \pi_{kl}$.
\end{lemma}
\begin{proof}
Proof of this result is recorded in \citet{cuturi2013sinkhorn}.
\end{proof}


% Notation
% 1. Bures B
% 2. p,\mu,m

%% UQ
\section{Additional simulation results for generalised linear regression in \ref{ssec:gen_reg}}\label{sup:glm}
In Table \ref{table:glm_1} in Section \ref{ssec:gen_reg} in the main document, we expand on the performance of $\mbox{D}$-BETEL for varying  extent of perturbations in the data generating mechanism with sample size $n=100$. Here we present additional simulation results for $n=250, 500$. In particular, we compare  $\mbox{D}$-BETEL against standard posterior based approach, as well as Bayesian ETEL \citep{Chib2018} with the estimating equations set to  $\mbox{E}[\partial \log l(\beta\mid X, Y)/ \partial \beta]=0$ to infer about the parameter $\beta$. 
\begin{table}[!htb]
  \caption{\emph{\textbf{Generalised linear regression (Poisson regression).} Here the \textbf{sample size $n$ is $250$}. We compare standard posterior yielded from the fully parametric model, moment conditional model (MCM) based on the maximum likelihood equations,
  %(\textcolor{blue}{again, say which model}) 
  and $\mbox{D}$-BETEL based parameter estimates over 50 replicated simulations with proportion of outlier $p=0.10, 0.12, 0.15$. %\textcolor{green}{Need to edit} %\textcolor{green}{Should I include $p=0.05,0.02?$} %\textcolor{blue}{Yes, if there is space}. 
  We report the $L_1$ error of posterior means, length of the HPD sets and associated coverage probabilities (within braces). $\mbox{D}$-BETEL is more resistant towards presence of outliers all values of $p$ considered, however it provides slightly wider $95\%$ credible sets while maintaining the high coverage probability. }}\label{table:glm_2}
  \centering
  \begin{tabular}{llllllll}
    \toprule
    %\multicolumn{2}{c}{Part}                   \\
    \cmidrule(r){1-2}
     &     &       \multicolumn{2}{c}{$\mbox{D}$-BETEL} & \multicolumn{2}{c}{Standard posterior} & \multicolumn{2}{c}{MCM} \\
    \midrule
    p & $\theta$     & $||\theta - \hat{\theta}||_1$     & HPD  & $||\theta - \hat{\theta}||_1$     & HPD & $||\theta - \hat{\theta}||_1$     & HPD   \\
    \midrule
    0.10 & $\beta_0$ & 0.03 & 0.19 (1.00)  & 0.14   & 0.12  (0.24)  &0.31 & 0.14 (0.12)\\
        & $\beta_1$ & 0.01 & 0.03 (1.00)  & 0.06 &0.02 (0.00) 
        & 0.05& 0.02 (0.22)\\
    \midrule
    0.12 & $\beta_0$ &0.05 &0.20 (0.95) &0.24& 0.12 (0.16)
    &0.36 & 0.20 (0.14)\\
        & $\beta_1$ & 0.01 &0.04 (0.95) &0.06& 0.02 (0.11)
        &0.05 & 0.04 (0.18)\\
    \midrule
    0.15 & $\beta_0$ & 0.05& 0.30 (1.00)  &0.30  & 0.11 (0.20) & 0.35 & 0.13 (0.06)\\
        & $\beta_1$ & 0.01 &0.05 (1.00)  &0.05& 0.01 (0.20) & 0.05 & 0.02 (0.08)\\
    
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[!h]
  \caption{\emph{\textbf{Generalised linear regression (Poisson regression.)} Here the \textbf{sample size $n$ is $500$}. We compare standard posterior yielded from the fully parametric model, moment conditional model (MCM) based on the maximum likelihood equations,
  %(\textcolor{blue}{again, say which model}) 
  and $\mbox{D}$-BETEL based parameter estimates over 50 replicated simulations with proportion of outlier $p=0.10, 0.12, 0.15$. %\textcolor{green}{Need to edit} %\textcolor{green}{Should I include $p=0.05,0.02?$} %\textcolor{blue}{Yes, if there is space}. 
  We report the $L_1$ error of posterior means, length of the HPD sets and associated coverage probabilities (within braces). $\mbox{D}$-BETEL is more resistant towards presence of outliers all values of $p$ considered, however it provides slightly wider $95\%$ credible sets while maintaining the high coverage probability}.}\label{table:glm_3}
  \centering
  \begin{tabular}{lllllllll}
    \toprule
    %\multicolumn{2}{c}{Part}                   \\
    \cmidrule(r){1-2}
     &     &       \multicolumn{2}{c}{$\mbox{D}$-BETEL} & \multicolumn{2}{c}{Standard posterior} & \multicolumn{2}{c}{MCM} \\
    \midrule
    p & $\theta$     & $||\theta - \hat{\theta}||_1$     & HPD  & $||\theta - \hat{\theta}||_1$     & HPD & $||\theta - \hat{\theta}||_1$     & HPD  \\
    \midrule
    0.10 & $\beta_0$ & 0.03 & 0.15 (0.91)  & 0.15   & 0.12  (0.47) & 0.24 & 0.15 (0.22)\\
        & $\beta_1$ & 0.01 & 0.02 (0.93) & 0.03 &0.02 (0.19) 
        & 0.03 & 0.03 (0.32)\\
    \midrule
    0.12 & $\beta_0$ &0.06 &0.19 (0.83) &0.19& 0.12 (0.20) & 0.24 & 0.23 (0.26)   \\
        & $\beta_1$ & 0.01 &0.04 (0.85) &0.03& 0.02 (0.35) & 0.03 & 0.04 (0.38) \\
    \midrule
    0.15 & $\beta_0$ & 0.08& 0.30 (0.92)  &0.21  & 0.12 (0.21) & 0.31 &0.16 (0.18)\\
        & $\beta_1$ & 0.02 &0.06 (0.92)  &0.03& 0.02 (0.25) & 0.04 & 0.03 (0.22)\\
    
    \bottomrule
  \end{tabular}
\end{table}

%\clearpage
%\section*{To be discarded}
%\begin{lemma}\label{th3:lemma2}
%For $w\in \wt{C}_{\theta,\varepsilon}$, $ \big|\prod_{h=1}^n w_{h}^{\star} - \prod_{h=1}^n w_{h}\big| <\frac{n}{N^{1-\delta^{\prime}}}$ for some $\delta^{\prime}$ satisfying $0<\delta<\delta^{\prime}\leq1$.


%\begin{proof}
%Fix a $w\in \wt{C}_{\theta,\varepsilon}$. Since the function $w\to\prod_{h=1}^n w_h$ is symmetric in its arguments, we shall only consider $w$ such that $w_1<w_2<\ldots<w_n$. We can write $w_h - w^{\star}_h = \Delta_{N, h}/N$  where $\Delta_{N, h}\in\{-N,\ldots, N\}$ and $\sum_{h=1}^n \Delta_{N, h} = 0$ . Further, for $0<\delta <\delta^{\prime}\leq1$, we introduce a partition of $\{1,\ldots, n\} = \mbox{H}_1\cup \mbox{H}_2$:
%\begin{align*}
%    |\Delta_{N, h}|=
%\begin{cases}
%\geq N^{\delta^{\prime}},\quad h\in \mbox{H}_1,\\
%<N^{\delta^{\prime}}, \quad h\in \mbox{H}_2.
%\end{cases}
%\end{align*}
%In the remaining of the proof, we shall show that  $\mbox{H}_1 = \mb{\phi}$, i.e the null set. To that end, consider $H(w^{\star}) - H(w)=$
%\begin{align*}
% &\sum_{h=1}^n w^{\star}_h\bigg[\log\bigg(w^{\star}_h + \frac{\Delta_{N, h}}{N}\bigg) - \log w^{\star}_h\bigg] + \sum_{h=1}^n\frac{\Delta_{N, h}}{N}\log\bigg(w^{\star}_h + \frac{\Delta_{N, h}}{N}\bigg),\\
% =& \sum_{h=1}^n\frac{\Delta_{N, h}}{N} +  \sum_{h=1}^n\frac{\Delta_{N, h}}{N} \log w^{\star}_h - \sum_{h=1}^n\frac{\Delta^2_{N, h}}{N^2} \frac{1}{(w_{h}^{\prime})^2},\quad\text{where} \quad |w_{h}^{\prime}-w_{h}^{\star}|\leq |w_{h}-w_{h}^{\star}|,\\
% =& \sum_{h=1}^n\frac{\Delta_{N, h}}{N} \log w^{\star}_h   - \sum_{h=1}^n\frac{\Delta^2_{N, h}}{N^2} \frac{1}{(w_{h}^{\prime})^2} \\
% =& \sum_{h\in H_1}\frac{\Delta_{N, h}}{N}  \log w^{\star}_h + \sum_{h\in H_2}\frac{\Delta_{N, h}}{N} \log w^{\star}_h  - \sum_{h=1}^n\frac{\Delta^2_{N, h}}{N^2} \frac{1}{(w_{h}^{\prime})^2}.
%\end{align*}
%Since by definition the left hand side of the equation is always positive and  the expression above is dominated by the first term, $\sum_{h\in H_1}\mbox{sign}[\Delta_{N, h}] \log w^{\star}_h$ is positive. This yields a contradiction to the fact that $w\in    \wt{C}_{\theta,\varepsilon}$ unless $\mbox{H}_1$ is empty. \textcolor{red}{This excludes  cases where $\sum_{h\in H_1} \mbox{sign}[\Delta_{N, h}] \log w^{\star}_h$ .} 
%We have
%\begin{align*}
%    \bigg|\prod_{h=1}^n w_{h}^{\star} - \prod_{h=1}^n w_{h}\bigg| \le \sum_{h=1}^n |w_h - w_h^\star|<\frac{n}{N^{1-\delta^{\prime}}},\quad \text{for some}\quad 0<\delta< \delta^{\prime} <1,
%\end{align*}
%via  repeated use of  the \emph{triangle inequality}. Hence we have the proof. 
%\end{proof}
%end{lemma}
%\textcolor{blue}{(
%Your $H_1$ should be $|\Delta| \ge $, not $=$. Also,
%I am not sure how you are bounding the second order Taylor terms as they should have terms like $1/w_h$, which can be as big as $N$.)} \textcolor{red}{-- As long as $w_h>0$,  $1/w_h$ cannot be like $N$.}

%\subsection*{An useful calculation under set up in lemma 9}
%Suppose the observations $x_1,\ldots, x_n\in\mb{R}$ and $F_{\theta} \equiv \mbox{ED}_h(m,\Sigma)$. Let us introduce the cumulative probabilities  $ F_{j}^{\star} = \sum_{h=1}^j w_{h}^{\star},\ F_j = \sum_{h=1}^j w_{h} =  F_{j}^{\star} +\sum_{h=1}^j \Delta_{N,h}/N$.
%We consider difference between our Wassertsein metric barring the entropic regularization:
%{\smaller
%\begin{align*}
%    &W_{\rm AR}^2[F_{\theta}, \nu(w^{\star}, x)] - W_{\rm AR}^2[F_{\theta}, \nu(w, x)]\notag\\
%    =&-\sum_{h=1}^{n}\frac{\Delta_{N,h}}{N}(x_h-m)^2 +\sum_{h=1}^{n}\bigg\{\int_{F_{h-1}^{\star}}^{F_{h}^{\star}}(F_{\theta}^{-1}(z) - x_{h-1})^2dz - \int_{F_{h-1}}^{F_{h}}(F_{\theta}^{-1}(z) - x_{h-1})^2dz
%    \bigg\}\\
%    =&-\sum_{h=1}^{n}\frac{\Delta_{N,h}}{N}(x_{h}^2-2mx_{h})- \sum_{h=1}^{n}\bigg[x_{h-1}^2\ \frac{\Delta_{N,h}}{N} - 2x_{h-1} F_{\theta}^{-1}(t_{h})\ \frac{\Delta_{N,h}}{N}  + 2x_{h-1}w_{h}^{\star} \bigg\{F_{\theta}^{-1}(t^{\star}_{h}) - F_{\theta}^{-1}(t_{h})\bigg\}\bigg]
   % =& \sum_{h\in H_1}\frac{\Delta_{N,h}}{N}\bigg[x_{h}^2 - 2x_{h} F_{\theta}^{-1}(t_{h})\  \bigg] + \sum_{h\in H_2}\frac{\Delta_{N,h}}{N}\bigg[x_{h}^2\  - 2x_{h} F_{\theta}^{-1}(t_{h})\  \bigg]+ 2\sum_{h=1}^n x_{h}w_{h}^{\star} \bigg\{F_{\theta}^{-1}(t_{h}^{\star}) - F_{\theta}^{-1}(t_{h})\bigg\}, 
%\end{align*}
%}
%where $F_{h-1}< t_h< F_{h}$, and  $F^{\star}_{h-1}< t^{\star}_h< F^{\star}_{h}$ which can be expressed as  $F_{h-1} - \sum_{l=1}^{h-1} \Delta_{N,l}/N < t^{\star}_h<F_{h} - \sum_{l=1}^{h} \Delta_{N,l}/N$. 


%\bibliography{paper-ref}
%\bibliographystyle{apalike}
%\bibliographystyle{plainnat}
%\bibliographystyle{biometrika}
\end{document}
