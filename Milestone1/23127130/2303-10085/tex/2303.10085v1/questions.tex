    %\documentclass[manuscript]{biometrika}
%\documentclass[11pt, oneside]{biometrika} # FORMAT CHANGE
\documentclass[12pt]{article}
\usepackage[left = 1 in, right = 1 in, top = 1 in, bottom = 1.5 in]{geometry}              
\geometry{letterpaper}  
%\usepackage{amsmath}

%% Please use the following statements for
%% managing the text and math fonts for your papers:
%\usepackage{times}
%\usepackage[cmbold]{mathtime}
%\usepackage{bm}
%\usepackage{natbib}

%\usepackage[plain,noend]{algorithm2e}
%%%%%%%%%%%%%%%%%%
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsthm, amssymb,natbib,xcolor,multicol,url}
\usepackage{graphicx,bbm}
\usepackage{booktabs,epstopdf,color}
\usepackage[space]{grffile}
\usepackage{lineno,comment}
\usepackage[plain,noend]{algorithm2e}
\usepackage{multirow}

\usepackage{hyperref}

\usepackage{amssymb}
%%% mine
% --- defs --- %
\def\m{\mathcal}
\def\mb{\mathbb}
\def\mr{\mathrm}
\def\ms{\mathscr}
\def\ind{\mathbbm{1}}
\def\wt{\widetilde}
\def\wth{\widehat}
%%% mine


\makeatletter
\renewcommand{\algocf@captiontext}[2]{#1\algocf@typo. \AlCapFnt{}#2} % text of caption
\renewcommand{\AlTitleFnt}[1]{#1\unskip}% default definition
\def\@algocf@capt@plain{top}
\renewcommand{\algocf@makecaption}[2]{%
  \addtolength{\hsize}{\algomargin}%
  \sbox\@tempboxa{\algocf@captiontext{#1}{#2}}%
  \ifdim\wd\@tempboxa >\hsize%     % if caption is longer than a line
    \hskip .5\algomargin%
    \parbox[t]{\hsize}{\algocf@captiontext{#1}{#2}}% then caption is not centered
  \else%
    \global\@minipagefalse%
    \hbox to\hsize{\box\@tempboxa}% else caption is centered
  \fi%
  \addtolength{\hsize}{-\algomargin}%
}
\makeatother

%%% User-defined macros should be placed here, but keep them to a minimum.
\def\Bka{{\it Biometrika}}
\def\AIC{\textsc{aic}}
\def\T{{ \mathrm{\scriptscriptstyle T} }}
\def\v{{\varepsilon}}

% -- declared math operators -- %
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\TV}{TV}

% --- defs --- %
\def\m{\mathcal}
\def\mb{\mathbb}
\def\mr{\mathrm}
\def\ms{\mathscr}
\def\ind{\mathbbm{1}}
\def\wt{\widetilde}
\def\wth{\widehat}

\def\T{{\mathrm{\scriptscriptstyle T} }}
\def\I{{\mathrm{\scriptscriptstyle I} }}
\def\D{{\mathrm{D} }}
\def\ELPD{{\mathrm{ELPD} }}
\def\EMM{{\mathrm{EMM} }}
\def\SE{{\mathrm{SE} }}
\def\Var{{\mathrm{Var} }}

\def\MF{{\mathrm{\scriptscriptstyle MF} }}
\def\DY{{\mathrm{\scriptscriptstyle DY} }}
\def\LV{{\mathrm{\scriptscriptstyle LV} }}


% -- new commands -- %
\newcommand{\ypbtodo}[1]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red]{#1}}
\newcommand \bbP{\mathbb{P}}
\newcommand \bbE{\mathbb{E}}
\newcommand{\blds}[1]{\mbox{\scriptsize \boldmath $#1$}}

\newcommand{\be}{\begin{equs}}
\newcommand{\ee}{\end{equs}}


\newcommand{\PX}{\mathscr P_2^r(\m X)}
\newcommand{\PXj}{\mathscr P_2^r(\m X_j)}


% -- theorems etc -- %
\numberwithin{equation}{section}
%\theoremstyle{plain} # FORMAT CHANGE


\addtolength\topmargin{35pt}
\DeclareMathOperator{\Thetabb}{\mathcal{C}}

\begin{document}

%\jname{Biometrika} # FORMAT CHANGE
%% The year, volume, and number are determined on publication
%\jyear{2017}
%\jvol{103}
%\jnum{1}
%% The \doi{...} and \accessdate commands are used by the production team
%\doi{10.1093/biomet/asm023}
%\accessdate{Advance Access publication on 31 July 2016}

%% These dates are usually set by the production team
%\received{2 January 2017}
%\revised{1 April 2017}
% Keywords command
% -- theorems etc -- %
\numberwithin{equation}{section}
%\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}

\newtheorem{definition}{Definition}

\providecommand{\keywords}[1]
{
  \small	
  \textbf{\textit{Keywords---}} #1
}

%% The left and right page headers are defined here:
%\markboth{Abhisek Chakraborty, Anirban Bhattacharya,  Debdeep Pati}{Robust probabilistic inference via a constrained transport metric}

%% Here are the title, author names and addresses
\title{Q \& A}

%\author{Abhisek Chakraborty, Anirban Bhattacharya, Debdeep Pati}
%# FORMAT CHANGE
%\affil{Department of Statistics, Texas A\&M University,\\ College Station, TX 77843, U.S.A. \email{cabhisek@stat.tamu.edu} \email{anirbanb@stat.tamu.edu}
%\email{debdeep@stat.tamu.edu}
%}


\maketitle

$\mathbf{(1)}$ \emph{The asymptotic regime under which the theory holds assumes the effect of the prior is washed away for a fixed n.  Is it a meaningful assumption for small n?  Next, this makes the MFM connection look a bit artificial -  why cannot we simply start with a non-informative prior for $P^{(N)}$ since we are anyway letting the mixture collapse to 1 component and the base to Haar? Is there anything special about this MFM formulation that makes the limit ETEL?  }\\

\textbf{Reply:} One salient feature of the MFM prior \ref{eqn:npbayes3}-\ref{eqn:npbayes2} along with our special asymptotic regime \ref{eqn:assumptions} is that, through the prior specification on $k$, it strongly prefers small number of unique atoms drawn from the base measure $\mbox{H}^{(N)}$. This effect does not wash away with fixed $n$ - on the contrary $k\mid x_{1:n}\to n$ \emph{almost surely}. 

\textcolor{red}{DP:} It is important to point out that the asymptotic regime allows the effect of the prior $P^{(N)}$ to get washed away for any finite $n$ so that the resulting posterior distribution of $P^{(N)}$ converges to the empirical distribution subject to the distributional constraint. The induced posterior distribution on $\theta$ then converges to the posterior obtained from the empirical likelihood.  Although in principle, the mixture of finite mixture prior for $P^{(N)}$ can be replaced any other distribution whose effect is allowed to weaken under the asymptotic regime, the MFM construct is a natural choice that allows the $P^{(N)}$ to asymptotically be degenerate at the observed data points. 

Ever since \cite{doi:10.1080/01621459.2016.1255636} established that mixture of finite mixtures (MFM) consistently estimate the number of clusters a posteriori, and enjoys a convenient stick-breaking representation if the prior on the number of clusters is properly chosen, it has been adopted to many popular statistical models where Dirichlet process prior is traditionally used, e.g, stochastic block models \cite{MFM-SBM, Ex-SBM}, Markov random fields \cite{MRF-MFM}, homogeneity pursuit regression \cite{Homogeneity}, to name a few. We envision that our theoretical results can potentially open up  new ways to device targeted and computationally efficient inference procedures on specific parameters of interest in complicated statistical models, where non-parametric Bayes methods are typically used.

$\mathbf{(2)}$ \emph{In npBayes, we typically need to choose a dispersion parameter which gives an idea of dispersion around a base measure.  It has a clear statistical interpretation.  The analogous quantity we have here is $\varepsilon$ which controls departure from $F_\theta$ in $W_{AR}$ metric which we do not have a good intuition about.  We mentioned ELPD based model averaging, but is the model averaged quantity meaningful in a statistical sense? Note that our basic motivation for using ETEL over a Gibbs type distribution is the probability interpretation that goes with it.  Seems like the ELPD based model averaging is taking us back to these Gibbs posteriors?}\\

\textbf{Reply:}  The lack of valid probabilistic interpretation of Gibbs type  posteriors poses a considerable challenge in  tuning the associated temperature parameter, and it has attracted a proliferation of excellent literature \cite{martin2019, safebayes} in recent past.  The basic motivation for using ETEL over a Gibbs type posterior is indeed the probability interpretation that goes with it. We device  a ELPD based model averaging to tune the hyper parameter $\varepsilon$. From the definition of ELPD, we can interpret it as a measure of the extent of unequal weighting of the observations. In the presence of contamination, our approach of selecting the hyper-parameter promotes  unequal weighting of the observations to ensure  -- under weighting of outlying observations, and over-weighting observations around the ``center". The inbuilt mechanism of ensuring immunity against outliers while maintaining  a valid generative model interpretation is what sets our method apart from lot of the existing pseudo-likelihood based approaches.

$\mathbf{(3)}$ \emph{Finally the usual theory comments, like can we show semi-parametric efficiency under a proper robust framework (e.g. allowing a contaminated data generating model), how robust the method is under misspecification of the contamination model? e.g. we are forcing departure from $F_\theta$ in $W_{AR}$, what if the true data is generated from contaminated $F_\theta$ under some other metric (you have partially addressed this in the sims, may be better to say it out loud)? }\\

\textbf{Reply:} (I am first referring to an email by Dr. Pati in response to a query raised in BNP13. Could we elaborate on this?) \textcolor{blue}{Showing robustness is a relevant question - there is existing literature on finding minimax rates in $\epsilon$-contamination model: \cite{epsiloncontamination}, which is somewhat related.  This is standalone theoretical problems on their own, which would require years of work.  Even framing the right problem is tricky here.  The least we can try to do at some point  is to see theoretically whether centering on a parametric family is better than the M-estimation restriction as in standard EL.} In numerical studies in section \ref{Robust_Bayes}, we empirically observe that, $\mbox{D}$-BETEL is more resistant towards the presence of outliers compared to standard Bayesian approaches. However the relative advantages wear off, as expected, as the extent of contamination in the data generation mechanism increases.



\bibliography{paper-ref}
%\bibliographystyle{apalike}
\bibliographystyle{plainnat}
%\bibliographystyle{biometrika}

\end{document}
