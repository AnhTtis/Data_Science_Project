\documentclass[11pt]{article}
\usepackage[left=1in,top=1in,right=1in,bottom=1in,head=.1in,nofoot]{geometry}

\setlength{\footskip}{24pt} % Page number/footer spacing
\usepackage{setspace,url,bm,amsmath} % For double-spacing, URL font, math symbols

\usepackage{titlesec} % Section header formatting
\titlelabel{\thetitle.\quad} % Section header formatting
%\titleformat*{\section}{\bf\large\center\uppercase} % Section header formatting

\usepackage[margin=20pt]{subcaption}



%% Packages
\RequirePackage{amsthm,amsmath,amsfonts,amssymb}
% \RequirePackage[numbers]{natbib}
\RequirePackage[authoryear]{natbib}%% uncomment this for author-year citations
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}%% uncomment this for coloring bibliography citations and linked URLs
\RequirePackage{graphicx}%% uncomment this for including figures
\RequirePackage{mathrsfs}  
% \RequirePackage{times}
%\usepackage[cmbold]{mathtime}
\RequirePackage{bm}
% \RequirePackage{natbib}
\RequirePackage{graphicx}
\RequirePackage{graphics}
\RequirePackage{multicol}
\RequirePackage{multirow}
\RequirePackage{enumitem}
\RequirePackage{booktabs}
\RequirePackage[table,xcdraw]{xcolor}
%\graphicspath{{./art/}}
\usepackage{indentfirst}
\setlength{\parindent}{2em} 
\usepackage{authblk}

\usepackage{appendix}
\RequirePackage[plain,noend]{algorithm2e}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{condition}{Condition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Assumption, Definition, Example,     %%
%% Notation, Property, Remark, Fact         %%
%% use \theoremstyle{remark}                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{remark}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}
\newtheorem{problem}{Problem}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please put your definitions here:        %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\T{{ \mathrm{\scriptscriptstyle T} }}
\def\v{{\varepsilon}}
\newcommand{\bbE}{{\mathbb{E}}}
\newcommand{\bbV}{{\text{var}}}
%\newcommand{\bbP}{{P}}
\newcommand{\cov}{{\text{cov}}}

\newcommand{\bx}{{\bf x}}
\newcommand{\ba}{{\bf a}}
\newcommand{\bv}{{\bf v}}
\newcommand{\bu}{{\bf u}}
\newcommand{\bw}{{\bf w}}
\newcommand{\bC}{{\bf C}}
%\newcommand{\bW}{{\bf W}}
\newcommand*{\QEDA}{\hfill\ensuremath{\blacksquare}}
\newcommand{\argmin}{{\operatorname{arg\,min}}}
\newcommand{\argmax}{{\operatorname{arg\,max}}}

\newcommand{\bZero}{{\boldsymbol 0}}
\newcommand{\bOne}{{\boldsymbol 1}}

\newcommand{\bB}{{\boldsymbol B}}
\newcommand{\bD}{{\boldsymbol D}}
\newcommand{\bE}{{\boldsymbol E}}
\newcommand{\bF}{{\boldsymbol F}}
\newcommand{\bI}{{\boldsymbol I}}
\newcommand{\bR}{{\boldsymbol R}}
\newcommand{\bS}{{\boldsymbol S}}
\newcommand{\bX}{{\boldsymbol X}}
\newcommand{\bY}{{\boldsymbol Y}}
\newcommand{\bV}{{\boldsymbol V}}
\newcommand{\bW}{{\boldsymbol W}}
\newcommand{\bU}{{\boldsymbol U}}
\newcommand{\bZ}{{\boldsymbol Z}}
\newcommand{\bZk}{{\boldsymbol Z_k}}

\newcommand{\bbeta}{{\boldsymbol \beta}}
\newcommand{\bmu}{{\boldsymbol \mu}}
\newcommand{\bepsilon}{{\boldsymbol \epsilon}}
\newcommand{\blambda}{{\boldsymbol \lambda}}
\newcommand{\bSigma}{{\boldsymbol \Sigma}}
\newcommand{\bLambda}{{\boldsymbol \Lambda}}
\newcommand{\bGamma}{{\boldsymbol \Gamma}}

\newcommand{\cB}{{\mathcal B}}
\newcommand{\cD}{{\mathcal D}}
\newcommand{\cP}{{\mathcal P}}
\newcommand{\cR}{{\mathcal R}}
\newcommand{\cS}{{\mathcal S}}
\newcommand{\cT}{{\mathcal T}}
\newcommand{\cU}{{\mathcal U}}
\newcommand{\cW}{{\mathcal W}}


\newcommand{\mW}{{\mathscr W}}
\newcommand{\mR}{{\mathscr R}}
\newcommand{\mS}{{\mathscr S}}
\newcommand{\mU}{{\mathscr U}}
\newcommand{\mT}{{\mathscr T}}

\newcommand{\bbI}{{\mathbb I}}
\newcommand{\bbP}{{\mathbb P}}
\newcommand{\bbR}{{\mathbb R}}

\def\bs{\boldsymbol}
\def\red{\color{red}}
\def\blue{\color{blue}}
\def\gre{\color{green}}
\def\cyan{\color{cyan}}


\begin{document}

\title{\bf Bayesian criterion for Re-randomization}
\author[1]{Zhaoyang Liu$^{*}$}
\author[1]{Tingxuan Han\footnote{These authors have contributed equally to this work.}}
\author[2]{Donald B. Rubin}
\author[1]{Ke Deng\footnote{Corresponding author: kdeng@tsinghua.edu.cn}}
\affil[1]{Center for Statistical Science \& Department of Industrial Engineering, Tsinghua University}
\affil[2]{Yau Mathematical Sciences Center, Tsinghua University}
\date{}
\renewcommand\Authands{ and }

\maketitle


\begin{abstract}
{
Re-randomization has gained popularity as a tool for experiment-based causal inference due to its superior covariate balance and statistical efficiency compared to classic randomized experiments. 
However, the basic re-randomization method, known as ReM, and many of its extensions have been deemed sub-optimal as they fail to prioritize covariates that are more strongly associated with potential outcomes.
To address this limitation and design more efficient re-randomization procedures, a more precise quantification of covariate heterogeneity and its impact on the causal effect estimator is in a great appeal.
This work fills in this gap with a Bayesian criterion for re-randomization and a series of novel re-randomization procedures derived under such a criterion.
Both theoretical analyses and numerical studies show that the proposed re-randomization procedures under the Bayesian criterion outperform existing ReM-based procedures significantly in effectively balancing covariates and precisely estimating the unknown causal effect.
}
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Main text entry area:
\section{Introduction}\label{sec:intro}

%concept of re-randmoization 
Covariate imbalance is to be avoided in randomized experiments, even though complete randomization balances all potential confounding factors on average and therefore becomes the ``gold standard'' in causal inference \citep{Holschuh1980,Wu1981,Urbach1985,Imai2008,Cox2009,Morgan2012}.
A natural way to avoid treatment assignment allocations with unbalanced covariates is to reject a ``bad'' allocation, and redo the randomization until a ``good'' allocation with properly balanced covariates is obtained, before the experiment is conducted. 
This is called \textit{re-randomization}, which was first suggested by \cite{fisher1992arrangement}. 
The general framework including the sufficient condition to guarantee unbiased estimation and related benefits from doing re-randomization when using Mahalanobis distance as the metric to measure imbalance (referred to as ReM) is provided in \cite{Morgan2012}.

%extensions & theoretical properties of ReM
Subsequent to the initial work, relevant literatures have shown extended applications and theoretical properties of ReM. 
Unequal weights on covariates are considered using \emph{ReM in tiers of covariates} (ReM$_T$) in \cite{Morgan2015}, where covariates are partitioned into tiers according to their relative importance, with a tighter threshold of ReM for covariates in a  more important tier. 
%Partitioning covariates into tiers is especially beneficial when the number of covariates is large and the sample size is limited. 
%The more important a particular tier of covariates is, the tighter the threshold of ReM within this tier should be. 
After re-randomization in one tier, residuals orthogonal to all previous tiers of covariates can be used to calculate Mahalanobis distance in the next tier in ReM$_T$, leading to reduced computational complexity and refined statistical efficiency of ReM when a large number of covariates are involved. 
%ReM$_T$ can save computational time, and refine the statisitical efficiency of re-randomization.
Applications of ReM in factorial and sequential designs are analyzed in \cite{Branson2016} and \cite{Zhou2018}. 
Asymptotic theory for the standard difference-in-means estimator of treatment effect under ReM is derived by an orthogonal decomposition of the final sampling distribution as a linear combination of a Gaussian random variable and a truncated Gaussian random variable \citep{Li2018,Li2020}. 
The asymptotic performance of ReM in stratified randomization and survey experiments are studied in \cite{wang2021rerandomization} and \cite{yang2021rejective}.
\cite{zhang2021pca} recommended applying ReM for the top-$k$ principal components of covariates only, leading to a variate of ReM named PCA-ReM, which reduces computational complexity of ReM significantly while retaining much of its benefits. 
Ridge-ReM proposed by \cite{Branson2021} utilizes modified Mahalanobis distance to deal with collinearities among covariates in situations with high-dimensional or highly-correlated covariates. 
Considering that setting a small acceptance probability will lead to heavy computational burden, \cite{zhu2022pair} proposed a pair-switching strategy to reduce the computational cost of ReM.

%alternative methods
Beyond the ReM-based approaches, alternative strategies for re-randomization based on other criteria have been proposed as well.
\cite{Kallus2018} proposed an optimal allocation  based on a minimax criterion, which remains controversial, as discussed in \cite{Johansson2020} and \cite{Kallus2020}. 
To tackle with this problem, \cite{wang2022rerandomization} proposed a procedure allowing the covariate imbalance to diminish as sample size increases.
On the other hand, \cite{Li_2021kde} suggested to estimate the density function of covariates in the treatment group and the control group via kernel density estimation, and measure the degree of covariate imbalance by comparing the estimated density functions.

%limitation of the exisiting mehods
% {\red 
On the other hand, however, all these existing re-randomization methods are sub-optimal in sense that they fail to prioritize covariates that are more strongly associated with potential outcomes, while there is a clear insight that covariates associated with the potential outcomes more closely should enjoy higher priority to be balanced.
Although ReM$_T$ considers covariate heterogeneity by partitioning the covariates into tiers and assigning higher priority to balancing covariates in the leading tiers, such a solution is incomplete because it treats covariates in the same tier equally. 
% We would have a better solution if we can quantify the heterogeneity of covariates more precisely and make it clear how the heterogeneity affects the casual effect estimator.
A more precise quantification of covariate heterogeneity and its impact on the causal effect estimator would lead to a better solution.

%Motivation of ReB
This study fills in this gap by establishing a Bayesian criterion for re-randomization, which formulates our knowledge on the relative importance of different covariates to the potential outcomes via a prior distribution, and derives a novel re-randomization procedure referred to as ReB.
%that is more efficient than the ReM-based ones.
We found that ReB 
%the re-randomization procedure derived under the Bayesian criterion (referred to as ReB) 
takes many existing ReM-based re-randomization procedures, e.g., ReM, ReM$_T$, PCA-ReM and Ridge-ReM, as special cases, forming a unified framework for studying various re-randomization approaches.
%, ReB establishes a unified framework .
Theoretical analyses show that the proposed ReB outperforms the ReM-based re-randomization procedures in terms of getting a more accurate causal effect estimator, as long as the prior distribution is informative to highlight the relative importance of different covariates.

%two-stage ReB
In case that no such a nice informative prior distribution is available, we propose a two-stage strategy to implement ReB, where a small pilot experiment is conducted in the first stage to establish the informative prior distribution we need, and ReB is conducted for the majority of samples in the second stage.
The final causal effect estimator can be obtained by properly integrating the two estimators obtained in both stages.
Theoretical analyses and simulation studies show that the two-stage ReB procedure also achieves superior covariate balance and thereby more precise estimation of causal effects than existing methods.
In the literature, the idea of using pre-experimental data in re-randomaization has been studied. 
For example, \cite{johansson2020rerandomization} proposed a rank-based balance measure for re-randomization and weight for each covariates was estimated using pre-experimental data; 
\cite{zhang2021response} introduced the response-adaptive design for re-randomization and took ethics into consideration. 
Compared to the proposed two-stage ReB procedure, however, these two methods do not make full use of the information obtained from the pre-experiment and suffer from the lack of theoretical guarantees.
% }

%paper structure
The following part of this paper is organized as follows. 
Section \ref{sec:Preliminary} defines the basic notations and briefly reviews various re-randomization criteria based on the Mahalanobis distance in the literature. Repeated sampling properties of  re-randomization are also included. 
Section \ref{sec:ReB} proposes the Bayesian criterion for re-randomization, establishes the corresponding theoretical properties and discusses its connections to other re-randomization criteria in the literature. Section \ref{sec:Two-stage} introduces the framework of a two-stage Bayesian re-randomization procedure and gives its asymptotic properties. 
Section \ref{sec:Simulation} and \ref{sec:RealDataAnalysis} support the proposed methodology with simulation studies and real data applications.
% Real data applications are provided in Section~\ref{sec:RealDataAnalysis} to demonstrate the strength of proposed approaches in practice. 
Finally, we conclude with a discussion in Section \ref{sec:Discussion}.
Detailed proofs of theoretical results and additional simulation results
% and simulations 
are provided in the Supplementary Material.


\section{Notation and preliminaries}\label{sec:Preliminary}
%basic causal model
Following the classic setting for causal inference in \cite{Rubin1974} and in \cite{Imbens2015}, we consider statistical inference for the \emph{average causal effect} (ACE) of an \emph{active treatment} $t$ with respect to a \emph{control treatment} $c$ over a finite population of units $\cP=\{1,\cdots,N\}$, i.e.,
\begin{equation}\label{eq:ACE}
    \tau=\frac{1}{N}\sum_{i=1}^N\tau_i=\frac{1}{N}\sum_{i=1}^N\big(Y_i(1)-Y_i(0)\big),   
\end{equation}
in a randomized experiment, where $Y_i(1)$ and $Y_i(0)$ are the two potential outcomes of unit $i\in\cP$ under treatment $t$ and control $c$ respectively, and $\tau_i=Y_i(1)-Y_i(0)$ is the unit-level causal effect.

%Basic elements of a case-control experiment
Let $W_i$ be the \emph{treatment assignment indicator} of unit $i$, where 
\begin{equation*}%\label{eq:AssignmentIndicator}
    W_{i}=\begin{cases}
1, & \mbox{if unit $i$ is assigned to receive the active treatment $t$,}\\
0,& \mbox{if unit $i$ is assigned to receive the control treatment $c$,}
\end{cases}
\end{equation*}
and $\bW=(W_1,\cdots,W_N)^T$ be the \emph{treatment assignment vector} for the experiment. 
In a randomized experiment with $N_t$ and $N_c$ being the group size of the treatment group and control group respectively (apparently $N_t+N_c=N$), the treatment assignment vector arises from a random sample from the space:
\begin{equation}%\label{eq:AssignmentSpace}
    \mW=\left\{\bW\in\{0,1\}^N: \sum_{i=1}^N W_i=N_t\right\},
\end{equation}
leading to the following unbiased estimator of  the causal estimand $\tau$ under Stable Unit Treatment Value Assumption (SUTVA) introduced by \cite{Rubin1980}:
\begin{equation}\label{eq:NeymanEstimtor}
    \hat\tau(\bW)=\frac{1}{N_t}\sum_{i=1}^NY_i(1)\cdot W_i-\frac{1}{N_c}\sum_{i=1}^NY_i(0)\cdot(1-W_i).
\end{equation}
% with 
% \begin{equation}\label{eq:NeymanEstimtor_Mean&Variance}
% E(\hat\tau)=\tau,\ var(\hat{\tau})=S_c^2/N_c+S_t^2/N_t-S_{tc}^2/N,
% \end{equation}
% where
% $$S_t^2=\frac{\sum_{i=1}^N(Y_i(1)-\bar{Y}(1))^2}{N-1},\quad S_c^2=\frac{\sum_{i=1}^N(Y_i(0)-\bar{Y}(0))^2}{N-1}\quad \mbox{and}\quad S_{tc}^2=\frac{\sum_{i=1}^N(\tau_i-\tau)^2}{N-1}$$
% are the sample variances of $Y_i(0)$, $Y_i(1)$ and the unit-level treatment effect respectively.

\subsection{Framework of re-randomization}
%ReM
Further assume that unit $i$ is associated with a group of covariates $\bX_{i}=(X_{i1},\cdots,X_{ip})^T$, which may predict the potential outcomes $(Y_i(1),Y_i(0))$, and thus the causal effect $\tau_i$. 
Let $\bX=(X_{ij})_{1\leq i\leq N,1\leq j\leq p}$ be the covariate matrix of the $N$ units in the population of interest.
%, with $\bX_{\cdot j}=(X_{1j},\cdots,X_{nj})^T$ being the vector of the $j$-th covariate. 
A re-randomization procedure, which can be traced back to R.A. Fisher as pointed out by \cite{Morgan2012}, defines a deterministic \emph{acceptance rule}
\begin{equation}\label{eq:AcceptanceRule}
\phi(\boldsymbol{X}, \boldsymbol{W})=\begin{cases}
1, & \mbox{if assignment $\bW$ is acceptable,}\; \\
0,& \mbox{otherwise,} \;
\end{cases}
\end{equation}
to reject ``bad'' assignments (i.e., ones making $\phi(\bX,\bW)=0$),
until a plausible assignment $\bW$ satisfying $\phi(\bX,\bW)=1$ is obtained. 
The acceptance rule $\phi(\bX,\bW)$ defines a subset of  acceptable treatment assignments $\mW_{\phi}\subseteq \mW$, which is referred to as the \emph{acceptance region} of procedure $\phi$.

% Re-randomization aims to reduce covariate imbalance, thereby leading to more efficient estimation of $\tau$ (i.e., with reduced sampling variance).
% To guarantee the unbiasedness of the causal estimator and the sufficient randomness of the treatment assignment, \cite{Morgan2012} pointed out that an acceptance rule $\phi$ for re-randomization should satisfy the following conditions:
% \begin{description}
%     \item[$\quad(C_1)$] the symmetry condition, i.e., if assignment $\mathbf{W} \in \mW_\phi$, then $\mathbf{1}-\mathbf{W} \in \mW_\phi$;
%     \item[$\quad(C_2)$] the acceptance-rate condition, i.e., $\bbP(\phi=1)\geq \alpha$ for a fixed acceptance rate $\alpha\in(0,1)$.
% \end{description}
% The symmetry condition $C_1$ guarantees that 
% $$\bbE(\hat\tau (\bW)\mid\phi=1)=\tau,$$
% i.e., $\hat\tau (\bW)$ is an unbiased estimator of $\tau$ under the re-randomization procedure.
% The acceptance-rate condition $C_2$ ensures that the acceptance region $\mW_\phi$ contains ``enough" allocations to support appropriate statistical inference.
% For a fixed acceptance rate $\alpha\in(0,1)$, we define $\Phi_\alpha$ to be the family of acceptance rules satisfying the above two conditions for a fixed $\alpha$.
%, i.e., all plausible re-randomization mechanisms with minimum acceptance rate $\alpha$. The study of re-randomization concerns on selecting an appropriate re-randomization mechanism, i.e., an acceptance rule $\phi$, from $\Phi_\alpha$ that can improve the efficiency of statistical inference of causal effect $\tau$.
%Considering that each acceptance rule $\phi\in\Phi_\alpha$ corresponds to an acceptance region $\mW_\phi\subseteq\mW$, $\Phi_\alpha$ induces a family of acceptance regions denoted by $\mR_\alpha$. Since there exists an one-to-one mapping between $\Phi_\alpha$ and $\mR_\alpha$, we may transfer between the two notations without further notice hereinafter.

\cite{Morgan2012} suggested specifying the re-randomization procedure $\phi$ by controlling the Mahalanobis distance between
$$\bar{\bX}_t=\frac{1}{N_t}\sum_{i=1}^N\bX_{i}W_i\quad\mbox{and}\quad\bar{\bX}_c=\frac{1}{N_c}\sum_{i=1}^N\bX_{i}(1-W_i),$$
which is defined as
\begin{equation}\label{eq:dM}
    d_M \triangleq \bD^T\bSigma_\bD^{-1}\bD
\end{equation}
with $\bD=\bar{\bX}_t-\bar{\bX}_c$ and $\bSigma_\bD=\operatorname{cov}\left(\bD\right)$ being the covariance matrix of $\bD$,
leading to the following \emph{re-randomization using the Mahalanobis distance} (ReM):
\begin{equation}
    \phi_M(\bX,\bW)=I\big(\bD^T\bSigma_\bD^{-1}\bD\leq a\big),
\end{equation}
where $I(\cdot)$ is the 0-1 indicator function, and the threshold $a$ can be determined based on the equation below for each specific acceptance rate $\alpha\in(0,1)$:
\begin{equation}
    \bbP(\phi_M=1)=\bbP\big(\bD^T\bSigma_\bD^{-1}\bD\leq a\big)=\alpha.
\end{equation}
%Because
%$$d_M \triangleq\bD^T\bSigma_\bD^{-1}\bD,$$
%the key element of the acceptance rule $\phi_M$, is the Mahalanobis distance between $\bar{\bX}_T$ and $\bar{\bX}_C$, they referred to $\phi_M$ as the \emph{re-randomization with Mahalanobis distance} or ReM.
%Denote the mean difference of covariates as $\boldsymbol{D}=\bar{\boldsymbol{X}}_T-\bar{\boldsymbol{X}}_C$.
%Mahalanobis distance has been used to construct the decision rule with $M \equiv\boldsymbol{D}^{\prime}\operatorname{cov}\left(\boldsymbol{D}\right)^{-1}\boldsymbol{D}$. 
%There are two main advantages of using $M$ as a measurement for decision.
% Further theoretical analysis shows that $d_M$ is invariant to affine transformations of $\bX$ and asymptotically follows $\chi_p^2$, the chi-square distribution with degrees of freedom $p$, in a completely randomized experiment. 
% Therefore, we have $a=\xi_{\alpha,p}$, the $\alpha$-quantile of $\chi^2_p$, resulting in the following acceptance rule for ReM with acceptance rate $\alpha$: 
% \begin{equation}\label{eq:ReM_AcceptanceRule}
%     \phi_M(\bX,\bW)=I(d_M\leq \xi_{\alpha,p}).
% \end{equation}
% Because $\phi_M$ improves the balance of covariates $\bX$ in a randomized experiment by rejecting ``bad" treatment allocations that lead to large Mahalanobis distance of $\boldsymbol{D}$, ReM 
%leading to an unbiased estimator of causal effect $\tau$, namely
Denote $(\hat\tau\mid\phi_M=1)$ as the difference-in-mean estimator of $\tau$ under the re-randomization mechanism $\phi_M$.
\cite{Morgan2012} showed that $(\hat\tau\mid\phi_M=1)$ is an unbiased estimator of $\tau$ satisfying
$$\bbE(\hat\tau\mid\phi_M=1)=\tau,$$
as long as $N_t=N_c$;
and, under the assumption that $\bX$ follows a multi-variate Gaussian distribution, $(\hat\tau\mid\phi_M=1)$ is a more efficient estimator of $\tau$ that the classic $\hat\tau$ under a completely randomized experiment with the following \emph{percent reduction in variance} (PRIV):
%under a linear regression model:
\begin{equation}\label{eq:VarianceReduction_ReM}
    PRIV=1-\frac{\bbV(\hat\tau\mid\phi_M=1)}{\bbV(\hat\tau)}=100\times(1-v_{\alpha,p})R^2,
\end{equation}
where $R^2$ represents the squared multiple correlation between the observed potential outcomes and the covariates within each treatment group, and
\begin{equation}\label{eq:v_AlphaP}
v_{\alpha,p}\triangleq \frac{\bbE(\chi^2_p\mid\chi^2_p\leq\xi_{\alpha,p})}{p} = \frac{\bbP(\chi^2_{p+2}\leq \xi_{\alpha,p})}{\bbP(\chi^2_p\leq \xi_{\alpha,p})}=\frac{\bbP(\chi^2_{p+2}\leq \xi_{\alpha,p})}{\alpha},
\end{equation}
with $\chi^2_p$ being a chi-square random variable with degree of freedom $p$ and $\xi_{\alpha,p}$ being the $\alpha$-quantile of $\chi^2_p$.
% \cite{Li2018} showed that the asymptotic distribution of $\hat{\tau}$ is a linear combination of a Gaussian random variable and a truncated Gaussian random variable under mild conditions. 

%ReM-T
Considering the computational challenges in balancing a large number of covariates where different covariates are unequally important for estimating $\tau$,
\cite{Morgan2015} proposed partitioning covariates into different tiers based on their relative importance, and then balancing them in a sequential fashion, with covariates in leading tiers enjoying higher priority to be balanced. 
\cite{Morgan2015} showed that ReM$_T$ enjoys similar theoretical properties as ReM with improved computational efficiency. 

%PCA-ReM
Alternatively, \cite{zhang2021pca} proposed balancing the top-$k$ principal components of $\bX$ only via ReM, leading to the following re-randomization mechanism referred to as the PCA-ReM:
\begin{equation}\label{eq:PCAReM_AcceptanceRule}
    \phi_{M}^{(k)}=I\big(d_{M}^{(k)}\leq a\big),
\end{equation}
where $d_{M}^{(k)}$ is the Mahalanobias distance between $\bar\bX_t^{(k)}$ and $\bar\bX_c^{(k)}$ with $\bar\bX_t^{(k)}$ and $\bar\bX_c^{(k)}$ denoting the mean vector of the top $k$ principal components of $\bX$ in the treatment and control group respectively.
% Similar to $d_M$ in ReM, $d_M^{(k)}$ in PCA-ReM asymptotically follows $\chi^2_k$, suggesting that $a=\xi_{\alpha,k}$ in this case in order to keep the acceptance rate of $\phi_M^{(k)}$ fixed to $\alpha$.
\cite{zhang2021pca} showed that PCA-ReM performs better than complete randomization and enjoys faster computation than ReM when $k<p$.

%Ridge-ReM
To stabilize the numerical calculation of $\bSigma_\bD^{-1}$ when $\bSigma_\bD$ is close to a singular matrix, \emph{ridge re-randomization} (Ridge-ReM) was proposed by \cite{Branson2021}, which inflates eigenvalues of $\bSigma_\bD$ in $d_M$ with a positive real number $\lambda$, leading to the alternative re-randomization procedure below: 
\begin{equation}\label{eq:RidgeReM_AcceptanceRule}
    \phi_{R,\lambda}(\bX,\bW)=I\big(\bD^T(\bSigma_\bD+\lambda\bI_p)^{-1}\bD\leq a\big).
\end{equation}
\cite{Branson2021} showed that Ridge-ReM $\phi_{R,\lambda}$ performs similarly as ReM $\phi_M$ does in terms of improving the covariance balance with improved numerical stability, and the parameter $\lambda$ can be optimized in pursuit of a causal effect estimator with smallest sampling variance.

% Framework of asymptotic analysis
\subsection{Repeated sampling inference under re-randomization}\label{subsec:asymptotic}
%As we mainly focus on the general sampling distribution of difference-in-means estimator for average treatment effect, some notations and useful conclusions for repeated sampling inference are introduced here.
Although Gaussian assumption of $\bX$ was made in the original work of \cite{Morgan2012} to motivate ReM, we do not rely on such a restrictive assumption to establish the general methodology of re-randomization.
In fact, for a specific re-randomization mechanism $\phi$, the asymptotic behavior of $(\hat\tau\mid\phi=1)$ can be well established under much milder conditions by a series of finite population central limit theorems.

% According to the finite population central limit theorem (i.e., Theorem 5)
Under complete randomization, Theorem 3 in \cite{Li2017} implies that  $\sqrt{N}\big(\hat{\tau}-\tau,\bm{D}^T\big)$ has the following covariance matrix: 
\begin{equation}
    \bm V_N = \left(\begin{array}{cc}
        V_{\tau\tau,N} & \bm V_{\tau x,N}  \\
        \bm V_{x\tau,N} & \bm V_{xx,N} 
    \end{array}\right)
= \left(\begin{array}{cc}
r_1^{-1}S^2_{Y(1)}+r_0^{-1}S^2_{Y(0)}-S^2_{\tau}\quad\quad & r_1^{-1}\bm S^2_{Y(1),\bm X}+r_0^{-1}\bm S^2_{Y(0),\bm X}  \\
r_1^{-1}\bm S^2_{\bm X,Y(1)}+r_0^{-1}\bm S^2_{\bm X,Y(0)}\quad & \bm (r_0r_1)^{-1}\bm S^2_{\bm X} 
\end{array}\right),
\end{equation}
where $r_0=N_c/N$ and $r_1=N_t/N$, finite-population variance and covariance matrices  $S^2_{Y(z)} = (N-1)^{-1} \sum_{i=1}^N \big(Y_i(z)-\bar{Y}(z)\big)^2,\quad S^2_{\tau} = (N-1)^{-1} \sum_{i=1}^N (\tau_i-\tau)^2, \quad \bm S^2_{\bm X} = (N-1)^{-1} \sum_{i=1}^N \big(\bm X_i-\bar{\bm X}\big)\big(\bm X_i-\bar{\bm X}\big)^T,\quad \bm S^2_{Y(z),\bm X}=\bm S^2_{\bm X,Y(z)} =
    (N-1)^{-1}\sum_{i=1}^N \big(Y_i(z)-\bar{Y}(z)\big)\big(\bm X_i-\bar{\bm X}\big)^T$, $z=0$ or $1$. 
% {\red [comment to Tingxuan: is this a trivial result? do we need a reference for this result?]}
Moreover, according to finite population central limit theorem, i.e., Theorem 5 in \cite{Li2017}, when Condition \ref{cond1} below is satisfied, $\sqrt{N}\big(\hat{\tau}-\tau,\bm{D}^T\big)$ will converge to a Gaussian distribution weakly as the sample size $N$ approaches infinity, i.e.,
\begin{equation}\label{eq:AssymptoticDistribution4HatTau_bD}
    \sqrt{N}\big(\hat{\tau}-\tau,\bm{D}^T\big) \xrightarrow{d} N(\bm 0,\bm V_{\infty}),
\end{equation}
where $\bm V_{\infty}$ is the limit of finite population covariance matrix $\bm V$, i.e.,
\[\begin{split}
    \bm V_{\infty} =\lim_{N\rightarrow \infty} \bm V_N =  \left(\begin{array}{cc}
        V_{\tau\tau,\infty} & \bm V_{\tau x,\infty}  \\
        \bm V_{x\tau,\infty} & \bm V_{xx,\infty} 
    \end{array}\right).
\end{split}\]
%when Condition \ref{cond1} is satisfied.
\begin{condition}\label{cond1}
As $N \rightarrow \infty$, for $z=0,1$,
(1) $r_z$ has positive limits; (2) the finite population variances and covariances $S^2_{Y(z)}$, $S^2_{\tau}$, $\bm S^2_{\bm X}$ and $\bm S^2_{\bm X,Y(z)}$ have finite limiting values, and the limit of $\bm S^2_{\bm X}$ is nonsingular; and (3) $\max_{1\leq i \leq N} |Y_i(z)-\bar{Y}(z)|^2/N \rightarrow 0$ and $\max_{1\leq i \leq N} ||\bm X_i-\bar{\bm X}||^2/N \rightarrow 0$.
\end{condition}

% One step further, we discuss the asymptotic properties of covariate balance $\bm D$ and the difference-in-means estimator of causal effect $\hat{\tau}$.  
Moreover, \cite{Li2018} has established more detailed asymptotic results for a special family of re-randomization mechanisms whose acceptance rule depends on $\sqrt{N}\bm D$ and $\bm V_{xx,N}$ only.
Denote $\phi(\sqrt{N}\bm D,\bm V_{xx,N})$ as such a re-randomization mechanism.
Let $\Phi$ be the family of all re-randomization mechanisms that is of the form $\phi\big(\sqrt{N}\bm D,\bm V_{xx,N}\big)$ and satisfies the Condition \ref{cond2} below.
%For simplicity, we adopt the notation in \cite{Li2018} and write the re-randomization criterion as $\phi(\sqrt{N}\bm D,\bm V_{xx})$, which means we only use the information in $\sqrt{N}\bm D$ and $\bm V_{xx}$.
Let $\mathcal{B}_{\phi} = \{\bm\mu:\phi(\bm\mu,\bm V_{xx,N})=1\}$ be the acceptance region for $\sqrt{N}\bm D$ induced by $\phi$, and $\mathcal{B}_{\phi,\infty} = \{\bm\mu:\phi(\bm\mu,\bm V_{xx,\infty})=1\}$ be the limit of $\mathcal{B}_{\phi}$. 
%Furthermore, the balance criterion $\phi$ of re-randomization under consideration should satisfy the following condition:
%Denote the set of balance criteria satisfying condition \ref{cond2} and of the form $\phi(\sqrt{N}\bm D,\bm V_{xx})$ as $\Phi$. 
\cite{Li2018} proved that 
\begin{equation}\label{eq:Asymptotics4Phi}
    \left.\left(\begin{array}{c}
        \sqrt{N}(\hat{\tau}-\tau)  \\
        \sqrt{N}\bm D 
    \end{array}\right)\right|\sqrt{N}\bm D \in \mathcal{B}_{\phi} \xrightarrow{d} 
    \left.\left(\begin{array}{c}
        A_{\infty}  \\
        \bm B_{\infty} 
    \end{array}\right)\right|\bm B_{\infty} \in \mathcal{B}_{\phi,\infty},\ \forall \phi \in \Phi, 
\end{equation}
where $(A_{\infty},\bm B_{\infty}^T) \sim N(\bm 0, \bm V_{\infty})$.
% Since 
% \[\begin{split}
%     A_{\infty}|\bm B_{\infty} \sim N(\bm V_{\tau x,\infty}\bm V_{xx,\infty}^{-1}\bm B_{\infty},V_{\tau\tau,\infty}-\bm V_{\tau x,\infty}\bm V_{xx,\infty}^{-1} \bm V_{x\tau,\infty}) \sim \epsilon + \bm V_{\tau x,\infty}\bm V_{xx,\infty}^{-1}\bm B_{\infty},
% \end{split}\]
\begin{condition}\label{cond2}
The covariate balance criterion $\phi(\cdot,\cdot)$ satisfies:
(1) $\phi(\cdot,\cdot)$ is almost surely continuous; (2) for $\bm B\sim N(0,\bm U)$, $\bbP(\phi(\bm B, \bm U)=1)>0$ for any $\bm U>0$, and $\bbV(\bm B|\phi(\bm B, \bm U)=1)$ is a continuous function of $\bm U$; (3) $\phi(\bm\mu,\bm U)=\phi(-\bm\mu,\bm U)$ for all $\bm \mu$ and $\bm U>0$.
\end{condition}

The symmetry condition for $\phi$ in Condition \ref{cond2} guarantees that $\hat\tau$ is an unbiased estimator of $\tau$ under any re-randomization mechanism $\phi\in\Phi$, i.e.,
$$\bbE(\hat\tau\mid\phi=1)=\tau,\ \forall\ \phi\in\Phi.$$
Moreover, based on \eqref{eq:Asymptotics4Phi}, we have
\[
    \begin{split}
        \big.\sqrt{N}(\hat{\tau}-\tau)\big|\sqrt{N}\bm D \in \mathcal{B}_{\phi} &\xrightarrow{d}  \left.A_{\infty}\right|\bm B_{\infty} \in \mathcal{B}_{\phi,\infty}\\ &\sim \epsilon + \bm V_{\tau x,\infty}\bm V_{xx,\infty}^{-1}\bm B_{\infty}|\bm B_{\infty} \in \mathcal{B}_{\phi,\infty},
    \end{split}
\]
where $\epsilon \sim N(0,V_{\tau\tau,\infty}-\bm V_{\tau x,\infty}\bm V_{xx,\infty}^{-1} \bm V_{x\tau,\infty})$ is independent of $\bm B_{\infty}$.
Thus, the asymptotic sampling variance of $\sqrt{N}(\hat{\tau}-\tau)$ under a re-randomization mechanism $\phi\in\Phi$ is 
\begin{equation}\label{eq:AsymptoticVariance4Phi}
    %\begin{split}
    %\bbV_a\left(\sqrt{N}(\hat{\tau}-\tau)|\phi=1\right) &= 
    %\bbV_a\left(\sqrt{N}(\hat{\tau}-\tau)|\sqrt{N}\bm D \in \mathcal{B}_{\phi}\right)\\
    %%&=(1-R^2_{\infty})V_{\tau\tau,\infty} + \bbV\left(\bm V_{\tau x,\infty}\bm V_{xx,\infty}^{-1}\bm B_{\infty}|\bm B_{\infty} \in \mathcal{B}_{\phi,\infty}\right),
    %&=(1-R^2_{\infty})V_{\tau\tau,\infty} + \bbV\left(\bm\beta_{\infty}^T\bm B_{\infty}|\bm B_{\infty} \in \mathcal{B}_{\phi,\infty}\right),
    %\end{split}
     \bbV_a\big(\sqrt{N}(\hat{\tau}-\tau)|\phi=1\big) = 
    (1-R^2_{\infty})V_{\tau\tau,\infty} + \bbE\left(\bm B_{\infty}^T\bbeta_{\infty}\bbeta_{\infty}^T\bm B_{\infty}|\bm B_{\infty} \in \mathcal{B}_{\phi,\infty}\right),
\end{equation}
where $R^2_{\infty}$ and $\bm\beta_{\infty}$ are constant scalar and vector defined below:
\begin{eqnarray}
    \label{eq:R2_infty}
    R^2_{\infty} &\triangleq& \bm V_{\tau x,\infty}\bm V_{xx,\infty}^{-1}\bm V_{x\tau,\infty}/V_{\tau \tau,\infty} = \lim_{N\rightarrow \infty} \bm V_{\tau x,N}\bm V_{xx,N}^{-1}\bm V_{x\tau,N}/V_{\tau \tau,N}=\lim_{N\rightarrow \infty} R^2_N,\\
    \label{eq:beta_infty}
    \bm\beta_{\infty} &\triangleq& \bm V_{xx,\infty}^{-1}\bm V_{x\tau,\infty}.
\end{eqnarray}
%$$R^2_{\infty} \triangleq \bm V_{\tau x,\infty}\bm V_{xx,\infty}^{-1}\bm V_{x\tau,\infty}/V_{\tau \tau,\infty} = \lim_{N\rightarrow \infty} \bm V_{\tau x,N}\bm V_{xx,N}^{-1}\bm V_{x\tau,N}/V_{\tau \tau,N}=\lim_{N\rightarrow \infty} {\red R^2_N}.$$
Apparently, $\bm\beta_{\infty}$ is the regression coefficient of the linear regression for individual causal effect and the involved covariates in the super population, and $R^2_\infty$ stands for the corresponding squared multiple correlation.

Because $(1-R^2_{\infty})V_{\tau\tau,\infty}<V_{\tau\tau,\infty}$, we would have from \eqref{eq:AsymptoticVariance4Phi} that
\[\begin{split}
    \bbV_a\big(\sqrt{N}(\hat{\tau}-\tau)|\phi=1\big)<V_{\tau\tau,\infty}= \bbV_a\big(\sqrt{N}(\hat{\tau}-\tau)\big),
\end{split}\]
which means that re-randomization $\phi$ would improve the statistical efficiency of the difference-in-mean estimator $\hat\tau$, once we can control 
%$\bbE\left(\bm\beta_{\infty}^T\bm B_{\infty}|\bm B_{\infty} \in \mathcal{B}_{\phi,\infty}\right)$
$\bbE\big(\bm B_{\infty}^T\bbeta_{\infty}\bbeta_{\infty}^T\bm B_{\infty}|\bm B_{\infty} \in \mathcal{B}_{\phi,\infty}\big)$
as a small number.
Such a result reveals the insight behind the advantage of re-randomization over complete randomization.

In practice, as suggested by \cite{Li2018}, we can measure the efficiency improvement by a re-randomization procedure $\phi$ relative to complete randomization via the \emph{Percent Reduction in Asymptotic Sampling Variance} (PRIASV) of $\sqrt{N}(\hat{\tau}-\tau)$ defined below:
\begin{equation}\label{eq:PRIAV}
    \text{PRIASV}_\phi \triangleq 100 \times \left[1-\frac{\bbV_a\big(\sqrt{N}(\hat{\tau}-\tau)\mid \phi=1\big)}{\bbV_a\big(\sqrt{N}(\hat{\tau}-\tau)\big)}\right].
\end{equation}
Because $\bbV_a\left(\sqrt{N}(\hat{\tau}-\tau)\mid \phi=1\right)$ is of the form in \eqref{eq:AsymptoticVariance4Phi} for any $\phi\in\Phi$, PRIASV$_\phi$ can be organized into a more convenient form as highlighted by the lemma below.

\begin{lemma}\label{lma:PRIAV_Phi}
For $\forall\ \phi\in\Phi$, we have
\begin{equation}\label{eq:PRIAV_Phi}
    \text{PRIASV}_\phi = 100 \times \left(1-r_\phi\right)\times R^2_\infty,
\end{equation}
where
\begin{equation}
r_\phi=\frac{\bbE\left(\bm B_{\infty}^T\bbeta_{\infty}\bbeta_{\infty}^T\bm B_{\infty}|\bm B_{\infty} \in \mathcal{B}_{\phi,\infty}\right)}{\bbeta_{\infty}^T \bm V_{xx,\infty}\bbeta_{\infty}}.
\end{equation}
\end{lemma}



\section{Bayesian perspective for re-randomization}\label{sec:ReB}

%Motivation of the study
In this study, we aim to propose novel re-randomization mechanisms that are more efficient than the classic ReM-based ones.
The key idea roots in the observation that all ReM-based re-randomization mechanisms, including ReM, ReM$_T$, PCA-ReM and ridge-ReM, fail to take full consideration of the heterogeneity of different covariates with respect to the causal effect, while there is a clear insight that covariates associated with causal effect more closely should enjoy higher priority to be balanced.
%Mahalanobis distance in ReM only considers the variance structure of covariates and assigns the same importance weight to each covariate.
Although ReM$_T$ considers covariate heterogeneity by partitioning the covariates into tiers and giving higher priority to balancing covariates in the leading tiers, such a strategy is sub-optimal because it treats covariates in the same tier equally.
We have better choices if we can quantify the heterogeneity of covariates with respect to causal effect more precisely.
%In other word, ReM$_T$ specifies a partial order on covariates to guide re-randomization.
%Although a partial order of covariates can be relatively easy to specify and is effective at improving the efficiency of re-randomization, 


\subsection{Re-randomization under the oracle}
To overcome the limitation of the ReM-based re-randomization mechanisms, we would like to follow a principled way to design more efficient re-randomization mechanisms.
For any $\phi \in \Phi$, we can define its asymptotic acceptance rate based on its asymptotic acceptance region $\mathcal{B}_{\phi,\infty}=\{\bm\mu:\phi(\bm\mu,\bm V_{xx,\infty})=1\}$ as 
\begin{equation}
    \gamma_{\phi,\infty} = \bbP\big(\bB_\infty \in \mathcal{B}_{\phi,\infty}\big) = \lim_{N \rightarrow \infty} \bbP\big(\sqrt{N}\bm D \in \mathcal{B}_{\phi}\big).
\end{equation}
For any $\alpha \in (0,1)$, let 
\begin{equation}
    \Phi_{\alpha,\infty} = \{\phi\in\Phi:\gamma_{\phi,\infty}=\alpha\}
\end{equation}
be the subset of $\Phi$ that covers all re-randomization mechanisms with asymptotic acceptance rate $\alpha$. 
% We want to find a class of balance criteria such that the asymptotic conditional sampling variance of $\sqrt{N}(\hat{\tau}-\tau)$, i.e., $\bbV_a\left(\sqrt{N}(\hat{\tau}-\tau)|\phi=1\right)$ reaches the minimum.
In principle, it is ideal to find the most efficient re-randomization mechanism in $\Phi_{\alpha,\infty}$ that minimizes the asymptotic conditional sampling variance $\bbV_a\big(\sqrt{N}(\hat{\tau}-\tau)|\phi=1\big)$, i.e,
\begin{equation}\label{eq:TargetOptimizationProblem4ReO}
\phi^*=\arg\min_{\phi\in\Phi_{\alpha,\infty}}\bbV_a\big(\sqrt{N}(\hat{\tau}-\tau)|\phi=1\big).
\end{equation}

Fortunately, insightful clues for resolving this problem can be found in the mathematical formulation of the asymptotic variance $\bbV_a\big(\sqrt{N}(\hat{\tau}-\tau)|\phi=1\big)$ in \eqref{eq:AsymptoticVariance4Phi}.
Because the first term in \eqref{eq:AsymptoticVariance4Phi}, i.e., $(1-R^2_{\infty})V_{\tau\tau,\infty}$, is a constant that does not depend on the specification of the re-randomization mechanism $\phi$, and the asymptotic sampling variance in \eqref{eq:AsymptoticVariance4Phi} is determined by the interaction of two factors $\bm\beta_{\infty}$ and $\bB_{\infty}$ in the second term.
Therefore, when $\bm\beta_{\infty}$ is precisely known, we can resolve the optimization problem in \eqref{eq:TargetOptimizationProblem4ReO} by minimizing $\bbE\left(\bB_{\infty}^T\bbeta_{\infty}\bbeta_{\infty}^T\bB_{\infty}|\bm B_{\infty} \in \mathcal{B}_{\phi,\infty}\right)$, i.e.,
$$\arg\min_{\phi\in\Phi_{\alpha,\infty}}\bbV_a\big(\sqrt{N}(\hat{\tau}-\tau)|\phi=1\big)
=\arg\min_{\phi\in\Phi_{\alpha,\infty}}\bbE\left(\bB_{\infty}^T\bbeta_{\infty}\bbeta_{\infty}^T\bB_{\infty}|\bm B_{\infty} \in \mathcal{B}_{\phi,\infty}\right).$$
% and it suffices to minimize $\bbE\left(
% \bm B_{\infty}^T\bm\beta_{\infty}\bm\beta_{\infty}^T\bm B_{\infty}|\bm B_{\infty} \in \mathcal{B}_{\phi,\infty}\right)$. 
The theorem below provides a general answer to such type of optimization problems.

\begin{theorem}\label{thm:GeneralSolution4OptimizationProblem}
Let $\bm Z$ be a random vector defined on a probability space $(\Omega,\mathcal{F},\mathcal{P})$, $g(\bm Z)$ be a measurable function of $\bm Z$ satisfying $\bbE|g(\bm Z)|<\infty$, and  
$$\Psi_{\alpha}=\left\{\phi(\bm Z)=0\text{ or }1: \bbP(\phi(\bm Z)=1)=\alpha\right\}$$
be the set of re-randomization mechanisms for $\bm Z$ with acceptance rate $\alpha$. 
Define 
\[\begin{split}
    \phi_{g,\alpha}(\bm Z) \triangleq I\left(g(\bm Z)\leq a\right) \text{ with } \bbP(\phi_g(\bm Z)=1) = \bbP(g(\bm Z)\leq a)=\alpha
\end{split}\]
as re-randomization mechanism that defines the acceptance region based on the contour line of $g(\cdot)$. 
We have:
\[\begin{split}
    \phi_{g,\alpha} = \arg \min_{\phi\in{\Psi_{\alpha}}} \bbE\big(g(\bm Z)|\phi(\bm Z)=1\big).
\end{split}\]
\end{theorem}

Define the following $\bbeta_\infty$-projected distance
\begin{equation}\label{db}
    d_{\bm\beta_\infty} = \big(\sqrt{N}\bm D\big)^T \bm\beta_{\infty}\bm\beta_{\infty}^T \big(\sqrt{N}\bD\big)= N\bm D^T \bm\beta_{\infty}\bm\beta_{\infty}^T \bm D
\end{equation}
to substitute the classic Mahalanobis distance $d_M=\bm D^T \bSigma_{\bm D}^{-1}\bm D=N\bm D^T \bV_{xx,N}^{-1}\bm D$ defined in \eqref{eq:dM} as the primary measure of covariate imbalance in re-randomization.
Application of Theorem~\ref{thm:GeneralSolution4OptimizationProblem} to $g(\bB_\infty)=\bB_\infty\bbeta_{\infty}\bbeta_{\infty}^T\bB_\infty$ suggests that the following $\bbeta_\infty$-specific re-randomization mechanism 
\begin{equation}\label{eq:ReO}
\phi_{\bbeta_\infty}\big(\sqrt{N}\bm D, \bm V_{xx,N}\big) = I\big(d_{\bbeta_\infty} \leq a\big)
\end{equation}
is exactly what we are pursuing, where the threshold $a$ is the root of the probability equation below:
$$\lim_{N\rightarrow\infty}\bbP(d_{\bbeta_\infty} \leq a)
%=\bbP(\bB_\infty\bbeta_\infty\bbeta_\infty^T\bB_\infty\leq a)
=\lim_{N\rightarrow\infty}\bbP\big((\sqrt{N}\bm D)^T \bm\beta_{\infty}\bm\beta_{\infty}^T (\sqrt{N}\bD) \leq a\big)
=\alpha.$$
Because $\sqrt{N}\bD$ converges to a multi-variate Gaussian distribution according to \eqref{eq:AssymptoticDistribution4HatTau_bD}, $d_{\bbeta_\infty}$ takes $\chi^2_1$ as its asymptotic distribution as pointed out by the lemma below.



\begin{lemma}\label{lma:AssymptoticDistribution4d_beta}
Suppose Condition \ref{cond1} holds and $\bm\beta_{\infty}$ is known. 
Under complete randomization, we have
%the covariance matrix of $\bm D$ is $\bm\Sigma_{\bm D} = \frac{1}{N}\bm V_{xx}$ and
\[\begin{split}
    d_{\bm\beta_{\infty}}/\sigma^2_{d_{\bm\beta_{\infty}}} \xrightarrow{d} \chi^2_1\ \text{ as }N \rightarrow \infty,
\end{split}\]
where 
$\sigma^2_{d_{\bm\beta_{\infty}}}= \bm\beta_{\infty}^T\bm V_{xx,N} \bm\beta_{\infty}$.
\end{lemma}



Combining Theorem~\ref{thm:GeneralSolution4OptimizationProblem} and Lemma~\ref{lma:AssymptoticDistribution4d_beta}, we have the following theorem.

\begin{theorem}\label{thm:ReO}
Suppose Condition \ref{cond1} holds and $\bm\beta_{\infty}$ is known. 
The following $\bm\beta_{\infty}$-specific re-randomization procedure
\begin{equation}\label{ReO-cri}
    \phi_{\bm\beta_{\infty}}\big(\sqrt{N}\bm D, \bm V_{xx,N}\big) = I\big(d_{\bm\beta_{\infty}} \leq \sigma^2_{d_{\bm\beta_{\infty}}}\cdot\xi_{\alpha,1}\big)
\end{equation}
satisfies $\phi_{\bm\beta_{\infty}} \in \Phi_{\alpha,\infty}$ and 
\[\begin{split}
    \bbV_a\big(\big.\sqrt{N}(\hat{\tau}-\tau)\big|\phi_{\bm\beta_{\infty}}=1\big) \leq \bbV_a\big(\big.\sqrt{N}(\hat{\tau}-\tau)\big|\phi=1\big),\ \forall \phi\in\Phi_{\alpha,\infty},
\end{split}\]
where $d_{\bm\beta_{\infty}}$ and $\sigma^2_{d_{\bm\beta_{\infty}}}$ are defined as in \eqref{db} and Lemma~\ref{lma:AssymptoticDistribution4d_beta}, $\xi_{\alpha,1}$ is the $\alpha$-quantile of the $\chi^2_1$ distribution.
\end{theorem}

%Proofs of Lemma~\ref{lma:AssymptoticDistribution4d_beta} and Theorem~\ref{thm:ReO} are detailed in the Appendix.
Theorem~\ref{thm:ReO} shows that $\phi_{\bm\beta_{\infty}}$ is the optimal re-randomization mechanism in $\Phi_{\alpha,\infty}$ when $\bm\beta_{\infty}$ is known, and $\phi_{\bm\beta_{\infty}}$ leads to a ribbon-shaped acceptance region in the space of $\bm D$, i.e.,
\begin{equation}\label{regionReO}
    \mathcal{B}_{\phi_{\bm\beta_{\infty}}}=\left\{
    \sqrt{N}\bm D: \big(\sqrt{N}\bm\beta_{\infty}^T\bm D\big)^2 \leq \bm\beta_{\infty}^T \bm V_{xx,N} \bm\beta_{\infty} \cdot \xi_{\alpha,1}\right\},
\end{equation}
whose orientation is orthogonal to the direction of $\bm\beta_{\infty}$. In addition, we note that 
\[\begin{split}
    \bm\beta_{\infty}^T \bm D = \frac{1}{N_t} \sum_{i=1}^N \bm Z_i(\bm\beta_{\infty})W_i - \frac{1}{N_c} \sum_{i=1}^N \bm Z_i(\bm\beta_{\infty})(1-W_i) = \bar{Z}_t(\bm\beta_{\infty}) - \bar{Z}_c(\bm\beta_{\infty}),
\end{split}\]
where $Z_i(\bm\beta_{\infty}) = \bm X_i^T \bm\beta_{\infty}$ is the index variable obtained by taking the inner product of $\bm X_i$ and $\bm\beta_{\infty}$. 
Therefore, controlling $d_{\bm\beta_{\infty}} = N\bm D^T \bm\beta_{\infty}\bm\beta_{\infty}^T \bm D$ as described in $\phi_{\bm\beta_{\infty}}$ is equivalent to controlling the imbalance of the index variable $Z_i(\bm\beta_{\infty}) = \bm X_i^T \bm\beta_{\infty}$ in the Euclidean space. 
Thus, when $\bm\beta_{\infty}$ is known, we can do dimension reduction first by projecting the original high-dimensional covariates into an one-dimensional sub-space spanned by $\bm\beta_{\infty}$, and then achieve more efficient covariate balance after the dimension reduction. 
Hereinafter, we refer to $\phi_{\bm\beta_{\infty}}$ as the \emph{re-randomization under the oracle} (ReO), as it is the most efficient re-randomization mechanism achievable in the asymptotic perspective when $\bm\beta_\infty$ is precisely known.

The theorem below calculates the 
%\emph{Percent Reduction in Asymptotic Sampling Variance} (PRIASV)
PRIASV of $\sqrt{N}(\hat{\tau}-\tau)$ by $\phi_{\bm\beta_{\infty}}$ relative to complete randomization.

\begin{theorem}\label{PRIAV-ReO}
Under Condition \ref{cond1}, the re-randomization under the oracle $\phi_{\bm\beta_{\infty}}$ achieves 
$r_{\phi_{\bbeta_{\infty}}} = v_{\alpha,1}$ and 
\begin{equation}\label{eq:PRIAV-ReO}
    \text{PRIASV}_{ReO} %= 100 \times \left[1-\frac{\bbV_a\left(\hat{\tau}\mid \phi_{\bbeta_{\infty}=1}\right)}{\bbV_a\left(\hat{\tau}\right)}\right]
    = 100 \times (1-v_{\alpha,1})R^2_{\infty},
\end{equation}
where $v_{\alpha,1} = \bbE\left(\chi^2_1|\chi^2_1\leq \xi_{\alpha,1}\right)$ and $R^2_{\infty} = \bm V_{\tau x,\infty}\bm V_{x x,\infty}^{-1}\bm V_{x\tau,\infty}/\bm V_{\tau \tau,\infty} = \lim_{N\rightarrow \infty} R^2_N$. 
\end{theorem}

Recall that \cite{Li2018} proved that ReM achieves 
\[\begin{split}
    \text{PRIASV}_{ReM} = 100 \times (1-v_{\alpha,p})R^2_{\infty},
\end{split}\]
where $v_{\alpha,p} = \bbE\left(\chi^2_p|\chi^2_p\leq \xi_{\alpha,p}\right)/p$. 
Therefore, the advantage of ReO over ReM in estimation efficiency can be quantified as follows.

\begin{corollary}\label{cor:ReOvsReM}
Under Condition \ref{cond1}, for any fixed $\alpha \in (0,1)$, 
\begin{equation}\label{compare}
    \frac{\text{PRIASV}_{ReM}}{\text{PRIASV}_{ReO}} = \frac{1-v_{\alpha,p}}{1-v_{\alpha,1}} = O\left(\frac{1}{\sqrt{p}}\right).
\end{equation}
\end{corollary}

\begin{figure}[ht]
\centering
\includegraphics[width=3in, height=2.25in]{Ratio.png}
\caption{Ratio of $\text{PRIASV}_{ReO}$ and $\text{PRIASV}_{ReM}$ for varying covariate dimension $p$ and logarithm acceptance probability $\alpha$.}
\label{fig:PRIAV_ReO/ReM}
\end{figure}

Figure \ref{fig:PRIAV_ReO/ReM} illustrates the superiority of ReO over ReM in terms of PRIASV by a heat map of $\text{PRIASV}_{ReO}/\text{PRIASV}_{ReM}$ as a function of covariate dimension $p$ and acceptance probability $\alpha$ based on Eq.~\eqref{compare}. 
%{\red 
From the figure, we observe an increasing performance advantage for ReO over ReM with the increase of covariate dimension $p$.
%, a result that is intuitively obvious.}


\subsection{Bayesian Criterion for Re-randomization}
A critical limitation of ReO, however, is that we must know $\bbeta_{\infty}$ for its implementation, which is often not realistic.
In this subsection, we retreat to a more realistic case where prior information about $\bbeta_{\infty}$, instead of $\bbeta_{\infty}$ itself, can be obtained, and try to establish a Bayesian criterion for re-randomization.

Suppose $\bbeta_{\infty}$ is associated with a prior distribution $\pi$.
Under the Bayesian framework, we quantify the performance of a re-randomization mechanism $\phi$ under prior $\pi$ by averaging its $\bbeta_{\infty}$-specific asymptotic sampling variance with respect to $\pi$. 
Formally, define the prior-integrated asymptotic sampling variance
\begin{equation}\label{prior-integrated}
    \begin{split}
    \bbV_{\pi,a}\big(\sqrt{N}(\hat{\tau}-\tau)\mid\phi=1\big)
    &= \bbE_{\pi}\Big[\bbV_a\big(\sqrt{N}(\hat{\tau}-\tau)\mid\phi=1\big)\Big]\\
    &\vspace{1cm} = \bbE_{\pi}\Big[(1-R^2_{\infty})V_{\tau\tau,\infty} + \bbE\left(\bm B_{\infty}^T\bbeta_{\infty}\bbeta_{\infty}^T\bm B_{\infty}|\bm B_{\infty} \in \mathcal{B}_{\phi,\infty}\right) \Big]
    \end{split}
\end{equation}
as the objective function for evaluating a re-randomization mechanism $\phi\in\Phi$ under the Bayesian setting.
We aim to search $\Phi_{\alpha,\infty}$ for the solution of the optimization problem below:
%find the most efficient re-randomization mechanism in $\Phi_\alpha$ that minimizes the asymptotic conditional sampling variance $\bbV_a(\hat{\tau}|\phi=1)$, i.e,
\begin{equation}\label{eq:TargetOptimizationProblem4ReB}
\phi^*=\arg\min_{\phi\in\Phi_{\alpha,\infty}}\bbV_{\pi,a}\big(\sqrt{N}(\hat{\tau}-\tau)|\phi=1\big).
\end{equation}

Considering that $\bbE_{\pi}\left[(1-R^2_{\infty})V_{\tau\tau,\infty}\right]$ is independent of $\phi$, it suffices to minimize the second term in \eqref{prior-integrated}, which can be organized as
\begin{equation}\label{eq:ObjectiveFunction4ReB}
    \begin{split}
 \bbE_{\pi}\left[\bbE\left(\bm B^T_{\infty}\bbeta_{\infty}\bbeta_{\infty}^T\bm B_{\infty}\mid\bm B_{\infty} \in \mathcal{B}_{\phi,\infty}\right)\right]
    &= \bbE\left[\bm B^T_{\infty}\bbE_{\pi}\left(\bbeta_{\infty}\bbeta_{\infty}^T\right)\bm B_{\infty}\mid\bm B_{\infty} \in \mathcal{B}_{\phi,\infty}\right]\\
    &= \bbE\left(\bm B^T_{\infty}\bm\Lambda_{\pi}\bm B_{\infty}\mid\bm B_{\infty} \in \mathcal{B}_{\phi,\infty}\right),
    \end{split}
\end{equation}
where 
$$\bm\Lambda_{\pi} = \bbE_\pi\left(\bbeta_{\infty}\bbeta_{\infty}^T\right) = \bm\mu_{\pi}\bm\mu_{\pi}^T + \bm\Sigma_{\pi}$$
is the second moment of $\pi$ and is referred to as the characteristic matrix of the prior distribution,
with $\bm\mu_{\pi}$ and $\bm\Sigma_{\pi}$ being the expectation vector and covariance matrix of $\pi$, respectively.
Compared with the objective function in ReO, i.e., $\bbE\big(\bm B^T_{\infty}\bbeta_{\infty}\bbeta_{\infty}^T\bm B_{\infty}\mid\bm B_{\infty} \in \mathcal{B}_{\phi,\infty}\big)$, the new objective function in \eqref{eq:ObjectiveFunction4ReB} replaces the matrix $\bbeta_{\infty}\bbeta_{\infty}^T$ with $\bm\Lambda_{\pi}$. 

Define the following prior-induced distance
\begin{equation}\label{dpi}
    d_{\pi} = \big(\sqrt{N}\bD\big)^T \bm\Lambda_{\pi} \big(\sqrt{N}\bD\big)
\end{equation}
as the measure of covariate imbalance in the Bayesian setting.
Application of Theorem~\ref{thm:GeneralSolution4OptimizationProblem} for $g(\bB_\infty)=\bB_\infty\bLambda_\pi\bB_\infty$ suggests that the following Bayesian re-randomization mechanism 
\begin{equation}\label{eq:ReB_phi}
\phi_\pi\big(\sqrt{N}\bm D, \bm V_{xx,N}\big) = I\big(d_\pi \leq a\big)
\end{equation}
is the optimal solution we are pursuing for the optimal problem in \eqref{eq:TargetOptimizationProblem4ReB}, where the threshold $a$ is the root of the probability equation below:
$$\lim_{N\rightarrow\infty}\bbP(d_\pi \leq a)
=\lim_{N\rightarrow\infty}\bbP\Big(\big(\sqrt{N}\bm D\big)^T \bLambda_{\pi} (\sqrt{N}\bD) \leq a\Big)
=\alpha.$$
Because $\sqrt{N}\bD$ converges to a multi-variate Gaussian distribution according to \eqref{eq:AssymptoticDistribution4HatTau_bD}, $d_\pi$ takes a weighted $\chi^2$ distribution as its asymptotic distribution, as summarized by the lemma below.

%Suppose $\bm V_{xx}^{1/2}$ is the Cholesky square root of $\bm V_{xx}$, i.e., $\bm V_{xx} = \bm V_{xx}^{1/2}\left(\bm V_{xx}^{1/2}\right)^T$, the following lemma derives the asymptotic distribution of $\bm d_{\pi}$.

\begin{lemma}\label{lma:AssymptoticDistribution4d_pi}
Under Condition \ref{cond1} and complete randomization, we have 
\begin{equation}\label{eq:dpi}
    d_{\pi} \overset{\cdot}{\sim} \chi^2_{\blambda} = \sum_{j=1}^p \lambda_j Z_j^2,
\end{equation}
where $\overset{\cdot}{\sim}$ means two random variables converge to the same distribution weakly, $Z_1,...,Z_p$ are i.i.d. random variables from standard Gaussian distribution, and $\blambda =(\lambda_1,\ldots,\lambda_p)$ is vector of eigenvalues of matrix $\bm P = \big(\bm V_{xx,N}^{1/2}\big)^T\bm\Lambda_{\pi}\bm V_{xx,N}^{1/2}$ with $\bm V_{xx,N}^{1/2}$ being the Cholesky square root of $\bm V_{xx,N}$ satisfying $\bm V_{xx,N}^{1/2}\big(\bm V_{xx,N}^{1/2}\big)^T=\bm V_{xx,N}$.
\end{lemma}

%We have the following theorem about the optimal re-randomization criterion under the Bayesian criterion.
Combining Theorem~\ref{thm:GeneralSolution4OptimizationProblem} and Lemma~\ref{lma:AssymptoticDistribution4d_pi}, we get the following theorem immediatly.

\begin{theorem}\label{thm:ReB}
Suppose Condition \ref{cond1} is satisfied and $\bbeta_{\infty}$ follows prior distribution $\pi$.  Let $\bm\mu_{\pi}$ and $\bSigma_{\pi}$ be the mean vector and covariance matrix of $\pi$
respectively, and $\bLambda_{\pi}=\bm\mu_{\pi}\bm\mu_{\pi}^T + \bSigma_{\pi}$ be the corresponding characteristic matrix. The Bayesian re-randomization mechanism 
\begin{equation}\label{eq:ReB}
    \phi_{\pi}\big(\sqrt{N}\bm D, \bm V_{xx,N}\big) = I\big(d_{\pi}\leq \xi_{\alpha,\blambda}\big) 
\end{equation}
satisfies $\phi_{\pi} \in \Phi_{\alpha,\infty}$, and 
\[\begin{split}
    \bbV_{\pi,a}\big(\sqrt{N}(\hat{\tau}-\tau)\mid\phi_{\pi}=1\big) \leq \bbV_{\pi,a}\big(\sqrt{N}(\hat{\tau}-\tau)\mid\phi=1\big),\ \forall \phi \in \Phi_{\alpha,\infty},
\end{split}\]
where $\xi_{\alpha,\blambda}$ is $\alpha$-quantile of $\chi^2_{\blambda}$ with $\blambda =(\lambda_1,\ldots,\lambda_p)$ being vector of eigenvalues of matrix $\bm P = \big(\bm V_{xx,N}^{1/2}\big)^T\bm\Lambda_{\pi}\bm V_{xx,N}^{1/2}$.
\end{theorem}

%Detailed proofs of Lemma~\ref{lma:AssymptoticDistribution4d_pi} and Theorem~\ref{thm:ReB} can be found in the Appendix.
The theorem tells us that $\phi_{\pi}$, the optimal re-randomization procedure in $\phi_{\alpha,a}$ under the Bayesian criterion, achieves the smallest prior-integrated asymptotic sampling variance and leads to an elliptical acceptance region, whose shape is determined by $\bm\Lambda_{\pi}$, i.e., the characteristic matrix of the prior distribution $\pi$ (see Figure \ref{fig:illustrative} for the graphical illustration). 
Analogous to $d_{\bm\beta_{\infty}} = N\bm D^T \bm\beta_{\infty}\bm\beta_{\infty}^T\bm D$ in ReO, $d_{\pi} = N\bD^T \bm\Lambda_{\pi} \bD$ plays a similar role in achieving covariance balance by projecting the high dimensional $\bm D$ into a lower dimensional space. 
Hereinafter, we refer to $\phi_{\pi}$ as the \emph{re-randomization under the Bayesian criterion} (ReB). 

\begin{figure}
\centering
\includegraphics[scale=0.3]{CompareB.pdf}
\caption{Acceptance region of $\bD=(D1,D2)$ under different criteria for re-randomization with only 2 covariates. $\bbeta_{\infty}=c(1,0)^T$ in ReO and $\bm\Lambda_\pi=\bbeta_{\infty}\bbeta_{\infty}^T+\mbox{diag}\{0.25,0.25\}$ in ReB when acceptance probability $\alpha=0.5$.} 
\label{fig:illustrative}
\end{figure}


\subsection{Connections to other re-randomization criteria}
Theorem \ref{thm:ReB} shows that the Bayesian criterion for re-randomization leads to the acceptance rule $\phi_\pi=I(N\bD^T\bLambda_\pi\bD\leq\xi_{\alpha,\blambda})$, which is completely determined by $\bLambda_\pi=\bmu_\pi\bmu_\pi^T+\bSigma_\pi$, the characteristic matrix of prior distribution $\pi$. 
By specifying $\bmu_\pi$ and $\bSigma_\pi$ to different values, we find that Bayesian re-randomization takes many other re-randomization criteria as special cases.

For example, if we specify $\bmu_\pi=\bbeta_{\infty}$ and $\bSigma_\pi=\bZero_{p\times p}$, i.e., let $\pi(\bbeta)=\delta_{\bbeta_{\infty}}$ being a point-mass prior concentrating at $\bbeta_{\infty}$, we would have
$$\bLambda_\pi=\bbeta_{\infty}^T\bbeta_{\infty}\quad\mbox{and}\quad d_\pi=N\bD^T\bbeta_{\infty}^T\bbeta_{\infty}\bD,$$
and the Bayesian re-randomization ReB with the point-mass prior $\delta_{\bbeta_{\infty}}$ simplifies to the oracle re-randomization ReO.
On the other hand, if we specify $\bmu_\pi=\bZero_p$ and $\bSigma_\pi=\bV_{xx,N}^{-1} = (N\bSigma_\bD)^{-1}$, i.e., let $\pi=N\big(\bZero_p,\bV_{xx,N}^{-1}\big)$, we have 
$$\bLambda_\pi=\bV_{xx,N}^{-1}\quad\mbox{and}\quad d_\pi=N\bD^T\bV_{xx,N}^{-1}\bD=\bD^T\bSigma_\bD^{-1}\bD,$$
and the Bayesian re-randomization ReB becomes classical ReM.
Moreover, if we specify $\bmu_\pi=\bZero_{p}$ and $\bSigma_\pi=N^{-1}\big(\bSigma_\bD+\lambda\bI_{p}\big)^{-1}$, we have 
$$\bLambda_\pi=N^{-1}\big(\bSigma_\bD+\lambda\bI_{p}\big)^{-1}\quad\mbox{and}\quad d_\pi=\bD^T\big(\bSigma_\bD+\lambda\bI_{p}\big)^{-1}\bD,$$
which leads to the Ridge-ReM.
Similarly, if we specify $\bmu_\pi=\bZero_p$ and $\bSigma_\pi=\Big(N\bSigma_\bD^{(k)}\Big)^{-1}$, where $\bSigma_\bD^{(k)}$
is the truncated covariance matrix of $\bD$ with the smallest $(p-k)$ eigenvalues of $\bSigma_\bD$ replaced by 0,
we have 
$$\bLambda_\pi=\Big(N\bSigma_\bD^{(k)}\Big)^{-1}\quad\mbox{and}\quad d_\pi=\bD^T\Big(\bSigma_\bD^{(k)}\Big)^{-1}\bD,$$
which leads to the PCA-ReM.

Thus, ReB provides us a unified framework to understand  various re-randomization criteria in the literature, and highlight their differences from the Bayesian perspectives.
We note that the prior distribution $\pi$ may depend on the covariates via $\bV_{xx,N}$ in the ReM-based methods.


\subsection{Invariance of ReB to the scale transformation of prior distribution $\pi$}
An interesting property of ReB is that its acceptance region shows invariance to any scale transformation of prior distribution $\pi$.
The following corollary based on Thm.~\ref{thm:ReB} summarizes this phenomenon explicitly.

\begin{corollary}\label{invariance}
Suppose that $\phi_{\pi_1}$ and $\phi_{\pi_2}$ are two ReB procedures in $\Phi_{\alpha,\infty}$ with different prior specifications $\pi_1$ and $\pi_2$, respectively. 
We have 
\[\begin{split}
    \phi_{\pi_2} = \phi_{\pi_1}\ \text{as long as } \bm\Lambda_{\pi_2} = r\bm\Lambda_{\pi_1}\ \text{for some }r>0.
\end{split}\]
\end{corollary}

Intuitively, ReB enjoys such a property because only the direction, not the magnitude, of $\bbeta_\infty$ matters in determining the relative importance of covariates, and thus the acceptance region.
The detailed proof can be found in the Supplementary Material.


\subsection{Frequentist properties of ReB}
%So far, we have derived a new re-randomization criterion based on Bayesian framework. 
Although ReB is established from the Bayesian perspective, we can study its performance with comparison to other re-randomization mechanisms, e.g., ReM, from the frequentist perspective. 
For a specific ReB mechanism $\phi_\pi$ with characteristic matrix $\bLambda_\pi$, since $\bLambda_\pi$ may be related to $\bm V_{xx,N}$, which will converge to $\bm V_{xx,\infty}$ as $N\rightarrow\infty$, as in ReM and its extensions, we  
denote the limit of $\bLambda_\pi$ as $\Tilde{\bLambda}_\pi = \lim_{N\rightarrow \infty} \bLambda_\pi$.
Then the asymptotic acceptance region of $\phi_{\pi}$ is 
\[\begin{split}
    \mathcal{B}_{\phi_{\pi},\infty} = \{\bm\mu:\phi_{\pi}(\bm\mu,\bm V_{xx,\infty})=1\} = \{\bm\mu:\bm\mu^T \Tilde{\bLambda}_\pi\bm\mu \leq\xi_{\alpha,\blambda_{\infty}}\},
\end{split}\]
where $\blambda_{\infty} = (\lambda_{1,\infty},...,\lambda_{p,\infty})^T$ is the vector of eigenvalues of $\bm P_{\infty}=\lim_{N\rightarrow\infty} \bm P = \big(\bV_{xx,\infty}^{1/2}\big)^T\Tilde{\bLambda}_\pi\bV_{xx,\infty}^{1/2}$.
According to \eqref{eq:AsymptoticVariance4Phi}, the asymptotic variance of $\sqrt{N}(\hat\tau-\tau)$ under $\phi_\pi$ is
%By Eq.~\eqref{asym}, the conditional asymptotic sampling variance of $\sqrt{N}\left(\hat{\tau}-\tau\right)$ under ReB is 
\[\begin{split}
    \bbV_a\big(\sqrt{N}(\hat{\tau}-\tau)\mid\phi_{\pi}=1\big)=
    (1-R^2_{\infty})V_{\tau\tau,\infty} + \bbE\big(\bm B^T_{\infty}\bbeta_{\infty}\bbeta_{\infty}^T\bm B_{\infty}\mid\bm B_{\infty}^T\Tilde{\bLambda}_\pi\bm B_{\infty}\leq \xi_{\alpha,\blambda_{\infty}}\big).
\end{split}\]
%Let 
%\begin{equation}\label{R2_pi}
%    v_{\alpha,\pi}  = \frac{\bbE\left(\bm B^T_{\infty}\bbeta_{\infty}\bbeta_{\infty}^T\bm B_{\infty}\mid\bm B_{\infty}^T\bLambda_{\pi}\bm B_{\infty}\leq \xi_{\alpha,\blambda_{\infty}} \right)}{\bbE\left(\bm B^T_{\infty}\bbeta_{\infty}\bbeta_{\infty}^T\bm B_{\infty}\right)},
%\end{equation}
%which is independent of the $L_2$ norm of $\bbeta_{\infty}$.
%Without loss of generality, we assume $||\bbeta_{\infty}||_2=1$.
%The following theorem gives the asymptotic performance of ReB.
Based on this result, the relative efficiency of $\phi_\pi$ with respect to complete randomization can be quantified in term of PRIASV of $\sqrt{N}(\hat\tau-\tau)$:
\begin{equation}\label{PRIAV_ReB}
    \text{PRIASV}_{ReB}=100\times(1-r_{\phi_{\pi}})R^2_{\infty},
\end{equation}
where 
\begin{equation}\label{R2_pi}
    r_{\phi_{\pi}}=v_{\alpha,\pi} \triangleq \frac{\bbE\big(\bm B^T_{\infty}\bbeta_{\infty}\bbeta_{\infty}^T\bm B_{\infty}\mid\bm B_{\infty}^T\Tilde{\bLambda}_\pi\bm B_{\infty}\leq \xi_{\alpha,\blambda_{\infty}} \big)}{\bbE\big(\bm B^T_{\infty}\bbeta_{\infty}\bbeta_{\infty}^T\bm B_{\infty}\big)}
\end{equation}
is a function of  $\bbeta_{\infty}$ and $\bV_{xx,\infty}$.
The theorem below gives an explicit form of $v_{\alpha,\pi}$.


\begin{theorem}\label{thm:ReB2fullrank}
Under the Bayesian setting with $\pi$ as prior distribution of $\bbeta_{\infty}$, $\Tilde{\bLambda}_\pi$ is symmetric and positive definite with Cholesky decomposition $\Tilde{\bLambda}_\pi = \bm M \bm M^T$, we have 
\begin{equation}\label{eq:v_alpha_pi}
    v_{\alpha,\pi} = \frac{\bbeta_{\infty}^T \big(\bm M^{-1}\big)^T\bm\Gamma^T_{\infty}
\bm C\bm\Gamma_{\infty} \bm M^{-1}\bbeta_{\infty}}{\bbeta_{\infty}^T \bm V_{xx,\infty}\bbeta_{\infty}}=\frac{\bbeta_{\infty}^T \big(\bm M^{-1}\big)^T\bm\Gamma^T_{\infty}
\bm C\bm\Gamma_{\infty} \bm M^{-1}\bbeta_{\infty}}{\bbeta_{\infty}^T \big(\bm M^{-1}\big)^T\bm\Gamma^T_{\infty}
\bm \Lambda\bm\Gamma_{\infty} \bm M^{-1}\bbeta_{\infty}},
\end{equation}
where $\bm\Gamma_{\infty}$ is an orthogonal matrix such that $\bm M^T \bm V_{xx,\infty} \bm M =\bm\Gamma^T_{\infty} diag\{\lambda_{1,\infty},\ldots,\lambda_{p,\infty}\} \bm\Gamma_{\infty}$, $\bm \Lambda = diag\{\lambda_{1,\infty},\ldots,\lambda_{p,\infty}\}$, and 
% $\bm C = diag\big\{\lambda_{1,\infty}\bbE\big(Z_1^2 \mid \sum_{j=1}^p \lambda_{j,\infty} Z_j^2 \leq \xi_{\alpha,\blambda_{\infty}}\big)\big.,$
% $\ldots,$
% $\big.\lambda_{p,\infty}\bbE\big(Z_p^2\mid \sum_{j=1}^p \lambda_{j,\infty} Z_j^2 \leq \xi_{\alpha,\blambda_{\infty}}\big)\big\},$
$\bm C = diag\{c_1,\ldots,c_p\}$ with 
$c_i = \bbE\left(\lambda_{i,\infty}Z_i^2\mid \sum_{j=1}^p \lambda_{j,\infty} Z_j^2 \leq \xi_{\alpha,\blambda_{\infty}}\right)$ for $i=1,\ldots,p$ and 
$Z_1,...,Z_p$ are i.i.d. standard normal distributed random variables.
\end{theorem}

Theorem \ref{thm:ReB2fullrank} gives the explicit expression of $v_{\alpha,\pi}$ when $\Tilde{\bLambda}_\pi$ is of full rank, which is easy to hold in practice. The difference between the numerator and denominator of Eq.~\eqref{eq:v_alpha_pi} depends on the eigenvalues $(\lambda_{1,\infty},\ldots,\lambda_{p,\infty})$ of $\lim_{N\rightarrow\infty}\big(\bV_{xx,N}^{1/2}\big)^T{\bLambda}_\pi\bV_{xx,N}^{1/2}$ via 
$\bbE\big(Z_i^2\mid \sum_{j=1}^p \lambda_{j,\infty} Z_j^2 \leq \xi_{\alpha,\blambda_{\infty}}\big), i=1,\ldots,p.$ This expectation will degenerate to $v_{\alpha,p} = \bbE\big(Z_i^2\mid \sum_{j=1}^p Z_j^2 \leq \xi_{\alpha,p}\big)$ in ReM, where ${\bLambda}_\pi=\bm V_{xx,N}^{-1}$ and thus $\lambda_{1,\infty}=\cdots=\lambda_{p,\infty}=1$. 
In that case, we have $\bm C = v_{\alpha,p} \cdot \bm \Lambda$ and $v_{\alpha,\pi} = v_{\alpha,p}$.

To build a geometry intuition on the effect of the precision of the prior on the performance of ReB, we further consider the case where the prior distribution $\pi$ degenerates to a point mass. 
The theorem below shows a simpler form of $v_{\alpha,\pi}$ in this scenario.

\begin{theorem}\label{thm:ReB2sigma0}
Under the Bayesian setting where the prior distribution $\pi$ degenerates to a point mass at $\bmu_{\pi}$, which means that $\bm\Sigma_{\pi}=\bm 0$ and $\Tilde{\bLambda}_{\pi}=\bmu_{\pi}\bmu_{\pi}^T$, we have
\begin{equation}\label{eq:vapi4sigma0}
    v_{\alpha,\pi}  = 1-(1-v_{\alpha,1})\cdot\frac{(\bmu_{\pi}^T\bV_{xx,\infty}\bbeta_{\infty})^2}{(\bmu_{\pi}^T\bV_{xx,\infty}\bmu_{\pi})(\bbeta_{\infty}^T\bV_{xx,\infty}\bbeta_{\infty})}.
\end{equation}
\end{theorem}
% \begin{remark}\label{remark1}
% If $\bLambda_{\pi}$ is not of full rank, which means that the prior distribution $\pi$ of $\bbeta_{\infty}$ is a degenerated distribution concentrating on a lower dimensional subspace of $\bbR^p$,
% %has no randomness, i.e., we are confident of knowing the true value, 
% Eq.~\eqref{v_alpha_pi} does not works any more.
% {\red But, we can add a small term to $\bLambda_{\pi}$ to make Eq.~\eqref{v_alpha_pi} work.
% [comment by DK: do we really need worry about this special case? Even when $\bLambda_{\pi}$ is not of full rank, the Cholesky decomposition still holds with some eigenvalues being 0. This is not a big deal for Eq.~\eqref{v_alpha_pi}, right?]
% }
% \end{remark}


% {\red
% \begin{remark}\label{remark2}
% Suppose the maximum eigenvalue of matrix $\left(\bm M^{-1}\right)^T\bm\Gamma^T
% \bm C\bm\Gamma \bm M^{-1}$ is $\lambda_{\max}$,
% we have 
% \[\begin{split}
% v_{\alpha,\pi} = \frac{\bbeta_{\infty}^T \left(\bm M^{-1}\right)^T\bm\Gamma^T
% \bm C\bm\Gamma \bm M^{-1}\bbeta_{\infty}}{\bbeta_{\infty}^T \bm V_{xx,\infty}\bbeta_{\infty}} \leq 
% \frac{\lambda_{\max}}{\bbeta_{\infty}^T \bm V_{xx,\infty}\bbeta_{\infty}}.
% \end{split}\]
% A sufficient condition for ReB to perform better than ReM is \[\begin{split}
% \lambda_{\max} < v_{\alpha,p}\cdot\bbeta_{\infty}^T \bm V_{xx,\infty}\bbeta_{\infty}.
% \end{split}\]

% Eq.~\eqref{v_alpha_pi} can be expressed as 
% \[\begin{split}
% v_{\alpha,\pi} = \frac{\bbeta_{\infty}^T \left(\bm M^{-1}\right)^T\bm\Gamma^T_{\infty}
% \bm C\bm\Gamma_{\infty} \bm M^{-1}\bbeta_{\infty}}{\bbeta_{\infty}^T \left(\bm M^{-1}\right)^T\bm\Gamma^T_{\infty}
% \bm \Lambda\bm\Gamma_{\infty} \bm M^{-1}\bbeta_{\infty}},
% % = 
% % \frac{\Tilde{\bbeta}_{\infty}^T
% % \bm C\Tilde{\bbeta}_{\infty}}{\Tilde{\bbeta}_{\infty}^T
% % \bm \Lambda \Tilde{\bbeta}_{\infty}},
% \end{split}\]
% where $\bm \Lambda = diag\{\lambda_{1,\infty},\ldots,\lambda_{p,\infty}\}$
% and 
% $\bm C = diag\left\{\lambda_{1,\infty}\bbE\left(Z_1^2 \mid \sum_{j=1}^p \lambda_{j,\infty} Z_j^2 \leq \xi_{\alpha,\blambda_{\infty}}\right)\right.,$
% $\left.\ldots,\lambda_{p,\infty}\bbE\left(Z_p^2\mid \sum_{j=1}^p \lambda_{j,\infty} Z_j^2 \leq \xi_{\alpha,\blambda_{\infty}}\right)\right\}.$
% \end{remark}
% % }
% Theorem \ref{thm:ReB2} shows the form of $v_{\alpha,\pi}$ when $\bLambda_{\pi}$ is of full rank, which always holds in practice. The performance of ReB, which depends on $\bV_{xx,\infty}$ and the prior $\bLambda_{\pi}$, is of great complexity in form in this scenario. 
Intuitively, when $\Tilde{\bLambda}_{\pi}=\bmu_{\pi}\bmu_{\pi}^T$, $\bmu_{\pi}$ becomes our prior guess for $\bbeta_{\infty}$.
Let $\theta$ be the angle between $\big(\bV_{xx,\infty}^{1/2}\big)^T\bmu_{\pi}$ and $\big(\bV_{xx,\infty}^{1/2}\big)^T\bbeta_{\infty}$.
Eq. \eqref{eq:vapi4sigma0} becomes $v_{\alpha,\pi} = 1-(1-v_{\alpha,1})(\cos\theta)^2$.
Thus, Thm.~\ref{thm:ReB2sigma0} actually tells that the performance of ReB only depends on the angle between $\bmu_{\pi}$ and the real $\bbeta_{\infty}$ after an affine transformation. 
%Then $v_{\alpha,\pi} = 1-(1-v_{\alpha,1})(\cos\theta)^2$ and leads to the following corollary. 
Apparently, a smaller $\theta$ would lead to a re-randomization procedure that behaves more like ReO, and thus enjoys a better chance to outperform the classic ReM.
The corollary below gives the sufficient and necessary condition for ReB (under the degenerated prior) to outperform ReM in terms of the admissible value range of $\theta$.

% To build an intuition, the following theorem gives a sufficient condition for ReB to outperform ReM when we only use the first moment of prior distribution, which is a special case in Remark \ref{remark1}.

% \begin{theorem}\label{point-estimation}
% Under the Bayesian setting with $\pi$ as the prior distribution of $\bbeta_{\infty}$, suppose $\bLambda_{\pi} = \bmu_{\pi}\bmu_{\pi}^T$, i.e., $\bm\Sigma_{\pi} = \bm 0$. Let the angle between $\bmu_{\pi}$ and $\bbeta_{\infty}$ be $\theta \in [0,\pi]$ and the maximum eigenvalue of $\bm V_{xx,\infty}$ be $\lambda_{\max}$. For any given $\bmu_{\pi}$, there exists $0 < \theta^* < \arctan\left(\sqrt{\bm \mu_{\pi}^T \bm V_{xx,\infty} \bm \mu_{\pi}/\lambda_{\max}}\right) \in (0,\pi/2)$ such that 
% $\theta^*$ is a root of 
% \begin{equation}
%   \left(\frac{\cos\theta \cdot \sqrt{v_{\alpha,1} \cdot \bm \mu_{\pi}^T \bm V_{xx,\infty} \bm \mu_{\pi}} + \sin\theta\cdot \sqrt{\lambda_{\max}}}{\cos\theta \cdot \sqrt{\bm \mu_{\pi}^T \bm V_{xx,\infty} \bm \mu_{\pi}} - \sin\theta\cdot \sqrt{\lambda_{\max}}}\right)^2 = v_{\alpha,p}.
% \end{equation}
% If $\theta$ satisfies $\min\{\theta,\pi-\theta\} < \theta^*$, we have
% $v_{\alpha,\pi} < v_{\alpha,p}$, which means that ReB outperforms ReM.
% \end{theorem}

\begin{corollary}\label{cor:outperformReM}
Under the same setting as in Thm.~\ref{thm:ReB2sigma0} with $\theta \in [0,\pi]$ standing for the angle between $\big(\bV_{xx,\infty}^{1/2}\big)^T\bmu_{\pi}$ and $\big(\bV_{xx,\infty}^{1/2}\big)^T\bbeta_{\infty}$. 
ReB $\phi_\pi$ outperforms ReM $\phi_M$ if and only if  \begin{equation}
(\cos\theta)^2 > \frac{1-v_{\alpha,p}}{1-v_{\alpha,1}}.
\end{equation}
\end{corollary}

Corollary \ref{cor:outperformReM} implies that once the probability mass of the prior distribution $\pi$ concentrates in a cone centered in the specific direction along $\bbeta_{\infty}$ (after the transformation according to $\bV_{xx,\infty}^{1/2}$), ReB would benefit from the prior information and outperform ReM.
%for a given $\bmu_{\pi}$, the angle between $\bmu_{\pi}$ and $\bbeta_{\infty}$ after transformation of $\bV_{xx,\infty}^{1/2}$ being sufficiently close to $0$ or $\pi$ is a sufficient and necessary condition for ReB to outperform ReM. 
%That means if the estimated $\bbeta_{\infty}$ is close to the real one, we would expect ReB to benefit from the prior information.
Figure \ref{fig:theta} illustrates how the ratio between the opening angle of the cone (when restricted on $[0,\pi/2]$) and $\pi/2$ changes with $\alpha$ and $p$ when $\bV_{xx,\infty}$ is an identity matrix.
From the figure, we can see that as the dimensionality $p$ gets larger, the admissible value range of $\theta$ enlarges to a wider region spanning from 0 to nearly $\pi/2$, indicating that the superiority of ReB over ReM in practice becomes less sensitive to the mis-specification of $\bbeta_\infty$ as $p$ gets larger.
Such a result gives us the confidence that ReB would be a useful re-randomization procedure in practice as long as our prior knowledge about $\bbeta_\infty$ is not that misleading.

% {\red [comment by DK: we need a figure to illustrate how the admissible angle between $\bmu_{\pi}$ and $\bbeta_{\infty}$ changes with $\alpha$ and $p$.]}

\begin{figure}
\centering
\includegraphics[width=3in, height=2.25in]{theta.png}
\caption{Largest angle between prior vector $\bmu_{\pi}$ and $\bbeta_{\infty}$ after transformation according to $\bV_{xx,\infty}^{1/2}$ that guarantees out-performance of ReB over ReM for varying covariate dimension $p$ and logarithm acceptance probability $\alpha$.}
\label{fig:theta}
\end{figure}


% To further simplify the case, the following Corollary gives an explicit form of $v_{\alpha,\pi}$ in a special case.

% \begin{corollary}\label{cor:I_p}
% Under the setting in Theorem \ref{point-estimation}, suppose $\bV_{xx,\infty} = \sigma^2\cdot \bm I_p$ and the angle between $\bmu_{\pi}$ and $\bbeta_{\infty}$ is $\theta \in [0,\pi]$. Then 
% \begin{equation}
%     v_{\alpha,\pi} = 1-(1-v_{\alpha,1})(\cos\theta)^2,
% \end{equation}
% which is decreasing in $(\cos\theta)^2$. Moreover, ReB outperforms ReM if and only if 
% \begin{equation}
% (\cos\theta)^2 > \frac{1-v_{\alpha,p}}{1-v_{\alpha,1}}.
% \end{equation}
% \end{corollary}

% This Corollary shows that if the influence of $\bV_{xx,\infty}$ on each direction is the same, closer the positional relationship  between $\bmu_{\pi}$ and $\bbeta_{\infty}$ is to collinearity, greater the advantage of ReB over ReM is.


\section{Implementing ReB via a two-stage experiment}\label{sec:Two-stage}
The theoretical analysis in the previous section shows that ReB is a more efficient re-randomization procedure than ReM when the prior distribution $\pi$ is informative to highlight $\bbeta_\infty$.
In case that such an informative prior is not available in advance, we still can implement ReB if a two-stage experiment is feasible, where the first stage involves a small pilot randomized experiment  with $N_1$ randomly selected units to learn an informative prior for the unknown $\bbeta_{\infty}$, while the second stage implements a larger-scale re-randomized experiment with other $N_2$ randomly selected units via ReB guided by the learned prior in pursuit of a more efficient causal estimator. 

% \subsection{Integrated estimator}
%We assume the sample sizes in the first and second stage of our experiment are $N_1$ and $N_2$, respectively, and the total sample size is $N=N_1+N_2$. 
%Denote $\hat{\tau}$ as the difference-in-means estimator of treatment effect when we conduct a complete randomization on the total $N$ units. 
%The procedure of the two-stage ReO is as follows.

In the pilot experiment in the first stage, we implement a balanced completely randomized experiment to get a preliminary causal effect estimator
\begin{equation}\label{eq:tau1}
    \hat{\tau}_1 = \bar{Y}_T^{(1)}-\bar{Y}_C^{(1)}
\end{equation}
satisfying 
\begin{equation}
    \bbE\left(\hat{\tau}_1\right) = \tau,\ 
    \bbV_a\big(\sqrt{N_1}(\hat{\tau}_1-\tau)\big) = \bbV_a\big(\sqrt{N}(\hat{\tau}-\tau)\big),
\end{equation}
where $\bar{Y}_T^{(1)}$ and $\bar{Y}_C^{(1)}$ are average responses in the two arms of the first-stage experiment, and $\hat{\tau}$ is the difference-in-means estimator of treatment effect when we conduct a complete randomization on the total $N=N_1+N_2$ units.
Moreover, based on the data collected in the first-stage experiment, $\bbeta_{\infty}$ can be inferred by a parallel regression between the responses and covariates of the two arms, leading to the informative prior distribution of $\bbeta_{\infty}$ below:
%Denote $\bD^{(1)}$ as the difference-in-means of covariates in the two arms of the first-stage experiment, then the projection of $\hat{\tau}_1-\tau$ on $\bD^{(1)}$, denoted as $\hat{\bbeta}_{N_1}$, will converge to $\bbeta_{\infty}$ when $N_1$ goes to infinity under Condition \ref{cond1}. 
%As $\hat{\bbeta}_{N_1}$ may deviate from the true $\bbeta_{\infty}$, we adopt the estimated regression variance of $\hat{\bbeta}_{N_1}$, denoted as $\widehat{\bbV}\left(\hat{\bbeta}_{N_1}\right)$, as the prior variance of $\bbeta_{\infty}$, and
% Let $\bm X_{N_1}$ be the matrix of covariates in the first stage, augmenting it with and $\bW = (W_1,...,W_{N_1})^T$ and a vector of $1$ in the first two columns leads to matrix $\bm X_1$. Then the regression variance of $\left(\hat{\tau},\hat{\beta}_0,\hat{\bbeta}_{N_1}^T\right)^T$ is 
%% $\left(\bm X_1^T \bm X_1\right)^{-1}$
%the prior of $\bbeta_{\infty}$ is
\begin{equation}\label{prior}
    \pi_{N_1} = N\big(\hat{\bbeta}_{N_1},\widehat{\bbV}\big(\hat{\bbeta}_{N_1}\big)\big),
\end{equation}
where $\hat{\bbeta}_{N_1}$ is the estimated regression coefficient and $\widehat{\bbV}\big(\hat{\bbeta}_{N_1}\big)$ is the corresponding estimated variance.
The characteristic matrix of $\pi_{N_1}$ is $\bLambda_{\pi_{N_1}}=\hat{\bbeta}_{N_1}\hat{\bbeta}_{N_1}^T + \widehat{\bbV}\big(\hat{\bbeta}_{N_1}\big)$.

In the second stage, we implement ReB on the other $N_2$ units with the $\pi_{N_1}$ derived in the first stage as the prior distribution of $\bbeta_\infty$, leading to a Bayesian re-randomization procedure referred to as $\phi_{\pi_{N_1}}$ 
%Hereinafter, we refer to the ReB criterion in the second stage with $\pi_{N_1}$ being the prior as $\phi_{\pi_{N_1}}$. 
with the balance criterion below: 
\begin{equation}\label{eq:2stageReO}
\phi_{\pi_{N_1}}\big(\sqrt{N_2}\bD^{(2)},\bV_{xx,N_2}^{(2)}\big) = I\big(N_2\cdot (\bD^{(2)})^T\bLambda_{\pi_{N_1}}\bD^{(2)}\leq  \xi_{\alpha,\blambda_{\pi_{N_1}}}\big),
\end{equation}
where $\bm D^{(2)}$ is the difference-in-means of covariates in the two arms of the second-stage experiment, $\bm V_{xx,N_2}^{ (2)}$ is the sampling variance of $\sqrt{ N_2}\bm X^{ (2)}$, $\bm X^{(2)}$ is the covariate matrix in the second stage
and
$\blambda_{\pi_{N_1}}$ is the vector of eigenvalues of matrix $\bm P_{N_1,{N_2}} = \left(\big(\bV_{xx,N_2}^{(2)}\big)^{1/2}\right)^T \bLambda_{\pi_{N_1}} \big(\bV_{xx,N_2}^{(2)}\big)^{1/2}$. 
This procedure leads to another unbiased causal effect estimator
\begin{equation}\label{eq:tau2}
    \hat{\tau}_2 = \bar{Y}_T^{(2)}-\bar{Y}_C^{(2)}
\end{equation}
satisfying 
\begin{equation}
    \bbE\left(\hat{\tau}_2\right) = \tau,\ 
    \bbV_a\big(\sqrt{N_2}(\hat{\tau}_2-\tau)\big) = \left[1-(1-v_{\alpha,\pi_{N_1}})R^2_{\infty}\right]\bbV_a\big(\sqrt{N}(\hat{\tau}-\tau)\big),
\end{equation}
where $\bar{Y}_T^{(2)}$ and $\bar{Y}_C^{(2)}$ are average responses in the two arms of the second-stage experiment.

Because $\hat{\tau}_1$ and $\hat{\tau}_2$ are both unbiased estimator of $\tau$ and independent of each other (as long as the units in the two stages are randomly sampled from a super-population), we can integrate them into a more efficient unbiased estimator as below:
\begin{equation}
    \Tilde{\tau}(w) = w \hat{\tau}_1 + (1-w)\hat{\tau}_2,
\end{equation}
where $w \in [0,1]$ is a weight that can be tuned to minimize the sampling variance of $\tilde\tau$.
%and $\hat{\tau}_2$. 
Denote the two-stage re-randomization procedure as $\phi_{BCRD\text{-}ReB}$.
When both $N_1$ and $N_2$ are sufficiently large, the sampling variance of $\Tilde{\tau}(w)$ can be well approximated as below:
\begin{equation}\label{eq:asymptotic-variance-2stage}
    \bbV\left(\Tilde{\tau}(w)\mid \phi_{BCRD\text{-}ReB} = 1 \right) 
    = w^2\cdot \bbV \left(\hat{\tau}_1\right)  + (1-w)^2\cdot\bbV \left(\hat{\tau}_2\mid\phi_{\pi_{N_1}}=1\right),
\end{equation}
which reaches its minimum when $w$ is specified to 
\begin{equation}\label{eq:w1}
    w^* = \frac{\bbV \left(\hat{\tau}_2|\phi_{\pi_{N_1}}=1\right)}{\bbV \left(\hat{\tau}_1\right) + \bbV \left(\hat{\tau}_2\mid\phi_{\pi_{N_1}}=1\right)}
    \approx \frac{N_1}{N_1+N_2/\left[1-(1-v_{\alpha,\pi_{N_1}})R^2_{\infty}\right]}.
\end{equation}
Similarly, BCRD in the first stage can be replaced by ReM, resulting in an alternative two-stage procedure referred to as $\phi_{ReM\text{-}ReB}$, whose optimal weight is 
\begin{equation}\label{eq:w2}
    w^*
    \approx \frac{N_1/\left[1-(1-v_{\alpha,p})R^2_{\infty}\right]}{N_1/\left[1-(1-v_{\alpha,p})R^2_{\infty}\right]+N_2/\left[1-(1-v_{\alpha,\pi_{N_1}})R^2_{\infty}\right]}.
\end{equation}

We note that the optimal weights in Eq.~\eqref{eq:w1} and \eqref{eq:w2} cannot be directly used for causal effect estimation, because they both depend on unknown quantities, such as $\bbeta_{\infty}$, $R^2_{\infty}$ and $\bm V_{xx,\infty}$, as highlighted in \eqref{eq:v_alpha_pi}. 
In practice, we can use the estimation of $w^*$ referred to as $\hat w^*$ instead. Hereinafter, we refer to $\Tilde{\tau}(\hat{w}^*)$ as $\Tilde{\tau}_{N_1,N_2}$ to emphasize its nature as a two-stage estimator. 
The following theorem gives the PRIASV of $\Tilde{\tau}_{N_1,N_2}$ under BCRD-ReB and ReM-ReB when $\hat{w}^*$ is a consistent estimator of $w^*$.
%in that $\lim_{N_1,N_2 \rightarrow \infty}\hat{w}^* = \lim_{N_1,N_2 \rightarrow \infty} w^*$.

%$v_{\alpha,\pi_{N_1}}$ and $R^2_\infty$ by their estimations
% {\red 
% In practice, we replace them with $R^2_{N_1}$ and 
% \[\begin{split}
%     \hat{v}_{\alpha,\pi_{N_1}} = \frac{\hat{\bbeta}_{N_1}^T \left(\bm M_{N_1}^{-1}\right)^T\bm\Gamma^T_{N_1}
% \bm C_{N_1}\bm\Gamma_{N_1} \bm M_{N_1}^{-1}\hat{\bbeta}_{N_1}}{\hat{\bbeta}_{N_1}^T \bm V_{xx,N}\hat{\bbeta}_{N_1}},
% \end{split}\]
% where $\hat{\bbeta}_{N_1}$ is least square estimator of $\bbeta_{\infty}$ using the $N_1$ units in first stage, $\bLambda_{\pi_{N_1}} = \bm M_{N_1} \bm M_{N_1}^T$, $\bm\Gamma_{N_1}$ is an orthogonal matrix such that $\bm M_{N_1}^T \bm V_{xx,N} \bm M_{N_1} =\bm\Gamma^T_{N_1} diag\{\lambda_{1},\ldots,\lambda_{p}\} \bm\Gamma_{N_1}$, $\blambda = (\lambda_{1},...,\lambda_{p})^T$ is the vector of eigenvalues of $$\bm P=\left(\bV_{xx,N}^{1/2}\right)^T\bLambda_{\pi_{N_1}}\bV_{xx,N}^{1/2},$$
% and
% $\bm C_{N_1} = diag\{c_1,\ldots,c_p\}$ with 
% $c_i = \bbE\left(\lambda_{i}Z_i^2\mid \sum_{j=1}^p \lambda_{j} Z_j^2 \leq \xi_{\alpha,\blambda}\right)$ for $i=1,\ldots,p$ and $Z_1,...,Z_p$ are i.i.d. standard normal distributed random variables.
% The estimated weight is denoted as $\hat{w}^*$.
% }


\begin{theorem}\label{2stageReO-variance}
Suppose Condition \ref{cond1} is satisfied, acceptance rate in the second stage of BCRD-ReB is $\alpha_1$ and that in the two stages of ReM-ReB are $\alpha_2$ and $\alpha_3$, respectively.
When $N_1$ and $N_2$ both go to infinity, $\frac{N_1}{N}=\frac{N_1}{N_1+N_2}$ converges to a constant $\varrho$, and 
%$\lim_{N_1,N_2 \rightarrow \infty}\hat{w}^* = \lim_{N_1,N_2 \rightarrow \infty} w^*$,
$\hat{w}^*$ and $w^*$ converge to the same limit, 
the PRIASV of $\Tilde{\tau}_{N_1,N_2}$ under BCRD-ReB is 
\begin{equation}\label{eq:asymptotic-variance2}
\text{PRIASV}_{BCRD\text{-}ReB}=100\times \left(1-
  \frac{1}{\varrho + \frac{1-\varrho}{1-(1-v_{\alpha_1,1})R^2_{\infty}}}\right).
\end{equation}
Similarly, the PRIASV of  $\Tilde{\tau}_{N_1,N_2}$ under ReM-ReB is 
\begin{equation}\label{eq:asymptotic-variance3}
  \text{PRIASV}_{ReM\text{-}ReB}=100\times \left(1-
  \frac{1}{\frac{\varrho}{1-(1-v_{\alpha_2,p})R^2_{\infty}} + \frac{1-\varrho}{1-(1-v_{\alpha_3,1})R^2_{\infty}}}\right).
\end{equation}
\end{theorem}

It is easy to see that both two-satge ReB procedures are more efficient than complete randomization. ReM-ReB is more efficient than BCRD-ReB when the acceptance rates in the second stage of both methods are the same, i.e., $\alpha_1=\alpha_3$. 
A simple way to construct a consistent $\hat{w}^*$ is to substitute $\bbeta_{\infty}$, $R^2_{\infty}$ and $\bm V_{xx,\infty}$ in $w^*$ by their finite sample estimators. 
The procedure and corresponding proofs are detailed in the Supplementary Material.

The following corollary is a simple application of the above analysis and it shows that if the proportion of sample size in first stage is small, BCRD-ReB and ReM-ReB with $\alpha_1=\alpha_3=\alpha$ are asymptotically equivalent to ReO with acceptance rate $\alpha$.

\begin{corollary}\label{N1infinity}
Under the same settings in Theorem \ref{2stageReO-variance}, if the acceptance rates in ReO, namely $\alpha$, is the same as acceptance rate in the second stage of BCRD-ReB and ReM-ReB, i.e., $\alpha=\alpha_1=\alpha_3$, then
\begin{equation}
    \text{PRIASV}_{BCRD\text{-}ReB}=\text{PRIASV}_{ReM\text{-}ReB}=\text{PRIASV}_{ReO},
    %\frac{PRIASV_{BCRD\text{-}ReB}}{PRIASV_{ReO}} \rightarrow 1\quad\text{and}\quad \frac{PRIASV_{ReM\text{-}ReB}}{PRIASV_{ReO}} \rightarrow 1
\end{equation}
as $N_1/N \rightarrow 0$.
\end{corollary}



\section{Simulation studies}\label{sec:Simulation}
% \subsection{Simulation Setting}
In this section, we validate the theoretical analyses in previous sections and compare the practical performance of  different re-randomization procedures, including ReM, ReO, ReB and two-stage ReB, via simulation.
For this purpose, we generated simulated datasets  according to the following linear regression model for potential outcomes:
\begin{equation}\label{eq:LinearModel4Simulation}
Y_i(W_i) = \tau W_i + \bbeta^T \bX_i + \epsilon_i,
\end{equation}
where the average causal effect $\tau=5$, covariates $\bX\sim N(\bmu_\bx,\bSigma_\bx)$, regression coefficient $\bbeta\sim\pi(\bbeta)=N(\bbeta_0,\bSigma_\bbeta)$, and $\epsilon_i\sim N(0,\sigma^2_\bepsilon)$.
In practice, we specify 
$$\bmu_\bX=\boldsymbol{0}_p,\ \bSigma_\bX=(1-\rho)\boldsymbol{I}_p+\rho \boldsymbol{1}_p\boldsymbol{1}_p^T,\ \bbeta_0=\boldsymbol{1}_p,\ \bSigma_\bbeta=\sigma^2_\bbeta\cdot\bI_p,$$
with $p\in \{2,5,10,20\}$, $\rho\in \{0,0.2,0.5\}$, $\sigma^2_\bbeta\in\{0,0.01,1\}$.
%, and sample size $N\in\{100,200,600,1200\}$, 
%leading to a configuration space $\cS$ containing $4\times 3\times 2\times 3\times 4=288$ combinations of the 5 controlling factors $(p,\rho,\sigma^2_\bbeta,\bar{R}^2,N)$.
For each combination of $(p,\rho,\sigma^2_\bbeta)$, we specifying $\sigma^2_\bepsilon$ properly to tune the average value of $R^2$ in the datasets referred to as $\bar{R}^2\in\{0.2, 0.5, 0.8\}$, and choose sample size $N\in\{100,200,600,1200\}$, leading to a configuration space $\cS$ containing $4\times 3\times 3\times 3\times 4=432$ combinations of the 5 controlling factors $(p,\rho,\sigma^2_\bbeta,\bar{R}^2,N)$.
Table \ref{tab:Simu_Factors} summarizes these controlling factors with an additional scheme factor representing different re-randomization procedures of interest.
For each simulation configuration in $\cS$, we randomly generate 100 independent datasets, leading to 43,200 simulated datasets in total.
In this section, we will conduct simulation studies based on all or part of these simulated datasets.
Note that every simulated dataset is associated with a specific regression coefficient vector $\bbeta$ randomly sampled from $\pi(\bbeta)$.


\begin{table}
\footnotesize
    \centering
    \caption{Six major factors in the simulation study.}
    \setlength{\tabcolsep}{5.5mm}{
    \begin{tabular}{cll}
    \midrule
    Factor & Levels & Description  \\
    \hline
    $p$ & \{2, 5, 10, 20\} & Dimensionality of $\bX$\\
    $\rho$ & \{0, 0.2, 0.5\} & Correlation coefficient of $\bX$\\
    $\sigma^2_\bbeta$ & $\{0, 0.01, 1\}$ & Variance of $\pi(\bbeta)$\\
    $\bar{R}^2$ & \{0.2, 0.5, 0.8\} & Noise level of regression model\\
    $N$ & \{100, 200, 600, 1200\}  & Total sample size\\
    %$r$ & \{1,2\} & Ratio of sample size\\
    %$\bbeta_0$ &\{$\boldsymbol{1}_p, (\boldsymbol{1}_{p/2}, 2\cdot\boldsymbol{1}_{p/2})$\} & Coefficient mean vector \\
    %$tp$ & \{0.1,0.2,0.5\} & Train proportion in 2-stage experiment\\
    $\mbox{Scheme}$ & \{ReM, ReO, ReB\} & Re-randomization procedures\\
    \midrule
    \end{tabular}
    }
    \label{tab:Simu_Factors}
\end{table}

\subsection{ReO and ReB versus ReM}\label{sec:onestage}
First, we study the performance of ReO and ReB versus ReM. 
%For this purpose, we generate covariates $\bX$ from a Gaussian distribution $N(\bmu_\bx,\bSigma_\bx)$, and the potential outcomes $Y(0)$ and $Y(1)$ according to the following linear regression model 
%\begin{equation}\label{eq:LinearModel4Simulation}
%Y_i(W_i) = \tau W_i + \bbeta^T \bX_i + \epsilon_i,
%\end{equation}
%with $\tau=5$, 
%$\bbeta\sim\pi(\bbeta)=N(\bbeta_0,\bSigma_\bbeta)$, and $\epsilon_i\sim N(0,\sigma^2_\bepsilon)$.
%In practice, we specify 
%$$\bmu_\bX=\boldsymbol{0}_p,\ \bSigma_\bX=(1-\rho)\boldsymbol{I}_p+\rho \boldsymbol{1}_p\boldsymbol{1}_p^T,\ \bbeta_0=\boldsymbol{1}_p,\ \bSigma_\bbeta=\sigma^2_\bbeta\cdot\bI_p,$$
%with $p\in \{2,5,10,20\}$, $\rho\in \{0,0.2,0.5\}$, and $\sigma^2_\bbeta\in\{1/1000,1\}$.
%For each combination of $(p,\rho,\sigma^2_\bbeta)$, we set $\bar{R}^2$, the average value of $R^2$ in the datasets, to 0.2, 0.5 or 0.8 by specifying $\sigma^2_\bepsilon$ properly, and choose sample size $N\in\{100,200,600,1200\}$.
%The $4\times 3\times 2\times 3\times 4=288$ combinations of the 5 controlling factors $(p,\rho,\sigma^2_\bbeta,\bar{R}^2,N)$ form the configuration space $\cS$ of the simulation study to control the data generating procedure.
%Table \ref{tab:Simu_Factors} summarizes these controlling factors with an additional scheme factor representing different re-randomization procedures of interest.
%For each simulation configuration in $\cS$, we randomly generate 100 independent datasets, leading to 28,800 simulated datasets in total.
%Apparently, every simulated dataset is associated with a specific regression coefficient vector $\bbeta$ randomly sampled from $\pi(\bbeta)$.
%\begin{table}
%    \centering
%    \caption{Six major factors in the simulation study}
%    \setlength{\tabcolsep}{5.5mm}{
%    \begin{tabular}{cll}
%    \midrule
%    Factor & Levels & Description  \\
%    \hline
%    $p$ & \{2, 5, 10, 20\} & Dimensionality of $\bX$\\
%    $\rho$ & \{0, 0.2, 0.5\} & Correlation coefficient of $\bX$\\
%    $\sigma^2_\bbeta$ & $\{1/1000, 1\}$ & Variance of $\bbeta$\\
%    $\bar{R}^2$ & \{0.2, 0.5, 0.8\} & Noise level of regression model\\
%    $N$ & \{100, 200, 600, 1200\}  & Total sample size\\
%    $\mbox{Scheme}$ & \{ReM, ReO, ReB\} & Re-randomization procedures\\
%    \midrule
%    \end{tabular}
%    }
%    \label{tab:Simu_Factors}
%\end{table}
Applying the 3 different randomization schemes of interest, i.e., ReO, ReB and ReM, to these simulated datasets, we could compare their performance on reducing the estimation variance of $\hat{\tau}$ with respect to BCRD (balanced complete randomized design) in a quantitative way.
In the experiment, we fix the acceptance probability $\alpha=0.05$ for all schemes, and always assume that the covariate dimensionality $p$ and sample size $N$ are known to the investigator and the correlation $\rho$ is estimable according to observed covariates before experiment implementation. 
The dataset-specific $\bbeta$ is treated as a known vector in ReO.
In ReB, the dataset-specific $\bbeta$ itself is unknown to the investigator, but its sampling distribution $\pi(\bbeta)$ is fed to the algorithm as the prior distribution of $\bbeta$.
ReM, however, only takes the simulated data as input, without digesting any additional information about $\bbeta$ itself or its sampling distribution.

For the $j$-th simulated dataset $\cD_{ij}$ under the $i$-th simulation configuration in $\cS$, the estimation variance $\bbV(\hat\tau\mid\phi=1)$ of a re-randomization procedure $\phi$ is estimated based on 500 accepted allocations from $\phi$ (referred to as $\widehat\bbV_{ij}(\hat\tau\mid\phi=1)$).
Based on the estimated $\widehat\bbV_{ij}(\hat\tau\mid\phi=1)$, PRIV achieved by $\phi$ with respect to BCRD can be obtained for $\cD_{ij}$ (denoted as PRIV$_{ij}(\phi)$), and the average performance of a re-randomization procedure $\phi$ in simulation setting $i\in\cS$ can be summarized by
\begin{equation}\label{eq:PRIV_i_phi}
    \mbox{PRIV}_{i}(\phi)=\frac{1}{100}\sum_{j=1}^{100} \mbox{PRIV}_{ij}(\phi),
\end{equation}
which is equivalent to averaging the PRIV by prior of $\bbeta$. 
% Hereinafter, we refer to this average PRIV as prior-integrated PRIV.


%\subsection{Simulation Results}
% \subsection{ANOVA for PRIV}

To investigate the contribution of the 6 different factors listed in Table \ref{tab:Simu_Factors} to PRIV of $\hat\tau$, we conducted an ANOVA for the estimated $\{$PRIV$_{ij}(\phi)\}$ with respect to these factors and their interactions.
Table \ref{tab:Sim_ANOVA} summarizes the results of ANOVA with the 11 most influential effects to explain the variance of PRIV$_{ij}(\phi)$.
%the PRIV$^*$ of $\hat{\tau}$ explained by factors in our factorial design. 
The residual refers to all remaining effects in ANOVA other than the above 11 most influential effects.
From the table, we can see the following facts.
First, the top 11 factors explain almost all variance of PRIV$_{ij}(\phi)$, while the top 3 factors explain more than 95\% of the total variance.
Second, $\bar{R}^2$ is the most influential factor for explaining the variance of PRIV, followed by scheme, $\sigma^2_\bbeta$, and their interactions, suggesting that the selection of re-randomization procedure does make difference, although $\bar{R}^2$ is the dominating factor.
Third, sample size $N$ does not show up in the table, indicating that the re-randomization procedures involved are not sensitive to the sample size.
All these facts are consistent to the theoretical results established in Section \ref{sec:ReB}.

\begin{table}[t]
    \centering
    \footnotesize
    \caption{The 11 most influential effects for the ANOVA simulation results.}
    % \resizebox{\linewidth}{!}{
    % }
    \setlength{\tabcolsep}{5.5 mm}{
    \begin{tabular}{lcccc}
    \midrule
    Sources & DF & MS & CP & $F$-value\\
    \hline
    $\bar{R}^2$ & 2 & 3007.51 & 88.61 & 315885.74\\
    $\mbox{Scheme}$ & 2 & 197.98 & 94.44 & 20794.10 \\
    $\sigma^2_\bbeta$ & 2 & 68.37 & 96.46 & 7180.89\\
    $\mbox{Scheme} \times p$ & 6 & 36.57 & 97.54 & 3841.12\\
    $\rho$ & 2 & 27.14 & 98.34 & 2850.94\\
    $\mbox{Scheme} \times \bar{R}^2$ & 4 & 21.08 & 98.96 & 2214.59\\
    $p$ & 3 & 4.88 & 99.10 & 512.40\\
    $\bar{R}^2\times \sigma^2_\bbeta$ & 4 & 4.60 & 99.24 & 483.67\\
    $\mbox{Scheme}\times\sigma^2_\bbeta$ & 4 & 4.23 & 99.36 & 444.04\\
    % $\sigma^2_\bbeta\times p$ & 3 & 3.08 & 99.32 & 291\\
    $p \times \rho$ & 6 & 4.09 & 99.48 & 429.92\\
    $\rho \times \sigma^2_{\bbeta}$ & 4 & 3.73 & 99.59 & 392.13\\
    \hline
    Residual & 129560 & 13.80 & 100 & \\
    \midrule
    \multicolumn{5}{l}{\footnotesize DF=``degrees of freedom'', MS=``mean square'' and CP=``cumulative percentage of MS ($\%$)''}
    \end{tabular}
    }
    \label{tab:Sim_ANOVA}
\end{table}


% \begin{table}
%     \centering
%     \caption{The 11 most influential effects for the ANOVA simulation results.}
%     % \resizebox{\linewidth}{!}{
%     % }
%     \setlength{\tabcolsep}{5.5 mm}{
%     \begin{tabular}{lcccc}
%     \midrule
%     Sources & DF & MS & CP & $F$-value\\
%     \hline
%     $\bar{R}^2$ & 2 & 1271.44 & 79.68 & 130000\\
%     $\mbox{Scheme}$ & 2 & 191.79 & 91.70 & 19600 \\
%     $\sigma^2_\bbeta$ & 1 & 49.49 & 94.81 & 5060\\
%     $\mbox{Scheme} \times p$ & 6 & 21.86 & 96.18 & 2240\\
%     $\bar{R}^2 \times \mbox{Scheme}$ & 4 & 21.08 & 97.50 & 2160\\
%     $\bar{R}^2\times\sigma^2_\bbeta$ & 2 & 9.66 & 98.10 & 988\\
%     $\rho$ & 2 & 6.35 & 98.50 & 650 \\
%     $\bar{R}^2\times \sigma^2_\bbeta$ & 2 & 6.23 & 98.89 & 637\\
%     $p$ & 3 & 4.03 & 99.14 & 412\\
%     $\sigma^2_\bbeta\times p$ & 3 & 2.85 & 99.32 & 291\\
%     $\mbox{Scheme}\times p\times \bar{R}^2$& 12&2.15 & 99.46 & 220 \\
%     \hline
%     Residual & 64603 & 8.67 & 100 & \\
%     \midrule
%     \multicolumn{5}{l}{\footnotesize DF=``degrees of freedom'', MS=``mean square'' and CP=``cumulative percentage of MS ($\%$)''}
%     \end{tabular}
%     }
%     \label{tab:Sim_ANOVA}
% \end{table}


% \begin{figure}
%     \centering
%     \includegraphics[scale=0.8]{PRIVvsPRIASV.png}
%     \caption{Boxplots of $PRIV_{ij}/PRIASV_{ij}$ for ReO, ReB and ReM when (A) $\sigma_{\bbeta}^2=0.01$ and (B) $\sigma_{\bbeta}^2=1$.}
%     \label{fig:PRIVvsPRIASV}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[scale=0.8]{Acceptance_rate.png}
%     \caption{The realized acceptance probability of ReO, ReB and ReM when (A) $\sigma_{\bbeta}^2=0.01$ and (B) $\sigma_{\bbeta}^2=1$.}
%     \label{fig:Acceptance_rate}
% \end{figure}

Table \ref{tab:SimReB} compares the average PRIV achieved by the 3 competing re-randomization procedures under various simulation settings in detail.
Because sample size $N$ plays an insignificant role based on the ANOVA and a larger $\sigma^2_\bbeta$ typically corresponds to more challenging scenarios for the Bayesian re-randomization procedures, we only report the results for the simulation settings where $N=200$ and $\sigma^2_\bbeta=1$ in the table.
The results in Table \ref{tab:SimReB} confirm the following facts.
First, ReO performs uniformly better than other methods across all simulation settings as we expected.
Second, the Bayesian re-randomization procedure (i.e., ReB) outperforms the classic ReM when dimension of covariates $p$ is not too small, and the comparative advantage increasing steadily with the increase of $p$.
%, which is consistent with the result in Theorem \ref{thm:ReB}.
%Third, ReM-ReB performs better than BCRD-ReB all the time.

\begin{table}[t]
    \centering
    \footnotesize
    \caption{Average PRIV achieved by different re-randomization procedures when $N=200$ and $\sigma^2_\bbeta=1$.}
    \begin{tabular}{cccccccccc}
    \midrule
    & \multicolumn{3}{c}{$\bar{R}^2=0.2$}&\multicolumn{3}{c}{$\bar{R}^2=0.5$}&\multicolumn{3}{c}{$\bar{R}^2=0.8$}\\
    \cmidrule(lr){2-4}
    \cmidrule(lr){5-7}
    \cmidrule(lr){8-10}
    Scheme & $\rho=0$ & 0.2 & 0.5 & 0 & 0.2 & 0.5 & 0 & 0.2 & 0.5 \\\hline
    \multicolumn{10}{c}{}\vspace{-0.2cm}\\
    & \multicolumn{9}{c}{$p=2$}\\\cmidrule(lr){2-10}
    ReO & 18.25 & 18.38 & 18.68 & 42.35 & 42.16 & 42.79 & 68.25 & 67.26 & 69.00 \\   
    ReB  & 18.08 & 18.29 & 17.98 & 41.66 & 41.57 & 41.86 & 66.93 & 66.15 & 67.70 \\
    ReM & 18.33 & 18.53 & 18.33 & 41.79 & 41.60 & 41.92 & 66.92 & 65.96 & 67.56 \\ 
    \cmidrule(lr){2-10}
    \multicolumn{10}{c}{}\vspace{-0.3cm}\\
    & \multicolumn{9}{c}{$p=5$}\\\cmidrule(lr){2-10}
    ReO & 20.70 & 20.44 & 18.40 & 47.04 & 46.82 & 45.47 & 74.84 & 74.11 & 74.68 \\  
    ReB & 17.61 & 18.76 & 16.47 & 42.14 & 44.09 & 40.26 & 67.82 & 70.03 & 65.89 \\
    ReM  & 16.11 & 16.91 & 15.36 & 38.75 & 39.37 & 38.29 & 62.64 & 62.60 & 63.05 \\ 
    %BCRD-ReB &16.8 &16.3 &17.9 &42.3 &43.8 &45.5 &72.9 &73.8 &75.8\\
    %ReM-ReB &17.5 &18.5 &19.4 &45.5 &46.4 &48.4 &74.7 &75.6 &76.9\\
    \cmidrule(lr){2-10}
    \multicolumn{10}{c}{}\vspace{-0.3cm}\\
    & \multicolumn{9}{c}{$p=10$}\\\cmidrule(lr){2-10}
    ReO & 20.71 & 22.90 & 20.03 & 48.65 & 51.34 & 48.73 & 77.52 & 78.82 & 78.25 \\
    ReB  & 17.64 & 21.10 & 15.55 & 42.59 & 48.22 & 38.14 & 67.91 & 74.18 & 61.14 \\ 
    ReM  & 13.29 & 15.09 & 13.80 & 32.61 & 34.69 & 33.54 & 52.81 & 53.85 & 53.79 \\
    %BCRD-ReB &14.1 &14.1 &16.4 &41.8 &42.2 &46.5 &73.7 &74.1 &77.0\\
    %ReM-ReB &16.3 &15.8 &19.6 &44.2 &44.5 &48.7 &74.7 &74.9 &77.7\\
    \cmidrule(lr){2-10}
    \multicolumn{10}{c}{}\vspace{-0.3cm}\\
    & \multicolumn{9}{c}{$p=20$}\\\cmidrule(lr){2-10}
    ReO  & 20.58 & 23.93 & 20.08 & 50.50 & 55.36 & 49.25 & 79.75 & 82.64 & 79.01 \\  
    ReB  & 18.81 & 23.51 & 13.33 & 45.86 & 53.67 & 34.72 & 71.99 & 79.71 & 56.50 \\ 
    ReM  & 11.03 & 12.86 & 10.98 & 27.05 & 29.50 & 26.69 & 42.74 & 44.05 & 42.51 \\
    %BCRD-ReB &12.3 &13.7 &17.4 &40.4 &42.7 &48.3 &72.7 &74.7 &78.5\\
    %ReM-ReB &14.6 &15.3 &19.3 &41.9 &44.5 &49.3 &73.5 &75.3 &79.0\\
    \midrule
    \end{tabular}
    \label{tab:SimReB}
\end{table}

% \subsection{Detailed comparison of ReO and ReB versus ReM}
To verify Theorem \ref{PRIAV-ReO} and compare the performance of ReO and ReM, Figure \ref{fig:Sim1ReO} visualizes the average PRIV of both methods as a function of covariate dimensionality $p$ under various specifications of $\bar{R^2}$ when $\rho=0$, $\sigma^2_\bbeta=0.01$ and $N=600$.
From the figure we can see that the average PRIV increases significantly with the increase of $\bar{R}^2$ for both methods.
For each fixed $\bar{R}^2$, the average PRIV decreases with the increase of $p$ in ReM, while keeps unchanged with respect to $p$ in ReO.
These results match the theoretical prediction by Theorem \ref{PRIAV-ReO} well.

\begin{figure}[ht]
    \centering    \includegraphics[scale=0.7]{ReOvsReM.png}
    \caption{ReO versus ReM in terms of average PRIV for different $p$ and $\bar{R}^2$ when $N=600$ and $\sigma^2_\bbeta=0.01$.}
    \label{fig:Sim1ReO}
\end{figure}

To investigate how the comparative advantage of ReB over ReM depends on different factors, we illustrate in Figure \ref{fig:Sim_Boxplot} the PRIV ratio of ReB over ReM when sample size $N$ is fixed at 600 and $\bar{R}^2$ is fixed at $0.5$ for different $p$ and $\rho$ with $\sigma^2_{\bbeta}=0.01$ and $\sigma^2_{\bbeta}=1$, respectively.
The box-plots are based on the 100 repeat experiments under each simulation setting.
From the figure, we can see that majority of the PRIV ratios are larger than 1 in all settings even when the variance of $\bbeta$ is large, and the proportion of the exceptions decreases quickly with the increase of $\rho$ and $p$.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.7]{ReBvsReM_mix.png}
    \caption{The PRIV ratio of ReB over ReM when sample size $N$ is fixed at 600 and $\bar{R}^2$ fixed at 0.5 for different $p$ and $\rho$ with (A) $\sigma^2_{\bbeta} = 0.01$ and (B) $\sigma^2_{\bbeta} = 1$.}
    \label{fig:Sim_Boxplot}
\end{figure}

To validate the re-randomization procedures we proposed, additional studies on the precision of the realized PRIV and acceptance probability of different re-randomization procedures are present in Section \ref{sec:validation}  of the Supplementary Material. 
The ratio of the realized value to their corresponding theoretical value are plotted. 
The results indicate that as sample size $N$ gets larger, PRIV and acceptance probability of the three re-randomization procedures get closer to the theoretical values. 
Our findings also suggest that ReO and ReB demonstrate superior performance compared to ReM.


\subsection{Two-stage ReB versus ReM and ReO}
Next, we study the performance of two-stage ReB procedures, i.e., BCRD-ReB and ReM-ReB, versus ReO and ReM. 
Moreover, we refer to two simpler forms of two-stage ReB which use $\bLambda_{\pi_{N_1}} = \hat{\bbeta}_{N_1}\hat{\bbeta}_{N_1}^T$ in the second stage as BCRD-ReO and ReM-ReO, and evaluate their performance as well.
% In two-stage procedures, we simplify the computation with $v_{\alpha,\pi_{N_1}}$ approximated by $v_{\alpha,1}$, which makes no difference to the asymptotic properties. 

% For fair comparison, we choose acceptance probabilities of different approaches carefully so that computational cost are comparable across them.  
% Suppose the sample sizes in two stages are $N_1$ and $N_2$, respectively, with $N=N_1+N_2$. 
% The following lemma gives the formula of acceptance probabilities.
% \begin{lemma}\label{lemma5}
% Suppose the acceptance probability of ReM and ReO is $\alpha$, acceptance probabilities in the second stage of BCRD-ReB and BCRD-ReO are $\alpha_1$, in both stages of ReM-ReB and ReM-ReO are $\alpha_2$.
% %Denote the proportion of sample in first stage as $r$, 
% If the acceptance probabilities are chosen according to the following equations, the two-stage ReB methods have the same computational cost as ReM and ReO when both $N_1$ and $N_2$ are large enough:
% \begin{align}
%     \label{AP1}
%     \alpha_1 &= \frac{(2p+1)(1-r)}{(2p+1)/\alpha-(4p^2+12p+17)r},\\
%     \alpha_2 &= \frac{2p+1}{(2p+1)/\alpha-(4p^2+12p+16)r}, \label{AP2}
% \end{align}
% \begin{align}
%     \label{AP1}
%     \alpha_1 &= \frac{(2p+1)(1-r)}{(2p+1)/\alpha-(2p^2+14p+21)r},\\
%     \alpha_2 &= \frac{2p+1}{(2p+1)/\alpha-(2p^2+14p+20)r}, \label{AP2}
% \end{align}
% where $r$ is the proportion of sample in the first stage.
% \end{lemma}

% We denote the above methods with given acceptance probabilities as ReM($\alpha$), ReO($\alpha$), 
% BCRD-ReO($\alpha_1$), BCRD-ReB($\alpha_1$), ReM-ReO($\alpha_2$) and ReM-ReB($\alpha_2$). 
% A direct deduction from Lemma~\ref{lemma5} tells us that as the proportion of samples in first stage $r=N_1/N \rightarrow 0$, we have $\alpha=\alpha_1=\alpha_2$. 
% Such a result implies that when sample size is large and the proportion of sample in first stage is small, the two-stage ReB methods would have the same computational cost as ReO and ReM if they have the same acceptance probability. 
% Moreover, based on \eqref{AP1} and \eqref{AP2}, we need to set the following constraint for $\alpha$ to keep $\alpha_1$ and $\alpha_2$ staying in $(0,1)$: 
% \begin{equation*}%\label{eq:alpharange}
%     \alpha < \frac{2p+1}{(4p^2+12p+16)r+2p+1}.
% \end{equation*}
% \begin{equation*}%\label{eq:alpharange}
%     \alpha < \frac{2p+1}{(2p^2+14p+20)r+2p+1}.
% \end{equation*}
For simplicity, we only considered simulated datasets with $\rho=0$, $\sigma^2_\bbeta=0$, $\bar{R}^2 = 0.5$, proportion of sample size in the first stage $r \in \{0.1,0.2,0.4\}$ and sample size $N\geq200$ for numerical comparison here.
We avoided datasets with $N=100$ because the two-stage ReB methods may fail in these cases due to too few samples in the first stage.
%The data generation process is as follows. We use the covariates $\bm X$ generated in section \ref{sec:onestage} with dimensionality $p \in \{2,5,10,20\}$, sample size $N \in \{200,600,1200\}$ and correlation coefficient $\rho=0$. Fix $\bbeta = \bm 1_p$, $\bar{R}^2 = 0.5$ and causal effect $\tau=5$. 
For fair comparison, we set the acceptance probability for ReM to be $\alpha=0.05$ and tune the acceptance probabilities of other re-randomization approaches numerically so that the computational cost, i.e., the running time of sampling 500 assignments, are comparable across them.  
%Then $\alpha_1$ and $\alpha_2$ are computed according to Eq.~\eqref{AP1}-\eqref{AP2}.
%The $4\times3\times 3=36$ combinations of the $3$ controlling factors $(p,N,r)$ form the configuration space $\mathcal{S}$ of the study. For each configuration in $\mathcal{S}$, we randomly generate 100 independent datasets. 
For each involved simulated dataset, PRIV of a specific re-randomization procedure was estimated based on 500 accepted allocations, based on which the average PRIV for each simulation configuration was estimated according to Eq.~\eqref{eq:PRIV_i_phi}. The detailed acceptance rate as well as the average running time of each re-randomization procedure under each configuration is recorded in Section \ref{sec:linear_time} of the Supplementary Material.
%Note that the average PRIV here is defined with respect to one certain $\bbeta$, which is different from the prior-integrated PRIV in subsection \ref{sec:onestage} in meanings. 


Average PRIV of different competing methods under various simulation settings are shown in Table \ref{tab:2stageReB}, where the best RPIV obtained among ReM and two-stage procedures in each configuration are in bold. 
% When $N=200$ and $r=0.1$, two-stage ReB can not be applied and we omit the results in this scenario. 
We have the following observations.
First, ReM-ReB has the best performance among ReM and two-stage procedures in most cases, except when proportion of sample in first stage is large or sample size is small. 
Second, the advantage of two-stage procedures over ReM is more significant when the covariates are in high dimension. 
Third, when sample size is small, BCRD-ReO and ReM-ReO perform much worse than BCRD-ReB and ReM-ReB, implying that the variance of estimated $\bbeta$ introduced by ReB helps improving the estimation of causal effect as the bias of point estimator can be large when sample size is relatively small. 

\begin{table}[ht]
    \centering
    \footnotesize
    \caption{Average PRIV achieved by different re-randomization procedures when $\bar{R}^2=0.5$.}
    \label{tab:2stageReB}
    % \renewcommand\arraystretch{1.0}
    \setlength{\tabcolsep}{2mm}{
% \normalsize
\begin{tabular}{cccccccccc}
    \midrule
    & \multicolumn{3}{c}{$N=200$}&\multicolumn{3}{c}{$N=600$}&\multicolumn{3}{c}{$N=1200$}\\
    \cmidrule(lr){2-4}
    \cmidrule(lr){5-7}
    \cmidrule(lr){8-10}
    Scheme & $r=0.1$ & 0.2 & 0.4 & 0.1 & 0.2 & 0.4 & 0.1 & 0.2 & 0.4 \\\hline
    \multicolumn{10}{c}{}\vspace{-0.2cm}\\
    & \multicolumn{9}{c}{$p=2$}\\\cmidrule(lr){2-10}
    \cellcolor[HTML]{D9D9D9} ReO &\cellcolor[HTML]{D9D9D9} 48.87 &\cellcolor[HTML]{D9D9D9} 48.87 & \cellcolor[HTML]{D9D9D9}48.87 & \cellcolor[HTML]{D9D9D9}49.34 & \cellcolor[HTML]{D9D9D9}49.34 & \cellcolor[HTML]{D9D9D9}49.40 & \cellcolor[HTML]{D9D9D9}49.95 & \cellcolor[HTML]{D9D9D9}49.88 & \cellcolor[HTML]{D9D9D9}49.95 \\     
 ReM-ReB& \textbf{47.73} & \textbf{47.81} & \textbf{48.40} & \textbf{48.96} & \textbf{48.56} & \textbf{48.40} & \textbf{49.49} & \textbf{49.65} & 49.22 \\ 
    ReM-ReO &  46.68 & 47.76 & 48.16 & 48.38 & 48.08 & 47.84 & 49.07 & 49.47 & \textbf{49.37} \\ 
    BCRD-ReB & 45.86 & 42.47 & 35.83 & 46.54 & 43.41 & 36.13 & 47.04 & 44.12 & 38.11 \\
    BCRD-ReO & 43.59 & 42.55 & 36.04 & 46.26 & 43.06 & 35.76 & 46.55 & 43.81 & 37.40 \\ 
    \cellcolor[HTML]{D9D9D9} ReM & \cellcolor[HTML]{D9D9D9}47.28 & \cellcolor[HTML]{D9D9D9}47.28 & \cellcolor[HTML]{D9D9D9}47.28 & \cellcolor[HTML]{D9D9D9}47.57 & \cellcolor[HTML]{D9D9D9}47.57 & \cellcolor[HTML]{D9D9D9}47.57 & \cellcolor[HTML]{D9D9D9}48.38 & \cellcolor[HTML]{D9D9D9}48.38 & \cellcolor[HTML]{D9D9D9}48.38 \\
    \cmidrule(lr){2-10}
    \multicolumn{10}{c}{}\vspace{-0.2cm}\\
    & \multicolumn{9}{c}{$p=5$}\\\cmidrule(lr){2-10}
    \cellcolor[HTML]{D9D9D9} ReO & \cellcolor[HTML]{D9D9D9}49.08 & \cellcolor[HTML]{D9D9D9}49.05 & \cellcolor[HTML]{D9D9D9}49.08 & \cellcolor[HTML]{D9D9D9}49.46 & \cellcolor[HTML]{D9D9D9}49.46 & \cellcolor[HTML]{D9D9D9}49.46 & \cellcolor[HTML]{D9D9D9}49.55 & \cellcolor[HTML]{D9D9D9}49.60 & \cellcolor[HTML]{D9D9D9}49.60 \\   
    ReM-ReB& \textbf{42.78} & \textbf{43.61} & 43.78 & \textbf{46.41} & \textbf{46.93} & 45.28 & \textbf{49.16} & \textbf{47.85} & \textbf{46.81} \\  
    ReM-ReO & 37.04 & 41.95 & \textbf{43.94} & 45.08 & 46.41 & \textbf{45.64} & 48.50 & 47.44 & 46.10 \\ 
    BCRD-ReB & 42.38 & 41.25 & 35.72 & 44.97 & 42.94 & 36.09 & 47.18 & 43.86 & 36.86 \\ 
    BCRD-ReO &  32.49 & 37.42 & 34.97 & 42.90 & 42.45 & 35.48 & 46.16 & 43.36 & 37.47 \\ 
 \cellcolor[HTML]{D9D9D9} ReM &\cellcolor[HTML]{D9D9D9} 40.97 &\cellcolor[HTML]{D9D9D9} 40.97 &\cellcolor[HTML]{D9D9D9} 40.97 &\cellcolor[HTML]{D9D9D9} 41.95 &\cellcolor[HTML]{D9D9D9} 41.95 &\cellcolor[HTML]{D9D9D9} 41.95 &\cellcolor[HTML]{D9D9D9} 42.32 &\cellcolor[HTML]{D9D9D9} 42.32 &\cellcolor[HTML]{D9D9D9} 42.32 \\
    \cmidrule(lr){2-10}
    \multicolumn{10}{c}{}\vspace{-0.2cm}\\
    & \multicolumn{9}{c}{$p=10$}\\\cmidrule(lr){2-10}
    \cellcolor[HTML]{D9D9D9} ReO & \cellcolor[HTML]{D9D9D9}49.37 & \cellcolor[HTML]{D9D9D9}49.37 & \cellcolor[HTML]{D9D9D9}49.47 & \cellcolor[HTML]{D9D9D9}49.00 & \cellcolor[HTML]{D9D9D9}48.98 & \cellcolor[HTML]{D9D9D9}48.92 & \cellcolor[HTML]{D9D9D9}48.62 & \cellcolor[HTML]{D9D9D9}48.50 & \cellcolor[HTML]{D9D9D9}48.62 \\  
    ReM-ReB& 33.07 & \textbf{39.59} & \textbf{41.12} & \textbf{43.44} & \textbf{44.48} & 42.19 & \textbf{45.46} & \textbf{46.13} & \textbf{42.81} \\ 
    ReM-ReO & 23.97 & 35.89 & 39.59 & 40.66 & 43.40 & \textbf{42.30} & 44.42 & 45.36 & 42.58 \\ 
    BCRD-ReB & \textbf{35.31} & 38.04 & 35.43 & 42.27 & 41.69 & 36.10 & 44.27 & 42.61 & 36.22 \\ 
    BCRD-ReO & 21.04 & 31.74 & 31.90 & 38.58 & 40.20 & 35.40 & 42.68 & 42.17 & 35.02 \\ 
    \cellcolor[HTML]{D9D9D9} ReM & \cellcolor[HTML]{D9D9D9}34.29 &\cellcolor[HTML]{D9D9D9} 34.29 & \cellcolor[HTML]{D9D9D9}34.29 & \cellcolor[HTML]{D9D9D9}34.64 & \cellcolor[HTML]{D9D9D9}34.64 & \cellcolor[HTML]{D9D9D9}34.64 & \cellcolor[HTML]{D9D9D9}33.91 & \cellcolor[HTML]{D9D9D9}33.91 & \cellcolor[HTML]{D9D9D9}33.91 \\ 
    \cmidrule(lr){2-10}
    \multicolumn{10}{c}{}\vspace{-0.2cm}\\
    & \multicolumn{9}{c}{$p=20$}\\\cmidrule(lr){2-10}
    \cellcolor[HTML]{D9D9D9} ReO & \cellcolor[HTML]{D9D9D9}50.03 & \cellcolor[HTML]{D9D9D9}50.11 & \cellcolor[HTML]{D9D9D9}50.05 & \cellcolor[HTML]{D9D9D9}49.66 & \cellcolor[HTML]{D9D9D9}49.71 & \cellcolor[HTML]{D9D9D9}49.66 & \cellcolor[HTML]{D9D9D9}49.94 & \cellcolor[HTML]{D9D9D9}49.94 & \cellcolor[HTML]{D9D9D9}50.00 \\  
    ReM-ReB& $--$ & 30.35 & \textbf{35.86} & \textbf{37.19} & \textbf{40.75} & \textbf{39.91} & \textbf{42.99} & \textbf{43.05} & \textbf{41.55} \\ 
    ReM-ReO & $--$ &24.56 & 33.27 & 31.91 & 38.72 & 38.47 & 40.19 & 42.48 & 41.24 \\
    BCRD-ReB & $--$ & \textbf{30.36} & 30.59 & 36.76 & 38.70 & 34.96 & 42.05 & 40.86 & 35.69 \\ 
    BCRD-ReO & $--$ & 21.69 & 25.57 & 30.47 & 35.65 & 33.84 & 39.27 & 39.37 & 34.76 \\ 
    \cellcolor[HTML]{D9D9D9} ReM & \cellcolor[HTML]{D9D9D9}26.39 & \cellcolor[HTML]{D9D9D9}26.39 & \cellcolor[HTML]{D9D9D9}26.39 & \cellcolor[HTML]{D9D9D9}26.67 & \cellcolor[HTML]{D9D9D9}26.67 & \cellcolor[HTML]{D9D9D9}26.67 & \cellcolor[HTML]{D9D9D9}25.61 & \cellcolor[HTML]{D9D9D9}25.61 & \cellcolor[HTML]{D9D9D9}25.61 \\  
    \midrule
    \end{tabular}}
\end{table}

% \newpage
% \begin{table}[t]
%     \centering
%     \caption{Average PRIV achieved by different re-randomization procedures when $\bar{R}^2=0.5$.}
%     \label{tab:2stageReB}
%     % \renewcommand\arraystretch{1.0}
%     \setlength{\tabcolsep}{2mm}{
% % \normalsize
% \begin{tabular}{cccccccccc}
%     \midrule
%     & \multicolumn{3}{c}{$N=200$}&\multicolumn{3}{c}{$N=600$}&\multicolumn{3}{c}{$N=1200$}\\
%     \cmidrule(lr){2-4}
%     \cmidrule(lr){5-7}
%     \cmidrule(lr){8-10}
%     Scheme & $r=0.1$ & 0.2 & 0.4 & 0.1 & 0.2 & 0.4 & 0.1 & 0.2 & 0.4 \\\hline
%     \multicolumn{10}{c}{}\vspace{-0.2cm}\\
%     & \multicolumn{9}{c}{$p=2$}\\\cmidrule(lr){2-10}
%     \cellcolor[HTML]{D9D9D9} ReO &\cellcolor[HTML]{D9D9D9} 48.87 &\cellcolor[HTML]{D9D9D9} 48.87 & \cellcolor[HTML]{D9D9D9}48.87 & \cellcolor[HTML]{D9D9D9}49.34 & \cellcolor[HTML]{D9D9D9}49.34 & \cellcolor[HTML]{D9D9D9}49.40 & \cellcolor[HTML]{D9D9D9}49.95 & \cellcolor[HTML]{D9D9D9}49.88 & \cellcolor[HTML]{D9D9D9}49.95 \\     
%  ReM-ReB& \textbf{47.73} & \textbf{47.81} & \textbf{48.40} & \textbf{48.96} & \textbf{48.56} & \textbf{48.40} & \textbf{49.49} & \textbf{49.65} & 49.22 \\ 
%     ReM-ReO &  46.68 & 47.76 & 48.16 & 48.38 & 48.08 & 47.84 & 49.07 & 49.47 & \textbf{49.37} \\ 
%     BCRD-ReB & 45.86 & 42.47 & 35.83 & 46.54 & 43.41 & 36.13 & 47.04 & 44.12 & 38.11 \\
%     BCRD-ReO & 43.59 & 42.55 & 36.04 & 46.26 & 43.06 & 35.76 & 46.55 & 43.81 & 37.40 \\ 
%     \cellcolor[HTML]{D9D9D9} ReM & \cellcolor[HTML]{D9D9D9}47.28 & \cellcolor[HTML]{D9D9D9}47.28 & \cellcolor[HTML]{D9D9D9}47.28 & \cellcolor[HTML]{D9D9D9}47.57 & \cellcolor[HTML]{D9D9D9}47.57 & \cellcolor[HTML]{D9D9D9}47.57 & \cellcolor[HTML]{D9D9D9}48.38 & \cellcolor[HTML]{D9D9D9}48.38 & \cellcolor[HTML]{D9D9D9}48.38 \\
%     \cmidrule(lr){2-10}
%     \multicolumn{10}{c}{}\vspace{-0.2cm}\\
%     & \multicolumn{9}{c}{$p=5$}\\\cmidrule(lr){2-10}
%     \cellcolor[HTML]{D9D9D9} ReO & \cellcolor[HTML]{D9D9D9}49.08 & \cellcolor[HTML]{D9D9D9}49.05 & \cellcolor[HTML]{D9D9D9}49.08 & \cellcolor[HTML]{D9D9D9}49.46 & \cellcolor[HTML]{D9D9D9}49.46 & \cellcolor[HTML]{D9D9D9}49.46 & \cellcolor[HTML]{D9D9D9}49.55 & \cellcolor[HTML]{D9D9D9}49.60 & \cellcolor[HTML]{D9D9D9}49.60 \\   
%     ReM-ReB& \textbf{42.78} & \textbf{43.61} & 43.78 & \textbf{46.41} & \textbf{46.93} & 45.28 & \textbf{49.16} & \textbf{47.85} & \textbf{46.81} \\  
%     ReM-ReO & 37.04 & 41.95 & \textbf{43.94} & 45.08 & 46.41 & \textbf{45.64} & 48.50 & 47.44 & 46.10 \\ 
%     BCRD-ReB & 42.38 & 41.25 & 35.72 & 44.97 & 42.94 & 36.09 & 47.18 & 43.86 & 36.86 \\ 
%     BCRD-ReO &  32.49 & 37.42 & 34.97 & 42.90 & 42.45 & 35.48 & 46.16 & 43.36 & 37.47 \\ 
%  \cellcolor[HTML]{D9D9D9} ReM &\cellcolor[HTML]{D9D9D9} 40.97 &\cellcolor[HTML]{D9D9D9} 40.97 &\cellcolor[HTML]{D9D9D9} 40.97 &\cellcolor[HTML]{D9D9D9} 41.95 &\cellcolor[HTML]{D9D9D9} 41.95 &\cellcolor[HTML]{D9D9D9} 41.95 &\cellcolor[HTML]{D9D9D9} 42.32 &\cellcolor[HTML]{D9D9D9} 42.32 &\cellcolor[HTML]{D9D9D9} 42.32 \\
%     \cmidrule(lr){2-10}
%     \multicolumn{10}{c}{}\vspace{-0.2cm}\\
%     & \multicolumn{9}{c}{$p=10$}\\\cmidrule(lr){2-10}
%     \cellcolor[HTML]{D9D9D9} ReO & \cellcolor[HTML]{D9D9D9}49.37 & \cellcolor[HTML]{D9D9D9}49.37 & \cellcolor[HTML]{D9D9D9}49.47 & \cellcolor[HTML]{D9D9D9}49.00 & \cellcolor[HTML]{D9D9D9}48.98 & \cellcolor[HTML]{D9D9D9}48.92 & \cellcolor[HTML]{D9D9D9}48.62 & \cellcolor[HTML]{D9D9D9}48.50 & \cellcolor[HTML]{D9D9D9}48.62 \\  
%     ReM-ReB& 33.07 & \textbf{39.59} & \textbf{41.12} & \textbf{43.44} & \textbf{44.48} & 42.19 & \textbf{45.46} & \textbf{46.13} & \textbf{42.81} \\ 
%     ReM-ReO & 23.97 & 35.89 & 39.59 & 40.66 & 43.40 & \textbf{42.30} & 44.42 & 45.36 & 42.58 \\ 
%     BCRD-ReB & \textbf{35.31} & 38.04 & 35.43 & 42.27 & 41.69 & 36.10 & 44.27 & 42.61 & 36.22 \\ 
%     BCRD-ReO & 21.04 & 31.74 & 31.90 & 38.58 & 40.20 & 35.40 & 42.68 & 42.17 & 35.02 \\ 
%     \cellcolor[HTML]{D9D9D9} ReM & \cellcolor[HTML]{D9D9D9}34.29 &\cellcolor[HTML]{D9D9D9} 34.29 & \cellcolor[HTML]{D9D9D9}34.29 & \cellcolor[HTML]{D9D9D9}34.64 & \cellcolor[HTML]{D9D9D9}34.64 & \cellcolor[HTML]{D9D9D9}34.64 & \cellcolor[HTML]{D9D9D9}33.91 & \cellcolor[HTML]{D9D9D9}33.91 & \cellcolor[HTML]{D9D9D9}33.91 \\ 
%     \cmidrule(lr){2-10}
%     \multicolumn{10}{c}{}\vspace{-0.2cm}\\
%     & \multicolumn{9}{c}{$p=20$}\\\cmidrule(lr){2-10}
%     \cellcolor[HTML]{D9D9D9} ReO & \cellcolor[HTML]{D9D9D9}50.03 & \cellcolor[HTML]{D9D9D9}50.11 & \cellcolor[HTML]{D9D9D9}50.05 & \cellcolor[HTML]{D9D9D9}49.66 & \cellcolor[HTML]{D9D9D9}49.71 & \cellcolor[HTML]{D9D9D9}49.66 & \cellcolor[HTML]{D9D9D9}49.94 & \cellcolor[HTML]{D9D9D9}49.94 & \cellcolor[HTML]{D9D9D9}50.00 \\  
%     ReM-ReB& $--$ & 30.35 & \textbf{35.86} & \textbf{37.19} & \textbf{40.75} & \textbf{39.91} & \textbf{42.99} & \textbf{43.05} & \textbf{41.55} \\ 
%     ReM-ReO & $--$ &24.56 & 33.27 & 31.91 & 38.72 & 38.47 & 40.19 & 42.48 & 41.24 \\
%     BCRD-ReB & $--$ & \textbf{30.36} & 30.59 & 36.76 & 38.70 & 34.96 & 42.05 & 40.86 & 35.69 \\ 
%     BCRD-ReO & $--$ & 21.69 & 25.57 & 30.47 & 35.65 & 33.84 & 39.27 & 39.37 & 34.76 \\ 
%     \cellcolor[HTML]{D9D9D9} ReM & \cellcolor[HTML]{D9D9D9}26.39 & \cellcolor[HTML]{D9D9D9}26.39 & \cellcolor[HTML]{D9D9D9}26.39 & \cellcolor[HTML]{D9D9D9}26.67 & \cellcolor[HTML]{D9D9D9}26.67 & \cellcolor[HTML]{D9D9D9}26.67 & \cellcolor[HTML]{D9D9D9}25.61 & \cellcolor[HTML]{D9D9D9}25.61 & \cellcolor[HTML]{D9D9D9}25.61 \\  
%     \midrule
%     \end{tabular}}
% \end{table}


\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.6]{linear1.png}
    \caption{Performance of BCRD-ReO, BCRD-ReB, ReM-ReO and ReM-ReB compared with ReM with $r=0.2$. Sub-figure (A) investigates into factor 
$N$ with covariate dimensionality $p$ fixed at 
10, while sub-figure (B) investigates 
into factor $p$ with $N$ fixed at 600.}
    \label{fig:linear}
\end{figure}
For better comparison of the performance of two-stage ReB procedures with respect to ReM, Figure \ref{fig:linear} visualizes the effect of different factors on the PRIV ratio of  BCRD-ReO, BCRD-ReB,
    ReM-ReO and ReM-ReB over ReM. Fix $r=0.2$, sub-figure (A) investigates into factor
$N$ with covariate dimensionality $p$ fixed at 10, while sub-figure (B) investigates into factor $p$ with $N$ fixed at 600. 
It shows that the majority of the PRIV ratios of ReM-ReB over ReM are larger than 1, and the proportion of exceptions decreases quickly with the increase of $N$ and $p$. 
The other two-stage methods, however, do not perform so well, especially when the sample or covariate dimensionality is small. 
%Additionally, it is not advisable to use BCRD-ReO and BCRD-ReB when the covariate dimension is small.

To further study the effect of the proportion of sample in first stage on the performance of two-stage ReB procedures, we conduct an additional experiment on ReM, ReO, BCRD-ReO, BCRD-ReB, ReM-ReO and ReM-ReB with $r$ ranging from 0.05 to 0.8 when $N=600$, $p=10$, $\bar{R}^2=0.5$ and $\rho=0$. 
We set acceptance rate of ReM as $\alpha=0.05$ and tune the proper acceptance probabilities of other re-randomization procedures numerically as before. 
The running time ratio of different re-randomization procedures over ReM is plot in Section \ref{sec:linear_time} of the Supplement.
Average PRIVs of these methods under different specifications of $r$ are plotted in Figure \ref{fig:r_plot2}, from which we can see the following facts immediately.
First, all two-stage methods achieve the best performance when $r$ falls into the region of $[0.1,0.3]$.
%It shows that sample in first stage shouldn't be too small in practical scenario, where sample size is finite. In our simulation setting, use about $10\%\sim20\%$ data in first stage is recommended. 
%It is important to note that in other settings, a suitable sample proportion in the first stage should be chosen based on $N$ and $p$. As $N$ gets larger and $p$ decreases, Corollary \ref{N1infinity} suggests that the recommended proportion of samples in the first stage should decrease. Specifically, as $N$ approaches infinity, the optimal proportion of samples in the first stage should be 0.
% It shows that when sample size is large and proportion of sample in first stage is small, two-stage ReB has comparable performance with ReO, which is consistent with Corollary \ref{N1infinity}.
% Also, we can see that the performance of two-stage procedures with same method in first stage are similar when sample size is large. 
Second, ReM-ReB exhibits the best robustness to different specifications of $r$ out of all the schemes considered.
Thus, as a rule of thumb, we recommend ReM-ReB with about 20\% of samples in the first stage as the primary method for practical usage.
%when dealing with a sample size that is not too small, it would be a wise decision to use ReM-ReB with a relatively small proportion of the sample in the initial stage.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.55]{r2.png}
    \caption{Average PRIV of ReO, ReM, ReM-ReB, ReM-ReO, BCRD-ReB and BCRD-ReO, with different proportion of sample in first stage $r$ when total sample size $N=600$ and covariate dimensionality $p=10$.}
    \label{fig:r_plot2}
\end{figure}


% \begin{table}
%     \centering
%     \caption{Average PRIV achieved by different re-randomization procedures when $\bar{R}^2=0.5$.}
%     \label{tab:2stageReB}
%     \setlength{\tabcolsep}{2.5mm}{
% % \normalsize
% \begin{tabular}{cccccccccc}
%     \midrule
%     & \multicolumn{3}{c}{$N_1=60$}&\multicolumn{3}{c}{$N_1=120$}&\multicolumn{3}{c}{$N_1=600$}\\
%     \cmidrule(lr){2-4}
%     \cmidrule(lr){5-7}
%     \cmidrule(lr){8-10}
%     Scheme & $r=0.1$ & 0.2 & 0.4 & 0.1 & 0.2 & 0.4 & 0.1 & 0.2 & 0.4 \\
%     \hline
%     & \multicolumn{9}{c}{$p=2$}\\\cmidrule(lr){2-10}
%     \cellcolor[HTML]{D9D9D9} ReO($\alpha$) &\cellcolor[HTML]{D9D9D9} 49.3 &\cellcolor[HTML]{D9D9D9} 49.2 &\cellcolor[HTML]{D9D9D9} 49.0 &\cellcolor[HTML]{D9D9D9} 49.6 &\cellcolor[HTML]{D9D9D9} 49.3 &\cellcolor[HTML]{D9D9D9} 49.2 &\cellcolor[HTML]{D9D9D9} 49.3 &\cellcolor[HTML]{D9D9D9} 49.9 &\cellcolor[HTML]{D9D9D9} 50.4 \\   
%     % BCRD-ReB($\alpha$) & 45.8 & 43.1 & 35.8 & 46.3 & 42.8 & 36.6 & 47.0 & 43.8 & 36.8 \\  
%     % BCRD-ReO($\alpha$) & 45.3 & 42.4 & 35.1 & 46.5 & 42.7 & 36.3 & 46.7 & 43.7 & 36.8 \\ 
%     % ReM-ReB($\alpha$) & 48.5 & 48.1 & 47.6 & 48.7 & 48.2 & 48.5 & 49.6 & 49.1 & 49.3 \\ 
%     % ReM-ReO($\alpha$) & 48.2 & 47.8 & 46.8 & 48.9 & 48.2 & 48.2 & 49.3 & 49.2 & 49.2 \\ 
%  ReM-ReB($\alpha_2$)& \textbf{48.8} & \textbf{48.4} & \textbf{48.0} & 48.8 & \textbf{48.4} & \textbf{48.5} & \textbf{49.7} & 49.2 & \textbf{49.0} \\ 
%     ReM-ReO($\alpha_2$) & 48.1 & 48.0 & 47.0 & \textbf{49.0} & \textbf{48.4} & \textbf{48.5} & 49.5 & \textbf{49.3} & 48.9 \\ 
%     BCRD-ReB($\alpha_1$) & 45.8 & 42.7 & 35.2 & 46.2 & 42.7 & 35.8 & 46.7 & 43.5 & 36.3 \\ 
%     BCRD-ReO($\alpha_1$) & 45.3 & 42.4 & 34.6 & 46.4 & 42.6 & 35.3 & 46.7 & 43.5 & 36.4 \\ 
%     \cellcolor[HTML]{D9D9D9} ReM($\alpha$) &\cellcolor[HTML]{D9D9D9} 48.1 &\cellcolor[HTML]{D9D9D9} 48.2 &\cellcolor[HTML]{D9D9D9} 47.9 &\cellcolor[HTML]{D9D9D9} 48.5 &\cellcolor[HTML]{D9D9D9} 48.1 &\cellcolor[HTML]{D9D9D9} 48.2 &\cellcolor[HTML]{D9D9D9} 48.5 &\cellcolor[HTML]{D9D9D9} 48.9 &\cellcolor[HTML]{D9D9D9} 48.6 \\ 
%     \cmidrule(lr){2-10}
%     \multicolumn{10}{c}{}\\
%     & \multicolumn{9}{c}{$p=5$}\\\cmidrule(lr){2-10}
%     \cellcolor[HTML]{D9D9D9} ReO($\alpha$) &\cellcolor[HTML]{D9D9D9} 49.3 &\cellcolor[HTML]{D9D9D9} 49.1 &\cellcolor[HTML]{D9D9D9} 49.4 &\cellcolor[HTML]{D9D9D9} 49.5 &\cellcolor[HTML]{D9D9D9} 49.3 &\cellcolor[HTML]{D9D9D9} 49.1 &\cellcolor[HTML]{D9D9D9} 49.2 &\cellcolor[HTML]{D9D9D9} 49.5 &\cellcolor[HTML]{D9D9D9} 49.5 \\  
%     % BCRD-ReB($\alpha$) & 45.3 & 41.1 & 35.1 & 45.8 & 43.0 & 35.9 & 46.1 & 43.7 & 35.8 \\
%     % BCRD-ReO($\alpha$) & 43.1 & 39.4 & 33.0 & 44.7 & 42.0 & 34.8 & 46.4 & 43.6 & 35.6 \\ 
%     % ReM-ReB($\alpha$) & 47.2 & 46.0 & 45.1 & 48.1 & 47.0 & 45.8 & 48.1 & 48.3 & 45.4 \\ 
%     % ReM-ReO($\alpha$) & 45.4 & 44.3 & 43.7 & 47.0 & 46.1 & 45.2 & 48.3 & 48.0 & 45.6 \\ 
%     ReM-ReB($\alpha_2$)& \textbf{47.2} & \textbf{45.8} & \textbf{44.4} & \textbf{48.1} & \textbf{46.8} & \textbf{45.4} & 48.1 & \textbf{48.5} & 45.0 \\ 
%     ReM-ReO($\alpha_2$) & 45.4 & 44.4 & 43.2 & 47.1 & 46.0 & \textbf{45.4} & \textbf{48.2} & 48.3 & \textbf{45.2}\\
%     BCRD-ReB($\alpha_1$) & 45.3 & 41.0 & 34.9 & 45.7 & 42.9 & 35.8 & 46.2 & 43.9 & 35.4 \\  
%     BCRD-ReO($\alpha_1$) & 43.2 & 39.5 & 32.8 & 44.7 & 41.9 & 34.6 & 46.3 & 43.5 & 35.5 \\ 
%  \cellcolor[HTML]{D9D9D9} ReM($\alpha$) &\cellcolor[HTML]{D9D9D9} 42.2 &\cellcolor[HTML]{D9D9D9} 41.2 &\cellcolor[HTML]{D9D9D9} 41.5 &\cellcolor[HTML]{D9D9D9} 41.4 &\cellcolor[HTML]{D9D9D9} 42.2 &\cellcolor[HTML]{D9D9D9} 41.2 &\cellcolor[HTML]{D9D9D9} 41.5 &\cellcolor[HTML]{D9D9D9} 42.0 &\cellcolor[HTML]{D9D9D9} 40.6 \\
%     \cmidrule(lr){2-10}
%     \multicolumn{10}{c}{}\\
%     & \multicolumn{9}{c}{$p=10$}\\\cmidrule(lr){2-10}
%     \cellcolor[HTML]{D9D9D9} ReO($\alpha$) &\cellcolor[HTML]{D9D9D9} 49.6 &\cellcolor[HTML]{D9D9D9} 49.4 &\cellcolor[HTML]{D9D9D9} 50.3 &\cellcolor[HTML]{D9D9D9} 49.4 &\cellcolor[HTML]{D9D9D9} 49.6 &\cellcolor[HTML]{D9D9D9} 49.4 &\cellcolor[HTML]{D9D9D9} 49.4 &\cellcolor[HTML]{D9D9D9} 49.0 &\cellcolor[HTML]{D9D9D9} 50.2 \\ 
%     % BCRD-ReB($\alpha$) & 42.2 & 39.5 & 32.2 & 44.5 & 40.5 & 34.4 & 45.8 & 42.4 & 36.5 \\
%     % BCRD-ReO($\alpha$) & 38.2 & 35.8 & 28.1 & 42.7 & 40.0 & 32.7 & 45.8 & 42.8 & 36.7 \\ 
%     % ReM-ReB($\alpha$) & 43.7 & 43.2 & 40.8 & 45.8 & 44.1 & 41.8 & 47.3 & 45.8 & 44.1 \\ 
%     % ReM-ReO($\alpha$) & 39.9 & 39.6 & 37.4 & 44.3 & 43.2 & 40.7 & 47.6 & 45.7 & 44.1 \\ 
%     ReM-ReB($\alpha_2$)& \textbf{43.6} & \textbf{43.1} & \textbf{38.6} & \textbf{45.9} & \textbf{44.1} & \textbf{40.7} & \textbf{47.5} & 45.5 & 43.6 \\
%     ReM-ReO($\alpha_2$) & 40.0 & 39.5 & 36.5 & 44.2 & 43.7 & 40.4 & 47.3 & \textbf{45.6} & \textbf{43.7} \\
%     BCRD-ReB($\alpha_1$) & 42.2 & 39.5 & 32.4 & 44.4 & 40.7 & 34.2 & 46.0 & 42.6 & 36.9 \\ 
%     BCRD-ReO($\alpha_1$) & 38.3 & 36.0 & 28.5 & 42.7 & 40.1 & 33.0 & 45.7 & 42.8 & 36.9 \\ 
%     \cellcolor[HTML]{D9D9D9} ReM($\alpha$) &\cellcolor[HTML]{D9D9D9} 34.3 &\cellcolor[HTML]{D9D9D9} 34.9 &\cellcolor[HTML]{D9D9D9} 34.5 &\cellcolor[HTML]{D9D9D9} 34.3 &\cellcolor[HTML]{D9D9D9} 34.3 &\cellcolor[HTML]{D9D9D9} 34.9 &\cellcolor[HTML]{D9D9D9} 33.7 &\cellcolor[HTML]{D9D9D9} 34.6 &\cellcolor[HTML]{D9D9D9} 35.1 \\ 
%     \cmidrule(lr){2-10}
%     \multicolumn{10}{c}{}\\
%     & \multicolumn{9}{c}{$p=20$}\\\cmidrule(lr){2-10}
%     \cellcolor[HTML]{D9D9D9} ReO($\alpha$) &\cellcolor[HTML]{D9D9D9} 49.8 &\cellcolor[HTML]{D9D9D9} 49.4 &\cellcolor[HTML]{D9D9D9} 48.0 &\cellcolor[HTML]{D9D9D9} 49.8 &\cellcolor[HTML]{D9D9D9} 49.8 &\cellcolor[HTML]{D9D9D9} 49.4 &\cellcolor[HTML]{D9D9D9} 50.2 &\cellcolor[HTML]{D9D9D9} 49.3 &\cellcolor[HTML]{D9D9D9} 49.9 \\
%     % BCRD-ReB($\alpha$) & 36.4 & 31.3 & 21.2 & 40.6 & 37.9 & 30.7 & 45.6 & 42.0 & 36.3 \\ 
%     % BCRD-ReO($\alpha$) & 29.8 & 24.1 & 14.7 & 38.3 & 35.5 & 27.8 & 45.5 & 42.0 & 36.0 \\ 
%     % ReM-ReB($\alpha$) & 37.8 & 34.5 & 29.4 & 42.1 & 40.8 & 36.4 & 46.6 & 44.2 & 41.2 \\ 
%     % ReM-ReO($\alpha$) & 31.6 & 28.1 & 23.6 & 39.7 & 38.8 & 34.4 & 46.5 & 44.0 & 41.0 \\ 
%     ReM-ReB($\alpha_2$)& \textbf{37.1} & \textbf{33.8} & 17.9 & \textbf{42.1} & \textbf{40.5} & 26.6 & \textbf{47.2} & 44.0 & 32.3 \\  
%     ReM-ReO($\alpha_2$) & 31.5 & 28.4 & 17.2 & 39.6 & 38.4 & 26.6 & 46.6 & \textbf{44.2} & 32.2 \\ 
%     BCRD-ReB($\alpha_1$) & 36.3 & 31.1 & 16.8 & 41.1 & 37.9 & \textbf{26.9} & 46.1 & 42.6 & \textbf{33.7} \\ 
%     BCRD-ReO($\alpha_1$) & 29.8 & 24.7 & 14.6 & 38.3 & 35.9 & 26.8 & 45.7 & 42.3 & \textbf{33.7} \\  
%     \cellcolor[HTML]{D9D9D9} ReM($\alpha$) &\cellcolor[HTML]{D9D9D9} 26.6 &\cellcolor[HTML]{D9D9D9} 26.5 &\cellcolor[HTML]{D9D9D9} \textbf{25.7} &\cellcolor[HTML]{D9D9D9} 26.2 &\cellcolor[HTML]{D9D9D9} 26.6 &\cellcolor[HTML]{D9D9D9} 26.5 &\cellcolor[HTML]{D9D9D9} 27.2 &\cellcolor[HTML]{D9D9D9} 25.9 &\cellcolor[HTML]{D9D9D9} 26.9 \\ 
%     \midrule
%     \end{tabular}}
% \end{table}



% \subsection{Performance under nonlinear models}
All above simulations are conducted under linear models. 
To evaluate the performance of two-stage ReB under non-linear models, we repeated the above numerical experiment with the linear regression model for potential outcomes in \eqref{eq:LinearModel4Simulation} replaced by the non-linear regression model below:
\begin{equation}\label{eq:NonLinearModel4Simulation}
    Y_i(W_i) = \tau\cdot W_i + \bbeta^T\exp\{\bm X_i\}  + \epsilon_i,\quad i=1,\ldots,N,
\end{equation}
with all the other simulation settings unchanged.
%Acceptance probability of ReM is set to be $\alpha=0.05$ and that of two-stage schemes are calculated according to Eq.~\eqref{AP1}-\eqref{AP2}. 
%When conducting two-stage ReB, we only use the first-order terms of $\bm X$.
Similar results were obtained for this more challenging setting (see Section \ref{sec:nonlinear_result} of the Supplementary Material for details), confirming that the two-stage ReB methods can still work robustly in the non-linear cases.


In summary, simulations conducted under both linear and non-linear settings confirm that the two-stage ReB strategy enjoys a clear edge over ReM
%without additional model assumptions and prior knowledge on the importance vector, 
when the overall sample size is reasonably large, as indicated by Theorem \ref{2stageReO-variance}. 
Among various two-stage ReB procedures, ReM-ReB is the preferred one in most cases.


% \begin{table}
%     \centering
% \caption{Average PRIV achieved by different re-randomization procedures under nonlinear model.}
% \label{tab:nonlinear}
%     \setlength{\tabcolsep}{2.5mm}{
% % \normalsize
% \begin{tabular}{cccccccccc}
%     \midrule
%     & \multicolumn{3}{c}{$N=200$}&\multicolumn{3}{c}{$N=600$}&\multicolumn{3}{c}{$N=1200$}\\
%     \cmidrule(lr){2-4}
%     \cmidrule(lr){5-7}
%     \cmidrule(lr){8-10}
%     Scheme & $r=0.1$ & 0.2 & 0.4 & 0.1 & 0.2 & 0.4 & 0.1 & 0.2 & 0.4 \\
%     \hline
%     & \multicolumn{9}{c}{$p=2$}\\\cmidrule(lr){2-10}
%     ReM-ReB($\alpha_2$) & \textbf{48.86}
%     & \textbf{47.84} & \textbf{48.49} & \textbf{48.89} & \textbf{48.66} & \textbf{49.31} & \textbf{48.93} & \textbf{49.35} & 48.55 \\
%     ReM-ReO($\alpha_2$) & 45.45 & 46.08 & 48.30 & 48.11 & 47.77 & 49.23 & 48.54 & 48.87 & \textbf{48.82} \\  
%     BCRD-ReB($\alpha_1$) & 46.46 & 43.25 & 38.45 & 45.85 & 42.14 & 35.84 & 46.24 & 44.44 & 36.56 \\ 
%     BCRD-ReO($\alpha_1$) & 42.35 & 40.81 & 37.66 & 45.02 & 41.99 & 35.97 & 45.81 & 43.38 & 36.19 \\
%     \cellcolor[HTML]{D9D9D9} ReM($\alpha$) &\cellcolor[HTML]{D9D9D9} 48.65 &\cellcolor[HTML]{D9D9D9} 47.49 &\cellcolor[HTML]{D9D9D9} 48.12 &\cellcolor[HTML]{D9D9D9} 48.61 &\cellcolor[HTML]{D9D9D9} 48.52 &\cellcolor[HTML]{D9D9D9} 49.22 &\cellcolor[HTML]{D9D9D9} 48.57 &\cellcolor[HTML]{D9D9D9} 48.48 &\cellcolor[HTML]{D9D9D9} 48.08 \\
%     \cmidrule(lr){2-10}
%     \multicolumn{10}{c}{}\\
%     & \multicolumn{9}{c}{$p=5$}\\\cmidrule(lr){2-10}
%     ReM-ReB($\alpha_2$) & \textbf{42.91} & \textbf{43.63} & \textbf{43.46} & \textbf{45.46} & \textbf{47.54} & \textbf{43.81} & \textbf{46.03} & \textbf{46.77} & \textbf{43.76} \\
%     ReM-ReO($\alpha_2$) & 34.69 & 39.08 & 42.05 & 43.49 & 45.81 & 43.32 & 44.35 & 46.35 & 43.42 \\
%     BCRD-ReB($\alpha_1$) & 40.50 & 39.28 & 34.61 & 43.91 & 42.88 & 34.78 & 44.27 & 42.09 & 33.88 \\ 
%     BCRD-ReO($\alpha_1$) & 31.52 & 33.59 & 31.75 & 41.20 & 41.05 & 34.19 & 42.51 & 41.47 & 33.99 \\ 
%     \cellcolor[HTML]{D9D9D9} ReM($\alpha$) &\cellcolor[HTML]{D9D9D9} 40.38 &\cellcolor[HTML]{D9D9D9} 40.84 &\cellcolor[HTML]{D9D9D9} 40.28 &\cellcolor[HTML]{D9D9D9} 41.11 &\cellcolor[HTML]{D9D9D9} 41.22 &\cellcolor[HTML]{D9D9D9} 40.52 &\cellcolor[HTML]{D9D9D9} 40.48 &\cellcolor[HTML]{D9D9D9} 41.14 &\cellcolor[HTML]{D9D9D9} 40.38 \\  
%     \cmidrule(lr){2-10}
%     \multicolumn{10}{c}{}\\
%     & \multicolumn{9}{c}{$p=10$}\\\cmidrule(lr){2-10}
%     ReM-ReB($\alpha_2$) & \textbf{34.37} & \textbf{38.44} & \textbf{37.82}& \textbf{42.41} & \textbf{42.43} & 40.53 & \textbf{42.98} & \textbf{43.02} & \textbf{40.45} \\
%     ReM-ReO($\alpha_2$) & 22.45 & 32.69 & 36.07 & 39.58 & 41.22 & \textbf{40.71} & 40.85 & 42.62 & 40.04 \\ 
%     BCRD-ReB($\alpha_1$) & 32.87 & 34.93 & 31.08 & 40.77 & 38.90 & 34.42 & 41.65 & 40.14 & 34.10 \\  
%     BCRD-ReO($\alpha_1$) & 20.32 & 27.73 & 27.60 & 37.49 & 37.78 & 34.09 & 39.06 & 38.85 & 33.80 \\ 
%     \cellcolor[HTML]{D9D9D9} ReM($\alpha$) &\cellcolor[HTML]{D9D9D9} 33.74 &\cellcolor[HTML]{D9D9D9} 33.94 &\cellcolor[HTML]{D9D9D9} 34.12 &\cellcolor[HTML]{D9D9D9} 33.34 &\cellcolor[HTML]{D9D9D9} 32.95 &\cellcolor[HTML]{D9D9D9} 33.59 &\cellcolor[HTML]{D9D9D9} 33.19 &\cellcolor[HTML]{D9D9D9} 33.38 &\cellcolor[HTML]{D9D9D9} 33.81 \\ 
%     \cmidrule(lr){2-10}
%     \multicolumn{10}{c}{}\\
%     & \multicolumn{9}{c}{$p=20$}\\\cmidrule(lr){2-10}
%     ReM-ReB($\alpha_2$) & $--$ & \textbf{24.22}& 20.19 & \textbf{33.96} & \textbf{37.63} & 27.38 & \textbf{40.05} & \textbf{40.25} & 28.87 \\
%     ReM-ReO($\alpha_2$) & $--$ & 15.76 & 19.67 & 28.39 & 36.12 & 27.66 & 38.03 & 39.83 & 28.93 \\ 
%     BCRD-ReB($\alpha_1$) & $--$ & 21.43 & 20.12 & 32.66 & 35.71 & \textbf{29.43} & 38.92 & 38.50 & \textbf{30.53} \\ 
%     BCRD-ReO($\alpha_1$) & $--$ & 11.04 & 18.52 & 26.97 & 33.19 & 29.21 & 36.20 & 37.51 & 30.40 \\
%     \cellcolor[HTML]{D9D9D9} ReM($\alpha$) &\cellcolor[HTML]{D9D9D9} 24.18 &\cellcolor[HTML]{D9D9D9} 23.92 &\cellcolor[HTML]{D9D9D9} \textbf{24.28} &\cellcolor[HTML]{D9D9D9} 25.65 &\cellcolor[HTML]{D9D9D9} 25.79 &\cellcolor[HTML]{D9D9D9} 25.80 &\cellcolor[HTML]{D9D9D9} 24.89 &\cellcolor[HTML]{D9D9D9} 24.96 &\cellcolor[HTML]{D9D9D9} 24.98 \\ 
%     \midrule
%     \end{tabular}
% }
% \end{table}

% \begin{table}
%     \centering
% \caption{Average PRIV achieved by different re-randomization procedures under nonlinear model.}
% \label{tab:nonlinear}
%     \setlength{\tabcolsep}{2.5mm}{
% % \normalsize
% \begin{tabular}{cccccccccc}
%     \midrule
%     & \multicolumn{3}{c}{$N_1=60$}&\multicolumn{3}{c}{$N_1=120$}&\multicolumn{3}{c}{$N_1=600$}\\
%     \cmidrule(lr){2-4}
%     \cmidrule(lr){5-7}
%     \cmidrule(lr){8-10}
%     Scheme & $r=0.1$ & 0.2 & 0.4 & 0.1 & 0.2 & 0.4 & 0.1 & 0.2 & 0.4 \\
%     \hline
%     & \multicolumn{9}{c}{$p=2$}\\\cmidrule(lr){2-10}
%     ReM-ReB($\alpha_2$) & 47.6 & \textbf{47.9} & \textbf{49.0} & \textbf{48.4} & \textbf{49.6} & \textbf{48.6} & \textbf{47.9} & 48.2 & \textbf{48.1} \\ 
%     ReM-ReO($\alpha_2$) & 47.2 & 46.9 & 48.0 & 47.3 & 49.4 & 48.5 & \textbf{47.9} & 48.2 & 47.7 \\ 
%     BCRD-ReB($\alpha_1$) & 45.0 & 42.4 & 35.6 & 45.7 & 43.9 & 35.3 & 45.3 & 42.4 & 34.3 \\ 
%     BCRD-ReO($\alpha_1$) & 44.1 & 41.4 & 33.8 & 44.6 & 43.4 & 34.6 & 45.4 & 42.1 & 34.6 \\ 
%     \cellcolor[HTML]{D9D9D9} ReM($\alpha$) &\cellcolor[HTML]{D9D9D9} \textbf{48.1} &\cellcolor[HTML]{D9D9D9} 47.9 &\cellcolor[HTML]{D9D9D9} 48.8 &\cellcolor[HTML]{D9D9D9} 48.2 &\cellcolor[HTML]{D9D9D9} 49.2 &\cellcolor[HTML]{D9D9D9} 48.5 &\cellcolor[HTML]{D9D9D9} 47.4 &\cellcolor[HTML]{D9D9D9} \textbf{48.5} &\cellcolor[HTML]{D9D9D9} 47.3 \\ 
%     \cmidrule(lr){2-10}
%     \multicolumn{10}{c}{}\\
%     & \multicolumn{9}{c}{$p=5$}\\\cmidrule(lr){2-10}
%     ReM-ReB($\alpha_2$) & \textbf{45.6} & \textbf{45.5} & \textbf{42.8} & \textbf{46.9} & \textbf{47.0} & \textbf{44.6} & \textbf{47.0} & \textbf{46.8} & \textbf{44.2} \\ 
%     ReM-ReO($\alpha_2$) & 43.0 & 42.9 & 40.7 & 45.6 & 46.1 & 44.3 & 46.7 & 46.7 & 43.7 \\ 
%     BCRD-ReB($\alpha_1$) & 43.8 & 40.6 & 32.9 & 44.4 & 42.5 & 34.7 & 45.0 & 42.3 & 34.7 \\ 
%     BCRD-ReO($\alpha_1$) & 40.7 & 37.9 & 30.2 & 43.2 & 41.2 & 33.7 & 44.7 & 41.8 & 34.6 \\ 
%     \cellcolor[HTML]{D9D9D9} ReM($\alpha$) &\cellcolor[HTML]{D9D9D9} 41.3 &\cellcolor[HTML]{D9D9D9} 41.4 &\cellcolor[HTML]{D9D9D9} 40.0 &\cellcolor[HTML]{D9D9D9} 40.8 &\cellcolor[HTML]{D9D9D9} 41.4 &\cellcolor[HTML]{D9D9D9} 41.5 &\cellcolor[HTML]{D9D9D9} 40.1 &\cellcolor[HTML]{D9D9D9} 40.8 &\cellcolor[HTML]{D9D9D9} 40.7 \\ 
%     \cmidrule(lr){2-10}
%     \multicolumn{10}{c}{}\\
%     & \multicolumn{9}{c}{$p=10$}\\\cmidrule(lr){2-10}
%     ReM-ReB($\alpha_2$) & \textbf{41.8} & \textbf{40.2} & \textbf{37.4} & \textbf{43.4} & \textbf{42.6} & \textbf{39.7} & \textbf{46.4} & \textbf{44.9} & 42.1 \\ 
%     ReM-ReO($\alpha_2$) & 38.4 & 36.4 & 34.5 & 41.5 & 40.9 & 38.1 & \textbf{46.4} & \textbf{44.9}& \textbf{42.2} \\ 
%     BCRD-ReB($\alpha_1$) & 39.9 & 36.5 & 30.1 & 41.5 & 39.5 & 32.9 & 45.1 & 41.4 & 35.2 \\   
%     BCRD-ReO($\alpha_1$) & 36.2 & 32.3 & 25.6 & 39.8 & 37.4 & 31.1 & 44.3 & 41.6 & 34.9 \\ 
%     \cellcolor[HTML]{D9D9D9} ReM($\alpha$) &\cellcolor[HTML]{D9D9D9} 33.4 &\cellcolor[HTML]{D9D9D9} 33.7 &\cellcolor[HTML]{D9D9D9} 33.6 &\cellcolor[HTML]{D9D9D9} 33.4 &\cellcolor[HTML]{D9D9D9} 33.1 &\cellcolor[HTML]{D9D9D9} 34.0 &\cellcolor[HTML]{D9D9D9} 33.3 &\cellcolor[HTML]{D9D9D9} 33.3 &\cellcolor[HTML]{D9D9D9} 33.8 \\
%     \cmidrule(lr){2-10}
%     \multicolumn{10}{c}{}\\
%     & \multicolumn{9}{c}{$p=20$}\\\cmidrule(lr){2-10}
%     ReM-ReB($\alpha_2$) & \textbf{35.7} & \textbf{30.1} & 16.1 & \textbf{39.2} & \textbf{37.4} & 22.6 & \textbf{45.2} & \textbf{42.6} & 31.8 \\ 
%     ReM-ReO($\alpha_2$) & 29.7 & 25.4 & 15.6 & 36.8 & 35.2 & 22.4 & 45.1 & 42.5 & 31.8 \\
%     BCRD-ReB($\alpha_1$) & 34.4 & 27.7 & 15.8 & 37.8 & 35.3 & 23.3 & 44.2 & 40.3 & \textbf{32.8}\\ 
%     BCRD-ReO($\alpha_1$) & 27.6 & 21.4 & 12.9 & 35.3 & 32.8 & 22.6 & 43.9 & 39.7 & 32.7 \\ 
%     \cellcolor[HTML]{D9D9D9} ReM($\alpha$) &\cellcolor[HTML]{D9D9D9} 25.7 &\cellcolor[HTML]{D9D9D9} 24.8 &\cellcolor[HTML]{D9D9D9} \textbf{24.6} &\cellcolor[HTML]{D9D9D9} 25.0 &\cellcolor[HTML]{D9D9D9} 25.7 &\cellcolor[HTML]{D9D9D9} \textbf{24.8} &\cellcolor[HTML]{D9D9D9} 26.2 &\cellcolor[HTML]{D9D9D9} 25.0 &\cellcolor[HTML]{D9D9D9} 26.5 \\ 
%     \midrule
%     \end{tabular}
% }
% \end{table}

% \begin{figure}
% \includegraphics[scale=0.6]{nonlinear2.png}
% \caption{Performance of BCRD-ReB($\alpha_1$), BCRD-ReO($\alpha_1$), ReM-ReB($\alpha_2$) and ReM-ReO($\alpha_2$) compared with ReM($\alpha$) with $r=0.2$ under nonlinear model. Sub-figure (A) investigates into factor 
% $N$ with covariate dimensionality $p$ fixed at 
% 10, while sub-figure (B) investigates 
% into factor $p$ with $N$ fixed at 600.}
% \label{fig:nonlinear}
% \end{figure}

\section{Real Data Application}\label{sec:RealDataAnalysis}

In this section, we study the performance of the proposed Bayesian re-randomization procedures in real application. We follow the strategy proposed by \cite{Hill2011} to adopt covariates from a real randomized experiment in the Infant Health and Development Program (IHDP) in  \cite{BROOKSGUNN1992350}, while simulate potential outcomes based on designed response surfaces exsuring ignorability. Aiming to investigate the effect of both intensive high-quality child care and home visits to low-birth-weight and premature infants, the IHDP experiment covers 747 units and 25 pretreatment variables, including 6 continuous standardized covariates and 19 binary covariates in \cite{Hill2011}. Here, we modify the parallel-regression-based response surfaces, i.e., response surface A in \cite{Hill2011} with a larger level of noise, of the form 
$$\bm Y(0) \sim N(\bm X\bbeta,3)\quad\text{and}\quad \bm Y(1) \sim N(\bm X\bbeta+4,3),$$
where $\bm X$ is the expanded covariate matrix (747$\times$26) with first column being a vector of ones, and
$\bbeta$ is a 25-dimensional random coefficient vector whose elements are random samples from a discrete support $\{0,1,2,3,4\}$ according to probability $(0.5,0.2,0.15,0.1,0.05)$. 

To keep a balanced design, we drop the last unit in the dataset, and allocate the first 746 units into two groups of equal sample size (i.e., $N_0=N_1=373)$ under 6 different randomization schemes, including BCRD, ReM, ReM-ReB, ReM-ReO, BCRD-ReB and BCRD-ReO with the acceptance probability of ReM fixed at $\alpha=0.05$ and that of other re-randomization procedures tuned numerically so that their computation cost in the design phrase is same as ReM. 
Running time ratio of other re-randomization procedures against ReM is shown in Section \ref{sec:realdata_time} of the Supplement.
In the two-stage procedures, we specify the proportion of samples in the first stage to different values ranging from $10\%$ to $80\%$ to investigate the robustness of these methods. 
PRIV of ReM, ReM-ReB, ReM-ReO, BCRD-ReB and BCRD-ReO over BCRD via 500 allocations accepted by each method are calculated for 100 times and the average PRIV of these methods are shown in Figure \ref{fig:priv}.

Based on the figure, we can see the following facts.
First, ReM-ReB and ReM-ReO perform better than ReM, even when a large proportion of samples are assigned to the first stage. 
Second, ReM-ReB and ReM-ReO are more robust to changes in the proportion of sample size in the first stage than BCRD-ReB and BCRD-ReO. 
Third, BCRD-ReO and ReM-ReO may not perform well when the sample size in the first stage is very small. 
Therefore, we suggest using ReM-ReB with a sample proportion of $20\%$ in the first stage for this particular setting. 


\begin{figure}[t]
    \centering
    \includegraphics[scale=0.55]{priv.png}
    \caption{PRIV of ReM, BCRD-ReB, BCRD-ReO, ReM-ReB and ReM-ReO with different proportion of sample in first stage $r$ in real application.}
    \label{fig:priv}
\end{figure}


\section{Conclusion and Discussions}\label{sec:Discussion}
In this study, we re-consider re-randomization from the Bayesian perspective. 
Assuming that the importance vector of covariates $\bbeta_{\infty}$ is uncertain with respect to a prior distribution $\pi$, a Bayesian criterion is established to derive a more efficient re-randomization procedure, referred to as ReB, by minimizing the prior-integrated conditional asymptotic variance of the casual effect estimator.
We find that ReB only depends on the second moment of prior distribution $\pi$, and enjoys a better efficiency than the ReM-based re-randomization procedures in balancing high-dimensional covariates and reducing the estimation variance of the difference-in-mean estimator for causal effect, as long as the prior distribution is informative to highlight the true importance vector.
We also note that ReB is invariant to any scale transformation of the prior distribution of $\bbeta_{\infty}$, indicating that re-scaling either $\pi$ or $\bX$ would not change the re-randomization procedure under the Bayesian criterion. 
Taking ReM, ReM$_T$, PCA-ReM, ridge-ReM as its special cases under different specifications of the prior distribution $\pi$, ReB provides us a unified framework to understand and interpret various re-randomization procedures.
More importantly, ReB extends the idea behind ReM$_T$, and establishes a principled way to utilize prior information about $\bbeta_{\infty}$ in re-randomization.

Additionally, the Bayesian nature of ReB also provides an additional avenue for designing more efficient experiments in sequential trials.
The success of the two-stage ReB gives us an important message: once we can observe the responses of some units before the arrival of other experimental units, we would be able to design a more efficient randomized experiment by following the spirit of ReB.
Now, suppose a typical scenario where experiment units arrive in a sequential fashion with their potential outcomes observed along the experiment process so that we can update our knowledge about $\bbeta_\infty$ gradually. 
In such cases, we would be able to adjust the measurement for covariate imbalance simultaneously, and thus improve the overall balance of covariates more efficiently by carefully allocating treatment assignments to upcoming units based on the adjusted imbalance measurement induced by the Bayesian principle for re-randomization.
Compared to the classic approaches for sequential design, which typically do not take into consideration the responses of units received along the experiment process, the above alternative approach is apparently more attractive.
Due to the limited space of this paper, however, we reserve this exciting topic for future research.

\section{Acknowledgments}
This work was supported by National Science Foundation of China (Grant No. 11931001).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{apalike}
\bibliography{Rerand}




\newpage
\appendix

\begin{center}
    \textbf{\centering \Large Supplementary Material}
\end{center}
\setcounter{equation}{0}
\renewcommand{\theequation}{A.\arabic{equation}}
\setcounter{figure}{0}
\renewcommand{\thefigure}{B.\arabic{figure}}
\setcounter{table}{0}
\renewcommand{\thetable}{B.\arabic{table}}

In this document, Appendix \ref{sec:proof} contains technical proofs of all theorems, lemmas and corollaries. 
Appendix \ref{sec:simulations} shows additional simulation results.
\section{Technical Proofs}\label{sec:proof}%% if no title is needed, leave empty \section*{}.
% \subsection{Proof of Lemma \ref{lma:PRIAV_Phi}}
% By equation \eqref{eq:AsymptoticVariance4Phi},
% \[\begin{split}
%         \bbV_a\left(\sqrt{N}(\hat{\tau}-\tau)\mid\phi=1\right)&=
%     (1-R^2_{\infty})V_{\tau\tau,\infty} + \bbE\left(\bm B^T_{\infty}\bbeta_{\infty}\bbeta_{\infty}^T\bm B_{\infty}\mid\bm B_{\infty} \in \mathcal{B}_{\phi,\infty}\right).
% \end{split}\]


\subsection{Proof of Theorem \ref{thm:GeneralSolution4OptimizationProblem}}
%\begin{proof}
Assume that there exists another $\phi^*(\bm Z) \in \Psi_\alpha$ that satisfies $\phi^* \neq \phi_g$ and 
$$\bbE(g(\bZ)\mid\phi^*(\bm Z)=1)
%=\int_{\phi^*=1} d_\bbeta^2 d\eta(\bD) 
< 
%\int_{\phi_\bbeta=1}d_\bbeta^2 d\eta(\bD)=
\bbE(g(\bZ)\mid\phi_g(\bm Z)=1).$$
Let $A \triangleq \left\{\omega: \phi_g(\bm Z(w))=1\right\} \in \mathcal{F}$, $A^* \triangleq \left\{\omega: \phi^*(\bm Z(w))=1\right\} \in \mathcal{F}$, then $\bZ$ is uniformly distributed on $A^*$ and $A$ under $\phi^*$ and $\phi$, and we have 
$$\int_{A^*} g(\bm Z) dF_{\bZ} < \int_{A} g(\bm Z) dF_{\bZ}.$$
Considering that
\begin{eqnarray*}
\int_{A^*} g(\bm Z) dF_{\bZ}&=&\int_{A^* \cap A} g(\bm Z) dF_{\bZ} + \int_{A^*\cap A^c} g(\bm Z) dF_{\bZ},\\
\int_{A} g(\bm Z) dF_{\bZ}&=&\int_{A^* \cap A} g(\bm Z) dF_{\bZ} + \int_{A\cap A^{*c}} g(\bm Z) dF_{\bZ},
\end{eqnarray*}
and $\bbE|g(\bm Z)| < \infty$, we have 
$$\int_{A^*\cap A^c} g(\bm Z) dF_{\bZ} < \int_{A\cap A^{*c}} g(\bm Z) dF_{\bZ}.$$
%By taking out of overlap of $\phi^*$ and $\phi_\bbeta$, i.e., $\phi_\bbeta=\phi^*=1$,
%$$\int_{\phi^*=1, \phi_\bbeta=0} d_\bbeta^2 d\eta(\bD) < \int_{ \phi_\bbeta=1, \phi^*=0} d_\bbeta^2 d\eta(\bD) $$ 

On the other hand, because $A = \left\{\omega: g(\bm Z(w))\leq a\right\}$, we have
\begin{eqnarray*}
\int_{A^*\cap A^c} g(\bm Z) dF_{\bZ}&>&a\cdot \mathbb{P}(A^*\cap A^c),\\
\int_{A\cap A^{*c}} g(\bm Z) dF_{\bZ}&\leq&a\cdot \mathbb{P}(A\cap A^{*c}).
\end{eqnarray*}
Considering that the acceptance rates of $\phi_g$ and $\phi^*$ are the same, we have 
$$\mathbb{P}(A^*\cap A^c)=\mathbb{P}(A\cap A^{*c}).$$
Therefore, 
\begin{eqnarray*}
\int_{A^*\cap A^c} g(\bm Z) dF_{\bZ}>
\int_{A\cap A^{*c}} g(\bm Z) dF_{\bZ},
\end{eqnarray*}
and we reach a contradiction.
\QEDA

%\end{proof}


\subsection{Proof of Lemma \ref{lma:AssymptoticDistribution4d_beta}}
%\begin{proof}
% Define $\bZ \equiv \bm\Sigma_\bD^{-1/2}\bD$, where $\bm\Sigma_\bD^{-1/2}$ is the Cholesky square root of $\bm\Sigma_\bD^{-1}$. Then $d_\bbeta$ can be decomposed as:
% \begin{align}
%     d_\bbeta &= \bD^T\bbeta_{\infty}\bbeta_{\infty}^T\bD \nonumber \\
%     &= \bZ^T\bm\Sigma_\bD^{1/2}\bbeta_{\infty}\bbeta_{\infty}^T\bm\Sigma_\bD^{1/2}\bZ \nonumber \\
%     &= \bZ^T\Tilde{\bbeta}_{\infty}\Tilde{\bbeta}_{\infty}^T\bZ \nonumber,
% \end{align}
% where $\Tilde{\bbeta}=\bm\Sigma_\bD^{1/2}\bbeta$.
Note that $\bm\Sigma_\bD=(1/N_t+1/N_c)\bm S_{\bm X}^2=\bV_{xx,N}/N$, where $\bm S_{\bm X}^2$ is the constant sample covariance matrix of $\bX$:
$$\bm S_{\bm X}^2 = \frac{1}{N-1} \sum_{i=1}^N (\bm X_i-\bar{\bX})(\bm X_i-\bar{\bX})^T.$$
% Since the difference in covariate means is
% $$\bD=\bar{\bX}_T-\bar{\bX}_C=\frac{\bX^T\bW}{N_t}-\frac{\bX^T(\boldsymbol{1}-\bW)}{N_c}=\frac{\bX^T(N\bW-N_t\boldsymbol{1})}{N_tN_c},$$
% when dimension of covariates $p=1$, we can get expectation by enumerating all possible cases:
% \begin{align}
%     \bbE(\bD)&=\frac{\bbE\left[\bX^T(N\bW-N_t\boldsymbol{1})\right]}{N_tN_c} \nonumber \\
%     &= \frac{1}{N_tN_c}\sum_{\bW\in\mW}\frac{\bX^T(N\bW-N_t\boldsymbol{1})}{\binom{N}{N_t}} \nonumber \\
%     &= \frac{1}{N_tN_c \binom{N}{N_t}
%     }\sum_{\bW\in\mW}\bX^T(N\bW-N_t\boldsymbol{1}) \nonumber \\
%     &= \frac{1}{N_tN_c\binom{N}{N_t}}\sum_{\bW\in\mW}\sum_{i=1}^N X_i(NW_i-N_t) \nonumber \\
%     &= \frac{1}{N_tN_c\binom{N}{N_t}}\left(\sum_{\bW\in\mW}\sum_{i=1}^NX_iNW_i- \sum_{\bW\in\mW}\sum_{i=1}^NN_tX_i\right)\nonumber \\
%     &= \frac{1}{N_tN_c\binom{N}{N_t}}\left(N\binom{N-1}{N_t-1}-N_t\binom{N}{N_t}\right)\sum_{i=1}^NX_i\nonumber \\
%     &= 0 \nonumber.
% \end{align}
% Similarly, we can get the second moment of $\bD$ as
% \begin{align}
%     \bbE(\bD^2)&=\frac{\bbE\left[\bX^T(N\bW-N_t\boldsymbol{1})\right]^2}{N_t^2N_c^2} \nonumber \\
%     &= \frac{1}{N_t^2N_c^2}\sum_{\bW\in\mW}\frac{\left[\bX^T(N\bW-N_t\boldsymbol{1})\right]^2}{\binom{N}{N_t}} \nonumber \\
%     &= \frac{1}{N_t^2N_c^2\binom{N}{N_t}}\sum_{\bW\in\mW}\left[\sum_{i=1}^N X_i(NW_i-N_t)\right]^2 \nonumber.
% \end{align}
% By enumerating all possible allocation cases, we can derive the following:
% \begin{align}
%     &\sum_{\bW\in\mW}\left[\sum_{i=1}^N X_i(NW_i-N_t)\right]^2 \nonumber \\
%     &=  \sum_{\bW\in\mW}\left\{\sum_{i=1}^N\left[X_i(NW_i-N_t)\right]^2 + \sum_{i\neq j}X_iX_j(NW_i-N_t)(NW_j-N_t)\right\} \nonumber \\
%     &= \binom{N-1}{N_t-1}\sum_{i=1}^N(X_iN_c)^2+\binom{N-1}{N_t}\sum_{i=1}^N(X_iN_t)^2+\binom{N-2}{N_t-2}\sum_{i\neq j}X_iX_jN_c^2 \nonumber \\
%     &-2\binom{N-2}{N_t-1}\sum_{i\neq j}X_iX_jN_tN_c+\binom{N-2}{N_t}\sum_{i\neq j}X_iX_jN^2_t \nonumber \\
%     &= \left(\binom{N-1}{N_t-1}N_c^2+\binom{N-1}{N_t}N_t^2\right)\sum_{i=1}^N X_i^2 \nonumber\\ &+\left(\binom{N-2}{N_t-2}N_c^2-2\binom{N-2}{N_t-1}N_tN_c+\binom{N-2}{N_t}N_t^2\right)\sum_{i\neq j}X_iX_j \nonumber \\
%     &= NN_tN_c\binom{N}{N_t}\times \frac{1}{N-1}\sum_{i=1}^N (X_i-\bar{\bX})^2 \nonumber \\
%     &= NN_tN_c\binom{N}{N_t} S_{\bm X}^2 \nonumber.
% \end{align}
% Hence,
% $$\Sigma_\bD=\cov(\bD)=\bbE(\bD^2)=\frac{N}{N_tN_c}S_{\bX}^2=\left(\frac{1}{N_t}+\frac{1}{N_c}\right)S_{\bX}^2.$$

% Similarly, when $p>1$, for any $m\neq n$, we have
% $$\cov(\bD_m,\bD_n)=\left(\frac{1}{N_t}+\frac{1}{N_c}\right)\cov(\bX_m,\bX_n),$$
% which implies
% $$\cov(\bD)_{m,n}=\left(\frac{1}{N_t}+\frac{1}{N_c}\right)\cov(\bX)_{m,n}, \ \forall m,n=1,\cdots, p.$$
% Hence, $$\bm\Sigma_\bD=\left(\frac{1}{N_t}+\frac{1}{N_c}\right)\bS_{\bX}^2 = \frac{1}{N} \bm V_{xx}.$$
According to the finite population central limit theorem, under regularity conditions in Condition \ref{cond1}, the large sampling distribution, over all randomizations, of $\sqrt{N}\bD$ is asymptotically Gaussian:
\[\begin{split}
    \sqrt{N}\bD \overset{\cdot}{\sim} \bm B,\ \bm B \sim N\left(\bm 0, \bV_{xx,N}\right),
\end{split}\]
where $\overset{\cdot}{\sim}$ means two random variables converge to the same distribution weakly. Hence,
\[\begin{split}
    \sqrt{N}\bD^T\bbeta_{\infty} \overset{\cdot}{\sim} \bm B^T\bbeta_{\infty} \sim N\left(\bm 0, \bbeta_{\infty}^T\bV_{xx,N}\bbeta_{\infty}\right).
\end{split}\]
Continuous mapping theorem and Slutsky's theorem imply that 
\[\begin{split}
    \frac{N\bD^T\bbeta_{\infty}\bbeta_{\infty}^T\bD}{\bbeta_{\infty}^T\bV_{xx,N}\bbeta_{\infty}} \overset{\cdot}{\sim} \frac{\bm B^T\bbeta_{\infty}\bbeta_{\infty}^T \bm B}{\bbeta_{\infty}^T\bV_{xx,N}\bbeta_{\infty}} \sim \chi^2_1.
\end{split}\]
Therefore, 
\[\begin{split}
    d_{\bm\beta_{\infty}}/\sigma^2_{d_{\bm\beta_{\infty}}} \xrightarrow{d} \chi^2_1,
\end{split}\]
where $\sigma^2_{d_{\bm\beta_{\infty}}} = \bm\beta_{\infty}^T\bV_{xx,N}\bm\beta_{\infty}$.
\QEDA
%\end{proof}

% \subsection{Proof of Lemma \ref{lemma3}}



\subsection{Proof of Theorem \ref{thm:ReO}}
It has been proved that minimizing $\bbV_a\big(\sqrt{N}(\hat{\tau}-\tau)|\phi=1\big)$ is equivalent to minimizing 
\[\begin{split}
    \bbE\left(\bm B_{\infty}^T\bm\beta_{\infty}\bm\beta_{\infty}^T\bm B_{\infty}|\bm B_{\infty} \in \mathcal{B}_{\phi,\infty}\right),
\end{split}\]
where $\bm B_{\infty} \sim N(\bm 0, \bm V_{xx,\infty})$ and $\mathcal{B}_{\phi,\infty}$ is asymptotic acceptance region of re-randomization procedure $\phi$.
Direct application of Theorem \ref{thm:GeneralSolution4OptimizationProblem} with $g(\bm Z)$ specified by $g(\bm Z)=\bm Z^T\bm\beta_{\infty}\bm\beta_{\infty}^T\bm Z$ leads to the following $\bm\beta_{\infty}$-specific optimal re-randomization acceptance region: 
\begin{equation}\label{criterion}
    \mathcal{B}_{\phi,\infty} = \big\{\bm \mu: \bm \mu^T\bm\beta_{\infty}\bm\beta_{\infty}^T\bm \mu \leq a\big\},
\end{equation}
where the threshold $a$ is determined by solving 
\[\begin{split}
    \bbP\left(\bm Z^T\bm\beta_{\infty}\bm\beta_{\infty}^T\bm Z \leq a\right) = \alpha.
\end{split}\]
Let $\bm Z = \bm B_{\infty}$ and we have $a = \bm\beta_{\infty}^T \bm V_{xx,\infty}\bm\beta_{\infty} \cdot \xi_{\alpha,1}$. Note that the balance criterion (\ref{ReO-cri}) can be equivalently expressed as 
\[\begin{split}
    \phi_{\bm\beta_{\infty}}\big(\sqrt{N}\bm D, \bm V_{xx,N}\big) = I\left(
    \frac{(\sqrt{N}\bm D)^T\bm\beta_{\infty}\bm\beta_{\infty}^T(\sqrt{N}\bm D)}{\bm\beta_{\infty}^T\bm V_{xx,N}\bm\beta_{\infty}}
    \leq \xi_{\alpha,1}\right),
\end{split}\]
which satisfies Condition \ref{cond2} for any $\alpha \in (0,1)$. Moreover, Lemma \ref{lma:AssymptoticDistribution4d_beta} implies that the asymptotic acceptance rate of $\phi_{\bm\beta_{\infty}}$ is 
\[\begin{split}
    \gamma_{\phi_{\bm\beta_{\infty}},\infty} = \lim_{N\rightarrow \infty} \bbP\big(d_{\bm\beta_{\infty}}/\sigma^2_{d_{\bm\beta_{\infty}}} \leq \xi_{\alpha,1}\big) = \alpha.
\end{split}\]
Hence, $\phi_{\bm\beta_{\infty}} \in \Phi_{\alpha,\infty}$. Also, the asymptotic acceptance region of $\phi_{\bm\beta_{\infty}}$ is 
\[\begin{split}
    \mathcal{B}_{\phi_{\bm\beta_{\infty}},\infty}=\left\{\bm\mu: \bm \mu^T\bm\beta_{\infty}\bm\beta_{\infty}^T\bm \mu \leq \bm\beta_{\infty}^T \bm V_{xx,\infty}\bm\beta_{\infty} \cdot \xi_{\alpha,1}\right\},
\end{split}\]
which is consistent with the form of (\ref{criterion}). Therefore, 
$$\phi_{\bbeta_{\infty}} = \argmin_{\phi \in \Phi_{\alpha,\infty}} \bbV_a\big(\sqrt{N}(\hat{\tau}-\tau)|\phi=1\big).$$
\QEDA
%\begin{proof}

\subsection{Proof of Theorem \ref{PRIAV-ReO}}
By Lemma \ref{lma:PRIAV_Phi}, 
$$r_{\phi_{\bbeta_{\infty}}} \triangleq \frac{\bbE\left(\bm B^T_{\infty}\bbeta_{\infty}\bbeta_{\infty}^T\bm B_{\infty}\mid\bm B_{\infty} \in \mathcal{B}_{\phi_{\bbeta_{\infty}},\infty}\right)}{\bbE\left(\bm B^T_{\infty}\bbeta_{\infty}\bbeta_{\infty}^T\bm B_{\infty}\right)},$$
where 
\[\begin{split}
    \mathcal{B}_{\phi_{\bbeta_{\infty}},\infty} = \left\{\bm\mu: \big(\bbeta_{\infty}^T\bmu\big)^2 \leq \bbeta_{\infty}^T \bm V_{xx,\infty} \bbeta_{\infty} \cdot \xi_{\alpha,1}\right\}.
\end{split}\]
Since $\bm B_{\infty} \sim N(\bm 0, \bm V_{xx,\infty})$, we have $\left(\bbeta_{\infty}^T\bm B_{\infty}\right)^2 \sim \bbeta_{\infty}^T \bm V_{xx,\infty} \bbeta_{\infty} \cdot \chi_1^2$ and
\[\begin{split}
    \bbE\left(\bm B^T_{\infty}\bbeta_{\infty}\bbeta_{\infty}^T\bm B_{\infty}\mid\bm B_{\infty} \in \mathcal{B}_{\phi_{\bbeta_{\infty}},\infty}\right)& = \bbE\left(\left(\bbeta_{\infty}^T\bm B_{\infty}\right)^2\mid\left(\bbeta_{\infty}^T\bm B_{\infty}\right)^2 \leq \bbeta_{\infty}^T \bm V_{xx,\infty} \bbeta_{\infty} \cdot \xi_{\alpha,1}\right)\\
    &=v_{\alpha,1}\cdot \bbeta_{\infty}^T \bm V_{xx,\infty} \bbeta_{\infty}.
\end{split}\]
Hence, $r_{\phi_{\bbeta_{\infty}}} =
    v_{\alpha,1}$ and
$$\text{PRIASV}_{ReO}= 100\times(1-v_{\alpha,1})R^2_{\infty}.$$
\QEDA

\subsection{Proof of Corollary \ref{cor:ReOvsReM}}
From the definition of $v_{\alpha,p}$, we have
$$v_{\alpha,p} = \frac{\bbP(\chi^2_{p+2}\leq \xi_{\alpha,p})}{\bbP(\chi^2_p\leq \xi_{\alpha,p})}
     =\frac{1}{\alpha}\cdot\frac{\int_0^{\xi_{\alpha,p}}{x^{\frac{p}{2}}e^{-\frac{x}{2}}}dx}{2^{\frac{p+2}{2}}\cdot\Gamma(\frac{p+2}{2})}.$$
     %=\frac{1}{\alpha}\cdot\frac{\int_0^{\xi_{\alpha,p}}{x^{\frac{p}{2}}e^{-\frac{x}{2}}}dx}{2^{\frac{p+2}{2}}\cdot\Gamma(\frac{p+2}{2})}.$$
Based on the integration by parts, 
\begin{eqnarray*}
\int_0^{\xi_{\alpha,p}}x^{\frac{p}{2}}e^{-\frac{x}{2}}dx
&=&-2\left[x^\frac{p}{2}e^{-\frac{x}{2}}|_{x=\xi_{\alpha,p}}-\frac{p}{2}\int_0^{\xi_{\alpha,p}}x^{\frac{p}{2}-1}e^{-\frac{x}{2}}dx\right]\\
&=&-2\cdot x^\frac{p}{2}e^{-\frac{x}{2}}|_{x=\xi_{\alpha,p}}+p\cdot\frac{\int_0^{\xi_{\alpha,p}}x^{\frac{p}{2}-1}e^{-\frac{x}{2}}dx}{2^{\frac{p}{2}}\cdot\Gamma(\frac{p}{2})}\cdot 2^{\frac{p}{2}}\cdot\Gamma(\frac{p}{2})\\
&=&-2\cdot x^\frac{p}{2}e^{-\frac{x}{2}}|_{x=\xi_{\alpha,p}}+p\cdot\bbP(\xi^2_p\leq\xi_{\alpha,p})\cdot 2^{\frac{p}{2}}\cdot\Gamma(\frac{p}{2})\\
&=&-2\cdot x^\frac{p}{2}e^{-\frac{x}{2}}|_{x=\xi_{\alpha,p}}+p\cdot\alpha\cdot 2^{\frac{p}{2}}\cdot\Gamma(\frac{p}{2}),
\end{eqnarray*}
indicating that
\begin{eqnarray*}
v_{\alpha,p}&=&\frac{1}{\alpha}\cdot\frac{-2\cdot x^\frac{p}{2}e^{-\frac{x}{2}}|_{x=\xi_{\alpha,p}}+p\cdot\alpha\cdot 2^{\frac{p}{2}}\cdot\Gamma(\frac{p}{2})}{2^{\frac{p+2}{2}}\cdot\Gamma(\frac{p+2}{2})}\nonumber\\
&=&-\frac{x^\frac{p}{2}e^{-\frac{x}{2}}|_{x=\xi_{\alpha,p}}}{\alpha\cdot 2^{\frac{p}{2}}\cdot\Gamma(\frac{p+2}{2})}+\frac{p\cdot\alpha\cdot 2^{\frac{p}{2}}\cdot\Gamma(\frac{p}{2})}{\alpha\cdot 2^{\frac{p+2}{2}}\cdot\Gamma(\frac{p+2}{2})}\nonumber\\
&=&-\frac{x^\frac{p}{2}e^{-\frac{x}{2}}|_{x=\xi_{\alpha,p}}}{\alpha\cdot 2^{\frac{p}{2}}\cdot\Gamma(\frac{p+2}{2})}+1,
\end{eqnarray*}
and thus,
$$1-v_{\alpha,p}=\frac{x^\frac{p}{2}e^{-\frac{x}{2}}|_{x=\xi_{\alpha,p}}}{\alpha\cdot 2^{\frac{p}{2}}\cdot\Gamma(\frac{p+2}{2})}.$$
Considering that $\chi_p^2$ is close to normal distribution $N(p,2p)$ when $p$ is large, we have
$$\xi_{\alpha,p}= p+\sqrt{2p}\cdot\Phi^{-1}(\alpha)+o(1),$$
and thus,
\begin{eqnarray*}
\frac{x^\frac{p}{2}\cdot e^{-\frac{x}{2}}|_{x=\xi_{\alpha,p}}}{p^{\frac{p}{2}}\cdot e^{-\frac{p}{2}}}
%&=&\left(p+\sqrt{2p}\cdot\Phi^{-1}(\alpha)+o(1)\right)^{\frac{p}{2}}\cdot\exp\left\{-\frac{p+\sqrt{2p}\cdot\Phi^{-1}(\alpha)+o(1)}{2}\right\}\\
&=&\left(1+\frac{\sqrt{2}\cdot\Phi^{-1}(\alpha)}{\sqrt{p}}+o\left(p^{-1}\right)\right)^{\frac{p}{2}}\cdot\exp\left\{-\frac{\sqrt{p}\cdot\Phi^{-1}(\alpha)+o(1)}{\sqrt{2}}\right\}\\
&\rightarrow&\exp\left\{\frac{\sqrt{p}\cdot\Phi^{-1}(\alpha)}{\sqrt{2}}\right\}\cdot\exp\left\{-\frac{\sqrt{p}\cdot\Phi^{-1}(\alpha)}{\sqrt{2}}\right\}=1,
\end{eqnarray*}
i.e., $x^\frac{p}{2}\cdot e^{-\frac{x}{2}}|_{x=\xi_{\alpha,p}}=O\left(p^{\frac{p}{2}}\cdot e^{-\frac{p}{2}}\right)$. 
On the other hand, because
$$\Gamma\left(\frac{p}{2}+1\right)=
\begin{cases}
\frac{p}{2}!, & p\mbox{ is even},\\
\frac{p!\sqrt{\pi}}{2^p(\frac{p-1}{2})!}, & p\mbox{ is odd},
\end{cases}
$$
and $p!=O(p^{p+\frac{1}{2}}\cdot e^{-p})$ based on the Stirling equation, we have 
$$\Gamma\left(\frac{p}{2}+1\right)=O\left(\left(\frac{p}{2}\right)^{\frac{p+1}{2}}\cdot e^{-\frac{p}{2}-1}\right).$$
And, thus
$$1-v_{\alpha,p}
=\frac{x^\frac{p}{2}e^{-\frac{x}{2}}|_{x=\xi_{\alpha,p}}}{\alpha\cdot 2^{\frac{p}{2}}\cdot\Gamma(\frac{p+2}{2})}
=\frac{O\left(p^{\frac{p}{2}}\cdot e^{-\frac{p}{2}}\right)}{\alpha\cdot 2^{\frac{p}{2}}\cdot O\left(\big(\frac{p}{2}\big)^{\frac{p+1}{2}}\cdot e^{-\frac{p}{2}-1}\right)}=O\left(\frac{1}{\sqrt{p}}\right).$$
Because $(1-v_{\alpha,1})$ is constant, we finally have
$$\frac{\text{PRIASV}_{ReM}}{\text{PRIASV}_{ReO}}=\frac{1-v_{\alpha,p}}{1-v_{\alpha,1}}=O\left(\frac{1}{\sqrt{p}}\right).$$
\QEDA

\subsection{Proof of Lemma \ref{lma:AssymptoticDistribution4d_pi}}
Suppose the Cholesky decomposition of $\bSigma_{\bD}$ is $\bSigma_{\bD} = \bSigma_{\bD}^{1/2}\big(\bSigma_{\bD}^{1/2}\big)^T$ and let
$\Tilde{\bZ} = \bSigma_{\bD}^{-1/2}\bD$. Then we have $\bD = \bSigma_{\bD}^{1/2}\Tilde{\bZ}$ and 
$$d_\pi=N\bD^T\bLambda_\pi\bD=N\Tilde{\bZ}^T\left(\big(\bSigma_{\bD}^{1/2}\big)^T\bLambda_\pi\bSigma_\bD^{1/2}\right)\Tilde{\bZ}=\Tilde{\bZ}^T\bm P\Tilde{\bZ},$$
where $\bm P=N\big(\bSigma_{\bD}^{1/2}\big)^T\bLambda_\pi\bSigma_\bD^{1/2}=\big(\bV_{xx,N}^{1/2}\big)^T\bLambda_\pi\bV_{xx,N}^{1/2}$ is a symmetric positive-definite real matrix with the following eigenvalue decomposition
$$\bm P=\bGamma diag\{\lambda_1,\cdots,\lambda_p\}\bGamma^T\quad\mbox{with}\quad\bGamma^T\bGamma=\bI_p.$$
Central limit theorem of finite population implies that $\sqrt{N}\bD$ is asymptotically Gaussian under Condition \ref{cond1}: 
\[\begin{split}
    \sqrt{N}\bD \overset{\cdot}{\sim} \bm B,\ \bm B \sim N(\bm 0, \bm V_{xx,N}).
\end{split}\]
Thus, 
$$\Tilde{\bZ} = \bSigma_{\bD}^{-1/2}\bD \overset{\cdot}{\sim} \bV_{xx,N}^{-1/2}\bm B \sim N(\bm 0, \bm I).$$
Therefore,
$$d_\pi=\Tilde{\bZ}^T\bm P\Tilde{\bZ} \overset{\cdot}{\sim} \sum_{j=1}^p\lambda_j Z_j^2,$$
where $Z_j$'s are i.i.d. standard normal random variables and weights $\lambda_j$'s are eigenvalues of matrix $\bm P$. \QEDA
%\end{proof}

\subsection{A useful lemma and its proof}
\begin{lemma}\label{lemma3}
Suppose random variables $X_n$ and $Y_n$ converge weakly to the same distribution as $X$, i.e.,
\[\begin{split}
    X_n \xrightarrow{d} X,\ Y_n \xrightarrow{d} X,
\end{split}\]
and the distribution function $F_X$ is continuous. Let $\xi_{Y_n,\alpha}$ be the $\alpha$-quantile of the distribution of $Y_n$, then 
\[\begin{split}
    \bbP\left(X_n\leq \xi_{Y_n,\alpha}\right) \rightarrow \alpha.
\end{split}\]
\end{lemma}
\begin{proof}\let\qed\relax
Let $\xi_{X,a}$ be the $\alpha$-quantile of distribution of random variable $X$, $F_{X_n}(\cdot),F_{Y_n}(\cdot),F_{X}(\cdot)$ be the distribution function of $X_n$, $Y_n$ and $X$ respectively. Polya's theorem implies 
\[\begin{split}
    &\lim_{n\rightarrow\infty} \sup_{x\in\mathcal{R}} \left|F_{X_n}(x) - F_X(x)\right|=0,\\
    &\lim_{n\rightarrow\infty} \sup_{x\in\mathcal{R}} \left|F_{Y_n}(x) - F_X(x)\right|=0.
\end{split}\]
Hence, 
$$\left|\bbP(X_n \leq \xi_{Y_n,\alpha}) - \bbP(X \leq \xi_{Y_n,\alpha})\right|\rightarrow 0.$$
Since
\[\begin{split}
    \left|\bbP(X \leq \xi_{Y_n,\alpha}) - \bbP(X \leq \xi_{X ,\alpha})\right| &= \left|F_X(\xi_{Y_n,\alpha}) -F_X(\xi_{X,\alpha}) \right|\\ &\leq \left|F_X(\xi_{Y_n,\alpha}) -F_{Y_n}(\xi_{Y_n,\alpha}) \right|+\left|F_{Y_n}(\xi_{Y_n,\alpha}) -F_X(\xi_{X,\alpha}) \right|\\ &= \left|F_X(\xi_{Y_n,\alpha}) -F_{Y_n}(\xi_{Y_n,\alpha}) \right| \rightarrow 0,
\end{split}\]
we have 
\[\begin{split}
    \left|\bbP(X_n \leq \xi_{Y_n,\alpha}) - \bbP(X \leq \xi_{X,\alpha})\right| &\leq \left|\bbP(X_n \leq \xi_{Y_n,\alpha}) - \bbP(X \leq \xi_{Y_n,\alpha})\right|\\&+
    \left|\bbP(X \leq \xi_{Y_n,\alpha}) - \bbP(X \leq \xi_{X,\alpha})\right| \rightarrow 0.
\end{split}\]
That is, 
\[\begin{split}
    \bbP(X_n \leq \xi_{Y_n,\alpha}) \rightarrow \alpha.
\end{split}\]
\QEDA
\end{proof}

%\begin{proof}
\subsection{Proof of Theorem \ref{thm:ReB}}
Denote the limit of $\bLambda_{\pi}$ as $\Tilde{\bLambda}_{\pi}$. Let
$\bm Z = \bm B_{\infty}$ and $g(\bm Z)=\bm Z^T\Tilde{\bLambda}_{\pi}\bm Z$, Theorem \ref{thm:GeneralSolution4OptimizationProblem}  implies that the optimal asymptotic acceptance region $\mathcal{B}_{\phi,\infty}$ of $\sqrt{N}\bm D$ for $\phi\in\Phi_{\alpha,a}$ under Bayesian framework should be in the form of 
\begin{equation}\label{B_phi}
    \mathcal{B}_{\phi,\infty} = \left\{\bm\mu: \bm\mu^T\Tilde{\bLambda}_{\pi}\bm\mu \leq a\right\}.
\end{equation}
The re-randomization criterion in \eqref{eq:ReB} can be formulated as 
\[\begin{split}
    \phi_{\pi}\big(\sqrt{N}\bm D,\bm V_{xx,N}\big) = I\Big(\big(\sqrt{N}\bm D\big)^T \bm\Lambda_{\pi} \big(\sqrt{N}\bm D\big)\leq \xi_{\alpha,\blambda}\Big).
\end{split}\]
For any $\alpha \in (0,1)$, $\phi_{\pi}$ satisfies Condition \ref{cond2}. As shown in Lemma \ref{lma:AssymptoticDistribution4d_pi}, 
\[\begin{split}
    \bm d_{\pi} \overset{\cdot}{\sim} \chi^2_{\blambda} = \sum_{j=1}^p \lambda_j Z_j^2.
\end{split}\]
Since 
\[\begin{split}
    \chi^2_{\blambda} = \sum_{j=1}^p \lambda_j Z_j^2 \xrightarrow{d} \chi^2_{\blambda_{\infty}} = \sum_{j=1}^p \lambda_{j,\infty} Z_j^2,
\end{split}\]
where $\blambda_{\infty} = (\lambda_{1,\infty},...,\lambda_{p,\infty})$ is vector composed of eigenvalues of $\bm P_{\infty} = \lim_{N\rightarrow\infty} \bm P = \lim_{N\rightarrow\infty}\big(\bm V_{xx,N}^{1/2}\big)^T\bLambda_{\pi}\bm V_{xx,N}^{1/2}$, Lemma \ref{lemma3} implies that the asymptotic acceptance probability of $\phi_{\pi}$ is 
\[\begin{split}
\gamma_{\phi_{\pi}} = \lim_{N\rightarrow\infty} \bbP\left(\bm d_{\pi} \leq \xi_{\alpha,\blambda}\right) = \alpha.
\end{split}\]
Hence, $\phi_{\bbeta_{\infty}} \in \Phi_{\alpha,\infty}$. Moreover, the asymptotic acceptance region of $\phi_{\pi}$ about $\sqrt{N}\bm D$ is 
\[\begin{split}
    \mathcal{B}_{\phi_{\pi},\infty} = \left\{\bm\mu: \bm\mu^T\Tilde{\bLambda}_{\pi}\bm\mu \leq \xi_{\alpha,\blambda_{\infty}}\right\},
\end{split}\]
which is consistent with the form in \eqref{B_phi}. Therefore, 
$$\phi_{\pi} = \argmin_{\phi \in \Phi_{\alpha,\infty}} \bbV_{\pi,a}\left(\sqrt{N}(\hat{\tau}-\tau)|\phi=1\right).$$
\QEDA

\subsection{Proof of Corollary \ref{invariance}}

Based on Theorem \ref{thm:ReB}, we have
\begin{eqnarray*}
&&\phi_{\pi_1}=I\left(d_{\pi_1}\leq \xi_{\alpha,\blambda_1}\right),\\
&&\phi_{\pi_2}=I\left(d_{\pi_2}\leq \xi_{\alpha,\blambda_2}\right),
\end{eqnarray*}
where $\blambda_1$ and $\blambda_1$ are vectors of eigenvalues of $\big(\bm V_{xx,N}^{1/2}\big)^T\bLambda_{\pi_1}\bm V_{xx,N}^{1/2}$ and $\big(\bm V_{xx,N}^{1/2}\big)^T\bLambda_{\pi_2}\bm V_{xx,N}^{1/2}$, respectively. Since $\bLambda_{\pi_2}=r\bLambda_{\pi_1}$, we have 
$$d_{\pi_2}=N\bD^T\bLambda_{\pi_2}\bD=rN\bD^T\bLambda_{\pi_1}\bD=r\cdot d_{\pi_1},\ \xi_{\alpha,\blambda_2} = r\cdot \xi_{\alpha,\blambda_1},$$
and thus
$$\phi_{\pi_2}=I\left(d_{\pi_2}\leq \xi_{\alpha,\blambda_2}\right)=I\left(r\cdot d_{\pi_1}\leq r \cdot \xi_{\alpha,\blambda_1}\right) = \phi_{\pi_1}.$$ The proof is complete. \QEDA

\subsection{Proof of Theorem \ref{thm:ReB2fullrank}}
% By equation \eqref{eq:AsymptoticVariance4Phi},
% \[\begin{split}
%         \bbV_a\left(\sqrt{N}(\hat{\tau}-\tau)\mid\phi_{\pi}=1\right)&=
%     (1-R^2_{\infty})V_{\tau\tau,\infty} + \bbE\left(\bm B^T_{\infty}\bbeta_{\infty}\bbeta_{\infty}^T\bm B_{\infty}\mid\bm B_{\infty} \in \mathcal{B}_{\phi_{\pi},\infty}\right),
% \end{split}\]
% where $\bm B_{\infty} \sim N(\bm 0, \bm V_{xx,\infty})$ and 
% \[\begin{split}
%     \mathcal{B}_{\phi_{\pi},\infty} = \left\{\bmu:\bmu^T \bLambda_{\pi}\bmu \leq \xi_{\alpha,\blambda_{\infty}}\right\},
% \end{split}\]
% where $\blambda_{\infty} = (\lambda_{1,\infty},...,\lambda_{p,\infty})^T$ is the vector of eigenvalues of  $\bm P_{\infty}=\left(\bV_{xx,\infty}^{1/2}\right)^T\bLambda_\pi\bV_{xx,\infty}^{1/2}$ and $\bm B_{\infty}^T\bLambda_{\pi}\bm B_{\infty} \sim \chi^2_{\blambda_{\infty}}$. 
By Lemma \ref{lma:PRIAV_Phi}, 
$$r_{\phi_{\pi}} \triangleq \frac{\bbE\left(\bm B^T_{\infty}\bbeta_{\infty}\bbeta_{\infty}^T\bm B_{\infty}\mid\bm B_{\infty} \in \mathcal{B}_{\phi_{\pi},\infty}\right)}{\bbeta_{\infty}^T \bm V_{xx,\infty} \bbeta_{\infty}},$$
where $\bm B_{\infty} \sim N(\bm 0, \bm V_{xx,\infty})$ and 
\[\begin{split}
    \mathcal{B}_{\phi_{\pi},\infty} = \left\{\bmu:\bmu^T \Tilde{\bLambda}_{\pi}\bmu \leq \xi_{\alpha,\blambda_{\infty}}\right\},
\end{split}\]
where $\blambda_{\infty} = (\lambda_{1,\infty},...,\lambda_{p,\infty})^T$ is the vector of eigenvalues of  $\bm P_{\infty}=\big(\bV_{xx,\infty}^{1/2}\big)^T\Tilde{\bLambda}_\pi\bV_{xx,\infty}^{1/2}$ and $\bm B_{\infty}^T\Tilde{\bLambda}_{\pi}\bm B_{\infty} \sim \chi^2_{\blambda_{\infty}}$. 
% then 
% \[\begin{split}
% \bbV_a\left(\sqrt{N}(\hat{\tau}-\tau)\mid\phi_{\pi}=1\right)&=
%     (1-R^2_{\infty})V_{\tau\tau,\infty} + v_{\alpha,\pi}\cdot
%     \bbE\left(\bm B^T_{\infty}\bbeta_{\infty}\bbeta_{\infty}^T\bm B_{\infty}\right)\\
%     &= (1-R^2_{\infty})V_{\tau\tau,\infty} + v_{\alpha,\pi}\cdot
%     R^2_{\infty}V_{\tau\tau,\infty}
% \end{split}\]
% and the PRIASV of criterion $\phi_{\pi}$ is
% $$PRIASV_{ReB} = 100\times (1-v_{\alpha,\pi})R^2_{\infty}.$$
% \\
% \textbf{(1) When $\bLambda_{\pi}$ is symmetric and positive definite:}
Since the Cholesky decomposition of $\Tilde{\bLambda}_{\pi}$ is $\Tilde{\bLambda}_{\pi} = \bm M \bm M^T$, let $\bm Q =  \bm M^T \bm B_{\infty}$, then
\[\begin{split}
\bbE\left(\bm B^T_{\infty}\bbeta_{\infty}\bbeta_{\infty}^T\bm B_{\infty}\mid \bm B_{\infty} \in \mathcal{B}_{\phi_{\pi},\infty}\right) &= 
\bbE\left(\bbeta_{\infty}^T\bm B_{\infty}\bm B^T_{\infty}\bbeta_{\infty}\mid\bm B_{\infty}^T\Tilde{\bLambda}_{\pi}\bm B_{\infty} \leq \xi_{\alpha,\blambda_{\infty}}\right)\\
&= \bbeta_{\infty}^T \left(\bm M^{-1}\right)^T
\bbE\left(\bm Q\bm Q^T\mid\bm Q^T\bm Q \leq \xi_{\alpha,\blambda_{\infty}}\right) \bm M^{-1}\bbeta_{\infty},
\end{split}\]
where $\bm Q^T\bm Q \sim \chi^2_{\blambda_{\infty}}$.
Since $\bm Q \sim N\left(\bm0,\bm M^T \bm V_{xx,\infty} \bm M\right)$ and $\bm M^T \bm V_{xx,\infty} \bm M$ has the same eigenvalues as $\bm P_{\infty}$, it can be expressed as 
\[\begin{split}
    \bm M^T \bm V_{xx,\infty} \bm M = \bm\Gamma^T_{\infty} diag\{\lambda_{1,\infty},...,\lambda_{p,\infty}\} \bm\Gamma_{\infty},
\end{split}\]
where $\bm\Gamma_{\infty}$ is an orthogonal matrix. Let $\bm \Lambda = diag\{\lambda_{1,\infty},...,\lambda_{p,\infty}\}$,
then $\Tilde{\bm Q} = \bm\Gamma_{\infty}\bm Q\sim N\left(\bm0,\bm \Lambda\right)$ and 
\[\begin{split}
&\bbE\big(\bm B^T_{\infty}\bbeta_{\infty}\bbeta_{\infty}^T\bm B_{\infty}\mid \bm B_{\infty} \in \mathcal{B}_{\phi_{\pi},\infty}\big)\\ &= 
\bbeta_{\infty}^T \left(\bm M^{-1}\right)^T\bm\Gamma^T_{\infty}
\bbE\big(\Tilde{\bm Q}\Tilde{\bm Q}^T\mid\Tilde{\bm Q}^T\Tilde{\bm Q} \leq \xi_{\alpha,\blambda_{\infty}}\big)\bm\Gamma_{\infty} \bm M^{-1}\bbeta_{\infty}.
\end{split}\]
Suppose $\bm \Tilde{\bm Q} = \left(q_1,...,q_p\right)^T$, for $i \not= j$, symmetry implies
\[\begin{split}
    \bbE\big(q_iq_j\mid\Tilde{\bm Q}^T\Tilde{\bm Q} \leq \xi_{\alpha,\blambda_{\infty}}\big) &= \bbE\left(\bbE\big(q_iq_j\mid q_j,\Tilde{\bm Q}^T\Tilde{\bm Q} \leq \xi_{\alpha,\blambda_{\infty}}\big)\mid\Tilde{\bm Q}^T\Tilde{\bm Q} \leq \xi_{\alpha,\blambda_{\infty}}\right)\\
    &= \bbE\left(q_j\bbE\big(q_i\mid q_j,\Tilde{\bm Q}^T\Tilde{\bm Q} \leq \xi_{\alpha,\blambda_{\infty}}\big)\mid\Tilde{\bm Q}^T\Tilde{\bm Q} \leq \xi_{\alpha,\blambda_{\infty}}\right)\\
    &= \bbE\big(q_j\times 0\mid\Tilde{\bm Q}^T\Tilde{\bm Q} \leq \xi_{\alpha,\blambda_{\infty}}\big)=0.
\end{split}\]
Let $c_i=\bbE\left(\lambda_{i,\infty} Z_i^2\mid \sum_{j=1}^p \lambda_{j,\infty} Z_j^2 \leq \xi_{\alpha,\blambda_{\infty}}\right)$, where $i=1,\ldots,p$ and $Z_1,\ldots,Z_p$ are i.i.d. standard normal distributed random variables, then
\[\begin{split}
\bbE\big(\Tilde{\bm Q}\Tilde{\bm Q}^T\mid\Tilde{\bm Q}^T\Tilde{\bm Q} \leq \xi_{\alpha,\blambda_{\infty}}\big) &= diag\left\{\bbE\big(q_i^2\mid \Tilde{\bm Q}^T\Tilde{\bm Q} \leq \xi_{\alpha,\blambda_{\infty}}\right)\big\}_{i=1}^p\\
&= diag\left\{\bbE\left(\lambda_{i,\infty} Z_i^2\mid \sum_{j=1}^p \lambda_{j,\infty} Z_j^2 \leq \xi_{\alpha,\blambda_{\infty}}\right)\right\}_{i=1}^p\\
&= \bm C = diag\{c_1,...,c_p\}.
\end{split}\]
Therefore, 
\[\begin{split}
& \indent \bbeta_{\infty}^T \big(\bm M^{-1}\big)^T\bm\Gamma^T_{\infty}
\bbE\big(\Tilde{\bm Q}\Tilde{\bm Q}^T\mid\Tilde{\bm Q}^T\Tilde{\bm Q} \leq \xi_{\alpha,\blambda_{\infty}}\big)\bm\Gamma_{\infty} \bm M^{-1}\bbeta_{\infty}\\ &= \bbeta_{\infty}^T \big(\bm M^{-1}\big)^T\bm\Gamma^T_{\infty}
\bm C\bm\Gamma_{\infty} \bm M^{-1}\bbeta_{\infty},
\end{split}\]
and thus, 
\[\begin{split}
    r_{\phi_{\pi}} = \frac{\bbE\big(\bm B^T_{\infty}\bbeta_{\infty}\bbeta_{\infty}^T\bm B_{\infty}\mid\bm B_{\infty} \in \mathcal{B}_{\phi_{\pi},\infty}\big)}{\bbeta_{\infty}^T \bm V_{xx,\infty} \bbeta_{\infty}} &= \frac{\bbeta_{\infty}^T \big(\bm M^{-1}\big)^T\bm\Gamma^T_{\infty}
\bm C\bm\Gamma_{\infty} \bm M^{-1}\bbeta_{\infty}}{\bbeta_{\infty}^T \bm V_{xx,\infty}\bbeta_{\infty}}.
\end{split}\]
\QEDA

\subsection{Proof of Theorem \ref{thm:ReB2sigma0}}
When $\bm\Sigma_{\pi}=\bm 0$, $\Tilde{\bLambda}_{\pi} = \bmu_{\pi}\bmu_{\pi}^T$, we have 
$$r_{\phi_{\pi}} = \frac{\bbE\left[(\bbeta_{\infty}^T\bm B_{\infty})^2\mid (\bm \mu_{\pi}^T\bm B_{\infty})^2 \leq \xi_{\alpha,\blambda_{\infty}}\right]}{\bbeta_{\infty}^T \bm V_{xx,\infty} \bbeta_{\infty}},$$
which does not depend on the $L_2$ norm of $\bbeta_{\infty}$ and $\bmu_{\pi}$. W.L.O.G., suppose $||\bbeta_{\infty}||_2 = ||\bmu_{\pi}||_2 =1$. $\bbeta_{\infty}$ can be decomposed to a combination of orthogonal vectors $\bmu_{\pi}$ and $\bm\epsilon_{\pi}$, i.e., 
\[\begin{split}
    \bbeta_{\infty} = \cos\theta_0 \cdot\bmu_{\pi} + \sin\theta_0 \cdot\bm\epsilon_{\pi},
\end{split}\]
where $\bm\epsilon_{\pi}$ is orthogonal to  $\bmu_{\pi}$, $\theta_0$ is the angle between $\bbeta_{\infty}$ and $\bmu_{\pi}$, and $||\bm\epsilon_{\pi}||_2 = ||\bmu_{\pi}||_2 = 1$. The joint distribution of $\bmu_{\pi}^T\bm B_{\infty}$ and $\bm\epsilon_{\pi}^T\bm B_{\infty}$ is
\[\begin{split}
    \left(\begin{array}{c}
        \bmu_{\pi}^T\bm B_{\infty} \\
        \bm\epsilon_{\pi}^T\bm B_{\infty}
    \end{array}\right) &\sim 
    N\left(\left(\begin{array}{c}
        0 \\
        0 
    \end{array}\right),
    \left(\begin{array}{cc}
        \bmu_{\pi}^T \bV_{xx,\infty} \bmu_{\pi} &
        \bmu_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi}\\
        \bmu_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi} &
        \bm\epsilon_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi}
    \end{array}\right)\right),
\end{split}\]
and the conditional distribution of $\bm\epsilon_{\pi}^T \bm B_{\infty}$ given 
$\bmu_{\pi}^T\bm B_{\infty}$ is 
\[\begin{split}
    \bm\epsilon_{\pi}^T \bm B_{\infty}\mid \bmu_{\pi}^T\bm B_{\infty} &\sim
    N\left(\frac{\bmu_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi}}{\bmu_{\pi}^T \bV_{xx,\infty} \bmu_{\pi}}\cdot \bmu_{\pi}^T\bm B_{\infty},
    \bm\epsilon_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi} - \frac{(\bmu_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi})^2}{\bmu_{\pi}^T \bV_{xx,\infty} \bmu_{\pi}}\right).
\end{split}\]
Therefore, 
\[\begin{split}
    &\indent \bbE\left[(\bbeta_{\infty}^T\bm B_{\infty})^2\mid (\bm \mu_{\pi}^T\bm B_{\infty})^2 \leq \xi_{\alpha,\blambda_{\infty}}\right]\\ &= \bbE\left[(\cos\theta_0\cdot\bmu_{\pi}^T\bm B_{\infty}+\sin\theta_0\cdot\bm\epsilon_{\pi}^T\bm B_{\infty})^2\mid (\bm \mu_{\pi}^T\bm B_{\infty})^2 \leq \xi_{\alpha,\blambda_{\infty}}\right]\\
    &= v_{\alpha,1}\cdot(\cos\theta_0)^2\cdot\bmu_{\pi}^T \bV_{xx,\infty} \bmu_{\pi} + 2v_{\alpha,1}\cdot \sin\theta_0\cos\theta_0\cdot \bmu_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi}\\ &+ (\sin\theta_0)^2\left[\bm\epsilon_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi} - (1-v_{\alpha,1})\cdot \frac{(\bmu_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi})^2}{\bmu_{\pi}^T \bV_{xx,\infty} \bmu_{\pi}}\right]\\
    &= v_{\alpha,1}\cdot \bbeta_{\infty}^T \bm V_{xx,\infty} \bbeta_{\infty}+(1-v_{\alpha,1})(\sin\theta_0)^2\left[\bm\epsilon_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi} - \frac{(\bmu_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi})^2}{\bmu_{\pi}^T \bV_{xx,\infty} \bmu_{\pi}}\right].
\end{split}\]
Since $\bm\epsilon_{\pi} = (\bbeta_{\infty} - \cos\theta_0 \cdot\bmu_{\pi})/\sin\theta_0$, we have 
\[\begin{split}
(\sin\theta_0)^2\left[\bm\epsilon_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi} - \frac{(\bmu_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi})^2}{\bmu_{\pi}^T \bV_{xx,\infty} \bmu_{\pi}}\right]
    &= \bbeta_{\infty}^T \bV_{xx,\infty} \bbeta_{\infty} - \frac{(\bmu_{\pi}^T \bV_{xx,\infty} \bbeta_{\infty})^2}{\bmu_{\pi}^T \bV_{xx,\infty} \bmu_{\pi}},
\end{split}\]
and thus,
$$r_{\phi_{\pi}} = \frac{\bbE\left[(\bbeta_{\infty}^T\bm B_{\infty})^2\mid (\bm \mu_{\pi}^T\bm B_{\infty})^2 \leq \xi_{\alpha,\blambda_{\infty}}\right]}{\bbeta_{\infty}^T \bm V_{xx,\infty} \bbeta_{\infty}}=1 - (1-v_{\alpha,1})\cdot\frac{(\bmu_{\pi}^T \bV_{xx,\infty} \bbeta_{\infty})^2}{(\bmu_{\pi}^T \bV_{xx,\infty} \bmu_{\pi})(\bbeta_{\infty}^T \bV_{xx,\infty} \bbeta_{\infty})}.$$
\QEDA
% \subsection{Proof of Theorem \ref{point-estimation}}
% By equation \eqref{eq:AsymptoticVariance4Phi},
% \[\begin{split}
%         \bbV_a\left(\sqrt{N}(\hat{\tau}-\tau)\mid\phi_{\pi}=1\right)&=
%     (1-R^2_{\infty})V_{\tau\tau,\infty} + \bbE\left(\bm B^T_{\infty}\bbeta_{\infty}\bbeta_{\infty}^T\bm B_{\infty}\mid\bm B_{\infty} \in \mathcal{B}_{\phi_{\pi},\infty}\right)\\
%     &= h_1(\bbeta_{\infty}) + h_2(\bbeta_{\infty},\phi_{\pi,\infty}).
% \end{split}\]
% If $\bm \Sigma_{\pi} = \bm 0$, $\bLambda_{\pi} = \bmu_{\pi}\bmu_{\pi}^T$, we have 
% \[\begin{split}
%     h_2(\bbeta_{\infty},\phi_{\pi,\infty})
% &=  
% \bbE\left[(\bbeta_{\infty}^T\bm B_{\infty})^2\mid (\bm \mu_{\pi}^T\bm B_{\infty})^2 \leq \xi_{\alpha,\blambda_{\infty}}\right],
% \end{split}\]
% where $\bm \mu_{\pi}^T\bm B_{\infty} \sim N(\bm0,\bm \mu_{\pi}^T \bm V_{xx,\infty} \bm \mu_{\pi})$. It's easy to see that when $\bmu_{\pi} = \bbeta_{\infty}$, 
% \[\begin{split}
% \bbE\left[(\bbeta_{\infty}^T\bm B_{\infty})^2\mid (\bm \mu_{\pi}^T\bm B_{\infty})^2 \leq \xi_{\alpha,\blambda_{\infty}}\right]
% = v_{\alpha,1}\bbE\left[(\bbeta_{\infty}^T\bm B_{\infty})^2\right].
% \end{split}\]
% When $\bmu_{\pi} \not= \bbeta_{\infty}$, suppose $||\bmu_{\pi}|| = ||\bbeta_{\infty}||=1$, $\bbeta_{\infty}$ can be decomposed to a combination of orthogonal vectors $\bmu_{\pi}$ and $\bm\epsilon_{\pi}$, i.e., 
% \[\begin{split}
%     \bbeta_{\infty} = \cos\theta \cdot\bmu_{\pi} + \sin\theta \cdot\bm\epsilon_{\pi},
% \end{split}\]
% where $\bm\epsilon_{\pi}$ is orthogonal to  $\bmu_{\pi}$ and $||\bm\epsilon_{\pi}|| = ||\bmu_{\pi}|| = 1$. The joint distribution of $\bmu_{\pi}^T\bm B_{\infty}$ and $\bm\epsilon_{\pi}^T\bm B_{\infty}$ is
% \[\begin{split}
%     \left(\begin{array}{c}
%         \bmu_{\pi}^T\bm B_{\infty} \\
%         \bm\epsilon_{\pi}^T\bm B_{\infty}
%     \end{array}\right) &\sim 
%     N\left(\left(\begin{array}{c}
%         0 \\
%         0 
%     \end{array}\right),
%     \left(\begin{array}{cc}
%         \bmu_{\pi}^T \bV_{xx,\infty} \bmu_{\pi} &
%         \bmu_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi}\\
%         \bmu_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi} &
%         \bm\epsilon_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi}
%     \end{array}\right)\right),
% \end{split}\]
% and the conditional distribution of $\bm\epsilon_{\pi}^T \bm B_{\infty}$ given 
% $\bmu_{\pi}^T\bm B_{\infty}$ is 
% \[\begin{split}
%     \bm\epsilon_{\pi}^T \bm B_{\infty}\mid \bmu_{\pi}^T\bm B_{\infty} &\sim
%     N\left(\frac{\bmu_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi}}{\bmu_{\pi}^T \bV_{xx,\infty} \bmu_{\pi}}\cdot \bmu_{\pi}^T\bm B_{\infty},
%     \bm\epsilon_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi} - \frac{(\bmu_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi})^2}{\bmu_{\pi}^T \bV_{xx,\infty} \bmu_{\pi}}\right).
% \end{split}\]
% Therefore, $\bm\epsilon_{\pi}^T \bm B_{\infty}$ given 
% $\bmu_{\pi}^T\bm B_{\infty}$ is identically distributed as $\bm Z_1+\bm Z_2$,
% where $\bm Z_1 =\frac{\bmu_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi}}{\bmu_{\pi}^T \bV_{xx,\infty} \bmu_{\pi}}\cdot \bmu_{\pi}^T\bm B_{\infty}$ and 
% $\bm Z_2 \sim N\left(0,\bm\epsilon_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi} - \frac{(\bmu_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi})^2}{\bmu_{\pi}^T \bV_{xx,\infty} \bmu_{\pi}}\right)$ are independent.
% Hence, 
% \[\begin{split}
%     \bbE\left[(\bm \epsilon_{\pi}^T\bm B_{\infty})^2\mid (\bm \mu_{\pi}^T\bm B_{\infty})^2 \leq \xi_{\alpha,\blambda_{\infty}}\right] &= 
%     \bbE\left[(\bm Z_1+\bm Z_2)^2\mid (\bm \mu_{\pi}^T\bm B_{\infty})^2 \leq \xi_{\alpha,\blambda_{\infty}}\right]\\
%     &= v_{\alpha,1} \cdot \frac{\left(\bmu_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi}\right)^2}{\bmu_{\pi}^T \bV_{xx,\infty} \bmu_{\pi}} + \bm\epsilon_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi} - \frac{(\bmu_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi})^2}{\bmu_{\pi}^T \bV_{xx,\infty} \bmu_{\pi}}\\
%     &= \bm\epsilon_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi} -(1-v_{\alpha,1}) \cdot \frac{\left(\bmu_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi}\right)^2}{\bmu_{\pi}^T \bV_{xx,\infty} \bmu_{\pi}}\\
%     &\leq \bm\epsilon_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi}.
% \end{split}\]
% Let the maximum eigenvalue of $\bV_{xx,\infty}$ be $\lambda_{\max}$, then $\bm\epsilon_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi} \leq \lambda_{\max}$ and thus, 
% \[\begin{split}
%     \bbE\left[(\bm \epsilon_{\pi}^T\bm B_{\infty})^2\mid (\bm \mu_{\pi}^T\bm B_{\infty})^2 \leq \xi_{\alpha,\blambda_{\infty}}\right] 
%     &\leq \lambda_{\max}.
% \end{split}\]

% For $\theta \in [0,\pi/2]$, Cauchy-Schwarz inequality implies 
% \[\begin{split}
% h_2(\bbeta_{\infty},\phi_{\pi,\infty})
% &= (\cos\theta)^2\cdot \bbE\left[(\bm \mu_{\pi}^T\bm B_{\infty})^2\mid (\bm \mu_{\pi}^T\bm B_{\infty})^2 \leq \xi_{\alpha,\blambda_{\infty}}\right]\\& + (\sin\theta)^2 \cdot \bbE\left[(\bm \epsilon_{\pi}^T\bm B_{\infty})^2\mid (\bm \mu_{\pi}^T\bm B_{\infty})^2 \leq \xi_{\alpha,\blambda_{\infty}}\right] \\&+ 2\sin\theta\cos\theta \cdot \bbE\left[(\bm \mu_{\pi}^T\bm B_{\infty})(\bm \epsilon_{\pi}^T\bm B_{\infty})\mid (\bm \mu_{\pi}^T\bm B_{\infty})^2 \leq \xi_{\alpha,\blambda_{\infty}}\right]\\
% &\leq (\cos\theta)^2\cdot \bbE\left[(\bm \mu_{\pi}^T\bm B_{\infty})^2\mid (\bm \mu_{\pi}^T\bm B_{\infty})^2 \leq \xi_{\alpha,\blambda_{\infty}}\right]\\& + (\sin\theta)^2 \cdot \bbE\left[(\bm \epsilon_{\pi}^T\bm B_{\infty})^2\mid (\bm \mu_{\pi}^T\bm B_{\infty})^2 \leq \xi_{\alpha,\blambda_{\infty}}\right] \\&+ 2\sin\theta\cos\theta \cdot \sqrt{\bbE\left[(\bm \mu_{\pi}^T\bm B_{\infty})^2\mid (\bm \mu_{\pi}^T\bm B_{\infty})^2 \leq \xi_{\alpha,\blambda_{\infty}}\right] \cdot \bbE\left[(\bm \epsilon_{\pi}^T\bm B_{\infty})^2\mid (\bm \mu_{\pi}^T\bm B_{\infty})^2 \leq \xi_{\alpha,\blambda_{\infty}}\right]}\\
% &= \left\{\cos\theta\cdot \sqrt{v_{\alpha,1}\cdot \bmu_{\pi}^T \bV_{xx,\infty}\bmu_{\pi}} + \sin\theta \cdot \sqrt{\bbE\left[(\bm \epsilon_{\pi}^T\bm B_{\infty})^2\mid (\bm \mu_{\pi}^T\bm B_{\infty})^2 \leq \xi_{\alpha,\blambda_{\infty}}\right]}\right\}^2.
% \end{split}\]
% Denote $a^2 \triangleq \bm \mu_{\pi}^T \bm V_{xx,\infty} \bm \mu_{\pi}$, $b^2 \triangleq \bm \epsilon_{\pi}^T\bm V_{xx,\infty}\bm \epsilon_{\pi}$ and $c^2 \triangleq \bbE\left[(\bm \epsilon_{\pi}^T\bm B_{\infty})^2\mid (\bm \mu_{\pi}^T\bm B_{\infty})^2 \leq \xi_{\alpha,\blambda_{\infty}}\right]$, then 
% \[\begin{split}
% \bbE\left[(\bbeta_{\infty}^T\bm B_{\infty})^2\mid (\bm \mu_{\pi}^T\bm B_{\infty})^2 \leq \xi_{\alpha,\blambda_{\infty}}\right]
% &\leq \left(\cos\theta\cdot \sqrt{v_{\alpha,1}}\cdot a + \sin\theta \cdot c\right)^2.
% \end{split}\]
% Since $0\leq c \leq \sqrt{\lambda_{\max}}$, we have 
% \[\begin{split}
% \bbE\left[(\bbeta_{\infty}^T\bm B_{\infty})^2\mid (\bm \mu_{\pi}^T\bm B_{\infty})^2 \leq \xi_{\alpha,\blambda_{\infty}}\right]
% &\leq \left(\cos\theta\cdot \sqrt{v_{\alpha,1}}\cdot a + \sin\theta \cdot \sqrt{\lambda_{\max}}\right)^2.
% \end{split}\]
% Similarly,  
% \[\begin{split}
% \bbE(\bbeta_{\infty}^T\bm B_{\infty})^2
% &= (\cos\theta)^2\cdot \bbE(\bm \mu_{\pi}^T\bm B_{\infty})^2 + (\sin\theta)^2 \cdot \bbE(\bm \epsilon_{\pi}^T\bm B_{\infty})^2+ 2\sin\theta\cos\theta \cdot \bbE(\bm \mu_{\pi}^T\bm B_{\infty})(\bm \epsilon_{\pi}^T\bm B_{\infty})\\
% &\geq (\cos\theta)^2\cdot \bbE(\bm \mu_{\pi}^T\bm B_{\infty})^2 + (\sin\theta)^2 \cdot \bbE(\bm \epsilon_{\pi}^T\bm B_{\infty})^2- 2\sin\theta\cos\theta \cdot \sqrt{\bbE(\bm \mu_{\pi}^T\bm B_{\infty})^2\cdot \bbE(\bm \epsilon_{\pi}^T\bm B_{\infty})^2}\\
% &= \left\{\cos\theta\cdot \sqrt{\bmu_{\pi}^T \bV_{xx,\infty}\bmu_{\pi}} - \sin\theta \cdot \sqrt{\bm\epsilon_{\pi}^T \bV_{xx,\infty}\bm\epsilon_{\pi}}\right\}^2\\
% &= \left(\cos\theta\cdot a - \sin\theta \cdot b\right)^2.
% \end{split}\]
% Denote the last expression on the RHS as $f(\theta,b)$.
% When $\sqrt{\lambda_{\max}} < a/\tan\theta$, which holds if $\theta < \theta_1 = \arctan\left( a/\sqrt{\lambda_{\max}}\right) \in (0,\pi/2)$, $f(\theta,b)$ is monotonically decreasing about $b$. Therefore, for $\theta < \theta_1$, 
% we have 
% \[\begin{split}
%     \bbE(\bbeta_{\infty}^T\bm B_{\infty})^2
% &\geq \left(\cos\theta\cdot a - \sin\theta \cdot \sqrt{\lambda_{\max}}\right)^2
% \end{split}\]
% and 
% \[\begin{split}
%     v_{\alpha,\pi} = \frac{\bbE\left[(\bbeta_{\infty}^T\bm B_{\infty})^2\mid (\bm \mu_{\pi}^T\bm B_{\infty})^2 \leq \xi_{\alpha,\blambda_{\infty}}\right]}{\bbE(\bbeta_{\infty}^T\bm B_{\infty})^2}
%     \leq \left(\frac{\cos\theta\cdot \sqrt{v_{\alpha,1}}\cdot a + \sin\theta \cdot \sqrt{\lambda_{\max}}}{\cos\theta \cdot a - \sin\theta \cdot \sqrt{\lambda_{\max}}}\right)^2.
% \end{split}\]
% Denote the RHS as $g(\theta)$, and $g(\theta) < v_{\alpha,p}$ is a sufficient condition for ReB to outperform ReM. 
% Notice that $g(0) = v_{\alpha,1} < v_{\alpha,p}$, $g(\theta_1-) = \infty > v_{\alpha,p}$
% and $g(\theta)$ is continuous and monotonically increasing about $\theta$ when $\theta < \theta_1$. Then there exists $\theta_2 \in (0,\theta_1)$ s.t. $g(\theta_2) = v_{\alpha,p}$ and $g(\theta) < v_{\alpha,p}$ for $\theta < \theta_2$. Let $\theta^* = \theta_2$, we have ReB outperforms ReM when $\theta < \theta^*$.

% For $\theta \in [\pi/2,\pi]$, $\cos\theta < 0$ and Cauchy-Schwarz inequality implies 
% \[\begin{split}
% h_2(\bbeta_{\infty},\phi_{\pi,\infty})
% &\leq \left\{\cos\theta\cdot \sqrt{v_{\alpha,1}\cdot \bmu_{\pi}^T \bV_{xx,\infty}\bmu_{\pi}} - \sin\theta \cdot \sqrt{\bbE\left[(\bm \epsilon_{\pi}^T\bm B_{\infty})^2\mid (\bm \mu_{\pi}^T\bm B_{\infty})^2 \leq \xi_{\alpha,\blambda_{\infty}}\right]}\right\}^2\\
% &= \left(-\cos\theta\cdot \sqrt{v_{\alpha,1}}\cdot a +\sin\theta \cdot c\right)^2
% \end{split}\]
% Since $c \leq \sqrt{\lambda_{\max}}$, we have 
% \[\begin{split}
% \bbE\left[(\bbeta_{\infty}^T\bm B_{\infty})^2\mid (\bm \mu_{\pi}^T\bm B_{\infty})^2 \leq \xi_{\alpha,\blambda_{\infty}}\right]
% &\leq \left(-\cos\theta\cdot \sqrt{v_{\alpha,1}}\cdot a +\sin\theta \cdot \sqrt{\lambda_{\max}}\right)^2.
% \end{split}\]
% Similarly, 
% \[\begin{split}
% \bbE(\bbeta_{\infty}^T\bm B_{\infty})^2
% &\geq (\cos\theta)^2\cdot \bbE(\bm \mu_{\pi}^T\bm B_{\infty})^2 + (\sin\theta)^2 \cdot \bbE(\bm \epsilon_{\pi}^T\bm B_{\infty})^2+ 2\sin\theta\cos\theta \cdot \sqrt{\bbE(\bm \mu_{\pi}^T\bm B_{\infty})^2\cdot \bbE(\bm \epsilon_{\pi}^T\bm B_{\infty})^2}\\
% &= \left\{\cos\theta\cdot \sqrt{\bmu_{\pi}^T \bV_{xx,\infty}\bmu_{\pi}} + \sin\theta \cdot \sqrt{\bm\epsilon_{\pi}^T \bV_{xx,\infty}\bm\epsilon_{\pi}}\right\}^2\\
% &= \left(\cos\theta\cdot a + \sin\theta \cdot b\right)^2.
% \end{split}\]
% Denote the last expression on the RHS as $f_2(\theta,b)$.
% When $\sqrt{\lambda_{\max}} < a/(-\tan\theta)$, which holds if $\theta > \theta_3 = \arctan\left(- a/\sqrt{\lambda_{\max}}\right) \in (\pi/2,\pi)$, $f_2(\theta,b)$ is monotonically decreasing about $b$. Therefore, for $\theta > \theta_3$, 
% we have 
% \[\begin{split}
%     \bbE(\bbeta_{\infty}^T\bm B_{\infty})^2
% &\geq \left(\cos\theta\cdot a + \sin\theta \cdot \sqrt{\lambda_{\max}}\right)^2
% \end{split}\]
% and
% \[\begin{split}
%     v_{\alpha,\pi} = \frac{\bbE\left[(\bbeta_{\infty}^T\bm B_{\infty})^2\mid (\bm \mu_{\pi}^T\bm B_{\infty})^2 \leq \xi_{\alpha,\blambda_{\infty}}\right]}{\bbE(\bbeta_{\infty}^T\bm B_{\infty})^2}
%     \leq \left(\frac{\cos\theta\cdot \sqrt{v_{\alpha,1}}\cdot a - \sin\theta \cdot \sqrt{\lambda_{\max}}}{\cos\theta \cdot a + \sin\theta \cdot \sqrt{\lambda_{\max}}}\right)^2.
% \end{split}\]
% Denote the RHS as $g_2(\theta)$, and $g_2(\theta) < v_{\alpha,p}$ is a sufficient condition for ReB to outperform ReM. 
% Notice that $g_2(\theta_3+) = \infty> v_{\alpha,p}$, $g_2(\pi) = v_{\alpha,1} < v_{\alpha,p}$
% and $g_2(\theta)$ is continuous and monotonically decreasing about $\theta$ when $\theta > \theta_3$. Then there exists $\theta_4 \in (\theta_3,\pi)$ s.t. $g_2(\theta_4) = v_{\alpha,p}$ and $g_2(\theta) < v_{\alpha,p}$ for $\theta > \theta_4$. Let $\theta^* = \theta_4$, we have ReB outperforms ReM when $\theta > \theta^*$.

% Notice that $\theta_3 = \pi - \theta_1$ and $\theta_4 = \pi - \theta_2$, let $\theta^* = \theta_2$, then 
% $\pi - \theta^* = \theta_4$. Therefore, for $\theta \in [0,\pi]$, ReB outperforms ReM when $\min\{\theta,\pi-\theta\} < \theta^*$.


% \subsection{Proof of Corollary \ref{cor:I_p}}
% Since $\bV_{xx,\infty} = \sigma^2\cdot \bm I_p$,
% the joint distribution of $\bmu_{\pi}^T\bm B_{\infty}$ and $\bm\epsilon_{\pi}^T\bm B_{\infty}$ is
% \[\begin{split}
%     \left(\begin{array}{c}
%         \bmu_{\pi}^T\bm B_{\infty} \\
%         \bm\epsilon_{\pi}^T\bm B_{\infty}
%     \end{array}\right) &\sim 
%     N\left(\left(\begin{array}{c}
%         0 \\
%         0 
%     \end{array}\right),
%     \left(\begin{array}{cc}
%         \bmu_{\pi}^T \bV_{xx,\infty} \bmu_{\pi} &
%         \bmu_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi}\\
%         \bmu_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi} &
%         \bm\epsilon_{\pi}^T \bV_{xx,\infty} \bm\epsilon_{\pi}
%     \end{array}\right)\right) = 
%     N\left(\left(\begin{array}{c}
%         0 \\
%         0 
%     \end{array}\right),
%     \left(\begin{array}{cc}
%         \sigma^2 &
%         0\\
%         0 &
%         \sigma^2
%     \end{array}\right)\right),
% \end{split}\]
% i.e., $\bmu_{\pi}^T\bm B_{\infty}$ and $\bm\epsilon_{\pi}^T\bm B_{\infty}$ are independent.
% Hence,
% \[\begin{split}
% \bbE\left[(\bbeta_{\infty}^T\bm B_{\infty})^2\mid (\bm \mu_{\pi}^T\bm B_{\infty})^2 \leq \xi_{\alpha,\blambda_{\infty}}\right]
% &= (\cos\theta)^2\cdot \bbE\left[(\bm \mu_{\pi}^T\bm B_{\infty})^2\mid (\bm \mu_{\pi}^T\bm B_{\infty})^2 \leq \xi_{\alpha,\blambda_{\infty}}\right]\\& + (\sin\theta)^2 \cdot \bbE\left[(\bm \epsilon_{\pi}^T\bm B_{\infty})^2\mid (\bm \mu_{\pi}^T\bm B_{\infty})^2 \leq \xi_{\alpha,\blambda_{\infty}}\right] \\&+ 2\sin\theta\cos\theta \cdot \bbE\left[(\bm \mu_{\pi}^T\bm B_{\infty})(\bm \epsilon_{\pi}^T\bm B_{\infty})\mid (\bm \mu_{\pi}^T\bm B_{\infty})^2 \leq \xi_{\alpha,\blambda_{\infty}}\right]\\
% &= \sigma^2 \left[(\cos\theta)^2\cdot v_{\alpha,1} + (\sin\theta)^2\right],
% \end{split}\]
% and 
% \[\begin{split}
% \bbE(\bbeta_{\infty}^T\bm B_{\infty})^2
% &= \bbeta_{\infty}^T \bV_{xx,\infty}\bbeta_{\infty} = \sigma^2.
% \end{split}\]
% By definition, 
% \[\begin{split}
% v_{\alpha,\pi} &= \frac{\bbE\left[(\bbeta_{\infty}^T\bm B_{\infty})^2\mid (\bm \mu_{\pi}^T\bm B_{\infty})^2 \leq \xi_{\alpha,\blambda_{\infty}}\right]}{\bbE(\bbeta_{\infty}^T\bm B_{\infty})^2}\\
% &= (\cos\theta)^2\cdot v_{\alpha,1} + (\sin\theta)^2 = 1-(1-v_{\alpha,1}) (\cos\theta)^2.
% \end{split}\]
% \QEDA

\subsection{Proof of Theorem \ref{2stageReO-variance}}
Take BCRD-ReB as an example and we suppose its acceptance probability in second stage be $\alpha_1$. 
% We denote the the asymptotic sampling variance of $\hat{\tau}$ under complete randomization as $\bbV_a(\hat{\tau})$ and the asymptotic conditional variance of $\Tilde{\tau}_{N_1,N_2}$ under $\phi_{BCRD-ReB}$ as $\bbV_{BCRD-ReB,a}\left(\Tilde{\tau}_{N_1,N_2}\right)$.
According to Eq.~\eqref{eq:asymptotic-variance-2stage},
\[\begin{split}
\bbV\left(\Tilde{\tau}_{N_1,N_2}\mid \phi_{BCRD\text{-}ReB}=1\right) &= (\hat{w}^{*})^2\cdot \bbV\left(\hat{\tau}_1\right)+(1-\hat{w}^*)^{2}
\cdot \bbV\left(\hat{\tau}_2\mid \phi_{\pi_{N_1}=1}\right).
\end{split}\]
Hence, for $N=N_1+N_2$,
\begin{equation}\label{eq:asympvariancetotal}
\begin{split}
    &\indent \lim_{N_1,N_2 \rightarrow \infty}\bbV\left(\sqrt{N}(\Tilde{\tau}_{N_1,N_2}-\tau)\mid \phi_{BCRD\text{-}ReB}=1\right)\\ &=
\lim_{N_1,N_2 \rightarrow \infty}\left[
\frac{(\hat{w}^{*})^2}{N_1/N}
\cdot \bbV\left(\sqrt{N_1}(\hat{\tau}_1-\tau)\right)+\frac{(1-\hat{w}^*)^{2}}{N_2/N}
\cdot \bbV\left(\sqrt{N_2}(\hat{\tau}_2-\tau)\mid \phi_{\pi_{N_1}=1}\right)\right].
\end{split}
\end{equation}
Denote $\hat{\tau}^{(2)}$ as the treatment effect estimator when we conduct complete randomization in the second stage. For fixed $N_1$, as $N_2 \rightarrow \infty$, we have
\[\begin{split}
    \lim_{N_2\rightarrow \infty}\bbV\left(\sqrt{N_2}(\hat{\tau}_2-\tau)\mid \phi_{\pi_{N_1}=1}\right) &= \left[1-(1-v_{\alpha_1,\pi_{N_1}})R^2_{\infty}\right]\cdot\lim_{N_2\rightarrow \infty} \bbV\left(\sqrt{N_2}(\hat{\tau}^{(2)}-\tau)\right)\\&=\left[1-(1-v_{\alpha_1,\pi_{N_1}})R^2_{\infty}\right]\cdot\lim_{N\rightarrow \infty} \bbV\left(\sqrt{N}(\hat{\tau}-\tau)\right).
\end{split}\]
Hence, 
\begin{equation}\label{eq:asympvariance1}
\lim_{N_1,N_2\rightarrow \infty}\bbV\left(\sqrt{N_2}(\hat{\tau}_2-\tau)\mid \phi_{\pi_{N_1}=1}\right) = \lim_{N_1,N_2\rightarrow \infty}\left[1-(1-v_{\alpha_1,\pi_{N_1}})R^2_{\infty}\right]\cdot \bbV\left(\sqrt{N}(\hat{\tau}-\tau)\right).
\end{equation}
% Also Note that 
% \begin{equation}\label{eq:asympvariance2}
%     \lim_{N_1 \rightarrow \infty}\bbV\left(\sqrt{N_1}(\hat{\tau}_1-\tau)\right) =\lim_{N \rightarrow \infty} \bbV\left(\sqrt{N}(\hat{\tau}-\tau)\right),
% \end{equation}
For $N_1/N \rightarrow \varrho$, suppose $\hat{w}^*$ has the same limit as $w^*$, we have
\begin{equation}\label{eq:asympw}
    \lim_{N_1,N_2\rightarrow\infty,N_1/N\rightarrow\varrho}\hat{w}^* = \lim_{N_1,N_2\rightarrow\infty,N_1/N\rightarrow\varrho}w^* = \lim_{N_1\rightarrow\infty}\frac{\varrho}{\varrho+(1-\varrho)/\left[1-(1-v_{\alpha,\pi_{N_1}})R^2_{\infty}\right]}.
\end{equation}
Therefore, plug Eq.~\eqref{eq:asympvariance1} , \eqref{eq:asympw} in Eq.~\eqref{eq:asympvariancetotal} and we have
\[\begin{split}
&\indent 
\lim_{N_1,N_2 \rightarrow \infty,N_1/N \rightarrow \varrho}\frac{\bbV\left(\sqrt{N}(\Tilde{\tau}_{N_1,N_2}-\tau)\mid \phi_{BCRD\text{-}ReB}=1\right)}{ \bbV\left(\sqrt{N}(\hat{\tau}-\tau)\right)}
\\ &=
\lim_{N_1,N_2 \rightarrow \infty, N_1/N \rightarrow \varrho}\left\{
\frac{(\hat{w}^{*})^2}{N_1/N}
+\frac{(1-\hat{w}^*)^{2}}{N_2/N}
\cdot \left[1-(1-v_{\alpha_1,\pi_{N_1}})R^2_{\infty}\right]\right\}\\&= \lim_{N_1 \rightarrow \infty}\frac{1}{\varrho + \frac{1-\varrho}{1-(1-v_{\alpha_1,\pi_{N_1}})R^2_{\infty}}},
\end{split}\]


% and thus, 
% \[\begin{split}
%     PRIASV_{BCRD-ReB} &= \lim_{N_1 \rightarrow \infty} 100\times \left(1-
%   \frac{1}{\rho + \frac{1-\rho}{1-(1-v_{\alpha,\pi_{N_1}})R^2_{\infty}}}\right).
% \end{split}\]
It suffices to show that $v_{\alpha_1,\pi_{N_1}} \rightarrow v_{\alpha_1,1}$ as $N_1 \rightarrow \infty$. For fixed $N_1$, as $N_2 \rightarrow \infty$,
$\Tilde{\bLambda}_{\pi_{N_1}} = \bLambda_{\pi_{N_1}}$. Under Condition \ref{cond1}, equation \eqref{eq:2stageReO} implies the acceptance region of the second stage is 
\begin{equation}
    \mathcal{B}_{\phi_{\pi_{N_1}}} = \left\{\sqrt{N_2}\bD^{(2)} :N_2\cdot \bD^{(2)T}\bLambda_{\pi_{N_1}}\bD^{(2)}\leq  \xi_{\alpha_1,\blambda_{\pi_{N_1}}}\right\},
\end{equation}
and its corresponding asymptotic acceptance region as $N_2\rightarrow\infty$ is 
\begin{equation}
    \mathcal{B}_{\phi_{\pi_{N_1}},\infty} = \left\{\sqrt{N_2}\bD^{(2)} :N_2\cdot \bD^{(2)T}\Tilde{\bLambda}_{\pi_{N_1}}\bD^{(2)}\leq  \xi_{\alpha_1,\blambda_{\pi_{N_1},\infty}}\right\},
\end{equation}
where 
$\blambda_{\pi_{N_1}}$ is the vector of eigenvalues of $\bm P_{N_1,N_2}=\left(\big(\bV_{xx,N_2}^{(2)}\big)^{1/2}\right)^T {\bLambda}_{\pi_{N_1}} \big(\bV_{xx,N_2}^{(2)}\big)^{1/2}$, and 
$\blambda_{\pi_{N_1},\infty}$ is the vector of eigenvalues of 
$\bm P_{N_1,\infty} = \big(\bm V_{xx,\infty}^{1/2}\big)^T \Tilde{\bLambda}_{\pi_{N_1}} \bm V_{xx,\infty}^{1/2}$.
Under Condition \ref{cond1}, as $N_1 \rightarrow \infty$, we have $\Tilde{\bLambda}_{\pi_{N_1}} = \bLambda_{\pi_{N_1}} \rightarrow \bbeta_{\infty}\bbeta_{\infty}^T$ and 
\[\begin{split}
    \bm P_{N_1,\infty}\rightarrow \big(\bm V_{xx,\infty}^{1/2}\big)^T \bbeta_{\infty}\bbeta_{\infty}^T\bm V_{xx,\infty}^{1/2} = \bm P_{\infty}.
\end{split}\]
Since $\bm P_{\infty}$ is a matrix of rank $1$, its eigenvalues are 
\[\begin{split}
    \blambda_{\infty} = \big(\bbeta_{\infty}^T\bm V_{xx,\infty}\bbeta_{\infty},0,\ldots,0\big)^T = \lim_{N_1 \rightarrow \infty} \blambda_{\pi_{N_1},\infty}. 
\end{split}\]
Hence, as $N_1 \rightarrow \infty$,
\[\begin{split}
    v_{\alpha_1,\pi_{N_1}} &= \frac{\bbE\big(\bm B^T_{\infty}\bbeta_{\infty}\bbeta_{\infty}^T\bm B_{\infty}\mid\bm B_{\infty}^T \bLambda_{\pi_{N_1}}\bm B_{\infty} \leq \xi_{\alpha_1,\blambda_{\pi_{N_1},\infty}} \big)}{\bbE\left(\bm B^T_{\infty}\bbeta_{\infty}\bbeta_{\infty}^T\bm B_{\infty}\right)}\\
    &\rightarrow \frac{\bbE\left(\bm B^T_{\infty}\bbeta_{\infty}\bbeta_{\infty}^T\bm B_{\infty}\mid\bm B_{\infty}^T \bbeta_{\infty}\bbeta_{\infty}^T\bm B_{\infty} \leq \xi_{\alpha_1,\blambda_{\infty}} \right)}{\bbE\left(\bm B^T_{\infty}\bbeta_{\infty}\bbeta_{\infty}^T\bm B_{\infty}\right)} = 
    v_{\alpha_1,1}.
\end{split}\]
Therefore, as $N_1,N_2 \rightarrow \infty$, we have
\[\begin{split}
    \text{PRIASV}_{BCRD-ReB} &= 100\times \left(1-\lim_{N_1 \rightarrow \infty}\frac{1}{\varrho + \frac{1-\varrho}{1-(1-v_{\alpha_1,\pi_{N_1}})R^2_{\infty}}}\right)
  \\ &
  = 100\times \left(1-\frac{1}{\varrho + \frac{1-\varrho}{1-(1-v_{\alpha_1,1})R^2_{\infty}}}\right).
\end{split}\]
PRIASV of ReM-ReB can be similarly derived.
\QEDA

\subsection{The procedure of estimating the optimal weight in two-stage ReB}\label{sec:estimateweight}
Denote the finite sample estimators of $\bbeta_{\infty}$, $R^2_{\infty}$ and $V_{xx,\infty}$ as $\hat\bbeta_{\infty}$, $\hat{R}^2_{\infty}$ and $\hat{\bm V}_{xx,\infty}$. Let $\hat\bbeta_{\infty} = \hat{\bbeta}_{N}$, which is the least square estimator of $\bbeta_{\infty}$ in parallel regression between the responses and
covariates of the two arms using all $N$ units, $\hat{R}^2_{\infty} = R^2_{N}$ be the corresponding squared correlation, 
and 
$\hat{\bm V}_{xx,\infty}=\bV_{xx,N}$
be the covariance matrix of $\sqrt{N}\bD^T$, where $\bD$ is the difference-in-means of covariates in the two arms of the full experiment. Then we replace  $v_{\alpha,\pi_{N_1}}$ with 
\begin{equation}\label{eq:estimatedv}
    \hat{v}_{\alpha,\pi_{N_1}} = \frac{\hat{\bbeta}_{N}^T \left(\bm M_{N_1}^{-1}\right)^T\bm\Gamma^T_{N_1}
\bm C_{N_1}\bm\Gamma_{N_1} \bm M_{N_1}^{-1}\hat{\bbeta}_{N}}{\hat{\bbeta}_{N}^T \bV_{xx,N}\hat{\bbeta}_{N}},
\end{equation}
where $\Tilde{\bLambda}_{\pi_{N_1}} = \bLambda_{\pi_{N_1}} = \bm M_{N_1} \bm M_{N_1}^T$, $\bm\Gamma_{N_1}$ is an orthogonal matrix such that $\bm M_{N_1}^T \bV_{xx,N} \bm M_{N_1} =\bm\Gamma^T_{N_1} diag\{\lambda_{1},\ldots,\lambda_{p}\} \bm\Gamma_{N_1}$, $\blambda = (\lambda_{1},...,\lambda_{p})^T$ is the vector of eigenvalues of
\begin{equation}\label{eq:pn1n1}
    \bm P_{N_1,N}=\big(\bV_{xx,N}^{1/2}\big)^T\bLambda_{\pi_{N_1}}\bV_{xx,N}^{1/2},
\end{equation}
and
$\bm C_{N_1} = diag\{c_1,\ldots,c_p\}$ with 
$c_i = \bbE\big(\lambda_{i}Z_i^2\mid \sum_{j=1}^p \lambda_{j} Z_j^2 \leq \xi_{\alpha,\blambda}\big)$ for $i=1,\ldots,p$ and $Z_1,...,Z_p$ are i.i.d. standard normal distributed random variables. Plug these estimators in Eq.~\eqref{eq:w1} or \eqref{eq:w2} and we get the estimated weight and denote it as $\hat{w}^*$. For example, The estimated weight in BCRD-ReB is
$$\hat{w}^* = \frac{N_1}{N_1 +N_2/\left[1-(1-\hat{v}_{\alpha_1,\pi_{N_1}})R^2_{N}\right]},$$
where $\hat{v}_{\alpha_1,\pi_{N_1}}$ is defined as in Eq.~\eqref{eq:estimatedv}. As $N_1,N_2\rightarrow \infty$ under Condition \ref{cond1}, we have $\bV_{xx,N} \rightarrow \bm \bV_{xx,\infty}$, $\hat{\bbeta}_{N} \rightarrow \bbeta_{\infty}$ and $R^2_{N} \rightarrow R^2_{\infty}$. Thus, as $N_1,N_2 \rightarrow \infty$, we have $\lim_{N_1,N_2 \rightarrow \infty}\hat{v}_{\alpha_1,\pi_{N_1}} = \lim_{N_1,N_2 \rightarrow \infty} v_{\alpha_1,\pi_{N_1}}$ and   
\begin{equation}\label{eq:asymptoticw}
    \lim_{N_1,N_2 \rightarrow \infty} \hat{w}^* = \lim_{N_1,N_2 \rightarrow \infty} \frac{N_1}{N_1 +N_2/\left[1-(1-v_{\alpha_1,\pi_{N_1}})R^2_{\infty}\right]} = \lim_{N_1,N_2 \rightarrow \infty} w^*.
\end{equation}
\QEDA

% \subsection{Proof of Lemma \ref{lemma5}}
% Suppose sample size in two stages are $N_1$ and $N_2$, respectively, such that $N=N_1+N_2$. For simplicity, we assume that $N_1$ and $N_2$ are large enough with respect to dimension of covariates $p$, and thus, only costs related to $N_1$ and $N_2$ will be taken into account.
% Note that the calculation cost of $\bm V_{xx,N}$ and $\hat{\tau}$, which is the same as that of computing($\hat{\tau}_1$,$\hat{\tau}_2$), is necessary for all methods, we can simply omit them when comparing the computational cost of these methods. 
% % Hereinafter, we refer to the computational cost as the cost excluding calculating $\bm V_{xx,N}$ and causal effect estimators.

% First, we derive the computational cost of ReM. Suppose the acceptance probability is $\alpha$. After calculation of $\bm V_{xx,N}$, the following procedure is repeated for about $1/\alpha$ times:
% \begin{enumerate}
% \item Randomly sample $N/2$ units and assign them to treatment group. Assign the last $N/2$ units to the control group.
% \item Calculate Mahalanobis distance $d_M = N\cdot \bm D^T \bm V_{xx,N}^{-1} \bm D$, where $\bm D = \bar{\bm X}_t - \bar{\bm X}_c$.
% \end{enumerate}
% The above operations take about $(N/2+pN)/\alpha$
% operations.
% Therefore, the computational cost of ReM is
% approximately $$\frac{(2p+1)N}{2\alpha},$$which holds for ReO and ReB as well.

% Then we take ReM-ReB as an example to show the computational cost of two-stage ReB procedures. Suppose units are arranged in the order they enter the experiment, i.e., $\bm Y^{(1)} = (Y_1,\ldots,Y_{N_1})$ are responses of units in first stage.
% Suppose the acceptance probability in both stages are $\alpha_2$. We follow the procedure below to implement ReM-ReB:
% \begin{enumerate}
% \item Apply ReM to the $N_1$ units in first stage and it takes $(2p+1)N_1/(2\alpha_2)$ operations. $\hat{\tau}_1$ is obtained. 
% \item Collect responses and obtain the least square estimator $\hat{\bbeta}_{N_1}$: Augment $\bm X^{(1)}$ with $\bm 1_{N_1}$ as the first column and we get $\Tilde{\bm X}^{(1)} \in \mathcal{R}^{N_1 \times (p+1)}$. We approximate $\bm Y^{(1)}(0)$, which represents the potential outcomes of units in first stage when assigned to control group, with $\Tilde{\bm Y}^{(1)}$, whose $i$-th element is $\Tilde{Y}^{(1)}_i = Y^{(1)}_i-W_i\cdot\hat{\tau}_1$, and this takes $N_1$ operations. Then $$\Tilde{\bbeta}_{N_1} = (\hat{\bbeta}_0,\hat{\bbeta}_{N_1})^T = \left(\Tilde{\bm X}^{(1)T}\Tilde{\bm X}^{(1)}\right)^{-1}\Tilde{\bm X}^{(1)T}\Tilde{\bm Y}^{(1)}.$$
% As $\Tilde{\bm X}^{(1)T}\Tilde{\bm X}^{(1)}$ is symmetric, its computational cost is $2N_1[(p+1)+p+\cdots+1]=(p^2+3p+2)N_1$. Since the calculation of $\Tilde{\bm X}^{(1)T}\Tilde{\bm Y}^{(1)}$ takes $2(p+1)N_1$ operations, cost of this step is about $(p^2+5p+5)N_1$.
% \item Calculate the estimated variance of $\hat{\bbeta}_{N_1}$: $\bm\Sigma_{\pi_{N_1}}$, which is the bottom right $p\times p$ block matrix of $\text{MSE} \times \left(\Tilde{\bm X}^{(1)T}\Tilde{\bm X}^{(1)}\right)^{-1}$, where $\text{MSE} = \frac{1}{N_1-1}\sum_{i=1}^{N_1} \left(\Tilde{Y}_i^{(1)}-\hat{Y}_i^{(1)}\right)^2$ and $\hat{\bm Y}^{(1)} = \Tilde{\bm X}^{(1)}\Tilde{\bbeta}_{N_1}$. Since the fitted value $\hat{\bm Y}^{(1)}$ costs $(2p+1)N_1$ and MSE takes $2N_1$ operations, computational cost of this step is about $(2p+3)N_1$.
% \item Calculate the characteristic matrix of the prior: $\bLambda_{\pi_{N_1}} = \hat{\bbeta}_{N_1}\hat{\bbeta}_{N_1}^T + \bm\Sigma_{\pi_{N_1}}$. This step only depends on $p$ and we neglect its cost.
% \item Calculate $R^2_{N_1} = 1-\text{SSE}/\text{SST}$, where $\text{SSE}=\sum_{i=1}^{N_1} \left(\Tilde{Y}_i^{(1)}-\hat{Y}_i^{(1)}\right)^2$ has been obtained when calculating MSE, and only $\text{SST} = \sum_{i=1}^{N_1} \left(\Tilde{Y}_i^{(1)}-\bar{\Tilde{Y}}^{(1)}\right)^2$ is needed, whose cost is $2N_1$, as cost of $\bar{\Tilde{Y}}^{(1)}$ is covered in that of calculating $\hat{\tau}_1$. 
% \item Estimate weight
% $$\hat{w}^* = \frac{N_1}{N_1/\left[1-(1-v_{\alpha,p})R^2_{N_1})\right]+N_2/\left[1-(1-\hat{v}_{\alpha_1,\pi_{N_1}})R^2_{N_1}\right]},$$
% with $\hat{\bbeta}_{N_1}$, $\bV_{xx,N}$ and $R^2_{N_1}$. In practical implementation, $\hat{v}_{\alpha_1,\pi_{N_1}}$ can be calculated according to its definition, i.e., Eq.~\eqref{R2_pi}, with Monte Carlo approximation. Cost of this step is also unrelated to $N_1$ and $N_2$ and is neglected.
% \item Conduct ReB on the remaining $N_2$ units, which takes $(2p+1)N_2/(2\alpha_2)$ operations.
% \item Calculate $\hat{\tau_2}$ and integrate it with $\hat{\tau_1}$  using $\hat{w}^*$.
% \end{enumerate}
% Overall, the computational cost of the above operations is about 
% \[\begin{split}
%     \frac{(2p+1)N}{2\alpha_2} + (p^2+7p+10)N_1.
% \end{split}\]
% Similarly, the computational cost of BCRD-ReB with acceptance probability $\alpha_1$ is 
% \[\begin{split}
%     \frac{N_1}{2} + \frac{(2p+1)N_2}{2\alpha_1} + (p^2+7p+10)N_1.
% \end{split}\]
% If we use $\bLambda_{\pi_{N_1}} = \hat{\bbeta}_{N_1}\hat{\bbeta}_{N_1}^T$ as prior in second stage, which leads to BCRD-ReO and ReM-ReO, the computational costs are the same as that of BCRD-ReB and ReM-ReB, respectively.
% Therefore, to control all these methods' computational cost asymptotically, their acceptance probabilities must satisfies the following equations:
% \[\begin{split}
% \frac{(2p+1)N}{2\alpha} &= 
% \frac{N_1}{2}+\frac{(2p+1)N_2}{2\alpha_1}+(p^2+7p+10)N_1\\
% &= \frac{(2p+1)N}{2\alpha_2}
% +(p^2+7p+10)N_1.
% \end{split}\]
% Given $\alpha$, the value of $\alpha_1,\alpha_2$ can be calculated.
% \QEDA


\newpage
\section{Additional simulation results}\label{sec:simulations}
\subsection{Validation of PRIV and acceptance probability}\label{sec:validation}

Figure \ref{fig:PRIVvsPRIASV} plots the ratio of $\{$PRIV$_{ij}\}$ and their corresponding theoretical PRIASV of ReO, ReM and ReB for different $\sigma_{\bbeta}^2$. As ReB degenerates to ReO when $\sigma_{\bbeta}^2=0$, we only show the result for $\sigma_{\bbeta}^2=0.01$ and $1$. It implies that PRIV of re-randomization schemes will get closer to the theoretical value PRIASV as sample size $N$ gets larger. Also, the precision of the realized PRIV of ReO and ReB is better than that of ReM. Moreover, to verify the realized acceptance probability of the three re-randomization schemes, average times of re-randomizing for each method in each setting is calculated. Figure \ref{fig:Acceptance_rate} shows the reciprocal of the average times, which is equivalent to acceptance rate for each method. We can see that as sample size increases, the acceptance probability will get closer to 0.05, which is exactly what we expect.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.8]{PRIVvsPRIASV.png}
    \caption{Boxplots of $PRIV_{ij}/PRIASV_{ij}$ for ReO, ReB and ReM when (A) $\sigma_{\bbeta}^2=0.01$ and (B) $\sigma_{\bbeta}^2=1$.}
    \label{fig:PRIVvsPRIASV}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.8]{Acceptance_rate.png}
    \caption{The realized acceptance probability of ReO, ReB and ReM when (A) $\sigma_{\bbeta}^2=0.01$ and (B) $\sigma_{\bbeta}^2=1$.}
    \label{fig:Acceptance_rate}
\end{figure}


\subsection{Tuned acceptance rates for different re-randomization procedures and their corresponding running time under linear settings}\label{sec:linear_time}

Table \ref{tab:ARlinear} and Table \ref{tab:RTlinear} show the numerically tuned acceptance rates of different re-randomization procedures and their corresponding average running time of sampling 500 assignments under linear settings. For the additional experiment that studies the performance of two-stage ReB under different proportion of sample in first stage with total sample size $N$ fixed at 600, Figure \ref{fig:r_time} shows the running time ratio of different re-randomization procedures against ReM when sampling 500 assignments. 
We can see that the computational cost of these methods are comparative under their acceptance rates and thus, the comparisons of PRIV are fair. 

\begin{table}[ht]
    \centering
    \footnotesize
% \caption{Running time (seconds) of sampling 500 assignments (and the corresponding acceptance rates) of different re-randomization procedures under linear settings.}
\caption{Acceptance rates of different re-randomization procedures under linear settings.}
\label{tab:ARlinear}
    \setlength{\tabcolsep}{2.5mm}{
% \normalsize
\begin{tabular}{cccccccccc}
    \midrule
    & \multicolumn{3}{c}{$N=200$}&\multicolumn{3}{c}{$N=600$}&\multicolumn{3}{c}{$N=1200$}\\
    \cmidrule(lr){2-4}
    \cmidrule(lr){5-7}
    \cmidrule(lr){8-10}
    Scheme & $r=0.1$ & 0.2 & 0.4 & 0.1 & 0.2 & 0.4 & 0.1 & 0.2 & 0.4 \\\hline
    \multicolumn{10}{c}{}\vspace{-0.2cm}\\
    & \multicolumn{9}{c}{$p=2$}\\\cmidrule(lr){2-10}
    ReO & 0.045 & 0.046 & 0.045 & 0.046 & 0.046 & 0.047 & 0.047 & 0.048 & 0.047 \\ 
    ReM-ReB & 0.086 & 0.085 & 0.083 & 0.070 & 0.068 & 0.074 & 0.063 & 0.063 & 0.064 \\  
    ReM-ReO & 0.086 & 0.085 & 0.084 & 0.070 & 0.069 & 0.073 & 0.063 & 0.062 & 0.063 \\ 
    BCRD-ReB & 0.049 & 0.045 & 0.041 & 0.046 & 0.040 & 0.038 & 0.045 & 0.043 & 0.035 \\ 
    BCRD-ReO & 0.049 & 0.045 & 0.041 & 0.045 & 0.041 & 0.038 & 0.047 & 0.043 & 0.035 \\ 
    ReM & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 \\ 
    \cmidrule(lr){2-10}
    \multicolumn{10}{c}{}\vspace{-0.3cm}\\
    & \multicolumn{9}{c}{$p=5$}\\\cmidrule(lr){2-10}
    ReO & 0.044 & 0.043 & 0.044 & 0.046 & 0.046 & 0.046 & 0.046 & 0.047 & 0.047 \\ 
    ReM-ReB & 0.094 & 0.094 & 0.084 & 0.073 & 0.070 & 0.074 & 0.063 & 0.065 & 0.064 \\ 
    ReM-ReO & 0.094 & 0.086 & 0.085 & 0.073 & 0.070 & 0.074 & 0.062 & 0.066 & 0.064 \\ 
    BCRD-ReB & 0.046 & 0.045 & 0.042 & 0.047 & 0.044 & 0.037 & 0.046 & 0.042 & 0.035 \\
    BCRD-ReO & 0.045 & 0.043 & 0.042 & 0.046 & 0.043 & 0.037 & 0.045 & 0.041 & 0.035 \\ 
    ReM & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 \\ 
    \cmidrule(lr){2-10}
    \multicolumn{10}{c}{}\vspace{-0.3cm}\\
    & \multicolumn{9}{c}{$p=10$}\\\cmidrule(lr){2-10}
    ReO & 0.043 & 0.043 & 0.041 & 0.046 & 0.045 & 0.045 & 0.047 & 0.049 & 0.047 \\ 
    ReM-ReB & 0.118 & 0.094 & 0.083 & 0.072 & 0.070 & 0.068 & 0.062 & 0.063 & 0.064 \\ 
    ReM-ReO & 0.118 & 0.092 & 0.082 & 0.072 & 0.068 & 0.067 & 0.062 & 0.063 & 0.064 \\ 
    BCRD-ReB & 0.044 & 0.044 & 0.041 & 0.046 & 0.041 & 0.035 & 0.046 & 0.041 & 0.037 \\ 
    BCRD-ReO & 0.044 & 0.042 & 0.038 & 0.046 & 0.042 & 0.035 & 0.044 & 0.041 & 0.036 \\ 
    ReM & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 \\ 
    \cmidrule(lr){2-10}
    \multicolumn{10}{c}{}\vspace{-0.3cm}\\
    & \multicolumn{9}{c}{$p=20$}\\\cmidrule(lr){2-10}
    ReO & 0.039 & 0.037 & 0.038 & 0.045 & 0.045 & 0.045 & 0.046 & 0.046 & 0.045 \\ 
    ReM-ReB & $--$ & 0.108 & 0.084 & 0.079 & 0.071 & 0.068 & 0.062 & 0.059 & 0.060 \\ 
    ReM-ReO & $--$ & 0.106 & 0.084 & 0.078 & 0.070 & 0.070 & 0.061 & 0.060 & 0.059 \\ 
    BCRD-ReB & $--$ & 0.041 & 0.038 & 0.046 & 0.041 & 0.037 & 0.045 & 0.039 & 0.033 \\ 
    BCRD-ReO & $--$ & 0.037 & 0.035 & 0.044 & 0.041 & 0.034 & 0.044 & 0.039 & 0.033 \\ 
    ReM & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 \\ 
    \midrule
    \end{tabular}
}
\end{table}

\begin{table}[ht]
    \centering
    \footnotesize
\caption{Average running time (seconds) of sampling 500 assignments using different re-randomization procedures under linear settings with tuned acceptance rates.}
\label{tab:RTlinear}
    \setlength{\tabcolsep}{2.5mm}{
% \normalsize
\begin{tabular}{cccccccccc}
    \midrule
    & \multicolumn{3}{c}{$N=200$}&\multicolumn{3}{c}{$N=600$}&\multicolumn{3}{c}{$N=1200$}\\
    \cmidrule(lr){2-4}
    \cmidrule(lr){5-7}
    \cmidrule(lr){8-10}
    Scheme & $r=0.1$ & 0.2 & 0.4 & 0.1 & 0.2 & 0.4 & 0.1 & 0.2 & 0.4 \\\hline
    \multicolumn{10}{c}{}\vspace{-0.2cm}\\
    & \multicolumn{9}{c}{$p=2$}\\\cmidrule(lr){2-10}
    ReO & 0.458 & 0.451 & 0.462 & 0.750 & 0.746 & 0.739 & 1.155 & 1.130 & 1.140 \\ 
    ReM-ReB & 0.452 & 0.454 & 0.479 & 0.762 & 0.758 & 0.718 & 1.149 & 1.159 & 1.131 \\
    ReM-ReO &  0.456 & 0.456 & 0.470 & 0.757 & 0.758 & 0.728 & 1.151 & 1.149 & 1.146 \\ 
    BCRD-ReB & 0.442 & 0.452 & 0.472 & 0.747 & 0.756 & 0.734 & 1.132 & 1.134 & 1.160 \\
    BCRD-ReO & 0.448 & 0.453 & 0.471 & 0.754 & 0.748 & 0.740 & 1.108 & 1.125 & 1.156 \\
    ReM & 0.453 & 0.451 & 0.455 & 0.734 & 0.727 & 0.731 & 1.137 & 1.130 & 1.119 \\
    \cmidrule(lr){2-10}
    \multicolumn{10}{c}{}\vspace{-0.3cm}\\
    & \multicolumn{9}{c}{$p=5$}\\\cmidrule(lr){2-10}
    ReO & 0.506 & 0.522 & 0.506 & 0.821 & 0.811 & 0.794 & 1.269 & 1.259 & 1.243 \\
    ReM-ReB & 0.501 & 0.475 & 0.508 & 0.815 & 0.823 & 0.767 & 1.256 & 1.230 & 1.249 \\
    ReM-ReO & 0.495 & 0.506 & 0.496 & 0.801 & 0.817 & 0.765 & 1.263 & 1.230 & 1.261 \\
    BCRD-ReB & 0.505 & 0.522 & 0.504 & 0.808 & 0.819 & 0.793 & 1.248 & 1.251 & 1.233 \\
    BCRD-ReO & 0.504 & 0.520 & 0.499 & 0.813 & 0.815 & 0.791 & 1.254 & 1.253 & 1.241 \\
    ReM & 0.498 & 0.502 & 0.498 & 0.815 & 0.810 & 0.787 & 1.234 & 1.253 & 1.233 \\ 
    \cmidrule(lr){2-10}
    \multicolumn{10}{c}{}\vspace{-0.3cm}\\
    & \multicolumn{9}{c}{$p=10$}\\\cmidrule(lr){2-10}
    ReO & 0.547 & 0.557 & 0.582 & 0.894 & 0.954 & 0.962 & 1.413 & 1.370 & 1.424 \\ 
    ReM-ReB & 0.552 & 0.553 & 0.581 & 0.904 & 0.974 & 0.968 & 1.414 & 1.430 & 1.419 \\ 
    ReM-ReO & 0.551 & 0.568 & 0.582 & 0.906 & 0.988 & 0.970 & 1.411 & 1.408 & 1.421 \\ 
    BCRD-ReB & 0.568 & 0.565 & 0.579 & 0.909 & 0.968 & 0.977 & 1.450 & 1.406 & 1.420 \\ 
    BCRD-ReO & 0.546 & 0.564 & 0.578 & 0.915 & 0.947 & 0.973 & 1.429 & 1.425 & 1.416 \\ 
    ReM & 0.551 & 0.560 & 0.560 & 0.894 & 0.936 & 0.926 & 1.390 & 1.407 & 1.408 \\ 
    \cmidrule(lr){2-10}
    \multicolumn{10}{c}{}\vspace{-0.3cm}\\
    & \multicolumn{9}{c}{$p=20$}\\\cmidrule(lr){2-10}
    ReO & 0.722 & 0.707 & 0.680 & 1.120 & 1.123 & 1.086 & 1.825 & 1.805 & 1.843 \\ 
    ReM-ReB & $--$ & 0.703 & 0.670 & 1.129 & 1.105 & 1.088 & 1.843 & 1.821 & 1.784 \\  
    ReM-ReO & $--$ & 0.719 & 0.659 & 1.136 & 1.112 & 1.055 & 1.837 & 1.793 & 1.841 \\  
    BCRD-ReB & $--$ & 0.698 & 0.679 & 1.133 & 1.116 & 1.037 & 1.823 & 1.817 & 1.810 \\ 
    BCRD-ReO & $--$ &0.700 & 0.667 & 1.126 & 1.117 & 1.089 & 1.836 & 1.808 & 1.813 \\  
    ReM & 0.710 & 0.673 & 0.663 & 1.112 & 1.096 & 1.081 & 1.825 & 1.798 & 1.787 \\ 
    \midrule
    \end{tabular}
}
\end{table}

\begin{figure}[ht]
\centering
\footnotesize
\includegraphics[scale=0.55]{r_time.png}
\caption{Running time ratio of re-randomization procedures over ReM under different proportion of sample in first stage $r$ when sample size $N$ fixed as 600 in linear setting.}
\label{fig:r_time}
\end{figure}

\subsection{Simulation results under non-linear settings}\label{sec:nonlinear_result}

Under non-linear settings, the acceptance rate of ReM is set to be $0.05$ and the acceptance probabilities of other methods are tuned numerically as in experiments under linear settings. The detailed acceptance rates as well as their corresponding running time of sampling 500 assignments are shown in Table \ref{tab:ARnonlinear} and Table \ref{tab:RTnonlinear}.

\begin{table}[ht]
    \centering
    \footnotesize
\caption{Acceptance rates of different re-randomization procedures under non-linear settings.}
\label{tab:ARnonlinear}
    \setlength{\tabcolsep}{2.5mm}{
% \normalsize
\begin{tabular}{cccccccccc}
    \midrule
    & \multicolumn{3}{c}{$N=200$}&\multicolumn{3}{c}{$N=600$}&\multicolumn{3}{c}{$N=1200$}\\
    \cmidrule(lr){2-4}
    \cmidrule(lr){5-7}
    \cmidrule(lr){8-10}
    Scheme & $r=0.1$ & 0.2 & 0.4 & 0.1 & 0.2 & 0.4 & 0.1 & 0.2 & 0.4 \\\hline
    \multicolumn{10}{c}{}\vspace{-0.2cm}\\
    & \multicolumn{9}{c}{$p=2$}\\\cmidrule(lr){2-10}
    ReM-ReB &  0.087 & 0.085 & 0.085 & 0.072 & 0.071 & 0.071 & 0.063 & 0.064 & 0.064 \\ 
    ReM-ReO & 0.087 & 0.085 & 0.085 & 0.071 & 0.071 & 0.071 & 0.063 & 0.063 & 0.064 \\
    BCRD-ReB & 0.048 & 0.045 & 0.043 & 0.048 & 0.043 & 0.037 & 0.044 & 0.042 & 0.037 \\ 
    BCRD-ReO & 0.047 & 0.046 & 0.043 & 0.047 & 0.043 & 0.037 & 0.045 & 0.042 & 0.036 \\
    ReM & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 \\ 
    \cmidrule(lr){2-10}
    \multicolumn{10}{c}{}\vspace{-0.3cm}\\
    & \multicolumn{9}{c}{$p=5$}\\\cmidrule(lr){2-10}
    ReM-ReB & 0.092 & 0.094 & 0.086 & 0.071 & 0.066 & 0.070 & 0.063 & 0.064 & 0.065 \\ 
    ReM-ReO & 0.090 & 0.086 & 0.086 & 0.070 & 0.066 & 0.071 & 0.063 & 0.063 & 0.064 \\ 
    BCRD-ReB &0.047 & 0.046 & 0.040 & 0.046 & 0.045 & 0.037 & 0.046 & 0.042 & 0.035 \\ 
    BCRD-ReO & 0.046 & 0.045 & 0.040 & 0.046 & 0.040 & 0.038 & 0.046 & 0.042 & 0.034 \\ 
    ReM & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 \\ 
    \cmidrule(lr){2-10}
    \multicolumn{10}{c}{}\vspace{-0.3cm}\\
    & \multicolumn{9}{c}{$p=10$}\\\cmidrule(lr){2-10}
    ReM-ReB & 0.118 & 0.092 & 0.084 & 0.073 & 0.072 & 0.066 & 0.063 & 0.063 & 0.063 \\ 
    ReM-ReO & 0.118 & 0.090 & 0.083 & 0.072 & 0.070 & 0.068 & 0.062 & 0.062 & 0.063 \\  
    BCRD-ReB & 0.047 & 0.045 & 0.045 & 0.046 & 0.043 & 0.036 & 0.046 & 0.043 & 0.034 \\ 
    BCRD-ReO & 0.044 & 0.043 & 0.039 & 0.044 & 0.042 & 0.034 & 0.045 & 0.041 & 0.035 \\ 
    ReM & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 \\ 
    \cmidrule(lr){2-10}
    \multicolumn{10}{c}{}\vspace{-0.3cm}\\
    & \multicolumn{9}{c}{$p=20$}\\\cmidrule(lr){2-10}
    ReM-ReB & $--$ & 0.114 & 0.085 & 0.079 & 0.069 & 0.070 & 0.062 & 0.060 & 0.060 \\ 
    ReM-ReO & $--$ & 0.110 & 0.083 & 0.078 & 0.072 & 0.067 & 0.061 & 0.059 & 0.062 \\  
    BCRD-ReB & $--$ & 0.042 & 0.039 & 0.046 & 0.041 & 0.035 & 0.044 & 0.040 & 0.032 \\ 
    BCRD-ReO & $--$ & 0.040 & 0.035 & 0.044 & 0.040 & 0.035 & 0.043 & 0.039 & 0.031 \\ 
    ReM & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 & 0.050 \\ 
    \midrule
    \end{tabular}
}
\end{table}

\begin{table}[ht]
    \centering
    \footnotesize
\caption{Average running time (seconds) of sampling 500 assignments using different re-randomization procedures under non-linear settings with tuned acceptance rates.}
\label{tab:RTnonlinear}
    \setlength{\tabcolsep}{2.5mm}{
% \normalsize
\begin{tabular}{cccccccccc}
    \midrule
    & \multicolumn{3}{c}{$N=200$}&\multicolumn{3}{c}{$N=600$}&\multicolumn{3}{c}{$N=1200$}\\
    \cmidrule(lr){2-4}
    \cmidrule(lr){5-7}
    \cmidrule(lr){8-10}
    Scheme & $r=0.1$ & 0.2 & 0.4 & 0.1 & 0.2 & 0.4 & 0.1 & 0.2 & 0.4 \\\hline
    \multicolumn{10}{c}{}\vspace{-0.2cm}\\
    & \multicolumn{9}{c}{$p=2$}\\\cmidrule(lr){2-10}
    ReM-ReB & 0.449 & 0.459 & 0.454 & 0.744 & 0.730 & 0.757 & 1.147 & 1.141 & 1.158 \\ 
    ReM-ReO & 0.450 & 0.460 & 0.452 & 0.746 & 0.722 & 0.756 & 1.136 & 1.169 & 1.151 \\ 
    BCRD-ReB & 0.449 & 0.454 & 0.447 & 0.731 & 0.728 & 0.753 & 1.133 & 1.138 & 1.146 \\ 
    BCRD-ReO & 0.459 & 0.456 & 0.450 & 0.744 & 0.736 & 0.755 & 1.130 & 1.137 & 1.146 \\  
    ReM & 0.449 & 0.454 & 0.442 & 0.740 & 0.727 & 0.741 & 1.125 & 1.134 & 1.135 \\ 
    \cmidrule(lr){2-10}
    \multicolumn{10}{c}{}\vspace{-0.3cm}\\
    & \multicolumn{9}{c}{$p=5$}\\\cmidrule(lr){2-10}
    ReM-ReB & 0.507 & 0.455 & 0.485 & 0.808 & 0.847 & 0.825 & 1.266 & 1.250 & 1.225 \\ 
    ReM-ReO & 0.515 & 0.488 & 0.482 & 0.813 & 0.845 & 0.800 & 1.265 & 1.257 & 1.246 \\  
    BCRD-ReB & 0.505 & 0.492 & 0.492 & 0.804 & 0.769 & 0.798 & 1.255 & 1.244 & 1.243 \\ 
    BCRD-ReO & 0.508 & 0.492 & 0.492 & 0.809 & 0.855 & 0.800 & 1.248 & 1.262 & 1.258 \\ 
    ReM & 0.496 & 0.486 & 0.486 & 0.790 & 0.794 & 0.795 & 1.247 & 1.253 & 1.239 \\ 
    \cmidrule(lr){2-10}
    \multicolumn{10}{c}{}\vspace{-0.3cm}\\
    & \multicolumn{9}{c}{$p=10$}\\\cmidrule(lr){2-10}
    ReM-ReB &  0.552 & 0.565 & 0.573 & 0.921 & 0.883 & 0.956 & 1.421 & 1.389 & 1.426 \\  
    ReM-ReO & 0.555 & 0.572 & 0.573 & 0.933 & 0.902 & 0.923 & 1.429 & 1.409 & 1.426 \\  
    BCRD-ReB & 0.539 & 0.559 & 0.523 & 0.920 & 0.898 & 0.930 & 1.419 & 1.380 & 1.427 \\ 
    BCRD-ReO & 0.558 & 0.559 & 0.567 & 0.920 & 0.902 & 0.965 & 1.407 & 1.413 & 1.424 \\ 
    ReM & 0.547 & 0.553 & 0.559 & 0.913 & 0.894 & 0.895 & 1.400 & 1.391 & 1.404 \\ 
    \cmidrule(lr){2-10}
    \multicolumn{10}{c}{}\vspace{-0.3cm}\\
    & \multicolumn{9}{c}{$p=20$}\\\cmidrule(lr){2-10}
    ReM-ReB & $--$ & 0.655 & 0.668 & 1.069 & 1.116 & 1.057 & 1.811 & 1.987 & 1.799 \\  
    ReM-ReO & $--$ & 0.689 & 0.676 & 1.081 & 1.065 & 1.094 & 1.832 & 2.032 & 1.751 \\ 
    BCRD-ReB & $--$ & 0.678 & 0.670 & 1.079 & 1.113 & 1.097 & 1.811 & 2.037 & 1.850 \\ 
    BCRD-ReO & $--$ &0.681 & 0.679 & 1.098 & 1.110 & 1.084 & 1.827 & 2.014 & 1.853 \\  
    ReM & 0.685 & 0.676 & 0.665 & 1.085 & 1.087 & 1.079 & 1.815 & 2.019 & 1.797 \\ 
    \midrule
    \end{tabular}
}
\end{table}

Table \ref{tab:nonlinear} shows the average
PRIV of different re-randomization methods with different $p, r$ and $N$. 
It implies that two-stage ReB still outperforms ReM in most cases, as long as sample size is not very small or the proportion of sample in first stage is not very large, even under nonlinear model. Figure \ref{fig:nonlinear} shows the effect of $p$ on these methods with fixed $r=0.2$ and $p=10$, 
and the effect of $N$ when $r=0.2$ and $N=600$. As sample size increases or dimension $p$ gets larger, the advantage of two-stage ReB schemes gets more obvious. Also, ReM-ReB has the best performance among the four schemes. 


\begin{table}
    \centering
    \footnotesize
\caption{Average PRIV achieved by different re-randomization procedures under non-linear model.}
\label{tab:nonlinear}
    \setlength{\tabcolsep}{2.5mm}{
% \normalsize
\begin{tabular}{cccccccccc}
    \midrule
    & \multicolumn{3}{c}{$N=200$}&\multicolumn{3}{c}{$N=600$}&\multicolumn{3}{c}{$N=1200$}\\
    \cmidrule(lr){2-4}
    \cmidrule(lr){5-7}
    \cmidrule(lr){8-10}
    Scheme & $r=0.1$ & 0.2 & 0.4 & 0.1 & 0.2 & 0.4 & 0.1 & 0.2 & 0.4 \\\hline
    \multicolumn{10}{c}{}\vspace{-0.2cm}\\
    & \multicolumn{9}{c}{$p=2$}\\\cmidrule(lr){2-10}
    ReM-ReB & \textbf{48.12} & \textbf{49.03} & \textbf{49.48} & \textbf{48.71} & \textbf{48.40} & \textbf{48.99} & \textbf{49.39} & 48.96 & \textbf{49.75} \\ 
    ReM-ReO & 45.28 & 47.65 & 48.53 & 47.56 & 47.91 & 48.80 & 48.81 & \textbf{49.15} & 49.48 \\ 
    BCRD-ReB & 46.62 & 44.64 & 37.23 & 46.00 & 43.35 & 36.61 & 47.39 & 43.83 & 37.06 \\ 
    BCRD-ReO & 42.46 & 42.50 & 36.28 & 44.76 & 42.84 & 36.59 & 46.27 & 43.34 & 36.43 \\
    \cellcolor[HTML]{D9D9D9} ReM & \cellcolor[HTML]{D9D9D9}47.98 & \cellcolor[HTML]{D9D9D9}48.20 & \cellcolor[HTML]{D9D9D9}48.92 & \cellcolor[HTML]{D9D9D9}47.82 & \cellcolor[HTML]{D9D9D9}47.23 & \cellcolor[HTML]{D9D9D9}48.14 & \cellcolor[HTML]{D9D9D9}48.35 & \cellcolor[HTML]{D9D9D9}48.45 & \cellcolor[HTML]{D9D9D9}48.55 \\ 
    \cmidrule(lr){2-10}
    \multicolumn{10}{c}{}\vspace{-0.3cm}\\
    & \multicolumn{9}{c}{$p=5$}\\\cmidrule(lr){2-10}
    ReM-ReB & \textbf{43.48} & \textbf{44.63} & \textbf{44.64} & \textbf{44.99} & \textbf{46.69} & 45.22 & \textbf{47.14} & \textbf{46.01} & \textbf{45.12} \\ 
    ReM-ReO & 37.47 & 42.39 & 43.24 & 42.47 & 46.13 & \textbf{45.38} & 45.71 & 45.97 & 44.90 \\ 
    BCRD-ReB & 43.25 & 41.99 & 36.13 & 43.42 & 43.26 & 36.92 & 45.42 & 42.18 & 35.62 \\ 
    BCRD-ReO & 33.46 & 38.21 & 34.01 & 40.16 & 41.21 & 36.06 & 43.99 & 41.43 & 34.94 \\ 
    \cellcolor[HTML]{D9D9D9} ReM & \cellcolor[HTML]{D9D9D9}42.21 & \cellcolor[HTML]{D9D9D9}42.01 & \cellcolor[HTML]{D9D9D9}41.98 & \cellcolor[HTML]{D9D9D9}41.26 & \cellcolor[HTML]{D9D9D9}42.14 & \cellcolor[HTML]{D9D9D9}42.10 & \cellcolor[HTML]{D9D9D9}40.91 & \cellcolor[HTML]{D9D9D9}41.13 & \cellcolor[HTML]{D9D9D9}41.09 \\ 
    \cmidrule(lr){2-10}
    \multicolumn{10}{c}{}\vspace{-0.3cm}\\
    & \multicolumn{9}{c}{$p=10$}\\\cmidrule(lr){2-10}
    ReM-ReB & 32.66 & \textbf{39.24} & \textbf{39.97} & \textbf{41.64} & \textbf{42.27} & \textbf{41.16} & \textbf{43.42} & \textbf{43.44} & \textbf{42.00} \\
    ReM-ReO & 23.56 & 34.43 & 37.68 & 38.59 & 41.02 & 40.99 & 42.46 & 43.26 & 41.55 \\ 
    BCRD-ReB & \textbf{34.74} & 37.04 & 33.71 & 40.65 & 40.01 & 34.75 & 42.52 & 40.46 & 34.93 \\ 
    BCRD-ReO &  22.09 & 30.47 & 30.36 & 37.11 & 38.19 & 34.13 & 41.06 & 40.44 & 34.43 \\ 
    \cellcolor[HTML]{D9D9D9} ReM & \cellcolor[HTML]{D9D9D9}32.90 & \cellcolor[HTML]{D9D9D9}32.95 & \cellcolor[HTML]{D9D9D9}33.33 & \cellcolor[HTML]{D9D9D9}33.89 & \cellcolor[HTML]{D9D9D9}33.80 & \cellcolor[HTML]{D9D9D9}33.59 & \cellcolor[HTML]{D9D9D9}32.71 & \cellcolor[HTML]{D9D9D9}32.51 & \cellcolor[HTML]{D9D9D9}32.69 \\ 
    \cmidrule(lr){2-10}
    \multicolumn{10}{c}{}\vspace{-0.3cm}\\
    & \multicolumn{9}{c}{$p=20$}\\\cmidrule(lr){2-10}
    ReM-ReB & $--$ & 27.26 & \textbf{33.27} & 33.77 & \textbf{37.83} & \textbf{36.75} & \textbf{40.10} & \textbf{40.87} & \textbf{39.21} \\ 
    ReM-ReO & $--$ & 22.23 & 31.14 & 29.06 & 35.47 & 36.65 & 38.32 & 39.97 & 38.99 \\ 
    BCRD-ReB & $--$ & \textbf{27.99} & 28.01 & \textbf{33.83} & 36.15 & 31.95 & 39.14 & 39.06 & 34.05 \\ 
    BCRD-ReO & $--$ & 17.92 & 23.83 & 27.30 & 33.24 & 30.72 & 36.67 & 38.09 & 33.73 \\ 
    \cellcolor[HTML]{D9D9D9} ReM & \cellcolor[HTML]{D9D9D9}25.16 & \cellcolor[HTML]{D9D9D9}25.37 & \cellcolor[HTML]{D9D9D9}25.31 & \cellcolor[HTML]{D9D9D9}25.67 & \cellcolor[HTML]{D9D9D9}25.71 & \cellcolor[HTML]{D9D9D9}25.74 & \cellcolor[HTML]{D9D9D9}25.00 & \cellcolor[HTML]{D9D9D9}24.95 & \cellcolor[HTML]{D9D9D9}24.96 \\ 
    \midrule
    \end{tabular}
}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[scale=0.6]{nonlinear2.png}
\caption{Performance of BCRD-ReO, BCRD-ReB, ReM-ReO and ReM-ReB compared with ReM with $r=0.2$ under non-linear model. Sub-figure (A) investigates into factor 
$N$ with covariate dimensionality $p$ fixed at 
10, while sub-figure (B) investigates 
into factor $p$ with $N$ fixed at 600.}
\label{fig:nonlinear}
\end{figure}

The effect of the proportion of sample in first stage on the performance of two-stage ReB procedures can also be studied by conducting experiment on ReM, BCRD-ReO, BCRD-ReB, ReM-ReO and ReM-ReB with $r$ ranging from 0.05 to 0.8 when $N=600$, $p=10$, $\bar{R}^2=0.5$ and $\rho=0$. Similar to the experiments in linear setting, we set the acceptance rate of ReM be $\alpha=0.05$ and calculate the acceptance probabilities of other re-randomization procedures numerically. The running time ratio of re-randomization procedures over ReM is shown in Figure \ref{fig:nonlinear_r_time}.
Average PRIVs of these methods under different specifications of $r$ are plotted in Figure \ref{fig:r_plot3} and it is easy to see that ReM-ReB still exhibits the best robustness to the proportion of sample in first stage and similar conclusions as in linear settings can be drawn.

\begin{figure}[t]
    \centering
\includegraphics[scale=0.55]{nonlinear_r_time.png}
    \caption{Running time ratio of re-randomization procedures over ReM under different proportion of sample in first stage $r$ when sample size $N$ fixed as 600 in non-linear setting.}
    \label{fig:nonlinear_r_time}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[scale=0.55]{r22.png}
    \caption{Average PRIV of ReM, ReM-ReB, ReM-ReO, BCRD-ReB and BCRD-ReO with different proportion of sample in first stage $r$ when total sample size $N=600$ and covariate dimensionality $p=10$ under non-linear model.}
    \label{fig:r_plot3}
\end{figure}

\subsection{Running time ratio in real-data analysis}\label{sec:realdata_time}

Figure \ref{fig:real_time} shows the running time ratio of re-randomization procedures against ReM under different proportion of sample in first stage $r$ in real application, i.e., Section \ref{sec:RealDataAnalysis}.

\begin{figure}[t]
    \centering
\includegraphics[scale=0.55]{real_time.png}
    \caption{Running time ratio of re-randomization procedures over ReM under different proportion of sample in first stage $r$ in real-data analysis.}
    \label{fig:real_time}
\end{figure}
\end{document}
