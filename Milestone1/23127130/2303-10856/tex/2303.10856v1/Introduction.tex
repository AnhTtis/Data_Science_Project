\IEEEPARstart{T}{he} recent success in deep learning is attributed to the availability of large labeled data~\cite{krizhevsky2012imagenet,carreira2017quo,zhou2018brief} and the assumption of i.i.d. between training and test datasets. Such assumptions could be violated when testing data features a drastic difference from the training data, e.g. training on synthetic images and test on real ones, or training on clean samples and test on corrupted ones. This situation is often referred to as domain shift~\cite{quinonero2008dataset,ben2010theory,pan2009survey}. To tackle this issue, domain adaptation~(DA)~\cite{wang2018deep} emerges and the labeled training data and unlabeled testing data are often referred to as source and target data/domains respectively.
The existing DA works either require the access to both source and target domain data during training~\cite{ganin2015unsupervised,tzeng2017adversarial,hoffman2018cycada} or training on multiple domains simultaneously~\cite{zhou2021domain}. The former approaches render the methods restrictive to limited scenarios where source domain data is always available during adaptation while the latter ones are computationally more expensive. 


To alleviate the dependence on source domain data, which may be inaccessible due to privacy issues or storage overhead, source-free domain adaptation (SFDA) emerges which handles DA on target data without access to source data~\cite{pmlr-v119-liang20a,kundu2020universal,yang2021generalized,xia2021adaptive,liu2021ttt++}. SFDA is often achieved through self-training~\cite{pmlr-v119-liang20a,qiu2021source}, self-supervised learning~\cite{liu2021ttt++,huang2021model} or introducing prior knowledge~\cite{pmlr-v119-liang20a} and it requires multiple training epochs on the full target {dataset} to allow model convergence. Despite easing the dependence on source data, SFDA has major drawbacks in a more realistic domain adaptation scenario where test data arrives in a stream and inference or prediction must be taken instantly, and this setting is often referred to as test-time training (TTT) or adaptation (TTA)~\cite{sun2020test,wang2020tent,iwasawa2021test,liu2021ttt++,chen2022contrastive,gandelsman2022test}. Despite the attractive feature of adaption at {test time}, we notice a confusion of what defines a test-time training and as a result comparing apples and oranges happens frequently in the community. In this work, we first categorize TTT by \textbf{two key factors} after summarizing various definitions made in existing works. First, under a realistic TTT setting, test samples are sequentially streamed and predictions should be made instantly upon the arrival of a new test sample. More specifically, the prediction of test sample $x_T$, arriving at time stamp $T$, should not be affected by any subsequent samples, $\{x_t\}_{t=T+1\cdots\infty}$. The sequential protocol widely exists in many real-world application. For example, in video surveillance, cameras are expected to function instantly after installment and adaptation to target domain must be carried out on-the-fly. Throughout this work, We refer to the sequential streaming setting as the \textbf{one-pass adaptation} protocol and any other protocols violating this assumption are referred to as \textbf{multi-pass adaptation} (model may be updated on all test data for multiple epochs before inference). Second, we notice some recent works must \textbf{modify source domain training loss}, e.g. by introducing additional self-supervised branch, to allow more effective TTT~\cite{sun2020test,liu2021ttt++}. This will introduce additional overhead in the deployment of TTT because re-training on some source dataset, e.g. ImageNet, is computationally expensive. Thus, we distinguish methods by whether source domain training objective is modified or not.
% Given the distinctions above, we provide a categorization of existing works, which are often confused under the umbrella of TTT, TTA and SFDA, and comparisons within each category are then fair.

In this work, we aim to tackle on the most realistic TTT protocol, i.e. one-pass test time training with no modifications to training objective. We refer to this new TTT protocol as \textbf{sequential test time training (sTTT)}. 
The proposed setting is similar to TTA proposed in~\cite{wang2020tent} except for not restricting access to a light-weight distribution information from the source domain. We believe having access to distribution information, e.g. distributions' mean and covariance, in the source domain is a realistic assumption for two reasons. First, the objective of TTT is efficient adaptation at {test time}, this assumption only requires storing the means and covariance matrices which are memory efficient. Moreover, feature distribution information will not pose any threat to privacy leakage as inverting backbone network, e.g. CNN, is known to be very challenging~\cite{gilbert2017towards}. Nevertheless, we are aware that under certain scenarios source domain distribution information is not always available for test-time training, i.e. \textbf{source-free test-time training}. Such a situation could happen when source distribution is not recorded during training, or model is trained through federated learning where access to the whole source data is prohibited~\cite{li2021survey}. A robust TTT method should therefore be versatile and still function well in the absence of source domain distribution information. %in the absence of source domain distribution information, we propose a technique to estimate the distribution parameters from model weights alone. %Empirical evaluation demonstrates the efficacy of learning source domain distributions.

%Moreoribution.ver,  is computationally efficient and improves TTT performance substantially.  %Contrary to source-free domain adaptation, where information about source domain is often strictly absent, we believe having access to statistical information in the source domain is a realistic assumption for two reasons. First, source domain 
%When source domain information is strictly prohibited, e.g. practitioners only have access to source domain pretrained models, we further differentiate a source-free sTTT setting which might be less common but poses more challenges to the design of TTT algorithms.

In this work, we propose four techniques to enable efficient and accurate sTTT regardless the availability of source domain distribution information. i) We are inspired by the recent progresses in unsupervised domain adaptation~\cite{tang2020unsupervised} that encourages testing samples to form clusters in the feature space. However, separately learning to cluster in the target domain without regularization from source domain does not guarantee improved adaptation~\cite{tang2020unsupervised}. 
To overcome this challenge, we identify clusters in both the source and target domains through a mixture of Gaussians with each component Gaussian corresponding to one category. Provided with the category-wise statistics from source domain as anchors, we match the target domain clusters to the anchors by minimizing the KL-Divergence as the training objective for sTTT. Therefore, we name the proposed method through feature clustering alone as \textit{test-time anchored clustering (TTAC)}. Since test samples are sequentially streamed, we develop an exponential moving averaging strategy to update the target domain cluster statistics to allow gradient-based optimization. ii) Each component Gaussian in the target domain is updated by the test sample features that are assigned to the corresponding category. Thus, incorrect assignments (pseudo labels) will harm the estimation of component Gaussian. To tackle this issue, we are inspired by the correlation between network's stability and confidence and pseudo label accuracy~\cite{lee2013pseudo,sohn2020fixmatch}, and propose to filter out potential incorrect pseudo labels. Component Gaussians are then updated by the samples that have passed the {filterings}. To exploit the filtered out samples, we  incorporate a global feature alignment~\cite{liu2021ttt++} objective. iii) Self-training~(ST) exploits unlabeled data through predicting pseudo labels with existing model and use the predicted pseudo labels as target to further update the model parameters. ST has demonstrated remarkable success for semi-supervised learning~\cite{sohn2020fixmatch} and domain adaptation~\cite{liu2021cycle}, and we hypothesize that self-training on target domain data could benefit TTT as well. However, as we discovered empirically, direct self-training on target domain yields much inferior results compared to anchored clustering. We attribute this phenomenon to the fact that when there is a significant distribution shift between source and target domains pseudo labels predicted on target domain are more likely to be incorrect. As a result, self-training on incorrect pseudo labels leads to inferior accuracy on target domain. To alleviate the impact of distribution shift on self-training, we use anchored clustering to regularize self-training such that we can simultaneously minimize distribution shift and exploit pseudo labels on target domain to update model parameters. We refer to the combined model as \textbf{TTAC++} to acknowledge the importance of anchored clustering. Extensive evaluations have demonstrated the effectiveness of combining anchored clustering and self-training. iv) When source domain distribution information is strictly absent, we propose to infer the source domain distribution by backpropagating classification loss through category-wise distribution parameters. We demonstrate through simple derivation that sampling from the distribution is not necessary during the optimization and the distribution parameters can be learned by efficient gradient descent methods.
%We also demonstrate TTAC is compatible with existing TTT techniques, e.g. contrastive learning branch~\cite{liu2021ttt++}, if source training loss is allowed to be modified. 

We summarize the contributions of this work as below.
%This is demonstrated to be crucial to the success of sTTT.
% Eventually, we observe that the proposed method is complementary to multiple existing domain adaptation techniques, e.g. global feature alignment~\cite{ganin2015unsupervised}, entropy minimization~\cite{wang2020tent} and self-supervised learning~\cite{liu2021ttt++}. The performance is further enhanced by combining our method with each of the above techniques.
% We validate the proposed sTTT approach on multiple test-time adaptation datasets, its effectiveness is demonstrated by achieving SOTA performance under all categorizations of TTT settings. 


% there are two concurrent lines of works emerging, namely source-free domain adaptation~(SFDA)~\cite{kundu2020universal} and test-time training~(TTT)~\cite{sun2020test,wang2020tent,iwasawa2021test,liu2021ttt++}. For both directions, the existing works assume no access to source domain and DA is carried out on the target data only. Despite the confusing terminologies, in this work, we first summarize the key differences in the above works in xxx aspects. Firstly, some works allows iterative training on target data for multiple epochs and then test on the same data~\cite{kundu2020universal, liu2021ttt++}, termed multi-pass throughout this work. This strategy will substantially improve the adaptation to test distribution, however assuming all test data being available before inference is sometimes unrealistic. In contrast, TTT allows a single pass on the test data in a sequential manner. Secondly, depending whether training objective is allowed to be modified, we further categorize these works into source domain training frozen and non-frozen approaches. The former does not require training objective to be changed~\cite{kundu2020universal,wang2020tent,iwasawa2021test} while the latter will add additional terms in the training objective function~\cite{sun2020test,liu2021ttt++}. Given the above categorizations, we first provide a fair benchmarking of existing SFDA and TTT works in this paper which will help follow-up research works to position to the correct track.


\begin{itemize}
\item In light of the confusions within TTT works, we provide a categorization of TTT protocols by {two} key factors. Comparison of TTT methods is now fair within each category.
% \item Based on the categorization we provide the first benchmark comparing existing test time adaptation works under a fair setting.
\item We adopt a realistic TTT setting, namely sTTT. To improve test-time feature learning, we propose TTAC by matching the statistics of the target clusters to the source ones. The target statistics are updated through moving averaging with filtered pseudo labels. 
\item To further exploit the unlabeled target domain data, we incorporate a self-training approach to update model w.r.t. classification loss, and we reveal that regularizing self-training with anchored clustering, referred to as TTAC++, consistently outperforms TTAC with minute additional computation overhead.
\item To enable strict source-free test-time training, we develop a light-weight method to infer source domain distributions. We demonstrate that TTAC++ outperforms state-of-the-art methods under the strict source-free sTTT protocol.
\item The proposed method is demonstrated on five test-time training datasets, among which three datasets~(CIFAR10/100-C \& ImageNet-C) focus on test-time adaptation to corrupted target domains, one~(CIFAR10.1) focuses on selected hard samples and another one~(VisDA) focuses on synthetic-to-real adaptation. We also evaluate test-time training on adversarially attacked target dataset. We demonstrate that TTAC++ achieves the state-of-the-art performance on all benchmarks under multiple TTT protocols.
\end{itemize}


% \section{Related Work}

% \noindent\textbf{Unsupervised Domain Adaptation}. 
% Domain adaptation aims to improve model generalization when source and target data are not drawn i.i.d. When target data are unlabeled, UDA~\cite{ganin2015unsupervised,tzeng2014deep} learns domain invariant feature representations on both source and target domains to improve generalization. Follow-up works improve DA by minimizing a divergence~\cite{gretton2012kernel,sun2016deep,zellinger2017central}, adversarial training~\cite{hoffman2018cycada} or discovering cluster structures in the target data~\cite{tang2020unsupervised}. Apart from formulating DA within a task-specific model, re-weighting has been adopted for domain adaptation by selectively up-weighting conducive samples in the source domain~\cite{jiang2007instance, yan2017mind}. Despite the efforts in UDA, it is inevitable to access the source domain data which may be not accessible due to privacy issues, storage overhead, etc. Deploying DA in more realistic scenarios has inspired research into source-free domain adaptation and test-time training/adaptation.

% \noindent\textbf{Source-Free Domain Adaptation}. 
% Without the access to source data, source-free domain adaptation (SFDA) develops domain adaptation through self-training~\cite{pmlr-v119-liang20a, kundu2020universal, iwasawa2021test}, self-supervised training~\cite{liu2021ttt++} or clustering in the target domain~\cite{yang2021generalized}. It has been demonstrated that SFDA performs well on seminal domain adaptation datasets even compared against UDA methods~\cite{tang2020unsupervised}. Nevertheless, SFDA still requires access to all testing data and training must be carried out iteratively on the testing data. In a more realistic DA scenario where inference and adaptation must be implemented simultaneously, SFDA will no longer be effective. Moreover, some statistical information on the source domain does not pose privacy issues and can be exploited to further improve adaptation on target data.

% \noindent\textbf{Test-Time Training}. 
% Collecting enough samples from target domain and adapt models in an offline manner restricts the application to adapting to static target domain. To allow fast and online adaptation, test-time training (TTT)~\cite{sun2020test,wang2022continual} or adaptation (TTA)~\cite{wang2020tent} emerges. Despite many recent works claiming to be test-time training, we notice a severe confusion over the definition of TTT. In particular, whether training objective must be modified~\cite{sun2020test,liu2021ttt++} and whether sequential inference on target domain data is possible~\cite{wang2020tent,iwasawa2021test}. Therefore, to reflect the key challenges in TTT, we define a setting called sequential test-time training (sTTT) which neither modifies the training objective nor violates  sequential inference. Under the more clear definition, some existing works, e.g. TTT~\cite{sun2020test} and TTT++~\cite{liu2021ttt++} is more likely to be categorized into SFDA. Several existing works~\cite{wang2020tent,iwasawa2021test} can be adapted to the sTTT protocol. Tent~\cite{wang2020tent} proposed to adjust affine parameters in the batchnorm layers to adapt to target domain data. The high TTA efficiency inevitably leads to limited performance gain on the target domain. T3A~\cite{iwasawa2021test} further proposed to update classifier prototype through pseudo labeling. Despite being efficient, updating classifier prototype alone does not affect feature representation for the target domain. Target feature may not form clusters at all when the distribution mismatch between source and target is large enough. In this work we propose to simultaneously cluster on the target domain and match target clusters to source domain classes, namely anchored clustering. To further constrain feature update, we introduce additional global feature alignment and pseudo label filtering. Through the introduced anchored clustering, we achieve test-time training of more network parameters and achieve the state-of-the-art performance.
