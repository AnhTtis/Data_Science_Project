

\subsection{Unsupervised Domain Adaptation}
Domain adaptation~\cite{wang2018deep} aims to improve model generalization when source and target data are not drawn i.i.d. Unsupervised domain adaptation~(UDA)~\cite{ganin2015unsupervised,tzeng2014deep,long2015learning} makes an assumption that labeled data is only available in the source domain and target domain data are totally unlabeled. UDA methods often simultaneously learn domain invariant feature representations on both source and target domains to improve generalization. This is achieved by introducing a domain discriminator~\cite{ganin2015unsupervised}, 
Follow-up works improve DA by minimizing a divergence~\cite{gretton2012kernel,sun2016deep,zellinger2017central}, adversarial training~\cite{hoffman2018cycada} or discovering cluster structures in the target data~\cite{tang2020unsupervised}. Apart from formulating DA within a task-specific model, re-weighting has been adopted for domain adaptation by selectively up-weighting conducive samples in the source domain~\cite{jiang2007instance, yan2017mind}. Despite the efforts in UDA, it is inevitable to access the source domain data which may be not accessible due to privacy issues, storage overhead, etc. Deploying DA in more realistic scenarios has inspired research into source-free domain adaptation and test-time training/adaptation.

\subsection{Source-Free Domain Adaptation}
UDA is often implemented by simultaneously updating model parameters on both source and target domain data~\cite{ganin2015unsupervised}. Having access to target domain data during model training may not be practical in real-world applications. For instance, users may buy pretrained model from suppliers and hope to adapt to proprietary data. Access to source domain data could be prohibited due to privacy or data storage issues. Without the access to source data, source-free domain adaptation (SFDA) emerges as a more realistic solution. SFDA is often developed through self-training~\cite{pmlr-v119-liang20a, kundu2020universal,qiu2021source, iwasawa2021test,liang2021source}, self-supervised training~\cite{liu2021ttt++} or clustering in the target domain~\cite{yang2021generalized}. It has been demonstrated that SFDA performs well on seminal domain adaptation datasets even compared against UDA methods~\cite{tang2020unsupervised}. SFDA often requires access to all testing data and model adaptation is carried out by iteratively updating on the testing data. Despite the advantage of not requiring source domain data during model adaptation, the iterative model updating strategy restricts the application of SFDA to scenarios where target domain distribution is fixed and training data in target domain is readily available. In a more realistic DA scenario where data arrives in a stream and inference and adaptation must be implemented simultaneously SFDA will no longer be effective. %Moreover, some statistical information on the source domain does not pose privacy issues and can be exploited to further improve adaptation on target data.

\subsection{Test-Time Training}
Collecting enough samples from target domain and adapt models in an offline manner restricts the application to adapting to static target domain. To allow fast and online adaptation, test-time training (TTT)~\cite{sun2020test,wang2022continual,iwasawa2021test,gandelsman2022test,goyaltest2022,chen2022contrastive,choi2021test} or adaptation (TTA)~\cite{wang2020tent} emerges. TTT tackles a scenario where a distribution shift between source and target domain exists and source model is preferably adapted to target domain in a light-weight fashion. Despite many recent works claiming to be test-time training, we notice a severe confusion over the definition of TTT. In particular, whether training objective must be modified~\cite{sun2020test,liu2021ttt++} and whether sequential inference on target domain data is possible~\cite{wang2020tent,iwasawa2021test}. Therefore, to reflect the key challenges in TTT, we define a setting called sequential test-time training (sTTT) which neither modifies the training objective nor violates  sequential inference. Under the more clear definition, some existing works, e.g. TTT~\cite{sun2020test} and TTT++~\cite{liu2021ttt++} is more likely to be categorized into SFDA. Several existing works~\cite{wang2020tent,iwasawa2021test} can be adapted to the sTTT protocol. Tent~\cite{wang2020tent} proposed to adjust affine parameters in the batchnorm layers to adapt to target domain data. The high TTA efficiency inevitably leads to limited performance gain on the target domain. T3A~\cite{iwasawa2021test} further proposed to update classifier prototype through pseudo labeling. Despite being efficient, updating classifier prototype alone does not affect feature representation for the target domain. Target feature may not form clusters at all when the distribution mismatch between source and target is large enough. In this work we propose to simultaneously cluster on the target domain and match target clusters to source domain classes, namely anchored clustering. To further constrain feature update, we introduce additional global feature alignment and pseudo label filtering. Through the introduced anchored clustering, we achieve test-time training of more network parameters and achieve the state-of-the-art performance.


\subsection{Self-Training}

Training models with predictions from their own has been a long-standing paradigm for learning from unlabeled data. In the realm of semi-supervised learning~\cite{van2020survey}, which aims to exploit few labeled data and large unlabeled, self-training has been widely adopted to produce pseudo labels for unlabeled data. Among these works, label-propagation~\cite{zhu2003semi} is implemented on the deep representations to provide pseudo labels for unlabeled data and self-training is achieved by training with the pseudo labels~\cite{iscen2019label}. FixMatch~\cite{sohn2020fixmatch} utilizes the predictions on weak augmented samples as pseudo label to supervise network training on unlabeled data. MixMatch~\cite{berthelot2019mixmatch} sharpens model prediction to serve as pseudo label for self-training. Self-training recently emerges as a promising solution to domain adaptation by updating model on target pseudo labels~\cite{liu2021cycle,kumar2020understanding,xu2022revisiting}. Some concurrent works also demonstrated that self-training is also effective when source domain data is absent ~\cite{wang2021target,sinha2022test}. In this work, we hypothesize that self-training could benefit test-time training by providing pseudo labels on the target domain samples. More importantly, we discover that self-training alone without any constraint is less effective for TTT under large domain shift due to the high noise in pseudo labels. Through combining anchored clustering and self-training, we demonstrate a significant improvement from the previous state-of-the-art thanks to the improved pseudo label quality.

% \subsection{Out-of-Distribution}

% Building robust test-time training algorithms requires selective adaptation at test stage. In many real applications, e.g. autonomous driving, target domain may contain semantic out of distribution (OOD) samples, which are defined as samples not belonging to the source domain classes. Adapting source domain irrespective of OOD samples may ruin the model's performance on in-distribution samples. The 

% In this section, we briefly review works relevant to learning under out of semantic classes