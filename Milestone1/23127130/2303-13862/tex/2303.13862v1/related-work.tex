\section{Related Work}

\textbf{Few-Shot Learning}.
Few-shot learning aims at rapidly generalizing to new tasks with limited samples, 
leveraging the prior knowledge learned from a large-scale base dataset.
The existing methods can be divided into two groups.
Optimization-based methods~\cite{lee2019meta,rusu2018meta} try to enable fast model adaptation with few-shot data.
Metric-based algorithms~\cite{zhang2020deepemd,ma2021transductive} utilize a pretrained backbone for feature extraction, and employ proper distance metrics between support and query instances.
Recent research tries to leverage GNNs to explore complex similarities among examples. 
DPGN~\cite{yang2020dpgn} builds up a dual graph to model distribution-level relations of examples for FSL. 
ECKPN~\cite{chen2021eckpn} proposes an end-to-end transductive GNN to explore the class-level knowledge.

\noindent
\textbf{Class-Incremental Learning}.
Class-Incremental Learning aims to learn from a sequence of new classes without forgetting old ones, which is now widely discussed in various computer vision tasks.
Current CIL algorithms can be divided into three groups.
The first group estimates the importance of each parameter and prevents important ones from being changed~\cite{aljundi2018memory,kirkpatrick2017overcoming}.
The second group utilizes knowledge distillation to maintain the model's discriminability~\cite{rebuffi2017icarl}.
Other methods rehearse former instances to overcome forgetting~\cite{zhao2020maintaining,zhu2021prototype}.
Pernici \etal~\cite{pernici2021class} pre-allocates classifiers for future classes, which needs extra memory for feature tuning and is unsuitable for FSCIL.


\noindent
\textbf{Few-Shot Class-Incremental Learning}
Few-Shot Class-Incremental Learning is recently proposed to address the few-shot inputs in the incremental learning scenario.
TOPIC~\cite{tao2020few} uses the neural gas structure to preserve the topology of features between old and new classes to resist forgetting.
\cite{cheraghian2021semantic} treats the word embedding as auxiliary information, and builds knowledge distillation terms to resist forgetting.
CEC~\cite{zhang2021few} utilizes an extra graph model to propagate context information between classifiers for adaptation.
FACT~\cite{zhou2022forward} efficiently incorporates new classes with forward compatibility and meanwhile resists the forgetting of old ones. 

