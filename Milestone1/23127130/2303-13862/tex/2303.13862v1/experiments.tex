\section{Experiment}
% In this section, we compare SCGN on benchmark and large scale few-shot class-incremental learning datasets 
% with state-of-the-art mehtods.

\begin{table*}[h]
	\centering
	\caption{Comparison with the state-of-the-art on MiniImageNet dataset.}\label{lab:mini}
	\resizebox{1.\linewidth}{!}{
		\begin{tabular}{l cccccccccc cc  }
			\bottomrule
			\multirow{2}{*}{\textbf{Methods}} &\multicolumn{9}{c}{\textbf{Accuracy in each session(\%)}}  & \multirow{2}{*}{\textbf{PD}} & \multicolumn{1}{c}{\textbf{Our relative}} \\
			\cline{2-10}
			&\textbf{0} &\textbf{1}  &\textbf{2}&\textbf{3} &\textbf{4}&\textbf{5}  &\textbf{6}&\textbf{7}&\textbf{8} & &\textbf{improvement}\\ 
			\hline														
			Finetune           &  61.31  &  27.22  &  16.37  &   6.08  &   2.54  &   1.56  &   1.93  &   2.60  &   1.40  &  59.91  &  \textbf{+38.66}  \\
			iCaRL~\cite{rebuffi2017icarl}            &  61.31  &  46.32  &  42.94  &  37.63  &  30.49  &  24.00  &  20.89  &  18.80  &  17.21  &  44.10  &  \textbf{+22.85}  \\
			EEIL~\cite{castro2018end}              &  61.31  &  46.58  &  44.00  &  37.29  &  33.14  &  27.12  &  24.10  &  21.57  &  19.58  &  41.73  & \textbf{+20.48}   \\
			Rebalancing~\cite{hou2019learning}        &  61.31  &  47.80  &  39.31  &  31.91  &  25.68  &  21.35  &  18.67  &  17.24  &  14.17  &  47.14  &  \textbf{+25.89}  \\
			TOPIC~\cite{tao2020few}              &  61.31  &  50.09  &  45.17  &  41.16  &  37.48  &  35.52  &  32.19  &  29.46  &  24.42  &  36.89  &  \textbf{+15.64}  \\
			Decoupled-Cosine~\cite{vinyals2016matching}   &  70.37  &  65.45  &  61.41  &  58.00  &  54.81  &  51.89  &  49.10  &  47.27  &  45.63  &  24.74  &  \textbf{+3.49}  \\
			Decoupled-DeepEMD~\cite{zhang2020deepemd}  &  69.77  &  64.59  &  60.21  &  56.63  &  53.16  &  50.13  &  47.79  &  45.42  &  43.41  &  26.36  &  \textbf{+5.11}  \\
			F2M~\cite{shi2021overcoming}                &  67.28  &  63.80  &  60.38  &  57.06  &  54.08  &  51.39  &  48.82  &  46.58  &  44.65  &  22.63  &  \textbf{+1.38}  \\
			CEC~\cite{zhang2021few}                &  72.00  &  66.83  &  62.97  &  59.43  &  56.70  &  53.73  &  51.19  &  49.24  &  47.63  &  24.37  &  \textbf{+3.12}  \\
			LIMIT~\cite{zhou2022few}             &  72.32  &  68.47  &  64.30  &  60.78  &  57.95  &  55.07  &  52.70  &  50.72  &  49.19  &  23.13  &  \textbf{+1.85}  \\
			FACT~\cite{zhou2022forward}               &  72.56  &  69.63  &  66.38  &  62.77  &  60.60  &  57.33  &  54.34  &  52.16  &  50.49  &  22.07  &  \textbf{+0.82}  \\
			\hline
			SCGN       	   &  \textbf{73.25}  &  \textbf{71.57}  &  \textbf{67.46}  &  \textbf{64.01}  &  \textbf{61.04}  &  \textbf{58.41}  &  \textbf{55.62}  &  \textbf{53.62}  &  \textbf{52.00}  &  \textbf{21.25}  &    \\
			% \hline
			\toprule																
	\end{tabular}}	
%	\vspace{-15pt}														
\end{table*}

\subsection{Implementation Details}
\noindent
\textbf{Dateset}: We evaluate on MiniImageNet, CUB200-2011 and CIFAR100.
MiniImageNet is a subset of ImageNet with 100 classes. CUB200-2011 is a fine-grained image classification task with 200 classes.
CIFAR100 contains 60,000 images from 100 classes. 

\noindent
\textbf{Dateset Split}: For MiniImageNet and CIFAR100, 100 classes are divided into 60 base classes and 40 new classes. 
The new classes are formulated into eight 5-way 5-shot incremental tasks.
For CUB200, 200 classes are divided into 100 base classes and 100 incremental classes, and the new classes are formulated into ten 10-way 5-shot incremental tasks. 

\noindent
\textbf{Compared methods}: We compare to classical CIL methods iCaRL~\cite{rebuffi2017icarl}, EEIL~\cite{castro2018end},
and Rebalancing~\cite{hou2019learning}. Besides, we also compare to current
SOTA FSCIL algorithms: TOPIC~\cite{tao2020few}, SPPR~\cite{zhu2021self},
Decoupled-DeepEMD/Cosine/NegCosine~\cite{liu2020negative,vinyals2016matching,zhang2020deepemd}, CEC~\cite{zhang2021few}, LIMIT~\cite{zhou2022few}
and FACT~\cite{zhou2022forward}. 
% We report the baseline method by finetuning the model with few-shot instances as ‘finetune’.

\noindent
\textbf{Training details}: All methods are implemented with Pytorch. 
% We use the same network backbone for all compared methods.
For CIFAR100, we use ResNet20, while for others we use ResNet18. We optimize with SGD+momentum, and the learning rate is set to 0.1 and decays with cosine annealing.

\noindent
\textbf{Evaluation Protocol}: We evaluate models after each session on the test set $ Z^{i} $ and report the Top 1 accuracy.
We also use a performance dropping rate(\textbf{PD}) that measures the absolute accuracy drops in the last session w.r.t.
the accuracy in the first session,\ie, $ \text{PD} =  \mathcal{A}_{0} - \mathcal{A}_{N} $, 
where $ \mathcal{A}_{0} $ is the classification accuracy in the base session 
and $ \mathcal{A}_{N} $ is the accuracy in the last session.



\begin{figure}[t]
	\centering
	\includegraphics[width=1.\linewidth]{task_figures.pdf}
%	\vspace{-15px}
	\caption{The accuracy of each session in MiniImageNet dataset is in the FSCIL task learning process.
	SCGN demonstrates superior performance in few-shot tasks within FSCIL tasks compared to other methods.}
	\label{fig:task_figures}
%	\vspace{-15px}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=1.\linewidth]{ablation.pdf}
%	\vspace{-15px}
	\caption{Ablation study on MiniImageNet, CIFAR100 and CUB-200-2011. Every part in SCGN improves the performance of FSCIL.}
	\label{fig:ablation}
%	\vspace{-15px}
\end{figure}

%\vspace{-15px}
\subsection{Major Comparison}
We report the performance over benchmark datasets in Fig.~\ref{fig:comparison}.
We can infer from Fig.~\ref{fig:comparison} that SCGN consistently outperforms the current SOTA method, \ie, FACT~\cite{zhou2022forward} on benchmark datasets.
We also report the detailed value on MiniImageNet dataset in Table~\ref{lab:mini}. 
The performance of SCGN method is higher than that of other methods in each session, and the performance dropping rate is lower than that of other methods. 
% It can be seen that SGPN is more effective in FSCIL.
The poor performance of CIL method (such as iCaRL) indicates that the method of a large number of sample tasks is not suitable for FSL tasks.
SCGN has better performance than Decoupled-DeepEMD/Cosine/NegCosine~\cite{liu2020negative,vinyals2016matching,zhang2020deepemd}, CEC~\cite{zhang2021few} and FACT~\cite{zhou2022forward}. 
It reveals that in FSCIL, it is important to make FSL tasks be trained well which strengthens new task constraints to reduce the impact on old tasks.
As shown in Fig.~\ref{fig:task_figures}, We compared the accuracy of each session on the MiniImageNet dataset with the CEC~\cite{zhang2021few} and FACT~\cite{zhou2022forward} methods.
It can be seen from the figure that in the FSCIL task learning process, the performance of each session is higher than that of other methods. 
This further proves the superiority of SCGN method.


%\begin{table}[h]
%	\centering
%	\vspace{-0px}
%	% \caption{Ablation study of miniImageNet and CIFAR100.}\label{lab:mini}
%	\resizebox{1.\linewidth}{!}{
%		\begin{tabular}{l cccccccccc cc  }
%			\bottomrule
%			\multirow{2}{*}{\textbf{Methods}} &\multicolumn{9}{c}{\textbf{Accuracy in each session(\%)} \red{miniImageNet$\uparrow$} and \red{CIFAR100$\downarrow$}}  & \multirow{2}{*}{\textbf{PD}}  \\
%			\cline{2-10}
%			&\textbf{0} &\textbf{1}  &\textbf{2}&\textbf{3} &\textbf{4}&\textbf{5}  &\textbf{6}&\textbf{7}&\textbf{8} & \\ 
%			\hline	
%			% \red{miniImageNet}		 \\										
%			baseline           &  69.17  &  63.74  &  59.13  &  55.13  &  51.63  &  48.44  &  45.67  &  43.58  &  41.74  &  27.43    \\
%			baseline+SGN       &  71.68  &  68.95  &  65.67  &  62.36  &  59.08  &  55.80  &  52.98  &  51.18  &  49.42  &  22.26    \\
%			baseline+CGN       &  71.50  &  67.75  &  63.64  &  60.43  &  57.00  &  53.85  &  51.11  &  49.19  &  47.81  &  23.69    \\
%			baseline+SGN+CGN   &  73.25  &  71.57  &  67.86  &  64.71  &  61.54  &  58.91  &  55.72  &  53.82  &  52.20  &  21.25    \\
%			\hline
%			% \red{CIFAR100}		 \\	
%			baseline           &  69.75  &  65.06  &  61.20  &  57.21  &  53.88  &  51.40  &  48.80  &  46.84  &  44.41  &  25.34    \\
%			baseline+SGN       &  73.07  &  68.88  &  65.26  &  61.19  &  58.09  &  55.57  &  53.22  &  51.34  &  49.14  &  23.93    \\
%			baseline+CGN       &  73.81  &  71.09  &  66.87  &  62.89  &  59.70  &  56.77  &  54.67  &  52.52  &  50.23  &  23.58    \\
%			baseline+SGN+CGN   &  75.15  &  73.07  &  68.31  &  64.61  &  61.94  &  59.41  &  57.62  &  55.62  &  53.19  &  21.96    \\
%			% \hline
%			\toprule																
%	\end{tabular}}	
%	\vspace{-10pt}														
%\end{table}
%\begin{table}[h]
%	\centering
%	% \caption{Ablation study of CUB200-2011.}\label{lab:cub}
%	\resizebox{1.\linewidth}{!}{
%		\begin{tabular}{l cccccccccc cc  }
%			\bottomrule
%			\multirow{2}{*}{\textbf{Methods}} &\multicolumn{11}{c}{\textbf{Accuracy in each session(\%)} \red{CUB200-2011}}  & \multirow{2}{*}{\textbf{PD}}  \\
%			\cline{2-12}
%			&\textbf{0} &\textbf{1}  &\textbf{2}&\textbf{3} &\textbf{4}&\textbf{5}  &\textbf{6}&\textbf{7}&\textbf{8}&\textbf{9}&\textbf{10} & \\ 
%			\hline	
%			baseline           &  68.68  &  62.85  &  58.43  &  53.68  &  51.19  &  47.88  &  45.65  &  44.07  &  41.17  &  40.63  &  38.33  &  30.35  \\
%			baseline+SGN       &  74.35  &  70.69  &  66.68  &  62.34  &  59.76  &  56.54  &  54.61  &  52.52  &  50.73  &  49.20  &  47.60  &  26.75  \\
%			baseline+CGN       &  74.85  &  71.94  &  68.50  &  63.50  &  62.43  &  58.27  &  57.73  &  55.81  &  54.83  &  53.52  &  52.28  &  22.57  \\
%			baseline+SGN+CGN   &  75.92  &  73.57  &  71.67  &  68.01  &  66.94  &  63.61  &  62.22  &  61.42  &  59.79  &  58.56  &  57.46  &  18.46    \\
%			
%			% \hline
%			\toprule																
%	\end{tabular}}	
%	\vspace{-10pt}														
%\end{table}


\begin{figure}[t]
	\centering
	\includegraphics[width=1\linewidth]{tsne.pdf}
%	\vspace{-20px}
	\caption{Visualization of decision boundary of training set and test set on CUB200-2011.
	\red{Circles represent sample features, stars represent class-level features, and different colors represent different categories.}}
	\label{fig:tsne}
%	\vspace{-15px}
\end{figure}

\subsection{Ablation Study}
We analyze the importance of each component of SCGN on MiniImageNet, CIFAR100 and CUB-200-2011 dataset in Fig.~\ref{fig:ablation}.
We separately construct models with different combinations of the core elements in SCGN.
Baseline represents that the backbone network is used to directly learn FSCIL tasks.
% The baseline+SGN experiment is to learn the FSL task randomly sampled from the base task, which means that no CGN module is used.
% The baseline+CGN experiment removes the sample-level graph network, 
% and the sampled FSL task calibrates the old class and the new class directly through the class mean.
From Fig.~\ref{fig:ablation} we can infer that the use of CGN module effectively alleviates the catastrophic forgetting of incremental learning of baseline in FSCIL tasks.
The use of SGN module improves the learning performance of the FSL task, 
and significantly improves the overall performance of each session, which also proves the importance of the training of FSL tasks.
The combination of the two modules not only improves the learning performance of FSL tasks but also takes into account the semantic conflict between the old class and the new class due to data imbalance and other reasons. 
% The performance of the FSCIL task has been further improved.
The ablation experiments validate that SGN and CGN modules are helpful for FSCIL tasks.

\subsection{Visualization of Incremental Session}
We visualize the learned decision boundaries with t-SNE on CUB-200-2011 dataset in Fig~\ref{fig:tsne}.
Fig.~\ref{fig:tsne}(a) stands for the decision boundary of the training set, where we train five old classes and three classes with few samples.
The circle represents the embedded space of the sample, and the star represents the class-level prototype.
We can find that a few samples of the new class are clustered, because the SGN learns more refined features through the association between samples.
In addition, CGN calibrates the categories with close similarity through the connection between the old class and the new class. 
It can be seen from the visualization that the class-level characteristics of the old class and the new class remain distinguishable
Fig.~\ref{fig:tsne}(b) tests the trained FSCIL task on the test set. 
It can be seen that SCGN helps to adapt the prototype and calibrate the decision boundary between old and new classes.









%\begin{table*}[h]
%	\centering
%	\caption{Comparison with the state-of-the-art on MiniImageNet dataset.}\label{lab:CUB}
%	\resizebox{1\linewidth}{!}{
	%		\begin{tabular}{l cccccccccccc cc  }
		%			\toprule
		%			\multirow{2}{*}{\textbf{Methods}} &\multicolumn{11}{c}{\textbf{Accuracy in each session(\%)}}  & \multirow{2}{*}{\textbf{PD}} & \multicolumn{1}{c}{\textbf{Our relative}} \\
		%			\cline{2-12}
		%			& \textbf{0} &\textbf{1}  &\textbf{2}&\textbf{3} &\textbf{4}&\textbf{5}  &\textbf{6}&\textbf{7}&\textbf{8}&\textbf{9}&\textbf{10}  & &\textbf{improvement}\\ 
		%			\hline														
		%			Finetune           &  68.68  &  43.70  &  25.05  &  17.72  &  18.08  &  16.95  &  15.10  &  10.06  &   8.93  &   8.93  &   8.47  &  60.21  &  +  \\
		%			iCaRL              &  68.68  &  52.65  &  48.61  &  44.16  &  36.62  &  29.52  &  27.83  &  26.26  &  24.01  &  23.89  &  21.16  &  47.52  &  +  \\
		%			EEIL               &  68.68  &  53.63  &  47.91  &  44.20  &  36.30  &  27.46  &  25.93  &  24.70  &  23.95  &  24.13  &  22.11  &  46.57  &  +  \\
		%			Rebalancing        &  68.68  &  57.12  &  44.21  &  28.78  &  26.71  &  25.66  &  24.62  &  21.52  &  20.12  &  20.06  &  19.87  &  48.81  &  +  \\
		%			TOPIC              &  68.68  &  62.49  &  54.81  &  49.99  &  45.25  &  41.40  &  38.35  &  35.36  &  32.22  &  28.31  &  26.26  &  42.40  &  +  \\
		%			SPPR               &  68.68  &  61.85  &  57.43  &  52.68  &  50.19  &  46.88  &  44.65  &  43.07  &  40.17  &  39.63  &  37.33  &  31.35  &  +  \\
		%			Decoupled-NetCosine&  74.96  &  70.57  &  66.62  &  61.32  &  60.09  &  56.06  &  55.03  &  52.78  &  51.50  &  50.08  &  48.47  &  26.49  &  +  \\
		%			Decoupled-Cosine   &  75.52  &  70.95  &  66.46  &  61.20  &  60.86  &  56.88  &  55.40  &  53.49  &  51.94  &  50.93  &  49.31  &  26.21  &  +  \\
		%			Decoupled-DeepEMD  &  75.35  &  70.69  &  66.68  &  62.34  &  59.76  &  56.54  &  54.61  &  52.52  &  50.73  &  49.20  &  47.60  &  27.75  &  +  \\
		%			CEC                &  75.85  &  71.94  &  68.50  &  63.50  &  62.43  &  58.27  &  57.73  &  55.81  &  54.83  &  53.52  &  52.28  &  23.57  &  +  \\
		%			FACT               &  75.90  &  73.23  &  70.84  &  66.13  &  65.56  &  62.15  &  61.74  &  59.83  &  58.41  &  57.89  &  56.94  &  18.96  &  +  \\
		%			\hline
		%			SCGN 		       &  75.92  &  73.57  &  71.67  &  68.01  &  66.94  &  63.61  &  62.22  &  61.42  &  59.79  &  58.56  &  57.46  &  18.46  &  +  \\
		%			\bottomrule																
		%	\end{tabular}}	
%	\vspace{-15pt}														
%\end{table*}


%\begin{table*}[h]
%	\centering
%	\caption{Comparison with the state-of-the-art on CIFAR100 dataset.}\label{lab:cif}
%	\resizebox{1\linewidth}{!}{
%			\begin{tabular}{l cccccccccc cc  }
%					\toprule
%					\multirow{2}{*}{\textbf{Methods}} &\multicolumn{9}{c}{\textbf{Accuracy in each session(\%)}}  & \multirow{2}{*}{\textbf{PD}} & \multicolumn{1}{c}{\textbf{Our relative}} \\
%					\cline{2-10}
%					& \textbf{0} &\textbf{1}  &\textbf{2}&\textbf{3} &\textbf{4}&\textbf{5}  &\textbf{6}&\textbf{7}&\textbf{8} & &\textbf{improvement}\\ 
%					\hline														
%					Finetune           &  64.10  &  39.61  &  15.37  &   9.80  &   6.67  &   3.80  &   3.70  &   3.14  &   2.65  &  61.45  &  +  \\
%					iCaRL              &  64.10  &  53.38  &  41.69  &  34.13  &  27.93  &  25.06  &  20.41  &  15.48  &  13.73  &  50.37  &  +  \\
%					EEIL               &  64.10  &  53.11  &  43.71  &  35.15  &  28.96  &  24.98  &  21.01  &  17.26  &  15.85  &  48.25  &  +  \\
%					Rebalancing        &  64.10  &  53.05  &  43.96  &  36.97  &  31.61  &  26.73  &  21.23  &  16.78  &  13.54  &  50.56  &  +  \\
%					TOPIC              &  64.10  &  55.88  &  47.07  &  45.16  &  40.11  &  36.38  &  33.96  &  31.55  &  29.37  &  34.73  &  +  \\
%					Decoupled-NetCosine&  74.36  &  68.23  &  62.84  &  59.24  &  55.32  &  52.88  &  50.86  &  48.98  &  46.66  &  27.70  &  +  \\
%					Decoupled-Cosine   &  74.55  &  67.43  &  63.63  &  59.55  &  56.11  &  53.80  &  51.68  &  49.67  &  47.68  &  26.87  &  +  \\
%					Decoupled-DeepEMD  &  69.75  &  65.06  &  61.20  &  57.21  &  53.88  &  51.40  &  48.80  &  46.84  &  44.41  &  25.34  &  +  \\
%					F2M                &  64.71  &  62.05  &  59.01  &  55.58  &  52.55  &  49.96  &  48.08  &  46.28  &  44.67  &  20.04  &  +  \\
%					CEC                &  73.07  &  68.88  &  65.26  &  61.19  &  58.09  &  55.57  &  53.22  &  51.34  &  49.14  &  23.93  &  +  \\
%					LIMIT              &  73.81  &  72.09  &  67.87  &  63.89  &  60.70  &  57.77  &  55.67  &  53.52  &  51.23  &  22.58  &  +  \\
%					FACT               &  74.60  &  72.09  &  67.56  &  63.52  &  61.38  &  58.36  &  56.28  &  54.24  &  52.10  &  22.50  &  +  \\
%					\hline
%					DMHN 		       &  75.15  &  73.07  &  68.31  &  64.61  &  61.94  &  59.41  &  57.62  &  55.62  &  53.19  &  21.96  &  +  \\
%					\bottomrule																
%			\end{tabular}}	
%	\vspace{-15pt}														
%\end{table*}




