
\begin{figure*}[t]
	\centering
	\includegraphics[width=1.\linewidth]{framework.pdf}
	\caption{Our incremental prototype learning scheme for few-shot class-incremental learning.
		(a) an overview of the SCGN framework
		(b) Sample-Level Graph Network(SGN)
		(c) Class-Level Graph Network(CGN)}
	\label{fig:framework}
%	\vspace{-10px}
\end{figure*}



\section{Method}

% We detail the incremental prototype learning scheme and its important components in this section.
% As shown in Fig.~\cred{\ref{fig:framework}},
% First of all, we demonstrate the paradigms of standard learning and our proposed 
% incremental prototype learning, respectively.
% Then two core components sample-level graph network(SGN) and 
% class-level graph network(CGN) are introduced.

% \section{Problem Description}
\subsection{Problem Description and Pretraining}
\noindent
\textbf{Problem Description}.
% The few-shot class-incremental learning (FSCIL) problem is defined as follows. 
We denote $ X $, $ Y $ and $ Z $ as the training set, the label set and the test set, respectively.
FSCIL task is to train a model from a continuous data stream in a class-incremental form, \ie, training sets  $ X^{0}, X^{1}, \dots X^{n} $, where samples of a set $ X^{i} $ are from the label set $ Y^{i} $,
and $ n $ represents the incremental session.
The incremental classes are disjoint, \ie, $ Y^{i} \bigcap Y^{j} = \varnothing $ for $i \neq j$.
For the base session, $ X^{0} $ has sufficient samples.
For each class in the subsequent sessions, we have only a few samples (e.g., 5 samples).
% which is consistent with the embodiment of FSCIL.
To measure a FSCIL model, we calculate the classification accuracy on the test set $ Z^{i} $ at each session $ i $.

\noindent
\textbf{Pretraining}. 
First, we will pretrain on the base dataset to obtain the class-level feature graph of the base session.
\red{In this case, the input of the model is only the query image $ Q $ to be predicted.}
We train a feature extractor $ f_{e} $ parameterized by $ \theta_{e} $ with a fully-connected layer as the classifier 
by minimizing the standard cross-entropy loss using the training samples of $ X^{0} $ under the supervision of target label $ T $.
We measure the relationship between the representation and the learnable class-level features $ \theta_{p} $
for all classes $ d(f_{e}(Q),\theta_{p}) $, $ d(\cdot,\cdot) $ represents the similarity measure, and we use the cosine similarity.
The pre-training optimization function can be expressed as:
\begin{equation}
	\theta_{\ast} = arg  \min\limits_{\theta} L(d(f_{e}(Q),\theta_{p}),T).
\end{equation}
Here $ \theta $ include above $ \theta_{e} $ and $ \theta_{p} $, $ L $ represents cross-entropy loss function.
To boost the ability of learning new classes in future tasks, we design a pseudo-incremental training paradigm at the base training based on meta-learning, which make the model learn how to learn a new class from a few samples.

%For the training process of the base classes $Y^{0}$, standard classification pipeline is adopted.
%In this case, the input of the model is only the query image $ Q $ to be predicted.
%Then a base feature extractor $ f_{e} $ parameterized 
%by $ \theta_{e} $ is utilized to learn the corresponding representation.
%Then, we measure the relationship between the representation and the learnable class-level features $ \theta_{p} $
%for all classes $ P(f_{e}(Q),\theta_{p}) $.
%Our task is to randomly sample query images from the dataset, train and optimize $ \theta $,
%thus minimizing the loss function $ L $ under the supervision of target label $ T $:
%\begin{equation}
%	\theta_{\ast} = arg  \min\limits_{\theta} L(P(f_{e}(Q),\theta_{p}),T).
%\end{equation}
%Here $ \theta $ include above $ \theta_{e} $ and $ \theta_{p} $.
%In classification tasks, $ L $ usually represents cross-entropy loss function.


%\subsection{\lyu{Standard Learning Paradigm}}
%For the training process of the base classes $Y^{0}$, standard classification pipeline is adopted.
%In this case, the input of the model is only the query image $ Q $ to be predicted.
%Then a base feature extractor $ f_{e} $ parameterized 
%by $ \theta_{e} $ is utilized to learn the corresponding representation:
%\begin{equation}
%	R_{q} = f_{e}(Q;\theta_{e}).
%\end{equation}
%Finally, we measure the relationship between the representation and the learnable prototypes $ \theta_{p} $
%for all classes.
%\begin{equation}
%	S = \text{softmax}(\text{cos}(R_{q}, \theta_{p})).
%\end{equation}
%where $ \text{cos}(\cdot,\cdot) $ is the cosine similarity. 
%The above formula can be written as:
%\begin{equation}
%	S_{i} = \frac{ \text{exp}(\eta((\theta_{p}^{i})^{T} \cdot R_{q})) }
%	{ \sum_{j}\text{exp}(\eta((\theta_{p}^{j})^{T} \cdot R_{q}))  }.
%\end{equation}
%where $ i $ is the calculated class, $ \eta $ is the scale factor, 
%and $ \cdot $ represents the operation of inner product.
%It is worth noting that all feature representations are normalized.
%Our task is to randomly sample query images from the dataset, train and optimize $ \theta $,
%thus minimizing the loss function $ L $ under the supervision of target label $ T $:
%\begin{equation}
%	\theta_{\ast} = arg  \min\limits_{\theta} L(S_{i},T).
%\end{equation}
%Here $ \theta $ include above $ \theta_{e} $ and $ \theta_{p} $.
%In classification tasks, $ L $ usually represents cross-entropy loss function.
%
%\subsection{Incremental Prototype Learning}
%Due to the lack of extensibility of the representation obtained from standard learning, 
%and the limited learning ability for few-shot learning tasks,
%we propose an incremental prototype learning scheme.
%There are two important components in the scheme as follows.


\subsection{Two-level Graph Network (SCGN)}
\noindent
\textbf{Pseudo incremental learning}.
In the FSCIL task, the model should have the ability to adapt to new classes of knowledge and expand to new knowledge.
However, it is difficult to have the ability with only a few samples.
Therefore, we simulated the FSCIL learning situation and designed a pseudo-incremental learning paradigm in the base session to enhance the model's ability to adapt to new FSL tasks.
Specifically, we randomly sample two N-way K-shot (N classes, K samples for each class) FSL tasks, \ie, $ C_{1} $ and $ C_{2} $, from the base training set $ X^{0} $ in each iteration and we have $  Y^{c_{1}} \bigcap Y^{c_{2}} = \varnothing $.
These two FSL tasks serve as base tasks in the pseudo-incremental process. 

Motivated by~\cite{verma2019manifold}, we fuse instances by manifold mixup and treat the fused instances as virtual incremental classes.
We decouple the embedding into two parts at the hidden layer $ f_{e}(x) = g(h(x)) $.
We fuse two FSL tasks to generate a new virtual FSL task $ C_{3} $ :
\begin{equation}
	r_{i}^{c_{3}} = \sum_{i}\nolimits^{NK}g[\lambda h(x_{i}^{c_{1}}) + (1 - \lambda)h(x_{i}^{c_{2}})],
\end{equation}
where $ \lambda \in [0,1] $ is sampled from Beta distribution, 
$ r_{i}^{c_{3}} $ represents the features of the sample in the virtual FSL task.
The pseudo-incremental learning paradigm needs to enable two-level graph network to build graph relationships among samples and classes in FSCIL. 

\noindent
\textbf{Sample-Level Graph Network (SGN)}.
%\subsection{Sample-Level Graph Network (SGN)}
As shown in Fig.~\ref{fig:framework}, we obtained class-level graph of base task through pretraining.
Then, we introduce the Sample-level Graph Network (SGN) to learn the FSL task.
SGN aggregates samples of the same class and distinguishes samples of different classes by exploring the relationship between a few samples, so as to mine refined class-level features.
This not only improves the performance of FSL tasks, but also increases the extensibility of feature representations.
The formula for the relationship between samples in each FSL task is as follows:
\begin{equation}
	e_{ij}^{c} = f_{r}( (r_{i}^{c} - r_{j}^{c} )^2 ),
\end{equation}
where $ r_{i}^{c}, r_{j}^{c} $ respectively represents the sample features of the $i$-th and $j$-th FSL task $ C $,
$  C \in \left \{ C_{1},C_{2} \right \} $ and 
$ f_{r} $ is the encoding network that transforms the instance similarity to a certain scale.
$ f_{r} $ contains two Conv-BN-ReLU blocks. 
We update the sample representation through the relationship parameters between samples.
The obtained embeddings are averaged for each class as a class-level feature:
\begin{equation}
	R_{s}^{c} = \text{mean}(\text{SGN}(r_{i}^{c} + \sum_{j}\nolimits^{NK} e_{ij}^{c} \cdot r_{j}^{c} )) ,
\end{equation}
where $ \text{SGN} $ is the aggregation network with parameter set $ \theta_{s} $.

%We randomly sample two N-way K-shot FSL tasks in each iteration, 
%and attempt to generate new classes by instance mixture as a virtual incremental few-shot learning task.
%Specificly, in addition to the above query image $ Q $, the input of the model contains two randomly selected
%N-way K-shot collection $ C_{1} $ and $ C_{2} $ from the base training set $ X^{0} $,
%note that $  Y^{c_{1}} \bigcap Y^{c_{2}} = \varnothing $.

%Motivated by the intuition that interpolations between two different clusters are often regions of low-confidence
%predictions~\cite{verma2019manifold}, we seek of fuse instances by manifold mixup and treat the fused instances as a virtual new class.
%We decouple the embedding into two parts at the middle layer: $ f_{e}(x) = g(h(x)) $.
%We fuse two FSL tasks to generate a new virtual FSL task with collection $ C_{3} $ :
%\begin{equation}
%	r_{i}^{c_{3}} = \sum_{i}^{NK}g[\lambda h(x_{i}^{c_{1}}) + (1 - \lambda)h(x_{i}^{c_{2}})].
%\end{equation}
%where $ \lambda \in [0,1] $ is sampled from Beta distribution.

%In order to improve the SGN model's learning ability on few shot tasks, 
%we align the prototype features learned by $ C_1 $ and $ C_2 $ few shot tasks with the prototype features $ \theta_{p} $.
%\begin{equation}
%	S_s = P( (R_{s}^{c_1} \bigcup R_{s}^{c_2} ),\theta_{p}).
%\end{equation}

\noindent
\textbf{Class-Level Graph Network (CGN)}.
%\subsection{Class-Level Graph Network (CGN)}
In the process of FSCIL incremental learning, the model should adjust itself with the new FSL task and perform well on old task.
However, the incremental model is optimized on the many-shot old classes, which is tailored to depict old classes' features.
As a result, there exists a semantic gap between the old classifiers and extracted new classes prototypes.
To solve the semantic gap between the old class and the new class,
we introduce the Class-level Graph Network (CGN).

%Motivated by the intuition that interpolations between two different clusters are often regions of low-confidence
%predictions~\cite{verma2019manifold}, we seek of fuse instances by manifold mixup and treat the fused instances as a virtual new class.
%We decouple the embedding into two parts at the middle layer: $ f_{e}(x) = g(h(x)) $.
%We fuse two FSL tasks to generate a new virtual FSL task $ C_{3} $ :
%\begin{equation}
%	r_{i}^{c_{3}} = \sum_{i}^{NK}g[\lambda h(x_{i}^{c_{1}}) + (1 - \lambda)h(x_{i}^{c_{2}})].
%\end{equation}
%where $ \lambda \in [0,1] $ is sampled from Beta distribution.


\begin{figure*}[t]
	\centering
	\includegraphics[width=1\linewidth]{compartion1.pdf}
%	\vspace{-25px}
	\caption{Comparison of our classification results with other methods on MiniImageNet, CIFAR100 and CUB200-2011.
	From the experimental results, it can be seen that SCGN outperforms the state-of-the-art (SOTA) methods.}
	\label{fig:comparison}
%	\vspace{-15px}
\end{figure*}


CGN should reflect the context relationship between old and new classes, 
so as to adjust the embedding space of prototype features of new classes in the class-level graph.
In our implementation, we combine the Transformer~\cite{vaswani2017attention} with the GNN.
Specifically, we use the multi-head attention mechanism to construct the relationship between the old class and the new class, 
and use the GNN to aggregate these information to calibrate the prototype features of the new class.
Transformer is a store of triplets in the form of (query $ Q $, key $ K $ and value $ V $). We set these parameters to
$ V =  [\theta_{p}, R_{s}^{c_{3}}] $,
$ K = W_{k}^{T} R_{s}^{c_{3}} $,
$ Q = W_{q}^{T} V $.
$ R_{s}^{c_{3}} $ is class-level features obtained by SGN learning virtual FSL task $ C_{3} $.
$ W_{k}$ and $ W_{q} $ are the learnable parameter of linear projection function.
The class-level features formula after CGN calibrating FSL $ C_{3} $ is as follows:
\begin{equation}
	\widetilde{R_{s}}^{c_{3}}  = \text{CGN} ( R_{s}^{c_{3}} + \sum\nolimits_{k} \alpha_{kq} \cdot V_{k} ) .
\end{equation}
where $ \alpha_{kq}  \propto \text{exp}( \frac{K Q^{T}}{\sqrt{d} } )  $ 
represents the association weight between the old class features and the new class features,
$ \text{CGN} $ is the aggregation network with parameter set $ \theta_{c} $.
%We extend the newly learned class prototype to the learned classifier, and evaluate the new class:\
%\begin{equation}
%	S_c = P( r_{i}^{c_{3}} ,[\theta_{p}, \widetilde{R_{s}}^{c_{3}} ]).
%\end{equation}


\subsection{FSCIL Training using SCGN}
Fig.~\ref{fig:framework} shows the training schematic of SCGN.
SGN matches the class-level features after learning with the base class graph, 
which not only strengthens SGN's ability to learn FSL tasks but also reduces the interference to other classes. 
CGN extends the calibrated class-level features to the base class graph and predicts the virtual samples constructed. 
Ensure performance while mitigating interference to old classes.
%In this section, we introduce the objective function used in our framework.
%We align the FSL class-level features learned by SGN with the pretrained class-level features, 
%which can improve SGN's ability to learn FSL tasks and adapt to FSCIL tasks.
We define the following loss function to learn SGN:
\begin{equation}
	\mathcal{L}_{1} = - \sum\nolimits_{i}^{2N}\text{cos}(\text{tanh}(R_{s}^{c_1} \cup R_{s}^{c_2}),\text{tanh}(\theta_{p})).
\end{equation}
% where $ tanh(\cdot) $ is the hyperbolic tangent function and $ \text{cos}(\cdot, \cdot) $ is the cosine similarity.
With continuous optimization, sample features of the same class in the base dataset will become more compact.
To keep the distinction between the new class and the old class, we define the following loss function to learn CGN:
\begin{equation}
	\mathcal{L}_{2} = L(d( r_{i}^{c_{3}} ,[\theta_{p}, \widetilde{R_{s}}^{c_{3}} ]),T),
\end{equation}
where $ [\cdot] $ denotes the concatenation operation, $ L $ represents cross-entropy loss function,
$ T $ represents the label of the virtual sample constructed.


%\begin{equation}
%	\theta_{\ast} = arg  \min\limits_{\theta} L(S_s \cup S_c,\theta_{p}),T).
%\end{equation}
%Here $ \theta $ include above $ \theta_{e} $, $ \theta_{s} $ and $ \theta_{p} $.
%$ T $ represents target label.
%
%
% the optimization directions of the feature representations and class-level features have also been significantly changed.













