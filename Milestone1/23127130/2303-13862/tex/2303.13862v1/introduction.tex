\section{Introduction}


% With the rapid development of supervised learning, current learning systems have achieved or even surpassed human-level
% performances in many tasks~\cite{joseph2021incremental}. 
In the real world, artificial intelligence often receives novel classes~\cite{zhou2021learning,du2022agcn}.
%In the real world, artificial intelligence often receives novel classes~\cite{du2022agcn}.
When updating the model with new classes, a fatal problem occurs, namely catastrophic forgetting~\cite{rebuffi2017icarl,lyu2021multi,sun2022exploring}, \ie, the discriminability of old classes drastically declines.
To meet the adaptation to new knowledge, Class-Incremental Learning (CIL)~\cite{pham2021dualnet,zhao2022deep,wang2022learning} recognizes new classes and maintains discriminability over old classes, which has become an important research area.
% The trade-off between learning new classes and retaining old classes is calles the stability-plasticity dilemma.
% There are extensive efforts in resisting forgetting from different perspectives,
% e.g., Rehearsal-based methods~\cite{pham2021dualnet}, Architecture-based methods~\cite{zhao2022deep}, and Prompt-based method~\cite{wang2022learning}.
Most solutions to CIL problems are with abundant training samples.
However, in practical applications, the instance labeling and collection cost are sometimes unbearable, where the incremental class may have few samples. 
% Taking rare bird classification model as an sample, 
% we can only use a few captured images to train incremental models, 
% because they are difficult to collect. 
This CIL task with few training samples is called Few-Shot Class-Incremental Learning (FSCIL). 
Similar to CIL, learning new classes can lead to catastrophic forgetting of previous classes. 
In addition, due to the lack of new class instances, it is easy to observe the overfitting phenomenon on these limited inputs, 
which increases the learning difficulty of incremental tasks.

It is unwise to directly adopt CIL methods in FSCIL, where limited training samples result in serious overfitting and poor performance on old classes~\cite{tao2020few}.
% The problem is caused by the limited instances\textemdash 
% training few-shot instances will trigger the overfitting phenomenon and destroy the pre-trained feature embeddings.
% As a result, the destroyed embedding is unable to capture the characteristics of former classes, which further exacerbates catastrophic forgetting. 
In recent years, several works~\cite{zhang2021few,zhou2022forward} are designed for FSCIL, which classify FSL tasks through the class mean (prototype feature) to alleviate the problem of overfitting. 
However, these methods are difficult to distinguish few-shot classes well because only the prototype can hardly mine the latent semantic similarities and dissimilarities among a few samples.
% alleviate the overfitting problem of Few-Shot Learning (FSL), they ignore another problem of FSL\textemdash how to learn FSL well?
% Because the FSL task can only obtain information from few samples, it is difficult to effectively learn the FSL task, which further increases the challenges in FSCIL.
% Prototype features of few samples are difficult to distinguish samples of this category, so existing methods are difficult to achieve good performance on FSL tasks, which will also affect the overall performance of FSCIL.
In traditional FSL, Graph Neural Network (GNN) can express complex interactions between samples by performing feature aggregation from neighbors, and mining refined information from a few samples between support and query data. 
% Therefore, the use of GNN has great potential to solve the FSL problem.
However, unlike traditional FSL, the training data of FSCIL is incremental and in sequence, and the data of past classes is unavailable.
% It is difficult to build graph in the incremental scenario.
That is, FSCIL not only needs to solve the few-shot problem, 
but also needs to overcome the semantic interference cross tasks. 
The GNN used in traditional FSL cannot effectively solve these two problems at the same time.
% GNN-based FSL can not be directly used for FSCIL, 
% GNN-based FSL exploits the relationship between a support set and a query to predict query set data. 
%  After learning the few shot task, support set data is no longer accessed.
% Additionly, FSL tasks do not need to consider the relationship between classes across tasks.


\begin{figure}[t]
	\centering
	\includegraphics[width=1.\linewidth]{motivation.pdf}
%	\vspace{-25px}
	\caption{Illustration of our proposed two-level graph network for FSCIL. 
		Top: the setting of FSCIL.	
		Bottom: Sample-level to class-level graphs.
		\red{Square nodes represent sample-level features, and circular nodes represent class-level features.
		Sample-level features are learned through sample-level graph networks to obtain class-level features, 
		and class-level features are used to achieve class incremental learning through class-level graph networks.}
	}
%	\vspace{-15px}
	\label{fig:motivation}	
\end{figure}

%\lyu{However, the existing methods focus on the impact of the FSL tasks on the old tasks, ignoring the learning problems of the FSL tasks themselves. 
%% FSL tasks because of the small number of samples, 
%% the information obtained is far less than a large number of sample tasks. 
%Prototype features have limited information mining and poor learning performance for FSL tasks.}
%% Therefore, we should seek ways to better learn FSL tasks while suppressing catastrophic forgetting.

% Inspired by graph neural network (GNN), 
% graph network uses the association relationship between samples to mine deep level information in limited samples. 

In this paper, we propose a novel two-level graph network SCGN for FSCIL.
As shown in Fig.~\ref{fig:motivation}, the two levels are respectively Sample-level Graph network (SGN) and Class-level Graph Network (CGN).
% We name this framework SCGN.
% SGN explores the relationship between a few samples to obtain refined features, so as to obtain refined class-level features and improve the performance of FSL tasks.
% CGN learns the relationship between the old class and the new class, and calibrates similar classes to make different classes more distinguishable and reduce catastrophic forgetting.
\ch{
% so we propose a two-level graph network structure at the sample-level and class-level.
% Besides, FSCIL is a long-term learning process. 
% We should enhance the long-term learning ability of the model to make it consistent with real scenarios.
Specifically, we propose a pseudo incremental paradigm based on meta-learning to simulate FSCIL learning scenarios at the base training.
In the pseudo incremental process, we randomly sample FSL tasks from the base dataset, and generate virtual FSL tasks as new FSCIL tasks.
Then, in the process of meta-learning, SGN learns the FSL task, calculates the similarity between samples, gathers category samples, and distinguishes samples of different categories to obtain refined features.
Moreover, in order to alleviate the semantic gap between tasks, CGN calibrates the categories with the semantic gap according to the relationship 
between old classes and new classes to reduce the impact of new classes on old classes.
}
Experiments on benchmark datasets under various settings are conducted,
validating the effectiveness of our method.



%\lyu{In this paper, we propose a novel Sample-Prototype Graph Network(SPGN) structure for FSCIL to learn FSL tasks and calibrate semantic conflicts between old tasks and new tasks. }
%As shown in Fig.~\ref{fig:motivation}, we first pretrain the base dataset to obtain the prototype features of the base task.
%To \lyu{enable the learning of SPGN???}, we designed a pseudo-incremental learning paradigm, which samples FSL tasks from the base dataset.
%Each pseudo-FSCIL consists of two FSL tasks and a virtual FSL task generated by mixing two FSL tasks. 
%The FSL task is aligned to the pretrained prototype features through Sampel-level Graph Network(SGN) learning, 
%mining the information of a few samples to achieve the effect of learning a large number of samples. 
%Virtual FSL tasks, as pseudo incremental tasks, are calibrated between old tasks and new tasks through Prototype-level Graph Network(PGN). 
%While learning new tasks, they maintain the distinguishability of old classes.


%Our main contributions are as follows:
%\begin{itemize}
%	\item We propose a dual graph network structure of sample-level graph network and class-level graph network to learn the relationship between samples of FSL tasks and the relationship between classes between new and old tasks in FSCIL.
%	\item We designed a pseudo incremental process of FSL to improve the extensibility of the model. Through class-level graph network, we can more effectively distinguish known classes from virtual classes, and reduce the catastrophic forgetting problem in incremental learning.
%	\item Extensive experiments on benchmark CIFAR-100, MiniImageNet and CUB200 datasets demonstrate the superiority of our proposed method over the state-of-the-art.
%\end{itemize}


