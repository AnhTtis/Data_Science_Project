\paragraph{Empirical risk minimization (ERM)}
A supervised learning problem is specified by a joint distribution $\mathcal{D}$
over the feature space $\mathcal{X}$ and the label space $\mathcal{Y}$, and a
family $\mathcal{F}$ of mappings from $\mathcal{X}$ to $\mathcal{Y}$ (i.e. the
hypothesis set).
The goal is to find an $f\in\mathcal{F}$ that well predicts the label $y$ given the feature $\mlvec{x}$ in
expectation, for pairs of $(\mlvec{x}, y)\in\mathcal{X}\times \mathcal{Y}$ drawn $i.i.d.$
from the distribution $\mathcal{D}$.

Given a training set $\mathcal{S}=\{\mlvec{x}_{j}, y_{j}\}_{j=1}^{m}$ composed of $m$ pairs of
features and labels, we search for the optimal $f\in\mathcal{F}$ by the
\emph{empirical risk minimization} (ERM): let $\ell$ be a loss function
$\ell: \mathcal{Y} \times \mathcal{Y} \rightarrow \real$, ERM finds an $f\in \mathcal{F}$ that minimizes the average loss:
  $\min_{f\in\mathcal{F}}\frac{1}{m}\sum_{i=1}^{m}\ell(\hat{y}_{i}, y_{i}), \text{
  where }\hat{y}_{i} = f(\mlvec{x}_{i})$.
We focus on the common choice of the \emph{square loss} $\ell(\hat{y}, y) = \frac{1}{2}(\hat{y} - y)^{2}$.

\paragraph{Classical neural networks} A popular choice of the hypothesis
set $\mathcal{F}$ in modern-day machine learning is the \emph{classical neural networks}. A
vanilla version of the $L$-layer feed-forward neural network takes the form
$f(x; \mlvec{W}_{1}, \cdots, \mlvec{W}_{L})
  = \mlvec{W}_{L}\sigma(\cdots \mlvec{W}_{2}\sigma(\mlvec{W}_{1}\sigma(x))\cdots )$,
where $\sigma(\cdot)$ is a non-linear activation function, and for all $l\in[L]$, $\mlvec{W}_{l} \in \real^{d_{l}\times d_{l-1}}$ is the
weights in the $l$-th layer, with $d_{L} = 1$ and $d_{0}$ the same as the dimension
of the feature space $\mathcal{X}$. It has been shown that, in the limit
$\min_{l=1}^{L-1}d_{l} \rightarrow \infty$, the training of neural networks with
square loss is close to kernel learning, and therefore enjoys a linear
convergence rate
\citep{jacot2018neural, arora2019exact,
  allenzhu2019convergence, oymak2020toward}.
\paragraph{Quantum neural networks} Quantum neural networks is a family of
parameterized hypothesis set analogous to its classical counterpart. At a high level, it has the layered-structure like a
classical neural network. At each layer, a linear transformation acts on the
output from the last layer. A quantum neural network
is different from its classical counterpart in the following three aspects.

\paragraph{(1) Quantum states as inputs} A $d$-dimensional quantum
state is represented by a \emph{density matrix} $\mlvec{\rho}$, which is a
positive semidefinite $d\times d$ Hermitian with trace $1$. A state is said to
be pure if $\mlvec\rho$ is rank-$1$. Pure states can therefore be equivalently
represented by a state vector $\mlvec{v}$ such that $\mlvec\rho = \mlvec{v}\mlvec{v}^{\dagger}$.
The inputs to QNNs are quantum states. They can either be drawn as samples from a quantum-physical problem or be the encodings of classical feature vectors.

\paragraph{(2) Parameterization}
In classical neural networks, each layer is
composed of a linear transformation and a non-linear activation, and the matrix
associated with the linear transformation can be directly optimized at each entry. In
QNNs, the entries of each linear transformation can not be directly manipulated. Instead
we update parameters in a variational ansatz to update the
linear transformations. More concretely, a general $p$-parameter ansatz $\mlvec{U}(\mlvec\theta)$ in a
$d$-dimensional Hilbert space can be specified by a set of $d\times d$ unitaries
$\{\mlvec{U}_{0}, \mlvec{U}_{1}, \cdots, \mlvec{U}_{p}\}$ and a set of
non-zero $d\times d$ Hermitians $\{\generatorH{1}, \generatorH{2}, \cdots, \generatorH{p}\}$ as
\begin{align}
  & \mlvec{U}_p\exp(-i\theta_p\generatorH{p})\mlvec{U}_{p-1}\exp(-i\theta_{p-1}\generatorH{p-1})
  \cdots\exp(-i\theta_{2}\generatorH{2}) \mlvec{U}_1\exp(-i\theta_1\generatorH{1})\mlvec{U}_0. \label{eq:general-ansatz}
\end{align}
Without loss of generality, we  assume that $\tr(\generatorH{l}) = 0$. This
is because adding a Hermitian proportional to $\mlvec{I}$ on the generator
$\generatorH{l}$ does not change the density matrix of the output states.
Notice that most $p$-parameter ansatze
$\mlvec{U}:\real^{p}\rightarrow \complex^{d\times d}$ can be expressed as
Equation~\ref{eq:general-ansatz}. One exception may be the anastz design with
intermediate measurements (e.g. \citet{cong2019quantum}).
In Section~\ref{sec:theory-fast}, we will also consider the periodic
anastz:
\begin{definition}[Periodic ansatz]
  \label{def:partial-ansatz}
  A $d$-dimensional $p$-parameter periodic anasatz  $\mlvec{U}(\mlvec\theta)$ is defined as
  \begin{align}
    \mlvec{U}_p\exp(-i\theta_p\mlvec{H})\cdot \cdots \cdot \mlvec{U}_1\exp(-i\theta_1\mlvec{H})\mlvec{U}_0, \label{eq:partial-ansatz}
  \end{align}
  where $\mlvec{U}_l$ are sampled $i.i.d.$ with respect to the Haar measure over
  the special unitary group $SU(d)$, and $\mlvec{H}$ is a non-zero trace-$0$ Hermitian.
\end{definition}
Up to a unitary transformation, the periodic ansatz is equivalent to an ansatz
in Line~(\ref{eq:general-ansatz}) where $\{\generatorH{l}\}_{l=1}^{p}$
sampled as $\mlvec{V}_{l}\mlvec{H}\mlvec{V}_{l}^{\dagger}$ with $\mlvec{V}_{l}$
being haar random $d\times d$ unitary matrices.
Similar ansatze have been considered in
\citet{mcclean2018barren,anschuetz2022critical, you2021exponentially, ourvqe2022}.

\paragraph{(3) Readout with measurements} Contrary to classical neural networks, the readout from a QNN
requires performing quantum \emph{measurements}. A measurement is specified by a
Hermitian $\mlvec{M}$.
The outcome of measuring a quantum state $\mlvec\rho$ with a measurement
$\mlvec{M}$ is $\tr(\mlvec\rho \mlvec{M})$, which is a linear function of $\mlvec\rho$. A common choice is the Pauli measurement: Pauli matrices are $2\times 2$ Hermitians that are also unitary. The Pauli measurements are tensor products of Pauli matrices, featuring eigenvalues of $\pm 1$.

A common choice is the Pauli measurement: Pauli matrices are $2\times 2$ Hermitians that are also unitary:
    \begin{align*}
        \sigma_X=\begin{bmatrix}0 & 1\\ 1 & 0\end{bmatrix},
        \sigma_Y=\begin{bmatrix}0 & -\imagi\\ \imagi & 0\end{bmatrix},
        \sigma_Z=\begin{bmatrix}1 & 0\\ 0 & -1\end{bmatrix}.
    \end{align*}
The Pauli measurements are tensor products of Pauli matrices, featuring eigenvalues of $\pm 1$.
\paragraph{ERM of quantum neural network.}
We focus on quantum neural networks equipped with the mean-square loss. Solving the
ERM for a dataset
$\S:=\{(\mlvec\rho_{j}, y_{j})\}_{j=1}^{m}\subseteq (\complex^{d\times d}\times \real)^{m}$
involves optimizing the objective function
$\min_{\mlvec\theta}L(\mlvec\theta):=\frac{1}{2m}\sum_{j=1}^{m}\big(\hat{y}_{j}(\mlvec\theta) - y_{j}\big)^{2}$,
where
$\hat{y}_{j}(\mlvec\theta) =\tr(\mlvec\rho_{j}\mlvec{U}^{\dagger}(\mlvec\theta)\qnnmeasure\mlvec{U}(\mlvec\theta))$
for all $j\in[m]$ with $\qnnmeasure$ being the quantum measurement and $\mlvec{U}(\mlvec\theta)$ being the variational ansatz. Typically, a QNN is trained by optimizing the ERM
objective function by gradient descent: at the $t$-th iteration, the parameters
are updated as
$\mlvec{\theta}(t+1) \leftarrow \mlvec{\theta}(t) - \eta \nabla L(\mlvec{\theta}(t))$,
where $\eta$ is the learning rate; for sufficiently small $\eta$, the
dynamics of gradient descent reduces to that of the gradient flow:
$d\mlvec{\theta}(t)/dt=-\eta \nabla L(\mlvec{\theta}(t))$. Here we focus on the
gradient flow setting following \citet{arora2019exact}.

\paragraph{Rate of convergence} In the optimization literature, the rate of
convergence describes how fast an iterative algorithm approaches an (approximate) solution. For
a general function $L$ with variables $\mlvec\theta$, let $\mlvec\theta(t)$ be
the solution maintained at the time step $t$ and  $\mlvec\theta^{\star}$ be the
optimal solution. The algorithm is said to be converging
\emph{exponentially fast} or at a \emph{linear rate} if
  $L(\mlvec\theta(t)) - L(\mlvec\theta^{\star}) \leq
  \alpha\exp(-c t)$
for some constants $c$ and $\alpha$.
In contrast, algorithms with the sub-optimal gap
$L(\mlvec\theta(t)) - L(\mlvec\theta^{\star})$ decreasing slower than
exponential are said to be converging with a \emph{sublinear} rate
(e.g.  $L(\mlvec\theta(t)) - L(\mlvec\theta^{\star})$ decaying with $t$ as a
polynomial of $1/t$).
We will mainly consider the setting where
$L(\mlvec\theta^{\star}) = 0$ (i.e. the \emph{realizable} case) with continuous time $t$.

\paragraph{Other notations}
We use $\opnorm{\cdot}$,
$\fronorm{\cdot}$ and $\trnorm{\cdot}$ to denote the operator norm (i.e. the
largest eigenvalue in terms of the absolute values), Frobenius norm and the
trace norm of matrices; we use $\norm{\cdot}_{p}$ to denote the $p$-norm of
vectors, with the subscript omitted for $p=2$. We use $\tr(\cdot)$ to denote the trace operation.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main.tex"
%%% End:
