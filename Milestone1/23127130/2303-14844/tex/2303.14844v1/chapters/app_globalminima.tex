In this section, we present the proof for Theorem~\ref{thm:smallest_eig} for
characterizing the rate of convergence at global minima:
\asympeig*

We start by presenting a few helper lemma:
\subsection{Helper lemma for \texorpdfstring{$\Kasym$}{Kasym}}
\begin{lemma}
  \label{lm:hadamard}
  Let $\mlvec{A},\mlvec{B}$ be $d\times d$ Hermitians. Let $\opnorm{\cdot}$
  denote the operator norm of a given Hermitian and let $\circ$ denote the
  Hadamard product (i.e. the elementwise multiplication) of two matrices, we have
  \begin{align}
    \opnorm{\mlvec{A}\circ\mlvec{B}} \leq \opnorm{\mlvec{A}}\opnorm{\mlvec{B}}.
  \end{align}
\end{lemma}
\begin{proof}
  For any $d\times d$ Hermitian matrix, let $\lambda_{i}(\cdot)$ denote its $i$-th
  smallest eigenvalue. The Hadamard product $\mlvec{A}\circ\mlvec{B}$ is a
  $d\times d$ principal submatrix of the Kronecker product
  $\mlvec{A}\otimes \mlvec{B}$, and by the Poincar\'e separation theorem (see
  e.g. Corollary 4.3.37 in \citet{horn2012matrix}):
  \begin{align}
    \lambda_{1}(\mlvec{A}\otimes \mlvec{B})\leq \lambda_{i}(\mlvec{A}\circ\mlvec{B})\leq \lambda_{d^{2}}(\mlvec{A}\otimes \mlvec{B}).
  \end{align}
  The statement follows from the fact that the eigenvalues of
  $\mlvec{A}\otimes \mlvec{B}$ take the form of
  $\lambda_{i}(\mlvec{A})\lambda_{j}(\mlvec{B})$ for $i,j\in[d]$.
\end{proof}
\begin{restatable}[$\Kasym$ for asymptotic dynamics]{lemma}{asympestimation}
\label{lm:asympkernelestimate}
Let $\mathcal{S}$ be a $m$-sample training set composed of pure states
$\{\mlvec{\rho}_{j} = \mlvec{v}_{j}\mlvec{v}_{j}^{\dagger}\}_{j=1}^{m}$. Let
$\qnnmeasure$ be a Pauli-like measurement with eigenvalues $\pm 1$ and
trace-$0$. Consider training a QNN with $\mathcal{S}$, measurement $\qnnmeasure$
and a scaling factor of $\gamma$.
The positive semidefinite matrix $\Kasym$ can be expressed entry-wise as
\begin{align}
  (\Kasym)_{ij}(\mlvec{M}(t)) = 8 \gamma^{2}Re(\mlvec{u}_{j}^{\dagger}(t)\mlvec{u}_{i}(t) \mlvec{w}_{i}^{\dagger}(t)\mlvec{w}_{j}(t)),
\end{align}
where $\mlvec{u}_{i}(t):= \mlvec{\Pi}_{+}(t)\mlvec{v}_{i} $ (resp.
$\mlvec{w}_{i}(t):=\mlvec{\Pi}_{-}(t)\mlvec{v}_{i}$) is the projection of
$\mlvec{v}_{i}$ into the postive (resp. negative) subspace of
$\mlvec{M}(t)=\gamma(\mlvec{\Pi}_{+}(t) - \mlvec{\Pi}_{-}(t))$.
Let $\mlvec{P}(t):=(\mlvec{u}_{i}^{\dagger}(t)\mlvec{u}_{j}(t))_{i.j\in[m]}$ and
$\mlvec{N}(t):=(\mlvec{w}_{i}^{\dagger}(t)\mlvec{w}_{j}(t))_{i,j\in[m]}$ be the Gram matrices of
$\{\mlvec{u}_{i}(t)\}_{i=1}^{m}$ and $\{\mlvec{w}_{i}(t)\}_{i=1}^{m}$, we have:
\begin{align}
  \lambda_{\min}(\Kasym(t)) \geq  8\gamma^{2}\lambda_{\min}(\mlvec{P}(t)) \min_{i\in[m]}(\mlvec{N}_{ii}(t)) \geq 8\gamma^{2}\lambda_{\min}(\mlvec{P}(t)) \lambda_{\min}(\mlvec{N}(t)).
\end{align}
\end{restatable}
\begin{proof}
  For succinctness, we drop the time dependency $t$ when there are no ambiguities.
Calculate the expression of $(\Kasym)_{ij}$ for pure states
$\mlvec{\rho}_{i} = \mlvec{v}_{i}\mlvec{v}_{i}^{\dagger}$:
\begin{align}
  (\Kasym(\mlvec{M}(t)))_{ij}
  &= \tr\big(\imagi[\mlvec{M}, \mlvec{\rho}_{i}] \ \imagi[\mlvec{M}, \mlvec{\rho}_{j}]\big)\\
  &= \tr\big(\mlvec{M}^{2}\mlvec{\rho}_{i} \mlvec{\rho}_{j}\big) + \tr\big(\mlvec{M}^{2}\mlvec{\rho}_{j} \mlvec{\rho}_{i}\big)
    - 2\tr\big(\mlvec{M}\mlvec{\rho}_{i}\mlvec{M} \mlvec{\rho}_{j}\big)\\
  &= 2\gamma^{2}\big(\tr(\mlvec{\rho}_{i} \mlvec{\rho}_{j}\big)
    -
    \tr((\mlvec{\Pi}_{+} - \mlvec{\Pi}_{-})\mlvec{\rho}_{i}(\mlvec{\Pi}_{+} - \mlvec{\Pi}_{-}) \mlvec{\rho}_{j})
    \big)
\end{align}
Plugging in $\mlvec{\rho}_{i} = \mlvec{v}_{i}\mlvec{v}_{i}^{\dagger}$, we have:
\begin{align}
  \frac{1}{2\gamma^{2}}(\Kasym(\mlvec{M}(t)))_{ij}
  &= |\mlvec{u}_{i}^{\dagger}\mlvec{u}_{j} + \mlvec{w}_{i}^{\dagger}\mlvec{w}_{j}|^{2} - |(\mlvec{u}_{i} + \mlvec{w}_{i})^{\dagger}(\mlvec{\Pi}_{+} - \mlvec{\Pi}_{-}) (\mlvec{u}_{j} + \mlvec{w}_{j})|^{2}\\
  &= |\mlvec{u}_{i}^{\dagger}\mlvec{u}_{j} + \mlvec{w}_{i}^{\dagger}\mlvec{w}_{j}|^{2} - |(\mlvec{u}_{i} + \mlvec{w}_{i})^{\dagger} (\mlvec{u}_{j} - \mlvec{w}_{j})|^{2}\\
  &= |\mlvec{u}_{i}^{\dagger}\mlvec{u}_{j} + \mlvec{w}_{i}^{\dagger}\mlvec{w}_{j}|^{2} - |\mlvec{u}_{i}^{\dagger}\mlvec{u}_{j} - \mlvec{w}_{i}^{\dagger}\mlvec{w}_{j}|^{2}\\
  &= 2 \mlvec{u}_{i}^{\dagger}\mlvec{u}_{j} \cdot \mlvec{w}_{j}^{\dagger}\mlvec{w}_{i} + 2 \mlvec{u}_{j}^{\dagger}\mlvec{u}_{i} \cdot \mlvec{w}_{i}^{\dagger}\mlvec{w}_{j}\\
  &= 4 Re(\mlvec{u}_{j}^{\dagger}\mlvec{u}_{i} \mlvec{w}_{i}^{\dagger}\mlvec{w}_{j}),
\end{align}
or $(\Kasym(\mlvec{M}(t)))_{ij} = {8\gamma^{2}} Re(\mlvec{u}_{j}^{\dagger}\mlvec{u}_{i} \mlvec{w}_{i}^{\dagger}\mlvec{w}_{j})$.

Let $\mlvec{P}(t)$ and $\mlvec{N}(t)$ be the Gram matrices for $\{\mlvec{u}_{i}(t)\}_{i=1}^{m}$
and $\{\mlvec{w}_{i}(t)\}_{i=1}^{m}$:
\begin{align}
  (\mlvec{P}(t))_{ij}  = \mlvec{u}(t)_{i}^{\dagger}\mlvec{u}(t)_{j},\ (\mlvec{N}(t))_{ij}  = \mlvec{w}(t)_{i}^{\dagger}\mlvec{w}(t)_{j},
\end{align}
the matrix $\Kasym$ can be expressed as
$\Kasym = 4\gamma^{2} \mlvec{P}\circ\mlvec{N}^{T} + 4\gamma^{2} \mlvec{P}^{T}\circ\mlvec{N}$,
where $\circ$ denotes the Hadamard product, with $\mlvec{P}$ and $\mlvec{N}$
being positive semidefinite matrices. Following a result of Schur's (e.g. see
Lemma 6.5 in \cite{oymak2020toward}), we estimate the smallest eigenvalue of
$\Kasym$ as
\begin{align}
  \lambda_{\min}(\Kasym(\mlvec{M}(\mlvec\theta))) \geq 8\gamma^{2}\max\big(\min_{i\in[m]}(\mlvec{N}_{ii})\lambda_{\min}(\mlvec{P}), \min_{i\in[m]}(\mlvec{P}_{ii})\lambda_{\min}(\mlvec{N})\big).
\end{align}
\end{proof}
The second statement in the limit suggests that the $\Kasym$ is positive definite unless the subspaces spanned by $\mlvec{u}_j$ or $\mlvec{w}_j$ are not full rank, though we do not make use of this fact in the proof of Theorem~\ref{thm:smallest_eig}.
\subsection{Proof of Theorem~\ref{thm:smallest_eig}}
\begin{proof}
For each input state $\mlvec{\rho}_{j}=\mlvec{v}_{j}\mlvec{v}_{j}^{\dagger}$, let $\mlvec{u}_{j}$
and $\mlvec{w}_{j}$ denote the projection of $\mlvec{v}_{j}$ onto the positive
and negative subspaces of the measurement. Since the measurment is updated
throughout the training, $\mlvec{u}_{j}$  and $\mlvec{w}_{j}$ are functions of
time. For a QNN with the scaling factor $\gamma$, the QNN prediction for the
input state $\mlvec{\rho}_{j}$ at time $t$ is
$\hat{y}_{j} = \gamma(\mlvec{u}_{j}^{\dagger}(t)\mlvec{u}_{j}(t) - \mlvec{w}_{j}^{\dagger}(t)\mlvec{w}_{j}(t))$.
Additionally by the normalization of quantum states and the orthogonality of the
training sample, we have
$\mlvec{u}_{j}^{\dagger}(t)\mlvec{u}_{j}(t) + \mlvec{w}_{j}^{\dagger}(t)\mlvec{w}_{j}(t) = \delta_{ij}$,
where $\delta_{ij}$ is the Kronecker delta function.
Combining these two conditions, we can solve that
$\mlvec{u}_{j}^{\dagger}\mlvec{u}_{j} = \frac{1}{2}(1\pm 1/\gamma)$ and
$\mlvec{v}_{j}^{\dagger}\mlvec{v}_{j} = \frac{1}{2}(1\mp 1/\gamma)$ for
$y_{j} = \pm 1$.

By Lemma~\ref{lm:asympkernelestimate}, the diagonal entries
$(\Kasym)_{jj} = 8\gamma^{2}Re(\mlvec{u}_{j}^{\dagger}\mlvec{u}_{j}\mlvec{w}_{j}^{\dagger}\mlvec{w}_{j}) = 8\gamma^{2}\cdot \frac{1}{2}(1\pm 1/\gamma) \cdot\frac{1}{2}(1\mp 1/\gamma) = 2\gamma^{2}(1-1/\gamma^{2})$.

Without loss of generality, assume $y_{1}=y_{2}=\cdots=y_{m/2} = 1$ and
$y_{m/2+1}=y_{m/2+2}=\cdots y_{m} = -1$. Then
$\mlvec{u}_{j}=\sqrt{\frac{1+1/\gamma}{2}}\hat{\mlvec{u}}_{j}$ for
$1\leq j\leq m/2$ and
$\mlvec{u}_{j}=\sqrt{\frac{1-1/\gamma}{2}}\hat{\mlvec{u}}_{j}$ for
$m/2+1\leq j \leq m$. Here $\hat{\mlvec{u}}_{j}$ are unit vectors defined as
$\mlvec{u}_{j}/\sqrt{\mlvec{u}_{j}^{\dagger}\mlvec{u}_{j}}$. For the
off-diagonal entries,
$(\Kasym)_{ij} = 8\gamma^{2}Re(\mlvec{u}_{i}^{\dagger}\mlvec{u}_{j}\mlvec{w}_{j}^{\dagger}\mlvec{w}_{i}) = 8\gamma^{2}Re(\mlvec{u}_{i}^{\dagger}\mlvec{u}_{j} \cdot  (-\mlvec{u}_{j}^{\dagger}\mlvec{u}_{i})) =  -8\gamma^{2}|\mlvec{u}_{i}^{\dagger}\mlvec{u}_{j}|^{2}$.
For the first equality we use the orthogonality among $\{\mlvec{v}_{j}\}_{j=1}^{m}$.

Define $m\times m$ Hermitian $\mlvec{G}$ such that
$G_{ij}=\hat{\mlvec{u}}_{i}^{\dagger}\hat{\mlvec{u}}_{j}$ and $\mlvec{R}$ such
that
$R_{ij} = \frac{1}{2}(1+1/\gamma)$ for $1\leq i,j\leq m/2$,
$R_{ij} = \frac{1}{2}(1-1/\gamma)$ for $m/2+1\leq i,j\leq m$, and
$R_{ij} = \frac{1}{2}\sqrt{1-1/\gamma^{2}}$ for
$1\leq i\leq m/2, m/2+1\leq j\leq m$ or
$m/2+1\leq i\leq m, 1\leq j\leq m/2$. The off-diagonal entries can be expressed $-8\gamma^{2}R_{ij}G_{ij}G_{ji}$.

Using the notations of $\mlvec{R}$ and $\mlvec{G}$, the matrix $\Kasym$ at the
global minima can be expressed as
\begin{align}
\Kasym = 2\gamma^{2}(1-1/\gamma^{2})\mlvec{I} - 8\gamma^{2}\mlvec{R}\circ(\mlvec{G}-\mlvec{I})\circ(\mlvec{G}^{T}-\mlvec{I}),
\end{align}
where $\mlvec{I}$ is the $m\times m$ identity matrix.

\paragraph{Eigenvalues of $\mathbf{R}$}
Let $\mlvec{e}_{1}$ and $\mlvec{e}_{2}$ denote the unit vectors
\begin{align}
  \mlvec{e}_{1} &= \sqrt{\frac{2}{m}}(1,1,\cdots, 1, 0, 0, \cdots, 0)^{T}\\
  \mlvec{e}_{2} &= \sqrt{\frac{2}{m}}(0,0 \cdots, 0, 1,1,\cdots, 1)^{T}
\end{align}
that are zero in the first (last) $m/2$ entries. The matrix $\mlvec{R}$ can be
written as
\begin{align}
  \frac{m}{2}(
  \frac{1}{2}(1+1/\gamma)\mlvec{e}_{1}\mlvec{e}_{1}^{\dagger} +
  \frac{1}{2}(1-1/\gamma)\mlvec{e}_{2}\mlvec{e}_{2}^{\dagger} +
  \frac{1}{2}\sqrt{1-1/\gamma^{2}}\mlvec{e}_{1}\mlvec{e}_{2}^{\dagger}+
  \frac{1}{2}\sqrt{1-1/\gamma^{2}}\mlvec{e}_{2}\mlvec{e}_{1}^{\dagger}
  )
\end{align}
and can be shown to have eigenvalues $(\frac{m}{2},0,\cdots,0)$ by
straight-forward calculation.
\paragraph{Eigenvalues of $\mathbf{G}$}
Over the uniform measure over all the global minima, the direction vectors
$\hat{\mlvec{u}}_{i}$ are sampled independently and uniformly from a
$d/2$-dimensional (complex) sphere. By the approximate isometric properties (see
e.g. Theorem 5.58 in \cite{vershynin2010introduction}), the gram matrix
$\mlvec{G}$ of $\{\hat{\mlvec{u}}_{j}\}_{j=1}^{m}$ is approximately an isometry:
with probability $\geq 1-2\exp(-c_{p}t^{2})$
\begin{align}
  \opnorm{\mlvec{G}-\mlvec{I}} \leq c_{m}\frac{\max\{\sqrt{m}, t\}}{\sqrt{d}}
\end{align}
for constants $c_{p}$ and $c_{m}$.

Applying Lemma~\ref{lm:hadamard} to $\mlvec{R}$, $\mlvec{G}-\mlvec{I}$ and
$\mlvec{G}^{T}-\mlvec{I}$, we have that with probability $\geq 1-\delta$, the
smallest eigenvalues of $\Kasym$ at global minima is greater than or equal to
\begin{align}
 2\gamma^{2}(1-1/\gamma^{2} - C_{2}\max\{\frac{m^{2}}{d}, \frac{m\log(2/\delta)}{d}\})
\end{align}
for some constant $C_2>0$.
\end{proof}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main.tex"
%%% End:
