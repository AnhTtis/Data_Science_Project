Consider a regression model on an $m$-sample training set:
for all $j\in[m]$, let $y_{j}$ and $\hat{y}_j$ be the label and the model prediction of the $j$-th sample. The \emph{residual} vector $\mlvec{r}$ is a
$m$-dimensional vector with $r_{j}:= y_{j}-\hat{y}_{j}$.
The dynamics of the kernel regression is signatured by the first-order linear
dynamics of the residual vectors: let $\mlvec{w}$ be the learned model
parameter, and let $\phi(\cdot)$ be the fixed non-linear map. Recall
that the kernel regression minimizes
$L(\mlvec{w})=\frac{1}{2m}\sum_{j=1}^{m}(\mlvec{w}^{T}\phi(\mlvec{x}_{j})-y_{j})^{2}$
for a training set $\mathcal{S}=\{(\mlvec{x}_{j}, y_{j})\}_{j=1}^{m}$, and the
gradient with respect to $\mlvec{w}$ is
$\frac{1}{m}\sum_{j=1}^{m}(\mlvec{w}^{T}\phi(\mlvec{x}_{j})-y_{j})\phi(\mlvec{x}_{j}) = -\frac{1}{m}\sum_{j=1}^{m}r_{j}\phi(\mlvec{x}_{j})$.
Under the gradient flow with learning rate $\eta$, the weight $\mlvec{w}$ updates as
$\frac{d\mlvec{w}}{dt} = \frac{\eta}{m}\sum_{j=1}^{m}r_{j}\phi(\mlvec{x}_{j})$,
and the $i$-th entry of the residual vector updates as
$dr_{i}/dt = -\phi(\mlvec{x}_{i})^{T}\frac{d\mlvec{w}}{dt} = -\frac{\eta}{m}\sum_{j=1}^{m}\phi(\mlvec{x}_{i})^{T}\phi(\mlvec{x}_{j})r_{j}$,
or more succinctly $d\mlvec{r}/dt = -\frac{\eta}{m}\mlvec{K}\mlvec{r}$ with
$\mlvec{K}$ being the kernel/Gram matrix defined as $K_{ij}=\phi(\mlvec{x}_{i})^{T}\phi(\mlvec{x}_{j})$ (see also
\citet{arora2019exact}). Notice that the kernel matrix $\mlvec{K}$ is a constant
of time and is independent of the weight $\mlvec{w}$ or the labels.

\paragraph{Dynamics of residual vectors}
We start by characterizing the dynamics of the residual vectors
for the general form of $p$-parameter QNNs and highlight the limitation of
viewing the over-parameterized QNNs as kernel regressions.
Similar to the kernel regression,
$\frac{dr_{j}}{dt} = -\frac{d \hat{y}_{j}}{dt}= -\tr(\mlvec{\rho}_{j}\frac{d}{dt}\mlvec{U}^{\dagger}(\mlvec\theta(t))\qnnmeasure\mlvec{U}(\mlvec\theta(t)))$
in QNNs. We derive the following dynamics of $\mlvec{r}$ by tracking the
parameterized measurement
$\mlvec{M}(\mlvec{\theta})=\mlvec{U}^{\dagger}(\mlvec\theta)\mlvec{M}_{0}\mlvec{U}(\mlvec\theta)$
as a function of time $t$.
\begin{restatable}[Dynamics of the residual vector]{lemma}{qnndynamics}
\label{lm:qnndynamics}
Consider a QNN instance with an ansatz $\mlvec{U}(\mlvec\theta)$ defined as in
Line~(\ref{eq:general-ansatz}), a training
dataset $\mathcal{S} = \{(\mlvec\rho_{j},y_{j})\}_{j=1}^{m}$, and a measurement
$\qnnmeasure$. Under the gradient flow for the objective function
  $L(\mlvec\theta)=\frac{1}{2m}\sum_{j=1}^{m}\big(\tr(\mlvec{\rho}_{j}\mlvec{U}^{\dagger}(\mlvec\theta)\qnnmeasure\mlvec{U}(\mlvec\theta))-y_{j}\big)^{2}$
  with learning rate $\eta$,
the residual vector $\mlvec{r}$ satisfies the differential equation
\begin{align}
  \label{eq:qnn-residue-kernel}
  \diffT{\mlvec{r}(\varytheta)} = -\frac{\eta}{m}\mlvec{K}(\paramM) \mlvec{r}(\varytheta),
\end{align}
where $\mlvec{K}$ is a positive semi-definite matrix-valued function of the
parameterized measurement. The $(i,j)$-th element of $\mlvec{K}$ is defined as
\begin{align}
  \sum_{l=1}^p\big(
  \tr\big(\compI[\paramM,\mlvec{\rho}_{i}]\tildeH{l}\big)
  \tr\big(\compI[\paramM,\mlvec{\rho}_{j}]\tildeH{l}\big)
  \big).
\end{align}
Here $\tildeH{l}:=\mlvec{U}_{0}^{\dagger}\mlvec{U}_{1:l-1}^{\dagger}(\mlvec\theta)\generatorH{l}\mlvec{U}_{1:l-1}(\mlvec\theta)\mlvec{U}_{0}$,
is a function of $\mlvec\theta$
with $ \mlvec{U}_{1:r}(\mlvec\theta)$ being the shorthand for
$\mlvec{U}_{r}\exp(-i\theta_{r}\generatorH{r})\cdots \mlvec{U}_{1}\exp(-i\theta_{1}\generatorH{1})$.
\end{restatable}

While Equation~(\ref{eq:qnn-residue-kernel}) takes a similar form to that of the kernel
regression, the matrix $\mlvec{K}$ is \emph{dependent} on the parameterized measurement
$\mlvec{M}(\mlvec\theta)$. This is a consequence of the unitarity: consider an alternative
parameterization, where the objective function
$\mlvec{L}(\mlvec{M})=\frac{1}{2m}\sum_{j=1}^{m}\big(\tr(\mlvec\rho_{j}\mlvec{M})-y_{j}\big)^{2}$
is optimized over all Hermitian matrices $\mlvec{M}$. It can be easily verified that the
corresponding dynamics is exactly the kernel regression with $K_{ij}=\tr(\mlvec{\rho}_{i}\mlvec{\rho}_{j})$.

Due to the unitarity of the evolution of quantum states, the spectrum of eigenvalues
of the parameterized measurement $\mlvec{M}(\mlvec\theta)$ is required to remain the
same throughout training.
In the proof of Lemma~\ref{lm:qnndynamics} (deferred to
Section~\ref{subsec:qnndynamics_proof} in the appendix),
we see that the derivative of $\mlvec{M}(\mlvec\theta)$ takes the form of a linear
combination of commutators $i[\mlvec{A}, \mlvec{M}(\mlvec\theta)]$ for some Hermitian $\mlvec{A}$.
As a result, the traces of the $k$-th matrix powers
$\tr(\mlvec{M}^{k}(\mlvec\theta))$ are constants of time for any integer $k$, since
$d\tr(\mlvec{M}^{k}(\mlvec\theta))/dt = k\tr(\mlvec{M}^{k-1}(\mlvec\theta)d\mlvec{M}(\mlvec{\theta})/dt) = k\tr(\mlvec{M}^{k-1}(\mlvec\theta)i[\mlvec{A}, \mlvec{M}(\mlvec{\theta})])=0$
for any Hermitian $\mlvec{A}$.
The spectrum of eigenvalues remains unchanged because the
coefficients of the characteristic polynomials of $\mlvec{M}(\mlvec\theta)$ is
completely determined by the traces of matrix powers. On the contrary, the eigenvalues are in
general not preserved for $\mlvec{M}$ evolving under the kernel regression.

Another consequence of the unitarity constraint is that a QNN can not
make predictions outside the range of the eigenvalues of $\qnnmeasure$, while
for the kernel regression with a strictly positive definite kernel, the model can
(over-)fit training sets with arbitrary label assignments. Here we further show that
the unitarity is pronounced in a typical QNN instance where the predictions are
within the range of the measurement.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main.tex"
%%% End:
