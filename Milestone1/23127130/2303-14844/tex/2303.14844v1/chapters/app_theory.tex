\subsection{Proof of Lemma~\ref{lm:resid_dyn_decomp}}
\label{subsec:lm_resid_dyn_decomp}
\decompresid*
Throughout the proof, we make use of the following notations. Let $\mathcal{H}$ be a $d$-dimensional Hilbert space, and let $\{\mlvec{e}_a\}_{a\in[d]}$ be a basis of $\mathcal{H}$. We use $\mlvec{I}_{d\times d}$ denote the identity matrix $\sum_{a\in[d]}\mlvec{e}_a\mlvec{e}_a^\dagger$.
We use $\otimes$ for kronecker products on vectors, matrices and Hilbert spaces.
For the $d^2\times d^2$-dimensional product space $\mathcal{H}\otimes \mathcal{H}$, let $\mlvec{W}_{d^2\times d^2}$ denote the swap matrix $\sum_{a,b\in[d]}\mlvec{e}_a\mlvec{e}_b^\dagger\otimes \mlvec{e}_b\mlvec{e}_a^\dagger$. 

We will also make use of the well-known integration formula with respect to the haar measure over $d$-dimensional unitaries (see e.g. \citet{collins2006integration} for more details). 
\begin{proof}
As proven in Lemma~\ref{lm:qnndynamics}, we track the
dynamics of the parameterized measurement $\mlvec{M}(\mlvec\theta)$:
\begin{align}
  \frac{d\mlvec{M}(\mlvec\theta)}{dt}
  = &\sum_{l=1}^{p}\frac{d\theta_{l}}{dt} \cdot \frac{\partial \mlvec{M}(\mlvec\theta)}{\partial \theta_{l}}\\
  = &\sum_{l=1}^{p}(-\eta)\tr\big(\imagi[\mlvec{H}_{l}, \mlvec{M}(\mlvec\theta)] \nabla_{\mlvec{M}}L \big) \imagi [\mlvec{H}_{l}, \mlvec{M}(\mlvec\theta)]\\
  = &\sum_{l=1}^{p}\eta\tr\big(\imagi[\nabla_{\mlvec{M}}L, \mlvec{M}(\mlvec\theta)] \mlvec{H}_{l} \big) \imagi [\mlvec{H}_{l}, \mlvec{M}(\mlvec\theta)]\\
  = &\sum_{l=1}^{p}\eta \imagi [\tr\big(\imagi[\nabla_{\mlvec{M}}L, \mlvec{M}(\mlvec\theta)] \mlvec{H}_{l} \big)\mlvec{H}_{l}, \mlvec{M}(\mlvec\theta)]\\
  = &\sum_{l=1}^{p}\eta \imagi [\tr_{1}\big((\imagi[\nabla_{\mlvec{M}}L, \mlvec{M}(\mlvec\theta)]\otimes\mlvec{I}) (\mlvec{H}_{l}\otimes\mlvec{H}_{l})\big), \mlvec{M}(\mlvec\theta)].
\end{align}
Here $\tr_{{1}}(\cdot)$ is the partial trace: Given the product of two Hilbert spaces $\mathcal{H}_1 \otimes \mathcal{H}_2$, the partial trace on the first Hilbert space is a linear mapping such that
\begin{align*}
    \tr_1\big(\mathbf{A}\otimes \mathbf{B}\big) = \tr(\mathbf{A})\mathbf{B}
\end{align*}
for any Hermitians $\mathbf{A}$ and $\mathbf{B}$ on the spaces $\mathcal{H}_1$ and $\mathcal{H}_2$. By linearity,
  \begin{align*}
    \tr_1\big(\sum_l\mathbf{A}_l\otimes \mathbf{B}_l\big) = \sum_l\tr(\mathbf{A}_l)\mathbf{B}_l
\end{align*}
for any Hermitians $\{\mathbf{A}_l\}$ and $\{\mathbf{B}_l\}$ on the spaces $\mathcal{H}_1$ and $\mathcal{H}_2$.

Let $Z(\mlvec{H},d)$ denote the ratio $\frac{\tr(\mlvec{H}^{2})}{d^{2}-1}$, the
learning rate $\eta$ can be expressed as $\frac{m}{pZ(\mlvec{H},d)}$. Let $\mlvec{Y}(\mlvec\theta(t))$ denote the normalized
  $d^{2}\times d^{2}$-complex matrix
  $\frac{1}{pZ(\mlvec{H},d)}\sum_{l=1}^{p}\mlvec{H}_{l}\otimes\mlvec{H}_{l}$ for $\mlvec{H}_l$ defined in Lemma~\ref{lm:qnndynamics}
  and let $\mlvec{Y}^{\star}$ denote
  $\mlvec{W}_{d^{2}\times d^{2}} - \frac{1}{d} \mlvec{I}_{d^{2}\times d^{2}}$,
  the asymptotic version of $\mlvec{Y}$. We can accordingly decompose the dynamics into the
  asymptotic dynamics and the deviation (perturbation) from the asymptotic
  dynamics:
  \begin{align}
    \frac{d\mlvec{M}(\mlvec\theta)}{dt}
    = &(\eta p Z(\mlvec{H},d)) \imagi [\tr_{1}\big((\imagi[\nabla_{\mlvec{M}}L, \mlvec{M}(\mlvec\theta)]\otimes\mlvec{I}) \mlvec{Y}\big), \mlvec{M}(\mlvec\theta)]\\
    =
    & (\eta p Z(\mlvec{H},d)) \imagi [\tr_{1}\big((\imagi[\nabla_{\mlvec{M}}L, \mlvec{M}(\mlvec\theta)]\otimes\mlvec{I}) \mlvec{Y}^{\star}\big), \mlvec{M}(\mlvec\theta)]\\
    & + (\eta p Z(\mlvec{H},d)) \imagi [\tr_{1}\big((\imagi[\nabla_{\mlvec{M}}L, \mlvec{M}(\mlvec\theta)]\otimes\mlvec{I}) (\mlvec{Y}(\mlvec\theta(t)) - \mlvec{Y}^{\star})\big), \mlvec{M}(\mlvec\theta)]\\
    =
    & (\eta p Z(\mlvec{H},d)) \imagi [(\imagi[\nabla_{\mlvec{M}}L, \mlvec{M}(\mlvec\theta)], \mlvec{M}(\mlvec\theta)]\\
    & + (\eta p Z(\mlvec{H},d)) \imagi [\tr_{1}\big((\imagi[\nabla_{\mlvec{M}}L, \mlvec{M}(\mlvec\theta)]\otimes\mlvec{I}) (\mlvec{Y}(\mlvec\theta(t)) - \mlvec{Y}^{\star})\big), \mlvec{M}(\mlvec\theta)]\\
    =
      & - (\eta p Z(\mlvec{H},d)) [\mlvec{M}(\mlvec\theta), [\mlvec{M}(\mlvec\theta), \nabla_{\mlvec{M}}L]]\\
      & -(\eta p Z(\mlvec{H},d)) [\mlvec{M}(\mlvec\theta), \tr_{1}\big(([\mlvec{M}(\mlvec\theta), \nabla_{\mlvec{M}}L]\otimes\mlvec{I}) (\mlvec{Y}(\mlvec\theta(t)) - \mlvec{Y}^{\star})\big)]
  \end{align}
  Plugging in that
  $\nabla_{\mlvec{M}}L(\mlvec{M}(\mlvec\theta)) = -\frac{1}{m}\sum_{i=1}^{m}r_{i}\mlvec\rho_{i}$
  with the residual
  $r_{i}:=y_{i}-\hat{y}_{i} = \tr(\mlvec{M}(\mlvec\theta)\rho_{i}) - y_{i}$:
  \begin{align}
    \frac{d\mlvec{M}(\mlvec\theta)}{dt}
      =& \sum_{j=1}^{m} r_{j} [\mlvec{M}(\mlvec\theta), [\mlvec{M}(\mlvec\theta), \mlvec{\rho}_{j}]]\\
      +& \sum_{j=1}^{m}r_{j} [\mlvec{M}(\mlvec\theta), \tr_{1}\big(([\mlvec{M}(\mlvec\theta), \mlvec{\rho}_{j}]\otimes\mlvec{I}) (\mlvec{Y}(\mlvec\theta(t)) - \mlvec{Y}^{\star})\big)]
  \end{align}
  Trace after multiplying $\mlvec{\rho}_{i}$ on both sides:
  \begin{align}
    \frac{dr_{i}}{dt} = -\tr(\mlvec{\rho}_{i}\frac{d\mlvec{M}(\mlvec\theta)}{dt}) =
      & - \sum_{j=1}^{m} r_{j} \tr\big(\mlvec{\rho}_{i} [\mlvec{M}(\mlvec\theta), [\mlvec{M}(\mlvec\theta), \mlvec{\rho}_{j}]]\big)\\
      & - \sum_{j=1}^{m}r_{j} \tr\big(\mlvec{\rho}_{i} [\mlvec{M}(\mlvec\theta), \tr_{1}\big(([\mlvec{M}(\mlvec\theta), \mlvec{\rho}_{j}]\otimes\mlvec{I}) (\mlvec{Y}(\mlvec\theta(t)) - \mlvec{Y}^{\star})\big)]\big)
  \end{align}
  The lemma follows directly from rearranging: for the first term,
  \begin{align}
    &-\sum_{j=1}^{m}r_{j}\tr(\mlvec{\rho}_{i}[\mlvec{M}(\mlvec\theta), [\mlvec{M}(\mlvec\theta), \mlvec{\rho}_{j}]])\\
    =&-\sum_{j=1}^{m}r_{j}\tr([\mlvec{\rho}_{i}, \mlvec{M}(\mlvec\theta)][\mlvec{M}(\mlvec\theta), \mlvec{\rho}_{j}])\\
    =&-\sum_{j=1}^{m}r_{j}\tr(\imagi[\mlvec{M}(\mlvec\theta),\mlvec{\rho}_{i}]\imagi[\mlvec{M}(\mlvec\theta), \mlvec{\rho}_{j}]).
  \end{align}
  For the second term,
  \begin{align}
    & - \sum_{j=1}^{m}r_{j} \tr\big(\mlvec{\rho}_{i} [\mlvec{M}(\mlvec\theta), \tr_{1}\big(([\mlvec{M}(\mlvec\theta), \mlvec{\rho}_{j}]\otimes\mlvec{I}) (\mlvec{Y}(\mlvec\theta(t)) - \mlvec{Y}^{\star})\big)]\big)\\
    =& - \sum_{j=1}^{m}r_{j} \tr\big( \imagi[\mlvec{M}(\mlvec\theta), \mlvec{\rho}_{i}] \tr_{1}\big((\imagi[\mlvec{M}(\mlvec\theta), \mlvec{\rho}_{j}]\otimes\mlvec{I}) (\mlvec{Y}(\mlvec\theta(t)) - \mlvec{Y}^{\star})\big)\big)\\
    =& - \sum_{j=1}^{m}r_{j} \tr\big((\mlvec{I}\otimes\imagi[\mlvec{M}(\mlvec\theta), \mlvec{\rho}_{i}]) (\imagi[\mlvec{M}(\mlvec\theta), \mlvec{\rho}_{j}]\otimes\mlvec{I}) (\mlvec{Y}(\mlvec\theta(t)) - \mlvec{Y}^{\star})\big)\\
    =& - \sum_{j=1}^{m}r_{j} \tr\big((\imagi[\mlvec{M}(\mlvec\theta), \mlvec{\rho}_{j}]\otimes\imagi[\mlvec{M}(\mlvec\theta), \mlvec{\rho}_{i}]) (\mlvec{Y}(\mlvec\theta(t)) - \mlvec{Y}^{\star})\big)\\
    =& - \sum_{j=1}^{m}r_{j} \tr\big((\imagi[\mlvec{M}(\mlvec\theta), \mlvec{\rho}_{i}]\otimes\imagi[\mlvec{M}(\mlvec\theta), \mlvec{\rho}_{j}]) (\mlvec{Y}(\mlvec\theta(t)) - \mlvec{Y}^{\star})\big)
  \end{align}
  The last equality follows from the fact that $\mlvec{Y}$ and
  $\mlvec{Y}^{\star}$ are invariant under the swapping of spaces.
  The lemma follows by identifying the matrix $\Delta(t)$ with $\mlvec{Y}(\mlvec\theta(t))-\mlvec{Y}^\star$.
\end{proof}

\subsection{Proof of Theorem~\ref{thm:qnn-mse-linear}}
\label{subsec:thm_qnn_mse_linear}
\qnnmselinear*
\begin{proof}
In Lemma~\ref{lm:resid_dyn_decomp}, we decompose the QNN dynamics into the
asymptotic term and the perturbation term depending on
$\Delta(t) = \mlvec{Y}(\mlvec\theta(t)) - \mlvec{Y}^{\star}$. We now show that the use of
the terms
``asymptotic'' and ``perturbation'' are exact, by showing that
$\mlvec{Y}(\mlvec\theta(t)) - \mlvec{Y}^{\star}$ vanishes as
$p\rightarrow \infty$. We make use of the characterization of a similarly-defined quantity in \cite{ourvqe2022}, restated as
Lemma~\ref{lm:concentration_init} and \ref{lm:concentration_train}, such that
for sufficiently large $p$,
$\opnorm{\mlvec{Y}(\mlvec\theta(t)) - \mlvec{Y}^{\star}}$ vanishes for all $t$
with high probability over the randomness in $\{\mlvec{U}_{l}\}_{l=0}^{p}$.
Recall that the perturbation term $\Kpert$ is defined as
\begin{align}
  (\Kpert(t))_{ij} &:=
      \tr\big((\imagi[\mlvec{M}(\mlvec\theta), \mlvec{\rho}_{i}]\otimes\imagi[\mlvec{M}(\mlvec\theta), \mlvec{\rho}_{j}]) (\mlvec{Y}(\mlvec\theta(t)) - \mlvec{Y}^{\star})\big).
\end{align}
By choosing sufficiently large $p$, we have $\opnorm{\Kpert(t)} \leq C_{0} / 10$
and therefore the loss function converging to zero at a rate $\geq C_{0} / 2$.
\end{proof}
\begin{lemma}[Concentration at initialization, adapted from Lemma 3.4 in \cite{ourvqe2022}]
  \label{lm:concentration_init}
     Over the randomness of ansatz initialization (i.e. for
     $\{\mlvec{U}_{l}\}_{l=1}^{p}$ sampled $i.i.d.$ with respect to
     the Haar measure), for any initial $\mlvec\theta(0)$, with probability $1 - \delta$:
     \begin{align}
     \opnorm{\YY(\mlvec\theta(0)) - \YY^{\star}} \leq \frac{1}{\sqrt{p}}\cdot \frac{2\opnorm{\mlvec{H}}^{2}}{Z}\sqrt{2\log\frac{d^{2}}{\delta}}.
     \end{align}
\end{lemma}
\begin{proof}
Define
\begin{align}
\mlvec{X}_{l}:=\frac{1}{Z(\mlvec{H},d)}\big(\mlvec{U}_{0:l-1}(\mlvec\theta(0))^{\dagger}\mlvec{H}\mlvec{U}^{\dagger}_{0:l-1}(\mlvec\theta(0))\big)^{\otimes 2} - \mlvec{Y}^{\star}.
\end{align}
By straight-forward calculation (e.g. using results in \citet{collins2006integration}) we know that $X_{l}$ is centered (i.e
$\EXP[X_{l}] = 0$). The set $\{\mlvec{X}_{l}\}$ can be viewed as independent random matrices as the Haar
random unitary removes all the correlation. The matrix on the left-hand side can
therefore be expressed as the arithmetic average of $p$ independent random matrices.
The square of $\mlvec{X}_{l}$ is bounded in operator norm:
\begin{align}
  \opnorm{\mlvec{X}_{l}^{2}} = \opnorm{\mlvec{X}_{l}}^{2}\leq (\frac{\opnorm{\mlvec{H}}^{2}}{Z} + \frac{d+1}{d})^{2} \leq (\frac{2\opnorm{\mlvec{H}}^{2}}{Z(\mlvec{H},d)})^{2}
 \end{align}
where the second inequality follows from the fact that the ratio $g_{1} = \opnorm{\HH}^{2} / \tr(\HH^{2})$ satisfies that $1 \geq g_{1} \geq 1/d$.
By Hoeffding's inequality(\cite{tropp2012user}, Thm 1.3), with probability
$\geq 1-\delta$,
\begin{align}
  \opnorm{\YY(\mlvec\theta(0)) - \YY^{\star}} \leq \frac{1}{\sqrt{p}}\cdot \frac{2\opnorm{\mlvec{H}}^{2}}{Z(\mlvec{H},d)}\sqrt{\log\frac{2d^{2}}{\delta}}.
\end{align}
\end{proof}

As we pointed out in the main body, a vanishing perturbation term at initialization is not
sufficient to guarantee the term remain perturbative throughout the training. We
now show in Lemma~\ref{lm:concentration_train} that
$\mlvec{Y}(\mlvec\theta(t)) - \mlvec{Y}^{\star}$ remain small during training by
showing $\mlvec{Y}(\mlvec\theta(t)) - \mlvec{Y}(\mlvec(0))$ vanishes in the
limit $p \rightarrow \infty$. But before that, we show that, while the QNN
predictions changes much during training, the change in the parameters measured
in $\ell_{2}$- or $\ell_{\infty}$-norm
($\|\mlvec{\theta}(t) - \mlvec{\theta}(0)\|_{2}$ or
$\|\mlvec{\theta}(t) - \mlvec{\theta}(0)\|_{\infty}$) vanishes as
$p \rightarrow \infty$ during the training of QNN:
\begin{restatable}[Slow-varying $\theta$ in QNNs]{lemma}{qnnslowtheta}
  \label{lm:qnn-slow-theta}
  Suppose that under learning rate $\eta=\frac{m}{p Z(\mlvec{H},d)}$, for all $0 \le t \le T$,
  the loss function $L(\mlvec\theta(t)) \leq  L(\mlvec\theta(0)) \exp(-at)$ for
  some constant $a$,
  then for all $0 \le t_1,t_2 \le T$:
  \begin{align}
    \|\mlvec{\theta}(t_2) - \mlvec{\theta}(t_1)\|_{\infty}
    &\leq \frac{1}{p}\frac{\sqrt{2}m\fronorm{\mlvec{H}}\fronorm{\mlvec{M}}\sqrt{L(\mlvec\theta(0))}}{Z}|t_{1} - t_{2}|,\\
    \|\mlvec{\theta}(t_2) - \mlvec{\theta}(t_1)\|_{2}
    &\leq \frac{1}{\sqrt{p}}\frac{\sqrt{2}m\fronorm{\mlvec{H}}\fronorm{\mlvec{M}}\sqrt{L(\mlvec\theta(0))}}{Z}|t_{1} - t_{2}|.
  \end{align}
\end{restatable}
\begin{proof}
  We first bound the absolute value of the derivative $\frac{d\theta_{l}}{dt}$:
  \begin{align}
    |\frac{d\theta_{l}}{dt}| = \eta |\frac{\partial L}{\partial \theta_{l}}|
    = \frac{\eta}{2m}|\sum_{i=1}^{m}r_{i}\tr(\imagi[\mlvec{M}(\mlvec\theta), \mlvec{H}_{l}]\mlvec\rho_{i})|.
  \end{align}
  Plugging in $\eta = \frac{m}{p Z}$, we have
  \begin{align}
    |\frac{d\theta_{l}}{dt}| = \frac{1}{2pZ}|\sum_{i=1}^{m}r_{i}\tr(\imagi[\mlvec{M}(\mlvec\theta), \mlvec{H}_{l}]\mlvec\rho_{i})|
    = \frac{1}{2pZ}|\<\mlvec{r}, \mlvec{a}\>|,
  \end{align}
  where the vector $\mlvec{a}$ is defined such that
  $a_{j} = \tr(i[\mlvec{M}(\mlvec\theta), \mlvec{H}_{l}]\mlvec{\rho}_{j})$ for
  $j \in [m]$. The $\ell_{2}$-norm of $\mlvec{a}$
  \begin{align}
    \|\mlvec{a}\|_{2}^{2} &= \sum_{j=1}^{m}\tr^{2}(\imagi [\mlvec{M}, \mlvec{H}_{l}]\mlvec{\rho}_{j})\\
    &= \tr\big((i[\mlvec{M}, \mlvec{H}_{l}])^{\otimes 2} \sum_{j=1}^{m}\mlvec{\rho_{j}}^{\otimes 2}\big)\\
    &\leq \fronorm{(i[\mlvec{M}, \mlvec{H}_{l}])^{\otimes 2}}\fronorm{\sum_{j=1}^{m}\mlvec{\rho_{j}}^{\otimes 2}}\\
    &\leq \fronorm{i[\mlvec{M}, \mlvec{H}_{l}]}^{2}\fronorm{\sum_{j=1}^{m}\mlvec{\rho_{j}}^{\otimes 2}}\\
    &\leq (2\fronorm{\mlvec{M}}\fronorm{\mlvec{H}_{l}})^{2}\sum_{j=1}^{m}\fronorm{\mlvec{\rho_{j}}^{\otimes 2}}\\
    &\leq (2 \fronorm{\mlvec{M}}\fronorm{\mlvec{H}} \sqrt{m})^{2}.
  \end{align}
  Therefore we can bound
  $|\frac{d\theta_{l}}{dt}|$ as
  \begin{align}
    |\frac{d\theta_{l}}{dt}|
    &\leq \frac{1}{2pZ}\|\mlvec{r}\|_{2}\|\mlvec{a}\|_{2}\\
    &\leq \frac{1}{2pZ}\sqrt{2mL(\mlvec\theta(t))} \cdot 2 \fronorm{\mlvec{M}}\fronorm{\mlvec{H}} \sqrt{m}\\
    &= \frac{1}{p}\frac{\sqrt{2} m \fronorm{\mlvec{M}}\fronorm{\mlvec{H}}}{Z} \sqrt{L(\mlvec\theta(t))}\\
    &\leq \frac{1}{p}\frac{\sqrt{2} m \fronorm{\mlvec{M}}\fronorm{\mlvec{H}}}{Z} \sqrt{L(\mlvec\theta(0))} \exp(-at / 2)
  \end{align}
  Hence for all $l\in[p]$:
  \begin{align}
    |\theta_{l}(t_{2}) - \theta_{l}(t_{1})|
    &= |\int_{t_{1}}^{t_{2}}dt d\theta_l(t)/dt| \leq \int_{t_{1}}^{t_{2}}dt |d\theta_l(t)/dt|\\
    &\leq \int_{t_{1}}^{t_{2}}dt \frac{1}{p}\frac{\sqrt{2} m \fronorm{\mlvec{M}}\fronorm{\mlvec{H}}}{Z} \sqrt{L(\mlvec\theta(0))} \exp(-at / 2)\\
    &\leq \frac{2}{a} \cdot \frac{1}{p}\frac{\sqrt{2} m \fronorm{\mlvec{M}}\fronorm{\mlvec{H}}}{Z} \sqrt{L(\mlvec\theta(0))} |\exp(-at_{1} / 2) - \exp(-at_{2}/2)|\\
    &\leq \frac{1}{p}\frac{\sqrt{2} m \fronorm{\mlvec{M}}\fronorm{\mlvec{H}}}{Z} \sqrt{L(\mlvec\theta(0))} |t_{1}-t_{2}|\\
  \end{align}
  The bounds on the $\ell_{2}$- and $\ell_{\infty}$-norm follows from direct computation.
\end{proof}

We are now ready to show $\mlvec{Y}(t_{2}) - \mlvec{Y}(t_{1})$ vanishes as
$p\rightarrow \infty$:
\begin{lemma}[Concentration during training, adapted from Lemma 3.5 in \cite{ourvqe2022}]
  \label{lm:concentration_train}
      Suppose that under learning rate $\eta=\frac{m}{p Z(\mlvec{H},d)}$, for
      all $0 \le t \le T$, the loss function $L(\mlvec\theta(t))$ decreases as $L(\mlvec\theta(0))\exp(-at)$
       then with probability $\geq 1-\delta$, for all $0 \leq t\leq T$:
      $\opnorm{\mlvec{Y}(\mlvec{\theta}(t)) - \mlvec{Y}(\mlvec{\theta}(0))} \leq C_3\cdot\frac{T}{\sqrt{p}}$,
      where $C_3$ is a constant of $T$ and $p$.
\end{lemma}
\begin{proof}
  To bound the supremum of the matrix-valued random field, we use an adapted
  version of the Dudley's inequality:

  \textit{Claim 1 (Dudley's inequality for matrix-valued random fields, adapted
    from Theorem 8.1.6 in High-dimensional probability (Vershynin, 2018).).} Let $\mlvec{\mathcal{R}}$ be a metric space equipped with a metric $\mathbf{d}(\cdot,\cdot)$, and $\mlvec{X}: \mlvec{\mathcal{R}} \mapsto \real^{D \times D}$ with subgaussian increments i.e. it satisfies
    $\Prob[\| \mlvec{X}(r_1) - \mlvec{X}(r_2)\|_{\mathsf{op}} > t] \le 2D\exp\left(-\frac{t^2}{C_{\sigma}^2\mathbf{d}(r_1,r_2)^2}\right)$.
  Then with probability at least $1 - 2D\exp(-u^2)$ for any subset $\mathcal{S} \subseteq \mlvec{\mathcal{R}}$:
   $
    \sup_{(r_1,r_2) \in \mathcal{S}} \|\mlvec{X}(r_1) - \mlvec{X}(r_2)\|_{\mathsf{op}} \le C \cdot C_{\sigma} \left[\int_0^{\mathrm{diam}(\mathcal{S})} \sqrt{\mathcal{N}(\mathcal{S},\mathbf{d},\epsilon)}\,d\epsilon + u \cdot \mathrm{diam}(\mathcal{S})\right]
   $
  for some constant $C$, where $\mathcal{N}(\mathcal{S},\mathbf{d},\epsilon)$ is the metric entropy defined as the logarithm of the $\epsilon$-covering number of $\mathcal{S}$ using metric $d$.

  To make use of \emph{Claim 1}, we now establish the sub-gaussian increment of
  $\mlvec{Y}(\mlvec\theta(t))$ through the following \emph{Claim 2} by applying \emph{McDiarmid inequality}:

  \textit{Claim 2 (Sub-gaussianity of $\mlvec{Y}$)}
    $\Prob[\opnorm{\mlvec{Y}(\mlvec{\theta}) - \mlvec{Y}(\mlvec{0})} > t] \le 2\exp\left(-\frac{-t^2 Z(\mlvec{H},d)^2}{2C_1\|\mlvec{\theta}\|_{2}^2}\right)$
    for some constant $C_{1}$. Then due to the Haar distribution of the unitaries $\{\mlvec{U}_{l}\}_{l=0}^{p}$,
    \begin{align}
    \Prob[\opnorm{\mlvec{Y}(\mlvec{\theta}_2) - \mlvec{Y}(\mlvec{\theta}_1)} > t] \le 2\exp\left(-\frac{-t^2 Z(\mlvec{H},d)^2}{2C_1\|\mlvec{\theta}_2 - \mlvec{\theta}_1\|_{2}^2}\right).
    \end{align}

  To see that \emph{Claim 2} is true, consider an alternative description of
  $\mlvec{Y}(\mlvec\theta)$. Recall that $\mlvec{Y}(\mlvec\theta)$ is defined as
  $\mlvec{Y}(\mlvec{\theta}) = \frac{1}{pZ(\mlvec{H},d)}\sum_{l=1}^{p} \mlvec{Y}_l$
  with$\mlvec{Y}_l(\mlvec{\theta})$ being $\mlvec{H}_l^{\otimes 2}$.
  We consider a re-parameterization of the random variables
  $\mlvec{H}_l(\theta)$ by constructing random variables that are identically
  distributed, but are functions on a different latent probability space.
  Defining $\mlvec{H}_{l}$ as
  $\mlvec{U}_{0}^{\dagger}\cdots \mlvec{U}_{l-1}^{\dagger}\mlvec{H}\mlvec{U}_{l-1}\cdots\mlvec{U}_{0}$,  $\mlvec{Y}$ can be rewritten as:
  \begin{align}
    \mlvec{Y}(\mlvec\theta) = \frac{1}{pZ} \sum_{l=1}^{p}
    \big(
    e^{i\theta_{1}\mlvec{H}_{1}}\cdots e^{i\theta_{l-1}\mlvec{H}_{l-1}}
    \mlvec{H}_{l}
    e^{-i\theta_{l-1}\mlvec{H}_{l-1}}\cdots e^{-i\theta_{1}\mlvec{H}_{1}}
    \big)^{\otimes 2}.
  \end{align}
  By the Haar randomness of $\{\mlvec{U}_{l}\}_{l=1}^{p}$, we can view
  $\{\mlvec{H}_{l}\}_{l=1}^{p}$ as random Hermitians generated by
  $\{\mlvec{V}_{l}\mlvec{H}\mlvec{V}^{\dagger}_{l}\}$ for \textit{i.i.d.} Haar
  random $\{\mlvec{V}_{l}\}_{l=1}^{p}$. This variable is identically distributed to $\mlvec{Y}$ and $\mlvec{Y}_{l}$ can be defined as each term in the sum.

We will apply the well-known McDiarmid inequality (e.g. Theorem~2.9.1 in High-dimensional probability (Vershynin, 2018)) that can be stated as follows: Consider independent random variables $X_1,\dots,X_k \in \mathcal{X}$. Suppose a random variable $\phi \colon \mathcal{X}^{k} \to \real$ satisfies the condition that for all $1 \le j \le k$ and for all $x_1,\dots,x_j,\dots,x_k,x'_j \in \mathcal{X}$,
\begin{align}
    |\phi(x_1,\dots,x_j,\dots,x_k) - \phi(x_1,\dots,x'_j,\dots,x_k)| \le c_j,
\end{align}
then the tails of the distribution satisfy
\begin{align}
    \Prob[|\phi(X_1,\dots,X_k) - \EXP\phi| \ge t] \le \exp\left(\frac{-2t^2}{\sum_{i=1}^{k}c_i^2}\right).
\end{align}

With our earlier re-parameterization we can consider $\mlvec{Y}$ and consequently $\mlvec{Y}_l$ as functions of the randomly sampled Hermitian operators $\mlvec{H}_l$. Define the variable $\mlvec{Y}^{(k)}$ as that obtained by resampling $\mlvec{H}_k$ independently, and $\mlvec{Y}_l^{(k)}$ correspondingly. Finally we define
\begin{align}
\Delta^{(k)} \mlvec{Y} = \opnorm{(\mlvec{Y}(\mlvec{\theta}) - \mlvec{Y}(0)) - (\mlvec{Y}^{(k)}(\mlvec{\theta}) - \mlvec{Y}^{(k)}(0))} = \opnorm{\mlvec{Y}(\mlvec{\theta}) - \mlvec{Y}^{(k)}(\mlvec{\theta})}.
\end{align}
Via the triangle inequality,
\begin{align}
    \Delta^{(k)} \mlvec{Y} &= \lVert \mlvec{Y}(\mlvec{\theta}) - \mlvec{Y}^{(k)}(\mlvec{\theta}) \rVert
    = \frac{1}{pZ}\lVert \sum_{l\geq k} \mlvec{Y}_{l}(\mlvec{\theta}) - \mlvec{Y}_{l}^{(k)}(\mlvec{\theta}) \rVert \\
    &\le \frac{1}{pZ} \sum_{l\geq k} \lVert \mlvec{Y}_{l}(\mlvec{\theta}) - \mlvec{Y}_{l}^{(k)}(\mlvec{\theta}) \rVert.
\end{align}
Then by definition,
    \begin{align*}
    &\|\mathbf{Y}_{l}(\mathbf{\theta}) - \mathbf{Y}_{l}^{(k)}(\boldsymbol{\theta})\| \nonumber\\
    =& \|(e^{\imagi\theta_1\mathbf{H}_1}\cdots e^{\imagi\theta_{k-1}\mathbf{H}_{k-1}})^{\otimes 2}
    \big((e^{\imagi{\theta}_{k}\mathbf{H}_{k}} \mathbf{K} e^{-\imagi{\theta}_{k}\mathbf{H}_{k}})^{\otimes 2}\nonumber\\
    -&(e^{\imagi{\theta}_{k}\mathbf{H}^{\prime}_{k}} \mathbf{K} e^{-\imagi{\theta}_{k}\mathbf{H}^{\prime}_{k}})^{\otimes 2}
     \big)
     (e^{-\imagi{\theta}_{k-1}\mathbf{H}_{k-1}}\cdots e^{-\imagi{\theta}_{1}\mathbf{H}_{1}})^{\otimes 2}\|\\
     =&\|(e^{\imagi{\theta}_{k}\mathbf{H}_{k}} \mathbf{K} e^{-\imagi{\theta}_{k}\mathbf{H}_{k}})^{\otimes 2} -(e^{\imagi{\theta}_{k}\mathbf{H}^{\prime}_{k}} \mathbf{K} e^{-\imagi{\theta}_{k}\mathbf{H}^{\prime}_{k}})^{\otimes 2} \|\\
     \leq&\|(e^{\imagi{\theta}_{k}\mathbf{H}_{k}} \mathbf{K} e^{-\imagi{\theta}_{k}\mathbf{H}_{k}})^{\otimes 2} - \mathbf{K}^{\otimes 2}\|+\|(e^{\imagi{\theta}_{k}\mathbf{H}^{\prime}_{k}} \mathbf{K} e^{-i{\theta}_{k}\mathbf{H}^{\prime}_{k}})^{\otimes 2} -\mathbf{K}^{\otimes2}\|.
\end{align*}
where $\mathbf{K}:=
    e^{\imagi{\theta}_{k+1}\mathbf{H}_{k+1}}\cdots e^{\imagi{\theta}_{l-1}\mathbf{H}_{l-1}}
    \mathbf{H}_{l}
    e^{-\imagi{\theta}_{l-1}\mathbf{H}_{l-1}}\cdots e^{-\imagi{\theta}_{k+1}\mathbf{H}_{k+1}}$.
    Let $\mathbf{K}(\phi)$ denote $e^{\imagi\phi\mathbf{H}_k}\mathbf{K} e^{-\imagi\phi\mathbf{H}_k}$, we can bound the first term on the righthand side as follows:
    \begin{align*}
        &\|(e^{\imagi{\theta}_{k}\mathbf{H}_{k}} \mathbf{K} e^{-\imagi{\theta}_{k}\mathbf{H}_{k}})^{\otimes 2} - \mathbf{K}^{\otimes 2}\|\\
        =& \| \mathbf{K}(\theta_k)^{\otimes 2} - \mathbf{K}(0)^{\otimes 2}\|\\
        =&\|\int_0^{\theta_k}d\phi \frac{d}{d\phi}(\mathbf{K}(\phi)^{\otimes 2})\|\\
        \leq&\int_0^{\theta_k}d\phi\| \frac{d}{d\phi}(\mathbf{K}(\phi)^{\otimes 2})\|\\
        \leq & 4|\theta_k|\|\mathbf{H}_k\|\|\mathbf{K}\|^2.
    \end{align*}
    The last inequality follows from the fact that
    \begin{align*}
        &\|\frac{d}{d\phi} \mathbf{K}(\phi)^{\otimes 2}\| \\
        =& \|(\exp(\imagi\phi\mathbf{H}_k))^{\otimes 2}
        \big(
        [-\imagi\mathbf{H}_k, \mathbf{K}]\otimes \mathbf{K}
        +
        \mathbf{K}\otimes[-\imagi\mathbf{H}_k, \mathbf{K}]
        \big)
        (\exp(-\imagi\phi\mathbf{H}_k))^{\otimes 2}\|\\
        =& \|
        [-\imagi\mathbf{H}_k, \mathbf{K}]\otimes \mathbf{K}
        +
        \mathbf{K}\otimes[-\imagi\mathbf{H}_k, \mathbf{K}]\|\\
        \leq& 4\|\mathbf{H}_k\|\|\mathbf{K}\|^2.
    \end{align*}
    The same reasoning holds for the term with $\mathbf{H}'_k$. Using the fact
    that $\|\mathbf{H}_k\|=\|\mathbf{H}'_k\| = \|\mathbf{H}\|$, and we have
    \begin{align*}
    \|\big(\mathbf{Y}_{l}(\boldsymbol{\theta}) - \mathbf{Y}_{l}(\boldsymbol{0})\big)
  - \big(\mathbf{Y}^{(k)}_{l}(\boldsymbol{\theta}) - \mathbf{Y}^{(k)}_{l}(\boldsymbol{0})\big)\|
  \leq 8|{\theta}_{k}|\|\mathbf{H}\|\|{\mathbf{K}\|^{2}
  = 8|\theta}_{k}|\|\mathbf{H}\|^{3}.
    \end{align*}
\emph{Claim 2} follows from the direct application of
McDiarmid inequality.


By Lemma~\ref{lm:qnn-slow-theta},
$\|\mlvec{\theta}(t_2) - \mlvec{\theta}(t_1)\|_2 \le \frac{C_{L}}{\sqrt{p}}|t_{2}-t_{1}|$
with $C_{L}$ being a constant with respect to $p$. Plugging this into
\textit{Claim 2}, we see that $\mlvec{Y}$ has sub-gaussian increments if we
define the metric
$\mathbf{d}(t_2,t_1) = \frac{C_{L}}{\sqrt{p}}\cdot |t_2 - t_1|$, thereby
satisfying the conditions for \textit{Claim 1}. Under this metric, the diameter
of the interval $[0,T]$ is of order $\frac{T}{\sqrt{p}}$. Applying \emph{Claim 1},
with $u = \sqrt{\log(2d/\delta)}$ to ensure a failure probability at most
$\delta$ we have
\begin{align}
  \sup_{t \in [0,T]} \|\mlvec{Y}(\theta(t)) - \mlvec{Y}(\theta(0))\|_{\mathsf{op}} \le C_3\cdot\frac{T}{\sqrt{p}},
\end{align}
where $C_3$ is a constant of $p$ and $T$ and depends polynomially on other
quantities including $d$ and $\log(1/\delta)$.
\end{proof}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main.tex"
%%% End:
