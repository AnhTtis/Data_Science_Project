\subsection{Experiment details}
\label{subsec:app_exp_details}
Our numerical experiments involve simulating both quantum neural networks and
the asymptotic dynamics.
\paragraph{QNN simulation}
We simulate the QNN experiments using
Pytorch~\citep{pytorchcite} with the periodic ansatze defined in
Definition~\ref{def:partial-ansatz}. The generating Hamiltonian $\mlvec{H}$ are
chosen to be a $d$-dimensional diagonal matrix with $d/2$ $\sqrt{d-d^{-1}}$ and $d/2$ $-\sqrt{d-d^{-1}}$ on
the diagonal (normalized such that $\tr(\mlvec{H^{2}})/(d^{2}-1)=1$). Each instance of the experiments is specified by the number of
samples $m$, system dimension $d$, number of parameters $p$ and the scaling
factor $\gamma$. A $m$-sample dataset is generated by randomly
sampled $m$ orthogonal pure states $\{\mlvec{v}_{i}\}_{i=1}^{m}\in\complex^{d}$ and randomly assigned
half of the samples with label $+1$ and the other half label $-1$ (i.e. $\{y_{i}\}_{i=1}^{m}\subset\{\pm 1\}^{m}$).

The optimizer we use is the standard gradient descent optimizer. To simulate the
dynamics of gradient flow, we choose the learning rate to be $0.001 / p$ and the
maximum number of epochs is set to be $10000$. We run the experiments on Amazon EC2 C5 Instances.
\paragraph{Asymptotic dynamics simulation}
Theorem~\ref{thm:qnn-mse-linear} allows us to examine the behavior of
QNN dynamics when $p\rightarrow \infty$ by studying the asymptotic dynamics:
\begin{align}
  \frac{d\mlvec{M}(t)}{dt} = - \eta\sum_{j=1}^{m}r_{j}[\mlvec{M}(t), [\mlvec{M}(t), \mlvec{\rho}_{j}]],
  \quad
  \text{where }
  \forall j\in[m], r_{j} := \tr(\mlvec{M}(t)\mlvec{\rho}_{j}) - y_{j}.
\end{align}
For a QNN asymptotic dynamics with number of samples $m$, system dimension $d$
and scaling
factor $\gamma$, we initialize $\mlvec{M}(0)$ as
\begin{align}
\gamma\mlvec{U}
\begin{bmatrix}
  +1 &  0 & \cdots & 0 & 0\\
   0 & +1 & \cdots & 0 & 0\\
   \vdots & \vdots & \ddots & \vdots & \vdots\\
   0 & 0 & \cdots & -1 & 0\\
   0 & 0 & \cdots & 0& -1
\end{bmatrix}
\mlvec{U}^{\dagger}
\end{align}
with $\mlvec{U}$ being a $d\times d$ haar random unitary. Similar to the QNN
simulation, the training set is chosen to be $m$ orthogonal pure states with labels randomly sampled from $\{\pm 1\}$.
The simulation of the asymptotic dynamics is run on Intel Core i7-7700HQ Processor (2.80Ghz) with 16G memory.
\subsection{\texorpdfstring{$\Kasym$}{Kasym} as a function of \texorpdfstring{$t$}{t}}
In Corollary~\ref{cor:onesample}, we see that the convergence rates for
one-sample QNNs change significantly during training.
Theorem~\ref{thm:qnn-mse-linear} allows us further verify this observation for
training sets with $m>1$ by simulating the asymptotic dynamics.

In Figure~\ref{fig:reKchg_vary}, we plot the relative change of the
$\mlvec{K}_{asym}(t)$ defined as
\begin{align}
  (\mlvec{K}_{asym}(t))_{ij}:=\tr\big(i[\mlvec{M}(t), \mlvec{\rho}_{i}] i [\mlvec{M}(t), \mlvec{\rho}_{j}]\big).
\end{align}
Each of the data point is averaged over 100 random initialization of
$\mlvec{M}(0)$. It is observed that $\mlvec{K}_{asym}(t)$ changes significantly
($\geq 5\%$) for each of the hyperparameters $d$, $m$ and $\gamma$. Therefore we
conclude that the deviation from the neural tangent kernel regression is
ubiquitous in general for practical settings. Particularly it rules out the
existing belief that the $d\rightarrow\infty$ alone can lead to a neural tangent
kernel-like behavior in QNNs. Same is observed for over-parameterized QNNs (Figure~\ref{fig:Kchg_vary_qnn})

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=.9\linewidth]{\imghome/reKchgI_varyd.png}\\
  \includegraphics[width=.91\linewidth]{\imghome/reKchgI_varygamma.png}
  \includegraphics[width=.91\linewidth]{\imghome/reKchgI_varym.png}
  \caption{
    Relative change of $\Kasym(t)$ in the QNN asymptotic dynamics
    for varying system dimension $d$, scaling factor $\gamma$ and number of
    training samples $m$.
    $\mlvec{K}_{asym}(t)$ changes significantly ($\geq 5\%$) throughout training.
  }
  \label{fig:reKchg_vary}
\end{figure}

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=.91\linewidth]{\imghome/QNN_K_scale2.png}
  \caption{
     Change of the $\lambda_{\min}(\Kasym(t))$ during the training in QNNs
     with $m=4, \gamma=2.0$ and varying $d$.
  }
  \label{fig:Kchg_vary_qnn}
\end{figure}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main.tex"
%%% End:
