Analogous to the classical logic gates, quantum gates are the basic building
blocks for quantum computing. A variational quantum circuit (also referred to as
an ansatz) is composed of 
parameterized quantum gates. A quantum neural network (QNN) is nothing but an
instantiation of learning with
parametric models using variational quantum circuits and quantum measurements:
A $p$-parameter $d$-dimensional QNN for a dataset $\{\mlvec{x}_{i},y_{i}\}$ is
specified by an encoding $\mlvec{x}_{i} \mapsto \mlvec\rho_{i}$ of the feature vectors into quantum states in an underlying
$d$-dimensional Hilbert space $\mathcal{H}$, a variational circuit
$\mlvec{U}(\mlvec{\theta})$ with real parameters $\mlvec{\theta}\in\real^{p}$, and a quantum
measurement $\mlvec{M}_{0}$. The predicted output $\hat{y}_{i}$ is obtained by
measuring $\mlvec{M}_{0}$ on the output $\mlvec{U}(\mlvec{\theta})\mlvec\rho_{i}\mlvec{U}^{\dagger}(\mlvec\theta)$.
Like deep neural networks, the parameters $\mlvec\theta$ in the variational circuits are
optimized by gradient-based methods to minimize an objective function that
measures the misalignments of the predicted outputs and the ground truth labels.

With the recent development of quantum
technology, the near-term Noisy Intermediate-Scale Quantum (NISQ) \citep{preskill2018quantum}
computer has become an important platform for demonstrating quantum advantage with practical applications.
As a hybrid of classical optimizers and quantum representations, QNNs is a
promising candidate for demonstrating such advantage on quantum computers
available to us in the near future: quantum machine learning models are proved
to have a margin over the classical counterparts in terms of the expressive
power due the to the exponentially large Hilbert space of
quantum states \citep{huang2021power, anschuetz2022critical}. On the other hand
by delegating the optimization procedures to classical computers, the hybrid
method requires significantly less quantum resources, which is crucial for
readily available quantum computers with limited coherence time and error
correction. There have been proposals of QNNs~\citep{Dunjko_2018,schuld2019quantum} for
classification~\citep{farhi2018classification,Romero_2017} and generative
learning~\citep{Lloyd_2018,Zoufal_2019,chakrabarti2019wass}.

Despite their potential there are challenges in the practical deployment of
QNNs. Most notably, the optimization problem for training QNNs can be highly
non-convex. The landscape of QNN training may be swarmed with spurious local
minima and saddle points that can trap gradient-based optimization methods
\citep{you2021exponentially, anschuetz2022quantum}.
QNNs with large dimensions also suffer from a phenomenon called the \emph{barren
plateau}~\citep{mcclean2018barren}, where the gradients of the parameters vanish
at random intializations, making convergence slow even in a trap-free landscape.
These difficulties in training QNNs, together with the challenge of
classically simulating QNNs at a decent scale, calls for a theoretical
understanding of the convergence of QNNs.

\paragraph{Neural Tangent Kernels}
Many of the theoretical difficulties in understanding QNNs have also been
encountered in the study of classical deep neural networks: despite the landscape of neural networks being non-convex and susceptible to
spurious local minima and saddle points, it has been empirically observed that the training errors decays exponentially in the training time~\citep{livni2014on, arora2019exact} in the highly \emph{over-parameterized} regime with sufficiently many number of trainable parameters.
This phenomenon is theoretically explained by connecting the training dynamics of neural networks to the kernel regression:
the kernel regression model generalizes the
linear regression by equipping the linear model with non-linear feature
maps. Given a training set
$\{\mlvec{x}_{j}, y_{j}\}_{j=1}^{m}\subset \mathcal{X}\times\mathcal{Y}$ and a
non-linear feature map $\phi:\mathcal{X}\rightarrow \mathcal{X}'$ mapping the
features to a potentially high-dimensional feature space $\mathcal{X}'$. The kernel
regression solves for the optimal weight $\mlvec{w}$ that minimizes the
mean-square loss
$\frac{1}{2m}\sum_{j=1}^{m}(\mlvec{w}^{T}\phi(\mlvec{x}_{j})-y_{j})^{2}$. The
name of kernel regression stems from the fact that the optimal hypothesis
$\mlvec{w}$ depends on the high-dimensional feature vectors
$\{\phi(\mlvec{x}_{j})\}_{j=1}^{m}$ through a $m\times m$ \emph{kernel} matrix
$\mlvec{K}$, such that $K_{ij}=\phi(\mlvec{x}_{i})^{T}\phi(\mlvec{x}_{j})$. The kernel regression enjoys a linear convergence (i.e. the mean square loss decaying exponentially over time) when $\mlvec{K}$ is positive definite.

The kernel matrix associated with a neural network is determined by tracking how the predictions for each training sample evolve jointly at random initialization. The study of the neural network convergence then reduces to characterizing the corresponding kernel matrices (the neural tangent kernel, or the NTK). 
In addition to the convergence results, NTK also serves as a tool for studying other aspect of neural networks including generalization \citep{canatar2021spectral, chen2020generalized} and stability \citep{bietti2019inductive}.

The key observation that justifies the study of neural networks with neural tangent kernels, is that the NTK becomes a constant (over time) during training in the limit of infinite layer widths. 
This has been theoretically established starting with the analysis of wide fully-connected neural networks~\citep{jacot2018neural,arora2019exact,chizat2018lazy} and later generalized to a variety of architectures (e.g. \citet{allenzhu2019convergence}).

\paragraph{Quantum NTKs}
Inspired by the success of NTKs, recent years have witnessed multiple works attempting to associate over-parameterized QNNs to kernel regression. 
Along the line there are two types of studies.
The first category investigates and compares the properties of the ``quantum'' kernel induced by the quantum encoding of classical features, where $K_{ij}$ associated with the $i$-th and $j$-th feature vectors $\mlvec{x}_{i}$ and $\mlvec{x}_{j}$ equals $\tr(\mlvec{\rho_{i}\mlvec{\rho}_{j}})$ with $\mlvec{\rho}_{i}$ and $\mlvec{\rho}_{j}$ being the quantum state encodings, without referring to the dynamics of training~\citep{schuld2019quantum, huang2021power, liu2021representation}.
The second category seeks to directly establish the quantum version of NTK for QNNs by examining the evolution of the model predictions at random initialization, which is the recipe for calculating the classical NTK in \citet{arora2019exact}:
\citet{shirai2021quantum} empirically evaluates the direct training of the quantum NTK instead of the original QNN formulation.
On the other hand, by analyzing the time derivative of the quantum NTK at initialization, \citet{liu2022analytic} conjectures that in the limit of over-parameterization, the quantum NTK is a constant over time and therefore the dynamics reduces to a kernel regression.

Despite recent efforts, a rigorous answer remains evasive whether the quantum NTK is a constant during training for over-parameterized QNNs. We show that the answer to this question is indeed, surprisingly negative: as a result of the unitarity of quantum circuits, there is a finite change in the conjectured quantum NTK as the training error decreases, even in the the limit of over-parameterization.

\paragraph{Contributions}
In this work, we focus on QNNs equipped with the mean square loss, trained using gradient flow, following~\citet{arora2019exact}.
In Section~\ref{sec:qnn-dynamics}, we show that, despite the formal resemblance to kernel
regression dynamics, the over-parameterized QNN does
not follow the dynamics of \emph{any} kernel regression due to the unitarity: for the widely-considered setting of classifications with Pauli
measurements, we show that the objective function at time $t$ decays at most as
a polynomial function of $1/t$ (Theorem~\ref{thm:sublinear-convergence}). This
contradicts the dynamics of any kernel regression with a positive definite kernel, which exhibits
convergence with $L(t)\leq L(0)\exp(-ct)$ for some  positive constant $c$. We also identify the true asymptotic dynamics of QNN training as regression with a time-varying Gram matrix $\Kasym$ (Lemma~\ref{lm:resid_dyn_decomp}), and show rigorously that the real dynamics concentrates to the asymptotic one in the limit $p \rightarrow \infty$ (Theorem~\ref{thm:qnn-mse-linear}). This reduces the problem of investigating QNN convergence to studying the convergence of the asymptotic dynamics governed by $\Kasym$. 

We also consider a model of QNNs where the final measurement is post-processed by a linear scaling.
In this setting, we provide a complete analysis of the convergence of the asymptotic dynamics in the case of $1$ training sample (Corollary~\ref{cor:onesample}), and provide further theoretical evidence of convergence in the neighborhood of most global minima when the number of samples $m > 1$ (Theorem~\ref{thm:smallest_eig}). These theoretical evidences are supplemented with an empirical study that demonstrates in generality, the convergence of the asymptotic dynamics when $m \ge 1$. Coupled with our proof of convergence, these form the strongest concrete evidences of the convergence of training for over-parameterized QNNs.

\paragraph{Connections to previous works}
Our result extends the existing literature on QNN landscapes (e.g. \citet{anschuetz2022critical,russell2016quantum}) and looks into the training dynamics, which allows us to characterize the rate of convergence and to show how the range of the measurements affects the convergence to global minima.
The dynamics for over-parameterized QNNs proposed by us can be reconciled with the existing calculations of quantum NTK as follows: in the regime of over-parameterization, the QNN dynamics coincides with the quantum NTK dynamics conjectured in \citet{liu2022analytic} at random initialization; yet it deviates from quantum NTK dynamics during training, and the deviation does not vanish in the limit of $p\rightarrow\infty$.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main.tex"
%%% End:
