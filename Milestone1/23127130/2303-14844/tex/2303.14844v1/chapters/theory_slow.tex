\paragraph{Sublinear convergence in QNNs}
One of the most common choices for designing QNNs is to use a (tensor product of)
Pauli matrices as the measurement (see e.g.
\citet{farhi2018classification,Dunjko_2018}). Such a choice
features a measurement $\qnnmeasure$ with eigenvalues $\{\pm 1\}$ and trace zero. Here we
show that in the setting of supervised learning on pure states with Pauli measurements,
the (neural tangent) kernel regression is insufficient to capture the convergence of
QNN training. For the kernel regression with a positive definite kernel $\mlvec{K}$,
the objective function $L$ can be expressed as
$\frac{1}{2m}\sum_{j=1}^{m}(\hat{y}_{j}-y_{j})^{2} = \frac{1}{2m}\mlvec{r}^{T}\mlvec{r}$; under the kernel dynamics of
$\frac{d\mlvec{r}}{dt} = - \frac{\eta}{m} \mlvec{K}\mlvec{r}$, it is easy to verify that
$\frac{d \ln L}{dt} = -\frac{2\eta}{m}\frac{\mlvec{r}^{T}\mlvec{K}\mlvec{r}}{\mlvec{r}^{T}\mlvec{r}}\leq -\frac{2\eta}{m}\lambda_{\min}(\mlvec{K})$
with $\lambda_{\min}(\mlvec{K})$ being the smallest eigenvalue of $\mlvec{K}$.
This
indicates that $L$ decays at a linear rate, i.e. $L(T) \leq L(0) \exp(-\frac{2\eta}{m}\lambda_{\min}(\mlvec{K})T)$.
In contrast,
we show that the rate of convergence of the QNN dynamics \emph{must} be sublinear,
slower than the linear convergence rate predicted by the kernel regression model
with a positive definite kernel.
\begin{restatable}[No faster than sublinear convergence]{theorem}{qnnsublinear}
 \label{thm:sublinear-convergence}
 Consider a QNN instance with a training set
 $\mathcal{S}=\{(\mlvec{\rho}_{j}, y_{j})\}$ such that $\mlvec{\rho}_{j}$ are
 pure states and $y_{j}\in\{\pm 1\}$, and a
 measurement $\qnnmeasure$ with eigenvalues in $\{\pm 1\}$. Under the gradient
 flow for the objective function $L(\mlvec\theta)=\frac{1}{2m}\sum_{j=1}^{m}\tr(\mlvec\rho_{j}\mlvec{M}(\mlvec\theta)-y_{j})^{2}$, for any ansatz $\mlvec{U}(\mlvec\theta)$ defined
in Line~(\ref{eq:general-ansatz}), $L$ converges to $zero$ at most at a
sublinear convergence rate.
More concretely, for $\mlvec{U}(\mlvec\theta)$ generated by $\{\generatorH{l}\}_{l=1}^{p}$,
let $\eta$ be the learning rate and $m$ be the sample size, the objective
function at time $t$:
\begin{align}
L(\mlvec{\theta}(t)) \geq 1 / (c_{0} + c_{1}t)^{2}.
\end{align}
Here the constant $c_{0} = 1/\sqrt{L(\mlvec\theta(0))}$ depends on the objective function at
initialization, and $c_{1}=12\eta\sum_{l=1}^{p}\opnorm{\generatorH{l}}^{2}$.
\end{restatable}
The constant $c_{1}$ in the theorem depends on the number of parameters $p$
through $\sum_{l=1}^{p}\opnorm{\generatorH{l}}^{2}$ if the operator norm of
$\generatorH{l}$ is a constant of $p$. We can get rid of the dependency on $p$
by scaling the learning rate $\eta$ or changing the time scale, which does not
affect the sublinearity of convergence.

By expressing the objective function $L(\varytheta)$ as
$\frac{1}{2m}\mlvec{r}(\varytheta)^{T}\mlvec{r}(\varytheta)$,
Lemma~\ref{lm:qnndynamics} indicates that the decay of
$\frac{d L(\varytheta)}{dt}$ is lower-bounded by
$\frac{-2\eta}{m}\lambda_{\max}(\mlvec{K}(\varytheta)) L(\varytheta)$, where
$\lambda_{\max}(\cdot)$ is the largest eigenvalue of a Hermitian matrix.
The full proof of Theorem~\ref{thm:sublinear-convergence} is deferred to
Section~\ref{subsec:slow_proof}, and follows from the fact that
when the QNN prediction for an input state $\mlvec{\rho}_{j}$ is close to the ground truth
$y_{j}= 1$ or $-1$, the diagonal entry $K_{jj}(\varytheta)$ vanishes. As a
result the largest eigenvalue $\lambda_{\max}(\mlvec{K}(\varytheta))$ also
vanishes as the objective function $L(\varytheta)$ approaches $0$ (which is the
global minima). Notice the sublinearity of convergence is independent of the
system dimension $d$, the choices of $\{\generatorH{l}\}_{l=1}^{p}$ in
$\mlvec{U}(\mlvec\theta)$ or the number of parameters $p$. This means that
the dynamics of QNN training is completely different from kernel regression even
in the limit where $d$ and/or $p\rightarrow\infty$.

\paragraph{Experiments: sublinear QNN convergence} To support
Theorem~\ref{thm:sublinear-convergence}, we simulate the training of QNNs using
$\qnnmeasure$ with eigenvalues $\pm 1$. For dimension $d=32$ and $64$, we
randomly sample four $d$-dimensional pure states that are orthogonal, with two
of samples labeled $+1$ and the other two labeled $-1$.
The training curves (plotted under the log scale) in Figure~\ref{fig:scale1_varyp} flattens as $L$
approaches $0$, suggesting the rate of convergence $-d\ln L/dt$ vanishes around
global minima, which is a signature of the sublinear convergence. Note that the
sublinearity of convergence is independent of the number of parameters $p$.
For gradient flow or gradient descent with sufficiently small step-size, the
scaling of a constant learning rate $\eta$ leads to a scaling of time $t$ and
does not fundamentally change the (sub)linearity of the convergence. For the
purpose of visual comparison, we scale $\eta$ with $p$ by choosing the learning
rate as $10^{-3} / p$. For more details on the experiments, please refer to
Section~\ref{sec:app_exp}.


\begin{figure}[!htbp]
  \centering
  \includegraphics[width=.7\linewidth]{\imghome/m4g1-crop.pdf}
  \caption{Sublinear convergence of QNN training. For QNNs with Pauli
    measurements for a classification task, the (log-scaled) training curves
    flatten as the number of iterations increases, indicating a sublinear
    convergence. The flattening of training curves remains for increasing
    numbers of parameters $p=10, 20, 40, 80$. The training curves are averaged
    over 10 random initialization, and the error bars are the halves of standard
    deviations.}
  \label{fig:scale1_varyp}
\end{figure}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main.tex"
%%% End:
