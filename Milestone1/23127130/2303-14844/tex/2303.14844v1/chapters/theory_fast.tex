As demonstrated in the previous section, the dynamics of the QNN training deviates
from the kernel regression for any choices of the number of parameters $p$ and the
dimension $d$ in the setting of Pauli measurements for classification. This
calls for a new characterization of the QNN dynamics in the regime of over-parameterization.
For a concrete definition of over-parameterization, we consider the family of the periodic
ansatze in Definition~\ref{def:partial-ansatz}, and refer to the limit of
$p\rightarrow\infty$ with a fixed generating Hamiltonian $\mlvec{H}$ as the
regime of over-parameterization.
In this section, we derive the asymptotic dynamics of QNN training when number
of parameters $p$ in the periodic ansatze goes to infinity. We start
by decomposing the dynamics of the residual $\varyr$ into a term corresponding
to the asymptotic dynamics, and a term of perturbation that vanishes as
$p\rightarrow\infty$.
As mentioned before, in the context of the gradient flow, the choice of
$\eta$ is merely a scaling of the time and therefore arbitrary.
For a QNN instance with $m$ training samples and
a $p$-parameter ansatz generated by a Hermitian $\mlvec{H}$ as defined in
Line~(\ref{eq:partial-ansatz}), we choose $\eta$ to be
$\frac{m}{p}\frac{d^{2}-1}{\tr(\mlvec{H}^{2})}$ to facilitate the presentation:
\begin{restatable}[Decomposition of the residual dynamics]{lemma}{decompresid}
  \label{lm:resid_dyn_decomp}
Let $\mathcal{S}$ be a training set with $m$ samples
$\{(\mlvec\rho_{j}, y_{j})\}_{j=1}^{m}$, and let $\mlvec{U}(\mlvec\theta)$ be
a $p$-parameter ansatz generated by a non-zero $\mlvec{H}$ as in Line~\ref{eq:partial-ansatz}.
Consider a QNN instance with a training set $\mathcal{S}$, ansatz
$\mlvec{U}(\mlvec\theta)$ and a measurement $\qnnmeasure$.  Under the
gradient flow with $\eta = \frac{m}{p}\frac{d^{2}-1}{\tr(\mlvec{H}^{2})}$, the
residual vector $\mlvec{r}(t)$ as a function of time $t$ through $\varytheta$
evolves as
\begin{align}
  \frac{d\mlvec{r}(t)}{dt} = - (\Kasym(t) + \Kpert(t)) \mlvec{r}(t)
\end{align}
where both $\Kasym$ and $\Kpert$ are functions of time through the parameterized measurement
$\varyM$, such that
\begin{align}
  (\Kasym(t))_{ij} &:= \tr\big(\imagi[\mlvec{M}(t), \mlvec{\rho}_{i}] \ \imagi[\mlvec{M}(t), \mlvec{\rho}_{j}]\big),\\
  (\Kpert(t))_{ij} &:= \tr\big(\imagi[\mlvec{M}(t), \mlvec{\rho}_{i}]
                      \otimes \imagi[\mlvec{M}(t), \mlvec{\rho}_{j}] \Delta(t)\big).
\end{align}
Here $\Delta(t)$ is a $d^{2}\times d^{2}$ Hermitian as a function of $t$ through $\mlvec\theta(t)$.
\end{restatable}
Under the random initialization by sampling $\{\mlvec{U}_{l}\}_{l=1}^{p}$ i.i.d.
from the haar measure over the special unitary group $SU(d)$,  $\Delta(0)$
concentrates at zero as $p$ increases. We further show that
$\Delta(t)-\Delta(0)$ has a bounded operator norm decreasing with number of
parameters. This allows us to associate the convergence of
the over-parameterized QNN with the properties of $\Kasym(t)$:
\begin{restatable}[Linear convergence of QNN with mean-square loss]{theorem}{qnnmselinear}
\label{thm:qnn-mse-linear}
Let $\mathcal{S}$ be a training set with $m$ samples
$\{(\mlvec\rho_{j}, y_{j})\}_{j=1}^{m}$, and let $\mlvec{U}(\mlvec\theta)$ be
a $p$-parameter ansatz generated by a non-zero $\mlvec{H}$ as in Line~(\ref{eq:partial-ansatz}).
Consider a QNN instance with the training set $\mathcal{S}$, ansatz
$\mlvec{U}(\mlvec\theta)$ and a measurement $\qnnmeasure$, trained by
gradient flow with $\eta = \frac{m}{p}\frac{d^{2}-1}{\tr(\mlvec{H}^{2})}$.
Then for sufficiently large number of parameters $p$, if the smallest eigenvalue of
$\Kasym(t)$ is greater than a constant $C_{0}$, then
with high probability over the random initialization of the periodic
ansatz, the loss function converges to zero at a linear rate
\begin{align}
  L(t) \leq L(0) \exp(-\frac{C_{0}t}{2}).
\end{align}
\end{restatable}
We defer the proof to Section~\ref{subsec:thm_qnn_mse_linear}. Similar to $\mlvec{r}(t)$, the
evolution of $\mlvec{M}(t)$ decomposes into an asymptotic term
\begin{align}
\frac{d}{dt}\mlvec{M}(t) = \sum_{j=1}^{m}r_{j}[\mlvec{M}(t), [\mlvec{M}(t),\mlvec\rho_{j}]]\label{eq:m_asymp_dynamics}
\end{align}
and a
perturbative term depending on $\Delta(t)$. Theorem~\ref{thm:qnn-mse-linear}
allows us to study the behavior of an over-parameterized QNN by simulating/characterizing the
asymptotic dynamics of $\mlvec{M}(t)$, which is significantly more accessible.

\paragraph{Application: QNN with one training sample}
To demonstrate the proposed asymptotic dynamics as a tool for analyzing
over-parameterized QNNs, we study the convergence of the QNN with one training
sample $m=1$. To set a separation from the regime of the sublinear convergence,
consider the following setting: let
$\qnnmeasure$ be a Pauli measurement, for any input
state $\mlvec{\rho}$, instead of assigning
$\hat{y}=\tr(\mlvec\rho\mlvec{U}(\mlvec\theta)^{\dagger}\qnnmeasure\mlvec{U}(\mlvec\theta))$,
take
$\gamma\tr(\mlvec\rho\mlvec{U}(\mlvec\theta)^{\dagger}\qnnmeasure\mlvec{U}(\mlvec\theta))$
as the prediction $\hat{y}$ at $\mlvec\theta$ for a scaling factor
$\gamma > 1.0$.
The $\gamma$-scaling of the measurement outcome can be viewed as a classical
processing in the context of quantum information, or as an activation function
(or a link function) in the context of machine learning, and is equivalent to a
QNN with measurement $\gamma\qnnmeasure$. The following corollary
implies the convergence of 1-sample QNN for $\gamma > 1.0$ under a mild
initial condition:
\begin{corollary}
  \label{cor:onesample}
  Let $\mlvec{\rho}$ be a $d$-dimensional pure state, and let $y$ be $\pm 1$.
  Consider a QNN instance with a Pauli measurement $\qnnmeasure$, an one-sample training set
  $\mathcal{S} = \{(\mlvec\rho, y)\}$ and an ansatz
  $\mlvec{U}(\mlvec\theta)$ defined in Line~(\ref{eq:partial-ansatz}). Assume the
  scaling factor $\gamma > 1.0$ and $p\rightarrow\infty$ with
  $\eta = \frac{d^{2}-1}{p\tr(\mlvec{H}^{2})}$. Under the initial condition that
  the prediction at $t=0$,
  $\hat{y}(0)$ is less than 1, the objective function converges linearly with
  \begin{align}
    L(t) \leq L(0) \exp(-C_{1}t)
  \end{align}
  with the convergence rate $C_{1} \geq \gamma^{2}-1$.
\end{corollary}
With a scaling factor $\gamma$ and training set
$\{(\mlvec\rho_{j}, y_{j})\}_{j=1}^{m}$, the objective function, as a
function of the parameterized measurement $\mlvec{M}(t)$, reads as:
$L(\mlvec{M}(t)) = \frac{1}{2m}\sum_{j=1}^{m}(\gamma\tr(\mlvec\rho_{j}\mlvec{M}(t)) - y_{j})^{2}$.
As stated in Theorem~\ref{thm:qnn-mse-linear}, for sufficiently large number of
parameters $p$, the convergence rate of the residual $\mlvec{r}(t)$ is
determined by $\Kasym(t)$, as the asymptotic dynamics of $\mlvec{r}(t)$ reads as
  $\frac{d}{dt}\mlvec{r} = -\Kasym(\mlvec{M}(t))\mlvec{r}(t)$ with the chosen $\eta$.
For $m = 1$, the asymptotic matrix $\Kasym$ reduces to a scalar
$k(t) = -\tr([\gamma\mlvec{M}(t),\mlvec\rho]^{2}) = 2(\gamma^{2}-\hat{y}(t)^{2})$.
$\hat{y}(t)$ approaches the label $y$ if $k(t)$ is strictly positive, which is
guaranteed for $\hat{y}(t) < \gamma$. Therefore $|\hat{y}(0)|<1$ implies that
$|\hat{y}(t)|<1$ and $k(t)\geq 2(\gamma^{2}-1)$ for all $t>0$.

In Figure~\ref{fig:onesample} (top), we plot the training curves of one-sample QNNs with
$p=320$ and
varying $\gamma = 1.2, 1.4, 2.0, 4.0, 8.0$ with the same learning rate
$\eta=1e-3/p$. As predicted in Corollary~\ref{cor:onesample}, the rate of convergence increases with the
scaling factor $\gamma$.
The proof of the corollary additionally implies that $k(t)$ depends on
$\hat{y}(t)$: the convergence rate changes over time as the prediction $\hat{y}$
changes. Therefore, despite the linear convergence, the dynamics is different from that of kernel
regression, where the kernel remains constant during training in the limit $p\rightarrow\infty$.

In Figure~\ref{fig:onesample} (bottom), we plot the empirical rate of convergence
$-\frac{d}{dt}\ln L(t)$ against the rate predicted by $\hat{y}$. Each data
point is calculated for QNNs with different $\gamma$ at different time steps by
differentiating the logarithms of the training curves. The scatter plot displays
an approximately linear dependency, indicating the proposed asymptotic dynamics is
capable of predicting how the convergence rate changes during training, which is
beyond the explanatory power of the kernel regression model. Note that the slope
of the linear relation is not exactly one. This is because we choose a learning rate much
smaller than $\eta$ in the corollary statement to simulate the dynamics of gradient flow.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.65\linewidth]{\imghome/m1-crop.pdf}
  \caption{
    (Top) The training curves of one-sample QNNs with varying $\gamma$. The
    smallest convergence rate $-d\ln L / dt$ during training (i.e. the slope of
    the training curves under the log scale) increases with $\gamma$.
    (Bottom) The convergence rate $-d\ln L/dt|_{t=T}$ as a function of
    $2(\gamma^{2}-\hat{y}^{2}(T))$ (jointly scaled by
    $1/\gamma^{2}$ for visualization) are
    evaluated at different time steps $T$ for different $\gamma$. The approximately
    linear dependency shows that the proposed dynamics captures the QNN
    convergence beyond the explanatory power of the kernel regressions.
  }
  \label{fig:onesample}
\end{figure}
QNNs with one training sample have been considered before (e.g.
\cite{liu2022analytic}), where the linear convergence has been shown under the assumption of ``frozen QNTK", namely assuming $\mlvec{K}$,
the time derivative of the log residual remains almost constant throughout training. In the corollary above, we provide an
end-to-end proof for the one-sample linear convergence without assuming a frozen
$\mathbf{K}$. In fact, we observe that in our setting
$\mathbf{K} = 2(\gamma^2 - \hat{y}(t))$ changes with $\hat{y}(t)$ (see also
Figure~\ref{fig:onesample}) and is therefore not frozen.


\paragraph{QNN convergence for $m>1$}
To characterize the convergence of QNNs with $m>1$, we
seek to empirically study the asymptotic dynamics in Line~(\ref{eq:m_asymp_dynamics}). According to
Theorem~\ref{thm:qnn-mse-linear}, the (linear) rate of convergence is
lower-bounded by the smallest eigenvalue of $\Kasym(t)$, up to an constant scaling. In
Figure~\ref{fig:smallest_eig}, we simulate the asymptotic dynamics with various
combinations of $(\gamma, d, m)$, and evaluate the smallest eigenvalue of
$\Kasym(t)$ throughout the dynamics (Figure~\ref{fig:smallest_eig}, details
deferred to Section~\ref{sec:app_exp}). For sufficiently large dimension $d$,
the smallest eigenvalue of $\Kasym$ depends on the ratio between the number of
samples and the system dimension $m/d$ and is proportional to the square of the
scaling factor $\gamma^{2}$.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=.7\linewidth]{\imghome/kasym_min-crop.pdf}
  \caption{
    The smallest eigenvalue of $\Kasym$ for the asymptotic dynamics with
    varying system dimension $d$, scaling factor $\gamma$ and number of
    training samples $m$. For sufficiently large $d$, the smallest eigenvalue
    depends on the ratio $m/d$ and is proportional to the square of the scaling factor $\gamma^{2}$.
  }
  \label{fig:smallest_eig}
\end{figure}

Empirically, we observe that the smallest convergence rates for training QNNs
are obtained near the global minima (See
Figure~\ref{fig:Kchg_vary_qnn} in the appendix), suggesting the bottleneck of convergence occurs when $L$ is small. 

We now give theoretical evidence that, at most of the global minima, the eigenvalues of $\Kasym$ are lower bounded by $2\gamma^2(1-1/\gamma^2 - O(m^2/d))$, suggesting a linear convergence in the neighborhood of these minima. 
To make this notion precise, we define the uniform measure over global minima as follows:
consider a set of pure input states $\{\mlvec\rho_{j}=\mlvec{v}_{j}\mlvec{v}_{j}^{\dagger}\}_{j=1}^{m}$ that are  mutually
orthogonal (i.e. $\mlvec{v}_{i}^{\dagger}\mlvec{v}_{j}=0$ if $i\neq j$).
For a large dimension $d$, the global minima of the asymptotic dynamics is achieved when the
objective function is $0$. Let $\mlvec{u}_{j}(t)$ (resp. $\mlvec{w}_{j}(t)$)
denote the components of $\mlvec{v}_{j}$ projected to the positive (resp. negative)
subspace of the measurement $\mlvec{M}(t)$ at the global minima. Recall that for a $\gamma$-scaled QNN with a
Pauli measurement, the predictions $\hat{y}(t) = \gamma\tr(\rho_{j}\mlvec{M}(t)) = \gamma(\mlvec{u}_{j}^{\dagger}(t)\mlvec{u}_{j}(t)-\mlvec{w}_{j}^{\dagger}(t)\mlvec{w}_{j}(t))$.
At the global minima, we have $\mlvec{u}_{j}(t) = \frac{1}{2}(1\pm 1/\gamma)\hat{\mlvec{u}}_{j}(t)$ for some unit
vector $\hat{\mlvec{u}}_{j}(t)$ for the $j$-th training sample with label $\pm 1$. On the other hand, given a set of unit vectors
$\{\hat{\mlvec{u}}_{j}\}_{j=1}^{m}$ in the positive subspace, there is a corresponding set of $\{\mlvec{u}_{j}(t)\}_{j=1}^{m}$ and
$\{\mlvec{w}_{j}(t)\}_{j=1}^{m}$ such that $L=0$ for sufficiently large $d$. By uniformly and independently
sampling a set of unit vectors $\{\hat{\mlvec{u}}_{j}\}_{j=1}^{m}$ from the
$d/2$-dimensional subspace associated with the positive eigenvalues of
$\mlvec{M}(t)$, we induce a uniform distribution over all the global minima.
The next theorem characterizes $\Kasym$ under such an induced uniform distribution over all the global minima:
\begin{restatable}{theorem}{asympeig}
\label{thm:smallest_eig}
Let $\mathcal{S}=\{(\mlvec\rho_{j}, y_{j})\}_{j=1}^{m}$ be a training set with
orthogonal pure states $\{\mlvec\rho_{j}\}_{j=1}^{m}$ and equal number of
positive and negative labels $y_{j}\in\{\pm 1\}$.
Consider the smallest eigenvalue $\lambda_{g}$ of $\Kasym$ at the global minima of the asymptotic dynamics of an over-parameterized
QNN with the training set $\mathcal{S}$, scaling factor $\gamma$ and system dimension $d$. With probability
$\geq 1 - \delta$ over the uniform
measure over all the global minima
\begin{align}
  \lambda_{g}\geq 2\gamma^{2}(1-\frac{1}{\gamma^{2}}-C_{2}\max\{\frac{m^{2}}{d}, \frac{m}{d}\log\frac{2}{\delta}\}),
\end{align}
which is strictly positive for large $\gamma > 1$ and $d=\Omega(\mathsf{poly}(m))$.
Here $C_{2}$ is a positive constant.
\end{restatable}
We defer the proof of Theorem~\ref{thm:smallest_eig} to Section~\ref{sec:app_globalminima} in the appendix. A similar notion of a uniform measure over global minima was also used in \citet{canatar2021spectral}.
Notice that the uniformness is dependent on the parameterization of the global minima, and the uniform measure over all the global minima is not necessarily the
measure induced by random initialization and gradient-based training. Therefore
Theorem~\ref{thm:smallest_eig} is not a rigorous depiction of the distribution of convergence rate for a randomly-initialized over-parameterized QNN.
Yet the prediction of the theorem aligns well with the empirical observations in Figure~\ref{fig:smallest_eig} and
suggests that by scaling the QNN measurements, a faster convergence can be achieved:
In Figure~\ref{fig:scale2_varyp}, we simulate $p$-parameter QNNs with dimension $d = 32$ and
$64$ with a scaling factor $\gamma = 4.0$ using the same setup as in
Figure~\ref{fig:scale1_varyp}. The training early stops  when the average $L(t)$ over the random seeds is less than $1\times 10^{-2}$.
In contrast to Figure~\ref{fig:scale1_varyp}, the convergence rate $-d\ln L/dt$
does not vanish as $L\rightarrow 0$, suggesting a simple (constant) scaling of the measurement outcome can lead to convergence within much fewer number of iterations. 
\begin{figure}[!htbp]
  \centering
  \includegraphics[width=.7\linewidth]{\imghome/m4g4-crop.pdf}
  \caption{
    Training curves of QNNs with $\gamma=4.0$ for learning a $4$-sample dataset
    with labels $\pm 1$. For $p=10, 20, 40, 80$, the rate of convergence is
    greater than $0$ as $L\rightarrow 0$, and it takes less than $1000$
    iterations for $L$ in most of the instances to convergence below
    $1\times 10^{-2}$. In contrast, in Figure~\ref{fig:scale1_varyp},
    $L > 1\times 10^{-1}$ after $10000$ iterations despite the increasing number
    of parameters.
  }
  \label{fig:scale2_varyp}
\end{figure}

Another implication of Theorem~\ref{thm:smallest_eig} is the deviation of QNN dynamics from any kernel regressions. By straight-forward calculation, the normalized matrix $\Kasym(0) / \gamma^2$  at the random initialization is independent of the choices of $\gamma$. In contrast, the typical value of $\lambda_g / \gamma^2$ in Theorem~\ref{thm:smallest_eig} is dependent on $\gamma^2$, suggesting non-negligible changes in the matrix $\Kasym(t)$ governing the dynamics of $\mlvec{r}$ for finite scaling factors $\gamma$. Such phenomenon is empirically verified in Figure~\ref{fig:reKchg_vary} in the appendix.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main.tex"
%%% End:
