\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/skip_analysis.pdf}
    \caption{Left: Commonly used skip connections in neural networks. Right: Skip connections investigation results.}
    \label{fig:skip_types}
\end{figure*}
\section{Related Works}
\ac{ANN}s use gradient descent to optimize the weights during training, but this has proven challenging in \ac{SNN} due to their non-differentiable nonlinear spikes \cite{Neftci19_51}. %Zenke, Tim P. Vogels.
A solution to this problem was to introduce surrogate derivatives during the backpropagation (BP) operation, which replaces the spiking activations with a smooth function \cite{Zenke21_899}.
While this has been successful at improving the accuracy of SNNs in solving certain problems, their performance often falls short compared to that of ANNs.
The reason for this discrepancy is twofold: 1) the surrogate gradient approaches only provide an approximation of the gradients
%and thus the weight update may not be optimal to bring the loss to the minimum 
and 2) the unfolding of SNNs in time to perform the backpropagation BP leads to the vanishing gradient problem, similar to the problem faced in vanilla Recurrent Neural Networks (RNNs).
%%
A study such as in \cite{Wunderlich21_12829} provides a method that computes exact gradients for arbitrary SNN architecture, but their applications were limited to relatively simple datasets, e.g., MNIST.
%%
%Solving the vanishing gradient problem in SNNs is particularly interesting since this type of network is intended to operate in real-time and can receive long temporal sequences.
%%
The incorporation of local losses in space \cite{Kaiser20_424} and time \cite{Zenke18_1541} has shown promising results in circumventing the vanishing gradient problem, but as these methods only roughly approximate the gradients, they lead to less competitive performance.
%Although, note that these methods were intentionally designed to help reduce the computational complexity of BP in neuromorphic hardware and not necessarily to solve this problem.
%%
To further improve the learning in SNNs, other studies took a different approach by introducing novel architectures and ingredients. For example, the work in \cite{Kim21_1638} showed that batch normalization through time could effectively train deep SNN.
%\added[id=YB]{
Another example is presented in%} 
\cite{DBLP:conf/eccv/KimLPVP22}, %\added[id=YB]{
where the authors implemented a dedicated neural architecture search (NAS) to find the best architecture within common NAS benchmarks, such as NAS-Bench-201. While their methodology is promising, we found that adapting ANN standard architectures such as resnet18, or densenet, yields better accuracies in less search time.
%%
In our study, we adapt standard architectures and explore the effect of skip connections on learning in SNNs.
%%  
%We test our approach on static images, e.g., CIFAR-10 as well as on event-stream datasets, e.g., CIFAR-10 DVS \cite{Li2017}, DVS128 GESTURE \cite{DVS_gesture_8100264} that are either generated or captured with low-power event-based sensors. 
%These datasets incorporate the time dimension, which can later be interpreted as spikes.

