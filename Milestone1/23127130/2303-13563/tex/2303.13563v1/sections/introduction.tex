\section{Introduction}
%\begin{itemize}
%    \item What are the benefits of SNNs
%    \item Problem: hard to train SNNs
%    \item In this project, we exploit different skip connections architectures to optimize the learning in SNNs
%\end{itemize}

While \ac{DL} has become a foundational technology to many applications, including autonomous driving, and medical imagining segmentation. Its inference is still energy-, resource- and time-expensive.

An ongoing challenge in \ac{DL} research is to devise energy-efficient inference algorithms and hardware platforms. Taking inspiration from the brain, two intertwined solutions are investigated: (1) designing neuromorphic hardware, in which memory and computations reside within the same node and (2) building bio-inspired networks such as \ac{SNN}, which can be more energy efficient than nonspiking neural networks. 
In this paper, we refer to this latter as \ac{ANN}. 

Although \ac{ANN} are brain-inspired, there are fundamental differences in their computations and learning mechanism compared to the brain. 
In the brain, neurons communicate with each other via sequences of an action potential, also known as \textit{spikes}. 
An action potential travels along the axon in a neuron and activates synapses. 
The synapses release neurotransmitters that arrive at the postsynaptic neuron. 
Here, the action potential rises with each incoming pulse of neurotransmitters. 
If the action potential reaches a certain threshold the postsynaptic neuron fires a spike itself. 
These individual spikes are sparse in time and space, which is the main reason behind the energy efficiency of biological neuronal networks, i.e., SNNs.
In addition, the information in SNN is conveyed by spike times and thus can have a large capacity for encoding and representing the input data at the edge.
%}{
%so each spike has high information content. 
%It is this mechanism that leads to the realm of \ac{SNN}. Thus, information in \ac{SNN} is conveyed by spike timing, including latencies, and
%spike rates. 
%This sparsity is the main reason for energy savings. 
%}

However, \ac{SNN} architecture design and training are still in their early phases. An important scientific question is to what extent architecture characteristics (e.g., operations, skip connections) are compliant with the spatial and temporal constraints in \ac{SNN}. 
To answer this question, this paper presents the following contribution: 

\begin{itemize}
    \item We investigate the relationship between the number of skip connections and accuracy drop that comes from standard architectures, including denseNet121~\cite{HuangLW16a}, resnet18~\cite{HeZRS15}, and mobilenetv2~\cite{mobilenet-v2} converted to \ac{SNN}. To the best of our knowledge, this is the first work introducing a dense spiking neural network.
    
    \item We propose an adaptation hyperparameter tuning algorithm that selects the best number of skip connections to optimize the trade-off between accuracy drop and energy efficiency. 
    %\item Finally, we investigate the bio-inspired backward connections during inference and their effect on the general accuracy of the models.  
\end{itemize}