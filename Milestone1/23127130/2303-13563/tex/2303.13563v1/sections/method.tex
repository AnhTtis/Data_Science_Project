
\section{Proposed Methodology}
In this section, we first describe the skip connection investigation. The investigation aims at understanding the importance of skip connections in \ac{SNN} and motivating the use of hyperparameter optimization on the number of skip connections. We then present the general steps of our hyperparameters optimization strategy. 

\subsection{Skip connections in SNN}
Common state-of-the-art topologies comprise a small repeated set of layers, called \textit{blocks}. The blocks are connected with a single sequential connection. The block's topology is described as a directed acyclic graph (DAG). Each vertex corresponds to a layer such as convolution, attention, or fully connected. The number of vertices in the graph is referred to as the depth of the block, $d_b$. An adjacency matrix represents the connections between vertices. 

%\begin{figure*}
%    \centering
%    \includegraphics[width=0.8\textwidth]{figures/analysis.pdf}
%    \caption{Skip connections investigation.}
%    \label{fig:analysis}
%\end{figure*}

Skip connections are commonly used inside the blocks to increase overall accuracy. Their main goal is to overcome the vanishing gradient problem that may prevent layers from training. 
There are two types of skip connections in the literature, as shown in figure~\ref{fig:skip_types} (a) and (b). 

\begin{itemize}
    \item Densenet-like Skip Connections (DSC)~\cite{HuangLW16a} concatenates previous layers' outputs, $l_{i-n}$ where $0<n<i$, as the input to the next layer. A direct mathematical relation is then created between the weights of layer $l_{i-n}$ and the output of layer $l_i$, which enhances backward gradient computation. However, adding these connections enlarges the input tensor which augments the number of multiply-accumulates operations (MAC). 

    \item Addition-type Skip Connections (ASC)~\cite{HeZRS15} perform element-wise summation of the outputs of previous layers, $l_{i-n}$ where $0<n<i$. The result is the input to the current layer $l_i$. This type is usually used in resnet-like architectures.  
\end{itemize}
To study the topological properties, we do not use the original architectures, such as DenseNets which contains all-to-all connections. 
Instead, we consider a generalized version where we vary the number of skip connections by randomly selecting only some channels for concatenation.
\begin{figure*}[t!]
    \centering
    \includegraphics[trim=0 1cm 0 0, clip,width=\textwidth]{figures/overview_snn_hyp.pdf}
    \caption{Overview of the hyperparameters optimization process.}
    \label{fig:overview}
    \vspace{-0.25cm}
\end{figure*}

We define $n_{\text{skip},i}$ as the number of skip connections coming to layer $i$. To analyze the skip connection effect, we first build a single-block architecture, with 4 convolution layers inside the block. Figure~\ref{fig:skip_types} (right) shows the results of varying the $n_\text{skip}$ and using both DSC and ASC types of skip connections. If $n_\text{skip}$ is greater than the number of previous layers, we use the number of previous layers instead. For example, the second layer can only have $n_\text{skip} = 1$, and the fourth layer can have up to $n_\text{skip} = 3$. The hyperparameters and setup are described in section~\ref{sec:setup}. We show the test accuracy on CIFAR-DVS~\cite{Li2017} as well as the average firing rate for the SNN models. The firing rate refers to the rate at which a block generates output signals, which are typically referred to as spikes. 

From figure~\ref{fig:skip_types} (right), we draw the following conclusions: 
\begin{itemize}
    \item Overall, increasing the number of skip connections, regardless of their type, enhances the model's accuracy and decreases the drop from the corresponding ANN version. 
    \item Adding DSC slightly increases the average firing rate compared to ASC. The firing rate of the original architecture, i.e., $n_\text{skip} = 0$, is low (11\%). The summation operation adds up the spikes of multiple inputs, which increases the overall firing rate of the network, while the concatenation operation combines the input signals into a single signal, which results in a lower overall firing rate but yields an increasing number of MACs.
    The choice of connection type between layers is critical.
   A lower firing rate and fewer operations can lead to energy efficiency and reduced computational requirements while offering a decent increase in accuracy.
\end{itemize}

\subsection{Skip Connections Optimization}
\label{sec:skip_connections_optimization}

We define the skip connections in the adjacency matrix of each block $A_b$, where $b$ is the index of the block in the overall topology. Each element of the adjacency matrix is defined in equation~\ref{eq:aij}. Note that the adjacency matrix does not include any backward connections. 

\begin {equation}
    a_{ij} = \left\{
\begin{array}{ll}
 0 & \text{if no connection between i and j } \\
 1 & \text{DSC connection between i and j } \\
 2 & \text{ASC connection between i and j } 
\end{array}
\right.
\label{eq:aij}
\end{equation}

Figure~\ref{fig:overview} shows the overall hyperparameter optimization strategy used to adapt a given ANN such as densenet~\cite{HuangLW16a}, resnet~\cite{HeZRS15}, or mobilenetv2~\cite{mobilenet-v2} to SNN. Given an initial ANN topology, denoted as $\alpha$, the optimization aims at finding the right number, position, and type of skip connections that minimize the drop between ANN accuracy and its SNN counterpart. The overall optimization process comprises two steps: (1) we begin by constructing the search space of all possible adjacency matrices. Each block is extracted from the given topology and the number of layers in each block as well as the initial adjacency matrices are defined. (2) We use bayesian optimization (BO) to optimize the accuracy drop. 

Formally, BO seeks to compute $A^* = argmin_{A \in \Lambda}(A)$, where $\Lambda$ is the set of all possible adjacency matrices and $f(A)$ denotes the accuracy drop between the topology obtained with the adjacency matrix $A$ and accuracy of $\alpha$. 
Over a sequence of iterations, the results from all previous iterations are used to model
the topology of $\{f(A)\}_{A \in \Lambda}$ using the posterior distribution of the model. The next architecture is then chosen by optimizing an acquisition function. Two design decisions need to be taken in the case of BO: (1) The prior model, and (2) The acquisition function. 

\begin{itemize}
    \item \textbf{The Prior:} defines the probability distribution over the objective function. We use a gaussian process~\cite{DBLP:conf/aistats/Song0Y19} (GP) to model this distribution. GP is a valuable surrogate model for such an expensive training objective. We do not use a predictor as it would require creating a dataset for each given topology. 
    \item \textbf{The acquisition function:} defines how we select the next point to sample, given a conditional distribution over the values of the objective given by the prior. The most common acquisition functions used in literature are expected improvement (EI), probability of improvement (PI), and upper confidence bound (UCB)~\cite{DBLP:journals/jmlr/Auer02}. The latter is used in our search strategy. The UCB algorithm enables us to balance exploration and exploitation. It shifts from concentrating on exploration, choosing the least preferred actions, to focusing on exploitation.
\end{itemize}

These functions balance exploration with exploitation during the search. 
The chosen architecture is then trained and used to update GP. Evaluating $f(A)$ in each iteration is the bottleneck of BO since we need to train the model. Because we optimize the skip connections, we can use previously trained weights and share them among all possible topologies $\{f(A)\}_{A \in \Lambda}$. We only fine-tune the networks for $n$ epochs to account for the removal or addition of skip connections. Besides, we use parallel BO, i.e., our strategy outputs $k$ architectures to train in each iteration, so that the $k$ architectures can be trained in parallel.