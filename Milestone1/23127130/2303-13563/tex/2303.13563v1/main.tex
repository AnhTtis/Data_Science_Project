\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{float}
\usepackage{acronym}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% references package
\usepackage{cleveref}
\crefname{supp}{supplement}{Supplements}
\crefname{app}{appendix}{Appendices}
\crefformat{equation}{Eq~(#2#1#3)}
\crefformat{figure}{Fig.\,#2#1#3}
\crefformat{section}{\,#2#1#3}
\crefformat{appendix}{App.\,#2#1#3}
\crefformat{table}{Tab.\,#2#1#3}


\acrodef{DL}[DL]{Deep Learning}
\acrodef{SNN}[SNN]{Spiking Neural Network}
\acrodef{ANN}[ANN]{Artificial Neural Network}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% change tracking
\usepackage{changes} %% see documentation @ https://ctan.org/pkg/changes
%% define authors and their colors
\definecolor{HB}{RGB}{0,200,0}
\definecolor{AO}{RGB}{200,0,200}
\definecolor{IH}{RGB}{200,150,150}
\definecolor{YB}{RGB}{0,150,255}
\definecolor{REV}{RGB}{220,0,0}
\definechangesauthor[color=TT]{TT}
\definechangesauthor[color=DW]{DW}
\definechangesauthor[color=MD]{MD}
\definechangesauthor[color=YB]{YB}
\definechangesauthor[color=REV]{REV}

\newcommand{\note}[2][]{\added[#1]{\footnotesize\it[#2]}}

\makeatletter % changes the catcode of @ to 11
\newcommand{\linebreakand}{%
  \end{@IEEEauthorhalign}
  \hfill\mbox{}\par
  \mbox{}\hfill\begin{@IEEEauthorhalign}
}
\makeatother % changes the catcode of @ back to 12


\begin{document}

%\title{How connected should spike neural networks be?}
%\title{Exploring the Impact of Skip Connections on Spiking Neural Network}
\title{Skip Connections in Spiking Neural Networks: An Analysis of Their Effect on Network Training
\thanks{* Completed during a School of AI Algiers reading session at Ecole Nationale Supérieure d'Informatique, Algiers, Algeria.}}


\author{
\IEEEauthorblockN{Hadjer Benmeziane*}
\IEEEauthorblockA{Univ. Polytechnique \\ Hauts-de-France,  \\ Valenciennes, France \\ 
hadjer.benmeziane@uphf.fr}
\and
\IEEEauthorblockN{Amine Ziad Ounnoughene*}
\IEEEauthorblockA{Belmihoub Abd El Rahmane \\ High School \\ Bordj bou arreidj, Algeria \\ amine.ziad.ounnoughene@gmail.com
}
\linebreakand
\IEEEauthorblockN{Imane Hamzaoui*}
\IEEEauthorblockA{Ecole Nationale\\ Supérieure d'Informatique \\ Algiers,Algeria \\
ji\_hamzaoui@esi.dz}
\and
\IEEEauthorblockN{Younes Bouhadjar*}
\IEEEauthorblockA{Peter Gr\"unberg Institute (PGI-7,15), \\ Forschungszentrum J\"ulich, Germany \\
y.bouhadjar@fz-juelich.de}
}

\maketitle

%\note[id=YB]{If you like, we can use the change tracking package to comment in the manuscript, see below how to use:}\\
%\deleted[id=YB]{test}\\
%\replaced[id=YB]{test1}{test2}\\
%\added[id=YB]{test}

\begin{abstract}
Spiking neural networks (SNNs) have gained attention as a promising alternative to traditional artificial neural networks (ANNs) due to their potential for energy efficiency and their ability to model spiking behavior in biological systems. 
However, the training of SNNs is still a challenging problem, and new techniques are needed to improve their performance. In this paper, we study the impact of skip connections on SNNs and propose a hyperparameter optimization technique that adapts models from ANN to SNN. We demonstrate that optimizing the position, type, and number of skip connections can significantly improve the accuracy and efficiency of SNNs by enabling faster convergence and increasing information flow through the network. Our results show an average +8\% accuracy increase on CIFAR-10-DVS and DVS128 Gesture datasets adaptation of multiple state-of-the-art models. 
\end{abstract}

\begin{IEEEkeywords}
Spiking Neural Network, efficient deep learning, neural architecture search
\end{IEEEkeywords}

\input{sections/introduction}

\input{sections/related_works}

\input{sections/method}

\input{sections/experiments}

%\section*{Acknowledgment}
\section{Conclusion and Future directions}
This paper presents novel insights into the design and training of spiking neural networks (SNNs) and highlights the potential of skip connections as a promising tool for advancing SNN research. Our study evaluated both densenet-like and addition-type skip connections and found that both improved accuracy, with densenet-like connections being more energy-efficient by slightly increasing the firing rate. Our comprehensive hyperparameter optimization process led to the discovery of the optimal ANN to SNN adaptation, resulting in an average accuracy improvement of 8\% within approximately 5 minutes. %\note[id=YB]{where this 5 minutes come from?}{It is the complete search time to adapt the networks}
These results demonstrate the significance of skip connections in designing and training SNNs and pave the way for further research in this field.
 In future work, we plan to further improve the performance of SNNs by incorporating backward connections into our hyperparameter optimization. Additionally, exploring the split and connectivity between ANN/SNN processing on edge devices and the cloud is another promising avenue for future research. The integration of these innovations could lead to more practical applications of SNNs in real-world scenarios.
\vspace{-0.25cm}
\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
