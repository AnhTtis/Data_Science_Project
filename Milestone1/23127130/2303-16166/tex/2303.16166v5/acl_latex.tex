% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\usepackage{csquotes}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{microtype}
\usepackage{colortbl}
\usepackage{subcaption}
\usepackage{twemojis}
\usepackage{tikz}
\usepackage{threeparttable}
\usepackage{ragged2e}
\usepackage{soul}
\usepackage{enumitem}
\usepackage{etoolbox}
\AtBeginEnvironment{tcolorbox}{%
\setlist[itemize]{nosep,
                 leftmargin=*,
                 label=\textbullet,
                 before=\begin{minipage}[t]{\linewidth}, % <---
                 after=\end{minipage}\medskip}                   % <---
                            }

\usepackage[most]{tcolorbox}
\tcbuselibrary{raster}

\newcommand{\bug}{\scalebox{1.75}{\twemoji{beetle}}}
\newcommand{\bugone}{\scalebox{1.25}{\twemoji{beetle}}\textsubscript{1}}
\newcommand{\bugtwo}{\scalebox{1.25}{\twemoji{beetle}}\textsubscript{2}}
\newcommand{\bugthree}{\scalebox{1.25}{\twemoji{beetle}}\textsubscript{3}}
\newcommand{\bugall}{\scalebox{1.25}{\twemoji{beetle}}\textsubscript{1,2,3}}
\newcommand{\smallbug}{\scalebox{1.25}{\twemoji{beetle}}}
\newcommand{\correct}{\scalebox{1.75}{\twemoji{check mark button}}}
\newcommand{\smallcorrect}{\scalebox{1.25}{\twemoji{check mark button}}}
\newcommand{\wrong}{\scalebox{1.75}{\twemoji{cross mark}}}
\newcommand{\smallwrong}{\scalebox{1.25}{\twemoji{cross mark}}}
\newcommand{\checkinbox}{\scalebox{1}{\twemoji{black square button}}}
\newcommand{\idea}{\scalebox{1.25}{\twemoji{light bulb}}}
\newcommand{\code}{\scalebox{1.25}{\twemoji{laptop}}}
\newcommand{\results}{\scalebox{1.25}{\twemoji{bar chart}}}
\newcommand{\school}{\scalebox{1.25}{\twemoji{school}}}
% \newcommand{\torchimpl}{PyTorch}
\newcommand{\torchimpl}{TorchAudio}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\urlstyle{same}
\interfootnotelinepenalty=10000


%\newcommand{\sara}{\textcolor{magenta}}
%\newcommand{\mn}{\textcolor{red}}
%\newcommand{\mg}{\textcolor{olive}}

\newcommand{\sara}{\textcolor{black}}
\newcommand{\mn}{\textcolor{black}}
\newcommand{\mg}{\textcolor{black}}

\title{When Good and Reproducible Results are a Giant with Feet of Clay:\\ The Importance of Software Quality in NLP}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
Sara Papi\textsuperscript{\idea\results\school}, Marco Gaido\textsuperscript{\idea\results}, Andrea Pilzer\textsuperscript{\code}, Matteo Negri\textsuperscript{\results} \\
  \results Fondazione Bruno Kessler \\
  \school University of Trento \\
  \code NVIDIA \\
  \texttt{\{spapi,mgaido,negri\}@fbk.eu,apilzer@nvidia.com}}

\begin{document}
\maketitle
\begin{abstract}
Despite its crucial role in research experiments, code correctness is often presumed solely based on the perceived quality of results. This assumption, however, comes with the risk of erroneous outcomes and, in turn, potentially misleading findings. To mitigate this risk, we posit that the current focus on reproducibility should go hand in hand with the emphasis on software quality. We support our arguments 
%by discussing 
\mg{with} a case study in which we identify and fix three bugs in widely used implementations of the state-of-the-art Conformer architecture.
%First, through experiments on speech recognition and translation in various languages, we demonstrate that the presence of such bugs does not prevent the attainment of good and reproducible results. Then, we show that such results can lead to incorrect conclusions that potentially misguide future research. 
\mg{Through experiments on speech recognition and translation in various languages, we demonstrate that the presence of bugs does not prevent the achievement of good and reproducible results, which however can lead to incorrect conclusions that potentially misguide future research.} 
As countermeasures, we release \texttt{pangoliNN}, a library dedicated to testing neural models, and propose a Code-quality Checklist, with the goal of promoting coding best practices and improving software quality within the NLP
%research 
community.
\end{abstract}

\section{Introduction}
\label{sec:intro}
%\blfootnote{\idea Denotes equal contributions.}
In the field of natural language processing (NLP), as well as in broader contexts, the validity and soundness of research findings are typically upheld by \enquote{\textit{establishing consistency of results versus existing implementations, standard benchmarks, or sanity checks via statistically significant experimental results}} \citep{Rozier-2014-repro}.
%
Embracing these recommendations as the exclusive criteria for validating scientific credibility,
% Following this definition, 
the 
%NLP community (and not only)
research community
% , both within and beyond NLP, 
has recently devoted significant attention to 
%the reproducibility of scientific research  
the reproducibility 
\citep{dodge-etal-2019-show,branco-etal-2020-shared,belz-etal-2021-reprogen,10.1162/coli_a_00448,the_turing_way_community_2022} and  the soundness of experimental settings and comparisons \citep{denkowski-neubig-2017-stronger,dror-etal-2018-hitchhikers,marie-etal-2021-scientific}.
% % , as a proxy of the scientific credibility of NLP research. 
% % In particular, in response to the recent evidence 
% \sara{Specifically, in response to evidence} indicating 
% % that several of these aspects are absent in many papers 
% \sara{the absence of these aspects in many research papers}
% \citep{NEURIPS2019_c429429b} 
% %and to the difficulties of many researchers in replicating the work of others, or even their own 
% \sara{and scientists' reported difficulties in replicating others' and their own works}
% \citep{prinz2011believe,Gundersen_Kjensmo_2018,10.1162/coli_a_00330,Chen2019,Gundersen_2019}, 
% also in the specific context of NLP \citep{wieling-etal-2018-squib,marie-etal-2021-scientific,narang-etal-2021-transformer,gehrmann2022repairing,arvan-etal-2022-reproducibility-code,belz-etal-2021-systematic,belz-etal-2022-quantified,belz-etal-2023-non}, 
% %top-level 
% \sara{top-tier} conferences have introduced dedicated checklists and \sara{questions in the reviewing forms}
% % fields in review forms 
% \citep{pineau2021improving,rogers-etal-2021-just-think} 
% %with the goal of 
% \sara{aimed at} mitigating this ``\textit{reproducibility crisis}'' \citep{Baker2016}.
% % , as we will discuss in \S\ref{sec:core-idea}.
% Top-tier conferences have responded to the apparent lack of reproducibility measures in many research papers (Raff, 2019) by incorporating dedicated checklists and questions into their reviewing forms (Pineau et al., 2021; Rogers et al., 2021). This initiative is aimed at addressing what has been termed a "reproducibility crisis."
Specifically, in response to evidence indicating the absence of these aspects in many research papers \citep{NEURIPS2019_c429429b} and to mitigate the so-called  
%\mg{whose foremost aim is mitigating}
%\mn{the so-called}
``\textit{reproducibility crisis}'' \citep{Baker2016},\footnote{The term ``\textit{reproducibility crisis}'' refers to the increasing difficulties reported by scientists in replicating others' and own works \citep{prinz2011believe,Gundersen_Kjensmo_2018,10.1162/coli_a_00330,Chen2019,Gundersen_2019}, also in the specific context of NLP \citep{wieling-etal-2018-squib,marie-etal-2021-scientific,narang-etal-2021-transformer,gehrmann2022repairing,arvan-etal-2022-reproducibility-code,belz-etal-2021-systematic,belz-etal-2022-quantified,belz-etal-2023-non}.} top-tier conferences have introduced dedicated checklists and targeted questions in the reviewing forms \citep{pineau2021improving,rogers-etal-2021-just-think}.
% \sara{Specifically, top-tier conferences have responded to recent evidence indicating the absence of these aspects in many research papers \citep{NEURIPS2019_c429429b} by incorporating dedicated checklists and questions into their reviewing forms \citep{pineau2021improving,rogers-etal-2021-just-think}, aimed at mitigating this \enquote{reproducibility crisis}.}
% \footnote{The term ``\textit{reproducibility crisis}'' refers to the increasing difficulties reported by scientists in replicating others' and own works \citep{prinz2011believe,Gundersen_Kjensmo_2018,10.1162/coli_a_00330,Chen2019,Gundersen_2019}, also in the specific context of NLP \citep{wieling-etal-2018-squib,marie-etal-2021-scientific,narang-etal-2021-transformer,gehrmann2022repairing,arvan-etal-2022-reproducibility-code,belz-etal-2021-systematic,belz-etal-2022-quantified,belz-etal-2023-non}.}


However, 
%despite these initiatives, 
a fundamental question remains regarding the initial assumption: \textbf{are reproducibility and thorough evaluation against robust baselines
% really 
sufficient to ensure the soundness of a research finding?}
% \mg{Assuming that the growing awareness of these concerns and the ongoing endeavors will tackle them,}
% a fundamental question remains: \textbf{are reproducibility and thorough evaluation against robust baselines sufficient to ensure the
% %validity
% \mg{soundness}
% of research findings?}
%
%\citet{Peng-2011-reproducible}  demonstrated that reproducibility alone is \underline{not sufficient} to \enquote{\textit{guarantee the quality, correctness, or validity of the published results}}, since the code employed to produce them may not accurately execute its intended purpose. Furthermore, flawed code that produces good and easily reproducible results can propagate as the foundation for further research, ultimately leading to further unreliable and potentially misleading findings \citep{McCullough-2008-replicable}.
According to \citet{Peng-2011-reproducible}, reproducibility alone
% is not sufficient to
does 
%\underline{not}
not
\enquote{\textit{guarantee the quality, correctness, or validity of the published results}} since the code employed to produce them may not accurately execute its intended purpose.
% \sara{Flawed code producing good and easily reproducible results poses a risk, as it can perpetuate as a foundation for further research, culminating in unreliable and potentially misleading findings \citep{McCullough-2008-replicable}.}
% Furthermore, flawed code that produces good and easily reproducible results 
% % runs the risk of
% % %becoming
% % \mg{spreading as}
% \sara{can propagate as}
% the foundation for further research, ultimately 
% % resulting in 
% \sara{leading to}
% further unreliable and potentially misleading findings \citep{McCullough-2008-replicable}.
This entails inherent risks, as flawed code that produces good and easily reproducible results can propagate as the foundation for further research, ultimately leading to further unreliable and potentially misleading findings \citep{McCullough-2008-replicable}.
% \sara{On the same line, we claim that the reproducibility and the evaluation against strong baselines not only are insufficient but also that our community is not paying attention at all to the importance of the software quality.}
% CHATGPT: Our stance contends that while reproducibility and benchmark evaluations are indisputably essential, the critical domain of software quality assurance (SQA) principles (Buckley and Poston, 1984; Tripathy and Naik, 2011) has remained relatively overlooked in the NLP community. 


% VERSIONE SARA
% \sara{Building upon these observations, we claim that while reproducibility and evaluations against robust baselines are 
% indisputably 
% essential, the principles of software quality assurance \citep{Buckley-1984-sqa,tripathy2011software}, or SQA, which represent the cornerstones in software development
% \citep{6341763,software_quality,electronics11162485}, have been completely overlooked within the research community. In response, our study serves as a \textbf{call to action}, aimed at underscoring the % pivotal role of SQA in research and proposing concrete steps to enhance the trustworthiness of published findings. Our contributions are:}

Expanding on these observations, this paper is a call to action, underpinned by empirical evidence, 
% to complement 
to bolster the dependability of NLP findings by complementing
current initiatives toward 
% the reproducibility and soundness of experimental validations 
reproducibility and experimental soundness
with equal emphasis on \textit{software quality}. 
% To bolster the dependability of NLP findings, we encourage the adoption of the principles of software quality assurance (SQA -- \citealt{Buckley-1984-sqa,tripathy2011software}), which has so far been neglected by our community. 
% \mn{As a reference framework to bolster the dependability of NLP findings, we refer to the principles of software quality assurance (SQA -- \citealt{Buckley-1984-sqa, tripathy2011software}), which have 
% so far been overlooked within our community.}
To this 
%aim,
end, we adopt as a reference framework the principles of software quality assurance (SQA -- \citealt{Buckley-1984-sqa,tripathy2011software}), which have so far been overlooked by our community.
%Our contributions can be summarized as follows:
Building on this foundation, we contribute as follows:

% CHATGPT-MERGE: This study serves as a call to action, emphasizing the paramount importance of incorporating principles of software quality assurance (SQA) in NLP research to enhance the dependability of published findings. While recognizing the essential aspects of reproducibility and evaluations against robust baselines, the study urges equal attention to software quality. The contributions include a plea for the integration of SQA principles within current initiatives for reproducibility and soundness in experimental validations.

% This study calls for integrating software quality assurance (SQA) principles into NLP research to enhance reliability. It underscores the overlooked importance of SQA alongside reproducibility and baseline evaluations, urging equal attention within current initiatives for experimental soundness.


% \mg{Building upon these observations, this paper is a 
% %\textbf{call to action} 
% \mn{call to action}
% to complement 
% %the current 
% \mn{current}
% initiatives toward reproducibility and soundness of experimental validations 
% %\mn{(indisputably essential)} 
% with equal attention to the \mn{(so far neglected)} software \textit{quality}, evaluated through the principles of software quality assurance (SQA -- \citealt{Buckley-1984-sqa,tripathy2011software}), in order to enhance the dependability of NLP findings. 
% %Our contributions are:
% \mn{Along this direction, our contributions are as follows:}}

% \mg{Building upon these observations, this paper is a \textbf{call to action} that, underpinned by empirical evidence, studies how software quality, evaluated through principles of software quality assurance (SQA -- \citealt{Buckley-1984-sqa,tripathy2011software}), contributes to the trustworthiness of research finding, proposing concrete steps to enhance the dependability of NLP findings. Our contributions are:}
% In light of the above considerations, this study is a call to action, underpinned by empirical evidence, to enhance the dependability of published NLP findings. In particular, our contributions are:
\begin{enumerate}[leftmargin=14pt]

\item We examine the extent to which research works consider the attributes studied in the SQA field
%the field of software quality assurance \citep{Buckley-1984-sqa,tripathy2011software}, or SQA
(\S\ref{sec:sqa}), 
%and show
highlighting
that code correctness has been neglected by the NLP community thus far (\S\ref{sec:core-idea});
    
\item Through a case study on open-source implementations of the widespread Conformer architecture \citep{gulati20_interspeech}, we
%prove 
show that:
\begin{itemize}[leftmargin=8pt]
\item[-] At least one impactful bug is present in all the analyzed implementations (\S\ref{subsec:analysis});
%All the analyzed implementations contain at least one impactful bug in the codebase (\S\ref{subsec:analysis});
\item[-] 
%The existence of bugs does not prevent from achieving
Such bugs do not
prevent from achieving
% hinder
%the attainment of
good and
reproducible results that outperform
%\mg{those of}
other architectures
%across different tasks and
in speech recognition and translation across different
language 
%settings
pairs
(\S\ref{subsec:impact_bug});
\item[-] 
%The presence of bugs can lead to incorrect findings when incorporating a new technique into the architecture
Undetected bugs can lead to erroneous conclusions when evaluating new techniques
(\S\ref{subsec:impact_code}).
\end{itemize}
\item We release a bug-free implementation of
%Conformer
Conformer,\footnote{\label{foot:sw_release}Availabe at \url{https://github.com/hlt-mt/FBK-fairseq/} under the Apache 2.0 License.} 
%and all our pre-trained models;
along with all the pre-trained models;
%the pre-trained models used in our experiments;
%
%
%
% \item  We release\textsuperscript{\ref{foot:sw_release}} a library including easily-usable tests named \texttt{pangoliNN}\footnote{As a pangolin looks for bugs and catches them, this library aims at finding bugs in neural networks (NN). Hence the name.} to enforce the proper behavior of neural models (\S\ref{subsec:pangolinn}) and propose to integrate the checklist of current conferences with questions about coding best practices in order to fostering code correctness and software quality in research works  (\S\ref{subsec:checklist}).
%
%
%
%
%
%
% \item  \mn{[IO SPEZZEREI IN 2 QUESTO CONTRIBUTO. PER UN JOURNAL PENSO VADA BENONE AVERE PIU' CONTRIBUTI DEI SOLITI 2-3]} We 
% %release\textsuperscript{\ref{foot:sw_release}} 
% \sara{introduce\textsuperscript{\ref{foot:sw_release}}}
% %\texttt{pangoliNN},\footnote{As a pangolin looks for bugs and catches them, this library aims at finding bugs in neural networks (NN). Hence the name.} 
% \texttt{pangoliNN},
% a library 
% %including 
% \sara{featuring} easily usable tests to enforce the proper behavior of neural models (\S\ref{subsec:pangolinn}), and propose 
% %to integrate 
% \sara{the integration of questions about coding best practices into current conference checklists to foster code correctness and software quality in future research works}
% %the checklist of current conferences with questions about coding best practices in order to fostering code correctness and software quality in research works  
% (\S\ref{subsec:checklist}).
%
%%%%%%%%%%%%%%%%% OLD
% \item \mn{We  release\textsuperscript{\ref{foot:sw_release}} \texttt{pangoliNN}, a library featuring easily usable tests to enforce the proper behavior of neural models (\S\ref{subsec:pangolinn});} 

% \item \mn{We propose 
% a 
% %code-quality checklist,
% \sara{Code-quality Checklist,}
% which integrates questions about coding best practices into current conference checklists in order to foster code correctness and software quality in future research works (\S\ref{subsec:checklist}).}

% \item We 
% %foster 
% promote
% code correctness and software quality in future research works by:
% \begin{itemize}[leftmargin=8pt]
% \item[-] Releasing \texttt{pangoliNN},\textsuperscript{\ref{foot:sw_release}} a library featuring easily usable unit tests to enforce the proper behavior of neural models (\S\ref{subsec:pangolinn});
% \item[-] Proposing the integration into current conferences checklists of a Code-quality section, 
%containing 
% which would include
% questions about coding best practices (\S\ref{subsec:checklist}).
%\end{itemize}
\item  \mg{We promote code correctness and software quality by releasing \texttt{pangoliNN},\footnote{Availabe at \url{https://github.com/hlt-mt/pangolinn/} under the Apache 2.0 License.}
%\textsuperscript{\ref{foot:sw_release}}
a library featuring easily-usable unit tests to enforce the proper behavior of neural models (\S\ref{subsec:pangolinn}), and proposing the integration into current conferences checklists of a Code-quality section, which would focus on coding best practices (\S\ref{subsec:checklist}).}
\end{enumerate}


\section{SQA and Research}
\label{sec:sqa}
\input{sections/sqa}





\section{Research Code Quality Evaluation}
\label{sec:core-idea}
%\url{https://aaai.org/conference/aaai/aaai-23/reproducibility-checklist/}
\input{sections/code_quality_eval}




\section{The Case Study}
\label{sec:case-study}
\input{sections/case_study}




\section{Increasing Research Code Correctness}
\label{sec:checklist}
\input{sections/proposal}



\section{Conclusions}

%%%% ORIG
%In parallel with the strand of action toward increasing the reproducibility of NLP research,  this paper \mn{investigated the impact of software quality attributes on research work} and their relevance to the field. 
%%%%% PROPOSAL MN
% In parallel with efforts to enhance the reproducibility of NLP research, this paper underscores the importance of software quality in ensuring the safe reuse of published code and the reliability of research findings derived from its use.
%%%%% PROPOSAL MG
In parallel with the current efforts to enhance the reproducibility of NLP research, this paper urges similar actions targeting the improvement of research software quality, underscoring its importance for the reliability of research findings.
%
% ORIGINAL
%Through a comparison of the attention that reproducibility and software quality receive within the research community, we highlighted the predominant neglect of code correctness and the risks associated with  assessing soundness solely on the basis of experimental results.
%%%%% PROPOSAL MN
%Specifically, in 
In comparison to the attention given to  
%reproducibility within the research community,
%reproducibility within our community,
reproducibility within our community,
we observed the predominant neglect of code correctness and elaborated on the risks associated with assessing soundness solely on the basis of experimental results.
% Such risks have a significant impact on current research findings and include the potential of drawing misleading conclusions, as we empirically demonstrated through a case study involving the widespread Conformer architecture.
As we empirically demonstrated through a case study involving the widespread Conformer architecture, such risks include the potential of drawing misleading conclusions 
%that build on 
from positive 
%evaluation  outcomes.
%evaluation  results that obscure the presence of bugs in the code.
%evaluation results achieved despite the presence of bugs in the code.
results obtained 
%with code affected by bugs.
% from 
using
flawed code.
%
%
% ORIGINAL
%As a countermeasure,  we proposed the adoption of a \enquote{Code-quality Checklist} with the goal of fostering coding best practices. In addition, we created the \texttt{pangoliNN} Python package to facilitate testing neural models. 
%%%%% PROPOSAL MN
As a countermeasure, 
besides releasing a corrected Conformer implementation, we created the \texttt{pangoliNN} Python package to facilitate testing neural models and proposed the adoption of a \enquote{Code-quality Checklist} 
%with the goal of 
aimed at fostering coding best practices.
While we acknowledge that these solutions are not a panacea, their purpose is to raise awareness within the NLP community about the importance of software quality. We hope that our 
%\mn{call to action} 
endeavor will inspire a collective commitment 
%toward the development of
to developing
%
% ORIG
%aimed at addressing the \mn{current lack of incentives} and promoting the development of high-quality code.
%%%% PROPOSAL MN 
%and incentives to  develop
%and incentives to promoting the development of high-quality code.
high-quality and reliable code.

\section*{Acknowledgements}
We acknowledge the support  of the PNRR project FAIR -  Future AI Research (PE00000013),  under the NRRP MUR program funded by the NextGenerationEU.
We acknowledge the CINECA award under the ISCRA initiative, for the availability of high-performance computing resources and support.

\section*{Limitations}
To back up our call to action toward the adoption of coding best practices aimed at fostering correctness and improving the quality of the developed software, we presented a case study involving the use of the Conformer architecture in the two most popular speech processing tasks: speech recognition and translation. Although the effects of the presence of bugs might be found also in other  scenarios, such as  text-to-speech, speech emotion recognition, spoken language understanding, and speech separation, we did not cover them in this paper.  While the undesired effect of the bugs we isolated (and corrected) was empirically demonstrated, extending the analysis to other research areas would be a natural extension of our study, which could provide a more comprehensive understanding of the impact of the identified bugs on the broader NLP community working on speech-related tasks.

Moreover, in our case study, we examined the open-source implementations of Conformer, in which we identified three types of bugs related to the Convolution Module, Initial Subsampling, and Positional Encodings. While we found efficient solutions for the first two bugs, for the last one our fix introduces a significant overhead. As a result, the implementation we release, although correct, increases the training time of the models. We are confident that, by open-sourcing our code, the community will soon find a way to optimize it and overcome this limitation, capitalizing on our findings and spreading the use of more reliable versions of a state-of-the-art architecture.
% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only




\bibliography{custom}

\appendix

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.95\textwidth]{img/conformer_con.drawio.png}
    \caption{Convolution module in the Conformer encoder layer. Convolutional blocks are 1D convolutions.}
    \label{fig:conf_conv}
\end{figure*}

\section{Conformer in ASR and ST}
% \subsection{Conformer in ASR and ST}
\label{subsec:speech_background}

ASR is the task in which an audio containing speech content is transcribed in its original language. In ST, instead, the source audio is translated into text in a different language. Nowadays, both tasks are commonly performed with end-to-end (or direct) models \citep{pmlr-v32-graves14,Chorowski-2014-asr,berard_2016,weiss2017sequence}, whose architecture is based on the Transformer~\citep{NIPS2017_3f5ee243}. The Transformer has been adapted to work with audio inputs~\citep{8462506,gangi19_interspeech} by introducing two convolutional layers that shrink the length of the input sequence by a factor of $4$, so as to reduce the otherwise excessive memory requirements. More recently, \citet{gulati20_interspeech} proposed the Conformer: a novel architecture with a modified encoder that led to significant improvements in both ASR and ST \citep{inaguma2021non}.

The changes introduced in the Conformer encoder layer structure can be summarized as follows:
\textit{i)} relative sinusoidal positional encodings \citep{dai-etal-2019-transformer} are introduced in the self-attention for improved generalization with respect to varying input lengths;
\textit{ii)} the FFN sublayer is replaced by two FFNs that wrap the self-attention, inspired by the Macaron-Net~\citep{lu-et-al-2016-macaron-net};
\textit{iii)}~a convolution module 
%(depicted in 
(Figure \ref{fig:conf_conv}) is added
%immediately 
after the self-attention, before the second FFN layer.
The convolution module, which is wrapped in a residual connection, applies layer normalization, followed by
%and then
a pointwise convolution that doubles the dimension of the feature vector, which
% to each feature  vector, doubling its dimension that
is restored to its original size by a Gated Linear Unit (GLU) activation function~\citep{Dauphin-2017-glu}. Then, a depthwise convolution with 31 kernel size is applied before a batch normalization \citep{ioffe-2015-batchnorm}, followed by the Swish activation function \citep{swish-2017}, and another pointwise convolution. Lastly, a dropout module \citep{Srivastava-2014-dropout} randomly
%masks (i.e. zeroes out)
zeroes out
a percentage of the
%feature values 
features
to prevent the network from overfitting.

\section{Experimental Settings}
\label{app:expsett}

Our Conformer-based architecture is composed of 12 Conformer \citep{gulati20_interspeech} encoder layers and 6 Transformer \citep{NIPS2017_3f5ee243} decoder layers, with 8 attention heads each. Embedding size is set to 512 and hidden neurons in the feed-forward layers to 2,048, with a total of 114,894,730 
%parameters for the model. 
model parameters. Dropout is set to 0.1 for feed-forward, attention, and convolution layers. The kernel size of the Convolution Module is set to 31 for both point-wise and depthwise convolutions.  We train all the models using Adam \citep{journals/corr/KingmaB14} optimizer (betas $(0.9, 0.98)$) and label-smoothed cross-entropy (LSCE) loss (smoothing factor 0.1). We also use an auxiliary Connectionist Temporal Classification or CTC loss \citep{Graves2006ConnectionistTC} during training to ease convergence and obtain competitive results without pre-training the encoder with that of an ASR model \citep{gaido-etal-2022-efficient}. The auxiliary loss is summed to the LSCE with 0.5 relative weight.  The learning rate is set to $2\cdot10^{-3}$ with Noam scheduler \citep{NIPS2017_3f5ee243} and 25k warm-up steps.  The vocabularies are based on SentencePiece models %\citep{sennrich-etal-2016-neural}
\citep{kudo-richardson-2018-sentencepiece}
with size 5,000 \citep{inaguma-etal-2020-espnet} for the English source 
and 8,000 \citep{di-gangi-etal-2020-target} for the ST target languages.
%\citep{inaguma-etal-2020-espnet} and 8,000 for each of the ST target languages \citep{di-gangi-etal-2020-target}. 
We set 100k maximum updates with early stopping after 10 epochs without loss decrease on the dev set and average 5 checkpoints around the best (best, two preceding, and two following). All trainings are performed with 40k tokens as batch size and 4 as update frequency on two GPUs. All other settings are the default of Fairseq-ST~\citep{wang2020fairseqs2t}, which we forked as a base of our implementation. SpecAugment \citep{Park2019} is applied during training, while utterance-level Cepstral mean and variance normalization is performed both at training and inference time. Trainings lasted 18-33 hours depending on the model configuration 
%(e.g., with the fixes or not\sara{)
(e.g., with or without the fixes)
%, with or without CTC compression) 
and the language pair due to the different sizes of the training data.



\section{CTC compression}
\label{app:ctc_compr}


CTC compression 
%\citep{liu2020bridging,gaido-etal-2021-ctc} 
has been proposed to reduce the difference in terms of sequence length between corresponding audio and text representations.
% It leverages the predictions of the CTC loss \citep{Graves2006ConnectionistTC}, which produces a probability distribution over the possible target labels augmented with a \texttt{<blank>} symbol.
% Different from 
In contrast with
fixed reduction methods like max pooling or strided convolutions that apply a predetermined reduction to each sequence, CTC compression leverages
% the predictions of the CTC loss \citep{Graves2006ConnectionistTC}, which produces a probability distribution over the possible target labels augmented with a \texttt{<blank>} symbol.
the probability distribution over the source vocabulary augmented with a \texttt{<blank>} symbol produced by the CTC module.
%These probability distributions are
These probabilities are
used to assign a label (the most likely one) to each vector of the sequence and collapse contiguous vectors corresponding to the same label by averaging them.
By dynamically determining which vectors of the audio sequence should be merged, it tries to avoid the mismatch in terms of sequence length with the sub-word 
% sequences 
sequence
of the corresponding 
%transcripts and differs from fixed reduction methods like max pooling or strided convolutions, which apply a predetermined reduction to each sequence.
% transcripts.
transcript.


\end{document}
