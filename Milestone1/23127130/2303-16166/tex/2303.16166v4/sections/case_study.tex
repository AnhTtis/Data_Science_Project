

In our case study, we examine the Conformer \citep{gulati20_interspeech} architecture --
% which is 
the state-of-the-art solution for speech processing tasks \citep{conformer-sota,ma2021end,conformer-sota2,conformer-sota3} such as automatic speech recognition (ASR) and speech-to-text translation (ST) -- whose rapid and wide adoption is
%. This architecture has been rapidly and  widely adopted,
%as 
evidenced by the $>$1,700 citations since 2020.\footnote{According to Google Scholar on August 11th, 2023.}
% For an introduction to the ASR and ST tasks and Conformer architecture, refer to Appendix \ref{subsec:speech_background}.

In the following, we first introduce  the ASR and ST tasks
%object of our study 
and the basics of the Conformer architecture (\S\ref{subsec:speech_background}). Then, we analyze the Conformer implementation of six widely-used open-source codebases, showing that they all contain at least one bug (\S\ref{subsec:analysis}). 
Lastly, through extensive experiments on the two tasks and on eight language pairs, we demonstrate that the presence of bugs can be hidden by good -- but incorrect -- results (\S\ref{subsec:impact_bug}) and consequently lead to wrong findings (\S\ref{subsec:impact_code}).





\subsection{Conformer in ASR and ST}
\label{subsec:speech_background}

ASR is the task in which an audio containing speech content is transcribed in its original language. In ST, instead, the source audio is translated into text in a different language. Nowadays, both tasks are commonly performed with end-to-end (or direct) models \citep{pmlr-v32-graves14,Chorowski-2014-asr,berard_2016,weiss2017sequence}, whose architecture is based on the Transformer~\citep{NIPS2017_3f5ee243}. The Transformer has been adapted to work with audio inputs~\citep{8462506,gangi19_interspeech} by introducing two convolutional layers that shrink the length of the input sequence by a factor of $4$, so as to reduce the otherwise excessive memory requirements. More recently, \citet{gulati20_interspeech} proposed the Conformer: a novel architecture with a modified encoder that led to significant improvements in both ASR and ST \citep{inaguma2021non}.

The changes introduced in the Conformer encoder layer structure can be summarized as follows:
\textit{i)} relative sinusoidal positional encodings \citep{dai-etal-2019-transformer} are introduced in the self-attention for improved generalization with respect to varying input lengths;
\textit{ii)} the FFN sublayer is replaced by two FFNs that wrap the self-attention, inspired by the Macaron-Net~\citep{lu-et-al-2016-macaron-net};
\textit{iii)}~a convolution module 
%(depicted in 
(Figure \ref{fig:conf_conv}) is added
%immediately 
after the self-attention, before the second FFN layer.
The convolution module, which is wrapped in a residual connection, applies layer normalization, followed by
%and then
a pointwise convolution \mg{that doubles the dimension of the feature vector, which}
% to each feature  vector, doubling its dimension that
is restored to its original size by a Gated Linear Unit (GLU) activation function~\citep{Dauphin-2017-glu}. Then, a depthwise convolution with 31 kernel size is applied before a batch normalization \citep{ioffe-2015-batchnorm}, followed by the Swish activation function \citep{swish-2017}, and another pointwise convolution. Lastly, a dropout module \citep{Srivastava-2014-dropout} randomly masks (i.e. zeroes out)
a percentage of the feature values to prevent the network from overfitting.

\subsection{Analysis of the Codebases}
\label{subsec:analysis}




We analyze the behavior of the open-source implementations of the Conformer by systematically varying a parameter that should not affect the results: the inference batch size (IBS).
With high IBSs, many samples are collected in the same batch, allowing for their parallel processing on GPU to reduce the overall computational cost.
When samples of different lengths are collected in the same batch -- a frequent situation in speech tasks, where the input length largely varies -- the input sequences are brought to the same dimension by filling them with padding. 
Since with correct implementations the results are independent of the presence of padding (and, therefore, of the IBS), research papers usually include only the training batch size (which, instead, is an important hyperparameter for the stability of the training). However, as we demonstrate in this section, the bugs present in the Conformer implementations undermine the above assumption.

We studied six widely-used repositories, namely: Fairseq-ST~\citep{wang2020fairseqs2t}, ESPnet-ST~\citep{inaguma-etal-2020-espnet}, NeMo~\citep{kuchaiev2019nemo}, SpeechBrain~\citep{speechbrain}, an open source codebase named \enquote{Conformer},\footnote{\url{https://github.com/sooftware/conformer}} and \torchimpl{}~\citep{yang2021torchaudio}.
We discovered that all these implementations return different results with different IBSs, showing that the presence of padding incorrectly alters the results.\footnote{We emphasize that our intention is not to single out the shortcomings of individual libraries. Conversely, we are extremely thankful for the invaluable contribution they represent to our community. Our analysis is only intended to further improve the reliability of codes and, consequently, of the experimental results, which we believe is of utmost importance.}
Upon inspection of the codes, we isolated three bugs associated with padding handling in:
% Conformer Convolutions (\bugone), Initial Subsampling (\bugtwo), and Positional Encodings (\bugthree).
% Due to space limitations, each bug is described in detail in Appendix \ref{sec:conformer_bugs}.

%As mentioned in \S\ref{sec:conformer_bugs}, open-source Conformer implementations are affected by three common bugs related to failing to account for the presence of padding. Specifically, the three elements that do not correctly handle padding are:


\begin{figure}[!tb]
\centering
\renewcommand*\thesubfigure{\arabic{subfigure}} 
     \begin{subfigure}[b]{0.46\textwidth}
        \centering
         \includegraphics[width=0.4\textwidth]{img/start.png}
         \caption{Before shifting, the Relative PE matrix ($P_{00},...,P_{22}$) is padded (zero values).}
     \end{subfigure}
     \par\medskip
     \begin{subfigure}[b]{0.46\textwidth}
        \centering
         \includegraphics[width=0.4\textwidth]{img/wrong.png}
         \caption{When relative shift is applied to the Relative PE matrix without considering padding, some values of the padding area (in \textcolor{red}{\textbf{red}}) are incorrectly 
         %included in 
         moved to
         the non-padding area.}
         %\smallbug\textsubscript{3}, 
        % some values ($P_{11}, P_{12}, P_{20}, P_{21}$) are moved to the padding part (in \textcolor{red}{\textbf{red}}), hence are not considered in the following computation, while some padding is instead incorrectly considered in the following computation (in \textcolor{teal}{\textbf{green}}). Please notice that the first row is always discarded.}
     \end{subfigure}
     \par\medskip
     \begin{subfigure}[b]{0.46\textwidth}
        \centering
         \includegraphics[width=0.4\textwidth]{img/correct.png}
         \caption{When relative shift is applied to the Relative PE matrix considering padding, the values $P_{00},...,P_{22}$ are not moved to the padding area.}
     \end{subfigure}
    \caption{Example of relative shift operation starting from a Relative PE matrix containing padding (1), both considering a codebase with \bugthree{} (2) and without (3) bug. The first row is always discarded.}
    \label{fig:relativePEs}
\end{figure}




\paragraph{Conformer Convolutions (\bugone)} The depthwise and pointwise convolutions of the Conformer convolution module do not consider the presence of padding and produce a non-padded output with non-zero values adjacent to the input sample. These values modify the behavior of the subsequent batch normalization and of the other convolutions, leading to incorrect alterations of all the valid values.

\paragraph{Initial Subsampling (\bugtwo)} The two initial convolutions that subsample the input sequence by a factor of 4 do not consider padding. For this reason, the second convolution is fed with non-zero values adjacent to the input sequence that lead to a wrong computation of the last valid elements.

\paragraph{Positional Encodings (\bugthree)} The relative sinusoidal positional encodings (PEs), which are added to the attention matrix, are computed by shifting a sinusoidal matrix. This shifting operation first prepends a zero column to the sinusoidal matrix and then reshapes it so that the last element of the first row becomes the first element of the second row, the last two elements of the second row become the first ones of the third row, and so on. By doing this, this operation assumes that all elements are valid. However, when a sequence is padded, only 
%a 
part of the attention matrix is valid (in green in Figure~\ref{fig:relativePEs}.1) and spurious values are moved to the beginning of the next row (Figure~\ref{fig:relativePEs}.2).  In Figure~\ref{fig:relativePEs}, for the sake of clarity of the example, we pretend that existing implementations set to 0 the PE in the padding area. While this is not what happens in practice (as the padding area contains other sinusoidal PEs), it shows that the correct values are discarded and the final matrix significantly differs from the one obtained without padding, which is instead shown in Figure~\ref{fig:relativePEs}.3.



\begin{table}[!t]
\small
    \centering
    \setlength{\tabcolsep}{3.7pt}
    \begin{tabular}{l|c|c|c}
    \specialrule{.1em}{.05em}{.05em} 
         \textbf{Repository} & \textbf{Conv. Mod.} & \textbf{SubSampl.} & \textbf{Pos. Enc.} \\
         \specialrule{.1em}{.05em}{.05em} 
         Fairseq-ST & \bugone & \bugtwo & \bugthree \\
         ESPnet-ST & \bugone & \bugtwo & \bugthree \\
         NeMo & - & \bugtwo & \bugthree \\
         SpeechBrain & \bugone & \bugtwo & \bugthree \\
         Conformer & \bugone & \bugtwo & \bugthree \\
         \torchimpl{} & \bugone & NA & NA \\
    \specialrule{.1em}{.05em}{.05em} 
    \end{tabular}
    \caption{Bugs present in the analyzed repositories. NA stands for \enquote{Not Applicable}.}
    \label{tab:bug}
\end{table}

In Table \ref{tab:bug}, we report the presence (or absence) of these bugs for each analyzed codebase in its current version.\footnote{Checked on January 8th 2023.}
All the implementations but one (NeMo) are affected by \bugone. Also, all are affected by \bugtwo{} and \bugthree, except for \torchimpl, whose implementation neither includes relative positional encodings in the attention nor the initial sub-sampling convolutional layers.
Having ascertained that all the analyzed implementations contain at least one bug, the next sections will concentrate on their impact on ASR and ST results and, in turn, the related findings.


% \begin{table}[!htb]
%     \centering
%     \small
%     \setlength{\tabcolsep}{4.3pt}
%     \begin{tabular}{l|cccccccc}
%     \specialrule{.1em}{.05em}{.05em} 
%         \textbf{Split} & \textbf{de} & \textbf{es} & \textbf{fr} & \textbf{it} & \textbf{nl} & \textbf{pt} & \textbf{ro} & \textbf{ru} \\      
%         \specialrule{.1em}{.05em}{.05em} 
%         train & 387 & 479 & 469 & 441 & 421 & 364 & 410 & 466 \\
%         dev & 2.5 & 2.5 & 2.5 & 2.5 & 2.5 & 2.5 & 2.5 & 2.5 \\
%         test & 4.1 & 4.1 & 4.1 & 4.1 & 4.1 & 4.1 & 4.1 & 4.1 \\
%     \specialrule{.1em}{.05em}{.05em} 
%     \end{tabular}
%     \caption{Number of hours of the train, dev, and test (tst-COMMON) sets for each target language.}
%     \label{tab:data-stats}
% \end{table}


\subsection{Experimental Settings}
We train and evaluate ASR and ST models on MuST-C
v1.0 \cite{CATTONI2021101155}, which contains parallel speech-to-text data with English (en) as source language and 8 target text languages, namely Dutch (nl), French (fr), German (de), Italian (it), Portuguese (pt), Romanian (ro), Russian (ru), and Spanish (es). 
The ASR model is trained on the transcripts of the MuST-C en-es  train set, as it is the largest section of the corpus\sara{.}
%(for data statistics, see Table \ref{tab:data-stats}).
For ST, 8 different models are trained, one for each language direction. 
% All the experimental settings, including model architecture, and training hyperparameters, are described in Appendix \ref{sec:exp_sett}.
Evaluation is performed on the
%MuST-C 
tst-COMMON, by computing  word error rate (WER) for ASR and SacreBLEU \citep{post-2018-call}\footnote{BLEU|\#:1|c:mixed|e:no|tok:13a|s:exp|v:2.0.0} for ST, and using bootstrap resampling \citep{koehn-2004-statistical} with 95\% confidence interval to assess statistical significance.

Our Conformer-based architecture is composed of 12 Conformer \citep{gulati20_interspeech} encoder layers and 6 Transformer \citep{NIPS2017_3f5ee243} decoder layers, with 8 attention heads each. Embedding size is set to 512 and hidden neurons in the feed-forward layers to 2,048, with a total of 114,894,730 
%parameters for the model. 
model parameters. Dropout is set to 0.1 for feed-forward, attention, and convolution layers. The kernel size of the Convolution Module is set to 31 for both pointwise and depthwise convolutions.  We train all the models using Adam \citep{journals/corr/KingmaB14} optimizer (betas $(0.9, 0.98)$) and label-smoothed cross-entropy (LSCE) loss (smoothing factor 0.1). We also use an auxiliary Connectionist Temporal Classification or CTC loss \citep{Graves2006ConnectionistTC} during training to ease convergence and obtain competitive results without pre-training the encoder with that of an ASR model \citep{gaido-etal-2022-efficient}. The auxiliary loss is summed to the LSCE with 0.5 relative weight.  The learning rate is set to $2\cdot10^{-3}$ with Noam scheduler \citep{NIPS2017_3f5ee243} and 25k warm-up steps.  The vocabularies are based on SentencePiece models \citep{sennrich-etal-2016-neural} with size 5,000 for the English source \citep{inaguma-etal-2020-espnet} and 8,000 for each of the ST target languages \citep{di-gangi-etal-2020-target}. We set 100k maximum updates but we early stop training after 10 epochs without loss decrease on the dev set and average 5 checkpoints around the best (best, two preceding, and two following). All trainings are performed with 40k tokens as batch size and 4 as update frequency on two GPUs. All other settings are the default of Fairseq-ST~\citep{wang2020fairseqs2t}, which we forked as a base of our implementation. SpecAugment \citep{Park2019} is applied during training, while utterance-level Cepstral mean and variance normalization is performed both at training and inference time. Trainings lasted 18-33 hours depending on the model configuration 
%(e.g., with the fixes or not\sara{)
(e.g., with or without the fixes)
%, with or without CTC compression) 
and the language pair due to the different sizes of the training data.


Trainings and inferences were performed on, respectively, two and one A40 GPU(s).
%By default, on Ampere GPUs the PyTorch backend
On Ampere GPUs, PyTorch  computes convolutions and matrix multiplications with TensorFloat-32\footnote{\url{https://pytorch.org/docs/stable/notes/cuda.html\#tensorfloat-32-tf32-on-ampere-devices}.} (\textbf{TF32}) tensor cores by default. TF32 speeds up the computation but introduces numeric errors that can cause small random fluctuations, e.g. in the presence of padding. 
In the following, we experiment both with and without TF32 (both at training and inference time) because padding has no effect on the final outputs only when TF32 is disabled.


\subsection{Impact of the Identified Bugs}
\label{subsec:impact_bug}


We evaluate the impact of the identified bugs (\S\ref{subsec:analysis}) on ASR and ST results by varying the IBSs as increasing the batch size introduces more padding, amplifying the effects of the bugs. Initially, experiments are conducted on 
% the
our correct codebase (\smallcorrect). Subsequently, we enable single precision (TF32).
Then, we reintroduce the bugs individually (\bugone, \bugtwo, and \bugthree), and all together (\bugall).




\begin{table}[!tb]
\small
\setlength{\tabcolsep}{12pt}
    \centering
    \footnotesize
    \begin{tabular}{l|ccc}
    \specialrule{.1em}{.05em}{.05em} 
        \multirow{2}{*}{\textbf{Code}} & 
         \multicolumn{3}{c}{\textbf{IBS}} \\
        \cline{2-4}
         & 1 & 10 & 100 \\
        \specialrule{.1em}{.05em}{.05em} 
        \smallcorrect & 10.52 & 10.52 & 10.52 \\
        \hline
        + TF32 & 10.73 & 10.73 & 10.73 \\
        \quad + \bugone & 10.72 & 11.25* & 19.50* \\
        \quad + \bugtwo & 10.73 & 10.74 & 10.74 \\
        \quad + \bugthree & \textbf{10.46} & 10.62 & 10.73 \\
        \quad + \smallbug\textsubscript{1,2,3} & 11.32* & 14.25* & 54.56* \\
    \specialrule{.1em}{.05em}{.05em} 
    \end{tabular}
    \caption{WER
    %($\downarrow$)
    %scores 
    for ASR with TF32 and bugs as IBS varies (1, 10, and 100 sentences). *~indicates that the difference with \smallcorrect{} is statistically significant.
    }
    \label{tab:ablationASR}
\end{table}

\paragraph{ASR}

Table \ref{tab:ablationASR} shows, in comparison to \smallcorrect{}, the impact of TF32 and of the different bugs on ASR performance.
First, TF32 causes a not statistically significant quality drop (+0.21 WER), which does not vary with the IBS (despite the presence of  minor variations in the outputs attested by  a slightly different number of generated words).
When the bugs are present (\bugone, \bugtwo, \bugthree), instead, the performance becomes sensitive to the IBS. This is particularly evident with \bugone, which significantly increases the error rate (+8.78 WER) when we introduce a considerable amount of padding (IBS=100).
It is noteworthy that most of the differences compared to the bug-free version (\smallcorrect{}) are not statistically significant, and the best result is achieved with \bugthree{} and 1 as IBS.
Only the presence of all bugs \bugall{} causes consistent and statistically significant quality drops. Nonetheless, the results with 1 and 10 as IBS are still far better than those obtained with Transformer architectures on the same benchmark (i.e. 26.61 by \citealt{CATTONI2021101155} and 15.6 by \citealt{gaido-etal-2021-ctc}). Moreover, their reproducibility is not hindered by the presence of bugs, as setting the IBS to any particular value consistently yields the same score. We can conclude that \textbf{\textit{even flawed code can produce competitive and reproducible results}} and, therefore, focusing only on these two aspects is not enough to ensure the trustworthiness of the code.



\begin{table}[!tb]
\small
\setlength{\tabcolsep}{2.5pt}
    \centering
    \footnotesize
    \begin{tabular}{l|ccc|ccc}
        \cline{2-7}
         & \multicolumn{3}{c|}{\textbf{en-de}} & \multicolumn{3}{c|}{\textbf{en-es}} \\
         \specialrule{.1em}{.05em}{.05em} 
        \multirow{2}{*}{\textbf{Code}} & \multicolumn{3}{c|}{\textbf{IBS}} & \multicolumn{3}{c}{\textbf{IBS}} \\
        \cline{2-7}
        & 1 & 10 & 100 & 1 & 10 & 100 \\
        \specialrule{.1em}{.05em}{.05em} 
        \smallcorrect & 24.67 & 24.67 & 24.67 & 30.34 & 30.34 & 30.34 \\
        \hline
        + TF32 & \textbf{24.84} & \textbf{24.84} & 24.83 & \textbf{30.63} & 30.62 & \textbf{30.63} \\
        \quad + \bugone & 24.52 & 24.65 & 24.67 & 29.53* & 29.41* & 27.71* \\
        \quad + \bugtwo & 24.56 & 24.57 & 24.58 & 30.53 & 30.53 & 30.53 \\
        \quad + \bugthree & 24.53 & 24.46 & 24.42 & 30.33 & 30.35 & 30.24\\
        \quad + \bugall & 24.68 & 24.58 & 23.23* & 28.57* & 27.81* & 21.15* \\
    \specialrule{.1em}{.05em}{.05em} 
    \end{tabular}
    \caption{BLEU
    %($\uparrow$)
    %scores 
    for ST with TF32 and bugs as IBS varies (1, 10, and 100 sentences).
    *~indicates that the difference with \smallcorrect{} is statistically significant.
    }
    \label{tab:ablationST}
\end{table}




\begin{table}[t]
\setlength{\tabcolsep}{4pt}
\centering
\small
\begin{tabular}{l|cc}
\specialrule{.1em}{.05em}{.05em} 
\textbf{Model} & \textbf{en-de} & \textbf{en-es} 
 \\
\specialrule{.1em}{.05em}{.05em} 
 ESPNet \cite{inaguma-etal-2020-espnet} & 22.9 & 28.0 \\
 Fairseq \cite{wang2020fairseqs2t} & 22.7 & 27.2 \\
 Speechformer \cite{papi-etal-2021-speechformer} & 23.6 & 28.5 \\
 E2E + ML \citep{zhao-etal-2021-mutual} & - & 28.5 \\
 SATE (no KD) \citep{xu-etal-2021-stacked} & 24.1 & - \\
 E2E-ST-FS \citep{pmlr-v162-zhang22i} & 23.0 & 28.0 \\
 S2T-Perceiver \cite{Tsiamas-2023-Perceivers} & 24.2 & 28.0\\
 \hline
 Conformer \bugall & \textbf{24.7} & \textbf{28.6} \\
\specialrule{.1em}{.05em}{.05em} 
\end{tabular}
\caption{BLUE
%($\uparrow$) scores 
of models trained on MuST-C en-de and en-es compared to Conformer \bugall{} (IBS=1).}
\label{tab:comparison_others}
\end{table}



\begin{table*}[t!]
\setcounter{table}{6}
\setlength{\tabcolsep}{4.5pt}
    \centering
    \small
    \begin{tabular}{c|c|c||c|c|c|c|c|c|c|c||c}
    \specialrule{.1em}{.05em}{.05em} 
        \textbf{Code} & \textbf{Model} & \textbf{IBS} & \textbf{en-de} & \textbf{en-es} & \textbf{en-fr} & \textbf{en-it} & \textbf{en-nl} & \textbf{en-pt} & \textbf{en-ro} & \textbf{en-ru} & \textbf{Avg} \\
        \specialrule{.1em}{.05em}{.05em} 
        \multirow{6}{*}{\correct} & \multirow{3}{*}{Conformer} & 1 & \multirow{3}{*}{24.67} & \multirow{3}{*}{30.34} & \multirow{3}{*}{36.22} & \multirow{3}{*}{25.73} & \multirow{3}{*}{30.04} & \multirow{3}{*}{\textbf{30.55}} & \multirow{3}{*}{23.43} & \multirow{3}{*}{17.29} & \multirow{3}{*}{27.28} \\
        & & 10 & & & & & & & & \\
        & & 100 & & & & & & & & \\
        \cline{2-12}
        & \multirow{3}{*}{\shortstack{Conformer\\+\\CTC Compr.}} & 1 & \multirow{3}{*}{24.97} & \multirow{3}{*}{30.48} & \multirow{3}{*}{\textbf{36.43}} & \multirow{3}{*}{\textbf{26.25*}} & \multirow{3}{*}{\textbf{30.31}} & \multirow{3}{*}{30.09\textsuperscript{$\dagger$}} & \multirow{3}{*}{\textbf{24.67*}} & \multirow{3}{*}{\textbf{17.35}} & \multirow{3}{*}{\textbf{27.57}} \\
        & & 10 & & & & & & & & \\
        & & 100 & & & & & & & & \\
        \hline
        \multirow{6}{*}{\bug\textsubscript{1,2,3}} & \multirow{3}{*}{Conformer} & 1 & 24.68 & 28.57 & 35.70 & 25.81 & 29.68 & 30.22 & 23.52 & 15.83 & 26.75 \\
        & & 10 & 24.58 & 27.81 & 35.65 & 25.70 & 29.35 & 30.02 & 23.43 & 15.36 & 26.49 \\
        & & 100 & 23.23 & 21.15 & 31.70 & 23.42 & 24.92 & 27.72 & 22.68 & 11.05 & 23.23 \\
        \cline{2-12}
        & \multirow{3}{*}{\shortstack{Conformer\\+\\CTC Compr.}} & 1 & 24.95 & 30.49* & 36.27* & 25.84 & 29.42 & 30.04 & 23.96* & 17.05* & 27.25 \\
        & & 10 & 25.21* & \textbf{30.72}* & 36.18* & 26.01 & 29.64 & 30.14 & 23.95* & 17.06* & 27.36 \\
        & & 100 & \textbf{25.26}* & 30.52* & 36.36* & 25.88* & 29.66* & 30.16* & 23.92* & 16.87* & 27.33 \\
    \specialrule{.1em}{.05em}{.05em} 
    \end{tabular}
    \caption{BLEU
    %($\uparrow$) scores 
    for ST of the correct/incorrect codebase with and without CTC 
    %compression 
    \sara{Compr.} as IBS varies (1, 10, and 100).
    */\textsuperscript{$\dagger$} indicate that the improvement/degradation of CTC %compression 
    \sara{Compr.} is statistically significant.}
    \label{tab:ST}
\end{table*}

\paragraph{ST} Table \ref{tab:ablationST} reports the same study on the two most used sections of MuST-C (en-de, and en-es).
The behavior is quite different between the two, but the best scores are always obtained with TF32 and without bugs.
On en-es, \bugone{} causes statistically significant drops that increase 
with the IBS and are exacerbated if combined with the other two bugs (\bugall).
On en-de, instead, none of the bugs significantly impacts the results.
Interestingly, the result obtained with all bugs (\bugall) and 1 as IBS is slightly higher (+0.01) than that without bugs (\smallcorrect).
Furthermore, by comparing the scores obtained with all bugs (\bugall) and 1 as IBS with those of previous ST works (Table \ref{tab:comparison_others}), we can notice that, as previously observed for ASR, the presence of bugs is  not evident from the results, which are still competitive with those of other models.
This supports our conclusion that \textbf{\textit{good (and reproducible) results do not imply code correctness}}, as this statement holds for different tasks and language pairs. 



\subsection{Impact of Building on Incorrect Code}
\label{subsec:impact_code}

We now showcase how incorrect code can lead to misleading conclusions when experimenting with the introduction of a new technique.
We choose to evaluate the CTC compression \citep{liu2020bridging,gaido-etal-2021-ctc}. As this technique reduces the length of the sequences and, hence, the amount of padding, we speculate that it limits the negative effects of the bugs identified in \S\ref{subsec:analysis}.
CTC compression 
%\citep{liu2020bridging,gaido-etal-2021-ctc} 
has been proposed to reduce the difference in terms of sequence length between corresponding audio and text representations.
% It leverages the predictions of the CTC loss \citep{Graves2006ConnectionistTC}, which produces a probability distribution over the possible target labels augmented with a \texttt{<blank>} symbol.
Different from fixed reduction methods like max pooling or strided convolutions that apply a predetermined reduction to each sequence, it leverages the predictions of the CTC loss \citep{Graves2006ConnectionistTC}, which produces a probability distribution over the possible target labels augmented with a \texttt{<blank>} symbol.
These probability distributions are used to assign a label (the most likely one) to each vector of the sequence and collapse contiguous vectors corresponding to the same label by averaging them.
By dynamically determining which vectors of the audio sequence should be merged, it tries to avoid the mismatch in terms of sequence length with  the sub-word sequences of the corresponding 
%transcripts and differs from fixed reduction methods like max pooling or strided convolutions, which apply a predetermined reduction to each sequence.
transcripts.
Introduced in the context of Transformer-based models, 
CTC compression considerably speeds up both the training and inference phases and limits the VRAM requirements. When
used with transcripts as the target of the CTC loss, as we do here, it only yields minimal (not statistically significant) gains in terms of translation quality \cite{gaido-etal-2021-ctc}.


% Specifically, we evaluate a method that we hypothesize to limit the negative effects of the bugs identified in \S\ref{subsec:analysis}, as it reduces the length of the sequences and, hence, the amount of padding: the CTC compression \citep{liu2020bridging,gaido-etal-2021-ctc}.
% \citet{gaido-etal-2021-ctc} demonstrated that, in the context of Transformer-based models, this technique considerably speeds up both the training and inference phases and limits the VRAM requirements, while providing minimal (not statistically significant) gains in terms of translation quality.
% A detailed description of the CTC compression is provided in Appendix \ref{app:ctc_compr}.

\begin{table}[h]
\setlength{\tabcolsep}{4.5pt}
\setcounter{table}{5}
    \centering
    \small
    \begin{tabular}{l|c|ccc}
    \specialrule{.1em}{.05em}{.05em} 
        \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Code}} & \multicolumn{3}{c}{\textbf{IBS}} \\
        \cline{3-5}
         & & 1 & 10 & 100 \\
        \specialrule{.1em}{.05em}{.05em} 
        Conformer & \multirow{2}{*}{\smallcorrect} & 10.52 & 10.52 & 10.52 \\
        \hspace{0.5em}+ CTC Compr. & & 10.64 & 10.64 & 10.64 \\
        \hline
        Conformer & \multirow{2}{*}{\bugall} & 11.32 & 14.25 & 54.56 \\
        \hspace{0.5em}+ CTC Compr. & & 10.39* & \textbf{10.34}* & 10.81* \\ 
    \specialrule{.1em}{.05em}{.05em} 
    \end{tabular}
    \caption{WER
    %($\downarrow$) scores 
    for ASR of the correct/incorrect codebase with and without CTC
    %compression 
    \sara{Compr.} as IBS varies (1, 10, and 100).
    * indicates that the improvement of CTC 
    %compression 
    \sara{Compr.} is statistically significant.}
    \label{tab:ASR}
\end{table}


\paragraph{ASR}
Table \ref{tab:ASR} shows the effects on 
% the ASR
ASR performance of 
%the introduction of the
introducing CTC compression  \mg{(CTC Compr.)}
%to both codebases (\bugall{} and \smallcorrect).
into the codebase with all bugs~(\bugall{}) and without them~(\smallcorrect).
We can notice that 
%the
CTC compression causes a small and not statistically significant performance degradation (+0.12 WER) when the correct implementation (\smallcorrect{}) is used (in accordance with the findings of \citealt{gaido-etal-2021-ctc} on the Transformer architecture).
When bugs are present in the codebase (\bugall), instead, the conclusion is overturned:
% the
CTC compression brings statistically significant
%improvements
gains
(-0.93
%nearly -1.00
WER even with 1 as IBS).
We can conclude that \textit{\textbf{building on incorrect code can produce misleading findings}}.
%In addition, 
Besides, the best overall result is achieved with the \bugall{} codebase (with 10 as IBS and CTC compression), reiterating that high scores do not imply code correctness.




\paragraph{ST}
Table \ref{tab:ST} reports the same analysis on the 8 language pairs of MuST-C.
As in ASR, the presence of bugs (\bugall) unduly rewards the CTC compression mechanism, which yields statistically significant gains on all the languages with 100 as IBS and on 4/5 out of 8 languages with 1/10 as IBS.
With the bug-free version (\smallcorrect), instead, the improvements are statistically significant only on two language pairs (en-it, and en-ro), while on en-pt there is a statistically significant degradation.
On average over all language pairs, the gain brought by 
%the
CTC compression in the presence of bugs (\bugall) ranges from 0.5 BLEU (with 1 as IBS) to 4.1 BLEU (with 100 as IBS), while it is only of 0.29 BLEU with the correct code~(\smallcorrect). 
We can hence confirm that \textbf{\textit{the presence of bugs led to erroneous findings}} also in the ST task, as
%the
CTC compression seems to significantly improve translation quality, which is not the case with the correct Conformer implementation
% of Conformer 
(as well as with Transformer, as proved by \citealt{gaido-etal-2021-ctc}).
%(and with the Transformer architecture as per \citealt{gaido-etal-2021-ctc}).
Moreover, the best scores for en-de and en-es are achieved with the presence of all bugs (\bugall), and the average performance gap between codebases with (\bugall{}) and without (\smallcorrect{}) bugs can be as little as 0.21 BLEU (when IBS is 10) and may be further
narrowed, or even overturned, 
by \enquote{tuning} the IBS.
This demonstrates again the \textit{\textbf{impossibility to assess code correctness only by looking at the results}}.
