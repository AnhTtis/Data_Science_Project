To assess the level of consideration given to the above SQA attributes within the NLP research community, we examined their inclusion in the review forms\footnote{Checked on January 15th 2023.} of top-tier conferences and journals in the field, namely: 
*ACL (AACL-IJCNLP, ACL, EACL, EMNLP, NAACL), ACL Rolling Review (ARR), ICASSP, ICML, ICLR, Interspeech, NeurIPS, and TACL.
We specifically focused on reproducibility (as a proxy of portability and usability) and correctness. Table \ref{tab:nlp} shows the results.



Most of the venues (5 out of 8) include an explicit score for reproducibility and NeurIPS mentions it among the factors contributing to the overall recommendation score. Reproducibility is commonly evaluated through dedicated checklists\footnote{E.g., NeurIPS (\url{https://neurips.cc/Conferences/2021/PaperInformation/PaperChecklist}), AAAI (\url{https://aaai.org/Conferences/AAAI-22/reproducibility-checklist}), and ARR (\url{https://aclrollingreview.org/responsibleNLPresearch}).} that mainly focus on the detailed descriptions of the hyperparameters and the software/hardware environment (while disregarding whether different hardware/software is supported, i.e. portability, which seems reasonable as seen in \S\ref{sec:sqa}).
Accordingly, these checklists are not strictly related to SQA, although they do include recommendations for proper code documentation, which is related to the software usability and maintainability.

Correctness is instead mentioned in fewer forms (3 out of 8). When present, its definition varies and is not explicitly related to the code: at ICLR and ICASSP, the scope of the term is not clearly defined, while in TACL it is included in the broader concept of \textit{soundness} of the experiments/results.
In this result-oriented definition, soundness pertains to assessing the significance of results (\textit{are the reported improvements robust to statistical fluctuations?}) with respect to either the state of the art (\textit{are the results competitive with those reported in recent literature?}) or strong baselines.
Soundness is also assessed at NeurIPS and ICML but, again, code correctness is never explicitly mentioned.
Notably, the Interspeech form contains a \enquote{Technical Correctness} score, which however refers to the reproducibility of the paper (\enquote{\textit{are enough details provided to be able to reproduce the experiments?}}).
In general, when considered, software is explicitly evaluated only in terms of accessibility (\textit{is the code released open-source?}) and potential usefulness (\textit{will the research community benefit from the use of the software?}).
For instance, the \enquote{Software} score in the ARR form only refers to the usefulness and documentation of newly-released  code rather than to its correctness, thus being again more related to its usability and maintainability.

\begin{table}[!t]
    \centering
    \small
    \setlength{\tabcolsep}{8pt}
    \begin{tabular}{l||c|c}
    \specialrule{.1em}{.05em}{.05em} 
        \textbf{Venue} & \textbf{Reproducibility} & \textbf{Correctness} \\
        \specialrule{.1em}{.05em}{.05em} 
        *ACL &  \smallcorrect{} & \smallwrong{} \\
        ARR & \smallcorrect{} & \smallwrong{} \\
        ICASSP & \smallwrong{} & \smallcorrect{} \\
        ICML & \smallwrong{} & \smallwrong{} \\
        ICLR & \smallcorrect{} & \smallcorrect{} \\
        Interspeech & \smallcorrect{} & \smallwrong{} \\
        NeurIPS & \smallwrong{} & \smallwrong{} \\
        TACL & \smallcorrect{} & \smallcorrect{} \\
    \specialrule{.1em}{.05em}{.05em} 
    \end{tabular}
    \caption{Reproducibility and correctness in the review forms of major NLP conferences/journals.}
    \label{tab:nlp}
\end{table}

We can conclude that, unlike reproducibility-related SQA attributes, correctness is largely neglected in favor of a result-based evaluation of soundness.
From the researchers' perspective, this entails the risk of basing future work on unreliable software that yields high and easily reproducible results but lacks guarantees of its correctness.
This risk, in turn, can lead to misleading findings \citep{McCullough-2008-replicable}. In the next section, we present a concrete instance of this problem with a case study analyzing open-source implementations of the widespread Conformer architecture.