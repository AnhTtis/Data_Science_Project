% File tacl2018v2.tex
% Sep 20, 2018

% The English content of this file was modified from various *ACL instructions
% by Lillian Lee and Kristina Toutanova
%
% LaTeXery is mostly all adapted from acl2018.sty.

\documentclass[11pt,a4paper]{article}
\usepackage{times,latexsym}
\usepackage{url}
\usepackage[T1]{fontenc}

%% Package options:
%% Short version: "hyperref" and "submission" are the defaults.
%% More verbose version:
%% Most compact command to produce a submission version with hyperref enabled
%%    \usepackage[]{tacl2018v2}
%% Most compact command to produce a "camera-ready" version
%%    \usepackage[acceptedWithA]{tacl2018v2}
%% Most compact command to produce a double-spaced copy-editor's version
%%    \usepackage[acceptedWithA,copyedit]{tacl2018v2}
%
%% If you need to disable hyperref in any of the above settings (see Section
%% "LaTeX files") in the TACL instructions), add ",nohyperref" in the square
%% brackets. (The comma is a delimiter in case there are multiple options specified.)

\usepackage[acceptedWithA]{tacl2018v2}




%%%% Material in this block is specific to generating TACL instructions
\usepackage{xspace,mfirstuc,tabulary}
\newcommand{\dateOfLastUpdate}{Sept. 20, 2018}
\newcommand{\styleFileVersion}{tacl2018v2}

\newcommand{\ex}[1]{{\sf #1}}

\newif\iftaclinstructions
\taclinstructionsfalse % AUTHORS: do NOT set this to true
\iftaclinstructions
\renewcommand{\confidential}{}
\renewcommand{\anonsubtext}{(No author info supplied here, for consistency with
TACL-submission anonymization requirements)}
\newcommand{\instr}
\fi

%
\iftaclpubformat % this "if" is set by the choice of options
\newcommand{\taclpaper}{final version\xspace}
\newcommand{\taclpapers}{final versions\xspace}
\newcommand{\Taclpaper}{Final version\xspace}
\newcommand{\Taclpapers}{Final versions\xspace}
\newcommand{\TaclPapers}{Final Versions\xspace}
\else
\newcommand{\taclpaper}{submission\xspace}
\newcommand{\taclpapers}{{\taclpaper}s\xspace}
\newcommand{\Taclpaper}{Submission\xspace}
\newcommand{\Taclpapers}{{\Taclpaper}s\xspace}
\newcommand{\TaclPapers}{Submissions\xspace}
\fi

%%%% End TACL-instructions-specific macro block
%%%%
\usepackage{graphicx}
\usepackage{csquotes}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{microtype}
\usepackage{colortbl}
\usepackage{subcaption}
%\usepackage[table,x11names,dvipsnames]{xcolor}
\usepackage{twemojis}
\usepackage{tikz}
\usepackage{threeparttable}
\usepackage{ragged2e}
\usepackage{soul}
\usepackage{enumitem}
\usepackage{etoolbox}
\AtBeginEnvironment{tcolorbox}{%
\setlist[itemize]{nosep,
                 leftmargin=*,
                 label=\textbullet,
                 before=\begin{minipage}[t]{\linewidth}, % <---
                 after=\end{minipage}\medskip}                   % <---
                            }

\usepackage[most]{tcolorbox}
\tcbuselibrary{raster}


\newcommand{\bug}{\scalebox{1.75}{\twemoji{beetle}}}
\newcommand{\bugone}{\scalebox{1.25}{\twemoji{beetle}}\textsubscript{1}}
\newcommand{\bugtwo}{\scalebox{1.25}{\twemoji{beetle}}\textsubscript{2}}
\newcommand{\bugthree}{\scalebox{1.25}{\twemoji{beetle}}\textsubscript{3}}
\newcommand{\bugall}{\scalebox{1.25}{\twemoji{beetle}}\textsubscript{1,2,3}}
\newcommand{\smallbug}{\scalebox{1.25}{\twemoji{beetle}}}
\newcommand{\correct}{\scalebox{1.75}{\twemoji{check mark button}}}
\newcommand{\smallcorrect}{\scalebox{1.25}{\twemoji{check mark button}}}
\newcommand{\wrong}{\scalebox{1.75}{\twemoji{cross mark}}}
\newcommand{\smallwrong}{\scalebox{1.25}{\twemoji{cross mark}}}
\newcommand{\checkinbox}{\scalebox{1}{\twemoji{black square button}}}
\newcommand{\idea}{\scalebox{1.25}{\twemoji{light bulb}}}
\newcommand{\code}{\scalebox{1.25}{\twemoji{laptop}}}
\newcommand{\results}{\scalebox{1.25}{\twemoji{bar chart}}}
\newcommand{\school}{\scalebox{1.25}{\twemoji{school}}}
% \newcommand{\torchimpl}{PyTorch}
\newcommand{\torchimpl}{TorchAudio}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\urlstyle{same}
\interfootnotelinepenalty=10000


% \newcommand{\sara}{\textcolor{magenta}}
% \newcommand{\mn}{\textcolor{red}}
% \newcommand{\mg}{\textcolor{olive}}


\newcommand{\sara}{\textcolor{black}}
\newcommand{\mn}{\textcolor{black}}
\newcommand{\mg}{\textcolor{black}}


\title{When Good and Reproducible Results are a Giant with Feet of Clay:\\ The Importance of Software Quality in NLP}



% Author information does not appear in the pdf unless the "acceptedWithA" option is given
% See tacl2018v2.sty for other ways to format author information
\author{Sara Papi\textsuperscript{\idea\results\school}, Marco Gaido\textsuperscript{\idea\results}, Andrea Pilzer\textsuperscript{\code}, Matteo Negri\textsuperscript{\results} \\
  \results Fondazione Bruno Kessler \\
  \school University of Trento \\
  \code NVIDIA \\
  \texttt{\{spapi,mgaido,negri\}@fbk.eu,apilzer@nvidia.com}
}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Despite its crucial role in research experiments, code correctness is often presumed only on the basis of the perceived quality of results. This assumption comes with the risk of erroneous outcomes and potentially misleading findings. To address this issue, we posit that the current focus on reproducibility should go hand in hand with 
the
% \mn{more}
emphasis on software quality. We present a case study in which we identify and fix three bugs in widely used implementations of the state-of-the-art Conformer architecture. Through experiments on speech recognition and translation in various languages, we demonstrate that the presence of bugs does not prevent the achievement of good and reproducible results, which however can lead to incorrect conclusions that potentially misguide future research. 
As a countermeasure, we propose a Code-quality Checklist and release \texttt{pangoliNN}, a library dedicated to testing neural models, with the goal of promoting coding best practices and improving research software quality within the NLP community.
\end{abstract}

\blfootnote{\idea Denotes equal contributions.}

\section{Introduction}
\label{sec:intro}
In the field of natural language processing (NLP), as well as in broader contexts, the validity and soundness of research findings are typically upheld by \enquote{\textit{establishing consistency of results versus existing implementations, standard benchmarks, or sanity checks via statistically significant experimental results}} \citep{Rozier-2014-repro}. Nevertheless, recent evidence  indicates that several of these aspects are absent in many papers \citep{NEURIPS2019_c429429b}, questioning the scientific credibility of NLP research. 

On one hand, many scientists have reported difficulties in replicating the work of others, or even their own, \citep{prinz2011believe,Gundersen_Kjensmo_2018,10.1162/coli_a_00330,Chen2019,Gundersen_2019}, also in the specific context of NLP \citep{wieling-etal-2018-squib,belz-etal-2021-systematic,marie-etal-2021-scientific,narang-etal-2021-transformer,gehrmann2022repairing,belz-etal-2022-quantified,arvan-etal-2022-reproducibility-code}. In response to this issue, referred to as the ``\textit{reproducibility crisis}'' \citep{Baker2016}, significant attention and dedicated initiatives have recently been focused on the crucial role of reproducibility in scientific research \citep{the_turing_way_community_2019_3233986,dodge-etal-2019-show,branco-etal-2020-shared,belz-etal-2021-reprogen,pineau2021improving,rogers-etal-2021-just-think,10.1162/coli_a_00448}.

On the other hand, \citet{denkowski-neubig-2017-stronger,dror-etal-2018-hitchhikers,marie-etal-2021-scientific} documented a disregard for best evaluation practices in machine translation (MT). This includes the failure to utilize strong baselines, standardized metric scores, consistent experimental settings, and proper statistical significance testing, all of which undermine the reliability of the research findings.

Assuming that the growing awareness of these concerns and the ongoing endeavors will tackle them, a fundamental question remains:  \textbf{Are reproducibility and thorough evaluation against robust baselines sufficient to ensure the validity of research findings?} \citet{Peng-2011-reproducible} showed that reproducibility alone is insufficient to \enquote{\textit{guarantee the quality, correctness, or validity of the published results}}, since the code employed to produce them may not accurately execute its intended purpose. Furthermore, a flawed code that produces good and easily reproducible  results runs the risk of becoming the foundation for further research, ultimately resulting in unreliable and potentially misleading findings \citep{McCullough-2008-replicable}.

In light of the above considerations, this study is a call to action, underpinned by empirical evidence, to enhance the dependability  of published NLP findings. In particular, our contributions are:
\begin{enumerate}[leftmargin=14pt]

\item We examine the extent to which research works consider the attributes studied in the field of software quality assurance \citep{Buckley-1984-sqa,tripathy2011software}, or SQA (\S\ref{sec:sqa}), and show  that code correctness has been neglected by the NLP community thus far (\S\ref{sec:core-idea});
    
\item Through a case study on  open-source implementations of the widespread Conformer architecture \citep{gulati20_interspeech}, we prove that:
\begin{itemize}[leftmargin=8pt]
\item[-] At least one impactful bug is present in all the analyzed implementations (\S\ref{subsec:analysis});
%All the analyzed implementations contain at least one impactful bug in the codebase (\S\ref{subsec:analysis});
\item[-] 
%The existence of bugs does not prevent from achieving
Bugs do not
prevent from achieving
% hinder
%the attainment of
good and
reproducible results that outperform
%\mg{those of}
other architectures
%across different tasks and
in speech recognition and translation across different
language 
%settings
pairs
(\S\ref{subsec:impact_bug});
\item[-] 
%The presence of bugs can lead to incorrect findings when incorporating a new technique into the architecture
Bugs can lead to erroneous conclusions when evaluating new techniques
(\S\ref{subsec:impact_code}).
\end{itemize}
\item We release\footnote{\label{foot:sw_release}Upon paper acceptance, under the Apache 2.0 License.} a bug-free implementation of Conformer and all 
our pre-trained models;
%the pre-trained models used in our experiments;
%
%
%
% \item  We release\textsuperscript{\ref{foot:sw_release}} a library including easily-usable tests named \texttt{pangoliNN}\footnote{As a pangolin looks for bugs and catches them, this library aims at finding bugs in neural networks (NN). Hence the name.} to enforce the proper behavior of neural models (\S\ref{subsec:pangolinn}) and propose to integrate the checklist of current conferences with questions about coding best practices in order to fostering code correctness and software quality in research works  (\S\ref{subsec:checklist}).
\item  \mn{We release\textsuperscript{\ref{foot:sw_release}} 
%\texttt{pangoliNN},\footnote{As a pangolin looks for bugs and catches them, this library aims at finding bugs in neural networks (NN). Hence the name.} 
\texttt{pangoliNN},
a library including easily-usable tests to enforce the proper behavior of neural models (\S\ref{subsec:pangolinn}), and propose to integrate the checklist of current conferences with questions about coding best practices in order to fostering code correctness and software quality in research works  (\S\ref{subsec:checklist}).}
\end{enumerate}


\section{SQA and Research}
\label{sec:sqa}
\input{sections/sqa}


\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.95\textwidth]{img/conformer_con.drawio.png}
    \caption{Convolution module in the Conformer encoder layer. Convolutional blocks are 1D convolutions.}
    \label{fig:conf_conv}
\end{figure*}


\section{Research Code Quality Evaluation}
\label{sec:core-idea}
%\url{https://aaai.org/conference/aaai/aaai-23/reproducibility-checklist/}
\input{sections/code_quality_eval}




\section{The Case Study}
\label{sec:case-study}
\input{sections/case_study}




\section{Increasing Research Code Correctness}
\label{sec:checklist}
\input{sections/proposal}



\section{Conclusions}

In parallel with the strand of action toward increasing the reproducibility of NLP research, this paper investigated the impact of software quality attributes on research work and their relevance to the field. By comparing the attention that reproducibility and software quality receive within the research community, we highlighted the predominant neglect of code correctness and the risks of assessing soundness only on the basis of experimental results. Such risks have a significant impact on current research findings and include the potential of drawing misleading conclusions, as we empirically demonstrated through a case study involving the widespread Conformer architecture. 
As a countermeasure,  
% we proposed the adoption of a \enquote{Code-quality Checklist} with the goal of fostering coding best practices. In addition, we created the \texttt{pangoliNN} Python package to facilitate testing neural models. 
we created the \texttt{pangoliNN} Python package to facilitate testing neural models and proposed the adoption of a \enquote{Code-quality Checklist} with the goal of fostering coding best practices.
While we acknowledge that these solutions are not a panacea, their purpose is to raise awareness within the NLP community about the importance of software quality. We hope that our call to action will inspire further initiatives aimed at addressing the current lack of incentives and promoting the development of high-quality code.



% \section*{Limitations}
% To back up our call to action toward the adoption of coding best practices aimed at fostering correctness and improving the quality of the developed software, we presented a case study involving the use of the Conformer architecture in the two most popular speech processing tasks: speech recognition and translation. Although the effects of the presence of bugs might be found also in other  scenarios, such as  text-to-speech, speech emotion recognition, spoken language understanding, and speech separation, we did not cover them in this paper.  While the undesired effect of the bugs we isolated (and corrected) was empirically demonstrated, extending the analysis to other research areas would be a natural extension of our study, which could provide a more comprehensive understanding of the impact of the identified bugs on the broader NLP community working on speech-related tasks.

% Moreover, in our case study, we examined the open-source implementations of Conformer, in which we identified three types of bugs related to the Convolution Module, Initial Subsampling, and Positional Encodings. While we found efficient solutions for the first two bugs, for the last one our fix introduces a significant overhead. As a result, the implementation we release, although correct, increases the training time of the models. We are confident that, by open-sourcing our code, the community will soon find a way to optimize it and overcome this limitation, capitalizing on our findings and spreading the use of more reliable versions of a state-of-the-art architecture.



\bibliography{tacl2018}
\bibliographystyle{acl_natbib}

\end{document}


