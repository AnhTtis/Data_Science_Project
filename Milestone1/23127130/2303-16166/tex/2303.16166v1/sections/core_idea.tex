%\url{https://aaai.org/conference/aaai/aaai-23/reproducibility-checklist/}

% \sara{Most of scientific publications are made of three key elements (Figure \ref{fig:scientificartifact}):}

In NLP, a 
%reputable 
scientific contribution typically builds on three main constituent elements (Fig. \ref{fig:scientificartifact}):
\sara{\textbf{concept} (the original idea or hypothesis being tested), \textbf{implementation} (the software developed to test the concept), and \textbf{outcomes} (the results obtained from the experiments carried out by using the code). }

%: the idea, the code, and the results}

%In a scientific publication,
%%introducing a scientific artifact,
%%In a scientific publication reporting experimental results, 
%we can identify three building blocks in the process \sara{leading} from the idea to the results (Figure \ref{fig:scientificartifact}) \mn{[oppure qualcosa tipo: ``In a broad sense, a reputable scientific contribution is based on three main constituent elements: the idea, the code, and the results....'']}:
%we can divide the process aimed at obtaining the empirical results starting from the idea into three elements (Figure \ref{fig:scientificartifact}):
%\begin{itemize}
%    \item[\idea] 
    %\textbf{idea}: the intuition or the theoretical motivation behind the work; 
%    \mn{\textbf{concept}:}  the original idea or hypothesis being tested;
    %, which should be novel, relevant, and well-supported by existing knowledge and literature; 
    %
    %
    % \item[\code] \textbf{code}: the piece of code developed     \sara{that implement}
    % the idea;
%    \item[\code] \mn{\textbf{implementation}: the software developed to test the concept;}
    % \item[\results] \textbf{results}: the 
    % %outputs 
    % \sara{outcomes}
    % obtained from the experiments done by using the code.
%    \item[\results] \mn{\textbf{outcomes}: the  results    obtained from the experiments carried out by using the code.}
%\end{itemize}
%
%
%
%The  \textbf{idea} is the foundation of the publication and is often evaluated in the reviewing process based on its \textit{novelty} (i.e., how original the idea is), \textit{soundness} (i.e., if the idea is based on solid theoretical basis), and \textit{impact} (i.e., how important the idea is for the scientific community).
The \textbf{concept} is the foundation of the publication. In the reviewing process it is typically evaluated based on its \textit{novelty} (i.e., how original the idea is), \textit{soundness} (i.e., if the idea is grounded on solid theoretical bases), and \textit{impact} (i.e., how important the idea is for the scientific community).
%
%
% 
%The \textbf{code} is a scientific artifact created to realize the idea and is generally evaluated by looking at its \textit{usefulness} (i.e., if the research community will benefit from the use of the software) and \textit{accessibility} \sara{(i.e., if the code is released open source)}.
The \textbf{implementation}
is a scientific artifact created to realize and test the concept. When present, it is generally evaluated by looking at its \textit{accessibility} (i.e., if the code is released open source), and its \textit{usefulness} (i.e., if the research community will benefit from the use of the software).
%, and \textit{usability} (i.e. if the code is adequately documented).
%
%
%
% The \textbf{results} are often carefully checked to be \textit{state of the art} 
% %\mn{[OCCHIO: NON SOLO...CI PUO' ESSERE UN NUOVO TASK PER CUI NON ESISTE SOTA...SEMMAI UNA BASELINE RAGIONEVOLE DA BATTERE.]} 
% (i.e., the results are competitive with those of recent works in literature) \sara{or better than those of a \textit{strong baseline} in absence of related works}, and \textit{significant} (i.e., the results actually represent an improvement or are relevant for the community \sara{and are robust from statistical fluctuations}).
\mn{The \textbf{outcomes} are usually 
%carefully 
checked in comparison either with the \textit{state of the art} (i.e., the results are competitive with those of recent works in literature) or with a \textit{strong baseline} (in absence of related works), looking at their \textit{statistical significance} (i.e., the 
%results actually represent an improvement that is robust from
improvements are robust to
statistical fluctuations).}






% Aside from considering these elements separately, it is important to evaluate how they are interconnected, \mn{i.e., the connection between the idea and the code, and between the code and the results.}
\mn{Aside from considering these elements separately, it is important to evaluate how they are interconnected.}
%The link between code and results is currently named under \textbf{reproducibility} and evaluating this aspect corresponds to answering the question: \textit{can the results be actually produced by the codebase?}
%
%
%
%Evaluating the link between code and results corresponds to answering the question: \textit{can the results be actually produced by the codebase?} In other words, are results \textbf{reproducible} with the code?
\mn{Evaluating the link between implementation and outcomes
corresponds to answering the question: ``\textit{Can the results be actually produced by the codebase?}'' or, in other words, ``\textit{Are the results \textbf{reproducible} with the code?}''.}
%Although it appears to be a question with a straightforward answer, there are numerous discussions regarding the definition of reproducibility in research. 
%\mg{Indeed, the}
%
%
%
%
% The Association for Computing Machinery (ACM) defines the term reproducible as 
\mn{According to the Association for Computing Machinery (ACM), 
%a code is reproducible if
reproducibility holds when}
\enquote{\textit{an independent group can obtain the same result using the authorâ€™s own artifacts}},\footnote{\url{https://www.acm.org/publications/policies/artifact-review-and-badging-current}} which is coherent with the definition given \mg{both} in other fields \citep{doi:10.1128/mBio.00525-18} and
%by related works on experimental standards in NLP 
\mg{in NLP} \citep{ulmer2022experimental}.
%Moreover, due to its importance and the lack of a unique widespread definition,
%\mg{Since reproducibility is a cornerstone of science, much attention has been dedicated to the topic, and}
%Although the answer seems straightforward,
%several
\mg{Several works have been dedicated to this topic, claiming}
%works claimed 
the presence of a \enquote{reproducibility crisis},
%systematically analyzed the reproducibility of published papers 
as their systematic analyses questioned the reproducibility of a large portion of published papers
\citep{prinz2011believe,Baker2016,Gundersen_Kjensmo_2018,NEURIPS2019_c429429b,Gundersen_2019}, also in the specific context of NLP \citep{wieling-etal-2018-squib,belz-etal-2021-systematic,belz-etal-2022-quantified}.
Most of them argue that the published works lack consistent evaluation settings \citep{marie-etal-2021-scientific,gehrmann2022repairing} and cannot be reproduced \citep{narang-etal-2021-transformer}, even 
%if codebases are available 
with codebases available
\citep{arvan-etal-2022-reproducibility-code}.
\mg{To mitigate this issue, numerous scientific organizations and associated journals and conferences, such as NeurIPS\footnote{\url{https://neurips.cc/Conferences/2021/PaperInformation/PaperChecklist}}, AAAI\footnote{\url{https://aaai.org/Conferences/AAAI-22/reproducibility-checklist/}}, and the ACL community\footnote{\url{https://aclrollingreview.org/responsibleNLPresearch/}}, have recently introduced the completion of checklists aimed at enhancing the reproducibility of published material \citep{dodge-etal-2019-show,pineau2021improving,rogers-etal-2021-just-think}.}
The emphasis on the topic is \mg{further} confirmed by the organization of dedicated workshops and shared tasks on this \sara{theme}
\citep{Pineau:2019,belz-etal-2021-reprogen}, and its inclusion as part of
%recent 
conference theme tracks.\footnote{\url{https://2022.emnlp.org/calls/main_conference_papers/\#emnlp-2022-theme-track}, \url{https://2023.aclweb.org/calls/main_conference/\#theme-track-reality-check}}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.45\textwidth]{img/idea.png}
    \caption{Elements of a scientific publication.}
    \label{fig:scientificartifact}
\end{figure}

%It emerges that extensive literature exists that places emphasis on the aspect of reproducibility which has also recently pushed the community 
%to develop reproducibility checklists to be compiled before the paper submission \citep{dodge-etal-2019-show,pineau2021improving,rogers-etal-2021-just-think}, 
%to organize workshops and shared tasks on this thematic \citep{Pineau:2019,belz-etal-2021-reprogen}, and to choose it as a part of the conferences theme track\footnote{\url{https://2022.emnlp.org/calls/main_conference_papers/\#emnlp-2022-theme-track}, \url{https://2023.aclweb.org/calls/main_conference/\#theme-track-reality-check}}. 
%But what about the link between the idea and the code?
\mg{But what about the link between the concept and the implementation?}
Evaluating this connection corresponds to 
%providing an 
answer
%to 
the question: \textit{does the 
%developed code
\mg{code}
exactly 
%realize the idea
\mg{implement the concept}?}
% In software engineering, this concept is associated with the \textbf{correctness} 
% %of the codebase and represents an important field of study . 
% and represents an important attribute of the codebases.
% Disciplines such as software quality assurance or SQA \citep{Buckley-1984-sqa,tripathy2011software} define the quality attributes of production-ready codebases \citep{ISO9126,ISO/IEC2010}, also by providing best practices to enforce them. 
% %These practices include the use of tests to isolate and verify every piece of code \citep{10.1145/987305.987309,10.1145/800027.808473} and which have to be executed at every change as suggested in the continuous integration (CI) practice \citep{Duvall-2007-ci}. 
% Along this line of study, correctness was formally defined as the \enquote{\textit{extent to which a program satisfies its specifications and fulfills the user's mission objectives}} \citep{McCall1977FactorsIS}. 
% %This means that a codebase has to be \textit{traceable} (i.e., there has to be a thread from the idea to the implementation of specific development and environment), \textit{consistent} (i.e., with uniform design and implementation techniques and notation), and \textit{complete} (i.e., required functions have to be fully implemented).
% Although this definition has targeted mostly production code, all the covered aspects %have an impact also in 
% should be also valid for codes released by the research community. 
% However, differently from reproducibility, correctness is not a currently evaluated aspect of the reviewing process of a paper.
This equals assessing the \enquote{\textit{extent to which a program satisfies its specifications}}  \citep{McCall1977FactorsIS}, which is a fundamental attribute of a codebase, known as \textit{functionality} \citep{ISO9126,ISO/IEC2010} or functional \textbf{correctness} in the field of software quality assurance \citep{Buckley-1984-sqa,tripathy2011software}.
If this characteristic is not enforced, there is no guarantee that the code actually does what it is expected, i.e. that the results of a paper actually 
%correspond to the described idea 
%\mn{[O, sempre in riferim. al passaggio del nostro chatGPT, qualcosa tipo: ``that the results of a paper actually validate the original idea or hypothesis being tested.'']}. 
%\sara{validate the original idea.}
\mg{validate the concept.}
Therefore, %\mn{[qui ci sta bene un ``we posit that...'']} 
\sara{we posit that}
\mg{enforcing code correctness}
%,
%although the definition of correctness has targeted mostly production code, it 
represents a crucial aspect for research %codebases 
\sara{code}\mg{.}
%as well.
However, differently from reproducibility, \mg{code} correctness \mg{has been so far neglected in the peer-review process.}
%is not a currently evaluated aspect of the reviewing process of a paper.


To demonstrate this phenomenon, we scraped the latest review forms\footnote{Checked on January 15th 2023.} of the most popular conferences/journals in the NLP field, namely: 
%AAAI, 
*ACL (AACL-IJCNLP, ACL, EACL, EMNLP, NAACL) through Rolling Review (ARR) and SoftConf,
%, *ACL through direct submission via Softconf, 
ICASSP, ICML, ICLR, Interspeech, NeurIPS, and TACL.
% Computer speech and language?
Table \ref{tab:nlp} shows \mg{that an explicit score is assigned to reproducibility in most of the conferences/journals (5 out of 8). In addition, in NeurIPS, although there is no explicit score, reproducibility is mentioned among the factors that should contribute to the overall score.}
%how reproducibility is actually evaluated by conferences/journals, being present in 7 out of 8 cases and confirming the recent trend of AI communities paying more attention to this aspect.
Conversely, correctness is never \mg{explicitly evaluated,}
%discussed 
%in the questions asked to reviewers 
except for the \sara{TACL journal and the} ICASSP \sara{and ICLR} conference\sara{s}. However, even if potentially mentioned in the review form under \enquote{Technical Correctness}, no description is provided to clarify what the scope of that 
%question \mn{[Se dite ``question'' forse meglio metterla esplicitamente; se no trovare altra parola per question.]} 
\sara{entry}
is \sara{by the ICASSP conference}. 
\sara{Similarly, in the ICLR reviewer form, the correctness aspect is evaluated but refers to the idea and not to the codebase (thus, resembling the soundness concept). In the TACL review form, explicit questions are asked for reproducibility (\enquote{\textit{Will members of the ACL community be able to reproduce or verify the results in this paper?}}) and correctness/soundness (\enquote{\textit{First, is the technical approach sound and well-chosen? Second, can one trust the claims of the paper -- are they supported by proper experiments and are the results of the experiments correctly interpreted?}}) but again 
%the correctness concept is not clearly stated and is also evaluated based on the results and not on the actual piece of code used to obtain them.
the codebase is neglected and the correctness of the entire paper is evaluated solely based on the results.
}

Moreover, it is notable to say that the Interspeech conference gives a wrong definition of \enquote{Technical Correctness} in its review form as it explicitly mentions reproducibility and not correctness by asking: \enquote{\textit{taking into account datasets, baselines, experimental design, are enough details provided to be able to reproduce the experiments?}}. 
%Similarly, in the ICLR review form, the correctness aspect is evaluated but refers to the idea and not to the codebase (thus, resembling the soundness concept). 
In the ARR process, a score is given to \enquote{Software} but it comprises aspects such as usefulness and documentation of the code, not its correctness.
%Lastly, in the TACL review form, explicit questions are asked for reproducibility (\enquote{\textit{Will members of the ACL community be able to reproduce or verify the results in this paper?}}) and correctness/soundness (\enquote{\textit{First, is the technical approach sound and well-chosen? Second, can one trust the claims of the paper -- are they supported by proper experiments and are the results of the experiments correctly interpreted?}}) but again 
%%the correctness of 
%the codebase is neglected and the correctness of the entire paper is  evaluated solely based on the results.
%%and not on the actual piece of code used to obtain them.

This further highlights how the concept of correctness is often neglected or, even worse, misused in our community. In this paper, we
%claim
posit
that the results of the experiments cannot be used as a measure of the correctness of the code
%, which has to be instead evaluated through precise and punctual questions, 
and that \sara{solely} looking at the results can 
%lead 
completely 
%astray from the actual correctness of the code 
\sara{mislead on the actual correctness of the code,}
as we will \sara{discuss through the case study in the next section.}
%prove in Section \ref{sec:case-study} \mn{[Introdurre parola ``case study''...tipo, ma detto meglio: ``..as we will discuss through the use case presented in the next section.'']}.

%claiming the need for precise and punctual questions clearly separating the concepts of reproducibility and correctness in the review forms.

\begin{table}[!tb]
    \centering
    \small
    \setlength{\tabcolsep}{8pt}
    \begin{tabular}{l||c|c}
    \specialrule{.1em}{.05em}{.05em} 
        %\multirow{2}{*}{\textbf{Venue}} & \multicolumn{2}{c}{\textbf{Review Form Content}} \\
        %\cline{2-3}
        \textbf{Venue} & \textbf{Reproducibility} & \textbf{Correctness} \\
        \specialrule{.1em}{.05em}{.05em} 
        %\multirow{2}{*}{AAAI} & Reproducibility & \smallcorrect{} Yes/No questions for each record of the Reproducibility Checklist\tnote{*} \\
        %    & Correctness & Evaluated by... \\
        % \hline
        *ACL (ARR) & \smallcorrect{} & \smallwrong{} \\
        *ACL (SoftConf) &  \smallcorrect{} & \smallwrong{} \\
        ICASSP & \smallwrong{} & \smallcorrect{} \\
        ICML & \smallwrong{} & \smallwrong{} \\
        ICLR & \smallcorrect{} & \smallcorrect{} \\
        Interspeech & \smallcorrect{} & \smallwrong{} \\
        NeurIPS & \smallwrong{} & \smallwrong{} \\
        TACL & \smallcorrect{} & \smallcorrect{} \\
    \specialrule{.1em}{.05em}{.05em} 
    \end{tabular}
    \caption{\mg{Which major conferences/journals in NLP explicitly require a score for reproducibility and correctness in the review forms?}}
    %\caption{Reproducibility and correctness aspects evaluated in the review forms of major conferences/journals in the NLP field.}
    %\mn{[PAURA! Sicuri, vero, che a NeurIPS e ICML non c'e' alcun check nemmeno sulla riproducibilita'???]}
    \label{tab:nlp}
    %\enquote{\smallcorrect{}} means the aspect is mentioned in the form, \enquote{\smallwrong{}} otherwise.
\end{table}




