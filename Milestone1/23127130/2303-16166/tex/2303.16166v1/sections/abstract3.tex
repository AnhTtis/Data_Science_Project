%Despite its paramount importance in the dissemination of research outcomes, code correctness (a component of technical soundness) is often presumed to be valid based on the perceived quality of the experimental results. This comes with the risk of erroneous outcomes that can propagate and reinforce misleading findings. To address this issue, we posit that the current focus on result reproducibility should go hand in hand with the emphasis on the correctness of research software. We bolster our call to the NLP community by presenting a case study, in which we identify (and correct) three bugs in widely used open-source implementations of the state-of-the-art Conformer architecture. Through comparative experiments on automatic speech recognition and translation in various language settings, we demonstrate that the existence of bugs yields results that, though easily reproducible, are nonetheless erroneous. In light of these findings, recognizing the need to avoid overburdening the peer-review process with costly code quality verification procedures, this paper is a call to action towards the adoption of best practices aimed at fostering correctness during code development, prior to its release.

% The peer-review process currently evaluates the technical correctness of a paper solely on the basis of the perceived quality of research results. The code used to obtain these results, instead, is presumed to be correct at the risk of erroneous outcomes and inaccurate, potentially misleading findings. To raise awareness of this problem, we argue that the current emphasis on result reproducibility should go hand in hand with the emphasis on the quality and correctness of research software. We bolster our call to the scientific community by presenting a case study that identifies (and corrects) three bugs in open-source implementations of the state-of-the-art Conformer architecture for two tasks (automatic speech recognition and translation).  The results of our experiments on 8 language pairs demonstrate that the presence of bugs yields reproducible and comparable results, which are nonetheless erroneous and potentially misleading. In response to this, we advise the adoption of best practices aimed at ensuring code correctness and improving software quality within the NLP community.

% CORRECTNESS IS EVALUATED ON RESULTS
% PRESENCE OF BUGS LEAD TO WRONG RESULTS THAT IS NOT EVIDENT 
%\chatgpt{Ensuring code correctness in research software is of paramount importance to avoid the risk of erroneous outcomes and potentially misleading findings. This paper emphasizes that the current focus on result reproducibility should be complemented with the emphasis on the quality and correctness of research software, and presents a case study identifying and correcting three bugs in open-source implementations of the state-of-the-art Conformer architecture. Comparative experiments on automatic speech recognition and translation in various language settings demonstrate that the existence of bugs yields easily reproducible but erroneous results, highlighting the need for best practices aimed at fostering correctness during code development prior to its release in the NLP community.}

%\sara{The code correctness of a paper is often presumed to be valid based on the perceived quality of the experimental results. This comes with the risk of erroneous outcomes that can propagate and reinforce misleading findings. To raise awareness of this problem, we posit that the current focus on result reproducibility should go hand in hand with the emphasis on the quality and correctness of research software. We bolster our call to the NLP community by presenting a case study in which we identify (and correct) three bugs in widely used open-source implementations of the state-of-the-art Conformer architecture.  Comparative experiments on automatic speech recognition and translation in various language settings demonstrate that the presence of bugs yields good and reproducible results, which are however erroneous and potentially misleading. In response to this, we advise the adoption of best practices aimed at fostering code correctness during code development and improving software quality within the NLP community.}




%\mn{Despite its paramount importance in the dissemination of research outcomes, code correctness 
%(a crucial aspect of technical soundness) 
%is often presumed to be valid based on the perceived quality of the experimental results. This comes with the risk of erroneous outcomes that can propagate and reinforce misleading findings. To address this issue, we posit that the current focus on result reproducibility should go hand in hand with the emphasis on the correctness of research software. We bolster our call to the NLP community by presenting a case study, in which we identify (and correct) three bugs in widely used open-source implementations of the state-of-the-art Conformer architecture. Through comparative experiments on automatic speech recognition and translation in various language settings, we demonstrate that the existence of bugs yields results that, though easily reproducible, are nonetheless erroneous. In response to this, recognizing the need to not add undue burden to the peer-review process with costly code quality verification procedures, this paper is a plea towards the adoption of best practices aimed at fostering correctness during code development, prior to its release.}


Despite its pivotal role in research experiments, code correctness is often presumed only on the basis of the perceived quality of the results. This comes with the risk of erroneous outcomes and potentially misleading findings. To address this issue, we posit that the current focus on result reproducibility should go hand in hand with the emphasis on coding best practices. We bolster our call to the NLP community by presenting a case study, in which we identify (and correct) three bugs in widely used open-source implementations of the state-of-the-art Conformer architecture. Through comparative experiments on automatic speech recognition and translation in various language settings, we demonstrate that the existence of bugs does not prevent the achievement of good and reproducible results and can lead to incorrect conclusions that potentially misguide future research. In response to this, this study is a call to action toward the adoption of coding best practices aimed at fostering correctness and improving the quality of the developed software.
