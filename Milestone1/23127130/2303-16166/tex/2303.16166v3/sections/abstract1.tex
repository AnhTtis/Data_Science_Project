% Supported by this empirical evidence, and 
% aware of the need not to further burden the peer-review process with code quality verification procedures, we advise authors to adopt best practices aimed at ensuring code correctness and promoting software quality within the NLP community.
%aware of the inherent difficulty to include code quality verification in the peer-review process, we advocate for the implementation of best practices aimed at ensuring code correctness and promoting software quality within the NLP community.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Despite the importance of code in scientific work, code correctness is not typically assessed in the peer-review process, and is often assumed based on the quality of results leading to potential errors and incorrect findings, even if the results are easily reproducible. In this paper, we prove the need for independent code assessment by analyzing the presence of bugs in open-source implementations of the Conformer architecture as a case study. We show that, even in the presence of bugs, it is possible to obtain reproducible and competitive results which are however actually wrong. To mitigate this problem, we propose a checklist of best practices for enforcing code correctness and enhancing software quality in the research community.

The peer-review process currently evaluates the technical correctness of a paper solely on the basis of the perceived quality of research results. The code used to obtain these results, instead, is presumed to be correct at the risk of  erroneous outcomes and inaccurate, potentially misleading findings. To raise awareness of this problem, we argue that the current emphasis on result reproducibility should go hand in hand with the emphasis on the quality and correctness of research software. We bolster our call to the scientific community by presenting a case study that identifies (and corrects) three bugs in open-source implementations of the state-of-the-art Conformer architecture for two tasks (automatic speech recognition and translation).  The results of our experiments on 8 language pairs demonstrate that the presence of bugs yields reproducible and comparable results, which are nonetheless erroneous and potentially misleading. In response to this, we advise the adoption of best practices aimed at ensuring code correctness and improving software quality within the NLP community.