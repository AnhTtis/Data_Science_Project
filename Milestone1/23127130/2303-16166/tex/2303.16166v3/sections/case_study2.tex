\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.95\textwidth]{img/conformer_con.drawio.png}
    \caption{Convolution module in the Conformer encoder layer. All convolutional blocks are 1D convolutions.}
    \label{fig:conf_conv}
\end{figure*}

As a proof of concept, we study the open-source implementation of the Conformer \citep{gulati20_interspeech} architecture, which is the state-of-the-art solution for speech processing tasks such as automatic speech recognition (ASR) and speech-to-text translation (ST).
%we choose to study the current state-of-the-art architecture Conformer \citep{gulati20_interspeech} for Speech Recognition and Translation.
After a brief description of 
%the concepts of Speech Recognition and Translation
the ASR and ST tasks and the Conformer architecture
(Section \ref{subsec:speech_background}), we analyze
%all the most popular open source codebases available for Conformer 
several open-source codebases that are widely used in research %and 
\sara{which}
contain a Conformer implementation, showing
%and show
that all the analyzed repositories 
%contain 
\sara{have}
at least one bug (Section \ref{subsec:analysis}). 
Through extensive experiments on the two tasks and on all the language pairs of the MuST-C \sara{v1.0} corpus \cite{CATTONI2021101155}, we demonstrate that the presence of bugs can be hidden by good results (Section \ref{subsec:impact_bug})
\sara{and can lead to wrong findings (Section \ref{subsec:impact_code}).}
%, while it can lead to wrong findings when building new solutions on the same codebases (Sections \ref{subsec:impact_code}).
%, and we demonstrate through extensive experiments that these bugs can lead to wrong findings, which can be hindered by good results (Sections \ref{subsec:impact_code}, and \ref{subsec:impact_bug}).


\subsection{Background}
\label{subsec:speech_background}
%Automatic speech recognition (ASR)
ASR is the task in which an audio containing speech content is transcribed in its original language while in 
%the speech translation (ST) task
ST
this content is translated into a different language.
In the last decade, traditional approaches for ASR relying on Gaussian Mixture Models \citep{gmm} have been replaced by deep neural models \citep{hinton-2012-asr} and then by an end-to-end or direct model \citep{pmlr-v32-graves14,Chorowski-2014-asr} capable of performing the ASR task within a single model.
A similar progression has taken place in the field of ST, where cascade architectures consisting of an ASR model followed by a machine translation (MT) model \citep{cascade,cascade2} have been recently replaced by direct models \citep{berard_2016,weiss2017sequence}. Such direct models do not leverage intermediate representations to perform the task like in the cascade solutions and all their parameters are jointly optimized for ST. 
Current direct architectures employed to perform both ASR and ST tasks are based on Transformer \citep{NIPS2017_3f5ee243} and have been adapted to work with audio inputs \citep{8462506,gangi19_interspeech} by introducing two convolutional layers that shrink the length of the input sequence by a factor of 4. 
Among these, \sara{the introduction of the Conformer \citep{gulati20_interspeech} with a modified structure in the encoder} 
%Conformer \citep{gulati20_interspeech}, which modifies the structure of the encoder layers with respect to the Transformer, 
has brought significant improvements both in ASR and in ST \citep{inaguma2021non}.
% represents the current state of the art
%, with significant improvements compared to the vanilla Transformer architectures \citep{inaguma2021non}.
%For this reason, we adopt the Conformer-based architecture for both tasks for the empirical part of this paper.
%Therefore, in the empirical section of this paper, we adopt the Conformer-based architecture for both ASR and ST tasks.

The changes introduced in the Conformer encoder layer can be summarized as follows:
\textit{i)} relative sinusoidal positional encodings \citep{dai-etal-2019-transformer} are introduced in the self-attention for improved generalization with respect to varying input lengths;
\textit{ii)} the FFN sublayer is replaced by two FFNs that wrap the self-attention, inspired by the Macaron-Net~\citep{lu-et-al-2016-macaron-net};
\textit{iii)}~a convolution module (depicted in Figure \ref{fig:conf_conv})
%\sara{, which is wrapped with a residual connection,} 
is added immediately after the self-attention
%, 
before the
%last 
\sara{second}
FFN
%module.
\sara{layer.}
\sara{The convolution module, which is wrapped in a residual connection, applies layer normalization and then a point-wise convolution to each feature vector, doubling its dimension that is restored to its original size by a Gated Linear Unit (GLU) activation function~\citep{Dauphin-2017-glu}. After, a depth-wise convolution with 31 kernel size is applied before a batch normalization \citep{ioffe-2015-batchnorm}, followed by the Swish activation function \citep{swish-2017}, and another point-wise convolution.
Lastly, a dropout module \citep{Srivastava-2014-dropout} randomly masks (i.e. zeroes out) a percentage of the feature values to prevent the network from overfitting.}

%The convolutional module is wrapped in a residual connection.
%%, and, after
%After a layer normalization, a pointwise convolution transforms each feature vector representing a time step in the same way 
%%with a pointwise convolution that
%and doubles the size of the features. The feature dimension is brought to the original size by the Gated Linear Unit (GLU) activation function~\citep{Dauphin-2017-glu}.
%Then, a depthwise convolution with 31 kernel size is applied before a batch normalization \citep{ioffe-2015-batchnorm}, the Swish activation function \citep{swish-2017}, and another pointwise convolution similar to the first one.
%At last, a dropout module \citep{Srivastava-2014-dropout} randomly masks (i.e. zeroes out) a percentage of the values to prevent the network from overfitting.

\subsection{Analysis of the Codebases}
\label{subsec:analysis}

We evaluate the behavior of the open-source implementation of the Conformer 
%with respect to 
\sara{focusing on}
a parameter that should not 
%change 
\sara{affect}
the results: the inference batch size (IBS). When samples of different lengths are collected in the same batch, a 
%common 
\sara{frequent} situation in speech tasks where 
% the length of the input sentence greatly 
\sara{the input length largely}
varies, the input sequences have to be brought to the same dimension by filling them with padding. 
\sara{This is realized to optimize the GPU computation, allowing several sentences to be processed in parallel and increasing efficiency without, in theory, changing the results.}
%By doing so, the computation on GPU is more efficient, because several sentences are processed in parallel and, in theory, results should not change as the padding should be a filler that is ignored. 
For this reason, research papers often include details only about the training batch size, which is an important hyperparameter for the stability of the training, while the IBS is not reported, as it should not affect the results.

Our analysis focuses on six open-source implementations 
%belonging to 
\sara{of}
widely adopted codebases, namely:
%
%To conduct our study about the presence of incorrectness in the codebases focusing on the state-of-the-art Conformer architecture, we collected the most used repository for ASR and ST, namely: 
Fairseq-ST \citep{wang2020fairseqs2t}, ESPnet-ST \citep{inaguma-etal-2020-espnet}, NeMo \citep{kuchaiev2019nemo}, SpeechBrain \citep{speechbrain}, an open source
%(Apache-2.0) 
implementation on github\footnote{\url{https://github.com/sooftware/conformer}} named \enquote{Conformer}, and the official Pytorch implementation from TorchAudio \citep{yang2021torchaudio}.
%\mg{We discovered that all these implementations returned different results with different IBS, showing that the presence of padding was (wrongly) altering the results.}
\sara{We will show that all these implementations returned different results with different IBSs, showing that the presence of padding (wrongly) alters the results.}\footnote{We would like to emphasize that our intention is not to blame any specific codebase or its developers. Conversely, we are extremely thankful for the important and worthy contribution of these open-source libraries to our community. Our analysis is not intended to point out the issues of the single libraries but is intended to further improve the correctness of codes and, consequently, of the experimental results which we believe is of utmost importance.}
We inspected \sara{the piece of code relative to the Conformer architecture and isolate three types of bug:}
%the code relative to the Conformer architecture and we isolate three types of bugs:

\begin{figure}[!tb]
\centering
\renewcommand*\thesubfigure{\arabic{subfigure}} 
     \begin{subfigure}[b]{0.46\textwidth}
        \centering
         \includegraphics[width=0.45\textwidth]{img/start.png}
         \caption{Before shifting, the Relative PE matrix ($P_{00},...,P_{22}$) is padded (zero values).}
     \end{subfigure}
     %\par\medskip
     \begin{subfigure}[b]{0.46\textwidth}
        \centering
         \includegraphics[width=0.45\textwidth]{img/wrong.png}
         \caption{When relative shift is applied to the Relative PE matrix without considering padding, %\smallbug\textsubscript{3}, 
         some values ($P_{11}, P_{12}, P_{20}, P_{21}$) are moved to the padding part (in \textcolor{red}{\textbf{red}}), hence are not considered in the following computation, while some padding is instead incorrectly considered in the following computation (in \textcolor{teal}{\textbf{green}}). Please notice that the first row is always discarded.}
     \end{subfigure}
     \begin{subfigure}[b]{0.46\textwidth}
        \centering
         \includegraphics[width=0.45\textwidth]{img/correct.png}
         \caption{When relative shift is applied to the Relative PE matrix considering padding, all the values $P_{00},...,P_{22}$ are considered in the computation and are never moved to the padded part. Please notice that the first row is always discarded.}
     \end{subfigure}
    \caption{Example of relative shift operation starting from a Relative PE matrix containing padding (1), both considering a codebase with \bugthree{} (2) and without (3) bug.}
    \label{fig:relativePEs}
\end{figure}

\begin{enumerate}
    \item[\bugone] \textbf{Convolution Module Bug:}
    the depth-wise and point-wise convolutions of the Conformer convolution module do not consider the presence of padding and \sara{produces a non-padded output with non-zero values adjacent to the input sample.}
    %cause the padding area to contain non-zero values (mutated from the learned \textit{bias} element of the convolution module and from the adjacent input). 
    These values alter the behavior of the \sara{subsequent} batch normalization and of the other convolutions, leading to 
    \sara{incorrect computations.}
    %alteration also of the real values that do not fall in the padding area.
    %in the Convolution Module of the Conformer Encoder, a series of gated Convolutional Layers \citep{pmlr-v70-dauphin17a} is applied to the input together with a Batch Normalization Layer. Since the output of the Convolutional Layers does not preserve padding, the final output obtained after the Convolution Module is not padded anymore. Also, 
    %%the 
    %Batch Normalization 
    %%Layer 
    %\citep{pmlr-v37-ioffe15} is applied to an input that is not padded. However, since input data is divided into batches during both training and inference of the model, and padding is consequently applied to fit more samples in a single batch, preserving the padding information is important to avoid incorrect computation.
    \item[\bugtwo] \textbf{Initial SubSampling Bug:} 
    the two initial convolutions that subsample the input audio sequence by a factor of 4 do not consider padding. For this reason, the second convolution is fed with 
    \sara{wrong non-zero values adjacent to the input sequence, leading to an incorrect computation.}
    %non-zero values in the padding area close to the end of the valid sequences. These (dirty) values contribute to the computation of the last valid elements of the resulting sequence, altering the correct result.
    %speech inputs are $\sim$10 longer than textual inputs, therefore an initial input dimension reduction is necessary to train an ASR/ST model due to the squared memory complexity on the input length of the attention operation present in all the current architectures \citep{gangi19_interspeech,wang2020fairseqs2t,inaguma-etal-2020-espnet}. For this reason, a block of convolutions with a striding factor $\geq2$ is applied to the input before feeding it to the Conformer Encoder \citep{vyas21_interspeech}, realizing a sub-sampling of the sequence. However, the convolution operation does not preserve padding, meaning that the second convolution operation is applied to a non-padded input and so on. 
    %This represents a problem since batches are used for both training and inference, and preserving padding between operations is important.
    %This constitutes an issue, as batches are utilized for both training and inference, and maintaining padding between operations is crucial.
    \item[\bugthree] \textbf{Positional Encodings Bug:} 
    the relative sinusoidal positional encodings (PEs)\sara{, which have to be added to the attention matrix, are}
    %are added to the attention matrix and 
    %are 
    computed by shifting a sinusoidal matrix. This shifting operation first prepends a zero column to the sinusoidal matrix and then reshapes the matrix so that the last element of the first row becomes the first element of the second, the last two elements of the second row become the first of the third row, and so on. By doing this, this operation assumes that all elements are valid. However, when a sentence is padded, only a part of the attention matrix is valid (%e.g., see the green
    \sara{in} green in Figure \ref{fig:relativePEs}.1), so what is moved to the beginning of the next row is not the correct value \sara{(Figure \ref{fig:relativePEs}.2)}. In Figure \ref{fig:relativePEs}, for the sake of clarity of the example, we pretend that existing implementations set to 0 the PE in the padding area. While this is not what happens in practice (as the padding area contains other sinusoidal PE), it shows that the correct values are discarded and the final matrix does not contain the correct values\sara{, which are instead shown in Figure \ref{fig:relativePEs}.3}.
    %in the Conformer Encoder attention, relative positional encodings (PEs) have been introduced in place of the absolute ones thanks to their improved performance in encoding information \citep{dai-etal-2019-transformer}. Relative PEs are dynamically calculated during training and, in their computation, the sinusoid encoding matrix \citep{NIPS2017_3f5ee243} is relatively shifted regardless of the padding (see Figure \ref{fig:relativePEs}), i.e. some "padding" is shifted together with the relevant information, thus causing an error in the final PEs. 
\end{enumerate}

\begin{table}[!t]
\small
    \centering
    \setlength{\tabcolsep}{6pt}
    \begin{tabular}{l|c|c|c}
    \specialrule{.1em}{.05em}{.05em} 
         \textbf{Repository} & \textbf{Conv. Mod.} & \textbf{SubSampl.} & \textbf{Pos. Enc.} \\
         \specialrule{.1em}{.05em}{.05em} 
         Fairseq-ST & \smallbug & \smallbug & \smallbug \\
         ESPnet-ST & \smallbug & \smallbug & \smallbug \\
         NeMo & & \smallbug & \smallbug \\
         SpeechBrain & \smallbug & \smallbug & \smallbug \\
         Conformer & \smallbug & \smallbug & \smallbug \\
         Pytorch & \smallbug & NA & NA \\
    \specialrule{.1em}{.05em}{.05em} 
    \end{tabular}
    \caption{Presence of bug(s) \smallbug{} in the codebase for each one of the analyzed repositories. NA stands for \enquote{Not Applicable}.}
    \label{tab:bug}
\end{table}

In Table \ref{tab:bug}, we report the presence (or absence) of one or more of these bugs for each repository in its current version.\footnote{Checked on January 8th 2023.}
%
%
%All the public implementations we analyzed contain at least one bug. 
Noticeably, all the implementations but one (NeMo) are affected by the Convolution Module Bug in the Conformer encoder. Moreover, all the implementations are affected by bugs in the initial SubSampling module and in the relative PEs computation, except for the implementation by Pytorch. The latter does not make use of relative PEs in the attention and replaces the initial sub-sampling convolutional layers with linear layers that map the input sequence in matrices of fixed dimensions. Therefore, we can conclude that all the public implementations we analyzed contain at least one bug.

\paragraph{The Case of TF32}
Another issue that potentially causes unexpected differences when padding is introduced 
\sara{do not rely}
%is not related 
to the code but
%We also identify a problem affecting the computation that is not directly related to the implementation itself but is relative 
to the use of NVIDIA Ampere GPUs (A40/A100) during training/inference.
By default on these GPUs, PyTorch backend computes convolutions and matrix multiplications with TensorFloat-32\footnote{\url{https://pytorch.org/docs/stable/notes/cuda.html\#tensorfloat-32-tf32-on-ampere-devices}.} (TF32) cores that speed up the computation but reduce precision. Numeric errors can cause small but random fluctuations in presence of padding. We explore also the impact of this aspect by
%For these GPUs, the computation is executed in TensorFlow-32 (TF32) precision by default, speeding up the calculation but reducing precision.\footnote{\url{https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices}.}
%\footnote{\url{https://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_tensor_float_32_execution}} 
%Hence, computations executed with and without single precision enabled can produce impacting differences in the results. We further explore this aspect by 
enabling and disabling TF32 during both training and inference.



\subsection{Experimental Settings}
%\subsection{Data and Evaluation}

We train and evaluate ASR and ST models on MuST-C v1.0 \cite{CATTONI2021101155}, which contains parallel speech-to-text data with English (en) as source language and 8 target text languages, namely Dutch (nl), French (fr), German (de), Italian (it), Portuguese (pt), Romanian (ro), Russian (ru), and Spanish (es). The ASR model is trained on the transcriptions of the MuST-C en-es train set, containing $\sim$260K sentences. For ST, 8 different models are trained, one for each language direction en$\rightarrow$\{de, es, fr, it, nl, pt, ro, ru\}. 
All the experimental settings, including model architecture, and training hyperparameters, are described in Appendix \ref{sec:exp_sett} and the codebase is released open source at \url{[anonymous\_url\_to\_be\_replaced\_upon\_acceptance]}.
%All the evaluations 
\sara{Evaluations}
are performed on the MuST-C tst-COMMON\sara{, whose outputs are generated using a single NVIDIA A40 with 40GB of RAM,} with word error rate (WER) metric for ASR and with sacreBLEU \citep{post-2018-call}\footnote{BLEU+case.mixed+smooth.exp+tok.13a+version.1.5.1} for ST.
%by using a single NVIDIA A40 with 40GB of RAM.



\subsection{Impact of Single Bugs}
\label{subsec:impact_bug}


%First,
\sara{To show the entity of the problems caused by the introduction of bugs in the code,}
%of all, 
we analyze the impact of the three bugs on the performance of the models and we compare 
%them with results from
\sara{the results with those obtained by}
previous works
%with 
\sara{on}
different architectures. To this aim, we evaluate different IBSs, as increasing the batch size brings more padding, amplifying the effects of 
%the 
bugs.




%To investigate the contribution of each bug to the final results, we perform an ablation study by first isolating the impact of TF32 and then by analyzing the performance drop after the introduction of each of the bugs \smallbug\textsubscript{1,2,3} described in Section \ref{subsec:analysis}. 



\begin{table}[!tb]
\small
\setlength{\tabcolsep}{12pt}
    \centering
    \footnotesize
    \begin{tabular}{l|ccc}
    \specialrule{.1em}{.05em}{.05em} 
        \multirow{2}{*}{\textbf{Code}} & \multicolumn{3}{c}{\textbf{IBS}} \\
        \cline{2-4}
         & 1 & 10 & 100 \\
        \specialrule{.1em}{.05em}{.05em} 
        \smallcorrect & 10.52 & 10.52 & 10.52 \\
        \hline
        + TF32 & 10.73 & 10.73 & 10.73 \\
        \quad + \bugone & 10.72 & 11.25* & 19.50* \\
        \quad + \bugtwo & 10.73 & 10.74 & 10.74 \\
        \quad + \bugthree & \textbf{10.46} & 10.62 & 10.73 \\
        \quad + \smallbug\textsubscript{1,2,3} & 11.32* & 14.25* & 54.56* \\
    \specialrule{.1em}{.05em}{.05em} 
    \end{tabular}
    \caption{WER ($\downarrow$) scores obtained with the different types of bug \bugall{} and TF32 for the ASR task on MuST-C 
    %en-es 
    tst-COMMON as IBS varies (1, 10, and 100 sentences). * indicates that the difference with \smallcorrect{} is statistically significant (computed with bootstrap resampling with 95\% confidence interval).}
    \label{tab:ablationASR}
\end{table}

\paragraph{ASR}

The impact of TF32 and of the different bugs on the performance of the Conformer for the ASR task is shown in Table \ref{tab:ablationASR}.
\sara{First of all, the TF32 seems not to introduce relevant errors in the output computation as the WER remains constant when IBS varies and the 0.21 WER increase compared to \smallcorrect{} is not statistically significant. However, we manually verified that the outputs obtained by varying IBS are not identical which is anyway an incorrect behavior.}
% First of all, the TF32 seems not to introduce relevant errors in the output computation as the IBS varies, as the WER is constant,  although the outputs are not identical (which is, anyway, not a correct behavior). The WER increase of 0.21 is not statistically significant, thus we cannot assert that it degrades the performance. 
Differently, as soon as any of the bugs (\bugone, \bugtwo, \bugthree) is introduced in the code, the performance is highly influenced by the IBS. This phenomenon is more present in the case of bug \smallbug\textsubscript{1} for which we observe a major performance degradation (+8.78 WER) when we introduce a considerable amount of padding by switching from 1 to 100 sentences per batch.
%However, 
Noticeably, 
the best result is obtained with \bugthree{} and IBS 1, and most of the differences with the bug-free version (\smallcorrect) are not statistically significant. Only the presence of all bugs \bugall{} leads to a consistent performance drop, although the results are \sara{still} very competitive with those obtained with Transformer-based architecture on the same benchmark\sara{:} \citealt{CATTONI2021101155} report 26.61 WER, while \citealt{gaido-etal-2021-ctc} obtain 15.6.
All in all, we can conclude that \textit{\textbf{the presence of each 
% of the bugs described above cannot be noticed from the results
\sara{bug is hidden by the competitive results}}}\sara{, even when bugs are present altogether,}
%and even the contemporaneous presence of all bugs cannot be spotted if compared with other works on the same benchmark with different architectures, 
unless different IBSs are tested.



\begin{table}[!htb]
\small
\setlength{\tabcolsep}{2.5pt}
    \centering
    \footnotesize
    \begin{tabular}{l|ccc|ccc|}
        \cline{2-7}
         & \multicolumn{3}{c|}{\textbf{en-de}} & \multicolumn{3}{c|}{\textbf{en-es}} \\
         \specialrule{.1em}{.05em}{.05em} 
        \multirow{2}{*}{\textbf{Code}} & \multicolumn{3}{c|}{\textbf{IBS}} & \multicolumn{3}{c|}{\textbf{IBS}} \\
        \cline{2-7}
        & 1 & 10 & 100 & 1 & 10 & 100 \\
        \specialrule{.1em}{.05em}{.05em} 
        \smallcorrect & 24.67 & 24.67 & 24.67 & 30.34 & 30.34 & 30.34 \\
        \hline
        + TF32 & \textbf{24.84} & \textbf{24.84} & 24.83 & \textbf{30.63} & 30.62 & \textbf{30.63} \\
        \quad + \bugone & 24.52 & 24.65 & 24.67 & 29.53* & 29.41* & 27.71* \\
        \quad + \bugtwo & 24.56 & 24.57 & 24.58 & 30.53 & 30.53 & 30.53 \\
        \quad + \bugthree & 24.53 & 24.46 & 24.42 & 30.33 & 30.35 & 30.24\\
        \quad + \bugall & 24.68 & 24.58 & 23.23* & 28.57* & 27.81* & 21.15* \\
    \specialrule{.1em}{.05em}{.05em} 
    \end{tabular}
    \caption{BLEU ($\uparrow$) obtained with the different types of bug \bugall{} and TF32 for the ST task on MuST-C tst-COMMON as IBS varies (1, 10, and 100 sentences). * indicates that the difference with \smallcorrect{} is statistically significant (computed with bootstrap resampling with 95\% confidence interval).}
    \label{tab:ablationST}
\end{table}



\paragraph{ST} Table \ref{tab:ablationST} reports the same study on the two 
%of the 
most used language pairs of the MuST-C benchmark (en-de, and en-es).
The behavior is quite different between the two language directions, although the best results are always obtained with TF32 and without bugs. Indeed, in en-es the presence of \bugone{} causes statistically significant drops
%, which 
\sara{that}
increase 
%with the IBS
\sara{as the IBS increases} and are exacerbated if combined with the other two bugs (\bugall).
In en-de, instead, 
%which is by far the most widely used benchmark in ST, 
none of the bugs has a significant impact on the results. Interestingly,
%with 1 as the IBS, 
the result obtained with all bugs (\bugall) \sara{and IBS 1} is slightly (0.01) higher than \sara{those} without bugs.
Furthermore, 
%by 
comparing the scores 
\sara{obtained with all bugs \bugall{} and IBS 1 with those}
of previous works \sara{on ST}
%and architectures with those obtained with \smallbug\textsubscript{1,2,3} and IBS 1 
(Table \ref{tab:comparison_others}), we can notice that
%, as in ASR, 
the presence of bugs is \sara{again} not evident from the results, \sara{as we already seen for the ASR task, even when they cause a significant drop (en-es).}
%not even in en-es where they cause a significant drop.
In light of this, we can strengthen our previous conclusion that \textbf{\textit{good results do not imply the correctness of the code}}, as this statement holds for different tasks and language pairs. 
\sara{We reiterate that the presence of bugs can be spotted only when using higher IBS which, however, should be in theory an irrelevant hyperparameter for the performance of the system.}
%Again, the presence of bugs was evident only by experimenting with different (high) IBSs, which should be an irrelevant hyperparameter.

\mg{DA DIRE DA QUALCHE PARTE CHE TUTTI QUESTI RISULTATI SONO RIPRODUCIBILI!}
%
%For the ST task, we present the impact of TF32 and of the different bugs \smallbug\textsubscript{1,2,3} on two language directions of MuST-C v1.0, en-de, and en-es, having respectively different and similar word ordering with respect to the source language. Results are presented in Table \ref{tab:ablationST}.
%Differently from the ASR case, there is no bug that has a prevalent role over the others, except for bug \smallbug\textsubscript{1} in en-es that shows a BLEU degradation from 0.81 to 2.63 points compared to the correct code (\smallcorrect) as the batch size increases. In all the other cases, the influence of every single bug seems marginal if considered alone but is exacerbated when all the bugs are inserted in the code (\smallbug\textsubscript{1,2,3}). Therefore, we can conclude that the additive effect of bugs on performance degradation does not depend on the particular task but is valid for both ASR and ST.



\begin{table}[t]
%\setlength{\tabcolsep}{3pt}
\centering
\small
\begin{tabular}{l|cc}
\specialrule{.1em}{.05em}{.05em} 
\textbf{Model} & \textbf{en-de} & \textbf{en-es} 
 \\
\specialrule{.1em}{.05em}{.05em} 
 ESPNet \cite{inaguma-etal-2020-espnet} & 22.9 & 28.0 \\
 Fairseq \cite{wang2020fairseqs2t} & 22.7 & 27.2 \\
 Speechformer \cite{papi-etal-2021-speechformer} & 23.3 & 28.5 \\
 MultiLang Adapters \citep{le-etal-2021-lightweight} & 24.7 & 28.7 \\
 AFS \citep{zhang-etal-2020-adaptive} & 22.4 & 26.9 \\
 \hline
 Conformer \bugall & 24.7 & 28.6 \\
\specialrule{.1em}{.05em}{.05em} 
\end{tabular}
\caption{Comparison of the BLEU scores on MuST-C en-de and en-es of the Conformer implementation with all bugs using 1 as IBS with other models presented in the literature.}
\label{tab:comparison_others}
\end{table}






\subsection{Impact of Building on Incorrect Code}
\label{subsec:impact_code}

To showcase how \sara{the use of incorrect code to develop new software can lead to}
%building new works on incorrect code can cause 
misleading findings, we evaluate the introduction of the CTC compression (or shrinking) mechanism \citep{liu2020bridging,gaido-etal-2021-ctc} \sara{to the Conformer encoder}. The CTC \citep{Graves2006ConnectionistTC} enables producing an output sequence of variable length that is \sara{often} shorter than the input one \sara{while preserving its original content.}
%, as in the case in which the input is audio and the target is the sequence of uttered symbols -- characters, sub-words -- corresponding to the transcript. 
For each input time step, the CTC produces a probability distribution over the possible target labels augmented with a dedicated \texttt{<blank>} symbol representing the absence of 
%a target value. 
\sara{any target labels.}
The CTC compression collapses contiguous vectors %(\sara{by} averaging them) 
corresponding to the same prediction \sara{by averaging them} and is \sara{generally} applied 
%in 
\sara{to}
an intermediate encoder layer 
%(in our case, the 8th out of 12). 
\sara{(8\textsuperscript{th} in our case).}
\citet{liu2020bridging} \sara{and} \citet{gaido-etal-2021-ctc} demonstrated that this method brings limited gains in terms of %translation 
\sara{output} quality, while it greatly reduces the training and inference costs. In this section, we analyze the effect of 
%adopting 
\sara{applying}
this technique 
%in 
\sara{to}
the Conformer architecture 
%both with all bugs and without them.
\sara{implemented on both the codebase with all bugs (\bugall) and without them (\smallcorrect).}

\begin{table}[htb]
%\setlength{\tabcolsep}{4pt}
    \centering
    \small
    \begin{tabular}{l|c|ccc}
    \specialrule{.1em}{.05em}{.05em} 
        \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Code}} & \multicolumn{3}{c}{\textbf{IBS}} \\
        \cline{3-5}
         & & 1 & 10 & 100 \\
        \specialrule{.1em}{.05em}{.05em} 
        Conformer & \multirow{2}{*}{\bug\textsubscript{1,2,3}} & 11.32 & 14.25 & 54.56 \\
        \hspace{0.5em}+ CTC Compr. & & 10.39* & \textbf{10.34}* & 10.81* \\
        \hline
        Conformer & \multirow{2}{*}{\correct} & 10.52 & 10.52 & 10.52 \\
        \hspace{0.5em}+ CTC Compr. & & 10.64 & 10.64 & 10.64 \\
    \specialrule{.1em}{.05em}{.05em} 
    \end{tabular}
    \caption{WER on the transcripts of MuST-C en-es tst-COMMON based on the correct/incorrect codebase of the Conformer model (with and without CTC compression) as IBS varies (1, 10, and 100). * indicates that the improvement given by the CTC compression is statistically significant (computed with bootstrap resampling with 95\% confidence interval).}
    \label{tab:ASR}
\end{table}

\begin{table*}[!htb]
\setlength{\tabcolsep}{6pt}
    \centering
    %\footnotesize
    \small
    \begin{tabular}{c|c|c||c|c|c|c|c|c|c|c||c}
    \specialrule{.1em}{.05em}{.05em} 
        \textbf{Code} & \textbf{Model} & \textbf{IBS} & \textbf{en-de} & \textbf{en-es} & \textbf{en-fr} & \textbf{en-it} & \textbf{en-nl} & \textbf{en-pt} & \textbf{en-ro} & \textbf{en-ru} & \textbf{Avg} \\
        \specialrule{.1em}{.05em}{.05em} 
        \multirow{6}{*}{\bug\textsubscript{1,2,3}} & \multirow{3}{*}{Conformer} & 1 & 24.68 & 28.57 & 35.70 & 25.81 & 29.68 & 30.22 & 23.52 & 15.83 & 26.75 \\
        %\cline{3-11}
        & & 10 & 24.58 & 27.81 & 35.65 & 25.70 & 29.35 & 30.02 & 23.43 & 15.36 & 26.49 \\
        %\cline{3-11}
        & & 100 & 23.23 & 21.15 & 31.70 & 23.42 & 24.92 & 27.72 & 22.68 & 11.05 & 23.23 \\
        \cline{2-12}
        & \multirow{3}{*}{\shortstack{Conformer\\+\\CTC Compr.}} & 1 & 24.95 & 30.49* & 36.27* & 25.84 & 29.42 & 30.04 & 23.96* & 17.05* & 27.25 \\
        %\cline{3-11}
        & & 10 & 25.21* & \textbf{30.72}* & 36.18* & 26.01 & 29.64 & 30.14 & 23.95* & 17.06* & 27.36 \\
        %\cline{3-11}
        & & 100 & \textbf{25.26}* & 30.52* & 36.36* & 25.88* & 29.66* & 30.16* & 23.92* & 16.87* & 27.33 \\
        \hline
        \multirow{6}{*}{\correct} & \multirow{3}{*}{Conformer} & 1 & \multirow{3}{*}{24.67} & \multirow{3}{*}{30.34} & \multirow{3}{*}{36.22} & \multirow{3}{*}{25.73} & \multirow{3}{*}{30.04} & \multirow{3}{*}{\textbf{30.55}} & \multirow{3}{*}{23.43} & \multirow{3}{*}{17.29} & \multirow{3}{*}{27.28} \\
        %\cline{3-11}
        & & 10 & & & & & & & & \\
        %\cline{3-11}
        & & 100 & & & & & & & & \\
        \cline{2-12}
        & \multirow{3}{*}{\shortstack{Conformer\\+\\CTC Compr.}} & 1 & \multirow{3}{*}{24.97} & \multirow{3}{*}{30.48} & \multirow{3}{*}{\textbf{36.43}} & \multirow{3}{*}{\textbf{26.25*}} & \multirow{3}{*}{\textbf{30.31}} & \multirow{3}{*}{30.09\textsuperscript{$\dagger$}} & \multirow{3}{*}{\textbf{24.67*}} & \multirow{3}{*}{\textbf{17.35}} & \multirow{3}{*}{\textbf{27.57}} \\
        %\cline{3-11}
        & & 10 & & & & & & & & \\
        %\cline{3-11}
        & & 100 & & & & & & & & \\
    \specialrule{.1em}{.05em}{.05em} 
    \end{tabular}
    \caption{BLEU ($\uparrow$) over all the 8 languages of MuST-C tst-COMMON v1.0 based on the correct/incorrect codebase of the Conformer model (with and without CTC compression) as IBS varies (1, 10, and 100). * and \textsuperscript{$\dagger$} indicates that, respectively, the improvement or the degradation given by the CTC compression is statistically significant (computed with bootstrap resampling with 95\% confidence interval).}
    \label{tab:ST}
\end{table*}


\paragraph{ASR}
Table \ref{tab:ASR} shows the effects \sara{on the ASR performance of the introduction of the CTC compression to both codebases \bugall{} and \smallcorrect.}
%on the ASR performance of introducing the CTC compression both with and without the bugs. 
We can notice that
%without the bugs, 
the CTC compression causes a small \sara{(+0.12 WER)}
%(
and not statistically significant
%) 
performance degradation \sara{when the correct implementation \smallcorrect{} is used (in accordance with the findings of \citealt{gaido-etal-2021-ctc} on the Transformer architecture).}
%, as the WER increases by 0.12 (in accordance with the findings of \citealt{gaido-etal-2021-ctc} on the Transformer architecture). 
%When the bugs are introduced, 
\sara{When bugs are present in the codebase,}
instead, the conclusion is completely overturned: the CTC compression brings statistically significant improvements (nearly -1 WER even with IBS 1). In addition, the best overall result is obtained with \sara{IBS 10 and}
%the 
CTC compression 
%and all the bugs
\sara{implemented in the \bugall{} codebase.}
%(and 10 as IBS). 
%As such, not only we can reiterate 
\sara{Therefore, we can affirm that}
%, but we also showed that 
\sara{\textit{\textbf{good results not only can hide incorrect codebases but also lead to misleading findings}} since the}
%\textit{\textbf{building on incorrect code can produce misleading findings}}, 
%and the 
introduction of noise at inference time does not necessarily degrade the results (as the padding introduced with IBS 10 improves the performance
%with respect to IBS 1).
\sara{compared to that obtained with IBS 1).}





\paragraph{ST}
Table \ref{tab:ST} reports the same analysis on the 8 language pairs of MuST-C v1.0. As in ASR, the presence of 
%the 
bugs (\bugall) unfairly rewards the CTC compression mechanism, which achieves statistically significant gains 
\sara{with IBS 100 among all the languages and also on (at least) 4 out of 8 languages with IBS 1 or 10.}
%on at least 4 out of 8 language pairs at every IBS and on all languages with 100 sentences as IBS. 
With the correct code (\smallcorrect), instead, the improvements are only statistically significant on two language pairs (en-it, and en-ro), while on en-pt there is a statistically significant degradation. 
%Interestingly, en-it is one of the only two languages where the improvement is significant with the correct code, while is among the languages where the improvements are statistically significant only with 100 as IBS with the bugs. 
On average over all \sara{the} language directions, the improvements
%brought 
\sara{achieved}
by the CTC compression \sara{on codebase \bugall{}} range from 0.5 BLEU 
%(IBS=1) 
\sara{(with IBS 1)}
to 4.1 BLEU 
%(IBS=100) 
\sara{(with IBS 100)}
%in the presence of the bugs 
while 
%the gain 
is only \sara{of} 0.29 BLEU with 
%the correct  code. 
\sara{codebase~\smallcorrect.}
Moreover, the best scores for en-de and en-es
%(which curiously are the most widespread benchmarks) 
are \sara{obtained with CTC Compession on the buggy code (\bugall).}
%achieved with the presence of the bugs.
\sara{We can hence confirm that the use of incorrect codebases can lead to misleading findings because, also for the ST task, the CTC compression seems to bring statistically significant gains while, instead, these gains are not real on a correct codebase.
This further demonstrates that \textit{\textbf{it is impossible to assess code correctness only by looking at the results}} since, for instance, the average performance gap between \bugall{} and \smallcorrect{} codebases can be as little as 0.21 BLEU (when IBS is 10) and can further be narrowed, or even overturned, with a simple IBS \enquote{tuning}.}

%We can hence confirm on the ST task the finding of the previous ASR section, i.e. that \textit{\textbf{building on incorrect code can produce misleading findings}}, as the presence of bugs may lead to erroneously concluding that the CTC compression brings statistically significant gains on most of the language pairs without degrading on any of them, while this is not true. In addition, it further demonstrates that is virtually \textit{\textbf{impossible to assess code correctness from the results}}, as, for instance, the difference between correct and buggy code with CTC compression can be as little as 0.21 BLEU (with IFS=10) on average over 8 language pairs, and we cannot exclude that a ``tuning'' of the IBS could further narrow the gap (or even overturn it).
