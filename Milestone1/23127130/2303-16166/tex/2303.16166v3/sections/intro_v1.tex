With recent advances in artificial intelligence, an increasing number of scientific artifacts are being publicly released, from code, to data, to models \citep{piwowar2011shares,10.1145/3442188.3445922}. As a result, the reproducibility of these artifacts is becoming increasingly important for the research community \citep{Gundersen_Kjensmo_2018,wieling-etal-2018-squib,Gundersen_2019,belz-etal-2021-systematic}. This has led many scientific organizations and, associated with them, journals and conferences such as NeurIPS\footnote{\url{https://neurips.cc/Conferences/2021/PaperInformation/PaperChecklist}}, and AAAI\footnote{\url{https://aaai.org/Conferences/AAAI-22/reproducibility-checklist/}} but also the entire ACL comunity\footnote{\url{https://aclrollingreview.org/responsibleNLPresearch/}} to require the completion of checklists attesting the reproducibility of the released material \citep{dodge-etal-2019-show,pineau2021improving,rogers-etal-2021-just-think}. Additionally, members of the scientific community are becoming increasingly aware of this issue and organizing related workshops\footnote{\url{http://4real.di.fc.ul.pt/},\url{https://rrpr2022.sciencesconf.org/},\url{https://rohanalexander.com/reproducibility.html}} and shared tasks \citep{Pineau:2019,belz-etal-2021-reprogen}.
More recently, \citep{EMNLP} questioned that the reproducibility of the published works is not only related to the resources to be of public access, advocating for the addition of small-scale experiments and ad-hoc scripts to generate the results reported in the paper by the authors during the papers submissions.
Also, \citet{ulmer2022experimental} raise the need for experimental standards in the context of natural language processing (NLP) research, providing suggestions and best practices for the release of data, code, and models and for publishing experimental papers.
However, all these works have always focused on reproducibility while never pushing toward the actual correctness of the codebases and, consequently, of the models built from them.

In this paper, we claim that experimental correctness should be considered a fundamental aspect, prior to reproducibility, when sharing scientific artifacts in order to avoid completely wrong findings for the research community. As a proof of concept and through extensive experiments on the automatic speech recognition (ASR) and speech translation (ST) state-of-the-art architecture Conformer \citep{gulati20_interspeech}, we show that even only partially incorrect codes can lead to wrong conclusions, incorrectness that are also shared among the most commonly used codebases by researchers. 
Moreover, in addition to encouraging the research community to produce more correct material, we summarize a list of best practices that should be required to be listed in papers along with experimental settings to check the reliability of the released resources and the soundness of the results.
Specifically, our contributions are:
\begin{itemize}
    \item We analyze commonly used open-source implementations of the Conformer architecture, including Fairseq \citep{ott2019fairseq} and ESPnet \citep{watanabe2018espnet}, and discover that all the repositories are affected by at least one bug in their code (\S \ref{sec:analysis});
    \item We show that the presence of bug(s) can incredibly impact the results, leading to completely inaccurate findings which, however, can be hidden by high-quality scores of the model itself (\S \ref{sec:impact});
    \item We provide a list of guidelines to be included in the experimental settings of the paper to show the reliability of the experiments and the validity of their results (\S \ref{sec:checklist}).
\end{itemize}