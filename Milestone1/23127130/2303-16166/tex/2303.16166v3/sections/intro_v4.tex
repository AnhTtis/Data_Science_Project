Conferences and journals rely on peer reviews to ensure the quality of published material \citep{Ziman1968-ZIMPKA,ravetz1971scientific,meadows-1974-science}. Currently, the peer-review process evaluates the technical correctness of a paper by examining the 
%validity
soundness of the hypotheses, conclusions, and experimental design,
%the hypothesis
% and conclusions, 
% identifying 
% %\mn{checking for}
% flaws
% in the experimental design,
and by comparing results with other
%studies. 
works.
Specifically, the correctness of a new algorithm 
%proposed in a paper is assessed by
is assessed by
%assessing the correctness of a new algorithm proposed in a paper and its code involves 
\enquote{\textit{establishing consistency of results versus existing implementations, standard benchmarks, or sanity checks via statistically significant experimental results}} \citep{Rozier-2014-repro}. 
%SHORTER ALTERNATIVE: However, the actual functioning of the code is never assessed during this process, leading to challenges in reproducing published papers even when their code, data, and models are released \citep{piwowar2011shares,10.1145/3442188.3445922}. 
%However, the actual functioning of the code is never assessed during this process. As a result, the reproducibility of papers, despite the release of their code, data, and models \citep{piwowar2011shares,10.1145/3442188.3445922}, remains a challenge \citep{arvan-etal-2022-reproducibility-code,Chen2019} 

%Accordingly, the actual functioning of the code is never assessed during this process, with several consequences.
The actual functioning of the underlying code, 
%though,
however,
is never assessed during this process, 
%with
and this has
several undesired consequences.
%\sara{During this process, the actual functioning of the code is never evaluated, leading to several consequences.}
One 
%problem is the lack of reproducibility of the published works, 
%\mn{is the so-called ``reproducibility crisis''}
is the lack of reproducibility of the published works, 
%\sara{that,}
%, which has recently become an increasing and widespread concern within the research community, leading to the rise of dedicated initiatives to mitigate the problem \citep{dodge-etal-2019-show,pineau2021improving,rogers-etal-2021-just-think}.
%Indeed, 
%
%
%
%which, despite the increase in the number of scientific artifacts (code, data, and models) that are released open source \citep{piwowar2011shares,10.1145/3442188.3445922}, still remains a challenge.
which remains a well-known issue despite the increasing number of scientific artifacts (code, data, and models) that are released open source \citep{piwowar2011shares,10.1145/3442188.3445922}.
Indeed, a considerable percentage of codebases fail to run without errors and/or miss dependencies \citep{Chen2019,arvan-etal-2022-reproducibility-code}.
%
%
%
%
% However, this is not the only issue, as reproducibility alone is not sufficient to \enquote{\textit{guarantee the quality, correctness, or validity of the published results}}~\citep{Peng-2011-reproducible}, and obtaining good and reproducible results does not imply that the code is \textbf{correct}, i.e. it performs what it is stated in the paper. Even worse, when a work achieves interesting results and findings and is easily reproducible, although the code is not correct, building future research on them is facilitated and could lead to incorrect conclusions \citep{McCullough-2008-replicable}.
% [L'UNICA ALTERNATIVA CHE TROVO A QUESTO HOWEVER E' NEVERTHLESS]
%
%
%\mn{However, this 
%This
But this
is not the only issue. %\mg{, though}. 
Akin to good performance, reproducibility alone is not sufficient to \enquote{\textit{guarantee the quality, correctness, or validity of the published results}}~\citep{Peng-2011-reproducible} or, in other words, to ensure that the code used to obtain the results is \textbf{correct} (i.e. it actually performs what it is supposed to do).
%
%
%
% \mg{The attainment of good results does not imply that the code is \textbf{correct} (i.e. it actually performs what it is supposed to do) either.}
% \mg{Furthermore,
% when a work obtains noteworthy results and is easily reproducible, future research is likely built on it \citep{McCullough-2008-replicable}, and, in case the code is flawed, the subsequent findings could be misleading.}
%
%%%%%%%\mn{Good and easily reproducible results, in fact, do not guarantee that the code used to obtain them is \textbf{correct} (i.e. it actually performs what it is supposed to do).
%
% The risk, when this condition does not hold, is 
% %the dissemination of erroneous, misleading findings
% that subsequent research will likely build on them
% The risk, when subsequent research builds noteworthy results that rest on flawed code \citep{McCullough-2008-replicable}
Therefore, since the replicability of good results 
%\mn{(especially if they are good)} 
%is often the starting point
facilitates the adoption of the related code as the starting point
of subsequent research \citep{McCullough-2008-replicable}, the risk is that 
% flawed code will contribute to propagate misleading findings.
building on flawed code 
%will propagate misleading findings.
can yield unreliable and misleading findings.
%can yield wrong results that propagate misleading findings.}
%
%
%
%\mg{Even worse, when a work achieves interesting results and is easily reproducible, building future research on it is facilitated \citep{McCullough-2008-replicable}. In case the code is not correct, this can lead to misleading findings.}
%
%although the code is not correct, building future research on them is facilitated and could lead to incorrect conclusions \citep{McCullough-2008-replicable}.}
% %Also, since 
%Since obtaining good and reproducible results does not imply that the code is \textbf{correct} 
%%(i.e. it performs what it is stated in the paper), 
%(i.e. it actually performs what it is supposed to do), 
%building future research on 
%easily reproducible results obtained with software affected by errors 
%them can lead to incorrect conclusions and to the propagation of misleading findings \citep{McCullough-2008-replicable}.



%In this paper, as a countermeasure to the problems mentioned above and a method to increase confidence in current findings and future research directions based on them, we posit that code correctness should be assessed on its own and promote the adoption of best practices of software quality assurance (SQA) in the research community. 
%In this paper, as a countermeasure to the aforementioned issues and to bolster confidence in published results and subsequent findings, as well as future research based on them, we posit that code correctness \mg{should not be assumed on the basis of good results}
%should undergo an independent assessment 
%and advocate for the adoption of best practices aimed at enhancing software quality within the research community.
%
%With the goal of enhancing confidence in published findings,
%and future research based on them, 
%in this paper, we
% \sara{In this paper, with the goal of enhancing confidence in published findings, we argue that good results alone are not an adequate measure to ensure the code correctness,}
%
%
%
%
% With the goal of enhancing confidence in published findings, in this paper we argue that good results alone are not an adequate measure to ensure the code correctness,
% %argue against the assumption of code correctness based solely on good results, 
% and propose the adoption of best practices aimed at enhancing software quality within the NLP research community.
% Our contributions can be summarized as follows:
%
%
%
% \mn{In light of the above,  this paper is a call to action, supported by empirical evidence, toward enhancing the reliability of published NLP findings through the adoption of best practices during code development. Specifically our contributions are:}
In light of the above,  this study is a call to action, underpinned by empirical evidence,
to enhance the dependability of published NLP findings through the 
implementation of best practices during code development. In particular, our contributions are:
%
\begin{enumerate}[leftmargin=14pt]
    %\setlength\itemsep{-3pt}
    \item %We show that the code correctness of papers is given for granted in current peer-review processes based on the quality of the results, 
    We demonstrate that the code correctness of a paper in current peer reviews is assumed solely based on the quality of the results,
    whereas we posit that it should undergo independent assessment, similarly to reproducibility (\S\ref{sec:core-idea});
%    \item \mn{[INVERTIREI: THROUGH A CASE STUDY...CONFORMER...DIMOSTRIAMO CHE IL TEMA E' URGENTE ...]} By using the open-source implementations of the widespread Conformer architecture \citep{gulati20_interspeech} as a case study, we prove that:
    \item Through a case study on  open-source implementations of the widespread Conformer architecture \citep{gulati20_interspeech}, we prove that:
    \begin{itemize}[leftmargin=8pt]
    \item[-] All the analyzed implementations contain at least one bug in the codebase (\S\ref{subsec:analysis});
    \item[-] The 
    %presence
    existence
    of bugs does not prevent from achieving reproducible results that outperform those of other 
    %architectures and works, 
    architectures
    across 
    %various tasks and languages
    different tasks and language settings
    %even in presence of bugs, it is possible to obtain reproducible results that are competitive with those of other architectures and works, across various tasks and languages 
    %pairs \mn{[non avendo detto su quali task e' fatta quest'analisi, suona strano leggere ``language pairs''.]}
    (\S\ref{subsec:impact_bug});
    \item[-] 
    The presence of bugs
    %\mn{This}
    can lead to incorrect findings when incorporating a new technique into the architecture (\S\ref{subsec:impact_code}).
    \end{itemize}
    \item We release an open-source version of Conformer 
    %implementation that 
    that
    is not affected by bugs, as well as all the pre-trained models, at \texttt{\url{https://github.com/hlt-mt/fbk-fairseq}} under the Apache 2.0 Licence;
    \item We propose a checklist of best practices to 
    %enforce code correctness
    foster
    %favour 
    code correctness prior to its release
    (\S \ref{sec:checklist}).
\end{enumerate}



