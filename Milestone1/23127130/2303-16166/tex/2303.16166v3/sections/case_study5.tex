As a case study, we examine existing open-source implementations of the Conformer \citep{gulati20_interspeech} architecture, which is the state-of-the-art solution for speech processing tasks \citep{conformer-sota,ma2021end,conformer-sota2,conformer-sota3} such as automatic speech recognition (ASR) and speech-to-text translation (ST). 
%After a brief description of the ASR and ST tasks and the Conformer architecture (\S\ref{subsec:speech_background}), 
After introducing the ASR and ST tasks object of our study, and the basics of the Conformer architecture (\S\ref{subsec:speech_background}),
we analyze several widely-used open-source codebases that contain a Conformer implementation, showing that all the analyzed repositories have at least one bug (\S\ref{subsec:analysis}). 
Through extensive experiments on the two tasks and on all the language pairs of the MuST-C v1.0 corpus \cite{CATTONI2021101155}, we demonstrate that the presence of 
%\mn{such} 
bugs can be hidden by good -- but incorrect -- results (\S\ref{subsec:impact_bug}) that consequently lead to wrong findings (\S\ref{subsec:impact_code}).


\subsection{Background}
\label{subsec:speech_background}
% ASR is the task in which an audio containing speech content is transcribed in its original language, while in ST the source content is translated into a different language.
ASR is the task in which an audio containing speech content is transcribed in its original language. In ST, instead, the source audio is translated into text in a different language.
Nowadays, both tasks are commonly performed with end-to-end (or direct) models \citep{pmlr-v32-graves14,Chorowski-2014-asr,berard_2016,weiss2017sequence}, whose architecture is based on the Transformer~\citep{NIPS2017_3f5ee243}. The Transformer has been adapted to work with audio inputs~\citep{8462506,gangi19_interspeech} by introducing two convolutional layers that shrink the length of the input sequence by a factor of 
%4.
$4$, so as to reduce the otherwise excessive memory requirements.
%make it manageable thanks to affordable memory requirements.}
% \mg{More recently, \citet{gulati20_interspeech} proposed the Conformer} 
% \sara{architecture} \mg{by modifying the structure of the encoder layers, with significant improvements both in ASR and in ST \citep{inaguma2021non}.}
%
%
%
% \mn{More recently, \citet{gulati20_interspeech} proposed the Conformer: a novel architecture
% %in which 
% with a  modified structure of the encoder layers that led to significant improvements  in both ASR and ST \citep{inaguma2021non}.}
More recently, \citet{gulati20_interspeech} proposed the Conformer: a novel architecture with a modified encoder that led to significant improvements in both ASR and ST \citep{inaguma2021non}.



The changes introduced in the Conformer encoder 
%layers
layer structure can be summarized as follows:
\textit{i)} relative sinusoidal positional encodings \citep{dai-etal-2019-transformer} are introduced in the self-attention for improved generalization with respect to varying input lengths;
\textit{ii)} the FFN sublayer is replaced by two FFNs that wrap the self-attention, inspired by the Macaron-Net~\citep{lu-et-al-2016-macaron-net};
\textit{iii)}~a convolution module (depicted in Figure \ref{fig:conf_conv}) is added immediately after the self-attention, before the second FFN layer.
The convolution module, which is wrapped in a residual connection, applies layer normalization and then a point-wise convolution to each feature 
vector, doubling its dimension that is restored to its original size by a Gated Linear Unit (GLU) activation function~\citep{Dauphin-2017-glu}. 
% vector that doubles its dimension \mn{(which is later restored to its original size by a Gated Linear Unit (GLU) activation function~\citep{Dauphin-2017-glu})}. 
% \mn{vector. This doubles its dimension, which is later restored to its original size by a Gated Linear Unit (GLU) activation function~\citep{Dauphin-2017-glu}.}
%After,
Then,
a depth-wise convolution with 31 kernel size is applied before a batch normalization \citep{ioffe-2015-batchnorm}, followed by the Swish activation function \citep{swish-2017}, and another point-wise convolution.
Lastly, a dropout module \citep{Srivastava-2014-dropout} randomly masks (i.e. zeroes out) a percentage of the feature values to prevent the network from overfitting.

\subsection{Analysis of the Codebases}
\label{subsec:analysis}

We analyze the behavior of the open-source implementations of the Conformer 
%when
by systematically varying a parameter that should not affect the results: the inference batch size (IBS).
With 
%a high
high
IBSs, multiple samples are collected in the same batch, allowing for their parallel processing on 
%GPU, thus reducing the overall computational cost.
GPU 
%and an overall reduction of computational costs.
to reduce the overall computational cost.
When samples of different lengths are collected in the same batch
%, 
--
a frequent situation in speech tasks where the input length largely varies
%, 
--
the input sequences have to be brought to the same dimension by filling them with padding. 
% \sara{For this reason, research papers often include details only on the training batch size, which is an important hyperparameter for the stability of the training, while the IBS is not reported.}
%Since with correct implementations the effect of padding is assumed to be independent of the IBS, 
Since with correct implementations the results are independent of the presence of padding (and, therefore,
%from
of
the IBS),
research papers usually
%include details only about
include only
the training batch size (which, in contrast, is an important hyperparameter for the stability of the training). However, as we
%will
demonstrate in this section, 
%the presence of bugs
the bugs present in widely-used Conformer implementations 
%breaks the above assumption.}
undermines the above assumption.

We studied six open-source implementations from widely adopted codebases, namely:
Fairseq-ST~\citep{wang2020fairseqs2t}, ESPnet-ST~\citep{inaguma-etal-2020-espnet}, NeMo~\citep{kuchaiev2019nemo}, SpeechBrain~\citep{speechbrain}, 
%an open source implementation on github\footnote{\url{https://github.com/sooftware/conformer}} named \enquote{Conformer}, and the official PyTorch implementation from TorchAudio~\citep{yang2021torchaudio}.
an open source codebase named \enquote{Conformer}\footnote{\url{https://github.com/sooftware/conformer}}, and \torchimpl{}~\citep{yang2021torchaudio}.
We discovered that all these implementations return different results with different IBSs, showing that the presence of padding has unpredictable effects on the results.\footnote{We 
%would like to 
emphasize 
that our intention is not to
single out the shortcomings of individual libraries. Conversely, we are extremely thankful for the 
%important and worthy 
invaluable
contribution they represent for our community. Our analysis is only intended to further improve the correctness of codes and, consequently, of the experimental results, which we
believe is of utmost importance.}
%
%
%
% \footnote{We 
% %would like to emphasize 
% remark
% that our intention is not to blame any specific 
% % codebase or 
% codebase, nor
% its developers. Conversely, we are extremely thankful for the important 
% %and worthy contribution 
% contribution
% of these open-source libraries to our community. Our analysis is not intended 
% %to point out the issues of the single libraries but is intended to further improve the correctness of codes and, consequently, of the experimental results which we believe is of utmost importance.
% to single out the shortcomings of individual libraries, 
% but rather to promote best practices aimed at increasing
% %improved
% code correctness and, as a result,
% %more reliable
% the reliability of 
% experimental findings, which we consider to be of paramount importance.}
%
%
%
%
%
%
Upon inspection of the codes relative to the Conformer architecture, we 
%isolate
isolated
three types of bugs:


\begin{figure}[!tb]
\centering
\renewcommand*\thesubfigure{\arabic{subfigure}} 
     \begin{subfigure}[b]{0.46\textwidth}
        \centering
         \includegraphics[width=0.4\textwidth]{img/start.png}
         \caption{Before shifting, the Relative PE matrix ($P_{00},...,P_{22}$) is padded (zero values).}
     \end{subfigure}
     %\par\medskip
     \begin{subfigure}[b]{0.46\textwidth}
        \centering
         \includegraphics[width=0.4\textwidth]{img/wrong.png}
         \caption{When relative shift is applied to the Relative PE matrix without considering padding, some values of the padding area (in \textcolor{red}{\textbf{red}}) are incorrectly 
         %included in 
         moved to
         the non-padding area.}
         %\smallbug\textsubscript{3}, 
        % some values ($P_{11}, P_{12}, P_{20}, P_{21}$) are moved to the padding part (in \textcolor{red}{\textbf{red}}), hence are not considered in the following computation, while some padding is instead incorrectly considered in the following computation (in \textcolor{teal}{\textbf{green}}). Please notice that the first row is always discarded.}
     \end{subfigure}
     \begin{subfigure}[b]{0.46\textwidth}
        \centering
         \includegraphics[width=0.4\textwidth]{img/correct.png}
         \caption{When relative shift is applied to the Relative PE matrix considering padding, the values $P_{00},...,P_{22}$ are not moved to the padding area.}
     \end{subfigure}
    \caption{Example of relative shift operation starting from a Relative PE matrix containing padding (1), both considering a codebase with \bugthree{} (2) and without (3) bug. The first row is always discarded.}
    \label{fig:relativePEs}
\end{figure}

\paragraph{Convolution Module Bug (\bugone)} The depth-wise and point-wise convolutions of the Conformer convolution module do not consider the presence of padding and produce a non-padded output with non-zero values adjacent to the input sample.
These values 
%alter 
modify
the behavior of the subsequent batch normalization and of the other convolutions, leading to incorrect alterations of all the valid values.

\paragraph{Initial SubSampling Bug (\bugtwo)} The two initial convolutions that subsample the input sequence by a factor of 4 do not consider padding. For this reason, the second convolution is fed with non-zero values adjacent to the input sequence that lead to a wrong computation of the last valid elements.

\paragraph{Positional Encodings Bug (\bugthree)} The relative sinusoidal positional encodings (PEs), which are added to the attention matrix, are computed by shifting a sinusoidal matrix. This shifting operation first prepends a zero column to the sinusoidal matrix and then reshapes it so that the last element of the first row becomes the first element of the second row, the last two elements of the second row become the first ones of the third row, and so on. By doing this, this operation assumes that all elements are valid. However, when a 
%sentence
sequence
is padded, only a part of the attention matrix is valid (in green in Figure~\ref{fig:relativePEs}.1) and spurious values are moved to the beginning of the next row (Figure~\ref{fig:relativePEs}.2). 
In Figure~\ref{fig:relativePEs}, for the sake of clarity of the example, we pretend that existing implementations set to 0 the PE in the padding area. While this is not what happens in practice (as the padding area contains other sinusoidal PEs), it shows that the correct values are discarded and the final matrix significantly differs from 
%that
the one
obtained without padding, which is
%does not contain the correct values, which are 
instead shown in Figure~\ref{fig:relativePEs}.3.
%\mg{(i.e., those obtained without padding)}

\begin{table}[!ht]
\small
    \centering
    \setlength{\tabcolsep}{6pt}
    \begin{tabular}{l|c|c|c}
    \specialrule{.1em}{.05em}{.05em} 
         \textbf{Repository} & \textbf{Conv. Mod.} & \textbf{SubSampl.} & \textbf{Pos. Enc.} \\
         \specialrule{.1em}{.05em}{.05em} 
         Fairseq-ST & \bugone & \bugtwo & \bugthree \\
         ESPnet-ST & \bugone & \bugtwo & \bugthree \\
         NeMo & & \bugtwo & \bugthree \\
         SpeechBrain & \bugone & \bugtwo & \bugthree \\
         Conformer & \bugone & \bugtwo & \bugthree \\
         \torchimpl{} & \bugone & NA & NA \\
    \specialrule{.1em}{.05em}{.05em} 
    \end{tabular}
    \caption{Presence of bug(s) in the codebase of the analyzed repositories. NA stands for \enquote{Not Applicable}.}
    \label{tab:bug}
\end{table}

In Table \ref{tab:bug}, we report the presence (or absence) of these bugs for each analyzed codebase in its current version.\footnote{Checked on January 8th 2023.}
All the implementations but one (NeMo) are affected by \bugone.
%Moreover, 
%
%
%
%
% Also,
% all the implementations are affected by \bugtwo{} and \bugthree, except for the implementation by Pytorch, which does not introduce relative PEs in the attention and replaces the initial sub-sampling convolutional layers with linear layers that map the input sequence in matrices of fixed dimensions.
% %\mg{We} can conclude that all the \mg{analyzed implementations} contain at least one bug.
% Having ascertained that all the analyzed implementations contain at least one bug, the next sections will concentrate on their impact on ASR and ST results and related findings.
%
Also, 
all are affected by \bugtwo{} and \bugthree, except for
%implementation by Pytorch,
%\mn{PyTorch one,}
%the \torchimpl{} implementation,
\torchimpl,
which 
%does not introduce
includes neither relative PEs in the attention nor
%and 
%\sara{does not include}
%%replaces 
the initial sub-sampling convolutional layers 
in its implementation.
%with linear layers that map the input sequence in matrices of fixed dimensions.
Having ascertained that all the analyzed implementations contain at least one bug, the next sections will concentrate on their impact on ASR and ST results 
%and
and, in turn, the
related findings.

% \mn{In Table \ref{tab:bug}, we report the presence (or absence) of these bugs for each 
% %repository
% \mn{analyzed codebase}
% in its current version.\footnote{Checked on January 8th 2023.} As it can be seen, all the implementations contain at least one bug. In particular, all but one (NeMo) are affected by \bugone.
% %Moreover, 
% \mn{Also,}
% all the implementations are affected by \bugtwo{} and \bugthree, except for the implementation by Pytorch\mg{, which} does not \mg{introduce} relative PEs in the attention and replaces the initial sub-sampling convolutional layers with linear layers that map the input sequence in matrices of fixed dimensions.}




\subsection{Experimental Settings}

We train and evaluate ASR and ST models on MuST-C v1.0 \cite{CATTONI2021101155}, which contains parallel speech-to-text data with English (en) as source language and 8 target text languages, namely Dutch (nl), French (fr), German (de), Italian (it), Portuguese (pt), Romanian (ro), Russian (ru), and Spanish (es). 
The ASR model is 
trained on the transcripts of the MuST-C en-es  train set,
%\mn{built on the transcripts of the MuST-C en-es training set,}
as it is the largest section of the corpus
%\sara{of the dataset}.
%, containing $\sim$260K samples.
%Data
(for data statistics,
%are presented in 
see Appendix \ref{sec:data_stats}).
For ST, 8 different models are trained, one for each language direction en$\rightarrow$\{de, es, fr, it, nl, pt, ro, ru\}. 
All the experimental settings, including model architecture, and training hyperparameters, are described in Appendix \ref{sec:exp_sett}.
Evaluation is performed on
%\sara{, whose outputs are generated using a single NVIDIA A40 with 40GB of RAM,}
%MuST-C tst-COMMON with word error rate (WER) metric for ASR and with sacreBLEU \citep{post-2018-call}
the MuST-C tst-COMMON, 
by computing  word error rate (WER) for ASR and SacreBLEU \citep{post-2018-call}\footnote{BLEU|\#:1|c:mixed|e:no|tok:13a|s:exp|v:2.0.0} for ST.
All our 
trainings and inferences 
%have been 
were respectively
performed on two and one NVIDIA Ampere A40 GPUs.

%\paragraph{The Case of TF32}
%Another issue that potentially causes unexpected differences when padding is introduced does not relate to the code but to the use of NVIDIA Ampere GPUs (A40/A100) during training/inference.
It is worth remarking 
that, by default, on the Ampere GPUs,
%\mn{that, by default, on the Ampere GPUs,}
the PyTorch backend computes convolutions and matrix multiplications with TensorFloat-32\footnote{\url{https://pytorch.org/docs/stable/notes/cuda.html\#tensorfloat-32-tf32-on-ampere-devices}.} (\textbf{TF32}) cores. TF32 speeds up the computation but introduces numeric errors that can cause small 
%but 
random fluctuations, e.g. in presence of padding. 
In the following, we hence experiment both with and without TF32 (both at training and inference time) 
%as
because
%the presence of padding 
padding
has no effect on the final outputs only when 
%disabling TF32.
TF32 is disabled.


%\subsection{Impact of Single Bugs}
\subsection{Impact of the Identified Bugs}
\label{subsec:impact_bug}


% \sara{To quantify the problems caused by the presence of bugs in the code,}
% we analyze the impact of the three bugs described in \S\ref{subsec:analysis} on the performance of the models and we compare 
% \sara{the results with those obtained by}
% previous works
% \sara{on}
% different architectures. 
To assess the impact of 
%bugs in 
flawed
code, we examine the effect of the three bugs described in \S\ref{subsec:analysis} on the performance of the models and compare our results to those from previous studies using different architectures.
To this aim, we evaluate different IBSs, as increasing the batch size brings more padding, thus amplifying the effects of bugs. 
% \sara{The experiments are first conducted on the correct codebase (\smallcorrect), successively single precision is enabled (TF32), then bugs are introduced one by one in the code during both training and inference (\bugone, \bugtwo{}, and \bugthree) to evaluate their individual contribution, and are then introduced all together (\bugall).}
The experiments are first conducted on the correct codebase (\smallcorrect); successively, by enabling single precision (TF32); then by reintroducing bugs one by one during both training and inference (\bugone, \bugtwo{}, and \bugthree) to evaluate their individual contribution; finally by reintroducing them all together (\bugall).




\begin{table}[!tb]
\small
\setlength{\tabcolsep}{12pt}
    \centering
    \footnotesize
    \begin{tabular}{l|ccc}
    \specialrule{.1em}{.05em}{.05em} 
        \multirow{2}{*}{\textbf{Code}} & \multicolumn{3}{c}{\textbf{IBS}} \\
        \cline{2-4}
         & 1 & 10 & 100 \\
        \specialrule{.1em}{.05em}{.05em} 
        \smallcorrect & 10.52 & 10.52 & 10.52 \\
        \hline
        + TF32 & 10.73 & 10.73 & 10.73 \\
        \quad + \bugone & 10.72 & 11.25* & 19.50* \\
        \quad + \bugtwo & 10.73 & 10.74 & 10.74 \\
        \quad + \bugthree & \textbf{10.46} & 10.62 & 10.73 \\
        \quad + \smallbug\textsubscript{1,2,3} & 11.32* & 14.25* & 54.56* \\
    \specialrule{.1em}{.05em}{.05em} 
    \end{tabular}
    \caption{WER ($\downarrow$) scores for ASR obtained with TF32 and bugs
    %the different
    %types of bug \bugall{} 
    %\mg{bugs}
    %and TF32 for \mg{ASR}
    %the ASR task
    % on 
    % MuST-C 
    % %en-es 
    % tst-COMMON 
    as IBS varies (1, 10, and 100 sentences). *~indicates that the difference with \smallcorrect{} is statistically significant, computed with bootstrap resampling \citep{koehn-2004-statistical} with 95\% confidence interval (CI).
    %(computed with bootstrap resampling with 95\% confidence interval).
    }
    \label{tab:ablationASR}
\end{table}

\paragraph{ASR}

%The impact of TF32 and of the different bugs on the performance of the Conformer for the ASR task is shown in Table \ref{tab:ablationASR}. First, TF32 introduces only limited variations in the generated output (the number of generated words is slightly different), as the IBS varies. In addition, the small increase in WER (0.21) compared to \smallcorrect{} is not statistically significant.
Table \ref{tab:ablationASR} shows, in comparison to \smallcorrect{}, the impact of TF32 and of the different bugs on ASR performance. First, for each IBS, TF32 alone causes an identical, not statistically significant quality drop (+0.21 WER). This is due to minor variations in the output, attested by a slightly different number of generated words.
%
%
%
%\sara{First of all, the TF32 seems not to introduce impactful errors in the output computation as the WER remains constant when IBS varies. Although the 0.21 WER increase compared to codebase \smallcorrect{} is not statistically significant, it 
%highlights that we already obtain different outputs, thus the code exhibits an incorrect behavior, when only TF32 is activated.}
%However, we manually verified that the outputs obtained by varying IBS are not identical, which is anyway an incorrect behavior \mn{[QUESTA E' UN PO' BUTTATA LI': in che senso ``manually verified''? in che senso ``non identical''? Con diverse IBS e' sempre 10.73...che pero', vero, e' 0.21 piu' della versione bug-free...Qualunque cosa sia, e' un fatto grave, no? mi pare che quasi sorvoliate.]}.} 
% First of all, the TF32 seems not to introduce relevant errors in the output computation as the IBS varies, as the WER is constant,  although the outputs are not identical (which is, anyway, not a correct behavior). The WER increase of 0.21 is not statistically significant, thus we cannot assert that it degrades the performance. 
%Differently, 
%\sara{Moreover,}
%as
%
%
%
%
%As soon as any of the bugs (\bugone, \bugtwo, \bugthree) is introduced in the code, instead, the performance becomes highly influenced by the IBS. This phenomenon is particularly evident in the case of \bugone, for which we observe a major performance degradation (+8.78 WER) when we introduce a considerable amount of padding by increasing IBS from 1 to 100 sentences. 
As soon as any of the bugs 
%(\bugone, \bugtwo, \bugthree) 
is reintroduced in the code, the performance becomes highly sensitive to the IBS. This is particularly evident with \bugone, which causes a significant performance drop (+8.78 WER) when we introduce a considerable amount of padding by increasing IBS from 1 to 100 sentences.
%
%
%
% Noticeably,  the best result is obtained with \bugthree{} and 1 as IBS, and most of the differences with the bug-free version (\smallcorrect{}) are not statistically significant. 
It is noteworthy that most of the differences compared to the bug-free version (\smallcorrect{}) are not statistically significant, and the best ASR result is achieved with \bugthree{} and 1 as IBS.
%
%
%
% Only the presence of all bugs \bugall{} leads to a consistent and significant performance drop, although the results are more than
% %very 
% competitive with those obtained with a Transformer-based architecture on the same benchmark: \citet{CATTONI2021101155} report 26.61 WER, while \citet{gaido-etal-2021-ctc} obtain 15.6. 
Only the presence of all bugs \bugall{} 
%leads to
produces
consistent and statistically significant performance drops. Despite this, the results with 1 and 10 as IBS are still far better than those obtained with a Transformer-based architecture on the same benchmark (i.e. 26.61 by \citealt{CATTONI2021101155} and 15.6 by \citealt{gaido-etal-2021-ctc}). 
% %\mg{In addition, the results can be easily reproduced by setting the IBS to 1.}
% %
% %
% %
% %
% Notice 
% %that the bugs do not prevent the results from being reproducible: by setting the IBS to a specified value, the same result is obtained consistently and can be easily replicated.
% that the presence of  bugs does not hinder the reproducibility of the results,
%and
Moreover,
their reproducibility is not hindered by the presence of bugs,
as setting the IBS to any particular value consistently yields the same score.
%
%
%In light of these results, we can conclude that \textbf{\textit{competitive and reproducible results can be achieved even in presence of bugs}}, and focusing only on these two aspects is not enough.
In light of these results, we can conclude that \textbf{\textit{even flawed code can produce competitive and reproducible results}} and, therefore, focusing only on these two aspects is not enough
%to guarantee reliable findings.
to ensure the trustworthiness of the code.
%All in all, we can conclude that\mn{, contrary to the idea that a scientific product should ...,} \mg{\textbf{\textit{the presence of bugs can be hidden by very competitive results}} and \textbf{\textit{the reproducibility of results does not ensure code correctness}} either.} 

%\textit{\textbf{the presence of each 
% of the bugs described above cannot be noticed from the results
%\sara{bug is hidden by the competitive results}}}\sara{, even when bugs are present altogether,}
%and even the contemporaneous presence of all bugs cannot be spotted if compared with other works on the same benchmark with different architectures, 
%unless different IBSs are tested.



\begin{table}[!tb]
\small
\setlength{\tabcolsep}{2.5pt}
    \centering
    \footnotesize
    \begin{tabular}{l|ccc|ccc}
        \cline{2-7}
         & \multicolumn{3}{c|}{\textbf{en-de}} & \multicolumn{3}{c|}{\textbf{en-es}} \\
         \specialrule{.1em}{.05em}{.05em} 
        \multirow{2}{*}{\textbf{Code}} & \multicolumn{3}{c|}{\textbf{IBS}} & \multicolumn{3}{c}{\textbf{IBS}} \\
        \cline{2-7}
        & 1 & 10 & 100 & 1 & 10 & 100 \\
        \specialrule{.1em}{.05em}{.05em} 
        \smallcorrect & 24.67 & 24.67 & 24.67 & 30.34 & 30.34 & 30.34 \\
        \hline
        + TF32 & \textbf{24.84} & \textbf{24.84} & 24.83 & \textbf{30.63} & 30.62 & \textbf{30.63} \\
        \quad + \bugone & 24.52 & 24.65 & 24.67 & 29.53* & 29.41* & 27.71* \\
        \quad + \bugtwo & 24.56 & 24.57 & 24.58 & 30.53 & 30.53 & 30.53 \\
        \quad + \bugthree & 24.53 & 24.46 & 24.42 & 30.33 & 30.35 & 30.24\\
        \quad + \bugall & 24.68 & 24.58 & 23.23* & 28.57* & 27.81* & 21.15* \\
    \specialrule{.1em}{.05em}{.05em} 
    \end{tabular}
    % \caption{BLEU ($\uparrow$) \mg{scores} obtained with the different
    % %types of bug \bugall{}
    % bugs and TF32 for
    % %the ST task on MuST-C tst-COMMON
    % \mg{ST}
    % as IBS varies (1, 10, and 100 sentences). * indicates that the difference with \smallcorrect{} is statistically significant, computed with bootstrap resampling (95\% CI).
    % %(computed with bootstrap resampling with 95\% confidence interval).}
    % }
    \caption{BLEU ($\uparrow$) scores for ST obtained with TF32 and bugs as IBS varies (1, 10, and 100 sentences).
    %the st task. 
    *~indicates that the difference with \smallcorrect{} is statistically significant, computed with bootstrap resampling (95\% CI).
    %(computed with bootstrap resampling with 95\% confidence interval).}
    }
    \label{tab:ablationST}
\end{table}



\paragraph{ST} Table \ref{tab:ablationST} reports the same study on the two 
%of the 
most used language pairs of the MuST-C benchmark (en-de, and en-es).
The behavior is quite different 
%between the two language directions, although
between
%\mg{them,}
the two,
although the best results are always obtained with TF32 and without bugs. Indeed, 
%in
on
en-es the presence of \bugone{} causes statistically significant drops, which
increase 
with the IBS
%\sara{as the IBS increases}
and are exacerbated if combined with the other two bugs (\bugall).
%In
On
en-de, instead, 
%which is by far the most widely used benchmark in ST, 
none of the bugs has a significant impact on the results. Interestingly,
%with 1 as the IBS, 
the result obtained with all bugs (\bugall) and 1 as IBS is slightly 
%(0.01) 
higher (+0.01) than 
%\sara{those}
that 
without bugs (\smallcorrect).
Furthermore, 
%by 
comparing the scores 
obtained with all bugs (\bugall) and 1 as IBS with those
of previous works on ST
%and architectures with those obtained with \smallbug\textsubscript{1,2,3} and IBS 1 
(Table \ref{tab:comparison_others}), we can notice 
%that\sara{, similar to the findings for the ASR task,}
that, as previously observed for 
%the ASR task,
ASR,
%, as we have already seen for the ASR task,
%, as in ASR, 
the presence of bugs 
%is again not evident from the results\sara{, even in the case of the en-es language pair} where they cause a significant drop.
is  not evident from the results, which 
%can still be 
are still
competitive with those achieved by other 
%approaches and 
architectures.
%, \sara{as we have already seen for the ASR task, even when they cause a significant drop (en-es).}
%not even in en-es where they cause a significant drop.
%In light of this, we can strengthen 
This supports 
our previous conclusion that \textbf{\textit{good (and reproducible) results do not imply the correctness of the code}}, as this statement holds for different tasks and language pairs. 
%\sara{We reiterate that the presence of bugs can be spotted only when using higher IBS which, however, should be in theory an irrelevant hyperparameter for the performance of the system.}
We reiterate that the presence of bugs is evident only by testing with different IBS, which should be 
%an irrelevant hyperparameter for the performance of the systems.
%uncorrelated with
irrelevant for
the performance of the systems.
%Again, the presence of bugs was evident only by experimenting with different (high) IBSs, which should be an irrelevant hyperparameter.

%\mg{DA DIRE DA QUALCHE PARTE CHE TUTTI QUESTI RISULTATI SONO RIPRODUCIBILI!}
%
%For the ST task, we present the impact of TF32 and of the different bugs \smallbug\textsubscript{1,2,3} on two language directions of MuST-C v1.0, en-de, and en-es, having respectively different and similar word ordering with respect to the source language. Results are presented in Table \ref{tab:ablationST}.
%Differently from the ASR case, there is no bug that has a prevalent role over the others, except for bug \smallbug\textsubscript{1} in en-es that shows a BLEU degradation from 0.81 to 2.63 points compared to the correct code (\smallcorrect) as the batch size increases. In all the other cases, the influence of every single bug seems marginal if considered alone but is exacerbated when all the bugs are inserted in the code (\smallbug\textsubscript{1,2,3}). Therefore, we can conclude that the additive effect of bugs on performance degradation does not depend on the particular task but is valid for both ASR and ST.



\begin{table}[t]
\setlength{\tabcolsep}{10pt}
\centering
\small
\begin{tabular}{l|cc}
\specialrule{.1em}{.05em}{.05em} 
\textbf{Model} & \textbf{en-de} & \textbf{en-es} 
 \\
\specialrule{.1em}{.05em}{.05em} 
 ESPNet \cite{inaguma-etal-2020-espnet} & 22.9 & 28.0 \\
 Fairseq \cite{wang2020fairseqs2t} & 22.7 & 27.2 \\
% AFS \citep{zhang-etal-2020-adaptive} & 22.4 & 26.9 \\
 Speechformer \cite{papi-etal-2021-speechformer} & 23.6 & 28.5 \\
 E2E + ML \citep{zhao-etal-2021-mutual} & - & 28.5 \\
% MultiLang Adapters \citep{le-etal-2021-lightweight} & \textbf{24.7} & \textbf{28.7} \\
 SATE (no KD) \citep{xu-etal-2021-stacked} & 24.1 & - \\
 E2E-ST-FS \citep{pmlr-v162-zhang22i} & 23.0 & 28.0 \\
 \hline
 Conformer \bugall & \textbf{24.7} & \textbf{28.6} \\
\specialrule{.1em}{.05em}{.05em} 
\end{tabular}
% \caption{Comparison of the BLEU scores on MuST-C en-de and en-es of the Conformer implementation with all bugs using 1 as IBS with other models presented in the literature.}
\caption{
%\mn{BLEU scores on MuST-C en-de and en-es of the Conformer implementation with all bugs using 1 as IBS, and other models presented in literature.}
BLUE ($\uparrow$) scores of models trained on MuST-C en-de and en-es compared to the Conformer implemented with all bugs and 1 as IBS.
}
\label{tab:comparison_others}
\end{table}


\begin{table*}[t!]
\setcounter{table}{6}
\setlength{\tabcolsep}{5.7pt}
    \centering
    %\footnotesize
    \small
    \begin{tabular}{c|c|c||c|c|c|c|c|c|c|c||c}
    \specialrule{.1em}{.05em}{.05em} 
        \textbf{Code} & \textbf{Model} & \textbf{IBS} & \textbf{en-de} & \textbf{en-es} & \textbf{en-fr} & \textbf{en-it} & \textbf{en-nl} & \textbf{en-pt} & \textbf{en-ro} & \textbf{en-ru} & \textbf{Avg} \\
        \specialrule{.1em}{.05em}{.05em} 
        \multirow{6}{*}{\correct} & \multirow{3}{*}{Conformer} & 1 & \multirow{3}{*}{24.67} & \multirow{3}{*}{30.34} & \multirow{3}{*}{36.22} & \multirow{3}{*}{25.73} & \multirow{3}{*}{30.04} & \multirow{3}{*}{\textbf{30.55}} & \multirow{3}{*}{23.43} & \multirow{3}{*}{17.29} & \multirow{3}{*}{27.28} \\
        %\cline{3-11}
        & & 10 & & & & & & & & \\
        %\cline{3-11}
        & & 100 & & & & & & & & \\
        \cline{2-12}
        & \multirow{3}{*}{\shortstack{Conformer\\+\\CTC Compr.}} & 1 & \multirow{3}{*}{24.97} & \multirow{3}{*}{30.48} & \multirow{3}{*}{\textbf{36.43}} & \multirow{3}{*}{\textbf{26.25*}} & \multirow{3}{*}{\textbf{30.31}} & \multirow{3}{*}{30.09\textsuperscript{$\dagger$}} & \multirow{3}{*}{\textbf{24.67*}} & \multirow{3}{*}{\textbf{17.35}} & \multirow{3}{*}{\textbf{27.57}} \\
        %\cline{3-11}
        & & 10 & & & & & & & & \\
        %\cline{3-11}
        & & 100 & & & & & & & & \\
        \hline
        \multirow{6}{*}{\bug\textsubscript{1,2,3}} & \multirow{3}{*}{Conformer} & 1 & 24.68 & 28.57 & 35.70 & 25.81 & 29.68 & 30.22 & 23.52 & 15.83 & 26.75 \\
        %\cline{3-11}
        & & 10 & 24.58 & 27.81 & 35.65 & 25.70 & 29.35 & 30.02 & 23.43 & 15.36 & 26.49 \\
        %\cline{3-11}
        & & 100 & 23.23 & 21.15 & 31.70 & 23.42 & 24.92 & 27.72 & 22.68 & 11.05 & 23.23 \\
        \cline{2-12}
        & \multirow{3}{*}{\shortstack{Conformer\\+\\CTC Compr.}} & 1 & 24.95 & 30.49* & 36.27* & 25.84 & 29.42 & 30.04 & 23.96* & 17.05* & 27.25 \\
        %\cline{3-11}
        & & 10 & 25.21* & \textbf{30.72}* & 36.18* & 26.01 & 29.64 & 30.14 & 23.95* & 17.06* & 27.36 \\
        %\cline{3-11}
        & & 100 & \textbf{25.26}* & 30.52* & 36.36* & 25.88* & 29.66* & 30.16* & 23.92* & 16.87* & 27.33 \\
    \specialrule{.1em}{.05em}{.05em} 
    \end{tabular}
    \caption{BLEU ($\uparrow$) scores for ST
    %over all the 8 languages of MuST-C v1.0
    %tst-COMMON 
    %based on 
    of
    the correct/incorrect codebase 
    %of the Conformer model (
    with and without CTC compression
    %) 
    as IBS varies (1, 10, and 100). * and \textsuperscript{$\dagger$} indicate that, respectively, the improvement or the degradation 
    %given by 
    of
    %the 
    CTC compression is statistically significant, computed with bootstrap resampling (95\% CI).
    %(computed with bootstrap resampling with 95\% confidence interval).}
    }
    \label{tab:ST}
\end{table*}

\subsection{Impact of Building on Incorrect Code}
\label{subsec:impact_code}
%\mn{TO SHOWCASE...\\
%1) FACCIAMO ESPERIMENTI MIRATI.\\
%2) QUESTI ESP. MIRATI COINVOLGONO LA CTC COMPRESSION, UNA TECNICA UTILIZZATA in studi recenti.\\
%3) LA SCELTA DI CTC COMPR. E' MOTIVATA DALLE SUE CARATTERISTICHE INTRINSECHE CHE LA RENDONO POTENZIALMENTE SENSISBILE AI TRE BUG TROVATI.\\
%4) INFATTI, LA CTC COMPR. FUNZIONA COSI'...BLA.\\
%5) COME VEDEREMO, QUESTA CARATTERISTICA (IL SUO FUNZINAM.) FA SI' CHE ESPERIMENTI COL CONFORMER BACATO PORTINO A RISULTATI MOLTO POSITIVI (IN QUALCHE MODO SOSPETTI) IN CONTRADDIZIONE CON QUELLI DI ALTRI LAVORI RECENTI...\\
%6) MA QUESTI RISULTATI SONO VERITIERI????
%%%%NO, COL CONFORMER NON BACATO, L'HYPE RIENTRA...
%}

To showcase how the development of new solutions on top of incorrect code can result in misleading conclusions, we conducted ad-hoc experiments on both the correct (\smallcorrect) and incorrect (\bugall) codebases.
These experiments evaluate the effect of adding a technique that, given its intrinsic characteristics, we %speculate is 
hypothesize to be
potentially sensitive to the bugs identified in \S\ref{subsec:analysis}: the CTC compression \citep{liu2020bridging,gaido-etal-2021-ctc}.
This technique has been proposed for Transformer-based models, as it significantly reduces training and inference costs while providing only limited (if any) gains in output quality.
%, and 
It is based on the CTC mechanism \citep{Graves2006ConnectionistTC}, which 
%.
enables predicting 
%an 
output sequences of variable length
%\sara{s}
but shorter than the input ones, as in our case where transcript sub-words are predicted from audio frames.
For each input frame, the CTC produces a probability distribution over the possible target labels augmented with a \texttt{<blank>} symbol. This probability distribution is leveraged by the CTC compression (here applied to the 8\textsuperscript{th} layer) to collapse contiguous vectors corresponding to the same label by averaging them.
As a result, the CTC compression dynamically reduces the sequence length of the input, and, in turn, the amount of padding.
This leads experiments with the flawed Conformer implementations to reward the introduction of this technique with much larger gains than those shown on the Transformer architecture.
But are these improvements due to the architecture change or to the bugs?

%%%%%%%%%%%%%%%%%%%%
% \mg{These experiments evaluate}
% \sara{
%% The aim of the experiments is to investigate 
% the effect of adding the CTC compression \citep{liu2020bridging,gaido-etal-2021-ctc}, a technique proposed for Transformer-based models that significantly reduces training and inference costs  while only providing limited gains in output quality.
% This technique is based on the CTC mechanism \citep{Graves2006ConnectionistTC} that allows the prediction of output sequences of variable length shorter than the input ones, as in the case where transcript sub-words are predicted from audio frames.
% For each input time step, the CTC produces a probability distribution over the possible target labels augmented with a \texttt{<blank>} symbol. This probability distribution is leveraged by the CTC compression (here applied to the 8\textsuperscript{th} layer) to collapse contiguous vectors corresponding to the same label by averaging them.
% As a result, the CTC compression dynamically reduces the sequence length of the input, and, in turn, the amount of padding.
% Given its intrinsic characteristics, the CTC compression is potentially sensitive to the bugs identified in \S\ref{subsec:analysis} and, as we will show, its introduction to the Conformer encoder leads experiments with the flawed code to yield very positive results, contradicting those performed on Transformer. But are these improvements also valid for the correct Conformer implementation?}
%%%%%%%%%%%%%%%%%%%%
%what will the results of the experiments reveal about the effect of adding the CTC compression to the correct Conformer implementation?
% which we presume to be potentially impacted by the bugs identified in \S\ref{subsec:analysis} due to its intrinsic characteristics.
%%%%%%%%%
% These experiments 
%%involve the use of CTC compression,
% focus on the addition of the CTC compression technique, which, given its intrinsic characteristics, we speculate to be potentially sensitive to the bugs identified in \S\ref{subsec:analysis}. 
%The CTC compression has been proposed
%%%%%%%%
% \mg{\citet{liu2020bridging} and \citet{gaido-etal-2021-ctc} proposed the CTC compression}
% for Transformer-based models, showing that it greatly reduces training and inference costs
%%, contextually bringing 
% \sara{but brings} limited gains in terms of output quality\mg{.} 
%%%%%%%%
%\citep{liu2020bridging,gaido-etal-2021-ctc}.
%
%a technique introduced for ST that greatly reduces training and inference costs, but brings limited gains in terms of output quality of Transformer-based models \citep{liu2020bridging,gaido-etal-2021-ctc}.}
%a popular technique adopted in speech processing that, given its intrinsic characteristics, is potentially sensitive to the bugs identified in \S\ref{subsec:analysis}.
%Specifically, CTC compression aims at reducing the sequence length of the input while maintaining the original content. 
%%%%%%%%%%%%%
% It is based on the CTC mechanism \citep{Graves2006ConnectionistTC}, which enables the prediction of output sequences of variable length shorter than the input ones, as in the case in which the transcript sub-words are predicted from audio frames.
%%source audio frames are used to predict the transcript sub-words.
% For each input time step, the CTC produces a probability distribution over the possible target labels augmented with a \texttt{<blank>} symbol. This probability distribution is leveraged by the CTC compression (here applied to the 8\textsuperscript{th} layer) to collapse contiguous vectors corresponding to the same label by averaging them.
% As a result, the CTC compression dynamically reduces the sequence length of the input, and, in turn, the amount of padding.
% As we will see, this characteristic leads experiments with the flawed code to yield very positive results, 
%%\mg{with significantly larger gains than those previously assessed. In the following, hence, we will examine: are these improvements real?}
% \sara{contradicting those performed on Transformer. In the following, we examine: are these improvements also valid for the correct Conformer implementation?} 
%%%%%%%%%
%contradicting those of other recent works on Transformers \citep{liu2020bridging,gaido-etal-2021-ctc} indicating that CTC compression greatly reduces training and inference costs but brings limited gains in terms of output quality.
%In the following, we will examine whether these improvements are also valid for the correct Conformer implementation.


\begin{table}[t]
\setcounter{table}{5}
%\setlength{\tabcolsep}{4pt}
    \centering
    \small
    \begin{tabular}{l|c|ccc}
    \specialrule{.1em}{.05em}{.05em} 
        \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Code}} & \multicolumn{3}{c}{\textbf{IBS}} \\
        \cline{3-5}
         & & 1 & 10 & 100 \\
        \specialrule{.1em}{.05em}{.05em} 
        Conformer & \multirow{2}{*}{\smallcorrect} & 10.52 & 10.52 & 10.52 \\
        \hspace{0.5em}+ CTC Compr. & & 10.64 & 10.64 & 10.64 \\
        \hline
        Conformer & \multirow{2}{*}{\bugall} & 11.32 & 14.25 & 54.56 \\
        \hspace{0.5em}+ CTC Compr. & & 10.39* & \textbf{10.34}* & 10.81* \\ 
    \specialrule{.1em}{.05em}{.05em} 
    \end{tabular}
    \caption{WER ($\downarrow$) scores for ASR
    %on
    %\mg{MuST-C}
    %the transcripts of MuST-C en-es tst-COMMON
    %based
    %on the correct/incorrect codebase of the Conformer model
    of the correct/incorrect codebase
    %Conformer implementations 
    with 
    %(\bugall) 
    and without %(\smallcorrect) 
    %bugs
    %(with and without 
    CTC compression
    %) 
    as IBS varies (1, 10, and 100). * indicates that the improvement 
    %given by the 
    of
    CTC compression is statistically significant, computed with bootstrap resampling (95\% CI).
    %(computed with bootstrap resampling with 95\% confidence interval).}
    }
    \label{tab:ASR}
\end{table}





\paragraph{ASR}
Table \ref{tab:ASR} shows the effects on the ASR performance of the introduction of the CTC compression to both codebases (\bugall{} and \smallcorrect).
%on the ASR performance of introducing the CTC compression both with and without the bugs. 
We can notice that
%without the bugs, 
the CTC compression causes a small 
% \sara{(+0.12 WER)}
%(
and not statistically significant
%) 
performance degradation
(+0.12 WER)
when the correct implementation (\smallcorrect{}) is used (in accordance with the findings of \citealt{gaido-etal-2021-ctc} on the Transformer architecture).
%, as the WER increases by 0.12 (in accordance with the findings of \citealt{gaido-etal-2021-ctc} on the Transformer architecture). 
%When the bugs are introduced, 
When bugs are present in the codebase (\bugall),
instead, the conclusion is overturned:
%completely overturned:
the CTC compression brings statistically significant improvements (nearly -1 WER even with 1 as IBS). In addition, the best overall result is 
%obtained
achieved with 
%\sara{10 as IBS and}
%the 
%CTC compression \mg{and the \bugall{} codebase}
the \bugall{} codebase (with 10 as IBS and CTC compression).
This suggests that the introduction of CTC compression yields significant performance improvements, a finding that however is not verified when the correct codebase is used.
%\mn{[A voi sembrera' banale/inutile, ma io espliciterei quindi il ``misleading finding'' a cui si giunge a causa dei bachi (la CTC porta anche a miglioramenti di prestazioni e non solo di riduzione dei costi, giusto? Quindi si sopravvaluta la CTC?)]}
Therefore, we can conclude that \textit{\textbf{building on incorrect code can produce misleading findings}}.
%, \sara{since the introduction of bugs does not necessarily degrade the results.}
%and all the bugs
%\sara{implemented in the \bugall{} codebase.}
%(and 10 as IBS). 
%As such, not only we can reiterate 
%\sara{Therefore, we can affirm that}
%, but we also showed that 
%\sara{\textit{\textbf{good results not only can hide incorrect codebases but also lead to misleading findings}} since the}
%\textit{\textbf{building on incorrect code can produce misleading findings}}, 
%and the introduction of noise \mn{[Sorry ma non capisco: cos'e' questo ``noise''?]} at inference time does not necessarily degrade the results (as the padding introduced with 10 as IBS improves the performance with respect to 1 as IBS). 
%\mn{[Siamo in un altro punto ``All in all'': limare al massimo e far brillare il messaggio. Quest'ultima frase e' una chiusa un po' criptica a non so quanto efficace. Il passaggio si dovrebbere chiudere con qualcosa di splendente su cosa significa avere bachi che portano a misleading findings. Ora, invece, recepisco la chiusa allo stesso modo in cui recepisco il testo della canzone di Madame a Sanremo ;-)]}
%\sara{compared to that obtained with IBS 1).}





\paragraph{ST}
Table \ref{tab:ST} reports the same analysis on the 8 language pairs of MuST-C v1.0. As in ASR, the presence of 
%the 
bugs (\bugall) unfairly rewards the CTC compression mechanism, which achieves statistically significant gains 
%with IBS 100
%among
on
all the languages with 100 as IBS and 
%also on (at least) 4 out of 8
on 4/5 out of 8
languages with 1/10 as IBS.
%1 or 10.
%
%
%on at least 4 out of 8 language pairs at every IBS and on all languages with 100 sentences as IBS. 
With the 
%correct code \mn{[
bug-free version
%of Conformer?]}
(\smallcorrect), instead, the improvements are 
%only 
statistically significant only on two language pairs (en-it, and en-ro), while on en-pt there is a statistically significant degradation. 
%Interestingly, en-it is one of the only two languages where the improvement is significant with the correct code, while is among the languages where the improvements are statistically significant only with 100 as IBS with the bugs. 
% On average 
On average,
on all the language directions, the 
% improvement achieved
gain brought 
by the CTC compression
%\sara{on codebase \bugall{}}
in presence of 
%the 
bugs (\bugall)
ranges from 0.5 BLEU 
%(IBS=1) 
%\sara{(with IBS 1)}
(with 1 as IBS)
to 4.1 BLEU 
%(IBS=100) 
%\sara{(with IBS 100)}
(with 100 as IBS),
%in the presence of the bugs 
while 
%the gain 
it is only of 0.29 BLEU with 
the correct code~(\smallcorrect). 
%\sara{codebase~\smallcorrect.}
Moreover, the best scores for en-de and en-es
%(which curiously are the most widespread benchmarks) 
are 
%\sara{obtained with CTC Compession on the buggy code~(\bugall).}
achieved with the presence of 
%the 
bugs 
%(\bugall+CTC compression).
(\bugall) and CTC compression.
We can hence affirm that
%utilizing flawed codebases
\textbf{\textit{the presence of 
%the 
bugs
led to erroneous findings}} also 
%in the case of the ST task,
in the ST task,
as the introduction of CTC compression appears to significantly improve 
% the translation quality
translation quality
%,
%result in significant performance improvements, 
while this is not the case with the correct implementation.
%which are instead not supported by the results obtained from a correct codebase.
%We can hence confirm that the use of incorrect codebases can lead to misleading findings because, also in the ST task, the CTC compression seems to bring statistically significant gains while, instead, these gains are not verified on a correct codebase.
%This further demonstrates that
In addition, it shows that
\textit{\textbf{it is impossible to assess code correctness only by looking at the results}} since, for instance, the average performance gap between 
%\bugall{} and \smallcorrect{} codebases
\bugall{} and \smallcorrect{}
can be as little as 0.21 BLEU (when IBS is 10) and
% can further be
may be further
narrowed, or even overturned, with a 
%simple IBS
IBS
\enquote{tuning}.

%We can hence confirm on the ST task the finding of the previous ASR section, i.e. that \textit{\textbf{building on incorrect code can produce misleading findings}}, as the presence of bugs may lead to erroneously concluding that the CTC compression brings statistically significant gains on most of the language pairs without degrading on any of them, while this is not true. In addition, it further demonstrates that is virtually \textit{\textbf{impossible to assess code correctness from the results}}, as, for instance, the difference between correct and buggy code with CTC compression can be as little as 0.21 BLEU (with IFS=10) on average over 8 language pairs, and we cannot exclude that a ``tuning'' of the IBS could further narrow the gap (or even overturn it).
