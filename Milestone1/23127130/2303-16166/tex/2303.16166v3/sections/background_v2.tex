\begin{table*}[!htb]
    \centering
    %\small
    \setlength{\tabcolsep}{2pt}
    \begin{tabular}{c|c|l}
    \specialrule{.1em}{.05em}{.05em} 
        \textbf{Venue} & \multicolumn{2}{c}{\textbf{Review Form Content}} \\
        \specialrule{.1em}{.05em}{.05em} 
        %\multirow{2}{*}{AAAI} & Reproducibility & \smallcorrect{} Yes/No questions for each record of the Reproducibility Checklist\tnote{*} \\
        %    & Correctness & Evaluated by... \\
        % \hline
        \multirow{2}{*}{\shortstack{ACL Circuit\\via ARR}} & Reproducibility & \smallcorrect{} Score \\
            & Correctness & \smallwrong{} (\enquote{Software} evaluates impact and documentation but not correctness) \\
        \hline
        \multirow{2}{*}{\shortstack{ACL Circuit\\via SoftConf}} & Reproducibility & \smallcorrect{} Score  \\
            & Correctness & \smallwrong{} \\
        \hline
        \multirow{2}{*}{ICASSP} & Reproducibility & \smallwrong{} \\
            & Correctness & \smallcorrect{} Score for \enquote{Technical Correctness} but no definition is provided \\
        \hline
        \multirow{2}{*}{ICML} & Reproducibility & \smallwrong{} \\
            & Correctness & \smallwrong{} (\enquote{Soundness} does not mention the correctness of the code) \\
        \hline
        \multirow{2}{*}{ICLR} & Reproducibility & \smallcorrect{} Open question \\
            & Correctness & \smallcorrect{} Score but there is no specific reference to codebase \\
        \hline
        \multirow{2}{*}{Interspeech} & Reproducibility & \smallcorrect{} Score for \enquote{Technical Correctness} but it refers to reproducibility \\
            & Correctness & \smallwrong{} \\
        \hline
        \multirow{2}{*}{NeurIPS} & Reproducibility & \smallwrong{} \\
            & Correctness & \smallwrong{} (\enquote{Soundness} does not mention the correctness of the code) \\
        \hline
        \multirow{2}{*}{TACL} & Reproducibility & \smallcorrect{} Open question \\
            & Correctness & \smallcorrect{} Open question \\
    \specialrule{.1em}{.05em}{.05em} 
    \end{tabular}
    \caption{Reproducibility and correctness aspects evaluated in the review forms of major conferences/journals in the NLP field. \enquote{\smallcorrect{}} means the aspect is mentioned in the form, \enquote{\smallwrong{}} otherwise.}
    \label{tab:nlp}
\end{table*}
%\url{https://aaai.org/conference/aaai/aaai-23/reproducibility-checklist/}

In this section, we introduce the two key concepts of the paper, correctness (Section \ref{subsec:correctness}) and reproducibility (Section \ref{subsec:repro}). Then, we evaluate if and how the concept of correctness is currently evaluated in our NLP community and how is distinguished (if it is) from the reproducibility concept by analyzing the reviewing process of the most popular conferences in our field (Section \ref{subsec:NLP}).

\subsection{Correctness}
\label{subsec:correctness}
Software quality assurance or SQA \citep{Buckley-1984-sqa,tripathy2011software} targets production-ready codebases, defining quality attributes to be met and practices to enforce them. 
Some of the covered aspects of SQA are not critical for research code, such as its 
%\textit{reliability/availability} (i.e., how long the code can run without failures) or \textit{robustness} (i.e., the ability of a system to recover from error conditions).
\textit{reliability} (\enquote{the probability of failure-free operation of a software
component or system in a specified environment for a
specified time} \citealt{10.1145/1010891.1010893})
%.
%But
but
some others 
%are of extreme importance for research as SQA mentions important attributes that 
are critical for any codebase to be considered as a real contribution to the community 
as the \textit{correctness}.
%(\textit{functional correctness} \textit{correctness}and \textit{accuracy}), and others that should be strongly encouraged (\textit{usability}, \textit{maintainability}, \textit{installability}, and \textit{documentation}).
With the term \textit{correctness} we define the \enquote{extent to which a program satisfies its specifications and fulfills the user's mission objectives} \citep{McCall1977FactorsIS}. This means that a codebase has to be \textit{traceable} (i.e., there has to be a thread from the requirements to the implementation about specific
development and environment), \textit{consistent} (i.e., with uniform design and implementation techniques and notation), and \textit{complete} (i.e., required functions have to be fully implemented) and these attributes are usually verified by the use of tests \citep{10.1145/987305.987309,10.1145/800027.808473}.
The (technical) correctness of a paper is currently evaluated by both the reviewers and our community only on the basis of the strength of the baselines, the soundness of the hypothesis and the assumptions made in the paper, and the adoption of state-of-the-art evaluation procedures. The 
%\textit{functional correctness} 
correctness of a codebase,
%of the developed code, 
instead, is given for granted, or it is assumed on the basis of the results reported in a paper.
However, we posit (and demonstrate) that this assumption is not always correct and that SQA best practices, such as testing code \citep{8048665}, are essential to ensure the 
%technical 
correctness of the research code.


\subsection{Reproducibility}
\label{subsec:repro}
The term \textit{reproducibility} of a scientific paper means that 
%refers to the ability to independently obtain the same results from a study as those reported in the scientific paper. 
\enquote{an independent group can obtain the same result using the authorâ€™s own artifacts}\footnote{\url{https://www.acm.org/publications/policies/artifact-review-and-badging-current}}. This concept has not to be confused with \textit{repeatability} (i.e., \enquote{a researcher can reliably repeat her own computation}), and \textit{repeatability} (i.e., \enquote{an independent group can obtain the same result using artifacts which they develop completely independently}), as recent works claim \citep{belz-etal-2021-systematic,belz-etal-2022-quantified}
The reproducibility involves having access to the same data, methods, and, codebases used in the original study, and following the same procedures and protocols to arrive at the same findings. This 
%concept 
is fundamental for the research community as it helps to validate the scientific results and build upon them. Due to its importance, several works analyzed the reproducibility of published papers \citep{Gundersen_Kjensmo_2018,NEURIPS2019_c429429b,Gundersen_2019} also in the specific context of computational linguistics \citep{wieling-etal-2018-squib,belz-etal-2021-systematic}.
Most of them argue that the published works lack consistent evaluation settings \citep{marie-etal-2021-scientific,gehrmann2022repairing}, and cannot be reproduced \citep{narang-etal-2021-transformer}, even if codebases are available \citep{EMNLP}.
In this paper, we posit that the correctness of the experimental setups and codebases is fundamental and is not strictly implied by 
%their 
reproducibility and that these two concepts
%(correctness and reproducibility) 
should not be used interchangeably.
%but has to be verified with ad-hoc tests.


\subsection{The Concept of Correctness in NLP}
\label{subsec:NLP}
Although the community showing interest in discussing about 
%the current state of development of the field of NLP, 
the ways in which reported performance improvements on NLP benchmarks are meaningful,
as demonstrated by the last years' theme tracks of conferences like EMNLP\footnote{\url{https://2022.emnlp.org/calls/main_conference_papers/\#emnlp-2022-theme-track}} and ACL\footnote{\url{https://2023.aclweb.org/calls/main_conference/\#theme-track-reality-check}}, little efforts have been devoted to push towards improved technical correctness of the published scientific artifacts.

To establish if and how the software correctness of a paper is currently evaluated, we scraped the latest review forms of the most popular conferences/journals in the NLP field, namely: 
%AAAI, 
ACL Circuit (AACL-IJCNLP, ACL, EACL, EMNLP, NAACL) through Rolling Review (ARR), ACL Circuit through direct submission via Softconf, ICASSP, ICML, ICLR, Interspeech, NeurIPS, and TACL.
Details are provided in Table \ref{tab:nlp}.

% Computer speech and language?

The reproducibility of a paper is evaluated by conferences/journals in 5 out of 8 cases, confirming the recent trend of AI communities paying more attention to this aspect. Conversely, the correctness concept is never discussed in the questions asked to reviewers except for the TACL journal and the ICASSP and ICLR conferences. However, even if potentially mentioned in the review form under \enquote{Technical Correctness}, no description is provided to clarify what the scope of that question is by the ICASSP conference. Similarly, in the ICLR reviewer form, the correctness aspect is evaluated but the correctness of codebases is never mentioned. In the TACL review form, explicit questions are asked for reproducibility (\enquote{\textit{Will members of the ACL community be able to reproduce or verify the results in this paper?}}) and correctness/soundness (\enquote{\textit{First, is the technical approach sound and well-chosen? Second, can one trust the claims of the paper -- are they supported by proper experiments and are the results of the experiments correctly interpreted?}}) but again the correctness concept is not clearly stated and is also evaluated based on the results and not on the actual piece of code used to obtain them.

Moreover, it is notable to say that the Interspeech conference gives a wrong definition of \enquote{Technical Correctness} in the review form as it explicitly mentions reproducibility and not correctness by asking: \enquote{\textit{taking into account datasets, baselines, experimental design, are enough details provided to be able to reproduce the experiments?}}. This further highlights how terminology is often misused, claiming the need for precise and punctual questions clearly separating the concepts of reproducibility and correctness in the review forms.
