%As a proof of concept, \mn{[Mah: proof of concept mi sembra riduttivo e forse perfino sbagliato...E' proprio un esempio paradigmatico mediante il quale volete dimostrare la veridicita' di quanto dite. Non mi piace molto ma non ho a portata un'alternativa...]} 
As a case study,
we 
%study
examine
%the 
\mn{existing}
open-source implementations of the Conformer \citep{gulati20_interspeech} architecture, which is the state-of-the-art solution for speech processing tasks such as automatic speech recognition (ASR) and speech-to-text translation (ST).
%we choose to study the current state-of-the-art architecture Conformer \citep{gulati20_interspeech} for Speech Recognition and Translation.
After a brief description of 
%the concepts of Speech Recognition and Translation
the ASR and ST tasks and the Conformer architecture
(\S\ref{subsec:speech_background}), we analyze
%all the most popular open source codebases available for Conformer 
several widely-used open-source codebases that 
%are widely used in research %and 
%\sara{which}
contain a Conformer implementation, showing
%and show
that all the analyzed repositories 
%contain 
have at least one bug (\S\ref{subsec:analysis}). 
Through extensive experiments on the two tasks and on all the language pairs of the MuST-C v1.0 corpus \cite{CATTONI2021101155}, we demonstrate that the presence of bugs can be hidden by good -- but incorrect -- results (\S\ref{subsec:impact_bug})
that
%\mg{and} can consequently
%that 
 consequently
%\mn{[Cosa comporta, veramente, questo? Che conseguenze puo' avere il fatto che dei buoni risultati nascondano un baco? Se il baco porta sistematicamente a buoni risultati, per esempio, e' pericoloso? Accertarsi che ``hidden by good results'' non sia attaccabile in modo banale (del tipo, appunto: un baco che produce sistematicamente ottimi risultati e' davvero un baco?). Per rendere l'idea, \textbf{copio qui da un commento che ho messo sotto a uno di Sara piu' avanti: se costruisco un motore "sbagliato" ma che sulla stessa strada fa 100 km con un litro oggi, 50 domani e 110 dopodomani, quali sono i rischi/problemi? Non e' comunque meglio di quello che ne fa sempre 12?} (si prendano 50, 100, 110, 12 in pseudo-analogia con le IBS)]} 
%and can 
lead to wrong findings (\S\ref{subsec:impact_code}).
%\mn{[Questo e' molto piu' robusto e inattaccabile del precedente ``hidden by good results''...]}
%, while it can lead to wrong findings when building new solutions on the same codebases (Sections \ref{subsec:impact_code}).
%, and we demonstrate through extensive experiments that these bugs can lead to wrong findings, which can be hindered by good results (Sections \ref{subsec:impact_code}, and \ref{subsec:impact_bug}).


\subsection{Background}
\label{subsec:speech_background}
%Automatic speech recognition (ASR)
ASR
is the task in which an audio containing speech content is transcribed in its original language, while in 
%the speech translation (ST) task
ST
the source content is translated into a different language.
%In the last decade, traditional approaches for ASR relying on Gaussian Mixture Models \citep{gmm} have been replaced by deep neural models \citep{hinton-2012-asr} and then by an end-to-end or direct model \citep{pmlr-v32-graves14,Chorowski-2014-asr} capable of performing the ASR task within a single model.
%A similar progression has taken place in the field of ST, where cascade architectures consisting of an ASR model followed by a machine translation (MT) model \citep{cascade,cascade2} have been recently replaced by direct models \citep{berard_2016,weiss2017sequence}. Such direct models do not leverage intermediate representations to perform the task like in the cascade solutions and all their parameters are jointly optimized for ST. 
Nowadays, both tasks are commonly performed with end-to-end (or direct) models \citep{pmlr-v32-graves14,Chorowski-2014-asr,berard_2016,weiss2017sequence}, whose architecture is
%Current direct architectures employed to perform both ASR and ST tasks and are
based on the Transformer \citep{NIPS2017_3f5ee243}. The Transformer has been adapted to work with audio inputs \citep{8462506,gangi19_interspeech} by introducing two convolutional layers that shrink the length of the input sequence by a factor of 4.
% %\mn{[Se   uno non si occupa di ST e' un livello di dettaglio incomprensibile. Serve 'sta frase?]}.
% %Recently,
% \sara{More recently,}
% the Conformer \sara{architecture} \citep{gulati20_interspeech}, %has also 
% with
% a modified structure in the encoder,% layers, % and
% %Among these, \sara{the introduction of the Conformer \citep{gulati20_interspeech} with a modified structure in the encoder}
% %Conformer \citep{gulati20_interspeech}, which modifies the structure of the encoder layers with respect to the Transformer, 
% has %brought
% \sara{been introduced, achieving}
% significant improvements both in ASR and in ST \citep{inaguma2021non}.
% % represents the current state of the art
% %, with significant improvements compared to the vanilla Transformer architectures \citep{inaguma2021non}.
% %For this reason, we adopt the Conformer-based architecture for both tasks for the empirical part of this paper.
% %Therefore, in the empirical section of this paper, we adopt the Conformer-based architecture for both ASR and ST tasks.
\mg{More recently, \citet{gulati20_interspeech} proposed the Conformer \sara{architecture} by modifying the structure of the encoder layers, with significant improvements both in ASR and in ST \citep{inaguma2021non}.}

The changes introduced in the Conformer encoder layer can be summarized as follows:
\textit{i)} relative sinusoidal positional encodings \citep{dai-etal-2019-transformer} are introduced in the self-attention for improved generalization with respect to varying input lengths;
\textit{ii)} the FFN sublayer is replaced by two FFNs that wrap the self-attention, inspired by the Macaron-Net~\citep{lu-et-al-2016-macaron-net};
\textit{iii)}~a convolution module (depicted in Figure \ref{fig:conf_conv})
%\sara{, which is wrapped with a residual connection,} 
is added immediately after the self-attention, 
before the
%last 
second
FFN
%module.
layer.
The convolution module, which is wrapped in a residual connection, applies layer normalization and then a point-wise convolution to each feature vector, doubling its dimension that is restored to its original size by a Gated Linear Unit (GLU) activation function~\citep{Dauphin-2017-glu}. After, a depth-wise convolution with 31 kernel size is applied before a batch normalization \citep{ioffe-2015-batchnorm}, followed by the Swish activation function \citep{swish-2017}, and another point-wise convolution.
Lastly, a dropout module \citep{Srivastava-2014-dropout} randomly masks (i.e. zeroes out) a percentage of the feature values to prevent the network from overfitting.

%The convolutional module is wrapped in a residual connection.
%%, and, after
%After a layer normalization, a pointwise convolution transforms each feature vector representing a time step in the same way 
%%with a pointwise convolution that
%and doubles the size of the features. The feature dimension is brought to the original size by the Gated Linear Unit (GLU) activation function~\citep{Dauphin-2017-glu}.
%Then, a depthwise convolution with 31 kernel size is applied before a batch normalization \citep{ioffe-2015-batchnorm}, the Swish activation function \citep{swish-2017}, and another pointwise convolution similar to the first one.
%At last, a dropout module \citep{Srivastava-2014-dropout} randomly masks (i.e. zeroes out) a percentage of the values to prevent the network from overfitting.

\subsection{Analysis of the Codebases}
\label{subsec:analysis}

We 
%evaluate
analyze
the behavior of the open-source implementations of the Conformer 
%with respect to 
%\sara{focusing on}
when varying
a parameter that should not 
%change 
affect
the results: the inference batch size (IBS).
With a high IBS, multiple samples are collected in the same batch, allowing for their parallel processing on GPU, thus reducing the overall computational cost.
When samples of different lengths are collected in the same batch, a 
%common 
frequent situation in speech tasks where 
% the length of the input sentence greatly 
the input length largely
varies, the input sequences have to be brought to the same dimension by filling them with padding. 
% %\sara{This is realized to optimize the GPU computation, allowing several sentences to be processed in parallel and increasing efficiency without, in theory, changing the results.}
% %By doing so, the computation on GPU is more efficient, because several sentences are processed in parallel and, in theory, results should not change as the padding should be a filler that is ignored. 
\sara{For this reason, research papers often include details only %about
on
the training batch size, which is an important hyperparameter for the stability of the training, while the IBS is not reported.}
%\mg{As the IBS should be irrelevant for the results, it is rarely reported in research papers, which include only the training batch size that is, instead, important for the stability of the training.}
%, 
%as it should not affect the results.

%Our 
%%analysis focuses on
%\mg{study inspects}
We studied six open-source implementations 
%belonging to 
%\sara{of}
from
widely adopted codebases, namely:
%
%To conduct our study about the presence of incorrectness in the codebases focusing on the state-of-the-art Conformer architecture, we collected the most used repository for ASR and ST, namely: 
Fairseq-ST~\citep{wang2020fairseqs2t}, ESPnet-ST~\citep{inaguma-etal-2020-espnet}, NeMo~\citep{kuchaiev2019nemo}, SpeechBrain~\citep{speechbrain}, an open source
%(Apache-2.0) 
implementation on github\footnote{\url{https://github.com/sooftware/conformer}} named \enquote{Conformer}, and the official Pytorch implementation from TorchAudio~\citep{yang2021torchaudio}.
%\mg{We discovered that all these implementations returned different results with different IBS, showing that the presence of padding was (wrongly) altering the results.}
We discovered that all these implementations return
%ed 
different results with different IBSs, showing that the presence of padding has unpredictable effects 
%that reduce the reliability and trustability of
on the results.%(wrongly) alters the results.
\footnote{We would like to emphasize that our intention is not to blame any specific codebase or its developers. Conversely, we are extremely thankful for the important and worthy contribution of these open-source libraries to our community. Our analysis is not intended to point out the issues of the single libraries but is intended to further improve the correctness of codes and, consequently, of the experimental results which we believe is of utmost importance.}
%We inspected 
Upon inspection of the codes relative to the Conformer architecture, we isolate three types of bugs:
%the code relative to the Conformer architecture and we isolate three types of bugs:

\begin{figure}[!tb]
\centering
\renewcommand*\thesubfigure{\arabic{subfigure}} 
     \begin{subfigure}[b]{0.46\textwidth}
        \centering
         \includegraphics[width=0.4\textwidth]{img/start.png}
         \caption{Before shifting, the Relative PE matrix ($P_{00},...,P_{22}$) is padded (zero values).}
     \end{subfigure}
     %\par\medskip
     \begin{subfigure}[b]{0.46\textwidth}
        \centering
         \includegraphics[width=0.4\textwidth]{img/wrong.png}
         \caption{When relative shift is applied to the Relative PE matrix without considering padding, some values of the padding area (in \textcolor{red}{\textbf{red}}) are incorrectly 
         %included in 
         \mg{moved to}
         the non-padding area.}
         %\smallbug\textsubscript{3}, 
        % some values ($P_{11}, P_{12}, P_{20}, P_{21}$) are moved to the padding part (in \textcolor{red}{\textbf{red}}), hence are not considered in the following computation, while some padding is instead incorrectly considered in the following computation (in \textcolor{teal}{\textbf{green}}). Please notice that the first row is always discarded.}
     \end{subfigure}
     \begin{subfigure}[b]{0.46\textwidth}
        \centering
         \includegraphics[width=0.4\textwidth]{img/correct.png}
         \caption{When relative shift is applied to the Relative PE matrix considering padding, the values $P_{00},...,P_{22}$ are not moved to the padding area.}
     \end{subfigure}
    \caption{Example of relative shift operation starting from a Relative PE matrix containing padding (1), both considering a codebase with \bugthree{} (2) and without (3) bug. The first row is always discarded.}
    \label{fig:relativePEs}
\end{figure}

\paragraph{Convolution Module Bug (\bugone)} The depth-wise and point-wise convolutions of the Conformer convolution module do not consider the presence of padding and produce a non-padded output with non-zero values adjacent to the input sample.
These values alter the behavior of the subsequent batch normalization and of the other convolutions, leading to incorrect \mg{alterations of all the valid values.}
%computations.

\paragraph{Initial SubSampling Bug (\bugtwo)} The two initial convolutions that subsample the input sequence by a factor of 4 do not consider padding. For this reason, the second convolution is fed with
%\mg{(}wrong\mg{)}
non-zero values adjacent to the input sequence \mg{that lead to a wrong computation of the last valid elements.}
%, leading to an incorrect computation.

\paragraph{Positional Encodings Bug (\bugthree)} The relative sinusoidal positional encodings (PEs), which
%have to be added 
are added to the attention matrix, are computed by shifting a sinusoidal matrix. This shifting operation first prepends a zero column to the sinusoidal matrix and then reshapes
%the matrix
it so that the last element of the first row becomes the first element of the second, the last two elements of the second row become the first ones of the third row, and so on. By doing this, this operation assumes that all elements are valid. However, when a sentence is padded, only a part of the attention matrix is valid (in green in Figure~\ref{fig:relativePEs}.1)
%, so what is moved to the beginning of the next row is not the correct value
and spurious values are moved to the beginning of the next row (Figure~\ref{fig:relativePEs}.2). In Figure~\ref{fig:relativePEs}, for the sake of clarity of the example, we pretend that existing implementations set to 0 the PE in the padding area. While this is not what happens in practice (as the padding area contains other sinusoidal PEs), it shows that the correct values are discarded and the final matrix does not contain the correct values \mg{(i.e., those obtained without padding)}, which are instead shown in Figure~\ref{fig:relativePEs}.3.

%\begin{enumerate}
    %\item[\bugone] \textbf{Convolution Module Bug:}
    %the depth-wise and point-wise convolutions of the Conformer convolution module do not consider the presence of padding and produce a non-padded output with non-zero values adjacent to the input sample.
    %%cause the padding area to contain non-zero values (mutated from the learned \textit{bias} element of the convolution module and from the adjacent input). 
    %These values alter the behavior of the \sara{subsequent} batch normalization and of the other convolutions, leading to 
    %\sara{incorrect computations.}
    %%alteration also of the real values that do not fall in the padding area.
    %%in the Convolution Module of the Conformer Encoder, a series of gated Convolutional Layers \citep{pmlr-v70-dauphin17a} is applied to the input together with a Batch Normalization Layer. Since the output of the Convolutional Layers does not preserve padding, the final output obtained after the Convolution Module is not padded anymore. Also, 
    %%the 
    %Batch Normalization 
    %%Layer 
    %%\citep{pmlr-v37-ioffe15} is applied to an input that is not padded. However, since input data is divided into batches during both training and inference of the model, and padding is consequently applied to fit more samples in a single batch, preserving the padding information is important to avoid incorrect computation.
    %%%% OLD TWO
    %\item[\bugtwo] \textbf{Initial SubSampling Bug:} 
    %the two initial convolutions that subsample the input audio sequence by a factor of 4 do not consider padding. For this reason, the second convolution is fed with 
    %\sara{wrong non-zero values adjacent to the input sequence, leading to an incorrect computation.}
    %%%%
    %non-zero values in the padding area close to the end of the valid sequences. These (dirty) values contribute to the computation of the last valid elements of the resulting sequence, altering the correct result.
    %speech inputs are $\sim$10 longer than textual inputs, therefore an initial input dimension reduction is necessary to train an ASR/ST model due to the squared memory complexity on the input length of the attention operation present in all the current architectures \citep{gangi19_interspeech,wang2020fairseqs2t,inaguma-etal-2020-espnet}. For this reason, a block of convolutions with a striding factor $\geq2$ is applied to the input before feeding it to the Conformer Encoder \citep{vyas21_interspeech}, realizing a sub-sampling of the sequence. However, the convolution operation does not preserve padding, meaning that the second convolution operation is applied to a non-padded input and so on. 
    %This represents a problem since batches are used for both training and inference, and preserving padding between operations is important.
    %This constitutes an issue, as batches are utilized for both training and inference, and maintaining padding between operations is crucial.
    %%%%% OLD THREE
    %\item[\bugthree] \textbf{Positional Encodings Bug:} 
    %the relative sinusoidal positional encodings (PEs)\sara{, which have to be added to the attention matrix, are}
    %%are added to the attention matrix and  
    %computed by shifting a sinusoidal matrix. This shifting operation first prepends a zero column to the sinusoidal matrix and then reshapes the matrix so that the last element of the first row becomes the first element of the second, the last two elements of the second row become the first ones of the third row, and so on. By doing this, this operation assumes that all elements are valid. However, when a sentence is padded, only a part of the attention matrix is valid (%e.g., see the green
    %\sara{in} green in Figure \ref{fig:relativePEs}.1), so what is moved to the beginning of the next row is not the correct value \sara{(Figure \ref{fig:relativePEs}.2)}. In Figure \ref{fig:relativePEs}, for the sake of clarity of the example, we pretend that existing implementations set to 0 the PE in the padding area. While this is not what happens in practice (as the padding area contains other sinusoidal PE), it shows that the correct values are discarded and the final matrix does not contain the correct values\sara{, which are instead shown in Figure \ref{fig:relativePEs}.3}.
    %%%
    %in the Conformer Encoder attention, relative positional encodings (PEs) have been introduced in place of the absolute ones thanks to their improved performance in encoding information \citep{dai-etal-2019-transformer}. Relative PEs are dynamically calculated during training and, in their computation, the sinusoid encoding matrix \citep{NIPS2017_3f5ee243} is relatively shifted regardless of the padding (see Figure \ref{fig:relativePEs}), i.e. some "padding" is shifted together with the relevant information, thus causing an error in the final PEs. 
%\end{enumerate}

\begin{table}[!ht]
\small
    \centering
    \setlength{\tabcolsep}{6pt}
    \begin{tabular}{l|c|c|c}
    \specialrule{.1em}{.05em}{.05em} 
         \textbf{Repository} & \textbf{Conv. Mod.} & \textbf{SubSampl.} & \textbf{Pos. Enc.} \\
         \specialrule{.1em}{.05em}{.05em} 
         Fairseq-ST & \bugone & \bugtwo & \bugthree \\
         ESPnet-ST & \bugone & \bugtwo & \bugthree \\
         NeMo & & \bugtwo & \smallbug \\
         SpeechBrain & \bugone & \bugtwo & \bugthree \\
         Conformer & \bugone & \bugtwo & \bugthree \\
         Pytorch & \bugone & NA & NA \\
    \specialrule{.1em}{.05em}{.05em} 
    \end{tabular}
    % \caption{Presence of bug(s) \smallbug{} in the codebase for each one of the analyzed repositories. NA stands for \enquote{Not Applicable}.}
    \caption{\mn{Presence of bug(s) in the codebase of the analyzed repositories. NA stands for \enquote{Not Applicable}.}}
    \label{tab:bug}
\end{table}

In Table \ref{tab:bug}, we report the presence (or absence) of
%one or more of these
these bugs for each repository in its current version.\footnote{Checked on January 8th 2023.}
%
%
%All the public implementations we analyzed contain at least one bug. 
%Noticeably, all
All the implementations but one (NeMo) are affected by \mg{\bugone.}
%the Convolution Module Bug in the Conformer encoder.
Moreover, all the implementations are affected by
%bugs in the initial SubSampling module and in the relative PEs computation, 
\bugtwo{} and \bugthree,
except for the implementation by Pytorch\mg{, which}
%. The latter 
does not 
%make use of
\mg{introduce} relative PEs in the attention and replaces the initial sub-sampling convolutional layers with linear layers that map the input sequence in matrices of fixed dimensions.
%Therefore, we
\mg{We} can conclude that all the
%public implementations we analyzed
\mg{analyzed implementations}
contain at least one bug.

\paragraph{The Case of TF32}
%\mn{[Questa cosa sembra laterale ma non mi e' ben chiaro come si rapporto con i bug. Quello che mi confonde e' anche il fatto che dopo, nella discussione, partite proprio da questo aspetto. E' importantissimo o si puo' omettere (o spostare in appendice)? Se e' importantissimo, riuscite ad anticipare come contribuisce, dopo, alla discussione dei risultati?]} 
Another issue that potentially causes unexpected differences when padding is introduced 
does not relate
%is not related 
to the code but
%We also identify a problem affecting the computation that is not directly related to the implementation itself but is relative 
to the use of NVIDIA Ampere GPUs (A40/A100) during training/inference.
By default on these GPUs, PyTorch backend computes convolutions and matrix multiplications with TensorFloat-32\footnote{\url{https://pytorch.org/docs/stable/notes/cuda.html\#tensorfloat-32-tf32-on-ampere-devices}.} (TF32) cores that speed up the computation but reduce precision. Numeric errors can cause small but random fluctuations in presence of padding. We explore also the impact of this aspect by
%For these GPUs, the computation is executed in TensorFlow-32 (TF32) precision by default, speeding up the calculation but reducing precision.\footnote{\url{https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices}.}
%\footnote{\url{https://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_tensor_float_32_execution}} 
%Hence, computations executed with and without single precision enabled can produce impacting differences in the results. We further explore this aspect by 
enabling and disabling TF32 during both training and inference.



\subsection{Experimental Settings}
%\subsection{Data and Evaluation}

We train and evaluate ASR and ST models on MuST-C v1.0 \cite{CATTONI2021101155}, which contains parallel speech-to-text data with English (en) as source language and 8 target text languages, namely Dutch (nl), French (fr), German (de), Italian (it), Portuguese (pt), Romanian (ro), Russian (ru), and Spanish (es). The ASR model is trained on the 
%transcriptions 
transcripts of the MuST-C en-es train set,
\mg{as it is the largest section, containing}
%containing 
$\sim$260K \mg{samples.}
%sentences.
For ST, 8 different models are trained, one for each language direction en$\rightarrow$\{de, es, fr, it, nl, pt, ro, ru\}. 
All the experimental settings, including model architecture, and training hyperparameters, are described in Appendix \ref{sec:exp_sett}\mg{.}
%and the codebase is released open source at \url{[anonymous\_url]}.
%All the evaluations 
Evaluation is performed on the MuST-C tst-COMMON\sara{, whose outputs are generated using a single NVIDIA A40 with 40GB of RAM,} with word error rate (WER) metric for ASR and with sacreBLEU \citep{post-2018-call}\footnote{\mg{BLEU|\#:1|c:mixed|e:no|tok:13a|s:exp|v:2.0.0}}
%\footnote{BLEU+case.mixed+smooth.exp+tok.13a+version.1.5.1}
for ST.
%by using a single NVIDIA A40 with 40GB of RAM.



\subsection{Impact of Single Bugs}
\label{subsec:impact_bug}


%First,
\sara{To show the extent of the problems caused by the presence of bugs in the code,}
%of all, 
we analyze the impact of the three bugs described in \S\ref{subsec:analysis} on the performance of the models and we compare 
%them with results from
\sara{the results with those obtained by}
previous works
%with 
\sara{on}
different architectures. To this aim, we evaluate different IBSs, as increasing the batch size brings more padding, thus amplifying the effects of 
%the 
bugs. 
\sara{The experiments are first conducted on the correct codebase \mg{(}\smallcorrect\mg{)}, successively single precision is enabled (TF32), then bugs are introduced one by one individually in the code during both training and inference (\bugone, \bugtwo{}, and \bugthree), and then are introduced all together (\bugall).}
%\mn{[Visto che l'ho capito solo molto dopo, spiegherei qui con cosa vi confrontate (la versione bug free) e come (reintroducendo introducendo i bugs uno alla volta? Se si', i bug sono \textbf{esattamente quelli della versione originale}? Questo aspetto penso sia importante per evitare che ci sia il sospetto di una vostra manipolazione del codice che esaspera il problema dei bugs. I bug riscontrati non devono in alcun modo essere ricollegabili a vostre reimplementazioni/approssimazioni.)]}




%To investigate the contribution of each bug to the final results, we perform an ablation study by first isolating the impact of TF32 and then by analyzing the performance drop after the introduction of each of the bugs \smallbug\textsubscript{1,2,3} described in Section \ref{subsec:analysis}. 



\begin{table}[!tb]
\small
\setlength{\tabcolsep}{12pt}
    \centering
    \footnotesize
    \begin{tabular}{l|ccc}
    \specialrule{.1em}{.05em}{.05em} 
        \multirow{2}{*}{\textbf{Code}} & \multicolumn{3}{c}{\textbf{IBS}} \\
        \cline{2-4}
         & 1 & 10 & 100 \\
        \specialrule{.1em}{.05em}{.05em} 
        \smallcorrect & 10.52 & 10.52 & 10.52 \\
        \hline
        + TF32 & 10.73 & 10.73 & 10.73 \\
        \quad + \bugone & 10.72 & 11.25* & 19.50* \\
        \quad + \bugtwo & 10.73 & 10.74 & 10.74 \\
        \quad + \bugthree & \textbf{10.46} & 10.62 & 10.73 \\
        \quad + \smallbug\textsubscript{1,2,3} & 11.32* & 14.25* & 54.56* \\
    \specialrule{.1em}{.05em}{.05em} 
    \end{tabular}
    \caption{WER ($\downarrow$) scores obtained with the different types of bug \bugall{} and TF32 for the ASR task on MuST-C 
    %en-es 
    tst-COMMON as IBS varies (1, 10, and 100 sentences). * indicates that the difference with \smallcorrect{} is statistically significant\mg{, computed with bootstrap resampling \citep{koehn-2004-statistical} with 95\% confidence interval (CI).}
    %(computed with bootstrap resampling with 95\% confidence interval).
    }
    \label{tab:ablationASR}
\end{table}

\paragraph{ASR}

The impact of TF32 
%\mn{[PERCHE' PARTITE DALL'ULTIMO ASPETTO, CHE NON ERA TRA I BUG?? AVETE APPENA DETTO ``we analyze the impact of the three bugs'' e poi partite da questa cosa che sembrava laterale...]} 
and of the different bugs on the performance of the Conformer for the ASR task is shown in Table \ref{tab:ablationASR}.
\sara{First of all, the TF32 seems not to introduce 
%relevant \mn{[very impactful? o robe del genere]} 
impactful errors in the output computation as the WER remains constant when IBS varies. Although the 0.21 WER increase compared to codebase \smallcorrect{} is not statistically significant, it 
highlights that we already obtain different outputs, thus the code exhibits an incorrect behavior, when only TF32 is activated.}
%However, we manually verified that the outputs obtained by varying IBS are not identical, which is anyway an incorrect behavior \mn{[QUESTA E' UN PO' BUTTATA LI': in che senso ``manually verified''? in che senso ``non identical''? Con diverse IBS e' sempre 10.73...che pero', vero, e' 0.21 piu' della versione bug-free...Qualunque cosa sia, e' un fatto grave, no? mi pare che quasi sorvoliate.]}.} 
% First of all, the TF32 seems not to introduce relevant errors in the output computation as the IBS varies, as the WER is constant,  although the outputs are not identical (which is, anyway, not a correct behavior). The WER increase of 0.21 is not statistically significant, thus we cannot assert that it degrades the performance. 
%Differently, 
\sara{Moreover,}
as soon as any of the bugs (\bugone, \bugtwo, \bugthree) is introduced 
%\mn{[In che senso ``is introduced''? Usate i sistemi cosi' come sono, e quindi i bug ci sono, o li avete corretti e li introducete uno alla volta? Non so se e' chiara la domanda, ma provate a ragionare su 'sto ``is introduced''. NOTA POSTUMA: Anche qui, aver capito da quanto scritto sotto che il confronto e' fatto con la bug-free version rende piu' chiaro il tutto. Resta comunque da dire PRIMA che i bug sono reintrodotti uno ad uno, no?]} 
in the code, the performance 
%is 
\sara{becomes}
highly influenced by the IBS. This phenomenon is 
%more present 
particularly evident in the case of bug \smallbug\textsubscript{1} for which we observe a major performance degradation (+8.78 WER) when we introduce a considerable amount of padding by increasing %the batch size 
\sara{IBS}
from 1 to 100 sentences. 
%switching from 1 to 100 sentences per batch.
%However, 
Noticeably, 
the best result is obtained with \bugthree{} and 1 as IBS, and most of the differences with the bug-free version \smallcorrect{} are not statistically significant. Only the presence of all bugs \bugall{} leads to a consistent performance drop, although the results are \sara{more than} 
%very 
competitive 
%\mn{[Beh, sono piu' che very competitive, visto che sono  5 punti WER sotto Gaido 2021 e 15 sotto Cattoni 2021...]} 
with those obtained with a Transformer-based architecture on the same benchmark\sara{:} \citealt{CATTONI2021101155} report 26.61 WER, while \citealt{gaido-etal-2021-ctc} obtain 15.6. 
%\mn{[RAFFINEREI QUESTO PASSAGGIO PERCHE' NON SI SA CHI FA COSA E COME I NUMERI SI COLLEGANO A ``although the results are still very competitive...'']}.
\mg{In addition, the results can be easily reproduced by setting the IBS to 1.}
All in all, we can conclude that\mn{, contrary to the idea that a scientific product should ...,} \mg{\textbf{\textit{the presence of bugs can be hidden by very competitive results}} and \textbf{\textit{the reproducibility of results does not ensure code correctness}} either.} 

%\textit{\textbf{the presence of each 
% of the bugs described above cannot be noticed from the results
%\sara{bug is hidden by the competitive results}}}\sara{, even when bugs are present altogether,}
%and even the contemporaneous presence of all bugs cannot be spotted if compared with other works on the same benchmark with different architectures, 
%unless different IBSs are tested.



\begin{table}[!htb]
\small
\setlength{\tabcolsep}{2.5pt}
    \centering
    \footnotesize
    \begin{tabular}{l|ccc|ccc|}
        \cline{2-7}
         & \multicolumn{3}{c|}{\textbf{en-de}} & \multicolumn{3}{c|}{\textbf{en-es}} \\
         \specialrule{.1em}{.05em}{.05em} 
        \multirow{2}{*}{\textbf{Code}} & \multicolumn{3}{c|}{\textbf{IBS}} & \multicolumn{3}{c|}{\textbf{IBS}} \\
        \cline{2-7}
        & 1 & 10 & 100 & 1 & 10 & 100 \\
        \specialrule{.1em}{.05em}{.05em} 
        \smallcorrect & 24.67 & 24.67 & 24.67 & 30.34 & 30.34 & 30.34 \\
        \hline
        + TF32 & \textbf{24.84} & \textbf{24.84} & 24.83 & \textbf{30.63} & 30.62 & \textbf{30.63} \\
        \quad + \bugone & 24.52 & 24.65 & 24.67 & 29.53* & 29.41* & 27.71* \\
        \quad + \bugtwo & 24.56 & 24.57 & 24.58 & 30.53 & 30.53 & 30.53 \\
        \quad + \bugthree & 24.53 & 24.46 & 24.42 & 30.33 & 30.35 & 30.24\\
        \quad + \bugall & 24.68 & 24.58 & 23.23* & 28.57* & 27.81* & 21.15* \\
    \specialrule{.1em}{.05em}{.05em} 
    \end{tabular}
    \caption{BLEU ($\uparrow$) obtained with the different
    %types of bug \bugall{}
    \mg{bugs} and TF32 for the ST task on MuST-C tst-COMMON as IBS varies (1, 10, and 100 sentences). * indicates that the difference with \smallcorrect{} is statistically significant\mg{, computed with bootstrap resampling (95\% CI).}
    %(computed with bootstrap resampling with 95\% confidence interval).}
    }
    \label{tab:ablationST}
\end{table}



\paragraph{ST} Table \ref{tab:ablationST} reports the same study on the two 
%of the 
most used language pairs of the MuST-C benchmark (en-de, and en-es).
The behavior is quite different between the two language directions, although the best results are always obtained with TF32 and without bugs. Indeed, in en-es the presence of \bugone{} causes statistically significant drops
%, which 
\sara{that}
increase 
%with the IBS
\sara{as the IBS increases} and are exacerbated if combined with the other two bugs (\bugall).
In en-de, instead, 
%which is by far the most widely used benchmark in ST, 
none of the bugs has a significant impact on the results. Interestingly,
%with 1 as the IBS, 
the result obtained with all bugs (\bugall) and 1 as IBS is slightly (0.01) higher than \sara{those} without bugs.
Furthermore, 
%by 
comparing the scores 
\sara{obtained with all bugs \bugall{} and 1 as IBS with those}
of previous works \sara{on ST}
%and architectures with those obtained with \smallbug\textsubscript{1,2,3} and IBS 1 
(Table \ref{tab:comparison_others}), we can notice that
%, as in ASR, 
the presence of bugs is \sara{again} not evident from the results, \sara{as we have already seen for the ASR task, even when they cause a significant drop (en-es).}
%not even in en-es where they cause a significant drop.
%In light of this, we can strengthen 
This supports 
our previous conclusion that \textbf{\textit{good (and reproducible) results do not imply the correctness of the code}}, as this statement holds for different tasks and language pairs. 
%\sara{We reiterate that the presence of bugs can be spotted only when using higher IBS which, however, should be in theory an irrelevant hyperparameter for the performance of the system.}
We reiterate that the presence of bugs is evident only by testing with different IBS, which should be an irrelevant hyperparameter for the performance of the systems.
%Again, the presence of bugs was evident only by experimenting with different (high) IBSs, which should be an irrelevant hyperparameter.

%\mg{DA DIRE DA QUALCHE PARTE CHE TUTTI QUESTI RISULTATI SONO RIPRODUCIBILI!}
%
%For the ST task, we present the impact of TF32 and of the different bugs \smallbug\textsubscript{1,2,3} on two language directions of MuST-C v1.0, en-de, and en-es, having respectively different and similar word ordering with respect to the source language. Results are presented in Table \ref{tab:ablationST}.
%Differently from the ASR case, there is no bug that has a prevalent role over the others, except for bug \smallbug\textsubscript{1} in en-es that shows a BLEU degradation from 0.81 to 2.63 points compared to the correct code (\smallcorrect) as the batch size increases. In all the other cases, the influence of every single bug seems marginal if considered alone but is exacerbated when all the bugs are inserted in the code (\smallbug\textsubscript{1,2,3}). Therefore, we can conclude that the additive effect of bugs on performance degradation does not depend on the particular task but is valid for both ASR and ST.



\begin{table}[t]
\setlength{\tabcolsep}{10pt}
\centering
\small
\begin{tabular}{l|cc}
\specialrule{.1em}{.05em}{.05em} 
\textbf{Model} & \textbf{en-de} & \textbf{en-es} 
 \\
\specialrule{.1em}{.05em}{.05em} 
 ESPNet \cite{inaguma-etal-2020-espnet} & 22.9 & 28.0 \\
 Fairseq \cite{wang2020fairseqs2t} & 22.7 & 27.2 \\
% AFS \citep{zhang-etal-2020-adaptive} & 22.4 & 26.9 \\
 Speechformer \cite{papi-etal-2021-speechformer} & 23.3 & 28.5 \\
 E2E + ML \citep{zhao-etal-2021-mutual} & - & 28.5 \\
% MultiLang Adapters \citep{le-etal-2021-lightweight} & \textbf{24.7} & \textbf{28.7} \\
 SATE (no KD) \citep{xu-etal-2021-stacked} & 24.1 & - \\
 E2E-ST-FS \citep{pmlr-v162-zhang22i} & 23.0 & 28.0 \\
 \hline
 Conformer \bugall & \textbf{24.7} & \textbf{28.6} \\
\specialrule{.1em}{.05em}{.05em} 
\end{tabular}
% \caption{Comparison of the BLEU scores on MuST-C en-de and en-es of the Conformer implementation with all bugs using 1 as IBS with other models presented in the literature.}
\caption{
%\mn{BLEU scores on MuST-C en-de and en-es of the Conformer implementation with all bugs using 1 as IBS, and other models presented in literature.}
\sara{BLUE \mg{($\uparrow$)} scores of models trained on MuST-C en-de and en-es compared to the Conformer implemented with all bugs and 1 as IBS.}
}
\label{tab:comparison_others}
\end{table}






\subsection{Impact of Building on Incorrect Code}
\label{subsec:impact_code}

To showcase how \sara{the use of incorrect code to develop new software can lead to}
%building new works on incorrect code can cause 
misleading findings, we evaluate the introduction of the CTC compression (or shrinking) mechanism \citep{liu2020bridging,gaido-etal-2021-ctc} \sara{to the Conformer encoder}. The CTC \citep{Graves2006ConnectionistTC} enables producing an output sequence of variable length that is \sara{often} shorter than the input one \sara{while preserving its original content.}
%, as in the case in which the input is audio and the target is the sequence of uttered symbols -- characters, sub-words -- corresponding to the transcript. 
For each input time step, the CTC produces a probability distribution over the possible target labels augmented with a dedicated \texttt{<blank>} symbol representing the absence of 
%a target value. 
\sara{any target labels.}
The CTC compression collapses contiguous vectors %(\sara{by} averaging them) 
corresponding to the same prediction \sara{by averaging them} and is \sara{generally} applied 
%in 
\sara{to}
an intermediate encoder layer 
%(in our case, the 8th out of 12). 
\sara{(8\textsuperscript{th} in our case).}
%\mn{[Magari esplicitare che Liu e Gaido hanno sperimentato l'uso di CTC in Transformer-based (NON conformer-based architectures). So che per voi e' ovvio ma temo che per il lettore non lo sia...se non lo dite, egli si chiedera': ``Ah, ma quindi esistono versioni del Conformer non bacate?!!??''.]}
\citet{liu2020bridging} \sara{and} \citet{gaido-etal-2021-ctc} demonstrated that this method brings limited gains in terms of %translation 
\sara{output} quality \sara{in the Transformer architecture}, while it greatly reduces the training and inference costs. In this section, we analyze the effect of 
%adopting 
\sara{applying}
this technique 
%in 
\sara{to}
the Conformer architecture 
%both with all bugs and without them.
\sara{implemented on both the correct codebase (\smallcorrect) and on that with all bugs present (\bugall).}

\begin{table}[htb]
%\setlength{\tabcolsep}{4pt}
    \centering
    \small
    \begin{tabular}{l|c|ccc}
    \specialrule{.1em}{.05em}{.05em} 
        \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Code}} & \multicolumn{3}{c}{\textbf{IBS}} \\
        \cline{3-5}
         & & 1 & 10 & 100 \\
        \specialrule{.1em}{.05em}{.05em} 
        Conformer & \multirow{2}{*}{\correct} & 10.52 & 10.52 & 10.52 \\
        \hspace{0.5em}+ CTC Compr. & & 10.64 & 10.64 & 10.64 \\
        \hline
        Conformer & \multirow{2}{*}{\bugall} & 11.32 & 14.25 & 54.56 \\
        \hspace{0.5em}+ CTC Compr. & & 10.39* & \textbf{10.34}* & 10.81* \\ 
    \specialrule{.1em}{.05em}{.05em} 
    \end{tabular}
    \caption{WER \mg{($\downarrow$)} on the transcripts of MuST-C en-es tst-COMMON based
    %on the correct/incorrect codebase of the Conformer model
    \mg{of the Conformer implementations with (\bugall) and without (\smallcorrect) bugs}
    (with and without CTC compression) as IBS varies (1, 10, and 100). * indicates that the improvement given by the CTC compression is statistically significant\mg{, computed with bootstrap resampling (95\% CI).}
    %(computed with bootstrap resampling with 95\% confidence interval).}
    }
    \label{tab:ASR}
\end{table}

\begin{table*}[!htb]
\setlength{\tabcolsep}{6pt}
    \centering
    %\footnotesize
    \small
    \begin{tabular}{c|c|c||c|c|c|c|c|c|c|c||c}
    \specialrule{.1em}{.05em}{.05em} 
        \textbf{Code} & \textbf{Model} & \textbf{IBS} & \textbf{en-de} & \textbf{en-es} & \textbf{en-fr} & \textbf{en-it} & \textbf{en-nl} & \textbf{en-pt} & \textbf{en-ro} & \textbf{en-ru} & \textbf{Avg} \\
        \specialrule{.1em}{.05em}{.05em} 
        \multirow{6}{*}{\correct} & \multirow{3}{*}{Conformer} & 1 & \multirow{3}{*}{24.67} & \multirow{3}{*}{30.34} & \multirow{3}{*}{36.22} & \multirow{3}{*}{25.73} & \multirow{3}{*}{30.04} & \multirow{3}{*}{\textbf{30.55}} & \multirow{3}{*}{23.43} & \multirow{3}{*}{17.29} & \multirow{3}{*}{27.28} \\
        %\cline{3-11}
        & & 10 & & & & & & & & \\
        %\cline{3-11}
        & & 100 & & & & & & & & \\
        \cline{2-12}
        & \multirow{3}{*}{\shortstack{Conformer\\+\\CTC Compr.}} & 1 & \multirow{3}{*}{24.97} & \multirow{3}{*}{30.48} & \multirow{3}{*}{\textbf{36.43}} & \multirow{3}{*}{\textbf{26.25*}} & \multirow{3}{*}{\textbf{30.31}} & \multirow{3}{*}{30.09\textsuperscript{$\dagger$}} & \multirow{3}{*}{\textbf{24.67*}} & \multirow{3}{*}{\textbf{17.35}} & \multirow{3}{*}{\textbf{27.57}} \\
        %\cline{3-11}
        & & 10 & & & & & & & & \\
        %\cline{3-11}
        & & 100 & & & & & & & & \\
        \hline
        \multirow{6}{*}{\bug\textsubscript{1,2,3}} & \multirow{3}{*}{Conformer} & 1 & 24.68 & 28.57 & 35.70 & 25.81 & 29.68 & 30.22 & 23.52 & 15.83 & 26.75 \\
        %\cline{3-11}
        & & 10 & 24.58 & 27.81 & 35.65 & 25.70 & 29.35 & 30.02 & 23.43 & 15.36 & 26.49 \\
        %\cline{3-11}
        & & 100 & 23.23 & 21.15 & 31.70 & 23.42 & 24.92 & 27.72 & 22.68 & 11.05 & 23.23 \\
        \cline{2-12}
        & \multirow{3}{*}{\shortstack{Conformer\\+\\CTC Compr.}} & 1 & 24.95 & 30.49* & 36.27* & 25.84 & 29.42 & 30.04 & 23.96* & 17.05* & 27.25 \\
        %\cline{3-11}
        & & 10 & 25.21* & \textbf{30.72}* & 36.18* & 26.01 & 29.64 & 30.14 & 23.95* & 17.06* & 27.36 \\
        %\cline{3-11}
        & & 100 & \textbf{25.26}* & 30.52* & 36.36* & 25.88* & 29.66* & 30.16* & 23.92* & 16.87* & 27.33 \\
    \specialrule{.1em}{.05em}{.05em} 
    \end{tabular}
    \caption{BLEU ($\uparrow$) over all the 8 languages of MuST-C tst-COMMON v1.0 based on the correct/incorrect codebase of the Conformer model (with and without CTC compression) as IBS varies (1, 10, and 100). * and \textsuperscript{$\dagger$} indicates that, respectively, the improvement or the degradation given by the CTC compression is statistically significant\mg{, computed with bootstrap resampling (95\% CI).}
    %(computed with bootstrap resampling with 95\% confidence interval).}
    }
    \label{tab:ST}
\end{table*}


\paragraph{ASR}
Table \ref{tab:ASR} shows the effects \sara{on the ASR performance of the introduction of the CTC compression to both codebases \bugall{} and \smallcorrect.}
%on the ASR performance of introducing the CTC compression both with and without the bugs. 
We can notice that
%without the bugs, 
the CTC compression causes a small 
% \sara{(+0.12 WER)}
%(
and not statistically significant
%) 
performance degradation
(+0.12 WER)
\sara{when the correct implementation \smallcorrect{} is used (in accordance with the findings of \citealt{gaido-etal-2021-ctc} on the Transformer architecture).}
%, as the WER increases by 0.12 (in accordance with the findings of \citealt{gaido-etal-2021-ctc} on the Transformer architecture). 
%When the bugs are introduced, 
\sara{When bugs are present in the codebase,}
instead, the conclusion is completely overturned: the CTC compression brings statistically significant improvements (nearly -1 WER even with 1 as IBS). In addition, the best overall result is obtained with 
%\sara{10 as IBS and}
%the 
%CTC compression \mg{and the \bugall{} codebase}
the \bugall{} codebase (with 10 as IBS and CTC compression)\sara{, leading to the conclusion that the introduction of CTC Compression results in significant performance improvements when, in reality, this is not true.}
%\mn{[A voi sembrera' banale/inutile, ma io espliciterei quindi il ``misleading finding'' a cui si giunge a causa dei bachi (la CTC porta anche a miglioramenti di prestazioni e non solo di riduzione dei costi, giusto? Quindi si sopravvaluta la CTC?)]}
Therefore, we can conclude that \textit{\textbf{building on incorrect code can produce misleading findings}},
%and all the bugs
%\sara{implemented in the \bugall{} codebase.}
%(and 10 as IBS). 
%As such, not only we can reiterate 
%\sara{Therefore, we can affirm that}
%, but we also showed that 
%\sara{\textit{\textbf{good results not only can hide incorrect codebases but also lead to misleading findings}} since the}
%\textit{\textbf{building on incorrect code can produce misleading findings}}, 
\sara{since the introduction of bugs does not necessarily degrade the results.}
%and the introduction of noise \mn{[Sorry ma non capisco: cos'e' questo ``noise''?]} at inference time does not necessarily degrade the results (as the padding introduced with 10 as IBS improves the performance with respect to 1 as IBS). 
%\mn{[Siamo in un altro punto ``All in all'': limare al massimo e far brillare il messaggio. Quest'ultima frase e' una chiusa un po' criptica a non so quanto efficace. Il passaggio si dovrebbere chiudere con qualcosa di splendente su cosa significa avere bachi che portano a misleading findings. Ora, invece, recepisco la chiusa allo stesso modo in cui recepisco il testo della canzone di Madame a Sanremo ;-)]}
%\sara{compared to that obtained with IBS 1).}





\paragraph{ST}
Table \ref{tab:ST} reports the same analysis on the 8 language pairs of MuST-C v1.0. As in ASR, the presence of 
%the 
bugs (\bugall) unfairly rewards the CTC compression mechanism, which achieves statistically significant gains 
\sara{with IBS 100 
%among
on
all the languages and also on (at least) 4 out of 8 languages with IBS 1 or 10.}
%
%
%on at least 4 out of 8 language pairs at every IBS and on all languages with 100 sentences as IBS. 
With the 
%correct code \mn{[
bug-free version
%of Conformer?]}
\smallcorrect, instead, the improvements are 
%only 
statistically significant only on two language pairs (en-it, and en-ro), while on en-pt there is a statistically significant degradation. 
%Interestingly, en-it is one of the only two languages where the improvement is significant with the correct code, while is among the languages where the improvements are statistically significant only with 100 as IBS with the bugs. 
On average 
%over 
on all \sara{the} language directions, the improvements
%brought 
\sara{achieved}
by the CTC compression \sara{on codebase \bugall{}} range from 0.5 BLEU 
%(IBS=1) 
\sara{(with IBS 1)}
to 4.1 BLEU 
%(IBS=100) 
\sara{(with IBS 100)}
%in the presence of the bugs 
while 
%the gain 
is only \sara{of} 0.29 BLEU with 
the correct  code (\smallcorrect). 
%\sara{codebase~\smallcorrect.}
Moreover, the best scores for en-de and en-es
%(which curiously are the most widespread benchmarks) 
are \sara{obtained with CTC Compession on the buggy code (\bugall).}
%achieved with the presence of the bugs.
\sara{We can hence confirm that the use of incorrect codebases can lead to misleading findings because, also 
%for
in
the ST task, the CTC compression seems to bring statistically significant gains while, instead, these gains are not 
%real \mn{[not real?]} 
\sara{verified}
on a correct codebase.
This further demonstrates that \textit{\textbf{it is impossible %\mn{[Eccessivo? E' inappropriato, puo' essere molto difficile,...ma proprio impossibile?]} 
to assess code correctness only by looking at the results}} since, for instance, the average performance gap between \bugall{} and \smallcorrect{} codebases can be as little as 0.21 BLEU (when IBS is 10) and can further be narrowed, or even overturned, with a simple IBS \enquote{tuning}.}

%We can hence confirm on the ST task the finding of the previous ASR section, i.e. that \textit{\textbf{building on incorrect code can produce misleading findings}}, as the presence of bugs may lead to erroneously concluding that the CTC compression brings statistically significant gains on most of the language pairs without degrading on any of them, while this is not true. In addition, it further demonstrates that is virtually \textit{\textbf{impossible to assess code correctness from the results}}, as, for instance, the difference between correct and buggy code with CTC compression can be as little as 0.21 BLEU (with IFS=10) on average over 8 language pairs, and we cannot exclude that a ``tuning'' of the IBS could further narrow the gap (or even overturn it).
