
Conferences and journals rely on peer reviews to enforce the quality of the published material \citep{Ziman1968-ZIMPKA,ravetz1971scientific,meadows-1974-science}.
%However, previous studies demonstrated that there is high inconsistency in the judgment of a paper between reviewers \citep{smith-2006-peer-review,Herron2012}, especially when it comes to detecting methodological flaws \citep{peters_ceci_1982,Schroter673}.
The technical correctness of a paper is evaluated in the peer-review process and in the research community in general by assessing the soundness of the hypothesis and conclusions, by trying to identify methodological flows in the experimental settings, and by comparing the results with other works. Specifically, when a paper proposes a new algorithm in our NLP community, assessing the correctness of the algorithm and its code 
%For new code or algorithms, correctness may
``mean establishing consistency of results versus existing implementations, standard benchmarks, or sanity checks via statistically significant experimental results'' \citep{Rozier-2014-repro}.

In this process, the actual functioning of the code is never assessed. As a result, despite the increase in the number of papers for which the code, data, and models are released \citep{piwowar2011shares,10.1145/3442188.3445922}, their reproducibility is still an issue \citep{arvan-etal-2022-reproducibility-code}, demonstrating that open sourcing the artifacts related to a paper is not enough \citep{Chen2019}. As reproducibility is a cornerstone of science, many scientific organizations and, associated with them, journals and conferences such as NeurIPS\footnote{\url{https://neurips.cc/Conferences/2021/PaperInformation/PaperChecklist}}, and AAAI\footnote{\url{https://aaai.org/Conferences/AAAI-22/reproducibility-checklist/}}, as well as the entire ACL comunity\footnote{\url{https://aclrollingreview.org/responsibleNLPresearch/}}, require the completion of checklists that aim at attesting (and improving) the reproducibility of the released material \citep{dodge-etal-2019-show,pineau2021improving,rogers-etal-2021-just-think}.


However, reproducibility ``does not guarantee the quality, correctness, or validity of the published results'' \citep{Peng-2011-reproducible}, and, as we demonstrate in $\S$\ref{sec:case-study}, good results do not imply that the code is \textit{correct}, i.e. it performs what it is stated in a paper.
Even worse, when a work achieves interesting results and findings and is easily reproducible, although the code is not correct, building future research on them is facilitated and could lead to other incorrect conclusions \citep{McCullough-2008-replicable}, as we also show in our case study ($\S$\ref{sec:case-study}).

%, nor its reproducibility (or replicability) does, as it ``does not guarantee the quality, correctness, or validity of the published results'' \citep{Peng-2011-reproducible}. Even worse, when interesting (but incorrect) findings can be reproduced, building future research on them is facilitated \citep{McCullough-2008-replicable}.

In this paper, as a countermeasure to the problems mentioned above and a method to increase confidence in current findings and future research directions based on them, we posit that code correctness should be assessed on its own and promote the adoption of best practices of software quality assurance (SQA) in the research community. Our contributions can be summarized as follows.


\begin{itemize}[leftmargin=12pt]
    \setlength\itemsep{-3pt}
    \item We illustrate that the code correctness of a paper is currently given for granted from the quality of the results in the peer-review process, while we posit that it should be assessed independently from them, as in the case of reproducibility (\S\ref{sec:core-idea});
    \item Using the open source implementations of the widespread Conformer architecture \citep{gulati20_interspeech} as a case study, we demonstrate that:
    \begin{itemize}[leftmargin=6pt]
    \item[-] all implementations contain at least one bug related to padding handling (\S\ref{subsec:analysis});
    \item[-] even in the presence of these bugs, it is possible to achieve reproducible results, which compare favorably to other architectures and works on different tasks and language pairs (\S\ref{subsec:impact_bug});
    \item[-] the presence of bugs can lead to incorrect findings when introducing a new technique in the architecture (\S\ref{subsec:impact_code}).
    \end{itemize}
    \item We will release open source our Conformer implementation, the first robust to padding, as well as all pre-trained models, and outputs upon paper acceptance.
    \item We suggest a checklist of best practices to follow with the goal of enforcing the correctness of the code (\S \ref{sec:checklist}).
\end{itemize}



