As a proof of concept, \mg{we study the open source implementation of the Conformer \citep{gulati20_interspeech} architecture, which is the state-of-the-art solution for speech processing tasks such as automatic speech recognition (ASR) and speech-to-text translation (ST).}
%we choose to study the current state-of-the-art architecture Conformer \citep{gulati20_interspeech} for Speech Recognition and Translation.
After a brief description of 
%the concepts of Speech Recognition and Translation
\mg{the ASR and ST tasks and the Conformer architecture}
(Section \ref{subsec:speech_background}), we analyze
%all the most popular open source codebases available for Conformer 
\mg{several open source codebases that are widely used in research and contain a Conformer implementation, showing}
%and show
that all the analyzed repositories contain at least one bug (Section \ref{subsec:analysis})\mg{. Through extensive experiments on the two tasks and on all the language pairs of the MuST-C corpus \cite{CATTONI2021101155}, we demonstrate that the presence of bugs can be hidden by good results (Section \ref{subsec:impact_bug}), while it can lead to wrong findings when building new solutions on the same codebases (Sections \ref{subsec:impact_code}). }
%, and we demonstrate through extensive experiments that these bugs can lead to wrong findings, which can be hindered by good results (Sections \ref{subsec:impact_code}, and \ref{subsec:impact_bug}).
All the experimental settings, including model architecture, data, and training parameters, are described in Appendix \ref{sec:exp_sett}.

\subsection{Background}
\label{subsec:speech_background}
%Automatic speech recognition (ASR)
\mg{ASR} is the task in which an audio containing speech content is transcribed in its original language while
%in the speech translation (ST) task
\mg{ST}
this content is translated into a different language. In the last decade, traditional approaches for ASR relying on Gaussian Mixture Models \citep{gmm} have been replaced by deep neural models \citep{hinton-2012-asr} and then by an end-to-end or direct model \citep{pmlr-v32-graves14,Chorowski-2014-asr} capable of performing the ASR task within a single model.
A similar progression has taken place in the field of ST, where cascade architectures consisting of an ASR model followed by a machine translation (MT) model \citep{cascade,cascade2} have been recently replaced by direct models \citep{berard_2016,weiss2017sequence}. Such direct models do not leverage intermediate representations to perform the task like in the cascade solutions and all their parameters are jointly optimized for ST. 
Current direct architectures employed to perform both ASR and ST tasks are based on Transformer \citep{NIPS2017_3f5ee243} and have been adapted to work with audio inputs \citep{8462506,gangi19_interspeech} \mg{by introducing two convolutional layers that shrink the length of the input sequence by a factor of 4}. Among these, Conformer \citep{gulati20_interspeech}\mg{, which modifies the structure of the encoder layers with respect to the Transformer, has brought significant improvements both in ASR and in ST \citep{inaguma2021non}.}
% represents the current state of the art
%, with significant improvements compared to the vanilla Transformer architectures \citep{inaguma2021non}.
%For this reason, we adopt the Conformer-based architecture for both tasks for the empirical part of this paper.
%Therefore, in the empirical section of this paper, we adopt the Conformer-based architecture for both ASR and ST tasks.

\mg{The changes introduced in the Conformer encoder layer can be summarized as follows:
\textit{i)} relative sinusoidal positional encoding \citep{dai-etal-2019-transformer} are introduced in the self-attention for improved generalization with respect to varying input lengths;
\textit{ii)} the FFN sublayer is replaced by two FFNs that wrap the self-attention, inspired by the Macaron-Net~\citep{lu-et-al-2016-macaron-net};
\textit{iii)}~a convolutional module (depicted in Figure \ref{fig:conf_conv}) is added immediately after the self-attention, before the last FFN module.
The convolutional module is wrapped in a residual connection.
%, and, after
After a layer normalization, a pointwise convolution transforms each feature vector representing a time step in the same way 
%with a pointwise convolution that
and doubles the size of the features. The feature dimension is brought to the original size by the Gated Linear Unit (GLU) activation function~\citep{Dauphin-2017-glu}.
Then, a depthwise convolution with 31 kernel size is applied before a batch normalization \citep{ioffe-2015-batchnorm}, the Swish activation function \citep{swish-2017}, and another pointwise convolution similar to the first one.
At last, a dropout module \citep{Srivastava-2014-dropout} randomly masks (i.e. zeroes out) a percentage of the values to prevent the network from overfitting.}



\subsection{Analysis of the Codebases}
\label{subsec:analysis}

\mg{We evaluated the behavior of the open-source implementation of the Conformer with respect to a parameter that should not change the results: the inference batch size. When samples of different length are collected in the same batch, a common situation in speech tasks where the length of the input sentence greatly varies, the input sequence have to be brought to the same dimension by filling them with padding. By doing so, the computation on GPU is more efficient, because several sentences are processed in parallel and, in theory, results should not change as the padding should be a filler that is ignored. For this reason, research papers often include details only about the training batch size, which is an important hyperparameter for the stability of the training, while the inference batch size is not reported, as it should not affect the results.}

\mg{Our analysis focused on six open source implementations, belonging to widely adopted codebases, namely:}
%
%To conduct our study about the presence of incorrectness in the codebases focusing on the state-of-the-art Conformer architecture, we collected the most used repository for ASR and ST, namely: 
Fairseq-ST \citep{wang2020fairseqs2t}, ESPnet-ST \citep{inaguma-etal-2020-espnet}, NeMo \citep{kuchaiev2019nemo}, SpeechBrain \citep{speechbrain}, an open source
%(Apache-2.0) 
implementation on github\footnote{\url{https://github.com/sooftware/conformer}} named \enquote{Conformer}, and the official Pytorch implementation from TorchAudio \citep{yang2021torchaudio}.
\mg{We discovered that all these implementations returned different results with different inference batch size, showing that the presence of padding was (wrongly) altering the results.}\footnote{We would like to emphasize that \mg{our intention is not to blame any specific codebase nor its developers. Conversely,} we are extremely thankful for the \mg{important and worthy} contribution of these open source libraries to our community. Our analysis is not intended to point out the issues of the single libraries but is intended to further improve the correctness of codes and, consequently, of the experimental results which we believe is of utmost importance.}
We inspected the code relative to the Conformer architecture and we isolated three types of bugs:



\begin{figure}[!tb]
\centering
\renewcommand*\thesubfigure{\arabic{subfigure}} 
     \begin{subfigure}[b]{0.46\textwidth}
        \centering
         \includegraphics[width=0.45\textwidth]{img/start.png}
         \caption{Before shifting, the Relative PE matrix ($P_{00},...,P_{22}$) is padded (zero values).}
     \end{subfigure}
     %\par\medskip
     \begin{subfigure}[b]{0.46\textwidth}
        \centering
         \includegraphics[width=0.45\textwidth]{img/wrong.png}
         \caption{When relative shift is applied to the Relative PE matrix without considering padding, %\smallbug\textsubscript{3}, 
         some values ($P_{11}, P_{12}, P_{20}, P_{21}$) are moved to the padding part (in \textcolor{red}{\textbf{red}}), hence are not considered in the following computation, while some padding is instead incorrectly considered in the following computation (in \textcolor{teal}{\textbf{green}}). Please notice that the first row is always discarded.}
     \end{subfigure}
     \begin{subfigure}[b]{0.46\textwidth}
        \centering
         \includegraphics[width=0.45\textwidth]{img/correct.png}
         \caption{When relative shift is applied to the Relative PE matrix considering padding, all the values $P_{00},...,P_{22}$ are considered in the computation and are never moved to the padded part. Please notice that the first row is always discarded.}
     \end{subfigure}
    \caption{Example of relative shift operation starting from a Relative PE matrix containing padding (1), both considering a codebase with \smallbug\textsubscript{3} (2) and without (3) bug.}
    \label{fig:relativePEs}
\end{figure}

\begin{enumerate}
    \item[\bugone] \textbf{Convolution Module Bug:}
    \mg{the depthwise and pointwise convolutions of the Conformer convolutional module do not consider the presence of padding and cause the padding area to contain non-zero values (mutated from the learned \textit{bias} element of the convolution module and from the adjacent input). These values alter the behavior of the batch normalization and of the other convolutions, leading to alteration also of the real values that do not fall in the padding area.}
    %in the Convolution Module of the Conformer Encoder, a series of gated Convolutional Layers \citep{pmlr-v70-dauphin17a} is applied to the input together with a Batch Normalization Layer. Since the output of the Convolutional Layers does not preserve padding, the final output obtained after the Convolution Module is not padded anymore. Also, 
    %%the 
    %Batch Normalization 
    %%Layer 
    %\citep{pmlr-v37-ioffe15} is applied to an input that is not padded. However, since input data is divided into batches during both training and inference of the model, and padding is consequently applied to fit more samples in a single batch, preserving the padding information is important to avoid incorrect computation.
    \item[\bugtwo] \textbf{Initial SubSampling Bug:} 
    \mg{the two initial convolutions that subsample the input audio sequence by a factor of 4 do not consider padding. For this reason, the second convolution is fed with non-zero values in the padding area close to the end of the valid sequences. These (dirty) values contribute to the computation of the last valid elements of the resulting sequence, altering the correct result.}
    %speech inputs are $\sim$10 longer than textual inputs, therefore an initial input dimension reduction is necessary to train an ASR/ST model due to the squared memory complexity on the input length of the attention operation present in all the current architectures \citep{gangi19_interspeech,wang2020fairseqs2t,inaguma-etal-2020-espnet}. For this reason, a block of convolutions with a striding factor $\geq2$ is applied to the input before feeding it to the Conformer Encoder \citep{vyas21_interspeech}, realizing a sub-sampling of the sequence. However, the convolution operation does not preserve padding, meaning that the second convolution operation is applied to a non-padded input and so on. 
    %This represents a problem since batches are used for both training and inference, and preserving padding between operations is important.
    %This constitutes an issue, as batches are utilized for both training and inference, and maintaining padding between operations is crucial.
    \item[\bugthree] \textbf{Positional Encodings Bug:} 
    \mg{the relative sinusoidal positional encodings (PEs) are added to the attention matrix and are computed by shifting a sinusoidal matrix. This shifting operation first prepends a zero column to the sinusoidal matrix and then reshapes the matrix so that the last element of the first row becomes the first element of the second, the last two elements of the second row become the first of the third row and so on. By doing this, this operation assumes that all elements are valid. However, when a sentence is padded, only a part of the attention matrix is valid (e.g., see the green area in Figure \ref{fig:relativePEs}.1), so what is moved to the beginning of the next row is not the correct value. In Figure \ref{fig:relativePEs}, for the sake of clarity of the example, we pretend that existing implementations set to 0 the PE in the padding area. While this is not what happens in practice (as the padding area contains other sinusoidal PE), it shows that the correct values are discarded and the final matrix does not contain the correct values.}
    %in the Conformer Encoder attention, relative positional encodings (PEs) have been introduced in place of the absolute ones thanks to their improved performance in encoding information \citep{dai-etal-2019-transformer}. Relative PEs are dynamically calculated during training and, in their computation, the sinusoid encoding matrix \citep{NIPS2017_3f5ee243} is relatively shifted regardless of the padding (see Figure \ref{fig:relativePEs}), i.e. some "padding" is shifted together with the relevant information, thus causing an error in the final PEs. 
\end{enumerate}




In Table \ref{tab:bug}, we report the presence (or absence) of one or more of these bugs for each repository in its current version.\footnote{\mg{Checked on January 8th 2023.}}

\begin{table}[ht]
    \centering
    \setlength{\tabcolsep}{2pt}
    \begin{tabular}{l|c|c|c}
    \specialrule{.1em}{.05em}{.05em} 
         \textbf{Repository} & \textbf{Conv. Mod.} & \textbf{SubSampl.} & \textbf{Pos. Enc.} \\
         \hline
         Fairseq-ST & \bug & \bug & \bug \\
         ESPnet-ST & \bug & \bug & \bug \\
         NeMo & & \bug & \bug \\
         SpeechBrain & \bug & \bug & \bug \\
         Conformer & \bug & \bug & \bug \\
         Pytorch & \bug & NA & NA \\
    \specialrule{.1em}{.05em}{.05em} 
    \end{tabular}
    \caption{Presence of bug(s) \smallbug{} in the codebase for each one of the analyzed repositories. NA stands for \enquote{Not Applicable}.}
    \label{tab:bug}
\end{table}

%All the public implementations we analyzed contain at least one bug. 
Noticeably, all the implementations but one (NeMo) are affected by the Convolution Module Bug in the Conformer encoder. Moreover, all the implementations are affected by bugs in the initial SubSampling module and in the relative PEs computation, except for the implementation by Pytorch. The latter does not make use of relative PEs in the attention and replaces the initial sub-sampling convolutional layers with linear layers that map the input sequence in matrices of fixed dimensions. Therefore, we can conclude that all the public implementations we analyzed contain at least one bug.

\paragraph{The Case of TF32}
\mg{Another issue that potentially causes unexpected differences when padding is introduced is not related to the code but}
%We also identify a problem affecting the computation that is not directly related to the implementation itself but is relative 
to the use of NVIDIA Ampere GPUs (A40/A100) during training/inference.
\mg{By default, on these GPUs PyTorch backend computes convolutions and matrix multiplications with TensorFloat-32\footnote{\url{https://pytorch.org/docs/stable/notes/cuda.html\#tensorfloat-32-tf32-on-ampere-devices}.} (TF32) cores that speed up the computation but reduce precision. Numeric errors can cause small but random fluctuations in presence of padding. We explore also the impact of this aspect by}
%For these GPUs, the computation is executed in TensorFlow-32 (TF32) precision by default, speeding up the calculation but reducing precision.\footnote{\url{https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices}.}
%\footnote{\url{https://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_tensor_float_32_execution}} 
%Hence, computations executed with and without single precision enabled can produce impacting differences in the results. We further explore this aspect by 
enabling and disabling TF32 during both training and inference.



\subsection{Impact of Incorrect Codebases}
\label{subsec:impact_code}
To assess the impact of code bugs on the findings that can be drawn from experiments, we analyze the effect of the presence or the absence of all the bugs described in Section \ref{subsec:analysis} on the results. 

\paragraph{ASR} Table \ref{tab:ASR} shows the ASR performance variations of the Conformer model with and without bugs in the code as the batch size varies. It is important to notice that the results \textbf{should not} theoretically vary when the batch size varies. 
However, the results do vary in the case of incorrect code (\smallwrong{}): the WER increases (thus, gets worse) from 11.3 to 14.3 when the batch size increases by an order of magnitude (from 1 to 10), and this effect is even exacerbated when we further increase the batch size to 100, with a performance drop of 43.3 WER. 
This indicates catastrophic behavior when the batch size is sufficiently high (it approaches 100).
Interestingly, the best result is obtained when the incorrect code is used in the configuration Conformer + CTC Compression, suggesting that the presence of the CTC compression not only prevents the results from changing as the batch size varies but also benefices from an additional noise introduced by the presence of the bugs. In any case, it is important to notice that the differences between the mentioned configuration and the configurations with the correct code (\smallcorrect{}) are minimal. More importantly, if we limit to drawing conclusions by only looking at the incorrect code (\smallwrong{}), it emerges that the use of the CTC compression is extremely helpful for the ASR task since it reduces the WER by at least 0.9 points. However, this finding is completely wrong since the introduction of CTC does not affect significantly the results when the correct code (\smallcorrect{}) is used. Therefore, we can conclude that the presence of bugs in the code has a huge impact on the final results and can lead to wrong findings.

\begin{table}[htb]
%\setlength{\tabcolsep}{4pt}
    \centering
    \small
    \begin{tabular}{l|c|ccc}
    \specialrule{.1em}{.05em}{.05em} 
        \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{\shortstack{Correct\\Code}}} & \multicolumn{3}{c}{\textbf{WER} $\downarrow$} \\
        \cline{3-5}
         & & 1 & 10 & 100 \\
        \specialrule{.1em}{.05em}{.05em} 
        Conformer & \multirow{2}{*}{\wrong} & 11.3 & 14.3 & 54.6 \\
        \hspace{0.5em}+ CTC Compr. & & \textbf{10.4} & \textbf{10.4} & \textbf{10.4} \\
        \hline
        Conformer & \multirow{2}{*}{\correct} & 10.5 & 10.5 & 10.5 \\
        \hspace{0.5em}+ CTC Compr. & & 10.6 & 10.6 & 10.6 \\
    \specialrule{.1em}{.05em}{.05em} 
    \end{tabular}
    \caption{ASR performance on MuST-C tst-COMMON based on the correct/incorrect codebase of the Conformer model (with and without CTC compression) as the batch size varies (1, 10, and 100 sentences).}
    \label{tab:ASR}
\end{table}

\paragraph{ST} To show the impact of the bugs in the Conformer model on the ST task as the batch varies, we report in Table \ref{tab:ST} the results for each language pair of MuST-C v1.0.
First of all, we notice that the undesired degradation effect of the output quality in the presence of bugs (\smallwrong{}) is evident also for ST: the BLEU decreases (thus, gets worse) from 26.75 to 23.23 (-3.52 BLEU) when passing from 1 to 100 sentences per batch in the case of Conformer configuration while it is only degraded by 0.02 BLEU in the case of Conformer + CTC Compression. 
Similar to the phenomenon observed for the ASR task, the use of CTC compression helps at attenuating the effect of the bugs in most of the cases, while also scoring the best result for a couple of language pairs (en-de, and en-es). 
%This suggests again that the CTC compression mechanism 
However, conversely from the ASR case, the Conformer + CTC Compression configuration is on average slightly better (+0.29 BLEU) than the Conformer one even when the correct code (\smallcorrect{}) is used. However, these gains are not as large as the results of the incorrect code case (\smallwrong{}) would lead us to believe. In fact, if we look at the Conformer results with 100 as batch size and we compare them with the Conformer + CTC Compression results for the same batch size, we observe an average improvement of 4.1 BLEU points that is completely misleading from the true BLEU gains observed for the correct code (\smallcorrect{}). Again, we can conclude that the presence of bugs in the code can lead to completely wrong findings and can hinder the true behavior of some of the elements of the architecture as the CTC compression mechanism.


\begin{table*}[!htb]
\setlength{\tabcolsep}{5pt}
    \centering
    %\footnotesize
    \small
    \begin{tabular}{c|c|c||c|c|c|c|c|c|c|c||c}
    \specialrule{.1em}{.05em}{.05em} 
        \multirow{2}{*}{\textbf{\shortstack{Correct\\Code}}} & \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{\shortstack{Batch\\Size}}} & \multirow{2}{*}{\textbf{en-de}} & \multirow{2}{*}{\textbf{en-es}} & \multirow{2}{*}{\textbf{en-fr}} & \multirow{2}{*}{\textbf{en-it}} & \multirow{2}{*}{\textbf{en-nl}} & \multirow{2}{*}{\textbf{en-pt}} & \multirow{2}{*}{\textbf{en-ro}} & \multirow{2}{*}{\textbf{en-ru}} & \multirow{2}{*}{\textbf{Avg}} \\
        & & & & & & & & & & \\
        \specialrule{.1em}{.05em}{.05em} 
        \multirow{6}{*}{\wrong} & \multirow{3}{*}{Conformer} & 1 & 24.68 & 28.57 & 35.70 & 25.81 & 29.68 & 30.22 & 23.52 & 15.83 & 26.75 \\
        %\cline{3-11}
        & & 10 & 24.58 & 27.81 & 35.65 & 25.70 & 29.35 & 30.02 & 23.43 & 15.36 & 26.49 \\
        %\cline{3-11}
        & & 100 & 23.23 & 21.15 & 31.70 & 23.42 & 24.92 & 27.72 & 22.68 & 11.05 & 23.23 \\
        \cline{2-12}
        & \multirow{3}{*}{\shortstack{Conformer\\+\\CTC Compr.}} & 1 & 24.95 & 30.49 & 36.27 & 25.84 & 29.42 & 30.04 & 23.96 & 17.05 & 27.25 \\
        %\cline{3-11}
        & & 10 & 25.21 & \textbf{30.72} & 36.18 & 26.01 & 29.64 & 30.14 & 23.95 & 17.06 & 27.36 \\
        %\cline{3-11}
        & & 100 & \textbf{25.26} & 30.52 & 36.36 & 25.88 & 29.66 & 30.16 & 23.92 & 16.87 & 27.33 \\
        \hline
        \multirow{6}{*}{\correct} & \multirow{3}{*}{Conformer} & 1 & \multirow{3}{*}{24.67} & \multirow{3}{*}{30.34} & \multirow{3}{*}{36.22} & \multirow{3}{*}{25.73} & \multirow{3}{*}{30.04} & \multirow{3}{*}{\textbf{30.55}} & \multirow{3}{*}{23.43} & \multirow{3}{*}{17.29} & \multirow{3}{*}{27.28} \\
        %\cline{3-11}
        & & 10 & & & & & & & & \\
        %\cline{3-11}
        & & 100 & & & & & & & & \\
        \cline{2-12}
        & \multirow{3}{*}{\shortstack{Conformer\\+\\CTC Compr.}} & 1 & \multirow{3}{*}{24.97} & \multirow{3}{*}{30.48} & \multirow{3}{*}{\textbf{36.43}} & \multirow{3}{*}{\textbf{26.25}} & \multirow{3}{*}{\textbf{30.31}} & \multirow{3}{*}{30.09} & \multirow{3}{*}{\textbf{24.67}} & \multirow{3}{*}{\textbf{17.35}} & \multirow{3}{*}{\textbf{27.57}} \\
        %\cline{3-11}
        & & 10 & & & & & & & & \\
        %\cline{3-11}
        & & 100 & & & & & & & & \\
    \specialrule{.1em}{.05em}{.05em} 
    \end{tabular}
    \caption{ST performance based on the correct/incorrect codebase of the Conformer model (with and without CTC compression) as the batch size varies (1, 10, and 100 sentences). Results are measured by BLEU ($\uparrow$) and computed over all the 8 languages of MuST-C tst-COMMON v1.0.}
    \label{tab:ST}
\end{table*}


\subsection{Impact of Single Bugs}
\label{subsec:impact_bug}

To investigate the contribution of each bug to the final results, we perform an ablation study by first isolating the impact of TF32 and then by analyzing the performance drop after the introduction of each of the bugs \smallbug\textsubscript{1,2,3} described in Section \ref{subsec:analysis}. 

\paragraph{ASR}
The impact on the performance of the Conformer (without CTC compression) of TF32 and of the different bugs for the ASR task is shown in Table \ref{tab:ablationASR}.
First of all, the TF32 seems not to introduce relevant errors in the output computation as the batch size varies but it shows a WER gain of 0.21 (thus, the performance degrades) compared to the correct code (\smallcorrect{}). 
Differently, as soon as any of the bugs \smallbug\textsubscript{1,2,3} is introduced in the code, the performance begins to vary as the batch size varies. This phenomenon is more present in the case of bug \smallbug\textsubscript{1} for which we observe a major performance drop of 8.78 WER when we switch from 1 to 100 sentences per batch. Noticeably, even if the contributions of bugs \smallbug\textsubscript{2} and \smallbug\textsubscript{3} considered alone are limited (and we also achieve the best overall result when bug \smallbug\textsubscript{3} is introduced in the code and batch size 1 is used), 
if we introduce all the bugs \smallbug\textsubscript{1,2,3}, the performance drops consistently with up to 44.04 WER increase.
%In any case, except for the case in which bug \smallbug\textsubscript{3} is introduced in the code and batch size 1 is used (and for which we also achieve the best overall result), there is a performance drop of when at least one bug is introduced in the
All in all, we can conclude that some bugs are more influential than others for ASR as we found that the bug in the Convolution Module of the Conformer architecture \smallbug\textsubscript{1} has a major impact on the performance. Moreover, we also found that the effect of every single bug is additive, leading to worse results if they are introduced altogether.
%In fact, we observe a relative drop of 43.24 WER when we switch from 1 to 100 as the batch size. Moreover, there is a 0.8 WER performance drop between the use of correct code and the pr


\begin{table}[!htb]
\setlength{\tabcolsep}{14pt}
    \centering
    \footnotesize
    \begin{tabular}{l||ccc}
    \specialrule{.1em}{.05em}{.05em} 
        \multirow{2}{*}{\textbf{Bug}} & \multicolumn{3}{c}{\textbf{WER $\downarrow$}} \\
        \cline{2-4}
        & 1 & 10 & 100 \\
        \specialrule{.1em}{.05em}{.05em} 
        \smallcorrect & 10.52 & \textbf{10.52} & \textbf{10.52} \\
        \hline
        - TF32 & 10.73 & 10.73 & 10.73 \\
        \quad + \smallbug\textsubscript{1} & 10.72 & 11.25 & 19.50 \\
        \quad + \smallbug\textsubscript{2} & 10.73 & 10.74 & 10.74 \\
        \quad + \smallbug\textsubscript{3} & \textbf{10.46} & 10.62 & 10.73 \\
        \quad + \smallbug\textsubscript{1,2,3} & 11.32 & 14.25 & 54.56 \\
    \specialrule{.1em}{.05em}{.05em} 
    \end{tabular}
    \caption{Impact of the different types of bug \smallbug\textsubscript{1,2,3} and TF32 for the ASR task on MuST-C tst-COMMON as the batch size varies (1, 10, and 100 sentences).}
    \label{tab:ablationASR}
\end{table}

\paragraph{ST} For the ST task, we present the impact of TF32 and of the different bugs \smallbug\textsubscript{1,2,3} on two language directions of MuST-C v1.0, en-de, and en-es, having respectively different and similar word ordering with respect to the source language. Results are presented in Table \ref{tab:ablationST}.
Differently from the ASR case, there is no bug that has a prevalent role over the others, except for bug \smallbug\textsubscript{1} in en-es that shows a BLEU degradation from 0.81 to 2.63 points compared to the correct code (\smallcorrect{}) as the batch size increases. In all the other cases, the influence of every single bug seems marginal if considered alone but is exacerbated when all the bugs are inserted in the code (\smallbug\textsubscript{1,2,3}). Therefore, we can conclude that the additive effect of bugs on performance degradation does not depend on the particular task but is valid for both ASR and ST.


\begin{table}[!htb]
\setlength{\tabcolsep}{4pt}
    \centering
    \footnotesize
    \begin{tabular}{l||ccc|ccc}
    \specialrule{.1em}{.05em}{.05em} 
        \multirow{2}{*}{\textbf{Bug}} & \multicolumn{3}{c|}{\textbf{en-de}} & \multicolumn{3}{c}{\textbf{en-es}} \\
        \cline{2-7}
        & 1 & 10 & 100 & 1 & 10 & 100 \\
        \specialrule{.1em}{.05em}{.05em} 
        \smallcorrect & 24.67 & 24.67 & 24.67 & 30.34 & 30.34 & 30.34 \\
        \hline
        - TF32 & \textbf{24.84} & \textbf{24.84} & \textbf{24.83} & \textbf{30.63} & \textbf{30.62} & \textbf{30.63} \\
        \quad + \smallbug\textsubscript{1} & 24.52 & 24.65 & 24.67 & 29.53 & 29.41 & 27.71 \\
        \quad + \smallbug\textsubscript{2} & 24.56 & 24.57 & 24.58 & 30.53 & 30.53 & 30.53 \\
        \quad + \smallbug\textsubscript{3} & 24.53 & 24.46 & 24.42 & 30.33 & 30.35 & 30.24\\
        \quad + \smallbug\textsubscript{1,2,3} & 24.68 & 24.58 & 23.23 & 28.57 & 27.81 & 21.15 \\
    \specialrule{.1em}{.05em}{.05em} 
    \end{tabular}
    \caption{Impact of the different types of bug \smallbug\textsubscript{1,2,3} and TF32 for the ST task on MuST-C tst-COMMON as the batch size varies (1, 10, and 100 sentences). Results are measured by BLEU ($\uparrow$).}
    \label{tab:ablationST}
\end{table}

%% STATISTICAL SIGNIFICANCE OF THE RESULTS

%% SCORE ALTI NON SIGNIFICANO CODICE GIUSTO
