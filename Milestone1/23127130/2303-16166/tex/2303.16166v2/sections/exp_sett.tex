\section{Data Statistics}
\label{sec:data_stats}

%\begin{table}[!htb]
%    \centering
%    \small
%    \setlength{\tabcolsep}{2pt}
%    \begin{tabular}{l|cccccccc}
%    \specialrule{.1em}{.05em}{.05em} 
%        \textbf{Split} & \textbf{de} & \textbf{es} & \textbf{fr} & \textbf{it} & \textbf{nl} & \textbf{pt} & \textbf{ro} & \textbf{ru} \\      
%        \specialrule{.1em}{.05em}{.05em} 
%        train & 225.3 & 260.1 & 269.3 & 248.2 & 243.5 & 201.5 & 231.5 & 259.5 \\
%        dev & 1.4 & 1.3 & 1.4 & 1.3 & 1.4 & 1.4 & 1.4 & 1.3 \\
%        test & 2.6 & 2.5 & 2.6 & 2.6 & 2.6 & 2.5 & 2.6 & 2.5 \\
%    \specialrule{.1em}{.05em}{.05em} 
%    \end{tabular}
%    \caption{Number of samples (in thousands) of the train, dev, and test (tst-COMMON) sets for each language of MuST-C v1.0.}
%    \label{tab:data-stats}
%\end{table}


\begin{table}[!htb]
    \centering
    \small
    \setlength{\tabcolsep}{2pt}
    \begin{tabular}{l|cccccccc}
    \specialrule{.1em}{.05em}{.05em} 
        \textbf{Split} & \textbf{de} & \textbf{es} & \textbf{fr} & \textbf{it} & \textbf{nl} & \textbf{pt} & \textbf{ro} & \textbf{ru} \\      
        \specialrule{.1em}{.05em}{.05em} 
        train & 387.5 & 478.6 & 469.5 & 441.5 & 421.4 & 363.7 & 409.8 & 466.4 \\
        dev & 2.5 & 2.5 & 2.5 & 2.5 & 2.5 & 2.5 & 2.5 & 2.5 \\
        test & 4.1 & 4.1 & 4.1 & 4.1 & 4.1 & 4.1 & 4.1 & 4.1 \\
    \specialrule{.1em}{.05em}{.05em} 
    \end{tabular}
    \caption{Number of hours of the train, dev, and test (tst-COMMON) sets for each language of MuST-C v1.0.}
    \label{tab:data-stats}
\end{table}

\section{Architecture and Training Settings}
\label{sec:exp_sett}

%\subsection{Architecture and Training Settings}
Our Conformer-based architecture is composed of 12 Conformer \citep{gulati20_interspeech} Encoder layers and 6 Transformer \citep{NIPS2017_3f5ee243} Decoder layers, with 8 attention heads each. Embedding size is set to 512 and hidden neurons in the feed-forward layers to 2,048, with a total of 
%$\sim$115M
114,894,730
parameters for the model. Dropout is set to 0.1 for feed-forward, attention, and convolution layers. The kernel size of the Convolution Module is set to 31 for both point- and depth-wise convolutions. 
We train all the models using Adam \citep{journals/corr/KingmaB14} optimizer (betas $(0.9, 0.98)$) and label-smoothed cross-entropy (LSCE) loss (smoothing factor 0.1).
We also use an auxiliary Connectionist Temporal Classification or CTC loss \citep{Graves2006ConnectionistTC} during training to ease convergence and obtain competitive results without pre-training the encoder with that of an ASR model \citep{gaido-etal-2022-efficient}. The auxiliary loss is summed to the LSCE with 0.5 relative weight. 
The learning rate is set to $2\cdot10^{-3}$ with Noam scheduler \citep{NIPS2017_3f5ee243} and 25k warm-up steps. 
The vocabularies are based on SentencePiece models \citep{sennrich-etal-2016-neural} with size 5,000 for the English source \citep{inaguma-etal-2020-espnet} and 8,000 for each of the ST target languages \citep{di-gangi-etal-2020-target}. We set 100k maximum updates but we early stop training after 10 epochs without loss decrease on the dev set and average 5 checkpoints around the best (best, two preceding, and two following). All trainings are performed 
%on 2 NVIDIA A40 40GB of RAM, 
with 40k tokens as batch size and 4 as update frequency on two GPUs.
All other settings are the default of Fairseq-ST~\citep{wang2020fairseqs2t}, which we forked as a base of our implementation.
SpecAugment \citep{Park2019} is applied during training, while utterance-level Cepstral mean and variance normalization is performed both at training and inference time. 
%The implementation is a modified version of Fairseq-ST \citep{wang2020fairseqs2t} that is released at \url{[anonymous\_url]}.
Trainings lasted 18-33 hours depending on the model configuration (e.g., with the fixes or not, with or without CTC compression) and the language pair due to the different sizes of the training data.

