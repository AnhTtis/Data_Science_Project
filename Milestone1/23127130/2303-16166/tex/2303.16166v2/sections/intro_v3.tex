Conferences and journals rely on peer reviews to ensure the quality of published material \citep{Ziman1968-ZIMPKA,ravetz1971scientific,meadows-1974-science}. Currently, the peer-review process evaluates the technical correctness of a paper by examining the validity of the hypothesis and conclusions, identifying any flaws in the experimental design, and comparing results with other studies. Specifically, assessing the correctness of a new algorithm proposed in a paper and its code involves \enquote{\textit{establishing consistency of results versus existing implementations, standard benchmarks, or sanity checks via statistically significant experimental results}} \citep{Rozier-2014-repro}. 
%SHORTER ALTERNATIVE: However, the actual functioning of the code is never assessed during this process, leading to challenges in reproducing published papers even when their code, data, and models are released \citep{piwowar2011shares,10.1145/3442188.3445922}. 
However, the actual functioning of the code is never assessed during this process. As a result, the reproducibility of papers, despite the release of their code, data, and models \citep{piwowar2011shares,10.1145/3442188.3445922}, remains a challenge \citep{arvan-etal-2022-reproducibility-code,Chen2019} \mn{[NON SONO SICURO DI SEGUIRE LA LOGICA: QUESTA FRASE STRESSA TANTO IL TEMA DELLA RIPRODUCIBILITA', FACENDOLO DIVENTARE LA VOSTRA CHALLENGE (e nella frase dopo ``issue''). LIMEREI QUESTA FRASE CHE E' QUELLA CHE OFFRE LA CHIAVE DI LETTURA DEL PAPER. ERO RIMASTO AL FATTO CHE IL PAPER SAREBBE INIZIATO PARLANDO DELL'ENFASI SULLA RIPRODUCIBIITA', PER POI DIRE CHE QUESTA NON E' TUTTO MA CHE C'E' ANCHE LA CORRETTEZZA (in linea col titolo). QUI LE DUE COSE SONO MESCOLATE; DITE CHE IL NON CONTROLLARE LA CORRETTEZZA DIVENTA POI UN PROBLEMA DI RIPRODUCIBIITA'...POI, DOPO, DITE CHE PER GARANTIRE LA RIPROD CI SONO LE CHECKLIST...PER POI DIRE FINALMENTE CHE LA RIPRODUCIBIITA' NON E' TUTTO. UN PO' INGARBUGLIATO.]}. 

To mitigate this issue, numerous scientific organizations and associated journals and conferences, such as NeurIPS\footnote{\url{https://neurips.cc/Conferences/2021/PaperInformation/PaperChecklist}}, AAAI\footnote{\url{https://aaai.org/Conferences/AAAI-22/reproducibility-checklist/}}, and the ACL community\footnote{\url{https://aclrollingreview.org/responsibleNLPresearch/}}, have recently introduced the completion of checklists aimed at enhancing the reproducibility of published material \citep{dodge-etal-2019-show,pineau2021improving,rogers-etal-2021-just-think}. Nonetheless, reproducibility alone is not sufficient to \enquote{\textit{guarantee the quality, correctness, or validity of the published results}} \citep{Peng-2011-reproducible} as obtaining good results does not imply that the code is \textbf{correct}, i.e. it performs what it is stated in the paper. 
Even worse, when a work achieves interesting results and findings and is easily reproducible, although the code is not correct, building future research on them is facilitated and could lead to incorrect conclusions \citep{McCullough-2008-replicable}.
%, as we also show in our case study ($\S$\ref{sec:case-study}).
%In fact, if a work produces interesting findings and results that are easily reproducible but the code is incorrect, this can inadvertently promote further research based on these results, leading to erroneous conclusions \citep{McCullough-2008-replicable}.
%, as exemplified in our case study ($\S$\ref{sec:case-study}).

%In this paper, as a countermeasure to the problems mentioned above and a method to increase confidence in current findings and future research directions based on them, we posit that code correctness should be assessed on its own and promote the adoption of best practices of software quality assurance (SQA) in the research community. 
In this paper, as a countermeasure to the aforementioned issues and to bolster confidence in published results and subsequent findings, as well as future research based on them, we posit that code correctness should undergo an independent assessment and advocate for the adoption of best practices aimed at enhancing software quality within the research community.
Our contributions can be summarized as follows:

\begin{itemize}[leftmargin=12pt]
    \setlength\itemsep{-3pt}
    \item %We show that the code correctness of papers is given for granted in current peer-review processes based on the quality of the results, 
    We demonstrate that the code correctness of a paper in current peer reviews is assumed solely based on the quality of the results,
    whereas we posit that it should undergo independent assessment, similarly to reproducibility (\S\ref{sec:core-idea});
    \item \mn{[INVERTIREI: THROUGH A CASE STUDY...CONFORMER...DIMOSTRIAMO CHE IL TEMA E' URGENTE ...]} By using the open-source implementations of the widespread Conformer architecture \citep{gulati20_interspeech} as a case study, we prove that:
    \begin{itemize}[leftmargin=8pt]
    \item[-] all the analyzed implementations contain at least one bug in the codebase (\S\ref{subsec:analysis});
    \item[-] even in presence of bugs, it is possible to obtain reproducible results that are competitive with those of other architectures and works, across various tasks and languages 
    %pairs \mn{[non avendo detto su quali task e' fatta quest'analisi, suona strano leggere ``language pairs''.]}
    (\S\ref{subsec:impact_bug});
    \item[-] the presence of bugs can lead to incorrect findings when incorporating a new technique into the architecture (\S\ref{subsec:impact_code}).
    \end{itemize}
    \item We release an open-source version of Conformer implementation that is not affected by bugs, as well as all pre-trained models and outputs, at \url{[anonymous_url]};
    \item We propose a checklist of best practices to enforce code correctness (\S \ref{sec:checklist}).
\end{itemize}



