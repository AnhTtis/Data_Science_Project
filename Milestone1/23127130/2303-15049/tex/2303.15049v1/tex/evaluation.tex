\section{Experiments}
\label{sec:evaluation}

For our experiments, the encoder and the decoder in BlenderBot 1.0 \cite{roller-etal-2021-recipes} are used.\footnote{There have been updated versions of BlenderBot introduced \cite{xu-etal-2022-beyond,https://doi.org/10.48550/arxiv.2208.03188}. However, we chose the first version for our experiments because we found it to be as effective yet much more efficient than the newer versions.}
% \footnote{We also experimented with BlenderBot 2.0, which was supposed to consider a longer dialogue history \cite{xu-etal-2022-beyond}. However, its generation was too slow for a real-time system and our new techniques could not be easily integrated into, while it did not show noticeable improvement. Thus, we decided to build our final model with BlenderBot 1.0 instead.}
Three models are developed as follows:
\begin{itemize}
%\setlength\itemsep{0em}
\item BB: Blenderbot Baseline Model
\item SW: Blenderbot with Sliding Window
\item CT: Blenderbot with Sliding Window and Concatenation of Topic Storing
\end{itemize}

\noindent All models are first trained on \texttt{raw} and finetuned on \texttt{TRN} in Table~\ref{tab:annotated-data}).
To assess real-life performance, 10 interviews are conducted per model, where each interview consists of exactly 30 turns. %  (for fair comparisons)
A qualitative analysis is performed on the top-3 most frequently occurring errors as follows:
\begin{itemize}
\item Repetitions: how often it repeats topics already covered in the previous utterances.
\item Early Ending (EE): implies to end the interview without covering a sufficient amount of topics.
\item Off Topic (OT): how often it makes utterances that are not relevant to the current topic.
\end{itemize}


\noindent Table~\ref{tab:interviewbot-error-analysis} shows the error analysis results.
The repetition rates are significantly reduced as the model gets more advanced.
Compared to the baseline, the CT model conducts 3.5 times longer conversations before it attempts to end the interview, while generating twice fewer off-topic utterances, which is very promising.
Examples of these error types are provided in Appendix~\ref{app:generation-errors}.


\begin{table}[htbp!]
\centering\resizebox{\columnwidth}{!}{
\begin{tabular}{c|ccc}
\toprule
\bf Model & \multicolumn{1}{c}{\bf Repetitions (R)} & \multicolumn{1}{c}{\bf Early Ending (EE)} & \multicolumn{1}{c}{\bf Off Topic (OT)} \\
\midrule
BB & 30.0 & 13.3 & 20.0 \\
SW & 16.7 & 23.3 & 26.7 \\
CT & $\:\:$\bf 6.7 & \bf 46.7 & \bf 10.0 \\
\bottomrule
\end{tabular}
}
\caption{The error analysis of all generation models. R: avg-\% of repeated topics, EE: avg-\% of the interview conducted before the model attempts to end (higher is better), OT: avg-\% of off-topic utterances.}
\label{tab:interviewbot-error-analysis}
\end{table}


\begin{table*}[htbp!]
\centering\small{ %\resizebox{\columnwidth}{!}{
\scalebox{.85}{
\begin{tabular}{c|l} 
\toprule
\bf Flag & \multicolumn{1}{c}{\bf Utterance} \\
\midrule
\multirow[c]{2}{*}{\tt B}
 & Hello, and thank you so much for coming today. uh, Let us start by having you just to state your full legal name \\
 & and also the school that you are currently attending?\\
\midrule
\tt S2 & Sure, my name is David, and I go to Shanghai high school in China.\\
\midrule
\tt Q & Great. um, yeah, {let's talk about leadership experiences.} Can you give me some specific examples?\\
\midrule
\multirow[c]{2}{*}{\tt S2}
 & Sure, when I was a junior, I was elected to be the Vice President in the Student Union and I organized many \\
 & activities to help students to enrich their life on campus.\\
\midrule
\multirow[c]{2}{*}{\tt S1}
 & Okay, great. Thank you so much for sharing that. so, um, tell me a little bit about your leadership style. What do \\
 & you think you bring to the student union?\\
\midrule
\tt S2 & Sure, I am very open-minded, and with some of my colleagues, we together established a lot of new programs on \\
       & top of our existing ones. Many of those programs became very popular. \\
\midrule
\tt S1 & Ok, great. and so, um, what, uh, what would you say has been the most challenging aspect of being a leader? \\
\midrule
\multicolumn{2}{c}{$\cdots$} \\
\midrule
\multirow[c]{2}{*}{\tt Q}
 & Great. uh, um, hmm, let' talk about your courses. What courses did you take in high school and which courses \\
 & do you like most? \\
\midrule
\tt S2 & Sure, I took Math, English, PE, Chemistry, Art, and so on. I like Math most. \\
\midrule
\tt Q & Okay. so, um, tell me a little bit about your, your interest in mathematics. \\
\midrule
\multicolumn{2}{c}{$\cdots$} \\
\midrule
\multirow[c]{2}{*}{\tt E}
 & Okay, great. well, thank you very much for sharing that with me. um, and thank you for taking the time out of \\
 & your day to interview with us.\\
\bottomrule
\end{tabular}
}}
\caption{An interview dialogue conducted by our best model (CT in Section~\ref{sec:evaluation}). \texttt{S1/S2}: interviewer/interviewee (chatbot/human), \texttt{B/E}: beginning/ending utterance (chatbot), \texttt{Q}: topical question (chatbot).}
\label{tab:interview-example}
\end{table*}


\subsection{Static Evaluation}

Following previous work, static evaluation is performed on the CT model, where the input is every batch of $k$-utterances and prior topics per interview, and its output is compared to the corresponding human response in \texttt{TST} (Table~\ref{tab:annotated-data}).
The average \textsc{Bleu}\LN score is 0.08 and cosine similarity is 0.19, which\LN are low.
However, such static evaluation assesses each output independently and obstructs dialogue fluency by artificially inserting human utterances to\LN the model, and thus, does not reveal its capability in conducting long contextualized interviews.

%To amend this shortcoming, the following evaluation is proposed.

\subsection{Real-time Evaluation}

The CT model is deployed to an online text-based platform in a public cloud.
For real-time evaluation, 5 professional interviewers and 10 students are invited to have conversations with our InterviewBot and give ratings from 1 to 5 to indicate their overall satisfactions. 
The average dialogue duration is 256 seconds. 
Almost half of the evaluators are satisfied (Scores 4 and 5) and another 40\% indicate positive attitude on the coverage of topics and discussions (Score 3), implying that it performs reasonably well for this realistic setting (Table \ref{tab:interviewbot-rating-scores}).
Overall, with the average score of 3.5, the InterviewBot has shown great potential in applying to practical applications.

\begin{table}[htbp!]
\centering\small{
\begin{tabular}{c|ccccc|c}
\toprule
    \bf Rating Score & \bf 5 & \bf 4 & \bf 3 & \bf 2 & \bf 1 & \bm{$\sum$} \\
\midrule
     Evaluator Count & 3 & 4 & 6 & 1 & 1 & 15 \\
\bottomrule
\end{tabular}
}
\caption{The rating distribution of the InterviewBot conversations for real-time evaluation. 5: very satisfied, 4: satisfied, 3: neutral, 2: unsatisfied, 1: very unsatisfied.}
\label{tab:interviewbot-rating-scores}
\end{table}

%\noindent The limitations are also summarized for future improvement.
%First, the InterviewBot has an early-ending issue in conversations, in which the ending utterances could be generated after a few turns, not covering sufficient discussions on diverse topics.
%Second, although InterviewBot has certain capabilities to follow up on topics brought up during the conversation, it is still expected to perform deeper discussions on more details.
%Third, InterviewBot cannot handle name entities, such as people's names, during conversations.
%Last, in some conversations, InterviewBot generates repeated or random ordering of words and punctuation.

