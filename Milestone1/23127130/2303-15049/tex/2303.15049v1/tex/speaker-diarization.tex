\section{Speaker Diarization}
\label{sec:speaker-diarization}

Speaker diarization is a task of segmenting an audio stream into utterances according to the speaker identity, and considered critical in automatic transcription \cite{anguera2012speaker}.
Conversation data with diarization errors can lead to major failure of building robust dialogue models.
Our most accurate transcriber, RevAI, still gives 27.3\% errors for speaker diarization (Section~\ref{sec:dataset}).
The main reason is\LN that audios from the interviewer (\texttt{S1}) and the interviewee (\texttt{S2}) are recorded in one channel, so that they are saved in a single waveform, while no clear pauses exist between \texttt{S1} \& \texttt{S2}'s speeches or their speeches often overlap.
The following example illustrates when the speech of \texttt{S2} (\uwave{underlined}) is not recognized as a separate utterance:

\vspace{1.5ex}
\noindent \texttt{S1}: Hi , it 's nice to meet you . \uwave{Nice to meet you .}\LN
\texttt{S1}: Um , can you tell me what is a topic that um ,

$\;\;\;$you cannot stop talking about ?
\vspace{1.5ex}

\noindent Thus, speaker diarization models are developed to provide clean data to our dialogue model (Sec.~\ref{sec:generation-model}).
Figure~\ref{fig:diarization-errors} depicts the distributions of different types of diarization errors found in 100 dialogues. % (about 39 turns per dialogue on average).
Most errors are caused by filler words and arbitrary concatenation (joining multiple utterances as one with no apparent patterns, not caused by filler words).



\begin{figure}[htbp!]
\centering
\includegraphics[width=\columnwidth]{img/diarization-errors.pdf}
\caption{Distributions of the diarization error types. Appendix~\ref{app:diarization-error-example} provides examples of each error type.}
\label{fig:diarization-errors}
\end{figure}


\subsection{Manual Annotation}
\label{ssec:manual-annotation}

440 dialogues are sampled, in which every token is annotated either \texttt{1} if it is one of the last two tokens of an utterance before the speaker is switched, and \texttt{0} otherwise. %\footnote{An utterance can include multiple sentences.}
For the above example, the 8-9'th tokens are the last two tokens of the utterance before it switches to \texttt{S2} and so are the 13-14'th tokens before switching to \texttt{S1}; thus, they are annotated~\texttt{1}:\footnote{We also annotated only the last token as \texttt{1}, or annotated all words from \texttt{S0} as \texttt{0} and from \texttt{S1} as \texttt{1}, which yielded worse results in terms of the end performance.}

\vspace{1.5ex}

\begin{tabular}{\CSP{0.5}c\CSP{0.5}c\CSP{0.5}c\CSP{0.5}c\CSP{0.5}c\CSP{0.5}c\CSP{0.5}c\CSP{0.5}c\CSP{0.5}c\CSP{0.5}c\CSP{0.5}c\CSP{0.5}c\CSP{0.5}c\CSP{0.5}c}
Hi & , & it & 's & nice & to & meet & you & . & Nice & to & meet & you & .\\
\texttt{0} & \texttt{0} & \texttt{0} & \texttt{0} & \texttt{0} & \texttt{0} & \texttt{0} & \texttt{1} & \texttt{1} & \texttt{0} & \texttt{0} & \texttt{0} & \texttt{1} & \texttt{1} \\
\end{tabular}
\vspace{0.1ex}

\noindent Doccano is used as the annotation tool \cite{doccano}, and ELIT is used for the tokenization \cite{DBLP:journals/corr/abs-2109-03903}.
To measure the inter-annotator agreement, 10 dialogues are double-annotated that show the high kappa score of 84.4\%.


\subsection{Pseudo Annotation}
\label{ssec:pseudo-annotation}

Because our annotated data are relatively small, a larger dataset is pseudo-created for this task using 2,400 dialogues in the Switchboard \cite{stolcke-etal-2000-dialogue} and 6,808 dialogues in the BlendedSkillTalk \cite{smith-etal-2020-put} datasets (thus, a total of 9,208 dialogues).
These two datasets are chosen because their dialogues sound more speech-originated than others, having an adequate amount of filler words.
Among the 4 types of diarization errors (Figure~\ref{fig:diarization-errors}), the ones caused by filler words (33\%) can be simulated on dialogues that do not contain such errors using statistical heuristics.\footnote{Filler words are inferred by the outputs of the part-of-speech tagger and the dependency parser in ELIT.}

\begin{table}[htbp!]
\centering\small{ %\resizebox{\columnwidth}{!}{
\begin{tabular}{\CSP{0.8}c\CSP{0.8}c|\CSP{0.8}c\CSP{0.8}c\CSP{0.8}c\CSP{0.8}c\CSP{0.8}c\CSP{0.8}c\CSP{0.8}c\CSP{0.8}c\CSP{0.8}c} 
\toprule
\bf \# & \bf Dist & \it okay & \it yeah & \it right & \it um & \it so & \it uh & \it well & \it like & \it oh \\
\midrule
2 &      40.4 & 46.7 & 16.0 & 8.0 & 8.5 & $\:\:$8.0 & 4.4 & 4.7 & 0.2 & 3.1 \\
3 &      35.9 & 33.3 & 29.8 & 3.9 & 8.5 & 11.1 & 6.2 & 2.3 & 0.6 & 4.1 \\
4 & $\:\:$8.6 & 33.7 & 24.5 & 5.6 & 9.7 & 11.2 & 5.1 & 3.6 & 1.0 & 5.1 \\
5 & $\:\:$7.3 & 28.9 & 30.7 & 6.6 & 4.2 & 15.1 & 6.6 & 4.8 & 0.6 & 2.4 \\
\bottomrule
\end{tabular}}
\caption{Distributions of filler words w.r.t. diarization errors. Dist: percentage of dialogues containing \# number of utterances with errors caused by the filler words. \textit{filler\_word}: percentage of the filler word appearing in the corresponding dialogue group.}
\label{tab:filler-distribution}
\end{table}

\noindent The errors associated with filler words are pseudo-inserted to dialogues from the two datasets by finding an utterance either beginning or ending with a filler word, and concatenating it with an utterance prior or next to it.
Global search is made to the entire dialogues for finding such utterances to mimic the distributions in Table~\ref{tab:filler-distribution} such that about 40.4\% of the dialogues in the pseudo-created data would contain 2 utterances with diarization errors, where 46.7\% of them are caused by the filler word \textit{okay}, and so on.
It is possible that more than two utterances get joined; in our case, up to 8 utterances are concatenated.
Table~\ref{tab:diarization-data} includes the statistics of our pseudo-created dataset for transfer learning.



\subsection{Joint Model}
\label{ssec:joint-diarization-model}

Figure~\ref{fig:diarization-model} shows an overview of our speaker diarization model.
Let $U_i$ = $\{w_{i}^\circ, w_{i1}, .., w_{in}\}$ be the $i$'th utterance to be handled, where $w_{i}^\circ$ is the special token representing $U_i$ and $w_{ij}$ is the $j$'th token in~$U_i$.

\begin{figure}[htbp!]
\centering
\includegraphics[width=0.65\columnwidth]{img/diarization-model.pdf}
\caption{The overview of our diarization model.}
\label{fig:diarization-model}
\end{figure}

\noindent $U_i$ is fed into the encoder $\mathcal{E}$ that generates the embeddings $\{e_{i}^\circ, e_{i1}, .., e_{in}\}$.
The previous utterances $\{U_{i-k}, .., U_{i-1}\}$ are also fed into $\mathcal{E}$ that generates $\{e_{i-k}^\circ, .., e_{i-1}^\circ\}$ (in our case, $k = 5$ that is the context window).
For utterance-level weighting, these embeddings are fed into a transformer layer, which creates the context embedding $e_c$.
Finally, $e_c \oplus e_i^\circ$ is fed into a softmax layer that outputs $o_u$ to make a binary decision of whether or not $U_i$ includes any error.
Jointly, each $e_c \oplus e_{ij}$ is fed into another softmax that outputs $o_j$ to decide whether or not $w_{ij}$ is\LN one of the last two tokens of an utterance.


\begin{table*}[htbp!]
\centering\small{ %\resizebox{\textwidth}{!}{
\begin{tabular}{c|r\CSP{1.8}r\CSP{1.8}r\CSP{1.8}r|r\CSP{1.8}r\CSP{1.8}r\CSP{1.8}r||r\CSP{1.8}r\CSP{1.8}r\CSP{1.8}r|r\CSP{1.8}r\CSP{1.8}r}
\toprule
 & \multicolumn{4}{c|}{\bf Switchboard} & \multicolumn{4}{c||}{\bf BlendedSkillTalk (BST)} & \multicolumn{4}{c|}{\bf Interview Data (Before)} & \multicolumn{3}{c}{\bf (After)} \\
 & \multicolumn{1}{c}{\bf D} & \multicolumn{1}{c}{\bf U} & \multicolumn{1}{c}{\bf S1} & \multicolumn{1}{c|}{\bf S2} & \multicolumn{1}{c}{\bf D} & \multicolumn{1}{c}{\bf U} & \multicolumn{1}{c}{\bf S1} & \multicolumn{1}{c||}{\bf S2} & \multicolumn{1}{c}{\bf D} & \multicolumn{1}{c}{\bf U} & \multicolumn{1}{c}{\bf S1} & \multicolumn{1}{c|}{\bf S2} & \multicolumn{1}{c}{\bf U} & \multicolumn{1}{c}{\bf S1} & \multicolumn{1}{c}{\bf S2} \\
\midrule
\tt TRN & 1,115 & 42.2 & 31.6 & 31.3 & 4,819 & 6.2 & 25.0 & 25.2 &   140 & 42.2 & 37.4 & 73.1 & 43.8 & 39.3 & 64.0 \\
\tt DEV & 21 & 16.6 & 49.4 & 43.7 & 1,009 & 6.1 & 25.8 & 25.3 & 150 & 44.1 & 34.9 & 67.3 & 45.0 & 36.2 & 60.3 \\
\tt TST & 19 & 32.7 & 32.9 & 32.9 & 980 & 6.2 & 26.2 & 26.2 &   150 & 44.2 & 34.2 & 69.0 & 44.3 & 37.8 & 61.3 \\
\midrule
\tt RAW & \multicolumn{4}{c|}{N/A} & \multicolumn{4}{c||}{N/A} & 6,921 & 39.6 & 38.2 & 75.1 & 40.4 & 41.5 & 67.6 \\
\bottomrule
\end{tabular}}
\caption{Distributions of the pseudo-created datasets (Switchboard, BST) and our interview data (before and after diarization). D: number of dialogues, U: avg-number of utterances, S1/S2: avg-number of tokens per utterance by \texttt{S1}/\texttt{S2}. \texttt{TRN}/\texttt{DEV}/\texttt{TST}: training/development/evaluation (annotated) sets. \texttt{RAW}: unannotated set. Note that we follow the same splits suggested by the original papers of the Switchboard and BST datasets for comparability.}
\label{tab:diarization-data}
\end{table*}


\subsection{Experiments}

For the encoder, the RoBERTa large model is used \cite{DBLP:journals/corr/abs-1907-11692}.\footnote{Several transformer encoders including BERT \cite{devlin-etal-2019-bert} were evaluated and RoBERTa yielded the best results.}
Table~\ref{tab:diarization-data} shows the distributions of the pseudo-created data (Section~\ref{ssec:pseudo-annotation}), as well as our interview data (Section~\ref{sec:dataset}) before and after the diarization where errors in the train/dev/test sets are manually annotated (Section~\ref{ssec:manual-annotation}) and errors in the raw set are automatically corrected by the joint model (Section~\ref{ssec:joint-diarization-model}).
After diarization, \texttt{S2}'s utterances with diarization errors get split such that the average length of \texttt{S2}'s utterances decreases while the average length of dialogues slightly increases.
Meanwhile, some parts of \texttt{S2}'s utterances, incorrectly separated from \texttt{S1}'s utterances by the transcriber, are recovered back to \texttt{S1}; thus, the average length of \texttt{S1}'s utterances increases.


\noindent Table~\ref{tab:diarization-results} shows results of three models: the \textit{baseline} model taking $U_i$ and producing $O_w = \{o_1, .., o_n\}$, the \textit{context} model taking $\mathcal{U}_c = \{U_{i-k}, .., U_i\}$ and producing $O_w$, as well as the \textit{joint} model taking $\mathcal{U}_c$ and producing $o_u$ and $O_w$ (Figure~\ref{fig:diarization-model}).
The baseline model does not create $e_c$, so $e_{i*}$ are directly fed to Softmax 2. Also, the baseline and context models do not use $e_i^\circ$, so only Softmax 2 is used to produce the outputs.
For evaluation, the F1-scores of the label \texttt{1} on the last two tokens are used.
All models are developed three times and their average scores and standard deviations are reported.


\begin{table}[htbp!]
\centering\small{ %\resizebox{\columnwidth}{!}{
\begin{tabular}{c|ccc}
\toprule
 & \multicolumn{1}{c}{\bf Baseline} & \multicolumn{1}{c}{\bf Context}  & \multicolumn{1}{c}{\bf Joint} \\
\midrule
Ours only   & 92.9$\pm$0.4 & 92.9$\pm$0.3 &     92.9$\pm$0.2 \\
Transferred & 93.2$\pm$0.3 & 93.4$\pm$0.3 & \bf 93.6$\pm$0.3 \\
\bottomrule
\end{tabular}}
\caption{Diarization model performance. Ours: trained on \texttt{TRN} of the Interview Data (After) in Table~\ref{tab:diarization-data}. Transferred: trained first on the \texttt{TRN} mixture of Switchboard and BST, then finetuned on \texttt{TRN} of the Interview Data.}
\label{tab:diarization-results}
\end{table}

\noindent When trained on only our data, all models perform similarly.
The joint model slightly outperforms the others when transfer learning is applied.
Although the improvement is marginal, the joint model has a benefit of identifying utterances with diarization errors, showing the F1 score of 93.6\% for this task, while the transferred models generally show much higher performance on the other datasets than the non-transferred models.
Thus, the joint transferred model is used to auto-correct all dialogues in \texttt{RAW}.