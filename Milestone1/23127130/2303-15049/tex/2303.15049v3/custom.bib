@misc{https://doi.org/10.48550/arxiv.2208.03188,
  doi = {10.48550/ARXIV.2208.03188},
  
  url = {https://arxiv.org/abs/2208.03188},
  
  author = {Shuster, Kurt and Xu, Jing and Komeili, Mojtaba and Ju, Da and Smith, Eric Michael and Roller, Stephen and Ung, Megan and Chen, Moya and Arora, Kushal and Lane, Joshua and Behrooz, Morteza and Ngan, William and Poff, Spencer and Goyal, Naman and Szlam, Arthur and Boureau, Y-Lan and Kambadur, Melanie and Weston, Jason},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{xu-etal-2022-beyond,
    title = "Beyond Goldfish Memory: Long-Term Open-Domain Conversation",
    author = "Xu, Jing  and
      Szlam, Arthur  and
      Weston, Jason",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.356",
    doi = "10.18653/v1/2022.acl-long.356",
    pages = "5180--5197",
    abstract = "Despite recent improvements in open-domain dialogue models, state of the art models are trained and evaluated on short conversations with little context. In contrast, the long-term conversation setting has hardly been studied. In this work we collect and release a human-human dataset consisting of multiple chat sessions whereby the speaking partners learn about each other{'}s interests and discuss the things they have learnt from past sessions. We show how existing models trained on existing datasets perform poorly in this long-term conversation setting in both automatic and human evaluations, and we study long-context models that can perform much better. In particular, we find retrieval-augmented methods and methods with an ability to summarize and recall previous conversations outperform the standard encoder-decoder architectures currently considered state of the art.",
}

@article{amiri2022chatbot,
  title={Chatbot use cases in the Covid-19 public health response},
  author={Amiri, Parham and Karahanna, Elena},
  journal={Journal of the American Medical Informatics Association},
  volume={29},
  number={5},
  pages={1000--1010},
  year={2022},
  publisher={Oxford University Press}
}

@article{fan2021utilization,
  title={Utilization of self-diagnosis health chatbots in real-world settings: case study},
  author={Fan, Xiangmin and Chao, Daren and Zhang, Zhan and Wang, Dakuo and Li, Xiaohua and Tian, Feng and others},
  journal={Journal of medical Internet research},
  volume={23},
  number={1},
  pages={e19928},
  year={2021},
  publisher={JMIR Publications Inc., Toronto, Canada}
}

@inproceedings{baier2018conversational,
  title={Conversational User Interfaces for Online Shops? A Categorization of Use Cases},
  author={Baier, Daniel and Rese, Alexandra and R{\"o}glinger, Maximilian and Baier, D and Rese, A and R{\"o}glinger, M},
  booktitle={Proceedings of the International Conference on Information Systems},
  series={ICIS},
  year={2018}
}

	@article{nichifor2021artificial,
  title={Artificial intelligence in electronic commerce: Basic chatbots and the consumer journey},
  author={Nichifor, Eliza and Trifan, Adrian and Nechifor, Elena Mihaela},
  journal={Amfiteatru Economic},
  volume={23},
  number={56},
  pages={87--101},
  year={2021},
  publisher={Bucharest Academy of Economic Studies, Faculty of Commerce}
}

@inproceedings{cunningham2019review,
  title={A review of chatbots in education: practical steps forward},
  author={Cunningham-Nelson, Sam and Boles, Wageeh and Trouton, Luke and Margerison, Emily},
  booktitle={30th Annual Conference for the Australasian Association for Engineering Education (AAEE 2019): Educators Becoming Agents of Change: Innovate, Integrate, Motivate},
  pages={299--306},
  year={2019},
  organization={Engineers Australia}
}

@article{bao2021plato,
  title={Plato-xl: Exploring the large-scale pre-training of dialogue generation},
  author={Bao, Siqi and He, Huang and Wang, Fan and Wu, Hua and Wang, Haifeng and Wu, Wenquan and Wu, Zhihua and Guo, Zhen and Lu, Hua and Huang, Xinxian and others},
  journal={arXiv preprint arXiv:2109.09519},
  year={2021}
}

@inproceedings{roller-etal-2021-recipes,
	abstract = {Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we highlight other ingredients. Good conversation requires blended skills: providing engaging talking points, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent persona. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models and code publicly available. Human evaluations show our best models outperform existing approaches in multi-turn dialogue on engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.},
	address = {Online},
	author = {Roller, Stephen and Dinan, Emily and Goyal, Naman and Ju, Da and Williamson, Mary and Liu, Yinhan and Xu, Jing and Ott, Myle and Smith, Eric Michael and Boureau, Y-Lan and Weston, Jason},
	booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	date-added = {2022-06-07 14:46:37 -0400},
	date-modified = {2022-06-07 14:46:37 -0400},
	doi = {10.18653/v1/2021.eacl-main.24},
	month = apr,
	pages = {300--325},
	publisher = {Association for Computational Linguistics},
	title = {Recipes for Building an Open-Domain Chatbot},
	url = {https://aclanthology.org/2021.eacl-main.24},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.eacl-main.24},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.eacl-main.24}}

@article{DBLP:journals/corr/abs-1907-11692,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692},
  eprinttype = {arXiv},
  eprint    = {1907.11692},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{smith-etal-2020-put,
	abstract = {Being engaging, knowledgeable, and empathetic are all desirable general qualities in a conversational agent. Previous work has introduced tasks and datasets that aim to help agents to learn those qualities in isolation and gauge how well they can express them. But rather than being specialized in one single quality, a good open-domain conversational agent should be able to seamlessly blend them all into one cohesive conversational flow. In this work, we investigate several ways to combine models trained towards isolated capabilities, ranging from simple model aggregation schemes that require minimal additional training, to various forms of multi-task training that encompass several skills at all training stages. We further propose a new dataset, BlendedSkillTalk, to analyze how these capabilities would mesh together in a natural conversation, and compare the performance of different architectures and training schemes. Our experiments show that multi-tasking over several tasks that focus on particular capabilities results in better blended conversation performance compared to models trained on a single skill, and that both unified or two-stage approaches perform well if they are constructed to avoid unwanted bias in skill selection or are fine-tuned on our new task.},
	address = {Online},
	author = {Smith, Eric Michael and Williamson, Mary and Shuster, Kurt and Weston, Jason and Boureau, Y-Lan},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-05-29 17:38:36 -0400},
	date-modified = {2022-05-29 17:38:36 -0400},
	doi = {10.18653/v1/2020.acl-main.183},
	month = jul,
	pages = {2021--2030},
	publisher = {Association for Computational Linguistics},
	title = {Can You Put it All Together: Evaluating Conversational Agents{'} Ability to Blend Skills},
	url = {https://aclanthology.org/2020.acl-main.183},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.acl-main.183},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.183}
}

@article{stolcke-etal-2000-dialogue,
    title = "Dialogue act modeling for automatic tagging and recognition of conversational speech",
    author = "Stolcke, Andreas  and
      Ries, Klaus  and
      Coccaro, Noah  and
      Shriberg, Elizabeth  and
      Bates, Rebecca  and
      Jurafsky, Daniel  and
      Taylor, Paul  and
      Martin, Rachel  and
      Van Ess-Dykema, Carol  and
      Meteer, Marie",
    journal = "Computational Linguistics",
    volume = "26",
    number = "3",
    year = "2000",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/J00-3003",
    pages = "339--374",
}

@article{DBLP:journals/corr/abs-2109-03903,
  author    = {Han He and
               Liyan Xu and
               Jinho D. Choi},
  title     = {{ELIT:} Emory Language and Information Toolkit},
  journal   = {CoRR},
  volume    = {abs/2109.03903},
  year      = {2021},
  url       = {https://arxiv.org/abs/2109.03903},
  eprinttype = {arXiv},
  eprint    = {2109.03903},
  timestamp = {Mon, 20 Sep 2021 16:29:41 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2109-03903.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{doccano,
  title={{doccano}: Text Annotation Tool for Human},
  url={https://github.com/doccano/doccano},
  note={Software available from https://github.com/doccano/doccano},
  author={
    Hiroki Nakayama and
    Takahiro Kubo and
    Junya Kamura and
    Yasufumi Taniguchi and
    Xu Liang},
  year={2018},
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inbook{10.1145/3313831.3376131,
author = {Xiao, Ziang and Zhou, Michelle X. and Chen, Wenxi and Yang, Huahai and Chi, Changyan},
title = {If I Hear You Correctly: Building and Evaluating Interview Chatbots with Active Listening Skills},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376131},
abstract = {Interview chatbots engage users in a text-based conversation to draw out their views and opinions. It is, however, challenging to build effective interview chatbots that can handle user free-text responses to open-ended questions and deliver engaging user experience. As the first step, we are investigating the feasibility and effectiveness of using publicly available, practical AI technologies to build effective interview chatbots. To demonstrate feasibility, we built a prototype scoped to enable interview chatbots with a subset of active listening skills-the abilities to comprehend a user's input and respond properly. To evaluate the effectiveness of our prototype, we compared the performance of interview chatbots with or without active listening skills on four common interview topics in a live evaluation with 206 users. Our work presents practical design implications for building effective interview chatbots, hybrid chatbot platforms, and empathetic chatbots beyond interview tasks.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14}
}

@article{finch2020emora,
  title={Emora: An inquisitive social chatbot who cares for you},
  author={Finch, Sarah E and Finch, James D and Ahmadvand, Ali and Dong, Xiangjue and Qi, Ruixiang and Sahijwani, Harshita and Volokhin, Sergey and Wang, Zihan and Wang, Zihao and Choi, Jinho D and others},
  journal={arXiv preprint arXiv:2009.04617},
  year={2020}
}

@article{ahmadvand2018emory,
  title={Emory irisbot: An open-domain conversational bot for personalized information access},
  author={Ahmadvand, Ali and Choi, Ingyu and Sahijwani, Harshita and Schmidt, Justus and Sun, Mingyang and Volokhin, Sergey and Wang, Zihao and Agichtein, Eugene},
  journal={Alexa Prize Proceedings},
  year={2018}
}

@article{wang2017emersonbot,
  title={Emersonbot: Information-focused conversational AI Emory university at the Alexa Prize 2017 challenge},
  author={Wang, Zihao and Ahmadvand, Ali and Choi, Jason Ingyu and Karisani, Payam and Agichtein, Eugene},
  journal={1st Proceeding of Alexa Prize},
  year={2017}
}

@inproceedings{li2017confiding,
  title={Confiding in and listening to virtual agents: The effect of personality},
  author={Li, Jingyi and Zhou, Michelle X and Yang, Huahai and Mark, Gloria},
  booktitle={Proceedings of the 22nd International Conference on Intelligent User Interfaces},
  pages={275--286},
  year={2017}
}


@inproceedings{Kim2019Comparing,
author = {Kim, Soomin and Lee, Joonhwan and Gweon, Gahgene},
year = {2019},
month = {04},
pages = {1-12},
booktitle={Proceedings of the 2019 CHI Conference},
title = {Comparing Data from Chatbot and Web Surveys: Effects of Platform and Conversational Style on Survey Response Quality},
doi = {10.1145/3290605.3300316}
}

@article{anguera2012speaker,
  title={Speaker diarization: A review of recent research},
  author={Anguera, Xavier and Bozonnet, Simon and Evans, Nicholas and Fredouille, Corinne and Friedland, Gerald and Vinyals, Oriol},
  journal={IEEE Transactions on audio, speech, and language processing},
  volume={20},
  number={2},
  pages={356--370},
  year={2012},
  publisher={IEEE}
}

@article{parthasarathi2018extending,
  title={Extending neural generative conversational model using external knowledge sources},
  author={Parthasarathi, Prasanna and Pineau, Joelle},
  journal={arXiv preprint arXiv:1809.05524},
  year={2018}
}

@article{okonkwo2021chatbots,
  title={Chatbots applications in education: A systematic review},
  author={Okonkwo, Chinedu Wilfred and Ade-Ibijola, Abejide},
  journal={Computers and Education: Artificial Intelligence},
  volume={2},
  pages={100033},
  year={2021},
  publisher={Elsevier}
}

@article{safi2020technical,
  title={Technical aspects of developing chatbots for medical applications: scoping review},
  author={Safi, Zeineb and Abd-Alrazaq, Alaa and Khalifa, Mohamed and Househ, Mowafa and others},
  journal={Journal of medical Internet research},
  volume={22},
  number={12},
  pages={e19127},
  year={2020},
  publisher={JMIR Publications Inc., Toronto, Canada}
}

@article{khoa2021impact,
  title={The Impact of Chatbots on the Relationship between Integrated Marketing Communication and Online Purchasing Behavior in The Frontier Market},
  author={Khoa, Bui Thanh},
  journal={Jurnal The Messenger},
  volume={13},
  number={1},
  pages={19--32},
  year={2021}
}

@article{ilievski2018goal,
  title={Goal-oriented chatbot dialog management bootstrapping with transfer learning},
  author={Ilievski, Vladimir and Musat, Claudiu and Hossmann, Andreea and Baeriswyl, Michael},
  journal={arXiv preprint arXiv:1802.00500},
  year={2018}
}

@article{lian2019learning,
  title={Learning to select knowledge for response generation in dialog systems},
  author={Lian, Rongzhong and Xie, Min and Wang, Fan and Peng, Jinhua and Wu, Hua},
  journal={arXiv preprint arXiv:1902.04911},
  year={2019}
}

@article{adiwardana2020towards,
  title={Towards a human-like open-domain chatbot},
  author={Adiwardana, Daniel and Luong, Minh-Thang and So, David R and Hall, Jamie and Fiedel, Noah and Thoppilan, Romal and Yang, Zi and Kulshreshtha, Apoorv and Nemade, Gaurav and Lu, Yifeng and others},
  journal={arXiv preprint arXiv:2001.09977},
  year={2020}
}

@inproceedings{serban2017hierarchical,
  title={A hierarchical latent variable encoder-decoder model for generating dialogues},
  author={Serban, Iulian and Sordoni, Alessandro and Lowe, Ryan and Charlin, Laurent and Pineau, Joelle and Courville, Aaron and Bengio, Yoshua},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={31},
  number={1},
  year={2017}
}

@article{mehri2019pretraining,
  title={Pretraining methods for dialog context representation learning},
  author={Mehri, Shikib and Razumovskaia, Evgeniia and Zhao, Tiancheng and Eskenazi, Maxine},
  journal={arXiv preprint arXiv:1906.00414},
  year={2019}
}

@article{zhou2021eva,
  title={Eva: An open-domain chinese dialogue system with large-scale generative pre-training},
  author={Zhou, Hao and Ke, Pei and Zhang, Zheng and Gu, Yuxian and Zheng, Yinhe and Zheng, Chujie and Wang, Yida and Wu, Chen Henry and Sun, Hao and Yang, Xiaocong and others},
  journal={arXiv preprint arXiv:2108.01547},
  year={2021}
}

@inproceedings{zhang-etal-2020-dialogpt,
    title = "{DIALOGPT} : Large-Scale Generative Pre-training for Conversational Response Generation",
    author = "Zhang, Yizhe  and
      Sun, Siqi  and
      Galley, Michel  and
      Chen, Yen-Chun  and
      Brockett, Chris  and
      Gao, Xiang  and
      Gao, Jianfeng  and
      Liu, Jingjing  and
      Dolan, Bill",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-demos.30",
    doi = "10.18653/v1/2020.acl-demos.30",
    pages = "270--278",
    abstract = "We present a large, tunable neural conversational response generation model, DIALOGPT (dialogue generative pre-trained transformer). Trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning from 2005 through 2017, DialoGPT extends the Hugging Face PyTorch transformer to attain a performance close to human both in terms of automatic and human evaluation in single-turn dialogue settings. We show that conversational systems that leverage DialoGPT generate more relevant, contentful and context-consistent responses than strong baseline systems. The pre-trained model and training pipeline are publicly released to facilitate research into neural response generation and the development of more intelligent open-domain dialogue systems.",
}

@misc{https://doi.org/10.48550/arxiv.2001.09977,
  doi = {10.48550/ARXIV.2001.09977},
  
  url = {https://arxiv.org/abs/2001.09977},
  
  author = {Adiwardana, Daniel and Luong, Minh-Thang and So, David R. and Hall, Jamie and Fiedel, Noah and Thoppilan, Romal and Yang, Zi and Kulshreshtha, Apoorv and Nemade, Gaurav and Lu, Yifeng and Le, Quoc V.},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Towards a Human-like Open-Domain Chatbot},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{bao-etal-2020-plato,
    title = "{PLATO}: Pre-trained Dialogue Generation Model with Discrete Latent Variable",
    author = "Bao, Siqi  and
      He, Huang  and
      Wang, Fan  and
      Wu, Hua  and
      Wang, Haifeng",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.9",
    doi = "10.18653/v1/2020.acl-main.9",
    pages = "85--96",
    abstract = "Pre-training models have been proved effective for a wide range of natural language processing tasks. Inspired by this, we propose a novel dialogue generation pre-training framework to support various kinds of conversations, including chit-chat, knowledge grounded dialogues, and conversational question answering. In this framework, we adopt flexible attention mechanisms to fully leverage the bi-directional context and the uni-directional characteristic of language generation. We also introduce discrete latent variables to tackle the inherent one-to-many mapping problem in response generation. Two reciprocal tasks of response generation and latent act recognition are designed and carried out simultaneously within a shared network. Comprehensive experiments on three publicly available datasets verify the effectiveness and superiority of the proposed framework.",
}

@misc{https://doi.org/10.48550/arxiv.2109.09519,
  doi = {10.48550/ARXIV.2109.09519},
  
  url = {https://arxiv.org/abs/2109.09519},
  
  author = {Bao, Siqi and He, Huang and Wang, Fan and Wu, Hua and Wang, Haifeng and Wu, Wenquan and Wu, Zhihua and Guo, Zhen and Lu, Hua and Huang, Xinxian and Tian, Xin and Xu, Xinchao and Lin, Yingzhan and Niu, Zhengyu},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.2109.09519,
  doi = {10.48550/ARXIV.2109.09519},
  
  url = {https://arxiv.org/abs/2109.09519},
  
  author = {Bao, Siqi and He, Huang and Wang, Fan and Wu, Hua and Wang, Haifeng and Wu, Wenquan and Wu, Zhihua and Guo, Zhen and Lu, Hua and Huang, Xinxian and Tian, Xin and Xu, Xinchao and Lin, Yingzhan and Niu, Zhengyu},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{minhas2022protecting,
  title={Protecting victim and witness statement: examining the effectiveness of a chatbot that uses artificial intelligence and a cognitive interview},
  author={Minhas, Rashid and Elphick, Camilla and Shaw, Julia},
  journal={AI \& SOCIETY},
  volume={37},
  number={1},
  pages={265--281},
  year={2022},
  publisher={Springer}
}

@inproceedings{siddig2019psychologist,
  title={A Psychologist Chatbot Developing Experience.},
  author={Siddig, Abubakr and Hines, Andrew},
  booktitle={AICS},
  pages={200--211},
  year={2019}
}

@inproceedings{ni2017mandy,
  title={Mandy: Towards a smart primary care chatbot application},
  author={Ni, Lin and Lu, Chenhao and Liu, Niu and Liu, Jiamou},
  booktitle={International symposium on knowledge and systems sciences},
  pages={38--52},
  year={2017},
  organization={Springer}
}

@inproceedings{xiao2019should,
  title={Who should be my teammates: Using a conversational agent to understand individuals and help teaming},
  author={Xiao, Ziang and Zhou, Michelle X and Fu, Wat-Tat},
  booktitle={Proceedings of the 24th International Conference on Intelligent User Interfaces},
  pages={437--447},
  year={2019}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{touvron2023llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{lewis2019bart,
      title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension}, 
      author={Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Ves Stoyanov and Luke Zettlemoyer},
      year={2019},
      eprint={1910.13461},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{montahaei2019jointly,
      title={Jointly Measuring Diversity and Quality in Text Generation Models}, 
      author={Ehsan Montahaei and Danial Alihosseini and Mahdieh Soleymani Baghshah},
      year={2019},
      eprint={1904.03971},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{9681776,
  author={Al Adel, Arij and Burtsev, Mikhail S.},
  booktitle={2021 International Conference Engineering and Telecommunication (En\&T)}, 
  title={Memory transformer with hierarchical attention for long document processing}, 
  year={2021},
  volume={},
  number={},
  pages={1-7},
  doi={10.1109/EnT50460.2021.9681776}}

@inproceedings{raheja-tetreault-2019-dialogue,
    title = "{D}ialogue {A}ct {C}lassification with {C}ontext-{A}ware {S}elf-{A}ttention",
    author = "Raheja, Vipul  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1373",
    doi = "10.18653/v1/N19-1373",
    pages = "3727--3733",
    abstract = "Recent work in Dialogue Act classification has treated the task as a sequence labeling problem using hierarchical deep neural networks. We build on this prior work by leveraging the effectiveness of a context-aware self-attention mechanism coupled with a hierarchical recurrent neural network. We conduct extensive evaluations on standard Dialogue Act classification datasets and show significant improvement over state-of-the-art results on the Switchboard Dialogue Act (SwDA) Corpus. We also investigate the impact of different utterance-level representation learning methods and show that our method is effective at capturing utterance-level semantic text representations while maintaining high accuracy.",
}

@INPROCEEDINGS{9533452,
  author={Ghosh, Soumitra and Varshney, Deeksha and Ekbal, Asif and Bhattacharyya, Pushpak},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Context and Knowledge Enriched Transformer Framework for Emotion Recognition in Conversations}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/IJCNN52387.2021.9533452}}
