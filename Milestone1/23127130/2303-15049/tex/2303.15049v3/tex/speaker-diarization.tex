\subsection{Speaker Diarization}
\label{sec:speaker-diarization}

\noindent Speaker diarization is the task of segmenting an audio stream into utterances according to the speaker's identity and is considered critical in automatic transcription \cite{anguera2012speaker}.
Conversation data with diarization errors can lead to a major failure in building robust dialogue models.
Our most accurate transcriber, RevAI, still gives 27.3\% errors for speaker diarization (Section~\ref{sec:dataset}).
The main reason is that audios from the interviewer (\texttt{S1}) and the interviewee (\texttt{S2}) are recorded in one channel, so that they are saved in a single waveform, while no clear pauses exist between \texttt{S1} \& \texttt{S2}'s speeches or their speeches often overlap.
The following example illustrates when the speech of \texttt{S2} (\uwave{underlined}) is not recognized as a separate utterance:

\vspace{1.5ex}
\noindent \texttt{S1}: Hi , it 's nice to meet you . \uwave{Nice to meet you .}\\
\texttt{S2}: Um , can you tell me what is a topic that um , you cannot stop talking about ?
\vspace{1.5ex}

\noindent Thus, speaker diarization models are developed to provide clean data to our dialogue model (Sec.~\ref{sec:generation-model}).
Figure~\ref{fig:diarization-errors} depicts the distributions of different types of diarization errors found in 100 dialogues. % (about 39 turns per dialogue on average).
Most errors are caused by filler words and arbitrary concatenation (joining multiple utterances as one with no apparent patterns, not caused by filler words).

\begin{figure}[htbp!]
\centering
\includegraphics[width=0.8\columnwidth]{img/country_demo.pdf}
\caption{The interviewee's country demographics.}
\label{fig:interviewee-country-demographics}
\end{figure}


\begin{figure}[htbp!]
\centering
\includegraphics[width=0.5\columnwidth]{img/gender_demo.pdf}
\caption{The interviewee's gender demographics.}
\label{fig:interviewee-gender-demographics}
\end{figure}

\begin{figure}[htbp!]
\centering
\includegraphics[width=0.7\textwidth]{img/diarization-errors.pdf}
\caption{Distributions of the diarization error types. Appendix~\ref{app:diarization-error-example} provides examples of each error type.}
\label{fig:diarization-errors}
\end{figure}


\subsubsection{Manual Annotation}
\label{ssec:manual-annotation}

\noindent 440 dialogues are sampled, in which every token is annotated either \texttt{1} if it is one of the last two tokens of an utterance before the speaker is switched, and \texttt{0} otherwise. %\footnote{An utterance can include multiple sentences.}
For the above example, the 8-9'th tokens are the last two tokens of the utterance before it switches to \texttt{S2} and so are the 13-14'th tokens before switching to \texttt{S1}; thus, they are annotated~\texttt{1}:\footnote{We also annotated only the last token as \texttt{1}, or annotated all words from \texttt{S0} as \texttt{0} and from \texttt{S1} as \texttt{1}, which yielded worse results in terms of the end performance.}

\vspace{1.5ex}
\begin{tabular}{cccccccccccccc}
Hi & , & it & 's & nice & to & meet & you & . & Nice & to & meet & you & .\\
\texttt{0} & \texttt{0} & \texttt{0} & \texttt{0} & \texttt{0} & \texttt{0} & \texttt{0} & \texttt{1} & \texttt{1} & \texttt{0} & \texttt{0} & \texttt{0} & \texttt{1} & \texttt{1} \\
\end{tabular}
\vspace{1.5ex}

\noindent Doccano is used as the annotation tool \cite{doccano}, and ELIT is used for the tokenization \cite{DBLP:journals/corr/abs-2109-03903}.
To measure the inter-annotator agreement, ten dialogues are double-annotated that show a high kappa score of 84.4\%.


\subsubsection{Pseudo Annotation}
\label{ssec:pseudo-annotation}

\noindent Because our annotated data are relatively small, a larger dataset is pseudo-created for this task using 2,400 dialogues in the Switchboard \cite{stolcke-etal-2000-dialogue} and 6,808 dialogues in the BlendedSkillTalk \cite{smith-etal-2020-put} datasets (thus, a total of 9,208 dialogues).
These two datasets are chosen because their dialogues sound more speech-originated than others, having an adequate amount of filler words.
Among the four types of diarization errors (Figure~\ref{fig:diarization-errors}), the ones caused by filler words (33\%) can be simulated on dialogues that do not contain such errors using statistical heuristics.\footnote{Filler words are inferred by the outputs of the part-of-speech tagger and the dependency parser in ELIT.}


The errors associated with filler words are pseudo-inserted into dialogues from the two datasets by finding an utterance either beginning or ending with a filler word and concatenating it with an utterance before or next to it.
Global search is made to the entire dialogues for finding such utterances to mimic the distributions in Table~\ref{tab:filler-distribution} such that about 40.4\% of the dialogues in the pseudo-created data would contain two utterances with diarization errors, where 46.7\% of them are caused by the filler word \textit{okay}, and so on.
It is possible that more than two utterances get joined; in our case, up to 8 utterances are concatenated.
Table~\ref{tab:diarization-data} includes the statistics of our pseudo-created dataset for transfer learning.


\begin{table}[htbp!]
\caption{Distributions of filler words w.r.t. diarization errors. Dist: percentage of dialogues containing \# number of utterances with errors caused by the filler words. \textit{filler\_word}: percentage of the filler word appearing in the corresponding dialogue group.}
\centering\small{ %\resizebox{\columnwidth}{!}{
\begin{tabular}{cc|ccccccccc} 
\toprule
\bf \# & \bf Dist & \it okay & \it yeah & \it right & \it um & \it so & \it uh & \it well & \it like & \it oh \\
\midrule
2 &      40.4 & 46.7 & 16.0 & 8.0 & 8.5 & $\:\:$8.0 & 4.4 & 4.7 & 0.2 & 3.1 \\
3 &      35.9 & 33.3 & 29.8 & 3.9 & 8.5 & 11.1 & 6.2 & 2.3 & 0.6 & 4.1 \\
4 & $\:\:$8.6 & 33.7 & 24.5 & 5.6 & 9.7 & 11.2 & 5.1 & 3.6 & 1.0 & 5.1 \\
5 & $\:\:$7.3 & 28.9 & 30.7 & 6.6 & 4.2 & 15.1 & 6.6 & 4.8 & 0.6 & 2.4 \\
\bottomrule
\end{tabular}}
\label{tab:filler-distribution}
\end{table}

\subsubsection{Joint Model}
\label{ssec:joint-diarization-model}

\noindent The joint model consists of two parts. First, we establish a binary classification task that enforces the model to learn to differentiate utterances that have diarization errors. 
The second part is a diarization model to tackle the problem specifically. 
The intention behind this design is that the binary classification task could enhance the embedding representation on a higher level to perform the diarization task better.

Figure~\ref{fig:diarization-model} shows an overview of our speaker diarization model.
Let $U_i$ = $\{w_{i}^\circ, w_{i1}, .., w_{in}\}$ be the $i$'th utterance to be handled, where $w_{i}^\circ$ is the special token representing $U_i$ and $w_{ij}$ is the $j$'th token in~$U_i$.
$U_i$ is fed into the encoder $\mathcal{E}$ that generates the embeddings $\{e_{i}^\circ, e_{i1}, .., e_{in}\}$.
The previous utterances $\{U_{i-k}, .., U_{i-1}\}$ are also fed into $\mathcal{E}$ that generates $\{e_{i-k}^\circ, .., e_{i-1}^\circ\}$ (in our case, $k = 5$ that is the context window).
These embeddings are fed into a transformer layer for utterance-level weighting, which creates the context embedding $e_c$.
Finally, $e_c \oplus e_i^\circ$ is fed into a softmax layer that outputs $o_u$ to make a binary decision of whether or not $U_i$ includes any error.
Jointly, each $e_c \oplus e_{ij}$ is fed into another softmax that outputs $o_j$ to decide whether or not $w_{ij}$ is one of the last two tokens of an utterance.



\begin{figure}[htbp!]
\centering
\includegraphics[width=0.55\columnwidth]{img/diarization-model.pdf}
\caption{The overview of our diarization model.}
\label{fig:diarization-model}
\end{figure}

