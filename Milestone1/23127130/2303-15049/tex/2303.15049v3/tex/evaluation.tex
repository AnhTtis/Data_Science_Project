\section{Results}
\label{sec:evaluation}

\subsection{Speaker Diarization Results}


\noindent Table~\ref{tab:diarization-data} shows the distributions of the pseudo-created data (Section~\ref{ssec:pseudo-annotation}), as well as our interview data (Section~\ref{sec:dataset}) before and after the diarization where errors in the train/dev/test sets are manually annotated (Section~\ref{ssec:manual-annotation}) and errors in the raw set are automatically corrected by the joint model (Section~\ref{ssec:joint-diarization-model}).
For the encoder, the RoBERTa large model is used \cite{DBLP:journals/corr/abs-1907-11692}.\footnote{Several transformer encoders including BERT \cite{devlin-etal-2019-bert} were evaluated and RoBERTa yielded the best results.}
After diarization, \texttt{S2}'s utterances with diarization errors get split such that the average length of \texttt{S2}'s utterances decreases while the average length of dialogues slightly increases.
Meanwhile, some parts of \texttt{S2}'s utterances, incorrectly separated from \texttt{S1}'s utterances by the transcriber, are recovered back to \texttt{S1}; thus, the average length of \texttt{S1}'s utterances increases.



\begin{table}[htbp!]
\caption{Distributions of the pseudo-created datasets (Switchboard, BST) and our interview data (before and after diarization). D: number of dialogues, U: avg-number of utterances, S1/S2: avg-number of tokens per utterance by \texttt{S1}/\texttt{S2}. \texttt{TRN}/\texttt{DEV}/\texttt{TST}: training/development/evaluation (annotated) sets. \texttt{RAW}: unannotated set. Note that we follow the same splits suggested by the original papers of the Switchboard and BST datasets for comparability.}
\centering\resizebox{\textwidth}{!}{
\begin{tabular}{c|rrrr|rrrr||rrrr|rrr}
\toprule
 & \multicolumn{4}{c|}{\bf Switchboard} & \multicolumn{4}{c||}{\bf BlendedSkillTalk (BST)} & \multicolumn{4}{c|}{\bf Interview Data (Before)} & \multicolumn{3}{c}{\bf (After)} \\
 & \multicolumn{1}{c}{\bf D} & \multicolumn{1}{c}{\bf U} & \multicolumn{1}{c}{\bf S1} & \multicolumn{1}{c|}{\bf S2} & \multicolumn{1}{c}{\bf D} & \multicolumn{1}{c}{\bf U} & \multicolumn{1}{c}{\bf S1} & \multicolumn{1}{c||}{\bf S2} & \multicolumn{1}{c}{\bf D} & \multicolumn{1}{c}{\bf U} & \multicolumn{1}{c}{\bf S1} & \multicolumn{1}{c|}{\bf S2} & \multicolumn{1}{c}{\bf U} & \multicolumn{1}{c}{\bf S1} & \multicolumn{1}{c}{\bf S2} \\
\midrule
\tt TRN & 1,115 & 42.2 & 31.6 & 31.3 & 4,819 & 6.2 & 25.0 & 25.2 &   140 & 42.2 & 37.4 & 73.1 & 43.8 & 39.3 & 64.0 \\
\tt DEV & 21 & 16.6 & 49.4 & 43.7 & 1,009 & 6.1 & 25.8 & 25.3 & 150 & 44.1 & 34.9 & 67.3 & 45.0 & 36.2 & 60.3 \\
\tt TST & 19 & 32.7 & 32.9 & 32.9 & 980 & 6.2 & 26.2 & 26.2 &   150 & 44.2 & 34.2 & 69.0 & 44.3 & 37.8 & 61.3 \\
\midrule
\tt RAW & \multicolumn{4}{c|}{N/A} & \multicolumn{4}{c||}{N/A} & 6,921 & 39.6 & 38.2 & 75.1 & 40.4 & 41.5 & 67.6 \\
\bottomrule
\end{tabular}}
\label{tab:diarization-data}
\end{table}


Table~\ref{tab:diarization-results} shows results of three models: the \textit{baseline} model taking $U_i$ and producing $O_w = \{o_1, .., o_n\}$, the \textit{context} model taking $\mathcal{U}_c = \{U_{i-k}, .., U_i\}$ and producing $O_u$, as well as the \textit{joint} model taking $\mathcal{U}_c$ and producing $O_u$ and $O_w$ (Figure~\ref{fig:diarization-model}).
The baseline model does not create $e_c$, so $e_{i*}$ are directly fed to Softmax 2. Also, the baseline and context models do not use $e_i^\circ$, so only Softmax 2 is used to produce the outputs.
For evaluation, the F1-scores of the label \texttt{1} on the last two tokens are used.
All models are developed three times and their average scores and standard deviations are reported.


\begin{table}[htbp!]
\caption{Diarization model performance. Ours: trained on \texttt{TRN} of our Interview data (After) in Table~\ref{tab:diarization-data}. Transferred: trained first on the \texttt{TRN} mixture of Switchboard and BST, then finetuned on \texttt{TRN} of our data.}
\centering{ %\resizebox{\columnwidth}{!}{
\begin{tabular}{c|ccc}
\toprule
 & \multicolumn{1}{c}{\bf Baseline} & \multicolumn{1}{c}{\bf Context}  & \multicolumn{1}{c}{\bf Joint} \\
\midrule
Ours only   & 92.9$\pm$0.4 & 92.9$\pm$0.3 &     92.9$\pm$0.2 \\
Transferred & 93.2$\pm$0.3 & 93.4$\pm$0.3 & \bf 93.6$\pm$0.3 \\
\bottomrule
\end{tabular}}

\label{tab:diarization-results}
\end{table}

\noindent When trained on only our data, all models perform similarly.
The joint model slightly outperforms the others when transfer learning is applied.
Although the improvement is marginal, the joint model has a benefit of identifying utterances with diarization errors, showing the F1 score of 93.6\% for this task, while the transferred models generally show much higher performance on the other datasets than the non-transferred models.
Thus, the joint transferred model is used to auto-correct all dialogues in \texttt{RAW}.



\subsection{Dialogue Generation Results}

\noindent For our experiments, on the diarized data from the diarization model, the encoder and the decoder in BlenderBot 1.0 \cite{roller-etal-2021-recipes} are used.\footnote{There have been updated versions of BlenderBot introduced \cite{xu-etal-2022-beyond,https://doi.org/10.48550/arxiv.2208.03188}. However, we chose the first version for our experiments because we found it to be as effective yet much more efficient than the newer versions since the newer models focused on improvement on different perspectives, such as privacy and external knowledge incorporation.}
% \footnote{We also experimented with BlenderBot 2.0, which was supposed to consider a longer dialogue history \cite{xu-etal-2022-beyond}. However, its generation was too slow for a real-time system and our new techniques could not be easily integrated into, while it did not show noticeable improvement. Thus, we decided to build our final model with BlenderBot 1.0 instead.}
Three models are developed as follows:

\begin{itemize}
%\setlength\itemsep{0em}
\item BB: Blenderbot Baseline Model
\item SW: Blenderbot with Sliding Window
\item CT: Blenderbot with Sliding Window and Concatenation of Topic Storing
\end{itemize}

\noindent All models are first trained on \texttt{raw} and finetuned on \texttt{TRN} in Table~\ref{tab:annotated-data}). We followed the setups on the training parameters in the original Blenderbot paper.
To assess real-life performance, ten interviews are conducted per model, where each interview consists of exactly 30 turns. %  (for fair comparisons)
Qualitative analysis is performed on the top-3 most frequently occurring errors as follows:

\begin{itemize}
\item Repetitions: how often it repeats topics already covered in the previous utterances.
\item Early Ending (EE): implies ending the interview without covering a sufficient amount of topics.
\item Off Topic (OT): how often it makes utterances that are not relevant to the current topic.
\end{itemize}


\noindent Table~\ref{tab:interviewbot-error-analysis} shows the error analysis results.
The repetition rates are significantly reduced as the model gets more advanced.
Compared to the baseline, the CT model conducts 3.5 times longer conversations before it attempts to end the interview while generating twice fewer off-topic utterances, which is very promising.
Examples of these error types are provided in Appendix~\ref{app:generation-errors}.


\begin{table}[htbp!]
\caption{The error analysis of all generation models. R: avg-\% of repeated topics, EE: avg-\% of the interview conducted before the model attempts to end (higher is better), OT: avg-\% of off-topic utterances.}
\centering\small{ %\resizebox{\columnwidth}{!}{
\begin{tabular}{c|ccc}
\toprule
\bf Model & \multicolumn{1}{c}{\bf Repetitions (R)} & \multicolumn{1}{c}{\bf Early Ending (EE)} & \multicolumn{1}{c}{\bf Off Topic (OT)} \\
\midrule
BB & 30.0 & 13.3 & 20.0 \\
SW & 16.7 & 23.3 & 26.7 \\
CT & $\:\:$\bf 6.7 & \bf 46.7 & \bf 10.0 \\
\bottomrule
\end{tabular}
}

\label{tab:interviewbot-error-analysis}
\end{table}


\subsubsection{Static Evaluation}

\noindent Following previous work \cite{montahaei2019jointly}, static evaluation is performed on the CT model, where the input is every batch of $k$-utterances and prior topics per interview, and its output is compared to the corresponding human response in \texttt{TST} (Table~\ref{tab:annotated-data}).
The average \textsc{Bleu} score is 0.08 and cosine similarity is 0.19, which are low.
However, such static evaluation assesses each output independently and obstructs dialogue fluency by artificially inserting human utterances into the model, and thus, does not reveal its capability in conducting long contextualized interviews.

%To amend this shortcoming, the following evaluation is proposed.




\subsubsection{Real-time Evaluation}

\noindent The CT model is deployed to an online text-based platform in a public cloud.
For real-time evaluation, five professional interviewers and ten students are invited to have conversations with our InterviewBot and give ratings from 1 to 5 to indicate their overall satisfaction. 
The average dialogue duration is 256 seconds. 
Almost half of the evaluators are satisfied (Scores 4 and 5) and another 40\% indicate a positive attitude on the coverage of topics and discussions (Score 3), implying that it performs reasonably well for this realistic setting (Table \ref{tab:interviewbot-rating-scores}).
Overall, with an average score of 3.5, the InterviewBot has shown great potential in applying to practical applications.

\begin{table}[htbp!]
\caption{The rating distribution of the InterviewBot conversations for real-time evaluation. 5: very satisfied, 4: satisfied, 3: neutral, 2: unsatisfied, 1: very unsatisfied.}

\centering\small{
\begin{tabular}{c|ccccc|c}
\toprule
    \bf Score & \bf 5 & \bf 4 & \bf 3 & \bf 2 & \bf 1 & \bf Average Score \\
\midrule
     \bf Interviewer (Count) & 1 & 1 & 2 & 1 & 0 & 3.4 \\
\midrule 
    \bf Student (Count) & 2 & 3 & 4 & 0 & 1 & 3.5 \\
\midrule \bf Total (Count) & 3 & 4 & 6 & 1 & 1 & 3.5 \\
\bottomrule
\end{tabular}
}
\label{tab:interviewbot-rating-scores}
\end{table}

%\noindent The limitations are also summarized for future improvement.
%First, the InterviewBot has an early-ending issue in conversations, in which the ending utterances could be generated after a few turns, not covering sufficient discussions on diverse topics.
%Second, although InterviewBot has certain capabilities to follow up on topics brought up during the conversation, it is still expected to perform deeper discussions on more details.
%Third, InterviewBot cannot handle name entities, such as people's names, during conversations.
%Last, in some conversations, InterviewBot generates repeated or random ordering of words and punctuation.

