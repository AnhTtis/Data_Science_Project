\subsection{Dialogue Generation}
\label{sec:generation-model}

%Although recent advances in dialogue systems have shown their strengths in the effectiveness and flexibility of engaging users \cite{li2017confiding, Kim2019Comparing, finch2020emora}, it is rare to find an end-to-end dialogue system conducting unscripted interviews to elicit information from the users for afterward decision makings.

\noindent Figure~\ref{fig:generation-model} depicts an overview of our dialogue generation model.
Since inputs to the encoder $\mathcal{E}$ and the decoder $\mathcal{D}$ are limited by the total number of tokens that the pretrained language model accepts, \textit{sliding window} (Sec.~\ref{ssec:sliding-window}) and \textit{context attention} (Sec.~\ref{ssec:context-attention}) are proposed to handle long utterances and contexts in the previous utterances, respectively.
In addition, \textit{topic storing} is used to remember user-oriented topics brought up during the interview (Sec.~\ref{ssec:topic-storing}).
%In addition, we prefix \texttt{S}, \texttt{E}, and \texttt{Q} to the beginning, ending and question utterances for emphasizing their identities in the modeling.
The input to $\mathcal{E}$ and output of $\mathcal{D}$ include the speaker ID \texttt{S1}, \texttt{S2} or special tokens to indicate the beginning \texttt{B}, the ending \texttt{E} and topic questions \texttt{Q} as the first token followed by an utterance from the interviewer or interviewee, respectively. 
Hyperparameters are finetuned by cross-validations.

\begin{figure*}[htbp!]
\centering
\includegraphics[width=\textwidth]{img/generation-model.pdf}
\caption{The overview of our dialogue generation model.}
\label{fig:generation-model}
%\vspace{-0.5em}
\end{figure*}


\subsubsection{Sliding Window}
\label{ssec:sliding-window}

\noindent The sliding window technique aims to overcome the limitation of input length by separating a long sentence into multiple sections within between. The mathematical formulations are described below.
Let $n = m + e$ be the max-number of tokens that $\mathcal{E}$ and $\mathcal{D}$ accept ($e < m < n$).
Every utterance $U$ whose length is greater than $n$ is split into $U^1$ and $U^2$ as follows ($w_i$ is the $i$'th token in $U$):

\begin{equation*}
\begin{split}
U^1 & = \{w_1, \ldots, w_m, w_{m+1}, \ldots, w_{n}\} \\
U^2 & = \{w_{m+1}, \ldots, w_{n}, w_{n+1}, \ldots, w_{n+m}\} \\
\end{split}
\end{equation*}

\noindent In our case, $n$ = $128$, $m$ = $100$, and $e$ = $28$ such that $n+m = 228$ is sufficiently long enough to handle most utterances based on our stats.
$\mathcal{E}$ takes $U^1$ and $U^2$ then produces $E^1 = \{e^1_1, \ldots, e^1_{n}\}$ and $E^2 = \{e^2_{m+1}, \ldots, e^2_{n+m}\}$ where $e^*_i \in \mathbb{R}^{1 \times d}$ is the embedding of $w_i$.
Finally, the embedding matrix $E \in \mathbb{R}^{(n+m) \times d}$ of $U$ is created by stacking all of the following embeddings:

$$
\{e^1_1, \ldots, \frac{1}{2}\sum_{i=1}^2(e^{i}_{m+1}), \ldots, \frac{1}{2}\sum_{i=1}^2(e^{i}_{n}), \ldots, e^2_{n+m}\}
$$

\noindent For utterances whose lengths are less than or equal to $n$, zero-padding is used to transform $\mathcal{E}$'s output from $\mathbb{R}^{n \times d}$ to $\mathbb{R}^{(n+m) \times d}$.


\subsubsection{Context Attention}
\label{ssec:context-attention}

\noindent Let $U_i$ be the $i$'th utterance to be generated as output.
Let $C \in \mathbb{R}^{\ell \times d}$ be the context matrix stacking the embedding matrices of the previous utterances $\{E_{i-k}, .., E_{i-1}\}$, where $k$ is the number of previous utterances to be considered and $\ell = k(n+m)$. %\footnote{In our case, $k = 5$ is used, showing the best performance.}
The transpose of $C$ is multiplied by the attention matrix $A \in \mathbb{R}^{\ell \times n}$ such that $C^T \cdot A \rightarrow S^T \in \mathbb{R}^{d \times n}$.
Thus, $S \in \mathbb{R}^{n \times d}$ represents the context summary of $U_{i-k}, .., U_{i-1}$, which is fed into the decoder $\mathcal{D}$.


\subsubsection{Topic Storing}
\label{ssec:topic-storing}

\noindent Even with the context attention, the model still has no memory of contexts prior to $U_{i-k}$, leading it to repeat the same topics that it has already initiated.
To overcome this issue, topic storage is introduced to remember key topics derived by the interviewer.
Every interview in our data came with 8-16 questions by the interviewer annotated after each interview by the data provider, who used those questions during the interview and thought they led to assessing crucial aspects of the interviewee.
Our final model considers these questions the “key topics” and dynamically stores them as the dialogue progresses.
During training, these questions are converted into embeddings and stored dynamically as a list of topics discussed in previous turns.
During decoding, the model generates such topical questions with a specific flag and stores them in the same way.


Let $Q = \{q_1, .., q_h\}$ be the topical question set.
During training, $\mathcal{D}$ learns to generate \texttt{Q} instead of \texttt{S1} as the first token of the interviewer's utterance that contains any $q_i \in Q$.
In addition, it generates \texttt{B}/\texttt{E} if the interviewer begins/ends the current dialogue with that utterance (Table~\ref{tab:interview-example}).
Any utterance starting with \texttt{Q} is encoded by $\mathcal{E}$ and Feed-forward layers that create abstract utterance embedding $v_i \in \mathbb{R}^{1 \times d}$ to represent topics.
These embeddings get stacked as the interview goes on to create the topic matrix $V \in \mathbb{R}^{h \times d}$.
If $|Q| < h$, then zero-padding is used to create $V$ (in our case, $h=16$).
Finally, $V$ is stacked with the context matrix $C$ (Sec.~\ref{ssec:context-attention}), and $(V \oplus C)^T \in \mathbb{R}^{d \times (h+\ell)}$ is multiplied by the attention matrix $A \in \mathbb{R}^{(h+\ell) \times n}$ to create the transpose of the context summary matrix $S \in \mathbb{R}^{n \times d}$.





