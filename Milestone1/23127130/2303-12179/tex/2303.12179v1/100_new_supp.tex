\setcounter{table}{0}
\renewcommand{\thetable}{S\arabic{table}}%
\setcounter{figure}{0}
\renewcommand{\thefigure}{S\arabic{figure}}%
\setcounter{section}{0}
\renewcommand{\thesection}{S\arabic{section}}%

\addcontentsline{toc}{section}{Supplementary Text}
\section{Supplementary Text}\label{sec:supp_text}

\addcontentsline{toc}{subsection}{Language literacy and visual information consumption}
\subsection{Language literacy and visual information consumption}\label{sec:vis}

We examine the relationship between populations' language literacy levels and their interest in different types of content. Because our assessment concerns the ability to process textual content, we assume there exists a negative relationship between a population's literacy estimate and their attention toward non-textual (e.g., visual) content. Fig.~\ref{fig:lit_visual} shows the correlations between countries' \olles ($x$-axes) and the relative time spent on visual content by the countries' Facebook population ($y$-axes).  As expected, populations with a lower level of language literacy tend to spend relatively more time on visual content. The global correlation is $-0.38$ (Spearman's rank correlation; $p<0.001$), with correlations over different areas ranging from $-0.29$ (Latin America \& the Caribbean; $p<0.062 $) to $-0.62$ (Europe/Oceania/Northern America; $p<0.001$)\footnote{As two of the seven geographical groups only have few countries, we merge the seven groups into five (based on proximity) to provide an adequate statistical description.}.

\addcontentsline{toc}{subsection}{Elbow range detection in popularity curves}
\subsection{Elbow range detection in popularity curves}\label{sec:elbow}
\Bigword are determined based on ``elbow'' range on the word popularity curve for each language, where the relative word frequencies begin to have a systematic decline. 
\yrepj{Fig.~\ref{fig:word_curve} shows the popularity of words in decreasing order, i.e., from the most to the least popular word, as popularity curves. It can be seen that a systematic decline in the word popularity appears at the point of maximum curvature in a popularity curve. In other words, the interest region associated with \bigword corresponds to the ``elbow'' (or ``knee'') point on the smoothed word popularity curve. Mathematically, the curvature is a mathematical measure of how much a function differs from a straight line \cite{satopaa2011finding,antunes2018knee}. Estimating the knee/elbow point for a continuous function is straightforward since the curvature is well-defined for continuous functions; however, it is a challenging task for discrete data. It is also an inherently heuristic process \cite{antunes2018knee}. To reliably detect the elbow range, we leverage the ``Kneedle'' detection \cite{satopaa2011finding}, an efficient algorithm that can efficiently detect knee points in discrete data, and the standard maximum curvature approach on a smoothed function learned from the discrete points. First, we employ generalized additive models with cross-validation to learn a smooth function for each of the popularity curves \cite{hastie1990generalized}. As shown in Fig.~1 E-G, we define an elbow range as an area between two points $k_0$ and $k_1$ (highlighted in red) that best describe the systematic decline in the curve. The two points were determined by combining two heuristic methods: (i) the standard maximum curvature points that can be calculated from any continuous function, and (ii) the approximate knee points (Kneedle detection method) based on the notion that knee points differ most from the straight line connecting the curve's two endpoints \cite{satopaa2011finding}. Note that while the two notions may be considered to be conceptually similar, the approximate knee points (the second notion) are not necessarily the maximum curvature points especially when the curves are skewed. In a right-skewed curve (as in the case of a word popularity curve), the approximate knee points tend to fall into the right of the maximum curvature points. Thus, we detect an elbow by two points $k_0$ and $k_1$ through maximum curvature measurement and approximate knee point detection method respectively. Unlike other knee/elbow detention methods that are sensitive to noises and rescaling, we found this hybrid approach is more robust to rescaling and small fluctuations in our data.} 
Words with ranks falling into the elbow range are considered to be the ``\bigword'' in the language. Fig.~1 A-C highlights the elbow range detected from the word popularity curve for each of the three most used languages, and Fig.~\ref{fig:word_curve} shows the detected elbow ranges for all 12 languages. 

\addcontentsline{toc}{subsection}{Procedure for estimating online language literacy}
\subsection{Procedure for estimating online language literacy}\label{sec:procedure}

For a given population with a given language, the procedure to measure the collective language literacy involves the following steps:

\begin{enumerate}
\item[(i)] Processing of user-generated texts: We use public posts written in any of the chosen languages created by Facebook users who are at least 18 years old and active during a 30-day period between April 20 and May 20, 2020. We exclude posts that did not contain any text or text that was shorter than 2 characters or longer than 1000 characters, as well as posts that contained URLs as these are more likely to be copied and pasted from other sources rather then composed by users. 

\item[(ii)] Aggregate statistics per user: After tokenizing the public posts, for each user, we quantify the number of unique words (unigrams) that falls in the range of \bigword. We then obtain a relative \bigw count $w_u$ that is normalized by the active level of post creation per user, i.e., $w_u$ is given by (the total number of \bigword observed from $u$'s public posts) / (the total number of $u$'s public posts). We count each unigram once per user, regardless of the frequency used, to avoid overestimating the use of particular words or the inflation from copy-pasted content.

\item[(iii)] Aggregate statistics per population: For each geographically bounded community (e.g. a county or a region) with at least 1000 active users observed in the study period, the population-level estimate is calculated as $\bar{w}$, the average of $w_u$'s over all active users $u$'s in the geographically bounded community. The gender- or region-disaggregate population-level estimates also require a minimum of 1000 active users observed in the study period in any of the disaggregate groups. The threshold of 1000 unique users from any group was chosen to ensure user privacy and the statistical power of our method. We also exclude users who produced a high volume of posts (above 75 percentiles) to avoid a small number of highly productive users dominating the measurement.
\end{enumerate}

Throughout the procedure, none of the personal identifiable information or any personal or private content was used. Only the aggregate statistics $w_u$ and $\bar{w}$ were generated from the process. 

\paragraph{How to retrieve pre-computed \bigword}
\yrrr{\bigword are determined based on the word popularity curves derived from the Facebook users' use of the up 200,000 most frequent words in each language. These pre-computed \bigword can be retrieved through the following steps:}
\begin{itemize}
\item[(i)] Install the fastText\footnote{\url{https://fasttext.cc/docs/en/python-module.html\#installation}}
\item[(ii)] Run \texttt{download\_model.py \$lang} to get the dictionary of a specific language, where \texttt{\$lang} is the language indicator (e.g., {\texttt en} for English, {\texttt es} for Spanish). This script will download the dictionary in a binary file (let \texttt{\$filename} be the filename of the downloaded file).
\item[(ii)] Run \texttt{fasttext dump \$filename dict > \$ofilename} to convert the binary dictionary file to a text file (let \texttt{\$ofilename} be the filename of the output text file). This file contains up to 200,000 lines where each line is a word and its frequency. The frequencies can be used to rank the words from the most to the least frequent. 
\item[(iv)] Extract the \bigword based on the knee points listed in Table~\ref{tab:knees}. For example, the \bigword in English correspond to the words in the fastText dictionary that are ranked between 5,000 to 9,000 in the decreasing order of word frequency.
\end{itemize}


\input{101_india}
\input{102_robustcheck_multi_lang}

\input{901_figures}
\clearpage
\input{901_tables}
\clearpage
