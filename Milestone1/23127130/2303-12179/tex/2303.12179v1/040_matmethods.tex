\section{Methods}\label{sec:method}

% \color{red}
\subsection{Facebook dataset}\label{sec:dataset}
To obtain a written sample of online populations worldwide, we collect public posts written in any of the 12 chosen languages created by Facebook users who are at least 18 years old and active during a 30-day period between April 20 and May 20, 2020. To ensure that the collected posts represent the writings of individual Facebook users, we exclude posts made by pages, organizational accounts, and public profiles. We also exclude posts that did not contain any text or text that was shorter than 2 characters or longer than 1000 characters, as well as posts that contained URLs as these are more likely to be copied and pasted from other sources rather than composed by users. \swvv{After pre-processing the text, the median length of posts in our dataset is 18 characters, the average is 51 characters, and the 95th percentile is 187 characters.}

%\wam{While we did not obtain explicit consent from Facebook users to contribute their data, use of this data for this project is permitted under the terms of service on the platform, and user privacy and anonymity was ensured throughout data collection and processing.}
%None of the personal identifiable information (PII) was collected during data collection and processing. No personal or private posts were collected or included in our dataset. 

All the analysis and statistics were computed over a populational aggregate, based on self-reported attributes such as country and gender. \olle were only calculated for groups with at least 1,000 active users, to minimize the risk of user de-anonymization. The original text, together with the intermediate results, were dropped after populational level \olle and other statics were calculated. 

More details about Facebook public post data collection and use can be found in \smappsec{sec:procedure}.
\color{black}

\subsection{Estimating online language literacy}\label{sec:estimate}
We use the vast amount of text produced by Facebook users to quantify populations' online language literacy skills. This method relies on two assumptions: (i) literacy across populations may be measured collectively by aggregating the observed data within country or regional borders, and (ii) vocabulary usage patterns observed in corpora of written texts can be used as a proxy for language practice in a digital environment. The first assumption comes from prior observations that literacy skill is locally clustered, largely determined by the socioeconomic status of the local community and by the abundance of public resources such as public school systems and libraries \cite{kutner2007literacy}. This also alleviates the need to analyze individual-level data, which are harder to de-identify, and often too sparse for reliable estimates. The second assumption is motivated by the relationship between literacy and vocabulary knowledge found in the education domain \cite{lee2011size,curtis2006role,ouellette2006s,national2012improving,schmitt2013introduction}. In practice, vocabulary size was often employed as a proxy for literacy skill, as reading comprehension cannot be achieved unless the reader knows 95\% of the words in the text \cite{lauren1989special}, and a certain vocabulary size is required for unassisted text comprehension \cite{nation2006large}. Vocabulary knowledge of words at various frequency levels has been used to measure total vocabulary size. For example, the Vocabulary Size Test (VST) tests a learner's knowledge of 140 or 200 words, with 10 words sampled from each 1000 word in frequency levels based on British National Corpus \cite{beglar2007vocabulary}. 


Analogous to the VST method, we \swepj{measure the aggregated use of lower-frequency words (``\bigword'') - secondary vocabulary words outside the high-frequency everyday lexicons - in public Facebook posts as a proxy for online populations' literacy skills in a given language.} \yrepj{Intentionally designed as a population-level, aggregated measure, our approach does not collect any personally identifiable information or any personal/private content (see \smappsec{sec:procedure}).} For maximal geographical coverage, we pick the twelve (12) most widely used languages (in terms of countries) and algorithmically define a set of lower-frequency words for each language. The 12 selected languages are: Arabic (ar), German (de), English (en), Spanish (es), French (fr), Italian (it),  Malay (ms), Dutch (nl), Portuguese (pt), Russian (ru), Turkish (tr), and Chinese (zh). The next section will detail our method to determine the sets of \bigword based on a multi-lingual reference corpus.

% \subsection*{Reference Language Corpus}
\subsection{Detecting \bigword from a reference language corpus}\label{sec:bigword}
\swepj{While the VST method relies on the British National Corpus (BNC) for baseline word-frequencies,} we use the fastText unigram data \cite{grave2018learning} as our reference corpus. \swepj{Comparing to other popular language corpora such as BNC and Google Books Ngram~\cite{google-ngrams}, fastText has two major advantages:} (i) coverage: The distributed fastText data currently supports 157 languages and covers all 12 languages considered in our study. (ii) \swepj{up-to-date representation of vocabularies used by online population: fastText is based on texts collected from Wikipedia and Common Crawl~\footnote{https://commoncrawl.org/}, containing petabytes of web page data collected since 2008.}  
 
Using fastText data, we are able to retrieve up 200,000 most frequent words in each language as the candidates for \bigword in that language. \swepj{Understanding that the exact range of \bigword may vary depending on the language, we first try to understand the use of these top 200,000 words by Facebook users through language-specific ``word popularity curves'':} a scatter plot where the x-axis represents the rank of a word by its frequency (0 for the most frequent), and the y-axis represents ``word popularity'', as measured by the percentage of unique users in the language population who have used that word in our data. Fig.~\ref{fig:word_curve} (\smapp) shows the word popularity curves for the 12 chosen languages in our study. 

 \swepj{As illustrated in Fig.~\ref{fig:word_curve}, the word popularity curves first decline sharply for the most frequent words before settling into a relatively flat region. The transitional area - the ``elbow'' (or ``knee'') region of the smoothed word popularity curve - corresponds to the words that are neither too popular nor too unpopular, thus ideally covering the set of \bigword for that language. Mathematically, the curvature is a mathematical measure of how much a function differs from a straight line \cite{satopaa2011finding,antunes2018knee}, and the elbow areas start at the point of maximum curvature in a popularity curve. Estimating the knee/elbow point for a continuous function is straightforward since the curvature is well-defined for continuous functions; however, it is a challenging task for discrete data. We leverage the ``Kneedle'' detection \cite{satopaa2011finding}, an efficient algorithm that can efficiently detect knee points in discrete data, and the standard maximum curvature approach on a smoothed function learned from the discrete points. We found this hybrid approach is more robust to rescaling and small fluctuations in our data. Details of this elbow detection is given in \smappsec{sec:elbow}.}



Fig.~\ref{fig:eval} A-C highlight the elbow range detected from the word popularity curve for each of the three most used languages (English, Spanish, and Arabic). Words falling into the elbow range are defined as the ``\bigword'' in the language. Fig.~\ref{fig:word_curve} shows the detected elbow ranges for all 12 languages. 


\subsection{Calibration for language use and bias}\label{sec:adjust}
After determining the \bigword per language, we can calculate the relative frequency of \bigword, among all the public text posted by Facebook users from a given country in a given language. We denote the relative frequency of \bigword as $\bar{w}_{c,l}$, with $l$ representing the 12 languages considered in this study and $c$ representing the 167 countries whose official or dominant languages are among these languages. 

While $\bar{w}_{c,l}$ makes it possible to track online language literacy for multiple languages in parallel, we decide to use one representative language per country in all our analyses for simplicity. For most countries, the official language is chosen as the representative language. For countries with no or multiple official languages, we use the language that is used by most users in that country. For example, for India, a country that uses both Hindi and English as official languages, we use English as the representative language for India since English was used by the largest number of Facebook users in India based on our data. Fig.~\ref{fig:countries_covered} (\smapp) shows the number of countries broken down by dominant language.


\swepj{As validation, we group countries by the representative language and compare the value of $\bar{w}_{c}$ with the official literacy rate data for each country. After excluding countries where the official literacy rate is not available or where the Internet penetration is lower than 25\% percent, the three most widely used languages (in terms of the number of countries covered) in our data are English (41 countries), Spanish (19 countries), and Arabic (15 countries). Fig.~\ref{fig:eval} D-F shows the relationship between the officially reported literacy rates and $\bar{w}_{c}$, for English, Spanish, and Arabic, respectively: each dot represents a country, with $x$ value corresponding to the rank-based quantile normalization\cite{beasley2009rank} of $\bar{w}_{c}$, and $y$ value corresponding to the rank-based quantile normalization of its official literacy rate. The shadow areas in Fig.~\ref{fig:eval} D-F visualize Spearman's rank correlation coefficient between the two variables, with 95\% CIs\footnote{\yrrr{Without specification otherwise, the confidence interval for all the correlation coefficients are produced using a nonparametric bootstrap procedure based on the percentile method (with 1000 bootstrap replicates).
}}. As seen in Fig.~\ref{fig:eval} D-F, all three sets of countries exhibit strong positive correlations, though the Spanish-speaking countries show a larger variance than countries mostly using English or Arabic. The positive correlations between $\bar{w}_c$ and the reported literacy rate in the three top languages suggest the efficacy of using the relative \bigw count as a measure of literacy across the different most-used languages.}
The correlations for other languages are not reported because none of the remaining languages cover more than 7 countries.

To generate a global online literacy estimate that is directly comparable across languages, we also calibrate $\bar{w}_{c}$ for all countries using the official literacy data collected by UNESCO~\cite{unesco-literacy-data}. 
Given that the two measures have different distributions, both $\bar{w}_{c}$ and the official literacy data were transformed to better fit a normal distribution. A rank-based ordered quantile normalization transformation \cite{beasley2009rank} was used for both $\bar{w}_c$ and the literacy rates, where the transformation $g(\cdot)$ is given by $g(x_i)=\Phi^{-1}\left(\frac{r_i-0.5}{n}\right)$ where $\Phi$ refers to the standard normal CDF, $x_i$ is a continuous measurement observed for each object $i$, $r_i$ refers to the sample rank of $i$ when the measurements are placed in ascending order, and $n$ refers to the number of observations. In the case of new values that fall outside the observed domain of $x$, we adopt the standard procedure and use generalized linear models to estimate the ranks beyond the bounds of the original domain of $x$. 

A linear fixed effect model is employed to quantify the systematic differences across languages {\it in relation to the offline literacy rates}. Let $l^{(i)}$ be the representative language for a (country) population $i$, the calibrated estimate, denoted as $\hat{w}_i$, is given as 
$\hat{w}_i \propto {y}_{il}$, where
\begin{align*}
{y}_{il} &= \beta x_{il} + \alpha_l + \epsilon_{il},
\end{align*}
where $x_{il} = g(\bar{w}_i)$ is the direct literacy estimate of population $i$, $\alpha_l$ is the language-specific effect, $\epsilon_{il}$ is the idiosyncratic error term with $\epsilon_{il} \sim \mathcal{N}(0,\,\hat{\sigma}_{\epsilon}^{2})$, and $\beta$ is the parameter. \wam{After $\beta$ is learned, \textbf{the global online language literacy estimate (\olle) is the calibrated estimate $\hat{w}_i$}, obtained by rescaling ${y}_{il}$ between 0 and 1.} This rescaling step is to make the \olle value more interpretable and to facilitate the comparison across populations and subpopulations by a single index. 

Since the information about the official literacy rates has been used in the calibration, to get an unbiased evaluation, we use the leave-one-out procedure to obtain an out-of-sample evaluation -- for each $i$, the calibrated estimate was generated by using all countries other than $i$ in the model (the estimated parameters can be found in Model (a) in Table \ref{tab:model-lit-main}). In other words, the language calibration is calculated from the residualized mean aggregated over the rest of the countries using the same language as $i$. 

\swepj{While the official literacy rates data has served an important role in our study to validate and calibrate \olle, they are not suitable as target variables for building a predictive model for online language literacy, for a few reasons: 1) conceptually, we want to distinguish online population literacy and general population literacy in this study; 2) technically, the official literacy rates data were collected for different countries at different time, with methodological variances over the years, introducing extra noise and latent variables for a robust supervised model.}


\subsection{Country-level socioeconomic covariates}\label{sec:socioecon}
To understand the relationship between literacy and other social factors, we collect information about countries' socioeconomic status and technical development from multiple data sources. Tables~\ref{tab:gender_corr} and \ref{tab:region_corr} list all variables used in our study, with definitions and the sources where the variables are gathered. The first-order correlations among these variables are provided in Table~\ref{tab:gender_corr}. Due to the heterogeneous distributions across variables, we report correlations using Spearman's rank correlation coefficient unless otherwise stated.

\swepj{Inspired by previous research showing the effect of a country's income, Internet penetration, and other gender inequality measure on the digital gender gap~\cite{fatehkia2018using}, we study the relationship between social factors and online literacy gaps through regression analysis with the gender or regional differences in \olle as the dependent variable and country-level variables as the independent variables.}
All the regression models presented in this work are based on standard OLS estimates, where all variables are first separately transformed to better fit a normal distribution. For example, the income variable was transformed logarithmically. Considering the geographical clustering of many of the socioeconomic variables, in each of the regression analyses, we provide models with and without controls for geographical groups. The control for geographical groups signifies whether the pattern observed in our study is a global phenomenon or particular to certain areas. For example, when predicting the gender gap in \olle, the coefficient estimates are consistent between the models with and without geographical information. We show in \smapp Tables~\ref{tab:model-gender-main}-\ref{tab:model-gender-3v-geo} the detailed estimates of regression models and their comparisons. The consistent coefficient estimates are also found in predicting regional disparity (\smapp Tables~\ref{tab:model-region-main}-\ref{tab:model-region-4v}).




