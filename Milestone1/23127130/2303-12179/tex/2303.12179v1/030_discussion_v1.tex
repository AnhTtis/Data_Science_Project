\section{Discussion}\label{sec:discussion}

\paragraph{A data-driven, cross-language, cross-country online literacy estimation}
Taking advantage of the abundance of user-generated text online, our proposed methodology of measuring online language literacy \yrv{can be scaled across languages and subpopulations, as long as population-level text corpora are available. \olle complements the traditional sources and makes it possible to} monitor future progress and answer questions such as: whether the online language skills improve faster (slower), or whether the literacy gap is closing (widening), particularly in low-literacy countries. \swepj{While our current dataset only contains text generated within a 30-day period, further collection of similar data over a longer period of time will offer new insights on the temporal evolution of \olles across the world.}

\paragraph{Tracking global trend in online language literacy}
This study reveals the current state of global online language literacy. Based on our study, the estimated global online language literacy has remarkably high correlation with documented country-level literacy rates and educational attainments data (with $\rho=0.78$ in both correlations; see Fig.~\ref{fig:eval} and \smapp Fig.~\ref{fig:corr_lit_edu}). This finding has two implications. First, given that 86\% of the world population are now reportedly literate (i.e., able to read and write)~\cite{unesco2017}, our study suggests the variation in language skills remains {\it within the literate, online population}. Even though many countries now have more than 95\% literate populations, our online literacy map (Fig.~\ref{fig:lit_map}) has revealed the nuanced differences among the online population's language skills in these countries. Second, beyond a few options in assessing a country's digital advancement, such as the Internet penetration measure, \olle's robust correlation with offline literacy and educational data make it a more relevant alternative for tracking the {\it outcome} of a country's access to education and resources for global literacy development. 

\paragraph{Women's empowerment, social inequalities, and online language literacy disparities}
In our study, we show that the gender difference in \olle is significantly correlated with various offline gender parity metrics, including gender gaps in literacy rate, education, GII (which also considers the economic standing across gender groups), and women's civic participation index. This suggests that a country's offline gender equity progress is crucially relevant to how literate populations across genders participate online. In contrast to existing studies that exposed the well-known correlation between the gender gaps and a country's economic and technical development \cite{world2020global,fatehkia2018using}, we find that the link is not trivial. The relationship between countries' Internet penetration and the gender gap in \olle is {\it not monotonic}, and only when there is a sufficiently high level of women participation in civic society does the \olle gender gap align with countries' Internet access. In countries with a low level of women's civic participation, the \olle gap favoring men persists even with the rise of the overall Internet penetration (Fig.~\ref{fig:gender_model}B). This finding highlights the crucial social condition that allows more literate women to participate online. We also observe non-trivial relationships among multiple inequalities in our analysis of within-country regional disparity. The regional disparity in \olle is positively associated with unequal education, but the relationship is not simple when comparing countries with different levels of income inequality -- for example, a lower disparity \olle may reflect the homogenized language skills from only the economically advantaged subpopulations, or those from only more educated subpopulations. Our study explicates the complex relationship between the multidimensional inequality measurements and their manifestation on digital populations' online literacy skills.


\subsection{Limitations and future research opportunities}\label{sec:discussion}
We discuss the limitations of this study and highlight where study results must be interpreted with caution, as well as future research opportunities.

\paragraph{Self-selection bias in Facebook data}
Traditional literacy surveys are expensive to implement and many areas of the world have limited resources for survey research. The challenge for gathering nationally representative samples is not unique to traditional survey research; more recent assessments -- for example, PIAAC, which was predominantly administered on computers, were subject to selection effects and therefore required additional adjustment \cite{yamamoto2013scaling}. Our analyses are not immune from self-selection bias where the use of Facebook varies in popularity across different demographics and the differences also vary with countries and regions \cite{Facebook2020Q2earning:online}, as well as user subcultures. \yrv{For example, the observed gender gaps may be due to the over-representation of more privileged women online  \cite{magno2014international,kashyap2021analysing}.}
On Facebook, a user's comfort level of posting likely depends on their language skills; those with very limited vocabulary may not be in the data, or may choose to communicate via other modalities, e.g., images or videos. 
This study only considers users' text-based interactions on Facebook and thus the estimates likely miss out on people at the low end of vocabulary skills. To some extent, sampling bias may be mitigated by post-sampling weightings with demographic information, as has been demonstrated in recent data-driven studies \cite{park2019global}. Such an approach nevertheless depends on sufficiently rich demographic information in the data. In our study, only data disaggregated by gender and coarse-grained geographical grouping are available. Future work may consider tackling the selection bias by separately collecting users' information on demographics and their social media interaction practice. 

\paragraph{Representative languages and language-based calibration}
\yrepj{In the current study, a country's online language literacy was measured based on a single representative language (either the official language or the most used language). One potential risk of relying on a single representative language is that the regional disparity measure in a multilingual country may simply capture the distribution of languages, rather than the diversity of language skills. To address this concern, we perform robust checks in the \smappsec{sec:domi} and Fig.~\ref{fig:domi} and do not see systematic biases associated with different penetration rates of the representative language. }

\yrepj{Another potential risk is to underestimate the language literacy of countries that have sizable language minorities (including people who use/speak a dialect), multilingual communities, or multiple monolingual subpopulations, since their data are largely excluded from our methodology.}  For example, an English-majority country with a larger Spanish-speaking population may score lower in a measure of English-language literacy skills. For such countries, focusing on improving a dominant-language literacy measure can be potentially harmful, since more resources may be allocated in favor of the dominant language.  
\swepj{In \smappsec{sec:india}, we present a case study using India as an example of a multilingual country and show that literacy estimation based on multiple languages has neglectable improvement over English-based estimation in its correlation with the official literacy data (see Fig.~\ref{fig:domi}). However, we acknowledge the official literacy data often have a bias against language minorities and recommend future work consider measuring the online language skills separately for all languages used by sizable populations within a country, to better understand the literacy skills and needs across diverse communities.}


\swepj{The use of official literacy data for cross-language \olle calibration also introduces potential biases and noises.} As mentioned in Section \ref{sec:world}, the post-calibration \olles for the Russian-speaking countries are likely to be overestimated due to their historically high literacy rates in the official data. On the other hand, \olles for Arabic-speaking countries be underestimated due to the fact that the alternative learning (e.g., religious education) provided in those countries was not included in the official literacy data. Languages concentrated in only one or a few countries, such as Japanese and Korean, are not considered in our study due to the lack of benchmark data that can be used for validation or calibration. Therefore, to establish an adequate common scale for more languages, future research will benefit from more comprehensive and up-to-date data for literacy skills across languages.

\paragraph{Thresholding vs. continuum measurement}
Our measure relies on thresholding the observed word frequency bands -- i.e., the set of \bigword was identified by the automatically determined word frequency cut-offs -- but one may also consider the continuum of the word frequency range. Our choice of focusing on particular word frequency bands is aligned with the existing literature in language comprehension research. For example, studies from English language comprehension distinguish the utility of high-, mid-, and low-frequency vocabulary: the high-frequency vocabulary (e.g., the most frequent 2000- or 3000-word families from a particular English corpus) provides the largest lexical coverage of any text but is not sufficient for adequate reading comprehension, while the low-frequency vocabulary (including the words over the 9000-word families) is too infrequent and thus has very limited utility; only the mid-frequency vocabulary gives the important range of words required for reading authentic materials \cite{nation2006large,masrai2019vocabulary}. \yrepj{However, different vocabulary sets may serve significant functions for different populations; for example, high-frequency vocabulary has been shown as an important source of knowledge for second-language learners \cite{masrai2019vocabulary}.} Future work may take into account the continuum of the word frequency range and investigate the level of contribution provided by the various word frequency bands to online language skills.


\paragraph{Heterogeneity in social media texts}
A potential concern about using social media text to measure the language skills of a population is how to deal with social media users' heterogeneous behaviors, e.g., some users may post more than others, and some tend to copy content from elsewhere, which could disproportionally impact the population-level measurement.
\swepj{We adopt a few methods to address this concern, including counting each unique unigram once per user, and leaving out posts that are likely to be copy-pasted (see \smappsec{sec:procedure} for more details). However, we did not perform efficacy evaluation for these methods, and would encourage} future work further examine the impact of text recycling and text production disparities for online literacy assessment. 

\paragraph{Aggregate vs. individual measures, and correlations}~\olles are generated based on aggregate data, which inherently poses risks of ecological fallacy compared to other literacy data collected through individual-level tests and surveys. 
In our study, the between-country correlations only involve the between-country differences in aggregate statistics of the within-country distributions, and the unmeasured within-country measures could be uncorrelated, or could even be correlated in the opposite direction. Taking into account individual assessment in a multi-level analysis \yrrr{with a proper privacy protection mechanism} may be a fruitful direction to reduce aggregation bias and the ecological fallacy in future research.
In the case of the observed association between the gender gap in \olle and women's civic engagement, a less ambiguous interpretation -- whether higher literacy empowers women for civic engagement, or civic engagement leads to legal and institutional changes that enhance literacy, or other cultural, religious, political, and socio-economic conditions influence both women's civic engagement and progress in online language literacy -- requires further research to carefully examine the causal pathways.




\subsection{Conclusions}
This work develops a scalable language literacy measurement to monitor the collective language literacy of the online population using social media data from more than 160 countries. The measure then allows for tracking the trends and inequalities in online language literacy and their relationships with various socioeconomic conditions. Our findings identify key regions and populations disproportionally impacted by literacy challenges, and suggest that education or technical infrastructure alone is not sufficient to explain the variance in online population language literacy skills. Our study calls out the need for more attention and resources to be allocated to populations with limited online literacy skills -- especially those who also suffer from poverty, low resource, and other structural discrimination, to empower them through global challenges such as misinformation and social inequality, and to sustain the overall progress in democratic and socioeconomic development.

