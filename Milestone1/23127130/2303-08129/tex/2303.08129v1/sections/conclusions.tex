\section{Conclusion}

\label{sec:conc}

In this work, we demonstrate PiMAE's simple framework is an effective and highly interactive multi-modal learning pipeline with strong feature extraction abilities on point cloud and image. 
We design three aspects for promoting cross-modality interaction. First, we explicitly align the mask patterns of both point cloud and image for better feature fusion. Next, we design a shared-decoder to accommodate mask tokens of both modalities. Finally, our cross-modality reconstruction enhances the learned semantics. In our extensive experiments and ablation studies performed on datasets of both modalities, we discover that PiMAE has great potential, improving multiple baselines and tasks.
