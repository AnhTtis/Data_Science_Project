
\begin{figure} 
    \centering 
    \includegraphics[width=0.47\textwidth]{images/intro.pdf}
    \caption{With our proposed design, PiMAE learns cross-modal representations by interactively dealing with multi-modal data and performing reconstruction.} 
    \label{Fig.main2} 
\end{figure}

\section{Introduction}

\label{sec:intro}
%0. Problem

The advancements in deep learning-based technology have developed many significant real-world applications, such as robotics and autonomous driving. In these scenarios, 3D and 2D data in the form of point cloud and RGB images from a specific view are readily available. Therefore, many existing methods perform multi-modal visual learning, a popular approach leveraging both 3D and 2D information for better representational abilities.

Intuitively, the paired 2D pixels and 3D points present different perspectives of the same scene. They encode different degrees of information that, when combined, may become a source of performance improvement.
Designing a model that interacts with both modalities, such as geometry and RGB, is a difficult task because directly feeding them to a model results in marginal, if not degraded, performance, as demonstrated by~\cite{P4Constrast }.

In this paper, we aim to answer the question: 
how to design a more interactive unsupervised multi-modal learning framework that is for better representation learning?
To this end, we investigate the Masked Autoencoders (MAE) proposed by He \etal~\cite{he2022masked}, which demonstrate a straightforward yet powerful pre-training framework for Vision Transformers\cite{vit} (ViTs) and show promising results for independent modalities of both 2D and 3D vision \cite{convmae, bachmann2022multimae, M3AE, pointm2ae, zhang2022i-mae}.  
However, these existing MAE pre-training objectives are limited to only a single modality.

%On the other hand, contrastive methods were proposed for unsupervised 3D point cloud and 2D representation learning, such as \cite{xie2020pointcontrast, chen2021mocov3, chen2020simple}. Ensuing, P4Contrast~\cite{P4Constrast,crosspoint} addresses cross-modal learning with point-pixel pairs. However, these contrastive multi-modal methods highly depend on negative samples and augmentation, which is impractical in real scenarios. Moreover, contrastive methods fall short in the generation of negative samples that causes sampling bias, as proven by \cite{NEURIPS2020_63c3ddcc,zhao2021graph,hong2021unbiased}. 



%Add Why contrastive bad.
%Moreover, in common contrastive methods, since the ground-truth labels are not available, negative samples are often generated from the training data, causing similarity between the training data, and leading to the problem of sampling bias, which harnesses the performance of the network. Although \cite{NEURIPS2020_63c3ddcc} proposed a debiased contrastive learning method to ease the problem, the ideal unbiased objective is still unreachable.

%serves to learn useful representations from RGB-D data for fine-grained tasks . on both 3D and 2D visual understanding.

%Problem with related works.

%. Multi-modality is also argued to be employed by biological organisms to develop resilience and better representations [21, 22, 74]

%Related works.
%1. Why SSL with pointcloud - important applications, however, data hard to get -> thus need SSL pretraining that can be better applied to finetune. Learn abundant data without lables; labels are expensive. 

%3. Why Multi Modality, why not just pointmae? 


%can MAE-based self-supervised frameworks fully leverage the multi-modal data provided RGB-D scans for 3D scene understanding and 2D representation learning? 
%Challenges in point cloud and image
% next existing problems in work
%Posing question of design.
% Next proposing our method.
% Joint encoder in our methods, specific problems, motivations.
% Interactive, 少强调self-supervised, RGB-Lidar point cloud.


While much literature has impressively demonstrated MAE approaches' superiority in multiple modalities, existing methods have yet to show promising results in bridging 3D and 2D data. For 2D scene understanding among multiple modalities, MultiMAE~\cite{bachmann2022multimae} generates pseudo-modalities to promote synergy for extrapolating features. Unfortunately, these methods rely on an adjunct model for generating pseudo-modalities, which is sub-optimal and makes it hard to investigate cross-modality interaction. %when RGB-D datasets have 2D and 3D pairs of readily available Moreover, the existing methods in multi-modality learning \cite{huang2022unsupervised} merely independently embed latent representations into separate spaces with little connection. 
On the other hand, contrastive methods for self-supervised 3D and 2D representation learning, such as \cite{xie2020pointcontrast, chen2021mocov3, chen2020simple, P4Constrast,crosspoint}, suffer from sampling bias when generating negative samples and augmentation, making them impractical in real-world scenarios~\cite{NEURIPS2020_63c3ddcc,zhao2021graph,hong2021unbiased}. 

% Motivation: Many Multi-modal methods don't do focus on 

% MAE multi, not with point cloud.

% Learn 2D knowledge to help 3D understanding, finetuning doesn't need image.

% Learn strong OOD, transferability.
% 互相cooperation, synergy, 互补信息, 充分挖掘信息。

To address the fusion of multi-modal point cloud and image data, we propose PiMAE, a simple yet effective pipeline that learns strong 3D and 2D features by increasing their interaction. Specifically, we pre-train pairs of points and images as inputs, employing a two-branched MAE learning framework to individually learn embeddings for the two modalities. To further promote feature alignment, we design three main features.

%4. If it is for PiMAE, why is it difficult / novel: alignment, information loss, symmetrical structure.

%Sample bias, negative sampling from same class. debias negative sampling.

First, we tokenize the image and point inputs, and to correlate the tokens from different modalities, we project point tokens to image patches, explicitly aligning the masking relationship between them. We believe a specialized masking strategy may help point cloud tokens embed information from the image, and vice versa.
%
Next, we utilize a novel symmetrical autoencoder scheme that promotes strong feature fusion. The encoder draws inspiration from \cite{AIST}, consisting of both separate branches of modal-specific encoders and a shared-encoder. However, we notice that since MAE's mask tokens only pass through the decoder\cite{he2022masked}, a shared-decoder design is critical in our scheme for mask tokens to learn mutual information before performing reconstructions in separate modal-specific decoders.
Finally, for learning stronger features inspired by~\cite{zhang2022learning,gaomimic}, PiMAE's multi-modal reconstruction module tasks point cloud features to explicitly encode image-level understanding through enhanced learning from image features.

To evaluate the effectiveness of our pre-training scheme, we systematically evaluate PiMAE with different fine-tuning architectures and tasks, including 3D and 2D object detection and few-shot image classification, performed on the RGB-D scene dataset SUN RGB-D~\cite{sunrgbd} and ScannetV2 \cite{dai2017scannet} as well as multiple 2D detection and classification datasets. We find PiMAE to bring improvements over state-of-the-art methods in all evaluated downstream tasks.

Our main contributions are summarized as:
\begin{itemize}
    \item To the best of our knowledge, we are the first to propose pre-training MAE with point cloud and RGB modalities interactively with three novel schemes.
    % , developing a highly interactive pre-training scheme with three novel schemes.
    %
    % \item To promote better interactive multi-modal learning, we investigate 
    % different masking designs to propose a simple complement alignment strategy, novelly introduce a shared-decoder critical to MAE, and propose a cross-modal reconstruction module for PiMAE, creating a more difficult and diverse task for the point cloud branch.
    \item To promote more interactive multi-modal learning, we novelly introduce a complementary cross-modal masking strategy, a shared-decoder, and cross-modal reconstruction to PiMAE.
    %\item We evaluate PiMAE robustness for Out of Distribution data, and find our features to be more robust than the baseline. 
    %\item We propose PiMAE, f
    %\item We design a new pretext task to strengthen cross-modality engagement and, namely, cross-modality reconstruction task. That is, given a sequence of masked 2D tokens, the model predicts the corresponding missing parts of the 3D tokens.
    %\item We examined different masking strategies and projection alignment, the possibility of information leakage between them. 
    % \item Through extensive experiments, we show it is nontrivial for PiMAE to pre-train with joint point cloud and image modalities. Specifically, fine-tuning our pre-trained models boosts performance by 2.9\%, 6.7\%, and 2.4\% over previous baselines in 3D detection, 2D detection, and few-shot classification, respectively. Moreover, we find our method to be more label-efficient.
    \item Shown by extensive experiments, our pre-trained models boost performance of 2D \& 3D detectors by a large margin, demonstrating PiMAE's effectiveness.
\end{itemize}






%The space after \eg, meaning ``for example'', should not be a sentence-ending space.
%So \eg is correct, {\em e.g.} is not.

%When citing a multi-author paper, you may save space by using ``et alia'', shortened to ``\etal'' (not ``{\em et.\ al.}'' as ``{\em et}'' is a complete word).
%If you use the \verb'\etal' macro provided, then you need not worry about double periods when used at the end of a sentence as in Alpher \etal.


% Update the cvpr.cls to do the following automatically.
% For this citation style, keep multiple citations in numerical (not
% chronological) order, so prefer \cite{Alpher03,Alpher02,Authors14} to
% \cite{Alpher02,Alpher03,Authors14}.




%------------------------------------------------------------------------