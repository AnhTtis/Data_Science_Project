\section{Related Work}
\label{sec:formatting}

\textbf{3D Object Detectors.}
3D object detection aims to predict oriented 3D bounding boxes of physical objects from 3D input data. Many CNN-based works propose two-stage methods for first generating region proposals and then classifying them into different object types.
Prior 3D object detection methods adapt popular 2D detection approaches to 3D scenes, projecting point cloud data to 2D views \cite{conv1, conv2, Xu_2018_CVPR} for 2D ConvNets to detect 3D bounding boxes. Other approaches adopt 3D ConvNets by grouping points into voxels \cite{voxelnet, imvoxelnet} and transposed convolutions for sparse detection~\cite{gwak2020gsdn}.
Recently, the Transformer architecture~\cite{transformers} has demonstrated consistent and impressive performance in vision, specifically with object detectors \cite{detr, Liu2020TANetR3,pointTrans, mao2021dual, zheng2020end, groupfree,misra2021-3detr, shi20193d,shi2020pv,rukhovich2021fcaf3d,zhang2023parameter}. Transformers are especially well-designed for 3D point clouds, needing not hand-crafted groupings and capable of having an invariant understanding. Specifically, ~\cite{misra2021-3detr} proposed an end-to-end Transformer-based object detection module using points cloud as input. Group-Free-3D~\cite{groupfree} designed a novel attention stacking scheme and estimated detection results by fusing object features in different stages.
In PiMAE, we draw inspiration from both projection-based and attention-based 3D object detectors; whereas the former projection mechanisms have been extensively utilized previously, the latter has shown better versatility and a more intuitive solution. Consequently, we design a MAE~\cite{he2022masked}-structured multi-modal learning framework that incorporates projection alignment for more interactive multi-modal learning.

\textbf{Point Cloud and Image Joint 
Representation Learning.}
3D point cloud and 2D image joint representation learning methods aim to explore the modal interaction between point clouds and images for feature fusion. Many recent studies have shown that cross-modal modules outperform single-modal methods on multiple tasks such as 3D object 
detection~\cite{pri3d, H3DNet,yang2022boosting,wang2021pointaugmenting,huang2022tig,wu2022eda}, 3D semantic segmentation~\cite{3D-SIS, peng2021sparse,jaritz2022cross}, and 3D open-world learning~\cite{zhu2022pointclip,zhang2022pointclip,zhang2022can,guo2022calip}.
In cross-modal self-supervised learning of point clouds and RGB images, several methods~\cite{li2022simipu, P4Constrast, houji} based on contrastive learning propose to design specialized structures for learning from multiple modalities surpass single modalities when fine-tuned on downstream tasks including 3D object detection.
As aforementioned, while contrastive methods have illustrated the significance of pairing RGB and point clouds, PiMAE has several advantages over contrastive methods, mainly requiring fewer augmentations.

\textbf{Masked Autoencoders (MAE).}
Recently, inspired by advances in masked language modeling, masked image modeling (MIM) approaches~\cite{he2022masked, xie2021simmim, beit} have shown superior performance, proposing a self-supervised training method based on masked image prediction. MAE~\cite{he2022masked}, in particular, predicts pixels from highly masked images using a ViT decoder.
Since MAE's success, several works~\cite{pointm2ae,pointmae,min2022voxel,zhang2021self,hess2022masked,fu2022pos} have applied the framework to point cloud data, proposing to segment point cloud into tokens and perform reconstruction. Moreover, MultiMAE~\cite{bachmann2022multimae} investigates the alignment of various modalities with MAE among RGB images, depth images, and semantic segmentation. Recently, I2P-MAE~\cite{zhang2022learning} explores leveraging 2D pre-trained knowledge to guide 3D MAE pre-training. In this work, however, we demonstrate that earlier methods do not maximize the potential of point cloud and RGB scene datasets, because they cannot incorporate the RGB inputs with ease and bring only trivial performance gain. To the best of our knowledge, this is the pioneering work aligning RGB images with point cloud with MAE pre-training.