\section{Methods}
\label{sec:methods}

\begin{figure} [t]
    \centering 
    \includegraphics[width=0.4\textwidth]{images/masking2.pdf} 
    \vspace{-0.05in}
    \caption{\textbf{Illustration of our projection operation and two different masking strategies.} A randomly sampled point cloud cluster (black circle) is projected onto the image patch (blue square), and the other clusters are done in a similar way (yellow squares). Under uniform masking, the yellow patches will be masked while other patches are visible. On the contrary, complement masking will result in a reversed masking pattern.} 
    \label{fig:masking} 
\end{figure}

\begin{figure*} [t]
  \centering
    \includegraphics[width=0.9\linewidth]{images/pipeline_final.pdf}
 % \vspace{-0.05in}
  \caption{\textbf{Pre-training pipeline for PiMAE.} The point cloud branch samples and clusters point cloud data into tokens and randomly masks the input. 
  Then the tokens pass through the masking alignment module to generate complement masks for image patches. After embedding, tokens go through a separate, then shared, and finally separated autoencoder structure. We lastly engage in a cross-modal reconstruction module to enhance point cloud representation learning. Point cloud is colored for better visualization.}
  \label{fig:pipeline}
\end{figure*}

In this section, we first give an overview of our pipeline. Then, we introduce our novelly 
designed masking strategy, which aligns the semantic information between tokens from two modalities. Followingly, we present our cross-modal encoders and decoders design. Notably, the shared-decoder is a pioneering architecture. Finally, we finish with our cross-modal reconstruction module.


\subsection{Pipeline Overview}

As shown in Fig.~\ref{fig:pipeline}, PiMAE learns cross-modal representations simultaneously by jointly learning features from point clouds and image modalities. In our proposed pipeline, we first embed point data into tokens by sampling and clustering algorithms and then perform random masking on point tokens. The mask pattern is transformed onto the 2D plane, where patches of images are complementarily masked and embedded into tokens.  

Following this, we utilize a symmetrical joint-encoder-decoder scheme that promotes strong feature fusion. The encoder-decoder architecture consists of both separate branches and shared modules, whereas the former protects modal-specific learning and the latter encourages cross-modal interaction for more robust features.

Finally, for learning stronger features from pre-training, PiMAE's cross-modal reconstruction module demands point cloud features to explicitly express image-level understanding. 

\subsection{Token Projection and Alignment}

We follow MAE\cite{he2022masked} and Point-M2AE\cite{pointm2ae} to generate input tokens from images and point clouds. An image is first divided into non-overlapping patches, before the embedding procedure that embeds patches by a linear projection layer with added Positional Embeddings (PE) and Modality Embeddings (ME). Correspondingly, a set of point clouds is processed into cluster tokens via Farthest Point Sampling (FPS) and K-Nearest Neighbour (KNN) algorithms and then embedded with a linear projection layer with added embeddings (i.e. PE, ME).

\textbf{Projection.} 
\label{sec:proj}
In order to achieve the alignment between multi-modality tokens, we build a link between the 3D point cloud and RGB image pixels by projecting the point cloud onto the camera's image plane. For 3D point $P \in \mathbb{R}^3$, a correlating 2D coordinate can be calculated using the projection function $Proj$ defined below,
\begin{equation}
    \label{eq:projection}
    \begin{bmatrix} u \\ v \\z \end{bmatrix} = Proj(P) = K \cdot R_t \cdot \begin{bmatrix} x \\ y \\ z \\ 1 \end{bmatrix}, 
\end{equation}
where $K \in 3\times 4$, $R_t \in 4\times 4$ are the camera intrinsic and extrinsic matrices. $(x,y,z)$, $(u,v)$ are the original 3D coordinate and projected 2D coordinate of point $P$.

\textbf{Masking with Alignment.} 
\label{sec:masking}
Next, we generate masked tokens using the aforementioned projection function. Since point cloud tokens are organized by the cluster centers, we randomly select a subset of center points as well as their corresponding tokens, while keeping the rest masked. 
For the visible point cloud tokens $T_p$, we project their center point $P \in \mathbb{R}^3$ to the corresponding camera plane and attain its 2D coordinate $p \in \mathbb{R}^2$, which can naturally fall into an area of shape $H \times W$ (i.e. image shape), thus obtaining its related image patch index $I_p$ by

\begin{equation}
    \label{eq:indexing}
    I_p = \lfloor \lfloor v \rfloor / S \rfloor \times \lfloor W / S \rfloor + \lfloor \lfloor u \rfloor / S \rfloor,
\end{equation}
where $u$ and $v$ denotes the $x$-axis value and $y$-axis value of 2D coordinate $p$, $S$ is the image patch size.

After projecting and indexing each visible point cloud token, we obtain their corresponding image patches. Next, we explicitly mask these patches to reach a complement mask alignment. The rationale is that such masking pattern can make visible tokens more semantically abundant than under the uniform setting, and thus the model is able to extract rich cross-modal features. For visual demonstration of our projection and alignment, see Fig.~\ref{fig:masking}.


\subsection{Encoding Phase}

\textbf{Encoder.}
\label{sec:encoder}
During this stage, we protect the integrity of different modalities. Inspired by AIST++~\cite{AIST}, our encoder consists of two modules: modal-specific encoders and a cross-modal encoder. The former is used to better extract modal-specific features, and the latter is used to perform interaction between cross-modal features. 

The modality-specific encoder part contains two branches for two modalities, where each branch consists of a ViT backbone.
First, for the encoders to learn modality differences through mapping inputs to feature spaces, we feed the aligned, visible tokens with their respective positional and modality embeddings to separate encoders. 

Later, we promote feature fusion and cross-modality interactions of visible patches with a shared-encoder. The alignment of masks during this stage becomes critical, as aligned tokens reveal similar information reflected in both 3D and 2D data.

Formally, in the separate encoding phase, $E_I: T_I\mapsto L_I^1$ and $E_P: T_P\mapsto L_P^1$, where $E_I$ and $E_P$ are the image-specific and point-specific encoders, $T_I$ and $T_P$ are the visible image and point patch tokens, and $L_I^1$ and $L_P^1$ are the image and point latent spaces. Then, the shared-encoder performs fusion on the different latent representations $E_S: L^1_I, L^1_P \mapsto L^2_S$.

\subsection{Decoding Phase}

\textbf{Decoder.}
\label{sec:decoder}
Generally, MAE encoders benefit from learning generalized encoders that capture high-dimensional data encoding representations for both image and point cloud data. Due to the differences between the two modalities, specialized decoders are needed to decode the high-level latent to the respective modality.

The purpose of additional shared-decoder layers is ultimately for the encoder to focus more on feature extraction and ignore the details of modality interactions.
Because MAE uses an asymmetric autoencoder design where the mask tokens do not pass the shared-encoder, we complement the mask tokens to pass through a shared-decoder, along with the visible tokens. 
Without such a design, the entire decoder branches are segmented, and the mask tokens of different modalities do not engage in feature fusion. After the shared-decoder, we then design specialized decoders for the different modalities for better reconstructions. 

Since the reconstructions of two modalities are involved, the losses of both the point cloud and the image modalities are obtained. For point clouds, we use $\ell_2$ Chamfer Distance \cite{chamferdistance} for loss calculation, denoted as $\mathcal{L}_{pc}$, and for images, we use MSE to measure the loss, denoted as $\mathcal{L}_{img}$. 

Formally, the input for our shared-decoder is $L_S^{2'}$, the full sets of tokens including encoded visible features and mask tokens of both modalities, and the shared-decoder performs cross-modal interaction on these latent representations $D_S: L_S^{2'} \mapsto L_I^3, L_I^3$.
Then, in the separate decoder phase, the decoders map back to the image and point cloud space, $D_I: L_I^3\mapsto T^{'}_I$ and $D_P: L_P^3\mapsto T^{'}_P$, where $D_I$ and $D_P$ are the image-specific and point-specific decoders, $T^{'}_I$ and $T^{'}_P$ are the visible image and point cloud patches, and $L_I^3$ and $L_P^3$ are the image and point cloud latent spaces. 


\begin{equation}
         \mathcal{L}_{pc} = CD(D_P(l), P_{GT}),
\end{equation}

where $CD$ is $\ell_2$ Chamfer Distance function \cite{chamferdistance}, $D_P$ represents the decoder reconstruction function, 
$l \in L_P^3$ is the point cloud latent representation,
$P_{GT}$ is the ground truth point cloud (i.e. point cloud input). 

\subsection{Cross-modal Reconstruction}
\label{sec:loss}

We train PiMAE using three different losses: the point cloud reconstruction loss, the image reconstruction loss, and a cross-modal reconstruction loss that we design to further strengthen the interaction between the two modalities. 
In the final reconstruction phase, we utilize the previously aligned relationship to obtain the corresponding 2D coordinates of the masked point clouds.
Then, we up-sample the reconstructed image features, such that each masked point cloud with a 2D coordinate can relate to a reconstructed image feature. Finally, the masked point cloud tokens go through a cross-modal prediction head of one linear projection layer to recover the corresponding visible image features. Note that we specifically avoid using visible point cloud tokens for this module, because they correspond to the masked image features (due to the complement masking strategy), which tend to have weaker representations and may harm representation learning. Formally, the cross-modal reconstruction loss is defined as

\begin{equation}
     \mathcal{L}_{cross} = MSE(D_P(l_p^3), l_i^3),
\end{equation}

where $MSE$ denotes the Mean Squared Error loss function, $D_P$ is the cross-modal reconstruction from the decoder, 
$l_p^3 \in L_P^3$ is the point cloud representation,
$l_i^3 \in L_I^3$ is the image latent representation.
% ,and $L_p$, $L_I$ stand for the point and image latent spaces, respectively. 

Our final loss is the sum of the previous loss terms, formulated in Eq.~\ref{eq:finalloss}. By such design, PiMAE learns 3D and 2D features separately while maintaining strong interactions between the two modalities.

\begin{equation}
     \mathcal{L} = \mathcal{L}_{pc} + \mathcal{L}_{img} +  \mathcal{L}_{cross}.
    \label{eq:finalloss}
\end{equation}

\begin{figure*}
  \centering
  \begin{subfigure}{\linewidth}
  \centering
    {\includegraphics[width=\linewidth]{images/recon3.pdf}
    }
    \label{fig:short-a}
  \end{subfigure}
  \hfill
  \vspace{-0.3in}
  \caption{\textbf{Reconstruction results of images and point cloud from PiMAE.} Our model is able to perform image and point cloud reconstruction simultaneously, showing a firm understanding of the two modalities. Image results are in the first row, point cloud results in the second. The masking ratio for both branches is 60\%. Point cloud is colored for better visualization.}
  \label{fig:reconstruction}
\end{figure*}
