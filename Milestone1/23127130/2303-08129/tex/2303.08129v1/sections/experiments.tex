\section{Experiments}
\label{sec:experiments}


\begin{table*}[h]
\centering

\begin{tabular}{l|ccccc}
\toprule
 & & \multicolumn{2}{c}{SUN RGB-D} & \multicolumn{2}{c}{ScanNetV2} \cr
Methods & Pre-trained & $AP_{25}$ & $AP_{50}$ &  $AP_{25}$ & $AP_{50}$\\
\midrule
DSS\cite{DSS} & \textit{None} & 42.1 & - & 15.2 & 6.8 \\
% 2D-driven\cite{2D-driven} & \textit{None} & 45.1 & - & - & - \\
PointFusion\cite{PointFusion} & \textit{None} & 45.4 & - & - & - \\
% F-PointNet\cite{F-PointNet} & \textit{None} & 54.0 & - & 19.8 & 10.8 \\
3D-SIS\cite{3D-SIS} & \textit{None} & - & - & 40.2 & 22.5 \\
VoteNet\cite{votenet} & \textit{None} & 57.7 & 32.9 & 58.6 & 33.5 \\
% \midrule
% ImVoteNet\cite{qi2020imvotenet} & \textit{None} & 63.4 & - & - & - \\
% +P4Constrast\cite{P4Constrast} & ScanNetV2 & 63.5(+0.1) & - & - & - \\
\midrule
 3DETR\cite{misra2021-3detr} & \textit{None} & 58.0 & 30.3 & 62.1 & 37.9 \\
 % +Point-BERT\cite{pointbert} & ScanNetV2 & - & - & 61.0(-1.1) & 38.3(+0.4) \\
 +Ours(from scratch) & \textit{None} & 58.7 & 31.7 & 59.7 & 40.0 \\
 +Ours & SUN RGB-D & 59.4(+1.4) & 33.2(+2.9) & 62.6(+0.5) & 39.4(+1.5) \\
\midrule
GroupFree3D\cite{groupfree} & \textit{None} & 63.0 & 45.2 & 67.3 & 48.9 \\
 +Ours(from scratch) & \textit{None} & 61.2 & 44.7 & 65.5 & 47.4 \\
 +Ours & SUN RGB-D & \bf 64.6(+1.6) & \bf 46.2(+1.0) & \bf 67.6(+0.3) & \bf 49.7(+0.8) \\ 
\bottomrule
\end{tabular}
\caption{\textbf{3D object detection results on ScanNetV2\cite{dai2017scannet} and SUN RGB-D\cite{sunrgbd}.} We adopt the average precision with 3D IoU thresholds of 0.25 ($AP_{25}$) and 0.5  ($AP_{50}$) for the evaluation metrics.}
\label{tab:detection}
\end{table*}


\begin{table*} [t]
\begin{minipage}[t]{0.48\textwidth}
\makeatletter\def\@captype{table}
\begin{tabular}{lccc} 
    \toprule
    Methods & $AP_{50}$ & $AP_{75}$ & $AP$ \\
    \midrule
    *DETR\cite{detr} & 39.8 & 26.2 & 25.3 \\ 
     + PiMAE & \textbf{46.5(+6.7)} & \textbf{30.3(+4.1)} & \textbf{29.5(+4.2)} \\ 
    \bottomrule
  \end{tabular}
  \vspace{-0.05in}
\caption{\textbf{2D object detection results on ScanNetV2 val set.} \\ * denotes our implementation on ScanNetV2. We later load \\ PiMAE pre-trained weights to the encoder.}
\label{tab:2ddection}
\end{minipage}
\begin{minipage}[t]{0.48\textwidth}
\makeatletter\def\@captype{table}
\begin{tabular}{lccc} 
    \toprule
     Methods & Easy & Mod. & Hard \\
    \midrule
    *MonoDETR\cite{zhang2022monodetr} & 23.1 & 17.3 & 14.5 \\ 
     + PiMAE & \textbf{26.6(+3.5)} & \textbf{18.8(+1.5)} & \textbf{15.5(+1.0)} \\ 
    \bottomrule
  \end{tabular}
  \vspace{-0.05in}
\caption{\textbf{Monocular 3D object detection results of car category on KITTI \textit{val} set.} * denotes our implementation with adjusted depth encoder, to which we later load PiMAE pre-trained weights.}
\label{tab:mono3d}
\end{minipage}
\end{table*}

In this section, we provide extensive experiments to qualify the superiority of our methods. The following experiments are conducted. a) We pre-train PiMAE on the SUN RGB-D~\cite{sunrgbd} training set. b) We evaluate PiMAE on various downstream tasks, including 3D object detection, 3D monocular detection, 2D detection, and classification. c) We ablate PiMAE with different multi-modal interaction strategies to show the effectiveness of our proposed design.

\subsection{Implementation Details}
\textbf{Datasets and metrics.} We pre-train our model on SUN RGB-D\cite{sunrgbd} and evaluate with different downstream tasks on several datasets including indoor 3D datasets (SUN RGB-D\cite{sunrgbd}, ScanNetV2\cite{dai2017scannet}), outdoor 3D dataset (KITTI\cite{kitti}), few-shot image classification datasets (CIFAR-FS\cite{cifar-fs}, FC100\cite{FC100}, miniImageNet\cite{miniImageNet}). Detailed descriptions of these datasets and evaluation metrics are in the Appendix.


\textbf{Network architectures.} Abiding by common practice~\cite{pointmae, pointm2ae}, we utilize a scaled-down PointNet~\cite{qi2016pointnet} before a ViT~\cite{vit} backbone in our point cloud branch. PointNet layers effectively reduce the sub-sampled points from 20,000 to 2,048. For the image branch, we follow~\cite{he2022masked} to divide images into regular patches with a size of $16 \times 16$, before the ViT backbone. 

\textbf{Pre-training}.
During this stage, we use the provided image and generated point cloud from SUN RGB-D~\cite{sunrgbd} to train PiMAE for 400 epochs. AdamW~\cite{adamw} optimizer with a base learning rate of 1e-3 and weight decay of 0.05 is used, applied with a warm-up for 15 epochs. No augmentation is performed on both image and point cloud inputs, for the main goal of maintaining consistency between the patches.
Experimentally, we find that a masking ratio of 60\% is more appropriate. The reconstructed visualization results are in Fig.~\ref{fig:reconstruction}.
Detailed configurations are in the Appendix.


\textbf{Fine-tuning.} 
With PiMAE's two multi-modal branches, we fine-tune and evaluate our learned features on both 3D and 2D tasks. 
For 3D tasks, we use the point cloud branch's specific encoder and the shared encoder as a 3D feature extractor. For 2D tasks, similarly, we utilize the image-specific encoder as well as the shared encoder as a 2D feature extractor. We fit our feature extractors into different baselines and keep the same training settings, except for the modifications on the backbone feature extractor. 
Detailed configurations are in the Appendix.


\subsection{Results on Downstream Tasks}
In this work, we evaluate our method on four different downstream tasks dealing with different modalities
, including 3D object detection, monocular 3D object detection, 2d object detection, and few-shot image classification.

\textbf{Indoor 3D object detection.}
We apply our 3D feature extractor on 3D detectors by replacing or inserting the encoder into different backbones to strengthen feature extraction. We report our performance on indoor 3D detection based on SOTA methods 3DETR\cite{misra2021-3detr} and GroupFree3D\cite{groupfree}. As shown in Tab.~\ref{tab:detection}, our model brings significant improvements to both models, surpassing previous baselines consistently in all datasets and criteria.

Furthermore, in the Appendix, we provide 3D object detection results with detailed per-class accuracy on SUN RGB-D, along with visualizations of detection results. 

\textbf{Outdoor monocular 3D object detection.}
To fully demonstrate the capacity of our approach, we report our performance in challenging outdoor scenarios, which have a large data distribution gap compared with our indoor pre-training data. As shown in Tab.~\ref{tab:mono3d}, we brought substantial improvement to MonoDETR~\cite{zhang2022monodetr}, validating that our pre-trained representations generalize well to both indoor and outdoor datasets.


\textbf{2D object detection.}
Similarly, we apply our 2D branch's feature extractor to 2D detector DETR \cite{detr} by replacing its vanilla transformer backbone. We conduct experiments on both pre-trained and scratch backbones and report our performance on the ScanNetV2 2D detection dataset. As shown in Tab.~\ref{tab:2ddection}, our model significantly improves the performance of DETR, demonstrating the strong generalization ability on 2D tasks of our model.


\textbf{Few shot image classification.}
We conduct few-shot image classification experiments on three different benchmarks to explore the feature-extracting ability of PiMAE's image encoder. To verify the effectiveness of PiMAE, we use no extra design for the classifier by only adding a linear layer to the feature encoder, predicting the class based on \textit{[CLS]} token as input.
Tab.~\ref{tab:fewshot} summarizes our results. We see significant improvements from PiMAE pre-training compared to models trained from scratch. Moreover, our performance surpasses previous SOTA self-supervised multi-modal learning method, CrossPoint\cite{crosspoint}.

\begin{table*}[!t]
  \centering
  \small 
  \begin{tabularx}{0.69\textwidth}{lcccccc}
    \toprule
    & \multicolumn{2}{c}{CIFAR-FS 5-way} & \multicolumn{2}{c}{FC100 5-way} & \multicolumn{2}{c}{miniImageNet 5-way} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
    Method  & 1-shot & 5-shot & 1-shot & 5-shot & 1-shot & 5-shot \\
    \midrule
    MAML\cite{MAML} & 58.9 & 71.5 & - & - & 48.7 & 63.1 \\
    Matching Networks\cite{miniImageNet} & - & - & - & - & 43.6 & 55.3 \\
    Prototypical Network\cite{protypical} & 55.5 & 72.0 & 35.3 & 48.6 & 49.3 & 68.2 \\
    Relation Network\cite{relationnetwork} & 55.0 & 69.3 & - & - & 50.4 & 65.3 \\
    % MetaOptNet\cite{metaoptnet} & 72.6 & 84.3 & 41.1 & 55.5 & 62.6 & 78.6 \\
    \midrule
    CrossPoint\cite{crosspoint}  & 64.5 & 80.1 & - & - & - & - \\
    \midrule
    PiMAE From Scratch  & 62.4 & 76.6 & 37.3 & 50.5 & 50.1 & 66.7 \\
    PiMAE Pre-trained  & \textbf{66.9} & \textbf{80.7} & \textbf{39.0} & \textbf{53.3} & \textbf{55.3} & \textbf{70.2} \\
    \bottomrule
  \end{tabularx}
  \vspace{-0.05in}
   \caption{\textbf{Few-shot image classification on CIFAR-FS, FC100 and miniImageNet test sets.} We report top-1 classification accuracy under 5-way 1-shot and 5-way 5-shot settings. Results of CrossPoint and previous methods are from \cite{crosspoint, cifar-fs}.}
    \vspace{-0.05in}
 \label{tab:fewshot}
\end{table*}

\subsection{Ablation Study}
In this section, we investigate our methods, evaluating the quality of different PiMAE pre-training strategies both qualitatively and quantitatively. 
First, we attempt different alignment strategies between the masked tokens. Next, we pre-train with different reconstruction targets. Then, we ablate performance pre-trained with only a single branch. Finally, we examine our data efficiency by training on limited data. Additional ablation studies on model architecture and masking ratios are in our Appendix. All experiments are based on 3DETR and performed on SUN RGB-D unless otherwise stated.

\begin{figure} [h]
    \centering 
    \includegraphics[width=0.48\textwidth]{images/attn7.pdf} 
    \vspace{-0.15in}
      \vspace{-0.05in}

    \caption{\textbf{Visualization of attention.} The encoder attention between the two modalities is visualized by computing self-attention from the query points (orange circle) to all the visible image tokens. We show the corresponding location (red square) of the query points after projection. See more examples in the Appendix.} 
    \label{Fig:attention} 
    \vspace{-0.05in}
\end{figure}


\textbf{Cross-modal masking.}  
To better study the mask relationships between the two modalities, we design two masking strategies based on projection alignment: uniform masking and complement masking.
Whereas the former masks both modalities in the same pattern that masked portions of one modality will correspond to the other when projected onto it, the latter is the opposite, i.e. a visible point cloud patch will be masked when projected on the image.

We pre-train both the uniform, complement as well as random masking strategies and evaluate their fine-tuning performance on 3D detection. Random masking acts as a baseline where the masking pattern of image and point patches are individually and randomly sampled. As shown in Tab.~\ref{tab:strategy}, masking the tokens from different modalities complementarily gains higher performance than randomly.


Compared to random masking, complement masking enables more cross-modal interactions between patches with diverse semantic information, thus helping our model to transfer 2D knowledge into the 3D feature extractor. However, with the uniform masking strategy, the extracted point cloud features and image features are semantically aligned, so the interaction does not help the model utilize 2D information better. 

Note that in Tab.~\ref{tab:strategy}, we purposely pre-train our model without the cross-modal reconstruction module. This is because the uniform masking strategy projects onto the masked image features, which are semantically weaker and may negatively influence our ablation study. 




\begin{table}
  \centering
  \small
  \begin{tabular}{ccc} 
    \toprule
     Masking Strategy & $AP_{25}$ & $AP_{50}$ \\
    \midrule
    Random & 58.0 & 32.9 \\
     Uniform  & 58.1 & 32.6 \\ 
     Complement & \textbf{59.0} & \textbf{33.0} \\
    \bottomrule
  \end{tabular}
    \vspace{-0.05in}
  \caption{\textbf{Comparisons of cross-modality masking strategies.}}
  \label{tab:strategy}
    \vspace{-0.05in}

\end{table}

\textbf{Effect of Cross-modal Reconstruction}. 
Other than reconstructing the inputs, we promote cross-modal reconstruction by demanding point cloud features to reconstruct features or pixels of the corresponding image. We assess the significance of such design by ablating results on 3D detection. Shown in Tab.~\ref{tab:reconstruction}, the additional feature-level cross-modal reconstruction target brings additional performance gains.%
The promoted cross-modal reconstruction at the feature level encourages further interactions between modalities and encodes 2D knowledge into our feature extractor, improving model performance on downstream tasks. 


\begin{table}
  \centering
  \small\begin{tabular}{@{}cccccc@{}} 
    \toprule
     \multicolumn{3}{c}{Point Cloud} & \multicolumn{1}{c}{RGB} & \multirow{2}{*}{$AP_{25}$} & \multirow{2}{*}{$AP_{50}$} \\
     \cmidrule(lr){1-3} \cmidrule(lr){4-4} 
    3D Geo & 2D feat & 2D pix & 2D pix \\
    \midrule
     \checkmark &  & & \checkmark & 59.0 & 33.0 \\
     \checkmark &  & \checkmark & \checkmark & 58.0 & 31.6\\
     \checkmark & \checkmark & & \checkmark & \textbf{59.4} & \textbf{33.2}\\
    \bottomrule
  \end{tabular}
\vspace{-0.05in}
   \caption{\textbf{Ablation studies of cross-modal reconstruction targets.} Geo, feat, and pix refer to coordinates, features, and pixels, respectively.}
  \label{tab:reconstruction}
\end{table}


\textbf{Necessity of joint pre-training.} 
To demonstrate the effectiveness and the cruciality of double-branch pre-training in PiMAE, we provide the comparison of PiMAE pre-trained with one branch only. As shown in Tab.~\ref{tab:single-branch}. A critical performance drop can be seen among double-branch PiMAE and single-branch PiMAE. Such evidence reveals the fact that PiMAE learns 3D and 2D features jointly and the cross-modal interactions that we propose help the model to utilize information from both modalities.

Furthermore, to demonstrate the effectiveness of PiMAE's cross-modal interaction design, we visualize the attention map in our shared encoder in Fig.~\ref{Fig:attention}. With our proposed design, PiMAE focuses on more foreground objects with higher attention values, showing a strong cross-modal understanding. See more examples in our Appendix.

\begin{table}
  \centering
  \small
  \scalebox{0.9}{
  \begin{tabular}{@{}lcccc@{}}
    \toprule
     & \multicolumn{2}{c}{3D Object Detection} & \multicolumn{2}{c}{Few-shot image classification} \\
     \cmidrule(lr){2-3} \cmidrule(lr){4-5} 
     Input & $AP_{25}$ & $AP_{50}$ & 5-way 1-shot & 5-way 5-shot \\
    \midrule
    RGB & - & - & 66.3 & 79.5 \\
    Geo & 58.4 & 32.3 & - & - \\
    \midrule
    RGB+Geo & \textbf{59.4} & \textbf{33.2} & \textbf{66.9} & \textbf{80.7} \\
    \bottomrule
  \end{tabular}}
    \vspace{-0.05in}

   \caption{\textbf{Improvement from joint pre-training.} We compare results on 3D object detection (on SUN RGB-D) and few-shot image classification (on CIFAR-FS) tasks when pre-trained with a single-branch PiMAE.}
  \label{tab:single-branch}
\end{table}

\textbf{Data efficiency.}
We train 3DETR and PiMAE-based 3DETR using limited annotated labels (varying from 1\% to 100\%) while testing on the full val set on SUN RGB-D. As shown in Tab.~\ref{Fig:data}, PiMAE is able to outperform the baseline in every scenario, with the largest difference being 17.4\% ($AP_{25}$) when 10\% of labels are used.

\begin{figure} 
    \centering 
    \includegraphics[width=0.4\textwidth]{images/data5.pdf} 
    \vspace{-0.05in}
    \caption{\textbf{Illustration of Data Efficiency.} Compared to 3DETR, PiMAE is able to ease the burden of data labeling and increase performance significantly.} 
    \label{Fig:data} 
    \vspace{-0.05in}
\end{figure}




