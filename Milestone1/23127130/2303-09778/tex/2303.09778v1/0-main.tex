\documentclass[sigconf]{acmart}


\usepackage{amsmath}
\usepackage{bm}
\usepackage{mathrsfs}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{url}
% \usepackage{tabularray}
% \UseTblrLibrary{booktabs}
\usepackage{colortbl}
\usepackage[ruled,linesnumbered,vlined]{algorithm2e}
\usepackage{makecell}
\usepackage{enumitem}
\usepackage{diagbox}
\usepackage{float}




%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.


\copyrightyear{2023}
\acmYear{2023}
\setcopyright{acmlicensed}\acmConference[WWW '23]{Proceedings of the ACM Web Conference 2023}{April 30-May 4, 2023}{Austin, TX, USA}
\acmBooktitle{Proceedings of the ACM Web Conference 2023 (WWW '23), April 30-May 4, 2023, Austin, TX, USA}
\acmPrice{15.00}
\acmDOI{10.1145/3543507.3583453}
\acmISBN{978-1-4503-9416-1/23/04}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%commands
\newcommand{\framework}{SE-GSL}
\newcommand\negimp[1]{\textcolor[RGB]{255,180,0}{#1}}
\newcommand\posimp[1]{\textcolor[RGB]{0,190,0}{#1}}
\theoremstyle{definition}
\newtheorem{define}{Definition}[]
%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{\framework: A General and Effective Graph Structure Learning Framework through Structural Entropy Optimization}

% \author{
% Dongcheng Zou$^1$,
% Xiang Huang$^1$,
% Hao Peng$^1$,
% Renyu Yang$^1$,
% Jia Wu$^2$,
% Jianxin Li$^1$,
% Chunyang Liu$^1$, and 
% Philip S. Yu$^3$
% }
% \affiliation{
% \institute{
% $^1$Beihang University;
% $^2$Macquarie University;
% $^3$University of Illinois Chicago
% }
% }
% \email{
% {zoudongcheng, huang.xiang, penghao, renyu.yang, lijx}@buaa.edu.cn, 
% jia.wu@mq.edu.au, 
% liuchunyang@didiglobal.com, 
% psyu@uic.edu
% }


\author{Dongcheng Zou}
\affiliation{%
  \institution{Beihang University}
  \city{Beijing}
  \country{China}
}
\email{zoudongcheng@buaa.edu.cn}

\author{Hao Peng}
\affiliation{%
  \institution{Beihang University}
  \city{Beijing}
  \country{China}
}
\email{penghao@buaa.edu.cn}
\authornote{Corresponding author}


\author{Xiang Huang}
\affiliation{%
  \institution{Beihang University}
  \city{Beijing}
  \country{China}
}
\email{huang.xiang@buaa.edu.cn}



\author{Renyu Yang}
\affiliation{%
  \institution{Beihang University}
  \city{Beijing}
  \country{China}
}
\email{renyu.yang@buaa.edu.cn}


\author{Jianxin Li}
\affiliation{%
  \institution{Beihang University}
  \city{Beijing}
  \country{China}
}
\email{lijx@buaa.edu.cn}

\author{Jia Wu}
\affiliation{%
  \institution{Macquarie University}
  \city{Sydney}
  %\state{NSW}
  \country{Australia}
}
\email{jia.wu@mq.edu.au}


\author{Chunyang Liu}
\affiliation{%
  \institution{Didi Chuxing}
  \city{Beijing}
  \country{China}
}
\email{liuchunyang@didiglobal.com}

\author{Philip S. Yu}
\affiliation{%
  \institution{University of Illinois Chicago}
  \city{Chicago}
  %\state{IL}
  \country{USA}
}
\email{psyu@uic.edu}


\renewcommand{\shortauthors}{Zou and Peng, et al.}


%%
%% The abstract is a short summary of the work to be presented in the article.
\begin{abstract}
Graph Neural Networks (GNNs) are de facto solutions to structural data learning. However, it is susceptible to low-quality and unreliable structure, which has been a norm rather than an exception in real-world graphs. Existing graph structure learning (GSL) frameworks still lack robustness and interpretability.
% in noisy or heterophily graphs. 
This paper proposes a general GSL framework, \framework{}, through structural entropy and the graph hierarchy abstracted in the encoding tree. Particularly, we exploit the one-dimensional structural entropy to maximize embedded information content when auxiliary neighbourhood attributes is fused to enhance the original graph. 
A new scheme of constructing optimal encoding trees is proposed to minimize the uncertainty and noises in the graph whilst assuring proper community partition in hierarchical abstraction. 
We present a novel sample-based mechanism for restoring the graph structure via node structural entropy distribution. It increases the connectivity among nodes with larger uncertainty in lower-level communities.
\ \framework\ is compatible with various GNN models and enhances the robustness towards noisy and heterophily structures. 
Extensive experiments show significant improvements in the effectiveness and robustness of structure learning and node representation learning.
\end{abstract}






%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010178</concept_id>
       <concept_desc>Computing methodologies~Artificial intelligence</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10002950.10003624.10003633.10010917</concept_id>
       <concept_desc>Mathematics of computing~Graph algorithms</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10002951.10003227.10003351</concept_id>
       <concept_desc>Information systems~Data mining</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Artificial intelligence}
\ccsdesc[300]{Mathematics of computing~Graph algorithms}
\ccsdesc[300]{Information systems~Data mining}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Graph structure learning, structural entropy, graph neural network}


\maketitle

\input{1-introduction}
\input{2-preliminaries}%% 2-related work
\input{3-purposed-method}%% 3-purposed method
\input{4-experiment}
\input{5-evaluation}
\input{6-related-work}
\input{7-conclusion}


\begin{acks}
This paper was supported by the National Key R\&D Program of China through grant 2021YFB1714800, NSFC through grant 62002007, S\&T Program of Hebei through grant 20310101D, Natural Science Foundation of Beijing Municipality through grant 4222030, CCF-DiDi GAIA Collaborative Research Funds for Young Scholars, the Fundamental Research Funds for the Central Universities, Xiaomi Young Scholar Funds for Beihang University, and in part by NSF under grants III-1763325, III-1909323,  III-2106758, and SaTC-1930941. 
\end{acks}


%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

%%
%% If your work has an appendix, this is the place to put it.
\clearpage
\appendix
\input{8-appendix}
 
\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
