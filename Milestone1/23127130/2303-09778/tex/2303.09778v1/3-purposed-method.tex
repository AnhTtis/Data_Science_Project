\section{Our Approach}\label{sec:framework}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.98\textwidth]{framework.pdf}
  \caption{The overall architecture of\ \framework.}
%   \FIXME{change $G^{(k)}$ to $G_{knn}^{(k)}$}
  \label{fig:framework}
  \Description{The overall architecture of SE-GSL.}
\end{figure*}







This section presents the architecture of \framework{}, then elaborate on how we enhance the graph structure learning by structural entropy-based optimization of the hierarchical encoding tree.


\subsection{Overview of ~\framework}
Fig.~\ref{fig:framework} depicts the overall pipeline. 
At the core of ~\framework{} is the structure optimization procedure that transforms and enhances the graph structure. 
More specifically, it encompasses multi-stages: graph structure enhancement, hierarchical encoding tree generation, and sampling-based structure reconstruction before an iterative representation optimization.

First, the original topological information is integrated with vertex attributes and the neighborhood in close proximity. 
Specifically, we devise a similarity-based edge reweighting mechanism and incorporate $k$-NN graph structuralization to provide auxiliary edge information. The most suitable $k$ is selected under the guidance of the one-dimensional structural entropy maximization strategy (\S~\ref{step1}). 
Upon the enhanced graph, we present a hierarchical abstraction mechanism to further suppress the edge noise and reveal the high-level hierarchical community structure (encoding tree) (\S~\ref{step2}). 
A novel sampling-based approach is designed to build new graph topology from the encoding tree, particularly by restoring the edge connectivity from the tree hierarchy (\S~\ref{step3}). 
The core idea is to weaken the association between high-level communities whilst establishing dense and extensive connections within low-level communities. 
To this end, we transform the node structural entropy into probability, rejecting the deterministic threshold. 
Through multi-iterative stochastic sampling, it is more likely to find favorable graph structures for GNNs. 
Afterward, the rebuilt graph will be fed into the downstream generic GNN encoders. To constantly improve both the node representation and the graph structure, the optimization pipeline is iterated for multiple epochs.
% -- the optimized node representation and graph structure will go into the next epoch as the new input. 
The training procedure of\ \framework{}\ is summarized in Appendix~\ref{appendix:overall algorithm}. 





%介绍1维结构熵图强化
\subsection{Graph Structure Enhancement}\label{step1}

To fully incorporate vertex attributes and neighborhood information in the graph structure, we perform feature fusion and edge reweighting so that the topological structure, together with the informative vertex adjacent similarity, can be passed on to the encoding tree generator. 
To begin with, we calculate the pair-wise similarity matrix $S \in \mathbb{R}^{|V|\times |V|}$ among vertices in graph $G$. 
To better depict the linear correlation between two vertex attributes, we take the Pearson correlation coefficient (PCC) as the similarity measure in the experiments, i.e.,
\begin{equation}\label{eq:pcc}
S_{ij}=\mathrm{PCC}(x_i,x_j)=\frac{E((x_i-u_i)(x_j-u_j))}{\sigma_i\sigma_j},
\end{equation}
where $x_i$ and $x_j \in \mathbb{R}^{1 \times d}$ are the attribute vectors of vertices $i$ and $j$, respectively. $u_i$ and $\sigma_i$ denote the mean value and variance  of $x_i$, and $E(\cdot)$ is the dot product function.
Based on $S$, we can intrinsically construct the $k$-NN graph $G_{knn} = \{V,E_{knn}\}$ where each edge in $E_{knn}$ represents a vertex and its $k$ nearest neighbors (e.g., the edges in red in Fig ~\ref{fig:framework}). We fuse $G_{knn}$ and the original $G$ to $G_{f} = \{V,E_{f} = E \cup E_{knn}\}$.  

A key follow-up step is pinpointing the most suitable number $k$ of nearest neighbors. An excessive value of $k$ would make $G_{f}$ over-noisy and computationally inefficient, while a small $k$ would result in insufficient information and difficulties in hierarchy extraction. As outlined in \S~\ref{prelim:1dse}, a larger one-dimensional structural entropy indicates more information that $G_{f}$ can potentially hold. Hence, we aim to maximize the one-dimensional structural entropy $H^1(G_{f})$ to guide the selection of $k$ for larger encoding information. In practice, we gradually increase the integer parameter $k$, generate the corresponding $G^{(k)}_{f}$ and compute  $H^1(G^{(k)}_{f})$. Observably, when $k$ reaches a threshold $k_m$, $H^1(G^{(k)}_{f})$ comes into a plateau without noticeable increase. This motivates us to regard this critical point $k_m$ as the target parameter. The $k$-selector algorithm is depicted in Appendix ~\ref{appendix:1dse algorithm}. 
In addition, the edge $e_{ij}$ between $v_i$ and $v_j$ is reweighted as: 
\begin{equation}\label{eq:reweighted}
    \omega_{ij}=S_{ij}+M, \quad M = \frac{1}{2|V|} \cdot \frac{1}{|E|}\sum_{1<i,j<n}{S_{ij}}, 
\end{equation}
where $M$ is a modification factor that amplifies the trivial edge weights and thus makes the $k$-selector more sensitive to noises.



\subsection{Hierarchical Encoding Tree Generation}\label{step2}

Our methodology of abstracting the fused graph into a hierarchy is inspired by the structural entropy theory~\cite{li2016structural,li2018decoding}. We intend to minimize the graph uncertainty (i.e., edge noises) and maximize the knowledge embedded (e.g., optimal partition) in the high-dimensional hierarchy. Correspondingly, the objective of structural entropy minimization is to find out an encoding tree $\mathcal{T}$ that minimizes $H^\mathcal{T}(G_f)$ defined in Eq.~\ref{eq:HT}.
Due to the difficulty in graph semantic complexity quantification, we restrict the optimization objective to the $K$-level tree with a hyperparameter $K$. The optimal $K$-dimensional encoding tree is represented as:
\begin{equation}\label{eq:T}
\mathcal{T^*} = \mathop{\arg\min}\limits_{\forall\mathcal{T}:height(\mathcal{T})\le K}(H^\mathcal{T}(G)).
\end{equation}

To address this optimization problem, we design a greedy-based heuristic algorithm to approximate $H^K(G)$. To assist the greedy heuristic, we define two basic operators:


\begin{define}
\label{def:CBop}
\textbf{Combining operator:}~
Given an encoding tree $\mathcal{T}$ for $G=(V,E)$, let $\alpha$ and $\beta$ be two nodes in $\mathcal{T}$ sharing the same parent $\gamma$. 
The combining operator $\mathrm{CB}_{\mathcal{T}}(\alpha,\beta)$ updates the encoding tree as: $\gamma \gets \delta^-; \delta \gets \alpha^-; \delta \gets \beta^-.$ 
A new node $\delta$ is inserted between $\gamma$ and its children $\alpha, \beta$.
\end{define}
\begin{define}
\label{def:LFop}
\textbf{Lifting operator:}~
Given an encoding tree $\mathcal{T}$ for $G=(V,E)$, let $\alpha$, $\beta$ and $\gamma$ be the nodes in $\mathcal{T}$, satisfying $\beta^-=\gamma$ and $\alpha^-=\beta$.
The lifting operator $\mathrm{LF}_{\mathcal{T}}(\alpha,\beta)$ updates the encoding tree as:  $\gamma \gets \alpha^-; \mathrm{IF:}T_{\beta}=\varnothing, \mathrm{THEN:}\mathrm{drop}(\beta).$
The subtree rooted at $\alpha$ is lifted by placing itself as $\gamma$'s child. If no more children exist after lifting, $\beta$ will be deleted from $\mathcal{T}$.
\end{define}



In light of the high-dimensional structural entropy minimization principle~\cite{li2018decoding}, we first build a full-height binary encoding tree by greedily performing the combining operations. 
Two children of the root are combined to form a new partition iteratively until the structural entropy is no longer reduced. 
To satisfy the height restriction, we further squeeze the encoding tree by lifting subtrees to higher levels. 
To do so, we select and conduct lifting operations between a non-root node and its parent node that can reduce the structural entropy to the maximum.
This will be repeated until the encoding tree height is less than $K$ and the structural entropy can no longer be decreased.
Eventually, we obtain an encoding tree with a specific height $K$ with minimal structural entropy. The pseudo-code is detailed in Appendix~\ref{appendix:kdse algorithm}.






\subsection{Sample-based Graph Reconstruction}
\label{step3}

The subsequent step is to restore the topological structure from the hierarchy whilst guaranteeing the established hierarchical semantics in optimal encoding tree $\mathcal{T}^*$. 
The key to graph reconstruction is determining which edges to augment or weaken. 
Intuitively, the nodes in real-life graphs in different communities tend to have different labels. The work~\cite{zhu2020cagnn} has demonstrated the effectiveness of strengthening intra-cluster edges and reducing inter-cluster edges in a cluster-awareness approach to refine the graph topology. However, for hierarchical communities, simply removing cross-community edges will undermine the integrity of the higher-level community. Adding edges within communities could also incur additional edge noises to lower-level partitioning. 

We optimize the graph structure with community preservation by investigating the structural entropy of \textit{deduction} between two interrelated nodes as the criterion of edge reconstruction:

\begin{define}
\label{def:deduct-se}
\textbf{Structural entropy of deduction:}~
Let $\mathcal{T}$ be an encoding tree of $G$. We define the structural entropy of the deduction from non-leaf node $\lambda$ to its descendant $\alpha$ as:
\begin{equation}\label{eq:deduct-se}
% H^{\mathcal{T}}(G;(\lambda,\alpha]) = \sum_{\beta,\lambda \subset \beta \subseteq \alpha}{H^{\mathcal{T}}(G;\beta)}.
H^{\mathcal{T}}(G;(\lambda,\alpha]) = \sum_{\beta,T_\alpha \subseteq T_\beta \subset T_\lambda}{H^{\mathcal{T}}(G;\beta)}.
% \vspace{-0.4em}
\end{equation}
 This node structure entropy definition exploits the hierarchical structure of the encoding tree and offers a generic measure of top-down deduction to determine a community or vertex in the graph.
\end{define}

From the viewpoint of message passing, vertices with higher structural entropy of deduction produce more diversity and uncertainty and thus require more supervisory information.
Therefore, such vertices need expanded connection fields during the graph reconstruction to aggregate more information via extensive edges.
To achieve this, we propose an analog sampling-based graph reconstruction method. 
The idea is to explore the node pairs at the leaf node level (the lowest semantic level) and stochastically generate an edge for a given pair of nodes with a certain probability associated with the deduction structural entropy. 

Specifically, for a given $\mathcal{T}$, assume the node $\delta$ has a set of child nodes $\{ \delta^{\left \langle 1 \right \rangle},\delta^{\left \langle 2 \right \rangle},\dots,\delta^{\left \langle n \right \rangle}\} $. The probability of the child $\delta^{\left \langle i \right \rangle}$ is defined as: 
$P(\delta^{\left \langle i \right \rangle}) = \sigma_\delta(H^{\mathcal{T}}(G_{f};(\lambda,\delta^{\left \langle i \right \rangle}]))$,
% \begin{equation}\label{eq:prob}
% P(\delta^{\left \langle i \right \rangle}) = \sigma_\delta(H^{\mathcal{T}}(G_{f};(\lambda,\delta^{\left \langle i \right \rangle}])),
% \end{equation}
where $\lambda$ is the root of $\mathcal{T}$ and $\sigma_\delta(\cdot)$ represents a distribution function. Take $\mathrm{softmax}$ function as an example, the probability of $\delta^{\left \langle i \right \rangle}$ can be calculated as: 
\begin{equation}\label{eq:prob-softmax}
% \vspace{-0.4em}
P(\delta^{\left \langle i \right \rangle}) = \frac{\mathrm{exp}(H^{\mathcal{T}}(G_{f};(\lambda,\delta^{\left \langle i \right \rangle}]))}
{ {\textstyle \sum_{j=1}^{n}}{\mathrm{exp}(H^{\mathcal{T}}(G_{f};(\lambda,\delta^{\left \langle j \right \rangle}]))} }.
\end{equation}

The probability of a non-root node can be acquired recursively. To generate new edges, we sample leaf node pairs by a top-down approach with a single sampling flow as follows: 

\noindent \textbf{(1)} For the encoding tree (or subtree) with root node $\delta$, two different child nodes $\delta^{\left \langle i \right \rangle}$ and $\delta^{\left \langle j \right \rangle}$ are selected by sampling according to $P(\delta^{\left \langle i \right \rangle})$ and $P(\delta^{\left \langle j \right \rangle})$. Let $\delta_1 \gets \delta^{\left \langle i \right \rangle}$ and $\delta_2 \gets \delta^{\left \langle j \right \rangle}$
\textbf{(2)} If $\delta_1$ is a non-leaf node, we perform sampling once on the subtree rooted at $\delta_1$ to get $\delta_1^{\left \langle i \right \rangle}$, then update $\delta_1 \gets \delta_1^{\left \langle i \right \rangle}$. The same is operated on $\delta_2$.
\textbf{(3)} After recursively performing step (2), we sample two leaf nodes $\delta_1$ and $\delta_2$, while adding the edge connecting vertex $v_1 = T_{\delta_1}$ and $v2 = T_{\delta_2}$ into the edge set $E'$ of graph $G'$. 
To establish extensive connections at all levels, multiple samplings are performed on all encoding subtrees. 
For each subtree rooted at $\delta$, we conduct independent samplings for $\theta \times n$ times, where $n$ is the number of $\delta$'s children, and $\theta$ is a hyperparameter that positively correlated with the density of reconstructed graph. 
For simplicity, we adopt a uniform $\theta$ for all subtrees. 
% If it is necessary to precisely control the sparsity within communities and the connectivity between communities, 
Separately setting and tuning $\theta$ of each semantic level for precise control is also feasible.

\subsection{Time Complexity of \ \framework{}}\label{timecomplexity}
The overall time complexity is $O(n^2+n+n\log ^2n)$, in which $n$ is the number of nodes. 
Separately, in \S ~\ref{step1}, the time complexity of calculating similarity matrix is $O(n^2)$ and of $k$-selector is $O(n)$. 
According to ~\cite{li2016structural}, the optimization of a high-dimensional encoding tree in \S ~\ref{step2} costs the time complexity of $O(n\log ^2n)$.
As for the sampling process in \S ~\ref{step3}, the time complexity can be proved as $O(2n)$.
We report the time cost of \ \framework{} in Appendix~\ref{appendix:baseline}.
