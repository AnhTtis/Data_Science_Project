\section{Introduction}
\label{sec:intro}

Graph Neural Networks (GNNs)~\cite{wu2020comprehensive,zhou2020graph} have become the cornerstone and de facto solution of structural representation learning. 
Most of the state-of-the-art GNN models employ message passing~\cite{gilmer2017neural} and recursive information aggregation from local neighborhoods ~\cite{velivckovic2017graph,leskovec2019powerful,peng2021reinforced,yang2023wsdm} to learn node representation. 
These models have been advancing a variety of tasks, including node classification~\cite{welling2016semi,xu2018powerful,peng2019event}, node clustering~\cite{bianchi2020spectral,peng2022event}, graph classification~\cite{ying2018hierarchical,Peng2020motif}, and graph generation~\cite{you2018graphrnn}, etc. 

GNNs are extremely sensitive to the quality of given graphs and thus require resilient and high-quality graph structures. 
However, it is increasingly difficult to meet such a requirement in real-world graphs. 
Their structures tend to be noisy, incomplete, adversarial, and heterophily (i.e., the edges with a higher tendency to connect nodes of different types), which can drastically weaken the representation capability of GNNs~\cite{pei2019geom,luo2021learning,dai2021nrgnn}.
Recent studies also reveal that even a minor perturbation in the graph structure can lead to inferior prediction quality ~\cite{bojchevski2019certifiable,zhang2020gnnguard,sun2022adversarial}. 
Additionally, GNNs are vulnerable to attacks since the raw graph topology is decoupled from node features, and attackers can easily fabricate links between entirely different nodes~\cite{zhang2020gnnguard,sun2022adversarial}.


To this end, Graph Structure Learning (GSL)~\cite{chen2020iterative,jin2020graph,wang2020gcn,zhu2021deep,zhu2021contrastive,SunGLS2022,Sun2022Position} becomes the recent driving force for learning superior task-relevant graph topology and for enhancing the resilience and robustness of node representation. 
% The existing works focus on parameterizing the adjacency matrix and on jointly optimizing GNN whilst imposing regularization constraints on refined graph structures ($e.g.$, sparsity, smoothness, and low-rank~\cite{chen2020iterative,luo2021learning,yang2023wsdm}).
The existing works focus on jointly optimizing GNN whilst imposing regularization on refined graph structures.
Typical methods include metric-based~\cite{chen2020iterative,wang2020gcn,li2018adaptive}, probabilistic sampling ~\cite{franceschi2019learning,rong2019dropedge,zheng2020robust}, and learnable structure approach~\cite{jin2020graph}, etc.
While promising, GNNs and GSL still have the following issues. 
\emph{i) robustness to system noises and heterophily graphs.} While many GSL models strive to fuse node features and topological features through edge reconstruction (e.g., add, prune, or reweight) ~\cite{wang2020gcn,zhang2020gnnguard,zhu2021contrastive}, additional noises and disassortative connections will be inevitably involved in the fused structure due to the unreliable priori topology and node embeddings, which would further degrade the GNNs representation capability~\cite{li2022reliable}. 
\emph{ii) model interpretability.}
Fully parameterizing the adjacency matrix will incur a non-negligible cost of parameter storage and updating and is liable to low model interpretability~\cite{gilpin2018explaining}. 
Although some studies on the improved GNN interpretability ~\cite{ying2019gnnexplainer,huang2022graphlime}, few works can effectively explain the topology evolution during graph structure learning.
Therefore, fusing the node and topological features in a noisy system environment to obtain GNN-friendly graphs by exploiting inherent graph structures is still an underexplored problem~\cite{wu2020graph}.


\begin{figure}[t]
  \centering
  \includegraphics[width=0.44\textwidth]{example.pdf}
  \caption {An illustrative example of the hierarchical community (semantics) in a simple social network. (1) Vertices and edges represent the people and their interconnectivity (e.g., common locations, interests, occupations). There are different abstraction levels, and each community can be divided into sub-communities in a finer-grained manner (e.g., students are placed in different classrooms while teachers are allocated different offices). The lowest abstraction will come down to the individuals with own attributes, and the highest abstraction is the social network system. (b) An encoding tree is a natural form to represent and interpret such a multi-level hierarchy.}
  % \caption {An illustrative example of the hierarchical community (semantics) in a simple social network. (1) Vertices and edges represent the people and their interconnectivity. Each community can be divided into sub-communities in a finer-grained manner at different abstraction levels (e.g., students are placed in different classrooms while teachers are allocated different offices). Each level of abstraction has realistic semantics (b) An encoding tree is a natural form to interpret such a multi-level hierarchy.
  % }
  \label{fig:intro}  
  \Description{An example of hierarchical communities in a simple social network.}
\end{figure}

In this paper, we present\ \framework, a general and effective graph structure learning framework that can adaptively optimize the topological graph structure in a learning-free manner and can achieve superior node representations, widely applicable to the mainstream GNN models. 
This study is among the first attempts to marry the structural entropy and encoding tree theory~\cite{li2016structural} with GSL, which offers an effective measure of the information embedded in an arbitrary graph and structural diversity. 
The multi-level semantics of a graph can be abstracted and characterized through an encoding tree.
Encoding tree~\cite{li2016structural,li2018decoding,zeng2023effective} represents a multi-grained division of graphs into hierarchical communities and sub-communities, thus providing a pathway to better interpretability.  
Fig.~\ref{fig:intro} showcases how such graph semantics are hierarchically abstracted. 
Specifically, we first enhance the original graph topology by incorporating the vertex similarities and auxiliary neighborhood information via the $k$-Nearest Neighbors ($k$-NN) approach, so that noise can be better identified and diminished. 
% In doing so, a $k$-Nearest Neighbors ($k$-NN) graph is generated and fused into the original graph structure. 
This procedure is guided by the $k$-selector that maximizes the amount of embedded information in the graph structure.
% We then propose a scheme to establish an optimal encoding tree by high-dimensional structural entropy minimization, with the intuition of minimizing the graph uncertainty and edge noises whilst maximizing the knowledge embedded in the hierarchical abstraction.
We then propose a scheme to establish an optimal encoding tree to minimize the graph uncertainty and edge noises whilst maximizing the knowledge in the encoding tree.
% The intuition is to minimize the uncertainty and edge noises in the graph whilst maximizing the knowledge embedded (e.g., optimal community partition) in the hierarchical abstraction. 
To restore the entire graph structure that can be further fed into GNN encoders, we recover edge connectivity between related vertices from the encoding tree taking into account the structural entropy distribution among vertices. 
The core idea is to weaken the association between vertices in high-level communities whilst establishing dense and extensive connections between vertices in low-level communities. The steps above will be iteratively conducted to co-optimize both graph structure and node embedding learning.
\framework\footnote{code is available at: https://github.com/RingBDStack/SE-GSL} is an interpretable GSL framework that effectively exploits the substantive structure of the graph. 
We conduct extensive experiments and demonstrate significant and consistent improvements in the effectiveness of node representation learning and the robustness of edge perturbations.
% We conduct extensive experiments on nine node classification datasets and demonstrate significant and consistent improvements in the effectiveness in node representation learning. 
% We artificially inject noisy edges into three typical datasets to evaluate the robustness of GSL in the event of graph perturbations.   

\textbf{Contribution highlights:}
i) \framework{} provides a generic GSL solution to improve both the effectiveness and robustness of the mainstream GNN approaches.
ii) \framework{} offers a new perspective of navigating the complexity of attribute noise and edge noise by leveraging structural entropy as an effective measure and encoding tree as the graph hierarchical abstraction. 
iii) \framework{} presents a series of optimizations on the encoding tree and graph reconstruction that can not only explicitly interpret the graph hierarchical meanings but also reduce the negative impact of unreliable fusion of node features and structure topology on the performance of GNNs. 
iv) We present a visualization study to reveal improved interpretability when the graph structure is evolutionary. 

