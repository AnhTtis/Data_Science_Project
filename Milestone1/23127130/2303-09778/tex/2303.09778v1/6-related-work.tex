pan2021information\section{Related Work}\label{sec:relate}

\noindent {\textbf{Graph structure learning and neighborhood optimization.}}
The performance of GNNs is heavily dependent on task-relevant links and neighborhoods.
Graph structure learning explicitly learns and adjusts the graph topology, and our \framework{} is one of them.
GDC~\cite{gasteiger_diffusion_2019} reconnects graphs through graph diffusion to aggregate from multi-hop neighborhoods.
Dropedge~\cite{rong2019dropedge}, Neuralsparse ~\cite{zheng2020robust} contribute to graph denoising via edge-dropping strategy while failing to renew overall structures.
LDS-GNN~\cite{franceschi2019learning} models edges by sampling graphs from the Bernoulli distribution. Meanwhile, we consider linking the structural entropy, which is more expressive of graph topology, to the sampling probability.
GEN~\cite{wang2021graph}, IDGL~\cite{chen2020iterative} explore the structure from the node attribute space by the $k$-NN method. Differently, instead of directly using attribute similarity, we regenerate edges from the hierarchical abstraction of graphs to avoid inappropriate metrics.
Besides adjusting the graph structure, methods to optimize aggregation are proposed with results on heterophily graphs.
MixHop~\cite{abu2019mixhop} learns the aggregation parameters for neighborhoods of different hops through a mixing network, while H$_2$GCN~\cite{zhu2020beyond} 
% iteratively concatenates the multi-hop neighborhood features for the final node embeddings.
identifies higher-order neighbor-embedding separation and intermediate representation combination, for adapting to heterophily graphs.
Geom-GCN~\cite{pei2019geom} aggregates messages over both the original graph and latent space by a designed geometric scheme.


\noindent {\textbf{Structural entropy with neural networks.}}
Structural information principles~\cite{li2016structural}, defining encoding trees and structural entropy, were first used in bioinformatic structure analysis~\cite{li2016three,li2018decoding}. 
Existing work mainly focuses on network analysis, node clustering and community detection\cite{li2017resistance,liu2019rem,pan2021information}.
As an advanced theory on graphs and hierarchical structure, structural information theory has great potential in combination with neural networks. 
SR-MARL~\cite{zeng2023effective} applies structural information principles to hierarchical role discovery in multi-agent reinforcement learning, thereby boosting agent network optimization.
SEP~\cite{wu2022structural} provides a graph pooling scheme based on optimal encoding trees to address local structure damage and suboptimal problem. It essentially uses structural entropy minimization for a multiple-layer coarsened graph.
MinGE~\cite{luo2021graph} and MEDE~\cite{yang2023wsdm} estimate the node embedding dimension of GNNs via structural entropy, which introduces both attribute entropy and structure entropy as objective.
Although these works exploit structural entropy to mine the latent settings of neural networks and GNNs, how to incorporate this theory in the optimization process is still understudied, and we are among the first attempts.
%% note 
%Although plenty of works have attributed a natural hierarchical structure to graphs, an approach to the most justified and explainable partition has been the angle of discussion. 