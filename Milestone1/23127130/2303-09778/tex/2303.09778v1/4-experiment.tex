\section{Experimental Setup}\label{sec:experi}




\begin{table*}[htb!]
    \renewcommand{\arraystretch}{1.05}
    \setlength{\abovecaptionskip}{0.25cm}
    \setlength{\belowcaptionskip}{-0.25cm}
    % \caption{Classification Accuracy (\%) comparison, with improvement range of ~\framework~ against the baselines.}
    \caption{Classification Accuracy (\%) comparison, with improvement range of ~\framework~ against the baselines. \textmd{The best results are bolded and the second-best are underlined. \posimp{Green} denotes the outperformance percentage, while \negimp{yellow} denotes underperformance.}}
    \label{tab:performance comparison}
\centering
    % \scalebox{1.0}{
    \setlength{\tabcolsep}{1.2mm}{
        % \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c}
        \begin{tabular}{l|ccccccccc}
        \toprule
        %\multirow{2}*{\textbf{Blocks}} & \multicolumn{2}{c|}{\textbf{Traditional}} & \multicolumn{4}{c|}{\textbf{Evolutionary}} & \multicolumn{5}{c}{\textbf{Dedicated}} \\
        %\cline{2-13}
       Method & {Cora} & {Citeseer} & {Pubmed} & {PT} & {TW} & {Actor} & {Cornell} & {Texas} & {Wisconsin} \\
        % \cline{1-1}
        %Method & & & & & & & & &\\
        \toprule     % cora    cite    pub     squ    cham    act      cor      tex     wis
        GCN          & 87.26$_{\pm0.63}$ & 76.22$_{\pm0.71}$ & 87.46$_{\pm0.12}$ & 67.62$_{\pm0.21}$ & 62.46$_{\pm1.94}$
                     & 27.65$_{\pm0.55}$ & 49.19$_{\pm1.80}$ & 57.30$_{\pm2.86}$ & 48.57$_{\pm4.08}$\\
        GAT          & 87.52$_{\pm0.69}$ & 76.04$_{\pm0.78}$ & 86.61$_{\pm0.15}$ & 68.76$_{\pm1.01}$ & 61.68$_{\pm1.20}$
                     & 27.77$_{\pm0.59}$ & 57.09$_{\pm6.32}$ & 58.10$_{\pm4.14}$ & 51.34$_{\pm4.78}$\\
        GCNII        & 87.57$_{\pm0.87}$ & 75.47$_{\pm1.01}$ & \underline{88.64$_{\pm0.23}$} & 68.93$_{\pm0.93}$ & 65.17$_{\pm0.47}$ 
                     & 30.66$_{\pm0.66}$ & 58.76$_{\pm7.11}$ & 55.36$_{\pm6.45}$ & 51.96$_{\pm4.36}$\\
        Grand        & \textbf{87.93$_{\pm0.71}$} & 77.59$_{\pm0.85}$ & 86.14$_{\pm0.98}$ & 69.80$_{\pm0.75}$ & \underline{66.79$_{\pm0.22}$}  
                     & 29.80$_{\pm0.60}$ & 57.21$_{\pm2.48}$ & 56.56$_{\pm1.53}$ & 52.94$_{\pm3.36}$ \\
        %Mixhop       & & & & & & & & & \\
        \midrule    % cora    cite     pub     squ    cham    act     cor     tex      wis
        % Geom-GCN-I   & 85.19 & 77.99 & 90.05 & - & - & 29.09 & 56.76 & 57.58 & 58.24 \\
        Mixhop     & 85.71$_{\pm0.85}$ & 75.94$_{\pm1.00}$ & 87.31$_{\pm0.44}$ & 69.48$_{\pm0.30}$ & 66.34$_{\pm0.22}$ & 33.72$_{\pm0.76}$ & 64.47$_{\pm4.78}$ & 63.16$_{\pm6.28}$ & 72.12$_{\pm3.34}$ \\
        Dropedge   & 86.32$_{\pm1.09}$  & 76.12$_{\pm1.32}$  & 87.58$_{\pm0.34}$  & 68.49$_{\pm0.91}$  & 65.24$_{\pm1.45}$  & 30.10$_{\pm0.71}$  & 58.94$_{\pm5.95}$ & 59.20$_{\pm5.43}$ & 60.45$_{\pm4.48}$ \\
        Geom-GCN-P   & 84.93 & 75.14 & 88.09 & - & - & 31.63 & 60.81 & 67.57 & 64.12 \\
        Geom-GCN-S   & 85.27 & 74.71 & 84.75 & - & - & 30.30 & 55.68 & 59.73 & 56.67 \\
        GDC          & 87.17$_{\pm0.36}$ & 76.13$_{\pm0.53}$ & 88.08$_{\pm0.16}$ & 66.14$_{\pm0.54}$ & 64.14$_{\pm1.40}$ 
                     & 28.74$_{\pm0.76}$ & 59.46$_{\pm4.35}$ & 56.42$_{\pm3.99}$ & 48.30$_{\pm4.29}$ \\ 
        % Pro-GNN      & 83.98$_{\pm0.86}$ & 72.41$_{\pm0.99}$ & 88.56$_{\pm0.37}$ & 35.89$_{\pm1.37}$ & 57.04$_{\pm2.20}$ 
                    %  & 27.96$_{\pm1.74}$ & 54.35$_{\pm6.79}$ & 56.76$_{\pm6.02}$ & 52.94$_{\pm5.88}$ \\
        GEN          & \underline{87.84$_{\pm0.69}$} & \textbf{78.77$_{\pm0.88}$} & 86.13$_{\pm0.41}$ & \underline{71.62$_{\pm0.78}$} & 65.16$_{\pm0.77}$
                     & \textbf{36.69$_{\pm1.02}$} & 65.57$_{\pm6.74}$ & 73.38$_{\pm6.65}$ & 54.90$_{\pm4.73}$ \\
        H$_2$GCN-2   & 87.81$_{\pm1.35}$ & 76.88$_{\pm1.77}$ & \textbf{89.59$_{\pm0.33}$} & 68.15$_{\pm0.30}$ & 63.33$_{\pm0.77}$
                     & 35.62$_{\pm1.30}$ & \textbf{82.16$_{\pm6.00}$} & \underline{82.16$_{\pm5.28}$} & \underline{85.88$_{\pm4.22}$} \\
        \midrule     
        % \framework   & 89.32$_{\pm1.24}$ & 78.85$_{\pm2.35}$ & 88.86$_{\pm0.75}$ & 36.07$_{\pm3.42}$ & 56.98$_{\pm2.81}$ 
        %              & 36.20$_{\pm2.07}$ & 76.85$_{\pm5.89}$ & 84.49$_{\pm4.80}$ & 86.27$_{\pm4.32}$ \\
        \framework   & \textbf{87.93$_{\pm1.24}$} & \underline{77.63$_{\pm1.65}$} & 88.16$_{\pm0.76}$ & \textbf{71.91$_{\pm0.66}$} & \textbf{66.99$_{\pm0.93}$} 
                     & \underline{36.34$_{\pm2.07}$} & \underline{75.21$_{\pm5.54}$} & \textbf{82.49$_{\pm4.80}$} & \textbf{86.27$_{\pm4.32}$} \\
        \midrule      
        Improvement  & \posimp{0.00}$\sim$\posimp{3.00}  & \negimp{-1.14}$\sim$\posimp{2.92}  & \negimp{-1.43}$\sim$\posimp{3.41}
                     & \posimp{0.29}$\sim$\posimp{5.77} & \posimp{0.20}$\sim$\posimp{5.31} &  \negimp{-0.35}$\sim$\posimp{8.69}
                     & \negimp{-6.95}$\sim$\posimp{26.02} & \posimp{0.33}$\sim$\posimp{27.13} & \posimp{0.39}$\sim$\posimp{37.97}\\
        
        \bottomrule
        % DGI          & 82.72 & XXXXX & XXXXX & 33.69 & 52.27 & 25.96 & 54.09 & 58.11 & 48.04\\
        % Dropedge(GCN)& XXX & XXX & XXX & XXX & 45.36 & 68.64 & XXX & 57.89 & XXX & XXX\\
        \end{tabular}
    }
\end{table*}
















\noindent \textbf{Software and Hardware.}  
All experiments are conducted on a Linux server with GPU (NVIDIA RTX A6000) and CPU (Intel i9-10980XE), using PyTorch 1.12 and Python 3.9.

\noindent \textbf{Datasets}. 
We experiment on nine open graph benchmark datasets, including three citation networks (i.e., Cora, Citeseer, and Pubmed), two social networks (i.e., PT and TW), three website networks from WebKB (i.e., Cornell, Texas, and Wisconsin), and a co-occurrence network. 
Their statistics are summarized in Appendix ~\ref{appendix:dataset}.

\noindent \textbf{Baseline and backbone models}. 
We compare ~\framework{} with baselines including general GNNs (i.e., GCN, GAT, GCNII, Grand) and graph learning/high-order neighborhood awareness methods (i.e. MixHop, Dropedge, Geom-GCN, GDC, GEN, H$_2$GCN). 
Four classic GNNs (GCN, GAT, GraphSAGE, APPNP) are chosen as the backbone encoder that ~\framework{} works upon. Details are in Appendix ~\ref{appendix:baseline}.

\noindent \textbf{Parameter settings}. 
For ~\framework~ with various backbones, we uniformly adopt two-layer GNN encoders. 
To avoid over-fitting, We adopt ReLU (ELU for GAT) as the activation function and apply a dropout layer with a dropout rate of 0.5. The training iteration is set to 10.
The embedding dimension $d$ is chosen from $\{8,16,32,48,64\}$, while the height of the encoding tree $K$ is searched among $\{2,3,4\}$, and the hyperparameter $\theta$ in \ref{step3} is tuned among $\{0.5, 1, 3, 5, 10,30\}$. 
We adopt the scheme of data split in Geom-GCN~\cite{pei2019geom} and H$_2$GCN~\cite{zhu2020beyond} for all experiments -- nodes are randomly divided into the train, validation, and test sets, which take up 48\%, 32\%, 20\%, respectively.
In each iteration, the GNN encoder optimization is carried out for 200 epochs, using the Adam optimizer, with an initial learning rate of $0.01$ and a weight decay of $5e-4$. The model with the highest accuracy on validation sets is used for further testing and reporting.
