% \newpage
% \newpage
\section{Appendix}\label{sec:append}
\subsection{Glossary of notations}
\label{appendix:notations}
In Table ~\ref{tab:notations}, we summarize the notations used in our work. 

\begin{table}[ht]
    \setlength{\abovecaptionskip}{0.25cm}
    \setlength{\belowcaptionskip}{-0.25cm}
    \caption{Glossary of Notations.}\label{tab:notations}
    % \vspace{-2.5mm}
    \centering
    \scalebox{1.0}{
        \begin{tabular}{l|l}
        \hline
        Notation & Description \\
        \hline
        $G;A;S$ & Graph; Adjacency matrix; Similarity matrix. \\
        % $A^{G}_{ij}$ & The weight of the edge between $v_i$ and $v_j$ in $G$.\\
        $v;e;x$ & Vertex; Edge; Vertex attribute. \\
        $V;E;X$ & Vertex set; Edge set; Attribute set. \\
        $|V|;|E|$ & The number of vertices and edges. \\
        $\mathcal{P};P_i$ & The partition of $V$; A community.\\
        $D;d(v_i)$ & The degree matrix; The degree of vertex $v_i$. \\
        $e_{ij}$ & The edge between $v_i$ and $v_j$. \\
        $w_{ij}$ & The weight of edge $e_{ij}$. \\
        $vol(G)$ & The volume of graph $G$, i.e., degree sum in $G$. \\
        $G^{(k)}_{knn}$ & The $k$-NN graph with parameter $k$.\\
        $G_{f}$  & Fusion graph.\\
        $G^{(k)}_{f}$ & The fusion graph with parameter $k$.\\
        \hline
        $\mathcal{T}$ & Encoding tree. \\
        $\mathcal{T}^*$ & The optimal encoding tree. \\
        % $\mathcal{T}^{(K)}$ & The encoding tree with height $K$. \\
        $\lambda$ & The root node of an encoding tree. \\
        $\alpha$ & A non-root node of an encoding tree. \\
        $\alpha^-$ & The parent node of $\alpha$. \\
        $\alpha^{\left \langle i \right \rangle}$ & the $i$-th child of $\alpha$.\\
        $T_\lambda$ & The label of $\lambda$, i.e., node set $V$. \\
        $T_\alpha$ & The label of $\alpha$, i.e., a subset of $V$.\\
        $\mathcal{V}_\alpha$ & Volume of graph $G$. \\
        $g_a$ & the sum weights of cut edge set $[T_\alpha,T_\alpha/T_\lambda]$. \\
        $N(\mathcal{T})$ & The number of non-root node in $\mathcal{T}$.\\
        \hline
        $H^\mathcal{T}(G)$ & Structural entropy of $G$ under $\mathcal{T}$.\\
        $H^K(G)$ & $K$-dimensional structural entropy.\\
        $H^1(G)$ & One-dimensional structural entropy.\\
        $H^\mathcal{T}(G;\alpha)$ & Structural entropy of node $\alpha$ in $\mathcal{T}$.\\
        $H^{\mathcal{T}}(G;(\lambda,\alpha])$ & Structural entropy of a deduction from $\lambda$ to $\alpha$.\\
        \hline
        \end{tabular}
    }
% \vspace{-6.5mm}
\end{table}

% \vspace{-0.8em}
\subsection{Dataset and Time Costs of \ \framework{}}
\label{appendix:dataset}
Our framework \framework~ is evaluated on nine graph datasets. the statistics of these datasets are shown in Table~\ref{tab:statistics}. The time costs of \ \framework{} on all datasets are shown in Table~\ref{tab:time comparasion}.
% \vspace{-0.2em}
\begin{table}[ht]
    \setlength{\abovecaptionskip}{0.25cm}
    \setlength{\belowcaptionskip}{-0.25cm}
    \caption{Statistics of benchmark datasets.}\label{tab:statistics}
    \centering
    % \vspace{-0.2em}
    \scalebox{1.0}{
        \begin{tabular}{c|ccccc}
        \hline
        Dataset & Nodes & Edges & Classes & Features & homophily\\
        \hline
        Cora & 2708 & 5429 & 7 & 1433 & 0.83\\
        Citeseer & 3327 & 4732 & 6 & 3703 & 0.71\\
        Pubmed & 19717 & 44338 & 3 & 500 & 0.79\\
        \hline
        % Chameleon & 2277 & 36101 & 5 & 2325 & 0.25\\
        % Squirrel & 5201 & 217073 & 5 & 2089 & 0.22\\
        PT & 1912 & 31299 & 2 & 3169 & 0.59\\
        TW & 2772 & 63462 & 2 & 3169 & 0.55\\
        \hline
        Actor & 7600 & 33544 & 5 & 931 & 0.24\\
        \hline
        Cornell & 183 & 295 & 5 & 1703 & 0.30\\
        Texas & 183 & 309 & 5 & 1703 & 0.11\\
        Wisconsin & 251 & 499 & 5 & 1703 & 0.21\\
        \hline
        \end{tabular}
    }
% \vspace{-4.5mm}
\end{table}

\begin{itemize}[leftmargin=*]
    \item \textbf{Citation networks}~\cite{yang2016revisiting,welling2016semi}. Cora, Citeseer, and Pubmed are benchmark datasets of citation networks. Nodes represent paper, and edges represent citation relationships in these networks. The features are bag-of-words representations of papers, and labels denote their academic fields.
    % \item \textbf{Wikipedia networks}~\cite{rozemberczki2021multi,pei2019geom}. Wikipedia dataset contain three page to page networks, Chameleon, Squirrel and Crocodile, which are originally designed for website traffic regression. In ~\cite{pei2019geom}, the author package monthly traffic into 5 categories, providing Chameleon and Squirrel for node classification task. In both networks, vertices are web pages, edges are hyper-links and features are nouns that characterize the page.
    \item \textbf{Social networks}~\cite{rozemberczki2021multi}.
    TW and PT are two subsets of Twitch Gamers dataset~\cite{rozemberczki2021twitch}, designed for binary node classification tasks, where nodes correspond to users and links to mutual friendships. 
    The features are liked games, location, and streaming habits of the users. 
    The labels denote whether a streamer uses explicit language (Taiwanese and Portuguese).
    \item \textbf{WebKB networks}~\cite{getoor2005link}. 
    Cornell, Texas, and Wisconsin are three subsets of WebKB, where nodes are web pages, and edges are hyperlinks. The features are the bag-of-words representation of pages. The labels denote categories of pages, including student, project, course, staff, and faculty.
    % The WebKB dataset consists of 877 scientific publications classified into one of five classes. The citation network consists of 1608 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1703 unique words. 
    \item \textbf{Actor co-occurrence network}~\cite{tang2009social}. This dataset is a subgraph of the film-director-actor-writer network, in which nodes represent actors, edges represent co-occurrence relation, node features are keywords of the actor, and labels are the types of actors.
    % \item \textbf{Karateclub dataset}
\end{itemize}
% \vspace{-0.5em}

% \vspace{-1.0em}
\subsection{Baselines}
\label{appendix:baseline}
% Extensive baselines are used for comparison, which is briefly described as follows\footnote{For GCN, GAT, GraphSAGE, and APPNP layers, we adopt implementation from DGL library~\cite{wang2019deep}:https://github.com/dmlc/dgl }:
Baselines are briefly described as follows\footnote{For GCN, GAT, GraphSAGE, and APPNP layers, we adopt implementation from DGL library~\cite{wang2019deep}:https://github.com/dmlc/dgl }:
\begin{itemize}[leftmargin=*]
% \vspace{-1.5mm}
    \item \textbf{GCN}~\cite{welling2016semi} 
    is the most popular GNN, which defines the first-order approximation of a localized spectral filter on graphs.
    \item \textbf{GAT}~\cite{velivckovic2017graph}  
    introduces a self-attention mechanism to important scores for different neighbor nodes.
    \item \textbf{GraphSAGE}~\cite{hamilton2017inductive} 
    is an inductive framework that leverages node features to generate embeddings by sampling and aggregating features from the local neighborhood. 
    \item \textbf{APPNP}~\cite{gasteiger2019predict} 
    combines GCN with personalized PageRank. 
    \item \textbf{GCNII}\footnote{https://github.com/chennnM/GCNII}~\cite{chen2020simple}
    employs residual connection and identity mapping.
    \item \textbf{Grand}\footnote{https://github.com/THUDM/GRAND}~\cite{feng2020graph}
    purposes a random propagation strategy for data augmentation, and uses consistency regularization to optimize.
    \item \textbf{Mixhop}\footnote{https://github.com/samihaija/mixhop}~\cite{abu2019mixhop} aggregates mixing neighborhood information.
    \item \textbf{Geom-GCN}\footnote{https://github.com/graphdml-uiuc-jlu/geom-gcn}~\cite{pei2019geom}
    exploits geometric relationships to capture long-range dependencies within structural neighborhoods. Three variant of Geom-GCN is used for comparison.
    \item \textbf{GDC}\footnote{https://github.com/gasteigerjo/gdc}~\cite{gasteiger_diffusion_2019}
    refines graph structure based on diffusion kernels.
    % \item \textbf{Pro-GNN}\footnote{https://github.com/DSE-MSU/DeepRobust}~\cite{jin2020graph}
    \item \textbf{GEN}\footnote{https://github.com/BUPT-GAMMA/Graph-Structure-Estimation-Neural-Networks}~\cite{wang2021graph} 
    estimates underlying meaningful graph structures.
    \item \textbf{H$_2$GCN}\footnote{https://github.com/GemsLab/H2GCN}~\cite{zhu2020beyond} combine multi-hop neighbor-embeddings for adapting to both heterophily and homophily graph settings.
    \item \textbf{DropEdge}\footnote{https://github.com/DropEdge/DropEdge}~\cite{rong2019dropedge}
    randomly removes edges from the input graph for over-fitting prevention. 
    \item \textbf{Jaccard}\footnote{https://github.com/DSE-MSU/DeepRobust}~\cite{wu2019adversarial}
    prunes the edges connecting nodes with small Jaccard similarity.
\end{itemize} 

% \footnote{The implementation provided by CogDL~\cite{cen2021cogdl} is adopted for GCNII, GDC and Grand: https://github.com/thudm/cogdl}
% For SGC, GCN~\cite{welling2016semi}, Chebnet, GAT~\cite{velivckovic2017graph},GraphSAGE and APPNP, we adopt the implementations from the Deep Graph Learning library ~\cite{wang2019deep}.
% For the remaining baselines

% we utilize two-layer GNN encoders, and comparing with our \framework with them as the backbones. 

\begin{table*}[htb!]
    \renewcommand{\arraystretch}{0.95}
    \setlength{\abovecaptionskip}{0.25cm}
    \setlength{\belowcaptionskip}{-0.25cm}
    \caption{Comparison of training time(hr.) of achieving the best performance based on GPU.}
    % \vspace{-2.5mm}
    \label{tab:time comparasion}
    \centering
    % \scalebox{1.0}{
    \setlength{\tabcolsep}{3.6mm}{
        \begin{tabular}{l|ccccccccc}
        \hline
      Method & {Cora} & {Citeseer} & {Pubmed} & {PT} & {TW} & {Actor} & {Cornell} & {Texas} & {Wisconsin} \\
        \cline{1-1}
        \hline     % cora    cite    pub     pt    tw    act      cor      tex     wis
        \framework$_{GCN}$  &  0.071 & 0.213 & 4.574 & 0.178 &  0.374
                     & 1.482 & 0.006 & 0.008 & 0.009 \\
        \framework$_{SAGE}$   & 0.074 & 0.076 & 4.852 & 0.169 &  0.214
                     & 0.817 &  0.006 & 0.007 & 0.009 \\
        \framework$_{GAT}$   & 0.071 & 0.180 & 4.602 & 0.172 &  0.329
                     & 1.273 & 0.006 & 0.008 & 0.009 \\
        \framework$_{APPNP}$   & 0.073 & 0.215 & 4.854 & 0.138 &  0.379
                     & 1.367 & 0.010 & 0.011 & 0.013 \\
        \hline
        \end{tabular}
    }
    % \vspace{-2.5mm}
\end{table*}

% \vspace{-1.0em}
\subsection{Overall algorithm of \framework}
The overall algorithm of \framework~ is shown in Algorithm ~\ref{algorithm:training}. Note that, if choose to retain the connection from the previous iteration, to ensure that the number of edges remains stable during the training, a percentage of edges in the reconstructed graph with low similarity will be dropped in each iteration.

\label{appendix:overall algorithm}
\begin{algorithm}[htb!]
\SetAlgoRefName{1}
\SetAlgoVlined
\KwIn{a graph $G=(V,E)$, features $X$, labels $Y_L$, mode $\in {True,False}$\\
iterations $\eta$, encoding tree height $K$, hyperparameter $\theta$}
\KwOut{optimized graph $G'=(V,E')$, prediction $Y_P$, GNN parameters $\Theta$ }
Initialize $\Theta$;\\
\For{$i=1$ to $\eta$}{
    Update $\Theta$ by classification loss $\mathcal{L}_{cls}(Y_L,Y_P)$;\\
    Getting node representation $X'=\mathrm{GNN}(X)$;\\
    Initialize $k=1$ for $k$-NN structuralization;\\
    Create fusion map $G_{f}$ according to Algorithm~\ref{algorithm:kselector};\\
    Create $K$-dimensional encoding tree $\mathcal{T}^*$ according to Algorithm~\ref{algorithm:KDimSEMinimize};\\
    \For{each non-root node $\alpha$ in $\mathcal{T}^*$}{
        Calculate $H^{\mathcal{T}^*}(G_{f};(\lambda,\alpha])$ through Eq.~\ref{eq:deduct-se};\\
        Assign probability $P(\alpha)$ to $\alpha$ through Eq.~\ref{eq:prob-softmax};\\
    }
    \For{each subtree rooted at $\alpha$ in $\mathcal{T}^*$}{
        Assuming $\alpha$ has $n$ children, set $t = \theta \times n$;\\ 
        \For{$j=1$ to $t$}{
            Sample a node pair $(v_m,v_n)$ according to \S~\ref{step3};\\
            Adding edge $e_{mn}$ to $G'$;\\
        }
    }
    \If{mode}{
        Let $E' = E \cup E'$, where $E'$ and $E$ are the edge set of $G'$ and $G$, respectively;\\
        Drop a percentage of edges in $G'$;
    }
    Update graph structure $G \gets G'$;
    Update node representation: $X \gets X'$;\\
}
Get prediction $Y_P$;\\
Return $G'$, $Y_P$ and $\Theta$;\\
\caption{Model training for~\framework}
\label{algorithm:training}
\end{algorithm}

% \vspace{-1.0em}
\subsection{Algorithm of one-dimensional structural entropy guided graph enhancement}
\label{appendix:1dse algorithm}
The $k$-selector is designed for choosing an optimal $k$ for $k$-NN structuralization under the guidance of one-dimensional structural entropy maximization. The algorithm of $k$-selector and fusion graph construction is shown in Algorithm~\ref{algorithm:kselector}.

\begin{algorithm}[htb!]
\SetAlgoRefName{2}
\SetAlgoVlined
\KwIn{a graph $G=(V,E)$, node representation $X$}
\KwOut{fusion graph $G_{f}$ }
Calculate $S \in \mathbb{R}^{|V|\times |V|}$ via Eq.~\ref{eq:pcc};\\
\For{$k=2$ to $|V|-1$}{
    Generate $G_{knn}$ by $S$;\\
    Generate $G^{(k)}_{f} = \{V,E_{f} = E \cup E_{knn} \}$;\\
    Reweight $G^{(k)}_{f}$ via Eq.~\ref{eq:reweighted};\\
    Calculate $H^1(G^{(k)}_{f})$ via Eq.~\ref{eq:H1};\\
    \If{$H^1(G^{(k)}_{f})$ reaches the maximal optima}{
        $G_{f} \gets G^{(k)}_{f}$;\\
        Return $G_{f}$;
    }
}
\caption{$k$-selector and fusion graph construction}
\label{algorithm:kselector}
\end{algorithm}

% \vspace{-1.0em}
\subsection{Algorithm of high-dimensional structural entropy minimization}
\label{appendix:kdse algorithm}
% \vspace{-0.5em}
% The high-dimensional structural entropy minimization is a heuristic algorithm that adjusts encoding tree structure by greedily executing specific operators. The pseudo-code is shown in Algorithm~\ref{algorithm:KDimSEMinimize}.
The pseudo-code of the high-dimensional structural entropy minimization algorithm is shown in Algorithm~\ref{algorithm:KDimSEMinimize}.
%%伪代码
\begin{algorithm}[htb!]
\SetAlgoRefName{3}
\SetAlgoVlined
\KwIn{a graph $G=(V,E)$, the height of encoding tree $k>1$}
\KwOut{Optimal high-dimensional encoding tree $\mathcal{T}^*$}
//Initialize an encoding tree $\mathcal{T}$ with height $1$ and root $\lambda$\\
Create root node $\lambda$;\\
\For{$v_i \in V$}{
    Create node $\alpha_i$. Let $T_{\alpha_i} = v_i$;\\
    $v_i^- = \lambda$;\\
}
//Generation of binary encoding tree\\
%% problem
\While{$\lambda$ has more than $2$ children}{
    Select $\alpha_i$ and $\alpha_j$ in $\mathcal{T}$, 
    condition on $\alpha_i^-=\alpha_j^-=\lambda$ and $\mathop{\arg\max}\limits_{\alpha_i,\alpha_j}(H^\mathcal{T}(G)-H^\mathcal{T}_{\mathrm{CB}(\alpha_{i},\alpha_{j})}(G))$;\\
    $ \mathrm{CB}(\alpha_{i},\alpha_{j})$ according to Definition ~\ref{def:CBop};\\
}
//Squeezing of encoding tree\\
\While{$\mathrm{height}(\mathcal{T}) > K$}{
    Select non-root node $\alpha$ and $\beta$ in $\mathcal{T}$, 
    condition on $\alpha^-=\beta$ and $\mathop{\arg\max}\limits_{\alpha,\beta}(H^\mathcal{T}(G)-H^\mathcal{T}_{\mathrm{LF}(\alpha,\beta)}(G))$;\\
    $ \mathrm{LF}(\alpha,\beta)$ according to Definition ~\ref{def:LFop};\\
}
Return $\mathcal{T}^* \gets \mathcal{T}$;
\caption{K-dimensional structural entropy minimization}
\label{algorithm:KDimSEMinimize}
\end{algorithm}


