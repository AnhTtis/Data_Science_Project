\section{Preliminaries}\label{sec:prelim}
This section formally reviews the basic concepts of Graph, Graph Neural Networks (GNNs), Graph Structure Learning (GSL), and Structural Entropy. 
Important notations are given in Appendix ~\ref{appendix:notations}.

\subsection{Graph and Graph Structure Learning}
\noindent \textbf{Graph and Community}. 
% \subsubsection{Graph and Communities}
Let $G = \{V,E,X\}$ denote a graph, where $V$ is the set of $n$ vertices\footnote{A vertex is defined in the graph and a node in the tree.}, $E \subseteq V \times V$ is the edge set, and $X \in \mathbb{R}^{n\times d}$ refers to the vertex attribute set.
$\mathrm {A} \in \mathbb{R}^{n \times n}$ denotes the adjacency matrix of $G$, where $\mathrm {A}_{ij}$ is referred to as the weight of the edge between vertex $i$ and vertex $j$ in $G$. 
Particularly, if $G$ is unweighted, $\mathrm {A} \in \{0,1\}^{n \times n}$ and $\mathrm {A}_{ij}$ only indicate the existence of the edges. 
In our work, we only consider the undirected graph, where $\mathrm {A}_{ij} = \mathrm {A}_{ji}$. 
For any vertex $v_i$, the degree of $v_i$ is defined as $d(v_i) = \sum_{j}\mathrm {A}_{ij}$, and $D = \mathrm {diag}(d(v_1),d(v_2),\dots,d(v_n))$ refers to the degree matrix.

Suppose that $\mathcal{P} =
\{P_1, P_2,\dots, P_L\}$ is a partition of $V$. 
Each $P_i$ is called a \textit{community} (aka. module or cluster), representing a group of vertices with commonality.
% such that the density of edges between vertices is higher than the density with the rest of the graph. 
Due to the grouping nature of a real-world network, each community of the graph can be hierarchically split into multi-level \textit{sub-communities}. 
Such \textit{hierarchical community} partition (i.e., hierarchical \textit{semantic}) of a graph can be intrinsically abstracted as the encoding tree~\cite{li2016structural,li2018decoding}, and each tree node represents a specific community. 
Take Fig.~\ref{fig:intro} as an example: at a high abstraction (semantic) level, the entire graph can be categorized as two coarse-grained communities, i.e., teachers (T) and students (S). 
Students can be identified as sub-communities like S.1 and S.2, as per the class placement scheme.














\noindent \textbf{Graph Structure Learning (GSL)}. 
For any given graph $G$, the goal of GSL~\cite{zhu2021deep} is to simultaneously learn an optimal graph structure $G^*$ optimized for a specific downstream task and the corresponding graph representation $Z$. 
In general, the objective of GSL can be summarized as $\mathcal{L}_{gsl} = \mathcal{L}_{task}(Z,Y) + \alpha \mathcal{L}_{reg}(Z,G^*,G)$,
% \begin{equation}\label{eq:LGSL}
% \end{equation}
where $\mathcal{L}_{task}$ refers to a task-specific objective with respect to the learned representation $Z$ and the ground truth $Y$.
$\mathcal{L}_{reg}$ imposes constraints on the learned graph structure and representations, and $\alpha$ is a hyper-parameter.




\subsection{Structural Entropy}
Different from information entropy (aka. Shannon entropy) that measures the uncertainty of probability distribution in information theory ~\cite{shannon1948mathematical}, \textit{structural entropy}~\cite{li2016structural} measures the structural system diversity, e.g., the uncertainty embedded in a graph.








\noindent \textbf{Encoding Tree}. 
Formally, the encoding tree $\mathcal{T}$ of graph $G=(V, E)$ holds the following properties:
\textbf{(1)} The root node $\lambda$ in $\mathcal{T}$ has a label $T_\lambda = V$, $V$ represents the set of all vertices in $G$.
\textbf{(2)} Each non-root node $\alpha$ has a label $T_\alpha \subset V$. 
Furthermore, if $\alpha$ is a leaf node, $T_\alpha$ is a singleton with one vertex in $V$.
\textbf{(3)} For each non-root node $\alpha$, its parent node in $T$ is denoted as $\alpha^-$.
\textbf{(4)} For each non-leaf node $\alpha$, its $i$-th children node is denoted as $\alpha^{\left \langle i \right \rangle }$ ordered from left to right as $i$ increases.
\textbf{(5)} For each non-leaf node $\alpha$, assuming the number of children $\alpha$ is $N$, all vertex subset $T_{\alpha^{\left \langle i \right \rangle}}$ form a partition of $T_\alpha$, written as $T_\alpha = {\textstyle \bigcup_{i=1}^{N}} T_{\alpha^{\left \langle i \right \rangle}}$ and ${\textstyle \bigcap_{i=1}^{N}} T_{\alpha^{\left \langle i \right \rangle}} = \varnothing$.
If the encoding tree's height is restricted to $K$, we call it \textit{$K$-level} encoding tree. 
Entropy measures can be conducted on different encoding trees.








\noindent \textbf{One-dimensional Structural Entropy}. \label{prelim:1dse}
In a single-level encoding tree $\mathcal{T}$, its structural entropy degenerates to the unstructured Shannon entropy, which is formulated as:
\begin{equation}\label{eq:H1}
H^1(G) = -\sum_{v \in V}{\frac{d_v}{vol(G)}\log_{2}{\frac{d_v}{vol(G)}}},
\end{equation}
where $d_v$ is the degree of vertex $v$, and $vol(G)$ is the sum of the degrees of all vertices in $G$.
According to the fundamental research~\cite{li2016structural}, one-dimensional structural entropy $H^1(G)$ measures the uncertainty of vertex set $V$ in $G$, which is also the upper bound on the amount of information embedded in $G$.

\noindent \textbf{High-dimensional Structural Entropy}. 
For the encoding tree $\mathcal{T}$, we define high-dimensional structural entropy of $G$ as:
\begin{equation}\label{eq:HK}
H^K(G) = \min_{\forall \mathcal{T}:height(\mathcal{T}) \le K}\{H^{\mathcal{T}}(G)\},
\end{equation}
\begin{equation}\label{eq:HT}
% \vspace{-0.1em}
H^{\mathcal{T}}(G) = \sum_{\alpha \in \mathcal{T},\alpha \ne \lambda} {H^{\mathcal{T}}(G;\alpha)} = -\sum_{\alpha \in \mathcal{T},\alpha \ne \lambda} {\frac{g_\alpha}{vol(G)}\log_{2}{\frac{\mathcal{V}_{\alpha}}{\mathcal{V}_{\alpha^-}}}},
\end{equation}
where $g_\alpha$ is the sum weights of the cut edge set $[T_\alpha,T_\alpha/T_\lambda]$, i.e., all edges connecting vertices inside $T_\alpha$ with vertices outside $T_\alpha$. $\mathcal{V}_\alpha$ is the sum of degrees of all vertices in $T_\alpha$. 
$H^{\mathcal{T}}(G;\alpha)$ is the structural entropy of node $\alpha$ and $H^{\mathcal{T}}(G)$ is the structural entropy of $\mathcal{T}$. 
$H^K(G)$ is the $K$-dimensional structural entropy, with the optimal encoding tree of $K$-level .