%%%% ijcai23.tex

\typeout{IJCAI--23 Instructions for Authors}

% These are the instructions for authors for IJCAI-23.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai23.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai23}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{multirow}
\usepackage[table]{xcolor}
\usepackage{array} 


\definecolor{myorange}{rgb}{.99, .55, .33}
\definecolor{myblue}{rgb}{.33,.59,.85}
\definecolor{mygrey}{rgb}{1.0,1.0,1.0}

% Comment out this line in the camera-ready submission
%\linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.
% Please **do not** include Title and Author information
\pdfinfo{
/TemplateVersion (IJCAI.2023.0)
}

\title{Multi-Frame Self-Supervised Depth Estimation with Multi-Scale Feature Fusion in Dynamic Scenes}


% Single author syntax
% \author{
%    Author Name
%     \affiliations
%   Affiliation
%     \emails
%     email@example.com
%}

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
%\iffalse
\author{
 Jiquan Zhong\and
 Xiaolin Huang
 \and
 Xiao Yu
 }
% \affiliations
% $^1$Department of Automation, Xiamen University, Xiamen, China\\
% $^2$Department of Automation, Shanghai Jiao Tong University, Shanghai, China\\
% \emails
% zhongjiquan@stu.xmu.edu.cn,
% xiaoyu@xmu.edu.cn,
% xiaolinhuang@sjtu.edu.cn
% }
%\fi

\begin{document}

\maketitle

\begin{abstract}
    %Self-supervised monocular depth learning methods are based on the assumption that the frames are captured in a static environment. Recent multi-frame depth estimation methods achieve better performance than single-frame methods. However, dynamic objects lead to heavier performance degradation in the multi-frame methods than that in the single-frame methods. 
    Multi-frame methods improve monocular depth estimation over single-frame approaches by aggregating spatial-temporal information via feature matching. However, the spatial-temporal feature leads to accuracy degradation in dynamic scenes. To enhance the performance, recent methods tend to propose complex architectures for feature matching and dynamic scenes. In this paper, we show that a simple learning framework, together with designed feature augmentation, leads to superior performance. (1) A novel dynamic objects detecting method with geometry explainability is proposed. The detected dynamic objects are excluded during training, which guarantees the static environment assumption and relieves the accuracy degradation problem of the multi-frame depth estimation. (2) Multi-scale feature fusion is proposed for feature matching in the multi-frame depth network, which improves feature matching, especially between frames with large camera motion. (3) The robust knowledge distillation with a robust teacher network and reliability guarantee is proposed, which improves the multi-frame depth estimation without computation complexity increase during the test. The experiments show that our proposed methods achieve great performance improvement on the multi-frame depth estimation.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Environmental depth perception is significant for autonomous mobile agents, such as autonomous vehicles and mobile robots. Like the human's depth perception ability from visual information, obtaining depth estimation from RGB images is a more attractive way than directly from the specialized hard-wares like Lidar sensors. Recent self-supervised methods enable learning monocular depth from arbitrary unlabelled image sequences~\cite{garg2016unsupervised}. While the conventional single-frame depth estimation method takes a single image as input and predicts the corresponding depth per pixel~\cite{vijayanarasimhan2017sfm}, the multi-frame method achieves better performance by considering temporal adjacent images as input~\cite{watson2021temporal}. The training procedure of both the single-frame and multi-frame methods is based on the assumption of the static environment as the self-supervision loss. However, there are many dynamic objects in real-world scenarios, which leads to the degradation on the accuracy of the learned monocular depth model~\cite{yin2018geonet}. 
%Training a monocular depth model directly with the reconstruction loss will cause the over-fitting problem on the dynamic regions.  
%With the embedded cost volume unreliable in the dynamic regions, the multi-frame depth model gets heavier over-fitting than the single-frame depth model. Despite using a separate single-frame depth model as a teacher, the multi-frame depth model still suffers from dynamic scenes especially when the single-frame teacher networks fail in dynamic regions. 
The embedded spatial-temporal feature in the multi-frame depth network brings performance improvements, but it also brings heavier over-fitting problems in the dynamic scenes. To relieve this problem, disentangling dynamic objects with semantic information and single frame depth prior~\cite{feng2022disentangling} is adopted. Recent works tend to use transformer~\cite{guizilini2022multi} and attention modules~\cite{ruhkamp2021attention} to improve feature matching, which increases resource consumption largely.

In this paper, a self-supervised monocular depth learning framework is proposed for multi-frame depth estimation.
%, as shown in Figure~\ref{fig:mainwork}. {\color{red}[HXL: it is better to at least describe the figure to show the whole framwors. Also, it is better to place Figure 1 in the next page]}. Our method requires neither an extra object motion network nor semantic information. We provide a more reasonable method to filter out the dynamic regions and improve both single-frame depth prediction and multi-frame depth prediction. 
As shown in Figure~\ref{fig:mainwork}, the depth inconsistency masks are generated to eliminate the dynamic objects first, multi-scale features are fused for context features and feature matching, and the robust teacher network with reliability guarantee is used for robust knowledge distillation.
In general, the contributions of this paper are as follows.
\begin{itemize}
	%\vspace{-0.2cm}
	\item \emph{Depth inconsistency mask} The dynamic regions break the static environment assumption and mislead both the depth prediction and the pose prediction. We leverage the over-fitting performance of the multi-frame depth estimation and propose an depth inconsistency mask to filter out the dynamic regions during training. Our depth inconsistency mask is off-the-shelf and can be used in other frameworks.
	 
	%\vspace{-0.0cm}
	\item \emph{Multi-scale feature fusion} The multi-scale feature is important for feature matching when building the cost volume in the multi-frame depth network. We propose a multi-scale feature fusion block to enhance the characterization of multi-scale features. The multi-scale feature fusion improves the context feature representation and the robustness of feature matching.
	
	%\vspace{-0.0cm}
	\item \emph{Robust knowledge distillation} To improve the multi-frame depth prediction in static frames and dynamic regions, the single-frame depth networks are used for pseudo supervision. We propose an approach using a robust teacher network and filtering out unreliable pseudo depth labels to achieve robust knowledge distillation.
	
\end{itemize}



\section{Related work}
\label{sec:relatework}

\subsection{Self-supervised monocular depth learning}

Self-supervised monocular depth learning~\cite{zhou2017unsupervised} aims to use unlabeled monocular videos to train a monocular depth network. To get a loss of depth labels while training, the photo-metric loss between target frames and reconstructed frames is adopted. However, the photo-metric loss usually failed at the occlusion, moving objects, texture-less, and illumination variance regions, causing the photo-metric loss to be based on the assumption of a static environment~\cite{zhou2017unsupervised}. Many methods are proposed to fix the artifacts of the raw photo-metric loss, for instance, minimal photo-metric loss between multiple source frames~\cite{godard2019digging}, leveraging segmentation information during training~\cite{lee2021attentive}, geometry-based occlusion masking~\cite{bian2019unsupervised}, illumination alignment~\cite{yang2020d3vo}, and objects motion detection~\cite{li2021unsupervised}. The camera intrinsic is usually needed during training, but the requirement can be loosened~\cite{gordon2019depth}. 


\subsection{Multi-frame monocular depth estimation}

Similar to the traditional SLAM methods~\cite{newcombe2011dtam}, self-supervised monocular depth methods can use multiple frames to refine the depth estimation during testing. Different from directly refining the depth estimation and ego-motion estimation as the SLAM methods, the test-time-refinement method uses the same photo-metric losses as training to fine-tune the depth network and pose network. However, in need of iterations of forward and backward computation, it costs much more time for the test-time-refinement method~\cite{chen2019self} to obtain final depth estimation.

Another method to leverage the multi-frame information is to use the recurrent network architecture~\cite{patil2020don,wang2019recurrent}. However, these recurrent monocular depth network lacks the geometry formation embedding. The spatial-temporal features are implicitly learned by the neural network which limits the representation capacity of the recurrent depth networks. These recurrent network based methods are more efficient than the test-time-refinement during testing, but demand much more time during training due to the recurrent computation.

The more efficient methods to leverage the multiple frames during training and testing are multi-view-stereo (MVS) based monocular depth networks. ManyDepth~\cite{watson2021temporal} and DepthFormer~\cite{guizilini2022multi} use ego-motion estimation and hypothesized depth bins to perform feature matching between adjacent frames. The aggregated spatial-temporal features together with the context feature are fed into the decoder for depth estimation. Different from directly using the $l1$ cost volume as the spatial-temporal features as ManyDepth~\cite{watson2021temporal}, DepthFormer~\cite{guizilini2022multi} adopts the grouped self-attention architecture to obtain the cost volume and achieve more robust spatial-temporal aggregation. TC-depth~\cite{ruhkamp2021attention} proposes a method using spatial attention and temporal attention to aggregate the spatial-temporal information and achieves scale invariant depth estimation with geometry consistency losses. 
%However, these multi-frame depth networks tend to rely on the aggregated spatial-temporal features, resulting inaccurate depth estimation areas when the cost volume or the spatial-temporal attention fails e.g. moving objects. An additional single-frame depth network is needed to regularize the multi-frame depth networks. 


\subsection{Self-supervised monocular depth learning in dynamic scenes}

The real-world scenarios are full of dynamic objects such as pedestrian, vehicles, bikes, and so on. The violation from the dynamic objects to the assumption on static objects in self-supervised depth learning needs to be relieved. CC~\cite{ranjan2019competitive} and Zhou et al.~\cite{zhou2017unsupervised} propose a method using a network to predict irregular pixels and assign low weights on the re-projection loss. Lee et al.~\cite{lee2021attentive} use the semantic box to detect dynamic objects. DynamicDepth~\cite{feng2022disentangling} proposes to use single-frame depth as prior depth, to relieve the motion of the semantic objects for a multi-frame depth network. Li et al.~\cite{li2021unsupervised} and RM-Depth~\cite{hui2022rm} propose to use a network to predict the motion of the dynamic objects and disentangle the static scenes and the dynamic objects on the re-projection loss. DiPE~\cite{jiang2020dipe} proposes an outlier mask to filter out the dynamic objects. However, the semantic-based methods need semantic labels of pre-trained semantic segmentation networks. The object's motion prediction-based methods need to train an extra network coupled with the re-projection loss. The irregular re-projection loss-based methods tend to fail when the depth prediction is over-fitting on the dynamic objects giving regular re-projection loss in the dynamic objects. %These problems reduce the generalizability of these methods.
%Different from previous works, we propose a novel dynamic detecting method based on the over-fitting performance of the multi-frame depth prediction. Our method does not need any semantic label and is an off-the-shelf method that saves space during training. 


\begin{figure*}
    \centering
     \vspace{-10pt}
    \includegraphics[width=1.0\linewidth]{mainwork_revised.pdf}
    \vspace{-20pt}
    \captionof{figure}{An overview of our proposed self-supervised multi-frame depth learning framework, including: (1) depth inconsistency mask generating, (2) multi-frame depth network with multi-scale feature fusion, and (3) self-supervised training with robust distillation.
    %We first train a multi-frame depth network aiming at over-fitting depth in dynamic regions and then generate over-fitting masks with the referee depth off-the-shelf. For the multi-frame depth network, multi-scale features are generated for context features and feature matching. A robust teacher network and unreliable pseudo supervision filtering method are used for robust knowledge distillation to the multi-frame depth network.
    }
    \vspace{-10pt}
    \label{fig:mainwork}
\end{figure*}

\section{Method}

%In this section, we first introduce the main framework of self-supervised multi-frame depth learning (Sec.~\ref{sec:revisit}). We then introduce three innovations which improve the multi-frame depth estimation: (1) depth inconsistency masks (Sec.~\ref{sec:overfitting}), (2) multi-scale feature fusion (Sec.~\ref{sec:msfusion}), and (3) robust knowledge distillation (Sec.~\ref{sec:robust}).

\subsection{Revisit multi-frame self-supervised monocular depth estimation}
\label{sec:revisit}

The joint self-supervised learning of the monocular depth and the camera ego-motion takes monocular videos to train a depth network and an ego-motion network. Single-frame depth network estimates the depth from one frame: $N_D(I_t) \rightarrow \hat{D}_{t,s}$. Pose network estimates the camera motion between two adjacent frames: $N_P(I_t, I_s) \rightarrow T_{t \rightarrow s}$. Multi-frame depth network estimates depth from adjacent frames with the ego-motion: $N_{MD}(I_t, I_s, T_{t \rightarrow s}) \rightarrow \hat{D}_t$. The joint learning of the depth network and ego-motion is the procedure of mainly minimizing the reconstruction photometric loss constructed by $l1$ norm and SSIM~\cite{wang2004image}:
\begin{equation}
    \mathcal{PE}(I_t,I_s) = \frac{\alpha}{2}(1-SSIM(I_t, I_{s \rightarrow t})) + (1-\alpha) ||I_t- I_{s \rightarrow t}||_1,
    \label{loss:proj}
\end{equation}
with $\alpha=0.85$. In Eq.~\eqref{loss:proj}, $I_{s \rightarrow t}$ is the reconstructed target frame $I_t$ from source frames $I_s$:
\begin{equation}
    I_{s \rightarrow t} = I_s < proj(\hat{D}_t, T_{t \rightarrow s}, K)>,
    \label{eqn:proj}
\end{equation}
where $proj(\cdot)$ is the camera re-projection function getting the corresponding coordinates determined by $\hat{D}_t$ and $T_{t \rightarrow s}$, $<>$ is the bi-linear sampling operator and $K$ is the camera intrinsic. The smoothness loss is adopted for regularization~\cite{zhou2017unsupervised}:
\begin{equation}
    L_{sm} = |\partial_x {\hat{d}_t}|e^{-\partial_x I_t} + |\partial_y {\hat{d}_t}|e^{-\partial_y I_t},
\end{equation}
where $\hat{d_t} = d_t / \Bar{d_t}$ is mean-normalized inverse depth.
%to relieve the shrink of the depth estimation. 
In order to filter out the unreliable occlusion areas, the per-pixel minimum reconstruction loss between multiple source frames is adopted~\cite{godard2019digging}:
\begin{equation}
    L_{ph} = \min_s \mathcal{PE}(I_t,I_s),
\end{equation}
where $s\in \{t-1,t+1\}$.
The multi-frame depth network leverages the multi-view-stereo method to embed the spatial-temporal features. The target frame $I_t$ and source frame $I_s$ are encoded to be pyramidal features $F_1$ and $F_2$. The features $F_2$ at $1/4$ raw resolution are used as context features $F_t$ and $F_s$ to balance the computation complexity and performance. 

Similar to  Eq.~\eqref{eqn:proj}, given the pose estimation $ T_{t \rightarrow s}$, the source features are re-projected to the target view $V_t$ as:
\begin{equation}
    F^i_{s \rightarrow t} = F_s < proj(d^i, T_{t \rightarrow s}, K)>,
    \label{eqn:featproj}
\end{equation}
where $d^i$ is the hypothesized depth which is obtained by discretizing the sampling in linear space, ranging from $d_{\min}$ to $d_{\max}$, into $D$ bins. Each $d^i$ is given by:
\begin{equation}
    d^i = d_{\min} + \frac{i(d_{\max}-d_{\min})}{D-1},\ i=0,1,\cdots,D-1.
\end{equation}
The $l1$ norm of $F_t - F^i_{s \rightarrow t}$ in the feature channel for each $d^i$ are aggregated to obtain the cost volume $C_{s\rightarrow t}\in R^{D\times H/4 \times W/4}$. The context features $F_t$ and cost volume $C_{s\rightarrow t}$ are fed into the encoder and decoder to generate multi-frame depth. As the photometric loss, the lower cost volume indicating the corresponding hypothesized depth is more likely to be the depth ground truth. This also means that the cost volume is unreliable in inconsistent regions such as occlusion areas and dynamic regions.
%Following Manydepth~\cite{watson2021temporal}, we use the multi-frame encoder as the depth encoder. The target $I_t$ and source $I_s$ are encoded to $F_t$ and $F_s$ at $1 / 4$ the original resolution in $C$-dimension. The source features $F_s$ are warped using $N$ discrete hypothesized depth bins and ego-motion $T_{t \rightarrow s}$. The $N$-dimensional cost volume is built by calculating the $l1$ distance between target features $F_t$ and warped source features $F_{s \rightarrow t}$ for $N$ depth bins. The cost volume and target features are concatenated and fed into the depth encoder to obtain the spatial-temporal features $F_c$. Then the target features $F_t$ at $1/2$ and $1/4$ the original resolution and encoded spatial-temporal features $F_c$ at $1/8$, $1/16$ and $1/32$ the original resolution are together fed into the depth decoder to obtain the multi-scale depth estimation. 
In order to regularize the unreliable multi-frame depth estimation, a single-frame depth teacher network is adopted for consistency loss:
\begin{equation}
  L_c = \sum M | \hat{D}_t - \hat{D}_{t,s} |,
\end{equation}
where 
%$D_t$ is the multi-frame depth estimation, $\hat{D}_{t,s}$ is the single-frame depth estimation of the teacher network, and 
$M$ is the consistency mask to figure out which areas the multi-frame depth network tends to over-fit on. The gradients to $\hat{D}_{t,s}$ are blocked. 
%of the consistency loss is blocked to the teacher network. 
The consistency mask is calculated by:
\begin{equation}
  M = [\max(\frac{D_{cv}-\hat{D}_{t,s}}{\hat{D}_{t,s}}, \frac{\hat{D}_{t,s}-D_{cv}}{D_{cv}}) > 1.0],
  \label{mask:cost}
\end{equation}
where $[\cdot ]$ is the Iverson bracket and $D_{cv}$ is the depth hints presented by the minimum cost volume among all the depth bins. The single-frame depth network and the multi-frame depth network share the same pose network. To relieve the impact of the static cameras, following~\cite{watson2021temporal}, static frame augmentation and zero cost volume augmentation are adopted.




\subsection{Learn from over-fitting}
 \label{sec:overfitting}

\subsubsection{How the depth over-fit in dynamic regions}

In the re-projection loss or the cost volume in the multi-frame depth network, the frame $I_s$ or the feature $F_s$ is warped to the target view through the re-projection function under the assumption of a static environment. 
%In real world, given depth prediction $D_t$, depth ground truth $\Tilde{D_t}$, 2D coordinate $P_{ij}$ and dynamic objects' motion $T_{obj}$, we can obtain:
%\begin{equation*}
%    D_t P_{ij} = \Tilde{D_t} P_{ij} + T_{obj},
%\end{equation*}
%as the projection function. In the dynamic regions where $T_{obj}$ is not zero, the depth prediction will over-fit through the re-projection loss. As the illustration in Figure~\ref{fig:overfit}
As the green line in Figure~\ref{fig:overfit} shows, given the depth prediction $\hat{D_t}$, camera intrinsic $K$, and the pixel coordinate $[u_t,v_t]$, the 3D coordinate $\hat{P_t}$ of the corresponding point can be obtained through the pinhole camera geometry~\cite{hartley2003multiple}:
\begin{equation}
    \hat{P_t} = \hat{D_t}K^{-1}[u_t,v_t,1]^T,
    \label{eqn:projs}
\end{equation}
where $\hat{P_t}$ is the 3D coordinate from the view $V_t$. With the pose estimation $T_{t \rightarrow t-1}$ and the motion $T_{obj}$ of the corresponding objects, the 3D coordinate $\hat{P}_{ t-1}$ from the view $V_{t-1}$ is calculated by:
\begin{equation}
    \hat{P}_{ t-1} = T_{t \rightarrow t-1} \hat{P_t} + T_{obj}.
    \label{eqn:dymotion}
\end{equation}
 Then the corresponding pixel $[\hat{u}_{t-1},\hat{v}_{t-1}]$ at time $t-1$ of $[u_t,v_t]$ can be obtained by:
\begin{equation}
    [\hat{u}_{t-1},\hat{v}_{t-1}, 1] ^ T = K \hat{P}_{ t-1}.
    \label{eqn:reproj}
\end{equation}
When the motion $T_{obj}$ of the corresponding dynamic objects is given, the point $P_{t-1}$ can be obtained by Eqs.~\eqref{eqn:projs} and \eqref{eqn:dymotion}. However, the motion $T_{obj}$ of the corresponding dynamic objects is regarded as zero under the static environment assumption. As the blue/red line in Figure~\ref{fig:overfit} shows, once the projection point $\hat{P_t}$ is on the line $\overline{V_{t-1}P_{t-1}}$, the same matching pixel coordinate $[\hat{u}_{t-1},\hat{v}_{t-1}]$ can be obtained. In order to obtain the matching pixel coordinate $[\hat{u}_{t-1},\hat{v}_{t-1}]$, the depth network will give a over-fitting depth prediction $\hat{D_t}$ and obtain the projection point $\hat{P_t}$. As illustrated in Figure~\ref{fig:overfit}, the over-fitting depth will be greater or smaller than the ground truth when the dynamic objects have a co-directional or contra-directional motion with regard to the camera.


\begin{figure}[t]
    \centering
     \vspace{-10pt}
    \includegraphics[width=0.8\linewidth]{overfit_revised.pdf}
    \vspace{-8pt}
    \captionof{figure}{The depth over-fitting problem of the dynamic objects. $V_t$ and $V_{t-1}$: camera views at time $t$ and $t-1$. $P_t$ and $P_{t-1}$: the 3D coordinates of the dynamic objects at time $t$ and $t-1$. $D_t$ and $\hat{D_t}$: the depth ground truth and over-fitting depth prediction.
    %The over-fitting problems of the dynamic objects with co-directional motion and contra-directional motion are represented in red and blue respectively.
    }
    \vspace{-6pt}
    \label{fig:overfit}
\end{figure}



\subsubsection{Generating depth inconsistency mask}

As mentioned above, training the depth network by minimizing the re-projection loss function under the static environment assumption will lead to over-fitting problems in dynamic regions. To relieve the problem of dynamic objects, we leverage the over-fitting performance of the multi-frame depth network to generate the depth inconsistency mask. Due to the embedded cost volume, the over-fitting problem of the multi-frame depth network is much heavier than that of the single-frame depth network. 

Under this observation, we first train a multi-frame depth network without depth supervision in the dynamic regions from the teacher network. We keep the static frame augmentation and zero cost volume augmentation~\cite{watson2021temporal} during training, such that the over-fitting multi-frame depth network can generate more reliable depth prediction in the static regions. The pre-trained multi-frame network is used to generate over-fitting depth prediction $D_{o}$. To obtain reference depth prediction $D_r$, we pre-train a single-frame depth network separately. 
%Although the single-frame depth network can only estimate depth from the context features, yet over-fitting problems still exist. 
For robust performance in dynamic regions, we use the HRNet-18~\cite{wang2020deep} as the backbone of the single-frame depth network. The different resolution exchanging information enhances the robustness of context feature extraction, leading to robust performance in the dynamic regions. Given the over-fitting depth prediction $D_{o}$ and robust depth prediction $D_r$, we express the depth inconsistency masks as $M_{co}$, $M_{con}$, and $M_{ground}$:
\begin{equation}
    \begin{array}{l}
    M_{co} = [  D_o^{\ast} > 2D_r],\\
    M_{con} = [  D_o^{\ast} < 0.85D_r], \\
    M_{ground} = [- y_g< \hat{Y} < y_g],\\
    \end{array}    
\end{equation}
where $D_o^{\ast}$ is the scale aligned depth by median value aligning between $D_o$ and $D_r$, $y_g$ is the camera height calculated by fitting the ground plane~\cite{xue2020toward} and $\hat{Y}$ is the $y$-coordinate of the corresponding pixel. The range of over-fitting depth variation on objects with opposite directions of motion is smaller than on objects with the same direction of motion. In order to detect the over-fitting regions caused by the dynamic objects with contra-direction motion, a small comparison threshold is used for $M_{con}$. 
%However, this brings the wrong detection in the static regions, which usually happens on the ground region. 
Due to the difference in network architecture and training procedure, the over-fitting depth and reference depth may disagree with each other even in some rigid regions, leading to wrong dynamic detection in the static regions. Noting that dynamic objects like cars and pedestrians are on the ground.
As shown in Figure~\ref{fig:masks}, we use the ground mask $M_{ground}$ to filter out the wrong detection regions. The final over-fitting binary mask $M_i$ is:
\begin{equation}
    M_i = (M_{co} \odot M_{con}) \bullet M_{ground},
\end{equation}
where $\odot$ is the logical or operator and $\bullet$ is the logical and operator. The depth inconsistency mask is set to $1$ in dynamic regions and $0$ in static regions. Besides, it is used to filter out the dynamic regions in the re-projection loss. Compared to the consistency mask $M$ from Eq.~\eqref{mask:cost} proposed by ManyDepth~\cite{watson2021temporal}, our depth inconsistency mask $M_i$ can better filter out dynamic regions.

\begin{figure}[t]
    \vspace{-4pt}	
    \centering
    \includegraphics[width=1.0\linewidth]{mask.pdf}
    \vspace{-2pt}	
    \begin{tabular}{p{2.32cm}<{\centering}p{2.32cm}<{\centering}p{2.32cm}<{\centering}}
    %\specialrule{0em}{2pt}{2pt}
    %\renewcommand\arraystretch{0.1}
    \footnotesize
    (a) $I_t$ &\footnotesize  $D_o$ &\footnotesize (c) $D_r$ 
    %\hline
    \end{tabular}
    
    \centering
    \includegraphics[width=1.0\linewidth]{mask2.pdf}
    \vspace{-2pt}	
    \begin{tabular}{p{2.32cm}<{\centering}p{2.32cm}<{\centering}p{2.32cm}<{\centering}}
    %\specialrule{0em}{2pt}{2pt}
    %\renewcommand\arraystretch{0.1}
    %\hline
    \footnotesize
    (d) $M_{con}$ &\footnotesize  (e) $M_{co}$ &\footnotesize (f) $D_{cv}$ 
    %\hline
    \end{tabular}
    
    \centering
    \includegraphics[width=1.0\linewidth]{mask3.pdf}
    \vspace{-2pt}	
    \begin{tabular}{p{2.32cm}<{\centering}p{2.32cm}<{\centering}p{2.32cm}<{\centering}}
    %\specialrule{0em}{2pt}{2pt}
    %\renewcommand\arraystretch{0.1}
    \footnotesize
    (g) $M_{ground}$ &\footnotesize  (h) $M_{i}$ &\footnotesize (i) $M$ 
    %\hline
    \end{tabular}
    \vspace{-2pt}
    \captionof{figure}{Intermediate results of our depth inconsistency mask $M_i$ and the comparison with previous consistency mask $M$. }
    \vspace{-4pt}
    \label{fig:masks}
    
%    \centering
%    \includegraphics[width=1.0\linewidth]{visual.pdf}
%    \vspace{-6pt}	
%    \begin{tabular}{p{4.15cm}<{\centering}p{4.15cm}<{\centering}p{4.15cm}<{\centering}p{4.15cm}<{\centering}}
%    RGB image &  over-fitting mask & without over-fitting mask  & depth
%    \end{tabular}
%    \vspace{-4pt}
%    \captionof{figure}{Examples of over-fitting masks and depth predictions on KITTI.}
%    \label{fig:visual2}
\end{figure}


\subsection{Multi-scale feature fusion}
\label{sec:msfusion}

The convolution networks are poor in scale-invariant feature representation. 
%This results in large performance degradation when the depth network trained on a fixed resolution is used for depth prediction in other resolutions. 
However, it is important to extract scale-invariant features for feature matching. Due to the motion of the camera view, the feature points to be matched are in different scales in adjacent frames. For instance, when the camera is approaching the objects, the scale of the objects in the image gradually becomes larger. To enhance the characterization ability of context features and the accuracy of feature matching, the multi-scale feature fusion block is proposed. Different from previous works~\cite{Guo_2020_CVPR}, we pursue finer-grained  multi-scale feature representations that preserve differences in pixel-level features, rather than hoping that the multi-scale features are embedded in the entire feature map. As illustrated in Figure~\ref{fig:msfusion}, the encoded features $F_1$ and $F_2$ are fed into the multi-scale feature fusion to generate multi-scale features. Given a high-resolution feature $F_1$ and a middle-resolution feature $F_2$, the fused features are generated by:
\begin{equation}
    \begin{array}{l}
    F_{12} = Convs(Convs(F_1) \oplus F_2),\\
    F_{32} = Up.(ResBlock(F_2)), \\
    F_{ms} = F_2 \oplus F_{12} \oplus F_{32},\\
    \end{array}    
\end{equation}
where $Up.(\cdot)$ is the bilinear upsample operator, $\oplus $ is the concatenation operator, and $ResBlock(\cdot)$ is the residual block~\cite{he2016deep}. The feature map $F_{12}$ with smaller scale features is generated from $F_1$ through the convolution block. The stride of the convolution block is set to 2 in order to align the resolution as $F_2$. The feature map $F_2$ is used to generate larger scale features through residual block first. Then, the feature map $F_{32}$ is upsampled to align the resolution as $F_2$. In order to align all features of different scales to the same feature space,
we fuse the low-scale feature map $F_{12}$ and the high-scale feature map $F_{32}$ with the feature map $F_2$ respectively. The feature maps of different scales are concatenated together to get multi-scale feature representation. The raw context features $F_2$ are then replaced by the fused multi-scale features $F_{ms}$ as the context features $F_t$ and $F_s$.


\begin{figure}[t]
    \centering
     \vspace{-10pt}
    \includegraphics[width=0.8\linewidth]{msfusion.pdf}
    \vspace{-8pt}
    \captionof{figure}{The MS Fusion block in figure~\ref{fig:mainwork}. Features with different scales are in different colors.}
    \vspace{-6pt}
    \label{fig:msfusion}
\end{figure}

\subsection{Robust knowledge distillation}
\label{sec:robust}


In this section, we introduce how to regularize the multi-frame depth network with knowledge distillation methods from the single-frame teacher in a robust way. As discussed above, using the high-resolution network as the backbone of the single-frame depth network provides robust performance in dynamic regions, which helps to detect over-fitting regions for the multi-frame depth network in Eq.~\eqref{mask:cost} in a robust way. However, in that, the re-projection is from both the depth and the pose, which means that the depth prediction will influence the training of the pose network. Once the depth prediction becomes accurate in dynamic regions, the mismatch in the dynamic regions will confuse the pose prediction. Then, the inaccurate pose prediction will infect the depth prediction in static regions and vice versa. Therefore, the depth inconsistency masks will be used for the training of the teacher network.

%Although the MVS methods bring spatial-temporal features, there are static scenes that the camera has no movement e.g. the car stops at the traffic lights. The cost volume will fail to provide extra features from the static frames with no baseline. To enable the multi-frame depth network to predict reliable depth prediction between static frames, static frames augmentation and zero cost volume augmentation methods~\cite{watson2021temporal} are adopted. 
With the static frame augmentation and zero cost volume augmentation~\cite{watson2021temporal}, the cost volume fails to provide helpful spatial-temporal features. When the cost volume fails, the depth hints $D_{cv}$ become unreliable in all regions, which makes the single-frame depth become the depth supervision for all regions rather than the dynamic regions. For the augmented samples, we check the reliability of the depth pseudo supervision between the single-frame depth and the multi-frame depth. Given the depth predictions, the robust mask $M_r$ is calculated by:
\begin{equation}
    M_r = [ L_{ph,s} < L_{ph}],
\end{equation}
where $L_{ph,s}$ and $L_{ph}$ are the photo-metric loss of the single-frame depth and multi-frame depth respectively, and $[\cdot ]$ is the Iverson bracket. The robust mask filters out the unreliable depth supervision from the teacher network and thus provides robust knowledge distillation for the multi-frame depth network. For the augmented samples, the consistency loss is:
\begin{equation}
    L_c = \sum (M_r \bullet M)|  \hat{D}_t - \hat{D}_{t,s}  |,
\end{equation}
where $\bullet$ is the logical and operator, and $M$ is the consistency mask from Eq.~\eqref{mask:cost}. The robust mask $M_r$ will not be used in the dynamic regions, since the over-fitting depth prediction leads to smaller re-projection loss compared with the single-frame depth reference. This, in turn, can leave the multi-frame depth network unsupervised by the teacher network in dynamic regions. For the training samples without augmentation, $M_r$ is set to $1$ for all pixels.

The final loss is $L= ((1-M)\bullet(1-M_i))L_{ph} + L_c + \beta L_{sm} + (1-M_i)L_{ph,s} + \beta L_{sm,s}$, where $L_{ph,s}$ and $L_{sm,s}$ are losses of the teacher network and $\beta = 1e^{-3}$. The teacher network is trained together throughout the training procedure.
\setlength{\tabcolsep}{0.9mm}
\begin{table*}[t]
	%\centering
    %\vspace{-10pt}
    \renewcommand\arraystretch{1.1}
	%\footnotesize
        %\small
        \normalsize


	\begin{center}
		\resizebox{1.0\textwidth}{!}
		{
			\begin{tabular}{|>{\columncolor{white}}c|l|c|c|c|cccc|ccc|}
                %\\
                \hline
				%\toprule
				& \multirow{2}{*}{\bf Method} & \multirow{2}{*}{Test frames} & \multirow{2}{*}{Semantic} & \multirow{2}{*}{$W\times H$}   & \cellcolor{myorange}AbsRel & \cellcolor{myorange}SqRel & \cellcolor{myorange}RMSE & \cellcolor{myorange}RMSE log & \cellcolor{myblue}$\delta < 1.25$ & \cellcolor{myblue}$\delta < 1.25^2$ &  \cellcolor{myblue}$\delta < 1.25^3$\\
				%\cline{6-12}
				& & & & & \multicolumn{4}{c|}{\cellcolor{myorange}$Lower\ is\ better$} & \multicolumn{3}{c|}{\cellcolor{myblue}$Higher\ is\ better$}\\
				\hline
                %\\
                \hline
                
                %& Monodepth2~\cite{godard2019digging} & 1 & & $640 \times 192$ & 0.115  &   0.903   &  4.863  &   0.193   &  0.877  &   0.959  &   0.981\\
                & Lee et al.~\cite{lee2021attentive} & 1  & $\surd$ & $832 \times 256$ & 0.114   &   0.876   &   4.715   &   0.191   &   0.872   &   0.955   &   0.981\\
                %& DiPE~\cite{jiang2020dipe} & 1 & & $640 \times 192$   &   0.112   &   0.875   &   4.795   &   0.190   &   0.880   &   0.960   &   0.981\\
                & InstaDM~\cite{lee2021learning}  & 1  & $\surd$ & $832 \times 256$ & 0.112   &   0.777   &   4.772   &   0.191   &   0.872   &   0.959   &   0.982\\
                %&Patil et al.~\cite{patil2020don} & N & & $640 \times 192$ &  0.111   &   0.821   &   4.650   &   0.187   &   0.883   &   0.961   &   0.982\\
                %&Packnet-SFM~\cite{guizilini20203d} & 1 & & $640 \times 192$ & 0.111   &   0.785   &   4.601   &   0.189   &   0.878   &   0.960   &   0.982\\
                &Wang et al.~\cite{wang20223d}& 1  &   & $832 \times 256$ &0.109   &   0.790   &   4.656   &   0.185   &   0.882   &   0.962   &   0.983\\
                &RM-depth~\cite{hui2022rm}& 1 & & $640 \times 192$ & 0.108   &   0.710   &   4.513   &   0.183   &   0.884   &   0.964   &   0.983\\
                %&Johnston et al.~\cite{johnston2020self} & 1 & & $640 \times 192$ & 0.106   &   0.861   &   4.699   &   0.185   &   0.889   &   0.962   &   0.982\\
                %&Wang et al.~\cite{wang2020self}& 2 (-1, 0) &          & $640 \times 192$ &0.106   &   0.799   &   4.662   &   0.187   &   0.889   &   0.961   &   0.982\\
                &TC-depth~\cite{ruhkamp2021attention}& 3 (-1, 0, +1) &          & $640 \times 192$ &0.103   &    0.746   &    4.483   &    0.180   &    0.894   &    0.965   &    0.983\\
                &DIFFNet~\cite{zhou2021self} & 1  &    & $640 \times 192$ & 0.102   &    0.764   &    4.483   &    0.180   &    0.896   &    0.965   &    0.983\\
                %&Guizilini et al.~\cite{guizilini2020semantically} & 1  & $\surd$ & $640 \times 192$ & 0.102   &   0.698   &   4.381   &   0.178   &   0.896   &   0.964   &   \underline{0.984}\\
				& ManyDepth~\cite{watson2021temporal}    & 2 (-1, 0) &          & $640 \times 192$ & 0.098     &   0.770    &    4.459    &    0.176    &    0.900    &    0.965    &    0.983\\
				& Dynamicdepth~\cite{feng2022disentangling} & 2 (-1, 0) & $\surd$  & $640 \times 192$ & \underline{0.096}    &    0.720    &    4.458    &    0.175    &    0.897    &    0.964    &    \underline{0.984}\\
                &RA-depth~\cite{he2022ra} & 1  &   & $640 \times 192$ & \underline{0.096}    &    \bf0.632    &    4.216    &    \underline{0.171}    &    0.903    &    \bf0.968    &    \bf0.985\\
				& DepthFormer~\cite{guizilini2022multi}  & 2 (-1, 0) &          & $640 \times 192$ & \bf0.090    &     0.661    &     \bf4.149    &     0.175    &     \underline{0.905}    &     \underline{0.967}    &     \underline{0.984}\\
                %\rowcolor{gray!30}
                \cline{2-12}%\rowcolor{gray!30}\cellcolor{mygrey}
                \rowcolor{gray!30}
                  \multirow{-11}{*}{\cellcolor{white}\rotatebox{90}{KITTI}} &  \bf Ours     & 2 (-1, 0) &          & $640 \times 192$ &   \bf0.090  &   \underline{0.653}  &   \underline{4.173}  &   \bf0.167  &   \bf0.911  &   \bf0.968  &   \bf0.985  \\
                \hline
                %\\
                \hline
                &Li et al.~\cite{li2021unsupervised} & 1  &   & $416 \times 128$ & 0.119   &    1.290   &    6.980   &    0.190   &    0.846   &    0.952   &    0.982\\
                & Lee et al.~\cite{lee2021attentive} & 1  & $\surd$ & $832 \times 256$ &0.116   &    1.213   &    6.695   &    0.186   &    0.852   &    0.951   &    0.982\\
                & InstaDM~\cite{lee2021learning}  & 1  & $\surd$ & $832 \times 256$ &0.111   &    1.158   &    6.437   &    0.182   &    0.868   &    0.961   &    0.983\\
                & ManyDepth~\cite{watson2021temporal}    & 2 (-1, 0) &          &    $416 \times 128$ &    0.114   &   1.193   &   6.223   &   0.170   &   0.875   &   0.967   &   0.989\\
                & DynamicDepth~\cite{feng2022disentangling} & 2 (-1, 0) &  $\surd$ &   $416 \times 128$  &   0.103   &    1.000    &   5.867    &   0.157   &    \underline{0.895}   &    0.974    &   0.991\\
                & RM-depth~\cite{hui2022rm}     &     1     &          &  $640 \times 192$   &   \underline{0.100}    &  \bf0.839    &   \underline{5.774}    &   \underline{0.154}    &   \underline{0.895}    &   \underline{0.976}    &   \bf0.993 \\
                \cline{2-12}
                \rowcolor{gray!30}
                 \multirow{-7}{*}{\cellcolor{white}\rotatebox{90}{Cityscapes}} & \bf Ours      & 2 (-1, 0) &          &  $416 \times 128$   &   \bf0.098  &  \underline {0.946}  &   \bf5.553  &   \bf0.148  &   \bf0.908  &   \bf0.977  &   \underline{0.992}  \\
				%\bottomrule
                \hline
			\end{tabular}
        }
	\end{center}
	\vspace{-8pt}
	\caption{The comparison of depth estimation on KITTI and Cityscapes datasets. The listed results of KITTI are all evaluated on origin depth from velodyne. Following ManyDepth, we use â€œA" crop for Cityscapes
 %the top and the sides of the depth predictions are cropped out for Cityscapes, 
 resulting in the $W\times H$ as $416 \times 128$ rather than $512 \times 192$.}
        \vspace{-8pt}
	%\vspace{-9pt}			
	\label{table:results}
\end{table*}



\begin{figure*}[!t]
    \vspace{-4pt}	
    \centering
    \includegraphics[width=1.0\linewidth]{visual_revise.pdf}
    \vspace{-6pt}	
    \begin{tabular}{p{4.15cm}<{\centering}p{4.15cm}<{\centering}p{4.15cm}<{\centering}p{4.15cm}<{\centering}}
    %\specialrule{0em}{2pt}{2pt}
    %\renewcommand\arraystretch{0.1}
    \footnotesize
    RGB image &\footnotesize  Monodepth2 &\footnotesize ManyDepth  &\footnotesize Ours
    %\hline
    \end{tabular}
    \vspace{-2pt}
    \captionof{figure}{Qualitative depth estimation comparison on KITTI. Our method performs better on dynamic objects and small-scale objects.}
    \vspace{-4pt}
    \label{fig:visual}

\end{figure*}

\section{Experiments}


\subsection{Datasets}

\subsubsection{KITTI}
The KITTI~\cite{geiger2012we} is a standard depth evaluation benchmark. To compare with other approaches, we use the data split of Eigen et al.~\cite{eigen2015predicting}. Following ManyDepth~\cite{watson2021temporal}, we adopt Zhou et al.'s~\cite{zhou2017unsupervised} pre-processing to remove the static frames. This results in 39810/4424/697 monocular samples for training/validation/testing.
%4424 monocular samples for validation, and 697 samples for testing.

\subsubsection{Cityscapes}
Cityscapes~\cite{cordts2016Cityscapes} is a challenging dataset full with dynamic scenes. Following ManyDepth~\cite{watson2021temporal}, we use 69731 monocular samples for training. For testing, we use the official testing set that contains 1525 samples with the SGM~\cite{hirschmuller2007stereo} disparity maps.

\subsection{Implementation details}

\subsubsection{Training details}
During training, we use color and flip augmentations on the images. The augmented images are fed into both depth and pose networks. The augmentation setting follows Monodepth2~\cite{godard2019digging}. We train with Adam for 20 epochs with the learning rate $1e^{-4}$ for the first 15 epochs and $1e^{-5}$. Different from ManyDepth~\cite{watson2021temporal}, we do not freeze the teacher network and pose network for all 20 epochs for KITTI~\cite{geiger2012we}. But for Cityscapes~\cite{cordts2016Cityscapes}, we freeze the teacher network and the pose network after 112000 training samples (14000 steps with batch size as 8) and de-freeze the teacher network and the pose network for the last 5 epochs. We find it important in this challenging dataset which contains more dynamic scenes and training samples than KITTI dataset. Then hypothesized depth range is fixed when the teacher network and pose network are frozen. The resolutions of the images are $640 \times 192$ and $512 \times 192$ for KITTI and Cityscapes respectively as ManyDepth~\cite{watson2021temporal}. 

\subsubsection{Networks}
As mentioned above, we use HRNet-18~\cite{wang2020deep} as the encoder of the robust teacher. For the ablation study, we use ResNet-18~\cite{he2016deep} as the encoder of the teacher, as ManyDepth~\cite{watson2021temporal} in the case without the robust teacher. We use ResNet-18~\cite{he2016deep} as the multi-frame depth encoder except for the multi-scale feature fusion block. Depth bins $D$ is set as $96$. We use the decoder with skips as Monodepth2~\cite{godard2019digging} and output four scales depth for the depth network with ResNet-18~\cite{he2016deep} as the encoder. Nevertheless, in order to save memory and improve the training speed, we only output single-scale depth when using HRNet-18~\cite{wang2020deep} as the encoder. The pose network is the same as ManyDepth~\cite{watson2021temporal}.



\setlength{\tabcolsep}{0.9mm}
\begin{table*}[t]
	%\centering
    %\normalsize
    \small
    %\large
    %\huge
    \renewcommand\arraystretch{1.1}
	%\footnotesize

	\begin{center}
		\resizebox{0.80\textwidth}{!}
		{
			\begin{tabular}{|>{\columncolor{white}}c|c|c|c|c|cccc|ccc|}
                \hline
				%\toprule
				& Over-fitting & Multi-scale& \multicolumn{2}{c}{Robust KD} & \cellcolor{myorange}AbsRel & \cellcolor{myorange}SqRel & \cellcolor{myorange}RMSE & \cellcolor{myorange}RMSE log & \cellcolor{myblue}$\delta < 1.25$ & \cellcolor{myblue}$\delta < 1.25^2$ &  \cellcolor{myblue}$\delta < 1.25^3$\\
				\cline{4-12}
				& Mask  &  Fusion & Robust teacher & Robust mask & \multicolumn{4}{c|}{\cellcolor{myorange}$Lower\ is\ better$} & \multicolumn{3}{c|}{\cellcolor{myblue}$Higher\ is\ better$}\\
				\hline
                \hline
                &  & & &  &   0.106  &   0.788  &   4.527  &   0.183  &   0.893  &   0.964  &   0.983  \\
                \cline{2-12} 
                \rowcolor{gray!20} 
                \multirow{-2}{*}{\cellcolor{white}\rotatebox{90}{K$\ast$}} &  $\surd$ & & &   &  \bf0.101  &   \bf0.738  &   \bf4.435  &   \bf0.178  &   \bf0.896  &   \bf0.965  &   \bf0.984  \\
                \hline
                \hline
                
				&  & & & & 0.098     &   0.770    &    4.459    &    0.176    &    0.900    &    0.965    &    0.983\\
                %\cline{2-12} 
                \rowcolor{gray!20} 
                \cellcolor{white}&  & $\surd$ & &   &  0.096  &   0.752  &   4.384  &   0.176  &   0.903  &   0.965  &   0.983  \\
                %\cline{2-12}
				&  $\surd$ & & &   & 0.096  &   0.735  &   4.369  &   0.173  &   0.902  &   0.966  &   0.984  \\
                %\cline{2-12}
                \rowcolor{gray!20} 
                \cellcolor{white}& & & $\surd$ &   & 0.096  &   0.697  &   4.275  &   0.174  &   0.903  &   0.966  &   0.984  \\
                %&  $\surd$ &  $\surd$ & &   & 0.095  &   0.727  &   4.319  &   0.172  &   0.906  &   0.967  &   0.984  \\
                %\cline{2-12}
                &  & & $\surd$ & $\surd$  &   0.094  &   0.719  &   4.288  &   0.171  &   0.905  &   0.967  &   0.984  \\
                %\cline{2-12}                
                %\cline{2-12}
                %&  $\surd$ &  & $\surd$ & $\surd$  & 0.093  &   0.700  &   4.254  &   0.170  &   0.908  &   0.967  &   0.984  \\
                %& \rowcolor{gray!20} &  $\surd$ & $\surd$ & $\surd$ &  0.093  &   0.689  &   4.226  &   0.171  &   0.907  &   \bf0.968  &   0.984  \\
                %\cline{2-12}
                \rowcolor{gray!20}
                \multirow{-6}{*}{\cellcolor{white}\rotatebox{90}{KITTI}}&  $\surd$ & $\surd$ & $\surd$  &$\surd$ & \bf0.090  &   \bf0.653  &   \bf4.173  &   \bf0.167  &   \bf0.911  &   \bf0.968  &   \bf0.985  \\ 
                \hline
                \hline
                
                &   & & &  &  0.114  &   1.274  &   6.188  &   0.170  &   0.882  &   0.967  &   0.988  \\
                %\cline{2-12}
                \rowcolor{gray!20}
                \cellcolor{white}& $\surd$  & &  &  &   0.103  &   1.092  &   5.845  &   0.157  &   0.897  &   0.973  &   0.990  \\
                %&   0.103   &    1.000    &   5.867    &   0.157   &    0.895   &    0.974    &   0.991\\
                %\cline{2-12}%
				&  $\surd$ &  & $\surd$ & $\surd$  & \bf0.098  &   0.970  &   5.631  &   0.149  &   0.905  &   \bf0.977  &   \bf0.992  \\
                %\cline{2-12}
                \rowcolor{gray!20}
                \multirow{-4}{*}{\cellcolor{white}\rotatebox{90}{Cityscapes}}&  $\surd$ & $\surd$ & $\surd$ & $\surd$  & \bf0.098  &  \bf0.946  &   \bf5.553  &   \bf0.148  &   \bf0.908  &   \bf0.977  &   \bf0.992  \\
				%\bottomrule
                \hline
			\end{tabular}
	   }
	\end{center}
        \vspace{-8pt}
 	\caption{The ablation study results of depth estimation on KITTI and Cityscapes. K$\ast$ is the ablation study of the teacher network on KITTI.}
        \vspace{-8pt}

	%\vspace{-9pt}			
	\label{table:ablation}
\end{table*}

\subsection{Depth evaluation results}

Following the previous works~\cite{watson2021temporal,feng2022disentangling}, we use the standard evaluation errors as metrics, perform scale alignment between the depth prediction and ground truth, and evaluate only less than 80 meters. 

Table~\ref{table:results} and Figure~\ref{fig:visual} show the comparison between the previous methods and our method. We rank methods based on the absolute-relative-error. Our method achieves state-of-the-art (SOTA) results compared to previous methods both on KITTI and Cityscapes datasets. Our method outperforms the previous works on both single-frame and multi-frame. Compared to the previous semantic-based methods~\cite{lee2021attentive,lee2021learning,feng2022disentangling}, we do not need semantic labels or pre-trained semantic segmentation model which leverages extra semantic labeling work. Different from the motion model based methods~\cite{li2021unsupervised,hui2022rm}, our depth inconsistency masks are off-the-shelf once generated, which saves the training time cost on the motion prediction networks. Our multi-scale feature fusion provides more accurate depth cues and is more efficient than the transformer based method~\cite{guizilini2022multi}. Our method needs only 3.8 GB GPU test memory and 20 training epochs, while DepthFormer~\cite{guizilini2022multi} needs 6.4 GB GPU test memory and 50 training epochs. In the challenging Cityscapes dataset, our method outperforms the previous multi-frame method with auxiliary semantic information and single depth prior~\cite{feng2022disentangling}. Our method achieves better results than the SOTA single-frame method~\cite{hui2022rm} even in a lower resolution. 








\subsection{Ablation study}

We perform ablation experiments on both the KITTI dataset and the Cityscapes dataset. The results are as shown in Table~\ref{table:ablation}. We use the depth evaluation results trained in our proposed freezing way for the baseline of Cityscapes, rather than the direct results from ManyDepth~\cite{watson2021temporal}
%to show the effectiveness of our method clearly. 

\subsubsection{Depth inconsistency mask}

As shown in Table~\ref{table:ablation},
%and Figure~\ref{fig:visual}, 
the results illustrate the effectiveness of our proposed depth inconsistency masks. Using our depth inconsistency masks to filter out the dynamic objects can improve the depth evaluation results, no matter whether the multi-scale feature fusion block or the robust knowledge distillation is adopted. The depth inconsistency masks improve the performance of the teacher network as shown in Table~\ref{table:ablation}. As mentioned in Sec.~\ref{sec:robust}, the robust depth prediction in dynamic regions will mislead the pose network through the re-projection loss. Therefore, using the depth inconsistency masks to filter out the dynamic regions during training improves the pose prediction and hence improves the single-frame depth prediction. The limitation is that our depth inconsistency mask is base on the camera motion assumption, and additional methods are required for static frames like stationary masks~\cite{godard2019digging}.

\subsubsection{Multi-scale feature fusion} 
As shown in Table~\ref{table:ablation}, the multi-scale feature fusion block improves the depth evaluation results. Compared to the experiments on Cityscapes~\cite{cordts2016Cityscapes}, the multi-scale feature fusion block achieves larger improvement on KITTI~\cite{geiger2012we}. That is because the frequency of the camera is higher in Cityscapes dataset than that in the KITTI dataset, which means a larger scale difference in the feature points between adjacent frames in the KITTI dataset. Therefore, the performance difference of the multi-scale feature fusion block in different datasets is also more indicative of the effectiveness of the method. As shown in Table~\ref{table:speed}, we evaluate the depth on the Cityscapes dataset with different test frames as inputs. The results show the effectiveness of the multi-scale feature fusion block when there is a larger difference in scales between adjacent frames.

\subsubsection{Robust knowledge distillation} 
As shown in Table~\ref{table:ablation}, the robust knowledge distillation improves the multi-frame depth evaluation results. Although using the robust knowledge distillation extends the training time from about eight hours to about eleven hours on a single Nvidia RTX 3070Ti GPU, yet the inference speed as well as the memory usage of the multi-frame depth network during testing are not affected, since the structure of the multi-frame depth network is not changed. 

\setlength{\tabcolsep}{0.9mm}
\begin{table}[t]
	\centering
 %\setlength{\abovecaptionskip}{0cm} 
        %\setlength{\belowcaptionskip}{0.25}
    \renewcommand\arraystretch{1.1}
	\footnotesize
	\begin{center}
		\resizebox{0.4\textwidth}{!}
		{
			\begin{tabular}{|>{\columncolor{white}}c|c|cc|cc|}
                \hline
				%\toprule
				%Test & MS & \cellcolor{myorange}AbsRel & \cellcolor{myorange}SqRel & \cellcolor{myorange}RMSE & \cellcolor{myorange}RMSE log & \cellcolor{myblue}$\delta < 1.25$ & \cellcolor{myblue}$\delta < 1.25^2$ &  \cellcolor{myblue}$\delta < 1.25^2$\\
                Test & Multi-scale & \cellcolor{myorange}AbsRel &  \cellcolor{myorange}RMSE log & \cellcolor{myblue}$\delta < 1.25$ & \cellcolor{myblue}$\delta < 1.25^2$ \\
				\cline{3-6}
				%Frames &  Fusion & \multicolumn{4}{c|}{\cellcolor{myorange}$Lower\ is\ better$} & \multicolumn{3}{c|}{\cellcolor{myblue}$Higher\ is\ better$}\\
                Frames &  Fusion & \multicolumn{2}{c|}{\cellcolor{myorange}$Lower\ is\ better$} & \multicolumn{2}{c|}{\cellcolor{myblue}$Higher\ is\ better$}\\
                \hline
               \hline
                
                & 
                %&   \bf0.098  &   0.970  &   5.631  &   0.149  &   0.905  &   \bf0.977  &   \bf0.992  \\
                &   \bf0.098  &   0.149  &   0.905  &   \bf0.977   \\

                %\cline{2-6}
                \rowcolor{gray!20}
                \multirow{-2}{*}{\cellcolor{white}$I_t,I_{t-1}$}&  $\surd$     
                %&   \bf0.098  &  \bf0.946  &   \bf5.553  &   \bf0.148  &   \bf0.908  &   \bf0.977  &   \bf0.992  \\
                &   \bf0.098   &   \bf0.148  &   \bf0.908  &   \bf0.977   \\
                
				\hline
                \hline
                
                & 
                %&   0.101  &   0.998  &   5.670  &   0.152  &   0.903  &   0.976  &   0.992  \\
                &   0.101   &   0.152  &   0.903  &   0.976   \\

                \cline{2-6}
                \rowcolor{gray!20}
                \multirow{-2}{*}{\cellcolor{white}$I_t,I_{t-2}$}&  $\surd$     
                %&   \bf0.099  &   \bf0.960  &   \bf5.566  &   \bf0.149  &   \bf0.906  &   \bf0.977  &   \bf0.992  \\
                &   \bf0.099   &   \bf0.149  &   \bf0.906  &   \bf0.977  \\
                
				\hline
                \hline
                
                & 
                %&   0.104  &   1.031  &   5.755  &   0.155  &   0.898  &   0.975  &   0.991  \\
                &   0.104   &   0.155  &   0.898  &   0.975   \\

                \cline{2-6}
                \rowcolor{gray!20}
                \multirow{-2}{*}{\cellcolor{white}$I_t,I_{t-3}$}& $\surd$     
                %&   \bf0.101  &   \bf0.983  &   \bf5.625  &   \bf0.152  &   \bf0.903  &   \bf0.976  &   \bf0.992  \\
                &   \bf0.101   &   \bf0.152  &   \bf0.903  &   \bf0.976   \\

				%\bottomrule
                \hline
			\end{tabular}
	   }
	\end{center}
	\vspace{-8pt}
	\caption{The depth evaluation results on Cityscapes dataset with different test frames.}
	\vspace{-8pt}			
	\label{table:speed}
\end{table}



\section{Conclusion}

In this paper, we propose a novel self-supervised multi-frame depth learning framework. The depth inconsistency mask which provides information about dynamic objects improves the performance of the depth network in dynamic scenes. The multi-frame depth network utilizes multi-scale feature fusion for context feature extraction and feature matching. The robust knowledge distillation improves the multi-frame depth with no increase in inference costs. With these contributions, our method achieves SOTA performance on the KITTI and Cityscapes datasets.

% \section*{Acknowledgments}
%  This work was supported by the National Key R\&D Program of China under Grant 2021ZD0112600, and the National Natural Science Foundation of China under Grants 62173283, and  61977046. 


\bibliographystyle{named}
\bibliography{ijcai23}
\newpage
\appendix
\section{Evaluation Metrics}
Following Eigen et al.~\cite{eigen2015predicting}, we use several error metrics and accuracy metrics to show the performance of the depth:
\begin{itemize}
    \item Relative Absolute Error (\textbf{AbsRel}): 
    
    $\frac{1}{N}\sum_{i,j}|\hat{D}^{i,j}-D^{i,j}_{gt}|/D^{i,j}_{gt}$;
    \item Relative Squared Error (\textbf{SqRel}):

    $\frac{1}{N}\sum_{i,j}(\hat{D}^{i,j}-D^{i,j}_{gt})^2/D^{i,j}_{gt}$;
    \item Root Mean Squared Error (\textbf{RMSE}):

    $\frac{1}{N}\sum_{i,j}\sqrt{(\hat{D}^{i,j}-D^{i,j}_{gt})^2}$;
    \item Root Mean Squared Logarithmic Error (\textbf{RMSE log}):

    $\frac{1}{N}\sum_{i,j}\sqrt{(log\hat{D}^{i,j}-logD^{i,j}_{gt})^2}$;

    \item threshold accuracy ($\delta < 1.25^k$): percentage of $\hat{D}^{i,j}$ s.t. $max(\frac{\hat{D}^{i,j}}{D^{i,j}_{gt}},\frac{D^{i,j}_{gt}}{\hat{D}^{i,j}}) < 1.25^k$.
\end{itemize}

\section{Quantitative Comparison}

As shown in Table~\ref{table:results_all}, we show more results for comparison.

\setlength{\tabcolsep}{0.9mm}
\begin{table*}[!t]
	%\centering
    %\vspace{-10pt}
    \renewcommand\arraystretch{1.1}
	%\footnotesize
        %\small
        \normalsize


	\begin{center}
		\resizebox{1.0\textwidth}{!}
		{
			\begin{tabular}{|>{\columncolor{white}}c|l|c|c|c|cccc|ccc|}
                %\\
                \hline
				%\toprule
				& \multirow{2}{*}{\bf Method} & \multirow{2}{*}{Test frames} & \multirow{2}{*}{Semantic} & \multirow{2}{*}{$W\times H$}   & \cellcolor{myorange}AbsRel & \cellcolor{myorange}SqRel & \cellcolor{myorange}RMSE & \cellcolor{myorange}RMSE log & \cellcolor{myblue}$\delta < 1.25$ & \cellcolor{myblue}$\delta < 1.25^2$ &  \cellcolor{myblue}$\delta < 1.25^3$\\
				%\cline{6-12}
				& & & & & \multicolumn{4}{c|}{\cellcolor{myorange}$Lower\ is\ better$} & \multicolumn{3}{c|}{\cellcolor{myblue}$Higher\ is\ better$}\\
				\hline
                %\\
                \hline
                
                & Monodepth2~\cite{godard2019digging} & 1 & & $640 \times 192$ & 0.115  &   0.903   &  4.863  &   0.193   &  0.877  &   0.959  &   0.981\\
                & Lee et al.~\cite{lee2021attentive} & 1  & $\surd$ & $832 \times 256$ & 0.114   &   0.876   &   4.715   &   0.191   &   0.872   &   0.955   &   0.981\\
                & DiPE~\cite{jiang2020dipe} & 1 & & $640 \times 192$   &   0.112   &   0.875   &   4.795   &   0.190   &   0.880   &   0.960   &   0.981\\
                & InstaDM~\cite{lee2021learning}  & 1  & $\surd$ & $832 \times 256$ & 0.112   &   0.777   &   4.772   &   0.191   &   0.872   &   0.959   &   0.982\\
                &Patil et al.~\cite{patil2020don} & N & & $640 \times 192$ &  0.111   &   0.821   &   4.650   &   0.187   &   0.883   &   0.961   &   0.982\\
                &Packnet-SFM~\cite{guizilini20203d} & 1 & & $640 \times 192$ & 0.111   &   0.785   &   4.601   &   0.189   &   0.878   &   0.960   &   0.982\\
                &Wang et al.~\cite{wang20223d}& 1  &   & $832 \times 256$ &0.109   &   0.790   &   4.656   &   0.185   &   0.882   &   0.962   &   0.983\\
                &RM-depth~\cite{hui2022rm}& 1 & & $640 \times 192$ & 0.108   &   0.710   &   4.513   &   0.183   &   0.884   &   0.964   &   0.983\\
                &Johnston et al.~\cite{johnston2020self} & 1 & & $640 \times 192$ & 0.106   &   0.861   &   4.699   &   0.185   &   0.889   &   0.962   &   0.982\\
                &Wang et al.~\cite{wang2020self}& 2 (-1, 0) &          & $640 \times 192$ &0.106   &   0.799   &   4.662   &   0.187   &   0.889   &   0.961   &   0.982\\
                &TC-depth~\cite{ruhkamp2021attention}& 3 (-1, 0, +1) &          & $640 \times 192$ &0.103   &    0.746   &    4.483   &    0.180   &    0.894   &    0.965   &    0.983\\
                &DIFFNet~\cite{zhou2021self} & 1  &    & $640 \times 192$ & 0.102   &    0.764   &    4.483   &    0.180   &    0.896   &    0.965   &    0.983\\
                &Guizilini et al.~\cite{guizilini2020semantically} & 1  & $\surd$ & $640 \times 192$ & 0.102   &   0.698   &   4.381   &   0.178   &   0.896   &   0.964   &   \underline{0.984}\\
				& ManyDepth~\cite{watson2021temporal}    & 2 (-1, 0) &          & $640 \times 192$ & 0.098     &   0.770    &    4.459    &    0.176    &    0.900    &    0.965    &    0.983\\
				& Dynamicdepth~\cite{feng2022disentangling} & 2 (-1, 0) & $\surd$  & $640 \times 192$ & \underline{0.096}    &    0.720    &    4.458    &    0.175    &    0.897    &    0.964    &    \underline{0.984}\\
                &RA-depth~\cite{he2022ra} & 1  &   & $640 \times 192$ & \underline{0.096}    &    \bf0.632    &    4.216    &    \underline{0.171}    &    0.903    &    \bf0.968    &    \bf0.985\\
				& DepthFormer~\cite{guizilini2022multi}  & 2 (-1, 0) &          & $640 \times 192$ & \bf0.090    &     0.661    &     \bf4.149    &     0.175    &     \underline{0.905}    &     \underline{0.967}    &     \underline{0.984}\\
                %\rowcolor{gray!30}
                \cline{2-12}%\rowcolor{gray!30}\cellcolor{mygrey}
                \rowcolor{gray!30}
                  \multirow{-11}{*}{\cellcolor{white}\rotatebox{90}{KITTI}} &  \bf Ours     & 2 (-1, 0) &          & $640 \times 192$ &   \bf0.090  &   \underline{0.653}  &   \underline{4.173}  &   \bf0.167  &   \bf0.911  &   \bf0.968  &   \bf0.985  \\
                \hline
                %\\
                \hline
                &Li et al.~\cite{li2021unsupervised} & 1  &   & $416 \times 128$ & 0.119   &    1.290   &    6.980   &    0.190   &    0.846   &    0.952   &    0.982\\
                & Lee et al.~\cite{lee2021attentive} & 1  & $\surd$ & $832 \times 256$ &0.116   &    1.213   &    6.695   &    0.186   &    0.852   &    0.951   &    0.982\\
                & InstaDM~\cite{lee2021learning}  & 1  & $\surd$ & $832 \times 256$ &0.111   &    1.158   &    6.437   &    0.182   &    0.868   &    0.961   &    0.983\\
                & ManyDepth~\cite{watson2021temporal}    & 2 (-1, 0) &          &    $416 \times 128$ &    0.114   &   1.193   &   6.223   &   0.170   &   0.875   &   0.967   &   0.989\\
                & DynamicDepth~\cite{feng2022disentangling} & 2 (-1, 0) &  $\surd$ &   $416 \times 128$  &   0.103   &    1.000    &   5.867    &   0.157   &    \underline{0.895}   &    0.974    &   0.991\\
                & RM-depth~\cite{hui2022rm}     &     1     &          &  $640 \times 192$   &   \underline{0.100}    &  \bf0.839    &   \underline{5.774}    &   \underline{0.154}    &   \underline{0.895}    &   \underline{0.976}    &   \bf0.993 \\
                \cline{2-12}
                \rowcolor{gray!30}
                 \multirow{-7}{*}{\cellcolor{white}\rotatebox{90}{Cityscapes}} & \bf Ours      & 2 (-1, 0) &          &  $416 \times 128$   &   \bf0.098  &  \underline {0.946}  &   \bf5.553  &   \bf0.148  &   \bf0.908  &   \bf0.977  &   \underline{0.992}  \\
				%\bottomrule
                \hline
			\end{tabular}
        }
	\end{center}
	\vspace{-8pt}
	\caption{The comparison of depth estimation on KITTI and Cityscapes datasets. The listed results of KITTI are all evaluated on origin depth from velodyne. Following ManyDepth, the top and the sides of the depth predictions are cropped out for Cityscapes, resulting in the $W\times H$ as $416 \times 128$ rather than $512 \times 192$.}
        \vspace{-8pt}
	%\vspace{-9pt}			
	\label{table:results_all}
\end{table*}

\section{Qualitative Examples}

As shown in Figure~\ref{fig:visual_all}, our depth estimation performs better on dynamic objects and small-scale objects. Our depth estimation has sharper edges compared to previous methods.

 \begin{figure*}[!t]
    \vspace{-4pt}	
    \centering
    \includegraphics[width=1.0\linewidth]{visual_appendix.pdf}
    \vspace{-6pt}	
    \begin{tabular}{p{4.15cm}<{\centering}p{4.15cm}<{\centering}p{4.15cm}<{\centering}p{4.15cm}<{\centering}}
    %\specialrule{0em}{2pt}{2pt}
    %\renewcommand\arraystretch{0.1}
    \footnotesize
    RGB image &\footnotesize  Monodepth2 &\footnotesize ManyDepth  &\footnotesize Ours
    %\hline
    \end{tabular}
    \vspace{-2pt}
    \captionof{figure}{Qualitative depth estimation comparison on KITTI. Our method performs better on dynamic objects and small-scale objects.}
    \vspace{-4pt}
    \label{fig:visual_all}
    
%    \centering
%    \includegraphics[width=1.0\linewidth]{visual.pdf}
%    \vspace{-6pt}	
%    \begin{tabular}{p{4.15cm}<{\centering}p{4.15cm}<{\centering}p{4.15cm}<{\centering}p{4.15cm}<{\centering}}
%    RGB image &  over-fitting mask & without over-fitting mask  & depth
%    \end{tabular}
%    \vspace{-4pt}
%    \captionof{figure}{Examples of over-fitting masks and depth predictions on KITTI.}
%    \label{fig:visual2}
\end{figure*}
%% The file named.bst is a bibliography style file for BibTeX 0.99c
%\newpage


\end{document}

