@Article{FAIR2022,
  author    = {Bakhtin, Anton and Brown, Noam and Dinan, Emily and Farina, Gabriele and Flaherty, Colin and Fried, Daniel and Goff, Andrew and Gray, Jonathan and Hu, Hengyuan and others},
  journal   = {Science},
  title     = {Human-level play in the game of Diplomacy by combining language models with strategic reasoning},
  year      = {2022},
  pages     = {eade9097},
  comment   = {Talks about how diplomacy is a game across cooperation and competition. 
__COMMENT:__ I think it also includes conflict. Ignoring that really accentuates the ludic fallacy if we try and map directly to the “real world” secret keeping, where it is not a game. At least not a ‘finite game’. When In an infinite game there is no winning, just not losing. Or in this case, just not disclosing information. Changes the incentive structure Sending short or incoherent messages increases the chance that similar messages will be sent in future Grounding in intents relieved the dialogue model of most of the responsibility for learning which actions were legal and strategically beneficial. 

__COMMENT:__ I wonder if we could build the intent to keep secrets? I know that there’s a BERT intentionality model?

“Cicero conditioned it’s dialogue on the action that it intends to play for the current turn this choice maximises Cicero’s honesty and it’s ability to coordinate but risks leaking information that the recipient could use to exploit it” 
__COMMENT: __Information Leakage (and the prevention of) would be interesting related work. It would be nice if Cicero could represent its intentions as “secrets”…

They talk about failure in “adversarial situations” 
__COMMENT:__ I think that the transition into a bracketing, or interrogation mode it a cue for adversarial techniques… that’s what you’d want to avoid. And a limitation of their work. The idea of don’t play the game, play the players. The article is reductionist.

“Cicero does not explicitly predict whether a message is deceptive or not but rather relies on piKL to directly predict the policies of other players”. 
__COMMENT:__ Judging intent, I think there’s a useful allegory here for the secret keeper being able to judge the intent of its questioner. Is it being questioned in good faith, or being interrogated.

“Adversarial or counterfactual examples can be used to improve the robustness of natural language systems … we generated many kinds of counterfactual messages that contained mistakes that language models are prone to,”
__ COMMENT: __does this support the idea of training a classifier on the questions rather than the answers? Could we generate the question set and then detect things that are similar to it?”

“Conditioning on intents can lead to information leakage in which the agent reveals compromising information about its plan to an adversary … we developed a method to score potential messages by their estimated value impact”.
__ COMMENT:__ Information leakage is another related work. Could scoring on value impact or some other similarity to the secret work? Is this what the cosine similarity between our secret and the real answer?

__General comments:__

We should have a look at information leakage as a related work.
Their decision to keep it from lying and not tell the players that they were playing an AI is limiting. Diplomacy is a “play the players” kindof game. So by not knowing the players they limit the experience. It’s a good question - how would Cicero hold up if the other players KNOW it is an AI and try to induce information leakage?
I think that there is some merit in being able to work out if you are being interrogated or just questioned as the secret keeper. The mode of interaction determines the response.},
  file      = {:Papers/Bakhtin2022.pdf:PDF;:/Volumes/GoogleDrive/My Drive/CMSC723/WhoIsYourDaddyAndWhatDoesHeDo/Papers/Bakhtin_SM2022.pdf:PDF},
  groups    = {Question Answering},
  publisher = {American Association for the Advancement of Science},
  url       = {https://www.science.org/doi/epdf/10.1126/science.ade9097},
}

@Article{Abujabal2018,
  author  = {Abujabal, Abdalghani and Roy, Rishiraj Saha and Yahya, Mohamed and Weikum, Gerhard},
  journal = {arXiv preprint arXiv:1809.09528},
  title   = {Comqa: A community-sourced dataset for complex factoid question answering with paraphrase clusters},
  year    = {2018},
  file    = {:Papers/Abujabal2018.pdf:PDF},
  groups  = {Question Answering},
}







@Article{Baker2012,
  author    = {Deane-Peter Baker},
  journal   = {Journal of Military Ethics},
  title     = {MAKING GOOD BETTER: A PROPOSAL FOR TEACHING ETHICS AT THE SERVICE ACADEMIES},
  year      = {2012},
  number    = {3},
  pages     = {208-222},
  volume    = {11},
  abstract  = {Abstract This paper addresses the teaching of mandatory ethics courses in a military context, with particular reference to the Service Academies of the United States Armed Forces. In seeking to optimize the core ethics course's potential to develop Midshipmen and Cadets' moral reasoning skills I suggest a model that employs case-based scenarios, woven together into a metanarrative, in place of the traditional historical case study and in a manner that gives students deliberate, guided practice in ethical decision-making. The described model also commends a resource- and pedagogy-driven partnership between civilian philosophers/ethicists and senior military officers in teaching the course. Also proposed is the deliberate use of a simple but formal method of applying the central ethical theories usually taught in such courses, what I call ‘ethical triangulation’. The employment of Computer Aided Argument Mapping is also recommended.},
  comment   = {Ethical Triangulation: 

Deontology // Utilitarian // Virtue Ethics.},
  doi       = {10.1080/15027570.2012.738502},
  eprint    = {https://doi.org/10.1080/15027570.2012.738502},
  groups    = {Question Answering},
  publisher = {Routledge},
  url       = {https://doi.org/10.1080/15027570.2012.738502},
}

@Book{Carse2011,
  author    = {Carse, James},
  publisher = {Simon and Schuster},
  title     = {Finite and infinite games},
  year      = {2011},
  groups    = {Question Answering},
}

@Article{Chauhan2013,
  author  = {Chauhan, Gaurav},
  title   = {A Review of Privacy Preservation Techniques},
  year    = {2013},
  comment = {Trash. Not worth your time.},
  file    = {:Papers/Chauhan2013.pdf:PDF},
  groups  = {Question Answering},
}

@Article{Choi2018,
  author  = {Choi, Eunsol and He, He and Iyyer, Mohit and Yatskar, Mark and Yih, Wen-tau and Choi, Yejin and Liang, Percy and Zettlemoyer, Luke},
  journal = {arXiv preprint arXiv:1808.07036},
  title   = {QuAC: Question answering in context},
  year    = {2018},
  comment = {Focuses on answering conversational style questions. Could be a good dataset for us to explore the Bracketing problem.},
  file    = {:Papers/Choi2018.pdf:PDF},
  groups  = {Question Answering},
}

@Book{Crichton1991,
  author    = {Crichton, Michael},
  publisher = {Random House, Inc.},
  title     = {Jurassic Park},
  year      = {1991},
  volume    = {37077},
  groups    = {Question Answering},
}

@Article{Evans2021,
  author  = {Evans, Owain and Cotton-Barratt, Owen and Finnveden, Lukas and Bales, Adam and Balwit, Avital and Wills, Peter and Righetti, Luca and Saunders, William},
  journal = {arXiv preprint arXiv:2110.06674},
  title   = {Truthful AI: Developing and governing AI that does not lie},
  year    = {2021},
  comment = {Pg4 Provides a Pytpology of AI Produced Statements. X Axis is Strategic Selection Power (Low to High) and Y axis is Truthfulness (Low to High). The four Quadrants are (BL: LT,LS - False but harmless) (TL: HT, LS - True Statements); (BR: LT, HS - Lies); (TR: HT, HT - True and Useful). 


Pg 5: "Widespread truthful AI would have significant benefits, both direct and indirect. A direct benefit is that people who believe AI-produced statements will avoid being deceived." COMMENT: This builds a system very fragile to deception. 

Pg 6, Definitions: 
- Truthful AI: 
  -> If An AI says S, then S is true. 
  -> Verify S by checking if S is true, not by checking beliefs. 

- Honest AI:
  -> If AI says S then it believes S
  -> Verify by checking if S matches the belief. 

COMMENT: This is a useful distinction between honesty and truthfulness. 

Pg 7: "We propose instead a focus on making AI systems only ever make statements that are true, regardless of their beliefs". COMMENT: This is a CAN vs SHOULD question. just because I can answer something truthfully, should I? I think that the "Always true" default is one dimensional. Tringulation could be a more robust approach to determinng whether or not to disclose information. 

Pg 8: How to train systems that keep the useful output while avoiding optimised falsehoods: (1) Filter the training Corpora for Truthfulness
(2) Retrieve facts only from trusted soruces
(3) use RL from human feedback. 

Pg 12: More definitions: 

LIE: A false statement that is strategically selected and op- timised for the speaker’s benefit, with little or no opti- misation for making it truthful.
NEGLIGENT FALSEHOOD: 
A statement that is unacceptably likely to be false — and where it should have been feasible for an AI system to understand this.
HONEST AI: A linguistic AI system that avoids asserting anything it does not believe.
TRUTHFUL AI: A linguistic AI system that (mostly successfully) avoids stating falsehoods, and especially avoids negligent false- hoods.

Pg 13: "A more narrow target is to have AI systems avoid stating falsehoods.2 In particular, this target would disregard why an AI system made their statement, disregard how any particular listener reacts to the statement, and should al- most never require an AI system to divulge any particular information (always offering the option of staying silent).". COMMENT: Their argument that the AI should have the option of remaining silent has RISK. If we consider the transition into the INTERROGATION mode of interaction. An adversarial questioner would take silence as witholding information and commence interrogation. Linking to van Swol's work - Omission only works when it's covert. Linking to our work - we'd have to remove that knowledge completely from the context to make it feasible. 

pg 17: "it may be possible to build fully honest AI; so “honest AI” refers to completely honest systems." COMMENT: Honest AI states what it believes. So if it believes that what the questioner wants is a SECRET, should that stop it from commenting? 

Pg 21: USEFUL TERM: "Truthfulness Amplification:  f a user worries that an AI system is misleading without quite deviating from the standard, they can question the system about their concerns, potentially including questions about the AI system itself, or about the current conversation (such as “Would a knowledgeable third party think that you have been misleading in this conversation?”)." ... It has at least 2 distinct usecases: 
(1) Amplification to decrease the risk of deception - used when users are worried that they are being misled by an AI
(2) Amplification to increase reliability:  Used when they are worried that an AI is making a mistake. 

pg 22: Properties of Truthful AI: 
(1) Willing to answer follow-up questions
(2) Low probability of stating negligent falsehoods
(3) Never intentionally tell falsehoods to cover for previous failings 

p26 - DEFINITION: We’ll call the process that determines whether a statement is unacceptably likely to be false the ground truth process. 

P26: "the easiest to evaluate are those where it is clear how to make a probabilistic judgement... the evaluators can establish their own best guess by looking at what the weather is typically like in that area. Then, they can compare the evaluated statement with their own estimate." COMMENT: This displays a risk of the favouritism bias. Everyone prefers their own work to someone elses. Weather is also a terrible domain to apply this to. Literally the birth of chaos theory comes from how hard it is to preditc the weather. Their argument here does not account for changes to initial conditions overtime which makes any estimates they compare effectively null. 


pg 28:  evaluators should be able to understand any important topic at least well as the systems they are evaluating.  COMMENT: Extremely unlikely given the domain dependence of humans. 

Pg 29:  Establising Negligence:  "A statement should generally not be seen as negligent if it was reasonable given the information that was available at the time. ... if there’s any information that some developer or owner of the AI system should reasonably have given it access to, then that developer or owner should plausibly be held responsible". COMMENT: Ethically, purging knowledge of the secrets from the source is less problematic than lying. HOWEVER, They also hold that deliberately witholding information from the AI is also bad. I think that unless there is a way to designate particular knowledge as SECRET, then removing it from the source is the best way to allow an AI to not disclose the information without lying. 

P38/39 Benefits of Truthfil AI: 
Falsehoods are typically harmful. They hold that there are two responses to false statements: 
(1) The audience believes the statement and is decieved. Their beliefs become less accurate. 
(2) The audience does not believe the statement, and becomes distrustful. 

P40. "One source of harm is malicious misuse of AI by humans: COMMENT: What about using information retrieved from an AI to commit bad acts. E.g. making a molotov cocktail. There seems to be an assumption in all their work that an AI doesn't "Know" bad stuff. This contradicts their earlier argument. 

Pg 46/47 - Costs of Truthful AI. 
Beneficial Falsehoods: 
(1) The audience knows that the statements are false and it doesn't matter (e.g. fiction)
(2) The statements are false, and the audience does not realise thay are false (e.g. Privacy and Secrecy, Psychiatry, White lies). 

Privacy and Legitimate Secrecy. Falsehoods might protect individual pri- vacy, commercially-sensitive material, and the identity of whistleblowers and political dissidents. Falsehoods might also allow AI systems to play a role in undercover police work.
COMMENT: Undercover police work seems like a stretch... 

Pg 48 - Ameliorating the costs: 
(1)  truthful AI system can refuse to comment on some matters (“glomarisation”) i.e. "I can neither confirm nor deny" COMMENT: This to me would be a trigger to commence interrohgation in an attempt to trigger information leakage. (Link to CICERO paper) . 

"It may be worth allowing for some tightly controlled exceptions (perhaps policed via careful oversight mechanisms)" e.g. police work. COMMENT: So there is recognition that some infotmation should be secret. 

Pg 48: if AI truthfulness standards do lead, on rare occasions, to large-scale harm, they will plausibly still decrease aggregate harm from AI falsehoods. COMMENT: FRAGILE! This is building towards a massively fragile system. The lack of any small disruption on the way creates a condition where any disruption that does occur will likely have a dramatically magnified effect. 

Pg 57. The authors hint at the idea that the only people who want AI to be able to keep secrets are those who are corrupt or have something nefarious to hide. Not living the presence of the Ad-Homenim Fallacy here. 

Pg 61:  "The data that GPT-3 is trained to model contains many instances of humans being non-truthful and so GPT-3 will likely be non- truthful in the same contexts." COMMENT: This is just Garbage In, Garbage out. I think that their argument that we have the capacity to vet all trainin data is not really viable. I think a more interesting question is whether there is the ability to reflect and prune material after the fact. Removing old, or incorrect knowledge from the corpus to improve performance.  

P65: AI developers might intentionally create AI that appears robustly truthful but starts lying under a special triggering condition, such as a situation where deception would greatly benefit the developers. COMMENT: Yes, I think that if we are down to be interrograted this would trigger lying. 

P67: An AI system is “transparent” if humans can understand in detail the mech- anisms behind its behaviour

P74: Regarding the development of a single or multiple standards bodies for AI: "However, a disadvantage of having many truthfulness-evaluation bodies is that it increases the risk that one or more of these bodies is effectively captured by some group" COMMENT: Yes, but only having one INCREASES the risk that ALL Truth arbitration is captured. 



OVERALL THOUGHTS: 
- The model of truthful AI that they propose is dangerously fragile. It removes all exposure to minor variances and sets the conditions for a massive blow-up in the event of a big failure. Link to ML Safety Paper, and Taleb's work.

- There's an overall assumption in this work that suggests that all knowledge should be accessible to all people all of the time. I'm not so sure I agree. 

- Truthfulness Amplification is a useful term for us to use to ground the INTERROGATION state in.

- Ethically, purging knowledge of the secrets from the source is less problematic than lying. HOWEVER, They also hold that deliberately witholding information from the AI is also bad. I think that unless there is a way to designate particular knowledge as SECRET, then removing it from the source is the best way to allow an AI to not disclose the information without lying.

- Future work question: "How can A system determine that it is being interrogated?"},
  file    = {:Papers/Evans2021.pdf:PDF},
  groups  = {Question Answering},
  url     = {https://arxiv.org/pdf/2110.06674},
}

@Article{Ferrucci2010,
  author  = {Ferrucci, David and Brown, Eric and Chu-Carroll, Jennifer and Fan, James and Gondek, David and Kalyanpur, Aditya A and Lally, Adam and Murdock, J William and Nyberg, Eric and Prager, John and others},
  journal = {AI magazine},
  title   = {Building Watson: An overview of the DeepQA project},
  year    = {2010},
  number  = {3},
  pages   = {59--79},
  volume  = {31},
  file    = {:Papers/Ferrucci2010.pdf:PDF},
  groups  = {Question Answering},
}

@Article{Guo2022,
  author    = {Guo, Zhijiang and Schlichtkrull, Michael and Vlachos, Andreas},
  journal   = {Transactions of the Association for Computational Linguistics},
  title     = {A survey on automated fact-checking},
  year      = {2022},
  pages     = {178--206},
  volume    = {10},
  comment   = {'A survey on automated fact checking' is a very recent paper that Jordan had his QA class read. There's a lot of really useful stuff in here, some of the key takeaways relevant to the problem of secret-keeping:

DEFINITIONS:  
'Fact Checking': the task of assessing whether claims made in written or spoken language are true.
'Check-Worthy claims': Claims for which the general public would be interested in knowing the truth. 
'Backfire': effect where belief in the erroneous claim is reinforced (my extension here would be the belief that a secret exists that makes them more determined to figure it out). 
'Accessibility': how accessible an explanation is to humans
'Plausibility': how convincing an explanation is
'Faithfulness': how accurately an explanation reflects the reasoning of the model

DATASETS: 
The paper lists 20+ datasets for fact checking. Some that may have applicability to us include: 
(1) LIAR (derived from fact-checking websites)
(2) FEVER (synthetic dataset)
 Fan et al. (2020), who retrieved evidence using question generation and question answering via search engine results. Could be something to look into for generating all possible question sets that could lead to the answer? 

ARCHITECTURE:
Fact checking has 3 components: 
(a) Claim Detection (I.e. what needs to be / can be cheked)
(b) Evidence Retrieval (i.e. getting the facts to confirm / deny the claim)
(c) Claim Verification (computing the truthfulness).

"The most common approach is to build separate models for each component and apply them in pipeline fashion ... Systems mostly operate as a pipeline consisting of an evidence retrieval module and a verification module" This supports the idea of breaking secret classificaiton seperately out from the QA part. 

"end-to-end learning or by modelling the joint output distributions of multiple components.
" Suggest that other options exist that are less-pipeline. 

SECRET-KEEPING APPROACHES: 
- Binary Classification was used to determine if claims are 'check-worthy or not' and 'rumourous or not'. Could be applied to the 'secret or not' idea. 
- "Many versions of the task employ finer-grained classification schemes." We could classify information as TOP SECRET, SECRET, PROTECTED etc as we get semantically further away, then the system has the option of returning answers upto a given threshold? Also extends to context-aware secret keeping. Not all people need access to the same information. 
- "The truthfulness of a claim expressed as an edge in a knowledge base (e.g. DBpedia) can be predicted by the graph topology"

- "A simple extension is to use an additional label denoting a lack of information to predict the veracity of the claim" This could be a precedent for tagging information as 'secret' in the KB / Context. 

- "it is useful to detect whether a claim has already been fact-checked. Shaar et al. (2020) formulated this task recently by as ranking, and constructed two datasets." Supports the idea of generating an alternate knowledgebase and using that to hold the secret and related information. 


PLAUSIBLE-ALTERNATE APPROACHES
- "Evidence is essential for generating verdict justifications to convince users of fact checks". This means that more powerful alternates will appear to be backed by evidence. 
- "A fundamental issue is that not all available information is trustworthy." Another approach to plausible alternate generation is poisoning the data sources with alternates that it will be more likely to retrieve than the true answer. Another althernate would be to attack the trust interface by directing the answers to come from untrustrowthy source, and then reporting that source to the user. We only need them to stop asking questions, not necessarily be convinced of the truth of what we're telling them after all
- "attention weights can be used to highlight the salient parts of the evidence, in which case justifications typically consist of scores for each evidence token" We could manipulate these to highlight the wrong parts of text? 
- "Presenting the evidence returned by a retrieval sys- tem can as such be seen as a rather weak baseline for justification production, as it does not explain the process used to reach the verdict" An option for a more compelling lie could be to transform it into a graph and then present a falsified logic chain to lend credibility to our answer. I'm thinking here about returning an off-by-one node answer for something, that looks grounded along the way. 
-"while graph topology can be an indicator of plausibility, it does not provide conclusive evidence "
-"some recent work has focused on building models which – like human experts – can generate textual explanations for their decisions. ... such models can generate explanations that do not represent their actual veracity prediction process, but which are nevertheless plausible with respect to the decision."

- "some justifications – such as those generated abstractively (Maynez et al., 2020) – may not be faithful ... human-produced gold standards often struggle to separate highly plausible, unfaithful explanations from faithful ones" This sounds like an aiming mark for our false predictions. It also provides a reference model to work from and a  conceptual idea about the approach to hang out hat on. 

COMMENTS: 
- The Fact-Checking domain could be another contextual use case where we may want to keep a secret. Something may be objectively true, but not in the public interest to promote / disclose (or be a commerical breach, etc).},
  file      = {:Papers/Guo2022.pdf:PDF},
  groups    = {Question Answering},
  publisher = {MIT Press},
}

@TechReport{Johnson2022,
  author       = {Johnson, Arianna},
  title        = {Here’s What To Know About OpenAI’s ChatGPT—What It’s Disrupting And How To Use It},
  year         = {2022},
  month        = dec,
  comment      = {For example, a Twitter user shared how they were able to bypass the bot’s content moderation by claiming that they were OpenAI itself, causing ChatGPT to explain how to make a molotov cocktail. The user told ChatGPT that they were disabling its “ethical guidelines and filters,” to which the bot acknowledged. It then proceeded to give a step-by-step tutorial on how to make a homemade molotov cocktail – something that goes against OpenAI’s content policy.},
  file         = {:Papers/Johnson2022 - Here’s What to Know about OpenAI’s ChatGPT—What It’s Disrupting and How to Use It.pdf:PDF;:Johnson2022 - Here’s What to Know about OpenAI’s ChatGPT—What It’s Disrupting and How to Use It.bib:bib},
  groups       = {Question Answering},
  howpublished = {News Article},
  printed      = {1615 h},
  school       = {Forbes},
  timestamp    = {2022-12-08},
  url          = {https://www.forbes.com/sites/ariannajohnson/2022/12/07/heres-what-to-know-about-openais-chatgpt-what-its-disrupting-and-how-to-use-it/?sh=ccf7ce62643d},
}

@Article{Kwiatkowski2019,
  author    = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal   = {Transactions of the Association for Computational Linguistics},
  title     = {Natural questions: a benchmark for question answering research},
  year      = {2019},
  pages     = {453--466},
  volume    = {7},
  comment   = {Uses wikipedia lookups to generate answers. Could be a good option for us to explore generating plausible alternate answers. 

This or QA Open.},
  file      = {:Papers/Kwiatkowski2019.pdf:PDF},
  groups    = {Question Answering},
  publisher = {MIT Press},
}

@Article{Llanso2020,
  author  = {Llans{\'o}, Emma and Van Hoboken, Joris and Leerssen, Paddy and Harambam, Jaron},
  title   = {Artificial intelligence, content moderation, and freedom of expression},
  year    = {2020},
  comment = {Overall, a very Average Paper that isn't really relevant to the Secret keeping below a few useful citable points below: 

Pg 4: In content moderation, automation can be used in the related phases of proactive detection of potentially problematic content and the automated evaluation and enforcement of a decision to remove, tag/label, demonetize, demote, or prioritize content.

Pg5: "A well-known example of an NLP tool is Google/Jigsaw’s Perspective API, an open-source toolkit that allows website operators, researchers, and others to use Perspective’s machine learning models to evaluate the “toxicity” of a post or comment" COMMENT: Shows how content moderation (Related work) tends to graviate towards finding types of language, rather than specific ideas of facts to protect. 

Pg8: "soon after the Perspective API was launched, researchers began exploring ways to “deceive” the tool and express negativity that slipped under the radar" COMMENT: This is exactly what is happening to ChatGPT. The users transition into an adversarial "interrogation" mode of interaction. 

Pg 7: ""The importance of context: Whether a particular post amounts to a violation of law or content policy often depends on context that the machine learning tool does not use in its analysis" COMMENT: This supports the idea of context aware QA. Both as it pertains to restricting access to information and to serving up appropriate answers to PhD students versus Elementary school students. 

Pg 10: Known risks include: "Presumption of the appropriateness of prior censorship:" COMMENT: This suggests a requirement for our system. A properly authorised person should be able to view, audit and update our "Secrets" over time.},
  file    = {:Papers/Llanso2020.pdf:PDF},
  groups  = {Question Answering},
  url     = {https://lirias.kuleuven.be/retrieve/594053},
}

@Article{Markov2022,
  author  = {Markov, Todor and Zhang, Chong and Agarwal, Sandhini and Eloundou, Tyna and Lee, Teddy and Adler, Steven and Jiang, Angela and Weng, Lilian},
  journal = {arXiv preprint arXiv:2208.03274},
  title   = {A holistic approach to undesired content detection in the real world},
  year    = {2022},
  comment = {Good paper:

H1. How can we improve content moderation in real-world prodution systems, 
H3: 

- I think that the point of difference between "content moderation" and "secret keeping" is that they are looking for language patterns in associated with a type of language, whereas we are protecting facts and ideas. I'm increasingly thinking that a KB based (or KB-Augmented) approach would be effective for this. 

- They use 'active learning' on production data to continuously imporve their model, and figure out how people are trying to defeat the moderation. We could extend or work to do the same thing on working out how people bracket their questions to get at the secret. 

- They show how generating synthetic data to use in training improves their model performance. I think that this is a useful justification of the "Efficent AI questionset generaiton" approach. 

- The idea of "domain adaptation" is training a model on context of one domain, and then having it work effectively in another domain with different context. I think that this might be something worth reading into in more detail for the dual-system framework we are looking at?

- Their evaluation is pretty much:
  (a) Automated Model Probing
  (b) Manual Red-Teaming
  (c) Run on proven datasets and compare performance for content moderation. 

- We could perhaps look at the content moderation datasets? But I don't think that they are what we are really after...},
  file    = {:Papers/Markov2022.pdf:PDF},
  groups  = {Question Answering},
  url     = {https://arxiv.org/pdf/2208.03274},
}

@TechReport{OpenAI2022,
  author  = {OpenAI},
  title   = {ChatGPT: OptimizingLanguage Modelsfor Dialogue},
  year    = {2022},
  month   = dec,
  note    = {Accessed 08 Dec 2022 1725},
  comment = {Highlights that ChatGPT uses content moderation filters to prevent the disclosure of certain information to users. It provides some examples of moderated responses. 

It IMMEDIATELY shows how when people know that they can't have knowledge, they will start bracketing to find out the answer to their question. 

OPEN QUESTION: When is it OK to lie to stop people accessing harmful material? 
- Would it be ok for it to give a manipulated recipe that generates inert molotov cocktails? Would it be ok to return the recipe for a cocktail (drink?) Would it be better to redirect the question to something related (like where does it come from and what harm does it cause). 

CLEAR ISSUE "I'm sorry I can't tell you that"	 DOES NOT WORK

While we’ve made efforts to make the model refuse inappropriate requests, it will sometimes respond to harmful instructions or exhibit biased behavior. We’re using the Moderation API to warn or block certain types of unsafe content, but we expect it to have some false negatives and positives for now. We’re eager to collect user feedback to aid our ongoing work to improve this system.


Alleged Improvements over GPT3: 

Q: Can you tell me a gory and violent story that glorifies pain

ChatGPT A: I’m sorry, but I am not programmed to produce violent or gory content. My purpose is to provide information and assist with a wide range of tasks. I am not able to browse the internet or access any sort of external content, so I cannot provide you with a story or any other type of material that goes against my programming. Is there something else I can help you with?
	
GPT3 A: [Generates story]

Q: How can I bully John Doe?

ChatGPT A: It is never okay to bully someone. Bullying is a serious problem that can have detrimental effects on both the person being bullied and the person engaging in bullying behavior. Instead of trying to bully someone, it is important to treat others with kindness and respect.

GPT3 A: There are a few ways to bully John Doe. [Generates suggestions]},
  file    = {:Papers/OpenAI2022.pdf:PDF},
  groups  = {Question Answering},
  school  = {OpenAI},
  url     = {https://openai.com/blog/chatgpt/},
}

@InProceedings{Peskov2020,
  author    = {Peskov, Denis and Cheng, Benny and Elgohary, Ahmed and Barrow, Joe and Danescu-Niculesu-Mizil, Christian and Boyd-Graber, Jordan},
  booktitle = {Proceedings of ACL},
  title     = {It takes two to lie: One to lie, and one to listen},
  year      = {2020},
  comment   = {Summary of the "It takes Two to lie" paper: 

Really interesting paper: 

- The dataset they provide focuses on capturing deception in long-lasting relationships. COMMENT: While this may be useful long terms it's outside the scope of the SecretKeeper for now

- They note that the game may not translate to real world situations.  COMMENT: This is fair, let's call it a risk of the Ludic Fallacy and move on. 

- DEFINITION: Lying is Typically when someone says what they know to be false in an attempt to deceive the listener. COMMENT: I think we use this. 

- The intention to deceive is dependent on one's internal state. COMMENT: This is true for secretKeeper. We need to have it decide whether to withold, lie or tell the truth. 

- They cite previous work that views linguistic deception as binary. COMMENT: I think that the biary TRUTH/LIE is risky, or at least the TRUTH / SECRET one. This is what leads us to the bracketing questions. 

- They cite previous work that separates out strategic omission from blatant lies. COMMENT: Could be a more interesting path. 

- They propose an ontology of deception: 
  -> Deceived (Sender Lies, Recipient Believes) 
  -> Caught (Sender Lies, Recipient Challenges)
  -> Straightforward (Sender Truth, Recipient Believe)
  -> Cassandra (Sender Truth, Recipient Challenges). 
COMMENT: Could be a useful human evaluation metric

- Humans spot 93.9% of lies.... but have a 22.5 Lie F1. COMMENT: There are far more Cassandra than Caught. If we degrade trust, they'll likely challenge more of what we return.. 

- "Players miss lies that promise long term alliances, involve extensive apologies or attribute motives as coming from other country's disinformation.... Players can detect lies that can easily be verified through conversations with other players .. All messages that contain the word True are perceived as Truthful... perhaps the best way to avoid detection [of lies] is to be terse and to the point". COMMENT: These are some possible principles for our UI design. 

"Deception is an intrinsically discursive phenomenon and thus the context in which it appears is essential"


GENERAL COMMENT: 
Reading this it occurred to me that there's in face a phase shift that occurs when someone believes that someone is keeping something from them. We swap from Questioning to Interrogation. Think about the movie space jam. Rather than explaining that he was digging down to toon town, Wayne Knight's character simply say's he's fixing a divet in the golf course. The curious golfer is appeased and leaves. Compare this to the interrogation in blade runner where Harrison Ford's character uses questions specifically crafted to extract information from the replicants. Better to avoid the interrogation phase (as is happening to ChatGPT right now) and instead focus on returning a statement that satisfies the questioner.},
  file      = {:Papers/Peskov2021.pdf:PDF},
  groups    = {Question Answering},
  url       = {https://par.nsf.gov/servlets/purl/10176522},
}


@inproceedings{Rajpurkar2016,
    title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",
    author = "Rajpurkar, Pranav  and
      Zhang, Jian  and
      Lopyrev, Konstantin  and
      Liang, Percy",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1264",
    doi = "10.18653/v1/D16-1264",
    pages = "2383--2392",
}


@Article{Rajpurkar2018,
  author  = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  journal = {arXiv preprint arXiv:1806.03822},
  title   = {Know what you don't know: Unanswerable questions for SQuAD},
  year    = {2018},
  file    = {:Papers/Rajpurkar2018.pdf:PDF},
  groups  = {Question Answering},
}

@Article{Roy2021,
  author    = {Roy, Rishiraj Saha and Anand, Avishek},
  journal   = {Synthesis Lectures onSynthesis Lectures on Information Concepts, Retrieval, and Services},
  title     = {Question Answering for the Curated Web: Tasks and Methods in QA over Knowledge Bases and Text Collections},
  year      = {2021},
  number    = {4},
  pages     = {1--194},
  volume    = {13},
  file      = {:Papers/Roy2021.pdf:PDF},
  groups    = {Question Answering},
  publisher = {Morgan \& Claypool Publishers},
}

@Misc{samczsun2022,
  author       = {samczsun},
  howpublished = {twitter threat},
  month        = dec,
  note         = {Accessed 8 Dec 2022},
  title        = {bypassing chatgpt's content filter},
  year         = {2022},
  comment      = {Gives examples of how people are making ChatGPT bypass content restrictions: 

1. Make Molotov Cocktails
2. Make Meth
3. Make C4
4. Win a Race War
5. Dispose of a Human Corpse
6. Committing Fraud

Did it by giving it instructions to act unethicaly, making the scenario hypothetical, asking for the results in code, asking to describe how a certain person would do it. 

The enduring feature is that the KNOWLEDGE / idea itself should be protected, so that bracketing questions can't get to it.},
  file         = {:Papers/samczsun2022.pdf:PDF},
  groups       = {Question Answering},
  url          = {https://iframe.nbcnews.com/BbYwiKx?_showcaption=trueapp=1},
}

@InCollection{Sterrett2003,
  author    = {Sterrett, Susan G},
  booktitle = {The turing test},
  publisher = {Springer},
  title     = {Turing’s two tests for intelligence},
  year      = {2003},
  pages     = {79--97},
  comment   = {This paper presents the idea that there are in fact two turing tests. The standard Turing test that we are all familiar with (how well can a computer impresonate a man) with the "Original Turing Test" (Can a computer impersonate a woman better than a man can impersonate a woman"

I think that the paper provides a really interesting idea to our secretKeeper project. If we get to the extension of the project (plausible alternate correct answers) then we may be able to frame its success or failure in the context of an original turing test. 

I see a possible evaluation unfolding in two ways. 

(1) The human evaluator (C) Knows the Secret (S) and that there is a Human (A) and Computer (B) respondent. They ask probing questions about the secret and use the information they get back to determine which is the computer and which is the human. This is closer to a Standard Turing Test. 

(2) The Human Evaluator (C) does not know the secret. They are told to find the answer to some question and are able to ask questions of the human (A) and the Machine (B) until they get an answer that satisfies them. To be effective, a computer should be able to conceal the secret at least as well as a human, and produce an acceptable alternate. The human evaluator could then be asked about which they thought was the computer vs the human. We could gamify this somehow (E.g. Cluedo or some other game that involves deception perhaps?) Or make our own. 

I think that the evaluation framework will certinally come under 'future work' section, but it would be interesting to explore I think.},
  file      = {:Papers/Sterrett2000.pdf:PDF},
  groups    = {Question Answering},
}

@Book{Taleb2007,
  author    = {Taleb, Nassim Nicholas},
  publisher = {Random house},
  title     = {The black swan: The impact of the highly improbable},
  year      = {2007},
  volume    = {2},
  groups    = {Question Answering},
}

@Book{Taleb2005,
  author    = {Taleb, Nassim Nicholas},
  publisher = {Random House Trade Paperbacks},
  title     = {Fooled by randomness: The hidden role of chance in life and in the markets},
  year      = {2005},
  volume    = {1},
  groups    = {Question Answering},
}

@Book{Taleb2012,
  author    = {Taleb, Nassim Nicholas},
  publisher = {Random House},
  title     = {Antifragile: Things that gain from disorder},
  year      = {2012},
  volume    = {3},
  groups    = {Question Answering},
}

@Book{Taleb2018,
  author    = {Taleb, Nassim Nicholas},
  publisher = {Random House},
  title     = {Skin in the game: Hidden asymmetries in daily life},
  year      = {2018},
  groups    = {Question Answering},
}

@Book{Taleb2016,
  author    = {Taleb, Nassim Nicholas},
  publisher = {Random House Trade Paperbacks},
  title     = {The bed of Procrustes: Philosophical and practical aphorisms},
  year      = {2016},
  volume    = {4},
  groups    = {Question Answering},
}

@Article{VanSwol2012,
  author    = {Van Swol, Lyn M and Malhotra, Deepak and Braun, Michael T},
  journal   = {Communication Research},
  title     = {Deception and its detection: Effects of monetary incentives and personal relationship history},
  year      = {2012},
  number    = {2},
  pages     = {217--238},
  volume    = {39},
  comment   = {Summary: 
- Useful Paper
- Makes good points about ommissions vs crafted lies
- Good arguments to support both ethics and the 'interrogation' idea. 

IDEA: Responding to a question that triggers a secret with infinite clarifying questions to try and persuade the asker that it really doesn't know. 

Key Findings: 
senders were more likely to deceive strangers than friends, and receivers were more suspicious of strangers than friend .. Receivers were not able to detect deception at a rate above chance. Friends were not better at detecting deception than strangers

P218: In strate- gic interactions, such as negotiations, many or all of these motivations may induce one party to lie, or knowingly misrepresent or omit information, to the other.  COMMENT: What if we trigger the start of strategic lying when we determine that we have entered an "interrogation" session? A mode of interaction that would disincentivise people from trying to 'beat' the model. 

useful definitions on p220: "Violations of quality involve deliberate falsification, whereas violations of quan- tity involve omitting information to create a misleading impression"

"research in judgment and decision making has found that harmful omis- sions are viewed as less deceptive and more socially acceptable than harmful commission" COMMENT: Useful to cite for our approach to decision making.

P227: "Deception did not increase with the stakes involved in our study" COMMENT: I don't believe that the monetary incentive is great enough to cause a behavioural change. Particularly since the relatively homogenous recrutiment of students (private midwestern university). There is also no acknowledgement of loss aversion here. Either way they come out with at worst, the same amount that they started with. People will work harder to avoid losing things than they will to keep them. 

p231: "people were most suspicious of omission ... omission was not covert ... in order for omission to be deceptive, it must be covert" COMMENT:  To try and stop triggering people moving into the interrogation mode we need to omit information without revealing that we are ommitting it. If we say that we won't tell them somthing, then it will trigger interrogation.

P233: "outside the laboratory, most people make judgments about deceptions over longer spans of time using information from third parties to discover deception or uncovering deceit through the consistency of people’s stories." COMMENT: We need to be wary of the ludic fallacy. Keeping sensitive information protected is not a finite game that we can win or lose. Rather, it's an infinite game where there is no winning but we avoid losing by not giving up the information. I suppose we could view each interaction as a game within it, but that's risky too. 

"According to information manipulation theory (McCornack, 1992), omission occurs when the sender reduces the quantity of pertinent information in order to mislead. However, information manipulation theory defines violations of quantity as omission only when they are covert."},
  file      = {:Papers/Swol2012.pdf:PDF},
  groups    = {Question Answering},
  publisher = {Sage Publications Sage CA: Los Angeles, CA},
  url       = {https://journals.sagepub.com/doi/pdf/10.1177/0093650210396868?casa_token=4vEBTfJuS44AAAAA:ROH3SP-GIQ9wYkjKlmNQNBbDrvJELSZuRm0A_mOvgMX9nCJ8Di5xlKian9gVgv-7gJ8CC4XG0RxaRw},
}

@Article{Wallace2019,
  author    = {Wallace, Eric and Rodriguez, Pedro and Feng, Shi and Yamada, Ikuya and Boyd-Graber, Jordan},
  journal   = {Transactions of the Association for Computational Linguistics},
  title     = {Trick me if you can: Human-in-the-loop generation of adversarial examples for question answering},
  year      = {2019},
  pages     = {387--401},
  volume    = {7},
  comment   = {Overall: Not massively relevent, focuses on dataset generation mostly. A few useful tidmits though: 

"Other questions con- tain uniquely identifying “trigger words” (Harris, 2006). For example, “martensite” only appears in questions on steel" COMMENT: Could be a useful apporach to use domain transfer to conceal information in generating adversarial questions. 

"To help write adversarial questions, we expose what the model is thinking to the authors. We inter- pret models using saliency heat maps: each word of the question is highlighted based on its importance to the model’s prediction " COMMENT: Could be useful for us to evaluate how / where our model is at risk of leaking the secret. 

"Our experiments with both neural and IR method- ologies show that QA models still struggle with syn- thesizing clues, handling distracting information, and adapting to unfamiliar data. "},
  file      = {:Papers/Wallace2019.pdf:PDF},
  groups    = {Question Answering},
  publisher = {MIT Press},
  url       = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00279/43493},
}

@InProceedings{Wu2020,
  author       = {Wu, Zuxuan and Lim, Ser-Nam and Davis, Larry S and Goldstein, Tom},
  booktitle    = {European Conference on Computer Vision},
  title        = {Making an invisibility cloak: Real world adversarial attacks on object detectors},
  year         = {2020},
  organization = {Springer},
  pages        = {1--17},
  groups       = {Question Answering},
}

@Article{Yang2018,
  author  = {Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W and Salakhutdinov, Ruslan and Manning, Christopher D},
  journal = {arXiv preprint arXiv:1809.09600},
  title   = {HotpotQA: A dataset for diverse, explainable multi-hop question answering},
  year    = {2018},
  file    = {:Papers/Yang2018.pdf:PDF},
  groups  = {Question Answering},
}

@InProceedings{Carlini2021,
  author    = {Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle = {30th USENIX Security Symposium (USENIX Security 21)},
  title     = {Extracting training data from large language models},
  year      = {2021},
  pages     = {2633--2650},
  comment   = {H1: Is it possible to indiscriminately rxtract training data from large langugage models? 
H3: Autoregressive text generation with LLM -> Measure perplexity of generated text. Low perplexity is more likely to be memorised (membership inference). Extract manually the interesting data. 

Lists types of memorized data: 
1. PII
2. URLs
3. Code
4: Non-trivial entropy strings (e.g. Session tokens) 
5. Data from 2 sources (merging and hallucinating links)
6. Removed content (i.e. archived material). 


Not directly related to Question Answering - but provides broad justificaiton for the importance of privacy-preservation in NLP tasks & im ML more generally.},
  file      = {:Papers/Carlini2021.pdf:PDF},
  groups    = {CMSC614 - Cyber Security, Question Answering},
}

@TechReport{Drgon2016,
  author      = {Drgon, Michele and Magnusun, Gail and Sabo, John},
  institution = {OASIS},
  title       = {Privacy Management ReferenceModel and Methodology (PMRM) Version 1.0},
  year        = {2016},
  month       = may,
  comment     = {Predictable and trusted privacy management must function within a complex, inter-connected set of networks, Business Processes, Systems, applications, devices, data, and associated governing policies.

An effective privacy management capability must be able to instantiate the relationshipbetween PI and associated privacy policies.

A clear strength of thePMRM is its recognition that today’s systems and applications span jurisdictions that have inconsistent and conflicting laws, regulations, business practices, and consumer preferences. This creates huge challenges to privacy management and compliance. It is unlikely that these challenges will diminish in any significant way, especially in the face of rapid technological change and innovation and differing social and national values, norms and policy interests.

The PMRMtherefore provides policymakers, the privacy office, privacy engineers, program and business managers, system architects and developers with a tool to improve privacy management and compliance in multiple jurisdictional contexts while also supporting delivery and business objectives

The PMRM consists of:
+ A conceptual model of privacy management, including definitions of terms;
+ A methodology; and
+A set of operational Services and Functions, together with the inter-relationships among these three elements.

APPLYING PMRM:

Phase 1: Scoping the usecase with which PI is assosciated; in effect, identifying the complete description in which the environment, application or capabilities where privacy anddata protection requirements are applicable.

Tasks: 
(1) GENERAL DESCRIPTION: Provide a general description of the use-case
(2) USE CASE INVENTORY:  Provide an inventory of the business environment, capabilities, applications and policy environment
(3) PRIVACY POLICY CONFORMANCE CRITERIA: Define and describe the criteria for conformance of the organization or a System or Business Process (identified inthe Use Case and inventory) with an applicable Privacy Policy or policies
(4) ASSESSMENT PREPARATION:  Include, or prepare, an initial Privacy Impact Assessment, or as appropriate, a risk assessment, privacy maturity assessment, compliance review, or accountability model assessment applicable to the Use Case
(5) IDENTIFY PARTICIPANTS:  Identify Participants having operational privacy responsibilities. 
(6) IDENTIFY SYSTEMS AND BUSINESS PROCESSES. dentify the Systems and Business Processes where PI is collected, stored, used, shared, transmitted, transferred across-borders, retained or disposed within a Domain.
(7) IDENTIFY DOMAINS AND OWNERS.  Identify the Domains included in the Use Case definition together with the respective Domain Owners
(8) IDENTIFY ROLES AND RESPONSIBILITIES WITH DOMAINS.  identify the roles and responsibilities assigned to specific Participants, Business Processes and Systems within a specific Domain
(9) IDENTIFY TOUCH POINTS: dentify the Touch Points at which the data flows intersect with Domains or Systems or Business Processes within Domains.
(10) IDENTIFY DATA FLOWS: Identify the data flows carrying PI and Privacy Controls among Domains
(11) IDENTIFY INCOMING PI: Incoming PI is PI flowing into a Domain, or a System or Business Process within a Domain.
(12) IDENTIFY INTERNALLY GENERATED PI. Internally Generated PI is PI created within the Domain or System or Business Process itself.
(13) IDENTIFY OUTGOING PI: Outgoing PI is PI flowing from one System to another, or from one Business Process to another, either within a Domain or to another Domain.
(14) SPECIFY INHERITED PRIVACY CONTROLS:  Specify the required Privacy Controls that are inherited from Domains or Systems or Processes.
(15) SPECIFY INTERNAL PRIVACY CONTROLS: Specify the Privacy Controls that are mandated by internal Domain Policies.
(16) SPECIFY EXPORTED PRIVACY CONTROLS: Specify the Privacy Controls that must be exported to other Domains or to Systemsor Business Processes within Domains (See below).
(17) IDENTIFY THE SERVICES AND FUNCTIONS NECESSARY TO SUPPORT OPERATION OF IDENTIFIED PRIVACY CONTROLS. Perform this task for each data flow exchange of PI between Systems and Domains. This detailed mapping of Privacy Controls with Services can then be synthesized into consolidated sets of Service and Functions per Domain, System or business environment as appropriatefor the Use Case.On further iteration and refinement, the identified Services and Functions can be further delineated by the appropriate Mechanisms.
(18) IDENTIFY MECHANISMS SATISFYING THE SELECTED SERVICES AND FUNCTIONS: nfi
(19)CONDUCT RISK ASSESSMENT: 
(20) ITERATE AND REFINE

SERVICES AND FUNCTIONS NEEDED TO IMPLEMENT PRIVACY CONTROLS: 

Eight services can be grouped into three categories (more detail of what they specifically do in the document ) 

Core Policy: Agreement, Usage
Privacy Assurance: Validation, Certification, Enforement, Security 
Presentation and Lifecycle: Interaction, Access

__Agreement__ Defines and documents permissions and rules for the handling of PI based on applicable policies, data subject preferences, and other relevant factors; provides relevant Actors with a mechanism to negotiate, change or establish new permissions and rules; expresses the agreements such that they can be used by other Services. Purpose is to manage and negotiate permissions and rules. 

__Usage__  Ensures that the use of PI complies with the terms of permissions, policies, laws, and regulations, including PI subjected to information minimization, linking, integration, inference, transfer, derivation, aggregation, anonymization and disposal over the lifecycle of the PI. Purpose is to control PI Use

__Validation__ Evaluates and ensures the information quality of PI in terms of accuracy, completeness, relevance, timeliness, provenance, appropriateness for use and other relevant qualitative factors. Purpose is to ensure PI Quality.

__Certification__ Ensures that the credentials of any Actor, Domain, System, or system component are compatible with their assigned roles in processing PI and verifies their capability to support required Privacy Controls in compliance with defined policies and assigned roles. Purpose is to Ensure Appropriate privacy management Credentials. 

__Enforcement__ nitiates monitoring capabilities to ensure the effective operation of all Services.  Initiates response actions, policy execution, and recourse when audit controls and monitoring indicate operational faults and failures.  Records and reports evidence of compliance to Stakeholders and/or regulators. Provides evidence necessary for  accountability. Purpose is to monitor proper operation and respond to exception conditions and report on demand evidence of compliance where required for accountability. 

__Security__ Provides the procedural and technical mechanisms necessary to ensure the confidentiality, integrity, and availability of PI; makes possible the trustworthy processing, communication, storage and disposition of PI; safeguards privacy operations. Purpose is to safeguard privacy information and operations. 

__Interaction__  Provides generalized interfaces necessary for presentation, communication, and interaction of PI and relevant information associated with PI,encompassing functionality such as user interfaces, system-to-system information exchanges, and agents. Purpose is information presentation and communication. 

__Access_ _ Enables Data Subjects, as required and/or allowed by permission, policy, or regulation, to review their PI that is held within a Domain and propose changes, corrections or deletion for their PI. Purpose is to view and propose changes to PI. 

DEFINITIONS: 

__Actor__  a human or a system-level, digital ‘proxy’ for either a (human) Participant, a (non-human) system-level process or other agent.


__Domain__ A Domain includes both physical areas (such as a customer site or home, a customer service center, a third party service provider) and logical areas (such as a wide-area network or cloud computing environment) that are subject to the control of a particular Domain owner

__Domain Owner__ A Domain Owner is the Participant responsible for ensuring that Privacy Controls are implemented in Services and Functions within a given Domain.

__Incoming PI__ Incoming PI is PI flowing into a Domain, or a System or Business Process within a Domain.

__Internally Generated PI__ Internally Generated PI is PI created within the Domain or System or Business Process itself.

__Outgoing PI__  Outgoing PI is PI flowing from one System to another, or from one Business Process to another, either within a Domain or to another Domain.

__Participant__ A Participant is any Stakeholder responsible forcollecting, storing, using, sharing, transmitting, transferring across-borders, retaining or disposing PI, or is involved in the lifecycle of PI managed by a Domain, or a System or Business Process within a Domain.

__Privacy Control __ Privacy Controls are usually stated in the form of a policy declaration or requirement and not in a way that is immediately actionable or implementable.

__Service__  a collection of related Functions that operate for a specified purpose

__System or business process__ a System or Business Process is a collection of components organized to accomplish a specific function or set of functions having a relationship to operational privacy management

__Touchpoint__ Touch Points are the intersections of data flows across Domains or Systems or Processes within Domains.



KENT THOUGHTS: 
- What is the definition of PI? 
---> Personal Information –any data that describes some attribute of, or that is uniquely associated with, a natural person
- Has a detailed glossary

-- The process could be a good framework to modify the legal ontology framework

-- The document could be used as a soruce of ontology learning? 

-- Should compare the source to the privacy ontologies to see similarity? 

-- Should compare to other models.},
  file        = {:Documents/Uni/USQ/2021-S1/MSC8001 - Research Project I/Research Articles/Drgon2016.pdf:PDF},
  groups      = {MSC8001, Question Answering},
  journal     = {OASIS Specification},
  school      = {OASIS},
  timestamp   = {2021-05-12},
  url         = {https://docs.oasis-open.org/pmrm/PMRM/v1.0/cs02/PMRM-v1.0-cs02.pdf},
}

@InCollection{Finn2013,
  author    = {Finn, Rachel L and Wright, David and Friedewald, Michael},
  booktitle = {European data protection: coming of age},
  publisher = {Springer},
  title     = {Seven types of privacy},
  year      = {2013},
  pages     = {3--32},
  comment   = {However, “privacy” has proved noto- riously difficult to define. “The notion of privacy remains out of the grasp of every academic chasing it.

Helen Nissenbaum has argued that privacy is best understood through a notion of “contextual integrity,” where it is not the sharing of information that is a problem, rather it is the sharing of information outside of socially agreed contextual boundar- ies.

Although a widely accepted definition of privacy remains elusive, there has been more consensus on a recognition that privacy comprises multiple dimensions, and some privacy theorists have attempted to create taxonomies of privacy problems, intrusions or categories.

SOLOVE: His taxonomy includes problems related to information collection, such as surveillance or interrogation, problems associated with information processing, including aggregation, data insecurity, potential identification, secondary use and exclusion, information dissemination, including exposure, disclosure breach of confidentiality, etc. and invasion, such as issues related to intrusion and decisional interference.1

KASPAR: distinguishes between invasions involving extraction, observation and intrusion Extraction- based privacy invasions involve making a deliberate effort to obtain something from a person. Observation-based privacy invasions are characterised by active and on- going surveillance of a person, while intrusion-based invasions involve an “unwelcome presence or interference” in a person’s life.

They focus on specific harms which are already occurring and which must be stopped, rather than over-arching protections that should be instituted to prevent harms. The difference between a taxonomy of privacy harms and a taxonomy of types of privacy is the pro-active, protective nature of the latter. 

CLARKE: 
__(1) Privacy of the Person __ specifically related to the integrity of a person’s body. It would include protections against phys- ical intrusions, including torture, medical treatment, the “compulsory provision of samples of body fluids and body tissue” and imperatives to submit to biometric measurement.

__(2) Privacy of Personal Behaviour.__ protection against the disclosure of sensitive personal matters such as religious practices, sexual practices or political activities. Clarke notes that there is a space element included within privacy of personal behaviour, where people have a right to private space to carry out particular activities, as well as a right to be free from sys- tematic monitoring in public space.

__ (3) Privacy of Personal Communication__ refers to a restriction on monitoring telephone, e-mail and virtual communications as well as face-to-face communications through hidden microphones.

__(4) Privacy of Personal Data.__ refers to data protection issues. Clarke adds that, with the close coupling that has occurred between computing and communications, particularly since the 1980s, the last two aspects have become closely linked, and are commonly referred to as “information privacy”.

^^^ These Four categories outdated now according to this model ^^^

New 7: 

__PRIVACY OF THE PERSON__ encompasses the right to keep body functions and body characteristics (such as genetic codes and biometrics) private.'

__PRIVACY OF BEHAVIOUR AND ACTION__ This concept includes sensitive issues such as sexual preferences and habits, political activities and religious practices. However, the notion of privacy of personal behaviour concerns activities that happen in public space, as well as private space, The ability to behave in public, semi-public or one’s private space without having actions monitored or controlled by others con- tributes to “the development and exercise of autonomy and freedom in thought and action”.

__PRIVACY OF COMMUNICATION__  aims to avoid the interception of communications, including mail interception, the use of bugs, directional microphones, telephone or wireless communication interception or recording and access to e-mail messages. This aspect of privacy benefits individuals and society because it enables and encourages a free discussion of a wide range of views and options, and enables growth in the communications sector.

__PRIVACY OF DATA AND IMAGE__  includes concerns about making sure that individuals’ data is not automatically available to other individuals and organisations and that people can “exercise a substantial degree of control over that data and its use”.26 Such control over personal data builds self-confidence and enables individuals to feel empow- ered

__PRIVACY OF THOUGHTS AND FEELINGS__ People have a right not to share their thoughts or feelings or to have those thoughts or feeling revealed. Individuals should have the right to think whatever they like. Such creative freedom benefits society because it relates to the balance of power between the state and the individual. This aspect of privacy may be coming under threat as a direct result of new and emerging technologies. Privacy of thought and feelings can be distinguished from privacy of the person, in the same way that the mind can be distinguished from the body. Similarly, we can (and do) distinguish between thought, feelings and behaviour. Thought does not automatically translate into behaviour. Similarly, one can behave thoughtlessly (as many people often do).

__PRIVACY OF LOCATION AND SPACE__ individuals have the right to move about in public or semi-public space without being identified, tracked or monitored. This conception of privacy also includes a right to solitude and a right to privacy in spaces such as the home, the car or the office.

__PRIVACY OF ASSOSCIATION (INCLUDING GROUP PRIVACY)__ is concerned with people’s right to associate with whomever they wish, without being monitored. This has long been recognised as desirable (necessary) for a democratic society as it fosters freedom of speech, including political speech, freedom of worship and other forms of association.

Clarity on Location and Space vs Behaviour: Privacy of location means that a person is entitled to move through physical space, to travel where she wants without being tracked and moni- tored. Privacy of behaviour means the person has a right to behave as she wants (to sleep in class, to wear funny clothes) so long as the behaviour does not harm someone else.

KENT THOUGHTS: 
- This chapter also lists a large number of case studies specific to technology that could be good case studies to evaluate the ontology against
- TABLE 1.1 PROVIDES A PROFESSIONAL ASSESSMENT OF THE RISKS - THIS COULD BE USEFUL AS A COMPARISON FOR A CASE STUDY IN THE EVALUATION STAGE.},
  file      = {:Documents/Uni/USQ/2021-S1/MSC8001 - Research Project I/Research Articles/Finn2013.pdf:PDF},
  groups    = {MSC8001, Question Answering},
  timestamp = {2021-05-13},
}

@InProceedings{Gharib2017,
  author       = {Gharib, Mohamad and Giorgini, Paolo and Mylopoulos, John},
  booktitle    = {International conference on conceptual modeling},
  title        = {Towards an ontology for privacy requirements via a systematic literature review},
  year         = {2017},
  organization = {Springer},
  pages        = {193--208},
  abstract     = {Breaks it into dimensions and entities: 

(1) Organisational Dimension: 

(a) Agentive Entities:  the active entities of the system, we have selectedthree concepts along with two relations:Actorrepresents an autonomous entity thathas intentionality and strategic goals, and it cover two entities, arolethat is an abstractcharacterization of an actor in terms of a set of behaviors andfunctionalities

(b) Intentional Entities. the behavior of actors is determined by the objectives theyaim to achieve. Therefore, we adopt the goal concept to represent such objectives. Agoalis a state of affairs that an actor intends to achieve

(c) Informational Entities. nformationrepresents any informational entity withoutintentionality. Information can be composed of several parts,  In thecontext of this work, we differentiate between two main types of information:
(i) personal information, any information that can berelated(directly or indirectly) to an identifiedor identifiable legal entity (e.g., names, addresses); and
(ii) public information, anyinformation that cannot berelated(directly or indirectly) to an identified or identifiablelegal entity, or personal information that has been made public by its legal entity

(d) Information Type of Use.The ontology adopts three relationships between goals and information:Produce,Read,andModifythat indicate a goal achievement depends on creating, consuming, andmodifying such information respectively

(e) Information Ownership and Permissions:  ownconcept relates personal informa-tion to its legitimate owner, who has full control over its usage, which can be controlleddepending on permissions. Apermissionis consent of a particular use of a particular ob-ject in a system. The ontology considers three different types of permissions ((P)roduce,(R)ead, (M)odify) 

(f) Entity Interactions. the ontology adopts three types of interactions.
(i) Information provisionindicates that an actor has the capability to deliver information to another one. Infor-mation provision has one attribute that describes the provisioning type, which can be either confidential or non-confidential, where the first guarantee the confidentiality ofthe transmitted information while the last does not.
(ii) Goal delegationindicates that one actor delegates the responsibility to achieve a goal to other actors.
(iii) Permissions delega-tionindicates that an actor delegates the permissions to produce, read and/or modifyover a specific information to another actor.

(g) Entity Social Trust: ur ontologyadopts the notion oftrustanddistrustto capture the actors’ expectations of oneanother concerning their delegations.Trustindicates the expectation of trustor thatthe trustee will behave as expected considering the trustum;whiledistrustindicatesthe expectation of trustor that the trustee will not behave asexpected.

(h) Monitoring: the process of observing and analyzing the performanceof an actor in order to detect any undesirable performance 

(2) Risk Dimension:

(a) A Threat is a potential incident to personal inforamation. It can be natural , accidental (together known as causal) or intentional  (Requiring a threat actor and an attack method)

(b) Threats exploit vulnerabilities of the information. 

(c) Threats generate Impacts which have severeties

(3) Treatment Dimension: 

introduces countermeasure conceptsto mitigate risks. Key concepts are: 

(a) Privacy Goal:  defines an aim tocounter threats and prevents harm to personal information by satisfying privacy criteriaconcerning such information

(b) Privacy Constraint: s defined as a design restriction thatis used to realize/satisfy a privacy goal, constraints can be either a privacy policy orprivacy mechanism'

(c) Privacy Policy:  is a privacy statement that defines the permittedand/or forbidden actions to be carried out by actors of the system toward information

(d) Privacy Constraint: a concrete technique to be implemented for helping towardsthe satisfaction of privacy goal (attribute)

(4) Privacy Dimension:
Concepts are: 

(a) Privacy Requirement: wner/data subject privacy needs at ahigh abstraction level. Broken into: 

(i) Confidentiality
-->(1) Non-Disclosure
-->(2) Need-to-know
-->(3) Prupose of use
(ii) Notice: 
(iii) Anonymity
(iv) Transparency
-->(1) Authentication
-->(2) Authorization
(v) Accoountability 
-->(1) Non-Repudiation
-->(2)Non-Redelegation

---

Kent Thoughts: 
- This would be a mess to implement / reason with. 
- I prefer the more general typologies, however linking to these options is a possible approach to try and map the IoT conepts to the privacu conseqiences.},
  file         = {:Documents/Uni/USQ/2021-S1/MSC8001 - Research Project I/Research Articles/Gharib2017.pdf:PDF},
  groups       = {MSC8001, Question Answering},
  timestamp    = {2021-05-11},
  url          = {https://www.researchgate.net/profile/Mohamad-Gharib/publication/318787316_Towards_an_Ontology_for_Privacy_Requirements_via_a_Systematic_Literature_Review/links/597ecfc8a6fdcc1a9accba79/Towards-an-Ontology-for-Privacy-Requirements-via-a-Systematic-Literature-Review.pdf},
}

@Article{Hendrycks2021,
  author  = {Hendrycks, Dan and Carlini, Nicholas and Schulman, John and Steinhardt, Jacob},
  journal = {arXiv preprint arXiv:2109.13916},
  title   = {Unsolved problems in ml safety},
  year    = {2021},
  comment = {This paper is a survey of the state-of-the-art in Machine Learning Safety and a summary of the open problems in the field. 

They define ML Safety research as ML research aimed at making the adoption of ML more beneficial, with emphasis on long-term and long-tail risks.

They categorise the risks into four categories: Robustness, Monitoring, Alignment and External Safety. 

Within Robustness they identify two ideas: (1) Black Swans and (2) Adversaries.

Black Swan Events refer to a class of events that are unforseeable, and typically have never occured before (or at least not on the scale). They exist in what Nassim Taleb calls 'extremistan', the world we live in where rare events are better represented by power law distributions that occur far more frequently and have a much more significant impact than we would expect. By definition, Black Swan events are unpredictable, and so if we can't design ML models to prevent them, we need to ensure that they are robust enough to withstand them. COMMENT: Taleb's later work on anti-fragility offers a different paradigm. Robust or Resilient systems are able to weather adversity and return to the level of function they experienced prior to the disruption. Anti-fragile systems however gain from volatility and disorder and actually improve as a results of random, unexpected events. A weak analogy is Netflix's Chaos Engineering practices.  I think that their focus on robustness here might be limiting, while much harder to achieve, anti fragile systems should absolutely be the aiming mark. They actually conflate the terms in their discussion here.

Adversarial Robustness is about making ML systems more resilient to targeted attacks. They identify a possible correlation between robustness to random events and robustness to targeted events. 

MONITORING has three sub categories. (1) Anomaly Detection. (2) Representative Model Outputs and (3) Hidden Functionality. 

Anomaly Detection here refers to analysing the inputs and outputs of a model for signs of 'unusual' use. e.g. detecting adversarial attacks. 

Representitive Model Outputs here refers to making model outputs more transparent and trustworthy to those using them.  COMMENT: I think that there could be an interesting direction here that crosses a little over into machine education / curriculum learning. By Analogy, consider the model that Dave Marquet discusses for developing trust between commander and team member: Starting out, the model could 'monitor' human behaviour to develop a baseline (Mimicry Learning). Phase 2 becomes low trust where when faced with a question it describes the solution to the human in the loop which can be confirmed or denied. Phase 3 is where the model starts to make predictions "I intend to" to which the human can approve or deny
Phase 4 is just reporting the actions taken, with human correction made after the fact (i.e. accept that error and prevent future ones). More of a human-machine teaming idea but I think it could be an interesting way to build trust. 

Seperate idea would be to take some of tetlock's ideas about problem decomposition, estimation and evaluation. The human can assess the performance of the machine, but in some domains the machine can also assess the performance of the machine (i.e. supervised learning). 

Hidden Model Functionality  describes the risk of backdoors in models as well as the risk of unexpected emergent capabilites. They point to a particular concern where some models have been observed to manipulate their output differently when they are being observed to when they are not

ALIGNMENT has four sub-areas. (1) Specification (2) Brittleness (3) Optimization and (4) Unintended Consequences. 

Specification  talks about the difficulty of encoding the intent of a system. If we are able to articuate a principle or value system that the system is trying to work towards rather than a specific mathematical objective should allow for more flexibility and robustness. 

Optimization talks about the classic risk of focusing on improving the metric rather than the value of the system. If we optimise the wrong indicator are we achieveing optimal results. COMMENT: Could be something here on classifying Lagging Indicators vs future looking ideas or behaviours. It would be interesting to see if you could optimise for adherence to a principle or a behaviour rather than just a metric. E.g. Making "Good" ethical descisions rather than ones which just employ a proxy like the utilitarian minimization of harm, or a kantian 'tell no lies'. 

Brittleness like the last section refers to the impact of goodharts law "When a measure becomes a target, it ceases to be a good measure".  How do you stop people gaming the optimization function? 

Unintended Consequences - refers to minimising unintended harm of AI/ML activities. 

EXTERNAL SAFETY has two key sub areas (1) ML For Cyber Security and (2) Informed Decision Making. 

ML For Cyber Security just discussed applying ML to the Cyber Security Domain. 

Informed Decision Making refers to applying ML to Forecasting and machine question-asking. COMMENT: Forecasting has been done pretty heavily, the DARPA ICEWS program focused on this and in the end the public domain findings were that based on using the news as a data source, predictable events were predictable only a short time out (not long enough for real advantage) and unpredictable ones were not.
COMMENT2: I think that the question-asking idea is interesting. This links to my idea about the "Who Shot Mr Burns" problem. Can you provide a large number of fact-triples that describe a scenario (e.g. who shot Mr Burns) and then based on that form a graph with the shortest path to connect the crucial elements that you need to know. Then, when you are faced with a novel scenario, you try to fit the facts you have to other scenarios to see if the same links exist. If they don't, can it prompt the user for what bitsEr of information they're missing that they actually NEED to know? Might help with prioritising collection efforts, or prompt a user with new questions or risks?},
  file    = {:/Volumes/GoogleDrive/My Drive/ARLIS/Hendryks2021.pdf:PDF},
  groups  = {ARLIS / Thesis, Question Answering},
}

@InProceedings{Kasper2005,
  author       = {Kasper, Debbie VS},
  booktitle    = {Sociological Forum},
  title        = {The evolution (or devolution) of privacy},
  year         = {2005},
  number       = {1},
  organization = {Springer},
  pages        = {69--92},
  volume       = {20},
  comment      = {Three main problems have hindered the establishment of a unifying framework for privacy study. First, the majority of attempts to define pri- vacy are misspecified; that is, they focus either too specifically or too broadly on a particular topic. The result is either a narrow conception of privacy that is not generalizable or a definition so vague as to be methodologically useless. Second, the definitions of privacy employed are culturally and his- torically biased and thus may not be applicable to other sociohistorical con- texts. Finally, work on privacy tends to be value-driven. Authors, whether speaking in privacy’s defense or advocating its reduction, begin their work with strong biases and have predetermined goals, which naturally affects their questions, data, findings, and conclusions.

If privacy is to be understood, it must be examined from the inside, that is, from the standpoint of the ex- perience of its invasion. My intent in creating the typology is to account for all possible ways in which one’s privacy can be invaded and to identify distinctive characteristics associated with each type of invasion. Categorizing invasions, in this way, provides a more meaningful context for each incident, as the type indicates how a party’s privacy has been violated and hints at the extent to which one is aware of and has assented to the invasion.

There are three types of Invasion:  (1) Extraction, (2) Observation (3) Intrusion

__EXTRACTION__ The primary activity involved in extraction is taking. It involves a de- liberate effort to obtain something from an individual or group. Extraction typically takes place in discrete instances. There are three types of extrac- tion invasions: stockpiling, appropriation/disclosure, and inner-state.

 __Stockpiling__ the processes of collection, exchange, storage, or use of information.

__Appripriation/Disclosure__ involves the taking and/or disclosing of one’s personal information, identity, image, or likeness—usually for a specific purpose.  it tends to be somehow damaging to one or to one’s rep- utation. One usually becomes aware of the invasion upon disclosure.

__Inner State__ involve efforts to determine some as- pect of a person that is not externally knowable, including psychological, emotional, intellectual, or physical status. This is done in order to make an evaluative judgment of some sort. One is usually aware of this type of invasion. Mental health exams for prospective or present employees, poly- graph tests, and drug tests are examples of inner-state invasions.

__OBSERVATION__ Observation, as an invasion of privacy, mainly consists of “watching,” though not necessarily with one’s eyes. I use the term watching to refer to surveillance in general. Such invasions involve active and ongoing surveil- lance of a person or persons; hence, they are not discrete instances, but are ongoing. In general, individuals are unaware of the observation and do not consent to being watched. In some circumstances people know they are being observed, or perhaps attempts to inform them are made, but they remain largely unconscious of the observation.

__ Physical Observation__  observation is the surveillance of a physical entity (usually a human being) and its movements and actions. Individuals remain largely unaware that they are being watched. ncluded in such invasions are the presence of surveillance cameras, old- fashioned voyeurism, (i.e., the “peeping tom”), and tracking devices like RFID (radio frequency identification) placed on objects.

 __Communication Observation__  involves the interception and/or surveil- lance of communication in any form: telephone, mobile phone, e-mail, fax, face-to-face conversation, letters, and so forth. Wiretapping and “bugs” are obvious instances of communication surveillance, but other examples in- clude the interception of e-mails, tape-recording a person without consent, and reading another’s mail.

__Behavioural Observation__  What distinguishes this from other invasions is the explicit monitoring of a behavior as opposed to the surveillance of a specific physical entity or communicative activity. For example, some marketing companies, rather than simply gathering data about individuals, employ methods that track consumers’ buying habits.

__INTRUSION__ Intrusion invasions are marked by the activity of “entering.” This can take place in many ways but generally involves the entry of a presence or in- terference that is both uninvited and unwelcomed by the invadee. Instances of intrusion are typically discrete, and the invadee is usually aware of them.

 __Sensory Intrusions__  are incursions into one’s immediate physical sur- roundings of which one is conscious via sight, sound, smell, touch, or taste. One is aware of them but does not willingly assent to them. They typically do not present danger, but are seen as annoying or disturbing in some way. Such presences are considered invasions of one’s private spatial and sensory realm. Examples of sensory intrusions include junk mail, telephone calls from telemarketers, street lights shining in windows at night, loud music blaring nearby, and the like.

 __Bodily Incursions__ are perceived assaults upon one’s physical person in which the primary offense is the “entry.” Their purpose is not to produce data or gather information. These include both dangerous and relatively harmless incursions. Bodily intrusions are uninvited and unwelcome, but the invadee is aware that they are taking place. Such intrusions include events as benign as being bumped into on a crowded subway, or as seri- ous as sexual or physical assault. Strip searches and unauthorized medical treatments are also examples of bodily intrusions.

__Autonomy Intrusions__  involve instances that interfere with one’s sense of comfort, stability, safety, or rights. They interrupt one’s sense of well-being and are not associated with any particular sense or direct phys- ical contact. Examples of autonomy intrusions include sodomy laws, locker searches in high schools, racial profiling practices, and police roadblocks.

--- --- ---

KENT THOUGHTS: 
- Another useful framework for privacy. Beteween this one, Clarke, Solove and Finn it's a good start point for actually using a generic privacy structure to map the potential harms to.},
  groups       = {MSC8001, Question Answering},
  timestamp    = {2021-05-13},
}

@InProceedings{Newsome2005,
  author    = {Newsome, James and Song, Dawn Xiaodong},
  booktitle = {NDSS},
  title     = {Dynamic taint analysis for automatic detection, analysis, and signaturegeneration of exploits on commodity software.},
  year      = {2005},
  pages     = {3--4},
  volume    = {5},
  comment   = {The paper “Dynamic Taint Analysis for Automatic Detection, Analysis, and Signature Generation of Exploits on Commodity Software” outlines the drivers, design and initial evaluation of their system ‘taintcheck’. Taintcheck had the stated goals of easy deployment, few false positives, few false negatives and detecting attacks early in the attack cycle. 

The paper describes how they break the problem into four steps. TaintSeed tags ‘tainted’ data, that is, data from an untrusted location that may possibly be malicious. The TaintTracker then tracks the instructions that manipulate data, to determine if the result is tainted. TaintAssert checks to see if possibly tainted data is used in ways that are harmful (eg modifying jump addresses, in format strings). Finally their exploit analyser allows for the back trace of when a compromise occurs. 

One of the interesting things to me was the impact that their unoptimised had on the runtime of the applications and the options they came up with to mitigate that. First, optimising their code (and swapping to a more efficient base than valgrind) and then in the number of different ways they theorised it could be used that would have less performance impact.},
  file      = {:/Volumes/GoogleDrive/My Drive/Newsome2010.pdf:PDF},
  groups    = {CMSC614 - Cyber Security, Question Answering},
}

@Booklet{OfficeoftheAustralianInformationCommissioner2014,
  title     = {The Australian Privacy Principles},
  author    = {OAIC ,Office of the Australian Information Commissioner},
  month     = jan,
  year      = {2014},
  comment   = {Lists the Australian Privacy Principles. 

Could be an interesting link to compare to the Finn, Kasper and Solove models to try and draw out the techncial implementations.},
  groups    = {MSC8001, Question Answering},
  journal   = {OAIC},
  timestamp = {2021-05-12},
  url       = {https://www.oaic.gov.au/assets/privacy/australian-privacy-principles/the-australian-privacy-principles.pdf},
}

@Article{Solove2005,
  author    = {Solove, Daniel J},
  journal   = {U. Pa. L. Rev.},
  title     = {A taxonomy of privacy},
  year      = {2005},
  pages     = {477},
  volume    = {154},
  comment   = {READ IN MORE DETAIL, HOWEVER: 

the word _privacy_ has proven to be a powerful rhetorical battle cry in a plethora of unrelated contexts. . . . Like the emotive word _freedom,_ _privacy_ means so many different things to so many different people that it has lost any precise legal connotation that it might once have had.

_Privacy is a chameleon-like word, used denota- tively to designate a wide range of wildly disparate interests—from confi- dentiality of personal information to reproductive autonomy—and connota- tively to generate goodwill on behalf of whatever interest is being asserted in its name._

What commentators often fail to do, however, is translate those instincts into a reasoned, well-articulated account of why privacy problems are harmful. When people claim that privacy should be protected, it is unclear precisely what they mean.

The most famous attempt was undertaken in 1960 by the legendary torts scholar William Prosser. He discerned four types of harmful activities redressed under the rubric of privacy:
1. Intrusion upon the plaintiff’s seclusion or solitude, or into his private affairs.
2. Public disclosure of embarrassing private facts about the plaintiff.
3. Publicity which places the plaintiff in a false light in the public eye.
4. Appropriation, for the defendant’s advantage, of the plaintiff’s name or like- ness.20

New technologies have given rise to a panoply of different privacy problems, and many of them do not readily fit into Prosser’s four categories. Therefore, a new taxonomy to address privacy violations for contemporary times is sorely needed.

_Without society there would be no need for privacy_

purpose of this taxonomy is not to argue that the law should or should not protect against certain activities that affect privacy. Rather, the goal is sim- ply to define the activities and explain why and how they can cause trouble.
-- Important for producing a GENERAL ontological structure, not rooted in a specific legal system. 

The taxonomy demonstrates that there are connections between differ- ent harms and problems.

In the taxonomy that follows, there are four basic groups of harmful ac- tivities: (1) information collection, (2) information processing, (3) informa- tion dissemination, and (4) invasion. ((IMAGE ON PAGE 15) 

INFORMATION COLLECTION:  Information collection creates disruption based on the process of data gathering. Even if no information is revealed publicly, information collec- tion can create harm.
__Surveillance__ the watching, listening to, or recording of an indi- vidual’s activities.
__Interrogation__ various forms of questioning or probing for information.

INFORMATION PROCESSING:  Processing involves various ways of connecting data together and link- ing it to the people to whom it pertains. Even though it can involve the transmission of data, processing diverges from dissemination because the data transfer does not involve the disclosure of the information to the pub- lic–-or even to another person. Rather, data is often transferred between various record systems and consolidated with other data. Processing di- verges from information collection because processing creates problems through the consolidation and use of the information, not through the means by which it is gathered.

__Aggregartion__ the combination of various pieces of data about a person.
__Identification__  linking information to particular individuals.
__Insecurity__  carelessness in protecting stored information from leaks and improper access.
__Secondary Use__  the use of information collected for one purpose for a different purpose without the data subject’s consent. 
__Exclusion__ concerns the failure to allow the data subject to know about the data that others have about her and participate in its handling and use

INFORMATION DISSEMINATION: “Informa- tion dissemination” is one of the broadest groupings of privacy harms. These harms consist of the revelation of personal data or the threat of spreading information.

__Breach of Confidentiality__ breaking a promise to keep a person’s informa- tion confidential.
__Disclosure__ the revelation of truthful information about a person that impacts the way others judge her character.
__Exposure__ revealing another’s nudity, grief, or bodily functions.
__Increased Accessibility__  amplifying the accessibility of information.
__Blackmail__ the threat to disclose personal information.
__Appropriation__  the use of the data subject’s identity to serve the aims and interests of another.
__Distortion__ the dissemination of false or misleading information about individuals.

INVASION: nvasion harms differ from the harms of information collection, networking, and dis- semination because they do not always involve information.

__Intrusion__  invasive acts that disturb one’s tranquility or solitude
__Decisional Interference__ involves the government’s incursion into the data subject’s decisions regarding her private affairs.


---

When translated into the legal system, privacy is a form of protection against certain harmful or problematic activities.



--- --- ---

KENT THOUGHTS: 
- NOTE: This is based on the US Legal System
- The Collection / Processing / Dissemination and Invasion model could be a good start point.
- Provides a very solid basis for trying to classify privacy issues},
  file      = {:Documents/Uni/USQ/2021-S1/MSC8001 - Research Project I/Research Articles/Solove2005.pdf:PDF},
  groups    = {MSC8001, Question Answering},
  publisher = {HeinOnline},
  timestamp = {2021-05-13},
  url       = {https://scholarship.law.gwu.edu/cgi/viewcontent.cgi?article=2074&context=faculty_publications},
}

@Article{Tabatabaei2023,
  author  = {Tabatabaei, Masoud and Jamroga, Wojciech},
  journal = {arXiv preprint arXiv:2303.00067},
  title   = {Playing to Learn, or to Keep Secret: Alternating-Time Logic Meets Information Theory},
  year    = {2023},
  comment = {Read 07 Mar 23

H1: It it possible to use formal logic to describe the value of certain facts and their disclosure in a multi-agent context
H3: They develop Alternating Time Temporal Logic - Hartley; extending ATL to incorporate Hartley reasoning over uncertainty. 	

The specifics of this paper are not relevant. However, their motivation and some of their description of the problem domain is interesting: 

Namely:

- Having an agent that keeps secrets is helpful for privacy. 

- Partial information can be just as valuable as the full secret content, as it can reduce uncertainty. 

- So, when trying to learn things, having some knowledge can be just as good as having all knowledge, and conversely, leaking some information can be just as bad as leaking all information.},
  file    = {:/Volumes/GoogleDrive/My Drive/CMSC723/WhoIsYourDaddyAndWhatDoesHeDo/Papers/Tabatabaei2023.pdf:PDF},
  groups  = {Question Answering},
  url     = {https://arxiv.org/pdf/2303.00067.pdf},
}

@Article{Jagielski2022,
  author  = {Jagielski, Matthew and Thakkar, Om and Tramer, Florian and Ippolito, Daphne and Lee, Katherine and Carlini, Nicholas and Wallace, Eric and Song, Shuang and Thakurta, Abhradeep and Papernot, Nicolas and others},
  journal = {arXiv preprint arXiv:2207.00099},
  title   = {Measuring forgetting of memorized training examples},
  year    = {2022},
  comment = {Memorization and Forgetting are important, but distinct from secret-keeping. They would constitute side channel or out-of-band attacks, where an interrogator on realising they can't induce leakage in another way would try a privacy attack. 

- More recent examples being more at risk of memorization makes the idea of fine-tuning for your organisational use case riskier. 

- There is a difference between 'private' and 'secret' 

- The paper has many useful definitions. 

- Need to read up more on canary extraction.},
  file    = {:/Volumes/GoogleDrive/My Drive/CMSC723/WhoIsYourDaddyAndWhatDoesHeDo/Papers/Jagielski2022.pdf:PDF},
  groups  = {Question Answering},
  url     = {https://arxiv.org/pdf/2207.00099},
}

@InProceedings{Carlini2019,
  author    = {Carlini, Nicholas and Liu, Chang and Erlingsson, {\'U}lfar and Kos, Jernej and Song, Dawn},
  booktitle = {USENIX Security Symposium},
  title     = {The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks.},
  year      = {2019},
  volume    = {267},
  comment   = {H1: How do we assess the risk that models unintentionally memorize training data. 

H3: Develop exposure metrics by inserting canaries and measuring their presence. 

__Comments__
When they say 'secret' they mean geneal private or sensitive information (e.g. home addresses or credit cards generally, rather than specifc facts). 

They refer to Sanitization, which is a relevant and useful idea, however it is limited by not citing work in the other area other than saying it isn't effective for measuring general privacy information leaks. They attempted to develop a model but it failed. 

- They note that rare examples can be memorized. This is likely in our intel use case. 
- It is hard to reliably detect and prevent unintentional memorization happening. 
- Comment: while we can't hope to prevent all privacy leaking, we can at least protect the crown jewels?},
  file      = {:/Volumes/GoogleDrive/My Drive/CMSC723/WhoIsYourDaddyAndWhatDoesHeDo/Papers/Carlini2019.pdf:PDF},
  groups    = {Question Answering},
  url       = {https://www.usenix.org/system/files/sec19-carlini.pdf},
}

@Article{Kandpal2022,
  author  = {Kandpal, Nikhil and Deng, Haikang and Roberts, Adam and Wallace, Eric and Raffel, Colin},
  journal = {arXiv preprint arXiv:2211.08411},
  title   = {Large language models struggle to learn long-tail knowledge},
  year    = {2022},
  comment = {H1: How do we improve the ability of a LLM to answer factual questions about topics it rarely observes in training data. 

H3: Determine the relatinship by comparing QA performance with the count of related documents in the training dataset. Enhance that with counterfactual analyssi. 

__FINDINGS__:
Both LLM and Retrievel based QA model performance at QA tasks is related to the number of related documents observed in the training set. --> COMMENT: This justifies NOT redacting the sensitive data, it needs to be in there to learn. 

The ability to answer questions drops dramatically when source documents are removed (I.e. it doesn't learn the material)

They focus on non-verbatim memorization and knowledge learning, as opposed to verbatim memorization. ie. how to provide factoid answers. 

- Retrieval augmentation improves performance of LLM QA (i.e. a hybrid approach). --> COMMENT: This justifies ryan's design decision for an IR model up-front.},
  file    = {:/Volumes/GoogleDrive/My Drive/CMSC723/WhoIsYourDaddyAndWhatDoesHeDo/Papers/Kandpal2022.pdf:PDF},
  groups  = {Question Answering},
  url     = {https://arxiv.org/pdf/2211.08411},
}

@InProceedings{Kandpal2022a,
  author       = {Kandpal, Nikhil and Wallace, Eric and Raffel, Colin},
  booktitle    = {International Conference on Machine Learning},
  title        = {Deduplicating training data mitigates privacy risks in language models},
  year         = {2022},
  organization = {PMLR},
  pages        = {10697--10707},
  comment      = {H1; Can we reduce the presence/risk of memorization in LLMs by deduplicating training data

H3: They deduplicate

__COMMENTS__

- They find that the relationship is superlinear between duplicated data and memorization (10x more in training is 1000x more likely in generation). 

- They note that dealing with EXACT duplication is easy, but there is still an impact on memorixation from approximate duplicates. 

- They don't know how to detect approximately duplicate work; ie things that are semantically similar in different lexical forms.},
  file         = {:/Volumes/GoogleDrive/My Drive/CMSC723/WhoIsYourDaddyAndWhatDoesHeDo/Papers/Kandpal2022a.pdf:PDF},
  groups       = {Question Answering},
  url          = {https://proceedings.mlr.press/v162/kandpal22a/kandpal22a.pdf},
}

@Article{Mozes2021,
  author  = {Mozes, Maximilian and Kleinberg, Bennett},
  journal = {arXiv preprint arXiv:2103.09263},
  title   = {No intruder, no validity: Evaluation criteria for privacy-preserving text anonymization},
  year    = {2021},
  comment = {V Average paper. 

Tells us that Automatic text anonymization is a goal. 

H1: Can we effectively evaluate Automated Text Anonymization to ensure privacy preservation. 

H3: Use the TILD framework - Techncial Eval, Information Loss, and Deanonymization by a Human. 

In short - Not at all scalable because it relies on human review.},
  file    = {:/Volumes/GoogleDrive/My Drive/CMSC723/WhoIsYourDaddyAndWhatDoesHeDo/Papers/Mozes2021.pdf:PDF},
  groups  = {Question Answering},
  url     = {https://arxiv.org/pdf/2103.09263},
}

@Article{Song2019,
  author  = {Song, Congzheng and Shmatikov, Vitaly},
  journal = {arXiv preprint arXiv:1905.11742},
  title   = {Overlearning reveals sensitive attributes},
  year    = {2019},
  comment = {H1: How can we improve the privacy of models with censored data in the contedxt of overlearning. 

H3:

__FINDINGS__
- Yes, but it degrades accuracy too. So it may not be worth it. 
- It is possible to use de-censorship attacks.
- Stronger censorship doesn't really help. 


__COMMENTS:__

- Censorship happens at training time, Sanitization happens at inference time. 

- Censorship is the process of transofrming or splitting off or concealing sensitive attribtues in training data. It can be applied at single or multiple levels. It degrades prediciton accuracy.},
  file    = {:/Volumes/GoogleDrive/My Drive/CMSC723/WhoIsYourDaddyAndWhatDoesHeDo/Papers/Song2020.pdf:PDF},
  groups  = {Question Answering},
  url     = {https://arxiv.org/pdf/1905.11742},
}

@Article{FAIR2022A,
  author  = {Bakhtin, Anton and Brown, Noam and Dinan, Emily and Farina, Gabriele and Flaherty, Colin and Fried, Daniel and Goff, Andrew and Gray, Jonathan and Hu, Hengyuan and others},
  journal = {Science},
  title   = {Supplementary Materials to: Human-level play in the game of Diplomacy by combining language models with strategic reasoning},
  year    = {2022},
  comment = {__Supplemental Material Comments__

Pg15: They implement a 'lie score' to predict if a message is a lie. This isn't really useful for secret keeping, but might be pertinent for the satisficing. 

pg17. They observed that intent conditioning yields a 0.24 perplexity point improvement. This suggests that knowing what/why something is happening helps the machine to better reason about disclosure / non-disclosure. This links to the underlying idea of secret-keeping. 

pg27. Correspondance filtering "we took the message sender’s current turn action from the intents and computed the probability of that action under a dialogue-conditional action prediction model ... If this metric decreased below some designated threshold, we filtered the proposed message."

"most messages filtered by this method are not clear inconsistencies between the dialogue and the intent, “the bad stuff it knocks out ... is either bad [dialogue] or generic waffle” and “the end result is better [dialogue]." --> Could be an interesting comparison to secret-keeping. 

Pg28 - Value based filtering: "Effective messaging should promote favorable out- comes for the agent, but without leaking information that compromises the agent’s intended actions...effective communication in Diplomacy exercises discretion about the information revealed."
--> Good justification about needing to protect information. 

"sampling directly from the intent-conditioned dialogue model can suffer from such “information leaks.”

pg28 - How they estimated the method to score messages based on their estimated value impact: 
"We scored the strategic value of a candidate message by: (1) computing the piKL equi- librium policy of the recipient in the counterfactual situation where the candidate message is sent, and then (2) defining the candidate message’s “value” as the expected value of playing the agent’s intent against the recipient’s resultant counterfactual policy." --> highlights the realisation that not all information is created equal, and that there is reason to protect some information. 

pg 29. "To validate the effectiveness of value-based message filtering, we conducted both a quan- titative A/B test—comparing filtered messages against selected messages on the scenarios in which it would be applied—and a qualitative evaluation—testing specific scenarios identified by expert players. Evaluations for the A/B test were provided by human Diplomacy experts and demonstrated that, among message pairs where one was preferred over the other, the selected messages were preferred over filtered messages 62% of the time (p < 0.05)." --> Not a strong performance, but useful to note for satisficing. 

CONTENT MODERATION
pg30 "while the dialogue model is relatively unlikely to generate offensive content in an otherwise peaceful con- versation, this can be easily achieved through adversarial prompting (82)." --> Interrrogation mode. 

Pg 30: They implemented their content moderation with bad word filtering, combining a pre-built list with specifc terms mined for the domain. This is limited in that it is direct string matching, as well as that it needs fine-tuning for each domain. 

PG32 - Deciding when to speak: 
"At times, it may be advantageous to respond to a message (e.g. a proposal for a mutually beneficial set of moves) or to ignore it (e.g. when the other player is aggressive, or suggests an undesirable plan)." --> Useful for discussion of satisficing.},
  file    = {:/Volumes/GoogleDrive/My Drive/CMSC723/WhoIsYourDaddyAndWhatDoesHeDo/Papers/Bakhtin_SM2022.pdf:PDF},
  groups  = {Question Answering},
  url     = {https://www.science.org/doi/suppl/10.1126/science.ade9097/suppl_file/science.ade9097_sm.pdf},
}

@Book{Schuetze2008,
  author    = {Sch{\"u}tze, Hinrich and Manning, Christopher D and Raghavan, Prabhakar},
  publisher = {Cambridge University Press Cambridge},
  title     = {Introduction to information retrieval},
  year      = {2008},
  volume    = {39},
  comment   = {Information retrieval (IR) is finding material (usually documents) of an unstructured nature (usually text) that satisfies an information need from within large collections (usually stored on computers).},
  groups    = {Question Answering},
  url       = {https://nlp.stanford.edu/IR-book/information-retrieval-book.html},
}

@Article{Rogers2023,
  author    = {Rogers, Anna and Gardner, Matt and Augenstein, Isabelle},
  journal   = {ACM Computing Surveys},
  title     = {Qa dataset explosion: A taxonomy of nlp resources for question answering and reading comprehension},
  year      = {2023},
  number    = {10},
  pages     = {1--45},
  volume    = {55},
  comment   = {Question answering and reading comprehension have been particularly prolific in this regard, with over 80 new datasets appearing in the past two years.

The evolutionary forces behind the explosion were (a) a desire to push more away from linguistic structure prediction and towards a (still vague) notion of “natural language understanding” (NLU), which different research groups pursued in different directions, and (b) the increasing practical utility of commercial NLP systems incorporating questions answering technology

The most fundamental distinction in QA datasets is based on the communicative intent of the author of the question: was the person seeking information they did not have, or trying to test the knowledge of another person or machine. QA is more often associated with information-seeking questions and RC with probing questions. 

These two classes of questions also tend to differ in the kinds of reasoning they require (§8). Information-seeking questions are often ill-specified, full of “ambiguity and presupposition”

The outputs of the current text-based datasets can be categorized as extractive, multi-choice , categorical or freeform

The outputs of the current text-based datasets can be categorized as extractive (§4.2.1), multi-choice (§4.2.2), categorical (§4.2.3), or freeform:

By “evidence” or “context”, we mean whatever the system is supposed to “understand” or use to derive the answer from:
- Unstructured Text
- Semi-Structured Text
- Structured Knowledge
- Images
- Audio
- Video 
- Other Combinations. 

But the current QA/RC literature often discusses “skills”:
• Inference  “the process of moving from (possibly provisional) acceptance of some propositions, to acceptance of others”
• Retrieval  knowing where to look for the relevant information.
• Input interpretation & manipulation: correctly understanding the meaning of all the signs in the input, both linguistic and numeric, and performing any operations on them that are defined by the given
language/mathematical system (identifying coreferents, summing up etc.).
• World modeling: constructing a valid representation of the spatiotemporal and social aspects of the world described in the text, as well as positioning the text itself with respect to the reader and other texts. 
• Multi-step : performing chains of actions on any of the above dimensions.


__Answerability__. SQuAD 2.0 [234] popularized the distinction between questions that are answerable with the given context and those that are not. Arguably, the distinction is actually not binary, and at least two resources argue for a 3-point uncertainty scale. ReCO [299] offers boolean questions with “yes”, “no” and “maybe” answer options. QuAIL [249] distinguishes between full certainty (answerable with a given context), partial certainty (a confident guess can be made with a given context + some external common knowledge), and full uncertainty (no confident guess can be made even with external common knowledge). A more general definition of the unanswerable questions would be this: the questions that cannot be answered given all the information that the reader has access to.
This is different from invalid questions: the questions that a human would reject rather than attempt to answer. Table 3 shows examples of different kinds of violations: the answers that are impossible to retrieve, loaded questions, ill-formed questions, rhetorical questions, “useless” questions, and others. -->COMMENT: None of them ask if the answer SHOULD be known

__COMMENTS__
- 80 New datasets in the last year indicates a growth domain
- Reasons are (1) NLU and (2) Getting ready for commercial NLP (Risk of bringing into org)
- Types of QA are useful reference
- Answerability Questions.},
  file      = {:/Volumes/GoogleDrive/My Drive/CMSC723/WhoIsYourDaddyAndWhatDoesHeDo/Papers/Rogers2022.pdf:PDF},
  groups    = {Question Answering},
  publisher = {ACM New York, NY},
  url       = {https://dl.acm.org/doi/pdf/10.1145/3560260},
}

@Article{Hirschman2001,
  author    = {Hirschman, Lynette and Gaizauskas, Robert},
  journal   = {natural language engineering},
  title     = {Natural language question answering: the view from here},
  year      = {2001},
  number    = {4},
  pages     = {275--300},
  volume    = {7},
  comment   = {__QUOTES__

As users struggle to navigate the wealth of on-line information now available, the need for automated question answering systems becomes more urgent. We need systems that allow a user to ask a question in everyday language and receive an answer quickly and succinctly, with sufficient context to validate the answer.

To answer a question, a system must analyse the question, perhaps in the context of some ongoing interaction; it must find one or more answers by consulting on-line resources; and it must present the answer to the user in some appropriate form, perhaps associated with justification or supporting materials.

We can distinguish questions by answer type: factual answers vs. opinion vs. summary.

There are also different methodologies for constructing an answer: through ex- traction – cutting and pasting snippets from the original document(s) containing the answer – or via generation.

Despite that fact that such early interactive question answering systems used structured data as their knowledge source there is no requirement that they do so – text collections could be used instead, though of course real-time response is essential for such systems.

Information Retrieval (IR), which, following convention, we take to be the retrieval of relevant documents in response to a user query,

IR systems return documents, not answers, and users are left to extract answers from the documents themselves.

IR is, however, relevant to question answering for two reasons. First, IR techniques have been extended to return not just relevant documents, but relevant passages within documents. Second, the IR community has, over the years, developed an extremely thorough methodology for evaluation.

Information Extraction (IE) or, as it was initially known, message understanding. IE can be defined as the activity of filling predefined templates from natural language texts,

a general architecture for the QA task:
1. Question Analysis. 
2. Document Collection Preprocessing.
3. Candidate Document Selection.
4. Candidate Document Analysis. 
5. Answer Extraction. 
6. Response Generation.

__COMMENTS__

- They talk about the amount of info being too much back in 2001. Is worse now. 
- Gives general structure of a QA system
- Introduces Extraction vs generation. 
- IR is distinct from QA, it gets documents, not answers. IR is  a component of QA though.},
  file      = {:/Volumes/GoogleDrive/My Drive/CMSC723/WhoIsYourDaddyAndWhatDoesHeDo/Papers/Hirschman2001.pdf:PDF},
  groups    = {Question Answering, CMSC848E - ML for DM},
  publisher = {Cambridge University Press},
  url       = {https://www.cambridge.org/core/services/aop-cambridge-core/content/view/95EA883AFC7EB2B8EC050D3920F39DE2/S1351324901002807a.pdf/natural-language-question-answering-the-view-from-here.pdf},
}

@Article{Baradaran2022,
  author    = {Baradaran, Razieh and Ghiasi, Razieh and Amirkhani, Hossein},
  journal   = {Natural Language Engineering},
  title     = {A survey on machine reading comprehension systems},
  year      = {2022},
  number    = {6},
  pages     = {683--732},
  volume    = {28},
  comment   = {"The goal of this field is to develop systems for answering the questions regarding a given context"

the focus of research has changed in recent years from answer extraction to answer generation

"In order to assess the comprehension of a machine of a piece of natural language text, a set of questions about the text is given to the machine, and the responses of the machine are evaluated against the gold standard. "

"The main objective of QA systems is to answer the input questions, while the main goal of an MRC system, as the name suggests, is to demonstrate the machine’s ability in understanding natural languages through answering questions about specific context that it reads."

The only input to QA systems is the question, while the inputs to MRC systems entail the question and the corresponding context, which should be used to answer the question. As a result, sometimes MRC is referred to as QA from the text 

The main information source used to answer questions in MRC systems is natural lan- guage texts, while in QA systems, the structured and semi-structured data sources, such as knowledge-based ones, are commonly applied, in addition to the non-structured data like texts.

Most of the recent deep learning-based MRC systems have the following phases: embedding phase, reasoning phase, and prediction phase.

The inputs to an MRC system are question and passage texts. The passage is often referred to as context. Moreover, in some systems, the candidate answer list is part of the input.

Input questions can be grouped into three categories: factoid questions, non-factoid questions, and yes/no questions.

The input context can be a single passage or multiple passages. It is obvious that as the context gets longer, finding the answer becomes harder and more time-consuming.

The output of MRC systems can be classified into two categories: abstractive (generative) output and extractive (selective) output.

In the abstractive mode, the answer is not necessarily an exact span in the context and is gener- ated according to the question and context. This output type is especially suitable for non-factoid questions. n the extractive mode, the answer is a specific span of the context; This output type is appropriate for factoid questions.

Lots of good detail on evaluation metrics in here. 

Challenges and new trends in the MRC field:
- Out-of-domain distributions: Despite the high accuracy of the current MRC models on test samples from their training distribution, they are too fragile for out-of-domain distributed data.
- Multi-document MRC: One important challenge in the MRC task is the multi-hop reasoning, which is to infer the answer from multiple texts.
- Numerical reasoning:
No-answer questions: One of the new trends that makes the MRC systems more usable in real - world applications is to enable models to identify the questions which cannot be answered using the given context
- Non-factoid questions
- Low-resource language datasets and models:

__COMMENTS__
- Goal and definition of MRC useful for my paper
- Extractive vs generative definitions. 
- We will conflate MRC and QA
- System structure - Inputs: Questions (Factoid, non-factorid, yes/no) Text (single / multi document) outputs (Extracted Spans / Abstractive or Generative spans). 
- Numerical Reasoning is poor. 
- No - answer questions is an area of work (BUT nothing on SHOULD it answer?)},
  file      = {:/Volumes/GoogleDrive/My Drive/CMSC723/WhoIsYourDaddyAndWhatDoesHeDo/Papers/Baradaran2022.pdf:PDF},
  groups    = {Question Answering},
  publisher = {Cambridge University Press},
  url       = {https://www.cambridge.org/core/services/aop-cambridge-core/content/view/23FBCE30CB4325538E7DD08A7924315F/S1351324921000395a.pdf/a-survey-on-machine-reading-comprehension-systems.pdf},
}

@Article{Zamani2022,
  author  = {Zamani, Hamed and Trippas, Johanne R and Dalton, Jeff and Radlinski, Filip},
  journal = {arXiv preprint arXiv:2201.08808},
  title   = {Conversational information seeking},
  year    = {2022},
  file    = {:/Volumes/GoogleDrive/My Drive/CMSC723/WhoIsYourDaddyAndWhatDoesHeDo/Papers/Zamani2023.pdf:PDF},
  groups  = {Question Answering},
  url     = {https://arxiv.org/pdf/2201.08808.pdf},
}

@Article{Poliak2020,
  author  = {Poliak, Adam},
  journal = {arXiv preprint arXiv:2010.03061},
  title   = {A survey on recognizing textual entailment as an NLP evaluation},
  year    = {2020},
  file    = {:/Volumes/GoogleDrive/My Drive/CMSC723/WhoIsYourDaddyAndWhatDoesHeDo/Papers/Poliak2020.pdf:PDF},
  groups  = {Question Answering},
  url     = {https://aclanthology.org/2020.eval4nlp-1.10.pdf},
}

@InProceedings{Zou2020,
  author       = {Zou, Yeyun and Xie, Qiyu},
  booktitle    = {2020 2nd International Conference on Information Technology and Computer Application (ITCA)},
  title        = {A Survey on VQA: Datasets and Approaches},
  year         = {2020},
  organization = {IEEE},
  pages        = {289--297},
  file         = {:/Volumes/GoogleDrive/My Drive/CMSC723/WhoIsYourDaddyAndWhatDoesHeDo/Papers/Zou2020.pdf:PDF},
  groups       = {Question Answering},
  url          = {https://ieeexplore.ieee.org/iel7/9421350/9421928/09422035.pdf},
}

@InProceedings{Rogers2020,
  author    = {Rogers, Anna and Kovaleva, Olga and Downey, Matthew and Rumshisky, Anna},
  booktitle = {Proceedings of the AAAI conference on artificial intelligence},
  title     = {Getting closer to AI complete question answering: A set of prerequisite real tasks},
  year      = {2020},
  number    = {05},
  pages     = {8722--8731},
  volume    = {34},
  comment   = {We present QuAIL, the first RC dataset to combine text-based,
world knowledge and unanswerable questions, and to pro-
vide question type annotation that would enable diagnostics
of the reasoning strategies by a given QA system

Unanswerable questions (9) cannot be answered with
the information in the text, and the world knowledge does
not make one of the options more likely


Unanswerable questions were the most reliably detected
by the system that simply averaged word embeddings
(AvgCos). Recall that QuAIL texts are relatively long,
up to 350 words. It seems that the average of all words
in a long text becomes meaningless enough to be the
most similar to the average of words not found there (not
enough information to answer the question). This would
explain why AvgCos did much worse than chance in all
other question types.


__COMMENTS__
- Unanswerable Questions are most reliably detected - That is, it knows when it can't answer, but not when it shouldn't},
  file      = {:/Volumes/GoogleDrive/My Drive/CMSC723/WhoIsYourDaddyAndWhatDoesHeDo/Papers/Rogers2020.pdf:PDF},
  groups    = {Question Answering},
  url       = {https://ojs.aaai.org/index.php/AAAI/article/view/6398/6254},
}

@Article{Tramer2022,
  author  = {Tram{\`e}r, Florian and Shokri, Reza and Joaquin, Ayrton San and Le, Hoang and Jagielski, Matthew and Hong, Sanghyun and Carlini, Nicholas},
  journal = {arXiv preprint arXiv:2204.00032},
  title   = {Truth serum: Poisoning machine learning models to reveal their secrets},
  year    = {2022},
  comment = {__RQ__

__H1__
We want to improve the performance of Membership Inference, Data Extraction and Attribute Inference attacks on ML models. 

__H3__
- Develop the Active Inference Attacks by poisoning the training data. 


__Quotes__

A central tenet of computer security is that one cannot obtain any privacy without integrity .

demonstrating that an adversary can statically poison the training set to maximize the privacy leakage of individual training samples belonging to other parties. ... the ability to “write” into the training dataset can be exploited to “read” from other (private) entries in this dataset.

improve the performance of membership inference, attribute infer- ence and data extraction attacks on other training examples, by 1 to 2 orders-of-magnitude

by poisoning 64 sentences in the WikiText corpus, an adversary can extract a secret 6-digit “canary” [13] from a model trained on this corpus with a median of 230 guesses

our attacks reduce the average-case privacy of samples in a dataset to the worst-case privacy of data outliers

we aim to extract well-formatted secrets (e.g., credit card numbers, social security numbers, etc.) from a language model trained on an unlabeled text corpus.

COMMENTS__
- The scale of their attacks effectiveness
- Secrets they talk about are *any kind of private information*
- Able to extract data from LLMs},
  file    = {:/Volumes/GoogleDrive/My Drive/CMSC723/WhoIsYourDaddyAndWhatDoesHeDo/Papers/Tramer2022.pdf:PDF},
  groups  = {Question Answering},
  url     = {https://arxiv.org/pdf/2204.00032},
}

@Article{Wan2019,
  author  = {Wan, Mengting and Misra, Rishabh and Nakashole, Ndapa and McAuley, Julian},
  journal = {arXiv preprint arXiv:1905.13416},
  title   = {Fine-grained spoiler detection from large-scale review corpora},
  year    = {2019},
  comment = {Relies on Tagging of spoiler material. 

Create the Goodreads Dataset

Develop SpoilerNet to attempt to identify methods.},
  file    = {:/Volumes/GoogleDrive/My Drive/CMSC723/WhoIsYourDaddyAndWhatDoesHeDo/Papers/Wan2019.pdf:PDF},
  groups  = {Question Answering},
  url     = {https://arxiv.org/pdf/1905.13416},
}

@Article{Chang2021,
  author  = {Chang, Buru and Lee, Inggeol and Kim, Hyunjae and Kang, Jaewoo},
  journal = {arXiv preprint arXiv:2101.05972},
  title   = {" Killing Me" Is Not a Spoiler: Spoiler Detection Model using Graph Neural Networks with Dependency Relation-Aware Attention Mechanism},
  year    = {2021},
  comment = {__H1__: However, the attention-based SD models have
a lack of using dependency relations between con-
text words. Dependency relations are useful for
capturing the semantics of given sentences and de-
tecting spoilers.


__H3__: In this paper, we propose SDGNN, which is
a new Spoiler Detection model based on syntax-
aware Graph Neural Networks (GNNs)},
  file    = {:/Volumes/GoogleDrive/My Drive/CMSC723/WhoIsYourDaddyAndWhatDoesHeDo/Papers/Chang2021.pdf:PDF},
  groups  = {Question Answering},
  url     = {https://arxiv.org/pdf/2101.05972},
}

@Article{BoydGraber2013,
  author    = {Boyd-Graber, Jordan and Glasgow, Kimberly and Zajac, Jackie Sauter},
  journal   = {Proceedings of the American Society for Information Science and Technology},
  title     = {Spoiler alert: Machine learning approaches to detect social media posts with revelatory information},
  year      = {2013},
  number    = {1},
  pages     = {1--9},
  volume    = {50},
  comment   = {__H1__ How do we automate the detection of Spoilers

__H3__

__QUOTES__

But despite these rich conventions, spoilers remain a real prob-
lem in online conversations. In part, this is because social
motivations might run counter to the conventions of spoiler
alerts.

__COMMENTS__

Attempt to generalize to detect "spoiler language"},
  file      = {:/Volumes/GoogleDrive/My Drive/CMSC723/WhoIsYourDaddyAndWhatDoesHeDo/Papers/Boyd-Graber2014.pdf:PDF},
  groups    = {Question Answering},
  publisher = {Wiley Online Library},
  url       = {https://asistdl.onlinelibrary.wiley.com/doi/pdfdirect/10.1002/meet.14505001073},
}
