\section{Settings and Experiments}
\label{sec:experiments}
    
    In this section, we summarize different scenarios and their experimental settings used for training and validating the compression and latent space approximation of histopathology images, and establish that the compression ratio our pipeline achieves is state of the art.
    
    % ~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>. Subsection
    \subsection{Training Settings}
    \label{subsec:training}
    
        For hyperparamter tuning, the effect of normalization of data on the quality of outcome was tested (based on visual inspection), and it was concluded that normalization is necessary for acceptable results (Fig. \ref{fig:2}-a). Since all datasets are normalized using the same procedure, the validation can be perceived as a metric to compare the performance of different models on different datasets. All experiments are conducted using and early stopping on validation loss with \texttt{patience} = 5 unless mentioned otherwise.
        
        As illustrated in Fig. \ref{fig:2}-b, higher batch sizes result into faster objective minimization, but lower batch sizes eventually results in better validation loss due to a higher regularization effect (\cite{Goodfellow-et-al-2016}). To take the middle ground, all experiments were conducted using a batch size of 128 unless mentioned otherwise. Also, as expected, higher latent dimensions resulted into a better performance.
        
        The model is developed with PyTorch Lightning API. All experiments were conducted using the DDP parallelization strategy on an NVIDIA DGX A100 with 8, 80 GB A100 GPUs, and a learning rate of $10^{-4}$. 
    
    \subsection{Compression Experiments}
    \label{subsec:compression}
    
        Experimental results demonstrate a better performance of our compression model on histopathology slides than is achieved on images of every day objects datasets such as in CIFAR10 (\cite{krizhevsky2009learning}). We first hypothesised that this diffeence is rooted in the difference of entropy between the average image in these two datasets. Entropy is a way of calculating the context information of a datapoint. We reasoned that low entropy images are more compressible  han high entropy ones. Therefore, we divided the CIFAR10 dataset by entropy with a high entropy fold (average entropy = 7.623) and a low entropy fold (average entropy = 7.039), each containing 30,000 images, and ran two experiments to see which one is more compressible when fed through our model. For both experiments, batch size was set to 256, latent dimension was set to 16, and the input images were of dimension $ 32 \times 32 \times 3 $. The results are shown in Fig. \ref{fig:3}. The final validation loss for low entropy and high entropy datasets are 0.601 and 0.570, respectively contradicted our original hypothesis. We ran the same experiment on the same number of patches sampled for the breast cancer slides, and although having lower entropy, it showed a better performance (numbers are reported in Fig\ref{fig:3}). Hence, we concluded that entropy is not a reliable factor to explain the SOTA performance of our VAE compression pipeline on cancer imaging data.
    
        We then hypothesized that color distribution may be a contributing factor. H\&E slides are limited to the colors present in tissue, while CIFAR10 images have a more diverse color distribution. For this hypothesis, we randomly chose 30,000 images from CIFAR10 dataset. Using the same settings, we ran one experiment on the sampled images and another on the same images but with grayscale transformation to eliminate olor diversity. The final validation loss for colored dataset is 0.599 and for the grayscale dataset is 0.525 (Fig. \ref{fig:2}-a). The lower validation loss indicates that less color content can be attributed to a better comprehensibility.
    
    
    \subsection{Validation Experiments}
    \label{subsec:val_exp}
    
        In order to examine whether the latent space preserves necessary information for downstream clinical tasks, we tested the accuracy of original slide images against regenerated slide images on CLAM (\cite{lu2021data}), the state-of-the-art model in lung cancer classification from H\&E slides. We first used CLAM on the original test set for the two classification tasks, i.e. "tumor vs. normal" and "sub-typing" between Lung Adenocarcinomas (LUAD) and Squamous Cell Carcinomas (LUSC). Then, we created a reconstructed (post compression) version of the test set using our inference pipeline (Fig. 1-b). This reconstructed test set was then run through the same classification problems as the original images. We then calculated the percentage of the images that had the same label for both original and reconstructed images over all test images as a measure of performance and observed that our compression did not decrease performance on clinical application tasks.
        
         To test the clinical information preservation of the latent space, we chose a model trained on lung tissues with the highest compression ratio (1:512) in our pipeline (Sec. \ref{subsec:val_exp}). This compression ratio is twice as high as the best models introduced in the literature (\cite{tellez2019neural, chen_fast_2022}). For the "tumor vs. normal" task, the reconstructed images did not show loss of performance, however, this level of compression made it difficult for lung cancer sub-typing model to perform as before.
        
        We used 900 images from the GDC TCGA (Sec. \ref{sec:availability}) including 450 samples for each LUAD and LUSC sub-types for training and 100 images (50 LUAD, 50 LUSC) for testing. The CLAM model has 10-fold validated pre-trained weights; thus, we calculated the performance in a 10-fold setting, too.
        
        \begin{figure}[t]
            \centering
            \begin{minipage}[b]{0.48\linewidth}
                \centering
                \centerline{\includegraphics[width=1\linewidth]{figures/fig4a-new.png}}
            \end{minipage}
            % \hfill
            \begin{minipage}[b]{0.5\linewidth}
                \centering
                \centerline{\includegraphics[width=\linewidth]{figures/fig4b.png}}
            \end{minipage}
            \caption{\textbf{(a)} The reconstruction results for breast cancer tissues at 5 different compression ratios. \textbf{(b)} UMAP plot generated on 4 different tissue types with a compression ratio of 1:64.}
            \label{fig:4}
        \end{figure}
        
        
    \subsection{UMAP Experiments}
    \label{subsec:umap_exp}
    
        To show that the latent space preserves important and clinically relevant information, 4 models were trained with a latent space of size 64 on different tissue types (brain, breast, bronchus and lung, and colon) on 20,000 patches of size $64 \times 64$ pixels, and tested them on 10,000 patches from their respective tissue type. We then ran the latent vectors of the test patches through the UMAP algorithm using the "cosine" distance as the similarity metric. The results are shown in Fig.\ref{fig:4}. 
