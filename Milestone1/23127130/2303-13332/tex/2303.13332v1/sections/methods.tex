\section{Methods}
\label{sec:methods}
    
    
    % ~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>. Subsection
    \subsection{Dataset}
    \label{subsec:dataset}
    
    The dataset we used for this study is publicly available at the NCI GDC data portal (Sec. \ref{sec:availability}). These are real samples from cancer patients in the US, and all samples contain cancerous cells. For this study, We downloaded 20\% of the available \texttt{.svs} samples for primary sites: "Brain", "Breast", "Bronchus and Lung", and "Colon" (647, 551, 580, and 267 images, respectively).
    
    
    
    % ~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>. Subsection
    \subsection{Latent Variables and VAE}
    \label{subsec:vae}
    
        For an observation $\boldsymbol{x^{(i)}}$, its latent vector of variables is assumed to be an unobserved random variable $\boldsymbol{z^{(i)}}$ sampled from a lower dimension space (latent space) that is involved in producing $\boldsymbol{x^{i}}$ in a random process \cite{kingma_auto-encoding_2014}. For a particular task, it is assumed that using latent variables removes non-informative dimensionality and is suitable for downstream machine learning tasks. Since the latent space is unobserved, latent variables should be somehow inferred. Autoencoders and Variational Auto Encoders are two very effective methods for inferring these latent variables and encoding very high-dimensional data into highly compressed latent space with minimal loss of information. VAEs, as opposed to regular Auto Encoders, try to find a distribution for each latent variable, rather than single point estimate, resulting in a regularized latent space with generative capability.
        
        VAEs are comprised of two parts: an encoder and a decoder (Fig.\ref{fig:1}-a). If the latent variable $\boldsymbol{z^{(i)}}$ and data point $\boldsymbol{x^{(i)}}$ are sampled from parametric probability distributions $p_{\boldsymbol{\theta}}(\boldsymbol{z})$ and $p_{\boldsymbol{\theta}}(\boldsymbol{x} | \boldsymbol{z})$ for some parameter $\boldsymbol{\theta}$, then the encoder will try to estimate the approximate posterior $q_{\boldsymbol{\phi}}(\boldsymbol{z} | \boldsymbol{x})$ with variational parameter $\boldsymbol{\phi}$. The decoder tries to find the likelihood $p_{\boldsymbol{\theta}}(\boldsymbol{x} | \boldsymbol{z})$. The model can be trained by minimizing the loss introduced in Eq.\ref{eq:1} over all observations (\cite{kingma_auto-encoding_2014}). The first term of the loss is called the Kullbackâ€“Leibler (KL) divergence term, which is introduced to ensure that the variational approximation is as informative as the generative true posterior. The second term is reconstruction loss, which makes sure the generated output from the learned latent distribution is close to the original input. In  our experiments, we used a weighted loss with a KL term coefficient of $0.1$.
        
        \begin{equation}
        \begin{aligned}
            \mathcal{L} \left(\boldsymbol\theta, \boldsymbol\phi; \boldsymbol{x^{(i)}} \right) =& -D_{KL} \left( q_{\boldsymbol\phi} \left( \boldsymbol{z}|\boldsymbol{x^{(i)}} || p_{\boldsymbol\theta} \left( \boldsymbol{z} \right)\right) \right)\\
            & + \mathbb{E}_{q_{\boldsymbol\phi} \left( \boldsymbol{z}|\boldsymbol{x^{(i)}} \right)} \left[ \log{p_\theta \left( \boldsymbol{x^{(i)}} | \boldsymbol{z} \right)} \right]
        \end{aligned}
        \label{eq:1}
        \end{equation}
    
    
    % ~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>. Subsection
    \subsection{Training and Inference Pipelines}
    \label{subsec:pipelines}
    
        As illustrated in Fig.\ref{fig:1}, we use two pipelines for training and inference. For the training phase (Fig.\ref{fig:1}-a), a selected number of patches from whole slide images (WSIs) in the training and validation set are randomly sampled. A white space filter is utilized to ensure that these patches are not blank, and that they do not overlap. The mean and standard deviation of all patches sampled from the training set is calculated and all patches are normalized using the standard score method with these values (not shown in Fig.\ref{fig:1}). The inverse transformations are also stored to be applied to the outputs of the model.
        
       Our model assumes a Gaussian prior and a Gaussian approximate posterior. The encoder learns the parameters of the Gaussian prior and the decoder uses a re-parameterized sample from this prior and tries to reconstruct the input. Both encoder and decoder use ResNet18 (\cite{he2016deep}) architectures. A ResNet50 archiecture (not-shown) provides similar performance; ResNet18 was selected to keep the number of model parameters as small as possible for future downstream deployment in the clinic. 
        
        During inference (Fig.\ref{fig:1}-b), to perform a  a compression/decompression task, the test image is fully tiled. Each patch is then fed into the trained networks and stitched together once all patches are reconstructed. However, for the UMAP experiment, the same patch sampling algorithm used for training is used to generate random patches to be fed to the model. For the reconstruction task, we want a whole image, but for the UMAP plot, sample latent variables are enough.
        
    % ~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>.~>. Subsection
    \subsection{Dimension Reduction and UMAP}
    \label{subsec:umap}
    
        Uniform Manifold Approximation and Projection (\cite{mcinnes2018umap}) is a manifold based dimensionality reduction algorithm used for visualizing and clustering high dimensional datasets. This algorithm tries to reduce the points in a manner that the distance between resulting points would be still meaningful. UMAP is utilized to visualize and demonstrate that not only do the latent vectors learned by our pipeline provide visually accurate decompressed images, but also they contain relevant clinical information from different cancer types (Fig.\ref{fig:4}). UMAP can use many metrics for distance calculation; "cosine similarity" was selected for its ability to capture correlation features.
        
        \begin{figure}[t]
            \centering
            \centerline{\includegraphics[width=\linewidth]{figures/fig3-new.pdf}}
            \caption{Effect of dataset entropy and color content on peroformance. All hyper-parameters are the same for all 5 models.}
            \label{fig:3}
        \end{figure}