@misc{mcinnes2020umap,
  title = {{UMAP}: {Uniform} {Manifold} {Approximation} and {Projection} for {Dimension} {Reduction}},
  author = {McInnes, Leland and Healy, John and Melville, James},
  year = {2020},
  month = sep,
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1802.03426},
  doi = {10.48550/arXiv.1802.03426},
  note = {arXiv:1802.03426 [cs, stat]},
  keywords = {Computer Science - Computational Geometry, Computer Science - Machine Learning, Statistics - Machine Learning},
  abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
  urldate = {2023-03-23},
  annote = {Comment: Reference implementation available at http://github.com/lmcinnes/umap}
}

@article{amir,
  title={Speckle noise reduction in optical coherence tomography images based on edge-sensitive cGAN},
  author={Ma, Yuhui and Chen, Xinjian and Zhu, Weifang and Cheng, Xuena and Xiang, Dehui and Shi, Fei},
  journal={Biomedical optics express},
  volume={9},
  number={11},
  pages={5129--5146},
  year={2018},
  publisher={Optica Publishing Group}
}

@inproceedings{kusner_grammar_2017,
	title = {Grammar {Variational} {Autoencoder}},
	url = {https://proceedings.mlr.press/v70/kusner17a.html},
	abstract = {Deep generative models have been wildly successful at learning coherent latent representations for continuous data such as natural images, artwork, and audio. However, generative modeling of discrete data such as arithmetic expressions and molecular structures still poses significant challenges. Crucially, state-of-the-art methods often produce outputs that are not valid. We make the key observation that frequently, discrete data can be represented as a parse tree from a context-free grammar. We propose a variational autoencoder which directly encodes from and decodes to these parse trees, ensuring the generated outputs are always syntactically valid. Surprisingly, we show that not only does our model more often generate valid outputs, it also learns a more coherent latent space in which nearby points decode to similar discrete outputs. We demonstrate the effectiveness of our learned models by showing their improved performance in Bayesian optimization for symbolic regression and molecule generation.},
	language = {en},
	urldate = {2022-10-15},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kusner, Matt J. and Paige, Brooks and Hernández-Lobato, José Miguel},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {1945--1954},
	file = {Full Text PDF:/Users/axh5735/Zotero/storage/WM7GETTK/Kusner et al. - 2017 - Grammar Variational Autoencoder.pdf:application/pdf;Supplementary PDF:/Users/axh5735/Zotero/storage/5SNMXMVR/Kusner et al. - 2017 - Grammar Variational Autoencoder.pdf:application/pdf},
}

@inproceedings{vahdat_nvae_2020,
	title = {{NVAE}: {A} {Deep} {Hierarchical} {Variational} {Autoencoder}},
	volume = {33},
	shorttitle = {{NVAE}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/e3b21256183cf7c2c7a66be163579d37-Abstract.html},
	abstract = {Normalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256x256 pixels. The source code is publicly available.},
	urldate = {2022-10-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vahdat, Arash and Kautz, Jan},
	year = {2020},
	pages = {19667--19679},
	file = {Full Text PDF:/Users/axh5735/Zotero/storage/SLHV38IJ/Vahdat and Kautz - 2020 - NVAE A Deep Hierarchical Variational Autoencoder.pdf:application/pdf},
}


@article{jamil2022learning,
  title={Learning-Driven Lossy Image Compression; A Comprehensive Survey},
  author={Jamil, Sonain and Piran, Md and others},
  journal={arXiv preprint arXiv:2201.09240},
  year={2022}
}

@article{lombardo2019deep,
  title={Deep generative video compression},
  author={Lombardo, Salvator and Han, Jun and others},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{yilmaz2021self,
  title={Self-Organized Variational Autoencoders (Self-Vae) For Learned Image Compression},
  author={Y{\'\i}lmaz, M Ak{\'\i}n and Keles{\c{s}}, Onur and others},
  booktitle={2021 IEEE International Conference on Image Processing (ICIP)},
  pages={3732--3736},
  year={2021},
  organization={IEEE}
}

@article{kwonautoencoder,
  title={Autoencoder Image Compression Algorithm for Reduction of Resource Requirements},
  author={Kwon, Young Joon Fred and BioDesign, MSE Sinai and Toussie, Danielle and Reina, G Anthony and Tang, Ping Tak Peter and Doshi, Amish H and Oermann, Eric K and Costa, Anthony B and BioDesign, Sinai}
}



@article{tellez2019neural,
  title={Neural image compression for gigapixel histopathology image analysis},
  author={Tellez, David and Litjens, Geert and others},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={43},
  number={2},
  pages={567--578},
  year={2019},
  publisher={IEEE}
}

% Mohammad Citations

@misc{kingma_auto-encoding_2014,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	doi = {10.48550/arXiv.1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2022-10-20},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Welling, Max},
	month = may,
	year = {2014},
	note = {arXiv:1312.6114 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\NASRS\\Zotero\\storage\\I5YDYZEH\\Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\NASRS\\Zotero\\storage\\MCJ7J7YN\\1312.html:text/html},
}

@inproceedings{karlova2021molecular,
  title={Molecular fingerprint vae},
  author={Karlova, Andrea and Dehaen, Wim and Svozil, Daniel},
  booktitle={ICML Workshop on Computational Biology},
  year={2021}
}

@article{way_extracting_2018,
	title = {Extracting a biologically relevant latent space from cancer transcriptomes with variational autoencoders},
	volume = {23},
	issn = {2335-6936},
	abstract = {The Cancer Genome Atlas (TCGA) has profiled over 10,000 tumors across 33 different cancer-types for many genomic features, including gene expression levels. Gene expression measurements capture substantial information about the state of each tumor. Certain classes of deep neural network models are capable of learning a meaningful latent space. Such a latent space could be used to explore and generate hypothetical gene expression profiles under various types of molecular and genetic perturbation. For example, one might wish to use such a model to predict a tumor's response to specific therapies or to characterize complex gene expression activations existing in differential proportions in different tumors. Variational autoencoders (VAEs) are a deep neural network approach capable of generating meaningful latent spaces for image and text data. In this work, we sought to determine the extent to which a VAE can be trained to model cancer gene expression, and whether or not such a VAE would capture biologically-relevant features. In the following report, we introduce a VAE trained on TCGA pan-cancer RNA-seq data, identify specific patterns in the VAE encoded features, and discuss potential merits of the approach. We name our method "Tybalt" after an instigative, cat-like character who sets a cascading chain of events in motion in Shakespeare's "Romeo and Juliet". From a systems biology perspective, Tybalt could one day aid in cancer stratification or predict specific activated expression patterns that would result from genetic changes or treatment effects.},
	language = {eng},
	journal = {Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing},
	author = {Way, Gregory P. and Greene, Casey S.},
	year = {2018},
	pmid = {29218871},
	pmcid = {PMC5728678},
	keywords = {Algorithms, Atlases as Topic, Computational Biology, Female, Gene Expression Profiling, Humans, Male, Models, Genetic, Neoplasms, Neural Networks, Computer, Nonlinear Dynamics, Ovarian Neoplasms, Systems Biology, Transcriptome, Unsupervised Machine Learning},
	pages = {80--91},
}

@article{ternes2022multi,
  title={A multi-encoder variational autoencoder controls multiple transformational features in single-cell image analysis},
  author={Ternes, Luke and Dane, Mark and Gross, Sean and Labrie, Marilyne and Mills, Gordon and Gray, Joe and Heiser, Laura and Chang, Young Hwan},
  journal={Communications biology},
  volume={5},
  number={1},
  pages={1--10},
  year={2022},
  publisher={Nature Publishing Group}
}



%Michael sources
@article{hu2021learning,
  title={Learning end-to-end lossy image compression: A benchmark},
  author={Hu, Yueyu and Yang, Wenhan and others},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2021},
  publisher={IEEE}
}
@article{kalra2020yottixel,
  title={Yottixel--an image search engine for large archives of histopathology whole slide images},
  author={Kalra, Shivam and Tizhoosh, Hamid R and others},
  journal={Medical Image Analysis},
  volume={65},
  pages={101757},
  year={2020},
  publisher={Elsevier}
}
@article{konsti2012effect,
  title={Effect of image compression and scaling on automated scoring of immunohistochemical stainings and segmentation of tumor epithelium},
  author={Konsti, Juho and Lundin, Mikael and others},
  journal={Diagnostic Pathology},
  volume={7},
  number={1},
  pages={1--9},
  year={2012},
  publisher={Springer}
}    
@article{gurcan2009histopathological,
  title={Histopathological image analysis: A review},
  author={Gurcan, Metin N and Boucheron, Laura E and others},
  journal={IEEE reviews in biomedical engineering},
  volume={2},
  pages={147--171},
  year={2009},
  publisher={IEEE}
}

@article{he2012histology,
  title={Histology image analysis for carcinoma detection and grading},
  author={He, Lei and Long, L Rodney and others},
  journal={Computer methods and programs in biomedicine},
  volume={107},
  number={3},
  pages={538--556},
  year={2012},
  publisher={Elsevier}
}    

@article{gutman2013cancer,
  title={Cancer Digital Slide Archive: an informatics resource to support integrated in silico analysis of TCGA pathology data},
  author={Gutman, David A and Cobb, Jake and Somanna, Dhananjaya and Park, Yuna and Wang, Fusheng and Kurc, Tahsin and Saltz, Joel H and Brat, Daniel J and Cooper, Lee AD and Kong, Jun},
  journal={Journal of the American Medical Informatics Association},
  volume={20},
  number={6},
  pages={1091--1098},
  year={2013},
  publisher={BMJ Publishing Group BMA House, Tavistock Square, London, WC1H 9JR}
}   

@article{soliman2006neural,
  title={A neural networks approach to image data compression},
  author={Soliman, Hamdy S and Omari, Mohammed},
  journal={Applied Soft Computing},
  volume={6},
  number={3},
  pages={258--271},
  year={2006},
  publisher={Elsevier}
}

@article{krupinski2012compressing,
  title={Compressing pathology whole-slide images using a human and model observer evaluation},
  author={Krupinski, Elizabeth A and Johnson, Jeffrey P and others},
  journal={Journal of pathology informatics},
  volume={3},
  number={1},
  pages={17},
  year={2012},
  publisher={Elsevier}
}

@article{niazi2019pathological,
  title={Pathological image compression for big data image analysis: Application to hotspot detection in breast cancer},
  author={Niazi, M Khalid Khan and Lin, Yuzhang and others},
  journal={Artificial intelligence in medicine},
  volume={95},
  pages={82--87},
  year={2019},
  publisher={Elsevier}
}


@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and others},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{mcinnes2018umap,
  title={Umap: Uniform manifold approximation and projection for dimension reduction},
  author={McInnes, Leland and Healy, John and Melville, James},
  journal={arXiv preprint arXiv:1802.03426},
  year={2018}
}


@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}


@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}




@article{lu2021data,
  title={Data-efficient and weakly supervised computational pathology on whole-slide images},
  author={Lu, Ming Y and Williamson, Drew FK and others},
  journal={Nature Biomedical Engineering},
  volume={5},
  number={6},
  pages={555--570},
  year={2021},
  publisher={Nature Publishing Group}
}



@article{chen_fast_2022,
	title = {Fast and scalable search of whole-slide images via self-supervised deep learning},
	copyright = {2022 The Author(s)},
	issn = {2157-846X},
	url = {https://www.nature.com/articles/s41551-022-00929-8},
	doi = {10.1038/s41551-022-00929-8},
	abstract = {The adoption of digital pathology has enabled the curation of large repositories of gigapixel whole-slide images (WSIs). Computationally identifying WSIs with similar morphologic features within large repositories without requiring supervised training can have significant applications. However, the retrieval speeds of algorithms for searching similar WSIs often scale with the repository size, which limits their clinical and research potential. Here we show that self-supervised deep learning can be leveraged to search for and retrieve WSIs at speeds that are independent of repository size. The algorithm, which we named SISH (for self-supervised image search for histology) and provide as an open-source package, requires only slide-level annotations for training, encodes WSIs into meaningful discrete latent representations and leverages a tree data structure for fast searching followed by an uncertainty-based ranking algorithm for WSI retrieval. We evaluated SISH on multiple tasks (including retrieval tasks based on tissue-patch queries) and on datasets spanning over 22,000 patient cases and 56 disease subtypes. SISH can also be used to aid the diagnosis of rare cancer types for which the number of available WSIs is often insufficient to train supervised deep-learning models. A self-supervised deep-learning algorithm searches for and retrieves gigapixel whole-slide images at speeds that are independent of the size of the image repository},
	language = {en},
	urldate = {2022-10-20},
	journal = {Nature Biomedical Engineering},
	author = {Chen, Chengkuan and Lu, Ming Y. and others},
	month = oct,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Bioinformatics, Biomedical engineering, Medical imaging, Pathology},
	pages = {1--15},
}


@article{roetzer-pejrimovsky_digital_2022,
	title = {The {Digital} {Brain} {Tumour} {Atlas}, an open histopathology resource},
	volume = {9},
	issn = {2052-4463},
	url = {https://doi.org/10.1038/s41597-022-01157-0},
	doi = {10.1038/s41597-022-01157-0},
	abstract = {Currently, approximately 150 different brain tumour types are defined by the WHO. Recent endeavours to exploit machine learning and deep learning methods for supporting more precise diagnostics based on the histological tumour appearance have been hampered by the relative paucity of accessible digital histopathological datasets. While freely available datasets are relatively common in many medical specialties such as radiology and genomic medicine, there is still an unmet need regarding histopathological data. Thus, we digitized a significant portion of a large dedicated brain tumour bank based at the Division of Neuropathology and Neurochemistry of the Medical University of Vienna, covering brain tumour cases from 1995–2019. A total of 3,115 slides of 126 brain tumour types (including 47 control tissue slides) have been scanned. Additionally, complementary clinical annotations have been collected for each case. In the present manuscript, we thoroughly discuss this unique dataset and make it publicly available for potential use cases in machine learning and digital image analysis, teaching and as a reference for external validation.},
	number = {1},
	journal = {Scientific Data},
	author = {Roetzer-Pejrimovsky, Thomas and Moser, Anna-Christina and others},
	month = feb,
	year = {2022},
	pages = {55},
}