% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt, twocolumn, letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Spatio-Temporal AU Relational Graph Representation Learning For Facial Action Units Detection}


% \author{Zihan Wang\\
% College of Computer Science and Software Engineering, Shenzhen University\\
% {\tt\small 2018112013@email.szu.edu.cn}
% \and
% Siyang Song\\
% School of Computing and Mathematical Sciences, University of Leicester\\
% Department of Computer Science and Technology, University of Cambridge\\
% First line of institution address\\
% {ss1535@leicester.ac.uk}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Cheng Luo\\
% College of Computer Science and Software Engineering, Shenzhen University\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% \and
% Weicheng Xie\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% \and
% Linlin Shen\\
% College of Computer Science and Software Engineering, Shenzhen University\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }}


\author{Zihan Wang$^{1,2}$,~
Siyang Song$^{3,4}$,~
Cheng Luo$^{1,2}$,~
Yuzhi Zhou$^{1,2}$,~
Shiling Wu$^{1,2}$,~\\
Weicheng Xie$^{1,2}$ ~ and ~  
Linlin Shen$^{1,2}$\thanks{Corresponding Author}\\
\textsuperscript{\rm1}Computer Vision Institute, School of Computer Science $\&$ Software Engineering, Shenzhen University\\
%\textsuperscript{\rm2}Shenzhen Institute of Artificial Intelligence $\&$ Robotics for Society\\
\textsuperscript{\rm2}National Engineering Laboratory for Big Data System Computing Technology, Shenzhen University \\
\textsuperscript{\rm3}School of Computing and Mathematical Sciences, University of Leicester\\
\textsuperscript{\rm4}Department of Computer Science and Technology, University of Cambridge\\
{\tt\small \{2018112013, luocheng2020, 2200271004, 2110276070\}@email.szu.edu.cn,}\\
{\tt\small \{wcxie, llshen\}@szu.edu.cn,}
{\tt\small ss1535@leicester.ac.uk}
}


\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
% With the higher requirements for the human affective behavior understanding ability of machine, affective behavior analysis have attracted more and more attention. 

This paper presents our Facial Action Units (AUs) recognition submission to the fifth Affective Behavior Analysis in-the-wild Competition (ABAW). Our approach consists of three main modules: (i) a pre-trained facial representation encoder which produce a strong facial representation from each input face image in the input sequence; (ii) an AU-specific feature generator that specifically learns a set of AU features from each facial representation; and (iii) a spatio-temporal graph learning module that constructs a spatio-temporal graph representation. This graph representation describes AUs contained in all frames and predicts the occurrence of each AU based on both the modeled spatial information within the corresponding face and the learned temporal dynamics among frames. The experimental results show that our approach outperformed the baseline and the spatio-temporal graph representation learning allows the model to generate the best results among all ablated systems.


\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}


%%% 第一段：研究的定义和背景：介绍一下AU是什么，AU识别的重要性
% In order to achieve artificial intelligence, human affective behavior analysis is an inevitable challenge. Action Units(AU), as a description of facial muscle movements, plays a significant role in affective behavior analysis. AU is defined according to the anatomical characteristics of the face and the muscle movement of the facial expression based on the Facial Action Coding System(FACS)\cite{ekman1978facial}. The different human expression can be deconstructing into the specific multiple Action Units. Facial AU recognition is a multi-label classification task and has received increasing attention as AU is a more objective and comprehensive representation of facial expression.

Human Facial Action Units (AUs), as a widely-used description for facial muscle movements, play a significant role in human behavior understanding \cite{song2020spectral,kollias2019expression,jaiswal2019automatic,song2018human}. Facial AUs are annotated according to the anatomical characteristics of multiple facial muscle movement based on Facial Action Coding System (FACS) \cite{ekman1978facial}. Compared to categorical facial expressions, AUs are more objective and comprehensive representation of facial expressions, and thus drew increasing attentions in recent years \cite{martinez2017automatic}. However, AUs recognition is a challenging multi-label classification task as AUs are subtle movement of facial muscles, and different facial muscles have different ranges of movement, which are affected by various person-specific factors (e.g., gender and age) as well as contexts (e.g., background, illumination ,and occlusion).
% etc. especially in uncontrolled environments.



%%% 第二段：当前研究方法总结，并提出当前研究方法的不足（这个不足要与本文相关，比如本文要用图，为什么之前不用图的不好）

% The 5th ABAW Competition\cite{kollias2023abaw} provide Action Unit Detection Challenge and the Aff-Wild2 \cite{kollias2019deep,kollias2019expression,kollias2019face,kollias2020analysing,kollias2021affect,kollias2021distribution,kollias2021analysing,kollias2022abaw,zafeiriou2017aff} database will be used. There are several papers in the previous ABAW Competitions, but few work pay attention to the relationships among different AU, they ignore that multiple AU co-occur simultaneously when people generate a certain expression, different AU have underlying relationships that can be represented by graph structure. Also, data imbalance is an issue in Aff-Wild2 database, causing overfitting to AU labels and identities with more samples.


The Action Unit Detection Challenge of the 5th ABAW Competition\cite{kollias2023abaw} is based on the Aff-Wild2 \cite{kollias2019deep,kollias2019expression,kollias2019face,kollias2020analysing,kollias2021affect,kollias2021distribution,kollias2021analysing,kollias2022abaw,zafeiriou2017aff} database. Some of the AU recognition approaches in the previous ABAW Competitions \cite{kollias2022abaw,kollias2020analysing,kollias2021analysing} fuse multi-modal features including video and audio to provide multi-dimensional information to predict AUs' occurrence \cite{zhang2022transformer,jin2021multi,jeong2022multi,wang2022action}. Meanwhile, other studies found that AU recognition performance can be benefited from multi-task learning\cite{deng2022estimating,jeong2022multi,zhang2021prior,nguyen2023affective}, i.e., jointly conducting expression recognition or valence/arousal estimation provides helpful cues for AU recognition. Moreover, temporal models such as GRU \cite{dey2017gate} or Transformer \cite{vaswani2017attention} are also introduced to model temporal dynamics among consecutive frames \cite{nguyen2022ensemble,wang2022action}. While AUs' activation status in each facial display are highly correlated, their relationships provide crucial cues for their occurrence recognition. Meanwhile, the annotations of AUs in the Aff-Wild2 database exhibit a notable imbalance (e.g.,samples of AU7,10,25 are far more than that of AU15,23,24,26 and some AUs only appear on certain identities.), which can result in the training of a biased model that are predisposed to learn AU patterns that have been annotated more frequently in the training set. However, to the best of our knowledge, there is no previous study can jointly address both problems.


% But few work in ABAW Competitions pay attention to the relationships among different AUs, ignoring that multiple AU co-occur simultaneously when people generate a certain expression. Also, data imbalance is an issue in Aff-Wild2 database, causing overfitting to AU labels and identities with more samples.


%%% 第三段：可以提一下IJCAI论文方法的优势可以解决上述问题，但是仍然具有缺陷（比如未考虑时序信息）
Recent studies show that the graph representation is powerful for modelling the underlying relationship among AUs \cite{luo2022learning,song2022gratis,song2021uncertain}. In particular, task-specific multi-dimensional edge features shows strong capability in explicitly describing the relationship between each pair of AUs. 
To overcome the overfitting problem, Ma et al.\cite{ma2022facial}introduces a robust facial representation model MAE-Face for AU analysis. But both of them ignore the temporal information. Considering that the Aff-Wild2 database consists of videos, there is a certain relationship between different frames in a video and the adjacent frames are relatively similar. Therefore, AU recognition benefits from temporal information.



% Luo et al. \cite{luo2022learning,song2022gratis} proposes an AU relationship modeling approach that deep learns a unique graph to explicitly describe the relationship between each pair of AU of the target facial display. 

\begin{figure*}[ht]
    \centering
    \includegraphics[scale=0.52]{overview.png}
    \caption{The pipeline of the proposed Spatio-Temporal AU Relational Graph Representation Learning approach}
    \label{fig:overview}
\end{figure*}


%%% 第四段：介绍本文方法的模块，创新性及优势
In this paper, we propose a spatio-temporal facial AU graph representation learning framework for the AU recognition challenge at the 5-th ABAW competition. Our framework starts with pre-training a masked autoencoder (MAE) \cite{he2022masked} from a set of face databases. This way, the pre-trained MAE can produce a strong facial representation from each input face image. Based on the facial representation, a AU-specific Feature Generator learns specific representation for each AU, which is considered as the node feature in the spatio-temporal AU graph. Then, a spatio-temporal graph learning (STGL) module is introduced to jointly model the spatio-temporal relationships among AUs of all face frames. Specifically, the update of a AU node in the graph is related to the nodes connected to it in the same frame and the nodes with the same type of it in all different frames, so that the relationships between AUs and the temporal information can interact and jointly guide the graph to learn representation for each AU node. The main contributions of this work are listed as follow:

% Then, we only retain the encoder of MAE to extract facial feature, which will be used to generate the nodes and edges of AU graph. The GCN and Transformer will be used to upgrade the AU graph in spatial dimension and temporal dimension. In this way, relationships between different AU and different frames can be learned. At last, the node feature are used to classify. 



% \begin{itemize}
%     \item We pre-train a MAE model based on human face databases, which can obtain a strong facial representation, to overcome the data imbalance problem in Aff-Wild2 database.

%     \item We propose a spatio-temporal graph learning module to learn spacial relationships between different AUs and temporal dependencies among different frames.
    
% \end{itemize}



\section{Related Work}

In this section, we systematically review previous AU recognition approaches, which are categorized into two types: Non-graph and graph-based AU recognition approaches.

%%% 1. 介绍目前的AU 识别方法（非图）

\subsection{Non-graph based AU recognition approaches}

Since each AU's activation can only appear in a sparse facial region \cite{kaili2016deep}, several facial region-based methods are proposed \cite{kaili2016deep,li2017eac,li2017action,shao2019facial,jacob2021facial}. For example, Li et al. \cite{li2017eac} first crop key facial regions from face images, and then learn deep features for each facial region individually. Jacob et al. \cite{jacob2021facial} set an attention module to enforce the model to focus on the facial regions corresponding to activated AUs. There are several studies \cite{jaiswal2016deep,chu2017learning,shao2020spatio,nguyen2022ensemble,song2021self} introduce temporal information as the facial muscle movement is a dynamic process. Chu et al. \cite{chu2017learning} use CNN to extract feature from each frame and model the temporal sequence by LSTM. Nguyen et al. \cite{nguyen2022ensemble} utilize Transformer to add temporal information in frame sequence and they rank 3rd in the AU detection challenge at the ABAW competition 2022. The methods summarised above are built on typical supervised learning, whose generalization capabilities are largely depending on the quality of AU annotations. Subsequently, self-supervised learning strategies \cite{li2019self,ma2022facial,chang2022knowledge,li2019self} recently have been frequently introduced  to AU recognition. In particular, MAE-Face \cite{ma2022facial} first learns a high-capacity model from a large amount of face images without any data annotations, then after being fine-tuned on downstream task including AU detection and AU intensity estimation, which exhibits convincing performance.

% MAE-Face \cite{ma2022facial}that proposed by Ma et al. first learns a high-capacity model from a large amount of face images without any data annotations. Then after being fine-tuned on AU datasets, it exhibits convincing performance for AU recognition. In the 3rd  ABAW competition, Zhang et al. \cite{zhang2022transformer} proposed a multimodal framework to fuse audio and visual feature for AU and expression recognition. Jiang et al. \cite{jiang2022model} pretrain the backbone on different task, including expression and face recognition, and they adopt an ensemble methodology to get the result of AU recognition. Nguyen et al.\cite{nguyen2022ensemble} utilize Gated Recurrent Unit (GRU) and Transformer to introduce temporal information in frame sequence.


\subsection{Graph-based AU recognition approaches}


Considering that relationships between AUs (i.e., AU co-occurrence pattern) play a significant role in AU recognition, some researchers utilize graph neural networks (GNNs) to model the underlying relationship. Li et al.\cite{li2019semantic} is the first attempt that employs the GNN for AU relationships modeling. Song et al. \cite{song2021uncertain} propose an uncertain graph neural network to capture the importance of the dependencies among AUs for each input and estimate the prediction uncertainties. More recently, Luo et al. \cite{luo2022learning,song2022gratis} propose to learn multi-dimensional edge feature-based AU relational graph, where the relationship between each pair of AUs can be explicitly modelled by a task-specific multi-dimensional edge feature. Song et al.\cite{song2022heterogeneous} construct a co-occurrence knowledge graph and a spatio-temporal Transformer module to capture the temporal and spatial relations of AUs. Nguyen et al.\cite{nguyen2023affective} use a facial graph to capture the association among action units for the multi-task learning challenge and rank the 4th in multi-task challenge at the ABAW competition 2022. These works exhibit the effectiveness of modeling AU relationships.

\section{Methodology}


Given $T$ consecutive facial frames $S = \{f_1, ... ,f_t, ...,  f_T\}$, our goal is to predict AUs' occurrences for each frame. Since there are multiple AUs defined for each facial display, our approach aims to jointly predict multiple AUs for the $t_{th}$ frame, which is denoted as $P_t = \{p_{1t},p_{2t},...,p_{Nt}\} $, where $N$ represents the number of predicted AUs and $p \in  \{0, 1\}$ can be either activated (1) or inactivated (0). The pipeline of our approach is illustrated in Fig.~\ref{fig:overview}, which consists of a Facial Representation Encoder (FRE) described in Sec.~\ref{subsec:FRE}, an AU-specific Feature Generator (AFG) described in Sec.~\ref{subsec:AFG} and a Spatio-Temporal Graph Learning (STGL) module described in Sec.~\ref{subsec:STGL}. Specifically, the FRE first extracts global facial features from each image of the input face sequence. Then, the AFG individually learns a representation for each AU of each facial frame based on its global representation. This way, a spatio-temporal facial graph representation can be constructed. Finally, the STGL module takes the spatio-temporal facial graph as the input and jointly predicts all AUs' occurrence for all frames, where both spatial facial displays and temporal facial dynamics are considered.


Masked autoencoder (MAE) \cite{he2022masked} is a self-supervised learned model which reconstructs original images from a set of masked images. It is made up of a linear projection layer, a 12-layer encoder and a 4-layer decoder that were defined by the Vision Transformer \cite{dosovitskiy2020image}. The well-trained  MAE can be fine-tuned for various downstream tasks. 

Since MAE has a strong representation learning capability and scalability, we propose to first pre-train a MAE model using a large amount of face images from CASIA-WebFace\cite{yi2014learning}, AffectNet\cite{mollahosseini2017affectnet}, IMDB-WIKI \cite{rothe2018deep} and CelebA\cite{liu2015deep}, making the pre-trained MAE to be able to generate strong facial representations from previous unseen face images. This pre-training strategy would not only help the model to alleviate the data imbalance problem in target AU database, but also improve the generalization ability of network in uncontrolled environments. Fig. \ref{fig:mae} illustrates the MAE model's pre-training, where randomly masked face images are fed to the encoder to generate latent features, and then the decoder reconstructs the original image from these latent features. 




\subsection{Facial Representation Encoder}
\label{subsec:FRE}
Subsequently, the linear projection layer and the encoder of the pre-trained MAE can generate a strong representation for each input face image, which are employed as the encoder for our facial AU recognition pipeline. To generate AU predictions for the input facial image sequence $S = \{ f_1, ... ,f_t, ...,  f_T \} \in \mathbb{R}^{T\times C\times H\times W}$ , the linear projection layer first encodes the sequence to a set of patches, which are treated as a set of tokens to be fed into the pre-trained transformer encoder without masking operation. As a result, a set of facial representations $X = \{ x_1,...,x_t,..., x_T \}\in \mathbb{R}^{T\times m \times d}$ can be generated, where each $x_t \in X$ represents a global facial representation of a face image; $m$ is the number of patches and $d$ denotes the dimension of each patch.


%It may not be a appropriate method to directly input the facial representation $X$ to the classification layer, like \cite{ma2022facial}. Global facial representation contains the high-level semantic features of the whole face, which makes is difficult to obtain the detailed information of each AU directly. 


Since each AU's activation only appears in a specific local facial region but will impact the whole face, we propose a AU-specific Feature Generator(AFG) to extract unique feature for each AU from the global facial representation $X$. In particular, the AFG consists $N$ branches, where each is made up of a fully connected layers (FC) followed by a global average pooling (GAP) layer. The FC layer of the $i_{th}$ branch first projects the $X$ to an AU-specific feature map $U_i \in \mathbb{R}^{T \times m \times d} $, and then GAP layers yields $T$ vector consisting  $V_i = \{ v_{i1},...,v_{it},...,v_{iT} \} \in \mathbb{R}^{T \times d}$, where $v_{it}$ denotes the representation of the $i_{th}$ AU in the $t_{th}$ frame.


\subsection{Spatial-Temporal Graph Learning}
\label{subsec:STGL}
As discussed before, AUs in each facial display are related to each other. Meanwhile, since human facial behaviours are continuous and smooth, AU activation status in adjacent frames are also temporally correlated. In this sense, our method jointly learns both the spatial relationship among AUs within each face frame as well as the their temporal relationship among face frames.
% Serveral works model the spacial AU relation by a single graph, and the edges connection are based on the prior knowledge from label distribution in training set, but the relationships in different face are not identical. 
Specifically, the Spatial-Temporal Graph Learning (STGL) module consists of a spatial AU relationship modelling module and a temporal AU relationship modelling module. Firstly, we employ the Facial Graph Generator (FGG) proposed by \cite{luo2022learning,song2022gratis} to learn a spatial AU graph representation for each face frame, which consists of $N$ nodes describing features of the $N$ target AUs. Then, the connectivity (edge presence) between each pair of nodes is defined according to the similarity of their features, i.e., each node connects with its $K$ nearest neighbour nodes with highest similarity scores. This way, the topology of the generated graph representation would have adapted topology for different facial display. After that, a GCN layer is adopted to update node features for the obtained facial graph representation. 


\begin{figure}
    \centering
    \includegraphics[scale=0.5]{mae.png}
    \caption{ Illustration of the masked autoencoder model}
    \label{fig:mae}
\end{figure}
\subsection{AU-specific Feature Generator}
\label{subsec:AFG}


% The new representation of the $i_{th}$ AU in the $t_{th}$ frame can be calculated by its neighbours in the spacial dimension as: 
% $$v_{it}^S =  \sigma [v_{it} + g(v_{it} ,\sum_{j=1}^{N}r(v_{jt},e_{it,jt}) ) ] $$
% where $\sigma$ is the activation function; g and r denote differentiable functions of the GCN layer, and $e_{it,jt} \in \{ 0 ,1 \}$denotes the connectivity between $v_{it}$ and $V_{jt}$. Specially, the above operations will be conducting in each frame of the input sequence.
% Although AU labels only have two status (0 or 1), the activation of each AU is a continuous process so that some researchers have used LSTM \cite{hochreiter1997long} to model the relationship in different frames.
Since transformer \cite{vaswani2017attention} is a superior model to learn long and short-range temporal dependencies, we then propose to utilize the transformer to update AU representations by considering temporal dynamics among facial frames. In temporal dimension, the graph nodes after GCN layer are considered as $N$ sequences for $N$ AUs, each of which consists of $T$ node. For the $i_{th}$ AU sequence $V_i^S = \{ v_{i1}^S,\cdots,v_{it}^S,\cdots,v_{iT}^S \}\in \mathbb{R}^{T \times d}$, the $T$ frames will be taken as $T$ tokens to feed to transformer. Each sequence updates its node individually and the updating process for the $i_{th}$ AU sequence is defined as:
% the nodes set which can be represented as $V_i^S \in \mathbb{R}^{T \times d} will be taken first to calculate self-attention in temporal dimension and then updated as follow:
\begin{equation}
\begin{split}
    & Q,K,V = V_i^S W_Q, V_i^S W_K, V_i^S W_V \\
    & Z = \text{LayerNorm}(V_i^S + \text{Att}(Q,K,V)) \\
    & V_i^{ST} = \text{LayerNorm}(Z + \text{FFN}(Z))    
\end{split}
\end{equation}
where Att denotes the self-attention function, and FFN is the feed forward network in transformer. We perform these operations on each AU in the temporal dimension.

In this paper, three Spatio-Temporal Graph Learning (STGL) modules are stacked to produce spatio-temporal AU graph representations. Then, the similarity calculating (SC) strategy \cite{luo2022learning} is employed to predict the probability. For the $i_{th}$ AU in the $t_{th}$ frame, a trainable vector $s_i$ which has the same dimension as $v_{it}^{ST}$, is shared across all frames. The prediction score can be denoted as:
\begin{equation}
 S_{it} = \frac{\sigma(v_{it}^{ST})^T \sigma(s_i)}{|| \sigma(v_{it}^{ST}) ||_2 || \sigma(s_i)||_2}   
\end{equation}
where $\sigma$ is the RELU activation function.











\begin{table*}[h]
    \centering
    \begin{tabular}{cccccccccccccc}
    \toprule
         &AU1&AU2&AU4&AU6&AU7&AU10&AU12&AU15&AU23&AU24&AU25&AU26&Average  \\
    \midrule
    Baseline & -& -& -& -& -& -& -& -& -& -& -& -& 39.0 \\
     Ours & 57.8&48.0&55.9&61.9&75.5&74.6&72.0&35.6&21.6&23.7&86.0&38.3&54.3 \\
    \bottomrule
    \end{tabular}
    \caption{F1 score (in \%) results achieved for 12 AUs on validation set.}
    \label{tab:result}
\end{table*}

\begin{table*}[h]
    \centering
     \resizebox{1\textwidth}{!}{
    \begin{tabular}{c|ccc|cc|c}
    \toprule
             & Pre-train (ImageNet)    &MAE (ImageNet) & MAE (hybrid dataset)  & Spatial    &  Temporal  &  Average F1-score \\
    
       
    \midrule
    (i) &\checkmark      &                                     &                             &  \checkmark     & &50.7\\
    (ii) &\checkmark      &                                     &                             &  \checkmark     &  \checkmark &52.6\\
  (iii) &   & \checkmark    &                  &\checkmark      &     & 48.5\\
    
    (iv) &   & \checkmark    &                  &\checkmark      &  \checkmark     &49.1\\

        
     (v) &   &                    &  \checkmark                 &   \checkmark                            &   & 51.6  \\
    (vi)&   &                    & \checkmark                  &\checkmark  &\checkmark               &  54.3\\    
    \bottomrule
    \end{tabular}
    }
    \caption{Ablation study on validation set.}
    % \caption{The F1 score (in \%) of ablation study on validation set.}
    \label{tab:my_label}
\end{table*}



\subsection{Loss Function}

We propose a two-stage training strategy to train our AU recognition model. At the first stage, we first pre-train the transformer-based facial representation encoder by reconstructing the masked patches of the original face images. Here, we employ Mean Square Error (MSE) loss to constrain the difference between the reconstructed patches and the original patches at the pixel-level. Suppose that $M$ patches are masked at the beginning, the pre-training loss $L_\text{pre}$ is formulated as:
\begin{equation}
L_\text{pre} = \sum_{m=1}^{M}(p_m^{gt} - p_m)^2 \tag{{3}}
\end{equation}
where $p_m^{gt}$ denotes the ground truth pixel and $p_m$ denotes the reconstructed pixel.

Since AU recognition is a multi-label binary classification problem, we use the BCE loss to optimize the network at the second training stage, which enforce the whole framework to jointly output multiple AUs occurrence prediction. The $L_{au}$ is denoted as:
\begin{equation}
L_{au} = - \sum_{i=1}^{N} \sum_{t=1}^{T} [y_{it} \log(p_{it}) +(1-y_{it}) \log(1-p_{it}) \tag{4}
\end{equation}
where $p_{it}, y_{it}$ are the prediction and ground truth; $N$ and $T$ are the numbers of AUs and frames of the input face sequence, respectively.




\section{Experimental results}

% Our training process can be divided into two stages. At the first stage, we leverage the state-of-the-art self-supervised strategy, MAE \cite{he2022masked} to pre-train our network on a hybrid face dataset, including CASIA-WebFace\cite{yi2014learning}, AffectNet\cite{mollahosseini2017affectnet}, IMDB-WIKI \cite{rothe2018deep} and CelebA\cite{liu2015deep}. 
% It allows the network to be initialized with a good priori of facial features.
% After that, the network is trained on the ABAW dataset to be capable of detecting action units.


% In this section, we will introduce the dataset details, experimental settings and our results on the validation set. To prove the effectiveness of each module, we also provide ablation studies.



\subsection{Dataset}

\textbf{Dataset for MAE:} Self-supervised pre-training helps neural networks learn effectively discriminative representations, however, it will bring limited gains for downstream tasks if the size of training data is limited. 
For this reason, we collect a hybrid face dataset from different in-the-wild datasets, following the collecting method in the prior work \cite{ma2022facial}.
This hybrid dataset involves four subsets from  
CASIA-WebFace\cite{yi2014learning}, AffectNet\cite{mollahosseini2017affectnet}, IMDB-WIKI \cite{rothe2018deep} and CelebA\cite{liu2015deep}, respectively.
These source datasets are well-known and widely used in fields spanning from face recognition to expression recognition.
However, we found some low-quality image data included in these datasets.
In order to make the training more effective, we remove all images with blur and incomplete faces.
Finally, we can obtain the hybrid dataset of around 1,920,000 face images without annotations for our pre-training.
% are combined to form the pre-training dataset.

% A large amount of image data are necessary for self-supervised pre-training. So We follow the approach of \cite{ma2022facial} to collect a hybrid face dataset from different in-the-wild datasets. The hybrid set consists of CASIA-WebFace\cite{yi2014learning}, AffectNet\cite{mollahosseini2017affectnet}, IMDB-WIKI \cite{rothe2018deep} and CelebA\cite{liu2015deep}, which are commonly used for face recognition or expression recognition task. All blur and incomplete faces are removed and totally about 1,920,000 face images without annotations are combined to form the pre-training dataset.

\textbf{Dataset for AU detection:} The AU detection challenge at 5th ABAW competition \cite{kollias2023abaw} provides 541 video sequences from Aff-Wild2 dataset.
Each frame of a video sequence in this dataset is manually or automatically annotated with labels of 12 AUs, namely AU1, AU2, AU4, AU6, AU7, AU10, AU12, AU15, AU23, AU24, AU25, and AU26.
Totally, this dataset contains 2,627,632 frames,  with 438 subjects, 268 of which are males and 170 are females. Meanwhile, this video dataset is split into a training set of 295 sequences, a validation set of 105 sequences, and a testing set of 141 sequences in a subject-independent manner.

% Besides, the videos are split into training, validation and testing sets in a subject independent manner.



% that contains annotations in terms of 12 AUs, namely AU1, AU2, AU4, AU6, AU7, AU10, AU12, AU15, AU23, AU24, AU25 and AU26. In total, 2, 627, 632 frames, with 438 subjects, 268 of which are male and 170 female, have been annotated manually and automatically. Besides, the videos are split into training, validation and testing sets in a subject independent manner.




% \begin{table*}[h]
%     \centering
%     \begin{tabular}{cccccc}
%     \toprule
%     \multirow{2}{*}{Backbone}         &pretrained   &     pretrained     & \multirow{2}{*}{Spacial}    &   \multirow{2}{*}{Temporal}  &  \multirow{2}{*}{Average F1-score} \\
%                                       &(ImageNet)    &    (hybrid set)    \\
       
%     \midrule
%     \multirow{2}{*}{Swin Transformer} & \checkmark   &                    &  \checkmark                 &                              &  50.7  \\
%                                       & \checkmark   &                    & \checkmark                  &    \checkmark               &  52.6 \\
%     \midrule
%     \multirow{3}{*}{MAE}              & \checkmark   &                    &  \checkmark                 &     \checkmark               &  49.1 \\
%                                       &              &   \checkmark       &  \checkmark                 &                              &  51.6 \\
%                                       &              &      \checkmark    &   \checkmark                &     \checkmark               &  54.3 \\
%     \bottomrule
%     \end{tabular}
%     \caption{Ablation study on validation set.}
%     % \caption{The F1 score (in \%) of ablation study on validation set.}
%     \label{tab:my_label}
% \end{table*}

\subsection{Experimental settings}

\textbf{Details for MAE pre-training:} We first leverage RetinaFace \cite{deng2020retinaface} to perform face detection and alignment for each image from the hybrid dataset and crop it to $256 \times 256$. The encoder and decoder  are initialized with the weights pre-trained on ImageNet-1k \cite{deng2009imagenet} dataset. 
When reconstructing each masked image, we applied random cropping augmentation and chose a mask ratio of $75\%$.
During training, a AdamW optimizer with the learning rate of $1.5e^{-4}$ is used, with batch size of 512, and weight decay of $0.05$.
Totally, we pre-train our network for 300 epochs, 40 of which are warm-up epochs.

% During reconstructing, Random cropping and a mask ratio of $75\%$ are applied, with the learning rate of $1.5e-4$ and a weight decay of $0.05$. We pre-train MAE for 300 epochs, with 40 warmup epochs. The batch size is set to 512 and the AdamW optimizer is also used.

\textbf{Details for AU detection training:} 
At this training stage, we follow the cropped-and-aligned version of the Aff-Wild2 dataset. Each subject from the training set is recorded with one video sequence. During training, we randomly select a video clip of 16 frames as input to our model.
For validating and testing, we split each video data into segments, each of which contains 16 frames.
If the number of frames of the segment is less than 16, we supplement it with blank frames.
During the training process, we employ an AdamW optimizer with a weight decay of $5e^{-4}$. The number $K$ for choosing the nearest neighbors is set to 4. The learning rate is set to $1e^{-4}$ and adjusted by a cosine decay learning rate scheduler.





% In training set, each subject have a video, in which we randomly select 16 continuous frames as a training sample in one epoch. In validation and test set, all videos are split to segments with 16 frames, with the vacant positions filled with the previous frame. During training, we employ an AdamW optimizer with a weight decay of $5e^{-4}$. The number K for choosing nearest neighbors is 4. The learning rate is set to 1e-4 and scheduled with cosine decay. We train the model for 10 epochs and the batch size is set to 16.

\textbf{Evaluation metrics:}
We evaluate the AU detection performance of methods by the average F1-score across all AUs. This metric is defined as: $F_1^{AU} = \frac{\sum_{n=1}^{N} F_1^{AU, n}}{N}$ where N denotes the number of AUs, and F1-score $F_1^{AU, n}$ for individual AU class is computed as:

\begin{equation}
    F_1^{AU,n} = 2 \cdot  \frac{ P^{AU,n} \cdot  R^{AU,n} }{ P^{AU,n} +  R^{AU,n} }
\end{equation}

where $P^{AU,n}$ is the calculated precision for the $\text{n}_{th}$ AU and  $R^{AU,n}$ is the recall rate for it.

% $$F_1^{AU,n} = 2 \times \frac{ { P } \cdot  {Recall}}{ {Precision}+  {Recall.}}$$


% $$F_1^{AU,n} = 2 \times \frac{ { Precision } \cdot  {Recall}}{ {Precision}+  {Recall }}$$


% For AU detection, the performance is measured using the average F1 Score across all AUs. Therefore, the evaluation criterion is defined as :
% $$F_1^{AU} = \frac{\sum_{n=1}^{N} F_1^n}{N}$$



\subsection{Results on validation set}
% The $5_{th}$ ABAW competition provides the official training and validation set. We split each validation video as several segments with fixed length and make a prediction for each frame in a segment. 

Tab.~\ref{tab:result} shows evaluation results of AU detection on the validation set and reports the F1-score for each AU. We can see that our method can achieve significant improvement over the baseline, i.e. from 39.0 to 54.3 average F1-score. 

\subsection{Ablation study}
Tab.~\ref{tab:my_label} presents the results of our ablation studies.
We can see that (ii) the model with proposed temporal graph learning achieves a large margin of improvement over (i) the model without it. It demonstrates that the modelling temporal relation between adjacent facial frames contributes much to AU detection. Furthermore, the model pre-trained on ImageNet using vanilla MAE (iv) is not capable of performing better than the baseline (ii). The possible reason could be that there is a  significant domain gap between the dataset for universal object recognition and the dataset for facial tasks. 
However, when we replace the MAE pre-train dataset with the collected hybrid dataset, the model (vi) shows superior performance ($54.3$ average F1-score) than the baseline (ii) ($52.6$ average F1-score). 




% To prove the effectiveness of MAE, we choose swin transformer as our backbone and make comparisons. 


% Swin transformer are pre-trained on ImageNet\cite{deng2009imagenet} and MAE in our hybrid face dataset, and then both add the spacial graph module, the AU average F1 score of two model are $50.7\%$ and $51.6\%$. After adding the temporal module, the AU average F1 score of them are $52.6\%$ and $54.3\%$，respectively. But when MAE is pretrained on ImageNet, it present a relatively bad performance. Therefore, the MAE with self-supervised learning is more advantageous. Then we consider the improvement that the temporal module brings. The data show that the average F1 score increases from $50.7\%$ to $52.6\% $when using MAE, and from $51.6\%$ to $54.3\%$ when backbone is swin transformer after introducing temporal module, which means that temporal information is quite beneficial for AU detection.


\section{Conclusion}
This paper proposes an effective spatio-temporal graph relation learning  based method for AU detection at the 5th Affective Behavior Analysis in-the-wild (ABAW) competition. We also introduce MAE as our facial representation encoder. The experimental results demonstrate that the proposed method achieves excellent performance in recognizing AUs.

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
