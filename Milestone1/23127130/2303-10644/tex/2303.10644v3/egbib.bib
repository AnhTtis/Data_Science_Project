@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@misc{Authors14,
 author = {FirstName LastName},
 title = {The frobnicatable foo filter},
 note = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},
 year = 2014
}

@misc{Authors14b,
 author = {FirstName LastName},
 title = {Frobnication tutorial},
 note = {Supplied as supplemental material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {FirstName Alpher},
title = {Frobnication},
journal = PAMI,
volume = 12,
number = 1,
pages = {234--778},
year = 2002
}

@article{Alpher03,
author = {FirstName Alpher and  FirstName Fotheringham-Smythe},
title = {Frobnication revisited},
journal = {Journal of Foo},
volume = 13,
number = 1,
pages = {234--778},
year = 2003
}

@article{Alpher04,
author = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},
title = {Can a machine frobnicate?},
journal = {Journal of Foo},
volume = 14,
number = 1,
pages = {234--778},
year = 2004
}

@inproceedings{Alpher05,
author = {FirstName Alpher and FirstName Gamow},
title = {Can a computer frobnicate?},
booktitle = CVPR,
pages = {234--778},
year = 2005
}

@article{ekman1978facial,
  title={Facial action coding system},
  author={Ekman, Paul and Friesen, Wallace V},
  journal={Environmental Psychology \& Nonverbal Behavior},
  year={1978}
}

@article{kollias2023abaw,
  title={ABAW: Valence-Arousal Estimation, Expression Recognition, Action Unit Detection \& Emotional Reaction Intensity Estimation Challenges},
  author={Kollias, Dimitrios and Tzirakis, Panagiotis and Baird, Alice and Cowen, Alan and Zafeiriou, Stefanos},
  journal={arXiv preprint arXiv:2303.01498},
  year={2023}
}


D. Kollias: "ABAW: Valence-Arousal Estimation, Expression Recognition, Action Unit Detection & Multi-Task Learning Challenges", IEEE CVPR, 2022

@inproceedings{kollias2022abaw, title={Abaw: Valence-arousal estimation, expression recognition, action unit detection \& multi-task learning challenges}, author={Kollias, Dimitrios}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages={2328--2336}, year={2022} } 

 

D. Kollias, et. al.: "Distribution Matching for Heterogeneous Multi-Task Learning: a Large-scale Face Study", 2021
@article{kollias2021distribution, title={Distribution Matching for Heterogeneous Multi-Task Learning: a Large-scale Face Study}, author={Kollias, Dimitrios and Sharmanska, Viktoriia and Zafeiriou, Stefanos}, journal={arXiv preprint arXiv:2105.03790}, year={2021} }

 

D. Kollias, et. al.: "Analysing Affective Behavior in the second ABAW2 Competition". ICCV, 2021
@inproceedings{kollias2021analysing, title={Analysing affective behavior in the second abaw2 competition}, author={Kollias, Dimitrios and Zafeiriou, Stefanos}, booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages={3652--3660}, year={2021}}

 

D. Kollias,S. Zafeiriou: "Affect Analysis in-the-wild: Valence-Arousal, Expressions, Action Units and a Unified Framework, 2021
@article{kollias2021affect, title={Affect Analysis in-the-wild: Valence-Arousal, Expressions, Action Units and a Unified Framework}, author={Kollias, Dimitrios and Zafeiriou, Stefanos}, journal={arXiv preprint arXiv:2103.15792}, year={2021}}

 

D. Kollias, et. al.: "Analysing Affective Behavior in the First ABAW 2020 Competition". IEEE FG, 2020
@inproceedings{kollias2020analysing, title={Analysing Affective Behavior in the First ABAW 2020 Competition}, author={Kollias, D and Schulc, A and Hajiyev, E and Zafeiriou, S}, booktitle={2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)}, pages={794--800}}

 

D. Kollias, S. Zafeiriou: "Expression, Affect, Action Unit Recognition: Aff-Wild2, Multi-Task Learning and ArcFace". BMVC, 2019
@article{kollias2019expression, title={Expression, Affect, Action Unit Recognition: Aff-Wild2, Multi-Task Learning and ArcFace}, author={Kollias, Dimitrios and Zafeiriou, Stefanos}, journal={arXiv preprint arXiv:1910.04855}, year={2019}}

 

D. Kollias, et at.: "Face Behavior a la carte: Expressions, Affect and Action Units in a Single Network", 2019
@article{kollias2019face,title={Face Behavior a la carte: Expressions, Affect and Action Units in a Single Network}, author={Kollias, Dimitrios and Sharmanska, Viktoriia and Zafeiriou, Stefanos}, journal={arXiv preprint arXiv:1910.11111}, year={2019}}

 

D. Kollias, et. al.: "Deep Affect Prediction in-the-wild: Aff-Wild Database and Challenge, Deep Architectures, and Beyond". International Journal of Computer Vision (IJCV), 2019
@article{kollias2019deep, title={Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond}, author={Kollias, Dimitrios and Tzirakis, Panagiotis and Nicolaou, Mihalis A and Papaioannou, Athanasios and Zhao, Guoying and Schuller, Bj{\"o}rn and Kotsia, Irene and Zafeiriou, Stefanos}, journal={International Journal of Computer Vision}, pages={1--23}, year={2019}, publisher={Springer} }

 

S. Zafeiriou, et. al. "Aff-Wild: Valence and Arousal in-the-wild Challenge". IEEE CVPR, 2017
@inproceedings{zafeiriou2017aff, title={Aff-wild: Valence and arousal ‘in-the-wild’challenge}, author={Zafeiriou, Stefanos and Kollias, Dimitrios and Nicolaou, Mihalis A and Papaioannou, Athanasios and Zhao, Guoying and Kotsia, Irene}, booktitle={Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on}, pages={1980--1987}, year={2017}, organization={IEEE} } 

@article{luo2022learning,
  title={Learning multi-dimensional edge feature-based au relation graph for facial action unit recognition},
  author={Luo, Cheng and Song, Siyang and Xie, Weicheng and Shen, Linlin and Gunes, Hatice},
  journal={arXiv preprint arXiv:2205.01782},
  year={2022}
}

@article{song2022gratis,
  title={GRATIS: Deep Learning Graph Representation with Task-specific Topology and Multi-dimensional Edge Features},
  author={Song, Siyang and Song, Yuxin and Luo, Cheng and Song, Zhiyuan and Kuzucu, Selim and Jia, Xi and Guo, Zhijiang and Xie, Weicheng and Shen, Linlin and Gunes, Hatice},
  journal={arXiv preprint arXiv:2211.12482},
  year={2022}
}

@article{ma2022facial,
  title={Facial Action Unit Detection and Intensity Estimation from Self-supervised Representation},
  author={Ma, Bowen and An, Rudong and Zhang, Wei and Ding, Yu and Zhao, Zeng and Zhang, Rongsheng and Lv, Tangjie and Fan, Changjie and Hu, Zhipeng},
  journal={arXiv preprint arXiv:2210.15878},
  year={2022}
}

@inproceedings{he2022masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16000--16009},
  year={2022}
}

@inproceedings{zhang2022transformer,
  title={Transformer-based multimodal information fusion for facial expression analysis},
  author={Zhang, Wei and Qiu, Feng and Wang, Suzhen and Zeng, Hao and Zhang, Zhimeng and An, Rudong and Ma, Bowen and Ding, Yu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2428--2437},
  year={2022}
}

@article{jin2021multi,
  title={A multi-modal and multi-task learning method for action unit and expression recognition},
  author={Jin, Yue and Zheng, Tianqing and Gao, Chao and Xu, Guoqiang},
  journal={arXiv preprint arXiv:2107.04187},
  year={2021}
}

@article{martinez2017automatic,
  title={Automatic analysis of facial actions: A survey},
  author={Martinez, Brais and Valstar, Michel F and Jiang, Bihan and Pantic, Maja},
  journal={IEEE transactions on affective computing},
  volume={10},
  number={3},
  pages={325--347},
  year={2017},
  publisher={IEEE}
}

@inproceedings{song2021self,
  title={Self-supervised learning of dynamic representations for static images},
  author={Song, Siyang and Sanchez, Enrique and Shen, Linlin and Valstar, Michel},
  booktitle={2020 25th international conference on pattern recognition (icpr)},
  pages={1619--1626},
  year={2021},
  organization={IEEE}
}

@article{song2020spectral,
  title={Spectral representation of behaviour primitives for depression analysis},
  author={Song, Siyang and Jaiswal, Shashank and Shen, Linlin and Valstar, Michel},
  journal={IEEE Transactions on Affective Computing},
  volume={13},
  number={2},
  pages={829--844},
  year={2020},
  publisher={IEEE}
}

@inproceedings{song2018human,
  title={Human behaviour-based automatic depression analysis using hand-crafted statistics and deep learned spectral features},
  author={Song, Siyang and Shen, Linlin and Valstar, Michel},
  booktitle={2018 13th IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2018)},
  pages={158--165},
  year={2018},
  organization={IEEE}
}

@inproceedings{jaiswal2019automatic,
  title={Automatic prediction of depression and anxiety from behaviour and personality attributes},
  author={Jaiswal, Shashank and Song, Siyang and Valstar, Michel},
  booktitle={2019 8th international conference on affective computing and intelligent interaction (acii)},
  pages={1--7},
  year={2019},
  organization={IEEE}
}

@inproceedings{song2021uncertain,
  title={Uncertain graph neural networks for facial action unit detection},
  author={Song, Tengfei and Chen, Lisha and Zheng, Wenming and Ji, Qiang},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)},
  volume={1},
  year={2021}
}

@inproceedings{deng2022estimating,
  title={Estimating multiple emotion descriptors by separating description and inference},
  author={Deng, Didan and Shi, Bertram E},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2392--2400},
  year={2022}
}

@inproceedings{jeong2022multi,
  title={Multi-Task Learning for Human Affect Prediction With Auditory-Visual Synchronized Representation},
  author={Jeong, Euiseok and Oh, Geesung and Lim, Sejoon},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2438--2445},
  year={2022}
}
@inproceedings{nguyen2022ensemble,
  title={An Ensemble Approach for Facial Behavior Analysis in-the-wild Video},
  author={Nguyen, Hong-Hai and Huynh, Van-Thong and Kim, Soo-Hyung},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2512--2517},
  year={2022}
}

@inproceedings{wang2022action,
  title={Action unit detection by exploiting spatial-temporal and label-wise attention with transformer},
  author={Wang, Lingfeng and Qi, Jin and Cheng, Jian and Suzuki, Kenji},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2470--2475},
  year={2022}
}

@article{zhang2021prior,
  title={Prior aided streaming network for multi-task affective recognitionat the 2nd abaw2 competition},
  author={Zhang, Wei and Guo, Zunhu and Chen, Keyu and Li, Lincheng and Zhang, Zhimeng and Ding, Yu},
  journal={arXiv preprint arXiv:2107.03708},
  year={2021}
}

@inproceedings{dey2017gate,
  title={Gate-variants of gated recurrent unit (GRU) neural networks},
  author={Dey, Rahul and Salem, Fathi M},
  booktitle={2017 IEEE 60th international midwest symposium on circuits and systems (MWSCAS)},
  pages={1597--1600},
  year={2017},
  organization={IEEE}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{song2022heterogeneous,
  title={Heterogeneous spatio-temporal relation learning network for facial action unit detection},
  author={Song, Wenyu and Shi, Shuze and Dong, Yu and An, Gaoyun},
  journal={Pattern Recognition Letters},
  volume={164},
  pages={268--275},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{nguyen2023affective,
  title={Affective Behavior Analysis Using Action Unit Relation Graph and Multi-task Cross Attention},
  author={Nguyen, Dang-Khanh and Pant, Sudarshan and Ho, Ngoc-Huynh and Lee, Guee-Sang and Kim, Soo-Hyung and Yang, Hyung-Jeong},
  booktitle={European Conference on Computer Vision},
  pages={132--142},
  year={2023},
  organization={Springer}
}


@inproceedings{li2019semantic,
  title={Semantic relationships guided representation learning for facial action unit recognition},
  author={Li, Guanbin and Zhu, Xin and Zeng, Yirui and Wang, Qing and Lin, Liang},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={8594--8601},
  year={2019}
}

@inproceedings{jiang2022model,
  title={Model level ensemble for facial action unit recognition at the 3rd abaw challenge},
  author={Jiang, Wenqiang and Wu, Yannan and Qiao, Fengsheng and Meng, Liyu and Deng, Yuanyuan and Liu, Chuanhe},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2337--2344},
  year={2022}
}

@inproceedings{li2017eac,
  title={Eac-net: A region-based deep enhancing and cropping approach for facial action unit detection},
  author={Li, Wei and Abtahi, Farnaz and Zhu, Zhigang and Yin, Lijun},
  booktitle={2017 12th IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2017)},
  pages={103--110},
  year={2017},
  organization={IEEE}
}

@inproceedings{kaili2016deep,
  title={Deep region and multi-label learning for facial action unit detection},
  author={Kaili, Zhao and Chu, Wen-Sheng and Zhang, Honggang},
  booktitle={In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3391--3399},
  year={2016}
}

@inproceedings{li2017action,
  title={Action unit detection with region adaptation, multi-labeling learning and optimal temporal fusing},
  author={Li, Wei and Abtahi, Farnaz and Zhu, Zhigang},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1841--1850},
  year={2017}
}

@inproceedings{li2019self,
  title={Self-supervised representation learning from videos for facial action unit detection},
  author={Li, Yong and Zeng, Jiabei and Shan, Shiguang and Chen, Xilin},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer vision and pattern recognition},
  pages={10924--10933},
  year={2019}
}

@inproceedings{jacob2021facial,
  title={Facial action unit detection with transformers},
  author={Jacob, Geethu Miriam and Stenger, Bjorn},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7680--7689},
  year={2021}
}

@article{shao2019facial,
  title={Facial action unit detection using attention and relation learning},
  author={Shao, Zhiwen and Liu, Zhilei and Cai, Jianfei and Wu, Yunsheng and Ma, Lizhuang},
  journal={IEEE transactions on affective computing},
  volume={13},
  number={3},
  pages={1274--1289},
  year={2019},
  publisher={IEEE}
}

@inproceedings{chu2017learning,
  title={Learning spatial and temporal cues for multi-label facial action unit detection},
  author={Chu, Wen-Sheng and De la Torre, Fernando and Cohn, Jeffrey F},
  booktitle={2017 12th IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2017)},
  pages={25--32},
  year={2017},
  organization={IEEE}
}

@inproceedings{jaiswal2016deep,
  title={Deep learning the dynamic appearance and shape of facial action units},
  author={Jaiswal, Shashank and Valstar, Michel},
  booktitle={2016 IEEE winter conference on applications of computer vision (WACV)},
  pages={1--8},
  year={2016},
  organization={IEEE}
}

@article{shao2020spatio,
  title={Spatio-temporal relation and attention learning for facial action unit detection},
  author={Shao, Zhiwen and Zou, Lixin and Cai, Jianfei and Wu, Yunsheng and Ma, Lizhuang},
  journal={arXiv preprint arXiv:2001.01168},
  year={2020}
}

@inproceedings{chang2022knowledge,
  title={Knowledge-driven self-supervised representation learning for facial action unit recognition},
  author={Chang, Yanan and Wang, Shangfei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={20417--20426},
  year={2022}
}

@article{mollahosseini2017affectnet,
  title={Affectnet: A database for facial expression, valence, and arousal computing in the wild},
  author={Mollahosseini, Ali and Hasani, Behzad and Mahoor, Mohammad H},
  journal={IEEE Transactions on Affective Computing},
  volume={10},
  number={1},
  pages={18--31},
  year={2017},
  publisher={IEEE}
}

@article{yi2014learning,
  title={Learning face representation from scratch},
  author={Yi, Dong and Lei, Zhen and Liao, Shengcai and Li, Stan Z},
  journal={arXiv preprint arXiv:1411.7923},
  year={2014}
}

@article{rothe2018deep,
  title={Deep expectation of real and apparent age from a single image without facial landmarks},
  author={Rothe, Rasmus and Timofte, Radu and Van Gool, Luc},
  journal={International Journal of Computer Vision},
  volume={126},
  number={2-4},
  pages={144--157},
  year={2018},
  publisher={Springer}
}

@inproceedings{liu2015deep,
  title={Deep learning face attributes in the wild},
  author={Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={3730--3738},
  year={2015}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@inproceedings{deng2020retinaface,
  title={Retinaface: Single-shot multi-level face localisation in the wild},
  author={Deng, Jiankang and Guo, Jia and Ververas, Evangelos and Kotsia, Irene and Zafeiriou, Stefanos},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={5203--5212},
  year={2020}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT press}
}

@article{zhang2023facial,
  title={Facial Affective Analysis based on MAE and Multi-modal Information for 5th ABAW Competition},
  author={Zhang, Wei and Ma, Bowen and Qiu, Feng and Ding, Yu},
  journal={arXiv preprint arXiv:2303.10849},
  year={2023}
}
@article{yu2023local,
  title={Local Region Perception and Relationship Learning Combined with Feature Fusion for Facial Action Unit Detection},
  author={Yu, Jun and Li, Renda and Cai, Zhongpeng and Zhao, Gongpeng and Xie, Guochen and Zhu, Jichao and Zhu, Wangyuan},
  journal={arXiv preprint arXiv:2303.08545},
  year={2023}
}
@article{vu2023vision,
  title={Vision Transformer for Action Units Detection},
  author={Vu, Tu and Huynh, Van Thong and Kim, Soo Hyung},
  journal={arXiv preprint arXiv:2303.09917},
  year={2023}
}
@article{zhou2023continuous,
  title={Continuous emotion recognition based on TCN and Transformer},
  author={Zhou, Weiwei and Lu, Jiada and Xiong, Zhaolong and Wang, Weifeng},
  journal={arXiv preprint arXiv:2303.08356},
  year={2023}
}
@article{savchenko2023emotieffnet,
  title={EmotiEffNet Facial Features in Uni-task Emotion Recognition in Video at ABAW-5 competition},
  author={Savchenko, Andrey V},
  journal={arXiv preprint arXiv:2303.09162},
  year={2023}
}
@article{wang2023facial,
  title={Facial Affective Behavior Analysis Method for 5th ABAW Competition},
  author={Wang, Shangfei and Chang, Yanan and Wu, Yi and Miao, Xiangyu and Wu, Jiaqiang and Zhu, Zhouan and Wang, Jiahe and Xiao, Yufei},
  journal={arXiv preprint arXiv:2303.09145},
  year={2023}
}

@article{zhang2023facial1,
  title={Facial Affect Recognition based on Transformer Encoder and Audiovisual Fusion for the ABAW5 Challenge},
  author={Zhang, Ziyang and An, Liuwei and Cui, Zishun and Dong, Tengteng and others},
  journal={arXiv preprint arXiv:2303.09158},
  year={2023}
}
@article{nguyen2023transformer,
  title={A transformer-based approach to video frame-level prediction in Affective Behaviour Analysis In-the-wild},
  author={Nguyen, Dang-Khanh and Ho, Ngoc-Huynh and Pant, Sudarshan and Yang, Hyung-Jeong},
  journal={arXiv preprint arXiv:2303.09293},
  year={2023}
}
@article{yin2023multi,
  title={Multi-modal Facial Action Unit Detection with Large Pre-trained Models for the 5th Competition on Affective Behavior Analysis in-the-wild},
  author={Yin, Yufeng and Tran, Minh and Chang, Di and Wang, Xinrui and Soleymani, Mohammad},
  journal={arXiv preprint arXiv:2303.10590},
  year={2023}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}