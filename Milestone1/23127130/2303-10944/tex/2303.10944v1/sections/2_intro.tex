\section{Introduction} %1-1.5 pages
Humans quickly abstract information from sight. The visual apparatus is our broadband interface to the world. As such, it heavily interconnects the brain to facilitate fast interpretation of sensory impressions reflected by our experience.
The task of computer-aided scene understanding reflects this process in silico with the aim of automatic visual scene interpretation from camera data. This task can be partitioned into many subtasks, including object and instance detection, localization, or segmentation. Much like the hierarchical abstraction in humans, more complex tasks aim to not only describe the spatial context of objects in a scene but also try to identify the relationships between them.
Scene graphs are one potent tool for describing relationships in a scene. They provide a structured representation using nodes, describing the objects (object instances) and edges connecting two nodes to encode the relationship between them.
Such semantic representation of a scene can be used as the foundation for multiple downstream tasks, such as Visual Question Answering (VQA)~\cite{hudson_learning_2019}, image captioning~\cite{zhong_learning_2021}, image retrieval~\cite{johnson_image_2015}, or image generation and manipulation~\cite{dhamo_graph--3d_2021, dhamo_semantic_2020}.
While computer vision has long surpassed human performance for standard tasks such as classification~\cite{he_delving_2015}, reliable hierarchical abstraction in the form of Scene Graph Generation remains a challenging open research question.\looseness=-1

Most existing Scene Graph Generation approaches~\cite{xu_scene_2017,zhang_graphical_2019, lin_gps-net_2020, tang_unbiased_2020, zellers_neural_2018, tang_learning_2019, wu_scenegraphfusion_2021, wald_learning_2022} divide the task into two sub-tasks: 1) object detection and 2) pairwise relationship prediction. Recent works also propose to use an end-to-end pipeline, where object detection and relationship prediction are performed and optimized jointly~\cite{teng_structured_2022, cong_reltr_2022, shit_relationformer_2022}. 

Nonetheless, to our knowledge, all previous methods require object locations in some part of their pipeline. However, object locations, such as bounding boxes, are not needed for many downstream applications~\cite{hudson_learning_2019, zhong_learning_2021, johnson_image_2015, dhamo_graph--3d_2021, dhamo_semantic_2020} but are mainly used to simplify the SGG pipeline. Their location information provides a straightforward way for instance-level matching in case multiple objects of the same type are present in the scene. Bounding boxes and semantic segmentation labels add significant overhead in creating datasets and can further include annotation errors.

In our work, we want to challenge the requirement of object localizations for Scene Graph Generation and evaluation and overcome the burden of location labels. Not requiring bounding box object annotations could not only significantly simplify the creation of scene graph datasets but it would also allow disentangling of the research fields of object detection and scene graph generation.
While there are many benefits of predicting scene graphs without object location annotations, existing approaches face two major pitfalls. Firstly, the detection task is deeply rooted in the design of most methods, and its removal makes the scene graph generation task impossible. Secondly, current scene graph evaluation metrics are unable to evaluate the performance without bounding box annotations and predictions. The metrics measure the intersection over union threshold between prediction and ground truth based on their locations and the subsequent evaluation of the pairwise predicates~\cite{xu_scene_2017}. Without localization, it is not possible to evaluate pairwise predicates for multiple different instances.

In this paper, we introduce the new task of location-free scene graph generation (LF-SGG) and make its objective evaluation feasible. We design the first baseline method, which overcomes the challenge of correctly identifying entities with their corresponding instances using a novel transformer-based sequence generation design, which does not rely on bounding box annotations during training or validation. While conceptually simple, our method can represent the complexities in an image or scene and produce corresponding scene graphs. Even though we neither train nor evaluate our model on the object location, our method intrinsically learns location awareness which can be retrieved after the fact using the transformer's attention maps.\looseness=-1

We further introduce the first location-free evaluation paradigm for SGG. The predicted scene graph is matched with the target graph using a heuristic tree search formulation. This provides a task-adjusted approximate solution to the NP-hard~\cite{gold_graduated_1996} matching problem of two scene graphs and thereby enables objective evaluation of the new task.

Extensive experiments on the visual genome~\cite{krishna_visual_2016} and 4D-OR~\cite{ozsoy_4d-or_2022} datasets help to understand the performance of the proposed model on this new task and justify design choices. On Visual Genome, our model performs only slightly worse than the current state-of-the-art methods that rely on significantly more labels during training~\cite{xu_scene_2017, tang_learning_2019, teng_structured_2022, tang_unbiased_2020, zellers_neural_2018}. On 4D-OR, we even significantly surpass the location-based state-of-the-art method~\cite{ozsoy_4d-or_2022}.

\noindent In summary, we make the following major contributions:

\begin{itemize}
\item We propose a new task of location-free scene graph generation (LF-SGG). The challenge is to predict scene graphs without access to bounding boxes at training or evaluation time.
\item A necessity for the objective evaluation of LF-SGG is scene graph matching. To this end, we design a task-specific efficient heuristic tree search algorithm.
\item We introduce Pix2SG, the first method to solve LF-SGG, and evaluate its performance on the Visual Genome and 4D-OR datasets.

\end{itemize}

