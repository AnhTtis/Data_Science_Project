

\section{Experiments} % 2.5-3 pages
%-------------------------------------------------------------------------
\subsection{Datasets}
% Explain the choice of datasets, especially 4D-OR
\noindent\textbf{Visual Genome (VG).} VG is the most frequently used scene graph dataset and is considered as one of the main benchmarking datasets for SGG. As most previous works~\cite{teng_structured_2022}, we use a split of VG with 150 most frequent objects and 50 predicates. While conventionally PredCls, SGCls, and SGGen are used as metrics, none are applicable to the case of LF-SGG. Instead, we evaluate all approaches with our proposed metric, LF-SGGen, where the recall is calculated directly by matching and comparing the predicted scene graph to the ground truth scene graph.

\noindent\textbf{4D-OR.} 4D-OR is a recently published surgical scene graph dataset. Unlike VG, which is sparse in its annotations, 4D-OR includes dense annotations, enabling the calculation of precision in addition to recall. As it includes images from multiple views per scene, it allows us to demonstrate the extension of our method for multiple image inputs per scene. Finally, as the dataset size is an order of magnitude smaller than VG, it allows us to evaluate the performance of our method in lower data regimes. 

%------------------------------------------------------------------------
\subsection{Implementation details}
We use EfficientNet-B7~\cite{tan_efficientnet_2020} as the image encoder backbone for VG and EfficientNet-B5 in the case of 4D-OR, both implemented in \cite{wightman_pytorch_2019} and pretrained on Imagenet~\cite{russakovsky_imagenet_2015}. We resize the input images to 600x600 pixels for VG, and 456x456 for 4D-OR. In 4D-OR, the four multiview images per scene are processed individually by the feature extraction backbone, then the feature maps are flattened and concatenated to build the input sequence. We use pix2seq~\cite{chen_pix2seq_2022} as the starting point of autoregressive sequence modeling implementation.
We use the categorical cross-entropy loss, with the entire vocabulary as target classes, and optimize our model with AdamW~\cite{loshchilov_decoupled_2019}, with a constant learning rate of $4 \times 10^{-5}$ and a weight decay of $1 \times 10^{-4}$. The Batch size is set to 16 in all our experiments and we train our methods for 200 epochs, employing early stopping. We use a Transformer~\cite{vaswani_attention_2017}, with a hidden size of 256, eight attention heads, six encoding, and two decoding layers. Unless otherwise specified, we predict 300 relations using nucleus sampling~\cite{holtzman_curious_2020} with a p-value of 0.95 and pick the top K unique predictions for Recall@K. We set the branching factor $B$ of our proposed heuristic three-matching algorithm to 3 for all the validation experiments except when indicated otherwise. For the baseline methods on  visual genome, we use the implementations provided by~\cite{tang_scene_2020}. 



\subsection{Results}
\label{sec:results}
\begin{figure*}[t]
  \centering
   \includegraphics[width=0.9\linewidth]{gfx/qual_vg.pdf}
   \caption{Qualitative Results of Pix2SG of the Visual Genome dataset. Images and corresponding Ground Truth Scene Graphs are shown. Nodes and edges correctly predicted by our model are highlighted in green. Many correctly classified predicates are of geometrical nature even though our method does not include the localization task.}
   \label{fig:qual_vg}
\end{figure*}

\begin{figure*}[t]
  \centering
   \includegraphics[width=0.8\linewidth]{gfx/qual_4dor_2.pdf}
   \caption{Qualitative Result of Pix2SG on 4D-OR dataset. Images from two of the six viewing angles and the corresponding ground truth scene graphs are displayed. Nodes and edges correctly predicted by our model are highlighted in green.}
   \label{fig:qual_4dor}
\end{figure*}

\begin{table}
  \centering
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{ l | c  c | c c c}
  \toprule
    %& \multicolumn{2}{c|}{Supervision} & \multicolumn{3}{c}{LF-SGGen} \\
    %\midrule
    Model & SG & BBox & R@20 & R@50 & R@100 \\
    \midrule
    IMP~\cite{xu_scene_2017} & $\checkmark$  & $\checkmark$ & 21.66 & 30.78 & 37.07 \\
    MOTIFS~\cite{zellers_neural_2018} &  $\checkmark$  & $\checkmark$ & 29.02 & 38.08 & 43.64 \\
    Transformer~\cite{tang_unbiased_2020} & $\checkmark$  & $\checkmark$ & 28.79 & 37.81 & 43.69 \\
    VCTree~\cite{tang_learning_2019} & $\checkmark$  & $\checkmark$ & 27.06 & 35.59 & 41.21 \\
    SS-R-CNN~\cite{teng_structured_2022} & $\checkmark$  & $\checkmark$ & 22.09 & 26.43 & 28.57 \\
    RelTR~\cite{cong_reltr_2022} & $\checkmark$  & $\checkmark$ & 25.86 & 30.99 & 33.31 \\
    SGTR~\cite{li_sgtr_2022} & $\checkmark$  & $\checkmark$ & 23.62 & 30.38 & 34.85 \\
    \bottomrule
    Pix2SG(Ours) & $\checkmark$  &  & 21.51 & 24.81 & 26.66\\
    \bottomrule
  \end{tabular}
  }
  \caption{LF-SGGen results of different SG models at R@k on Visual Genome dataset. Checkmarks indicating the use of the supervision signal during model training.}
  \label{tab:vg_results}
\end{table}


\begin{table}
  \centering
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{ l | c  c | c c c}
  \toprule
    %& \multicolumn{2}{c|}{Supervision} & \multicolumn{3}{c}{LF-SGGen} \\
    %\midrule
    Model & SG & BBox & Prec. & Rec & F1 \\
    \midrule
    4D-OR SOTA~\cite{ozsoy_4d-or_2022} & $\checkmark$  & $\checkmark$ & 0.68 & 0.87 & 0.75 \\
    \bottomrule
    Pix2SG(Ours) &  $\checkmark$  & & 0.88 & 0.92 & 0.90 \\
    \bottomrule
  \end{tabular}
  }
  \caption{LF-SGGen results of 4D-OR method and Pix2SG on 4D-OR dataset. Checkmarks indicating the use of the supervision signal during model training.}
  \label{tab:4d_or_results}
\end{table}

\begin{figure}[t]
  \centering
  
   \includegraphics[width=0.75\linewidth]{gfx/attention_2.pdf}

   \caption{Visualization of the attention maps of Pix2SG on three images. While the subject attention seems to focus on a few entities, the object and predicate attentions tend to focus on the surroundings as well.}
   \label{fig:attention}
\end{figure}

\noindent\textbf{Visual Genome (VG).} As we introduce a new task and a new metric, we first reevaluate existing methods trained on the task of SGG with bounding boxes, with our LF-SGG evaluation method. While their reliance on bounding boxes makes them not directly comparable to the task of LF-SGG, we still provide these results as a rough but valuable guideline. We then evaluate our approach, Pix2SG, which is trained and evaluated without any bounding boxes. We present these results in \cref{tab:vg_results}. 
As we are the first and only method to not require location information at any stage, our method and results serve as the first baseline for the new task of LF-SGG. In addition to the quantitative results, we also provide qualitative results in \cref{fig:qual_vg}, and visualize the attention maps for three quintuple predictions \cref{fig:attention}. It can be seen that while some attention maps correspond highly with the entity locations such as for "bottle" and "man", for the "sidewalk" we see a more scattered attention. 

\noindent\textbf{4D-OR.} As the evaluation method proposed in 4D-OR~\cite{ozsoy_4d-or_2022} is applicable to our method, we can directly compare our method to the existing SOTA in \cref{tab:4d_or_results}. We significantly improve upon the SOTA, from 75\% F1 to 90\% F1. Importantly, we achieve this without using bounding boxes, depth, or 3D point clouds, which are all used by the existing SOTA. We provide qualitative results for 4D-OR in \cref{fig:qual_4dor}.

\subsection{Ablation Studies}
\label{sec:ablationstudies}
We perform ablation studies to validate the performance of our evaluation method, as well as to demonstrate the importance of nucleus sampling.

In \cref{fig:convergence_20}, we evaluate five different branching factors for our tree search-based matching on the four baseline methods, IMP, VCTree, Motifs, and Transformers. The results show that our algorithm converges at $B=3$, providing a good balance between speed and matching performance. We, therefore, set $B$ to 3 in all other experiments. 

In \cref{tab:sampling_results}, we show the importance of nucleus sampling by comparing it against always choosing the token with the highest probability. By reducing repetition and increasing variance, it leads to significantly higher Recall in all thresholds.

\begin{figure}[t]
  \centering

   \includegraphics[width=0.9\linewidth]{gfx/convergence_of_branching_algorithm.png}

   \caption{Ablation of Branching factor with $B=3$ providing a good trade-off between speed and matching performance.}
   \label{fig:convergence_20}
\end{figure}

\begin{table}
  \centering
  \begin{tabular}{l c c c}
  \toprule
    %&  \multicolumn{3}{c}{LF-SGGen} \\
    %\midrule
    Sampling & R@20 & R@50 & R@100 \\
    \midrule
    Maximum Likelihood & 19.05 & 21.18 & 23.19 \\
    Nucleus \cite{holtzman_curious_2020} & 21.51 & 24.81 & 26.66\\
    \bottomrule
  \end{tabular}
  \caption{Effect of nucleus sampling with a p-value of 0.95 on VG compared to conventional maximum likelihood selection for LF-SGGen.}
  \label{tab:sampling_results}
\end{table}


\subsection{Discussion \& Limitations}
The conventional scene graph annotation consists of subtasks, scene graph annotation and bounding box annotation. For the creation of the bounding box labels, the necessary time investment~\cite{su_crowdsourcing_2012} has been reported as 42 seconds for a single bounding box, which breaks down into drawing (25.5~sec), quality verification (9~sec), and coverage verification (7.8~sec).
This allows us to approximate the labeling effort for the dataset used in our experiments which are summarized in \cref{tab:annotation_effort}.
%To illustrate the overhead of the additional workload for the bounding box labels, we multiplied the required time for one bounding box annotation with the total amount of bounding boxes and summarized the results in \cref{tab:annotation_effort}.
The approximated additional workload to create the Bbox labels is significant for both datasets. While it would take a single person 276 days to create the bounding boxes for the 4D-OR dataset, for the Visual Genome dataset, the time required is 1993 days. Even though there exist many methods that can support annotators in improving their efficiency by a factor of 6-9x~\cite{su_crowdsourcing_2012}, this result strongly motivates the design of algorithms that can avoid the use of these costly additional annotations.

\begin{table}[ht]
  \centering
  \begin{tabular}{l r r}
  \toprule
    & Visual Genome & 4D-OR \\
    \midrule
    %Dataset & SG-target & BBox-target & N & SG-sum & BBox-sum & total \\
    \# of BBox & 1.3M & 189K \\
    %Number SG & 1,6 mio & 104k \\ 
    %\midrule
    hours total & 15,945 & 2,205 \\
    %\midrule
    person days & 1,993 & 276 \\
    %$@$8h/day\\
    %SG-time & & \\
    \bottomrule
  \end{tabular}
  \caption{Approximate annotation effort for bounding box labels with median annotation time per bounding box of 42~sec and 8h working days.}
  \label{tab:annotation_effort}
\end{table}

Finally, while we reach 74.12\% of the location-supervised SOTA~\cite{zellers_neural_2018,tang_unbiased_2020} performance on R@20 on the VG dataset (\cref{tab:vg_results}), our results also indicate that the location cues provide orthogonal information to scene graph labels beneficial for the SGG task. Therefore, this work does not advocate excluding location information when it is available. Instead, it presents a novel approach that enables the use of scene graphs even in settings where location information is not present.
