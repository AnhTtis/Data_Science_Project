\section{Methods} % 2.5-3 pages

In this section, we first introduce the new location-free scene graph generation task. To our knowledge, all existing SGG methods require bounding boxes in some parts of their pipeline, making them unsuitable for the LF-SGG task. Therefore, in~\cref{sec:prop_solution}, we present our novel architecture, specifically designed for LF-SGG, visualized in~\cref{fig:autoreg_sg}. Lastly, we design a heuristic tree search-based matching algorithm to enable objective evaluation on our new task.

\subsection{Problem Formulation}
\noindent
In this section, we introduce the location-free scene graph generation task. In this task, the input is an image $I$, and the goal is to produce a scene graph prediction $G = (V,E)$, where $V$ corresponds to the entity nodes, and $E$ to the pairwise relationships. For an objective evaluation the matching
\begin{equation} \label{graph_matching}
M\left( G,G' \right) = G_{m}
\end{equation}
 fits the prediction $G$ to the ground truth $G'$, to produce the mapped graph prediction $G_{m}$.
Recall@K $\mathcal{R}_K \left( G_{m},G' \right)$ is computed between $G_{m}$ and $G'$ to evaluate the task. With this formulation, unlike the conventional SGG task, the LF-SGG task refrains from using location information for both subject and object entities. The task of LF-SGG has increased difficulty compared to SGG, where the bounding box labels provide a strong supervision signal.

\subsection{Proposed Solution}
\label{sec:prop_solution}

\begin{figure}[t]
  \centering
  
   \includegraphics[width=0.95\linewidth]{gfx/sg_no_loc_2.pdf}

   \caption{Conversion of existing location-based Scene Graph Annotations to location-free Scene Graphs with instance identification and mapping to the graph representation.}
   \label{fig:sg_no_loc}
\end{figure}

\begin{figure*}[t]
  \centering
  
   \includegraphics[width=0.95\linewidth]{gfx/autoreg_sg_4.pdf}

   \caption{Pix2SG Architecture: An image encoder encodes the image as a feature map that is flattened and used as the input sequence to the Autoregressive Transformer module. The Autoregressive Transformer predicts the components of the scene graph, token by token, considering all its previous predictions until the output SG-sequence is completed.}
   \label{fig:autoreg_sg}
\end{figure*}

In this section, we introduce our proposed architecture, Pix2SG, the first location-free Scene Graph Generation method.
Inspired by Pix2Seq~\cite{chen_pix2seq_2022}, Pix2SG follows the autoregressive pattern, which is a well-known paradigm in natural language processing but so far has not been used for the task of Scene Graph Generation. We simultaneously predict objects as entity labels, entity instance indices, as well as pairwise relationships directly from a given image. Our model produces the entire output in a straight-forward, autoregressive fashion, producing one token after another, visualized in \cref{fig:autoreg_sg}. One advantage of this autoregressive formulation is that it allows our neural network to exploit the dependencies and relations between different tokens and triplets to improve its prediction. 

\noindent\textbf{Vocabulary.} To formulate scene graph generation as autoregressive sequence generation, we first define a vocabulary of tokens uniquely identifying all entities and predicates in the scene graph dataset. Each entity is represented by two tokens, where the first token represents the entity class and the second token the entity instance, identifying entities of the same class in one image. Additionally, we represent each predicate with a single token $pred_{cls}$. Therefore, a relation between two entities can be encoded using our vocabulary as a quintuple with a combination of five tokens, 
\begin{equation} \label{SceneGraphQuintupel} (sub_{cls},sub_{idx},obj_{cls},obj_{idx},pred_{cls}) \end{equation}
where $sub_{cls}$ and $obj_{cls}$ represent the entity class and $sub_{idx}$ and $obj_{idx}$ the instance ids of the subject and object. 

\noindent\textbf{Ground Truth Sequence Generation.} 
To train our proposed sequence generation model, we convert the scene graph labels $G'$ into a sequence of tokens from our vocabulary. This SG-sequence will be used in both the training and inference of our method.
We first convert the scene graph into individual quintuples as visualized in \cref{fig:sg_no_loc}, then randomly concatenate them to convert all SG-quintuples into a sequence to generate the SG-sequence. We assign ascending entity IDs to multiple instances of the same class, where the first appearance of a class in the sequence will have the instance id 0, the next one 1, and so on. We design our evaluation method to be invariant to the order of the triplets as well as to the order of the instance ids to eliminate any ambiguities during validation.

\noindent\textbf{Network.} Our approach, visualized in~\cref{fig:autoreg_sg}, first employs an image encoding step to generate a flattened representation of the image features encoding the visual information of the frame. In accordance with previous works~\cite{chen_pix2seq_2022}, we augment the flattened image information with a positional encoding which allows the model to identify the spatial location of each token in the feature map.

In the decoding step of the method, the flattened image feature and the start token are used as the first input to the decoder network to generate the first output token. Each predicted output token is subsequently appended to the token sequence that accumulates the predicted tokens. This token sequence is then again used as input to generate the next output token in an autoregressive manner. By the repetition of this autoregressive step, our method is generating an output sequence. This output sequence of tokens can then be effortlessly translated into the desired scene graph representation~(\cref{fig:sg_no_loc}).

\noindent\textbf{Inference.} 
During inference, the next predicted token of the SG-sequence should intuitively be selected using the confidence values of the prediction. However, we noticed that this can lead to unwanted repetitions of the same token in the SG-sequence. To address this problem, we propose the use of nucleus sampling~\cite{holtzman_curious_2020}, which introduces a limited amount of randomness for the selection of the next token. We noticed that we can generate the highest recall results for a p-value of 0.95. We always generate a fixed number of tokens and convert the results into an output of $N$ SG-quintuples (\cref{SceneGraphQuintupel}) with $N$ defining the number of total relations.

\subsection{Objective Evaluation Process}

\begin{figure*}[t]
  \centering
   \includegraphics[width=1\linewidth]{gfx/sg_matching.pdf}
   \caption{Illustration of the Scene Graph Matching problem. Ground Truth Scene Graph and Prediction have to be correctly matched for the evaluation. A suboptimal matching can obscure the actual model performance.}
   \label{fig:sg_matching_problem}
\end{figure*}

\begin{algorithm}
\caption{Heuristic Tree Search (HTS)} 
\textbf{Input:} gt graph $y$, predicted graph $\hat{y}$, branching $B$\\
\textbf{Output:} best mapping $m_{best}$ \\
\textbf{function} HTS($y, \hat{y}, B, m$)
\begin{algorithmic}
\State $y\_inst \gets$ instance from $y$ with highest node degree
\State $\hat{y}\_insts \gets $ instances from $\hat{y}$ with same class as $y\_inst$
\State $y\_nbhd \gets$ connected nodes and edges of $y\_inst$
\For{$\hat{y}\_inst$ in $\hat{y}\_insts$}
\State $\hat{y}\_nbhd \gets$ connected nodes and edges of $\hat{y}\_inst$
\State $overlaps \gets $ append $|y\_nbhd \cup \hat{y}\_nbhd| / |y\_nbhd|$
        \EndFor
    \State $\hat{y}\_insts \gets \hat{y}\_insts$ sorted by $overlaps$
        \State $M \gets \varnothing $  set for branched mappings %$m_i$
    \For{$i = 0$ : $B$}{~$m_i \gets m \cup (y\_inst\mapsto$ $\hat{y}\_insts[i]) $}
        \If{$y\backslash y\_inst == \varnothing$} $M \gets M \cup m_i$
        \Else{~$M \gets M \cup $ $\small\text{HTS}(y\backslash y\_inst, \hat{y}\backslash \hat{y}\_insts[i], B, m_i)$}    
        \EndIf
   \EndFor\\
\textbf{return} $M$

\end{algorithmic}
$m_{best} \gets$ select highest recall from $\small\text{HTS}(y, \hat{y}, B, \varnothing$)
\label{tree_based_matching}
\end{algorithm}

To facilitate an objective evaluation process for LF-SGG, scene graph matching has to be performed to calculate $M\left( G,G' \right) = G_{m}$ (\cref{graph_matching}). The evaluation of LF-SGGen has to be performed by only relying on the graph information. However, as shown in \cref{fig:sg_matching_problem}, matching graphs with each other is not straightforward, especially when multiple class instances are present in one scene. In this case, node identification has multiple possible solutions, and identifying the correct instance can only be done by considering the entire graph. Finding the optimal correspondence between prediction and ground truth involves calculating all possible combinations for the matching, maximizing the overlap of graph nodes and relationships. On the one hand, an exhaustive search over all possible matching combinations is too computationally expensive. On the other hand, the Hungarian matching algorithm~\cite{kuhn_hungarian_1955} cannot address the task appropriately, as the cost of an assignment can not necessarily be derived from the direct vicinity of a node.

Therefore, we propose a new algorithm shown in \cref{tree_based_matching} to approximate an exhaustive search with an emphasis on 1) matching quality, 2) computational efficiency, and 3) flexibility and configurability. Our matching algorithm $M^{\ast}\left( G,G',B \right) = G_{m}^{B}$ retrieves an approximate solution $G_{m}^{B}$ to the matching problem $M$ (eqn.\ref{graph_matching}). It is inspired by tree search-based algorithms. We define a branching factor $B$ to control the depth of our algorithmic tree search. By setting $B$ to 1, our algorithm performs greedy matching where iteratively, each predicted entity instance is mapped to only the most overlapping ground truth entity instance. To compute the overlap, we first get the respective local one-hop neighborhoods, represented as a list of (predicate, entity class) tuples. We count how many of these tuples are the same for both of them, where both the predicate and the entity classes have to match. This score is normalized by the node degree. For $B > 1$, the $B$ highest overlaps will be used to create new branches, representing multiple alternative mappings. These branches will be explored recursively until all the instances are mapped. For $B \geq N$, where $N$ is the number of instances of the most common entity class in a scene, our algorithm performs an exhaustive search to find the exact but computationally expensive solution. It holds $G_{m}^{B} \xrightarrow{B \to N} G_{m}$. To further optimize the performance, we start our tree search from the ground truth entity node with the highest node degree, arguing that this node will likely be a key component of our scene.

The result of our heuristic tree search is a set of graph matches between prediction and ground truth, and we subsequently select the match producing the highest evaluation metric as visualized in \cref{fig:sg_matching_problem}. We then use this matching to convert $G$ to $G_{m}$. We provide an efficient implementation of our algorithm in C++, with a sub-second runtime for most samples using three as Branching-factor $B$. We empirically motivate the choice of $B$ in \cref{sec:ablationstudies} and \cref{fig:convergence_20}. 

