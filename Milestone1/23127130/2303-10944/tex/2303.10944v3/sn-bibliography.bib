
@article{yu_learning_2020,
	title = {Learning from a tiny dataset of manual annotations: a teacher/student approach for surgical phase recognition},
	shorttitle = {Learning from a tiny dataset of manual annotations},
	url = {http://arxiv.org/abs/1812.00033},
	abstract = {Vision algorithms capable of interpreting scenes from a real-time video stream are necessary for computer-assisted surgery systems to achieve context-aware behavior. In laparoscopic procedures one particular algorithm needed for such systems is the identification of surgical phases, for which the current state of the art is a model based on a CNN-LSTM. A number of previous works using models of this kind have trained them in a fully supervised manner, requiring a fully annotated dataset. Instead, our work confronts the problem of learning surgical phase recognition in scenarios presenting scarce amounts of annotated data (under 25\% of all available video recordings). We propose a teacher/student type of approach, where a strong predictor called the teacher, trained beforehand on a small dataset of ground truth-annotated videos, generates synthetic annotations for a larger dataset, which another model - the student - learns from. In our case, the teacher features a novel CNN-biLSTM-CRF architecture, designed for offline inference only. The student, on the other hand, is a CNN-LSTM capable of making real-time predictions. Results for various amounts of manually annotated videos demonstrate the superiority of the new CNN-biLSTM-CRF predictor as well as improved performance from the CNN-LSTM trained using synthetic labels generated for unannotated videos. For both offline and online surgical phase recognition with very few annotated recordings available, this new teacher/student strategy provides a valuable performance improvement by efficiently leveraging the unannotated data.},
	urldate = {2022-02-04},
	journal = {arXiv:1812.00033 [cs, stat]},
	author = {Yu, Tong and Mutter, Didier and Marescaux, Jacques and Padoy, Nicolas},
	month = sep,
	year = {2020},
	note = {arXiv: 1812.00033},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, \#SKIMMED},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\SZL6GMF3\\Yu et al. - 2020 - Learning from a tiny dataset of manual annotations.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\H5TWRMHX\\1812.html:text/html},
}

@article{czempiel_tecno_2020,
	title = {{TeCNO}: {Surgical} {Phase} {Recognition} with {Multi}-{Stage} {Temporal} {Convolutional} {Networks}},
	shorttitle = {{TeCNO}},
	url = {http://arxiv.org/abs/2003.10751},
	abstract = {Automatic surgical phase recognition is a challenging and crucial task with the potential to improve patient safety and become an integral part of intra-operative decision-support systems. In this paper, we propose, for the first time in workflow analysis, a Multi-Stage Temporal Convolutional Network (MS-TCN) that performs hierarchical prediction refinement for surgical phase recognition. Causal, dilated convolutions allow for a large receptive field and online inference with smooth predictions even during ambiguous transitions. Our method is thoroughly evaluated on two datasets of laparoscopic cholecystectomy videos with and without the use of additional surgical tool information. Outperforming various state-of-the-art LSTM approaches, we verify the suitability of the proposed causal MS-TCN for surgical phase recognition.},
	urldate = {2022-02-04},
	journal = {arXiv:2003.10751 [cs, eess]},
	author = {Czempiel, Tobias and Paschali, Magdalini and Keicher, Matthias and Simson, Walter and Feussner, Hubertus and Kim, Seong Tae and Navab, Nassir},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.10751},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, \#TOSKIM, TCN},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\2WJAZZES\\Czempiel et al. - 2020 - TeCNO Surgical Phase Recognition with Multi-Stage.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\D84IJ2BE\\2003.html:text/html},
}

@incollection{martel_tecno_2020,
	address = {Cham},
	title = {{TeCNO}: {Surgical} {Phase} {Recognition} with {Multi}-stage {Temporal} {Convolutional} {Networks}},
	volume = {12263},
	isbn = {978-3-030-59715-3 978-3-030-59716-0},
	shorttitle = {{TeCNO}},
	url = {https://link.springer.com/10.1007/978-3-030-59716-0_33},
	language = {en},
	urldate = {2022-02-04},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2020},
	publisher = {Springer International Publishing},
	author = {Czempiel, Tobias and Paschali, Magdalini and Keicher, Matthias and Simson, Walter and Feussner, Hubertus and Kim, Seong Tae and Navab, Nassir},
	editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
	year = {2020},
	doi = {10.1007/978-3-030-59716-0_33},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {343--352},
	file = {Submitted Version:C\:\\Users\\felix\\Zotero\\storage\\PUSV6MTC\\Czempiel et al. - 2020 - TeCNO Surgical Phase Recognition with Multi-stage.pdf:application/pdf},
}

@article{liu_convnet_2022,
	title = {A {ConvNet} for the 2020s},
	url = {http://arxiv.org/abs/2201.03545},
	abstract = {The "Roaring 20s" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually "modernize" a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8\% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.},
	urldate = {2022-02-04},
	journal = {arXiv:2201.03545 [cs]},
	author = {Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
	month = jan,
	year = {2022},
	note = {arXiv: 2201.03545},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Convolutional neural network, \#READ, Transformers},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\N99KEIV3\\Liu et al. - 2022 - A ConvNet for the 2020s.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\UDG2DXYI\\2201.html:text/html},
}

@article{srivastav_unsupervised_2021,
	title = {Unsupervised domain adaptation for clinician pose estimation and instance segmentation in the {OR}},
	url = {http://arxiv.org/abs/2108.11801},
	abstract = {The fine-grained localization of clinicians in the operating room (OR) is a key component to design the new generation of OR support systems. Computer vision models for person pixel-based segmentation and body-keypoints detection are needed to better understand the clinical activities and the spatial layout of the OR. This is challenging, not only because OR images are very different from traditional vision datasets, but also because data and annotations are hard to collect and generate in the OR due to privacy concerns. To address these concerns, we first study how joint person pose estimation and instance segmentation can be performed on low resolutions images from 1x to 12x. Second, to address the domain shift and the lack of annotations, we propose a novel unsupervised domain adaptation method, called {\textbackslash}emph\{AdaptOR\}, to adapt a model from an {\textbackslash}emph\{in-the-wild\} labeled source domain to a statistically different unlabeled target domain. We propose to exploit explicit geometric constraints on the different augmentations of the unlabeled target domain image to generate accurate pseudo labels, and using these pseudo labels to train the model on high- and low-resolution OR images in a {\textbackslash}emph\{self-training\} framework. Furthermore, we propose {\textbackslash}emph\{disentangled feature normalization\} to handle the statistically different source and target domain data. Extensive experimental results with detailed ablation studies on the two OR datasets {\textbackslash}emph\{MVOR+\} and {\textbackslash}emph\{TUM-OR-test\} show the effectiveness of our approach against strongly constructed baselines, especially on the low-resolution privacy-preserving OR images. Finally, we show the generality of our method as a semi-supervised learning (SSL) method on the large-scale {\textbackslash}emph\{COCO\} dataset, where we achieve comparable results with as few as {\textbackslash}textbf\{1{\textbackslash}\%\} of labeled supervision against a model trained with 100{\textbackslash}\% labeled supervision.},
	urldate = {2022-02-04},
	journal = {arXiv:2108.11801 [cs]},
	author = {Srivastav, Vinkle and Gangi, Afshin and Padoy, Nicolas},
	month = sep,
	year = {2021},
	note = {arXiv: 2108.11801},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#NONE, Domain Adaption, Self-Training, Semi-Supervised Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\MJAJK7AA\\Srivastav et al. - 2021 - Unsupervised domain adaptation for clinician pose .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\ACUTI4WZ\\2108.html:text/html},
}

@article{srivastav_self-supervision_2020,
	title = {Self-supervision on {Unlabelled} {OR} {Data} for {Multi}-person {2D}/{3D} {Human} {Pose} {Estimation}},
	volume = {12261},
	url = {http://arxiv.org/abs/2007.08354},
	doi = {10.1007/978-3-030-59710-8_74},
	abstract = {2D/3D human pose estimation is needed to develop novel intelligent tools for the operating room that can analyze and support the clinical activities. The lack of annotated data and the complexity of state-of-the-art pose estimation approaches limit, however, the deployment of such techniques inside the OR. In this work, we propose to use knowledge distillation in a teacher/student framework to harness the knowledge present in a large-scale non-annotated dataset and in an accurate but complex multi-stage teacher network to train a lightweight network for joint 2D/3D pose estimation. The teacher network also exploits the unlabeled data to generate both hard and soft labels useful in improving the student predictions. The easily deployable network trained using this effective self-supervision strategy performs on par with the teacher network on {\textbackslash}emph\{MVOR+\}, an extension of the public MVOR dataset where all persons have been fully annotated, thus providing a viable solution for real-time 2D/3D human pose estimation in the OR.},
	urldate = {2022-02-04},
	journal = {arXiv:2007.08354 [cs]},
	author = {Srivastav, Vinkle and Gangi, Afshin and Padoy, Nicolas},
	year = {2020},
	note = {arXiv: 2007.08354},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, 3D, \#NONE, Human Pose Estimation},
	pages = {761--771},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\LT3DS923\\Srivastav et al. - 2020 - Self-supervision on Unlabelled OR Data for Multi-p.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\TAI7MHAX\\2007.html:text/html},
}

@article{czempiel_opera_2021,
	title = {{OperA}: {Attention}-{Regularized} {Transformers} for {Surgical} {Phase} {Recognition}},
	shorttitle = {{OperA}},
	url = {http://arxiv.org/abs/2103.03873},
	abstract = {In this paper we introduce OperA, a transformer-based model that accurately predicts surgical phases from long video sequences. A novel attention regularization loss encourages the model to focus on high-quality frames during training. Moreover, the attention weights are utilized to identify characteristic high attention frames for each surgical phase, which could further be used for surgery summarization. OperA is thoroughly evaluated on two datasets of laparoscopic cholecystectomy videos, outperforming various state-of-the-art temporal refinement approaches.},
	urldate = {2022-02-04},
	journal = {arXiv:2103.03873 [cs]},
	author = {Czempiel, Tobias and Paschali, Magdalini and Ostler, Daniel and Kim, Seong Tae and Busam, Benjamin and Navab, Nassir},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.03873},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, \#READ, Transformers},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\CCW4MJL8\\Czempiel et al. - 2021 - OperA Attention-Regularized Transformers for Surg.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\AT5Y5DCK\\2103.html:text/html},
}

@article{yengera_less_2018,
	title = {Less is {More}: {Surgical} {Phase} {Recognition} with {Less} {Annotations} through {Self}-{Supervised} {Pre}-training of {CNN}-{LSTM} {Networks}},
	shorttitle = {Less is {More}},
	url = {http://arxiv.org/abs/1805.08569},
	abstract = {Real-time algorithms for automatically recognizing surgical phases are needed to develop systems that can provide assistance to surgeons, enable better management of operating room (OR) resources and consequently improve safety within the OR. State-of-the-art surgical phase recognition algorithms using laparoscopic videos are based on fully supervised training. This limits their potential for widespread application, since creation of manual annotations is an expensive process considering the numerous types of existing surgeries and the vast amount of laparoscopic videos available. In this work, we propose a new self-supervised pre-training approach based on the prediction of remaining surgery duration (RSD) from laparoscopic videos. The RSD prediction task is used to pre-train a convolutional neural network (CNN) and long short-term memory (LSTM) network in an end-to-end manner. Our proposed approach utilizes all available data and reduces the reliance on annotated data, thereby facilitating the scaling up of surgical phase recognition algorithms to different kinds of surgeries. Additionally, we present EndoN2N, an end-to-end trained CNN-LSTM model for surgical phase recognition and evaluate the performance of our approach on a dataset of 120 Cholecystectomy laparoscopic videos (Cholec120). This work also presents the first systematic study of self-supervised pre-training approaches to understand the amount of annotations required for surgical phase recognition. Interestingly, the proposed RSD pre-training approach leads to performance improvement even when all the training data is manually annotated and outperforms the single pre-training approach for surgical phase recognition presently published in the literature. It is also observed that end-to-end training of CNN-LSTM networks boosts surgical phase recognition performance.},
	urldate = {2022-02-04},
	journal = {arXiv:1805.08569 [cs]},
	author = {Yengera, Gaurav and Mutter, Didier and Marescaux, Jacques and Padoy, Nicolas},
	month = may,
	year = {2018},
	note = {arXiv: 1805.08569},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#READ, CholecXXX, SSL},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\E7HPESWZ\\Yengera et al. - 2018 - Less is More Surgical Phase Recognition with Less.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\LT5ZQY8D\\1805.html:text/html},
}

@article{twinanda_endonet_2016,
	title = {{EndoNet}: {A} {Deep} {Architecture} for {Recognition} {Tasks} on {Laparoscopic} {Videos}},
	shorttitle = {{EndoNet}},
	url = {http://arxiv.org/abs/1602.03012},
	abstract = {Surgical workflow recognition has numerous potential medical applications, such as the automatic indexing of surgical video databases and the optimization of real-time operating room scheduling, among others. As a result, phase recognition has been studied in the context of several kinds of surgeries, such as cataract, neurological, and laparoscopic surgeries. In the literature, two types of features are typically used to perform this task: visual features and tool usage signals. However, the visual features used are mostly handcrafted. Furthermore, the tool usage signals are usually collected via a manual annotation process or by using additional equipment. In this paper, we propose a novel method for phase recognition that uses a convolutional neural network (CNN) to automatically learn features from cholecystectomy videos and that relies uniquely on visual information. In previous studies, it has been shown that the tool signals can provide valuable information in performing the phase recognition task. Thus, we present a novel CNN architecture, called EndoNet, that is designed to carry out the phase recognition and tool presence detection tasks in a multi-task manner. To the best of our knowledge, this is the first work proposing to use a CNN for multiple recognition tasks on laparoscopic videos. Extensive experimental comparisons to other methods show that EndoNet yields state-of-the-art results for both tasks.},
	urldate = {2022-02-04},
	journal = {arXiv:1602.03012 [cs]},
	author = {Twinanda, Andru P. and Shehata, Sherif and Mutter, Didier and Marescaux, Jacques and de Mathelin, Michel and Padoy, Nicolas},
	month = may,
	year = {2016},
	note = {arXiv: 1602.03012},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Dataset, \#NONE, CholecXXX},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\ZDH8Q88M\\Twinanda et al. - 2016 - EndoNet A Deep Architecture for Recognition Tasks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\N7YRJJ2G\\1602.html:text/html},
}

@misc{3blue1brown_mathematically_2022,
	title = {The mathematically optimal {Wordle} strategy},
	url = {https://www.youtube.com/watch?v=v68zYyaEmEA},
	abstract = {An excuse to teach a lesson on information theory and entropy.
Help fund future projects: https://www.patreon.com/3blue1brown​
Special thanks to these supporters: https://3b1b.co/thanks
An equally valuable form of support is to simply share the videos.

Note, the way I wrote the rules for coloring while doing this project differs slightly from the real Wordle when it comes to multiple letters. For example, suppose in a word like "woody" the first 'o' is correct, hence green, then in the real Wordle that second 'o' would be grey, whereas the way I wrote things the rule as simply any letter which is in the word somewhere, but not in the right position, will be yellow.

To be honest, even after realizing this differed from the proper rule, I stuck with it because it made the computation of the full matrix of word-combination patterns more elegant (and faster), and the normal rule has always slightly bothered me. Of course, it doesn't make any difference for the actual lesson here on entropy, which is the primary goal, and at least as I've gone back tried rerunning some of the models with the correct convention, it doesn't really change the final results.

Contents:
0:00 - What is Wordle?
2:43 - Initial ideas
8:04 - Information theory basics
18:15 - Incorporating word frequencies
27:49 - Final performance

Original wordle site:
https://www.powerlanguage.co.uk/wordle/

Music by Vincent Rubinetti.
https://www.vincentrubinetti.com/

Shannon and von Neumann artwork by Kurt Bruns.

Code for this video:
https://github.com/3b1b/videos/blob/m...

These animations are largely made using a custom python library, manim.  See the FAQ comments here:
https://www.3blue1brown.com/faq\#manim
https://github.com/3b1b/manim
https://github.com/ManimCommunity/manim/

You can find code for specific videos and projects here:
https://github.com/3b1b/videos/

------------------

3blue1brown is a channel about animating math, in all senses of the word animate.  And you know the drill with YouTube, if you want to stay posted on new videos, subscribe: http://3b1b.co/subscribe

Various social media stuffs:
Website: https://www.3blue1brown.com
Twitter: https://twitter.com/3blue1brown
Reddit: https://www.reddit.com/r/3blue1brown
Instagram: https://www.instagram.com/3blue1brown...
Patreon: https://patreon.com/3blue1brown
Facebook: https://www.facebook.com/3blue1brown},
	urldate = {2022-02-06},
	author = {{3Blue1Brown}},
	month = feb,
	year = {2022},
}

@article{wang_semi-supervised_2020,
	title = {{SEMI}-{SUPERVISED} {DEEP} {LEARNING} {WITH} {APPLICATIONS} {IN} {SURGICAL} {VIDEO} {ANALYSIS} {AND} {BIOINFORMATICS}},
	url = {https://www.semanticscholar.org/paper/SEMI-SUPERVISED-DEEP-LEARNING-WITH-APPLICATIONS-IN-Wang/e41271876ea38ae51c6fae7822e4c1b4b4542df5},
	abstract = {This thesis aims at proposing semi-supervised deep learning models to introduce unlabeled data into model training to get better model performance. SEMI-SUPERVISED DEEP LEARNING WITH APPLICATIONS IN SURGICAL VIDEO ANALYSIS AND BIOINFORMATICS SHENG WANG, Ph.D. The University of Texas at Arlington, 2020 Supervising Professor: Junzhou Huang In the current era of big data, deep learning has been the state-of-the-art model for various applications. Image-based applications such as image classification, object detection, image segmentation, benefit most from deep learning networks. One reason for the successful applications of deep learning is that there are a large number of labeled training samples for the model to learn from. People are interested in reducing the cost of getting labeled training samples, and there are various research going on with unsupervised, semi-supervised, and self-supervised deep learning. The cost of health-related data is even higher. Labeling the surgical videos with tools being used and surgical phase needs surgical related domain knowledge, it is not feasible to use general cloud labeling. Getting molecule properties even cost more since it usually needs expensive laboratory experiments. How to utilize the unlabeled data to improve the model performance attracts increasing research interests. In this thesis, we aim at proposing semi-supervised deep learning models to introduce unlabeled data into model training to get better model performance. Specifically, this thesis focuses},
	language = {en},
	urldate = {2022-02-08},
	journal = {undefined},
	author = {Wang, Sheng},
	year = {2020},
	keywords = {surgical data science, \#NONE, Semi-Supervised Learning},
	file = {Snapshot:C\:\\Users\\felix\\Zotero\\storage\\JF5JVFGH\\e41271876ea38ae51c6fae7822e4c1b4b4542df5.html:text/html;Wang - 2020 - SEMI-SUPERVISED DEEP LEARNING WITH APPLICATIONS IN.pdf:C\:\\Users\\felix\\Zotero\\storage\\3N4UJU2R\\Wang - 2020 - SEMI-SUPERVISED DEEP LEARNING WITH APPLICATIONS IN.pdf:application/pdf},
}

@article{sahu_surgical_2020,
	title = {Surgical phase recognition by learning phase transitions},
	volume = {6},
	issn = {2364-5504},
	url = {https://www.degruyter.com/document/doi/10.1515/cdbme-2020-0037/html},
	doi = {10.1515/cdbme-2020-0037},
	abstract = {Abstract
            Automatic recognition of surgical phases is an important component for developing an intra-operative context-aware system. Prior work in this area focuses on recognizing short-term tool usage patterns within surgical phases. However, the difference between intra- and inter-phase tool usage patterns has not been investigated for automatic phase recognition. We developed a Recurrent Neural Network (RNN), in particular a state-preserving Long Short Term Memory (LSTM) architecture to utilize the long-term evolution of tool usage within complete surgical procedures. For fully automatic tool presence detection from surgical video frames, a Convolutional Neural Network (CNN) based architecture namely ZIBNet is employed. Our proposed approach outperformed EndoNet by 8.1\% on overall precision for phase detection tasks and 12.5\% on meanAP for tool recognition tasks.},
	language = {en},
	number = {1},
	urldate = {2022-02-08},
	journal = {Current Directions in Biomedical Engineering},
	author = {Sahu, Manish and Szengel, Angelika and Mukhopadhyay, Anirban and Zachow, Stefan},
	month = sep,
	year = {2020},
	keywords = {\#TOSKIM, CholecXXX},
	pages = {20200037},
	file = {Full Text:C\:\\Users\\felix\\Zotero\\storage\\JAVZHISJ\\Sahu et al. - 2020 - Surgical phase recognition by learning phase trans.pdf:application/pdf},
}

@inproceedings{huang_improving_2020,
	address = {Seattle, WA, USA},
	title = {Improving {Action} {Segmentation} via {Graph}-{Based} {Temporal} {Reasoning}},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9156462/},
	doi = {10.1109/CVPR42600.2020.01404},
	abstract = {Temporal relations among multiple action segments play
an important role in action segmentation especially when
observations are limited (e.g., actions are occluded by other
objects or happen outside a field of view). In this paper, we
propose a network module called Graph-based Temporal
Reasoning Module (GTRM) that can be built on top of ex-
isting action segmentation models to learn the relation of
multiple action segments in various time spans. We model
the relations by using two Graph Convolution Networks
(GCNs) where each node represents an action segment. The
two graphs have different edge properties to account for
boundary regression and classification tasks, respectively.
By applying graph convolution, we can update each node’s
representation based on its relation with neighboring nodes.
The updated representation is then used for improved action segmentation. We evaluate our model on the challenging egocentric datasets namely EGTEA and EPIC-Kitchens, where actions may be partially observed due to the viewpoint restriction. The results show that our proposed GTRM outperforms state-of-the-art action segmentation models by a large margin. We also demonstrate the effectiveness of our model on two third-person video datasets, the 50Salads dataset and the Breakfast dataset.},
	urldate = {2022-02-14},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Huang, Yifei and Sugano, Yusuke and Sato, Yoichi},
	month = jun,
	year = {2020},
	keywords = {GDL, Reasoning, \#TOSKIM, Action Segmentation},
	pages = {14021--14031},
	file = {Huang et al. - 2020 - Improving Action Segmentation via Graph-Based Temp.pdf:C\:\\Users\\felix\\Zotero\\storage\\CL4RIYPE\\Huang et al. - 2020 - Improving Action Segmentation via Graph-Based Temp.pdf:application/pdf},
}

@article{wald_learning_2020,
	title = {Learning {3D} {Semantic} {Scene} {Graphs} from {3D} {Indoor} {Reconstructions}},
	url = {http://arxiv.org/abs/2004.03967},
	abstract = {Scene understanding has been of high interest in computer vision. It encompasses not only identifying objects in a scene, but also their relationships within the given context. With this goal, a recent line of works tackles 3D semantic segmentation and scene layout prediction. In our work we focus on scene graphs, a data structure that organizes the entities of a scene in a graph, where objects are nodes and their relationships modeled as edges. We leverage inference on scene graphs as a way to carry out 3D scene understanding, mapping objects and their relationships. In particular, we propose a learned method that regresses a scene graph from the point cloud of a scene. Our novel architecture is based on PointNet and Graph Convolutional Networks (GCN). In addition, we introduce 3DSSG, a semiautomatically generated dataset, that contains semantically rich scene graphs of 3D scenes. We show the application of our method in a domain-agnostic retrieval task, where graphs serve as an intermediate representation for 3D-3D and 2D-3D matching.},
	language = {en},
	urldate = {2022-02-17},
	journal = {arXiv:2004.03967 [cs]},
	author = {Wald, Johanna and Dhamo, Helisa and Navab, Nassir and Tombari, Federico},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.03967},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Semantic Scene Graph, 3D, \#NONE},
	file = {Wald et al. - 2020 - Learning 3D Semantic Scene Graphs from 3D Indoor R.pdf:C\:\\Users\\felix\\Zotero\\storage\\7UE4GE2Q\\Wald et al. - 2020 - Learning 3D Semantic Scene Graphs from 3D Indoor R.pdf:application/pdf},
}

@article{wald_learning_2022,
	title = {Learning {3D} {Semantic} {Scene} {Graphs} with {Instance} {Embeddings}},
	issn = {0920-5691, 1573-1405},
	url = {https://link.springer.com/10.1007/s11263-021-01546-9},
	doi = {10.1007/s11263-021-01546-9},
	abstract = {A 3D scene is more than the geometry and classes of the objects it comprises. An essential aspect beyond object-level perception is the scene context, described as a dense semantic network of interconnected nodes. Scene graphs have become a common representation to encode the semantic richness of images, where nodes in the graph are object entities connected by edges, so-called relationships. Such graphs have been shown to be useful in achieving state-of-the-art performance in image captioning, visual question answering and image generation or editing. While scene graph prediction methods so far focused on images, we propose instead a novel neural network architecture for 3D data, where the aim is to learn to regress semantic graphs from a given 3D scene. With this work, we go beyond object-level perception, by exploring relations between object entities. Our method learns instance embeddings alongside a scene segmentation and is able to predict semantics for object nodes and edges. We leverage 3DSSG, a large scale dataset based on 3RScan that features scene graphs of changing 3D scenes. Finally, we show the effectiveness of graphs as an intermediate representation on a retrieval task.},
	language = {en},
	urldate = {2022-02-17},
	journal = {International Journal of Computer Vision},
	author = {Wald, Johanna and Navab, Nassir and Tombari, Federico},
	month = jan,
	year = {2022},
	keywords = {3D, \#SKIMMED, Scene Graphs},
	file = {Wald et al. - 2022 - Learning 3D Semantic Scene Graphs with Instance Em.pdf:C\:\\Users\\felix\\Zotero\\storage\\IJTEHWKT\\Wald et al. - 2022 - Learning 3D Semantic Scene Graphs with Instance Em.pdf:application/pdf},
}

@article{lu_visual_2016,
	title = {Visual {Relationship} {Detection} with {Language} {Priors}},
	url = {http://arxiv.org/abs/1608.00187},
	abstract = {Visual relationships capture a wide variety of interactions between pairs of objects in images (e.g. “man riding bicycle” and “man pushing bicycle”). Consequently, the set of possible relationships is extremely large and it is diﬃcult to obtain suﬃcient training examples for all possible relationships. Because of this limitation, previous work on visual relationship detection has concentrated on predicting only a handful of relationships. Though most relationships are infrequent, their objects (e.g. “man” and “bicycle”) and predicates (e.g. “riding” and “pushing”) independently occur more frequently. We propose a model that uses this insight to train visual models for objects and predicates individually and later combines them together to predict multiple relationships per image. We improve on prior work by leveraging language priors from semantic word embeddings to ﬁnetune the likelihood of a predicted relationship. Our model can scale to predict thousands of types of relationships from a few examples. Additionally, we localize the objects in the predicted relationships as bounding boxes in the image. We further demonstrate that understanding relationships can improve content based image retrieval.},
	language = {en},
	urldate = {2022-02-17},
	journal = {arXiv:1608.00187 [cs]},
	author = {Lu, Cewu and Krishna, Ranjay and Bernstein, Michael and Fei-Fei, Li},
	month = jul,
	year = {2016},
	note = {arXiv: 1608.00187},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#TOSKIM, Scene Graphs},
	file = {Lu et al. - 2016 - Visual Relationship Detection with Language Priors.pdf:C\:\\Users\\felix\\Zotero\\storage\\JNM6U6K8\\Lu et al. - 2016 - Visual Relationship Detection with Language Priors.pdf:application/pdf},
}

@article{teng_target_2021,
	title = {Target {Adaptive} {Context} {Aggregation} for {Video} {Scene} {Graph} {Generation}},
	url = {http://arxiv.org/abs/2108.08121},
	abstract = {This paper deals with a challenging task of video scene graph generation (VidSGG), which could serve as a structured video representation for high-level understanding tasks. We present a new detect-to-track paradigm for this task by decoupling the context modeling for relation prediction from the complicated low-level entity tracking. Speciﬁcally, we design an efﬁcient method for frame-level VidSGG, termed as Target Adaptive Context Aggregation Network (TRACE), with a focus on capturing spatio-temporal context information for relation recognition. Our TRACE framework streamlines the VidSGG pipeline with a modular design, and presents two unique blocks of Hierarchical Relation Tree (HRTree) construction and Target-adaptive Context Aggregation. More speciﬁc, our HRTree ﬁrst provides an adpative structure for organizing possible relation candidates efﬁciently, and guides context aggregation module to effectively capture spatio-temporal structure information. Then, we obtain a contextualized feature representation for each relation candidate and build a classiﬁcation head to recognize its relation category. Finally, we provide a simple temporal association strategy to track TRACE detected results to yield the video-level VidSGG. We perform experiments on two VidSGG benchmarks: ImageNetVidVRD and Action Genome, and the results demonstrate that our TRACE achieves the state-of-the-art performance. The code and models are made available at https:// github.com/MCG-NJU/TRACE.},
	language = {en},
	urldate = {2022-02-17},
	journal = {arXiv:2108.08121 [cs]},
	author = {Teng, Yao and Wang, Limin and Li, Zhifeng and Wu, Gangshan},
	month = aug,
	year = {2021},
	note = {arXiv: 2108.08121},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Video Scene Graph Generation, \#TOREAD, GDLMA 2022, Scene Graphs},
	file = {Teng et al. - 2021 - Target Adaptive Context Aggregation for Video Scen.pdf:C\:\\Users\\felix\\Zotero\\storage\\9RWEHNRU\\Teng et al. - 2021 - Target Adaptive Context Aggregation for Video Scen.pdf:application/pdf},
}

@article{cong_spatial-temporal_2021,
	title = {Spatial-{Temporal} {Transformer} for {Dynamic} {Scene} {Graph} {Generation}},
	url = {http://arxiv.org/abs/2107.12309},
	abstract = {Dynamic scene graph generation aims at generating a scene graph of the given video. Compared to the task of scene graph generation from images, it is more challenging because of the dynamic relationships between objects and the temporal dependencies between frames allowing for a richer semantic interpretation. In this paper, we propose Spatial-temporal Transformer (STTran), a neural network that consists of two core modules: (1) a spatial encoder that takes an input frame to extract spatial context and reason about the visual relationships within a frame, and (2) a temporal decoder which takes the output of the spatial encoder as input in order to capture the temporal dependencies between frames and infer the dynamic relationships. Furthermore, STTran is ﬂexible to take varying lengths of videos as input without clipping, which is especially important for long videos. Our method is validated on the benchmark dataset Action Genome (AG). The experimental results demonstrate the superior performance of our method in terms of dynamic scene graphs. Moreover, a set of ablative studies is conducted and the effect of each proposed module is justiﬁed. Code available at: https://github.com/yrcong/STTran.},
	language = {en},
	urldate = {2022-02-17},
	journal = {arXiv:2107.12309 [cs]},
	author = {Cong, Yuren and Liao, Wentong and Ackermann, Hanno and Rosenhahn, Bodo and Yang, Michael Ying},
	month = aug,
	year = {2021},
	note = {arXiv: 2107.12309},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#READ, GDLMA 2022},
	file = {Cong et al. - 2021 - Spatial-Temporal Transformer for Dynamic Scene Gra.pdf:C\:\\Users\\felix\\Zotero\\storage\\X8IV28WJ\\Cong et al. - 2021 - Spatial-Temporal Transformer for Dynamic Scene Gra.pdf:application/pdf},
}

@article{nwoye_rendezvous_2021,
	title = {Rendezvous: {Attention} {Mechanisms} for the {Recognition} of {Surgical} {Action} {Triplets} in {Endoscopic} {Videos}.},
	abstract = {Out of all existing frameworks for surgical workflow analysis in endoscopic videos, action triplet recognition stands out as the only one aiming to provide truly fine-grained and comprehensive information on surgical activities. This information, presented as   combinations, is highly challenging to be accurately identified. Triplet components can be difficult to recognize individually; in this task, it requires not only performing recognition simultaneously for all three triplet components, but also correctly establishing the data association between them. To achieve this task, we introduce our new model, the Rendezvous (RDV), which recognizes triplets directly from surgical videos by leveraging attention at two different levels. We first introduce a new form of spatial attention to capture individual action triplet components in a scene; called the Class Activation Guided Attention Mechanism (CAGAM). This technique focuses on the recognition of verbs and targets using activations resulting from instruments. To solve the association problem, our RDV model adds a new form of semantic attention inspired by Transformer networks. Using multiple heads of cross and self attentions, RDV is able to effectively capture relationships between instruments, verbs, and targets. We also introduce CholecT50 - a dataset of 50 endoscopic videos in which every frame has been annotated with labels from 100 triplet classes. Our proposed RDV model significantly improves the triplet prediction mAP by over 9\% compared to the state-of-the-art methods on this dataset.},
	journal = {arXiv: Computer Vision and Pattern Recognition},
	author = {Nwoye, Chinedu Innocent and Yu, Tong and Gonzalez, Cristians and Seeliger, Barbara and Mascagni, Pietro and Mutter, Didier and Marescaux, Jacques and Padoy, Nicolas},
	month = sep,
	year = {2021},
	note = {MAG ID: 3198379050},
}

@article{wang_graph_2019,
	title = {Graph {Convolutional} {Nets} for {Tool} {Presence} {Detection} in {Surgical} {Videos}},
	doi = {10.1007/978-3-030-20351-1_36},
	abstract = {Surgical tool presence detection is one of the key problems in automatic surgical video content analysis. Solving this problem benefits many applications such as the evaluation of surgical instrument usage and automatic surgical report generation. Given the fact that each video is only sparsely labeled at the frame level, meaning that only a small portion of video frames will be properly labeled, existing approaches only model this problem as an image (frame) classification problem without considering temporal information in surgical videos. In this paper, we propose a deep neural network model utilizing both spatial and temporal information from surgical videos for surgical tool presence detection. The proposed model uses Graph Convolutional Networks (GCNs) along the temporal dimension to learn better features by considering the relationship between continuous video frames. To the best of our knowledge, this is the first work taking videos as input to solve the surgical tool presence detection problem. Our experiments demonstrate the employment of temporal information offers a significant improvement to this problem, and the proposed approach achieves better performance than all state-of-the-art methods.},
	author = {Wang, Sheng and Xu, Zheng and Yan, Chaochao and Huang, Junzhou},
	month = jun,
	year = {2019},
	doi = {10.1007/978-3-030-20351-1_36},
	note = {MAG ID: 2946750843},
	keywords = {GCN, \#TOSKIM, Tool Detection},
	pages = {467--478},
	file = {Wang et al. - 2019 - Graph Convolutional Nets for Tool Presence Detecti.pdf:C\:\\Users\\felix\\Zotero\\storage\\5KLPTL4Q\\Wang et al. - 2019 - Graph Convolutional Nets for Tool Presence Detecti.pdf:application/pdf},
}

@inproceedings{cao_medical_2021,
	address = {Houston, TX, USA},
	title = {Medical {Scene} {Graphs} and {Reasoning}},
	isbn = {978-1-66540-126-5},
	url = {https://ieeexplore.ieee.org/document/9669882/},
	doi = {10.1109/BIBM52615.2021.9669882},
	urldate = {2022-02-18},
	booktitle = {2021 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	publisher = {IEEE},
	author = {Cao, Chuxue and He, Yiming and Chen, Yuzhen and Song, Chunli and Ling, Hao and Guan, Renchu and Feng, Xiaoyue},
	month = dec,
	year = {2021},
	keywords = {\#NONE},
	pages = {645--648},
	file = {Cao et al. - 2021 - Medical Scene Graphs and Reasoning.pdf:C\:\\Users\\felix\\Zotero\\storage\\74QD8DES\\Cao et al. - 2021 - Medical Scene Graphs and Reasoning.pdf:application/pdf},
}

@inproceedings{jalal_surgical_2021,
	address = {Cham},
	series = {{IFMBE} {Proceedings}},
	title = {Surgical {Tool} {Detection} in {Laparoscopic} {Videos} by {Modeling} {Temporal} {Dependencies} {Between} {Adjacent} {Frames}},
	isbn = {978-3-030-64610-3},
	doi = {10.1007/978-3-030-64610-3_117},
	abstract = {Video-based surgical tool detection is an important yet challenging problem for developing context-aware systems (CASs) in the operating theatre. In this paper, we address tool presence detection in laparoscopic videos using a combination of convolutional neural network (CNN) and Long-short term memory (LSTM) network. Firstly, a pre-trained CNN model was fine-tuned to learn visual features from laparoscopic images. Since the data is sparsely labelled, an LSTM network was then employed to learn temporal dependencies from short sequences of adjacent frames. Several experiments have been conducted with the Cholec80 dataset to validate the proposed framework and investigate the effect of the video clip length on tool prediction performance. Results demonstrate the advantage of employing temporal information to the tool detection task and show the most notable improvement (mAP of 91.1\%) is achieved when sequences of previous and next frames were employed.},
	language = {en},
	booktitle = {8th {European} {Medical} and {Biological} {Engineering} {Conference}},
	publisher = {Springer International Publishing},
	author = {Jalal, N. A. and Abdulbaki Alshirbaji, T. and Docherty, P. D. and Neumuth, T. and Moeller, K.},
	editor = {Jarm, Tomaz and Cvetkoska, Aleksandra and Mahnič-Kalamiza, Samo and Miklavcic, Damijan},
	year = {2021},
	keywords = {Convolutional neural network, Endoscopic video, LSTM, Surgical tool detection, \#TOSKIM},
	pages = {1045--1052},
	file = {Jalal et al. - 2021 - Surgical Tool Detection in Laparoscopic Videos by .pdf:C\:\\Users\\felix\\Zotero\\storage\\H4E5K6DD\\Jalal et al. - 2021 - Surgical Tool Detection in Laparoscopic Videos by .pdf:application/pdf},
}

@article{seenivasan_global-reasoned_2022,
	title = {Global-{Reasoned} {Multi}-{Task} {Learning} {Model} for {Surgical} {Scene} {Understanding}},
	volume = {7},
	issn = {2377-3766, 2377-3774},
	url = {http://arxiv.org/abs/2201.11957},
	doi = {10.1109/LRA.2022.3146544},
	abstract = {Global and local relational reasoning enable scene understanding models to perform human-like scene analysis and understanding. Scene understanding enables better semantic segmentation and object-to-object interaction detection. In the medical domain, a robust surgical scene understanding model allows the automation of surgical skill evaluation, real-time monitoring of surgeon's performance and post-surgical analysis. This paper introduces a globally-reasoned multi-task surgical scene understanding model capable of performing instrument segmentation and tool-tissue interaction detection. Here, we incorporate global relational reasoning in the latent interaction space and introduce multi-scale local (neighborhood) reasoning in the coordinate space to improve segmentation. Utilizing the multi-task model setup, the performance of the visual-semantic graph attention network in interaction detection is further enhanced through global reasoning. The global interaction space features from the segmentation module are introduced into the graph network, allowing it to detect interactions based on both node-to-node and global interaction reasoning. Our model reduces the computation cost compared to running two independent single-task models by sharing common modules, which is indispensable for practical applications. Using a sequential optimization technique, the proposed multi-task model outperforms other state-of-the-art single-task models on the MICCAI endoscopic vision challenge 2018 dataset. Additionally, we also observe the performance of the multi-task model when trained using the knowledge distillation technique. The official code implementation is made available in GitHub.},
	number = {2},
	urldate = {2022-02-18},
	journal = {IEEE Robotics and Automation Letters},
	author = {Seenivasan, Lalithkumar and Mitheran, Sai and Islam, Mobarakol and Ren, Hongliang},
	month = apr,
	year = {2022},
	note = {arXiv: 2201.11957},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing, Computer Science - Robotics, \#READ, Reasoning, GDLMA 2022},
	pages = {3858--3865},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\8X3L6JKS\\Seenivasan et al. - 2022 - Global-Reasoned Multi-Task Learning Model for Surg.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\BZSRYKZH\\2201.html:text/html},
}

@article{liang_visual-semantic_2021,
	title = {Visual-{Semantic} {Graph} {Attention} {Networks} for {Human}-{Object} {Interaction} {Detection}},
	url = {http://arxiv.org/abs/2001.02302},
	abstract = {In scene understanding, robotics benefit from not only detecting individual scene instances but also from learning their possible interactions. Human-Object Interaction (HOI) Detection infers the action predicate on a {\textless}human, predicate, object{\textgreater} triplet. Contextual information has been found critical in inferring interactions. However, most works only use local features from single human-object pair for inference. Few works have studied the disambiguating contribution of subsidiary relations made available via graph networks. Similarly, few have learned to effectively leverage visual cues along with the intrinsic semantic regularities contained in HOIs. We contribute a dual-graph attention network that effectively aggregates contextual visual, spatial, and semantic information dynamically from primary human-object relations as well as subsidiary relations through attention mechanisms for strong disambiguating power. We achieve comparable results on two benchmarks: V-COCO and HICO-DET. Code is available at {\textbackslash}url\{https://github.com/birlrobotics/vs-gats\}.},
	urldate = {2022-02-18},
	journal = {arXiv:2001.02302 [cs, eess]},
	author = {Liang, Zhijun and Rojas, Juan and Liu, Junfa and Guan, Yisheng},
	month = mar,
	year = {2021},
	note = {arXiv: 2001.02302
version: 3},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, GDL, \#TOSKIM, Attention, HOI},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\35DTQE5N\\Liang et al. - 2021 - Visual-Semantic Graph Attention Networks for Human.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\PJRVE6Y4\\2001.html:text/html},
}

@inproceedings{nwoye_recognition_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Recognition of {Instrument}-{Tissue} {Interactions} in {Endoscopic} {Videos} via {Action} {Triplets}},
	isbn = {978-3-030-59716-0},
	doi = {10.1007/978-3-030-59716-0_35},
	abstract = {Recognition of surgical activity is an essential component to develop context-aware decision support for the operating room. In this work, we tackle the recognition of fine-grained activities, modeled as action triplets ⟨instrument,verb,target⟩⟨instrument,verb,target⟩{\textbackslash}langle instrument, verb, target {\textbackslash}rangle representing the tool activity. To this end, we introduce a new laparoscopic dataset, CholecT40, consisting of 40 videos from the public dataset Cholec80 in which all frames have been annotated using 128 triplet classes. Furthermore, we present an approach to recognize these triplets directly from the video data. It relies on a module called class activation guide, which uses the instrument activation maps to guide the verb and target recognition. To model the recognition of multiple triplets in the same frame, we also propose a trainable 3D interaction space, which captures the associations between the triplet components. Finally, we demonstrate the significance of these contributions via several ablation studies and comparisons to baselines on CholecT40.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2020},
	publisher = {Springer International Publishing},
	author = {Nwoye, Chinedu Innocent and Gonzalez, Cristians and Yu, Tong and Mascagni, Pietro and Mutter, Didier and Marescaux, Jacques and Padoy, Nicolas},
	editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
	year = {2020},
	keywords = {Endoscopic video, CholecT40, Deep learning, Surgical activity recognition, Tool-tissue interaction, \#READ, Action Triplets},
	pages = {364--374},
	file = {Submitted Version:C\:\\Users\\felix\\Zotero\\storage\\CJJHNSXJ\\Nwoye et al. - 2020 - Recognition of Instrument-Tissue Interactions in E.pdf:application/pdf},
}

@article{vercauteren_cai4cai_2020,
	title = {{CAI4CAI}: {The} {Rise} of {Contextual} {Artificial} {Intelligence} in {Computer} {Assisted} {Interventions}},
	volume = {108},
	issn = {0018-9219},
	shorttitle = {{CAI4CAI}},
	doi = {10.1109/JPROC.2019.2946993},
	abstract = {Data-driven computational approaches have evolved to enable extraction of information from medical images with a reliability, accuracy and speed which is already transforming their interpretation and exploitation in clinical practice. While similar benefits are longed for in the field of interventional imaging, this ambition is challenged by a much higher heterogeneity. Clinical workflows within interventional suites and operating theatres are extremely complex and typically rely on poorly integrated intra-operative devices, sensors, and support infrastructures. Taking stock of some of the most exciting developments in machine learning and artificial intelligence for computer assisted interventions, we highlight the crucial need to take context and human factors into account in order to address these challenges. Contextual artificial intelligence for computer assisted intervention, or CAI4CAI, arises as an emerging opportunity feeding into the broader field of surgical data science. Central challenges being addressed in CAI4CAI include how to integrate the ensemble of prior knowledge and instantaneous sensory information from experts, sensors and actuators; how to create and communicate a faithful and actionable shared representation of the surgery among a mixed human-AI actor team; how to design interventional systems and associated cognitive shared control schemes for online uncertainty-aware collaborative decision making ultimately producing more precise and reliable interventions.},
	language = {eng},
	number = {1},
	journal = {Proceedings of the IEEE. Institute of Electrical and Electronics Engineers},
	author = {Vercauteren, Tom and Unberath, Mathias and Padoy, Nicolas and Navab, Nassir},
	month = jan,
	year = {2020},
	pmid = {31920208},
	pmcid = {PMC6952279},
	keywords = {computer assisted interventions, context-aware user interface, data fusion, interventional workflow, intra-operative imaging, surgical data science, surgical planning, surgical scene understanding, \#TOSKIM, Review, Machine Learning, AI},
	pages = {198--214},
	file = {Full Text:C\:\\Users\\felix\\Zotero\\storage\\ZXCPTP8G\\Vercauteren et al. - 2020 - CAI4CAI The Rise of Contextual Artificial Intelli.pdf:application/pdf},
}

@inproceedings{islam_learning_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Learning and {Reasoning} with the {Graph} {Structure} {Representation} in {Robotic} {Surgery}},
	isbn = {978-3-030-59716-0},
	doi = {10.1007/978-3-030-59716-0_60},
	abstract = {Learning to infer graph representations and performing spatial reasoning in a complex surgical environment can play a vital role in surgical scene understanding in robotic surgery. For this purpose, we develop an approach to generate the scene graph and predict surgical interactions between instruments and surgical region of interest (ROI) during robot-assisted surgery. We design an attention link function and integrate with a graph parsing network to recognize the surgical interactions. To embed each node with corresponding neighbouring node features, we further incorporate SageConv into the network. The scene graph generation and active edge classification mostly depend on the embedding or feature extraction of node and edge features from complex image representation. Here, we empirically demonstrate the feature extraction methods by employing label smoothing weighted loss. Smoothing the hard label can avoid the over-confident prediction of the model and enhances the feature representation learned by the penultimate layer. To obtain the graph scene label, we annotate the bounding box and the instrument-ROI interactions on the robotic scene segmentation challenge 2018 dataset with an experienced clinical expert in robotic surgery and employ it to evaluate our propositions.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2020},
	publisher = {Springer International Publishing},
	author = {Islam, Mobarakol and Seenivasan, Lalithkumar and Ming, Lim Chwee and Ren, Hongliang},
	editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
	year = {2020},
	keywords = {GDL, \#READ, GDLMA 2022, Scene Graphs},
	pages = {627--636},
	file = {Submitted Version:C\:\\Users\\felix\\Zotero\\storage\\9Q67JENU\\Islam et al. - 2020 - Learning and Reasoning with the Graph Structure Re.pdf:application/pdf},
}

@article{pakhomov_deep_2017,
	title = {Deep {Residual} {Learning} for {Instrument} {Segmentation} in {Robotic} {Surgery}},
	abstract = {Detection, tracking, and pose estimation of surgical instruments are crucial tasks for computer assistance during minimally invasive robotic surgery. In the majority of cases, the first step is the automatic segmentation of surgical tools. Prior work has focused on binary segmentation, where the objective is to label every pixel in an image as tool or background. We improve upon previous work in two major ways. First, we leverage recent techniques such as deep residual learning and dilated convolutions to advance binary-segmentation performance. Second, we extend the approach to multi-class segmentation, which lets us segment different parts of the tool, in addition to background. We demonstrate the performance of this method on the MICCAI Endoscopic Vision Challenge Robotic Instruments dataset.},
	journal = {arXiv: Computer Vision and Pattern Recognition},
	author = {Pakhomov, Daniil and Premachandran, Vittal and Allan, Max and {Mahdi Azizian} and Navab, Nassir},
	month = mar,
	year = {2017},
	note = {MAG ID: 2603501458},
	keywords = {\#TOSKIM, Tracking},
}

@article{laina_concurrent_2017,
	title = {Concurrent {Segmentation} and {Localization} for {Tracking} of {Surgical} {Instruments}},
	abstract = {Real-time instrument tracking is a crucial requirement for various computer-assisted interventions. In order to overcome problems such as specular reflections and motion blur, we propose a novel method that takes advantage of the interdependency between localization and segmentation of the surgical tool. In particular, we reformulate the 2D instrument pose estimation as heatmap regression and thereby enable a concurrent, robust and near real-time regression of both tasks via deep learning. As demonstrated by our experimental results, this modeling leads to a significantly improved performance than directly regressing the tool position and allows our method to outperform the state of the art on a Retinal Microsurgery benchmark and the MICCAI EndoVis Challenge 2015.},
	journal = {arXiv: Computer Vision and Pattern Recognition},
	author = {Laina, Iro and Rieke, Nicola and Rupprecht, Christian and Vizcaíno, Josué Page and Eslami, Abouzar and Tombari, Federico and Navab, Nassir},
	month = mar,
	year = {2017},
	note = {MAG ID: 2952028771},
	keywords = {\#TOSKIM, Tracking},
}

@misc{bresson_week_2020,
	title = {Week 13 – {Lecture}: {Graph} {Convolutional} {Networks} ({GCNs}) - {YouTube}},
	url = {https://www.youtube.com/watch?v=Iiv9R6BjxHM&t=2459s},
	urldate = {2022-02-21},
	author = {Bresson, Xavier and LeCun, Yann},
	year = {2020},
	keywords = {\#READ},
	file = {Week 13 – Lecture\: Graph Convolutional Networks (GCNs) - YouTube:C\:\\Users\\felix\\Zotero\\storage\\XSLH66QH\\watch.html:text/html},
}

@article{bruna_spectral_2014,
	title = {Spectral {Networks} and {Locally} {Connected} {Networks} on {Graphs}},
	url = {http://arxiv.org/abs/1312.6203},
	abstract = {Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.},
	urldate = {2022-02-21},
	journal = {arXiv:1312.6203 [cs]},
	author = {Bruna, Joan and Zaremba, Wojciech and Szlam, Arthur and LeCun, Yann},
	month = may,
	year = {2014},
	note = {arXiv: 1312.6203},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, GCN, \#NONE},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\TBT7DFM3\\Bruna et al. - 2014 - Spectral Networks and Locally Connected Networks o.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\Z5VR6J5P\\1312.html:text/html},
}

@article{henaff_deep_2015,
	title = {Deep {Convolutional} {Networks} on {Graph}-{Structured} {Data}},
	url = {http://arxiv.org/abs/1506.05163},
	abstract = {Deep Learning's recent successes have mostly relied on Convolutional Networks, which exploit fundamental statistical properties of images, sounds and video data: the local stationarity and multi-scale compositional structure, that allows expressing long range interactions in terms of shorter, localized interactions. However, there exist other important examples, such as text documents or bioinformatic data, that may lack some or all of these strong statistical regularities. In this paper we consider the general question of how to construct deep architectures with small learning complexity on general non-Euclidean domains, which are typically unknown and need to be estimated from the data. In particular, we develop an extension of Spectral Networks which incorporates a Graph Estimation procedure, that we test on large-scale classification problems, matching or improving over Dropout Networks with far less parameters to estimate.},
	urldate = {2022-02-21},
	journal = {arXiv:1506.05163 [cs]},
	author = {Henaff, Mikael and Bruna, Joan and LeCun, Yann},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.05163},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, GCN, \#NONE},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\LLP2VVZP\\Henaff et al. - 2015 - Deep Convolutional Networks on Graph-Structured Da.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\NLWYD5MZ\\1506.html:text/html},
}

@article{defferrard_convolutional_2016,
	title = {Convolutional {Neural} {Networks} on {Graphs} with {Fast} {Localized} {Spectral} {Filtering}},
	abstract = {In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words’ embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efﬁcient numerical schemes to design fast localized convolutional ﬁlters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.},
	language = {en},
	author = {Defferrard, Michaël and Bresson, Xavier and Vandergheynst, Pierre},
	year = {2016},
	keywords = {GCN, \#NONE},
	pages = {9},
	file = {Defferrard et al. - Convolutional Neural Networks on Graphs with Fast .pdf:C\:\\Users\\felix\\Zotero\\storage\\B47XKQTU\\Defferrard et al. - Convolutional Neural Networks on Graphs with Fast .pdf:application/pdf},
}

@article{hamilton_inductive_2017,
	title = {Inductive {Representation} {Learning} on {Large} {Graphs}},
	abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general inductive framework that leverages node feature information (e.g., text attributes) to efﬁciently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node’s local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classiﬁcation benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
	language = {en},
	author = {Hamilton, William L and Ying, Rex and Leskovec, Jure},
	year = {2017},
	keywords = {GDL, GCN, \#NONE},
	pages = {19},
	file = {Hamilton et al. - Inductive Representation Learning on Large Graphs.pdf:C\:\\Users\\felix\\Zotero\\storage\\WUYCUGU8\\Hamilton et al. - Inductive Representation Learning on Large Graphs.pdf:application/pdf},
}

@article{xu_how_2019,
	title = {How {Powerful} are {Graph} {Neural} {Networks}?},
	url = {http://arxiv.org/abs/1810.00826},
	abstract = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
	urldate = {2022-02-21},
	journal = {arXiv:1810.00826 [cs, stat]},
	author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
	month = feb,
	year = {2019},
	note = {arXiv: 1810.00826},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, GDL, \#NONE},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\ZZE6JNGC\\Xu et al. - 2019 - How Powerful are Graph Neural Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\ZIZXVZN7\\1810.html:text/html},
}

@article{gao_polarimetric_2021,
	title = {Polarimetric {Pose} {Prediction}},
	url = {http://arxiv.org/abs/2112.03810},
	abstract = {Light has many properties that can be passively measured by vision sensors. Colour-band separated wavelength and intensity are arguably the most commonly used ones for monocular 6D object pose estimation. This paper explores how complementary polarisation information, i.e. the orientation of light wave oscillations, can influence the accuracy of pose predictions. A hybrid model that leverages physical priors jointly with a data-driven learning strategy is designed and carefully tested on objects with different amount of photometric complexity. Our design not only significantly improves the pose accuracy in relation to photometric state-of-the-art approaches, but also enables object pose estimation for highly reflective and transparent objects.},
	urldate = {2022-02-25},
	journal = {arXiv:2112.03810 [cs]},
	author = {Gao, Daoyi and Li, Yitong and Ruhkamp, Patrick and Skobleva, Iuliia and Wysock, Magdalena and Jung, HyunJun and Wang, Pengyuan and Guridi, Arturo and Navab, Nassir and Busam, Benjamin},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.03810},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#NONE},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\LZBBWTN6\\Gao et al. - 2021 - Polarimetric Pose Prediction.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\8FQ98NJ8\\2112.html:text/html},
}

@article{cong_spatial-temporal_2021-1,
	title = {Spatial-{Temporal} {Transformer} for {Dynamic} {Scene} {Graph} {Generation}},
	abstract = {Dynamic scene graph generation aims at generating a scene graph of the given video. Compared to the task of scene graph generation from images, it is more challenging because of the dynamic relationships between objects and the temporal dependencies between frames allowing for a richer semantic interpretation. In this paper, we propose Spatial-temporal Transformer (STTran), a neural network that consists of two core modules: (1) a spatial encoder that takes an input frame to extract spatial context and reason about the visual relationships within a frame, and (2) a temporal decoder which takes the output of the spatial encoder as input in order to capture the temporal dependencies between frames and infer the dynamic relationships. Furthermore, STTran is flexible to take varying lengths of videos as input without clipping, which is especially important for long videos. Our method is validated on the benchmark dataset Action Genome (AG). The experimental results demonstrate the superior performance of our method in terms of dynamic scene graphs. Moreover, a set of ablative studies is conducted and the effect of each proposed module is justified. Code available at: https://github.com/yrcong/STTran.},
	language = {en},
	author = {Cong, Yuren and Liao, Wentong and Ackermann, Hanno and Rosenhahn, Bodo and Yang, Michael Ying},
	year = {2021},
	pages = {11},
	file = {Cong et al. - Spatial-Temporal Transformer for Dynamic Scene Gra.pdf:C\:\\Users\\felix\\Zotero\\storage\\7RMYUKIJ\\Cong et al. - Spatial-Temporal Transformer for Dynamic Scene Gra.pdf:application/pdf},
}

@inproceedings{gu_ava_2018,
	address = {Salt Lake City, UT},
	title = {{AVA}: {A} {Video} {Dataset} of {Spatio}-{Temporally} {Localized} {Atomic} {Visual} {Actions}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{AVA}},
	url = {https://ieeexplore.ieee.org/document/8578731/},
	doi = {10.1109/CVPR.2018.00633},
	abstract = {This paper introduces a video dataset of spatiotemporally localized Atomic Visual Actions (AVA). The AVA dataset densely annotates 80 atomic visual actions in 437 15-minute video clips, where actions are localized in space and time, resulting in 1.59M action labels with multiple labels per person occurring frequently. The key characteristics of our dataset are: (1) the deﬁnition of atomic visual actions, rather than composite actions; (2) precise spatio-temporal annotations with possibly multiple annotations for each person; (3) exhaustive annotation of these atomic actions over 15-minute video clips; (4) people temporally linked across consecutive segments; and (5) using movies to gather a varied set of action representations. This departs from existing datasets for spatio-temporal action recognition, which typically provide sparse annotations for composite actions in short video clips.},
	language = {en},
	urldate = {2022-03-02},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Gu, Chunhui and Sun, Chen and Ross, David A. and Vondrick, Carl and Pantofaru, Caroline and Li, Yeqing and Vijayanarasimhan, Sudheendra and Toderici, George and Ricco, Susanna and Sukthankar, Rahul and Schmid, Cordelia and Malik, Jitendra},
	month = jun,
	year = {2018},
	keywords = {Dataset, \#NONE},
	pages = {6047--6056},
	file = {Gu et al. - 2018 - AVA A Video Dataset of Spatio-Temporally Localize.pdf:C\:\\Users\\felix\\Zotero\\storage\\RGH2EDZY\\Gu et al. - 2018 - AVA A Video Dataset of Spatio-Temporally Localize.pdf:application/pdf},
}

@inproceedings{kazi_inceptiongcn_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{InceptionGCN}: {Receptive} {Field} {Aware} {Graph} {Convolutional} {Network} for {Disease} {Prediction}},
	isbn = {978-3-030-20351-1},
	shorttitle = {{InceptionGCN}},
	doi = {10.1007/978-3-030-20351-1_6},
	abstract = {Geometric deep learning provides a principled and versatile manner for integration of imaging and non-imaging modalities in the medical domain. Graph Convolutional Networks (GCNs) in particular have been explored on a wide variety of problems such as disease prediction, segmentation, and matrix completion by leveraging large, multi-modal datasets. In this paper, we introduce a new spectral domain architecture for deep learning on graphs for disease prediction. The novelty lies in defining geometric ‘inception modules’ which are capable of capturing intra- and inter-graph structural heterogeneity during convolutions. We design filters with different kernel sizes to build our architecture. We show our disease prediction results on two publicly available datasets. Further, we provide insights on the behaviour of regular GCNs and our proposed model under varying input scenarios on simulated data.},
	language = {en},
	booktitle = {Information {Processing} in {Medical} {Imaging}},
	publisher = {Springer International Publishing},
	author = {Kazi, Anees and Shekarforoush, Shayan and Arvind Krishna, S. and Burwinkel, Hendrik and Vivar, Gerome and Kortüm, Karsten and Ahmadi, Seyed-Ahmad and Albarqouni, Shadi and Navab, Nassir},
	editor = {Chung, Albert C. S. and Gee, James C. and Yushkevich, Paul A. and Bao, Siqi},
	year = {2019},
	keywords = {GCN, \#NONE, Disease Prediction},
	pages = {73--85},
	file = {Submitted Version:C\:\\Users\\felix\\Zotero\\storage\\8D2VHPZC\\Kazi et al. - 2019 - InceptionGCN Receptive Field Aware Graph Convolut.pdf:application/pdf},
}

@inproceedings{kazi_graph_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Graph {Convolution} {Based} {Attention} {Model} for {Personalized} {Disease} {Prediction}},
	isbn = {978-3-030-32251-9},
	doi = {10.1007/978-3-030-32251-9_14},
	abstract = {Clinicians implicitly incorporate the complementarity of multi-modal data for disease diagnosis. Often a varied order of importance for this heterogeneous data is considered for personalized decisions. Current learning-based methods have achieved better performance with uniform attention to individual information, but a very few have focused on patient-specific attention learning schemes for each modality. Towards this, we introduce a model which not only improves the disease prediction but also focuses on learning patient-specific order of importance for multi-modal data elements. In order to achieve this, we take advantage of LSTM-based attention mechanism and graph convolutional networks (GCNs) to design our model. GCNs learn multi-modal but class-specific features from the entire population of patients, whereas the attention mechanism optimally fuses these multi-modal features into a final decision, separately for each patient. In this paper, we apply the proposed approach for disease prediction task for Parkinson’s and Alzheimer’s using two public medical datasets.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2019},
	publisher = {Springer International Publishing},
	author = {Kazi, Anees and Shekarforoush, Shayan and Arvind Krishna, S. and Burwinkel, Hendrik and Vivar, Gerome and Wiestler, Benedict and Kortüm, Karsten and Ahmadi, Seyed-Ahmad and Albarqouni, Shadi and Navab, Nassir},
	editor = {Shen, Dinggang and Liu, Tianming and Peters, Terry M. and Staib, Lawrence H. and Essert, Caroline and Zhou, Sean and Yap, Pew-Thian and Khan, Ali},
	year = {2019},
	keywords = {GCN, \#NONE, Attention, Disease Prediction},
	pages = {122--130},
}

@article{vivar_simultaneous_2021,
	title = {Simultaneous imputation and disease classification in incomplete medical datasets using {Multigraph} {Geometric} {Matrix} {Completion} ({MGMC})},
	volume = {117},
	issn = {09333657},
	url = {http://arxiv.org/abs/2005.06935},
	doi = {10.1016/j.artmed.2021.102097},
	abstract = {Large-scale population-based studies in medicine are a key resource towards better diagnosis, monitoring, and treatment of diseases. They also serve as enablers of clinical decision support systems, in particular Computer Aided Diagnosis (CADx) using machine learning (ML). Numerous ML approaches for CADx have been proposed in literature. However, these approaches assume full data availability, which is not always feasible in clinical data. To account for missing data, incomplete data samples are either removed or imputed, which could lead to data bias and may negatively affect classification performance. As a solution, we propose an end-to-end learning of imputation and disease prediction of incomplete medical datasets via Multigraph Geometric Matrix Completion (MGMC). MGMC uses multiple recurrent graph convolutional networks, where each graph represents an independent population model based on a key clinical meta-feature like age, sex, or cognitive function. Graph signal aggregation from local patient neighborhoods, combined with multigraph signal fusion via self-attention, has a regularizing effect on both matrix reconstruction and classification performance. Our proposed approach is able to impute class relevant features as well as perform accurate classification on two publicly available medical datasets. We empirically show the superiority of our proposed approach in terms of classification and imputation performance when compared with state-of-the-art approaches. MGMC enables disease prediction in multimodal and incomplete medical datasets. These findings could serve as baseline for future CADx approaches which utilize incomplete datasets.},
	urldate = {2022-03-02},
	journal = {Artificial Intelligence in Medicine},
	author = {Vivar, Gerome and Kazi, Anees and Burwinkel, Hendrik and Zwergal, Andreas and Navab, Nassir and Ahmadi, Seyed-Ahmad},
	month = jul,
	year = {2021},
	note = {arXiv: 2005.06935},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, GCN, \#TOSKIM, Data Imputation},
	pages = {102097},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\MX6J6J5D\\Vivar et al. - 2021 - Simultaneous imputation and disease classification.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\TN6UB75W\\2005.html:text/html},
}

@article{keicher_u-gat_2021,
	title = {U-{GAT}: {Multimodal} {Graph} {Attention} {Network} for {COVID}-19 {Outcome} {Prediction}},
	shorttitle = {U-{GAT}},
	url = {http://arxiv.org/abs/2108.00860},
	abstract = {During the first wave of COVID-19, hospitals were overwhelmed with the high number of admitted patients. An accurate prediction of the most likely individual disease progression can improve the planning of limited resources and finding the optimal treatment for patients. However, when dealing with a newly emerging disease such as COVID-19, the impact of patient- and disease-specific factors (e.g. body weight or known co-morbidities) on the immediate course of disease is by and large unknown. In the case of COVID-19, the need for intensive care unit (ICU) admission of pneumonia patients is often determined only by acute indicators such as vital signs (e.g. breathing rate, blood oxygen levels), whereas statistical analysis and decision support systems that integrate all of the available data could enable an earlier prognosis. To this end, we propose a holistic graph-based approach combining both imaging and non-imaging information. Specifically, we introduce a multimodal similarity metric to build a population graph for clustering patients and an image-based end-to-end Graph Attention Network to process this graph and predict the COVID-19 patient outcomes: admission to ICU, need for ventilation and mortality. Additionally, the network segments chest CT images as an auxiliary task and extracts image features and radiomics for feature fusion with the available metadata. Results on a dataset collected in Klinikum rechts der Isar in Munich, Germany show that our approach outperforms single modality and non-graph baselines. Moreover, our clustering and graph attention allow for increased understanding of the patient relationships within the population graph and provide insight into the network's decision-making process.},
	urldate = {2022-03-02},
	journal = {arXiv:2108.00860 [cs, eess]},
	author = {Keicher, Matthias and Burwinkel, Hendrik and Bani-Harouni, David and Paschali, Magdalini and Czempiel, Tobias and Burian, Egon and Makowski, Marcus R. and Braren, Rickmer and Navab, Nassir and Wendler, Thomas},
	month = jul,
	year = {2021},
	note = {arXiv: 2108.00860},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, GDL, \#TOSKIM, Attention},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\M7KD9SC5\\Keicher et al. - 2021 - U-GAT Multimodal Graph Attention Network for COVI.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\8QCN8886\\2108.html:text/html},
}

@inproceedings{burwinkel_adaptive_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Adaptive {Image}-{Feature} {Learning} for {Disease} {Classification} {Using} {Inductive} {Graph} {Networks}},
	isbn = {978-3-030-32226-7},
	doi = {10.1007/978-3-030-32226-7_71},
	abstract = {Recently, Geometric Deep Learning (GDL) has been introduced as a novel and versatile framework for computer-aided disease classification. GDL uses patient meta-information such as age and gender to model patient cohort relations in a graph structure. Concepts from graph signal processing are leveraged to learn the optimal mapping of multi-modal features, e.g. from images to disease classes. Related studies so far have considered image features that are extracted in a pre-processing step. We hypothesize that such an approach prevents the network from optimizing feature representations towards achieving the best performance in the graph network. We propose a new network architecture that exploits an inductive end-to-end learning approach for disease classification, where filters from both the CNN and the graph are trained jointly. We validate this architecture against state-of-the-art inductive graph networks and demonstrate significantly improved classification scores on a modified MNIST toy dataset, as well as comparable classification results with higher stability on a chest X-ray image dataset. Additionally, we explain how the structural information of the graph affects both the image filters and the feature learning.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2019},
	publisher = {Springer International Publishing},
	author = {Burwinkel, Hendrik and Kazi, Anees and Vivar, Gerome and Albarqouni, Shadi and Zahnd, Guillaume and Navab, Nassir and Ahmadi, Seyed-Ahmad},
	editor = {Shen, Dinggang and Liu, Tianming and Peters, Terry M. and Staib, Lawrence H. and Essert, Caroline and Zhou, Sean and Yap, Pew-Thian and Khan, Ali},
	year = {2019},
	keywords = {Representation learning, Disease Classification, GCN, \#TOSKIM},
	pages = {640--648},
	file = {Submitted Version:C\:\\Users\\felix\\Zotero\\storage\\UY8428MS\\Burwinkel et al. - 2019 - Adaptive Image-Feature Learning for Disease Classi.pdf:application/pdf},
}

@article{burwinkel_physics-aware_2022,
	title = {Physics-aware learning and domain-specific loss design in ophthalmology},
	volume = {76},
	issn = {1361-8415},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841521003595},
	doi = {10.1016/j.media.2021.102314},
	abstract = {The human cataract, a developing opacification of the human eye lens, currently constitutes the world’s most frequent cause for blindness. As a result, cataract surgery has become the most frequently performed ophthalmic surgery in the world. By removing the human lens and replacing it with an artificial intraocular lens (IOL), the optical system of the eye is restored. In order to receive a good refractive result, the IOL specifications, especially the refractive power, have to be determined precisely prior to surgery. In the last years, there has been a body of work to perform this prediction by using biometric information extracted from OCT imaging data, recently also by machine learning (ML) methods. Approaches so far consider only biometric information or physical modelling, but provide no effective combination, while often also neglecting IOL geometry. Additionally, ML on small data sets without sufficient domain coverage can be challenging. To solve these issues, we propose OpticNet, a novel optical refraction network based on an unsupervised, domain-specific loss function that explicitly incorporates physical information into the network. By providing a precise and differentiable light propagation eye model, physical gradients following the eye optics are backpropagated into the network. We further propose a new transfer learning procedure, which allows the unsupervised pre-training on the optical model and fine-tuning of the network on small amounts of surgical patient data. We show that our method outperforms the current state of the art on five OCT-image based data sets, provides better domain coverage within its predictions, and achieves better physical consistency.},
	language = {en},
	urldate = {2022-03-02},
	journal = {Medical Image Analysis},
	author = {Burwinkel, Hendrik and Matz, Holger and Saur, Stefan and Hauger, Christoph and Trost, Michael and Hirnschall, Nino and Findl, Oliver and Navab, Nassir and Ahmadi, Seyed-Ahmad},
	month = feb,
	year = {2022},
	keywords = {Domain prior incorporation, IOL calculation, Physics-based learning},
	pages = {102314},
	file = {ScienceDirect Snapshot:C\:\\Users\\felix\\Zotero\\storage\\IR8JUDH9\\S1361841521003595.html:text/html},
}

@article{burwinkel_physics-aware_2022-1,
	title = {Physics-aware learning and domain-specific loss design in ophthalmology},
	volume = {76},
	issn = {1361-8415},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841521003595},
	doi = {10.1016/j.media.2021.102314},
	abstract = {The human cataract, a developing opacification of the human eye lens, currently constitutes the world’s most frequent cause for blindness. As a result, cataract surgery has become the most frequently performed ophthalmic surgery in the world. By removing the human lens and replacing it with an artificial intraocular lens (IOL), the optical system of the eye is restored. In order to receive a good refractive result, the IOL specifications, especially the refractive power, have to be determined precisely prior to surgery. In the last years, there has been a body of work to perform this prediction by using biometric information extracted from OCT imaging data, recently also by machine learning (ML) methods. Approaches so far consider only biometric information or physical modelling, but provide no effective combination, while often also neglecting IOL geometry. Additionally, ML on small data sets without sufficient domain coverage can be challenging. To solve these issues, we propose OpticNet, a novel optical refraction network based on an unsupervised, domain-specific loss function that explicitly incorporates physical information into the network. By providing a precise and differentiable light propagation eye model, physical gradients following the eye optics are backpropagated into the network. We further propose a new transfer learning procedure, which allows the unsupervised pre-training on the optical model and fine-tuning of the network on small amounts of surgical patient data. We show that our method outperforms the current state of the art on five OCT-image based data sets, provides better domain coverage within its predictions, and achieves better physical consistency.},
	language = {en},
	urldate = {2022-03-02},
	journal = {Medical Image Analysis},
	author = {Burwinkel, Hendrik and Matz, Holger and Saur, Stefan and Hauger, Christoph and Trost, Michael and Hirnschall, Nino and Findl, Oliver and Navab, Nassir and Ahmadi, Seyed-Ahmad},
	month = feb,
	year = {2022},
	keywords = {Domain prior incorporation, IOL calculation, Physics-based learning, Unsupervised Learning, \#NONE, Cataract},
	pages = {102314},
	file = {ScienceDirect Snapshot:C\:\\Users\\felix\\Zotero\\storage\\548567SR\\S1361841521003595.html:text/html},
}

@inproceedings{burwinkel_domain-specific_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Domain-{Specific} {Loss} {Design} for {Unsupervised} {Physical} {Training}: {A} {New} {Approach} to {Modeling} {Medical} {ML} {Solutions}},
	isbn = {978-3-030-59713-9},
	shorttitle = {Domain-{Specific} {Loss} {Design} for {Unsupervised} {Physical} {Training}},
	doi = {10.1007/978-3-030-59713-9_52},
	abstract = {Today, cataract surgery is the most frequently performed ophthalmic surgery in the world. The cataract, a developing opacity of the human eye lens, constitutes the world’s most frequent cause for blindness. During surgery, the lens is removed and replaced by an artificial intraocular lens (IOL). To prevent patients from needing strong visual aids after surgery, a precise prediction of the optical properties of the inserted IOL is crucial. There has been lots of activity towards developing methods to predict these properties from biometric eye data obtained by OCT devices, recently also by employing machine learning. They consider either only biometric data or physical models, but rarely both, and often neglect the IOL geometry. In this work, we propose OpticNet, a novel optical refraction network, loss function, and training scheme which is unsupervised, domain-specific, and physically motivated. We derive a precise light propagation eye model using single-ray raytracing and formulate a differentiable loss function that back-propagates physical gradients into the network. Further, we propose a new transfer learning procedure, which allows unsupervised training on the physical model and fine-tuning of the network on a cohort of real IOL patient cases. We show that our network is not only superior to systems trained with standard procedures but also that our method outperforms the current state of the art in IOL calculation when compared on two biometric data sets.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2020},
	publisher = {Springer International Publishing},
	author = {Burwinkel, Hendrik and Matz, Holger and Saur, Stefan and Hauger, Christoph and Evren, Ayşe Mine and Hirnschall, Nino and Findl, Oliver and Navab, Nassir and Ahmadi, Seyed-Ahmad},
	editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
	year = {2020},
	keywords = {IOL calculation, Physical learning, Transfer learning, \#TOSKIM},
	pages = {540--550},
	file = {Submitted Version:C\:\\Users\\felix\\Zotero\\storage\\NLKDPUMQ\\Burwinkel et al. - 2020 - Domain-Specific Loss Design for Unsupervised Physi.pdf:application/pdf},
}

@article{vivar_multi-modal_2019,
	title = {Multi-modal {Graph} {Fusion} for {Inductive} {Disease} {Classification} in {Incomplete} {Datasets}},
	url = {http://arxiv.org/abs/1905.03053},
	abstract = {Clinical diagnostic decision making and population-based studies often rely on multi-modal data which is noisy and incomplete. Recently, several works proposed geometric deep learning approaches to solve disease classification, by modeling patients as nodes in a graph, along with graph signal processing of multi-modal features. Many of these approaches are limited by assuming modality- and feature-completeness, and by transductive inference, which requires re-training of the entire model for each new test sample. In this work, we propose a novel inductive graph-based approach that can generalize to out-of-sample patients, despite missing features from entire modalities per patient. We propose multi-modal graph fusion which is trained end-to-end towards node-level classification. We demonstrate the fundamental working principle of this method on a simplified MNIST toy dataset. In experiments on medical data, our method outperforms single static graph approach in multi-modal disease classification.},
	urldate = {2022-03-02},
	journal = {arXiv:1905.03053 [cs, stat]},
	author = {Vivar, Gerome and Burwinkel, Hendrik and Kazi, Anees and Zwergal, Andreas and Navab, Nassir and Ahmadi, Seyed-Ahmad},
	month = may,
	year = {2019},
	note = {arXiv: 1905.03053},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Disease Classification, GDL, \#NONE},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\PRJ62IHJ\\Vivar et al. - 2019 - Multi-modal Graph Fusion for Inductive Disease Cla.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\3G4YWFE4\\1905.html:text/html},
}

@inproceedings{burwinkel_decision_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Decision {Support} for {Intoxication} {Prediction} {Using} {Graph} {Convolutional} {Networks}},
	isbn = {978-3-030-59713-9},
	doi = {10.1007/978-3-030-59713-9_61},
	abstract = {Every day, poison control centers (PCC) are called for immediate classification and treatment recommendations of acute intoxication cases. Due to their time-sensitive nature, a doctor is required to propose a correct diagnosis and intervention within a minimal time frame. Usually the toxin is known and recommendations can be made accordingly. However, in challenging cases only symptoms are mentioned and doctors have to rely on clinical experience. Medical experts and our analyses of regional intoxication records provide evidence that this is challenging, since occurring symptoms may not always match textbook descriptions due to regional distinctions or institutional workflow. Computer-aided diagnosis (CADx) can provide decision support, but approaches so far do not consider additional patient data like age or gender, despite their potential value for the diagnosis. In this work, we propose a new machine learning based CADx method which fuses patient symptoms and meta data using graph convolutional networks. We further propose a novel symptom matching method that allows the effective incorporation of prior knowledge into the network and evidently stabilizes the prediction. We validate our method against 10 medical doctors with different experience diagnosing intoxications for 10 different toxins from the PCC in Munich and show our method’s superiority for poison prediction.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2020},
	publisher = {Springer International Publishing},
	author = {Burwinkel, Hendrik and Keicher, Matthias and Bani-Harouni, David and Zellner, Tobias and Eyer, Florian and Navab, Nassir and Ahmadi, Seyed-Ahmad},
	editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
	year = {2020},
	keywords = {Representation learning, Disease Classification, GCN, \#TOSKIM},
	pages = {633--642},
	file = {Submitted Version:C\:\\Users\\felix\\Zotero\\storage\\VBTXICRS\\Burwinkel et al. - 2020 - Decision Support for Intoxication Prediction Using.pdf:application/pdf},
}

@inproceedings{ji_action_2020,
	address = {Seattle, WA, USA},
	title = {Action {Genome}: {Actions} {As} {Compositions} of {Spatio}-{Temporal} {Scene} {Graphs}},
	isbn = {978-1-72817-168-5},
	shorttitle = {Action {Genome}},
	url = {https://ieeexplore.ieee.org/document/9157115/},
	doi = {10.1109/CVPR42600.2020.01025},
	abstract = {Action recognition has typically treated actions and activities as monolithic events that occur in videos. However, there is evidence from Cognitive Science and Neuroscience that people actively encode activities into consistent hierarchical part structures. However, in Computer Vision, few explorations on representations that encode event partonomies have been made. Inspired by evidence that the prototypical unit of an event is an action-object interaction, we introduce Action Genome, a representation that decomposes actions into spatio-temporal scene graphs. Action Genome captures changes between objects and their pairwise relationships while an action occurs. It contains 10K videos with 0.4M objects and 1.7M visual relationships annotated. With Action Genome, we extend an existing action recognition model by incorporating scene graphs as spatiotemporal feature banks to achieve better performance on the Charades dataset. Next, by decomposing and learning the temporal changes in visual relationships that result in an action, we demonstrate the utility of a hierarchical event decomposition by enabling few-shot action recognition, achieving 42.7\% mAP using as few as 10 examples. Finally, we benchmark existing scene graph models on the new task of spatio-temporal scene graph prediction.},
	language = {en},
	urldate = {2022-03-02},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Ji, Jingwei and Krishna, Ranjay and Fei-Fei, Li and Niebles, Juan Carlos},
	month = jun,
	year = {2020},
	keywords = {Dataset, Action Recognition, \#TOREAD, Scene Graphs},
	pages = {10233--10244},
	file = {Ji et al. - 2020 - Action Genome Actions As Compositions of Spatio-T.pdf:C\:\\Users\\felix\\Zotero\\storage\\K7RJJTI9\\Ji et al. - 2020 - Action Genome Actions As Compositions of Spatio-T.pdf:application/pdf},
}

@article{ozsoy_multimodal_2021,
	title = {Multimodal {Semantic} {Scene} {Graphs} for {Holistic} {Modeling} of {Surgical} {Procedures}},
	url = {http://arxiv.org/abs/2106.15309},
	abstract = {From a computer science viewpoint, a surgical domain model needs to be a conceptual one incorporating both behavior and data. It should therefore model actors, devices, tools, their complex interactions and data ﬂow. To capture and model these, we take advantage of the latest computer vision methodologies for generating 3D scene graphs from camera views. We then introduce the Multimodal Semantic Scene Graph (MSSG) which aims at providing a uniﬁed symbolic, spatiotemporal and semantic representation of surgical procedures. This methodology aims at modeling the relationship between diﬀerent components in surgical domain including medical staﬀ, imaging systems, and surgical devices, opening the path towards holistic understanding and modeling of surgical procedures. We then use MSSG to introduce a dynamically generated graphical user interface tool for surgical procedure analysis which could be used for many applications including process optimization, OR design and automatic report generation. We ﬁnally demonstrate that the proposed MSSGs could also be used for synchronizing diﬀerent complex surgical procedures. While the system still needs to be integrated into real operating rooms before getting validated, this conference paper aims mainly at providing the community with the basic principles of this novel concept through a ﬁrst prototypal partial realization based on MVOR dataset.},
	language = {en},
	urldate = {2022-03-02},
	journal = {arXiv:2106.15309 [cs]},
	author = {Özsoy, Ege and Örnek, Evin Pınar and Eck, Ulrich and Tombari, Federico and Navab, Nassir},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.15309},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, 3D, \#NONE, Video Scene Graph Generation, Scene Graphs},
	file = {Özsoy et al. - 2021 - Multimodal Semantic Scene Graphs for Holistic Mode.pdf:C\:\\Users\\felix\\Zotero\\storage\\B9LECGLB\\Özsoy et al. - 2021 - Multimodal Semantic Scene Graphs for Holistic Mode.pdf:application/pdf},
}

@article{grammatikopoulou_cadis_2022,
	title = {{CaDIS}: {Cataract} {Dataset} for {Image} {Segmentation}},
	shorttitle = {{CaDIS}},
	url = {http://arxiv.org/abs/1906.11586},
	abstract = {Video feedback provides a wealth of information about surgical procedures and is the main sensory cue for surgeons. Scene understanding is crucial to computer assisted interventions (CAI) and to post-operative analysis of the surgical procedure. A fundamental building block of such capabilities is the identification and localization of surgical instruments and anatomical structures through semantic segmentation. Deep learning has advanced semantic segmentation techniques in the recent years but is inherently reliant on the availability of labelled datasets for model training. This paper introduces a dataset for semantic segmentation of cataract surgery videos complementing the publicly available CATARACTS challenge dataset. In addition, we benchmark the performance of several state-of-the-art deep learning models for semantic segmentation on the presented dataset. The dataset is publicly available at https://cataracts-semantic-segmentation2020.grand-challenge.org/.},
	urldate = {2022-03-03},
	journal = {arXiv:1906.11586 [cs]},
	author = {Grammatikopoulou, Maria and Flouty, Evangello and Kadkhodamohammadi, Abdolrahim and Quellec, Gwenol'e and Chow, Andre and Nehme, Jean and Luengo, Imanol and Stoyanov, Danail},
	month = feb,
	year = {2022},
	note = {arXiv: 1906.11586
version: 4},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#TOSKIM, Dataset, Cataract, Segmentation},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\QBSD4YWD\\Grammatikopoulou et al. - 2022 - CaDIS Cataract Dataset for Image Segmentation.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\UCWEE9ZT\\1906.html:text/html},
}

@article{ban_concept_2022,
	title = {Concept {Graph} {Neural} {Networks} for {Surgical} {Video} {Understanding}},
	url = {http://arxiv.org/abs/2202.13402},
	abstract = {We constantly integrate our knowledge and understanding of the world to enhance our interpretation of what we see. This ability is crucial in application domains which entail reasoning about multiple entities and concepts, such as AI-augmented surgery. In this paper, we propose a novel way of integrating conceptual knowledge into temporal analysis tasks via temporal concept graph networks. In the proposed networks, a global knowledge graph is incorporated into the temporal analysis of surgical instances, learning the meaning of concepts and relations as they apply to the data. We demonstrate our results in surgical video data for tasks such as verification of critical view of safety, as well as estimation of Parkland grading scale. The results show that our method improves the recognition and detection of complex benchmarks as well as enables other analytic applications of interest.},
	urldate = {2022-03-03},
	journal = {arXiv:2202.13402 [cs]},
	author = {Ban, Yutong and Eckhoff, Jennifer A. and Ward, Thomas M. and Hashimoto, Daniel A. and Meireles, Ozanan R. and Rus, Daniela and Rosman, Guy},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.13402
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#READ, Scene Graphs},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\5WQWQJ49\\Ban et al. - 2022 - Concept Graph Neural Networks for Surgical Video U.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\F8DIAHB2\\2202.html:text/html},
}

@article{nwoye_rendezvous_2022,
	title = {Rendezvous: {Attention} {Mechanisms} for the {Recognition} of {Surgical} {Action} {Triplets} in {Endoscopic} {Videos}},
	shorttitle = {Rendezvous},
	url = {http://arxiv.org/abs/2109.03223},
	abstract = {Out of all existing frameworks for surgical workflow analysis in endoscopic videos, action triplet recognition stands out as the only one aiming to provide truly fine-grained and comprehensive information on surgical activities. This information, presented as {\textless}instrument, verb, target{\textgreater} combinations, is highly challenging to be accurately identified. Triplet components can be difficult to recognize individually; in this task, it requires not only performing recognition simultaneously for all three triplet components, but also correctly establishing the data association between them. To achieve this task, we introduce our new model, the Rendezvous (RDV), which recognizes triplets directly from surgical videos by leveraging attention at two different levels. We first introduce a new form of spatial attention to capture individual action triplet components in a scene; called Class Activation Guided Attention Mechanism (CAGAM). This technique focuses on the recognition of verbs and targets using activations resulting from instruments. To solve the association problem, our RDV model adds a new form of semantic attention inspired by Transformer networks; called Multi-Head of Mixed Attention (MHMA). This technique uses several cross and self attentions to effectively capture relationships between instruments, verbs, and targets. We also introduce CholecT50 - a dataset of 50 endoscopic videos in which every frame has been annotated with labels from 100 triplet classes. Our proposed RDV model significantly improves the triplet prediction mean AP by over 9\% compared to the state-of-the-art methods on this dataset.},
	urldate = {2022-03-05},
	journal = {arXiv:2109.03223 [cs]},
	author = {Nwoye, Chinedu Innocent and Yu, Tong and Gonzalez, Cristians and Seeliger, Barbara and Mascagni, Pietro and Mutter, Didier and Marescaux, Jacques and Padoy, Nicolas},
	month = mar,
	year = {2022},
	note = {arXiv: 2109.03223},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#READ, Dataset, Action Triplets, Attention, CholecXXX},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\WXVFHQEA\\Nwoye et al. - 2022 - Rendezvous Attention Mechanisms for the Recogniti.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\U6LQLN7Z\\2109.html:text/html},
}

@article{chang_comprehensive_2022,
	title = {A {Comprehensive} {Survey} of {Scene} {Graphs}: {Generation} and {Application}},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {A {Comprehensive} {Survey} of {Scene} {Graphs}},
	url = {http://arxiv.org/abs/2104.01111},
	doi = {10.1109/TPAMI.2021.3137605},
	abstract = {Scene graph is a structured representation of a scene that can clearly express the objects, attributes, and relationships between objects in the scene. As computer vision technology continues to develop, people are no longer satisfied with simply detecting and recognizing objects in images; instead, people look forward to a higher level of understanding and reasoning about visual scenes. For example, given an image, we want to not only detect and recognize objects in the image, but also know the relationship between objects (visual relationship detection), and generate a text description (image captioning) based on the image content. Alternatively, we might want the machine to tell us what the little girl in the image is doing (Visual Question Answering (VQA)), or even remove the dog from the image and find similar images (image editing and retrieval), etc. These tasks require a higher level of understanding and reasoning for image vision tasks. The scene graph is just such a powerful tool for scene understanding. Therefore, scene graphs have attracted the attention of a large number of researchers, and related research is often cross-modal, complex, and rapidly developing. However, no relatively systematic survey of scene graphs exists at present. To this end, this survey conducts a comprehensive investigation of the current scene graph research. More specifically, we first summarized the general definition of the scene graph, then conducted a comprehensive and systematic discussion on the generation method of the scene graph (SGG) and the SGG with the aid of prior knowledge. We then investigated the main applications of scene graphs and summarized the most commonly used datasets. Finally, we provide some insights into the future development of scene graphs. We believe this will be a very helpful foundation for future research on scene graphs.},
	urldate = {2022-03-10},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Chang, Xiaojun and Ren, Pengzhen and Xu, Pengfei and Li, Zhihui and Chen, Xiaojiang and Hauptmann, Alex},
	year = {2022},
	note = {arXiv: 2104.01111},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#TOREAD, SGG, Scene Graphs},
	pages = {1--1},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\IZTPDD8K\\Chang et al. - 2022 - A Comprehensive Survey of Scene Graphs Generation.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\HFA86NZV\\2104.html:text/html},
}

@article{allan_2018_2020,
	title = {2018 {Robotic} {Scene} {Segmentation} {Challenge}},
	url = {http://arxiv.org/abs/2001.11190},
	abstract = {In 2015 we began a sub-challenge at the EndoVis workshop at MICCAI in Munich using endoscope images of ex-vivo tissue with automatically generated annotations from robot forward kinematics and instrument CAD models. However, the limited background variation and simple motion rendered the dataset uninformative in learning about which techniques would be suitable for segmentation in real surgery. In 2017, at the same workshop in Quebec we introduced the robotic instrument segmentation dataset with 10 teams participating in the challenge to perform binary, articulating parts and type segmentation of da Vinci instruments. This challenge included realistic instrument motion and more complex porcine tissue as background and was widely addressed with modifications on U-Nets and other popular CNN architectures. In 2018 we added to the complexity by introducing a set of anatomical objects and medical devices to the segmented classes. To avoid over-complicating the challenge, we continued with porcine data which is dramatically simpler than human tissue due to the lack of fatty tissue occluding many organs.},
	urldate = {2022-03-10},
	journal = {arXiv:2001.11190 [cs]},
	author = {Allan, Max and Kondo, Satoshi and Bodenstedt, Sebastian and Leger, Stefan and Kadkhodamohammadi, Rahim and Luengo, Imanol and Fuentes, Felix and Flouty, Evangello and Mohammed, Ahmed and Pedersen, Marius and Kori, Avinash and Alex, Varghese and Krishnamurthi, Ganapathy and Rauber, David and Mendel, Robert and Palm, Christoph and Bano, Sophia and Saibro, Guinther and Shih, Chi-Sheng and Chiang, Hsun-An and Zhuang, Juntang and Yang, Junlin and Iglovikov, Vladimir and Dobrenkii, Anton and Reddiboina, Madhu and Reddy, Anubhav and Liu, Xingtong and Gao, Cong and Unberath, Mathias and Kim, Myeonghyeon and Kim, Chanho and Kim, Chaewon and Kim, Hyejin and Lee, Gyeongmin and Ullah, Ihsan and Luna, Miguel and Park, Sang Hyun and Azizian, Mahdi and Stoyanov, Danail and Maier-Hein, Lena and Speidel, Stefanie},
	month = aug,
	year = {2020},
	note = {arXiv: 2001.11190},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Dataset, \#NONE, Segmentation},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\EKJLMLPI\\Allan et al. - 2020 - 2018 Robotic Scene Segmentation Challenge.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\VW6TI6K3\\2001.html:text/html},
}

@article{liu_fully_2021,
	title = {Fully {Convolutional} {Scene} {Graph} {Generation}},
	url = {http://arxiv.org/abs/2103.16083},
	abstract = {This paper presents a fully convolutional scene graph generation (FCSGG) model that detects objects and relations simultaneously. Most of the scene graph generation frameworks use a pre-trained two-stage object detector, like Faster R-CNN, and build scene graphs using bounding box features. Such pipeline usually has a large number of parameters and low inference speed. Unlike these approaches, FCSGG is a conceptually elegant and efficient bottom-up approach that encodes objects as bounding box center points, and relationships as 2D vector fields which are named as Relation Affinity Fields (RAFs). RAFs encode both semantic and spatial features, and explicitly represent the relationship between a pair of objects by the integral on a sub-region that points from subject to object. FCSGG only utilizes visual features and still generates strong results for scene graph generation. Comprehensive experiments on the Visual Genome dataset demonstrate the efficacy, efficiency, and generalizability of the proposed method. FCSGG achieves highly competitive results on recall and zero-shot recall with significantly reduced inference time.},
	urldate = {2022-03-10},
	journal = {arXiv:2103.16083 [cs]},
	author = {Liu, Hengyue and Yan, Ning and Mortazavi, Masood S. and Bhanu, Bir},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.16083},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#TOREAD, SGG, Scene Graphs},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\FZTCE8AR\\Liu et al. - 2021 - Fully Convolutional Scene Graph Generation.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\U2EV63D3\\2103.html:text/html},
}

@article{shi_proposalclip_2022,
	title = {{ProposalCLIP}: {Unsupervised} {Open}-{Category} {Object} {Proposal} {Generation} via {Exploiting} {CLIP} {Cues}},
	shorttitle = {{ProposalCLIP}},
	url = {http://arxiv.org/abs/2201.06696},
	abstract = {Object proposal generation is an important and fundamental task in computer vision. In this paper, we propose ProposalCLIP, a method towards unsupervised opencategory object proposal generation. Unlike previous works which require a large number of bounding box annotations and/or can only generate proposals for limited object categories, our ProposalCLIP is able to predict proposals for a large variety of object categories without annotations, by exploiting CLIP (contrastive language-image pre-training) cues. Firstly, we analyze CLIP for unsupervised open-category proposal generation and design an objectness score based on our empirical analysis on proposal selection. Secondly, a graph-based merging module is proposed to solve the limitations of CLIP cues and merge fragmented proposals. Finally, we present a proposal regression module that extracts pseudo labels based on CLIP cues and trains a lightweight network to further reﬁne proposals. Extensive experiments on PASCAL VOC, COCO and Visual Genome datasets show that our ProposalCLIP can better generate proposals than previous state-of-the-art methods. Our ProposalCLIP also shows beneﬁts for downstream tasks, such as unsupervised object detection.},
	language = {en},
	urldate = {2022-03-11},
	journal = {arXiv:2201.06696 [cs]},
	author = {Shi, Hengcan and Hayat, Munawar and Wu, Yicheng and Cai, Jianfei},
	month = jan,
	year = {2022},
	note = {arXiv: 2201.06696},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Unsupervised Learning, \#NONE, Object Proposal},
	file = {Shi et al. - 2022 - ProposalCLIP Unsupervised Open-Category Object Pr.pdf:C\:\\Users\\felix\\Zotero\\storage\\3PN75SDS\\Shi et al. - 2022 - ProposalCLIP Unsupervised Open-Category Object Pr.pdf:application/pdf},
}

@article{padoy_workflow_2009,
	title = {Workflow monitoring based on {3D} motion features},
	doi = {10.1109/ICCVW.2009.5457648},
	abstract = {This work proposes to use Workflow-HMMs, a form of HMMs augmented with phase probability variables that model the complete workflow process and takes into account the full temporal context which improves on-line recognition of the phases, especially in case of partial labeling. Activity recognition has primarily addressed the identification of either actions or well-defined interactions among objects in a scene. In this work, we extend the scope to the study of workflow monitoring. In a workflow, ordered groups of activities (phases) with different durations take place in a constrained environment and create temporal patterns across the workflow instances. We address the problem of recognizing phases, based on exemplary recordings. We propose to use Workflow-HMMs, a form of HMMs augmented with phase probability variables that model the complete workflow process. This model takes into account the full temporal context which improves on-line recognition of the phases, especially in case of partial labeling. Targeted applications are workflow monitoring in hospitals and factories, where common action recognition approaches are difficult to apply. To avoid interfering with the normal workflow, we capture the activity of a room with a multiple-camera system. Additionally, we propose to rely on real-time low-level features (3D motion flow) to maintain a generic approach. We demonstrate our methods on sequences from medical procedures performed in a mock-up operating room. The sequences follow a complex workflow, containing various alternatives.},
	journal = {2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops},
	author = {Padoy, N. and Mateus, D. and Weinland, Daniel and Berger, M. and Navab, Nassir},
	year = {2009},
	keywords = {3D, \#NONE},
	file = {Submitted Version:C\:\\Users\\felix\\Zotero\\storage\\MDM84LC4\\Padoy et al. - 2009 - Workflow monitoring based on 3D motion features.pdf:application/pdf},
}

@article{chen_graph-based_2018,
	title = {Graph-{Based} {Global} {Reasoning} {Networks}},
	url = {http://arxiv.org/abs/1811.12814},
	abstract = {Globally modeling and reasoning over relations between regions can be beneficial for many computer vision tasks on both images and videos. Convolutional Neural Networks (CNNs) excel at modeling local relations by convolution operations, but they are typically inefficient at capturing global relations between distant regions and require stacking multiple convolution layers. In this work, we propose a new approach for reasoning globally in which a set of features are globally aggregated over the coordinate space and then projected to an interaction space where relational reasoning can be efficiently computed. After reasoning, relation-aware features are distributed back to the original coordinate space for down-stream tasks. We further present a highly efficient instantiation of the proposed approach and introduce the Global Reasoning unit (GloRe unit) that implements the coordinate-interaction space mapping by weighted global pooling and weighted broadcasting, and the relation reasoning via graph convolution on a small graph in interaction space. The proposed GloRe unit is lightweight, end-to-end trainable and can be easily plugged into existing CNNs for a wide range of tasks. Extensive experiments show our GloRe unit can consistently boost the performance of state-of-the-art backbone architectures, including ResNet, ResNeXt, SE-Net and DPN, for both 2D and 3D CNNs, on image classification, semantic segmentation and video action recognition task.},
	urldate = {2022-03-16},
	journal = {arXiv:1811.12814 [cs]},
	author = {Chen, Yunpeng and Rohrbach, Marcus and Yan, Zhicheng and Yan, Shuicheng and Feng, Jiashi and Kalantidis, Yannis},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.12814
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, GDL, Reasoning, \#TOSKIM},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\QI2B5CVK\\Chen et al. - 2018 - Graph-Based Global Reasoning Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\HXU76ITQ\\1811.html:text/html},
}

@article{czempiel_surgical_2022,
	title = {Surgical {Workflow} {Recognition}: from {Analysis} of {Challenges} to {Architectural} {Study}},
	shorttitle = {Surgical {Workflow} {Recognition}},
	url = {http://arxiv.org/abs/2203.09230},
	abstract = {Algorithmic surgical workﬂow recognition is an ongoing research ﬁeld and can be divided into laparoscopic (Internal) and operating room (External) analysis. So far many diﬀerent works for the internal analysis have been proposed with the combination of a frame-level and an additional temporal model to address the temporal ambiguities between diﬀerent workﬂow phases. For the External recognition task, Clip-level methods are in the focus of researchers targeting the local ambiguities present in the OR scene. In this work we evaluate combinations of diﬀerent model architectures for the task of surgical workﬂow recognition to provide a fair comparison of the methods for both Internal and External analysis. We show that methods designed for the Internal analysis can be transferred to the external task with comparable performance gains for diﬀerent architectures.},
	language = {en},
	urldate = {2022-03-21},
	journal = {arXiv:2203.09230 [cs]},
	author = {Czempiel, Tobias and Sharghi, Aidean and Paschali, Magdalini and Mohareri, Omid},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.09230},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#READ},
	file = {Czempiel et al. - 2022 - Surgical Workflow Recognition from Analysis of Ch.pdf:C\:\\Users\\felix\\Zotero\\storage\\LALMTU5N\\Czempiel et al. - 2022 - Surgical Workflow Recognition from Analysis of Ch.pdf:application/pdf},
}

@article{jiao_more_2022,
	title = {{MORE}: {Multi}-{Order} {RElation} {Mining} for {Dense} {Captioning} in {3D} {Scenes}},
	shorttitle = {{MORE}},
	url = {http://arxiv.org/abs/2203.05203},
	abstract = {3D dense captioning is a recently-proposed novel task, where point clouds contain more geometric information than the 2D counterpart. However, it is also more challenging due to the higher complexity and wider variety of inter-object relations. Existing methods only treat such relations as by-products of object feature learning in graphs without specifically encoding them, which leads to sub-optimal results. In this paper, aiming at improving 3D dense captioning via capturing and utilizing the complex relations in the 3D scene, we propose MORE, a Multi-Order RElation mining model, to support generating more descriptive and comprehensive captions. Technically, our MORE encodes object relations in a progressive manner since complex relations can be deduced from a limited number of basic ones. We first devise a novel Spatial Layout Graph Convolution (SLGC), which semantically encodes several first-order relations as edges of a graph constructed over 3D object proposals. Next, from the resulting graph, we further extract multiple triplets which encapsulate basic first-order relations as the basic unit and construct several Object-centric Triplet Attention Graphs (OTAG) to infer multi-order relations for every target object. The updated node features from OTAG are aggregated and fed into the caption decoder to provide abundant relational cues so that captions including diverse relations with context objects can be generated. Extensive experiments on the Scan2Cap dataset prove the effectiveness of our proposed MORE and its components, and we also outperform the current state-of-the-art method.},
	urldate = {2022-03-21},
	journal = {arXiv:2203.05203 [cs]},
	author = {Jiao, Yang and Chen, Shaoxiang and Jie, Zequn and Chen, Jingjing and Ma, Lin and Jiang, Yu-Gang},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.05203},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, 3D, \#NONE, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\MZ4WA9FH\\Jiao et al. - 2022 - MORE Multi-Order RElation Mining for Dense Captio.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\PW5JJVP5\\2203.html:text/html},
}

@article{huaulme_offline_2020,
	title = {Offline identification of surgical deviations in laparoscopic rectopexy},
	volume = {104},
	issn = {1873-2860},
	doi = {10.1016/j.artmed.2020.101837},
	abstract = {OBJECTIVE: According to a meta-analysis of 7 studies, the median number of patients with at least one adverse event during the surgery is 14.4\%, and a third of those adverse events were preventable. The occurrence of adverse events forces surgeons to implement corrective strategies and, thus, deviate from the standard surgical process. Therefore, it is clear that the automatic identification of adverse events is a major challenge for patient safety. In this paper, we have proposed a method enabling us to identify such deviations. We have focused on identifying surgeons' deviations from standard surgical processes due to surgical events rather than anatomic specificities. This is particularly challenging, given the high variability in typical surgical procedure workflows.
METHODS: We have introduced a new approach designed to automatically detect and distinguish surgical process deviations based on multi-dimensional non-linear temporal scaling with a hidden semi-Markov model using manual annotation of surgical processes. The approach was then evaluated using cross-validation.
RESULTS: The best results have over 90\% accuracy. Recall and precision for event deviations, i.e. related to adverse events, are respectively below 80\% and 40\%. To understand these results, we have provided a detailed analysis of the incorrectly-detected observations.
CONCLUSION: Multi-dimensional non-linear temporal scaling with a hidden semi-Markov model provides promising results for detecting deviations. Our error analysis of the incorrectly-detected observations offers different leads in order to further improve our method.
SIGNIFICANCE: Our method demonstrated the feasibility of automatically detecting surgical deviations that could be implemented for both skill analysis and developing situation awareness-based computer-assisted surgical systems.},
	language = {eng},
	journal = {Artificial Intelligence in Medicine},
	author = {Huaulmé, Arnaud and Jannin, Pierre and Reche, Fabian and Faucheron, Jean-Luc and Moreau-Gaudry, Alexandre and Voros, Sandrine},
	month = apr,
	year = {2020},
	pmid = {32499005},
	keywords = {\#TOSKIM, Computer Systems, Dynamic time warping, Hidden semi-Markov model, Humans, Intraoperative event detection, Laparoscopy, Rectopexy, Surgeons, Surgical process model, Workflow, Anomaly},
	pages = {101837},
	file = {Submitted Version:C\:\\Users\\felix\\Zotero\\storage\\G5UFUSZX\\Huaulmé et al. - 2020 - Offline identification of surgical deviations in l.pdf:application/pdf},
}

@article{katic_bridging_2016,
	title = {Bridging the gap between formal and experience-based knowledge for context-aware laparoscopy},
	volume = {11},
	issn = {1861-6429},
	url = {https://doi.org/10.1007/s11548-016-1379-2},
	doi = {10.1007/s11548-016-1379-2},
	abstract = {Computer assistance is increasingly common in surgery. However, the amount of information is bound to overload processing abilities of surgeons. We propose methods to recognize the current phase of a surgery for context-aware information filtering. The purpose is to select the most suitable subset of information for surgical situations which require special assistance.},
	language = {en},
	number = {6},
	urldate = {2022-03-24},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {Katić, Darko and Schuck, Jürgen and Wekerle, Anna-Laura and Kenngott, Hannes and Müller-Stich, Beat Peter and Dillmann, Rüdiger and Speidel, Stefanie},
	month = jun,
	year = {2016},
	keywords = {\#NONE, Random Forest},
	pages = {881--888},
	file = {Springer Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\5XMCE69L\\Katić et al. - 2016 - Bridging the gap between formal and experience-bas.pdf:application/pdf},
}

@article{bengio_curriculum_2009,
	title = {Curriculum learning},
	url = {https://www.semanticscholar.org/paper/Curriculum-learning-Bengio-Louradour/8de174ab5419b9d3127695405efd079808e956e8},
	abstract = {It is hypothesized that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions). Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them \&quot;curriculum learning\&quot;. In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
	language = {en},
	urldate = {2022-03-25},
	journal = {undefined},
	author = {Bengio, Yoshua and Louradour, J. and Collobert, Ronan and Weston, J.},
	year = {2009},
	keywords = {\#NONE, AI Fundamentals},
	file = {Bengio et al. - 2009 - Curriculum learning.pdf:C\:\\Users\\felix\\Zotero\\storage\\HTAMHJCN\\Bengio et al. - 2009 - Curriculum learning.pdf:application/pdf;Snapshot:C\:\\Users\\felix\\Zotero\\storage\\YUR526AX\\8de174ab5419b9d3127695405efd079808e956e8.html:text/html},
}

@article{feichtenhofer_x3d_2020,
	title = {{X3D}: {Expanding} {Architectures} for {Efficient} {Video} {Recognition}},
	shorttitle = {{X3D}},
	url = {http://arxiv.org/abs/2004.04730},
	abstract = {This paper presents X3D, a family of efficient video networks that progressively expand a tiny 2D image classification architecture along multiple network axes, in space, time, width and depth. Inspired by feature selection methods in machine learning, a simple stepwise network expansion approach is employed that expands a single axis in each step, such that good accuracy to complexity trade-off is achieved. To expand X3D to a specific target complexity, we perform progressive forward expansion followed by backward contraction. X3D achieves state-of-the-art performance while requiring 4.8x and 5.5x fewer multiply-adds and parameters for similar accuracy as previous work. Our most surprising finding is that networks with high spatiotemporal resolution can perform well, while being extremely light in terms of network width and parameters. We report competitive accuracy at unprecedented efficiency on video classification and detection benchmarks. Code will be available at: https://github.com/facebookresearch/SlowFast},
	urldate = {2022-03-25},
	journal = {arXiv:2004.04730 [cs]},
	author = {Feichtenhofer, Christoph},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.04730},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#SKIMMED, Architecture, Video Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\JH769QFR\\Feichtenhofer - 2020 - X3D Expanding Architectures for Efficient Video R.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\DY2UDVNU\\2004.html:text/html},
}

@inproceedings{jiang_auto-x3d_2022,
	title = {Auto-{X3D}: {Ultra}-{Efficient} {Video} {Understanding} via {Finer}-{Grained} {Neural} {Architecture} {Search}},
	shorttitle = {Auto-{X3D}},
	doi = {10.1109/WACV51458.2022.00241},
	abstract = {Efficient video architecture is the key to deploying video recognition systems on devices with limited computing resources. Unfortunately, existing video architectures are often computationally intensive and not suitable for such applications. The recent X3D work presents a new family of efficient video models by expanding a hand-crafted image architecture along multiple axes, such as space, time, width, and depth. Although operating in a conceptually large space, X3D searches one axis at a time, and merely explored a small set of 30 architectures in total, which does not sufficiently explore the space. This paper bypasses existing 2D architectures, and directly searched for 3D architectures in a fine-grained space, where block type, filter number, expansion ratio and attention block are jointly searched. A probabilistic neural architecture search method is adopted to efficiently search in such a large space. Evaluations on Kinetics and Something-Something-V2 benchmarks confirm our AutoX3D models outperform existing ones in accuracy up to 1.3\% under similar FLOPs, and reduce the computational cost up to ×1.74 when reaching similar performance.},
	booktitle = {2022 {IEEE}/{CVF} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Jiang, Yifan and Gong, Xinyu and Wu, Junru and Shi, Humphrey and Yan, Zhicheng and Wang, Zhangyang},
	month = jan,
	year = {2022},
	note = {ISSN: 2642-9381},
	keywords = {\#TOSKIM, Architecture, Analysis and Understanding Deep Learning -{\textgreater} Efficient Training and Inference Methods for Networks, Benchmark testing, Computational modeling, Computer architecture, Computer vision, Probabilistic logic, Search methods, Visual Reasoning, X3D},
	pages = {2354--2363},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\felix\\Zotero\\storage\\QBL3Q3II\\9706685.html:text/html;Submitted Version:C\:\\Users\\felix\\Zotero\\storage\\VU8V7WVA\\Jiang et al. - 2022 - Auto-X3D Ultra-Efficient Video Understanding via .pdf:application/pdf},
}

@article{hansen_graphregnet_2021,
	title = {{GraphRegNet}: {Deep} {Graph} {Regularisation} {Networks} on {Sparse} {Keypoints} for {Dense} {Registration} of {3D} {Lung} {CTs}},
	volume = {40},
	issn = {1558-254X},
	shorttitle = {{GraphRegNet}},
	doi = {10.1109/TMI.2021.3073986},
	abstract = {In the last two years learning-based methods have started to show encouraging results in different supervised and unsupervised medical image registration tasks. Deep neural networks enable (near) real time applications through fast inference times and have tremendous potential for increased registration accuracies by task-specific learning. However, estimation of large 3D deformations, for example present in inhale to exhale lung CT or interpatient abdominal MRI registration, is still a major challenge for the widely adopted U-Net-like network architectures. Even when using multi-level strategies, current state-of-the-art DL registration results do not yet reach the high accuracy of conventional frameworks. To overcome the problem of large deformations for deep learning approaches, in this work, we present GraphRegNet, a sparse keypoint-based geometric network for dense deformable medical image registration. Similar to the successful 2D optical flow estimation of FlowNet or PWC-Net we leverage discrete dense displacement maps to facilitate the registration process. In order to cope with enormously increasing memory requirements when working with displacement maps in 3D medical volumes and to obtain a well-regularised and accurate deformation field we 1) formulate the registration task as the prediction of displacement vectors on a sparse irregular grid of distinctive keypoints and 2) introduce our efficient GraphRegNet for displacement regularisation, a combination of convolutional and graph neural network layers in a unified architecture. In our experiments on exhale to inhale lung CT registration we demonstrate substantial improvements (TRE below 1.4 mm) over other deep learning methods. Our code is publicly available at https://github.com/multimodallearning/graphregnet.},
	number = {9},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Hansen, Lasse and Heinrich, Mattias P.},
	month = sep,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	keywords = {GDL, \#NONE, Biomedical imaging, Computed tomography, Deformable registration, Estimation, Feature extraction, graph learning, Lung, Strain, thoracic CT, Three-dimensional displays, GDLMA 2022, Registration},
	pages = {2246--2257},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\felix\\Zotero\\storage\\3V93GDQK\\9406964.html:text/html},
}

@article{schiffer_2d_2021,
	title = {{2D} histology meets {3D} topology: {Cytoarchitectonic} brain mapping with {Graph} {Neural} {Networks}},
	volume = {12908},
	shorttitle = {{2D} histology meets {3D} topology},
	url = {http://arxiv.org/abs/2103.05259},
	doi = {10.1007/978-3-030-87237-3_38},
	abstract = {Cytoarchitecture describes the spatial organization of neuronal cells in the brain, including their arrangement into layers and columns with respect to cell density, orientation, or presence of certain cell types. It allows to segregate the brain into cortical areas and subcortical nuclei, links structure with connectivity and function, and provides a microstructural reference for human brain atlases. Mapping boundaries between areas requires to scan histological sections at microscopic resolution. While recent high-throughput scanners allow to scan a complete human brain in the order of a year, it is practically impossible to delineate regions at the same pace using the established gold standard method. Researchers have recently addressed cytoarchitectonic mapping of cortical regions with deep neural networks, relying on image patches from individual 2D sections for classification. However, the 3D context, which is needed to disambiguate complex or obliquely cut brain regions, is not taken into account. In this work, we combine 2D histology with 3D topology by reformulating the mapping task as a node classification problem on an approximate 3D midsurface mesh through the isocortex. We extract deep features from cortical patches in 2D histological sections which are descriptive of cytoarchitecture, and assign them to the corresponding nodes on the 3D mesh to construct a large attributed graph. By solving the brain mapping problem on this graph using graph neural networks, we obtain significantly improved classification results. The proposed framework lends itself nicely to integration of additional neuroanatomical priors for mapping.},
	urldate = {2022-03-25},
	journal = {arXiv:2103.05259 [cs, eess]},
	author = {Schiffer, Christian and Harmeling, Stefan and Amunts, Katrin and Dickscheid, Timo},
	year = {2021},
	note = {arXiv: 2103.05259},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, GDL, \#NONE, GDLMA 2022},
	pages = {395--404},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\ISMIDZU7\\Schiffer et al. - 2021 - 2D histology meets 3D topology Cytoarchitectonic .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\YQDLSNXS\\2103.html:text/html},
}

@article{maurya_improving_2021,
	title = {Improving {Graph} {Neural} {Networks} with {Simple} {Architecture} {Design}},
	url = {http://arxiv.org/abs/2105.07634},
	abstract = {Graph Neural Networks have emerged as a useful tool to learn on the data by applying additional constraints based on the graph structure. These graphs are often created with assumed intrinsic relations between the entities. In recent years, there have been tremendous improvements in the architecture design, pushing the performance up in various prediction tasks. In general, these neural architectures combine layer depth and node feature aggregation steps. This makes it challenging to analyze the importance of features at various hops and the expressiveness of the neural network layers. As different graph datasets show varying levels of homophily and heterophily in features and class label distribution, it becomes essential to understand which features are important for the prediction tasks without any prior information. In this work, we decouple the node feature aggregation step and depth of graph neural network and introduce several key design strategies for graph neural networks. More specifically, we propose to use softmax as a regularizer and "Soft-Selector" of features aggregated from neighbors at different hop distances; and "Hop-Normalization" over GNN layers. Combining these techniques, we present a simple and shallow model, Feature Selection Graph Neural Network (FSGNN), and show empirically that the proposed model outperforms other state of the art GNN models and achieves up to 64\% improvements in accuracy on node classification tasks. Moreover, analyzing the learned soft-selection parameters of the model provides a simple way to study the importance of features in the prediction tasks. Finally, we demonstrate with experiments that the model is scalable for large graphs with millions of nodes and billions of edges.},
	urldate = {2022-03-25},
	journal = {arXiv:2105.07634 [cs, stat]},
	author = {Maurya, Sunil Kumar and Liu, Xin and Murata, Tsuyoshi},
	month = may,
	year = {2021},
	note = {arXiv: 2105.07634
version: 1},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, GDL, \#NONE, GDLMA 2022},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\PQLT2PAQ\\Maurya et al. - 2021 - Improving Graph Neural Networks with Simple Archit.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\XWUVTKG5\\2105.html:text/html},
}

@inproceedings{he_sat2graph_2020,
	title = {{Sat2Graph}: {Road} {Graph} {Extraction} through {Graph}-{Tensor} {Encoding}},
	shorttitle = {{Sat2Graph}},
	doi = {10.1007/978-3-030-58586-0_4},
	abstract = {Whereas prior work only infers planar road graphs, the proposed Sat2Graph approach is capable of inferring stacked roads, and does so robustly, and surpasses prior methods on two widely used metrics, TOPO and APLS. Inferring road graphs from satellite imagery is a challenging computer vision task. Prior solutions fall into two categories: (1) pixel-wise segmentation-based approaches, which predict whether each pixel is on a road, and (2) graph-based approaches, which predict the road graph iteratively. We find that these two approaches have complementary strengths while suffering from their own inherent limitations. 
In this paper, we propose a new method, Sat2Graph, which combines the advantages of the two prior categories into a unified framework. The key idea in Sat2Graph is a novel encoding scheme, graph-tensor encoding (GTE), which encodes the road graph into a tensor representation. GTE makes it possible to train a simple, non-recurrent, supervised model to predict a rich set of features that capture the graph structure directly from an image. We evaluate Sat2Graph using two large datasets. We find that Sat2Graph surpasses prior methods on two widely used metrics, TOPO and APLS. Furthermore, whereas prior work only infers planar road graphs, our approach is capable of inferring stacked roads (e.g., overpasses), and does so robustly.},
	booktitle = {{ECCV}},
	author = {He, Songtao and Bastani, F. and Jagwani, Satvat and Alizadeh, M. and Balakrishnan, H. and Chawla, S. and Elshrif, M. and Madden, S. and Sadeghi, M.},
	year = {2020},
	keywords = {GDL, \#NONE, GDLMA 2022},
	file = {Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\FM3AF2AI\\He et al. - 2020 - Sat2Graph Road Graph Extraction through Graph-Ten.pdf:application/pdf},
}

@article{nakao_igcn_2021,
	title = {{IGCN}: {Image}-to-graph {Convolutional} {Network} for {2D}/{3D} {Deformable} {Registration}},
	shorttitle = {{IGCN}},
	url = {http://arxiv.org/abs/2111.00484},
	abstract = {Organ shape reconstruction based on a single-projection image during treatment has wide clinical scope, e.g., in image-guided radiotherapy and surgical guidance. We propose an image-to-graph convolutional network that achieves deformable registration of a 3D organ mesh for a single-viewpoint 2D projection image. This framework enables simultaneous training of two types of transformation: from the 2D projection image to a displacement map, and from the sampled per-vertex feature to a 3D displacement that satisfies the geometrical constraint of the mesh structure. Assuming application to radiation therapy, the 2D/3D deformable registration performance is verified for multiple abdominal organs that have not been targeted to date, i.e., the liver, stomach, duodenum, and kidney, and for pancreatic cancer. The experimental results show shape prediction considering relationships among multiple organs can be used to predict respiratory motion and deformation from digitally reconstructed radiographs with clinically acceptable accuracy.},
	urldate = {2022-03-25},
	journal = {arXiv:2111.00484 [cs, eess]},
	author = {Nakao, Megumi and Nakamura, Mitsuhiro and Matsuda, Tetsuya},
	month = oct,
	year = {2021},
	note = {arXiv: 2111.00484},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, GDL, GCN, \#NONE, GDLMA 2022, Registration},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\M58ZJTZT\\Nakao et al. - 2021 - IGCN Image-to-graph Convolutional Network for 2D.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\C36Z6YBJ\\2111.html:text/html},
}

@article{rivoir_pitfalls_2022,
	title = {On the {Pitfalls} of {Batch} {Normalization} for {End}-to-{End} {Video} {Learning}: {A} {Study} on {Surgical} {Workflow} {Analysis}},
	shorttitle = {On the {Pitfalls} of {Batch} {Normalization} for {End}-to-{End} {Video} {Learning}},
	url = {http://arxiv.org/abs/2203.07976},
	abstract = {Batch Normalization's (BN) unique property of depending on other samples in a batch is known to cause problems in several tasks, including sequential modeling, and has led to the use of alternatives in these fields. In video learning, however, these problems are less studied, despite the ubiquitous use of BN in CNNs for visual feature extraction. We argue that BN's properties create major obstacles for training CNNs and temporal models end to end in video tasks. Yet, end-to-end learning seems preferable in specialized domains such as surgical workflow analysis, which lack well-pretrained feature extractors. While previous work in surgical workflow analysis has avoided BN-related issues through complex, multi-stage learning procedures, we show that even simple, end-to-end CNN-LSTMs can outperform the state of the art when CNNs without BN are used. Moreover, we analyze in detail when BN-related issues occur, including a "cheating" phenomenon in surgical anticipation tasks. We hope that a deeper understanding of BN's limitations and a reconsideration of end-to-end approaches can be beneficial for future research in surgical workflow analysis and general video learning.},
	urldate = {2022-03-29},
	journal = {arXiv:2203.07976 [cs]},
	author = {Rivoir, Dominik and Funke, Isabel and Speidel, Stefanie},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.07976},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#TOREAD, AI Fundamentals, Surgical Workflow Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\4FKAWFQN\\Rivoir et al. - 2022 - On the Pitfalls of Batch Normalization for End-to-.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\IWIGECVE\\2203.html:text/html},
}

@article{cong_reltr_2022,
	title = {{RelTR}: {Relation} {Transformer} for {Scene} {Graph} {Generation}},
	shorttitle = {{RelTR}},
	url = {http://arxiv.org/abs/2201.11460},
	abstract = {Different objects in the same scene are more or less related to each other, but only a limited number of these relationships are noteworthy. Inspired by DETR, which excels in object detection, we view scene graph generation as a set prediction problem and propose an end-to-end scene graph generation model RelTR which has an encoder-decoder architecture. The encoder reasons about the visual feature context while the decoder infers a fixed-size set of triplets subject-predicate-object using different types of attention mechanisms with coupled subject and object queries. We design a set prediction loss performing the matching between the ground truth and predicted triplets for the end-to-end training. In contrast to most existing scene graph generation methods, RelTR is a one-stage method that predicts a set of relationships directly only using visual appearance without combining entities and labeling all possible predicates. Extensive experiments on the Visual Genome and Open Images V6 datasets demonstrate the superior performance and fast inference of our model.},
	urldate = {2022-03-30},
	journal = {arXiv:2201.11460 [cs]},
	author = {Cong, Yuren and Yang, Michael Ying and Rosenhahn, Bodo},
	month = jan,
	year = {2022},
	note = {arXiv: 2201.11460},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#TOREAD, SGG, GDLMA 2022},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\5DGTL3ST\\Cong et al. - 2022 - RelTR Relation Transformer for Scene Graph Genera.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\ALH4N9TJ\\2201.html:text/html},
}

@article{cong_reltr_2022-1,
	title = {{RelTR}: {Relation} {Transformer} for {Scene} {Graph} {Generation}},
	shorttitle = {{RelTR}},
	url = {http://arxiv.org/abs/2201.11460},
	abstract = {Different objects in the same scene are more or less related to each other, but only a limited number of these relationships are noteworthy. Inspired by DETR, which excels in object detection, we view scene graph generation as a set prediction problem and propose an end-to-end scene graph generation model RelTR which has an encoder-decoder architecture. The encoder reasons about the visual feature context while the decoder infers a fixed-size set of triplets subject-predicate-object using different types of attention mechanisms with coupled subject and object queries. We design a set prediction loss performing the matching between the ground truth and predicted triplets for the end-to-end training. In contrast to most existing scene graph generation methods, RelTR is a one-stage method that predicts a set of relationships directly only using visual appearance without combining entities and labeling all possible predicates. Extensive experiments on the Visual Genome and Open Images V6 datasets demonstrate the superior performance and fast inference of our model.},
	urldate = {2022-03-30},
	journal = {arXiv:2201.11460 [cs]},
	author = {Cong, Yuren and Yang, Michael Ying and Rosenhahn, Bodo},
	month = jan,
	year = {2022},
	note = {arXiv: 2201.11460},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\JNHDVJ7T\\Cong et al. - 2022 - RelTR Relation Transformer for Scene Graph Genera.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\HJVCCTE4\\2201.html:text/html},
}

@article{zhang_metadt_2022,
	title = {{MetaDT}: {Meta} {Decision} {Tree} for {Interpretable} {Few}-{Shot} {Learning}},
	shorttitle = {{MetaDT}},
	url = {http://arxiv.org/abs/2203.01482},
	abstract = {Few-Shot Learning (FSL) is a challenging task, which aims to recognize novel classes with few examples. Recently, lots of methods have been proposed from the perspective of meta-learning and representation learning for improving FSL performance. However, few works focus on the interpretability of FSL decision process. In this paper, we take a step towards the interpretable FSL by proposing a novel decision tree-based meta-learning framework, namely, MetaDT. Our insight is replacing the last black-box FSL classifier of the existing representation learning methods by an interpretable decision tree with meta-learning. The key challenge is how to effectively learn the decision tree (i.e., the tree structure and the parameters of each node) in the FSL setting. To address the challenge, we introduce a tree-like class hierarchy as our prior: 1) the hierarchy is directly employed as the tree structure; 2) by regarding the class hierarchy as an undirected graph, a graph convolution-based decision tree inference network is designed as our meta-learner to learn to infer the parameters of each node. At last, a two-loop optimization mechanism is incorporated into our framework for a fast adaptation of the decision tree with few examples. Extensive experiments on performance comparison and interpretability analysis show the effectiveness and superiority of our MetaDT. Our code will be publicly available upon acceptance.},
	urldate = {2022-03-30},
	journal = {arXiv:2203.01482 [cs]},
	author = {Zhang, Baoquan and Jiang, Hao and Li, Xutao and Feng, Shanshan and Ye, Yunming and Ye, Rui},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.01482},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Reasoning, \#NONE, GDLMA 2022, Few-Shot, Interpretability},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\KNW8BJWB\\Zhang et al. - 2022 - MetaDT Meta Decision Tree for Interpretable Few-S.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\AUIQRV2I\\2203.html:text/html},
}

@article{talak_neural_2021,
	title = {Neural {Trees} for {Learning} on {Graphs}},
	url = {http://arxiv.org/abs/2105.07264},
	abstract = {Graph Neural Networks (GNNs) have emerged as a flexible and powerful approach for learning over graphs. Despite this success, existing GNNs are constrained by their local message-passing architecture and are provably limited in their expressive power. In this work, we propose a new GNN architecture -- the Neural Tree. The neural tree architecture does not perform message passing on the input graph, but on a tree-structured graph, called the H-tree, that is constructed from the input graph. Nodes in the H-tree correspond to subgraphs in the input graph, and they are reorganized in a hierarchical manner such that the parent of a node in the H-tree always corresponds to a larger subgraph in the input graph. We show that the neural tree architecture can approximate any smooth probability distribution function over an undirected graph. We also prove that the number of parameters needed to achieve an \${\textbackslash}epsilon\$-approximation of the distribution function is exponential in the treewidth of the input graph, but linear in its size. We prove that any continuous \${\textbackslash}mathcal\{G\}\$-invariant/equivariant function can be approximated by a nonlinear combination of such probability distribution functions over \${\textbackslash}mathcal\{G\}\$. We apply the neural tree to semi-supervised node classification in 3D scene graphs, and show that these theoretical properties translate into significant gains in prediction accuracy, over the more traditional GNN architectures. We also show the applicability of the neural tree architecture to citation networks with large treewidth, by using a graph sub-sampling technique.},
	urldate = {2022-03-30},
	journal = {arXiv:2105.07264 [cs]},
	author = {Talak, Rajat and Hu, Siyi and Peng, Lisa and Carlone, Luca},
	month = oct,
	year = {2021},
	note = {arXiv: 2105.07264},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, \#NONE, AI Fundamentals, GDLMA 2022, GNN, Scene Graphs},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\U8RXCM5P\\Talak et al. - 2021 - Neural Trees for Learning on Graphs.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\BNZZK5QT\\2105.html:text/html},
}

@article{sun_context_2020,
	title = {Context {Matters}: {Graph}-based {Self}-supervised {Representation} {Learning} for {Medical} {Images}},
	shorttitle = {Context {Matters}},
	url = {http://arxiv.org/abs/2012.06457},
	abstract = {Supervised learning method requires a large volume of annotated datasets. Collecting such datasets is time-consuming and expensive. Until now, very few annotated COVID-19 imaging datasets are available. Although self-supervised learning enables us to bootstrap the training by exploiting unlabeled data, the generic self-supervised methods for natural images do not sufficiently incorporate the context. For medical images, a desirable method should be sensitive enough to detect deviation from normal-appearing tissue of each anatomical region; here, anatomy is the context. We introduce a novel approach with two levels of self-supervised representation learning objectives: one on the regional anatomical level and another on the patient-level. We use graph neural networks to incorporate the relationship between different anatomical regions. The structure of the graph is informed by anatomical correspondences between each patient and an anatomical atlas. In addition, the graph representation has the advantage of handling any arbitrarily sized image in full resolution. Experiments on large-scale Computer Tomography (CT) datasets of lung images show that our approach compares favorably to baseline methods that do not account for the context. We use the learnt embedding to quantify the clinical progression of COVID-19 and show that our method generalizes well to COVID-19 patients from different hospitals. Qualitative results suggest that our model can identify clinically relevant regions in the images.},
	urldate = {2022-03-30},
	journal = {arXiv:2012.06457 [cs, eess]},
	author = {Sun, Li and Yu, Ke and Batmanghelich, Kayhan},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.06457},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, \#NONE, GDLMA 2022, GNN, SSL},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\TG7N2G2A\\Sun et al. - 2020 - Context Matters Graph-based Self-supervised Repre.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\3WTCQZMD\\2012.html:text/html},
}

@inproceedings{zhao_airway_2021,
	address = {Cham},
	title = {Airway {Anomaly} {Detection} by {Prototype}-{Based} {Graph} {Neural} {Network}},
	isbn = {978-3-030-87240-3},
	doi = {10.1007/978-3-030-87240-3_19},
	abstract = {Detecting the airway anomaly can be an essential part to aid the lung disease diagnosis. Since normal human airways share an anatomical structure, we design a graph prototype whose structure follows the normal airway anatomy. Then, we learn the prototype and a graph neural network from a weakly-supervised airway dataset, i.e., only the holistic label is available, indicating if the airway has anomaly or not, but which bronchus node has the anomaly is unknown. During inference, the graph neural network predicts the anomaly score at both the holistic level and node-level of an airway. We initialize the airway anomaly detection problem by creating a large airway dataset with 2589 samples, and our prototype-based graph neural network shows high sensitivity and specificity on this new benchmark dataset. The code is available at https://github.com/tznbz/Airway-Anomaly-Detection-by-Prototype-based-Graph-Neural-Network.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2021},
	publisher = {Springer International Publishing},
	author = {Zhao, Tianyi and Yin, Zhaozheng},
	editor = {de Bruijne, Marleen and Cattin, Philippe C. and Cotin, Stéphane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
	year = {2021},
	keywords = {\#NONE, GDLMA 2022, GNN, Anomaly, Prototype},
	pages = {195--204},
	file = {Springer Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\US596AX8\\Zhao and Yin - 2021 - Airway Anomaly Detection by Prototype-Based Graph .pdf:application/pdf},
}

@article{yuan_explainability_2021,
	title = {On {Explainability} of {Graph} {Neural} {Networks} via {Subgraph} {Explorations}},
	url = {http://arxiv.org/abs/2102.05152},
	abstract = {We consider the problem of explaining the predictions of graph neural networks (GNNs), which otherwise are considered as black boxes. Existing methods invariably focus on explaining the importance of graph nodes or edges but ignore the substructures of graphs, which are more intuitive and human-intelligible. In this work, we propose a novel method, known as SubgraphX, to explain GNNs by identifying important subgraphs. Given a trained GNN model and an input graph, our SubgraphX explains its predictions by efficiently exploring different subgraphs with Monte Carlo tree search. To make the tree search more effective, we propose to use Shapley values as a measure of subgraph importance, which can also capture the interactions among different subgraphs. To expedite computations, we propose efficient approximation schemes to compute Shapley values for graph data. Our work represents the first attempt to explain GNNs via identifying subgraphs explicitly and directly. Experimental results show that our SubgraphX achieves significantly improved explanations, while keeping computations at a reasonable level.},
	urldate = {2022-03-31},
	journal = {arXiv:2102.05152 [cs]},
	author = {Yuan, Hao and Yu, Haiyang and Wang, Jie and Li, Kang and Ji, Shuiwang},
	month = may,
	year = {2021},
	note = {arXiv: 2102.05152},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, \#NONE, GDLMA 2022, GNN, Explainabile AI},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\S3KACEEB\\Yuan et al. - 2021 - On Explainability of Graph Neural Networks via Sub.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\3DDJHC7X\\2102.html:text/html},
}

@article{jin_overlooked_2022,
	title = {The {Overlooked} {Classifier} in {Human}-{Object} {Interaction} {Recognition}},
	url = {http://arxiv.org/abs/2112.06392},
	abstract = {Human-Object Interaction (HOI) recognition is challenging due to two factors: (1) significant imbalance across classes and (2) requiring multiple labels per image. This paper shows that these two challenges can be effectively addressed by improving the classifier with the backbone architecture untouched. Firstly, we encode the semantic correlation among classes into the classification head by initializing the weights with language embeddings of HOIs. As a result, the performance is boosted significantly, especially for the few-shot subset. Secondly, we propose a new loss named LSE-Sign to enhance multi-label learning on a long-tailed dataset. Our simple yet effective method enables detection-free HOI classification, outperforming the state-of-the-arts that require object detection and human pose by a clear margin. Moreover, we transfer the classification model to instance-level HOI detection by connecting it with an off-the-shelf object detector. We achieve state-of-the-art without additional fine-tuning.},
	urldate = {2022-03-31},
	journal = {arXiv:2112.06392 [cs]},
	author = {Jin, Ying and Chen, Yinpeng and Wang, Lijuan and Wang, Jianfeng and Yu, Pei and Liang, Lin and Hwang, Jenq-Neng and Liu, Zicheng},
	month = mar,
	year = {2022},
	note = {arXiv: 2112.06392
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#SKIMMED, GDLMA 2022, Scene Graphs},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\GJHCAT8G\\Jin et al. - 2022 - The Overlooked Classifier in Human-Object Interact.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\89TFVDJZ\\2112.html:text/html},
}

@article{feng_exploiting_2021,
	title = {Exploiting {Long}-{Term} {Dependencies} for {Generating} {Dynamic} {Scene} {Graphs}},
	url = {http://arxiv.org/abs/2112.09828},
	abstract = {Structured video representation in the form of dynamic scene graphs is an effective tool for several video understanding tasks. Compared to the task of scene graph generation from images, dynamic scene graph generation is more challenging due to the temporal dynamics of the scene and the inherent temporal fluctuations of predictions. We show that capturing long-term dependencies is the key to effective generation of dynamic scene graphs. We present the detect-track-recognize paradigm by constructing consistent long-term object tracklets from a video, followed by transformers to capture the dynamics of objects and visual relations. Experimental results demonstrate that our Dynamic Scene Graph Detection Transformer (DSG-DETR) outperforms state-of-the-art methods by a significant margin on the benchmark dataset Action Genome. We also perform ablation studies and validate the effectiveness of each component of the proposed approach.},
	urldate = {2022-03-31},
	journal = {arXiv:2112.09828 [cs]},
	author = {Feng, Shengyu and Tripathi, Subarna and Mostafa, Hesham and Nassar, Marcel and Majumdar, Somdeb},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.09828},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#TOREAD, SGG, GDLMA 2022, Temporal, Scene Graphs},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\FXJFQGCN\\Feng et al. - 2021 - Exploiting Long-Term Dependencies for Generating D.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\685Y3YSJ\\2112.html:text/html},
}

@article{ying_hierarchical_2019,
	title = {Hierarchical {Graph} {Representation} {Learning} with {Differentiable} {Pooling}},
	url = {http://arxiv.org/abs/1806.08804},
	abstract = {Recently, graph neural networks (GNNs) have revolutionized the field of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classification and link prediction. However, current GNN methods are inherently flat and do not learn hierarchical representations of graphs---a limitation that is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph. Here we propose DiffPool, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. DiffPool learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a set of clusters, which then form the coarsened input for the next GNN layer. Our experimental results show that combining existing GNN methods with DiffPool yields an average improvement of 5-10\% accuracy on graph classification benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of five benchmark data sets.},
	urldate = {2022-03-31},
	journal = {arXiv:1806.08804 [cs, stat]},
	author = {Ying, Rex and You, Jiaxuan and Morris, Christopher and Ren, Xiang and Hamilton, William L. and Leskovec, Jure},
	month = feb,
	year = {2019},
	note = {arXiv: 1806.08804},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, \#NONE, GDLMA 2022, GNN, Computer Science - Social and Information Networks},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\G2K2FTPI\\Ying et al. - 2019 - Hierarchical Graph Representation Learning with Di.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\KLQQJ543\\1806.html:text/html},
}

@article{huang_edge-variational_2020,
	title = {Edge-variational {Graph} {Convolutional} {Networks} for {Uncertainty}-aware {Disease} {Prediction}},
	url = {http://arxiv.org/abs/2009.02759},
	abstract = {There is a rising need for computational models that can complementarily leverage data of different modalities while investigating associations between subjects for population-based disease analysis. Despite the success of convolutional neural networks in representation learning for imaging data, it is still a very challenging task. In this paper, we propose a generalizable framework that can automatically integrate imaging data with non-imaging data in populations for uncertainty-aware disease prediction. At its core is a learnable adaptive population graph with variational edges, which we mathematically prove that it is optimizable in conjunction with graph convolutional neural networks. To estimate the predictive uncertainty related to the graph topology, we propose the novel concept of Monte-Carlo edge dropout. Experimental results on four databases show that our method can consistently and significantly improve the diagnostic accuracy for Autism spectrum disorder, Alzheimer's disease, and ocular diseases, indicating its generalizability in leveraging multimodal data for computer-aided diagnosis.},
	urldate = {2022-03-31},
	journal = {arXiv:2009.02759 [cs, eess]},
	author = {Huang, Yongxiang and Chung, Albert C. S.},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.02759},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Computer Science - Artificial Intelligence, \#NONE, GDLMA 2022, GNN, Uncertainty},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\57H44B3R\\Huang and Chung - 2020 - Edge-variational Graph Convolutional Networks for .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\NPC7U6QH\\2009.html:text/html},
}

@article{borisov_deep_2022,
	title = {Deep {Neural} {Networks} and {Tabular} {Data}: {A} {Survey}},
	shorttitle = {Deep {Neural} {Networks} and {Tabular} {Data}},
	url = {http://arxiv.org/abs/2110.01889},
	abstract = {Heterogeneous tabular data are the most commonly used form of data and are essential for numerous critical and computationally demanding applications. On homogeneous data sets, deep neural networks have repeatedly shown excellent performance and have therefore been widely adopted. However, their application to modeling tabular data (inference or generation) remains highly challenging. This work provides an overview of state of the art deep learning methods for tabular data. We start by categorizing them into three groups: data transformations, specialized architectures, and regularization models. We then provide a comprehensive overview of the main approaches in each group. A discussion of deep learning approaches for generating tabular data is complemented by strategies for explaining deep models on tabular data. Our primary contribution is to address the main research streams and existing methodologies in this area, while highlighting relevant challenges and open research questions. We also provide an empirical comparison of traditional machine learning methods with deep learning approaches on real tabular data sets of different sizes and with different learning objectives. Our results indicate that algorithms based on gradient-boosted tree ensembles still outperform the deep learning models. To the best of our knowledge, this is the first in-depth look at deep learning approaches for tabular data. This work can serve as a valuable starting point and guide for researchers and practitioners interested in deep learning with tabular data.},
	urldate = {2022-03-31},
	journal = {arXiv:2110.01889 [cs]},
	author = {Borisov, Vadim and Leemann, Tobias and Seßler, Kathrin and Haug, Johannes and Pawelczyk, Martin and Kasneci, Gjergji},
	month = feb,
	year = {2022},
	note = {arXiv: 2110.01889},
	keywords = {Computer Science - Machine Learning, \#TOSKIM, Tabular Data, XGBoost},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\DTJKNS6H\\Borisov et al. - 2022 - Deep Neural Networks and Tabular Data A Survey.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\RJJJ3KLS\\2110.html:text/html},
}

@article{reinke_common_2022,
	title = {Common {Limitations} of {Image} {Processing} {Metrics}: {A} {Picture} {Story}},
	shorttitle = {Common {Limitations} of {Image} {Processing} {Metrics}},
	url = {http://arxiv.org/abs/2104.05642},
	abstract = {While the importance of automatic image analysis is continuously increasing, recent meta-research revealed major flaws with respect to algorithm validation. Performance metrics are particularly key for meaningful, objective, and transparent performance assessment and validation of the used automatic algorithms, but relatively little attention has been given to the practical pitfalls when using specific metrics for a given image analysis task. These are typically related to (1) the disregard of inherent metric properties, such as the behaviour in the presence of class imbalance or small target structures, (2) the disregard of inherent data set properties, such as the non-independence of the test cases, and (3) the disregard of the actual biomedical domain interest that the metrics should reflect. This living dynamically document has the purpose to illustrate important limitations of performance metrics commonly applied in the field of image analysis. In this context, it focuses on biomedical image analysis problems that can be phrased as image-level classification, semantic segmentation, instance segmentation, or object detection task. The current version is based on a Delphi process on metrics conducted by an international consortium of image analysis experts from more than 60 institutions worldwide.},
	urldate = {2022-04-06},
	journal = {arXiv:2104.05642 [cs, eess]},
	author = {Reinke, Annika and Tizabi, Minu D. and Sudre, Carole H. and Eisenmann, Matthias and Rädsch, Tim and Baumgartner, Michael and Acion, Laura and Antonelli, Michela and Arbel, Tal and Bakas, Spyridon and Bankhead, Peter and Benis, Arriel and Cardoso, M. Jorge and Cheplygina, Veronika and Cimini, Beth and Collins, Gary S. and Farahani, Keyvan and Glocker, Ben and Hamprecht, Fred and Hashimoto, Daniel A. and Heckmann-Nötzel, Doreen and Hoffmann, Michael M. and Huisman, Merel and Isensee, Fabian and Jannin, Pierre and Kahn, Charles E. and Karargyris, Alexandros and Karthikesalingam, Alan and Kainz, Bernhard and Kavur, Emre and Kenngott, Hannes and Kleesiek, Jens and Kooi, Thijs and Kozubek, Michal and Kreshuk, Anna and Kurc, Tahsin and Landman, Bennett A. and Litjens, Geert and Madani, Amin and Maier-Hein, Klaus and Martel, Anne L. and Mattson, Peter and Meijering, Erik and Menze, Bjoern and Moher, David and Moons, Karel G. M. and Müller, Henning and Nickel, Felix and Petersen, Jens and Polat, Gorkem and Rajpoot, Nasir and Reyes, Mauricio and Rieke, Nicola and Riegler, Michael and Rivaz, Hassan and Saez-Rodriguez, Julio and Gutierrez, Clarisa Sanchez and Schroeter, Julien and Saha, Anindo and Shetty, Shravya and Stieltjes, Bram and Summers, Ronald M. and Taha, Abdel A. and Tsaftaris, Sotirios A. and van Ginneken, Bram and Varoquaux, Gaël and Wiesenfarth, Manuel and Yaniv, Ziv R. and Kopp-Schneider, Annette and Jäger, Paul and Maier-Hein, Lena},
	month = apr,
	year = {2022},
	note = {arXiv: 2104.05642},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, \#TOSKIM, Medical Imaging, Metrics},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\UXTGQM9D\\Reinke et al. - 2022 - Common Limitations of Image Processing Metrics A .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\RWJ6HFXC\\2104.html:text/html},
}

@article{rojas-munoz_daisi_2020,
	title = {{DAISI}: {Database} for {AI} {Surgical} {Instruction}},
	shorttitle = {{DAISI}},
	url = {http://arxiv.org/abs/2004.02809},
	abstract = {Telementoring surgeons as they perform surgery can be essential in the treatment of patients when in situ expertise is not available. Nonetheless, expert mentors are often unavailable to provide trainees with real-time medical guidance. When mentors are unavailable, a fallback autonomous mechanism should provide medical practitioners with the required guidance. However, AI/autonomous mentoring in medicine has been limited by the availability of generalizable prediction models, and surgical procedures datasets to train those models with. This work presents the initial steps towards the development of an intelligent artificial system for autonomous medical mentoring. Specifically, we present the first Database for AI Surgical Instruction (DAISI). DAISI leverages on images and instructions to provide step-by-step demonstrations of how to perform procedures from various medical disciplines. The dataset was acquired from real surgical procedures and data from academic textbooks. We used DAISI to train an encoder-decoder neural network capable of predicting medical instructions given a current view of the surgery. Afterwards, the instructions predicted by the network were evaluated using cumulative BLEU scores and input from expert physicians. According to the BLEU scores, the predicted and ground truth instructions were as high as 67\% similar. Additionally, expert physicians subjectively assessed the algorithm using Likert scale, and considered that the predicted descriptions were related to the images. This work provides a baseline for AI algorithms to assist in autonomous medical mentoring.},
	urldate = {2022-04-07},
	journal = {arXiv:2004.02809 [cs, eess]},
	author = {Rojas-Muñoz, Edgar and Couperus, Kyle and Wachs, Juan},
	month = mar,
	year = {2020},
	note = {arXiv: 2004.02809},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Computer Science - Artificial Intelligence, \#TOSKIM, Dataset},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\UASE9U4W\\Rojas-Muñoz et al. - 2020 - DAISI Database for AI Surgical Instruction.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\MYJ6QDW3\\2004.html:text/html},
}

@article{bawa_saras_2021,
	title = {The {SARAS} {Endoscopic} {Surgeon} {Action} {Detection} ({ESAD}) dataset: {Challenges} and methods},
	shorttitle = {The {SARAS} {Endoscopic} {Surgeon} {Action} {Detection} ({ESAD}) dataset},
	url = {http://arxiv.org/abs/2104.03178},
	abstract = {For an autonomous robotic system, monitoring surgeon actions and assisting the main surgeon during a procedure can be very challenging. The challenges come from the peculiar structure of the surgical scene, the greater similarity in appearance of actions performed via tools in a cavity compared to, say, human actions in unconstrained environments, as well as from the motion of the endoscopic camera. This paper presents ESAD, the first large-scale dataset designed to tackle the problem of surgeon action detection in endoscopic minimally invasive surgery. ESAD aims at contributing to increase the effectiveness and reliability of surgical assistant robots by realistically testing their awareness of the actions performed by a surgeon. The dataset provides bounding box annotation for 21 action classes on real endoscopic video frames captured during prostatectomy, and was used as the basis of a recent MIDL 2020 challenge. We also present an analysis of the dataset conducted using the baseline model which was released as part of the challenge, and a description of the top performing models submitted to the challenge together with the results they obtained. This study provides significant insight into what approaches can be effective and can be extended further. We believe that ESAD will serve in the future as a useful benchmark for all researchers active in surgeon action detection and assistive robotics at large.},
	urldate = {2022-04-07},
	journal = {arXiv:2104.03178 [cs]},
	author = {Bawa, Vivek Singh and Singh, Gurkirt and KapingA, Francis and Skarga-Bandurova, Inna and Oleari, Elettra and Leporini, Alice and Landolfo, Carmela and Zhao, Pengfei and Xiang, Xi and Luo, Gongning and Wang, Kuanquan and Li, Liangzhi and Wang, Bowen and Zhao, Shang and Li, Li and Stabile, Armando and Setti, Francesco and Muradore, Riccardo and Cuzzolin, Fabio},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.03178},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#TOSKIM, Dataset},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\279KAKRB\\Bawa et al. - 2021 - The SARAS Endoscopic Surgeon Action Detection (ESA.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\EFCJCEB3\\2104.html:text/html},
}

@article{bawa_saras_2021-1,
	title = {The {SARAS} {Endoscopic} {Surgeon} {Action} {Detection} ({ESAD}) dataset: {Challenges} and methods},
	shorttitle = {The {SARAS} {Endoscopic} {Surgeon} {Action} {Detection} ({ESAD}) dataset},
	url = {http://arxiv.org/abs/2104.03178},
	abstract = {For an autonomous robotic system, monitoring surgeon actions and assisting the main surgeon during a procedure can be very challenging. The challenges come from the peculiar structure of the surgical scene, the greater similarity in appearance of actions performed via tools in a cavity compared to, say, human actions in unconstrained environments, as well as from the motion of the endoscopic camera. This paper presents ESAD, the ﬁrst large-scale dataset designed to tackle the problem of surgeon action detection in endoscopic minimally invasive surgery. ESAD aims at contributing to increase the eﬀectiveness and reliability of surgical assistant robots by realistically testing their awareness of the actions performed by a surgeon. The dataset provides bounding box annotation for 21 action classes on real endoscopic video frames captured during prostatectomy, and was used as the basis of a recent MIDL 2020 challenge. We also present an analysis of the dataset conducted using the baseline model which was released as part of the challenge, and a description of the top performing models submitted to the challenge together with the results they obtained. This study provides signiﬁcant insight into what approaches can be eﬀective and can be extended further. We believe that ESAD will serve in the future as a useful benchmark for all researchers active in surgeon action detection and assistive robotics at large.},
	language = {en},
	urldate = {2022-04-07},
	journal = {arXiv:2104.03178 [cs]},
	author = {Bawa, Vivek Singh and Singh, Gurkirt and KapingA, Francis and Skarga-Bandurova, Inna and Oleari, Elettra and Leporini, Alice and Landolfo, Carmela and Zhao, Pengfei and Xiang, Xi and Luo, Gongning and Wang, Kuanquan and Li, Liangzhi and Wang, Bowen and Zhao, Shang and Li, Li and Stabile, Armando and Setti, Francesco and Muradore, Riccardo and Cuzzolin, Fabio},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.03178},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Bawa et al. - 2021 - The SARAS Endoscopic Surgeon Action Detection (ESA.pdf:C\:\\Users\\felix\\Zotero\\storage\\L4KRNCSI\\Bawa et al. - 2021 - The SARAS Endoscopic Surgeon Action Detection (ESA.pdf:application/pdf},
}

@article{smaira_short_2020,
	title = {A {Short} {Note} on the {Kinetics}-700-2020 {Human} {Action} {Dataset}},
	url = {http://arxiv.org/abs/2010.10864},
	abstract = {We describe the 2020 edition of the DeepMind Kinetics human action dataset, which replenishes and extends the Kinetics-700 dataset. In this new version, there are at least 700 video clips from different YouTube videos for each of the 700 classes. This paper details the changes introduced for this new release of the dataset and includes a comprehensive set of statistics as well as baseline results using the I3D network.},
	urldate = {2022-04-20},
	journal = {arXiv:2010.10864 [cs]},
	author = {Smaira, Lucas and Carreira, João and Noland, Eric and Clancy, Ellen and Wu, Amy and Zisserman, Andrew},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.10864},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Dataset, \#NONE},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\AN76IPUW\\Smaira et al. - 2020 - A Short Note on the Kinetics-700-2020 Human Action.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\GXE78IWN\\2010.html:text/html},
}

@article{hong_cholecseg8k_2020,
	title = {{CholecSeg8k}: {A} {Semantic} {Segmentation} {Dataset} for {Laparoscopic} {Cholecystectomy} {Based} on {Cholec80}},
	shorttitle = {{CholecSeg8k}},
	url = {http://arxiv.org/abs/2012.12453},
	abstract = {Computer-assisted surgery has been developed to enhance surgery correctness and safety. However, researchers and engineers suffer from limited annotated data to develop and train better algorithms. Consequently, the development of fundamental algorithms such as Simultaneous Localization and Mapping (SLAM) is limited. This article elaborates on the efforts of preparing the dataset for semantic segmentation, which is the foundation of many computer-assisted surgery mechanisms. Based on the Cholec80 dataset [3], we extracted 8,080 laparoscopic cholecystectomy image frames from 17 video clips in Cholec80 and annotated the images. The dataset is named CholecSeg8K and its total size is 3GB. Each of these images is annotated at pixel-level for thirteen classes, which are commonly founded in laparoscopic cholecystectomy surgery. CholecSeg8k is released under the license CC BY- NC-SA 4.0.},
	urldate = {2022-04-30},
	journal = {arXiv:2012.12453 [cs]},
	author = {Hong, W.-Y. and Kao, C.-L. and Kuo, Y.-H. and Wang, J.-R. and Chang, W.-L. and Shih, C.-S.},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.12453},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Dataset, \#NONE, Cholecystectomy},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\Z83STCKF\\Hong et al. - 2020 - CholecSeg8k A Semantic Segmentation Dataset for L.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\HEB6SVA7\\2012.html:text/html},
}

@article{sestini_fun-sis_2022,
	title = {{FUN}-{SIS}: a {Fully} {UNsupervised} approach for {Surgical} {Instrument} {Segmentation}},
	shorttitle = {{FUN}-{SIS}},
	url = {http://arxiv.org/abs/2202.08141},
	abstract = {Automatic surgical instrument segmentation of endoscopic images is a crucial building block of many computer-assistance applications for minimally invasive surgery. So far, state-of-the-art approaches completely rely on the availability of a ground-truth supervision signal, obtained via manual annotation, thus expensive to collect at large scale. In this paper, we present FUN-SIS, a Fully-UNsupervised approach for binary Surgical Instrument Segmentation. FUN-SIS trains a per-frame segmentation model on completely unlabelled endoscopic videos, by solely relying on implicit motion information and instrument shape-priors. We define shape-priors as realistic segmentation masks of the instruments, not necessarily coming from the same dataset/domain as the videos. The shape-priors can be collected in various and convenient ways, such as recycling existing annotations from other datasets. We leverage them as part of a novel generative-adversarial approach, allowing to perform unsupervised instrument segmentation of optical-flow images during training. We then use the obtained instrument masks as pseudo-labels in order to train a per-frame segmentation model; to this aim, we develop a learning-from-noisy-labels architecture, designed to extract a clean supervision signal from these pseudo-labels, leveraging their peculiar noise properties. We validate the proposed contributions on three surgical datasets, including the MICCAI 2017 EndoVis Robotic Instrument Segmentation Challenge dataset. The obtained fully-unsupervised results for surgical instrument segmentation are almost on par with the ones of fully-supervised state-of-the-art approaches. This suggests the tremendous potential of the proposed method to leverage the great amount of unlabelled data produced in the context of minimally invasive surgery.},
	urldate = {2022-05-04},
	journal = {arXiv:2202.08141 [cs]},
	author = {Sestini, Luca and Rosa, Benoit and De Momi, Elena and Ferrigno, Giancarlo and Padoy, Nicolas},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.08141},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#TOSKIM, Unsupervised Learning, Segmentation, CholecXXX},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\YR4DDC44\\Sestini et al. - 2022 - FUN-SIS a Fully UNsupervised approach for Surgica.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\VS3JXPYN\\2202.html:text/html},
}

@article{nwoye_data_2022,
	title = {Data {Splits} and {Metrics} for {Method} {Benchmarking} on {Surgical} {Action} {Triplet} {Datasets}},
	url = {http://arxiv.org/abs/2204.05235},
	abstract = {In addition to generating data and annotations, devising sensible data splitting strategies and evaluation metrics is essential for the creation of a benchmark dataset. This practice ensures consensus on the usage of the data, homogeneous assessment, and uniform comparison of research methods on the dataset. This study focuses on CholecT50, which is a 50 video surgical dataset that formalizes surgical activities as triplets of {\textless}instrument, verb, target{\textgreater}. In this paper, we introduce the standard splits for the CholecT50 and CholecT45 datasets and show how they compare with existing use of the dataset. CholecT45 is the first public release of 45 videos of CholecT50 dataset. We also develop a metrics library, ivtmetrics, for model evaluation on surgical triplets. Furthermore, we conduct a benchmark study by reproducing baseline methods in the most predominantly used deep learning frameworks (PyTorch and TensorFlow) to evaluate them using the proposed data splits and metrics and release them publicly to support future research. The proposed data splits and evaluation metrics will enable global tracking of research progress on the dataset and facilitate optimal model selection for further deployment.},
	urldate = {2022-05-04},
	journal = {arXiv:2204.05235 [cs]},
	author = {Nwoye, Chinedu Innocent and Padoy, Nicolas},
	month = apr,
	year = {2022},
	note = {arXiv: 2204.05235},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Dataset, \#NONE},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\ZF3GI3C3\\Nwoye and Padoy - 2022 - Data Splits and Metrics for Method Benchmarking on.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\T6LF9W3W\\2204.html:text/html},
}

@article{stulberg_association_2020,
	title = {Association {Between} {Surgeon} {Technical} {Skills} and {Patient} {Outcomes}},
	volume = {155},
	issn = {2168-6262},
	doi = {10.1001/jamasurg.2020.3007},
	abstract = {Importance: Postoperative complications remain common after surgery, but little is known about the extent of variation in operative technical skill and whether variation is associated with patient outcomes.
Objectives: To examine the (1) variation in technical skill scores of practicing surgeons, (2) association between technical skills and patient outcomes, and (3) amount of variation in patient outcomes explained by a surgeon's technical skill.
Design, Setting, and Participants: In this quality improvement study, 17 practicing surgeons submitted a video of a laparoscopic right hemicolectomy that was then rated by at least 10 blinded peer surgeons and 2 expert raters. The association between surgeon technical skill scores and risk-adjusted outcomes was examined using data from the American College of Surgeons National Surgical Quality Improvement Program. The association between technical skill scores and outcomes was examined for colorectal procedures and noncolorectal procedures (ie, assessed on whether technical skills demonstrated during colectomy were associated with patient outcomes across other cases). In addition, the proportion of patient outcomes explained by technical skill scores was examined using robust regression techniques. The study was conducted from September 23, 2016, to February 10, 2018; data analysis was performed from November 2018 to January 2019.
Exposures: Colorectal and noncolorectal procedures.
Main Outcomes and Measures: Any complication, mortality, unplanned hospital readmission, unplanned reoperation related to principal procedure, surgical site infection, and death or serious morbidity.
Results: Of the 17 surgeons included in the study, 13 were men (76\%). The participants had a range from 1 to 28 years in surgical practice (median, 11 years). Based on 10 or more reviewers per video and with a maximum quality score of 5, overall technical skill scores ranged from 2.8 to 4.6. From 2014 to 2016, study participants performed a total of 3063 procedures (1120 colectomies). Higher technical skill scores were significantly associated with lower rates of any complication (15.5\% vs 20.6\%, P = .03; Spearman rank-order correlation coefficient r = -0.54, P = .03), unplanned reoperation (4.7\% vs 7.2\%, P = .02; r = -0.60, P = .01), and a composite measure of death or serious morbidity (15.9\% vs 21.4\%, P = .02; r = -0.60, P = .01) following colectomy. Similar associations were found between colectomy technical skill scores and patient outcomes for all types of procedures performed by a surgeon. Overall, technical skill scores appeared to account for 25.8\% of the variation in postcolectomy complication rates and 27.5\% of the variation when including noncolectomy complication rates.
Conclusions and Relevance: The findings of this study suggest that there is wide variation in technical skill among practicing surgeons, accounting for more than 25\% of the variation in patient outcomes. Higher colectomy technical skill scores appear to be associated with lower complication rates for colectomy and for all other procedures performed by a surgeon. Efforts to improve surgeon technical skills may result in better patient outcomes.},
	language = {eng},
	number = {10},
	journal = {JAMA surgery},
	author = {Stulberg, Jonah J. and Huang, Reiping and Kreutzer, Lindsey and Ban, Kristen and Champagne, Bradley J. and Steele, Scott R. and Johnson, Julie K. and Holl, Jane L. and Greenberg, Caprice C. and Bilimoria, Karl Y.},
	month = oct,
	year = {2020},
	pmid = {32838425},
	pmcid = {PMC7439214},
	keywords = {\#NONE, Humans, Laparoscopy, Clinical Competence, Colectomy, Female, Male, Postoperative Complications, Prospective Studies, Quality Improvement, Treatment Outcome, CAMP2},
	pages = {960--968},
	file = {Full Text:C\:\\Users\\felix\\Zotero\\storage\\TI8ZDB3G\\Stulberg et al. - 2020 - Association Between Surgeon Technical Skills and P.pdf:application/pdf},
}

@article{kadkhodamohammadi_patg_2022,
	title = {{PATG}: position-aware temporal graph networks for surgical phase recognition on laparoscopic videos},
	issn = {1861-6429},
	shorttitle = {{PATG}},
	url = {https://doi.org/10.1007/s11548-022-02600-8},
	doi = {10.1007/s11548-022-02600-8},
	abstract = {We tackle the problem of online surgical phase recognition in laparoscopic procedures, which is key in developing context-aware supporting systems. We propose a novel approach to take temporal context in surgical videos into account by precise modeling of temporal neighborhoods.},
	language = {en},
	urldate = {2022-05-12},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {Kadkhodamohammadi, Abdolrahim and Luengo, Imanol and Stoyanov, Danail},
	month = mar,
	year = {2022},
	keywords = {\#READ, GNN, Positional encoder, Surgical AI, Surgical data science, Surgical phase recognition, Workflow analysis, IPCAI 2022, Surgical Phase Recognition},
	file = {Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\EMTLJK3A\\Kadkhodamohammadi et al. - 2022 - PATG position-aware temporal graph networks for s.pdf:application/pdf},
}

@article{sanchez-matilla_data-centric_2022,
	title = {Data-centric multi-task surgical phase estimation with sparse scene segmentation},
	issn = {1861-6429},
	url = {https://doi.org/10.1007/s11548-022-02616-0},
	doi = {10.1007/s11548-022-02616-0},
	abstract = {Surgical workflow estimation techniques aim to divide a surgical video into temporal segments based on predefined surgical actions or objectives, which can be of different granularity such as steps or phases. Potential applications range from real-time intra-operative feedback to automatic post-operative reports and analysis. A common approach in the literature for performing automatic surgical phase estimation is to decouple the problem into two stages: feature extraction from a single frame and temporal feature fusion. This approach is performed in two stages due to computational restrictions when processing large spatio-temporal sequences.},
	language = {en},
	urldate = {2022-05-12},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {Sanchez-Matilla, Ricardo and Robu, Maria and Grammatikopoulou, Maria and Luengo, Imanol and Stoyanov, Danail},
	month = may,
	year = {2022},
	keywords = {\#TOREAD, Surgical data science, IPCAI 2022, Surgical Phase Recognition, Multi-task, Scene segmentation, Surgical phases, CholecSeg8k, CholecXXX},
	file = {Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\GG7KS625\\Sanchez-Matilla et al. - 2022 - Data-centric multi-task surgical phase estimation .pdf:application/pdf},
}

@article{hamoud_self-supervised_2022,
	title = {Self-supervised learning via cluster distance prediction for operating room context awareness},
	issn = {1861-6429},
	url = {https://doi.org/10.1007/s11548-022-02629-9},
	doi = {10.1007/s11548-022-02629-9},
	abstract = {Semantic segmentation and activity classification are key components to create intelligent surgical systems able to understand and assist clinical workflow. In the operating room, semantic segmentation is at the core of creating robots aware of clinical surroundings, whereas activity classification aims at understanding OR workflow at a higher level. State-of-the-art semantic segmentation and activity recognition approaches are fully supervised, which is not scalable. Self-supervision can decrease the amount of annotated data needed.},
	language = {en},
	urldate = {2022-05-12},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {Hamoud, Idris and Karargyris, Alexandros and Sharghi, Aidean and Mohareri, Omid and Padoy, Nicolas},
	month = apr,
	year = {2022},
	keywords = {\#SKIMMED, IPCAI 2022, Activity classification, da Vinci surgical system, OR scene understanding, Semantic segmentation, External view, SSL},
	file = {Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\SJSN5I5Q\\Hamoud et al. - 2022 - Self-supervised learning via cluster distance pred.pdf:application/pdf},
}

@article{basiev_open_2021,
	title = {Open surgery tool classification and hand utilization using a multi-camera system},
	url = {http://arxiv.org/abs/2111.06098},
	abstract = {Purpose: The goal of this work is to use multi-camera video to classify open surgery tools as well as identify which tool is held in each hand. Multi-camera systems help prevent occlusions in open surgery video data. Furthermore, combining multiple views such as a Top-view camera covering the full operative field and a Close-up camera focusing on hand motion and anatomy, may provide a more comprehensive view of the surgical workflow. However, multi-camera data fusion poses a new challenge: a tool may be visible in one camera and not the other. Thus, we defined the global ground truth as the tools being used regardless their visibility. Therefore, tools that are out of the image should be remembered for extensive periods of time while the system responds quickly to changes visible in the video. Methods: Participants (n=48) performed a simulated open bowel repair. A Top-view and a Close-up cameras were used. YOLOv5 was used for tool and hand detection. A high frequency LSTM with a 1 second window at 30 frames per second (fps) and a low frequency LSTM with a 40 second window at 3 fps were used for spatial, temporal, and multi-camera integration. Results: The accuracy and F1 of the six systems were: Top-view (0.88/0.88), Close-up (0.81,0.83), both cameras (0.9/0.9), high fps LSTM (0.92/0.93), low fps LSTM (0.9/0.91), and our final architecture the Multi-camera classifier(0.93/0.94). Conclusion: By combining a system with a high fps and a low fps from the multiple camera array we improved the classification abilities of the global ground truth.},
	urldate = {2022-05-12},
	journal = {arXiv:2111.06098 [cs]},
	author = {Basiev, Kristina and Goldbraikh, Adam and Pugh, Carla M. and Laufer, Shlomi},
	month = nov,
	year = {2021},
	note = {arXiv: 2111.06098},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#SKIMMED, IPCAI 2022},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\J4IJUYJ7\\Basiev et al. - 2021 - Open surgery tool classification and hand utilizat.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\3T56ILWK\\2111.html:text/html},
}

@article{das_reducing_2022,
	title = {Reducing prediction volatility in the surgical workflow recognition of endoscopic pituitary surgery},
	issn = {1861-6429},
	url = {https://doi.org/10.1007/s11548-022-02599-y},
	doi = {10.1007/s11548-022-02599-y},
	abstract = {Workflow recognition can aid surgeons before an operation when used as a training tool, during an operation by increasing operating room efficiency, and after an operation in the completion of operation notes. Although several methods have been applied to this task, they have been tested on few surgical datasets. Therefore, their generalisability is not well tested, particularly for surgical approaches utilising smaller working spaces which are susceptible to occlusion and necessitate frequent withdrawal of the endoscope. This leads to rapidly changing predictions, which reduces the clinical confidence of the methods, and hence limits their suitability for clinical translation.},
	language = {en},
	urldate = {2022-05-12},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {Das, Adrito and Bano, Sophia and Vasconcelos, Francisco and Khan, Danyal Z. and Marcus, Hani J. and Stoyanov, Danail},
	month = apr,
	year = {2022},
	keywords = {\#SKIMMED, IPCAI 2022, Surgical video analysis, Temporal smoothing functions},
	file = {Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\BCYPW2FJ\\Das et al. - 2022 - Reducing prediction volatility in the surgical wor.pdf:application/pdf},
}

@article{khan_automated_2021,
	title = {Automated operative workflow analysis of endoscopic pituitary surgery using machine learning: development and preclinical evaluation ({IDEAL} stage 0)},
	volume = {-1},
	issn = {1933-0693, 0022-3085},
	shorttitle = {Automated operative workflow analysis of endoscopic pituitary surgery using machine learning},
	url = {https://thejns.org/view/journals/j-neurosurg/aop/article-10.3171-2021.6.JNS21923/article-10.3171-2021.6.JNS21923.xml},
	doi = {10.3171/2021.6.JNS21923},
	abstract = {OBJECTIVE Surgical workflow analysis involves systematically breaking down operations into key phases and steps. Automatic analysis of this workflow has potential uses for surgical training, preoperative planning, and outcome prediction. Recent advances in machine learning (ML) and computer vision have allowed accurate automated workflow analysis of operative videos. In this Idea, Development, Exploration, Assessment, Long-term study (IDEAL) stage 0 study, the authors sought to use Touch Surgery for the development and validation of an ML-powered analysis of phases and steps in the endoscopic transsphenoidal approach (eTSA) for pituitary adenoma resection, a first for neurosurgery. METHODS The surgical phases and steps of 50 anonymized eTSA operative videos were labeled by expert surgeons. Forty videos were used to train a combined convolutional and recurrent neural network model by Touch Surgery. Ten videos were used for model evaluation (accuracy, F1 score), comparing the phase and step recognition of surgeons to the automatic detection of the ML model. RESULTS The longest phase was the sellar phase (median 28 minutes), followed by the nasal phase (median 22 minutes) and the closure phase (median 14 minutes). The longest steps were step 5 (tumor identification and excision, median 17 minutes); step 3 (posterior septectomy and removal of sphenoid septations, median 14 minutes); and step 4 (anterior sellar wall removal, median 10 minutes). There were substantial variations within the recorded procedures in terms of video appearances, step duration, and step order, with only 50\% of videos containing all 7 steps performed sequentially in numerical order. Despite this, the model was able to output accurate recognition of surgical phases (91\% accuracy, 90\% F1 score) and steps (76\% accuracy, 75\% F1 score). CONCLUSIONS In this IDEAL stage 0 study, ML techniques have been developed to automatically analyze operative videos of eTSA pituitary surgery. This technology has previously been shown to be acceptable to neurosurgical teams and patients. ML-based surgical workflow analysis has numerous potential uses—such as education (e.g., automatic indexing of contemporary operative videos for teaching), improved operative efficiency (e.g., orchestrating the entire surgical team to a common workflow), and improved patient outcomes (e.g., comparison of surgical techniques or early detection of adverse events). Future directions include the real-time integration of Touch Surgery into the live operative environment as an IDEAL stage 1 (first-in-human) study, and further development of underpinning ML models using larger data sets.},
	language = {EN},
	number = {aop},
	urldate = {2022-05-16},
	journal = {Journal of Neurosurgery},
	author = {Khan, Danyal Z. and Luengo, Imanol and Barbarisi, Santiago and Addis, Carole and Culshaw, Lucy and Dorward, Neil L. and Haikka, Pinja and Jain, Abhiney and Kerr, Karen and Koh, Chan Hee and Horsfall, Hugo Layard and Muirhead, William and Palmisciano, Paolo and Vasey, Baptiste and Stoyanov, Danail and Marcus, Hani J.},
	month = nov,
	year = {2021},
	note = {Publisher: American Association of Neurological Surgeons
Section: Journal of Neurosurgery},
	pages = {1--8},
	file = {Snapshot:C\:\\Users\\felix\\Zotero\\storage\\JW4B3YBP\\article-10.3171-2021.6.JNS21923.html:text/html},
}

@article{khan_automated_2021-1,
	title = {Automated operative workflow analysis of endoscopic pituitary surgery using machine learning: development and preclinical evaluation ({IDEAL} stage 0)},
	issn = {1933-0693},
	shorttitle = {Automated operative workflow analysis of endoscopic pituitary surgery using machine learning},
	doi = {10.3171/2021.6.JNS21923},
	abstract = {OBJECTIVE: Surgical workflow analysis involves systematically breaking down operations into key phases and steps. Automatic analysis of this workflow has potential uses for surgical training, preoperative planning, and outcome prediction. Recent advances in machine learning (ML) and computer vision have allowed accurate automated workflow analysis of operative videos. In this Idea, Development, Exploration, Assessment, Long-term study (IDEAL) stage 0 study, the authors sought to use Touch Surgery for the development and validation of an ML-powered analysis of phases and steps in the endoscopic transsphenoidal approach (eTSA) for pituitary adenoma resection, a first for neurosurgery.
METHODS: The surgical phases and steps of 50 anonymized eTSA operative videos were labeled by expert surgeons. Forty videos were used to train a combined convolutional and recurrent neural network model by Touch Surgery. Ten videos were used for model evaluation (accuracy, F1 score), comparing the phase and step recognition of surgeons to the automatic detection of the ML model.
RESULTS: The longest phase was the sellar phase (median 28 minutes), followed by the nasal phase (median 22 minutes) and the closure phase (median 14 minutes). The longest steps were step 5 (tumor identification and excision, median 17 minutes); step 3 (posterior septectomy and removal of sphenoid septations, median 14 minutes); and step 4 (anterior sellar wall removal, median 10 minutes). There were substantial variations within the recorded procedures in terms of video appearances, step duration, and step order, with only 50\% of videos containing all 7 steps performed sequentially in numerical order. Despite this, the model was able to output accurate recognition of surgical phases (91\% accuracy, 90\% F1 score) and steps (76\% accuracy, 75\% F1 score).
CONCLUSIONS: In this IDEAL stage 0 study, ML techniques have been developed to automatically analyze operative videos of eTSA pituitary surgery. This technology has previously been shown to be acceptable to neurosurgical teams and patients. ML-based surgical workflow analysis has numerous potential uses-such as education (e.g., automatic indexing of contemporary operative videos for teaching), improved operative efficiency (e.g., orchestrating the entire surgical team to a common workflow), and improved patient outcomes (e.g., comparison of surgical techniques or early detection of adverse events). Future directions include the real-time integration of Touch Surgery into the live operative environment as an IDEAL stage 1 (first-in-human) study, and further development of underpinning ML models using larger data sets.},
	language = {eng},
	journal = {Journal of Neurosurgery},
	author = {Khan, Danyal Z. and Luengo, Imanol and Barbarisi, Santiago and Addis, Carole and Culshaw, Lucy and Dorward, Neil L. and Haikka, Pinja and Jain, Abhiney and Kerr, Karen and Koh, Chan Hee and Layard Horsfall, Hugo and Muirhead, William and Palmisciano, Paolo and Vasey, Baptiste and Stoyanov, Danail and Marcus, Hani J.},
	month = nov,
	year = {2021},
	pmid = {34740198},
	keywords = {Computer vision, endoscopic transsphenoidal surgery, pituitary adenoma, pituitary surgery, surgical workflow, Machine Learning, Neural Networks, AI},
	pages = {1--8},
}

@techreport{ying_transformers_2021,
	title = {Do {Transformers} {Really} {Perform} {Bad} for {Graph} {Representation}?},
	url = {http://arxiv.org/abs/2106.05234},
	abstract = {The Transformer architecture has become a dominant choice in many domains, such as natural language processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how Transformers could perform well for graph representation learning. In this paper, we solve this mystery by presenting Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of effectively encoding the structural information of a graph into the model. To this end, we propose several simple yet effective structural encoding methods to help Graphormer better model graph-structured data. Besides, we mathematically characterize the expressive power of Graphormer and exhibit that with our ways of encoding the structural information of graphs, many popular GNN variants could be covered as the special cases of Graphormer.},
	number = {arXiv:2106.05234},
	urldate = {2022-05-17},
	institution = {arXiv},
	author = {Ying, Chengxuan and Cai, Tianle and Luo, Shengjie and Zheng, Shuxin and Ke, Guolin and He, Di and Shen, Yanming and Liu, Tie-Yan},
	month = nov,
	year = {2021},
	doi = {10.48550/arXiv.2106.05234},
	note = {arXiv:2106.05234 [cs]
type: article},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, \#TOSKIM, GNN},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\KXZBVHJH\\Ying et al. - 2021 - Do Transformers Really Perform Bad for Graph Repre.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\539CI5TL\\2106.html:text/html},
}

@misc{noauthor_unreasonable_2015,
	title = {The {Unreasonable} {Effectiveness} of {Recurrent} {Neural} {Networks} - karpathy.github.io/},
	url = {http://karpathy.github.io/2015/05/21/rnn-effectiveness/},
	urldate = {2022-05-17},
	year = {2015},
	keywords = {\#READ, RNN},
	file = {The Unreasonable Effectiveness of Recurrent Neural Networks - karpathy.github.io/:C\:\\Users\\felix\\Zotero\\storage\\EAVUZXE6\\rnn-effectiveness.html:text/html},
}

@misc{giraud-carrier_toward_2005,
	title = {Toward a {Justification} of {Meta}-learning : {Is} the {No} {Free} {Lunch} {Theorem} a {Showstopper} ?},
	shorttitle = {Toward a {Justification} of {Meta}-learning},
	url = {https://www.semanticscholar.org/paper/Toward-a-Justification-of-Meta-learning-%3A-Is-the-No-Giraud-Carrier/fee1abe79f179f465d2725be63e97a50034bc511},
	abstract = {It is shown that, although cross-validation still is not a viable way to construct general-purpose learning algorithms, meta-learning offers a natural alternative, and the No Free Lunch theorem is of little relevance to research in Machine Learning. We present a preliminary analysis of the fundamental viability of meta-learning, revisiting the No Free Lunch (NFL) theorem. The analysis shows that given some simple and very basic assumptions, the NFL theorem is of little relevance to research in Machine Learning. We augment the basic NFL framework to illustrate that the notion of an Ultimate Learning Algorithm is well defined. We show that, although cross-validation still is not a viable way to construct general-purpose learning algorithms, meta-learning offers a natural alternative. We still have to pay for our lunch, but the cost is reasonable: the necessary fundamental assumptions are ones we all make anyway.},
	language = {en},
	urldate = {2022-05-18},
	author = {Giraud-Carrier, C.},
	year = {2005},
	keywords = {\#TOSKIM},
	file = {Snapshot:C\:\\Users\\felix\\Zotero\\storage\\HI6APTPM\\fee1abe79f179f465d2725be63e97a50034bc511.html:text/html},
}

@article{giraud-carrier_toward_2005-1,
	title = {Toward a justification of meta-learning: {Is} the no free lunch theorem a show-stopper?},
	shorttitle = {Toward a justification of meta-learning},
	abstract = {We present a preliminary analysis of the fun-damental viability of meta-learning, revisit-ing the No Free Lunch (NFL) theorem. The analysis shows that given some simple and very basic assumptions, the NFL theorem is of little relevance to research in Machine Learning. We augment the basic NFL frame-work to illustrate that the notion of an Ulti-mate Learning Algorithm is well defined. We show that, although cross-validation still is not a viable way to construct general-purpose learning algorithms, meta-learning offers a natural alternative. We still have to pay for our lunch, but the cost is reasonable: the nec-essary fundamental assumptions are ones we all make anyway.},
	journal = {Proceedings of the ICML-2005 Workshop on Meta-learning},
	author = {Giraud-Carrier, Christophe and Provost, Foster},
	month = jan,
	year = {2005},
	keywords = {\#TOSKIM},
	file = {Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\ZNU7YCXY\\Giraud-Carrier and Provost - 2005 - Toward a justification of meta-learning Is the no.pdf:application/pdf},
}

@article{ho_people_2022,
	title = {People construct simplified mental representations to plan},
	copyright = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-022-04743-9},
	doi = {10.1038/s41586-022-04743-9},
	abstract = {One of the most striking features of human cognition is the ability to plan. Two aspects of human planning stand out—its efficiency and flexibility. Efficiency is especially impressive because plans must often be made in complex environments, and yet people successfully plan solutions to many everyday problems despite having limited cognitive resources1–3. Standard accounts in psychology, economics and artificial intelligence have suggested that human planning succeeds because people have a complete representation of a task and then use heuristics to plan future actions in that representation4–11. However, this approach generally assumes that task representations are fixed. Here we propose that task representations can be controlled and that such control provides opportunities to quickly simplify problems and more easily reason about them. We propose a computational account of this simplification process and, in a series of preregistered behavioural experiments, show that it is subject to online cognitive control12–14 and that people optimally balance the complexity of a task representation and its utility for planning and acting. These results demonstrate how strategically perceiving and conceiving problems facilitates the effective use of limited cognitive resources.},
	language = {en},
	urldate = {2022-05-21},
	journal = {Nature},
	author = {Ho, Mark K. and Abel, David and Correa, Carlos G. and Littman, Michael L. and Cohen, Jonathan D. and Griffiths, Thomas L.},
	month = may,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {\#SKIMMED, Decision making, Human behaviour},
	pages = {1--8},
	file = {Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\FGENQ99T\\Ho et al. - 2022 - People construct simplified mental representations.pdf:application/pdf;Snapshot:C\:\\Users\\felix\\Zotero\\storage\\WEKPB8NV\\s41586-022-04743-9.html:text/html},
}

@techreport{ding_free_2022,
	title = {Free {Lunch} for {Surgical} {Video} {Understanding} by {Distilling} {Self}-{Supervisions}},
	url = {http://arxiv.org/abs/2205.09292},
	abstract = {Self-supervised learning has witnessed great progress in vision and NLP; recently, it also attracted much attention to various medical imaging modalities such as X-ray, CT, and MRI. Existing methods mostly focus on building new pretext self-supervision tasks such as reconstruction, orientation, and masking identification according to the properties of medical images. However, the publicly available self-supervision models are not fully exploited. In this paper, we present a powerful yet efficient self-supervision framework for surgical video understanding. Our key insight is to distill knowledge from publicly available models trained on large generic datasets4 to facilitate the self-supervised learning of surgical videos. To this end, we first introduce a semantic-preserving training scheme to obtain our teacher model, which not only contains semantics from the publicly available models, but also can produce accurate knowledge for surgical data. Besides training with only contrastive learning, we also introduce a distillation objective to transfer the rich learned information from the teacher model to self-supervised learning on surgical data. Extensive experiments on two surgical phase recognition benchmarks show that our framework can significantly improve the performance of existing self-supervised learning methods. Notably, our framework demonstrates a compelling advantage under a low-data regime. Our code is available at https://github.com/xmed-lab/DistillingSelf.},
	number = {arXiv:2205.09292},
	urldate = {2022-05-21},
	institution = {arXiv},
	author = {Ding, Xinpeng and Liu, Ziwei and Li, Xiaomeng},
	month = may,
	year = {2022},
	doi = {10.48550/arXiv.2205.09292},
	note = {arXiv:2205.09292 [cs]
version: 1
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#READ, SSL},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\JKVW5V6R\\Ding et al. - 2022 - Free Lunch for Surgical Video Understanding by Dis.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\9BLS3GBQ\\2205.html:text/html},
}

@techreport{yang_convolutions_2022,
	title = {Convolutions are competitive with transformers for protein sequence pretraining},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. The copyright holder has placed this preprint in the Public Domain. It is no longer restricted by copyright. Anyone can legally share, reuse, remix, or adapt this material for any purpose without crediting the original authors.},
	url = {https://www.biorxiv.org/content/10.1101/2022.05.19.492714v1},
	abstract = {Pretrained protein sequence language models largely rely on the transformer architecture. However, transformer run-time and memory requirements scale quadratically with sequence length. We investigate the potential of a convolution-based architecture for protein sequence masked language model pretraining and subsequent finetuning. CNNs are competitive on the pretraining task with transformers across several orders of magnitude in parameter size while scaling linearly with sequence length. More importantly, CNNs are competitive with and occasionally superior to transformers across an extensive set of downstream evaluations, including structure prediction, zero-shot mutation effect prediction, and out-of-domain generalization. We also demonstrate strong performance on sequences longer than the positional embeddings allowed in the current state-of-the-art transformer protein masked language models. Finally, we close with a call to disentangle the effects of pretraining task and model architecture when studying pretrained protein sequence models.},
	language = {en},
	urldate = {2022-05-23},
	institution = {bioRxiv},
	author = {Yang, Kevin K. and Lu, Alex X. and Fusi, Nicolo K.},
	month = may,
	year = {2022},
	doi = {10.1101/2022.05.19.492714},
	note = {Section: New Results
Type: article},
	keywords = {Transformers, \#SKIMMED, ConvNets},
	pages = {2022.05.19.492714},
	file = {Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\N2CVJLYT\\Yang et al. - 2022 - Convolutions are competitive with transformers for.pdf:application/pdf;Snapshot:C\:\\Users\\felix\\Zotero\\storage\\96VMQCYQ\\2022.05.19.html:text/html},
}

@techreport{wu_pay_2019,
	title = {Pay {Less} {Attention} with {Lightweight} and {Dynamic} {Convolutions}},
	url = {http://arxiv.org/abs/1901.10430},
	abstract = {Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.},
	number = {arXiv:1901.10430},
	urldate = {2022-05-23},
	institution = {arXiv},
	author = {Wu, Felix and Fan, Angela and Baevski, Alexei and Dauphin, Yann N. and Auli, Michael},
	month = feb,
	year = {2019},
	doi = {10.48550/arXiv.1901.10430},
	note = {arXiv:1901.10430 [cs]
type: article},
	keywords = {Transformers, \#SKIMMED, Computer Science - Computation and Language, ConvNets},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\TQHSCL5F\\Wu et al. - 2019 - Pay Less Attention with Lightweight and Dynamic Co.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\D97H56XI\\1901.html:text/html},
}

@misc{weng_what_2021,
	title = {What are {Diffusion} {Models}?},
	url = {https://lilianweng.github.io/posts/2021-07-11-diffusion-models/},
	abstract = {[Updated on 2021-09-19: Highly recommend this blog post on score-based generative modeling by Yang Song (author of several key papers in the references)].
So far, I’ve written about three types of generative models, GAN, VAE, and Flow-based models. They have shown great success in generating high-quality samples, but each has some limitations of its own. GAN models are known for potentially unstable training and less diversity in generation due to their adversarial training nature.},
	language = {en},
	urldate = {2022-05-24},
	author = {Weng, Lilian},
	month = jul,
	year = {2021},
	note = {Section: posts},
	keywords = {\#SKIMMED, Generative Models, Diffusion Model},
	file = {Snapshot:C\:\\Users\\felix\\Zotero\\storage\\PR96XTAS\\2021-07-11-diffusion-models.html:text/html},
}

@misc{phd_diffusion_2022,
	title = {Diffusion {Models} {Made} {Easy}},
	url = {https://towardsdatascience.com/diffusion-models-made-easy-8414298ce4da},
	abstract = {Understanding the Basics of Denoising Diffusion Probabilistic Models},
	language = {en},
	urldate = {2022-05-24},
	journal = {Medium},
	author = {PhD, J. Rafid Siddiqui},
	month = may,
	year = {2022},
	keywords = {\#SKIMMED, Generative Models},
	file = {Snapshot:C\:\\Users\\felix\\Zotero\\storage\\MFGUHFIM\\diffusion-models-made-easy-8414298ce4da.html:text/html},
}

@techreport{qi_learning_2018,
	title = {Learning {Human}-{Object} {Interactions} by {Graph} {Parsing} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1808.07962},
	abstract = {This paper addresses the task of detecting and recognizing human-object interactions (HOI) in images and videos. We introduce the Graph Parsing Neural Network (GPNN), a framework that incorporates structural knowledge while being differentiable end-to-end. For a given scene, GPNN infers a parse graph that includes i) the HOI graph structure represented by an adjacency matrix, and ii) the node labels. Within a message passing inference framework, GPNN iteratively computes the adjacency matrices and node labels. We extensively evaluate our model on three HOI detection benchmarks on images and videos: HICO-DET, V-COCO, and CAD-120 datasets. Our approach significantly outperforms state-of-art methods, verifying that GPNN is scalable to large datasets and applies to spatial-temporal settings. The code is available at https://github.com/SiyuanQi/gpnn.},
	number = {arXiv:1808.07962},
	urldate = {2022-06-01},
	institution = {arXiv},
	author = {Qi, Siyuan and Wang, Wenguan and Jia, Baoxiong and Shen, Jianbing and Zhu, Song-Chun},
	month = aug,
	year = {2018},
	doi = {10.48550/arXiv.1808.07962},
	note = {arXiv:1808.07962 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#READ, GDLMA 2022, GNN},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\4UQSVS3H\\Qi et al. - 2018 - Learning Human-Object Interactions by Graph Parsin.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\T6IZ2K9K\\1808.html:text/html},
}

@techreport{saharia_photorealistic_2022,
	title = {Photorealistic {Text}-to-{Image} {Diffusion} {Models} with {Deep} {Language} {Understanding}},
	url = {http://arxiv.org/abs/2205.11487},
	abstract = {We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.},
	number = {arXiv:2205.11487},
	urldate = {2022-06-01},
	institution = {arXiv},
	author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S. Sara and Lopes, Rapha Gontijo and Salimans, Tim and Ho, Jonathan and Fleet, David J. and Norouzi, Mohammad},
	month = may,
	year = {2022},
	doi = {10.48550/arXiv.2205.11487},
	note = {arXiv:2205.11487 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, \#TOSKIM, Diffusion Model},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\EZ5HDITH\\Saharia et al. - 2022 - Photorealistic Text-to-Image Diffusion Models with.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\PJHK7PT8\\2205.html:text/html},
}

@inproceedings{shen_scaling_2018,
	title = {Scaling {Human}-{Object} {Interaction} {Recognition} {Through} {Zero}-{Shot} {Learning}},
	doi = {10.1109/WACV.2018.00181},
	abstract = {Recognizing human object interactions (HOI) is an important part of distinguishing the rich variety of human action in the visual world. While recent progress has been made in improving HOI recognition in the fully supervised setting, the space of possible human-object interactions is large and it is impractical to obtain labeled training data for all interactions of interest. In this work, we tackle the challenge of scaling HOI recognition to the long tail of categories through a zero-shot learning approach. We introduce a factorized model for HOI detection that disentangles reasoning on verbs and objects, and at test-time can therefore produce detections for novel verb-object pairs. We present experiments on the recently introduced large-scale HICODET dataset, and show that our model is able to both perform comparably to state-of-the-art in fully-supervised HOI detection, while simultaneously achieving effective zeroshot detection of new HOI categories.},
	booktitle = {2018 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Shen, Liyue and Yeung, Serena and Hoffman, Judy and Mori, Greg and Fei-Fei, Li},
	month = mar,
	year = {2018},
	keywords = {\#TOSKIM, HOI, Feature extraction, Heating systems, Task analysis, Training, Training data, Visualization, Object Detection},
	pages = {1568--1576},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\felix\\Zotero\\storage\\4DH5A4ZJ\\8354279.html:text/html;Shen et al. - 2018 - Scaling Human-Object Interaction Recognition Throu.pdf:C\:\\Users\\felix\\Zotero\\storage\\BZCUGE6S\\Shen et al. - 2018 - Scaling Human-Object Interaction Recognition Throu.pdf:application/pdf},
}

@techreport{chao_learning_2018,
	title = {Learning to {Detect} {Human}-{Object} {Interactions}},
	url = {http://arxiv.org/abs/1702.05448},
	abstract = {We study the problem of detecting human-object interactions (HOI) in static images, defined as predicting a human and an object bounding box with an interaction class label that connects them. HOI detection is a fundamental problem in computer vision as it provides semantic information about the interactions among the detected objects. We introduce HICO-DET, a new large benchmark for HOI detection, by augmenting the current HICO classification benchmark with instance annotations. To solve the task, we propose Human-Object Region-based Convolutional Neural Networks (HO-RCNN). At the core of our HO-RCNN is the Interaction Pattern, a novel DNN input that characterizes the spatial relations between two bounding boxes. Experiments on HICO-DET demonstrate that our HO-RCNN, by exploiting human-object spatial relations through Interaction Patterns, significantly improves the performance of HOI detection over baseline approaches.},
	number = {arXiv:1702.05448},
	urldate = {2022-06-06},
	institution = {arXiv},
	author = {Chao, Yu-Wei and Liu, Yunfan and Liu, Xieyang and Zeng, Huayi and Deng, Jia},
	month = feb,
	year = {2018},
	doi = {10.48550/arXiv.1702.05448},
	note = {arXiv:1702.05448 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Dataset, \#NONE},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\N4HLWRRI\\Chao et al. - 2018 - Learning to Detect Human-Object Interactions.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\QKATBHKE\\1702.html:text/html},
}

@techreport{gupta_visual_2015,
	title = {Visual {Semantic} {Role} {Labeling}},
	url = {http://arxiv.org/abs/1505.04474},
	abstract = {In this paper we introduce the problem of Visual Semantic Role Labeling: given an image we want to detect people doing actions and localize the objects of interaction. Classical approaches to action recognition either study the task of action classification at the image or video clip level or at best produce a bounding box around the person doing the action. We believe such an output is inadequate and a complete understanding can only come when we are able to associate objects in the scene to the different semantic roles of the action. To enable progress towards this goal, we annotate a dataset of 16K people instances in 10K images with actions they are doing and associate objects in the scene with different semantic roles for each action. Finally, we provide a set of baseline algorithms for this task and analyze error modes providing directions for future work.},
	number = {arXiv:1505.04474},
	urldate = {2022-06-06},
	institution = {arXiv},
	author = {Gupta, Saurabh and Malik, Jitendra},
	month = may,
	year = {2015},
	doi = {10.48550/arXiv.1505.04474},
	note = {arXiv:1505.04474 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Dataset, \#NONE},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\9W3V763Y\\Gupta and Malik - 2015 - Visual Semantic Role Labeling.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\RB6ACL7L\\1505.html:text/html},
}

@article{zhao_compositional_2022,
	title = {Compositional action recognition with multi-view feature fusion},
	volume = {17},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0266259},
	doi = {10.1371/journal.pone.0266259},
	abstract = {Most action recognition tasks now treat the activity as a single event in a video clip. Recently, the benefits of representing activities as a combination of verbs and nouns for action recognition have shown to be effective in improving action understanding, allowing us to capture such representations. However, there is still a lack of research on representational learning using cross-view or cross-modality information. To exploit the complementary information between multiple views, we propose a feature fusion framework, and our framework is divided into two steps: extraction of appearance features and fusion of multi-view features. We validate our approach on two action recognition datasets, IKEA ASM and LEMMA. We demonstrate that multi-view fusion can effectively generalize across appearances and identify previously unseen actions of interacting objects, surpassing current state-of-the-art methods. In particular, on the IKEA ASM dataset, the performance of the multi-view fusion approach improves 18.1\% over the performance of the single-view approach on top-1.},
	language = {en},
	number = {4},
	urldate = {2022-06-07},
	journal = {PLOS ONE},
	author = {Zhao, Zhicheng and Liu, Yingan and Ma, Lei},
	month = apr,
	year = {2022},
	note = {Publisher: Public Library of Science},
	keywords = {Deep learning, \#TOSKIM, Data processing, Data visualization, Face recognition, Human learning, Learning, Linguistic morphology, ActionGenome},
	pages = {e0266259},
	file = {Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\DHRBVW9W\\Zhao et al. - 2022 - Compositional action recognition with multi-view f.pdf:application/pdf;Snapshot:C\:\\Users\\felix\\Zotero\\storage\\889WX457\\article.html:text/html},
}

@techreport{zhuo_explainable_2019,
	title = {Explainable {Video} {Action} {Reasoning} via {Prior} {Knowledge} and {State} {Transitions}},
	url = {http://arxiv.org/abs/1908.10700},
	abstract = {Human action analysis and understanding in videos is an important and challenging task. Although substantial progress has been made in past years, the explainability of existing methods is still limited. In this work, we propose a novel action reasoning framework that uses prior knowledge to explain semantic-level observations of video state changes. Our method takes advantage of both classical reasoning and modern deep learning approaches. Specifically, prior knowledge is defined as the information of a target video domain, including a set of objects, attributes and relationships in the target video domain, as well as relevant actions defined by the temporal attribute and relationship changes (i.e. state transitions). Given a video sequence, we first generate a scene graph on each frame to represent concerned objects, attributes and relationships. Then those scene graphs are linked by tracking objects across frames to form a spatio-temporal graph (also called video graph), which represents semantic-level video states. Finally, by sequentially examining each state transition in the video graph, our method can detect and explain how those actions are executed with prior knowledge, just like the logical manner of thinking by humans. Compared to previous works, the action reasoning results of our method can be explained by both logical rules and semantic-level observations of video content changes. Besides, the proposed method can be used to detect multiple concurrent actions with detailed information, such as who (particular objects), when (time), where (object locations) and how (what kind of changes). Experiments on a re-annotated dataset CAD-120 show the effectiveness of our method.},
	number = {arXiv:1908.10700},
	urldate = {2022-06-07},
	institution = {arXiv},
	author = {Zhuo, Tao and Cheng, Zhiyong and Zhang, Peng and Wong, Yongkang and Kankanhalli, Mohan},
	month = aug,
	year = {2019},
	doi = {10.48550/arXiv.1908.10700},
	note = {arXiv:1908.10700 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Reasoning, \#SKIMMED, Scene Graphs},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\MFTZFVWK\\Zhuo et al. - 2019 - Explainable Video Action Reasoning via Prior Knowl.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\DEEB3GF7\\1908.html:text/html},
}

@techreport{koppula_learning_2013,
	title = {Learning {Human} {Activities} and {Object} {Affordances} from {RGB}-{D} {Videos}},
	url = {http://arxiv.org/abs/1210.1207},
	abstract = {Understanding human activities and object affordances are two very important skills, especially for personal robots which operate in human environments. In this work, we consider the problem of extracting a descriptive labeling of the sequence of sub-activities being performed by a human, and more importantly, of their interactions with the objects in the form of associated affordances. Given a RGB-D video, we jointly model the human activities and object affordances as a Markov random field where the nodes represent objects and sub-activities, and the edges represent the relationships between object affordances, their relations with sub-activities, and their evolution over time. We formulate the learning problem using a structural support vector machine (SSVM) approach, where labelings over various alternate temporal segmentations are considered as latent variables. We tested our method on a challenging dataset comprising 120 activity videos collected from 4 subjects, and obtained an accuracy of 79.4\% for affordance, 63.4\% for sub-activity and 75.0\% for high-level activity labeling. We then demonstrate the use of such descriptive labeling in performing assistive tasks by a PR2 robot.},
	number = {arXiv:1210.1207},
	urldate = {2022-06-07},
	institution = {arXiv},
	author = {Koppula, Hema Swetha and Gupta, Rudhir and Saxena, Ashutosh},
	month = may,
	year = {2013},
	doi = {10.48550/arXiv.1210.1207},
	note = {arXiv:1210.1207 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Robotics, \#TOSKIM, Dataset, CAD-120},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\Y9ZQHSLX\\Koppula et al. - 2013 - Learning Human Activities and Object Affordances f.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\QSWB4LXH\\1210.html:text/html},
}

@article{jin_complex_nodate,
	title = {Complex {Video} {Action} {Reasoning} via {Learnable} {Markov} {Logic} {Network}},
	abstract = {Profiting from the advance of deep convolutional networks, current state-of-the-art video action recognition models have achieved remarkable progress. Nevertheless, most of existing models suffer from low interpretability of the predicted actions. Inspired by the observation that temporally-configured human-object interactions often serve as a key indicator of many actions, this work crafts an action reasoning framework that performs Markov Logic Network (MLN) based probabilistic logical inference. Crucially, we propose to encode an action by first-order logical rules that correspond to the temporal changes of visual relationships in videos. The main contributions of this work are two-fold: 1) Different from existing black-box models, the proposed model simultaneously implements the localization of temporal boundaries and the recognition of action categories by grounding the logical rules of MLN in videos. The weight associated with each such rule further provides an estimate of confidence. These collectively make our model more explainable and robust. 2) Instead of using hand-crafted logical rules in conventional MLN, we develop a data-driven instantiation of the MLN. In specific, a hybrid learning scheme is proposed. It combines MLN’s weight learning and reinforcement learning, using the former’s results as a self-critic for guiding the latter’s training. Additionally, by treating actions as logical predicates, the proposed framework can also be integrated with deep models for further performance boost. Comprehensive experiments on two complex video action datasets (Charades \& CAD-120) clearly demonstrate the effectiveness and explainability of our proposed method.},
	language = {en},
	author = {Jin, Yang and Zhu, Linchao and Mu, Yadong},
	keywords = {\#SKIMMED, ActionGenome, CAD-120},
	pages = {10},
	file = {Jin et al. - Complex Video Action Reasoning via Learnable Marko.pdf:C\:\\Users\\felix\\Zotero\\storage\\PHP76PYG\\Jin et al. - Complex Video Action Reasoning via Learnable Marko.pdf:application/pdf},
}

@inproceedings{schoeffmann_cataract-101_2018,
	address = {Amsterdam Netherlands},
	title = {Cataract-101: video dataset of 101 cataract surgeries},
	isbn = {978-1-4503-5192-8},
	shorttitle = {Cataract-101},
	url = {https://dl.acm.org/doi/10.1145/3204949.3208137},
	doi = {10.1145/3204949.3208137},
	abstract = {Cataract surgery is one of the most frequently performed microscopic surgeries in the field of ophthalmology. The goal behind this kind of surgery is to replace the human eye lense with an artificial one, an intervention that is often required due to aging. The entire surgery is performed under microscopy, but co-mounted cameras allow to record and archive the procedure. Currently, the recorded videos are used in a postoperative manner for documentation and training. An additional benefit of recording cataract videos is that they enable video analytics (i.e., manual and/or automatic video content analysis) to investigate medically relevant research questions (e.g., the cause of complications). This, however, necessitates a medical multimedia information system trained and evaluated on existing data, which is currently not publicly available. In this work we provide a public video dataset of 101 cataract surgeries that were performed by four different surgeons over a period of 9 months. These surgeons are grouped into moderately experienced and highly experienced surgeons (assistant vs. senior physicians), providing the basis for experience-based video analytics. All videos have been annotated with quasi-standardized operation phases by a senior ophthalmic surgeon.},
	language = {en},
	urldate = {2022-06-08},
	booktitle = {Proceedings of the 9th {ACM} {Multimedia} {Systems} {Conference}},
	publisher = {ACM},
	author = {Schoeffmann, Klaus and Taschwer, Mario and Sarny, Stephanie and Münzer, Bernd and Primus, Manfred Jürgen and Putzgruber, Doris},
	month = jun,
	year = {2018},
	keywords = {Dataset, \#NONE, CataractXXX},
	pages = {421--425},
	file = {Schoeffmann et al. - 2018 - Cataract-101 video dataset of 101 cataract surger.pdf:C\:\\Users\\felix\\Zotero\\storage\\VPPUWVFH\\Schoeffmann et al. - 2018 - Cataract-101 video dataset of 101 cataract surger.pdf:application/pdf},
}

@article{yue_simulated_2022,
	title = {Simulated {Dataset} for {Spatio}-temporal {Scene} {Graph} {Generation}},
	volume = {2022},
	doi = {10.11517/jsaisigtwo.2022.SWO-056_12},
	abstract = {This paper proposes a novel spatio-temporal scene graph dataset. Spatio-temporal scene graph generation is an essential task in household activity recognition that aims to identify human-object interactions. Constructing a dataset with per-frame object region and consistent relationship annotations requires extremely high labor costs. Existing datasets sparsely annotate frames sampled from videos, resulting in the lack of dense spatio-temporal correlation in videos. Additionally, existing datasets contain inconsistent relationship annotations, leading to the problem of learning ambiguous temporal associations. Moreover, existing datasets mainly discuss relationships that can be inferred from a single frame, ignoring the significance of temporal associations. To resolve those issues, we created a simulated dataset with per-frame consistent annotations and introduced a range of relationships requiring both spatial and temporal context.},
	number = {SWO-056},
	journal = {人工知能学会第二種研究会資料},
	author = {Yue, Qiu and Hara, Kensho and Kataoka, Hirokatsu and Iwata, Kenji and Satoh, Yutaka},
	year = {2022},
	keywords = {\#TOSKIM, Dataset, SGG, Scene Graphs},
	pages = {12},
	file = {Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\GSUJE5H5\\Yue et al. - 2022 - Simulated Dataset for Spatio-temporal Scene Graph .pdf:application/pdf;J-Stage - Snapshot:C\:\\Users\\felix\\Zotero\\storage\\MUXFZ7A6\\ja.html:text/html},
}

@techreport{shit_relationformer_2022,
	title = {Relationformer: {A} {Unified} {Framework} for {Image}-to-{Graph} {Generation}},
	shorttitle = {Relationformer},
	url = {http://arxiv.org/abs/2203.10202},
	abstract = {A comprehensive representation of an image requires understanding objects and their mutual relationship, especially in image-to-graph generation, e.g., road network extraction, blood-vessel network extraction, or scene graph generation. Traditionally, image-to-graph generation is addressed with a two-stage approach consisting of object detection followed by a separate relation prediction, which prevents simultaneous object-relation interaction. This work proposes a unified one-stage transformer-based framework, namely Relationformer, that jointly predicts objects and their relations. We leverage direct set-based object prediction and incorporate the interaction among the objects to learn an object-relation representation jointly. In addition to existing [obj]-tokens, we propose a novel learnable token, namely [rln]-token. Together with [obj]-tokens, [rln]-token exploits local and global semantic reasoning in an image through a series of mutual associations. In combination with the pair-wise [obj]-token, the [rln]-token contributes to a computationally efficient relation prediction. We achieve state-of-the-art performance on multiple, diverse and multi-domain datasets that demonstrate our approach's effectiveness and generalizability.},
	number = {arXiv:2203.10202},
	urldate = {2022-06-08},
	institution = {arXiv},
	author = {Shit, Suprosanna and Koner, Rajat and Wittmann, Bastian and Paetzold, Johannes and Ezhov, Ivan and Li, Hongwei and Pan, Jiazhen and Sharifzadeh, Sahand and Kaissis, Georgios and Tresp, Volker and Menze, Bjoern},
	month = mar,
	year = {2022},
	doi = {10.48550/arXiv.2203.10202},
	note = {arXiv:2203.10202 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Transformers, \#TOREAD, SGG, Visual Genome},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\ILCKRCK9\\Shit et al. - 2022 - Relationformer A Unified Framework for Image-to-G.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\ZT3SWTJK\\2203.html:text/html},
}

@techreport{zhu_scene_2022,
	title = {Scene {Graph} {Generation}: {A} {Comprehensive} {Survey}},
	shorttitle = {Scene {Graph} {Generation}},
	url = {http://arxiv.org/abs/2201.00443},
	abstract = {Deep learning techniques have led to remarkable breakthroughs in the field of generic object detection and have spawned a lot of scene-understanding tasks in recent years. Scene graph has been the focus of research because of its powerful semantic representation and applications to scene understanding. Scene Graph Generation (SGG) refers to the task of automatically mapping an image into a semantic structural scene graph, which requires the correct labeling of detected objects and their relationships. Although this is a challenging task, the community has proposed a lot of SGG approaches and achieved good results. In this paper, we provide a comprehensive survey of recent achievements in this field brought about by deep learning techniques. We review 138 representative works that cover different input modalities, and systematically summarize existing methods of image-based SGG from the perspective of feature extraction and fusion. We attempt to connect and systematize the existing visual relationship detection methods, to summarize, and interpret the mechanisms and the strategies of SGG in a comprehensive way. Finally, we finish this survey with deep discussions about current existing problems and future research directions. This survey will help readers to develop a better understanding of the current research status and ideas.},
	number = {arXiv:2201.00443},
	urldate = {2022-06-08},
	institution = {arXiv},
	author = {Zhu, Guangming and Zhang, Liang and Jiang, Youliang and Dang, Yixuan and Hou, Haoran and Shen, Peiyi and Feng, Mingtao and Zhao, Xia and Miao, Qiguang and Shah, Syed Afaq Ali and Bennamoun, Mohammed},
	month = jan,
	year = {2022},
	doi = {10.48550/arXiv.2201.00443},
	note = {arXiv:2201.00443 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#TOSKIM, SGG},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\UKY8PRL6\\Zhu et al. - 2022 - Scene Graph Generation A Comprehensive Survey.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\QFMVFJFC\\2201.html:text/html},
}

@incollection{wang_memory-based_2020,
	address = {New York, NY, USA},
	title = {Memory-{Based} {Network} for {Scene} {Graph} with {Unbalanced} {Relations}},
	isbn = {978-1-4503-7988-5},
	url = {https://doi.org/10.1145/3394171.3413507},
	abstract = {The scene graph which can be represented by a set of visual triples is composed of objects and the relations between object pairs. It is vital for image captioning, visual question answering, and many other applications. However, there is a long tail distribution on the scene graph dataset, and the tail relation cannot be accurately identified due to the lack of training samples. The problem of the nonstandard label and feature overlap on the scene graph affects the extraction of discriminative features and exacerbates the effect of data imbalance on the model. For these reasons, we propose a novel scene graph generation model that can effectively improve the detection of low-frequency relations. We use the method of memory features to realize the transfer of high-frequency relation features to low-frequency relation features. Extensive experiments on scene graph datasets show that our model significantly improved the performance of two evaluation metrics R@K and mR@K compared with state-of-the-art baselines.},
	urldate = {2022-06-08},
	booktitle = {Proceedings of the 28th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Weitao and Liu, Ruyang and Wang, Meng and Wang, Sen and Chang, Xiaojun and Chen, Yang},
	year = {2020},
	keywords = {\#TOSKIM, SGG, Scene Graphs, memory feature, Memory, Neural Networks},
	pages = {2400--2408},
	file = {Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\9CDFEVEH\\Wang et al. - 2020 - Memory-Based Network for Scene Graph with Unbalanc.pdf:application/pdf},
}

@misc{khan_spatiotemporal_2021,
	title = {Spatiotemporal {Deformable} {Models} for {Long}-{Term} {Complex} {Activity} {Detection}},
	url = {http://arxiv.org/abs/2104.08194},
	abstract = {Long-term complex activity recognition and localisation can be crucial for the decision-making process of several autonomous systems, such as smart cars and surgical robots. Nonetheless, most current methods are designed to merely localise short-term action/activities or combinations of atomic actions that only last for a few frames or seconds. In this paper, we address the problem of longterm complex activity detection via a novel deformable, spatiotemporal parts-based model. Our framework consists of three main building blocks: (i) action tube detection, (ii) the modelling of the deformable geometry of parts, and (iii) a sparsity mechanism. Firstly, action tubes are detected in a series of snippets using an action tube detector. Next, a new 3D deformable RoI pooling layer is designed for learning the ﬂexible, deformable geometry of the constellation of parts. Finally, a sparsity strategy differentiates between activated and deactivate features. We also provide temporal complex activity annotation for the recently released ROAD autonomous driving dataset and the SARAS-ESAD surgical action dataset, to validate our method and show the adaptability of our framework to different domains. As they both contain long videos portraying long-term activities they can be used as benchmarks for future work in this area.},
	language = {en},
	urldate = {2022-06-08},
	publisher = {arXiv},
	author = {Khan, Salman and Cuzzolin, Fabio},
	month = apr,
	year = {2021},
	note = {Number: arXiv:2104.08194
arXiv:2104.08194 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, \#TOSKIM, SARAS-ESAD},
	file = {Khan and Cuzzolin - 2021 - Spatiotemporal Deformable Models for Long-Term Com.pdf:C\:\\Users\\felix\\Zotero\\storage\\PFZAJTPJ\\Khan and Cuzzolin - 2021 - Spatiotemporal Deformable Models for Long-Term Com.pdf:application/pdf},
}

@article{sonogashira_towards_2022,
	title = {Towards {Open}-{Set} {Scene} {Graph} {Generation} {With} {Unknown} {Objects}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3145465},
	abstract = {Scene graph generation (SGG) aims to detect objects and their relationships in an image, thereby enabling a detailed understanding of a complex scene for various real-world applications. In SGG applications such as robot vision, it is important to correctly detect all objects without recognizing any object as another kind of object or ignoring it. However, previous studies on SGG do not consider unknown objects whose classes are unseen in training. Consequently, current SGG methods wrongly classify them as known object classes or overlook them. In this paper, we propose a new problem named “open-set SGG” with unknown objects, focusing on detecting even unknown objects and their relationships. Specifically, we formally define this new problem and propose an evaluation protocol, including an extended dataset with unknown objects and novel evaluation metrics designed for the open-set setting. We also build baseline methods by employing and extending existing SGG methods and compare them through experiments to establish the current baseline performance of open-set SGG. Finally, we discuss the limitations of the current SGG methodology in the open-set setting and point out future research directions.},
	journal = {IEEE Access},
	author = {Sonogashira, Motoharu and Iiyama, Masaaki and Kawanishi, Yasutomo},
	year = {2022},
	note = {Conference Name: IEEE Access},
	keywords = {\#NONE, SGG, Training, Training data, Image recognition, open-set, Predictive models, Protocols, Robots, Object Detection},
	pages = {11574--11583},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\felix\\Zotero\\storage\\V28PWBYK\\9690166.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\AIX6VVRX\\Sonogashira et al. - 2022 - Towards Open-Set Scene Graph Generation With Unkno.pdf:application/pdf},
}

@techreport{cherian_251d_2022,
	title = {(2.5+1){D} {Spatio}-{Temporal} {Scene} {Graphs} for {Video} {Question} {Answering}},
	url = {http://arxiv.org/abs/2202.09277},
	abstract = {Spatio-temporal scene-graph approaches to video-based reasoning tasks, such as video question-answering (QA), typically construct such graphs for every video frame. These approaches often ignore the fact that videos are essentially sequences of 2D "views" of events happening in a 3D space, and that the semantics of the 3D scene can thus be carried over from frame to frame. Leveraging this insight, we propose a (2.5+1)D scene graph representation to better capture the spatio-temporal information flows inside the videos. Specifically, we first create a 2.5D (pseudo-3D) scene graph by transforming every 2D frame to have an inferred 3D structure using an off-the-shelf 2D-to-3D transformation module, following which we register the video frames into a shared (2.5+1)D spatio-temporal space and ground each 2D scene graph within it. Such a (2.5+1)D graph is then segregated into a static sub-graph and a dynamic sub-graph, corresponding to whether the objects within them usually move in the world. The nodes in the dynamic graph are enriched with motion features capturing their interactions with other graph nodes. Next, for the video QA task, we present a novel transformer-based reasoning pipeline that embeds the (2.5+1)D graph into a spatio-temporal hierarchical latent space, where the sub-graphs and their interactions are captured at varied granularity. To demonstrate the effectiveness of our approach, we present experiments on the NExT-QA and AVSD-QA datasets. Our results show that our proposed (2.5+1)D representation leads to faster training and inference, while our hierarchical model showcases superior performance on the video QA task versus the state of the art.},
	number = {arXiv:2202.09277},
	urldate = {2022-06-08},
	institution = {arXiv},
	author = {Cherian, Anoop and Hori, Chiori and Marks, Tim K. and Roux, Jonathan Le},
	month = mar,
	year = {2022},
	doi = {10.48550/arXiv.2202.09277},
	note = {arXiv:2202.09277 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, \#SKIMMED, Scene Graphs},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\GAVQLUM5\\Cherian et al. - 2022 - (2.5+1)D Spatio-Temporal Scene Graphs for Video Qu.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\SWXTQ9HE\\2202.html:text/html},
}

@techreport{arnab_unified_2021,
	title = {Unified {Graph} {Structured} {Models} for {Video} {Understanding}},
	url = {http://arxiv.org/abs/2103.15662},
	abstract = {Accurate video understanding involves reasoning about the relationships between actors, objects and their environment, often over long temporal intervals. In this paper, we propose a message passing graph neural network that explicitly models these spatio-temporal relations and can use explicit representations of objects, when supervision is available, and implicit representations otherwise. Our formulation generalises previous structured models for video understanding, and allows us to study how different design choices in graph structure and representation affect the model's performance. We demonstrate our method on two different tasks requiring relational reasoning in videos -- spatio-temporal action detection on AVA and UCF101-24, and video scene graph classification on the recent Action Genome dataset -- and achieve state-of-the-art results on all three datasets. Furthermore, we show quantitatively and qualitatively how our method is able to more effectively model relationships between relevant entities in the scene.},
	number = {arXiv:2103.15662},
	urldate = {2022-06-08},
	institution = {arXiv},
	author = {Arnab, Anurag and Sun, Chen and Schmid, Cordelia},
	month = mar,
	year = {2021},
	doi = {10.48550/arXiv.2103.15662},
	note = {arXiv:2103.15662 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#TOREAD, ActionGenome},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\LLCIK6Q4\\Arnab et al. - 2021 - Unified Graph Structured Models for Video Understa.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\VDCNF3XJ\\2103.html:text/html},
}

@inproceedings{ji_detecting_2021,
	address = {Montreal, QC, Canada},
	title = {Detecting {Human}-{Object} {Relationships} in {Videos}},
	isbn = {978-1-66542-812-5},
	url = {https://ieeexplore.ieee.org/document/9710314/},
	doi = {10.1109/ICCV48922.2021.00800},
	abstract = {We study a crucial problem in video analysis: humanobject relationship detection. The majority of previous approaches are developed only for the static image scenario, without incorporating the temporal dynamics so vital to contextualizing human-object relationships. We propose a model with Intra- and Inter-Transformers, enabling joint spatial and temporal reasoning on multiple visual concepts of objects, relationships, and human poses. We find that applying attention mechanisms among features distributed spatio-temporally greatly improves our understanding of human-object relationships. Our method is validated on two datasets, Action Genome and CAD-120-EVAR, and achieves state-of-the-art performance on both of them.},
	language = {en},
	urldate = {2022-06-08},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Ji, Jingwei and Desai, Rishi and Niebles, Juan Carlos},
	month = oct,
	year = {2021},
	keywords = {\#TOREAD},
	pages = {8086--8096},
	file = {Ji et al. - 2021 - Detecting Human-Object Relationships in Videos.pdf:C\:\\Users\\felix\\Zotero\\storage\\DIBX6YU3\\Ji et al. - 2021 - Detecting Human-Object Relationships in Videos.pdf:application/pdf},
}

@techreport{li_bipartite_2021,
	title = {Bipartite {Graph} {Network} with {Adaptive} {Message} {Passing} for {Unbiased} {Scene} {Graph} {Generation}},
	url = {http://arxiv.org/abs/2104.00308},
	abstract = {Scene graph generation is an important visual understanding task with a broad range of vision applications. Despite recent tremendous progress, it remains challenging due to the intrinsic long-tailed class distribution and large intra-class variation. To address these issues, we introduce a novel confidence-aware bipartite graph neural network with adaptive message propagation mechanism for unbiased scene graph generation. In addition, we propose an efficient bi-level data resampling strategy to alleviate the imbalanced data distribution problem in training our graph network. Our approach achieves superior or competitive performance over previous methods on several challenging datasets, including Visual Genome, Open Images V4/V6, demonstrating its effectiveness and generality.},
	number = {arXiv:2104.00308},
	urldate = {2022-06-08},
	institution = {arXiv},
	author = {Li, Rongjie and Zhang, Songyang and Wan, Bo and He, Xuming},
	month = apr,
	year = {2021},
	doi = {10.48550/arXiv.2104.00308},
	note = {arXiv:2104.00308 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, \#TOSKIM, SGG, Visual Genome},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\RPCNLFDD\\Li et al. - 2021 - Bipartite Graph Network with Adaptive Message Pass.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\4AZBKA6J\\2104.html:text/html},
}

@techreport{maier-hein_metrics_2022,
	title = {Metrics reloaded: {Pitfalls} and recommendations for image analysis validation},
	shorttitle = {Metrics reloaded},
	url = {http://arxiv.org/abs/2206.01653},
	abstract = {The field of automatic biomedical image analysis crucially depends on robust and meaningful performance metrics for algorithm validation. Current metric usage, however, is often ill-informed and does not reflect the underlying domain interest. Here, we present a comprehensive framework that guides researchers towards choosing performance metrics in a problem-aware manner. Specifically, we focus on biomedical image analysis problems that can be interpreted as a classification task at image, object or pixel level. The framework first compiles domain interest-, target structure-, data set- and algorithm output-related properties of a given problem into a problem fingerprint, while also mapping it to the appropriate problem category, namely image-level classification, semantic segmentation, instance segmentation, or object detection. It then guides users through the process of selecting and applying a set of appropriate validation metrics while making them aware of potential pitfalls related to individual choices. In this paper, we describe the current status of the Metrics Reloaded recommendation framework, with the goal of obtaining constructive feedback from the image analysis community. The current version has been developed within an international consortium of more than 60 image analysis experts and will be made openly available as a user-friendly toolkit after community-driven optimization.},
	number = {arXiv:2206.01653},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Maier-Hein, Lena and Reinke, Annika and Christodoulou, Evangelia and Glocker, Ben and Godau, Patrick and Isensee, Fabian and Kleesiek, Jens and Kozubek, Michal and Reyes, Mauricio and Riegler, Michael A. and Wiesenfarth, Manuel and Baumgartner, Michael and Eisenmann, Matthias and Heckmann-Nötzel, Doreen and Kavur, A. Emre and Rädsch, Tim and Tizabi, Minu D. and Acion, Laura and Antonelli, Michela and Arbel, Tal and Bakas, Spyridon and Bankhead, Peter and Benis, Arriel and Cardoso, M. Jorge and Cheplygina, Veronika and Cimini, Beth and Collins, Gary S. and Farahani, Keyvan and van Ginneken, Bram and Hashimoto, Daniel A. and Hoffman, Michael M. and Huisman, Merel and Jannin, Pierre and Kahn, Charles E. and Karargyris, Alexandros and Karthikesalingam, Alan and Kenngott, Hannes and Kopp-Schneider, Annette and Kreshuk, Anna and Kurc, Tahsin and Landman, Bennett A. and Litjens, Geert and Madani, Amin and Maier-Hein, Klaus and Martel, Anne L. and Mattson, Peter and Meijering, Erik and Menze, Bjoern and Moher, David and Moons, Karel G. M. and Müller, Henning and Nickel, Felix and Nichyporuk, Brennan and Petersen, Jens and Rajpoot, Nasir and Rieke, Nicola and Saez-Rodriguez, Julio and Gutiérrez, Clarisa Sánchez and Shetty, Shravya and van Smeden, Maarten and Sudre, Carole H. and Summers, Ronald M. and Taha, Abdel A. and Tsaftaris, Sotirios A. and Van Calster, Ben and Varoquaux, Gaël and Jäger, Paul F.},
	month = jun,
	year = {2022},
	doi = {10.48550/arXiv.2206.01653},
	note = {arXiv:2206.01653 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#TOSKIM},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\GNZXM6I5\\Maier-Hein et al. - 2022 - Metrics reloaded Pitfalls and recommendations for.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\C73L888V\\2206.html:text/html},
}

@misc{noauthor_yann_nodate,
	title = {Yann {LeCun} on a vision to make {AI} systems learn and reason like animals and humans},
	url = {https://ai.facebook.com/blog/yann-lecun-advances-in-ai-research/},
	abstract = {Meta's Chief AI Scientist Yann LeCun sketches how the ability to learn “world models” – internal models of how the world works – may be the key to building human-level AI.},
	language = {de},
	urldate = {2022-06-13},
	keywords = {\#READ},
	file = {Snapshot:C\:\\Users\\felix\\Zotero\\storage\\SUDF2PWQ\\yann-lecun-advances-in-ai-research.html:text/html},
}

@misc{marcus_does_2022,
	type = {Substack newsletter},
	title = {Does {AI} really need a paradigm shift?},
	url = {https://garymarcus.substack.com/p/does-ai-really-need-a-paradigm-shift},
	abstract = {Probably so. A response to Scott Alexander’s essay, “Somewhat Contra Marcus On AI Scaling”},
	urldate = {2022-06-13},
	journal = {The Road to AI We Can Trust},
	author = {Marcus, Gary},
	month = jun,
	year = {2022},
	keywords = {\#READ},
	file = {Snapshot:C\:\\Users\\felix\\Zotero\\storage\\SVGQR6BE\\does-ai-really-need-a-paradigm-shift.html:text/html},
}

@misc{li_efficientformer_2022,
	title = {{EfficientFormer}: {Vision} {Transformers} at {MobileNet} {Speed}},
	shorttitle = {{EfficientFormer}},
	url = {http://arxiv.org/abs/2206.01191},
	doi = {10.48550/arXiv.2206.01191},
	abstract = {Vision Transformers (ViT) have shown rapid progress in computer vision tasks, achieving promising results on various benchmarks. However, due to the massive number of parameters and model design, e.g., attention mechanism, ViT-based models are generally times slower than lightweight convolutional networks. Therefore, the deployment of ViT for real-time applications is particularly challenging, especially on resource-constrained hardware such as mobile devices. Recent efforts try to reduce the computation complexity of ViT through network architecture search or hybrid design with MobileNet block, yet the inference speed is still unsatisfactory. This leads to an important question: can transformers run as fast as MobileNet while obtaining high performance? To answer this, we first revisit the network architecture and operators used in ViT-based models and identify inefficient designs. Then we introduce a dimension-consistent pure transformer (without MobileNet blocks) as a design paradigm. Finally, we perform latency-driven slimming to get a series of final models dubbed EfficientFormer. Extensive experiments show the superiority of EfficientFormer in performance and speed on mobile devices. Our fastest model, EfficientFormer-L1, achieves \$79.2{\textbackslash}\%\$ top-1 accuracy on ImageNet-1K with only \$1.6\$ ms inference latency on iPhone 12 (compiled with CoreML), which \{ runs as fast as MobileNetV2\${\textbackslash}times 1.4\$ (\$1.6\$ ms, \$74.7{\textbackslash}\%\$ top-1),\} and our largest model, EfficientFormer-L7, obtains \$83.3{\textbackslash}\%\$ accuracy with only \$7.0\$ ms latency. Our work proves that properly designed transformers can reach extremely low latency on mobile devices while maintaining high performance.},
	urldate = {2022-06-15},
	publisher = {arXiv},
	author = {Li, Yanyu and Yuan, Geng and Wen, Yang and Hu, Eric and Evangelidis, Georgios and Tulyakov, Sergey and Wang, Yanzhi and Ren, Jian},
	month = jun,
	year = {2022},
	note = {Number: arXiv:2206.01191
arXiv:2206.01191 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#TOSKIM, Transformers},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\9FVZKUUB\\Li et al. - 2022 - EfficientFormer Vision Transformers at MobileNet .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\USZRHVLN\\2206.html:text/html},
}

@misc{zhang_unveiling_2022,
	title = {Unveiling {Transformers} with {LEGO}: a synthetic reasoning task},
	shorttitle = {Unveiling {Transformers} with {LEGO}},
	url = {http://arxiv.org/abs/2206.04301},
	doi = {10.48550/arXiv.2206.04301},
	abstract = {We propose a synthetic task, LEGO (Learning Equality and Group Operations), that encapsulates the problem of following a chain of reasoning, and we study how the transformer architecture learns this task. We pay special attention to data effects such as pretraining (on seemingly unrelated NLP tasks) and dataset composition (e.g., differing chain length at training and test time), as well as architectural variants such as weight-tied layers or adding convolutional components. We study how the trained models eventually succeed at the task, and in particular, we are able to understand (to some extent) some of the attention heads as well as how the information flows in the network. Based on these observations we propose a hypothesis that here pretraining helps merely due to being a smart initialization rather than some deep knowledge stored in the network. We also observe that in some data regime the trained transformer finds "shortcut" solutions to follow the chain of reasoning, which impedes the model's ability to generalize to simple variants of the main task, and moreover we find that one can prevent such shortcut with appropriate architecture modification or careful data preparation. Motivated by our findings, we begin to explore the task of learning to execute C programs, where a convolutional modification to transformers, namely adding convolutional structures in the key/query/value maps, shows an encouraging edge.},
	urldate = {2022-06-15},
	publisher = {arXiv},
	author = {Zhang, Yi and Backurs, Arturs and Bubeck, Sébastien and Eldan, Ronen and Gunasekar, Suriya and Wagner, Tal},
	month = jun,
	year = {2022},
	note = {Number: arXiv:2206.04301
arXiv:2206.04301 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, \#TOSKIM, Transformers, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\WUS4GZ6W\\Zhang et al. - 2022 - Unveiling Transformers with LEGO a synthetic reas.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\U573MTEY\\2206.html:text/html},
}

@misc{maier-hein_surgical_2021,
	title = {Surgical {Data} {Science} -- from {Concepts} toward {Clinical} {Translation}},
	url = {http://arxiv.org/abs/2011.02284},
	doi = {10.48550/arXiv.2011.02284},
	abstract = {Recent developments in data science in general and machine learning in particular have transformed the way experts envision the future of surgery. Surgical Data Science (SDS) is a new research field that aims to improve the quality of interventional healthcare through the capture, organization, analysis and modeling of data. While an increasing number of data-driven approaches and clinical applications have been studied in the fields of radiological and clinical data science, translational success stories are still lacking in surgery. In this publication, we shed light on the underlying reasons and provide a roadmap for future advances in the field. Based on an international workshop involving leading researchers in the field of SDS, we review current practice, key achievements and initiatives as well as available standards and tools for a number of topics relevant to the field, namely (1) infrastructure for data acquisition, storage and access in the presence of regulatory constraints, (2) data annotation and sharing and (3) data analytics. We further complement this technical perspective with (4) a review of currently available SDS products and the translational progress from academia and (5) a roadmap for faster clinical translation and exploitation of the full potential of SDS, based on an international multi-round Delphi process.},
	urldate = {2022-06-15},
	publisher = {arXiv},
	author = {Maier-Hein, Lena and Eisenmann, Matthias and Sarikaya, Duygu and März, Keno and Collins, Toby and Malpani, Anand and Fallert, Johannes and Feussner, Hubertus and Giannarou, Stamatia and Mascagni, Pietro and Nakawala, Hirenkumar and Park, Adrian and Pugh, Carla and Stoyanov, Danail and Vedula, Swaroop S. and Cleary, Kevin and Fichtinger, Gabor and Forestier, Germain and Gibaud, Bernard and Grantcharov, Teodor and Hashizume, Makoto and Heckmann-Nötzel, Doreen and Kenngott, Hannes G. and Kikinis, Ron and Mündermann, Lars and Navab, Nassir and Onogur, Sinan and Sznitman, Raphael and Taylor, Russell H. and Tizabi, Minu D. and Wagner, Martin and Hager, Gregory D. and Neumuth, Thomas and Padoy, Nicolas and Collins, Justin and Gockel, Ines and Goedeke, Jan and Hashimoto, Daniel A. and Joyeux, Luc and Lam, Kyle and Leff, Daniel R. and Madani, Amin and Marcus, Hani J. and Meireles, Ozanan and Seitel, Alexander and Teber, Dogu and Ückert, Frank and Müller-Stich, Beat P. and Jannin, Pierre and Speidel, Stefanie},
	month = jul,
	year = {2021},
	note = {Number: arXiv:2011.02284
arXiv:2011.02284 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, \#TOSKIM, Review, Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\PCD3E6LR\\Maier-Hein et al. - 2021 - Surgical Data Science -- from Concepts toward Clin.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\XIIAEYFB\\2011.html:text/html},
}

@inproceedings{johnson_image_2015,
	address = {Boston, MA, USA},
	title = {Image retrieval using scene graphs},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7298990/},
	doi = {10.1109/CVPR.2015.7298990},
	abstract = {This paper develops a novel framework for semantic image retrieval based on the notion of a scene graph. Our scene graphs represent objects (“man”, “boat”), attributes of objects (“boat is white”) and relationships between objects (“man standing on boat”). We use these scene graphs as queries to retrieve semantically related images. To this end, we design a conditional random ﬁeld model that reasons about possible groundings of scene graphs to test images. The likelihoods of these groundings are used as ranking scores for retrieval. We introduce a novel dataset of 5,000 human-generated scene graphs grounded to images and use this dataset to evaluate our method for image retrieval. In particular, we evaluate retrieval using full scene graphs and small scene subgraphs, and show that our method outperforms retrieval methods that use only objects or low-level image features. In addition, we show that our full model can be used to improve object localization compared to baseline methods.},
	language = {en},
	urldate = {2022-06-21},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Johnson, Justin and Krishna, Ranjay and Stark, Michael and Li, Li-Jia and Shamma, David A. and Bernstein, Michael S. and Fei-Fei, Li},
	month = jun,
	year = {2015},
	keywords = {\#NONE},
	pages = {3668--3678},
	file = {Johnson et al. - 2015 - Image retrieval using scene graphs.pdf:C\:\\Users\\felix\\Zotero\\storage\\M44AEFBE\\Johnson et al. - 2015 - Image retrieval using scene graphs.pdf:application/pdf},
}

@misc{li_grounded_2022,
	title = {Grounded {Language}-{Image} {Pre}-training},
	url = {http://arxiv.org/abs/2112.03857},
	doi = {10.48550/arXiv.2112.03857},
	abstract = {This paper presents a grounded language-image pre-training (GLIP) model for learning object-level, language-aware, and semantic-rich visual representations. GLIP unifies object detection and phrase grounding for pre-training. The unification brings two benefits: 1) it allows GLIP to learn from both detection and grounding data to improve both tasks and bootstrap a good grounding model; 2) GLIP can leverage massive image-text pairs by generating grounding boxes in a self-training fashion, making the learned representation semantic-rich. In our experiments, we pre-train GLIP on 27M grounding data, including 3M human-annotated and 24M web-crawled image-text pairs. The learned representations demonstrate strong zero-shot and few-shot transferability to various object-level recognition tasks. 1) When directly evaluated on COCO and LVIS (without seeing any images in COCO during pre-training), GLIP achieves 49.8 AP and 26.9 AP, respectively, surpassing many supervised baselines. 2) After fine-tuned on COCO, GLIP achieves 60.8 AP on val and 61.5 AP on test-dev, surpassing prior SoTA. 3) When transferred to 13 downstream object detection tasks, a 1-shot GLIP rivals with a fully-supervised Dynamic Head. Code is released at https://github.com/microsoft/GLIP.},
	urldate = {2022-06-23},
	publisher = {arXiv},
	author = {Li, Liunian Harold and Zhang, Pengchuan and Zhang, Haotian and Yang, Jianwei and Li, Chunyuan and Zhong, Yiwu and Wang, Lijuan and Yuan, Lu and Zhang, Lei and Hwang, Jenq-Neng and Chang, Kai-Wei and Gao, Jianfeng},
	month = jun,
	year = {2022},
	note = {Number: arXiv:2112.03857
arXiv:2112.03857 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, \#NONE, Computer Science - Computation and Language, Computer Science - Multimedia},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\LN5GFEUK\\Li et al. - 2022 - Grounded Language-Image Pre-training.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\LG3ZKEID\\2112.html:text/html},
}

@misc{chen_pix2seq_2022,
	title = {Pix2seq: {A} {Language} {Modeling} {Framework} for {Object} {Detection}},
	shorttitle = {Pix2seq},
	url = {http://arxiv.org/abs/2109.10852},
	doi = {10.48550/arXiv.2109.10852},
	abstract = {We present Pix2Seq, a simple and generic framework for object detection. Unlike existing approaches that explicitly integrate prior knowledge about the task, we cast object detection as a language modeling task conditioned on the observed pixel inputs. Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train a neural network to perceive the image and generate the desired sequence. Our approach is based mainly on the intuition that if a neural network knows about where and what the objects are, we just need to teach it how to read them out. Beyond the use of task-specific data augmentations, our approach makes minimal assumptions about the task, yet it achieves competitive results on the challenging COCO dataset, compared to highly specialized and well optimized detection algorithms.},
	urldate = {2022-06-27},
	publisher = {arXiv},
	author = {Chen, Ting and Saxena, Saurabh and Li, Lala and Fleet, David J. and Hinton, Geoffrey},
	month = mar,
	year = {2022},
	note = {Number: arXiv:2109.10852
arXiv:2109.10852 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, \#TOSKIM, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\Z4YWXYQV\\Chen et al. - 2022 - Pix2seq A Language Modeling Framework for Object .pdf:application/pdf},
}

@misc{nwoye_cholectriplet2021_2022,
	title = {{CholecTriplet2021}: {A} benchmark challenge for surgical action triplet recognition},
	shorttitle = {{CholecTriplet2021}},
	url = {http://arxiv.org/abs/2204.04746},
	doi = {10.48550/arXiv.2204.04746},
	abstract = {Context-aware decision support in the operating room can foster surgical safety and efficiency by leveraging real-time feedback from surgical workflow analysis. Most existing works recognize surgical activities at a coarse-grained level, such as phases, steps or events, leaving out fine-grained interaction details about the surgical activity; yet those are needed for more helpful AI assistance in the operating room. Recognizing surgical actions as triplets of {\textless}instrument, verb, target{\textgreater} combination delivers comprehensive details about the activities taking place in surgical videos. This paper presents CholecTriplet2021: an endoscopic vision challenge organized at MICCAI 2021 for the recognition of surgical action triplets in laparoscopic videos. The challenge granted private access to the large-scale CholecT50 dataset, which is annotated with action triplet information. In this paper, we present the challenge setup and assessment of the state-of-the-art deep learning methods proposed by the participants during the challenge. A total of 4 baseline methods from the challenge organizers and 19 new deep learning algorithms by competing teams are presented to recognize surgical action triplets directly from surgical videos, achieving mean average precision (mAP) ranging from 4.2\% to 38.1\%. This study also analyzes the significance of the results obtained by the presented approaches, performs a thorough methodological comparison between them, in-depth result analysis, and proposes a novel ensemble method for enhanced recognition. Our analysis shows that surgical workflow analysis is not yet solved, and also highlights interesting directions for future research on fine-grained surgical activity recognition which is of utmost importance for the development of AI in surgery.},
	urldate = {2022-07-14},
	publisher = {arXiv},
	author = {Nwoye, Chinedu Innocent and Alapatt, Deepak and Yu, Tong and Vardazaryan, Armine and Xia, Fangfang and Zhao, Zixuan and Xia, Tong and Jia, Fucang and Yang, Yuxuan and Wang, Hao and Yu, Derong and Zheng, Guoyan and Duan, Xiaotian and Getty, Neil and Sanchez-Matilla, Ricardo and Robu, Maria and Zhang, Li and Chen, Huabin and Wang, Jiacheng and Wang, Liansheng and Zhang, Bokai and Gerats, Beerend and Raviteja, Sista and Sathish, Rachana and Tao, Rong and Kondo, Satoshi and Pang, Winnie and Ren, Hongliang and Abbing, Julian Ronald and Sarhan, Mohammad Hasan and Bodenstedt, Sebastian and Bhasker, Nithya and Oliveira, Bruno and Torres, Helena R. and Ling, Li and Gaida, Finn and Czempiel, Tobias and Vilaça, João L. and Morais, Pedro and Fonseca, Jaime and Egging, Ruby Mae and Wijma, Inge Nicole and Qian, Chen and Bian, Guibin and Li, Zhen and Balasubramanian, Velmurugan and Sheet, Debdoot and Luengo, Imanol and Zhu, Yuanbo and Ding, Shuai and Aschenbrenner, Jakob-Anton and van der Kar, Nicolas Elini and Xu, Mengya and Islam, Mobarakol and Seenivasan, Lalithkumar and Jenke, Alexander and Stoyanov, Danail and Mutter, Didier and Mascagni, Pietro and Seeliger, Barbara and Gonzalez, Cristians and Padoy, Nicolas},
	month = apr,
	year = {2022},
	note = {arXiv:2204.04746 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#READ, CholecXXX, Challenge},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\BCXCPGSV\\Nwoye et al. - 2022 - CholecTriplet2021 A benchmark challenge for surgi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\W4IQ8EB7\\2204.html:text/html},
}

@misc{dehghani_efficiency_2022,
	title = {The {Efficiency} {Misnomer}},
	url = {http://arxiv.org/abs/2110.12894},
	abstract = {Model efﬁciency is a critical aspect of developing and deploying machine learning models. Inference time and latency directly affect the user experience, and some applications have hard requirements. In addition to inference costs, model training also have direct ﬁnancial and environmental impacts. Although there are numerous wellestablished metrics (cost indicators) for measuring model efﬁciency, researchers and practitioners often assume that these metrics are correlated with each other and report only few of them. In this paper, we thoroughly discuss common cost indicators, their advantages and disadvantages, and how they can contradict each other. We demonstrate how incomplete reporting of cost indicators can lead to partial conclusions and a blurred or incomplete picture of the practical considerations of different models. We further present suggestions to improve reporting of efﬁciency metrics.},
	language = {en},
	urldate = {2022-07-14},
	publisher = {arXiv},
	author = {Dehghani, Mostafa and Arnab, Anurag and Beyer, Lucas and Vaswani, Ashish and Tay, Yi},
	month = mar,
	year = {2022},
	note = {arXiv:2110.12894 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, \#TOSKIM, Computer Science - Computation and Language},
	file = {Dehghani et al. - 2022 - The Efficiency Misnomer.pdf:C\:\\Users\\felix\\Zotero\\storage\\WT97SH6V\\Dehghani et al. - 2022 - The Efficiency Misnomer.pdf:application/pdf},
}

@misc{chen_pix2seq_2022-1,
	title = {Pix2seq: {A} {Language} {Modeling} {Framework} for {Object} {Detection}},
	shorttitle = {Pix2seq},
	url = {http://arxiv.org/abs/2109.10852},
	doi = {10.48550/arXiv.2109.10852},
	abstract = {We present Pix2Seq, a simple and generic framework for object detection. Unlike existing approaches that explicitly integrate prior knowledge about the task, we cast object detection as a language modeling task conditioned on the observed pixel inputs. Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train a neural network to perceive the image and generate the desired sequence. Our approach is based mainly on the intuition that if a neural network knows about where and what the objects are, we just need to teach it how to read them out. Beyond the use of task-specific data augmentations, our approach makes minimal assumptions about the task, yet it achieves competitive results on the challenging COCO dataset, compared to highly specialized and well optimized detection algorithms.},
	urldate = {2022-07-14},
	publisher = {arXiv},
	author = {Chen, Ting and Saxena, Saurabh and Li, Lala and Fleet, David J. and Hinton, Geoffrey},
	month = mar,
	year = {2022},
	note = {arXiv:2109.10852 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, \#TOSKIM, Computer Science - Computation and Language, Object Detection},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\VPWU5LQ6\\Chen et al. - 2022 - Pix2seq A Language Modeling Framework for Object .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\FWZ9WP9X\\2109.html:text/html},
}

@misc{goodman_real-time_2021,
	title = {A real-time spatiotemporal {AI} model analyzes skill in open surgical videos},
	url = {http://arxiv.org/abs/2112.07219},
	doi = {10.48550/arXiv.2112.07219},
	abstract = {Open procedures represent the dominant form of surgery worldwide. Artificial intelligence (AI) has the potential to optimize surgical practice and improve patient outcomes, but efforts have focused primarily on minimally invasive techniques. Our work overcomes existing data limitations for training AI models by curating, from YouTube, the largest dataset of open surgical videos to date: 1997 videos from 23 surgical procedures uploaded from 50 countries. Using this dataset, we developed a multi-task AI model capable of real-time understanding of surgical behaviors, hands, and tools - the building blocks of procedural flow and surgeon skill. We show that our model generalizes across diverse surgery types and environments. Illustrating this generalizability, we directly applied our YouTube-trained model to analyze open surgeries prospectively collected at an academic medical center and identified kinematic descriptors of surgical skill related to efficiency of hand motion. Our Annotated Videos of Open Surgery (AVOS) dataset and trained model will be made available for further development of surgical AI.},
	urldate = {2022-07-28},
	publisher = {arXiv},
	author = {Goodman, Emmett D. and Patel, Krishna K. and Zhang, Yilun and Locke, William and Kennedy, Chris J. and Mehrotra, Rohan and Ren, Stephen and Guan, Melody Y. and Downing, Maren and Chen, Hao Wei and Clark, Jevin Z. and Brat, Gabriel A. and Yeung, Serena},
	month = dec,
	year = {2021},
	note = {arXiv:2112.07219 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Dataset, \#TOREAD, AVOS},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\X7Z7SIX3\\Goodman et al. - 2021 - A real-time spatiotemporal AI model analyzes skill.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\W3NHFQBH\\2112.html:text/html},
}

@article{lavanchy_automation_2021,
	title = {Automation of surgical skill assessment using a three-stage machine learning algorithm},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-84295-6},
	doi = {10.1038/s41598-021-84295-6},
	abstract = {Surgical skills are associated with clinical outcomes. To improve surgical skills and thereby reduce adverse outcomes, continuous surgical training and feedback is required. Currently, assessment of surgical skills is a manual and time-consuming process which is prone to subjective interpretation. This study aims to automate surgical skill assessment in laparoscopic cholecystectomy videos using machine learning algorithms. To address this, a three-stage machine learning method is proposed: first, a Convolutional Neural Network was trained to identify and localize surgical instruments. Second, motion features were extracted from the detected instrument localizations throughout time. Third, a linear regression model was trained based on the extracted motion features to predict surgical skills. This three-stage modeling approach achieved an accuracy of 87 ± 0.2\% in distinguishing good versus poor surgical skill. While the technique cannot reliably quantify the degree of surgical skill yet it represents an important advance towards automation of surgical skill assessment.},
	language = {en},
	number = {1},
	urldate = {2022-07-28},
	journal = {Scientific Reports},
	author = {Lavanchy, Joël L. and Zindel, Joel and Kirtac, Kadir and Twick, Isabell and Hosgor, Enes and Candinas, Daniel and Beldi, Guido},
	month = mar,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {\#SKIMMED, Biomedical engineering, Translational research, Skill Assessment},
	pages = {5197},
	file = {Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\UIJIEDWR\\Lavanchy et al. - 2021 - Automation of surgical skill assessment using a th.pdf:application/pdf;Snapshot:C\:\\Users\\felix\\Zotero\\storage\\6DJTLE4E\\s41598-021-84295-6.html:text/html},
}

@misc{zia_surgical_2018,
	title = {Surgical {Activity} {Recognition} in {Robot}-{Assisted} {Radical} {Prostatectomy} using {Deep} {Learning}},
	url = {http://arxiv.org/abs/1806.00466},
	doi = {10.48550/arXiv.1806.00466},
	abstract = {Adverse surgical outcomes are costly to patients and hospitals. Approaches to benchmark surgical care are often limited to gross measures across the entire procedure despite the performance of particular tasks being largely responsible for undesirable outcomes. In order to produce metrics from tasks as opposed to the whole procedure, methods to recognize automatically individual surgical tasks are needed. In this paper, we propose several approaches to recognize surgical activities in robot-assisted minimally invasive surgery using deep learning. We collected a clinical dataset of 100 robot-assisted radical prostatectomies (RARP) with 12 tasks each and propose `RP-Net', a modified version of InceptionV3 model, for image based surgical activity recognition. We achieve an average precision of 80.9\% and average recall of 76.7\% across all tasks using RP-Net which out-performs all other RNN and CNN based models explored in this paper. Our results suggest that automatic surgical activity recognition during RARP is feasible and can be the foundation for advanced analytics.},
	urldate = {2022-07-28},
	publisher = {arXiv},
	author = {Zia, Aneeq and Hung, Andrew and Essa, Irfan and Jarc, Anthony},
	month = jun,
	year = {2018},
	note = {arXiv:1806.00466 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#SKIMMED},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\MIMLABDP\\Zia et al. - 2018 - Surgical Activity Recognition in Robot-Assisted Ra.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\EU6CX57Q\\1806.html:text/html},
}

@misc{luongo_deep_2020,
	title = {Deep learning-based computer vision to recognize and classify suturing gestures in robot-assisted surgery},
	url = {http://arxiv.org/abs/2008.11833},
	doi = {10.48550/arXiv.2008.11833},
	abstract = {Our previous work classified a taxonomy of suturing gestures during a vesicourethral anastomosis of robotic radical prostatectomy in association with tissue tears and patient outcomes. Herein, we train deep-learning based computer vision (CV) to automate the identification and classification of suturing gestures for needle driving attempts. Using two independent raters, we manually annotated live suturing video clips to label timepoints and gestures. Identification (2395 videos) and classification (511 videos) datasets were compiled to train CV models to produce two- and five-class label predictions, respectively. Networks were trained on inputs of raw RGB pixels as well as optical flow for each frame. Each model was trained on 80/20 train/test splits. In this study, all models were able to reliably predict either the presence of a gesture (identification, AUC: 0.88) as well as the type of gesture (classification, AUC: 0.87) at significantly above chance levels. For both gesture identification and classification datasets, we observed no effect of recurrent classification model choice (LSTM vs. convLSTM) on performance. Our results demonstrate CV's ability to recognize features that not only can identify the action of suturing but also distinguish between different classifications of suturing gestures. This demonstrates the potential to utilize deep learning CV towards future automation of surgical skill assessment.},
	urldate = {2022-07-28},
	publisher = {arXiv},
	author = {Luongo, Francisco and Hakim, Ryan and Nguyen, Jessica H. and Anandkumar, Animashree and Hung, Andrew J.},
	month = aug,
	year = {2020},
	note = {arXiv:2008.11833 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, \#SKIMMED, JIGSAWS},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\23J6C7XT\\Luongo et al. - 2020 - Deep learning-based computer vision to recognize a.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\E843VLM3\\2008.html:text/html},
}

@misc{liu_interactiveness_2022,
	title = {Interactiveness {Field} in {Human}-{Object} {Interactions}},
	url = {http://arxiv.org/abs/2204.07718},
	doi = {10.48550/arXiv.2204.07718},
	abstract = {Human-Object Interaction (HOI) detection plays a core role in activity understanding. Though recent two/one-stage methods have achieved impressive results, as an essential step, discovering interactive human-object pairs remains challenging. Both one/two-stage methods fail to effectively extract interactive pairs instead of generating redundant negative pairs. In this work, we introduce a previously overlooked interactiveness bimodal prior: given an object in an image, after pairing it with the humans, the generated pairs are either mostly non-interactive, or mostly interactive, with the former more frequent than the latter. Based on this interactiveness bimodal prior we propose the "interactiveness field". To make the learned field compatible with real HOI image considerations, we propose new energy constraints based on the cardinality and difference in the inherent "interactiveness field" underlying interactive versus non-interactive pairs. Consequently, our method can detect more precise pairs and thus significantly boost HOI detection performance, which is validated on widely-used benchmarks where we achieve decent improvements over state-of-the-arts. Our code is available at https://github.com/Foruck/Interactiveness-Field.},
	urldate = {2022-07-28},
	publisher = {arXiv},
	author = {Liu, Xinpeng and Li, Yong-Lu and Wu, Xiaoqian and Tai, Yu-Wing and Lu, Cewu and Tang, Chi-Keung},
	month = apr,
	year = {2022},
	note = {arXiv:2204.07718 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#SKIMMED, HOI, Code available},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\PLBWTJNW\\Liu et al. - 2022 - Interactiveness Field in Human-Object Interactions.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\7WDHJJII\\2204.html:text/html},
}

@article{park_consistency_nodate,
	title = {Consistency {Learning} via {Decoding} {Path} {Augmentation} for {Transformers} in {Human} {Object} {Interaction} {Detection}},
	abstract = {Human-Object Interaction detection is a holistic visual recognition task that entails object detection as well as interaction classification. Previous works of HOI detection has been addressed by the various compositions of subset predictions, e.g., Image → HO → I, Image → HI → O. Recently, transformer based architecture for HOI has emerged, which directly predicts the HOI triplets in an endto-end fashion (Image → HOI). Motivated by various inference paths for HOI detection, we propose cross-path consistency learning (CPC), which is a novel end-to-end learning strategy to improve HOI detection for transformers by leveraging augmented decoding paths. CPC learning enforces all the possible predictions from permuted inference sequences to be consistent. This simple scheme makes the model learn consistent representations, thereby improving generalization without increasing model capacity. Our experiments demonstrate the effectiveness of our method, and we achieved significant improvement on V-COCO and HICO-DET compared to the baseline models. Our code is available at https://github.com/mlvlab/CPChoi.},
	language = {en},
	author = {Park, Jihwan and Lee, SeungJun and Heo, Hwan and Choi, Hyeong Kyu and Kim, Hyunwoo J},
	keywords = {\#SKIMMED, HOI, Code available},
	pages = {10},
	file = {Park et al. - Consistency Learning via Decoding Path Augmentatio.pdf:C\:\\Users\\felix\\Zotero\\storage\\SLBN3FD4\\Park et al. - Consistency Learning via Decoding Path Augmentatio.pdf:application/pdf},
}

@misc{ma_relvit_2022,
	title = {{RelViT}: {Concept}-guided {Vision} {Transformer} for {Visual} {Relational} {Reasoning}},
	shorttitle = {{RelViT}},
	url = {http://arxiv.org/abs/2204.11167},
	doi = {10.48550/arXiv.2204.11167},
	abstract = {Reasoning about visual relationships is central to how humans interpret the visual world. This task remains challenging for current deep learning algorithms since it requires addressing three key technical problems jointly: 1) identifying object entities and their properties, 2) inferring semantic relations between pairs of entities, and 3) generalizing to novel object-relation combinations, i.e., systematic generalization. In this work, we use vision transformers (ViTs) as our base model for visual reasoning and make better use of concepts defined as object entities and their relations to improve the reasoning ability of ViTs. Specifically, we introduce a novel concept-feature dictionary to allow flexible image feature retrieval at training time with concept keys. This dictionary enables two new concept-guided auxiliary tasks: 1) a global task for promoting relational reasoning, and 2) a local task for facilitating semantic object-centric correspondence learning. To examine the systematic generalization of visual reasoning models, we introduce systematic splits for the standard HICO and GQA benchmarks. We show the resulting model, Concept-guided Vision Transformer (or RelViT for short) significantly outperforms prior approaches on HICO and GQA by 16\% and 13\% in the original split, and by 43\% and 18\% in the systematic split. Our ablation analyses also reveal our model's compatibility with multiple ViT variants and robustness to hyper-parameters.},
	urldate = {2022-07-28},
	publisher = {arXiv},
	author = {Ma, Xiaojian and Nie, Weili and Yu, Zhiding and Jiang, Huaizu and Xiao, Chaowei and Zhu, Yuke and Zhu, Song-Chun and Anandkumar, Anima},
	month = jun,
	year = {2022},
	note = {arXiv:2204.11167 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, \#SKIMMED, HOI, Code available},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\XUN8MVHZ\\Ma et al. - 2022 - RelViT Concept-guided Vision Transformer for Visu.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\AYTMXGX3\\2204.html:text/html},
}

@article{wang_learning_nodate,
	title = {Learning {Transferable} {Human}-{Object} {Interaction} {Detector} with {Natural} {Language} {Supervision}},
	abstract = {It is difficult to construct a data collection including all possible combinations of human actions and interacting objects due to the combinatorial nature of human-object interactions (HOI). In this work, we aim to develop a transferable HOI detector for unseen interactions. Existing HOI detectors often treat interactions as discrete labels and learn a classifier according to a predetermined category space. This is inherently inapt for detecting unseen interactions which are out of the predefined categories. Conversely, we treat independent HOI labels as the natural language supervision of interactions and embed them into a joint visualand-text space to capture their correlations. More specifically, we propose a new HOI visual encoder to detect the interacting humans and objects, and map them to a joint feature space to perform interaction recognition. Our visual encoder is instantiated as a Vision Transformer with new learnable HOI tokens and a sequence parser to generate unique HOI predictions. It distills and leverages the transferable knowledge from the pretrained CLIP model to perform the zero-shot interaction detection. Experiments on two datasets, SWIG-HOI and HICO-DET, validate that our proposed method can achieve a notable mAP improvement on detecting both seen and unseen HOIs. Our code is available at https://github.com/scwangdyd/promting\_ hoi.},
	language = {en},
	author = {Wang, Suchen and Duan, Yueqi and Ding, Henghui and Tan, Yap-Peng and Yap, Kim-Hui and Yuan, Junsong},
	keywords = {\#NONE, HOI, Code available},
	pages = {10},
	file = {CVPR2022_4126.pdf:C\:\\Users\\felix\\Zotero\\storage\\X8AA26CE\\CVPR2022_4126.pdf:application/pdf},
}

@article{zhang_exploring_nodate,
	title = {Exploring {Structure}-{Aware} {Transformer} {Over} {Interaction} {Proposals} for {Human}-{Object} {Interaction} {Detection}},
	abstract = {Recent high-performing Human-Object Interaction (HOI) detection techniques have been highly influenced by Transformer-based object detector (i.e., DETR). Nevertheless, most of them directly map parametric interaction queries into a set of HOI predictions through vanilla Transformer in a one-stage manner. This leaves rich interor intra-interaction structure under-exploited. In this work, we design a novel Transformer-style HOI detector, i.e., Structure-aware Transformer over Interaction Proposals (STIP), for HOI detection. Such design decomposes the process of HOI set prediction into two subsequent phases, i.e., an interaction proposal generation is first performed, and then followed by transforming the nonparametric interaction proposals into HOI predictions via a structure-aware Transformer. The structure-aware Transformer upgrades vanilla Transformer by encoding additionally the holistically semantic structure among interaction proposals as well as the locally spatial structure of human/object within each interaction proposal, so as to strengthen HOI predictions. Extensive experiments conducted on V-COCO and HICO-DET benchmarks have demonstrated the effectiveness of STIP, and superior results are reported when comparing with the stateof-the-art HOI detectors. Source code is available at https://github.com/zyong812/STIP.},
	language = {en},
	author = {Zhang, Yong and Pan, Yingwei and Yao, Ting and Huang, Rui and Mei, Tao and Chen, Chang-Wen},
	keywords = {Transformers, \#SKIMMED, HOI, Code available},
	pages = {10},
	file = {Zhang et al. - Exploring Structure-Aware Transformer Over Interac.pdf:C\:\\Users\\felix\\Zotero\\storage\\55CBWXKU\\Zhang et al. - Exploring Structure-Aware Transformer Over Interac.pdf:application/pdf},
}

@misc{zhang_fine-grained_2022,
	title = {Fine-{Grained} {Scene} {Graph} {Generation} with {Data} {Transfer}},
	url = {http://arxiv.org/abs/2203.11654},
	doi = {10.48550/arXiv.2203.11654},
	abstract = {Scene graph generation (SGG) is designed to extract (subject, predicate, object) triplets in images. Recent works have made a steady progress on SGG, and provide useful tools for high-level vision and language understanding. However, due to the data distribution problems including long-tail distribution and semantic ambiguity, the predictions of current SGG models tend to collapse to several frequent but uninformative predicates (e.g., on, at), which limits practical application of these models in downstream tasks. To deal with the problems above, we propose a novel Internal and External Data Transfer (IETrans) method, which can be applied in a plug-and-play fashion and expanded to large SGG with 1,807 predicate classes. Our IETrans tries to relieve the data distribution problem by automatically creating an enhanced dataset that provides more sufficient and coherent annotations for all predicates. By training on the enhanced dataset, a Neural Motif model doubles the macro performance while maintaining competitive micro performance. The code and data are publicly available at https://github.com/waxnkw/IETrans-SGG.pytorch.},
	urldate = {2022-07-28},
	publisher = {arXiv},
	author = {Zhang, Ao and Yao, Yuan and Chen, Qianyu and Ji, Wei and Liu, Zhiyuan and Sun, Maosong and Chua, Tat-Seng},
	month = mar,
	year = {2022},
	note = {arXiv:2203.11654 [cs]
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, \#NONE, SGG, Scene Graphs, Code available},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\LW7H39WQ\\Zhang et al. - 2022 - Fine-Grained Scene Graph Generation with Data Tran.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\TP237MRB\\2203.html:text/html},
}

@misc{chang_biasing_2022,
	title = {Biasing {Like} {Human}: {A} {Cognitive} {Bias} {Framework} for {Scene} {Graph} {Generation}},
	shorttitle = {Biasing {Like} {Human}},
	url = {http://arxiv.org/abs/2203.09160},
	doi = {10.48550/arXiv.2203.09160},
	abstract = {Scene graph generation is a sophisticated task because there is no specific recognition pattern (e.g., "looking at" and "near" have no conspicuous difference concerning vision, whereas "near" could occur between entities with different morphology). Thus some scene graph generation methods are trapped into most frequent relation predictions caused by capricious visual features and trivial dataset annotations. Therefore, recent works emphasized the "unbiased" approaches to balance predictions for a more informative scene graph. However, human's quick and accurate judgments over relations between numerous objects should be attributed to "bias" (i.e., experience and linguistic knowledge) rather than pure vision. To enhance the model capability, inspired by the "cognitive bias" mechanism, we propose a novel 3-paradigms framework that simulates how humans incorporate the label linguistic features as guidance of vision-based representations to better mine hidden relation patterns and alleviate noisy visual propagation. Our framework is model-agnostic to any scene graph model. Comprehensive experiments prove our framework outperforms baseline modules in several metrics with minimum parameters increment and achieves new SOTA performance on Visual Genome dataset.},
	urldate = {2022-07-28},
	publisher = {arXiv},
	author = {Chang, Xiaoguang and Wang, Teng and Sun, Changyin and Cai, Wenzhe},
	month = mar,
	year = {2022},
	note = {arXiv:2203.09160 [cs]
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#NONE, SGG, Scene Graphs, Visual Genome},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\J34EL9I9\\Chang et al. - 2022 - Biasing Like Human A Cognitive Bias Framework for.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\ALJQ5594\\2203.html:text/html},
}

@misc{li_unimo-2_2022,
	title = {{UNIMO}-2: {End}-to-{End} {Unified} {Vision}-{Language} {Grounded} {Learning}},
	shorttitle = {{UNIMO}-2},
	url = {http://arxiv.org/abs/2203.09067},
	doi = {10.48550/arXiv.2203.09067},
	abstract = {Vision-Language Pre-training (VLP) has achieved impressive performance on various cross-modal downstream tasks. However, most existing methods can only learn from aligned image-caption data and rely heavily on expensive regional features, which greatly limits their scalability and performance. In this paper, we propose an end-to-end unified-modal pre-training framework, namely UNIMO-2, for joint learning on both aligned image-caption data and unaligned image-only and text-only corpus. We build a unified Transformer model to jointly learn visual representations, textual representations and semantic alignment between images and texts. In particular, we propose to conduct grounded learning on both images and texts via a sharing grounded space, which helps bridge unaligned images and texts, and align the visual and textual semantic spaces on different types of corpora. The experiments show that our grounded learning method can improve textual and visual semantic alignment for improving performance on various cross-modal tasks. Moreover, benefiting from effective joint modeling of different types of corpora, our model also achieves impressive performance on single-modal visual and textual tasks. Our code and models are public at the UNIMO project page https://unimo-ptm.github.io/.},
	urldate = {2022-07-28},
	publisher = {arXiv},
	author = {Li, Wei and Gao, Can and Niu, Guocheng and Xiao, Xinyan and Liu, Hao and Liu, Jiachen and Wu, Hua and Wang, Haifeng},
	month = mar,
	year = {2022},
	note = {arXiv:2203.09067 [cs]
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#NONE, Computer Science - Computation and Language, Code Available, Grounded Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\WM3SBTMG\\Li et al. - 2022 - UNIMO-2 End-to-End Unified Vision-Language Ground.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\LIUPBYYC\\2203.html:text/html},
}

@misc{li_adapting_2022,
	title = {Adapting {CLIP} {For} {Phrase} {Localization} {Without} {Further} {Training}},
	url = {http://arxiv.org/abs/2204.03647},
	doi = {10.48550/arXiv.2204.03647},
	abstract = {Supervised or weakly supervised methods for phrase localization (textual grounding) either rely on human annotations or some other supervised models, e.g., object detectors. Obtaining these annotations is labor-intensive and may be difficult to scale in practice. We propose to leverage recent advances in contrastive language-vision models, CLIP, pre-trained on image and caption pairs collected from the internet. In its original form, CLIP only outputs an image-level embedding without any spatial resolution. We adapt CLIP to generate high-resolution spatial feature maps. Importantly, we can extract feature maps from both ViT and ResNet CLIP model while maintaining the semantic properties of an image embedding. This provides a natural framework for phrase localization. Our method for phrase localization requires no human annotations or additional training. Extensive experiments show that our method outperforms existing no-training methods in zero-shot phrase localization, and in some cases, it even outperforms supervised methods. Code is available at https://github.com/pals-ttic/adapting-CLIP .},
	urldate = {2022-07-28},
	publisher = {arXiv},
	author = {Li, Jiahao and Shakhnarovich, Greg and Yeh, Raymond A.},
	month = apr,
	year = {2022},
	note = {arXiv:2204.03647 [cs]
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, \#NONE, Code available, Grounding},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\FB6KIJE3\\Li et al. - 2022 - Adapting CLIP For Phrase Localization Without Furt.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\K6AGRBJH\\2204.html:text/html},
}

@inproceedings{yang_tubedetr_2022,
	title = {{TubeDETR}: {Spatio}-{Temporal} {Video} {Grounding} {With} {Transformers}},
	shorttitle = {{TubeDETR}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Yang_TubeDETR_Spatio-Temporal_Video_Grounding_With_Transformers_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-07-28},
	author = {Yang, Antoine and Miech, Antoine and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
	year = {2022},
	keywords = {\#NONE, Code available, Video, Grounding},
	pages = {16442--16453},
	file = {Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\4KI2T7P6\\Yang et al. - 2022 - TubeDETR Spatio-Temporal Video Grounding With Tra.pdf:application/pdf;Snapshot:C\:\\Users\\felix\\Zotero\\storage\\FHJQDPV7\\Yang_TubeDETR_Spatio-Temporal_Video_Grounding_With_Transformers_CVPR_2022_paper.html:text/html},
}

@misc{yan_multiview_2022,
	title = {Multiview {Transformers} for {Video} {Recognition}},
	url = {http://arxiv.org/abs/2201.04288},
	doi = {10.48550/arXiv.2201.04288},
	abstract = {Video understanding requires reasoning at multiple spatiotemporal resolutions -- from short fine-grained motions to events taking place over longer durations. Although transformer architectures have recently advanced the state-of-the-art, they have not explicitly modelled different spatiotemporal resolutions. To this end, we present Multiview Transformers for Video Recognition (MTV). Our model consists of separate encoders to represent different views of the input video with lateral connections to fuse information across views. We present thorough ablation studies of our model and show that MTV consistently performs better than single-view counterparts in terms of accuracy and computational cost across a range of model sizes. Furthermore, we achieve state-of-the-art results on six standard datasets, and improve even further with large-scale pretraining. Code and checkpoints are available at: https://github.com/google-research/scenic/tree/main/scenic/projects/mtv.},
	urldate = {2022-07-28},
	publisher = {arXiv},
	author = {Yan, Shen and Xiong, Xuehan and Arnab, Anurag and Lu, Zhichao and Zhang, Mi and Sun, Chen and Schmid, Cordelia},
	month = may,
	year = {2022},
	note = {arXiv:2201.04288 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Transformers, \#SKIMMED, Code available, Video},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\RXEKD43Y\\Yan et al. - 2022 - Multiview Transformers for Video Recognition.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\SNJPITFC\\2201.html:text/html},
}

@misc{caron_emerging_2021,
	title = {Emerging {Properties} in {Self}-{Supervised} {Vision} {Transformers}},
	url = {http://arxiv.org/abs/2104.14294},
	abstract = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
	urldate = {2022-07-28},
	publisher = {arXiv},
	author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jégou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
	month = may,
	year = {2021},
	note = {arXiv:2104.14294 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Transformers, \#SKIMMED, SSL},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\IVHCGHPQ\\Caron et al. - 2021 - Emerging Properties in Self-Supervised Vision Tran.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\VXFQCZR2\\2104.html:text/html},
}

@article{yule_surgical_2021,
	title = {Surgical {Sabermetrics}: {Applying} {Athletics} {Data} {Science} to {Enhance} {Operative} {Performance}},
	volume = {2},
	issn = {2691-3593},
	shorttitle = {Surgical {Sabermetrics}},
	url = {https://journals.lww.com/aosopen/fulltext/2021/06000/surgical_sabermetrics__applying_athletics_data.3.aspx},
	doi = {10.1097/AS9.0000000000000054},
	abstract = {An abstract is unavailable.},
	language = {en-US},
	number = {2},
	urldate = {2022-07-28},
	journal = {Annals of Surgery Open},
	author = {Yule, Steven and Janda, Allison and Likosky, Donald S.},
	month = jun,
	year = {2021},
	keywords = {\#TOSKIM, SDS},
	pages = {e054},
	file = {Snapshot:C\:\\Users\\felix\\Zotero\\storage\\36DG8C7X\\surgical_sabermetrics__applying_athletics_data.3.html:text/html},
}

@misc{ramesh_dissecting_2022,
	title = {Dissecting {Self}-{Supervised} {Learning} {Methods} for {Surgical} {Computer} {Vision}},
	url = {http://arxiv.org/abs/2207.00449},
	doi = {10.48550/arXiv.2207.00449},
	abstract = {The field of surgical computer vision has undergone considerable breakthroughs in recent years with the rising popularity of deep neural network-based methods. However, standard fully-supervised approaches for training such models require vast amounts of annotated data, imposing a prohibitively high cost; especially in the clinical domain. Self-Supervised Learning (SSL) methods, which have begun to gain traction in the general computer vision community, represent a potential solution to these annotation costs, allowing to learn useful representations from only unlabeled data. Still, the effectiveness of SSL methods in more complex and impactful domains, such as medicine and surgery, remains limited and unexplored. In this work, we address this critical need by investigating four state-of-the-art SSL methods (MoCo v2, SimCLR, DINO, SwAV) in the context of surgical computer vision. We present an extensive analysis of the performance of these methods on the Cholec80 dataset for two fundamental and popular tasks in surgical context understanding, phase recognition and tool presence detection. We examine their parameterization, then their behavior with respect to training data quantities in semi-supervised settings. Correct transfer of these methods to surgery, as described and conducted in this work, leads to substantial performance gains over generic uses of SSL - up to 7\% on phase recognition and 20\% on tool presence detection - as well as state-of-the-art semi-supervised phase recognition approaches by up to 14\%. The code will be made available at https://github.com/CAMMA-public/SelfSupSurg.},
	urldate = {2022-08-01},
	publisher = {arXiv},
	author = {Ramesh, Sanat and Srivastav, Vinkle and Alapatt, Deepak and Yu, Tong and Murali, Aditya and Sestini, Luca and Nwoye, Chinedu Innocent and Hamoud, Idris and Fleurentin, Antoine and Exarchakis, Georgios and Karargyris, Alexandros and Padoy, Nicolas},
	month = jul,
	year = {2022},
	note = {arXiv:2207.00449 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#SKIMMED, CholecXXX, SSL},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\2JW2JXUL\\Ramesh et al. - 2022 - Dissecting Self-Supervised Learning Methods for Su.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\3SUP9NTY\\2207.html:text/html},
}

@misc{selva_video_2022,
	title = {Video {Transformers}: {A} {Survey}},
	shorttitle = {Video {Transformers}},
	url = {http://arxiv.org/abs/2201.05991},
	doi = {10.48550/arXiv.2201.05991},
	abstract = {Transformer models have shown great success modeling long-range interactions. Nevertheless, they scale quadratically with input length and lack inductive biases. These limitations can be further exacerbated when dealing with the high dimensionality of video. Proper modeling of video, which can span from seconds to hours, requires handling long-range interactions. This makes Transformers a promising tool for solving video related tasks, but some adaptations are required. While there are previous works that study the advances of Transformers for vision tasks, there is none that focus on in-depth analysis of video-specific designs. In this survey we analyse and summarize the main contributions and trends for adapting Transformers to model video data. Specifically, we delve into how videos are embedded and tokenized, finding a very widspread use of large CNN backbones to reduce dimensionality and a predominance of patches and frames as tokens. Furthermore, we study how the Transformer layer has been tweaked to handle longer sequences, generally by reducing the number of tokens in single attention operation. Also, we analyse the self-supervised losses used to train Video Transformers, which to date are mostly constrained to contrastive approaches. Finally, we explore how other modalities are integrated with video and conduct a performance comparison on the most common benchmark for Video Transformers (i.e., action classification), finding them to outperform 3D CNN counterparts with equivalent FLOPs and no significant parameter increase.},
	urldate = {2022-08-01},
	publisher = {arXiv},
	author = {Selva, Javier and Johansen, Anders S. and Escalera, Sergio and Nasrollahi, Kamal and Moeslund, Thomas B. and Clapés, Albert},
	month = jan,
	year = {2022},
	note = {arXiv:2201.05991 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#TOSKIM, Transformers},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\PDGS3LLG\\Selva et al. - 2022 - Video Transformers A Survey.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\W59D87XS\\2201.html:text/html},
}

@article{li_sgtr_nodate,
	title = {{SGTR}: {End}-to-{End} {Scene} {Graph} {Generation} {With} {Transformer}},
	language = {en},
	author = {Li, Rongjie and Zhang, Songyang and He, Xuming},
	keywords = {\#TOSKIM, Transformers, SGG},
	pages = {11},
	file = {Li et al. - SGTR End-to-End Scene Graph Generation With Trans.pdf:C\:\\Users\\felix\\Zotero\\storage\\D6GKJ9H2\\Li et al. - SGTR End-to-End Scene Graph Generation With Trans.pdf:application/pdf},
}

@misc{bello_revisiting_2021,
	title = {Revisiting {ResNets}: {Improved} {Training} and {Scaling} {Strategies}},
	shorttitle = {Revisiting {ResNets}},
	url = {http://arxiv.org/abs/2103.07579},
	doi = {10.48550/arXiv.2103.07579},
	abstract = {Novel computer vision architectures monopolize the spotlight, but the impact of the model architecture is often conflated with simultaneous changes to training methodology and scaling strategies. Our work revisits the canonical ResNet (He et al., 2015) and studies these three aspects in an effort to disentangle them. Perhaps surprisingly, we find that training and scaling strategies may matter more than architectural changes, and further, that the resulting ResNets match recent state-of-the-art models. We show that the best performing scaling strategy depends on the training regime and offer two new scaling strategies: (1) scale model depth in regimes where overfitting can occur (width scaling is preferable otherwise); (2) increase image resolution more slowly than previously recommended (Tan \& Le, 2019). Using improved training and scaling strategies, we design a family of ResNet architectures, ResNet-RS, which are 1.7x - 2.7x faster than EfficientNets on TPUs, while achieving similar accuracies on ImageNet. In a large-scale semi-supervised learning setup, ResNet-RS achieves 86.2\% top-1 ImageNet accuracy, while being 4.7x faster than EfficientNet NoisyStudent. The training techniques improve transfer performance on a suite of downstream tasks (rivaling state-of-the-art self-supervised algorithms) and extend to video classification on Kinetics-400. We recommend practitioners use these simple revised ResNets as baselines for future research.},
	urldate = {2022-08-02},
	publisher = {arXiv},
	author = {Bello, Irwan and Fedus, William and Du, Xianzhi and Cubuk, Ekin D. and Srinivas, Aravind and Lin, Tsung-Yi and Shlens, Jonathon and Zoph, Barret},
	month = mar,
	year = {2021},
	note = {arXiv:2103.07579 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#TOSKIM, Architecture, Training, ResNet},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\HHEKD8G6\\Bello et al. - 2021 - Revisiting ResNets Improved Training and Scaling .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\ZLNESQEY\\2103.html:text/html},
}

@article{reinke_common_2021,
	title = {Common {Limitations} of {Image} {Processing} {Metrics}: {A} {Picture} {Story}},
	shorttitle = {Common {Limitations} of {Image} {Processing} {Metrics}},
	abstract = {This living dynamically document has the purpose to illustrate important limitations of performance metrics commonly applied in the field of image analysis, and focuses on biomedical image analysis problems that can be phrased as image-level classification, semantic segmentation, instance segmentations, or object detection task. While the importance of automatic image analysis is continuously increasing, recent meta-research revealed major flaws with respect to algorithm validation. Performance metrics are particularly key for meaningful, objective, and transparent performance assessment and validation of the used automatic algorithms, but relatively little attention has been given to the practical pitfalls when using specific metrics for a given image analysis task. These are typically related to (1) the disregard of inherent metric properties, such as the behaviour in the presence of class imbalance or small target structures, (2) the disregard of inherent data set properties, such as the non-independence of the test cases, and (3) the disregard of the actual biomedical domain interest that the metrics should reflect. This living dynamically document has the purpose to illustrate important limitations of performance metrics commonly applied in the field of image analysis. In this context, it focuses on biomedical image analysis problems that can be phrased as image-level classification, semantic segmentation, instance segmentation, or object detection task. The current version is based on a Delphi process on metrics conducted by an international consortium of image analysis experts from more than 60 institutions worldwide. ∗The complete list of affiliations and authors’ addresses can be found in the Appendix A. Common Limitations of Image Processing Metrics: A Picture Story 3},
	journal = {ArXiv},
	author = {Reinke, Annika and Eisenmann, M. and Tizabi, M. and Sudre, C. and Radsch, Tim and Antonelli, M. and Arbel, T. and Bakas, S. and Cardoso, M. and Cheplygina, V. and Farahani, K. and Glocker, B. and Heckmann-Notzel, Doreen and Isensee, F. and Jannin, P. and Kahn, C. E. and Kleesiek, J. and Kurç, T. and Kozubek, M. and Landman, B. and Litjens, G. and Maier-Hein, Klaus H. and Menze, B. and Muller, H. and Petersen, Jens and Reyes, M. and Rieke, Nicola and Stieltjes, B. and Summers, R. and Tsaftaris, S. and Ginneken, B. and Kopp-Schneider, A. and Jager, Paul F. and Maier-Hein, L.},
	year = {2021},
	keywords = {\#TOSKIM, Metrics, Training},
	file = {Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\TTGF62A7\\Reinke et al. - 2021 - Common Limitations of Image Processing Metrics A .pdf:application/pdf},
}

@misc{henaff_object_2022,
	title = {Object discovery and representation networks},
	url = {http://arxiv.org/abs/2203.08777},
	doi = {10.48550/arXiv.2203.08777},
	abstract = {The promise of self-supervised learning (SSL) is to leverage large amounts of unlabeled data to solve complex tasks. While there has been excellent progress with simple, image-level learning, recent methods have shown the advantage of including knowledge of image structure. However, by introducing hand-crafted image segmentations to define regions of interest, or specialized augmentation strategies, these methods sacrifice the simplicity and generality that makes SSL so powerful. Instead, we propose a self-supervised learning paradigm that discovers this image structure by itself. Our method, Odin, couples object discovery and representation networks to discover meaningful image segmentations without any supervision. The resulting learning paradigm is simpler, less brittle, and more general, and achieves state-of-the-art transfer learning results for object detection and instance segmentation on COCO, and semantic segmentation on PASCAL and Cityscapes, while strongly surpassing supervised pre-training for video segmentation on DAVIS.},
	urldate = {2022-08-03},
	publisher = {arXiv},
	author = {Hénaff, Olivier J. and Koppula, Skanda and Shelhamer, Evan and Zoran, Daniel and Jaegle, Andrew and Zisserman, Andrew and Carreira, João and Arandjelović, Relja},
	month = jul,
	year = {2022},
	note = {arXiv:2203.08777 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, \#TOSKIM, SSL, \#SSSGG},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\KRPL8Q66\\Hénaff et al. - 2022 - Object discovery and representation networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\HQKCCZ46\\2203.html:text/html},
}

@inproceedings{araslanov_dense_2021,
	title = {Dense {Unsupervised} {Learning} for {Video} {Segmentation}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/d516b13671a4179d9b7b458a6ebdeb92-Abstract.html},
	abstract = {We present a novel approach to unsupervised learning for video object segmentation (VOS). Unlike previous work, our formulation allows to learn dense feature representations directly in a fully convolutional regime. We rely on uniform grid sampling to extract a set of anchors and train our model to disambiguate between them on both inter- and intra-video levels. However, a naive scheme to train such a model results in a degenerate solution. We propose to prevent this with a simple regularisation scheme, accommodating the equivariance property of the segmentation task to similarity transformations. Our training objective admits efficient implementation and exhibits fast training convergence. On established VOS benchmarks, our approach exceeds the segmentation accuracy of previous work despite using significantly less training data and compute power.},
	urldate = {2022-08-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Araslanov, Nikita and Schaub-Meyer, Simone and Roth, Stefan},
	year = {2021},
	keywords = {\#SKIMMED, VOS},
	pages = {25308--25319},
	file = {Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\WRZQFSLE\\Araslanov et al. - 2021 - Dense Unsupervised Learning for Video Segmentation.pdf:application/pdf},
}

@article{araslanov_dense_nodate,
	title = {Dense {Unsupervised} {Learning} for {Video} {Segmentation}},
	abstract = {We present a novel approach to unsupervised learning for video object segmentation (VOS). Unlike previous work, our formulation allows to learn dense feature representations directly in a fully convolutional regime. We rely on uniform grid sampling to extract a set of anchors and train our model to disambiguate between them on both inter- and intra-video levels. However, a naive scheme to train such a model results in a degenerate solution. We propose to prevent this with a simple regularisation scheme, accommodating the equivariance property of the segmentation task to similarity transformations. Our training objective admits efficient implementation and exhibits fast training convergence. On established VOS benchmarks, our approach exceeds the segmentation accuracy of previous work despite using significantly less training data and compute power.},
	language = {en},
	author = {Araslanov, Nikita and Schaub-Meyer, Simone and Roth, Stefan},
	pages = {12},
	file = {Araslanov et al. - Dense Unsupervised Learning for Video Segmentation.pdf:C\:\\Users\\felix\\Zotero\\storage\\9HG6V6FK\\Araslanov et al. - Dense Unsupervised Learning for Video Segmentation.pdf:application/pdf},
}

@article{philipp_annotation-efficient_2022,
	title = {Annotation-efficient learning of surgical instrument activity in neurosurgery},
	volume = {8},
	issn = {2364-5504},
	url = {https://www.degruyter.com/document/doi/10.1515/cdbme-2022-0008/html},
	doi = {10.1515/cdbme-2022-0008},
	abstract = {Machine learning-based solutions rely heavily on the quality and quantity of the training data. In the medical domain, the main challenge is to acquire rich and diverse annotated datasets for training. We propose to decrease the annotation efforts and further diversify the dataset by introducing an annotation-efficient learning workflow. Instead of costly pixel-level annotation, we require only image-level labels as the remainder is covered by simulation. Thus, we obtain a large-scale dataset with realistic images and accurate ground truth annotations. We use this dataset for the instrument localization activity task together with a studentteacher approach. We demonstrate the benefits of our workflow compared to state-of-the-art methods in instrument localization that are trained only on clinical datasets, which are fully annotated by human experts.},
	language = {en},
	number = {1},
	urldate = {2022-08-03},
	journal = {Current Directions in Biomedical Engineering},
	author = {Philipp, Markus and Alperovich, Anna and Lisogorov, Alexander and Gutt-Will, Marielena and Mathis, Andrea and Saur, Stefan and Raabe, Andreas and Mathis-Ullrich, Franziska},
	month = jul,
	year = {2022},
	note = {Publisher: De Gruyter},
	keywords = {\#SKIMMED, Annotation-efficiency learning, instrument localization, medical deep learning, neurosurgery, Surgical Tool Localization},
	pages = {30--33},
	file = {Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\6RXWT5DM\\Philipp et al. - 2022 - Annotation-efficient learning of surgical instrume.pdf:application/pdf},
}

@inproceedings{read_multi-label_2008,
	address = {Pisa, Italy},
	title = {Multi-label {Classification} {Using} {Ensembles} of {Pruned} {Sets}},
	isbn = {978-0-7695-3502-9},
	url = {http://ieeexplore.ieee.org/document/4781214/},
	doi = {10.1109/ICDM.2008.74},
	abstract = {This paper presents a Pruned Sets method (PS) for multilabel classiﬁcation. It is centred on the concept of treating sets of labels as single labels. This allows the classiﬁcation process to inherently take into account correlations between labels. By pruning these sets, PS focuses only on the most important correlations, which reduces complexity and improves accuracy. By combining pruned sets in an ensemble scheme (EPS), new label sets can be formed to adapt to irregular or complex data. The results from experimental evaluation on a variety of multi-label datasets show that [E]PS can achieve better performance and train much faster than other multi-label methods.},
	language = {en},
	urldate = {2022-08-21},
	booktitle = {2008 {Eighth} {IEEE} {International} {Conference} on {Data} {Mining}},
	publisher = {IEEE},
	author = {Read, Jesse and Pfahringer, Bernhard and Holmes, Geoff},
	month = dec,
	year = {2008},
	keywords = {\#SKIMMED},
	pages = {995--1000},
	file = {Read et al. - 2008 - Multi-label Classification Using Ensembles of Prun.pdf:C\:\\Users\\felix\\Zotero\\storage\\WRZJISHU\\Read et al. - 2008 - Multi-label Classification Using Ensembles of Prun.pdf:application/pdf},
}

@article{kurmann_mask_2021,
	title = {Mask then classify: multi-instance segmentation for surgical instruments},
	volume = {16},
	issn = {1861-6410, 1861-6429},
	shorttitle = {Mask then classify},
	url = {https://link.springer.com/10.1007/s11548-021-02404-2},
	doi = {10.1007/s11548-021-02404-2},
	abstract = {Purpose The detection and segmentation of surgical instruments has been a vital step for many applications in minimally invasive surgical robotics. Previously, the problem was tackled from a semantic segmentation perspective, yet these methods fail to provide good segmentation maps of instrument types and do not contain any information on the instance afﬁliation of each pixel. We propose to overcome this limitation by using a novel instance segmentation method which ﬁrst masks instruments and then classiﬁes them into their respective type.
Methods We introduce a novel method for instance segmentation where a pixel-wise mask of each instance is found prior to classiﬁcation. An encoder–decoder network is used to extract instrument instances, which are then separately classiﬁed using the features of the previous stages. Furthermore, we present a method to incorporate instrument priors from surgical robots.
Results Experiments are performed on the robotic instrument segmentation dataset of the 2017 endoscopic vision challenge. We perform a fourfold cross-validation and show an improvement of over 18\% to the previous state-of-the-art. Furthermore, we perform an ablation study which highlights the importance of certain design choices and observe an increase of 10\% over semantic segmentation methods.
Conclusions We have presented a novel instance segmentation method for surgical instruments which outperforms previous semantic segmentation-based methods. Our method further provides a more informative output of instance level information, while retaining a precise segmentation mask. Finally, we have shown that robotic instrument priors can be used to further increase the performance.},
	language = {en},
	number = {7},
	urldate = {2022-08-23},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {Kurmann, Thomas and Márquez-Neila, Pablo and Allan, Max and Wolf, Sebastian and Sznitman, Raphael},
	month = jul,
	year = {2021},
	keywords = {\#SKIMMED},
	pages = {1227--1236},
	file = {s11548-021-02404-2.pdf:C\:\\Users\\felix\\Zotero\\storage\\Q8QETFBQ\\s11548-021-02404-2.pdf:application/pdf},
}

@misc{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	doi = {10.48550/arXiv.1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2022-09-05},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv:1512.03385 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#NONE, Architecture},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\WMPMWC3W\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\M3E7JEFL\\1512.html:text/html},
}

@article{selvaraju_grad-cam_2020,
	title = {Grad-{CAM}: {Visual} {Explanations} from {Deep} {Networks} via {Gradient}-based {Localization}},
	volume = {128},
	issn = {0920-5691, 1573-1405},
	shorttitle = {Grad-{CAM}},
	url = {http://arxiv.org/abs/1610.02391},
	doi = {10.1007/s11263-019-01228-7},
	abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.},
	number = {2},
	urldate = {2022-09-05},
	journal = {International Journal of Computer Vision},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	month = feb,
	year = {2020},
	note = {arXiv:1610.02391 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, \#NONE, Interpretability},
	pages = {336--359},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\MI5I7XGX\\Selvaraju et al. - 2020 - Grad-CAM Visual Explanations from Deep Networks v.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\9Y3IH9GI\\1610.html:text/html},
}

@incollection{vedaldi_rethinking_2020,
	address = {Cham},
	title = {Rethinking {Class} {Activation} {Mapping} for {Weakly} {Supervised} {Object} {Localization}},
	volume = {12360},
	isbn = {978-3-030-58554-9 978-3-030-58555-6},
	url = {https://link.springer.com/10.1007/978-3-030-58555-6_37},
	abstract = {Weakly supervised object localization (WSOL) is a task of localizing an object in an image only using image-level labels. To tackle the WSOL problem, most previous studies have followed the conventional class activation mapping (CAM) pipeline: (i) training CNNs for a classiﬁcation objective, (ii) generating a class activation map via global average pooling (GAP) on feature maps, and (iii) extracting bounding boxes by thresholding based on the maximum value of the class activation map. In this work, we reveal the current CAM approach suﬀers from three fundamental issues: (i) the bias of GAP that assigns a higher weight to a channel with a small activation area, (ii) negatively weighted activations inside the object regions and (iii) instability from the use of the maximum value of a class activation map as a thresholding reference. They collectively cause the problem that the localization to be highly limited to small regions of an object. We propose three simple but robust techniques that alleviate the problems, including thresholded average pooling, negative weight clamping, and percentile as a standard for thresholding. Our solutions are universally applicable to any WSOL methods using CAM and improve their performance drastically. As a result, we achieve the new state-of-the-art performance on three benchmark datasets of CUB-200-2011, ImageNet-1K, and OpenImages30K.},
	language = {en},
	urldate = {2022-09-05},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Bae, Wonho and Noh, Junhyug and Kim, Gunhee},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58555-6_37},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {\#NONE},
	pages = {618--634},
	file = {Bae et al. - 2020 - Rethinking Class Activation Mapping for Weakly Sup.pdf:C\:\\Users\\felix\\Zotero\\storage\\CSAFU5JP\\Bae et al. - 2020 - Rethinking Class Activation Mapping for Weakly Sup.pdf:application/pdf},
}

@InProceedings{ozsoy_4d-or_2022,
author="{\"O}zsoy, Ege
and {\"O}rnek, Evin P{\i}nar
and Eck, Ulrich
and Czempiel, Tobias
and Tombari, Federico
and Navab, Nassir",
editor="Wang, Linwei
and Dou, Qi
and Fletcher, P. Thomas
and Speidel, Stefanie
and Li, Shuo",
title="4D-OR: Semantic Scene Graphs for OR Domain Modeling",
booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="475--485",
abstract="Surgical procedures are conducted in highly complex operating rooms (OR), comprising different actors, devices, and interactions. To date, only medically trained human experts are capable of understanding all the links and interactions in such a demanding environment. This paper aims to bring the community one step closer to automated, holistic and semantic understanding and modeling of OR domain. Towards this goal, for the first time, we propose using semantic scene graphs (SSG) to describe and summarize the surgical scene. The nodes of the scene graphs represent different actors and objects in the room, such as medical staff, patients, and medical equipment, whereas edges are the relationships between them. To validate the possibilities of the proposed representation, we create the first publicly available 4D surgical SSG dataset, 4D-OR, containing ten simulated total knee replacement surgeries recorded with six RGB-D sensors in a realistic OR simulation center. 4D-OR includes 6734 frames and is richly annotated with SSGs, human and object poses, and clinical roles. We propose an end-to-end neural network-based SSG generation pipeline, with a rate of success of 0.75 macro F1, indeed being able to infer semantic reasoning in the OR. We further demonstrate the representation power of our scene graphs by using it for the problem of clinical role prediction, where we achieve 0.85 macro F1. The code and dataset are publicly available at github.com/egeozsoy/4D-OR.",
isbn="978-3-031-16449-1"
}



@misc{venturi_mori-zwanzig_2022,
	title = {The {Mori}-{Zwanzig} formulation of deep learning},
	url = {http://arxiv.org/abs/2209.05544},
	abstract = {We develop a new formulation of deep learning based on the Mori-Zwanzig (MZ) formalism of irreversible statistical mechanics. The new formulation is built upon the well-known duality between deep neural networks and discrete stochastic dynamical systems, and it allows us to directly propagate quantities of interest (conditional expectations and probability density functions) forward and backward through the network by means of exact linear operator equations. Such new equations can be used as a starting point to develop new effective parameterizations of deep neural networks, and provide a new framework to study deep-learning via operator theoretic methods. The proposed MZ formulation of deep learning naturally introduces a new concept, i.e., the memory of the neural network, which plays a fundamental role in low-dimensional modeling and parameterization. By using the theory of contraction mappings, we develop sufﬁcient conditions for the memory of the neural network to decay with the number of layers. This allows us to rigorously transform deep networks into shallow ones, e.g., by reducing the number of neurons per layer (using projection operators), or by reducing the total number of layers (using the decaying property of the memory operator).},
	language = {en},
	urldate = {2022-09-14},
	publisher = {arXiv},
	author = {Venturi, Daniele and Li, Xiantao},
	month = sep,
	year = {2022},
	note = {arXiv:2209.05544 [cond-mat, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, \#NONE, Condensed Matter - Statistical Mechanics, Math},
	file = {Venturi and Li - 2022 - The Mori-Zwanzig formulation of deep learning.pdf:C\:\\Users\\felix\\Zotero\\storage\\5K8TAQ62\\Venturi and Li - 2022 - The Mori-Zwanzig formulation of deep learning.pdf:application/pdf},
}

@article{feldman_sages_2020,
	title = {{SAGES} {Video}-{Based} {Assessment} ({VBA}) program: a vision for life-long learning for surgeons},
	volume = {34},
	issn = {1432-2218},
	shorttitle = {{SAGES} {Video}-{Based} {Assessment} ({VBA}) program},
	url = {https://doi.org/10.1007/s00464-020-07628-y},
	doi = {10.1007/s00464-020-07628-y},
	language = {en},
	number = {8},
	urldate = {2022-10-06},
	journal = {Surgical Endoscopy},
	author = {Feldman, Liane S. and Pryor, Aurora D. and Gardner, Aimee K. and Dunkin, Brian J. and Schultz, Linda and Awad, Michael M. and Ritter, E. Matthew},
	month = aug,
	year = {2020},
	keywords = {\#NONE, Medical, Surgical Skill},
	pages = {3285--3288},
	file = {Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\NQ2ZNKIG\\Feldman et al. - 2020 - SAGES Video-Based Assessment (VBA) program a visi.pdf:application/pdf},
}

@misc{yi_asformer_2021,
	title = {{ASFormer}: {Transformer} for {Action} {Segmentation}},
	shorttitle = {{ASFormer}},
	url = {http://arxiv.org/abs/2110.08568},
	abstract = {Algorithms for the action segmentation task typically use temporal models to predict what action is occurring at each frame for a minute-long daily activity. Recent studies have shown the potential of Transformer in modeling the relations among elements in sequential data. However, there are several major concerns when directly applying the Transformer to the action segmentation task, such as the lack of inductive biases with small training sets, the deficit in processing long input sequence, and the limitation of the decoder architecture to utilize temporal relations among multiple action segments to refine the initial predictions. To address these concerns, we design an efficient Transformer-based model for action segmentation task, named ASFormer, with three distinctive characteristics: (i) We explicitly bring in the local connectivity inductive priors because of the high locality of features. It constrains the hypothesis space within a reliable scope, and is beneficial for the action segmentation task to learn a proper target function with small training sets. (ii) We apply a pre-defined hierarchical representation pattern that efficiently handles long input sequences. (iii) We carefully design the decoder to refine the initial predictions from the encoder. Extensive experiments on three public datasets demonstrate that effectiveness of our methods. Code is available at {\textbackslash}url\{https://github.com/ChinaYi/ASFormer\}.},
	urldate = {2022-10-06},
	publisher = {arXiv},
	author = {Yi, Fangqiu and Wen, Hongyu and Jiang, Tingting},
	month = oct,
	year = {2021},
	note = {arXiv:2110.08568 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#TOSKIM, Action Recognition, Transformer},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\UNABVGPH\\Yi et al. - 2021 - ASFormer Transformer for Action Segmentation.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\V4RAK8TI\\2110.html:text/html},
}

@misc{noauthor_endika_nodate,
	title = {Endika {Bengoetxeas} {PhD} {Thesis} - www.sc.ehu.es/},
	url = {http://www.sc.ehu.es/acwbecae/ikerkuntza/these/},
	urldate = {2022-10-10},
	keywords = {\#NONE},
	file = {Endika Bengoetxeas PhD Thesis - www.sc.ehu.es/:C\:\\Users\\felix\\Zotero\\storage\\VQ8LQGPU\\these.html:text/html},
}

@misc{saleh_bending_2022,
	title = {Bending {Graphs}: {Hierarchical} {Shape} {Matching} using {Gated} {Optimal} {Transport}},
	shorttitle = {Bending {Graphs}},
	url = {http://arxiv.org/abs/2202.01537},
	doi = {10.48550/arXiv.2202.01537},
	abstract = {Shape matching has been a long-studied problem for the computer graphics and vision community. The objective is to predict a dense correspondence between meshes that have a certain degree of deformation. Existing methods either consider the local description of sampled points or discover correspondences based on global shape information. In this work, we investigate a hierarchical learning design, to which we incorporate local patch-level information and global shape-level structures. This flexible representation enables correspondence prediction and provides rich features for the matching stage. Finally, we propose a novel optimal transport solver by recurrently updating features on non-confident nodes to learn globally consistent correspondences between the shapes. Our results on publicly available datasets suggest robust performance in presence of severe deformations without the need for extensive training or refinement.},
	urldate = {2022-10-10},
	publisher = {arXiv},
	author = {Saleh, Mahdi and Wu, Shun-Cheng and Cosmo, Luca and Navab, Nassir and Busam, Benjamin and Tombari, Federico},
	month = feb,
	year = {2022},
	note = {arXiv:2202.01537 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, GDL, \#TOSKIM, Computer Science - Computational Geometry, Computer Science - Graphics},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\HQEYC6J2\\Saleh et al. - 2022 - Bending Graphs Hierarchical Shape Matching using .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\LIRVTXSK\\2202.html:text/html},
}

@misc{feng_exploiting_2022,
	title = {Exploiting {Long}-{Term} {Dependencies} for {Generating} {Dynamic} {Scene} {Graphs}},
	url = {http://arxiv.org/abs/2112.09828},
	doi = {10.48550/arXiv.2112.09828},
	abstract = {Dynamic scene graph generation from a video is challenging due to the temporal dynamics of the scene and the inherent temporal fluctuations of predictions. We hypothesize that capturing long-term temporal dependencies is the key to effective generation of dynamic scene graphs. We propose to learn the long-term dependencies in a video by capturing the object-level consistency and inter-object relationship dynamics over object-level long-term tracklets using transformers. Experimental results demonstrate that our Dynamic Scene Graph Detection Transformer (DSG-DETR) outperforms state-of-the-art methods by a significant margin on the benchmark dataset Action Genome. Our ablation studies validate the effectiveness of each component of the proposed approach. The source code is available at https://github.com/Shengyu-Feng/DSG-DETR.},
	urldate = {2022-10-21},
	publisher = {arXiv},
	author = {Feng, Shengyu and Tripathi, Subarna and Mostafa, Hesham and Nassar, Marcel and Majumdar, Somdeb},
	month = oct,
	year = {2022},
	note = {arXiv:2112.09828 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#SKIMMED, SGG},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\Y96G32FF\\Feng et al. - 2022 - Exploiting Long-Term Dependencies for Generating D.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\M6VVS2NH\\2112.html:text/html},
}

@inproceedings{fox_pixel-based_2020,
	title = {Pixel-{Based} {Tool} {Segmentation} in {Cataract} {Surgery} {Videos} with {Mask} {R}-{CNN}},
	doi = {10.1109/CBMS49503.2020.00112},
	abstract = {Automatically detecting surgical tools in recorded surgery videos is an important building block of further content-based video analysis. In ophthalmology, the results of such methods can support training and teaching of operation techniques and enable investigation of medical research questions on a dataset of recorded surgery videos. While previous methods used frame-based classification techniques to predict the presence of surgical tools - but did not localize them, we apply a recent deep-learning segmentation method (Mask R-CNN) to localize and segment surgical tools used in ophthalmic cataract surgery. We add ground-truth annotations for multi-class instance segmentation to two existing datasets of cataract surgery videos and make resulting datasets publicly available for research purposes. In the absence of comparable results from literature, we tune and evaluate the Mask R-CNN approach on these datasets for instrument segmentation/localization and achieve promising results (61\% mean average precision on 50\% intersection over union for instance segmentation, working even better for bounding box detection or binary segmentation), establishing a reasonable baseline for further research. Moreover, we experiment with common data augmentation techniques and analyze the achieved segmentation performance with respect to each class (instrument), providing evidence for future improvements of this approach.},
	booktitle = {2020 {IEEE} 33rd {International} {Symposium} on {Computer}-{Based} {Medical} {Systems} ({CBMS})},
	author = {Fox, Markus and Taschwer, Mario and Schoeffmann, Klaus},
	month = jul,
	year = {2020},
	note = {ISSN: 2372-9198},
	keywords = {\#NONE, Cataract, Visualization, cataract surgeries, Cataracts, deep neural networks, Image segmentation, instrument segmentation, Instruments, ophthalmology, Surgery, tool annotation, Tools, Videos},
	pages = {565--568},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\felix\\Zotero\\storage\\JNUY9PFD\\9183116.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\K7KJBDLA\\Fox et al. - 2020 - Pixel-Based Tool Segmentation in Cataract Surgery .pdf:application/pdf},
}

@inproceedings{sokolova_pixel-based_2020,
	title = {Pixel-{Based} {Iris} and {Pupil} {Segmentation} in {Cataract} {Surgery} {Videos} {Using} {Mask} {R}-{CNN}},
	doi = {10.1109/ISBIWorkshops50223.2020.9153367},
	abstract = {Automatically detecting clinically relevant events in surgery video recordings is becoming increasingly important for documentary, educational, and scientific purposes in the medical domain. From a medical image analysis perspective, such events need to be treated individually and associated with specific visible objects or regions. In the field of cataract surgery (lens replacement in the human eye), pupil reaction (dilation or restriction) during surgery may lead to complications and hence represents a clinically relevant event. Its detection requires automatic segmentation and measurement of pupil and iris in recorded video frames. In this work, we contribute to research on pupil and iris segmentation methods by (1) providing a dataset of 82 annotated images for training and evaluating suitable machine learning algorithms, and (2) applying the Mask R-CNN algorithm to this problem, which—in contrast to existing techniques for pupil segmentation—predicts free-form pixel-accurate segmentation masks for iris and pupil. The proposed approach achieves consistent high segmentation accuracies on several metrics while delivering an acceptable prediction efficiency, establishing a promising basis for further segmentation and event detection approaches on eye surgery videos.},
	booktitle = {2020 {IEEE} 17th {International} {Symposium} on {Biomedical} {Imaging} {Workshops} ({ISBI} {Workshops})},
	author = {Sokolova, Natalia and Taschwer, Mario and Sarny, Stephanie and Putzgruber-Adamitsch, Doris and Schoeffmann, Klaus},
	month = apr,
	year = {2020},
	keywords = {\#NONE, Cataract, Biomedical imaging, Cataracts, Image segmentation, Surgery, Videos, cataract surgery videos, deep learning, Iris, Iris recognition, mask RCNN, object segmentation},
	pages = {1--4},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\felix\\Zotero\\storage\\XVAZWK4Y\\9153367.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\8F8ETDNT\\Sokolova et al. - 2020 - Pixel-Based Iris and Pupil Segmentation in Catarac.pdf:application/pdf},
}

@misc{namazi_laptool-net_2019,
	title = {{LapTool}-{Net}: {A} {Contextual} {Detector} of {Surgical} {Tools} in {Laparoscopic} {Videos} {Based} on {Recurrent} {Convolutional} {Neural} {Networks}},
	shorttitle = {{LapTool}-{Net}},
	url = {http://arxiv.org/abs/1905.08983},
	doi = {10.48550/arXiv.1905.08983},
	abstract = {We propose a new multilabel classifier, called LapTool-Net to detect the presence of surgical tools in each frame of a laparoscopic video. The novelty of LapTool-Net is the exploitation of the correlation among the usage of different tools and, the tools and tasks - namely, the context of the tools' usage. Towards this goal, the pattern in the co-occurrence of the tools is utilized for designing a decision policy for a multilabel classifier based on a Recurrent Convolutional Neural Network (RCNN) architecture to simultaneously extract the spatio-temporal features. In contrast to the previous multilabel classification methods, the RCNN and the decision model are trained in an end-to-end manner using a multitask learning scheme. To overcome the high imbalance and avoid overfitting caused by the lack of variety in the training data, a high down-sampling rate is chosen based on the more frequent combinations. Furthermore, at the post-processing step, the prediction for all the frames of a video are corrected by designing a bi-directional RNN to model the long-term task's order. LapTool-net was trained using a publicly available dataset of laparoscopic cholecystectomy. The results show LapTool-Net outperforms existing methods significantly, even while using fewer training samples and a shallower architecture.},
	urldate = {2022-10-21},
	publisher = {arXiv},
	author = {Namazi, Babak and Sankaranarayanan, Ganesh and Devarajan, Venkat},
	month = may,
	year = {2019},
	note = {arXiv:1905.08983 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, \#SKIMMED, Tool Detection, Laparoscopy},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\LCNH6HIM\\Namazi et al. - 2019 - LapTool-Net A Contextual Detector of Surgical Too.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\7IUY65RC\\1905.html:text/html},
}

@misc{jin_exploring_2022,
	title = {Exploring {Intra}- and {Inter}-{Video} {Relation} for {Surgical} {Semantic} {Scene} {Segmentation}},
	url = {http://arxiv.org/abs/2203.15251},
	doi = {10.48550/arXiv.2203.15251},
	abstract = {Automatic surgical scene segmentation is fundamental for facilitating cognitive intelligence in the modern operating theatre. Previous works rely on conventional aggregation modules (e.g., dilated convolution, convolutional LSTM), which only make use of the local context. In this paper, we propose a novel framework STswinCL that explores the complementary intra- and inter-video relations to boost segmentation performance, by progressively capturing the global context. We firstly develop a hierarchy Transformer to capture intra-video relation that includes richer spatial and temporal cues from neighbor pixels and previous frames. A joint space-time window shift scheme is proposed to efficiently aggregate these two cues into each pixel embedding. Then, we explore inter-video relation via pixel-to-pixel contrastive learning, which well structures the global embedding space. A multi-source contrast training objective is developed to group the pixel embeddings across videos with the ground-truth guidance, which is crucial for learning the global property of the whole data. We extensively validate our approach on two public surgical video benchmarks, including EndoVis18 Challenge and CaDIS dataset. Experimental results demonstrate the promising performance of our method, which consistently exceeds previous state-of-the-art approaches. Code is available at https://github.com/YuemingJin/STswinCL.},
	urldate = {2022-10-21},
	publisher = {arXiv},
	author = {Jin, Yueming and Yu, Yang and Chen, Cheng and Zhao, Zixu and Heng, Pheng-Ann and Stoyanov, Danail},
	month = jun,
	year = {2022},
	note = {arXiv:2203.15251 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Cataract, \#SKIMMED, CholecXXX, \#VidSGG},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\ALIQF4RG\\Jin et al. - 2022 - Exploring Intra- and Inter-Video Relation for Surg.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\WVL8H3LW\\2203.html:text/html},
}

@misc{zhang_is_2022,
	title = {Is an {Object}-{Centric} {Video} {Representation} {Beneficial} for {Transfer}?},
	url = {http://arxiv.org/abs/2207.10075},
	doi = {10.48550/arXiv.2207.10075},
	abstract = {The objective of this work is to learn an object-centric video representation, with the aim of improving transferability to novel tasks, i.e., tasks different from the pre-training task of action classification. To this end, we introduce a new object-centric video recognition model based on a transformer architecture. The model learns a set of object-centric summary vectors for the video, and uses these vectors to fuse the visual and spatio-temporal trajectory 'modalities' of the video clip. We also introduce a novel trajectory contrast loss to further enhance objectness in these summary vectors. With experiments on four datasets -- SomethingSomething-V2, SomethingElse, Action Genome and EpicKitchens -- we show that the object-centric model outperforms prior video representations (both object-agnostic and object-aware), when: (1) classifying actions on unseen objects and unseen environments; (2) low-shot learning of novel classes; (3) linear probe to other downstream tasks; as well as (4) for standard action classification.},
	urldate = {2022-10-21},
	publisher = {arXiv},
	author = {Zhang, Chuhan and Gupta, Ankush and Zisserman, Andrew},
	month = oct,
	year = {2022},
	note = {arXiv:2207.10075 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#TOREAD, \#VidSGG},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\GM3SGKG6\\Zhang et al. - 2022 - Is an Object-Centric Video Representation Benefici.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\GV5YXHHT\\2207.html:text/html},
}

@misc{ucu_transformer-based_2021,
	title = {Transformer-based self-supervised learning for medical images},
	url = {https://medium.com/@mllabucu/transformer-based-self-supervised-learning-for-medical-images-41395d069829},
	abstract = {In this blog we share our results in testing self-supervised approaches (originally designed for natural scene datasets) for problems in…},
	language = {en},
	urldate = {2022-10-21},
	journal = {Medium},
	author = {UCU, Machine Learning Lab},
	month = sep,
	year = {2021},
	keywords = {\#SKIMMED, Transformer, Self-Supervised Learning},
	file = {Snapshot:C\:\\Users\\felix\\Zotero\\storage\\9NI8Y7VL\\transformer-based-self-supervised-learning-for-medical-images-41395d069829.html:text/html},
}

@misc{wang_freesolo_2022,
	title = {{FreeSOLO}: {Learning} to {Segment} {Objects} without {Annotations}},
	shorttitle = {{FreeSOLO}},
	url = {http://arxiv.org/abs/2202.12181},
	doi = {10.48550/arXiv.2202.12181},
	abstract = {Instance segmentation is a fundamental vision task that aims to recognize and segment each object in an image. However, it requires costly annotations such as bounding boxes and segmentation masks for learning. In this work, we propose a fully unsupervised learning method that learns class-agnostic instance segmentation without any annotations. We present FreeSOLO, a self-supervised instance segmentation framework built on top of the simple instance segmentation method SOLO. Our method also presents a novel localization-aware pre-training framework, where objects can be discovered from complicated scenes in an unsupervised manner. FreeSOLO achieves 9.8\% AP\_\{50\} on the challenging COCO dataset, which even outperforms several segmentation proposal methods that use manual annotations. For the first time, we demonstrate unsupervised class-agnostic instance segmentation successfully. FreeSOLO's box localization significantly outperforms state-of-the-art unsupervised object detection/discovery methods, with about 100\% relative improvements in COCO AP. FreeSOLO further demonstrates superiority as a strong pre-training method, outperforming state-of-the-art self-supervised pre-training methods by +9.8\% AP when fine-tuning instance segmentation with only 5\% COCO masks. Code is available at: github.com/NVlabs/FreeSOLO},
	urldate = {2022-10-21},
	publisher = {arXiv},
	author = {Wang, Xinlong and Yu, Zhiding and De Mello, Shalini and Kautz, Jan and Anandkumar, Anima and Shen, Chunhua and Alvarez, Jose M.},
	month = apr,
	year = {2022},
	note = {arXiv:2202.12181 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#TOSKIM, \#SSSGG},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\9SDFLXNY\\Wang et al. - 2022 - FreeSOLO Learning to Segment Objects without Anno.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\R73AVVLN\\2202.html:text/html},
}

@misc{choudhury_guess_2022,
	title = {Guess {What} {Moves}: {Unsupervised} {Video} and {Image} {Segmentation} by {Anticipating} {Motion}},
	shorttitle = {Guess {What} {Moves}},
	url = {http://arxiv.org/abs/2205.07844},
	doi = {10.48550/arXiv.2205.07844},
	abstract = {Motion, measured via optical flow, provides a powerful cue to discover and learn objects in images and videos. However, compared to using appearance, it has some blind spots, such as the fact that objects become invisible if they do not move. In this work, we propose an approach that combines the strengths of motion-based and appearance-based segmentation. We propose to supervise an image segmentation network with the pretext task of predicting regions that are likely to contain simple motion patterns, and thus likely to correspond to objects. As the model only uses a single image as input, we can apply it in two settings: unsupervised video segmentation, and unsupervised image segmentation. We achieve state-of-the-art results for videos, and demonstrate the viability of our approach on still images containing novel objects. Additionally we experiment with different motion models and optical flow backbones and find the method to be robust to these change. Project page and code available at https://www.robots.ox.ac.uk/{\textasciitilde}vgg/research/gwm.},
	urldate = {2022-10-25},
	publisher = {arXiv},
	author = {Choudhury, Subhabrata and Karazija, Laurynas and Laina, Iro and Vedaldi, Andrea and Rupprecht, Christian},
	month = oct,
	year = {2022},
	note = {arXiv:2205.07844 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#TOSKIM, Segmentation, Self-Supervised Learning, \#SSSGG},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\GZMNMSC8\\Choudhury et al. - 2022 - Guess What Moves Unsupervised Video and Image Seg.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\HDGQY5Z3\\2205.html:text/html},
}

@article{hira_video-based_2022,
	title = {Video-based assessment of intraoperative surgical skill},
	volume = {17},
	issn = {1861-6429},
	url = {https://link.springer.com/10.1007/s11548-022-02681-5},
	doi = {10.1007/s11548-022-02681-5},
	abstract = {Purpose Surgeons’ skill in the operating room is a major determinant of patient outcomes. Assessment of surgeons’ skill is necessary to improve patient outcomes and quality of care through surgical training and coaching. Methods for video-based assessment of surgical skill can provide objective and efﬁcient tools for surgeons. Our work introduces a new method based on attention mechanisms and provides a comprehensive comparative analysis of state-of-the-art methods for video-based assessment of surgical skill in the operating room.
Methods Using a dataset of 99 videos of capsulorhexis, a critical step in cataract surgery, we evaluated image feature-based methods and two deep learning methods to assess skill using RGB videos. In the ﬁrst method, we predict instrument tips as keypoints and predict surgical skill using temporal convolutional neural networks. In the second method, we propose a frame-wise encoder (2D convolutional neural network) followed by a temporal model (recurrent neural network), both of which are augmented by visual attention mechanisms. We computed the area under the receiver operating characteristic curve (AUC), sensitivity, speciﬁcity, and predictive values through ﬁvefold cross-validation.
Results To classify a binary skill label (expert vs. novice), the range of AUC estimates was 0.49 (95\% conﬁdence interval; CI = 0.37 to 0.60) to 0.76 (95\% CI = 0.66 to 0.85) for image feature-based methods. The sensitivity and speciﬁcity were consistently high for none of the methods. For the deep learning methods, the AUC was 0.79 (95\% CI = 0.70 to 0.88) using keypoints alone, 0.78 (95\% CI = 0.69 to 0.88) and 0.75 (95\% CI = 0.65 to 0.85) with and without attention mechanisms, respectively.
Conclusion Deep learning methods are necessary for video-based assessment of surgical skill in the operating room. Attention mechanisms improved discrimination ability of the network. Our ﬁndings should be evaluated for external validity in other datasets.},
	language = {en},
	number = {10},
	urldate = {2022-10-25},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {Hira, Sanchit and Singh, Digvijay and Kim, Tae Soo and Gupta, Shobhit and Hager, Gregory and Sikder, Shameema and Vedula, S. Swaroop},
	month = may,
	year = {2022},
	keywords = {\#NONE, Attention, Surgical Skill},
	pages = {1801--1811},
	file = {Hira et al. - 2022 - Video-based assessment of intraoperative surgical .pdf:C\:\\Users\\felix\\Zotero\\storage\\QMLJXJIM\\Hira et al. - 2022 - Video-based assessment of intraoperative surgical .pdf:application/pdf},
}

@misc{bardes_vicregl_2022,
	title = {{VICRegL}: {Self}-{Supervised} {Learning} of {Local} {Visual} {Features}},
	shorttitle = {{VICRegL}},
	url = {http://arxiv.org/abs/2210.01571},
	doi = {10.48550/arXiv.2210.01571},
	abstract = {Most recent self-supervised methods for learning image representations focus on either producing a global feature with invariance properties, or producing a set of local features. The former works best for classification tasks while the latter is best for detection and segmentation tasks. This paper explores the fundamental trade-off between learning local and global features. A new method called VICRegL is proposed that learns good global and local features simultaneously, yielding excellent performance on detection and segmentation tasks while maintaining good performance on classification tasks. Concretely, two identical branches of a standard convolutional net architecture are fed two differently distorted versions of the same image. The VICReg criterion is applied to pairs of global feature vectors. Simultaneously, the VICReg criterion is applied to pairs of local feature vectors occurring before the last pooling layer. Two local feature vectors are attracted to each other if their l2-distance is below a threshold or if their relative locations are consistent with a known geometric transformation between the two input images. We demonstrate strong performance on linear classification and segmentation transfer tasks. Code and pretrained models are publicly available at: https://github.com/facebookresearch/VICRegL},
	urldate = {2022-10-25},
	publisher = {arXiv},
	author = {Bardes, Adrien and Ponce, Jean and LeCun, Yann},
	month = oct,
	year = {2022},
	note = {arXiv:2210.01571 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, \#TOSKIM, Self-Supervised Learning, \#SSSGG},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\HW3R8RYL\\Bardes et al. - 2022 - VICRegL Self-Supervised Learning of Local Visual .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\Y8MPTDM9\\2210.html:text/html},
}

@misc{han_image_2021,
	title = {Image {Scene} {Graph} {Generation} ({SGG}) {Benchmark}},
	url = {http://arxiv.org/abs/2107.12604},
	doi = {10.48550/arXiv.2107.12604},
	abstract = {There is a surge of interest in image scene graph generation (object, attribute and relationship detection) due to the need of building fine-grained image understanding models that go beyond object detection. Due to the lack of a good benchmark, the reported results of different scene graph generation models are not directly comparable, impeding the research progress. We have developed a much-needed scene graph generation benchmark based on the maskrcnn-benchmark and several popular models. This paper presents main features of our benchmark and a comprehensive ablation study of scene graph generation models using the Visual Genome and OpenImages Visual relationship detection datasets. Our codebase is made publicly available at https://github.com/microsoft/scene\_graph\_benchmark.},
	urldate = {2022-10-26},
	publisher = {arXiv},
	author = {Han, Xiaotian and Yang, Jianwei and Hu, Houdong and Zhang, Lei and Gao, Jianfeng and Zhang, Pengchuan},
	month = jul,
	year = {2021},
	note = {arXiv:2107.12604 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#TOREAD, SGG},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\3QR6AHIM\\Han et al. - 2021 - Image Scene Graph Generation (SGG) Benchmark.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\45L5Y8QA\\2107.html:text/html},
}

@inproceedings{li_sgtr_2022,
	address = {New Orleans, LA, USA},
	title = {{SGTR}: {End}-to-end {Scene} {Graph} {Generation} with {Transformer}},
	isbn = {978-1-66546-946-3},
	shorttitle = {{SGTR}},
	url = {https://ieeexplore.ieee.org/document/9879818/},
	doi = {10.1109/CVPR52688.2022.01888},
	language = {en},
	urldate = {2022-11-02},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Li, Rongjie and Zhang, Songyang and He, Xuming},
	month = jun,
	year = {2022},
	keywords = {\#SKIMMED, SGG, Transformer},
	pages = {19464--19474},
	file = {Li et al. - 2022 - SGTR End-to-end Scene Graph Generation with Trans.pdf:C\:\\Users\\felix\\Zotero\\storage\\ND2737K6\\Li et al. - 2022 - SGTR End-to-end Scene Graph Generation with Trans.pdf:application/pdf},
}

@inproceedings{gao_classification-then-grounding_2022,
	address = {New Orleans, LA, USA},
	title = {Classification-{Then}-{Grounding}: {Reformulating} {Video} {Scene} {Graphs} as {Temporal} {Bipartite} {Graphs}},
	isbn = {978-1-66546-946-3},
	shorttitle = {Classification-{Then}-{Grounding}},
	url = {https://ieeexplore.ieee.org/document/9879749/},
	doi = {10.1109/CVPR52688.2022.01889},
	abstract = {Today’s VidSGG models are all proposal-based methods, i.e., they ﬁrst generate numerous paired subject-object snippets as proposals, and then conduct predicate classiﬁcation for each proposal. In this paper, we argue that this prevalent proposal-based framework has three inherent drawbacks: 1) The ground-truth predicate labels for proposals are partially correct. 2) They break the high-order relations among different predicate instances of a same subject-object pair. 3) VidSGG performance is upper-bounded by the quality of the proposals. To this end, we propose a new classiﬁcationthen-grounding framework for VidSGG, which can avoid all the three overlooked drawbacks. Meanwhile, under this framework, we reformulate the video scene graphs as temporal bipartite graphs, where the entities and predicates are two types of nodes with time slots, and the edges denote different semantic roles between these nodes. This formulation takes full advantage of our new framework. Accordingly, we further propose a novel BIpartite Graph based SGG model: BIG. It consists of a classiﬁcation stage and a grounding stage, where the former aims to classify the categories of all the nodes and the edges, and the latter tries to localize the temporal location of each relation instance. Extensive ablations on two VidSGG datasets have attested to the effectiveness of our framework and BIG. Code is available at https://github.com/Dawn-LX/VidSGG-BIG.},
	language = {en},
	urldate = {2022-11-09},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Gao, Kaifeng and Chen, Long and Niu, Yulei and Shao, Jian and Xiao, Jun},
	month = jun,
	year = {2022},
	keywords = {\#SKIMMED, SGG, Action Genome, \#VidSGG},
	pages = {19475--19484},
	file = {Gao et al. - 2022 - Classification-Then-Grounding Reformulating Video.pdf:C\:\\Users\\felix\\Zotero\\storage\\KSSFJ4WX\\Gao et al. - 2022 - Classification-Then-Grounding Reformulating Video.pdf:application/pdf},
}

@inproceedings{li_dynamic_2022,
	address = {New Orleans, LA, USA},
	title = {Dynamic {Scene} {Graph} {Generation} via {Anticipatory} {Pre}-training},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9878927/},
	doi = {10.1109/CVPR52688.2022.01350},
	abstract = {Humans can not only see the collection of objects in visual scenes, but also identify the relationship between objects. The visual relationship in the scene can be abstracted into the semantic representation of a triple subject, predicate, object and thus results in a scene graph, which can convey a lot of information for visual understanding. Due to the motion of objects, the visual relationship between two objects in videos may vary, which makes the task of dynamically generating scene graphs from videos more complicated and challenging than the conventional image-based static scene graph generation. Inspired by the ability of humans to infer the visual relationship, we propose a novel anticipatory pre-training paradigm based on Transformer to explicitly model the temporal correlation of visual relationships in different frames to improve dynamic scene graph generation. In pre-training stage, the model predicts the visual relationships of current frame based on the previous frames by extracting intra-frame spatial information with a spatial encoder and inter-frame temporal correlations with a progressive temporal encoder. In the ﬁne-tuning stage, we reuse the spatial encoder and the progressive temporal encoder while the information of the current frame is combined for predicting the visual relationship. Extensive experiments demonstrate that our method achieves state-of-the-art performance on Action Genome dataset.},
	language = {en},
	urldate = {2022-11-09},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Li, Yiming and Yang, Xiaoshan and Xu, Changsheng},
	month = jun,
	year = {2022},
	keywords = {\#SKIMMED, SGG, Action Genome, \#VidSGG},
	pages = {13864--13873},
	file = {Li et al. - 2022 - Dynamic Scene Graph Generation via Anticipatory Pr.pdf:C\:\\Users\\felix\\Zotero\\storage\\QJX9XQS4\\Li et al. - 2022 - Dynamic Scene Graph Generation via Anticipatory Pr.pdf:application/pdf},
}

@inproceedings{xu_scene_2017,
	title = {Scene {Graph} {Generation} by {Iterative} {Message} {Passing}},
	doi = {10.1109/CVPR.2017.330},
	abstract = {Understanding a visual scene goes beyond recognizing individual objects in isolation. Relationships between objects also constitute rich semantic information about the scene. In this work, we explicitly model the objects and their relationships using scene graphs, a visually-grounded graphical structure of an image. We propose a novel end-to-end model that generates such structured scene representation from an input image. Our key insight is that the graph generation problem can be formulated as message passing between the primal node graph and its dual edge graph. Our joint inference model can take advantage of contextual cues to make better predictions on objects and their relationships. The experiments show that our model significantly outperforms previous methods on the Visual Genome dataset as well as support relation inference in NYU Depth V2 dataset.},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Xu, Danfei and Zhu, Yuke and Choy, Christopher B. and Fei-Fei, Li},
	month = jul,
	year = {2017},
	note = {ISSN: 1063-6919},
	keywords = {\#NONE, Message Passing, Scene Graphs, Visualization, Predictive models, Image edge detection, Message passing, Proposals, Semantics},
	pages = {3097--3106},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\felix\\Zotero\\storage\\54GZVYXP\\8099813.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\64FGUG4U\\Xu et al. - 2017 - Scene Graph Generation by Iterative Message Passin.pdf:application/pdf},
}

@inproceedings{zhang_graphical_2019,
	title = {Graphical {Contrastive} {Losses} for {Scene} {Graph} {Parsing}},
	doi = {10.1109/CVPR.2019.01180},
	abstract = {Most scene graph parsers use a two-stage pipeline to detect visual relationships: the first stage detects entities, and the second predicts the predicate for each entity pair using a softmax distribution. We find that such pipelines, trained with only a cross entropy loss over predicate classes, suffer from two common errors. The first, Entity Instance Confusion, occurs when the model confuses multiple instances of the same type of entity (e.g. multiple cups). The second, Proximal Relationship Ambiguity, arises when multiple subject-predicate-object triplets appear in close proximity with the same predicate, and the model struggles to infer the correct subject-object pairings (e.g. mis-pairing musicians and their instruments). We propose a set of contrastive loss formulations that specifically target these types of errors within the scene graph parsing problem, collectively termed the Graphical Contrastive Losses. These losses explicitly force the model to disambiguate related and unrelated instances through margin constraints specific to each type of confusion. We further construct a relationship detector, called RelDN, using the aforementioned pipeline to demonstrate the efficacy of our proposed losses. Our model outperforms the winning method of the OpenImages Relationship Detection Challenge by 4.7\% (16.5\% relatively) on the test set. We also show improved results over the best previous methods on the Visual Genome and Visual Relationship Detection datasets.},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Zhang, Ji and Shih, Kevin J. and Elgammal, Ahmed and Tao, Andrew and Catanzaro, Bryan},
	month = jun,
	year = {2019},
	note = {ISSN: 2575-7075},
	keywords = {\#NONE, Categorization, Recognition: Detection, Retrieval, Scene Analysis and Understanding},
	pages = {11527--11535},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\felix\\Zotero\\storage\\WNTPHW46\\stamp.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\3GPZNZBR\\Zhang et al. - 2019 - Graphical Contrastive Losses for Scene Graph Parsi.pdf:application/pdf},
}

@inproceedings{zareian_weakly_2020,
	address = {Seattle, WA, USA},
	title = {Weakly {Supervised} {Visual} {Semantic} {Parsing}},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9157764/},
	doi = {10.1109/CVPR42600.2020.00379},
	abstract = {Scene Graph Generation (SGG) aims to extract entities, predicates and their semantic structure from images, enabling deep understanding of visual content, with many applications such as visual reasoning and image retrieval. Nevertheless, existing SGG methods require millions of manually annotated bounding boxes for training, and are computationally inefﬁcient, as they exhaustively process all pairs of object proposals to detect predicates. In this paper, we address those two limitations by ﬁrst proposing a generalized formulation of SGG, namely Visual Semantic Parsing, which disentangles entity and predicate recognition, and enables sub-quadratic performance. Then we propose the Visual Semantic Parsing Network, VSPNET, based on a dynamic, attention-based, bipartite message passing framework that jointly infers graph nodes and edges through an iterative process. Additionally, we propose the ﬁrst graphbased weakly supervised learning framework, based on a novel graph alignment algorithm, which enables training without bounding box annotations. Through extensive experiments, we show that VSPNET outperforms weakly supervised baselines signiﬁcantly and approaches fully supervised performance, while being several times faster. We publicly release the source code of our method1.},
	language = {en},
	urldate = {2022-11-10},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zareian, Alireza and Karaman, Svebor and Chang, Shih-Fu},
	month = jun,
	year = {2020},
	keywords = {\#TOREAD},
	pages = {3733--3742},
	file = {Zareian et al. - 2020 - Weakly Supervised Visual Semantic Parsing.pdf:C\:\\Users\\felix\\Zotero\\storage\\IV55C347\\Zareian et al. - 2020 - Weakly Supervised Visual Semantic Parsing.pdf:application/pdf},
}

@inproceedings{zhong_learning_2021,
	address = {Montreal, QC, Canada},
	title = {Learning to {Generate} {Scene} {Graph} from {Natural} {Language} {Supervision}},
	isbn = {978-1-66542-812-5},
	url = {https://ieeexplore.ieee.org/document/9711462/},
	doi = {10.1109/ICCV48922.2021.00184},
	abstract = {Learning from image-text data has demonstrated recent success for many recognition tasks, yet is currently limited to visual features or individual visual concepts such as objects. In this paper, we propose one of the ﬁrst methods that learn from image-sentence pairs to extract a graphical representation of localized objects and their relationships within an image, known as scene graph. To bridge the gap between images and texts, we leverage an offthe-shelf object detector to identify and localize object instances, match labels of detected regions to concepts parsed from captions, and thus create “pseudo” labels for learning scene graph. Further, we design a Transformer-based model to predict these “pseudo” labels via a masked token prediction task. Learning from only image-sentence pairs, our model achieves 30\% relative gain over a latest method trained with human-annotated unlocalized scene graphs. Our model also shows strong results for weakly and fully supervised scene graph generation. In addition, we explore an open-vocabulary setting for detecting scene graphs, and present the ﬁrst result for open-set scene graph generation.},
	language = {en},
	urldate = {2022-11-10},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhong, Yiwu and Shi, Jing and Yang, Jianwei and Xu, Chenliang and Li, Yin},
	month = oct,
	year = {2021},
	keywords = {\#TOREAD, SGG},
	pages = {1803--1814},
	file = {Zhong et al. - 2021 - Learning to Generate Scene Graph from Natural Lang.pdf:C\:\\Users\\felix\\Zotero\\storage\\I4QWDW8I\\Zhong et al. - 2021 - Learning to Generate Scene Graph from Natural Lang.pdf:application/pdf},
}

@inproceedings{shi_simple_2021,
	address = {Montreal, QC, Canada},
	title = {A {Simple} {Baseline} for {Weakly}-{Supervised} {Scene} {Graph} {Generation}},
	isbn = {978-1-66542-812-5},
	url = {https://ieeexplore.ieee.org/document/9710723/},
	doi = {10.1109/ICCV48922.2021.01608},
	abstract = {We investigate the weakly-supervised scene graph generation, which is a challenging task since no correspondence of label and object is provided. The previous work regards such correspondence as a latent variable which is iteratively updated via nested optimization of the scene graph generation objective. However, we further reduce the complexity by decoupling it into an efficient first-order graph matching module optimized via contrastive learning to obtain such correspondence, which is used to train a standard scene graph generation model. The extensive experiments show that such a simple pipeline can significantly surpass the previous state-of-the-art by more than 30\% on the Visual Genome dataset, both in terms of graph matching accuracy and scene graph quality. We believe this work serves as a strong baseline for future research. Code is available at https://github.com/jshi31/WS-SGG.},
	language = {en},
	urldate = {2022-11-10},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Shi, Jing and Zhong, Yiwu and Xu, Ning and Li, Yin and Xu, Chenliang},
	month = oct,
	year = {2021},
	keywords = {\#TOREAD, SGG},
	pages = {16373--16382},
	file = {Shi et al. - 2021 - A Simple Baseline for Weakly-Supervised Scene Grap.pdf:C\:\\Users\\felix\\Zotero\\storage\\ZS48B6H2\\Shi et al. - 2021 - A Simple Baseline for Weakly-Supervised Scene Grap.pdf:application/pdf},
}

@misc{wang_scene_2018,
	title = {Scene {Graph} {Parsing} as {Dependency} {Parsing}},
	url = {http://arxiv.org/abs/1803.09189},
	doi = {10.48550/arXiv.1803.09189},
	abstract = {In this paper, we study the problem of parsing structured knowledge graphs from textual descriptions. In particular, we consider the scene graph representation that considers objects together with their attributes and relations: this representation has been proved useful across a variety of vision and language applications. We begin by introducing an alternative but equivalent edge-centric view of scene graphs that connect to dependency parses. Together with a careful redesign of label and action space, we combine the two-stage pipeline used in prior work (generic dependency parsing followed by simple post-processing) into one, enabling end-to-end training. The scene graphs generated by our learned neural dependency parser achieve an F-score similarity of 49.67\% to ground truth graphs on our evaluation set, surpassing best previous approaches by 5\%. We further demonstrate the effectiveness of our learned parser on image retrieval applications.},
	urldate = {2022-11-10},
	publisher = {arXiv},
	author = {Wang, Yu-Siang and Liu, Chenxi and Zeng, Xiaohui and Yuille, Alan},
	month = mar,
	year = {2018},
	note = {arXiv:1803.09189 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#NONE, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\S83Q85TF\\Wang et al. - 2018 - Scene Graph Parsing as Dependency Parsing.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\48CYWTJY\\1803.html:text/html},
}

@inproceedings{yan_pcpl_2020,
	address = {New York, NY, USA},
	series = {{MM} '20},
	title = {{PCPL}: {Predicate}-{Correlation} {Perception} {Learning} for {Unbiased} {Scene} {Graph} {Generation}},
	isbn = {978-1-4503-7988-5},
	shorttitle = {{PCPL}},
	url = {https://doi.org/10.1145/3394171.3413722},
	doi = {10.1145/3394171.3413722},
	abstract = {Today's scene graph generation (SGG) task is largely limited in realistic scenarios, mainly due to the extremely long-tailed bias of predicate annotation distribution. Thus, tackling the class imbalance trouble of SGG is critical and challenging. In this paper, we first discover that when predicate labels have strong correlation with each other, prevalent re-balancing strategies (e.g., re-sampling and re-weighting) will give rise to either over-fitting the tail data (e.g., bench sitting on sidewalk rather than on), or still suffering the adverse effect from the original uneven distribution (e.g., aggregating varied parked on/standing on/sitting on into on). We argue the principal reason is that re-balancing strategies are sensitive to the frequencies of predicates yet blind to their relatedness, which may play a more important role to promote the learning of predicate features. Therefore, we propose a novel Predicate-Correlation Perception Learning (PCPL for short) scheme to adaptively seek out appropriate loss weights by directly perceiving and utilizing the correlation among predicate classes. Moreover, our PCPL framework is further equipped with a graph encoder module to better extract context features. Extensive experiments on the benchmark VG150 dataset show that the proposed PCPL performs markedly better on tail classes while well-preserving the performance on head ones, which significantly outperforms previous state-of-the-art methods.},
	urldate = {2022-11-11},
	booktitle = {Proceedings of the 28th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Yan, Shaotian and Shen, Chen and Jin, Zhongming and Huang, Jianqiang and Jiang, Rongxin and Chen, Yaowu and Hua, Xian-Sheng},
	year = {2020},
	keywords = {\#NONE, SGG, correlation perception, long-tailed bias},
	pages = {265--273},
	file = {Submitted Version:C\:\\Users\\felix\\Zotero\\storage\\3QD68G3L\\Yan et al. - 2020 - PCPL Predicate-Correlation Perception Learning fo.pdf:application/pdf},
}

@inproceedings{chiou_recovering_2021,
	address = {Virtual Event China},
	title = {Recovering the {Unbiased} {Scene} {Graphs} from the {Biased} {Ones}},
	isbn = {978-1-4503-8651-7},
	url = {https://dl.acm.org/doi/10.1145/3474085.3475297},
	doi = {10.1145/3474085.3475297},
	abstract = {Given input images, scene graph generation (SGG) aims to produce comprehensive, graphical representations describing visual relationships among salient objects. Recently, more efforts have been paid to the long tail problem in SGG; however, the imbalance in the fraction of missing labels of different classes, or reporting bias, exacerbating the long tail is rarely considered and cannot be solved by the existing debiasing methods. In this paper we show that, due to the missing labels, SGG can be viewed as a “Learning from Positive and Unlabeled data” (PU learning) problem, where the reporting bias can be removed by recovering the unbiased probabilities from the biased ones by utilizing label frequencies, i.e., the per-class fraction of labeled, positive examples in all the positive examples. To obtain accurate label frequency estimates, we propose Dynamic Label Frequency Estimation (DLFE) to take advantage of training-time data augmentation and average over multiple training iterations to introduce more valid examples. Extensive experiments show that DLFE is more effective in estimating label frequencies than a naive variant of the traditional estimate, and DLFE significantly alleviates the long tail and achieves state-of-the-art debiasing performance on the VG dataset. We also show qualitatively that SGG models with DLFE produce prominently more balanced and unbiased scene graphs. The source code is publicly available1.},
	language = {en},
	urldate = {2022-11-11},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Chiou, Meng-Jiun and Ding, Henghui and Yan, Hanshu and Wang, Changhu and Zimmermann, Roger and Feng, Jiashi},
	month = oct,
	year = {2021},
	keywords = {\#NONE, SGG, Visual Genome},
	pages = {1581--1590},
	file = {Chiou et al. - 2021 - Recovering the Unbiased Scene Graphs from the Bias.pdf:C\:\\Users\\felix\\Zotero\\storage\\4GIAR9A9\\Chiou et al. - 2021 - Recovering the Unbiased Scene Graphs from the Bias.pdf:application/pdf},
}

@inproceedings{tang_unbiased_2020,
	address = {Seattle, WA, USA},
	title = {Unbiased {Scene} {Graph} {Generation} {From} {Biased} {Training}},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9157682/},
	doi = {10.1109/CVPR42600.2020.00377},
	abstract = {Today’s scene graph generation (SGG) task is still far from practical, mainly due to the severe training bias, e.g., collapsing diverse human walk on/ sit on/lay on beach into human on beach. Given such SGG, the down-stream tasks such as VQA can hardly infer better scene structures than merely a bag of objects. However, debiasing in SGG is not trivial because traditional debiasing methods cannot distinguish between the good and bad bias, e.g., good context prior (e.g., person read book rather than eat) and bad long-tailed bias (e.g., near dominating behind/in front of). In this paper, we present a novel SGG framework based on causal inference but not the conventional likelihood. We ﬁrst build a causal graph for SGG, and perform traditional biased training with the graph. Then, we propose to draw the counterfactual causality from the trained graph to infer the effect from the bad bias, which should be removed. In particular, we use Total Direct Effect as the proposed ﬁnal predicate score for unbiased SGG. Note that our framework is agnostic to any SGG model and thus can be widely applied in the community who seeks unbiased predictions. By using the proposed Scene Graph Diagnosis toolkit1 on the SGG benchmark Visual Genome and several prevailing models, we observed signiﬁcant improvements over the previous state-of-the-art methods.},
	language = {en},
	urldate = {2022-11-11},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Tang, Kaihua and Niu, Yulei and Huang, Jianqiang and Shi, Jiaxin and Zhang, Hanwang},
	month = jun,
	year = {2020},
	keywords = {\#SKIMMED, SGG},
	pages = {3713--3722},
	file = {Tang et al. - 2020 - Unbiased Scene Graph Generation From Biased Traini.pdf:C\:\\Users\\felix\\Zotero\\storage\\58JSH7NH\\Tang et al. - 2020 - Unbiased Scene Graph Generation From Biased Traini.pdf:application/pdf},
}

@inproceedings{lin_gps-net_2020,
	address = {Seattle, WA, USA},
	title = {{GPS}-{Net}: {Graph} {Property} {Sensing} {Network} for {Scene} {Graph} {Generation}},
	isbn = {978-1-72817-168-5},
	shorttitle = {{GPS}-{Net}},
	url = {https://ieeexplore.ieee.org/document/9156878/},
	doi = {10.1109/CVPR42600.2020.00380},
	language = {en},
	urldate = {2022-11-11},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Lin, Xin and Ding, Changxing and Zeng, Jinquan and Tao, Dacheng},
	month = jun,
	year = {2020},
	keywords = {\#NONE},
	pages = {3743--3752},
	file = {Lin et al. - 2020 - GPS-Net Graph Property Sensing Network for Scene .pdf:C\:\\Users\\felix\\Zotero\\storage\\9QDQWCKJ\\Lin et al. - 2020 - GPS-Net Graph Property Sensing Network for Scene .pdf:application/pdf},
}

@inproceedings{tang_learning_2019,
	address = {Long Beach, CA, USA},
	title = {Learning to {Compose} {Dynamic} {Tree} {Structures} for {Visual} {Contexts}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8954146/},
	doi = {10.1109/CVPR.2019.00678},
	abstract = {We propose to compose dynamic tree structures that place the objects in an image into a visual context, helping visual reasoning tasks such as scene graph generation and visual Q\&A. Our visual context tree model, dubbed VCTREE, has two key advantages over existing structured object representations including chains and fully-connected graphs: 1) The efﬁcient and expressive binary tree encodes the inherent parallel/hierarchical relationships among objects, e.g., “clothes” and “pants” are usually co-occur and belong to “person”; 2) the dynamic structure varies from image to image and task to task, allowing more content/task-speciﬁc message passing among objects. To construct a VCTREE, we design a score function that calculates the task-dependent validity between each object pair, and the tree is the binary version of the maximum spanning tree from the score matrix. Then, visual contexts are encoded by bidirectional TreeLSTM and decoded by task-speciﬁc models. We develop a hybrid learning procedure which integrates end-task supervised learning and the tree structure reinforcement learning, where the former’s evaluation result serves as a self-critic for the latter’s structure exploration. Experimental results on two benchmarks, which require reasoning over contexts: Visual Genome for scene graph generation and VQA2.0 for visual Q\&A, show that VCTREE outperforms state-of-the-art results while discovering interpretable visual context structures.},
	language = {en},
	urldate = {2022-11-11},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Tang, Kaihua and Zhang, Hanwang and Wu, Baoyuan and Luo, Wenhan and Liu, Wei},
	month = jun,
	year = {2019},
	keywords = {\#NONE},
	pages = {6612--6621},
	file = {Tang et al. - 2019 - Learning to Compose Dynamic Tree Structures for Vi.pdf:C\:\\Users\\felix\\Zotero\\storage\\5B33B26X\\Tang et al. - 2019 - Learning to Compose Dynamic Tree Structures for Vi.pdf:application/pdf},
}

@inproceedings{wu_scenegraphfusion_2021,
	address = {Nashville, TN, USA},
	title = {{SceneGraphFusion}: {Incremental} {3D} {Scene} {Graph} {Prediction} from {RGB}-{D} {Sequences}},
	isbn = {978-1-66544-509-2},
	shorttitle = {{SceneGraphFusion}},
	url = {https://ieeexplore.ieee.org/document/9578559/},
	doi = {10.1109/CVPR46437.2021.00743},
	abstract = {Scene graphs are a compact and explicit representation successfully used in a variety of 2D scene understanding tasks. This work proposes a method to incrementally build up semantic scene graphs from a 3D environment given a sequence of RGB-D frames. To this end, we aggregate PointNet features from primitive scene components by means of a graph neural network. We also propose a novel attention mechanism well suited for partial and missing graph data present in such an incremental reconstruction scenario. Although our proposed method is designed to run on submaps of the scene, we show it also transfers to entire 3D scenes. Experiments show that our approach outperforms 3D scene graph prediction methods by a large margin and its accuracy is on par with other 3D semantic and panoptic segmentation methods while running at 35Hz.},
	language = {en},
	urldate = {2022-11-11},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Wu, Shun-Cheng and Wald, Johanna and Tateno, Keisuke and Navab, Nassir and Tombari, Federico},
	month = jun,
	year = {2021},
	keywords = {3D, \#NONE, Scene Graphs},
	pages = {7511--7521},
	file = {Wu et al. - 2021 - SceneGraphFusion Incremental 3D Scene Graph Predi.pdf:C\:\\Users\\felix\\Zotero\\storage\\8MI5I2JZ\\Wu et al. - 2021 - SceneGraphFusion Incremental 3D Scene Graph Predi.pdf:application/pdf},
}

@inproceedings{dhamo_graph--3d_2021,
	address = {Montreal, QC, Canada},
	title = {Graph-to-{3D}: {End}-to-{End} {Generation} and {Manipulation} of {3D} {Scenes} {Using} {Scene} {Graphs}},
	isbn = {978-1-66542-812-5},
	shorttitle = {Graph-to-{3D}},
	url = {https://ieeexplore.ieee.org/document/9710451/},
	doi = {10.1109/ICCV48922.2021.01604},
	abstract = {Controllable scene synthesis consists of generating 3D information that satisfy underlying speciﬁcations. Thereby, these speciﬁcations should be abstract, i.e. allowing easy user interaction, whilst providing enough interface for detailed control. Scene graphs are representations of a scene, composed of objects (nodes) and inter-object relationships (edges), proven to be particularly suited for this task, as they allow for semantic control on the generated content. Previous works tackling this task often rely on synthetic data, and retrieve object meshes, which naturally limits the generation capabilities. To circumvent this issue, we instead propose the ﬁrst work that directly generates shapes from a scene graph in an end-to-end manner. In addition, we show that the same model supports scene modiﬁcation, using the respective scene graph as interface. Leveraging Graph Convolutional Networks (GCN) we train a variational Auto-Encoder on top of the object and edge categories, as well as 3D shapes and scene layouts, allowing latter sampling of new scenes and shapes.},
	language = {en},
	urldate = {2022-11-11},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Dhamo, Helisa and Manhardt, Fabian and Navab, Nassir and Tombari, Federico},
	month = oct,
	year = {2021},
	keywords = {\#TOSKIM, Scene Graphs},
	pages = {16332--16341},
	file = {Dhamo et al. - 2021 - Graph-to-3D End-to-End Generation and Manipulatio.pdf:C\:\\Users\\felix\\Zotero\\storage\\D3PZZ5P3\\Dhamo et al. - 2021 - Graph-to-3D End-to-End Generation and Manipulatio.pdf:application/pdf},
}

@inproceedings{dhamo_semantic_2020,
	address = {Seattle, WA, USA},
	title = {Semantic {Image} {Manipulation} {Using} {Scene} {Graphs}},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9157808/},
	doi = {10.1109/CVPR42600.2020.00526},
	abstract = {Image manipulation can be considered a special case of image generation where the image to be produced is a modiﬁcation of an existing image. Image generation and manipulation have been, for the most part, tasks that operate on raw pixels. However, the remarkable progress in learning rich image and object representations has opened the way for tasks such as text-to-image or layout-to-image generation that are mainly driven by semantics. In our work, we address the novel problem of image manipulation from scene graphs, in which a user can edit images by merely applying changes in the nodes or edges of a semantic graph that is generated from the image. Our goal is to encode image information in a given constellation and from there on generate new constellations, such as replacing objects or even changing relationships between objects, while respecting the semantics and style from the original image. We introduce a spatiosemantic scene graph network that does not require direct supervision for constellation changes or image edits. This makes it possible to train the system from existing real-world datasets with no additional annotation effort.},
	language = {en},
	urldate = {2022-11-11},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Dhamo, Helisa and Farshad, Azade and Laina, Iro and Navab, Nassir and Hager, Gregory D. and Tombari, Federico and Rupprecht, Christian},
	month = jun,
	year = {2020},
	keywords = {\#NONE},
	pages = {5212--5221},
	file = {Dhamo et al. - 2020 - Semantic Image Manipulation Using Scene Graphs.pdf:C\:\\Users\\felix\\Zotero\\storage\\TWFHWJQL\\Dhamo et al. - 2020 - Semantic Image Manipulation Using Scene Graphs.pdf:application/pdf},
}

@misc{hudson_learning_2019,
	title = {Learning by {Abstraction}: {The} {Neural} {State} {Machine}},
	shorttitle = {Learning by {Abstraction}},
	url = {http://arxiv.org/abs/1907.03950},
	abstract = {We introduce the Neural State Machine, seeking to bridge the gap between the neural and symbolic views of AI and integrate their complementary strengths for the task of visual reasoning. Given an image, we ﬁrst predict a probabilistic graph that represents its underlying semantics and serves as a structured world model. Then, we perform sequential reasoning over the graph, iteratively traversing its nodes to answer a given question or draw a new inference. In contrast to most neural architectures that are designed to closely interact with the raw sensory data, our model operates instead in an abstract latent space, by transforming both the visual and linguistic modalities into semantic concept-based representations, thereby achieving enhanced transparency and modularity. We evaluate our model on VQA-CP and GQA, two recent VQA datasets that involve compositionality, multi-step inference and diverse reasoning skills, achieving state-of-the-art results in both cases. We provide further experiments that illustrate the model’s strong generalization capacity across multiple dimensions, including novel compositions of concepts, changes in the answer distribution, and unseen linguistic structures, demonstrating the qualities and efﬁcacy of our approach.},
	language = {en},
	urldate = {2022-11-11},
	publisher = {arXiv},
	author = {Hudson, Drew A. and Manning, Christopher D.},
	month = nov,
	year = {2019},
	note = {arXiv:1907.03950 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, \#NONE, Computer Science - Computation and Language},
	file = {Hudson and Manning - 2019 - Learning by Abstraction The Neural State Machine.pdf:C\:\\Users\\felix\\Zotero\\storage\\ZJMP3CFA\\Hudson and Manning - 2019 - Learning by Abstraction The Neural State Machine.pdf:application/pdf},
}

@misc{he_delving_2015,
	title = {Delving {Deep} into {Rectifiers}: {Surpassing} {Human}-{Level} {Performance} on {ImageNet} {Classification}},
	shorttitle = {Delving {Deep} into {Rectifiers}},
	url = {http://arxiv.org/abs/1502.01852},
	doi = {10.48550/arXiv.1502.01852},
	abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
	urldate = {2022-11-11},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = feb,
	year = {2015},
	note = {arXiv:1502.01852 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, \#NONE},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\5EMDU57Q\\He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Lev.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\2WQMPB2X\\1502.html:text/html},
}

@misc{ren_faster_2016,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	shorttitle = {Faster {R}-{CNN}},
	url = {http://arxiv.org/abs/1506.01497},
	doi = {10.48550/arXiv.1506.01497},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	urldate = {2022-11-11},
	publisher = {arXiv},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jan,
	year = {2016},
	note = {arXiv:1506.01497 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#NONE},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\FVXUPZKF\\Ren et al. - 2016 - Faster R-CNN Towards Real-Time Object Detection w.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\BJFF82SV\\1506.html:text/html},
}

@article{kuhn_hungarian_1955,
	title = {The {Hungarian} method for the assignment problem},
	volume = {2},
	issn = {00281441, 19319193},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/nav.3800020109},
	doi = {10.1002/nav.3800020109},
	language = {en},
	number = {1-2},
	urldate = {2022-11-11},
	journal = {Naval Research Logistics Quarterly},
	author = {Kuhn, H. W.},
	month = mar,
	year = {1955},
	keywords = {\#NONE},
	pages = {83--97},
	file = {Kuhn - 1955 - The Hungarian method for the assignment problem.pdf:C\:\\Users\\felix\\Zotero\\storage\\IJCJV582\\Kuhn - 1955 - The Hungarian method for the assignment problem.pdf:application/pdf},
}

@misc{zhong_comprehensive_2020,
	title = {Comprehensive {Image} {Captioning} via {Scene} {Graph} {Decomposition}},
	url = {http://arxiv.org/abs/2007.11731},
	doi = {10.48550/arXiv.2007.11731},
	abstract = {We address the challenging problem of image captioning by revisiting the representation of image scene graph. At the core of our method lies the decomposition of a scene graph into a set of sub-graphs, with each sub-graph capturing a semantic component of the input image. We design a deep model to select important sub-graphs, and to decode each selected sub-graph into a single target sentence. By using sub-graphs, our model is able to attend to different components of the image. Our method thus accounts for accurate, diverse, grounded and controllable captioning at the same time. We present extensive experiments to demonstrate the benefits of our comprehensive captioning model. Our method establishes new state-of-the-art results in caption diversity, grounding, and controllability, and compares favourably to latest methods in caption quality. Our project website can be found at http://pages.cs.wisc.edu/{\textasciitilde}yiwuzhong/Sub-GC.html.},
	urldate = {2022-11-11},
	publisher = {arXiv},
	author = {Zhong, Yiwu and Wang, Liwei and Chen, Jianshu and Yu, Dong and Li, Yin},
	month = jul,
	year = {2020},
	note = {arXiv:2007.11731 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#TOSKIM},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\GDIJP5QM\\Zhong et al. - 2020 - Comprehensive Image Captioning via Scene Graph Dec.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\UZMKTPGK\\2007.html:text/html},
}

@misc{johnson_image_2018,
	title = {Image {Generation} from {Scene} {Graphs}},
	url = {http://arxiv.org/abs/1804.01622},
	doi = {10.48550/arXiv.1804.01622},
	abstract = {To truly understand the visual world our models should be able not only to recognize images but also generate them. To this end, there has been exciting recent progress on generating images from natural language descriptions. These methods give stunning results on limited domains such as descriptions of birds or flowers, but struggle to faithfully reproduce complex sentences with many objects and relationships. To overcome this limitation we propose a method for generating images from scene graphs, enabling explicitly reasoning about objects and their relationships. Our model uses graph convolution to process input graphs, computes a scene layout by predicting bounding boxes and segmentation masks for objects, and converts the layout to an image with a cascaded refinement network. The network is trained adversarially against a pair of discriminators to ensure realistic outputs. We validate our approach on Visual Genome and COCO-Stuff, where qualitative results, ablations, and user studies demonstrate our method's ability to generate complex images with multiple objects.},
	urldate = {2022-11-11},
	publisher = {arXiv},
	author = {Johnson, Justin and Gupta, Agrim and Fei-Fei, Li},
	month = apr,
	year = {2018},
	note = {arXiv:1804.01622 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, \#NONE},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\LKPY5FPY\\Johnson et al. - 2018 - Image Generation from Scene Graphs.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\LNM7UNQV\\1804.html:text/html},
}

@misc{tang_scene_2020,
	title = {A {Scene} {Graph} {Generation} {Codebase} in {PyTorch}},
	author = {Tang, Kaihua},
	year = {2020},
}

@misc{wightman_pytorch_2019,
	title = {{PyTorch} {Image} {Models}},
	url = {https://github.com/rwightman/pytorch-image-models},
	publisher = {GitHub},
	author = {Wightman, Ross},
	year = {2019},
	doi = {10.5281/zenodo.4414861},
	note = {Publication Title: GitHub repository},
	keywords = {\#NONE},
}

@inproceedings{johnson_image_2015-1,
	address = {Boston, MA, USA},
	title = {Image retrieval using scene graphs},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7298990/},
	doi = {10.1109/CVPR.2015.7298990},
	abstract = {This paper develops a novel framework for semantic image retrieval based on the notion of a scene graph. Our scene graphs represent objects (“man”, “boat”), attributes of objects (“boat is white”) and relationships between objects (“man standing on boat”). We use these scene graphs as queries to retrieve semantically related images. To this end, we design a conditional random ﬁeld model that reasons about possible groundings of scene graphs to test images. The likelihoods of these groundings are used as ranking scores for retrieval. We introduce a novel dataset of 5,000 human-generated scene graphs grounded to images and use this dataset to evaluate our method for image retrieval. In particular, we evaluate retrieval using full scene graphs and small scene subgraphs, and show that our method outperforms retrieval methods that use only objects or low-level image features. In addition, we show that our full model can be used to improve object localization compared to baseline methods.},
	language = {en},
	urldate = {2022-11-11},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Johnson, Justin and Krishna, Ranjay and Stark, Michael and Li, Li-Jia and Shamma, David A. and Bernstein, Michael S. and Fei-Fei, Li},
	month = jun,
	year = {2015},
	keywords = {\#NONE},
	pages = {3668--3678},
	file = {Johnson et al. - 2015 - Image retrieval using scene graphs.pdf:C\:\\Users\\felix\\Zotero\\storage\\2WLPVBZL\\Johnson et al. - 2015 - Image retrieval using scene graphs.pdf:application/pdf},
}

@article{gold_graduated_1996,
	title = {A graduated assignment algorithm for graph matching},
	volume = {18},
	issn = {1939-3539},
	doi = {10.1109/34.491619},
	abstract = {A graduated assignment algorithm for graph matching is presented which is fast and accurate even in the presence of high noise. By combining graduated nonconvexity, two-way (assignment) constraints, and sparsity, large improvements in accuracy and speed are achieved. Its low order computational complexity [O(lm), where l and m are the number of links in the two graphs] and robustness in the presence of noise offer advantages over traditional combinatorial approaches. The algorithm, not restricted to any special class of graph, is applied to subgraph isomorphism, weighted graph matching, and attributed relational graph matching. To illustrate the performance of the algorithm, attributed relational graphs derived from objects are matched. Then, results from twenty-five thousand experiments conducted on 100 mode random graphs of varying types (graphs with only zero-one links, weighted graphs, and graphs with node attributes and multiple link types) are reported. No comparable results have been reported by any other graph matching algorithm before in the research literature. Twenty-five hundred control experiments are conducted using a relaxation labeling algorithm and large improvements in accuracy are demonstrated.},
	number = {4},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Gold, S. and Rangarajan, A.},
	month = apr,
	year = {1996},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {\#NONE, Computer vision, Computational complexity, Focusing, Gold, Heart, Labeling, Noise robustness, Optimization methods, Polynomials},
	pages = {377--388},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\felix\\Zotero\\storage\\HIH8HVG4\\491619.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\MNTVCV3K\\Gold and Rangarajan - 1996 - A graduated assignment algorithm for graph matchin.pdf:application/pdf},
}

@inproceedings{teng_structured_2022,
	address = {New Orleans, LA, USA},
	title = {Structured {Sparse} {R}-{CNN} for {Direct} {Scene} {Graph} {Generation}},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9879707/},
	doi = {10.1109/CVPR52688.2022.01883},
	abstract = {Scene graph generation (SGG) is to detect object pairs with their relations in an image. Existing SGG approaches often use multi-stage pipelines to decompose this task into object detection, relation graph construction, and dense or dense-to-sparse relation prediction. Instead, from a perspective on SGG as a direct set prediction, this paper presents a simple, sparse, and unified framework, termed as Structured Sparse R-CNN. The key to our method is a set of learnable triplet queries and a structured triplet detector which could be jointly optimized from the training set in an end-to-end manner. Specifically, the triplet queries encode the general prior for object pairs with their relations, and provide an initial guess of scene graphs for subsequent refinement. The triplet detector presents a cascaded architecture to progressively refine the detected scene graphs with the customized dynamic heads. In addition, to relieve the training difficulty of our method, we propose a relaxed and enhanced training strategy based on knowledge distillation from a Siamese Sparse R-CNN. We perform experiments on several datasets: Visual Genome and Open Images V4/V6, and the results demonstrate that our method achieves the state-of-the-art performance. In addition, we also perform in-depth ablation studies to provide insights on our structured modeling in triplet detector design and training strategies. The code and models are made available at https://github.com/MCGNJU/Structured-Sparse-RCNN .},
	language = {en},
	urldate = {2022-11-11},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Teng, Yao and Wang, Limin},
	month = jun,
	year = {2022},
	keywords = {\#NONE},
	pages = {19415--19424},
	file = {Teng and Wang - 2022 - Structured Sparse R-CNN for Direct Scene Graph Gen.pdf:C\:\\Users\\felix\\Zotero\\storage\\XDDBW2B5\\Teng and Wang - 2022 - Structured Sparse R-CNN for Direct Scene Graph Gen.pdf:application/pdf},
}

@inproceedings{zellers_neural_2018,
	address = {Salt Lake City, UT},
	title = {Neural {Motifs}: {Scene} {Graph} {Parsing} with {Global} {Context}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {Neural {Motifs}},
	url = {https://ieeexplore.ieee.org/document/8578709/},
	doi = {10.1109/CVPR.2018.00611},
	abstract = {We investigate the problem of producing structured graph representations of visual scenes. Our work analyzes the role of motifs: regularly appearing substructures in scene graphs. We present new quantitative insights on such repeated structures in the Visual Genome dataset. Our analysis shows that object labels are highly predictive of relation labels but not vice-versa. We also ﬁnd that there are recurring patterns even in larger subgraphs: more than 50\% of graphs contain motifs involving at least two relations. Our analysis motivates a new baseline: given object detections, predict the most frequent relation between object pairs with the given labels, as seen in the training set. This baseline improves on the previous state-of-the-art by an average of 3.6\% relative improvement across evaluation settings. We then introduce Stacked Motif Networks, a new architecture designed to capture higher order motifs in scene graphs that further improves over our strong baseline by an average 7.1\% relative gain. Our code is available at github.com/rowanz/neural-motifs.},
	language = {en},
	urldate = {2022-11-11},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Zellers, Rowan and Yatskar, Mark and Thomson, Sam and Choi, Yejin},
	month = jun,
	year = {2018},
	keywords = {\#NONE, SGG},
	pages = {5831--5840},
	file = {Zellers et al. - 2018 - Neural Motifs Scene Graph Parsing with Global Con.pdf:C\:\\Users\\felix\\Zotero\\storage\\L2GBRN5A\\Zellers et al. - 2018 - Neural Motifs Scene Graph Parsing with Global Con.pdf:application/pdf},
}

@misc{yang_graph_2018,
	title = {Graph {R}-{CNN} for {Scene} {Graph} {Generation}},
	url = {http://arxiv.org/abs/1808.00191},
	doi = {10.48550/arXiv.1808.00191},
	abstract = {We propose a novel scene graph generation model called Graph R-CNN, that is both effective and efficient at detecting objects and their relations in images. Our model contains a Relation Proposal Network (RePN) that efficiently deals with the quadratic number of potential relations between objects in an image. We also propose an attentional Graph Convolutional Network (aGCN) that effectively captures contextual information between objects and relations. Finally, we introduce a new evaluation metric that is more holistic and realistic than existing metrics. We report state-of-the-art performance on scene graph generation as evaluated using both existing and our proposed metrics.},
	urldate = {2022-11-11},
	publisher = {arXiv},
	author = {Yang, Jianwei and Lu, Jiasen and Lee, Stefan and Batra, Dhruv and Parikh, Devi},
	month = aug,
	year = {2018},
	note = {arXiv:1808.00191 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, \#NONE},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\GCT8KDLI\\Yang et al. - 2018 - Graph R-CNN for Scene Graph Generation.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\2CRRW9UN\\1808.html:text/html},
}

@article{kuznetsova_open_2020,
	title = {The {Open} {Images} {Dataset} {V4}: {Unified} image classification, object detection, and visual relationship detection at scale},
	volume = {128},
	issn = {0920-5691, 1573-1405},
	shorttitle = {The {Open} {Images} {Dataset} {V4}},
	url = {http://arxiv.org/abs/1811.00982},
	doi = {10.1007/s11263-020-01316-z},
	abstract = {We present Open Images V4, a dataset of 9.2M images with unified annotations for image classification, object detection and visual relationship detection. The images have a Creative Commons Attribution license that allows to share and adapt the material, and they have been collected from Flickr without a predefined list of class names or tags, leading to natural class statistics and avoiding an initial design bias. Open Images V4 offers large scale across several dimensions: 30.1M image-level labels for 19.8k concepts, 15.4M bounding boxes for 600 object classes, and 375k visual relationship annotations involving 57 classes. For object detection in particular, we provide 15x more bounding boxes than the next largest datasets (15.4M boxes on 1.9M images). The images often show complex scenes with several objects (8 annotated objects per image on average). We annotated visual relationships between them, which support visual relationship detection, an emerging task that requires structured reasoning. We provide in-depth comprehensive statistics about the dataset, we validate the quality of the annotations, we study how the performance of several modern models evolves with increasing amounts of training data, and we demonstrate two applications made possible by having unified annotations of multiple types coexisting in the same images. We hope that the scale, quality, and variety of Open Images V4 will foster further research and innovation even beyond the areas of image classification, object detection, and visual relationship detection.},
	number = {7},
	urldate = {2022-11-11},
	journal = {International Journal of Computer Vision},
	author = {Kuznetsova, Alina and Rom, Hassan and Alldrin, Neil and Uijlings, Jasper and Krasin, Ivan and Pont-Tuset, Jordi and Kamali, Shahab and Popov, Stefan and Malloci, Matteo and Kolesnikov, Alexander and Duerig, Tom and Ferrari, Vittorio},
	month = jul,
	year = {2020},
	note = {arXiv:1811.00982 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Dataset, \#NONE},
	pages = {1956--1981},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\SG2DVMN9\\Kuznetsova et al. - 2020 - The Open Images Dataset V4 Unified image classifi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\RC8K4YPD\\1811.html:text/html},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2022-11-11},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {\#NONE, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\DQ8LYCPH\\Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\QP28779T\\2005.html:text/html},
}

@misc{alayrac_flamingo_2022,
	title = {Flamingo: a {Visual} {Language} {Model} for {Few}-{Shot} {Learning}},
	shorttitle = {Flamingo},
	url = {http://arxiv.org/abs/2204.14198},
	doi = {10.48550/arXiv.2204.14198},
	urldate = {2022-11-11},
	publisher = {arXiv},
	author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andrew and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},
	month = apr,
	year = {2022},
	note = {arXiv:2204.14198 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2022-11-11},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {\#NONE, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\RJIDBWRV\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\Q26NLI88\\1810.html:text/html},
}

@misc{krishna_visual_2016,
	title = {Visual {Genome}: {Connecting} {Language} and {Vision} {Using} {Crowdsourced} {Dense} {Image} {Annotations}},
	shorttitle = {Visual {Genome}},
	url = {http://arxiv.org/abs/1602.07332},
	doi = {10.48550/arXiv.1602.07332},
	abstract = {Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked "What vehicle is the person riding?", computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) in order to answer correctly that "the person is riding a horse-drawn carriage". In this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense annotations of objects, attributes, and relationships within each image to learn these models. Specifically, our dataset contains over 100K images where each image has an average of 21 objects, 18 attributes, and 18 pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases in region descriptions and questions answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answers.},
	urldate = {2022-11-11},
	publisher = {arXiv},
	author = {Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A. and Bernstein, Michael S. and Li, Fei-Fei},
	month = feb,
	year = {2016},
	note = {arXiv:1602.07332 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Dataset, \#NONE, Visual Genome},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\DDKX6TFQ\\Krishna et al. - 2016 - Visual Genome Connecting Language and Vision Usin.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\D7ASZIT7\\1602.html:text/html},
}

@misc{holtzman_curious_2020,
	title = {The {Curious} {Case} of {Neural} {Text} {Degeneration}},
	url = {http://arxiv.org/abs/1904.09751},
	abstract = {Despite considerable advances in neural language modeling, it remains an open question what the best decoding strategy is for text generation from a language model (e.g. to generate a story). The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, maximization-based decoding methods such as beam search lead to degeneration — output text that is bland, incoherent, or gets stuck in repetitive loops.},
	language = {en},
	urldate = {2022-11-11},
	publisher = {arXiv},
	author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
	month = feb,
	year = {2020},
	note = {arXiv:1904.09751 [cs]},
	keywords = {\#NONE, Computer Science - Computation and Language},
	file = {Holtzman et al. - 2020 - The Curious Case of Neural Text Degeneration.pdf:C\:\\Users\\felix\\Zotero\\storage\\UYKGDZ4I\\Holtzman et al. - 2020 - The Curious Case of Neural Text Degeneration.pdf:application/pdf},
}

@misc{tan_efficientnet_2020,
	title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
	shorttitle = {{EfficientNet}},
	url = {http://arxiv.org/abs/1905.11946},
	abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a ﬁxed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefﬁcient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.},
	language = {en},
	urldate = {2022-11-12},
	publisher = {arXiv},
	author = {Tan, Mingxing and Le, Quoc V.},
	month = sep,
	year = {2020},
	note = {arXiv:1905.11946 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Tan and Le - 2020 - EfficientNet Rethinking Model Scaling for Convolu.pdf:C\:\\Users\\felix\\Zotero\\storage\\WWZV3DKU\\Tan and Le - 2020 - EfficientNet Rethinking Model Scaling for Convolu.pdf:application/pdf},
}

@misc{russakovsky_imagenet_2015,
	title = {{ImageNet} {Large} {Scale} {Visual} {Recognition} {Challenge}},
	url = {http://arxiv.org/abs/1409.0575},
	doi = {10.48550/arXiv.1409.0575},
	abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
	urldate = {2022-11-12},
	publisher = {arXiv},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	month = jan,
	year = {2015},
	note = {arXiv:1409.0575 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#NONE, I.4.8, I.5.2},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\PJWMZCRT\\Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\FFDXMCFT\\1409.html:text/html},
}

@misc{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	urldate = {2022-11-12},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Machine Learning, \#NONE, Computer Science - Computation and Language},
}

@misc{loshchilov_decoupled_2019,
	title = {Decoupled {Weight} {Decay} {Regularization}},
	url = {http://arxiv.org/abs/1711.05101},
	doi = {10.48550/arXiv.1711.05101},
	urldate = {2022-11-12},
	publisher = {arXiv},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = jan,
	year = {2019},
	note = {arXiv:1711.05101 [cs, math]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, \#NONE, Mathematics - Optimization and Control},
}

@inproceedings{su_crowdsourcing_2012,
	title = {Crowdsourcing {Annotations} for {Visual} {Object} {Detection}},
	booktitle = {{HCOMP}@{AAAI}},
	author = {Su, Hao and Deng, Jia and Fei-Fei, Li},
	year = {2012},
	keywords = {\#NONE},
	file = {Su et al. - Crowdsourcing Annotations for Visual Object Detect.pdf:C\:\\Users\\felix\\Zotero\\storage\\YYEA9LQ7\\Su et al. - Crowdsourcing Annotations for Visual Object Detect.pdf:application/pdf},
}

@inproceedings{valderrama_towards_2022,
	address = {Berlin, Heidelberg},
	title = {Towards {Holistic} {Surgical} {Scene} {Understanding}},
	isbn = {978-3-031-16448-4},
	url = {https://doi.org/10.1007/978-3-031-16449-1_42},
	doi = {10.1007/978-3-031-16449-1_42},
	abstract = {Most benchmarks for studying surgical interventions focus on a specific challenge instead of leveraging the intrinsic complementarity among different tasks. In this work, we present a new experimental framework towards holistic surgical scene understanding. First, we introduce the Phase, Step, Instrument, and Atomic Visual Action recognition (PSI-AVA) Dataset. PSI-AVA includes annotations for both long-term (Phase and Step recognition) and short-term reasoning (Instrument detection and novel Atomic Action recognition) in robot-assisted radical prostatectomy videos. Second, we present Transformers for Action, Phase, Instrument, and steps Recognition (TAPIR) as a strong baseline for surgical scene understanding. TAPIR leverages our dataset’s multi-level annotations as it benefits from the learned representation on the instrument detection task to improve its classification capacity. Our experimental results in both PSI-AVA and other publicly available databases demonstrate the adequacy of our framework to spur future research on holistic surgical scene understanding.},
	urldate = {2022-11-14},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2022: 25th {International} {Conference}, {Singapore}, {September} 18–22, 2022, {Proceedings}, {Part} {VII}},
	publisher = {Springer-Verlag},
	author = {Valderrama, Natalia and Ruiz Puentes, Paola and Hernández, Isabela and Ayobi, Nicolás and Verlyck, Mathilde and Santander, Jessica and Caicedo, Juan and Fernández, Nicolás and Arbeláez, Pablo},
	month = sep,
	year = {2022},
	keywords = {Dataset, \#SKIMMED, Holistic surgical scene understanding, Robot-assisted surgical endoscopy, Vision transformers},
	pages = {442--452},
	file = {Valderrama et al. - 2022 - Towards Holistic Surgical Scene Understanding.pdf:C\:\\Users\\felix\\Zotero\\storage\\FZDJ6CEJ\\Valderrama et al. - 2022 - Towards Holistic Surgical Scene Understanding.pdf:application/pdf},
}

@misc{fathollahi_video-based_2022,
	title = {Video-based {Surgical} {Skills} {Assessment} using {Long} term {Tool} {Tracking}},
	url = {http://arxiv.org/abs/2207.02247},
	doi = {10.48550/arXiv.2207.02247},
	abstract = {Mastering the technical skills required to perform surgery is an extremely challenging task. Video-based assessment allows surgeons to receive feedback on their technical skills to facilitate learning and development. Currently, this feedback comes primarily from manual video review, which is time-intensive and limits the feasibility of tracking a surgeon's progress over many cases. In this work, we introduce a motion-based approach to automatically assess surgical skills from surgical case video feed. The proposed pipeline first tracks surgical tools reliably to create motion trajectories and then uses those trajectories to predict surgeon technical skill levels. The tracking algorithm employs a simple yet effective re-identification module that improves ID-switch compared to other state-of-the-art methods. This is critical for creating reliable tool trajectories when instruments regularly move on- and off-screen or are periodically obscured. The motion-based classification model employs a state-of-the-art self-attention transformer network to capture short- and long-term motion patterns that are essential for skill evaluation. The proposed method is evaluated on an in-vivo (Cholec80) dataset where an expert-rated GOALS skill assessment of the Calot Triangle Dissection is used as a quantitative skill measure. We compare transformer-based skill assessment with traditional machine learning approaches using the proposed and state-of-the-art tracking. Our result suggests that using motion trajectories from reliable tracking methods is beneficial for assessing surgeon skills based solely on video streams.},
	urldate = {2022-11-15},
	publisher = {arXiv},
	author = {Fathollahi, Mona and Sarhan, Mohammad Hasan and Pena, Ramon and DiMonte, Lela and Gupta, Anshu and Ataliwala, Aishani and Barker, Jocelyn},
	month = jul,
	year = {2022},
	note = {arXiv:2207.02247 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#SKIMMED},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\2N47FAGJ\\Fathollahi et al. - 2022 - Video-based Surgical Skills Assessment using Long .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\2KJMPJ6L\\2207.html:text/html},
}

@article{khan_global_2021,
	title = {A global review of publicly available datasets for ophthalmological imaging: barriers to access, usability, and generalisability},
	volume = {3},
	issn = {2589-7500},
	shorttitle = {A global review of publicly available datasets for ophthalmological imaging},
	url = {https://www.thelancet.com/journals/landig/article/PIIS2589-7500(20)30240-5/fulltext},
	doi = {10.1016/S2589-7500(20)30240-5},
	language = {English},
	number = {1},
	urldate = {2022-11-17},
	journal = {The Lancet Digital Health},
	author = {Khan, Saad M. and Liu, Xiaoxuan and Nath, Siddharth and Korot, Edward and Faes, Livia and Wagner, Siegfried K. and Keane, Pearse A. and Sebire, Neil J. and Burton, Matthew J. and Denniston, Alastair K.},
	month = jan,
	year = {2021},
	note = {Publisher: Elsevier},
	keywords = {Dataset, \#SKIMMED, Review},
	pages = {e51--e66},
	file = {Full Text:C\:\\Users\\felix\\Zotero\\storage\\N7GDRTF3\\Khan et al. - 2021 - A global review of publicly available datasets for.pdf:application/pdf;Snapshot:C\:\\Users\\felix\\Zotero\\storage\\7GU4RP3P\\fulltext.html:text/html},
}

@misc{fan_multiscale_2021,
	title = {Multiscale {Vision} {Transformers}},
	url = {http://arxiv.org/abs/2104.11227},
	doi = {10.48550/arXiv.2104.11227},
	abstract = {We present Multiscale Vision Transformers (MViT) for video and image recognition, by connecting the seminal idea of multiscale feature hierarchies with transformer models. Multiscale Transformers have several channel-resolution scale stages. Starting from the input resolution and a small channel dimension, the stages hierarchically expand the channel capacity while reducing the spatial resolution. This creates a multiscale pyramid of features with early layers operating at high spatial resolution to model simple low-level visual information, and deeper layers at spatially coarse, but complex, high-dimensional features. We evaluate this fundamental architectural prior for modeling the dense nature of visual signals for a variety of video recognition tasks where it outperforms concurrent vision transformers that rely on large scale external pre-training and are 5-10x more costly in computation and parameters. We further remove the temporal dimension and apply our model for image classification where it outperforms prior work on vision transformers. Code is available at: https://github.com/facebookresearch/SlowFast},
	urldate = {2022-11-30},
	publisher = {arXiv},
	author = {Fan, Haoqi and Xiong, Bo and Mangalam, Karttikeya and Li, Yanghao and Yan, Zhicheng and Malik, Jitendra and Feichtenhofer, Christoph},
	month = apr,
	year = {2021},
	note = {arXiv:2104.11227 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, \#SKIMMED, \#VidSGG},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\29TTD9B4\\Fan et al. - 2021 - Multiscale Vision Transformers.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\IDVIP2VM\\2104.html:text/html},
}

@inproceedings{mangalam_reversible_2022,
	address = {New Orleans, LA, USA},
	title = {Reversible {Vision} {Transformers}},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9879039/},
	doi = {10.1109/CVPR52688.2022.01056},
	abstract = {We present Reversible Vision Transformers, a memory efficient architecture design for visual recognition. By decoupling the GPU memory footprint from the depth of the model, Reversible Vision Transformers enable memory efficient scaling of transformer architectures. We adapt two popular models, namely Vision Transformer and Multiscale Vision Transformers, to reversible variants and benchmark extensively across both model sizes and tasks of image classification, object detection and video classification. Reversible Vision Transformers achieve a reduced memory footprint of up to 15.5× at identical model complexity, parameters and accuracy, demonstrating the promise of reversible vision transformers as an efficient backbone for resource limited training regimes. Finally, we find that the additional computational burden of recomputing activations is more than overcome for deeper models, where throughput can increase up to 3.9× over their non-reversible counterparts. Code and models are available at https:// github.com/facebookresearch/mvit.},
	language = {en},
	urldate = {2022-11-30},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Mangalam, Karttikeya and Fan, Haoqi and Li, Yanghao and Wu, Chao-Yuan and Xiong, Bo and Feichtenhofer, Christoph and Malik, Jitendra},
	month = jun,
	year = {2022},
	keywords = {\#SKIMMED, \#VidSGG},
	pages = {10820--10830},
	file = {Mangalam et al. - 2022 - Reversible Vision Transformers.pdf:C\:\\Users\\felix\\Zotero\\storage\\IA6GAM6Q\\Mangalam et al. - 2022 - Reversible Vision Transformers.pdf:application/pdf},
}

@misc{chen_generalist_2022,
	title = {A {Generalist} {Framework} for {Panoptic} {Segmentation} of {Images} and {Videos}},
	url = {http://arxiv.org/abs/2210.06366},
	doi = {10.48550/arXiv.2210.06366},
	abstract = {Panoptic segmentation assigns semantic and instance ID labels to every pixel of an image. As permutations of instance IDs are also valid solutions, the task requires learning of high-dimensional one-to-many mapping. As a result, state-of-the-art approaches use customized architectures and task-specific loss functions. We formulate panoptic segmentation as a discrete data generation problem, without relying on inductive bias of the task. A diffusion model based on analog bits is used to model panoptic masks, with a simple, generic architecture and loss function. By simply adding past predictions as a conditioning signal, our method is capable of modeling video (in a streaming setting) and thereby learns to track object instances automatically. With extensive experiments, we demonstrate that our generalist approach can perform competitively to state-of-the-art specialist methods in similar settings.},
	urldate = {2022-11-30},
	publisher = {arXiv},
	author = {Chen, Ting and Li, Lala and Saxena, Saurabh and Hinton, Geoffrey and Fleet, David J.},
	month = oct,
	year = {2022},
	note = {arXiv:2210.06366 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, \#SKIMMED, Computer Science - Multimedia, \#VidSGG},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\74EIPZIL\\Chen et al. - 2022 - A Generalist Framework for Panoptic Segmentation o.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\RDS3B238\\2210.html:text/html},
}

@inproceedings{melas-kyriazi_deep_2022,
	address = {New Orleans, LA, USA},
	title = {Deep {Spectral} {Methods}: {A} {Surprisingly} {Strong} {Baseline} for {Unsupervised} {Semantic} {Segmentation} and {Localization}},
	isbn = {978-1-66546-946-3},
	shorttitle = {Deep {Spectral} {Methods}},
	url = {https://ieeexplore.ieee.org/document/9880176/},
	doi = {10.1109/CVPR52688.2022.00818},
	language = {en},
	urldate = {2022-11-30},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Melas-Kyriazi, Luke and Rupprecht, Christian and Laina, Iro and Vedaldi, Andrea},
	month = jun,
	year = {2022},
	keywords = {\#TOSKIM, \#SSSGG},
	pages = {8354--8365},
	file = {Melas-Kyriazi et al. - 2022 - Deep Spectral Methods A Surprisingly Strong Basel.pdf:C\:\\Users\\felix\\Zotero\\storage\\UK2MKWJT\\Melas-Kyriazi et al. - 2022 - Deep Spectral Methods A Surprisingly Strong Basel.pdf:application/pdf},
}

@misc{noauthor_surgical_nodate,
	title = {Surgical {Tool} {Datasets} for {Machine} {Learning} {Research}: {A} {Survey} {\textbar} {SpringerLink} - link.springer.com/},
	url = {https://link.springer.com/article/10.1007/s11263-022-01640-6},
	urldate = {2022-11-30},
	keywords = {\#SKIMMED, \#VidSGG, \#SGApp},
	file = {Surgical Tool Datasets for Machine Learning Research\: A Survey | SpringerLink - link.springer.com/:C\:\\Users\\felix\\Zotero\\storage\\VXZ7ICHP\\s11263-022-01640-6.html:text/html},
}

@article{rodrigues_surgical_2022,
	title = {Surgical {Tool} {Datasets} for {Machine} {Learning} {Research}: {A} {Survey}},
	volume = {130},
	issn = {0920-5691, 1573-1405},
	shorttitle = {Surgical {Tool} {Datasets} for {Machine} {Learning} {Research}},
	url = {https://link.springer.com/10.1007/s11263-022-01640-6},
	doi = {10.1007/s11263-022-01640-6},
	abstract = {This paper is a comprehensive survey of datasets for surgical tool detection and related surgical data science and machine learning techniques and algorithms. The survey offers a high level perspective of current research in this area, analyses the taxonomy of approaches adopted by researchers using surgical tool datasets, and addresses key areas of research, such as the datasets used, evaluation metrics applied and deep learning techniques utilised. Our presentation and taxonomy provides a framework that facilitates greater understanding of current work, and highlights the challenges and opportunities for further innovative and useful research.},
	language = {en},
	number = {9},
	urldate = {2022-11-30},
	journal = {International Journal of Computer Vision},
	author = {Rodrigues, Mark and Mayo, Michael and Patros, Panos},
	month = sep,
	year = {2022},
	keywords = {\#TOSKIM, \#SGApp},
	pages = {2222--2248},
	file = {s11263-022-01640-6.pdf:C\:\\Users\\felix\\Zotero\\storage\\JJK8GYQM\\s11263-022-01640-6.pdf:application/pdf},
}

@misc{shi_yolov_2022,
	title = {{YOLOV}: {Making} {Still} {Image} {Object} {Detectors} {Great} at {Video} {Object} {Detection}},
	shorttitle = {{YOLOV}},
	url = {http://arxiv.org/abs/2208.09686},
	doi = {10.48550/arXiv.2208.09686},
	abstract = {Video object detection (VID) is challenging because of the high variation of object appearance as well as the diverse deterioration in some frames. On the positive side, the detection in a certain frame of a video, compared with in a still image, can draw support from other frames. Hence, how to aggregate features across different frames is pivotal to the VID problem. Most of existing aggregation algorithms are customized for two-stage detectors. But, the detectors in this category are usually computationally expensive due to the two-stage nature. This work proposes a simple yet effective strategy to address the above concerns, which spends marginal overheads with significant gains in accuracy. Concretely, different from the traditional two-stage pipeline, we advocate putting the region-level selection after the one-stage detection to avoid processing massive low-quality candidates. Besides, a novel module is constructed to evaluate the relationship between a target frame and its reference ones, and guide the aggregation. Extensive experiments and ablation studies are conducted to verify the efficacy of our design, and reveal its superiority over other state-of-the-art VID approaches in both effectiveness and efficiency. Our YOLOX-based model can achieve promising performance (e.g., 87.5{\textbackslash}\% AP50 at over 30 FPS on the ImageNet VID dataset on a single 2080Ti GPU), making it attractive for large-scale or real-time applications. The implementation is simple, the demo code and models have been made available at https://github.com/YuHengsss/YOLOV .},
	urldate = {2022-11-30},
	publisher = {arXiv},
	author = {Shi, Yuheng and Wang, Naiyan and Guo, Xiaojie},
	month = aug,
	year = {2022},
	note = {arXiv:2208.09686 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#TOREAD, \#VidSGG},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\QKUV7XGE\\Shi et al. - 2022 - YOLOV Making Still Image Object Detectors Great a.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\GWEI9U7B\\2208.html:text/html},
}

@misc{noauthor_tracking_nodate,
	title = {Tracking {Any} {Pixel} in a {Video} – {Machine} {Learning} {Blog} {\textbar} {ML}@{CMU} {\textbar} {Carnegie} {Mellon} {University} - blog.ml.cmu.edu/},
	url = {https://blog.ml.cmu.edu/2022/09/09/tracking-any-pixel-in-a-video/},
	urldate = {2022-11-30},
	keywords = {\#TOSKIM, Tracking, \#SSSGG},
	file = {Tracking Any Pixel in a Video – Machine Learning Blog | ML@CMU | Carnegie Mellon University - blog.ml.cmu.edu/:C\:\\Users\\felix\\Zotero\\storage\\79CUYMWP\\tracking-any-pixel-in-a-video.html:text/html},
}

@misc{ghamsarian_deeppyramid_2022,
	title = {{DeepPyramid}: {Enabling} {Pyramid} {View} and {Deformable} {Pyramid} {Reception} for {Semantic} {Segmentation} in {Cataract} {Surgery} {Videos}},
	shorttitle = {{DeepPyramid}},
	url = {http://arxiv.org/abs/2207.01453},
	doi = {10.48550/arXiv.2207.01453},
	abstract = {Semantic segmentation in cataract surgery has a wide range of applications contributing to surgical outcome enhancement and clinical risk reduction. However, the varying issues in segmenting the different relevant structures in these surgeries make the designation of a unique network quite challenging. This paper proposes a semantic segmentation network, termed DeepPyramid, that can deal with these challenges using three novelties: (1) a Pyramid View Fusion module which provides a varying-angle global view of the surrounding region centering at each pixel position in the input convolutional feature map; (2) a Deformable Pyramid Reception module which enables a wide deformable receptive field that can adapt to geometric transformations in the object of interest; and (3) a dedicated Pyramid Loss that adaptively supervises multi-scale semantic feature maps. Combined, we show that these modules can effectively boost semantic segmentation performance, especially in the case of transparency, deformability, scalability, and blunt edges in objects. We demonstrate that our approach performs at a state-of-the-art level and outperforms a number of existing methods with a large margin (3.66\% overall improvement in intersection over union compared to the best rival approach).},
	urldate = {2022-11-30},
	publisher = {arXiv},
	author = {Ghamsarian, Negin and Taschwer, Mario and Sznitman, Raphael and Schoeffmann, Klaus},
	month = jul,
	year = {2022},
	note = {arXiv:2207.01453 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#TOSKIM, Cataract, \#SGApp},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\BYCHCTFA\\Ghamsarian et al. - 2022 - DeepPyramid Enabling Pyramid View and Deformable .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\L3ELQW9Z\\2207.html:text/html},
}

@misc{yang_panoptic_2022,
	title = {Panoptic {Scene} {Graph} {Generation}},
	url = {http://arxiv.org/abs/2207.11247},
	doi = {10.48550/arXiv.2207.11247},
	abstract = {Existing research addresses scene graph generation (SGG) -- a critical technology for scene understanding in images -- from a detection perspective, i.e., objects are detected using bounding boxes followed by prediction of their pairwise relationships. We argue that such a paradigm causes several problems that impede the progress of the field. For instance, bounding box-based labels in current datasets usually contain redundant classes like hairs, and leave out background information that is crucial to the understanding of context. In this work, we introduce panoptic scene graph generation (PSG), a new problem task that requires the model to generate a more comprehensive scene graph representation based on panoptic segmentations rather than rigid bounding boxes. A high-quality PSG dataset, which contains 49k well-annotated overlapping images from COCO and Visual Genome, is created for the community to keep track of its progress. For benchmarking, we build four two-stage baselines, which are modified from classic methods in SGG, and two one-stage baselines called PSGTR and PSGFormer, which are based on the efficient Transformer-based detector, i.e., DETR. While PSGTR uses a set of queries to directly learn triplets, PSGFormer separately models the objects and relations in the form of queries from two Transformer decoders, followed by a prompting-like relation-object matching mechanism. In the end, we share insights on open challenges and future directions.},
	urldate = {2022-11-30},
	publisher = {arXiv},
	author = {Yang, Jingkang and Ang, Yi Zhe and Guo, Zujin and Zhou, Kaiyang and Zhang, Wayne and Liu, Ziwei},
	month = jul,
	year = {2022},
	note = {arXiv:2207.11247 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, \#TOSKIM, Segmentation, SGG, Computer Science - Computation and Language, Computer Science - Multimedia},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\4DFB76LY\\Yang et al. - 2022 - Panoptic Scene Graph Generation.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\Q3NE2ACX\\2207.html:text/html},
}

@misc{yan_temporally_2022,
	title = {Temporally {Consistent} {Video} {Transformer} for {Long}-{Term} {Video} {Prediction}},
	url = {http://arxiv.org/abs/2210.02396},
	doi = {10.48550/arXiv.2210.02396},
	abstract = {Generating long, temporally consistent video remains an open challenge in video generation. Primarily due to computational limitations, most prior methods limit themselves to training on a small subset of frames that are then extended to generate longer videos through a sliding window fashion. Although these techniques may produce sharp videos, they have difficulty retaining long-term temporal consistency due to their limited context length. In this work, we present Temporally Consistent Video Transformer (TECO), a vector-quantized latent dynamics video prediction model that learns compressed representations to efficiently condition on long videos of hundreds of frames during both training and generation. We use a MaskGit prior for dynamics prediction which enables both sharper and faster generations compared to prior work. Our experiments show that TECO outperforms SOTA baselines in a variety of video prediction benchmarks ranging from simple mazes in DMLab, large 3D worlds in Minecraft, and complex real-world videos from Kinetics-600. In addition, to better understand the capabilities of video prediction models in modeling temporal consistency, we introduce several challenging video prediction tasks consisting of agents randomly traversing 3D scenes of varying difficulty. This presents a challenging benchmark for video prediction in partially observable environments where a model must understand what parts of the scenes to re-create versus invent depending on its past observations or generations. Generated videos are available at https://wilson1yan.github.io/teco},
	urldate = {2022-11-30},
	publisher = {arXiv},
	author = {Yan, Wilson and Hafner, Danijar and James, Stephen and Abbeel, Pieter},
	month = oct,
	year = {2022},
	note = {arXiv:2210.02396 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, \#SKIMMED, \#VidSGG},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\KI7BBDYK\\Yan et al. - 2022 - Temporally Consistent Video Transformer for Long-T.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\K5RHPNLQ\\2210.html:text/html},
}

@misc{doersch_tap-vid_2022,
	title = {{TAP}-{Vid}: {A} {Benchmark} for {Tracking} {Any} {Point} in a {Video}},
	shorttitle = {{TAP}-{Vid}},
	url = {http://arxiv.org/abs/2211.03726},
	doi = {10.48550/arXiv.2211.03726},
	abstract = {Generic motion understanding from video involves not only tracking objects, but also perceiving how their surfaces deform and move. This information is useful to make inferences about 3D shape, physical properties and object interactions. While the problem of tracking arbitrary physical points on surfaces over longer video clips has received some attention, no dataset or benchmark for evaluation existed, until now. In this paper, we first formalize the problem, naming it tracking any point (TAP). We introduce a companion benchmark, TAP-Vid, which is composed of both real-world videos with accurate human annotations of point tracks, and synthetic videos with perfect ground-truth point tracks. Central to the construction of our benchmark is a novel semi-automatic crowdsourced pipeline which uses optical flow estimates to compensate for easier, short-term motion like camera shake, allowing annotators to focus on harder sections of video. We validate our pipeline on synthetic data and propose a simple end-to-end point tracking model TAP-Net, showing that it outperforms all prior methods on our benchmark when trained on synthetic data.},
	urldate = {2022-11-30},
	publisher = {arXiv},
	author = {Doersch, Carl and Gupta, Ankush and Markeeva, Larisa and Recasens, Adrià and Smaira, Lucas and Aytar, Yusuf and Carreira, João and Zisserman, Andrew and Yang, Yi},
	month = nov,
	year = {2022},
	note = {arXiv:2211.03726 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning, \#TOSKIM, Tracking, \#SSSGG},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\B2GWFNCB\\Doersch et al. - 2022 - TAP-Vid A Benchmark for Tracking Any Point in a V.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\TRD5WGIP\\2211.html:text/html},
}

@article{guzman-garcia_automatic_2022,
	title = {Automatic {Assessment} of {Procedural} {Skills} {Based} on the {Surgical} {Workflow} {Analysis} {Derived} from {Speech} and {Video}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2306-5354},
	url = {https://www.mdpi.com/2306-5354/9/12/753},
	doi = {10.3390/bioengineering9120753},
	abstract = {Automatic surgical workflow analysis (SWA) plays an important role in the modelling of surgical processes. Current automatic approaches for SWA use videos (with accuracies varying from 0.8 and 0.9), but they do not incorporate speech (inherently linked to the ongoing cognitive process). The approach followed in this study uses both video and speech to classify the phases of laparoscopic cholecystectomy, based on neural networks and machine learning. The automatic application implemented in this study uses this information to calculate the total time spent in surgery, the time spent in each phase, the number of occurrences, the minimal, maximal and average time whenever there is more than one occurrence, the timeline of the surgery and the transition probability between phases. This information can be used as an assessment method for surgical procedural skills.},
	language = {en},
	number = {12},
	urldate = {2022-12-08},
	journal = {Bioengineering},
	author = {Guzmán-García, Carmen and Sánchez-González, Patricia and Oropesa, Ignacio and Gómez, Enrique J.},
	month = dec,
	year = {2022},
	note = {Number: 12
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {\#TOSKIM, artificial intelligence, procedural skills, skills’ assessment, surgical training},
	pages = {753},
	file = {Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\INC3S4NH\\Guzmán-García et al. - 2022 - Automatic Assessment of Procedural Skills Based on.pdf:application/pdf;Snapshot:C\:\\Users\\felix\\Zotero\\storage\\8ZXN2WIB\\753.html:text/html},
}

@misc{barak_uneasy_2022,
	title = {The uneasy relationship between deep learning and (classical) statistics},
	url = {https://windowsontheory.org/2022/06/20/the-uneasy-relationship-between-deep-learning-and-classical-statistics/},
	abstract = {An often-expressed sentiment is that deep learning (and machine learning in general) is “simply statistics,” in the sense that it uses different words to describe the same concepts statisticians ha…},
	language = {en},
	urldate = {2022-12-08},
	journal = {Windows On Theory},
	author = {Barak, {\textasciitilde} Boaz},
	month = jun,
	year = {2022},
	keywords = {\#TOREAD},
	file = {Snapshot:C\:\\Users\\felix\\Zotero\\storage\\MHRM9HJV\\the-uneasy-relationship-between-deep-learning-and-classical-statistics.html:text/html},
}

@misc{sorscher_beyond_2022,
	title = {Beyond neural scaling laws: beating power law scaling via data pruning},
	shorttitle = {Beyond neural scaling laws},
	url = {http://arxiv.org/abs/2206.14486},
	doi = {10.48550/arXiv.2206.14486},
	abstract = {Widely observed neural scaling laws, in which error falls off as a power of the training set size, model size, or both, have driven substantial performance improvements in deep learning. However, these improvements through scaling alone require considerable costs in compute and energy. Here we focus on the scaling of error with dataset size and show how in theory we can break beyond power law scaling and potentially even reduce it to exponential scaling instead if we have access to a high-quality data pruning metric that ranks the order in which training examples should be discarded to achieve any pruned dataset size. We then test this improved scaling prediction with pruned dataset size empirically, and indeed observe better than power law scaling in practice on ResNets trained on CIFAR-10, SVHN, and ImageNet. Next, given the importance of finding high-quality pruning metrics, we perform the first large-scale benchmarking study of ten different data pruning metrics on ImageNet. We find most existing high performing metrics scale poorly to ImageNet, while the best are computationally intensive and require labels for every image. We therefore developed a new simple, cheap and scalable self-supervised pruning metric that demonstrates comparable performance to the best supervised metrics. Overall, our work suggests that the discovery of good data-pruning metrics may provide a viable path forward to substantially improved neural scaling laws, thereby reducing the resource costs of modern deep learning.},
	urldate = {2022-12-08},
	publisher = {arXiv},
	author = {Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari S.},
	month = nov,
	year = {2022},
	note = {arXiv:2206.14486 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, \#TOSKIM},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\KVPPK6BM\\Sorscher et al. - 2022 - Beyond neural scaling laws beating power law scal.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\F25NQDDI\\2206.html:text/html},
}

@misc{kapoor_leakage_2022,
	title = {Leakage and the {Reproducibility} {Crisis} in {ML}-based {Science}},
	url = {http://arxiv.org/abs/2207.07048},
	doi = {10.48550/arXiv.2207.07048},
	abstract = {The use of machine learning (ML) methods for prediction and forecasting has become widespread across the quantitative sciences. However, there are many known methodological pitfalls, including data leakage, in ML-based science. In this paper, we systematically investigate reproducibility issues in ML-based science. We show that data leakage is indeed a widespread problem and has led to severe reproducibility failures. Specifically, through a survey of literature in research communities that adopted ML methods, we find 17 fields where errors have been found, collectively affecting 329 papers and in some cases leading to wildly overoptimistic conclusions. Based on our survey, we present a fine-grained taxonomy of 8 types of leakage that range from textbook errors to open research problems. We argue for fundamental methodological changes to ML-based science so that cases of leakage can be caught before publication. To that end, we propose model info sheets for reporting scientific claims based on ML models that would address all types of leakage identified in our survey. To investigate the impact of reproducibility errors and the efficacy of model info sheets, we undertake a reproducibility study in a field where complex ML models are believed to vastly outperform older statistical models such as Logistic Regression (LR): civil war prediction. We find that all papers claiming the superior performance of complex ML models compared to LR models fail to reproduce due to data leakage, and complex ML models don't perform substantively better than decades-old LR models. While none of these errors could have been caught by reading the papers, model info sheets would enable the detection of leakage in each case.},
	urldate = {2022-12-08},
	publisher = {arXiv},
	author = {Kapoor, Sayash and Narayanan, Arvind},
	month = jul,
	year = {2022},
	note = {arXiv:2207.07048 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, \#TOSKIM, Statistics - Methodology, General},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\YJ4XKAVK\\Kapoor and Narayanan - 2022 - Leakage and the Reproducibility Crisis in ML-based.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\3PNPSGVJ\\2207.html:text/html},
}

@misc{kassem_federated_2022,
	title = {Federated {Cycling} ({FedCy}): {Semi}-supervised {Federated} {Learning} of {Surgical} {Phases}},
	shorttitle = {Federated {Cycling} ({FedCy})},
	url = {http://arxiv.org/abs/2203.07345},
	doi = {10.48550/arXiv.2203.07345},
	abstract = {Recent advancements in deep learning methods bring computer-assistance a step closer to fulfilling promises of safer surgical procedures. However, the generalizability of such methods is often dependent on training on diverse datasets from multiple medical institutions, which is a restrictive requirement considering the sensitive nature of medical data. Recently proposed collaborative learning methods such as Federated Learning (FL) allow for training on remote datasets without the need to explicitly share data. Even so, data annotation still represents a bottleneck, particularly in medicine and surgery where clinical expertise is often required. With these constraints in mind, we propose FedCy, a federated semi-supervised learning (FSSL) method that combines FL and self-supervised learning to exploit a decentralized dataset of both labeled and unlabeled videos, thereby improving performance on the task of surgical phase recognition. By leveraging temporal patterns in the labeled data, FedCy helps guide unsupervised training on unlabeled data towards learning task-specific features for phase recognition. We demonstrate significant performance gains over state-of-the-art FSSL methods on the task of automatic recognition of surgical phases using a newly collected multi-institutional dataset of laparoscopic cholecystectomy videos. Furthermore, we demonstrate that our approach also learns more generalizable features when tested on data from an unseen domain.},
	urldate = {2022-12-08},
	publisher = {arXiv},
	author = {Kassem, Hasan and Alapatt, Deepak and Mascagni, Pietro and Consortium, AI4SafeChole and Karargyris, Alexandros and Padoy, Nicolas},
	month = mar,
	year = {2022},
	note = {arXiv:2203.07345 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, \#TOSKIM, I.2.10, Semi-Supervised},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\MGNSXLJR\\Kassem et al. - 2022 - Federated Cycling (FedCy) Semi-supervised Federat.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\MWU3479P\\2203.html:text/html},
}

@misc{gruber_better_2022,
	title = {Better {Uncertainty} {Calibration} via {Proper} {Scores} for {Classification} and {Beyond}},
	url = {http://arxiv.org/abs/2203.07835},
	doi = {10.48550/arXiv.2203.07835},
	abstract = {With model trustworthiness being crucial for sensitive real-world applications, practitioners are putting more and more focus on improving the uncertainty calibration of deep neural networks. Calibration errors are designed to quantify the reliability of probabilistic predictions but their estimators are usually biased and inconsistent. In this work, we introduce the framework of proper calibration errors, which relates every calibration error to a proper score and provides a respective upper bound with optimal estimation properties. This relationship can be used to reliably quantify the model calibration improvement. We theoretically and empirically demonstrate the shortcomings of commonly used estimators compared to our approach. Due to the wide applicability of proper scores, this gives a natural extension of recalibration beyond classification.},
	urldate = {2022-12-08},
	publisher = {arXiv},
	author = {Gruber, Sebastian and Buettner, Florian},
	month = sep,
	year = {2022},
	note = {arXiv:2203.07835 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, \#TOSKIM, General},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\8BD66YIB\\Gruber and Buettner - 2022 - Better Uncertainty Calibration via Proper Scores f.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\4GGDI8S5\\2203.html:text/html},
}

@misc{frasca_understanding_2022,
	title = {Understanding and {Extending} {Subgraph} {GNNs} by {Rethinking} {Their} {Symmetries}},
	url = {http://arxiv.org/abs/2206.11140},
	doi = {10.48550/arXiv.2206.11140},
	abstract = {Subgraph GNNs are a recent class of expressive Graph Neural Networks (GNNs) which model graphs as collections of subgraphs. So far, the design space of possible Subgraph GNN architectures as well as their basic theoretical properties are still largely unexplored. In this paper, we study the most prominent form of subgraph methods, which employs node-based subgraph selection policies such as ego-networks or node marking and deletion. We address two central questions: (1) What is the upper-bound of the expressive power of these methods? and (2) What is the family of equivariant message passing layers on these sets of subgraphs?. Our first step in answering these questions is a novel symmetry analysis which shows that modelling the symmetries of node-based subgraph collections requires a significantly smaller symmetry group than the one adopted in previous works. This analysis is then used to establish a link between Subgraph GNNs and Invariant Graph Networks (IGNs). We answer the questions above by first bounding the expressive power of subgraph methods by 3-WL, and then proposing a general family of message-passing layers for subgraph methods that generalises all previous node-based Subgraph GNNs. Finally, we design a novel Subgraph GNN dubbed SUN, which theoretically unifies previous architectures while providing better empirical performance on multiple benchmarks.},
	urldate = {2022-12-08},
	publisher = {arXiv},
	author = {Frasca, Fabrizio and Bevilacqua, Beatrice and Bronstein, Michael M. and Maron, Haggai},
	month = oct,
	year = {2022},
	note = {arXiv:2206.11140 [cs]},
	keywords = {Computer Science - Machine Learning, \#TOSKIM, General},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\3VA8QQUE\\Frasca et al. - 2022 - Understanding and Extending Subgraph GNNs by Rethi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\NH95EYSM\\2206.html:text/html},
}

@incollection{martel_automatic_2020,
	address = {Cham},
	title = {Automatic {Operating} {Room} {Surgical} {Activity} {Recognition} for {Robot}-{Assisted} {Surgery}},
	volume = {12263},
	isbn = {978-3-030-59715-3 978-3-030-59716-0},
	url = {https://link.springer.com/10.1007/978-3-030-59716-0_37},
	abstract = {Automatic recognition of surgical activities in the operating room (OR) is a key technology for creating next generation intelligent surgical devices and workﬂow monitoring/support systems. Such systems can potentially enhance eﬃciency in the OR, resulting in lower costs and improved care delivery to the patients. In this paper, we investigate automatic surgical activity recognition in robot-assisted operations. We collect the ﬁrst large-scale dataset including 400 full-length multiperspective videos from a variety of robotic surgery cases captured using Time-of-Flight cameras. We densely annotate the videos with 10 most recognized and clinically relevant classes of activities. Furthermore, we investigate state-of-the-art computer vision action recognition techniques and adapt them for the OR environment and the dataset. First, we ﬁnetune the Inﬂated 3D ConvNet (I3D) for clip-level activity recognition on our dataset and use it to extract features from the videos. These features are then fed to a stack of 3 Temporal Gaussian Mixture layers which extracts context from neighboring clips, and eventually go through a Long Short Term Memory network to learn the order of activities in full-length videos. We extensively assess the model and reach a peak performance of ∼88\% mean Average Precision.},
	language = {en},
	urldate = {2022-12-13},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2020},
	publisher = {Springer International Publishing},
	author = {Sharghi, Aidean and Haugerud, Helene and Oh, Daniel and Mohareri, Omid},
	editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
	year = {2020},
	doi = {10.1007/978-3-030-59716-0_37},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Dataset},
	pages = {385--395},
	file = {Sharghi et al. - 2020 - Automatic Operating Room Surgical Activity Recogni.pdf:C\:\\Users\\felix\\Zotero\\storage\\DLCUPJA8\\Sharghi et al. - 2020 - Automatic Operating Room Surgical Activity Recogni.pdf:application/pdf},
}

@article{maier-hein_heidelberg_2021,
	title = {Heidelberg colorectal data set for surgical data science in the sensor operating room},
	volume = {8},
	copyright = {2021 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-021-00882-2},
	doi = {10.1038/s41597-021-00882-2},
	abstract = {Image-based tracking of medical instruments is an integral part of surgical data science applications. Previous research has addressed the tasks of detecting, segmenting and tracking medical instruments based on laparoscopic video data. However, the proposed methods still tend to fail when applied to challenging images and do not generalize well to data they have not been trained on. This paper introduces the Heidelberg Colorectal (HeiCo) data set - the first publicly available data set enabling comprehensive benchmarking of medical instrument detection and segmentation algorithms with a specific emphasis on method robustness and generalization capabilities. Our data set comprises 30 laparoscopic videos and corresponding sensor data from medical devices in the operating room for three different types of laparoscopic surgery. Annotations include surgical phase labels for all video frames as well as information on instrument presence and corresponding instance-wise segmentation masks for surgical instruments (if any) in more than 10,000 individual frames. The data has successfully been used to organize international competitions within the Endoscopic Vision Challenges 2017 and 2019.},
	language = {en},
	number = {1},
	urldate = {2022-12-13},
	journal = {Scientific Data},
	author = {Maier-Hein, Lena and Wagner, Martin and Ross, Tobias and Reinke, Annika and Bodenstedt, Sebastian and Full, Peter M. and Hempe, Hellena and Mindroc-Filimon, Diana and Scholz, Patrick and Tran, Thuy Nuong and Bruno, Pierangela and Kisilenko, Anna and Müller, Benjamin and Davitashvili, Tornike and Capek, Manuela and Tizabi, Minu D. and Eisenmann, Matthias and Adler, Tim J. and Gröhl, Janek and Schellenberg, Melanie and Seidlitz, Silvia and Lai, T. Y. Emmy and Pekdemir, Bünyamin and Roethlingshoefer, Veith and Both, Fabian and Bittel, Sebastian and Mengler, Marc and Mündermann, Lars and Apitz, Martin and Kopp-Schneider, Annette and Speidel, Stefanie and Nickel, Felix and Probst, Pascal and Kenngott, Hannes G. and Müller-Stich, Beat P.},
	month = apr,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Dataset, \#NONE, HeiCo, Endoscopy, Medical imaging},
	pages = {101},
	file = {Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\37ETXAYA\\Maier-Hein et al. - 2021 - Heidelberg colorectal data set for surgical data s.pdf:application/pdf;Snapshot:C\:\\Users\\felix\\Zotero\\storage\\TECLPGAV\\s41597-021-00882-2.html:text/html},
}

@misc{zisimopoulos_deepphase_2018,
	title = {{DeepPhase}: {Surgical} {Phase} {Recognition} in {CATARACTS} {Videos}},
	shorttitle = {{DeepPhase}},
	url = {http://arxiv.org/abs/1807.10565},
	abstract = {Automated surgical workﬂow analysis and understanding can assist surgeons to standardize procedures and enhance post-surgical assessment and indexing, as well as, interventional monitoring. Computerassisted interventional (CAI) systems based on video can perform workﬂow estimation through surgical instruments’ recognition while linking them to an ontology of procedural phases. In this work, we adopt a deep learning paradigm to detect surgical instruments in cataract surgery videos which in turn feed a surgical phase inference recurrent network that encodes temporal aspects of phase steps within the phase classiﬁcation. Our models present comparable to state-of-the-art results for surgical tool detection and phase recognition with accuracies of 99 and 78\% respectively.},
	language = {en},
	urldate = {2022-12-22},
	publisher = {arXiv},
	author = {Zisimopoulos, Odysseas and Flouty, Evangello and Luengo, Imanol and Giataganas, Petros and Nehme, Jean and Chow, Andre and Stoyanov, Danail},
	month = jul,
	year = {2018},
	note = {arXiv:1807.10565 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Zisimopoulos et al. - 2018 - DeepPhase Surgical Phase Recognition in CATARACTS.pdf:C\:\\Users\\felix\\Zotero\\storage\\RKL5N3V3\\Zisimopoulos et al. - 2018 - DeepPhase Surgical Phase Recognition in CATARACTS.pdf:application/pdf},
}

@article{al_hajj_cataracts_2019,
	title = {{CATARACTS}: {Challenge} on automatic tool annotation for {cataRACT} surgery},
	volume = {52},
	issn = {13618415},
	shorttitle = {{CATARACTS}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S136184151830865X},
	doi = {10.1016/j.media.2018.11.008},
	abstract = {Surgical tool detection is attracting increasing attention from the medical image analysis community. The goal generally is not to precisely locate tools in images, but rather to indicate which tools are being used by the surgeon at each instant. The main motivation for annotating tool usage is to design eﬃcient solutions for surgical workﬂow analysis, with potential applications in report generation, surgical training and even real-time decision support. Most existing tool annotation algorithms focus on laparoscopic surgeries. However, with 19 million interventions per year, the most common surgical procedure in the world is cataract surgery. The CATARACTS challenge was organized in 2017 to evaluate tool annotation algorithms in the speciﬁc context of cataract surgery. It relies on more than nine hours of videos, from 50 cataract surgeries, in which the presence of 21 surgical tools was manually annotated by two experts. With 14 participating teams, this challenge can be considered a success. As might be expected, the submitted solutions are based on deep learning. This paper thoroughly evaluates these solutions: in particular, the quality of their annotations are compared to that of human interpretations. Next, lessons learnt from the diﬀerential analysis of these solutions are discussed. We expect that they will guide the design of eﬃcient surgery monitoring tools in the near future.},
	language = {en},
	urldate = {2023-01-20},
	journal = {Medical Image Analysis},
	author = {Al Hajj, Hassan and Lamard, Mathieu and Conze, Pierre-Henri and Roychowdhury, Soumali and Hu, Xiaowei and Maršalkaitė, Gabija and Zisimopoulos, Odysseas and Dedmari, Muneer Ahmad and Zhao, Fenqiang and Prellberg, Jonas and Sahu, Manish and Galdran, Adrian and Araújo, Teresa and Vo, Duc My and Panda, Chandan and Dahiya, Navdeep and Kondo, Satoshi and Bian, Zhengbing and Vahdat, Arash and Bialopetravičius, Jonas and Flouty, Evangello and Qiu, Chenhui and Dill, Sabrina and Mukhopadhyay, Anirban and Costa, Pedro and Aresta, Guilherme and Ramamurthy, Senthil and Lee, Sang-Woong and Campilho, Aurélio and Zachow, Stefan and Xia, Shunren and Conjeti, Sailesh and Stoyanov, Danail and Armaitis, Jogundas and Heng, Pheng-Ann and Macready, William G. and Cochener, Béatrice and Quellec, Gwenolé},
	month = feb,
	year = {2019},
	pages = {24--41},
	file = {Al Hajj et al. - 2019 - CATARACTS Challenge on automatic tool annotation .pdf:C\:\\Users\\felix\\Zotero\\storage\\8NXKLWUH\\Al Hajj et al. - 2019 - CATARACTS Challenge on automatic tool annotation .pdf:application/pdf},
}

@misc{wang_beyond_2023,
	title = {Beyond {Graph} {Convolutional} {Network}: {An} {Interpretable} {Regularizer}-centered {Optimization} {Framework}},
	shorttitle = {Beyond {Graph} {Convolutional} {Network}},
	url = {http://arxiv.org/abs/2301.04318},
	doi = {10.48550/arXiv.2301.04318},
	abstract = {Graph convolutional networks (GCNs) have been attracting widespread attentions due to their encouraging performance and powerful generalizations. However, few work provide a general view to interpret various GCNs and guide GCNs' designs. In this paper, by revisiting the original GCN, we induce an interpretable regularizer-centerd optimization framework, in which by building appropriate regularizers we can interpret most GCNs, such as APPNP, JKNet, DAGNN, and GNN-LF/HF. Further, under the proposed framework, we devise a dual-regularizer graph convolutional network (dubbed tsGCN) to capture topological and semantic structures from graph data. Since the derived learning rule for tsGCN contains an inverse of a large matrix and thus is time-consuming, we leverage the Woodbury matrix identity and low-rank approximation tricks to successfully decrease the high computational complexity of computing infinite-order graph convolutions. Extensive experiments on eight public datasets demonstrate that tsGCN achieves superior performance against quite a few state-of-the-art competitors w.r.t. classification tasks.},
	urldate = {2023-01-22},
	publisher = {arXiv},
	author = {Wang, Shiping and Wu, Zhihao and Chen, Yuhong and Chen, Yong},
	month = jan,
	year = {2023},
	note = {arXiv:2301.04318 [cs]
version: 1},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, \#TOSKIM, \#SGApp, 68T07, I.5.1},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\J2UZLTCG\\Wang et al. - 2023 - Beyond Graph Convolutional Network An Interpretab.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\P8LE9QKP\\2301.html:text/html},
}

@misc{murali_latent_2022,
	title = {Latent {Graph} {Representations} for {Critical} {View} of {Safety} {Assessment}},
	url = {http://arxiv.org/abs/2212.04155},
	doi = {10.48550/arXiv.2212.04155},
	abstract = {Assessing the critical view of safety in laparoscopic cholecystectomy requires accurate identification and localization of key anatomical structures, reasoning about their geometric relationships to one another, and determining the quality of their exposure. In this work, we propose to capture each of these aspects by modeling the surgical scene with a disentangled latent scene graph representation, which we can then process using a graph neural network. Unlike previous approaches using graph representations, we explicitly encode in our graphs semantic information such as object locations and shapes, class probabilities and visual features. We also incorporate an auxiliary image reconstruction objective to help train the latent graph representations. We demonstrate the value of these components through comprehensive ablation studies and achieve state-of-the-art results for critical view of safety prediction across multiple experimental settings.},
	urldate = {2023-01-22},
	publisher = {arXiv},
	author = {Murali, Aditya and Alapatt, Deepak and Mascagni, Pietro and Vardazaryan, Armine and Garcia, Alain and Okamoto, Nariaki and Mutter, Didier and Padoy, Nicolas},
	month = dec,
	year = {2022},
	note = {arXiv:2212.04155 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#TOREAD, \#SGApp},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\6TQXQNEU\\Murali et al. - 2022 - Latent Graph Representations for Critical View of .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\WTC9RY22\\2212.html:text/html},
}

@misc{pissas_effective_2021,
	title = {Effective semantic segmentation in {Cataract} {Surgery}: {What} matters most?},
	shorttitle = {Effective semantic segmentation in {Cataract} {Surgery}},
	url = {http://arxiv.org/abs/2108.06119},
	doi = {10.48550/arXiv.2108.06119},
	abstract = {Our work proposes neural network design choices that set the state-of-the-art on a challenging public benchmark on cataract surgery, CaDIS. Our methodology achieves strong performance across three semantic segmentation tasks with increasingly granular surgical tool class sets by effectively handling class imbalance, an inherent challenge in any surgical video. We consider and evaluate two conceptually simple data oversampling methods as well as different loss functions. We show significant performance gains across network architectures and tasks especially on the rarest tool classes, thereby presenting an approach for achieving high performance when imbalanced granular datasets are considered. Our code and trained models are available at https://github.com/RViMLab/MICCAI2021\_Cataract\_semantic\_segmentation and qualitative results on unseen surgical video can be found at https://youtu.be/twVIPUj1WZM.},
	urldate = {2023-01-22},
	publisher = {arXiv},
	author = {Pissas, Theodoros and Ravasio, Claudio and Da Cruz, Lyndon and Bergeles, Christos},
	month = aug,
	year = {2021},
	note = {arXiv:2108.06119 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#SKIMMED, \#SGApp},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\UV4CRBBG\\Pissas et al. - 2021 - Effective semantic segmentation in Cataract Surger.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\NDG4ATH2\\2108.html:text/html},
}

@misc{luengo_2020_2022,
	title = {2020 {CATARACTS} {Semantic} {Segmentation} {Challenge}},
	url = {http://arxiv.org/abs/2110.10965},
	doi = {10.48550/arXiv.2110.10965},
	abstract = {Surgical scene segmentation is essential for anatomy and instrument localization which can be further used to assess tissue-instrument interactions during a surgical procedure. In 2017, the Challenge on Automatic Tool Annotation for cataRACT Surgery (CATARACTS) released 50 cataract surgery videos accompanied by instrument usage annotations. These annotations included frame-level instrument presence information. In 2020, we released pixel-wise semantic annotations for anatomy and instruments for 4670 images sampled from 25 videos of the CATARACTS training set. The 2020 CATARACTS Semantic Segmentation Challenge, which was a sub-challenge of the 2020 MICCAI Endoscopic Vision (EndoVis) Challenge, presented three sub-tasks to assess participating solutions on anatomical structure and instrument segmentation. Their performance was assessed on a hidden test set of 531 images from 10 videos of the CATARACTS test set.},
	urldate = {2023-01-22},
	publisher = {arXiv},
	author = {Luengo, Imanol and Grammatikopoulou, Maria and Mohammadi, Rahim and Walsh, Chris and Nwoye, Chinedu Innocent and Alapatt, Deepak and Padoy, Nicolas and Ni, Zhen-Liang and Fan, Chen-Chen and Bian, Gui-Bin and Hou, Zeng-Guang and Ha, Heonjin and Wang, Jiacheng and Wang, Haojie and Guo, Dong and Wang, Lu and Wang, Guotai and Islam, Mobarakol and Giddwani, Bharat and Hongliang, Ren and Pissas, Theodoros and Ravasio, Claudio and Huber, Martin and Birch, Jeremy and Rio, Joan M. Nunez Do and da Cruz, Lyndon and Bergeles, Christos and Chen, Hongyu and Jia, Fucang and KumarTomar, Nikhil and Jha, Debesh and Riegler, Michael A. and Halvorsen, Pal and Bano, Sophia and Vaghela, Uddhav and Hong, Jianyuan and Ye, Haili and Huang, Feihong and Wang, Da-Han and Stoyanov, Danail},
	month = feb,
	year = {2022},
	note = {arXiv:2110.10965 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, \#TOSKIM, \#SGApp, CATARACTS},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\2PZ9P9GV\\Luengo et al. - 2022 - 2020 CATARACTS Semantic Segmentation Challenge.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\ZUAKZNRY\\2110.html:text/html},
}

@misc{noauthor_cataracts2020_nodate,
	title = {{CATARACTS2020} - {Grand} {Challenge}},
	url = {https://cataracts2020.grand-challenge.org/},
	abstract = {Challenge on AuTomatic Activity Recognition for catarACT Surgery},
	language = {en},
	urldate = {2023-01-22},
	journal = {grand-challenge.org},
	keywords = {\#READ},
	file = {Snapshot:C\:\\Users\\felix\\Zotero\\storage\\Y4C9RL7R\\cataracts2020.grand-challenge.org.html:text/html},
}

@misc{noauthor_cataracts_nodate,
	title = {{CATARACTS} - {Grand} {Challenge}},
	url = {https://cataracts.grand-challenge.org/Data/},
	abstract = {The Challenge on Automatic Tool Annotation for cataRACT Surgery aims at evaluating image-based tool detection algorithms in the context of the most common surgical procedure in the world.},
	language = {en},
	urldate = {2023-01-22},
	journal = {grand-challenge.org},
	keywords = {\#READ, CATARACTS},
	file = {Snapshot:C\:\\Users\\felix\\Zotero\\storage\\K38FYUR9\\Data.html:text/html},
}

@misc{noauthor_endovis_nodate,
	title = {{EndoVis} / {CATARACTS} 2018 - {Grand} {Challenge}},
	url = {https://cataracts2018.grand-challenge.org/data/},
	abstract = {Design an algorithm to detect surgical tools in cataract surgery videos.},
	language = {en},
	urldate = {2023-01-22},
	journal = {grand-challenge.org},
	keywords = {\#SKIMMED, \#SGApp, CATARACTS},
	file = {Snapshot:C\:\\Users\\felix\\Zotero\\storage\\96QHJ3HM\\data.html:text/html},
}

@misc{noauthor_artorg_2020,
	title = {{ARTORG} wins {MICCAI} 2020 {Cataract} surgery challenge},
	url = {https://www.artorg.unibe.ch/about_us/news/2020/artorg_wins_miccai_2020_cataract_surgery_challenge/index_eng.html},
	abstract = {Cataract surgery is the most frequent surgery worldwide with a rising tendency. This year’s international challenge on AuTomatic Activity Recognition for catarACT Surgery hosted by MICCAI 2020 aimed to push research in increased safety and efficiency of this intervention. On 4 October 2020, a team from the ARTORG Artificial Intelligence in Medical Imaging (AIMI) lab was ranked first among all challenge competitors with a method recognizing both surgical tools and activity from a microscope video stream.},
	language = {eng},
	urldate = {2023-01-22},
	journal = {ARTORG Center for Biomedical Engineering Research},
	month = oct,
	year = {2020},
	keywords = {\#SKIMMED, \#SGApp, CATARACTS},
	file = {Snapshot:C\:\\Users\\felix\\Zotero\\storage\\WJUIXVM2\\index_eng.html:text/html},
}

@misc{noauthor_cataracts2020_nodate-1,
	title = {{CATARACTS2020} - {Grand} {Challenge}},
	url = {https://cataracts2020.grand-challenge.org/},
	abstract = {Challenge on AuTomatic Activity Recognition for catarACT Surgery},
	language = {en},
	urldate = {2023-01-22},
	journal = {grand-challenge.org},
	keywords = {\#SKIMMED, \#SGApp, CATARACTS},
	file = {Snapshot:C\:\\Users\\felix\\Zotero\\storage\\GFRD79ST\\cataracts2020.grand-challenge.org.html:text/html},
}

@misc{infosagebaseorg_synapse_nodate,
	title = {Synapse {\textbar} {Sage} {Bionetworks}},
	url = {https://www.synapse.org},
	abstract = {Synapse is a platform for supporting scientific collaborations centered around shared biomedical data sets.  Our goal is to make biomedical research more transparent, more reproducible, and more accessible to a broader audience of scientists.  Synapse serves as the host site for a variety of scientific collaborations, individual research projects, and DREAM challenges.},
	language = {en-US},
	urldate = {2023-01-22},
	author = {info@sagebase.org, Sage Bionetworks},
	keywords = {\#SKIMMED, \#SGApp, CATARACTS},
	file = {Snapshot:C\:\\Users\\felix\\Zotero\\storage\\5JXQMGU5\\www.synapse.org.html:text/html},
}

@misc{infosagebaseorg_synapse_nodate-1,
	title = {Synapse {\textbar} {Sage} {Bionetworks}},
	url = {https://www.synapse.org},
	abstract = {Synapse is a platform for supporting scientific collaborations centered around shared biomedical data sets.  Our goal is to make biomedical research more transparent, more reproducible, and more accessible to a broader audience of scientists.  Synapse serves as the host site for a variety of scientific collaborations, individual research projects, and DREAM challenges.},
	language = {en-US},
	urldate = {2023-01-22},
	author = {info@sagebase.org, Sage Bionetworks},
	keywords = {\#SKIMMED, \#SGApp, CATARACTS},
	file = {Snapshot:C\:\\Users\\felix\\Zotero\\storage\\IYAZ546T\\www.synapse.org.html:text/html},
}

@misc{sharma_rendezvous_2022,
	title = {Rendezvous in {Time}: {An} {Attention}-based {Temporal} {Fusion} approach for {Surgical} {Triplet} {Recognition}},
	shorttitle = {Rendezvous in {Time}},
	url = {http://arxiv.org/abs/2211.16963},
	doi = {10.48550/arXiv.2211.16963},
	abstract = {One of the recent advances in surgical AI is the recognition of surgical activities as triplets of (instrument, verb, target). Albeit providing detailed information for computer-assisted intervention, current triplet recognition approaches rely only on single frame features. Exploiting the temporal cues from earlier frames would improve the recognition of surgical action triplets from videos. In this paper, we propose Rendezvous in Time (RiT) - a deep learning model that extends the state-of-the-art model, Rendezvous, with temporal modeling. Focusing more on the verbs, our RiT explores the connectedness of current and past frames to learn temporal attention-based features for enhanced triplet recognition. We validate our proposal on the challenging surgical triplet dataset, CholecT45, demonstrating an improved recognition of the verb and triplet along with other interactions involving the verb such as (instrument, verb). Qualitative results show that the RiT produces smoother predictions for most triplet instances than the state-of-the-arts. We present a novel attention-based approach that leverages the temporal fusion of video frames to model the evolution of surgical actions and exploit their benefits for surgical triplet recognition.},
	urldate = {2023-01-22},
	publisher = {arXiv},
	author = {Sharma, Saurav and Nwoye, Chinedu Innocent and Mutter, Didier and Padoy, Nicolas},
	month = nov,
	year = {2022},
	note = {arXiv:2211.16963 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, \#TOSKIM, CholecT50},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\FCUYA36Y\\Sharma et al. - 2022 - Rendezvous in Time An Attention-based Temporal Fu.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\RXETEVAD\\2211.html:text/html},
}

@inproceedings{ye_linguistic_2021,
	address = {Nashville, TN, USA},
	title = {Linguistic {Structures} as {Weak} {Supervision} for {Visual} {Scene} {Graph} {Generation}},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9578041/},
	doi = {10.1109/CVPR46437.2021.00819},
	abstract = {Prior work in scene graph generation requires categorical supervision at the level of triplets—subjects and objects, and predicates that relate them, either with or without bounding box information. However, scene graph generation is a holistic task: thus holistic, contextual supervision should intuitively improve performance. In this work, we explore how linguistic structures in captions can beneﬁt scene graph generation. Our method captures the information provided in captions about relations between individual triplets, and context for subjects and objects (e.g. visual properties are mentioned). Captions are a weaker type of supervision than triplets since the alignment between the exhaustive list of human-annotated subjects and objects in triplets, and the nouns in captions, is weak. However, given the large and diverse sources of multimodal data on the web (e.g. blog posts with images and captions), linguistic supervision is more scalable than crowdsourced triplets. We show extensive experimental comparisons against prior methods which leverage instance- and image-level supervision, and ablate our method to show the impact of leveraging phrasal and sequential context, and techniques to improve localization of subjects and objects.},
	language = {en},
	urldate = {2023-01-29},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Ye, Keren and Kovashka, Adriana},
	month = jun,
	year = {2021},
	pages = {8285--8295},
	file = {Ye and Kovashka - 2021 - Linguistic Structures as Weak Supervision for Visu.pdf:C\:\\Users\\felix\\Zotero\\storage\\7HRJM36Z\\Ye and Kovashka - 2021 - Linguistic Structures as Weak Supervision for Visu.pdf:application/pdf},
}

@misc{zisimopoulos_deepphase_2018-1,
	title = {{DeepPhase}: {Surgical} {Phase} {Recognition} in {CATARACTS} {Videos}},
	shorttitle = {{DeepPhase}},
	url = {http://arxiv.org/abs/1807.10565},
	abstract = {Automated surgical workﬂow analysis and understanding can assist surgeons to standardize procedures and enhance post-surgical assessment and indexing, as well as, interventional monitoring. Computerassisted interventional (CAI) systems based on video can perform workﬂow estimation through surgical instruments’ recognition while linking them to an ontology of procedural phases. In this work, we adopt a deep learning paradigm to detect surgical instruments in cataract surgery videos which in turn feed a surgical phase inference recurrent network that encodes temporal aspects of phase steps within the phase classiﬁcation. Our models present comparable to state-of-the-art results for surgical tool detection and phase recognition with accuracies of 99 and 78\% respectively.},
	language = {en},
	urldate = {2023-02-03},
	publisher = {arXiv},
	author = {Zisimopoulos, Odysseas and Flouty, Evangello and Luengo, Imanol and Giataganas, Petros and Nehme, Jean and Chow, Andre and Stoyanov, Danail},
	month = jul,
	year = {2018},
	note = {arXiv:1807.10565 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Zisimopoulos et al. - 2018 - DeepPhase Surgical Phase Recognition in CATARACTS.pdf:C\:\\Users\\felix\\Zotero\\storage\\SJ7FHJFS\\Zisimopoulos et al. - 2018 - DeepPhase Surgical Phase Recognition in CATARACTS.pdf:application/pdf},
}

@misc{giannantonio_intra-operative_2023,
	title = {Intra-operative {Brain} {Tumor} {Detection} with {Deep} {Learning}-{Optimized} {Hyperspectral} {Imaging}},
	url = {http://arxiv.org/abs/2302.02884},
	doi = {10.48550/arXiv.2302.02884},
	abstract = {Surgery for gliomas (intrinsic brain tumors), especially when low-grade, is challenging due to the infiltrative nature of the lesion. Currently, no real-time, intra-operative, label-free and wide-field tool is available to assist and guide the surgeon to find the relevant demarcations for these tumors. While marker-based methods exist for the high-grade glioma case, there is no convenient solution available for the low-grade case; thus, marker-free optical techniques represent an attractive option. Although RGB imaging is a standard tool in surgical microscopes, it does not contain sufficient information for tissue differentiation. We leverage the richer information from hyperspectral imaging (HSI), acquired with a snapscan camera in the 468-787 nm range, coupled to a surgical microscope, to build a deep-learning-based diagnostic tool for cancer resection with potential for intra-operative guidance. However, the main limitation of the HSI snapscan camera is the image acquisition time, limiting its widespread deployment in the operation theater. Here, we investigate the effect of HSI channel reduction and pre-selection to scope the design space for the development of cheaper and faster sensors. Neural networks are used to identify the most important spectral channels for tumor tissue differentiation, optimizing the trade-off between the number of channels and precision to enable real-time intra-surgical application. We evaluate the performance of our method on a clinical dataset that was acquired during surgery on five patients. By demonstrating the possibility to efficiently detect low-grade glioma, these results can lead to better cancer resection demarcations, potentially improving treatment effectiveness and patient outcome.},
	urldate = {2023-02-14},
	publisher = {arXiv},
	author = {Giannantonio, Tommaso and Alperovich, Anna and Semeraro, Piercosimo and Atzori, Manfredo and Zhang, Xiaohan and Hauger, Christoph and Freytag, Alexander and Luthman, Siri and Vandebriel, Roeland and Jayapala, Murali and Solie, Lien and de Vleeschouwer, Steven},
	month = feb,
	year = {2023},
	note = {arXiv:2302.02884 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, \#NONE, Hyperspectral},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\ZAD95Q39\\Giannantonio et al. - 2023 - Intra-operative Brain Tumor Detection with Deep Le.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\A5I4RTTR\\2302.html:text/html},
}

@misc{shmueli_matthews_2020,
	title = {Matthews {Correlation} {Coefficient} is {The} {Best} {Classification} {Metric} {You}’ve {Never} {Heard} {Of}},
	url = {https://towardsdatascience.com/the-best-classification-metric-youve-never-heard-of-the-matthews-correlation-coefficient-3bf50a2f3e9a},
	abstract = {Still using accuracy and F1-score? Time to upgrade.},
	language = {en},
	urldate = {2023-02-15},
	journal = {Medium},
	author = {Shmueli, Boaz},
	month = may,
	year = {2020},
	keywords = {\#READ, Metrics},
	file = {Snapshot:C\:\\Users\\felix\\Zotero\\storage\\R4T266AJ\\the-best-classification-metric-youve-never-heard-of-the-matthews-correlation-coefficient-3bf50a.html:text/html},
}

@misc{nwoye_cholectriplet2022_2023,
	title = {{CholecTriplet2022}: {Show} me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection},
	shorttitle = {{CholecTriplet2022}},
	url = {http://arxiv.org/abs/2302.06294},
	doi = {10.48550/arXiv.2302.06294},
	abstract = {Formalizing surgical activities as triplets of the used instruments, actions performed, and target anatomies is becoming a gold standard approach for surgical activity modeling. The benefit is that this formalization helps to obtain a more detailed understanding of tool-tissue interaction which can be used to develop better Artificial Intelligence assistance for image-guided surgery. Earlier efforts and the CholecTriplet challenge introduced in 2021 have put together techniques aimed at recognizing these triplets from surgical footage. Estimating also the spatial locations of the triplets would offer a more precise intraoperative context-aware decision support for computer-assisted intervention. This paper presents the CholecTriplet2022 challenge, which extends surgical action triplet modeling from recognition to detection. It includes weakly-supervised bounding box localization of every visible surgical instrument (or tool), as the key actors, and the modeling of each tool-activity in the form of {\textless}instrument, verb, target{\textgreater} triplet. The paper describes a baseline method and 10 new deep learning algorithms presented at the challenge to solve the task. It also provides thorough methodological comparisons of the methods, an in-depth analysis of the obtained results, their significance, and useful insights for future research directions and applications in surgery.},
	urldate = {2023-03-02},
	publisher = {arXiv},
	author = {Nwoye, Chinedu Innocent and Yu, Tong and Sharma, Saurav and Murali, Aditya and Alapatt, Deepak and Vardazaryan, Armine and Yuan, Kun and Hajek, Jonas and Reiter, Wolfgang and Yamlahi, Amine and Smidt, Finn-Henri and Zou, Xiaoyang and Zheng, Guoyan and Oliveira, Bruno and Torres, Helena R. and Kondo, Satoshi and Kasai, Satoshi and Holm, Felix and Özsoy, Ege and Gui, Shuangchun and Li, Han and Raviteja, Sista and Sathish, Rachana and Poudel, Pranav and Bhattarai, Binod and Wang, Ziheng and Rui, Guo and Schellenberg, Melanie and Vilaça, João L. and Czempiel, Tobias and Wang, Zhenkun and Sheet, Debdoot and Thapa, Shrawan Kumar and Berniker, Max and Godau, Patrick and Morais, Pedro and Regmi, Sudarshan and Tran, Thuy Nuong and Fonseca, Jaime and Nölke, Jan-Hinrich and Lima, Estevão and Vazquez, Eduard and Maier-Hein, Lena and Navab, Nassir and Mascagni, Pietro and Seeliger, Barbara and Gonzalez, Cristians and Mutter, Didier and Padoy, Nicolas},
	month = feb,
	year = {2023},
	note = {arXiv:2302.06294 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, \#SKIMMED, CholecXXX},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\94YWLTHJ\\Nwoye et al. - 2023 - CholecTriplet2022 Show me a tool and tell me the .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\RXZPFHQI\\2302.html:text/html},
}

@inproceedings{li_integrating_2022,
	address = {New York, NY, USA},
	series = {{MM} '22},
	title = {Integrating {Object}-aware and {Interaction}-aware {Knowledge} for {Weakly} {Supervised} {Scene} {Graph} {Generation}},
	isbn = {978-1-4503-9203-7},
	url = {https://doi.org/10.1145/3503161.3548164},
	doi = {10.1145/3503161.3548164},
	abstract = {Recently, increasing efforts have been focused on Weakly Supervised Scene Graph Generation (WSSGG). The mainstream solution for WSSGG typically follows the same pipeline: they first align text entities in the weak image-level supervisions (e.g., unlocalized relation triplets or captions) with image regions, and then train SGG models in a fully-supervised manner with aligned instance-level "pseudo" labels. However, we argue that most existing WSSGG works only focus on object-consistency, which means the grounded regions should have the same object category label as text entities. While they neglect another basic requirement for an ideal alignment: interaction-consistency, which means the grounded region pairs should have the same interactions (i.e., visual relations) as text entity pairs. Hence, in this paper, we propose to enhance a simple grounding module with both object-aware and interaction-aware knowledge to acquire more reliable pseudo labels. To better leverage these two types of knowledge, we regard them as two teachers and fuse their generated targets to guide the training process of our grounding module. Specifically, we design two different strategies to adaptively assign weights to different teachers by assessing their reliability on each training sample. Extensive experiments have demonstrated that our method consistently improves WSSGG performance on various kinds of weak supervision.},
	urldate = {2023-03-04},
	booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Li, Xingchen and Chen, Long and Ma, Wenbo and Yang, Yi and Xiao, Jun},
	year = {2022},
	keywords = {\#NONE, knowledge distillation, SGG, Visual Genome, weakly supervised},
	pages = {4204--4213},
	file = {Submitted Version:C\:\\Users\\felix\\Zotero\\storage\\5RUNC959\\Li et al. - 2022 - Integrating Object-aware and Interaction-aware Kno.pdf:application/pdf},
}

@inproceedings{lin2020gps,
  title={Gps-net: Graph property sensing network for scene graph generation},
  author={Lin, Xin and Ding, Changxing and Zeng, Jinquan and Tao, Dacheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3746--3753},
  year={2020}
}


@inproceedings{yang2022panoptic,
  title={Panoptic scene graph generation},
  author={Yang, Jingkang and Ang, Yi Zhe and Guo, Zujin and Zhou, Kaiyang and Zhang, Wayne and Liu, Ziwei},
  booktitle={European Conference on Computer Vision},
  pages={178--196},
  year={2022},
  organization={Springer}
}

@article{ozsoy2023labrad,
  title={LABRAD-OR: Lightweight Memory Scene Graphs for Accurate Bimodal Reasoning in Dynamic Operating Rooms},
  author={{\"O}zsoy, Ege and Czempiel, Tobias and Holm, Felix and Pellegrini, Chantal and Navab, Nassir},
  journal={arXiv preprint arXiv:2303.13293},
  year={2023}
}
@inproceedings{visual_question_answering,
  title={Vqa: Visual question answering},
  author={Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2425--2433},
  year={2015}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@inproceedings{yang2019auto,
  title={Auto-encoding scene graphs for image captioning},
  author={Yang, Xu and Tang, Kaihua and Zhang, Hanwang and Cai, Jianfei},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10685--10694},
  year={2019}
}

@inproceedings{gu2019unpaired,
  title={Unpaired image captioning via scene graph alignments},
  author={Gu, Jiuxiang and Joty, Shafiq and Cai, Jianfei and Zhao, Handong and Yang, Xu and Wang, Gang},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10323--10332},
  year={2019}
}