@article{nelson1990control,
  title={Control variate remedies},
  author={Nelson, Barry L},
  journal={Operations Research},
  volume={38},
  number={6},
  pages={974--992},
  year={1990},
  publisher={INFORMS}
}

@inproceedings{shulgin2022shifted,
  title={Shifted compression framework: Generalizations and improvements},
  author={Shulgin, Egor and Richt{\'a}rik, Peter},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={1813--1823},
  year={2022},
  organization={PMLR}
}

@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}

@article{spokoiny2012parametric,
  title={Parametric estimation. Finite sample theory},
  author={Spokoiny, Vladimir},
  journal={The Annals of Statistics},
  pages={2877--2909},
  year={2012},
  publisher={JSTOR}
}

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}

@inproceedings{mai2021stability,
  title={Stability and convergence of stochastic gradient clipping: Beyond lipschitz continuity and smoothness},
  author={Mai, Vien V and Johansson, Mikael},
  booktitle={International Conference on Machine Learning},
  pages={7325--7335},
  year={2021},
  organization={PMLR}
}

@article{zhang2020adaptive,
  title={Why are adaptive methods good for attention models?},
  author={Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15383--15393},
  year={2020}
}

@article{zhang2019gradient,
  title={Why gradient clipping accelerates training: A theoretical justification for adaptivity},
  author={Zhang, Jingzhao and He, Tianxing and Sra, Suvrit and Jadbabaie, Ali},
  journal={arXiv preprint arXiv:1905.11881},
  year={2019}
}

@inproceedings{pascanu2013difficulty,
  title={On the difficulty of training recurrent neural networks},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={1310--1318},
  year={2013},
  organization={Pmlr}
}

@inproceedings{allen2017katyusha,
  title={Katyusha: The first direct acceleration of stochastic gradient methods},
  author={Allen-Zhu, Zeyuan},
  booktitle={Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing},
  pages={1200--1205},
  year={2017}
}

@article{defazio2016simple,
  title={A simple practical accelerated method for finite sums},
  author={Defazio, Aaron},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{mishchenko2019stochastic,
  title={A stochastic decoupling method for minimizing the sum of smooth and non-smooth functions},
  author={Mishchenko, Konstantin and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1905.11535},
  year={2019}
}

@article{salim2022dualize,
  title={Dualize, split, randomize: Toward fast nonsmooth optimization algorithms},
  author={Salim, Adil and Condat, Laurent and Mishchenko, Konstantin and Richt{\'a}rik, Peter},
  journal={Journal of Optimization Theory and Applications},
  volume={195},
  number={1},
  pages={102--130},
  year={2022},
  publisher={Springer}
}

@article{garrigos2023handbook,
  title={Handbook of Convergence Theorems for (Stochastic) Gradient Methods},
  author={Garrigos, Guillaume and Gower, Robert M},
  journal={arXiv preprint arXiv:2301.11235},
  year={2023}
}

@article{huang2022tackling,
  title={Tackling data heterogeneity: A new unified framework for decentralized sgd with sample-induced topology},
  author={Huang, Yan and Sun, Ying and Zhu, Zehan and Yan, Changzhi and Xu, Jinming},
  journal={arXiv preprint arXiv:2207.03730},
  year={2022}
}

@inproceedings{richtarik20223pc,
  title={{3PC}: Three point compressors for communication-efficient distributed training and a better theory for lazy aggregation},
  author={Richt{\'a}rik, Peter and Sokolov, Igor and Gasanov, Elnur and Fatkhullin, Ilyas and Li, Zhize and Gorbunov, Eduard},
  booktitle={International Conference on Machine Learning},
  pages={18596--18648},
  year={2022},
  organization={PMLR}
}

@article{condat2022ef,
  title={{EF-BV}: A unified theory of error feedback and variance reduction mechanisms for biased and unbiased compression in distributed optimization},
  author={Condat, Laurent and Yi, Kai and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2205.04180},
  year={2022}
}

@inproceedings{condat2022murana,
  title={{MURANA}: A generic framework for stochastic variance-reduced optimization},
  author={Condat, Laurent and Richt{\'a}rik, Peter},
  booktitle={Mathematical and Scientific Machine Learning},
  pages={81--96},
  year={2022},
  organization={PMLR}
}

@inproceedings{koloskova2020unified,
  title={A unified theory of decentralized sgd with changing topology and local updates},
  author={Koloskova, Anastasia and Loizou, Nicolas and Boreiri, Sadra and Jaggi, Martin and Stich, Sebastian},
  booktitle={International Conference on Machine Learning},
  pages={5381--5393},
  year={2020},
  organization={PMLR}
}



@article{gower2021stochastic,
  title={Stochastic quasi-gradient methods: Variance reduction via Jacobian sketching},
  author={Gower, Robert M and Richt{\'a}rik, Peter and Bach, Francis},
  journal={Mathematical Programming},
  volume={188},
  pages={135--192},
  year={2021},
  publisher={Springer}
}

@inproceedings{taylor2019stochastic,
  title={Stochastic first-order methods: non-asymptotic and computer-aided analyses via potential functions},
  author={Taylor, Adrien and Bach, Francis},
  booktitle={Conference on Learning Theory},
  pages={2934--2992},
  year={2019},
  organization={PMLR}
}

@book{nesterov2003introductory,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  volume={87},
  year={2003},
  publisher={Springer Science \& Business Media}
}

@article{kulunchakov2020estimate,
  title={Estimate sequences for stochastic composite optimization: Variance reduction, acceleration, and robustness to noise},
  author={Kulunchakov, Andrei and Mairal, Julien},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={6184--6235},
  year={2020},
  publisher={JMLRORG}
}

@article{ajalloeian2020convergence,
  title={On the convergence of sgd with biased gradients},
  author={Ajalloeian, Ahmad and Stich, Sebastian U},
  journal={arXiv preprint arXiv:2008.00051},
  year={2020}
}

@article{beznosikov2022smooth,
  title={Smooth Monotone Stochastic Variational Inequalities and Saddle Point Problems--Survey},
  author={Beznosikov, Aleksandr and Polyak, Boris and Gorbunov, Eduard and Kovalev, Dmitry and Gasnikov, Alexander},
  journal={arXiv preprint arXiv:2208.13592},
  year={2022}
}

@article{beznosikov2022unified,
  title={A unified analysis of variational inequality methods: Variance reduction, sampling, quantization andcoordinate descent},
  author={Beznosikov, A and Gasnikov, A and Zainulina, K and Maslovskiy, A and Pasechnyuk, D},
  journal={arXiv preprint arXiv:2201.12206},
  year={2022}
}

@inproceedings{gorbunov2022stochastic,
  title={Stochastic extragradient: General analysis and improved rates},
  author={Gorbunov, Eduard and Berard, Hugo and Gidel, Gauthier and Loizou, Nicolas},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={7865--7901},
  year={2022},
  organization={PMLR}
}

@article{juditsky2011solving,
  title={Solving variational inequalities with stochastic mirror-prox algorithm},
  author={Juditsky, Anatoli and Nemirovski, Arkadi and Tauvel, Claire},
  journal={Stochastic Systems},
  volume={1},
  number={1},
  pages={17--58},
  year={2011},
  publisher={INFORMS}
}

@article{korpelevich1976extragradient,
  title={The extragradient method for finding saddle points and other problems},
  author={Korpelevich, Galina M},
  journal={Matecon},
  volume={12},
  pages={747--756},
  year={1976}
}

@article{beznosikov2022stochastic,
  title={Stochastic gradient descent-ascent: Unified theory and new efficient methods},
  author={Beznosikov, Aleksandr and Gorbunov, Eduard and Berard, Hugo and Loizou, Nicolas},
  journal={arXiv preprint arXiv:2202.07262},
  year={2022}
}

@book{facchinei2003finite,
  title={Finite-dimensional variational inequalities and complementarity problems},
  author={Facchinei, Francisco and Pang, Jong-Shi},
  year={2003},
  publisher={Springer}
}

@article{rajawat2020primal,
  title={A primal-dual framework for decentralized stochastic optimization},
  author={Rajawat, Ketan and Kumar, Chirag},
  journal={arXiv preprint arXiv:2012.04402},
  year={2020}
}

@inproceedings{gorbunov2021local,
  title={Local SGD: Unified theory and new efficient methods},
  author={Gorbunov, Eduard and Hanzely, Filip and Richt{\'a}rik, Peter},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3556--3564},
  year={2021},
  organization={PMLR}
}

@inproceedings{seide20141,
  title={1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns},
  author={Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
  booktitle={Fifteenth annual conference of the international speech communication association},
  year={2014}
}

@article{stich2018sparsified,
  title={Sparsified SGD with memory},
  author={Stich, Sebastian U and Cordonnier, Jean-Baptiste and Jaggi, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{gorbunov2020linearly,
  title={Linearly converging error compensated SGD},
  author={Gorbunov, Eduard and Kovalev, Dmitry and Makarenko, Dmitry and Richt{\'a}rik, Peter},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={20889--20900},
  year={2020}
}

@incollection{danilova2022recent,
  title={Recent theoretical advances in non-convex optimization},
  author={Danilova, Marina and Dvurechensky, Pavel and Gasnikov, Alexander and Gorbunov, Eduard and Guminov, Sergey and Kamzolov, Dmitry and Shibaev, Innokentiy},
  booktitle={High-Dimensional Optimization and Probability: With a View Towards Data Science},
  pages={79--163},
  year={2022},
  publisher={Springer}
}

@article{li2020unified,
  title={A unified analysis of stochastic gradient methods for nonconvex federated optimization},
  author={Li, Zhize and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2006.07013},
  year={2020}
}

@article{khaled2020better,
  title={Better theory for SGD in the nonconvex world},
  author={Khaled, Ahmed and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2002.03329},
  year={2020}
}

@article{khaled2020unified,
  title={Unified analysis of stochastic gradient methods for composite convex and smooth optimization},
  author={Khaled, Ahmed and Sebbouh, Othmane and Loizou, Nicolas and Gower, Robert M and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2006.11573},
  year={2020}
}

@article{necoara2019linear,
  title={Linear convergence of first order methods for non-strongly convex optimization},
  author={Nesterov, Yu},
  journal={Mathematical Programming},
  volume={175},
  pages={69--107},
  year={2019},
  publisher={Springer}
}

@book{beck2017first,
  title={First-order methods in optimization},
  author={Beck, Amir},
  year={2017},
  publisher={SIAM}
}

@article{nesterov2012efficiency,
  title={Efficiency of coordinate descent methods on huge-scale optimization problems},
  author={Nesterov, Yu},
  journal={SIAM Journal on Optimization},
  volume={22},
  number={2},
  pages={341--362},
  year={2012},
  publisher={SIAM}
}

@article{richtarik2014iteration,
  title={Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function},
  author={Richt{\'a}rik, Peter and Tak{\'a}{\v{c}}, Martin},
  journal={Mathematical Programming},
  volume={144},
  number={1-2},
  pages={1--38},
  year={2014},
  publisher={Springer}
}

@article{ghadimi2012optimal,
  title={Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={22},
  number={4},
  pages={1469--1492},
  year={2012},
  publisher={SIAM}
}

@inproceedings{nguyen2017sarah,
  title={{SARAH}: A novel method for machine learning problems using stochastic recursive gradient},
  author={Nguyen, Lam M and Liu, Jie and Scheinberg, Katya and Tak{\'a}{\v{c}}, Martin},
  booktitle={International Conference on Machine Learning},
  pages={2613--2621},
  year={2017},
  organization={PMLR}
}

@article{mishchenko2019distributed,
  title={Distributed learning with compressed gradient differences},
  author={Mishchenko, Konstantin and Gorbunov, Eduard and Tak{\'a}{\v{c}}, Martin and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1901.09269},
  year={2019}
}

@article{horvath2022stochastic,
  title={Stochastic distributed learning with gradient quantization and double-variance reduction},
  author={Horv{\'a}th, Samuel and Kovalev, Dmitry and Mishchenko, Konstantin and Richt{\'a}rik, Peter and Stich, Sebastian},
  journal={Optimization Methods and Software},
  pages={1--16},
  year={2022},
  publisher={Taylor \& Francis}
}

@article{khirirat2018distributed,
  title={Distributed learning with compressed gradients},
  author={Khirirat, Sarit and Feyzmahdavian, Hamid Reza and Johansson, Mikael},
  journal={arXiv preprint arXiv:1806.06573},
  year={2018}
}

@article{alistarh2017qsgd,
  title={QSGD: Communication-efficient SGD via gradient quantization and encoding},
  author={Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{horvath2021better,
  title={A Better Alternative to Error Feedback for Communication-Efficient Distributed Learning},
  author={Horv{\'a}th, Samuel and Richtarik, Peter},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{sahu2021rethinking,
  title={Rethinking gradient sparsification as total error minimization},
  author={Sahu, Atal and Dutta, Aritra and M Abdelmoniem, Ahmed and Banerjee, Trambak and Canini, Marco and Kalnis, Panos},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={8133--8146},
  year={2021}
}

@article{beznosikov2020biased,
  title={On biased compression for distributed learning},
  author={Beznosikov, Aleksandr and Horv{\'a}th, Samuel and Richt{\'a}rik, Peter and Safaryan, Mher},
  journal={arXiv preprint arXiv:2002.12410},
  year={2020}
}

@article{kairouz2021advances,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={14},
  number={1--2},
  pages={1--210},
  year={2021},
  publisher={Now Publishers, Inc.}
}

@article{danilova2022distributed,
  title={Distributed methods with absolute compression and error compensation},
  author={Danilova, Marina and Gorbunov, Eduard},
  journal={arXiv preprint arXiv:2203.02383},
  year={2022}
}

@article{qian2021lsvrg,
author = {Qian, Xun and Qu, Zheng and Richt\'{a}rik, Peter},
title = {{L-SVRG} and {L-Katyusha} with Arbitrary Sampling},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
month = {jul},
articleno = {112},
numpages = {49},
keywords = {ESO, L-Katyusha, expected smoothness, arbitrary sampling, L-SVRG}
}

@article{hofmann2015variance,
  title={Variance reduced stochastic gradient descent with neighbors},
  author={Hofmann, Thomas and Lucchi, Aurelien and Lacoste-Julien, Simon and McWilliams, Brian},
  journal={Advances in Neural Information Processing Systems},
  volume={28},
  year={2015}
}

@inproceedings{kovalev2020don,
  title={Donâ€™t jump through hoops and remove those loops: SVRG and Katyusha are better without the outer loop},
  author={Kovalev, Dmitry and Horv{\'a}th, Samuel and Richt{\'a}rik, Peter},
  booktitle={Algorithmic Learning Theory},
  pages={451--467},
  year={2020},
  organization={PMLR}
}

@article{bregman1967relaxation,
  title={The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming},
  author={Bregman, Lev M},
  journal={USSR computational mathematics and mathematical physics},
  volume={7},
  number={3},
  pages={200--217},
  year={1967},
  publisher={Elsevier}
}

@article{nemirovskij1983problem,
  title={Problem complexity and method efficiency in optimization},
  author={Nemirovskij, Arkadij Semenovi{\v{c}} and Yudin, David Borisovich},
  year={1983},
  publisher={Wiley-Interscience}
}

@article{hanzely2019one,
  title={One method to rule them all: variance reduction for data, parameters and many new methods},
  author={Hanzely, Filip and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1905.11266},
  year={2019}
}

@article{gower2020variance,
  title={Variance-reduced methods for machine learning},
  author={Gower, Robert M and Schmidt, Mark and Bach, Francis and Richt{\'a}rik, Peter},
  journal={Proceedings of the IEEE},
  volume={108},
  number={11},
  pages={1968--1983},
  year={2020},
  publisher={IEEE}
}

@article{defazio2014saga,
  title={{SAGA}: A fast incremental gradient method with support for non-strongly convex composite objectives},
  author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{schmidt2017minimizing,
  title={Minimizing finite sums with the stochastic average gradient},
  author={Schmidt, Mark and Le Roux, Nicolas and Bach, Francis},
  journal={Mathematical Programming},
  volume={162},
  number={1},
  pages={83--112},
  year={2017},
  publisher={Springer}
}

@article{schmidt2013fast,
  title={Fast convergence of stochastic gradient descent under a strong growth condition},
  author={Schmidt, Mark and Roux, Nicolas Le},
  journal={arXiv preprint arXiv:1308.6370},
  year={2013}
}

@inproceedings{vaswani2019fast,
  title={Fast and faster convergence of sgd for over-parameterized models and an accelerated perceptron},
  author={Vaswani, Sharan and Bach, Francis and Schmidt, Mark},
  booktitle={The 22nd international conference on artificial intelligence and statistics},
  pages={1195--1204},
  year={2019},
  organization={PMLR}
}

@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{ma2018power,
  title={The power of interpolation: Understanding the effectiveness of SGD in modern over-parametrized learning},
  author={Ma, Siyuan and Bassily, Raef and Belkin, Mikhail},
  booktitle={International Conference on Machine Learning},
  pages={3325--3334},
  year={2018},
  organization={PMLR}
}

@article{liu2022loss,
  title={Loss landscapes and optimization in over-parameterized non-linear systems and neural networks},
  author={Liu, Chaoyue and Zhu, Libin and Belkin, Mikhail},
  journal={Applied and Computational Harmonic Analysis},
  volume={59},
  pages={85--116},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{gower2019sgd,
  title={{SGD}: General analysis and improved rates},
  author={Gower, Robert Mansel and Loizou, Nicolas and Qian, Xun and Sailanbayev, Alibek and Shulgin, Egor and Richt{\'a}rik, Peter},
  booktitle={International Conference on Machine Learning},
  pages={5200--5209},
  year={2019},
  organization={PMLR}
}

@article{bottou2018optimization,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={Siam Review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018},
  publisher={SIAM}
}

@article{stich2019unified,
  title={Unified optimal analysis of the (stochastic) gradient method},
  author={Stich, Sebastian U},
  journal={arXiv preprint arXiv:1907.04232},
  year={2019}
}

@book{nesterov2018lectures,
  title={Lectures on convex optimization},
  author={Nesterov, Yurii and others},
  volume={137},
  publisher={Springer}
}

@inproceedings{gorbunov2020unified,
  title={A unified theory of {SGD}: Variance reduction, sampling, quantization and coordinate descent},
  author={Gorbunov, Eduard and Hanzely, Filip and Richt{\'a}rik, Peter},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={680--690},
  year={2020},
  organization={PMLR}
}