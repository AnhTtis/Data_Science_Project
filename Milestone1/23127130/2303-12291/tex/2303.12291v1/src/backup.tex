\begin{center}
    \section*{\Large Appendix}
\end{center}
The Appendix is organized as follows. 
\squishlist
\item Section A theoretically demonstrates why special treatments on sub-populations are necessary, and why \textbf{F}airness \textbf{R}egularizer (\textbf{FR}) improves learning from the noisily labeled long-tailed data. 
\item Section B includes all omitted proofs for theoretical conclusions.
\item Section C gives additional experiment details and results. 
\squishend

\section{Long-Tailed Sub-Populations Deserve Special Treatments}
\label{sec:theo_analysis}
In light of the empirical observations, we now theoretically explore the impacts of sub-populations when learning with long-tailed noisy data, through a binary Gaussian example. Note that classes could be viewed as a special case of sub-populations, we adopt the class-level long-tailed distributions for illustration.

\subsection{Formulation}
Consider the binary classification task such that $K=2$, and the data samples are generated by $P_{XY}$, which is the mixture of two Gaussians. Suppose $X^\pm:=(X|Y=\pm 1) \sim \mathcal{N}(\mu_\pm, \sigma^2)$ where $\mathcal{N}$ is the Gaussian distribution, and $\p(Y=+1)=\p(Y=-1)$. W.l.o.g., we assume that $\mu_+>\mu_-$. Suppose a classifier $f$ was trained on the noisy (and potentially imbalanced/long-tailed) training data $\Xxi:=\{x_i\}_{i=1}^{n}$ where the corresponding noisy label of $x_i$ is $\ny_i\in\nY$, $\forall i\in [n]$. Samples $x_i$ were drawn non-uniformly (i.e., imbalance) from $X$, and the ground-truth label of samples $x_i$ is $y_i$ given by $Y$. 

To inspect on the influence of sub-populations, we further split the imbalanced noisily labeled training data into two parts by referring to their clean labels: head-Gaussian data and tail-Gaussian data. For $x_i \sim X^{\pm}, y\in\{\pm1\}$, we denote the set of head and tail data in class $\pm1$ as $\Shb:=X_{\text{I}}\cap\Xhb, \Stb:=X_{\text{I}}\cap\Xtb$, where $\Xhb:=\{x\sim X^{\pm}| \frac{x-\mu_{\pm}}{\sigma}\cdot y \geq-\eta\}$, $\Xtb:=\{x\sim X^{\pm}| \frac{x-\mu_{\pm}}{\sigma}\cdot y< -\eta\}$ respectively. To clarify, replacing all ``$\pm$'' symbols by $+1$ will return the notation for class $+1$. And $G\in \{\text{H}, \text{T}\}$ in this setting.

\begin{figure}[!htb]
    \centering   \includegraphics[width=0.8\textwidth]{figures/gaussian.pdf}
    \vspace{-0.1in}
    \caption{An illustration of head/tail separations: when $\mu_+=+5, \mu_-=-5, \sigma^2=1, \eta=1$, the probability density distribution of Class +1 (Pink) and Class -1 (Blue) are drawn. $x$-axis indicates the Gaussian samples drawn from two Gaussian distributions, $y$-axis is the corresponding probability density of samples being equal to $x$.  }\label{fig:gaussian}
    \end{figure}


In the view of sub-populations, we assume that the noise transition differs w.r.t. the head and tail proportion, since tail populations are more misleading in the classification (i.e., in Figure \ref{fig:gaussian}, the label of samples in the ``tail-zone" is more likely to be wrongly given). Assume that the noise transition matrix in the head samples and tail samples follow $T_{\text{H}}, T_{\text{T}}$ respectively:
$$T_{\text{H}}=
    \begin{pmatrix}
         1-\emh  & \emh  \\
         \eph & 1 - \eph  
    \end{pmatrix}, \quad T_{\text{T}}=
    \begin{pmatrix}
         1-\emt  & \emt  \\
         \ept & 1 - \ept  
    \end{pmatrix}.
  $$
To refer to the noisy labels, we add the ~ {\color{red}$\widetilde{\cdot}$} ~ sign for the notations that are w.r.t. $\nY$ instead of $Y$: i.e., $\widetilde{X^{\pm}}:=(X|\nY=\pm1)$, $\widetilde{\Shb}:=X_{\text{I}}\cap \widetilde{\Xhb}$ with $\widetilde{\Xhb}:=\{x\sim \widetilde{X^{\pm}}| \frac{x-\mu_{\pm}}{\sigma}\cdot \ny \geq-\eta\}$, etc. W.l.o.g., we assume that the noise rates are not too large, i.e., $\ebh, \ebt \in [0. 0.5)$. Besides, we are interested in the scenario where the ground-truth samples are imbalanced. And we can assume that the imbalance ratio $r:=\frac{|\Shp|+|\Stp|}{|\Shm|+|\Stm|}$ satisfies $r>1$. 

\subsection{The Error Probability}

Given a classifier $f$, the \emph{Error probability} is defined as the percentage of error rates under a given data distribution:
\begin{definition}[Error probability]
The error probability of a classifier $f$ on the data distribution $(X, Y)$ is defined as $\text{Err}_X(f):=\p_{(X,Y)}(f(X)\neq Y)$.
\end{definition}
Denote by $\Phi$ the cumulative distribution function (CDF) of the standard Gaussian distribution $\mathcal{N}(0,1)$, we derive the error probability for the four populations as:
\begin{proposition}\label{prop:error}
For any linear classifier of the form $f(x)=\sign(x-\theta)$, we have: \begin{align*}
&\text{Err}_{\Xtb}(f) - \text{Err}_{\Xhb}(f)\quad \propto \quad ~\Phi\left((\theta-\mu_{\pm})\cdot (\pm1)\right)\cdot \sign\left((\mu_{\pm}-\theta)\cdot (\pm1) - \eta\sigma\right).\end{align*}
\end{proposition}
$\mu_{\pm}$ denotes the mean of two Gaussians and the Bayes optimal classifier adopts the threshold $\theta^*:=\frac{\mu_-+\mu_+}{2}$. We take the tail class $-1$ (replace all symbols ``$\pm$'' by ``$-$'') as an illustration:
\squishlist
\item When $\theta\geq \mu_-+\eta\sigma$,  the error probability gap $\text{Err}_{\Xtb}(f) - \text{Err}_{\Xhb}(f)$ is monotonically increasing w.r.t. the increase of {\small$ \Phi\left((\theta-\mu_{-})\right)\cdot \sign\left((\theta-\mu_{-})-\eta\sigma\right)=\Phi\left((\theta-\mu_{-})\right).$} Without additional treatments, the classifier over-fits on the head class to achieve a lower error probability. As a result, $\theta$ decreases, and the error gap between two populations in the tail class enlarges.
\item When $\theta$ decreases small enough, i.e., $\theta<\mu_-+\eta\sigma$, the error probability gap $\text{Err}_{\Xtb}(f) - \text{Err}_{\Xhb}(f)$ is monotonically increasing w.r.t. the increase of {\small$-\Phi\left((\theta-\mu_{-})\right).$} Further decreasing $\theta$ will make both populations in the tail class yield a large error probability.
\squishend

\subsection{The Bias of The Estimator}

We next adopt the estimation bias as a metric to show how class-imbalance and noisy labels degrade the model performance of estimated classifier $f$. The following proposition captures the influence of each sub-population on the bias of the estimator, compared to the Bayes optimal classifier. 
\begin{proposition}\label{prop:same_acc}
Denote the estimator on the imbalanced noisy data $\widetilde{X_{\text{I}}}$ as $\tilde{f}^*=\sign(x-\tilde{\theta})$,
$\forall \delta>0$, with probability at least $p$, {\small $\bias:=\frac{\mu_--\mu_+}{2}\cdot \Big(\frac{\eph|\nShp|+\ept|\nStp|}{|\nShp|+|\nStp|}-\frac{\emh|\nShm|+\emt|\nStm|}{|\nShm|+|\nStm|}\Big)$},
we have:
\[\bias-\delta\leq \left|\tilde{\theta}-\theta^*\right|\leq \bias+\delta.\]
\end{proposition}
We defer the form of $p$ till the proof of Proposition \ref{prop:same_acc} to not complicate the presentation. Briefly speaking, $p$ is large when (i) sample complexity is rich (large $|\nSib|$); (ii) balanced class ($|\nSip| \to |\nSim|$); or (iii) balanced sub-populations ($|\nShb| \to |\nStb|$). Going contrary to any above-mentioned scenario will result in a smaller probability $p$ to obtain a high-qualified estimator. 
\begin{remark}
The term $\bias$ indicates the quality/distance of the estimator by referring to the threshold $\theta^*$ of Bayes optimal classifier $f^*$. The term $\bias$ reduces to 0 and $\tilde{\theta}=\hat{\theta}=\theta^*$ when $T_{\text{T}}$ and $T_{\text{H}}$ have all zero off-diagonal elements. And non-zero off-diagonal entries of noise transition matrices are likely to return a biased estimator ($\bias\neq 0$). $\tilde{\theta}$ shifts away from the class with a lower weighted noise rate of the noisy labels to achieve a low error probability. {\color{black}Moreover, note that with the presence of label noise, the non-zero difference of the population level noise rate as well as the sample complexity further exerts differed impacts of sub-populations on the estimator.} 
 \end{remark} 

\subsection{Why Does \fr{} Help with Improvements}\label{app:fr_helps}
Building upon the previous discussions, to show why \fr{} helps with improving the learning, we first derive the per sub-population error probability w.r.t. the noisy labels, since in practice, clean labels are not available for the \fr{} to constrain.

\begin{lemma}\label{thm:err_noise}
The error probability of a classifier $f$ on the per-population noisy data distribution $(X, \nY)$ could be expressed in the forms of error probabilities under the clean data distribution, specifically:
\begin{align*}
\text{Err}_{\widetilde\Xhp}(f)
    &=p\cdot (1-\eph)\cdot \text{Err}_{\Xhp} + (1-p)\cdot \emh\cdot (1-\text{Err}_{\Xhm});\\
\text{Err}_{\widetilde\Xhm}(f)
    &=p\cdot \eph\cdot \text{Err}_{\Xhp} + (1-p)\cdot (1-\emh)\cdot (1-\text{Err}_{\Xhm});\\
\text{Err}_{\widetilde\Xtp}(f)
    &=p\cdot (1-\ept)\cdot \text{Err}_{\Xtp} + (1-p)\cdot \emt\cdot (1-\text{Err}_{\Xtm});\\
    \text{Err}_{\widetilde\Xtm}(f)
    &=p\cdot \ept\cdot \text{Err}_{\Xtp} + (1-p)\cdot (1-\emt)\cdot (1-\text{Err}_{\Xtm}).
\end{align*}
\end{lemma}
Although the overall error probability on the clean data distribution is:
\begin{align*}
   \text{Err}(f):= \p(\Xhp)\cdot \text{Err}_{\Xhp}(f) + \p(\Xhm)\cdot \text{Err}_{\Xhm}(f) + \p(\Xtp)\cdot \text{Err}_{\Xtp}(f) + \p(\Xtm)\cdot \text{Err}_{\Xtm}(f). 
\end{align*}
when learning with noisy data distribution with imbalanced sub-populations, the optimal $f$ w.r.t. the noisy data distribution is supposed to be given by the optimum of the following Risk Minimization:
\begin{align*}
     \text{RM:} \quad &\min_f \quad \widetilde{\text{Err}}(f):=\p(\widetilde{\Xhp})\cdot \text{Err}_{\widetilde{\Xhp}}(f) + \p(\widetilde{\Xhm})\cdot \text{Err}_{\widetilde{\Xhm}}(f) + \p(\widetilde{\Xtp})\cdot \text{Err}_{\widetilde{\Xtp}}(f) + \p(\widetilde{\Xtm})\cdot \text{Err}_{\widetilde{\Xtm}}(f).
\end{align*}
To distinguish the overall error probability under noisy and clean data distribution, we offer Theorem \ref{thm:gap}:

\begin{theorem}\label{thm:gap}
When $\p(Y=+1)=\p(Y=-1)$, an equivalent form of the minimization w.r.t. $\widetilde{\text{Err}}(f)$ is characterized by:
\begin{align*}
    \min_f \quad  \widetilde{\text{Err}}(f) ~\Longleftrightarrow~&\min_f \quad \text{Err}(f)-2r\cdot \rho_\text{H}\cdot \left(\text{Err}_{\widetilde\Xhp}(f)-\text{Err}_{\widetilde\Xhm}(f)\right)-\rho_\text{T}\cdot \left(\text{Err}_{\widetilde\Xtp}(f)-\text{Err}_{\widetilde\Xtm}(f)\right),
\end{align*}
where we define the noise rate gaps as $\rho_\text{H}:=\eph-\emh, \rho_\text{T}:=\ept-\emt,$ and  the sub-population imbalance ratio as $r:=\frac{1-\Phi(-\eta)}{\Phi(-\eta)}$.
\end{theorem}
  
Thus, constraining the classifier to perform fair performances (i.e., same training error probabilities such as $\text{Err}_{\widetilde\Xhp}(f)=\text{Err}_{\widetilde\Xhm}(f)$, and $\text{Err}_{\widetilde\Xtp}(f)=\text{Err}_{\widetilde\Xtm}(f)$), the optimal classifier training on the noisy data distribution with fairness regularizer yields the optimal classifier by refer to the clean data distribution! We then have:
\begin{corollary}
When $\p(Y=+1)=\p(Y=-1)$, \fr{} constrains the error probability (performance gap) between $\widetilde\Xhp$ and $\widetilde\Xhm$, $\widetilde\Xtp$ and $\widetilde\Xtm$. As a result, $\min_f ~\widetilde{\text{Err}}(f) \Longleftrightarrow\min_f ~\text{Err}(f)$.
\end{corollary}
The proof is straightforward from the result in Theorem \ref{thm:gap}.

\section{Omitted Proofs}


\subsection{Proof of Proposition \ref{prop:error}}

\begin{proof}
For the head population in Class $+1$, we can derive the error probability as:
\begin{align*}
    &\text{Err}_{\Xhp}(f):=\p_{(\Xhp,Y)}(f(\Xhp)\neq Y)=\p_{(\Xhp,Y)}\big((\Xhp-\theta)Y<0\big)= \p_{(\Xhp,Y)}\big(\Xhp<\theta\big). 
\end{align*}
Due to the separation of head and tail in Class $+1$, we then have:
\begin{align*}
    \text{Err}_{\Xhp}(f)
     =&  \frac{\p_{x\sim\mathcal{N}(\mu_+,\sigma^2)}\big(x<\theta, \frac{x-\mu_+}{\sigma}\geq-\eta\big)}{\p_{x\sim\mathcal{N}(\mu_+,\sigma^2)}(\frac{x-\mu_+}{\sigma}\geq -\eta)} \\
    =& \frac{\p\big(\mathcal{N}(\mu_+,\sigma^2)<\theta,\mathcal{N}(0,\sigma^2)\geq -\eta\sigma\big)}{\p(\mathcal{N}(0,1)\geq -\eta)} \\
     =& \frac{\p\big(\mathcal{N}(0,1)<\frac{\theta-\mu_+}{\sigma},\mathcal{N}(0,1)\geq -\eta\big)}{1-\Phi(-\eta)}. 
\end{align*}
where we denote by $\Phi$ the CDF of the standard Gaussian distribution $\mathcal{N}(0,1)$, and $\Phi(a)=1-\Phi(-a)$. Similarly, for the tail population in Class $+1$, we can derive the error probability as:
\begin{align*}
    &\text{Err}_{\Xtp}(f):=\p_{(\Xtp,Y)}(f(\Xtp)\neq Y)=\p_{(\Xtp,Y)}\big((\Xtp-\theta)Y<0\big)= \p_{(\Xtp,Y)}\big(\Xtp<\theta\big) \\
    =&  \frac{\p_{x\sim\mathcal{N}(\mu_+,\sigma^2)}\big(x<\theta, \frac{x-\mu_+}{\sigma}<-\eta\big)}{\p_{x\sim\mathcal{N}(\mu_+,\sigma^2)}(\frac{x-\mu_+}{\sigma}< -\eta)} \\
    =& \frac{\p\big(\mathcal{N}(\mu_+,\sigma^2)<\theta,\mathcal{N}(0,\sigma^2)<-\eta\sigma\big)}{\p(\mathcal{N}(0,1)< -\eta)} \\
     =& \frac{\p\big(\mathcal{N}(0,1)<\frac{\theta-\mu_+}{\sigma},\mathcal{N}(0,1)<-\eta\big)}{\Phi(-\eta)}. 
\end{align*}
For the populations in Class $-1$, we have:
\begin{align*}
    &\text{Err}_{\Xhm}(f):=\p_{(\Xhm,Y)}(f(\Xhm)\neq Y)=\p_{(\Xhm,Y)}\big((\Xhm-\theta)Y>0\big)= \p_{(\Xhm,Y)}\big(\Xhm>\theta\big) \\
    =&  \frac{\p_{x\sim\mathcal{N}(\mu_-,\sigma^2)}\big(x>\theta, \frac{x-\mu_-}{\sigma}\leq\eta\big)}{\p_{x\sim\mathcal{N}(\mu_-,\sigma^2)}(\frac{x-\mu_-}{\sigma}\leq \eta)} \\
    =& \frac{\p\big(\mathcal{N}(\mu_-,\sigma^2)>\theta,\mathcal{N}(0,\sigma^2)\leq \eta\sigma\big)}{\p(\mathcal{N}(0,1)\leq \eta)} \\
     =& \frac{\p\big(\mathcal{N}(0,1)>\frac{\theta-\mu_-}{\sigma},\mathcal{N}(0,1)\leq \eta\big)}{1-\Phi(-\eta)}. 
\end{align*}
\begin{align*}
    &\text{Err}_{\Xtm}(f):=\p_{(\Xtm,Y)}(f(\Xtm)\neq Y)\\
    =&\p_{(\Xtm,Y)}\big((\Xtm-\theta)Y>0\big)\\
    =& \p_{(\Xtm,Y)}\big(\Xtm>\theta\big) \\
    =&  \frac{\p_{x\sim\mathcal{N}(\mu_-,\sigma^2)}\big(x>\theta, \frac{x-\mu_-}{\sigma}>\eta\big)}{\p_{x\sim\mathcal{N}(\mu_-,\sigma^2)}(\frac{x-\mu_-}{\sigma}>\eta)} \\
    =& \frac{\p\big(\mathcal{N}(\mu_-,\sigma^2)>\theta,\mathcal{N}(0,\sigma^2)>\eta\sigma\big)}{\p(\mathcal{N}(0,1)>\eta)} \\
     =& \frac{\p\big(\mathcal{N}(0,1)>\frac{\theta-\mu_-}{\sigma},\mathcal{N}(0,1)>\eta\big)}{\Phi(-\eta)}. 
\end{align*}

The above thresholds can be further simplified into following forms given the cumulative distribution function (CDF) of the normal Gaussian distribution. If $\theta\leq \mu_+-\eta\sigma$, then:
\[\text{Err}_{\Xhp}(f)=\frac{0}{1-\Phi(-\eta)}=0, \quad \text{Err}_{\Xtp}(f)=\frac{\p\big(\mathcal{N}(0,1)<\min(\frac{\theta-\mu_+}{\sigma},-\eta)\big)}{\Phi(-\eta)}= \frac{\Phi\big(\frac{\theta-\mu_+}{\sigma}\big)}{\Phi(-\eta)}; \]
otherwise, we have $\theta>\mu_+-\eta\sigma$ and:
\[\text{Err}_{\Xhp}(f)=\frac{\p\big(-\eta\leq\mathcal{N}(0,1)<\frac{\theta-\mu_+}{\sigma}\big)}{1-\Phi(-\eta)}=\frac{\Phi(\frac{\theta-\mu_+}{\sigma})-\Phi(-\eta)}{1-\Phi(-\eta)},\quad \text{Err}_{\Xtp}(f)=1.\]
As for the class $-1$, when $\theta\geq \mu_-+\eta\sigma$, we obtain:
\[\text{Err}_{\Xhm}(f)=\frac{0}{1-\Phi(-\eta)}=0, \quad \text{Err}_{\Xtm}(f)=\frac{\p\big(\mathcal{N}(0,1)>\max(\frac{\theta-\mu_-}{\sigma},\eta)\big)}{\Phi(-\eta)}= \frac{\Phi\big(\frac{\mu_--\theta}{\sigma}\big)}{\Phi(-\eta)}; \]
otherwise, we have $\theta<\mu_-+\eta\sigma$ and:
\[\text{Err}_{\Xhm}(f)=\frac{\p\big(\frac{\theta-\mu_-}{\sigma}<\mathcal{N}(0,1)\leq \eta\big)}{1-\Phi(-\eta)}=\frac{\Phi(\frac{\mu_--\theta}{\sigma})-\Phi(-\eta)}{1-\Phi(-\eta)},\quad \text{Err}_{\Xtm}(f)=1.\]

We take Class $-1$ for illustration, when $\theta\geq \mu_-+\eta\sigma$, we have: $(\mu_{-}-\theta)\cdot (-1) - \eta\sigma\geq 0$. In this case, the difference of error probabilities in tail and head populations becomes:
\begin{align*}
  &\text{Err}_{\Xtm}(f) - \text{Err}_{\Xhm}(f)=\frac{\Phi\big(\frac{\mu_--\theta}{\sigma}\big)}{\Phi(-\eta)} \propto  \Phi\big(\frac{\mu_--\theta}{\sigma}\big) \propto  \Phi (\mu_--\theta)\\
  \propto\quad  & \Phi\left((\theta-\mu_{-})\cdot (-1)\right)\cdot \sign\left((\mu_{-}-\theta)\cdot (-1) - \eta\sigma\right).
\end{align*}
When $\theta< \mu_-+\eta\sigma$, we have: $(\mu_{-}-\theta)\cdot (-1) - \eta\sigma< 0$. In this case, the difference of error probabilities in tail and head populations becomes:
\begin{align*}
  &\text{Err}_{\Xtm}(f) - \text{Err}_{\Xhm}(f)=1-\frac{\Phi(\frac{\mu_--\theta}{\sigma})-\Phi(-\eta)}{1-\Phi(-\eta)}\\
  =&\frac{1-\Phi(\frac{\mu_--\theta}{\sigma})}{1-\Phi(-\eta)}\\
  = \quad &\frac{\Phi(\frac{\theta-\mu_-}{\sigma})}{1-\Phi(-\eta)} \propto   \Phi\big(\frac{\theta-\mu_-}{\sigma}\big) \propto  \Phi (\mu_--\theta)  \cdot (-1)\\
  \propto\quad  & \Phi\left((\theta-\mu_{-})\cdot (-1)\right)\cdot \sign\left((\mu_{-}-\theta)\cdot (-1) - \eta\sigma\right).
\end{align*}
For Class $+1$, the conclusion could be derived similarly. 
\end{proof}

\subsection{Proof of Proposition \ref{prop:same_acc}}
\begin{proof}

For sample $X_i\in \nSip:=\nShp\cup \nStp$, we express $X_i$ by $X_i=(1-\iip)(\mu_--\mu_+)+\Zip$, where $\Zip\sim \mathcal{N}(\mu_+, \sigma^2)$ and $\iip$ satisfies that:
\begin{align*}
    &\text{If }X_i\in\nStp: \qquad \frac{\zip-\mu_+}{\sigma}<-\eta, \qquad \iip\sim \text{Bernoulli}[\widetilde{\apt}];\\
    &\text{If }X_i\in\nShp: \qquad\frac{\zip-\mu_+}{\sigma}\geq- \eta,\qquad \iip\sim \text{Bernoulli}[\widetilde{\aph}],
\end{align*}
and $\widetilde{\aph}:=1-\eph, \widetilde{\apt}:=1-\ept$ indicate the accuracy of noisy labels/annotations for the two populations in class $+1$.

Similarly, for sample $X_i\in \nSim:=\nShm\cup\nStm$, we express $X_i$ by $X_i=(1-\iim)(\mu_+-\mu_-)+\Zim$, where $\Zim\sim \mathcal{N}(\mu_-, \sigma^2)$ and $\iim$ satisfies that:
\begin{align*}
    &\text{If }X_i\in\nStm: \qquad \frac{\zim-\mu_-}{\sigma}>\eta, \qquad \iim\sim \text{Bernoulli}[\widetilde{\amt}];\\
    &\text{If }X_i\in\nShm: \qquad\frac{\zim-\mu_-}{\sigma}\leq \eta,\qquad \iim\sim \text{Bernoulli}[\widetilde{\amh}],
\end{align*}
with $\widetilde{\amh}:=1-\eph, \widetilde{\amt}:=1-\ept$ being the accuracy of noisy labels/annotations for the two populations in class $-1$.

To illustrate the quantities, we take $X_i\in \nSip$ as an example. $\iip$ can be viewed as the random variable of the correct annotations in a Bernoulli trial, where there only two outcomes: correct annotation and wrong annotation. If the noisy label $\nY_i$ of $X_i$ is correct, i.e., $\nY_i=Y_i$, we have $\iip=1$ and: $X_i= (1-1)(\mu_--\mu_+)+\Zip=\Zip$. Otherwise, we have $\nY_i\neq Y_i$, and $X_i= (1-0)(\mu_--\mu_+)+\Zip=\Zim$.

Now we switch our focus on the accuracy of estimator $\tilde{\theta}$:
{\small\begin{align*}
    2\tilde{\theta}=&\sum_{X_i\in \nSip}\dfrac{X_i}{|\widetilde{\Sip|}} + \sum_{X_i\in \nSim}\dfrac{X_i}{|\nSim|}\\
    =&\sum_{X_i\in \nSip}\dfrac{(1-\iip)(\mu_--\mu_+)+\Zip}{|\nSip|} + \sum_{X_i\in \nSim}\dfrac{(1-\iim)(\mu_+-\mu_-)+\Zim}{|\nSim|}\\
    =&\Bigg(\sum_{X_i\in \nSip}\dfrac{\iip(\mu_+-\mu_-)}{|\nSip|}+ \sum_{X_i\in \nSim}\dfrac{\iim(\mu_--\mu_+)}{|\nSim|}\Bigg)+\underbrace{\Bigg(\sum_{X_i\in \nSip}\dfrac{\Zip}{|\nSip|} + \sum_{X_i\in \nSim}\dfrac{\Zim}{|\nSim|}\Bigg)}_{\sim \mathcal{N}\Big(\mu_++\mu_-, \big((|\nSip|+|\nSim|)\sigma^2/(|\nSip||\nSim|)\Big)}.
\end{align*}}
Define 
{\small\begin{align*}
    \bias&=\frac{\mu_--\mu_+}{2}\Bigg(-\dfrac{\widetilde{\aph}|\nShp|+\widetilde{\apt}|\nStp|}{|\nSip|}+\dfrac{\widetilde{\amh}|\nShm|+\widetilde{\amt}|\nStm|}{|\nSim|}\Bigg),
\end{align*}}
we have:
{\small\begin{align*}
    &|2\tilde{\theta}-(\mu_++\mu_-)-2\bias|\\
    =& 
    \Bigg|\Bigg(\sum_{X_i\in \nShp}\dfrac{\iip(\mu_+-\mu_-)}{|\nSip|}+ \sum_{X_i\in \nStp}\dfrac{\iip(\mu_+-\mu_-)}{|\nSip|}+\sum_{X_i\in \nShm}\dfrac{\iim(\mu_--\mu_+)}{|\nSim|}+ \sum_{X_i\in \nStm}\dfrac{\iim(\mu_--\mu_+)}{|\nSim|}-\bias\Bigg)\\
    &+\Bigg(\sum_{X_i\in \nSip}\dfrac{\Zip}{|\nSip|} + \sum_{X_i\in \nSim}\dfrac{\Zim}{|\nSim|}-(\mu_++\mu_-)\Bigg)\Bigg|\\
    =& 
    \Bigg|(\mu_+-\mu_-)\Bigg(\sum_{X_i\in \nShp}\dfrac{\iip}{|\nSip|}-\dfrac{\widetilde{\aph}|\nShp|}{|\nSip|}\Bigg)+ (\mu_+-\mu_-)\Bigg(\sum_{X_i\in \nStp}\dfrac{\iip}{|\nSip|}-\dfrac{\widetilde{\apt}|\nStp|}{|\nSip|}\Bigg)\\
    &+(\mu_--\mu_+)\Bigg(\sum_{X_i\in \nShm}\dfrac{\iim}{|\nSim|}-\dfrac{\widetilde{\amh}|\nShm|}{|\nSim|}\Bigg)+ (\mu_--\mu_+)\Bigg(\sum_{X_i\in \nStm}\dfrac{\iim}{|\nSim|}-\dfrac{\widetilde{\amt}|\nStm|}{|\nSim|}\Bigg)\\
    &+\Bigg(\sum_{X_i\in \nSip}\dfrac{\Zip}{|\nSip|} + \sum_{X_i\in \nSim}\dfrac{\Zim}{|\nSim|}-(\mu_++\mu_-)\Bigg)\Bigg|\\
    \leq& \underbrace{|\mu_+-\mu_-|
    \Bigg|\dfrac{\sum_{X_i\in \nShp}\iip}{|\nSip|}-\dfrac{\widetilde{\aph}|\nShp|}{|\nSip|}\Bigg|}_{\text{Term \textcircled{1}}}+\underbrace{ |\mu_+-\mu_-|\Bigg|\dfrac{\sum_{X_i\in \nStp}\iip}{|\nSip|}-\dfrac{\widetilde{\apt}|\nStp|}{|\nSip|}\Bigg|}_{\text{Term \textcircled{2}}}\\
    &+\underbrace{|\mu_--\mu_+|\Bigg|\dfrac{\sum_{X_i\in \nShm}\iim}{|\nSim|}-\dfrac{\widetilde{\amh}|\nShm|}{|\nSim|}\Bigg|}_{\text{Term \textcircled{3}}}+ \underbrace{|\mu_--\mu_+|\Bigg|\dfrac{\sum_{X_i\in \nStm}\iim}{|\nSim|}-\dfrac{\widetilde{\amt}|\nStm|}{|\nSim|}\Bigg|}_{\text{Term \textcircled{4}}}\\
    &+\underbrace{\Bigg|\Bigg(\sum_{X_i\in \nShp}\dfrac{\Zip}{|\nSip|} + \sum_{X_i\in \nShm}\dfrac{\Zim}{|\nSim|}\Bigg)-(\mu_++\mu_-) \Bigg|}_{\text{Term \textcircled{5}}}.
\end{align*}}
By using the Hoeffding inequality, for some $\epsilon>0$, we have the following inequality for the head data:
{\small\begin{align*}
    \p\Bigg(\Bigg|\sum_{X_i\in \nShp,} \iip - \widetilde{\aph} \cdot|\nShp|\Bigg|>\epsilon |\nShp|\Bigg)\leq 2e^{-2\epsilon^2|\nShp|},
\end{align*}}
which is equivalent to:
{\small\begin{align*}
    \p\Bigg(\Bigg|\dfrac{\sum_{X_i\in \nShp} \iip}{|\nSip|} - \widetilde{\aph}\dfrac{|\nShp|}{|\nSip|}\Bigg|>\dfrac{|\nShp|}{|\nSip|}\epsilon \Bigg)\leq 2e^{-2\epsilon^2|\nShp|}.
\end{align*}}
Mapping $\epsilon\leftarrow \frac{|\nShp|}{|\nSip|}\epsilon$, the above inequality further becomes:
{\small\begin{align*}
    \p\Bigg(\Bigg|\dfrac{\sum_{X_i\in \nShp} \iip}{|\nSip|} - \widetilde{\aph}\dfrac{|\nShp|}{|\nSip|}\Bigg|>\epsilon \Bigg)\leq 2e^{-2\epsilon^2|\nSip|^2/|\nShp|}.
\end{align*}}
Thus, for Term \textcircled{1} and $\epsilon=\frac{2\delta}{5|\mu_+-\mu_-|}$, we have the following bound:
{\small\begin{align*}
    &\p\Bigg(\dfrac{1}{2}\underbrace{|\mu_+-\mu_-|
    \Bigg|\dfrac{\sum_{X_i\in \nShp}\iip}{|\nSip|}-\dfrac{\widetilde{\aph}|\widetilde{\Shp|}}{|\nSip|}\Bigg|}_{\text{Term \textcircled{1}}}> \dfrac{\delta}{5} \Bigg)\leq 2e^{-\dfrac{8\delta^2|\nSip|^2}{25(\mu_+-\mu_-)^2|\nShp|}}.
\end{align*}}
For the tail data in $\nSip$, we can similarly obtain:
\begin{align*}
    \p\Bigg(\Bigg|\dfrac{\sum_{X_i\in \nStp} \iip}{|\nSip|} - \widetilde{\apt}\dfrac{|\nStp|}{|\nSip|}\Bigg|>\epsilon \Bigg)\leq 2e^{-2\epsilon^2|\nSip|^2/|\nStp|}.
\end{align*}
Thus, for Term \textcircled{2} and $\epsilon=\frac{2\delta}{5|\mu_+-\mu_-|}$, we have the following bound:
{\small\begin{align*}
    &\p\Bigg(\dfrac{1}{2}\underbrace{|\mu_+-\mu_-|
    \Bigg|\dfrac{\sum_{X_i\in \nStp}\iip}{|\nSip|}-\dfrac{\widetilde{\apt}|\nStp|}{|\nSip|}\Bigg|}_{\text{Term \textcircled{2}}}> \dfrac{\delta}{5} \Bigg)\leq 2e^{-\dfrac{8\delta^2|\nSip|^2}{25(\mu_+-\mu_-)^2|\nStp|}}.
\end{align*}}
When $X_i\in \nSim$, we can obtain the following two inequality for head and tail data, respectively:
{\small\begin{align*}
    \p\Bigg(\Bigg|\dfrac{\sum_{X_i\in \nShm} \iim}{|\nSim|} - \widetilde{\amh}\dfrac{|\nShm|}{|\nSim|}\Bigg|>\epsilon \Bigg)\leq 2e^{-2\epsilon^2|\nSim|^2/|\nShm|},
\end{align*}}
{\small\begin{align*}
    \p\Bigg(\Bigg|\dfrac{\sum_{X_i\in \nStm} \iim}{|\nSim|} - \widetilde{\amt}\dfrac{|\nStm|}{|\nSim|}\Bigg|>\epsilon \Bigg)\leq 2e^{-2\epsilon^2|\nSim|^2/|\nStm|}.
\end{align*}}
Thus, for Term \textcircled{3}, \textcircled{4} and $\epsilon=\frac{2\delta}{5|\mu_+-\mu_-|}$, we have the following bounds:
{\small\begin{align*}
    &\p\Bigg(\dfrac{1}{2}\underbrace{|\mu_+-\mu_-|
    \Bigg|\dfrac{\sum_{X_i\in \nShm}\iim}{|\nSim|}-\dfrac{\widetilde{\amh}|\nShm|}{|\nSim|}\Bigg|}_{\text{Term \textcircled{3}}}> \dfrac{\delta}{5} \Bigg)\leq 2e^{-\dfrac{8\delta^2|\nSim|^2}{25(\mu_+-\mu_-)^2|\nShm|}},
\end{align*}}
{\small\begin{align*}
    &\p\Bigg(\dfrac{1}{2}\underbrace{|\mu_+-\mu_-|
    \Bigg|\dfrac{\sum_{X_i\in \nStm}\iim}{|\nSim|}-\dfrac{\widetilde{\amt}|\nStm|}{|\nSim|}\Bigg|}_{\text{Term \textcircled{4}}}> \dfrac{\delta}{5} \Bigg)\leq 2e^{-\dfrac{8\delta^2|\nSim|^2}{25(\mu_+-\mu_-)^2|\nStm|}}.
\end{align*}}
Note that 
{\small\[\Bigg(\sum_{X_i\in \nSip}\dfrac{\Zip}{|\nSip|} + \sum_{X_i\in \nSim}\dfrac{\Zim}{|\nSim|}\Bigg)\sim \mathcal{N}\Big(\mu_++\mu_-, \big((|\nSip|+|\nSim|)\sigma^2/(|\nSip||\nSim|)\Big),\]}
as for Term \textcircled{5}, with the standard Gaussian concentration property, we have:
{\small\begin{align*}
    \p(\text{Term \textcircled{5}}> \epsilon)\leq 2e^{-\dfrac{\epsilon^2(|\nSip||\nSim|)}{2\sigma^2(|\nSip|+|\nSim|)}},
\end{align*}}
which is equivalent to:
{\small\begin{align*}
    \p(\text{Term \textcircled{5}}> \dfrac{2\delta}{5})\leq 2e^{-\dfrac{2\delta^2(|\nSip||\nSim|)}{25\sigma^2(\mu_+-\mu_-)^2(|\nSip|+|\nSim|)}}.
\end{align*}}
Thus, with probability $p$, we have:
{\small\begin{align*}
\dfrac{1}{2}\Big|2\tilde{\theta}-(\mu_++\mu_-)-\bias\Big|\leq5\cdot \dfrac{\delta}{5}=\delta,
\end{align*}}
where:
{\small\begin{align*}
    p:=&1-\sum_{i\in[5]}\p(\text{Term \textcircled{i}}> \dfrac{2\delta}{5})\\
    =&1-2e^{-\dfrac{8\delta^2|\nSip|^2}{25(\mu_+-\mu_-)^2|\nShp|}}-2e^{-\dfrac{8\delta^2|\nSip|^2}{25(\mu_+-\mu_-)^2|\nStp|}}-2e^{-\dfrac{8\delta^2|\nSim|^2}{25(\mu_+-\mu_-)^2|\nShm|}}\\
    &-2e^{-\dfrac{8\delta^2|\nSim|^2}{25(\mu_+-\mu_-)^2|\nStm|}}-2e^{-\dfrac{2\delta^2(|\nSip||\nSim|)}{25\sigma^2(\mu_+-\mu_-)^2(|\nSip|+|\nSim|)}}.
\end{align*}}
\paragraph{Further simplification:} Now that we have with probability at least $p$:
{\small\begin{align*}
    |2\tilde{\theta}-(\mu_++\mu_-)-2\bias|\leq \delta \Longleftrightarrow \bias-\delta\leq \left|\tilde{\theta}-\theta^*\right|\leq \bias+\delta.
\end{align*}}
The Bias term could be further simplified as:
{\small\begin{align*}
    \bias&=(\mu_--\mu_+)\Bigg(-\dfrac{\widetilde{\aph}|\nShp|+\widetilde{\apt}|\nStp|}{|\nSip|}+\dfrac{\widetilde{\amh}|\nShm|+\widetilde{\amt}|\nStm|}{|\nSim|}\Bigg)\\
    &=(\mu_--\mu_+)\Bigg(-\dfrac{(1-\eph)|\nShp|+(1-\ept)|\nStp|}{|\nSip|}+\dfrac{(1-\emh)|\nShm|+(1-\emt)|\nStm|}{|\nSim|}\Bigg)\\
    &=\frac{\mu_--\mu_+}{2}\cdot \Big(\frac{\eph|\nShp|+\ept|\nStp|}{|\nShp|+|\nStp|}-\frac{\emh|\nShm|+\emt|\nStm|}{|\nShm|+|\nStm|}\Big) \qquad \text{(Due to }|\nSib|=|\nShb| + |\nStb|).
\end{align*}}
\end{proof}

\subsection{Proof of Lemma \ref{thm:err_noise}}
\begin{proof}
    Note that:
\begin{align*}
    &\text{Err}_{\Xhp}(f)=\frac{\p\big(\mathcal{N}(0,1)<\frac{\theta-\mu_+}{\sigma},\mathcal{N}(0,1)\geq -\eta\big)}{1-\Phi(-\eta)}, \qquad \text{Err}_{\Xtp}(f)=\frac{\p\big(\mathcal{N}(0,1)<\frac{\theta-\mu_+}{\sigma},\mathcal{N}(0,1)<-\eta\big)}{\Phi(-\eta)}. \\
       &\text{Err}_{\Xhm}(f)=\frac{\p\big(\mathcal{N}(0,1)>\frac{\theta-\mu_-}{\sigma},\mathcal{N}(0,1)\leq \eta\big)}{1-\Phi(-\eta)}, \qquad \text{Err}_{\Xtm}(f)=
     \frac{\p\big(\mathcal{N}(0,1)>\frac{\theta-\mu_-}{\sigma},\mathcal{N}(0,1)>\eta\big)}{\Phi(-\eta)}.
\end{align*}
Thus, denote by $p:=\p(Y=+)$, we have:
\begin{align*}
    \text{Err}_{\widetilde\Xhp}(f)&=\p_{(\widetilde\Xhp,\nY)}(f(\widetilde\Xhp)\neq \nY)\\
    &=\p(\nY=+,Y=+|\Xhp)\cdot \p_{(\widetilde\Xhp,\nY)}\big((\widetilde\Xhp-\theta)Y<0\big) + \p(\nY=+,Y=-|\Xhm)\cdot \p_{(\widetilde\Xhp,\nY)}\big((\widetilde\Xhp-\theta)Y>0\big)\\
    &=p\cdot (1-\eph)\cdot \p_{(\Xhp, Y=+)}\big((\Xhp-\theta)Y<0\big) + (1-p)\cdot \emh\cdot \p_{(\Xhm,Y=-)}\big((\Xhm-\theta)Y>0\big)\\
    &=p\cdot (1-\eph)\cdot \p_{(\Xhp, Y=+)}\big(\Xhp<\theta\big) + (1-p)\cdot \emh\cdot \p_{(\Xhm,Y=-)}\big(\Xhm<\theta\big)\\
    &=p\cdot (1-\eph)\cdot \text{Err}_{\Xhp} + (1-p)\cdot \emh\cdot (1-\text{Err}_{\Xhm}).
\end{align*}
Similarly, we could derive:
\begin{align*}
    \text{Err}_{\widetilde\Xhm}(f)&=\p_{(\widetilde\Xhm,\nY)}(f(\widetilde\Xhm)\neq \nY)\\
    &=\p(\nY=-,Y=+|\Xhp)\cdot \p_{(\widetilde\Xhm,\nY)}\big((\widetilde\Xhm-\theta)\nY>0\big) + \p(\nY=-,Y=-|\Xhm)\cdot \p_{(\widetilde\Xhm,\nY)}\big((\widetilde\Xhm-\theta)\nY>0\big)\\
    &=p\cdot \eph\cdot \p_{(\Xhp, Y=+)}\big((\Xhp-\theta)Y<0\big) + (1-p)\cdot (1-\emh)\cdot \p_{(\Xhm,Y=-)}\big((\Xhm-\theta)Y>0\big)\\
    &=p\cdot \eph\cdot \p_{(\Xhp, Y=+)}\big(\Xhp<\theta\big) + (1-p)\cdot (1-\emh)\cdot \p_{(\Xhm,Y=-)}\big(\Xhm<\theta\big)\\
    &=p\cdot \eph\cdot \text{Err}_{\Xhp} + (1-p)\cdot (1-\emh)\cdot (1-\text{Err}_{\Xhm}).\\
      \text{Err}_{\widetilde\Xtp}(f)
    &=p\cdot (1-\ept)\cdot \text{Err}_{\Xtp} + (1-p)\cdot \emt\cdot (1-\text{Err}_{\Xtm}).\\
    \text{Err}_{\widetilde\Xtm}(f)
    &=p\cdot \ept\cdot \text{Err}_{\Xtp} + (1-p)\cdot (1-\emt)\cdot (1-\text{Err}_{\Xtm}).
\end{align*}
\end{proof}

\subsection{Proof of Theorem \ref{thm:gap}}
\begin{proof}
    For balanced clean prior ($p=0.5$),
we have:
\begin{align*}
   \text{Err}(f):= (1-\Phi(-\eta))\cdot\left( \text{Err}_{\Xhp}(f) + \text{Err}_{\Xhm}(f)\right) + \Phi(-\eta)\cdot\left( \text{Err}_{\Xtp}(f) + \text{Err}_{\Xtm}(f)\right), 
\end{align*}
and 
\begin{align*}
    \text{RM:} \quad &\min_f \quad \p(\widetilde{\Xhp})\cdot \text{Err}_{\widetilde{\Xhp}}(f) + \p(\widetilde{\Xhm})\cdot \text{Err}_{\widetilde{\Xhm}}(f) + \p(\widetilde{\Xtp})\cdot \text{Err}_{\widetilde{\Xtp}}(f) + \p(\widetilde{\Xtm})\cdot \text{Err}_{\widetilde{\Xtm}}(f)\\
     =&\min_f \quad (1-\Phi(-\eta))\cdot\left( (1-\eph+\emh)\cdot \text{Err}_{\widetilde{\Xhp}}(f) + (1+\eph-\emh)\cdot \text{Err}_{\widetilde{\Xhm}}(f)\right)\\&\quad + \Phi(-\eta)\cdot\left( (1-\ept+\emt)\cdot\text{Err}_{\widetilde{\Xtp}}(f) + (1-\emt+\ept)\cdot\text{Err}_{\widetilde{\Xtm}}(f)\right).
\end{align*}
Define the noise rate gaps $\rho_\text{H}:=\eph-\emh, \rho_\text{T}:=\ept-\emt,$ and imbalance ratio $r:=\frac{1-\Phi(-\eta)}{\Phi(-\eta)}$, we then have: 
\begin{align}
    \text{RM} \Longleftrightarrow&\min_f \quad r\cdot\left( (1-\rho_\text{H})\cdot \text{Err}_{\widetilde{\Xhp}}(f) + (1+\rho_\text{H})\cdot \text{Err}_{\widetilde{\Xhm}}(f)\right) + \left( (1-\rho_\text{T})\cdot\text{Err}_{\widetilde{\Xtp}}(f) + (1+\rho_\text{T})\cdot\text{Err}_{\widetilde{\Xtm}}(f)\right)\notag\\
    \Longleftrightarrow&\min_f \quad r\cdot\left( (1-\rho_\text{H})\cdot \left[(1-\eph)\cdot \text{Err}_{\Xhp} + \emh\cdot (1-\text{Err}_{\Xhm}) \right] + (1+\rho_\text{H})\cdot \left[\eph\cdot \text{Err}_{\Xhp} + (1-\emh)\cdot (1-\text{Err}_{\Xhm})\right]\right) \notag\\
    &\quad + \left( (1-\rho_\text{T})\cdot \left[(1-\ept)\cdot \text{Err}_{\Xtp} + \emt\cdot (1-\text{Err}_{\Xtm}) \right] + (1+\rho_\text{T})\cdot \left[\ept\cdot \text{Err}_{\Xtp} + (1-\emt)\cdot (1-\text{Err}_{\Xtm})\right]\right) \notag\\
    \Longleftrightarrow&\min_f \quad r\cdot\left(\left[(1-\eph)\cdot \text{Err}_{\Xhp} + \emh\cdot (1-\text{Err}_{\Xhm}) \right] +  \left[\eph\cdot \text{Err}_{\Xhp} + (1-\emh)\cdot (1-\text{Err}_{\Xhm})\right]\right) \notag\\
    &\quad + \left( \left[(1-\ept)\cdot \text{Err}_{\Xtp} + \emt\cdot (1-\text{Err}_{\Xtm}) \right] + \left[\ept\cdot \text{Err}_{\Xtp} + (1-\emt)\cdot (1-\text{Err}_{\Xtm})\right]\right) \notag\\
        &\quad -r\cdot \rho_\text{H}\cdot\left(  \left[(1-\eph)\cdot \text{Err}_{\Xhp} + \emh\cdot (1-\text{Err}_{\Xhm}) - \eph\cdot \text{Err}_{\Xhp} - (1-\emh)\cdot (1-\text{Err}_{\Xhm})\right]\right) \notag\\
    &\quad - \rho_\text{T}\cdot \left[(1-\ept)\cdot \text{Err}_{\Xtp} + \emt\cdot (1-\text{Err}_{\Xtm}) - \ept\cdot \text{Err}_{\Xtp} - (1-\emt)\cdot (1-\text{Err}_{\Xtm})\right]\notag\\
    \Longleftrightarrow&\min_f \quad r\cdot \left(\text{Err}_{\Xhp} + \text{Err}_{\Xhm}\right) + \left(\text{Err}_{\Xtp} + \text{Err}_{\Xtm}\right)\notag\\
        &\quad -r\cdot \rho_\text{H}\cdot\left(  \left[(1-2\eph)\cdot \text{Err}_{\Xhp} - (1-2\emh)\cdot (1-\text{Err}_{\Xhm})\right]\right)\notag \\
    &\quad - \rho_\text{T}\cdot \left[(1-2\ept)\cdot \text{Err}_{\Xtp} - (1-2\emt)\cdot (1-\text{Err}_{\Xtm}) \right]\notag\\
       \Longleftrightarrow&\min_f \quad \text{Err}(f)-r\cdot \rho_\text{H}\cdot\left(  \left[(1-2\eph)\cdot \text{Err}_{\Xhp} - (1-2\emh)\cdot (1-\text{Err}_{\Xhm})\right]\right) \notag\\
    &\quad - \rho_\text{T}\cdot \left[(1-2\ept)\cdot \text{Err}_{\Xtp} - (1-2\emt)\cdot (1-\text{Err}_{\Xtm}) \right].\label{eq:rm}
\end{align}


To achieve relatively fair performances between $\widetilde\Xtp$ and $\widetilde\Xtm$, i.e., the performance gap between $ \text{Err}_{\widetilde\Xtp}(f)$ and $\text{Err}_{\widetilde\Xtm}(f)$ is supposed to be small. Note that:
\begin{align*}
    &\text{Err}_{\widetilde\Xtp}(f)-\text{Err}_{\widetilde\Xtm}(f)\\
    &=\left[p\cdot (1-\ept)\cdot \text{Err}_{\Xtp} + (1-p)\cdot \emt\cdot (1-\text{Err}_{\Xtm})\right] - \left[p\cdot \ept\cdot \text{Err}_{\Xtp} + (1-p)\cdot (1-\emt)\cdot (1-\text{Err}_{\Xtm})\right] \\ 
    &=\frac{1}{2}\cdot\left[ (1-2\ept)\cdot \text{Err}_{\Xtp} -  (1-2\emt)\cdot (1-\text{Err}_{\Xtm})\right].
\end{align*}
Similarly, for the two head sub-populations, we could derive that:
\begin{align*}
    &\text{Err}_{\widetilde\Xhp}(f)-\text{Err}_{\widetilde\Xhm}(f)\\
    &=\left[p\cdot (1-\eph)\cdot \text{Err}_{\Xhp} + (1-p)\cdot \emt\cdot (1-\text{Err}_{\Xhm})\right] - \left[p\cdot \eph\cdot \text{Err}_{\Xhp} + (1-p)\cdot (1-\emh)\cdot (1-\text{Err}_{\Xhm})\right] \\ 
    &=\frac{1}{2}\cdot \left[(1-2\eph)\cdot \text{Err}_{\Xhp} -  (1-2\emh)\cdot (1-\text{Err}_{\Xhm})\right].
\end{align*}
Thus, by incorporating the above two performance gaps into Eqn. (\ref{eq:rm}), the RM has its equivalent form as below:
\begin{align*}
    \text{RM} \Longleftrightarrow&\min_f \quad \text{Err}(f)-r\cdot \rho_\text{H}\cdot\left(  \left[(1-2\eph)\cdot \text{Err}_{\Xhp} - (1-2\emh)\cdot (1-\text{Err}_{\Xhm})\right]\right) \\
    &\quad - \rho_\text{T}\cdot \left[(1-2\ept)\cdot \text{Err}_{\Xtp} - (1-2\emt)\cdot (1-\text{Err}_{\Xtm}) \right]\\
    \Longleftrightarrow&\min_f \quad \text{Err}(f)-2r\cdot \rho_\text{H}\cdot \left(\text{Err}_{\widetilde\Xhp}(f)-\text{Err}_{\widetilde\Xhm}(f)\right)-\rho_\text{T}\cdot \left(\text{Err}_{\widetilde\Xtp}(f)-\text{Err}_{\widetilde\Xtm}(f)\right).
\end{align*}
\end{proof}

\section{Additional Experiment Results and Details}


\subsection{Influences of sub-populations on test data}\label{app:more}
\paragraph{Influences on Class-Level Test Accuracy}
When removing all samples from the sub-population $\mathcal{G}_i$ during the whole training procedure, remember that we defined the (class-level) test accuracy changes as:
\begin{align*}
\text{Acc}_{\text{c}}(\mathcal{A}, \widetilde{S}, i, j):=&\P_{\substack{f\leftarrow \mathcal{A}(\widetilde{S})\\
  (X', Y'=j)}} (f(X')=Y') - \P_{\substack{f\leftarrow \mathcal{A}(\widetilde{S}^{\backslash i})\\
  (X', Y'=j)}} (f(X')=Y'),
\end{align*}
where $(X', Y'=j)$ indicates the test data distribution given the clean label is $j$. In Figure \ref{fig:clean_class_full}, the $x$-axis denotes the loss function for training, and the $y$-axis visualized the distribution of $\{\text{Acc}_{\text{c}}(\mathcal{A}, S, i, j)\}_{j\in[10]}$ (above the dashed line) and $\{\text{Acc}_{\text{c}}(\mathcal{A}, \widetilde{S}, i, j)\}_{j\in[10]}$ (below the dashed line) for several long-tailed sub-populations ($i=52, 37, 75, 19, 81, 36, 91, 63, 70, 55, 67, 41, 98, 40, 61, 87, 71$) under each robust method. The blue zone shows the 25-th percentile ($Q_1$) and 75-th percentile ($Q_3$) accuracy changes, and the orange line indicates the median value. Accuracy changes that are drawn as circles are viewed as outliers. Note that all sub-figures have the same limits for $y$-axis, Observation \ref{obs1} holds for more tail sub-populations under class-level test accuracy as well. 

\begin{figure*}[!htb]
    \centering
    \includegraphics[width=\textwidth]{figures/acc_coarse_00_new.pdf}
    ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------
    \includegraphics[width=\textwidth]{figures/acc_coarse_02_new.pdf}
    \caption{(Completed version) Box plot of the class-level test accuracy changes when removing all samples of a  selected long-tailed sub-population during the training w.r.t. 4 methods. (Above the dashed line: trained on clean labels; below the dashed line: trained on noisy labels.)}\label{fig:clean_class_full}
    \vspace{-0.1in}
    \end{figure*}

 
\paragraph{Influences on Population-Level Test Accuracy}
When removing all samples from the sub-population $\mathcal{G}^{(i)}$ during the whole training procedure, remember the (population-level) test accuracy changes are defined as:
\begin{align*}
\text{Acc}_{\text{p}}(\mathcal{A}, \widetilde{S}, i, j):=&\P_{\substack{f\leftarrow \mathcal{A}(\widetilde{S})\\
  (X', Y', G=j)}} (f(X')=Y') - \P_{\substack{f\leftarrow \mathcal{A}(\widetilde{S}^{\backslash i})\\
   (X', Y', G=j)}} (f(X')=Y'),
\end{align*}
where $(X', Y', G=j)$ indicates the test data distribution given that the samples are from the $j$-th population. In Figure \ref{fig:clean_sub}, we repeat the previous step while visualize the distribution of $\{\text{Acc}_{\text{p}}(\mathcal{A}, S, i, j)\}_{j\in[100]}$ (Above the dashed line) and $\{\text{Acc}_{\text{p}}(\mathcal{A}, \widetilde{S}, i, j)\}_{j\in[100]}$ (Below the dashed line). Similarly, by referring to the wide range of box plotted distributions, Observation \ref{obs1} holds for more tail sub-populations as well. Besides, the variance and the extremer of the changes in the test accuracy 
are much larger in the view of sub-populations. 

\begin{figure}[!ht]
    \centering
    \vspace{0.1in}
    \includegraphics[width=\textwidth]{figures/influence/acc_fine_00_new.pdf}
    
    ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------
    \includegraphics[width=\textwidth]{figures/influence/acc_fine_02_new.pdf}
    \vspace{-0.25in}
    \caption{(Complete version) Box plot of the population-level test accuracy changes when removing all samples of a selected long-tailed sub-population during the training w.r.t. 4 methods. (Above the dashed line: trained on clean labels; Below the dashed line: trained on noisy labels.)}\label{fig:clean_sub}
    \end{figure}
    

\paragraph{Influences on Sample-Level Prediction Confidence}
Remember that we characterize the influence of a sub-population on a test sample as: 
\begin{align*}
    \text{Infl}(\mathcal{A}, \widetilde{S}, i, j):=&\P_{f\leftarrow \mathcal{A}(\widetilde{S})} (f(x'_j)=y'_j) -\P_{f\leftarrow \mathcal{A}(\widetilde{S}^{\backslash i})} (f(x'_j)=y'_j),
\end{align*}
where in the above two quantities, $f\leftarrow\mathcal{A}(\widetilde{S})$ denotes that the classifier $f$ is trained from the whole noisy training dataset $\widetilde{S}$ via Algorithm $\mathcal{A}$, $f\leftarrow \mathcal{A}(\widetilde{S}^{\backslash i})$ means $f$ got trained on $\widetilde{S}$ without samples in the sub-population $\mathcal{G}^{(i)}$. And $\text{Infl}(\mathcal{A}, \widetilde{S}, i, j)$ quantifies the influence of a certain sub-population on specific test data.
 As shown in Figure \ref{fig:influence_clean}, we visualize $\text{Infl}(\mathcal{A}, S, i, j)$ (1st row) and $\text{Infl}(\mathcal{A}, \widetilde{S}, i, j)$ (2nd row), where $j\in [10000]$ means 10K test samples. With the presence of label noise, Observations \ref{obs2} holds for more tail sub-population as well.
\begin{figure*}[!ht]
    \centering
    \vspace{-0.1in}
    \includegraphics[width=\textwidth]{figures/influence/influence_00.pdf} ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------
    \includegraphics[width=\textwidth]{figures/influence/influence_02.pdf}\vspace{-0.1in}
    \caption{(Complete version) Distribution plot w.r.t. the changes of model confidence on the test data samples using CE loss and label smoothing (Above the dashed line: trained on clean labels; Below the dashed line: trained on noisy labels).}\label{fig:influence_clean}
    \vspace{-0.1in}
    \end{figure*}


\subsection{Experiment Details on CIFAR Datasets}\label{app:exp_details}

\paragraph{Hyper-Parameter Settings} 
For each baseline method, we adopt mini-batch size 128, optimizer SGD, initial learning rate 0.1, momentum 0.9, weight decay 0.0005, number of epochs 200. As for the learning rate scheduler, we followed \cite{cui2019class} and chose a linear warm-up of learning rate \cite{goyal2017accurate} in the first 5 epochs, then decay 0.01 after the $160$-th epoch and $180$-th epoch. Standard data augmentation is applied to each synthetic CIFAR dataset. We did not make use of any advanced re-sampling strategies or data augmentation techniques. All experiments run on a cluster of Nvidia RTX A5000 GPUs.

\paragraph{The Value of $\lambda$ in FR (KNN)}
We tuned the performance of FR (KNN) w.r.t. a set $\{0.0, 0.1, 0.2, 0.4, 0.6, 0.8, 1.0, 2.0\}$, where $\lambda=0.0$ indicates the training of baseline methods without \textbf{FR}. Regarding the reported results in the main paper: for all methods w.r.t. CIFAR-100 dataset, we set $\lambda=0.1$ since calculating the accuracy of tail sub-populations may be unstable. As for methods on  CIFAR-10 Imb noise, we set $\lambda=1.0$ for CE loss, NLS, Focal loss and Peer Loss. One exception is that LS requires a relative small $\lambda$, i.e., $\lambda=0.4$. Also, we observe that for Imb noise, a larger $\lambda$ could be more beneficial for CE loss and Logit-adj loss under a higher noise regime. For methods on  CIFAR-10 Sym noise, the $\lambda$ selection for LS, NLS, PeerLoss remains the same as that in the Imb setting. For CE and Focal loss, a larger $\lambda$ (i.e., $\lambda=2.0$) could be more beneficial. While Logit-adj prefers a smaller $\lambda$ (i.e., $\lambda=0.5$).

\paragraph{The Value of $\lambda$ in FR (G2)}
Since there are only two sub-populations considered, the experiment results on CIFAR-100 would be more stable than FR (KNN), hence a larger $\lambda$ could be utilized. For CE loss, NLS and Focal loss, we adopt $\lambda=1.0$ for all CIFAR-10 experiments and $\lambda=2.0$ for all CIFAR-100 experiments. For LS, we set $\lambda=0.4$ for all experiments, except for the extreme case (CIFAR-100 with large noise $\rho=0.5$), where we decide on a larger $\lambda$ (0.8). As for PeerLoss, we have to set a relatively small $\lambda$ (i.e., $\lambda=0.8$) for CIFAR-10 experiments and an even smaller one ($\lambda=0.2$) on CIFAR-100 due to the scale of its loss. And we set $\lambda=1.0$ for Logit-adj under all settings. We do observe better results when adopting varied $\lambda$ under each setting, but fixing the $\lambda$ for reporting under a specific dataset tends is more convenient and practical.


\subsection{Experiment Details on Clothing1M Dataset}\label{app:exp_details_c1m}
We adopt the same baselines as reported in CIFAR experiments for Clothing1M. 
 All methods use the pre-trained ResNet50, optimizer SGD, momentum 0.9,
and weight decay 1e-3. The initial learning rate is 0.01, then it decays 0.1 for every 30 epochs so there are 120 epochs in all. Negative Label Smoothing (NLS) \cite{wei2021understanding} resumes the last epoch training of CE, and proceeds to train with NLS for another 40 epochs (learning rate 1e-7).


\subsection{Influences of head sub-populations on test data}\label{app:exp} 
Table \ref{tab:head} briefly introduces the influences of head populations ($>500$ samples) on the overall test accuracy. Clearly, the mentioned 5 head populations have different impacts: with the presence of label noise, Pop-06 becomes harmful while Pop-02 and Pop-04 remain helpful.  
\begin{table}[!htb]
~\vspace{-0.2in}
\centering
\caption{Influences of head populations on the overall test accuracy.}
\scalebox{0.8}{
\begin{tabular}{c|ccccc}
\hline 
\shortstack{{Clean} \\\textbf{}} & \shortstack{{Remove} \\{Pop-02}} & \shortstack{{Remove} \\{Pop-04}}& \shortstack{{Remove} \\{Pop-03}}& \shortstack{{Remove} \\{Pop-06}}& \shortstack{{Remove} \\{Pop-00}} \\
\hline \hline 
Test Acc & {\color{red}-4.06} & {\color{red}-3.09}  & {\color{red}-0.96}  & {\color{red}-0.15} & +0.29 \\
\hline
\hline 
\shortstack{{Noisy} \\\textbf{$\rho=0.2$}} & \shortstack{{Remove} \\{Pop-02}} & \shortstack{{Remove} \\{Pop-04}}& \shortstack{{Remove} \\{Pop-03}}& \shortstack{{Remove} \\{Pop-06}}& \shortstack{{Remove} \\{Pop-00}} \\
\hline \hline 
Test Acc & {\color{red}-3.75}& {\color{red}-4.80}  & {\color{red}-3.79}  & +0.62  & +0.18 \\
\hline
\end{tabular}}\label{tab:head}
\vspace{-0.2in}
\end{table}

	