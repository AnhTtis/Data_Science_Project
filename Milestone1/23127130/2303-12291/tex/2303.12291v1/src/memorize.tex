\section{Disparate Influences of Sub-Populations: An Empirical Study}
\label{sec:emp_analysis}
In this section, we empirically illustrate the disparate influence of sub-populations when learning with noisily labeled data. Inspired by the literature on using the influence function to capture the impact of a subset of training data, we define influence metrics at the sub-population level and perform a multi-faceted evaluation of how the imbalanced sub-populations affect the learning performance. 
We take the long-tailed sub-populations for illustration and defer the results of head populations to Appendix \ref{app:exp}. 

\textbf{Influences:}
In the literature of explainable deep learning, the notions of influence can be different, e.g., the influences of features on an individual sample prediction \cite{ribeiro2016should,sundararajan2017axiomatic, lundberg2017unified, feldman2020neural}, the influences of features on the loss/accuracy of the model \cite{owen2017shapley,owen2014sobol}, the influences of training samples on the loss/accuracy of the model \cite{jia2019towards}. In this section, we focus on the influence of a sub-population on both the sub-population level and the individual sample level.

We now empirically demonstrate the role of sub-populations when measuring the test accuracy, and the prediction of model confidence on test samples. For the synthetic long-tailed noisy training dataset, we first flip clean labels of the class-balanced CIFAR-10 dataset to any other classes, and there exist 20\% wrong labels in all. We then adopt the class-imbalanced \cite{cui2019class} CIFAR-10 dataset to select a long-tailed distributed amount of samples for each class (by referring to clean labels). As for the separation of sub-populations, we adopt the $k$-means clustering to categorize the extracted features of each feature given by the Image-Net pre-trained model. Since sub-population information sometimes may not be available for training use, understanding the influences of such division of sub-populations is beneficial. More separation details and experiment designs could be found at \ref{sec:exp_detail}. 

We explore the influences of tail sub-populations on performances of cross-entropy (ce) loss, the forward loss correction (fw) \cite{patrini2017making}, label smoothing (ls) \cite{lukasik2020does}, and the peer loss (pl) \cite{liu2020peer}. There are 17 sub-populations (train) with less than 50 instances considered as the tail section.

We illustrate observations on several randomly selected tail sub-populations. Results of more sub-populations are deferred to Appendix  \ref{app:more}.


\subsection{Influences on Sub-Population Level (Test Accuracy)}
We start with the influence of sub-populations in the test set. We adopt the (population-level) test accuracy changes when removing all samples in the sub-population $\mathcal G_i$ during the training procedure to capture the influences of a sub-population on each sub-population at the test set: 
\begin{mybox1}
\begin{align*}
&\text{Acc}_{\text{p}}(\mathcal{A}, \widetilde{S}, i, j)=\P_{\substack{f\leftarrow \mathcal{A}(\widetilde{S})\\
  (X', Y', G=j)}} (f(X')=Y') - \P_{\substack{f\leftarrow \mathcal{A}(\widetilde{S}^{\backslash i})\\
   (X', Y', G=j)}} (f(X')=Y'),
\end{align*}
\end{mybox1}
where in the above two quantities, $f\leftarrow\mathcal{A}(\widetilde{S})$ indicates that the classifier $f$ is trained from the whole noisy training dataset $\widetilde{S}$ via Algorithm $\mathcal{A}$, $f\leftarrow \mathcal{A}(\widetilde{S}^{\backslash i})$ means $f$ is trained on $\widetilde{S}$ without samples in the sub-population $\mathcal{G}_i$. $(X', Y', G=j)$ denotes the test data distribution given that the samples are from the $j$-th sub-population. 

In Figure \ref{fig:clean_class}, the $x$-axis denotes the loss function for training, and the $y$-axis visualized the distribution of $\{\text{Acc}_{\text{p}}(\mathcal{A}, S, i, j)\}_{j\in[100]}$ (top) and $\{\text{Acc}_{\text{p}}(\mathcal{A}, \widetilde{S}, i, j)\}_{j\in[100]}$ (bottom) for several long-tailed sub-populations ($i=52, 70, 91$) under each robust method, where "$S$" refers to the clean training samples and "$\widetilde{S}$" denotes the noisy version. The blue zone shows the 25-th percentile ($Q_1$) and 75-th percentile ($Q_3$) accuracy changes, and the orange line indicates the median value. Accuracy changes that are drawn as circles are viewed as outliers. Note that all sub-figures have the same limits for $y$-axis. It is clear to observe the top three figures have lower variance than the bottom ones, indicating that: \begin{mybox}
\begin{observation}\label{obs1}
Compared with clean training, tail sub-populations in noisy training tend to have a more significant influence on the test accuracy. 
\end{observation}
\end{mybox}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/influence/acc_fine_00_part_new.pdf}\hspace{0.2in}
    \includegraphics[width=0.48\textwidth]{figures/influence/acc_fine_02_part_new.pdf}
    \caption{Box plot of the population-level test accuracy changes when removing all samples of a selected long-tailed sub-population during the training w.r.t. 4 methods. (Left: trained on clean labels; Right: trained on noisy labels.)}\label{fig:clean_class}
    \vspace{-0.1in}
    \end{figure}

\subsection{Influences on Sample Level (Prediction Confidence)}
Note that grouping testing samples into classes/sub-populations for analysis may ignore some individual behavior changes, we next consider the influence of sub-populations on the individual test samples. Instead of insisting on the accuracy measure, we adopt the model prediction confidence as a proxy, to see how each test sample got influenced. And we introduce $\text{Infl}(\mathcal{A}, \widetilde{S}, i, j)$ to quantify the influence of a sub-population on a specific test sample:
\vspace{-5pt}
\begin{mybox3}
\vspace{-5pt}
\begin{align*}
    &\text{Infl}(\mathcal{A}, \widetilde{S}, i, j)=\P_{f\leftarrow \mathcal{A}(\widetilde{S})} (f(x'_j)=y'_j) -\P_{f\leftarrow \mathcal{A}(\widetilde{S}^{\backslash i})} (f(x'_j)=y'_j).
\end{align*}
\end{mybox3}

As shown in Figure \ref{fig:influence_clean_part}, we visualize $\text{Infl}(\mathcal{A}, S, i, j)$ (Top) and $\text{Infl}(\mathcal{A}, \widetilde{S}, i, j)$ (Bottom), where $j\in [10000]$ means 10K test samples. For example, $\text{Infl}(\mathcal{A}, \widetilde{S}, i, j)=-1$ means the model prediction confidence on test sample $x'_j$ changed from $0$ to $1$. With the presence of label noise, we observe:
\begin{mybox2}\label{obs2}
\begin{observation}
    Compared with clean training, removing certain tail sub-populations lead to significant changes/influences on the model prediction confidence of more test samples. 
\end{observation} 
\end{mybox2}



\begin{figure}[!htb]
    \centering
\includegraphics[width=0.48\textwidth]{figures/influence/influence_00_part_new.pdf}\hspace{0.2in}
    \includegraphics[width=0.48\textwidth]{figures/influence/influence_02_part_new.pdf}
    \caption{Distribution plot w.r.t. the changes of model confidence on the test data samples using CE loss (Left: trained on clean labels; Right: trained on noisy labels). See Appendix~\ref{app:more} for more details. Removing population 91 appears to result in much worse model prediction confidence on test samples.}\label{fig:influence_clean_part}
    \end{figure}


