\section{Experiments}
\label{sec:exp}

In this section, we verify the effectiveness of \fr{}  on the synthetic long-tailed noisy CIFAR datasets \cite{krizhevsky2009learning} and real-world large-scale noisily labeled long-tailed dataset CIFAR-10N \cite{wei2022learning}, CIFAR-20N \cite{wei2022learning}, CIFAR-100N \cite{wei2022learning}, Animal-10N \cite{song2019selfie}, and Clothing1M \cite{xiao2015learning}. 


\subsection{Experiment Designs on Synthetic Noisily Labeled Long-Tailed CIFAR Datasets}\label{sec:exp_detail}
We empirically test the performance of \fr\ on CIFAR-10 and CIFAR-100 datasets \cite{krizhevsky2009learning}. The original CIFAR-10 \cite{krizhevsky2009learning} dataset contains 60k 32 Ã— 32 color images, 50k images for training, and 10k images for testing. The dataset is balanced and each image belongs to one of ten completely mutually exclusive classes. CIFAR-100 dataset shares the same statistics, except for containing 100 completely mutually exclusive classes.


\paragraph{Generation of Synthetic Long-Tailed Data with Noisy Labels}

Note that the class information could be viewed as a special case of sub-populations, in this subsection, we treat classes as the natural separation of sub-populations and consider the class-imbalance experiment setting with noisy labels. For the balanced $K$-class  classification task with $n$ samples per class, the synthetic long-tail setting assumes that $k-$th class has only $n/(r^{\frac{k-1}{K-1}})$ samples by referring to the ground-truth labels \cite{cui2019class}. We adopt two label-noise transition models below.

\textbf{Model 1 (Imb):} The entries of the noise transition matrix are given by:
$$T_{i, j}:=\p(\nY=j|Y=i, X=x)=\begin{cases}
  1-\rho, & i=j, \\
\frac{\p(Y=j)\cdot \rho}{1-\p(Y=i)}, & i\neq j,
\end{cases}$$
where $\rho$ is viewed as the overall error/noise rate. The Imb noise model \cite{wei2021robust} assumes that samples are more likely to be mislabeled as frequent ones in real-world situations. 

\textbf{Model 2 (Sym):} 
The generation of the symmetric noisy dataset is adopted from \cite{kim2019nlnl}, where it assumed that $T_{i,j}=\frac{\rho}{K-1}, \forall i\neq j$, indicating that any other 
classes $i \neq j$ has the same chance of being flipped to class $j$. The diagonal entry $T_{i,i}$ (chance of a correct label) becomes $1-\rho$. 

For both noise settings, we test \fr{ }with noise rates $\rho\in \{0.2, 0.5\}$, meaning the proportion of wrong labels in the long-tailed training set is $0.2$ or $0.5$.  

\paragraph{Separation of $\mathcal G_i$} For the experiments w.r.t. \fr, we consider two kinds of sub-population separation methods.
\squishlist
    \item \textbf{Separation with Clustering Methods:} $\forall x\in X$, the representation of feature $x$ is given by the representation extractor $\phi(x)$, where $\phi(\cdot): X\to \mathbb{R}^d$ maps the feature $x$ to a $d$-dimensional representation vector. Given a distance metric $\text{DM}$ (i.e., the Euclidean distance), the distance between two extracted representations $\phi(x_1), \phi(x_2)$ is $\text{DM}(\phi(x_1), \phi(x_2))$. The sub-population could be separated through clustering algorithms such as $k$-means ($k=N$ here). Admittedly, obtaining a good representation extractor is non-trivial \cite{zhu2022detecting}, we want to highlight that the separation of sub-populations is not highly demanding on the quality of the representation extractor, and the focus is to perform fairness regularizations on varied features.
    \item \textbf{Separation Directly via Pre-Trained Models:} In this case, $\forall x\in X$, we adopt an (Image-Net) pre-trained model for the separation, i.e., such a feature extractor $\phi(\cdot)$ maps each $x$ into a $d=N$-dimensional representation vector so that all features are automatically categorized into $N$ sub-populations.
\squishend


\subsection{Experiment Results on Synthetic Noisily Labeled Long-Tailed CIFAR Datasets}

\begin{table*}[!tb]
	\caption{Performance comparisons on synthetic long-tailed noisy CIFAR datasets (noise type: imbalance-noise \& symmetric noise), best-achieved test accuracy are reported. Results in \textbf{bold} (and green-colored) mean \fr{} improves the performance of the baseline methods, respectively.}
	\begin{center}
    \scalebox{0.78}{\begin{tabular}{c|ccc|ccc|ccc|ccc}
    \hline \multicolumn{13}{c}{\textbf{Noise type: Imbalance Noise}}\\
			\hline 
	Noise Ratio  & \multicolumn{3}{c}{CIFAR-10 ($\rho=0.2$)}  & \multicolumn{3}{c}{CIFAR-10 ($\rho=0.5$)}  & \multicolumn{3}{c}{CIFAR-100 ($\rho=0.2$)}  & \multicolumn{3}{c}{CIFAR-100 ($\rho=0.5$)} 
	\\\hline
	Imbalance Ratio
				  &$r=10$  &$r=50$ 
  &$r=100$ 
				  &$r=10$  &$r=50$ 
  &$r=100$ 
				  &$r=10$  &$r=50$ 
  &$r=100$ 
				  &$r=10$  &$r=50$ 
  &$r=100$ \\
				\hline\hline
				CE&79.75 & 65.98 & 60.03 & 65.38 & 47.51 & 37.44 & 46.02 & 31.44 & 26.98 & 29.58 & 16.93 & 13.87 \\
    CE + FR (KNN)  &\good{}\textbf{80.46} & \good{}\textbf{69.00} & \good{}\textbf{61.64} & \good{}\textbf{65.87}  & 46.69 & \good{}\textbf{39.97} & \good{}\textbf{46.18}  & 31.03 & \good{}\textbf{27.60} & \good{}\textbf{30.25}  & \good{}\textbf{16.79} & \good{}\textbf{15.19} \\
    CE + FR (G2)  & \good{}\textbf{80.44} & \good{}\textbf{67.29} & \good{}\textbf{65.12}& \good{}\textbf{68.62} & \good{}\textbf{49.43}&\good{}\textbf{39.69} & \good{}\textbf{46.38} & \good{}\textbf{32.32} & \good{}\textbf{28.53}& \good{}\textbf{32.35} & \good{}\textbf{19.03}& \good{}\textbf{15.93} \\ 
				
		   	\hline
		   	LS \cite{lukasik2020does}&82.52 & 69.08 & 59.07 & 67.73 & 36.17 & 32.92 & 47.80 & 33.66 & 26.36 & 34.02 & 17.28 & 14.10\\
      LS + FR (KNN) & \good{}\textbf{82.78} & \good{}\textbf{70.06} & \good{}\textbf{59.27}&\good{}\textbf{68.99}& \good{}\textbf{36.55}& \good{}\textbf{36.63} &\good{}\textbf{48.27}  & 33.01& \good{}\textbf{27.60}& 32.01& 17.14& 14.07\\
      LS + FR (G2)  & 82.02   & \good{}\textbf{70.24}& \good{}\textbf{60.33}& \good{}\textbf{70.50}  & \good{}\textbf{44.11}&\good{}\textbf{35.49}& 47.30  & \good{}\textbf{33.86} &\good{}\textbf{29.67}& \good{}\textbf{34.51}  & \good{}\textbf{17.84}& \good{}\textbf{16.68}\\
		   	\hline
		   	NLS \cite{wei2021understanding}&79.91 & 65.98 & 58.82 & 64.74 & 41.01 & 34.16 & 46.05 & 31.48 & 27.09 & 29.86 & 16.84 & 13.87\\
      NLS + FR (KNN)& \good{}\textbf{80.17}& \good{}\textbf{68.61} & \good{}\textbf{62.88}& \good{}\textbf{68.65}& \good{}\textbf{47.42}& \good{}\textbf{36.79}& 45.72& \good{}\textbf{32.25}& 27.01 & 28.85& \good{}\textbf{17.23}& \good{}\textbf{14.18}\\
     NLS + FR (G2) & \good{}\textbf{80.36}  & \good{}\textbf{68.25} & \good{}\textbf{63.50} &\good{}\textbf{69.70}     & \good{}\textbf{49.01} & \good{}\textbf{38.26} & 43.15   & \good{}\textbf{33.78} & \good{}\textbf{28.69} & \good{}\textbf{32.30}   & \good{}\textbf{19.62} &  \good{}\textbf{15.64}\\
		   	\hline
      Focal \cite{lin2017focal} &76.24 & 64.16 & 57.68 & 62.40 & 40.25 & 34.56 & 43.63 & 29.10 & 24.88 & 26.93 & 14.45 & 12.57 \\
       Focal + FR (KNN) & \good{}\textbf{77.54}   & 62.97 & 57.24 & 61.47   & \good{}\textbf{42.28} & \good{}\textbf{37.04} & 42.44   & 28.90 & \good{}\textbf{25.14} & \good{}\textbf{28.34} & \good{}\textbf{16.02} & \good{}\textbf{13.27}\\
       Focal + FR (G2)  & \good{}\textbf{78.56}   & \good{}\textbf{66.07}& 56.55& \good{}\textbf{64.10}  & \good{}\textbf{43.61}& \good{}\textbf{38.15}& \good{}\textbf{45.63}   &\good{}\textbf{31.87}  &  \good{}\textbf{27.58}& \good{}\textbf{29.80} & \good{}\textbf{17.67} &\good{}\textbf{15.30} \\
       \hline
		   	PL \cite{liu2020peer} &78.43 & 55.61 & 54.20 & 47.71 & 31.96 & 30.13 & 45.32 & 33.05 & 29.91 & 28.01 & 20.25 & 16.65\\
      PL + FR (KNN) & \good{}\textbf{79.50} & \good{}\textbf{65.37}   & 53.36 & \good{}\textbf{51.82}   & \good{}\textbf{35.68}   & \good{}\textbf{30.16} & 44.89   & \good{}\textbf{33.12} & 28.63 & 27.66 & 19.79 & \good{}\textbf{17.72}\\
      PL + FR (G2) & \good{}\textbf{78.79} & \good{}\textbf{66.16} & \good{}\textbf{54.39} & \good{}\textbf{50.72}  & \good{}\textbf{33.22} &  28.01 &   44.78  & \good{}\textbf{33.35}  & 29.51  & \good{}\textbf{29.82}  &  20.15  & \good{}\textbf{16.81}    \\
		   	\hline
      Logit-adj \cite{menon2021longtail} &82.09 & 73.23 & 68.18 & 68.30 & 51.51 & 42.17 & 47.28 & 33.11 & 29.47 & 30.92 & 17.97 & 14.68\\
      Logit-adj + FR (G2)  & \good{}\textbf{82.92}   & \good{}\textbf{75.67} & \good{}\textbf{72.20}& \good{}\textbf{73.72}  &  \good{}\textbf{55.09} & 40.85   &  41.21    & \good{}\textbf{35.39} &  28.84 &   27.57  
    & \good{}\textbf{18.93}  & \good{}\textbf{15.44}   \\
    Logit-adj + FR (KNN) & \good{}\textbf{82.48}   & \good{}\textbf{73.65} & \good{}\textbf{68.48} & \good{}\textbf{70.89}  & 49.23 &\good{}\textbf{42.93} & \good{}\textbf{47.66}  & \good{}\textbf{33.18} & \good{}\textbf{29.50} & \good{}\textbf{31.85} & 17.59 & \good{}\textbf{15.25} 
    \\
		   	\hline
      
    \hline \multicolumn{13}{c}{\textbf{Noise type: Symmetric Noise}}\\
			\hline 
	Noise Ratio  & \multicolumn{3}{c}{CIFAR-10 ($\rho=0.2$)}  & \multicolumn{3}{c}{CIFAR-10 ($\rho=0.5$)}  & \multicolumn{3}{c}{CIFAR-100 ($\rho=0.2$)}  & \multicolumn{3}{c}{CIFAR-100 ($\rho=0.5$)} 
	\\\hline
	Imbalance Ratio
				  &$r=10$  &$r=50$ 
  &$r=100$ 
				  &$r=10$  &$r=50$ 
  &$r=100$ 
				  &$r=10$  &$r=50$ 
  &$r=100$ 
				  &$r=10$  &$r=50$ 
  &$r=100$ \\
				\hline\hline
				CE&80.70 & 65.04 & 61.80 & 70.48 & 51.53 & 36.44 & 46.02 & 30.93 & 26.98 & 29.93 & 16.70 & 4.76\\
    CE + FR (KNN)   &\good{}\textbf{81.19} & \good{}\textbf{69.95} & 63.97 & \good{}\textbf{71.75}   & \good{}\textbf{52.93} & \good{}\textbf{45.63}& \good{}\textbf{46.33}  & 30.82 & \good{}\textbf{27.19} & \good{}\textbf{31.12} & \good{}\textbf{17.68} & \good{}\textbf{15.39}\\
    CE + FR (G2)   & \good{}\textbf{81.64} & \good{}\textbf{70.84}& \good{}\textbf{65.14}& \good{}\textbf{71.44}  &\good{}\textbf{56.50}  & \good{}\textbf{46.33}&\good{}\textbf{47.70}   & \good{}\textbf{34.34} & \good{}\textbf{30.78}&\good{}\textbf{31.58}  &\good{}\textbf{21.70} & \good{}\textbf{19.10}\\
		   	\hline
		   	LS \cite{lukasik2020does}&83.23 & 71.69 & 65.69 & 72.85 & 50.59 & 30.98 & 47.90 & 33.81 & 29.95 & 26.56 & 21.74 & 19.39\\
      LS + FR (KNN)& \good{}\textbf{83.28}  & 70.64 & 60.91 & \good{}\textbf{73.92}  & \good{}\textbf{53.01} & \good{}\textbf{43.48}& \good{}\textbf{49.05} &  33.40 & \good{}\textbf{30.05} & \good{}\textbf{34.86} & 20.73& 19.10\\
      LS + FR (G2) & 82.22   & 70.85 & 62.43 & \good{}\textbf{74.59}  & \good{}\textbf{54.15} & \good{}\textbf{44.77} & \good{}\textbf{48.16}   & \good{}\textbf{34.08} & \good{}\textbf{30.69} & \good{}\textbf{36.40}  & \good{}\textbf{22.06} & \good{}\textbf{20.10} \\
		   	\hline
		   	NLS \cite{wei2021understanding}&80.79 & 66.22 & 61.47 & 70.11 & 50.57 & 36.55 & 46.11 & 31.14 & 27.32 & 30.51 & 17.16 & 5.18\\
      NLS + FR (KNN)& \good{}\textbf{81.08}  & \good{}\textbf{69.29}& \good{}\textbf{63.58} & \good{}\textbf{70.27}& \good{}\textbf{54.86} & 36.50& \good{}\textbf{48.20} & \good{}\textbf{35.03}& \good{}\textbf{28.29}& 28.87& \good{}\textbf{19.10}& \good{}\textbf{6.65}\\
     NLS + FR (G2) & \good{}\textbf{81.37}  & \good{}\textbf{70.60} & \good{}\textbf{64.73} & \good{}\textbf{71.30}  & \good{}\textbf{56.24} &  \good{}\textbf{37.29}& \good{}\textbf{47.67}   & \good{}\textbf{34.32} & \good{}\textbf{30.75} & 29.62   & \good{}\textbf{22.17} & \good{}\textbf{8.04}\\
		   	\hline
      Focal \cite{lin2017focal}&77.77 & 61.54 & 56.02 & 67.20 & 43.12 & 38.20 & 35.93 & 23.23 & 21.84 & 27.31 & 16.18 & 14.71\\
       Focal + FR (KNN) & \good{}\textbf{78.03}   & \good{}\textbf{64.57} & \good{}\textbf{56.77} & \good{}\textbf{67.87}   & 41.89 & 36.34 & \good{}\textbf{42.79}   & \good{}\textbf{30.17} & \good{}\textbf{25.08} & \good{}\textbf{28.22} & \good{}\textbf{16.37} & 14.50\\
       Focal + FR (G2) & \good{}\textbf{78.83}   & \good{}\textbf{65.56} & \good{}\textbf{60.35} & \good{}\textbf{68.21}   & \good{}\textbf{47.09} & \good{}\textbf{41.74} & \good{}\textbf{46.33}   & \good{}\textbf{32.56} & \good{}\textbf{27.77} & \good{}\textbf{29.70} & \good{}\textbf{16.47} & \good{}\textbf{15.29}  \\
       \hline
		   	PL \cite{liu2020peer}&79.73 & 66.82 & 42.12 & 55.52 & 33.18 & 33.06 & 44.60 & 32.91 & 28.69 & 27.38 & 18.52 & 17.25\\
      PL + FR (KNN) & 79.42   & 64.91 & \good{}\textbf{58.80} & 53.86   & \good{}\textbf{38.41} & 32.71 & \good{}\textbf{45.60}  & 32.32& 28.34 & \good{}\textbf{27.63} & \good{}\textbf{18.86} & 16.48\\
      PL + FR (G2) & 79.37   & 66.71 & \good{}\textbf{58.98} & \good{}\textbf{55.68} & \good{}\textbf{38.08} & \good{}\textbf{33.52} & \good{}\textbf{46.83} & \good{}\textbf{33.17}  & \good{}\textbf{29.67} & \good{}\textbf{28.12} & \good{}\textbf{19.48} & \good{}\textbf{17.62}  \\
		   	\hline
      Logit-adj \cite{menon2021longtail} &80.50 & 62.42 & 50.28 & 60.38 & 32.45 & 27.32 & 46.50 & 29.24 & 23.80 & 28.79 & 12.65 & 9.22\\
     Logit-adj + FR (KNN) & \good{}\textbf{80.66}  & 62.07 & \good{}\textbf{51.04} & \good{}\textbf{62.32}   & 31.23 & 22.41 & \good{}\textbf{47.22}   & \good{}\textbf{29.34} & \good{}\textbf{24.70} & \good{}\textbf{29.95} & 12.44 &\good{}\textbf{9.28}\\
      Logit-adj + FR (G2)  & \good{}\textbf{81.82}   & \good{}\textbf{62.62} & \good{}\textbf{52.35} & \good{}\textbf{63.34}   & 31.14 & 21.93 & \good{}\textbf{48.13}  & \good{}\textbf{30.18} & \good{}\textbf{24.06} & \good{}\textbf{29.35}  & 12.37 & \good{}\textbf{9.26} \\
		   	\hline
	\end{tabular}}
			\end{center}\label{tab:cifar}
			\vspace{-0.12in}
	\end{table*}
 
 
In Table~\ref{tab:cifar}, we empirically show how \fr{} helps with improving the classifier's performance when complemented with several methods in robust losses as well as approaches in class-imbalanced learning, under synthetic class-imbalanced CIFAR datasets with noisy labels, including Cross-Entropy loss (CE), Label Smoothing (LS) \cite{lukasik2020does}, Negative Label Smoothing (NLS) \cite{wei2021understanding}, Focal Loss \cite{lin2017focal}, PeerLoss (PL) \cite{liu2020peer}, and Logit-adjustment (Logit-adj) \cite{menon2021longtail}. We fix the same training samples and labels for all methods. More details are available in Appendix \ref{app:exp_details}.

For \fr{}, we adopted the fixed $\lambda$ for all sub-populations. We consider two types of sub-population separation methods: (i) KNN clustering, which splits the extracted features into $K$ clusters, with $K$ being the number of classes; (ii) Generate the separation by referring to the direct prediction made by an (Image-Net) pre-trained model. In our experiments, this method separates features into a head and a tail sub-population, and the ratio w.r.t. the amount of samples between two sub-populations is usually close to 5.


\paragraph{Results}
In Table \ref{tab:cifar}, we provide the baseline performance as well as the corresponding performances when \textbf{FR} is introduced. \textbf{FR} (KNN) denotes the scenario where we adopt the KNN clustering for sub-population separation, and the number of sub-populations is the same as the number of classes. We did not consider the noisy (class) labels as the sub-population index due to the fact that the noisy labels may contain the wrong ones.  Empirically, we observe that \textbf{FR} (KNN) consistently improves the baseline methods on the class-imbalanced CIFAR-10 dataset, under the Imb and Sym noise. However, \textbf{FR} (KNN) could not improve significantly on the class-imbalanced CIFAR-100 dataset. One reason is that, in the batch update, the number of samples in each sub-population is too small (the average number is $128/100=1.28$), resulting in large variance in calculating \fr\ as Eqn.~(\ref{eq:relax_constraint}). As an alternative, we report the performance of \textbf{FR} (G2) as well, where samples are categorized into 2 sub-populations by the (Image-Net) pre-trained model. Surprisingly, \textbf{FR} (G2) improves the performance of 6 baselines in most settings, as highlighted in Table~\ref{tab:cifar}. Thus, constraining the classifier to have  relatively fair performances improves learning from noisy and long-tailed data. 

We further adopt the CIFAR-10 dataset ($\rho=0.5, r=50$) and visualize how \fr{} influences the per-class accuracy by referring to the performance of each baseline. Each blue point in Figure~\ref{fig:acc_com_10} indicates the scenario where \fr{} improves the test accuracy of a class over the corresponding baseline. Points in the lower left corner (where tail populations are usually located) further illustrate that \fr{} consistently improves the performance of tail sub-populations.
\begin{figure*}[!htb]
    \centering
\includegraphics[width=\textwidth]{figures/visualize2.png}
    \caption{How \fr{} improves per class test accuracy w.r.t. the baseline method on CIFAR-10. In each sub-figure, the $x$-axis indicates the accuracy of a baseline. $y$-axis denotes the performance of baseline when \fr{} is introduced.
    Each dot denotes the test accuracy pair $(\text{Acc}_{\text{Method}}, \text{Acc}_{\text{Method+FR}})$ for each sub-population. The black line $y=x$ stands for the case that \fr{} has no effects on a particular sub-population.
    The {\color{blue}blue} ({\color{red}red}) dot {\color{blue}above} ({\color{red}below}) the line shows the robust treatment has {\color{blue}positive} ({\color{red}negative}) effect on this sub-population compared with CE.}
    \label{fig:acc_com_10}
    \vspace{-0.15in}
    \end{figure*}
    
We next adopt hypothesis testing to demonstrate the effectiveness of \fr. 



\paragraph{Hypothesis Testing w.r.t. FR}
We adopt paired student t-test to verify the conclusion that \fr{} helps with improving the test accuracy. Briefly speaking, for each dataset and each baseline method, we statistically test whether \fr{} results in significant test accuracy improvements in Table \ref{tab:cifar}. 

Denote by $\text{PA}_{\text{Method}}^{\rho, r}:=(\text{Acc}_{\text{Method}}^{\rho, r}, \text{Acc}_{\text{Method+FR}}^{\rho, r})$ the Paired Accuracies without/with \fr{} under each setting, i.e., when $\rho=0.2, r=10$, Method$=$CE, we have: $\text{PA}_{\text{Method}}^{\rho, r}=(79.75, 80.46)$ (w.r.t. FR (KNN)). The null and alternative hypotheses could be expressed as:
\begin{align*}
    \mathbf{H_0}:& \{\text{Acc}_{\text{Method+FR}}^{\rho, r}\}_{\rho, r} ~\text{come from the \emph{same} distribution as }\{\text{Acc}_{\text{Method}}^{\rho, r})\}_{\rho, r};\\
    \mathbf{H_1}:& \{\text{Acc}_{\text{Method+FR}}^{\rho, r}\}_{\rho, r} ~\text{come from \emph{different} distributions as }\{\text{Acc}_{\text{Method}}^{\rho, r})\}_{\rho, r},
\end{align*}
where the above accuracy list $\{\text{Acc}_{\text{Method}}^{\rho, r}\}_{\rho, r}$ includes both noise types (imb \& sym), $\rho\in [0.2, 0.5]$, and $r\in[10, 50, 100]$, thus 12 elements for either dataset, similarly for the accuracy list $\{\text{Acc}_{\text{Method+FR}}^{\rho, r}\}_{\rho, r}$.


In Table \ref{tab:t_test}, positive statistics indicate that the \fr{} generally improves the performance (test accuracy) of the baseline method. The $p$-value that is smaller than 0.1 means there exist significant differences between the two accuracy lists. In such scenarios, we should reject the null hypothesis and adopt the alternative hypothesis. 
Table~\ref{tab:t_test} shows that \fr\ (G2) brings significant performance improvements in most settings (5/6 in CIFAR-10 and 6/6 in CIFAR-100), indicating the effectiveness of our method. Besides, \fr\ (KNN) shows significant performance improvements only in several settings (but there are still improvements in most cases), which can be explained by our previous discussion that a large number of sub-populations may make the learning unstable. More details appear in Appendix \ref{app:exp_details}.


\begin{table}[!tb]
	\caption{Paired student t-test results w.r.t. the effectiveness of \fr. Rows marked with "$\surd$" mean \fr{} improve the performance of the baseline methods significantly ($p$-value satisfies that $p<0.1$ and the statistics is positive); "--" indicates there exist no significant differences after adopting \fr.}
 \vspace{-0.1in}
	\begin{center}
    \scalebox{0.86}{\begin{tabular}{c|c|ccc|ccc}
			\hline 
	  &  & \multicolumn{3}{c}{\textbf{CIFAR-10}}  & \multicolumn{3}{c}{\textbf{CIFAR-100}}  
	\\\hline
	\textbf{Method} & \textbf{FR Type}
				 &\textbf{statistics}  &\textbf{$p$-value}   & \textbf{Better} 
  &\textbf{statistics}  &\textbf{$p$-value} & \textbf{Better}    \\
				\hline\hline
			CE  & FR (KNN) & 2.962 & 0.013 & $\surd$ & 1.489 & 0.165 & --\\ 
			CE  & FR (G2) & 4.313 & 0.001 & $\surd$ & 3.083 & 0.010  & $\surd$ \\ 
		   	\hline
			LS  & FR (KNN) & 1.214 & 0.250 & -- & 0.748 & 0.470 & -- \\ 
			LS  & FR (G2) & 1.851 & 0.091 & $\surd$ &1.926 & 0.080 & $\surd$ \\ 
		   	\hline
			NLS  & FR (KNN) & 4.235 & 0.001& $\surd$  & 1.692 & 0.119 & --\\ 
			NLS  & FR (G2) & 4.909 & <0.000 & $\surd$ & 3.237 & 0.008& $\surd$ \\ 
		   	\hline
			PL  & FR (KNN) & 1.859 & 0.090 & $\surd$ & -0.620 & 0.548 & --\\ 
			PL  & FR (G2) & 1.847 & 0.092 & $\surd$ & 2.345 & 0.039 & $\surd$ \\ 
		   	\hline
			Focal  & FR (KNN) & 0.886 & 0.395 & -- & 2.218 & 0.049 & $\surd$ \\ 
			Focal  & FR (G2) & 5.249 & <0.000& $\surd$  & 4.105 & 0.002& $\surd$ \\ 
		   	\hline
			Logit-adj  & FR (KNN) &1.171 & 0.266 & -- & -0.419 & 0.684 & --\\ 
			Logit-adj  & FR (G2) & 0.255 & 0.803 & -- & 2.410 & 0.035& $\surd$ \\ 
		   	\hline
	\end{tabular}}
			\end{center}\label{tab:t_test}
	\end{table}

\subsection{Experiments on Long-Tailed Data with Real-World Noisy Labels}
We further provide more experiment results on real-world noisily labeled long-tailed data, including long-tailed CIFAR-10N, CIFAR-20N, CIFAR-100N, and Animal-10N.


\paragraph{Dataset Statistics}

Denote by $\rho$ the percentage of wrong labels among the training set, CIFAR-10N \cite{wei2022learning} provides three types of real-world noisy human annotations on the CIFAR-10 training dataset, with $\rho=0.09, 0.18, 0.40$. CIFAR-100N \cite{wei2022learning} provides each CIFAR-100 training image with a human annotation where $\rho=0.40$. In CIFAR-20N \cite{wei2022learning} ($\rho=0.25$), each training/test image includes a coarse label (among 20 coarse classes) and a finer label (among 100 fine classes). We view the coarse label as the class information for training, and we illustrate the effectiveness of all methods by referring to their averaged performance on 100 sub-populations at the test time. Neither the number of sub-population nor the sub-population information of each image is utilized during the training. Animal-10N \cite{song2019selfie} dataset is a ten classes classification task including 5 pairs of confusing animals with a total of 55,000 images. The simulation of long-tailed samples follows the same procedure as the results in Table 1.

\paragraph{Hyperparameters}

The training of CIFAR-10N, CIFAR-20N, and CIFAR-100N is the same as that of synthetic noisy CIFAR datasets. For Animal10N, we adopt VGG19, a different backbone from ResNet. In Animal10N, the settings follow the work \cite{song2019selfie}: we use VGG-19 with batch normalization and the SGD optimizer. The network trained 100 epochs and we adopted an initial learning rate of 0.1, which is divided by 5 at 50\% and 75\% of the total number of epochs. 


\paragraph{Results}

As shown in the three tables below, we report the test accuracy on the class-balanced dataset with clean labels, as we have done in Table 1 of the main paper. We adopt $\lambda=2$ for CE+FR for all settings and $\lambda=1$ for Logit-adj loss for all settings. Experiment results show that FR helps with improving the test accuracy in most settings, given CE loss or logit-adj loss. And we can conclude that constraining the classifier to have relative fairness performances is beneficial when learning with noisy and long-tailed data.


\begin{table*}[!htb]
	\vspace{-0.1in}
	\caption{Performance comparisons on  long-tailed data with real-world noisy datasets. Best-achieved test accuracies are reported. Results in \textbf{bold} (and green-colored) mean \fr{} improves the performance of the baseline methods, respectively.}
	\begin{center}
    \scalebox{0.8}{\begin{tabular}{c|cccccc}
    \hline \multicolumn{7}{c}{\textbf{Noise type: Real-World Human Noise}}\\
			\hline 
	Imbalance Ratio ($r=10$)&CIFAR-10N (Agg) & CIFAR-10N (Rand1) & CIFAR-10N (Worse) & CIFAR-100N  & CIFAR-20N & Animal-10N\\
				\hline\hline
				CE& 83.60	&81.53	&71.83	&43.10	&60.08&	71.18\\
    CE + FR (G2) &\good{}\textbf{83.69}	&\good{}\textbf{81.86}&	\good{}\textbf{74.67}&	\good{}\textbf{44.79}	&\good{}\textbf{61.24}	&\good{}\textbf{72.04} \\ 
		   	\hline
      Logit-adj \cite{menon2021longtail} & 	84.03&	78.85&	64.41	&41.93	&58.98	&67.78\\
      Logit-adj + FR (G2)  &\good{}\textbf{84.88}&	\good{}\textbf{80.37}	&\good{}\textbf{65.15}	&\good{}\textbf{43.48}&	\good{}\textbf{59.89}&	\good{}\textbf{69.64} \\
		   	\hline
      \hline 
	Imbalance Ratio ($r=50$)&CIFAR-10N (Agg) & CIFAR-10N (Rand1) & CIFAR-10N (Worse) & CIFAR-100N  & CIFAR-20N & Animal-10N\\
				\hline\hline
				CE& 74.54	&71.55	&61.36	&32.51	&50.83&	52.60\\
    CE + FR (G2)  &	\good{}\textbf{74.93}	&\good{}\textbf{73.22}&	\good{}\textbf{65.01}	&\good{}\textbf{34.49}	&\good{}\textbf{51.93}	&51.88\\ 
		   	\hline
      Logit-adj \cite{menon2021longtail}& 75.67	&66.15	&52.93&	30.06&	46.83&	56.28\\
      Logit-adj + FR (G2)  & \good{}\textbf{75.81}	&65.94	&\good{}\textbf{54.36}	&\good{}\textbf{30.47}&	\good{}\textbf{47.00}	&\good{}\textbf{62.18}\\
		   	\hline
      \hline 
	Imbalance Ratio ($r=100$)&CIFAR-10N (Agg) & CIFAR-10N (Rand1) & CIFAR-10N (Worse) & CIFAR-100N  & CIFAR-20N & Animal-10N\\
				\hline\hline
				CE& 69.09&	64.58&	57.40	&29.04	&45.64	&42.64\\
    CE + FR (G2)& \good{}\textbf{69.75}	&\good{}\textbf{66.43}&	\good{}\textbf{59.14}&	\good{}\textbf{31.97}&	\good{}\textbf{46.70}	&\good{}\textbf{45.28} \\ 
		   	\hline
      Logit-adj \cite{menon2021longtail} 	&71.35&	61.96	&47.86&	26.91&	40.80	&13.06\\
      Logit-adj + FR (G2)  & 	70.39&	59.19	&47.48	&\good{}\textbf{28.12}	&\good{}\textbf{41.09}	&\good{}\textbf{48.30}\\
		   	\hline
	\end{tabular}}
			\end{center}\label{tab:rebuttal}
			\vspace{-0.12in}
	\end{table*}

 
\subsection{Experiment Results on Clothing1M Dataset}

Clothing1M is a large-scale feature-dependent human-level noisy clothes dataset. We adopt the same baselines as reported in CIFAR experiments for Clothing1M. Due to space limits, we defer detailed descriptions into Appendix \ref{app:exp_details_c1m}.

We try implementing \fr\ with different $\lambda$ chosen from the set $\{0.0, 0.1, 0.2, 0.4, 0.6, 0.8, 1.0, 2.0\}$, where $\lambda=0.0$ indicates the training of baseline methods without \textbf{FR}. In Table \ref{tab:c1m}, the default setting of \textbf{FR} ($\lambda=1.0$) consistently reaches competitive performances by comparing to other $\lambda$s, except for the experiments w.r.t. NLS. Besides, we observe that most positive $\lambda$s that are close to $\lambda=1.0$ tend to have better performances than those close to $\lambda=0.0$, indicating the effectiveness as well as hyper-parameter in-sensitiveness of the introduced fairness regularizer.  

\begin{table}[!tb]
	\caption{Performance comparisons on real-world imbalanced noisily labeled dataset (Clothing1M), best and last-epoch achieved test accuracy are reported. Results in \textbf{bold} mean \fr{}  improves the performance of the baseline methods, respectively. Performances of \fr{} with different $\lambda$s are provided. }
 \vspace{-0.1in}
	\begin{center}
    \scalebox{0.86}{\begin{tabular}{cc|c|ccccccc}
 \hline
	
			\textbf{Method}	 & $\lambda$ &$0.0$  &$0.1$ & $0.2$ & $0.4$ & $0.6$ & $0.8$ & $1.0$ & $2.0$    \\ \hline
 \multirow{2}{*}{\textbf{CE}}   
 &  Best & 72.68 & 72.44 & \textbf{72.93} & \textbf{72.74}& \textbf{73.10} & \textbf{72.80} & \textbf{72.99} & 72.45\\
   &  Last& 72.22 & 71.99 & \textbf{72.25} & \textbf{72.24} & \textbf{72.51} & \textbf{72.53} & \textbf{72.58} & 72.20
 \\ \hline	
   \multirow{2}{*}{\textbf{LS}}     & Best & 72.55 & \textbf{72.71} & \textbf{72.69} & 72.34 & 72.41 & 72.44 & \textbf{72.70} & \textbf{72.56}\\
    & Last& 72.03 & \textbf{72.11} & \textbf{72.14} & \textbf{72.12} & \textbf{72.12}& \textbf{72.06} & \textbf{72.33} & \textbf{72.24} \\ \hline
   \multirow{2}{*}{\textbf{NLS}}   &  Best & 74.46 & \textbf{74.48} & \textbf{74.47} &\textbf{74.49} &\textbf{74.48} &\textbf{74.49} & \textbf{74.49} & \textbf{74.50}\\
   &  Last& 74.00 & 73.99 &73.97 &  73.98 &  73.98 &  73.97 & 73.97 &73.97 
 \\ \hline	
 \multirow{2}{*}{\textbf{PL}} &  Best & 73.00 & \textbf{73.27} & \textbf{73.13}& \textbf{73.15} & \textbf{73.13}& \textbf{73.22} &\textbf{73.08} &\textbf{73.02} \\
   &  Last& 72.73 & \textbf{72.91} & \textbf{72.87} & 72.69 &  \textbf{72.76} & \textbf{73.12} & 72.71 & 72.69 \\ \hline
  \multirow{2}{*}{\textbf{Focal}}  &   Best & 72.71 & 72.60 & \textbf{72.71} & 72.60 & \textbf{72.92} & 72.66 & \textbf{72.91} & 72.46\\
    & Last& 72.16 & \textbf{72.21} & 72.04 & \textbf{72.18} & \textbf{72.30} & \textbf{72.36} &\textbf{72.51} & \textbf{72.46}
 \\ \hline	
   \multirow{2}{*}{\textbf{Logit-adj}}  & Best & 72.43 & \textbf{72.52} & \textbf{72.48} & 71.88 & 72.22 & \textbf{72.45} &  \textbf{72.67} & 72.06 \\
   &  Last& 72.22 & 72.15 & 72.14 & 71.58 & 71.83 & 71.94 & \textbf{72.23} & 71.92
 \\ \hline	
 \end{tabular}}
			\end{center}\label{tab:c1m}
	\end{table}
