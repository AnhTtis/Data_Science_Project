

Concluding this section, we have shown that given certain robust methods, significant disparate impacts on sub-populations are observed, when learning from long-tailed data with noisy labels. Such impacts also differ when complemented with different robust solutions, i.e., robust loss functions implicitly incur disparate impacts on the populations/samples.
Recall in Figure \ref{fig:acc_com_50}, we revealed that existing robust treatments may result in unfair performances among sub-populations, when learning from noisily labeled long-tailed data. All these observations motivate us to explore ways that will reduce the gaps between the head and the tail populations. 

\section{Fairness Regularizer (\fr)}

In this section, we propose to assign fairness constraints to the learning objective function. Leveraged into its Lagrangian form, such fairness constraints could be viewed as fairness regularizers that explicitly encourage the classifier to achieve fair performances among sub-populations. We name our solution the \textbf{F}airness \textbf{R}egularizer (\fr), which encourages the learned classifier to achieve fair performance across sub-populations. 
 

\subsection{Fairness Constraints}
Note that when learning with robust methods, the classifier tends to result in fitting on certain sub-populations more easily. We propose to constrain the classifier's performance on sub-populations, i.e.,  
\begin{align}\label{eq:cons}
    &\min_{f: \text{domain}(X)\to [K]} ~\mathbb{E}_{(X, \nY)\sim \nD}[\ell(f(X), \nY)]\notag\\
    &\text{s.t.}~~  \text{Constraint w.r.t. }\mathbb{P}( f(X)= \widetilde Y ~|~ G=i),
\end{align}
where $\ell$ is a generic loss function that could be any robust losses and the ultimate goal of the classifier $f$ is categorizing the feature $X$ into a specific class within $[K]$. Since we do not wish certain sub-populations to fall much behind others, i.e., in terms of accuracy, we constrain the performance gap between any two sub-populations by adopting the following constraint for Eqn. (\ref{eq:cons}), specifically, for any sub-population $i\in[N]$, we require its performance to have a bounded distance from the average performance.
Denote the distance (absolute performance gap) by 
\begin{align*}
    \dist_{i}:=\Big|\mathbb{P}( f(X)= \widetilde Y ~|~ G=i) -\mathbb{P}( f(X)= \widetilde Y)\Big|,
\end{align*}
then the optimization problem is formulated as:
\begin{align}\label{eq:cons_fr}
    &\min_{f: X\to [K]} ~\mathbb{E}_{(X, \nY)\sim \nD}[\ell(f(X), \nY)],\quad \text{s.t.}~ \dist_{i}\leq \delta, \forall i\in [N],
\end{align}
where $\delta\geq 0$ is a constant. Setting $\delta=0$ implies that the classifier should achieve fair performances among all sub-populations, in order to satisfy the constraints.

\subsection{Using Fairness Constraints as a Regularizer}
In practice, forcing sub-populations to achieve absolutely fair or equalized  performances (i.e., accuracy) may produce side effects. For example, one trivial solution to achieve $\delta=0$ is simply reducing the performance of all the other sub-populations to be aligned with the worst sub-population, leading to poor overall performance.
Even though we can fine-tune $\delta$ to set an appropriate tolerance of the gap, the sub-population with the worst performance may still violate the constraint. Noting our goal is to improve the overall performance on clean and balanced test data, it is arguably a better strategy to not over-addressing the worst sub-population.

To balance the trade-off between mitigating the disparate impacts among sub-populations and the possible negative effect due to constraining, rather than strictly solving the constrained optimization problem in Eqn.~(\ref{eq:cons_fr}), we use the constraint as a regularizer by converting it to its Lagrangian form as follows.
 \[\min_{f: X\to [K]} \mathcal{L}_{\lambda}(f):=\mathbb{E}_{(X,\nY)\sim \nD}[\ell(f(X), \nY)]+\underbrace{\sum_{i=1}^N  \lambda_i \dist_i}_{\textbf{\fr}},\]
 where $\lambda_i\geq 0$. Different from the traditional dual ascent of Lagrange multipliers \cite{boyd2011distributed}, we fix $\lambda_i$ during our training. Intuitively, applying dual ascent is likely to result in a large $\lambda_i$ on the worst sub-population, inducing possible negative effects as we discussed above.
 Therefore, in such a minimization task, the accuracy/performance gaps between sub-populations are encouraged to be small and do not have to be exactly lower than any threshold. 

 
 \paragraph{Implementation} Denote by $\bm f_x[\ny]$ the model's prediction probability on the noisy label $\ny$ given input $x$. Noting the probability in $\dist_i$ is non-differentiable w.r.t $f$, we apply the following empirical relaxation \cite{wang2022understanding}:
 \begin{align}\label{eq:relax_constraint}
    \dist_{i}:=\Big| \frac{\sum_{k=1}^N \bm f_{x_k}[\ny_k] \cdot \BR(g_k = i)}{\sum_{k=1}^N \BR(g_k = i)}  - \frac{\sum_{k=1}^N \bm f_{x_k}[\ny_k] }{N}  \Big|,
\end{align}
where $\BR(g_k=i)=1$ when $g_k=i$ and $0$ otherwise.
 For simplicity, we set all $\lambda_i$ to a constant, i.e. $\lambda_i = \lambda$.
 
To demonstrate why \fr{} helps with improving the learning from noisily labeled long-tailed data, we will provide extensive experiment studies in the next section. We have also adopted a binary Gaussian example and theoretically show:
\begin{mybox2}
\begin{observation}
When solving the risk minimization on the noisily labeled long-tailed data under the introduced fairness constraints returns the Bayes optimal classifier. 
\end{observation}
\end{mybox2}
Due to space limits, we defer the detailed discussion to Appendix \ref{app:fr_helps}.