



\section{Preliminary}

We go through the preliminaries in this section.

\subsection{Sub-Populations of Features}\label{sec:formulation}
 
In a $K$-class classification task, denote a set of data samples with clean labels  as $S:=\{(x_i, y_i)\}_{i=1}^n$, given by random variables $(X, Y)$, which is assumed to be drawn from $\mathcal{D}$. In this work, we are interested in how sub-populations intervene with learning. Formally, we denote $G \in \{1,2,...,N\}$ as the random variable for the index of sub-population, and each sample $(x_i, y_i)$ is further associated with a $g_i$. The set of sub-population $k$ could then be denote as $\mathcal G_k := \{i: g_i = k\}$. We consider a long-tail scenario where the head population and the tail population differ significantly in their sizes, i.e., $\max_k |\mathcal G_k| \gg \min_{k'} |\mathcal G_{k'}|$. 

Consider Figure \ref{fig:dis_longtail} for an example of sub-population separations using the CIFAR-100 dataset \cite{krizhevsky2009learning}: images are grouped into 20 coarse classes, and each coarse class could be further categorized into 5 fine classes. For example, the coarse class "aquatic mammals" was further split into "beaver", "dolphin", "otter", "seal", 
 and "whale". From Figure \ref{fig:dis_longtail}, we observe a strong imbalanced distribution of different sub-populations and a long-tailed pattern. In Section \ref{sec:exp_detail}, we provide more details on long-tail data generation models for our synthetic experiments. 

\paragraph{Clarification}
Throughout this work, the saying of sub-populations is a generalized definition of the separation of samples, which includes many popular settings as special cases, i.e.,
\squishlist
    \item \emph{Class-relevant}: the class name is actually a natural separation of samples, such separations could be more fine-grained class-related (such as further splitting the class “cat” into finer separations by referring to the breed of cats);
    \item \emph{Class-irrelevant}: such population could also be class-irrelevant, for example, in image classification tasks where the gender information is the (hidden) attribute information of each image while the class/label does not disclose this information.
\squishend

\begin{figure}[!t]
    \centering
  \begin{center}
\includegraphics[width=0.5\textwidth]{figures/influence/longtail_sub.pdf}
  \end{center}
  \caption{Count plot of a synthetic long-tailed CIFAR-100 train dataset: $x$-axis denotes the sub-population index; $y$-axis indicates the number of samples in each sub-population.}\label{fig:dis_longtail}
\end{figure}


\subsection{Our Task}
In practice, obtaining "clean" labels from human annotators is both time-consuming and expensive. The obtained human-annotated labels usually consist of certain noisy labels \cite{xiao2015learning,lee2018cleannet,jiang2020beyond,wei2021learning}. The flipping from clean labels to noisy labels is usually described by the noise transition matrix $T(X)$, with its element denoted by
$T_{ij}(X)=\p(\nY=j|Y=i, X)$. We denote the obtained noisy training dataset as $\widetilde{S}:=\{(x_i, \tilde{y}_i)\}_{i=1}^n$, given by random variables $(X, \nY)$, which is assumed to be drawn from $\widetilde{\mathcal{D}}$. 

Though we only have access to noisily labeled long-tailed data $\widetilde{S}$, 
our goal remains to obtain the optimal classifier with respect to a clean and balanced distribution $\mathcal D$:
   $ \min_{f\in\mathcal{F}} ~\mathbb{E}_{(X, Y)\thicksim \mathcal D} \left[\ell(f(X), Y)\right]
$,
where $f$ is the classifier chosen from the hypothesis space $\mathcal{F}$, and $\ell(\cdot)$ is a calibrated loss function (e.g., CE)
Furthermore, we do not assume the knowledge of the sub-population information during training. We are interested in how sub-populations intervene with the learning performance and how we could improve by treating the sub-populations with special care. 

