\documentclass[10pt]{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{nomencl}
\usepackage{etoolbox}
\usepackage{algorithm} 
\usepackage{algpseudocode} 
\usepackage{nomencl}
\usepackage[T1]{fontenc}
\usepackage{cite}
\usepackage{array,multirow,graphicx}
\usepackage{multirow}
\usepackage[dvipsnames]{xcolor}
\usepackage{xcolor}
% \usepackage{caption}
% \usepackage{subcaption}

% \renewcommand{\baselinestretch}{0.99}
\renewcommand{\baselinestretch}{1.0}

\title{Trainable Variational Quantum-Multiblock ADMM Algorithm for Generation Scheduling}
% \title{Learning Aided Variational Quantum Algorithm for Solving Unit Commitment}
% \title{Infusing Learning into a Quantum Variational Algorithm for Solving Unit Commitment}
% \title{Hybrid Learning Aided Quantum-Classical Unit Commitment}
\author{Reza Mahroo, \textit{Student Member, IEEE}, Amin Kargarian, \textit{Senior Member, IEEE}
\date{February 2022}
\thanks{R. Mahroo and A. Kargarian are with the Department of Electrical and Computer Engineering, Louisiana State University, Baton Rouge, LA 70803 USA (e-mail: rmahro1@lsu.edu, kargarian@lsu.edu).}}

\begin{document}
\maketitle
\begin{abstract}
The advent of quantum computing can potentially revolutionize how complex problems are solved. This paper proposes a two-loop quantum-classical solution algorithm for generation scheduling by infusing quantum computing, machine learning, and distributed optimization. The aim is to facilitate employing noisy near-term quantum machines with a limited number of qubits to solve practical power system optimization problems such as generation scheduling. The outer loop is a 3-block quantum alternative direction method of multipliers (QADMM) algorithm that decomposes the generation scheduling problem into three subproblems, including one quadratically unconstrained binary optimization (QUBO) and two non-QUBOs. The inner loop is a trainable quantum approximate optimization algorithm (T-QAOA) for solving QUBO on a quantum computer. The proposed T-QAOA translates interactions of quantum-classical machines as sequential information and uses a recurrent neural network to estimate variational parameters of the quantum circuit with a proper sampling technique. T-QAOA determines the QUBO solution in a few quantum-learner iterations instead of hundreds of iterations needed for a quantum-classical solver. The outer 3-block ADMM coordinates QUBO and non-QUBO solutions to obtain the solution to the original problem. The conditions under which the proposed QADMM is guaranteed to converge are discussed. Two mathematical and three generation scheduling cases are studied. Analyses performed on quantum simulators and classical computers show the effectiveness of the proposed algorithm. The advantages of T-QAOA are discussed and numerically compared with QAOA which uses a stochastic gradient descent-based optimizer.
\end{abstract}
%\vspace{12pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{IEEEkeywords}
Quantum computing, variational quantum algorithm, machine learning, distributed optimization, generation Scheduling.
\end{IEEEkeywords}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-12pt}
\section{Introduction}
% \subsection{Motivation}
\IEEEPARstart{C}{omputional} challenges have always played a significant role in the design and operation of complex systems \cite{eskandarpour2020quantum}. Power system modernization poses complexity and computational challenges that classical computers and solvers may not meet \cite{tovar2021sustainable}. Generation scheduling is a fundamental problem in power systems, which determines generating units’ status to provide the load while satisfying operational constraints cost-effectively \cite{conejo2018power}. This problem is mathematically set as a mixed-integer linear programming (MIP) problem \cite{abujarad2017recent}. As the number of generating units and the penetration of renewables and distributed resources increase, generation scheduling complexity and computational burden increase exponentially. Computationally efficient and scalable approaches must be developed to deal with this problem.

Quantum computing, a rapidly emerging technology, potentially opens new opportunities in solving computationally challenging problems that classical resources may not be able to address \cite{abd2022guest}. Quantum computing uses quantum mechanical laws to perform computation operations. Despite significant progress in recent years, universal error-corrected quantum computers have yet to be achieved. Currently, noisy intermediate-scale quantum technology is available to implement quantum algorithms and recognize problems for which quantum computing outperforms classical counterparts.

Solving combinatorial optimization is a research scope that quantum speedup is expected. This is achieved using methods such as quantum approximate optimization algorithm (QAOA) \cite{farhi2014quantum}, Grover algorithm \cite{grover1996fast}, and variational quantum eigensolver \cite{peruzzo2014variational}. The application of quantum computing for optimization is limited to solving quadratic unconstrained binary optimization (QUBO) problems \cite{zahedinejad2017combinatorial, kochenberger2014unconstrained}, which is a form of combinatorial optimization to find an optimal solution for a cost function whose solution area is discrete in large configuration space. A QUBO problem can be transformed into an Ising model using a Hamiltonian that includes the weighted tensor product of Pauli-Z operators \cite{tanahashi2019application}. Limitations of quantum computing algorithms and noisy intermediate-scale quantum devices have initiated the development of hybrid quantum-classical techniques to cope with large-scale problems. The aforementioned quantum algorithms, e.g., QAOA, are designed based on gate-based quantum computers, which are still in the early stage of development and unable to solve large problems \cite{nannicini2019performance}. Also, a special-purpose design of quantum computers to solve QUBOs has been developed recently based on quantum annealing  \cite{mcgeoch2020theory}. Such hybrid algorithms aim to decompose a problem into one subproblem that can be efficiently handled in a classical computer and another subproblem that can be assigned to a quantum processing unit (QPU). Hybrid quantum-classical algorithms can be found in the literature for both general-purpose and special-purpose problems.

To solve MIP problems, \cite{chang2020quantum} and \cite{paterakis2021hybrid} have presented hybrid techniques based on the Benders technique to decompose the original problem into a MIP master problem and a convex linear programming subproblem. In \cite{chang2020quantum}, continuous variables are discretized to convert the master problem into a QUBO. This discretization needs ancillary qubits. In \cite{paterakis2021hybrid}, a Benders cut selection scheme manages the size of the master problem. The cut selection strategy is a QUBO problem assigned to a QPU. Although Benders-based algorithms guarantee convergence, they take many iterations. A hybrid technique based on the multi-block alternating direction method of multipliers (ADMM) \cite{boyd2011distributed} is presented in \cite{gambella2020multiblock} for MIP problems. Complicating variables are relaxed. The problem is split into a QUBO solvable by QPU and continuous blocks solvable by classical solvers. A two-block and three-block version of this method is adopted in \cite{nikmehr2022quantum} and \cite{mahroo2022hybrid}, respectively, to solve unit commitment. Although this approach is useful for adjusting the complexity of binary subproblems by encoding them through entangled states encoded within qubits, it might not converge as the problem size increases. Surrogate Lagrangian relaxation is presented in \cite{feng2022novel} for a hybrid solution of unit commitment. In addition to these algorithms, \cite{ajagekar2019quantum, ajagekar2020quantum, braine2021quantum} present similar ideas for solving specific problems using quantum computing. The authors in \cite{ajagekar2019quantum} and \cite{ajagekar2020quantum} discretize continuous variables into $h$ parts to turn them into binary variables. This method is not practical as for a problem with $n$ continuous variables, $n(h+1)$ ancillary qubits are required. In \cite{braine2021quantum}, heuristic methods extend the variational quantum eigensolver for solving MIP problems. The main drawback is the inability to guarantee convergence to the global or local optimum. The proposed approach in \cite{ajagekar2022hybrid} integrates a classical heuristic algorithm with a quantum annealer to achieve better-quality solutions in a shorter time frame than traditional methods. Authors in \cite{morstyn2022annealing} convert the combinatorial optimal power flow problem into a QUBO problem, which is amenable to quantum annealing. In \cite{raghav2021optimal}, a quantum teaching learning-based algorithm for optimal energy management of microgrids is proposed, which outperforms other optimization algorithms in terms of convergence speed and accuracy. We note that previous studies have mainly proposed approaches for exploiting quantum computers. The computational process of quantum algorithms, such as QAOA, has not been sufficiently investigated.

QAOA is a variational quantum algorithm for solving combinatorial optimization problems. It is a promising candidate for demonstrating quantum advantage in the near future \cite{braine2021quantum}. The main concept of QAOA is to alternately repeat the cost Hamiltonian, in which its ground state encodes the problem solution and mixing Hamiltonian. It relies on preparing a parameterized quantum circuit on a quantum device and a classical optimizer to find the best quantum circuit parameters. QAOA is introduced in \cite{farhi2014quantum} to solve combinatorial optimization problems. The model and study are derived from a MaxCut problem. Following that work, \cite{lin2016performance} has studied QAOA's ability to solve more complex problems. The quality of the solution resulting from QAOA is affected by the quality of variational parameters prepared in a classical optimizer. Therefore, developing effective variational parameters optimization techniques is crucial to achieving quantum advantages. Various approaches are proposed for optimizing variational parameters, including gradient-based \cite{wang2018quantum} and gradient-free methods \cite{wecker2016training, streif2020training}. Neural networks and learning techniques have also been used to optimize the variational parameters \cite{streif2020training, shaydulin2019evaluating}. Optimization-based methods take many more iterations to achieve optimal results as compared to learning-based methods \cite{khairy2020learning}. Scalability is also a key point that learning-based methods cannot address easily.

In this paper, we develop a trainable two-loop quantum-classical optimization algorithm for generation scheduling. Generation scheduling is decomposed into one QUBO and two non-QUBO subproblems. The inner QAOA loop solves QUBO on quantum computers. The iterative interactions between the quantum circuit and classical optimizer are translated as sequential time series-type information. A scalable deep recurrent neural network plays the role of an optimizer mimicking the iterative trace between QPU and a conventional computer to determine the optimal quantum circuit variational parameters. With a proper sampling technique, the proposed trainable QAOA (T-QAOA) converges after a pre-determined number of iterations instead of hundreds to thousands of iterations that may take by the conventional QAOA. An outer  3-block quantum-ADMM (QADMM) loop is designed to coordinate generation scheduling QUBO and non-QUBO subproblems. The inner loop learner stays unchanged at every outer QADMM iteration. The scalability and convergence conditions of the proposed algorithm are discussed. Numerical results on a real quantum computer and quantum simulator show the effectiveness of the proposed trainable two-loop algorithm.

The paper is organized as follows. Section II presents generation scheduling and a preliminary discussion on quantum computing. The generation scheduling decomposition and the T-QAOA algorithm are proposed in Section III. Numerical results are discussed in Section IV, and concluding remarks are provided in Section V.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-10pt}
\section{Preliminaries}
%This section presents a compact formulation of the unit commitment problem and a preliminary discussion of QC and quantum optimization.
\subsection{Compact Generation Scheduling Formulation}
Assume that $p_{i,t}$ denotes continuous variables (e.g., power output), $y_{i,t}$ denotes discrete variables (e.g., on/off status) of unit $i$ at time $t$. Let $\mathcal{I}=\{1,2,...,N\}$ and $\mathcal{T}=\{1,2,...T\}$ be the set of generating units and time periods. 
% For brevity of notation, we use $(\mathrm{p},\mathrm{y})$ and $(\mathrm{p}_i,\mathrm{y}_i)$ in the following equations where $(\mathrm{p}_i,\mathrm{y}_i)$ refers to vectors of unit $i$ containing variables $(p_{i,t},y_{i,t}), \forall t \in \mathcal{T}$, and $(\mathrm{p},\mathrm{y})$ refers to vectors containing variables $(p_{i,t},y_{i,t}), \forall i \in \mathcal{I}$ and $\forall t \in \mathcal{T}$. 
The generation scheduling problem is as follows:

\begin{equation} \tag{1a}
\label{1a}
\min_{p_{i,t} \in \mathbb{R} , y_{i,t} \in \chi} \sum_{i \in \mathcal{I}}\sum_{t \in \mathcal{T}} [f (p_{i,t}) + g (y_{i,t})],
\end{equation}
s.t. 
\begin{equation} \tag{1b}
\label{1b}
 h(p_{i,t}) = 0; \forall i \in \mathcal{I}, \forall t \in \mathcal{T},
\end{equation}
\begin{equation} \tag{1c}
\label{1c}
 j(y_{i,t}) = 0; \forall i \in \mathcal{I}, \forall t \in \mathcal{T},
\end{equation}
\begin{equation} \tag{1d}
\label{1d}
 k(p_{i,t},y_{i,t}) \leq 0; \forall i \in \mathcal{I}, \forall t \in \mathcal{T}.
\end{equation}

The objective function (1a) is the generation and startup/shutdown costs. Equality (1b) represents constraints associated with continuous variables such as power balance. Generators on/off logic equations are considered in (1c). Constraint (1d) represents the power output limitations, transmission line bounds, reserve requirements, and ramping rate, including binary and continuous variables. For brevity of notation, we write objective function (1a) as:
% \begin{equation} \tag{2a}
% \label{2a}
% \mathcal{F}(\mathrm{p})= \sum_{i \in \mathcal{I}} f_i (\mathrm{p}_{i})= \sum_{i \in \mathcal{I}}\sum_{t \in \mathcal{T}} \left(a_{i} \cdot p_{i,t}^2 + b_i \cdot p_{i,t}   \right),
% \end{equation}
\begin{equation} \tag{2a}
\label{2a}
\mathcal{F}(\mathrm{p})= \sum_{i \in \mathcal{I}}\sum_{t \in \mathcal{T}} b_i \cdot p_{i,t} ,
\end{equation}
\begin{equation} \tag{2b}
\label{2b}
\mathcal{G}(\mathrm{y})= \sum_{i \in \mathcal{I}} \sum_{t \in \mathcal{T}} c_{i} \cdot y_{i,t},
\end{equation}
 where $a_i$ and $b_i$ are fixed cost coefficients of unit $i$. $c_i$ represents the standby cost of unit $i$, including no-load cost, startup, and shutdown cost.

 \vspace{-10pt}
\subsection{Quantum Computing}
Classical computers encode information in binary bits, which are either 0s or 1s, and use integrated circuits that contain millions of transistors. Quantum computers also operate data as a series of qubits. Unlike regular bits, qubits can simultaneously be at both $|0\rangle$ and $|1\rangle$ states with a certain probability. Therefore, one qubit can store 2 bits of information. This quality results in a system's exponential scaling advantage regarding the number of required qubits \cite{nielsen2002quantum}. In a processor with $n$ qubits, $2^n$ bits of information can be stored. Qubits contain two main properties called superposition and entanglement. Superposition refers to the quantum system's ability to be in multiple states simultaneously, and the correlation between quantum particles is referred to as entanglement \cite{steane1998quantum}. 
 
Quantum computers use quantum gates to execute calculations within quantum circuits, as logical circuits and logic gates do in classical computers. Qubits, after initialization, travel through quantum gates, experiencing a rotation, which basically refreshes the probability of each state as $|\psi \rangle = U | \psi_0 \rangle$. A typical fixed quantum circuit is shown in Fig. 1(a). Where $|\psi_0 \rangle$ is the initial state, $U$ represents the equal unitary operator of the circuit at the given angles, and $|\psi \rangle$ is the output state. The measurement occurs at the end of the circuit, as states with a higher probability appear more frequently in the measurement. Quantum circuits designed based on free parameters are known as variational  quantum circuits (VQC) \cite{zhou2022noise}, as in Fig. 1(b). The output of a VQC, $|\psi_\beta \rangle = U (\beta) | \psi_0 \rangle$, is controlled by its variational parameters $\beta$. By optimizing this parameter, VQC conducts different tasks, e.g., solving QUBO problems \cite{farhi2014quantum} and system of linear equations \cite{bravo2019variational, Amani2023quantum}.
% \begin{figure}
%     \centering
%          \begin{subfigure}[b]{0.24\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{Fixed Circuit.png}
%          \caption{Fixed quantum circuit}
%          \label{1.a}
%      \end{subfigure}
%     \hfill
%      \begin{subfigure}[b]{0.24\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{Variational Circuit.png}
%          \caption{Variational quantum circuit}
%          \label{1.b}
%      \end{subfigure}
    
%          \caption{Schematic of the fixed and variational quantum circuits.}
%          \label{fig1}
% \end{figure}

\begin{figure}
\centering
    \includegraphics[width=.45\textwidth]{Fixed_Variational_Circuits.png}
\caption{Schematic of the fixed and variational quantum circuits.}
\label{fig1}
\vspace{-10pt}
\end{figure}

To perform optimization using quantum computers, we should prepare the Hamiltonian of the Ising model or QUBO corresponding to the objective function and constraints of the considered optimization problem. The ground state of a corresponding Ising Hamiltonian is the optimal solution to a QUBO problem \cite{moll2018quantum}. The Ising model or QUBO can be represented by a graph $G(V,E)$ where $V$ and $E$ refer to the set of vertices and edges. Fig. 2 shows an example of an undirected graph based on which the Hamiltonian Ising model can be constructed. For a random graph $G$, the Hamiltonian of the Ising model is as follows  \cite{tanahashi2019application}:
\begin{equation} \tag{3}
\label{3}
\mathcal{H}(\mathrm{s})= \sum_{k \in V} h_{k}  s_{k} + \sum_{(k,j) \in E }  J_{kj}  s_k s_j, \hspace{0.5cm} s_k=\pm 1,
\end{equation}
where $s_k$ is the spin at vertex $k \in V$, $J_{kj}$ pertains to the interaction between qubits $k$ and $j$, $(k,j) \in E$, and $h_k$ is the external magnetic field at vertex $k \in V$. From the physical point of view, vertices are physical qubits, and edges represent the potential locations of two-qubit gates. The left-hand-side argument of (3) represents the array of spins $|\mathrm{s}\rangle=|s_1,s_2,...,s_N\rangle$, where $N$ is the number of vertices.

\begin{figure}
\centering
    \includegraphics[width=.3\textwidth]{Picture3.png}
\caption{A sample graph of a Hamiltonian Ising model.}
\label{fig2}
\vspace{-10pt}
\end{figure}

A quadratic unconstrained binary optimization is an energy function that can be transformed into an Ising Hamiltonian ($\{ \pm1\}^n$) with a simple conversion of variables \cite{kumar2020large}. Consider the following QUBO with a binary variable $x_k$, linear term $t_k$, and quadratic term $q_{kj}$.
\begin{equation} \tag{4}
\label{4}
C(x)= \sum_{k \in V} t_{k}  x_{k} + \sum_{(k,j) \in E }  q_{kj}  x_k x_j, \hspace{0.2cm} x_k \in \{0,1\}.
\end{equation}

To convert QUBO (4) into an Ising model, the relation $x_k = (1+s_k)/2$ is used \cite{boettcher2019analysis}. In the case of existing soft constraints $g(x)=0$, a QUBO problem can be retrieved by adding a quadratic penalty term $\rho ||g(x)||^2$ to the objective function based on augmented Lagrangian \cite{wang2009unified}. When inequality constraints are in place, they need to be converted to equality constraints using slack variables, then they can be treated like equality constraints. In a nutshell, the total Ising Hamiltonian for an objective function $C(x)$, with equality and inequality constraints $g(x)=0$ and $h(x) \leq 0$, is as follows:
\begin{equation} \tag{5}
\label{5}
\mathcal{H}_c= \mathcal{H}_{obj}+\varrho_1 ||g(\frac{1+s}{2})||^2+ \varrho_2||(h(\frac{1+s}{2})+\frac{1+s}{2})||^2,
\end{equation}
where $\varrho_1$ and $\varrho_2$ are large positive coefficients. After transforming the QUBO problem to the Ising Hamiltonians, it can be solved using QAOA.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-10pt}
\section{3-Block Quantum Alternating Direction Method of Multipliers (QADMM)}
A decomposition technique is developed to convert the generation scheduling problem (1) into three blocks. The first block formulation is QUBO, which will be solved by T-QAOA on a quantum computer. The other two blocks are convex optimization problems that can be efficiently solved using classical solvers. The three subproblems are coordinated using 3B-ADMM. Fig. 3  shows a schematic of the proposed solution algorithm.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\centering
    \includegraphics[width=.5\textwidth]{loop3.png}
\caption{Overview of the proposed algorithm.}
% \label{fig3}
\vspace{-12pt}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-10pt}
\subsection{Decomposition Technique}
generation scheduling (1) includes continuous and binary variables. We apply a reformulation to decompose this problem. Binary variables $\mathrm{y}$ are relaxed to vary continuously as $0 \leq \mathrm{y} \leq 1$. A set of auxiliary binary variables $\mathrm{z}$ and continuous variables $\mathrm{r}$ are defined. Three sets of constraints (6c), (6d), and (6e) guarantee the binary nature of $\mathrm{y}$. Optimization (1) is now reformulated as (6).
\begin{equation} \tag{6a}
\label{6a}
\min_{\mathrm{p}, \mathrm{y}, \mathrm{r} \in \mathbb{R}, \mathrm{z} \in \chi} \mathcal{F}(\mathrm{p}) + \mathcal{G}(\mathrm{y}),
\end{equation}
s.t. 
\begin{equation} \tag{6b}
\label{6b}
 (1b)-(1d),
\end{equation}
\begin{equation} \tag{6c}
\label{6c}
 A_0 \cdot \mathrm{y} + A_1 \cdot \mathrm{z} +A_2 \cdot \mathrm{r} =0: \lambda,
\end{equation}
\begin{equation} \tag{6d}
\label{6d}
 \mathrm{r} = 0, \mathrm{r} \mbox{  free},
\end{equation}
\begin{equation} \tag{6e}
\label{6e}
 j(\mathrm{z}) = 0, %\forall i \in \mathcal{I}
\end{equation}
where $\mathrm{z} \in \{0,1\}$, $A_0$, $A_1$, and $A_2$ are identity matrices related as $A_0 =-A_1 = A_2= I$. The auxiliary variables $\mathrm{r}$ have to be zero, so the relaxed variables $\mathrm{y}$ become binary. Let $\hat{\mathrm{p}}=[\mathrm{p}^T,\mathrm{y}^T]^T$ and $\mathcal{F}(\hat{\mathrm{p}})=\mathcal{F}(\mathrm{p}) + \mathcal{G}(\mathrm{y})+ \iota_{\chi}(\mathrm{p},\mathrm{y})$, where $\iota_{\chi}(\mathrm{p},\mathrm{y})$ is an indicator function, and $\chi=\{(\mathrm{p} ,\mathrm{y})\subseteq \mathbb{R}|(6b)\}$.
Variables of (6) are split into three sets as $\hat{\mathrm{p}}=\{\mathrm{p},\mathrm{y}\}$, $\mathrm{z}$, and $\mathrm{r}$. 
With (6c) relaxed, the problem has a decomposable structure with respect to each set of variables. We dualize (6c) using augmented Lagrangian and relax the soft constraint (6c)-(6e) by adding penalty terms to the objective function as follows:
\begin{equation} \tag{7}
\label{7}
\begin{aligned}
   {} & \mathcal{L}_\rho(\hat{\mathrm{p}},\mathrm{z},\mathrm{r},\lambda)=\mathcal{F}(\hat{\mathrm{p}}) \\
   & +\sum_{i \in \mathcal{I}}\sum_{t \in \mathcal{T}}\Big[\frac{\sigma}{2}||r_{i,t}||^2_2 +\frac{\omega}{2}||j(\mathrm{z}_{i,t}) ||^2_2\Big]\\
   & +\sum_{i \in \mathcal{I}}\sum_{t \in \mathcal{T}}\lambda^T \Big[A_0 \cdot y_{i,t} + A_1 \cdot z_{i,t} +A_2 \cdot r_{i,t}\Big]\\
   & +\sum_{i \in \mathcal{I}}\sum_{t \in \mathcal{T}}\Big[\frac{\rho}{2}||A_0 \cdot y_{i,t} + A_1 \cdot z_{i,t} +A_2 \cdot r_{i,t}||^2_2\Big].\\
\end{aligned}
\end{equation}
We then decompose (7) and coordinate subproblems as described in Algorithm 1.
\begin{algorithm}
	\caption{QADMM algorithm} 
	\begin{algorithmic}[1]
	\State Initialize: $m=1$, $\lambda^{(0)}$, $\rho>\sigma,\omega>0$, $\hat{\mathrm{p}}^{(0)}, \mathrm{r}^{(0)}, \epsilon >0$.
		\For {$m=1,2,\ldots ,$}
		   \State QUBO block update:\\ $\mathrm{z}^{(m)} \leftarrow \underset{\mathrm{z}}{\operatorname{argmin}}\mathcal{L}_\rho(\hat{\mathrm{p}}^{(m-1)},\mathrm{z},\mathrm{r}^{(m-1)},\lambda^{(m-1)})$. 
		   \State Second block update:\\ $\mathrm{r}^{(m)} \leftarrow \underset{\mathrm{r}}{\operatorname{argmin}}\mathcal{L}_\rho(\hat{\mathrm{p}}^{(m-1)},\mathrm{z}^{(m)},\mathrm{r},\lambda^{(m-1)})$.
		   \State Third block update:\\ $\hat{\mathrm{p}}^{(m)} \leftarrow \underset{\hat{\mathrm{p}}}{\operatorname{argmin}}\mathcal{L}_\rho(\hat{\mathrm{p}},\mathrm{z}^{(m)},\mathrm{r}^{(m)},\lambda^{(m-1)})$.
		   \State Dual variable update:\\ $\lambda^{(m)} \leftarrow \rho (A_0 \cdot \mathrm{y}^{(m)} + A_1 \cdot \mathrm{z}^{(m)} +A_2 \cdot \mathrm{r}^{(m)})+\lambda^{(m-1)}$.
		   \If {$||A_0 \cdot \mathrm{y}^{(m)} + A_1 \cdot \mathrm{z}^{(m)} +A_2 \cdot \mathrm{r}^{(m)}|| \leq \epsilon$}
		        {Stop}. \Else
		        \State $m \leftarrow m+1$.
		   \EndIf
		\EndFor
		\State Return $(\hat{\mathrm{p}},\mathrm{z},\mathrm{r})$.
	\end{algorithmic} 
\end{algorithm}

The iteration index $m$, penalty factors, decision variables, and stopping criteria are set in the initialization step. The first block, called QUBO block, is an optimization problem over the auxiliary variables $\mathrm{z}$ given $\hat{\mathrm{p}}$ and $\mathrm{r}$. This block can be assigned to a quantum computer. The second block is a quadratic unconstrained optimization problem with a cheap computational burden over auxiliary variables $\mathrm{r}$ given $\hat{\mathrm{p}}$ and $\mathrm{z}$. The third block, which is a relaxed version of (1) with relaxed binary variables, is a quadratic problem over variables $\hat{\mathrm{p}}$ given $\mathrm{z}$ and $\mathrm{r}$. The third block is a computationally much easier problem to solve than the original problem (1). If this problem is infeasible, it also implies that the original problem (1) is infeasible, and Algorithm 1 stops. Since the QUBO block is a non-convex problem, ADMM is generally heuristic. However, Algorithm 1 is guaranteed to converge to a stationary point under some conditions for a large enough $\rho > (\sigma,\omega)$.

\subsubsection{Convergence of Mixed-Integer ADMM} The conditions under which Algorithm 1 is guaranteed to converge to a stationary point of the augmented Lagrangian $\mathcal{L}_\rho$ are \cite{gambella2020multiblock, sun2019two, mahroo2022robust}:
\begin{enumerate}
  \item (Coercivity). The objective function $\mathcal{F}(\hat{\mathrm{p}})+\frac{\sigma}{2}||\mathrm{r}||^2_2 +\frac{\omega}{2}||B \cdot \mathrm{z} - H||^2_2$ is coercive over the constraint $A_0 \cdot \mathrm{y} + A_1 \cdot \mathrm{z} +A_2 \cdot \mathrm{r}=0$: This condition holds since the term $\frac{\sigma}{2}||\mathrm{r}||^2_2$ is quadratic and all other generation scheduling variables are bounded.
    \item (Feasibility). Im($A_T$) $\subseteq$ Im($A_2$), where $A_T=[A_0,A_1]$: Since Im($A_2$) is the entire space, this condition holds for any Im($A_T$).
  \item (Lipschitz subminimization paths). It is possible to have a constant $M>0$ at iteration $m$ such that:
  \begin{equation} \tag{8}
||\mathrm{y}^{m-1}-\mathrm{y}^{m}|| \leq M||A_0\mathrm{y}^{m-1}-A_0\mathrm{y}^{m}||.
\end{equation}
This condition holds by setting $M$ equal to 1. Note that the same condition is true for variables $\mathrm{z}$ and $\mathrm{r}$ with $M=1$.
  \item (Objective regularity). The objective $\mathcal{F}(\hat{\mathrm{p}})+\frac{\sigma}{2}||\mathrm{r}||^2_2$ is a lower semi-continuous function: Since $\mathcal{F}(\hat{\mathrm{p}})$ is a sum of convex functions and an indicator function of a convex set, it is restricted prox-regular \cite{gambella2020multiblock}. Also with a constant $\sigma$, $\frac{\sigma}{2}||\mathrm{r}||^2_2$ is Lipschitz differentiable. Therefore, this condition holds.
\end{enumerate}
Also, this algorithm converges to the global optimum if $\mathcal{L}_\rho$ is Kurdyka–Łojasiewicz (KŁ) function \cite{bolte2007lojasiewicz, attouch2013convergence}. Where function (7) holds this condition since it is a semi-algebraic function. Therefore using Algorithm 1, $\mathcal{L}_\rho$, which is a soft-constrained version of the problem (1), converges to a stationary point for a Large enough $\rho > (\sigma,\omega)$.

\subsubsection{Discussion on multi-block ADMM}
We note that fixing $\mathrm{r}$ to zero and skipping the second block update turns out to be a two-block implementation of ADMM. However, including variable $\mathrm{r}$ has two advantages. First, to decompose the linear constraint (6c), a three-block implementation is required, and the second block is an identity matrix whose image represents the entire space. It means for any fixed $\mathrm{y}$ and $\mathrm{z}$, there always exists an $\mathrm{r}$ such that satisfies (6c), and the feasibility of the problem is guaranteed. Second, constraint (6d) can be handled separately from (6c) and return a convex and Lipschitz differentiable term that can be included in the objective function as $\frac{\sigma}{2}||\mathrm{r}||^2_2$. There is evidence that, in some cases, a two-block implementation of ADMM may converge faster than a three-block. However, the 2-block ADMM is prone to non-convergence and adheres to the local optimality \cite{gambella2020multiblock} in large problems.
\vspace{-10pt}
\subsection{Distributed Coordination}
A power system comprises several subsystems with their local computing processors. Power system problems need solution algorithms that can deal with the growing complexity of the system, protect entities' privacy, and provide a solution in a reasonable period of time \cite{kargarian2016toward}. Meanwhile, near-term quantum computers cannot centrally handle large problems due to their limited qubits. This section addresses a distributed QADMM strategy adaptive to the practical implementation of generation scheduling.

Consider a system equipped with a two-way communication infrastructure enabling data exchange between a system coordinator and $i$ subsystems. A local CPU and QPU are embedded in each subsystem and can optimize, control, and coordinate the operation cycle of their generation units. The system coordinator also has a CPU to coordinate the subsystems. Each subsystem tries to schedule its generation to the system by solving its QUBO and non-QUBO optimizations, while the system coordinator coordinates the generation statuses to meet the system's physical limitations. The distributed coordination process can be outlined as follows:

\emph{Step 1:} given $(\mathrm{z}^{(m-1)}, \mathrm{r}^{(m-1)}, \hat{\mathrm{p}}^{(m-1)})$, system coordinator updates the dual variable $\lambda^{(m)}$ and send it to subsystems.

\emph{Step 2:} Solve the first, second, and third optimization blocks. QUBO subproblems are solved using a QPU to find $\mathrm{z}^{(m)}$ and pass them to CPU for updating $\mathrm{r}^{(m)}$ and $\hat{\mathrm{p}}^{(m)}$ by solving second and third optimization blocks.

\emph{Step 3:} Stop if termination criteria are satisfied, otherwise, go to \emph{Step 1}.

Generally, generation scheduling has decomposed over units and yields binary and continuous subproblems coordinated through adjusting Lagrangian multipliers iteratively. The continuous subproblems are solved in a classical computer using linear programming methods. The binary subproblems are mapped into Ising models by relaxing constraints, adding their penalty terms to the objective functions, and converting them into QUBO. The derived QUBO subproblems optimize the units' on/off decisions and are solved on QPUs. The continuous subproblems optimize the units' power generation level and are solved on CPUs.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\centering
    \includegraphics[width=.35\textwidth]{Picture1.png}
\caption{CPU and QPU information flow in distributed QADMM.}
\label{fig3}
\vspace{-12pt}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-10pt}
\subsection{Quantum Approximate Optimization Algorithm}
% Let us suppose Hamiltonian $\check{H}_c$ encodes the solutions of the QUBO block optimization problem $\mathcal{C}(z)$. QAOA introduced by Farhi et al \cite{farhi2014quantum} intends to approximate QUBO problem $\mathcal{C}(z)$ by minimizing the expected value of the Ising Hamiltonian of the optimization problem.
QAOA finds the solution to the previously developed QUBO block by minimizing the expected value of its Hamiltonian. This expectation function is derived from quantum states that entangle all possible states in the same probability. The optimal expected value of the Hamiltonian happens by optimizing the rotation angles of quantum gates in a classical solver.

QAOA acts on $n$ qubits, i.e., $2^n$ dimensional Hilbert space, with each qubit representing the state of a binary variable. The quantum process begins with initializing all qubits at state $|0 \rangle$, and then making an equal superposition of all computational basis states by applying a Hadamard gate to each qubit as:
\begin{equation} \tag{9}
\label{9}
|+ \rangle^{n} = \frac{1}{\sqrt{2^n}} \sum_{z\in\{0,1\}^n} |z\rangle.
\end{equation}

Given the problem Hamiltonian $\mathcal{H}_c$, QAOA applies a parameterized unitary operator $U(\mathcal{H}_c,\gamma)$, called cost Hamiltonian, depending on angle $\gamma$, and then makes a rotation of the resulting state using $U(\mathcal{H}_B,\beta)$, called mixing Hamiltonian, depending on angle $\beta$.
\begin{equation} \tag{10}
\label{10}
|\psi(\vec{\gamma},\vec{\beta}) \rangle = U(\mathcal{H}_c, \gamma_p)U(\mathcal{H}_B, \beta_p) ... U(\mathcal{H}_c, \gamma_{1})U(\mathcal{H}_B, \beta_{1}) |+\rangle^n,
\end{equation}
where these operators are formulated as:
\begin{equation} \tag{11}
\label{11}
U(\mathcal{H}_c, \gamma) = e^{-i \gamma \mathcal{H}_c} = \prod_{(k,j) \in edges} e^{-i \gamma \mathcal{H}_c^{(k,j)}},
\end{equation}

\begin{equation} \tag{12}
\label{12}
U(\mathcal{H}_B, \gamma) = e^{-i \beta B} = \prod_{k=1}^N e^{-i \beta X_k},
\vspace{-6pt}
\end{equation}
where variational parameters $\gamma \in [0,2\pi]$ and $\beta \in [0,\pi]$, or so-called angles \cite{farhi2014quantum}, denote the evolution times of the quantum circuit and are utilized to construct a parameterized QAOA circuit. $X_k$ refers to the Pauli-X operator, $B=\sum_{k=1}^N X_k$, and $p$ represents the depth of the circuit, i.e., the number of layers for which parameterized $U(\mathcal{H}_c, \gamma)$ and $U(\mathcal{H}_B, \beta)$ are repeated. At every layer, the number of cost Hamiltonian depicts edges, and the number of mixing Hamiltonian depicts the number of vertices of graph $G$. Given the depth of the circuit, $2p$ angles, i.e., $\gamma = [\gamma_1, ...,\gamma_p]$ and $\beta = [\beta_1,...,\beta_p]$, need to be tuned to make QAOA yield the optimal result. In practice, a trade-off determines the number of layers $p$ between the obtained approximation ratio, the parameter optimization complexity, and accumulated errors. Ideally, increasing $p$ enhances the QAOA solution quality, although the complexity of optimizing QAOA parameters with higher $p$ limits its benefits. Fig. 5 shows a prototype of a QAOA circuit diagram.

After preparing the state $|\psi(\vec{\gamma},\vec{\beta}) \rangle$, another important component of QAOA is to calculate the expectation value of $\mathcal{H}_c$ in the state of $|\psi(\vec{\gamma},\vec{\beta}) \rangle$. The expectation value is defined as:
\begin{equation} \tag{13}
% \label{10}
F(\vec{\gamma},\vec{\beta}) = \langle \psi(\vec{\gamma},\vec{\beta}) | \mathcal{H}_c |\psi(\vec{\gamma},\vec{\beta}) \rangle.
\end{equation}
QAOA minimizes the expectation value by updating the quantum state $|\psi(\vec{\gamma},\vec{\beta}) \rangle$ using a classical computer such that the expected function (13) is minimized to obtain $\vec{\gamma}^*$ and $\vec{\beta}^*$ as:

\begin{equation} \tag{14}
% \label{10}
(\vec{\gamma}^*,\vec{\beta}^*) = \arg \min_{\vec{\gamma},\vec{\beta}} F(\vec{\gamma},\vec{\beta})
\end{equation}
As shown in Fig. 5, variational parameters $\gamma$ and $\beta$ are prepared in a classical computer and fed to the quantum circuit iteratively until convergence. Thus, the entire QADMM process involves two loops, as shown in Fig. 3. The outer loop performs ADMM and incorporates QAOA, and the inner loop is a hybrid quantum-classical process. A good initialization and optimization process will lead to fewer iterations to find the optimal variational parameters. An overview of the QAOA steps is provided in Algorithm 2. The optimal variational parameter finding strategy introduced in (13)-(14) is the approach presented by the original QAOA paper \cite{farhi2014quantum}. Typically, different optimizers require hundreds to possibly thousands of quantum-classical iterations to reach a comparable parameter landscape optimum. Also,  the classical optimizer requires more time to obtain the results at every iteration as the size of the expectation function (14) increases. These are the major QAOA bottlenecks, which are resolved in the next section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\centering
    \includegraphics[width=.4\textwidth]{Picture2.png}
\caption{Quantum circuit diagram for QAOA.}
\label{fig4}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{algorithm}
	\caption{QAOA steps} 
	\begin{algorithmic}[1]
		   \State Initialize the quantum state by applying the Hadamard operator. 
		   \State Model the cost Hamiltonian using the QUBO function.
		   \State Model the mixing Hamiltonian.
              \State Create the circuits.
		   \State Run and measure the final state.
		   \State Update the variational parameters $\gamma$ and $\beta$.
		   \State Go to step 2.
	\end{algorithmic} 
\end{algorithm}
\vspace{-10pt}
\subsection{Trainable-QAOA (T-QAOA)}
We aim to train a learner to play the role of an optimizer for QAOA to update the variational parameters. The expectation value $F(\vec{\gamma},\vec{\beta})$ of problem Hamiltonian is used as the cost function with respect to parameterized state $|\psi(\vec{\gamma},\vec{\beta}) \rangle$ evolved from the QAOA circuit. To choose an optimizer architecture, the QAOA cost function and parameters evaluations are translated over several quantum-classical iterations as a sequential learning problem. Recurrent neural networks (RNNs) are a type of neural network commonly used to process such sequential information \cite{sun2017multiple}. RNNs are networks that take an input vector, create an output vector, and possibly store some information in memory for later use. A particular type of RNN framework that is used for the problem at hand is long-short-term memory (LSTM) which has outperformed other RNN architectures in many applications \cite{hochreiter1997long}. 

Fig. 6 shows the structure of the proposed T-QAOA. At an iteration $\nu$, variational parameters $(\gamma,\beta)_{\nu -1}$, the estimated cost function $z_{\nu -1}$, and the hidden state of the classical network $h_{\nu -1}$ are fed to LSTM from the prior step. LSTM has its trainable hyperparameters $\phi$ and employs a generalized mapping as:
\begin{equation} \tag{15}
% \label{10}
h_{\nu+1},(\gamma,\beta)_{\nu+1} = LSTM_\phi(h_{\nu},(\gamma,\beta)_{\nu},z_\nu),
\end{equation}
which suggests new variational parameters and a new internal hidden state. After training the weights $\phi$, the new set of generated variational parameters is sent to QPU for evaluation. This loop continues upon convergence. To generate the first query, we arbitrarily fix the variational parameters to a dummy value $(\gamma,\beta)_0=(0,0)$, implement the QAOA circuit, and set the cost function to the obtained $z_0$.

It is important to select an appropriate loss function during training to measure the LSTM performance on the training dataset. 
% In \cite{chen2017learning}, a measure called "observed improvement" is used for general-purpose tasks. Here, our task is straightforward. 
We use another loss function $\mathcal{L}(\phi)$ as:
\begin{equation} \tag{16}
% \label{10}
\mathcal{L}(\phi) = \mathbf{w}.\mathbf{z}_{\nu}(\phi)
\end{equation}
which is called "cumulative regret" and is the summation of loss function history at all iterations uniformly averaged over the horizon. $\mathbf{w}$ are coefficients that weigh the progression of the recurrence loop. We set a higher weight for the last steps 
as they contain more important information. This way, during the first steps of optimization, LSTM is freer to explore a larger section of parameter space, whereas, towards the end, it is restricted to choosing an optimal solution. The LSTM optimizer is trained to run for a fixed number of iterations. However, it is possible to allow it to optimize for more iterations than it was initially trained, but later iterations may have weak performance.

The merit of the proposed LSTM optimizer over other learning-based approaches is its scalability, which our problem desperately needs. Since QADMM is sequential, it returns the same QUBO problem at every iteration with only some coefficients updated. It means the QPU faces a slightly different QUBO problem at every ADMM iteration, such that the QAOA circuit remains the same, but the cost Hamiltonian changes. As such, the optimizer should be scalable so that a single set of training data will cover the entire process. Moreover, every standard optimizer follows a step-by-step path to update the variational parameters iteratively. Applying a learner only to predict the optimal variational parameters might yield a warm starting point for variational parameters. LSTM-based learners can be trained to find the paths every standard optimizer might take to reach the optimal values in a few iterations. Therefore, even if their results are not optimal, since they interact with the quantum circuit through predicting process, they provide more efficient starting points for other optimizers that execute local searches. 

Another challenge is preparing a dataset to train a learning-based optimizer for large-scale systems. As established in \cite{chen2017learning}, for an LSTM-based optimizer, the training dataset can be driven from the same system with fewer ranges of qubits. Therefore, for a power system with thousands of units, we can train an LSTM optimizer using the dataset driven from a subset of the given system. In addition, the power system topology is prone to change over time by adding or removing lines. In this case, a trained LSTM optimizer is expected to work based on the previous dataset, as it is not sensitive to a slight change in the grid topology. To generate the training dataset, given $\mathcal{H}_c$  and the fixed number of required qubits, we sample random values of coefficients  $J_{kj}$ and $h_k$  using independent Gaussian distributions with zero mean and unit variance. Then, the Ising model circuit is constructed using (10) for the sampled Hamiltonian.

% We used 1000 instances from the above-described distribution as our dataset and 20 percent as test instances. Moreover, unit commitment case studies will be used to test the LSTM optimizer after training the neural network.

% \subsection{Practical Usage of Current Quantum Computers}
% In the NISQ era, the most elevated quantum processors contain a few hundred qubits. Although highly developed, they lack fault tolerance and are not large enough to make sustained benefits from quantum supremacy.
% This section briefly covers the current obstacles faced by quantum computers in solving practical problems: 
% \begin{itemize}
%     \item Quantum Computers' architecture:
%     \item Number of qubits:
%     \item 
% \end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\centering
    \includegraphics[width=.4\textwidth]{Picture4.png}
\caption{Unrolled trainable QAOA diagram.}
% \label{fig6}
\vspace{-12pt}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-10pt}
\section{Simulation}
Two illustrative mathematical examples and three generation scheduling problems are used to validate the performance of QADMM and the LSTM optimizer. A study of the variational parameters will also be conducted. In the first example, we use a typical MIP problem to show the convergence and correctness of QADMM. In the second example, we use a QUBO problem to study the effect of variational parameters and validate the LSTM optimizer performance by comparing it with a standard optimization technique based on stochastic gradient descent (SGD). Also, the performance of QADMM and LSTM optimizer are evaluated on a 3-unit, 3-hour generation scheduling example. The results of QADMM are compared with the classical implementation of 3-block ADMM, and the LSTM optimizer performance is compared with SGD. To test the model's scalability, a 24-hour generation scheduling problem for 10- and 100-unit systems is solved, and the proposed LSTM optimizer is used to optimize the variational parameters. A noise-free quantum simulator (statevector) is used in combination with IBM's Qiskit \cite{Qiskit}, Terra, and IBMQ providers. Neural network training and inference are conducted in Keras and TensorFlow \cite{abadi2016tensorflow}.
\vspace{-10pt}
\subsection{Illustrative Mathematical Examples}
\subsubsection{Example 1}
We examine an example of QADMM's performance under simple setups. Consider the following MIP problem \cite{gambella2020multiblock}:
\begin{equation} \tag{17a}
\label{17a}
\min_{x \in \chi, y \in \mathbb{R}} x_1 + x_2 + x_3 + 5(y-2)^2,
\end{equation}
s.t. 
\begin{equation} \tag{17b}
\label{17b}
 x_1 + x_2 = 1,
\end{equation}
\begin{equation} \tag{17c}
\label{17c}
 x_1 + x_2 + x_3 \geq 1,
\end{equation}
\begin{equation} \tag{17d}
\label{17d}
 x_1 + x_2 + x_3 + y \leq 3,
\end{equation}
The steps in Algorithm 1 are followed to solve (17). We initialize penalty factors as $\rho=1001$, $\sigma=1000$, and $\omega=900$ \cite{gambella2020multiblock}. According to the QADMM direction, we decompose the problem into a QUBO subproblem solved by a quantum computer and a continuous and quadratic unconstrained subproblem solved by a classical computer. Fig. 6 shows the reduction of the constraint's residual, defined as $r'=||A_0\mathrm{y}+A_1\mathrm{z}+A_2\mathrm{r}||$ in Algorithm 1. The algorithm converges after 19 iterations and yields the optimal solution as $x=[1,0,0]$ and $y=2$ with an optimality gap of zero.
\begin{figure}
\centering
    \includegraphics[width=.4\textwidth]{illustrative_1.png}
\caption{QADMM residual for illustrative example 1.}
% \label{fig7}
\vspace{-10pt}
\end{figure}
\subsubsection{Example 2} We examine how QAOA solves a simple QUBO problem. Then we apply the trained LSTM optimizer to facilitate the process and evaluate its performance. This is a MaxCut problem on a triangular graph with two edges weighing five and one weighing 1 (see Fig. 7). The objective function is the sum of weights for edges connecting nods between the two subsets, i.e., $C(x)=\sum_{k,j=1}^n J_{kj}x_k(1-x_j)$ and $x \in \{0,1\}$. To solve this problem on a quantum computer, we need to translate it into an Ising Hamiltonian form, $H=\sum_{(k,j) \in E}\frac{1}{2}J_{kj}(1-s_k s_j)$ and $s\in \{ \pm1\}$ by taking the relation $x_k \rightarrow (1+s_k)/2$. We apply the steps explained in Algorithm 2. We create $U(\mathcal{H}_c, \gamma)$ and $U(\mathcal{H}_B, \beta)$ from the Ising Hamiltonian and build a circuit as in Fig. 8, which includes three steps: 
\begin{figure}
\centering
    \includegraphics[width=.25\textwidth]{illustrative_2.png}
\caption{Graph of illustrative example 2.}
% \label{fig8}
\vspace{-10pt}
\end{figure}

\begin{enumerate}
    \item Applying Hadamard gates: This step prepares the initial state, i.e., an equal superposition of all the possible states.
    \item Applying cost Hamiltonian: Three edges in the graph need three blocks of gates, each consisting of a two-qubit controlled. We assign an angle $\gamma$ to the edge with a weight of 1 and $5\gamma$ to the edges with a weight of 5.
    \item Applying mixing Hamiltonian: A layer of $R_x$ gates is applied, parameterized with $2\beta$.
\end{enumerate}
The measurement of the final state regarding the set of $(\gamma,\beta)$ happens after applying the above steps. To address data passing between classical and quantum processors, we build a custom model of an LSTM network. The LSTM optimizer is trained for five iterations, i.e., the CPU and QPU exchange information five times. We set $\mathbf{w}=\frac{1}{5}(0.1,0.2,0.4,0.6,0.8)$, giving higher priority to the latter steps in the loop. One hundred data are randomly generated using Gaussian distribution with zero mean and unit variance given the problem's objective function and the number of required qubits. The trained LSTM optimizer performance is compared with SGD in Figs. 10 and 11. LSTM terminates in fewer iterations than SGD. Fig. 10 shows the cost function over iterations. Fig. 11 illustrates the path suggested by LSTM and SGD in the space of the parameters. Given the periodic nature of variational parameters, Fig. 11  shows more than one optimal solution. The LSTM optimizer yields the point $(\gamma,\beta) = (-0.31, 0.62)$ in 5 iterations, and the SGD yields $(\gamma,\beta) = (1.26, 0.62)$ in 60 iterations. $\gamma$ provided by SGD is $\frac{\pi}{2}$ ahead of the one provided by LSTM optimizer. In the dataset used to train the LSTM, there are samples that $(\gamma,\beta)$ converge to four different optimal points. LSTM optimizer learns the periodic behavior and offers the closest optimal point.

\begin{figure}
\centering
    \includegraphics[width=.4\textwidth]{illustrative_2.2.png}
\caption{QAOA circuit of illustrative example 2.}
% \label{fig9}
\vspace{-10pt}
\end{figure}

\begin{figure}
\centering
    \includegraphics[width=.35\textwidth]{illustrative_2_.png}
\caption{LSTM vs. SGD cost function of illustrative example 2.}
% \label{fig10}
\vspace{-10pt}
\end{figure}

\begin{figure}
\centering
    \includegraphics[width=.35\textwidth]{illustrative_2_2.png}
\caption{Example 2 expectation function contour plot.}
% \label{fig11}
\vspace{-10pt}
\end{figure}
\vspace{-10pt}
\subsection{QADMM Generation Scheduling}
This section verifies the correctness and effectiveness of the proposed QADMM and LSTM optimizer as applied to generation scheduling problems. QADMM is compared with the classical 3-block ADMM and the LSTM optimizer with the SGD optimization approach. We consider three scenarios for each case study:
\begin{itemize}
    \item \emph{S1}: Algorithm 1 is applied to problem (1), and the QUBO block is solved using a classical solver (Gurobi).
    \item \emph{S2}: Algorithm 1 is applied to problem (1), the QUBO block is solved using Algorithm 2, and QAOA classical optimizer is SGD.
    \item \emph{S3}: Algorithm 1 is applied to problem (1), the QUBO block is solved using Algorithm 2, and QAOA classical optimizer is LSTM (the proposed algorithm).
\end{itemize}
Note that Algorithm 1's initializations are the same for all three scenarios.
\subsubsection{3-unit 3-hour generation scheduling}
The system includes three units supplying a demand of 160 MW, 500 MW, and 400 MW at three consecutive hours. There are three subproblems for the problem decomposed over generating units. Each subproblem contains nine binary variables: units on/off, start-up, and shut-down status at time $t$ ($3 \times 3 = 9$). The units' characteristics are given in Table I. An LSTM optimizer is trained for eight iterations with 800 observations and tested with 200 observations. The optimal status of units and the cost for each scenario are provided in Table II. The optimal on/off status and the operation cost obtained are the same for all scenarios. Fig. 12 portrays the reduction of the constraint residual for all scenarios at every ADMM iteration. The ADMM convergence performance is similar in all three cases. The residual approaches to the stopping criterion after 25 iterations. In $S2$ and $S3$, at every outer ADMM iteration, a hybrid quantum-classical interaction is conducted to find the optimal QAOA variational parameters. Though starting from the same initial point, the LSTM optimizer is trained to terminate after 8 interactions, while the SGD optimizer needs 86 iterations on average to converge. Fig. 13 shows the step-by-step moving toward an optimal point in $S2$ and $S3$ in the last ADMM iteration, which takes 65 iterations for SGD to converge.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}
\setlength{\arrayrulewidth}{0.3mm}
\begin{center}
\caption{Data of 3-Unit Generations}
\label{table:1}
\begin{tabular}{ m{0.5cm}  c c c c c c c c} \hline
 Unit & $P_{min}$& $P_{max}$ & $R_d$  & $R_{up}$  & $c_f$ & $c_{su}$ & $c_{sd}$ & b\\ \hline
 1    & 50MW & 350MW & 300 & 200 & 5 & 20 & 0.5 & 0.1\\ \hline
 2    & 80MW & 300MW & 150 & 100 & 7 & 18 & 0.3 & 0.125\\ \hline
 3    & 40MW & 140MW & 100 & 100 & 6 & 5  &  1  & 0.15\\ \hline
\end{tabular}
\end{center}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}
\setlength{\arrayrulewidth}{0.3mm}
\begin{center}
\caption{Generation Scheduling Results and Operation Cost of \emph{S1}-\emph{S3} ($\$$)}
\label{table:2}
\begin{tabular}{ m{0.5cm}  c c c c c c } \hline
 Unit & $t_{1}$& $t_{2}$ & $t_{3}$ &  Cost \emph{S1} & Cost \emph{S2} & Cost \emph{S3}\\ \hline
 1    & on   & on   & on  & 121   & 121  & 121\\ 
 2    &      & on   &     & 37.9  & 37.8 & 37.8\\ 
 3    &      & on   & on  & 54    & 54   & 54\\ \hline
 Total& -    & -    & -   & 212.9 & 212.8& 212.8\\ \hline
\end{tabular}
\end{center}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\centering
    \includegraphics[width=.5\textwidth]{3_unit_Residual.png}
\caption{QADMM residual for 3-unit generation scheduling problem.}
% \label{fig12}
\end{figure}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\centering
    \includegraphics[width=.35\textwidth]{3_unit_Contour2.png}
\caption{Contour plot of expectation function for 3-unit system.}
% \label{fig13}
\vspace{-10pt}
\end{figure}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{table}
% \setlength{\arrayrulewidth}{0.3mm}
% \begin{center}
% \caption{Optimal Power Output of 3-Unit System [MW]}
% \label{table:3}
% \begin{tabular}{ m{0.4cm}   m{1cm}  m{2cm} m{1cm}  m{2cm}} \hline
%  \multirow{2}{4em}{Unit} & \multicolumn{2}{c}{\emph{S2}} & \multicolumn{2}{c}{\emph{S3}} \\ \cline{2-5}
%     &  Outer Iter. & Inner Iter  & Outer Iter. & Inner Iter \\ \hline
%  1    &  -  & 42 &  - & 8 \\ \hline
%  2    &   - & 30 &  - & 8 \\ \hline
%  3    &  -  & 36 &  - & 8 \\ \hline
%  Ave. & -   &    & 11 &    \\ \hline
% \end{tabular}
% \end{center}
% \end{table}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Large-scale generation scheduling}
A study of the performance of QADMM and the LSTM optimizer in large-scale generation scheduling problems is conducted. The number of considered units is 10 and 100, and the time horizon has been extended to 24 hours. In every case, we have the same number of subproblems equal to the number of generating units. Each subproblem contains 72 binary variables: units on/off, start-up, and shut-down status at time $t$ ($3\times24=72$). Two LSTM optimizers are trained for ten iterations with 800 observations and tested with 200 observations. The first LSTM is trained using a 72-qubit quantum circuit and a Gaussian distribution of coefficients for the Ising model. The second learner is trained using quantum circuits with a number of qubits in the range of [40, 50] and the same Gaussian distribution of coefficients. Table III shows the optimal on/off status of 5 selected generating units of the 100-unit case. The on/off status obtained by QADMM is the same as those of the classical central solution determined by Gurobi.
Fig. 14 represents the reduction of the constraints residual for both 10-unit and 100-unit cases. As the system scales, QADMM maintains its convergence capabilities. Scaling up the system results in more subproblems and slower convergence. The 10-unit system converged after 77 iterations, and the 100-unit system converged after 126 iterations. A comparison between the SGD and LSTM optimizers is given in Fig. 15 for subproblem 1 in the last ADMM iteration. Both approaches started from the same initial point. LSTM optimizers terminate after ten iterations, while the SGD optimizer continues 84 iterations on average to converge. Furthermore, classical optimization takes more time to solve the problem as the dimension of expectation value (13) increases. A comparison between LSTM1, a well-trained optimizer, and LSTM2, a randomly-trained optimizer, is shown in Fig. 15. Though, from different paths, both optimizers obtain the optimal results after ten iterations.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[ht]
\caption{Schedule of 5 Selected Units of the 100-Unit Problem}
\label{table:3}
\centering
\begin{tabular}{p{0.03\linewidth}p{0.016\linewidth}p{0.016\linewidth}p{0.016\linewidth}p{0.016\linewidth}p{0.016\linewidth}p{0.016\linewidth}p{0.016\linewidth}p{0.016\linewidth}p{0.016\linewidth}p{0.016\linewidth}p{0.016\linewidth}p{0.016\linewidth}p{0.016\linewidth}p{0.016\linewidth}p{0.016\linewidth}p{0.016\linewidth}p{0.016\linewidth}p{0.016\linewidth}p{0.016\linewidth}p{0.016\linewidth}p{0.016\linewidth}p{0.016\linewidth}p{0.016\linewidth}p{0.016\linewidth}}
\hline
Unit & $t_1$ & $t_2$ & $t_3$ & $t_4$ & $t_5$ & $t_6$ & $t_7$ & $t_8$ & $t_9$ & $t_{10}$ & $t_{11}$ & $t_{12}$ & $t_{13}$ & $t_{14}$ & $t_{15}$ & $t_{16}$ & $t_{17}$ & $t_{18}$ & $t_{19}$ & $t_{20}$ & $t_{21}$ & $t_{22}$ & $t_{23}$ & $t_{24}$\\
\hline
%70    & On & On & On &  &  &  &  & On &  & On\\
1     & on & on & on & on & on & on & on & on & on & on & on & on & on & on & on & on & on & on & on & on & on & on & on & on\\
10    &    &    &    &    &    &    & on & on & on &    &    &    & on & on & on & on & on & on & on &    &    &    &    &  \\
45    &    &    &    &    &    & on & on & on & on & on & on & on & on & on & on & on & on & on & on & on &    &    &    &  \\
81    &    & on & on & on & on & on & on & on & on & on & on & on & on & on & on & on & on & on & on & on & on & on & on & on\\
90    &    &    &    &    &    &    & on & on & on & on &    &    & on & on & on & on & on & on & on &    &    &    &    &   \\ \hline
% $D_t$ &    &    &    &    &    &    &    &    &    &    &    &    &    & on & on & on & on & on & on &    &    &    &    &  \\
% \hline
\end{tabular}
\vspace{-10pt}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\centering
    \includegraphics[width=.5\textwidth]{10_100_unit_Residual.png}
\caption{QADMM residual for 10- and 100-unit generation scheduling problems.}
% \label{fig14}
\vspace{-10pt}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\centering
    \includegraphics[width=.35\textwidth]{10_100_unit_Contour2.png}
\caption{Contour plot of expectation function for 100-unit system.}
% \label{fig15}
\vspace{-10pt}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
This paper presents a scalable two-loop quantum-classical algorithm to solve the generation scheduling problem within the quantum computing framework. A three-block decomposition breaks generation scheduling into a QUBO and two non-QUBOs. A trainable QAOA solves the QUBO, and 3B-ADMM coordinates computation operations of quantum computer to solve the QUBO and conventional computer to solve non-QUBOs.

Simulation results on two mathematical and three generation scheduling problems show that T-QAOA terminates within a predetermined number of iterations, much fewer than that of the traditional QAOA. For instance, for a MaxCut problem, T-QAOA converges after five iterations, while the traditional QAOA with stochastic gradient descent converges after 60 iterations. For generation scheduling with 100 generators, T-QAOA takes ten iterations to find the optimal results obtained after 84 iterations of the traditional QAOA. Also, 3B-ADMM coordinates conventional and quantum computers computation operations to achieve optimal results of the original generation scheduling problem. The overall QADMM convergence residual is in the range of $10^{-6}$, which is promising.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{ieeetr}
\bibliography{Q-Classical-UC.bib}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}


