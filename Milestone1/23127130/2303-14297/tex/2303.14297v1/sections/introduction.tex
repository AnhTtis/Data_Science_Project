\section{Introduction}
\begin{figure}[hbt!]
\centering
\includegraphics[width=1\linewidth]{figures/Big/Asset_40.png}
\centering
\caption{Our AgileGAN3D enables  3D stylized portraits creation from a single input image. A new 3D style can be obtained with only a few unpaired 2D style exemplars ($\sim$ 20). Compared to the baseline method (directly fine-tune the 3D GAN model~\cite{chan2022efficient}), our approach produces high-quality, multi-view-consistent renderings of the portrait, with detailed in-style geometry. }
\label{fig:teaser}
\end{figure}

% \textcolor{blue}{[Jiashi: moved to comment editor]}

%background + 
Portrait painting as an art form goes back to prehistoric times, and has been primarily serving the rich and powerful. Fast forward with the technology advancement, people nowadays can enjoy a high fidelity digital portrait within seconds, and even capturing one's detailed facial 3D geometry\cite{Chai:2015:HQH,3DPortraitfrom}.
Driven by the creative nature of human beings, people are no longer satisfied with simply a faithful depiction of their appearance. Portraiture has evolved into more expressive interpretations with a plethora of styles, such as abstract art, cubism and cartoon. However, most previous works are limited to stylized portraits in 2D image space\cite{pinkney2020resolution, Song:2021:AgileGAN,yang2022Pastiche,chong2021jojogan}. Automatically creating 3D stylized portrait with detailed geometry using just a single selfie as input is still an open problem. To the best of our knowledge, we are the first work that can automatically create 3D stylized portrait with detailed geometry, using just a single selfie as input. The result 3D portrait can be adapted into a wide range of artistic styles, as long as a few 2D style exemplars are provided (see Fig.\ref{fig:teaser}). Such new format enables a lot of applications like 3D printed postcards, dynamic profile pictures (by changing viewing angles, or lighting directions), as well as personalized 3D contents in augmented and virtual reality worlds.

%Keys Problems
The core challenge that prevents us from creating visually appealing, personalized 3D portraits is rooted in the shortage of high quality 3D data. A traditional approach to create customized 3D portraits for users is by assembling a 3D avatar system with tons of graphics assets (e.g. Zepeto\footnote{https://zepeto.me/}, ReadyPlayer\footnote{https://readyplayer.me/}, and \cite{Sang2022AgileAvatar}). However, it's almost impossible to capture all the diversities in real world appearances given only a few hundred assets and a base morphable 3D face model. Therefore such approach usually generates less personalized results.

The latest generative models such as StyleGAN2~\cite{karras2020analyzing}, latent diffusion models~\cite{Rombach_2022_CVPR} are very powerful architectures in producing highly diversified imageries, largely credited to the huge data sources that these models have been trained on. Fine-tuning a generative model to produce highly personalized portraits thus becomes possible, though still primarily in 2D image space\cite{pinkney2020resolution, Song:2021:AgileGAN,yang2022Pastiche,chong2021jojogan}. Lifting arbitrary stylized portraits from 2D into 3D space remains to be an unsolved problem, partly due to the lack of 3D prior knowledge for target artistic pictures. Promising early results have emerged for general 3D objects~\cite{poole2022dreamfusion,jain2021dreamfields}, but none of them can produce desirable quality for 3D portraits yet. 
% Automatically high quality stylized portraiture has undergone rapid progress in recent years due to advances in high resolution generative model. Several generative pipelines used only a few unpaired exemplars and had the ability to generate promising high quality 2D stylized portrait in target style, by employment of \emph{transfer learning} a pre-trained generative network StyleGAN2\cite{Karras2019stylegan2}. A natural extension question comes: can we create a 3D generative stylization system using limited 2D examples that can stylize user's portrait in 3D and design their high quality personalized avatar?

Recent rapid progress in geometry-aware GANs \cite{chan2021pi,niemeyer2021giraffe,chan2022efficient,or2021stylesdf,deng2021gram,epigraf,zhou2021CIPS3D} inspired our work. Particularly EG3D ~\cite{chan2022efficient} demonstrated strikingly realistic 3D face synthesis capability by using only unstructured 2D photos. Its tri-plane structure serves as an efficient representation for 3D content generation, combined with a neural volumetric renderer, making multi-view-consistent, photo-realistic image synthesis possible. However, even with such a powerful 3D generator, there are still a few obstacles ahead before we can have a usable 3D stylized portrait generator. First, even in 2D image format, it is nontrivial to collect a large number of diverse portraits in a consistent style, let alone obtaining robust camera pose estimations from artistic portraits, which is a critical step in training 3D GANs. Secondly, in order to personalize user photos into 3D stylized portraits, a reliable 3D GAN inversion method is required. The inversion module has to work in a way that balances the reconstruction fidelity and the stylization quality.

% We therefore leverage the power of EG3D for creation of high-quality and identity-preserving stylized portraits in 3D. Similar to prior few-shot 2D stylization methods, we embed the single-view input image in a good latent correspondence of EG3D~\cite{chan2022efficient} and then fine-tune its generator to target styles.   
% However, directly fine-tuning the 3D GAN generator with provided limited 2D exemplars does not work well for 3D GANs and suffers from blurry artifacts and loss of 3D visual appeal. We unveil the causes in two-folds. First, we observe the successful training of 3D GANs, such as EG3D, heavily depends on reliable estimation of camera parameters for the training images. However, for unpaired 2D exemplars in non-realistic styles, it poses a challenge to obtain such 3D camera estimations. Secondly, adapting the 3D-aware generator to stylistic domain requires fine-tuning both the convolutional modules but also the neural radiance representation in 3D. Without adequate visual observations from prior style images in various views, it is very challenging for the network to adapt the generation of neural radiance fields in target style domain.  


% Our key insight is that this directly transfer learning using 2D few examples can not provide adequate supervision to enable good multi-view consistent stylistic rendering and generalization ability for 3D GANs. What we discover is that instead directly using few examples if we first can build a style prior of 2D GANs from those images and use the prior to guide transfer learning, the generator works much better. %In other words, matching 3D generative model to 2D style prior leads to the better robust multi-view stylization. 

% Inspired by recent advancements in geometry aware GANs \lj{cite eg3d}, usually a combination of GAN training and NeRF models, we designed a novel system for creating highly personalized artistic portraits, in 3D. 

To tackle the aforementioned challenges, we propose \emph{AgileGAN3D}, a novel \emph{augmented transfer learning} framework for generating high quality stylized 3D portraits, using only a few 2D style exemplars ($\sim$20).
With extensive experiments, we observe the successful transfer learning for 3D GANs relies on adequate style visual supervisions with well estimated camera labels. 
To address the shortage of stylized training data issue, we started with \emph{style prior creation}, that leverages the existing 2D portrait stylization capabilities. Specifically, we trained a 2D portrait stylization module following AgileGAN\cite{Song:2021:AgileGAN}, to first obtain a large number of stylized portraits using real face photos as inputs. 
Some extra benefits of this way of generating 2D style exemplars are that we naturally obtain: 1) pair data between stylized faces and real faces; 2) fairly accurate head pose estimations of generated stylized portraits (by reusing the poses estimated from the corresponding real faces). 
Both of these benefits are incorporated into our \emph{guided transfer learning} step with a reconstruction loss, that helps improve the 3D stylization for out-of-domain samples. Equipped with a transfer-learned style generator, we further introduce an 3D GAN encoder that embeds a real image into an enlarged latent space for better identity-preserved 3D stylization.  A cycle consistency loss is proposed in the 3D GAN encoder training to further improve the multi-view reconstruction fidelity.
% the 3D stylization 
% the training of our 3D GAN inversion module. 
% A cycle consistency loss is added in the 3D GAN encoder training to further improve the multi-view reconstruction fidelity. \lj{i bla bla the following sentences are a bit vague, as we can't explain certain things very clearly yet:} A few key insights we observed while creating a working system include: 1) Geometry-aware 3D GAN fine-tuning is much harder than 2D, it requires a larger amount of supervision samples with accurate head pose conditions. This renders the importance of our \emph{style prior creation} step; 2) 3D GAN inversion is a more complicated process than the 2D one, as we need to preserve both identity and camera pose at the same time, where we have to impose \emph{guided transfer learning} to ensure the out of domain samples are well learned, and add a cycle loss term to stabilize the inversion into a 3D GAN latent space. \lj{I think above section needs more attention. this is the weakness of our paper...} 
To best of our knowledge, we are the first paper to propose generative NeRF based 3D stylized portrait creation using only a limited number of 2D style exemplars.

% This allows us to create high quality stylized Avatar models in a variety of target styles and consistently render in different views. (Fig. \ref{fig:teaser})%Details. 
% In our system, we first design an effective encoder for EG3D to embed the input real image into its latent space. 
% Our encoder preserves the user identity to the largest extent while maintains the visual quality when changing the rendering views, thanks to our novel multiview cycle consistency losses tailored for 3D GANs. 
% After the image embedding, we introduce a novel 2D style prior creation that guides the transfer learning of the 3D GAN generator. Specifically the style prior is learnt from the limited 2D exemplars and is able to generate infinite number of high-resolution stylized examples. This largely expands the stylized visual cues in different views to supervise the transfer learning of the generative radiance fields. Moreover, we apply the style prior to widely accessible real image collections from which we can reliably estimate the rendering cameras. As such, we are able to fine-tune the 3D generator with large corpus of paired style examples with accurate camera labels. We further enhance the 3D stylization with fine-grained details by learning the visual cues from our prior.  We extensively evaluate our methods on various styles including 3D cartoons, comics and oil paintings. We demonstrate visually appealing and identity-preserving stylization of a portrait, from a single-view input image, with full control over camera views. 
% To better capture user input facial features, we present an inversion learning with multi-view cycle loss and regularization to avoid out-of-domain inversion problem. 

% To achieve high quality 3D portrait stylization in our framework, we also introduce a novel style prior creation based on StyleGAN2 to perform guided transfer learning. Our style prior can provide infinite high resolution stylized examples and pairs to guide the transfer learning , which can significantly improve 3D stylization quality. We evaluated our methods on different style including 3D cartoons, comics and oil paintings. We show that we can achieve comparable high quality portrait stylization quality to previous 2D state-of-the-art stylization methods, but with consistent 3D view manipulation ability. To best of our knowledge, we are the first paper to propose generative NeRF based 3D stylized portrait creation using only a limited number of 2D style exemplars ($\sim$100). 

%Contributions
To summarize the contributions of our work:
%\vspace{-0.2cm}

\begin{itemize}
\vspace{-0.05in}
\item A novel pipeline for creating 3D stylized portraits with detailed geometry, given only a single user photo as input. New stylization can be achieved with only a few unpaired 2D style exemplars (around 20). 
\vspace{-0.05in}
\item A simple yet efficient way to fine-tune 3D GAN, first with \emph{style prior creation} to improve data diversity, combined with \emph{guided transfer learning} to increase the stylization domain coverage;
\vspace{-0.05in}
\item A 3D GAN encoder that inverts real face images into corresponding latent space, trained with cycle consistent loss to improve identity preservation, while achieving high stylization quality.
\end{itemize}




\section{Related Work}
%We review some existing researches that are relevant to the portrait stylization problem addressed in this paper.

%\paragraph{Few Examples Stylization}
\paragraph{Face Stylization}
Stylizing facial images in an artistic manner has been explored in the context of non-photorealistic rendering. Early approaches relied on low level histogram matching using linear filters~\cite{10.1145/218380.218446}. Neural style transfer~\cite{7780634}, by matching feature statistics in convolutional layers, led to early exciting results via deep learning. However, they usually fail on styles involving significant geometric deformation of facial features, such as cartoonization. For more general cross-domain stylization, Toonify~\cite{pinkney2020resolution} proposed a GAN interpolation framework for controllable cross-domain image synthesis for cartoonization.
%However, their optimization-based inversion mapping when applied to real images often introduces undesired artifacts. 
A following method AgileGAN~\cite{Song:2021:AgileGAN} proposed VAE inversion to enhance distribution consistency in the latent space, leading to fewer artifacts and better results for real input images. Besides, Huang \etal ~\cite{huang2021unsupervised} achieves multi-domain stylization via a layer swapping technique. Recent exemplar-based approaches~\cite{yang2022Pastiche,chong2021jojogan,li2021anigan} enable one-shot portrait stylization given a single style exemplar. There are also several 2D stylization works for video generations~\cite{yang2022Vtoonify,back2022webtoonme}. In contrast, our proposed \emph{AgileGAN3D} produces highly detailed 3D stylized portraits using the same amount of input as used in 2D stylization methods.

%Recently, Sang et al.\cite{Sang2022AgileAvatar} propose a cascaded domain bridging method to convert a user portrait into a stylized 3D parametric avatar using few exemplars and self-supervised learning. However, this parametric avatar model relies on manually designed assets, and their pipeline cannot quickly adapt to new styles. \lj{remove agileavatar citation, maybe add Vtoonify?}
%However, this 3D parametric avatar asset system requires many designers and creator efforts to build, and their pipeline cannot quickly adopt to new styles. 

% However, this avatar system requires manually designed assets, and their pipeline cannot quickly adopt to new styles

\paragraph{Geometry-Aware GANs}
Generative adversarial networks~\cite{goodfellow2014generative,karras2019style,Karras2020stylegan2} have been used to synthesize images ideally matching the training dataset distribution via adversarial learning.
%, and have gained popularity over the last decade due to their remarkable abilities in photo-realistic image synthesis. 
Built on the success of 2D GANs, recent works have extended to multiview consistent image synthesis with unsupervised learning from unstructured single-view images. The key idea is to combine differential rendering with 3D representations, such as meshes~\cite{Szabo:2019,Liao2020CVPR,gao2022get3d}, point clouds~\cite{li2019pu}, voxels~\cite{wu2016learning,hologan,nguyen2020blockgan}, and recently implicit neural representation~\cite{chan2021pi,niemeyer2021giraffe,chan2022efficient,or2021stylesdf,deng2021gram,epigraf,zhou2021CIPS3D}. Especially the Neural Radiance Fields (NeRF)\cite{gu2021stylenerf,or2021stylesdf} representations, which have proven to generate high-fidelity results in novel view synthesis, are introduced to 3D-aware generative models. They typically use StyleGAN2~\cite{karras2020analyzing} as the backbone to generate intermediate features for MLP to query and perform neural volumetric rendering for image synthesis. Recently, EG3D\cite{chan2022efficient} uses an efficient triplane-based neural radiance field, combined with CNN-based upsampling and a pose-aware dual discriminator to improve synthesis quality and multi-view consistency. Our work further extends the success of existing 3D GAN models to generate stylized results with only a few unpaired 2D stylized exemplars.

\paragraph{GAN Inversion}
%GANs are typically designed to generate realistic images by sampling from a known distribution in latent 
Given an input image, GAN inversion addresses the complementary problem of finding the most accurate latent code to reconstruct that image. Existing approaches roughly fall into three categories: optimization-based, learning-based and hybrid GAN inversion. Optimization-based approaches \cite{karras2019stylebased,Image2StyleGAN,10.1145/3414685.3417803} directly optimize the latent code to minimize the pixel-wise reconstruction loss for a single input instance. Learning-based approaches~\cite{zhu2016generative} train a deterministic model by minimizing the difference between the input and synthesized images. Some works combine these ideas, e.g. learning an encoder that produces a good initialization for subsequent optimization \cite{bau2019seeing}. In addition to image reconstruction, some methods also use inversion when undertaking image manipulation. For example, Zhu \etal~\cite{zhu2020indomain} introduced a hybrid method to encode images into a semantic manipulable domain for image editing. Richardson \etal~\cite{richardson2021encoding} presented the generic Pixel2Style2Pixel (pSp) encoder to embed image into StyleGAN $W^+$ space, based on a dedicated identity loss. %for embedding images in several real image translation tasks such as inpainting and super resolution. 
To balance reconstruction and editing ability for inversion, E4E encoder~\cite{tov2021designing} later uses a progressive training strategy to stimulate $W$ space for $W^+$ space. ReStyle~\cite{alaluf2021restyle} trains a residual based encoder with iterative refinement. Besides training encoder, Pivotal Tuning Inversion\cite{roich2021pivotal} also fine-tunes the generator parameters each time for recovering image details that cannot be encoded in the latent space to improve reconstruction. Wang \etal ~\cite{wang2022high} proposed an approach to achieve high fidelity inversion without inference time optimization. Recent works~\cite{alaluf2022hyperstyle,dinh2022hyperinverter} employ hyper networks~\cite{ha2016hypernetworks} to improve StyleGAN inversion. However, directly adopting 2D GAN encoders for 3D GAN models won't work well, as it is not taking account of the multi-view generation aspect of a 3D GAN model. In our work, we introduce a \emph{multi-view cycle consistent loss} to improve our 3D GAN encoder's capability to preserve subject identities while balancing the stylization quality.

%However, considering efficiency, our method is build upon E4E encoder and introduce a cycle loss to improve multi-view generation.

