\section{Method}


\begin{figure*}[hbt!]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/pipeline.png}
	\centering
	\caption{Pipeline overview. Our 3D stylization pipeline consists of three stages, style prior creation, 3D GAN inversion and guided style transfer learning, with different data training flows indicated in different colors. Specifically given a few 2D style exemplars, we create a 2D style prior (left, yellow) that augments stylistic training samples with well-estimated camera labels from real images. We then perform transfer learning of 3D-aware image generator to target styles using augmented labeled style samples (orange), under the paired guidance of 2D stylization. Additionally we train an  encoder for 3D GAN image inversion (green) into a corresponding latent code in $W^+$ space, from which we can turn into an identity-preserved 3D style portraits.}  
% 	The grey arrows indicate image inversion, and the orange ones are for guided transfer learning using a style prior based on StyleGAN2\cite{Karras2019stylegan2}.}
	\label{fig:overview}
\end{figure*}


%\paragraph{Overview of Pipeline}
As shown in Fig.~\ref{fig:overview}, we first use \emph{style prior creation} to augment the limited 2D style exemplars, in order to supply downstream 3D GAN transfer learning with sufficient training data with well-estimated camera labels (Section~\ref{sec:spc}). Then we train an encoder to map input images into 3D GAN latent space ($W^+$), which well preserves facial identities with a \emph{multi-view cycle consistent loss} (Section~\ref{sec:inversion}). To further improve the stylization quality, we add \emph{guided transfer learning} that removes out-of-domain stylization artifacts(Section~\ref{sec:transfer}). 

\paragraph{3D-Aware Image Generation.}
\label{sec:EG3D}
For multi-view consistent image generation, we build our pipeline on top of a state-of-the-art geometry-aware 3D GAN model named EG3D~\cite{chan2022efficient}. To synthesize an image, the 3D generator $\mathcal{G}_{\phi}$ takes two variables: a latent code $z$, from a standard Gaussian distribution, that determines the geometry and appearance of a subject; a conditional camera pose label $\hat{p}$ added to the latent code. $z$ and $\hat{p}$ are passed through a multi-layer perceptron (MLP) mapping network to obtain a $w$ code, which is duplicated multiple times to 
modulate the synthesis convolution layers that produce tri-plane features. These features are sampled into a neural radiance field at the desired camera angle $p$ and accumulated to generate a raw feature image via volumetric rendering. Finally, the raw feature images are up-sampled by a super resolution module to synthesize the final RGB images. A camera-conditioned dual discriminator $D$ is used to examine the image fidelity in adversarial training, while ensuring multi-view consistency.


%This step enables us to determine the corresponding latent embedding of the input image with its facial features being largest represented by the 3D generator. (B) Following that, we create a high-quality 2D style prior using given exemplars, with which we created a large collection of paired 2D stylized portraits and estimated 3D cameras from real images. We fine tune the 3D generator to target styles using the paired images, and guide the transfer learning accordingly (Section~\ref{sec:transfer}). 


% reorganize method section
%Then starting from a copy of the pre-trained 3D GAN, we fine-tune this generator using this style prior and previously trained encoder.% such that if we encode real image into latent space, it will generate images that better match the stylistic prior.

%Notice that the input of style prior could be few stylized exemplars, single example or text. And the compartmentalization of the pipeline allows greater agility, whereby new style domains can be incorporated by only fine-tuning the generator instead of the entire pipeline.

%two training stages are execution sequentially and can be trained in parallel. However, structurally the two stages have shared pivot latent spaces (Z+ and W+ described later), and are also jointly anchored by the fixed StyleGAN2 generator.

%Notice that the two training stages are execution sequentially and can be trained in parallel. However, structurally the two stages have shared pivot latent spaces (Z+ and W+ described later), and are also jointly anchored by the fixed StyleGAN2 generator.

%There are three unique benefits of breaking down the entire generation problem into two stages: 1) the training does not require paired datasets, unlike typical image-to-image translation methods\cite{pix2pix2017}. 2) the separation of training also enables higher resolutions by reducing computational load in making backpropagation more effective and efficient. 3) the compartmentalization of the pipeline allows greater agility, whereby new style domains can be incorporated by only fine-tuning the generator instead of the entire pipeline.

% Over the following sections, we will first briefly introduce 3D Generator. And, we present our inversion framework including embedding space design and training strategy to improve generative quality. We then present an guided transfer learning strategy that leverages strong stylized StyleGAN2 prior from given style exemplars, and fine tune the generator using a combination of GAN loss, reconstruction loss and regularization loss. Finally at inference stage, the encoder and stylized generator are combined to form a single-pass pipeline.


\subsection{Style Prior Creation}
\label{sec:spc}



\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/Big/Asset_43.png}
    \centering
    \caption[Caption for LOF]{The evolution of transfer learning results using different number of training samples by our 2D style prior. Better visual quality is achieved with more training style exemplars. }
    \label{fig:alba_volume}
\end{figure}

Unlike the 2D GAN-based stylization tasks, the few-shot transfer learning on 3D GAN model is less well studied. A straightforward attempt to 3D portrait stylization will be through fine-tuning a pretrained 3D generator $\mathcal{G}_{\phi}$ directly with the few shot samples, e.g., 20 stylized exemplars. However, plain transfer learning generates poorly in both perceptual quality and user similarity. 
We suspect the problem is rooted in two aspects:
insufficient style exemplars due to the more complicated nature of a 3D GAN architecture and using inaccurate camera pose estimation from style exemplars. 
%Our proposed pipeline augments the vanilla transfer learning with several important improvements as detailed in sections below.

% into a large collection of consistent stylized images, with both precise camera pose conditions and corresponding real face pairs. 

%Unlike the 2D GAN-based stylization tasks, the requirements for 3D GAN transfer learning are less well studied. How many style exemplars are required? How accurate do the camera pose estimations have to be? How about the distribution of the viewing angles? To better answer these questions, we need a controlled way to augment few shot style exemplars into a large quantity of diverse samples, with accurate camera pose estimations, %\lj{distribution?}.

%\hy{maybe need to explain a bit more on 2d style prior creation. as least cite the related papers.}

%Specifically, we trained a 2D portrait stylization module following AgileGAN\cite{Song:2021:AgileGAN}, to first obtain a large number of stylized portraits using real face photos as inputs ($\sim20k$). Therefore we naturally obtain benefits of: 1) pair data between stylized faces and real faces; 2) fairly accurate head pose estimations of generated stylized portraits (by reusing the poses estimated from the corresponding real faces). Both prove to be critical in 3D GAN style transfer.

To mitigate the above problems, instead of directly using given stylized exemplars to fine-tune the 3D generator, we create a style prior based on 2D GAN to guide the transfer learning, which are less complicated and does not need camera pose.
Here we leverage the capability of the state-of-the-art 2D stylization methods for \emph{style prior creation}. Inspired by recent 2D stylization works~\cite{pinkney2020resolution,Song:2021:AgileGAN}, we perform transfer learning on top of the original StyleGAN2 generator $\mathcal{S}$ trained on FFHQ dataset~\cite{karras2019style}, with the few shot style exemplars. We denote the fine-tuned styled generator as $\mathcal{S}_{t}$. This gives us the capability to turn widely-accessible real portrait images into augmented 2D style samples. Moreover, this augmentation approach naturally offers pairs of stylized and real images, from the latter of which we can obtain accurate camera pose labels with off-the-shelf pose estimator such as \cite{3DPortraitfrom}. 


\paragraph{Transfer Learning Loss}
 By sampling from prior latent space, we can get infinite high-quality diverse stylized images for transfer learning on 3D GANs. We use an adversarial loss to fine-tune the pre-trained 3D-aware generator $\mathcal{G}$ with respect to its parameter $\phi$ as well as its dual discriminator $D$, that matches the distribution of the translated images to the style prior distribution:
\begin{equation}\label{eq:loss_decoder_adv}
\begin{split}
    \mathcal{L}_{prior} = &
      \mathbb{E}_{z_s \sim N(0,I) }[min(0, -1+D(\mathcal{S}_t(z_s), p))] + \\
    & \mathbb{E}_{z \sim N(0,I)} [min(0, -1-D(\mathcal{G}_{\phi}(z, p), p))]
\end{split}
\end{equation}
where the latent code $z_s$ and $z$ are from StyleGAN2 and EG3D latent space respectively. 
We also apply regularization terms for stable fine-tuning. For discriminators, we use R$_1$ path regularization. 


Our style prior leads to significant quality improvements, as shown in Fig.~\ref{fig:alba_volume}. We believe this is a requirement imposed by the NeRF module inside the 3D GAN pipeline. Typically, visual observations from a wide camera distribution are necessary for NeRF to reason the underlying 3D scene geometry and its corresponding appearance. Thus fine-tuning the cross-domain 3D generation of neural radiance features requires much more style samples than prior 2D GAN-based approaches. 

 
% \begin{figure}
%     \centering
%     \includegraphics[width=1.0\linewidth]{figures/FS/pose.png}
%     \centering
%     \caption[Caption for LOF]{Using more accurate pose labels improves expression preservation. Please note the subject's mouth.}
%     \label{fig:alba_pose}
% \end{figure}

Another point is the camera pose. For certain artistic styles, direct camera pose estimations from the input style exemplars might not be very accurate, which also affects 3D stylization. Different from 2D StyleGANs that directly up-sample feature maps into images via several convolution layers, 3D GAN synthesizes images by first accumulating neural radiance features via volume rendering to a feature map and then rely on super resolution to obtain the final image. Both the volume rendering and dual discriminator require reliable estimation of camera parameters, which are not easy to accurately obtained from non-realistic style examples. %Fig.~\ref{fig:alba_pose} demonstrates the benefits of the accurate poses obtained through our \emph{style prior creation} step.


%With the collection of 2D pose-labeled stylized images synthesised from our style prior,


\subsection{Encoder Inversion with $W^+$}
\label{sec:inversion}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/cycle_loss2.png}
    \centering
    \caption{Illustration of multiview cycle consistency loss. Sampling from the Gaussian-distributed latent space, we synthesize facial images at different random poses, from which we minimize the difference of our encoded latent codes with the input.}
    \label{fig:encoder}
\end{figure}

Assisted with the style prior creation step, we are able to achieve desirable 3D GAN stylization quality. One remaining challenge to complete the pipeline is the capability to invert a real face image into the latent space for 3D stylization.
\paragraph{Embedding Space and encoding}
 The pre-trained EG3D model~\cite{chan2022efficient} is equipped with two latent spaces: the original latent space $Z$ under a Gaussian distribution, and a less entangled $W$ space via a mapping network from $z$ with a conditional camera pose label $\hat{p}$. In spite of the camera swapping strategy, we observe the tri-plane generation is not fully decoupled from the pose, inducing varying geometry and appearance along with the change of $\hat{p}$. Therefore we use $W$ space for our image embedding, and augment it to $W^+$ space that significantly increases the model expressiveness. In contrast to modulating the convolutional kernels with a same  code $w$, $W^+$ produces a different $w$ latent code for each layer, allowing for individual attribute control. In our case, a code $w^+$ in $W^+$ space has a dimension of $17 \times 512$, which can be represented as 17 $w_i$ codes, where the $w_0$,...,$w_{13}$ codes are for tri-plane generation, and $w_{14},...,w_{16}$ are used in super resolution module. 

% a code $w^+$ in $W^+$ space has of $17 \times 512$, which can be represented as 17 512-dimensional $w_i$ codes, where the $w_0$,...,$w_{13}$ codes are for tri-plane generation, and $w_{14},...,w_{16}$ are used in super resolution module. 
 
 %This is equivalent to stacking multiple versions of the original latent space $Z$ to form a new space $Z^+$. Although $W+$ provides greater space for pixel-level reconstruction,  to reduce out-of-domain inversion problems. 1) $W$ space is generated by combination of $Z$ and camera pose condition. We can use pose information. 2) $W+$ is rather complex space, and directly encoding images into it leads to out of domain problems, resulting in blur generative image and artifact on geometry surface.

%\paragraph{Hierarchical Encoder.}
For fast inference, we train an encoder for image inversion, with the expectation of preserving user features to the largest extent. We design our encoder based on the architecture used in StyleGAN2 encoder, E4E~\cite{tov2021designing}, but fully exploit the unique proprieties of 3D GAN in generating view-consistent contents. In particular, we utilize the hierarchy of a pyramid network to capture different levels of detail from different layers. The input image resized at 256$\times$256 resolution is passed through a headless pyramid network $\mathcal{E}_\theta$ to produce three levels of feature maps at different sizes. Each level's feature map then goes through a separate sub-encoder block to produce the $W^+$ style code. % corresponding to tri-plane generation, and super resolution module.

\paragraph{Encoder Training Loss}
Even though our chosen $W^+$ space offers a large degree of freedom and expressiveness in representing real human faces, straightforward encoding without regularization can easily lead to out-of-domain issues, where the synthesized images present undesired artifacts like blurriness and noise. To prevent the encoder from over-drifting from the representation domain of $\mathcal{G_{\phi}},$ we introduce a \emph{multi-view cycle consistent loss}, as shown in Fig~\ref{fig:encoder}. The core idea is that the encoder should reproduce the latent code from a synthesized image conditioned on $w$ but rendered from arbitrary views. 
Specifically, a collection of latent codes randomly sampled under the standard Gaussian distribution, together with a fixed frontal camera pose, are fed into the mapping network and obtain $w$ samples. Note that these in-domain $w$ samples are complied with the original distribution of EG3D and likely to synthesize high-quality images without artifacts, and are a special form in $W^+$ space as well. Essentially training the encoder with these in-domain samples prevents the output $w^+$ codes from drifting far-away from the $W$ space. We synthesize the images with $N$ random camera poses $p_1, p_2,..$ from training dataset camera distribution and supervise the training of $\mathcal{E_\theta}$ with ground-truth $w^+$ labels. 


\begin{equation} 
\label{eq:loss_cycle}
 \begin{split}
\mathcal{L}_{cyc} =\sum_{i=1}^{N}\mathcal{L}_{2}(w^+,\mathcal{E}_\theta(\mathcal{G_{\phi}}(w,p_i))).
 \end{split}
\end{equation}




In addition to the \emph{multi-view cycle consistent loss}, our encoder is at the same time trained with reconstruction losses and regularization loss in a weighted combination manner, while freezing the EG3D generator weights.

%\lj{which figure shows the effectiveness of our encoder? Ideally we reference the figure from here.}

Let $x$ be the input image, passed through an encoder and decoder to yield $\hat{x} = \mathcal{G}_{\phi}(\mathcal{E}_\theta (x) )$
\begin{equation}
\label{eq:loss_rec}
\begin{split}
    \mathcal{L}_{rec} = \mathcal{L}_{2}(x, \hat{x}) + \mathcal{L}_{lpips}(x, \hat{x}) + \mathcal{L}_{arc}(x, \hat{x})
 \end{split}
\end{equation}
The $\mathcal{L}_{2}, \mathcal{L}_{lpips}, \mathcal{L}_{arc}$ respectively measure the pixel-level, perceptual-level similarities \cite{zhang2018perceptual} and facial recognition-level similarity differences. $\mathcal{L}_{arc}$ is based on the cosine similarity between intermediate features extracted from a pre-trained ArcFace recognition network~\cite{deng2018arcface}, evaluating the identity similarity. A regularization term is further introduced to reduce the divergence of $w^+$ code to mimic the origin $W$ space for the best of image quality, 
\begin{equation}\label{eq:loss_reg}
\mathcal{L}_{reg}   =  || \textrm{div}(\mathcal{E}_\theta (x)) ||_2.
\end{equation}


% In combination, our total loss becomes
% \begin{equation}\label{eq:loss_rec_all}
% \underset{\theta}{\mathrm{min}} ~  \mathcal{L}_{rec}+ w_{cyc} \mathcal{L}_{cyc} +w_{reg} \mathcal{L}_{reg}
% \end{equation}
% where $w_{cyc},w_{reg}$ are relative weights for the cycle loss and divergence loss respectively. The EG3D generator is frozen during the inversion learning.

% \vspace{0.15cm}


\subsection{Guided Transfer Learning}
\label{sec:transfer}

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/Big/Asset_52.png}
    \centering
    \caption[Caption for LOF]{Our guided transfer learning loss helps improve generative quality and resolve fine-level visual artifacts.}
    % results where style prior components in the generator are ablated (without guide loss, without style prior and our full method). \lj{show the effectiveness of SPC, need to update the labels under the images, maybe show quantity, pose distribution, how they affect the final quality}}
    \label{fig:alba_guide}
\end{figure}
% get infinite number of high-quality style images by sampling its latent space $Z_s$. 

% To solve the inaccuracy camera pose problem, we utilize the natural correspondence between the $\mathcal{S}(z_s)$  and $\mathcal{S}_{t}(z_s)$ when given the same latent codes, and get relatively accuracy camera pose label from realistic image $\mathcal{S}(z_s)$ from an off-the-shield pose estimator. In Fig.~\ref{fig:alba_prior}, we show the strength of using prior.%Please note that 2D GAN stylization can also use one-shot image, or language as input. We can also integrate it into our pipeline without additional cost. 
To further improve the 3D stylization quality, especially for cases where the inverted codes might be still not well aligned with the original distribution, the stylized images might contain artifacts, such as blurriness. By combining fine-tuning and inversion, we propose a guided transfer learning to enlarge the transfer learning space from its Z space to $W^+$ space with stronger generative stylization capability.

Thanks to the real to stylized face paired data that are produced in \emph{style prior creation} step, we are able to guide the transfer learning of our 3D generator with a reconstruction loss. Given a real image $x$ with estimated camera $p$ and its 2D stylized pair $x_s$, let $\hat{x_s} = \mathcal{G}_{\phi}(\mathcal{E}_{\theta}(x), p)$ we have:
\begin{equation}\label{eq:loss_decoder_rec}
\begin{split}
    \mathcal{L}_{guide} = \mathcal{L}_{2}(x_s, \hat{x_s}) + \mathcal{L}_{lpips}(x_s, \hat{x_s}) 
\end{split}
\end{equation}
This guidance loss can help stabilize the generation training, and also improve the generative quality and user similarity, as illustrated in Fig.~\ref{fig:alba_guide}. 
We fine-tune the 3D generator and discriminator with the encoder and style prior frozen. 

% Besides using the prior to solve dataset problems, we can also further improve the 3D generative quality by using a reconstruction loss during transfer learning. For given real images $I$, we have the trained EG3D encoder $\mathcal{E}$ to encode the real image into EG3D space. On the other hand, we encoded it into StyleGAN2 latent space via off-the-shield pre-trained encoder $\mathcal{E}_{s}$ and obtained its corresponding stylized images $I_s=\mathcal{S}_{t}(\mathcal{E}_{s}(I))$, which can be used as a supervision for the transfer learning from reconstruction and perceptual level:


% Finally, the generator and discriminators are jointly trained to optimize the combined objective of:
% \begin{equation}\label{eq:loss_decoder_all}
% \underset{\phi}{\mathrm{min}} ~\underset{D}{\mathrm{max}} \mathcal{L}_{prior}+  w_{guide}  \mathcal{L}_{guide} + w_{R1}  \mathcal{L}_{R1} ,
% \end{equation}
% where $w_{guide} =1, w_{R1}=1$ are relative weights for the adversarial loss and regularization loss respectively. And the EG3D encoder and style prior are frozen during the transfer learning.

% \vspace{0.15cm}
% \paragraph{Dataset.}

% \vspace{-0.15cm}




%\subsection{Inference}
%Given an input face image $x$, it is first warped and normalized to 256$\times$256 based on its landmarks, and encoded by the hVAE to get the latent Gaussian posterior distribution $q(z|x)$. Since this posterior / importance distribution is only relevant during hVAE training, we typically do not sample from this distribution during inference, but directly use the distribution mean as the latent code $z$, which will also better maintain temporal consistency. This $z$ is then passed to the chosen stylized generator to generate a 1024$\times$1024 stylized image. However, in rare cases there may be high frequency artifacts generated.
%, such as black blobs.
%In these cases, we can fall back on sampling multiple instances from the imputed Gaussian distribution, leading to multiple output images. We can then select one without artifacts, either manually or choosing the one with the smallest average perceptual distance \cite{zhang2018perceptual} among the output images. For the gender attribute, a simple external pre-trained gender detector network~\cite{6906255} is used. In total, the inference stage takes around 130ms per image.




