\section{Experimental Analysis}

\begin{figure*}[h]
	\centering
	\includegraphics[width=0.95\linewidth]{figures/Big/Asset_53.png} %compare_encoder2.pdf 
	\centering
	\caption[Caption for LOF]{3D artistic portraits generated from a variety of input images. From left to right, we show the input image, and generated stylistic multi-view images and geometry with our pipeline. Please refer to supplementary materials for higher-resolution qualitative results.}
	\label{fig:our_result}
\end{figure*}

\subsection{Implementation Details}
Our encoder is trained on the CelebA-HQ dataset~\cite{CelebAMask-HQ} containing 30,000 high quality face images, where we use the first 28000 for training and the rest 2000 for testing. For consideration of computational efficiency, the input images are down-sampled to 256$\times$256. The pre-trained EG3D uses the weights from the FFHQ 512-128 model \cite{Karras2019stylegan2}.  We empirically set $\lambda_{reg}=0.001$ and $\lambda_{cyc}=1$ with 2 random camera poses. We minimize the objective function for 20 epochs using the Rectified Adam solver \cite{liu2019radam}, with a batch size of 2 and a learning rate of $5 \times 10^{-4}$.

For transfer learning, we collect the initial 20 2D style exemplars from multi-image asset websites\cite{turbosquid, pinterest} for each style, with which we train the style prior. For 3D GAN transfer learning, we use CelebA-HQ as real images in the guided transfer learning loss. Initialized with a pretrained EG3D model, the weights of the generator and discriminators are fine-tuned at a learning rate of 0.002 with a batch size of 4. We limit the number of iterations around 8K images. 
%Additionally we modify the volume rendering camera extrinsics to comply with CelebA-HQ format.



% \subsection{Artistic Portrait Generation}
% \lj{what is the point of this section? maybe remove?}


\subsection{Comparisons}
\subsubsection{3D-Aware Stylization}
In Fig.~\ref{fig:our_result}, we present more 3D portrait stylization results from our method. A diverse range of styles demonstrate that our method can robustly handle input images that represent a variety of genders, face shapes and hair styles under different illumination conditions, creating visually appealing, multi-view consistent stylization.

Since there is no prior few-shot 3D stylization methods that we can compare directly, we fine-tune a 3D generator as used in our method, following 2D stylization methods AgileGAN\cite{Song:2021:AgileGAN} and Toonify\cite{pinkney2020resolution}. We name these two hybrid methods as \emph{AgileGAN-EG3D} and \emph{Toonify-EG3D}, and compare to them both quantitatively and qualitatively. 
For \emph{Toonify-EG3D}, we perform direct transfer learning of the generator with the provided style exemplars, and the inversion is achieved with an optimization. For \emph{AgileGAN-EG3D}, in addition to transfer learning the generator, we follow their setting to train a hierarchical variational encoder for image inversion. 
%More results and test code are provided in the supplementary file.

\paragraph{Qualitative Evaluation} 
In Fig.~\ref{fig:compare_sota}, we present visual comparisons against the two baseline methods. In constrast with \emph{AgileGAN-EG3D} and \emph{Toonify-EG3D} results exhibiting noticeable artifacts, our approach demonstrates 3D stylization with superior perceptual quality and identity preservation. 

\begin{figure*}
	\centering
	\includegraphics[width=0.95\linewidth]{figures/Big/Asset_54.png}
	\centering
	\caption[Caption for LOF]{Our method visually outperforms direct transfer learning of EG3D generator following 2D few-shot stylization  AgileGAN\cite{Song:2021:AgileGAN} and Toonify\cite{pinkney2020resolution}. Our AgileGAN3D depicts identity-preserved 3D style portraits with fine-level details.}
	\label{fig:compare_sota}
\end{figure*}
%\vspace{0.1cm}

\begin{table}
\caption{Stylization LPIPS  $\downarrow$ for different stylization methods}
\centering
\begin{tabular}{cccc}
    \toprule
       &Ours & AgileGAN-EG3D & Toonify-EG3D \\
    \midrule
    Cartoon &\textbf{0.195} &0.440& 0.481   \\
    Oil Painting &\textbf{0.212} &0.433& 0.525   \\
    Comic &\textbf{0.218} &0.379& 0.486   \\
    Sam Yang &\textbf{0.20} &0.445& 0.506   \\
    Sculpture &\textbf{0.234} &0.469& 0.549   \\
    \bottomrule
\end{tabular}
\label{tab:quantitative}
\end{table}


%\vspace{0.1cm}
\paragraph{Quantitative Evaluation}
In  Table~\ref{tab:quantitative}, we also quantitatively measure the visual quality by evaluating a perceptual distance loss between the results of 3D style generator and 2D style prior, which we refer as Stylization LPIPS. The evaluation is performed on CelebA-HQ test images. Given the high quality 2D stylization (without 3D consistent manipulation capability though), we can consider a lower perceptual distance indicating higher visual quality, where our method outperforms the baselines substantially. We also compute a perceptual evaluation with user study, and please refer to our supplemental materials.

%We also compute a perceptual distance between user study in which 51 participants were shown stylization results from different methods, and asked to select the best cartoonized images. Each participant was shown 15 questions randomly selected from a question pool containing 200 examples (using images with indices 0-200 in the CelebA-HQ dataset). Table.~ \ref{tab:quantitative} shows that results from our proposed method had the majority preference.

%Another metric to quantitatively evaluate generative quality is the Fr√©chet Inception Distance (FID) score \cite{NIPS2017_8a1d6947}, which measures the visual similarity and distribution between two datasets of images. Each method generated stylized images from the CelebA-HQ dataset as input, and we computed the FID to the training cartoon dataset. We can see from Table~\ref{tab:quantitative} that our method also achieved the best performance on this metric, \textcolor{black}{although it should be noted that since there are fewer than 5K images in the CelebA-HQ test set, FID scores may not be reliable.}

%\vspace{0.1cm}
% \paragraph{Alternative Encoder Methods}

% \begin{figure*}
% 	\centering
% 	\includegraphics[width=1.0\linewidth]{figures/compare_encoder.png} %compare_encoder2.pdf 
% 	\centering
% 	\caption[Caption for LOF]{Stylization results of our transfer-learned model when using the different encoder methods of E4E\cite{tov2021designing}, hVAE \cite{Song:2021:AgileGAN}, optimization (as proposed in \cite{chan2022efficient}). Notice that results from other methods had unnatural blurry texture, and identity loss.}
% 	\label{fig:compare_encoder}
% \end{figure*}

% We compared our encoder to alternative embedding methods, including E4E encoder\cite{tov2021designing}, hVAE encoder\cite{Song:2021:AgileGAN} and optimization based inversion, by evaluating with these substitutes. For the E4E and hVAE encoders, we re-trained their model using the authors' original code and settings for EG3D on the CelebA-HQ dataset. We also compared to the directly iterative optimization in $Z$ space.

% In Fig.~\ref{fig:compare_encoder}, it can be seen that our method created inverted images that are perceptually more pleasant. The structure and losses of the alternative encoders are geared towards image reconstruction rather than cross-domain consistency, which may lead to artifacts such as unnatural color patches and blur in texture. 

\subsubsection{Multiview Manipulation Consistency} 
\begin{figure}
	\centering
	\includegraphics[width=1.0\linewidth]{figures/smooth_check.png}
	\centering
	\caption[Caption for LOF]{
	We compare our method (b) against 2D AgileGAN\cite{Song:2021:AgileGAN} (a) in multiview consistency. We manipulate stylization result from the left to right and horizontally stack a vertical segment of pixels from each generated image (middle). Our method shows a more natural visual transition, indicating better view consistency.}
	\label{fig:smooth_check}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{figures/Big/Asset_47.png}
\centering
\caption[Caption for LOF]{With the prior knowledge of camera extrinsics, our method demonstrates more robust large-pose stylization, compared to 2D AgileGAN\cite{Song:2021:AgileGAN}.}
	\label{fig:large_pose}
\end{figure}


% \lj{does this belong to ablation study? I think Figure 10, 11 actually belong to Qualitative Evaluation.}
By leveraging a 3D-aware image generator, our method achieves multiview consistent stylization. 2D stylization approaches like AgileGAN\cite{Song:2021:AgileGAN} support limited view manipulation via modifying the latent code\cite{shen2020closedform} but exhibits noticeable visual inconsistency. 
In Fig.~\ref{fig:smooth_check}, we  visually compare the view consistency using Epipolar Line Images (EPI) similar to~\cite{xiang2022gram}, where our method shows smooth and natural pattern transition when rotating the rendering camera.  Additionally benefiting from camera-disentangled image synthesis capability, our AgileGAN3D is more robust in  large-pose stylization as depicted in Fig.~\ref{fig:large_pose}.
% To further demonstrate 3D stylization, we compare our method to 2D stylization AgileGAN\cite{Song:2021:AgileGAN} on view manipulation and large pose stylization tasks. We follows AgileGAN\cite{Song:2021:AgileGAN} settings and its cartoon dataset, and retrain our model. AgileGAN\cite{Song:2021:AgileGAN} supports view manipulation via latent code manipulation\cite{shen2020closedform}. In Fig.~\ref{fig:smooth_check}, it shows that our method can better generate smooth view animation. Further results can be seen in the supplementary video. Also in Fig.~\ref{fig:large_pose}, our method can generate better visual stylization quality for large pose case, due to 3D neural representation.

\subsection{Ablation Studies}

% To further verify the usefulness of the designed modules in our guided transfer learning, we conducted ablation studies by removing each component, with results shown in Fig.~\ref{fig:alba_generator}: (1) 'w/o guide': remove the guide loss. (2) 'w/o prior': remove the style prior and transfer learning the generator directly from given few exemplars. For each study, we retrained the generator using the same settings, and also used same encoder. For 'w/o guide', we can see using our guide loss, the generator can perceptually better stylization results with reduced artifacts on the skin. For 'w/o prior', removing style prior led to insufficient generative ability and blur texture. 




\begin{figure}
	\centering
	\includegraphics[width=1.0\linewidth]{figures/Big/Asset_51.png}
	\centering
	\caption[Caption for LOF]{Multiview cycle consistent loss improves image inversion with higher perceptual quality and similarity. }
	\label{fig:abla_encoder}
\end{figure}


\paragraph{Inversion Learning}
In Fig.~\ref{fig:abla_encoder}, we evaluate the efficacy of our cycle consistency loss introduced in our encoder for image inversion. With our loss, the encoder presents higher perceptual quality and identity similarity, numerically evidenced with better reconstruction losses in Tab.~\ref{tab:encoder_quantitative}. 
% We can see using our loss, and the encoder can generate perceptually better reconstruction results with reduced artifacts, which can lead to lower LPIPS score.  





\begin{table}
	\caption{Quantitative ablation of cycle consistency loss, evaluated from 2K testing images.}
	\centering
	\begin{tabular}{cccc}
		\toprule
		Algorithm & MSE  $\downarrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ \\
		\midrule
		Ours w/o Cycle Loss & 0.0203 & 0.525 & 0.317 \\
		Ours  & \textbf{0.0200} & \textbf{0.543} &\textbf{0.194} \\
		\bottomrule
	\end{tabular}
	\label{tab:encoder_quantitative}
\end{table}

\paragraph{Transfer Learning}
We evaluate the effect of training sample quantity over the stylization quality both numerically in Tab.~\ref{tab:transfer_quantitative} and qualitatively in Fig.~\ref{fig:alba_volume}. The experiment is performed over sam yang style. We compare against 2D AgileGAN stylization over test CelebA-HD images, where we observe closer perceptual quality as the number of training exemplars increase, demonstrating the efficacy of our augmented transfer learning. Additionally, our guided transfer learning further improves the perceptual score,  as also visually evidenced in Fig.~\ref{fig:alba_guide}. We note that using camera labels estimated from style images leads to degenerated perceptual quality. That being said, our paired camera labels from real images help 3D GAN transfer learning. 
% We evaluate the efficacy of our style prior and guided transfer learning.  

% With our style prior to provide large amount samples, pairs data and relative accurate pose label, the generator  presents higher perceptual quality with less visual artifacts, numerically evidenced with better reconstruction losses to the style prior in Tab.~\ref{tab:transfer_quantitative}.    

%Without accurate pose means the pose labels estimated from stylistic image.
\begin{table}
\caption{Stylization Perceptual scores $\downarrow$ with different training samples}
\centering
\begin{tabular}{cc}
\toprule
	\# Training samples & LPIPS$\downarrow$ \\
		\midrule
    20 &0.41   \\
    100 &0.252 \\
    1000 &0.236    \\
    8000 &0.227    \\
    \midrule
    8000(with guided loss, without accurate pose) &0.211 \\
    8000(with guided loss) &\textbf{0.200} \\
\bottomrule
\end{tabular}
\label{tab:transfer_quantitative}
\end{table}



% \begin{figure}
% 	\centering
% 	\includegraphics[width=1.0\linewidth]{figures/more_result.png}
% 	\centering
% 	\caption[Caption for LOF]{More styles.}
% 	\label{fig:more_styles}
% \end{figure}


% \vspace{-0.1cm}
% \paragraph{Multi-Path Structure}

% To evaluate the usefulness of our multi-path generation module, we replaced it with a single-path structure and retrained the network on our cartoon dataset.  From Fig.~\ref{fig:compare_dual}, we can see that using a dual-path structure can better generate gender-associated facial features in terms of facial geometry and length of eye lashes, \textcolor{black}{and also enhance age-based features such as wrinkles.}



% \paragraph{Fine-tuned Only vs Layer Swapping}

% \textcolor{black}{Instead of using a fine-tuned generator directly, Toonify's stylization is done by additionally swapping or blending in higher layer weights from the original StyleGAN2. We also investigated this approach for our framework, which trades off stylization for increased realism. Fig.~\ref{fig:layer_swap} compares our method to Toonify under two settings: when only the fine-tuned stylized model is used, and when layer swapping is used. We can see that our method produced perceptually better stylization results and contained less artifacts.}


% \subsection{Applications}







