% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% Project specific imports
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{nicematrix,tikz}
\usepackage{stackengine}
\usepackage{authblk}
\usepackage{enumitem,kantlipsum}
\usepackage{arydshln}


% Project specific commands
\newcommand{\coref}[0]{CR}
%% DRAFTING:
\newcommand{\todo}[1]{ {\color{olive} #1} }

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Investigating Failures to Generalize for Coreference Resolution Models}

\author[1]{Ian Porada}
\author[2]{Alexandra Olteanu}
\author[2]{Kaheer Suleman}
\author[2]{Adam Trischler}
\author[1]{Jackie Chi Kit Cheung}
\affil[1]{Mila, McGill University}
\affil[ ]{{\tt \{ian.porada@mail, jcheung@cs\}.mcgill.ca}}
\affil[2]{Microsoft Research Montr\'eal}
\affil[ ]{{\tt \{alexandra.olteanu, kasulema, adam.trischler\}@microsoft.com}}

\begin{document}
\maketitle
\begin{abstract}
Coreference resolution models are often evaluated on multiple datasets.
%
Datasets vary, however, in how coreference is realized---i.e., how the theoretical concept of coreference is \emph{operationalized} in the dataset---due to factors such as the choice of corpora and annotation guidelines.
% AO: leaving some notes to incorporate later -- I think we want models to generalize across types of coreference instances and types, so perhaps we can mention understanding performance variation both across and within datasets
We investigate the extent to which errors of current coreference resolution models are associated with existing differences in operationalization across datasets (OntoNotes, PreCo, and Winogrande).
%
Specifically, we distinguish between and break down model performance into categories corresponding to several types of coreference, including coreferring generic mentions, compound modifiers, and copula predicates, among others.
%
This break down helps us investigate how state-of-the-art models might vary in their ability to generalize across different coreference types.
In our experiments, for example, models trained on OntoNotes perform poorly on generic mentions and copula predicates in PreCo.
%
Our findings help calibrate expectations of current coreference resolution models; and, future work can explicitly account for those types of coreference that are empirically associated with poor generalization when developing models.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

\begin{table}[t]
    \centering
    \small
    \begin{tabular}{@{}l  p{0.75\linewidth}@{}}
       % \toprule
        Dataset & Example \\
        \midrule
        OntoNotes & \ldots \, [President Bush]\textsubscript{1} unveiled [a seven point one billion dollar plan to help protect Americans against the threat from bird flu]\textsubscript{2}. [It]\textsubscript{2} includes stockpiling enough vaccine to protect twenty million people. \, \ldots \\[.6em]
        PreCo & \ldots \, [American inventors of [the Solar Road]\textsubscript{1}]\textsubscript{2} said that [it]\textsubscript{1} will power [itself]\textsubscript{1}, and reduce [[the country's]\textsubscript{3} carbon marks]\textsubscript{4}. \, \ldots
\\[.6em]
Winogrande & [Robert]\textsubscript{1} woke up at 9:00am while [Samuel]\textsubscript{2} woke up at 6:00am, so [he]\textsubscript{1} had less time to get ready for school. \\
        \bottomrule
    \end{tabular}
    \vspace{-8pt}
    \caption{Illustrative examples of annotated coreference from each dataset we consider. Brackets mark each mention, and the subscript identifies to which entity the mention refers. The Winogrande example is reproduced from \citet{winogrande_2020}.}
    \label{tab:coref-examples}
\end{table}

Coreference resolution (CR) is the task of identifying expressions in a discourse that refer to the same entity, concept, or event (see Table~\ref{tab:coref-examples} for examples). CR is considered a core part of constructing representations of natural language \citep{hobbs1978resolving,sukthanker2020anaphora} and important in tasks such as question answering \citep{dasigi-etal-2019-quoref}, summarization \citep{liu-etal-2021-coreference}, and machine translation \citep{ohtani-etal-2019-context}.

CR models are increasingly evaluated on multiple datasets \citep{xia-van-durme-2021-moving,toshniwal-etal-2021-generalization}.  Datasets vary, however, in how coreference is realized---i.e., how the theoretical concept of coreference  is operationalized in the dataset---due to factors such as the annotation guidelines, task format, and corpora, among others. The relationship between these factors and model performance is often obscured in existing studies of generalization which report performance mostly using aggregate metrics that do not account for differences in how coreference might be operationalized \citep{zeldes-zhang-2016-annotation,toshniwal-etal-2021-generalization,crac-2022-crac}.

We conjecture that certain errors CR models make across datasets are associated with differences in how coreference is operationalized, and failing to account for these differences can thus make it difficult to assess to what extent these models actually generalize.

This could occur, for example, when a certain type of coreference is included in the annotation guidelines of a dataset the model is trained on, but not in another dataset the model might be tested on.
These types of differences have not been previously quantified, with prior work examining differences in operationalizations focusing largely on cross-genre adaptation~\cite{moosavi-strube-2017-lexical,zhu-etal-2021-ontogum}.
We argue that investigating variations in performance that depend on the types of coreference models are trained and tested on is critical to developing more generalizable CR models.

We therefore investigate errors that occur when models trained on one dataset are tested on another without any additional training. We first identify different types of coreference based on several decisions related to how the datasets operationalize coreference: how coreference instances are annotated (according to the annotation guidelines) and the underlying corpora.
We then report desegregated evaluations across the types of coreference we identified, including generic mentions, compound modifiers, and copula predicates, among others.  
%
In our evaluation, for example, a state-of-the-art model trained on OntoNotes incorrectly predicts that these two mentions of ``street artists'' are not coreferring in the PreCo development set:
\begin{quote}
    Now, even art museums and galleries are collecting the work of [street artists]\textsubscript{1}. \ldots \ Also, because it is illegal to paint public and private property without permission, [street artists]\textsubscript{1} usually work secretly.
\end{quote}
This could be because bare plurals (plurals that do not contain an article) are not annotated in OntoNotes as generic mentions.

In our experiments, we focus on three English-language CR datasets: OntoNotes 5.0 \citep{hovy-etal-2006-ontonotes}, PreCo \citep{chen-etal-2018-preco}, and Winogrande \citep{winogrande_2020}. OntoNotes and PreCo are annotated at a document level for entity CR, while Winogrande is based on difficult, intra-sentence pronominal anaphora that are manually crafted to require commonsense reasoning to resolve. Incorporating both general and commonsense CR datasets allows us to probe the strengths and weaknesses of models in generalizing between the different ways these datasets operationalize CR.

We find that model generalization varies greatly across the types of coreferences that we consider. For example, models trained on OntoNotes fail to generalize to certain generic mentions in the PreCo dataset that are not annotated in the OntoNotes training data.
%
At the same time, we observe cases where models do appear to generalize to a certain extent to given types of coreference in datasets other than that on which they were trained.

Our findings demonstrate that to effectively assess how well CR models generalize, we need to make explicit how coreference is operationalized within and across datasets and investigate performance variations based on different operationalizations. While this work focuses on the type of coreference instances that are included or annotated in a dataset, other operationalization aspects like variations in data processing or task format may have a similar impact.

\section{Related Work}
\label{sec:related-work}

\paragraph{Models} Early CR models were based on hand-designed rules and features \citep{raghunathan-etal-2010-multi}; however, state-of-the-art models are currently based on supervised training of a given pre-trained language model encoder, such as BERT \citep{joshi-etal-2019-bert,https://doi.org/10.48550/arxiv.2210.14698}. These approaches broadly use either \textit{mention/entity ranking} \citep{lee-etal-2017-end} or \textit{shift-reduce} \citep{webster-curran-2014-limited} architectures. In our analysis, we include the state-of-the-art mention ranking models \textit{Word-level Coreference} (\textit{WLC}; \citealp{dobrovolskii-2021-word}) and \textit{LingMess} \citep{otmazgin2022lingmess}, as well as the entity-ranking model \textit{Longdoc} \citep{toshniwal-etal-2021-generalization}.

\paragraph{Error Analysis}
Our work also relates to efforts to identify the type of errors models make. 
Existing error analyses of CR models, however, have largely focused on performance on a single dataset and identified difficult types of coreference such as those requiring semantic knowledge \citep{uryupina-2008-error,durrett-klein-2013-easy}. \citet{lu-ng-2020-conundrums} find that many types of errors are consistent across datasets, but they do not consider how models generalize from one dataset to another.

\paragraph{Generalization}
To improve CR models ability to generalize, several approaches have been proposed for training CR models using multiple training sets \citep{xia-van-durme-2021-moving,https://doi.org/10.48550/arxiv.2210.07602,zhao-ng-2014-domain,yuan-etal-2022-adapting, yang-etal-2012-domain,toshniwal-etal-2021-generalization}.
%
Studies that consider the generalization of models to datasets directly, without additional training data have evaluated performance using aggregate metrics such as CoNLL F1 \citep{guha-etal-2015-removing,zeldes-zhang-2016-annotation}.

Performance has been found to decrease when models are evaluated on genres not included in the training dataset \citep{zhu-etal-2021-ontogum}.
%
Furthermore, \citet{moosavi-strube-2017-lexical} show that training and evaluating on OntoNotes likely leads to overfitting due to a high train/test overlap. Similarly, \citet{subramanian-roth-2019-improving} show that models trained on OntoNotes struggle to generalize to novel named entities. They present an adversarial training method to help overcome this issue and improve generalization. Additionally, existing work has improved generalization by incorporating explicit linguistic features \citep{moosavi-strube-2018-using,otmazgin2022lingmess}.

\paragraph{Pronominal Anaphora} Existing work has also examined the generalization of CR models to difficult examples of pronominal anaphora \citep{rahman-ng-2012-resolving}. For instance, models trained on OntoNotes have been shown to generalize poorly to Winograd schemas \citep{peng-etal-2015-solving,toshniwal-etal-2021-generalization} as well as other pronominal anaphora datasets \citep{webster-etal-2018-mind,emami-etal-2019-knowref}.

\section{Operationalizations of Coreference}
\label{sec:datasets}

For our analysis, we select three commonly used CR datasets which operationalize coreference in varying ways: 1) OntoNotes: a canonical dataset for training and evaluating CR models \citep{hovy-etal-2006-ontonotes}; 2) PreCo: a more recent and larger dataset of coreference \citep{chen-etal-2018-preco}; and 3) Winogrande: a crowd-sourced dataset of difficult anaphora \citep{winogrande_2020}. See Table~\ref{tab:dataset-stats} for dataset sizes and Appendix~\ref{sec:dataset-details} for more detailed descriptions of the corpora used in each dataset. OntoNotes and PreCo are annotated at a document level. Winogrande examples are of at most three sentences.

\paragraph{Construct and Operationalization}

\textit{Coreference}---which describes the phenomena of two or more textual expressions referring to the same entity---cannot always be directly identified or measured. To distinguish between the theoretical concept of coreference and the practical realization in a given dataset-based evaluation, we adopt the vocabulary of \textit{construct validity} from the social sciences \citep{adcock_collier_2001} which has recently been applied in the context of natural language datasets \citep{blodgett-etal-2021-stereotyping}.
%
Specifically, we describe the theoretical concept being measured by a given dataset as the \textit{construct} and all factors related to how the construct is realized in practice as the \textit{operationalization} of the construct \citep{jacobs2021measurement}.

\begin{table}[t]
    \centering
    \small
    \begin{tabular}{ccc}
        Dataset & Num. Examples & Num. Clusters \\
        \midrule
        OntoNotes & 3,493 & 195,747 \\
        PreCo & 37,120 & 1,959,936 \\
        Winogrande & 43,432 & 43,432 \\
        \bottomrule
    \end{tabular}
    \caption{Statistics summarizing the size of each dataset. ``Num. clusters'' refers to the number of clusters of two or more mentions that corefer to the same entity.} 
    \label{tab:dataset-stats}
\end{table}

\begin{table*}[ht]
    \centering
    \small
    \begin{tabular}{l@{\hskip 2em} p{0.27\linewidth}@{\hskip 2em} p{0.27\linewidth}@{\hskip 2em} p{0.15\linewidth}}
         & OntoNotes & PreCo & Winogrande \\
        \midrule
        Nesting & When two nested mentions share a head, the dominant mention is annotated. Appositives are not nested. & Appositives and spans with shared heads are annotated as nesting. & None. \\
        \midrule
        Generic Mentions & Generics mentions are only annotated when they co-refer with a pronoun or determinate noun phrase, or when they occur in a news headline. & All generic noun phrases are annotated. & Antecedents may be generic mentions. \\
        \midrule
        Compound Modifiers & Annotated if not generic and not an acronym. & Nominal entities and countries or job titles that function as modifiers are annotated as coreferring. & None. \\
        \midrule
        Copula Predicates & Copular structures are not annotated as coreferring. & The referent and attribute are annotated as coreferring in a copular structure. & None. \\
        \bottomrule
    \end{tabular}
    \caption{Differences in how different types of coreference are annotated in each dataset according to the dataset annotation guidelines.}
    \label{tab:general-cr-differences}
\end{table*}

\subsection{OntoNotes}
\label{ssec:onto-notes}

We use the English-language 2012 CoNLL v4 shared task splits of OntoNotes 5.0 \citep{pradhan-etal-2012-conll}. According to the OntoNotes annotation guidelines \citep{bbn2007guidelines}, ``[t]he purpose of this OntoNotes task is to co-reference, or `link,' all the specific mentions in a text that point (`refer') to the same entities and events, and to distinguish between types of coreference as needed to improve accuracy and scope. Texts annotated in this way will help the computer learn to correctly identify multiple mentions of the same entity.''

OntoNotes consists of diverse corpora (seven genres) annotated for coreference by two expert annotators and an adjudicator based on a 30-page document of annotation guidelines that includes instructions for the coreferences to be annotated alongside 95 examples \citep{hovy-etal-2006-ontonotes}.

\subsection{PreCo}
\label{ssec:preco}

PreCo is a large-scale dataset of annotated coreferences intended to have a higher coverage of lexical items than previous datasets. This high coverage was achieved by annotating a relatively large corpus of 12.5M words. Crowdworkers annotated the dataset based on annotation guidelines that are a variant of those used for OntoNotes. The corpora annotated consist of the RACE dataset \citep{lai-etal-2017-race} as well as other English comprehension exams scraped from the web.

``The dataset is designed to embody the core challenges in coreference, such as entity representation, by alleviating the challenge of low overlap between training and test sets and enabling separated analysis of mention detection and mention clustering'' \cite{chen-etal-2018-preco}.

Singleton mentions, which are mentions that do not corefer to any other mentions in the document, are also annotated in PreCo. We do not include any singleton mentions in our analysis since they are not coreferring; however, models trained on PreCo do incorporate singleton mentions in their training objective following from the original PreCo baselines.


\subsection{Winogrande}
\label{ssec:winogrande}

Winogrande consists of short, 15 to 30 words examples inspired by the difficult anaphora resolution of the Winograd Schema Challenge \citep{levesque_winograd_2012}. These examples were written by crowdworkers who were given some seed topic randomly sampled from WikiHow, a website of instructive articles. While the dataset was motivated as pronominal anaphora, the final format of the dataset is a fill-in-the-blank, cloze style where an underscore ambiguously refers to one of two possible antecedents; we recast these examples as anaphora resolution as described in Appendix~\ref{sec:converting-winogrande}.

The purpose of the Winogrande dataset is to assess ``the true capabilities of machine commonsense'' using ``problems that are inspired
by the original design of WSC, but modified to improve both
the scale and hardness'' \citep{winogrande_2020}. The test set is adversarially filtered to not include certain examples that can be resolved using surface-level patterns.


\section{Types of Coreference}
\label{ssec:types-of-coreference}

To investigate how well CR models handle different operationalizations of coreference, our analysis focuses on several types of coreference that vary across the annotation guidelines of each dataset. We derive these types from the original dataset publications as well as differences noted in the broader literature.

\citet{zeldes-zhang-2016-annotation} note that certain cases of generic mentions, compound modifiers, predication, nesting, cataphora, and metonymy are not annotated in OntoNotes.
%
\citet{can_we_fix} similarly notes cases of generic mentions, compound modifiers, and predicatives are not annotated in OntoNotes, and adds that temporal scope is also not considered in OntoNotes annotation.

As noted in its release, PreCo's annotation guidelines differ from OntoNotes in how they handle verbs, non-proper modifiers, copular structures, appositives, and acronyms \citep{chen-etal-2018-preco}.

We overview these differences in Table~\ref{tab:general-cr-differences}, and the working definitions for the types of coreference we consider in our analysis are described below. In some cases we identify these coreference types using universal dependency tags \citep{nivre-etal-2020-universal}. For OntoNotes, we determine these tags by converting the dataset's  ground-truth constituency parses into a universal dependency parse using the Stanford CoreNLP library \citep{manning-etal-2014-stanford}. For PreCo and Winogrande, we use the Stanza parser \citep{qi2020stanza} to dependency parse the dataset.

\begin{enumerate}[wide, labelindent=0pt]
    \item \textbf{Exact match}: {\em Cases where two coreferring mentions consist of exactly the same sub-string.}
    \begin{quote}
        \small
        [Mr. Richardson] said, \ \ldots \ management bought out the passive investors' holding, [Mr. Richardson] said.
    \end{quote}
    
    \item \textbf{Nested Mentions}: {\em Cases where one coreferring mention is a subspan of another.} 
    These examples vary in how they are annotated in OntoNotes versus PreCo: while nested mentions are annotated in both, in OntoNotes mentions with a shared syntactic head should not be nested according to the annotation guidelines. PreCo also annotates appositives as nested references, while OntoNotes does not.
    \begin{quote}
        \small
        President Chen said, [he [himself]] has not returned to his hometown \ \ldots
    \end{quote}
    
    \item \textbf{Compound Modifiers}: {\em Cases where a coreferring mention is a modifier.} We determine these cases using the \texttt{nn:compound} universal dependencies tag.
        \begin{quote}
        \small
        \ldots \ we miss our [Taiwan] compatriots even more, and \ldots \ gave a speech, expressing hopes that [Taiwan] authorities would \ \ldots
    \end{quote}
    
    \item \textbf{Generics}: {\em Cases where a mention is a generic noun phrase.} Following the OntoNotes guidelines, we consider a mention to be generic if it has an indefinite article or is a bare plural (a plural with no article).
    \begin{quote}
        \small
        \ldots you know yeah they had [a farm] when they were first married \ldots about maybe two three years I don't know how many years they had [a dairy farm] \ldots
    \end{quote}
    
    \item \textbf{Copula Predicates}: {\em Cases where two mentions are in a copula relation.} We consider the case where one mention is in an \texttt{nsubj} relation with the other, and there is also a copula (\texttt{cop})  dependency relation in the sentence with the same head as the \texttt{nsubj} relation.
    \begin{quote}
        \small
        Yet I realize that in my father's eyes, [I] will always be [his little girl].
    \end{quote}

\end{enumerate}

\subsection{Pronominal Anaphora}
\label{sec:types-of-pronominal-anaphora}

Pronominal anaphora are cases of coreference where a pronoun refers to an antecedent mention. We follow the formalization of \citet{zhang-etal-2019-incorporating} and consider pronominal anaphora where the antecedent occurs within the preceding two sentences. We additionally consider the same set of relative pronouns following \citet{zhang-etal-2019-incorporating}. We refer to intra-sentence anaphora with a constraint that there is only one coreference cluster in the sentence and the pronoun appears after all mentions as ``local context.''

A Winograd schema consists of a sentence with a pronoun $r$ that refers exclusively to one of two possible antecedents in the sentence. We use $h$ to denote the correct antecedent and $h'$ to denote the incorrect antecedent. E.g., in the example ``\textit{Robert} woke up at 9:00am while \textit{Samuel} woke up at 6:00am, so \textit{he} had less time to get ready for school.'' $r$ is the token \textit{he}, $h$ the token \textit{Robert}, and $h'$ the token \textit{Samuel}.

Winograd schemas by design contain all of the characteristics listed below. We consider each characteristic independently to better understand to what extent models can generalize to or from that characteristic of Winograd schemas.

\begin{enumerate}[wide, labelindent=0pt]
    \item \textbf{Ambiguous Grammatical Attributes}: Winograd schemas are designed such that the pronoun cannot be resolved based on certain grammatical attributes. These attributes always agree with both of the two possible antecedents (e.g., for $r=\textit{it}$, $(h, h')$ could be $(\textit{the dog}, \textit{the cat})$ but not $(\textit{the dog}, \textit{the cat\underline{s}})$). In other datasets, however, some pronouns may be resolved solely based on grammatical attributes (e.g., ``\textit{The dog} chased \textit{the cats} and \textit{it} \ldots''). We separately consider cases of pronominal anaphora where there is only one noun phrase preceding the pronoun with consistent grammatical attributes. We define these attributes to be the morphological features tagged by the Stanza parser. These features that we use are number and person.
    \item \textbf{Antecedent Subject or Object}: Winograd schemas are balanced between antecedents that are a subject and object. We therefore consider two types of pronominal anaphora: those where the antecedent is a) the subject (e.g., ``\textit{The dog} chased \textit{the cats} and \textit{it} \ldots'') or b) object of the sentence (e.g., ``\textit{The dog} chased \textit{the cats} and \textit{they} \ldots'').
\end{enumerate}

\begin{table*}[t]
    \centering
    \small
    \begin{tabular}{lcccccc}
        \toprule
Development Split & T5\textsubscript{ON} & WLC\textsubscript{ON} & LingMess\textsubscript{ON} & Longdoc\textsubscript{ON;PC} & T5\textsubscript{PC} & T5\textsubscript{WG} \\
\midrule
OntoNotes & 79.9 & \textbf{80.7} & \textbf{80.7} & 79.3 & 60.7 & \hphantom08.2 \\
\hdashline
\rule{0pt}{10pt}\quad Exact Match & 87.9 & \textbf{89.0} & 88.0 & 87.5 & 71.5 & 12.1 \\
\quad Nested Mentions & 46.3 & \textbf{59.6} & 53.4 & 47.8 & \hphantom08.3 & \hphantom00.0 \\
\quad Generic Mentions & 37.3 & 36.7 & 40.8 & \textbf{42.4} & 13.1 & \hphantom03.3 \\
\quad Compound Modifiers & \textbf{75.6} & 75.4 & 70.7 & 72.7 & 28.1 & \hphantom00.0 \\
\quad Copula Predicates & 50.0 & 42.2 & \textbf{62.5} & 23.5 & \hphantom01.3 & \hphantom00.0 \\
\hdashline
\rule{0pt}{10pt}\quad Pronominal & \textbf{84.8} & 83.2 & 84.7 & 84.5 & 70.9 & 19.8 \\
\quad\quad Local Context & \textbf{73.9} & 71.1 & 70.1 & 72.1 & 68.1 & 63.5 \\
\quad\quad\quad Grammatical Attributes & 84.7 & 81.8 & \textbf{85.0} & 83.8 & 71.3 & 65.1 \\
\quad\quad\quad Subj. Antecedent & \textbf{71.1 }& 69.9 & 66.0 & 71.0 & 66.3 & 70.4 \\
\quad\quad\quad Obj. Antecedent  & \textbf{75.7} & 73.5 & 72.2 & 73.4 & 68.6 & 60.0 \\
\midrule
PreCo & 64.6 & 63.7 & 64.2 & 84.6 & \textbf{85.1} & \hphantom08.8 \\
\hdashline
\rule{0pt}{10pt}\quad Exact Match & 71.3 & 70.4 & 70.5 & 93.0 & \textbf{93.3} & 10.8 \\
\quad Nested Mentions & 11.9 & 12.1 & 11.3 & 70.8 & \textbf{73.3} & \hphantom00.4 \\
\quad Generic Mentions & 12.3 & 13.0 & 11.3 & 75.6 & \textbf{76.7} & \hphantom05.8 \\
\quad Compound Modifiers & \hphantom05.3 & \hphantom05.7 & \hphantom06.1 & 81.2 & \textbf{81.6} & \hphantom00.0 \\
\quad Copula Predicates & \hphantom00.6 & \hphantom00.7 & \hphantom00.8 & 74.5 & \textbf{75.9} & \hphantom00.3 \\
\midrule
Winogrande & 44.1 & 44.0 & 42.5 & 41.9 & 42.0 & \textbf{74.4} \\
\bottomrule
% \CodeAfter
% \tikz \draw [dashed, shorten < = 4pt, shorten > = 4pt] (3-|1) -- (3-|8) ;
% \tikz \draw [dashed, shorten < = 4pt, shorten > = 4pt] (8-|1) -- (8-|8) ;
% \tikz \draw [dashed, shorten < = 4pt, shorten > = 4pt] (14-|1) -- (14-|8) ;
    \end{tabular}
    \caption{Performance on the OntoNotes, PreCo, and Winogrande development sets for each type of coreference (CoNLL F1 score). Subscripts denote the model's training set: OntoNotes (ON), Winogrande (WG), or PreCo (PC).}
    \label{tab:results}
\end{table*}

\section{Models}
\label{sec:models}

For our experiments, we implement a T5-based model that we fine-tune on each coreference dataset in a uniform way. This allows us to control for the training dataset by ensuring other factors related to training remain the same.
%
In addition, we consider three other recent state-of-the-art models from the literature using the publicly released weights.

\paragraph{T5 for Coreference}
We use the framework of \citet{toshniwal-etal-2021-generalization} but replace the BERT encoder with T5-large.
%
For Winogrande, at test time, we use greedy decoding to calculate a model output and consider a candidate to be the correct antecedent if the model output is a subset of this candidate, or vice versa. This follows the method of the original T5 model for Winograd schemas \citet{2020t5}.

We use the Adam optimizer with a learning rate of 1e-4 and epsilon 1e-8 train for up to 100,000 steps.

\paragraph{Word-level Coreference (WLC)}
Presented by \citet{dobrovolskii-2021-word}, the word-level coreference model is a popular mention ranking model that uses a pre-trained RoBERTa-large encoder and ranks mentions at the granularity of words as opposed to spans of words. Spans are then reconstructed after words have been resolved as coreferring. WLC also uses speaker and genre information on OntoNotes.

\paragraph{LingMess} \citet{otmazgin2022lingmess} combine a state-of-the-art, span-based, mention ranking model with explicit linguistic features in order to better generalize between datasets. We use the Longformer-large enocder model. For LingMess, we do not use speaker or genre information.

\paragraph{Longdoc} A state-of-the-art entity-ranking model \citep{toshniwal-etal-2021-generalization}. We use the publicly released Longformer-large encoder model that has been jointly trained on OntoNotes and PreCo. This model uses speaker but not genre information on OntoNotes.

\begin{figure*}%
    \centering
    \subfloat[\centering OntoNotes]{{\includegraphics[width=5cm]{figs/mcc_ontonotes.pdf} }}%
    \subfloat[\centering PreCo]{{\includegraphics[width=5cm]{figs/mcc_preco.pdf}}}
    \subfloat[\centering Winogrande]{{\includegraphics[width=5cm]{figs/mcc_winogrande.pdf}}}%
    \subfloat{\raisebox{6ex}{\includegraphics[height=4cm]{figs/cbar.pdf}}}%
    \caption{Correlation (MCC) of model resolution accuracy on the corresponding development sets. Subscripts denote the model's training set: OntoNotes (ON), Winogrande (WG), or PreCo (PC).}%
    \label{fig:iom-results}%
\end{figure*}

\section{Results}
\label{sec:results}

We evaluate coreference resolution accuracy using the CoNLL F1 metric. To evaluate a given model on a given type of coreference, we compute CoNLL F1 for the subset of the model predictions and ground-truth annotations that are of said type.

For the LingMess, WLC, and Longdoc models, we evaluate the public model weights. For the T5-based model, we compare the performance of the model fine-tuned on each dataset. We refer to models as in-domain when they are trained and tested on the same training dataset, and out-of-domain when they are evaluated on a test set from a dataset other than that on which they were trained.

\subsection{OntoNotes}
\label{ssec:ontonotes-results}

In Table~\ref{tab:results}, we present the results on the OntoNotes development set. Out-of-domain models have a significantly lower performance on nested mentions, generic mentions, compound modifiers, and copula predicates as compared to the in-domain models. These are all types of coreference where the annotation guidelines differ between datasets.
%
F1 on the full development set, exact match, and pronominal anaphora is also lower for the out-of-domain models; however, do not observe the same degree of performance difference for the T5 model trained on PreCo in these cases.

Interestingly, the Longdoc model trained on both OntoNotes and PreCo has an F1 competitive with other in-domain models on the full development set, yet still has significantly lower performance on the copula predicates coreference type.

The T5 model trained on Winogrande is able to generalize to the local-context pronominal anaphora in OntoNotes, approximately matching the performance of those models trained in-domain.

\subsection{PreCo and Winogrande}
\label{ssec:preco-and-winogrande-results}

The performance on the PreCo and Winogrande datasets is also presented in Table~\ref{tab:results}. In each case, the model trained in-domain outperforms all other models. For all but exatch-match and full-dataset evaluations, we see a drastic  difference in performance between in-domain and out-of-domain models.

In the case of the PreCo dataset, there is a significant gap between the in-domain and out-of-domain models in all cases except for the full development set and exact match coreferences.

For the Winogrande dataset, the in-domain model is again more accurate than all out-of-domain models.

\subsection{Correlation of Predictions}

Our original proposal was that the operationalization of the CR task in a models' training dataset is associated with the model's errors. If this is the case, then we might expect all models trained on the same training set to be correlated in the errors that they make.

We specifically consider the correlation of models' resolution accuracy \citep{lu-ng-2020-conundrums}. Resolution accuracy is defined as: for every pair of annotated co-referring mentions, does the model correctly predict that these two mentions are coreferring. We quantify the correlation of model output using Matthew's Correlation Coefficient (MCC) in terms of model resolution accuracy on each annotated mention pair. This intuitively describes the correlation between two models predicting a coreference correctly.

The pair-wise correlations of the resolution accuracy are presented in Figure~\ref{fig:iom-results}. Models trained in-domain on the same dataset are more correlated in their correct predictions than those trained on different datasets. The T5 model trained on Winogrande is almost uncorrelated with all other model predictions, even on the Winogrande dataset.

These correlation results reinforce the idea that there is a relationship between some factors of a training dataset and the errors of models trained on that dataset.

\begin{table*}[th]
    \small
    \centering
    \begin{tabular}{l  p{0.75\linewidth}}
        Type & Example \\
        \midrule
         Exact Match & [\textsubscript{(T5,0)} Mr. Lutsenko ]\textsubscript{(T5,0)} told [\textsubscript{(GOLD,0)} Nedelya ]\textsubscript{(GOLD,0)} that he recently had been to the U.S. to pick up the rights to show 5,000 U.S. films in the Soviet Union . [\textsubscript{(GOLD,0)} Nedelya ]\textsubscript{(GOLD,0)} 's article was accompanied by a picture of [\textsubscript{(T5,0)} Mr. Lutsenko ]\textsubscript{(T5,0)} interviewing singer John Denver in Colorado . \\
         \midrule
         Nested Mention & [\textsubscript{(T5,0)} Even [\textsubscript{(GOLD,0)} Kuanyin , worshipped for [\textsubscript{(T5,0)} [\textsubscript{(GOLD,0)} her ]\textsubscript{(GOLD,0)} ]\textsubscript{(T5,0)} limitless compassion and mercy ]\textsubscript{(T5,0)} ]\textsubscript{(GOLD,0)} , must be creasing her brow at this rampant overuse of the mountain . \\
         \midrule
         Generic Mention & According to what has been reported in the Soviet " Science and Life " magazine , biologists in Moscow have discovered that [\textsubscript{(GOLD,0)} a mold element in the cells of plants]\textsubscript{(GOLD,0)}  can not only control the propagation of plasmodium in mosquitoes , but can also kill mosquitoes . This method will not pollute the environment . Biologists mixed [\textsubscript{(GOLD,0)} a mold element in the cells of [\textsubscript{(T5,0)} plants]\textsubscript{(GOLD,0)} ]\textsubscript{(T5,0)}  with pearl powder to produce a granulated drug . This kind of drug can slowly discharge the mold element from the cells of [\textsubscript{(T5,0)} plants]\textsubscript{(T5,0)}  in water . \\
         \midrule
         Compound Modifier & But the [\textsubscript{(T5,0)} [\textsubscript{(GOLD,0)} Peugeot]\textsubscript{(GOLD,0)} ]\textsubscript{(T5,0)}  breakthrough came as a nationwide dispute by [\textsubscript{(GOLD,1)} Finance Ministry]\textsubscript{(GOLD,1)}  employees disrupted border checkpoints and threatened the government 's ability to pay its bills . The [\textsubscript{(T5,0)} [\textsubscript{(GOLD,0)} Peugeot]\textsubscript{(GOLD,0)} ]\textsubscript{(T5,0)}  metalworkers began filing out of the shop , which makes auto parts , at the plant in Mulhouse after voting 589 to 193 to abandon the occupation . \ldots [\textsubscript{(GOLD,1)} Ministry]\textsubscript{(GOLD,1)}  employees complain that they are poorly paid because of a complex job - rating system they say fails to take into account their education and level of technical expertise . \\
         \midrule
         Copula Predicates & The whole process , he said , is a bit like being nibbled to death by a duck . For The World , [\textsubscript{(T5,0)} this]\textsubscript{(T5,0)}  is [\textsubscript{(T5,0)} Clark Boyd]\textsubscript{(T5,0)} . \\
         \midrule
         Pronominal Anaphora & El Salvador 's government opened a new round of talks with the country 's leftist rebels in an effort to end a decade - long civil war . A spokesman said the guerrillas would present a cease - fire proposal during the negotiations in Costa Rica that includes constitutional and economic changes . The State Department said there was a `` possibility '' that [\textsubscript{(T5,0)} [\textsubscript{(GOLD,0)} some Nicaraguan rebels]\textsubscript{({'GOLD'}, {0})}]\textsubscript{({'T5'}, {0})} were [\textsubscript{(T5,1)} selling]\textsubscript{({'T5'}, {1})} [\textsubscript{(T5,0)} [\textsubscript{(GOLD,0)} their]\textsubscript{({'GOLD'}, {0})}]\textsubscript{({'T5'}, {0})} U.S. - supplied arms to Salvadoran guerrillas , but insisted [\textsubscript{(T5,1)} it]\textsubscript{({'T5'}, {1})} was n't an organized effort . \\
        \bottomrule
    \end{tabular}
    \caption{Examples from the OntoNotes dataset where the T5 model trained on OntoNotes (T5\textsubscript{ON}) predictions do not match the ground-truth annotations. The subscript for each bracket is a tuple which indicates the model name and entity cluster, respectively.}
    \label{tab:error-examples}
\end{table*}


\section{Conclusions} % and Discussion}
\label{sec:discussion}

Our experimental results suggest a strong relationship between: 1) a CR model's accuracy and 2) factors relating to how coreference is operationalized in the dataset used to train the model. In this work, we focused on factors surrounding the choice of training data and annotation guidelines. Future work could explore additional factors regarding: 1) the data processing, such as what tokenization was used; 2) factors regarding the CR task format, such as cross-document coreference; 3) and factors regarding the annotation process, such as noise and ambiguity in annotation. Ultimately, a more holistic understanding of the way that coreference is operationalized can provide better insights into model accuracy and guide future modeling decisions.

We provide qualitative examples of model failures for the T5 model trained on OntoNotes in Table~\ref{tab:error-examples}. We observe that model failures are sometimes intuitive which agrees with the observations of \citet{can_we_fix}. In the case of the generic mentions, for example, one might expect the two mentions of ``plants'' predicted as coreferring by the model to indeed be annotated coreferring (based on an intuitive understanding of coreference).

% Note: OntoNotes -> PreCo or PreCo -> OntoNotes has explicit negatives which do not exist in WG -> * (I.e., it's not that the types of coreference aren't annotated in WG, just that they don't appear at all.)

There are several interesting directions for future work. This analysis required manually defining the types coreferences, whereas some coreference types might be discoverable.

%\section{Conclusion}
%\label{sec:conclusion}

In this work, we examine on which types of coreference CR models are more likely to fail when evaluated on datasets beyond their training regimen. We found that performance degrades on types of coreference that differ in how they are operationalized in each dataset.
%
Interestingly, nested coreferences and generic mentions are relatively difficult for all models. This could be an area to focus on to improve coreference models.

\paragraph{Limitations}
\label{sec:limitations}
Our empirical study only establishes correlation between model performance and the types of CR on which model performance varies when generalizing to multiple datasets. %In addition, we did not perform statistical significance testing for the results.
%
We considered three types of coreference for two document-level and one sentence-level datasets. This limitation could be overcome by expanding the study to datasets with annotation schemes such as ARRAU \citep{poesio-etal-2018-anaphora} which are annotated for additional types of coreference such as bridging anaphora or split antecedents.

% Limitation about biases in models? \citet{webster-etal-2018-mind}

% \section*{Acknowledgements}

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}

\appendix

\begin{table*}[ht]
    \centering
    \small
    \begin{tabular}{l p{0.275\linewidth} p{0.275\linewidth} p{0.275\linewidth}}
        \toprule
         & OntoNotes & PreCo & Winogrande \\
        \midrule
        Corpora & News; Broadcast and Phone Conversations; Web Data; Biblical Text
        & 2/3 composed of the RACE dataset \citep{lai-etal-2017-race} and 1/3 of English tests scraped from the web. & Artificial examples written by crowdworkers who were primed with a randomly chose topic. \\
        \midrule
        Annotation & Annotated by two experts and adjudicated by an expert following a 30-page document of annotation guidelines. & Three annotators (trained English-speaking Chinese crowdworkers) annotate each document. Four additional annotators merge these annotations. & Three annotators agree on the resolution, that it is unambiguous, and that it cannot use simple word associations. \\
        % Nested Coreferences & & \\
        % Compound Modifiers  & & & \\
        % Generic Mentions & & & \\
        \bottomrule
    \end{tabular}
    \caption{A summary of the textual data for each dataset. OntoNotes and PreCo are tokenized using Penn Treebank tokenization.}
    \label{tab:dataset-corpora}
\end{table*}

\section{Recasting Winogrande}
\label{sec:converting-winogrande}

While inspired by Winograd schemas, the final format of Winogrande is as a fill-in-the-blank, cloze task. E.g., ``Robert woke up at 9:00am while Samuel woke up at 6:00am, so he had less time to get ready for school.'' is formatted as ``\ldots  so \underline{\hspace{1.66em}} had less time \ldots'' where the task is to predict the correct noun phrase that occurs in place of the underscore (in this example, either Robert or Samuel).

We re-format Winogrande as pronominal anaphora by first parsing each example using the Stanza parser \citep{qi2020stanza}. We then replace the underscore with the corresponding English third-person pronoun based on agreement with the morphological features of both candidates. These morphological features are determined from the universal dependency parse. In the case of a personal pronoun, we further select the grammatical gender (\textit{he} or \textit{she}) based on corpus statistics following the method of \citet{emami-etal-2019-knowref}.

To resolve each of the two answer candidates to a mention in the example text, we first check if there is a single exact match for the candidate. If so, we take that to be the mention. If not, we take the constituent noun phrase of the longest-common substring between the candidate and the original sentence.

Certain examples cannot be directly recast as pronominal anaphora. For the example, ``As the blue ink stain spread across her green top, she wished it had been a \underline{\hspace{1.66em}} pen that broke so the stain would have blended in with her blouse." with candidates ``Blue'' or ``Green,'' there is no obvious pronoun. Similarly, for ``The accountant was late for his appointment but couldn't decide whether to walk or take an Uber. He thought his destination was too close to take an \underline{\hspace{1.66em}} .'' with candidates ``Uber'' or ``walk.'' We discard these examples which are detected based on the dependency parse: either because the candidates are not a noun phrase or cannot be resolved to a noun phrase in the original sentence. The original dataset train / development / test sizes are 40,398 / 1,267 / 1,767. After conversion, our pronominal anaphora recast of Winogrande consists of 39,654 / 1,241 / 1,715 examples. For training, we always use the entire ``XL'' training set.

\section{Dataset Details}
\label{sec:dataset-details}

A summary of the corpus and annotation details of each corpus is given in Table~\ref{tab:dataset-corpora}.

\end{document}
