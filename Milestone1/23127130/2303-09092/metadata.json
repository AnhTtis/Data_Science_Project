{
    "arxiv_id": "2303.09092",
    "paper_title": "Investigating Failures to Generalize for Coreference Resolution Models",
    "authors": [
        "Ian Porada",
        "Alexandra Olteanu",
        "Kaheer Suleman",
        "Adam Trischler",
        "Jackie Chi Kit Cheung"
    ],
    "submission_date": "2023-03-16",
    "revised_dates": [
        "2023-03-17"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CL"
    ],
    "abstract": "Coreference resolution models are often evaluated on multiple datasets. Datasets vary, however, in how coreference is realized -- i.e., how the theoretical concept of coreference is operationalized in the dataset -- due to factors such as the choice of corpora and annotation guidelines. We investigate the extent to which errors of current coreference resolution models are associated with existing differences in operationalization across datasets (OntoNotes, PreCo, and Winogrande). Specifically, we distinguish between and break down model performance into categories corresponding to several types of coreference, including coreferring generic mentions, compound modifiers, and copula predicates, among others. This break down helps us investigate how state-of-the-art models might vary in their ability to generalize across different coreference types. In our experiments, for example, models trained on OntoNotes perform poorly on generic mentions and copula predicates in PreCo. Our findings help calibrate expectations of current coreference resolution models; and, future work can explicitly account for those types of coreference that are empirically associated with poor generalization when developing models.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.09092v1"
    ],
    "publication_venue": null
}