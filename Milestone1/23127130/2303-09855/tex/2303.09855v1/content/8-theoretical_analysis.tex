\section{Theoretical Analysis}
\label{section:theory}

%study the accuracy and efficiency of our randomized algorithms from theoretical perspective. Specifically, we analyze the failure probability of a query and expected evaluated dimensionality of a non-KNN object. 

% \subsection{Failure Probability}
% Let's continue with our analysis in Section 3.2. ARSearch algorithm fails only when some positive objects of a search are wrongly rejected at some level $l$. Let $\mathcal O_+$ be the set of positive objects and $r $ be the radius of distance comparison of positive object $i$. We have
% \begin{align}
%     \mathbb{P} \left\{ fail \right\}  = \mathbb{P} \left\{ \exists l \le L, \exists i \in \mathcal O_{+}, dis ' > r  \cdot \gamma (d_l) \right\}  
% \end{align}
% Note that $r  > dis $ because object $i$ is positive.
% \begin{align}
%     &\le \mathbb{P} \left\{ \exists l \le L, \exists i\in \mathcal O_{+}, dis ' > dis  \cdot \gamma(d_l) \right\}  
% \end{align}
% We then plugin the definition of $dis , dis '$ and $\gamma(d_l)$:
% \begin{align}
%     =\mathbb{P} \left\{ \exists l \le L, \exists i \in \mathcal O_{+},  \sqrt {\frac{D}{d_l} } \left\|\mathbf{y} |_{[1,2,...,d_l]} \right\|
%     %\frac{\left\| \mathbf{y} |_{[1,2,...,d_l]} \right\|}{\gamma(d_l)}   
%     > \left( 1 + \frac{\epsilon _1}{\sqrt {d_l} }  \right) \| \mathbf{y}  \|  \right\} \label{eq:plugin} 
% \end{align}
% Applying union bound, i.e. the probability of the union of events is no greater than the sum of their individual probability, we have:
% \begin{align}
%     \le \sum_{l=1}^{L} \sum_{i\in \mathcal O_+} \mathbb{P} \left\{ \sqrt {\frac{D}{d_l} } \left\|\mathbf{y} |_{[1,2,...,d_l]} \right\|
%     > \left( 1 + \frac{\epsilon _1}{\sqrt {d_l} }  \right)  \| \mathbf{y}  \|  \right\} 
% \end{align}
% With the equivalence between random projection and row sampling with random transformation and the distance-preserving property of random orthogonal transformation, we have:
% \begin{align}
%     &= \sum_{l=1}^{L} \sum_{i\in \mathcal O_+} \mathbb{P} \left\{ \sqrt {\frac{D}{d_{l}} } \left\| P'|_{[1,2,...,d_l]}\mathbf{x}  \right\|   > \left( 1 + \frac{\epsilon _1}{\sqrt {d_l} }  \right)\| \mathbf{x}  \| \right\} 
% \end{align}
% Note that $P'|_{[1,2,...,d_l]}$ is a $d_l \times D$ random projection matrix. Applying Lemma~\ref{eq:concen}, we finally have
% \begin{align}
%     \mathbb{P} \left\{ fail \right\}  &\le \sum_{l=1}^{L} \sum_{i\in \mathcal O_+} \exp(-c_0 \epsilon_1^2) = L \cdot N_{pos} \cdot \exp(-c_0 \epsilon_1^2)  \\
%     &\le D \cdot N_{pos} \cdot \exp (-c_0 \epsilon _{1}^{2})  \label{eq:superexp}
%     %\\ &= \exp (-c_0 \epsilon_1^2 + \log |\mathcal O_+| + \log L)  
% \end{align}
% Letting the failure probability to be no greater than $\delta$ to solve $\epsilon_1$, then we finally have Theorem~\ref{theorem:eps}.
% In terms of Corollary~\ref{corollary:eps}, it simply shifts our concern from all positive objects to KNN objects only. %Note that the corollary is not at all related with the cardinality of a database $N$. %As a result, as a database scales up, there's no need to tune the parameters of our adaptive algorithms. 
% %when $L$ and $|\mathcal{O}_+|$ are not changed.

% \begin{comment}

% For a fixed query, suppose that $N_{vis}$ vectors are visited. For object $i$, our algorithm depends on the fact that $dis^*  \le dis $ with high probability. A failure happens if there exists an object $i$ fails at a level $l$. Thus, the overall failure probability is given as:
% \begin{align}
%     \mathbb{P}\left\{ \exists i \le N_{vis}, l < L, dis ^* > dis  \right\}
% \end{align}
% We plug-in the definiton of $dis $ and $dis^* $: 
% \begin{align}
%     =\mathbb{P} \left\{ \exists i\le N_{vis}, l < L, \sqrt {\frac{D}{d_l} } \frac{\left\| \mathbf{y} |_{[1,2,...,d_l]} \right\|}{\gamma(d_l)}   > \| \mathbf{y}  \|  \right\} \label{eq:plugin}  
% \end{align}
% Applying union bound, i.e. the probability of the union of events is no greater than the sum of their individual probability, we have:
% \begin{align}
%     \le \sum_{l=1}^{L-1} \sum_{i=1}^{N_{vis}} \mathbb{P} \left\{ \sqrt {\frac{D}{d_l} } \frac{\left\| \mathbf{y} |_{[1,2,...,d_l]} \right\|}{\gamma(d_l)}   > \| \mathbf{y}  \|  \right\}  
% \end{align}
% With the equivalence between random projection and row sampling with random transformation and preset constant significance, we finally obtain: 
% \begin{align}
%     &= \sum_{l=1}^{L-1} \sum_{i=1}^{N_{vis}} \mathbb{P} \left\{ \sqrt {\frac{D}{d_{l}} } \left\| P\mathbf{x}  \right\|   > (1 + \frac{\epsilon _1}{\sqrt {d_l} } )\| \mathbf{x}  \| \right\}  
%     \\ &\le \sum_{l=1}^{L-1} \sum_{i=1}^{N_{vis}} \exp(-c_0 \epsilon_1^2) < L \cdot N_{vis} \cdot \exp(-c_0 \epsilon_1^2)  
%     \\ &= \exp (-c_0 \epsilon_1^2 + \log N_{vis} + \log L)  
% \end{align}
% Letting the failure probability be no greater than $\delta$, then $\epsilon_1$ should be $\Theta \left( \sqrt {\log \frac{L\cdot N_{vis}}{\delta}}  \right)$.
% \end{comment}

%{\color{red} To avoid distration, I commented out the discussion about the tightness of the theoretical results.}
%{\color{red} There is a gap between theory and practice. Note that the theory only provides the correct order of $\epsilon_1$. The gap is due to 1) The tail bound of random projection is not tight in constant (but tight in order). 2) Union bound guarantees the failure probability for the worst case among all the datasets, so it highly overestimates the failure probability for real world datasets which have some good properties (they are embeddings produced by models, so they cannot be some arbitrary things.). 3) Failures not necessarily affect the correctness of result. For example, failures happened at negative objects (Non-KNN objects) even benefit the algorithm for it stops unnecessary sampling earlier. //I think it might need discussion.} 


%{\color{red} Note that improving the number of testing number and also overhead from extra priority queue operations. We should be careful about the settings of layers.}

%\subsection{Expected Terminate Dimensionality}
%We next investigate the expected dimensionality of negative objects. For a negative object $i$, supposing that it's compared with radius $r $, let $(1+\alpha )$ be the ratio between $dis $ and $r $ (though we don't know $dis $ and $\alpha $ in prior). Let random variable $\hat D $ be the terminate dimensionality of object $i$. We assume that we do hypothesis testing every time after sampling a new dimension. 
In this section, we prove Lemma~\ref{theorem:ADSampling efficiency} in detail. 
% For a negative object, let $\alpha = (dis - r) / r$ be the gap between $dis$ and $r$. Let random variable $\hat D$ be the terminate dimension (corresponds to time complexity). 
We assume that {\CHENGB we sample one additional dimension of $\mathbf{y}$ each time.}
% we do hypothesis testing each time after sampling one dimension. 
Let $\gamma(d) = (1 + \epsilon_0 / \sqrt {d} )$. 
% \begin{proof}
% Note that $\hat D$ is a non-negative random variable. 
We have
\begingroup
\allowdisplaybreaks
\begin{align}
    \mathbb{E} \left[ \hat D \right]  
    % =& \sum_{d=1}^{+\infty} \mathbb{P} \left\{ \hat D \ge d \right\}
    =& \sum_{d=1}^{D} d\cdot \mathbb{P} \left\{ \hat D = d \right\}
    = \sum_{d=1}^{D} \mathbb{P} \left\{ \hat D \ge d \right\}
% \end{align}
% {\CHENGB Here,} $\hat D \ge d$ {\CHENGB represents the event that} all the previous hypothesis testing cannot reject the hypothesis, i.e.,
% \begin{align}
    \\=& \sum_{d=1}^{D} \mathbb{P} \left\{  \forall p<d,  \sqrt {\frac{D}{p}}  \| \mathbf{y}|_{1,2,...,p}\| \le \gamma(p) \cdot r  \right\}      \label{reduction:interpret}
    \\=& \sum_{d=1}^{D} \mathbb{P} \left\{  \forall p<d, \sqrt {\frac{D}{p}} \| \mathbf{y}|_{1,2,...,p}\| \le \gamma(p) \cdot \frac{\| \mathbf{y}\| }{1+\alpha} \right\} 
% \end{align}
% We relax the condition of all previous hypothesis testing to the last testing:
% %because the filter-out probability decays exponentially. The last testing dominates the filter-out probability. 
% \begin{align}
    \\\le& 1 + \sum_{d=1}^{D-1} \mathbb{P} \left\{ \sqrt {\frac{D}{d}} \| \mathbf{y}|_{1,2,...,d}\| \le \gamma(d) \cdot \frac{\| \mathbf{y}\| }{1+\alpha}  \right\}    \label{reduction: relax hypothesis testing}  
    % \\\le&  1 + d_0   + \sum_{d=d_0 +1 }^{D} \mathbb{P} \left\{ \sqrt {\frac{D}{d}} \| \mathbf{y}|_{1,2,...,d}\| \le \frac{\gamma (d)}{1+\alpha } \| \mathbf{y} \| \right\}  \label{reduction:separate analyze}
\end{align}
\endgroup
where (\ref{reduction:interpret}) is because $\hat D \ge d$ {\CHENGB represents the event that} all the previous hypothesis testings cannot reject the hypothesis and (\ref{reduction: relax hypothesis testing}) relaxes the event corresponding to all the testings (i.e., $\forall p < d$) to that corresponding to the last testing (i.e., $p=d-1$). 
% Let $\tilde d= \epsilon _{0}^{2} /\alpha^2$ and $d_0 = \mathrm{ceil} (\tilde d)$. (\ref{reduction:separate analyze}) relaxes the probability for $d \le d_0$ to 1. 

We denote $\tilde d:= \epsilon _{0}^{2} / \alpha^2, d_0 := \mathrm{ceil} (\tilde d) $ and relax the probability for $d \le d_0$ to $1$:
\begin{align}
    \mathbb{E} \left[ \hat D \right]  \le  1 + d_0   + \sum_{d=d_0 +1 }^{D} \mathbb{P} \left\{ \sqrt {\frac{D}{d}} \| \mathbf{y}|_{1,2,...,d}\| \le \frac{\gamma (d)}{1+\alpha } \| \mathbf{y} \| \right\}  \label{reduction:separate analyze}
\end{align}
% Then we separately analyze the cases when 1) $\gamma(d) \ge 1+\alpha$ and 2) when $\gamma(d) < 1+\alpha $ for different $d$.
% %, because for $d$ whose $\gamma(d) \ge \alpha + 1$, the bound provided by Lemma~\ref{eq:concen} is trivial. 
% We denote
% \begin{align}
%     \tilde d = \lceil \frac{1}{\alpha^2} \cdot \epsilon_0^2  \rceil 
%     %= \Theta \left( \frac{1}{\alpha ^2} \cdot \log \frac{D \cdot K}{\delta} \right)
%     %\max \left( \log \frac{D}{\delta} ,\log \frac{K}{\delta}  \right) 
% \end{align}
% Note that we have $\gamma(d) \ge \alpha + 1$ for $d \le \tilde d$ and $\gamma(d) < \alpha+1$ for $d > \tilde{d}$. 
%Without loss of generality we assume that $l_0$ is an integer (otherwise, we let $l_0$ be $\lfloor \epsilon_1^2/\alpha ^2 \rfloor $). 
% With relaxing the probability for $d \le \tilde d$ to $1$, we have
% \begin{align}
%     \mathbb{E} \left[ \hat D  \right]  \le  1 + d_0   + \sum_{d=d_0 +1 }^{D} \mathbb{P} \left\{ \sqrt {\frac{D}{d}} \| \mathbf{y}|_{1,2,...,d}\| \le \frac{\gamma (d)}{1+\alpha } \| \mathbf{y} \| \right\}  \label{reduction:separate analyze}
% \end{align}
% Note again that now for $d > \tilde{d}$, $\gamma(d) /(1+\alpha) < 1$. 
% Note that for $d>d_0$, $\gamma(d) < 1 + \alpha$. 
Let's focus on the last term of (\ref{reduction:separate analyze}) and deduce from it as follows,
\begingroup
\allowdisplaybreaks
\begin{align}
    &\sum_{d=d_0 +1 }^{D} \mathbb{P} \left\{ \sqrt {\frac{D}{d}} \| \mathbf{y}|_{1,2,...,d}\| \le \frac{\gamma (d)}{1+\alpha } \| \mathbf{y} \| \right\}   \\
    =&\sum_{d=d_0 +1}^{D} \mathbb{P} \left\{ \sqrt {\frac{D}{d}} \| \mathbf{y}|_{1,2,...,d}\| \le \left[ 1- \left( 1-\frac{\gamma(d)}{1+\alpha}  \right)  \right]   \| \mathbf{y} \| \right\}    \label{reduction:rewrite}
% \end{align}
% Then with Lemma~\ref{eq:concen}, we have 
% \begin{align}
    \\\le& \sum_{d=d_0+1}^{D} \exp \left[ -c_0 d \left( 1 - \frac{\gamma(d)}{1+\alpha}  \right)^2 \right]  
    % \qquad \text{(Lemma~\ref{eq:concen})}  
    \label{reduction:lemma3.1}
    \\=&\sum_{d=d_0+1}^{D} \exp \left[ -\frac{c_0 \alpha^2}{(1+\alpha)^2}\left( \sqrt {d} - \sqrt{\tilde{d}}  \right) ^2  \right]  \label{reduction:expand and sort out}
% \end{align}
% Note that this expression is monotonically decreasing with respect to $d$, so the summation is bounded by the integration:
% \begin{align}
    \\\le& \int_{d_0}^{D} \exp \left[ -\frac{c_0 \alpha^2}{(1+\alpha)^2}\left( \sqrt {x} - \sqrt {\tilde{d}}  \right) ^2  \right] \mathrm{d} x  \label{reduction:integration}
    \\=& \int_{d_0}^{D} \exp \left[ -\frac{c_0 \epsilon_0^2}{(1+\alpha)^2}\left( \sqrt {\frac{x}{\tilde{d}} } - 1 \right) ^2  \right] \mathrm{d} x  
% \end{align}
% Let $u = x / \tilde{d}$, then we have: 
% \begin{align}
    \\=&\tilde{d} \int_{d_0 /\tilde d}^{D/\tilde{d}} \exp \left[ - \frac{c_0 \epsilon_0^2}{(1+\alpha)^2} \left( \sqrt {u} -1 \right)^2  \right]  \mathrm{d} u  
    % \quad \text{(Let}~u= x/\tilde d) 
    \label{reduction:substitute u}
    \\\le& \tilde{d} \int_{1}^{+\infty} \exp \left[ - \frac{c_0 \epsilon_0^2}{(1+\alpha)^2} \left( \sqrt {u} -1 \right)^2  \right]  \mathrm{d} u \label{reduction:relaxing to inf}
\end{align}
\endgroup
{\CHENGC where
(\ref{reduction:rewrite}) rewrites it to fit the format of Lemma~\ref{eq:concen}, 
(\ref{reduction:lemma3.1}) applies Lemma~\ref{lemma:concen}, 
(\ref{reduction:expand and sort out}) plugs in $\gamma(d)$, 
(\ref{reduction:integration}) relaxes (\ref{reduction:expand and sort out}) to an integration,
(\ref{reduction:substitute u}) substitutes $u = x / \tilde d$, and
(\ref{reduction:relaxing to inf}) relaxes the integration to $[1,+\infty)$.}
{\JIANYANG 
Next we first analyze the case of $\alpha \le \epsilon_0$ as follows.
% Under the region of reasonable accuracy, we can assume $\epsilon_0 \ge 1$ and yield (\ref{reduction:eps >= 1}). 
}
% Without loss of generality, we assume that $\tilde{d} \ge 1$ because when $\tilde{d} < 1$ the distance gap $\alpha$ is larger than the error bound $\epsilon_0$, in which constant dimensions are enough to provide firmed comparison results. Then we have $\epsilon_0 \ge \alpha $. 
\begin{align}
    \text{(\ref{reduction:relaxing to inf})}\le& \tilde{d} \int_1^{+\infty} \exp \left[ - \frac{c_0 \epsilon_0^2}{(1+\epsilon_0)^2} \left( \sqrt {u} -1 \right)^2  \right]  \mathrm{d} u  \label{reduction:alpha <= eps}
% \end{align}
% Under the region of reasonable accuracy, we can assume that $\epsilon_0\ge 1$. 
% %With {\color{red} LÃ©vy's Concentration Inequality~\cite{wainwright_2019}}, the constant $c_0$ is no smaller than $1/2$. 
% Then we have:
% \begin{align}
    %&\le \tilde{d} \int_1^{+\infty} \exp \left[ - \frac{1}{8} \left( \sqrt {u} -1 \right)^2  \right]  \mathrm{d} u
    \\\le& \tilde{d} \int_1^{+\infty} \exp \left[ - \frac{c_0}{4} \left( \sqrt {u} -1 \right)^2  \right]  \mathrm{d} u \label{reduction:eps >= 1}
\end{align}
{\CHENGC where (\ref{reduction:alpha <= eps}) is because $\alpha \le \epsilon_0$ and (\ref{reduction:eps >= 1}) is yielded when setting $\epsilon_0 \ge 1$ for reasonable accuracy.}
Note that the integration is convergent so as to be bounded by a constant. For the case of $\alpha \le \epsilon_0$, we have 
\begin{align}
    \mathbb{E} \left[ \hat D  \right]  = 1 + d_0 + O(\tilde{d}) = O(\tilde{d}) = O \left( \alpha^{-2} \cdot \epsilon_0^2 \right)  \label{eq:quadratic}
\end{align}
{\JIANYANG
{\CHENGB For the case of} $\alpha > \epsilon_0$, its expected dimensionality must be no greater than the case of $\alpha =\epsilon_0$ because its distance gap $\alpha$ is larger. Thus, its expected dimensionality is upper bounded by $O(1)$.
% , i.e., a constant.
}
%Applying the $\epsilon_0$ given in Theorem~\ref{theorem:eps}, we have Theorem~\ref{theorem:efficiency}.
% \end{proof}

% \begin{comment}

% \begin{theorem}
% For a KNN query in $D$-dimensional space, let $(1+\alpha )$ be the ratio between negative object $i$ and its corresponding largest positive object. Then the expected evaluated dimensionality of $i$ is 
% \begin{align}
%     %\mathbb{E} \left[ \hat d  \right]  = O \left( \frac{1}{\alpha ^2} \cdot \log \frac{L \cdot |\mathcal O_+|}{\delta} \right)  
%     %\mathbb{E} \left[ \hat d  \right]  = O \left( \min ( D,  \frac{1}{\alpha ^2}\log D + \frac{1}{\alpha ^2}\log \frac{K}{\delta} )  \right)   
%     \mathbb{E} \left[ \hat d  \right]  = O \left(   \frac{1}{\alpha ^2}\log D + \frac{1}{\alpha ^2}\log \frac{K}{\delta} \right)   
% \end{align}
% \end{theorem}
% \end{comment}

