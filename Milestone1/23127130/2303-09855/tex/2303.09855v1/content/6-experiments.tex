\section{Experiment}

\captionsetup[subfigure]{aboveskip=10pt}

\label{section:experiment}


% In this section, we conduct experimental study to compare the end-to-end performance of two algorithms (\texttt{IVF}, \texttt{HNSW}) combined with or without our plugin adaptive resolution algorithms. 

\subsection{Experimental Setup}
\label{subsec: experimental setup}
\noindent
\textbf{Datasets.} We {\CHENG use} six public datasets with varying {\CHENG sizes and dimensionalities}~\footnote{{\JIANYANG Note that our techniques introduce nearly no extra space consumption (the only extra space consumption is brought by a $D\times D$ random orthogonal matrix, which is ignorable compared with the huge $D$-dimensional database of size $N$). Thus, {\CHENGC they do not affect} the scalability of the AKNN algorithms. We thus focus on million-scale datasets to verify {\CHENGC their} effectiveness in speeding up the AKNN algorithms.}}, whose details are shown in Table~\ref{tab:data}. These datasets {\CHENG have been} widely used to benchmark AKNN algorithms ~\cite{lu2021hvs,adaptive2020ml, li2019approximate}.
%{\CHENG ***Better provide some references here.***}
% {\CHENG We note that these {\JIANYANGREVISION public} datasets 
% % involve 
% {\JIANYANGREVISION provide} both data {\JIANYANGB and query} vectors}.
{\JIANYANGREVISION We note that these  public datasets 
% involve 
provide both data and query vectors}.
% , which is independent with database vectors.

% \begin{comment}
% \begin{table}[h]
%     \caption{Dataset Statistics}
%     \label{tab:data}
% \begin{tabular}{c|cc}
% \hline
% Dataset & Size      & Dimensionality \\ \hline
% NUS     & 268,643   & 500            \\
% MSong   & 992,272   & 420            \\
% DEEP    & 1,000,000 & 256            \\
% GIST    & 1,000,000 & 960            \\
% GloVe   & 2,196,017 & 300            \\
% Tiny5M  & 5,000,000 & 384            \\ \hline
% \end{tabular}
% \end{table}
% \end{comment}
\begin{table}[h]
% \vspace{-4mm}
\caption{Dataset Statistics}
\vspace{-4mm}
\label{tab:data}
\begin{tabular}{c|cccc}
\hline
Dataset & Size      & $D$ & {\JIANYANGREVISION Query Size} & Data Type \\ \hline
Msong   & 992,272   & 420       & {\JIANYANGREVISION 200}     & Audio     \\
DEEP    & 1,000,000 & 256       & {\JIANYANGREVISION 1,000}     & Image     \\
Word2Vec & 1,000,000 & 300      & {\JIANYANGREVISION 1,000}      & Text      \\
GIST    & 1,000,000 & 960       & {\JIANYANGREVISION 1,000}     & Image     \\
GloVe   & 2,196,017 & 300       & {\JIANYANGREVISION 1,000}     & Text      \\
Tiny5M  & 5,000,000 & 384       & {\JIANYANGREVISION 1,000}     & Image     \\ \hline
\end{tabular}
% \vspace{-4mm}
\end{table}

\smallskip
\noindent
% \textbf{{\CHENGB Algorithms and} Performance Metric.} 
{\JIANYANGREVISION \textbf{{\CHENGB Algorithms}.} }
{\CHENGB For reliable DCOs, we compare our proposed method \texttt{ADSampling} with the conventional \texttt{FDScanning} {\JIANYANGREVISION and \texttt{PDScanning} (Partial Dimension Scanning), which we explain below. \texttt{PDScanning} 
% is a method which 
incrementally scans the dimensions of {\chengr a} raw vector 
% as \texttt{ADSampling} does on the randomized vector, while different from \texttt{ADSampling}, it
{\chengr and} terminates {\chengr the process} when the distance based on the partially scanned $d$ dimensions, i.e., $\sqrt {\sum_{i=1}^d x_i^2}$, is greater than the distance threshold $r$. {\chengr We note that \texttt{PDScanning} starts with zero dimensions but not a pre-set number of dimensions since (1) it is hard to set the number and (2) starting from a certain number of dimensions or zero dimensions have very similar performance given the fact that the dimensions are scanned incrementally.} We also note that \texttt{PDScanning} is an exact algorithm for {\chengr DCOs} and has the worst-case time complexity of $O(D)$. We name the AKNN algorithms {\chengr with} \texttt{PDScanning} {\chengr for DCOs} as 
% \texttt{AKNN^*}
\texttt{AKNN}*
and the one with a further optimized data layout as 
% \texttt{AKNN^{**}}
\texttt{AKNN}**
(for \texttt{IVF} only).} We exclude those distance approximation methods such as product quantization and random projection from comparison since as explained in Section~\ref{sec:introduction} and further verified in Section~\ref{subsubsec:reliable-dco}, they can hardly achieve reliable DCOs. 
{\JIANYANGREVISION For AKNN algorithms, we  mainly focus on \texttt{HNSW}~\cite{malkov2018efficient} and \texttt{IVF}~\cite{jegou2010product} for providing the contexts of DCOs since they correspond to two state-of-the-art AKNN algorithms as benchmarked in~\cite{annbenchmark, li2019approximate}. We note that these methods are widely adopted in industry (including 
% Faiss~\cite{johnson2019billion}~\footnote{\url{https://github.com/facebookresearch/faiss}}, 
Faiss~\cite{johnson2019billion}, 
Milvus~\cite{milvus} and PASE~\cite{PASE}).}}
% {\JIANYANGREVISION For better comprehensiveness, we also consider one of the best tree-based methods \texttt{Annoy}~\footnote{\url{https://github.com/spotify/annoy}} (as benchmarked in \cite{annbenchmark, li2019approximate}) and a hashing-based method \texttt{PMLSH}~\cite{zheng2020pm}. We note that their performance of time-accuracy tradeoff is suboptimal compared with \texttt{HNSW} and \texttt{IVF}. 
{\JIANYANGREVISION For better comprehensiveness, we also consider one of the best tree-based methods \texttt{Annoy}~\cite{annoy} (as benchmarked in \cite{annbenchmark, li2019approximate}) and a hashing-based method \texttt{PMLSH}~\cite{zheng2020pm}. We note that their performance of time-accuracy tradeoff is suboptimal compared with \texttt{HNSW} and \texttt{IVF}. 
% To avoid potential distraction, 
Due to the limit of space, 
we include their results in 
Appendix~\ref{appendix:section tree and hashing}.}
% the technical report~\cite{technical_report}.}
% We use recall, i.e., the ratio between the number of successfully retrieved ground truth KNNs and $K$, to measure accuracy and {\JIANYANG query-per-second (QPS), i.e., the number of handled queries per second, to measure efficiency. 

\smallskip
\noindent
{\JIANYANGREVISION \textbf{{\CHENGB Performance Metrics}.} }
{\JIANYANGREVISION We use two metrics to measure the accuracy: (1) 
recall~\cite{annbenchmark, li2019approximate, malkov2018efficient, jegou2010product}, i.e., the ratio between the number of successfully retrieved ground truth KNNs and $K$ and (2) {\chengr average distance} ratio~\cite{reviewer_paper, c2lsh, huang2015query, sun2014srs, reviewer_SISAP_metric}, i.e., the average of the {\chengr distance ratios 
% (i.e., relative distance errors 
% %  $+1$
% )
(which equals to the average relative error on distance {\chengr plus one})
of}
% corresponding distance ratio between 
the retrieved $K$ objects {\chengr wrt} the ground truth KNNs.
% {\chengr where each distance is incremented by 1 for avoiding undefined ratios}. 
We adopt the query-per-second (QPS), i.e., the number of handled queries per second, to measure efficiency. }
% Note that the query time is measured \textit{end-to-end}, meaning that the time cost of random transformation on query vector is included. 
{\JIANYANGB Note that the query time is measured \textit{end-to-end} (i.e., including the time of random transformation on query vectors). {\JIANYANGREVISION We decompose the time cost in Section~\ref{subsec: time decompostion}.}}
% We also measure the number of {\CHENG sampled} dimensions {\CHENG by \texttt{ADSampling} for a DCO operation} to verify our {\CHENG theoretical results}.
{\CHENG We also measure the total number of dimensions evaluated by an algorithm. For \texttt{AKNN} algorithms, it means the total number of dimensions of the candidates 
% considered by the algorithm 
(since for each candidate, all of its dimensions are used for computing its distance). 
% For \texttt{AKNN+} {\JIANYANGREVISION (\texttt{AKNN}*)} and \texttt{AKNN++} {\JIANYANGREVISION (\texttt{AKNN}**)} algorithms, it means the total number of {\chengr sampled (scanned)} dimensions of the candidates (since for a candidate, only those sampled {\chengr (scanned)} dimensions are used for computing its distance approximately). {\JIANYANGREVISION All the mentioned metrics are averaged over the whole query set.}
{\JIANYANGREVISION For \texttt{AKNN+} (\texttt{AKNN}*) and \texttt{AKNN++} (\texttt{AKNN}**) algorithms, it means the total number of {\chengr sampled (scanned)} dimensions of the candidates (since for a candidate, only those sampled {\chengr (scanned)} dimensions are used for computing its distance approximately). All the mentioned metrics are averaged over the whole query set.}
}

\smallskip
\noindent
\textbf{Implementation.} 
% {\JIANYANG We verify our techniques on two state-of-the-art AKNN algorithms \texttt{HNSW}~\cite{malkov2018efficient} and \texttt{IVF}~\cite{jegou2010product}.} 
The implementation of an AKNN algorithm consists of two phases. During {\CHENG the} \underline{index phase}, we first generate a random orthogonal transformation matrix with the NumPy library, store it and apply the transformation to all {\CHENG data} vectors. 
% Then we feed the transformed vectors {\JIANYANGREVISION (the raw vectors for \texttt{AKNN}, \texttt{AKNN}* and \texttt{AKNN}**)} into existing AKNN algorithms. In particular, 
% % for \texttt{HNSW}, \texttt{HNSW+} and \texttt{HNSW++} 
% {\JIANYANGREVISION for \texttt{HNSW}, \texttt{HNSW+},  \texttt{HNSW++} and \texttt{HNSW}*}
% (note that they have the same graph structure), our implementation is based on hnswlib~\cite{malkov2018efficient}, while 
% % for \texttt{IVF}, \texttt{IVF+}, and \texttt{IVF++} 
% {\JIANYANGREVISION for \texttt{IVF}, \texttt{IVF+}, \texttt{IVF++}, \texttt{IVF}* and \texttt{IVF}**}
% (note that they have the same cluster structure), our implementation of {\CHENGB K-means} clustering is based on the Faiss library~\cite{johnson2019billion}. 
% Then during {\CHENG the} \underline{query phase}, all {\CHENG algorithms} are implemented in C++.
% For a new query, we first transform the query vector with the Eigen library~\cite{eigenweb} for fast matrix multiplication {\JIANYANGREVISION when running \texttt{AKNN+} and \texttt{AKNN}++ algorithms (For \texttt{AKNN}, \texttt{AKNN}* and \texttt{AKNN}**, they involve no transformation). } Then we feed the vector into 
% % the \texttt{AKNN}, \texttt{AKNN+} and \texttt{AKNN++} 
% {\JIANYANGREVISION the \texttt{AKNN}, \texttt{AKNN+},  \texttt{AKNN++}, \texttt{AKNN}* and \texttt{AKNN}**}
% algorithms. 
{\JIANYANGREVISION Then we feed the transformed vectors (the raw vectors for \texttt{AKNN}, \texttt{AKNN}* and \texttt{AKNN}**) into existing AKNN algorithms. In particular, 
% for \texttt{HNSW}, \texttt{HNSW+} and \texttt{HNSW++} 
for \texttt{HNSW}, \texttt{HNSW+},  \texttt{HNSW++} and \texttt{HNSW}*
(note that they have the same graph structure), our implementation is based on hnswlib~\cite{malkov2018efficient}, while 
% for \texttt{IVF}, \texttt{IVF+}, and \texttt{IVF++} 
for \texttt{IVF}, \texttt{IVF+}, \texttt{IVF++}, \texttt{IVF}* and \texttt{IVF}**
(note that they have the same cluster structure), our implementation of {\CHENGB K-means} clustering is based on the Faiss library~\cite{johnson2019billion}.} 
Then during {\CHENG the} \underline{query phase}, all {\CHENG algorithms} are implemented in C++.
For a new query, we first transform the query vector with the Eigen library~\cite{eigenweb} for fast matrix multiplication when running \texttt{AKNN+} and \texttt{AKNN}++ algorithms 
{\JIANYANGREVISION (For \texttt{AKNN}, \texttt{AKNN}* and \texttt{AKNN}**, they involve no transformation). Then we feed the vector into 
% the \texttt{AKNN}, \texttt{AKNN+} and \texttt{AKNN++} 
the \texttt{AKNN}, \texttt{AKNN+},  \texttt{AKNN++}, \texttt{AKNN}* and \texttt{AKNN}**
algorithms.}
Following~\cite{graphbenchmark, li2019approximate}, we disable all hardware-specific optimizations including SIMD, memory prefetching and multi-threading ({\JIANYANG including those in the Eigen library}) {\CHENG so as to focus on the comparison among algorithms themselves}. 

%We {\CHENG conduct} random orthogonal transformation with the NumPy library and KMeans {\CHENG clustering (for \texttt{IVF}, \texttt{IVF+}, and \texttt{IVF++})} with the Faiss library during index phase. {\CHENG ***The reference to the Faiss library can be better provided.***}
%During query phase, all benchmarks were implemented in C++. {\CHENG ***We need conduct the random transformation on a query (i.e., during the query phase), for which NumPy library is used, and this is not in C++? I feel this part needs to be revised.***}
%Our implementation of \texttt{HNSW} is based on hnswlib~\cite{malkov2018efficient} and that of matrix multiplication (for random orthogonal transformation) is based on the Eigen library. {\CHENG ***The description is not clear enough. It is mentioned earlier that random orthogonal transformation is based NumPy.***} 
%Following~\cite{graphbenchmark, li2019approximate}, we disable all hardware-specific optimizations including SIMD, memory prefetching and multi-threading ({\JIANYANG including those in the Eigen library}) {\CHENG so as to focus on the comparison among algorithms themselves}. 

\smallskip
\noindent
\textbf{Parameter Setting.} 
{\JIANYANG For \texttt{HNSW}, two parameters are preset to control {\CHENG the construction of the graph}, namely $M$ to control the number of connected neighbors and $efConstruction$ to control the quality of approximate nearest neighbors. We follow the parameter settings of its original work~\cite{malkov2018efficient} where the parameters are set as $M=16$ and $efConstruction=500$.} 
For \texttt{IVF}, as suggested in the Faiss library~\footnote{\url{https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index}}, the number of clusters should be around the square root of the cardinality of the database. Since we focus on million-scale datasets, it's set to be {\CHENG 4,096}. 
For \texttt{ADSampling}, 
% though Theorem~\ref{theorem:eps} provides the asymptotic order of $\epsilon_0$, empirically here we suggest to 
we vary $\epsilon_0$ {\CHENG with values of 
% 1, 2, and 3 
1.5, 1.8, 2.1, 2.4, 2.7 and 3.0
and study its effects in Section~\ref{subsub:parameter}. Based on the results, we adopt the setting of 
% $\epsilon_0 = 2$
$\epsilon_0 = 2.1$
as the default one}.
% from 1.0 to 3.0, within which, we set $\epsilon_0=2.0$ by default. 
% {\CHENG ***Note that $\epsilon_0$ has been changed to $\epsilon_0$. This should be done throughout this section for consistency.***}
{\CHENG 
% In \texttt{ADSampling}, it aggressively and incrementally sample dimensions of a data vector in iterations and at each iteration, it performs a hypothesis testing. 
{\JIANYANG Recall that in \texttt{ADSampling}, it 
% aggressively and incrementally 
{\JIANYANGREVISION incrementally samples} some dimensions of a data vector and performs a hypothesis testing in iterations. }
To avoid the overhead of 
frequent hypothesis testings, {\JIANYANG we sample $\Delta_d$ dimensions at each iteration. By default, we set $\Delta_d=32$.}
{\JIANYANG Its parameter study is also provided in Section~\ref{subsub:parameter}.}
%double the number of dimensions sampled at each iteration. Specifically, it samples $d_1=32$, $d_2=64$, $d_3=128$, ..., $D$ dimensions at the 1st, 2nd, 3rd, ..., last iterations, respectively.
}
% To avoid the overhead of frequent hypothesis testings, we set the testing dimensions to be the power of 2 starting from 32, i.e. $d_1=32, d_2=64, d_3=128, ..., d_L = D$. 
% {\color{red} It makes sure that the actual evaluated dimension is at most twice as the theoretically expected. (possibly not true)} 

% \textbf{Environment.} 
All C++ source codes are complied by g++ 9.4.0 with 
% the standard 
\texttt{-O3} optimization under
% the environment of 
Ubuntu 20.04LTS. The Python source codes (which are used during the index phase) are run on Python 3.8. All experiments are {\CHENG conducted} on a machine with AMD Threadripper PRO 3955WX 3.9GHz 16C/32T processor and 64GB RAM. The code and datasets are available at \url{https://github.com/gaoj0017/ADSampling}.
%{\CHENG ***We normally specify the OS as well.***}

% \begin{figure*}[ht]
% % \vspace*{-4mm}
%   \centering 
%   % \includesvg[width=17cm]{experimental result/time-accuracy.svg}
%   % \includesvg[width=17cm]{revision experimental result/time-accuracy.svg}
%   \includesvg[width=17cm]{revision experimental result/time-accuracy-main.svg}
%   \vspace*{-4mm}
%   \caption{{\JIANYANGREVISION Time-Accuracy Tradeoff (\texttt{HNSW} and \texttt{IVF}).}}
%   % \vspace*{-4mm}
%   \label{figure:time-accuracy}
% \end{figure*}

\begin{figure*}[ht]
% \vspace*{-4mm}
  \centering 
    \includegraphics[width=17cm]{revision experimental result/time-accuracy-main.pdf}
  % \includesvg[width=17cm]{revision experimental result/time-accuracy-main.svg}
  \vspace*{-4mm}
  \caption{{\JIANYANGREVISION Time-Accuracy Tradeoff (\texttt{HNSW} and \texttt{IVF}).}}
  % \vspace*{-4mm}
  \label{figure:time-accuracy}
\end{figure*}

\begin{figure*}[ht]

% \captionsetup[subfigure]{aboveskip=-1pt,belowskip=-1pt}
% \vspace*{-4mm}
% \setlength{\abovecaptionskip}{-4.mm}
   %  \begin{subfigure}[b]{0.32\linewidth}
   %      \includegraphics[width=\textwidth]{revision experimental result/IVFPQ.pdf}
   %      \caption{\texttt{IVFPQ}}
	  % \label{fig:cost IVFPQ}
   %  \end{subfigure} 
   \captionsetup[subfigure]{aboveskip=-5pt}
\begin{subfigure}[b]{0.48\linewidth}
    % \includesvg[width=8.5cm]{revision experimental result/GIST-HNSW.svg}
    \includegraphics[width=\textwidth]{revision experimental result/GIST-HNSW.pdf}
    \caption{\texttt{HNSW}}
    \label{figure:evaluated_dimension HNSW}
\end{subfigure}  
\begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\textwidth]{revision experimental result/GIST-IVF.pdf}
    % \includesvg[width=8.5cm]{revision experimental result/GIST-IVF.svg}
    \caption{\texttt{IVF}}
    \label{figure:evaluated_dimension IVF}
\end{subfigure} 
% \subfigure[\texttt{HNSW}]{
%     % \captionsetup{skip=10pt}
% % \subfigure[]{
%     % \vspace*{-4mm}
%     \includesvg[width=8.5cm]{revision experimental result/GIST-HNSW.svg}
%     \label{figure:evaluated_dimension HNSW}
% }
% \subfigure[\texttt{IVF}]{
% % \subfigure[]{
%     % \vspace*{-4mm}
% 	\includesvg[width=8.5cm]{revision experimental result/GIST-IVF.svg}
% 	\label{figure:evaluated_dimension IVF}
% }
\vspace*{-4mm}
\caption{\JIANYANGREVISION Evaluated Dimensionality and Accuracy.}
\vspace*{-4mm}
\label{figure:evaluated_dimension}
\end{figure*}

\subsection{{\CHENG Experimental Results}}
\label{section:time-accuracy}
\subsubsection{\textbf{{\CHENG Overall Results (Time-Accuracy Trade-Off)}}}
\label{subsec:main result}
% We first compare the end-to-end search performance of the \texttt{IVF} and \texttt{HNSW} algorithms, each with and without our adpative resolution plugins. 
{\JIANYANGREVISION We {\CHENG plot} the QPS-recall curve (upper panels, upper-right is better) and the QPS-ratio curve (lower panels, upper-left is better) by varying $N_{ef}$ for \texttt{HNSW}/\texttt{HNSW}*/\texttt{HNSW+}/\texttt{HNSW++} and $N_{probe}$ for \texttt{IVF}/\texttt{IVF}*/\texttt{IVF}**/\texttt{IVF+}/\texttt{IVF++} in Figure~\ref{figure:time-accuracy}.} 
We focus only on the region with the recall at least 80\% {\CHENG based on} practical needs.
%
Overall, with the results in Figure~\ref{figure:time-accuracy}, we can observe clearly 
%except for $K=20$ on NUS 
% that the AKNN algorithms equipped with our adaptive plugins outperform the plain baseline significantly. 
% {\CHENG that (1) the \texttt{AKNN+} algorithms (represented by yellow curves) outperform the plain \texttt{AKNN} algorihtms (represented by greeen curves) and (2) the \texttt{AKNN++} algorithms (represented by red curves) further outperform the \texttt{AKNN+} algorithms.}
% {\CHENG ***We may elaborate on the results more with discussions (e.g., when the gap is larger (in terms of recall, datasets, etc.), how large is the gap in general, and any other findings, any comments on how \texttt{ADSampling} improves \texttt{HNSW} and \texttt{IVF} differently, comments on the comparison between \texttt{HNSW} and \texttt{IVF} (e.g., it can be mentioned that no algorithm dominates the other one on all datasets), etc. These results are the major ones in this paper, and it is fine that we discuss these results in more detail.***}
%Even for $K=20$ on NUS, our algorithms only fail in high accuracy region of \texttt{HNSW} algorithm, where it's outperformed by \texttt{IVF}, which proves the effectiveness of our techniques.
% Besides, within our adaptive resolution algorithms, it shows that the ARSelect and ARRoute (green) stably outperform the adaptive resolution search (yellow), which verifies the effectiveness of our algorithm-specific optimization. 
%
{\JIANYANG that (1) the \texttt{AKNN+} algorithms (represented by the orange curves) outperform the plain \texttt{AKNN} algorithms (represented by the red curves), (2) the \texttt{AKNN++} algorithms (represented by the green curves) further outperform the \texttt{AKNN+} algorithms}, {\JIANYANGREVISION (3) the baseline method \texttt{HNSW}* (represented by the blue curves) brings very minor improvements on \texttt{HNSW} for all the tested datasets and (4) the baseline methods \texttt{IVF}* {\chengr (represented by the blue curves)} and \texttt{IVF}** (represented by the violet curves) are outperformed by \texttt{IVF+} consistently and significantly ({\chengr and by} \texttt{IVF++} {\chengr with an} even larger margin).}

{\JIANYANG
Besides, we have the following observations. (1) Our techniques bring more improvements on \texttt{IVF} than on \texttt{HNSW} (even when \texttt{IVF} performs better than \texttt{HNSW}, e.g., on Word2Vec). We ascribe it to the fact that other computations than DCOs of \texttt{HNSW} are heavier than {\CHENG those} of \texttt{IVF} (as shown in Figure~\ref{fig:cost statistics}). (2) Our techniques in general bring more improvements on high accuracy region than on low accuracy region (e.g., GIST 95\% v.s. 85\%). This is because {\CHENGB when} an AKNN algorithm targets higher accuracy, it unavoidably generates more low-quality candidates with larger distance gap $\alpha$, 
% which needs fewer dimensions.
{\CHENGB for which it needs fewer dimensions for reliable DCOs.} {\JIANYANGREVISION (3) The data layout optimization brings more improvements on \texttt{IVF+} (i.e., \texttt{IVF++} v.s. \texttt{IVF+}) than on \texttt{IVF}* (i.e., \texttt{IVF}** v.s. \texttt{IVF}*). This is because \texttt{ADSampling} has the logarithmic complexity while the baseline \texttt{PDScanning} has the linear complexity. Specifically, the first $\Delta_d$ dimensions are sufficient for many DCOs when using \texttt{ADSampling} and thus, many accesses to the second array $A_2$ in Figure~\ref{fig:data layout ARScan} can be avoided. When using \texttt{PDScanning} for the DCOs, it will still access the second array frequently because it needs {\chengr more than $\Delta_d$} dimensions.}
}

% \textbf{\texttt{ADSampling} on datasets with different dimensionality.} We study the improvement \texttt{ADSampling} brings on datasets with different dimensionality. 

% \textbf{\texttt{HNSW}/\texttt{HNSW+}/\texttt{HNSW++}.} 

% \textbf{\texttt{IVF}/\texttt{IVF+}/\texttt{IVF++}.} 

\subsubsection{\textbf{{\CHENG Results of Evaluated Dimensions and Recall}}}
\label{subsub:dimensions and recall}
% {\CHENG ***From this point on, the descriptions are out-dated - they are based on the previous presentation flow and notions. Please revise them to be betwee aligned with the latest presentation and notions.***}

{\JIANYANG 
We then study the number of evaluated dimensions and the recall of 
% \texttt{AKNN}/\texttt{AKNN+}/\texttt{AKNN++}
{\JIANYANGREVISION \texttt{AKNN}/\texttt{AKNN+}/\texttt{AKNN++}/\texttt{AKNN}* (\texttt{AKNN}** has exactly the same curve as \texttt{AKNN}* and thus, is omitted)}
under the same search parameter setting ($N_{ef}$ for \texttt{HNSW} and $N_{probe}$ for \texttt{IVF}). {\CHENG For the number of evaluated dimensions, we measure its ratio over that of 
% the plain
\texttt{AKNN} in percentage for the ease of comparison. The results are shown in Figure~\ref{figure:evaluated_dimension}.}
% Specifically, for \texttt{HNSW} (resp. IVF) we plot the total evaluated dimension\footnote{For conciseness, we show its ratio over that of the plain \texttt{AKNN} in percentage.}-$N_{ef}$ (resp. $N_{probe}$) and recall-${N_{ef}}$ (resp. recall-${N_{probe}}$) curves in Figure~\ref{figure:evaluated_dimension}.
}

%We then verify our statement that our adaptive algorithms can almost preserve the search results while avoid a large number of unnecessary dimension evaluation. We study the total evaluated dimensions and accuracy of \texttt{HNSW} and \texttt{IVF} with and without our adaptive resolution algorithms under the same search parameter ($N_{probe}$ for \texttt{IVF} and $N_{ef}$ for \texttt{HNSW}) setting. In Figure~\ref{figure:evaluated_dimension}, the upper panel shows the curve of the ratio of evaluated dimension between adaptive resolution algorithms (yellow and green) and the plain baseline (red) and the lower shows the recall curve, both with respect to the search parameters $N_{ef}$ and $N_{probe}$. 

% \begin{comment}
% \begin{figure*}[ht]
%   \centering 
%   \includesvg[width=18cm]{experimental result/evaluated_dimension.svg}
%   \caption{Evaluated Dimensionality and Accuracy.}
%   \label{figure:evaluated_dimension}
% \end{figure*}
% \end{comment}

% \begin{figure*}[ht]
% % \vspace*{-4mm}
% \subfigure[\texttt{HNSW}]{
%     % \vspace*{-4mm}
%     \includesvg[width=8.5cm]{experimental result/HNSW.svg}
%     \label{figure:evaluated_dimension HNSW}
% }
% \subfigure[\texttt{IVF}]{
%     % \vspace*{-4mm}
% 	\includesvg[width=8.5cm]{experimental result/IVF.svg}
% 	\label{figure:evaluated_dimension IVF}
% }
% \vspace*{-4mm}
% \caption{Evaluated Dimensionality and Accuracy.}
% \vspace*{-4mm}
% \label{figure:evaluated_dimension}
% \end{figure*}

% \begin{figure*}[ht]
% % \captionsetup[subfigure]{aboveskip=-1pt,belowskip=-1pt}
% % \vspace*{-4mm}
% % \setlength{\abovecaptionskip}{-4.mm}
%    %  \begin{subfigure}[b]{0.32\linewidth}
%    %      \includegraphics[width=\textwidth]{revision experimental result/IVFPQ.pdf}
%    %      \caption{\texttt{IVFPQ}}
% 	  % \label{fig:cost IVFPQ}
%    %  \end{subfigure} 
%    \captionsetup[subfigure]{aboveskip=-5pt}
% \begin{subfigure}[b]{0.48\linewidth}
%     % \includesvg[width=8.5cm]{revision experimental result/GIST-HNSW.svg}
%     \includegraphics[width=\textwidth]{revision experimental result/GIST-HNSW.pdf}
%     \caption{\texttt{HNSW}}
%     \label{figure:evaluated_dimension HNSW}
% \end{subfigure}  
% \begin{subfigure}[b]{0.48\linewidth}
%     \includegraphics[width=\textwidth]{revision experimental result/GIST-IVF.pdf}
%     % \includesvg[width=8.5cm]{revision experimental result/GIST-IVF.svg}
%     \caption{\texttt{IVF}}
%     \label{figure:evaluated_dimension IVF}
% \end{subfigure} 
% % \subfigure[\texttt{HNSW}]{
% %     % \captionsetup{skip=10pt}
% % % \subfigure[]{
% %     % \vspace*{-4mm}
% %     \includesvg[width=8.5cm]{revision experimental result/GIST-HNSW.svg}
% %     \label{figure:evaluated_dimension HNSW}
% % }
% % \subfigure[\texttt{IVF}]{
% % % \subfigure[]{
% %     % \vspace*{-4mm}
% % 	\includesvg[width=8.5cm]{revision experimental result/GIST-IVF.svg}
% % 	\label{figure:evaluated_dimension IVF}
% % }
% \vspace*{-4mm}
% \caption{\JIANYANGREVISION Evaluated Dimensionality and Accuracy.}
% \vspace*{-4mm}
% \label{figure:evaluated_dimension}
% \end{figure*}

% \begin{figure}[thb]
%   \centering 
% %   \vspace{-4mm}
%   \includesvg[width=\linewidth]{revision experimental result/dimension-tradeoff.svg}
%   \vspace{-8mm}
%   \caption{Evaluated Dimensionality and Accuracy.}
%   \vspace{-4mm}
%   \label{figure:evaluated_dimension}
%   \label{figure:evaluated_dimension IVF}
%   \label{figure:evaluated_dimension HNSW}
% \end{figure}

% \begin{figure}[thb]
%   \centering 
% %   \vspace{-4mm}
%   % \include[width=\linewidth]{revision experimental result/dimension-tradeoff.svg}
%   \includegraphics[width=\linewidth]{revision experimental result/decomposition.pdf}
%   \vspace{-8mm}
%   \caption{Decomposition of Time Cost.}
%   \vspace{-4mm}
%   \label{fig:decomposition}
% \end{figure}



% \begin{figure}[ht]
% % \vspace*{-4mm}
% \subfigure[\texttt{HNSW}]{
%     \vspace{-4mm}
%     \includesvg[width=\linewidth]{experimental result/HNSW.svg}
%     \label{figure:evaluated_dimension HNSW}
% }
% \subfigure[\texttt{IVF}]{
%     \vspace{-4mm}
% 	\includesvg[width=\linewidth]{experimental result/IVF.svg}
% 	\label{figure:evaluated_dimension IVF}
% }
% \vspace{-8mm}
% \caption{Evaluated Dimensionality and Accuracy.}
% \vspace{-4mm}
% \label{figure:evaluated_dimension}
% \end{figure}

\smallskip
\noindent
\textbf{{\CHENG Overall Results.}} 
%As shown in the lower panel, the recall curves of the plain baseline, adaptive resolution search and ARSelect (ARRoute) largely overlap, indicating that our methods themselves indeed preserve the search results well. At the same time, as shown in the upper panel, under the same parameter setting, all our adaptive algorithms skip lots of dimensions (on \texttt{IVF}, GIST, K=20, it even saves up to 89.95\% total dimension evaluation).
{\JIANYANG
In Figure~\ref{figure:evaluated_dimension}, we can observe clearly that \texttt{AKNN+} and \texttt{AKNN++} {\CHENG evaluate} much fewer dimensions than \texttt{AKNN} while {\CHENG reaching} nearly the same recall. Specifically, on GIST, for all tested values of $N_{ef}$, the accuracy loss of \texttt{HNSW+} and \texttt{HNSW++} (compared with \texttt{HNSW}) is no more than 0.14\% and that of \texttt{IVF+} and \texttt{IVF++} is no more than 0.1\%. At the same time,
\texttt{HNSW++} saves from 39.4\% to 75.3\% of the total dimensions, 
\texttt{HNSW+} saves from 34.5\% to 39.4\% and 
\texttt{IVF+}/\texttt{IVF++} save from 76.5\% to 89.2\%. {\JIANYANGREVISION The baseline method \texttt{HNSW}* saves from 10.9\% to 15.7\%, which explains its minor improvement on \texttt{HNSW}. \texttt{IVF}*/\texttt{IVF}** saves from 28.9\% to 40.4\%.} 
}

\begin{figure}[thb]
  \centering 
  % \vspace{-4mm}
  % \includesvg[width=\linewidth]{revision experimental result/decomposition_merge.svg}
  \includegraphics[width=\linewidth]{revision experimental result/decomposition_merge.pdf}
  \vspace{-8mm}
  \caption{{\JIANYANGREVISION Decomposition of Time Cost (At a particular {\chengr recall}, the three horizontal bars from top to bottom, represent the \texttt{AKNN++}, \texttt{AKNN+} and \texttt{AKNN} algorithms, {\chengr respectively}. The time cost is normalized by the cost of the original \texttt{AKNN} algorithms).}}
  \vspace{-4mm}
  \label{fig:decomposition_simple}
\end{figure}

\smallskip
\noindent
% \textbf{{\CHENG Results of \texttt{HNSW} Algorithms}.}
\textbf{{\JIANYANG \texttt{HNSW+} v.s. \texttt{HNSW++}}.}
{\JIANYANG
We further compare \texttt{HNSW+} and \texttt{HNSW++}. According to Figure~\ref{figure:evaluated_dimension HNSW}, we have the following observations.
(1) \texttt{HNSW++} evaluates \emph{fewer dimensions} than \texttt{HNSW+}, 
% {\CHENG which is well aligned with our theoretical results in Theorem~\ref{theorem:time-accuracy of ADSampling} (recall that the ratio $\alpha$ in \texttt{HNSW++} is larger than that in \texttt{HNSW+}.}
% which verifies our claim that \texttt{HNSW++} improves \texttt{HNSW+} by reducing more dimensions. 
{\CHENG which largely explains the result that \texttt{HNSW++} runs faster than \texttt{HNSW+}.}
(2) \texttt{HNSW++} reaches \emph{nearly the same recall} as \texttt{HNSW+}, which empirically shows that using approximate distances for graph routing has nearly the same effectiveness as using exact distances. 
% (3) \texttt{HNSW++} brings more improvement in the case of $K=20$ than $K=100$. This is because in \texttt{HNSW++}, a DCO is conducted between a candidate and the current $K^{th}$ NN. A smaller $K$ implies a larger distance gap $\alpha$ and consequently {\CHENG fewer} needed dimensions {\CHENG (according to Theorem~\ref{theorem:time-accuracy of ADSampling})}.
% (4) For \texttt{HNSW+}, there is no difference between $K=20$ and $K=100$. This is because in \texttt{HNSW+}, DCO is conducted between a candidate and the current $N_{ef}^{th}$ NN, which is independent of $K$.
(3) The evaluated dimensions (its ratio over those of \texttt{HNSW} in percentage) of \texttt{HNSW+} increases wrt $N_{ef}$ while those of \texttt{HNSW++} decreases wrt $N_{ef}$. This is because \texttt{HNSW+} conducts DCOs with the $N_{ef}^{th}$ NN's distance as the threshold, whose distance increases wrt $N_{ef}$, and thus a larger $N_{ef}$ leads to smaller distance gap $\alpha$, which entails more dimensions for a reliable DCO. 
For \texttt{HNSW++},
% conducts DCOs with the $K^{th}$ NN's distance as the threshold, which is unchanged wrt $N_{ef}$. At the same time, 
as mentioned in \ref{subsec:main result}, when targeting high recall, an AKNN algorithm inevitably generates many low-quality candidates with larger $\alpha$'s, and thus, it needs fewer dimensions for DCOs.
}
%{\CHENG ***The description here does not look sorted to me. Needs to be revised.***}Let's focus on ARSearch and ARRoute on \texttt{HNSW}. In terms of ARRoute (green), by increasing $N_{ef}$, we're searching for the very few remaining KNNs with visiting a lot of candidates, most of which can be far away from the query, and as a result, have large space for dimension reduction. The phenomenon that the greed curve decreases monotonically with $N_{ef}$ matches our expectation perfectly. Also, since DCO is conducted between a new candidate and $dis_{i_K}$, a smaller $K$ implies more space for dimension reduction. As a result, when K=20, it brings more benefits than the case of K=100. However, in terms of adaptive resolution search (yellow), as it aims to preserve the full search path of greedy beam search, its performance depends only on $N_{ef}$ rather than $K$, which causes the yellow lines of K=20 and K=100 are exactly identical. Plus, as $N_{ef}$ increases, the radius of DCO, i.e. the distance of $N_{ef}$NN, grows respectively. Thus, even though for large $N_{ef}$ we're visiting a large number of low-quality candidates, the space for dimension reduction decreases.

\smallskip
\noindent
% \textbf{{\CHENG Results of \texttt{IVF} Algorithms}.}
\textbf{{\JIANYANG \texttt{IVF+} v.s. \texttt{IVF++}}.} \texttt{IVF+} and \texttt{IVF++} differ only in data layout, and thus they have exactly the same accuracy and evaluated dimensions. 
%We exhibit the results of ARSearch and ARScan (green) on \texttt{IVF}\footnote{Note that these two methods differ only in data layout, so their evaluated dimensions and recall should be exactly the same}. We also show another curve, the one where we provide ground truth $dis_{i_K^*}$ as the distance threshold. Note that in KNN query, such threshold is obviously the minimum and safe threshold for maintaining KNN set (black). Based on Figure~\ref{figure:evaluated_dimension IVF}, we verified two statements. \underline{First}, ARSearch and ARScan indeed reduce a lot of dimensions (up to 89.95\% on GIST) without harming accuracy for \texttt{IVF}. \underline{Second}, there is little space for reducing more dimensions because our ARSearch and ARScan reduce almost as many dimensions as the case of providing the minimum ground truth radius.

%Figure~\ref{figure:evaluated_dimension IVF} shows that (1) .

%we also compare the results of ARSearch/ARScan on \texttt{IVF}. We can see their curves highly overlap with each other, indicating that the evaluated dimensions are very close, {\color{red} it's currently contradictory to our expectation.}

{\JIANYANGREVISION
\subsubsection{\textbf{Results of Time Cost Decomposition}}
\label{subsec: time decompostion}
We note that applying \texttt{ADSampling} entails the extra cost of randomly transforming the data and query vectors. In particular, the cost of transforming the data vectors lies in the index phase and can be amortized by all the subsequent queries on the same database. The transformation of the query vectors is conducted during the query phase when a query comes and its cost can be amortized by all the DCOs involved for answering the same query. 
% This step takes $O(D^2)$ 
{\chengr We implement this step {\JIANYANGREVISION (a.k.a, Johnson-Lindenstrauss Transformation~\cite{johnson1984extensions, JL_survey})} as a matrix multiplication operation for simplicity, which takes $O(D^2)$ time. We note that this step can be performed in less time with advanced algorithms~\cite{JL_survey}, e.g., it takes $O(D\log D)$ time with Kac's Walk~\cite{kac_walk}.}
% it takes  with a straightforward method (i.e., matrix multiplication) and }. 
We show the results of time cost decomposition on the dataset GIST. It has the highest dimensionality and correspondingly the largest overhead for {\chengr random transformation}. 
% (TODO, discuss on the time decomposition figure and describe it)
We decompose the time cost in 
% Version 1
% Figure~\ref{fig:decomposition}.
% Version 2
Figure~\ref{fig:decomposition_simple}.
% At a particular accuracy, the three horizontal bars from top to bottom, represent the \texttt{AKNN++}, \texttt{AKNN+} and \texttt{AKNN} algorithms correspondingly. The time cost is normalized by the cost of the original \texttt{AKNN} algorithms.
% Version 1
% We note that the cost of randomization for \texttt{HNSW+}/\texttt{HNSW++} takes at most 5.42\% of the total cost of the original \texttt{HNSW}. As the requirement of accuracy increases, its cost takes smaller proportion (e.g., 95\% recall, it reduces to 1.83\%). For \texttt{IVF+}, its cost is no greater than 1.00\%.
% Version 2
We note that the cost of {\chengr random transformation} for \texttt{HNSW+}/\texttt{HNSW++} takes at most 6.18\% of the total cost of the original \texttt{HNSW}. As the accuracy increases, the percentage decreases (e.g., for 95\% recall, it reduces to 2.02\%). For \texttt{IVF+}, the percentage is no greater than 1.11\%.

% We note that our study initiates \texttt{ADSampling} with the very basic algorithm for randomization, and its performance is good enough. There are also plentiful of advanced algorithms for fast randomization (e.g., Kac's Walk which runs in $O(D\log D)$~\cite{kac_walk}), which can further reduce the overhead. We refer readers to a recent comprehensive survey~\cite{JL_survey}. 
}

% {\JIANYANGB

\subsubsection{\textbf{Results for {\CHENGB Evaluating} the Feasibility {\CHENGB of Distance Approximation Techniques} for Reliable DCOs}}
\label{subsubsec:reliable-dco}
\begin{figure}[thb]
  \centering 
  % \vspace{-4mm}
%   \includesvg[width=\linewidth]{revision experimental result/acc_clear.svg}
   % \includesvg[width=\linewidth]{revision experimental result/acc.svg}
   \includegraphics[width=\linewidth]{revision experimental result/acc.pdf}
  % \includesvg[width=\linewidth]{revision experimental result/feasibility.svg}
  \vspace{-8mm}
  \caption{{\JIANYANGREVISION Feasibility for Reliable DCOs (Recall).}}
  \vspace{-4mm}
  \label{figure:feasibility}
\end{figure}

\begin{figure}[thb]
    \centering
    % \vspace{-1mm}
    % \includesvg[width=\linewidth]{revision experimental result/qps_clear.svg}
    % \includesvg[width=\linewidth]{revision experimental result/qps.svg}
    \includegraphics[width=\linewidth]{revision experimental result/qps.pdf}
    % \includesvg[width=\linewidth]{revision experimental result/qps_linear.svg}
    \vspace{-8mm}
    \caption{{\JIANYANGREVISION Feasibility for Reliable DCOs (QPS).}}
    \vspace{-4mm}
    \label{fig:qps linear}
\end{figure}


We next study the feasibility of 
% {\CHENGB the distance approximation} methods mentioned in Section~\ref{subsec: experimental setup}, 
{\CHENGB two distance approximation methods, including random projection and product quantization~\cite{jegou2010product} {\JIANYANGREVISION (with the typical setting of 256 centroids per partition~\cite{jegou2010product, learningtohashsurvey})},} for reliable DCOs. {\CHENGB We include the results of \texttt{ADSampling}, {\JIANYANGREVISION \texttt{PDScanning}} and \texttt{FDScanning} for comparison.} To test the best possible recall a method can reach, we conduct this experiment with an exact KNN algorithm, namely linear scan. Specifically, for random projection and product quantization, we scan all the data objects and return the K objects with the minimum approximate distances. For \texttt{ADSampling} and {\JIANYANGREVISION \texttt{PDScanning}}, like \texttt{IVF}, we maintain a KNN set and conduct DCOs for each object sequentially. We plot the recall-number of dimensions/lookups
~\footnote{
For product quantization, it refers to the quantization code size,
% {\JIANYANGREVISION (the number of partitions)} 
where evaluating each code would look up a table in memory (i.e., access memory randomly). For other methods, it refers to the number of dimensions, where evaluating each dimension applies some arithmetic computations. Note that they are not directly comparable because in modern CPUs, the former is much slower than the latter. 

% {\JIANYANGREVISION This issue is also acknowledged by the authors of the classic PQ~\cite{jegou2010product} in the Faiss open-source community~\url{https://github.com/facebookresearch/faiss/issues/148}.}
% We emphasize that the number of lookups of product quantization is not directly comparable with the dimensionality of other methods because evaluating a quantization code entails looking up a table in memory (i.e., access memory randomly) while evaluating a dimension applies arithmetic computations. In modern CPUs, the former is much slower than the latter. 
% In the extreme case that the code size is 50\% of the dimensionality, it runs even slower than \texttt{FDScanning}.
} 
curves in Figure~\ref{figure:feasibility}. For \emph{random projection}, we vary the dimensionality of the projected vectors and observe that (1) it introduces 3.71\% accuracy loss {\CHENGB while} reducing only 1.04\% dimensions (2) {\CHENGB when} reducing half of the dimensionality, its recall is no more than 70\%. For \emph{product quantization}, we vary its quantization code size {(i.e., the number of partitions)} and observe that 
in the best possible case
{\JIANYANGREVISION (i.e., the case of encoding every two dimensions with one code)},
% ~\footnote{That is the case when the code size is 50\% of the full dimensionality (encoding two dimensions with one code). We emphasize that in this case, it runs even slower than \texttt{FDScanning} {\JIANYANGREVISION as shown in Figure~\ref{fig:qps linear}}.}
it still introduces 6.2\% accuracy loss. 
{\CHENGC Therefore, neither product quantization nor random projection can achieve reliable DCOs with remarkably better efficiency than \texttt{FDScanning}.}
% its best possible recall is 93.842\% {\JIANYANG (i.e., one code for two dimensions, with this size, it runs even slower than \texttt{FDScanning} because evaluating quantization codes is slow)}, which introduces unignorable loss. Therefore, the distance approximation methods can hardly achieve reliable DCOs. 

For \texttt{ADSampling}, we test two settings $\Delta_d=1$ and $\Delta_d=32$. The former represents the best possible recall-dimension tradeoff of our method and the latter represents a practical setting with less frequent hypothesis testing (i.e., our default setting). We plot their curves by varying $\epsilon_0$ from 0.0 to 4.0. We observe that for $\Delta_d=1$, it samples 6.61\% of the total dimensions {\CHENGB while reaching} >99.9\% recall and for $\Delta_d=32$, it samples 7.11\% of the total dimensions {\CHENGB while reaching} > 99.9\% recall. Thus, \texttt{ADSampling} achieves much better recall-dimension tradeoff than \texttt{FDScanning}. 

\begin{figure}[thb]
  \centering 
  % \vspace{-4mm}
  % \includesvg[width=\linewidth]{experimental result/epsilon.svg}
  \includegraphics[width=\linewidth]{experimental result/epsilon.pdf}
  \vspace{-8mm}
  \caption{Parameter Study on $\epsilon_0$ of \texttt{AKNN++} Algorithms.}
  \vspace{-4mm}
  \label{figure:parameter}
\end{figure}


\begin{figure}[thb]
  \centering 
  % \vspace{-1mm}
  % \includesvg[width=\linewidth]{experimental result/gap.svg}
  \includegraphics[width=\linewidth]{experimental result/gap.pdf}
  \vspace{-8mm}
  \caption{Parameter Study on the $\Delta_d$ of \texttt{AKNN++} Algorithms.}
  \vspace{-4mm}
  \label{figure:parameter gap}
\end{figure}

\begin{figure}[thb]
  \centering 
  % \vspace{-1mm}
  % \includesvg[width=\linewidth]{experimental result/verification.svg}
  \includegraphics[width=\linewidth]{experimental result/verification.pdf}
  \vspace{-8mm}
  \caption{Verification for Theoretical Analysis.}
  \vspace{-4mm}
  \label{figure:verification}
\end{figure}

{\JIANYANGREVISION 
In Figure~\ref{fig:qps linear}, we plot the QPS-dimensions/lookups curves. We have the following observations. 
% (1) When {\chengf product quantization} achieves its best possible accuracy (still 6.2\% recall loss), its efficiency is worse than \texttt{FDScanning}.
% We can further improve the cache-friendliness of \texttt{ADSampling} by re-organizing the data layout like \texttt{IVF++} (the result is not included). 
(1) {\chengr \texttt{ADSampling} (with default setting), which is marked with a green cross within a green circle, has the QPS significantly higher than \texttt{FDScanning} and \texttt{PDScanning}. This is because it exploits only 7.11\% of the total dimensions while achieving a recall over $99.9\%$.} 
% has the recall over $99.9\%$. We note that it exploits only 7.11\% of the total dimensions of \texttt{FDScanning}, while its improvement on the QPS is less than 10x. This is also due to the issue of cache. 
(2) At the same dimensionality, random projection has its efficiency better than \texttt{ADSampling}. This is because random projection has fixed dimensionality and can organize the projected vectors sequentially in an array to achieve better cache-friendliness. However, we note that when random projection has the same QPS as \texttt{ADSampling} (default), its recall does not exceed 40\%. 
% At the same dimensionality, \texttt{ADSampling} has its efficiency worse than random projection. This is because random projection sequentially organizes the projected vectors in an array and thus, is more cache-friendly. {\chengr We emphasize that random projection cannot perform reliable DCOs as indicated by the results in Figure~\ref{figure:feasibility}.}

% In Figure~\ref{fig:qps linear}, we plot the QPS-dimensions/lookups curve for the methods which achieve reliable DCOs including \texttt{FDScanning}, \texttt{PDScanning} and \texttt{ADSampling} ($\Delta_d=32,\epsilon_0=2.1$). We also include the results of product quantization~\cite{jegou2010product}. It shows that (1) when PQ achieves its best possible accuracy (still 6.2\% accuracy loss), its efficiency is worse than \texttt{FDScanning} and (2) though \texttt{ADSampling} exploits only 7.11\% of the total dimensions of \texttt{FDScanning}, its improvement on the QPS is less than 10x.
% We observe in Figure~\ref{figure:feasibility} that 
% We verify the fact that distance approximation techniques including random projection~\cite{johnson1984extensions} and product quantization~\cite{jegou2010product} cannot achieve 
}

\subsubsection{\textbf{{\CHENG Results of} Parameter Study}}
\label{subsub:parameter}

% \begin{figure}[ht]
%   \centering 
%   \includesvg[width=\linewidth]{experimental result/verification_merge.svg}
%   \caption{Verification for Time-Accuracy Tradeoff.}
%   \label{figure:verification}
% \end{figure}

% \input{content/statistics}

{\JIANYANG
% We then conduct parameter study on \texttt{AKNN+} and \texttt{AKNN++} algorithms. Note that the targets of \texttt{AKNN+} and \texttt{AKNN++} are different. \texttt{AKNN+} aims to preserve the results of {\CHENG a general} AKNN algorithm while \texttt{AKNN++} targets better efficiency. Thus, we design two different types of experiments for them. In particular, for \texttt{AKNN+}, we focus on verifying our theoretical analysis, which is discussed later in Section~\ref{subsubsec:theoretical results}, while for \texttt{AKNN++} we provide empirical advice on parameter tuning. 
%

{\CHENG Parameter $\epsilon_0$ is a critical parameter for the \texttt{ADSampling} algorithm since it directly controls the trade-off between the accuracy and the efficiency (recall that a larger $\epsilon_0$ means a smaller significance value for the hypothesis testing, which further implies a more accurate result of the hypothesis testing).}
Figure~\ref{figure:parameter} plots the QPS-recall curves of \texttt{HNSW++} (left panel) and \texttt{IVF++} (right panel) with different $\epsilon_0$. 
%
In general, we {\CHENGC observe} from the figures that with {\CHENG a larger} $\epsilon_0$, the QPS-recall curves moves lower right. This is because {\CHENG a larger} $\epsilon_0$ leads to better accuracy at the cost of efficiency.
% When $\epsilon_0$ is small 
% (e.g., $\epsilon_0=1.5, 1.8$), 
% decreasing $\epsilon_0$ would introduce unignorable accuracy loss
% and increase the efficiency. However, when it is large 
% (e.g., $\epsilon_0=2.7, 3.0$), increasing $\epsilon_0$ does not improve accuracy {\CHENG further}
% ~\footnote{{\JIANYANGB Note that $\epsilon_0$ can only control the probability that \texttt{ADSampling} successfully finds out KNN objects from the generated candidates.}}
% {\CHENG but would} decrease efficiency. 
% Thus, we suggest to set $\epsilon_0$ small when targeting low-accuracy region and to tune it around $2.1$ when targeting improving efficiency without losing much accuracy.
{\JIANYANGB We observe that when $\epsilon_0=2.1$, it introduces little accuracy loss while further increasing $\epsilon_0$ would decrease the efficiency. Thus, in order to improve the efficiency without losing much accuracy, we suggest to set $\epsilon_0$ around $2.1$.}
% nearly no extra accuracy loss. 
%
The results for \texttt{HNSW+} and \texttt{IVF+} are similar and omitted due to the page limit.

% Note that the recall 
% %(the proportion of retrieved ground truth KNNs) 
% is affected by two factors: \texttt{AKNN} algorithms and \texttt{ADSampling}, where $\epsilon_0$ controls only the second part.
% Another interesting phenomenon is about the movement of the curve when increasing $\epsilon_0$. Basically, we see from the figures that with increasing $\epsilon_0$, the QPS-recall curves in general moves lower right. This is because increasing $\epsilon_0$ leads to better accuracy at the cost of efficiency. What is interesting is that 
% {\CHENG We also observe that} when $\epsilon_0$ is small, increasing it mainly causes horizontal movement (e.g., from 1.5 (red) to 1.8 (orange)) while when it's large, the curve almost moves vertically (e.g., from 2.7 (violet) to 3.0 (purple)). Note that in both cases the increment is 0.5. This phenomenon can be clearly explained by our theoretical analysis of Lemma~\ref{theorem:ADSampling accuracy} and \ref{theorem:ADSampling efficiency}. Specifically, on one hand, with respect to $\epsilon_0$, the failure probability decays quadratic-exponentially (Lemma~\ref{theorem:ADSampling accuracy}), implying that it decays fast for small $\epsilon_0$ and slow for large $\epsilon_0$. On the other hand, the expected time complexity grows quadratically (Lemma~\ref{theorem:ADSampling efficiency}), implying that it grows slow for small $\epsilon_0$ and fast for large $\epsilon_0$. 
% As a result, when $\epsilon_0$ is small, the curve moves mainly horizontally while when it's large, it mainly moves vertically, which matches the experimental results well. 

Figure~\ref{figure:parameter gap} plots the QPS-recall curves of \texttt{HNSW++} and \texttt{IVF++} with different $\Delta_d$. 
We observe that too frequent hypothesis testing (e.g., when $\Delta_d=1$) would do harm to the performance. 
It's worth noting that a small $\Delta_d$ implies that it can terminate sampling immediately when enough information is collected, {\CHENGB but it would} require more arithmetic operations for hypothesis testing. Our empirical study shows that when $\Delta_d=16, 32, 64$, it achieves the best {\CHENGC trade-off.}
}
%Moreover, though it's discussed that increasing $\epsilon_0$ leads to better accuracy at the cost of efficiency (as a result, the curve moves towards lower right), the extent of movement on each axis is unclear. 
%An interesting phenomenon in Figure~\ref{figure:parameter} is that when $\epsilon_0$ is small, increasing it mainly causes horizontal movement, e.g. from 1.0 (red) to 1.5 (orange), while when it's large, the curve almost moves vertically, e.g. from 2.5 (blue) to 3.0 (purple). Note that in both cases the increments are 0.5. This phenomenon can be perfectly explained by our theoretical analysis in Section 5. Specifically, on the one hand, with respect to $\epsilon_0$, the failure probability decays super-exponentially (Equation~\ref{eq:superexp}), implying that it decays fast for for small $\epsilon_0$ and slow for large $\epsilon_0$. On the other hand, the expected time complexity grows quadratically (Equation~\ref{eq:quadratic}), implying that it grows slow for small $\epsilon_0$ and fast for large $\epsilon_0$. As a result, when $\epsilon_0$ is small, horizontal movement dominates the behavior of the curve while when it's large, it mainly moves vertically, which matches the experimental results well. 

%when $\epsilon_0$ is small, the \texttt{AKNN++} algorithms themselves introduce unignorable accuracy loss. However, at the same time, they also increase the efficiency at low-accuracy region, suggesting that when targeting low-accuracy region, $\epsilon_0$ 

%from the figures that when $\epsilon_0$ is small, the adaptive algorithms themselves introduce unignorable accuracy loss (about 5\% for $\epsilon_0=1.0$ and $1\%$ for $\epsilon_0=1.5$). However, they also increase the efficiency at low-accuracy region, suggesting that when targeting low-accuracy region, $\epsilon_0$ should be tuned smaller, while targeting nearly no extra accuracy loss, $\epsilon_0$ is suggested to be set around $2.0$. 

%: (1) whether an AKNN algorithm generates 

%{\JIANYANG Note that the targets of our adaptive algorithms are different. The generic ARSearch aims to preserve the semantics of an arbitrary AKNN algorithm, while the specific ARRoute and ARScan are proposed for better efficiency. We next do parameter study on ARRoute (on \texttt{HNSW}) and ARScan (on \texttt{IVF}) to provide empirical advice on parameter tuning and ARSearch (on linear scan 
%with providing ground truth $dis_{i_K^*}$ to eliminate the error caused by AKNN algorithms
%) to verify our theoretical analysis on DCO (Lemma~\ref{theorem:ADSampling accuracy} and \ref{theorem:ADSampling efficiency}). } 
%Figure~\ref{figure:parameter} plots the QPS-recall curves with different $\epsilon_0$ of \texttt{HNSW} + ARRoute (left panel) and \texttt{IVF} + ARScan (right panel). Note that the recall is controlled by two factors: 1) whether AKNN algorithms visit the ground truth KNNs 2) whether it's successfully leveled up to level $L$ and updates the KNN set $\mathcal{K}$, where $\epsilon_0$ controls only the second part. 
%We see from the figures that when $\epsilon_0$ is small, the adaptive algorithms themselves introduce unignorable accuracy loss (about 5\% for $\epsilon_0=1.0$ and $1\%$ for $\epsilon_0=1.5$). However, they also increase the efficiency at low-accuracy region, suggesting that when targeting low-accuracy region, $\epsilon_0$ should be tuned smaller, while targeting nearly no extra accuracy loss, $\epsilon_0$ is suggested to be set around $2.0$. 


{\JIANYANG
\smallskip
\noindent
\subsubsection{\textbf{{\CHENG Results for Verifying Theoretical Results}}}
\label{subsubsec:theoretical results}

We further empirically verify Lemma~\ref{theorem:ADSampling accuracy} and ~\ref{theorem:ADSampling efficiency}. 
% {\JIANYANGB Because this study is conducted for verifying theoretical analysis, we .}
As a verification study, the experimental setting is different. 
To eliminate the accuracy loss caused by AKNN algorithms, we conduct the verification study based on linear scan, which itself is an exact KNN algorithm.
{\JIANYANGB  Note that in KNN query processing, the result of the former DCOs can affect the distance thresholds of the latter DCOs, which introduces some bias into a verification study. To eliminate it, we provide a fixed distance threshold (the exact distance of the ground truth Kth NN) to make the DCOs independent with each other.}
% To focus on the problem of DCO, we also provide a fixed distance threshold (the exact distance of the ground truth Kth NN). 
To test the actual needed dimensionality, we set $\Delta_d=1$.
% do hypothesis testing after sampling every single dimension
{\JIANYANGB With this setting, the recall represents the proportion of successful DCOs for positive objects (recall that those for negative objects will never fail), and thus, it empirically reflects the success probability of a single {\CHENGC DCO with} \texttt{ADSampling}.}
The left panel of Figure~\ref{figure:verification} shows that the failure probability indeed decays following a quadratic-exponential trend and reaches near 100\% accuracy around $\epsilon_0=2$. The right panel shows that {\CHENG the number of evaluated dimensions} increases following a quadratic trend, which is slow when $\epsilon_0$ is small (when $\epsilon_0=2$, the total {\CHENG number of} evaluated dimensions {\CHENG is} around $3\%$ of {\CHENG that of} the plain \texttt{FDScanning}). 
% Note that this result is not directly comparable with Figure~\ref{figure:evaluated_dimension} due to their different experimental settings. 

%Also, note that in terms of KNN query, in our framework, the order of evaluation also affect time complexity. Let's investigate two extreme cases: (1) Candidates are given in decreasing order with respect to distance. Then all the comparisons will produce positive results. Thus, it brings no speedup. (2) Candidates are given in increasing order with respect to distance. Then we can get the ground truth $dis_{i_K}$ very soon, meaning that for the following comparisons, we are comparing their distance with the minimum possible threshold, which reaches the optimal complexity. 
}
%Recall that $\epsilon_0$ is the parameter that controls the failure probability and expected time complexity of our adaptive algorithms. Thus, increasing $\epsilon_0$ leads to higher accuracy and lower QPS (moving towards bottom right), {\color{red} which is verified by the experimental results}. 
%When $\epsilon_0$ is small ($\epsilon_0=1.0, 1.5$), the figures show that .
%Note that according to the proof of Theorem~\ref{theorem:eps} and \ref{theorem:efficiency} in Section 4, with respect to $\epsilon_0$, the failure probability decays super-exponentially and the time complexity grows quadratically, indicating that when $\epsilon_0$ is small, slightly increasing it largely reduces the error while 
