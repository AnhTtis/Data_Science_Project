

{\CHENG
% \section{\texttt{ADSampling} for Improving AKNN Algorithms}
% \label{sec:adsampling-aknn}
\section{\texttt{AKNN+}: Improving AKNN Algorithms with \texttt{ADSampling} as a Plug-in Component}
\label{sec:aknn+}



% \subsection{\texttt{ADSampling} for a General AKNN Algorithm: As a Plug-in Component}
% \subsection{\texttt{AKNN+}: AKNN Algorithms with \texttt{ADSampling} as a Plug-in Component}
\label{subsec:adsampling-any-aknn}
Recall that an AKNN algorithm, which we denote by \texttt{AKNN} and could be any one among many existing algorithms~\cite{malkov2018efficient, jegou2010product, muja2014scalable, fu2019fast, datar2004locality}, involves many DCOs. 
% Each operation is to decide whether an object $\mathbf{o}$ has its distance $dis$ from a query $\mathbf{q}$ smaller than a threshold $r$, and if so, returns $dis$. 
% The AKNN algorithm typically conducts each operation by computing $dis$ exactly, which runs in $O(D)$ time. 
In the literature, \texttt{FDScanning} is typically adopted for DCOs and runs in $O(D)$ time.
Given that \texttt{ADSampling} can conduct 
% DCOs with lower time complexities
% than $O(D)$ 
% and high {\JIANYANG success} probabilities, 
% DCOs with better cost-effectiveness,
{\CHENGB reliable DCOs with better efficiency,}
a natural idea is to improve the AKNN algorithms by adopting \texttt{ADSampling} for the DCOs.
% in a \emph{plug-in} manner. 
{\JIANYANG Specifically,} since \texttt{ADSampling} is based on randomly transformed data vectors and query vectors, before any query comes, we randomly transform all data vectors, and when a query comes, we randomly transform the query vector. Then, we run the AKNN algorithm based on the transformed data and query vectors. Recall that the time cost of transforming the data vectors can be amortized across different queries and the time cost of transforming the query vector can be amortized across many different DCOs involved for answering the query. During the running process of the AKNN algorithm, whenever it conducts a DCO, we use the \texttt{ADSampling} method. For example, for graph-based methods such as \texttt{HNSW}, we use the \texttt{ADSampling} method when comparing {\CHENGC the distance of} a newly visited object with the maximum in the result set $\mathcal R$. 
% For quantization-based methods such as 
{\chengf For other AKNN algorithms such as} \texttt{IVF}, we apply \texttt{ADSampling} when {\CHENGB comparing the distance of a candidate and the maximum in the currently maintained KNN set $\mathcal K$} for selecting the final KNNs from the generated candidates.
% we compare it with the maximum in the currently maintained KNN set $\mathcal K$.

For an AKNN algorithm \texttt{AKNN}, which adopts \texttt{ADSampling} for DCOs, we call it \texttt{AKNN+}. For example, we call \texttt{HNSW} and \texttt{IVF} with \texttt{ADSampling} adopted for DCOs \texttt{HNSW+} and \texttt{IVF+}, respectively. 

{\JIANYANG
\smallskip\noindent\textbf{{\CHENGB Theoretical Analysis}.} 
Recall that \texttt{ADSampling} improves the efficiency of DCOs on negative objects at the cost of the accuracy of those on positive objects. 
% Therefore, there exists a tradeoff between the time complexity {\CHENG (of negative objects)} and the failure probability {\CHENG (of positive objects)}. 
% Thus, it is necessary to analyze the them together instead of individually. 
% Recall that as analyzed in Section~\ref{sec:problem setting}, the time cost of an AKNN algorithm is dominated by DCOs, in particular of negative objects. 
% With replacing \texttt{FDScanning} with \texttt{ADSampling}, we reduce the time consumption of DCOs of negative objects while keep that of the remaining computation. Thus, here we focus on analyzing the cost and gains of DCOs. 
% We first investigate when an \texttt{AKNN+} algorithm would fail to return the same results as its corresponding \texttt{AKNN} algorithm does. 
% Note that we only change the DCO, so if all \texttt{ADSampling} produce correct results, the final results would be preserved. 
{\CHENGB We 
% of the time-accuracy tradeoff of \texttt{AKNN+}.
show the relationship between the probability that \texttt{AKNN+} fails to return the same results as \texttt{AKNN} and the time complexity of the DCO on a negative object involved in \texttt{AKNN+} below.} 
{\JIANYANGB Basically, to preserve the returned results of \texttt{AKNN}, it suffices to produce correct results for all DCOs, whose number is at most $N$. Then with union bound, the failure probability of \texttt{AKNN+} is upper {\CHENG bounded} by the sum of the failure probability of each single DCO. Thus, making the failure probability of \texttt{ADSampling} be $\delta = \delta' / N$ yields the following corollary.}
\begin{corollary}
Let $\delta'$ be the probability that \texttt{AKNN+} fails to return the same results as \texttt{AKNN}. The expected time complexity of the DCO on a negative object with distance gap $\alpha$ is reduced to
\begin{align}
    \mathbb{E} \left[ \hat D \right]  = O \left[ \min \left( D, \frac{1}{\alpha ^2} \log \frac{DN}{\delta'}  \right)  \right] 
\end{align}
and the remaining time cost ({\CHENGC for} DCOs on positive objects and other {\CHENGC computations}) is unchanged.
\end{corollary}
% \begin{proof}
% % As analyzed above, i
% In order to preserve the returned results of \texttt{AKNN}, it's sufficient to produce correct results for all DCOs, whose number is at most $N$.
% % \footnote{It's worth noting that as analyzed in Lemma~\ref{lemma:ADSampling accuracy}, the failure probability of negative objects is 0, so it's sufficient to apply union bound on positive objects only, whose number is supposed to be $O(K \log N)$ as analyzed in Section~\ref{sec:problem setting}. Because the time complexity is logarithmically dependent on it, which makes very minor difference, for conciseness, we present the corollary with union bound on $N$ rather than $O(K \log N)$.}. 
% Then with union bound, the failure probability of \texttt{AKNN+} is upper {\CHENG bounded} by the sum of failure probability of each single DCO. Thus, it suffices to make the failure probability of \texttt{ADSampling} be $\delta = \delta' / N$. Then with Theorem~\ref{theorem:time-accuracy of ADSampling}, we have this corollary. 
% \end{proof}

Furthermore, for those \texttt{AKNN+} algorithms which generate candidates all at once (e.g., {\CHENGC{\texttt{IVF}}}), 
% we apply \texttt{ADSampling} when finding out KNNs among the generated candidates. 
producing correct DCO results for KNN objects ($K$ objects) rather than for all objects (at most $N$ objects) is sufficient to return the same results as its corresponding \texttt{AKNN} algorithm. This is because once we produce correct results for {\CHENGB the DCOs on} the true KNN objects, we also obtain their exact distances {\CHENGC (note that all of them would be positive objects)}. It ensures to return them as the final answers. Thus, we have the following corollary.

\begin{corollary}
Let $\delta'$ be the probability that \texttt{AKNN+} fails to return the KNNs of the generated candidates. The expected time complexity of DCO on a negative object with distance gap $\alpha$ is reduced to
\begin{align}
    \mathbb{E} \left[ \hat D \right]  = O \left[ \min \left( D, \frac{1}{\alpha ^2} \log \frac{DK}{\delta'}  \right)  \right] 
\end{align}
and the remaining time cost ({\CHENGC for} DCOs on positive objects and other {\CHENGC computations}) is unchanged.
\label{corollary: find out KNNs}
\end{corollary}

}
% \smallskip\noindent\textbf{Failure Probability Analysis.} We investigate the probability that an \texttt{AKNN+} algorithm would fail to return the same results as \texttt{AKNN} does. We bound this probability with \emph{union bound}. That is, the probability that \texttt{AKNN+} fails is at most the sum of the failure probabilities of the DCOs for positive objects (recall that those for negative objects are equal to 0). Specifically, we have the following result of the failure probability of \texttt{AKNN+}.

% \begin{corollary}
% The probability that \texttt{AKNN+} fails to return the same results as \texttt{AKNN} is at most $\exp \left( -c_0 \epsilon_0^2 + \log (DK\log N) \right)$. %where $N_{pos}$ is the number of DCOs for positive objects.
% \label{theorem: AKNN+ failure probability}
% \end{corollary}
% \begin{proof}
% With the union bound over all $O(K \log N)$ positive objects and Lemma~\ref{theorem:ADSampling accuracy}, we have this corollary.
% \end{proof}
% \smallskip\noindent\textbf{Time Complexity Analysis.} We analyze the time complexity of \texttt{AKNN+}. Its time cost consists of the cost of conducting DCOs with \texttt{ADSampling}, which we denote by $C_1$ and that of the rest computations, which we denote by $C_2$. We first analyze $C_1$. Let $N_s$ be the number of candidates considered by the \texttt{AKNN+} algorithm and we have $N_s \le N$. We know that there would be $N_s$ DCOs. Recall that the time complexity of \texttt{ADsampling} for a DCO for an object with a distance $dis$ and a threshold $r$ is $O(D)$ if the object is a positive one and $O(\min(D, \frac{1}{\alpha^2}\log \frac{D}{\delta}))$ if the object is a negative one, where $\alpha = (dis-r)/r$ and $\delta$ is the failure probability of {\JIANYANG a single} \texttt{ADSampling}. Furthermore, we verify that among the $N_s$ DCOs, $O(K\log N)$ operations are for positive objects and $O(N_s)$ operations are for negative objects (for which the proof is provided in an appendix). We then analyze $C_2$. $C_2$ is different for different \texttt{AKNN+} algorithms. For example, for \texttt{HNSW+}, $C_2$ is $O(N_s \log N_s)$, and for \texttt{IVF+}, $C_2$ is $O(N_s \log K)$. In summary, we provide the time complexity results of \texttt{AKNN+} below.

% \begin{corollary}
% Let $\delta'$ be the probability that \texttt{AKNN+} fails to return the same results as \texttt{AKNN}. The time complexity of \texttt{AKNN+} is $O(C_1 + C_2)$. 
% % The expected time complexity of \texttt{AKNN+} is 
% For $C_1$, its expectation is as follows.
% \begin{align}
%     \mathbb{E} \left[ C_1 \right] = O \left( K \log N \cdot D +  N_s \cdot c_\alpha \log \frac{DK\log N}{\delta'} \right) 
%     % \mathbb{E} \left[ C_1 \right] = O \left( DK \log N + \min\{N_s D, N_s \cdot \bar{\alpha}^{-2} \log \frac{DK\log N}{\delta'}\} \right) 
% \end{align}
% where $c_\alpha  =  \frac{1}{N_{s}} \sum_{i=1}^{N_{s}} \min \left( \alpha_i^{-2}, D / \log \frac{DK \log N}{\delta'}  \right)$ and $\alpha_i$ is the $alpha$ value of the $i^{th}$ candidate object. 
% % where $\bar{alpha}$ is the average of the $\alpha$'s of the negative objects.
% For $C_2$, it is dependent on the specific \texttt{AKNN+} algorithm. 
% For \texttt{HNSW+}, $C_2$ is $O(N_s \log N_s)$, and for \texttt{IVF+}, $C_2$ is $O(N_s \log K)$. 
% %
% In a simplified form, the total time complexity of \texttt{HNSW+} is $O( \log N\cdot D + N_s \cdot c_{\alpha} \log D + N_s \log N_s)$, where we treat $K$ and $\delta'$ as constants.
% % ***In this theorem, we can define $\delta'$ as the failure probability for \texttt{AKNN+}, which should be based on $\delta$.***
% \label{theorem:AKNN+ time-accuracy}
% \end{corollary}
% \begin{proof}
% {\JIANYANG To make the failure probability of \texttt{AKNN+} provided in Theorem~\ref{theorem: AKNN+ failure probability} be $\delta'$, it suffices to make the failure probability of a single \texttt{ADSampling} be no greater than $\delta = \frac{\delta'}{K \log N}$.
% }
% % Let the failure probability of \texttt{AKNN+} provided in Theorem~\ref{theorem: AKNN+ failure probability} be $\delta'$. The failure probability of a single DCO should be no greater than $\delta = \delta' / (K\log N)$. 
% Then with Lemma~\ref{theorem:ADSampling efficiency}, we obtain the complexity for each DCO on a negative object: $\mathbb{E} \left[ \hat D \right] = 
% % \min\left( D, \alpha^{-2} \log (DK\log N)/\delta' \right) $
% O \left( \min\left( D, \alpha^{-2} \log \frac{DK\log N}{\delta'} \right) \right)  $. Gathering the cost of positive objects and negative objects, we prove the theorem. 
% \end{proof}

% \smallskip\noindent\textbf{\texttt{AKNN+} v.s. \texttt{AKNN}.}
% % Recall that the time complexity of \texttt{AKNN} is 
% Recall that \texttt{AKNN} has $C_1$ as $O(N_s D)$, which is normally significantly larger $C_2$.
% %
% \texttt{AKNN+} improves \texttt{AKNN} by reducing the most costly part $C_1$ to $O \left( K \log N \cdot D +  N_s \cdot c_\alpha \log \frac{DK\log N}{\delta'} \right)$ (in expectation). 

% Compared with \texttt{AKNN}, \texttt{AKNN+} has the time complexity of $C_1$ (which is significantly larger than $C_2$) 
% Our algorithm improves each DCO of negative objects from $O(D)$ to $O(c_\alpha \log \frac{DK\log N}{\delta'} )$. 

% ***Better to make some comments on the comparison between \texttt{AKNN}'s time complexity and \texttt{AKNN+}'s and highlight the improvement here.***
}

