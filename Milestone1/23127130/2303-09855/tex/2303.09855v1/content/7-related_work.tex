\section{Related Work}
\label{sec:related work}

\noindent
\textbf{Approximate K Nearest Neighbor Search.} 
Existing AKNN algorithms can be categorized into four types: 
% {\JIANYANGREVISION tree-based ~\cite{muja2014scalable, dasgupta2008random, ram2019revisiting, beygelzimer2006cover, reviewer_M_tree}, graph-based~\cite{malkov2018efficient, NSW, li2019approximate, fu2019fast, fu2021high, SISAP_graph}}, quantization-based~\cite{jegou2010product, ge2013optimized, guo2020accelerating, ITQ, additivePQ, imi} and hashing-based~\cite{indyk1998approximate, datar2004locality, c2lsh, tao2010efficient, huang2015query, sun2014srs, lu2020vhp, zheng2020pm} algorithms. 
{\JIANYANGREVISION (1) graph-based~\cite{malkov2018efficient, NSW, li2019approximate, fu2019fast, fu2021high, SISAP_graph}}, (2) quantization-based~\cite{jegou2010product, ge2013optimized, guo2020accelerating, ITQ, additivePQ, imi}, (3) {\JIANYANGREVISION tree-based ~\cite{muja2014scalable, dasgupta2008random, ram2019revisiting, beygelzimer2006cover, reviewer_M_tree}} and (4) hashing-based~\cite{indyk1998approximate, datar2004locality, c2lsh, tao2010efficient, huang2015query, sun2014srs, lu2020vhp, zheng2020pm, james_cheng}.
In particular, graph-based methods show superior performance for in-memory AKNN query. 
Quantization-based methods are powerful when memory is limited. Hashing-based methods provide rigorous theoretical guarantee. 
We refer readers to recent tutorials~\cite{tutorialThemis, tutorialXiao}, {\JIANYANGREVISION reviews and benchmarks~\cite{li2019approximate, annbenchmark, SISAP_benchmark, reviewer_paper, graphbenchmark}} for details. 
{\JIANYANG
There are also plentiful {\CHENG studies, which apply}
% works `
machine learning (ML) to accelerate AKNN~\cite{learning2route2019ml, reinforcement2route, adaptive2020ml, Dong2020Learning}. \cite{learning2route2019ml, reinforcement2route} apply reinforcement learning in graph routing which substitutes greedy beam search. \cite{adaptive2020ml} learns to early terminate searching. \cite{Dong2020Learning} uses ML to construct an index structure. Note that all above methods apply ML for candidate generation. 
{\CHENG These ML-based methods are orthogonal to our techniques} and our techniques can {\JIANYANGB help them with} 
% enhance these methods {\CHENG in their re-ranking phase for}
finding KNNs among the generated candidates.
}

% \smallskip
% \noindent
% \textbf{Distance Approximation.} There are two main forms of distance approximation that have been used for AKNN: (1) product quantization~\cite{ge2013optimized, jegou2010product,ITQ, additivePQ, guo2020accelerating} and (2) dimension reduction, in which dimension reduction can be further categorized into optimization-based dimension reduction~\cite{wold1987principal, kruskal1964multidimensional} and random projection~\cite{johnson1984extensions, blockjlt, fftjlt}. The above methods can produce approximate distances, but they cannot help with DCO. Specifically, product quantization and optimization-based dimension reduction optimize a compressed representation to minimize the total approximation error instead of the maximum one, which fails to guarantee an error bound. Though random projection provides probabilistic guarantee on the error bound, existing methods only support a fixed resolution. Thus, for the DCOs with large distance gap, it can be redundant, while for those with small distance gap, it might not be enough. It's worth noting that hashing-based methods~\cite{indyk1998approximate, datar2004locality, c2lsh, tao2010efficient, huang2015query, lu2020vhp} are also popular in AKNN for data compression. They target to map close vectors into similar hashing code and use code comparison as a proxy of DCO. However, it does not explicitly approximate distances and thus cannot help with DCO. Consequently, since 
% the methods above cannot help with DCO, in AKNN query, they can be used only in the first stage (i.e., generating candidates), but not the second (finding out KNNs among generated candidates).

\smallskip
\noindent
\textbf{Random Projection {\CHENGB for AKNN}.} 
% The seminal work of random projection is the Johnson-Lindenstrauss Lemma~\cite{johnson1984extensions}.
% % , {\CHENGB which shows that the difference between the distance random projection provides probabilistic guarantee on the error bound}. 
% % In a recent work~\cite{larsen2017optimality}, it was proved to be theoretically optimal. 
% A series of studies~\cite{larsen2017optimality,optimalJL, optimalJL2} prove its optimality in theory. 
% For AKNN query, besides dimension reduction, 
{\CHENGB While random projection can hardly be used for reliable DCOs during the phase of re-ranking candidates of KNNs as explained and verified earlier, it has been} widely applied in LSH~\cite{indyk1998approximate, datar2004locality, c2lsh, tao2010efficient, sun2014srs, huang2015query, lu2020vhp} and random partition/projection tree~\cite{ram2019revisiting, dasgupta2008random} {\CHENGB during  the phase of generating candidates of} KNNs. 
{\CHENGB Our study differs from these studies in (1) we project different objects to vectors with different dimensions flexibly while these studies project all objects to vectors with equal dimensions; (2) we set the number of dimensions of a projected vector for an object automatically based on its DCO via hypothesis testing while these studies need to set the number with manual efforts; and (3) we use the projected vectors (in DCOs) during the phase of 
% re-ranking 
{\JIANYANGB finding out KNNs from the generated}
candidates while these studies use the projected vectors during the phase of generating candidates. Therefore, these studies are orthogonal to our study.}
% Most of these methods guarantee to return $(\delta,c)$-approximate AKNNs (i.e., with at most $\delta$ failure probability, the distances of the returned objects are at most $c$ times larger than those of the ground-truth KNNs), where $c>1$ {\CHENGB holds} strictly. Note that since $c > 1$ {\CHENGB holds} strictly, these methods have no guarantee to return the ground-truth KNNs. The only exception is the SRS-2 algorithm in \cite{sun2014srs}, {\CHENGB which} can make $c=1$, but does not guarantee to have better time complexity than the brute force linear scan. {\CHENGB In contrast, our \texttt{ADSampling} technique}, when combined with an exact KNN algorithm, would return $(\delta,1)$-approximate algorithm,~\footnote{Our methods also support $(\delta,c)$-approximate query by trivially replacing $(1+\epsilon_0/\sqrt {d} )$ with $(1+\epsilon_0/\sqrt {d} )/c$ in hypothesis testing. However, when recall is the main concern, we don't suggest to do so because such $c$-approximation brings slight acceleration but disastrously decreases recall.} which has its time complexity significantly better than the brute force linear scan (Corollary~\ref{corollary: find out KNNs}). 

{\JIANYANGREVISION 
\smallskip
\noindent
\textbf{Dimension Sampling {\CHENGB for AKNN}.} 
We notice that a MAB (multi-armed bandit)-oriented approach~\cite{dimension_sampling_aistat} also applies dimension sampling and claims logarithmic complexity. Our study is different from \cite{dimension_sampling_aistat} in the following aspects. Problem-wise, \cite{dimension_sampling_aistat} targets the AKNN problem itself and aims to find a superset of the set containing the KNNs. It is non-trivial to adapt \cite{dimension_sampling_aistat} to DCOs (the focus of our paper). Theory-wise, \cite{dimension_sampling_aistat}'s logarithmic complexity relies on some strong assumptions on the data (which may not hold in practice) while ours relies on no assumptions. Technique-wise, (1) \cite{dimension_sampling_aistat} samples the original vectors directly while ours first randomly transforms the vectors and then samples the transformed vectors. Our way has the advantage that the error bound of an approximate distance is based on the concentration inequality of random projection and does not rely on any assumptions as \cite{dimension_sampling_aistat} does; and (2) \cite{dimension_sampling_aistat} uses some lower/upper bounds to determine the number of sampled dimensions while ours uses sequential hypothesis testing. Our way has no false positives while \cite{dimension_sampling_aistat} has both false positives and false negatives. In summary, \cite{dimension_sampling_aistat} and our work only share a high-level idea of dimension sampling and differ in many aspects including problem, theory and technique.
% , which we shall discuss in a revision.
}
% Many other works~\cite{fftjlt, blockjlt} target to construct structured random matrices to accelerate Johnson-Lindenstrauss transformation. 
% Random projection was also utilized in nearest neighbor search in the scheme of dimension reduction~\cite{fftjlt}, Locality Sensitive Hashing(LSH)~\cite{datar2004locality, c2lsh} and random partition/projection tree~\cite{ram2019revisiting, dasgupta2008random}. 

%With the necessity of guaranteed error bound, we revisit random projection due to its plentiful theoretical results~\cite{johnson1984extensions,  larsen2017optimality, fftjlt, datar2004locality, blockjlt}. Random projection is a famous technique for low-distortion metric embedding. It provides theoretical guarantee on error bound in a probablistic manner. To be specific, the seminal work of random projection, Johnson-Lindenstrauss Lemma~\cite{johnson1984extensions} (JL Lemma), states that with the probability of $1-\delta$, random projection preserves mutual distance among a finite set $\mathcal X$ of $N$ objects with at most $\epsilon$ multiplicative error when reducing dimensionality to $\Theta(\epsilon^{-2} \log \frac{N}{\delta})$. Though random projection was utilized in nearest neighbor search in the scheme of dimension reduction~\cite{fftjlt, blockjlt}, Locality Sensitive Hashing(LSH)~\cite{datar2004locality, c2lsh} and random partition/projection tree~\cite{ram2019revisiting, dasgupta2008random}, existing works always evaluate a fixed-sized code during query. Also, though random projection guarantees the error bound, it has no guarantee on the result of an arbitrary distance comparison. We propose a novel scheme originated from the core lemma of JL Lemma, i.e. concentration inequality, but make it flexible, adaptive and recoverable. 
%\textbf{Random Projection}Extensive studies have been conducted on both applications and theories of random projection. Since the seminal work of Johnson-Lindenstrauss Lemma~\cite{johnson1984extensions} in 1984, 

% \textbf{Flexibility and Adaptivity} In terms of flexibility of resolution, \cite{lu2021hvs} proposed a method comprised of multi-granular quantization, which is motivated by the heuristics that initial search steps require coarser distance. It achieves flexibility of resolution but not adaptivity to a particular query. \cite{adaptive2020ml} proposed to do early termination with machine learning models, i.e. adaptively determining the number of objects to visit, which is orthogonal to our work of adaptive resolution. Sequential hypothesis testing~\cite{wald1945sequential, Berger2017sequential} is a historical topic in statistics, with which practitioners keep sampling until a significant conclusion can be obtained, making the number of samples adaptive. It is especially meaningful for the scenarios where sampling is expensive, e.g. clinical analysis~\cite{korosteleva2008clinical}. This technique was also applied in problems in computer science including network intrusion detection~\cite{network}, pattern matching~\cite{computervision} and LSH for all-pairs similarity search~\cite{satuluri2011bayesian, sequentialLSH}. Note that the main difference of sequential hypothesis testing under different scenarios is how a statistical problem is formulated. Our formulation is based on concentration inequality of random projection for Euclidean space, which is totally different from that of \cite{satuluri2011bayesian, sequentialLSH}.

%Recently, a hybrid method of graph and quantization, HVS~\cite{lu2021hvs} utilized multi-granular quantization, which is motivated by the heuristics that initial search steps require coarser distance. 

%Flexibility and adaptivity of distance resolution are seldom researched under the context of high-dimensional nearest neighbor search.  However, the list size and code size at each stage are preset hyper-parameters, so it's very limited in flexibility and cannot adapt to a particular query. \cite{lu2021hvs} proposed a method comprised of multi-granular quantization, which is motivated by the heuristics that initial search steps require coarser distance. It achieves flexibility of resolution but not adaptivity to a particular query. Last but not least, to the best of our knowledge, no existing methods can recover exact distance without evaluating raw vectors from scratch.  %\cite{adaptive2020ml} proposed to adaptively early terminate ANN search with machine learning. The resolution of its representation is fixed, so it's orthogonal to our work. 

%topology to navigate ANN search, while compression-based ones accelerate distance computation with short code generated by quantization~\cite{jegou2010product, ge2013optimized, guo2020accelerating}, hashing and dimensionality reduction. 

%Random projection is a technique widely used in nearest neighbor search~\cite{achlioptas2003database} {\color{red} cite}, kernel approximation~\cite{choromanski2017unreasonable, yu2016orthogonal} {\color{red} cite} and machine learning{\color{red} cite}. 

%Adaptive methods for KNN query - machine learning for database - SIGMOD 2020, Tao arxiv 2022, unlike these two works, our work depends on adaptation instead of prediction.

% {\JIANYANG
% \subsection{\textbf{Remarks}}
% %\subsubsection{ {\color{red} \textbf{Remarks}} } 
% %{\color{red} I think we'll need to clarify our contribution here? need discussion}

% We emphasize that ADSampling is a highly coupled framework. Some of its components were individually applied in many other AKNN algorithms, but only our method linked them up associatedly:

% \underline{Guaranteed error bound of random projection} was widely applied in LSH~\cite{datar2004locality, c2lsh, huang2015query} and random projection/partition tree~\cite{dasgupta2008random, ram2019revisiting} to guarantee the quality of returned AKNNs. Specifically, they guarantee that their returned AKNNs are $(\delta,c)$-approximate, i.e., with at most $\delta$ failure probability, the distances of the returned KNN objects are at most $c$ times larger than those of the ground-truth KNNs, where $c$ must be strictly greater than $1$ and empirically not so small~\cite{datar2004locality}. Our method, when combined with an exact KNN algorithm, instead provides the guarantee of $(\delta,1)$-approximation which cannot be achieved by the aforementioned algorithms~\footnote{Our method also support $(\delta,c)$-approximate query by trivially replacing $\gamma(d)$ with $\gamma_c(d):=\gamma(d)/c$. However, when recall is the main concern, we don't suggest to do so because such $c$-approximation brings slight acceleration while disastrously decrease recall (data not shown). }. \cite{sun2014srs} uses the guarantee for early termination at the re-ranking stage, which is similar to our hypothesis testing. Its SRS-2 algorithm also provides the $(\delta, 1)$-approximation guarantee. However, it lacks of flexibility, adaptivity and recoverability. Also it was not made a plugin for other AKNN algorithms.

% \underline{Sequential hypothesis testing} is a historic topic in statistics~\cite{wald1945sequential}. In AKNN-related problems, it was used in LSH for all-pairs similarity search under non-Euclidean metrics~\cite{sequentialLSH, satuluri2011bayesian}. Note that the main difference of sequential hypothesis testing in different scenarios is how a sampling problem is formulated. Our formulation is based on concentration inequality of random orthogonal projection of 
%$\ell_2$ norm 
% Euclidean space, where samples (dimensions) are \textbf{not} independent with each other (because the rows of random orthogonal transformation are correlated to each other), which is totally different from the independent sampling of binary hashing code in \cite{sequentialLSH, satuluri2011bayesian}.

% \underline{Random orthongoal transformation} was widely used in quantization~\cite{jegou2010product, ITQ} to balance the variance among all dimensions heuristically. However, none of these works investigate its properties quantitatively (e.g., concentration inequality), which is, however, essential in our framework.
% }

% \smallskip 
% \noindent\textbf{(2) Insufficiency of existing distance approximation methods for DCOs.}
% \subsubsection{Insufficiency of existing distance approximation methods for DCOs.}
% Unfortunately, no existing distance approximation methods satisfy all of the four aforementioned desiderata, to the best of our knowledge. 
% While there there quite a few distance approximation methods that have been developed for AKNN, none of them satisfy all of the four aforementioned desiderata, to the best of our knowledge.
% There are two main forms of distance approximation that have been used for AKNN: 
% % nearest neighbor search: 
% 1) quantization (QT)~\cite{ge2013optimized, jegou2010product,ITQ, additivePQ, guo2020accelerating}, and 2) dimension reduction, in which dimension reduction can be further categorized into optimization-based dimension reduction (DR)~\cite{wold1987principal, kruskal1964multidimensional} and random projection (RP)~\cite{johnson1984extensions, blockjlt, fftjlt}.
% {\CHENG Unfortunately, none of these existing methods has all of the aforementioned four desiderata} (a summary is presented in Table~\ref{tab:freq}). 
% First, QT and DR (e.g., PCA~\cite{wold1987principal}) optimize a compressed representation to minimize the \emph{total} approximation error instead of the \emph{maximum} one, which fails to guarantee an error bound. {\CHENG Only RP provides some probabilistic error bounds.
% Second, DR and RP do not support flexible resolutions of approximate distances.
% % Flexible resolution and is a long but implicitly adopted strategy in QT. 
% Only QT supports flexible resolutions to some extent with a three-stage strategy \cite{jegou2010product, imi, surveyl2hash}:}
% % to apply different resolution on different objects: 
% 1) generate candidate lists with coarse code; 2) shrink the list with finer code; 3) re-rank the list with exact distance~\footnote{Some stages could be skipped according to specific requirements, e.g., memory constraint~\cite{johnson2019billion, jegou2011searching}.}. 
% {\CHENG Nevertheless, the list size and code size at each stage are preset hyper-parameters and fixed for all queries, and thus QT provides very limited flexibility only.}
% % so it's very limited in flexibility and hard to tune. 
% Third, to the best of our knowledge, no existing methods achieve adaptivity and recoverability on resolution of distance for high-dimensional nearest neighbor search. It's worth noting that hashing-based methods~\cite{indyk1998approximate, datar2004locality, c2lsh, tao2010efficient, huang2015query, lu2020vhp} are also popular for data compression. They target to map close vectors to similar hash codes and {\CHENG use code comparison as a proxy of DCO}.
% % DCO with hash code comparison. 
% However, hashing 
% %doesn't 
% does not explicitly approximate distances, and thus they're not within the scope of our discussion.  \footnote{Also, since different vectors may be mapped to the same code, hashing cannot 
% % identify their order.
% {\CHENG help with exact DCO}.
% }
% {\JIANYANG We also emphasize that hashing and quantization cannot provide guarantee for DCO, and thus can be used only in the first stage of AKNN query, i.e., generating candidates but not the second.}