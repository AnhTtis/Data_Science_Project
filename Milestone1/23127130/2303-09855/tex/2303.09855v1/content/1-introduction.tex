\section{Introduction}
\label{sec:introduction}

%{\color{red} rename to get a good abbreviation}


%{\color{red} High dimensional KNN is a query that cannot leave with specific query and data.}
K nearest neighbor (KNN) search in the high-dimensional Euclidean {\JIANYANGREVISION vector} space
% , which is to find the top-K nearest neighbors in a database for a given query object/vector, 
is a fundamental problem and has {\CHENGB a wide range of} applications in information retrieval~\cite{KNNimageretrieval}, data mining~\cite{KNNClassficiation} and recommendations~\cite{KNNrecommendation}. However, due to the curse of dimensionality~\cite{indyk1998approximate}, exact KNN query usually requires unacceptable response time. To achieve better time and accuracy tradeoff, many researchers turn to its relaxed version, {\CHENGB namely} approximate K nearest neighbor (AKNN) search~\cite{datar2004locality, muja2014scalable, jegou2010product, malkov2018efficient, ge2013optimized, guo2020accelerating}.

%K Nearest neighbor (KNN) search in high-dimensional Euclidean space, the query of finding the top-K nearest neighbors in a database for a given query object/vector, is a fundamental problem and has wide applications in information retrieval~\cite{KNNimageretrieval}, data mining~\cite{KNNalgorithm} and recommendations~\cite{KNNrecommendation}.
%However, exact KNN query in high-dimensional space is not easy. One source of difficulty is the complicated structure of high-dimensional space, which disables efficient and safe pruning
% indicating that one should usually 
%{\CHENG and makes it often necessary to scan a large number of objects in the database}. Another, also a more unavoidable one, {\CHENG is the expensive cost of evaluating the distances between the scanned objects and the query object, e.g., for each scanned object, it requires to iterate over all dimensions (note that there can be hundreds or even thousands dimensions in high-dimensional space).}
% huge time consumption of distance evaluations, e.g.,even for a single object, distance evaluation scan hundreds of dimensions, which takes up most of the query time. 
%As a result, exact KNN query usually requires unacceptable response time~\cite{indyk1998approximate}. To achieve better time and accuracy tradeoff, many researchers turn to its relaxed version, approximate K nearest neighbor (AKNN) search~\cite{datar2004locality, muja2014scalable, jegou2010product, malkov2018efficient, ge2013optimized, guo2020accelerating}.

%However, due to the curse of dimensionality, exact NN query needs to evaluate distance for a large amount of high-dimensional vectors, which leads to unacceptable response time. Thus, its relaxed version, approximate nearest neighbor(ANN) query, is studied for better time and accuracy tradeoff. 

%%%Literature of AKNN algorithms and introduction of the DCO

{\CHENG Many algorithms have been developed for AKNN, including 
% (1) graph-based~\cite{malkov2018efficient, li2019approximate, FuNSG17, fu2019fast, diskann, NSW}, 
% remove identical reference
% {\JIANYANGREVISION (1) graph-based~\cite{malkov2018efficient, li2019approximate, fu2019fast, diskann, NSW}, }
% (2) quantization-based~\cite{ge2013optimized, jegou2010product, ITQ, guo2020accelerating}, (3) tree-based~\cite{muja2014scalable, dasgupta2008random, ram2019revisiting, beygelzimer2006cover}, and (4) hashing-based~\cite{indyk1998approximate, datar2004locality, c2lsh, tao2010efficient, huang2015query, sun2014srs, lu2020vhp, zheng2020pm}. 
{\JIANYANGREVISION (1) graph-based~\cite{malkov2018efficient, NSW, li2019approximate, fu2019fast, fu2021high, SISAP_graph}}, (2) quantization-based~\cite{jegou2010product, ge2013optimized, guo2020accelerating, ITQ, additivePQ, imi}, (3) {\JIANYANGREVISION tree-based ~\cite{muja2014scalable, dasgupta2008random, ram2019revisiting, beygelzimer2006cover, reviewer_M_tree}} and (4) hashing-based~\cite{indyk1998approximate, datar2004locality, c2lsh, tao2010efficient, huang2015query, sun2014srs, lu2020vhp, zheng2020pm, james_cheng}.
% (1) graph-based~\cite{malkov2018efficient, NSW, li2019approximate, fu2019fast, fu2021high, SISAP_graph}, (2) quantization-based~\cite{jegou2010product, ge2013optimized, guo2020accelerating, ITQ, additivePQ, imi}, (3) tree-based ~\cite{muja2014scalable, dasgupta2008random, ram2019revisiting, beygelzimer2006cover, reviewer_M_tree} and (4) hashing-based~\cite{indyk1998approximate, datar2004locality, c2lsh, tao2010efficient, huang2015query, sun2014srs, lu2020vhp, zheng2020pm}.
{\JIANYANGREVISION In particular, we focus on in-memory AKNN algorithms which assume that all raw data vectors and indexes can be hosted in the main memory~\cite{malkov2018efficient, li2019approximate, fu2019fast,NSW, jegou2010product, muja2014scalable, beygelzimer2006cover, zheng2020pm}.}
These algorithms all adopt the strategy of first generating {\JIANYANGLAST some} candidates {\JIANYANGLAST for} KNNs and then finding {\JIANYANGLAST out} the KNNs among 
% the candidates
{\JIANYANGLAST them}.~\footnote{Graph-based methods generate candidates and find {\JIANYANGLAST out} the KNNs among the candidates generated so far \emph{iteratively}.} \underline{First}, they differ in their ways of generating candidates of KNNs. 
For example, graph-based methods organize the vectors with a graph and conduct a heuristic-based search (e.g., greedy search) on the graph for generating candidates.
% \emph{dynamically} 
% during the search process. 
% Quantization-based methods compute some quantization codes of the data vectors (e.g., cluster IDs) and generate the candidates based on their quantization codes. Tree-based methods organize the vectors with a tree structure, perform a traversal from the root to some leaf nodes, and consider the data vectors in the leaf nodes as candidates. Hashing-based methods
%~\cite{indyk1998approximate, datar2004locality, c2lsh, tao2010efficient, huang2015query, sun2014srs, lu2020vhp, zheng2020pm} compute hash codes of data vectors and the query and generate the candidates based on their hash codes. 
\underline{Second}, these algorithms largely share their ways of finding KNNs among the candidates. 
Specifically, they maintain a KNN set $\mathcal Q$~\footnote{In graph-based methods, the size of $\mathcal Q$ is set to be an integer $N_{ef} > K$ since the the distance of the $N_{ef}$th 
% $$N_{ef}^{th}$ 
NN is required for generating candidates. In other AKNN methods, the size of $\mathcal Q$ is set to be $K$.} (technically, a max-heap), and for a new candidate, they check whether its distance {\JIANYANGLAST {\CHENGC from the query}} is 
% at most
{\JIANYANGLAST no greater than}
% the maximum in $\mathcal Q$. If so, they include the candidate to $\mathcal Q$ with the distance as a key;~\footnote{{\CHENGB In the case that the distance is equal to the maximum in $\mathcal{Q}$, we can choose not to include the candidate in $\mathcal{Q}$.}} otherwise, the candidate is discarded. 
the maximum in $\mathcal Q$. If so, they include the candidate to $\mathcal Q$ with the distance as a key;~\footnote{{\CHENGB {\JIANYANGCAMERA When} the distance is equal to the maximum in $\mathcal{Q}$, {\chengf they} can choose not to include {\JIANYANGCAMERA it.}}} otherwise, the candidate is discarded. 
We call the computation of \textbf{checking whether an object has its distance from a query 
% at most
{\JIANYANGLAST no greater than}
a distance threshold and providing its distance if so} a \emph{distance comparison operation} (DCO).
%
Given an object $\mathbf{o}$ and a distance threshold $r$, we say that $\mathbf{o}$ is a \emph{positive} object (wrt $r$) if $\mathbf{o}$'s distance from the query is at most $r$ and a \emph{negative} object otherwise. 
% Thus, the DCO for $\mathbf{o}$ is to decide whether $\mathbf{o}$ is positive or not and if so, compute its distance.

% We call this a \emph{distance comparison} operation. 

%%%Elaboration of the importance of distance comparison oepration
% We observe that for each of these AKNN algorithms, the cost of performing distance operations contributes to a significant part (often a dominating part) of the overall cost. 
All existing AKNN algorithms adopt the following method for the DCO for an object and a threshold. It first computes the object's distance (from the query) and then compares the computed distance against the threshold. We call this method \texttt{FDScanning} since it scans full dimensions of the object for the operation. Clearly, \texttt{FDScanning} has the time complexity of $O(D)$, where $D$ is the number of dimensions of an object. 
% We observe that with \texttt{} adopted for DCOs, 
Based on \texttt{FDScanning}, nearly all AKNN algorithms have their time costs dominated by that of performing DCOs. We consider one of the most popular AKNN algorithms, \texttt{HNSW}~\cite{malkov2018efficient}, 
% which is a graph-based method, 
for illustration (elaborations on other algorithms will be provided in Section~\ref{subsec:aknn}). 
Let $N_s$ be the number of generated candidates of KNNs. The total time cost of \texttt{HNSW} is $O(N_s D + N_s \log N_s)$, where $O(N_s D)$ is the cost of performing DCOs and $O(N_s \log N_s)$ is the cost of other computations (detailed analysis can be found in Section~\ref{subsec:aknn}). 
% and $D$ is the number of dimensions of a data vector. 
Since $D$ can be 
% hundreds or thousands
{\JIANYANGREVISION hundreds}
while $\log N_s$ is a few dozens only for a big dataset involving millions of objects in practice, the cost of performing DCOs dominates the total cost of \texttt{HNSW} {\JIANYANG (we empirically verify the statement in Section~\ref{subsec:aknn}.)}. {\JIANYANG For example, on {\CHENG a real dataset DEEP, which involves 256} dimensions, the DCOs take 77.2\% of the total running time.}
% ***If appropriate, we can include some empirical results to strengthening the argument here.***

% We have two observations. \underline{First}, \texttt{FDScanning} is an over-kill for DCOs on negative objects since for negative objects, we only need to confirm that their distances are larger than the threshold distance $r$ \emph{without returning their exact distances} - recall that negative objects would be discarded and would not be put in the queue $\mathcal{Q}$. 
% %
% \underline{Second}, among the DCOs involved in an AKNN algorithm, often most would be for negative objects. A clue for this is that when the DCO is conducted on an object, the distance threshold corresponds to some small distance (e.g., the $N_{ef}^{th}$ (resp. $K^{th}$) smallest distance seen far in\texttt{HNSW}(resp. IVF)). As result, the object would likely have its distance larger than the threshold and corresponds to a negative object. We verify this statement empirically (details can be found in Section~\ref{subsec:aknn}). For example, for HNSW, the number of negative objects is 4.3x to 5.2x times more than that of the positive ones; for IVF, the number of negative objects is 60x to 869x more than that of the positive ones.
% {\JIANYANG Specifically, assume that we evaluate all the objects in a random order. The expected number of positive objects is $O(K \log N)$. Then note that in practice we evaluate candidates in an AKNN-determined order, which is expected to be better than the random one. Thus, the number of positive objects in an AKNN algorithm is supposed to be bounded by $O(K \log N)$. }
% ***we don't have space to provide the proof*** 
% {Thus, we focus on optimizing the DCOs for negative objects.}
%***Can provide some intuition and/or theoretical evidence here.*** 

We have two observations.
% \underline{First}, for positive objects, it is required to compute their distances \emph{exactly} by definition of DCO. In this case, \emph{all} dimensions of a positive object are needed by the DCO on the object, which implies that \texttt{FDScanning} cannot be improved further for a positive object.
% Therefore, there is not much room of improvement on \texttt{FDScanning} for positive objects.
%
\underline{First}, for {\CHENGB DCOs on} negative objects, we only need to confirm that the objects' distances are larger than the threshold distance \emph{without returning their exact distances} - recall that negative objects would be discarded.
% and would not be put in the queue $\mathcal{Q}$. 
{\CHENGB Therefore, \texttt{FDScanning}, which always computes the distance of an object for a DCO on the object, conducts more than necessary computation for negative objects.}
%
\underline{Second}, among the DCOs involved in an AKNN algorithm, often most would be for negative objects. A clue for this is that when the DCO is conducted on an object, the distance threshold corresponds to some small distance (e.g., the $K$th smallest distance seen so far in many AKNN algorithms). As a result, the object would likely have its distance larger than the threshold and correspond to a negative object. We verify this statement empirically (details can be found in Section~\ref{subsec:aknn}). For example, 
% for \texttt{HNSW}, the number of negative objects is 4.3x to 5.2x times more than that of the positive ones; 
for a representative 
% quantization-based 
algorithm \texttt{IVF}~\cite{jegou2010product}, the number of negative objects is 60x to 869x more than that of the positive ones.
%
These observations collectively show that \texttt{FDScanning} is an over-kill for most of the DCOs that are involved for answering a KNN query, and thus there exists much room to achieve 
% better cost-effectiveness.
{\CHENGB reliable (nearly-exact) DCOs{\CHENGB~\footnote{We target reliable DCOs since DCOs are used for finding KNNs among the generated candidates, and if their accuracy is compromised, the quality of the found KNNs would be largely affected.}} with better efficiency.

% The problem with this target has not been explored before, and it is non-trivial to adapt any existing solution to achieve this goal.

In the literature, no efforts have been devoted to achieving 
% better cost-effectiveness 
reliable DCOs with better efficiency
than \texttt{FDScanning}, to the best of our knowledge. 
An immediate attempt is to use some distance approximation techniques such as \emph{product quantization}~\cite{ge2013optimized, jegou2010product,ITQ} and \emph{random projection}~\cite{johnson1984extensions} for DCOs in order to achieve better efficiency. However, as widely found in the literature~\cite{learningtohashsurvey, adaptive2020ml} and also empirically verified in our experiments, these techniques cannot avoid accuracy sacrifice in order to achieve some remarkable time cost savings, and thus they can hardly be used to achieve \emph{reliable} DCOs with better efficiency. For example, {\JIANYANG according to \cite{learningtohashsurvey}, on the dataset SIFT1M with one million 128-dimensional vectors, none of the 
% quantization-based
{\JIANYANGLAST quantization}
algorithms achieve more than 60\% recall without re-ranking.} Indeed, these techniques have only been used for generating candidates of KNNs~\cite{learningtohashsurvey}, but not (in DCOs) for 
% finding the KNNs among the generated candidates.
{\JIANYANGB finding them out from the generated candidates}.}

Therefore, in this paper, we develop a new method called \texttt{ADSampling} to fulfill this goal. 
At its core, \texttt{ADSampling} projects the objects to 
% vectors with \emph{fewer} dimensions 
% {\JIANYANGB a lower-dimensional space}
% {\CHENGC different lower-dimensional spaces}
spaces with different dimensionalities
and conduct DCOs based on the projected vectors for better efficiency.
% \emph{random projection} process~\cite{johnson1984extensions, datar2004locality, indyk1998approximate, sun2014srs, c2lsh}, 
Different from the conventional 
% and widely-adopted 
random projection technique~\cite{johnson1984extensions, datar2004locality, sun2014srs, c2lsh}, which projects \emph{all} objects to vectors with \emph{equal} dimensions, \texttt{ADSampling} is novel in the following aspects. \underline{First}, it projects \emph{different} objects to vectors with \emph{different} 
numbers of dimensions
% {\JIANYANGB dimensionalities}
{\CHENGB during the query phase} \emph{flexibly}. The rationale is that for {\JIANYANG negative} objects that are farther away from the query, it would be sufficient to project them to a space with fewer dimensions for {\CHENGC reliable} DCOs; whereas for {\JIANYANG negative} objects that are closer to the query, they should be projected to a space with more dimensions for {\CHENGC reliable} DCOs. We note that for positive objects (i.e., those that have their distances from the query at most a threshold), their distances should ideally not be distorted. \texttt{ADSampling} achieves this flexibility with two steps. (1) It first {\JIANYANG preprocesses} the objects via a \emph{random orthogonal transformation}~\cite{choromanski2017unreasonable, randomortho} {\CHENGB during the index phase (i.e., before a query comes)}. This step merely {\JIANYANG randomly} rotates the objects without distorting their distances. 
{\JIANYANG 
% [For Comparison - Option 1]
(2) Then during the query phase, when handling DCOs {\CHENGB on different objects}, it samples different numbers of dimensions of their transformed vectors. {\CHENGB We verify that the sampled vectors produced by these two steps 
% follow an identical distribution as
{\JIANYANG are identically distributed with}
those} 
% These two steps make the sampled vectors be equivalent to the vectors 
obtained from random projection, and thus, the approximate distances based on the sampled vectors, like those based on random projection, {\CHENGB correspond to} good estimations of the true distances while {\CHENGB achieving} the aforementioned flexibility of dimensionality.} 
% [For Comparison - Option 2]It then samples for different objects different numbers of dimensions of their transformed vectors as their projected vectors flexibly during the query phase (i.e., when answering a query).
% (2) It then \emph{incrementally} and \emph{aggressively} samples the dimensions of a transformed vector until it can confidently conduct the DCO on the object based on the sampled dimensions of its transformed vector {\CHENGB during the query phase (i.e., when answering a query.} 
% We note that the random transformation step on the objects in the database can be conducted during the index phase (i.e., before a query comes), and for different objects, different numbers of dimensions can be sampled during the query phase, depending on the query adaptively.

\underline{Second}, {\CHENGB it decides the number of dimensions to be sampled for each object \emph{adaptively} based on 
% the query 
% {\JIANYANGLAST its corresponding DCO}
{\CHENGC the DCO on the object}
during the query phase, but not pre-sets it to a certain number during the index phase (which is knowledge demanding and difficult to set in practice).}
% it leverages \emph{hypothesis testing} to decide the number of dimensions of the projected vector of each object during the query phase instead of pre-setting it during the index phase as the conventional random projection typically does. 
Specifically, it 
% \emph{incrementally} and \emph{aggressively}
{\JIANYANGREVISION \emph{incrementally}}
samples the dimensions of a transformed vector until it can confidently conduct the DCO on the object based on 
% the sampled dimensions of its transformed vector
{\JIANYANGLAST the sampled vector}. 
With 
% the sampled dimensions for an object
{\JIANYANGLAST the sampled vector}, 
{\CHENGC it determines whether there has been enough evidence for a reliable DCO by computing an approximate distance of the object and then conducting a \emph{hypothesis testing} based on the computed approximate distance.}
% with the null hypothesis that the object's distance is at most the distance threshold.
% {\JIANYANGB to determine whether there has been enough evidence for a firmed DCO.}
This is possible due to 
% the fact that there exists a concentration bound on the difference between the approximate distance and the true distance 
{\JIANYANGB the fact that there is a theoretical error bound on the approximate distance}
{\CHENGB(recall the aforementioned equivalence between a projected vector by \texttt{ADSampling} and that by the conventional random projection)}.
% (i.e., the approximate distance is a random variable depending on the true distance), 
% which 
% [Option 1]is also verified by;
% [Option 2]
% we verify by establishing the equivalence between a projected vector by \texttt{ADSampling} and that by the conventional random projection. 
% If the hypothesis is rejected, which implies that the distance is at least the threshold with high probability, it concludes that the object is a negative object and terminates the process immediately; otherwise, it continues to sample some more dimensions and performs another hypothesis testing until either the hypothesis is rejected or all dimensions have been sampled. In the former case, it would conclude a negative object, and in the latter case, the computed approximate distance would be equal to the true distance (since with all dimensions sampled, the projected vector corresponds to the transformed one, whose distance is not distorted by the random transformation step as discussed earlier) and concludes a positive or negative object {\CHENGB exactly}.
% For a positive object, its distance would have been computed by the end.
%

% Motivated by these observations, in this paper, we aim to achieve a more cost-effective method than \texttt{FDScanning}, and a natural idea is to use approximate distances for DCOs. To this end, we first identify four desiderata of such a method, namely (1) bounded error, (2) flexibility, (3) adaptivibility, and (4) recoverability. For bounded error, it means that the error of an approximate distance needs to be theoretically bounded since otherwise DCOs based on approximate distances would not be reliable with accuracy guarantees. For flexibility, it means that the method can provide approximate distances with different ``resolutions'' for different objects flexibly, e.g., for objects that are close enough to the query (e.g., positive objects), the distances should be very close to (or even equal to) exact ones, and for objects that are far away from the query, the distances can deviate from the exact ones more for saving the cost of computing the approximate distances. For adaptivity, it means that the method should be able to determine minimal resolutions of distance approximation for different objects adaptively. For recoverability, it means that the method is able to compute exact distances at the highest resolution since it can be necessary for positive objects and also those negative objects that have their distances very close to the distance threshold $r$ and reliable DCOs on them are not possible without computing exact distances. 

% We then review existing methods, which can compute approximate distances, including XX, XX, XX, and XX. ... Detailed review on these methods are provided in Section~\ref{}. However, none of them have all the aforementioned four desiderata, as summarized in Table~\ref{} and explained in Section~\ref{}. 

% Therefore, in this paper, we propose a new method called \texttt{ADSampling}, which is a randomized method and has all the aforementioned four desiderata. Specifically, \texttt{ADSampling} achieves (1) bounded error, (2) flexibility and (4) recoverability with a novel idea of first \emph{randomly transforming} data vectors and the query vector via a \emph{random orthogonal transformation} process~\cite{} and them sampling the a certain number of dimensions of the transformed vectors. This is because (1) the approximate distance based on the sampled vectors is equivalent ...

% {\JIANYANG
% [for comparison with the last two paragraphs]
% In the literature, no efforts have been devoted to achieving better cost-effectiveness than \texttt{FDScanning}, to the best of our knowledge. Therefore, in this paper, we develop a new method called \texttt{ADSampling} to fulfill this goal. 
% }
% {\JIANYANG 
% At its core, for reliably conducting the DCOs of different objects, \texttt{ADSampling} \emph{flexibly} and \emph{adaptively} samples different number of dimensions, where the sampling is based on the vectors which are preprocessed by a random orthogonal transformation (geometrically, random rotation). 
% The rationales are two-folded. \underline{First}, it samples different number of dimensions for different DCOs \emph{flexibly} and \emph{adaptively} because for negative objects that are farther away from the query, it would be sufficient to use approximate distances with lower accuracy (correspondingly fewer dimensions) to conclude that it is negative; whereas for those closer to the query, they need more dimensions to produce a firmed result. We note that for positive objects (i.e., those that have their distances from the query smaller than the thresholds), their distances should ideally not be distorted.
% \underline{Second}, it samples on randomly transformed vectors because with this preprocessing, the approximate distances calculated with the sampled dimensions are identically distributed with those calculated with the low-dimensional vectors obtained by a random projection~\cite{johnson1984extensions}. As a result, it share all the advantages of random projection such as its power in low-distortion metric embedding (i.e., it can largely reduces the dimensionality while introduces small error) and the corresponding theoretical guarantee. 

% \texttt{ADSampling} achieves this adaptivity with the following steps. 
% (1) During the index phase (i.e., before a query comes), it preprocesses all data vectors with a \emph{random orthogonal transformation}~\cite{randomortho, choromanski2017unreasonable}. This step helps it to share all the advantages of random projection. We also note that this step merely randomly rotates the space without distorting any distance information. 
% (2) During the query phase, when handling a DCO, it \emph{incrementally} and \emph{aggressively} samples the dimensions of the transformed vector. After sampling some dimensions, it leverages \emph{hypothesis testing} to decide whether the collected information are sufficient to claim that the result is negative. If so, it terminates the process immediately. Otherwise, it continues to alternatively sample some more dimensions and perform a hypothesis testing until either it concludes a negative result or all the dimensions have been sampled. 
% In the former case, it would conclude a negative result with sufficient evidence, and in the latter case, the computed approximate distance would be equal to the true distance (since with all dimensions sampled, the vectors are merely rotated and their distances are not distorted as discussed earlier) and it would conclude a positive or negative result exactly. 

% Its novelty is two-folded. \underline{First}, it fully inherits the advantages of the random projection technique~\cite{johnson1984extensions} while introduces extra flexibility in resolution. Specifically, random projection is well known for its power in low-distortion metric embedding, i.e., it can largely reduce the dimensionality while introduces small error on distance information. Sampling on randomly transformed vectors achieves the same effectiveness in low-distortion metric embedding, but allows to evaluate the different number of dimensions for different objects, which is different from the conventional application of random projection~\cite{johnson1984extensions, datar2004locality, sun2014srs, c2lsh}.

% }

{\CHENGB \texttt{ADSampling} achieves reliable DCOs with better efficiency
% is much more cost-effective 
than \texttt{FDScanning}, which we explain as follows.}
% {\JIANYANG Motivated by the above two observations, we 
% % focus on optimizing the costly 
% {\CHENG aim to conduct faster} DCOs for 
% % a large number of 
% negative objects.} 
% %
% Specifically, we propose a \emph{randomized} solution called \texttt{ADSampling} for DCOs.
% Motivated by the above two observations, we propose a \emph{randomized} solution called \texttt{ADSampling} for DCOs. 
First, for each negative object, {\JIANYANG we prove that} \texttt{ADSampling} would always return the correct answer 
% (i.e., it does not fail)
and run in \emph{logarithmic} time wrt $D$ {\JIANYANG in expectation}
% (recall that \texttt{FDScanning} runs in $O(D)$ time, which is linear wrt $D$)
{\JIANYANG (recall that \texttt{FDSanning} runs in linear time wrt $D$). }
% For a positive object, it might fail with a probability, which is small and decays exponentially, and runs in $O(D)$ time. 
Second, {\JIANYANG for each positive object, it succeeds with high probability and runs in $O(D)$ {\CHENGC time}.}
% {\JIANYANG For positive objects, it might fail with a probability, which we control it to be very small theoretically and practically (e.g., less than 0.1\%), and runs in $O(D)$ time.}
% ***Can we emphasize that the probability here is small theoretically and practically?*** 
% It runs in $O(D)$ time if it succeeds and in $XXX$ time if it fails. 
% In summary, \texttt{ADSampling} trades the accuracy of positive objects for the efficiency of negative objects. 
% \texttt{ADSampling} is much more cost-effective than \texttt{FDScanning} since (1) the improvement of the time complexity for negative objects is \emph{significant} (i.e., from being linear wrt $D$ to being logarithmic wrt $D$), (2) the accuracy compromise for positive objects is \emph{tiny},
% and \emph{controllable} 
% (i.e., it decays exponentially wrt the time cost)
% {\JIANYANG (e.g., its decay is exponentially faster than the increase of the time cost)}, 
% and (3) 
% Third, there are much more negative objects than positive objects, as mentioned earlier.
Third, there are much more negative objects than positive objects.
% ($N_s$ v.s. $K \log N$). 


% Specifically, \texttt{ADSampling} involves two major ideas. \underline{First}, during the index phase, it \emph{randomly transforms} each data vector (with \emph{random orthogonal transformation}~\cite{choromanski2017unreasonable, yu2016orthogonal, ITQ, jegou2010product}, geometrically, to randomly rotate it) and obtains the corresponding transformed data vector. 
% In addition, it randomly transforms the query vector once at the beginning of the query phase. 
% % It has two properties.
% These transformed data vectors and query vector (but not the original ones) would be used for the DCOs since they have two nice properties, namely P1: The distance between a data vector and the query vector is equal to that between the transformed data vector and the transformed query vector; and P2: Randomly sampling $d$ dimensions of the transformed vector is equivalent to performing a \emph{random projection}~\cite{johnson1984extensions, blockjlt, fftjlt} over the original vector. 
% %We note that the cost of transforming data vectors is attributed to the index cost and that of transform
% % \texttt{ADSampling} would refer to refers an object (resp. a query) to its transformed data vector (resp. the transformed query vector).
% %
% \underline{Second}, for the DCO on an object $\mathbf{o}$ and a distance threshold $r$, \texttt{ADSampling} \emph{aggressively} and \emph{incrementally} samples dimensions of $\mathbf{o}$'s transformed data vector (and correspondingly those of the transformed query vector). With the sampled dimensions, it computes an approximate distance of $\mathbf{o}$ and conducts a \emph{hypothesis testing} with the null hypothesis that $\mathbf{o}$'s distance is smaller than $r$. This is possible due to the aforementioned property P2, which implies that there exists a concentration bound on the difference between the approximate distance and the true distance (i.e., the approximate distance is a random variable depending on the true distance). If the hypothesis is rejected, which implies that the distance is at least $r$ with high probability, it concludes that the object is a negative object and terminates immediately; otherwise, it continues to sample some more dimensions and performs another hypothesis testing until either the hypothesis is rejected or all dimensions have been sampled. In the former case, it would conclude a negative object, and in the latter case, it would be able to compute the true distance (due to the aforementioned property P1) and concludes a positive or negative object accordingly. For a positive object, its distance would have been computed by the end.

We summarize the major contributions of this paper as follows.
\begin{enumerate}
\item We systematically review existing AKNN algorithms and identify the \emph{distance comparison operation} (DCO), which is ubiquitous in AKNN algorithms. With the existing method \texttt{FDScanning}, the costs of DCOs dominate the overall costs for nearly all AKNN algorithms, which we verify both theoretically and empirically. (Section~\ref{sec:dco})

\item We propose a new method \texttt{ADSampling}, 
which achieves 
% better cost-effectiveness 
{\CHENGB reliable DCOs with better efficiency}
% than \texttt{FDScanning} 
{\JIANYANGREVISION for the high-dimensional Euclidean space}. 
Specifically, in most of the cases (i.e., for negative objects), \texttt{ADSampling} runs in \emph{logarithmic} time wrt $D$ and always returns a correct answer. (Section~\ref{sec:adsampling})

\item For a general AKNN algorithm (which we denote by \texttt{AKNN}), we replace \texttt{FDScanning} with \texttt{ADSampling} for DCOs and achieve a new algorithm (which we denote by \texttt{AKNN+}). 
% We theoretically analyze the time-accuracy tradeoff of \texttt{AKNN+}. (Section~\ref{sec:aknn+})
{\JIANYANG We prove that an \texttt{AKNN+} algorithm preserves the results of its corresponding \texttt{AKNN} algorithm with high probability and significantly reduces the time complexity. (Section~\ref{sec:aknn+})}

\item 
% We further develop two techniques, which are AKNN algorithm-specific and would further improve the cost-effectiveness of \texttt{AKNN+} algorithms. 
{\JIANYANGCAMERA We further develop two AKNN-algorithm-specific techniques to improve the cost-effectiveness of \texttt{AKNN+} algorithms.}
For example, for 
% the \texttt{HNSW+} algorithm
{\JIANYANG graph-based algorithms (with \texttt{HNSW+} as a representative)}, we incorporate more approximations and obtain a new algorithm (which we call \texttt{HNSW++}); for other algorithms (with \texttt{IVF+} as a representative), we improve their cost-effectiveness with cache-friendly storage. (Section~\ref{sec:aknn++})


\item We conduct extensive experiments on real datasets, which verify our techniques.
% that (1) the costs of DCOs dominate the overall costs of AKNN algorithms including \texttt{HNSW} and \texttt{IVF}, 
% (2) \texttt{AKNN+} algorithms (using \texttt{ADSampling} for DCOs) are more cost-effective than \texttt{AKNN} algorithms (using \texttt{FDScanning} for DCOs), and (3) \texttt{AKNN++} algorithms further improves \texttt{AKNN+} algorithms. 
{\JIANYANG
% (2) \texttt{AKNN+} and \texttt{AKNN++} algorithms (using \texttt{ADSampling} for DCOs) are more cost-effective than \texttt{AKNN} algorithms (using \texttt{FDSanning} for DCOs), and (3) \texttt{AKNN+} and \texttt{AKNN++} algorithms reduce the number of evaluated dimensions significantly {\CHENGB while introducing} little accuracy loss.  
% For example, \texttt{ADSampling} brings up to 2.65x speed-up {\CHENGB on} \texttt{HNSW} and up to 5.58x speed-up {\CHENGB on} \texttt{IVF} {\CHENG while providing} the same accuracy. 
For example, \texttt{ADSampling} brings up to 2.65x speed-up on \texttt{HNSW} and 5.58x on \texttt{IVF} {\CHENG while providing} the same accuracy. 
Besides, 
% in terms of dimensionality, 
it helps to save up to 75.3\% of the evaluated dimensions for \texttt{HNSW} and up to 89.2\% of those for \texttt{IVF} with the accuracy loss of no more than 0.14\%. (Section~\ref{section:experiment})
}


% speed up \texttt{HNSW} by up to 2.65 times and \texttt{IVF} by up to  under the same accuracy (Section~\ref{section:experiment})

\end{enumerate}
%
% For the rest of the paper, we review the related work in Section~\ref{sec:related work}, provide a detailed proof for the main result in Section~\ref{section:theory}, and conclude the paper in Section~\ref{sec:conclusion}.
{\JIANYANGREVISION For the rest of the paper, we review the related work in Section~\ref{sec:related work} and conclude the paper in Section~\ref{sec:conclusion}.}
}

% To accelerate AKNN query, two strategies 
% % are naturally applied
% have been widely adopted: 1) heuristically decreasing the number of 
% %evaluated objects
% scanned objects~\cite{muja2014scalable, malkov2018efficient, datar2004locality, jegou2010product} and 2) accelerating distance evaluations with approximation~\cite{ge2013optimized, guo2020accelerating}. 
% {\CHENG ***Some discussions on 1) and 2) are expected, e.g., how are they are achieved in a concrete existing AKNN algorithm, are they orthogonal to each other, etc.}
% {\JIANYANG might need further discussion}
% %
% In terms of the second direction, however, due to the sensitivity of distance in high-dimensional space, {\CHENG the approximate} KNNs selected based on approximate distances hardly achieve satisfactory accuracy, which necessitates a re-ranking stage with exact distance~\cite{datar2004locality, sun2014srs, surveyl2hash}.
% {\CHENG }
% As a consequence, for nearly all KNN algorithms, no matter using approximate distance or not, a conventional and unavoidable operation is to compute exact distance to maintain a KNN set $\mathcal K$ (technically, a max-heap). 
% Specifically, for a new candidate, one should evaluate its exact distance, compare it with the maximum in $\mathcal K$ (denoted as $dis_{i_K}$). If it's smaller, then it'll be inserted into $\mathcal K$ to replace the maximum. 
% Though it's widely reported that existing methods achieve great success in accelerating AKNN query~\cite{li2019approximate, annbenchmark, graphbenchmark}, in many cases, the aforementioned KNN set operation still requires to evaluate exact distance of many high-dimensional vectors for the purpose of exactly comparing data objects, which takes up a large proportion of running time and has become a bottleneck.

% {\CHENG 12-7-Cheng: (1) The above paragraph is very compact (which makes it not easy to follow); (2) It does not leave a strong impression that the so-called set operation is critical to AKNN (the linkage between them is weak); (3) Overall, this part should be improved significantly (we can discuss more on this part later on).}

%To accelerate ANN query, two strategies are mainly applied: 1) decreasing the number of distance evaluation and 2) approximating distance with data compression. Specifically, existing ANN algorithms can be categorized into four types: tree-based ~\cite{muja2014scalable, dasgupta2008random, beygelzimer2006cover}, graph-based~\cite{malkov2018efficient, malkov2014approximate, li2019approximate, fu2019fast, fu2021high}, quantization-based~\cite{jegou2010product, ge2013optimized, guo2020accelerating} and hashing-based~\cite{indyk1998approximate, datar2004locality, c2lsh, tao2010efficient, huang2015query, sun2014srs, lu2020vhp, zheng2020pm} algorithms. The first two methods build tree or graph structure as indexes to navigate ANN search. Since graph-based methods show dominant performance, we focus on its query workflow. Specifically, .During query phase, they efficiently navigate ANN search so as to decrease the number of evaluated points by orders of magnitude. The last two usually follow the framework of filter-and-verification which first shortlists candidates with short hashing or quantization code and then re-ranks them with exact distance~\cite{imi, jegou2010product, datar2004locality, sun2014srs}. 


% \begin{comment}
% In ANN query, two strategies are mainly applied: 1) unsafely pruning data objects to decrease the number of distance evaluation
% %decreasing the number of distance evaluation 
% and 2) accelerating distance evaluation with allowing some approximation.
% %approximating distance with data compression. 
% Specifically, existing ANN algorithms can be categorized into four types: tree-based ~\cite{muja2014scalable, dasgupta2008random, beygelzimer2006cover}, graph-based~\cite{malkov2018efficient, malkov2014approximate, li2019approximate, fu2019fast, fu2021high}, quantization-based~\cite{jegou2010product, ge2013optimized, guo2020accelerating} and hashing-based~\cite{indyk1998approximate, datar2004locality, c2lsh, tao2010efficient, huang2015query, sun2014srs, lu2020vhp, zheng2020pm} algorithms. The first two methods build tree or graph structure as indexes. During query phase, they efficiently navigate greedy search so as to decrease the number of evaluated points by orders of magnitude. The last two usually follow the framework of filter-and-verification which first shortlists candidates with short hashing or quantization code and then re-ranks them with exact distance~\cite{imi, jegou2010product, datar2004locality, sun2014srs}. 
% %In particular, within the methods of indexes, graph-based methods show dominant performance~\cite{li2019approximate, annbenchmark}. During query, it performs greedy beam search 
% Though it's widely reported that these methods achieve great success in accelerating ANN query~\cite{li2019approximate, annbenchmark, graphbenchmark}, during greedy searching and candidate re-ranking, one still needs to evaluate exact distance of many high-dimensional vectors for the purpose of exactly ordering \textbf{}data objects, which takes up a large proportion of total running time and has become a bottleneck. 
% \end{comment}


%to exactly order objects in the aforementioned greedy searching and candidate re-ranking, they still need to evaluate exact distance for many high-dimensional vectors, which has become a bottleneck.

%However, exactly ordering data objects not necessarily requires exact distance. A long ignored fact is that %NN is a query based on distance comparison, which implies that

%However, exact distance comparison not necessarily requires exact distance. A long ignored fact is that with approximate distance and \textit{guaranteed error bound}, we may also conclude exact order. For example, given a query vector $\mathbf{q}_A$ and two database vectors $\mathbf{o}_1, \mathbf{o}_2$, assuming that we have known their approximate distance: $dis_{1,A}'=0.95$, $dis_{2,A}'=2.1$, and the corresponding relative error bound: $\epsilon = 5\%$, then the exact distance of object 2 is at least $dis_{2,A}'/(1+\epsilon) =2$ and that of object 1 is at most $dis_{1,A}' / (1-\epsilon)=1$. In this case, we conclude that object 1 is nearer to query A than object 2 without knowing exact distance. 

%However, exact distance comparison does not necessarily require exact distance. A long ignored fact that with approximate distance and \textit{guaranteed error bound}, we may still be able to conclude exact order. For example, given a query vector $\mathbf{q}_A$ and a candidate database vector $\mathbf{o}_1$ {\CHENG (named object 1)}, assuming that we have known its approximate distance $dis_{1,A}'=2.1$, the corresponding relative error bound $\epsilon=5\%$ and the current maximum distance in $\mathcal K$ $dis_{i_K}=1$, then the exact distance of object 1 is at least $dis_{1,A}'/(1+\epsilon )=2$. In this case, we can safely conclude that object 1 cannot update $\mathcal K$ without knowing its exact distance.
%
%Though it provides some space for approximation (at the same time, acceleration), there are also some new problems. One problem is that fixed resolution cannot provide exact ordering for all distance comparisons. Back to the example, we consider another query B whose approximate distance to the objects are $dis'_{1,B}=1$, $dis'_{2,B}=1.01$. Unlike query A, with the error bound of $5\%$, the order of query B is unidentifiable. We investigate the variance of needed resolution on real-world datasets. It reveals that fixed resolution either causes inaccuracy or redundancy, which necessitates resolution's \textit{flexibility}. Another problem comes from the way we determine the resolution for a certain comparison. 
%For a newly coming comparison, we have no prior knowledge about its minimum needed resolution. Thus, it entails a mechanism to determine the resolution \textit{adaptively}. Finally, our study also reveals that some comparisons are extremely fragile to approximation. Conventional methods usually evaluate exact distance with raw vectors from scratch when fragility is detected. However, it gives rise to re-evaluation overhead. We claim that a method should better progressively \textit{recover} exact distance from calculated approximate distance to avoid re-evaluation cost. In summary, to efficiently produce exact ordering results with approximate distance, a method should have guaranteed error bound, flexibility of resolution, adaptivity to a particular comparison and recoverability from approximate distance. %
%
% Though it provides some space for approximation (at the same time, acceleration), there are also some new problems. 
\if 0
{\CHENG In this paper, we propose to conduct \emph{nearly-exact} DCOs based on approximate distances with probabilistic guarantees for better efficiency. 
% Specifically, we aim to develop a method of computing approximate distances of data vectors and then conduct distance comparisons based on the approximate distances.
% Specifically, we aim to compute approximate distances and then conduct distance comparisons based on the approximate distances. 
% Apart from the ability of providing an error guarantee for the approximate distance, 
We identify four desiderata of the method to fully unleash the power of using approximate distances for reliable distance comparisons.
\underline{First}, it should be able to provide an error guarantee for an approximate distance since with the error guarantees of approximate distances, the results of distance comparisons would be accurate (with guarantees) and even exact.
For example, suppose the maximum distance in $\mathcal{K}$ is 1 and a data vector $\mathbf{o}_1$ has an approximate distance $1.05$ (with a relative error bound $5\%$. Then, we know that the exact distance of $\mathbf{o}_1$, which is at least $1.05 / (1+0.05)=1$, is not smaller than the maximum in $\mathcal{K}$ (i.e., 1).
\underline{Second}, it should have the \emph{flexibility} of achieving different ``resolutions'' of approximate distances for different data objects as candidates of KNNs, and correspondingly, it would provide different error bounds for different objects.
% One problem is that fixed resolution cannot provide exact comparison results for all candidates. 
Back to the aforementioned example, we consider another candidate $\mathbf{o}_2$. Suppose $\mathbf{o}_2$'s approximate distance to the query, when measured at the same resolution as that of $\mathbf{o}_1$'s approximate distance, is $1.01$, and the corresponding error bound is $5\%$. In this case, we are not able to conclude on whether $\mathbf{o}_2$ has a smaller distance than the maximum one in $\mathcal{K}$. Instead, we need to increase the resolution somehow so that it would provide a better error bound to be able to make a conclusion, e.g., at a resolution with its corresponding error bound of $1\%$, we know that the exact distance of $\mathbf{o}_2$, which is at least $1.01 / (1+0.01)=1$, is not smaller than the maximum in $\mathcal{K}$ (i.e., 1).
% $dis_{2,A} \ge dis'_{2,A} / (1+1\%) = 1 = dis_{i_K}$.
}
% can have exact distance comparison results with the approximate distance.}
% {\CHENG For object 2, if the resolution of the approximate distance an error bound of $5\%$ is not sufficient for deciding whether object 2 has a smaller distance from the query object than the current Kth NN in $\mathcal{K}$ or not, but an error bound of $1\%$ is.}
% Unlike object 1, with the error bound of $5\%$, the order between object 2 and the current Kth NN in $\mathcal{K}$ is undecidable. It becomes decidable once the error bound becomes $1\%$, but then Once we increase the resolution to $1\%$, however, it would be redundant for candidate 1 so as to harm efficiency. Thus, it's of vital importance to realize \textit{flexibility} of resolution, i.e. different resolution for different candidates. 
% Another problem comes from the way we set the resolution for a certain candidate. For a newly visited object, we have no prior knowledge about it. To find its minimum needed resolution, we need a mechanism to determine it \textit{adaptively}. 
{\CHENG \underline{Third}, it should have the ability to \emph{adaptively} determine an appropriate resolution of the approximate distance for an object such that the it is neither more than enough (which means some computation of distance evaluation could be saved otherwise) nor lower than necessary (which means a firmed result of distance comparison cannot be obtained).}
% Finally, some candidates are extremely fragile to approximation, which really requires exact distance to obtain comparison results. Conventional methods evaluate exact distance with raw vectors from scratch when fragility is detected. However, it gives rise to re-evaluation overhead. We claim that a method should better progressively \textit{recover} exact distance from known approximate distance to guarantee that it's no worse than the plain distance evaluation at any case.
%
{\CHENG\underline{Fourth}, it should have the ability to \textit{recover} the exact distance by reaching the highest resolution with the error bound of 
%1 
{\JIANYANG 0\%}.
% from known approximate distance to guarantee that it's no worse than the plain distance evaluation at any case. 
This is because some candidates have their distances extremely fragile to approximation, and in these cases, exact distances are not avoidable to obtain firmed results of distance comparison.}
%
In summary, to efficiently produce exact distance comparison results with approximate distances, a method should have (1) guaranteed error bound, (2) flexibility of multiple resolutions, (3) adaptivity of reaching an appropriate resolution and (4) recoverability of the exact distance. 

\begin{table}[h]\small
  \caption{Methods of Distance Approximation}
  \label{tab:freq}
  \begin{tabular}{c|cccc}
    \toprule
    &DR &QT &RP &\textbf{ours}\\
    \midrule
   Error Bound &NO &NO & \textbf{Probabilistic} &\textbf{Probabilistic}\\
   Flexibility &NO &\textbf{Limited} & NO &\textbf{YES}\\
   Adaptivity &NO &NO & NO &\textbf{YES}\\
   Recoverability &NO &NO  & NO &\textbf{YES}\\
%   \midrule
%   Distance Comparison &NO &NO &NO &\textbf{Probabilistic}\\
  \bottomrule
\end{tabular}
\end{table}

% Unfortunately, no existing methods of distance satisfy them all together. 
There are two main forms of distance approximation that have been used for AKNN: 
% nearest neighbor search: 
1) quantization (QT)~\cite{ge2013optimized, jegou2010product,ITQ, additivePQ, guo2020accelerating}, and 2) dimension reduction, in which dimension reduction can be further categorized into optimization-based dimension reduction (DR)~\cite{wold1987principal, kruskal1964multidimensional} and random projection (RP)~\cite{johnson1984extensions, blockjlt, fftjlt}.
{\CHENG Unfortunately, none of these existing methods has all of the aforementioned four desiderata} (a summary is presented in Table~\ref{tab:freq}). 
First, QT and DR (e.g., PCA~\cite{wold1987principal}) optimize a compressed representation to minimize the \emph{total} approximation error instead of the \emph{maximum} one, which fails to guarantee an error bound. {\CHENG Only RP provides some probabilistic error bounds.
Second, DR and RP do not support flexible resolutions of approximate distances.
% Flexible resolution and is a long but implicitly adopted strategy in QT. 
Only QT supports flexible resolutions to some extent with a three-stage strategy \cite{jegou2010product, imi, surveyl2hash}:}
% to apply different resolution on different objects: 
1) generate candidate lists with coarse code; 2) shrink the list with finer code; 3) re-rank the list with exact distance~\footnote{Some stages could be skipped according to specific requirements, e.g., memory constraint~\cite{johnson2019billion, jegou2011searching}.}. 
{\CHENG Nevertheless, the list size and code size at each stage are preset hyper-parameters and fixed for all queries, and thus QT provides very limited flexibility only.}
% so it's very limited in flexibility and hard to tune. 
Third, to the best of our knowledge, no existing methods achieve adaptivity and recoverability on resolution of distance for high-dimensional nearest neighbor search. It's worth noting that hashing-based methods~\cite{indyk1998approximate, datar2004locality, c2lsh, tao2010efficient, huang2015query, lu2020vhp} are also popular for data compression. They target to map close vectors to similar hash codes and {\CHENG use code comparison as a proxy of distance comparison}.
% distance comparison with hash code comparison. 
However, since hashing 
%doesn't 
does not explicitly approximate distances, they're not within the scope of our discussion.  \footnote{Also, since different vectors may be mapped to the same code, hashing cannot 
% identify their order.
{\CHENG help with exact distance comparison}.
}
{\JIANYANG We also emphasize that since hashing and quantization cannot provide guarantee for distance comparison. They can be used only in the first stage of AKNN query, i.e., generating candidates but not the second.}
\fi
%In particular, quantization~\cite{jegou2010product, ge2013optimized, guo2020accelerating} builds a codebook where a few vectors are constructed to approximate all database vectors in a one-to-many manner. During query phase, the distance-to-query of database vectors is approximated by that of codebook vectors so as to largely reduce distance evaluation. The codebook is built with optimization, i.e. minimizing the total approximation error. 


%In terms of guaranteed error bound, conventional learning-based methods like PCA~\cite{wold1987principal}, MDS~\cite{kruskal1964multidimensional} and quantization~\cite{ge2013optimized, jegou2010product} optimize a compressed representation to decrease total error instead of maximum error, which fails to guarantee an error bound. Flexible resolution is a long adopted strategy in quantization-based methods. \cite{jegou2010product, imi} suggest a three-stage strategy to apply different resolution on different objects: 1) generate candidate lists with coarse code; 2) shrink the list with finer code; 3) re-rank the list with exact distance~\footnote{Some stages could be skipped according to specific requirements, e.g.,memory constraint~\cite{johnson2019billion, jegou2011searching}.}. However, the list size and code size at each stage are preset hyper-parameters and fixed for any queries, so it's very limited in flexibility. Last but not least, to the best of our knowledge, no existing methods achieve adaptivity and recoverability on resolution of distance for high-dimensional nearest neighbor search. 

% With the necessity of guaranteed error bound, we revisit random projection due to its plentiful theoretical results~\cite{johnson1984extensions,  larsen2017optimality, fftjlt, datar2004locality, blockjlt}. Random projection is a famous technique for low-distortion metric embedding. It provides theoretical guarantee on error bound in a probablistic manner. To be specific, the seminal work of random projection, Johnson-Lindenstrauss Lemma~\cite{johnson1984extensions} (JL Lemma), states that with the probability of $1-\delta$, random projection preserves mutual distance among a finite set $\mathcal X$ of $N$ objects with at most $\epsilon$ multiplicative error when reducing dimensionality to $\Theta(\epsilon^{-2} \log \frac{N}{\delta})$. Though random projection was utilized in nearest neighbor search in the scheme of dimension reduction~\cite{fftjlt, blockjlt}, Locality Sensitive Hashing(LSH)~\cite{datar2004locality, c2lsh} and random partition/projection tree~\cite{ram2019revisiting, dasgupta2008random}, existing works always evaluate a fixed-sized code during query. Also, though random projection guarantees the error bound, it has no guarantee on the result of an arbitrary distance comparison. We propose a novel scheme originated from the core lemma of JL Lemma, i.e. concentration inequality, but make it flexible, adaptive and recoverable. 

%Intuitively, the distortion rate is negatively correlated with the average recall of NN queries. However, the direct effect of random projection on NN query is never investigated. 

%Existing ANN algorithms can be generally categorized into two types: index and compression. Index-based methods build tree or graph topology to navigate ANN search, while compression-based ones accelerate distance computation with short code generated by quantization, hashing and dimensionality reduction. 

%Random projection is a popular technique in NN query. Locality sensitive hashing(LSH) family~\cite{indyk1998approximate, datar2004locality, c2lsh, tao2010efficient} is one random-projection-based method. Its framework was firstly proposed in \cite{indyk1998approximate}, targeting the ANN query in high-dimensional Hamming space. Later it was extended to Euclidean space in E2LSH~\cite{datar2004locality}. The algorithm can be summarized as two stages: random Gaussian projection and bucketization, i.e. . Recent LSH-based works~\cite{huang2015query, sun2014srs, lu2020vhp, zheng2020pm} drop physical bucketization while resort to query-centered virtual buckets which are given by distance thresholds. Their dominant performance among the LSH family~\cite{huang2015query, sun2014srs} shows the potential of dropping bucktization and solely doing random projection for dimensionality reduction.



%In general, NN is a query based on comparison. For different queries, the inherent difficulty of comparison might vary a lot. For example, given two database vectors $\mathbf{o}_1,\mathbf{o} _2\in \mathbb{R}^D $ and two query vectors $\mathbf{q} _A, \mathbf{q} _B \in \mathbb{R}^D $, we assume that their distances are given as $\|\mathbf{o}_1-\mathbf{q}_A\|  =1,\| \mathbf{o}_2 - \mathbf{q}_A\|  =2$ and $\| \mathbf{o}_1 - \mathbf{q}_B\|  =1, \| \mathbf{o}_2 - \mathbf{q}_B\|  =1.01$. Then, in condition of correctly ordering $\mathbf{o}_1 $ and $\mathbf{o}_2 $, query A tolerates $100\%$ multiplicative error while query B just tolerates $1\%$. {\color{red} more connections needed here} With the optimality of JL Lemma~\cite{larsen2017optimality}, it reveals that the inherent difficulty, the minimum dimensions needed to correctly compare two objects, might vary a lot, which motivates us to investigate the variance of difficulty among queries on real-world datasets and design adaptive algorithms. 

%We investigate the reordering after random projection on real-world query workloads. For a given query $\mathbf{q} $,  we assume that the KNNs  {\color{red} footprint needed here to explain KNN} are reordered to be $rank_1,...,rank_k$, the phenomenon of which we name as KNN expansion. To retrieve the KNNs, we'll need to evaluate the true distance for the nearest $\max_i rank_i$ objects in the reduced space. {\color{red} We find that ... in ... datasets ...} {\color{red} We also provide theoretical analysis on KNN expansion.}

%The study exhibits the possibilities and impossibilities of dimensionality reduction with random projection. {\color{red} On the one hand, it's capable of reducing the cost of distance computation for some queries. bad sentence} On the other hand, we cannot expect too much because some other queries are inherently fragile to random projection. Thus, we claim that it's necessary to apply different dimensions for different queries. Furthermore, we aim to extend the adaptation from each query to each individual distance comparison. 

%With the theoretical error bound of random projeciton, we still have three main challenges: 1) How do we make dimensionality flexible? 2) We have no prior knowledge about the difficulty of a query. How do we determine the dimensionality? 3) Some queries are extremely fragile to random projection. How do we recover full-precision distances when needed? 

\if 0
In this paper, we propose \textbf{adaptive dimension sampling} (ADSampling),
% to resolve these issues. ADSampling 
which is a probabilistic distance comparison framework with {\CHENG guaranteed error bounds,} flexibility, adaptivity and recoverability. 
{\CHENG ADSampling involves two major ideas. \underline{First}, it first performs the \emph{random orthogonal transformation}~\cite{choromanski2017unreasonable, yu2016orthogonal, ITQ, jegou2010product} (geometrically, random rotation) over all data vectors as a pre-processing step. It guarantees that (1) the distance between 
%two data vectors
{\JIANYANG any two vectors}
is preserved as the distance between their rotated vectors and (2) randomly sampling the dimensions of a rotated vector is equivalent to performing random projection over the original vector.
%
\underline{Second}, when conducting distance comparison for a data vector, it \emph{progressively} samples dimensions of its rotated vector (and those of the query vector), compares the approximate distance computed based on the sampled dimensions of the two vectors, and adopts a \emph{sequential hypothesis testing}~\cite{wald1945sequential, Berger2017sequential, network, computervision, satuluri2011bayesian, sequentialLSH} strategy to decide when to stop sampling dimensions
% (i.e. it continues sampling until there is enough evidence to give a conclusion about a distance comparison).
such that the distance comparison results are correct with sufficient reliability.
%
ADSampling achieves (1) probabilistic error bounds (because of its equivalence to random projection), (2) flexibility of resolution (because it can sample different numbers of dimensions of rotated vectors), (3) adaptivity of setting an appropriate resolution (because of its progressive sampling manner and smart strategy for terminating sampling dimensions), and (4) recoverability of exact distances (because it preserves the distances during the rotation step and can recover the exact distance by sampling all dimensions of rotated vectors).}
% It formulates distance evaluation as a statistical problem, i.e. sampling, where each sample corresponds to a dimension. 
% It achieves flexibility by flexibly sampling dimensions on our pre-processed vectors. 
% In terms of adaptivity, the needed dimensionality is determined by sequential hypothesis testing, i.e. it continues sampling until there's enough evidence to give a conclusion about a distance comparison. 
% Finally, with the distance-preserving property of random orthogonal transformation, the exact distance is guaranteed to be recovered when $D$ dimensions are sampled. 
% It's worth noting that since our framework enhances a very fundamental component in KNN query, i.e. distance comparison, it's expected to accelerate all existing KNN algorithms in a plugin manner. 
\fi
% {\color{red} TODO}We next equip existing methods with our ADSampling framework. We in general discuss its applications on two different scenarios: the dynamic and the static. The dynamic case refers to the algorithms where candidates are dynamically generated, and the distance comparison results of current candidates affect further search path. To accelerate it, we propose \textbf{adaptive resolution search} (ARSearch) and \textbf{adaptive resolution route} (ARRoute), where the first fully preserves the search path with high probability and the second allows a larger extent of approximation with little accuracy loss.
% In the static case, candidates are generated all at once, so compared with the dynamic one, we can manipulate evaluation order for further optimization, which motivates our algorithm \textbf{adaptive resolution select}(ARSelect). It guarantees to select exact top-K objects within generated candidates with high probability. 
% We did rigorous theoretical analysis on its failure probability and expected evaluated dimensionality of each data object (time complexity). It shows that unlike JL Lemma, it's independent of the size of the database indicating that it naturally suits large-scale databases without the need to tune parameters.

% %{\color{red} this paragraph may need reorganization.} We next consider equipping existing methods with our ADSampling framework. For index methods, since it's widely reported that graph-based methods significantly outperform tree-based ones~\cite{fu2019fast, li2019approximate, graphbenchmark, annbenchmark}, we just focus on graph. We propose \textbf{adaptive-resolution search} to accelerate graph routing. For filter-and-verification methods, we propose \textbf{adaptive-resolution re-ranking} to efficiently find out KNNs within generated candidates. Both two methods are randomized algorithms with high end-to-end probability to produce the same semantics as that of exact distance evaluation. In other word, with high probability, our adaptive methods themselves don't introduce any extra error. We further design an algorithm \textbf{adaptive-resolution search*} that approximately preserves the path of graph routing. It enables a larger extent of approximation, while just decreases the accuracy slightly. For a KNN query in $D$-dimensional space, we prove that with failure probability of $\delta$, our methods reduce {\color{red} the expected time complexity of distance comparison} from $O(D)$ to $\min(O(\frac{1}{c^2}  (\log \frac{D}{\delta} +\log K), O(D))$, where $(1+c)$ is the distance ratio between the compared two objects.


% %We illustrate the statement with rigorous theoretical analysis and experimental studies. {\color{red} Another good property of our method is that, the closer one object is to KNNs, the higher its distance resolution will be, which exactly matches its importance, i.e. during KNN search, the closer one object is to KNNs, the more informative its distance would be to index routing. Based on this property, we propose \textbf{adaptive-resolution search*}, which surprisingly does little impact on the accuracy but further decreases the evaluated dimensionality much.These methods decrease the evaluated dimensionality to 25\%.}

% We did exhaustive experimental study on real-world datasets to verify the effectiveness of our techniques. Specifically, we equip our techniques on two state-of-the-art in-memory AKNN algorithms: hierarchical navigable small world graphs (HNSW)~\cite{malkov2018efficient} and inverted file (IVF)~\cite{jegou2010product}, representing the dynamic case and the static one correspondingly. It shows that our methods reach the same accuracy while skipping up to $89.95\%$ dimension evaluation. We also compared the AKNN algorithms with and without our plugins on the accuracy and evaluated dimensions under the same search configuration. It successfully verifies our claim that our methods introduce little accuracy loss while skip lots of dimension evaluation. Parameter study is further conducted to provide suggestions on parameter tuning. 

%To verify the failure probability of our method, we equip exhaustive linear scan with adaptive resolution K selection {\color{red} It shows that for most queries, it reaches 100\% recall. with ... speed-up}. {\color{red} We also did parameter study.}

%To be specific, we equip adaptive-resolution search/search* on the state-of-the-art graph-based methods, HNSW~\cite{malkov2018efficient}. It {\color{red}saves up to  75\% dimension evaluation/achieves up to 2x speed-up} at the same accuracy. We also equip a state-of-the-art method of filter-and-verification, IMI+OPQ~\cite{ge2013optimized, imi} with adaptive-resolution partial sort. {\color{red} Result - It saves up to ... dimensionality evaluation.} To verify the exactness of our method, we equip exhaustive linear scan with adaptive-resolution partial sort. {\color{red} It shows that for most queries, it reaches 100\% recall. with ... speed-up}. {\color{red} We also did parameter study.}

%We then design two novel adaptive algorithms, adaptive priority queue(AdaQ) and adaptive partial sort(AdaSort), which are classic textbook priority queue and partial sort algorithms equipped with our AdaSampling framework. AdaQ is a randomized Monte Carlo algorithm with high end-to-end probability to produce true results for all access operations of priority queue in a single query. It supports full functionality of conventional priority queue while avoids unnecessary dimensionality evaluation. AdaSort is also a randomized Monte Carlo algorithm. It efficiently finds {\color{red} true} KNNs within a given set of objects with high probability. We argue that almost all algorithms for KNN query can be equipped with our adaptive algorithms. In tree-based and graph-based methods, trivially replacing conventional priority queue with AdaQ brings {\color{red} up to ... improvement}. For any methods of filter-and-verification framework including hashing and quantization, AdaSort can be applied to find KNNs within generated candidates, {\color{red} which improves the efficiency by ...}
%It shows good empirical performance though its theoretical complexity $O(N_{can} D + K \log N_{can}$ is worse than that of partial sort $O(N_{can} D + N_{can} + K \log K)$. 

%In terms of compression, in spite of its significant speedup, it also inevitably introduces approximation error. Previous works show strong correlation between total error and average recall. However, we claim that, for a certain error, its impact on different queries might vary a lot. For example, given two database vectors $x_1,x_2\in \mathbb{R}^D $ and two query vectors $q_A, q_B \in \mathbb{R}^D $, we assume that their distance are given as $dist(x_1,q_A)=1,dist(x_2,q_A)=2;dist(x_1,q_B)=1,dist(x_2,q_B)=1.01$. Then, in condition of correctly sorting $x_1$ and $x_2$, query A tolerates $100\%$ multiplicative error while query B just tolerates $1\%$, which indicates the importance of query-aware analysis. 

%Thus, instead of approximation error, we investigate the reordering after compression for each individual query. Assuming that the KNNs are be reordered to $rank_1,...,rank_k$, to retrieve the KNNs, we'll need to evaluate the true distance for the nearest $\max_i rank_i$ objects in the compressed space. Empirically, we observe that $\max_i rank_i$ is much greater than $K$, so we name the phenomenon as KNN expansion and define $\alpha_K:= \frac{1}{K} \max_i rank_i $ to be expansion rate. 

%In terms of compression-based methods, in spite of their significant speedup, they also inevitably introduce approximation error, which might do harm to the recall of KNN queries. 

%Existing KNN algorithms can be generally categorized into four types: tree-based {\color{red} cite}, graph-based {\color{red} cite}, hashing-based {\color{red} cite} and quantization-based {\color{red} cite}. The first two types construct indexes to navigate KNN search, while the last two compress vectors to accelerate distance computation with approximation. 

%(1)Exact nearest neighbor search is hard. Researchers turn to approximate nearest neighbor search. trade-off between accuracy and latency, memory.

%% introduce with KNN Expansion
%Locality sensitive hashing(LSH) family~\cite{indyk1998approximate, datar2004locality, c2lsh, tao2010efficient} is one of the most popular hashing-based methods. Its framework was firstly proposed in \cite{indyk1998approximate}, targeting the KNN query in high-dimensional Hamming space. Later it was extended to Euclidean space in E2LSH~\cite{datar2004locality}. 
%The algorithm can be summarized as two steps: random projection and bucktization. To be specific, for a vector $\mathbf{o} \in \mathbb{R}^D $, first E2LSH projects it onto a random line $\mathbf{g} \in \mathbb{R}^D $ with each entry $g_i$ sampled from standard Gaussian distribution $g_i \sim \mathcal N (0, 1)$. The random line is uniformly partitioned into buckets $[kw,(k+1)w), k\in \mathbb{Z} $ of width $w$. Then after a uniform random shift $b \sim \mathrm{Unif} (0,w)$, it's assigned to the corresponding bucket.
%$$ h(\mathbf{o} ; \mathbf{g}, b ) = \left\lfloor \frac{\mathbf{o} \cdot \mathbf{g}  + b}{w}  \right\rfloor  $$

%{\color{red} Do I need to introduce more about LSH here? distance preserving, hashing function, search?}

%Recent LSH-based works~\cite{huang2015query, sun2014srs, lu2020vhp, zheng2020pm} drop physical bucktization while resort to distance computation under after-projection space. QALSH~\cite{huang2015query} computes and stores only the result of random projection $\mathbf{o} \cdot \mathbf{g} $ during the indexing phase. Then for a newly coming query, it virtually sets query-centered buckets by computing the distance on each random line. When $| \mathbf{o} \cdot \mathbf{g} - \mathbf{q} \cdot \mathbf{g}| < w/2$, it's defined to be a collision. SRS~\cite{sun2014srs} and PM-LSH~\cite{zheng2020pm} project vectors into extremely low-dimensional spaces and handle KNN query on it based on multi-dimensional indexes. VHP~\cite{lu2020vhp} conduct both the "query-aware" collision counting and distance computation. 

%and counts the number of within-threshold dimensions. SRS~\cite{sun2014srs} .



%Naturally, it can be combined with .


% (1)Methods for approximate nearest neighbor search are mainly categorized into tree-based, graph-based, locality sensitive hashing and quantization. The first two methods indexing a database with constructing topology ... . The last two improve the efficiency by simplifying distance comparison. Locality sensitive hashing converts distance computation to binary bucket detecting(need discussion). Quantization xxx. (3) Specifically, by .

%As the size of compressed code determines the tradeoff between efficiency and accuracy, it's always a question to seek for a good balance. To be specific, by setting a proper code size, we hope it to be accurate enough to keep the order of objects as well as not wasting time on more detailed computation. (2) A series of works provide some metrics to show the difficulty of a dataset. ratio constrast, local intrinsic dimensionality, which could be a hint to choose a proper code size. 

%However, with our observation on xxx datasets, we find that the code size needed varies largely among queries: for simple queries, we can distinguish KNNs apart from other objects with low dimensions, while for difficult queries, they are highly sensitive to compression error so precise distance comparison is needed. Thus, a fixed dimensionality is usually either redundant or inaccurate. For xx dataset, ...

%What's more, even within a query, the difficulty of comparing the distance-to-query of two objects still varies a lot. For example, it's easy to determine the order when $dis_{large}= 100 \cdot dis_{small}$ while hard for the case when $dis_{large}= (1+\epsilon ) \cdot dis_{small}$.

%Thus, based on the observation, we argue that it's necessary to evaluate different dimensions for each individual comparison. To be specific, for objects whose distances to the query are largely distinct, we only need low dimensions while for those with similar distances, we'll need to evaluate full dimensions. 

%One question is that we don't know the difficulty of an individual comparison a prior. To solve the issue, we design an adaptive framework by incrementally increasing dimensionality: for a given distance $d_{ \mathrm{min} }$ and an object $x$, supposing that currently we calculated distance in low-dimensional space $d_x^*$. If we don't have enough confidence on the statement that the true distance $d_x$ is greater than $d_{ \mathrm{min} }$ or not, we calculate more dimensions. 

%We determine the confidence with statistical hypothesis testing and concentration inequality of random projection. For distance between queries and objects, it shows that the probability distribution of the after-projection distance concentrates around the real distance. The tail probability decays extremely fast.(I know its precise expression, but I don't think reviewers know what sub-gaussian tail is.) 

%With the technique, we design Adaptive Priority Queue, which is widely used in KNN. (Even all KNN methods.) {\bf{Adaptive Priority Queue} } is designed for replacing ordinary classic priority queue where dynamic pop and push operations are supported. With front operation, we ensure it to return the nearest object in the queue with high confidence without calculating full-precision distance for all in-queue objects. Since classic priority queue is necessary for most tree-based and graph-based KNN methods, Adaptive Priority Queue brings improvements for these methods in a plugin manner. Plus, KNN methods usually follow the framework of filter-and-verificaiton, say indexes firstly generate candidates (filter) and then we sort them with true distance-to-query (verification). In verification phase, Adaptive Priority Queue returns the K Nearest Neighbor with high confidence without calculating full-precision distance for all in-queue objects. With the generality of Adaptive Priority Queue, we argue that random projection should be an accelerator for indexing methods instead of a competitor.

%We emphasize that compression based methods are also orthogonal to our adaptive framework. levels... (not introduced here, but let me think how to arrange the statement)

%Thus, our adaptive framework works as a plugin tool for most of the KNN methods. It helps index routing for tree and graph based methods, benefits all filter-and-verification methods and it can easily be combined with compression methods such as quantization.


% Our key contributions are summarized as below: 1) We identify the possibility to produce exact comparison results with approximate distance and highlight four requirements to realize it.
% %with empirical studies on real-world datasets
% 2) We propose adaptive dimension sampling framework, and design three algorithms 
% %adaptive-resolution search/search* and adaptive-resolution re-ranking 
% for universally enhancing existing KNN algorithms. 3) We conduct extensive experimental study on real-world datasets to verify the effectiveness of our techniques. 4) We provide rigorous theoretical analysis on the failure probability and expected complexity of our algorithms.

% The rest of this paper is organized as follows. Section 2 introduces preliminaries of KNN query, random projection and state-of-the-art AKNN algorithms. Section 3 presents our novel framework ADSampling and its applications on existing AKNN algorithms. Section 4 shows experimental results. Section 5 provides rigorous theoretical analysis. Section 6 discusses related works. Section 7 concludes the paper. 

%\begin{itemize}
%    \item We propose KNN expansion as a query-specific metric to evaluate the accuracy of a compressed code, illustrate the effect of random projection on it from both empirical and theoretical views and identifiesthe variance of difficulty in distance comparison. It reveals the capability and impossibility of random projection.
    
%    \item We propose a probabilistic adaptive distance comparison framework Adaptive Dimensionality Sampling based on random orthogonal transformation.
    
%    \item We design a novel data structure Adaptive Priority Queue. Adaptive Priority Queue can replace classic priority queue which supports dynamic push in, pop out and access the nearest object. It helps both {\color{red} filter and verification} phase. It's easy to use and simply improves nearly all methods for KNN.
    
%    \item {\color{red} We show the work's orthogonality to other methods, providing specific procedures for algorithm combination. It's easy to implement and can be applied with little overhead. }
    
%    \item We conduct extensive experiments showing the effectiveness and generality of the method. It shows significant improvement...xxx
    
%    \item We give rigorous theoretical studies on query-specific difficulty of random projection. The theory explains our observation well and provides a metric to evaluate the inherent difficulty/value of a particular query. 
%\end{itemize}

%{\color{red} The rest of this paper is organized as follows. Section 2 introduces preliminaries of random projection and KNN. Section 3 shows the variance of difficulty among queries. Section 4 presents our framework of adaptive distance comparison, Adaptive Priority Queue, and their applications on existing state-of-the-art KNN methods. Section 5 shows experimental results. Section 6 provides  theoretical analysis on query-dependent inherent difficulty. Section 7 presents related works. Section 8 concludes the paper.}