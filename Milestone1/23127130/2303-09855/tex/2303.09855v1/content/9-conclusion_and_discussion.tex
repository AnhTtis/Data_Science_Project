\section{Conclusion and Discussion}
\label{sec:conclusion}

% {\color{red} It's also trivial to give a space-efficient LSH like index structure by simply sampling from the rotated space. Based on the experiment of Section 3, a reasonable dimensionality is as low as around 30-40d. We also want to point out that even if we do KNN query with the space-efficient LSH like indexes, we'll need to evaluate a large amount of full-precision distance to select the true KNN due to the large KNN expansion rate. Thus, we highly suggest not to do so.}

We identify the distance comparison operation which dominates the time cost of {\CHENGB nearly all} AKNN algorithms and demonstrate opportunities to improve its efficiency. 
We propose a new randomized algorithm for the DCO which runs in logarithmic time wrt $D$ {\CHENGB in most cases} and succeeds with high probability. Based on it, we further develop one {\chengf generic} and two algorithm-specific techniques for AKNN algorithms. 
Our experiments show that the enhanced AKNN algorithms outperform the original ones consistently. 
We also provide rigorous theoretical analysis for all our techniques. 

We would like to highlight the following extensions and applications of our techniques.
(1) Our techniques can be trivially extended to two other {\chengf widely}-adopted similarity metrics, {\chengf namely} cosine similarity and inner product, via simple transformation. 
Specifically, the cosine-based similarity search on some given data and query vectors is equivalent to the Euclidean nearest neighbor search on their normalized data and query vectors where \texttt{ADSampling} is applicable.  
The inner product comparison of whether $\left< \mathbf{o}, \mathbf{q}  \right> \ge r$ can be reduced to the DCO of whether $\left\| \mathbf{o} / \|\mathbf{o} \|- \mathbf{q} / \|\mathbf{q} \| \right\|  \le \sqrt {2 - 2r / (\| \mathbf{o}\| \cdot \|\mathbf{q}\| )} $, where the distance threshold equals to $\sqrt {2 - 2r / (\| \mathbf{o}\| \cdot \|\mathbf{q}\| )} $~\footnote{It can be verified as follows: $\left< \mathbf{o}, \mathbf{q}  \right> \ge r \iff  \left< \mathbf{o} / \|\mathbf{o} \|, \mathbf{q} / \|\mathbf{q} \|\right> \ge r / (\| \mathbf{o}\| \cdot \|\mathbf{q}\| ) $
$\iff \| \mathbf{o} / \|\mathbf{o} \| \|^2 - 2\left< \mathbf{o} / \|\mathbf{o} \|, \mathbf{q} / \|\mathbf{q} \|\right> + \| \mathbf{q} / \|\mathbf{q} \|\|^2 \le 2 - 2r / (\| \mathbf{o}\| \cdot \|\mathbf{q}\| ) \\ \iff \left\| \mathbf{o} / \|\mathbf{o} \|- \mathbf{q} / \|\mathbf{q} \| \right\| ^2 \le 2 - 2r / (\| \mathbf{o}\| \cdot \|\mathbf{q}\| )$.}.
% The inner product comparison of whether $\left< \mathbf{o}, \mathbf{q}  \right> \ge r$ can be reduced to the DCO of vectors $\left\| \mathbf{o} / \|\mathbf{o} \|- \mathbf{q} / \|\mathbf{q} \| \right\| ^2 \le 2 - 2r / (\| \mathbf{o}\| \cdot \|\mathbf{q}\| )$.
% \begin{equation}
%     \left< \mathbf{o}, \mathbf{q}  \right> \ge r \iff \left\| \frac{\mathbf{o} }{ \|\mathbf{o} \|}- \frac{\mathbf{q} }{ \|\mathbf{q} \|} \right\| ^2 \le 2 - \frac{2r}{\| \mathbf{o}\| \cdot \|\mathbf{q}\| } 
% \end{equation}
(2) DCOs are also ubiquitous in many other {\chengf tasks} of high-dimensional data management and analysis such as clustering~\cite{kmeans} and outlier detection~\cite{outlier_detection}. Our techniques have the potential to accelerate existing methods for those {\chengf tasks} by reducing the cost of DCOs while keeping the accuracy. 

% We demonstrate that the distance comparison operation dominates the time cost of most AKNN algorithms and there 



% Distance comparison operations dominate the time cost of most AKNN algorithms. We show 
