
{\CHENG
\section{\texttt{AKNN++}: Improving \texttt{AKNN+} Algorithms with Algorithm Specific Optimizations}
\label{sec:aknn++}

\subsection{\texttt{HNSW++}: Towards More Approximation}
\label{subsec:hnsw++}
% \subsection{\texttt{ADSampling} for HNSW: Towards More Approximation}
% \label{subsec:adsampling-hnsw}

Recall that \texttt{HNSW+} maintains a result set $\mathcal{R}$ with a max-heap of size $N_{ef}$ and distances as keys, where $N_{ef} > K$. For each newly generated candidate object, it checks whether its distance is 
% at most 
{\JIANYANGLAST no greater than}
the largest distance (of an object) in $\mathcal{R}$ and if so, it inserts the object in the set $\mathcal{R}$. Specifically, it uses \texttt{ADSampling} to conduct the DCO for each candidate object with the largest distance in $\mathcal{R}$ as the threshold distance. We identify two roles played by the set $\mathcal{R}$. 
\underline{First}, it maintains the KNNs with the smallest distances among those candidates generated so far. These KNNs would be returned as the outputs of the algorithm at the end. 
\underline{Second}, it maintains the $N_{ef}^{th}$ largest distance among the candidates generated so far. This distance is used as the threshold distance of the DCOs through the course of the algorithm, whose results would affect how the candidates are generated. 
% (e.g., if a neighbor of a candidate in the graph involved in \texttt{HNSW+} has its distance at most the $N_{ef}^{th}$ distance in $\mathcal{R}$, it would be generated as a candidate). 
{\JIANYANG Specifically, if a candidate {\CHENGC generated by} \texttt{HNSW+} has its distance at most the $N_{ef}^{th}$ distance in $\mathcal{R}$, it would be {\CHENGB added to} the search set $\mathcal S$ for further candidate generation.}

This dual-role design is attributed to the fact that in \texttt{HNSW+}, \emph{exact} distances are used for fulfilling both roles. 
{\JIANYANG As shown in 
% Figure~\ref{fig:illustration HNSW+ v.s. HNSW++}(a)
Figure~\ref{fig:illustration HNSW+}, \texttt{HNSW+} always maintains $\mathcal{R} $ and $\mathcal{S} $ with exact distances (dark green), and the first $K$ objects in $\mathcal{R}$ are the KNN objects.}
Using the exact distances is desirable for the first role (of maintaining the KNNs) 
since the outputs of the algorithms are defined based on the exact distances. Yet we argue that it may not be cost-effective for the second role (of maintaining the $N_{ef}^{th}$ largest distance) since the procedure that uses this distance for generating candidates is a heuristic one (i.e., greedy beam search) and may still work well with an approximate distance. 

Therefore, we propose to decouple the two roles of $\mathcal{R}$ by maintaining two sets $\mathcal{R}_1$ and $\mathcal{R}_2$, one for each role {\JIANYANG (as illustrated in 
% Figure~\ref{fig:illustration HNSW+ v.s. HNSW++}(b)
Figure~\ref{fig:illustration HNSW++})}. 
%Set $\mathcal{R}_1$ serves the first role and set $\mathcal{R}_2$ serves the second one. 
Set $\mathcal{R}_1$ has a size of $K$ and is based on exact distances {\JIANYANG (dark green)}. Set $\mathcal{R}_2$ has its size of $N_{ef}$ and is based on distances, {\CHENGC which could be either exact or approximate}. Specifically, for each newly generated candidate, it checks whether its distance is 
% at most
{\JIANYANGLAST no greater than}
the maximum distance in set $\mathcal{R}_1$, and if so, it inserts the candidate in set $\mathcal{R}_1$. 
{\JIANYANG Furthermore, this DCO produces a by-product, namely the observed distance $dis'$ (light green) when {\CHENG \texttt{ADSampling} terminates}, which could be exact (if all $D$ dimensions are sampled) or approximate (if it terminates with $d<D$). }
% Furthermore, a by-product of this DCO is a distance of the candidate, which is exact if \texttt{ADSampling} for the operation samples all $D$ dimensions or approximate if it samples less than $D$ dimensions.
It then maintains the set $\mathcal{R}_2$ and the set $\mathcal{S}$ based on the 
% computed 
{\JIANYANG observed} distances similarly as \texttt{HNSW+} maintains $\mathcal{R}$ and $\mathcal{S}$, respectively. We call the resulting algorithm that is based on this decoupled-role design \texttt{HNSW++}. 

\begin{figure}[thb]
    \centering
    \vspace{-4mm}
    \captionsetup[subfigure]{aboveskip=-1pt}
   %  \begin{subfigure}[b]{0.32\linewidth}
   %      \includegraphics[width=\textwidth]{revision experimental result/IVFPQ.pdf}
   %      \caption{\texttt{IVFPQ}}
	  % \label{fig:cost IVFPQ}
   %  \end{subfigure} 
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=0.8\textwidth]{figure/illu_HNSW+.pdf}
        \caption{\texttt{HNSW+}}
        \label{fig:illustration HNSW+}
    \end{subfigure}   
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=0.8\textwidth]{figure/illu_HNSW++.pdf}
        \caption{\texttt{HNSW++}}
        \label{fig:illustration HNSW++}
    \end{subfigure}   
 %    \subfigure[\texttt{HNSW+}]{
 %        \includegraphics[width=0.45\linewidth]{figure/[illu]HNSW+.pdf}
 %    \label{fig:illustration HNSW+}
 %    }
 %    \subfigure[\texttt{HNSW++}]{
	% \includegraphics[width=0.45\linewidth]{figure/[illu]HNSW++.pdf}
	% \label{fig:illustration HNSW++}
 %    }
    \vspace{-4mm}
    \caption{\texttt{HNSW+} v.s. \texttt{HNSW++}}
    \vspace{-4mm}
    \label{fig:illustration HNSW+ v.s. HNSW++}
\end{figure}
% \smallskip\noindent\textbf{Failure Probability Analysis.} We note that different from \texttt{HNSW+}, which would return the same results as \texttt{HNSW} with high probabilities, \texttt{HNSW++} does not aim to return the same results as \texttt{HNSW} {\JIANYANG (though in practice, it returns nearly the same results). 
% \smallskip\noindent\textbf{Time-Accuracy Tradeoff.} 
\smallskip\noindent\textbf{{\CHENGB Theoretical Analysis}.}
We note that different from \texttt{HNSW+}, which would return the same results as \texttt{HNSW} with high probability, \texttt{HNSW++} does not aim to return the same results as \texttt{HNSW} {\JIANYANG (though in practice, it returns nearly the same results as verified in Section~\ref{subsub:dimensions and recall}). 
% Therefore, \texttt{HNSW++} does not provide the guarantee as Theorem~\ref{theorem: AKNN+ failure probability}. However, since it maintains KNN set $\mathcal R_1$ with the \emph{nearly-exact} DCO algorithm \texttt{ADSampling}, it guarantees to return the KNNs of generated candidates. 
% We note that in Theorem~\ref{theorem: AKNN+ failure probability} when aiming at preserving the results, the algorithm is supposed to produce correct results for all DCOs. Thus, the overall failure probability should be derived from the union bound over all objects with $dis\le r$ (because the failure probability of $dis > r$ is $0$), whose number is $O(K\log N)$. However, in order to return the KNNs of generated candidates, producing correct results for the KNNs is sufficient because in this case, we obtain exact distance of the KNNs, which would be sufficient to keep them in the KNN set $\mathcal R_1$. Thus, applying union bound over these $K$ candidates, we have the following theorem.
{\CHENG Specifically, \texttt{HNSW++} would generate a set of candidates, which might be different from that of \texttt{HNSW+} or \texttt{HNSW}. 
{
% \JIANYANG However, it's worth noting that \texttt{HNSW++} still maintains KNNs with \texttt{ADSampling}, which guarantees the high probability of finding out KNNs among generated candidates (though the candidates can be different from those of \texttt{HNSW}). Then under this type of guarantee, its time-accuracy tradeoff is provided in Corollary~\ref{corollary: find out KNNs}.
Among the generated candidates, \texttt{HNSW++} guarantees to return their KNNs with high probability because it still maintains KNNs with \texttt{ADSampling}, {\CHENG and its guarantee is the same as the one in Corollary~\ref{corollary: find out KNNs}}. 
% Then under such type of guarantee, i.e., returning KNNs of generated candidates (though the candidates can be different from those of \texttt{HNSW} and \texttt{HNSW+}), its time-accuracy tradeoff is provided in Corollary~\ref{corollary: find out KNNs}.
}
}
% Among the generated candidates, \texttt{HNSW++} would return the KNNs with a high probability as shown in the following theorem.
}

% \begin{corollary}
% The probability that \texttt{HNSW++} fails to return the KNNs of generated candidates is at most $\exp \left( -c_0 \epsilon_0^2 + \log (DK) \right)$. %where $N_{pos}$ is the number of DCOs for positive objects.
% \label{theorem: HNSW++ failure probability}
% \end{corollary}
% \begin{proof}
% Different from the analysis of Corollary~\ref{theorem: AKNN+ failure probability}, to return KNNs of generated candidates only requires the success of the DCOs of KNN objects rather than all positive objects. This is because with the success of KNN objects, we can obtain their exact distance, which makes sure we return them as final answers. As a result, in this case, we apply union bound of all KNN objects instead of all $O(K \log N)$ positive objects.
% \end{proof}
% }
% %Therefore, we would not define a failure probability for \texttt{HNSW+}.

% % {\JIANYANG
% %In terms of theoretical guarantee, \texttt{HNSW++} does not guarantee to preserve the full process of greedy beam search. 

% %However, since it does KNN checking with ADSampling for all candidates, it provides the guarantee of returning KNNs of generated candidates.
% % %another type of guarantee , i.e., guarantee to return KNNs of the generated candidates. 
% % Under the context of such type of failure, we provide the following theorem to show the time-accuracy tradeoff of ARRoute. 

% % \begin{corollary}
% % Let $\delta$ be an upper bound of the probability of failing to find out KNNs of generated candidates. For a DCO with threshold $r$ and a negative object. Let $(1+\alpha)$ be the ratio between $dis$ and $r$, $\alpha >0$. The expected terminate dimensionality is 
% % \begin{align}
% %     \mathbb{E} \left[ \hat D \right]  = O \left[ \min \left( D, \frac{1}{\alpha ^2} \log \frac{D\cdot K}{\delta}  \right)  \right] 
% % \end{align}

% % }

% \smallskip\noindent\textbf{Time Complexity Analysis.} 
% % Similar to \texttt{AKNN+}, \texttt{AKNN++}'s time cost consists of $C_1$ for DCO of \texttt{ADSampling} and $C_2$ for the rest computations. 
% % With the same analysis of Theorem~\ref{theorem:AKNN+ time-accuracy}, we provide the time complexity results of \texttt{HNSW++} in the following theorem. 
% Similarly as we analyze the time complexity of \texttt{AKNN+}, we analyze the time complexity of \texttt{AKNN++} and present the results as follows.

% \begin{corollary}
% Let $\delta'$ be the probability that \texttt{HNSW++} fails to return the KNNs of the generated candidates. The expected time complexity of \texttt{HNSW++} is $C_1 + C_2$.
% For $C_1$, we have
% \begin{align}
%     C_1 = O \left( DK \log N + c_\alpha N_s \log \frac{DK}{\delta'} \right) 
% \end{align}
% where $c_\alpha  =  \frac{1}{N_{s}} \sum_{i=1}^{N_{s}} \min \left( \alpha_i^{-2}, D / \log \frac{DK}{\delta'}  \right)$ and $\alpha_i$ is the $alpha$ value of the $i^{th}$ candidate object.
% For $C_2$, we have $C_2 = O(N_s \log N_s)$. 
% % ***In this theorem, we can define $\delta'$ as the failure probability for \texttt{AKNN+}, which should be based on $\delta$.***
% \label{theorem:HNSW++ time-accuracy}
% \end{corollary}
% \begin{proof}
% It follows directly from Corollary~\ref{theorem: HNSW++ failure probability} and Lemma~\ref{theorem:ADSampling efficiency}.
% \end{proof}

% ***Jianyang, please help to include the time complexity of \texttt{HNSW++} here. ***

\smallskip\noindent\textbf{\texttt{HNSW++} v.s. \texttt{HNSW+}.}
Compared with \texttt{HNSW+}, \texttt{HNSW++} is expected to have a better time-accuracy trade-off, which we explain as follows. \underline{First}, consider the time cost. In \texttt{HNSW++}, for each DCO, the threshold distance is the $K^{th}$ largest distance, which is smaller than that used in \texttt{HNSW+} (i.e., the $N_{ef}^{th}$ largest distance). Correspondingly, in \texttt{HNSW++}, the $\alpha$ value, which is defined as $(dis - r)/r$, is larger than that in \texttt{HNSW+}. Therefore, the time cost for this DCO would be smaller than that in \texttt{HNSW+} according to the time complexity analysis of \texttt{ADSampling} in Section~\ref{subsection:theoretical analysis of ADSampling}. \underline{Second}, consider the effectiveness. While \texttt{HNSW++} and \texttt{HNSW+} use different distances for generating the candidates, we expect that they would generate candidates with similar qualities given that (1) the distances used by the two algorithms should be close (or the same in some cases) and (2) the method used for generating candidates, i.e., greedy beam-search, has a heuristic nature and there is no strong clue that it favors exact distances over approximate ones.

{ \JIANYANG
\noindent\textbf{Remarks.} We note that the technique of \texttt{HNSW++} can also be used in other 
% graph-based methods~\cite{malkov2018efficient, li2019approximate, FuNSG17, fu2019fast, diskann, NSW}.
graph-based methods~\cite{malkov2018efficient, li2019approximate, fu2019fast, diskann, NSW}.
This is because these algorithms also apply the greedy beam search {\CHENGC based on a set $\mathcal{R}$} in the query phase.
}

\subsection{\texttt{IVF++}: Towards Cache Friendliness}
\label{subsec:ivf++}
% \subsection{\texttt{ADSampling} for IVF: Towards Cache Friendliness}
% \label{subsec:adsampling-ivf}
}

% \section{Adaptive Resolution Algorithms for AKNN}
% As discussed in Section 1, DCO is ubiquitous and vital in existing AKNN algorithms. Thus, a straight-forward idea is to simply replace the plain DCO with our ADSampling framework, which forms our first basic algorithm ARSearch. Then by exploiting the properties of existing AKNN algorithms, we further propose adaptive resolution route (ARRoute) and adaptive resolution select (ARSelect), which bring more significant improvement.


% \subsection{Adaptive Resolution Search}
% \label{subsection:ARSearch}

% % \begin{figure}[thb]
% %     \centering
% %     \includegraphics[width=\linewidth]{figure/workflow.pdf}
% %     \caption{Workflow of Adaptive Algorithms}
% %     \label{fig:workflow}
% % \end{figure}

% % {\CHENG 14-07-Cheng: It looks that the above figure is for all three adaptive resolution algorithms but not for ARSearch only. Not sure if we can figures to illustrate different adaptive resolution algorithms.}

% % {\JIANYANG 15-07-Jianyang: Yes, it is for all three adaptive resolution algorithms. The workflows of all of them are identical except for the "Querying with Adaptive Algorithms" (red) part. The figure wants to emphasize that our algorithms just need a simple rotation.}

% ARSearch is a randomized algorithm which with high probability fully preserves the semantics of the original AKNN algorithms while be able to skip unnecessary dimension evaluation. Its workflow is extremely simple. During index phase, we simply apply random orthogonal transformation $P'$ on all database vectors and feed them into an existing AKNN index algorithm. During query phase, we first transform the query with $P'$, and feed it into the corresponding AKNN search algorithm while replacing DCO with ADSampling. Specifically, for graph-based methods which represent the dynamic cases, we apply ADSampling when comparing a newly visited object $i$ with the maximum in the result set $\mathcal R$. For IVF which represents the static cases, ADSampling is applied during selecting final KNNs from generated candidates, i.e. when scanning a new candidate, we compare it with the maximum in the currently maintained KNN set $\mathcal K$.

% {\JIANYANG We then investigate \emph{the failure probability of ARSearch}, where its failure is formally defined to be \emph{failing to preserve the full search process of an AKNN algorithm.} Let's consider when ARSearch would fail. \underline{First}, for graph-based methods, we are dynamically visiting new candidates and comparing their distance with the maximum in the result set $\mathcal R$ to determine whether to update the result set $\mathcal R$ and the search set $\mathcal S$. \underline{Second}, for IVF, we are scanning generated candidates and comparing their distance with the maximum in the KNN set $\mathcal K$. Suppose that none of DCOs fail in ARSearch, then for negative objects, we successfully prevent them from updating $\mathcal R$ or $\mathcal K$. In terms of the positive objects, we successfully evaluate their exact distance and use them to update $\mathcal R$ or $\mathcal K$, which also correctly preserve the search process. As a result, the success of all DCOs in ARSearch is a \emph{sufficient condition} of the success of ARSearch. Note that the ADSampling has no false negative failure. Thus, we have the following corollary:

% \begin{corollary}
% Let $\delta$ be an upper bound of failure probability of ARSearch. For a DCO with threshold $r$ and a negative object. Let $(1+\alpha)$ be the ratio between $dis$ and $r$, $\alpha >0$, and $N_{pos}$ be the number of positive objects, $N_{pos} \le N$. The expected terminate dimensionality is 
% \begin{align}
%     \mathbb{E} \left[ \hat D \right]  = O \left[ \min \left( D, \frac{1}{\alpha ^2} \log \frac{D\cdot N_{pos}}{\delta}  \right)  \right]  \label{equation: ARSearch time complexity}
% \end{align}
% The overall expected time complexity of ARSearch is
% %of DCOs in ARSearch is 
% \begin{align}
%     %O \left( DK \log N + c_\alpha  N_{s}\log \frac{DK}{\delta}  + c_\alpha  N_{s} \log\log N_s \right) \label{equation: overall ARSearch time complexity}
%     O \left( c_\alpha  N_{s}\log \frac{D}{\delta}  + c_\alpha N_s \log K  + c_\alpha  N_{s} \log\log N_s \right) \label{equation: overall ARSearch time complexity}
% \end{align}
% where $c_\alpha  =  \frac{1}{N_{s}} \sum_{i=1}^{N_{s}} \min \left( \alpha_i^{-2}, D / \log \frac{DK \log N_s}{\delta}  \right) $.
% \label{corollary:ARSearch}
% \end{corollary}

% \begin{proof}
% With Lemma~\ref{theorem:ADSampling accuracy} and union bound, we have 
% \begin{align}
%     \mathbb{P} \left\{ \emph{failure of ARSearch} \right\} \le  \exp \left( -c_0 \epsilon_0^2 + \log D \cdot N_{pos} \right) 
% \end{align}
% Then let the failure probability be $\delta$, we can obtain the corresponding $\epsilon_0$. Then by substituting $\epsilon_0$ in Lemma~\ref{theorem:ADSampling efficiency}, we have the Equation~\ref{equation: ARSearch time complexity}.
% % Like the proof of Theorem~\ref{theorem:time-accuracy of ADSampling}, we first show how $\epsilon_0$ controls failure probability and time complexity with Lemma~\ref{theorem:ADSampling accuracy} and \ref{theorem:ADSampling efficiency}, and them combine them to get the overall time-accuracy tradeoff.
% % First, with Lemma~\ref{theorem:ADSampling accuracy} and union bound, we have 
% % \begin{align}
% %     \mathbb{P} \left\{ \emph{failure of ARSearch} \right\} \le N_{pos}  \cdot \exp \left( -c_0 \epsilon_0^2 + \log D \right) 
% % \end{align}
% % Then, let the failure probability be $\delta$, we obtain 
% % \begin{align}
% %     \epsilon_0 = O \left( \sqrt {\log \frac{D\cdot N_{pos}}{\delta} }  \right) 
% % \end{align}
% % Substitute the $\epsilon_0$ in Lemma~\ref{theorem:ADSampling efficiency}, we proved Equation~\ref{equation: ARSearch time complexity}.

% In terms of $N_{pos}$, we analyze the $\hat N_{pos}$ of a random permutation as its upper bound because an AKNN algorithm is expected to generate candidates in a better order than random permutation. Note that this is a classic problem of "the expected number of updates of $K$ maxima", whose result is given by $\mathbb{E} \left[ \hat N_{pos} \right] \approx K\log N $. Due to the limit of space, we omit its proof here.
% % We then analyze the order of $N_{pos}$. Instead of the $N_{pos}$ of an AKNN algorithm, we analyze the $\hat N_{pos}$ of a random permutation as an upper bound because an AKNN algorithm is expected to generate candidates in a better order than random permutation. Let $I_i$ be the indicator random variable of rank-$i$ object. In particular, $I_i=1$ ($I_i=0$) indicates the rank-$i$ object would have positive (negative) comparison results. Then
% % \begin{align}
% %     \mathbb{E} \left[ \hat N_{pos} \right]  = \mathbb{E} \left[ \sum_{i=1}^{N} I_i \right]  = \sum_{i=1}^{N} \mathbb{E} \left[ I_i \right] 
% % \end{align}
% % Then looking into $I_i$, we notice that the rank-$i$ is positive if and only if there are no more than $K-1$ smaller objects on its left side, which means that within the top $i$ objects, the rank-$i$ object should be placed at $1-K$th position in the permutation, whose probability is $K / i$. Thus, $\mathbb{E} \left[ I_i \right] = K / i  $. As a result, 
% % \begin{align}
% %     \mathbb{E} \left[ \hat N_{pos} \right] \le \sum_{i=1}^{N} K / i = K \sum_{i=1}^{N} 1 / i
% % \end{align}
% % %where the last equation is due to harmonic series. 
% % Note that it's a harmonic series, we have
% % \begin{align}
% %     N_{pos} = O(K \log N)
% % \end{align}
% Then note that in ARSearch for DCOs of $N_{pos}$ positive objects, the time complexity of each single is $O(D)$ and for those of negative objects, that of each single is given in Equation~\ref{equation: ARSearch time complexity}. We have the overall complexity given in Equation~\ref{equation: overall ARSearch time complexity}.
% \end{proof}
% } 

% %We then investigate its failure probability, i.e. the probability of failing to preserve the semantics of the original AKNN algorithms. Let's first consider when our algorithm would fail. For graph-based methods, we're consistently visiting new objects and using them to update the result set $\mathcal R$ and the search set $\mathcal S$. However, only part of the visited objects could affect the search process, while others never become the minimum of the search set $\mathcal S$. We refer to those that affect the search as \textit{the positive} and those not as \textit{the negative}. Let $N_{pos}$ be the number of positive objects. Then once we ensure these $N_{pos}$ objects are correctly leveled up to $L$ and used to update $\mathcal R$ and $\mathcal S$, the result of search will be preserved. 
% %For IVF, similarly, once we ensure the KNNs among the candidates are correctly leveled up to $L$ and used to update $\mathcal K$ (as a result, $N_{pos}=K$ for IVF), the result of search will be preserved. 
% %{\JIANYANG In terms of IVF, similarly, a failure happens only when we wrongly rejected some KNN objects (positive) at level $l<L$. We emphasize that non-KNN objects (negative) do not affect the failure probability. Specifically, if a non-KNN object is rejected at level $l<L$, then it's correctly rejected. Otherwise, if it's leveled up to $L$, then its exact distance is obtained and an exact comparison can be made. As a result, in terms of the problem of DCO, our ADSampling framework will never produce negative positive results. } 
% %Recall that as introduced in Section 3.1, the failure probability is controlled by parameter $\epsilon_1$. It should be large enough to ensure that $N_{pos}$ objects would not be wrongly rejected at some level $l$. Following these thoughts, we have the following theorem whose detailed proof is later given in Section 5.

% % \begin{theorem}
% % For a KNN query in $D$-dimensional space, given an AKNN algorithm, let $N_{pos}$ be its number of positive objects. Replacing plain DCO with ADSampling, the error bound parameter
% % \begin{align}
% %     \epsilon_1 = O \left( \sqrt { \log D + \log \frac{N_{pos}}{\delta} }  \right) 
% % \end{align}
% % guarantees the probability of failing to fully preserve the results of the algorithm to be no greater than $\delta$. 
% % \label{theorem:eps}
% % \end{theorem}

% % {\JIANYANG
% % alternative expression

% % \begin{theorem}
% % For a KNN query in $D$-dimensional space, given an AKNN algorithm, let $N_{pos}$ be its number of positive objects and $L$ be the number of levels of ADSampling. Replacing plain DCO with ADSampling, the probability of failing to fully preserve the results of the algorithm decays super-exponentially with respect to $\epsilon_1$: 
% % \begin{align}
% %     \mathbb{P} \left\{ fail \right\}  \le \exp \left( -c_0 \cdot \epsilon_1^2 + \log L + \log N_{pos} \right) 
% % \end{align}
% % \label{theorem:eps_alter}
% % \end{theorem}
% % }

% % {\CHENG 14-07-Cheng: (1) Better to comment on the above theoretical result (e.g., what we can interpret from this result); (2) Would it possible to collect some empirical results for this theoretical result, e.g., we vary $\epsilon_1$ and show the failure probability results?}

% % {\JIANYANG 15-07-Jianyang: (1) I think this theorem is very counter-intuitive and it would be hard to provide intuitive interpretation. Actually, the format of this theorem is quite similar to Johnson-Lindenstrauss Lemma, which is seen by all to be quite counter-intuitive... (2) Yes. I think this experiment might need to be conducted by combing ARSearch with linear scan to eliminate the error introduced by AKNN algorithms. What do you think of it?}

% %Under the condition of Theorem~\ref{theorem:eps}, we then talk about the benefits ARSearch brings. As discussed in Section 3.1, our methods bring acceleration by avoiding unnecessary dimension evaluation when the distance of object $i$ is greater than its corresponding threshold $r$ of DCO. We're curious about how many dimensions are needed to reject object $i$. Formally, assuming that we do hypothesis testing every time after sampling one dimension, let random variable $\hat D_i$ be the terminate dimension of object $i$. We want to figure out its expected value $\mathbb{E} \left[ \hat D_i \right] $.

% %An obvious fact about $\mathbb{E} \left[ \hat D_i \right]  $ is that the more different $dis_i$ and $r$ are, the fewer dimensions are needed. Let $(1+\alpha_i)$ be the ratio between $dis_i$ and $r$, $\alpha_i>1$ (though we don't know $dis_i$ and $\alpha_i$ in prior). 
% %We claim that $\alpha_i$ is the maximum allowed multiplicative error (minimum resolution) to correctly compare object $i$ and threshold $r$, because with an error larger than $\alpha_i$, even if we have the precise estimation of $dis_i$, we cannot conclude the comparison results. 
% %We show that our methods reach the theoretically optimal result, i.e. rejecting an object with its minimum needed resolution, with the following theorem whose detailed proof is also given in Section 5.

% % \begin{theorem}
% % Under the condition of Theorem~\ref{theorem:eps}, let $(1+\alpha_i)$ be the ratio between a negative object $i$ and its corresponding distance threshold $r$. The expected terminate dimension of $i$ is 
% % \begin{align}
% %     \mathbb{E} \left[ \hat D_i \right]  = O \left[   \min \left( D, \frac{1}{\alpha_i^2}\log D + \frac{1}{\alpha_i^2}\log \frac{N_{pos}}{\delta} \right)  \right]  
% % \end{align}\label{theorem:efficiency}
% % \end{theorem}


% % {\JIANYANG
% % alternative expression
% % \begin{theorem}
% % Let $(1+\alpha_i)$ be the ratio between the exact distance of a negative object $i$ and its corresponding distance threshold $r$. The expected terminate dimensionality of $i$ is: 
% % \begin{align}
% %     \mathbb{E} \left[ \hat D_i \right]  = O \left[   \min \left( D,  \frac{1}{\alpha_i^2} \epsilon_1^2 \right)  \right]  
% % \end{align}
% % \label{theorem:efficiency_alter}
% % \end{theorem}

% % }


% % {\JIANYANG

% % Combing Theorem~\ref{theorem:efficiency_alter} and \ref{theorem:eps_alter}, we have the following corollary:

% % \begin{corollary}
% % For a KNN query in $D$-dimensional space, given an AKNN algorithm, let $N_{pos}$ be its number of positive objects and $(1+\alpha_i)$ be the ratio between a negative object $i$ and its corresponding distance threshold $r$. To guarantee the failure probability no greater than $\delta$, the expected terminate dimensionality is: 
% % \begin{align}
% %     \mathbb{E} \left[ \hat D_i \right]  = O \left[   \min \left( D, \frac{1}{\alpha_i^2}\log D + \frac{1}{\alpha_i^2}\log \frac{N_{pos}}{\delta} \right)  \right]  
% % \end{align}
% % \label{corollary:Npos}
% % \end{corollary}

% % }

% {\JIANYANG
% TODO: discussion on JL Lemma is commented first
% \begin{comment}

% We compare the corollary with the seminal work of random projection, Johnson-Lindenstrauss Lemma~\cite{johnson1984extensions} (JL Lemma), which states that with the failure probability of at most $\delta$, random projection preserves mutual distance among a finite set $\mathcal X$ of $N$ objects with at most $\epsilon$ multiplicative error when reducing dimensionality to $\Theta(\epsilon^{-2} \log \frac{N}{\delta})$, which is very similar to Corollary~\ref{corollary:Npos}. However, our algorithm achieves adaptivity by reducing the dimensionality according to the unknown $\alpha_i$. It's also worth noting that $N_{pos} < N$ (usually $N_{pos}$ is at the order of $K$, which is independent of the size of the databse). 

% Also according to JL Lemma, we conjecture that the tight complexity should be 
% \begin{conjecture}
% For a KNN query in Euclidean space, given an AKNN algorithm, let $N_{pos}$ be its number of positive objects, object $i$ be a negative object and $(1+\alpha_i)$ be the ratio between $dis_i$ and distance threshold $r$. To guarantee the failure probability no greater than $\delta$, the expected terminate dimensionality is
% \begin{align}
%     \mathbb{E} \left[ \hat D_i \right]  =O \left[ \min \left( D, \frac{1}{\alpha _{i}^{2} } \log \frac{N_{pos}}{\delta}  \right)  \right] 
% \end{align}
% \end{conjecture}
% It requires more careful theoretical analysis, which we leave as future work. 
% \end{comment}
% }

%Note that due to the recoverability of our method, when the comparison is extremely fragile ($\alpha_i$ is extremely small), ADSampling guarantees to produce exact DCO results with $D$ dimensions, which makes sure that it's at least no worse than the plain comparison. Plus, the theorem also shows that the complexity largely depends on the ratio $\alpha_i$, i.e. increasing the ratio can reduce the complexity, which motivates our following proposed algorithms.

%In ARSearch, the definitions of positive and negative are slightly different from that of KNN query. Note that in graph routing, some vertices are visited but never become the minimum object in the search set $\mathcal S$. Thus, they have no effect on the following search path. We refer to those that affect the following search as \textit{vertices in the path} and those not as \textit{vertices out of the path}. Now our task is to distinguish apart those in the path and those out of the path.

%According to our discussion, leveling up positive objects to level $L$ releases more space for approximation. However, in practice, we have no prior knowledge on the positive objects, i.e. when visiting a vertex, we don't know whether it'll affect the search path or not. Thus, we can only make an inference based on current searching results. Specifically, in standard greedy beam search, a necessary condition that an object becomes positive is that it should be smaller than $\max_{j\in \mathcal R} dis_j$. Thus, we compare a newly visited object with $\max_{j\in \mathcal R} dis_j$ to determine its dimensionality, which leads to our algorithm ARSearch:

%\input{pseudocode/ARSearch (algorithm package)}

%The main difference between ARSearch and standard greedy beam search lies line 13 to line 21. When visiting a vertex, standard greedy beam search always evaluates exact distance, while our method applies adaptive dimension sampling so that evaluates fewer dimensions when the visited object is negative. When it's positive, it'll always be leveled up to $L$, so that like the standard search, $\mathcal{R}$ and $\mathcal S$ only contain objects with exact distance.

% {\CHENG 14-07-Cheng: As we have discussed, (1) we can think about use shorter names for the three adaptive resolution algorithms; (2) re-organize the algorithms by whether they are general or specific for one scenario.}

% {\JIANYANG modified}

% {\JIANYANG
% Besides the general ARSearch for all AKNN algorithms, we also propose two specific methods: ARRoute and ARSelect corresponding to graph-based AKNN algorithms (represented by \texttt{HNSW}) and those AKNN algorithms generating candidates in batch (represented by \texttt{IVF}).
% }

% \subsection{Adaptive Resolution Route}
% \label{subsection: adaptive resolution route}
% {\JIANYANG
% We first optimize our algorithms for graph routing. Based on Corollary~\ref{corollary:ARSearch}, the complexity of DCO in ARSearch is largely determined by $\alpha = \frac{dis}{r} - 1 $ (When $dis < r$, we let $\alpha =0^+$, i.e., an extremely small positive number). To increase $\alpha$, since the exact distance of a candidate is always fixed, we target to decrease the distance threshold $r$.

% Recall that in ARSearch for graph-based methods, we compare candidates with the current $N_{ef}$th NN so as to preserve the full search process and consequently preserve the returned AKNNs. However, rather than preserving the full search process, our actual goal is to retrieve KNNs. It suggests that fully preserving the search process might not be necessary. We may allow some approximation on the search process while do nearly-exact DCOs for maintaining KNNs. In this case, we can do DCO with a smaller threshold, i.e. the $K$th NN instead of the $N_{ef}$th NN.

% Following these thoughts, we propose our adaptive resolution route (ARRoute) algorithm. In ARRoute, we consider that besides $\mathcal {R,S}$ we maintain an extra KNN set $\mathcal K$. For a new candidate, we first compare its distance with the maximum in $\mathcal K$ to maintain KNNs. Note that such comparison will produce an observed distance as a by-product (it can be exact or approximate) when terminating sampling. Then we use the observed distance to maintain $\mathcal R$ and $\mathcal S$ for greedy beam search. We notice that such observed distance has a promising property for graph routing, i.e., the closer one candidate is to the query ($dis$ is small and as a result $\alpha$ is small), the finer its observed distance would be (terminate dimension $\hat D$ would be large), which matches the importance of distance information for graph routing. Thus, potentially, it only introduces slight random perturbation on the heuristic greedy beam search, which might not harm the performance. We empirically verify this statement in Section~\ref{section:experiment}. 

% %we compare its distance with the maximum in $\mathcal K$. Then for two possible cases, we have the following discussion. (1) It's smaller and we obtain its exact distance. In this case, we first update the KNN set.

% %Specifically, besides result set $\mathcal R$ and search set $\mathcal S$, we explicitly maintain a KNN set $\mathcal K$. 




% %In ARRoute, to explicitly maintain KNNs, we propose to.

% %which motivates us to explicitly maintain a KNN set $\mathcal K$ besides the result set $\mathcal R$ and the search set $\mathcal S$ during graph routing.


% %As dicussed in Section~\ref{subsec:aknn}, DCOs with the $N_{ef}$th NN have two functions. (1) Implicitly maintaining KNN set. (2) Guiding the search process of greedy beam search. A natural question is that can we do KNN checking with nearly-exact DCOs while allows some approximation in generating search process?  

% %Thus, a natural question is that with the purpose of returning high quality AKNNs, can we do DCO with a smaller distance threshold, e.g., the distance of the current $K$th NN? 


% %We then investigate the obstacles when we do DCO with the $K$th NN. 

% %Furthermore, note that the returned AKNNs are the first $K$th objects in the result set $\mathcal R$. Thus, the result set $\mathcal R$ actually plays a dual role. (1) It \emph{implicitly maintains currently searched KNNs}. (2) It maintains greedy beam search. 

% %Combined with the discussion above, we propose to do DCO with $K$th NN. 

% %maintain currently searched KNNs with exact.


% %to preserve the returned AKNNs, the aforementioned condition is not \emph{necessary}. Suppose that we have obtained all the candidates generated from graph routing (we'll specify later how we actually generate them). We claim that for each candidate, doing DCO with the currently searched $K$th NN (denoted as $dis_{i_K}$) is enough (note that $K < N_{ef}$, so the distance threshold is decreased). Moreover, the success of all DCOs between KNN objects and the currently searched $K$th NN (technically, we maintain a max-heap $\mathcal K$ by exact distance as the KNN set to realize so) is \emph{sufficient and necessary} to preserve the returned AKNNs. 

% %In terms of \underline{sufficiency}, when evaluating a candidate, there are two cases. (1) The candidate is a KNN object. With the assumption of its success in DCO (it returns positive results because it's a KNN object), we can obtain its exact distance and use it to update $\mathcal K$. (2) The candidate a non-KNN object. When the current $\mathcal K$ is filled with KNN objects, since ADSampling does not produce false positive, it must correctly return negative results. When the current $\mathcal K$ still contains some non-KNN objects. We don't care about the comparison results because later when we meet KNN objects, all non-KNN objects will be successfully replaced. Thus, the success of the DCOs of all KNN objects is sufficient to preserve the returned AKNNs. In terms of \underline{necessity}, suppose that there is a failure of DCO of a KNN object. Then it cannot correctly update $\mathcal K$. As a result, it fails to preserve the returned AKNNs. Thus, to preserve the returned AKNN, doing DCO with the currently searched $K$th NN is safe and efficient.

% %Let's get back to an aforementioned by yet to be solved problem, i.e., how to generate candidates for graph routing when we only compare distance with the $K$th NN. Note that in both original graph routing and ARSearch, the exact distance of all objects in the search process will be obtained and used for heuristic greedy beam search. However, in our new setting of comparing distance with $dis_{i_K}$, when the distance a new candidate is larger than $dis_{i_K}$, we cannot obtain its exact distance. We resolve this issue with a by-product of \texttt{ADSampling}, i.e., the observed distance obtained when terminating sampling (termed terminate distance). 

% %In \texttt{ADSampling}, recall that when an observed distance falls into the rejection region, we terminate sampling. Such terminate distance is an estimator of the true distance and has good properties for graph routing, i.e., the smaller its distance is (corresponds to a small $dis$ and as a result, a small $\alpha$), the more accurate the estimator would be (corresponds to a larger $\mathbb{E} \left[ \hat D \right]  $). It implies that it would be coarse when a candidate is far away from the query and fine when it's close, which matches the importance of distance information for graph routing. We


% %We also note that greedy beam search is highly heuristic. Though using terminate distance for graph routing introduces slight random perturbation on the search process, we expect it to do little harm to its performance on AKNN query. We empirically verify this statement in Section~\ref{figure:evaluated_dimension}. 

% %Gathering all the components above, we propose our adaptive resolution route (ARRoute) algorithm. As described in Algorithm~\ref{code:ARRoute}, technically, instead of maintaining two sets in ARSearch: a result set $\mathcal R$ ({\JIANYANG technically, a max-heap by } exact distance) and a search set $\mathcal S$ ({\JIANYANG a min-heap by } exact distance), here we maintain three sets: a result set $\mathcal R$ ({\JIANYANG a max-heap by terminate distance}), a search set $\mathcal S$ ({\JIANYANG a max-heap by terminate distance}) and a KNN set $\mathcal K$ ({\JIANYANG a max-heap by } exact distance). During search, we compare a newly visited object with the current maximum of $\mathcal K$. When it returns positive results $dis_i \le r$, then it's used to update $\mathcal K$ (line 12-18) as well as updating $\mathcal R$ and $\mathcal K$ (line 19-22). If it returns negative $dis_i > r$ and only obtain approximate distance $dis_i'$ unlike what we did in ARSearch, we don't drop it directly. Instead, we still use its $dis_i'$ to update $\mathcal R$ and $\mathcal S$ (line 19-22). 


% }

%We next further optimize our adaptive resolution algorithm for graph routing. Note that for preserving the full greedy search path, ARSearch does ADSampling to compare a new object $i$ and the maximum of the result set $\mathcal R$, whose size $N_{ef}$ is usually larger than $K$, meaning that we are comparing $i$ with the current $N_{ef}$NN. However, to check whether a newly visited object can update the answers, we only need to compare it with the current KNN. Since the distance of $N_{ef}$NN is larger than that of KNN, it actually decreases the distance ratio $\alpha_i$ so as to increase complexity. 
%, which decreases the ratio $\alpha_i$ as well as increasing complexity. In a word, fully preserving search path does harm to efficiency in the context ADSampling. 
%At the same time, fully preserving search path is not necessary. Graph-based methods are highly heuristic and have no theoretical guarantee. There's no evidence to show the necessity of strictly following the path of greedy search. Slight perturbation might do little harm to its performance. Motivated by these two points, we propose ARRoute.
%which allows a larger extent of approximation on graph routing.

% \input{pseudocode/ARRoute}

%The core idea of ARRoute is to do exact comparison for KNN checking and  "approximate comparison" for graph routing. 
%{\JIANYANG To achieve this, we'll need to access the maximum of the current KNNs efficiently, which necessitates maintaining an extra KNN set $\mathcal K$. } 
%As described in Algorithm~\ref{code:ARRoute}, technically, instead of maintaining two sets in ARSearch: a result set $\mathcal R$ ({\JIANYANG technically, a max-heap by } exact distance) and a search set $\mathcal S$ ({\JIANYANG a min-heap by } exact distance), here we maintain three sets: a result set $\mathcal R$ ({\JIANYANG a max-heap by } approximate distance), a search set $\mathcal S$ ({\JIANYANG a max-heap by } approximate distance) and a KNN set $\mathcal K$ ({\JIANYANG a max-heap by } exact distance). During search, we compare a newly visited object with the current maximum of $\mathcal K$ instead of $\mathcal R$ with ADSampling (line 11). If it's leveled up to $L$ and $dis_i < r$, then it's used to update $\mathcal K$ (line 12-18). However, if it's rejected at some level $l$ and only obtain approximate distance $dis_i'$, unlike what we did in ARSearch, we don't drop it directly. Instead, we still use its approximate distance to update $\mathcal R$ and $\mathcal S$ (line 19-22). 

% {\CHENG 14-07-Cheng: For the above new design of using a third queue, we'd better explain the intuitions behind the design. Currently, we simply tell what we do but do not explain why we do this. We talked about some intuitions in an earlier paragraph, but there is a gap here, e.g., can we directly compare the Kth distance in set S, but not to introduce a third set K?}

% {\JIANYANG modified}

%We emphasize that though we don't expect ARRoute to produce exact search path, it would still be desirable if the approximate distance is of high accuracy when an object is close to the current KNNs, while is allowed to be coarse when it's far away. Luckily, such adaptivity for graph routing can also be naturally realized by ADSampling. Suppose that we're comparing a newly visited non-KNN object $i$ (so it's only for graph routing) with the distance of the current KNN. Then its terminate dimension (at the same time, accuracy) is determined by its distance ratio $\alpha_i$. The closer it is to the query, the higher the accuracy would be. Thus, it naturally achieve the adaptivity of resolution for graph routing.

%During standard greedy beam search, exact distance is evaluated for two purposes: 1) to maintain KNNs and 2) to route graph search. In ARRoute, for a newly visited object, we consider preserving the result of KNN checking while allows approximation for graph routing.
%We present our ARRoute in Algorithm~\ref{code:ARRoute}. Technically, instead of maintaining two sets in ARSearch: a result set $\mathcal R$ (exact distance) and a search set $\mathcal S$ (exact distance), here we maintain three sets: a result set $\mathcal R$ (approximate distance), a search set $\mathcal S$ (approximate distance) and a KNN set $\mathcal K$ (exact distance). During search, we compare a newly visited object with the current maximum of $\mathcal K$ instead of $\mathcal R$ with ADSampling (line 11). If it's leveled up to $L$ and $dis_i < r$, then it's used to update $\mathcal K$ (line 12-18). However, if it's rejected at some level $l$ and only obtain approximate distance $dis_i'$, unlike what we did in ARSearch, we don't drop it directly. Instead, we still use its approximate distance to update $\mathcal R$ and $\mathcal S$ (line 19-22). 

% {\JIANYANG
% In terms of theoretical guarantee, ARRoute does not guarantee to exactly preserve the full search process of greedy beam search. However, since it does KNN checking with ADSampling for all candidates, it provides the guarantee of returning KNNs of generated candidates.
% %another type of guarantee , i.e., guarantee to return KNNs of the generated candidates. 
% Under the context of such type of failure, we provide the following theorem to show the time-accuracy tradeoff of ARRoute. 

% \begin{corollary}
% Let $\delta$ be an upper bound of the probability of failing to find out KNNs of generated candidates. For a DCO with threshold $r$ and a negative object. Let $(1+\alpha)$ be the ratio between $dis$ and $r$, $\alpha >0$. The expected terminate dimensionality is 
% \begin{align}
%     \mathbb{E} \left[ \hat D \right]  = O \left[ \min \left( D, \frac{1}{\alpha ^2} \log \frac{D\cdot K}{\delta}  \right)  \right] 
% \end{align}
% The overall expected time complexity is
% %of DCOs in ARSearch is 
% \begin{align}
%     O \left( c_\alpha  N_{s}\log \frac{D}{\delta}  + c_{\alpha} N_s \log K \right) \label{equation: overall ARSearch time complexity}
% \end{align}
% where $c_\alpha  =  \frac{1}{N_{s}} \sum_{i=1}^{N_{s}} \min \left( \alpha_i^{-2}, D / \log \frac{D\cdot K}{\delta}  \right) $.
% \label{corollary:ARRoute}
% \end{corollary}
% \begin{proof}
% Note that Corollary~\ref{corollary:ARRoute} replaces $N_{pos}$ in Corollary~\ref{corollary:ARSearch} with $K$. This is because within generated candidates, if the DCOs of KNN objects are successful (as a result, we obtain their exact distance), then they can be successfully retrieved. Then the proof is the same as Corollary~\ref{corollary:ARSearch}. 
% \end{proof}
% }

%the failure of ARRoute is defined to be failing to find out KNNs among generated candidates.
%However, it still guarantees to find out KNNs from visited objects with high probability, which is a straight-forward corollary of the aforementioned theorems.

% \begin{corollary}
% For a KNN query in $D$-dimensional space, applying adaptive resolution algorithms, the error bound parameter
% \begin{align}
%     \epsilon_1 = O \left( \sqrt { \log D + \log \frac{K}{\delta} }  \right) 
% \end{align}
% guarantees the probability of failing to find out the KNNs of visited objects to be no greater than $\delta$. 
% \label{corollary:eps}
% \end{corollary}

% \begin{corollary}
% Under the condition of Corollary~\ref{corollary:eps}, let $(1+\alpha_i)$ be the ratio between a negative object $i$ and its corresponding distance threshold $r$. The expected terminate dimension of $i$ is 
% \begin{align}
%     \mathbb{E} \left[ \hat D_i \right]  = O \left[   \min \left( D, \frac{1}{\alpha_i^2}\log D + \frac{1}{\alpha_i^2}\log \frac{K}{\delta} \right)  \right]  
% \end{align}
% \label{corollary:efficiency}
% \end{corollary}
%We maintain one more set $\mathcal{K}$ in graph routing, whose size is constrained within $K$ to maintain KNNs among currently visited objects. During search, a newly visited is compared with the current maximum of $\mathcal {K}$ instead of $\mathcal R$ to determine its dimensionality. If it reaches level $L$, then it's used to update $\mathcal{K}$. In terms of $\mathcal R$ and $\mathcal{S}$, unlike ARSearch, we always update them even when the distance is approximate. 
%Then if it reaches level $L$ and can update $\mathcal{K}$, it's also used to update $\mathcal R$ and $\mathcal S$. Otherwise, when it cannot update $\mathcal {K}$, it might not be leveled up to $L$, meaning that its distance is approximate. Then we also use the approximate distance to update $\mathcal R$ and $\mathcal S$. As a result, unlike ARSearch, search* not always uses exact distance for graph routing. 
% It's worth noting that Corollary~\ref{corollary:ARRoute} is independent of the size of the database, indicating that there is no need to tune the parameters as a database scales up. 

% \subsection{Adaptive Resolution Scan}
{\JIANYANG

% We then further optimize AKNN algorithms which generate candidates in batch (represented by \texttt{IVF}). Recall that in ARSearch for \texttt{IVF}, we first generate all candidates in batch and sequentially conduct DCO for each candidate with the currently searched $K$th NN. According to our discussion in Section~\ref{subsection: adaptive resolution route}, such comparison is sufficient and necessary to preserve returned AKNNs. Thus, in this direction there is no space of further optimization unless we can predict the ground truth $dis_{i_K^*}$ in the very beginning. 

% Jianyang: Would it be helpful to introduce cache if we prove that the algorithm cannot be improved from the perspective of dimensionality? 

% {\CHENG 04-08-Cheng: I think we can drop the above discussion since we cannot theoretically justify that this idea would not work.}

% We turn to another direction, i.e., optimizing data layout. 
In the original \texttt{IVF} algorithm, the vectors in the same cluster are stored sequentially. When evaluating their distances, the algorithm scans all the dimensions of these vectors \emph{sequentially}, which exhibits strong locality of reference, {\CHENG and thus it is} cache-friendly. {\CHENG Figure~\ref{fig:data layout plain} illustrates the corresponding data layout {\JIANYANGB (as indicated by the arrow)} and 
% the sequence of data accesses.
{\CHENGB the data needed (as indicated by the colored background).}
% , which is indicated by the arrow.
} In {\CHENG \texttt{IVF+}}, 
% with the same data layout, 
though {\CHENG it scans} fewer dimensions than {\texttt{IVF}}, 
% its locality of reference is not fully utilized. 
% In Figure~\ref{fig:data layout}, we depict the data layout and the needed dimensions during scanning where the arrow line indicates the sequential store of dimensions. Note that in the plain case, since no dimension can be avoided, naturally storing data vectors one by one is cache-friendly. However, under the context of \texttt{ADSampling}, as shown in Figure~\ref{fig:data layout ARSearch}, many dimensions can be avoided. In this case, such storing strategy 
it would not be cache-friendly with the same data layout. 
Specifically, when {\CHENG \texttt{IVF+}} terminates {\CHENG the dimension sampling process} for a data vector, 
% due to the principle of locality, 
the subsequent dimensions {\CHENG would probably have been} loaded into cache from main memory though they are not needed. {\CHENG Figure~\ref{fig:data layout ARSearch} illustrates the corresponding data layout {\CHENGB and data needed.}
% {\JIANYANGB (as indicated by the arrow)} 
% and the sequence of data accesses of \texttt{IVF+} 
% {\JIANYANGB (as indicated by the colored background)}.
}
% , which brings overhead. 
}

{\CHENG
% Thus, the main issue is how we design an order to access the needed dimensions to make it as sequential as possible. 
We propose to re-organize the data layout of the candidates and adjust the order of the dimensions of the candidates to be fed to \texttt{ADSampling} accordingly so as to achieve more cache-friendly data accesses. Recall that for each candidate, \texttt{ADSampling} would definitely sample a few, say $d_1$, dimensions of the candidate first and then 
% aggressively 
{\JIANYANGREVISION incrementally}
sample more dimensions depending on the hypothesis testing outcomes. That is, the first $d_1$ dimensions of each candidate would be accessed for sure. Motivated by this, we store the first $d_1$ dimensions of all candidates sequentially in an array $A_1$ and the remaining $D - d_1$ dimensions of all candidates sequentially in another array $A_2$. 
% Figure~\ref{fig:data layout ARScan} shows this data layout. 
We note that the process of re-organizing the data layout can be conducted during the index phase. During the query phase, when using \texttt{ADSampling} for DCOs on the candidates, we follow the following order of the dimensions of the candidates: the first $d_1$ dimensions of the first candidate, the first $d_1$ dimensions of the second candidate, ..., the first $d_1$ dimensions of the last candidate, the $D-d_1$ dimensions of the first candidate, ..., the $D-d_1$ dimensions of the last candidate. 
Figure~\ref{fig:data layout ARScan} 
% {\JIANYANGB shows this order of the storage of the dimensions}
illustrates the corresponding data layout {\CHENGB and data needed.}
% dimensions to be fed to \texttt{ADSampling} 
% (as indicated by the arrow),
% {\JIANYANGB and the colored part represents the dimensions needed by \texttt{ADSampling}.}
% The resulting algorithm, which we call \texttt{IVF++}, differs from \texttt{IVF+} in its data layout and the order of dimensions of candidates to be fed to \texttt{ADSampling}. 
We call the resulting algorithm \texttt{IVF++}. \texttt{IVF++} and \texttt{IVF+} would produce exactly the same results, but the former is more cache friendly since it utilizes the locality of reference for the first $d_1$ dimensions of all candidates.
}
% Recall that in \texttt{ADSampling}, for a candidate, we start from sampling a few dimensions. It indicates that at any case, these dimensions are unavoidable. Thus, our strategy is to separately store the first few dimensions and the remaining. Specifically, during index phase we pick the first $d_1$ dimensions of all data vectors in a cluster and store them sequentially in an array $A_1$. For the rest $D - d_1$ dimensions of these vectors, we store them sequentially in another array $A_2$ as shown in Figure~\ref{fig:data layout ARScan}. {\CHENG ***Did we count the overhead of re-organizing the data layout? I believe we should do so.***} Then during query phase, when scanning data vectors in a cluster, we first scan the first $d_1$ dimensions of all data vectors (i.e., scanning $A_1$ in the way that the plain algorithm scans all dimensions in Figure~\ref{fig:data layout plain}) to obtain an approximate distance of $d_1$ dimensions. This process could be viewed as the suspension of \texttt{ADSampling} operations at dimension $d_1$. Then we enumerate the data vectors one by one. With the approximate distance of $d_1$ dimensions, if we can firmly claim that $dis > r$ (reject $H_0$ in hypothesis testing), then we skip the whole rest dimensions of the vector in $A_2$. Otherwise, we restart \texttt{ADSampling} from dimension $d_1$ by sampling more dimensions in the rest dimensions of $A_2$. Note that the algorithm differ from \texttt{IVF+} only in data layout, i.e., the order of dimension evaluation. Therefore, their number of evaluated dimensions and accuracy must be the same.

% {\CHENG ***The description in this paragraph is too brief. I cannot figure out how exactly the algorithm works with the description here.***}
% {\JIANYANG It has been modified.}

%\input{pseudocode/ARKSelect}

\begin{figure}[thb]
    \centering
    \vspace{-2mm}
    % \captionsetup[subfigure]{aboveskip=-1pt}
   %  \begin{subfigure}[b]{0.32\linewidth}
   %      \includegraphics[width=\textwidth]{revision experimental result/IVFPQ.pdf}
   %      \caption{\texttt{IVFPQ}}
	  % \label{fig:cost IVFPQ}
   %  \end{subfigure} 
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[height=0.9\textwidth]{figure/ARScanPlain.pdf}
        \caption{{\CHENG \texttt{IVF}}}
        \label{fig:data layout plain}
    \end{subfigure}       
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[height=0.9\textwidth]{figure/ARScanARSearch.pdf}
        \caption{{\CHENG \texttt{IVF+}}}
        \label{fig:data layout ARSearch}
    \end{subfigure}       
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[height=0.9\textwidth]{figure/ARScanARScan.pdf}
        \caption{{\CHENG \texttt{IVF++}}}
        \label{fig:data layout ARScan}
    \end{subfigure}       
 %    \subfigure[{\CHENG \texttt{IVF}}]{
 %        \includegraphics[height=0.25\linewidth]{figure/ARScanPlain.pdf}
 %        \label{fig:data layout plain}
 %    }
 %    \subfigure[{\CHENG \texttt{IVF+}}]{
	% \includegraphics[height=0.25\linewidth]{figure/ARScanARSearch.pdf}
 %        \label{fig:data layout ARSearch}
 %    }
 %    \subfigure[{\CHENG \texttt{IVF++}}]{
	% \includegraphics[height=0.25\linewidth]{figure/ARScanARScan.pdf}
 %        \label{fig:data layout ARScan}
 %    }
    \vspace{-4mm}
    \caption{Data Layout {\CHENGC and Data Needed}}
    \vspace{-4mm}
    \label{fig:data layout}
\end{figure}


%In ARRoute, we further accelerate the adaptive algorithm by increasing the distance ratio $\alpha_i$ for graph routing (the dynamic case). Now following the same thread, we propose adaptive resolution select to further optimize the static case, where all candidates are given all at once. Different from the dynamic case, whose order of candidates is largely determined by graph routing, in the static case, we could manipulate the evaluation order for further acceleration. 

%Suppose that we evaluate the candidates sequentially. Let's first investigate two extreme examples: 1) Candidates are given in the decreasing order. Then every time, the scanned object is supposed to be leveled up to $L$ and update the KNN set $\mathcal K$. Thus, the algorithm degrades to the plain exact distance evaluation.  2) Candidates are given in the increasing order. In this case, we can get the ground truth $dis_{i_K}$ very soon, meaning that for the following candidates, we are comparing their distance with the minimum possible threshold, which reaches the optimal complexity. Thus, the core of the problem is find a good estimation of $dis_{i_K}$ as soon as possible.

%We just considered the scenario in graph routing, where new objects are dynamically visited and evaluated. In this case, since we have no prior knowledge about the ground truth distance of Kth NN $dis_{i_k^*}$, we can only proxy it with that of the currently visited Kth NN $\max_{j\in \mathcal K} dis_j$, which obviously overestimates it and reduces the space for approximation. Also, since the order of objects is largely determined by graph routing, we cannot manipulate it on our own to obtain $dis_{i_k^*}$ earlier. However, in the framework of filter-and-verification, we're facing a static case, where candidates are given all at once. Thus, we may determine the evaluation order to obtain $dis_{i_k^*}$ as soon as possible, which motivates us to design adaptive resolution re-ranking: 

%We next propose AdaSort. Compared with the dynamic case handled by priority queue, partial sort is the optimal algorithm for the static case, i.e. given a set of vectors all at once without dynamic insertion. A conventional solution is to first evaluate full-precision distance for all the objects, and then apply partial sort to find the smallest Ks. {\color{red} cite? textbook algorithm.} It's commonly used in filter-and-verification-based KNN algorithms: after $N$ candidates are generated, partial sort is applied to find the true KNNs.

%Unlike the dynamic case where we cannot determine the insertion and pop-out order, in static case, we seek for a good order to mitigate the overhead introduced by AdaSampling. The overhead of AdaSampling mainly comes from the unknown true distance of the Kth NN $dis_K$. If we know it a prior, we can deal with each object independently by leveling it up until $dis_i^* > dis_K$  or $l_i = L$ without dynamically maintaining a partial heap. Thus, for the static case, our strategy is to find $dis_K$ as soon as possible and do AdaSampling based on $dis_K$.

%\input{pseudocode/ARKSelect}

% {\CHENG 14-07-Cheng: (1) As we discussed before, we explain on more benefits of this design (e.g., data layout). (2) Does it make sense to first level up to a ith level and tune i as a hyperparameter. When i=1, it reduces the algorithm we consider now.}

% {\JIANYANG 15-07-Jianyang: (1) TODO (2) It might not bring significant benefits because tuning i is equivalent to predict the minimum needed resolution, which is achieved by the ADSampling framework.}

%To obtain a good estimation of $dis_{i_K}$, our strategy is to select K objects with coarse code as initial KNNs $\mathcal K$. Specifically, as shown in Algorithm~\ref{code:ARKSelect} we first level up all the candidates to level $1$ (line 1-2) and select the first $K$ objects according their approximate $dis'_i$ with quick select~\cite{quickselect} (line 3). The first $K$ objects with the minimum approximate distance are likely to provide good estimation of $dis_{i_K}$, so we start with evaluating them (line 4-6) and then sequentially the others (line 8-13). Note that during evaluating other objects, since we have leveled up all objects to 1, we can simply continue ADSampling from it. In practice, for the concern of cache, we separately store the dimensions of the first level and the rest. %Note that our method is different from the multi-stage filter-and-verification because after selecting the first $K$ we still scan the remaining. The step of selection is simply for acquiring better estimation of $dis_{i_k^*}$.
%In terms of theoretical guarantee, since for the static case, the only concern is to correctly find out the KNNs of the generated candidates. Thus, it has the same guarantee as Corollary~\ref{corollary:eps} and \ref{corollary:efficiency}.

{\CHENGB
\smallskip
\noindent\textbf{Theoretical Analysis.}}
{\JIANYANG 
% In terms of theoretical analysis, note that 
% Since \texttt{IVF++} and \texttt{IVF+} differ only in data layout, they have the same theoretical guarantee, which is provided in Corollary~\ref{corollary: find out KNNs}.
Since \texttt{IVF++} and \texttt{IVF+} differ only in data layout, they have the same theoretical guarantee (Corollary~\ref{corollary: find out KNNs}).
}

% {\JIANYANG In terms of theoretical guarantee, though Theorem~\ref{theorem:AKNN+ time-accuracy} provides a general result for all \texttt{AKNN+} algorithms (including \texttt{IVF+}, at the same time \texttt{IVF++} because they differ only in data layout), for methods which generate candidates all at once like IVF, we can provide a stronger guarantee. {\CHENG ***I don't quite understand why we have stronger guarantees for IVF than for HNSW. Because it uses a smaller distance threshold in general?***} We recall that as discussed in Theorem~\ref{theorem:HNSW++ time-accuracy}, returning KNNs of generated candidates only needs to produce correct comparison results for KNN objects. Thus, we have the following theorem. 

% \begin{corollary}
% The probability that \texttt{IVF+}/\texttt{IVF++} fails to return the same results as \texttt{IVF} is at most $\exp \left( -c_0 \epsilon_0^2 + \log (DK) \right)$. %where $N_{pos}$ is the number of DCOs for positive objects.
% \label{theorem: HNSW++ failure probability}
% \end{corollary}

% And correspondingly, its time complexity is given as follows.

% \begin{corollary}
% Let $\delta'$ be the failure probability of \texttt{IVF+/IVF++}. The expected time complexity of \texttt{IVF+/IVF++} is 
% \begin{align}
%     C_1 = O \left( DK \log N + c_\alpha N_s \log \frac{DK}{\delta'} \right) 
% \end{align}
% where $c_\alpha  =  \frac{1}{N_{s}} \sum_{i=1}^{N_{s}} \min \left( \alpha_i^{-2}, D / \log \frac{DK}{\delta'}  \right)$. $C_2 = O(N_s \log K)$ for \texttt{IVF}. 
% % ***In this theorem, we can define $\delta'$ as the failure probability for \texttt{AKNN+}, which should be based on $\delta$.***
% \label{theorem:HNSW++ time-accuracy}
% \end{corollary}

% %same as the discussion in Section~\label{subsection: adaptive resolution route}, returning correct results for KNN objects within generated candidates is sufficient and necessary to preserve returned AKNNs. Thus, it has the same guarantee of Corollary~\ref{corollary:ARRoute}.
% }

%recall that our ADSampling framework has no false positive failure. Thus, a query succeeds if and only if the positive are correctly leveled up to $L$ and used to update $\mathcal K$. In the static case, the positive is the KNNs objects of generated candidates. As a result, it has the same guarantee as Corollary~\ref{corollary:eps} and \ref{corollary:efficiency}.}
%Since for the static case, a query succeeds if and only if the KNNs objects are successfully leveled up to $L$ and used to update $\mathcal K$.}
%To obtain better estimation of $dis_{i^*_k}$, the strategy is to select K objects with coarse code. Specifically, we first level up all the objects to level $1$ and select the first $K$ objects with minimum $dis'_i$ with quick select~\cite{quickselect}. The first $K$ objects with the minimum approximate distance are likely to provide good estimation of $dis_{i_k^*}$, so we start with evaluating them and then sequentially the others. Note that our method is different from the multi-stage filter-and-verification because after selecting the first $K$ we still scan the remaining. The step of selection is simply for acquiring better estimation of $dis_{i_k^*}$.

%do conventional partial sort by the key of safe distance $dis^*_i$. The objects with small safe distance are likely to small true distance. Then we evaluate the full-precision distance of the first K elements and use a max-heap $H$ to maintain $dis_K$. Note that $H$ only contains objects of full precision. Then we scan and evaluate the remaining objects with AdaSampling independently. Once one object is leveled up to $L$, we insert it into $H$ and pop out the maximum to maintain $dis_K$. {\color{red} The complexity of AdaSort is $O(N \log K)$ instead of $O(L\cdot N \log N)$.  } 

%{\color{red} And the failure probability is different from AdaQ. }

%{\color{red} Some questions when describing time complexity: should I include the complexity of time evaluation? }

%{\color{red} Adaptive Partial Sort is a little bit trivial?}




%\subsection{Applications and Implementations}
%{\color{red} Is it necessary to write this part? I thought previous subsections have already given the way of applications and implementations.}

%\subsubsection{\textbf{Tree and Graph}}

%\subsubsection{\textbf{Filter-and-Verification Framework}}

%\subsubsection{\textbf{More High-Dimensional Data Management Problems}}

{\CHENG
\smallskip
\noindent\textbf{Remarks.} We note that the technique used for improving \texttt{IVF+} with cache friendliness can also be used for improving some other \texttt{AKNN+} algorithms, including those of tree-based methods~\cite{muja2014scalable, dasgupta2008random, ram2019revisiting, beygelzimer2006cover}, quantization-based methods~\cite{jegou2010product, imi} and hashing-based methods~\cite{datar2004locality, c2lsh, Dong2020Learning}.
% FLANN~\cite{muja2014scalable},  LSH~\cite{datar2004locality} and Neural LSH~\cite{Dong2020Learning}.
% XXX, XXX, and XXX
This is because all these algorithms generate the candidates in a batch and then re-rank the candidates for finding out KNNs.
}


% {\color{red} Let me first comment out remarks here to avoid distraction. It discusses our advantages.}

\begin{comment}

\subsubsection{\textbf{Remarks}}
{\color{red} may need discussion, emphasize the advantages}
\begin{itemize}
    %\item Note that AdaQ and AdaSort target to make improvements on real world datasets. It cannot decrease the worst case complexity because for extremely fragile queries, their dimensionality is inherently irreducible. {\color{red} It's also worth noting that the value of extremely fragile queries might need to be questioned. }
    %\item We emphasize that our methods target to produce correct results instead of $c$-approximate ones, which is different from the most popular setting of random-projection-based methods adopted by LSH family {\color{red} cite} and random partition tree {\color{red} cite}. Our methods themselves don't introduce any errors with high probability. 
    %\item Our methods also support the relaxed and decisive versions of KNN, i.e. $c$-KNN, $r$-NN. They adapt to $c$-approximate version by simply dividing $\gamma(d)$ by a factor of $c$, $\gamma_c(d) := \gamma(d)/c$. While for radius-based queries, it could be regarded as KNN queries providing the ground truth $dis_K$ in the beginning. Thus, it enables independent ADSampling just as we did in AdaSort, which improves the performance.
    %\item Our methods are friendly to practitioners: 1) Their implementations and applications are extremely easy. To apply them, one only needs to randomly transform datasets and queries, and substitutes classic algorithms with our adaptive ones.
    {\color{red} 2) They support dynamic update. Since they are data-oblivious, so unlike machine-learning-based adaptive methods, dynamic update doesn't corrupt their performance. }
    %{3) Our adaptive algorithms suit nearest neighbor search libraries consisted of multiple methods. Unlike algorithm-specific adaptive methods {\color{red} cite}, our methods suit all algorithms with one preprocessing. }
\end{itemize}

\end{comment}