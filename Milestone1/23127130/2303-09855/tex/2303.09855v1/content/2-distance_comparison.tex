

\section{The Distance Comparison Operation}
\label{sec:dco}
% In this section, we give problem settings of KNN query, introduce random projection and discuss two classes of algorithms. 

\subsection{KNN Query and Distance Comparison Operation}
\label{subsec:definition}

% Let $\mathcal O = \{\mathbf{o}_1,\mathbf{o}_2,...,\mathbf{o}_N\}$ 
Let $\mathcal O$ be a database of $N$ objects in a $D$-dimensional Euclidean space $\mathbb{R}^D$ and $\mathbf{q}$ be a query. {\CHENG In this paper, we use ``object'' (resp. ``query'') and ``data vector'' (resp. ``query vector'') interchangeably}. 
% We denote $\mathbf{x}_i:=\mathbf{o}_i - \mathbf{q}$ and $\mathcal X := \{\mathbf{x}_1,\mathbf{x}_2,...,\mathbf{x}_N\}$ for simplicity. Then the Euclidean norm $\|\mathbf{x}_i\| $ is the to-query-distance of object $i$.
{\CHENG We note that operations on $\mathcal{O}$ can be conducted before the query $\mathbf{q}$ comes (i.e., the index phase) while those on the query $\mathbf{q}$ can only be conducted after it comes (i.e., the query phase). 
For an object $\mathbf{o}$, we define its difference from the query $\mathbf{q}$ as $\mathbf{x}$, i.e., $\mathbf{x} = \mathbf{o} - \mathbf{q}$. We refer to an object $\mathbf{o}$ by its corresponding vector $\mathbf{x}$ when the context is clear.}
Without ambiguity, by ``the distance of an object $\mathbf{o}$'', we refer to its distance {\CHENGC from} the query vector $\mathbf{q}$, {\CHENG which we denote by $dis$}. 
% For smooth reading, we also use $dis_i$ to represent the distance of object $i$ in algorithm descriptions.
%
%Considering a database of $N$ data objects $\mathcal O = \{\mathbf{o}_1,\mathbf{o}_2,...,\mathbf{o}_N   \}$ in $D$-dimensional Euclidean space $\mathbb{R}^D $, for a given query $\mathbf{q} \in \mathbf{R}^D $, we denote the residual vectors as $\mathcal X = \{\mathbf{x}_i=\mathbf{o}_i - \mathbf{q}| \forall \mathbf{o}_i \in \mathcal O\}$. In order to keep smooth reading, we denote the distance between $\mathbf{o}_i $ and $\mathbf{q} $(distance of $\mathbf{o}_i$ for short) as $\|\mathbf{x} _i\|$ for theoretical analysis, while $dis_i$ for algorithm description.
%
The \textbf{K nearest neighbor (KNN)} query is to find the top-K objects with the minimum distance {\CHENGC from} the query $\mathbf{q}$. 
% In particular, we denote the ground-truth KNNs {\CHENG by their indices} as $i^*_1, i^*_2, ..., i^*_K$ ordered by exact distance increasingly. 
% During search, we maintain currently searched KNNs in a KNN set $\mathcal K$, whose elements (KNNs) are similarly denoted as $i_1, i_2, ..., i_K$.
%Similarly, the currently maintained KNNs in $\mathcal{K}$ is denoted as $i_1, i_2, ..., i_K$. 

%The $c$-approximate K nearest neighbor ($c$-AKNN) search query only requires the returned object $i_j$ to be $c$-approximate: $\|\mathbf{x}_{i_j} \| \le c \| \mathbf{x}_{i_j^*}\|$. 
%The $r$-near neighbor ($r$-NN) search is to find one object $i$ whose distance is no greater than $r$: $\| \mathbf{x}_i\| \le r $. The $c$-approximate $r$-near neighbor ($cr$-NN) is to return one object $i$ whose distance is no greater than $cr$: $\| \mathbf{x}_i \| \le cr $ if there exists an object $i^*$ with $\| \mathbf{x}_{i^*}\| \le r$. 
% In the present work, we focus on an important component in KNN query, i.e. \textit{distance comparison}. 

{\CHENG In this paper, we study the \emph{distance comparison operation} (DCO), which is defined as follows.}
%
\begin{definition}[Distance Comparison Operation]
% Let positive number $r$ be a distance threshold and object $i$ be a data vector. The problem of distance comparison is to 
{\CHENG Given an object $\mathbf{o}$ and a distance threshold $r$, the \textbf{distance comparison operation} (DCO) is to decide whether object $\mathbf{o}$ has its distance $dis$ {\JIANYANGLAST no greater than $r$}
% at most $r$ 
and if so, return $dis$.
% return the order between object $i$'s exact distance $dis_i$ and the distance threshold $r$. 
% {\CHENG decide whether object $i$'s exact distance $dis_i$ is at most the distance threshold $r$.}
% In particular, we use $1$ (positive) and $0$ (negative) to represent the comparison results of $dis_i\le r$ and $dis_i > r$, respectively.
In particular, we say object $\mathbf{o}$ is a \emph{positive} object if {\CHENGC $dis\le r$} and a \emph{negative} object otherwise.}
\end{definition}
%Formally, let $r$ be a threshold (in practice, usually $dis_{i_K}$). For a new candidate data vector $\mathbf{o}_i$, we want to conclude the order between its distance $dis_i$ and $r$ without evaluating all $D$ dimensions.
%
% The distance comparison operation plays a vital role in most (if not all) existing AKNN algorithms, which we elaborate on next.

{\CHENG As mentioned in Section~\ref{sec:introduction}, DCOs are heavily involved in many AKNN algorithms. These algorithms conduct the DCO for an object $\mathbf{o}$ and a distance $r$ naturally by computing $\mathbf{o}$'s distance and comparing the distance against $r$. We call this conventional method \texttt{FDScanning} since it uses \emph{all} dimensions of $\mathbf{o}$ {\CHENGC for computing} the distance. Clearly, \texttt{FDScanning} has the time complexity of $O(D)$. 
% With \texttt{FDScanning} adopted for DCOs, nearly all existing AKNN algorithms have their time costs dominated by those of DCOs. 

Next, we review the existing AKNN algorithms and validate both theoretically and empirically the critical role of DCOs in these algorithms.}
% With a conventional method for the distance comparison operation on object $i$ is to compute its distance $dis_i$ and compare $dis_i$ against $r$. and dominate their time costs.

% {\CHENG 14-07-Cheng: Once this problem is presented, it is better to repeat the explanations on how this problem/operation is involved in many existing AKNN algorithms.}

% {\JIANYANG 15-07-Jianyang: I thought it was the same problem as the one in Section 1, which might need further discussion.}




% {\CHENG As mentioned in Section~\ref{sec:introduction}, distance comparison operations are heavily involved in many AKNN algorithms, including\texttt{HNSW}and IVF, and dominate their time costs. A conventional method for the distance comparison operation on object $i$ is to compute its distance $dis_i$ and compare $dis_i$ against $r$. This method has its time complexity of $O(D)$. Considering that $D$ is usually hundreds and even thousands for high-dimensional objects and 
% % the distance comparison operation is frequently invoked by an AKNN algorithm
% the \texttt{FDScanning} method is an overkill for negative objects which take a large proportion of generated candidates
% , in this paper, we aim to 
% % conduct distance comparison operations \emph{approximately} with correctness guarantees for better efficiency.
% {\JIANYANG
% optimize distance comparison operations of negative objects for better time-accuracy tradeoff.
% }
% }

% {\JIANYANG
% By providing both theoretical and empirical evidence, we first justify our statements: (1) distance comparison operation dominates the time cost of most (if not all) AKNN algorithms (2) negative objects further take dominant proportion among generated candidates of AKNN algorithms.
% \underline{First}, we study the time complexity of two state-of-the-art AKNN algorithms, HNSW~\cite{malkov2018efficient} and IVF~\cite{jegou2010product}. Let $N_s$ be the number of generated candidates of an AKNN algorithm. The total time complexity of\texttt{HNSW}(resp. IVF) is $O(N_s D + N_s  \log N_s$ (resp. $O(N_s D + N_s \log K)$), where $O(N_s D)$ is the cost of distance comparison and $O(N_s \log N_s)$ (resp. $O(N_s \log K)$) is that of other operations. Since $D$ can be hundreds or even thousands of dimensions while $\log N_s$ (resp. $\log K$) is only a few dozens, the cost of AKNN algorithms is dominated by the distance comparison operations. 
% \underline{Second}, assume that we evaluated all the objects in a random order (i.e., a random permutation). Then the expected number of positive objects is $O(K \log N)$ ($O(N_{ef} \log N)$ for HNSW) (due to the limit of space, we'll provide detailed proof in technical report). Note that in practice we evaluate candidates in an AKNN-determined order, which is expected to be better than the random one. Thus, the number of positive objects in an AKNN algorithm is supposed to be bounded by $O(K \log N)$ ($O(N_{ef} \log N)$ for HNSW).  
% We verify the above statements empirically. Figure~\ref{fig:cost statistics} profiles the time consumption of AKNN algorithms on three real-world datasets when targeting 95\% recall with $K = 100$. We observe that (1) on datasets with various dimensions from 256 to 960, distance comparison takes up 77.2\% to 87.6\% of total running time of\texttt{HNSW}and 85.0\% to 95.3\% of that of IVF;  (2) For HNSW, the number of negative objects is 4.3x to 5.2x times more than the positive, and for IVF, the number of negative objects is 60x to 869x more than the positive. Thus, in the following sections, we focus on optimizing the distance comparison operation, in particular for negative objects.
% }

% \subsection{Random Projection}
% \label{subsec:rp}
% Random projection is a method that projects high-dimensional vectors into a low-dimensional space with random matrices and at the same time, largely preserves distance information. There are a large class of random matrices used in random projection~\cite{DBfriendly, blockjlt, datar2004locality, johnson1984extensions, fftjlt}. The most popular kind in database community is \textit{random Gaussian projection}~\cite{datar2004locality, c2lsh, huang2015query, tao2010efficient,sun2014srs, zheng2020pm} whose entries of the random matrix are independent standard Gaussian variables. 

% Another famous kind
% % , the one we adopted in the present work, 
% is \textit{random orthogonal projection}~\cite{johnson1984extensions, choromanski2017unreasonable,yu2016orthogonal}\footnote{{\JIANYANG Random orthogonal projection~\cite{johnson1984extensions} for low distortion metric embedding was studied prior to random Gaussian projection~\cite{indyk1998approximate}, and they show similar empirical effectiveness. 
% %At the same time, these two methods show very similar empirical effectiveness. Since generating random Gaussian projection is simpler, it becomes the most popular in database community.
% }}. 
% Its corresponding random matrix $P \in \mathbb{R}^{d\times D}$ is composed of $d$ mutually orthogonal unit row vectors, which aims at projecting vectors to a $d$-dimensional subspace drawn "uniformly" from all $d$-dimensional subspaces. A common way to generate a random orthogonal projection is to first generate a square random Gaussian projection matrix $G\in \mathbb{R}^{D\times D} $, and then orthogonalize it with QR decomposition $G=QR$, where $Q$ is an orthogonal matrix and $R$ is an upper triangular matrix. By taking arbitrary $d$ rows of $Q$, a random orthogonal projection matrix in $\mathbb{R}^{d \times D}$ is obtained~\cite{choromanski2017unreasonable}. In particular, the random orthogonal projection with a square matrix in $\mathbb{R}^{D\times D} $ is usually termed as \textit{random orthogonal transformation}~\cite{ITQ, randomortho} (or random rotation). Without ambiguity, in the following sections, we refer to random orthogonal projection as \textit{random projection} and random orthogonal transformation as \textit{random transformation} for short. 
% The process in which we take $d$ rows of a matrix or a column vector is termed \textit{row sampling}. 

% \begin{figure}
%     \centering
%     \includegraphics[height=0.3\linewidth]{figure/random projection.pdf}
%     \caption{Random Projection}
%     \label{fig:random projection}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[height=0.3\linewidth]{figure/dimension sampling.pdf}
%     \caption{Dimension Sampling}
%     \label{fig:dimension sampling}
% \end{figure}

% {\CHENG 14-07-Cheng: Would it be better to use figures to illustrate two kinds of random projection?}

% {\JIANYANG 14-07-Jianyang: Geometric interpretation of a particular random projection is hard to give since random projection is an abstract mathematical concept.}

% {\JIANYANG 23-07-Jianyang: Modified}

%For NN query, random projection provides a low-distortion metric embedding which largely reduces dimensionality while just introduces acceptable error. The most popular kind in database community is \textit{random Gaussian projection}~\cite{datar2004locality, c2lsh, huang2015query, tao2010efficient,sun2014srs, zheng2020pm}. We also introduce another one, i.e. \textit{random orthogonal projection}, which was also widely researched and utilized~\cite{johnson1984extensions,choromanski2017unreasonable, yu2016orthogonal}.

%Let $P \in \mathbb{R}^{d\times D} $ be a random projection matrix to a $d$-dimensional space. Random Gaussian projection generates $P$ by making entries independent standard Gaussian variables $\mathcal N(0,1)$\footnote{It should be scaled by a factor of $1/\sqrt {D}$ to make Lemma~\ref{eq:concen} hold.}. For random orthogonal projection, it first generates a square random Gaussian projection matrix $G \in \mathbb{R}^{D\times D} $, and then orthogonalize it with QR decomposition $G=QR$, where $Q$ is an orthogonal matrix and $R$ is an upper triangular matrix, then by taking arbitrary $d$ rows of $Q$, a random orthogonal projection matrix  in $\mathbb{R}^{d \times D}$ is obtained\footnote{It's shown by Bartlett decomposition theorem~\cite{muirhead2009aspects} that $Q$ is uniformly distributed on the space of all orthogonal matrices (Stiefel manifold).}. In the following sections we refer to random orthogonal(Gaussian) projections of square matrices $\mathbb{R}^{D\times D} $ as \textit{random orthogonal(Gaussian) transformation}. The process in which we takes $d$ rows of a matrix or column vector is termed \textit{row sampling}.

%Another way {\color{red} that shows great performance in machine learning community} is random orthogonal projection~\cite{yu2016orthogonal, choromanski2017unreasonable}. It first generates a $D \times D$ random Gaussian matrix $G$ and does QR decomposition, where the $Q$ matrix is an orthogonal matrix and $R$ is an upper triangular matrix. It's shown by Bartlett decomposition theorem~\cite{muirhead2009aspects} that $Q$ is uniformly distributed on the space of all orthogonal matrices(Stiefel manifold). Then by selecting the first $d$ rows, we get a random orthogonal projection matrix in $\mathbb{R}^{d\times D} $. 

% \begin{figure}[t]
%   \centering 
%   \includegraphics[width=\linewidth]{figure/concentration.pdf}
%   %\includesvg[width=\linewidth]{figure/concentration.svg}
%   \caption{Concentration of Random Projection.
%   %: probability density function of $\sqrt {D/d} \frac{\| P \mathbf{x}\|}{\| \mathbf{x} \| }  $ concentrates around 1.
%   }
%   \label{fig:concen}
% \end{figure}

% The performance of random projection on metric embedding is theoretically guaranteed by the well-known concentration inequality of random projection~\cite{vershynin_2018}:
% %Random orthogonal projection exhibit good performance on low-distortion metric embedding, which is theoretically guaranteed by the well-known concentration inequality of random projection~\cite{vershynin_2018}:
% \begin{lemma}
%     For a fixed point $\mathbf{x} \in \mathbb{R}^D  $, a random projection $P \in \mathbb{R}^{d\times D} $ preserves its Euclidean norm with $\epsilon $ multiplicative error bound with the probability of
% \begin{align}
%     \mathbb{P} \left\{ \left| \sqrt {\frac{D}{d} } \| P \mathbf{x}\| -\| \mathbf{x} \| \right| \le \epsilon \| \mathbf{x} \|  \right\}   \ge 1 - 2e^{-c_0 d \epsilon ^2} 
%     \label{equ:concentration}
% \end{align}
% %    The corresponding failure probability is bounded by
% %\begin{align}
% %    \mathbb{P} \left\{ \left| \sqrt {\frac{D}{d} } \| P \mathbf{x}\| -\| \mathbf{x} \| \right| > \epsilon \| \mathbf{x} \|  \right\}   \le 2e^{-c_0 d \epsilon ^2} \notag 
% %\end{align}
%     where $c_0$ is a constant factor. \label{eq:fail_concen}\label{eq:concen}
%     \label{lemma:concen}
% \end{lemma}

% {\CHENG 14-07-Cheng: Perhaps, we should provide the details of the constant $c_0$ here. Note that the significance of the hypothesis to be dicussed later on relies on the value of $c_0$.}

% {\JIANYANG 14-07-Jianyang: The problem is that this expression is only tight in order but not in constant, meaning that if we set the parameter $\epsilon_1$ according to the theoretically given $c_0$, it would not be the optimal choice in practice. If we provide concrete $c_0$, we'll need to explain the gap between theory and practice.}

% {\CHENG 22-07-Cheng: I don't quite understand the explanation above. Let's discuss later on.}

%{\color{red} An intuitive explanation is that random projection approximately uniformly assigns the squared norm $\| \mathbf{x} \|^2 =\sum_{i=1}^{D} x_i^2 $ to each single dimension, so the square of each single dimension is approximately $\| \mathbf{x}\|^2 / D$. Consequently, the average of after-projection squared norm $\|P \mathbf{x}\|^2 / d$ should be a good estimation for the true average of squared norm $\| \mathbf{x}\|^2 / D$. } Correspondingly, we have the upper bound of failure probability on the right hand side: 

% Note that there are three parameters in Lemma~\ref{eq:fail_concen}: (1) multiplicative error bound $\epsilon$ ; (2) dimensionality $d$; (3) failure probability {\JIANYANG (i.e., the probability that $\sqrt {D/d} \| P \mathbf{x}\| $ falls outside the $\epsilon$ error bound)} $2\exp (-c_0 d \epsilon ^2)$. Fixing any one of them, the relationship between the other two is determined, which derives a certain interpretation. Here we investigate two cases:

% \textbf{\textit{Property 1.} Concentration of Distance.} Fixing dimensionality, the failure probability $2\exp (-c_0 d \epsilon ^2)$ decays 
% %super-exponentially 
% {\JIANYANG quadratic-exponentially}
% with respect to $\epsilon$. It intuitively indicates that the distribution of the approximate distance $\sqrt {D/d}\|P \mathbf{x}\| $ highly concentrates around the exact distance $\|\mathbf{x}\|$.
% %as shown in Figure~\ref{fig:concen}.

% \textbf{\textit{Property 2.} Decrease of Uncertainty.} Fixing failure probability, as the dimensionality $d$ increases, the multiplicative error bound $\epsilon$ decays at the order of $\epsilon \propto 1/\sqrt {d}$.
% %as shown by the width between the dash lines in Figure~\ref{fig:concen}, indicating that it becomes more concentrated.

% {\JIANYANG Such concentration inequality directly derives a powerful theorem as well as a Monte Carlo randomized algorithm for low-distortion metric embedding for $\ell_2$ Euclidean space, i.e., Johnson-Lindenstrauss Lemma~\cite{johnson1984extensions}. 

% \begin{theorem}[Johnson-Lindenstrauss Lemma (Restate)]
% Given multiplicative error bound $\epsilon$, and a finite set $X \subset \mathbb{R}^D $ of $N$ points. A random projection $P \in \mathbb{R}^{d \times D} $, $d = \Theta (\epsilon^{-2}\log \frac{N}{\delta} )$ guarantees to preserve the mutual distance of points in $X$ within multiplicative error $1 \pm \epsilon $ with probability at least $1- \delta$.
% \end{theorem}

% Note that it provides a very non-trivial result for dimension reduction by claiming that $\Theta(\epsilon^{-2} \log \frac{N}{\delta} )$ dimensions are enough to describe the distance information of a finite set with arbitrary structure and regardless of its original dimension $D$. We aim to incorporate its power into the problem of distance comparison.
% }
% {\CHENG 22-07-Cheng: Comments on Figure~\ref{fig:concen}: (1) Are the distributions real? They look to be Gaussian like distributions.  (2) To me, it does not help that much for illustrating Properties 1 and 2. Perhaps, we drop this figure.}

% {\JIANYANG 22-07-Jianyang: (1) It's actually sub-Gaussian (a large class of distribution with the same order of decay speed as Gaussian.) This figure is simply for illustration. It's drawn with square root of a $\chi^2(d)$ distribution, which should be the result of random Gaussian projection. (2) Sure. }

%where $c_0$ is a constant factor. The failure probability decays at the rate of $\exp (-c_0d \epsilon ^2)$ which is termed subgaussian tail bound, indicating that it decays by the same order of speed as Gaussian distribution. Intuitively, as the failure probability decays exponentially, the distribution of the observed distance in the $d$-dimensional space $\|P \mathbf{x}\| $ highly concentrates around the true distance $\|\mathbf{x}\| $ after scaling by $\sqrt {d/D } $.  {\color{red} I may need some figures here.}

%\subsection{Hypothesis Testing}
%Hypothesis testing is a famous method for statistical inference. It answers the question that whether sampled data are sufficient enough to support a hypothesis about a distribution or its parameters. 

%The procedure can be breifly summarized as follows:
%\begin{itemize}
%    \item Determine the unknown parameter we want to test. 
%    \item Set up a null hypothesis $H_0$ and its corresponding alternative hypothesis $H_1$ for the parameter. 
%    \item Construct statistics based on raw samples.
%    \item Set up a significance level to control the failure probability.
%    \item Derive rejection region for the constructed statistics.
%\end{itemize}

% {\JIANYANG
% \subsection{Hypothesis Testing}
% Hypothesis testing is a classic technique of statistical inference. It aims at drawing some conclusions about an \textbf{unknown parameter} with some samples drawn from a distribution, where the distribution is dependent on the unknown parameter. Usual procedure of hypothesis testing is summarized as below:
% \begin{enumerate}
%     % \item Specify a question about an unknown parameter (e.g., let exact distance $dis_i$ be the unknown parameter. We are interested in the result of a distance comparison, i.e., whether $dis_i \le r$).  
%     \item Propose a null hypothesis $H_0$ and its corresponding alternative hypothesis $H_1$ about the question.
%     % (e.g., $H_0:dis_i \le r, H_1: dis_i > r$ ).
%     \item Use a random variable as an estimator of the unknown parameter and specify their relationship.
%     % (e.g., let approximate distance obtained by random projection $dis_i'$ be the estimator. Its distribution is bounded by Lemma~\ref{eq:concen}, which is dependent on $dis_i$.).
%     % \item Consider that if the hypothesis $H_0$ holds, what the distribution of the estimator would be like (e.g., note that when $H_0: dis_i \le r$ holds, since $dis_i'$ should concentrate around $dis_i$, $dis_i'$ cannot be much larger than $r$.). 
%     % \item Select a significance level $\delta$, i.e., we'll reject the null hypothesis if the probability of the observed event is below $\delta$. Note that we do not directly calculate the probability of an event.
%     \item Compute the rejection region of the estimator based on a significance level $\delta$: the event that the observed value falls inside the rejection region indicates that the probability of the observed event is below $\delta$.
%     % {\CHENG (e.g., the rejection region of $dis_i'$ corresponds to a range in the form of $(\gamma\cdot r, +\infty)$, where 
%     %$\gamma$ is a positive value
%     % {\JIANYANG $\gamma > 1$}. The intuition is that the event that the observed $dis_i'$ is much larger than $r$ (i.e., $dis_i' > \gamma\cdot r$) implies that $H_0$ is probably not true).} 
%     \item Check whether an observation of the estimator falls inside the rejection region. If so, reject $H_0$. Otherwise, do not reject it. Note that ``not reject'' does not mean ``accept''. 
% \end{enumerate}

% }

\subsection{AKNN Algorithms and Their DCOs}
\label{subsec:aknn}


\begin{figure}[thb]
    \vspace{-4mm}
    \centering
    \begin{subfigure}[b]{0.5\linewidth}
        \centering
        \includegraphics[width=0.8\textwidth]{figure/HNSW.pdf}
        \caption{\texttt{HNSW}}
        \label{fig:routing}
    \end{subfigure} 
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=0.8\textwidth]{figure/IVF.pdf}
        \caption{\texttt{IVF}}
	\label{fig:ivf}
    \end{subfigure} 
    % \subfigure[\texttt{HNSW}]{
    %     \includegraphics[height=0.3\linewidth]{figure/HNSW.pdf}
    %     \label{fig:routing}
    % }
    % \subfigure[\texttt{IVF}]{
	   %  \includegraphics[height=0.3\linewidth]{figure/IVF.pdf}
	   %  \label{fig:ivf}
    % }
    \vspace{-4mm}
    \caption{Illustrations of AKNN algorithms.}
    \vspace{-4mm}
    \label{fig:aknn}
\end{figure}

\subsubsection{Graph-Based Methods}
%Similarity graph is one family of state-of-the-art methods that exhibits dominant performance on time-accuracy tradeoff for in-memory AKNN query. It builds a graph upon a database, where each vertex corresponds to a data vector. One famous graph-based method is the hierarchical navigable small world graphs (HNSW)~\cite{malkov2018efficient}.
{\JIANYANG Graph-based methods are one family of state-of-the-art AKNN algorithms that exhibit dominant performance on the time-accuracy tradeoff for 
% in-memory AKNN query~\cite{malkov2018efficient, li2019approximate, FuNSG17, fu2019fast, diskann, NSW}.
{\JIANYANGREVISION in-memory AKNN query~\cite{malkov2018efficient, NSW, li2019approximate, fu2019fast, fu2021high, SISAP_graph}}.
{\CHENGC These methods construct graphs based on the data vectors, where} a vertex corresponds to a data vector. }
One famous graph-based method is the hierarchical navigable small world graphs (\texttt{HNSW})~\cite{malkov2018efficient}. It's composed of several layers. Layer 0 (base layer) contains all data vectors and layer $i+1$ only keeps a subset of the vectors in layer $i$ randomly. 
The size of each layer decays exponentially as it goes up.
% and the subset to keep is chosen randomly. 
In particular, the top layer contains only one vertex. Within each layer, a vertex is connected to its several approximate nearest neighbors,
%\footnote{Two parameters are preset to control its construction: $M$ to control the number of neighbors and $efConstruction$ to control the quality of approximate nearest neighbors.}
{\JIANYANG while between adjacent layers, two vertexes are connected only if they represent the same vector. }
% only the vertexes representing the same vector are connected.
{\CHENG An illustration of the \texttt{HNSW} graph is provided in Figure~\ref{fig:routing}.}

During the query phase, greedy search is first performed on upper layers to find a good entry at layer 0 (the base layer). 
Specifically, the search starts from the only vertex of the top layer. 
%Within each layer, it does greedy search until it falls into a minimum. 
{\JIANYANG Within each layer, it does greedy search iteratively. At each iteration, it accesses all the neighbors of its currently located vertex and goes to the one with the minimum distance. It terminates the search when {\CHENG none of the neighbors has a smaller distance than the currently located vertex.}} Then it goes to the next layer and repeats the process until {\CHENG it arrives at} layer 0. 
% As for the search algorithm in layer 0, it's a general strategy applied in most graph-based methods~\cite{malkov2018efficient, li2019approximate, FuNSG17, fu2019fast, diskann, NSW} as summarized in \cite{graphbenchmark}, i.e. \textit{greedy beam search} (\textit{best first search} in \cite{graphbenchmark}).
{\CHENG At layer 0, it conducts \textit{greedy beam search}~\cite{graphbenchmark} (\textit{best first search}), which is adopted by 
% most graph-based methods~\cite{malkov2018efficient, li2019approximate, FuNSG17, fu2019fast, diskann, NSW}.
most graph-based methods~\cite{malkov2018efficient, li2019approximate, fu2019fast, diskann, NSW}.
}
%As summarized in \cite{graphbenchmark}, most existing methods~\cite{malkov2018efficient, li2019approximate, FuNSG17, fu2019fast, diskann, NSW} adopt the strategy of greedy beam search (also termed best first search in \cite{graphbenchmark}).
%\textbf{Best First Search} (also refer to as greedy beam search in \cite{adaptive2020ml, learning2route2019ml})}. 
To be specific, {\CHENG greedy beam search maintains two sets}: a search set $\mathcal S$ (a min-heap by exact distances) and a result set $\mathcal R$ (a max-heap by exact distances). The search set $\mathcal S$ {\CHENG has its size unbounded and} maintains candidates yet to be searched. The result set $\mathcal R$ {\CHENG has its size bounded by $N_{ef}$ and} maintains $N_{ef}$ nearest neighbors {\CHENG visited so far}, where the size $N_{ef}$ is the parameter to control time-accuracy trade-off. 
At the beginning, a start point {\CHENG at layer 0} is inserted into both $\mathcal S$ and $\mathcal R$. Then {\CHENG it proceeds in iterations. At each iteration, it pops 
% an object from set $\mathcal{S}$
{\JIANYANG the object with the smallest distance in set $\mathcal{S}$}
and enumerates the neighbors of the object. 
For each neighbor, it \textbf{checks whether its distance from the query object is 
% at most
{\JIANYANGLAST no greater than}
the maximum distance in set $\mathcal{R}$ and if so, it computes the distance} (i.e., it conducts a DCO).
% (we call this a \underline{distance comparison} operation)
% evaluates its distance from query object, and 
In addition, 
% if the object is a positive one,
{\CHENGB if the distance is smaller than the maximum distance in $\mathcal{R}$,}
it (1) pushes the object {\JIANYANGLAST into} both set $\mathcal{S}$ and set $\mathcal{R}$ (using the computed distance as the key)
% if the distance is at most the maximum one in $\mathcal{R}$. 
and (2) pops {\CHENGC the object with the maximum distance} from set $\mathcal{R}$ whenever $\mathcal{R}$ involves more than $N_{ef}$ objects so that the size of $\mathcal{R}$ is bounded by $N_{ef}$.
It returns $K$ objects in $\mathcal{R}$ with the smallest distances when the minimum distance in $\mathcal S$ becomes larger than the maximum distance in $\mathcal R$ and stops.
% {\JIANYANG It's worth noting that the greedy beam search follows the framework of iteratively generating candidates and maintaining KNN in an implicit way. The KNN set is implicitly maintained inside the result set $\mathcal R$.}
% {\JIANYANG Finally, it returns the $K$ nearest objects in set $\mathcal{R}$ as the results. }
%When visiting a new candidate, greedy beam search compares its distance with the maximum in $\mathcal R$ (the $N_{ef}$th NN) instead of the current $K$th NN because it needs exact distance to guide graph routing, while at the same time, its KNNs are also successfully maintained in $\mathcal R$.}
%
We note that the greedy search at upper layers corresponds to a greedy beam search process with $N_{ef}=1$.

 
% at each iteration, it greedily fetches the object with the minimum distance in $\mathcal S$ {\CHENG via a pop operation}, and evaluates the distance of all its neighbors to update $\mathcal S$ and $\mathcal R$, i.e. if the distance of a newly visited object is at most the maximum of $\mathcal R$, then it's inserted into $\mathcal S$ and replace the maximum in $\mathcal R$. 
% Finally, when the minimum in $\mathcal S$ is larger than the maximum in $\mathcal R$, indicating that the search gets into a local minimum, then the algorithm terminates and returns the best $K$ out of $N_{ef}$ objects in $\mathcal R$. 
% In this algorithm, distance comparison lies in the process of evaluating distance and dynamically updating $\mathcal S$ and $\mathcal R$, which can be enhanced by our framework. 
%it maintains a search set $\mathcal S$ and a result set $\mathcal R$ with the restriction of $|\mathcal R| \le N_r$. The search starts by initializing $\mathcal {S,R}$ with some entry points. Then iteratively, each time, it greedily accesses the vertex  with the minimum distance in $\mathcal S$ and visits all its neighbors. For a neighbor, if its distance is at most that of the maximum of $\mathcal R$, then it's inserted into $\mathcal R$ to update the result set. To keep the size of $\mathcal{R}$, the vertices with the maximum distance are deleted if $|\mathcal R| > N_r$. The algorithm terminates when the minimum in $\mathcal S$ is larger than the maximum of $\mathcal R$, i.e. $\mathcal R$ is not updated anymore. Obviously, a larger $N_r$ leads to more visited candidates, and as a result, higher accuracy and longer response time.

% {\CHENG 21-07-Cheng: (1) We need discuss the importance of the distance comparison operation; (2) Is the time complexity of a graph-based method is dominated by the cost of greedy beam search process?

% \begin{figure}[thb]
%     \centering
%     \vspace{-4mm}
%     \subfigure[\texttt{HNSW}]{
%         \includegraphics[width=0.45\linewidth]{experimental result/HNSW.pdf}
%        \label{fig:cost HNSW}
%     }
%     \subfigure[\texttt{IVF}]{
% 	    \includegraphics[width=0.45\linewidth]{experimental result/IVF.pdf}
% 	    \label{fig:cost IVF}
%     }
%     \vspace{-4mm}
%     \caption{{\CHENG Breakdown of Running Times of} AKNN Algorithms.}
%     % \vspace{-4mm}
%     \label{fig:cost statistics}
% \end{figure}

\begin{figure}[thb]
    \captionsetup[subfigure]{aboveskip=-3pt}
    \centering
    \vspace{-4mm}
    \begin{subfigure}[b]{0.32\linewidth}
        \includegraphics[width=\textwidth]{revision experimental result/HNSW.pdf}
        \caption{\texttt{HNSW}}
        \label{fig:cost HNSW}
    \end{subfigure} 
    \begin{subfigure}[b]{0.32\linewidth}
        \includegraphics[width=\textwidth]{revision experimental result/IVF.pdf}
        \caption{\texttt{IVF}}
	  \label{fig:cost IVF}
    \end{subfigure} 
    \begin{subfigure}[b]{0.32\linewidth}
        \includegraphics[width=\textwidth]{revision experimental result/IVFPQ.pdf}
        \caption{\texttt{IVFPQ}}
	  \label{fig:cost IVFPQ}
    \end{subfigure} 
    % \subfigure[\texttt{HNSW}]{
    %     \includegraphics[width=0.3\linewidth]{revision experimental result/HNSW.pdf}
    %    \label{fig:cost HNSW}
    % }
    % \subfigure[\texttt{IVF}]{
	   %  \includegraphics[width=0.3\linewidth]{revision experimental result/IVF.pdf}
	   %  \label{fig:cost IVF}
    % }
    % \subfigure[\texttt{IVFPQ}]{
	   %  \includegraphics[width=0.3\linewidth]{revision experimental result/IVFPQ.pdf}
	   %  \label{fig:cost IVFPQ}
    % }
    \vspace{-4mm}
    \caption{{\JIANYANGREVISION {\CHENG Breakdown of Running Times of} AKNN Algorithms.}}
    \vspace{-4mm}
    \label{fig:cost statistics}
\end{figure}


\smallskip
\noindent\textbf{DCO v.s. Overall Time Costs.} We review the time complexity of \texttt{HNSW} 
% {\JIANYANGLAST with} 
assuming {\JIANYANGLAST that} it adopts \texttt{FDScanning} for DCOs. Let $N_s$ be the number of {\JIANYANGLAST the} candidates of KNN objects, which are visited by \texttt{HNSW}.
% and $D$ be the number of dimensions of an object. 
Then, the total cost of the DCOs is $O(N_s D)$ and that of updating the sets $\mathcal{S} $ and $\mathcal{R} $ is $O(N_s \log N_s)$. Therefore, the time complexity of \texttt{HNSW} is $O(N_s D + N_s \log N_s)$. In practice, the total cost of DCOs should be the dominating part since $D$ can be 
% hundreds or thousands
{\JIANYANGREVISION hundreds}
while $\log N_s$ is a few dozens only for a big dataset involving millions of objects.
%
We verify this empirically as well. 
Figure~\ref{fig:cost HNSW} profiles the time consumption of \texttt{HNSW} on three real-world datasets when targeting 95\% recall with $K = 100$. 
According to the results, on datasets with various dimensions from 256 to 960, DCOs take from 77.2\% to 87.6\% of the total running time of \texttt{HNSW} (as indicated by the blue and orange portions of the bars). 
%
% With techniques developed in this paper, the time cost of each distance comparison would be reduced from
% $O(D)$ to $O(\alpha^{-2}\log D)$ in most cases
% being linear wrt $D$ to being logarithmic wrt $D$ in most cases with accuracy guarantees.
% and correspondingly, the time cost of \texttt{HNSW} would be reduced to 
% % $O(N_s \cdot %\log DK\log D+ N_s \cdot \log N_s)$.
% $O(D \log N + c_{\alpha} \cdot N_s \log D + N_s \log N_s)$,
% where $c_{\alpha}$...

\smallskip
\noindent\textbf{Positive v.s. Negative Objects.} We verify empirically that for \texttt{HNSW}, the number of DCOs on negative objects is significantly larger than that of DCOs on positive objects. 
The results are shown in Figure~\ref{fig:cost HNSW}. We note that in the figure, the ratio between the cost of DCOs (on  negative objects) and that (on positive objects) reflects the ratio between the numbers of negative and positive objects since a DCO on a negative object and that on a positive object have the same cost. 
According to the results, the number of negative objects is 4.3x to 5.2x times more than that of the positive ones.
% ; for IVF, the number of negative objects is 60x to 869x more than that of the positive ones.
}
% {\JIANYANG Moreover, the greedy search at upper layers is a special case of greedy beam search with $N_{ef}=1$. Thus, overall, distance comparison is the dominant part all through the searching algorithm.}
% {\JIANYANG Our techniques improve the time complexity of each distance comparison to be \textbf{logarithmically} dependent on $D$.}

% {\CHENG ***(1) Please give it some thoughts whether we can provide some running time statistics of the search process above layer 0, distance comparison operations (at layer 0) and updating the sets (at layer 0). (2) We can also make a comment on how much we have improved the time complexity of time comparison operations in this paper here.***}

% {\JIANYANG 22-07-Jianyang: (1) We may reduce the greedy search at upper layers to greedy beam search.(modified above) *** (2) I think without the background of Johnson-Lindenstrauss Lemma, readers might not feel the improvement. ***}

% {\JIANYANG 23-07-Jianyang: modified}



\subsubsection{Inverted File Index}

Inverted file~\cite{jegou2010product} index is another popular index method for AKNN query. {\CHENG According to~\cite{adaptive2020ml}, \texttt{IVF} is one of the state-of-the-art approaches for AKNN. Indeed, according to our experimental results in Section~\ref{section:time-accuracy}, it outperforms \texttt{HNSW} on some datasets. 
% \texttt{IVF} has two phases, namely index phase and query phase.
} 
During the index phase, the algorithm clusters data vectors with the K-means algorithm, builds a bucket for each cluster and assigns each data vector to its corresponding bucket. Then during the query phase, for a given query, the algorithm first selects the $N_{probe}$ nearest clusters based on their centroids, retrieves all vectors in these corresponding buckets as candidates, and then 
% selects final 
finds out KNNs {\CHENG among the retrieved vectors.}
% In the \texttt{IVF} algorithm, 
Here,
$N_{probe}$ is a user parameter which controls 
% the number of visited buckets so as to control 
the time-accuracy trade-off. 
%{\JIANYANG
%When selecting final KNNs, it first pre-computes exact distance for all retrieved vectors.
% for upcoming distance comparisons. 
%Then to find out KNNs, it applies quick select~\cite{quickselect}, where distance comparisons are heavily utilized. 
%*** By the way, I thought it's already been very obvious that selecting K nearest neighbor from a candidate set needs distance comparison. ***
%In particular, the time complexity of exact distance evaluation is $O(N_s \cdot D)$ and that of quick select is $O(N_s)$. 
%As a result, distance evaluation is still the dominant part of this algorithm.
%}
% {\JIANYANG
% When selecting final KNNs, it maintains a KNN set $\mathcal K$ as scanning candidates. Similar to HNSW, let $N_s$ be the number of candidate objects. The total cost of the DCOs is $O(N_s \cdot D)$ and that of updating $\mathcal K$ is $O(N_s \log K)$, in which the cost of DCO is also the dominant part.}
{\CHENG When {\JIANYANGLAST finding out} KNNs, a commonly used method is to maintain a KNN set $\mathcal K$ with a max-heap of size $K$. It then scans all candidates, and for each one, it \textbf{checks whether its distance is 
% at most
{\JIANYANGLAST no greater than}
the maximum of $\mathcal{K}$ and if so, it computes the distance} (i.e., it conducts a DCO). Here, the maximum distance is defined to be $+\infty$ if $\mathcal{K}$ is not full.
% If the result of the DCO is yes, 
{\CHENGB If the distance is smaller than the maximum distance in $\mathcal{K}$,}
it 
% inserts the candidate to $\mathcal{K}$
{\JIANYANG updates $\mathcal{K}$ with the candidate}
(by using the computed distance as the key). It returns the objects in $\mathcal{K}$ at the end. An illustration of the \texttt{IVF} structure is provided in Figure~\ref{fig:ivf}.

\smallskip
\noindent\textbf{DCO v.s. Overall Time Costs.} We review the time complexity of \texttt{IVF}
% {\JIANYANGLAST with} 
assuming {\JIANYANGLAST that} it adopts \texttt{FDScanning} for DCOs.
Let $N_s$ be the number of candidate objects. The total cost of \texttt{IVF} is $O(N_s D + N_s\log K)$, where the first term is the cost of the DCOs and the second term is that of updating $\mathcal K$. As can be noticed, the cost of DCOs is the dominating part. }
%
We verify this empirically as we did for \texttt{HNSW}. 
Figure~\ref{fig:cost IVF} shows the results.
% profiles the time consumption of \texttt{IVF} on three real-world datasets when targeting 95\% recall with $K = 100$. 
According to the results, on datasets with various dimensions from 256 to 960, DCOs take from 85.0\% to 95.3\% of the total running time of \texttt{IVF}. 
%
% With the techniques developed in this paper, the time time complexity of \texttt{IVF} would be reduced to $O(N_s\cdot \log D + N_s\log K)$.}
% {\CHENG ***Again, it would be better to talk about how the algorithm selects the final KNNs and show that distance comparisons are involved. In addition, we can provide some discussions on the importance of the DCOs (e.g., with time complexity analysis as we do for the \texttt{HNSW} algorithm).***}
% with evaluating exact distance. 
% In this algorithm, distance comparison happens during evaluating candidates and selecting final KNNs, which can be accelerated by our framework. 
% It's worth noting that \texttt{IVF} is also usually used in combination with product quantization~\cite{jegou2010product,annbenchmark, ge2013optimized, ITQ} {\CHENG for cases with limited memory.}

\smallskip
\noindent\textbf{Positive v.s. Negative Objects.} We verify empirically that for \texttt{IVF}, the number of DCOs on negative objects is significantly larger than that of DCOs on positive objects. 
The results are shown in Figure~\ref{fig:cost IVF}. 
According to the results, the number of negative objects is 60x to 869x more than that of the positive ones.
% In these cases, the algorithms would also involve distance comparisons, which can be improved with the techniques developed in this paper.
% for the concern of limited memory, which is not the focus of our work.

%Filter-and-verification is a popular framework in NN query, especially for quantization-based and hashing-based algorithms. In particular, quantization-based methods~\cite{jegou2010product, imi} suggest a three-stage strategy: 1) generate candidate lists with coarse quantization code; 2) shrink the list with finer code; 3) re-rank the list with exact distance. {\color{red} Hashing-based methods also follow filter-and-verification framework~\cite{datar2004locality, c2lsh}.} During indexing phase, hashing-based methods map all data vectors into hash codes and put vectors with the same code into the same bucket. Then during query phase, they first map query vectors to hash codes and then generate candidates by collecting objects in the corresponding buckets or adjacent buckets. The generated candidates are then re-ranked with exact distance to find out NNs. {\color{red} bad paragraph}

%Though the re-ranking stage is seldom emphasized and sometimes even dropped due to memory constraint~\cite{johnson2019billion, jegou2011searching}, {\color{red} it's widely and independently reported that without the re-ranking stage~\cite{ge2013optimized, adaptive2020ml, sun2014srs}}, the accuracy(measured by recall at K for KNN query) is usually unacceptablely low, especially for queries with large K(e.g. K=100), which reveals the importance of it. Our methods target to accelerate re-ranking while disregard the specific methods of generating candidates. 

% {\CHENG 14-07-Cheng: (1) After each AKNN is reviewed, make some comment on how the distance comparison plays a role in the algorithm; (2) Better to include a paragraph discussing other AKNN algorithms (somehow in a bit more brief way) and and also comment on how distance comparison plays a role in them.}

% {\JIANYANG 15-07-Jianyang: (1) (2) I thought it was the same problem as the one in Section 1, which might need further discussion.}


\subsubsection{Other AKNN Algorithms}
% {\JIANYANGREVISION (1) graph-based~\cite{malkov2018efficient, NSW, li2019approximate, fu2019fast, fu2021high, SISAP_graph}}, (2) quantization-based~\cite{jegou2010product, ge2013optimized, guo2020accelerating, ITQ, additivePQ, imi}, (3) tree-based ~\cite{muja2014scalable, dasgupta2008random, ram2019revisiting, beygelzimer2006cover, reviewer_M_tree} and (4) hashing-based~\cite{indyk1998approximate, datar2004locality, c2lsh, tao2010efficient, huang2015query, sun2014srs, lu2020vhp, zheng2020pm}.

{\JIANYANG
In other AKNN algorithms {\CHENGB including tree-based, hashing-based, and quantization-based methods}, DCOs are also ubiquitous. 
% The main difference among AKNN algorithms lies in how they generate candidate vectors. 
{\JIANYANGREVISION Tree-based methods~\cite{muja2014scalable, dasgupta2008random, ram2019revisiting, beygelzimer2006cover, reviewer_M_tree}} generate candidate vectors through tree routing and {\JIANYANGLAST find out} KNNs with DCOs {\JIANYANGB (similarly as \texttt{IVF} does).}
% \texttt{HNSW} (resp. \texttt{IVF}) does when the candidates are generated dynamically (resp. in a batch)).
% \footnote{Some might also generate candidates and update KNNs alternatively like graph-based methods}.
Hashing-based methods~\cite{indyk1998approximate, datar2004locality, c2lsh, tao2010efficient, huang2015query, sun2014srs, lu2020vhp, zheng2020pm} generate candidate vectors via hashing {\CHENGC codes} and {\JIANYANGLAST find out} KNNs with DCOs (similarly as \texttt{IVF} does). 
Quantization-based methods~\cite{jegou2010product, learningtohash,ge2013optimized, ITQ, imi, additivePQ} generate candidates with short quantization codes, and conduct re-ranking (for {\JIANYANGLAST finding out} KNNs) with DCOs (similarly as \texttt{IVF} does).
%
% For nearly all of the AKNN algorithms above, 
{\CHENG For tree-based and hashing-based methods,}
the cost of DCOs is dominant because (1) one time tree routing or hashing bucket probing generates multiple candidates ({\CHENG which entail} multiple DCOs) and (2) tree routing and hashing bucket probing are much faster than a DCO (which has the time complexity of $O(D)$). 
{\CHENG For product quantization-based methods~\cite{jegou2010product, ge2013optimized, ITQ, imi}, DCOs are involved in its re-ranking stage, which is less dominant because the main cost lies in evaluating quantization codes.} {\JIANYANGREVISION {\chengr For comparison, we show the time decomposition results of \texttt{IVFPQ}, which is a quantization-based method~\cite{jegou2010product},}
% We also show its results of time decomposition 
in Figure~\ref{fig:cost IVFPQ} under the typical setting of \cite{learningtohash, jegou2010product}.}
% where DCOs are not involved. 
% {\CHENG Project quantization involves DCOs in its re-ranking stage.}
% However, there are also some DCOs in its subsequent re-ranking stage, in which our methods can bring some improvements.
}
% {\CHENG 22-07-Cheng: I think we'd better review briefly those other types of AKNN algorithms and point out that they also involve DCOs for which our algorithm can help with though we do not conduct the experiments for them.}

