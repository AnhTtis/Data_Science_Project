
% In this section, we begin with a novel framework for DCO, adaptive dimension sampling. This framework serves as the foundation of our algorithms ARSearch, ARRoute and ARSelect proposed and analyzed as follows.



\section{The \texttt{ADSampling} Method}
\label{sec:adsampling}
% As discussed in previous sections, the inherent needed resolution varies among different candidates. To ensure preciseness and conciseness, we suggest to adaptively set the resolution. However, there are three main challenges: 1) How can we make resolution flexible? 2) We have no prior knowledge about the difficulty of a query. How do we find the minimum needed resolution? 3) Some queries are extremely fragile. How do we recover full-accuracy distance when needed? 

% \subsection{Motivations and Overview}
% \label{subsec:overview}

% {\CHENG 
% % We propose to conduct \emph{nearly-exact} DCOs based on \emph{approximate distances} with probabilistic guarantees for better efficiency. 
% For better efficiency, it is a natural idea is to conduct DCOs based on \emph{approximate distances}.
% Below, we explain (1) the desiderata an approximate distance based method, (2) insufficiency of existing methods (wrt the desiderata), and (3) the overview of our new method \texttt{ADSampling}.
% % Specifically, we aim to develop a method of computing approximate distances of data vectors and then conduct DCOs based on the approximate distances.
% % Specifically, we aim to compute approximate distances and then conduct DCOs based on the approximate distances. 
% % Apart from the ability of providing an error guarantee for the approximate distance, 

% \smallskip 
% \noindent\textbf{(1) Desiderata of distance approximation methods for DCOs.}
% % \subsubsection{Desiderata of distance approximation methods for DCOs.}
% % We identify four desiderata of the method to fully unleash the power of using approximate distances for reliable DCOs.
% \underline{First}, it should be able to provide an error guarantee of an approximate distance since 
% % otherwise, the accuracy of DCOs would not be guaranteed.
% with the error guarantees of approximate distances, the results of DCOs can be accurate with guarantees and even exact.
% For example, suppose the distance threshold $r$ is 1 and a data vector $\mathbf{o}_1$ has an approximate distance $1.05$ (with a relative error bound $5\%$). Then, we know that the exact distance of $\mathbf{o}_1$ is at least $1.05 / (1+0.05)=1$ (i.e., $r$).
% % , is not smaller than the maximum in $\mathcal{K}$ (i.e., 1).
% \underline{Second}, it should have the \emph{flexibility} of achieving different ``resolutions'' of approximate distances for different datan objects, and correspondingly, it would provide different error bounds for different objects.
% % One problem is that fixed resolution cannot provide exact comparison results for all candidates. 
% Back to the aforementioned example, we consider another candidate $\mathbf{o}_2$. Suppose $\mathbf{o}_2$'s approximate distance to the query, when measured at the same resolution as that of $\mathbf{o}_1$'s approximate distance, is $1.01$, and the corresponding error bound is $5\%$. In this case, we are not able to conclude on whether $\mathbf{o}_2$ has a smaller distance than $r$ or not.
% % the maximum one in $\mathcal{K}$. 
% Instead, we need to increase the resolution so that it would provide a better error bound to be able to make a conclusion, e.g., at a resolution with its corresponding error bound of $1\%$, we know that the exact distance of $\mathbf{o}_2$ is at least $1.01 / (1+0.01)=1$ (i.e., $r$).
% %
% In general, for objects that are farther away from the query vector, approximate distances with lower resolutions would be sufficient for accurate DCOs; whereas for objects that are closer from the query vector, for objects closer from the query vector, approximate distances with higher resolutions (or even full resolutions) are necessary for accurate DCOs.
% % , is not smaller than the maximum in $\mathcal{K}$ (i.e., 1).
% % $dis_{2,A} \ge dis'_{2,A} / (1+1\%) = 1 = dis_{i_K}$.
% }
% % can have exact DCO results with the approximate distance.}
% % {\CHENG For object 2, if the resolution of the approximate distance an error bound of $5\%$ is not sufficient for deciding whether object 2 has a smaller distance from the query object than the current Kth NN in $\mathcal{K}$ or not, but an error bound of $1\%$ is.}
% % Unlike object 1, with the error bound of $5\%$, the order between object 2 and the current Kth NN in $\mathcal{K}$ is undecidable. It becomes decidable once the error bound becomes $1\%$, but then Once we increase the resolution to $1\%$, however, it would be redundant for candidate 1 so as to harm efficiency. Thus, it's of vital importance to realize \textit{flexibility} of resolution, i.e. different resolution for different candidates. 
% % Another problem comes from the way we set the resolution for a certain candidate. For a newly visited object, we have no prior knowledge about it. To find its minimum needed resolution, we need a mechanism to determine it \textit{adaptively}. 
% {\CHENG \underline{Third}, it should have the ability to \emph{adaptively} determine an appropriate resolution of the approximate distance for an object such that the it is neither more than enough (which means some computation of distance evaluation could be saved otherwise) nor lower than necessary (which means a firmed result of DCO cannot be obtained).}
% % Finally, some candidates are extremely fragile to approximation, which really requires exact distance to obtain comparison results. Conventional methods evaluate exact distance with raw vectors from scratch when fragility is detected. However, it gives rise to re-evaluation overhead. We claim that a method should better progressively \textit{recover} exact distance from known approximate distance to guarantee that it's no worse than the plain distance evaluation at any case.
% %
% {\CHENG\underline{Fourth}, it should have the ability to \textit{recover} the exact distance by reaching the highest resolution with the error bound of 
% %1 
% {\JIANYANG 0\%}.
% % from known approximate distance to guarantee that it's no worse than the plain distance evaluation at any case. 
% This is because some candidates have their distances extremely fragile to approximation, and in these cases, exact distances are not avoidable to obtain firmed results of DCO.}
% %
% In summary, to efficiently produce reliable DCO results with approximate distances, a method should have (1) guaranteed error bound, (2) flexibility of multiple resolutions, (3) adaptivity of reaching an appropriate resolution and (4) recoverability of the exact distance. 

% \begin{table}[h]\small
%   \caption{Methods of Distance Approximation}
%   \label{tab:freq}
%   \begin{tabular}{c|cccc}
%     \toprule
%     &DR &QT &RP &\textbf{ADSampling}\\
%     \midrule
%   Error Bound &NO &NO & \textbf{Probabilistic} &\textbf{Probabilistic}\\
%   Flexibility &NO &\textbf{Limited} & NO &\textbf{YES}\\
%   Adaptivity &NO &NO & NO &\textbf{YES}\\
%   Recoverability &NO &NO  & NO &\textbf{YES}\\
% %   \midrule
% %   DCO &NO &NO &NO &\textbf{Probabilistic}\\
%   \bottomrule
% \end{tabular}
% \end{table}

% \smallskip 
% \noindent\textbf{(2) Insufficiency of existing distance approximation methods for DCOs.}
% % \subsubsection{Insufficiency of existing distance approximation methods for DCOs.}
% % Unfortunately, no existing distance approximation methods satisfy all of the four aforementioned desiderata, to the best of our knowledge. 
% % While there there quite a few distance approximation methods that have been developed for AKNN, none of them satisfy all of the four aforementioned desiderata, to the best of our knowledge.
% There are two main forms of distance approximation that have been used for AKNN: 
% % nearest neighbor search: 
% 1) quantization (QT)~\cite{ge2013optimized, jegou2010product,ITQ, additivePQ, guo2020accelerating}, and 2) dimension reduction, in which dimension reduction can be further categorized into optimization-based dimension reduction (DR)~\cite{wold1987principal, kruskal1964multidimensional} and random projection (RP)~\cite{johnson1984extensions, blockjlt, fftjlt}.
% {\CHENG Unfortunately, none of these existing methods has all of the aforementioned four desiderata} (a summary is presented in Table~\ref{tab:freq}). 
% First, QT and DR (e.g., PCA~\cite{wold1987principal}) optimize a compressed representation to minimize the \emph{total} approximation error instead of the \emph{maximum} one, which fails to guarantee an error bound. {\CHENG Only RP provides some probabilistic error bounds.
% Second, DR and RP do not support flexible resolutions of approximate distances.
% % Flexible resolution and is a long but implicitly adopted strategy in QT. 
% Only QT supports flexible resolutions to some extent with a three-stage strategy \cite{jegou2010product, imi, surveyl2hash}:}
% % to apply different resolution on different objects: 
% 1) generate candidate lists with coarse code; 2) shrink the list with finer code; 3) re-rank the list with exact distance~\footnote{Some stages could be skipped according to specific requirements, e.g., memory constraint~\cite{johnson2019billion, jegou2011searching}.}. 
% {\CHENG Nevertheless, the list size and code size at each stage are preset hyper-parameters and fixed for all queries, and thus QT provides very limited flexibility only.}
% % so it's very limited in flexibility and hard to tune. 
% Third, to the best of our knowledge, no existing methods achieve adaptivity and recoverability on resolution of distance for high-dimensional nearest neighbor search. It's worth noting that hashing-based methods~\cite{indyk1998approximate, datar2004locality, c2lsh, tao2010efficient, huang2015query, lu2020vhp} are also popular for data compression. They target to map close vectors to similar hash codes and {\CHENG use code comparison as a proxy of DCO}.
% % DCO with hash code comparison. 
% However, hashing 
% %doesn't 
% does not explicitly approximate distances, and thus they're not within the scope of our discussion.  \footnote{Also, since different vectors may be mapped to the same code, hashing cannot 
% % identify their order.
% {\CHENG help with exact DCO}.
% }
% {\JIANYANG We also emphasize that hashing and quantization cannot provide guarantee for DCO, and thus can be used only in the first stage of AKNN query, i.e., generating candidates but not the second.}

{\CHENGB Recall that our goal is to achieve \emph{reliable} DCOs with better efficiency than \texttt{FDScanning}. To this end,}
we develop a new method called \texttt{ADSampling}. 
At its core, \texttt{ADSampling} projects the objects to vectors with \emph{fewer} dimensions and conduct DCOs based on the projected vectors for better efficiency.
% \emph{random projection} process~\cite{johnson1984extensions, datar2004locality, indyk1998approximate, sun2014srs, c2lsh}, 
Different from the conventional and widely-adopted random projection technique~\cite{johnson1984extensions, datar2004locality, sun2014srs, c2lsh}, which projects \emph{all} objects to vectors with \emph{equal} dimensions, \texttt{ADSampling} is novel in the following aspects. 
First, it projects \emph{different} objects to vectors with \emph{different} numbers of dimensions during the query phase \emph{flexibly}.
We will elaborate on details of how this idea is implemented in Section~\ref{subsec:idea-1}. 
Second, it decides the number of dimensions to be sampled for each object \emph{adaptively} based on 
% the query 
% {\JIANYANGLAST its corresponding DCO}
{\CHENGC the DCO on the object}
during the query phase, but not pre-sets it to a certain number during the index phase (which is knowledge demanding and difficult to set in practice).
We will elaborate on details of how this idea is implemented in Section~\ref{subsec:idea2}. 
In addition, we summarize \texttt{ADSampling} and 
% show
{\JIANYANG prove}
that it has its time \emph{logarithmic} wrt $D$ for negative objects (which is significantly better than the time complexity $O(D)$ of \texttt{FDScanning}) in Section~\ref{subsection:theoretical analysis of ADSampling}.
% {\CHENG
% Recall a DCO operation is to decide for a given object $\mathbf{x}$, whether its distance $dis$ is 
% % larger than a 
% smaller than or equal to a threshold $r$ (i.e., $dis \le r$). As explained in Section~\ref{sec:introduction}, almost all existing AKNN algorithms (such as the \texttt{HNSW} algorithm and the \texttt{IVF} algorithm) have their time costs largely dominated by that of DCO operations. Therefore, the efficiency of this operation is critical and largely determines that of an AKNN algorithm. 

% In an AKNN algorithm, this operation is typically conducted by evaluating the distance and comparing the distance with $r$, which has the time complexity of $O(D)$. Since $D$ is usually hundreds and even thousands for high-dimensional objects in practice and this operation is frequently conducted by an AKNN algorithm, the efficiency of the AKNN algorithm is largely affected. To achieve better efficiency of an AKNN algorithm, we aim to conduct the DCO operation \emph{near-exactly} based on \emph{approximate distances}. 
%
% \smallskip 
% \noindent\textbf{(3) Overview of a new method \texttt{ADSampling} for DCOs.}
% % \subsubsection{Overview of a new method \texttt{ADSampling} for DCOs.}
% In this paper, we develop a method called \texttt{ADSampling}, which has all the aforementioned four desiderata with two core ideas. 
% In this paper, we develop a method called \texttt{ADSampling}, 
% % which projects different objects to vectors with different numbers of dimensions \emph{flexibly} and \emph{adaptively} and then computes approximate distances based on the sampled dimensions for reliable DCOs.
% {\JIANYANG 
% which \emph{flexibly} and \emph{adaptively} samples different number of dimensions for reliably conducting the DCOs of different objects. 
% }
% Specifically, it has two core 
% % ideas
% {\JIANYANG steps.}
% % which has all the aforementioned four desiderata with two core ideas. 
% \underline{First}, 
% {\JIANYANG during the index phase, it preprocesses data vectors with \emph{random orthogonal transformation}~\cite{johnson1984extensions, choromanski2017unreasonable, randomortho, vershynin_2018}.}
% % it randomly transforms an object with \emph{random orthogonal transformation}~\cite{johnson1984extensions, choromanski2017unreasonable, randomortho, vershynin_2018} and computes an approximate distance based on some sampled dimensions of the transformed vector. 
% % {\JIANYANG \underline{First}, we consider incorporating the power of random projection into our method. Specifically, we target to make row sampling on vectors equivalent to random projection so as to utilize Lemma~\ref{lemma:concen}, and hopefully, sampling all $D$ dimensions equivalent to evaluating exact distance.} 
% % We will show in Section~\ref{subsec:idea-1} that this can be achieved by pre-processing data vectors (during index phase) and query vectors (during query phase) with random orthogonal transformation.
% % This idea enables \texttt{ADSampling} to achieve \emph{guaranteed error bound} on the approximate distances, \emph{flexible resolutions} of approximate distances, and \emph{recoverability} of exact distance.
% This idea enables \texttt{ADSampling} to achieve \emph{flexible resolutions} of the approximate distances since it can sample different numbers of dimensions of the transformed vectors of different objects.
% %
% In addition, 
% {\JIANYANG the approximate distances computed from sampled dimensions has some equivalence to those from the low-dimensional vectors produced by random projection~\cite{johnson1984extensions, vershynin_2018} and thus, share all its advantages including low error rate and rigorous theoretical guarantee.
% }
% % the errors of the computed approximate distances are bounded, which we verify by establishing an equivalence between the vector of sampled dimensions and that produced by random projection~\cite{johnson1984extensions, vershynin_2018}.
% %
% We will elaborate on more details of this idea in Section~\ref{subsec:idea-1}. 
% \underline{Second}, {\JIANYANG during the query phase, when handling a DCO,} it samples the dimensions of a transformed vector in an \emph{aggressive} and \emph{incremental} manner (i.e., it starts with a few dimensions and samples more and more dimensions) and leverages hypothesis testing for deciding whether a sufficiently confident conclusion for a DCO can be made with the approximate distance computed based on the dimensions sampled so far. This idea enables \texttt{ADSampling} to achieve the \emph{adaptivity} of sampling the minimum number of dimensions for reliable DCOs. 
% We will elaborate on more details of this idea in Section~\ref{subsec:idea-2}. 
% Finally, we summarize \texttt{ADSampling} and 
% % show
% {\JIANYANG prove}
% that it has its time \emph{logarithmic} wrt $D$ for negative objects (which is significantly better than the time complexity $O(D)$ of \texttt{FDScanning}) in Section~\ref{subsection:theoretical analysis of ADSampling}.
% }

%To solve these issues, we propose adaptive dimension sampling (AdaSampling) framework. AdaSampling is a probabilistic DCO framework for high-dimensional Euclidean space. It automatically adapts to the inherent difficulty of each comparison. The idea is as follows: for two vectors $\mathbf{o}_i$ and $\mathbf{o}_j$, to compare their distance to query $\mathbf{q} $, supposing that we have the true distance $dis_i$ of object $i$, we first sample a small number of dimensions and calculate the observed distance $dis'_j$ for object $j$ on the sampled space. Then with the observed distance, if we don't have enough confidence to conclude their order, then we "level it up" by sampling more dimensions. Repeatedly, we keep leveling up object $j$ until their order can be confidently determined. 

%{\color{red} I need some figures here.}
\subsection{\textbf{Dimension Sampling over {\JIANYANG Randomly} Transformed Vectors}}
\label{subsec:idea-1}

{\CHENG 
% To achieve flexible resolutions of approximate distances, a natural idea is to take different numbers of dimensions of an object flexibly. However, simply taking several dimensions of an object can hardly preserve the distance of the object in practice, let alone guaranteed error bound. To achieve error bounds of approximate distances, 
{\CHENGB For better efficiency of a DCO, a natural idea is} to conduct a random projection~\cite{johnson1984extensions, vershynin_2018} on an object (i.e., to multiply the object (specifically its vector) with a $\mathbb{R}^{d\times D} $ random matrix $P$ where $d < D$~\footnote{{\JIANYANG There are multiple types of random matrices used for random projection~\cite{blockjlt, fftjlt, datar2004locality, johnson1984extensions}. 
% The most popular type in the database community is random Gaussian matrix whose entries are independent standard Gaussian random variable~\cite{datar2004locality, c2lsh, sun2014srs, huang2015query}. 
In the present work, by random projection, we refer to the random projection based on random orthogonal matrix, which can be generated through orthonormalizing a random Gaussian matrix, {\JIANYANGB whose entries are independent standard Gaussian random variables}~\cite{choromanski2017unreasonable, randomortho, johnson1984extensions, vershynin_2018}.}}),
{\CHENGB and then conduct the DCO using the approximate distance that can be computed based on the projected vector, namely $\sqrt {D/d } \| P \mathbf{x}\|$.}
%
It is well-known that there exists a concentration inequality on the approximate distance as presented in the following lemma~\cite{vershynin_2018}.
%
\begin{lemma}%~\cite{vershynin_2018}
    For a given object $\mathbf{x} \in \mathbb{R}^D  $, a random projection $P \in \mathbb{R}^{d\times D} $ preserves its Euclidean norm with $\epsilon $ multiplicative error bound with the probability of
\begin{align}
    \mathbb{P} \left\{ \left| \sqrt {\frac{D}{d} } \| P \mathbf{x}\| -\| \mathbf{x} \| \right| \le \epsilon \| \mathbf{x} \|  \right\}   \ge 1 - 2e^{-c_0 d \epsilon ^2} 
    \label{equ:concentration}
\end{align}
%    The corresponding failure probability is bounded by
%\begin{align}
%    \mathbb{P} \left\{ \left| \sqrt {\frac{D}{d} } \| P \mathbf{x}\| -\| \mathbf{x} \| \right| > \epsilon \| \mathbf{x} \|  \right\}   \le 2e^{-c_0 d \epsilon ^2} \notag 
%\end{align}
    where $c_0$ is a constant factor and {\JIANYANGREVISION $\epsilon \in (0, +\infty)$}. \label{eq:fail_concen}\label{eq:concen}
    \label{lemma:concen}
\end{lemma}
%
Nevertheless, once an object is projected, the corresponding approximate distance would have a certain resolution {\CHENGC that would be fixed}.
% the reduced dimensionality $d$ is usually fixed and uniform for all objects and thus 
Therefore, it lacks of flexibility of achieving different reduced dimensionalities for different objects (correspondingly different resolutions of approximate distances) during the query phase.

% We aim to combine merits of both the dimension sampling (i.e., its flexibility) and the random projection (i.e., its guaranteed error bounds) while avoiding their shortcomings. 
{\CHENGB We aim to project \emph{different} objects to vectors with \emph{different} numbers of dimensions during the query phase \emph{flexibly}.}
To this end, we propose to \emph{randomly transform} an object (with \emph{random orthogonal transformation}~\cite{johnson1984extensions, vershynin_2018, ITQ}, geometrically, to randomly rotate it) and then flexibly sample dimensions of the transformed vector for computing an approximate distance.
% sample conduct \emph{dimension sampling} on the \emph{random transformed} of data vector via random orthogonal transformation. 
Formally, given an object $\mathbf{x}$, we first apply a \emph{random orthogonal matrix} $P' \in \mathbb{R}^{D \times D} $
% ~\footnote{Following \cite{choromanski2017unreasonable}, we generate random orthogonal matrix $P'$ by (1) generating a square random Gaussian matrix $G \in \mathbb{R}^{D\times D}$, (2) orthogonalizing $G$ with QR decomposition $G=QR$, where $Q$ is an orthogonal matrix and $R$ is an upper triangular matrix, and (3) using $Q$ as $P'$.} 
to $\mathbf{x}$ and then sample $d$ rows on it (for simplicity, the first $d$ rows).
The result is denoted by $(P' \mathbf{x})|_{[1,2,...,d]}$. 
%
% Here, the random orthogonal matrix $P'$ can be obtained by (1) generating a square random Gaussian projection matrix $G\in \mathbb{R}^{D\times D} $, (2) orthogonalizing $G$ with QR decomposition $G=QR$, where $Q$ is an orthogonal matrix and $R$ is an upper triangular matrix, and (3) using $Q$ as $P'$~\cite{choromanski2017unreasonable, yu2016orthogonal}.
%
{\CHENGB This method entails two benefits.} 
First, we achieve the flexibility since we can sample $d$ dimensions of a rotated vector for different $d$'s during the query phase. Second, we achieve a guaranteed error bound since sampling $d$ dimensions on a transformed vector is equivalent to obtaining a $d$-dimensional vector via random projection, which we explain as follows. 

% Suppose that we conduct a random projection operation on $\mathbf{x}$. It would apply 
Recall that a random projection on $\mathbf{x}$ is to apply a random projection matrix $P \in \mathbb{R}^{d\times D}$ to $\mathbf{x}$, and the result is denoted by $P \mathbf{x} $.}
% To achieve 
% % the flexibility of resolution
% {\CHENG a flexible resolution of approximate distances}, a straight-forward idea is to {\CHENG take different numbers of dimensions of a data vector flexibly}. However, 
% % for distance evaluation, each vector is a whole. 
% simply taking several dimensions of a data vector can hardly 
% % produces reasonable results, 
% {\CHENG preserve the distance of the data vector}, let alone guaranteed error bound. 
% %Though optimization-based methods can align subspaces according to their importance, for unseen queries, their performances are totally unpredictable.
% To resolve this issue, we propose to conduct \emph{dimension sampling} on randomly transformed vectors, for which we next show its equivalence to random projection. Such equivalence enables us to ``set different dimensionality of random projection for different candidates'' during query time, which brings flexibility. 
%
% %. To provide clear interpretation and rigorous guarantee for it, we first show the equivalence between random projection and row sampling on randomly transformed vectors.
%
% Consider that we target to reduce the dimensionality of vector $\mathbf{x} $ from $D$ to $d$. A conventional random projection operation is to apply a random projection matrix $P \in \mathbb{R}^{d\times D} $ to $\mathbf{x}$ and get $P \mathbf{x} $. However, in terms of our proposal, we first apply a random orthogonal matrix $P' \in \mathbb{R}^{D \times D} $ to $\mathbf{x} $, sample $d$ rows on it (for simplicity, the first $d$ rows) and get $(P' \mathbf{x})|_{1,2,...,d}$. 
% We claim the above two operations are equivalent in the sense that 
We claim that $(P' \mathbf{x})|_{[1,2,...,d]}$ (the result of our proposed method) and $P \mathbf{x}$ (the result of a random projection) are identically distributed. This is based on an elementary property of matrix multiplication 
% as intuitively illustrated in Figure~\ref{fig:equivalence}, 
that row samplings before and after a matrix multiplication are identical:
%Random projection is to apply a random projection matrix $P \in \mathbb{R}^{d\times D} $ to $\mathbf{x} $. In terms of row sampling on randomly transformed vectors, it first applies a random orthogonal matrix $P' \in \mathbb{R}^{D\times D} $ to $\mathbf{x}$ , and samples $d$ rows on it (for simplicity, the first $d$ rows) to get $(P' \mathbf{x})|_{1,2,...,d}$. We claim the above two methods are equivalent in the sense that $(P' \mathbf{x})|_{1,2,...,d} $ and $P \mathbf{x} $ are identically distributed. This conclusion comes from an elementary property of matrix multiplication, {\color{red} i.e. row sampling before and after a matrix multiplication are identical}:
\begin{align}
    (P'\mathbf{x} )|_{[1,2,...,d]} = P'|_{[1,2,...,d]} \mathbf{x} 
\end{align}
%
% Recall that as introduced in Section~\ref{subsec:rp}, 
We note that {\CHENGB $P'|_{[1,2,...,d]}$ corresponds to a random matrix $P$ for random projection since} one conventional way to generate a random projection matrix $P$ is to sample rows of a $D\times D$ random orthogonal matrix~\cite{choromanski2017unreasonable}.
% , and thus $P'|_{1,2,...,d}$ and $P$ are identically distributed. 
% As a result, sampling $d$ dimensions on a randomly transformed vector is equivalent to % evaluating a $d$-dimensional embedding from {\CHENG obtaining a $d$-dimensional vector via} random projection. 
%
% Note that mathematically, we simply shift row sampling from projection matrices to transformed vectors, while algorithmically, such shift enables us to set dimensionality at any time we want, which brings flexibility. What's more, 
%
Therefore, the concentration inequality for random projection over raw objects (as given in Equation~(\ref{equ:concentration})) can be applied to dimension sampling over {\JIANYANG randomly} transformed vectors, which provides solid foundation for our following discussion. 
% For the sake of convenience, we denote the transformed vectors as $\mathbf{y} := P' \mathbf{x}$.
%

{\CHENGB We denote the transformed vector as $\mathbf{y} := P' \mathbf{x}$.}
{\CHENG Based on the sampled dimensions, we can compute an approximate distance of $\mathbf{x}$, denoted by $dis'$, as follows,
\begin{align}
    dis' := \sqrt { \frac{D}{d}} \left\| \mathbf{y}|_{[1,2,...,d]} \right\|  
    % = \sqrt { \frac{D}{d} \sum_{j=1}^d y_{j}^2} 
\end{align}
where $d$ is the number of sampled dimensions. We note that the time complexity of computing an approximate distance based on $d$ sampled dimensions is $O(d)$.
%
Furthermore, when all $D$ dimensions are sampled, the distance $dis'$ computed based on the sampled dimensions would be equal to the true distance $dis$, which is due to the fact
{\JIANYANG that random orthogonal transformation preserves the norm of any vector (since it simply rotates the space without distorting the distances). 
% This is the reason we adopt random orthogonal matrix instead of some random Gaussian matrices that are commonly used for random projection.
}
}
%a randomly transformed vector has its norm the same as the original vector. This is the reason why we randomly transform each data vector before we sample its dimensions.}

% {\CHENG 14-07-Cheng: If possible, we can include some remarks/discussions on any new ideas proposed, e.g., we can discuss why we adopt random orthogonal project instead of conventional random projection {\JIANYANG it's now discussed in section 3.1.3}, we can explain how we use random orthogonal projection different from other existing studies that use random orthogonal projection, etc.} {\JIANYANG it's now discussed in section 3.1.4}

% {\JIANYANG 15-07-Jianyang: I modified the figures and illustrated Figure.3(c) in the next page. }

% {\CHENG 22-07-Cheng: Some discussions you included can be too late. We can discuss further on these issues.}

%A standard way, Gaussian random projection, is to generate a random projection matrix $G \in \mathbb{R}^{d\times D} $ with i.i.d. standard Gaussian entries scaled by a factor of $1/\sqrt {d} $. The second way we consider here is to first generate a square transformation matrix $G' \in \mathbb{R}^{D \times D} $ in the same way. Then by sampling $d$ rows of $G'$ (for simplicity we take the first $d$ rows), we obtain a submatrix $G'|_{[1,2,...,d]} \in \mathbb{R}^{d\times D} $. 

%We aim to show that the distribution of $G'|_{[1,2,...,d]}$ is identical to that of $G$, i.e. :
%\begin{align}
%    \forall A \in \mathbb{R}^{d\times D}, p_{G}(A) = \int_{\mathbb{R}^{(D-d) \times D} } p_{G'}(A, B) \mathrm{d} B  \label{eq:eq1}
    %\mathbb{P}\left[ G = A \right]  = \int \mathbb{P} \left[ G'|_{[1,2,...,d]}=A, G'|_{[d+1,...,D]} = B \right]  \mathrm{d} B
%\end{align}
%where $A \in \mathbb{R}^{d\times D} ,B \in \mathbb{R}^{(D-d) \times D} $ are submatrices with $d$ and $D-d$ rows. $p_G (A)$ and  $p_{G'} (A, B)$ are the probabilistic density functions of $G$ and $G'$ correspondingly. We marginalize the probability over $B$ because only the first $d$ rows are taken. For random Gaussian projection, Equation (\ref{eq:eq1}) obviously holds because any $d$ rows are generated exactly in the same way of $G$. Thus, we claim that random projection and row sampling with random transformation are identical in distribution for the scaled i.i.d. Gaussian case. {\color{red} would it be distracting if I repeatedly emphasize Gaussian? These properties don't trivially hold for any random projections. I'll state that they hold for random orthogonal transformation later. }

%Then an another elementary fact is that sampling before and after projection are equivalent. It is a basic property of matrix multiplication, shown as 
%\begin{align}
%    G'|_{[1,2,...,d]} \mathbf{x}  = (G'\mathbf{x} )|_{[1,2,...,d]}
%\end{align}

%Consequently, we note that $G \mathbf{x} $ and $(G'\mathbf{x})|_{[1,2,...,d]}$ are identically distributed. During indexing phase, we perform a $G'$ random transformation on all database vectors. Then during query phase, we first transform the query vector and next we can sample a certain amount of dimensions as needed. With its equivalence to random projection, we realize the flexibility of dimensionality. In the following sections, for simplicity, we denote the transformed residual vectors as $\mathbf{y}_i=G' \mathbf{o}_i - G' \mathbf{q} $. {\color{red} I may need figures here.}

%{\color{red} To achieve the flexibility of dimensions, we'll need to ensure that a subspace spanned by canonical basis is meaningful. bad sentence, need refine}.  We consider two ways to generate random projection matrix $G \in \mathbb{R}^{d \times D} $. In the first way, we simply generate each entry with standard Gaussian distribution $g_{ij} \sim \mathcal N (0,1)$. In the second way, we generate a larger matrix $G \in \mathbb{R} ^{D\times D} $ and uniformly sample $d$ rows from it (for simplicity, take the first $d$ rows).{\color{red} it's intuitive that they're equivalent.}

%{\color{red} It's also obvious that the sampling before and after the projection is equivalent.}

%{\color{red} Do I need permutation invariance here? NO I don't, it might confuse readers, but if I use fast orthogonal Johnson-Lindenstrauss Transformation, I may need to do so.}

% \begin{figure}[htb]
%     \centering
%     \subfigure[Equivalence]{
%         \includegraphics[height=0.3\linewidth]{figure/equivalence.pdf}
%     \label{fig:equivalence}
%     }
%     \subfigure[Level Up]{
% 	\includegraphics[height=0.3\linewidth]{figure/levelup.pdf}
% 	\label{fig:levelup}
%     }
%     \caption{Dimension Sampling}
% \end{figure}

% \subsection{\textbf{Sequential Hypothesis Testing}}
% \subsection{\textbf{Aggressive and Incremental Sampling with Hypothesis Testing}} 
\subsection{{\JIANYANGREVISION \textbf{Incremental Sampling with Hypothesis Testing}}}
\label{subsec:idea-2}
\label{subsec:idea2}

{\CHENG 
% Recall the problem is to decide whether the distance of an object $\mathbf{o}$ is larger than the threshold $r$ (i.e., $dis > r$) or not based on some sampled dimensions of the randomly rotated vector of $\mathbf{x}$ (i.e., $\mathbf{y}$). 
% We have proposed to sample some dimensions of the randomly rotated vector of $\mathbf{x}$ (i.e., $\mathbf{y}$)
One remaining issue is how to determine the number of dimensions of $\mathbf{y}$ we need to sample in order to make a sufficiently confident conclusion for the DCO (i.e., to decide whether $dis \le r$). Intuitively, with more sampled dimensions, the approximate distance $dis'$ would be 
% closer to the true distance $dis$ with guaranteed probability,
{\JIANYANGB more accurate, }
% (according to the aforementioned Property 2), 
and we would be able to make a more confident conclusion. On the other hand, sampling more dimensions would result in higher cost of computing the approximate distance (since the cost is linear wrt the number of sampled dimensions). We aim to sample the minimum possible number of dimensions, which are sufficient to make a confident conclusion. 

Specifically, we propose to sample the dimensions of $\mathbf{y}$ in an 
% \emph{aggressive} and 
{\JIANYANGREVISION \emph{incremental}}
manner, i.e., we start with a few dimensions. If with the current sampled dimensions, we cannot make a confident conclusion, we continue to sample 
% more dimensions
{\JIANYANGLAST some more}
until we can make a confident conclusion or we have sampled all dimensions. 
% {\JIANYANG (Similar techniques have also been used in sampling binary hashing codes for shortlisting candidates ~\cite{sequentialLSH, satuluri2011bayesian}.)}
% In the case that we sample all dimensions, the distance computed based on the sampled dimensions would be equal to the \emph{true} distance, as explained in Section~\ref{subsec:idea-1}. With the true distance, we can make an \emph{exact} conclusion.
%
As a result, the problem reduces to the one of deciding whether we can make a sufficiently confident conclusion with a certain, say $d$, sampled dimensions? In a statistics language, the observed distance $dis'$ (computed based on the sampled dimensions) is an estimator of the true distance $dis$ and its distribution depends only on the true value $dis$ and the number of sampled dimensions $d$. The task is to draw a conclusion about a true value $dis$ (i.e., whether $dis \le r$) with an observed value $dis'$. It's exactly what \emph{hypothesis testing} typically does. Motivated by this, we propose to leverage hypothesis testing to solve the problem.
% of deciding whether we can make a sufficiently confident conclusion on whether $dis \le r$ with $dis'$. 
Specifically, we conduct the hypothesis testing as follows.

\begin{enumerate}
    \item We define a null hypothesis $H_0:dis \le r$ and its alternative $H_1: dis > r$.
    % \begin{equation}
    %     H_0:dis \le r~~and~~H_1: dis > r
    % \end{equation}
    \item We use $dis'$ as the estimator of $dis$. The relationship between $dis'$ and $dis$ is provided in Lemma~\ref{eq:concen} (i.e., the difference between $dis'$ and $dis$ is bounded by $\epsilon\cdot dis$ with the failure probability at most $2\exp (-c_0 d  \epsilon^2)$).
    \item We set the significance level $p$ to be $2 \exp(-c_0  \epsilon_0^2)$, where $\epsilon_0$ is a parameter to be tuned empirically. 
    % Then, the rejection region of $dis'$ corresponds to a range in the form of $( (1 + \epsilon_0/\sqrt{d}) \cdot r, +\infty)$.
    % % (which can be derived based on Property 2). 
    % The intuition is that 
    % We derive that (1) 
    {\CHENGB With this,} the event that the observed $dis'$ is much larger than $r$ (i.e., $dis' > (1 + \epsilon_0/\sqrt{d})\cdot r$) has its probability below the significance level $p$ (which can be verified based on Lemma~\ref{eq:concen} with $\epsilon = \epsilon_0/\sqrt{d}$ and $H_0:dis \le r$).
    % and (2) if this event happens (i.e., $dis'$ falls in the rejection region), it implies that $H_0$ is not true with sufficient confidence.
    \item We check whether 
    % $dis'$ falls in the rejection region, i.e., 
    {\CHENGB the event happens} ($dis' > (1 + \epsilon_0/\sqrt{d})\cdot r$). If so, {\CHENGB we can reject $H_0$ and conclude} $H_1: dis > r$ {\CHENGB with sufficient confidence}; otherwise, we cannot.
\end{enumerate}

There are three cases for the outcome of the hypothesis testing. \underline{Case 1}: we reject the hypothesis (i.e., we conclude $dis > r$) and $d < D$. In this case, the time cost (which is mainly for evaluating the approximate distance) is $O(d)$, which is smaller than that of computing the true distance in $O(D)$ time. \underline{Case 2}: we cannot reject the hypothesis and $d < D$. In this case, we would continue to sample some more dimensions of $\mathbf{y}$ \emph{incrementally} and conduct another hypothesis testing. \underline{Case 3}: $d = D$. In this case, we have sampled all dimensions of $\mathbf{y}$ and the approximate distance based on the sampled vector is equal to the true distance. Therefore, we can conduct an \emph{exact} DCO. We note that the 
% aggressive and 
{\JIANYANGREVISION incremental} dimension sampling process with (potentially sequential) hypothesis testing would have its time cost strictly smaller than $O(D)$ (when it terminates in Case 1) and equal to $O(D)$ (when it terminates in Case 3). 
}
% The next question is about the way we find the minimum needed dimensionality for a particular candidate. In general, we design an adaptation-based mechanism to ensure that it keeps sampling until we have enough information. 

{\CHENGB We note that hypothesis testing has also been used for deciding a certain number of hashes for LSH in the context of similarity search~\cite{sequentialLSH, satuluri2011bayesian}. The differences between our technique and \cite{sequentialLSH, satuluri2011bayesian} include: (1) ours is based on a random process of sampling dimensions of a transformed vector while \cite{sequentialLSH, satuluri2011bayesian} are on one of sampling hash functions, which entail significantly different hypothesis testings and (2) ours targets the Euclidean distance function while \cite{sequentialLSH, satuluri2011bayesian} target similarity functions such as Jaccard and Cosine similarity measures (it remains non-trivial to adapt the latter to the Euclidean space),}
{\JIANYANGB and (3) ours guarantees to be no worse than the method of evaluating exact distances (in our case, i.e., \texttt{FDScanning}) because it obtains exact distances when it has sampled all the dimensions while \cite{sequentialLSH, satuluri2011bayesian} have no such guarantee (when they have sampled all the hash functions and still cannot produce a firmed result, they would have to re-evaluate exact similarities from scratch).}
 
%For two given objects $i$ and $j$, the goal is to find the one with smaller distance. Assume that we already have object $i$'s true distance $dis_i$ and object $j$'s $d$-dimensional observed distance $dis'_j$, which is formally defined by

% Consider that we are dealing with a new candidate $i$, i.e. compare its distance with the current maintained $dis_{i_K}$ in $\mathcal K$. In particular, we want to check whether its distance is smaller than $dis_{i_K}$ so as to be able to update $\mathcal K$. 
% Assume that we have already sampled $d$ dimensions and have the corresponding $d$-dimensional observed distance, which is formally defined by: 
% %Consider that we are comparing distance for two given objects $i$ and $j$. Under the context of NN query, in paticular, we want to find out the one with smaller distance. Assume that we already have object $i$'s true distance $dis_i$ and object $j$'s $d$-dimensional observed distance, which is formally defined by
% \begin{align}
%     dis'_i := \sqrt { \frac{D}{d}} \left\| \mathbf{y}_i|_{[1,2,...,d]} \right\|   = \sqrt { \frac{D}{d} \sum_{j=1}^d y_{i,j}^2} 
% \end{align}
% Our questions are: 1) Is $dis_i$ smaller than $dis_{i_K}$? 2) Just with the observed distance $dis'_i$, how much confidence do we have? 

% From the perspective of statistics, the observed distance $dis_i'$ is an estimator of the true distance $dis_i$. Its distribution depends only on the true value $dis_i$ and the number of samples $d$, and luckily, is explicitly and sharply bounded by concentration inequality. Now our task is draw conclusions about a true value $dis_i$ (whether it's smaller than $dis_{i_K}$) with an observed value $dis_i'$.
% %and the number of samples $d$. 
% It's exactly what hypothesis testing does. 

% %Note that based on Lemma~\ref{eq:concen}, the distribution of $dis_j'$ concentrates around $dis_j$ as shown in Figure~\ref{fig:concen}.  Meanwhile, as the sampled dimensionality $d$ increases, it becomes more concentrated. This phenomenon matches our intuition that the more samples we have, the less uncertain we are. Now we want to draw conclusions about the true distance $dis_j$ with observed data $dis_j'$. It's exactly what statistical hypothesis testing does. 



% %Statistical hypothesis testing is base on the philosophy of falsificationism: with finite observed data, we cannot fully accept a statement, but with one counterexample, we can reject it. We hypothesize that 
% Specifically, we propose a null hypothesis $H_0$ and the corresponding alternative hypothesis $H_1$ that
% \begin{align}
%     H_0: dis_i \le dis_{i_K}, H_1: {\CHENG dis_i > dis_{i_K} } \notag
% \end{align}
% If hypothesis $H_0$ holds, then the observed distance $dis'_i$ is almost impossible to be much larger than $dis_{i_K}$ because it should fall around the true distance $dis_i$ with high probability. On the contrary, if we observe that $dis'_i \gg dis_{i_K}$ 
% %(i.e. falling in rejection region)
% , either an extremely-low-probability event happens, or the hypothesis $H_0$ is wrong. In this case, we can confidently reject it, terminate sampling and claim $dis_i > dis_{i_K}$.
% %, as well as returning object $i$ as the nearer object. 
% However, when we cannot reject it, unlike conventional hypothesis testing, we don't accept $H_0$ immediately. According to \textit{Property 2} of Lemma~\ref{eq:concen}, as the sampled dimensionality $d$ increases, the distribution of $dis_i'$ becomes more concentrated. This phenomenon matches our intuition that the more samples we have, the less uncertain we are. Thus, we continue sampling to collect more evidence, and then do another hypothesis testing. The process is repeated until $H_0$ is rejected or all $D$ dimensions are evaluated. \footnote{Rigorously, our hypothesis testing is based on multiple statistics. Only if none of them can reject $H_0$, do we accept $H_0$.}



% \begin{figure*}[htb]
%     \centering
% \subfigure[$dis_i \le r$]{
% 	\includesvg[width=0.47
% 	%0.47
% 	\linewidth]{figure/ADSamplingPositive.svg}
% 	\label{fig:ADSamplingpos}
% }
% \subfigure[$dis_i > r$]{
% 	\includesvg[width=0.47
% 	%0.47
% 	\linewidth]{figure/ADSamplingNegative.svg}
% 	\label{fig:ADSamplingneg}
% }
% \caption{ADSampling}
% \label{fig:ADSampling}
% \end{figure*}

% % {\CHENG 14-07-Cheng: Comments on the above figure: (1) Can better put all the matrices and vectors upside down so that the level up operation looks more like going one level up; (2) in Figure (a), it looks like one box represents one entry (and one dimension in a vector) while in Figure (b) and (c), one box can represent a few dimensions. Some revision is needed for better consistency; (3) Figure 3 needs further elaborations (e.g., what does it mean by positive, lines with arrows, and distributions, etc.). Currently, this figure is simply referred to without much explanation (you may give it some more thoughts whether this figure really illustrates the ADSampling framework. In addition, the font size in this figure is too small.}

% % {\JIANYANG The figures are modified. May we have a discussion? I thought there might not be much space for us to illustrate it in this way?}

% More specifically, we consider progressively doing hypothesis testing for $L$ times, each at dimensionality $d_1,d_2,...,d_L$ respectively, where $0 = d_0 < d_1 <... <d_L = D$. For an integer $l\in[0,L]$, we say an object is currently at level $l$ if we have evaluated $d_{l}$ dimensions of it. The current level of object $i$ is denoted as $l_i$. In case that $H_0$ cannot be rejected at level $l_i$, we next sample another $d_{l_i+1}-d_{l_i}$ dimensions (for simplicity, we just take the consecutive ones). For simplicity, we name this operation as "level up", which is described in Algorithm~\ref{code:levelup} and depicted in Figure~\ref{fig:levelup}, where line 1 recovers the sum of squared difference of evaluated dimensions, line 2-3 evaluate new dimensions and line 4-5 update level and observed distance. In particular, $y_{i,j}$ is computed with the transformed database vector $P' \mathbf{o}_i $ and query vector $P' \mathbf{q}$ on the fly, formally $y_{i,j}=(P' \mathbf{o}_i )_j -(P' \mathbf{q} )_j$.
% \input{pseudocode/levelup}
% % as described in Algorithm~\ref{code:levelup}. 
% %\input{pseudocode/levelup}
% %It recovers the squared distance in line 1, evaluates new dimensions in line 2-4 and updates the level and observed distance in line 5-6. 
% %it doesn't mean $dis_j \le dis_i$. It means that we don't have enough evidence yet, so more dimensions should be sampled. \footnote{Unlike conventional hypothesis testing, we don't accept $H_1$ immediately when $H_0$ cannot be rejected. Rigorously our hypothesis testing is based on multiple statistics. Only if we fail to reject $H_0$ with all of them, do we accept $H_1$.}

% \input{pseudocode/ADSampling}

% Quantitatively, we investigate \textit{Property 2} again to obtain the rejection region of hypothesis testing. We preset the significance (i.e., the false negative failure probability) of a single hypothesis testing to be $2\exp(-c_0\epsilon_0^2)$ ({\CHENG ***explain (1) we can do hypothesis without knowing the exact value of $c_0$; (2) we set $\epsilon_0$ empirically***}), where $\epsilon_0$ is a parameter we actually tune to control the failure probability. 
% {\JIANYANG Note that the physical interpretation of $\epsilon_0$ is the multiplicative error bound of $d=1$. Based on our discussion in \textit{Property 2}, when the significance is fixed, the error bound for $d < D$ can be given by $\epsilon_0/\sqrt {d} $, which can be computed with only $\epsilon_0$ and $d$. Thus, we can do hypothesis testing without knowing the exact value of $c_0$. In terms of the actual significance, we refer readers to our empirical study in Figure~\ref{figure:verification}.
% ***This significance is for a single hypothesis testing, while the empirical result is for a DCO. ***}
% %{\JIANYANG Note that $\epsilon_0$ is also the error bound of $d=1$. In this case, the random projection matrix consists of only one row. Note that for one row, there is no orthogonalization operation, so it is indeed generated with normalized i.i.d standard Gaussian entries. Thus, the result of random projection is also a Gaussian random variable and the error bound $(1+\epsilon_0)$ corresponds to the failure probability of $(1+\epsilon_0)\sigma$-region of Gaussian distribution (e.g., $\epsilon_0=1, 2, 3$ corresponds to $2\sigma, 3\sigma, 4\sigma$-region, whose failure probability is around $5\%, 0.3\%, 0.006\%$). }
% %Based on \textit{Property 2}, for a given dimensionality $d_{l_i} < D$, to reach the preset significance, the corresponding multiplicative error bound should be $\epsilon _1/\sqrt {d_{l_i}}$. 
% Then the rejection region is given as:
% \begin{align}
%     dis_i' > \gamma (d_{l_i}) \cdot dis_{i_K}  
%     \label{eq:testing}
% \end{align}
% where $\gamma(d)$ is defined to be $\gamma(d):=1 + \epsilon _1 / \sqrt {d} $ for $d<D$. Intuitively, it implies that we can confidently claim $dis_i > dis_{i_K}$ when the observed distance $dis_i'$ is $\gamma(d_{l_i})$ times larger than $dis_{i_K}$. In discussion of the parameter $\epsilon_0$, we consider two extreme cases. When $\epsilon_0 = 0$, it means that we fully trust the comparison result after random projection: only if $dis_i' \le dis_{i_K}$, do we consider the possibility that $dis_i \le dis_{i_K}$. When $\epsilon_0 \to \infty$, it means that we are extremely risk-averse, always doing comparison with exact distances. We'll show in Section 5 that the failure probability decays super-exponentially with respect to $\epsilon_0$ (Equation~\ref{eq:superexp}). Gathering all discussed components, we get the algorithm ADSampling described in Algorithm~\ref{code:adasampling}. Note that this algorithm can be applied to anywhere DCO is needed, so to emphasize its universality we use $r$ instead of $dis_{i_K}$ to denote the comparison threshold.

% {\JIANYANG 

% We depict the workflow of our ADSampling algorithm in Figure~\ref{fig:ADSampling}. 
% The panels from top to bottom describe four levels from 1 to 4. The empty dot and the dashed curve represent the exact distance $dis_i$ and density of approximate distance $dis_i'$. Note that they only exist conceptually and are unknown by us. The colored dot represents the observed $dis_i'$. It follows the distribution described by the colored dash curve. The black dot and vertical line corresponds to the distance threshold $r$ and $\gamma(d) \cdot r$. The right hand side of $\gamma(d) \cdot r$ is the rejection region.
% Figure~\ref{fig:ADSamplingneg} depicts the case of \underline{$dis_i > r$ (negative)}. At level 1, we observe $dis_i' > r$. However, it's less than $\gamma(d_1) \cdot r$, indicating that it's still possible to be positive. Similarly, at level 2, $dis_i' < \gamma(d_2) \cdot r$ suggests that we should continue sampling. At level 3, it's worth noting that $dis_i > \gamma(d_3) \cdot r$ and more than a half of the density is on the right side of $\gamma(d_3) \cdot r$, indicating that there is at least $\frac{1}{2} $ probability to correctly terminate sampling, though the observed $dis_i'$ might also be less than $\gamma(d_3)\cdot r$. Finally at level 4, we find that $dis_i'$ falls into the rejection region, so we can terminate sampling and claim $dis_i > r$.
% Figure~\ref{fig:ADSamplingpos} corresponds to the case of \underline{$dis_i \le r$ (positive)}. The process is similar. It's worth noting that: (1) it's possible that $dis_i' > r$ (level 2) (2) it's almost impossible that $dis_i' > \gamma(d) \cdot r$ (most of the density is always on the left side of $\gamma(d) \cdot r$). 

% % \begin{comment}
% % In Figure~\ref{fig:ADSampling}, we depict the process of our ADSampling algorithm for a comparison between object $i$ and distance threshold $r$. The four panels from top to bottom describe the status of our algorithm at four different levels. Specifically, in each panel, the solid vertical line represents the distance threshold $r$. The dash vertical line and the colored dash curve represent the exact distance $dis_i$ and the density function of the approximate distance $dis_i'$ respectively. Note that we \textbf{do not} explicitly know them all through the algorithm. The only known information to inference $dis_i$ is the observed $dis_i'$ (dot) at each level. In the beginning, we first level up object $i$ to level 1 and obtain the observed distance $dis_i'$ (red dot). Though $dis'_i$ in this case is larger than $r$, according to our hypothesis testing, only if $dis_i' > r \cdot \gamma(d_1)$ (equivalently $dis_i' / \gamma(d_1) > r$), can we conclude $dis_i > r$. However, $dis_i' / \gamma(d_1)$ (red left-arrow) is smaller than $r$, so there is still unignorable possibility that $dis_i \le r$. Thus, we should continue sampling. Then at level 2, with more sampled dimensions, the observed distance becomes more accurate (closer to $dis_i$). At the same time, since $\gamma(d_2) < \gamma(d_1)$, the error bar (between left-arrow to dot) also shrinks. However, since $dis_i'/\gamma(d_2)$ is still less than $r$, we should keep sampling. Finally, at level 4, we find currently $dis_i'/\gamma(d_4)$ (green left-arrow) to be larger than $r$. We can confidently claim that the exact $dis_i$ is larger than $r$ and terminate sampling. 
% % \end{comment}
% }

%{\JIANYANG Note that the above hypothesis testing terminates sampling only when $dis_i > r$ can be concluded. We can also apply the same technique for the cases where $dis'_i \ll r$. However, we don't suggest to do so because (1) it leads to false positive failure and (2) exact distance is needed for following DCO. Specifically, \underline{first}, concluding $dis_i \le r$ with approximate distance may wrongly replace a KNN object with an non-KNN object, which is unacceptable. \underline{Second}, using approximate distance for following comparison cannot guarantee correctness. Thus, for those objects that cannot be rejected, we keep leveling them up even their observed approximate distance is much less than $r$.}

%$dis_i \le r$ indicates that object $i$ should be inserted into the KNN set $\mathcal K$. Terminating sampling in this case means that we are maintaining $\mathcal K$ with approximate distance and using it for following DCOs. However, directly using approximate distance for following DCOs cannot guarantee correctness, i.e., one distance larger than the approximate distance is not necessarily larger than the corresponding exact distance. Another option is to inference a safe upper bound of exact distance with approximate distance 
%(e.g., $dis_i'/(1-\frac{\epsilon_0}{\sqrt {d_l} })$, right-arrow in Figure~\ref{fig:ADSampling})
%. However, this operation definitely overestimates it (or it is not a safe upper bound)
%as is also illustrated in Figure~\ref{fig:ADSampling}, i.e., right-arrow is on the right hand side of $dis_i$
%. As a result, it increases distance threshold $r$ for \textbf{ALL} the following comparisons and consequently increases the needed dimensions, which is not worthy. 

%In Figure~\ref{fig:ADSampling}, the vertical solid line represents the unknown exact distance and the colored dash curve depicts the corresponding distribution of the observed distance. When running ADSampling, we only know the observed distance (colored dot) and the range of possible true distance $dis_i$ (colored arrow) according its current level. At level 1, with the observed distance and the error bar, we cannot make sure that.

% {\CHENG 14-07-Cheng: As we discussed before, some discussions on cases where the approximate distances are smaller than the threshold by a certain extent should be better provided.}


% \begin{comment}
% {\color{red} I commented out the definition of safe distance because we may drop adaptive priority queue.}

% Equivalent to Equation (\ref{eq:testing}), we have
% \begin{align}
%      \frac{dis'_j}{\gamma(d_{l_j})} > dis_i \label{eq:safe_distance}
% \end{align}
% Note that the left hand side depends only on $j$. We define it to be the \textit{safe distance} $dis^*_j$ of object $j$. Formally, 
% \begin{align}
%     dis^*_j := \frac{dis_j'}{\gamma(d_{l_j})} 
% \end{align}
% It's named safe distance because when $dis^*_j > dis_i$, it induces that $dis'_j > \gamma(d_{l_j}) \cdot dis_i$, so it's safe to claim that $dis_j > dis_i$ based on hypothesis testing. In other word, safe distance $dis^*_i$ is a lower bound of true distance $dis_i$ with high probability. Note that safe distance $dis^*_j$ and observed distance $dis'_j$ can be easily converted to each other by multiplying or dividing by $\gamma(d_{l_j})$, so for conciseness, we skip the level up operation of safe distance.

% \end{comment}

%The next question is about the timing of hypothesis testing. The ideally optimal strategy is to do hypothesis tesing every time after sampling one new dimension: once we have enough information, we stop immediately. However, frequent hypothesis testing, on the one hand, increases the overall failure probability, and on the other hand, introduces testing overhead. {\color{red} Thus, in practice, to anchor hypothesis testing, we preset a few discrete dimensionalities which we term "levels" in the following sections.} Formally, assuming that we have $L+1$ levels, each level $i$ corresponds to a dimensionality $d_i$ where $0 = d_0 < d_1 <... <d_L = D$. Then, for an object $i$, we grant it with two states: its current level $l_i$ and the corresponding $d_{l_i}$-dimensional observed distance $dis_i'$. By "level up" operation, we evaluate the dimensions in $(d_{l_i}, d_{l_i+1}]$, modify the observed distance and increase the level by $1$. With hypothesis testing, leveling up stops when we have enough confidence. Thus, the dimensionality automatically adapts to its inherent difficulty. {\color{red} extremely simple, need pseudo code?}

%The next question is about the timing of hypothesis testing. The ideally optimal strategy is to do hypothesis tesing every time after sampling one new dimension: once we have enough information, we stop immediately. However, frequent hypothesis testing, on the one hand, increases the overall failure probability, and on the other hand, introduces testing overhead. Thus, in practice we preset a few discrete levels to anchor hypothesis testing. Formally, assuming that we have $L+1$ levels, each level $i$ corresponds to a dimensionality $d_i$ where $0 = d_0 < d_1 <... <d_L = D$. Then, for an object $i$, we grant it with two states: its current level $l_i$ and the corresponding $d_{l_i}$-dimensional observed distance $dis_i'$. By "level up" operation, we evaluate the dimensions in $(d_{l_i}, d_{l_i+1}]$, modify the observed distance and increase the level by $1$. With hypothesis testing, leveling up stops when we have enough confidence. Thus, the dimensionality automatically adapts to its inherent difficulty. {\color{red} extremely simple, need pseudo code?}

% \subsection{\textbf{Random Orthogonal Transformation}}

% Finally we come to the problem of dealing with extremely fragile comparisons.
% %According to the study in Section 3, some queries are extremely fragile to random projection. Thus, it would be of vital importance to recover full accuracy. 
% In high-dimensional space, some DCOs are extremely fragile to approximation, i.e. the ratio between distances are very close to 1, which disables dimension reduction. A naive idea is to re-evaluate distance with raw vectors from scratch.
% %However, it might double the time consumption. 
% However, re-evaluation leads to twice cost, i.e. $D$ dimensions on transformed vector + $D$ dimensions on raw vector. 
% %We utilize random orthogonal transformation here to resolve this issue.
% Random orthogonal transformation resolves this issue naturally. To be specific, random orthogonal transformation exhibits both stochastic and deterministic properties. Like other types of random projections (e.g. Gaussian), random orthogonal projection demonstrates concentration properties that approximately preserves distance in $d$-dimensional subspace. However, unlike other types of projections, it fully preserves distance when all $D$ dimensions are evaluated. Thus, it naturally recovers exact distance when all $D$ dimensions are sampled. That's exactly the reason we adopt random orthogonal transformation instead of others.

% In summary, our adaptive dimension sampling makes resolution flexible by doing dimension sampling on randomly transformed vectors, determines the needed dimensionality by progressive sequential hypothesis testing and recovers exact distance naturally with the distance-preserving property of random orthogonal transformation. 



\subsection{Summary and Theoretical Analysis}
\label{subsection:theoretical analysis of ADSampling}

{\CHENG \noindent\textbf{Summary.} 
% \input{content/pseudo_ADSampling_original.tex}
\input{content/pseudo_ADSampling_revised.tex}

\vspace{-4mm}
\smallskip\noindent\textbf{Failure Probability Analysis.}
%
Note that \texttt{ADSampling} terminates in either Case 1 (with the hypothesis being rejected and $d < D$) or Case 3 (with $d = D$). When it terminates in Case 3, there would be no failure since in this case, the approximate distance $dis'$ is equal to the true distance $dis$ and the DCO result is exact. When it terminates in Case 1, a failure would happen if $dis \le r$ holds since in this case, it concludes that $dis > r$ (by rejecting the null hypothesis). We analyze the probability of the failure. {\JIANYANG As discussed in Section~\ref{subsec:idea-2}, we can control the failure probability with $\epsilon_0$. The following lemma presents the relationship between $\epsilon_0$ and the failure probability of a DCO with \texttt{ADSampling}. } 
}

\begin{lemma}
For a DCO in $D$-dimensional space, the failure probability of \texttt{ADSampling} is given by
\begin{align}
    &\mathbb{P}\left\{ {\CHENG failure}  \right\}  =0 {\CHENG \text{~~if $dis > r$}}
    % {\CHENG \text{~~for a negative object}}
    \\&
    \mathbb{P}\left\{ {\CHENG failure} \right\}  \le \exp \left( -c_0 \epsilon_0^2 + \log D \right) 
    {\CHENG \text{~~if $dis \le r$}}
    % {\CHENG \text{~~for a positive object}}
\end{align}
\label{theorem:ADSampling accuracy}
\label{lemma:ADSampling accuracy}
\end{lemma}
\begin{proof}
% The case of false positive is as specified above. In terms of the false negative, with our discussion above, 
{\CHENG The correctness for the case of $dis > r$ is obvious and that of the other case ($dis \le r$) can be verified as follows.}
\begin{align}
    &\mathbb{P}\left\{ {\CHENG failure} \right\} = \mathbb{P} \left\{ \exists d < D, dis' > (1 + \epsilon_0 / \sqrt {d} ) \cdot r \right\} \label{eq:rejection}
    % \\\le &\mathbb{P} \left\{ \exists d < D, dis' > (1 + \epsilon_0 / \sqrt {d} ) \cdot dis \right\} \label{eq:dis<=r}
    \\\le &\sum_{d=1}^{D-1} \mathbb{P} \left\{ dis' > (1 + \epsilon_0 / \sqrt {d} ) \cdot dis \right\}  \label{eq:union bound}
    \\\le &\sum_{d=1}^{D-1} \exp \left( -c_0 \epsilon_0^{2}  \right)\le \exp \left( -c_0 \epsilon_0^2 + \log D \right) \label{eq:concentrationlemma3.1}
    % \\\le &\sum_{d=1}^{D-1} \exp \left( -c_0 \epsilon_0^{2}  \right)= \exp \left( -c_0 \epsilon_0^2 + \log D \right) \label{eq:concentrationlemma3.1}
    % \\&= \mathbb{P} \left\{ \exists d < D, \sqrt {\frac{D}{d} } \| \mathbf{y}|_{[1,2,...,d]}\|  > (1 + \frac{\epsilon_0}{\sqrt {d} } ) \cdot \|\mathbf{y}\|  \right\}
    % \\&\le \sum_{d=1}^{D-1} \mathbb{P} \left\{  \sqrt {\frac{D}{d} } \| \mathbf{y}|_{[1,2,...,d]}\|  > (1 + \frac{\epsilon_0}{\sqrt {d} } ) \cdot \|\mathbf{y}\| \right\}  
    % \\&= \sum_{d=1}^{D-1} \mathbb{P} \left\{ \sqrt {\frac{D}{d} } \| P|_{[1,2,...,d]} \mathbf{x} \|  > (1 + \frac{\epsilon_0}{\sqrt {d} } ) \cdot \|\mathbf{x}\|  \right\}
    % \\&\le \sum_{d=1}^{D-1} \exp \left( -c_0 \epsilon_0^{2}  \right) \le D \cdot \exp \left( -c_0 \epsilon_0^{2}  \right)
    % \\&= \exp \left( -c_0 \epsilon_0^{2} + \log D \right) 
\end{align}
{\JIANYANG 
where (\ref{eq:rejection}) is because a failure happens if and only if we accidentally reject the hypothesis for some $d<D$; 
% (\ref{eq:dis<=r}) is because $dis \le r$; (\ref{eq:union bound}) applies union bound; 
(\ref{eq:union bound}) applies union bound and the fact that $dis \le r$; and (\ref{eq:concentrationlemma3.1}) is due to Lemma~\ref{eq:concen}.
}
\end{proof}


% {\CHENG 27-07-Cheng: The first equation in the above does not look clear to me and needs some explanation.}

% {\JIANYANG 27-07-Jianyang: It corresponds to the sentence "In terms of false negative, it happens if and only if at some dimension $d < D$, the approximate distance of a positive object ($dis_i \le r$) accidentally falls into the rejection region."}

% {\CHENG 27-07-Cheng: I still don't understand. I believe the event that ``$\exists d < D, dis' > \gamma(d) \cdot r$'' happens is a necessary but not sufficient condition, and thus the ``equation'' should be ``less than'' in deduction. Perhaps, I got some misunderstanding here. If necessary, we can arrange a chat soon to discuss this issue and some other issues.}

{\CHENG

\smallskip\noindent\textbf{Time Complexity Analysis.}
Let $\hat{D}$ be the number of sampled dimensions by \texttt{ADSampling}. Clearly, the time complexity \texttt{ADSampling} is $O(\hat{D})$. Given the stochastic nature of the method, $\hat{D}$ is a random variable. Next, we analyze the expectation of $\hat{D}$, denoted by $\mathbb{E}[\hat{D}]$. 
% We discuss in two cases. \underline{Case 1:} $dis \le r$. In this case, \texttt{ADSampling} would most likely terminate in Case 3 by sampling all $D$ dimensions and very unlikely terminate in Case 1 by concluding $dis > r$ (which corresponds to a failure). Therefore, we have $\mathbb{E}[\hat{D}] \le D$. \underline{Case 2:} $dis > r$. In this case, 
First of all, since $\hat{D}$ is always at most $D$, we know $\mathbb{E}[\hat{D}] \le D$. Furthermore, for the DCO on a negative object with $dis > r$, we can derive that $\mathbb{E}[\hat{D}]$ relies on $\epsilon_0$ and $\alpha = (dis-r)/r$ {\JIANYANG (which we call the \emph{distance gap} between $dis$ and $r$)}, as presented below (detailed proof can be found in 
% Section~\ref{section:theory}
{\JIANYANGREVISION Appendix~\ref{section:theory}).}
% {\JIANYANGREVISION a technical report~\cite{technical_report}}).
%
\begin{lemma}
% For a DCO with threshold $r$ and a negative object, let $(1+\alpha)$ be the ratio between $dis$ and $r$, $\alpha > 0$. The expected terminate dimensionality is 
When \texttt{ADSampling} is used for the DCO on an object and a threshold $r$ with $dis > r$, letting $\alpha =(dis-r)/r$, we have
\begin{align}
    \mathbb{E} \left[ \hat D  \right]  = O \left[ \min \left( D, \alpha _{}^{-2} \cdot \epsilon _{0}^{2}  \right)  \right] 
\end{align}
\label{theorem:ADSampling efficiency}
\end{lemma}
%
The above result is well aligned with the intuitions that (1) when the distance gap between $dis$ and $r$, i.e., $(dis-r)/r$, is larger, fewer dimensions would be sampled for making a sufficiently confident conclusion and (2) when $\epsilon_0$ is larger (i.e., the significance value of the hypothesis testings is smaller, which means a higher requirement on the confidence), more dimensions would be sampled.

% Based on Lemma~\ref{theorem:ADSampling accuracy} and Lemma \ref{theorem:ADSampling efficiency}, 
We further derive the time-accuracy trade-off of \texttt{ADSampling}.
% for DCOs.
% on a negative object.
% and let the false negative failure probability be $\delta$. Then we have the following theorem:
\begin{theorem}
% Let $\delta$ be an upper bound of the false negative failure probability of ADSampling. 
% For a DCO with threshold $r$ and a negative object. Let $(1+\alpha)$ be the ratio between $dis$ and $r$, $\alpha >0$. The expected terminate dimensionality is
When \texttt{ADSampling} is used for the DCO on an object and a threshold $r$ with $dis > r$, letting $\alpha =(dis-r)/r$, we have
\begin{align}
    \mathbb{E} \left[ \hat D \right]  = O \left[ \min \left( D, \frac{1}{\alpha ^2} \log \frac{D}{\delta}  \right)  \right] 
\end{align}
for achieving its failure probability {\JIANYANG (of positive objects)} at most $\delta$.
% \footnote{We note that since random orthogonal projection exactly preserves the distance between any two points, when all $D$ dimensions are sampled }.
\label{theorem:time-accuracy of ADSampling}
\end{theorem}
}

{\JIANYANG 
\begin{proof}
Making the failure probability in Lemma~\ref{theorem:ADSampling accuracy} be equal to $\delta$, we obtain the corresponding $\epsilon_0$. Then by substituting $\epsilon_0$ in Lemma~\ref{theorem:ADSampling efficiency}, we have the theorem. 
\end{proof}
% We next quantitatively investigate the time-accuracy tradeoff of ADSampling, i.e., the failure probability and expected time complexity. 

% First, the failure of a DCO can be categorized into two types: \emph{false positive} (an object is negative $dis_i > r$, but we wrongly claim that it's positive $dis_i \le r$) and \emph{false negative} (an object is positive $dis_i \le r$, but we wrongly claim that it's negative $dis_i > r$).
% Let's consider during ADSampling in which cases these two types of failure can happen. Recall that during ADSampling, when we sampled \underline{$d<D$ dimensions} of a data vector, based on the observed approximate distance $dis_i'$, there are two circumstances: (1) we terminate sampling and claim $dis_i > r$ (return a negative result) when $dis_i' > \gamma(d) \cdot r$, i.e., falls into the rejection region and (2) we cannot reject it and keep sampling when $dis_i' \le \gamma(d) \cdot r$. When we sampled \underline{all $D$ dimensions}, we obtained exact distance and can return the exact comparison results. Note that our algorithm returns a positive result only when it sampled all $D$ dimensions and obtained exact distance. Thus, it will never produce false positive failure. In terms of false negative, it happens if and only if at some dimension $d < D$, the approximate distance of a positive object ($dis_i \le r$) accidentally falls into the rejection region. Based on these thoughts, we have the following theorem: 
% \begin{theorem}
% For a DCO in $D$-dimensional space, the failure probability of ADSampling is given by
% \begin{align}
%     &\mathbb{P}\left\{ {false\ positive}  \right\}  =0 
%     \\&\mathbb{P}\left\{ {false\ negative} \right\}  \le \exp \left( -c_0 \epsilon_0^2 + \log D \right) 
% \end{align}
% \label{theorem:ADSampling dimensionality}
% \end{theorem}
% \begin{proof}
% The case of false positive is as specified above. In terms of the false negative, with our discussion above, 
% \begin{align}
%     &\mathbb{P}\left\{ {false\ negative} \right\} = \mathbb{P} \left\{ \exists d < D, dis' > \gamma(d) \cdot r \right\} 
%     \\\le &\mathbb{P} \left\{ \exists d < D, dis' > \gamma(d) \cdot dis \right\}
%     \\\le &\sum_{d=1}^{D-1} \mathbb{P} \left\{ dis' > \gamma(d) \cdot dis \right\}  \le \sum_{d=1}^{D-1} \exp \left( -c_0 \epsilon_0^{2}  \right) 
%     \\\le &D\exp \left( -c_0 \epsilon_0^{2}  \right)= \exp \left( -c_0 \epsilon_0^2 + \log D \right) 
%     % \\&= \mathbb{P} \left\{ \exists d < D, \sqrt {\frac{D}{d} } \| \mathbf{y}|_{[1,2,...,d]}\|  > (1 + \frac{\epsilon_0}{\sqrt {d} } ) \cdot \|\mathbf{y}\|  \right\}
%     % \\&\le \sum_{d=1}^{D-1} \mathbb{P} \left\{  \sqrt {\frac{D}{d} } \| \mathbf{y}|_{[1,2,...,d]}\|  > (1 + \frac{\epsilon_0}{\sqrt {d} } ) \cdot \|\mathbf{y}\| \right\}  
%     % \\&= \sum_{d=1}^{D-1} \mathbb{P} \left\{ \sqrt {\frac{D}{d} } \| P|_{[1,2,...,d]} \mathbf{x} \|  > (1 + \frac{\epsilon_0}{\sqrt {d} } ) \cdot \|\mathbf{x}\|  \right\}
%     % \\&\le \sum_{d=1}^{D-1} \exp \left( -c_0 \epsilon_0^{2}  \right) \le D \cdot \exp \left( -c_0 \epsilon_0^{2}  \right)
%     % \\&= \exp \left( -c_0 \epsilon_0^{2} + \log D \right) 
% \end{align}
% \end{proof}
%Note that the above hypothesis testing terminates sampling only when $dis_i > r$ can be concluded. We can also apply the same technique for the cases where $dis'_i \ll r$. However, we don't suggest to do so because (1) it leads to false positive failure and (2) exact distance is needed for following DCO. Specifically, \underline{first}, concluding $dis_i \le r$ with approximate distance may wrongly replace a KNN object with an non-KNN object, which is unacceptable. \underline{Second}, using approximate distance for following comparison cannot guarantee correctness. Thus, for those objects that cannot be rejected, we keep leveling them up even their observed approximate distance is much less than $r$.
% Then we analyze the time complexity of ADSampling. Note that in ADSampling, the terminate dimension depends on the random $dis_i'$. Thus, the terminate dimension is also a random variable, which we denoted as $\hat D$. In terms of the time complexity of ADSampling, we analyzes the expected terminate dimension $\mathbb{E} \left[ \hat D \right]$. It's also worth noting that since ADSampling always requires exact DCO for positive objects, it brings acceleration only for negative objects. 
% For a negative object, an obvious fact about $\mathbb{E} \left[ \hat D \right]  $ is that the more different $dis$ and $r$ are, the fewer dimensions are needed, where the difference can be measured by their ratio, i.e., $dis / r$. Similar to the failure probability analysis, $\epsilon_0$ also controls time complexity because intuitively, to achieve better accuracy, we need to evaluate more dimensions. Quantitatively, the following lemma provides the relationship between $\mathbb{E}\left[ \hat D \right]  $ and $\alpha, \epsilon_0$, whose detailed proof is given in Section~\ref{section:theory}: 
%Then we analyze the expected time complexity, i.e., the expected terminate dimensionality of negative objects. 
% \begin{lemma}
% For a DCO with threshold $r$ and a negative object, let $(1+\alpha)$ be the ratio between $dis$ and $r$, $\alpha > 0$. The expected terminate dimensionality is 
% \begin{align}
%     \mathbb{E} \left[ \hat D  \right]  = O \left[ \min \left( D, \alpha _{}^{-2} \cdot \epsilon _{0}^{2}  \right)  \right] 
% \end{align}
% \label{theorem:ADSampling efficiency}
% \end{lemma}
% Note that due to the recoverability of our method, when the comparison is extremely fragile ($\alpha$ is extremely small), ADSampling guarantees to produce exact DCO results with $D$ dimensions, which makes sure that it's at least no worse than the plain comparison. 
% {\JIANYANGB
% \smallskip
% \noindent\textbf{Remarks.}
% We compare Theorem~\ref{theorem:time-accuracy of ADSampling} with Johnson-Lindenstrauss Lemma (JL Lemma)~\cite{johnson1984extensions}, which states that a random projection to the dimensionality $\Theta( \frac{1}{\epsilon^2} \log \frac{1}{\delta} )$ can preserve the norm of a vector with $\epsilon$ multiplicative error with at most $\delta$ failure probability. We note that our method has very similar result as JL Lemma despite that (1) applying JL Lemma needs to set the dimensionality according to the needed multiplicative error bound while ours automatically adapts to the needed dimensionality according to the distance gap $\alpha$ and (2) Theorem~\ref{theorem:time-accuracy of ADSampling} is weaker by a factor of $\log D$. This is because Lemma~\ref{lemma:ADSampling accuracy} has not been tightly analyzed yet. Since the tightness of the theoretical analysis is not the focus of this paper, we leave the careful analysis in future works. 

% % We also emphasize that a series of studies~\cite{larsen2017optimality,optimalJL, optimalJL2} prove the theoreticaly optimality of JL Lemma, i.e., there exists a set of size $N$, if the dimensionality is reduced to $o(\frac{1}{\epsilon^2} \log \frac{N}{\delta})$, then its distance information would be largely distorted. Note that for a DCO, its maximum allowed multiplicative error bound should be the distance gap $\alpha$. These two points imply that our method achieves the theoretically instance-optimality.
% }
% (1) our method has very simi (1) applying Johnson-Lindenstrauss Lemma requires to 
% with failure probability at most $\delta$, to guarantee a multiplicative error of at most $\epsilon$ for a single object, .

\smallskip
\noindent\textbf{\texttt{ADSampling v.s. FDScanning.}}
Compared with \texttt{FDScanning}, \texttt{ADSampling} improves the complexity for negative objects from being linear to being logarithmic wrt $D$ at the cost of the accuracy for positive objects (Theorem~\ref{theorem:time-accuracy of ADSampling}).
%
% According to Lemma~\ref{theorem:ADSampling accuracy} and \ref{theorem:ADSampling efficiency}, we notice that 
We emphasize that the failure probability (of positive objects) decays \textbf{quadratic-exponentially} (Lemma~\ref{theorem:ADSampling accuracy}) while the time complexity (of negative objects) grows \textbf{quadratically} (Lemma~\ref{theorem:ADSampling efficiency}), both with respect to $\epsilon_0$. It indicates that to achieve \emph{nearly-exact} DCOs, we only need sample a few dimensions.  
{\JIANYANG 
We empirically verify these results in Section~\ref{subsubsec:theoretical results}. It shows that with \texttt{ADSampling} as a plugin, an exact KNN algorithm, namely linear scan, needs only on average 55 dimensions per vector on GIST (originally 960 dimensions) to achieve >99.9\% recall. }

% To better illustrate the time-accuracy tradeoff, we combine Lemma~\ref{theorem:ADSampling accuracy} and \ref{theorem:ADSampling efficiency} and let the false negative failure probability be $\delta$. Then we have the following theorem:
% \begin{theorem}
% Let $\delta$ be an upper bound of the false negative failure probability of ADSampling. For a DCO with threshold $r$ and a negative object. Let $(1+\alpha)$ be the ratio between $dis$ and $r$, $\alpha >0$. The expected terminate dimensionality is
% \begin{align}
%     \mathbb{E} \left[ \hat D \right]  = O \left[ \min \left( D, \frac{1}{\alpha ^2} \log \frac{D}{\delta}  \right)  \right] 
% \end{align}
% \label{theorem:time-accuracy of ADSampling}
% \end{theorem}

% For further discussion, note that we can also trade off between the complexity for the positive and the accuracy of the negative by adding another hypothesis testing with $H_0: dis_i > r, H_1: dis_i \le r$. However, under the context of KNN, we don't suggest to do so because (1) exact distance of positive objects is needed for following DCO (2) in KNN query, the comparisons for negative objects are dominant. In terms of the second point, specifically, providing ground truth $dis_{i_K^*}$ as the distance threshold, we have $K$ positive objects (at most hundreds) and $N-K$ negative objects (millions or billions). In our setting, we reduce the time complexity of the majority (all negative objects) and at the same time, prevent them from false positive failure. While in terms of the positive, though they need full $D$ dimensions, there are only $K$ of them. At the same time, though they might cause false negative failure, in our setting we only need to suppress the failure probability (by increasing $\epsilon_0$) of $K$ objects instead of the remaining $N-K$.
} 

%Now we introduce adaptive dimension sampling for a single comparison between two objects. We next specify how it adapts to KNN query.

\begin{comment}
\begin{figure}[htb]
  \centering 
  \includesvg[width=0.5\linewidth]{figure/KNN3.svg}
  \caption{Draft - Adaptive Dimension Sampling for KNN Query.}
  \label{fig:KNNcomp}
\end{figure}

\subsubsection{\textbf{Adaptive Dimension Sampling for KNN Query}}
As is already discussed in Section 3, KNN is a query to distinguish apart positive objects and negative objects. The result of a comparison matters only when a positive object and a negative one are compared. In condition of producing the correct result for it, the total quota of approximation has already been determined by its inherent difficulty, i.e. distance ratio. However, as we can determine the dimensionality of each object individually, the quota of approximation is still to be assigned. In other word, once we increase the resolution of one object, there would be more space of approximation left for the other. At the same time, in KNN query, positive objects are much less than the negative, which implies that increasing the resolution of one positive object can benefits many comparisons with the negative as shown in figure~\ref{fig:KNNcomp}. Thus, we make it exact for all potential positive objects during KNN search. {\color{red} (That's why we consider the case where one is exact and the other is approximate in hypothesis testing.) } 



Then we equip two classes of ANN algorithms, i.e. graph and filter-and-verification framework with our adaptive dimension sampling framework.
\end{comment}



%Random orthogonal transformation exhibits both deterministic and stochastic properties. On the one hand, it fully preserves the distance between any two vectors when all $D$ dimensions are evaluated (as a result, $\gamma(D)=1,dis_i^*=dis_i$ when $d=D$). On the other hand, properties needed in the previous sections also hold: 1)its $d$-dimensional row sampling is equivalent to $d$-dimensional random orthogonal projection, which is shown by the process of its generation. 2) concentration inequality also holds for random orthogonal projection \footnote{Its proof relies on the concentration of Lipschitz function over $D$-dimensional unit sphere $\mathbb{S}^{D-1}$~\cite{vershynin_2018}.}. Thus, by applying random orthogonal transformation instead of Gaussian, we naturally recover full-precision distance when all $D$ dimensions are sampled, which ensures the number of evaluated dimensions to be no greater than $D$. 

%Random orthogonal transformation is a technique widely used in KNN query. {\color{red} cite } It exhibits both deterministic and stochastic properties. First, it fully preserves the distance between any two vectors when all its $D$ dimensions are evaluated. Then, its $d$-row sampling is equivalent to $d$-dimensional random orthogonal projection, for which concentration inequality still holds. {\color{red} (cite: it's a conclusion in a textbook, maybe I'll need to prove the row-sampling thing.)} Thus, by replacing random Gaussian transformation with random orthogonal transformation, we naturally recovers full accuracy when all $D$ dimensions are sampled, which ensures the number of dimensions we evaluate to be strictly no greater than the full-key evaluation. {\color{red} I didn't talk about its generation.}

%In terms of random projection, empirically, Gaussian and orthogonal transformation show comparable performance for low target dimensions. In terms of random Gaussian transformation, the distribution of the observed distance (or its square) can be explicitly given by $\chi^2(d)$ distribution or approximately by Chernoff bound {\color{red} cite}, both of which are tighter than the given subgaussian tail bound, while for random orthogonal transformation, we cannot provide such a concrete distribution as $\chi^2(d)$. Also, we drop some high ordered terms $O(\epsilon ^3)$ for simplicity {\color{red} cite}. {\color{red} However, {\textbf{we argue that }} the high ordered terms have little effect on the performance when $\epsilon $ is small, and also the tightness of the bound is not the focus of our work. Thus, we just initiate the framework with the subgaussian tail bound.}

%In terms of random projection, empirically, Gaussian and orthogonal transformation show comparable performance for low target dimensions. For random Gaussian transformation, the distribution of the observed distance (or its square) can be explicitly given by $\chi^2(d)$ distribution or approximately by Chernoff bound {\color{red} cite}, both of which are tighter than the given subgaussian tail bound, while for random orthogonal transformation, we cannot provide such a concrete distribution as $\chi^2(d)$. {\color{red} However, the tightness of the bound is not the focus of our work. Thus, we just initiate the framework with subgaussian tail bound.}

%the high ordered terms have little effect on the performance when $\epsilon $ is small, and also


% We then specify how ADSampling is applied to KNN query with three proposed algorithms. 

%We emphasize that AdaSampling is a highly coupled framework. Components in it cannot be replaced trivially. For example, PCA, a famous optimization-based dimensionality reduction method, cannot trivially substitute random projection because it fails to provide a probabilistic maximum error bound for hypothesis testing. We also note that each component in this framework was individually used in KNN query. {\color{red} cite: xxx, xxx, \cite{lu2020vhp}} in LSH family samples some sub-codes from a large codebook for each individual query. \cite{lu2020vhp} evaluates distance under different subspaces.  \cite{ram2019revisiting} also shares the same spirit when discussing random partition tree and KD-Tree. {\color{red} (I'll need to carefully reread the paper to see what was talked about.  I'll still need to cite another paper from the same author.)} However, none of these methods make it progressive and design adaptation mechanisms with theoretical guarantees. SRS~\cite{sun2014srs} utilized cumulative density function of $\chi^2(d)$ distribution for early termination, which shares the same spirit of hypothesis testing. However, they only consider the $c$-approximation case. {\color{red} (I should be careful about it. It may need discussion.)} Random orthogonal transformation is used in quantization methods {\color{red} cite} with the heuristic motivation of uniformly distributing distance information. {\color{red} I should be careful.} Thus, we emphasize that our contribution is more about the whole adaptive dimension sampling framework but not each individual component.

%Plus, note that our method targets to produce correct results for all DCOs, which means with high probability the framework itself does not introduce any error. Thus, the al . However, the most common setting of random projection, the one given by LSH family, targets the relaxed version of NN query. exact NN query with high successful probability instead of approximate NN query. As shown in the experiment using Adaptive Priority Queue for linear scan, it reaches full accuracy. Thus, the Adaptive Priority Queue itself doesn't lose accuracy with high confidence. Though we target exact NN case, for approximate NN case, our method is still applicable in the sense that we can further loosen the ratio by dividing a factor of $1+c$, which we don't discuss in detail. 
