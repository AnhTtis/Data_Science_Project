
% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkha{\"u}ser" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }

%Entries

@article{choromanski2017unreasonable,
  title={The unreasonable effectiveness of structured random orthogonal embeddings},
  author={Choromanski, Krzysztof M and Rowland, Mark and Weller, Adrian},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{ram2019revisiting,
  title={Revisiting kd-tree for nearest neighbor search},
  author={Ram, Parikshit and Sinha, Kaushik},
  booktitle={Proceedings of the 25th acm sigkdd international conference on knowledge discovery \& data mining},
  pages={1378--1388},
  year={2019}
}

 @MISC{eigenweb,
  author = {Ga\"{e}l Guennebaud and Beno\^{i}t Jacob and others},
  title = {Eigen v3},
  howpublished = {http://eigen.tuxfamily.org},
  year = {2010}
 }
 
@ARTICLE{KNNClassficiation,  author={Cover, T. and Hart, P.},  journal={IEEE Transactions on Information Theory},   title={Nearest neighbor pattern classification},   year={1967},  volume={13},  number={1},  pages={21-27},  doi={10.1109/TIT.1967.1053964}}

@inproceedings{
Dong2020Learning,
title={Learning Space Partitions for Nearest Neighbor Search},
author={Yihe Dong and Piotr Indyk and Ilya Razenshteyn and Tal Wagner},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkenmREFDr}
}

@article{reinforcement2route,
author = {Feng, Chao and Lian, Defu and Wang, Xiting and Liu, Zheng and Xie, Xing and Chen, Enhong},
title = {Reinforcement Routing on Proximity Graph for Efficient Recommendation},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1046-8188},
url = {https://doi.org/10.1145/3512767},
doi = {10.1145/3512767},
abstract = {We focus on Maximum Inner Product Search (MIPS), which is an essential problem in many machine learning communities. Given a query, MIPS finds the most similar items with the maximum inner products. Methods for Nearest Neighbor Search (NNS) which is usually defined on metric space don’t exhibit the satisfactory performance for MIPS problem since inner product is a non-metric function. However, inner products exhibit many good properties compared with metric functions, such as avoiding vanishing and exploding gradients. As a result, inner product is widely used in many recommendation systems, which makes efficient Maximum Inner Product Search a key for speeding up many recommendation systems.Graph based methods for NNS problem show the superiorities compared with other class methods. Each data point of the database is mapped to a node of the proximity graph. Nearest neighbor search in the database can be converted to route on the proximity graph to find the nearest neighbor for the query. This technique can be used to solve MIPS problem. Instead of searching the nearest neighbor for the query, we search the item with maximum inner product with query on the proximity graph. In this paper, we propose a reinforcement model to train an agent to search on the proximity graph automatically for MIPS problem if we lack the ground truths of training queries. If we know the ground truths of some training queries, our model can also utilize these ground truths by imitation learning to improve the agent’s search ability. By experiments, we can see that our proposed mode which combines reinforcement learning with imitation learning shows the superiorities over the state-of-the-art methods.},
note = {Just Accepted},
journal = {ACM Trans. Inf. Syst.},
month = {jan},
keywords = {Non-metric, Proximity graph, MIPS, Graph Convolutional Network, Reinforcement learning, Reward shaping}
}


@article{li2019approximate,
  title={Approximate nearest neighbor search on high dimensional data—experiments, analyses, and improvement},
  author={Li, Wen and Zhang, Ying and Sun, Yifang and Wang, Wei and Li, Mingjie and Zhang, Wenjie and Lin, Xuemin},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={32},
  number={8},
  pages={1475--1488},
  year={2019},
  publisher={IEEE}
}

@article{jegou2010product,
  title={Product quantization for nearest neighbor search},
  author={Jegou, Herve and Douze, Matthijs and Schmid, Cordelia},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={33},
  number={1},
  pages={117--128},
  year={2010},
  publisher={IEEE}
}

@inproceedings{guo2020accelerating,
  title={Accelerating large-scale inference with anisotropic vector quantization},
  author={Guo, Ruiqi and Sun, Philip and Lindgren, Erik and Geng, Quan and Simcha, David and Chern, Felix and Kumar, Sanjiv},
  booktitle={International Conference on Machine Learning},
  pages={3887--3896},
  year={2020},
  organization={PMLR}
}

@inproceedings{jegou2011searching,
  title={Searching in one billion vectors: re-rank with source coding},
  author={J{\'e}gou, Herv{\'e} and Tavenard, Romain and Douze, Matthijs and Amsaleg, Laurent},
  booktitle={2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={861--864},
  year={2011},
  organization={IEEE}
}

@ARTICLE{imi,
  author={Babenko, Artem and Lempitsky, Victor},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={The Inverted Multi-Index}, 
  year={2015},
  volume={37},
  number={6},
  pages={1247-1260},
  doi={10.1109/TPAMI.2014.2361319}}

@inproceedings{ge2013optimized,
  title={Optimized product quantization for approximate nearest neighbor search},
  author={Ge, Tiezheng and He, Kaiming and Ke, Qifa and Sun, Jian},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2946--2953},
  year={2013}
}

@article{fu2021high,
  title={High dimensional similarity search with satellite system graph: Efficiency, scalability, and unindexed query compatibility},
  author={Fu, Cong and Wang, Changxu and Cai, Deng},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2021},
  publisher={IEEE}
}

@article{annbenchmark,
author = {Aum\"{u}ller, Martin and Bernhardsson, Erik and Faithfull, Alexander},
title = {ANN-Benchmarks: A Benchmarking Tool for Approximate Nearest Neighbor Algorithms},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {87},
number = {C},
issn = {0306-4379},
url = {https://doi.org/10.1016/j.is.2019.02.006},
doi = {10.1016/j.is.2019.02.006},
journal = {Inf. Syst.},
month = {jan},
numpages = {13},
keywords = {Benchmarking, 97P30, Evaluation, Nearest neighbor search}
}

@article{wold1987principal,
  title={Principal component analysis},
  author={Wold, Svante and Esbensen, Kim and Geladi, Paul},
  journal={Chemometrics and intelligent laboratory systems},
  volume={2},
  number={1-3},
  pages={37--52},
  year={1987},
  publisher={Elsevier}
}

@inproceedings{diskann,
 author = {Jayaram Subramanya, Suhas and Devvrit, Fnu and Simhadri, Harsha Vardhan and Krishnawamy, Ravishankar and Kadekodi, Rohan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {DiskANN: Fast Accurate Billion-point Nearest Neighbor Search on a Single Node},
 url = {https://proceedings.neurips.cc/paper/2019/file/09853c7fb1d3f8ee67a61b6bf4a7f8e6-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{PASE,
author = {Yang, Wen and Li, Tao and Fang, Gai and Wei, Hong},
title = {PASE: PostgreSQL Ultra-High-Dimensional Approximate Nearest Neighbor Search Extension},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386131},
doi = {10.1145/3318464.3386131},
abstract = {Similarity search has been widely used in various fields, particularly in the Alibaba ecosystem. The open-source solutions to a similarity search of vectors can only support a query with a single vector, whereas real-life scenarios generally require a processing of compound queries. Moreover, existing open-source implementations only provide runtime libraries, which have difficulty meeting the requirements of industrial applications. To address these issues, we designed a novel scheme for extending the index-type of PostgreSQL (PG), which enables a similar vector search and achieves a high-performance level and strong reliability of PG. Two representative types of nearest neighbor search (NNS) algorithms are presented herein. These algorithms achieve a high performance, and afford advantages such as the support of composite queries and seamless integration of existing business data. The other NNS algorithms can be easily implemented under the proposed framework. Experiments were conducted on large datasets to illustrate the efficiency of the proposed retrieval mechanism.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2241–2253},
numpages = {13},
keywords = {nearest neighbor search, PostgreSQL, index, HNSW, high dimensional similarity search, approximate nearest neighbor search (ANN)},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@article{reviewer_paper,
author = {Patella, Marco and Ciaccia, Paolo},
title = {Approximate Similarity Search: A Multi-Faceted Problem},
year = {2009},
issue_date = {March, 2009},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {7},
number = {1},
issn = {1570-8667},
url = {https://doi.org/10.1016/j.jda.2008.09.014},
doi = {10.1016/j.jda.2008.09.014},
abstract = {We review the major paradigms for approximate similarity queries and propose a classification schema that easily allows existing approaches to be compared along several independent coordinates. Then, we discuss the impact that scheduling of index nodes can have on performance and show that, unlike exact similarity queries, no provable optimal scheduling strategy exists for approximate queries. On the positive side, we show that optimal-on-the-average schedules are well-defined and that their performance is indeed the best among practical schedules.},
journal = {J. of Discrete Algorithms},
month = {mar},
pages = {36–48},
numpages = {13},
keywords = {Scheduling strategy, Similarity search, Approximate queries}
}

@article{kac_walk,
author = {Vishesh Jain and Natesh S. Pillai and Ashwin Sah and Mehtaab Sawhney and Aaron Smith},
title = {{Fast and memory-optimal dimension reduction using Kac’s walk}},
volume = {32},
journal = {The Annals of Applied Probability},
number = {5},
publisher = {Institute of Mathematical Statistics},
pages = {4038 -- 4064},
keywords = {Johnson–Lindenstrauss lemma, Kac walk, restricted isometry property},
year = {2022},
doi = {10.1214/22-AAP1784},
URL = {https://doi.org/10.1214/22-AAP1784}
}


@article{JL_survey,
  author    = {Casper Benjamin Freksen},
  title     = {An Introduction to Johnson-Lindenstrauss Transforms},
  journal   = {CoRR},
  volume    = {abs/2103.00564},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.00564},
  eprinttype = {arXiv},
  eprint    = {2103.00564},
  timestamp = {Thu, 04 Mar 2021 17:00:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-00564.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{milvus,
author = {Wang, Jianguo and Yi, Xiaomeng and Guo, Rentong and Jin, Hai and Xu, Peng and Li, Shengjun and Wang, Xiangyu and Guo, Xiangzhou and Li, Chengming and Xu, Xiaohai and Yu, Kun and Yuan, Yuxing and Zou, Yinghao and Long, Jiquan and Cai, Yudong and Li, Zhenxiang and Zhang, Zhifeng and Mo, Yihua and Gu, Jun and Jiang, Ruiyi and Wei, Yi and Xie, Charles},
title = {Milvus: A Purpose-Built Vector Data Management System},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457550},
doi = {10.1145/3448016.3457550},
abstract = {Recently, there has been a pressing need to manage high-dimensional vector data in data science and AI applications. This trend is fueled by the proliferation of unstructured data and machine learning (ML), where ML models usually transform unstructured data into feature vectors for data analytics, e.g., product recommendation. Existing systems and algorithms for managing vector data have two limitations: (1) They incur serious performance issue when handling large-scale and dynamic vector data; and (2) They provide limited functionalities that cannot meet the requirements of versatile applications.This paper presents Milvus, a purpose-built data management system to efficiently manage large-scale vector data. Milvus supports easy-to-use application interfaces (including SDKs and RESTful APIs); optimizes for the heterogeneous computing platform with modern CPUs and GPUs; enables advanced query processing beyond simple vector similarity search; handles dynamic data for fast updates while ensuring efficient query processing; and distributes data across multiple nodes to achieve scalability and availability. We first describe the design and implementation of Milvus. Then we demonstrate the real-world use cases supported by Milvus. In particular, we build a series of 10 applications (e.g., image/video search, chemical structure analysis, COVID-19 dataset search, personalized recommendation, biological multi-factor authentication, intelligent question answering) on top of Milvus. Finally, we experimentally evaluate Milvus with a wide range of systems including two open source systems (Vearch and Microsoft SPTAG) and three commercial systems. Experiments show that Milvus is up to two orders of magnitude faster than the competitors while providing more functionalities. Now Milvus is deployed by hundreds of organizations worldwide and it is also recognized as an incubation-stage project of the LF AI &amp; Data Foundation. Milvus is open-sourced at https://github.com/milvus-io/milvus.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {2614–2627},
numpages = {14},
keywords = {heterogeneous computing, data science, vector database, high-dimensional similarity search, machine learning},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@article{johnson2019billion,
  title={Billion-scale similarity search with {GPUs}},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={IEEE Transactions on Big Data},
  volume={7},
  number={3},
  pages={535--547},
  year={2019},
  publisher={IEEE}
}

@Inbook{Berger2017sequential,
author="Berger, James O.",
title="Sequential Analysis",
bookTitle="The New Palgrave Dictionary of Economics",
year="2017",
publisher="Palgrave Macmillan UK",
address="London",
pages="1--3",
abstract="Data often arrives sequentially, rather than as a collection. When this occurs -- or when the experiment can be designed so that this occurs -- there can be a considerable advantage in using statistical methods, called sequential analysis, that are tailored to such situations. Classical frequentist statistical methods require analysis with a pre-specified collection of data, and hence cannot be used in such sequential settings. Interestingly, Bayesian methods can be directly used in sequential settings.",
isbn="978-1-349-95121-5",
doi="10.1057/978-1-349-95121-5_1295-2",
url="https://doi.org/10.1057/978-1-349-95121-5_1295-2"
}

@InProceedings{SISAP_graph,
author="Iwasaki, Masajiro",
editor="Amsaleg, Laurent
and Houle, Michael E.
and Schubert, Erich",
title="Pruned Bi-directed K-nearest Neighbor Graph for Proximity Search",
booktitle="Similarity Search and Applications",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="20--33",
abstract="In this paper, we address the problems with fast proximity searches for high-dimensional data by using a graph as an index. Graph-based methods that use the k-nearest neighbor graph (KNNG) as an index perform better than tree-based and hash-based methods in terms of search precision and query time. To further improve the performance of the KNNG, the number of edges should be increased. However, increasing the number takes up more memory, while the rate of performance improvement gradually falls off. Here, we propose a pruned bi-directed KNNG (PBKNNG) in order to improve performance without increasing the number of edges. Different directed edges for existing edges between a pair of nodes are added to the KNNG, and excess edges are selectively pruned from each node. We show that the PBKNNG outperforms the KNNG for SIFT and GIST image descriptors. However, the drawback of the KNNG is that its construction cost is fatally expensive. As an alternative, we show that a graph can be derived from an approximate neighborhood graph, which costs much less to construct than a KNNG, in the same way as the PBKNNG and that it also outperforms a KNNG.",
isbn="978-3-319-46759-7"
}





@InProceedings{SISAP_benchmark,
author="Boytsov, Leonid
and Naidan, Bilegsaikhan",
editor="Brisaboa, Nieves
and Pedreira, Oscar
and Zezula, Pavel",
title="Engineering Efficient and Effective Non-metric Space Library",
booktitle="Similarity Search and Applications",
year="2013",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="280--293",
abstract="We present a new similarity search library and discuss a variety of design and performance issues related to its development. We adopt a position that engineering is equally important to design of the algorithms and pursue a goal of producing realistic benchmarks. To this end, we pay attention to various performance aspects and utilize modern hardware, which provides a high degree of parallelization. Since we focus on realistic measurements, performance of the methods should not be measured using merely the number of distance computations performed, because other costs, such as computation of a cheaper distance function, which approximates the original one, are oftentimes substantial. The paper includes preliminary experimental results, which support this point of view. Rather than looking for the best method, we want to ensure that the library implements competitive baselines, which can be useful for future work.",
isbn="978-3-642-41062-8"
}



@INPROCEEDINGS{additivePQ,
  author={Babenko, Artem and Lempitsky, Victor},
  booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Additive Quantization for Extreme Vector Compression}, 
  year={2014},
  volume={},
  number={},
  pages={931-938},
  doi={10.1109/CVPR.2014.124}}

@article{KNNimageretrieval,
title = {A survey of content-based image retrieval with high-level semantics},
journal = {Pattern Recognition},
volume = {40},
number = {1},
pages = {262-282},
year = {2007},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2006.04.045},
url = {https://www.sciencedirect.com/science/article/pii/S0031320306002184},
author = {Ying Liu and Dengsheng Zhang and Guojun Lu and Wei-Ying Ma},
keywords = {Content-based image retrieval, Semantic gap, High-level semantics, Survey},
abstract = {In order to improve the retrieval accuracy of content-based image retrieval systems, research focus has been shifted from designing sophisticated low-level feature extraction algorithms to reducing the ‘semantic gap’ between the visual features and the richness of human semantics. This paper attempts to provide a comprehensive survey of the recent technical achievements in high-level semantic-based image retrieval. Major recent publications are included in this survey covering different aspects of the research in this area, including low-level image feature extraction, similarity measurement, and deriving high-level semantic features. We identify five major categories of the state-of-the-art techniques in narrowing down the ‘semantic gap’: (1) using object ontology to define high-level concepts; (2) using machine learning methods to associate low-level features with query concepts; (3) using relevance feedback to learn users’ intention; (4) generating semantic template to support high-level image retrieval; (5) fusing the evidences from HTML text and the visual content of images for WWW image retrieval. In addition, some other related issues such as image test bed and retrieval performance evaluation are also discussed. Finally, based on existing technology and the demand from real-world applications, a few promising future research directions are suggested.}
}

@Inbook{KNNrecommendation,
author="Schafer, J. Ben
and Frankowski, Dan
and Herlocker, Jon
and Sen, Shilad",
editor="Brusilovsky, Peter
and Kobsa, Alfred
and Nejdl, Wolfgang",
title="Collaborative Filtering Recommender Systems",
bookTitle="The Adaptive Web: Methods and Strategies of Web Personalization",
year="2007",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="291--324",
abstract="One of the potent personalization technologies powering the adaptive web is collaborative filtering. Collaborative filtering (CF) is the process of filtering or evaluating items through the opinions of other people. CF technology brings together the opinions of large interconnected communities on the web, supporting filtering of substantial quantities of data. In this chapter we introduce the core concepts of collaborative filtering, its primary uses for users of the adaptive web, the theory and practice of CF algorithms, and design decisions regarding rating systems and acquisition of ratings. We also discuss how to evaluate CF systems, and the evolution of rich interaction interfaces. We close the chapter with discussions of the challenges of privacy particular to a CF recommendation service and important open research questions in the field.",
isbn="978-3-540-72079-9",
doi="10.1007/978-3-540-72079-9_9",
url="https://doi.org/10.1007/978-3-540-72079-9_9"
}



@article{KNNalgorithm,
 ISSN = {03067734, 17515823},
 URL = {http://www.jstor.org/stable/1403797},
 author = {Evelyn Fix and J. L. Hodges},
 journal = {International Statistical Review / Revue Internationale de Statistique},
 number = {3},
 pages = {238--247},
 publisher = {[Wiley, International Statistical Institute (ISI)]},
 title = {Discriminatory Analysis. Nonparametric Discrimination: Consistency Properties},
 urldate = {2022-07-12},
 volume = {57},
 year = {1989}
}



@article{satuluri2011bayesian,
  title={Bayesian locality sensitive hashing for fast similarity search},
  author={Satuluri, Venu and Parthasarathy, Srinivasan},
  journal={arXiv preprint arXiv:1110.1328},
  year={2011}
}

@ARTICLE{computervision,  author={Pele, Ofir and Werman, Michael},  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},   title={Robust Real-Time Pattern Matching Using Bayesian Sequential Hypothesis Testing},   year={2008},  volume={30},  number={8},  pages={1427-1443},  doi={10.1109/TPAMI.2007.70794}}

@INPROCEEDINGS{network,
  author={Jaeyeon Jung and Paxson, V. and Berger, A.W. and Balakrishnan, H.},
  booktitle={IEEE Symposium on Security and Privacy, 2004. Proceedings. 2004}, 
  title={Fast portscan detection using sequential hypothesis testing}, 
  year={2004},
  volume={},
  number={},
  pages={211-225},
  doi={10.1109/SECPRI.2004.1301325}}

@inproceedings{sequentialLSH,
author = {Chakrabarti, Aniket and Parthasarathy, Srinivasan},
title = {Sequential Hypothesis Tests for Adaptive Locality Sensitive Hashing},
year = {2015},
isbn = {9781450334693},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2736277.2741665},
doi = {10.1145/2736277.2741665},
abstract = {All pairs similarity search is a problem where a set of data objects is given and the task is to find all pairs of objects that have similarity above a certain threshold for a given similarity measure-of-interest. When the number of points or dimensionality is high, standard solutions fail to scale gracefully. Approximate solutions such as Locality Sensitive Hashing (LSH) and its Bayesian variants (BayesLSH and BayesLSHLite) alleviate the problem to some extent and provide substantial speedup over traditional index based approaches. BayesLSH is used for pruning the candidate space and computation of approximate similarity, whereas BayesLSHLite can only prune the candidates, but similarity needs to be computed exactly on the original data. Thus where ever the explicit data representation is available and exact similarity computation is not too expensive, BayesLSHLite can be used to aggressively prune candidates and provide substantial speedup without losing too much on quality. However, the loss in quality is higher in the BayesLSH variant, where explicit data representation is not available, rather only a hash sketch is available and similarity has to be estimated approximately. In this work we revisit the LSH problem from a Frequentist setting and formulate sequential tests for composite hypothesis (similarity greater than or less than threshold) that can be leveraged by such LSH algorithms for adaptively pruning candidates aggressively. We propose a vanilla sequential probability ratio test (SPRT) approach based on this idea and two novel variants. We extend these variants to the case where approximate similarity needs to be computed using fixed-width sequential confidence interval generation technique. We compare these novel variants with the SPRT variant and BayesLSH/Bayes-LSHLite variants and show that they can provide tighter qualitative guarantees over BayesLSH/BayesLSHLite -- a state-of-the-art approach -- while being upto 2.1x faster than a traditional SPRT and 8.8x faster than AllPairs.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {162–172},
numpages = {11},
keywords = {locality sensitive hashing, sequential hypothesis tests and confidence intervals, allpairs similarity search},
location = {Florence, Italy},
series = {WWW '15}
}

@book{korosteleva2008clinical,
  title={Clinical statistics: introducing clinical trials, survival analysis, and longitudinal data analysis},
  author={Korosteleva, Olga},
  year={2008},
  publisher={Jones \& Bartlett Publishers}
}

@article{tutorialThemis,
author = {Echihabi, Karima and Zoumpatianos, Kostas and Palpanas, Themis},
title = {New Trends in High-D Vector Similarity Search: Al-Driven, Progressive, and Distributed},
year = {2021},
issue_date = {July 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3476311.3476407},
doi = {10.14778/3476311.3476407},
abstract = {Similarity search is a core operation of many critical applications, involving massive collections of high-dimensional (high-d) objects. Objects can be data series, text, multimedia, graphs, database tables or deep network embeddings. In this tutorial, we revisit the similarity search problem in light of the recent advances in the field and the new big data landscape. We discuss key data science applications that require efficient high-d similarity search, we survey recent approaches and share surprising insights about their strengths and weaknesses, and we discuss open research problems, including the directions of AI-driven, progressive, and distributed high-d similarity search.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {3198–3201},
numpages = {4}
}

@inproceedings{tutorialXiao,
author = {Qin, Jianbin and Wang, Wei and Xiao, Chuan and Zhang, Ying and Wang, Yaoshu},
title = {High-Dimensional Similarity Query Processing for Data Science},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3470811},
doi = {10.1145/3447548.3470811},
abstract = {Similarity query (a.k.a. nearest neighbor query) processing has been an active research topic for several decades. It is an essential procedure in a wide range of applications (e.g., classification &amp; regression, deduplication, image retrieval, and recommender systems). Recently, representation learning and auto-encoding methods as well as pre-trained models have gained popularity. They basically deal with dense high-dimensional data, and this trend brings new opportunities and challenges to similarity query processing. Meanwhile, new techniques have emerged to tackle this long-standing problem theoretically and empirically. This tutorial aims to provide a comprehensive review of high-dimensional similarity query processing for data science. It introduces solutions from a variety of research communities, including data mining (DM), database (DB), machine learning (ML), computer vision (CV), natural language processing (NLP), and theoretical computer science (TCS), thereby highlighting the interplay between modern computer science and artificial intelligence technologies. We first discuss the importance of high-dimensional similarity query processing in data science applications, and then review query processing algorithms such as cover tree, locality sensitive hashing, product quantization, proximity graphs, as well as recent advancements such as learned indexes. We analyze their strengths and weaknesses and discuss the selection of algorithms in various application scenarios. Moreover, we consider the selectivity estimation of high-dimensional similarity queries, and show how researchers are bringing in state-of-the-art ML techniques to address this problem. We expect that this tutorial will provide an impetus towards new technologies for data science.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
pages = {4062–4063},
numpages = {2},
keywords = {similarity query processing, similarity search, high-dimensional data, nearest neighbor},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@book{wainwright_2019, place={Cambridge}, series={Cambridge Series in Statistical and Probabilistic Mathematics}, title={High-Dimensional Statistics: A Non-Asymptotic Viewpoint}, DOI={10.1017/9781108627771}, publisher={Cambridge University Press}, author={Wainwright, Martin J.}, year={2019}, collection={Cambridge Series in Statistical and Probabilistic Mathematics}}

@INPROCEEDINGS{rerankpq,  author={Jégou, Hervé and Tavenard, Romain and Douze, Matthijs and Amsaleg, Laurent},  booktitle={2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},   title={Searching in one billion vectors: Re-rank with source coding},   year={2011},  volume={},  number={},  pages={861-864},  doi={10.1109/ICASSP.2011.5946540}}

@ARTICLE{ITQ,
  author={Gong, Yunchao and Lazebnik, Svetlana and Gordo, Albert and Perronnin, Florent},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Iterative Quantization: A Procrustean Approach to Learning Binary Codes for Large-Scale Image Retrieval}, 
  year={2013},
  volume={35},
  number={12},
  pages={2916-2929},
  doi={10.1109/TPAMI.2012.193}}

@article{wald1945sequential,
  title={Sequential tests of statistical hypotheses},
  author={Wald, Abraham},
  journal={The annals of mathematical statistics},
  volume={16},
  number={2},
  pages={117--186},
  year={1945},
  publisher={JSTOR}
}

@ARTICLE{surveyl2hash,  author={Wang, Jingdong and Zhang, Ting and song, jingkuan and Sebe, Nicu and Shen, Heng Tao},  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},   title={A Survey on Learning to Hash},   year={2018},  volume={40},  number={4},  pages={769-790},  doi={10.1109/TPAMI.2017.2699960}}

@article{quickselect,
author = {Hoare, C. A. R.},
title = {Algorithm 65: Find},
year = {1961},
issue_date = {July 1961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/366622.366647},
doi = {10.1145/366622.366647},
journal = {Commun. ACM},
month = {jul},
pages = {321–322},
numpages = {2}
}

@INPROCEEDINGS{randomortho,  author={Jégou, Hervé and Douze, Matthijs and Schmid, Cordelia and Pérez, Patrick},  booktitle={2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},   title={Aggregating local descriptors into a compact image representation},   year={2010},  volume={},  number={},  pages={3304-3311},  doi={10.1109/CVPR.2010.5540039}}

@article{DBfriendly,
title = {Database-friendly random projections: Johnson-Lindenstrauss with binary coins},
journal = {Journal of Computer and System Sciences},
volume = {66},
number = {4},
pages = {671-687},
year = {2003},
note = {Special Issue on PODS 2001},
issn = {0022-0000},
doi = {https://doi.org/10.1016/S0022-0000(03)00025-4},
url = {https://www.sciencedirect.com/science/article/pii/S0022000003000254},
author = {Dimitris Achlioptas},
abstract = {A classic result of Johnson and Lindenstrauss asserts that any set of n points in d-dimensional Euclidean space can be embedded into k-dimensional Euclidean space—where k is logarithmic in n and independent of d—so that all pairwise distances are maintained within an arbitrarily small factor. All known constructions of such embeddings involve projecting the n points onto a spherically random k-dimensional hyperplane through the origin. We give two constructions of such embeddings with the property that all elements of the projection matrix belong in {−1,0,+1}. Such constructions are particularly well suited for database environments, as the computation of the embedding reduces to evaluating a single aggregate over k random partitions of the attributes.}
}

@InProceedings{learning2route2019ml,
  title = 	 {Learning to Route in Similarity Graphs},
  author =       {Baranchuk, Dmitry and Persiyanov, Dmitry and Sinitsin, Anton and Babenko, Artem},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {475--484},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/baranchuk19a/baranchuk19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/baranchuk19a.html},
  abstract = 	 {Recently similarity graphs became the leading paradigm for efficient nearest neighbor search, outperforming traditional tree-based and LSH-based methods. Similarity graphs perform the search via greedy routing: a query traverses the graph and in each vertex moves to the adjacent vertex that is the closest to this query. In practice, similarity graphs are often susceptible to local minima, when queries do not reach its nearest neighbors, getting stuck in suboptimal vertices. In this paper we propose to learn the routing function that overcomes local minima via incorporating information about the graph global structure. In particular, we augment the vertices of a given graph with additional representations that are learned to provide the optimal routing from the start vertex to the query nearest neighbor. By thorough experiments, we demonstrate that the proposed learnable routing successfully diminishes the local minima problem and significantly improves the overall search performance.}
}

@inproceedings{james_cheng,
author = {Li, Jinfeng and Yan, Xiao and Zhang, Jian and Xu, An and Cheng, James and Liu, Jie and Ng, Kelvin K. W. and Cheng, Ti-chung},
title = {A General and Efficient Querying Method for Learning to Hash},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3183750},
doi = {10.1145/3183713.3183750},
abstract = {As an effective solution to the approximate nearest neighbors (ANN) search problem, learning to hash (L2H) is able to learn similarity-preserving hash functions tailored for a given dataset. However, existing L2H research mainly focuses on improving query performance by learning good hash functions, while Hamming ranking (HR) is used as the default querying method. We show by analysis and experiments that Hamming distance, the similarity indicator used in HR, is too coarse-grained and thus limits the performance of query processing. We propose a new fine-grained similarity indicator, quantization distance (QD), which provides more information about the similarity between a query and the items in a bucket. We then develop two efficient querying methods based on QD, which achieve significantly better query performance than HR. Our methods are general and can work with various L2H algorithms. Our experiments demonstrate that a simple and elegant querying method can produce performance gain equivalent to advanced and complicated learning algorithms.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {1333–1347},
numpages = {15},
keywords = {large-scale similarity search, distributed computing},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@article{NSW,
title = {Approximate nearest neighbor algorithm based on navigable small world graphs},
journal = {Information Systems},
volume = {45},
pages = {61-68},
year = {2014},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2013.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0306437913001300},
author = {Yury Malkov and Alexander Ponomarenko and Andrey Logvinov and Vladimir Krylov},
keywords = {Similarity search, k-Nearest neighbor, Approximate nearest neighbor, Navigable small world, Distributed data structure},
abstract = {We propose a novel approach to solving the approximate k-nearest neighbor search problem in metric spaces. The search structure is based on a navigable small world graph with vertices corresponding to the stored elements, edges to links between them, and a variation of greedy algorithm for searching. The navigable small world is created simply by keeping old Delaunay graph approximation links produced at the start of construction. The approach is very universal, defined in terms of arbitrary metric spaces and at the same time it is very simple. The algorithm handles insertions in the same way as queries: by finding approximate neighbors for the inserted element and connecting it to them. Both search and insertion can be done in parallel requiring only local information from the structure. The structure can be made distributed. The accuracy of the probabilistic k-nearest neighbor queries can be adjusted without rebuilding the structure. The performed simulation for data in the Euclidean spaces shows that the structure built using the proposed algorithm has small world navigation properties with log2(n) insertion and search complexity at fixed accuracy, and performs well at high dimensionality. Simulation on a CoPHiR dataset revealed its high efficiency in case of large datasets (more than an order of magnitude less metric computations at fixed recall) compared to permutation indexes. Only 0.03\% of the 10 million 208-dimensional vector dataset is needed to be evaluated to achieve 0.999 recall (virtually exact search). For recall 0.93 processing speed 2800queries/s can be achieved on a dual Intel X5675 Xenon server node with Java implementation.}
}

@ARTICLE{kmeans,
  author={Lloyd, S.},
  journal={IEEE Transactions on Information Theory}, 
  title={Least squares quantization in PCM}, 
  year={1982},
  volume={28},
  number={2},
  pages={129-137},
  doi={10.1109/TIT.1982.1056489}}


@inproceedings{outlier_detection,
author = {Bay, Stephen D. and Schwabacher, Mark},
title = {Mining Distance-Based Outliers in near Linear Time with Randomization and a Simple Pruning Rule},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956758},
doi = {10.1145/956750.956758},
abstract = {Defining outliers by their distance to neighboring examples is a popular approach to finding unusual examples in a data set. Recently, much work has been conducted with the goal of finding fast algorithms for this task. We show that a simple nested loop algorithm that in the worst case is quadratic can give near linear time performance when the data is in random order and a simple pruning rule is used. We test our algorithm on real high-dimensional data sets with millions of examples and show that the near linear scaling holds over several orders of magnitude. Our average case analysis suggests that much of the efficiency is because the time to process non-outliers, which are the majority of examples, does not depend on the size of the data set.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {29–38},
numpages = {10},
keywords = {outliers, anomaly detection, distance-based operations, diskbased algorithms},
location = {Washington, D.C.},
series = {KDD '03}
}

@article{fu2019fast,
author = {Fu, Cong and Xiang, Chao and Wang, Changxu and Cai, Deng},
title = {Fast Approximate Nearest Neighbor Search with the Navigating Spreading-out Graph},
year = {2019},
issue_date = {January 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.14778/3303753.3303754},
doi = {10.14778/3303753.3303754},
abstract = {Approximate nearest neighbor search (ANNS) is a fundamental problem in databases and data mining. A scalable ANNS algorithm should be both memory-efficient and fast. Some early graph-based approaches have shown attractive theoretical guarantees on search time complexity, but they all suffer from the problem of high indexing time complexity. Recently, some graph-based methods have been proposed to reduce indexing complexity by approximating the traditional graphs; these methods have achieved revolutionary performance on million-scale datasets. Yet, they still can not scale to billion-node databases. In this paper, to further improve the search-efficiency and scalability of graph-based methods, we start by introducing four aspects: (1) ensuring the connectivity of the graph; (2) lowering the average out-degree of the graph for fast traversal; (3) shortening the search path; and (4) reducing the index size. Then, we propose a novel graph structure called Monotonic Relative Neighborhood Graph (MRNG) which guarantees very low search complexity (close to logarithmic time). To further lower the indexing complexity and make it practical for billion-node ANNS problems, we propose a novel graph structure named Navigating Spreading-out Graph (NSG) by approximating the MRNG. The NSG takes the four aspects into account simultaneously. Extensive experiments show that NSG outperforms all the existing algorithms significantly. In addition, NSG shows superior performance in the E-commercial scenario of Taobao (Alibaba Group) and has been integrated into their billion-scale search engine.},
journal = {Proc. VLDB Endow.},
month = {jan},
pages = {461–474},
numpages = {14}
}

@misc{annoy,
howpublished = {\url{https://github.com/spotify/annoy}},
author ={Annoy},
Title = {Annoy}, 
year={2016}
}

@misc{technical_report,
howpublished = {\url{https://github.com/gaoj0017/ADSampling/technical_report.pdf} (figures better viewed in Adobe PDF)},
author ={Gao, Jianyang and Long, Cheng},
Title = {High-Dimensional Approximate Nearest Neighbor Search: with Reliable and Efficient Distance Comparison Operations (Technical Report)},
year={2023}
}

@INPROCEEDINGS{reviewer_SISAP_metric,
  author={Patella, Marco and Ciaccia, Paolo},
  booktitle={First International Workshop on Similarity Search and Applications (sisap 2008)}, 
  title={The Many Facets of Approximate Similarity Search}, 
  year={2008},
  volume={},
  number={},
  pages={10-21},
  doi={10.1109/SISAP.2008.18}}

@ARTICLE{malkov2018efficient,
  author={Malkov, Yu A. and Yashunin, D. A.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs}, 
  year={2020},
  volume={42},
  number={4},
  pages={824-836},
  doi={10.1109/TPAMI.2018.2889473}}

@inproceedings{beygelzimer2006cover,
  title={Cover trees for nearest neighbor},
  author={Beygelzimer, Alina and Kakade, Sham and Langford, John},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={97--104},
  year={2006}
}

@INPROCEEDINGS{linkandcode,  author={Douze, Matthijs and Sablayrolles, Alexandre and Jégou, Hervé},  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},   title={Link and Code: Fast Indexing with Graphs and Compact Regression Codes},   year={2018},  volume={},  number={},  pages={3646-3654},  doi={10.1109/CVPR.2018.00384}}

@inproceedings{dasgupta2008random,
  title={Random projection trees and low dimensional manifolds},
  author={Dasgupta, Sanjoy and Freund, Yoav},
  booktitle={Proceedings of the fortieth annual ACM symposium on Theory of computing},
  pages={537--546},
  year={2008}
}

@article{muja2014scalable,
  title={Scalable nearest neighbor algorithms for high dimensional data},
  author={Muja, Marius and Lowe, David G},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={36},
  number={11},
  pages={2227--2240},
  year={2014},
  publisher={IEEE}
}

@InProceedings{optimalJL2,
author="Kane, Daniel
and Meka, Raghu
and Nelson, Jelani",
editor="Goldberg, Leslie Ann
and Jansen, Klaus
and Ravi, R.
and Rolim, Jos{\'e} D. P.",
title="Almost Optimal Explicit Johnson-Lindenstrauss Families",
booktitle="Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="628--639",
abstract="The Johnson-Lindenstrauss lemma is a fundamental result in probability with several applications in the design and analysis of algorithms. Constructions of linear embeddings satisfying the Johnson-Lindenstrauss property necessarily involve randomness and much attention has been given to obtain explicit constructions minimizing the number of random bits used. In this work we give explicit constructions with an almost optimal use of randomness: For 0{\thinspace}<{\thinspace}$\epsilon$,$\delta${\thinspace}<{\thinspace}1/2, we obtain explicit generators G:{\{}0,1{\}}r{\thinspace}{\textrightarrow}{\thinspace}ℝs {\texttimes}dfor s{\thinspace}={\thinspace}O(log(1/$\delta$)/$\epsilon$2) such that for all d-dimensional vectors w of norm one,",
isbn="978-3-642-22935-0"
}



@article{optimalJL,
author = {Jayram, T. S. and Woodruff, David P.},
title = {Optimal Bounds for Johnson-Lindenstrauss Transforms and Streaming Problems with Subconstant Error},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1549-6325},
url = {https://doi.org/10.1145/2483699.2483706},
doi = {10.1145/2483699.2483706},
abstract = {The Johnson-Lindenstrauss transform is a dimensionality reduction technique with a wide range of applications to theoretical computer science. It is specified by a distribution over projection matrices from Rn → Rk where k n and states that k = O(ε−2 log 1/δ) dimensions suffice to approximate the norm of any fixed vector in Rn to within a factor of 1 ± ε with probability at least 1 − δ. In this article, we show that this bound on k is optimal up to a constant factor, improving upon a previous Ω((ε−2 log 1/δ)/log(1/ε)) dimension bound of Alon. Our techniques are based on lower bounding the information cost of a novel one-way communication game and yield the first space lower bounds in a data stream model that depend on the error probability δ.For many streaming problems, the most na\"{\i}ve way of achieving error probability δ is to first achieve constant probability, then take the median of O(log 1/δ) independent repetitions. Our techniques show that for a wide range of problems, this is in fact optimal! As an example, we show that estimating the ℓp-distance for any p ∈ [0,2] requires Ω(ε−2 log n log 1/δ) space, even for vectors in {0,1}n. This is optimal in all parameters and closes a long line of work on this problem. We also show the number of distinct elements requires Ω(ε−2 log 1/δ + log n) space, which is optimal if ε−2 = Ω(log n). We also improve previous lower bounds for entropy in the strict turnstile and general turnstile models by a multiplicative factor of Ω(log 1/δ). Finally, we give an application to one-way communication complexity under product distributions, showing that, unlike the case of constant δ, the VC-dimension does not characterize the complexity when δ = o(1).},
journal = {ACM Trans. Algorithms},
month = {jun},
articleno = {26},
numpages = {17},
keywords = {entropy, Communication complexity, distinct elements, frequency moments, data streams}
}

@article{yu2016orthogonal,
  title={Orthogonal random features},
  author={Yu, Felix Xinnan X and Suresh, Ananda Theertha and Choromanski, Krzysztof M and Holtmann-Rice, Daniel N and Kumar, Sanjiv},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{achlioptas2003database,
  title={Database-friendly random projections: Johnson-Lindenstrauss with binary coins},
  author={Achlioptas, Dimitris},
  journal={Journal of computer and System Sciences},
  volume={66},
  number={4},
  pages={671--687},
  year={2003},
  publisher={Elsevier}
}

@article{kruskal1964multidimensional,
  title={Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis},
  author={Kruskal, Joseph B},
  journal={Psychometrika},
  volume={29},
  number={1},
  pages={1--27},
  year={1964},
  publisher={Springer}
}

@article{jlintro,
  author    = {Casper Benjamin Freksen},
  title     = {An Introduction to Johnson-Lindenstrauss Transforms},
  journal   = {CoRR},
  volume    = {abs/2103.00564},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.00564},
  eprinttype = {arXiv},
  eprint    = {2103.00564},
  timestamp = {Thu, 04 Mar 2021 17:00:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-00564.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{jltspace,
author = {Kleinberg, Jon M.},
title = {Two Algorithms for Nearest-Neighbor Search in High Dimensions},
year = {1997},
isbn = {0897918886},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/258533.258653},
doi = {10.1145/258533.258653},
booktitle = {Proceedings of the Twenty-Ninth Annual ACM Symposium on Theory of Computing},
pages = {599–608},
numpages = {10},
location = {El Paso, Texas, USA},
series = {STOC '97}
}

@article{blockjlt,
author = {Kane, Daniel M. and Nelson, Jelani},
title = {Sparser Johnson-Lindenstrauss Transforms},
year = {2014},
issue_date = {January 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {61},
number = {1},
issn = {0004-5411},
url = {https://doi.org/10.1145/2559902},
doi = {10.1145/2559902},
abstract = {We give two different and simple constructions for dimensionality reduction in ℓ2 via linear mappings that are sparse: only an O(ε)-fraction of entries in each column of our embedding matrices are non-zero to achieve distortion 1 + ε with high probability, while still achieving the asymptotically optimal number of rows. These are the first constructions to provide subconstant sparsity for all values of parameters, improving upon previous works of Achlioptas [2003] and Dasgupta et al. [2010]. Such distributions can be used to speed up applications where ℓ2 dimensionality reduction is used.},
journal = {J. ACM},
month = {jan},
articleno = {4},
numpages = {23},
keywords = {streaming algorithms, numerical linear algebra, Johnson-Lindenstrauss, Dimensionality reduction}
}

@article{fftjlt,
author = {Ailon, Nir and Chazelle, Bernard},
title = {The Fast Johnson–Lindenstrauss Transform and Approximate Nearest Neighbors},
journal = {SIAM Journal on Computing},
volume = {39},
number = {1},
pages = {302-322},
year = {2009},
doi = {10.1137/060673096},

URL = { 
        https://doi.org/10.1137/060673096
    
},
eprint = { 
        https://doi.org/10.1137/060673096
    
}
,
    abstract = { We introduce a new low-distortion embedding of \$\ell\_2^d\$ into \$\ell\_p^{O(\log n)}\$ (\$p=1,2\$) called the fast Johnson–Lindenstrauss transform (FJLT). The FJLT is faster than standard random projections and just as easy to implement. It is based upon the preconditioning of a sparse projection matrix with a randomized Fourier transform. Sparse random projections are unsuitable for low-distortion embeddings. We overcome this handicap by exploiting the “Heisenberg principle” of the Fourier transform, i.e., its local-global duality. The FJLT can be used to speed up search algorithms based on low-distortion embeddings in \$\ell\_1\$ and \$\ell\_2\$. We consider the case of approximate nearest neighbors in \$\ell\_2^d\$. We provide a faster algorithm using classical projections, which we then speed up further by plugging in the FJLT. We also give a faster algorithm for searching over the hypercube. }
}


@inproceedings{adaptive2020ml,
author = {Li, Conglong and Zhang, Minjia and Andersen, David G. and He, Yuxiong},
title = {Improving Approximate Nearest Neighbor Search through Learned Adaptive Early Termination},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380600},
doi = {10.1145/3318464.3380600},
abstract = {In applications ranging from image search to recommendation systems, the problem of identifying a set of "similar" real-valued vectors to a query vector plays a critical role. However, retrieving these vectors and computing the corresponding similarity scores from a large database is computationally challenging. Approximate nearest neighbor (ANN) search relaxes the guarantee of exactness for efficiency by vector compression and/or by only searching a subset of database vectors for each query. Searching a larger subset increases both accuracy and latency. State-of-the-art ANN approaches use fixed configurations that apply the same termination condition (the size of subset to search) for all queries, which leads to undesirably high latency when trying to achieve the last few percents of accuracy. We find that due to the index structures and the vector distributions, the number of database vectors that must be searched to find the ground-truth nearest neighbor varies widely among queries. Critically, we further identify that the intermediate search result after a certain amount of search is an important runtime feature that indicates how much more search should be performed. To achieve a better tradeoff between latency and accuracy, we propose a novel approach that adaptively determines search termination conditions for individual queries. To do so, we build and train gradient boosting decision tree models to learn and predict when to stop searching for a certain query. These models enable us to achieve the same accuracy with less total amount of search compared to the fixed configurations. We apply the learned adaptive early termination to state-of-the-art ANN approaches, and evaluate the end-to-end performance on three million to billion-scale datasets. Compared with fixed configurations, our approach consistently improves the average end-to-end latency by up to 7.1 times faster under the same high accuracy targets. Our approach is open source at github.com/efficient/faiss-learned-termination.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2539–2554},
numpages = {16},
keywords = {information retrieval, approximate nearest neighbor search},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@ARTICLE{learningtohashsurvey,  author={Wang, Jingdong and Zhang, Ting and song, jingkuan and Sebe, Nicu and Shen, Heng Tao},  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},   title={A Survey on Learning to Hash},   year={2018},  volume={40},  number={4},  pages={769-790},  doi={10.1109/TPAMI.2017.2699960}}

@article{learningtohash,
  author    = {Jun Wang and
               Wei Liu and
               Sanjiv Kumar and
               Shih{-}Fu Chang},
  title     = {Learning to Hash for Indexing Big Data - {A} Survey},
  journal   = {Proc. {IEEE}},
  volume    = {104},
  number    = {1},
  pages     = {34--57},
  year      = {2016},
  url       = {https://doi.org/10.1109/JPROC.2015.2487976},
  doi       = {10.1109/JPROC.2015.2487976},
  timestamp = {Fri, 09 Apr 2021 18:31:26 +0200},
  biburl    = {https://dblp.org/rec/journals/pieee/WangLKC16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{subramanya2019diskann,
author = {Subramanya, Suhas Jayaram and Devvrit,  and Kadekodi, Rohan and Krishnaswamy, Ravishankar and Simhadri, Harsha},
title = {DiskANN: Fast Accurate Billion-point Nearest Neighbor Search on a Single Node},
booktitle = {NeurIPS 2019},
year = {2019},
month = {November},
abstract = {Current state-of-the-art approximate nearest neighbor search (ANNS) algorithms generate indices that must be stored in main memory for fast high-recall search. This makes them expensive and limits the size of the dataset. We present a new graph-based indexing and search system called DiskANN that can index, store, and search a billion point database on a single workstation with just 64GB RAM and an inexpensive solid-state drive (SSD). Contrary to current wisdom, we demonstrate that the SSD-based indices built by DiskANN can meet all three desiderata for large-scale ANNS: high-recall, low query latency and high density (points indexed per node). On the billion point SIFT1B bigann dataset, DiskANN serves > 5000 queries a second with < 3ms mean latency and 95%+ 1-recall@1 on a 16 core machine, where state-of-the-art billion-point ANNS algorithms with similar memory footprint like FAISS [18] and IVFOADC+G+P [8] plateau at around 50% 1-recall@1. Alternately, in the high recall regime, DiskANN can index and serve 5 − 10x more points per node compared to state-of-the-art graphbased methods such as HNSW [21] and NSG [13]. Finally, as part of our overall DiskANN system, we introduce Vamana, a new graph-based ANNS index that is more versatile than the existing graph indices even for in-memory indices.},
url = {https://www.microsoft.com/en-us/research/publication/diskann-fast-accurate-billion-point-nearest-neighbor-search-on-a-single-node/},
}

@article{FuNSG17,
  author    = {Cong Fu and Chao Xiang and Changxu Wang and Deng Cai},
  title     = {Fast Approximate Nearest Neighbor Search With The Navigating Spreading-out Graphs},
  journal   = {{PVLDB}},
  volume    = {12},
  number    = {5},
  pages     = {461 - 474},
  year      = {2019},
  url       = {http://www.vldb.org/pvldb/vol12/p461-fu.pdf},
  doi       = {10.14778/3303753.3303754}
}


@inproceedings{reviewer_M_tree,
author = {Ciaccia, Paolo and Patella, Marco and Zezula, Pavel},
title = {M-Tree: An Efficient Access Method for Similarity Search in Metric Spaces},
year = {1997},
isbn = {1558604707},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the 23rd International Conference on Very Large Data Bases},
pages = {426–435},
numpages = {10},
series = {VLDB '97}
}

@ARTICLE{reviewer_DTW_distance,
  author={Bartolini, I. and Ciaccia, P. and Patella, M.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={WARP: accurate retrieval of shapes using phase of Fourier descriptors and time warping distance}, 
  year={2005},
  volume={27},
  number={1},
  pages={142-147},
  doi={10.1109/TPAMI.2005.21}}


@INPROCEEDINGS{reviewer_PAC,
  author={Ciaccia, P. and Patella, M.},
  booktitle={Proceedings of 16th International Conference on Data Engineering (Cat. No.00CB37073)}, 
  title={PAC nearest neighbor queries: Approximate and controlled search in high-dimensional and metric spaces}, 
  year={2000},
  volume={},
  number={},
  pages={244-255},
  doi={10.1109/ICDE.2000.839417}}


@InProceedings{dimension_sampling_aistat,
  title = 	 {Adaptive Estimation for Approximate $k$-Nearest-Neighbor Computations},
  author =       {LeJeune, Daniel and Heckel, Reinhard and Baraniuk, Richard},
  booktitle = 	 {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3099--3107},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/lejeune19a/lejeune19a.pdf},
  url = 	 {https://proceedings.mlr.press/v89/lejeune19a.html},
  abstract = 	 {Algorithms often carry out equally many computations for "easy" and "hard" problem instances. In particular, algorithms for finding nearest neighbors typically have the same running time regardless of the particular problem instance.  In this paper, we consider the approximate $k$-nearest-neighbor problem, which is the problem of finding a subset of O(k) points in a given set of points that contains the set of $k$ nearest neighbors of a given query point. We propose an algorithm based on adaptively estimating the distances, and show that it is essentially optimal out of algorithms that are only allowed to adaptively estimate distances. We then demonstrate both theoretically and experimentally that the algorithm can achieve significant speedups relative to the naive method.}
}


@article{graphbenchmark,
author = {Wang, Mengzhao and Xu, Xiaoliang and Yue, Qiang and Wang, Yuxiang},
title = {A Comprehensive Survey and Experimental Comparison of Graph-Based Approximate Nearest Neighbor Search},
year = {2021},
issue_date = {July 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3476249.3476255},
doi = {10.14778/3476249.3476255},
abstract = {Approximate nearest neighbor search (ANNS) constitutes an important operation in a multitude of applications, including recommendation systems, information retrieval, and pattern recognition. In the past decade, graph-based ANNS algorithms have been the leading paradigm in this domain, with dozens of graph-based ANNS algorithms proposed. Such algorithms aim to provide effective, efficient solutions for retrieving the nearest neighbors for a given query. Nevertheless, these efforts focus on developing and optimizing algorithms with different approaches, so there is a real need for a comprehensive survey about the approaches' relative performance, strengths, and pitfalls. Thus here we provide a thorough comparative analysis and experimental evaluation of 13 representative graph-based ANNS algorithms via a new taxonomy and fine-grained pipeline. We compared each algorithm in a uniform test environment on eight real-world datasets and 12 synthetic datasets with varying sizes and characteristics. Our study yields novel discoveries, offerings several useful principles to improve algorithms, thus designing an optimized method that outperforms the state-of-the-art algorithms. This effort also helped us pinpoint algorithms' working portions, along with rule-of-thumb recommendations about promising research directions and suitable algorithms for practitioners in different fields.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {1964–1978},
numpages = {15}
}

@article{lu2021hvs,
author = {Lu, Kejing and Kudo, Mineichi and Xiao, Chuan and Ishikawa, Yoshiharu},
title = {HVS: Hierarchical Graph Structure Based on Voronoi Diagrams for Solving Approximate Nearest Neighbor Search},
year = {2021},
issue_date = {October 2021},
publisher = {VLDB Endowment},
volume = {15},
number = {2},
issn = {2150-8097},
url = {https://doi.org/10.14778/3489496.3489506},
doi = {10.14778/3489496.3489506},
abstract = {Approximate nearest neighbor search (ANNS) is a fundamental problem that has a wide range of applications in information retrieval and data mining. Among state-of-the-art in-memory ANNS methods, graph-based methods have attracted particular interest owing to their superior efficiency and query accuracy. Most of these methods focus on the selection of edges to shorten the search path, but do not pay much attention to the computational cost at each hop. To reduce the cost, we propose a novel graph structure called HVS. HVS has a hierarchical structure of multiple layers that corresponds to a series of subspace divisions in a coarse-to-fine manner. In addition, we utilize a virtual Voronoi diagram in each layer to accelerate the search. By traversing Voronoi cells, HVS can reach the nearest neighbors of a given query efficiently, resulting in a reduction in the total search cost. Experiments confirm that HVS is superior to other state-of-the-art graph-based methods.},
journal = {Proc. VLDB Endow.},
month = {oct},
pages = {246–258},
numpages = {13}
}

@book{vershynin_2018, place={Cambridge}, series={Cambridge Series in Statistical and Probabilistic Mathematics}, title={High-Dimensional Probability: An Introduction with Applications in Data Science}, DOI={10.1017/9781108231596}, publisher={Cambridge University Press}, author={Vershynin, Roman}, year={2018}, collection={Cambridge Series in Statistical and Probabilistic Mathematics}}

@inproceedings{datar2004locality,
  title={Locality-sensitive hashing scheme based on p-stable distributions},
  author={Datar, Mayur and Immorlica, Nicole and Indyk, Piotr and Mirrokni, Vahab S},
  booktitle={Proceedings of the twentieth annual symposium on Computational geometry},
  pages={253--262},
  year={2004}
}

@inproceedings{indyk1998approximate,
  title={Approximate nearest neighbors: towards removing the curse of dimensionality},
  author={Indyk, Piotr and Motwani, Rajeev},
  booktitle={Proceedings of the thirtieth annual ACM symposium on Theory of computing},
  pages={604--613},
  year={1998}
}

@article{huang2015query,
  title={Query-aware locality-sensitive hashing for approximate nearest neighbor search},
  author={Huang, Qiang and Feng, Jianlin and Zhang, Yikai and Fang, Qiong and Ng, Wilfred},
  journal={Proceedings of the VLDB Endowment},
  volume={9},
  number={1},
  pages={1--12},
  year={2015},
  publisher={VLDB Endowment}
}

@article{lu2020vhp,
  title={VHP: approximate nearest neighbor search via virtual hypersphere partitioning},
  author={Lu, Kejing and Wang, Hongya and Wang, Wei and Kudo, Mineichi},
  journal={Proceedings of the VLDB Endowment},
  volume={13},
  number={9},
  pages={1443--1455},
  year={2020},
  publisher={Association for Computing Machinery (ACM)}
}

@ARTICLE{6809191,  author={Muja, Marius and Lowe, David G.},  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},   title={Scalable Nearest Neighbor Algorithms for High Dimensional Data},   year={2014},  volume={36},  number={11},  pages={2227-2240},  doi={10.1109/TPAMI.2014.2321376}}

@article{yu2016orthogonal,
  title={Orthogonal random features},
  author={Yu, Felix Xinnan X and Suresh, Ananda Theertha and Choromanski, Krzysztof M and Holtmann-Rice, Daniel N and Kumar, Sanjiv},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@book{muirhead2009aspects,
  title={Aspects of multivariate statistical theory},
  author={Muirhead, Robb J},
  year={2009},
  publisher={John Wiley \& Sons}
}

@article{sun2014srs,
  title={SRS: solving c-approximate nearest neighbor queries in high dimensional euclidean space with a tiny index},
  author={Sun, Yifang and Wang, Wei and Qin, Jianbin and Zhang, Ying and Lin, Xuemin},
  journal={Proceedings of the VLDB Endowment},
  year={2014}
}

@article{zheng2020pm,
  title={PM-LSH: A fast and accurate LSH framework for high-dimensional approximate NN search},
  author={Zheng, Bolong and Xi, Zhao and Weng, Lianggui and Hung, Nguyen Quoc Viet and Liu, Hang and Jensen, Christian S},
  journal={Proceedings of the VLDB Endowment},
  volume={13},
  number={5},
  pages={643--655},
  year={2020},
  publisher={VLDB Endowment}
}

@article{johnson1984extensions,
  title={Extensions of Lipschitz mappings into a Hilbert space 26},
  author={Johnson, William B and Lindenstrauss, Joram},
  journal={Contemporary mathematics},
  volume={26},
  pages={28},
  year={1984}
}

@inproceedings{larsen2017optimality,
  title={Optimality of the Johnson-Lindenstrauss lemma},
  author={Larsen, Kasper Green and Nelson, Jelani},
  booktitle={2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)},
  pages={633--638},
  year={2017},
  organization={IEEE}
}

@article{tao2010efficient,
  title={Efficient and accurate nearest neighbor and closest pair search in high-dimensional space},
  author={Tao, Yufei and Yi, Ke and Sheng, Cheng and Kalnis, Panos},
  journal={ACM Transactions on Database Systems (TODS)},
  volume={35},
  number={3},
  pages={1--46},
  year={2010},
  publisher={ACM New York, NY, USA}
}

@inproceedings{c2lsh,
author = {Gan, Junhao and Feng, Jianlin and Fang, Qiong and Ng, Wilfred},
title = {Locality-Sensitive Hashing Scheme Based on Dynamic Collision Counting},
year = {2012},
isbn = {9781450312479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2213836.2213898},
doi = {10.1145/2213836.2213898},
abstract = {Locality-Sensitive Hashing (LSH) and its variants are well-known methods for solving the c-approximate NN Search problem in high-dimensional space. Traditionally, several LSH functions are concatenated to form a "static" compound hash function for building a hash table. In this paper, we propose to use a base of m single LSH functions to construct "dynamic" compound hash functions, and define a new LSH scheme called Collision Counting LSH (C2LSH). If the number of LSH functions under which a data object o collides with a query object q is greater than a pre-specified collision threhold l, then o can be regarded as a good candidate of c-approximate NN of q. This is the basic idea of C2LSH.Our theoretical studies show that, by appropriately choosing the size of LSH function base m and the collision threshold l, C2LSH can have a guarantee on query quality. Notably, the parameter m is not affected by dimensionality of data objects, which makes C2LSH especially good for high dimensional NN search. The experimental studies based on synthetic datasets and four real datasets have shown that C2LSH outperforms the state of the art method LSB-forest in high dimensional space.},
booktitle = {Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data},
pages = {541–552},
numpages = {12},
keywords = {locality sensitive hashing, dynamic collision counting},
location = {Scottsdale, Arizona, USA},
series = {SIGMOD '12}
}
