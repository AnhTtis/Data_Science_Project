\documentclass[a4paper]{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document languagex
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage{adjustbox}
\usepackage{blindtext}
\usepackage[a4paper,top=1.041in,bottom=2cm,left=0.9cm,right=0.9cm,marginparwidth=1cm]{geometry}
%\usepackage{blindtext}
%\usepackage[a4paper, total={6in, 8in}]{geometry}

% \usepackage{geometry}
%  \geometry{
%  a4paper,
%  total={170mm,257mm},
%  left=10mm,
%  top=1in,
%  }

\evensidemargin -0.23in
\oddsidemargin -0.23in
\setlength\textheight{9.0in}
\setlength\textwidth{6.75in}


% Useful packages
\usepackage{natbib}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{authblk}
\usepackage{titling}% the wheel somebody else kindly made for us earlier
\usepackage{fancyhdr}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage{amsthm}
\usepackage{bbm}
%\usepackage[round]{natbib}
\usepackage{dsfont}
\usepackage{multirow}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{xcolor}
\usepackage{float}

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{caption}
\usepackage{tabularx}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}




%\setlength {\marginparwidth }{2cm}
\begin{document}



\pretitle{ \vspace{-2cm}   \noindent\vrule height 1pt width \textwidth
\begin{center} \Large\bfseries\vskip-0.1in}%, make the fonts bigger, make the title (only) bold
\posttitle{%
  \end{center}%
  \noindent\vrule height 1pt width \textwidth
  \vskip .75em plus .25em minus .25em% increase the vertical spacing a bit, make this particular glue stretchier
}


% \renewcommand\maketitlehooka{%
%   \noindent\vrule height 2.5pt width \textwidth
% }

%\addtolength{\topmargin}{-0.5in} % adjust the value as desired



\preauthor{%
  \begin{center}
   \bfseries\lineskip 0.75em%
    \begin{tabular}[t]{@{}l@{}}%
}
\postauthor{%
    \end{tabular}
    \vskip -.9em
    \par
  \end{center}%
}



% unlike the default command, the redefined \maketitle makes it possible to reuse the title information in the headers
\pagestyle{fancy}
\fancyhf{}
\fancyhf[lh]{\itshape}
\fancyhf[ch]{\itshape \myshorttitle{}}
\fancyhf[rh]{\itshape }
\fancyhf[cf]{--- \thepage ---}

%\setlength{\headheight}{12pt}
\title{Combining Distance to Class Centroids and Outlier Discounting for Improved Learning with Noisy Labels}
\newcommand{\myshorttitle}{\textbf{NCOD}}

% Set up the header and footer
\pagestyle{fancy}
\fancyhf{} % clear the header and footer
\fancyhead[C]{\myshorttitle}


\author{
\textbf{Farooq Ahmad Wani}$^1$~~~   
\textbf{Maria Sofia Bucarelli}$^1$~~~
\textbf{Fabrizio Silvestri}$^1$~~~
\smallskip 
\\
$^1$Department of Computer, Control and Management Engineering   Antonio Ruberti, \\ Sapienza University of Rome, Italy
}

\renewcommand\Authands{ and }


\date{}


%\newgeometry{top=1in} % set the top margin to 1 inch for the title page only


\maketitle

\begin{abstract}
In this paper, we propose a new approach for addressing the challenge of training machine learning models in the presence of noisy labels. By combining a clever usage of distance to class centroids in the items' latent space with a discounting strategy to reduce the importance of samples far away from all the class centroids (i.e., outliers), our method effectively addresses the issue of noisy labels. Our approach is based on the idea that samples farther away from their respective class centroid in the early stages of training are more likely to be noisy. We demonstrate the effectiveness of our method through extensive experiments on several popular benchmark datasets. Our results show that our approach outperforms the state-of-the-art in this area, achieving significant improvements in classification accuracy when the dataset contains noisy labels.
We released the code for our loss function at \url{https://github.com/RSTLess-research/NCOD-Learning-with-noisy-labels}.
\end{abstract}
\section{Introduction}
\label{introduction}
Deep learning models have achieved state-of-the-art performance on various tasks, including image classification \citep{Krizhevsky2012}, natural language processing \citep{Hochreiter1997}, and speech recognition \citep{Deng2013}. However, the performance of these models is highly dependent on the quality of the training data. When the training data is noisy, deep learning models are prone to overfit the noisy training labels, which degrade the model performance \citep{Zhang2016}.

There is a very large amount of literature on training machine learning models when the training datasets contain noisy labels. To mitigate the negative effect of noisy labels on the quality of the trained models, researchers have proposed many strategies, most of them based on loss correction techniques. 
Label noise in training data can occur for various reasons: human error, the ambiguity of the task, or simply due to the existence of multiple correct labels for a single instance. As stated above, these issues can lead to a degradation in model performance, usually due to overfitting on noisy training samples and a subsequent decrease in generalization performance.

This paper proposes Noisy Centroids Outlier Discounting (NCOD), a novel training algorithm for deep neural networks based on two main mechanisms.  The first one, which we call ``\textit{class embeddings}'', exploits the natural assumption that similar samples tend to have the same label. In this case, we summarize each class with a representative vector, a sort of centroid used to compute the similarity between a sample and all the samples belonging to each class. The second mechanism, which we refer to as ``\textit{outliers discounting}'',  reduces the contribution of the noisy labels in computing the loss by considering them as outliers in the clustering induced by the class embedding mechanism. 

The main contributions are summarized as follows:
\begin{itemize}
\item  We propose a novel loss function that uses the learned similarity between a sample and a representation of the class to which the sample belongs as a soft label;
\item We propose a novel ``regularization'' term used to discount the contributions of samples' noise;
\item We extensively experiment on datasets in which both simulated artificial and natural noise are included. Experiments demonstrate that our method greatly improves state-of-the-art performances.
\item The technique we present in this paper does not need to predict noise rates and does not need anchor points. Therefore making it simpler to be used in services that need to be developed in real-world applications.

\end{itemize}


\section{Related Work}


\paragraph{Label noise previous work}

This paper addresses the problem of learning from corrupted labels.
There is a large body of work devoted to this challenge. 
Prior approaches have focused mostly on loss correction techniques. 
%Several methods have been proposed to reduce the negative effects of label noise.
$\ell_1$ loss, or mean absolute error (MAE) has been shown to be  robust to label noise \citep{ghosh2017robust, ghosh2015making} and is a common technique for dealing with corrupted labels. 
%The property of MAE of being noise tolerant come from the fact that is a  symmetric function, i.e. the sum of the risks over all categories is equivalent to a constant for each arbitrary example. 
Even though MAE is noise resistant, it might perform poorly in DNN-challenging domains \citep{zhang2018generalized}.
The idea of using symmetrized versions of cross entropy (CE) or loss functions that are the generalization of MAE and CE has had several new expansions recently \citep{wang2019symmetric,zhang2018generalized,feng2021can}.
%A different method \citet{menon2019can} uses a simple variant of gradient clipping to introduce a noise-resistant alternative to the standard cross-entropy loss that performs well experimentally.
Numerous methods have been presented for estimating the noise transition matrix (i.e. the probabilities that correct labels are changed for incorrect ones). Some works add a noise adaptation layer at the top of an underlying DNN to learn the label transition process \citep{goldberger2016training}. Other approaches use a limited number of anchor points (that is, samples that with probability one belong to a certain class) to approximate the noise transition matrix and correct the loss using that matrix \citep{patrini2017making,menon2015learning,zhu2022detecting,hendrycks2018using}. However, estimating the noise transition matrix often requires significant assumptions about the noise or a set of clean samples, both of which are difficult to get in real-world scenarios.
Despite their theoretical foundations, these strategies frequently outperform their counterparts.

\cite{xia2019anchor, zhu2021clusterability} propose a method for calculating the transition matrix without using anchor points. %Indeed \citet{xia2019anchor} utilize the examples with the greatest class posterior probability as anchor points;
In particular \citet{zhu2021clusterability} method is based on the clusterability condition, that is an example belongs to the same true class of its nearest-neighbors representations.
A little differently also \citet{zhu2022detecting}  makes the hypothesis that ``closer" instances are more likely to share the same clean label. Based on the neighborhood information, they propose two training-free methods. The first technique employs ``local voting'' to check noisy label consensuses of neighboring features based on neighborhood knowledge. The second is a ranking-based method that ranks each instance and removes a guaranteed amount of corrupted instances.
While our technique may seem to be a clusterability method, it differs from \citet{zhu2021clusterability} approach indeed we do not exploit the labels of the points' neighbors but rather their similarity to the mean of the point having a certain class.

Another kind of approach exploits the ``disagreement'' of two trained networks \citep{han2018co,wei2020combating}.  To avoid the model from memorizing noisy labels, \citet{lu2022ensemble} propose an ensemble model that consists of two networks.
SSL-based DivideMix \citep{li2020dividemix}  trains two networks simultaneously and achieves robustness to noise through dataset co-divide, label co-refinement, and co-guessing. However, the methods need a clean set to provide high accuracy of correctly-labeled data. 
\citet{liu2020early} study a new strategy for detecting the start of the memorization phase for each category during training and adaptively correcting the noisy annotations to exploit early learning.



 %\citet{Lee2020Robust}
 \citet{liu2022robust}
 , starting from the idea that noise is sparse and incoherent with the network learned from clean data, model the noisy labels via another sparse overparametrized term and exploit implicit regularization to recover and separate underlying corruptions.
\citet{algan2022metalabelnet} present a technique in which a classifier is trained on soft-labels generated based on a meta-objective. %The meta-objective adjusts the soft-labels prior to conventional training to reorganise the loss function to minimise meta-data loss.
Our solution shares with theirs the use of soft labels that are updated during training, but it is fundamentally different since it employs a single model rather than two neural networks.
\citet{iscen2022learning} also describe a technique that makes use of sample similarities; however, their approach, unlike ours, takes advantage of sample similarities between neighbors samples from the same batch.

We direct the interested reader to the survey papers  \citet{han2020survey} and \citet{song2022survey} for a comprehensive analysis of the existing scientific literature.
\section{Preliminaries}

%instance independnent noise 


 In this section, we formulate the task of learning with noisy labels and define the notation that is used in this paper. 
\textbf{Formulating the task and notation}: Assume that we have noisy labeled dataset $D$ defined as $D = \{(x_i, y_i) \mid i \in [1, n]\}$, here $x_i$ is the $i$-th training  data sample and $y_i$ is its corresponding label. %In this particular setting, we are not aware whether $y$ actually reflects the true label or it is a corrupted data label, the percentage of corrupted data labels is also unknown. 


In this setting, it is unknown whether $y$ is the real label or a corrupted data label, and the percentage of corrupted data labels is also unknown.
The goal of our work is to find the optimal model 
$$ f(\theta, x_i): R^d \rightarrow R^C $$
that learns only the true labeled data from our training data and ignores the noisy labeled data as much as possible. 
In typical settings where the training data is clean, the optimal model can be achieved as
$$ \operatorname{argmin}_{\theta} L(f(\theta, x_i), y_i) + R(\theta). $$ 
Where $L(f(\theta, x_i), y_i)$ is a loss function that measures the difference between the predicted output, $f(\theta, x_i)$ and the true output, $y_i$, and $R(\theta)$ is a regularization term that helps to prevent overfitting. 
We will not employ an explicit regularisation term since, in fact, overparameterized networks generalize effectively without any explicit regularisation \citep{kubo2019implicit, neyshabur2017implicit}.

For each sample $i$, we denote its feature $\phi(x_i)$ the output of the second to the last  layer of our network, we refer to this vector as embedding in the feature or latent  space, and we denote its normalized version by $h_i$, where $ h_i = \frac{\phi(x_i)}{||\phi(x_i)||}$. The symbol $|| .|| $ denote the $l_2 $ norm, i.e. $ || x|| = \sqrt{\sum_{i=1}^d x_i^2 }$.
We denote by $C$ the number of classes and by $e_j$ the $j$-th standard canonical vector in $\mathbb{R}^C$, meaning the vector with $1$ in the $j$-th position and zero in all other positions. 
%The $y_i$ for the $i_th$ sample in canonical form can be denoted as $e_{ci}$.
Therefore, the labels set using one-hot encoding representation are given by
$\mathcal{Y} = \{e_1, \dots , e_C \} \subset \{0,1\}^C $.

\section{Methodology}
This section describes our method of combining distance-to-class centroids and outlier discounting for improved learning with noisy labels. This approach aims to leverage the inherent relationships between representations of data points of the same class.

\subsection{Class Embeddings}
We leverage the assumption that a sample close to many samples from a given class is likelier to be part of that class than not. Therefore, for each data sample, we consider two labels. The label from the dataset (which can be noisy) and the (soft) label from close data points. 

%CUTTING THIS PART BECAUSE WE DON'T HAVE EXPERIMENTS
%FOR THE FUTURE LET's TRY TO HAVE THEM :)

%Initially, we approached this problem with a simple similarity-based soft labeling, where for every sample in the current training batch of size $B$, the label $y_i$ was replaced by a soft label $\hat{y}_i$ obtained as:
%$$\hat{y}_{i} = \frac{\sum_{k=1}^{B}[(h_{i}h^T_{k})(y_{i}y_{k}^T)]}{\sum_{k=1}^{d}[(y_{i}y_{k}^T)]},\text{for } i = 1 \dots d$$
%Notice that the term $y_i y_k^T$ is zero if the sample $i$ and $k$ don't belong to the same class.
%Thus, using this approach, the soft label of the $i$-th sample was obtained by exploiting only its similarity to the samples sharing the same class in the current batch.
%This approach is highly sensitive to data in the current batch. It can be problematic if the training data is very noisy, if the number of classes is high, or if the batch size is small and, therefore, the batch distribution is not representative of the sample distribution.
%Another drawback of the above definition of soft labels is that their irregular dynamics, which quickly vary and fluctuate widely, make the training convergence slow and less stable.
%To overcome these issues,


We propose to use the similarity between a sample and a summary of all the samples in the training dataset belonging to the same class as soft-label.
In particular, for each class $c \in \{1, \dots C\}$, we constructed a representation in latent space by averaging all the latent representations of that class's training samples.
% for each class $c  \in \{ 1, \dots C\}$, we constructed an representation in the latent space   by averaging over all the latent representations of the training samples of that class. 
In our case, the latent representation is the mean of all the individual hidden space representations $\phi(x)$ for all training samples $x$ from a particular class $c$. We denote this vector by 
%the embedding vector 
$\phi(c)$,
%for each class $c  \in \{ 1, \dots C\}$.
it can be computed as:
$$\phi(c) = \frac{\sum_{i=1, y_i=c}^n \phi(x_i)}{n_c}$$

here $n$ is the size of all training set, and $n_c$ is the number of samples of class $c$. %and $y_i$ is the label of sample $i$ and $\phi(x_i)$ is the hidden feature representaion of $i_{th}$ sample.

We denote by $\bar h_c$ the vector obtained normalizing $\phi(c)$ %for each class $c  \in \{ 1, \dots C\}$
and  we refer to as the class embedding:
%$\bar h_c$:
$$ \bar h_c = \frac{\phi(c)}{||\phi(c)||}    \text { for } c  \in \{ 1, \dots C\}$$
 
%$$ \phi(c) : = \frac{1}{|\{ i \in \{1, \dots n  \} \text{ s.t. } y_i = e_c|}\sum_{1\le i\le n\atop i s.t. y_i =e_c} %\frac{\phi(x_i)}{|| \phi(x_i)||}.$$
Then, the soft label for the $i$-th sample, having label $c$, is the similarity between the normalized feature representation $h_i$ of the sample $x_i$ and its class embedding $h_c$, namely, denoting by $\hat{y}_i $ the soft label for $i$-th sample: %and $y_i=e_{ci}$, 
%then $\hat{y}_i$ can be calculated as
 $$\hat{y}_i= [\max({h_i \bar h_c^T},0) y_i] $$ %Here $hi= \frac{\phi{x_i}} {||\phi{x_i}||}$ is the normalized hidden representation of the $i$-th sample and $\bar h_c$ is the normalized hidden representation of class $c$. 
$\hat{y}_i$ is, therefore, a vector of dimension $C$ with $0$ in all the positions where $y_i$ was zero and in the cases where $y_i$ is $1$ it is the maximum between zero and the similarity of the sample and its class embedding.
In building $\hat{y}_i$ for the entries where $y_i =1$, we take the maximum between the similarity and zero to remove the negative similarities, resulting in labels with non-negative values and preserving the cross-entropy loss convexity.
%We employed these labels to calculate the cross entropy instead of the actual label.

The strategy of using the similarity between a sample label and the class embedding has shown promising results but only when the number of training epochs was low. In subsequent epochs, indeed, we observed a decrease in the accuracy of the model in the validation see (\cref{fig:Figure_Acc}).
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{image_acc_drop.png}
    \caption{This picture shows the test accuracy for CIFAR $100$ with $50$ percent of symmetrical noise. It can be observed that the average accuracy increases for the first $100$ epochs, but then  starts decreasing.
    %as the model begins to learn also from the smallest similarities, that instead should be ignored because referring to noisy samples. 
    }
    \label{fig:Figure_Acc}
\end{figure}
\noindent
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{image_noisy_test_acc.png}
    \caption{Shows that the avg test accuracy drops as the model starts learning for noisy samples in the later part of the training. Here the percentage of noisy samples is  $50 \%$ and the noise is symmetric.
    %as the model begins to learn also from the smallest similarities, that instead should be ignored because referring to noisy samples. 
    }
    \label{fig:Noise_test_acc}
\end{figure}
\noindent
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\linewidth]{image_avg_sim_com.png}
    \caption{ Shows the model initially learns for pure samples which correspond to higher similarities and after approx. 100 epochs it significantly starts learning for smaller insignificant similarities corresponding to noisy samples.
   We obtained the plot  using $50\%$ symmetrical noise.
    }
    \label{fig:avg_sim_comp}
\end{figure}


One possible explanation for this drop in accuracy is that the model initially learns from samples with higher similarity values, which correspond to the samples most similar to their averages, and thus to the clean samples. 
Once higher prediction probability values are obtained for these samples, the classifier starts to make inaccurate predictions because it has memorized the noisy labels. This effect has been investigated in a paper by \citet{liu2020early} where it has been shown that deep neural networks, when trained on noisy labels, learn to training data with clean labels first during an ``early learning'' phase, and then they start memorizing samples with corrupted labels at later stages.
 In other words, once the model has learned from data in ``denser'' areas of the sample space, it starts overfitting on the ``outlier'' that are far away from these dense areas.
 
 % shows that our model effectly learns representation in the embedding space so that lower similarities correspond mainly to the noisy samples of a class and higher similarity corresponds to  clean data.
 As shown in \cref{fig:avg_sim_comp},our model initially effectively learns representations in the embedding space such that noisy samples have lower similarities with the corresponding class representation  while  clean data show higher similarities.
 %In fact, any additional information gained from the samples with lower similarities with the class embedding is leading to a drop in the accuracy of the model. 


\subsection{Outlier Discounting}
As the model starts to learn the noisy samples, the training accuracy increases, but we observe a corresponding decrease in test accuracy (\cref{fig:Noise_test_acc}). To prevent this, we introduced another learnable parameter $u \in [0,1]^n$ of size $n$, i.e., the number of samples in the training dataset.
We exploit this variable as a regularization threshold to avoid our model overfitting on samples with very small similarities.
We proposed to add a new term in the loss function, specifically $\mathcal{L}_2$, that is used to learn the parameter $u$.
%To learn $u$ as a threshold variable for the samples having lower similarity scores we introduced modified $\mathcal{L}_2$ loss.

Summarizing, the loss function $\mathcal{L}(x_i,y_i, u_i )$ that we adopted in our method to learn the parameters is thus:
\begin{equation}
\begin{split}
\label{eq:loss}
&\mathcal{L}(x_i,y_i, u_i )= \mathcal{L}_1 + \mathcal{L}_2. \\
& \quad \quad \quad  \quad\text{   where   }\\
&\mathcal{L}_1 =  \mathcal{L}_{\text{CE}} (f(x_i, \theta)) + u_i\textcolor{black}{ \cdot y_i}, \hat{y}_i) , \\
&\mathcal{L}_2 = || f(x_i, \theta) +u_i\textcolor{black}{ \cdot y_i}  -{y}_i ||^2_2
\end{split}
\end{equation}

\textcolor{black}{correction with respect to the version submitted to ICML: 
$u_i$ is a scalar for each sample, so it doesn't make sense to write $f(x_i,\theta) - u_i$, we could think it was implicit, but  better if we write $u_i  \cdot y$ Maybe to not modify everything later, as I'm doing now, we could simply write at the beginning that $u_i= u_i  \cdot y_i$.}
We used stochastic gradient descent to update the parameters, but we perform back-propagation only through the parameters $\theta$ in the first term  $\mathcal{L}_1$, and $u$ in the second term $\mathcal{L}_2$.
Specifically, we update the parameter using the following procedure:
\begin{align*}
\theta_{t+1} = \theta_{t} - \alpha \nabla_{\theta} \mathcal{L}_1 \\
u_{t+1} = u_t - \beta \nabla_{u} \mathcal{L}_2.
\end{align*}
Figure \ref{fig:avg_u} shows that the parameter $u_i$ learns how noisy a label is. The smaller the value of $u_i$, the cleaner the sample $x_i$ is.
We exploit this fact to compute the class embedding vector $\phi(c)$ in a better way. Indeed instead of  making use of all the samples from class $c$ for all epochs we gradually decrease the number of samples used choosing the subset of samples from class $c$ that correspond to the lowest values of $u$. Consequently, as the model learns to discriminate between noisy and clean samples, we exploit this fact and select only the samples considered clean with the highest probability to compute the class representation.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{image_avg_u.png}
    \caption{Shows that the value of u increases significantly for the Noisy samples and it only shows a marginal increase for pure samples. Plot obtained using $50\%$ symmetrical noise.}
    \label{fig:avg_u}
\end{figure}


To achieve this, we utilize a proportion of samples that decreases linearly with the number of epochs. For a particular class $c$, all samples with label $c$ are used to calculate $\phi(c)$ in the first epoch, and this proportion is gradually reduced such that only $50\%$ of the samples of that class are used in the final epoch. This ensures that the class representation is only calculated using the most informative and less noisy samples, thus reducing the overfitting problem and increasing the accuracy of the model.

We observe that we utilize $50\%$ of the labels in the last epoch since we consider the percentage of noise as unknown. If one knew the percentage of noise advance, it would be better to utilize the percentage of clean data in the total dataset as the final percentage of data used to compute the mean.

When we refer to our training strategy as NCOD  we assume that the loss is the one shown in equation (\ref{eq:loss}).

In Addition to this, we followed the literature guidelines \citep{liu2022robust,berthelot2019mixmatch,  li2019learning,tanaka2018joint}, and added two commonly used regularization terms namely the KL divergence between the network prediction on original images and the prediction on corresponding augmented images. 
%This regularization term is called consistency regularizer.
%and ensures the consistency of the network predictions, 
We also used a class balance regularization term that prevents the network from assigning all the predictions to the same class.%this term is called class balance regularizer.
When we add these two terms to the loss we refer to our method as NCOD+. 

Our training method results in a model that is naturally robust to noise.
%, without the need for additional regularization terms.
\cref{fig:Figure_u} shows the effectiveness of our method. Indeed, the similarity between the clean samples and the class mean grows with epochs when utilizing our loss, but the similarity between the noisy samples and their mean falls with training.
This means that the network is capable of learning significant embeddings. The clean samples are embedded closer to their class mean, whereas the noisy examples are embedded farther away.
\noindent
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{image_sim_after_u.png}
    \caption{Average similarity for clean samples, in blue, and for noisy samples in red for CIFAR-100 dataset. We can notice that similarity for pure samples increases as epochs increase and for noisy samples, it is small and does not show any significant increase. We used $50\%$ symmetrical noise. }
    \label{fig:Figure_u}
\end{figure}
\noindent
\vspace{-0.5cm}
\section{Analysis of Our Training Strategy}

\begin{figure*}
\centering

\subfigure[The figure illustrates the sample embeddings' behavior during the early training stages. The clean data points are closer to the centroids, indicating a higher similarity. At the same time, the noisy samples are closer to the boundaries of the classes and have not moved significantly from their original positions. in the case of the CIFAR-100 dataset with $50\%$, the model starts to memorize the noisy samples approximately after 100 epochs]{
\includegraphics[width=0.3\linewidth]{algo1.png}
}\quad
\subfigure[Without using the parameter $u$, the noisy samples will be more likely to move closer to the centroids of the class they are incorrectly labeled as, which makes it more difficult to distinguish them from the true samples of that class. This increases their similarity and makes it harder to separate them from the data points in that class.]{
\includegraphics[width=0.3\linewidth]{algo2.png}
}\quad
\subfigure[By incorporating $u$ into the model, we can prevent the model from learning from noisy samples, which helps to keep them farther away from the centroids. This in turn, allows us to disregard these samples when they are mislabeled for a certain class.]{
\includegraphics[width=0.3\linewidth]{algo3.png}
}
\caption{This figure provides a visual representation of the operation of our method. The three geometric shapes represent three different classes, with the solid-filled shapes representing clean samples and the zig-zag-filled shapes representing noisy samples. The black shapes indicate the class centroid, the angles of the clean samples concerning their respective class centroids are shown in green, and the angles for the noisy samples are shown in red.}
\end{figure*}

For each class, $c$, the set of samples in the training dataset that have class $c$ as their label can be partitioned into $C$ subsets, where each subset contains examples belonging to the same clean label.
To make learning from data possible, we must assume that each subset of samples with the corrupted label in $c$ should be smaller than the subset having the clean label of $c$. If that is not the case, then the observed label is independent of the data, and no learning is possible.
More formally, let $n$ be the number of samples in the training dataset. For each class $c$, we denote by $n_{c,c'}$ the number of samples labeled as $c$ whose correct label is instead $c'$, and by $n_{c,c}$ the number of samples whose labels are correctly labeled as $c$. The above assumption can thus be expressed as $n_{c,c} \geq n_{c,c'} \; \forall \; c' \neq c, \; c' \in \{ 1, \dots, C\} $.

%\textit{Way using the similarity is interesting}
%\textcolor{black}{Please read it because probably everything is false :) }

Our training procedure can be explained by considering three consecutive stages.

\paragraph{First Stage: Initialization.} At the beginning of the learning phase, since the weights are initialized randomly, all the errors on the training samples are of comparable magnitude.
More formally, let $n$ be the number of samples of the training set; then, we can write the loss as
\begin{equation}
\begin{split}
&\sum_{i=1}^n  \mathcal{L}_{\theta}(f(x_i, \theta) + u_i\textcolor{black}{ \cdot y_i}, \hat{y}_i  ) \\
&= \sum_{c = 1}^C  \sum_{j=1}^{n_c}\mathcal{L}_{\theta}(f(x_j, \theta) + u_j \textcolor{black}{ \cdot y_j}, \hat{y}_j  )\\
 &= \sum_{c = 1}^C  \sum_{c'=1}^C \sum_{j=1}^{n_{c,c'} } \mathcal{L}_{\theta}(f(x_j, \theta) + u_j\textcolor{black}{ \cdot y_j}, \hat{y}_j  )
\end{split}
\end{equation}

% By assuming that the magnitude of the loss of each sample $\mathcal{L}_{\theta}(f(x_j, \theta) + u_j,\hat{y}_j ) $ is similar, for the set of samples having label $c$, then major contribution in the loss is given by the subset of samples that have more samples. This means that the best thing we can do to minimize the loss is to minimize the loss on that subset of samples.
Given that initially, the representations for each sample are chosen randomly, the values of the loss $\mathcal{L}_{\theta}(f(x_j, \theta) + u_j\textcolor{black}{ \cdot y_j},\hat{y}_j ) $ for each sample belonging to the class $c$, are all similar. For this reason, the major contribution to the final value of the loss is given by the largest subset of samples, which in this case is the subset of samples correctly labeled. To minimize the loss, thus, the best strategy consists of minimizing the loss on that subset of samples, and the gradient will point to the direction mostly aligned with their gradients. Therefore, the updated representations will be influenced mostly by the samples with the correct labels. 

\paragraph{Second Stage: Neighborhood Similarity.} At this stage, the similarity between samples and their class embeddings is the factor that contributes more heavily to the loss.
This is mainly due to the effect of two phenomena. Firstly, the samples with the correct labels are more likely to be close to the right class embedding and, therefore ``moved" before the others. Secondly, their subset is more numerous than any other subset for that specific class. It will thus carry more weight in determining the location of the average in the embedding space. So, we expect the similarity to be higher with that class embedding making similarity as a soft label a suitable choice in our loss.
%Indeed, even if the difference in similarities computed on clean samples and that computed on noisy samples with their class representations is small, our loss will force learning using the clean samples, first.
Samples with higher similarity with their respective class representation in the embedding space lead to an increase in loss. Recall that the contribution of a sample $i$ to the loss is given by $\hat{y}_i \log(f(x_i)) $, the closer $\hat{y}_i $ is to one (which is the highest possible value for similarity) the larger is the contribution of that sample to the total loss. 
This explains the observed phenomenon in experiments, see Figure \ref{fig:avg_sim_comp}, in which the similarities of clean samples grow quicker than the similarities of noisy samples, and the accuracy on those samples increases faster.

\paragraph{Third stage: avoid memorization.} As reported in \citet{liu2020early}, the model learns more from correct samples at the early phase of the training. After this initial phase, it starts memorizing, i.e., overfitting on noisy labels. According to that observation. we introduce the outlier discounting parameter $u$ in our loss to avoid overfitting on noisy labels.
%indeed the usage of parameter $u$,  reduces the magnitude of this impact. 
We already showed experimentally (see Figure \ref{fig:avg_u}) the correlation between this parameter and the noise level in a sample. %Therefore, given how $u$ is defined, its value, during  the training, will increase for noisy samples.
The parameter $u$ is initialized using a normal distribution, with very small mean and variance parameters (the actual values are $\mathcal{N}(1 e-8, 1e-9)$). The loss we use to train $u$ is the term $\mathcal{L}_2 = || u_i \textcolor{black}{ \cdot y_i} - ( {y}_i- f(x_i, \theta))||^2_2$, this writing shows that to a minimum is obtain for  $ u_i \textcolor{black}{ \cdot y_i}=  {y}_i- f(x_i, \theta)$,  i.e. the difference between the network prediction and the label of each sample.
Notice that for the second term on the loss $\mathcal{L}_2$, we are using the hard labels provided in the dataset  and not the similarity soft-label.
This makes sure that $u$ is updated only when there is a mismatch between the label distribution and prediction distribution. Thus updates $u$ for only noisy samples.
%The value to which $u$ tends when the loss is minimized is the difference between the network prediction and the label of each sample.
% If our model could already discriminate between correct and noisy labels, it would provide predictions that were identical to the labels in the dataset for clean samples and different from the dataset for noisy samples.

As observed in our experiments and previously in \citet{liu2020early}, during the first two phases of the training, the model learns mostly from the correct samples, and the error between $y$ and $f(x, \theta)$ is minimized.
Additionally, as previously discussed, using similarity as a soft label will increase this effect.
The parameter $u$ tends to measure exactly this difference $y - f(x, \theta)$, therefore, in the ``early stage of training'' it will be kept small for clean samples, and it will increase for noisy ones.
Now, let us observe what happens to the loss's first term, $\mathcal{L}_1$, when we add $u$.
Let $(x, y)$ be a sample and $u$ the parameter for that sample. 

Suppose the sample belongs to class $1$.
%We recall that the output of the network $f(x, \theta) $ is a vector of dimension $C$ and that 
The prediction on the network is given by $ \operatorname{argmax}_{c \in \{1 \dots, C\} } {f(\theta)}_c $.
%with ${f(\theta)}_c$ denoting the $c$-th components of the vector.
As explained previously, $\hat{y}$ will also be such that only its first component is non-zero.
The contribution of the sample to the first term of the loss $\mathcal{L}_1$ is, therefore, $- \sum_{c=1}^C \hat{y}_c \log(f(x, \theta)_c) = - \hat{y}_1 \log(f(x, \theta)_1)$. To minimize this term, the output of the network $f(x,\theta)$ must be maximized in the first component; however, if the sample is noisy and it does not belong to the first class, we are learning the wrong label.
If we add the scalar $u$, the loss becomes $ - \hat{y}_1 \log(f(x, \theta)_1 + u\textcolor{black}{ \cdot y})$, if $u$ is big enough, we do not need $f(x, \theta)_1$ to be the maximum among the $C$ components to obtain $\operatorname{argmax}_{c=1^C} [f(\theta) + u\textcolor{black}{ \cdot y} ]_c =1$. 


As a result, as $u$ grows, noisy data are penalized as we only use samples with the lowest $u$ values (more likely to be correct) to compute the centroids. Therefore, the impact of the usage of similarity with the centroids will become even bigger.

It is the sequence of the above three stages that enable our model to outperform state-of-the-art methods of classification with noisy labels.

%\textit{We assume that the embedding are already learned:}
%In the case where the embedding are already learned in a meaningful way, namely with classes already well separeted. 
%If classes are already well separeted, and we are in the setting where we have several well-defined clusterings but non-homogeneous labels within the same cluster.
%If we look at a particular cluster, for what we are assuming there is a class that is dominant among the others. Let assume that for the cluster we are looking at this class, is class $c$. We have that the majority of the samples of the training dataset, labelled with class $c$  are in that  clusters. Of course there are some samples labelled with class $c$ in other clusters, but if they are dominant in any other cluster, we can't learn anything. 
%So the embedding representation for class $c$, $\phi(c)$, will be placed ``close" to the center of that cluster. 
%Using the loss in \ref{eq:loss}, we are trying to align the learned labels to the similarity between samples and their respective class.
%If a sample being labelled with class $c$ is very dissimilar from its mean, it  means that it is placed in a clustering where its class is not the dominant one, namely that it is a noisy sample.  So it's soft label is very small, meaning that that sample doesn't contribute to the loss too much (informal).

%\textit{Why using the average helps in finding clusters}
%At the very beginning the embedding mean nothing when we train from scratch. 
%Nevertheless, suppose the embeddings are putted randomly in the space. Since, even in the case of noisy labels, for each class $c$ the class that with the majority of the labels  is exactly $c$. It means that what we are trying to do, using the similarity with the mean as soft label, we are somehow trying to learn some clustering, and we

\begin{table*}[ht]
\centering


\begin{tabular}{|l|l|l|l|l|l|l|} 
\hline
\multirow{2}{*}{Method} & \multicolumn{3}{l|}{Symmetric} & Asym  & \multirow{2}{*}{Results taken} & \multirow{2}{*}{Network}  \\ 
\cline{2-5}
                        & 20\%  & 50\%  & 80\%           & 40\%  &                                &                           \\ 
\hline
CE                      & 58.1  & 47.1  & 23.8           & 43.3  & paper                          & Resnet34                  \\ 
\hline
MixUp                   & 69.9  & 57.3  & 33.6           & ~-~   & paper                          & Resnet34                  \\ 
\hline
DivideMix               & 77.1  & 74.6  & 60.2           & 72.1  & paper                          & Resnet34                  \\ 
\hline
ELR+                    & 77.7  & 73.8  & 60.8           & 77.5  & paper                          & Resnet34                  \\ 
\hline
SOP+                    & 78.8  & 75.9  & 63.3           & 78.0  & paper                          & PreActResNet18            \\ 
\hline
SOP+                    & 78.23 & 74.83 & 58.83          & 78.1  & we ran                         & PreActResNet18            \\ 
\hline
NCOD+                   & \textbf{80.34} & \textbf{77.96} & 62.1           & \textbf{78.66} & ours                           & PreActResNet18            \\ 
\hline
SOP+                    & 78.49 & 76.43 & 61.96          &       & we ran                         & PreActResNet34            \\ 
\hline
NCOD+                   & \textbf{80.66} & \textbf{78.0}    & \textbf{68.1}           & \textbf{79.18} & ours                           & PreActResNet34            \\
\hline
\end{tabular}
\caption{Comparison with the state of art methods for CIFAR 100 that uses the two networked ensembles architecture of self-learning under the symmetrical and asymmetrical noise of different percentages }
\label{tab:ensemble100}
\end{table*}



\begin{table*}[!ht]
\centering

\begin{tabular}{|l|l|l|l|l|l|l|l|} 
\hline
\multirow{2}{*}{Method} & \multicolumn{4}{l|}{Symmetric Noise} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Results \\taken\end{tabular}} & \multirow{2}{*}{Network} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Train\\Images\end{tabular}}  \\ 
\cline{2-5}
                        & 20\%  & 40\%  & 60\%  & 80\%         &                                                                          &                          &                                                                         \\ 
\hline
CE                      & 51.43 & 45.23 & 36.31 & 20.23        & paper                                                                    & Resnet34                 &                                                                         \\ 
\hline
Forward                 & 39.19 & 31.05 & 19.12 & ~8.99~       & paper                                                                    & Resnet34                 &                                                                         \\ 
\hline
GCE                     & 66.81 & 61.77 & 53.16 & 29.16        & paper                                                                    & Resnet34                 &                                                                         \\ 
\hline
SL                      & 70.38 & 62.27 & 54.82 & 25.91        & paper                                                                    & Resnet34                 &                                                                         \\ 
\hline
ELR                     & 74.21 & 68.28 & 59.28 & 29.78        & paper                                                                    & Resnet34                 &                                                                         \\ 
\hline
SOP~                    & 74.67 & 70.12 & 60.26 & 30.20        & paper                                                                    & Resnet34                 & 45k                                                                     \\ 
\hline
SOP                     & 70.4  & 67.7  & 57.1  & 29.26        & we ran                                                                   & Resnet34                 & 45k                                                                     \\ 
\hline
NCOD                    & 72.39 & \textbf{70.14} & \textbf{61.6}  & \textbf{37.54}        & ours                                                                     & Resnet34                 & 45k                                                                     \\ 
\hline
SOP                     & 68.44 & 66.3  & 58.52 & 32.84        & We ran                                                                   & Resnet34                 & 50k                                                                     \\ 
\hline
NCOD                    & \textbf{74.56} & \textbf{70.72} & \textbf{62.94} & \textbf{40.5}         & ours                                                                     & Resnet34                 & 50k                                                                     \\
\hline
\end{tabular}
\caption{Comparison of the test accuracy of the CIFAR-100 with different percentages of the symmetrical noise. We have recorded the mean over three runs }
\label{tab:simple100}
\end{table*}

\section{Experiments\label{section:experiments}}
In our experiments, we demonstrate the effectiveness of our method on datasets with both synthetic and realistic label noise. The results of our method are presented in Table \ref{tab:ensemble100}, Table \ref{tab:simple100}, and Table \ref{tab:miniwebvison}. In this section, we detail the process of performing the experiments on datasets with realistic label noise and datasets with synthetic label noise. The section begins by providing an overview of the datasets, followed by a description of the experimental setup, and concludes with a comparison of our results to those obtained from other methods such as those based on transition matrix estimation (Forward and Backward (\citet{patrini2017making}), loss function design (GCE \citet{zhang2018generalized} and SL \citet{wang2019symmetric}), training two networks (Co-teaching \citet{han2018co}), and label noise correction (ELR \citet{liu2020early}, CORES2 \citet{cheng2020learning}), and SOP \cite{liu2022robust}.


\subsection{Datasets}
 Our experiments use datasets with both synthetic and realistic label noise. The CIFAR 100 dataset (\citet{articleCifar100}), which has 50,000 training images and 10,000 test images of size 32*32, was used to generate synthetic label noise following the methodologies used by (\citet{https://doi.org/10.48550/arxiv.1804.06872}; \citet{liu2020early}; \citet{xia2021robust}), which are considered to be standard techniques for generating synthetic label noise, and, it allows us to have a fair comparison of results with other works that have used the same techniques. The CIFAR 100 dataset has clean labels. Symmetric label noise was generated by randomly flipping a certain percentage of labels across all classes, while asymmetric label noise was generated by randomly flipping labels for specific pairs of classes. To compare our results with datasets that have realistic label noise, we also tested on the mini web-vision dataset (\citet{webvision}), which has 65,945 training images with 50 classes and 50 validation images per class. The image sizes vary, more information can be found in the Appendix.
\subsection{Network structures and hyperparameters}
In our experiments, we utilized the Torch library version 1.12.1 and used specific hyperparameters and network types for both NCOD and NCOD+ on two different datasets. These details are provided in Table \ref{tab:hyperparameters}. It is worth noting that for certain experiments, retraining the last saved model for a small number of additional epochs, such as 5 to 10, improved the performance of the model. This highlights the effectiveness of our method even when applied to pre-trained models. Also, to avoid computational overhead, we calculated the mean representation of each class only once at the beginning of each epoch.
\subsection{Experimental results}

The performance of our method on both datasets for different noise levels is shown in Table \ref{tab:ensemble100}, Table \ref{tab:simple100}, and Table \ref{tab:miniwebvison}. These tables  provide a comprehensive comparison of the performance of our technique and other methods. Our results have proved that our approach is superior to other methods in all levels of noise for CIFAR-100, as evident from the comparison table. In order to reproduce the results of the SOP method \citep{liu2022robust} we have used a reduced set of $45k$  training samples for CIFAR-100 as mentioned in their work.  (while the standard size for the training set for CIFAR-100 is $50k$).
The results have shown that as the number of training samples increases, the performance of our model improves, while we observed a drop in performance for the SOP method for some levels of noise.
We also achieve the state of art performance using pre-trained models for mini-web vision. 
When we trained InceptionResnetV2 from scratch using mini-web vision  using the same hyperparameters as stated in \cite{liu2022robust}, we were not able to reproduce their results, our model obtained the better results for the same experimental settings and we recorded both in Table \ref{tab:miniwebvison}.



 
 % \usepackage{multirow}


% \usepackage{multirow}


%__________________________________________________________________________________________-
\noindent
\setlength{\belowcaptionskip}{-5pt}
\begin{table}
\centering
\begin{tabular}{|l|l|l|l|l|} 
\hline
                                                                      & \multicolumn{4}{c|}{~ ~ ~ ~ ~ ~ ~ Method}                                                                                                                                 \\ 
\hline
Network                                                               & CE    & \begin{tabular}[c]{@{}l@{}}SOP\\paper\end{tabular} & \begin{tabular}[c]{@{}l@{}}SOP\\We ran\end{tabular} & \begin{tabular}[c]{@{}l@{}}NCOD~\\(ours)\end{tabular}  \\ 
\hline
InceptionResNetV2                                                     & -     & 76.6                                               & 72.1                                                & 73.94                                                  \\ 
\hline
\begin{tabular}[c]{@{}l@{}}Resnet50\\pretrained\end{tabular}          & 81.15 & -                                                  & 82.28                                               & \textbf{83.33}                                                  \\ 
\hline
\begin{tabular}[c]{@{}l@{}}InceptionResnetV2\\pretrained\end{tabular} & 81.06 & -                                                  & -                                                   & \textbf{83.49}                                                  \\
\hline
\end{tabular}
\caption{This table shows the comparison of the test accuracy of the mini web vision dataset }
\label{tab:miniwebvison}
\end{table}

\section*{Conclusions and Future Directions}
%In this paper, we showed experimentally and theoretically that our approach, NCOD, is effective in handling noisy labels as it utilizes the inherent relationships between samples of the same class. We have experimentally shown that our method is effective in handling noisy labels and is able to achieve higher accuracy compared to other methods.Our studies proved that our approach surpasses currently available techniques in terms of both accuracy and robustness to noise. We showed that when using our loss function, the similarity between pure samples and their class average increases as training progresses, but the similarity between noisy samples and their respective class averages decreases. This indicates that the network is capable of learning meaningful representations and can distinguish between clean and noisy samples.In conclusion, our NCOD method provides an efficient and effective solution for training deep neural networks with noisy labels. The utilization of class embeddings and outlier discounting, in combination with consistency and class balance regularization terms, leads to a model that is naturally robust to noise and achieves higher accuracy on different  datasets.Although our approach works with both instance-dependent and instance-independent noise, in this work we only conducted experiments with the injection of instance-independent noise. Furthermore, we consider testing on datasets including feature-dependent noise, such as CIFAR-10 N.Additionally, we are considering using different types of discounting functions, as we currently only use a linear function. This is because we do not have information about the level of noise in the data. If we had prior knowledge or confidence about the noise levels, we could adjust the function accordingly.In future work, we propose to investigate if the geometric median, as compared to the mean, would result in a more robust representation of the class.

We presented NCOD, a technique for training deep neural networks with noisy labels, in this publication. We have shown experimentally that our strategy is successful in dealing with noisy labels and outperforms other strategies. Through the use of class embeddings, outlier discounting, and a balance of consistency and class regularisation, our NCOD technique leverages the inherent links between samples of the same class and takes use of the fact that the model first learns from clean data. As a result, the model is inherently resilient to noise and achieves improved accuracy across different datasets.
While our technique manages both instance-independent and instance-dependent noise, the experiments in this paper exclusively utilize instance-independent noise. In the future, we want to investigate feature-dependent noise, such as that  in the CIFAR-10 N dataset. Furthermore, we would like to  investigate the use of different discounting functions; we are currently using linear discounting, which is the simplest way when no information about the noise levels is provided, but we are interested in investigating the potential of others.

% \section*{Acknowledgements}
% This research was supported by the Italian Ministry of Education, University and Research (MIUR) under the grant ``Dipartimenti di eccellenza 2018--2022'' of the Department of Computer Science and the Department of Computer Engineering at Sapienza University of Rome. Partially supported by the ERC Advanced Grant 788893 AMDROMA ``Algorithmic and Mechanism Design Research in Online Markets'', the EC H2020RIA project ``SoBigData++'' (871042), and the MIUR PRIN project ALGADIMAR ``Algorithms, Games, and Digital Markets''. All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.





\bibliographystyle{apalike}
\bibliography{bibliography}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section*{Training details for NCOD and NCOD+}
% \usepackage{multirow}





\subsection*{Definition of label Noise}
In this paper, we have used two types of datasets: those with realistic label noise and those with synthetic label noise. For the CIFAR-100 dataset, we generated synthetic noise using the methodologies described in the work of \citet{liu2022robust}. On the other hand, the mini Webvision dataset has realistic noise. The synthetic noise was generated by following the procedures outlined in the \citet{liu2022robust} paper. This allows us to evaluate the performance of our method on datasets with both synthetic and realistic label noise and provides a comprehensive understanding of its effectiveness in various settings.
\subsection*{NCOD+}
To use NCOD+ for the ensemble network architecture experiments, we have also employed two additional regularization terms as used in \citet{liu2022robust}. The definition of these two regularizations are taken directly from their work and are used to improve the consistency and class balance of the network's predictions. These regularization terms are the consistency regularizer $L_C$ and class-balance regularizer $L_B$. 
Both of these terms are defined using the Kullback-Leibler divergence and help to improve the accuracy of the network's predictions by encouraging consistency and preventing the network from assigning all data points to the same class.
The final loss function for NCOD+ is constructed by combining these regularization terms with the original loss function and is optimized to improve the overall performance of the network.

\begin{table}[ht]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|l|l|l|l|l|} 
\hline
                  & \multicolumn{3}{c|}{CIFAR-100}                                                                                                                                                               & \multicolumn{2}{c|}{mini webvision}                                                                     \\ 
\hline
Architecture      & ResNet34                                                       & PreActResnet18                                               & PreActResnet34                                               & InceptionResNetV2 & \begin{tabular}[c]{@{}l@{}}InceptionResNetV2,\\ResNet-50 \\both(pretrained)\end{tabular}  \\ 
\hline
batch size        & 128                                                            & 128                                                          & 128                                                          &   32                &   256                                                                                  \\ 
\hline
learning rate     & 0.02                                                           & 0.02                                                         & 0.02                                                         &  0.02                 &  0.1                                                                                   \\ 
\hline
lr~ decay         & \begin{tabular}[c]{@{}l@{}}[80,120]\\multistep\\gamma =0.1\end{tabular} & \begin{tabular}[c]{@{}l@{}}Cosine \\Annealing \end{tabular}                                     & \begin{tabular}[c]{@{}l@{}}Cosine \\Annealing \end{tabular}                                           &  \begin{tabular}[c]{@{}l@{}}[50]\\multistep\\gamma =0.1\end{tabular}                 &  \begin{tabular}[c]{@{}l@{}}init 0.1\\LambdaLr\\warmup =5\end{tabular}                                                                                   \\ 
\hline
weight decay      & $5*10^{-4} $                                                     & $5*10^{-4}$                                                    & $5*10^{-4} $                                                   & $5*10^{-4} $                &  0.001                                                                                   \\ 
\hline
\begin{tabular}[c]{@{}l@{}}training \\epochs \end{tabular}   & $150$                                                             & $300$                                                          & $300$                                                         & $100$                  & $25$                                                                                    \\ 
\hline
\begin{tabular}[c]{@{}l@{}}training \\examples \end{tabular}  & 50 k                                                           & $50$k                                                          & $50$k                                                          &  $66k$                 &   $66k$                                                                                  \\ 
\hline
lr for u          & sym =$0.1$                                                       & \begin{tabular}[c]{@{}l@{}}sym = $0.1$\\Asym =$0.3$\end{tabular} & \begin{tabular}[c]{@{}l@{}}sym = $0.1$\\Asym =$0.3$\end{tabular} & $0.3$                  & $0.3$                                                                                    \\ 
\hline
wd for u          & $1e^{-8} $                                                       & $1e^{-8}$                                                      & $1e^{-8}$                                                      & $1e^{-8}$              &  $1e^{-8}$                                                                                   \\ 
\hline
init. std. for u        & $1e^{-9}$                                                       & $1e^{-9}$                                                      & $1e^{-9}$                                                      & $1e^{-9}$              &  $1e^{-9}$                                                                                    \\ 
\hline
$\lambda^c$                & $0.0$                                                            & $0.9$                                                          & $0.9$                                                          & $0.0$               &   $0.0$                                                                                  \\ 
\hline
$\lambda^b$                & $0.0$                                                            & $0.1$                                                          & $0.1$                                                          & $0.0$               &  $0.0$                                                                                   \\
\hline
\end{tabular}}
\caption{The table shows the hyperparameters used for our experiments, which were kept consistent across all experiments to ensure a fair comparison with other methods, particularly SOP (\citet{liu2022robust}). This consistency in hyperparameters allows for an accurate comparison of the performance of our method with respect to other methods and helps to eliminate any bias that may be introduced by variations in the experimental setup. The table provides a detailed description of the settings that were used to conduct the experiments, which is useful for reproducing the results and for understanding the experimental conditions under which our method was evaluated. It's also worth noting that, in case we retrain from the previous best-saved model, the learning rate for 'u' is given as high as 3. This is done to fine-tune the model and improve its performance. Additionally, The values of $\lambda^c $ and $\lambda^b$ are the coefficients for consistency regularization and class balance regularization respectively, as used in the experiments done by \citet{liu2022robust}. These coefficients are used to balance the trade-off between consistency and class balance in the model, and their values are determined through experimentation. }
\label{tab:hyperparameters}
\end{table}



\subsection*{Experimental Settings}
In our experiments on CIFAR-100, we use simple data augmentations including random crop and horizontal flip, following the same approach used in previous works (\citet{patrini2017making};\citet{liu2020early}). To improve the performance of our method, NCOD+, we also use unsupervised data augmentation techniques as described in (\citet{augmentation}) to create additional views of the data for consistency training.
For the  mini WebVision, we first resize the images to 256x256, and then perform a random crop to 227x227, along with a random horizontal flip. All images are standardized by their means and variances to ensure consistency during the experiments.
In our experiments, we use the Stochastic Gradient Descent (SGD) optimizer without weight decay for parameter U. We keep all the hyperparameters fixed for different levels of label noise to ensure fair comparison across experiments. To perform a fair comparison, we use the same settings of hyperparameters and architectures for both NCOD and NCOD+. A detailed description of the used hyperparameters can be found in Table \ref{tab:hyperparameters}. 
\subsection*{Additional details}
It is important to note that in Figure \ref{fig:Figure_u}, the average similarity of samples starts at around $0.20$ instead of $1$. This is because, in the first epoch, the class representative embeddings are initialized randomly, resulting in different similarity values and soft labels for each sample. This ensures that the noisy labeled data does not have an initial advantage and can begin learning. As training continues, the similarity of pure samples increases and predictions improve, but at some point, it starts overfitting, and accuracy decreases. However, the inclusion of $u$ in our method prevents overfitting by removing the effect of noisy samples in the cross-entropy loss. As previously stated, in the first few epochs, even the similarity of pure samples is a bit low, causing $u$ to learn for them as well. But eventually, as their similarity increases, $u$ starts decreasing for pure samples. In the case of noisy samples, as their similarity drops, their predictions decrease, and $u$ increases to compensate  the effect of de in $\mathcal{L}_1$ 
and to decrease the loss of $ \mathcal{L}_2 $.
The change in $u$ can be seen in Figure \ref{fig:sample_noise} for 2 noisy and 2 pure samples, and their corresponding similarities can be seen in Figure \ref{fig:sample_sim}.

\begin{figure*}
\centering
\subfigure[Noisy samples]{
\includegraphics[width=0.47\linewidth]{noisy_u_samples.png}
}\quad
\subfigure[Pure samples]{
\includegraphics[width=0.47\linewidth]{pure_u_samples.png}
}
\caption{
The figure depicts the learning of parameter $u$ for two noisy and two pure samples in $50\%$ symmetrical noise for CIFAR-100
}
\label{fig:sample_noise}
\end{figure*}
%________________________________________________________________________________

\begin{figure*}
\centering
\subfigure[Noisy samples]{
\includegraphics[width=0.45\linewidth]{noisy_sim_samples.png}
}\quad
\subfigure[Pure samples]{
\includegraphics[width=0.45\linewidth]{pure_sim_samples.png}
}
\caption{The figure shows how similarity changes during training for two noisy and two pure samples in $50\%$ symmetrical noise for CIFAR-100}
\label{fig:sample_sim}
\end{figure*}

\end{document}
