\section{Introduction}
    
    Nowadays, the development of deep neural network (DNN) architectures, such as ResNet~\cite{resnet}, ResNeXt~\cite{Xie2016resnext}, Faster R-CNN~\cite{ren2015faster}, and PSPNet~\cite{zhao2017pspnet}, has led to significant performance improvements for various computer vision tasks, such as image classification, object detection, and semantic segmentation.  
    However, the high performance of these DNN models comes at the cost of large size and high computational requirements for these architectures, which poses challenges for deployment of them in resource-constrained environments.
    \input{comparison_in_different_tasks}
    To address this problem, knowledge distillation~\cite{hinton2015distilling} has been proposed to achieve high performance with reduced computational cost by transferring the knowledge from a large model (teacher) to a smaller model (student).
    
    Specifically, feature-based knowledge distillation methods, which transfer knowledge from the intermediate layer features of the teacher model to the student model, have been intensively studied and demonstrated as a more effective and generic approach for improving the performance of student model.
    
    As point out in~\cite{lin2022tat}, due to the feature misalignment of the teacher and student model, directly mimicking the intermediate features of the teacher model via vanilla $L_2$ distances may enforce overly strict constraints on the student, leading to sub-optimal performance.
    
    To alleviate this problem, existing works design novel distillation loss functions~\cite{heo2019overhaul,shu2021cwd} or feature transformation modules~\cite{zhang2020fkd,yang2022fgd,lin2022tat,reviewkd,kzagoruyko2016at} to indirectly mimic the teacher’s features. Specifically, the latter kind of approaches often focus on the feature transformations along the spatial dimension, such as, guiding the student's attention towards the key regions of the feature map~\cite{kzagoruyko2016at} or the relationship between different pixels~\cite{zhang2020fkd,yang2022fgd,lin2022tat}.
    
     In this paper, we focus on the feature-based knowledge distillation and make an effort to address the feature misalignment problem along the channel dimension rather than spatial dimensions. 
     We have observed that channel-wise transformations (e.g., 1x1 convolution) have been widely used to align the features of different channel sizes in many tasks including the feature-based knowledge distillation. Moreover, for the feature-based knowledge distillation task, these channel-wise transformation module are discarded when the channel sizes of the teacher’s feature and student’ s feature are already the same. However, we empirically find that a linear channel-wise transformation, \emph{i.e.}, 1x1 convolution , can result in consistent performance improvements for feature-based knowledge distillation, even when the channel sizes of teacher’s feature and student’ s feature are already the same, the results are shown in table~\ref{table:DifferentTasks}. 
     
     Inspired by our empirical findings about the importance of channel-wise transformations for feature-based distillation, we propose a simple and generic approach that focuses on channel-wise feature alignment. Specifically, without careful selection or design of transformation modules, we implement the channel-wise transformation as a non-linear MLP with one hidden layer, which has been demonstrated to have universal approximation capabilities~\cite{cybenko1989approximation}. Together with this simple channel-wise transformation module and the conventional $L2$-distance loss, we propose a very simple and generic method for feature-based distillation. 
     With only one tun-able hyper-parameter, our method is easy to apply to different tasks.
     
     Our extensive evaluation, as shown in Table~\ref{table:Comparewithsota}, reveals that our method consistently outperforms existing feature-based distillation methods on dense prediction tasks. In object detection, we observed consistent performance gains over two-stage, anchor-based, and anchor-free single-stage detectors, with an average improvement of +3.5\% in bbox mAP across these settings. For semantic segmentation, our method delivered an average improvement of +4.0\% in mIoU over heterogeneous and homogeneous distillation settings on the ResNet-18-based PSPNet. Our method also achieves strong performance on the classification task, with an average increase of +2.4\% in Top-1 accuracy, regardless of whether the number of channels in the student and teacher feature maps are the same or not. 
     \input{comparison_with_sota}

To sum up, our main contributions are three-folds:

\begin{itemize}
    
    \item We reinstate the importance of channel-wise transformation for aligning the student's and teacher's features in feature-based knowledge distillation.
    \item We propose a simple and generic framework for feature-based knowledge distillation which uses MLP as the channel-wise transformation module to help student learn more powerful features.
    \item We achieve state-of-the-art distillation results for multiple dense prediction tasks, and comparable state-of-the-art results for classification task.
\end{itemize}


\label{sec:intro}
