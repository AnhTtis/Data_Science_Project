
\begin{figure*}
    \centering
     \begin{subfigure}[b]{0.35\textwidth}
         \centering
         \includegraphics[width=0.67\textwidth]{NL.pdf}
         \caption{Non-Local Block used in FKD~\cite{zhang2020fkd}}
         \label{fig:transform_nl}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.35\textwidth}
         \centering
         \includegraphics[width=0.7\textwidth]{GCBlock.pdf}
         \caption{Global Context Block used in FGD~\cite{yang2022fgd}}
         \label{fig:transform_gc}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.2\textwidth}
         \centering
         \includegraphics[width=0.57\textwidth]{MLP.pdf}
         \caption{MLP (Ours)}
         \label{fig:transform_mlp}
     \end{subfigure}
    \caption{Comparison of different transformation modules in knowledge distillation. FKD~\cite{zhang2020fkd} uses a non-local module (a), while FGD~\cite{yang2022fgd} employs GCBlock (b) to model the relationships between pixels in an image. Our method utilizes a simple yet effective channel-wise transformation through an MLP (c), consisting of two 1$\times$1 convolution layers and a ReLU activation layer.}
    \vspace{-2mm}
    \label{fig:transform}
\end{figure*}
