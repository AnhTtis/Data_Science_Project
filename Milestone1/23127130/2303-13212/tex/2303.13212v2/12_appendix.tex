\appendix



\section{Additional Experimental Results}

\subsection{Extended Detection Results}
\input{more_coco}
Consistent with the experimental setting used in \cite{jang2022glamd}, we evaluate the performance of our proposed method with other knowledge distillation methods, utilizing both single-stage (RetinaNet\cite{lin2017retina}) and two-stage detectors (Faster-RCNN~\cite{ren2015faster}). The hyper-parameter $\alpha$ is assigned values of $2\times 10^{-5}$ and $5\times 10^{-7}$ for the single-stage and two-stage detectors, respectively. The results of this evaluation are detailed in Table~\ref{table:more_coco_results}, which demonstrates that our method outperforms all previous approaches in terms of distillation performance. Notably, our method surpasses both FRS~\cite{zhixing2021frs} and GLAMD~\cite{jang2022glamd}, state-of-the-art distillation methods specifically tailored for object detection tasks. These empirical results demonstrate the simplicity and efficacy of our proposed method.


\subsection{Alternative Loss Functions}
\input{loss}
In line with the majority of feature-based knowledge distillation approaches, our method employs the $L2$-distance as its loss function. In this subsection, we investigate the impact of various loss functions within the context of the PspNet-Res101 distilling DeepLabV3-Res18 setting applied to the CityScapes dataset. The results are presented in Table~\ref{table:loss}. It can be seen that there is no significant gap of using different loss functions, and $L2$-distance show slightly better result.


\subsection{Sensitivity Analysis}
\input{sensitivity}
\input{more_sensitivity}
In this subsection, we examine the sensitivity of the hyper-parameter $\alpha$ for both image classification and object detection tasks.

For the image classification task, we use ResNet-34 as the teacher model and ResNet-18 as the student model, employing the ImageNet dataset~\cite{deng2009imagenet}. We specifically train the model on 1.2 million images from the ImageNet training set and test it on 50,000 images from the validation set, using Top-1 accuracy as the evaluation metric. Our training procedure follows a standard approach, consisting of 100 epochs with learning rate decay at the 30th, 60th, and 90th epochs. We utilize Stochastic Gradient Descent (SGD) as the optimizer, with an initial learning rate of 0.1. The training is conducted on 8 GPUs, each with a batch size of 32 images.

For the object detection task, we employ RetinaNet-ResNext101 (3x training schedule) as the teacher model and RetinaNet-ResNet50 as the student model, evaluating our method on the COCO dataset~\cite{lin2014coco}. We train the model on 120,000 images from the COCO training set and test it on 5,000 images from the validation set, using mean Average Precision (mAP) as the evaluation metric. Our training procedure adheres to a standard 1x schedule, encompassing 12 epochs with learning rate reduction at the 8th and 11th epochs. The optimization is carried out using Stochastic Gradient Descent (SGD), and the model is trained on 8 GPUs, each with a batch size of 2.

As illustrated in Fig.~\ref{fig:more_sensitivity}, our method is not sensitive to $\alpha$, which serves only to balance the overall loss.



\section{Additional Visualizations}
\subsection{Impact of the Channel-wise Transformation Module}
\input{L2_after_MLP}

To further assess the effect of the proposed channel-wise transformation module, we compare our method with FGD~\cite{yang2022fgd} on the detection task. Our method achieves a higher mAP than FGD (42.3 vs. 42.0); however, the student features learned by our method exhibit a larger $L2$-distance with the teacher features compared to the $L2$-distance between the FGD~\cite{yang2022fgd} learned student features and the teacher features. As illustrated in Fig~\ref{fig:l2_after_mlp}, after incorporating the channel-wise transformation module (MLP), the $L2$-distance relative to the teacher features is significantly reduced and becomes lower than the student-teacher feature distance of FGD. These findings suggest that a learnable channel-wise transformation module can act as a semantic translator, facilitating the student model's approximation to the teacher model in an indirect manner.



\subsection{Activation Pattern Visualization}
\input{visual_activation}
In this section, we present an example, depicted in Fig.\ref{fig:visual_activation}, to demonstrate the role of the MLP module as a semantic ``translator''. On the left, we adopt a method akin to that of Yang et al.\cite{yang2022fgd} to calculate the channel-wise attention map, observing the value distribution across different channels. We find that although the student model after distillation exhibits a distinct distribution compared to the teacher model, it becomes more similar to the channel attention map following the implementation of the MLP module. On the right, we display the activation patterns of the teacher feature, distilled student feature, and feature after the MLP module.

\section{Experiment details}
In this section, we present the experiment details in table.\ref{table:DifferentTasks}, 
For the classification task, we use ResNet-34 as the teacher model and ResNet-18 as the student model, conducting feature distillation on the feature map of the last stage of the backbone (both teacher and student feature maps have 512 channels). We employ the ImageNet dataset~\cite{deng2009imagenet}, training the model on 1.2 million images from the ImageNet training set and testing it on 50,000 images from the validation set, using Top-1 accuracy as the evaluation metric. Our training procedure follows a standard approach, consisting of 100 epochs with learning rate decay at the 30th, 60th, and 90th epochs. We utilize Stochastic Gradient Descent (SGD) as the optimizer, with an initial learning rate of 0.1. The training is conducted on 8 GPUs, each with a batch size of 32 images.

For the detection task, we use RetinaNet-ResNext101 as the teacher model and RetinaNet-ResNet50 as the student model. Following FGD~\cite{yang2022fgd}, we conduct distillation on the neck (both teacher and student feature maps have 256 channels). We evaluate our method on the COCO dataset~\cite{lin2014coco}, training the model on 120,000 images from the COCO training set and testing it on 5,000 images from the validation set, using mean Average Precision (mAP) as the evaluation metric. Our training procedure adheres to a standard 1x schedule, encompassing 12 epochs with learning rate reduction at the 8th and 11th epochs. The optimization is carried out using Stochastic Gradient Descent (SGD), and the model is trained on 8 GPUs, each with a batch size of 2.

For the segmentation task, we use PSPNet-ResNet34 as the teacher model and PSPNet-ResNet18 as the student model, conducting feature distillation on the feature map of the last stage of the backbone (both teacher and student feature maps have 512 channels). We evaluate our method using the CityScapes dataset~\cite{cordts2016cityscapes}, conducting experiments on 2,975 training images and 500 validation images, using mean Intersection over Union (mIoU) as the evaluation metric.. The models are trained for 40,000 iterations using the SGD optimizer on 8 GPUs, each with a batch size of 2.

For different Transformation Modules, The "Identity" approach directly mimics features using the l2-distance metric, the "Linear" approach employs a linear conv1x1 transformation, while the "Task-Specific" method utilizes established techniques specifically designed for each task. such as TaT-Cls~\cite{lin2022tat} for classification, which involves using a target-aware transformer. FGD~\cite{yang2022fgd} for object detection, which involves using different attention masks and global context module, and TaT-Seg~\cite{lin2022tat} for semantic segmentation, which involves using patch-group distillation and anchor-point distillation.







