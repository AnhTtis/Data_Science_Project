
\section{Method}


In this section, we first briefly introduce the basic form of intermediate feature-based knowledge distillation, and then present the details of our proposed method.

\subsection{Revisiting Feature-based Knowledge Distillation}
In feature-based knowledge distillation, a student model is generally supervised by a teacher model as~\cite{gou2021knowledge} :
% Generally, the pipeline for feature-based knowledge distillation can be formulated as:
\begin{equation}
L_{feat} = \mathcal{L}_{KD}\left(\mathcal{T}_t\left(\boldsymbol{F}_t\right), \mathcal{T}_s\left(\boldsymbol{F}_s\right)\right),
\end{equation}\label{egu:kd}
% \begin{equation}
% \mathcal{L}_{\text {mimic }}=\frac{1}{H W C}\left\|\boldsymbol{F}^{(t)}-\phi\left(\boldsymbol{F}^{(s)}\right)\right\|_2^2
% \end{equation}
where  $\mathcal{L}_{KD}$ represents the similarity function used to match the feature maps of the teacher model, $\bf{F}_t$, and the student model, $\bf{F}_s$. Furthermore, the transformation functions, $\mathcal{T}_t$ and $\mathcal{T}_s$, will be performed when the feature maps of the teacher and student models are not of the same shape (e.g., a linear projection layer to align the number of channels in $\bf{F}_s$ with those in $\bf{F}_t$).

%Early works in knowledge distillation introduced various types of similarity function $\mathcal{L}_{KD}$, such as Kullbackâ€“Leibler (KL) divergence~\cite{shu2021cwd} and maximum mean discrepancy~\cite{huang2017like}. For example, CWD~\cite{shu2021cwd} uses KL divergence between the channel-wise probability maps of the student and teacher models as the similarity function $\mathcal{L}_{KD}$. On the other hand,
%recent works introduce complex transformation ($\mathcal{T}_t$ and $\mathcal{T}_s$) to guide and help students for learning knowledge (aligning feature) from teachers. For example, as shown in Figure~\ref{fig:fkd}, FKD~\cite{zhang2020fkd} and FGD~\cite{yang2022fgd} exploit (1) specific module (i.e., Non-local module~\cite{wang2018non} or GCBlock~\cite{cao2019gcnet}) and (2) channel and spatial attention masks applied to both the teacher and student features. 
%Recent works utilize complex transformations ($\mathcal{T}_t$ and $\mathcal{T}_s$) to assist students in learning knowledge (feature alignment) from teachers. For instance, as depicted in Figure~\ref{fig:fkd}, FKD~\cite{zhang2020fkd} and FGD~\cite{yang2022fgd} employ (1) specific modules (\textit{i.e}., Non-local module~\cite{wang2018non} or GCBlock~\cite{cao2019gcnet}) and (2) channel-wise and spatial-wise attention masks to the teacher and student features.
%This raises a question: 
%Do student networks really need the assistance of well-designed modules to learn better features from the teacher?
Recently, several works have employed complex transformations ($\mathcal{T}_t$ and $\mathcal{T}_s$) to facilitate the acquisition of knowledge (feature alignment) by student networks from teacher networks. For example, as depicted in Figure~\ref{fig:fkd}, both FKD~\cite{zhang2020fkd} and FGD~\cite{yang2022fgd} utilize (1) specific modules, such as the Non-local module~\cite{wang2018non} or GCBlock~\cite{cao2019gcnet}, and (2) channel-wise and spatial-wise attention masks to align the features of the teacher and student networks.
This raises the question of whether the use of well-designed modules is necessary for student networks to learn more effective features from the teacher.
\input{transform_module}

In this paper, we perform empirical studies to address the question raised above and find that student models can enhance their representations through a straightforward nonlinear channel-wise transformation. Based on this finding, as illustrated in Figure~\ref{fig:sgkd}, we introduce a simple method that incorporates a Multi-Layer Perceptron (MLP) into the student features and aligns the transformed student features with the teacher features using a conventional $L_2$ distance. The specifics of our proposed method are outlined in the subsequent subsection.
% specifically, we propose the ConvMixer, an extremely simple model that is similar in spirit to the ViT and the even-more-basic MLP-Mixer in that it operates directly on patches as input, separates the mixing of spatial and channel dimensions, and maintains equal size and resolution throughout the network. In
%  (e.g., global context block~\cite{}, channel and spatial attention~\cite{} and task-specific mask~\cite{}) 


% Suppose $F_T \in \mathbb{R}^{C \times H \times W}$ is the feature map of the teacher model, $F_S \in \mathbb{R}^{C \times H \times W}$ from the student model. Then, without changing its content, we can reshape $F_S$,$F_T$ into 2D metrices using 
% \begin{equation}
%     f_s = Flatten(F_S)\in \mathbb{R}^{N \times C}
% \end{equation}
% \begin{equation}
%   f_t = Flatten(F_T)\in \mathbb{R}^{N \times C} 
% \end{equation}
% $N=H \times W$ and $Flatten(\cdot)$ is a function that tiles a three-dimensional feature tensor into a two-dimensional matrix, where each row of the matrix is associated with a pixel in the feature tensor in spatial order. Then $f_s$ and $f_t$ can be described as
% \begin{equation}
%     \begin{aligned}
%         f_s = [f_{s1},f_{s2},\dots,f_{tN}] \\
%         f_t = [f_{t1},f_{t2},\dots,f_{tN}]
%     \end{aligned}
% \end{equation}

% The initial work can be seen as reducing the distance between set $f_s$ and $f_t$ in a one-to-one manner and can be written as:
% \begin{equation}
%      L_{feat} = \sum_{i}^{N}criterion(f_{i}^{S},f_{i}^{T})
% \end{equation}
% Here, $criterion$ a is the function used to measure the difference between two features, often chosen as KL divergence, L2 distance, etc.

% This approach assumes that each part of the feature should receive the same degree of guidance. And for different tasks, different regions do not necessarily have the same level of importance, directly treating all regions equally leads to sub-optimal results, especially on tasks that vary greatly in importance between regions such as object detection and semantic segmentation.
%     \begin{equation}
%      L_{feat} = \sum_{i}^{N}criterion(f_{i}^{S},f_{i}^{T})
%     \end{equation}
% \subsection{Task specific knowledge distillation}
% To address the above issues, previous work has designed different knowledge distillation region selection strategies in terms of task-specific inductive bias, and can be written as:
%     \begin{equation}
%         L_{feat} = criterion(\phi_S(f^{S}),\phi_T(f^{T}))
%     \end{equation}
% Here $\phi_S(\cdot),\phi_T(\cdot)$ are the region selection strategies for student and teacher responses, respectively, and these strategies often differ depending on the task, e.g., in the object detection task, $\phi_S$ can be finding a foreground-background separation function, and in the semantic segmentation task, $\phi_S$ can be seen as a function for separating different classes of pixels. Usually $\phi_T$ is the same function as $\phi_S$ or just a identical function $\phi_T(X) = X$. Most hand-designed functions can be seen as a kind of adjustment of weights or extraction ofindingnsional information from the feature map, while the former function has difficulty in capturing higher-order information of the feature map, such as global information, and the latter method tends to cut the feature dimension and lose some information. 



\subsection{Learnable channel-wise transformation}
%\input{figs/method_illustration}
Instead of using complex transformation on both spatial and channel dimension, we propose to use a learnable nonlinear channel-wise transformation to align the feature maps of the student and the teacher model. In detail, we use a non-linear MLP with one hidden layer for the student (\textit{i.e.}, $\mathcal{T}_t = identity$ and $\mathcal{T}_s = MLP$ in Equ.~\ref{egu:kd}):

\begin{equation}
\operatorname{MLP}(F)=W_{2}\left(\sigma \left(W_{1}(F)\right)\right),
\end{equation}


where $W1$ and $W2$ are learnable parameters implemented as 1$\times$1 convolutions, and $\sigma$ represents ReLU activation. As illustrated in Figure~\ref{fig:transform}, our transformation module (Figure~\ref{fig:transform_mlp}) is much simpler than the methods proposed in  FKD~\cite{zhang2020fkd} and  FGD~\cite{yang2022fgd}.

Without bells and whistles, we choose $L_2$ distance for supervising transformed student feature and teacher feature. Specifically, the feature distillation loss is formulated as:
     \begin{equation}
        L_{feat} = \sum_{i}^{N}(MLP(\boldsymbol{F}_s ) - \boldsymbol{F}_t)^2.
    \end{equation}

\input{algorithm}
Our approach is straightforward and can be efficiently executed using prevalent machine learning libraries, such as PyTorch, as shown in Algorithm~\ref{alg:code}. The ease of implementation enables us to leverage existing infrastructure, facilitating the training process of our model.

\subsection{Overall loss}
Our method can be easily used in various tasks. Combined with task-specific losses, the overall loss can be formulated as:
\begin{equation}
     L_{total} = L_{task} + \alpha L_{feat}
\end{equation}
where $\alpha$ is a hyper-parameter to balance the weight of knowledge distillation loss.

\label{sec:method}

