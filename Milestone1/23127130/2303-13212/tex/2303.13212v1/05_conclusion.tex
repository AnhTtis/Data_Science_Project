\section{Conclusion}

In this paper, we first present a novel discovery that aligning the feature maps between teacher and student along the channel-wise dimension is also effective for addressing the feature misalignment issue in feature-based knowledge distillation. Then, we exploit a Multi-Layer Perceptron (MLP) as the channel-wise transformation module to align the features of the student and the teacher model. Further, we propose a simple and generic framework for feature distillation based on it, with only one hyper-parameter to balance the distillation loss and the task specific loss. Extensive experimental are conducted and the results demonstrate that the proposed method can achieves significant performance improvements in various computer vision tasks including image classification, object detection, instance segmentation, and semantic segmentation, even outperforming the state-of-the-art feature-based knowledge distillation methods in some tasks.

\label{sec:conclusion}

