
\section{Related Work}
    The concept of knowledge distillation was first proposed by Hinton et al. ~\cite{hinton2015distilling}, with the goal of transferring dark knowledge from a cumbersome teacher model to a smaller student model to improve the student's performance. Based on the types of dark knowledge, mainstream knowledge distillation methods can be divided into two categories: Logits-based knowledge distillation and feature-based knowledge distillation.
\subsection{Logits-based knowledge distillation}
    Classical logits-based knowledge distillation methods~\cite{hinton2015distilling} minimize the KL divergence between the output logits of teacher and student models. One recent line of research focuses on refining the vanilla knowledge distillation loss function to better leverage the logits information. WSLD~\cite{zhou2020wsld} rethinks the knowledge distillation process from a bias-variance trade-off perspective and proposes weighted soft labels for knowledge distillation. DKD~\cite{zhao2022dkd}, reformulates the classical knowledge distillation loss into the target and non-target part and calculates the distillation loss separately. While these works have improved the performance of logits-based knowledge distillation methods on classification tasks, they have often not achieved significant results on other tasks, such as dense prediction tasks.
%Head-related:   
    %However, this distillation manner is related to the classification head, for more sophisticated tasks such as object detection which involves the classification task and the regression task, Logits-based methods often fail to achieve ideal performance due to the task-specific property such as the imbalanced distribution of pixels between fore-groud and background.
%Trial on different tasks: 
%Recently, there are some works that try to use the logit-based knowledge distillation methods on other tasks, which often consider a reformulation of traditional Knowledge distillation loss.
    Another line of work involves modeling other tasks into a classification task, and adopting the logits-based knowledge distillation on other tasks.
        LD\cite{zheng2022ld}, reformulates the output form of the regression head to a probability distribution and applies classical knowledge distillation to the regression task. However, it is only for object detection tasks and requires changes to the detection head. RMKD\cite{li2022rm} reformulates the ordering between anchors into the form of the probability distribution for knowledge transfer and applies classical knowledge distillation to the regression task. However, it is only limited to anchor-based detectors. 
\subsection{Feature-based knowledge distillation}
% \paragraph{Feature-based knowledge distillation for classification}



\paragraph{For classification.}
The feature of the teacher model is another kind of dark knowledge and was first used in~\cite{romero2014fitnets}. Subsequent works have primarily focused on finding more effective ways to utilize this type of dark knowledge.
    AT\cite{kzagoruyko2016at} extracts attention maps to help the student to pay attention to the import regions. However, it squeezes the channel dimension of the feature map and fails to utilize the channel information, resulting in limited improvements to the student models
    OFD\cite{heo2019overhaul} designs a new loss function and used marginal ReLU to extract the major information in the network. 
    CRD\cite{tian2019crd} incorporates the idea of contrastive learning into knowledge distillation, and although it has achieved relatively good performance, However, its training cost is high due to the use of memory banks for a large amount of negative samples.
    KR\cite{reviewkd} proposes conducting knowledge distillation on multi-level features in a review manner,resulting in good performance, especially on classification tasks.
    TaT~\cite{lin2022tat} propose a novel one-to-all spatial matching approach for knowledge distillation based on similarity generated from a target-aware transformer.
% \paragraph{Feature-based knowledge distillation for dense prediction}


\paragraph{For objection detection.}
% For objection task:
    Object detection is a significantly more complex task than image classification. The extreme imbalance between foreground and background pixels poses a major challenge for object detection. To address this issue, many knowledge distillation methods attempt to have the student model imitate the key regions of the teacher model.
    FGFI\cite{wang2019fgfi} leverages fine-grained masks to force students to focus on foreground regions.
    GID\cite{dai2021GID} identifies regions where the student and teacher models perform differently as the key regions for distillation, without relying on anchors.
    Defeat\cite{guo2021defeat} finds that the background region also contains valuable information and proposes distilling foreground and background regions separately.
    Recent methods have also discovered that the relationships between different pixels are important knowledge for distillation and propose various global modules to address this problem. 
    FKD\cite{zhang2020fkd} employs attention masks to direct the student model's focus on key regions and non-local modules to capture the relationships between different pixels, resulting in improved knowledge distillation.
    FGD\cite{yang2022fgd} proposes focal and global distillation mechanisms, forcing the student model to learn the teacher's crucial region and global information through a global context block~\cite{cao2019gcnet}


\vspace{-4mm}
\paragraph{For semantic segmentation.}
\input{arch}
% For semantic segmentation:
    Semantic segmentation is a per-pixel prediction problem, and strictly aligning the feature maps between the student and teacher models may impose overly strict constraints and lead to sub-optimal results.~\cite{shu2021cwd}. Recent works~\cite{liu2019skds,wang2020ifvd} try to force the student to learn the correlations among different spatial regions.
    IFVD\cite{wang2020ifvd} focuses on the intra-class feature variation among pixels with the same label and designs an IFV module to transfer the structural knowledge. 
    SKDS~\cite{liu2019skds} combines pixel-wise distillation, pair-wise distillation, and holistic distillation using a GAN-based approach to align the output maps of teacher and student models.
    CIRKD\cite{yang2022cirkd} aims to model the pixel-to-pixel and pixel-to-region relationships as supervisory signals for knowledge distillation in the semantic segmentation task.
    CWD~\cite{shu2021cwd} derives probability maps by normalizing the activation maps of each channel of intermediate features, and minimizes the KL divergence between these probability maps, applying the method to dense prediction tasks, including object detection and semantic segmentation. 
\vspace{-4mm}
\paragraph{For general tasks.}
% General:
MGD~\cite{yang2022mgd} employs a generative approach that involves the use of  random masks that randomly to erases a portion of the student's feature map and then forces it to generate features similar to the teacher's through an adversarial generator and applies it to classification, detection, and segmentation tasks.  

In this paper, we focus on channel-wise transformations, and propose a simple and generic method for feature-based knowledge distillation. 
\label{sec:related}


