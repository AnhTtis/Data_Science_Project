\section{Experiment}
Our approach, which is a feature-based method, is easy to implement on various models and tasks. In this paper, we conduct experiment on image classification, object detection, instance segmentation, and semantic segmentation, to demonstrate the simplicity, effectiveness, and generality of our method.
\subsection{Image Classification on ImageNet}
\input{imgnet_classification_results}

\paragraph{Settings}
	To evaluate our method on the image classification task, we use the Imagenet\cite{deng2009imagenet} dataset. Specifically, we use 1.2 million images from the Imagenet training set to train the model and 50,000 images from the validation set to test the model and use Top-1 accuracy as the evaluation metric. We use a standard training procedure, which involves training the model for 100 epochs, with a learning rate decay at the 30th, 60th, and 90th epochs, The optimizer used for training is SGD, and the initial learning rate is set to 0.1. The training is performed on 8 GPUs with a batch size of 32 images per GPU.
% 	We select two sets of models setting a is resnet50 as the teacher model and MobileNet as the student model, setting b is resnet34 as the teacher model and resnet18 as the student model, both setting a and setting b is on the output of the feature from the last stage of model backbone Calculate distillation loss, distillation loss weight $\alpha$ is set to $7\times10^{-5}$.

    We evaluate our method on both homogeneous and heterogeneous distillation settings. Specifically, we test our method on two model configurations: a) ResNet34 as the teacher model and ResNet18 as the student model, and b) ResNet50 as the teacher model and MobileNet as the student model. In both configurations, we use the feature maps from the last stage of the backbone to calculate the distillation loss, and set the distillation loss weight $\alpha$ to $7\times10^{-5}$. Moreover, our method is compared with not only single layer feature-based methods\cite{park2019rkd,tian2019crd,yang2022mgd} but also other state-of-the-art logits-based methods\cite{hinton2015distilling,zhao2022dkd} and multi-layer feature-based methods\cite{kzagoruyko2016at,heo2019overhaul,reviewkd}.
% 	We compare our method not only to single-feature-based methods but also to other state-of-the-art methods. 
% 	For a fair comparison, we only compare the methods that train under the same settings.
	
\paragraph{Comparison to baseline.} 
As presented in Table~\ref{table:classification results}, our method demonstrates its effectiveness on the image classification task. Specifically, under the homogeneous setting, the Top-1 accuracy of ResNet-18 is improved by +3.28\%. Similarly, under the heterogeneous setting, the Top-1 accuracy of MobileNet is improved by +1.61\%. These results highlight the superiority of our method in comparison to the baseline models.

\paragraph{Comparison to single feature distillation methods}
    Our method outperforms all single-feature distillation methods\cite{park2019rkd,tian2019crd,yang2022mgd} in the heterogeneous setting and is on par with the state-of-the-art method MGD\cite{yang2022mgd}. These results highlight the effectiveness of our method in comparison to other single-feature distillation techniques.
\paragraph{Comparison to previous state-of-the-art}
    Compared to the previous state-of-the-art method KR~\cite{reviewkd}, which uses multi-stage features, our simple method achieves comparable performance with a difference of less than 0.1\% in terms of Top-1 accuracy on both homogeneous and heterogeneous settings.
\label{sec:cls_experiment}
% \subsection{Object Detection and Instance Segmentation}
\subsection{Object Detection on COCO}
\paragraph{Settings} For the object detection task, we evaluate our method on the COCO\cite{lin2014coco} dataset. Specifically, we use 120,000 images from the COCO training set for model training and 5,000 images from the validation set for model testing, with mAP as the evaluation metric. Our training procedure follows a standard 2x schedule, consisting of 24 training epochs, with the reduction of the learning rate at epochs 16 and 22. The optimization process is performed using Stochastic Gradient Descent (SGD) and the model is trained on 8 GPUs, each with a batch size of 2.

We experiment on multiple detector architectures, including two-stage, single-stage anchor-based, and single-stage anchor-free detectors. The distillation loss is computed on all feature maps output from the neck, and the distillation loss weight $\alpha$ is set to $5\times10^{-7}$ for the two-stage detector and $2\times10^{-5}$ for the one-stage detector. For the instance segmentation task, we use a ResNext-101-based Cascade Mask R-CNN as the teacher model and a ResNet-50-based Mask R-CNN as the student model. The experimental configuration follows that of the two-stage detector distillation.
\input{cityscapes_segmentation_results}
\paragraph{Object Detection}
\input{coco_detection_results}  We compare our method with previous state-of-the-art methods designed for object detection~\cite{zhang2020fkd,yang2022fgd} and a recent generic distillation method~\cite{yang2022mgd}. As shown in Table~\ref{table:more results}, our simple method can achieve competitive results. For example, on the two-stage detector Faster RCNN-ResNet50, we get the mAP of the student model to rise from 38.4 to 42.3, surpassing the previous state-of-the-art method. On the anchor-based single-stage detector RetinaNet-ResNet50 and the anchor-free single-stage detector Reppoints-ResNet50, we also achieve mAP increases of 3.6 and 3.4, respectively, which are comparable to the results of the state-of-the-art method.

\paragraph{Instance Segmentation}
Our method demonstrates its effectiveness on the instance segmentation task, as shown in Table~\ref{table:more results}. The results show that our simple approach leads to +3.2\% improvement in bounding box AP and +2.4\% improvement in mask AP, respectively, outperforming state-of-the-art methods. 
\label{sec:det_experiment}

\subsection{Semantic segmentation on CityScapes}
\paragraph{Settings} For the semantic segmentation task, we evaluate our method with the CityScapes dataset\cite{cordts2016cityscapes}. Specifically, our experiments are conducted on 2975 training images and 500 validation images, and the evaluation metric is mIoU. The models are trained for 40,000 iterations using the SGD optimizer on 8 GPUs with a batch size of 2. 

We conduct experiments on two model configurations: a) a homogeneous setting with PSPNet-Res101 as the teacher model and PSPNet-Res18 as the student model, and b) a heterogeneous setting with PSPNet-Res101 as the teacher model and DeepLabv3-Res18 as the student model. The input size for both configurations is set to $512\times512$, and the distillation loss is computed from the features of the last stage of the model neck. The distillation loss weight $\alpha$ for the homogeneous set is set to $2\times10^{-5}$, and for the heterogeneous set is $\alpha$ to $1\times10^{-5}$.



\paragraph{Results} 

As shown in Table~\ref{table:segmentation results}, our method achieves remarkable results in both homogeneous and heterogeneous configurations. Specifically, the ResNet-18-based PspNet model obtains a mIoU increase of +4.66\% under the homogeneous setting, and the ResNet-18-based deeplabv3 model obtains a mIoU increase of +3.28\% under the heterogeneous setting. Furthermore, when compared to the state-of-the-art method MGD\cite{yang2022mgd}, our method achieves an improvement of +0.88\% mIoU and +0.53\% mIoU on the homogeneous and heterogeneous settings, respectively. These results demonstrate the effectiveness of our method on the semantic segmentation task.
\label{sec:seg_experiment}
\subsection{Ablation Studies and Analysis}


\subsubsection{Benefits of Channel-wise Transformation}

As shown in Table~\ref{table:DifferentTasks}, directly using the features from the teacher and the student without channel-wise transformation can result in significant distillation performance drops in the semantic segmentation task.

To better understand this phenomenon, we calculate the $L_2$-distance between the student feature map and the teacher feature map on the validation dataset. 

The results in Table~\ref{table:L2Dis} show that directly mimicking the teacher feature (corresponding to ‘Identity’ transformation) can achieve a lower $L_2$-distance to the teacher, but obtain significantly poorer performance compared to those using channel-wise transformations. Compared with it, channel-wise transformation methods can obtain an even lower $L_2$-distance after the channel-wise transformation, but the $L_2$-distance before the channel-wise transformation is much larger. Moreover, the distillation performance of the channel-wise transformation methods is much better than directly mimicking.

In the process of distillation, the student model is supervised by two signals: distillation losses and task-specific losses. We conjecture that the limited capacity of the student model makes it difficult to fully capture the knowledge of the teacher, and applying strict distillation constraints (\textit{i.e}., directly mimicking the teacher feature) may over-optimize the student feature with the distillation supervision and prevent them from being trained with the task-specific supervision, leading to performance degradation. On the contrary, our method exploits the channel-wise transformation module to achieve a better balance between the task-specific supervision and the distillation supervision.
%better balancing the student features with the distillation supervision.
\input{l2_dis_with_teacher}

\subsubsection{Ablation of Transformation Modules}

In this section, we further demonstrate the importance of the channel-wise transformation module in cases where the size of the teacher's feature and the student's feature are not equivalent, \textit{i.e.}, when the number of channels is unequal. We show this by performing distillation of PspNet-Res101 onto DeepLabV3-Res18 on the CityScapes dataset. 

As shown in Table~\ref{table:Transform}, the student model only achieves minimal improvement when there is only a single linear layer without non-linear activation (\textit{i.e.}, ReLU). These results demonstrate that the non-linear transformation plays an important role in improving the student's representation ability.  Compared with MLP, additional local spatial transformation with non-linear activation (implemented with Conv3$\times$3-ReLU-Conv3$\times$3) achieves worse performance. Besides, global spatial transformation with Non-Local block~\cite{wang2018non}  achieves the lowest mIoU. 

These results show that the transformation of spatial dimension does not bring additional gain to our method. We conjecture that an overly complex and powerful learnable transformation will make the distillation process concentrate on optimizing the transformation module rather than the student network itself.
\input{transform}








\subsubsection{Location of MLP}
Previous research on feature-based distillation techniques, such as FKD~\cite{zhang2020fkd} and FGD~\cite{yang2022fgd}, have employed complex masks in their transformation modules to transform both the student and teacher features. In contrast, our method utilizes a learnable Multi-Layer Perceptron (MLP) for feature transformation. 

 The advantage of using a learnable MLP is that it can guide the student to better learn the representations from the teacher. However, applying the transformation to both the student and teacher features can quickly lead to a trivial solution where the distillation loss approaches zero, as demonstrated in Fig.~\ref{fig:loss_comp}. This invalidates the effectiveness of feature distillation. To avoid this problem, our method only applies the transformation to the student feature.

 
\input{loss_comparison}


\label{sec:experiment}