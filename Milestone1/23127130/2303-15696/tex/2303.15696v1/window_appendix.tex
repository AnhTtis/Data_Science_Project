The window factors, $\bar{w}^4_\mathrm{ovl}$ and $\bar{w}^4$, used in Sec.~\ref{sec:postproc} are defined as in Eqs.~(34) and (24) in~\cite{lazz_romano_windowing_note}. They are used to correct for the effect windowing has on our estimate of the variances. Actually, these corrections should include contributions from the autocorrelation function (PSD) of the individual detectors or their cross-correlation (see, e.g. Eqs.~(22) and (32) of the same note). However, if the frequency response of the window is sufficiently strongly peaked around zero, then we can treat the transformed windows as delta functions~\cite{whelan_CC_dcc} and our expressions for these quantities reduce to
\begin{align}
    \bar{w}^4 = \frac{1}{N}\sum_{i=1}^Nw_i^4,
\end{align}
where $w_i$ represents the $i^{\textrm{th}}$ sample of the Hann window we use. Likewise, we need to account for the covariance between point estimates calculated in adjacent time segments. The point estimates are each quadratic in the data, windowed, and use 50\% overlapping segments of data, and so we must account for the overlapping of the windows applied to the two segments
\begin{align}
   \bar{w}_{\textrm{ovl}}^4 = \frac{1}{N/2}\sum_{i=N/2+1}^N w_i^2 w_{i-N/2}^2,
\end{align}
where we see this now as the cross-correlation of the pieces of the two windows that overlap for the two segments.

When calculating the variance of our point estimate, we must estimate the quantity 
$\left(P_{1, f} P_{2, f}\right)^{-1}$, which is the expression that appears in the Gaussian likelihood used to construct our optimal estimators~\cite{Matas:2020roi}, and is therefore the relevant quantity when considering the variance of the point estimates. We briefly summarize how to properly estimate this quantity based on the discussion in Appendix B of ~\cite{Matas:2020roi}, noting that they do not consider the effect of windowing, which we also discuss below. 

For a segment of length $T$ we calculate estimators for the PSDs, $\hat P_{I,f}$, where $I=1,2$ labels the detector, using Welch's method~\cite{welch_method_and_window_factor}. We break our time segment $T$ into 50\% overlapping chunks, calculate the PSD in each chunk, and average those estimates together. If we want a PSD with frequency resolution $\Delta f$ then we have $K$ overlapping segments where $K=2T\Delta f -1$. We can assume our (noisy) estimators for the individual PSDs are unbiased and can be written as the true PSD plus some small deviation, $\hat P_{1,f} = P_{1,f} + \delta P_{1,f}$. We now look at the quantity of interest in calculating our variance
\begin{align}
   \frac{1}{\hat P_{1,f}\hat P_{2,f}} =&\frac{1}{\left[P_{1,f} + \delta P_{1,f}\right]\left[P_{2,f} + \delta P_{2,f}\right]}.
   \end{align}
   We can expand the denominator, take the expectation value of both sides, and use the fact that $\langle\delta P_{I,f}\rangle=0$ and $\langle\delta P_{I,f}^2\rangle = \textrm{var} P_{I,f}$, where $I=1,2$ labels the detector. This gives us
   \begin{align}
    \Braket{\frac{1}{\hat P_{1, f}\hat P_{2, f}}}\approx&\frac{1}{P_{1, f}  P_{2, f}} \left(1 + \frac{\textrm{var}P_{1, f}}{ P_{1, f}^2} + \frac{\textrm{var}P_{2, f}}{ P_{2, f}^2} + \cdots\right)\\
    =&\frac{1}{ P_{1, f}  P_{2, f}}\left(
    1 + \frac{2\kappa}{K} \right).
\end{align}
This expression can be compared to Eq. (B8) in~\cite{Matas:2020roi}, noting that we have an extra term in the variance of our PSDs, $\kappa$. This term reduces the ``effective'' number of averages we perform due to our windowing, where we apply a Hann window with amplitude $\{w_i\}$ at each sample $i$, as well as the overlapping of our chunks of data. The correction factor is given by~\cite{welch_method_and_window_factor}
\begin{align}
   \kappa = \left[1 + 2\left(\frac{\sum_{i=N/2+1}^N w_i w_{i-N/2}}{\sum_{i=1}^Nw_i^2}\right)^2\frac{K-1}{K}\right].
\end{align}
In practice, we ignore the term $(K-1)/K$, as it leads to extra corrections that are $\mathcal {O}(K^{-2})$ that are quite small.

We can now define a bias correction factor based on the windowing we choose and the number of averages used in constructing $\hat P_{I,f}$. 
Defining $N_{\textrm{eff}} = \kappa^{-1}K$, we have
\begin{align}
\hat\sigma^{-2}(f) = \left(1 + \frac{2}{N_\textrm{eff}}\right) \sigma^{-2}(f),
\end{align}
where we have used simplified notation again where the hat indicates our estimator for Eq.~(\ref{eq:Variance}) and the unhatted indicates the true value.

Taking the square root of both sides and inverting it gives us
\begin{align}
    \sigma = b(N_\textrm{eff})\hat\sigma,
\end{align}
where the bias factor, $b(N_{\textrm{eff}})$, is given by
\begin{align}
    b(N_{\textrm{eff}}) = \frac{N_{\textrm{eff}}}{N_{\textrm{eff}}-1},
\end{align}
assuming $N_{\textrm{eff}}$ is large. %
In Sec.~\ref{Sec:DeltaSigma}, two different bias factors are discussed. In one case, the ``naive'' $\sigma$ is estimated using one segment of length $T$, which results in fewer effective averages, and a larger bias correction than our typical estimate of $\sigma$ which uses two adjacent segments of length $T$ and there twice as many averages.