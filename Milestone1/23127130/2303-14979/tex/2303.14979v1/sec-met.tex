\section{Preliminaries} \label{sec:background}

In this section, we give a brief review of dense retrieval and then present how to apply models to multilingual dense retrieval.

\paratitle{Overview.} Given a query $q$ and a corpus $C$, the retrieval task aims to find the relevant passages for the query from a large corpus. Usually, a dense retrieval model employs two dense encoders~(\ie BERT) $E_Q(\cdot)$ and $E_P(\cdot)$. They encode queries and passages into dense embeddings, respectively. Then, the model uses a similarity function, often dot-product, to perform retrieval:
\begin{equation}
    f(q, p) = E_Q(q) \cdot E_P(p),
\end{equation}
where $f$ denotes the similarity function, $q$ and $p$ denote the query and the passage, respectively. During the inference stage, we apply the passage encoder $E_P(\cdot)$ to all the passages and index them using FAISS~\cite{johnson2019billion} which is an extremely efficient, open-source library for similarity search. Then given a query $q$, we derive its embedding by $\bm{v}_q = E_Q(q)$ and retrieve the top $k$ passages with embeddings closest to $\bm{v}_q$.

\paratitle{Training.} The training of retrieval is metric learning essentially. The goal is to narrow the distance between the query and the relevant passages~(\aka positive passages) and widen the distance between the query and the irrelevant passages~(\aka negative passages). Let $\{q_i, p^{+}_{i}, p^{-}_{i,0}, p^{-}_{i,1}, \dots, p^{-}_{i,n}\}$ be the $i$-th training sample. It consists of one query, one positive passage, and $n$ negative passages. Then we can employ a contrastive loss function, called InfoNCE~\cite{oord2018representation}, to optimize the model:
\begin{equation} 
    \mathcal{L} = - \log \frac{e^{f(q_i, p^{+}_{i})}}{e^{f(q_i, p^{+}_{i})} + \sum_{j=0}^{n} e^{f(q_i, p^{-}_{i,j})}}.
\end{equation}

In practice, we cannot use all passages in the corpus $C$ as negative passages due to the limitation of resources. Therefore, a common practice is sampling a subset from the corpus $C$ as negative samples, and many studies focus on which distribution the negative passages sampled from is better~\cite{xiong2020approximate,qu2021rocketqa}.

\paratitle{Multilingual Setting.} This setting aims to transfer knowledge from the source language to the target languages. In this setting, only labeled data from the source language is available. And the trained model will be directly evaluated on target languages. Note that the setting is different to \emph{cross-lingual retrieval} whose queries and passages are in different languages. In this setting, the queries and the passages are in the same language and just training data from the source language~(\eg English) is available.

\section{Methodology}

In this section, we present the proposed \name. The overview is presented in Figure~\ref{fig:overview}. We first present the augmentation method which combines sparse and dense retrievers. Then, we present how to use the mined data to train the query generator, generate new data, filter the generated samples, and fine-tune the dense retriever. Finally, we summarize the full training process.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/overview.pdf}
    \caption{Overview of the training process.}
    \label{fig:overview}
\end{figure}

\subsection{Lexicon-enhanced Retrieval Module}

In target languages, we do not have the labeled training data, but we have unlabeled queries and passages. To effectively utilize the unlabeled queries and passages, we design a mining method shown in Algorithm~\ref{alg:pm}.

This augmentation is based on the intuition that the sparse retriever and the dense retriever solve different problems and they are complementary. Specifically, the sparse retriever depends on word match, it is more effective than the dense retriever for words that do not appear in the training set. On the contrary, the dense retriever depends on neural networks, it is more effective than the sparse retriever for synonyms and semantics of the sentence. 
As a result, for a passage, if both of them regard it are relevant to the query, we then regard the passage as a positive passage. If one of them regards it as relevant but the other regards it as irrelevant, we regard the passage does not meet all conditions~(\ie keyword match and semantic match), and the passage is a hard case. 
Although we cannot judge whether it is a negative passage, we think its relevance is weaker than the positive passage. As a result, we hope the score of the positive passage is higher than the hard case and then we regard the hard case as a hard negative passage.

In practice, because the score distributions of sparse retrievers and dense retrievers are different, we use the ranking position to measure the relevance between passages and queries. Then, we present our method as follows:
\begin{enumerate}[label=(\arabic*)]
    \item We introduce two parameters to define relevant and irrelevant passages: $S$ and $L$, \ie for a query, the retriever retrieves passages and ranks them with scores, if the ranking position of a passage is less than $S$, we regard the passage is relevant to the query and if the ranking position is greater than $L$, we regard the passage is irrelevant to the query.
    \item We retrieve $L$ and $S$ passages by both sparse and dense retrievers, respectively. We define the top-$L$ passage set as $\mathbb{L}$ and the top-$S$ passage set as $\mathbb{S}$, and use subscript $s$ and $d$ to denote sparse and dense retrievers, respectively. 
    \item We traverse all passages in the corpus. For each passage, if it exists in both $\mathbb{S}_s$ and $\mathbb{S}_d$, we add it to the positive passage set; if it exists in one $\mathbb{S}$ but does not exist in another $\mathbb{L}$~(\aka exists in $\mathbb{S}_s$ but not exists in $\mathbb{L}_d$ or exists in $\mathbb{S}_d$ but does not exist in $\mathbb{L}_s$), we add it to the negative passage set. 
    \item For each mined sample, we add random negative passages like DPR~\cite{karpukhin2020dense}: 1) random passages from the corpus; 2) positive passages of other queries~(\aka in-batch negative). And we use our mined negative passage set to replace ``top passages returned by BM25'' in DPR.
\end{enumerate}

\begin{algorithm}[t] \small
    \caption{Lexicon-enhanced Retrieval Module.}
    \label{alg:pm} 
    \LinesNumbered
    \KwIn{One query $q$ and candidate passages $P$.}
    \KwOut{Positive passages and Negative passages.}
    
    Set $L$ and $S$\;
    $\mathbb{L}_s ~\&~ \mathbb{S}_s \leftarrow$ Top-L and Top-S of sparse retriever\;
    $\mathbb{L}_d ~\&~ \mathbb{S}_d \leftarrow$ Top-L and Top-S of dense retriever\;
    $P^{+} \leftarrow \varnothing$; $P^{-} \leftarrow \varnothing$\;
    \For{$p \in P$}{
        \If{$p \in \mathbb{S}_s ~\&~ p \in \mathbb{S}_d$}{
            $P^{+} \leftarrow P^{+} \cup \{p\}$
        }
        \If{$p \in \mathbb{S}_s ~\&~ p \notin \mathbb{L}_d$}{
            $P^{-} \leftarrow P^{-} \cup \{p\}$
        }
        \If{$p \notin \mathbb{L}_s ~\&~ p \in \mathbb{S}_d$}{
            $P^{-} \leftarrow P^{-} \cup \{p\}$
        }
    }
    \textbf{Return} $P^{+}$, $P^{-}$.
\end{algorithm}

To sum up, our mined training data includes a query, mined positive and negative passages, and random negative passages.

\subsection{Query Generation Module}

Due to the limited number of unlabeled queries, we leverage a query generator to generate more queries for unlabeled passages in target languages. Note that the generated queries are in the same languages as the corresponding passage.

Specifically, for a trained generator, we randomly select some passages and leverage the fine-tuned query generator to generate queries for these passages.
To tackle the noisy label problem introduced by the generator, we use both sparse and dense retrievers to filter the generated pairs. We retrieve the top-1 passage for each generated query with both sparse and dense retrievers and only accept pairs where the best passages from both sparse and dense retrievers are the corresponding passage. 
Finally, for each filtered sample, we select negative passages like DPR~\cite{karpukhin2020dense}: 
1) random passages from the corpus; 
2) top passages returned by sparse and dense retrievers~(the passages returned by dense retriever are more effective~\cite{xiong2020approximate,qu2021rocketqa});
3) positive passages of other queries.

To sum up, our generated training data includes one positive passage, generated query, random negative passages, and top passages returned by retrievers as hard negative passages.

\subsection{Model Training}

\begin{algorithm}[t] 
    \caption{The training algorithm.}
    \label{alg:overview} 
	\LinesNumbered
    \KwIn{Labeled English training data and unlabeled queries and passages in target languages.}
    Construct index for sparse retriever\;
    Initialize the dense retriever and generator with pre-trained models\;
    Train the retriever and generator with English data\;
    Build ANN index for the retriever\;
    \While{models has not converged}{
        Generating training data $\mathbb{D}_p$ with passage mining module\;
        Generating training data $\mathbb{D}_g$ with query generation module\;
        Fine-tune the generator and retriever with both $\mathbb{D}_p$ and $\mathbb{D}_g$\;
        Refresh ANN index for the retriever.
    }
\end{algorithm}

Previously, we introduced the lexicon-enhanced retrieval module and the query generation module. In this part, we present the full training process.

As shown in Algorithm~\ref{alg:overview}, 
firstly, we train the warm-up dense retriever and query generator with data in the source language. We note that the input to the generator is the positive passage, and the label is the query. 
Secondly, we generate training data in target languages with the proposed two modules.
Finally, we fine-tune the retriever and the generator with the generated data. 
Based on these steps, we can conduct iteratively generating and training procedures to improve the performance. 
Note that due to the grammatical adjustment and accidental translation problems in the zero-shot multilingual generator, we only use the lexicon-enhanced retrieval module to generate data in the first iteration.

Considering the query generator is more sensitive to the quality of data, we set $S=1$ when generating data for the query generator.
