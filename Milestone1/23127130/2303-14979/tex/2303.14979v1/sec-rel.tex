\section{Related Work}

\paratitle{Information Retrieval.} Information retrieval aims to search relevant passages from a large corpus for a given query. Traditionally, researchers use bag-of-words~(BOW) based methods such as TF-IDF and BM25~\cite{robertson2009probabilistic}. These methods use a sparse vector to represent the text, so we call them sparse retrievers. Recently, some studies use neural networks to improve the sparse retriever. For example, DocTTTTTQuery~\cite{nogueira2019doc2query} proposes to expand the document to narrow the vocabulary gap and DeepCT~\cite{dai2019context} generates a weight for each term to emphasize the import terms. 

In contrast to sparse retrievers, dense retrievers usually encode both queries and passages into dense vectors whose lengths are much less than sparse vectors. There are two kinds of dense retrieval methods: 1) pre-training with unlabeled data and 2) fine-tuning with labeled data. 
For pre-training, ORQA~\cite{lee2019latent} proposes Inverse Cloze Task~(ICT) which aims to predict the context of a given sentence, and REALM~\cite{guu2020realm} proposes to predict the masked text based on an end-to-end retriever-reader model. 
Furthermore, SEED~\cite{lu2021less}, Condenser~\cite{gao2021condenser}, and coCondenser~\cite{gao2021unsupervised} propose pre-training tasks to encode more information into the dense vectors.
For fine-tuning, one major method is how to incorporate hard negative samples during training, including static sparse hard negative samples~\cite{karpukhin2020dense,luan2020sparse} and dynamic dense hard negative samples~\cite{xiong2020approximate,zhan2021optimizing}.
Another major method is training the retriever with a cross-attention encoder jointly, including extractive reader~\cite{yang2020retriever}, generative reader~\cite{izacard2020distilling}, and cross-encoder re-ranker~\cite{qu2021rocketqa,ren2021rocketqav2,zhang2021adver}. In addition, some works trade time for performance by using multiple vectors to represent the passage~\cite{khattab2020colbert,humeau2020poly,tang2021improving,zhang2022multi}.

\paratitle{Cross-lingual~(domain) Retrieval.}
These tasks aims to investigate the retrieval capabilities under cross-lingual~\cite{zhang2021mr,asai2020xor} or cross-domain~\cite{thakur2021beir} setting.
The methods for these tasks can be divided into two main categories: model transfer methods and data transfer methods.

The model transfer methods for cross-domain focus on pre-training sentence representation. For example, GTR~\cite{ni2021large} and CPT~\cite{neelakantan2022text} propose that scaling up the model size can significantly improve the performance of dense models. Contriever~\cite{izacard2021towards} and LaPraDoR~\cite{xu2022laprador} propose to use contrastive learning to learn sentence aware representation. For cross-lingual, they focus on learning multilingual representations by pre-training~\cite{lample2019cross,chi2020infoxlm,feng2022language} such as mBERT~\cite{devlin2018bert} and XLMR~\cite{conneau2019unsupervised}.

The data transfer methods mainly focus on obtaining noisy training data in the target domain or target languages. For example, Back-training~\cite{kulshreshtha2021back} and QGen~\cite{ma2020zero} propose to use a query generator to generate in-domain queries. CORA~\cite{asai2021one} leverages a generator to help mine retrieval training data and DR.DECR~\cite{li2021learning} mines lots of parallel data to perform cross-lingual distillation. 