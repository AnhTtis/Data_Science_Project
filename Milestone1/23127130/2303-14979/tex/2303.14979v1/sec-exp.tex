\section{Experiments}

In this section, we construct experiments to demonstrate the effectiveness of the proposed method.

\subsection{Experimental Setup}

\subsubsection{Dataset} 

\paratitle{Mr. TYDI.} The Mr. TYDI dataset~\cite{zhang2021mr} is constructed from TYDI~\cite{clark2020tydi} dataset and can be viewed as the ``open-retrieval'' condition of the TYDI dataset. It is a multilingual dataset for monolingual retrieval in 11 languages. The detailed statistics of the Mr. TYDI dataset are presented in Appendix~\ref{sec:dataset}.

\paratitle{DeepQA.} An Q\&A task dataset from one commercial Q\&A system, with 18,000 labeled cases in three languages: English~(En), German~(De), French~(Fr). Each case consists of two parts, \ie query and passage. The detailed statistics of the DeepQA dataset are presented in Appendix~\ref{sec:dataset}. 

\subsubsection{Evaluation Metrics.} 
Following Mr. TYDI, we use MRR@100 and Recall@100 as evaluation metrics, where MRR denotes the mean of reciprocal rank across queries and Recall@k denotes the proportion of queries to which the top k retrieved
passages contain positives. For DeepQA, due to the smaller size of the corpus~(only 1,220,030 passages in the corpus, for comparison, the Mr. TYDI data has 58,043,326 passages, which is times that of DeepQA), we use MRR@10 and Recall@10 as metrics.

\subsubsection{Implementation Details.} 

For the warm-up training stage, although Mr. TYDI proposed to use NQ~\cite{kwiatkowski2019natural} as English training data, we follow Xinyu~\etal~\cite{zhang2022towards} to use MS-MARCO as English training data. Xinyu~\etal find that MS-MARCO is better than NQ for zero-shot multilingual retrieval. We have further constructed experiments on NQ in Appendix~\ref{sec:nq}.

For the iteratively training stage, both the retriever and the generator are scheduled to train with 500 mini-batches in each iteration. The document index is refreshed after each iteration of training. The hyper-parameters are shown in Appendix~\ref{sec:param}.

All the experiments run on 8 NVIDIA Tesla A100 GPUs. The implementation code
is based on HuggingFace Transformers~\cite{wolf2020transformers}. For sentence embedding, we use the corresponding hidden state of the \emph{[CLS] token} for mBERT~\cite{devlin2018bert} and the average hidden states of all tokens for XLM-R~\cite{conneau2019unsupervised}. For the generator, we leverage mBART~\cite{liu2020mbart} as the pre-trained model.

\begin{table}[t] \small
\centering
\setlength\tabcolsep{4pt}
\caption{Results on Mr. TYDI test set. The best results except supervised training are in bold. We copy the results of BM25, tuned BM25, and zero-shot mBERT from \cite{zhang2022towards} and re-implement the zero-shot mBERT. $\ast$ denotes that our method significantly outperforms self-training at the level of 0.01. $\dag$ denotes that our method significantly outperforms back-training at the level of 0.01.}
\begin{tabular}{c|c|cc} \toprule
\multicolumn{2}{c|}{Methods} & MRR@100 & Recall@100 \\
\midrule \multirow{2}{*}{\shortstack{Sparse \\ Method}}
& BM25          & 32.1          & 73.2          \\ 
& (tuned)       & 33.3          & 75.8          \\ \midrule 

\multirow{7}{*}{mBERT}
& Zero-Shot     & 34.4          & 73.4          \\
& (reimpl)      & 36.5          & 73.3          \\ \cmidrule(lr){2-4}
& Self-Training & 37.2          & 78.5          \\
& Back-Training & 41.1          & 82.0          \\
& \name         & \textbf{49.0}$^{\ast \dag}$ & \textbf{83.6}$^{\ast \dag}$ \\ \cmidrule(lr){2-4}
& Supervised    & 54.6          & 87.0          \\ \midrule 

\multirow{6}{*}{XLM-R}
& Zero-Shot     & 30.4          & 74.3          \\  \cmidrule(lr){2-4}
& Self-Training & 35.0          & 78.6          \\
& Back-Training & 29.6          & 77.5          \\
& \name         & \textbf{47.2}$^{\ast \dag}$ & \textbf{82.7}$^{\ast \dag}$ \\ \cmidrule(lr){2-4}
& Supervised    & 54.5          & 87.2          \\ \midrule 

\end{tabular}

\label{tab:mrtydi}
\end{table}

\subsection{Results}

\subsubsection{Baselines}
As we investigate retrieval in the multilingual setting, in this paper, the main baselines methods include BM25, and multilingual DPR with mBERT~\cite{devlin2018bert}, XLM-R~\cite{conneau2019unsupervised} as the multilingual pre-trained model. 
Furthermore, we compare our method with two state-of-the-art domain adaption methods: self-training~\cite{yarowsky1995unsupervised} and back-training~\cite{kulshreshtha2021back}. Following Back-training,  we train the models 3 iterations with 5 epochs per iteration. Then we present the results with the best MRR@100.
In addition, we present the supervised performance as an upper limit reference. When constructing the supervised training data, we follow DPR~\cite{karpukhin2020dense} to select three kinds of negative passages.

\subsubsection{Mr. TYDI}

Table~\ref{tab:mrtydi} shows the result on Mr. TYDI. The first group is the sparse retriever, \ie BM25~\cite{robertson2009probabilistic} and tuned BM25. For each pre-trained model, the first group is the multilingual pre-trained models which are only fine-tuned on MS-MARCO data. The second block is the multilingual pre-trained models which are fine-tuned on MS-MARCO data and data augmentation method. We conduct pair t-test~\cite{hsu2014paired} between our method and other data augmentation method~(self-training and back-training). The final block is the multilingual pre-trained models which are fine-tuned on Mr. TYDI dataset. Due to the limited space, we only present the average performance among all languages in Table~\ref{tab:mrtydi} and present results for each language in Appendix~\ref{sec:lang}. 

Based on the results, we have the following findings.
Firstly, comparing the performance of domain adaption methods (the second block for each pre-trained model) and zero-shot performance, we can find that all domain adaption methods are effective. 
Secondly, comparing the three domain adaption methods, we can find that our method is better than the other methods. 
Finally, comparing our method and supervised dense retriever, we can find that the performance of our method is closed to the supervised performance on Recall@100, but is still worse than supervised performance with a clear edge on MRR@100. This indicates that the augmented data are noisy, for example, the mined passages are relevant to the queries but are not the best passages, and there may be more relevant passages for the queries. So, it is more helpful to Recall@100 but less helpful to MRR@100.

\begin{table}[t] \small
\centering
\setlength\tabcolsep{4pt}
\caption{Results on DeepQA test set. The best results except supervised training are in bold. $\ast$ denotes that our method significantly outperforms self-training at the level of 0.01. $\dag$ denotes that our method significantly outperforms back-training at the level of 0.01.}
\subtable[MRR@10]{\begin{tabular}{c|ccc|c} \toprule
Methods       & En             & De             & Fr             & Avg            \\ \midrule 
BM25          & 22.5           & 31.4           & 40.1           & 31.3           \\ \midrule 
Zero-Shot     & 24.0           & 29.4           & 37.7           & 30.3           \\ \midrule
Self-Training & 25.3           & 31.4           & 42.3           & 33.0           \\ 
Back-Training & 25.8           & 32.0           & 42.0           & 33.3           \\ 
\name          & \textbf{27.2}$^{\ast \dag}$  & \textbf{34.6}$^{\ast \dag}$  & \textbf{43.0}$^{\ast \dag}$  & \textbf{35.0}$^{\ast \dag}$  \\ \midrule
Supervised    & 23.0           & 33.9           & 39.7           & 32.2           \\ \bottomrule 
\end{tabular}}

\subtable[Recall@10]{\begin{tabular}{c|ccc|c} \toprule
Methods       & En             & De             & Fr             & Avg            \\ \midrule 
BM25          & 37.2           & 52.1           & 56.8           & 48.7           \\ \midrule 
Zero-Shot     & 39.1           & 49.3           & 56.7           & 48.4           \\ \midrule
Self-Training & 41.8           & 55.9           & 60.9           & 52.7           \\
Back-Training & 42.6           & 55.8           & 61.4           & 53.2           \\
\name          & \textbf{44.2}$^{\ast \dag}$  & \textbf{57.9}$^{\ast \dag}$  & \textbf{62.9}$^{\ast \dag}$  & \textbf{55.0}$^{\ast \dag}$  \\ \midrule
Supervised    & 38.8           & 62.8           & 63.0           & 54.7           \\ \bottomrule 
\end{tabular}}

\label{tab:deep}
\end{table}

\begin{table}[t] \small
\centering
\setlength\tabcolsep{4pt}
\caption{Performance of zero-shot dense retriever on DeepQA training set and test set.}
\begin{tabular}{l|cc} \toprule
~            & MRR@10         & Recall@10 \\ \midrule 
Training set & 35.1           & 48.5      \\ 
Test set     & 30.3           & 48.4      \\ \bottomrule 
\end{tabular}
\label{tab:train-test}
\end{table}

\subsubsection{DeepQA}

Due to the limited space, we only construct experiments on DeepQA based on mBERT. Table~\ref{tab:deep} presents the performance of all methods. As we can see, our method achieves the best performance among all the compared methods. It indicates that our method is effective for unsupervised multilingual dense retrieval.

In addition, we find that the unsupervised methods~(\ie self-training and back-training) perform better than the supervised training on MRR@10 but worse on Recall@10. A possible reason is that the size of DeepQA is small and there is a large gap between the distributions of the training data and test data. To demonstrate that, we evaluate the performance of the zero-shot dense retriever on both training data and test data. As shown in Table~\ref{tab:train-test}, there is a large gap between the MRR@10 on the training set and the test set of DeepQA. That indicates the gap between the training set of the test set is large. The dense model trained on the training set may seriously suffer from the overfitting problem. These results also indicate that our method is even more effective than supervised training when the training data in target languages is limited.

\begin{table}[t] \small
\centering
\setlength\tabcolsep{4pt}
\caption{Ablation results based on mBERT. ``LR'' denotes the lexicon-enhanced retrieval module. ``QG'' denotes the query generation module.}
\begin{tabular}{l|cc} \toprule
Methods      & MRR@100       & Recall@100    \\ \midrule
\name         & \textbf{49.0} & \textbf{83.6} \\ \midrule
w/o LR       & 46.9          & 81.6          \\
w/o LR$_+$   & 37.9          & 77.9          \\
w/o QG       & 48.1          & 83.2          \\ 
w/o ALL      & 36.5          & 73.3          \\ \bottomrule
\end{tabular}
\label{tab:ablation}
\end{table}

\subsection{Ablation Study}

In our method, we have incorporated two data augmentation modules, namely lexicon-enhanced retrieval, and query generation. Here, we would like to check how each module contributes to the final performance. We construct the ablation experiments on the Mr. TYDI data. We prepare four variants of our method that try all combinations: 
\begin{itemize}
    \item {\ul w/o LR} denotes that the retriever does not be fine-tuned with data from the lexicon-enhanced retrieval module. But the generator also is fine-tuned with data from the lexicon-enhanced retrieval module.
    \item {\ul w/o LR$_+$} denotes that both the retriever and the generator do not be fine-tuned with data from the lexicon-enhanced retrieval module. 
    \item {\ul w/o QG} denotes that the retriever does not be fine-tuned with data from the query generation module.
    \item {\ul w/o ALL} denotes without both the two modules, \aka zero-shot multilingual retrieval.
\end{itemize}

Table~\ref{tab:ablation} presents all comparison results of the four variants. Due to the limited space, we present results for each language in Appendix~\ref{sec:lang}. 
As we can see, the performance rank can be given as follows w/o ALL < w/o QG  < \name. These results indicate that both the two augmentation modules are essential to improve performance. And we can find that the lexicon-enhanced retrieval module is more effective than the query generation module, because of w/o LR < w/o QG. In addition, we find that w/o LR > w/o LR$_+$, it denotes the zero-shot multilingual query generation suffers from lots of problems and it also can demonstrate the effectiveness of the lexicon-enhanced retrieval module.

\begin{table}[t] \small
\centering
\setlength\tabcolsep{4pt}
\caption{Effect of lexicon-enhanced retrieval module.}
\begin{tabular}{l|cc} \toprule
    Methods                 & MRR@100       & Recall@100    \\ \midrule
    BM25                    & 32.1          & 73.2          \\
    Zero-shot mBERT         & 36.5          & 73.3          \\ \midrule
    \name w/o QG            & \textbf{47.5} & \textbf{82.5} \\ \midrule
    Sparse $+$ Dense        & 46.6          & 81.1          \\
    Sparse $\times$ Dense   & 40.8          & 80.4          \\
    Double Dense Retrievers & 44.5          & 81.9          \\ \midrule
    w/o HN                  & 42.5          & 80.4          \\
    w/ Sparse HN            & 43.3          & 79.2          \\ \bottomrule
\end{tabular}
\label{tab:passage}
\end{table}

\subsection{Method Analysis}

\subsubsection{Effect of Lexicon-enhanced Retrieval} 
In our lexicon-enhanced retrieval module, we combine the results of the sparse and dense retrievers to mine new training data. To show the effectiveness of our mining method, we construct the five variants~(for more conciseness, we use mBERT {\ul w/o QG + iterative refinement} as the base model):

\begin{itemize}
    \item {\ul Sparse $+$ Dense} combines results of sparse and dense retrievers by adding their scores.
    \item {\ul Sparse $\times$ Dense} combines results of sparse and dense retrievers by multiplying their scores. 
    \item {\ul Double Dense Retrievers} mines positive and negative passages with results from two dense retrievers which are trained on different data~(MS-MARCO and NQ).
    \item {\ul w/o Hard Negatives~(HN)} fine-tunes the model with mined positive passages and only in-batch negative passages.
    \item {\ul w/ Sparse Hard Negatives~(HN)} fine-tunes the model with mined positive passages, in-batch negative passages, and top passages returned by sparse retriever as negative passages.
\end{itemize}

Table~\ref{tab:passage} presents all comparison results of the five variants.  Based on the results, we have the following findings. 
Firstly, our mining method is more effective than the hybrid results of sparse and dense models. It demonstrates that our method can effectively leverage the knowledge of both sparse and dense retrievers. 
Secondly, mining data with sparse and dense retrievers are more effective than two dense retrievers. It demonstrates that sparse and dense retrievers have noticed different characteristics of retrieval.
Finally, mined negatives are more effective than sparse negatives. It demonstrates that negatives are important in dense retrieval tasks and our methods can provide more effective negatives. 

\begin{figure}[t]
    \centering
    \subfigure[$S$ in Algorithm~\ref{alg:pm}.]{
        \includegraphics[width=0.75\columnwidth]{figures/thresh.pdf}
        \label{fig:sens-thresh}
    }
    \subfigure[The number of generated queries.]{
        \includegraphics[width=0.75\columnwidth]{figures/generate.pdf}
        \label{fig:sens-gen}
    }
    \caption{Parameter sensitivity.}
    \label{fig:sensitivity}
\end{figure}

\subsubsection{Effect of Parameters} \label{sec:params}
In our method, we introduce two parameters in the lexicon-enhanced retrieval module to define relevant and irrelevant passages: $S$ and $L$. And the number of generated queries also influences the final performance. 
Here, we tune the  $S$ and $L$ based on mBERT w/o QG. We vary $S$ in the set $\{1, 2, 3, 4, 5\}$. And for more conciseness, we set $L = S \times 10$. 
In addition, we tune the number of generated queries based on mBERT. We vary the number of generated queries per language in the set $\{1000, 2000, 3000, 4000, 5000\}$.

Figure~\ref{fig:sens-thresh} presents the effect of the parameter $S$. We can observe that $S = 1$ leads to poor performance on both MRR@100 and Recall@100. Because the method with little $S$ mines few samples and leads to the overfitting problem. When we set $S > 2$, it leads to better Recall@100 but poorer MRR@100. A possible reason is that large $S$ leads to more noisy samples. As we mentioned above, noisy samples are helpful to Recall@100 but harmful to MRR@100. 

Figure~\ref{fig:sens-gen} presents the effect of the number of generated queries. As we can see, the large number of generated queries improves the MRR@100 but cannot improve the Recall@100. A possible reason is that the generated queries mainly focus on a few kinds~(\eg when or where something happened). They are helpful to MRR@100 for these kinds of queries but less helpful to both Recall@100 and MRR@100 for other kinds of queries.