\clearpage

\section*{Appendix}
\appendix

\section{Dataset Statistics} \label{sec:dataset}

\paratitle{Mr.TYDI.} Mr. TYDI is a multilingual retrieval benchmark based on the TYDI dataset. Mr. TYDI covers 11 languages. The corpus for each language is drawn from Wikipedia, and the query and judgments are prepared by native speakers of that language. Table~\ref{tab:stat} presents statistics of the Mr. TYDI dataset, copied from the original paper.

\paratitle{DeepQA.} DeepQA is a Q\&A dataset from one commercial Q\&A system, with 18,000 labeled cases in three languages. Each case consists of two parts, \ie query and passage. The following briefly describes how the data is collected. Firstly, for each query, the top 10 relevant documents returned by the search engine are selected to form <query, url> pairs; Then passages are further extracted from these documents to form <query, url, passage> triples; These <query, passage> pairs are sampled and sent to crowd sourcing judges. Specifically, each <query, passage> pair is required to get judged by three judges. Those cases with more than 2/3 positive labels will get positive labels, otherwise negative. The detailed statistics of the DeepQA dataset are presented in Table~\ref{tab:deep-stat}. 

\begin{table}[h] \small
\centering
\caption{Statistics for DeepQA: number of queries (\# Q), judgments (\# J), and the number of passages.}
\begin{tabular}{l|rr|rr|r} \toprule
    \multirow{2.5}{*}{L} & \multicolumn{2}{c|}{Train} & \multicolumn{2}{c|}{Test} & \multirow{2.5}{*}{Corpus Size} \\ \cmidrule(lr){2-5}
    ~ & \# Q & \# J & \# Q & \# J & ~ \\ \midrule
    En & 4,437 & 6,022 & 1,703 & 1,978 & 741,840 \\
    De & 2,401 & 3,977 & 910 & 1,023 & 385,440 \\
    Fr & 2,976 & 4,000 & 936 & 1,000 & 92,750 \\ \midrule
    All & 9,814 & 13,999 & 3,549 & 4,001 & 1,220,030 \\ \bottomrule
\end{tabular}
\label{tab:deep-stat}
\end{table}

\begin{table}[t] \small
\centering
\caption{Hyper-parameters.}
\begin{tabular}{c|l|c} \toprule
     ~ & Parameters & Value \\ \midrule
     ~ & Max Query Length & 32 \\
     ~ & Max Passage Length & 128 \\ \midrule
     \multirow{7}{*}{\shortstack{Train \\ warm-up \\ retriever}} & Learning Rate & 1e-5 \\
     ~ & Batch Size & 128 \\
     ~ & Negative Size & 255 \\
     ~ & Optimizer & AdamW \\ 
     ~ & Scheduler & Linear \\
     ~ & Warmup Proportion & 0.1 \\
     ~ & Training Epoch & 3 \\ \midrule
     \multirow{6}{*}{\shortstack{Train \\ warm-up \\ generator}} & Learning Rate & 1e-5 \\
     ~ & Batch Size & 64 \\
     ~ & Optimizer & AdamW \\ 
     ~ & Scheduler & Linear \\
     ~ & Warmup Proportion & 0.1 \\
     ~ & Training Epoch & 1 \\ \midrule
     \multirow{10}{*}{\shortstack{Iteraively \\ training of \\ retriever}} & Learning Rate & 1e-6 \\
     ~ & Batch Size & 128 \\
     ~ & Negative Size & 255 \\
     ~ & Optimizer & AdamW \\ 
     ~ & Scheduler & Linear \\
     ~ & Warmup Proportion & 0.1 \\
     ~ & Training Epoch & 5 \\
     ~ & $S$ in Algorithm~\ref{alg:pm} & 2 \\
     ~ & $L$ in Algorithm~\ref{alg:pm} & 20 \\
     ~ & \# of Generated Queries & 5000 \\
     ~ & \# of Iteration & 3 \\\midrule
     \multirow{6}{*}{\shortstack{Iteraively \\ training of \\ generator}} & Learning Rate & 1e-5 \\
     ~ & Batch Size & 64 \\
     ~ & Optimizer & AdamW \\ 
     ~ & Scheduler & Linear \\
     ~ & Warmup Proportion & 0.1 \\
     ~ & Training Epoch & 5 \\
     ~ & \# of Iteration & 3 \\\bottomrule
\end{tabular}
\label{tab:params}
\end{table}

\begin{table}[!ht] \small
\centering
\caption{Efficiency Report.}
\begin{tabular}{l|l|c} \toprule
     \multirow{5}{*}{Training} & Warm-up & 0.5h \\
     ~ & Per Iteration & 0.2h \\
     ~ & Index Refresh & 1.7h \\
     ~ & Generate Queries & 0.5h \\ 
     ~ & Overall & 8.9h \\ \midrule
     \multirow{3}{*}{Inference} & Build Index & 1.7h \\
     ~ & Query Encoding & 40ns \\
     ~ & Dense Retrieval & 2ms \\ \midrule
\end{tabular}
\label{tab:efficiency}
\end{table}

\section{Hyper-parameters} \label{sec:param}

We have analyzed the parameters of our method in Section~\ref{sec:params}. Here, we present the other hyper-parameters of our method in Table~\ref{tab:params}, most of them follow Back-training~\cite{kulshreshtha2021back} and DPR~\cite{karpukhin2020dense}.


\section{Efficiency Report}

We list the time cost of training and inference in Table~\ref{tab:efficiency}. The evaluation is made with 8 NVIDIA A100 GPUs. The number of iterations is set as 3.

\section{Additional Experiments}

\begin{table}[t] \small
    \centering
    \caption{A filtered out generated query.}
    \begin{tabular}{p{0.9\columnwidth}}
        \toprule
        \textbf{Passage:} Baada ya uhuru wa Fiji (1970) kutoka kwa Waingereza, yalifuata mapinduzi ya kijeshi yaliyotokea mwaka 1987, hali iliyosababishwa na wakazi wa Fiji kulaumu serikali yao kutawaliwa na watu wa kabila la Indofijian au Wahindi. \\ \textbf{Translation:} After Fiji's independence (1970) from the British, it followed a military coup in 1987, a situation in which Fijians blamed their government for being ruled by Indofijian or Indian people. \\
        \midrule
        \textbf{Generated query:} Kwa upi uhuru wa Fiji ulifanyika mwaka gani? \\ \textbf{Translation:} In what year did Fiji's independence take place? \\
        \midrule
        \textbf{Top-1 passage:} Fiji ilijipatia uhuru wake kutoka katika utawala wa kikoloni wa Uingereza tarehe 10 Oktoba 1970. \\
        \textbf{Translation:} Fiji gained its independence from British colonial rule on October 10, 1970. \\
        \bottomrule
    \end{tabular}
    \label{tab:query-out}
\end{table}

\begin{table*}[t] \small
\centering
\caption{Effect of query filter. ``QF'' denotes query filter and ``HN'' denotes hard negative passages which are top passages returned by sparse and dense retrievers.}
\subtable[MRR@100]{\begin{tabular}{l|ccccccccccc|c} \toprule
Methods      & Ar             & Bn             & En             & Fi             & Id             & Ja             & Ko             & Ru             & Sw             & Te             & Th             & Avg           \\ \midrule
\name         & \textbf{58.5}  & \textbf{49.5}  & \textbf{37.3}  & \textbf{45.6}  & \textbf{51.3}  & \textbf{38.6}  & \textbf{43.6}  & \textbf{46.2}  & \textbf{48.6}  & \textbf{66.5}  & \textbf{53.1}  & \textbf{49.0} \\
w/o QF       & 53.5           & 47.1           & 33.1           & 39.9           & 47.5           & 36.6           & 36.2           & 44.0           & 44.2           & 27.6           & 42.0           & 41.1          \\
w/o QF \& HN & 58.1           & 48.6           & 37.0           & 45.2           & 50.6           & 38.4           & \textbf{43.6}  & 46.0           & 47.7           & 54.6           & 53.0           & 47.5          \\ \bottomrule
\end{tabular}}

\subtable[Recall@100]{\begin{tabular}{l|ccccccccccc|c} \toprule
Methods       & Ar             & Bn             & En             & Fi             & Id             & Ja             & Ko             & Ru             & Sw             & Te             & Th             & Avg           \\ \midrule
\name          & 85.9           & 89.2           & 77.6           & 83.1           & 86.4           & 77.0           & 74.9           & 81.8           & 83.4           & 95.6           & 85.3           & 83.6          \\
w/o QF        & 83.7           & 91.4           & 74.5           & 79.3           & 85.4           & 76.7           & 69.5           & 82.1           & 81.6           & 91.9           & 81.9           & 81.6          \\
w/o QF \& HN  & \textbf{86.2}  & \textbf{90.1}  & \textbf{79.1}  & \textbf{83.4}  & \textbf{87.3}  & \textbf{77.5}  & \textbf{76.8}  & \textbf{83.3}  & \textbf{82.9}  & \textbf{96.1}  & \textbf{86.7}  & \textbf{84.5} \\ \bottomrule
\end{tabular}}

\label{tab:query-lang}
\end{table*}

\subsection{Effect of Query Filter}
In our query generation module, we use the dense retriever to filter the generated queries. Here, we analyze the effectiveness of filtering based on mBERT. We present the result for {\ul w/o Queries Filter} and {\ul w/o Queries Filter \& Hard Negative passages} in Table~\ref{tab:query-lang}. As we can see, filtering generated queries can lead to better performance. We also find that w/o QF \& HN $>$ \name $>$ w/o QF on Recall@100. It denotes that the generated queries are relevant to the passages but the top passages returned by retrievers may be more relevant.
We present an example of a query that is filtered out in Table~\ref{tab:query-out}. 
As we can see, although the generated query can be answered by the passage, the main statement of the passage is not intended to answer the generated query and there is a more relevant passage to answer the generated query. As a result, these samples~(w/o hard negative passages) are helpful to Recall@100 but harmful to MRR@100.

\subsection{Visualization of the Training Procedure} 
Our method employs iteratively training to improve the performance. Here, we report the iterative performance of our method in Figure~\ref{fig:iter}. To better show the effectiveness of our method, we set the number of iterations as 5 in the experiments. As we can see, the performance of our method increases with iteration and it holds steady when the model convergences. It shows that the distribution of mined data is similar to the distribution of real data, so the model does not suffer from the overfitting problem. In the end, the MRR@100 is improved by approximately 12\%, and Recall@100 is improved by approximately 10\%. It demonstrates the effectiveness of the mined data.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\columnwidth]{figures/iteration.pdf}
    \caption{Iterative performance for the proposed \name based on mBERT.}
    \label{fig:iter}
\end{figure}

\subsection{Effect of English Data} \label{sec:nq}

In this section, we test the influence of different English data. As Xinyu \etal~\cite{zhang2022towards} said, MS-MARCO~\cite{bajaj2016ms} has a larger dataset size than NQ~\cite{kwiatkowski2019natural}, and the data size is a more critical factor. To test the effectiveness of our methods, we construct experiments on both MS-MARCO and NQ. Note that the only difference is the English data and we directly use the tuned parameters based on MS-MARCO for all experiments.
Table~\ref{tab:nq} presents the performance of two English data based on mBERT. Note that our re-implement mBERT based on NQ is better than the Mr. TYDI paper~\cite{zhang2021mr}\footnote{\href{https://github.com/castorini/mr.tydi}{https://github.com/castorini/mr.tydi}}. Because we share the parameters of the query encoder and the passage encoder, the trick leads to better performance.

As we can see, the gap between the performance of our method based on MS-MARCO data and NQ data is smaller than other methods. And the Recall@100 on NQ data is even higher than Recall@100 on MS-MACRO data. A possible reason is that NQ is closer to Mr. TYDI, both of them contain relatively well-formed queries posed against Wikipedia. The zero-shot performance of English data in Mr. TYDI data~(the Recall@100 of English is 75.1 for the model trained on MS-MARCO data and 78.3 for the model trained on NQ data) can demonstrate that. So, the mined data of dense retriever trained by NQ is more effective than MS-MARCO, and more effective data in target languages can lead to better performance.

\section{Detailed Results} \label{sec:lang}

Due to the limited space, we only present average performance in the experiment section. Here, we present the performance of each language. First, we present the detailed performance of both our method and baselines in Table~\ref{tab:mrtydi-lang}. Second, we present the detailed performance of ablation results in Table~\ref{tab:ablation-lang}. Finally, we present the detailed performance of variants of the passage mining module in Table~\ref{tab:passage-lang}. As we can see, our method performs better in most languages in these settings.

\begin{table*}[t] \footnotesize
\centering
\caption{Performance comparison for different English data based on mBERT.}

\subtable[MRR@100]{\begin{tabular}{c|ccccccccccc|c} \toprule
Methods       & Ar             & Bn             & En             & Fi             & Id             & Ja             & Ko             & Ru             & Sw             & Te             & Th             & Avg            \\ \midrule
\multicolumn{13}{c}{MS-MARCO} \\ \midrule
Zero-Shot     & 47.7           & 41.5           & 32.4           & 34.9           & 41.9           & 30.9           & 30.8           & 35.6           & 40.4           & 34.8           & 30.2           & 36.5           \\ \midrule 
Self-Training & 45.4           & 40.5           & 28.5           & 33.8           & 40.2           & 32.1           & 32.1           & 34.1           & 43.8           & 42.9           & 36.2           & 37.2           \\ 
Back-Training & 49.0           & 48.7           & 31.7           & 38.9           & 44.5           & 34.5           & 34.8           & 38.3           & 46.0           & 46.7           & 38.9           & 41.1           \\ 
\name          & \textbf{58.5}  & 49.5           & 37.3           & \textbf{45.6}  & \textbf{51.3}  & 38.6           & \textbf{43.6}  & \textbf{46.2}  & \textbf{48.6}  & 66.5           & \textbf{53.1}  & \textbf{49.0}  \\ \midrule
\multicolumn{13}{c}{NQ} \\ \midrule
Zero-Shot     & 30.0           & 38.7           & 30.6           & 25.6           & 30.7           & 30.6           & 23.4           & 29.6           & 28.1           & 24.1           & 22.6           & 28.5           \\ \midrule
Self-Training & 32.2           & 41.5           & 27.9           & 27.3           & 33.5           & 30.4           & 24.2           & 30.4           & 33.3           & 36.5           & 28.0           & 31.4           \\ 
Back-Training & 35.4           & 42.3           & 29.7           & 28.5           & 33.2           & 31.5           & 27.7           & 32.8           & 40.8           & 24.7           & 30.5           & 32.5           \\ 
\name          & 54.8           & \textbf{56.6}  & \textbf{41.0}  & 42.3           & 51.2           & \textbf{41.6}  & 38.9           & 46.1           & 46.2           & \textbf{66.7}  & 49.1           & 48.6           \\ \bottomrule
\end{tabular}}

\subtable[Recall@100]{\begin{tabular}{c|ccccccccccc|c} \toprule
Methods       & Ar             & Bn             & En             & Fi             & Id             & Ja             & Ko             & Ru             & Sw             & Te             & Th             & Avg            \\ \midrule
\multicolumn{13}{c}{MS-MARCO} \\ \midrule
Zero-Shot     & 80.6           & 78.8           & 75.1           & 74.7           & 79.3           & 67.8           & 65.5           & 73.1           & 70.4           & 77.1           & 63.7           & 73.3           \\ \midrule 
Self-Training & 82.3           & 86.9           & 74.4           & 78.2           & 81.3           & 72.7           & 67.0           & 77.6           & 78.7           & 91.0           & 73.6           & 78.5           \\ 
Back-Training & 84.6           & 90.1           & 76.5           & 81.4           & 84.4           & 76.2           & 73.6           & 82.2           & 80.8           & 89.6           & 83.1           & 82.0           \\ 
\name          & \textbf{85.9}  & 89.2           & 77.6           & 83.1           & 86.4           & 77.0           & 74.9           & 81.8           & 83.4           & 95.6           & 85.3           & 83.5           \\ \midrule
\multicolumn{13}{c}{NQ} \\ \midrule
Zero-Shot     & 70.3           & 80.2           & 78.3           & 69.2           & 76.4           & 74.3           & 60.9           & 72.7           & 65.7           & 69.9           & 59.9           & 70.7           \\ \midrule
Self-Training & 78.9           & 86.5           & 79.1           & 77.7           & 83.3           & 79.0           & 69.6           & 77.9           & 76.7           & 90.5           & 74.8           & 79.5           \\ 
Back-Training & 81.7           & 88.7           & 80.1           & 80.0           & 86.0           & 80.8           & 73.5           & 81.9           & 83.4           & 93.6           & 83.0           & 83.0           \\ 
\name          & \textbf{85.9}  & \textbf{91.9}  & \textbf{83.9}  & \textbf{84.3}  & \textbf{88.7}  & \textbf{83.2}  & \textbf{78.4}  & \textbf{85.5}  & \textbf{82.9}  & \textbf{95.7}  & \textbf{85.7}  & \textbf{86.0}  \\ \bottomrule
\end{tabular}}

\label{tab:nq}
\end{table*}

\begin{table*}[t] \small
\centering
\caption{Statistics for Mr. TYDI: number of queries (\# Q), judgments (\# J), and the number of passages.}
\begin{tabular}{l|rr|rr|rr|r} \toprule
    \multirow{2.5}{*}{L} & \multicolumn{2}{c|}{Train} & \multicolumn{2}{c|}{Dev} & \multicolumn{2}{c|}{Test} & \multirow{2.5}{*}{Corpus Size} \\ \cmidrule(lr){2-7}
    ~ & \# Q & \# J & \# Q & \# J & \# Q & \# J & ~ \\ \midrule
    Ar & 12,377 & 12,377 & 3,115 & 3,115 & 1,081 & 1,257 & 2,106,586 \\
    Bn & 1,713 & 1,719 & 440 & 443 & 111 & 130 & 304,059 \\
    En & 3,547 & 3,547 & 878 & 878 & 744 & 935 & 32,907,100 \\
    Fi & 6,561 & 6,561 & 1,738 & 1,738 & 1,254 & 1,451 & 1,908,757 \\
    Id & 4,902 & 4,902 & 1,224 & 1,224 & 829 & 961 & 1,469,399 \\
    Ja & 3,697 & 3,697 & 928 & 928 & 720 & 923 & 7,0000,027 \\
    Ko & 1,295 & 1,317 & 303 & 307 & 421 & 492 & 1,496,126 \\
    Ru & 5,366 & 5,366 & 1,375 &  1,375 & 955 & 1,168 & 9,597,504 \\
    Sw & 2,072 & 2,401 & 526 & 623 & 670 & 743 & 136,689 \\
    Te & 3,880 & 3,880 & 983 & 983 & 646 & 664 & 548,224 \\
    Th & 3,319 & 3,360 & 807 & 817 & 1,190 & 1,368 & 568,855 \\ \midrule
    All & 48,729 & 49,127 & 12,317 & 12,431 & 8,661 & 10,092 & 58,043,326 \\ \bottomrule
\end{tabular}
\label{tab:stat}
\end{table*}


\begin{table*}[t] \footnotesize
\centering
\caption{Detail results on Mr. TYDI test set.}

\subtable[MRR@100]{\begin{tabular}{c|c|ccccccccccc|c}

\toprule
\multicolumn{2}{c|}{Methods} 
                & Ar             & Bn             & En             & Fi             & Id             & Ja             & Ko             & Ru             & Sw             & Te             & Th             & Avg            \\
\midrule \multirow{2}{*}{\shortstack{Sparse \\ Retriever}}
& BM25          & 36.8           & 41.8           & 14.0           & 28.4           & 37.6           & 21.1           & 28.5           & 31.3           & 38.9           & 34.3           & 40.1           & 32.1           \\ 
& (tuned)       & 36.7           & 41.3           & 15.1           & 28.8           & 38.2           & 21.7           & 28.1           & 32.9           & 39.6           & 42.4           & 41.7           & 33.3           \\ \midrule 

\multirow{7}{*}{mBERT}
& Zero-Shot     & 44.4           & 38.3           & 31.5           & 30.6           & 37.8           & 31.4           & 29.7           & 33.7           & 36.9           & 36.3           & 28.2           & 34.4           \\  
& (reimpl)      & 47.7           & 41.5           & 32.4           & 34.9           & 41.9           & 30.9           & 30.8           & 35.6           & 40.4           & 34.8           & 30.2           & 36.5           \\ \cmidrule(lr){2-14}
& Self-Training & 45.4           & 40.5           & 28.5           & 33.8           & 40.2           & 32.1           & 32.1           & 34.1           & 43.8           & 42.9           & 36.2           & 37.2           \\ 
& Back-Training & 49.0           & 48.7           & 31.7           & 38.9           & 44.5           & 34.5           & 34.8           & 38.3           & 46.0           & 46.7           & 38.9           & 41.1           \\ 
& \name          & \textbf{58.5}  & \textbf{49.5}  & \textbf{37.3}  & \textbf{45.6}  & \textbf{51.3}  & \textbf{38.6}  & \textbf{43.6}  & \textbf{46.2}  & \textbf{48.6}  & \textbf{66.5}  & \textbf{53.1}  & \textbf{49.0}  \\ \cmidrule(lr){2-14}
& Supervised    & 64.2           & 55.2           & 45.4           & 52.7           & 53.8           & 43.1           & 42.8           & 46.4           & 58.8           & 83.3           & 54.5           & 54.6           \\ \midrule 

\multirow{6}{*}{XLM-R}
& Zero-Shot     & 31.7           & 40.9           & 18.1           & 30.3           & 31.7           & 22.9           & 31.7           & 27.0           & 24.4           & 36.0           & 39.5           & 30.4           \\ \cmidrule(lr){2-14}
& Self-Training & 37.8           & 41.6           & 21.6           & 31.6           & 36.0           & 25.6           & 29.4           & 28.1           & 40.5           & 54.6           & 38.4           & 35.0           \\  
& Back-Training & 32.0           & 43.0           & 20.2           & 28.7           & 31.3           & 25.1           & 30.4           & 29.0           & 20.2           & 27.5           & 38.8           & 29.6           \\ 
& \name          & \textbf{54.0}  & \textbf{51.7}  & \textbf{33.0}  & \textbf{43.9}  & \textbf{49.9}  & \textbf{34.8}  & \textbf{40.3}  & \textbf{41.4}  & \textbf{45.1}  & \textbf{70.1}  & \textbf{54.9}  & \textbf{47.2}  \\ \cmidrule(lr){2-14}
& Supervised    & 62.9           & 61.5           & 43.0           & 51.6           & 53.6           & 39.9           & 41.2           & 44.0           & 57.6           & 83.3           & 60.9           & 54.5           \\ \midrule 

\end{tabular}}

\subtable[Recall@100]{\begin{tabular}{c|c|ccccccccccc|c}

\toprule
\multicolumn{2}{c|}{Methods} 
                & Ar             & Bn             & En             & Fi             & Id             & Ja             & Ko             & Ru             & Sw             & Te             & Th             & Avg            \\
\midrule \multirow{2}{*}{\shortstack{Sparse \\ Retriever}}
& BM25          & 79.3           & 86.9           & 53.7           & 71.9           & 84.3           & 64.5           & 61.9           & 64.8           & 76.4           & 75.8           & 85.3           & 73.2           \\ 
& (tuned)       & 80.0           & 87.4           & 55.4           & 72.5           & 84.6           & 65.6           & 79.7           & 66.0           & 76.4           & 81.3           & 85.3           & 75.8           \\ \midrule 

\multirow{7}{*}{mBERT}
& Zero-Shot     & 79.9           & 82.0           & 75.8           & 69.3           & 75.8           & 73.8           & 64.5           & 72.8           & 68.6           & 79.7           & 64.8           & 73.4           \\  
& (reimpl)      & 80.6           & 78.8           & 75.1           & 74.7           & 79.3           & 67.8           & 65.5           & 73.1           & 70.4           & 77.1           & 63.7           & 73.3           \\ \cmidrule(lr){2-14}
& Self-Training & 82.3           & 86.9           & 74.4           & 78.2           & 81.3           & 72.7           & 67.0           & 77.6           & 78.7           & 91.0           & 73.6           & 78.5           \\ 
& Back-Training & 84.6           & \textbf{90.1}  & 76.5           & 81.4           & 84.4           & 76.2           & 73.6           & \textbf{82.2}  & 80.8           & 89.6           & 83.1           & 82.0           \\ 
& \name          & \textbf{85.9}  & 89.2           & \textbf{77.6}  & \textbf{83.1}  & \textbf{86.4}  & \textbf{77.0}  & \textbf{74.9}  & 81.8           & \textbf{83.4}  & \textbf{95.6}  & \textbf{85.3}  & \textbf{83.6}  \\ \cmidrule(lr){2-14}
& Supervised    & 90.2           & 92.3           & 84.2           & 85.8           & 87.7           & 82.1           & 78.8           & 84.7           & 85.9           & 96.2           & 88.7           & 87.0           \\ \midrule 

\multirow{6}{*}{XLM-R}
& Zero-Shot     & 76.3           & 84.2           & 68.8           & 74.6           & 82.3           & 66.7           & 67.9           & 68.9           & 60.3           & 81.2           & 86.5           & 74.3           \\ \cmidrule(lr){2-14}
& Self-Training & 78.9           & 85.1           & 68.8           & 78.6           & 84.0           & 70.2           & 66.8           & 71.6           & 78.2           & 93.6           & 88.4           & 78.6           \\  
& Back-Training & 78.9           & \textbf{88.3}  & 72.4           & 77.3           & 84.5           & 73.1           & 71.1           & 75.2           & 59.7           & 80.6           & 91.1           & 77.5           \\ 
& \name          & \textbf{85.0}  & 86.0           & \textbf{76.2}  & \textbf{82.6}  & \textbf{86.4}  & \textbf{74.6}  & \textbf{73.6}  & \textbf{77.8}  & \textbf{80.2}  & \textbf{95.4}  & \textbf{92.1}  & \textbf{82.7} \\ \cmidrule(lr){2-14}
& Supervised    & 89.3           & 92.8           & 83.1           & 86.3           & 89.8           & 80.1           & 78.7           & 82.6           & 87.0           & 96.7           & 92.6           & 87.2           \\ \midrule 

\end{tabular}}

\label{tab:mrtydi-lang}
\end{table*}

\begin{table*}[t] \small
\centering
\caption{Ablation results based on mBERT. }
\subtable[MRR@100]{\begin{tabular}{l|ccccccccccc|c} \toprule
Methods      & Ar             & Bn             & En             & Fi             & Id             & Ja             & Ko             & Ru             & Sw             & Te             & Th             & Avg           \\ \midrule
All          & 58.5           & 49.5           & \textbf{37.3}  & \textbf{45.6}  & \textbf{51.3}  & \textbf{38.6}  & \textbf{43.6}  & \textbf{46.2}  & \textbf{48.6}  & 66.5           & \textbf{53.1}  & \textbf{49.0} \\ \midrule
w/o LR       & 55.7           & \textbf{49.6}  & 35.9           & 43.0           & 49.7           & 38.1           & 40.1           & 43.5           & 46.9           & \textbf{66.9}  & 45.9           & 46.9          \\
w/o LR$_+$   & 49.1           & 45.5           & 32.4           & 36.4           & 43.4           & 32.0           & 32.4           & 39.0           & 41.9           & 45.9           & 39.0           & 39.7          \\
w/o QG       & \textbf{58.8}  & 49.4           & 37.0           & 45.1           & 51.0           & 38.0           & 42.7           & 45.5           & 47.2           & 62.7           & 51.4           & 48.1          \\
w/o LR + QG  & 47.7           & 41.5           & 32.4           & 34.9           & 41.9           & 30.9           & 30.8           & 35.6           & 40.4           & 34.8           & 30.2           & 36.5          \\ \bottomrule
\end{tabular}}

\subtable[Recall@100]{\begin{tabular}{l|ccccccccccc|c} \toprule
Methods      & Ar             & Bn             & En             & Fi             & Id             & Ja             & Ko             & Ru             & Sw             & Te             & Th             & Avg           \\ \midrule
All          & \textbf{85.9}  & \textbf{89.2}  & 77.6           & 83.1           & 86.4           & \textbf{77.0}  & \textbf{74.9}  & 81.8           & \textbf{83.4}  & \textbf{95.6}  & \textbf{85.3}  & \textbf{83.6} \\ \midrule
w/o LR       & 84.0           & 87.4           & 76.0           & 80.4           & 85.1           & 76.2           & 70.7           & 80.7           & 81.7           & 93.8           & 81.6           & 81.6          \\
w/o LR$_+$   & 80.8           & 85.1           & 73.7           & 76.8           & 82.5           & 71.1           & 65.7           & 77.7           & 77.5           & 89.5           & 76.6           & 77.9          \\
w/o QG       & 85.7           & 88.3           & \textbf{78.6}  & \textbf{83.3}  & \textbf{86.9}  & 76.2           & 74.4           & 81.7           & 80.8           & 95.4           & 84.3           & 83.2          \\
w/o LR + QG  & 80.6           & 78.8           & 75.1           & 74.7           & 79.3           & 67.8           & 65.5           & 73.1           & 70.4           & 77.1           & 63.7           & 73.3          \\ \bottomrule
\end{tabular}}

\label{tab:ablation-lang}
\end{table*}

\begin{table*}[t] \small
\centering
\caption{Effect of lexicon-enhanced retrieval module.}
\subtable[MRR@100]{\begin{tabular}{l|ccccccccccc|c} \toprule
Methods                 & Ar             & Bn             & En             & Fi             & Id             & Ja             & Ko             & Ru             & Sw             & Te             & Th             & Avg  \\ \midrule
Sparse                  & 36.8           & 41.8           & 14.0           & 28.4           & 37.6           & 21.1           & 28.5           & 31.3           & 38.9           & 34.3           & 40.1           & 32.1 \\ 
Dense                   & 47.7           & 41.5           & 32.4           & 34.9           & 41.9           & 30.9           & 30.8           & 35.6           & 40.4           & 34.8           & 30.2           & 36.5 \\ \midrule
\name w/o QG             & \textbf{58.5}  & 50.8           & \textbf{37.0}  & \textbf{45.0}  & \textbf{52.1}  & \textbf{38.2}  & \textbf{41.7}  & \textbf{45.0}  & 46.6           & \textbf{59.9}  & 48.2           & \textbf{47.5}  \\ \midrule
Sparse $+$ Dense        & 57.1           & \textbf{54.1}  & 34.3           & 43.6           & 50.2           & 37.4           & 38.6           & 44.3           & \textbf{47.7}  & 55.7           & \textbf{49.9}  & 46.6  \\
Sparse $\times$ Dense   & 48.5           & 48.3           & 25.7           & 37.1           & 44.0           & 31.2           & 34.6           & 39.0           & 42.4           & 49.8           & 47.8           & 40.8  \\
Double Dense Retrievers               & 52.5           & 52.5           & 36.4           & 40.6           & 48.2           & 37.9           & 38.1           & 40.8           & 45.1           & 56.4           & 40.7           & 44.5  \\ \midrule
w/o HN                  & 52.2           & 47.4           & 32.6           & 39.9           & 46.7           & 35.3           & 36.6           & 39.8           & 45.5           & 53.2           & 38.5           & 42.5  \\ 
w/ Sparse HN            & 54.3           & 45.6           & 34.8           & 41.5           & 49.3           & 35.7           & 37.1           & 41.5           & 46.1           & 50.9           & 39.9           & 43.3  \\ \bottomrule
\end{tabular}}

\subtable[Recall@100]{\begin{tabular}{l|ccccccccccc|c} \toprule
Methods                 & Ar             & Bn             & En             & Fi             & Id             & Ja             & Ko             & Ru             & Sw             & Te             & Th             & Avg  \\ \midrule
Sparse                  & 79.3           & 86.9           & 53.7           & 71.9           & 84.3           & 64.5           & 61.9           & 64.8           & 76.4           & 75.8           & 85.3           & 73.2 \\
Dense                   & 80.6           & 78.8           & 75.1           & 74.7           & 79.3           & 67.8           & 65.5           & 73.1           & 70.4           & 77.1           & 63.7           & 73.3 \\ \midrule
\name w/o QG             & \textbf{85.9}  & 89.2           & 78.9           & \textbf{82.5}  & \textbf{85.8}  & 75.2           & 73.6           & \textbf{81.3}  & \textbf{79.3}  & \textbf{94.0}  & \textbf{81.9}  & \textbf{82.5}  \\ \midrule
Sparse $+$ Dense        & 85.1           & \textbf{89.6}  & 77.0           & 79.8           & \textbf{85.8}  & \textbf{77.4}  & \textbf{75.5}  & 79.3           & 78.3           & 85.8           & 78.7           & 81.1  \\
Sparse $\times$ Dense   & 84.5           & \textbf{89.6}  & 75.3           & 78.9           & 85.0           & 76.1           & 75.1           & 78.6           & 76.5           & 85.7           & 78.7           & 80.4  \\
Double Dense Retrievers             & 84.5           & 88.7           & \textbf{79.5}  & 81.7           & 85.5           & 77.1           & 73.9           & 79.7           & \textbf{79.3}  & 92.2           & 78.2           & 81.9  \\ \midrule
w/o HN                  & 84.8           & 86.9           & 77.0           & 80.6           & 83.0           & 74.2           & 72.4           & 80.2           & 77.5           & 92.2           & 75.6           & 80.4  \\ 
w/ Sparse HN            & 84.2           & 83.3           & 77.4           & 80.2           & 83.8           & 71.8           & 70.3           & 78.1           & 77.4           & 89.9           & 74.7           & 79.2  \\ \bottomrule
\end{tabular}}
\label{tab:passage-lang}
\end{table*}
