\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{newtxmath}
\usepackage{bm}
\usepackage{algorithm, algorithmic}
\usepackage{cite}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{hypertex=true,
            colorlinks=true,
            linkcolor=black,
            anchorcolor=black,
            citecolor=black}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}
\begin{document}
\title{A Learning-based Adaptive Compliance Method for Symmetric Bi-manual Manipulation}
\author{Yuxue Cao$^{*}$, Shengjie Wang$^{*}$, Xiang Zheng, Wenke Ma, and Tao Zhang $^{\dag}$
\thanks{$*$ Equal contribution. $\dag$ Corresponding authors. (E-mail: taozhang@tsinghua.edu.cn)

Yuxue Cao is with Beijing Institute of Control Engineering, 100094 Beijing, China, and also with National Key Laboratory of Space Intelligent Control, 100094 Beijing, China (e-mail: sg18801138270@163.com).

Shengjie Wang and Tao Zhang are with the Department of Automation, Tsinghua University, 100084 Beijing, China (e-mail: shengjiewang@mail.tsinghua.edu.cn, taozhang@tsinghua.edu.cn).

Xiang Zheng is with the Department of Computer Science, City University of Hong Kong, China (e-mail: xzheng235-c@my.cityu.edu.hk).

Wenke Ma is with Qian Xuesen Laboratory of Space Technology, Beijing, China (e-mail: wenkema2021@163.com).}}



\maketitle

\begin{abstract}
Symmetric bi-manual manipulation is essential for various on-orbit operations due to its potent load capacity. As a result, there exists an emerging research interest in the problem of achieving high operation accuracy while enhancing adaptability and compliance. However, previous works relied on an inefficient algorithm framework that separates motion planning from compliant control. Additionally, the compliant controller lacks robustness due to manually adjusted parameters. This paper proposes a novel Learning-based Adaptive Compliance algorithm (LAC) that improves the efficiency and robustness of symmetric bi-manual manipulation. Specifically, first, the algorithm framework combines desired trajectory generation with impedance-parameter adjustment to improve efficiency and robustness. Second, we introduce a centralized Actor-Critic framework with LSTM networks, enhancing the synchronization of bi-manual manipulation. LSTM networks pre-process the force states obtained by the agents, further ameliorating the performance of compliance operations. When evaluated in the dual-arm cooperative handling and peg-in-hole assembly experiments, our method outperforms baseline algorithms in terms of optimality and robustness.
\end{abstract}

\def\abstractname{Note to Practitioners}
\begin{abstract}
The dual-arm system is conducive to handling objects with large inertia in on-orbit operations. Moreover, the peg-in-hole assembly operation is a critical stage in on-orbit assembly of large space facilities and extraterrestrial base construction. However, the previous methods have limited efficiency, requiring trajectory re-planning and adjustment of compliance controller's parameters as target position changes. Additionally, these works concentrated only on the handling operation and have rarely studied the assembly operation. Recently, reinforcement learning has shown promising results in robot manipulation tasks. However, there is no relevant research on its application in symmetric bi-manual manipulation due to the strong motion constraints and high requirements for synchronization of dual-arm control. To improve the manipulation's efficiency and robustness, we design a novel adaptive compliance algorithm based on reinforcement learning, which integrates motion planning and impedance-parameter adjustment in a unified framework. The centralized framework simultaneously considers the motion and force states of both arms, fulfilling the requirements of dual-arm synchronous control. The simulation results in two practical dual-arm cooperative tasks show that our proposed method can complete symmetric bi-manual manipulation with high efficiency, robustness, and synchronization.
\end{abstract}

\begin{IEEEkeywords}
Dual-arm manipulation, motion planning, compliance control, reinforcement learning.
\end{IEEEkeywords}


\section{Introduction}
\IEEEPARstart{T}{he} rapid development of aerospace technology promotes broad applications of space manipulators. Planning and control of manipulators play an essential role in on-orbit services, such as assembling and maintaining large space facilities, refuelling various spacecraft, and establishing extraterrestrial bases. Dual-arm systems have expanded working space, better load-carrying capacity, and potent flexibility compared with a single-arm system, making them ideal for space operations. This paper focuses on symmetric bi-manual manipulation, categorized as cooperative manipulation \cite{surdilovic2010compliance}, where both arms act simultaneously on the same object. Symmetric bi-manual manipulation is credible for handling objects with large inertia in on-orbit operations. Furthermore, performing the peg-in-hole assembly operation is urgent for the on-orbit assembly of large space facilities or the construction of extraterrestrial bases.

Symmetric bi-manual manipulation faces challenges such as inaccurate dynamic models, unsynchronized control of manipulators, and interference from the external environment. These challenges may result in fluctuations in dual-arm operating forces, leading to the object's falling or irreversible damage to the system. Additionally, the environmental contact force interacting with the object must also be considered during the manipulation. Therefore, simple motion planning and position control are not sufficient, and it is crucial to apply compliance control. Impedance control is more commonly used in compliance operations than master-slave and hybrid position-force control \cite{1998Synthesis}. However, there are two significant problems with current methods for compliance operation. Firstly, most popular methods perform motion planning first and then track the desired trajectory with impedance control. A significant disadvantage of this approach is that it is inefficient due to its two separate parts. In particular, when the target position changes, the path needs to be re-planned. Secondly, traditional impedance control requires manual adjustment of parameters, and the parameters remain fixed throughout the operation process, limiting the algorithm's robustness. While some researchers have proposed various optimization methods for selecting impedance parameters online\cite{8593853,2021Coordinated,20140717299306,20201908638489}, previous methods need prior information and extra assumptions (contact model definition). Therefore, adapting impedance parameters remains an essential problem in contact-rich operations.

In recent years, model-free reinforcement learning (RL) has shown promising results in robot manipulation tasks. We can take advantage of RL to elevate the performance of traditional impedance control. First, model-free RL methods enable a unified framework for planning and parameter adjustment, improving manipulation efficiency. Secondly, it facilitates optimization of the planning policy and impedance parameters through interaction experience, delivering a more robust solution for contact-rich manipulation. In addition, specific contact dynamics models are unnecessary for model-free methods, eliminating the need for prior information. Although some methods exist in the single-arm peg-in-hole assembly\cite{8967946,9830834,20191206663241,8868579,9361338,8594353}, none of them has yet considered symmetric bi-manual manipulation. 
% However, there are few studies on the dual-arm compliance operation, especially for symmetric bi-manual manipulation. 
Compared with single-arm manipulation, symmetric bi-manual manipulation requires higher synchronization of dual-arm control. 
% To our knowledge, previous research did not consider symmetric bi-manual manipulation when using impedance control. 
This aspect is particularly important when implementing cooperative handling and peg-in-hole assembly tasks, as we have done in this paper. However, RL-based methods have shown promise in solving the dual-arm problems mentioned above, as centralized policy networks can achieve dual-arm synchronization through training due to dense connections in networks. In conclusion, RL-based methods can facilitate symmetric bi-manual manipulation to achieve high efficiency, robustness, and synchronization.

In this paper, we propose the Learning-based Adaptive Compliance (\textbf{LAC}) algorithm for symmetric bi-manual manipulation\footnote{For experiment videos and more details, please see our project page at \href{https://sites.google.com/view/lac-manipulation}{https://sites.google.com/view/lac-manipulation}.}. The method we propose completes the dual-arm cooperative handling operation with random target positions and the peg-in-hole assembly operation with 2mm clearance in which the two arms hold the same peg. The contributions of this paper are as follows:
\begin{itemize}
\item{We design a novel algorithm framework to complete symmetric bi-manual manipulation based on reinforcement learning. The LAC generates the desired trajectory of dual-arm operations, improving motion planning efficiency. Meanwhile, it adaptively updates impedance parameters to overcome the poor robustness of impedance control.}
\item{We design a centralized Actor-Critic network structure with LSTM networks to achieve cooperative planning and impedance-parameter modification. The centralization of the framework is particularly useful in operations requiring high dual-arm synchronization. Moreover, results demonstrate that the force trend feature captured by LSTM networks significantly impacts the performance of compliance operations.}
\item{Thanks to the efficiency and robustness of LAC, we realize two typical symmetric bi-manual manipulations, the dual-arm cooperative handling operation with random target positions and the peg-in-hole assembly operation in which two arms hold the same peg.}
\end{itemize}  

The rest of this paper is organized as follows. Section II discusses the research related to our work. Section III formulates our problem and describes the general workflow for symmetric bi-manual manipulation. Section IV describes the proposed method for motion planning and impedance adaptive adjustment based on reinforcement learning. Section V details the design of the state space, action space, and reward function in the dual-arm handling and peg-in-hole assembly operations. Section VI shows and discusses the simulation results in these two practical operations. Finally, Section VII provides conclusions and future research perspectives.

\section{Related Works}
This section reviews the literature on algorithms that aim to accomplish symmetric bi-manual manipulation.

\subsection{Traditional Methods}
Out of the three compliant control methods commonly used, master-slave position-force control cannot deal with the problem that the manipulated object interacts with the environment \cite{1977Force}. On the other hand, hybrid position-force control switches between position control and force control, resulting in poor system stability as the manipulator moves from free to constrained space \cite{12073,1993Coordinated,4154704}. N. Hogan introduced mechanical impedance to compliant operations and proposed impedance control \cite{4788393,1985Impedance, Neville1985Impedance,1985Impedance1}. By adjusting the dynamic relationship between the end-effector's pose and contact force, impedance control allows the interaction of the manipulator and the external environment to exhibit compliant characteristics.

In the dual-arm cooperative handling operation, researchers such as Lei Y \cite{8593853}, Hu \cite{2021Coordinated}, and Liu \cite{s21144653} utilized impedance control to manage the manipulators' operating forces. Stanley A proposed the concept of object-impedance \cite{143355}, establishing an impedance model between the object and the environment to control their interaction force. Additionally, Caccavale proposed a double-loop impedance control method based on the above ideas \cite{4639601}. In this method, the centralized impedance controller (outer impedance) facilitates compliant contact between the object and the environment, while the decentralized impedance controller (inner impedance) controls the force between the end-effectors and the object. A host of researchers, including Heck\cite{20140717299306} and Tarbourich\cite{20201908638489}, further refined the compliant control framework for symmetric bi-manual manipulation based on \cite{4639601}.

While the algorithm presented in \cite{4639601} considers the control of internal and external forces, it is currently only applied to dual-arm cooperative handling operations. Though there are several studies on the single-arm peg-in-hole assembly \cite{WOS:000583937200001, WOS:000778988400068, WOS:000543336600003, WOS:000509585900111}, research on dual-arm assemblies is scarce and mainly involves unsymmetrical cooperative operations, where one arm holds the peg and the other holds the hole \cite{WOS:000900230000001, peg-in-hole2020yanjiang}. To our knowledge, there is no published research on the symmetrical dual-arm peg-in-hole assembly operation, where both arms hold the same peg. Due to the system's closed-chain constraint, synchronous control is critical, making such a precision operation challenging.

Most of the research cited above utilized manually calibrated impedance parameters, which cannot adapt to changes in the system or environment. Variable impedance provides an essential guarantee of robust and safe interaction between the manipulator and the environment. To control the contact force between end-effectors and the object within a desired range, several online-adjust impedance algorithms have been proposed, such as those based on nonlinear optimization \cite{8273239}, quadratic optimization \cite{8593853}, adaptive law compensation \cite{9254995}, and neural network \cite{8812496}. Researchers introduced variable impedance to the double-loop impedance method proposed in \cite{4639601} to address interactions between the object and the environment, improving the flexibility of dual-arm operation \cite{7053267,7556196,duan2018adaptive}.

\subsection{Reinforcement Learning Methods}
Although variable impedance has improved the compliance of dual-arm cooperative operation to a certain extent, previous studies followed an inefficient framework of performing motion planning first and then designing a compliance controller to track the planned trajectory. When the target position changes, the trajectory must be re-planned and tracked. In addition, adjusting impedance parameters usually requires dynamic models of the contact process. However, establishing contact dynamic models often takes much work and can be inaccurate due to time-varying interactions. Therefore, there exist great limitations in realizing compliance control by establishing dynamic models \cite{5686290}.

Model-free reinforcement learning does not require modelling the dynamics of contact states but learns the policy from the interaction experience between the agent and the environment. Its adaptability and generalization improve the efficiency of motion planning and the compliance of dual-arm operations. In the early stage, model-free reinforcement learning has been used for single-arm compliant operations. Freek Stulp used the $PI^2$ algorithm to learn variable impedance, adapting to both deterministic and stochastic force fields \cite{6227337}. With the development of neural networks, deep reinforcement learning has further improved the agent's exploration and decision-making, promoting the use of reinforcement learning in robot manipulation tasks.

Roberto Martín compared the performance of variable impedance control based on reinforcement learning in three typical operations: path-tracking (non-contact task), door-opening (motion-constrained task), and surface-wiping (continuous contact task)  \cite{8968201}.
Cristian C. B applied reinforcement learning to single-arm peg-in-hole assembly, which utilized TCN and MLP networks to pre-process observations \cite{20200595648}. Several references, including \cite{8967946,9830834,20191206663241,8868579,9361338,8594353}, focused on adjusting impedance parameters to reduce the contact force between the peg and the hole and the assembly time.

Although reinforcement learning has made remarkable achievements in single-arm operations, few studies have focused on dual-arm operations. Marvin Alles proposed a reinforcement learning framework with a centralized policy network and two decentralized single-arm controllers \cite{20210361614}. Its effectiveness was verified in the asymmetric peg-in-hole assembly. However, as far as we know, no study has yielded the effect of reinforcement learning in symmetric bi-manual manipulation with strong kinematics and dynamics constraints. The closed-chain system requires higher synchronization of dual-arm control, which brings new challenges to the application of reinforcement learning. In this work, we design a novel adaptive compliance algorithm based on reinforcement learning to enhance the efficiency and adaptability of symmetric bi-manual manipulation. The algorithm can complete dual-arm cooperative handling and peg-in-hole assembly operations.

\section{Problem Formulation}
This section formulates the closed-chain model of the dual-arm system and the impedance control, and provides details of the general workflow for symmetric bi-manual manipulation.
\subsection{Kinematic and Dynamic Model of the Dual-Arm Closed-Chain System}
\subsubsection{Kinematic Model}
Fig. \ref{KDmodel} shows the kinematic and dynamic model of the dual-arm closed-chain system. The coordinate systems in Fig. \ref{KDmodel} are defined as: $O_L$ represents the object's coordinate system, $O_{b}^i$ and $O_{e}^i$ represent the base and end-effector coordinate system of the i-th manipulator respectively, and $O_W$ represents the world coordinate system.
\begin{figure}[!t]
  \centering
  \includegraphics[width=\hsize]{figs/DualArm.jpg}
  \caption{Closed-chain model of the dual-arm robot.}
  \label{KDmodel}
\end{figure}

To maintain a stable connection between the end-effectors and the object, the whole movement process must comply with the closed-chain kinematics constraints formulated as follows:
\begin{equation}
\label{eq1}
{T^{{b}^i}_{{e}^i}}={T^{{b}^i}_{W}}T^{W}_{L}{T^L_{{e}^i}}
\end{equation}
where $T^B_A$ represents the transformation matrix of the $A$ coordinate system relative to the $B$ coordinate system. $T^L_{{e}^i}$ depends on the position of the contact point between the end-effector and the object.

\subsubsection{Dynamic model}
In Fig. \ref{KDmodel}, $f_L$ and $t_L$ are the external force and moment exerted on the object by the environment. $f_{e}^i$ and $\tau_{e}^i$ are the force and moment applied to the object by the i-th manipulator. $r_L$, $r_1$, $r_2$ are vectors from the force points of $f_L$, $f_{e}^1$, $f_{e}^2$ to the object's centroid. $v_L$ and $w_L$ are the velocity and angular velocity of the object. $M_L$, $I_L$ and $G_L$ are the object's mass, moment of inertia and gravity, respectively. The dynamic model of the dual-arm closed-chain system can be formulated as:
\begin{equation}
\label{eq2}
\begin{split}
\left[\begin{matrix}E&O\\\left(r_1\right)^\ast&E\\\end{matrix}\right]\left[\begin{matrix}f_{e}^1\\\tau_{e}^1\\\end{matrix}\right]+\left[\begin{matrix}E&O\\\left(r_2\right)^\ast&E\\\end{matrix}\right]\left[\begin{matrix}f_{e}^2\\\tau_{e}^2\\\end{matrix}\right]+\left[\begin{matrix}G_L\\O\\\end{matrix}\right]\\+\left[\begin{matrix}E&O\\\left(r_L\right)^\ast&E\\\end{matrix}\right]\left[\begin{matrix}f_L\\\tau_L\\\end{matrix}\right]=\left[\begin{matrix}m_L\dot{v}_L\\I_L{\dot{\omega}}_L+\omega_L\times\left(I_L\omega_L\right)\\\end{matrix}\right]
\end{split}
\end{equation}

Eq. \ref{eq2} can be simplified to Eq. \ref{eq3}, where the generalized force $F$ represents the vector composed of force and moment. 
\begin{equation}
\label{eq3}
-\Gamma_1F_{e}^1-\Gamma_2F_{e}^2=-{\overline{G}}_L-\Gamma_LF_L+F_{IL}
\end{equation}

In symmetric bi-manual manipulation, the motion planning algorithms generate the desired trajectory for the object, which, in turn, determines $F_{IL}$. Thus, the primary challenge of this problem is to decompose the desired generalized force of the object's centroid into the desired generalized forces of the end-effectors. This paper refers to the shared force mode proposed in \cite{yan2016coordinated}, obtaining the desired generalized forces of the end-effectors as follows:
\begin{equation}
\label{eq4}
\left[\begin{matrix}F_{e}^1\\F_{e}^2\\\end{matrix}\right]=\left[\begin{matrix}-\Gamma_1&-\Gamma_2\\\end{matrix}\right]^{\#}\left(-{\overline{G}}_L-\Gamma_L F_L+F_{IL}\right)
\end{equation}
Where $\#$ represents the pseudo inverse of the matrix.

\subsection{Position-based Impedance Control}
The position-based impedance control converts force errors into position adjustment, and realizes force control by adjusting the desired trajectory. The second-order dynamic model between force and position is:
\begin{equation}
\label{eq5}
M_d{(\ddot{X}-\ddot{X}}_d)+B_d(X-{\dot{X}}_d)+K_d(X-X_d)=F-F_d
\end{equation}
where $M_d$, $B_d$ and $K_d$ are the positive definite matrices of desired inertia, damping and stiffness of the impedance model, respectively. The diagonal matrix is usually selected to obtain the linear decoupling response. $\ddot{X}$, $\dot{X}$, $X$ represent the actual acceleration, velocity and position, respectively. ${\ddot{X}}_d$, ${\dot{X}}_d$, $X_d$ are the desired acceleration, velocity and position. $F_d$ and $F$ are the desired and the actual contact force, respectively.

The Lagrangian transformation and discretization are applied to Eq. \ref{eq5} to deduce position adjustment from force errors, as Eq. \ref{eq6} shows.
\begin{equation}
\begin{split}
\label{eq6}
\delta X\left(t\right)=\frac{T^2}{\omega_1}\left(E\left(t\right)+2E\left(t-1\right)+E\left(t-2\right)\right)-\\\frac{\omega_2}{\omega_1}\delta X\left(t-1\right)-\frac{\omega_3}{\omega_1}\delta X\left(t-2\right)
\end{split}
\end{equation}
where $T$ is the sampling period, $\omega_1=4M_d+2B_dT+K_dT^2$, $\omega_2=-8M_d+2K_dT^2$, and $\omega_3=4M_d-2B_dT+K_dT^2$.

Then the actual trajectory at time $t$ is:
\begin{equation}
\label{eq7}
X\left(t\right)=X_d\left(t\right)+\delta X\left(t\right)
\end{equation}

\subsection{Workflow of Symmetric Bi-manual Manipulation}
The main issue for symmetric bi-manual manipulation is controlling the interaction force between end-effectors and the object. Through force control, the object can follow the desired motion and satisfy the closed-chain constraints. The algorithm must also consider external force control to prevent violent collisions between the object and the environment. In this paper, we extend the double-loop impedance method proposed in \cite{4639601} and adapt it to symmetric bi-manual manipulation. The manipulation workflow is illustrated in Fig. \ref{Framework}(a).

\begin{figure*}
  \centering
  \includegraphics[width=\hsize]{figs/Framework.jpg}
  \caption{Algorithm framework of \textbf{LAC}. (a) Workflow of symmetric bi-manual manipulation consists of three parts. (b) Adaptive compliance algorithm based on reinforcement learning.}
  \label{Framework}
\end{figure*}

The workflow of symmetric bi-manual manipulation comprises three parts. The motion planning model generates the object's desired trajectory, denoted as $X_{d}^o$. Next, the outer impedance adjusts the object's position to minimize the external force, denoted as $F_{env}$, which the environment exerts on the object. The second-order impedance model can be rewritten as Eq. \ref{eq8} for the outer impedance.

\begin{equation}
\label{eq8}
M_{d}^o{\ddot{X}}_{e}^o+B_{d}^o{\dot{X}}_{e}^o+K_{d}^oX_{e}^o=F_{env}
\end{equation}
where $X_{e}^o=X_o-X_{d}^o$ is the adjustment to the object's desired trajectory $X_{d}^o$. The complaint desired trajectory $X_o$ decreases the interactive environmental force.

The second part pertains to the dual-arm closed-chain kinematics and dynamics model. This part calculates the desired trajectory $X_{d}^i$ and the desired force $F_{d}^i$ of the manipulator according to Eq.\ref{eq1} and Eq. \ref{eq4}, respectively. 

Once $X_{d}^i$ and $F_{d}^i$ are calculated, the inner impedance and single-arm control are implemented to drive the motion of manipulators. The inner loop's second-order impedance model is formulated as:
\begin{equation}
\label{eq9}
M_{d}^i{\ddot{X}}_{e}^i+B_{d}^i{\dot{X}}_{e}^i+K_{d}^iX_{e}^i=F_{e}^i
\end{equation}
where $X_{e}^i=X_i-X_{d}^i$ is the adjustment to the desired trajectory of the i-th manipulator's end-effector $X_{d}^i$. $F_{e}^i=F_{d}^i-F_i$ is the error between the end-effector's desired force and the actual contact force.

\section{Adaptive Compliance Algorithm Based on Reinforcement Learning}
This section discusses designing the adaptive compliance algorithm, which merges reinforcement learning with the workflow of symmetric bi-manual manipulation featured in the prior section.
\subsection{Reinforcement Learning}
The process of reinforcement learning can be described as a Markov decision process (MDP). The MDP process comprises five components: $<S,\ A,\ R,\ P,\ \rho_0>$. The agent gets the environment state $s_t \in S$ and generates the action $a_t \in A$ using the current policy $\pi(s_t)$. Once $a_t$ is performed, the environment transitions to a new state $s_{t+1}$, and the agent obtains a reward $r_t$. Reinforcement learning endeavors to find the optimal policy $\pi^*$ that yields the maximum expected reward value.

For this paper, we utilize the Soft Actor-Critic (SAC) algorithm. Unlike other deep reinforcement learning algorithms, SAC takes into account both the reward value and policy entropy maximization. Maximizing policy entropy facilitates the exploration of policy, thereby allowing the algorithm to find the optimal solution more effectively in high-dimensional optimization problems. In addition, as an off-policy algorithm, SAC stores the empirical samples in the replay buffer for later network training, which improves the sample efficiency. We implemented our SAC algorithm based on rlkit, a deep reinforcement learning library that uses Pytorch 2.0 (https://github.com/rail-berkeley/rlkit.git).


\subsection{Learning-based Adaptive Compliance(\textbf{LAC}) Algorithm}
We design an adaptive compliance algorithm based on reinforcement learning, which aims to enhance the efficiency of motion planning and symmetric bi-manual manipulation compliance.
\subsubsection{Algorithm Framework}
Fig.\ref{Framework} displays the algorithm framework of the learning-based adaptive compliance method for symmetric bi-manual manipulation. It is a centralized framework comprising two components. The high-level module, which runs reinforcement learning at 20 Hz, provides the object's desired trajectory and impedance parameters. Additionally, it acquires the states of two manipulators simultaneously. On the other hand, the low-level module involves impedance control operating at a frequency lower than the high-level module. The slower control frequency of reinforcement learning allows the agent to process environmental states and generate the next action. In contrast, the higher frequency of the low-level module ensures precise and prompt control of the manipulator.

Compared to motion planning problems, symmetric bi-manual manipulation involves force information. The states are classified into two types: motion states and force states. The former includes the motion information of the manipulators and the object, such as joint angles, joint angular velocities, and centroid velocity. The latter refers to the operating force of the manipulator and the contact force between the object and the environment. Unlike motion states restricted by motion limitations, force states can change with large amplitude and rapid rate. Hence, inputting the two states into the neural network as a whole state vector is inappropriate. Consequently, the states require pre-processing before being used by the agent. The next subsection provides detailed information on the pre-processing.

The reinforcement learning policy outputs $\left\{a_x,a_p\right\}$, where $a_x\in R^3$ corresponds to the object's position change $[\Delta x, \Delta y, \Delta z]$ and represents the motion planning module. On the other hand, $a_p=[B_d, K_d] \in R^6$ corresponds to the desired damping matrix and stiffness matrix diagonal values of the impedance controller. The desired inertia matrix remains constant because it significantly affects the system's stability. Furthermore, to maintain the control system's stability, the impedance parameters' upper and lower limits are defined as hyper-parameters of the algorithm, $a_{p}^{max}$ and $a_{p}^{min}$, respectively. In the low-level module, the impedance controller with variable parameters modifies the desired trajectory $X_{d}$ output by the policy, resulting in compliant operations.

The adaptive compliance algorithm based on reinforcement learning is adaptable to different symmetric bi-manual manipulations. Different scenarios should select and design appropriate state space, action space and reward function.

\subsubsection{Neural Network Architecture}
The agent obtains environmental states at the same frequency as the policy outputs actions. As mentioned earlier, environmental states $s_t$ are divided into motion states $m_t$ and force states $f_t$. Because the manipulator's motion constraints limit the change rate of $m_t$, the states between two moments have strong continuity. $f_t$ are related to current contact dynamics and hence show intense volatility. Moreover, adjusting the sub-goal position and impedance parameters should consider the current force information and its trend. Therefore, we enable the agent to obtain the force states at the low-level frequency to enhance the states' continuity and promote better algorithm performance. Thus, we can get the force states time series for each moment.

However, directly taking the force states time series as the input increases the state space dimension, making training more challenging. Moreover, the Multi-layer Perceptron (MLP) is not adept at extracting time-series features. Unlike MLP, Long Short-Term Memory (LSTM) establishes connections between nodes of different hidden layers. This network structure enables it to use past moments' information to infer the future moment's state. LSTM is widely used to predict time series.

This paper introduces LSTM networks to the original deep reinforcement learning MLP network architecture, intending to process the force states sequence. Assuming that the low-level module impedance controller's control frequency is $h$, the agent obtains force states as a time series with a length of $h/20$. At each moment, the sequence, which has a shape of $[h/20, 6]$, is input into LSTM networks. A fully connected layer then processes the output of the LSTM networks to obtain the feature vector of the force states. After that, the motion states are connected with the pre-processed force feature vector, which is then input into the agent's neural networks.

The algorithm pseudo-code of our approach is available in Algorithm \ref{alg:1}.

\begin{algorithm}[]
	\renewcommand{\algorithmicrequire}{\textbf{Input:}}
	\renewcommand{\algorithmicensure}{\textbf{Output:}}
	\caption{Learning-based Adaptive Compliance(LAC)}
	\label{alg:1}
	\begin{algorithmic}[1]
	    \STATE Randomly initialize the parameters of the state pre-process networks(LSTM, MLP) and the agent networks(actor, two critic) with $l_{a},l_{c}^1,l_{c}^2, w_{a},w_c^1,w_c^2, \phi,\theta_{1}, \theta_{2}$
	    \STATE Initialize replay buffer $\mathcal D$, the frequency of the high-level module $h_{h}$ and low-level module $h_{l}$
	    \STATE Initialize the parameters of target network with ${l^i_{c}}^{\prime}, {w^i_{c}}^{\prime}, \theta^\prime_{i} \leftarrow l_{c}^i, w_{c}^i, \theta_{i}, i=1,2$
	    \FOR {episode $m = 1, M $}
	     \STATE Sample initial state $s_0$ 
	    \FOR {step $t=0, T-1$} 
	    \STATE Divide $s_t$ into $m_t$ and $f_t$, and input $f_t$ into LSTM networks to get the force feature vector $v_t$
	    \STATE Combine $m_t$ with $v_t$ to obtain the feature vector $c_t$
	    \STATE Sample an action $a_t=\left\{a_{x}^t,a_{p}^t\right\}$ from $\pi_\phi(a_t|c_t)$
            \FOR {step $t=0, h_{l}/h_{h}-1$}
            \IF {outer impedance}
	    \STATE Set outer impedance parameters to be $a_{p}^t$, and use $a_{x}^t$ to calculate $X_o$ according to Eq.\ref{eq8}
            \STATE Use $X_o$ to calculate $X_{d}^i$ and $F_{d}^i$ of the end-effectors according to Eq.\ref{eq1} and Eq.\ref{eq4}
            \ELSIF{inner impedance}
            \STATE Use $a_{x}^t$ to calculate $X_{d}^i$ and $F_{d}^i$ of the end-effectors according to Eq.\ref{eq1} and Eq.\ref{eq4}
            \STATE Set inner impedance parameters to be $a_{p}^t$
            \ENDIF
	    \STATE Calculate $X_i$ according to Eq.\ref{eq9}, and convert it to the desired joint angles ${\theta_{d}^t}$ to control the manipulator
            \ENDFOR
	    \STATE observe a new state $s_{t+1}$and store $<s_t,a_t,r_t,s_{t+1}>$ into $\mathcal{D}$
	    \ENDFOR
	    \FOR{iteration $n = 1, N$}
	    \STATE Sample a minibatch $\mathcal B$ from replay buffer $\mathcal D$
	    \STATE Update $l_{c}^i, w_{c}^i, \theta_{i}, i=1,2$ using minibatch $\mathcal B$
	    \STATE Update $l_{a}, w_{a}, \phi$ using minibatch $\mathcal B$
	    \STATE Update the parameters of target networks, ${l^i_{c}}^\prime, {w^i_{c}}^\prime, \theta^\prime_{i}, i=1,2$
	    \ENDFOR
	    \ENDFOR
	\end{algorithmic}
\end{algorithm}

\section{Formulation of two practical scenarios}
This section details the application of the LAC algorithm in two typical operations in on-orbit construction or assembly. It delves into the state space, action space, and reward function design for the two practical scenarios.
\subsection{Dual-arm Cooperative Handling}
Figure \ref{SimEnv}(a) depicts the simulation environment of the dual-arm cooperative handling operation, which involves carrying an object to the target position with two manipulators. This task is ideal for handling materials with large inertia. During the operation, we assume no interaction force exists between the object and the external environment ($F_{env}=0$). As a result, $X_o$ equals $X_{d}^o$, and we only need to consider the inner impedance control between the dual-arm end-effectors and the object.

The agent's observations include the manipulators' joint angles $\theta_{1,2}\in R^6$, joint angular velocities ${\dot{\theta}}_{1,2}\in R^6$, end-effectors' poses $P_{e}^{1,2},\Phi_{e}^{1,2}\in R^6$, velocities $v_{e}^{1,2},\omega_{e}^{1,2}\in R^6$, contact force $F_{1,2}\in R^3$, and the pose of the object's centroid $P_o,\Phi_o\in R^6$. Additionally, the error vector between the current position and the target position of the object is included in the state space to enhance learning efficiency. The output of the policy includes the position variation $[\Delta x, \Delta y, \Delta z]$ of the object's centroid, as well as the desired damping $B_{d}^{1,2}\in R^3$ and stiffness $K_{d}^{1,2}\in R^3$ of the inner impedance controller.

The main objective of dual-arm cooperative handling is to safely transport the object to a target position while maintaining a stable connection between end-effectors and the object. Therefore, the reward function should aim to minimize the error between the object's current position and target position to generate a feasible motion path $X_{d}^o$. Once the desired trajectory has been obtained, the desired contact force $F_{d}^i$ of the end-effector can be calculated according to Eq. \ref{eq4}. By transforming the dual-arm cooperative handling operation into the problem of tracking $F_{d}^i$, the objective becomes minimizing the error between the actual contact force $F_i$ and the desired force $F_{d}^i$. Additionally, we set two force thresholds $F_{min}$ and $F_{max}$. $F_{min}$ is the minimum force to keep the end-effector and the object firmly connected. $F_{max}$ is the maximum force to ensure that the object and the manipulators are not damaged. In summary, the reward function is designed as:
\begin{equation}
\label{eq10}
r = \left\{ 
\begin{aligned}
&1-(tanh(|d|)+tanh(F_{e}^i))/2, \enspace F_{min} \leq F_{in} \leq F_{max} \\
&-3 , \enspace F_{in} \leq F_{min} \ or \ F_{in} \geq F_{max}
\end{aligned}
\right.
\end{equation}
where $\left|d\right|$ is the Euclidean distance from the object centroid's actual position to the target position. $ F_{e}^i=F_{d}^i-F_i$ is the error between the end-effector's actual force and the desired force.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\hsize]{figs/SimulationEnv.jpg}
  \caption{Simulation environment. (a) Dual-arm cooperative handling operation. (b) Dual-arm cooperative peg-in-hole assembly operation.}
  \label{SimEnv}
\end{figure}

\subsection{Dual-arm Cooperative Peg-in-hole Assembly}
Fig. \ref{SimEnv}(b) depicts the simulation environment of the dual-arm cooperative peg-in-hole assembly operation. After moving the object to the desired position, the peg-in-hole assembly operation is performed to fasten it in place. Normally, this operation comprises two phases: searching and insertion. Assuming that the hole position is known and the peg is near the hole, this paper focuses on the insertion phase. The dual-arm cooperative peg-in-hole assembly operation aims to insert the peg into the hole by adjusting the object's position using interaction force feedback. During this process, the interaction force between the assembly parts must be regularly controlled within restricted thresholds to prevent irreversible damage. Furthermore, given the precision involved in this operation, the inner impedance controller is equipped with high fixed stiffness and damping parameters, thereby prompting a faster response. As a result, reinforcement learning is only concerned with the outer impedance control related to the peg and hole interaction.

The agent's observations differ slightly from the handling operation. The algorithm primarily focuses on the interaction states between the peg and hole during assembly. As a result, the state space omits the manipulators' joint information but still contains other relevant data used in the handling operation. Additionally, the state space includes the contact force between the peg and the hole ($F_{env}\in R^3$) and the peg insertion depth ($d$). The policy outputs the object's centroid positional variation $[\Delta x, \Delta y, \Delta z]$, as well as the desired damping $B_{d}^o\in R^3$ and stiffness $K_{d}^o\in R^3$ of the outer impedance controller.

To improve learning efficiency in dual-arm assembly, we adopt a hybrid policy that leverages the knowledge of the PD position error controller. The object's desired trajectory ($X_{d}^o$) is the sum of the PD position controller's output ($X_g$) and the reinforcement learning policy's output ($a_x$), expressed as $X_{d}^o=X_g+a_x$. The hybrid policy accelerates the search process before the peg contacts the hole. With this approach, reinforcement learning can concentrate on the insertion process and finely adjust the peg's position by using the PD controller's information.

When designing the reward function, the peg’s insertion depth is the primary indicator of successful assembly. Additionally, minimizing the interaction force between the peg and hole reduces wear between parts during insertion. However, to prevent the agent from avoiding insertion to decrease the contact force, we decrease the weight of contact force in the reward function. Completion time is another performance indicator. The reward value increases as the completion time shortens, encouraging the agent to complete the task quickly. Therefore, the reward function is structured as follows. 
\begin{equation}
\label{eq12}
r = \left\{ 
\begin{aligned}
&1.05-tanh(d)-0.05 \times tanh(F_{env}), \enspace F_{env} \leq F_{max} \\
&100+((1-t/T) \times 100), \enspace d \geq d_{s} \\
&-10, \enspace          Not \enspace safe 
\end{aligned}
\right.
\end{equation}
where $d_{s}$ is the target insertion depth. Once the peg insertion depth $d$ exceeds the target depth, we consider the task completed and give the agent a large success reward. When safety problems occur, we will penalize the agent with -10 and terminate the current episode. The safety problems include two aspects: 1) the interaction force between the peg and the hole exceeds the defined threshold $F_{max}$; 2) the closed-chained constraint is not satisfied during the assembly process, causing the object to fall away.


\section{Simulation Results And Analysis}
\subsection{Simulation Environment}
We build simulation environments for dual-arm cooperative handling and peg-in-hole assembly using Mujoco, a widely used simulation platform in reinforcement learning. Fig. \ref{SimEnv} displays the two simulation environments, where the manipulator's end-effector is a plane clamping tool. Additionally, the force information $F_{e}^i$ is obtained via force/torque sensors. For the dual-arm cooperative handling operation, the target position of the object's centroid is located in a $[0.1m, 0.3m, 0.12m]$ cube. The hyperparameters of \textbf{LAC} are illustrated in Table \ref{hp of LAC}.

\begin{table}
\begin{center}
\caption{Hyperparameters of \textbf{LAC}}
\label{hp of LAC}
\begin{tabular}{|c|c|}
\hline
Hyperparameters& LAC\\
\hline
LSTM/MLP network & 16/3 \\
Actor network & (256,256) \\
Critic network & (256,256) \\
Learning rate of actor & 1.e-3  \\
Learning rate of critic & 5.e-4  \\
Optimizer & Adam \\
ReplayBuffer size& $10^6$ \\
Discount ($\gamma$)& 0.995 \\
Polyak ($1-\tau$)& 0.995 \\
Batch size & 128 \\
Length of an episode & 200 steps \\
Maximum steps & 1e6 steps \\
\hline 
\end{tabular}
\end{center}
\end{table}

We argue that our algorithm's performance improvement depends on three key factors: utilizing impedance control as the low-level module, variable impedance parameters, and pre-processing force states with LSTM networks. Ablation experiments are conducted on two typical dual-arm symmetric manipulation scenarios to verify the importance of these factors in the designed method. We provide the abbreviation of each algorithm: the algorithm without impedance controller (\textbf{LAC w/o Imp}), the algorithm with fixed impedance parameters (\textbf{LAC w/ fixed Imp}) and the algorithm without LSTM networks (\textbf{LAC w/o LSTM}).

\subsection{Dual-arm Cooperative Handling}
\subsubsection{Ablation Studies}
Fig.\ref{MoveR} displays the average reward curve during the training of different algorithms. The results illustrate that \textbf{LAC} attains the highest reward value among the four algorithms as it converges. On the other hand, the reward value of \textbf{LAC w/o Imp} remains low, implying that the manipulator's position control alone cannot complete the handling operation.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\hsize]{figs/Move_MeanReward2.png}
  \caption{Mean reward curve during the training of different algorithms for the dual-arm handling operation.}
  \label{MoveR}
\end{figure}

Table \ref{MoveSuccess} shows the success rate, the average force errors throughout the handling process, and the object's position error at the last time step of different algorithms attempting to reach random target positions in the test environment. \textbf{LAC w/o Imp} cannot execute the handling operation, as the object falls during the movement while testing its model. Therefore, the table only displays the test results of the other three algorithms. Our method achieves the highest success rate of $96\%$. As shown in the table, the average force errors of the three algorithms are similar. However, the object's final average position error of \textbf{LAC} is much lower than the other two algorithms, indicating better accuracy.

\begin{table}
\begin{center}
\caption{Test success rate and average force/position errors of different algorithms.}
\label{MoveSuccess}
\begin{tabular}{|c|c|c|c|}
\hline
Algorithms & Success & Force err(Arm1$\:$/$\:$2),N & Pos err, mm\\
\hline
\textbf{LAC(ours)} & \bm{$96\%$} & 2.385{\tiny(2.884)} / 2.494{\tiny(2.886)} & \textbf{0.881{\tiny(1.587)}}\\
LAC w/ fixed Imp & $82\%$ & \textbf{2.066{\tiny(3.289)} / 2.196\tiny{(3.301)}} & 3.722\tiny{(3.018)} \\
LAC w/o LSTM & $88\%$ & 2.911{\tiny(2.987)} / 3.033\tiny{(2.989)} & 2.177\tiny{(1.895)} \\
\hline 
\end{tabular}
\end{center}
\end{table}

\subsubsection{Robustness Studies}
Fig. \ref{MovePosE} demonstrates the change of position error between the object's centroid and the target position. Fig. \ref{MoveForceE} displays the manipulators' operating force errors during handling. At the initial moment, the end-effectors' poses satisfy the closed-chain constraint. As no force has been applied to the object, the force error is high on the Y-axis (the clamping direction). However, the inner impedance control rapidly reduces this error to below 5N when $timestep=10(t=0.5s)$, creating a stable connection between the end-effectors and the object. The force errors in all three directions remain within the desired threshold throughout the handling. When $timestep=200$, the object reaches the target position, and the entire system converges to a stable state after a short period of oscillation.

To verify the robustness of \textbf{LAC}, we subject the manipulators to random interference torques during the test experiment when $timestep=50$ and $timestep=200$, respectively. The amplitude of the interference torques is set at half the current joint torques. As shown in Fig. \ref{MoveForceE}, the results reveal that the torque disturbance causes pulse fluctuations in the force errors. However, the fluctuations are rapidly eliminated and do not affect the object's position, verifying the robustness of our method in the event of interference.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\hsize]{figs/Move_Err.jpg}
  \caption{Position errors between the object's centroid and the target position throughout the handling process.}
  \label{MovePosE}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[width=\hsize]{figs/Move_Force_Err.jpg}
  \caption{Operating force errors of manipulators throughout the handling process.(a) Left arm force errors. (b) Right arm force errors.}
  \label{MoveForceE}
\end{figure}

Fig. \ref{MoveSim} displays the screenshots of the handling operation at several representative times. The green cube represents the target position. Videos can be found at \href{https://sites.google.com/view/lac-manipulation}{https://sites.google.com/view/lac-manipulation}.
\begin{figure}[!t]
  \centering
  \includegraphics[width=\hsize]{figs/MoveSim.jpg}
  \caption{Video snapshot of the dual-arm cooperative handling process.}
  \label{MoveSim}
\end{figure}


\subsection{Dual-arm Cooperative Assembly}
\subsubsection{Ablation Studies}
Fig. \ref{PegReward} depicts the average reward curve during the training of different algorithms. Our algorithm displays a more stable and larger convergence value compared to the others. The reward value of \textbf{LAC w/o Imp} increases initially but declines rapidly and ultimately maintains a low value. This result indicates that the interaction force between the peg and the hole exceeds the acceptable range without outer impedance. \textbf{LAC w/ fixed Imp} only outputs the position change of the object's centroid, which has a lower action space dimension than the other algorithms. \textbf{LAC w/o LSTM} does not need to learn the parameters of LSTM networks. Therefore, the reward values of these two algorithms rise more rapidly than \textbf{LAC} at the initial stage. However, \textbf{LAC w/ fixed Imp} cannot real-time adjust the impedance relationship between the peg and the hole. \textbf{LAC w/o LSTM} only considers the current force rather than the trend of force change. Thus, their average reward values fluctuate violently during the training.  

\begin{figure}[!t]
  \centering
  \includegraphics[width=\hsize]{figs/Peg_MeanReward2.png}
  \caption{Mean reward curve during the training of different algorithms for the dual-arm peg-in-hole assembly operation.}
  \label{PegReward}
\end{figure}

\subsubsection{Evaluation of the Generalization}
We design two test scenarios to verify the generalization of our algorithm in dual-arm cooperative peg-in-hole assembly.

The first scenario is to verify the effectiveness of \textbf{LAC} in the presence of a position error in the assembly environment. Specifically, a random error ranging from 1mm to 5mm is added to the hole centre on the y-z plane. Furthermore, the PD position error controller and the agent cannot obtain this error information. In 20 test experiments, our algorithm has the highest number of successful attempts (18), followed by \textbf{LAC w/o LSTM} (14), and \textbf{LAC w/ fixed Imp} with the minimum (13). Fig. \ref{PegErr} visually shows the mean and standard deviation of the three algorithms on the maximum environmental contact force and assembly steps. The results indicate that our method better balances assembly speed and contact force. Although the average assembly time of \textbf{LAC w/o LSTM} is slightly lower than ours, the average contact force it generated is larger. On the other hand, the average maximum contact force and assembly time of \textbf{LAC w/ fixed Imp} are the largest because it cannot adjust impedance parameters according to the current interaction states.


\begin{figure}[!t]
  \centering
  \includegraphics[width=\hsize]{figs/PegErr.jpg}
  \caption{Mean and standard deviation of the maximum environmental contact force and assembly steps in the test environment with a random error ranging from 1mm to 5mm of the hole centre on the y-z plane. (For better visualization, the force and assembly steps are normalized by the maximum contact force of 20N and the timesteps of an episode of 300 steps.)}
  \label{PegErr}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[width=\hsize]{figs/PegInsert.jpg}
  \caption{(a) Peg insertion depth during the assembly process. (b) Contact force between the peg and the hole during the assembly process.}
  \label{PegInsert}
\end{figure}

The other is to test the adaptivity of our algorithm in various material environments. Unlike moderate stiffness and damping of the hole in training, we set up four test environments with large (small) stiffness and large (small) damping. Table \ref{PegForce} and Table \ref{PegStep} display the maximum environmental contact force and assembly steps of different algorithms in varying stiffness and damping test environments. The tables show that the variable impedance has excellent adaptability in large stiffness environments. In comparison, the maximum contact force of \textbf{LAC w/ fixed Imp} exceeds the specified safety threshold and cannot complete assembly operation in large stiffness environments. All three algorithms show an increase in maximum contact force in environments with large damping. Our algorithm's assembly steps demonstrate remarkable stability in different environments, while the assembly steps of the other two algorithms increase to varying degrees.

\begin{table}
\begin{center}
\caption{Max contact force of different algorithms in different stiffness and damping test environments.}
\label{PegForce}
\begin{tabular}{|c|c|c|c|c|}
\hline
Algorithms & $K_l, B_l$/N & $K_l, B_s$/N & $K_s, B_l$/N & $K_s, B_s$/N\\
\hline
\textbf{LAC(ours)} & \textbf{12.776}  & \textbf{5.454} & \textbf{13.668} & \textbf{3.260}\\
LAC w/ fixed Imp & 27.366 & 21.812 & 13.962 & 8.815\\
LAC w/o LSTM & 14.447 & 7.063 & 14.620 & 7.597 \\
\hline 
\end{tabular}
\end{center}
\end{table}

\begin{table}
\begin{center}
\caption{Assembly steps of different algorithms in different stiffness and damping test environments.}
\label{PegStep}
\begin{tabular}{|c|c|c|c|c|}
\hline
Algorithms & $K_l, B_l$ & $K_l, B_s$ & $K_s, B_l$ & $K_s, B_s$\\
\hline
\textbf{LAC(ours)} & \textbf{124}  & \textbf{128} & \textbf{127} & \textbf{123}\\
LAC w/ fixed Imp & None & None & 201 & 127\\
LAC w/o LSTM & 142 & 138 & 147 & 130 \\
\hline 
\end{tabular}
\end{center}
\end{table}

\subsubsection{Results}
Fig. \ref{PegInsert} shows the peg insertion depth and contact force between the peg and hole during the assembly. The test environment is the same as the training environment. The plot indicates that collision occurs between the peg and the hole when $timestep=30$. However, with the outer impedance control, the contact force on the X-axis (the impact direction) rapidly decreases, and the peg continues to insert into the hole. The contact forces on the X and Z axes remain within a small range throughout the insertion. In the final insertion stage, the force on the Y-axis notably increases at $timestep=90$, but it decreases from $timestep=110$ with the adjustment of \textbf{LAC}. The peg eventually reaches the target insertion depth at $timestep=123$.

Fig. \ref{PegSim} displays the screenshots of assembly operation at several representative times. Videos can be found at \href{https://sites.google.com/view/lac-manipulation}{https://sites.google.com/view/lac-manipulation}.
\begin{figure}[!t]
  \centering
  \includegraphics[width=\hsize]{figs/PegSim.jpg}
  \caption{Video snapshot of the dual-arm cooperative peg-in-hole assembly operation.}
  \label{PegSim}
\end{figure}

\section{Conclusion}
The current framework used for completing symmetric bi-manual manipulation is inefficient and lacks adaptability. The framework performs motion planning first and then tracks the desired trajectory with a compliance controller. Furthermore, due to the difficulty in establishing contact dynamics and the high requirements for dual-arm synchronous control,  no research has particularly focused on the dual-arm cooperative peg-in-hole operation, wherein two arms hold the same peg. To address these challenges, we propose the Learning-based Adaptive Compliance (LAC) algorithm, which incorporates the advantages of reinforcement learning to improve the flexibility, adaptability, and robustness in symmetric bi-manual manipulation.

Simulation results demonstrate that the proposed algorithm exhibits significant flexibility in transporting the object to random positions within a specific space without the need to re-plan the trajectory. Moreover, it facilitates dual-arm cooperative control with considerable environmental adaptability in high-precision assembly operations. In the future, we intend to consider the object's attitude adjustment, thus further expanding the application of LAC in dual-arm cooperative operations.

%Bibliography
\bibliographystyle{unsrt}  
\bibliography{main}


\end{document}


