\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{media/}}     % organize your images and other figures under media/ folder

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{Running Title for Header}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
\title{RetinaNet: Reservoir-Enabled Time Integrated Attention Network for Event-based Video Processing}

\author{
  Sangmin Yoo, Eric Yeu-Jer Lee, Ziyu Wang, Xinxin Wang, Wei D. Lu\thanks{Corresponding author} \\
  Department of Electrical Engineering and Computer Science \\
  University of Michigan \\
  Ann arbor, MI, USA\\
  \texttt{\{ysangmin, ericylee, ziwa, xinxinw, wluee\}@umich.edu} \\
  %% examples of more authors
}


\begin{document}
\maketitle


\begin{abstract}
Event-based cameras are inspired by the sparse and asynchronous spike representation of the biological visual system. However, processing the even data requires either using expensive feature descriptors to transform spikes into frames, or using spiking neural networks that are difficult to train. In this work, we propose a neural network architecture based on simple convolution layers integrated with dynamic temporal encoding reservoirs with low hardware and training costs. The Reservoir-enabled Time Integrated Attention Network (RetinaNet) allows the network to efficiently process asynchronous temporal features, and achieves the highest accuracy of 99.2\% for DVS128 Gesture reported to date, and one of the highest accuracy of 67.5\% for DVS Lip dataset at a much smaller network size. By leveraging the internal dynamics of memristors, asynchronous temporal feature encoding can be implemented at very low hardware cost without preprocessing or dedicated memory and arithmetic units. The use of simple DNN blocks and backpropagation based training rules further reduces its implementation cost. Code will be publicly available.
\end{abstract}

%\footnote{\dag Corresponding author}

% keywords can be removed
\keywords{Neuromorphic Vision \and Event-based Camera \and Motion classification \and Lip reading \and Reservoir computing \and Memristor}


\section{Introduction}
Event-based cameras were invented to produce visual signals as asynchronous spikes that allow better energy efficiency and latency.[1] Earlier datasets were generated by reproducing conventional visual task datasets such as MNIST[2], CIFAR10[3] and Caltech100[2], where stationary images were placed in front of the camera and moved around to create pixel intensity differences along time. Many networks have been proposed for and performed well on these datasets where the object does not actually change over time.[4]–[15] Behaviorally dynamic datasets, such as DVS128 Gesture[16], DVS Lip[17] were later created to maximize the event camera’s strength where the time evolutions of both the object’s shape and movement are critical.\par
To process these more complex event datasets, deep neural networks (DNNs) consisting of convolution and fully connected layers, and spiking neural networks (SNNs) have been adopted.[4]–[24] To process temporally sequential (global) information hidden in the series of features, recurrent units, such as long short-term memory (LSTM)[25], [26], gated recurrent unit (GRU)[27] and bidirectional gated recurrent unit (BiGRU) [17]–[20] are typically required. Data are processed synchronously in temporally local frames which are created either by simple accumulation of spikes within a prefixed time range[22] or using input representation algorithms like Graph construction[24], [28], 3D point cloud[21], Event frame[29],  Event spike tensor[23] and voxel grid.[17], [30]. Recurrent units require storing multiple state data for each node and are more expensive to train than DNNs. Synchronized processing require storing and analyzing a large number of events as a pre-processing step, which diminishes the asynchronous and sparse spike generation features of event-based cameras.\par
Alternatively, SNN-based networks have been adopted. SNNs store temporal data in the neuron dynamics using models such as leaky integrate-and-fire (LIF) neurons[6], [10] and can be trained using gradient-based approaches such as backpropagation through time (BPTT)[31]. To address the non-differentiability of spikes, surrogate approaches that replace spikes with neuron membrane potential have been developed.[32] However, surrogate gradient and rate and temporal coding may lead to loss of temporal information, and BPTT is expensive during training. These factors limit performance of SNN networks.\par
%figure 1
\begin{figure}[!t]
  \centering
  %\fbox{\rule[-.5cm]{4cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \includegraphics[width=\linewidth]{Figure_1.png}
  \caption{RetinaNet structure. $\emph{R}_{in}$ and $\emph{R}_{f}$ are reservoir layers for temporal feature encoding. Bottom left: outputs from $\emph{R}_{in}$ from a representative input in the DVS Lip dataset. Bottom right: outputs from $\emph{R}_{f}$. Outputs from $\emph{R}_{f}$ are reshaped in 2D for better visualization.}
  %\setlength{\belowcaptionskip}{-10pt}
  \label{fig:fig1}
\end{figure}
To minimize network complexity and use spikes directly, feature descriptors such as Time Surface (TS) [14], [15], [33] were proposed. TS stores only the last spike event for every pixel, and converts time to the last spike information into an analog value. The analog surface after TS conversion allows the encoded data to be processed with DNN methodology and trained using gradient-based training. However, since TS only stores the last spike, it lacks capability to handle spatiotemporal features whose correlation is beyond its temporal neighbors, which is common in real-world problems. More advanced approaches such as Leaky Surface were subsequently developed to encode the temporal information beyond the last spike[34]. However, expensive pre-processing is still required, for example, to store the previous node state and time elapsed from the latest spike, and to calculate the new state for every pixel at every time instance.\par
In this paper, we introduce a neural network architecture, Reservoir-Enabled Time Integrated Attention Network (RetinaNet), that uses a simple reservoir layer to natively encode the temporal data of asynchronous spikes. RetinaNet can be trained with simple DNN backpropogation rules, while maintaining better temporal information than TS encoding at lower cost. Physical implementations of RetinaNet can be achieved with simple DNN blocks and reservoir nodes (RNs) based on short-term memory (STM) memristors.[35], [36] A STM memristor can be natively excited by an input spike, followed by a spontaneous decay. This property has allowed STM memristors to build reservoirs for tasks such as time-series analysis and prediction, which have already been physically implemented.[35], [36] Specifically, the STM memristor state  allows native feature description of the temporal information in all prior spike inputs produced by the event-based camera, without the use of recurrent units or dedicated memory and logic circuits to implement complex feature descriptor algorithms. [35]–[38] In this regard, RNs in the network can be thought of analogous to cells in a retina which directly sense and encode raw and asynchronous visual inputs, and transmit to the brain.[39] An example of the proposed RetinaNet is shown in Figure 1.\par
The main contributions of this work can be summarized below:\par
•	We implement a neural network structure that can directly process spatiotemporal data generated by event-based cameras.\par
•	RNs based on STM memristors allow efficient temporal spike encoding at lower cost. The use of simple DNN blocks allows efficient gradient-based learning rules.\par
•	RetinaNet achieves a highest reported accuracy of 99.2\% on DVS128 Gesture, and one of the highest classification accuracy of 67.5\% on DVS Lip dataset with much a lighter network structure than the previously reported networks.

\section{Background}
\label{sec:headings}



%\lipsum[4] See Section \ref{sec:headings}.

\subsection{Temporal Feature Descriptor}
There are various ways to process event (spike) inputs generated either from the event camera or from neurons in the preceding layers. SNNs offer neuron dynamics and can theoretically process temporal data in an asynchronous fashion. However, backpropagation through time training (BPTT) is expensive and surrogate gradient methods and simplified encoding methods result in loss of accuracy.  DNN-based networks lack neuron dynamics and require the spiking inputs (events) to be converted to frames using various feature descriptor techniques to encode the temporal information. The simplest, and often used method, is to neglect temporal information in the spike stream and just accumulate the temporally neighboring spikes to build a local frame.[22] More sophisticated input representation techniques, such as Point clouds[21], Graph construction[24], [28], Grid-like[23], Image-like[22] and Voxel grid[30] have been proposed to convert the local spike streams into frame formats for DNN processing.[30]–[32] However, these proposed feature descriptors require extensive preprocessing including random[21] or local density-based[24] down-sampling, time-scaling[17] and dedicated memory and arithmetic units to store spatiotemporal information of the spikes and perform the signal conversion. These costs diminish the latency and power advantages of event-based cameras and disable a real-time operation.\par
In a representative feature descriptor such as TS, the amplitude of each node is reset to 1 every time it receives a spike and then relaxes following a decay function:\par
\begin{equation}
S(t)=e^{-\frac{t-T(t)}{\tau}}
\end{equation}
where \emph{S(t)} is an analog vector representing a node on the time surface, \emph{t} is the current time, \emph{T(t)} is the time information of the last spike received by the node, and $\tau$ is a pre-defined time constant.[33] Since only the last spike time instance needs to be recorded, this encoding method reduces memory and computing cost compared to other feature descriptors. However, useful information prior to the last spike, which may be essential for features that evolve over longer history, was discarded. Additionally, storing the last spike timing information for each pixel (node), and calculating the analog state based on equation (1) still incurs substantial costs. Other techniques such as Leaky Surface were proposed to resolve the limitation of the last-spike-dominant encoding, but require higher hardware overhead to keep track of the state and time elapsed for all nodes.[34]\par

\subsection{Encoding with reservoir nodes}
We note that in a reservoir computing (RC) system, the reservoir maintains short-term memory and performs nonlinear transformation (encoding) of the temporal input data into the reservoir states, represented by the states of the reservoir nodes (RNs). Generally, due to the STM property, RNs will be more strongly affected by the near history events and weakly affected by far history events, with the extent of the nonlinearity determined by the internal RN time constant.  Inspired by this principle, we hypothesize that RNs can be directly used to encode temporal spike data similar to what TS aims to accomplish, but at much lower cost and with an ability to extend beyond the last spike since RNs nonlinearly accumulate all incoming spike information. The proposed approach will thus be able to offer better performance at lower cost, compared other feature descriptor approaches.\par
In particular, a RN can be implemented using only a single STM memristor, making hardware implementation very light weight.[35], [36] In a STM memristor, the memristor’s state (i.e. conductance) is natively excited by the incoming spikes and relaxes in between the spikes,[38] as described by the following equation\par
\begin{equation}
G_{t}=P_{c}*(G_{max}-G_{t-1})*\delta_{spk}(t)+G_{t-1}*e^{-\frac{1}{\tau}}
\end{equation}
where $\emph{G}_{t}$ is the device state at time \emph{t}, $\emph{P}_{c}$ is a potentiation factor, $\emph{G}_{max}$ is the upper bound of the device state, $\tau$ is the characteristic relaxation time constant, and $\delta_{spk}$\emph{(t)} is a delta function representing a spiking event: \par
\begin{equation}
  \delta_{spk}(t)=\left\{
    \begin{array}{ll}
      0, & \mbox{when no spike}.\\
      1, & \mbox{when receiving spike}.
    \end{array}
  \right.
\end{equation}
Figure 2 shows the comparison of the RN approach implemented with a single STM memristor vs the TS approach. Both approaches convert the spiking patterns into an analog state that can be processed by subsequent DNN blocks. Compared to TS encoding that reset the state to 1 after each spike, the RN implementation allows longer term history to be represented in the input in a non-linear fashion. For example, at t=50, the TS output is identical to that at t=30, since both values were reset to 1 10 time steps earlier (at t=20 and 40, respectively). However, this representation missed the differences in the two temporal sequences (e.g. an additional preceding spike at t=15). On the other hand, the RN implementation clearly differentiates the two cases as all inputs prior to the current time are accumulated nonlinearly. Additionally, RN can be readily implemented using a single memristor (or a resistance-capacitance unit) without the need of additional memory (to store t and T(t)) and arithmetic and logic units (ALUs) to convert the temporal data. Instead, equation (2) is natively implemented in the device due to internal device physics (ionic and electronic dynamic processes) [35], [36].\par

%figure 2
\begin{figure}[!t]
  \centering
  %\includegraphics[width=\linewidth, height = 5]{Figure_2.ps}
  \includegraphics[scale=1]{Figure_2.png}
  \caption{Dynamics of reservoir node (blue) and time surface (red) under incoming spikes (black) shown below.}
  %\setlength{\belowcaptionskip}{-10pt}
  \label{fig:fig1}
\end{figure}
With the proposed approach, the RetinaNet can directly take asynchronous spikes as they are generated, and the memristor-based RNs transform the temporal spikes into analog values following Equation (2). The conductance values from the RNs are only retrieved when the network operates a forward-pass, as shown in Figure 1, analogous to visual inputs encoded by cells in a retina.[39] In the subsequent sections we show that the network can be efficiently trained using backprogation, and produce state-of-the-art accuracy for advanced spatiotemporal datasets at a much smaller network size.\par

\subsection{Related works}
Several studies based on TS encoding have been demonstrated incorporating convolution and fully-connected layers to leverage the backpropagation learning rule for efficient training.[14], [15], [33], [40] Networks for tasks requiring longer temporal features have been implemented with the combination of DNN and local feature descriptor layers including 3D point Cloud[21], 3D Graph construction[24], [28], grid-like representation[23], Voxel grid[30], and recurrent units such as BiGRU to discover the global correlation between temporally neighboring local features detected by DNN.[17] RC systems with simple fully-connected readout layers have also been demonstrated and implemented in hardware, focusing on 1D time-series input data instead of spatiotemporal datasets.[35], [36]

\section{Method}
RetinaNet and its operations will be introduced in Section 3.1. The datasets used in this study are discussed in Section 3.2.
\subsection{Model Architecture}
The network is formed sequentially by an RN layer ($\emph{R}_{in}$) as a feature descriptor for local temporal encoding, Convolution layers ($\emph{C}_{1}$-$\emph{C}_{4}$), a spike conversion (SC) layer, another RN layer ($\emph{R}_{f}$) for global temporal encoding and fully-connected layers (FC, $\emph{F}_{1}$-$\emph{F}_{2}$) for classification. Figure 1 illustrates the overall architecture of the proposed network.\par
The $\emph{R}_{in}$ layer has the same size as the inputs. Each RN in the $\emph{R}_{in}$ layer processes input spikes asynchronously from a corresponding pixel in the event camera following equation (2) in real time without preprocessing. Examples of the output generated by $\emph{R}_{in}$ for DVS Lip inputs are shown in the left bottom insets of Figure 1.\par
Depending on the application, different temporal resolutions can be chosen by adjusting the time constant $\tau$ of equation 2 and the frequency of $\emph{R}_{in}$ output acquisitions. A shorter acquisition interval creates more frequent activations for the following layers. A shorter time constant and more frequent acquisitions produce temporally finer outputs, while a longer time constant and less frequent acquisitions produce spatially more detailed outputs owing to the slower decay.[17] To keep the discussions simple, here we use the same time constant and acquisition frequency for both datasets. The convolution layers ($\emph{C}_{1}$-$\emph{C}_{4}$ in Figure 1) then process the encoded data from $\emph{R}_{in}$ and the output from the last conv layer, O4, is flattened and sent to the SC layer. We use a predetermined threshold to produce spikes in the SC layer. Hence, each spike generated after SC represents the existence of a local spatiotemporal feature detected by the convolution layers from $\emph{R}_{in}$ outputs.\par
Similar to the RNs in the $\emph{R}_{in}$ layer, RNs (e.g. also implemented with STM memristors) in $\emph{R}_{f}$ naturally potentiates/relaxes with the presence/absence of spikes from the SC layer. The state of $\emph{R}_{f}$ thus represents the historical (global) spatiotemporal feature information detected by the DNN layers over time, and is then used by the subsequent FC layers to perform the final classification functions. Examples of the states of $\emph{R}_{f}$ evolving over time for DVS Lip inputs are shown in the right bottom inset of Figure 1.

%figure 3
\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{Figure_3.png}
  %\includegraphics[scale=1]{Figure_2.png}
  \caption{Consecutive outputs captured from $\emph{R}_{in}$ for an input in the (a) DVS128 Gesture dataset and (b) DVS Lip dataset. Deeper red represents higher amplitude value of a RN state.}
  \label{fig:fig1}
\end{figure}
\subsection{Datasets}
We used the DVS128 Gesture and DVS Lip datasets to test our model. The DVS128 Gesture dataset is for human action recognition and the DVS Lip dataset is for word recognition based on lip motions of speaking participants. DVS128 Gesture data are labeled in 11 categories, while DVS Lip data are labeled in 100 categories.\par
Figure 3 shows examples of 5 temporally consecutive outputs from $\emph{R}_{in}$ for the two datasets. As shown in the figure, the RNs in $\emph{R}_{in}$ natively forget inputs that are temporally too far from the current time instant thanks to the RN’s relaxation (the second term of equation (2)), while frequent spikes leads to stronger outputs due to the excitation term (the first term of equation (2)). These properties of the $\emph{R}_{in}$ layer allow it to memorize the spatiotemporal feature of the inputs, as shown in Figure 3. For example, in Figure 3b, both the speaking motion and movement (top to bottom) of the lips can be captured by the RNs’ state. Temporally fast movement can be reflected in a single output (e.g. 3rd state of RNs in Figure 3b), while temporally slow movements are reflected in multiple outputs captured at the different time instants. These converted spatiotemporal features are then processed by the DNN blocks, and their evolution over time are then captured by the Rf layer for the classification layers to produce the final outputs.\par
To reflect both the short time and long time scale features, we adopted the virtual node concept[41] to capture multiple (i.e. 5) outputs from $\emph{R}_{f}$ over the whole video clip and feed them to the FC layers.\par

\section{Experiments}
In this section, we describe the implementation of RetinaNet for the two datasets, and compare the results with existing networks.
\subsection{Implementation details}
$\emph{R}_{in}$ is placed at the first stage of the entire network to receive the asynchronous spike inputs. As discussed earlier, the RNs in $\emph{R}_{in}$ natively encode the spatiotemporal information in the spikes into analog outputs represented by the RNs’ states (i.e., memristor conductance). For both datasets, the states of $\emph{R}_{in}$ are captured every 30ms and sent to the subsequent DNN blocks. 
The DVS128 Gesture dataset consists of repetitions of the same motion of a participant along the clip, while lip motions in DVS Lip dataset is not repetitive, as shown in Figure 3. As a result, a fraction of a clip should be sufficient to classify action in DVS128 Gesture, while the whole clip is necessary for DVS Lip classification. Based on this understanding, we only used the first 1.5s for all videos in the DVS128 Gesture dataset during training and inference, corresponding to ~10\% of the longest video (15.5s). As a result, 50 sequences of $\emph{R}_{in}$ outputs will be sent to the subsequent DNN blocks. The DVS Lip dataset has variable input lengths due to the irregular length of the words and the unique speaker’s traits, with most video clips (99\%) having lengths between 0.75s and 1.5s. To make the data size regular across the dataset, we chose to keep the captured $\emph{R}_{in}$ outputs at 50 by simply applying null (no spike) for clips shorter than 1.5s and clipping videos longer than 1.5s during DVS Lip training and inference. X- and Y-dimension were set as 128 and 128, respectively. $\emph{P}_{c}$ and $\tau$ in Eq (2) were set to 0.5 and 60ms. Detailed information for both datasets are summarized in Table 1. 
\begin{table}[!h]
 \caption{Information of both datasets used in this work.}
  \centering
  \begin{tabular}{lllllll}
    \toprule
    Dataset &   Time Interval   & Total length & $\emph{P}_{c}$ & $\tau$ & $\sharp$ of Train Data & $\sharp$ of Test Data\\
            &   [ms]            &    [s]       &                & [ms]   &\\
    \midrule
    Gesture & 30       & 1.5 &  0.5 & 60 &  1077 &  264\\
    Lip     & 30       & 1.5 &  0.5 & 60 & 14896 & 4975\\
    \bottomrule
  \end{tabular}
  \label{tab:table}
\end{table}

We also adopted several generalization techniques, such as random/center crop, horizontal flip and Gaussian noise to minimize overfitting effects. These techniques are used only for the DVS Lip dataset since motions in DVS128 Gesture such as a left/right hand waving are sensitive to the horizontal flip and objects positions. In case of random/center crop, we referred to existing works.[17], [19] During training, we center-cropped the original input dimension into 96 x 96 and then random cropped them into 76 x 76, while the inputs are directly center-cropped into 76 x 76 in testing phase. Horizontal flip with a probability of 0.5 and Gaussian noise with standard deviation of 5e-4 were used during training for the same purpose. These techniques helped generalize the input data and reduce overfitting during training.\par
Due to the different input dimension and the different   number of categories in the two datasets, the convolution layers and FC layers in RetinaNet are configured differently. The detailed network configurations for both datasets are summarized in Table 2.\par


\begin{table}[!h]
 \caption{Table 2. Network configuration of RetinaNet for DVS128 Gesture dataset (left columns) and DVS Lip dataset (right columns).}
  \centering
  \begin{tabular}{cccccccccc}
    \toprule
    \multicolumn{5}{c}{DVS128 Gesture} & \multicolumn{5}{c}{DVS128 Lip}                   \\
    \cmidrule(r){1-5}
    \cmidrule(r){6-10}
    Layer     & Kernel & Out     & Pad/   & Output & Layer     & Kernel & Out     & Pad/   & Output\\
         & Size   & channel & Stride &   Dim  &     & Size   & channel & Stride &   Dim\\
    \midrule
    $\emph{R}_{in}$ & -  & 2  & - & 128 x 128 & $\emph{R}_{in}$ & - & 2 & - & 76 x 76   \\
    MaxPool  & \centering 2 &  2  & 0 / 2 & 64 x 64& Conv1   & 5 & 64  & 0 / 2 & 36 x 36 \\
    Conv1    & 3 &  64 & 0 / 1 & \centering 62 x 62& Conv2   & 3 & 128 & 1 / 1 & 36 x 36 \\
    MaxPool  & 3 &  64 & 0 / 2 & 30 x 30& MaxPool & 3 & 128 & 0 / 2 & 17 x 17 \\
    Conv2    & 3 & 128 & 1 / 1 & 30 x 30& Conv3   & 3 & 128 & 1 / 1 & 17 x 17 \\
    MaxPool  & 3 & 128 & 0 / 2 & 14 x 14& Conv4   & 3 & 256 & 1 / 1 & 17 x 17 \\
    Conv3    & 3 & 256 & 0 / 1 & 12 x 12& MaxPool & 3 & 256 & 0 / 2 & 8 x 8 \\
    Conv4    & 3 & 512 & 1 / 1 & 12 x 12& Conv5   & 3 & 256 & 1 / 1 & 8 x 8 \\
    MaxPool  & 3 & 512 & 0 / 2 & 5 x 64 & Conv6   & 3 & 512 & 1 / 1 & 8 x 8 \\
    Conv5    & 3 & 512 & 0 / 1 & 3 x 64 & MaxPool & 3 & 512 & 0 / 2 & 3 x 3 \\
    MaxPool  & 3 & 512 & 0 / 1 & 1 x 64 & Conv7   & 3 & 512 & 0 / 1 & 1 x 1 \\
    $\emph{R}_{f}$ & - & 512 & - & -    & $\emph{R}_{f}$ & - & 512 & - & -     \\
    FC1      & - & 512 & -     & -      & FC1   & - & 512 & -     & - \\
    FC2      & - & 100 & -     & -      & FC2   & - & 11  & -     & - \\
    \bottomrule
  \end{tabular}
  \label{tab:table}
\end{table}

The output of a DNN block consists of a convolution and a pooling layer (if exists), following:\par
\begin{equation}
O_{N}=f(BN(P(C(I_{N}))))
\end{equation}
where $O_{N}$ is the output of N-th layer, \emph{f(x)} is a ReLU activation function, \emph{BN(x)} is a batch normalization function, \emph{P(x)}/\emph{C(x)} is a pooling/convolution operation, and $I_{N}$ is input of the N-th layer.[42]\par
After the last convolution layer, the outputs are converted to spikes using the snn.leaky function from the Snntorch package[43] for subsequent temporal feature encoding at Rf. We set a threshold value of 0.3 during spike conversion for both datasets.\par

%figure 4
\begin{figure}[!h]
  \centering
  \includegraphics[width=\linewidth]{Figure_4.png}
  %\includegraphics[scale=1]{Figure_2.png}
  \caption{(a) Visualization of asynchronous input spikes and (b) spikes generated after the spike conversion (SC) layer.}
  \label{fig:fig1}
\end{figure}

Spikes generated after the SC layer are shown in Figure 4. Compared with the original spiking inputs, the spikes after SC represent the local spatiotemporal features captured by the $\emph{R}_{in}$ layer and the DNN layers, and become much sparser. These spikes are then applied to the $\emph{R}_{f}$ layer for global spatiotemporal feature capture and subsequent classification in the FC layers.\par

%figure 5
\begin{figure}[!h]
  \centering
  \includegraphics[width=\linewidth]{Figure_5.png}
  %\includegraphics[scale=1]{Figure_2.png}
  \caption{Visualization of outputs from $\emph{R}_{f}$ over the whole 1.5s clip, along with spikes generated from the spike conversion layer. For Data2 whose input video length is only 0.9s, the RN states will continue to relax and still used for classification.}
  \label{fig:fig1}
\end{figure}
 $\emph{R}_{f}$ encodes the global spatiotemporal features of the spikes, and its outputs are captured every 0.3s for both dataset (corresponding to 5 virtual node outputs during the 1.5s clip). In case of clips shorter than 1.5s, the RNs in $\emph{R}_{f}$ will continue to relax after the last meaningful input. To keep the process simple, we decide not to pad the input data with new spikes (e.g., through repeating the clip). Rather, we continue to capture these relaxed states without new input spikes and feed them to the classification layers. The illustration of the RN states at different time instances are shown in Figure 5, along with spikes they receive from the SC layer. For better visualization, the data are reshaped in 2D format. As shown in the figure, even without any new inputs (e.g., after 0.9s for Data2), the RN states do not immediately decay to zero due to the slow decay term in equation (2), and the evolution of the $\emph{R}_{f}$ outputs still represents useful information. For example, output 4 and 5 encode cumulative effects of the spikes prior to 0.9s as well as decay of the outputs after 0.9s. This simplifies the training and inference process and allows the system to handle inputs with irregular lengths. $\emph{P}_{c}$ / $\tau$ for $\emph{R}_{f}$ are set as 0.1/0.2s and 0.3/2s for DVS128 Gesture and DVS Lip dataset, respectively. A longer $\tau$ is used in $\emph{R}_{f}$ than in $\emph{R}_{in}$ to process the longer temporal correlations.\par

A classification block consists of a FC layer, an activation function, and a batch normalization layer, following: 
\begin{equation}
O_{F}=f(BN(FC(I)))
\end{equation}
where $O_{F}$ is the output of the block, \emph{f(x)} is a ReLU activation function, \emph{BN(x)} is a batch normalization function, \emph{FC(x)} is a FC layer, and \emph{I} is the input to the FC layer. In the proposed RetinaNet, two classification blocks are used, and output of the 2nd block is obtained without the activation and normalization function, and used as the final output of the network.\par 
 The Pytorch framework[46] was used for all the experiments with methods described above. SoftMax function was chosen to calculate the probability of an output neuron in final output, and the neuron with the largest possibility was selected as the classification result. During training, the cost was derived by categorical cross-entropy function.[47] ATan was selected as surrogate gradient calculation  for the SC layer during training.[43] We also used Adam optimizer[48] with an initial learning rate of 7$e^{-3}$ / 6$e^{-5}$ and a weight decay of 0 / 6$e^{-3}$ as model optimizer and ReduceLROnPlateau with a factor of 0.9 / 0.9, patience of 2 / 1, threshold of 1$e^{-5}$ / 1$e^{-6}$, minimum learning rate of 1$e^{-4}$ / 1$e^{-7}$ as learning scheduler for DVS128 Gesture / DVS Lip. Batch size of 32 and 150 epochs were used for both datasets.\par

\subsection{Experimental Results}
RetinaNet shows excellent performance on both datasets; 99.2\% for DVS128 Gesture (highest reported to date) and 67.5\% for DVS Lip. Comparisons of classification accuracy with existing networks are shown Table 3 and Table 4. Accuracies of the networks listed on Table 4 were obtained from [17].
In DVS128 Gesture, RetinaNet outperforms all existing networks based on SNN and CNN platforms, while only using the first 10 percent of the longest clip. The accuracy was achieved without costly training rules such as BPTT. For DVS Lip, RetinaNet achieves top 2 accuracy among event-based models, only behind MSTP[17] which is a multi-branch network with different input channels generating frames in different temporal granularity, and employs Voxel Grid as the feature descriptor and BiGRU and ResNet-18 network architectures. By comparison, ReginaNet uses a single branch and a much lighter network, without expensive feature description/preprocessing and recurrent units that can lead to significant hardware and latency overheads. The relevant model sizes and their performances are listed in Table 4. The parameter for ResNet variants are obtained from [49].\par
\begin{table}[!h]
 \caption{Comparison of existing models and RetinaNet on DVS Lip.}
  \centering
  \begin{tabular}{lll}
    \toprule
    Model &   Base & ACC[\%]\\
    \midrule
    Rollout[12]     & SNN       & 95.7\\
    DECOLLE[11]     & SNN       & 95.5\\
    SEW-ResNet[7]   & SNN       & 97.9\\
    tdBN[9]         & SNN       & 96.9\\
    PLIF[8]         & SNN       & 97.6\\
    LIAF-Net[10]    & SNN       & 97.6\\
    TA-SNN[6]       & SNN       & 98.6\\
    TCJA-SNN[4]     & SNN       & 99.0\\
    Amir et al[16]  & CNN       & 94.6\\
    Event Clouds[21]& CNN       & 95.3\\
    \cmidrule(r){1-3}
    \textbf{RetinaNet} & \textbf{SNN + CNN} & \textbf{99.2}\\
    \bottomrule
  \end{tabular}
  \label{tab:table}
\end{table}

\begin{table}[!h]
\small
 \caption{Comparison of existing models and RetinaNet on Lip reading dataset including DVS Lip.}
  \centering
  \resizebox{\columnwidth}{!}{%
  \begin{tabular}{llcclllll}
    \toprule
    Model &   Input & ACC &Params&Base model& Preprocess& Local encoding & 1st& Global encoding\\
    &&[\%]&[M]&&&&&\\
    \midrule
    TANet[44]     & Video       & 68.7 & 42.8 & ResNet-101 & - & TAM & 2D &Temporal AvgPool\\
    ACTION-Net[45]     & Video  & 68.8 & 28.1 & ResNet-50  & Random Sampling (RS) & STE/CE/ME & 3D &Temporal AvgPool\\
    DFTN[18]     & Video       & 63.2 & 40.5 & ResNet-18 & - & DFN & 3D &BiGRU\\
    Feng et al[19]     & Video & 63.4 & 11.2 & ResNet-18 & Mix Up & - & 3D &BiGRU\\
    Martinez et al[20] & Video & 65.5 & 11.2 & ResNet-18 & Greyscaling & - & 3D &Multi-scale TCN\\
    Event Clouds[21] & Event & 42.2 & 8.1 & PointNet & Segmentation + RS & 3D point cloud & 3D & 3D point cloud\\
    EV-Gait-3DGraph[24] & Event & 32.0 & 7.2 & - & OctreeGrid & 3D-Graph & 3D & N/A\\
    EV-Gait-IMG[22]     & Event & 34.5 & 64.6 & - & Event NC & Image-like & 2D & N/A\\
    EST[23] & Event & 48.7 & 21.5 & ResNet-34 & EMF & MLP (Grid) & 3D & N/A\\
    MSTP[17] & Event & 72.1 & 38.5 & ResNet-18 & Time-scaling & Voxel Grid & 3D & BiGRU\\
    \cmidrule(r){1-9}
    \textbf{RetinaNet} & \textbf{Event} & \textbf{67.5} & \textbf{7.5} & \textbf{-} & \textbf{X} & \textbf{Reservoir} & \textbf{2D} & \textbf{Reservoir}\\
    \bottomrule
  \end{tabular}%
  }
  \label{tab:table}
\end{table}


\section{Conclusions}
In this work, we propose a hybrid reservoir/DNN network, Reservoir-Enabled Time Integrated Attention Network (RetinaNet), for event-based video processing. The use of dynamic reservoir nodes allows efficient spatiotemporal feature encoding in real time and eliminates expensive memory and logic/recurrent units. RetinaNet achieves the highest DVS128 Gesture classification accuracy, and one of the highest DVS Lip classification accuracy with a light network structure. The reservoir nodes can be directly implemented with STM memristors, taking advantage of internal device physics to perform signal processing. RetinaNet can be implemented with simple DNN blocks and backpropagation training to further reduce hardware and training costs.\par

\section*{Acknowledgments}
This work was supported in part by the National Science Foundation through Awards $\sharp$ 1915550 and $\sharp$ CCF- 1900675, and SRC and DARPA through the Applications Driving Architectures (ADA) Research Center\par


%Bibliography  
\section*{References}
[1] G. Gallego et al., “Event-Based Vision: A Survey,” IEEE Trans Pattern Anal Mach Intell, vol. 44, no. 1, pp. 154–180, 2022, doi: 10.1109/TPAMI.2020.3008413.\par
[2]	G. Orchard, A. Jayawant, G. K. Cohen, and N. Thakor, “Converting static image datasets to spiking neuromorphic datasets using saccades,” Front Neurosci, vol. 9, no. NOV, 2015, doi: 10.3389/fnins.2015.00437.\par
[3]	H. Li, H. Liu, X. Ji, G. Li, and L. Shi, “CIFAR10-DVS: An Event-Stream Dataset for Object Classification,” Front Neurosci, vol. 11, 2017, doi: 10.3389/fnins.2017.00309.\par
[4]	R.-J. Zhu et al., “TCJA-SNN: Temporal-Channel Joint Attention for Spiking Neural Networks,” Jun. 2022, [Online]. Available: http://arxiv.org/abs/2206.10177\par
[5]	S. Deng, Y. Li, S. Zhang, and S. Gu, “Temporal Efficient Training of Spiking Neural Network via Gradient Re-weighting,” International Conference on Learning Representations (ICLR), pp. 1–17, 2022.\par
[6]	M. Yao et al., “Temporal-wise Attention Spiking Neural Networks for Event Streams Classification,” in International Conference on Computer Vision (ICCV), Jul. 2021. [Online]. Available: http://arxiv.org/abs/2107.11711\par
[7]	W. Fang, Z. Yu, Y. Chen, T. Huang, T. Masquelier, and Y. Tian, “Deep Residual Learning in Spiking Neural Networks,” Adv Neural Inf Process Syst, vol. 25, no. NeurIPS, pp. 21056–21069, 2021.\par
[8]	W. Fang, Z. Yu, Y. Chen, T. Masquelier, T. Huang, and Y. Tian, “Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks,” Proceedings of the IEEE International Conference on Computer Vision, vol. 1, pp. 2641–2651, 2021, doi: 10.1109/ICCV48922.2021.00266.\par
[9]	H. Zheng, Y. Wu, L. Deng, Y. Hu, and G. Li, “Going Deeper With Directly-Trained Larger Spiking Neural Networks,” 35th AAAI Conference on Artificial Intelligence, AAAI 2021, vol. 12B, pp. 11062–11070, 2021, doi: 10.1609/aaai.v35i12.17320.\par
[10]	Z. Wu, H. Zhang, Y. Lin, G. Li, M. Wang, and Y. Tang, “LIAF-Net: Leaky Integrate and Analog Fire Network for Lightweight and Efficient Spatiotemporal Information Processing,” IEEE Trans Neural Netw Learn Syst, vol. 33, no. 11, pp. 6249–6262, 2022, doi: 10.1109/TNNLS.2021.3073016.\par
[11]	J. Kaiser, H. Mostafa, and E. Neftci, “Synaptic Plasticity Dynamics for Deep Continuous Local Learning (DECOLLE)  ,” Frontiers in Neuroscience  , vol. 14. 2020. [Online]. Available: https://www.frontiersin.org/articles/10.3389/fnins.2020.00424\par
[12]	A. Kugele, T. Pfeil, M. Pfeiffer, and E. Chicca, “Efficient Processing of Spatio-Temporal Data Streams With Spiking Neural Networks  ,” Frontiers in Neuroscience  , vol. 14. 2020. [Online]. Available: https://www.frontiersin.org/articles/10.3389/fnins.2020.00439\par
[13]	S. B. Shrestha and G. Orchard, “Slayer: Spike layer error reassignment in time,” Adv Neural Inf Process Syst, vol. 2018-Decem, no. NeurIPS, pp. 1412–1421, 2018.\par
[14]	X. Lagorce, G. Orchard, F. Galluppi, B. E. Shi, and R. B. Benosman, “HOTS: A Hierarchy of Event-Based Time-Surfaces for Pattern Recognition,” IEEE Trans Pattern Anal Mach Intell, vol. 39, no. 7, pp. 1346–1359, Jul. 2017, doi: 10.1109/TPAMI.2016.2574707.\par
[15]	A. Sironi, M. Brambilla, N. Bourdis, X. Lagorce, and R. Benosman, “HATS: Histograms of Averaged Time Surfaces for Robust Event-based Object Classification,” in Computer Vision and Pattern Recognition (CVPR), 2018.\par
[16]	A. Amir et al., “A Low Power , Fully Event-Based Gesture Recognition System,” in Computer vision and pattern recognition, 2017, pp. 7243–7252.\par
[17]	G. Tan, Y. Wang, H. Han, Y. Cao, F. Wu, and Z. J. Zha, “Multi-grained Spatio-Temporal Features Perceived Network for Event-based Lip-Reading,” Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, vol. 2022-June, pp. 20062–20071, 2022, doi: 10.1109/CVPR52688.2022.01946.\par
[18]	J. Xiao, S. Yang, Y. Zhang, S. Shan, and X. Chen, “Deformation Flow Based Two-Stream Network for Lip Reading,” Proceedings - 2020 15th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2020, pp. 364–370, 2020, doi: 10.1109/FG47880.2020.00132.\par
[19]	D. Feng, S. Yang, S. Shan, and X. Chen, “Learn an Effective Lip Reading Model without Pains,” pp. 1–6, 2020, [Online]. Available: http://arxiv.org/abs/2011.07557\par
[20]	B. Martinez, P. Ma, S. Petridis, and M. Pantic, “Lipreading Using Temporal Convolutional Networks,” ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings, vol. 2020-May, pp. 6319–6323, 2020, doi: 10.1109/ICASSP40776.2020.9053841.\par
[21]	Q. Wang, Y. Zhang, J. Yuan, and Y. Lu, “Space-time event clouds for gesture recognition: From RGB cameras to event cameras,” Proceedings - 2019 IEEE Winter Conference on Applications of Computer Vision, WACV 2019, pp. 1826–1835, 2019, doi: 10.1109/WACV.2019.00199.\par
[22]	Y. Wang et al., “EV-gait: Event-based robust gait recognition using dynamic vision sensors,” Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, vol. 2019-June, pp. 6351–6360, 2019, doi: 10.1109/CVPR.2019.00652.\par
[23]	D. Gehrig, A. Loquercio, K. Derpanis, and D. Scaramuzza, “End-to-end learning of representations for asynchronous event-based data,” Proceedings of the IEEE International Conference on Computer Vision, vol. 2019-Octob, no. ICCV, pp. 5632–5642, 2019, doi: 10.1109/ICCV.2019.00573.\par
[24]	Y. Wang et al., “Event-Stream Representation for Human Gaits Identification Using Deep Neural Networks,” IEEE Trans Pattern Anal Mach Intell, vol. 44, no. 7, pp. 3436–3449, 2022, doi: 10.1109/TPAMI.2021.3054886.\par
[25]	S. Hochreiter and J. Schmidhuber, “Long Short-Term Memory,” Neural Comput, vol. 9, no. 8, pp. 1735–1780, 1997, doi: 10.1162/neco.1997.9.8.1735.\par
[26]	A. Graves, “Generating Sequences With Recurrent Neural Networks,” pp. 1–43, 2013.\par
[27]	J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling,” pp. 1–9, 2014.\par
[28]	Y. Bi, A. Chadha, A. Abbas, E. Bourtsoulatze, and Y. Andreopoulos, “Graph-based object classification for neuromorphic vision sensing,” Proceedings of the IEEE International Conference on Computer Vision, vol. 2019-Octob, pp. 491–501, 2019, doi: 10.1109/ICCV.2019.00058.\par
[29]	H. Rebecq, T. Horstschaefer, and D. Scaramuzza, “Real-time visual-inertial odometry for event cameras using keyframe-based nonlinear optimization,” British Machine Vision Conference 2017, BMVC 2017, 2017, doi: 10.5244/c.31.16.\par
[30]	A. Z. Zhu, L. Yuan, K. Chaney, and K. Daniilidis, “Unsupervised event-based learning of optical flow, depth and egomotion,” IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, vol. 2019-June, p. 1694, 2019, doi: 10.1109/CVPRW.2019.00216.\par
[31]	P.J. Werbos, “Backpropagation Through Time: What It Does and How to Do It,” Proceedings of the IEEE, vol. 78, no. 10. pp. 1550–1560, 1990.\par
[32]	E. O. Neftci, H. Mostafa, and F. Zenke, “Surrogate Gradient Learning in Spiking Neural Networks,” pp. 1–25, 2019.\par
[33]	A. Grimaldi, V. Boutin, S. Ieng, and R. Benosman, “A robust event-driven approach to always-on object recognition,” TechRxiv, 2022, doi: 10.36227/techrxiv.18003077.v1.\par
[34]	M. Cannici, M. Ciccone, A. Romanoni, and M. Matteucci, “Asynchronous convolutional networks for object detection in neuromorphic cameras,” IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, vol. 2019-June, pp. 1656–1665, 2019, doi: 10.1109/CVPRW.2019.00209.\par
[35]	C. Du, F. Cai, M. A. Zidan, W. Ma, S. H. Lee, and W. D. Lu, “Reservoir computing using dynamic memristors for temporal information processing,” Nat Commun, vol. 8, no. 1, pp. 1–10, 2017, doi: 10.1038/s41467-017-02337-y.\par
[36]	J. Moon et al., “Temporal data classification and forecasting using a memristor-based reservoir computing system,” Nat Electron, vol. 2, no. 10, pp. 480–487, 2019, doi: 10.1038/s41928-019-0313-3.\par
[37]	J. Moon, Y. Wu, and W. D. Lu, “Hierarchical architectures in reservoir computing systems,” Neuromorphic Computing and Engineering, vol. 1, no. 1, p. 014006, Sep. 2021, doi: 10.1088/2634-4386/ac1b75.\par
[38]	T. Chang, S. Jo, and W. Lu, “Short-Term Memory to Long-Term Memory Transition in a Nanoscale Memristor,” ACS Nano, vol. 5, no. 9, pp. 7669–7676, 2011.\par
[39]	R. H. Masland, “The neuronal organization of the retina.,” Neuron, vol. 76, no. 2, pp. 266–280, Oct. 2012, doi: 10.1016/j.neuron.2012.10.002.\par
[40]	R. Tapiador-Morales, J.-M. Maro, A. Jimenez-Fernandez, G. Jimenez-Moreno, R. Benosman, and A. Linares-Barranco, “Event-Based Gesture Recognition through a Hierarchy of Time-Surfaces for FPGA,” Sensors, vol. 20, no. 12. 2020. doi: 10.3390/s20123404.\par
[41]	M. L. Alomar et al., “Digital implementation of a single dynamical node reservoir computer,” IEEE Transactions on Circuits and Systems II: Express Briefs, vol. 62, no. 10, pp. 977–981, 2015.\par
[42]	S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network training by reducing internal covariate shift,” 32nd International Conference on Machine Learning, ICML 2015, vol. 1, pp. 448–456, 2015.\par
[43]	J. K. Eshraghian, X. Wang, M. Bennamoun, M. Ward, W. D. Lu, and G. Lenz, “Training spiking neural networks using lessons from deep learning,” ArXiv, 2022.\par
[44]	Z. Liu, L. Wang, W. Wu, C. Qian, and T. Lu, “TAM: Temporal Adaptive Module for Video Recognition,” Proceedings of the IEEE International Conference on Computer Vision, pp. 13688–13698, 2021, doi: 10.1109/ICCV48922.2021.01345.\par
[45]	Z. Wang, Q. She, and A. Smolic, “Action-Net: Multipath excitation for action recognition,” Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, no. 15, pp. 13209–13218, 2021, doi: 10.1109/CVPR46437.2021.01301.\par
[46]	A. Paszke et al., “PyTorch: An Imperative Style, High-Performance Deep Learning Library,” in Advances in Neural Information Processing Systems, 2019, vol. 32, pp. 8024–8035. [Online]. Available: https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf\par
[47]	Z. Zhang and M. R. Sabuncu, “Generalized cross entropy loss for training deep neural networks with noisy labels,” Adv Neural Inf Process Syst, vol. 2018-Decem, no. NeurIPS, pp. 8778–8788, 2018.\par
[48]	D. P. Kingma and J. L. Ba, “Adam: A method for stochastic optimization,” 3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings, pp. 1–15, 2015.\par
[49]	M. C. Leong, D. K. Prasad, Y. T. Lee, and F. Lin, “Semi-CNN architecture for effective spatio-temporal learning in action recognition,” Applied Sciences (Switzerland), vol. 10, no. 2, 2020, doi: 10.3390/app10020557.\par


\end{document}
