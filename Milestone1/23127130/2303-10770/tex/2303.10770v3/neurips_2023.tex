\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{sort&compress, square, numbers}{natbib}
% before loading neurips_2023

% ready for submission
%\usepackage{Supplementary}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%\usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}       % graphics
\usepackage{adjustbox}



\title{RN-Net: Reservoir Nodes-Enabled Neuromorphic Vision Sensing Network}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Sangmin Yoo, Eric Yeu-Jer Lee, Ziyu Wang, Xinxin Wang, Wei D. Lu\thanks{Corresponding author} \\
  Department of Electrical Engineering and Computer Science\\
  University of Michigan\\
  Ann arbor, MI, USA\\
  \texttt{\{ysangmin, ericylee, ziwa, xinxinw, wluee\}@umich.edu} \\
}


\begin{document}


\maketitle


\begin{abstract}
  Event-based cameras are inspired by the sparse and asynchronous spike representation of the biological visual system. However, processing the event data requires either using expensive feature descriptors to transform spikes into frames, or using spiking neural networks that are expensive to train. In this work, we propose a neural network architecture, Reservoir Nodes-enabled neuromorphic vision sensing Network (RN-Net), based on simple convolution layers integrated with dynamic temporal encoding reservoirs for local and global spatiotemporal feature detection with low hardware and training costs. The RN-Net allows efficient processing of asynchronous temporal features, and achieves the highest accuracy of 99.2\% for DVS128 Gesture reported to date, and one of the highest accuracy of 67.5\% for DVS Lip dataset at a much smaller network size. By leveraging the internal device and circuit dynamics, asynchronous temporal feature encoding can be implemented at very low hardware cost without preprocessing and dedicated memory and arithmetic units. The use of simple DNN blocks and standard backpropagation-based training rules further reduces implementation costs.
\end{abstract}


\section{Introduction}


Event-based cameras are neuromorphic vision sensors that produce visual signals as asynchronous spikes.\citep{Gallego2022} An event camera produces a spike when and only when a momentary pixel intensity difference exceeds a threshold, which can offer better energy efficiency and latency when compared with conventional cameras that produce data at a constant frame rate even when the scene is stationary during video recording.\par
Various visual tasks have been implemented using event-based cameras. Earlier datasets were generated by reproducing conventional image classification datasets such as MNIST\citep{Orchard2015}, CIFAR10\citep{Li2017-CIFARDVS}, and Caltech100\citep{Orchard2015}, where stationary images were placed in front of the camera and moved around to create pixel intensity differences along time. Many networks have been proposed for and performed well on these datasets where the object does not actually change over time.\citep{Zhu2022,Deng2022,Yao2021,Fang2021-deep,Fang2021-incorporating,Zheng2021,Wu2022-LIAF,kaiser2020,kugele2020,Shrestha2018,Lagorce2017,Sironi2018} Behaviorally dynamic datasets, such as DVS128 Gesture\citep{Amir2017}, DVS Lip\citep{Tan2022} were later created to maximize the event camera’s strength where the temporal evolutions of both the object’s shape and movement are critical.\par
To process these behaviorally and morphologically more dynamic event datasets, deep neural networks (DNNs) consisting of convolution and fully connected layers, and spiking neural networks (SNNs) have been adopted.\citep{Zhu2022,Deng2022,Yao2021,Fang2021-deep,Fang2021-incorporating,Zheng2021,Wu2022-LIAF,kaiser2020,kugele2020,Shrestha2018,Lagorce2017,Sironi2018,Amir2017,Tan2022,Xiao2020,Feng2020,Martinez2020,Wang2019,Wang2019-EV-gait,Gehrig2019,Wang2022-event-stream} Given that DNNs are specialized in processing static data, recurrent units, such as long short-term memory (LSTM)\citep{Hochreiter1997,Graves2013}, gated recurrent unit (GRU)\citep{Chung2014} and bidirectional gated recurrent unit (BiGRU)\citep{Tan2022,Xiao2020,Feng2020,Martinez2020} are typically required to process temporally sequential (global) information hidden in the series of momentary features. The features are processed synchronously in temporally local frames which are created either by simple accumulation of spikes within a prefixed time range\citep{Wang2019-EV-gait} or using input representation algorithms like Graph construction\citep{Wang2022-event-stream,Bi2019}, 3D point cloud\citep{Wang2019}, Event frame\citep{Rebecq2017},  Event spike tensor\citep{Gehrig2019} and voxel grid.\citep{Tan2022,Zhu2019}. Synchronized processing requires storing and analyzing a large number of events as a pre-processing step, which diminishes the advantages of asynchronous and sparse spike generation features of event-based cameras. Recurrent units require storing multiple state data for each node and increase the training cost.\par
%Alternatively, SNN-based networks have been adopted.
SNNs store temporal information in the neuron dynamics using models such as leaky integrate-and-fire (LIF) neurons\citep{Yao2021,Wu2022-LIAF} and can be trained using gradient-based approaches such as backpropagation through time (BPTT).\citep{werbos1990} To address the non-differentiability of spikes, surrogate approaches that replace spikes with neuron membrane potential have been developed.\citep{Neftci2019} However, BPTT requires backpropagation through both the network layers and through time, and is expensive to train.\par
To use spikes directly, feature descriptors such as Time Surface (TS)\citep{Lagorce2017,Sironi2018,Grimaldi2022} were proposed. TS stores only the last spike event for every pixel, and converts time to the last spike information into an analog value. The analog surface after TS conversion allows the encoded data to be processed with DNN and trained using gradient-based training. However, since TS only stores the last spike, it lacks capability to handle spatiotemporal features whose correlation is beyond its temporal neighbors, which is common in real-world problems. More advanced approaches such as Leaky Surface were subsequently developed to encode the temporal information beyond the last spike\citep{Cannici2019}. However, expensive pre-processing is still required, for example, to store the previous node state and time elapsed from the latest spike, and to calculate the new state for every pixel at every time instant.\par
%figure 1
\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{Figure1.png}
  \caption{RN-Net structure. $\emph{R}_{in}$ and $\emph{R}_{f}$ are reservoir layers for local and global temporal feature encoding, respectively. Bottom left: outputs from $\emph{R}_{in}$ from a representative input in the DVS Lip dataset. Bottom right: outputs from $\emph{R}_{f}$. Outputs from $\emph{R}_{f}$ are reshaped in 2D for better visualization. Deeper color in $\emph{R}_{in}$, $\emph{R}_{f}$ and output layers of Convolution (Conv) blocks and Fully-Connected ($\emph{FC}_{1-2}$) layers represents a higher analog value. Hidden layers within Conv blocks are not presented. The DNN structure for DVS Lip dataset is representatively illustrated. $\emph{C}_{N}$, $\emph{D}_{N}$, $\emph{MP}$, $\emph{FC}_{N}$ represent N-th Conv layer, kernel Depth of N-th Conv layer, Max Pooling layer and N-th FC layer, respectively.}
  \label{fig:fig1}
\vspace{-6mm}
\end{figure}
In this paper, we introduce a neural network architecture, Reservoir Nodes-enabled neuromorphic vision sensing Network (RN-Net) (Figure 1), that uses simple reservoir node (RN) layers along with DNN blocks for efficient processing of asynchronous data with low hardware implementation and training costs. Two RN layers are employed, with the one at the front natively encoding the temporally local information of asynchronous input spikes (events) for compatibility with DNN process, and the one at the back encoding the temporally global information for final classification, respectively.
%hidden in the series of the local spatiotemporal features captured by the front reservoir and convolution layers, respectively.
Physical implementations of RN-Net can be achieved with simple DNN blocks and RNs based on short-term memory (STM) memristors.\citep{Du2017,Moon2019}
%A STM memristor can be natively excited by an input spike, followed by a spontaneous decay. This property has allowed STM memristors to build reservoirs for tasks such as time-series analysis and prediction, which have already been physically implemented for lower-dimension tasks.\citep{Du2017,Moon2019}
Specifically, the RNs allow native feature description of the temporal information in all prior spike inputs produced by the event-based camera or hidden layers, without the use of recurrent units or dedicated memory and logic circuits to implement complex feature descriptor algorithms.\citep{Du2017,Moon2019,Moon2021,Chang2011} In this regard, RN-Net can be trained with simple DNN backpropogation rules, while maintaining better temporal information than TS encoding at lower hardware cost. RNs in the network can be thought of as analogous to cells in a retina which directly sense and encode raw and asynchronous visual inputs, and transmit them to the brain.\citep{Masland2012} An example of the proposed RN-Net is shown in Figure 1.\par
The main contributions of this work can be summarized below:\par
•	We implement a neural network structure that can directly process asynchronous spatiotemporal data generated by event-based cameras.\par
•	RNs based on STM memristors allow efficient temporal spike encoding at lower cost. The use of simple DNN blocks allows efficient gradient-based learning rules.\par
•	RN-Net achieves the highest reported accuracy of 99.2\% on DVS128 Gesture, and one of the highest classification accuracies of 67.5\% on DVS Lip dataset with a much lighter network structure than the previously reported networks.\par

\section{Background}

\subsection{Event-Based Dataset}

Beyond converting conventional static visual task datasets using the event-based camera\citep{Orchard2015,Li2017-CIFARDVS}, dedicated datasets have been created to maximize the strengths of event-based cameras.\citep{Amir2017,Tan2022,Bi2019-graph,Mueggler2016, Gehrig2021,Serrano-Gotarredona2015} Among them, we use the DVS128 Gesture and DVS Lip datasets to test our model. The DVS128 Gesture dataset is for human action recognition and the DVS Lip dataset is for word recognition based on lip motions of speaking participants. Both datasets include temporally dynamic data in terms of both shape and movement. DVS128 Gesture data are labeled in 11 categories (10 designated actions and 1 random action), while DVS Lip data are labeled in 100 categories with 50 relatively easy words and 50 relatively hard words. The number of training/test data of DVS128 Gesture and DVS Lip dataset are 1077/264 and 14896/4975, respectively.

%\subsection{Temporal Feature Descriptor}
\subsection{Related Work}

There are various approaches to process event (spike) inputs generated either from the event camera or from neurons in the preceding layers. SNNs rely on neuronal dynamics and can theoretically process temporal data in an asynchronous fashion. However, BPTT training is expensive, and local training rules result in loss of accuracy. DNN-based networks lack neuronal dynamics and require the spiking inputs (events) to be converted to analog static frames using various feature descriptor techniques to encode the temporal information. The simplest, and often used, method, is to neglect temporal information in the spike stream and just accumulate the temporally neighboring spikes to build frames as inputs.\citep{Wang2019-EV-gait} More sophisticated input representation techniques including Point clouds\citep{Wang2019}, Graph construction\citep{Wang2022-event-stream,Bi2019}, Grid-like\citep{Gehrig2019}, Image-like\citep{Wang2019-EV-gait} and Voxel grid\citep{Zhu2019} have been proposed to convert the temporally local spike streams into frames for DNN processing.\citep{Zhu2019,werbos1990,Neftci2019} However, these proposed feature descriptors require extensive preprocessing including random\citep{Wang2019} or local density-based\citep{Wang2022-event-stream} down-sampling, time-scaling\citep{Tan2022} and dedicated memory and arithmetic units to store the spatiotemporal information of the spikes and to perform the signal conversion. These costs diminish the latency and power advantages of event-based cameras, and the preprocessing often needs to be performed offline and prevents real-time operation.\par
In a representative feature descriptor such as TS, the amplitude of each node is reset to 1 every time it receives a spike and then relaxes following a decay function:

\vspace{-4mm}
\begin{equation}
S(t)=e^{-\frac{t-T(t)}{\tau}}
\end{equation}
\vspace{-6mm}

where \emph{S(t)} is an analog vector representing a node on the time surface, \emph{t} is the current time, \emph{T(t)} is the time information of the last spike received by the node, and $\tau$ is a pre-defined time constant.\citep{Grimaldi2022,tapiador-morales2020} Since only the last spike instant needs to be recorded, this encoding method reduces memory and computing cost compared to other feature descriptors. However, useful information prior to the last spike, which may be essential for features that evolve over longer history, is discarded. Additionally, storing the last spike timing information for each pixel (node), and calculating the analog state based on equation (1) still incurs substantial costs. Other techniques such as Leaky Surface were proposed to resolve the limitation of the last-spike-dominant encoding, but require higher hardware overhead to keep track of the state and time elapsed for all nodes.\citep{Cannici2019}

\subsection{Reservoir Nodes}

We note that in a reservoir computing (RC) system, the reservoir maintains short-term memory and performs nonlinear transformation (encoding) of the temporal input data into the reservoir states, represented by the states of the reservoir nodes (RNs). RC systems have been efficiently implemented in hardware using devices such as memristors for vision (MNIST handwritten digits recognition), speech (NIST TI46) and Time-series forecasting (Mackey-Glass time series) tasks.\citep{Du2017,Moon2019} In these systems, the reservoir nodes encode the spikes’ spatiotemporal information naturally following the internal device dynamics, without any external memory or arithmetic and logic units (ALUs). By leveraging the internal device and circuit dynamics to process temporal data, these implementations have shown excellent energy efficiency and performance.\citep{Du2017,Moon2019,Li2017,Farronato2022,Fang2021}\par
Generally, due to the STM property, RNs will be more strongly affected by the near history events and weakly affected by far history events, with the extent of the nonlinearity determined by the internal RN time constant.\citep{Chang2011,Du2015} Inspired by this principle, we hypothesize that RNs can be directly used to encode temporal spike data similar to what TS aims to accomplish, but at a lower cost and performing better spatiotemporal feature extraction beyond the last spike since RNs \emph{nonlinearly} accumulate all incoming spike information.


\section{Method}

The mechanism of RN encoding will be discussed in Section 3.1. The RN-Net architecture and training method will be introduced in Section 3.2 and Section 3.3, respectively.

\subsection{Event Encoding with Reservoir Nodes}

A reservoir node can be implemented using only a single STM memristor, making hardware implementation very light weight.\citep{Du2017,Moon2019} In a STM memristor, the node state (i.e. device conductance) is natively excited by the incoming spikes and relaxes in between the spikes,\citep{Chang2011,Du2015} as described by the following equation:\par
\vspace{-4mm}
\begin{equation}
G_{t}=P_{c}*(G_{max}-G_{t-1})*\delta_{spk}(t)+G_{t-1}*e^{-\frac{1}{\tau}}
\end{equation}
where $\emph{G}_{t}$ is the device (node) state at time $\emph{t}$, $\emph{P}_{c}$ is a potentiation factor, $\emph{G}_{max}$ is the upper bound of the device state, $\tau$ is the characteristic relaxation time constant, and $\delta_{spk}$\emph{(t)} is a delta function representing a spiking event:\par
\vspace{-4mm}
\begin{equation}
  \delta_{spk}(t)=\left\{
    \begin{array}{ll}
      0, & \mbox{when no spike}.\\
      1, & \mbox{when receiving spike}.
    \end{array}
  \right.
\end{equation}
$\delta_{spk}$\emph{(t)} can represent incoming events from an event-based camera or spikes from preceding layers within the network.\par
%figure 2
\begin{figure}[!ht]
  \centering
  %\includegraphics[width=\linewidth, height = 5]{Figure_2.ps}
  \includegraphics[scale=0.65]{Figure_2.png}
  \caption{Dynamics of a reservoir node (blue) and a time surface node (red) under identical spikes (black) shown below.}
  %\setlength{\belowcaptionskip}{-10pt}
  \label{fig:fig2}
\vspace{-5mm}
\end{figure}
Figure 2 shows the comparison of the RN approach implemented with a single STM memristor based on equation (2) versus the TS approach introduced in Section 2.2. Both approaches convert the spiking patterns into an analog state that can be processed by subsequent DNN blocks. Compared to TS encoding that resets the state to 1 after each spike, the RN implementation allows longer term history to be represented in the state in a non-linear fashion. For example, at $\emph{t}$=50, the TS output is identical to that at $\emph{t}$=30, since both values were reset to 1 10 time steps earlier (at $\emph{t}$=20 and 40, respectively). However, this representation missed the differences in the two temporal sequences (e.g. an additional preceding spike at $\emph{t}$=15). On the other hand, the RN implementation clearly differentiates the two cases as all inputs prior to the current time are accumulated nonlinearly. Combined with the hardware efficiency, e.g., equation (2) is natively implemented in a single device owing to internal device physics (ionic and electronic dynamic processes)\citep{Du2017,Moon2019} without the need of additional dedicated memory (to store $\emph{t}$ and \emph{T(t)}) and ALUs to convert the temporal data, we believe the RN-Net to be a suitable solution for asynchronous, real-time processing of event data.\par
The proposed RN-Net operates by directly taking asynchronous spikes as they are generated, and the memristor-based RNs transform the temporal spikes into analog values following equation (2) in real time. The conductance values from the RNs are retrieved only when the network operates a forward-pass, as shown in Figure 1, analogous to visual inputs encoded by cells in a retina.\citep{Masland2012}\par
%figure 3
\begin{figure}[!ht]
  \centering
  %\includegraphics[width=\linewidth]{Figure_3.png}
  \includegraphics[scale=0.4]{Figure_3.png}
  \caption{Temporally consecutive states of the input reservoir nodes, responding to asynchronous events in the (a) DVS128 Gesture dataset and (b) DVS Lip dataset. Each state is retrieved at a constant time interval of 30ms. Deeper red represents higher amplitude value of a RN state.}
  \label{fig:fig3}
\end{figure}
Figure 3 shows examples of 5 temporally consecutive states of the input layer ($\emph{R}_{in}$) RNs, responding to asynchronous events from the two datasets, respectively. The states are obtained at a constant time interval (i.e. 30ms). As shown in the figure, frequent input spikes lead to stronger RN states due to the excitation term (the first term of equation (2)), while RN states natively relax for inputs that are temporally too far from the current time instant due to the internal decay term (the second term of equation (2)). These properties of the RNs allow the $\emph{R}_{in}$ layer to capture both the spatial and temporal features of the inputs, as shown in Figure 3. For example, in Figure 3b, both the speaking motion and movement (top to bottom) of the lips can be captured by the RNs’ states in the $\emph{R}_{in}$ layer. Temporally fast movements are reflected in a single moment (e.g. the 3rd plot in Figure 3b), while temporally slow movements are reflected in multiple moments captured at the different time instants (e.g. the first two plots).


\subsection{Model Architecture}

The RN-Net is formed sequentially by an input RN layer ($\emph{R}_{in}$) as a feature descriptor for temporally local encoding, convolution (Conv) layers ($\emph{C}_{N}$), a spike conversion (SC) layer, another RN layer ($\emph{R}_{f}$) for temporally global encoding, and fully-connected (FC) layers ($\emph{F}_{N}$) for classification. Figure 1 illustrates the overall architecture of the proposed network for the DVS Lip dataset.\par
The $\emph{R}_{in}$ layer has as many RNs as the output dimension of the event-based camera. Each RN in the $\emph{R}_{in}$ layer processes input spikes asynchronously from a corresponding pixel in the event camera in real time without any preprocessing, following equation (2). Examples of the outputs generated by $\emph{R}_{in}$ are shown in Figure 3.\par
Depending on the application, different temporal resolutions can be chosen by adjusting the potentiation factor  $\emph{P}_{c}$ and the time constant $\tau$ of equation (2), and the interval of $\emph{R}_{in}$ state acquisitions. A shorter acquisition interval creates more frequent activations for the following layers. A shorter time constant $\tau$ and more frequent acquisitions produce temporally finer outputs, while a longer time constant and less frequent acquisitions produce spatially more detailed outputs owing to more input spikes and the slower internal decay.\citep{Tan2022} The convolution layers ($\emph{C}_{1}$-$\emph{C}_{7}$ with depths $\emph{D}_{1}$-$\emph{D}_{7}$ in Figure 1) then process the encoded spatiotemporal features in $\emph{R}_{in}$ at a constant time interval (i.e. 30ms). Similar to Conv blocks in conventional DNNs, the output $\emph{O}$ from the conv layers reflects the spatial features existing in the $\emph{R}_{in}$ states, and in this case, capturing the spatiotemporal features from the original spikes.\par
%figure 4
\begin{figure}[!ht]
  \centering
  \vspace{-3mm}
  %\includegraphics[width=\linewidth]{Figure_4.png}
  \includegraphics[scale=0.5]{Figure_4.png}
  \caption{(a) Visualization of asynchronous input spikes and (b) spikes generated after the spike conversion (SC) layer.}
  \label{fig:fig4}
\vspace{-3mm}
\end{figure}
Output $\emph{O}$ is then flattened and sent to the spike conversion SC layer. We use a predetermined threshold to convert the outputs $\emph{O}$ from the Conv layers into spikes in the SC layer. Spikes generated after the SC layer are shown in Figure 4. Thus, the spikes after SC represent the local spatiotemporal features captured by the $\emph{R}_{in}$ layer and the Conv layers, and become much sparser compared with the original spiking inputs.\par
The spikes from the SC layer are then supplied to the RNs in the $\emph{R}_{f}$ layer for global spatiotemporal feature capture and subsequent classification in the FC layers. Similar to the RNs in the $\emph{R}_{in}$ layer, RNs (e.g. also implemented with STM memristors) in $\emph{R}_{f}$ naturally potentiates/relaxes with the presence/absence of spikes from the SC layer. The state of $\emph{R}_{f}$ thus represents the historical (global) spatiotemporal features by encoding the processed local spatiotemperal features over time, and is then used by the subsequent FC layers ($\emph{FC}_{1}$-$\emph{FC}_{2}$ in Figure 1) to perform final classification functions. Similar to the RNs in the $\emph{R}_{in}$ layer, RN states in $\emph{R}_{f}$ are retrieved at a certain time interval (i.e. 0.3s, which is longer than that used in $\emph{R}_{in}$) over the whole video clip and fed to the FC layers.\citep{Alomar2015}\par

\subsection{Training Method}

We train RN-Net with standard backpropagation, using the output potentials calculated by $\emph{FC}_{2}$ at the end of data presentation. Unlike BPTT that calculates error and gradient across each timestep\citep{werbos1990}, error and gradient of RN-Net is calculated only once for one input training video, leading to lower training cost. To address the non-differentiability arising from spike generation in the SC layer, we adopted surrogate gradient for the SC layer. Since surrogate is used only in one layer of the network, we expect the error introduced by this approach is lower when compared with SNN approaches which require surrogates at all layers.\par
We also applied several data augmentation techniques, such as random/center crop, horizontal flip and Gaussian noise to minimize overfitting effects. In the case of random/center crop, we referred to existing works.\citep{Tan2022,Feng2020} During training, we center-cropped the original input dimension (128 x 128) to 96 x 96 and then random cropped them to 76 x 76, while the inputs are directly center-cropped to 76 x 76 in the testing phase. Horizontal flip with a probability of 0.5 and Gaussian noise with standard deviation of 5$e^{-4}$ were used during training for the same purpose. These techniques helped generalize the input data and reduce overfitting during training. These techniques were used only for the DVS Lip dataset since motions in DVS128 Gesture such as a left/right hand waving are sensitive to the horizontal flip and objects positions.\par

\section{Experiments}
\label{gen_inst}

The experimental setups for the two datasets are in Section 4.1, and the results are discussed in Section 4.2. In Section 4.3, we present an ablation study to show the utility of each component of the network in a controlled manner.\par

\subsection{Experiment Setup}
\label{headings}

The DVS128 Gesture dataset consists of repetitions of the same motion of a participant along the clip, while lip motions in the DVS Lip dataset are not repetitive, as shown in Figure 3. As a result, a fraction of a clip should be sufficient to classify the action in DVS128 Gesture, while the whole clip is necessary for DVS Lip classification. Based on this understanding, we only used the first 1.5s for all videos in the DVS128 Gesture dataset during training and inference, corresponding to ~10\% of the longest video (15.5s). The DVS Lip dataset has variable input lengths due to the irregular length of the words and the unique speaker’s traits, with most video clips (99\%) having lengths between 0.75s and 1.5s. To make the data size regular across the dataset, we chose to keep the captured $\emph{R}_{in}$ outputs at 50 by simply applying null (no spike) for clips shorter than 1.5s and clipping videos longer than 1.5s during DVS Lip training and inference. X- and Y-dimension were set as 128 and 128, respectively. $\emph{P}_{c}$ and $\tau$ in equation (2) were set to 0.5 and 60ms.\par
For both datasets, the same potentiation factor $\emph{P}_{c}$ (0.5) and time constant $\tau$ (60ms) in equation (2) are used for the RNs in $\emph{R}_{in}$. The states of $\emph{R}_{in}$ are captured every 30ms and sent to the subsequent Conv layers.\par 
Due to the different input dimensions and the different number of categories in the two datasets, the convolution layers and FC layers in RN-Net are configured accordingly. The detailed network configurations for both datasets are summarized in Table 1.\par

\begin{table}[!ht]
 \caption{Network configuration of RN-Net for DVS128 Gesture dataset (left columns) and DVS Lip dataset (right columns).}
  \centering
  \resizebox{\columnwidth}{!}{%
  
  %\begin{adjustbox}{scale=0.8,center}
  \begin{tabular}{cccccccccc}
    \toprule
    \multicolumn{5}{c}{DVS128 Gesture} & \multicolumn{5}{c}{DVS128 Lip}                   \\
    \cmidrule(r){1-5}
    \cmidrule(r){6-10}
    Layer     & Kernel & Out     & Pad/   & Output & Layer     & Kernel & Out     & Pad/   & Output\\
         & Size   & channel & Stride &   Dim  &     & Size   & channel & Stride &   Dim\\
    \midrule
    $\emph{R}_{in}$ & -  & 2  & - & 128 x 128 & $\emph{R}_{in}$ & - & 2 & - & 76 x 76   \\
    MaxPool  & \centering 2 &  2  & 0 / 2 & 64 x 64& Conv1   & 5 & 64  & 0 / 2 & 36 x 36 \\
    Conv1    & 3 &  64 & 0 / 1 & \centering 62 x 62& Conv2   & 3 & 128 & 1 / 1 & 36 x 36 \\
    MaxPool  & 3 &  64 & 0 / 2 & 30 x 30& MaxPool & 3 & 128 & 0 / 2 & 17 x 17 \\
    Conv2    & 3 & 128 & 1 / 1 & 30 x 30& Conv3   & 3 & 128 & 1 / 1 & 17 x 17 \\
    MaxPool  & 3 & 128 & 0 / 2 & 14 x 14& Conv4   & 3 & 256 & 1 / 1 & 17 x 17 \\
    Conv3    & 3 & 256 & 0 / 1 & 12 x 12& MaxPool & 3 & 256 & 0 / 2 & 8 x 8 \\
    Conv4    & 3 & 512 & 1 / 1 & 12 x 12& Conv5   & 3 & 256 & 1 / 1 & 8 x 8 \\
    MaxPool  & 3 & 512 & 0 / 2 & 5 x 64 & Conv6   & 3 & 512 & 1 / 1 & 8 x 8 \\
    Conv5    & 3 & 512 & 0 / 1 & 3 x 64 & MaxPool & 3 & 512 & 0 / 2 & 3 x 3 \\
    MaxPool  & 3 & 512 & 0 / 1 & 1 x 64 & Conv7   & 3 & 512 & 0 / 1 & 1 x 1 \\
    $\emph{R}_{f}$ & - & 512 & - & -    & $\emph{R}_{f}$ & - & 512 & - & -     \\
    FC1      & - & 512 & -     & -      & FC1   & - & 512 & -     & - \\
    FC2      & - & 11 & -     & -      & FC2   & - & 100  & -     & - \\
    \bottomrule
  \end{tabular}
  }
  \label{tab:table1}
%\end{adjustbox}
%\vspace{-1mm}
\end{table}

The output of a DNN block consists of a convolution and a pooling layer (if exists), following:
\begin{equation}
O_{N}=f(BN(MP(C(I_{N}))))
\end{equation}
where $\emph{O}_{N}$ is the output of N-th layer, $\emph{f(x)}$ is a ReLU activation function, $\emph{BN(x)}$ is a batch normalization function, $\emph{MP(x)}$/$\emph{C(x)}$ is a maxpooling/convolution operation, and $\emph{I}_{N}$ is input of the N-th layer.\citep{Ioffe2015}\par
After the last convolution layer, the outputs are converted to spikes for subsequent temporal feature encoding at $\emph{R}_{f}$.\citep{Eshraghian2022-training} The SC layer generates spikes by comparing the analogue values in $\emph{O}$ (Figure 1) with a pre-fixed threshold value, set as 0.3 for both datasets.\par

%figure 5
\begin{figure}[!ht]
  \centering
  %\includegraphics[width=\linewidth]{Figure_5.png}
  \includegraphics[scale=0.45]{Figure_5.png}
  \caption{Visualization of outputs from $\emph{R}_{f}$ over the whole 1.5s clip, along with spikes generated from the spike conversion layer. For Data2 whose input video length is only 0.9s, the RN states will continue to relax and still used for classification.}
  \label{fig:fig5}
\vspace{-6mm}
\end{figure}

RNs in the $\emph{R}_{f}$ layer encode the global spatiotemporal features. $\emph{P}_{c}$ and $\tau$ for the RNs in $\emph{R}_{f}$ are set as 0.1, 2s and 0.3, 2s for DVS128 Gesture and DVS Lip dataset, respectively. A longer $\tau$ is used in $\emph{R}_{f}$ than in $\emph{R}_{in}$ to process the longer temporal correlations. The RN states in the $\emph{R}_{f}$ layer are captured every 0.3s for both datasets (corresponding to 5 $\emph{R}_{f}$ acquisitions during a 1.5s clip). In the case of clips shorter than 1.5s, we let RNs in $\emph{R}_{f}$ continue relaxing after the last meaningful input without padding the input data with new spikes (e.g., through repeating the clip). These relaxed states are still captured following the normal schedule and fed to the classification layers. Examples of RN states at different time instants are shown in Figure 5, along with the spikes they receive from the SC layer. For better visualization, the data are reshaped in 2D format. As shown in the figure, even without any new inputs (e.g., after 0.9s for Data2), the RN states do not immediately decay to zero due to the slow decay term in equation (2), and the evolution of the RNs still represents useful information. This approach also simplifies training and inference processes and allows the system to handle inputs with irregular lengths.\par
A classification block consists of a FC layer, an activation function, and a batch normalization layer, following: 
\begin{equation}
O_{F}=f(BN(FC(I)))
\end{equation}
where $\emph{O}_{F}$ is the output of the block, $\emph{f(x)}$ is a ReLU activation function, $\emph{BN(x)}$ is a batch normalization function, $\emph{FC(x)}$ is an FC layer, and $\emph{I}$ is the input to the FC layer. In the proposed RN-Net, two classification blocks are used, and output of the 2nd block is obtained without the activation and the normalization function, and used as the final output of the network.\par
The Pytorch framework\citep{NEURIPS2019_bdbca288} was used for all the experiments with methods described above. SoftMax function was chosen to calculate the probability of an output neuron in final output, and the neuron with the largest probability was selected as the classification result. During training, the cost was derived by categorical cross-entropy function.\citep{Zhang2018} ATan was selected as surrogate gradient calculation for the SC layer during training.\citep{Eshraghian2022-training} We also used Adam optimizer\citep{Kingma2015} with an initial learning rate of 7$e^{-3}$ / 7$e^{-5}$ and a weight decay of 0 / 5$e^{-3}$ as the model optimizer and ReduceLROnPlateau with a factor of 0.9 / 0.9, patience of 2 / 1, threshold of 1$e^{-5}$ / 1$e^{-6}$, minimum learning rate of 1$e^{-4}$ / 1$e^{-7}$ as the learning scheduler for DVS128 Gesture / DVS Lip, respectively. Batch size of 32 and 150 epochs were used for both datasets. All experiments are performed on an Intel Xeon Gold 6226R and an NVIDIA A40.\par

\subsection{Experimental Results}
RN-Net shows excellent performance on both datasets; 99.2\% for DVS128 Gesture (highest reported to date) and 67.5\% for DVS Lip (one of the highest for event-inputs). Comparisons of classification accuracy with existing networks are shown in Table 2 and Table 3. The accuracies of networks listed in Table 2 were obtained from \citep{Tan2022}. For DVS128 Gesture, RN-Net outperforms all existing networks based on SNN and CNN platforms, while only using the first 10 percent of the longest clip. For DVS Lip, RN-Net achieves top 2 accuracy among event-based models, only behind MSTP\citep{Tan2022} which is a multi-branch network with different input channels generating frames in different temporal granularity, and employs Voxel Grid as the feature descriptor and using larger BiGRU and ResNet-18 network architectures. By comparison, RN-Net uses a single branch and a much lighter DNN network, without expensive feature description/preprocessing and recurrent units that can lead to significant hardware and latency overheads. The relevant model sizes and their performances are listed in Table 2. The parameters for ResNet variants are obtained from \citep{Leong2020}. Power of a proposed hardware system running RN-Net for both tasks is estimated based on existing device technology and reported values.\citep{Du2015} When running DVS128 Gesture and DVS Lip tasks, the proposed RN-Net hardware is estimated to consume 10.2mW and 11.4mW, respectively. Detailed explanation and methodology of the power estimation can be found in Supplementary information.\par

\begin{table}[!ht]
\small
 \caption{Comparison of existing models and RN-Net on Lip reading dataset including DVS Lip.}
  \centering
  \resizebox{\columnwidth}{!}{%
  \begin{tabular}{llcclllll}
    \toprule
    Model &   Input & ACC &Params&Base model& Preprocess& Local encoding & Global encoding\\
    &&[\%]&[M]&&&&&\\
    \midrule
    TANet\citep{Liu2021-TAM}     & Video       & 68.7 & 42.8 & ResNet-101 & - & TAM & Temporal AvgPool\\
    ACTION-Net\citep{Wang2021-action-net}     & Video  & 68.8 & 28.1 & ResNet-50  & Random Sampling & STE/CE/ME &Temporal AvgPool\\
    DFTN\citep{Xiao2020}     & Video       & 63.2 & 40.5 & ResNet-18 & - & DFN & BiGRU\\
    Feng et al\citep{Feng2020}     & Video & 63.4 & 11.2 & ResNet-18 & - & - & BiGRU\\
    Martinez et al\citep{Martinez2020} & Video & 65.5 & 11.2 & ResNet-18 & Greyscaling & - & Multi-scale TCN\\
    Event Clouds\citep{Wang2019} & Event & 42.2 & 8.1 & PointNet & Random Sampling & 3D point cloud & 3D point cloud\\
    EV-Gait-3DGraph\citep{Wang2022-event-stream} & Event & 32.0 & 7.2 & - & OctreeGrid Filtering & 3D-Graph & N/A\\
    EV-Gait-IMG\citep{Wang2019-EV-gait}     & Event & 34.5 & 64.6 & - & Event Noise Cancelling & Image-like & N/A\\
    EST\citep{Gehrig2019} & Event & 48.7 & 21.5 & ResNet-34 & Normalized Time Stamp & MLP (Grid) & N/A\\
    MSTP\citep{Tan2022} & Event & 72.1 & 38.5 & ResNet-18 & Multiple Time-Scaling & Voxel Grid & BiGRU\\
    \cmidrule(r){1-9}
    \textbf{RN-Net} & \textbf{Event} & \textbf{67.5} & \textbf{7.5} & \textbf{-} & \textbf{-} & \textbf{Reservoir} & \textbf{Reservoir}\\
    \bottomrule
  \end{tabular}%
  }
  \label{tab:table2}
\vspace{-2mm}
\end{table}


%\begin{table}[!h]
% \caption{Comparison of existing models and RN-Net for DVS128 Gesture.}
%  \centering
%  \begin{adjustbox}{scale=0.7,center}
%  \begin{tabular}{lll}
%    \toprule
%    Model &   Base & ACC[\%]\\
%    \midrule
%    Rollout[12]     & SNN       & 95.7\\
%    DECOLLE[11]     & SNN       & 95.5\\
%    SEW-ResNet[7]   & SNN       & 97.9\\
%    tdBN[9]         & SNN       & 96.9\\
%    PLIF[8]         & SNN       & 97.6\\
%    LIAF-Net[10]    & SNN       & 97.6\\
%    TA-SNN[6]       & SNN       & 98.6\\
%    TCJA-SNN[4]     & SNN       & 99.0\\
%    Amir et al[16]  & DNN       & 94.6\\
%    Event Clouds[21]& DNN       & 95.3\\
%    \cmidrule(r){1-3}
%    \textbf{RN-Net} & \textbf{DNN} & \textbf{99.2}\\
%    \bottomrule
%  \end{tabular}
%  \label{tab:table}
%\end{adjustbox}
%\vspace{-2mm}
%\end{table}

\begin{table}[ht]
    \parbox[t]{.40\linewidth}{
     \caption{Comparison of existing models and RN-Net for DVS128 Gesture.}
      \centering
      \begin{adjustbox}{scale=0.9,center}
        \begin{tabular}{lll}
            \toprule
            Model &   Base & ACC[\%]\\
            \midrule
            Rollout\citep{kugele2020}     & SNN       & 95.7\\
            DECOLLE\citep{kaiser2020}     & SNN       & 95.5\\
            SEW-ResNet\citep{Fang2021-deep}   & SNN       & 97.9\\
            tdBN\citep{Zheng2021}         & SNN       & 96.9\\
            PLIF\citep{Fang2021-incorporating}         & SNN       & 97.6\\
            LIAF-Net\citep{Wu2022-LIAF}    & SNN       & 97.6\\
            TA-SNN\citep{Yao2021}       & SNN       & 98.6\\
            TCJA-SNN\citep{Zhu2022}     & SNN       & 99.0\\
            Amir et al\citep{Amir2017}  & DNN       & 94.6\\
            Event Clouds\citep{Wang2019}& DNN       & 95.3\\
            \cmidrule(r){1-3}
            \textbf{RN-Net} & \textbf{DNN} & \textbf{99.2}\\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
    }
    \hfill
    \parbox[t]{.55\linewidth}{
        \caption{Performance comparison of different $\emph{R}_{in}$/$\emph{R}_{f}$ configurations for DVS128 Gesture and DVS Lip tasks.}
        \centering
        \begin{tabular}{llll}
            \toprule
            $\emph{R}_{in}$ &   $\emph{R}_{f}$ & DVS128 Gesture & DVS Lip\\
            \midrule
            TS          & TS            & 96.2          & 44.5\\
            TS          & RN          & 97.7          & 49.7\\
            RN          & TS            & 97.0          & 61.0\\
            \textbf{RN} & \textbf{RN} & \textbf{99.2} & \textbf{67.5}\\
            RN          & TAP        & 96.6          & 62.5\\
            \bottomrule
        \end{tabular}
    }
\vspace{-4mm}
\end{table}




\subsection{Ablation Study}

%\begin{table}[!h]
% \caption{Performance comparison of different Rin/Rf layer configurations for DVS128 Gesture and DVS Lip tasks.}
%  \centering
%  \begin{adjustbox}{scale=0.7,center}
%  \begin{tabular}{llll}
%    \toprule
%    $\emph{R}_{in}$ &   $\emph{R}_{f}$ & DVS128 Gesture & DVS Lip\\
%    \midrule
%    Time Surface            & Time Surface            & 96.2          & 44.5\\
%    Time Surface            & Reservoir Node          & 97.7          & 49.7\\
%    Reservoir Node          & Time Surface            & 97.0          & 61.0\\
%    \textbf{Reservoir Node} & \textbf{Reservoir Node} & \textbf{99.2} & \textbf{67.5}\\
%    Reservoir Node          & Temporal AvgPool        & 96.6          & 62.5\\
%    \bottomrule
%  \end{tabular}
%  \label{tab:table}
%\end{adjustbox}
%\vspace{-2mm}
%\end{table}

The effects of the RN in $\emph{R}_{in}$ and $\emph{R}_{f}$ layers on model performance are investigated in an ablation study by replacing each RN layer with a TS or Temporal average pooling (TAP, only for $\emph{R}_{f}$) layer without modifying the rest of the network. (Table 4) In both tasks, the case with both RN layers shows the best performance, while the case with TS at both places shows the worst performance. However, replacing $\emph{R}_{f}$ with TS degrades the performance more than that of $\emph{R}_{f}$ for the DVS128 Gesture task, while the DVS Lip task shows the opposite tendency. We attribute this difference to different characteristics of the datasets. Data in DVS128 Gesture are repetitive along the clip, which can tolerate less precise input encoding because there are opportunities to capture the lost information at a different time instant. On the other hand, in DVS Lip information at different time instants is unique, which makes input encoding more critical for the network performance.
Interestingly, although the use of RNs achieves the best results for both datasets, the use of TAP in the $\emph{R}_{f}$ layer achieves better results than the use of TS for the DVS Lip task, while the use of TS is better for the DVS128 Gesture task. We speculate that this is caused by the characteristics of TS, which completely discards far history that is critical for the DVS Lip dataset. On the other hand, the repetitive inputs in DVS128 Gesture alleviates this problem for TS implementation, where its capability to capture local temporal dynamics outperforms TAP. As a result, this study illustrates that the RNs’ capability to capture both local and global temporal dynamics allows networks based on them (i.e. RN-Net) to achieve higher accuracy for both tasks.\par

\section{Conclusion}

In this work, we propose a hybrid reservoir/DNN network, Reservoir Nodes-enabled neuromorphic vision sensing Network (RN-Net), for asynchronous event-based video processing. The use of dynamic reservoir nodes enables efficient spatiotemporal encoding of both local and global features at different hierarchies in real time and eliminates external memory and logic/recurrent units. RN-Net achieves the highest DVS128 Gesture classification accuracy, and one of the highest DVS Lip classification accuracies with a much lighter network structure. The reservoir nodes can be efficiently implemented with STM memristors, taking advantage of internal device physics to perform signal processing. The low hardware and training costs of RN-Net make it an attractive option for event-camera applications.\par


%\section*{Acknowledgments}
\acksection
This work was supported in part by the National Science Foundation through Awards $\sharp$ 1915550 and $\sharp$ CCF- 1900675, and SRC and DARPA through the Applications Driving Architectures (ADA) Research Center\par

%References follow the acknowledgments in the camera-ready paper. Use unnumbered first-level heading for
%the references. Any choice of citation style is acceptable as long as you are
%consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
%when listing the references.
%Note that the Reference section does not count towards the page limit.
\medskip

\bibliographystyle{unsrt}
\small
\bibliography{References}

\end{document}