\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{sort&compress, square, numbers}{natbib}
% before loading neurips_2023

% ready for submission
\usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}       % graphics
\usepackage{adjustbox}



\title{RN-Net: Reservoir Nodes-Enabled Neuromorphic Vision Sensing Network}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Sangmin Yoo, Eric Yeu-Jer Lee, Ziyu Wang, Xinxin Wang, Wei D. Lu\thanks{Corresponding author} \\
  Department of Electrical Engineering and Computer Science\\
  University of Michigan\\
  Ann arbor, MI, USA\\
  \texttt{\{ysangmin, ericylee, ziwa, xinxinw, wluee\}@umich.edu} \\
}

\renewcommand{\thesection}{\Alph{section}}

\renewcommand{\theequation}{{S}\arabic{equation}}

\renewcommand{\thetable}{{S}\arabic{table}}

\usepackage{enumerate}
 
    



\begin{document}


\maketitle

\section{Power Estimation}

The power efficiency largely depends on implementation methods of reservoir nodes (RNs), using either a memristor or a resistor-capacitor (RC) unit.\citep{Du2017,Moon2019,Fang2021} In the following sections, the power of memristor-based hardware is estimated.\par

\subsection{Spike Encoding}

One physical RN device (i.e. a memristor) is assigned to one pixel of the event-based camera. Every time the pixel senses an intensity difference larger than the pre-fixed value, it produces a spike (event), which is input to the dedicated RN. Then, the RN natively encodes temporal information of the spikes following the equation:\par

\begin{equation}
G_{t}=P_{c}*(G_{max}-G_{t-1})*\delta_{spk}(t)+G_{t-1}*e^{-\frac{1}{\tau}}
\end{equation}
where $\emph{G}_{t}$ is the device state at time $\emph{t}$, $\emph{P}_{c}$ is a potentiation factor, $\emph{G}_{max}$ is the upper bound of the device state, $\tau$ is the characteristic relaxation time constant of the device, and $\delta_{spk}$\emph{(t)} is a delta function representing a spiking event:\par
\begin{equation}
  \delta_{spk}(t)=\left\{
    \begin{array}{ll}
      0, & \mbox{when no spike}.\\
      1, & \mbox{when receiving spike}.
    \end{array}
  \right.
\end{equation}
$\delta_{spk}$\emph{(t)} can represent incoming events from an event-based camera or spikes from preceding layers within the network.\par


The total spike encoding energy consumed by RNs in the $\emph{R}_{in}$ layer during a clip can be calculated following:\par

\begin{equation}
E_{encoding,in}=\sum_{n=1}^{N} {V_{pulse}}^{2}*G(x_{n},y_{n},p_{n})*t_{pulse}
\end{equation}

where $\emph{E}_{encoding}$ is the total energy consumed in the $\emph{R}_{in}$ layer, N is the total number of input spikes in the video clip, $\emph{V}_{pulse}$ is the (fixed) amplitude of the input spike, $\emph{G}$($\emph{x}_{n}$,$\emph{y}_{n}$,$\emph{p}_{n}$) is the conductance of the RN device assigned to the pixel located at $\emph{x}_{n}$, $\emph{y}_{n}$, with polarity $\emph{p}_{n}$, and $\emph{t}_{pulse}$ is the (fixed) time duration of the spike.\par

Similarly, in $\emph{R}_{f}$, an RN device is assigned to a neuron in the output layer of the convolution blocks. The total spike encoding energy of $\emph{R}_{f}$ during a clip is calculated as:\par

\begin{equation}
E_{encoding,f}=\sum_{i=1}^{N_{f}} {V_{pulse}}^{2}*G(N_{i})*t_{pulse}
\end{equation}
where $\emph{G}$($\emph{N}_{i}$) is the conductance of the RN device assigned with the $\emph{i}$-th neuron in the output layer of convolution blocks along a clip. $\emph{N}_{f}$ is the total number of spikes from the output layer during the state retrieval window.\par
Considering the minimum time interval (1$\mu$s) of events for typical event-based cameras,\citep{Amir2017,Tan2022} we set  $\emph{t}_{pulse}$ to 1$\mu$s for both $\emph{R}_{in}$ and $\emph{R}_{f}$. $\emph{V}_{pulse}$ and $\emph{G}_{max}$ are set to 1.5 V and 100 $\mu$S, according to \citep{Du2015}.\par
We simulated the average total energy consumption of $\emph{R}_{in}$ and $\emph{R}_{f}$ for a typical clip in DVS128 Gesture and DVS Lip tasks. Using the total energy and the total number of spikes, the average energy consumption per spike was also derived. The results are shown in Table S1.\par

\begin{table}[!h]
\small
 \caption{Total encoding energy and the energy per spike for spike encoding in $\emph{R}_{in}$ and $\emph{R}_{f}$, for a typical DVS128 Gesture and DVS Lip video clip.}
  \centering
  %\resizebox{\columnwidth}{!}{%
  
  \begin{adjustbox}{scale=1.1,center}
  \begin{tabular}{cccc}
    \toprule
    Dataset &   $\emph{R}_{in}$/$\emph{R}_{f}$ & Total Energy & Energy/Spike\\
    &&[$\mu$J]&[pJ]\\
    \midrule
    DVS128 Gesture & $\emph{R}_{in}$  & 15.0 & 156.2\\
                   & $\emph{R}_{f}$   & 1.1  & 123.3\\
    DVS Lip        & $\emph{R}_{in}$  & 0.3  & 22.8 \\
                   & $\emph{R}_{f}$   & 0.7  & 185.7\\
    \bottomrule
  \end{tabular}%
  %}
  \label{tab:table}
\end{adjustbox}
\end{table}

\subsection{State Retrieval}

The states of RNs in $\emph{R}_{in}$ and $\emph{R}_{f}$ are retrieved for the forward-pass every 30ms and 300ms, respectively. The total retrieval energy of $\emph{R}_{in}$ and $\emph{R}_{f}$ is calculated as:\par

\begin{equation}
E_{retrieval,in}=\sum_{p=0}^{1} \sum_{y=1}^{128} \sum_{x=1}^{128} {V_{read}}^{2}*G(x,y,p)*t_{read}
\end{equation}

\begin{equation}
E_{retrieval,f}=\sum_{i=1}^{D_{7}} {V_{read}}^{2}*G(N_{i})*t_{read}
\end{equation}

where $\emph{E}_{encoding,in}$ and $\emph{E}_{encoding,f}$ are the energy consumption for the state retrieval of $\emph{R}_{in}$ and $\emph{R}_{f}$, respectively, $\emph{V}_{read}$ is the amplitude of the read pulse, $\emph{G(x,y,p)}$ is the conductance of a node located at $\emph{x}_{n}$,$\emph{y}_{n}$, with polarity $\emph{p}_{n}$ in $\emph{R}_{in}$, $\emph{G}$($\emph{N}_{i}$) is the conductance of a node assigned to the $\emph{i}$-th node in $\emph{R}_{f}$, and $\emph{t}_{read}$ is the duration of the read pulse.\par
Different from spike encoding, the read operations do not induce conductance change. For state retrieval, the $\emph{V}_{read}$ amplitude was set to 0.5 V according to \citep{Du2015}, and pulse duration was set as 1$\mu$s for both $\emph{R}_{in}$ and $\emph{R}_{f}$ layers.\par

\begin{table}[!h]
\small
 \caption{Total retrieval energy during a DVS128 Gesture and DVS Lip video clip, and the energy per state retrieval and per node, for $\emph{R}_{in}$ and $\emph{R}_{f}$ RN layers.}
  \centering
  %\resizebox{\columnwidth}{!}{%
  
  \begin{adjustbox}{scale=1.1,center}
  \begin{tabular}{ccccc}
    \toprule
    Dataset &   $\emph{R}_{in}$/$\emph{R}_{f}$ & Total Energy & Energy/Retrieval & Energy/Node\\
    &&[nJ]&[nJ]&[pJ]\\
    \midrule
    DVS128 Gesture & $\emph{R}_{in}$  & 2234.9 & 44.7 & 1.4\\
                   & $\emph{R}_{f}$   & 24.6   & 4.9  & 9.6\\
    DVS Lip        & $\emph{R}_{in}$  & 178.7  & 3.6  & 0.3\\
                   & $\emph{R}_{f}$   & 33.4   & 6.7  & 13.0\\
    \bottomrule
  \end{tabular}%
  %}
  \label{tab:table}
  
\end{adjustbox}
\end{table}

Based on the energy per read, the number of read retrievals and the number of nodes in both reservoir layers, the average total energy of RN state retrieval during the whole video clip, energy per state retrieval, and energy per node were calculated. The values are presented in Table S2.\par



\subsection{Overall System Power Estimation}
The total energy of the RN-Net hardware system can be calculated by summing up the energy used by the $\emph{R}_{in}$ and $\emph{R}_{f}$ layers calculated above, plus the energy used for the DNN blocks to run a clip for DVS128 Gesture and DVS Lip. To calculate the energy of the DNN blocks, we derived the number of MAC operations for running a video clip in RN-Net, according to Table 1. The number of MAC operations during a video clip was calculated based on the number of MAC operations in the convolution and fully-connected layers, multiplied by the number of forward passes (i.e. 50) through these layers during the clip. We use published, conservative TOPS/W value (i.e. 2 TOPS/W from the Google Edge TPU \citep{TPU_edge}) to estimate the energy used by the DNN blocks since these blocks can be implemented in any digital accelerator. By adding all energy costs, the total energy consumed by RN-Net per video clip, and the average power of RN-Net are estimated and presented in Table S3.\par

\begin{table}[!h]
\small
 \caption{Total energy and average power of the RN-Net system, along with the total number of MAC operations in the DNN blocks when running DVS128 Gesture and DVS Lip tasks.}
  \centering
  %\resizebox{\columnwidth}{!}{%
  
  \begin{adjustbox}{scale=1.1,center}
  \begin{tabular}{cccccc}
    \toprule
    Dataset & Convolution/ & Fully-connected & Whole Network & Energy & Power\\
    &forward-pass&&&&\\
    &[MOPS]&[MOPS]&[GOPS]&[mJ]&[mW]\\
    \midrule
    DVS128 Gesture & 608.7 & 2.7 & 30.4 & 15.2 & 10.2\\
    DVS Lip        & 686.3 & 2.7 & 34.3 & 17.2 & 11.4\\
    \bottomrule
  \end{tabular}%
  %}
  \label{tab:table}
  
\end{adjustbox}
\end{table}
\medskip

\bibliographystyle{unsrt}
\small
\bibliography{References}

\end{document}