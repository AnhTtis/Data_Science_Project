{
    "arxiv_id": "2303.12737",
    "paper_title": "Comparing Trajectory and Vision Modalities for Verb Representation",
    "authors": [
        "Dylan Ebert",
        "Chen Sun",
        "Ellie Pavlick"
    ],
    "submission_date": "2023-03-08",
    "revised_dates": [
        "2023-03-23"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
    ],
    "abstract": "Three-dimensional trajectories, or the 3D position and rotation of objects over time, have been shown to encode key aspects of verb semantics (e.g., the meanings of roll vs. slide). However, most multimodal models in NLP use 2D images as representations of the world. Given the importance of 3D space in formal models of verb semantics, we expect that these 2D images would result in impoverished representations that fail to capture nuanced differences in meaning. This paper tests this hypothesis directly in controlled experiments. We train self-supervised image and trajectory encoders, and then evaluate them on the extent to which each learns to differentiate verb concepts. Contrary to our initial expectations, we find that 2D visual modalities perform similarly well to 3D trajectories. While further work should be conducted on this question, our initial findings challenge the conventional wisdom that richer environment representations necessarily translate into better representation learning for language.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.12737v1"
    ],
    "publication_venue": "4 pages, 1 figure"
}