\noindent \textbf{Traditional Network Pruning}
%The relationship of neural networks to biological models can be extended to the field of network pruning: human brains contain sparse connections \cite{friston_hierarchical_2008}, which can be key to how information is stored and processed \cite{ahmad_how_2016}.  %this sentence is shit.  needs rewrite
The effectiveness of sparse neural networks was first demonstrated by Lecun et. al \cite{lecun_optimal_1989}.  With the advent of deep learning, the size and efficiency of ML models quickly became a critical limitation.  Naturally, research aimed at decreasing size \cite{han_learning_2015, hinton_distilling_2015}, and limiting power and energy consumption \cite{yang_designing_2017}.  

%Neural architecture search has also been proposed to limit model size.  Such searches sometimes utilize pruning strategies \cite{engelbrecht_new_2001}, or genetic algorithms to find weightless architectures \cite{gaier_weight_2019}. % to find weightless architectures.  
\vspace{-2pt}
\noindent \textbf{Lottery Ticket Hypothesis}
%The Lottery Ticket Hypothesis  \cite{frankle_lottery_2019} has spawned several avenues of applied and theoretical research.  
The Lottery Ticket Hypothesis found that dense networks contained randomly-initialized subnetworks that, when trained on their own, achieved accuracy comparable to the original dense model. However, the approach required training a dense network in order to identify winning tickets.  Subsequent work identified strategies to prune \gls{dnns} \textit{without} a pretrained model using greedy forward selection \cite{ye_good_2020}, mask distances \cite{you_drawing_2022}, flow preservation techniques \cite{wang_picking_2019,tanaka_pruning_2020}, and  channel importance \cite{wang_pruning_2019}.  
%\vspace{-3pt}
%Theoretical justification was initiated for these claims with the Strong \gls{lth} \cite{malach_proving_2020}, additionally showing that random weights could provide strong subnetworks.  Subsequent works showed improvements on the size of the network required for high-performance \cite{pensia_optimal_2020, orseau_logarithmic_2020}.  %Applied methods being proposed alongside these 

%\citet{malach_proving_2020} first showed mathematical justification for the \gls{lth} with the \textit{Strong} \gls{lth}, showing that the width of a randomly-initialized network must exceed some target network by a polynomial term. Additionally they showed that the neural networks do not need to be trained at all.   \citet{pensia_optimal_2020} and \citet{orseau_logarithmic_2020} offered improvements on this theorem, showing that the network width needs to be only logarithmically wider than fully-trained dense networks.  
%\citet{ramanujan_whats_2020} provided substantial empirical results showing that \textit{randomly-initialized} neural networks contain high-accuracy subnetworks, finding high-performing networks masks across a range of architectures and datasets.  
%\citet{chijiwa_pruning_2021} propose re-randomizing neural networks iteratively in order to find highly-accurate subnetworks in smaller parameter spaces.  They modified the Edge-popup algorithm described in \cite{ramanujan_whats_2020} to empirically demonstrate their results.   Finally, the recently proposed Multi-Prize Lottery Ticket poses that there are \textit{many} randomly-initialized subnetworks which attain accuracy's comparable to dense fully-trained models.  They extend this finding to binary neural networks, detailed below.  

\noindent \textbf{Randomized Neural Networks}
Important to work described in \cite{ramanujan_whats_2020,chijiwa_pruning_2021,malach_proving_2020},
 randomized neural networks \cite{gallicchio_deep_2020} have also been explored in shallow architectures.  Several applications explore randomization, including random vector functional links \cite{needell_random_2020,pao_learning_1994,pao_functional-link_1992},  random features for kernel approximations \cite{le_fastfood_2013,rahimi_random_2007,hamid_compact_nodate}, reservoir computing \cite{lukosevicius_reservoir_2009}, and stochastic configuration networks \cite{wang_stochastic_2017}.  %Such applications employ unique training processes that can be be much simpler than training a full architecture, while potentially losing some accuracy as a result.  

\noindent \textbf{Binary Neural Networks} \gls{bnns} studied in this paper fall into the class of quantized neural networks.  Like pruning, quantization is a natural approach for model compression. %\cite{neill_overview_2020}.  \citet{diffenderfer_multi-prize_2021} employ binary quantization (binarization) of network weights MPT(1/32) and additionally network weights \textit{and} activations MPT(1/1).   Biprop searches for \gls{mpts} in both scenarios, with strong results in MPT(1/32)'s and promising results in MPT(1/1). In this work, we consider networks with binary weights (MPT(1/32)'s).  
Common techniques for creating quantized networks include post-training quantization with retraining \cite{gysel_ristretto_2018, dettmers_8-bit_2016} and quantization-aware training \cite{gupta}.  In the Biprop algorithm proposed in \cite{diffenderfer_multi-prize_2021}, quantization-aware binary neural networks are trained with parameter $\alpha$, which enables floating-point weights to learn a scale parameter prior to binarization   \cite{martinez_training_2020}.  

%Past work has found success in quantizing pretrained models and then retraining \, however a key 

%Theoretical progress for the \gls{lth} was first shown by \citet{malach_proving_2020}, who  mathematically formalized \gls{lth}, using an approximation theorem to describe a lower bound on the width required for a random subnetwork to be found that approximates some target network.  Subsequent works relaxed these proposed bounds, showing  to a log

