In this section, we review current state-of-the-art methods for pruning randomly initialized and binary randomly initialized neural networks. 


\noindent \textbf{Randomly Initialized \gls{dnns}}
Given a neural network $f(x;\theta)$ with layers 1,...$L$, weight parameters $\theta\in\mathbb{R}^{n}$ randomly sampled from distribution $\mathcal{D}$ over $\mathbb{R}$, and dataset $x$, we can express a subnetwork of $f(x;\theta)$ as $f(x;\theta   \odot \mathrm{M})$, where $\mathrm{M}\in\{0,1\}^n$ is a binary mask and $\odot$ is the Hadamard product. 

Edge-popup \cite{ramanujan_whats_2020} finds $\mathrm{M}$ within a randomly-initialized DNN by optimizing weight scoring parameter $\mathrm{S}\in\mathbb{R}^{n}$ where $\mathrm{S}\sim\mathcal{D}_{score}$.  $\mathrm{S}_i$ can be intuitively thought of as an \textit{importance score} computed for each weight $\theta_i$.  The algorithm takes pruning rate hyperparameter $p \in [0,1]$, and on the forward pass computes $\mathrm{M}$ at $\mathrm{M}_i$ as

\vspace*{-\abovedisplayskip}
%\vspace{-2pt}
\begin{equation}
    \mathrm{M}_i=   \begin{cases}
    1 & \text{if $|\mathrm{S}_i| \in \{\tau(i)_{i=1}^{k_j}\ge[k_jp/100]\}$}\\%, {|\mathrm{S}^j_{\tau(i)}|} \le |\mathrm{S}^j_{\tau(i+1)}|$ } \\
    0 & \text{otherwise}
  \end{cases}
\end{equation}
\vspace*{-\abovedisplayskip}

where $\tau$ sorts indices $\{i\}^j_{i=1}\in\mathrm{S}$ such that $|S_{\tau(i)}|\le|S_{\tau(i+1)}|$.
%where $\mathrm{S}^{top_k}$denotes top 100(1-p)\% of $|\mathrm{S}|$.  
In other words, masks are computed at each weight by taking the absolute value of scores for each layer, and setting the mask to 1 if the absolute score value falls within the top 100*$p$\%, otherwise they set the mask to zero.  
They use the straight-through estimator \cite{bengio_estimating_2013} to backpropagrate through the mask and update $\mathrm{S}$ via SGD.  

Chijiwa et. al \cite{chijiwa_pruning_2021} improved on the Edge-Popup algorithm with the IteRand algorithm.  They show that by rerandomizing pruned network weights
 during training, better subnetworks can be found. They theoretically prove their results using an approximation theorem indicating rerandomization operations effectively reduce the required number of parameters needed to achieve high-accuracy subnetworks.  
 
 The IteRand algorithm is mainly driven by two hyperparameters: $K_{per}$ and re-randomization rate $r$.  $K_{per}$ drives the frequency weights will be re-randomized.   The second hyperparameter, $r$, denotes a  \textit{partial} re-randomization of pruned weights.  To achieve the best results, the authors set $r$ to 0.1, meaning re-randomizing 10\% of pruned weights. 
 \vspace{.3em}

%https://tex.stackexchange.com/questions/149162/how-can-i-define-a-foreach-loop-on-the-basis-of-the-existing-forall-loop
\begin{algorithm}
\caption{Edge-Popup with IteRand}\label{alg:cap}
\begin{algorithmic}[1]
\State \textbf{Require:}$\theta \sim \mathcal{D}_{weight},$ $ \mathrm{S} \sim \mathcal{D}_{score}$, $p$, $K_{per}$, $r$
\State \textbf{Input:  } Dataset(X,Y)
        \Function{EdgePopup}{\textrm{S}, \textrm{M},  f(x)}
        \ForEach {$l \in L $}  %\COMMENT{Edge-Popup}
        %\ForEach {$ \in \mathrm{S} $}
        \IIf{$|s_i|\in$ \text{top} $k$ $|\mathrm{S}_l|$} $\mathrm{M}_i= 1$ \ElseIIf$\mathrm{M}_i= 0$\EndIIf
        %\EndFor
        \EndFor
        \State \Return $\mathrm{S}, \mathrm{M}$
        \EndFunction
      \For{\textit{i=1 ..., N-1} }
        \State $x,y \gets \Call{MiniBatch}{X,Y}$
        \State $\mathrm{S}, \mathrm{M} \gets \Call{Edge-Popup}{\mathrm{S}, \mathrm{M}, f(x)}$
        \If{$i \mod K_{per}=0$}
        \State $\theta \gets Rerandomize(\theta, \mathrm{M})$
        \EndIf
      \EndFor
\end{algorithmic}
\end{algorithm}


  \vspace{.3em}




\noindent \textbf{Randomly-Initialized \gls{bnns} }\label{biprop}
Complementary to the findings reported in the previous section, Diffenderfer and Kailkhura \cite{diffenderfer_multi-prize_2021} described a new method for finding high accuracy subnetworks within binary-weighted models.  This finding provides us the ability to store bit size weights rather than floating-point (32 bit) numbers, leading to substantial compression of large models.  In this section, we summarize the Biprop algorithm.  

We start with a modification of the function described in the previous section, replacing $\theta\in\mathbb{R}^{n}$ with binary weights $\mathcal{B}\in\{-1,1\}$.  The resulting network function becomes $f(x;\mathcal{B} \odot \mathrm{M})$, with mask $\mathrm{M}$  over binary weights. Further, Biprop introduces scale parameter $\alpha\in\mathbb{R}$, which utilizes floating-point weights $\theta$ \textit{prior} to binarization \cite{martinez_training_2020}.  The learned parameter rescales binary weights to $\{-\alpha,\alpha\}$, and the resulting network function becomes $f(x;\alpha(\mathcal{B} \odot \mathrm{M}))$.  Parameter $\alpha$ is updated with $||\mathrm{M}\odot\theta||_1/||\mathrm{M}||_1$, with  $\mathrm{M}$ being multiplied by $\alpha$ for gradient descent (the straight-through estimator is still used for backpropagation). During test-time, the learned alpha parameter simply scales a binarized weight vector. As a result, only bit representations of the weights are needed at positive mask values ($\pm1$ where $\mathrm{M}=1$), substantially reducing memory, storage, and inference costs.  

 %To optimize \mathrm{m} over binary weights, \citet{diffenderfer_multi-prize_2021} introduce unique network modifications distinct from the Edge-Popup algorithm.  First, floating-point weights are used during the training process.  Specifically, $\alpha$ is learned by calculating the sum of absolute values of unpruned floating-point weights, and dividing it by the total number of unpruned weights. Mask \mathrm{m} is then multiplied by $\alpha$ for gradient descent (the straight-through estimator is still used for backpropagation).  During test-time, the learned alpha parameter simply scales a binarized weight vector. As a result, only bit representations of the weights are needed at positive mask values ($\pm1$ where \mathrm{m}=1), substantially reducing memory, storage, and inference costs.  
 
 %The use of continuous randomly-initialized weights during the training process is tactfully demonstrated programmatically, with the forward pass of the network only computing the binary representations of the continuous weight vectors.  As a result, during test time, only bit representations of the weights at positive mask values are needed, substantially reducing memory, storage, and inference costs.  
 
 Empirically, Diffenderfer and Kailkhura \cite{diffenderfer_multi-prize_2021} are able to produce high accuracy binary subnetworks using Biprop on a range of network architectures, and theoretically prove this result on models with sufficient over-parameterization.  
 In the subsequent section we show how we can modify this algorithm, as well as Edge-Popup, with Weight Recycling to achieve increased performance.  