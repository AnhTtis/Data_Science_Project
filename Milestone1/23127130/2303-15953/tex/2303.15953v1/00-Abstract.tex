  %Recent progress in pruning dense neural networks has shown that we can find high-accuracy subnetworks in randomly initialized models.  
 % The Multi-Prize Lottery Ticket Hypothesis \citep{diffenderfer_multi-prize_2021} posits that randomly initialized neural networks contain several subnetworks that achieve comparable accuracy to fully trained models of the same architecture.  However, current research requires that the network is \textit{sufficiently overparameterized}.   In this work, we propose a modification to two state-of-the-art algorithms (Edge-Popup and Biprop) that finds subnetworks with a higher accuracy with no additional storage cost or scaling. The algorithm, Iterative Weight Recycling, replaces irrelevant weights in a randomly initialized network with weights deemed as important.  Empirically we show improvements on smaller network architectures, finding that model parameterization can be reduced by reusing already existing weights.
 % In addition to Iterative Weight Recycling, we complement the Multi-Prize Lottery Ticket Hypothesis with a reciprocal finding:  high-accuracy, randomly initialized subnetwork's produce diverse masks, despite being generated with the same hyperparameter's and pruning strategy.  We explore the landscapes of these masks, which show extreme variability.  
 

  The Multi-Prize Lottery Ticket Hypothesis posits that randomly initialized neural networks contain several subnetworks that achieve comparable accuracy to fully trained models of the same architecture.  However, current methods require that the network is sufficiently overparameterized.   In this work, we propose a modification to two state-of-the-art algorithms (Edge-Popup and Biprop) that finds high-accuracy subnetworks with no additional storage cost or scaling. The algorithm, Iterative Weight Recycling, identifies subsets of important weights within a randomly initialized network for intra-layer reuse.  Empirically we show improvements on smaller network architectures and higher prune rates, finding that model sparsity can be increased through the "recycling" of existing weights.
  In addition to Iterative Weight Recycling, we complement the Multi-Prize Lottery Ticket Hypothesis with a reciprocal finding:  high-accuracy, randomly initialized subnetwork's produce diverse masks, despite being generated with the same hyperparameter's and pruning strategy.  We explore the landscapes of these masks, which show high variability.  