In Figure \ref{norms}, we analyze the norms of the weights chosen by various subnetwork identification algorithms.  We train each subnetwork identification algorithm on Kaiming Normal randomly initialized weights with a 50\% prune rate.  

In Figure \ref{norms}, we measure the Frobenius norms of each layer for the unpruned masks (denoted with a "+"), as well as the norms of weights for the pruned masks (denoted with a "-").  Unpruned masks ("+") are the identified subnetworks chosen by each algorithm.   Results show that for each of the four algorithms, the weights at positive masks contain higher norms than the weights of disposed zero masks.  Notably, the two highest identified norms, IteRand and Weight Recycling, perform the strongest.  Weight Recycling negative masks overlap with its positive masks almost identically, and Edge-Popup and Biprop negative masks overlap almost identically with each other.  

This shows evidence that high norm weights are beneficial to a successful subnetwork in each algorithm, with Iterative Weight Recycling emphasizing the reuse of these high norm weights across its pruned and unpruned weight masks.  

\begin{figure}[!h]
\begin{center}

   \includegraphics[scale=0.75]{appendix/figures/norms2.pdf}
    \caption{  Frobenius norms across algorithms and model layers for pruned and unpruned weights. "+" denotes the learned high-accuracy subnetwork, "-" denotes the pruned weights.  Each algorithm keeps weights with higher norms, discarding lower norm weights.  Weight Recycling reuses high norm weights to identify the best subnetwork.     }\label{norms}
    \centering
\end{center}
\end{figure}
%\FloatBarrier