For CIFAR-10 experiments, we test each algorithm using Conv-2 to Conv-8 architectures as well as ResNet18.  Additionally, we test each algorithm with layer width parameter $p$ for Conv-2 to Conv-8 algorithms.  These architectures are built using the same codebase as Edge-Popup and IteRand.  We refer to the Edge-Popup GitHub for model definitions: \newline
\href{https://github.com/allenai/hidden-networks/tree/master/models}{https://github.com/allenai/hidden-networks/tree/master/models}
%\href[https://github.com/allenai/hidden-networks/tree/master/models]{https://github.com/allenai/hidden-networks/tree/master/models} 

\noindent \textbf{Baseline models}
For our baseline models, we train dense models with learned weights.  We use the SGD optimizer initialized with kaiming normal weights, with a learning rate of 0.01 for 100 epochs with batch size 128.  We additionally use a weight decay of 1e-4 and momentum 0.9.  Cosine decay learning rate policy is used for all models. 

\noindent \textbf{Subnetwork algorithms}
We train Edge-Popup, IteRand, Biprop, and Weight Recycling with similar hyperparameters.  We try to train baseline algorithms with the same hyperparameters as used in the original papers (ass denoted below).  

All models are are trained with hyperparameters as follows: \vspace{-1em}
\begin{enumerate}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
  \item SGD optimizer, learning rate 0.1
  \item 250 Epochs
  \item Batch Size=128
  \item Weight Decay=1e-4 for Conv2-Conv8 models, 5e-4 for ResNet
  \item Cosine decay learning rate policy
  \item Non-Affine BatchNorm
  \item Score parameter $\mathrm{S}$ initialized with Kaiming Uniform
\end{enumerate}
\vspace{-1em}

\begin{table}
\begin{center}
\renewcommand{\arraystretch}{1.15}
\begin{tabular}{|c| c| } 
 \hline
 Algorithm & Weight Init.  \\ [0.5ex] 
 \hline
 Edge-Popup& Signed Constant \\ 
 Edge-Popup+IteRand &  Signed Constant \\

 Biprop & Kaiming Normal\\
 
 Biprop+IteRand &  Kaiming Normal \\
 Edge-Popup+Weight Recycle &Signed Constant \\
 Biprop+Weight Recycle &  Kaiming Normal \\

 \hline
\end{tabular}
\caption{Weight initializations used for each experiment.  }
\end{center}
\end{table}


\noindent \textbf{IteRand with Biprop}
We try several configurations for Biprop + IteRand, applying the same code as the original authors from each paper.  Configurations  
Signed Constant and Unsigned Constant initialization, modified learning rate, modified rerandomization frequency and rate (we chose parameters identical to Weight Recycling, 10 and 0.2, respectively), modified score seed, modified weight seed.  In the result tables below, we use the same hyperparameters as the original paper, using Kaiming Normal initialization for Biprop+IteRand experiments.  

\noindent \textbf{Model Sizes}

\begin{center}
\renewcommand{\arraystretch}{1.15}
\begin{tabular}{c c c c c c c} 
 %\hline
 Prune Rate & Width Factor & Conv2 & Conv4 & Conv6 & Conv8 & ResNet18 \\ [0.5ex] 
 \hline\hline
 0 & 1 & 4,300,992&2,425,024&2,261,184&5,275,840&11,678,912 \\ 
 \hline 
  0 & 0.1 &39,761&22,505&21,630&51,614&- \\ 
 \hline
   0 & 0.25 & 269,616&152,368&142,128&330,032&- \\ 
 \hline
   0 & 0.5 &1,076,320&607,328&566,368&1,319,392&-  \\ 
 \hline
 0.2 & 1 & 3,440,794 & 1,940,019 & 1,808,947 & 4,220,672 & 9,343,129.60\\
  \hline
 0.4 & 1 & 2,580,595 & 1,455,014 & 1,356,710 & 3,165,504 & 7,007,347.20\\
  \hline
 0.5 & 1 & 2,150,496 & 1,212,512 & 1,130,592 & 2,637,920 & 5,839,456.00\\
  \hline
 0.6 & 1 & 1,720,397 & 970,010 & 904,474 & 2,110,336 & 4,671,564.80\\
  \hline
   0.8 & 1 & 860,198 & 485,005 & 452,237 & 1,055,168 & 2,335,782.40\\
  \hline
 0.9 & 1 & 430,099 & 242,502 & 226,118 & 527,584 & 1,167,891.20\\
  \hline
 0.95 & 1 & 215,050 & 121,251 & 113,059 & 263,792 & 583,945.60\\
  \hline
 0.98 & 1 & 86,020 & 48,500 & 45,224 & 105,517 & 233,578.24\\
  \hline
 0.99 & 1 & 43,010 & 24,250 & 22,612 & 52,758 & 116,789.12\\
  \hline
 0.5 & 0.1 & 19,881 &11,253  & 10,815 & 25,807 & -\\
  \hline
 0.5 & 0.25 & 134,808 & 76,184 & 71,064 & 165,016 & -\\
  \hline
 0.5 & 0.5 & 538,160 & 303,664 & 283,184 & 659,696 & -\\
 \hline

 \hline
\end{tabular}
\end{center}