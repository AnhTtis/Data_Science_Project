In this work, we propose a novel algorithm for finding highly accurate subnetworks within randomly initialized models.  Iterative Weight Recycling is successful on both \gls{dnns} (Edge-Popup) as well as  \gls{bnns} (Biprop).  Our results indicate that smaller networks are able to achieve higher accuracy than previously thought.  Practically, this allows us to create accurate and compressed models in limited compute environments.  

In addition, we show evidence of abundant \gls{mpts} by creating variegated subnetworks with nearly identical hyperparameters.  This provides several avenues for further investigation:  1) Deriving the theoretical limits on the total number of \gls{mpts} in a given architecture 2)  Exploring the properties of the unpruned weights to better understand weight optimization, and 3) Exploring weight pruning in different problem domains such as NLP.  

