
@inproceedings{lazarevich_post-training_2021,
	title = {Post-{Training} {Deep} {Neural} {Network} {Pruning} via {Layer}-{Wise} {Calibration}},
	url = {https://openaccess.thecvf.com/content/ICCV2021W/LPCV/html/Lazarevich_Post-Training_Deep_Neural_Network_Pruning_via_Layer-Wise_Calibration_ICCVW_2021_paper.html},
	language = {en},
	urldate = {2022-03-09},
	author = {Lazarevich, Ivan and Kozlov, Alexander and Malinin, Nikita},
	year = {2021},
	pages = {798--805},
	file = {Full Text PDF:/Users/matthewgorbett/Zotero/storage/5YKLJYD5/Lazarevich et al. - 2021 - Post-Training Deep Neural Network Pruning via Laye.pdf:application/pdf;Snapshot:/Users/matthewgorbett/Zotero/storage/PUNXLE4X/Lazarevich_Post-Training_Deep_Neural_Network_Pruning_via_Layer-Wise_Calibration_ICCVW_2021_pape.html:text/html},
}

@article{sehwag_hydra_2020,
	title = {{HYDRA}: {Pruning} {Adversarially} {Robust} {Neural} {Networks}},
	shorttitle = {{HYDRA}},
	url = {http://arxiv.org/abs/2002.10509},
	abstract = {In safety-critical but computationally resource-constrained applications, deep learning faces two key challenges: lack of robustness against adversarial attacks and large neural network size (often millions of parameters). While the research community has extensively explored the use of robust training and network pruning independently to address one of these challenges, only a few recent works have studied them jointly. However, these works inherit a heuristic pruning strategy that was developed for benign training, which performs poorly when integrated with robust training techniques, including adversarial training and verifiable robust training. To overcome this challenge, we propose to make pruning techniques aware of the robust training objective and let the training objective guide the search for which connections to prune. We realize this insight by formulating the pruning objective as an empirical risk minimization problem which is solved efficiently using SGD. We demonstrate that our approach, titled HYDRA, achieves compressed networks with state-of-the-art benign and robust accuracy, simultaneously. We demonstrate the success of our approach across CIFAR-10, SVHN, and ImageNet dataset with four robust training techniques: iterative adversarial training, randomized smoothing, MixTrain, and CROWN-IBP. We also demonstrate the existence of highly robust sub-networks within non-robust networks. Our code and compressed networks are publicly available at {\textbackslash}url\{https://github.com/inspire-group/compactness-robustness\}.},
	urldate = {2022-03-09},
	journal = {arXiv:2002.10509 [cs, stat]},
	author = {Sehwag, Vikash and Wang, Shiqi and Mittal, Prateek and Jana, Suman},
	month = nov,
	year = {2020},
	note = {arXiv: 2002.10509},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: NeurIPS 2020},
	file = {arXiv Fulltext PDF:/Users/matthewgorbett/Zotero/storage/2A9ESGF7/Sehwag et al. - 2020 - HYDRA Pruning Adversarially Robust Neural Network.pdf:application/pdf;arXiv.org Snapshot:/Users/matthewgorbett/Zotero/storage/HU28MPML/2002.html:text/html},
}

@article{diffenderfer_winning_nodate,
	title = {A {Winning} {Hand}: {Compressing} {Deep} {Networks} {Can} {Improve} {Out}-{Of}-{Distribution} {Robustness}},
	language = {en},
	author = {Diffenderfer, James and Bartoldson, Brian R and Chaganti, Shreya and Zhang, Jize and Kailkhura, Bhavya},

	file = {Diffenderfer et al. - A Winning Hand Compressing Deep Networks Can Impr.pdf:/Users/matthewgorbett/Zotero/storage/TV5HCBQH/Diffenderfer et al. - A Winning Hand Compressing Deep Networks Can Impr.pdf:application/pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},

	year = {2021},
}

@article{frankle_lottery_2019,
	title = {The {Lottery} {Ticket} {Hypothesis}: {Finding} {Sparse}, {Trainable} {Neural} {Networks}},
	shorttitle = {The {Lottery} {Ticket} {Hypothesis}},
	url = {http://arxiv.org/abs/1803.03635},
	abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the "lottery ticket hypothesis:" dense, randomly-initialized, feed-forward networks contain subnetworks ("winning tickets") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20\% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
	urldate = {2022-03-09},
	booktitle = {International Conference on Learning Representations (ICLR)},
	author = {Frankle, Jonathan and Carbin, Michael},
	month = mar,
	year = {2019},
	note = {arXiv: 1803.03635},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: ICLR camera ready},

}

@inproceedings{ye_good_2020,
	title = {Good {Subnetworks} {Provably} {Exist}: {Pruning} via {Greedy} {Forward} {Selection}},
	shorttitle = {Good {Subnetworks} {Provably} {Exist}},
	url = {https://proceedings.mlr.press/v119/ye20b.html},
	abstract = {Recent empirical works show that large deep neural networks are often highly redundant and one can find much smaller subnetworks without a significant drop of accuracy. However, most existing methods of network pruning are empirical and heuristic, leaving it open whether good subnetworks provably exist, how to find them efficiently, and if network pruning can be provably better than direct training using gradient descent. We answer these problems positively by proposing a simple greedy selection approach for finding good subnetworks, which starts from an empty network and greedily adds important neurons from the large network. This differs from the existing methods based on backward elimination, which remove redundant neurons from the large network. Theoretically, applying the greedy selection strategy on sufficiently large \{pre-trained\} networks guarantees to find small subnetworks with lower loss than networks directly trained with gradient descent. Our results also apply to pruning randomly weighted networks. Practically, we improve prior arts of network pruning on learning compact neural architectures on ImageNet, including ResNet, MobilenetV2/V3, and ProxylessNet. Our theory and empirical results on MobileNet suggest that we should fine-tune the pruned subnetworks to leverage the information from the large model, instead of re-training from new random initialization as suggested in {\textbackslash}citet\{liu2018rethinking\}.},
	language = {en},
	urldate = {2022-03-09},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning} (ICML)},
	publisher = {PMLR},
	author = {Ye, Mao and Gong, Chengyue and Nie, Lizhen and Zhou, Denny and Klivans, Adam and Liu, Qiang},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {10820--10830},
	file = {Full Text PDF:/Users/matthewgorbett/Zotero/storage/8IKS2U6S/Ye et al. - 2020 - Good Subnetworks Provably Exist Pruning via Greed.pdf:application/pdf;Supplementary PDF:/Users/matthewgorbett/Zotero/storage/ZBEJ9J8I/Ye et al. - 2020 - Good Subnetworks Provably Exist Pruning via Greed.pdf:application/pdf},
}

@incollection{gallicchio_deep_2020,
	address = {Cham},
	series = {Studies in {Computational} {Intelligence}},
	title = {Deep {Randomized} {Neural} {Networks}},
	isbn = {978-3-030-43883-8},
	url = {https://doi.org/10.1007/978-3-030-43883-8_3},
	abstract = {Randomized Neural Networks explore the behavior of neural systems where the majority of connections are fixed, either in a stochastic or a deterministic fashion. Typical examples of such systems consist of multi-layered neural network architectures where the connections to the hidden layer(s) are left untrained after initialization. Limiting the training algorithms to operate on a reduced set of weights inherently characterizes the class of Randomized Neural Networks with a number of intriguing features. Among them, the extreme efficiency of the resulting learning processes is undoubtedly a striking advantage with respect to fully trained architectures. Besides, despite the involved simplifications, randomized neural systems possess remarkable properties both in practice, achieving state-of-the-art results in multiple domains, and theoretically, allowing to analyze intrinsic properties of neural architectures (e.g. before training of the hidden layers’ connections). In recent years, the study of Randomized Neural Networks has been extended towards deep architectures, opening new research directions to the design of effective yet extremely efficient deep learning models in vectorial as well as in more complex data domains. This chapter surveys all the major aspects regarding the design and analysis of Randomized Neural Networks, and some of the key results with respect to their approximation capabilities. In particular, we first introduce the fundamentals of randomized neural models in the context of feed-forward networks (i.e., Random Vector Functional Link and equivalent models) and convolutional filters, before moving to the case of recurrent systems (i.e., Reservoir Computing networks). For both, we focus specifically on recent results in the domain of deep randomized systems, and (for recurrent models) their application to structured domains.},
	language = {en},
	urldate = {2022-03-09},
	booktitle = {Recent {Trends} in {Learning} {From} {Data}: {Tutorials} from the {INNS} {Big} {Data} and {Deep} {Learning} {Conference} ({INNSBDDL2019})},
	publisher = {Springer International Publishing},
	author = {Gallicchio, Claudio and Scardapane, Simone},
	editor = {Oneto, Luca and Navarin, Nicolò and Sperduti, Alessandro and Anguita, Davide},
	year = {2020},
	doi = {10.1007/978-3-030-43883-8_3},
	pages = {43--68},
}

@article{engelbrecht_new_2001,
	title = {A new pruning heuristic based on variance analysis of sensitivity information},
	volume = {12},
	issn = {1941-0093},
	doi = {10.1109/72.963775},
	abstract = {Architecture selection is a very important aspect in the design of neural networks (NNs) to optimally tune performance and computational complexity. Sensitivity analysis has been used successfully to prune irrelevant parameters from feedforward NNs. This paper presents a new pruning algorithm that uses the sensitivity analysis to quantify the relevance of input and hidden units. A new statistical pruning heuristic is proposed, based on the variance analysis, to decide which units to prune. The basic idea is that a parameter with a variance in sensitivity not significantly different from zero, is irrelevant and can be removed. Experimental results show that the new pruning algorithm correctly prunes irrelevant input and hidden units. The new pruning algorithm is also compared with standard pruning algorithms.},
	number = {6},
	journal = {IEEE Transactions on Neural Networks},
	author = {Engelbrecht, A.P.},
	month = nov,
	year = {2001},
	note = {Conference Name: IEEE Transactions on Neural Networks},
	keywords = {Neural networks, Computer architecture, Training data, Analysis of variance, Computational complexity, Computational efficiency, Computer errors, Computer science, Information analysis, Sensitivity analysis},
	pages = {1386--1399},
	file = {IEEE Xplore Abstract Record:/Users/matthewgorbett/Zotero/storage/36JCRBVT/963775.html:text/html},
}

@inproceedings{orseau_logarithmic_2020,
	title = {Logarithmic {Pruning} is {All} {You} {Need}},
	volume = {33},

	abstract = {The Lottery Ticket Hypothesis is a conjecture that every large neural network contains a subnetwork that, when trained in isolation, achieves comparable performance to the large network.
An even stronger conjecture has been proven recently: Every sufficiently overparameterized network contains a subnetwork that, even without training, achieves comparable accuracy to the trained large network.
This theorem, however, relies on a number of strong assumptions and guarantees a polynomial factor on the size of the large network compared to the target function.
In this work, we remove the most limiting assumptions of this previous work while providing significantly tighter bounds: 
the overparameterized network only needs a logarithmic factor (in all variables but depth) number of neurons per weight of the target subnetwork.},
	urldate = {2022-04-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Orseau, Laurent and Hutter, Marcus and Rivasplata, Omar},
	year = {2020},
	pages = {2925--2934},
	file = {Full Text PDF:/Users/matthewgorbett/Zotero/storage/CG8T8Q9Y/Orseau et al. - 2020 - Logarithmic Pruning is All You Need.pdf:application/pdf},
}

@inproceedings{pensia_optimal_2020,
	title = {Optimal {Lottery} {Tickets} via {Subset} {Sum}: {Logarithmic} {Over}-{Parameterization} is {Sufficient}},
	volume = {33},
	shorttitle = {Optimal {Lottery} {Tickets} via {Subset} {Sum}},

	urldate = {2022-04-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Pensia, Ankit and Rajput, Shashank and Nagle, Alliot and Vishwakarma, Harit and Papailiopoulos, Dimitris},
	year = {2020},
	pages = {2599--2610},
	file = {Full Text PDF:/Users/matthewgorbett/Zotero/storage/XGLCWCS2/Pensia et al. - 2020 - Optimal Lottery Tickets via Subset Sum Logarithmi.pdf:application/pdf},
}

@inproceedings{ramanujan_whats_2020,
	title = {What's {Hidden} in a {Randomly} {Weighted} {Neural} {Network}?},
	booktitle={Computer Vision and Pattern Recognition (CVPR)},
	author = {Ramanujan, Vivek and Wortsman, Mitchell and Kembhavi, Aniruddha and Farhadi, Ali and Rastegari, Mohammad},
	year = {2020}
}

@inproceedings{malach_proving_2020,
	title = {Proving the {Lottery} {Ticket} {Hypothesis}: {Pruning} is {All} {You} {Need}},
	shorttitle = {Proving the {Lottery} {Ticket} {Hypothesis}},
	language = {en},
	urldate = {2022-04-16},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Malach, Eran and Yehudai, Gilad and Shalev-Schwartz, Shai and Shamir, Ohad},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {6682--6691},
	file = {Full Text PDF:/Users/matthewgorbett/Zotero/storage/3HA7GDMD/Malach et al. - 2020 - Proving the Lottery Ticket Hypothesis Pruning is .pdf:application/pdf;Supplementary PDF:/Users/matthewgorbett/Zotero/storage/JYKZVJQG/Malach et al. - 2020 - Proving the Lottery Ticket Hypothesis Pruning is .pdf:application/pdf},
}

@inproceedings{morcos_one_2019,
	title = {One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers},
	volume = {32},
	shorttitle = {One ticket to win them all},
	abstract = {The success of lottery ticket initializations (Frankle and Carbin, 2019) suggests that small, sparsified networks can be trained so long as the network is initialized appropriately. Unfortunately, finding these "winning ticket'' initializations is computationally expensive. One potential solution is to reuse the same winning tickets across a variety of datasets and optimizers. However, the generality of winning ticket initializations remains unclear. Here, we attempt to answer this question by generating winning tickets for one training configuration (optimizer and dataset) and evaluating their performance on another configuration. Perhaps surprisingly, we found that, within the natural images domain, winning ticket initializations generalized across a variety of datasets, including Fashion MNIST, SVHN, CIFAR-10/100, ImageNet, and Places365, often achieving performance close to that of winning tickets generated on the same dataset. Moreover, winning tickets generated using larger datasets consistently transferred better than those generated using smaller datasets. We also found that winning ticket initializations generalize across optimizers with high performance. These results suggest that winning ticket initializations generated by sufficiently large datasets contain inductive biases generic to neural networks more broadly which improve training across many settings and provide hope for the development of better initialization methods.},
	urldate = {2022-04-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Morcos, Ari and Yu, Haonan and Paganini, Michela and Tian, Yuandong},
	year = {2019},
	file = {Full Text PDF:/Users/matthewgorbett/Zotero/storage/JDA8FGN4/Morcos et al. - 2019 - One ticket to win them all generalizing lottery t.pdf:application/pdf},
}


@article{liu_rethinking_2019,
	title = {Rethinking the {Value} of {Network} {Pruning}},
	abstract = {Network pruning is widely used for reducing the heavy inference cost of deep models in low-resource settings. A typical pruning algorithm is a three-stage pipeline, i.e., training (a large model), pruning and fine-tuning. During pruning, according to a certain criterion, redundant weights are pruned and important weights are kept to best preserve the accuracy. In this work, we make several surprising observations which contradict common beliefs. For all state-of-the-art structured pruning algorithms we examined, fine-tuning a pruned model only gives comparable or worse performance than training that model with randomly initialized weights. For pruning algorithms which assume a predefined target network architecture, one can get rid of the full pipeline and directly train the target network from scratch. Our observations are consistent for multiple network architectures, datasets, and tasks, which imply that: 1) training a large, over-parameterized model is often not necessary to obtain an efficient final model, 2) learned "important" weights of the large model are typically not useful for the small pruned model, 3) the pruned architecture itself, rather than a set of inherited "important" weights, is more crucial to the efficiency in the final model, which suggests that in some cases pruning can be useful as an architecture search paradigm. Our results suggest the need for more careful baseline evaluations in future research on structured pruning methods. We also compare with the "Lottery Ticket Hypothesis" (Frankle \& Carbin 2019), and find that with optimal learning rate, the "winning ticket" initialization as used in Frankle \& Carbin (2019) does not bring improvement over random initialization.},
	urldate = {2022-04-16},
	booktitle = {International Conference on Learning Representations (ICLR)},
	author = {Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor},
	month = mar,
	year = {2019},
	note = {arXiv: 1810.05270},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ICLR 2019. Significant revisions from the previous version},
	file = {arXiv Fulltext PDF:/Users/matthewgorbett/Zotero/storage/LW57M6TZ/Liu et al. - 2019 - Rethinking the Value of Network Pruning.pdf:application/pdf;arXiv.org Snapshot:/Users/matthewgorbett/Zotero/storage/J929QWJA/1810.html:text/html},
}

@inproceedings{gaier_weight_2019,
	title = {Weight {Agnostic} {Neural} {Networks}},
	abstract = {Not all neural network architectures are created equal, some perform much better than others for certain tasks. But how important are the weight parameters of a neural network compared to its architecture? In this work, we question to what extent neural network architectures alone, without learning any weight parameters, can encode solutions for a given task. We propose a search method for neural network architectures that can already perform a task without any explicit weight training. To evaluate these networks, we populate the connections with a single shared weight parameter sampled from a uniform random distribution, and measure the expected performance. We demonstrate that our method can find minimal neural network architectures that can perform several reinforcement learning tasks without weight training. On a supervised learning domain, we find network architectures that achieve much higher than chance accuracy on MNIST using random weights.},
	urldate = {2022-04-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Gaier, Adam and Ha, David},
	year = {2019},
}

@inproceedings{tanaka_pruning_2020,
	title = {Pruning neural networks without any data by iteratively conserving synaptic flow},
	volume = {33},

	abstract = {Pruning the parameters of deep neural networks has generated intense interest due to potential savings in time, memory and energy both during training and at test time. Recent works have identified, through an expensive sequence of training and pruning cycles, the existence of winning lottery tickets or sparse trainable subnetworks at initialization. This raises a foundational question: can we identify highly sparse trainable subnetworks at initialization, without ever training, or indeed without ever looking at the data? We provide an affirmative answer to this question through theory driven algorithm design. We first mathematically formulate and experimentally verify a conservation law that explains why existing gradient-based pruning algorithms at initialization suffer from layer-collapse, the premature pruning of an entire layer rendering a network untrainable. This theory also elucidates how layer-collapse can be entirely avoided, motivating a novel pruning algorithm Iterative Synaptic Flow Pruning (SynFlow). This algorithm can be interpreted as preserving the total flow of synaptic strengths through the network at initialization subject to a sparsity constraint. Notably, this algorithm makes no reference to the training data and consistently competes with or outperforms existing state-of-the-art pruning algorithms at initialization over a range of models (VGG and ResNet), datasets (CIFAR-10/100 and Tiny ImageNet), and sparsity constraints (up to 99.99 percent). Thus our data-agnostic pruning algorithm challenges the existing paradigm that, at initialization, data must be used to quantify which synapses are important.},
	urldate = {2022-04-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Tanaka, Hidenori and Kunin, Daniel and Yamins, Daniel L and Ganguli, Surya},
	year = {2020},
	pages = {6377--6389},
	file = {Full Text PDF:/Users/matthewgorbett/Zotero/storage/46DR4ZNK/Tanaka et al. - 2020 - Pruning neural networks without any data by iterat.pdf:application/pdf},
}


@article{bengio_estimating_2013,
	title = {Estimating or {Propagating} {Gradients} {Through} {Stochastic} {Neurons} for {Conditional} {Computation}},
	url = {http://arxiv.org/abs/1308.3432},
	urldate = {2022-04-16},
	journal = {arXiv:1308.3432 [cs]},
	author = {Bengio, Yoshua and Léonard, Nicholas and Courville, Aaron},
	month = aug,
	year = {2013},
	note = {arXiv: 1308.3432},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: arXiv admin note: substantial text overlap with arXiv:1305.2982},
	file = {arXiv Fulltext PDF:/Users/matthewgorbett/Zotero/storage/WKJF5743/Bengio et al. - 2013 - Estimating or Propagating Gradients Through Stocha.pdf:application/pdf},
}


@inproceedings{chijiwa_pruning_2021,
	title = {Pruning {Randomly} {Initialized} {Neural} {Networks} with {Iterative} {Randomization}},
	volume = {34},
	urldate = {2022-04-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chijiwa, Daiki and Yamaguchi, Shin' ya and Ida, Yasutoshi and Umakoshi, Kenji and INOUE, Tomohiro},
	year = {2021},
	pages = {4503--4513},
	file = {Full Text PDF:/Users/matthewgorbett/Zotero/storage/ACK6TZ69/Chijiwa et al. - 2021 - Pruning Randomly Initialized Neural Networks with .pdf:application/pdf},
}



@article{diffenderfer_multi-prize_2021,
	title = {Multi-{Prize} {Lottery} {Ticket} {Hypothesis}: {Finding} {Accurate} {Binary} {Neural} {Networks} by {Pruning} {A} {Randomly} {Weighted} {Network}},
	shorttitle = {Multi-{Prize} {Lottery} {Ticket} {Hypothesis}},
	url = {http://arxiv.org/abs/2103.09377},
	urldate = {2022-04-16},
	journal = {arXiv:2103.09377 [cs]},
	author = {Diffenderfer, James and Kailkhura, Bhavya},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.09377},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/matthewgorbett/Zotero/storage/5FXRVJX8/Diffenderfer and Kailkhura - 2021 - Multi-Prize Lottery Ticket Hypothesis Finding Acc.pdf:application/pdf},
}




@article{wang_pruning_2019,
	title = {Pruning from {Scratch}},
	url = {http://arxiv.org/abs/1909.12579},
	abstract = {Network pruning is an important research field aiming at reducing computational costs of neural networks. Conventional approaches follow a fixed paradigm which first trains a large and redundant network, and then determines which units (e.g., channels) are less important and thus can be removed. In this work, we find that pre-training an over-parameterized model is not necessary for obtaining the target pruned structure. In fact, a fully-trained over-parameterized model will reduce the search space for the pruned structure. We empirically show that more diverse pruned structures can be directly pruned from randomly initialized weights, including potential models with better performance. Therefore, we propose a novel network pruning pipeline which allows pruning from scratch. In the experiments for compressing classification models on CIFAR10 and ImageNet datasets, our approach not only greatly reduces the pre-training burden of traditional pruning methods, but also achieves similar or even higher accuracy under the same computation budgets. Our results facilitate the community to rethink the effectiveness of existing techniques used for network pruning.},
	urldate = {2022-04-17},
	journal = {arXiv:1909.12579 [cs]},
	author = {Wang, Yulong and Zhang, Xiaolu and Xie, Lingxi and Zhou, Jun and Su, Hang and Zhang, Bo and Hu, Xiaolin},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.12579},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 12 pages, 9 figures},
	file = {arXiv Fulltext PDF:/Users/matthewgorbett/Zotero/storage/RZBNGYTW/Wang et al. - 2019 - Pruning from Scratch.pdf:application/pdf;arXiv.org Snapshot:/Users/matthewgorbett/Zotero/storage/RLSB3Q28/1909.html:text/html},
}

@inproceedings{wang_picking_2019,
	title = {Picking {Winning} {Tickets} {Before} {Training} by {Preserving} {Gradient} {Flow}},
	url = {https://openreview.net/forum?id=SkgsACVKPH},
	abstract = {We introduced a pruning criterion for pruning networks before training by preserving gradient flow.},
	language = {en},
	urldate = {2022-04-17},
	author = {Wang, Chaoqi and Zhang, Guodong and Grosse, Roger},
	month = sep,
	year = {2019},
	file = {Full Text PDF:/Users/matthewgorbett/Zotero/storage/I7XQNJ3L/Wang et al. - 2019 - Picking Winning Tickets Before Training by Preserv.pdf:application/pdf;Snapshot:/Users/matthewgorbett/Zotero/storage/CEFI9479/forum.html:text/html},
}

@article{you_drawing_2022,
	title = {Drawing {Early}-{Bird} {Tickets}: {Towards} {More} {Efficient} {Training} of {Deep} {Networks}},
	shorttitle = {Drawing {Early}-{Bird} {Tickets}},
	url = {http://arxiv.org/abs/1909.11957},
	abstract = {(Frankle \& Carbin, 2019) shows that there exist winning tickets (small but critical subnetworks) for dense, randomly initialized networks, that can be trained alone to achieve comparable accuracies to the latter in a similar number of iterations. However, the identification of these winning tickets still requires the costly train-prune-retrain process, limiting their practical benefits. In this paper, we discover for the first time that the winning tickets can be identified at the very early training stage, which we term as early-bird (EB) tickets, via low-cost training schemes (e.g., early stopping and low-precision training) at large learning rates. Our finding of EB tickets is consistent with recently reported observations that the key connectivity patterns of neural networks emerge early. Furthermore, we propose a mask distance metric that can be used to identify EB tickets with low computational overhead, without needing to know the true winning tickets that emerge after the full training. Finally, we leverage the existence of EB tickets and the proposed mask distance to develop efficient training methods, which are achieved by first identifying EB tickets via low-cost schemes, and then continuing to train merely the EB tickets towards the target accuracy. Experiments based on various deep networks and datasets validate: 1) the existence of EB tickets, and the effectiveness of mask distance in efficiently identifying them; and 2) that the proposed efficient training via EB tickets can achieve up to 4.7x energy savings while maintaining comparable or even better accuracy, demonstrating a promising and easily adopted method for tackling cost-prohibitive deep network training. Code available at https://github.com/RICE-EIC/Early-Bird-Tickets.},
	urldate = {2022-04-17},
	journal = {arXiv:1909.11957 [cs, stat]},
	author = {You, Haoran and Li, Chaojian and Xu, Pengfei and Fu, Yonggan and Wang, Yue and Chen, Xiaohan and Baraniuk, Richard G. and Wang, Zhangyang and Lin, Yingyan},
	month = feb,
	year = {2022},
	note = {arXiv: 1909.11957},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Accepted as ICLR2020 Spotlight},
	file = {arXiv Fulltext PDF:/Users/matthewgorbett/Zotero/storage/ZR4BQKF2/You et al. - 2022 - Drawing Early-Bird Tickets Towards More Efficient.pdf:application/pdf;arXiv.org Snapshot:/Users/matthewgorbett/Zotero/storage/BQ4LKPDG/1909.html:text/html},
}

@article{evci_rigging_2021,
	title = {Rigging the {Lottery}: {Making} {All} {Tickets} {Winners}},
	shorttitle = {Rigging the {Lottery}},
	url = {http://arxiv.org/abs/1911.11134},
	abstract = {Many applications require sparse neural networks due to space or inference time restrictions. There is a large body of work on training dense networks to yield sparse networks for inference, but this limits the size of the largest trainable sparse model to that of the largest trainable dense model. In this paper we introduce a method to train sparse neural networks with a fixed parameter count and a fixed computational cost throughout training, without sacrificing accuracy relative to existing dense-to-sparse training methods. Our method updates the topology of the sparse network during training by using parameter magnitudes and infrequent gradient calculations. We show that this approach requires fewer floating-point operations (FLOPs) to achieve a given level of accuracy compared to prior techniques. We demonstrate state-of-the-art sparse training results on a variety of networks and datasets, including ResNet-50, MobileNets on Imagenet-2012, and RNNs on WikiText-103. Finally, we provide some insights into why allowing the topology to change during the optimization can overcome local minima encountered when the topology remains static. Code used in our work can be found in github.com/google-research/rigl.},
	urldate = {2022-04-17},
	journal = {International Conference on Machine Learning (ICML)},
	author = {Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich},
	year = {2020},
	note = {arXiv: 1911.11134},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Published in Proceedings of the 37th International Conference on Machine Learning. Code can be found in github.com/google-research/rigl},
	file = {arXiv Fulltext PDF:/Users/matthewgorbett/Zotero/storage/QBMFKLVV/Evci et al. - 2021 - Rigging the Lottery Making All Tickets Winners.pdf:application/pdf;arXiv.org Snapshot:/Users/matthewgorbett/Zotero/storage/78PRUYDJ/1911.html:text/html},
}



@inproceedings{lecun_optimal_1989,
	title = {Optimal {Brain} {Damage}},
	volume = {2},

	abstract = {We  have used  information-theoretic ideas  to derive  a class of prac(cid:173) tical  and  nearly  optimal schemes  for  adapting the size  of a  neural  network.  By  removing  unimportant  weights  from  a  network,  sev(cid:173) eral  improvements  can  be  expected:  better  generalization,  fewer  training examples required,  and improved speed  of learning and/or  classification.  The  basic  idea  is  to  use  second-derivative  informa(cid:173) tion to make a  tradeoff between  network  complexity  and  training  set error.  Experiments confirm  the usefulness  of the methods on a  real-world  application.},
	urldate = {2022-04-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Morgan-Kaufmann},
	author = {LeCun, Yann and Denker, John and Solla, Sara},
	year = {1989},
	file = {Full Text PDF:/Users/matthewgorbett/Zotero/storage/G882SY6K/LeCun et al. - 1989 - Optimal Brain Damage.pdf:application/pdf},
}



@article{friston_hierarchical_2008,
	title = {Hierarchical {Models} in the {Brain}},
	volume = {4},
	issn = {1553-7358},

	doi = {10.1371/journal.pcbi.1000211},
	abstract = {This paper describes a general model that subsumes many parametric models for continuous data. The model comprises hidden layers of state-space or dynamic causal models, arranged so that the output of one provides input to another. The ensuing hierarchy furnishes a model for many types of data, of arbitrary complexity. Special cases range from the general linear model for static data to generalised convolution models, with system noise, for nonlinear time-series analysis. Crucially, all of these models can be inverted using exactly the same scheme, namely, dynamic expectation maximization. This means that a single model and optimisation scheme can be used to invert a wide range of models. We present the model and a brief review of its inversion to disclose the relationships among, apparently, diverse generative models of empirical data. We then show that this inversion can be formulated as a simple neural network and may provide a useful metaphor for inference and learning in the brain.},
	language = {en},
	number = {11},

	journal = {PLOS Computational Biology},
	author = {Friston, Karl},
	month = nov,
	year = {2008},
	note = {Publisher: Public Library of Science},
	keywords = {Covariance, Dynamical systems, Kalman filter, Motion, Neuronal plasticity, Nonlinear dynamics, Optimization, Sensory perception},
	pages = {e1000211},
	file = {Full Text PDF:/Users/matthewgorbett/Zotero/storage/T2UUVP7A/Friston - 2008 - Hierarchical Models in the Brain.pdf:application/pdf;Snapshot:/Users/matthewgorbett/Zotero/storage/28AV3G53/article.html:text/html},
}




@article{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	urldate = {2022-04-17},
	booktitle = {Neural Information Processing Systems (NIPS)},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2014},
	note = {arXiv: 1503.02531},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: NIPS 2014 Deep Learning Workshop},
	file = {arXiv Fulltext PDF:/Users/matthewgorbett/Zotero/storage/DXRP87CW/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf:application/pdf;arXiv.org Snapshot:/Users/matthewgorbett/Zotero/storage/GKR3CQGP/1503.html:text/html},
}

@article{han_learning_2015,
	title = {Learning both {Weights} and {Connections} for {Efficient} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1506.02626},
	abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.},
	urldate = {2022-04-17},
	booktitle = {Neural Information Processing Systems (NIPS)},
	author = {Han, Song and Pool, Jeff and Tran, John and Dally, William J.},
	month = oct,
	year = {2015},
	note = {arXiv: 1506.02626},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Published as a conference paper at NIPS 2015},
	
}

@article{yang_designing_2017,
	title = {Designing {Energy}-{Efficient} {Convolutional} {Neural} {Networks} using {Energy}-{Aware} {Pruning}},
	url = {http://arxiv.org/abs/1611.05128},
	abstract = {Deep convolutional neural networks (CNNs) are indispensable to state-of-the-art computer vision algorithms. However, they are still rarely deployed on battery-powered mobile devices, such as smartphones and wearable gadgets, where vision algorithms can enable many revolutionary real-world applications. The key limiting factor is the high energy consumption of CNN processing due to its high computational complexity. While there are many previous efforts that try to reduce the CNN model size or amount of computation, we find that they do not necessarily result in lower energy consumption, and therefore do not serve as a good metric for energy cost estimation. To close the gap between CNN design and energy consumption optimization, we propose an energy-aware pruning algorithm for CNNs that directly uses energy consumption estimation of a CNN to guide the pruning process. The energy estimation methodology uses parameters extrapolated from actual hardware measurements that target realistic battery-powered system setups. The proposed layer-by-layer pruning algorithm also prunes more aggressively than previously proposed pruning methods by minimizing the error in output feature maps instead of filter weights. For each layer, the weights are first pruned and then locally fine-tuned with a closed-form least-square solution to quickly restore the accuracy. After all layers are pruned, the entire network is further globally fine-tuned using back-propagation. With the proposed pruning method, the energy consumption of AlexNet and GoogLeNet are reduced by 3.7x and 1.6x, respectively, with less than 1\% top-5 accuracy loss. Finally, we show that pruning the AlexNet with a reduced number of target classes can greatly decrease the number of weights but the energy reduction is limited. Energy modeling tool and energy-aware pruned models available at http://eyeriss.mit.edu/energy.html},
	urldate = {2022-04-17},
	booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	author = {Yang, Tien-Ju and Chen, Yu-Hsin and Sze, Vivienne},
	month = apr,
	year = {2017},
	note = {arXiv: 1611.05128},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Published as a conference paper at CVPR 2017},
	file = {arXiv Fulltext PDF:/Users/matthewgorbett/Zotero/storage/AZ5YB4MG/Yang et al. - 2017 - Designing Energy-Efficient Convolutional Neural Ne.pdf:application/pdf;arXiv.org Snapshot:/Users/matthewgorbett/Zotero/storage/Z5FHMKDA/1611.html:text/html},
}




@article{pao_functional-link_1992,
	title = {Functional-link net computing: theory, system architecture, and functionalities},
	volume = {25},
	issn = {1558-0814},
	shorttitle = {Functional-link net computing},
	doi = {10.1109/2.144401},
	abstract = {A system architecture and a network computational approach compatible with the goal of devising a general-purpose artificial neural network computer are described. The functionalities of supervised learning and optimization are illustrated, and cluster analysis and associative recall are briefly mentioned.{\textless}{\textgreater}},
	number = {5},
	journal = {Computer},
	author = {Pao, Y.-H. and Takefuji, Y.},
	month = may,
	year = {1992},
	note = {Conference Name: Computer},
	keywords = {Clustering algorithms, Computer architecture, Computer errors, Computer networks, Equations, Gaussian approximation, Gaussian processes, Sampling methods, Supervised learning},
	pages = {76--79},
	file = {IEEE Xplore Abstract Record:/Users/matthewgorbett/Zotero/storage/S5YACAWX/144401.html:text/html},
}

@article{pao_learning_1994,
	series = {Backpropagation, {Part} {IV}},
	title = {Learning and generalization characteristics of the random vector functional-link net},
	volume = {6},
	issn = {0925-2312},
	abstract = {In this paper we explore and discuss the learning and generalization characteristics of the random vector version of the Functional-link net and compare these with those attainable with the GDR algorithm. This is done for a well-behaved deterministic function and for real-world data. It seems that ‘overtraining’ occurs for stochastic mappings. Otherwise there is saturation of training.},
	language = {en},
	number = {2},
	urldate = {2022-04-17},
	journal = {Neurocomputing},
	author = {Pao, Yoh-Han and Park, Gwang-Hoon and Sobajic, Dejan J.},
	month = apr,
	year = {1994},
	keywords = {auto-enhancement, functional mapping, Functional-link net, generalized delta rule, Neural net, overtraining and generalization},
	pages = {163--180},
	file = {ScienceDirect Snapshot:/Users/matthewgorbett/Zotero/storage/BNZW6Y26/0925231294900531.html:text/html},
}

@article{needell_random_2020,
	title = {Random {Vector} {Functional} {Link} {Networks} for {Function} {Approximation} on {Manifolds}},
	url = {http://arxiv.org/abs/2007.15776},
	abstract = {The learning speed of feed-forward neural networks is notoriously slow and has presented a bottleneck in deep learning applications for several decades. For instance, gradient-based learning algorithms, which are used extensively to train neural networks, tend to work slowly when all of the network parameters must be iteratively tuned. To counter this, both researchers and practitioners have tried introducing randomness to reduce the learning requirement. Based on the original construction of Igelnik and Pao, single layer neural-networks with random input-to-hidden layer weights and biases have seen success in practice, but the necessary theoretical justification is lacking. In this paper, we begin to fill this theoretical gap. We provide a (corrected) rigorous proof that the Igelnik and Pao construction is a universal approximator for continuous functions on compact domains, with approximation error decaying asymptotically like \$O(1/{\textbackslash}sqrt\{n\})\$ for the number \$n\$ of network nodes. We then extend this result to the non-asymptotic setting, proving that one can achieve any desired approximation error with high probability provided \$n\$ is sufficiently large. We further adapt this randomized neural network architecture to approximate functions on smooth, compact submanifolds of Euclidean space, providing theoretical guarantees in both the asymptotic and non-asymptotic forms. Finally, we illustrate our results on manifolds with numerical experiments.},
	urldate = {2022-04-17},
	journal = {arXiv:2007.15776 [cs, math, stat]},
	author = {Needell, Deanna and Nelson, Aaron A. and Saab, Rayan and Salanevich, Palina},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.15776},
	keywords = {62M45, Computer Science - Information Theory, Computer Science - Machine Learning, Mathematics - Probability, Statistics - Machine Learning},
	annote = {Comment: 40 pages, 1 figure},
	file = {arXiv Fulltext PDF:/Users/matthewgorbett/Zotero/storage/6DTVUTRQ/Needell et al. - 2020 - Random Vector Functional Link Networks for Functio.pdf:application/pdf;arXiv.org Snapshot:/Users/matthewgorbett/Zotero/storage/MYTW8SGV/2007.html:text/html},
}

@article{hamid_compact_nodate,
	title = {Compact {Random} {Feature} {Maps}},
	abstract = {Kernel approximation using random feature maps has recently gained a lot of interest. This is mainly due to their applications in reducing training and testing times of kernel based learning algorithms. In this work, we identify that previous approaches for polynomial kernel approximation create maps that can be rank deﬁcient, and therefore may not utilize the capacity of the projected feature space effectively. To address this challenge, we propose compact random feature maps (CRAFTMaps) to approximate polynomial kernels more concisely and accurately. We prove the error bounds of CRAFTMaps demonstrating their superior kernel reconstruction performance compared to the previous approximation schemes. We show how structured random matrices can be used to efﬁciently generate CRAFTMaps, and present a single-pass algorithm using CRAFTMaps to learn non-linear multi-class classiﬁers. We present experiments on multiple standard data-sets with performance competitive with state-of-the-art results.},
	language = {en},
	author = {Hamid, Raffay and Xiao, Ying and Gittens, Alex and DeCoste, Dennis},
	pages = {9},
	file = {Hamid et al. - Compact Random Feature Maps.pdf:/Users/matthewgorbett/Zotero/storage/RX9FLGUF/Hamid et al. - Compact Random Feature Maps.pdf:application/pdf},
}

@inproceedings{rahimi_random_2007,
	title = {Random {Features} for {Large}-{Scale} {Kernel} {Machines}},
	volume = {20},

	abstract = {To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. The features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user speciﬁed shift- invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classiﬁcation and regression tasks linear machine learning al- gorithms applied to these features outperform state-of-the-art large-scale kernel machines.},
	urldate = {2022-04-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Rahimi, Ali and Recht, Benjamin},
	year = {2007},
	file = {Full Text PDF:/Users/matthewgorbett/Zotero/storage/XJP6LQDN/Rahimi and Recht - 2007 - Random Features for Large-Scale Kernel Machines.pdf:application/pdf},
}

@inproceedings{le_fastfood_2013,
	title = {Fastfood - {Computing} {Hilbert} {Space} {Expansions} in loglinear time},
	url = {https://proceedings.mlr.press/v28/le13.html},
	abstract = {Fast nonlinear function classes are crucial for nonparametric estimation, such as in kernel methods. This paper proposes an improvement to random kitchen sinks that offers significantly faster computation in log-linear time without sacrificing accuracy. Furthermore, we show how one may adjust the regularization properties of the kernel simply by changing the spectral distribution of the projection matrix. We provide experimental results which show that even for for moderately small problems we already achieve two orders of magnitude faster computation and three orders of magnitude lower memory footprint.},
	language = {en},
	urldate = {2022-04-17},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Le, Quoc and Sarlos, Tamas and Smola, Alexander},
	month = may,
	year = {2013},
	note = {ISSN: 1938-7228},
	pages = {244--252},
	file = {Full Text PDF:/Users/matthewgorbett/Zotero/storage/FESKP7TQ/Le et al. - 2013 - Fastfood - Computing Hilbert Space Expansions in l.pdf:application/pdf},
}

@article{jaeger_echo_nodate,
	title = {The “echo state” approach to analysing and training recurrent neural networks – with an {Erratum} note},
	language = {en},
	author = {Jaeger, Herbert},
	pages = {47},
	file = {Jaeger - The “echo state” approach to analysing and trainin.pdf:/Users/matthewgorbett/Zotero/storage/E6NUZLCV/Jaeger - The “echo state” approach to analysing and trainin.pdf:application/pdf},
}

@article{lukosevicius_reservoir_2009,
	title = {Reservoir computing approaches to recurrent neural network training},
	volume = {3},
	issn = {1574-0137},
	url = {https://www.sciencedirect.com/science/article/pii/S1574013709000173},
	doi = {10.1016/j.cosrev.2009.03.005},
	abstract = {Echo State Networks and Liquid State Machines introduced a new paradigm in artificial recurrent neural network (RNN) training, where an RNN (the reservoir) is generated randomly and only a readout is trained. The paradigm, becoming known as reservoir computing, greatly facilitated the practical application of RNNs and outperformed classical fully trained RNNs in many tasks. It has lately become a vivid research field with numerous extensions of the basic idea, including reservoir adaptation, thus broadening the initial paradigm to using different methods for training the reservoir and the readout. This review systematically surveys both current ways of generating/adapting the reservoirs and training different types of readouts. It offers a natural conceptual classification of the techniques, which transcends boundaries of the current “brand-names” of reservoir methods, and thus aims to help in unifying the field and providing the reader with a detailed “map” of it.},
	language = {en},
	number = {3},
	urldate = {2022-04-17},
	journal = {Computer Science Review},
	author = {Lukoševičius, Mantas and Jaeger, Herbert},
	month = aug,
	year = {2009},
	pages = {127--149},
	file = {ScienceDirect Snapshot:/Users/matthewgorbett/Zotero/storage/ABBLVSEJ/S1574013709000173.html:text/html},
}

@article{wang_stochastic_2017,
	title = {Stochastic {Configuration} {Networks}: {Fundamentals} and {Algorithms}},
	volume = {47},
	issn = {2168-2275},
	shorttitle = {Stochastic {Configuration} {Networks}},
	doi = {10.1109/TCYB.2017.2734043},
	abstract = {This paper contributes to the development of randomized methods for neural networks. The proposed learner model is generated incrementally by stochastic configuration (SC) algorithms, termed SC networks (SCNs). In contrast to the existing randomized learning algorithms for single layer feed-forward networks, we randomly assign the input weights and biases of the hidden nodes in the light of a supervisory mechanism, and the output weights are analytically evaluated in either a constructive or selective manner. As fundamentals of SCN-based data modeling techniques, we establish some theoretical results on the universal approximation property. Three versions of SC algorithms are presented for data regression and classification problems in this paper. Simulation results concerning both data regression and classification indicate some remarkable merits of our proposed SCNs in terms of less human intervention on the network size setting, the scope adaptation of random parameters, fast learning, and sound generalization.},
	number = {10},
	journal = {IEEE Transactions on Cybernetics},
	author = {Wang, Dianhui and Li, Ming},
	month = oct,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Cybernetics},
	keywords = {Approximation algorithms, Computational modeling, Data analytics, Data models, incremental learning, Machine learning algorithms, Neural networks, randomized algorithms, stochastic configuration networks (SCNs), Stochastic processes, Training, universal approximation property},
	pages = {3466--3479},
	file = {IEEE Xplore Abstract Record:/Users/matthewgorbett/Zotero/storage/J6U7DR5E/8013920.html:text/html;Submitted Version:/Users/matthewgorbett/Zotero/storage/42F9R9TA/Wang and Li - 2017 - Stochastic Configuration Networks Fundamentals an.pdf:application/pdf},
}






@article{neill_overview_2020,
	title = {An {Overview} of {Neural} {Network} {Compression}},
	url = {http://arxiv.org/abs/2006.03669},
	abstract = {Overparameterized networks trained to convergence have shown impressive performance in domains such as computer vision and natural language processing. Pushing state of the art on salient tasks within these domains corresponds to these models becoming larger and more difficult for machine learning practitioners to use given the increasing memory and storage requirements, not to mention the larger carbon footprint. Thus, in recent years there has been a resurgence in model compression techniques, particularly for deep convolutional neural networks and self-attention based networks such as the Transformer. Hence, this paper provides a timely overview of both old and current compression techniques for deep neural networks, including pruning, quantization, tensor decomposition, knowledge distillation and combinations thereof. We assume a basic familiarity with deep learning architectures{\textbackslash}footnote\{For an introduction to deep learning, see {\textasciitilde}{\textbackslash}citet\{goodfellow2016deep\}\}, namely, Recurrent Neural Networks{\textasciitilde}{\textbackslash}citep[(RNNs)][]\{rumelhart1985learning,hochreiter1997long\}, Convolutional Neural Networks{\textasciitilde}{\textbackslash}citep\{fukushima1980neocognitron\}{\textasciitilde}{\textbackslash}footnote\{For an up to date overview see{\textasciitilde}{\textbackslash}citet\{khan2019survey\}\} and Self-Attention based networks{\textasciitilde}{\textbackslash}citep\{vaswani2017attention\}{\textbackslash}footnote\{For a general overview of self-attention networks, see {\textasciitilde}{\textbackslash}citet\{chaudhari2019attentive\}.\},{\textbackslash}footnote\{For more detail and their use in natural language processing, see{\textasciitilde}{\textbackslash}citet\{hu2019introductory\}\}. Most of the papers discussed are proposed in the context of at least one of these DNN architectures.},
	urldate = {2022-04-17},
	journal = {arXiv:2006.03669 [cs, stat]},
	author = {Neill, James O'},
	month = aug,
	year = {2020},
	note = {arXiv: 2006.03669},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 53 pages},
	file = {arXiv Fulltext PDF:/Users/matthewgorbett/Zotero/storage/78S2847B/Neill - 2020 - An Overview of Neural Network Compression.pdf:application/pdf;arXiv.org Snapshot:/Users/matthewgorbett/Zotero/storage/APEEDEDC/2006.html:text/html},
}

@article{gysel_ristretto_2018,
	title = {Ristretto: {A} {Framework} for {Empirical} {Study} of {Resource}-{Efficient} {Inference} in {Convolutional} {Neural} {Networks}},
	volume = {29},
	issn = {2162-2388},
	shorttitle = {Ristretto},
	doi = {10.1109/TNNLS.2018.2808319},
	abstract = {Convolutional neural networks (CNNs) have led to remarkable progress in a number of key pattern recognition tasks, such as visual scene understanding and speech recognition, that potentially enable numerous applications. Consequently, there is a significant need to deploy trained CNNs to resource-constrained embedded systems. Inference using pretrained modern deep CNNs, however, requires significant system resources, including computation, energy, and memory space. To enable efficient implementation of trained CNNs, a viable approach is to approximate the network with an implementation-friendly model with only negligible degradation in classification accuracy. We present Ristretto, a CNN approximation framework that enables empirical investigation of the tradeoff between various number representation and word width choices and the classification accuracy of the model. Specifically, Ristretto analyzes a given CNN with respect to numerical range required to represent weights, activations, and intermediate results of convolutional and fully connected layers, and subsequently, it simulates the impact of reduced word width or lower precision arithmetic operators on the model accuracy. Moreover, Ristretto can fine-tune a quantized network to further improve its classification accuracy under a given number representation and word width configuration. Given a maximum classification accuracy degradation tolerance of 1\%, we use Ristretto to demonstrate that three ImageNet networks can be condensed to use 8-bit dynamic fixed point for network weights and activations. Ristretto is available as a popular open-source software project1 and has already been viewed over 1000 times on Github as of the submission of this brief.},
	number = {11},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Gysel, Philipp and Pimentel, Jon and Motamedi, Mohammad and Ghiasi, Soheil},
	month = nov,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Arithmetic precision, convolutional neural network (CNN), Dynamic range, efficient inference, Embedded systems, Energy dissipation, Learning systems, Neural networks, number representation, Quantization (signal), Training},
	pages = {5784--5789},
	file = {IEEE Xplore Abstract Record:/Users/matthewgorbett/Zotero/storage/D8J8MGJX/8318896.html:text/html},
}

@article{gupta,
	title = {Deep {Learning} with {Limited} {Numerical} {Precision}},
	abstract = {Training of large-scale deep neural networks is often constrained by the available computational resources. We study the effect of limited precision data representation and computation on neural network training. Within the context of lowprecision ﬁxed-point computations, we observe the rounding scheme to play a crucial role in determining the network’s behavior during training. Our results show that deep networks can be trained using only 16-bit wide ﬁxed-point number representation when using stochastic rounding, and incur little to no degradation in the classiﬁcation accuracy. We also demonstrate an energy-efﬁcient hardware accelerator that implements low-precision ﬁxed-point arithmetic with stochastic rounding.},
	language = {en},
	booktitle={ International Conference on Machine
Learning (ICML)},
	author = {Gupta, Suyog and Agrawal, Ankur and Gopalakrishnan, Kailash and Narayanan, Pritish},
	pages = {10},
	year={2015},
	file = {Gupta et al. - Deep Learning with Limited Numerical Precision.pdf:/Users/matthewgorbett/Zotero/storage/IQE4LSMH/Gupta et al. - Deep Learning with Limited Numerical Precision.pdf:application/pdf},
}

@article{dettmers_8-bit_2016,
	title = {8-{Bit} {Approximations} for {Parallelism} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/1511.04561},
	abstract = {The creation of practical deep learning data-products often requires parallelization across processors and computers to make deep learning feasible on large data sets, but bottlenecks in communication bandwidth make it difficult to attain good speedups through parallelism. Here we develop and test 8-bit approximation algorithms which make better use of the available bandwidth by compressing 32-bit gradients and nonlinear activations to 8-bit approximations. We show that these approximations do not decrease predictive performance on MNIST, CIFAR10, and ImageNet for both model and data parallelism and provide a data transfer speedup of 2x relative to 32-bit parallelism. We build a predictive model for speedups based on our experimental data, verify its validity on known speedup data, and show that we can obtain a speedup of 50x and more on a system of 96 GPUs compared to a speedup of 23x for 32-bit. We compare our data types with other methods and show that 8-bit approximations achieve state-of-the-art speedups for model parallelism. Thus 8-bit approximation is an efficient method to parallelize convolutional networks on very large systems of GPUs.},
	urldate = {2022-04-17},
	journal = {arXiv:1511.04561 [cs]},
	author = {Dettmers, Tim},
	month = feb,
	year = {2016},
	note = {arXiv: 1511.04561},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/matthewgorbett/Zotero/storage/M6CFQLDW/Dettmers - 2016 - 8-Bit Approximations for Parallelism in Deep Learn.pdf:application/pdf;arXiv.org Snapshot:/Users/matthewgorbett/Zotero/storage/3MI947N2/1511.html:text/html},
}

@article{martinez_training_2020,
	title = {Training {Binary} {Neural} {Networks} with {Real}-to-{Binary} {Convolutions}},
	url = {http://arxiv.org/abs/2003.11535},
	abstract = {This paper shows how to train binary networks to within a few percent points (\${\textbackslash}sim 3-5 {\textbackslash}\%\$) of the full precision counterpart. We first show how to build a strong baseline, which already achieves state-of-the-art accuracy, by combining recently proposed advances and carefully adjusting the optimization procedure. Secondly, we show that by attempting to minimize the discrepancy between the output of the binary and the corresponding real-valued convolution, additional significant accuracy gains can be obtained. We materialize this idea in two complementary ways: (1) with a loss function, during training, by matching the spatial attention maps computed at the output of the binary and real-valued convolutions, and (2) in a data-driven manner, by using the real-valued activations, available during inference prior to the binarization process, for re-scaling the activations right after the binary convolution. Finally, we show that, when putting all of our improvements together, the proposed model beats the current state of the art by more than 5\% top-1 accuracy on ImageNet and reduces the gap to its real-valued counterpart to less than 3\% and 5\% top-1 accuracy on CIFAR-100 and ImageNet respectively when using a ResNet-18 architecture. Code available at https://github.com/brais-martinez/real2binary.},
	urldate = {2022-04-17},
	booktitle = {International Conference on Learning Representations (ICLR)},
	author = {Martinez, Brais and Yang, Jing and Bulat, Adrian and Tzimiropoulos, Georgios},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.11535},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ICLR 2020},
	file = {arXiv Fulltext PDF:/Users/matthewgorbett/Zotero/storage/WNRT3EHW/Martinez et al. - 2020 - Training Binary Neural Networks with Real-to-Binar.pdf:application/pdf;arXiv.org Snapshot:/Users/matthewgorbett/Zotero/storage/XQKCPLWC/2003.html:text/html},
}


@article{jaccard_distribution_1912,
	title = {The {Distribution} of the {Flora} in the {Alpine} {Zone}.1},
	volume = {11},
	issn = {1469-8137},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-8137.1912.tb05611.x},
	doi = {10.1111/j.1469-8137.1912.tb05611.x},
	language = {en},
	number = {2},
	urldate = {2022-04-24},
	journal = {New Phytologist},
	author = {Jaccard, Paul},
	year = {1912},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-8137.1912.tb05611.x},
	pages = {37--50},
	file = {Full Text PDF:/Users/matthewgorbett/Zotero/storage/335LR55F/Jaccard - 1912 - The Distribution of the Flora in the Alpine Zone.1.pdf:application/pdf;Snapshot:/Users/matthewgorbett/Zotero/storage/GSK7UGB2/j.1469-8137.1912.tb05611.html:text/html},
}
@article{tanimoto,
	title = {An elementary mathematical theory of classification and prediction},

	booktitle = {International Business Machines Corporation},
	author = {Tanimoto, T. T},
	year = {1956},

	pages = {37--50},

}


@article{rand_objective_1971,
	title = {Objective {Criteria} for the {Evaluation} of {Clustering} {Methods}},
	volume = {66},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1971.10482356},
	doi = {10.1080/01621459.1971.10482356},
	abstract = {Many intuitively appealing methods have been suggested for clustering data, however, interpretation of their results has been hindered by the lack of objective criteria. This article proposes several criteria which isolate specific aspects of the performance of a method, such as its retrieval of inherent structure, its sensitivity to resampling and the stability of its results in the light of new data. These criteria depend on a measure of similarity between two different clusterings of the same set of data; the measure essentially considers how each pair of data points is assigned in each clustering.},
	number = {336},
	urldate = {2022-04-24},
	journal = {Journal of the American Statistical Association},
	author = {Rand, William M.},
	month = dec,
	year = {1971},
	pages = {846--850},
	file = {Snapshot:/Users/matthewgorbett/Zotero/storage/UE6T8CEN/01621459.1971.html:text/html;Submitted Version:/Users/matthewgorbett/Zotero/storage/W7827K2D/Rand - 1971 - Objective Criteria for the Evaluation of Clusterin.pdf:application/pdf},
}


@article{ahmad_how_2016,
	title = {How do neurons operate on sparse distributed representations? {A} mathematical theory of sparsity, neurons and active dendrites},
	shorttitle = {How do neurons operate on sparse distributed representations?},
	url = {https://arxiv.org/abs/1601.00720v2},
	doi = {10.48550/arXiv.1601.00720},
	abstract = {We propose a formal mathematical model for sparse representations and active dendrites in neocortex. Our model is inspired by recent experimental findings on active dendritic processing and NMDA spikes in pyramidal neurons. These experimental and modeling studies suggest that the basic unit of pattern memory in the neocortex is instantiated by small clusters of synapses operated on by localized non-linear dendritic processes. We derive a number of scaling laws that characterize the accuracy of such dendrites in detecting activation patterns in a neuronal population under adverse conditions. We introduce the union property which shows that synapses for multiple patterns can be randomly mixed together within a segment and still lead to highly accurate recognition. We describe simulation results that provide further insight into sparse representations as well as two primary results. First we show that pattern recognition by a neuron with active dendrites can be extremely accurate and robust with high dimensional sparse inputs even when using a tiny number of synapses to recognize large patterns. Second, equations representing recognition accuracy of a dendrite predict optimal NMDA spiking thresholds under a generous set of assumptions. The prediction tightly matches NMDA spiking thresholds measured in the literature. Our model matches many of the known properties of pyramidal neurons. As such the theory provides a mathematical framework for understanding the benefits and limits of sparse representations in cortical networks.},
	language = {en},
	urldate = {2022-05-07},
	author = {Ahmad, Subutai and Hawkins, Jeff},
	month = jan,
	year = {2016},
	file = {Snapshot:/Users/matthewgorbett/Zotero/storage/HKWIVXH4/1601.html:text/html},
}

@techreport{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	number = {arXiv:1512.03385},
	urldate = {2022-05-19},
	institution = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv:1512.03385 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech report},
	file = {arXiv Fulltext PDF:/Users/matthewgorbett/Zotero/storage/SLBE3GEF/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/Users/matthewgorbett/Zotero/storage/PF5M9U8J/1512.html:text/html},
}

@techreport{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	number = {arXiv:1409.1556},
	urldate = {2022-05-19},
	institution = {arXiv},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	doi = {10.48550/arXiv.1409.1556},
	note = {arXiv:1409.1556 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/matthewgorbett/Zotero/storage/SRNJ54MP/Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf;arXiv.org Snapshot:/Users/matthewgorbett/Zotero/storage/HPSV4QVZ/1409.html:text/html},
}
