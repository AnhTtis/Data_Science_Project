The Lottery Ticket Hypothesis  \cite{frankle_lottery_2019} demonstrated that randomly initialized \gls{dnns} contain sparse subnetworks that, when trained in isolation, achieve comparable accuracy to a fully-trained dense network of the same structure.   The results of the hypothesis indicate that over-parameterized \gls{dnns} are no longer necessary; instead, finding "winning ticket" sparse subnetworks can yield high accuracy models.  The consequences of winning tickets are abundant in practical use: we can train \gls{dnns} with a decreased computational cost \cite{morcos_one_2019} including memory consumption and inference time, and  additionally enable wide-spread democratization of \gls{dnns} with a low carbon footprint.  %Further, these compresse

\begin{figure*}
   \includegraphics[width=1.0\textwidth,height=5.5cm]{figures/depth_all.pdf}
    \caption{ \textbf{Performance of Iterative Weight Recycling at varying network depth}: We compare the accuracy of our algorithm against Edge-Popup, Biprop, and a densely trained baseline.  We apply our weight recycling algorithm to Biprop (red) and Edge-Popup (green). We use varying prune rates (x-axis) and varying network depths (Conv-2 to Conv-8) on VGG-like architectures.  We train and evaluate all algorithms on the CIFAR-10 dataset using the same hyperparameter's and training procedure.  }\label{depth_bin}
    \centering
\end{figure*}


Expanding on the Lottery Ticket Hypothesis, Ramanujan et al. \cite{ramanujan_whats_2020} reported a remarkable finding: we do not have to train neural networks at all to find winning tickets.  
Their algorithm, Edge-Popup, uncovered sparse subnetworks 
within \textit{randomly initialized} \gls{dnns} that achieved comparable accuracy to fully trained models. This phenomena was mathematically proven in the Strong Lottery Ticket Hypothesis \cite{malach_proving_2020}.
%Instead, within a randomly initialized DNN there exists a sparse subnetwork that achieves comparable accuracy to a fully trained model.  %With sufficient over-parameterization, the authors argue, the combinatorial search space of a model is large enough that there should exist a high-performing subnetwork.   
%Their algorithm, Edge-Popup, solves a discrete-optimization problem over a neural network to produce  high-accuracy masks.  
%This phenomenon was also mathematically described in the Strong Lottery Ticket Hypothesis \cite{malach_proving_2020}.%, who defined a lower bound on the width required for a random subnetwork to be found that approximates a target network.
Practically, this finding showed that gradient-based weight optimization is not necessary for a neural network to achieve high accuracy.  Moreover, it allows us to overcome difficulties of gradient-based sparsification, such as getting stuck at local minima and incompatible backpropagation \cite{diffenderfer_multi-prize_2021}.  Finally, randomly initialized "winning ticket" subnetworks have been shown to be more robust than other pruning methods \cite{diffenderfer_winning_nodate}. %Moreover, this 

%The implications of this finding


Despite this fascinating discovery, it also marked a key limitation to existing work: randomly initialized \gls{dnns} require a large number of parameters in order to achieve high-accuracy.  In other words, to reach the same level of performance as dense networks trained with weight-optimization, randomly initialized models need more parameters, and hence more memory space.  
Subsequent works have relaxed the bounds proposed by the Strong Lottery Ticket Hypothesis \cite{pensia_optimal_2020,orseau_logarithmic_2020}, showing mathematically that network width needs to be only logarithmically wider than dense networks.
Chijiwa et. al \cite{chijiwa_pruning_2021} proposed an algorithmic modification to Edge-Popup, iterative randomization (IteRand), showing that we can reduce the required network width for weight pruning to the same as a fully trained model up to constant factors. % to tighten the bounds on the required overparameterization of a randomly initialized model.  %They theoretically showed that re-randomizing operations are able to discover improved subnetworks, reducing the required network width up to constant factors.  

In addition to these findings, the Multi-Prize Lottery Ticket Hypothesis \cite{diffenderfer_multi-prize_2021} showed there are \textit{several} subnetworks (\gls{mpts}) in randomly initialized models that achieve high-accuracy compared to dense networks. Importantly, the authors translates this finding into \gls{bnns}, where they propose a new algorithm (Biprop) to identify winning tickets in randomly initialized \gls{bnns}.    The implications of this finding allow for extreme compression of large, over-parameterized models. %They achieve outstanding performance with binary weights, and show promising results with binary weights \textit{and} activation's.  Finally, they justify their claims with a theorem indicating the possibly of sparse \gls{bnns} being able to approach the accuracy of fully-trained \gls{dnns}.  

In this work, we propose an algorithm to find accurate sparse subnetworks in randomly initialized \gls{dnns} and \gls{bnns}.  Our approach exploits existing weights in a network layer, identifying subsets of trivial weights and replacing them with weights influential to a strong subnetwork.  We demonstrate our results in Figure \ref{depth_bin}, showing improvements on a variety of architectures and prune rates. Additionally, we provide confirmation for the Multi-Prize Lottery Ticket Hypothesis, showing evidence that subnetworks generated under tight hyperparameter control exhibit fundamentally dissimilar structure.  


Our contributions are as follows:
\vspace{-14pt}
\begin{itemize}
    \item We propose a new algorithm, Iterative Weight Recycling, which improves the ability to find highly accurate sparse subnetworks within randomly initialized neural networks.  The algorithm is an improvement to both Edge-Popup (for \gls{dnns}) as well as Biprop (\gls{bnns}).  The algorithm identifies $k$ extraneous weights in a model layer and replaces them with $k$ relevant weight values. %Empirically, the algorithm shows improvements in prune rates above 90\%.  %Additionally, the algorithm performs comparatively, or slight better than IteRand, the current state-of-the-art approach to finding sparse subnetworks in \gls{dnns}.  In contrast to IteRand, the algorithm requires no extra storage or overhead from randomizing operations.  Additionally, it requires fewer recycling operations during training. 
    
   % \item We confirm the Multi-Prize Lottery Ticket Hypothesis with tighter overparameterization requirements, showing empirically that sparse subnetworks in limited parameter models achieve high accuracy compared to their dense counterpart.  We show that \textit{recycling} weights in a randomly initialized network enables us to achieve this.  
    \vspace{-8pt}
    \item We examine \gls{mpts} generated under strict hyperparameter control, showing that, under almost identical conditions, \gls{mpts} display diverse mask structures.  These results indicate that, not only do there exist multiple lottery tickets within randomly initialized neural networks, but rather an \textit{abundance} of lottery tickets. 
\end{itemize}