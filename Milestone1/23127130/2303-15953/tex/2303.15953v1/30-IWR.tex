In this section we detail Iterative Weight Recycling, first summarizing the methodology behind the approach, and subsequently detailing the experimental setup and results.  Finally we perform empirical analysis on the algorithm, with results showing that Iterative Weight Recycling emphasizes keeping high norm weights values similar to the traditional L1 pruning technique.  
\subsection{Method}
\vspace{-.5em}
We consider $f(x;\theta)$ as an $l$-layered neural network with ReLU activations, dataset $x\in\mathbb{R}^{n}$ with weight parameters $\theta\sim\mathcal{D}_{weight}$.  We freeze $\theta$ and additionally turn off the bias term for each $l$.  Our model is initialized similarly to Edge-Popup and Biprop: a score pararmeter $\mathrm{S}$ for each $\theta$, where $\mathrm{S}_i$ learns the importance of $\theta_i$.  Additionally, we set a pruning rate $p \in [0,1]$. $\mathcal{D}_{weight}$ is initialized using Kaiming Normal initialization (without scale fan) for Biprop and Signed Constant Initialization for Edge-Popup.  Further, $\mathcal{D}_{score}$ is initialized with Kaiming Uniform with seed 0, except in Section \ref{variability}, where we explore different $\mathrm{S}$ initializations.  




%need notation for LENGTH of score vector s.  using #s for now
%https://math.stackexchange.com/questions/562620/why-are-set-cardinality-and-absolute-value-denoted-the-same-way
Weight recycling works on an iterative basis, similar to IteRand \cite{chijiwa_pruning_2021}.  We define two hyperparameters, $K_{per}$ and $r$, where $K_{per}$ is the frequency we change weights, and $r$ is the \textit{recycling rate}.  During the recycling phase, we compute $k$ as the number of weights we want to change in a given layer as $j*r$, where $j$ is the size of $\mathrm{S}$ at layer $l$ and $r \in [0,1]$.  We retrieve subsets $\mathrm{S}^{low}_l,\mathrm{S}^{high}_l \subset\mathrm{S}_l$ containing the lowest absolute $k$ scores and highest absolute  $k$ scores at each layer: 




\vspace*{-\abovedisplayskip}
\begin{equation}
   \mathrm{S}^{low}_l=\{\tau(i)^{k}_{i=1}\} 
    \text{, } \quad 
  \mathrm{S}^{high}_l=\{\tau(i)^{j}_{i=j-k}\}
  %\text{,} \quad
  %\tau(i) \text{ sorts } \mathrm{S} \text{ s.t. }  |\mathrm{S}^j_{\tau(i)}| \le |\mathrm{S}^j_{\tau(i+1)}|
\end{equation}
\vspace*{-\abovedisplayskip}


where $\tau$ sorts $\{i\}^j_{i=1}\in\mathrm{S}$ such that $|S_{\tau(i)}|\le|S_{\tau(i+1)}|$.  Here, $\{i\}^j_{i=1}$ equates to the index values associated with set $\{|S_{\tau}|\}^j_{i=1}$.  Next, we retrieve weight values associated with $\mathrm{S}^{high}_l$ and $\mathrm{S}^{low}_l$, with $\{i,...,k\}_{S}=\{i,...,k\}_{\theta}$ .   Finally, we set $\theta^{low}_l$=$\theta^{high}_l$.  Effectively, the Iterative Weight Recycling algorithm finds $\mathrm{S}$ values and their associated index (where $i_{\mathrm{S}}=i_{\theta}$) and retrieves the weight value associated to the index, for both high and low $\mathrm{S}$ scores.  The algorithm replaces low $\mathrm{S}$ weight values with high  $\mathrm{S}$ weight values, discarding of low  $\mathrm{S}$  weight values.  Algorithm \ref{alg2} denotes the equation in pseudo-code form.  

%taking the weight values associated with high scores high scoring indices, unimportant scores with the values of learned important scores.  


%index values $\mathrm{S}_{\mathrm{S}^{top_k}}$ and $\mathrm{S}_{\mathrm{S}^{bottom_k}}$, and set weight subset $\theta{\mathrm{S}_{\mathrm{S}^{bottom_k}}}]$ =$\theta[{\mathrm{S}_{\mathrm{S}^{top_k}}}]$, effectively replacing the weights of lower, unimportant scores with the values of learned important scores.  


 \vspace{.3em}

\begin{algorithm*}
\caption{Weight Recycling.  Replace line 14 in Algorithm \ref{alg:cap} with the following method}\label{alg2}
\begin{algorithmic}[1]
        \Function{WeightRecycle}{\textrm{S}, $\theta$}
        \ForEach {$l \in L $}  \Comment{\texttt{layer of size j}}
        \State $k \gets j*r$ \Comment{\texttt{Calculate number of weights to change}}
        \State $\mathrm{S_l}^{high}\gets \textit{highest k} \mkern9mu |\mathrm{S}_l|$ \Comment{\texttt{Retrieve \textbf{indices} of top k abs(score) values}}
               \State $\mathrm{S_l}^{low}\gets \textit{lowest k} \mkern9mu |\mathrm{S}_l|$ \Comment{\texttt{Retrieve indices of bottom k abs(score) values}}
        \State $\theta_l[\mathrm{S_l}^{low}] \gets \theta_l[\mathrm{S_l}^{high}] $\Comment{\texttt{Replace low $\theta_l$ with high $\theta_l$}}
        \EndFor
        \State \Return $\mathrm{S}, \mathrm{M}$
        \EndFunction
\end{algorithmic}
\end{algorithm*}

\begin{figure*}[h]
   \includegraphics[width=1.0\textwidth,height=5.5cm]{figures/width_biprop.pdf}
    \caption{\textbf{Effects of Varying Width at 50\% Prune Rate} Results of baseline models (Dense, Biprop, Edge-Popup) and Iterative Weight Recycling models at varying network widths up to one.  Using Iterative Weight Recycling yields winning tickets with a comparable accuracy to densely trained models at just 50\% width factor in all architectures.  Error bars are denoted for Iterative Weight Recycling algorithms. }\label{width_bin}
    \centering
\end{figure*}

\subsection{Experimental Setup}
To begin, we use model architectures and datasets similar to the three previous works. Conv-2 to Conv-8 are VGG-like CNNs \cite{simonyan_very_2015} with depth $d$= 2 to 8 .  We additionally use their "wide" analogues, which introduces a scale parameter at each layer to influence the specific width of each layer width $w$=0.1 to 1.   Additionally we use ResNets \cite{he_deep_2015}, which utilize skip connections and batch normalization.  We test the models on both CIFAR-10 and ImageNet datasets.  Non-affine transformation is used for all CIFAR-10 experiments, and ResNets use a learned batch normalization similar to \cite{diffenderfer_multi-prize_2021}.  
We apply similar pruning rates to previous works $\{0.2,0.4,0.5,0.6,0.8,0.9\}$, and additionally test our method at prune rates above 0.9.  In Iterative Weight Recycling experiments, we use three different initializations, and report the average accuracy, with error bars denoting the lowest and highest accuracy.  

We compare the performance of Iterative Weight Recycling to Edge-Popup, Biprop, and IteRand algorithms using the same hyperparameters.   For each baseline algorithm, we use the hyperparameters that yielded the best results in the original papers:  Signed Constant initialization for Edge-Popup/IteRand, and Kaiming Normal with scale fan for Biprop.  For our algorithm, we use these same initialization strategies, except for Biprop with Weight Recycling we did not use scale fan as this yielded slightly better results.  Additionally, for IteRand we use the same $K_{per}$ and $r$ as the paper: $K_{per}=1$ (once per epoch), with $r=0.1$.  For our algorithm, we choose $K_{per}=10$  and $r=0.2$ for all models.  We found that less frequent recycling yielded better results, hypothesizing that recycling too frequently yielded redundant values.  %We include these results in the Appendix.  %We note that we attempted to use Biprop with the IteRand algorithm, but failed to achieve strong results using multiple configurations.  Thus, we exclude these results from the main paper and include the configurations and results in the Appendix.  

%\vspace{-.1em}



\subsection{Results}
%\vspace{-.8em}
In this section we test the effects of network overparameterization and prune rate on subnetwork performance, with the goal of empirically verifying the Iterative Weight Recycling compared to Edge Popup \cite{ramanujan_whats_2020}, Biprop \cite{diffenderfer_multi-prize_2021}, and IteRand \cite{chijiwa_pruning_2021} algorithms. 
We follow previous works and test neural networks with varying depth and width, and additionally test each algorithm at high prune rates.  
%Our secondary goal is to assess the overparameterization necessary to achieve results of a dense model with trained weights.  

%We follow previous works and test neural networks with varying depth and width, and additionally test algorithms .  %Further, we empirically test the hypothesis that networks need to be sufficiently overparameterized to reach the performance of a network with learned weights.  


\begin{figure*}
   \includegraphics[width=1.0\textwidth]{figures/params_resnet.pdf}
    \caption{ \textbf{Pruning Algorithm Performance with Limited Parameters} We test the effect of high prune rates on model performance, showing that Weight Recycling achieves high accuracy compared to IteRand, Edge-Popup, and Biprop.  \textbf{Left:} Accuracies of various VGG architectures with prune rates greater than 80\% and parameter count less than 500k.  We include architecture, prune rate, and original parameters of the model in four of the datapoints.  \textbf{Right:} ResNet18 with prune rates greater than 95\%.  With just under 600k parameters (95\% prune rate), ResNet18 with Iterative Weight Recycling achieves higher accuracy than a dense baseline model (93.1\%).   }\label{params}
    \centering
\end{figure*}

\noindent \textbf{Varying Depth}
In Figure \ref{depth_bin}, we vary the depth of VGG architectures from 2 to 8 and compare the test accuracy at various prune rates.  We observe a clear advantage to using Iterative Weight Recycling:  at every prune rate and model architecture,  Iterative Weight Recycling outperforms both Biprop and Edge-Popup.  Additionally, Biprop with Weight Recycling generally outperforms Edge-Popup with Weight Recycling at higher prune rates.  Iterative Weight Recycling outperforms dense models in each architecture except when 90\% of the weights have been pruned. Notably, we discover that Iterative Weight Recycling is able to achieve accuracy exceeding the dense model in Conv-2 architectures.   This is the first such observation on a low-depth model -- Edge-Popup and Biprop research reported test accuracy \textit{near} the dense model, however never clearly exceeded it.  Further, Biprop+Iterative Weight Recycling is able to achieve an 80.23\% test accuracy with just 20\% of the weights. 



\noindent \textbf{Varying Width}
We also consider network width as a factor to control network parameterization. Previous showed that as we increase width, our chances of finding winning tickets increased.  Edge-Popup found winning tickets at width factors greater than 1, while Biprop reported winning tickets around width factor 1.  

In Figure \ref{width_bin}, we demonstrate the efficacy of Iterative Weight Recycling on networks with width factors less than one.  Results show that in each architecture, we can find winning tickets at just 50\% width.  In practical terms, in a Conv-4 architecture this equates to just 25\% of the parameters compared to a Conv-4 with width factor 1 (600k vs. 2.4m).  Additionally, our Conv-4 architecture with width factor 0.5 achieved an accuracy of 86.5\% compared to 86.66\% for a dense Conv-4 with width factor 1.  






\noindent \textbf{Varying Prune Rate}\label{vary_prune}
In Figure \ref{params}, we demonstrate the results of Biprop, Edge-Popup, IteRand, and Iterative Weight Recycling (Biprop) on \gls{dnns} with prune rates above 80\%.  Iterative Weight Recycling shows favorable results with limited parameter counts.  Notably, the algorithm consistently outperformed IteRand at aggressive prune rates between 80\% and 99\%.  At more modest prune rates (20\%-60\%), Weight Recycling was comparable to IteRand, which we summarize in Section \ref{analysis}.  

In the ResNet18 architecture (11 million parameters), our algorithm was able to find winning tickets with just 5\% of the random weights.  These results are further evidence that overparameterization helps in the identification of high performing subnetworks.  


%results comparable to, or slightly better than, IteRand, and both algorithms show a distinct advantage over Edge-Popup.  While these results may not show a conclusive advantage to using Weight Recycling over IteRand, we note that our method requires no extra costs from rerandomizing weights.  Further, our method recycles weights at $\frac{1}{10}$ the rate of IteRand ($K_{per}=1$ for IteRand and $K_{per}=10$ for Iterative Weight Recycling), allowing for faster training times due to less weight manipulation.  We further discuss the limitations of IteRand in Section \ref{background}



\begin{figure}
\centering
\setlength{\tabcolsep}{2pt}
  \begin{tabular}{lccc}
    \toprule
    Algorithm & Prune \% & \# Params& Acc.(\%) \\
    \midrule
    Edge-Popup&70\% & 7.6M&67.13\\
    Edge-Popup+IteRand&70\% & 7.6M&69.11\\
    \textbf{Edge-Popup+IWR}&70\% & 7.6M&69.02\\
    \textbf{Edge-Popup+IWR}&80\% & 5.1M&68.87\\
    \hline
        Biprop&70\% & 7.6M&67.76\\
            Biprop+IteRand&70\% & 7.6M&43.76\\
       \textbf{Biprop+IWR}&70\% & 7.6M&69.85\\
        \textbf{Biprop+IWR}&80\% & 5.1M&68.65\\
    \bottomrule
  \end{tabular}
  \makeatletter\def\@captype{table}\makeatother
  \caption{\textbf{ImageNet results on ResNet50:} We test various pruning algorithms on the ImageNet dataset with the ResNet50 architecture.  Results show that Iterative Weight Recycling (bold) achieves similar results to Edge-Popup, Biprop, and IteRand with 2.5 million less parameters.  }\label{imagenet}
%\end{wrapfigure}
\end{figure}


\noindent \textbf{ImageNet Results}
In Table \ref{imagenet}, we highlight the results of our algorithm on the ImageNet dataset.  We choose a ResNet50 architecture which contains 25.5 million total parameters.  We train each baseline algorithm with a 70\% prune rate, similar to previous papers.  Results for IteRand and Edge-Popup were within 0.1\% of the original papers results.   %For Iterative Weight Recycling, we used a 80\% prune rate in order to assess the effects of a more aggressive prune rate.  

Results show that our algorithm performs well under more aggressive pruning rates compared to Edge-Popup, IteRand, and Biprop, similar to what we found in Section \ref{vary_prune}.  Specifically, our algorithm performed similar to, or better than, previous algorithms with 2.5 million less parameters.  




%\begin{figure}
   %\includegraphics[width=1.0\textwidth]{figures/width_epu2.pdf}
    %\caption{probably delete this one }
    %\centering
%\end{figure}



\subsection{Analysis}\label{analysis}

In this section we provide empirical justification for Iterative Weight Recycling.  

\noindent \textbf{Effect of Random Weights}
We first perform an ablation study on the effects of random weights by assessing whether $\mathrm{S}^{low}$ can be replaced by \textit{any} subset of weights.  Specifically, to justify the reuse of "important" weights as identified $\mathrm{S}$, we replace $\mathrm{S}^{high}$ at $l$ with $\mathrm{S}^{high}_l=\{\tau(i)^{2k}_{i=k+1}\}$.  Effectively, this recycles weight values deemed to be in the second tier of "unimportance" as measured by parameter $\mathrm{S}$. 

In our experiments, we train a Conv-6 architecture with both Edge-Popup and Biprop Weight Recycling algorithms with 3 different initializations to compare the effectiveness of the approach to the baseline algorithm.  We use the same hyperparameters as the initial experiments, except we use Kaiming Normal initialization for Edge-Popup. 

Biprop accuracy dropped from 90.9\%($\pm0.2$ ) to 89.7\%($\pm0.5$), and Edge-Popup accuracy dropped from 88.9\%($\pm0.3$ ) to 87.15\%($\pm0.5$). Results of these experiments indicate a benefit to recycling high importance weights as opposed to other random weights.   Finally, we note that recycling weights is more computationally efficient than re-randomizing weights. %This experiment was similar to using the IteRand algorithm, which we cover in a later section.  



\noindent \textbf{Norms on Pretrained Models}
In this section we study the Frobenius norms of various pruning algorithms.  We first train a dense Conv-8 network using a standard training procedure on the CIFAR-10 dataset, and subsequently apply Edge-Popup, Biprop, and Iterative Weight Recycling algorithms to prune the trained model.  
Frobenius norms of each model layer and algorithm are depicted in Figure \ref{norms}, with each algorithm using a 50\% prune rate except for the dense network.  %L1 unstructured pruning retains high norms as expected, with accuracy dropping from 89.1\% (dense model) to 85.7\% (L1 pruned).  
%Next, we train each subnetwork identification algorithm on the pretrained dense model.  

%We show that Weight Recycling attains norms similar to L1 pruning, and higher than Biprop, Edge-Popup,and IteRand (IteRand results excluded for visualization).   %This result indicates that Weight Recycling emphasizes keeping high norm weight features present in the pretrained model.

Analyzing the norms of unpruned weights (depicted with a "+") compared to pruned weights ("-") shows that each algorithm exhibits higher norms in its \textit{unpruned} weight mask compared to its pruned weights, except for Iterative Weight Recycling.  Interestingly, even Biprop, which uses floating-point weights \textit{prior} to binarization, exhibits higher norms in its unpruned weights.  
Iterative Weight Recycling, on the other hand, exhibits similar norms in both its pruned and unpruned weights. %, indicating that Iterative Weight Recycling reuses  high norm weights present in the pretrained model.% as a result of recycling high-norm weight values into pruned weight indices.  

%\begin{wrapfigure}[22]{r}{0.5\textwidth}
\begin{figure}


  %\begin{center}
    \includegraphics[width=0.45\textwidth]{figures/norms.pdf}
 % \end{center}
  \caption{\textbf{ Norms across layers}. We apply various pruning algorithms to a dense pretrained Conv-8 architecture, showing that Iterative Weight Recycling exhibits high norms. "+" indicates unpruned norms, "-" pruned norms. }\label{norms}
%\end{wrapfigure}
\end{figure}

Results of this analysis indicate that high-norm weight values are chosen naturally in both Edge-Popup and Biprop.  We show that Weight Recycling emphasizes the reuse of high-norm weight values, creating a search space of good candidates compared to a randomly initialized population.  

We show similar results on randomly initialized networks in the Appendix, with each algorithm choosing high norm weights.   IteRand exhibited similar results, however we excluded these results in Figure \ref{norms} for visualization purposes.  


\begin{figure*}[t]

% preliminary
\sbox\twosubbox{%
  \resizebox{\dimexpr.98\textwidth-1em}{!}{%
    \includegraphics[height=2cm]{figures/heatmap_jaccard_0.5.pdf}\label{galore1}
    \includegraphics[height=2cm]{figures/masks_graph_jaccard_all.pdf}\label{galore2}
  }%
}
\setlength{\twosubht}{\ht\twosubbox}

% typeset

\centering

%\subcaptionbox{\label{galore1}}{%
  \includegraphics[height=\twosubht]{figures/heatmap_jaccard_0.5.pdf}\label{galore1}
%}\quad
%\subcaptionbox{\label{galore2}}{%
  \includegraphics[height=\twosubht]{figures/masks_graph_jaccard_all.pdf}\label{galore2}
%}

\caption{\textbf{Comparing \gls{mpts}:} We train multiple models with the same weight initialization and constrain hyperparameters such as prune rate.  The only hyperparameter we modify is the seed of masking parameter $\mathrm{S}$.  Under these constraints, our results yield diverse masks, confirming that multiple winning tickets exist in a randomly initialized network.  The similarity matrix (left) highlights the low \gls{ji} when comparing 15 Edge-Popup models and 15 Biprop models with the same weight initialization and prune rate (0.5).     Despite this, \gls{ji} values fall within small ranges, indicating the algorithms converge to a similar search space.  The bottom triangle compares Edge-Popup and the top triangle Biprop. The layer-by-layer similarity scores (right) summarizes the mean \gls{ji} across models layers.  In the error bars we denote the min and max model values for each layer.  } \label{fig:winningtix}

\end{figure*}

\noindent \textbf{Iterative Weight Recycling compared to IteRand}
Comparing Iterative Weight Recycling to IteRand yields statistically insignificant improvements in the Edge-Popup algorithm using Signed Constant initialization. We argue that any weight randomization works well under this initialization because constant weight values make recycling less relevant.  However, weight recycling performs as well, if not slightly better than IteRand with less computational cost.  
Finally, weight recycling outperforms IteRand at aggressive prune rates, as depicted in Figure \ref{params}.  

In addition to these arguments we note several key limitations to IteRand: \textbf{1)} Additional storage cost.  By iteratively re-randomizing weights, we need to save additional random seeds and binary masks every $K_{per}$ during training.  In limited compute environments this may become a restraining factor.  In the original work, $K_{per}$ was set to aggressive values: one to ten times per epoch. 
\textbf{2)} Periodically re-randomizing weights creates an artificially over-parameterized model.  If pruned weights are re-initialized with rate $r$ every $K_{per}$ for $K$ epochs, a network with $n$ weights needs $n+K{}/K_{per}(n \cdot p \cdot r)$ weight values to achieve high-accuracy.  Iterative Weight Recycling instead shows that relevant weight values exist as a \textit{subset} of the original $n$ parameters, and identifies those values for reuse. 
\textbf{3)} In the original paper, IteRand was only tested on Edge-Popup. We implemented IteRand on the Biprop algorithm and were unable to achieve successful results on over half a dozen configurations.  Table \ref{imagenet} (Biprop) depicts these results.  


