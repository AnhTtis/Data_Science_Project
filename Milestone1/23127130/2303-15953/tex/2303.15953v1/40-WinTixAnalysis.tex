

The Multi-Prize Lottery Ticket Hypothesis \cite{diffenderfer_multi-prize_2021} posits that a randomly initialized neural network contains \textit{several} winning tickets  that achieve accuracy comparable to weight trained models of the same architecture.  In this section, we further assess this hypothesis by asking the following question: \textit{Given a sufficiently overparameterized network, can we find multiple winning tickets (\gls{mpts}) under strict hyperparameter control?}

While \gls{mpts} were shown to be theoretically possible \cite{diffenderfer_multi-prize_2021}, empirical results were mostly limited to showing the existence of \gls{mpts} by varying the prune rate.   While this was sufficient evidence for the proof, we instead seek to evaluate whether winning tickets exhibit differing structures in a constrained environment.  In particular, we restrict hyperparameters such as prune rate in order to evaluate the heterogeneity of \gls{mpts}.

We hypothesize that winning tickets, i.e. unique mask structures, exist in larger quantities than has previously been reported.  To exemplify this, consider the smallest layer of a Conv-6 network (the first layer) containing 1,258 weights.  
When restricting the search for a mask to a specific prune rate, say 95\%, there are $1,258 \choose 63$ possible masks to choose from, an astronomical number.  % Scaling this search space over an entire network with several layers provides reasonable confidence that we can find multiple winning tickets with little algorithmic modification.   %that returns "undefined" in many modern calculators. 







\noindent \textbf{Experiments} We use the Conv-6 network since it is relatively compact (2.26m parameters) and also generates winning tickets at multiple prune rates.  To perform hyperparameter control, we initialize each model with an identical configuration; additionally, we seed each run to a) initialize the same weights b) execute the same training feed (i.e. batches are identical in both data and ordering), and c) facilitate consistency across libraries and devices (e.g.. NumPy/PyTorch, CPU/GPU). We set the torch CUDA backend to 'deterministic', as is recommended in documentation. Our single hyperparameter modification is the seed for score parameter $\mathrm{S}$, which we increment by one for each subsequent model.  

We train models using the standard Edge-Popup and Biprop algorithms.  At 50\% prune rate, we train 15 models for each algorithm, and at 75\% and 90\% prune rates we train 5 models for each algorithm.  We note that each pruned model achieved test accuracy similar to other models of the same algorithm and prune rate: Edge-Popup at 50\% prune rate ($\mu=89.57\%\pm0.2$), 75\% prune rate ($\mu=86.75\%\pm0.1$), 90\% prune rate ($\mu=79.73\%\pm0.1$). Biprop at 50\% prune rate($\mu=89.7\%\pm0.2$), 75\% prune rate ($\mu=88.56\%\pm0.2$), 90\% prune rate ($\mu=82.56\%\pm0.1$).  For Biprop,  scale parameter $\alpha$ is converted to one for each mask in order to compute mask equality.  %Otherwise, the resulting comparison method 



We evaluate the similarity of binary masks using \gls{smc} \cite{rand_objective_1971}, and \gls{ji} \cite{jaccard_distribution_1912,tanimoto}.  \gls{smc} measures the total percentage of matching masks, whereas \gls{ji} measures the percentage of masks equal to one, excluding mutual absence from the denominator  ($M_{11}/(M_{01}+M_{10}+M_{11})$.  


\begin{figure}
\centering

%\begin{wrapfigure}[13]{r}{0.375\textwidth}
  \begin{tabular}{lccc}
    \toprule
    Alg. &Pruned & SMC& JI \\
    \midrule
    Edge-Popup&50\%& 0.51&0.25\\
    Edge-Popup&75\%&0.63&0.13\\
        Edge-Popup&90\%&0.82&0.05\\
    Biprop&50\%&0.58&0.29\\
    Biprop&75\%&0.68&0.18\\
        Biprop&90\%&0.84&0.09\\
    \bottomrule
  \end{tabular}
  \makeatletter\def\@captype{table}\makeatother
  \caption{\textbf{Mask similarity statistics:} Statistics of  model combinations with varying algorithms and prune rates. }\label{statistics}
\vspace*{-\abovedisplayskip}
\end{figure}


\noindent \textbf{Results} 
In Figure \ref{fig:winningtix}, we show the heatmap of \gls{ji} coefficients at 50\% prune rate as well as layer level \gls{ji} for each algorithm.  Results across coefficients, model layers, and prune rates indicate that masks generated with different scoring seeds produce contrasting structures.  For example, the similarity matrix in Figure \ref{fig:winningtix} and the summary statistics in Table \ref{statistics} show all algorithm combinations yield \gls{ji} averages less than 0.29, a large difference between masks in all circumstances.  \gls{smc} values yielded higher scores (up to 0.84 in Table  \ref{statistics}), but this is expected at higher prune rates as most masks will have matching zeroes.  A telling distinction of uniqueness is the \gls{ji} at higher prune rates: at 90\% pruning, there is very low commonality between positive masks chosen for the subnetworks (0.05 and 0.09). 





Results also indicate several similarities.  Figure \ref{galore1} compares each model against 14 others for each algorithm, with \gls{ji} scores falling within $1/100$ of a decimal place of each other.  We speculate this as being a result of the algorithm and the prune rate: since each layer is confined to a specific prune rate, its match rate compared to the same layer in another model will be constrained by the prune rate.   A second similarity can be seen in layerwise coefficients yielding similar patterns across models. The first layer exhibits the highest similarity in all cases, indicating the masking structure needed to learn dataset inputs is important.  Middle layers yielded diverse masks, showing that weights at these layers exhibit more interchangeable utility.   Finally, the last layer generally yielded higher similarity coefficients, indicating the importance of specific weights for classification.   



Practically, this analysis provides several avenues for future research.  First, the existence of MPT's under hyperparameter control shows evidence that the total quantity of MPT's may be large.  Understanding this theoretical quantity may guide us in the search for better models.   And second, assessing the similarities of weights across MPT's can help us in understanding the desirable properties inherent to successful subnetworks.