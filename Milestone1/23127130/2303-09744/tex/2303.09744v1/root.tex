%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage[sort,compress,nospace]{cite}
% \usepackage{amsthm}
% \usepackage[numbers,sort&compress]{natbib}
\usepackage{color}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage{siunitx}
\usepackage{epstopdf}
\usepackage[caption=false, font=footnotesize]{subfig}

\title{\LARGE \bf
Identifying Occluded Agents in Dynamic Games\\ with Noise-Corrupted Observations
}


\author{Tianyu Qiu$^{1}$ and David Fridovich-Keil$^{2}$% <-this % stops a space
\thanks{$^{1}$Tianyu Qiu is with the Department of Automation in the School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai 200240, China. {\tt\small qty2898734013@sjtu.edu.cn}}
\thanks{$^{2}$D. Fridovich-Keil is with the Aerospace Engineering department, UT Austin. 
This research is supported by the National Science Foundation, under grant 2211548. {\tt\small dfk@utexas.edu}}
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


\begin{abstract}
To provide safe and efficient services, robots must rely on observations from sensors (lidar, camera, etc.) to have a clear knowledge of the environment. In multi-agent scenarios, robots must further reason about the intrinsic motivation underlying the behavior of other agents in order to make inferences about their future behavior. Occlusions, which often occur in robot operating scenarios, make the decision-making of robots even more challenging. In scenarios without occlusions, dynamic game theory provides a solid theoretical framework for predicting the behavior of agents with different objectives interacting with each other over time. Prior work proposed an inverse dynamic game method to recover the game model that best explains observed behavior. However, an apparent shortcoming is that it does not account for agents that may be occluded. Neglecting these agents may result in risky navigation decisions. To address this problem, we propose a novel inverse dynamic game technique to infer the behavior of occluded, unobserved agents that best explains the observation of visible agents' behavior, and simultaneously to predict the agents' future behavior based on the recovered game model. We demonstrate our method in several simulated scenarios. Results reveal that our method robustly estimates agents' objectives and predicts trajectories for both visible and occluded agents from a short sequence of noise corrupted trajectory observation of only the visible agents.
\end{abstract}

\section{Introduction}
Robots depend on sensor observations to be alert to the presence of static and dynamic obstacles. Yet, sensors are fundamentally limited due to occlusion or sensing range. In practice, humans can use their prior experience to make inferences about occluded agents and avoid potential risks. For example, in Figure \ref{traffic_scene}, the blue pedestrian is running across the road while the green and red vehicle are driving along the road in the same direction. However, the blue pedestrian is occluded by the red vehicle, and thus is invisible to the green vehicle. The green vehicle will likely collide with the blue pedestrian if the driver maintains its current speed. However, he notices that the red vehicle beside him brakes. Thus, he infers that someone is moving in the occluded area, forcing the red vehicle's deceleration. Consequently, he can actively brake to avoid the potential collision before the occluded agent finally comes into view. This example reveals the fact that the behaviors of occluded agents can be inferred from those of visible agents since both are affected by their interaction. The green vehicle is then capable of acting based on the knowledge of the invisible agents.

\begin{figure}[!t]
\centering
\includegraphics[width=0.45\textwidth]{interaction_220904.png}
% \centerline{\includegraphics[width=0.48\textwidth]{interaction_220904.png}}
\caption{Traffic scenario, in which the green vehicle's view is occluded by the red vehicle so it is not able to observe the occluded blue pedestrian. Our method utilizes only observations of visible agents' behavior, solves an inverse dynamic game to recover a game model that best explains the observed behavior, and indirectly identifies the behavior of occluded agents. In this traffic scenario, the driver in the green vehicle notices the deceleration of the red vehicle next to him. Our method infers that the blue pedestrian is running across the road within the occluded area. As a consequence, the driver in the green vehicle can actively brake to avoid a potential collision.}
\label{traffic_scene}
\end{figure}

Prior works \cite{peters2021inferring,fridovich2020efficient} have investigated multi-agent interaction problems with dynamic game techniques. Fridovich-Keil \textit{et al.}\cite{fridovich2020efficient} proposed an iterative linear quadratic game algorithm to compute optimal trajectories in multi-agent noncooperative scenarios. Based on this work, Peters \textit{et al.}\cite{peters2021inferring} solved the inverse problem to learn the game model from noisy observations, estimated agents' trajectories from the recovered model, and achieved high-quality estimation performance in occlusion-free scenarios.

Inspired by these works \cite{peters2021inferring,fridovich2020efficient}, we propose a novel technique that identifies the unknown parameters of each agent's cost function and estimates the trajectories of \textit{occluded} agents in a Nash game based on realistic sensor measurements of only visible agents. We evaluate our method in various scenarios at different levels of observation noise. Results reveal that our method is noise robust and provides accurate trajectory prediction for both visible and invisible agents.

\section{Related Work}
\subsection{Social Occlusion Inference}
Many works have explored methods to make inferences for occluded areas based on observation of visible agents' behavior in social settings. These works utilize the fundamental fact that one's behavior is affected by the surrounding environment to a great extent. Representative works often apply computer vision techniques \cite{hara2018recognizing,hara2020predicting} and occupancy grid map generating techniques \cite{afolabi2018people,itkina2022multi} to make inferences about the occluded area. Hara \textit{et al.} \cite{hara2018recognizing} proposed a CNN that identifies the existence of a person in blind spots with artificial occlusions in a volleyball game dataset. Subsequent work \cite{hara2020predicting} further extended to traffic scenarios and proposed a spatio-temporal D-CNN to predict whether a vehicle would come into view soon based on the tracking of visible pedestrians from first-person view video input. Afolabi \textit{et al.}\cite{afolabi2018people} proposed a mapping framework that incorporates people as sensors to generate an occupancy grid map for the occluded area. Itkina \textit{et al.}\cite{itkina2022multi} further trained a conditional variational autoencoder to generate an occupancy grid map from the observation of driver trajectories. 

% They fused the trajectory observation and standard sensor measurements to achieve real-time reliable occlusion inference. 

However, despite these encouraging achievements in social occlusion inference, these works only solve the problem of whether the occluded area is occupied, or whether an agent is coming out from the occluded area. They cannot infer the actual behavior or intent of agents in occluded areas, which prevents robots in the scene from making optimal decisions. Other techniques must be utilized to model the behavior of agents in occluded areas more precisely from the observation of only visible agents' social behavior.

\subsection{Planning-based Agent Behavior Modeling}
Before we move towards the ultimate goal of precise inference about the occluded agents' behavior, we first introduce several planning-based methods of agent behavior modeling which are closely related to our work. Planning-based methods make the assumption that humans rationally make decisions about their trajectories while walking or driving; that is, their trajectories must explicitly or implicitly follow certain rules and these rules can be applied to predict their future behaviors. This assumption casts the behavior inference problem as one of (inverse) optimal control, and the goal becomes to reconstruct such rules from the observation of agents' behavior.
\subsubsection{Single agent behavior modeling}
With the fundamental investigation of inverse optimal control (IOC) in \cite{kalman1963linear}, researchers extensively applied IOC techniques in modelling single agent behaviors\cite{maroger2020walking,albrecht2011imitating, maroger2022inverse}, where an explicit optimal control model was proposed and parameters were identified to best fit the observations of human behaviors. The advance of inverse reinforcement learning (IRL) introduced in \cite{ng2000algorithms} also gave rise to applications\cite{ziebart2009planning,kitani2012activity,monfort2015intent,previtali2016predicting} towards single agent behavior modeling. Representative work \cite{ziebart2009planning} described human behavior based on a Markov decision process with the principle of maximum entropy. Results were utilized to improve a robot's ability to navigate in the presence of pedestrians.

% For single agent scenarios, inverse reinforcement learning (IRL) \cite{ziebart2009planning,monfort2015intent,kretzschmar2016socially,sun2019behavior} and inverse optimal control (IOC) \cite{albrecht2011imitating, maroger2022inverse} techniques are widely applied to infer an agent's preferences from observed behavior. Ziebart \textit{et al.}\cite{ziebart2009planning} described human behavior based on a Markov decision process with the principle of maximum entropy. Results were utilized to improve a robot's ability to navigate in the presence of pedestrians. Beyond trajectory forecasting, subsequent work \cite{monfort2015intent} also achieved intent prediction via inverse linear quadratic regulation. {\color{red} Maroger \textit{et al.}\cite{maroger2022inverse} proposed an explicit optimal control model to predict human behaviors and solve the IOC problem to estimate the value weighting parameters in the model that best explain observations of human behaviors.}

\subsubsection{Multi-agent behavior modeling}
Aside from the interaction between agents and the environment, each agent's behavior affects that of other agents. A typical framework is multi-agent IRL generalized from IRL for single agent, under which human social behaviors are investigated\cite{henry2010learning,pfeiffer2016predicting}. Subsequent works further make applications in robot social navigation tasks\cite{kim2016socially,kretzschmar2016socially} to improve a robot's ability to navigate in the presence of pedestrians, and autonomous driving scenarios\cite{morales2018towards,sun2019behavior} to guarantee safe driving performance.

\subsection{Dynamic Games and Inverse Dynamic Games}
Our work builds upon dynamic game theory, which generalizes single agent optimal control cases to multi-agent scenarios and provides a solid framework to model multi-agent behaviors. Fridovich-Keil \textit{et al.}\cite{fridovich2020efficient} proposed an forward iterative linear quadratic game to depict interaction between agents in traffic scenarios and computed feedback Nash strategies. To model human behavior more accurately, more recent works \cite{peters2021inferring}, \cite{le2021lucidgames,peters2023learning,li2023cost} optimized the model by solving inverse dynamic games. Le Cleac’h \textit{et al.}\cite{le2021lucidgames} iteratively solved the inverse dynamic game problem to update a Bayesian estimate of agents' cost function parameters by recasting it in a recursive parameter-estimation framework. Peters \textit{et al.}\cite{peters2021inferring,peters2023learning} jointly optimized player objectives and trajectory estimates by coupling them through Nash equilibrium constraints based on noisy, partial state observations.

We emphasize that these works share the same limitation, which is that observations of all agents are required. To deal with scenarios where agents are occluded, inspired by \cite{peters2021inferring}, we propose a novel inverse dynamic game technique to identify the unknown weighting parameters in each agent's cost function, and simultaneously estimate both visible and invisible agents' trajectories that best explain the observations of visible agents' trajectories.

\section{Discrete Time Open-loop Nash Games}
% We first introduce symbols and notations that are used in this paper:
\begin{table}[H]
\caption{Main Symbols and Notations\label{tab:table1}}
\centering
\begin{tabular}{l|l}
\hline
${J^i}$ & Cost function (Objective) of the ${i^{\text{th}}}$ agent\\
${M\in \mathbb{N}^*}$ & Number of agents in the game\\
${\theta=(\theta^1,\cdots,\theta^M)}$ & Tuple of weighting parameters of all agents\\
${T\in \mathbb{N}^*}$ & Time horizon of the game\\
${x_t^i\in\mathbb{R}^n}$ & State of the ${i^{\text{th}}}$ agent at time ${t}$\\
${\textbf{x}^i:=(x_1^i,\cdots,x_T^i)}$ & Trajectory of the ${i^{\text{th}}}$ agent\\
${\textbf{x}_t:=(x_t^1,\cdots,x_t^M)}$ & State of all agents at time ${t}$\\
${\textbf{x}:=(\textbf{x}^1,\cdots,\textbf{x}^M)}$ & Tuple of trajectories of all agents\\
${u_t^i\in\mathbb{R}^m}$ & Control input of the ${i^{\text{th}}}$ agent at time ${t}$\\
${\textbf{u}^i:=(u_1^i,\cdots,u_T^i)}$ & Control sequence of the ${i^{\text{th}}}$ agent\\
${\textbf{u}_t:=(u_t^1,\cdots,u_t^M)}$ & Control input of all agents at time ${t}$\\
${\textbf{u}:=(\textbf{u}^1,\cdots,\textbf{u}^M)}$ & Tuple of control sequences of all agents\\
${\mathcal{L}^i}$ & Lagrangian of the ${i^{\text{th}}}$ agent\\
${\lambda_t^i\in\mathbb{R}^n}$ & Lagrange multiplier of the ${i^{\text{th}}}$ agent at time ${t}$\\
${\pmb{\lambda}^i:=(\lambda_1^i,\cdots,\lambda_T^i)}$ & Sequence of ${\lambda_t^i}$ of the ${i^{\text{th}}}$ agent\\
${\pmb{\lambda}_t:=(\lambda_t^1,\cdots,\lambda_t^M)}$ & Tuple of ${\lambda_t^i}$ of all agents at time ${t}$\\
${\pmb{\lambda}:=(\pmb{\lambda}^1,\cdots,\pmb{\lambda}^M)}$ & Tuple of sequences of ${\lambda_t^i}$ of all agents\\
${\mathcal{V}}$ & Set of visible agents in the game\\
${\mathcal{O}}$ & Set of occluded agents in the game\\
\hline
\end{tabular}
\end{table}
A discrete time open-loop Nash game with $M$ agents is characterized by the state ${x_t^i\in\mathbb{R}^n}$ and control inputs ${u_t^i\in\mathbb{R}^m}$, ${\forall i\in[M]}$, at time step $t$. The dynamics ${\textbf{x}_{t+1}=f(\textbf{x}_t,\textbf{u}_t)}$ describe how the game evolves with each agent's control input, at each time step ${t\in[T]}$. ${J^i:=\sum_{t=1}^T g_t^i(\textbf{x}_t,\textbf{u}_t)}$ for each agent evaluates their cost in the game. The game is thus fully characterized by the tuple of all agents' cost functions, the initial condition ${\textbf{x}_1}$, and the dynamics, which is denoted by ${\Gamma:=(\{J^i\}_{i=1}^M,\textbf{x}_1,f)}$.
In a Nash game, each agent aims to minimize his cost function, subject to dynamic feasibility constraints, i.e.
\begin{subequations}
    \begin{align}
    \min_{\textbf{x},\textbf{u}^i}\quad&J^i(\textbf{u};\textbf{x}_1), &\forall i\in[M],\label{costfunc}\\
    \text{s.t.}\quad &\textbf{x}_{t+1}=f(\textbf{x}_t,\textbf{u}_t), &\forall t\in[T-1].\label{dynamics}
    \end{align}
\end{subequations}
Given that each agent decides his control input rationally at equilibrium, the following inequalities are satisfied:
\begin{equation}
\label{nasheq}
    J^i(\textbf{u}^*;\textbf{x}_1)\leq J^i(\textbf{u}^i,\textbf{u}^{-i*};\textbf{x}_1), \forall i\in[M],
\end{equation}
and ${\textbf{u}^*:=(\textbf{u}^{1*},\cdots,\textbf{u}^{M*})}$ is called a Nash strategy. Equation \eqref{nasheq} reveals the fact that no agents will decrease cost by unilaterally deviating from Nash strategy ${\textbf{u}^{i*}}$\cite{bacsar1998dynamic}.

To make these concepts concrete, we introduce the following running example. Consider $M=2$ pedestrians walking toward their goal, avoiding each other. ${\textbf{x}_t:=(x_t^1,x_t^2)}$ denotes the position of both pedestrians and they follow single-integrator dynamics at time discretization $\Delta t$:
\begin{equation}
    \begin{aligned}
    x_{t+1}^i&=\left\{\begin{aligned}
        &p_{x,t+1}^i=p_{x,t}^i+v_{x,t}^i\Delta t,\\
        &p_{y,t+1}^i=p_{y,t}^i+v_{y,t}^i\Delta t,
    \end{aligned}\right.\\
    &=x_t^i + u_t^i\Delta t,\quad t\in[T-1],i\in\{1,2\},\\
    \textbf{x}_1 &\text{ is known.}
    \end{aligned}
\end{equation}
${u_t^i=[v_{x,t}^i,v_{y,t}^i]^\top}$ denotes the velocity of each pedestrian and ${\textbf{x}_1}$ is the initial state of two pedestrians. Each pedestrian's objective is the sum of the running cost ${g_t^i}$ over time, where ${g_t^i}$ is the combination of different features weighted by non-negative weighting parameters ${\theta^i}$:
\begin{equation}
    g_t^i=\sum_{j=1}^n\theta_j^ig_{j,t}^i\left\{\begin{aligned}
    &g_{1,t}^i=\|x_t^i-x_d^i\|_2^2\\
    &g_{2,t}^i=-\log(\|x_t^i-x_t^{-i}\|_2^2)\\
    &g_{3,t}^i=\|u_t^i\|_2^2
    \end{aligned}\right.,
\end{equation}
where ${\|\cdot\|_2^2}$ denotes the squared Euclidean norm. The pedestrian aims to get closer to his destination (${g_{1,t}^i}$), and to keep far away from the other agents (${g_{2,t}^i}$) without great energy consumption (${g_{3,t}^i}$). In practice, ${g_t^i}$ can be readily modified to accommodate different scenarios.

\section{Our Approach}
In this work, we introduce two roles in the dynamic game: the participants and the observer. The participants compete with each other in the game, while the observer is outside the game, observing the interaction between the agents. All participants are observable to each other. However, only some of the participants are visible to the observer, while others are occluded, and hence invisible. No matter whether the agents are visible or not, their interaction remains the same.

In practice, the observer could be a robot wishing to navigating in human-rich environments. To do so, he must estimate both visible and invisible agents' objectives and trajectories from the observation of only visible agents, and thereby improve his navigation performance.

For clarity, we seek to estimate the value of all weighting parameters ${\theta}$ in the game model as well as the trajectories ${\textbf{x}}$ of all agents that maximize the likelihood of a given sequence of state observations. ${\textbf{y}^\mathcal{V}:=(\textbf{y}^i),i\in\mathcal{V}}$:
\begin{subequations}
\begin{align}
\max_{\theta,\textbf{x},\textbf{u}}\quad&p(\textbf{y}^\mathcal{V}|\textbf{x},\textbf{u}),\label{obj}\\
\text{s.t.}\quad &(\textbf{x},\textbf{u})\text{ is an OLNE of }\Gamma(\theta),\label{inverseconstraint}\\
&(\textbf{x},\textbf{u})\text{ is dynamically feasible under } f,\label{inversedynamic}
\end{align}
\label{inversedynamicgame}
\end{subequations}

\noindent where ${\theta}$ is the tuple of weighting parameters over all agents, i.e. ${\theta:=(\theta^1,\cdots,\theta^M)}$, and ${p(\textbf{y}^\mathcal{V}|\textbf{x},\textbf{u})}$ denotes a likelihood model based on observation ${\textbf{y}^\mathcal{V}}$. Note that this formulation extends that of \cite{peters2021inferring} for our problem. In particular, we have constructed a new objective \eqref{obj} and kept the constraints \eqref{inverseconstraint}, \eqref{inversedynamic} the same since the visibility to the observer does not interfere with the participants' interaction.

To solve \eqref{inversedynamicgame}, we need to first solve the OLNE in the forward dynamic game \eqref{nasheq}. Since the constraints are the same, akin to the method in \cite{peters2021inferring}, we construct each player's Lagrangian with additional Lagrange multipliers ${\lambda_t^i}$:
\begin{equation*}
    \mathcal{L}^i=J^i+\sum_{t=1}^{T-1}{\lambda_t^i}^\top\left(x_{t+1}^i-f(x_t^i,u_t^i)\right).
\end{equation*}
The first-order necessary conditions for the optimal solution are given by the KKT conditions for each agent:
\begin{equation}
\begin{aligned}
\textbf{G}(\textbf{x}^i,\textbf{u}^i,\pmb{\lambda}^i):=\begin{bmatrix}
    \nabla_{\textbf{x}^i}\mathcal{L}^i\\
    \nabla_{\textbf{u}^i}\mathcal{L}^i\\
    x_{t+1}^i-f(x_t^i,u_t^i), t\in[T-1]
    \end{bmatrix}=\textbf{0},\\
\forall i\in [M].
\end{aligned}
\label{kkt}
\end{equation}
Thus \eqref{inverseconstraint} and \eqref{inversedynamic} are replaced by \eqref{kkt} and the inverse dynamic game model for occluded agents evolves into
\begin{subequations}
\begin{align}
\max_{\theta,\textbf{x},\textbf{u},\pmb{\lambda}}\quad&p(\textbf{y}^\mathcal{V}|\textbf{x},\textbf{u})\label{originobj}\\
\text{s.t.}\quad &\textbf{G}(\textbf{x}^i,\textbf{u}^i,\pmb{\lambda}^i)=\textbf{0}, \forall i\in[M].\label{finalconstraints}
\end{align}
\label{finalinversemodel}
\end{subequations}

\noindent The objective of \eqref{finalinversemodel} is to estimate the unknown weighting parameters and all agents' states and actions from the observation of only visible agents' trajectories under noise corruption. To deal with the observation noise, we assume that observations take white Gaussian noise, i.e. ${\textbf{n}_t\sim\mathcal{N}(\textbf{0},\Sigma)}$, and the observation of visible agents ${\textbf{y}_t^{\mathcal{V}}:=\textbf{x}_t^{\mathcal{V}}+\textbf{n}_t}$ in this work. We then solve \eqref{finalinversemodel} by substituting the likelihood maximization problem of \eqref{originobj} with the negative log-likelihood minimization problem with objective ${\sum_{t\in[T]}\sum_{i\in\mathcal{V}}\|y_t^i-x_t^i\|_2^2}$.

In the next section, we conduct simulation experiments to evaluate the estimation performance of our method and how it functions under different levels of observation noise.

\section{Simulation Experiments}
We implement our proposed approach in YALMIP \cite{lofberg2004yalmip}, a MATLAB interface for mathematical programming. We use the open-source COIN-OR IPOPT algorithm \cite{wachter2006implementation} as a low-level solver. We conduct Monte Carlo studies to analyze the performance of our proposed method in several simulated scenarios.
\subsection{Experiment Setup}
To demonstrate the performance and robustness of our method, we perform a sequence of Monte Carlo studies. For each scenario, we fix the weighting parameters ${\theta}$ for each agent and find the corresponding OLNE trajectories. We hide the invisible agents' trajectories and then corrupt the visible agents' trajectories with white Gaussian noise. To evaluate the performance of our method at different levels of noise corruption, we generate 24 sets of random observation sequences at 21 different levels of noise. For each of the resulting 504 observation sequences, we run our method to recover estimates of weights for each agent as well as the trajectory for invisible agents.
\subsection{Evaluation Metrics}
To evaluate the performance of weighting parameter recovery, we first follow \cite{peters2021inferring} and measure the cosine dissimilarity between the unobserved true weighting parameters $\theta_{\text{true}}$ and the estimated parameters ${\theta_{\text{est}}}$
\begin{equation}
    D(\theta_{\text{true}},\theta_{\text{est}}):=1-\frac{1}{M}\sum_{i\in[M]}\frac{{\theta_{\text{true}}^i}^\top\theta_{\text{est}}^i}{\|\theta_{\text{true}}^i\|_2\|\theta_{\text{est}}^i\|_2},
\label{parametererror}
\end{equation}
which denotes the degree of dissimilarity between $\theta_{\text{true}}$ and $\theta_{\text{est}}$.

To evaluate the performance of trajectory estimation for all agents, we measure the trajectory estimation error between the unobserved true trajectory based on true weighting parameters and the estimated trajectory based on recovered weighting parameters for both visible and invisible agents with the average displacement error (ADE) metric:
\begin{equation}
\begin{aligned}
ADE_{\text{visible}}&:=\frac{1}{|\mathcal{V}|\cdot T}\sum_{i\in\mathcal{V}}\sum_{t\in[T]}\|x_{\text{GT},t}^i-x_{\text{est},t}^i\|_2,\\
ADE_{\text{invisible}}&:=\frac{1}{|\mathcal{O}|\cdot T}\sum_{i\in\mathcal{O}}\sum_{t\in[T]}\|x_{\text{GT},t}^i-x_{\text{est},t}^i\|_2,
\end{aligned}
\label{positionerror}
\end{equation}
where ${x_{\text{GT},t}^i}$ and ${x_{\text{est},t}^i}$ denote the ground truth state and the estimated position of the $i^{\text{th}}$ agent at time step $t$, respectively. Note that we measure the trajectory estimation error separately for visible and invisible agents, rather than measuring the total error, as in \cite{peters2021inferring}. Results reveal the great difference of estimation performance between the two categories, and hence the necessity of separation.

\subsection{Simulation Experiments}
\subsubsection{Identifying an agent with constant velocity}\label{constvelexample}
We evaluate our method in a Monte Carlo study of the running example of a visible agent avoiding collision with an occluded agent with constant velocity. This example represents a simplified scenario where the occluded agent does not react to other agents and keeps his velocity constant. In this example, the problem degrades from solving a dynamic game problem to solving the following forward optimal control problem
\begin{equation}
    \begin{aligned}
    \min_{\textbf{x},\textbf{u}^1}\quad &J^1(\textbf{u};\textbf{x}_1), &\forall i\in[M],\\
    \text{s.t.}\quad &\textbf{x}_{t+1}^1=x_t^1+u_t^1\Delta t, &\\
    &\textbf{x}_{t+1}^2=x_t^2+v\Delta t, &\forall t\in[T-1],
    \end{aligned}
    \label{optimal_control}
\end{equation}
where $v$ takes an unknown constant value, and the inverse optimal control problem
\begin{equation*}
\begin{aligned}
\max_{\theta,\textbf{x},\textbf{u},v}\quad&p(\textbf{y}^1|\textbf{x},\textbf{u},v)\\
\text{s.t.}\quad &(\textbf{x}^1,\textbf{u}^1)\text{ is optimal under \eqref{optimal_control}},\\
&(\textbf{x},\textbf{u},v)\text{ is dynamically feasible under } f.
\end{aligned}
\end{equation*}

Figure \ref{constvel_para} shows the parameter estimation performance at different levels of observation noise. We present the median and the interquartile range (IQR) for parameter dissimilarity. Our method accurately recovers the unobserved weighting parameters with which the model best explains the observed noisy trajectories of visible agents. The result also reveals that our method is noise-robust since the error remains small ($\times10^{-3}$ order of magnitude) even as the noise increases.

Benefiting from the accurate recovery of weighting parameters, our method also outputs accurate trajectory estimates for both visible and invisible agents. Figure \ref{constvel_pos} displays the trajectory estimation performance at different levels of observation noise. The median estimation error for the visible agent is under \SI{0.15}{\meter} and that for the invisible agent is under \SI{0.35}{\meter} at all levels of observation noise.

Note that the trajectory estimation error for visible agents is on the order of the standard deviation of the observation noise, which means that observation noise contributes the majority of the error. The narrow IQR also indicates that our method achieves high precision in trajectory estimation for visible agents. In contrast, the trajectory estimation for invisible agents suffers from greater median error and larger IQR. The extra error and uncertainty is due to the lack of observation. This fact reflects the importance of observation in robot operating scenarios. Still, it is interesting to note that the estimation error of the occluded agent's trajectory is around twice the error of visible agents' trajectory, which means our method is fairly accurate.

\subsubsection{Identifying an agent applying a Nash strategy}\label{nashexample}
\begin{figure*}[!ht]
	\centering
	\subfloat[]{\includegraphics[width=0.25\textwidth]{demo_gt.png}
		\label{demo_gt}}
	\subfloat[]{\includegraphics[width=0.25\textwidth]{demo_obs.png}
		\label{demo_obs}}
	\subfloat[]{\includegraphics[width=0.25\textwidth]{demo_est.png}
		\label{demo_est}}
	\caption{Demonstration for the 2-agent running example in Section \ref{nashexample} at noise with standard deviation $\sigma=0.05$ for 24 different observation sequences, where both agents apply open-loop Nash strategy. \ref{demo_gt}: Ground truth trajectory for both agents. \ref{demo_obs}: Observations from the observer's perspective, where the trajectories of the red agent are corrupted by noise and the blue agent is invisible. \ref{demo_est}: Trajectories estimated by solving the recovered dynamic game based on noisy observations. Despite the large observation noise in \ref{demo_obs}, our method can reliably estimate even the invisible agent’s trajectory in \ref{demo_est}.}
	\label{nashdemo}
\end{figure*}

In this example, we evaluate our method for collision avoidance with two agents, both of whom apply open-loop Nash strategies. Figure \ref{demo_gt} shows the ground truth trajectory of both agents. Figure \ref{demo_obs} shows the sensor measurements from the observer's perspective: the trajectories of the visible agent are corrupted by noise, while the invisible agent cannot be observed. Figure \ref{demo_est} shows the predicted trajectories for both agents based on the game model recovered by our method.

Figure \ref{2person_para} and Figure \ref{2person_pos} display the estimation performance for the weighting parameters and the trajectory for both agents at different levels of observation noise. Compared with the example in Section \ref{constvelexample}, the median parameter estimation dissimilarity and median trajectory estimation error for both agents are higher, and IQR for both metrics is larger in this example. We attribute this degradation to this example's added complexity. It is reasonable to infer that the estimation performance decreases with the number of agents.

\subsubsection{Identifying an occluded pedestrian in traffic}\label{trafficexample}
Next, we consider a more realistic road crossing scenario, depicted in Figure \ref{traffic_scene}. In this scenario, each agent not only tries to keep a safe distance from other agents but also seeks to move forward while keeping himself in the lane according to traffic rules. Figure \ref{traffic_scene} demonstrates that the blue pedestrian and the driver in the red vehicle are observable to each other. However, the blue pedestrian is occluded by the red vehicle and hence invisible to the driver in the green vehicle. Both the blue pedestrian and the red vehicle apply open-loop Nash strategies: the red vehicle brakes and the blue pedestrian sightly turns left to avoid collisions. Although the movement of the blue pedestrian is invisible to the driver in the green vehicle, he can make the inference that someone is running across the road and predict that agent's trajectory based on the observed deceleration of the red vehicle next to him. He can then also brake in advance to avoid a potential collision with the blue pedestrian.

Figure \ref{traffic_para} and Figure \ref{traffic_pos} display the estimation performance for the weighting parameters and the trajectory for both agents at different levels of observation noise. Our method still achieves accurate and noise-robust estimation of the unknown weighting parameters as well as the visible agent's trajectory. Compared with the simpler example in Section \ref{nashexample}, a more complicated game model contributes to a greater estimation error of the invisible agent's trajectory. Note that in the traffic scenario, the visible agent's motion is highly restricted by the traffic rules, and we only have access to part of the trajectory observation (first 5 steps), therefore the estimation error of the visible agent's trajectory is lower than that of the two agent example in Section \ref{nashexample}.

% \subsection{Real World Experiment}
% Beyond simulation experiments, we apply our method to the real world experiment. {\color{red} the experiment not finished yet}

\section{Conclusion \& Future Work}
In this work, we have proposed a novel method based on the inverse dynamic game technique to identify behaviors of invisible agents in occluded areas from noise-corrupted observations of only visible agents. Our proposed method recovers unobserved weighting parameters in the game model that best explain the observed trajectories, and simultaneously computes open-loop Nash trajectories for both visible and invisible agents. The computed results can be further utilized in agents' trajectory estimation and prediction. Numerical results in simulation experiments show that our method is robust to observation noise and provides an accurate estimation for both weighting parameters and invisible agents' trajectories.

Currently, we evaluate our method in simulation experiments. Future work should investigate techniques to incorporate our method with first-person view sensing results from real sensors (lidar, camera, etc.) so that our proposed method can be utilized in real urban traffic scenarios.
\begin{figure*}[!t]
	\centering
	\subfloat[]{\includegraphics[width=0.24\textwidth]{constvel_para.png}
		\label{constvel_para}}
	\subfloat[]{\includegraphics[width=0.24\textwidth]{2person_para.png}
		\label{2person_para}}
	\subfloat[]{\includegraphics[width=0.24\textwidth]{traffic_para.png}
		\label{traffic_para}}

	\subfloat[]{\includegraphics[width=0.24\textwidth]{constvel_pos.png}
		\label{constvel_pos}}
	\subfloat[]{\includegraphics[width=0.24\textwidth]{2person_pos.png}
		\label{2person_pos}}
	\subfloat[]{\includegraphics[width=0.24\textwidth]{traffic_pos.png}
		\label{traffic_pos}}
	\caption{Estimation performance for the our method in different examples at different levels of observation noise. Figures on the top demonstrate the parameter estimation dissimilarity and figures on the bottom demonstrate the trajectory estimation error for each example. \ref{constvel_para} and \ref{constvel_pos}: the example in Section \ref{constvelexample}, where the invisible agent takes a constant velocity. \ref{2person_para} and \ref{2person_pos}: the example in Section \ref{nashexample}, where both the visible and invisible agent apply open loop Nash strategies. \ref{traffic_para} and \ref{traffic_pos}: the example of the traffic scenario in Section \ref{trafficexample}, where both the visible and invisible agent apply open loop Nash strategies. In each figure, we plot the median dissimilarity \eqref{parametererror}, displacement error \eqref{positionerror} and their IQR over 24 samples at each level of observation noise.}
	\label{resultstat}
\end{figure*}

% \bibliographystyle{IEEEtran}
% \bibliography{IEEEabrv,reference.bib}
% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{peters2021inferring}
L.~Peters, D.~Fridovich-Keil, V.~Rubies-Royo, C.~J. Tomlin, and C.~Stachniss,
  ``Inferring objectives in continuous dynamic games from noise-corrupted
  partial state observations,'' \emph{arXiv preprint arXiv:2106.03611}, 2021.

\bibitem{fridovich2020efficient}
D.~Fridovich-Keil, E.~Ratner, L.~Peters, A.~D. Dragan, and C.~J. Tomlin,
  ``Efficient iterative linear-quadratic approximations for nonlinear
  multi-player general-sum differential games,'' in \emph{2020 IEEE
  international conference on robotics and automation (ICRA)}.\hskip 1em plus
  0.5em minus 0.4em\relax IEEE, 2020, pp. 1475--1481.

\bibitem{hara2018recognizing}
K.~Hara, H.~Kataoka, M.~Inaba, K.~Narioka, and Y.~Satoh, ``Recognizing people
  in blind spots based on surrounding behavior,'' in \emph{Proceedings of the
  European Conference on Computer Vision (ECCV) Workshops}, 2018, pp. 0--0.

\bibitem{hara2020predicting}
K.~Hara, H.~Kataoka, M.~Inaba, K.~Narioka, R.~Hotta, and Y.~Satoh, ``Predicting
  vehicles appearing from blind spots based on pedestrian behaviors,'' in
  \emph{2020 IEEE 23rd International Conference on Intelligent Transportation
  Systems (ITSC)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2020, pp. 1--8.

\bibitem{afolabi2018people}
O.~Afolabi, K.~Driggs-Campbell, R.~Dong, M.~J. Kochenderfer, and S.~S. Sastry,
  ``People as sensors: Imputing maps from human actions,'' in \emph{2018
  IEEE/RSJ International Conference on Intelligent Robots and Systems
  (IROS)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2018, pp. 2342--2348.

\bibitem{itkina2022multi}
M.~Itkina, Y.-J. Mun, K.~Driggs-Campbell, and M.~J. Kochenderfer, ``Multi-agent
  variational occlusion inference using people as sensors,'' in \emph{2022
  International Conference on Robotics and Automation (ICRA)}.\hskip 1em plus
  0.5em minus 0.4em\relax IEEE, 2022, pp. 4585--4591.

\bibitem{kalman1963linear}
R.~E. Kalman, ``When is a linear control system optimal?'' in \emph{Joint
  Automatic Control Conference}, no.~1, 1963, pp. 1--15.

\bibitem{maroger2020walking}
I.~Maroger, O.~Stasse, and B.~Watier, ``Walking human trajectory models and
  their application to humanoid robot locomotion,'' in \emph{2020 IEEE/RSJ
  International Conference on Intelligent Robots and Systems (IROS)}.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 2020, pp. 3465--3472.

\bibitem{albrecht2011imitating}
S.~Albrecht, K.~Ramirez-Amaro, F.~Ruiz-Ugalde, D.~Weikersdorfer, M.~Leibold,
  M.~Ulbrich, and M.~Beetz, ``Imitating human reaching motions using physically
  inspired optimization principles,'' in \emph{2011 11th IEEE-RAS International
  Conference on Humanoid Robots}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2011, pp. 602--607.

\bibitem{maroger2022inverse}
I.~Maroger, O.~Stasse, and B.~Watier, ``Inverse optimal control to model human
  trajectories during locomotion,'' \emph{Computer Methods in Biomechanics and
  Biomedical Engineering}, vol.~25, no.~5, pp. 499--511, 2022.

\bibitem{ng2000algorithms}
A.~Y. Ng, S.~Russell \emph{et~al.}, ``Algorithms for inverse reinforcement
  learning.'' in \emph{Icml}, vol.~1, 2000, p.~2.

\bibitem{ziebart2009planning}
B.~D. Ziebart, N.~Ratliff, G.~Gallagher, C.~Mertz, K.~Peterson, J.~A. Bagnell,
  M.~Hebert, A.~K. Dey, and S.~Srinivasa, ``Planning-based prediction for
  pedestrians,'' in \emph{2009 IEEE/RSJ International Conference on Intelligent
  Robots and Systems}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2009, pp.
  3931--3936.

\bibitem{kitani2012activity}
K.~M. Kitani, B.~D. Ziebart, J.~A. Bagnell, and M.~Hebert, ``Activity
  forecasting,'' in \emph{Computer Vision--ECCV 2012: 12th European Conference
  on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part IV
  12}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2012, pp. 201--214.

\bibitem{monfort2015intent}
M.~Monfort, A.~Liu, and B.~Ziebart, ``Intent prediction and trajectory
  forecasting via predictive inverse linear-quadratic regulation,'' in
  \emph{Twenty-Ninth AAAI Conference on Artificial Intelligence}, 2015.

\bibitem{previtali2016predicting}
F.~Previtali, A.~Bordallo, L.~Iocchi, and S.~Ramamoorthy, ``Predicting future
  agent motions for dynamic environments,'' in \emph{2016 15th IEEE
  International Conference on Machine Learning and Applications (ICMLA)}.\hskip
  1em plus 0.5em minus 0.4em\relax IEEE, 2016, pp. 94--99.

\bibitem{henry2010learning}
P.~Henry, C.~Vollmer, B.~Ferris, and D.~Fox, ``Learning to navigate through
  crowded environments,'' in \emph{2010 IEEE international conference on
  robotics and automation}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2010,
  pp. 981--986.

\bibitem{pfeiffer2016predicting}
M.~Pfeiffer, U.~Schwesinger, H.~Sommer, E.~Galceran, and R.~Siegwart,
  ``Predicting actions to act predictably: Cooperative partial motion planning
  with maximum entropy models,'' in \emph{2016 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS)}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2016, pp. 2096--2101.

\bibitem{kim2016socially}
B.~Kim and J.~Pineau, ``Socially adaptive path planning in human environments
  using inverse reinforcement learning,'' \emph{International Journal of Social
  Robotics}, vol.~8, pp. 51--66, 2016.

\bibitem{kretzschmar2016socially}
H.~Kretzschmar, M.~Spies, C.~Sprunk, and W.~Burgard, ``Socially compliant
  mobile robot navigation via inverse reinforcement learning,'' \emph{The
  International Journal of Robotics Research}, vol.~35, no.~11, pp. 1289--1307,
  2016.

\bibitem{morales2018towards}
L.~Y. Morales, A.~Naoki, Y.~Yoshihara, and H.~Murase, ``Towards predictive
  driving through blind intersections,'' in \emph{2018 21st International
  Conference on Intelligent Transportation Systems (ITSC)}.\hskip 1em plus
  0.5em minus 0.4em\relax IEEE, 2018, pp. 716--722.

\bibitem{sun2019behavior}
L.~Sun, W.~Zhan, C.-Y. Chan, and M.~Tomizuka, ``Behavior planning of autonomous
  cars with social perception,'' in \emph{2019 IEEE Intelligent Vehicles
  Symposium (IV)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2019, pp.
  207--213.

\bibitem{le2021lucidgames}
S.~Le~Cleac’h, M.~Schwager, and Z.~Manchester, ``Lucidgames: Online unscented
  inverse dynamic games for adaptive trajectory prediction and planning,''
  \emph{IEEE Robotics and Automation Letters}, vol.~6, no.~3, pp. 5485--5492,
  2021.

\bibitem{peters2023learning}
L.~Peters, V.~Rubies-Royo, C.~J. Tomlin, L.~Ferranti, J.~Alonso-Mora,
  C.~Stachniss, and D.~Fridovich-Keil, ``Learning players' objectives in
  continuous dynamic games from partial state observations,'' \emph{arXiv
  preprint arXiv:2302.01999}, 2023.

\bibitem{li2023cost}
J.~Li, C.-Y. Chiu, L.~Peters, S.~Sojoudi, C.~Tomlin, and D.~Fridovich-Keil,
  ``Cost inference for feedback dynamic games from noisy partial state
  observations and incomplete trajectories,'' \emph{arXiv preprint
  arXiv:2301.01398}, 2023.

\bibitem{bacsar1998dynamic}
T.~Ba{\c{s}}ar and G.~J. Olsder, \emph{Dynamic noncooperative game
  theory}.\hskip 1em plus 0.5em minus 0.4em\relax SIAM, 1998.

\bibitem{lofberg2004yalmip}
J.~Lofberg, ``Yalmip: A toolbox for modeling and optimization in matlab,'' in
  \emph{2004 IEEE international conference on robotics and automation (IEEE
  Cat. No. 04CH37508)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2004, pp.
  284--289.

\bibitem{wachter2006implementation}
A.~W{\"a}chter and L.~T. Biegler, ``On the implementation of an interior-point
  filter line-search algorithm for large-scale nonlinear programming,''
  \emph{Mathematical programming}, vol. 106, no.~1, pp. 25--57, 2006.

\end{thebibliography}

% \bibliography{reference.bib}
\end{document}
