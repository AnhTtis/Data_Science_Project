\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a content stream. Unable to analyze the PDF file.
%This is a known problem with the pdfLaTeX conversion filter. The file cannot be opened with Acrobat Reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found at http:\\www.ctan.org
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage[sort,compress,nospace]{cite}
% \usepackage{amsthm}
% \usepackage[numbers,sort&compress]{natbib}
\usepackage{color}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage{siunitx}
\usepackage{epstopdf}
\usepackage[caption=false, font=footnotesize]{subfig}
\usepackage{svg}

\title{\LARGE \bf
Inferring Occluded Agent Behavior in Dynamic Games\\ from Noise-Corrupted Observations
}


\author{Tianyu Qiu$^{1}$ and David Fridovich-Keil$^{1}$% <-this % stops a space
\thanks{$^{1}$Tianyu Qiu and D. Fridovich-Keil are with the Aerospace Engineering department, University of Texas at Austin. This research is supported by the National Science Foundation under grant 2211548. {\tt\small tianyuqiu@utexas.edu, dfk@utexas.edu}}
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


\begin{abstract}
Robots and autonomous vehicles must rely on sensor observations, e.g., from lidars and cameras, to comprehend their environment and provide safe, efficient services. In multi-agent scenarios, they must additionally account for other agents' intrinsic motivations, which ultimately determine the observed and future behaviors. Dynamic game theory provides a theoretical framework for modeling the behavior of agents with different objectives who interact with each other over time. Previous works employing dynamic game theory often overlook occluded agents, which can lead to risky navigation decisions. To tackle this issue, this paper presents an inverse dynamic game technique which optimizes the game model itself to infer unobserved, occluded agents' behavior that best explains the observations of visible agents. Our framework concurrently predicts agents' future behavior based on the reconstructed game model. Furthermore, we introduce and apply a novel receding horizon planning pipeline in several simulated scenarios. Results demonstrate that our approach offers 1) robust estimation of agents' objectives and 2) precise trajectory predictions for both visible and occluded agents from observations of only visible agents. Experimental findings also indicate that our planning pipeline leads to safer navigation decisions compared to existing baseline methods.
\end{abstract}

\section{Introduction}

Robots depend extensively on sensor observations of static and dynamic obstacles. However, sensors are fundamentally limited, primarily due to occlusion or sensing range constraints. To illustrate this challenge, consider the scenario in Figure \ref{traffic_scene} where a blue pedestrian is crossing a road while green and red vehicles are driving along their lanes. The red vehicle blocks the green one's view of the blue pedestrian, making it invisible to the green vehicle. If the green vehicle maintains its current speed, it is likely to collide with the blue pedestrian. However, it senses that the red vehicle is decelerating, suggesting that someone is moving in the occluded area. Therefore, it brakes in advance to avoid a potential collision, even before the occluded agent becomes visible. 

This example illustrates how the interaction among agents determines their observed behavior, and conversely, their observed behavior reveals the underlying interaction structure. By understanding this point, robots and autonomous vehicles can anticipate and respond to potential dangers promptly.

\begin{figure}[!t]
\centering
% \includesvg[inkscapelatex=false,width=0.485\textwidth]{traffic_demo.svg}
\includegraphics[width=0.485\textwidth]{traffic_demo.png}
\caption{
A traffic scenario in which the view of the autonomous vehicle in green is occluded by the red vehicle, so it cannot observe the blue pedestrian running across the street. With our proposed method, the green vehicle notices the red vehicle's deceleration. It then infers that an agent is crossing the road from the occluded area. Consequently, it can brake to avoid a collision.
}
\label{traffic_scene}
\end{figure}

Prior works \cite{fisac2019hierarchical,wang2019game,wang2021game,fridovich2020efficient,peters2021inferring,peters2023ijrr,li2023cost,peters2023learning} have investigated multi-agent interaction and planning problems with dynamic game techniques. A dynamic Nash game is solved in \cite{fisac2019hierarchical,wang2019game,fridovich2020efficient,wang2021game} to model the interaction among agents. To model the interaction more precisely, recent works \cite{peters2021inferring,peters2023ijrr,peters2023learning,li2023cost} solve an inverse problem to infer parameters of the game and estimated agents' trajectories from noisy observations. These methods work well when visibility is not a problem. However, dealing with occlusions, which is a common issue in the real world, has been largely overlooked.

To address this, we introduce a novel inverse dynamic game technique.
This approach identifies the unknown parameters in each agent's cost function and estimates the trajectories of both \textit{occluded} and \textit{visible} agents in a Nash game based on realistic sensor measurements of only visible agents. We evaluate our method in various scenarios with different levels of observation noise and incorporate it into a receding horizon planning pipeline for robot navigation and autonomous driving tasks. Results reveal that our approach is robust to observation noise and provides accurate trajectory predictions for both visible and occluded agents, which ultimately leads to safer navigation decisions than the existing baseline methods.

\section{Related Work}

\subsection{Planning-based Agent Behavior Modeling}\label{planning_intro}
We begin by introducing planning-based agent behavior modeling methods. These approaches assume that agents make rational decisions about their trajectories, which follow specific rules. Inferring these rules is the key to behavior modeling tasks. Inverse optimal control (IOC) \cite{kalman1963linear,maroger2020walking,albrecht2011imitating, maroger2022inverse} and inverse reinforcement learning (IRL)\cite{ng2000algorithms,ziebart2009planning,kitani2012activity,monfort2015intent,previtali2016predicting} are two popular frameworks to this end. These approaches focus on recovering the rules that underlie agent-environment interactions and use these rules to predict future behavior. Recent multi-agent IRL techniques \cite{henry2010learning,pfeiffer2016predicting,kim2016socially,kretzschmar2016socially,morales2018towards,sun2019behavior} explore rules governing agents' social behaviors and have applications in robot navigation \cite{kim2016socially,kretzschmar2016socially} and autonomous driving \cite{morales2018towards,sun2019behavior} scenarios.

\subsection{Dynamic Games and Inverse Dynamic Games}\label{game_intro}
Our work builds upon dynamic game theory\cite{bacsar1998dynamic}, which generalizes single-agent optimal control techniques to model agent interactions in multi-agent scenarios. Such techniques \cite{fisac2019hierarchical,wang2019game,fridovich2020efficient,wang2021game} model agents' interaction in a given dynamic game where rational agents minimize their costs through Nash equilibrium solutions. Recent efforts \cite{peters2021inferring,peters2023ijrr,li2023cost,peters2023learning} aim to model agent behaviors more precisely by introducing inverse dynamic games. These techniques jointly optimize agent objectives and trajectory estimates by coupling them through Nash equilibrium constraints, and use noisy, partial state observations to derive the game model that best explains agents' behavior.

Like the planning-based approaches discussed in Section \ref{planning_intro}, these methods establish a causal relationship between agents' decision-making and their behaviors. Therefore, they can be directly applied to robot navigation and autonomous driving tasks, which motivates our work. However, it is crucial to note that these approaches share a common limitation: they require observations from all agents and do not account for occluded agents, which leads to imprecise estimations and suboptimal navigation outcomes in the presence of occlusions.

\subsection{Social Occlusion Inference}\label{soi_intro}
To address occlusions, many works have explored inference methods based on observations of visible agents' behavior in social settings. These works leverage the fact that an individual's behavior is significantly influenced by the surrounding environment. Representative works often employ computer vision techniques \cite{hara2018recognizing,hara2020predicting} and occupancy grid map generating techniques \cite{afolabi2018people,itkina2022multi,mun2023occlusion} to make inferences about the occluded area. 

While these methods primarily focus on determining whether the occluded area is occupied or if an agent is emerging from it, they neither provide precise information about agents' behavior nor the direct control policies \cite{mun2023occlusion}, which planning-based methods offer.

To overcome the shortcomings of both categories of methods, we propose a novel inverse dynamic game technique to identify the unknown weighting parameters in each agent's cost function and simultaneously estimate both visible and occluded agents' trajectories that best explain the observations of visible agents' trajectories. Our proposed method achieves 1) occlusion-aware inference and 2) precise behavior estimation. We also develop a receding horizon planning pipeline that seamlessly integrates our inference method into navigation and autonomous driving tasks.

\section{Preliminaries}\label{Preliminaries}
\begin{table}[b]
\caption{Main Symbols and Notations\label{tab:table1}}
\centering
\begin{tabular}{l|l}
\hline
${J^i}$ & Cost function (Objective) of the ${i^{\text{th}}}$ agent\\
${M\in \mathbb{N}^*}$ & Number of agents in the game\\
${\theta=(\theta^1,\dots,\theta^M)}$ & Tuple of all agents' weighting parameters\\
${T\in \mathbb{N}^*}$ & Time horizon of the game\\
${x_t^i\in\mathbb{R}^n}$ & ${i^{\text{th}}}$ agent's state at time ${t}$\\
${\textbf{x}^i:=(x_1^i,\dots,x_T^i)}$ & ${i^{\text{th}}}$ agent's trajectory\\
${\textbf{x}_t:=(x_t^1,\dots,x_t^M)}$ & All agents' state at time ${t}$\\
${\textbf{x}_{t|k}:=(x_{t|k}^1,\dots,x_{t|k}^M})$ & ${\textbf{x}_t}$ with initial state $\textbf{x}_k$\\
${\textbf{x}:=(\textbf{x}^1,\dots,\textbf{x}^M)}$ & Tuple of all agents' trajectories\\
${u_t^i\in\mathbb{R}^m}$ & ${i^{\text{th}}}$ agent's control input at time ${t}$\\
${u_{t|k}^i\in\mathbb{R}^m}$ & ${u_t^i}$ planned at time ${k}$\\
${\textbf{u}^i:=(u_1^i,\dots,u_T^i)}$ & ${i^{\text{th}}}$ agent's control sequence\\
${\textbf{u}_t:=(u_t^1,\dots,u_t^M)}$ & All agents' control input at time ${t}$\\
${\textbf{u}:=(\textbf{u}^1,\dots,\textbf{u}^M)}$ & Tuple of all agents' control sequences\\
${\textbf{u}^{-i}}$ & ${\textbf{u}\setminus\textbf{u}^i}$ \\
% $\Gamma(\theta,\textbf{x}_1,f)$ & Dynamic game featured by $\theta,\textbf{x}_1,f$\\
% $\Gamma_k(\theta,\textbf{x}_k,f)$ & ${\Gamma}$ at time $k$ in receding horizon games\\
${\mathcal{L}^i}$ & Lagrangian of the ${i^{\text{th}}}$ agent\\
${\lambda_t^i\in\mathbb{R}^n}$ & ${i^{\text{th}}}$ agent's Lagrange multiplier at time ${t}$\\
${\pmb{\lambda}^i:=(\lambda_1^i,\dots,\lambda_T^i)}$ & Sequence of ${\lambda_t^i}$ of the ${i^{\text{th}}}$ agent\\
${\pmb{\lambda}_t:=(\lambda_t^1,\dots,\lambda_t^M)}$ & Tuple of ${\lambda_t^i}$ of all agents at time ${t}$\\
${\pmb{\lambda}:=(\pmb{\lambda}^1,\dots,\pmb{\lambda}^M)}$ & Tuple of sequences of ${\lambda_t^i}$ of all agents\\
${\mathcal{V}}$ & Set of visible agents in the game\\
${\mathcal{O}}$ & Set of occluded agents in the game\\
\hline
\end{tabular}
\end{table}
A discrete time open-loop Nash game with $M$ agents is characterized by the state ${x_t^i\in\mathbb{R}^n}$ and control inputs ${u_t^i\in\mathbb{R}^m}$, ${i\in\{1,\dots,M\}}$, at time step $t$. The dynamics ${\textbf{x}_{t+1}=f(\textbf{x}_t,\textbf{u}_t)}$ govern each agent's control input and state transition from time $t$ to $t+1$. ${J^i:=\sum_{t=1}^T g_t^i(\textbf{x}_t,\textbf{u}_t)}$ defines the ${i^{\text{th}}}$ agent's cumulative cost over a planning horizon $T$. The game is thus fully characterized by the tuple of all agents' cost functions $\{J^i\}_{i=1}^M$, the initial condition ${\textbf{x}_1}$, and the dynamics $f$, and is denoted by ${\Gamma:=(\{J^i\}_{i=1}^M,\textbf{x}_1,f)}$.
In this Nash game, each agent aims to minimize its cost function, adhering to dynamic feasibility constraints, i.e.,
\begin{subequations}
    \begin{align}
    \min_{\textbf{x},\textbf{u}^i}\quad&J^i(\textbf{u};\textbf{x}_1), \quad i\in\{1,\dots,M\},\label{costfunc}\\
    \text{s.t.}\quad &\textbf{x}_{t+1}=f(\textbf{x}_t,\textbf{u}_t), \quad t\in\{0,\dots,T-1\}.\label{dynamics}
    \end{align}
    \label{nashoptimize}%
\end{subequations}
Given that each agent decides its control input rationally at equilibrium, the following inequalities are satisfied:
\begin{equation}
\label{nasheq}
    J^i(\textbf{u}^*;\textbf{x}_1)\leq J^i(\textbf{u}^i,\textbf{u}^{-i*};\textbf{x}_1),\quad i\in\{1,\dots,M\},
\end{equation}
and ${\textbf{u}^*:=(\textbf{u}^{1*},\dots,\textbf{u}^{M*})}$ is called a Nash strategy with ${\textbf{x}^*:=(\textbf{x}^{1*},\dots,\textbf{x}^{M*})}$ the corresponding open-loop Nash equilibrium (OLNE) Nash trajectory. Equation \eqref{nasheq} reveals the fact that no agents will decrease cost by unilaterally deviating from Nash strategy ${\textbf{u}^{i*}}$\cite{bacsar1998dynamic}. To make these concepts concrete, we introduce the following example:\\
\textbf{Running Example}: Consider a scenario with $M=2$ pedestrians walking towards their goals while avoiding each other. ${\textbf{x}_t:=(x_t^1,x_t^2)}$ denotes the position of both pedestrians at time $t$, and they follow single-integrator dynamics at at discrete time step $\Delta t$, i.e.,
\begin{equation}
    \begin{aligned}
    x_{t+1}^i&=\begin{bmatrix}
            p_{x,t+1}^i\\
            p_{y,t+1}^i
        \end{bmatrix}=\begin{bmatrix}
            p_{x,t}^i+v_{x,t}^i\Delta t\\
            p_{y,t}^i+v_{y,t}^i\Delta t
        \end{bmatrix}\\
    &=x_t^i + u_t^i\Delta t,\quad t\in\{0,\dots,T-1\},i\in\{1,2\}.
    \end{aligned}
\end{equation}
where ${u_t^i=[v_{x,t}^i,v_{y,t}^i]^\top}$ denotes each pedestrian's velocity and ${\textbf{x}_1}$ is their initial state. Each pedestrian's objective is the sum of the running cost ${g_t^i}$ over time, where ${g_t^i}$ is characterized by different features with non-negative weighting parameters ${\theta^i}$:
\begin{equation}
    g_t^i=\sum_{j=1}^n\theta_j^ig_{j,t}^i\left\{\begin{aligned}
    &g_{1,t}^i=\|x_t^i-x_g^i\|_2^2\\
    &g_{2,t}^i=-\sum_{j\neq i}\ln(\|x_t^i-x_t^{j}\|_2^2)\\
    &g_{3,t}^i=\|u_t^i\|_2^2
    \end{aligned}\right.,
    \label{running cost}
\end{equation}
These objectives guide both pedestrians towards their destinations (${g_{1,t}^i}$) and to keep away from the other agents (${g_{2,t}^i}$) and minimizing energy expenditure (${g_{3,t}^i}$). In practice, ${g_t^i}$ can be readily modified to accommodate different scenarios.

\section{Occluded Agent Behavior Inference}
In this work, we introduce two roles in the dynamic game with occlusion: the participants and the observer. The participants compete within the game while the observers, such as robots or autonomous vehicles, observe agent interactions. All participants are observable to each other, while observers may only see visible, i.e., unoccluded, participants. The observer's goal is to estimate both \textit{visible} and \textit{occluded} agents' objectives and trajectories based solely on observations of visible agents.

For clarity, we seek to estimate the value of all weighting parameters ${\theta}$ in the running cost \eqref{running cost} as well as the trajectories ${\textbf{x}}$ of all visible and occluded agents, which maximize the likelihood of a given sequence of noise-corrupted state observations ${\textbf{y}^\mathcal{V}:=(\textbf{y}^i),i\in\mathcal{V}}$, i.e.,
\begin{subequations}
\begin{align}
\max_{\theta,\textbf{x},\textbf{u}}\quad&p(\textbf{y}^\mathcal{V}|\textbf{x},\textbf{u}),\label{obj}\\
\text{s.t.}\quad &(\textbf{x},\textbf{u})\text{ is dynamically feasible under } f,\label{inversedynamic}\\
&(\textbf{x},\textbf{u})\text{ is an OLNE solution to }\Gamma(\theta,\textbf{x}_1,f),\label{inverseconstraint}\end{align}
\label{inversedynamicgame}%
\end{subequations}
where ${p(\textbf{y}^\mathcal{V}|\textbf{x},\textbf{u})}$ denotes a likelihood model. Note that this formulation extends that of \cite{peters2021inferring} to deal with scenarios containing occluded agents. In particular, we construct a new objective \eqref{obj} utilizing only the visible agents' trajectory observations and preserve the constraints that strategies of all agents are dynamically feasible \eqref{inversedynamic} and constitute an OLNE of the game \eqref{inverseconstraint}, since visibility to the observer does not interfere with the participants' interaction.

To solve the maximum likelihood problem in \eqref{inversedynamicgame}, we must first solve for an OLNE solution to the dynamic game in \eqref{nasheq}. We construct each agent's Lagrangian with multipliers ${\lambda_t^i}$:
\begin{equation}
    \mathcal{L}^i=J^i+\sum_{t=1}^{T-1}{\lambda_t^i}^\top\left(x_{t+1}^i-f(x_t^i,u_t^i)\right).
\end{equation}
The KKT conditions give the first-order necessary conditions for each agent:
\begin{equation}
\begin{aligned}
\textbf{G}(\textbf{x}^i,\textbf{u}^i,\pmb{\lambda}^i):=\begin{bmatrix}
    \nabla_{\textbf{x}^i}\mathcal{L}^i\\
    \nabla_{\textbf{u}^i}\mathcal{L}^i\\
    x_{t+1}^i-f(x_t^i,u_t^i), 
    \end{bmatrix}=\textbf{0},\\
    t\in\{0,\dots,T-1\}, i\in \{1,\dots,M\}.
\end{aligned}
\label{kkt}
\end{equation}
We thus substitute \eqref{inverseconstraint} and \eqref{inversedynamic} with \eqref{kkt}, and the inverse dynamic game model for occluded agents becomes
\begin{subequations}
\begin{align}
\max_{\theta,\textbf{x},\textbf{u},\pmb{\lambda}}\quad&p(\textbf{y}^\mathcal{V}|\textbf{x},\textbf{u})\label{originobj}\\
\text{s.t.}\quad &\textbf{G}(\textbf{x}^i,\textbf{u}^i,\pmb{\lambda}^i)=\textbf{0}, i\in\{1,\dots,M\}.\label{finalconstraints}
\end{align}
\label{finalinversemodel}%
\end{subequations}
In realistic scenarios, observations often come with noise which is modeled as white Gaussian noise, i.e., $ {\textbf{y}_t^{\mathcal{V}}:=\textbf{x}_t^{\mathcal{V}}+\textbf{n}_t},{\textbf{n}_t\sim\mathcal{N}(\textbf{0},\sigma^2I)}$. To solve \eqref{finalinversemodel}, we substitute the likelihood maximization objective in \eqref{originobj} with the negative log-likelihood minimization objective, i.e.,
\begin{equation}
    \sum_{t\in[T]}\sum_{i\in\mathcal{V}}\|y_t^i-x_t^i\|_2^2.\label{mse}
\end{equation}
By solving \eqref{mse}, we estimate the unknown weighting parameters $\theta$, all agents' trajectories $\textbf{x}$, and control sequences $\textbf{u}$ from the noise-corrupted observation of only visible agents' trajectories $\textbf{y}^{\mathcal{V}}$.

In the next section, we design a receding horizon planning pipeline that utilizes our inference approach to facilitate safe navigation for robots and autonomous vehicles in occluded scenarios.

\section{Inference-based Receding Horizon Planning}

\begin{figure*}[!ht]
\centering
% \subfloat[Ground truth]{\includesvg[inkscapelatex=false,width=0.32\textwidth]{nash_gt.svg}
% \label{demo_gt}}
% \subfloat[Observation for a single experiment]{\includesvg[inkscapelatex=false,width=0.32\textwidth]{nash_obs.svg}
% \label{demo_obs}}
% \subfloat[Estimation]{\includesvg[inkscapelatex=false,width=0.32\textwidth]{nash_est.svg}
% \label{demo_est}}
\subfloat[Ground truth]{\includegraphics[width=0.32\textwidth]{nash_gt.png}
\label{demo_gt}}
\subfloat[Observation for a single experiment]{\includegraphics[width=0.32\textwidth]{nash_obs.png}
\label{demo_obs}}
\subfloat[Estimation]{\includegraphics[width=0.32\textwidth]{nash_est.png}
\label{demo_est}}
\caption{Demonstration for the 2-agent running example in Section \ref{nashexample}, with observation noise standard deviation $\sigma=0.05$\si{m} across 24 different observation sequences, in which both agents apply open-loop Nash strategies. (a)  Ground truth trajectory for both agents. (b)  Observations, where the trajectories of the red agent are corrupted by noise and the blue agent is occluded. (c)  Estimated trajectories by solving the recovered dynamic game. Despite the large observation noise in (b), our method can reliably estimate the trajectories of both agents in (c).}
\label{nashdemo}
\end{figure*}

In this section, we apply our inference approach to navigation and driving scenarios, where two differences must be considered: 1) robots and autonomous vehicles are now participants in the dynamic game rather than just observers; 2) the interaction among agents evolves with time in a receding horizon.

To cope with the change in the robot's role and interaction, we assign the robot the role of the ${(M+1)^{\text{th}}}$ agent of the game with cost function $J^{M+1}$, and design a receding horizon Nash game:
\begin{subequations}
    \begin{align}
    \min_{\textbf{x},\textbf{u}^i}\quad&J^i(\textbf{u};\textbf{x}_k),\quad i\in\{1,\dots,M+1\},\label{receding costfunc}\\
    \text{s.t.}\quad &\textbf{x}_{k+t|k}=f(\textbf{x}_{k+t-1|k},\textbf{u}_{k+t-1|k}),\quad t\in\{1,\dots,T\}.\label{receding dynamics}
    \end{align}
    \label{receding nash}%
\end{subequations}
Here, $J^i(\textbf{u};\textbf{x}_k)$ denotes the ${i^{\text{th}}}$ agent's objective for the game played at time step $k$ and the corresponding decision variables are $\textbf{x}_{k+1|k},\dots,\textbf{x}_{k+T|k},u_{k|k}^i,\dots,u_{k+T-1|k}^i$. We denote the game by ${\Gamma_k(\theta,\textbf{x}_k,f):=(\{J^i\}_{i=1}^M,\textbf{x}_k,f)}$ and the entire receding horizon Nash game by $\Gamma_{\text{RH}}(\theta):=\{\Gamma_k(\theta,\textbf{x}_k,f)\}_{k=1}^\infty$. At time step $k$, the $i^{\text{th}}$ agent derives an open-loop Nash strategy $(u_{k|k}^{i*},\dots,u_{k+T-1|k}^{i*})$ of ${\Gamma_k(\theta,\textbf{x}_k,f)}$. It then applies $u_{k|k}^{i*}$ and repeats the process at future time steps.

We summarize this receding horizon planning pipeline in Algorithm \ref{algorithm}: At each time $k$, the robot estimates the parameters from the previous observations $\textbf{y}_1^{\mathcal{V}},\dots,\textbf{y}_{k-1}^{\mathcal{V}}$ and the new observation $\textbf{y}_k^{\mathcal{V}}$. It simultaneously derives an open-loop Nash strategy $\textbf{u}_{k|k}^*,\dots,\textbf{u}_{k+T-1|k}^*$ by solving the maximum likelihood problem, which is now denoted by
\begin{subequations}
\begin{align}
\max_{\theta,\textbf{x},\textbf{u}}\quad&p(\{\textbf{y}_t^\mathcal{V}\}_{t=1}^k|\textbf{x},\textbf{u}),\label{newobj}\\
\text{s.t.}\quad &(\textbf{x},\textbf{u})\text{ is dynamically feasible under } f,\label{inverserecedingdynamic}\\
&(\textbf{x},\textbf{u})\text{ is an OLNE solution to }\Gamma_k(\theta,\textbf{x}_k,f),\label{inverserecedingconstraint}\end{align}
\label{inverserecedinggame}%
\end{subequations}
Here $\theta$, $\textbf{x}:=\{\textbf{x}_1,\dots,\textbf{x}_k,\textbf{x}_{k+1|k},\dots,\textbf{x}_{k+T|k}\}$, and $\textbf{u}:=\{\textbf{u}_1,\dots,\textbf{u}_{k-1},\textbf{u}_{k|k},\dots,\textbf{u}_{k+1|k}\}$ are the new decision variables. The resulting $u_{k|k}^{(M+1)*}$ is thus the robot's optimal planning strategy of the receding horizon game at time step $k$, and the pipeline is repeated for future planning.

In the next section, we conduct simulation experiments to evaluate the estimation performance of our method under observation noise and the receding horizon planning pipeline's contribution to safer decision-making in driving scenarios.

\section{Simulation Experiments}

\begin{algorithm}[t]
\caption{Planning in receding horizon games}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\REQUIRE receding horizon game $\Gamma_{\text{RH}}(\theta)$, trajectory observation $\{\textbf{y}_k^\mathcal{V}\}_{k=1}^\infty$, game horizon $T$
\ENSURE  robot's control sequence in receding horizon\\ ${\textbf{u}^{M+1}=\{u_{k|k}^{M+1}\}_{k=1}^\infty}$
\FOR {$k=1$ to $\infty$}
\STATE $\theta_{\text{est}},\textbf{x}_1,\dots,\textbf{x}_k,\textbf{x}_{k+1|k}^*,\dots,\textbf{x}_{k+T|k}^*$\\
$\leftarrow$ solve the maximum likelihood problem given $(\Gamma_k(\theta,\textbf{x}_k,f),\textbf{y}_{1}^\mathcal{V},\dots,\textbf{y}_{k}^\mathcal{V})$ in \eqref{inverserecedinggame}.
\RETURN $u_{t|t}^{(M+1)*}$
\ENDFOR
\end{algorithmic}
\label{algorithm}
\end{algorithm}

\subsection{Experiment Implementation \& Setup}
We implement our proposed approach in YALMIP \cite{lofberg2004yalmip}, a MATLAB interface for mathematical programming, and use the open-source COIN-OR IPOPT algorithm \cite{wachter2006implementation} as a low-level solver.

We conducted a sequence of Monte Carlo studies to evaluate our method's performance at different levels of noise corruption. We fix the game model's weighting parameters ${\theta}$ for each scenario and derive the corresponding OLNE trajectories. Occluded agents' trajectories are unavailable for observation, and the visible agents' trajectories are corrupted with white Gaussian noise. We generate 24 sets of random observation sequences at 21 different noise levels. In every single sequence, we run our method to recover estimates of weighting parameter ${\theta}$ and trajectories $\textbf{x}$ of all agents.
\subsection{Evaluation Metrics}
We evaluate our method using the following metrics:\\
\textbf{Weighting Parameter Estimation Precision}: We calculate the cosine dissimilarity \cite{peters2021inferring}, denoted as
\begin{equation}
    D(\theta_{\text{GT}},\theta_{\text{est}}):=1-\frac{1}{M}\sum_{i=1}^M\frac{{\theta_{\text{GT}}^i}^\top\theta_{\text{est}}^i}{\|\theta_{\text{GT}}^i\|_2\|\theta_{\text{est}}^i\|_2},
\label{parametererror}
\end{equation}
which quantifies the precision of estimated weighting parameters $\theta_{\text{est}}$ in comparison to the ground truth $\theta_{\text{GT}}$.\\
\textbf{Trajectory Estimation Accuracy}: We compute the average displacement error (ADE) between the estimated trajectories based on recovered parameters and the ground truth:
\begin{equation}
\begin{aligned}
\text{ADE}_{\text{visible}}&:=\frac{1}{|\mathcal{V}|\cdot T}\sum_{i\in\mathcal{V}}\sum_{t\in\{1,\dots,T\}}\|x_{\text{GT},t}^i-x_{\text{est},t}^i\|_2,\\
\text{ADE}_{\text{occluded}}&:=\frac{1}{|\mathcal{O}|\cdot T}\sum_{i\in\mathcal{O}}\sum_{t\in\{1,\dots,T\}}\|x_{\text{GT},t}^i-x_{\text{est},t}^i\|_2,
\end{aligned}
\label{positionerror}
\end{equation}
where ${x_{\text{GT},t}^i}$ and ${x_{\text{est},t}^i}$ denote the ground truth state and the estimated position of the $i^{\text{th}}$ agent at time step $t$, respectively. We measure the trajectory estimation error separately for visible and occluded agents rather than the total error as \cite{peters2021inferring} did. Results reveal that there can be a significant difference in estimation performance between the two categories.

\subsection{Simulation Results and Analysis}

\subsubsection{Running example}\label{nashexample}
We evaluate our method in the two-agent collision avoidance scenario described in Section \ref{Preliminaries}, where one agent is visible and the other is occluded.

The ground truth trajectory is demonstrated in Figure \ref{demo_gt}. However, the observer can only observe the noise-corrupted trajectory of the red agent (Figure \ref{demo_obs}). Our method enables the observer to estimate the objective and trajectory of both agents (Figure \ref{demo_est}). Figures \ref{2person_para} and \ref{2person_pos} display the parameter and trajectory estimation error of both agents at different levels of observation noise. Results reveal that our method provides accurate and robust parameter and trajectory estimates, with the estimate of the visible agent being more accurate than that of the occluded agent because the observation is available.

\begin{figure}[!t]
\centering
% \subfloat[]{\includesvg[inkscapelatex=false,width=0.475\textwidth]{nash_para.svg}
\subfloat[]{\includegraphics[width=0.475\textwidth]{nash_para.png}
\label{2person_para}}

% \subfloat[]{\includesvg[inkscapelatex=false,width=0.475\textwidth]{nash_pos.svg}
\subfloat[]{\includegraphics[width=0.475\textwidth]{nash_pos.png}
\label{2person_pos}}
\caption{Estimation performance for our method in Section \ref{nashexample} at different levels of observation noise. (a) Parameter estimation dissimilarity. (b) Trajectory estimation error of both agents, with visible in blue and occluded in green. In each figure, we plot the median dissimilarity \eqref{parametererror}, trajectory error \eqref{positionerror}, and their Interquartile Range (IQR) over 24 samples at each level of observation noise.
}
\label{resultstat}
\end{figure}

\begin{figure*}[!ht]
\centering
% \includesvg[inkscapelatex=false,width=0.99\textwidth]{traffic_demo_8_in_1.svg}
\includegraphics[width=0.99\textwidth]{traffic_demo_8_in_1.png}
\caption{The traffic scenario, in which the red and green vehicles drive along their lanes and encounter the blue pedestrian running across the street. (b) Being occlusion-aware, the green vehicle observes the red vehicle's deceleration and infers that an agent is coming out from the occluded area. It then plans to brake to avoid a collision even if the blue pedestrian is still out of sight. (c) The blue pedestrian is not in sight, and the green vehicle's speed is slow enough for collision avoidance. (d) The collision is avoided due to our occlusion-aware planning pipeline. (f) Being occlusion-unaware, the green vehicle maintains a high speed. (h) The blue pedestrian suddenly comes into view, forcing the green vehicle to brake in an emergency. (h) The green vehicle fails to avoid collision due to a late reaction to the occluded pedestrian.
}
\label{trafficdemo}
\end{figure*}

\subsubsection{Planning under occlusions in traffic}\label{trafficexample}
Next, we consider a more realistic road crossing scenario, depicted in Figure \ref{traffic_scene}. The running cost of each agent is formulated as follows:
\begin{equation}
    g_t^i=\sum_{j=1}^n\theta_j^ig_{j,t}^i\left\{\begin{aligned}
    &g_{1,t}^i=-\sum_{j\neq i}\ln(\|x_t^i-x_t^{j}\|_2^2)\\
    &g_{2,t}^i=\|x_t^i-x_{g,t}^i\|_2^2\\
    &g_{3,t}^i=(d_t^i)^2\\
    &g_{4,t}^i=\|u_t^i\|_2^2
    \end{aligned}\right.,
\end{equation}
Here $d_t^i$ denotes the distance between the $i^{\text{th}}$ agent and the center line of the lane it belongs to, and the goal of each agent $x_{g,t}^i$ evolves with time. In this scenario, each agent not only tries to keep a safe distance from other agents ($g_{1,t}^i$) while moving forward ($g_{2,t}^i$), but also seeks to keep itself in the lane according to traffic rules ($g_{3,t}^i$) without heavy energy consumption ($g_{4,t}^i$). We then state our planning pipeline and compare it with the following baseline:

\textit{Our pipeline}: Equipped with the occluded agent inference method, the autonomous vehicle employs proactive reasoning. It initiates inference about the occluded agent's behavior from the outset and reacts accordingly with a Nash strategy according to Algorithm \ref{algorithm}.

\textit{Baseline}: When the occluded agent is not in sight, the autonomous vehicle maintains a constant speed moving forward. Once the occluded agent becomes visible, the autonomous vehicle initiates the receding horizon game-solving process by solving \eqref{receding nash} to determine a Nash strategy.

Note that the primary difference between our approach and the baseline is the timing of when the autonomous vehicle begins to consider the occluded agent during navigation. Our approach instantly responds to observed deceleration by the visible agent and takes preemptive braking measures to avoid potential collisions. In contrast, the baseline strategy delays taking any evasive actions until the occluded agent becomes fully visible.

Figure \ref{trafficdemo} demonstrates how the initial scenario (Figure \ref{trafficdemo}a) evolves as the green vehicle plans with our approach (Figures \ref{trafficdemo}b-\ref{trafficdemo}d) and the baseline (Figures \ref{trafficdemo}f-\ref{trafficdemo}h). At $t=4$ (Figure \ref{trafficdemo}b), the green autonomous vehicle that is occlusion-aware observes the red vehicle's deceleration. It infers the presence of the occluded pedestrian and predicts its trajectory, leading to a preemptive braking maneuver. At $t=5$ (Figure \ref{trafficdemo}c), the green vehicle can see the blue pedestrian, and is able to avoid the potential collision (Figure \ref{trafficdemo}d) thanks to the inference of the blue agent's trajectory.

Conversely, at $t=4$ (Figure \ref{trafficdemo}f), the green vehicle that is occlusion-unaware maintains a high speed and overtakes the red vehicle. It does not notice the blue pedestrian until $t=5$ (Figure \ref{trafficdemo}g), which is too late to decelerate and avoid collision (Figure \ref{trafficdemo}h).

These experiments indicate that our planning pipeline, which integrates occluded agent inference and proactive decision-making, outperforms the baseline strategy that lacks occlusion awareness and underscores our approach's potential to achieve safer navigation outcomes.

\section{Conclusion \& Future Work}
In this work, we introduced a novel inverse dynamic game approach to infer the behavior of occluded agents from noise-corrupted observations of only visible agents. Our method recovers parameters of an underlying game model that best explains the observed trajectories and simultaneously computes open-loop Nash trajectories for both visible and occluded agents. 

We seamlessly integrated our inference method into a receding horizon planning pipeline, demonstrating its practical application in simulated scenarios. Experiment results underscore the robustness of our approach in providing accurate estimates of the game model and agents' trajectories. Our planning pipeline outperforms an occlusion-unaware baseline method, revealing its potential for proactively avoiding collisions with occluded agents during navigation.

While our evaluation was conducted in simulated environments, our future research will focus on extending our approach to incorporate real sensor data, such as lidar and camera observations, for deployment in realistic urban traffic scenarios. This evolution promises to enhance the feasibility of our method in real-world autonomous navigation and robotic systems.

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv, reference.bib}
\end{document}
