% \subsection{Main Symbols and Notations}
\begin{table}[H]
\caption{Main Symbols and Notations\label{tab:table1}}
\centering
\begin{tabular}{l|l}
\hline
${J^i}$ & Cost function (objective) of the ${i^{\text{th}}}$ agent\\
${M\in \mathbb{N}^*}$ & Number of agents in the game\\
${\textbf{w}^i=(w_1^i,w_2^i,\dots)}$ & Weighting parameters of the ${i^{\text{th}}}$ agent\\
${\textbf{w}=(\textbf{w}^1,\dots,\textbf{w}^M)}$ & Tuple of all agents' weighting parameters\\
${T\in \mathbb{N}^*}$ & Planning horizon of the game\\
${K\in \mathbb{N}^*}$ & Observation interval of the game\\
$\Delta$ & Time interval between two adjacent steps\\
${x_k^i\in\mathbb{R}^n}$ & ${i^{\text{th}}}$ agent's state at time ${k}$\\
${\textbf{x}^i:=(x_1^i,\dots,x_T^i)}$ & ${i^{\text{th}}}$ agent's trajectory\\
${\textbf{x}_k:=(x_k^1,\dots,x_k^M)}$ & All agents' state at time ${k}$\\
% ${\textbf{x}_{t|k}:=(x_{t|k}^1,\dots,x_{t|k}^M})$ & ${\textbf{x}_t}$ planned at state $\textbf{x}_k$\\
${\textbf{x}:=(\textbf{x}^1,\dots,\textbf{x}^M)}$ & Tuple of all agents' trajectories\\
${u_k^i\in\mathbb{R}^m}$ & ${i^{\text{th}}}$ agent's control input at time ${k}$\\
% ${u_{t|k}^i\in\mathbb{R}^m}$ & ${u_t^i}$ planned at time ${k}$\\
${\textbf{u}^i:=(u_1^i,\dots,u_T^i)}$ & ${i^{\text{th}}}$ agent's control sequence\\
${\textbf{u}_k:=(u_k^1,\dots,u_k^M)}$ & All agents' control input at time ${k}$\\
${\textbf{u}:=(\textbf{u}^1,\dots,\textbf{u}^M)}$ & Tuple of all agents' control sequences\\
% ${\textbf{u}^{-i}}$ & ${\textbf{u}\setminus\textbf{u}^i}$ \\
% $\Gamma(\theta,\textbf{x}_1,f)$ & Dynamic game featured by $\theta,\textbf{x}_1,f$\\
% $\Gamma_k(\theta,\textbf{x}_k,f)$ & ${\Gamma}$ at time $k$ in receding horizon games\\
${\mathcal{L}^i}$ & Lagrangian of the ${i^{\text{th}}}$ agent\\
${\lambda_k^i\in\mathbb{R}^n}$ & ${i^{\text{th}}}$ agent's Lagrange multiplier at time ${k}$\\
${\pmb{\lambda}^i:=(\lambda_1^i,\dots,\lambda_T^i)}$ & Sequence of ${\lambda_k^i}$ of the ${i^{\text{th}}}$ agent\\
% ${\pmb{\lambda}_k:=(\lambda_k^1,\dots,\lambda_k^M)}$ & Tuple of ${\lambda_k^i}$ of all agents at time ${k}$\\
% ${\pmb{\lambda}:=(\pmb{\lambda}^1,\dots,\pmb{\lambda}^M)}$ & Tuple of sequences of ${\lambda_k^i}$ of all agents\\
${\mathcal{V}}\subseteq[M]$ & Set of visible agents in the game\\
${\mathcal{V}_i}\subseteq[M]$ & Set of agents visible to the $i^{\text{th}}$ agent\\
${\mathcal{O}}:=[M]\setminus\mathcal{V}$ & Set of occluded agents in the game\\
$\theta\in\Theta$ & Set of hypotheses $\theta$ in a contingency game\\
\hline
\end{tabular}
\end{table}

% \subsection{Nash Games and Nash Equilibrium}
A discrete time open-loop Nash game with $M$ agents is characterized by state ${x_k^i\in\mathbb{R}^n}$ and control inputs ${u_k^i\in\mathbb{R}^m}$, ${i\in \{0,\cdots,M-1\} \equiv [M]}$,
% \footnote{Given $M$ a positive integer, we define $[M]:=\{0,\cdots,M-1\}$.}
at time step $k$. The dynamics ${x_{k+1}^i=f(x_k^i,u_k^i)}$ govern each agent's state transition from time $k$ to $k+1$. ${J^i(\textbf{x},\textbf{u};\textbf{w}^i):=\sum_{k=1}^K g_k^i(\textbf{x}_k,\textbf{u}_k;\textbf{w}^i)}$ defines the ${i^{\text{th}}}$ agent's weighted cumulative cost over a planning horizon $T$, and is parameterized by vector $\textbf{w}^i$. The game is thus fully characterized by the tuple of all agents' cost functions $\{J^i(\textbf{x},\textbf{u};\textbf{w}^i)\}_{i=1}^M$, the initial states ${\textbf{x}_0}$, and the dynamics $f$, and is denoted by ${\Gamma(\textbf{w},\textbf{x}_0,f)}$.
In this Nash game, each agent $i$ aims to minimize its cost function while satisfying dynamic feasibility constraints, i.e.
\begin{subequations}
    \begin{align}
    \min_{\textbf{x}^i,\textbf{u}^i}\ &J^i(\textbf{x},\textbf{u};\textbf{w}^i) %, \quad i\in[M],
    \label{eqn:nashobj}\\
    \text{s.t.}\ &x_{k+1}^i=f(x_k^i,u_k^i), \quad k\in[T].\label{eqn:nashdynamics}
    \end{align}
    \label{eqn:nashgame}%
\end{subequations}
\textbf{Open-Loop Nash Equilibrium}: If the inequalities
\begin{equation}
\label{nasheq}
    J^i(\textbf{x}^*,\textbf{u}^*;\textbf{w}^i)\leq J^i(\textbf{x}^i,\textbf{x}^{-i*},\textbf{u}^i,\textbf{u}^{-i*};\textbf{w}^i),\quad i\in[M],
\end{equation}
are satisfied for all $\textbf{x}^i, \textbf{u}^i$ that remain feasible with respect to \eqref{eqn:nashdynamics}, then $\textbf{u}^{i*}:=\{u_0^{i*},\cdots,u_{T-1}^{i*}\}$ is called a Nash strategy with ${\textbf{x}^{i*}:=\{x_1^{i*},\cdots,x_{T}^{i*}\}}$ being the corresponding open-loop Nash equilibrium (OLNE) trajectory. This inequality indicates that no agent can reduce their cost by unilaterally deviating from Nash strategy ${\textbf{u}^{i*}}$\cite{bacsar1998dynamic}.

We use the following example to concretize the concept.\\
\textbf{Running Example}: Consider a scenario with $M$ agents moving towards their goals while avoiding each other. At each time $k$, the $i^{\text{th}}$ agent's state $x_k^i$ encodes its position ${p_k^i=[p_{x,k}^i,p_{y,k}^i]^\top\in\mathbb{R}^2}$ and velocity ${v_k^i=[v_{x,k}^i,v_{y,k}^i]^\top\in\mathbb{R}^2}$ and evolves according to double-integrator dynamics, i.e.,
\begin{equation}
    \begin{aligned}
    x_{k+1}^i&=\begin{bmatrix}
            p_{k+1}^i\\
            v_{k+1}^i\\
        \end{bmatrix}=\begin{bmatrix}
            I_{2} & I_{2}\Delta\\
            \textbf{0} & I_{2}
        \end{bmatrix}\cdot \begin{bmatrix}
            p_{k}^i\\
            v_{k}^i\\
        \end{bmatrix}+\begin{bmatrix}
            \textbf{0} \\ 
            I_{2}
        \end{bmatrix}\Delta\cdot a_k^i\\
    &=Ax_k^i + Bu_k^i\Delta,\quad k\in[T],\ i\in[M].
    \end{aligned}
    \label{eqn:double-integrator dynamic}
\end{equation}%
where the control input ${u_k^i=a_k^i=[a_{x,k}^i,a_{y,k}^i]^\top}$ denotes its acceleration and $\Delta$ denotes the time interval between two adjacent steps. Its objective is to minimize the sum of the running cost ${g_k^i}$ over time, where ${g_k^i}$ is characterized by various features with non-negative weighting parameters ${\textbf{w}^i}$:
\begin{equation}
    g_k^i(\textbf{x}_k,\textbf{u}_k;\textbf{w}^i)=\sum_{l=1}^3w_l^ig_{l,k}^i\left\{\begin{aligned}
    &g_{1,k}^i=\|p_k^i-p_g^i\|_2^2\\
    &g_{2,k}^i=\sum_{\substack{j=1\\j\neq i}}^M\frac{1}{\|p_k^i-p_k^{j}\|_2^2}\\
    &g_{3,k}^i=\|u_k^i\|_2^2
    \end{aligned}\right.
    \label{eqn:running cost}
\end{equation}%
This form of objective guides the $i^{\text{th}}$ agent towards its destination $p_g^i$ (${g_{1,k}^i}$) while maintaining a safe distance from the other agents (${g_{2,k}^i}$) with minimal energy expenditure (${g_{3,k}^i}$). In practice, ${g_k^i}$ can be readily modified to accommodate different scenarios.