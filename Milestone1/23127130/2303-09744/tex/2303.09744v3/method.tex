\subsection{Occlusion-Aware Behavior Inference}
\label{sec:occluded_agent_behavior_inference}
We first introduce two roles in a dynamic game with occlusions: the participants and the observer. The participants are visible to and compete with each other, while the observer, such as a robot or an autonomous vehicle, observes the trajectories of visible (unoccluded) participants only. The observer's goal is to estimate both \textit{visible} and \textit{occluded} agents' objectives and trajectories based solely on observations of \textit{visible} agents. To this end, the observer seeks to estimate all agents' weighting parameters ${\textbf{w}}$ in the running cost \eqref{eqn:running cost} and trajectories ${\textbf{x}}$, which maximize the likelihood of the given noise-corrupted partial state observations (position only) from visible agents ${\textbf{y}^\mathcal{V}:=(\textbf{y}^i),i\in\mathcal{V}}$, i.e.,
\begin{subequations}
\begin{align}
\max_{\textbf{w},\textbf{x},\textbf{u}}\ &p(\textbf{y}^\mathcal{V}|\textbf{x},\textbf{u}),\label{eqn:likelihood}\\
\text{s.t.}\ &(\textbf{x},\textbf{u})\text{ OLNE of }\Gamma(\textbf{w},\textbf{x}_0,f),\label{eqn:nasheqconstraint}\end{align}
\label{eqn:MLE model}%
\end{subequations}
where $\mathcal{V}$ denotes the set of visible agents in the game.

We encode the Nash equilibrium condition \eqref{eqn:nasheqconstraint} by concatenating the first-order necessary (KKT) conditions: 
\begin{equation}
\begin{aligned}
\textbf{G}&(\textbf{x},\textbf{u},\pmb{\lambda}^i;\textbf{w}^i)\\&=\begin{bmatrix}
    \nabla_{\textbf{x}^i}\mathcal{L}^i(\textbf{x},\textbf{u},\pmb{\lambda}^i;\textbf{w}^i)\\
    \nabla_{\textbf{u}^i}\mathcal{L}^i(\textbf{x},\textbf{u},\pmb{\lambda}^i;\textbf{w}^i)\\
    x_{k+1}^i-f(x_k^i,u_k^i),\ k\in[T]
    \end{bmatrix}=\textbf{0},\ i\in[M],
\end{aligned}
\label{eqn:kkt}
\end{equation}
where $$\mathcal{L}^i(\textbf{x},\textbf{u},\pmb{\lambda}^i;\textbf{w}^i)=J^i(\textbf{x},\textbf{u};\textbf{w}^i)+\sum_{k=0}^{T-1}{\lambda_k^i}^\top\big(x_{k+1}^i-f(x_k^i,u_k^i)\big)$$ denotes each agent's Lagrangian with multipliers $\pmb{\lambda}^i$. Assuming that the observations are corrupted by white Gaussian noise:
$ {\textbf{y}_t^{\mathcal{V}}:=\begin{bmatrix}
    I_2 & \textbf{0}
\end{bmatrix}\textbf{x}_k^{\mathcal{V}}+\textbf{n}_k}, {\textbf{n}_k\sim\mathcal{N}(\textbf{0},\sigma^2I_2)}$, the maximum likelihood objective in (5a) becomes a least-square objective as follows:
\begin{subequations}
\begin{align}
\min_{\textbf{w},\textbf{x},\textbf{u},\pmb{\lambda}}\ &\frac{1}{|\mathcal{V}|\cdot T}\sum_{k=0}^{T-1}\sum_{j\in\mathcal{V}}\left\|y_k^j-\begin{bmatrix}
        I_2 & \textbf{0}\end{bmatrix}x_k^j\right\|_2^2, \label{eqn:squareerror}\\
\text{s.t.}\ &\textbf{G}(\textbf{x}^i,\textbf{u}^i,\pmb{\lambda}^i;\textbf{w}^i)=\textbf{0},\ i\in[M].\label{eqn:lse_kkt}
\end{align}
\label{eqn:LSE_model}%
\end{subequations}
The estimator then solves \eqref{eqn:LSE_model} to estimate all agents' weighting parameters $\textbf{w}$, trajectories $\textbf{x}$, and control sequences $\textbf{u}$ from the noise-corrupted observation of only visible agents' trajectories $\textbf{y}^{\mathcal{V}}$.

\subsection{Occlusion-Aware Contingency Game}
\label{sec:occlusion-aware contingency game}
Next, we outline the proposed occlusion-aware planning algorithm. A robot may be uncertain about the presence of potentially occluded agents during navigation. To manage this uncertainty, we construct a contingency game model akin to that of \cite{peters2024contingency}, which encodes both possibilities
% hypotheses (occluded agents exist or not) 
with parameters ${\theta\in\Theta=\{\theta_1, \theta_2\}}$ and their corresponding belief likelihood $b(\theta)$, satisfying ${0\leq b(\theta_1)=1-b(\theta_2)\leq1}$.
We denote
\begin{equation}
    \begin{aligned}
    &\theta_1\text{: Occluded agents DO exist},\\
    &\theta_2\text{: Occluded agents DO NOT exist}.
    \end{aligned}
    \label{eqn:hypothesis}%
\end{equation}
A $M$-agent occlusion-aware contingency game is divided into two phases by the branching time $t_b$, after which it is assumed that any occluded agents will be visible.
For each time step $k < t_b$, the $i^\text{th}$ agent must choose a single control input which balances both hypotheses $(\theta_1, \theta_2)$. 
Because the $i^\text{th}$ agent knows that any occluded agents will be visible after $k = t_b$, planned control inputs for times $k \ge t_b$ are conditioned upon the value of $\theta$. The contingency game is thus denoted as $\Gamma_\text{con}(\Theta, t_b, \textbf{w}, \textbf{x}_0, f)$.

To encode this contingency structure, the $i^\text{th}$ agent constructs a larger game with two copies of each other agent: one for each hypothesis $\theta$. In this larger problem, the $j^\text{th}$ agent ($j\neq i$) is assumed to optimize
\begin{subequations}
    \begin{align}
    \min_{\textbf{x}^j_{\theta,i},\textbf{u}^j_{\theta,i}}\quad&J^j_{\theta,i}(\textbf{x},\textbf{u};\textbf{w}^j)%, \quad j\in[M],\ j\neq i,
    \label{eqn:contingency_sub_costfunc}\\
    \text{s.t.}\quad &x_{k+1;\theta,i}^j=f(x_{k;\theta,i}^j,u_{k;\theta,i}^j), \quad k\in[T],\label{eqn:contingency_sub_dynamics}
    \end{align}
    \label{eqn:contingency_sub}%
\end{subequations}
where the $(\theta, i)$ subscripts indicate that this is the $j^\text{th}$ agent associated to observability condition $\theta$ in the $i^\text{th}$ agent's contingency game.
The $i^\text{th}$ agent then wishes to minimize its expected cost under distribution $b(\theta)$:
\begin{subequations}
    \begin{align}
    \min_{\textbf{x}^i_{\theta,i},\textbf{u}^i_{\theta,i}}\quad&\sum_{\theta\in\Theta}b(\theta)J_{\theta,i}^i(\textbf{x},\textbf{u};\textbf{w}^i)\label{eqn:contingency_main_costfunc}\\
    \text{s.t.}\quad &x_{k+1;\theta,i}^i=f(x_{k;\theta,i}^i,u_{k;\theta,i}^i), \ k\in[T],\label{eqn:contingency_main_dynamics}\\
    & u_{k;\theta_1,i}^i=u_{k;\theta_2,i}^i,\ k<t_b.
    \label{eqn:contingnecy_constraint}
    \end{align}
    \label{eqn:contingency_main}%
\end{subequations}
subject to the contingency constraint \eqref{eqn:contingnecy_constraint} that its control inputs in both interactions are identical for both $\theta_1$ and $\theta_2$ when $k<t_b$. When $k\geq t_b$ and the $i^{\text{th}}$ agent is certain about the (non)existence of occluded agents ($b(\theta_1)=1$ or $b(\theta_2)=1$),  it will pick the corresponding control input $u_{k;\theta,i}^i$. 

We use the following example to concretize the concept:\\
\textbf{Running Example}: Consider the perspective of Agent 1 (A1) in a 4-agent scenario illustrated in Figure \ref{fig:contingency_planner}. It plans its strategy in an occlusion-aware contingency game as follows:
\begin{itemize}
    \item $\theta_1$: A1 assumes that there is an occluded A3, resulting a 4-agent interaction (A1, A2, A3, A4).
    \item $\theta_2$: A1 assumes that there are only visible agents A2 and A4, resulting a 3-agent interaction (A1, A2, A4).
\end{itemize}
When $k<t_b$, A1 chooses a single control input $u_k^1$ in \eqref{eqn:contingnecy_constraint} balancing both hypotheses $(\theta_1, \theta_2)$. When $k\geq t_b$, A1 assumes it will know whether the occluded A3 exists or not, and can therefore choose its strategy based on the ground truth value of $\theta$.

\subsection{Receding Horizon Estimation and Planning}
\begin{figure*}[!t]
\centering
\includegraphics[width=0.99\textwidth]{figures/contingency_system_new.png}
\caption{4-agent occlusion-aware contingency game. When $k<\ ${\color{red}$t_b$}, agent $i$ considers two possibilities: {\color{orange}$\theta_1$ (occluded agents are present)} and {\color{NavyBlue}$\theta_2$ (there are no occluded agents)}. It chooses a single control input $u_k^i=\ ${\color{orange}$u_{k;\theta_1,i}^i$}$=\ ${\color{NavyBlue}$u_{k;\theta_2,i}^i$} balancing its uncertainty between both hypotheses $(${\color{orange}$\theta_1$}, {\color{NavyBlue}$\theta_2$}$)$.
% Agent $i$'s objective $J^i_{\Theta}(\textbf{x},\textbf{u};\textbf{w}^i)$ and control inputs {\color{red}$u_t^i$} are identical for both interactions. 
When $k\geq\ ${\color{red}$t_b$}, any occluded agents are visible, and the $i^\text{th}$ agent will pick either {\color{orange}$u_{k;\theta_1,i}^i$} or {\color{NavyBlue}$u_{k;\theta_2,i}^i$} based on the ground truth value of $\theta$, i.e. the (non)existence of the occluded agents, which is assumed to be revealed at {\color{red}$t_b$}.}
\label{fig:contingency_planner}
\vspace{-3ex}
\end{figure*}%
\begin{algorithm}[!b]
\caption{Receding Horizon Estimation and Planning Pipeline in an Occlusion-Aware Contingency Game}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\REQUIRE receding horizon game $\Gamma_{\text{RH}}(\textbf{w},\textbf{x}_0,f)$, receding horizon contingency game $\Gamma_\text{con,RH}(\Theta, t_b, \textbf{w}, \textbf{x}_0, f)$, trajectory observation $\textbf{y}^{\mathcal{V}_i}$, game horizon $T$, observation interval $K$, branching time $t_b$.
\ENSURE  Agent $i$'s control sequence in receding horizon\\ ${\textbf{u}_\text{RH}^i=\{u_{k|k}^{i}\}_{k=K}^\infty}$.
\FOR {$k=K$ to $\infty$}
\STATE {\color{blue}$\hat{\textbf{w}}^{-i},\hat{\textbf{x}}_k^{-i}$}$\leftarrow$ solving the maximum likelihood problem given in \eqref{eqn:recedinghorizonmlemodel}. {\color{blue}[Estimation]}
\STATE {\color{red}$u_{k|k}^{i*}$}$\leftarrow$ solving $\Gamma_\text{con,k}(\Theta, t_b, \textbf{w}^i,${\color{blue}$\hat{\textbf{w}}^{-i}$}$, x_k^i,${\color{blue}$\hat{\textbf{x}}_k^{-i}$}$,f)$ given by \eqref{eqn:contingency_sub} and \eqref{eqn:contingency_main}. {\color{red}[Planning]}
\STATE $x_{k+1}^i\leftarrow f(x_k^i,${\color{red} $u_{k|k}^{i*}$}$)$ by state update \eqref{eqn:nashdynamics}.
\ENDFOR
\end{algorithmic}
\label{alg:algorithm}
\end{algorithm}

We now integrate the occlusion-aware game estimator in Section \ref{sec:occluded_agent_behavior_inference} and the occlusion-aware contingency game planner in Section \ref{sec:occlusion-aware contingency game}. This enables the robot to estimate the states of both visible and occluded agents from noisy position observations and then plan its own control input in a receding horizon fashion based on the estimated states.\\ % in the realistic setting.\\

\noindent\textbf{Receding Horizon Game}: We denote an $M$-agent receding horizon game by $\Gamma_{\text{RH}}(\textbf{w},\textbf{x}_0,f):=\{\Gamma_k(\textbf{w},\textbf{x}_k,f)\}_{k=0}^\infty$, consisting of games with initial states $\textbf{x}_k$ over all time  steps $k$. At each step, the $i^{\text{th}}$ agent solves for an OLNE strategy $(u_{k|k}^{i},\dots,u_{k+T-1|k}^{i})$ for ${\Gamma_k(\textbf{w},\textbf{x}_k,f)}$, It then implements the first control input $u_{k|k}^{i}$ and updates its state according to \eqref{eqn:nashdynamics}. The resulting control sequence is denoted ${\textbf{u}_{\text{RH}}^{i}=\{u_{0|0}^{i*},u_{1|1}^{i*},\cdots\}}$. % is the $i^{\text{th}}$. 
% agent's control sequence to $\Gamma_{\text{RH}}(\textbf{w},\textbf{x}_0,f)$. 
We similarly define the receding horizon contingency game as ${\Gamma_\text{con,RH}(\Theta, t_b, \textbf{w}, \textbf{x}_0, f):=\{\Gamma_\text{con,k}(\Theta, t_b, \textbf{w}, \textbf{x}_0, f)\}_{k=0}^\infty}$.\\

\noindent\textbf{Receding Horizon Open-Loop Nash Equilibrium}: We define ${\textbf{u}_{\text{RH}}^{i*}:=\{u_{0|0}^{i*},u_{1|1}^{i*},\cdots\}}$ as the receding horizon Nash strategy for $\Gamma_{\text{RH}}(\textbf{w}, \textbf{x}_0, f)$ if $\forall k, u_{k|k}^{i*}$ is an OLNE strategy to $\Gamma_k(\textbf{w},\textbf{x}_k,f)$, with ${\textbf{x}_{\text{RH}}^{i*}:=\{x_{1|0}^{i*},x_{2|1}^{i*},\cdots\}}$ being the corresponding Receding Horizon Open-Loop Nash Equilibrium (RHOLNE) trajectory of the $i^{\text{th}}$ agent.\\

\noindent\textbf{Receding Horizon Estimation and Planning}: We summarize the receding horizon estimation and planning pipeline in Algorithm \ref{alg:algorithm}. At each time $k$, each agent estimates the parameters and states of the other agents from the previous $K$ observations $\{\textbf{y}_t^{\mathcal{V}_i}\}_{t=k-K}^k$, i.e.,
\begin{subequations}
\begin{align}
\max_{\textbf{w},\textbf{x}_{\text{RH}},\textbf{u}_{\text{RH}}}\ &p(\{\textbf{y}_t^{\mathcal{V}_i}\}_{t=k-K}^k|\textbf{x}_{\text{RH}},\textbf{u}_{\text{RH}}),\label{eqn:recedinghorizonmle}\\
\text{s.t.}\ &(\textbf{x}_{\text{RH}},\textbf{u}_{\text{RH}}) \text{ RHOLNE of }\Gamma_{\text{RH}}(\textbf{w},\textbf{x}_0,f),\label{eqn:recedinghorizonmleconstraint}\end{align}
\label{eqn:recedinghorizonmlemodel}%
\end{subequations}
where $\mathcal{V}_i$ denotes the set of agents that are visible to the $i^{\text{th}}$ agent. The agent then plans its strategy by solving the contingency game ${\Gamma_\text{con,k}(\Theta, t_b, \textbf{w}, \textbf{x}_0, f)}$ based on the estimated states of the other agents and implements the first control input $u_{k|k}^{i*}$.

In the next section, we conduct simulation experiments to evaluate the estimation performance of our method under observation noise and the receding horizon planning pipeline's contribution to safer decision-making in driving scenarios.
