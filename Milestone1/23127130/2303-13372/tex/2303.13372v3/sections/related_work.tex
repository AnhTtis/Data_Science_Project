\label{sec:related_work}

\textbf{ML in Static Malware Detection.} There have been several studies of how malware executables can be classified using machine learning. As early as 2001 \citet{924286} proposed a data mining technique for malware detection using three different types of static features.
Pioneered by \citet{MalwareImages}, CNN-based techniques for malware detection became popular among security researchers \citep{kalash2018malware, yan2018detecting}. 
Eventually, \citet{raff2018malware} proposed a static classifier, named MalConv, that takes raw byte sequences and detects malware using a convolution neural network. Surprisingly, MalConv is still considered a state-of-the-art for detection with raw byte inputs, potentially attributing to the issue of limited accessibility to benign executables, which we will discuss later in this section. We will use it as base classifiers in this work.


\textbf{Adversarial Attacks and Defenses in Malware Detection.}
Along with the detection research, there has been plenty of research on adversarial attacks on these models. These attacks fall into different categories. For example, attacks proposed by \citet{kolosnjaji2018malware, kreuk2018deceiving, suciu2019exploring} appended and/or injected adversarial bytes in the malware computed by gradient. \citet{demetrio2019explaining, demetrio2021adversarial, nisi2021lost} proposed attacks that modify or extend DOS and Header fields. \citet{demetrio2021functionality} extracted payloads from benign files to be appended and injected into malware files. Recent work by \citet{lucas2021malware} used two types of code transformation to generate adversarial samples.
%\textbf{Prior Defenses for Adversarial Attacks.} 
For defenses, \citet{fleshman2018non} proposed a defense, MalConv (NonNeg), by constraining weights in the last layer of MalConv to be non-negative. However, this model achieves low accuracy of $88.36\%$, and has been shown to be as vulnerable as MalConv in some cases \citep{wang2023mpass, wang2022black, ceschin2019shallow}. 
Another defense strategy, adversarial training cannot guarantee defense against attacks other than the one used during training, which limits its usage: \citet{lucas2023adversarial} showed training it on Kreuk-0.01 degraded the true positive rates to $84.4\% \sim 90.1\%$.
%However, recent work \citep{lucas2023adversarial} showed that adversarial training on one attack cannot guarantee defense against other attacks which makes it inadequate for real-world use. Also, they showed training it on Kreuk-0.01 dropped the TPR to $84.4\% \sim 90.1\%$. 
Notably, where variants of randomized smoothing schemes have been proposed for vision domains \citep{cohen2019certified, lecuyer2019certified, salman2019provably, levine2020randomized, levine2020robustness}, they remain under-explored in the context of malware detection.


\textbf{Limited Accessibility to Benign Executables.}
Though there have been a large amount of work on malware detection, most of the works were done using private or enterprise dataset with restrictive access. 
%And most of them could not publish the benign raw executables due to copyright issue \citep{lucas2021malware}. 
Prior works \citep{anderson2018ember, yang2021bodmas, downing2021deepreflect} explain the copyright issue and only published the feature vector of benign files (see Table \ref{table:other_dataset}). This impose many constraints to the advancement of malware detection techniques, especially to have a complete model that requires raw executables as inputs.
%To solve this issue, we are going to publish our benign dataset following common standards.