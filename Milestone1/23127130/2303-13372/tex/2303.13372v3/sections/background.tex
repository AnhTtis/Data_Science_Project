\label{sec:backgorund}

We denote the set of all bytes of a file as $X \subseteq [0, N-1]$, where $ N = 256$. 
A binary file is a sequence of k bytes $x = (x_1, x_2, x_3, ... x_k)$, where $ x_i \in X$ for all $1 \leq i \leq k$. Note that the length $k$ varies for different files, thus $k$ is not fixed. However, the input vector fed into the network has to be of a fixed dimension. So, the common approach is to -- pad zeros at the end of $x$ if $k < D$, or extract the first $D$ bytes from $x$, to fix the input length to $D$.


\subsection{Base Classifier}
\label{sec:base_classifier}


In this work, we will be using the state-of-the-art static malware detection model to this date, named MalConv \citep{raff2018malware}, as our base classifier.
While there are other models like Ember, GBDT \citep{anderson2018ember} for malware detection, note that -- these models can work only on feature vectors, and our work focuses on raw binary executables files.
 Let us represent the MalConv model (see Figure \ref{fig:malconv_model}) as $F_{\theta} : X \rightarrow [0,1]$ with a set of parameters $\theta$ that it learns through training. If the output of $F_{\theta}(X)$ is greater than $0.5$ then the prediction is considered as 1 or malicious, and vice versa. We set the input length as $2$MB following the original paper.

%\smk{we can take this para to appendix if needed}\\
MalConv takes in each byte $x_i$ from file $X$ and then passes it to an embedding layer with an embedding matrix $Z \in \mathbb{R}^{D \times 8}$, which generates an embedding vector $z_i = \phi (x_i)$ of 8 elements. This vector is then passed through two convolution layers, using ReLU and sigmoid activation functions. These activation outputs are combined through a gating which performs an element-wise multiplication to mitigate the vanishing gradient problem. The output is then fed into a temporal max pooling layer, followed by a fully connected layer. Finally, a softmax layer calculates the probability of $X$ being a malware or benign file.

\subsection{Threat Model}
\label{sec:threat_model}

Unless otherwise specified, we assume that the attacker has the full knowledge of the victim, including architectures and model parameters. This is typically referred to as the white-box setting. The white-box setting considers potentially strong attackers, which is desired when assessing defenses.

%From a defender's perspective, such a white-box manner is realistic and conservative since -- `security through obscurity' is not a good approach, and it is possible to deploy transfer attack or query-based attack even in the black-box setting \citep{papernot2016transferability, demetrio2021functionality}.

In the primary threat model that we consider when developing our defense, the attacker can modify or add any bytes in a contiguous portion of the input sample in test time to evade the model. So, the goal of the attacker is to generate a perturbation $\delta$ that creates the adversarial malware $x^{'} = x + \delta$, for which $F_{\theta}(x^{'}) < 0.5$, i.e., the classifier predicts it as a benign file. Here, the attacker knows the classifier model $F$ and its parameters $\theta$, and can modify the original malware file $x$. However, finding the perturbation $\delta$ in a binary file is more challenging than vision due to its inherited file structures. For any arbitrary change in a malware file, the file can lose its semantics, i.e. malicious functionality, in the worst case, the file can get corrupted. 
%We assume that the attacker is powerful enough to find such perturbation $\delta$.

Even after such challenges in binary modification, prior attacks have been successful by adding contiguous adversarial bytes in one \citep{kreuk2018deceiving, demetrio2021adversarial} or multiple locations \cite{suciu2019exploring, demetrio2021functionality}, or modifying bytes at specific locations\citep{demetrio2019explaining, nisi2021lost}, to evade a model. 
In this work, we consider not only the former ones, which fall directly within our primary threat model but also the latter ones which do not. 
In addition, we also consider recent, more sophisticated attacks \citep{lucas2021malware, lucas2023adversarial} where the attacker has the power to disassemble malware and apply different code transformations at any place in the file. For coherence, we defer the details about these attacks to Section \ref{sec:emp_robustness}, where we evaluate the empirical robustness of our defenses against them.
%We emphasize that -- though such attacks fall outside of our threat model, we consider them to show that -- the empirical robustness of our model surpasses the theoretical one (see Section \ref{sec:emp_robustness}).