\label{sec:evaluation}

\label{sec:eval}

\subsection{Standard Accuracy}
\label{sec:accuracy}

For evaluation, we compare our DRSM models with 
%its base classifier 
MalConv\citep{raff2018malware} which is still one of the state-of-the-art models for static malware detection. 
%While there are other models like Ember\citep{anderson2018ember}, note that -- this work focuses on raw binary executables files, and such models can work only on feature vectors. 
Moreover, we consider the non-negative weight constraint variant of MalConv which was proposed as a defense against adversarial attack in prior work \citep{fleshman2018non}. We train and evaluate these models on the same train and test set (Section \ref{sec:dataset}).


\begin{table}[h]
  %\centering
  \caption{Standard and Certified Accuracy of Models. MalConv and MalConv(NonNeg) cannot provide certified accuracy}
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{ lccc|ccc }
    \hline
      \multirow{2}{*}{Model}  & \multicolumn{3}{c}{Standard Accuracy (in \%) $\uparrow$} & \multicolumn{3}{c}{Certified Accuracy [$\Delta=2$] (in \%) $\uparrow$}\\\cline{2-4}\cline{5-7}
             & Train-set & Validation-set & Test-set & Train-set & Validation-set & Test-set \\
     \hline
    MalConv  & $99.73$ & $98.87$ & $98.61$ & $-$ & $-$ & $-$\\
    MalConv(NonNeg) & $88.56$ & $87.56$ & $88.36$ & $-$ & $-$ & $-$\\
    DRSM-4   & $99.49$ & $98.12$ & $98.18$ & $14.74$ & $7.84$ & $12.2$\\
    DRSM-8   & $99.67$ & $97.88$ & $97.79$ & $52.74$ & $43.9$ & $40.85$\\
    DRSM-12  & $96.07$ & $95.58$ & $95.88$ & $45.77$ & $44.43$ & $46.21$\\
    DRSM-16  & $94.29$ & $93.00$ & $93.3$ & $59.1$ & $50.52$ & $49.17$\\
    DRSM-20  & $91.17$ & $91.05$ & $91.15$ & $51.64$ & $51.92$ & $52.68$\\
    DRSM-24  & $90.22$ & $89.80$ & $90.24$ & $54.19$ & $54.88$ & $53.97$\\
     \hline
\end{tabular}
}
\end{center}

\label{table:std_acc}
\end{table}

For DRSM-n, we choose $n= \{ 4,8,12,16,20,24 \}$ for our experiments and show the standard accuracy on the left side of the Table \ref{table:std_acc}. 
%Notably, `standard accuracy' is the percentage of files that are correctly classified by a model. 
Recall that -- for DRSM-n, a file is correctly classified if the winning class from majority voting matches the true label for that file (Section \ref{sec:de_random_smoothing}). For ties, we consider `malware' as the winning class. From the Table \ref{table:std_acc}, we can see that -- DRSM-4 ($98.18\%$) and DRSM-8 ($97.79\%$) can achieve comparable accuracy to the MalConv model ($98.61\%$). However, increasing the $n$ has a negative impact on the standard accuracy. For example, DSRM-20 and DSRM-24 achieve $91.15\%$ and $90.24\%$ standard accuracy, respectively. We investigate and find that -- with more ablations (smaller window), the probability of one window containing enough malicious features to make a stable prediction becomes less.
%and thus, the model starts struggling to learn those features. 
On the other hand, the MalConv (NonNeg) model has a lower accuracy, which is consistent with the results by \citet{fleshman2018non}.


\subsection{Certified Accuracy}



Besides standard accuracy, we also evaluate the certified accuracy for DRSM-n models. Recall that -- `certified accuracy' is the percentage of files for which the inequality \ref{eqn:cond_certtify} holds true for DRSM-n models. In short, it denotes the lower bound of model performance even when the attacker can perturb bytes in $\Delta$ number of ablated windows and alter predictions for all of them. 
So, we run experiments on DRSM-n models by varying the $\Delta$ in equation \ref{eqn:cond_certtify}, i.e., perturbation budget for the attacker.
To maintain consistency between standard and certified accuracy, we take `malware' as the winning class for ties by tweaking the first inequality in \ref{eqn:cond_certtify} to $n_m(x) \geq n_b(x) + 2 \Delta$.
%Since in case of equal votes, we take the malware as winning class for standard accuracy, we follow the same rule for certified accuracy by tweaking the first inequality in \ref{eqn:cond_certtify} to $n_m(x) \geq n_b(x) + 2 \Delta$. 

\begin{figure}[tbp]
        \centering
        \includegraphics[width=1.0\linewidth]{images/cert_acc_test.pdf}
        \caption{Certified Accuracy (\%) of DRSM-n models for different perturbation budgets (Test-set). While MalConv and MalConv (NonNeg) are not certifiably robust, their standard accuracy is highlighted for references.}
        \label{fig:cert_acc_test_plot}
\end{figure}

 
%Notably, the range of $\Delta$ starts from $2$ to the point where the inequality \ref{eqn:cond_certtify} becomes impossible to be true, i.e., $\Delta \leq \frac{n}{2}$. 
Notably, $\Delta \in \{ 2, 3, ... , \frac{n}{2} \}$. The range starts from $2$, because any perturbation smaller than the window size can overlap with at most $2$ ablated sequences, and goes up to $\frac{n}{2}$, because the inequality \ref{eqn:cond_certtify} will never hold beyond this point.
The right side of Table \ref{table:std_acc} shows the certified accuracy of DRSM-n models for $\Delta=2$.
In Figure \ref{fig:cert_acc_test_plot}, we show the result of certified accuracy on the test set for all values of $\Delta$, i.e., perturbation budget for the attacker (x-axis). See Tables \ref{table:cert_acc_all} and \ref{table:cert_acc_range} in Appendix \ref{app:results} for more details. We emphasize that even with small $\Delta=2 (=\lceil \frac{255K}{256K} \rceil + 1)$, an attacker can change up to $255K$ bytes for DRSM-8, and yet the model maintains $40.85\%$ certified accuracy.

%Right side of Table \ref{table:std_acc} shows the ranges of certified accuracy for different models. The range starts from $\Delta = 1$ to the point where the inequality \ref{eqn:cond_certtify} becomes impossible to be true, i.e., $\Delta < \frac{n}{2}$. For DRSM-4, there is only one value because $\Delta=\{1\}$, and note that -- even with such small $\Delta$, an attacker can change up to $511K$ bytes and perturb at most $2$ window ablations (when a perturbation overlaps multiple windows) in DRSM-4. For example, in DRSM-16, if the attackers have a perturbation budget up to $200K$ bytes, they can perturb  at most $\Delta = \lceil \frac{p}{w} \rceil + 1 = \lceil \frac{200K}{128K} \rceil + 1 = 3$ window ablations, i.e., alter at most $3$ predictions, and still the model would provide $31.79\%$ certified accuracy. Figure \ref{fig:cert_acc_test_plot} shows the change in certified accuracy for each model with varying the perturbation budget. 

By analyzing Table \ref{table:std_acc}, we can see that $n$ has a positive and negative correlation with certified and standard accuracy, respectively. While DRSM-24 provides the highest certified accuracy ($53.97\%$), it has the lowest standard accuracy ($90.24\%$) among all DRSM-n models. In contrast, DRSM-4 provides the highest standard accuracy ($98.18\%$) with $12.2\%$ certified accuracy. 
%Though the performance seems like a trade-off, models like DRSM-8, DRSM-16 achieves high certified robustness along with considerably good standard accuracy, while prior defense like MalConv (NonNeg) achieves low standard accuracy like $88.36\%$. 
This observation may suggest a performance trade-off.
However, it's worth highlighting that models like DRSM-8 and DRSM-16 strike a balance, delivering robust certified performance alongside commendable standard accuracy, while prior defense MalConv (NonNeg) achieves lower standard accuracy $88.36\%$.
We also want to emphasize that -- perturbing even $200$KB in a $2$MB file ($=10\%$)  is very challenging in a malware file, and yet our DRSM-n models can provide $37\% {\sim} 64\%$ certified accuracy for such perturbation (from Figure \ref{fig:cert_acc_test_plot}). Remember that -- this accuracy reports the theoretical lower bound and in practice, our DRSM-n models provide even higher robustness (shown in Section \ref{sec:emp_robustness}).


