\label{app:model_impl}


\begin{figure}[h]
        \centering
        \includegraphics[width=1.0\linewidth]{images/malconv_archi_2.png}
        \caption{MalConv model architecture}
        \label{fig:malconv_model}
\end{figure}


For MalConv and MalConv (NonNeg) implementation, we used input size of $2MB$. For our optimizer, we used -- 
\begin{itemize}
    \item Optimizer: \texttt{SGD}
    \item learning-rate: $0.01$
    \item momentum: $0.9$
    \item nesterov: \texttt{True}
    \item weight-decay: $1e-3$
\end{itemize}
We used the same setting for every model -- MalConv, MalConv (NonNeg), and DRSM-n.
For training on VTFeed and our dataset, the batch size was 16 and 32, respectively. All the models were re-trained for 10 epochs. We trained the models using multiple gpus at different times. But mostly used gpus were 4 NVIDIA RTX A4000 and 2 RTX A5000.