
\label{app_algo}

\textit{‘UniversalPatch’} is the main function that gets called at first. 
$X$ is the set of malware that is used to generate the patch, $numBytes$
 is the patch size for universal patch, and $\epsilon$
 is the step size for perturbation. Initially, this function creates an embedding vector $e_{init}$
 of $numBytes$
 size and calls the function \textit{‘PerturbEmbedding’} for each malware $x_i$
 in set $X$
.

The \textit{‘PerturbEmbedding’} function perturbs (or modifies) the patch $e_0$
 for that specific malware sample $x_0$
 so that it can evade the model. This function calls three other functions sequentially.

The \textit{‘GetEmbedding’} function takes a sample in problem space and generates its embedding representation (the first layer of the MalConv model). We used this function to generate embedding $e_x$
 for the original malware sample $x_0$
.

The \textit{‘Append’} function appends the patch embedding $e_0$
 at the end of malware embedding $e_x$
, and generates an embedding vector $e$
.

The \textit{‘GradientAttack’} function takes in the embedding $e$
, step size $\epsilon$
, and generates a perturbed embedding $e_u$
 that can evade the model. This uses the classification loss $l$
 of the output with respect to the target label (benign in our case). It takes the sign of the gradient $\Delta_l$
 over the embedding $e$
 and changes the embedding $e$
 to that direction for $\epsilon$
 amount. Thus, it gets to a point where the embedding $e$
 would be misclassified by the model.

At the end, the \textit{‘PerturbEmbedding’} function returns the embedding of $numBytes$
 size from its end since only this last part represents the patch.

The \textit{‘UniversalPatch’} function keeps a list of all perturbed patches from each malware sample and computes the average on them. Thus, we get the embedding representation of our universal patch, $e_{universal}$
.

Then the \textit{‘EmbeddingMapping’} function maps this embedding from embedding space to problem space (byte space in our case). We followed the same embedding mapping strategy as described in ~\cite{suciu2019exploring} and ~\cite{8553214}. We refer interested readers to these original papers for more details.