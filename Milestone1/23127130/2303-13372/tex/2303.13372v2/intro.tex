
%\shoumik{
%Malware classification are 2 types, Ml has been used for long time, Most static analyzer are on feature level and same for the attacks, but we are working on byte-level, applied FGSM attack, extended it to universal patch attack, applied defense from vision transformer.\\
%Contributions - diverse dataset, universal patch attack, defense from vision transformer}

%\yck{Hey all, please review the intro when you have time, I rewrote it altogether.}
Machine learning has started to see more and more adaptation in malware detection as it also has in many other mission-critical applications.
%
However, unlike other applications, malware detection is inherently adversarial in nature.
%
Defenders, such as anti-virus vendors, constantly develop new solutions, and adversaries find ways to evade them either by developing novel malware or, more commonly, by making minor but deliberate changes to existing malware~\cite{graziano2015needles}.
%
In this arms race, because of their efficiency, vendors often prefer static analysis features that are extracted from programs without executing them.


Traditionally, ML models that use static features have required a manual feature engineering step due to the large size and complex nature of programs.
%
More recently, however, researchers have proposed models that can consume whole programs simply as binary files to eliminate this step.
%
Most notably, MalConv~\cite{raff2018malware}, which uses a convolutional neural network, has achieved state-of-the-art results and pioneered the efforts into applying deep learning advances in malware detection.

Albeit promising, the prospect of deep learning in malware detection opens up a new way for attackers to evade: crafting adversarial examples.
%
Research has shown cases where models such as MalConv are severely vulnerable to minor file perturbations.
%
For example,~\citeauthor{suciu2019exploring} have shown that appending carefully crafted bytes to malware files can have over 70\% evasion rate against MalConv.
%
Although such attacks have raised doubts about the feasibility of ML for malware detection, it is still unknown whether ML robustness methods against adversarial examples can be applied in this domain.
%
Our work simulates the arms race and studies the problem from both perspectives to answer this question.


\textbf{Universal Adversarial Patch (UAP) Attacks in Malware Detection.}
%
First, as an adversary, we design a practical and effective attack against MalConv-type models.
%
We develop an algorithm to craft \emph{universal} adversarial patches to address a major shortcoming of prior attacks: being input-specific.
%
In an input-specific attack, the adversary needs to recompute the patch, i.e., the appended bytes, every time they wish to perform the attack on a new sample.
%
This leads to a significant overhead for an adversary considering how frequently anti-virus vendors update their systems to blocklist new samples.
%
In our attack, on the other hand, the patch is computed ahead of time once and then can be used to make any malware sample evasive in constant time.
%
Despite being more constrained, our universal attack achieves an evasion rate comparable to prior input-specific attacks---98\% vs 80\%.
%
To our knowledge, this is the first extension of universal patch attacks~\cite{brown2017adversarial} to the byte-level malware detection domain.

\textbf{Certifiably Robust Malware Detectors.}
%
Second, as a defender, we investigate viable solutions to patch attacks, input-specific or universal.
%
Research has shown that most heuristic defenses to adversarial examples are eventually defeated by stronger attacks~\cite{athalye2018obfuscated}.
%
We believe this trend will likely be more pronounced in malware detection due to real incentives to bypass defenses.
%
This guides us into designing a \emph{certified} defense to provide robustness guarantees regardless of how the attack was created.


In particular, we adapt de-randomized smoothing~\cite{levine2020randomized}, which was designed primarily for computer vision tasks, to malware detection.
%
The differences between images and binary files as inputs make this adaptation challenging.
%
To this end, we use window ablations as an analogue of band ablations in original de-randomized smoothing.
%
Moreover, we augment the original MalConv to improve its performance and efficiency on ablated files.
%
As a result, we obtain \emph{smoothed-MalConv}: a certifiably robust model to patch attacks.
We found that the `smoothed-MalConv' model can not only achieve nearly similar standard accuracy to the `original MalConv' model but also higher robustness. For $10K$ window size, it can get $99.42\%$ standard accuracy and $62.32\%$ certified accuracy. Moreover, only $19.35\%$ and $59.02\%$ adversarial malware generated by UAP and input-specific AP respectively could evade this model.
Overall, these results suggest that certified defenses are feasible and promising in malware detection.
%
%Further, we also build another MalConv variant based on vision transformers (ViTs)~\cite{dosovitskiy2020image} to tap into their innate ability to handle ablated inputs~\cite{salman2022certified}.
%
%We find that the resulting \emph{MalConv-ViT} offers significantly more resilience against adversarial malware samples than other MalConv variants.
%

%\yck{Shoumik, please add some results/numbers in this last paragraph to compare robustness of different models.}

%\yck{if you want to share you code with the reviewers, make sure that it is hosted on an anonymous repository, you can use this website: \url{https://anonymous.4open.science/}}
%To this date, we make our code and implementations publicly available - \href{https://github.com/ShoumikSaha/MalConv-Smoothed-ViT}{github.com/ShoumikSaha/MalConv}. 


To summarize, we make the following contributions: 
\begin{itemize}[noitemsep,topsep=0pt, leftmargin=*]
    \item We develop a universal variant of a prior input-specific attack that poses a greater threat by granting an adversary the ability to reuse the same patch on multiple malware samples to evade detection (Section~\ref{sec:attacks}). 

    \item We show the first adaptation of de-randomized smoothing to malware detection to achieve certified robustness to any patch attack (Section~\ref{sec:defense}).
    
    \item We systematically evaluate our attack and defense in tandem to identify defensive trade-offs, limitations and promising directions for future work (Section~\ref{sec:evaluation}).  
\end{itemize}
 

%Defenses for patch based attacks have been identified in the computer vision space, but have yet to see widespread research and adoption in malware classifiers. One such defense is the application of a vision transformer, which modifies the input prior to classification to classify subsections of the image separately to avoid local imbalances due to the patch. We adopted such defense mechanism from vision community and came up with a new more-robust model, named `MalConv-ViT'. We found this variant to perform 



% Machine learning has become a central component to many aspects of computer science, security included. 
% %
% As malware become more advanced, so must the methods of malware detection. Machine learning classifiers have evolved to become more efficient and effective in classifying unusual inputs, including malicious files. There are two main types of malware classifiers -- static and dynamic. Dynamic works on studying the behavior of the software once it is running and needs to be run on a separate environment which can be sandbox or cloud. As a result, such analysis requires more computational power and overhead. So, static classification is used as the first-line of defense which can run the programs locally, by analyzing the file before execution. Consequently, such classification tends to be faster, and safer in its analysis, and will be the classification strategy we explore in the paper. 

% Though machine learning has come a long way in detecting malware, most of the static detections are based on extracted features of malware or benign files. In this paper, we go in-depth and do the analysis on byte-level. We used state-of-the-art MalConv model \cite{raff2018malware} as our static classifier that takes byte executables and predits their class (malware or benign). For training-testing purpose, we created our own dataset with 550K executable binaries. We focused on maintaining the diversity of families and the source the files come from.
 %we explore the intersection of vision transformers, adversarial patch malware, and classifier strategies. Most static analyzers work on the feature level, looking at the input software at a high level and analyzing chunks of similar code. Our classifier operates at the byte-level, specifically looking at the chain of bytes being fed into the classifier.

% With the popularity of adversarial machine learning, it has already been heavily explored on machine learning based malware detectors too. There are several strategies for classification evasion, or a strategy to modify the input in a way, or cause some form of change to the system that results in the classifier misclasifying the input from malicious to benign. One such attack is the Fast Gradient Signed Method (FGSM) attack, which modifies the input with a `patchâ€™ \cite{goodfellow2014explaining}. In our case, it is a sequence of byte change that causes the algorithm to classify the malicious input as benign. We implemented such an attack that pads some byte, i.e. 'adversarial patch' at the end of a malware to evade the MalConv model. Besides input-specific patch, we also generated universal adversarial patch (UAP) for malware and tested that on MalConv model.

% \textbf{Why `Universal Adversarial Patch'?}
% Though input-specific patch attacks can bypass a detection model, they are not so realistic since most AV softwares do adversarial training on newly discovered adversarial malwares and keep their model updated. As a result, it has been a cat-and-mouse game between AV systems and malware attackers. Moreover, it takes time to calculate gradients and generate a single adversarial sample. However, in case of universal adversarial patch (UAP), attackers can append the patch at the end of their malware in constant time and evade the static classifier. To our knowledge, we are the first who applied universal adversarial patch attack on malware detection model at the byte-level.

%We got more than 80\% evasion accuracy just using one single universal patch that has a size of 4\% of the input length. 

%Most FGSM attacks operate by analyzing a specific input to generate a specific patch associated with the input. We use the FGSM attack and extend it to a universal FGSM attack, where a universal patch is generated from malware samples, which can then be applied to any malware to provide evasion success. 

% \textbf{Can we make the classifier certifiably robust?} 
% Along with the patch-based attack, we explored certified patch robustness for MalConv model leveraging the concept of de-randomized smoothing. Though such defense strategy has been heavily used in the computer vision field, we are the first one to use this for malware classifier. However, it was challenging to adopt such strategy due to the difference in image and byte file. We partitioned a byte file into multiple consecutive parts using a specific window size which is similar to the band ablation \cite{levine2020randomized} in computer vision. As the base classifier, we used a variation of MalConv model that can efficiently handle the ablated byte file. As a result, we came up with a new certifiably robust model, named `smoothed-MalConv'.


%Defenses for patch based attacks have been identified in the computer vision space, but have yet to see widespread research and adoption in malware classifiers. One such defense is the application of a vision transformer, which modifies the input prior to classification to classify subsections of the image separately to avoid local imbalances due to the patch. We adopted such defense mechanism from vision community and came up with a new more-robust model, named `MalConv-ViT'. We found this variant to perform significantly better on adversarial malwares than other models. 

% In our FGSM based input-specific patch attack, we had 98.4\% evasion rate, i.e. the rate of MalConv predicting the malware as benign. This evasion rate did not drop significantly (94.3\%) even when we reduced the patch size to 1\% of the file size. When we extended this attack to universal patch, we got more than 80\% evasion rate just using one single universal patch that has a size of 4\% of the input length, and these adversarial malwares were generated in constant time. On the other hand, our `smoothed-MalConv' achieved almost identical standard accuracy as MalConv model. But it could detect the adversarial malwares that were created using the UAP in more than 98\% cases. Besides this, we empirically found that `smoothed-MalConv' performs well against input-specific adversarial patch too.

