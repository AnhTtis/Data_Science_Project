In the cat-and-mouse game between the AV systems and malware authors, the latter try to evade the classifier by generating adversarial malware samples, while the former re-train the model on the newly generated adversarial samples
to make the classifier robust. In this paper, we assume that the attacker has full knowledge of the neural network, including the architecture and model parameters, but cannot modify them. Such white-box manner is a realistic and conservative assumption since -- `security through obscurity' is not a good approach and it is possible to transfer an attack from a substitute model to a target model even in the black-box setting \cite{papernot2016transferability}. Having the model, the attacker can modify the input sample in test time and evade the model. So, the goal of the attacker is to generate a patch $\delta$ that creates the adversarial malware $x^{'} = x + \delta$, for which $F_{\theta}(x^{'}) < 0.5$, i.e., the classifier predicts it as a benign file. According to our assumption, the attacker knows the classifier model $F$ and its parameters $\theta$ here, and can modify the original malware file $x$.

Though there are multiple attack strategies to evade a machine-learning based malware classifier, there are some limitations in case of binary files. Any change to a byte value in binary file is not possible due to its structural interdependence among adjacent bytes. The Portable Executable (PE) standard \cite{pefile} has a fixed structure for these files, containing a header with file metadata and pointers to the sections of file, followed by sections that contain the actual code and data. For any arbitrary change in such malware file, the file can lose its semantics, i.e. malicious functionality, in the worst case, the file can get corrupted.

Recent works \cite{8553214} \cite{kreuk2018deceiving} \cite{suciu2019exploring} suggest append-based patch attack strategies to address this limitation. In such attacks, a patch $\delta$, i.e. sequence of bytes, is appended at the end of the malware binary files since they are not within the defined boundaries of the PE files. Thus, the addition of patches does not have any impact on the semantics of malware but is enough to evade the classifier model. So, the attacker can generate such patches and evade the model when he has full access to the model. 
In this paper, we have followed the same threat model and implemented the patch-append attack for both the cases -- input-specific and universal. 
%For the first time, we extended this patch append attack from input-specific to input-agnostic.




%There are multiple attack strategies to evade a machine-learning based classifier, i.e., model misclassifies a sample to a target label.  However, unlike other application domains, we have to preserve the malware semantics, and perturbing a binary in random places can disrupt that (in worst cases, it can corrupt the whole file). So, append attack is the most viable one in this case.