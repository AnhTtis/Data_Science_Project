\subsection{Dataset}
%\wwx{to Shoumik\&Soheil: Do we want to incorporate this as part of the contribution?}\smk{Personally I don't want to mention this as a major contribution.} 
%\SF{what is your reasoning about it? I think gathering a high quality dataset is certainly a good contribution}
%A significant portion of this project was to collect a wide range of unbiased data. 
Since there are many different families of malware, it is important to train on all of them. On the other hand, just getting binaries from Microsoft can be disastrous because the model may determine that a file is benign if the binaries were signed by Microsoft. This is due to the fact that many Windows executable contains strings indicating that they were signed by Microsoft. So, gathering large amount of diverse malicious and benign executable binaries was very important for this work.

For malware files, most released datasets list the hash and features they extracted from the files. However, in this work, our machine learning model ingests the binaries of files, so this is not useful. So, we collected malware binary executables from VirusShare, Microsoft Malware Classification Challenge (BIG 2015) \cite{microsoft_dataset} and Dike Datasets \cite{dike_dataset}.
%Most reputable source of malware files do not release them publicly, due to monetary reasons, and the fact that these can cause actual harm to machines. For example, to gain access to VirusShare and VirusTotal, we had to submit a request and give reasons why we needed the files for academic research. Eventually, we got the response from  VirusShare only.
It was even more difficult to gather benign files. For privacy reasons, many sources do not release the binary executable of benign files. The method chosen in this project was to crawl  popular websites, e.g., SourceForge, CNET, Net.exe, etc., to download the Windows binaries. 

As seen in Table \ref{table:Mdataset} and \ref{table:Bdataset}, multiple sources were used to gather large amounts of binary files. In total, $535,137$ malicious files  and $15,568$ benign files were collected. As shown by the order of magnitude difference, it is easier to collect malicious files than benign as they are a big area of study. But it is important to train a model with balanced dataset to not skew it. Eventually, we created a balanced dataset of 13K samples with equal number of malware and benign binaries for the training and testing purpose. 

\begin{table}[h]
  \centering
\begin{center}
\begin{tabular}{ |m{15em}|m{5em}| }
 \hline
  Source & Number of Binaries \\
  \hline
Virus Share & 524,296 \\
Microsoft Malware Classification Challenge (BIG 2015) \cite{microsoft_dataset} &  10,868 \\
Dike Dataset \cite{dike_dataset}  & 10,841 \\
 \hline
Total  &  535,137 \\

 \hline
\end{tabular}
\end{center}
\caption{Dataset of the Malicious Files}
\label{table:Mdataset}
\end{table}

\begin{table}[h]
  %\centering
%\begin{center}
\begin{tabular}{ |m{15em}|m{5em}| }
\hline
  Source  & Number of Binaries \\
 \hline
SourceForge  & 7,865 \\
CNET  & 3,661 \\
Net  & 2,534 \\
Softonic  & 1,152 \\
DikeDataset  & 1,082 \\
Netwindows  & 185 \\
Manually Obtained from Windows OS  & 89 \\
 \hline
Total & 15,568\\
  \hline
\end{tabular}
%\end{center}
\caption{Dataset of the Benign Files}
\label{table:Bdataset}
\end{table}


\subsection{Model Performance}

%\subsubsection{MalConv: Static Malware Classifier}
We first created a static malware classifier based on MalConv ~\cite{raff2018malware}. MalConv was chosen because it is a commonly used, state-of-the-art malware classifier. It takes binary executable file as input and outputs a decision, $0$ or $1$, for benign or malware, respectively. 
%Although many implementations of the Malconv model have been created over the time, most of them were created before 2019. Those implementations are not compatible with the current tensorflow version, and engineering efforts had to be expended to create a new code base for MalConv model that is compatible with the latest versions and computational power. It can be found in our publicly-available code-base \href{https://github.com/ShoumikSaha/MalConv-Smoothed-ViT}{github.com/ShoumikSaha/MalConv}. 

We created multiple models based on randomly selected binaries from our data-set. Finally, we took the one that was trained on a set of $13,040$ files, a balanced data-set of benign and malware binary files. We use this same model as the target model for attack and as the base classifier for defense strategy. Implementation details of training the model can be found in the appendix \ref{app_malconv}.

{\bf Evaluation.} We found the MalConv model to be highly accurate. After training, we evaluated the model on a test set of $1000$ samples and got $97.9\%$ test accuracy.
The overall accuracy on the full dataset was $98.15\%$. 

%{\bf Hyperparameters.} We changed the input size of the model to 250K from 200K which was in the original implementation, so that it can cover more files. We used the `Adam' optimizer and `binary-cross entropy' loss function for our model implementation.

\subsection{Attacks}
\subsubsection{\textbf{Adversarial Patch Attack}}\label{result_adv_patch}


Our next step was to create an attack to evade the Malconv model. Based on existing work ~\cite{suciu2019exploring}, we applied the FGM Append attack demonstrated in Algorithm \ref{alg:FGMattack}. This works by adding a small patch to a malware file to evade the static classifier. We randomly collected $200$ malwares (outside of Train-set) and applied the attack with different pad percentage.
The FGSM was run for $10$ iterations for each sample by setting the step size $\epsilon$ as $0.5$.
%The patch part of the name, adversarial patch attack is due to the original application, where a ``patch'' was added to an image to cause a model to mis-classify it. In order to not hamper to logic of the executable, the patch is added at the end of the file.

Our results are shown in Table \ref{table:APA}. At first, we generated adversarial patch just adding a patch of 1\% of the file size, and modifying that. So, for a file size of $2MB$, the patch size would be of $\sim20KB$ only. Even with such a small patch, we achieved 94.3\% evasion rate. We got even higher evasion rate of 98.4\% when we increased the pad percentage to 5\%. 

\begin{table}[!htb]
  \centering
\begin{center}
\begin{tabular}{ |c|c|c|c|c| }
 \hline
  Pad  & \multirow{2}{3em}{Total} & Bytes  & \multirow{2}{3em}{Evaded} & Evasion  \\
  Percent &  & Addable & & Rate \\
  \hline
    $1\%$ & $200$ & $123$ & $116$ & $94.3\%$ \\
 \hline
    $5\%$ & $200$ & $123$ & $120$ & $\textbf{98.4\%}$ \\
 \hline
\end{tabular}
\end{center}
\caption{Evasion Rate for Input-specific Adversarial Patch Attack}
\label{table:APA}
\end{table}

\iffalse
\begin{table*}[!htb]
  \centering
\begin{center}
\begin{tabular}{ |c|c|c|c|c|c| }
 \hline
  \textbf{Iterations} & \textbf{Pad Percent} & \textbf{Total} & \textbf{Bytes Addable} & \textbf{Evaded} & \textbf{Evasion Accuracy} \\
  \hline
10 [e=0.5]  & $1\%$ & $200$ & $123$ & $116$ & $94.3\%$ \\
 \hline
10 [e=0.5]  & $5\%$ & $200$ & $123$ & $120$ & $98.4\%$ \\
 \hline
\end{tabular}
\end{center}
\caption{Evasion Rate for Input-specific Adversarial Patch Attack}
\label{table:APA}
\end{table*}
\fi



\subsubsection{\textbf{Universal Adversarial Patch Attack}} \label{result_adv_patch_uni}
%A big drawback of the adversarial patch attack is that a unique patch must be created for each malicious binary. This might not always be realistic as the computational efforts are to great. An optimization to this attack ~\cite{labaca2021realizable} extends this work by creating a universal adversarial patch attack. 
Instead of creating a custom patch for a particular malware file everytime, a universal adversarial patch (UAP) can be used on multiple malware files in constant time. This allows an adversary to efficiently create multiple malicious files that can evade detection. To the best of our knowledge, this is the first time a universal adversarial patch attack is applied on the byte level. We implemented this through a similar process to the FGSM attack, but taking the average perturbation on multiple samples.

To carry out this attack, we took $200$ malware files that were not seen by the model in training phase and divided them into two sets, one to generate the universal adversarial patch, and other to test. We generated our universal patch by running the Algorithm \ref{alg:FGMattack_uni} on the first set. The patch length was set to $10$k bytes or $4\%$ of the original input files size ($40$KB in disk size). This generated universal patch was tested on the second set, and we made sure that the samples in this set are unseen to the both parties -- model and attacker. After adding this universal patch at the end of these malwares, we fed them to the MalConv model for prediction. As shown in Table \ref{table:UAPA}, we got 80\% evasion accuracy on the second set. This shows that this attack is very viable and scalable in large scale.

\begin{table}[!htb]
  \centering
\begin{center}
\begin{tabular}{ |c|c|c| }
 \hline
   & First Set & Second Set \\
  \hline
    Total Files      & $150$     & $50$ \\
 \hline
    Bytes Addable    & $93$      & $30$ \\
 \hline
    Evaded           & $79$      & $24$ \\
 \hline
    Evasion Rate     & $\textbf{84.95\%}$ & $\textbf{80\%}$ \\
 \hline
\end{tabular}
\end{center}
\caption{Evasion Rate for Universal Adversarial Patch Attack}
\label{table:UAPA}
\end{table}

\subsubsection{\textbf{Limitations of Attack}}
Though both the input-specific and the universal adversarial patch attack are very successful in evading the MalConv model, we want to acknowledge some of the limitations of our attack. Firstly, here the attacker has to have the full-knowledge of the model, i.e. white-box attack. Without full-knowledge, it is not possible to run FGSM attack. For black-box setting, one work-around can be using a similar model as unknown-target model to generate the adversarials. However, it does not ensure the evasion since transferability is not guaranteed in adversarial machine learning. Secondly, if the file size already exceeds the input size of the MalConv model, then the append-based attacks do not work. Finally, if the UAP is made publicly-available, the AV systems can just update themselves against the patch. 


\subsection{Defense with De-randomized Smoothing} \label{sec:smoothed-malconv}

%\wwx{to Shoumik, Could you finish this part and refer to results in your table-5 and table-6? \\additionally: 
%\begin{enumerate}
%\item (Important) What is the clean acc of ViT vs MalConv?  \smk{do you mean standard acc?} \wwx{yes, good catch}
%\item (If possible) do we ahve results for smoothed-ViT? 
%\item (Important) What is the parameters for smoothing? We should definitly include some abalation study for the parameters (e.g. how different length of the ablated input affect accuracy and robustness) \smk{ok, working on it}
%\end{enumerate}}

For ablation, we used `window ablation' strategy to adapt the band smoothing from \cite{levine2020randomized} (recall from \ref{sec:defense}). We experimented with different `window ablation' sizes, e.g., 62.5K, 50K, 25K, and 10K. For each experiment, the input dimension and the number of base classifiers had to be changed to be compatible with the dimension of the binary file (details can be found in Appendix \ref{app_smooth_malconv}).

We evaluated the standard accuracy and certified accuracy of our `smoothed-MalConv' model for different sets of samples. Table \ref{tab:result_smooth_mlcnv} represents such five sets. Here, the `Clean Samples' set consists of all malware and benign files from the training set (total 13K samples), and we got $99.59\%$ standard accuracy in this set. The `Original Malware' set contains only the malware from 'Clean Sample' set that has not been perturbed yet. 

Moreover, to evaluate the robustness of our `smoothed-MalConv' model, we generated three universal patches of different sizes (20K, 10K, 5K) attacking the `original-MalConv' model. These adversarial samples are generated using the same strategy described in \ref{subsec:uap}. Then we evaluated the `smoothed-MalConv' model against these samples and found that the model can detect adversarial samples generated using UAP in $72\% \sim 93\%$ cases.

Intuitively, the adversarial attack is weaker when a smaller size is assumed for the adversarial patch and thus higher standard accuracy and certified accuracy of defenses may be obtained. This is confirmed in Table \ref{tab:result_smooth_mlcnv}, where we see that in every row, both standard and certified accuracy increase when considering smaller adversarial patches for malware: 
All of the models have the highest accuracy for UAP = $5K$. 
Notably, smoothed-MalConv with $w=62.5K$ is expected to have $0\%$ certified accuracy because theoretically, a patch can intersect with two out of a total of four ablated sequences and so $n_{m}$ has to be greater than $2\Delta(=4)$ in order to contribute to certified accuracy, which is impossible in this case (see equation \ref{eqn:cond_certtify} for details). 
On the other hand, in columns of Table \ref{tab:result_smooth_mlcnv}, the certified accuracy increases as we decrease the window size $w$. 
Such a trend of certified accuracy originates from the increased length of ablated sequences associated with reduced window size (see Appendix \ref{app_defense} for more insights). 


Moreover, we evaluated the robustness of `smoothed-MalConv' against input-specific attacks and found that it offers stronger robustness than `original-MalConv' as well, as shown in Table \ref{table:original_vs_smoothed}.
Notably, we also experimented with the `non-negative' ~\cite{fleshman2018non} version of MalConv model. Though it has better robustness for UAP and input-specific patch attack, it has poor performance for clean samples. On the other hand, the `smoothed-MalConv' offers better clean accuracy with considerate robustness, indicating a much better solution balancing performance and robustness. 

%For our `window ablation', we chose the window size $w$ as 50K since the highest length patch is 20K and that can modify at most two ablated sequences. So, the rest three unmodified ablated sequences still should be correctly detected by the base classifier. However, experiments with different ablation sizes will be done later. 

%At first, we took the vision transformer from \cite{dosovitskiy2020image} as the base classifier and followed the original implementation since this was the first work to have a good result on ImageNet by incorporating the vision transformer. Since such base classifiers are for images, rather than byte files, their performance in terms of standard accuracy was not satisfactory. However, this model had better accuracy in detecting adversarial malware than the `original MalConv' model. 

%This motivated us to incorporate derandomized-smoothing with MalConv model to have better robustness. We empirically found that our `smoothed-MalConv' has 98\%\~100\% accuracy in detecting adversarial malware generated using UAP From the table \ref{table:defense_result}, it is clear that our `smoothed-MalConv' model outperforms the `smoothed-ViT' model. We extended our robustness experiment to input-specific adversarial patch and found that `smoothed-MalConv' performs well there too. It could detect 61.62\% adversarial malware generated using input-specific AP whereas the `smoothed-ViT' detected only 18.69\%.

%Along with higher robustness, `smoothed-MalConv' has 98.24\% standard test accuracy whereas the `original MalConv' achieved 99.15\% test accuracy (see table \ref{table:binary_cls_performance}).

 

%Our project draws influence from work done on certified patch robustness via smoothed vision transformers\cite{salman2022certified}. The idea has previously been applied to vision classifiers as a means to provide robustness against patch based adversarial attacks. The steps are to generate an ablation from an input image, which represent a portion of the original input that is then passed individually to the base malware classifier. The output is aggregated, and a final classification determination is given. 

%We took the novel idea and applied it to our updated MalConv classifier. We call this new classifier MalConv-ViT. Our strategy was since the input is not a classic image, to treat the input as an image that is just one pixel wide, with a length of 250,000. Ablations were generated for each sample consisting of an array of 50k bytes each. These were then fed to the base classifier of MalConv. MalConv-ViT had success it detecting 90.43\% percent of adversarial malware that were generated using UAP, malware specifically designed with universal patch based attacks. Although MalConv-ViT had greater success in detecting adversarial malware than the base classifier, due to the greater specification of MalConv-ViT, overall classification accuracy dropped to 75\%. 

%Though vision transformer with ablations was proposed for UAP, we applied our MalConv-ViT on adversarial malware thats were generated using input-specific patch with pad percentage 1\%. For such case, MalConv-ViT successfully detected 59.29\% adversarial malwares.

%However, a model that utilizes both MalConv for general malware classification, as well as MalConv-ViT specifically for adversarial based malware would be more effective since MalConv-ViT alone is not good for overall accuracy. We want to explore that in the future.
%Weighting the outputs to improve our base classifierâ€™s detection of both standard malware, and adversarial malware. 

\begin{table*}[]
    \centering
    \resizebox{1\linewidth}{!}{
    \begin{tabular}{|m{8em}|c|c|c|c|c|}
    \hline
     \multirow{3}{5em}{\centering Model}    & \multicolumn{5}{c|}{Standard Accuracy (Certified Accuracy)} \\
    \cline{2-6}
         & Clean Samples & Original Malware & \multicolumn{3}{c|}{Adversarial Malware} \\
    \cline{4-6}
         & (Patch Size = 20K) & (Patch Size = 20K) & UAP=20K & UAP=10K & UAP=5K \\
    \hline
    smoothed-ViT  \ \ ($w=50K$) & 68.73\% (28.26\%) & 54.4\% (5.31\%) & 29.03\% (5.38\%) & 33.33\% (6.45\%) & 37.63\% (6.45\%) \\
    \hline
    smoothed-MalConv ($w=62.5K$) & \textbf{99.59\%} (0\%) & \textbf{99.62\%} (0\%) & \textbf{81.72\%} (0\%) & \textbf{89.25\%} (0\%) & \textbf{92.47\%} (0\%) \\
    \hline
    smoothed-MalConv ($w=50K$) & 99.45\% (36.04\%) & 99.29\% (45.23\%) & 79.57\% (5.38\%) & \textbf{89.25\%} (5.38\%) & 91.4\% (5.38\%) \\
    \hline
    smoothed-MalConv ($w=25K$) & 97.91\% (49.03\%) & 97.61\% (62.62\%) & 72.04\% (35.48\%) & 86.02\% (39.78\%) & 89.25\% (40.86\%) \\
    \hline
    smoothed-MalConv ($w=10K$) & 99.42\% (\textbf{62.32\%}) & 99.45\% (\textbf{76.44\%}) & 80.65\% (\textbf{44.09\%}) & 86.02\% (\textbf{61.29\%}) & 89.25\% (\textbf{65.59\%}) \\
    \hline
    \end{tabular}
    }
    \caption{Standard and Certified Accuracy of Models}
    \label{tab:result_smooth_mlcnv}
\end{table*}


\begin{table*}[!htb]
  \centering
\begin{center}
%\begin{tabular}{ |m{7em}|m{7em}|m{4em}|m{4em}| }
\begin{tabular} {|c|c|c|c|}
 \hline
  \multirow{2}{5em}{Model} & \multirow{2}{10em}{Misclassification Rate on Clean Samples} & \multicolumn{2}{c|}{Evasion Rate} \\
  \cline{3-4}
  &  & Universal AP (20K) & Input-specific AP (1\%) \\
  \hline
  original-MalConv & 1.85\% & 83.74\% & 94.3\% \\ \hline
  non-negative-MalConv & 9.2\% & 0.0\% & 15.57\% \\ \hline
  smoothed-MalConv ($w=62.5K$) & \textbf{0.41\%} & \textbf{18.28\%} & 66.39\% \\ \hline
  smoothed-MalConv ($w=50K$) & 0.55\% & 20.43\% & 81.97\% \\ \hline
  smoothed-MalConv ($w=25K$) & 2.09\% & 27.96\% & 91.8\% \\ \hline
  smoothed-MalConv ($w=10K$) & 0.58\% & 19.35\% & \textbf{59.02\%} \\ 

  \hline
\end{tabular}
\end{center}
\caption{Comparison between original-MalConv and smoothed-MalConv}
\label{table:original_vs_smoothed}
\end{table*}

\iffalse
\begin{table*}[!htb]
  \centering
\begin{center}
\begin{tabular}{ |c|c|c|c|c| }
 \hline
  \multirow{2}{6em}{Base Classifier}  & \multicolumn{3}{|c|}{Universal AP (Clean/Certified Accuracy)} & Input-Specific AP\\
  \cline{2-5}
  & 5k & 10k & 20k & \\
 \hline
 smoothed-ViT \cite{dosovitskiy2020image} & 45.16\% & 46.24\% & 43.62\% & 18.69\%\\
 \hline \hline
 smoothed-MalConv ($w=62.5K$) & 100\%(0\%) & 100\%(0\%) & 98.94\%(0\%) & 61.62\% \\
 \hline
 smoothed-MalConv ($w=50K$) & 100\%(83.87\%) & 100\%(87.1\%) & 98.94\%(90.32\%) & 61.62\%\\
 \hline
 smoothed-MalConv ($w=25K$) & 91.40\% & 91.40\% & 90.43\% & 55.56\% \\
 \hline
 smoothed-MalConv ($w=10K$) & 100\%(93.55\%) & 100\%(97.85\%) & 98.94\%(98.94\%) & 61.62\%\\
 \hline
\end{tabular}
\end{center}
\caption{Detection Rate for Adversarial Malware}
%\wwx{Why are results duplicated for $w=50K$ and $w=10K$?}
%\wwx{Do you mean empirical/certified acc? Why is certified accuracy higher for larger patches?}
\label{table:defense_result}
\end{table*}



\begin{table*}[!htb]
  \centering
\begin{center}
\begin{tabular}{ |m{9em}|c|c|m{9em}|m{9em}| }
 \hline
  Model & Train Accuracy & Test Accuracy & Evasion Rate for universal AP (patch size = 20K) & Evasion Rate for input-specific AP (pad percent = 1\%) \\
  \hline
  smoothed-ViT \cite{dosovitskiy2020image} & 95.51\% & 95.25\% & 56.38\% & 81.31\% \\
  \hline
  Original MalConv & 97.9\% & 99.15\% & 83.74\% * & 94.3\% \\
 \hline \hline
 smoothed-MalConv ($w=50K$) & 99.99\% & 98.24\% & 20.43\% & 38.38\% \\
 \hline
 smoothed-MalConv ($w=25K$) & 99.98\% & 98.08\% & 16.13\% & 44.44\% \\
 \hline
 smoothed-MalConv ($w=10K$) & 99.94\% & 96.86\% &  & 38.38\% \\
 \hline
\end{tabular}
\end{center}
\caption{Binary Classification Accuracy and Robustness of Different Models }
\label{table:binary_cls_performance}
\end{table*}
\fi