\label{sec:defense}


\begin{figure*}
        \centering
        \includegraphics[width=0.90\textwidth]{Images/smoothed_malconv.png}
        \caption{smoothed-MalConv Model}
        \label{fig:smoothed_malconv}
\end{figure*}

%\wwx{not clear, too many details missing; \\Could you maybe add mathematical descriptions with explanations and details (e.g. what is the length of entire byte strings, length of ablated input, size of blocks, how these parameters are choosed); emphasize on changes made to adapt it to inputting bytes} \smk{modified} 
%\wwx{Since only emprical robustness are evaluated, we should emphasize here that we are using a technique that is originally motivated for certified defenses but we focus on obtaining empirical robustness from it}

\textbf{Defensive Assumptions.} we assume that the AV defenders do not know the exact attack strategy of the attackers. Though the patch is appended at the end of the file, the defenders assume that the patch can be anywhere in the file. The intuition of taking such a conservative assumption is -- if a classifier model is robust against any patch append attack, it should be robust against patches that are only appended at the end of the file too.  So, the goal of the defender is to -- create a model $G_\theta$ that can detect any adversarial malware $x^{'}$ that was generated by adding the patch $\delta$ anywhere of it.


For certified defense against the adversarial patch attack, smoothed vision models have long been used in the vision domain. However, such defense method is still untouched by malware community. Since the malware detection problem cannot be directly mapped to typical vision problems, we had to adapt the smoothing strategy in some ways.
%Figure \ref{fig:vit} is from \cite{salman2022certified} where they applied column ablation on images and applied vision transformer as their base classifier. 

Unlike images, our input samples are one-dimensional sequence of bytes, which makes them incompatible with common ablations, e.g., column, row, block ablations, etc., applied in vision. So, we came up with the `window ablation' strategy where we divided the input sample into multiple contiguous sequences of the same size. If the input length of MalConv model is $L$, and the size of `window ablation' is $w$, then there will be $\lceil \frac{L}{w} \rceil$ ablated sequences of length $w$ resulting into the ablated sequence set $S(x)$ (we pick such $w$ that $L$ is divisible by it). So, even if an attacker perturbs a byte file with a patch of size $p$, it can intersect at most $ \Delta = \lceil \frac{p}{w} \rceil + 1$ ablated sequences. Since a patch can only modify a limited number of ablated sequences, it cannot directly change the decision of the classifier model. This intuition motivated us to use such defense strategy for the malware classification model.

We train the MalConv model as the base classifier, changing its input dimension to $w$ so that it is compatible with the ablated sequences of size $w$. 
%Then we feed the outputs for each ablated sequences to a deeply connected neural network layer with `sigmoid' activation. 
Then the resulting `smoothed-MalConv' model returns the most frequent predicted class over the ablated sequence set $S(x)$.
%This model, named \textit{`smoothed-MalConv'}, achieves almost identical standard accuracy as the `original MalConv' model but with higher robustness. 
Specifically, for an input file $x$, ablated sequence set $S(x)$, and MalConv model $F_\theta$, the `smoothed-MalConv' model $G_\theta$ can be defined as:
$$ G_{\theta}(x) = \arg \max_{c} n_c(x) $$
where, 
$$ n_c(x) = \sum_{x^{'} \in S(x)} I\{F_{\theta}(x^{'})=c\} $$ denotes the number of ablated sequences that were predicted class $c$. The percentage of file that are correctly classified by the `smoothed-MalConv' $G_\theta$ is the `standard accuracy'.

We can call this `smoothed-MalConv' model certifiably robust on an ablated sequence set if the number of prediction for correct class exceeds the other one by a large enough margin. This large margin makes it impossible for an attacker to change the prediction of the smoothed classifier since a patch can only modify  $ \Delta = \lceil \frac{p}{w} \rceil + 1$ ablated sequences.
Mathematically, the `smoothed-MalConv' model $G_{\theta}$ is certifiably robust on input $x$ for predicting class $c$ if:
$$ n_c(x) > max_{c \neq c^{'}} n_{c^{'}}(x) + 2 \Delta $$
Since our problem is a binary classification problem, this can be rewritten as for predicting a malware file as malware:
\begin{equation} \label{eqn:cond_certtify}
    n_m(x) > n_b(x) + 2 \Delta
\end{equation}
%$$ n_m(x) > n_b(x) + 2 \Delta $$
where, $n_m(x)$ and $n_b(x)$ are the number of ablated sequences predicted as malware and benign by the `smoothed-MalConv' model, respectively. We represent an overview of our strategy in Figure \ref{fig:smoothed_malconv}. 



We found that our `smoothed-MalConv' performs quite well on detecting the adversarial malware that were generated by using both universal and input-specific adversarial patch. Performance details can be found in \ref{sec:smoothed-malconv}.

%Then we took the majority voting on the predicted class and changed loss function so that the difference between the number of prediction for malware and benign increases. As this defense mechanism is borrowed from ViT model, we named our defense model `MalConv-ViT'. We found that, `MalConv-ViT' performs quite well on detecting the adversarial malwares. Performance details can be found in \ref{sec:smoothed-malconv}.