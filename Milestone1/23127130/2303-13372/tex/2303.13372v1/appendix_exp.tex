\subsection{MalConv Model} 
\label{app_malconv}
We followed the same implementation as original paper \cite{raff2018malware} where the vocab size in embedding layer is 256, the window size and number of filters in convolution layers are 500 and 128, respectively.
We changed the input length of the model to 250K from 200K so that it can cover more files.
We used the ‘Adam’ optimizer and ‘binary-cross entropy’ loss function for our model implementation, and trained it for 50 epochs with 512 batch size. We saved the best model with respect to the validation accuracy.

%\subsection{Patch Append Attacks} 
%\label{app_attack}

\subsection{`smoothed-MalConv' Model} 
\label{app_smooth_malconv}
We used the `original MalConv' as the base classifier, and changed its input dimension from 250K to $w$, i.e., the ablation size. For example, when $w=50K$, there are 5 ablated sequences in the ablation set $S(x)$ and we trained 5 MalConv models of input dimension $w$ for each ablated sequence. The labels for the ablated sequences are the same as the original label of the file. 

We did not change the architecture and any parameters, e.g., vocab size, window size, number of filters, etc., of the `original MalConv' model. While training, we used Adam optimizer and `binary-cross entropy (without reduction)' as the loss function. We trained `smoothed-MalConv' for four different ablation sizes. All the models were trained for 10 epochs with a batch size of 512.