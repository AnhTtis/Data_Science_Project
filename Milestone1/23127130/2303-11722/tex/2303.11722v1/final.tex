\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbding}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[table]{xcolor}
\usepackage{subcaption}
\usepackage{multicol}
\usepackage[colorlinks,linkcolor=red, breaklinks=true,bookmarks=false]{hyperref}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Implicit Neural Representation for Cooperative Low-light Image Enhancement}

\author{
Shuzhou Yang$^{1,2}$, Moxuan Ding$^{3}$, Yanmin Wu$^{1}$, Zihan Li$^{4}$, Jian Zhang$^{1,2*}$\\
$^{1}$Peking University Shenzhen Graduate School $\quad^{2}$Peng Cheng Laboratory \\
$^{3}$Dalian University of Technology $\quad^{4}$University of Illinois at Urbana-Champaign \\
%{\tt\small xuanyuzhang21@gmail.com; ybzhang08@hit.edu.cn;rqxiong@pku.edu.cn;}\\
%{\tt\small sunqilin@cuhk.edu.cn; zhangjian.sz@pku.edu.cn}
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
   The following three factors restrict the application of existing low-light image enhancement methods: unpredictable brightness degradation and noise, inherent gap between metric-favorable and visual-friendly versions, and the limited paired training data. To address these limitations, we propose an implicit \textit{\textbf{Ne}}ural \textit{\textbf{R}}epresentation method for \textit{\textbf{Co}}operative low-light image enhancement, dubbed \textit{\textbf{NeRCo}}. It robustly recovers perceptual-friendly results in an unsupervised manner. Concretely, NeRCo unifies the diverse degradation factors of real-world scenes with a controllable fitting function, leading to better robustness. In addition, for the output results, we introduce semantic-oriented supervision with priors from the pre-trained vision-language model. Instead of merely following reference images, it encourages results to meet subjective expectations, finding more visual-friendly solutions. Further, to ease the reliance on paired data and reduce solution space, we develop a dual-closed-loop constrained enhancement module. It is trained cooperatively with other affiliated modules in a self-supervised manner. Finally, extensive experiments demonstrate the robustness and superior effectiveness of our proposed NeRCo. Our code is available at \url{https://github.com/Ysz2022/NeRCo}.
\end{abstract}

\begin{figure}[t]
  \centering
  \begin{subfigure}{0.49\linewidth}
    \includegraphics[width=1.\linewidth]{Image/Fig1/Input.png}\vspace{-0.3em}
    
    \centerline{Input}
    
    \includegraphics[width=1.\linewidth]{Image/Fig1/SCI.png}\vspace{-0.3em}
    
    \centerline{\small SCI (2022) \cite{Ma_2022_CVPR}}
    \vspace{-0.8em}
    
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.49\linewidth}
    \includegraphics[width=1.\linewidth]{Image/Fig1/URetinex.png}\vspace{-0.3em}
    
    \centerline{\small URetinexNet (2022) \cite{Wu_2022_CVPR}}
    
    \includegraphics[width=1.\linewidth]{Image/Fig1/Ours.png}\vspace{-0.3em}
    
    \centerline{Ours}
    \vspace{-0.6em}
    
  \end{subfigure}

  \caption{Comparison with two state-of-the-art methods on the LIME \cite{Guo_2017_TIP} dataset. One can see that we recovered more authentic color and visual-friendly contents.}
  \label{Fig1}
  \vspace{-2.em}
\end{figure}

\vspace{-1.em}

%%%%%%%%% BODY TEXT
\section{Introduction}
Due to the degraded brightness in low-light images covering objects and reducing contrast, low-light images have severely impacted the subsequent high-level computer visual tasks (\textit{e.g.}, object detection \cite{Liu_2016_ECCV}, and semantic segmentation \cite{Islam_2020_IROS}, \textit{etc.}). Hence, it is of practical importance to remedy the brightness degradation for assisting the exploration of sophisticated dark environment. Low-light image enhancement, which aims to recover the desired content in degraded regions, has drawn wide attention in recent years \cite{Pisano_1998_JDI,Fu_2015_TIP,Fu_2016_CVPR,Guo_2017_TIP,Lore_2017_PR,Guo_2020_CVPR,Jiang_2021_TIP,Xu_2022_CVPR}.

Over the past few years, prolific algorithms have been proposed to address this classic ill-posed problem, which can be roughly categorized into two groups: conventional model-based methods (\textit{e.g.}, histogram equalization \cite{Pisano_1998_JDI}, gamma correction \cite{Moroney_2000_CIC}, Retinex-based model \cite{Ng_2011_SIAM}, and unsharp masking algorithms \cite{Deng_2011_TIP}) and recent deep learning-based methods \cite{Liu_2021_CVPR,Zhang_2021_ICCV,Jiang_2021_TIP,Wu_2022_CVPR}. The former formulates the degradation as a physical model and treats enhancement as the problem of estimating model parameters, but is limited in characterizing diverse low-light factors and requires massive hand-crafted priors. The latter elaborates various models to adjust tone and contrast, which is able to learn from massive data automatically. Essentially, they are trained to learn a mapping from input to output domain. In real-world scenarios, however, many samples are far away from the feature space of input domain, causing a trained model to lack stable effect. We propose to normalize the degradation before enhancement to bring these samples closer to the input domain. Besides, existing supervised methods highly rely on paired training data and mainly attempt to produce metric-favorable results, \textit{i.e.}, similar to the ground truth. But the limited supervised datasets and the inherent gap between metric-oriented and visual-friendly versions inevitably impact their effectiveness. We develop a self-supervised training strategy to address this issue. As shown in \cref{Fig1}, we conduct evaluation on the LIME \cite{Guo_2017_TIP} dataset, which only consists of low-light images without normal-light references. One can see that even the recently proposed top-performing algorithms perform severe color cast.

Specifically, our key insights are: \textbf{i) Normalizing the input with a controllable fitting function to reduce the unpredictable degradation features in real-world scenarios.} We adopt neural representation to reproduce the degraded scene before the enhancement operation. By manipulating the positional encoding, we selectively avoid regenerating extreme degradation, which objectively realizes normalization and thereby decreases enhancement difficulty. \textbf{ii) Supervising the output with different modalities to achieve both metric-favorable and perceptual-oriented enhancement.} We employ multi-modal learning to supervise from both textual and image perspectives. Compared with image supervision, which contains varying brightness across different samples, the feature space of the designed prompt is more stable and accurate in describing brightness. During training, our results are not only encouraged to be similar to references, but also forced to match their related prompts. In this way, we bridge the gap between the metric-favorable and the perceptual-friendly versions. \textbf{iii) Developing an unsupervised training strategy to ease the reliance on the paired data.} We propose to train the enhancement module with a dual-closed-loop cooperative adversarial constraint procedure, which learns in an unsupervised manner. More related loss functions are also proposed to further reduce the solution space. Benefiting from these, we recover more authentic tone and better contrast (see \cref{Fig1}). Overall, our contributions are as follows:
\begin{itemize}
    \item[$\bullet$]
    We are the first to utilize the controllable fitting capability of neural representation in low-light image enhancement. It normalizes lightness degradation and removes natural noise without any additional operations, providing new ideas for future work.
    \item[$\bullet$]
    For the first time, we introduce multi-modal learning to low-light image enhancement. Benefiting from its efficient vision-language priors, our method learns diverse features, resulting in perceptually better results.
    \item[$\bullet$]
    We develop an unsupervised cooperative adversarial learning strategy to ease the reliance on the paired training data. In which the appearance-based discrimination ensures authenticity from both color and detail levels, improving the quality of the restored results.
    \item[$\bullet$]
    Extensive experiments are conducted on representative benchmarks, manifesting the superiority of our NeRCo against a rich set of state-of-the-art algorithms. Especially, it even outperforms some supervised methods.
\end{itemize}


% To address these issues, in this paper, our key insights are: i) normalizing the input with a controllable fitting function to reduce the unpredictable degradation features in real-world scenarios, ii) supervising the output with different modalities to achieve both metric-favorable and perceptual-oriented enhancement, and iii) developing an unsupervised training strategy to ease the reliance on the paired data. 
% 1
% To this end, on the one hand, for the input image, we adopt neural representation to reproduce it before the enhancement operation. By manipulating the positional encoding, we selectively avoid regenerating extreme degradation, which objectively realizes normalization and thereby decreases enhancement difficulty. 
% 2
% On the other hand, for the output image, we employ multi-modal learning to supervise it with both textual and image information. Compared with image supervision, which contains varying brightness across different samples, the feature space of the designed prompt is more stable and accurate in describing brightness. During training, our intermediate results are not only encouraged to be similar to references, but also forced to match their related prompts. In this way, we bridge the gap between the metric-favorable and the perceptual-friendly versions. 
% 3
% Finally, to address the limitation of paired low-light-normal-light training data, we propose to train the enhancement module with a dual-closed-loop cooperative adversarial constraint procedure, which learns in an unsupervised manner. More related loss functions are also developed to further reduce the solution space. 
% Benefiting from these, we recover more authentic tone and better contrast (see \cref{Fig1}). Overall, our contributions are as follows:
% \begin{itemize}
%     \item[$\bullet$]
%     We are the first to utilize the controllable fitting capability of neural representation in low-light image enhancement. It normalizes lightness degradation and removes natural noise without any additional operations, providing new ideas for future work.
%     \item[$\bullet$]
%     For the first time, we introduce multi-modal learning to low-light image enhancement. Benefiting from its efficient vision-language priors, our method learns diverse features, resulting in perceptually better results.
%     \item[$\bullet$]
%     We develop an unsupervised cooperative adversarial learning strategy to ease the reliance on the paired training data. Besides, a color-edge discriminative structure ensures authenticity from both tone and detail levels, improving the quality of restored results.
%     \item[$\bullet$]
%     Extensive experiments are conducted on representative benchmarks, manifesting the superiority of our NeRCo against a rich set of state-of-the-art methods.
% \end{itemize}

%-------------------------------------------------------------------------

%------------------------------------------------------------------------
\section{Related Work}
\subsection{Low-light Image Enhancement}
To improve the visibility of low-light images, model-based methods are first widely adopted. Retinex theory \cite{Zia-ur_2004_JEI} decomposes the observation into illumination and reflectance (\textit{i.e.}, clear prediction), but tends to over-expose the appearance. Various hand-crafted priors are further introduced into models as regularization terms. Fu~\textit{et al.} \cite{Fu_2016_CVPR} developed a weighted variational model to simultaneously estimate reflectance and illumination layers. Cai~\textit{et al.} \cite{Cai_2017_ICCV} proposed an edge-preserving smoothing algorithm to model brightness. Guo~\textit{et al.} \cite{Guo_2017_TIP} predicted the illumination by adopting the relative total variation \cite{Xu_2012_TOG}. However, these defined priors are labor-intensive and perform poor generalization towards real-world scenarios.

Due to these limitations, researchers took advantage of deep learning to recover in a data-driven manner \cite{Yan_2016_TOG,Lore_2017_PR,Cai_2018_TIP,Guo_2020_CVPR,Liu_2021_CVPR,Zhang_2021_ICCV,Xu_2022_CVPR,Wu_2022_CVPR}, which exploits priors from massive data automatically. For example, Guo~\textit{et al.} \cite{Guo_2020_CVPR} formulated light enhancement as a task of image-specific curve estimation with a lightweight deep model. Jiang~\textit{et al.} \cite{Jiang_2021_TIP} introduced adversarial training for learning from unpaired supervision. Wei~\textit{et al.} \cite{LOL} designed an end-to-end trainable RetinexNet but still troubled by heavy noise. To ameliate it, Zhang~\textit{et al.} \cite{Zhang_2021_IJCV} tuned up model structure and developed denoising training loss. Zhang~\textit{et al.} \cite{SSIENet} proposed a decomposition-type architecture to impose constraint on reflectance. Liu~\textit{et al.} \cite{Liu_2021_CVPR} employed architecture search and built an unrolling network. Although these well-designed models have realized impressive effectiveness, they are not stable in real-world applications. To improve robustness, we pre-modulate the degradation to a uniform level with neural representation before the enhancement procedure.

\begin{figure*}[t]
  \centering
  \begin{subfigure}{1\linewidth}
    \includegraphics[width=1.\linewidth]{Image/flowimage/1.pdf}
  \end{subfigure}
  \vspace{-1.1em}
  
  \caption{Workflow of our NeRCo. It presents a cooperative adversarial enhancement process containing dual-closed-loop branches, each of which contains an enhancement operation and a degradation operation. We embed a Mask Extractor (ME) to portrait the degradation distribution and a Neural Representation Normalization (NRN) module to normalize the degradation condition of the input low-light image. All of them are trained together to constrain each other, locking on to a more accurate target domain. The \textcolor{red}{red} means the transfer of the attention map.}
  \label{totnet}
  \vspace{-1.3em}
  
\end{figure*}

\subsection{Neural Representation for Images}
Recently, neural representation has been widely adopt to depict images. Chen~\textit{et al.} \cite{Chen_2021_CVPR} firstly utilized implicit image representation for continuous image super-resolution. However, the MultiLayer Perceptron (MLP) tends to distort high-frequency components. To address this issue, Lee~\textit{et al.} \cite{Lee_2022_CVPR} developed a dominant-frequency estimator to predict local texture for natural images. Lee~\textit{et al.} \cite{Lee_2022_ECCV} further utilized implicit neural representation to warp images into continuous shape. Dupont~\textit{et al.} \cite{Dupont_2022_pmlr} tried to produce different objects with one MLP by manipulating the latent code from its hidden layers. Saragadam~\textit{et al.} \cite{Saragadam_2022_ECCV} adopted multiple MLPs to represent a single image in a multi-scale manner. Sun~\textit{et al.} \cite{Sun_2021_TCI} predicted continuous information based on the captured tomographic features. Tancik~\textit{et al.} \cite{Tancik_2021_CVPR} introduced meta-learning to initialize the parameters of MLP to accelerate training. Reed~\textit{et al.} \cite{Reed_2021_ICCV} adopted neural representation and parametric motion fields to predict the shape and location of organs. Further, some researchers adopted neural representation to compress videos \cite{Bai_2022_arxiV,Chen_2021_NIPS,Zhang_2022_ICLR}.

However, existing neural representation is mainly applied on image compressing, denoising and depicting continuous information, \textit{etc.} We are the first to apply its controllable fitting capability to low-light image enhancement.

\subsection{Multi-modal Learning}
In recent years, learning across visual and linguistic modalities has attracted extensive attention. Various vision-language models are developed. Radford~\textit{et al.} \cite{Radford_2021_ICML} proposed to learn visual model from language supervision, called CLIP. After training on 400 million image-text pairs, it can describe any visual concept with natural language and transfer to other tasks without any specific training. Furthermore, Zhou~\textit{et al.} \cite{Zhou_2022_IJCV} developed soft prompts to replace the hand-crafted ones, which uses learnable vectors to model context words and obtains task-relevant context. To further refine prompts to the instance-level, Rao~\textit{et al.} \cite{Rao_2022_CVPR} designed context-aware prompting to combine prompts with visual features. Cho~\textit{et al.} \cite{Cho_2021_ICML} shared priors across different tasks by updating a uniform framework towards a common target of seven multi-modal tasks. Ju~\textit{et al.} \cite{Ju_2022_ECCV} adopted the pre-trained CLIP model to video understanding.

Existing methods mainly focus on high-level computer vision tasks such as image classification. For the first time, we apply priors of the pre-trained vision-language model to low-light image enhancement, developing semantic-oriented guidance and realizing better performance.

%------------------------------------------------------------------------

%------------------------------------------------------------------------
\section{Our Method}
\subsection{Framework Architecture}
%To normalize real-world degradation, produce both metric-favorable and visual-friendly results, and ease the reliance on the real-world paired data, we develop an unsupervised framework called NeRCo.

As shown in \cref{totnet}, given an input low-light image $\textit{\textbf{I}}_{\rm{L}}$, we first normalize it by neural representation (NRN, \cref{sec:NR}) to improve the robustness of the model to different degradation conditions. Then the Mask Extractor (ME, \cref{sec:DLGP}) module extracts the attention mask from the image to guide the enhancement of different regions. After that, the Enhance Module $\textit{G}_{H}$ (represented by ResNet) generates an intermediate high-light image $\widetilde{\textbf{\textit{I}}}_{\rm{H}}$. To ensure the quality of generated images, we design a  Text-driven Appearance Discriminator (TAD, \cref{sec:TAD}) to supervise image generation, where text-driven supervision guarantees semantic reliability, and appearance supervision guarantees visual reliability. The generated high-light image $\widetilde{\textbf{\textit{I}}}_{\rm{H}}$ is then passed through the Degrade Module $\textit{G}_{L}$ to convert the image back to the low-light domain $\widetilde{\widetilde{\textbf{\textit{I}}}}_{\rm{L}}$ and calculate the consistency loss (\cref{sec:DLGP}) with the original low-light image $\textit{\textbf{I}}_{\rm{L}}$. The upper right branch of \cref{totnet} inputs the high-light image $\textit{\textbf{I}}_{\rm{H}}$, implemented in a similar way.

The network is implemented in a dual-loop way (\cref{sec:DLGP}) to achieve stable constraints based on the unpaired data in an unsupervised manner. It operates bidirectional mapping: enhance-degrade ($\textit{\textbf{I}}_{\rm{L}} \to \widetilde{\textbf{\textit{I}}}_{\rm{H}} \to \widetilde{\widetilde{\textbf{\textit{I}}}}_{\rm{L}}$) and degrade-enhance ($\textit{\textbf{I}}_{\rm{H}} \to \widetilde{\textbf{\textit{I}}}_{\rm{L}} \to \widetilde{\widetilde{\textbf{\textit{I}}}}_{\rm{H}}$). This dual loop constraint fully exploits the latent general distinction between the low-light and high-light domains. Besides, the cooperative loss (\cref{sec:DLGP}) encourages all components in the framework to supervise each other collaboratively, which further reduces the solution space.

%Specifically, for the input low-light image $\textit{\textbf{I}}_{\rm{L}}$, before enhancement operation, we use ME to extract its brightness mask and use BPP to normalize its degradation features. The former guides the subsequent generation procedure. The latter reduces variance of the degradation across different scenarios, which decreases enhancement difficulty and is illustrated in \cref{sec:NR}. After predicting the intermediate results (\textit{e.g.}, $\widetilde{\textbf{\textit{I}}}_{\rm{H}}$ and $\widetilde{\textbf{\textit{I}}}_{\rm{L}}$ in \cref{totnet}), we adopt diverse features from different modalities for comprehensive supervision, which is denoted as Text-driven Appearance Discriminator (TAD) in \cref{totnet} and demonstrated in \cref{sec:TAD}. Since priors of a pre-trained vision-language model introduce semantic-oriented guidance while reference images provide low-level constraints, we bridge the gap between the perceptual-friendly and metric-favorable versions. Furthermore, considering detail distortion during image processing, we embed a high-frequency path in TAD to realize the trade-off between color and texture. 
%Finally, to achieve stable constraint based on the unpaired data, we develop a dual-loop generation procedure. It operates bidirectional mapping instead of directional low-light-to-high-light mapping. Details are given in \cref{sec:DLGP}. Through this dual loop constraint, the latent general distinction between the low-light and high-light domains is fully exploited. In \cref{sec:DLGP}, the cooperative objective function we designed is discussed in detail. It encourages all components in the framework to supervise each other in a collaborative manner, which further reduces the solution space.

% It forces all components in the framework to supervise each other, which further reduces the solution space and improves model effectiveness.

% It forces all components in the framework to supervise each other in a collaborative manner. Since this function reduces the solution space, model performance is further improved.

During \textbf{training}, we run the whole process in \cref{totnet}. We input two images (\textit{i.e.}, low-light $\textit{\textbf{I}}_{\rm{L}}$ and high-light $\textit{\textbf{I}}_{\rm{H}}$). $\textit{\textbf{I}}_{\rm{L}}$ is enhanced to $\widetilde{\textbf{\textit{I}}}_{\rm{H}}$, then translated back to low-light $\widetilde{\widetilde{\textbf{\textit{I}}}}_{\rm{L}}$. And vice versa for $\textit{\textbf{I}}_{\rm{H}}$. Noting that $\textit{\textbf{I}}_{\rm{H}}$ is used for training purposes only, \textit{i.e.}, training model in an unsupervised manner for better enhancement rather than degradation. Hence, we only use NRN to enhance $\textit{\textbf{I}}_{\rm{L}}$ but not degrade $\textit{\textbf{I}}_{\rm{H}}$. For \textbf{inference}, $\textit{\textbf{I}}_{\rm{L}}$ is directly enhanced to $\widetilde{\textbf{\textit{I}}}_{\rm{H}}$ without any other operations, as shown in the top left part of \cref{totnet}.

\subsection{Neural Representation for Normalization}
\label{sec:NR}
% As shown in the top row of \cref{NRPP}, in real-world applications, algorithms have to face scenarios with different degradation levels (\textit{e.g.}, brightness and noise). These various input is challenging for a trained model. Inspired by Neural Representation (NR), we attempt to normalize the degradation level before enhancement operation. By constraining these variable degradation signals, we reduce the difficulty of subsequent task.
\textbf{Motivation.} Images captured in real-word typically exhibit varying degradation levels due to lighting conditions or camera parameters, as shown in Fig.~\ref{NRPP} (a)(b). We report their pixel value distribution on the Y channel in Fig.~\ref{NRPP} (c). The inconsistency between these samples is challenging for a well-trained model. We attempt to normalize degradation level (see Fig.~\ref{NRPP} (d)(e)) with neural representations (NR) to obtain a more consistent degradation distribution (see Fig.~\ref{NRPP} (f)) to reduce the difficulty of subsequent operations.

\textbf{Neural Representation.} Concretely, in NR, image $\textit{\textbf{I}}_{\rm{L}}$ is transformed into a feature map $\textbf{E} \in \mathbb{R}^{H \times W \times C}$, where \textit{H} and \textit{W} are image resolution. While the location of each pixel is recorded in a coordinate set $\textbf{X} \in \mathbb{R}^{H \times W \times 2}$, where 2 means horizontal and vertical coordinates. $\textit{\textbf{I}}_{\rm{L}}$ can thus be represented by its features and a set of coordinates. As shown in the Neural Representation Normalization (NRN) module of \cref{totnet}, we fuse $\textbf{X}$ and $\textbf{E}$, and use a decoding function $\textbf{\textit{F}}_{\rm{MLP}}$ to output image $\textit{\textbf{I}}_{\rm{NR}}$, which is parameterized as a MultiLayer Perceptron (MLP). The neural representation of the image is expressed as:
\begin{eqnarray}
\begin{aligned}
	\label{shi10}
	\textit{\textbf{I}}_{\rm{NR}}[\textit{i}, \textit{j}] = \textbf{\textit{F}}_{\rm{MLP}}(\textbf{E}[\textit{i}, \textit{j}],\textbf{X}[\textit{i}, \textit{j}]),
\end{aligned}
\end{eqnarray}
% In NR, each image $\textit{\textbf{I}}_{\rm{L}}$ is transformed into a 2D feature map $\textbf{E} \in \mathbb{R}^{H \times W \times C}$, where \textit{H} and \textit{W} are image resolution. $\textit{\textbf{I}}_{\rm{L}}$ can thus be represented by coordinates and related features. As shown in the Brightness Pre-Process (BPP) module of \cref{totnet}, we first generate a coordinate set $\textbf{X} \in \mathbb{R}^{H \times W \times 2}$ to record the location of each pixel, where 2 means horizontal and vertical coordinates. Then an image feature $\textbf{E}$ is extracted with the encoder. $\textbf{X}$ and $\textbf{E}$ are fused and fed to a decoding function $\textbf{\textit{F}}_{\rm{MLP}}$, which is parameterized as a MultiLayer Perceptron (MLP). Expressed as:
% \begin{eqnarray}
% \begin{aligned}
% 	\label{shi10}
% 	\textit{\textbf{I}}_{\rm{NR}}[\textit{i}, \textit{j}] = \textbf{\textit{F}}_{\rm{MLP}}(\textbf{E}[\textit{i}, \textit{j}],\textbf{X}[\textit{i}, \textit{j}]),
% \end{aligned}
% \end{eqnarray}
where $[\textit{i}, \textit{j}]$ is the location of a pixel and $\textit{\textbf{I}}_{\rm{NR}}[\textit{i}, \textit{j}]$ is the generated RGB value. By predicting RGB of each pixel, an image $\textit{\textbf{I}}_{\rm{NR}}$ is reproduced. We encourage $\textit{\textbf{I}}_{\rm{NR}}$ to be similar to $\textit{\textbf{I}}_{\rm{L}}$ through $\textit{l}_1$-norm. This NR-related loss expressed as:
\begin{eqnarray}
\begin{aligned}
	\mathcal{L}_{NR} = ||\textit{\textbf{I}}_{\rm{NR}}-\textit{\textbf{I}}_{\rm{L}}||_1.
\end{aligned}
\end{eqnarray}

\textbf{Why Neural Representation Works.} With the trained $\textbf{\textit{F}}_{\rm{MLP}}$, each feature map $\textbf{E}$ can form a function $\textbf{\textit{F}}_{\rm{MLP}}(\textbf{E},\cdot) : \textbf{X} \rightarrow \textit{\textbf{I}}_{\rm{NR}}$, which maps coordinates to its predicted RGB values. Without $\textbf{E}$, it is impossible for $\textbf{\textit{F}}_{\rm{MLP}}$ to depict various RGB values with the same coordinates $\textbf{X}$. Without $\textbf{X}$, we cannot normalize degradation by adjusting fitting capability, which is explained below.

\begin{figure}[t]
  \centering
  \begin{subfigure}{0.99\linewidth}
    \includegraphics[width=1.\linewidth]{Image/re/aa.pdf}
  \end{subfigure}
  \vspace{-1.em}

  \caption{Comparisons between the captured low-light scenes (top row) and the results of NRN (bottom row). The low-light samples are from the SICE \cite{Cai_2018_TIP} dataset. It contains numerous image sets, each with common content and varying lighting conditions. The pixel value distribution of images on the Y channel is given on the right. One can see that NRN normalizes the brightness to be similar.}
  \label{NRPP}
  \vspace{-1.7em}
\end{figure}

\begin{figure*}[t]
  \centering
  \begin{subfigure}{1\linewidth}
    \includegraphics[width=1.\linewidth]{Image/flowimage/discr.pdf}
  \end{subfigure}
  \vspace{-1.1em}
  
  \caption{The details of our proposed text-driven appearance discriminator (in the left region) and our collaborative attention module (in the right region). The former supervises the input with text and image modalities, and focuses on high-frequency components. The latter adjusts attention to different channels and outputs attention map.}
  \label{discr}
  \vspace{-1.5em}
  
\end{figure*}

According to \cite{Rahaman_2019_ICML}, neural networks tend to portrait lower frequency information. Despite our decoding function can approximate RGB values, some high-frequency components may be discarded during rendering. For example, for adjacent pixels around the edge, their RGB values vary a lot but coordinates vary little. It means $\textbf{\textit{F}}_{\rm{MLP}}$ should output different results based on similar inputs, which is difficult. Inspired by \cite{Mildenhall_2020_ECCV}, to fit high-frequency variation better, we map the input coordinates to a higher dimensional space before passing them to $\textbf{\textit{F}}_{\rm{MLP}}$, which is called positional encoding. As shown in the gray region of \cref{totnet}, before fusing coordinates with image feature, we use a high-frequency function $\boldsymbol{\gamma}(\cdot)$ to map the original coordinates $\textbf{\textit{x}}$ from $\mathbb{R}$ into a higher dimensional space $\mathbb{R}^{2L}$, expressed as:
\begin{eqnarray}
\begin{aligned}
	\label{shi11}
	\boldsymbol{\gamma}(\textbf{\textit{x}}) = (\cdot\cdot\cdot, \rm{sin}(2^{\textit{i}}\pi\textbf{\textit{x}}), \rm{cos}(2^{\textit{i}}\pi\textbf{\textit{x}}), \cdot\cdot\cdot),
	\\
\end{aligned}
\end{eqnarray}
where $\textit{i}$ values from 0 to $\textit{L} - 1$, $\textit{L}$ is a hyperparameter that determines the dimension value. The final coordinates are composed as: $\textbf{\textit{x}}^{\prime} = \boldsymbol{\gamma}(\textbf{\textit{x}})$. Noting that by manipulating the value of $\textit{L}$, we can change the fitting capacity of our NRN module, \textit{i.e.}, a bigger $\textit{L}$ results in a more precise fit.

However, stronger fitting capability is not always better. Since NRN aims to normalize various degradation, its output is not expected to be exactly the same as the input, especially the degradation components. We want to choose an $\textit{L}$ that does not overfit to remain all information while faithfully preserves desired content. Chen \textit{et al.} \cite{Chen_2021_NIPS} have demonstrated that NR is robust to perturbations and can denoise without any special design. We think this is because MLPs lack spatial correlation priors, some extreme information is thus hard to be reproduced faithfully. Hence, this underfitting property objectively limits the unpredictable degradation. We further found in experiments that MLPs tend to learn an average brightness range of training data rather than fitting the unique brightness of different images. As shown in \cref{NRPP}, the output lightness of a trained MLP is similar. By visualizing their pixel value distribution in the Y channel, we further prove the normalization property of our NRN. More results are given in the \textcolor{red}{supplementary material (SM)}. Thereby, we set $\textit{L}$ = 8 for the trade-off between degradation normalization and content fidelity. By reducing these fickle degradation signals, we decrease the difficulty of subsequent enhancement procedure.



% Compared with CNN-based models, MLPs inherently lack spatial correlation priors. Since Chen \textit{et al.} \cite{Chen_2021_NIPS} has demonstrated their robustness to perturbations, some extreme high-frequency information such as noise will not be reproduced faithfully. Besides, MLPs tend to learn an average brightness range of training data rather than fitting variable brightness levels across different images. The lightness of their outputs will be limited to this range.

% Text-driven Color-edge Discrimination
\subsection{Text-driven Appearance Discriminator}
\label{sec:TAD}
\textbf{Motivation.} Existing methods mainly adopt image-level supervision, \textit{i.e.}, forcing the output to be close to the target images. However, the brightness across different references varies greatly, confusing model training; some references are visually poor (\textit{e.g.}, unnatural lightness), leading to visual-unfriendly results. To reduce training difficulty and bridge the gap between the metric-favorable and visual-friendly versions, we design a Text-driven Appearance Discriminator (TAD) to supervise image generation from the semantic level and appearance level, respectively.

\textbf{Text-driven Discrimination}. We denote the low-light domain as $\boldsymbol{\mathcal{L}}$ and the high-light domain as $\boldsymbol{\mathcal{H}}$. As shown in \cref{discr}, we introduce multi-modal learning to supervise images with both image and text modalities. Concretely, inspired by Radford~\textit{et al.} \cite{Radford_2021_ICML}, we employ the recent well-known CLIP model to get efficient priors. It consists of two pre-trained encoders, \textit{i.e.}, $\textbf{\textit{Enc}}_{\rm{t}}$ for text and $\textbf{\textit{Enc}}_{\rm{i}}$ for image. We first manually design two prompts, \textit{i.e.}, $\textit{low-light image}$ and $\textit{high-light image}$, to describe $\boldsymbol{\mathcal{L}}$ and $\boldsymbol{\mathcal{H}}$ respectively, denoted as $\textit{\textbf{T}}_{\rm{L}}$ and $\textit{\textbf{T}}_{\rm{H}}$ in \cref{discr}. Experiments on more other texts are given in the \textcolor{red}{SM}. $\textbf{\textit{Enc}}_{\rm{t}}$ extracts two feature vectors of size $1 \times 512$ from two prompts. Similarly, $\textbf{\textit{Enc}}_{\rm{i}}$ extracts a vector of the same size from our intermediate result $\textit{\textbf{I}}$. We compute the cosine similarity between the image vector and the text vector to measure their discrepancy, formulated as:
\begin{eqnarray}
\begin{aligned}
	\label{cosh}
	\mathcal{D}_{cos}(\textit{\textbf{I}}, \textit{\textbf{T}}) &= \frac{\langle \textbf{\textit{Enc}}_{\rm{i}}(\textit{\textbf{I}}), \textbf{\textit{Enc}}_{\rm{t}}(\textit{\textbf{T}})\rangle}{||\textbf{\textit{Enc}}_{\rm{i}}(\textit{\textbf{I}})|| \, ||\textbf{\textit{Enc}}_{\rm{t}}(\textit{\textbf{T}})||},
\end{aligned}
\end{eqnarray}
where $\textit{\textbf{I}}$ denotes the predicted image and $\textit{\textbf{T}}$ is the prompt. For the enhanced results (\textit{e.g.}, $\widetilde{\textbf{\textit{I}}}_{\rm{H}}$ and $\widetilde{\widetilde{\textbf{\textit{I}}}}_{\rm{H}}$ in \cref{totnet}), we encourage their vectors to be similar to those of $\textit{\textbf{T}}_{\rm{H}}$ and away from $\textit{\textbf{T}}_{\rm{L}}$, and vice versa for low-light predictions. In this way, we encourage semantically consistent outputs. This cosine objective function is formulated as:
\begin{equation}
\begin{aligned}
	\label{cosh}
	\mathcal{L}_{cos}(\textit{\textbf{I}}_{\rm{H}},\textit{\textbf{T}}_{\rm{L}},\textit{\textbf{T}}_{\rm{H}}) = \mathcal{D}_{cos}(\textit{\textbf{I}}_{\rm{H}},\textit{\textbf{T}}_{\rm{L}}) - \mathcal{D}_{cos}(\textit{\textbf{I}}_{\rm{H}},\textit{\textbf{T}}_{\rm{H}}),
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
	\mathcal{L}_{cos}(\textit{\textbf{I}}_{\rm{L}},\textit{\textbf{T}}_{\rm{L}},\textit{\textbf{T}}_{\rm{H}}) = \mathcal{D}_{cos}(\textit{\textbf{I}}_{\rm{L}},\textit{\textbf{T}}_{\rm{H}}) - \mathcal{D}_{cos}(\textit{\textbf{I}}_{\rm{L}},\textit{\textbf{T}}_{\rm{L}}).
\end{aligned}
\end{equation}

\textbf{Appearance-based Discrimination.} Admittedly, text descriptions cannot provide low-level guidance like images. To generate faithful content, image supervision is necessary. As shown in the purple region of \cref{discr}, we stack a discriminator to distinguish the predicted results from real images, encouraging image-level authenticity (\textit{e.g.}, color, texture, and structure). Considering detail distortion in image processing, we embed a high-frequency path consisting of a high-pass filter and a discriminator of the same structure. The filter extracts high-frequency components and the discriminator supervises them at edge-level. Based on this double-path color-edge discriminative structure, we realize the trade-off between color and detail.

During training, TAD plays an adversarial role in learning bidirectional mapping relationship between $\boldsymbol{\mathcal{L}}$ and $\boldsymbol{\mathcal{H}}$. We develop an adversarial loss on each generation loop to realize it. As shown in \cref{totnet}, for the enhancement operation $\textit{G}_{H}(\cdot)$: $\textit{\textbf{I}}_{\rm{L}}$ $\rightarrow$ $\widetilde{\textbf{\textit{I}}}_{\rm{H}}$, we apply a TAD module and dub its appearance discrimination as $\textit{D}_{H}$. Developing an adversarial objective function as:
\begin{small}
\begin{equation}
\begin{aligned}
	\label{shi3}
	\mathcal{L}_{adv}(\textit{\textbf{I}}_{\rm{L}},\textit{\textbf{I}}_{\rm{H}},\textit{G}_{H},\textit{D}_{H}) &= \mathcal{L}_{cos}(\textit{G}_{H}(\textit{\textbf{I}}_{\rm{L}}),\textit{\textbf{T}}_{\rm{L}},\textit{\textbf{T}}_{\rm{H}}) \\
	&+ \mathbb{E}_{\textit{\textbf{I}}_{\rm{H}} \sim \boldsymbol{\mathcal{H}}}[\log\textit{D}_{H}(\textit{\textbf{I}}_{\rm{H}})] \\
	&+ \mathbb{E}_{\textit{\textbf{I}}_{\rm{L}} \sim \boldsymbol{\mathcal{L}}}[\log(1-\textit{D}_{H}(\textit{G}_{H}(\textit{\textbf{I}}_{\rm{L}})))],
\end{aligned}
\end{equation}
\end{small}where $\textit{D}_{H}$ aims to determine whether an image is captured or generated, that is, distinguish the enhanced results $\textit{G}_{H}(\textit{\textbf{I}}_{\rm{L}})$ from the real high-light domain $\boldsymbol{\mathcal{H}}$. While $\textit{G}_{H}(\cdot)$ aims at deceiving $\textit{D}_{H}$, \textit{i.e.}, generating results close to $\boldsymbol{\mathcal{H}}$. Simultaneously, the aforementioned cosine constraint is also adopted to supervise $\textit{G}_{H}(\cdot)$. The reverse mapping $\textit{G}_{L}(\cdot)$: $\textit{\textbf{I}}_{\rm{H}}$ $\rightarrow$ $\widetilde{\textbf{\textit{I}}}_{\rm{L}}$ adopts a similar objective function, which is supervised by another TAD.
%We denote its double-path discriminator as $\textit{D}_{L}$ and formulate as:
%\begin{small}
%\begin{equation}
%\begin{aligned}
%	\label{shi4}
%	\mathcal{L}_{adv}(\textit{\textbf{I}}_{\rm{H}},\textit{\textbf{I}}_{\rm{L}},\textit{G}_{L},\textit{D}_{L}) &= \mathcal{L}_{cos}(\textbf{\textit{Enc}}_{\rm{i}}(\textit{G}_{L}(\textit{\textbf{I}}_{\rm{H}})),\cdot) \\
%	&+ \mathbb{E}_{\textit{\textbf{I}}_{\rm{L}} \sim \boldsymbol{\mathcal{L}}}[\log\textit{D}_{L}(\textit{\textbf{I}}_{\rm{L}})] \\
%	&+ \mathbb{E}_{\textit{\textbf{I}}_{\rm{H}} \sim \boldsymbol{\mathcal{H}}}[\log(1-\textit{D}_{L}(\textit{G}_{L}(\textit{\textbf{I}}_{\rm{H}})))].
%\end{aligned}
%\end{equation}
%\end{small}

In both functions, generators try to minimize the objective, whilst TAD modules maximize it. During this adversarial learning, our model realizes better semantic consistency, which is proved by experiments presented in \textcolor{red}{SM}.
%------------------------------------------------------------------------

\subsection{Dual Loop Generation Procedure}
\label{sec:DLGP}
Previous methods mainly map the low-light image $\textit{\textbf{I}}_{\rm{L}}$ to its high-light version $\widetilde{\textbf{\textit{I}}}_{\rm{H}}$ directly. To provide stable constraint without the paired data, we stack a forward enhancement module with a backward degradation one for bidirectional mapping, which operates in an unsupervised manner.

\textbf{Dual Loop.} Specifically, the forward enhancement procedure aims to realize the mapping $\textit{G}_{H}(\cdot)$: $\textit{\textbf{I}}_{\rm{L}}$ $\rightarrow$ $\widetilde{\textbf{\textit{I}}}_{\rm{H}}$. While the other does the opposite, \textit{i.e.}, depicting a low-light scene from a clean image $\textit{\textbf{I}}_{\rm{H}}$ with the mapping $\textit{G}_{L}(\cdot)$: $\textit{\textbf{I}}_{\rm{H}}$ $\rightarrow$ $\widetilde{\textbf{\textit{I}}}_{\rm{L}}$. Our generation loop is composed of alternating these two operations. As shown in the left end of \cref{totnet}, our input is the observed low-light image. It is first extracted an attention-guidance $\textit{\textbf{I}}_{\rm{A}}$ and normalized by a neural representation module. Then the subsequent procedure first translates it to the high-light domain, and maps the enhanced image $\widetilde{\textbf{\textit{I}}}_{\rm{H}}$ back to the low-light version $\widetilde{\widetilde{\textbf{\textit{I}}}}_{\rm{L}}$. This enhancement-degradation generation branch is formulated as:
\begin{eqnarray}
	\label{shi1}
	\widetilde{\widetilde{\textbf{\textit{I}}}}_{\rm{L}} = \textit{G}_{L}( \widetilde{\textbf{\textit{I}}}_{\rm{H}}) = \textit{G}_{L}(\textit{G}_{H}(\textit{ME}(\textbf{\textit{I}}_{\rm{L}}) \otimes \textit{NRN}(\textbf{\textit{I}}_{\rm{L}}))),
\end{eqnarray}
where $\textit{G}_{H}(\cdot)$ and $\textit{G}_{L}(\cdot)$ denote enhancement and degeneration operations, respectively, and $\textit{NRN}$ is the neural representation normalization module discussed in \cref{sec:NR}. $\textit{ME}$ means mask extractor, as shown in the green region of \cref{totnet}, in which we develop a Collaborative Attention Module (CAM) to extract the attention map $\textit{\textbf{I}}_{\rm{A}}$. The details of CAM are displayed at the right end of \cref{discr}.

The degeneration-enhancement generation branch, as shown in the right end of  \cref{totnet}, is formulated similarly:
%For the other branch, as shown in the right end of \cref{totnet}, we input a high-light image with completely different content. With the attention guidance of $\textit{\textbf{I}}_{\rm{A}}$, it is first translated to the low-light version $\widetilde{\textbf{\textit{I}}}_{\rm{L}}$ with the characteristics of dark environments. Then the next enhancement module recovers a high-light image $\widetilde{\widetilde{\textbf{\textit{I}}}}_{\rm{H}}$ from it again, expressed as:
\begin{eqnarray}
	\label{shi2}
	\widetilde{\widetilde{\textbf{\textit{I}}}}_{\rm{H}} = \textit{G}_{H}(\widetilde{\textbf{\textit{I}}}_{\rm{L}}) = \textit{G}_{H}(\textit{G}_{L}(\textit{ME}(\textbf{\textit{I}}_{\rm{L}}) \otimes \textbf{\textit{I}}_{\rm{H}})).
\end{eqnarray}

To constrain this bidirectional mapping, during training, we develop cycle consistency to directly impose supervision at the pixel-level. For example, for the left branch in \cref{totnet}, we ensure that: $\textbf{\textit{I}}_{\rm{L}} \approx \widetilde{\widetilde{\textbf{\textit{I}}}}_{\rm{L}} = \textit{G}_{L}(\textit{G}_{H}(\textbf{\textit{I}}_{\rm{L}}))$. Accordingly, the other cycle follows: $\textbf{\textit{I}}_{\rm{H}} \approx \widetilde{\widetilde{\textbf{\textit{I}}}}_{\rm{H}} = \textit{G}_{H}(\textit{G}_{L}(\textbf{\textit{I}}_{\rm{H}}))$. We adopt the $\textit{l}_1$-norm to measure discrepancy and develop the consistency constraint as:
\begin{eqnarray}
\begin{aligned}
	\label{shi5}
	\mathcal{L}_{con} &= \mathbb{E}_{\textit{\textbf{I}}_{\rm{L}} \sim \boldsymbol{\mathcal{L}}}[||\textit{G}_{L}(\textit{G}_{H}(\textit{\textbf{I}}_{\rm{L}}))-\textit{\textbf{I}}_{\rm{L}}||_1]\\ &+ \mathbb{E}_{\textit{\textbf{I}}_{\rm{H}} \sim \boldsymbol{\mathcal{H}}}[||\textit{G}_{H}(\textit{G}_{L}(\textit{\textbf{I}}_{\rm{H}}))-\textit{\textbf{I}}_{\rm{H}}||_1].
\end{aligned}
\end{eqnarray}

% 这一段合并到公式（9）下面去了
%It is worth noting that we elaborate an efficient mask extractor for better attention guidance. As shown in the green region of \cref{totnet}, we develop a Collaborative Attention Module (CAM) to portrait the attention map $\textit{\textbf{I}}_{\rm{A}}$. The details of CAM are displayed at the right end of \cref{discr}. Specifically, we first apply average and max pooling operations on the resolution dimension to get a feature vector, based on which fully-connected layers predict weights for each channel. Then, we apply these weights to the input to focus on important channels. Finally, pooling operations fuse information on the channel dimension and produce $\textit{\textbf{I}}_{\rm{A}}$. 

\begin{figure}[t]
  \centering
  \begin{subfigure}{1\linewidth}
    \includegraphics[width=1.\linewidth]{Image/flowimage/loss.pdf}
  \end{subfigure}
  \vspace{-1.1em}
  
  \caption{Details of our proposed cooperative loss function.}
  
  \label{loss}
  \vspace{-1.6em}
  
\end{figure}

\begin{table*}[t]
	\centering
	\resizebox{\linewidth}{!}{
	\begin{tabular}{c|c|c c|c c c|c c c c c|c}
		\toprule[1.5pt]
		\multirow{2}{*}{Datasets} & \multirow{2}{*}{Metrics} & \multicolumn{2}{c|}{\cellcolor{gray!40}\emph{Model-based Methods}} & \multicolumn{3}{c|}{\cellcolor{gray!40}\emph{Supervised Learning Methods}} & \multicolumn{6}{c}{\cellcolor{gray!40}\emph{Unsupervised Learning Methods}} \\ \cline{3-13}
		& & \cellcolor{gray!40}LECARM & \cellcolor{gray!40}SDD & \cellcolor{gray!40}RetinexNet & \cellcolor{gray!40}KinD & \cellcolor{gray!40}URetinexNet & \cellcolor{gray!40}ZeroDCE & \cellcolor{gray!40}SSIENet & \cellcolor{gray!40}RUAS & \cellcolor{gray!40}EnGAN & \cellcolor{gray!40}SCI & \cellcolor{gray!40}Ours \\ \bottomrule
        \toprule
		\multirow{4}{*}{LOL \cite{LOL}} & PSNR $\uparrow$ & 14.41 & 13.34 & 16.77 & 17.65 & \textbf{\color{blue}19.54} & 14.80 & 19.50 & 16.40 & 17.48 & 14.78 & \textbf{\color{red}19.84} \\
		& SSIM $\uparrow$ & 0.5448 & 0.6342 & 0.4249 & 0.7614 & \textbf{\color{blue}0.7621} & 0.5607 & 0.7003 & 0.5034 & 0.6515 & 0.5254 & \textbf{\color{red}0.7713} \\ \cline{2-13}
		& NIQE $\downarrow$ & 12.34 & 13.77 & 12.51 & 14.81 & 11.39 & 12.62 & 15.89 & \textbf{\color{red}11.19} & 12.53 & 11.72 & \textbf{\color{blue}11.26} \\
		& LOE $\downarrow$ & 187.9 & 263.8 & 486.2 & 350.8 & 158.0 & 216.6 & 224.1 & 125.6 & 366.2 & \textbf{\color{red}102.3} & \textbf{\color{blue}117.7}  \\ \hline
		\multirow{4}{*}{LSRW \cite{LSRW}} & PSNR $\uparrow$ & 15.34 & 14.71 & 15.48 & 16.41 & \textbf{\color{blue}18.10} & 15.80 & 16.14 & 14.11 & 17.06 & 15.24 & \textbf{\color{red}19.00} \\
		& SSIM $\uparrow$ & 0.4212 & 0.4849 & 0.3468 & 0.4760 & \textbf{\color{blue}0.5149} & 0.4450 & 0.4627 & 0.4112 & 0.4601 & 0.4192 & \textbf{\color{red}0.5360} \\ \cline{2-13}
		& NIQE $\downarrow$ & 18.31 & 11.68 & 10.31 & 11.13 & 10.76 & 11.83 & 12.70 & 11.08 & 11.94 & \textbf{\color{blue}10.22} & \textbf{\color{red}9.23} \\
		& LOE $\downarrow$ & \textbf{\color{red}146.3} & 218.5 & 535.6 & 255.4 & 202.4 & 216.0 & 196.0 & 198.9 & 385.1 & 234.6 & \textbf{\color{blue}189.5} \\ \hline
		\multirow{2}{*}{LIME \cite{Guo_2017_TIP}} & NIQE $\downarrow$ & 12.80 & 15.21 & \textbf{\color{blue}11.88} & 14.72 & 14.48 & 12.85 & 16.16 & 12.44 & 14.59 & 12.38 & \textbf{\color{red}11.01} \\
		& LOE $\downarrow$ & 261.7 & 217.5 & 589.6 & 249.6 & \textbf{\color{red}166.7} & 192.1 & 216.6 & 288.7 & 421.1 & 212.6 & \textbf{\color{blue}187.2} \\ \bottomrule[1.5pt]
	\end{tabular}
	}
	\vspace{-1.em}
	
	\caption{\label{Quantitative} Quantitative comparison on three benchmarks. The best and the second best results are highlighted in \textbf{\color{red}red} and \textbf{\color{blue}blue} respectively.}
	\vspace{-1.em}
	
\end{table*}

\begin{figure*}[t]
  \centering
  \begin{subfigure}{0.162\linewidth}
    \includegraphics[width=1.\linewidth]{Image/Compare/LSRW2045/Input.png}\vspace{-0.4em}
    \centerline{Input}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Compare/LSRW2045/ZeroDCE.png}\vspace{-0.2em}
    \centerline{ZeroDCE}\medskip
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.162\linewidth}
    \includegraphics[width=1.\linewidth]{Image/Compare/LSRW2045/LECARM.png}\vspace{-0.2em}
    \centerline{LECARM}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Compare/LSRW2045/SSIENet.png}\vspace{-0.2em}
    \centerline{SSIENet}\medskip
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.162\linewidth}
    \includegraphics[width=1.\linewidth]{Image/Compare/LSRW2045/SDD.png}\vspace{-0.2em}
    \centerline{SDD}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Compare/LSRW2045/RUAS.png}\vspace{-0.2em}
    \centerline{RUAS}\medskip
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.162\linewidth}
    \includegraphics[width=1.\linewidth]{Image/Compare/LSRW2045/RetinexNet.png}\vspace{-0.2em}
    \centerline{RetinexNet}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Compare/LSRW2045/EnGAN.png}\vspace{-0.2em}
    \centerline{EnGAN}\medskip
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.162\linewidth}
    \includegraphics[width=1.\linewidth]{Image/Compare/LSRW2045/KinD.png}\vspace{-0.2em}
    \centerline{KinD}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Compare/LSRW2045/SCI.png}\vspace{-0.2em}
    \centerline{SCI}\medskip
  \end{subfigure}
  \begin{subfigure}{0.162\linewidth}
    \includegraphics[width=1.\linewidth]{Image/Compare/LSRW2045/URetinexNet.png}\vspace{-0.2em}
    \centerline{URetinexNet}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Compare/LSRW2045/Ours.png}\vspace{-0.2em}
    \centerline{Ours}\medskip
  \end{subfigure}
  \vspace{-1.2em}

  \caption{Subjective comparison on the LSRW dataset among state-of-the-art low-light image enhancement algorithms.}
  \label{comlsrw}
  \vspace{-1.5em}
\end{figure*}

% \subsection{Cooperative Loss Function}
% \label{sec:CL}
\textbf{Cooperative Loss.} To reduce solution space and provide more accurate attention guidance, we elaborate Cooperative Loss (CL). Inspired by Lee~\textit{et al.} \cite{Lee_2021_TMI}, this function trains different modules cooperatively by encouraging them to constrain each other. As shown in \cref{loss}, we indirectly supervise attention guidance $\textit{\textbf{I}}_{\rm{A}}$ and provide stronger constraints for all modules.

Concretely, as shown in Mask Extractor (ME) of \cref{totnet}, since attention map $\textit{\textbf{I}}_{\rm{A}}$ is generated based on the extracted image features, the content of features heavily impacts the quality of $\textit{\textbf{I}}_{\rm{A}}$. To get better $\textit{\textbf{I}}_{\rm{A}}$, we generate a \textbf{lightness mask} $\textit{\textbf{I}}_{\rm{M}}$ from the same features and co-supervise it with other information, including our enhanced image.

As shown in the left end of \cref{loss}, for the low-light input $\textit{\textbf{I}}_{\rm{L}}$, on the one hand, we extract $\textit{\textbf{I}}_{\rm{M}}$ with ME, and obtain a pseudo-high-light image $\overline{\textbf{\textit{I}}}_{\rm{H}}$ by adding $\textit{\textbf{I}}_{\rm{M}}$ with $\textit{\textbf{I}}_{\rm{L}}$. On the other hand, we subtract $\textit{\textbf{I}}_{\rm{L}}$ at the pixel level from the predicted high-light result $\widetilde{\textbf{\textit{I}}}_{\rm{H}}$, obtaining a pseudo-lightness mask $\overline{\textit{\textbf{I}}}_{\rm{M}}$. By using consistency loss, we encourage the estimated $\overline{\textit{\textbf{I}}}_{\rm{M}}$ to be consistent with the extracted $\textit{\textbf{I}}_{\rm{M}}$, the calculated $\overline{\textbf{\textit{I}}}_{\rm{H}}$ to be similar to the depicted $\widetilde{\textbf{\textit{I}}}_{\rm{H}}$, expressed as:
\begin{eqnarray}
\begin{aligned}
	\label{shi6}
	\mathcal{L}_{\rm{rec}1} = ||\overline{\textit{\textbf{I}}}_{\rm{M}}-\textit{\textbf{I}}_{\rm{M}}||_1 + ||\overline{\textbf{\textit{I}}}_{\rm{H}}-\widetilde{\textbf{\textit{I}}}_{\rm{H}}||_1.
\end{aligned}
\end{eqnarray}

For another branch of given the high-light input $\textit{\textbf{I}}_{\rm{H}}$, as shown in the right part of \cref{loss}, the loss is similar:
%for the high-light input $\textit{\textbf{I}}_{\rm{H}}$, we first subtract $\textit{\textbf{I}}_{\rm{M}}$ from it at pixel level, calculating a pseudo-low-light image $\overline{\textbf{\textit{I}}}_{\rm{L}}$. Then, enhancement module translates it to $\widetilde{\overline{\textbf{\textit{I}}}}_{\rm{H}}$. We enforce it to be similar to the original input $\textit{\textbf{I}}_{\rm{H}}$. Finally, ME is applied to extract lightness mask $\widetilde{\textit{\textbf{I}}}_{\rm{M}}$ from our intermediate result $\widetilde{\textbf{\textit{I}}}_{\rm{L}}$. We encourage $\textit{\textbf{I}}_{\rm{H}}$ can be regarded as the linear summation of the predicted image $\widetilde{\textbf{\textit{I}}}_{\rm{L}}$ and its related lightness mask $\widetilde{\textit{\textbf{I}}}_{\rm{M}}$, expressed as:
\begin{eqnarray}
\begin{aligned}
	\label{shi7}
	\mathcal{L}_{\rm{rec}2} = ||\widetilde{\overline{\textbf{\textit{I}}}}_{\rm{H}}-\textbf{\textit{I}}_{\rm{H}}||_1 + ||\textbf{\textit{I}}_{\rm{H}}-\widetilde{\textbf{\textit{I}}}_{\rm{L}}-\widetilde{\textit{\textbf{I}}}_{\rm{M}}||_1.
\end{aligned}
\end{eqnarray}

For the calculated images based on $\textit{\textbf{I}}_{\rm{M}}$, \textit{i.e.}, $\overline{\textbf{\textit{I}}}_{\rm{H}}$ and $\overline{\textbf{\textit{I}}}_{\rm{L}}$, we further use the double-path discriminator from our TAD to inspect their authenticity, formulated as:
\begin{eqnarray}
\begin{aligned}
	\label{shi8}
	\mathcal{L}_{\rm{insp}}(\overline{\textit{\textbf{I}}}_{\rm{H}},\overline{\textbf{\textit{I}}}_{\rm{L}},\textit{D}_{H},\textit{D}_{L}) &= \mathbb{E}_{\overline{\textit{\textbf{I}}}_{\rm{H}} \sim \overline{\boldsymbol{\mathcal{H}}}}[\log(1-\textit{D}_{H}(\overline{\textbf{\textit{I}}}_{\rm{H}}))]\\ &+ \mathbb{E}_{\overline{\textbf{\textit{I}}}_{\rm{L}} \sim \overline{\boldsymbol{\mathcal{L}}}}[\log(1-\textit{D}_{L}(\overline{\textbf{\textit{I}}}_{\rm{L}}))],
\end{aligned}
\end{eqnarray}
where $\overline{\boldsymbol{\mathcal{H}}}$ and $\overline{\boldsymbol{\mathcal{L}}}$ mean the pseudo high-light and low-light image domain respectively. The final cooperative loss is:
\begin{eqnarray}
\begin{aligned}
	\label{shi9}
	\mathcal{L}_{CL} = \mathcal{L}_{\rm{rec}1} + \mathcal{L}_{\rm{rec}2} + \mathcal{L}_{\rm{insp}}.
\end{aligned}
\end{eqnarray}

Combined with the NR-related loss in \cref{sec:NR}, the adversarial constraint in \cref{sec:TAD}, and the consistency loss in \cref{sec:DLGP}, our final objective function is expressed as:
\begin{eqnarray}
\begin{aligned}
	\label{shi12}
	\mathcal{L} = \mathcal{L}_{NR} + \mathcal{L}_{adv} + \mathcal{L}_{con} + \mathcal{L}_{CL}.
\end{aligned}
\end{eqnarray}

\section{Experiments}
\label{sec:Exp}
In this section, we first present the implementation details of our approach. Then we compare it with the state-of-the-art methods through multiple benchmarks. To identify the contribution of each component, we further conduct ablation analyses. All experiments are implemented with PyTorch and conducted on a single NVIDIA Tesla V100 GPU.

% In this section, we first introduce our implementation details. Then we make comparison with the state-of-the-art methods through multiple benchmarks. Finally, we conduct ablation analysis to prove the contribution of each component. All experiments are implemented with PyTorch and conducted on a single NVIDIA Tesla V100 GPU.

\subsection{Implementation Details}
\textbf{Parameter Settings.} For training, we adopt Adam optimizer \cite{Adam} with the hyper parameters $\beta_1$ = 0.5, $\beta_2$ = 0.999, and $\epsilon$ = 10$^{-8}$. Our model is trained for 300 epochs with an initial learning rate of 2$\times$10$^{-4}$ and decaying linearly to 0 in the last 200 epochs. Batch size is set to be 1 and patch size is resized to 256$\times$256 for training in the concern of efficiency. Heuristically, we adopt the MLP with 3 hidden layers to normalize the degradation level.

\textbf{Benchmarks and Metrics.} To validate the effectiveness of our method, we train and test the model on LSRW dataset \cite{LSRW}, which contains 1000 low-light-normal-light image pairs for training and 50 pairs for evaluation. Each pair consists of a degraded image and a corresponding well-exposed reference, which are captured from real world at different exposure times. For a more convincing comparison, we further extend evaluation to other benchmarks such as LOL dataset \cite{LOL}, which includes 485 pairs for learning and 15 ones for evaluation, and LIME \cite{Guo_2017_TIP} dataset, which only includes real-world low-light images. To demonstrate the generalization to real-world degradation scenarios, we test on LIME with the model trained on LSRW. Noting that to prove the superiority of our unsupervised learning manner, during training, we only adopt the low-light part of the paired training data, and replace the references with 300 images from BSD300 dataset \cite{Martin_2001_ICCV}. We use two full-reference metrics, \textit{i.e.}, Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) \cite{Zhou_2004_TIP}, and two no-reference metrics, namely NIQE \cite{Mittal_2013_SPL} and LOE \cite{Wang_2013_TIP}, to evaluate the effectiveness of different algorithms objectively. In general, a higher PSNR or SSIM means more authentic restored results, while a lower NIQE or LOE represents higher quality details, lightness, and tone.

% \begin{figure}[t]
%   \centering
%   \begin{subfigure}{0.32\linewidth}
%     \includegraphics[width=1.\linewidth]{Image/Ablation/3018/input.png}\vspace{-0.4em}
%     \centerline{Input}\vspace{-0.6em}\medskip
    
%     \includegraphics[width=1.\linewidth]{Image/Ablation/3018/N.png}\vspace{-0.2em}
%     \centerline{w/o \textit{N}}\vspace{-0.2em}\medskip
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.32\linewidth}
%     \includegraphics[width=1.\linewidth]{Image/Ablation/3018/NTE.png}\vspace{-0.2em}
%     \centerline{w/o \textit{NTE}}\vspace{-0.6em}\medskip
    
%     \includegraphics[width=1.\linewidth]{Image/Ablation/3018/final.png}\vspace{-0.2em}
%     \centerline{NeRCo}\vspace{-0.2em}\medskip
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.32\linewidth}
%     \includegraphics[width=1.\linewidth]{Image/Ablation/3018/NT.png}\vspace{-0.2em}
%     \centerline{w/o \textit{NT}}\vspace{-0.6em}\medskip
    
%     \includegraphics[width=1.\linewidth]{Image/Ablation/3018/GT.png}\vspace{-0.2em}
%     \centerline{Ground Truth}\vspace{-0.2em}\medskip
%   \end{subfigure}
%   \vspace{-1.3em}

%   \caption{Visual comparison of ablation study. The full setting recovers even better result than the ground truth, especially in the regions boxed in green and red.}
%   \label{ablat}
%   \vspace{-1.5em}
  
% \end{figure}

\begin{figure}[t]
  \centering
  \begin{subfigure}{0.32\linewidth}
    \includegraphics[width=1.\linewidth]{Image/Ablation/3018/input.png}\vspace{-0.5em}
    \centerline{Input}\vspace{-0.6em}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Ablation/3018/N.png}\vspace{-0.4em}
    \centerline{\#3}\vspace{-0.2em}\medskip
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.32\linewidth}
    \includegraphics[width=1.\linewidth]{Image/Ablation/3018/NTE.png}\vspace{-0.3em}
    \centerline{\#1}\vspace{-0.6em}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Ablation/3018/final.png}\vspace{-0.4em}
    \centerline{NeRCo}\vspace{-0.2em}\medskip
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.32\linewidth}
    \includegraphics[width=1.\linewidth]{Image/Ablation/3018/NT.png}\vspace{-0.3em}
    \centerline{\#2}\vspace{-0.6em}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Ablation/3018/GT.png}\vspace{-0.4em}
    \centerline{Ground Truth}\vspace{-0.2em}\medskip
  \end{subfigure}
  \vspace{-1.3em}

  \caption{Visual comparison of ablation study. The full setting recovers even better result than the ground truth, especially in the regions boxed in green and red.}
  \label{ablat}
  \vspace{-1.5em}
  
\end{figure}

\subsection{Comparison with the State-of-the-Art}
% 1. 几乎所有数据集的所有指标达到了 SOTA，表明所提出框架的优越性
% 2. 甚至超越了有监督方法，
% 3. EnGAN, SCI
For a more comprehensive analysis, we compare our method with two recently-proposed model-based methods, namely LECARM \cite{LECARM} and SDD \cite{SDD}, three advanced supervised learning methods (\textit{i.e.}, RetinexNet \cite{LOL}, KinD \cite{Zhang_2021_IJCV}, and URetinex-Net \cite{Wu_2022_CVPR}), and five unsupervised learning methods, including ZeroDCE \cite{Guo_2020_CVPR}, SSIENet \cite{SSIENet}, RUAS \cite{Liu_2021_CVPR}, EnGAN \cite{Jiang_2021_TIP}, and SCI \cite{Ma_2022_CVPR}.

\textbf{Quantitative analysis.} We obtain quantitative results of other methods by adopting official pre-trained models and running their respective public codes. As shown in \cref{Quantitative}, our method achieves nearly SOTA performance in both full-reference and no-reference metrics across all benchmarks. It validates the superior effect of the proposed framework. Noting that our method even performs better than the supervised ones. Compared with some recent competitive unsupervised approaches such as EnGAN \cite{Jiang_2021_TIP}, and SCI \cite{Ma_2022_CVPR}. We provide stronger constraints than EnGAN, including visual-oriented guidance from prompts. Besides, due to the normalization property, our method outperforms SCI especially on some challenging scenes. The visual results of all methods (including NeRCo and SCI) in a scenario with severe brightness degradation are given below.

\textbf{Qualitative analysis.} For a more intuitive comparison, we report the visual results of all approaches in \cref{comlsrw}. Our input is a severely degraded image. One can see that the recent traditional methods can not recover enough brightness. While the advanced deep learning-based methods over-smooth background or introduce unknown veils, resulting in miserable artifacts and unnatural tone. In particular, SCI fails to effectively enhance such a challenging scenario. By comparison, our model realizes the best visual quality with prominent contrast and vivid colors. Details are also remained well. More results are presented in \textcolor{red}{SM}.

\begin{table}[t]
	\centering
	\resizebox{\linewidth}{!}{
	\begin{tabular}{c|c c c|c|c|c|c}
		\toprule[1.pt]
		\cellcolor{gray!40}index & \cellcolor{gray!40}NRN & \cellcolor{gray!40}TAD & \cellcolor{gray!40}CL\&ME & \cellcolor{gray!40}PSNR $\uparrow$ & \cellcolor{gray!40}SSIM $\uparrow$ & \cellcolor{gray!40}NIQE $\downarrow$ & \cellcolor{gray!40}LOE $\downarrow$ \\ \bottomrule
		\toprule
		\#1 & \XSolidBrush & \XSolidBrush & \XSolidBrush & 16.77 & 0.4565 & 12.34 & 272.4  \\ \cline{5-8}
		\#2 & \XSolidBrush & \XSolidBrush & \CheckmarkBold & 17.65 & 0.5023 & \textbf{\color{blue}10.60} & 247.9 \\ \cline{5-8}
		\#3 & \XSolidBrush & \CheckmarkBold & \CheckmarkBold & \textbf{\color{blue}18.32} & \textbf{\color{blue}0.5201} & 10.83 & \textbf{\color{blue}230.9} \\ \cline{5-8}
		NeRCo & \CheckmarkBold & \CheckmarkBold & \CheckmarkBold & \textbf{\color{red}19.00} & \textbf{\color{red}0.5360} & \textbf{\color{red}9.23} & \textbf{\color{red}189.5} \\ \bottomrule[1.pt]
	\end{tabular}
	}
	\vspace{-1.em}
	
	\caption{\label{blatq} Quantitative evaluation on the enhanced results obtained from different settings. The best and the second best results are highlighted in \textbf{\color{red}red} and \textbf{\color{blue}blue} respectively.}
	\vspace{-1.5em}
	
\end{table}


% \begin{table}[t]
% 	\centering
% 	\resizebox{\linewidth}{!}{
% 	\begin{tabular}{c|c|c|c|c}
% 		\toprule[1.pt]
% 		\cellcolor{gray!40}Models & \cellcolor{gray!40}PSNR $\uparrow$ & \cellcolor{gray!40}SSIM $\uparrow$ & \cellcolor{gray!40}NIQE $\downarrow$ & \cellcolor{gray!40}LOE $\downarrow$ \\ \bottomrule
% 		\toprule
% 		w/o NTE & 16.77 & 0.4565 & 12.34 & 272.4  \\ \hline
% 		w/o NT & 17.65 & 0.5023 & \textbf{\color{blue}10.60} & 247.9 \\ \hline
% 		w/o N & \textbf{\color{blue}18.32} & \textbf{\color{blue}0.5201} & 10.83 & \textbf{\color{blue}230.9} \\ \hline
% 		NeRCo & \textbf{\color{red}19.00} & \textbf{\color{red}0.5360} & \textbf{\color{red}9.23} & \textbf{\color{red}189.5} \\ \bottomrule[1.pt]
% 	\end{tabular}
% 	}
% 	\vspace{-1.em}
	
% 	\caption{\label{blatq} Quantitative evaluation on the enhanced results obtained from different settings. The best and the second best results are highlighted in \textbf{\color{red}red} and \textbf{\color{blue}blue} respectively.}
% 	\vspace{-1.5em}
	
% \end{table}


\subsection{Ablation Study}
% #1 Dual Loop 的无监督框架，已经有 comparable 性能，说明网络结构的高效性；
% #2 添加 cooperative loss，能 .....
% #3 添加文本与外观判别器，进一步提升性能 （更细粒度的消融？）
% #4 添加神经隐式表征，性能提升最明显，提升了对不同退化条件的鲁棒性
As shown in \cref{blatq}, we consider three ablation settings by adding the proposed components to the dual loop successively. All ablation studies are conducted on the LSRW dataset. \textbf{i)} ``\#1'' is a naive dual loop without any other operations. This unsupervised framework has achieved comparable scores, indicating its effectiveness. \textbf{ii)} ``\#2'' adds the mask extractor (ME) and the related cooperative loss (CL) to ``\#1''. ME provides attention guidance and CL encourages all modules to fully train each other, which reduces solution space. \textbf{iii)} ``\#3'' adds the text-driven appearance discriminator (TAD) to ``\#2''. It employs visual-oriented guidance from textual modality and focuses on high-frequency components, further improves performance. \textbf{iv)} Finally, we adopt neural representation to realize a complete NeRCo. Due to the improved robustness to different degradation conditions, this setting achieves apparent performance gain.

We report qualitative results of all settings in \cref{ablat}. One can see that in the first sample, ``\#1'' increases contrast and roughly recovers objects in dark region, but the enhanced tone is still inauthentic. ``\#2'' relieves color cast phenomenon to a certain degree but still remains undesired veils. ``\#3'' can generate a image which is visually consistent with the ground truth. Further, the result of NeRCo faithfully preserves the most details and performs the best perceptual effectiveness, especially in the regions boxed in green and red. We attribute this to the neural representation normalization module, which unifies degradation level and reduces the difficulty of enhancement task. 

% In addition, we also summarize quantitative results with four metrics in \cref{blatq}. Compared with other ablation settings, NeRCo yields the best scores across all metrics, indicating that removing any component attenuates the performance evidently.


\section{Conclusion}
We proposed an implicit Neural Representation method for Cooperative low-light image enhancement, dubbed NeRCo, to recover visual-friendly results in an unsupervised manner. Firstly, for the input degraded image, we employed neural representation to normalize degradation levels (\textit{e.g.}, dark lightness and natural noise). Besides, for the output enhanced image, we equipped the discriminator with a high-frequency path and utilized priors from the pre-trained vision-language model to impart perceptual-oriented guidance. Finally, to ease the reliance on paired data and enhance low-light scenarios in a self-supervised manner, a dual-closed-loop cooperative constraint was developed to train the enhancement module. It encourages all components to constrain each other, further reducing solution space. Experiments proved the superiority of our method compared with other top-performing ones.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage
\appendix
\onecolumn
\input{sup}
\end{document}
