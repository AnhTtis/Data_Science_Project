%%%%%%%%% TITLE - PLEASE UPDATE
\onecolumn
\begin{center}
\textbf{\Large{Implicit Neural Representation for Cooperative Low-light Image Enhancement \\ -- Supplemental Document --}}
\vspace{1cm}
\centering
\captionsetup{type=figure}
\def\svgwidth{1.0\linewidth}
\end{center}


%%%%%%%%% ABSTRACT
\begin{abstract}

This is the supplementary material for the paper: ``Implicit Neural Representation for Cooperative Low-light Image Enhancement''. Firstly, we provide more results of our NRN module in \textbf{Section A} for a comprehensive illustration. Besides, in \textbf{Section B}, we train the framework with different prompts and compare their performance with other ablation settings, which verifies the effect of text-driven supervision. In \textbf{Section C}, we further conduct ablation experiments on TAD to determine the role of each path. In \textbf{Section D}, to demonstrate the semantic advantage of our method, we classify the enhanced results of different methods with a pre-trained vision-language model and report their accuracy. Our results are considered best for textual description of high-light image. Finally, more qualitative analyses on three well-known benchmarks are displayed in \textbf{Section E}, including LSRW dataset, LOL dataset and LIME dataset. It is obvious that the proposed NeRCo achieves the best performance, further verifying our superiority.
\end{abstract}
% \vspace{2em}



%%%%%%%%% BODY TEXT
\section{Normalized Results}
Deep learning based models learn to map a sample from the input domain to the target domain. In real-world application, however, degradation conditions are various. For some inputs far from the learned input domain, it is hard for a trained model to perform stable superior performance. Hence, we developed Neural Representation Normalization (NRN) module to normalize different conditions, which has been illustrated in \cref{sec:NR}. In order to provide more convincing proof, here we add here more experimental results. 

As shown in \cref{NRPP1}, we adopted three sets of images from the SICE \cite{Cai_2018_TIP} dataset, each set contains three images with different brightness and the same content. We box them with blue, red and green lines respectively. All of them are processed by our NRN module, the corresponding results are boxed with the same color. One can see that the brightness of original inputs varies a lot, while the output brightness of NRN is similar. For more intuitive, on the right, we provide a visualization of their pixel distribution on the Y channel. It is obvious that NRN constricts the the range of brightness changes and normalizes degradation levels.


\section{Experiments with Alternative Textual Prompts}
To investigate the contribution of our proposed text-driven supervision, we compare the performance of models trained on different prompts. Specifically, we consider three pairs of alternative prompts to guide model training: i) $\textit{dark}$ and $\textit{bright}$. ii) $\textit{dim}$ and $\textit{light}$. iii) $\textit{night}$ and $\textit{day}$. These alteration experiments are conducted on the ``\#3'' ablation setting mentioned in \cref{sec:AB}, which removes the neural representation function from the NeRCo.

As shown in \cref{blatq1}, we report the results of different settings on LSRW dataset \cite{LSRW}. We design different prompts to study the impact of different texts on model performance. One can see that the ``\#3'' settings with diverse prompts realize decent scores on all four metrics. Although their values are different, they are within a stable range, \textit{i.e.}, better than other ablation settings and worse than NeRCo. On the one hand, we can see that text-driven supervision does have a gain in performance. On the other hand, it also indirectly proves the contribution of our proposed Neural Representation Normalization (NRN) module. Since $\textit{low-light image}$ and $\textit{high-light image}$ are two texts widely adopted to describe images in this task, we used this pair in other experiments for more intuitive validation.

\section{TAD Ablation}
TAD contains three paths: color discrimination, edge supervision, and text-driven discrimination. To further define the role of each component, we conduct ablation study on TAD, which removes different paths from the ablation setting ``\#3'' in the submitted paper. Note that as at least one supervision is required, we adopt color discrimination as the base discriminator. Results are given in \cref{blatTAD}, one can see that both edge path and text supervision improve the effect.

\begin{figure*}[t]
  \centering
  \begin{subfigure}{0.98\linewidth}
    \includegraphics[width=1.\linewidth]{Image/Supp/1.pdf}
    
  \end{subfigure}
  \vspace{-0.7em}

  \caption{Comparisons between the captured low-light scenes ($\textit{\textbf{I}}_{\rm{L}}$) and the results of NRN ($\textit{\textbf{I}}_{\rm{NR}}$). The low-light samples are from the SICE \cite{Cai_2018_TIP} dataset. It contains numerous image sets, each with common content and varying degradation conditions. The pixel value distribution of images on the Y channel is given on the right. One can see that NRN normalizes the brightness to be similar.}
  \label{NRPP1}
  \vspace{-1.em}
  
\end{figure*}

\clearpage
\section{Semantic Evaluation}
In order to demonstrate the superiority of our NeRCo at the semantic level, we employ the pre-trained CLIP model \cite{Radford_2021_ICML} to calculate semantic score of different methods. Concretely, we first design a prompt $\textit{high-light image}$. The image vector and the text vector are then generated by CLIP model. We calculate their cosine discrepancy, and use a softmax function to obtain the semantic score, which values from 0 to 1. A higher score represents the better semantic consistency between the enhanced image and the text $\textit{high-light image}$. 

We report the average prediction accuracy of the results from different methods in \cref{Semantic}. One can see that the pre-trained CLIP considers the input low-light image to be the least likely high-light image, while the high-light references are classified accurately, except for LIME \cite{Guo_2017_TIP} which only contains degraded scenarios. Although some methods output semantically impressive results, our NeRCo achieves the best scores, even better than the ground truth. It proves that the quality of the reference images from the dataset is semantically good, as they achieve the second-best scores. Due to the text-driven discrimination during training, our method produces more perceptual-friendly results than references.
\begin{table*}[t]
	\centering
	\scriptsize
	\resizebox{\linewidth}{!}{
	\begin{tabular}{c|c|c|c|c|c|c|c}
		\toprule[1.pt]
		\multirow{2}{*}{Models} & \multirow{2}{*}{\#1} & \multirow{2}{*}{\#2} & \#3 & \#3 & \#3 & \#3 & \multirow{2}{*}{NeRCo} \\
        &&& ($\textit{low-light image}$ / $\textit{high-light image}$) & ($\textit{dark}$ / $\textit{bright}$) & ($\textit{dim}$ / $\textit{light}$) & ($\textit{night}$ / $\textit{day}$) & \\
		\bottomrule
		\toprule
		PSNR $\uparrow$ & 16.77 & 17.65 & 18.32 & \textbf{\color{blue}18.58} & 18.23 & 18.56 & \textbf{\color{red}19.00} \\ \hline
		SSIM $\uparrow$ & 0.4565 & 0.5023 & 0.5201 & 0.5118 & 0.5208 & \textbf{\color{blue}0.5277} & \textbf{\color{red}0.5360} \\ \hline
		NIQE $\downarrow$ & 12.34 & 10.60 & 10.83 & 9.45 & 9.59 & \textbf{\color{blue}9.33} & \textbf{\color{red}9.23} \\ \hline
		LOE $\downarrow$ & 272.4 & 247.9 & 230.9 & 202.9 & \textbf{\color{blue}191.0} & 206.6 & \textbf{\color{red}189.5} \\ \bottomrule[1.pt]
	\end{tabular}
	}
	\vspace{-1.2em}
	
	\caption{\label{blatq1} Ablation study with different prompt options. The best and the second best results are highlighted in \textbf{\color{red}red} and \textbf{\color{blue}blue} respectively. One can see that settings trained with prompts outperform other versions, and prompts can be replaced with synonyms. It proves that text-driven supervision has a gain in model performance.}
	\vspace{-1.em}
	
\end{table*}

\begin{table*}[t]
	\centering
	\resizebox{\linewidth}{!}{
	\begin{tabular}{c|c c c c c c c c c c c c c}
		\toprule[1.5pt]
		\multirow{2}{*}{Datasets} & \multirow{2}{*}{Input} & LECARM & SDD & RetinexNet & KinD & URetinexNet & ZeroDCE & SSIENet & RUAS & EnGAN & SCI & \multirow{2}{*}{NeRCo} & \multirow{2}{*}{Reference} \\ 
		& & \cite{LECARM} & \cite{SDD} & \cite{LOL} & \cite{Zhang_2019_MM} & \cite{Wu_2022_CVPR} & \cite{Guo_2020_CVPR} & \cite{SSIENet} & \cite{Liu_2021_CVPR} & \cite{Jiang_2021_TIP} & \cite{Ma_2022_CVPR} & &
		\\ \bottomrule
        \toprule
		LOL \cite{LOL} & 0.1590 & 0.3685 & 0.3397 & 0.4831 & 0.5130 & 0.5554 & 0.3463 & 0.4639 & 0.4381 & 0.4450 & 0.3402 & \textbf{\color{red}0.6366} & \textbf{\color{blue}0.5910} \\ \hline
		LSRW \cite{LSRW} & 0.3164 & 0.6028 & 0.5907 & 0.6653 & 0.6052 & 0.6539 & 0.6584 & 0.6746 & 0.6418 & 0.5969 & 0.6176 & \textbf{\color{red}0.7581} & \textbf{\color{blue}0.6955} \\ \hline
		LIME \cite{Guo_2017_TIP} & 0.3281 & 0.5168 & 0.4669 & 0.5657 & 0.5589 & \textbf{\color{blue}0.6265} & 0.4719 & 0.6147 & 0.5427 & 0.4912 & 0.5781 & \textbf{\color{red}0.7499} & - \\ \bottomrule[1.5pt]
	\end{tabular}
	}
	\vspace{-1.2em}
	
	\caption{\label{Semantic} The average semantic scores of different settings on three benchmarks. The best and the second best results are highlighted in \textbf{\color{red}red} and \textbf{\color{blue}blue} respectively. One can see that the pre-trained vision-language model classifies our results more accurately than those of other methods, which demonstrates the better semantic consistency of our method.}
    \vspace{-1.em}

\end{table*}

\begin{table*}[t]
	\centering
	\scriptsize
	\resizebox{120mm}{!}{
    \begin{tabular}{c|c|c|c|c|c|c}
    % \toprule[0.5pt]
    \toprule
    Color Path & Edge Path & Text Supervision & PSNR $\uparrow$ & SSIM $\uparrow$ & NIQE $\downarrow$ & LOE $\downarrow$ \\ \bottomrule
    \toprule
    \CheckmarkBold & \XSolidBrush & \XSolidBrush & 17.37 & 0.4942 & 11.72 & 252.1 \\
    \CheckmarkBold & \CheckmarkBold & \XSolidBrush & \textbf{\color{blue}17.65} & \textbf{\color{blue}0.5023} & \textbf{\color{red}10.60} & \textbf{\color{blue}247.9} \\
    \CheckmarkBold & \CheckmarkBold & \CheckmarkBold & \textbf{\color{red}18.32} & \textbf{\color{red}0.5201} & \textbf{\color{blue}10.83} & \textbf{\color{red}230.9} \\
    %\bottomrule[0.5pt]
    \bottomrule
    \end{tabular}
    }
	\vspace{-1.2em}
	
	\caption{\label{blatTAD} Ablation study on TAD, based on the ablation setting ``\#3''. We conduct experiments on LSRW dataset. The best and the second best results are highlighted in \textbf{\color{red}red} and \textbf{\color{blue}blue} respectively.}
	\vspace{-1.em}
	
\end{table*}

\section{Qualitative Analysis}
We have provided adequate quantitative results (\cref{Quantitative}) in our paper. However, due to the limit of space, only parts of visual comparisons are given (\cref{comlsrw}). Here, we supplement more qualitative analysis compared with other SOTA methods, including LECARM \cite{LECARM}, SDD \cite{SDD}, RetinexNet \cite{LOL}, KinD \cite{Zhang_2019_MM}, URetinex-Net \cite{Wu_2022_CVPR}, ZeroDCE \cite{Guo_2020_CVPR}, SSIENet \cite{SSIENet}, RUAS \cite{Liu_2021_CVPR}, EnGAN \cite{Jiang_2021_TIP}, and SCI \cite{Ma_2022_CVPR}.

\cref{comlsrw1} displays the enhanced results on LSRW dataset. One can see that conventional model-based methods cannot recover sufficient brightness, while some other comparison methods suffer from color cast. RetinexNet, KinD, ZeroDCE, and RUAS, \textit{etc.} develop the post-processing denoising operations to remove the inherent noise in dark regions, but they tend to discard details. In general, our NeRCo is capable of color adjustment and detail preservation, demonstrating its superiority over other algorithms. Furthermore, we provide visual comparisons between our proposed NeRCo and other SOTA mthods on other well-known benchmarks. \cref{comlol} shows the comparisons on LOL dataset and \cref{comlime} displays the qualitative results on LIME dataset. Obviously, across all these comparisons, our method recovers the most authentic tones and provides visual-friendly results, which proves its effectiveness.

\begin{figure*}[t]
  \centering
  \begin{subfigure}{0.162\linewidth}
    \includegraphics[width=1.\linewidth]{Image/Supp/LSRW/2037/Input.png}\vspace{-0.4em}
    \centerline{Input}\vspace{-0.5em}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Supp/LSRW/2037/ZeroDCE.png}\vspace{-0.2em}
    \centerline{ZeroDCE}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Supp/LSRW/3014/Input.png}\vspace{-0.4em}
    \centerline{Input}\vspace{-0.5em}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Supp/LSRW/3014/ZeroDCE.png}\vspace{-0.2em}
    \centerline{ZeroDCE}\medskip
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.162\linewidth}
    \includegraphics[width=1.\linewidth]{Image/Supp/LSRW/2037/LECARM.png}\vspace{-0.2em}
    \centerline{LECARM}\vspace{-0.5em}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Supp/LSRW/2037/SSIENet.png}\vspace{-0.2em}
    \centerline{SSIENet}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Supp/LSRW/3014/LECARM.png}\vspace{-0.2em}
    \centerline{LECARM}\vspace{-0.5em}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Supp/LSRW/3014/SSIENet.png}\vspace{-0.2em}
    \centerline{SSIENet}\medskip
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.162\linewidth}
    \includegraphics[width=1.\linewidth]{Image/Supp/LSRW/2037/SDD.png}\vspace{-0.2em}
    \centerline{SDD}\vspace{-0.5em}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Supp/LSRW/2037/RUAS.png}\vspace{-0.2em}
    \centerline{RUAS}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Supp/LSRW/3014/SDD.png}\vspace{-0.2em}
    \centerline{SDD}\vspace{-0.5em}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Supp/LSRW/3014/RUAS.png}\vspace{-0.2em}
    \centerline{RUAS}\medskip
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.162\linewidth}
    \includegraphics[width=1.\linewidth]{Image/Supp/LSRW/2037/RetinexNet.png}\vspace{-0.2em}
    \centerline{RetinexNet}\vspace{-0.5em}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Supp/LSRW/2037/EnGAN.png}\vspace{-0.2em}
    \centerline{EnGAN}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Supp/LSRW/3014/RetinexNet.png}\vspace{-0.2em}
    \centerline{RetinexNet}\vspace{-0.5em}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Supp/LSRW/3014/EnGAN.png}\vspace{-0.2em}
    \centerline{EnGAN}\medskip
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.162\linewidth}
    \includegraphics[width=1.\linewidth]{Image/Supp/LSRW/2037/KinD.png}\vspace{-0.2em}
    \centerline{KinD}\vspace{-0.5em}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Supp/LSRW/2037/SCI.png}\vspace{-0.2em}
    \centerline{SCI}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Supp/LSRW/3014/KinD.png}\vspace{-0.2em}
    \centerline{KinD}\vspace{-0.5em}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Supp/LSRW/3014/SCI.png}\vspace{-0.2em}
    \centerline{SCI}\medskip
  \end{subfigure}
  \begin{subfigure}{0.162\linewidth}
    \includegraphics[width=1.\linewidth]{Image/Supp/LSRW/2037/URetinexNet.png}\vspace{-0.2em}
    \centerline{URetinexNet}\vspace{-0.5em}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Supp/LSRW/2037/Ours.png}\vspace{-0.2em}
    \centerline{Ours}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Supp/LSRW/3014/URetinexNet.png}\vspace{-0.2em}
    \centerline{URetinexNet}\vspace{-0.5em}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Supp/LSRW/3014/Ours.png}\vspace{-0.2em}
    \centerline{Ours}\medskip
  \end{subfigure}
  \vspace{-1.em}

  \caption{Subjective comparison on the LSRW dataset among state-of-the-art low-light image enhancement algorithms. Obviously, the proposed method has achieved the best performance, further verifying its effectiveness.}
  \label{comlsrw1}
  \vspace{-1.em}
\end{figure*}

\begin{figure*}[t]
  \centering
  \begin{subfigure}{0.162\linewidth}
    \includegraphics[width=1.\linewidth]{Image/Supp/LOL/Input.png}\vspace{-0.4em}
    \centerline{Input}\vspace{-0.5em}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Supp/LOL/ZeroDCE.png}\vspace{-0.2em}
    \centerline{ZeroDCE}\medskip
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.162\linewidth}
    \includegraphics[width=1.\linewidth]{Image/Supp/LOL/LECARM.png}\vspace{-0.2em}
    \centerline{LECARM}\vspace{-0.5em}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Supp/LOL/SSIENet.png}\vspace{-0.2em}
    \centerline{SSIENet}\medskip
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.162\linewidth}
    \includegraphics[width=1.\linewidth]{Image/Supp/LOL/SDD.png}\vspace{-0.2em}
    \centerline{SDD}\vspace{-0.5em}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Supp/LOL/RUAS.png}\vspace{-0.2em}
    \centerline{RUAS}\medskip
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.162\linewidth}
    \includegraphics[width=1.\linewidth]{Image/Supp/LOL/RetinexNet.png}\vspace{-0.2em}
    \centerline{RetinexNet}\vspace{-0.5em}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Supp/LOL/EnGAN.png}\vspace{-0.2em}
    \centerline{EnGAN}\medskip
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.162\linewidth}
    \includegraphics[width=1.\linewidth]{Image/Supp/LOL/KinD.png}\vspace{-0.2em}
    \centerline{KinD}\vspace{-0.5em}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Supp/LOL/SCI.png}\vspace{-0.2em}
    \centerline{SCI}\medskip
  \end{subfigure}
  \begin{subfigure}{0.162\linewidth}
    \includegraphics[width=1.\linewidth]{Image/Supp/LOL/URetinexNet.png}\vspace{-0.2em}
    \centerline{URetinexNet}\vspace{-0.5em}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Supp/LOL/Ours.png}\vspace{-0.2em}
    \centerline{Ours}\medskip
  \end{subfigure}
  \vspace{-1.em}

  \caption{Subjective comparison on the LOL dataset among state-of-the-art low-light image enhancement algorithms. It is obvious that our method recovers the most authentic results, demonstrating its superiority.}
  \label{comlol}
  \vspace{-1.em}
  
\end{figure*}

\begin{figure*}[t]
  \centering
  \begin{subfigure}{0.162\linewidth}
    \includegraphics[width=1.\linewidth]{Image/Supp/LIME/Input.pdf}\vspace{-0.4em}
    \centerline{Input}\vspace{-0.5em}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Supp/LIME/ZeroDCE.pdf}\vspace{-0.2em}
    \centerline{ZeroDCE}\medskip
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.162\linewidth}
    \includegraphics[width=1.\linewidth]{Image/Supp/LIME/LECARM.pdf}\vspace{-0.2em}
    \centerline{LECARM}\vspace{-0.5em}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Supp/LIME/SSIENet.pdf}\vspace{-0.2em}
    \centerline{SSIENet}\medskip
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.162\linewidth}
    \includegraphics[width=1.\linewidth]{Image/Supp/LIME/SDD.pdf}\vspace{-0.2em}
    \centerline{SDD}\vspace{-0.5em}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Supp/LIME/RUAS.pdf}\vspace{-0.2em}
    \centerline{RUAS}\medskip
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.162\linewidth}
    \includegraphics[width=1.\linewidth]{Image/Supp/LIME/RetinexNet.pdf}\vspace{-0.2em}
    \centerline{RetinexNet}\vspace{-0.5em}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Supp/LIME/EnGAN.pdf}\vspace{-0.2em}
    \centerline{EnGAN}\medskip
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.162\linewidth}
    \includegraphics[width=1.\linewidth]{Image/Supp/LIME/KinD.pdf}\vspace{-0.2em}
    \centerline{KinD}\vspace{-0.5em}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Supp/LIME/SCI.pdf}\vspace{-0.2em}
    \centerline{SCI}\medskip
  \end{subfigure}
  \begin{subfigure}{0.162\linewidth}
    \includegraphics[width=1.\linewidth]{Image/Supp/LIME/URetinexNet.pdf}\vspace{-0.2em}
    \centerline{URetinexNet}\vspace{-0.5em}\medskip
    
    \includegraphics[width=1.\linewidth]{Image/Supp/LIME/Ours.pdf}\vspace{-0.2em}
    \centerline{Ours}\medskip
  \end{subfigure}
  \vspace{-1.em}

  \caption{Subjective comparison on the LIME dataset among state-of-the-art low-light image enhancement algorithms. Our model still performs best on this low-light image-only dataset, which proves its effectiveness.}
  \label{comlime}
  \vspace{-1.em}
  
\end{figure*}
%------------------------------------------------------------------------
