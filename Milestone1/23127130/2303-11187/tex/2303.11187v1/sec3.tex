% !TEX root = paper.tex

\section{Estimation}\label{sec:Estimation}
In this section, we use the method of minimax estimation to transform the conditional moment restrictions in the IES into an unconditional moment minimax estimator with respect to $\cL_\cD(\cdot)$.
We then build the confidence set for the CATE as the level set in $\vH$ under metric $\cL_\cD(\cdot)$ and threshold $e_\cD$. Based on the confidence set, we integrate pessimism in policy optimization.
\paragraph{RMSE and unconditional moment criteria.}
We let $\vH=\cH_1\times\cdots\times\cH_{K-1}\times\cG$ be the hypothesis space for $\vh$.
Following the idea of projected residual mean squared error (RMSE) minimization \citep{dikkala2020minimax}, our estimation target is good generalization performance subject to the following RMSE, 
\begin{align}\label{def:RMSE}
    \nbr{\vT\vh}^2_{\mu, 2}:= \sum_{k=1}^K \EE_{\mu_k}\sbr{\cT_k \vh(\vZ_k)}^2 = \sum_{k=1}^K \EE_{\mu_k}\sbr{\rbr{\EE_{\mu_k}\sbr{\alpha_k(\vh(X), \vY_k)\given \vZ_k}}^2}, 
\end{align}
where $\vT\vh=(\cT_1\vh, \cdots, \cT_K\vh)$ and $\mu=(\mu_1, \cdots, \mu_K)$. Note that solving the conditional moment equations \eqref{eq:operator=0} corresponds to finding the $\vh\in\vH$ that minimizes the RMSE.
However, learning the causal relationship with conditional moment restrictions is a challenging task. It has been investigated in the existing literature how to transform conditional moment conditions into unconditional moment conditions, e.g., methods of importance weighting using conditional density ratio \citep{kato2021learning} or using linear sieve estimator \citep{ai2003efficient} where the estimator has a rate of $n^{-1/4}$.
Inspired by the method of minimax estimation \citep{dikkala2020minimax, duan2021risk, uehara2021finite} with fast rate of $n^{-1/2}$, we propose to approximate \eqref{def:RMSE} with an unconditional minimax estimator. 
Specifically, we introduce a test function $\theta_k:\vZ_k\rightarrow \RR$ for each linear operator $T_k$. The function class for $\theta_k$ is $\Theta_k$, which we refer to as the dual function class.
The unconditional moment loss function that is used to replace the RMSE in \eqref{def:RMSE} is then given by
\begin{align}\label{def:L}
    \cL(\vh) = \sum_{k=1}^K \cL_k(\vh), \text{ where } \cL_k(\vh)=\sup_{\theta_k\in\Theta_k}\EE_{\mu_k}\sbr{\alpha_k(\vh(X), \vY_k) \theta_k(\vZ_k)}-\frac 1 2 \norm{\theta_k}_{\mu_k, 2}^2.
\end{align}
By assuming the test function class $\Theta_k$ to be star-shaped, we always have $\cL_k(\vh)\ge 0$ (otherwise by letting $\theta_k=0$ there is a conflict with the fact that $\cL_k(\vh)$ takes the supremum over $\Theta_k$).
Note that as long as $\cT_k\vh\in\Theta_k$, the loss function $\cL(\cdot)$ is equivalent to the RMSE $\nbr{\cT(\cdot)}^2_{\mu, 2}$, which can be verified by the property of Fenchel duality (see \S\ref{lem:Fenchel} for a detailed proof). Therefore, if we have $\cT_k\vh\in\Theta_k$, we see that any solution to \eqref{eq:operator=0} is a minimizer to the unconditional moment loss function, i.e., $\vhstar=\arg\inf_{\vh} \cL(\vh)$.
In line with \eqref{def:L}, 
we define the empirical loss function on the dataset $\cD$ as follows,
\begin{align*}
    \cL_\cD(\vh) = \sum_{k=1}^K \cL_{k, \cD}(\vh), \text{ where } \cL_{k,\cD}(\vh)=\sup_{\theta_k\in\Theta_k}\EE_{\cD_k}\sbr{\alpha_k(\vh(X), \vY_k) \theta_k(\vZ_k)}-\frac 1 2 \norm{\theta_k}_{\cD_k, 2}^2, 
\end{align*}
where $\cD_k$ is a subset of $\cD$ and we have $\cD_k\sim \mu_k$, as is discussed in \S\ref{sec:united form}.
We also have $\cL_{k, \cD}(\vh)\ge 0$ following the same argument that $\cL_{k, \cD}$ takes the supremum over the dual function class.
Now we are ready to build the confidence set $\CIH$ as the level set with respect to the metric $\cL_\cD(\cdot)$ and threshold $e_\cD$ as follows,
\begin{align}\label{def:CI}
    \CIH(e_\cD) = \cbr{\vh\in\vH:  \vcL_{\cD}(\vh) \le  \inf_{\vh\in\vH}\vcL_{\cD}(\vh) + e_\cD}.
\end{align}
Correspondingly, the confidence set for the CATE is given by
\begin{align*}
    \CICATE(e_\cD) = \cbr{g\in\cG: \exists\vh\in\CIH(e_\cD), \text{ s.t., } g=\vh^{(K)}}, 
\end{align*}
where $\vh^{(K)}$ is the last element of $\vh$. 
Therefore, we have $g=h^{(K)}$ by noting that the last element of $\vh$ is the estimated CATE following both Theorems \ref{thm:IV identification} and \ref{thm:PV ID}.
We remark that building the confidence set is a way to address the aleatoric uncertainty stemming from the data generating process, as will be shown in Theorem \ref{thm:Fast rate} that $\CIH(e_\cD)$ can capture an estimator $\vhHstar$ with small realizability error. On the other hand, we can eliminate the spurious correlation in \eqref{eq:spurious subopt} with pessimism on such a confidence set, i.e., greedily selecting the policy that optimizes the pessimistic average reward function with $g\in\CICATE(e_\cD)$,
\begin{align*}
    \piepessi = \arg\sup_{\pie\in\Pie} \inf_{g\in\CICATE(e_\cD)} v(g, \pie),  
\end{align*}
where $v$ is the average reward function defined in \eqref{def:v approx}.
Here, we denote by $\gpessi{\pie}=\arg\inf_{g\in\CICATE(e_\cD)}v(g, \pie)$ the estimated pessimistic CATE under the interventional policy $\pie$.  
By plugging in the definition of $\cL_\cD(\cdot)$ and $\CICATE(\cdot)$ in  Algorithm \ref{alg:meta}, we obtain the complete Causal-Adjusted Pessimistic (CAP) policy learning algorithm.




% \iffalse
% In this section, we will show the formalization of confounded contextual bandit with instrumental variable  (CCB-IV) and discuss how min-max integral equation solver could be applied to solve the offline learning problem of CCB-IV.

% \subsection{CCB-IV Formalization}
% We formalize the CCB-IV by adding the side information of instrumental variable $Z$ into the model.

% \subsubsection{Instrumental Variables}
% \begin{definition}[Instrumental Variables \cite{angrist1995identification}]
% Suppose $U$ is a confounder of $(X,Y)$. An observable variable $Z$ is called an instrumental variable (IV) of $(X, Y)$ if
% \begin{equation*}
%     \left\{
%     \begin{aligned}
%         & X = f(Z, U) \\
%         & Y = g(X, U)
%     \end{aligned}
%     \right.
% \end{equation*}
% where $Z\indep U$ and $f$ is not a trivial function.
% \end{definition}

% The directed acyclic graph of an IV is showed in Figre~\ref{fig:IV}.
% \Yitan{Draw a DAG of IV here later.}

% \Yitan{Write an example of IV here later.}

% \subsubsection{CCB-IV}\label{sec:CCB-IV definition}
% We extend the classic contextual bandit setting with inclusion of side information from IV.
% A CCB-IV is defined as $B=(\cS, \cA, \vZ, \cU, r; \xi, \cP_u, \cP_z, \pi_b)$, where $\cS\subseteq\RR^{d^{(x)}}$ denotes the state space, $\cA$ denotes the action space, $\vZ\subseteq\RR^{d^{(z)}}$ denotes the IV space, $\cU\subseteq\RR^{d^{(u)}}$ denotes the confounder space, and $r:\cS\times\cA\rightarrow\RR$ denotes the reward function.
% Additionally, we also let $\xi\in\Delta(\cS)$ be the distribution of initial state, $\cP_u$ be the distribution of confounder and $\cP_z$ be the distribution of IV.
% Finally, we let $\pi_b\in\Delta(\cA|\cS,\vZ,\cU)$ represent the behavior policy.

% Figre~\ref{fig:CCB-IV} illustrates the directed acyclic graph of CCB-IV. 
% \Yitan{Draw a DAG of CCB-IV here later.}

% \subsubsection{Data Generation by Behavior Policy}\label{sec:CCB-IV data generation}
% As mentioned in Section \ref{sec:CCB-IV definition}, we included a behavior policy in our CCB-IV model since we are discussing the offline setting.
% In $i$-th round of CCB-IV, a state $x^{(i)}\sim\xi$ is given by the environment.
% Confounder $u^{(i)}$ and IV $z^{(i)}$ are also generated according to $u^{(i)}\sim\cP_u$ and $z^{(i)}\sim\cP_Z$ respectively.
% Then the action is sampled by the behavior policy $\pi_b$ as $a^{(i)}\sim\pi_b(\cdot|x^{(i)},z^{(i)},u^{(i)})$.
% Finally the reward is given by $r^{(i)}=r(x^{(i)}, a^{(i)})$.
% The tuple $O_i=(x^{(i)}, z^{(i)}, a^{(i)}, r^{(i)})$ is the observation for $i$-th round.
% For two rounds $i$ and $j$, $O_i$ and $O_j$ are generated independently.
% The data set generated by the behavior policy $\pi_b$ consists of the whole collection of all observations $\{O_i:i\in[n]\}$.

% \subsubsection{Evaluation of Policy}\label{sec:CCB-IV evalutation}
% Let $\pi\in\Delta(\cA|\cS)$ be a policy. We emphasize that the policy to be evaluated only has access to to the state $\cS$ and could not observe the confounder $U$.
% Hence the major difference between $\pi$ and $\pi_b$ is that the confounder does not impact $\pi\in\Delta(\cA|\cS)$ but has effect on $\pi_b\in\Delta(\cA|\cS,\vZ,\cU)$.
% We define the value function of $\pi$ as
% \begin{align}
%     V^\pi(x) = \E_{\pi}\left[r(x,a) | x\right]
% \end{align}
% The optimal policy $\pi^*$ is defined as
% \begin{align}
%     \pi^*(x) = \argmax_a r(x,a)
% \end{align}
% Our goal is to search for the optimal policy $\pi^*$, or equivalently minimize the suboptimality of $\pi$
% \begin{align}
%     \text{SubOpt}(\pi, x) = V^{\pi^*}(x) - V^{\pi}(x)
% \end{align}

% \subsection{Pessimism with IV}
% \fi


