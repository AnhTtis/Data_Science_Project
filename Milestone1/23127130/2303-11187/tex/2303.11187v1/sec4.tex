% !TEX root = paper.tex

\section{Theoretical Results} \label{sec: theoretical results}
Let $\vhstar$ denote the exact solution to \eqref{eq:united form}.
We allow the model to be misspecified, i.e., the exact solution might not be fully captured by the hypothesis space $\vH$.
To characterize the approximation error, we pose the following assumption on the realizability of the hypothesis class $\vH$.
\begin{assumption}[Realizability of hypothesis class]\label{asp:Realizability}
Let $\vareH>0$ be the minimal positive value such that there exists $\vhHstar=\{\hHstark{1}, \cdots, \hHstark{K-1}, \gHstar\}\in\vH$ satisfying,
\begin{itemize}
    \item[(i)] $\nbr{\vT\vhHstar}_{ \mu, 2} \le \vareH$, where $\nbr{\vT\vhHstar}_{\mu, 2}$  is the RMSE defined in \eqref{def:RMSE}.
    \item[(ii)] $\sup_{v\in\cV} \nbr{\gHstar-\gstar}_{v, 2}\le \vareH$, where $\cV=\{v: v(x, a)=\tpr(x)\pie(a\given x), \forall \pie\in\Pie\}$.
\end{itemize}
\end{assumption}
Here, (i) characterizes the approximation error of $\vhHstar$ under the metric $\nbr{\cT(\cdot)}_{\mu, 2}$ where $\mu$ represents the distribution in the dataset, and the approximation error in (ii) is the supremum over all the possible measure that is realizable by the policy class $\Pie$.
% The optimal estimator $\vhHstar=\{\hHstark{1}, \cdots, \hHstark{K}\}$ in the hypothesis class corresponding to the unconditional moment criteria \eqref{def:L} is defined as
% \begin{align}
%     \vhHstar=\arg\inf_{\vh\in\vH} \cL(\vh). \label{def:vhHstar}
% \end{align}
We remark that $\vhHstar$ can be softly viewed as the projection of $\vhstar$ onto the hypothesis space $\vH$ with approximation error $\vareH$.
We then pose the following assumption on the compatibility of the test function class $\Theta_k$.
\begin{assumption}[Compatibility of test function class]\label{asp:compatibility}
Suppose that for any $\vh\in\vH$ and for any $k\in\{1, \cdots, K\}$, it holds that
$\inf_{\theta_k\in\Theta_k} \nbr{\theta_k - \cT_k\vh}_{\mu_k, 2}\le \vareTheta$.
\end{assumption}
We give an example where the test function class has full compatibility. Following the discussion in \cite{dikkala2020minimax}, we consider a case where $\Theta_k$ lies in a Reproducing Kernel Hilbert space (RKHS) $\HH_{K_{\theta_k}}$ with RKHS kernel $K_{\theta_k}: \cZ_k\times\cZ_k\rightarrow \RR$. If $\alpha_k$ lies in another RKHS space $\HH_{K_{\alpha_k}}$ and the conditional density function $\pr(\vX, \vY_k\given \vZ_k)$ satisfies $\pr(\vX, \vY_k\given \cdot)\in \HH_{K_{\Theta_k}}$, we then have $\cT_k\vh\in \HH_{K_{\Theta_k}}$, which means that the dual function class has full compatibility. In addition, we pose the following assumption on the regularity of function classes $\vH$, $\vTheta$ and linear function $\alpha_k$.
\begin{assumption}[Regularity]\label{asp:regularity}
We assume that $\alpha_k$ is $L_{\alpha, 1}$-Lipschitz continuous with respect to $h_j$ and $Y_k$ for all  $j,k\in\{1, \cdots, K\}$.
We assume that the support of $Y_k$ is bounded, i.e., $\nbr{\text{supp}(Y_k)}_\infty\le L_Y$.
Moreover, we assume that $\sup_{h\in\vH}\norm{h}_\infty\le L_{h}$ and $\sup_{\theta\in\vTheta}\norm{\theta}_\infty\le L_{\theta}$.
\end{assumption}
We justify the regularity assumption by the examples of CCB-IV and CCB-PV.
Following Theorems \ref{thm:IV identification} and \ref{thm:PV ID}, 
we see that the continuity of $\alpha_k$ is apparent.
The regularity of $\vH$ and $\vTheta$ is easy to satisfy by choosing bounded function classes.
With bounded reward, it is straightforward that the linear function $\alpha_k(\vh, \vY_k)$ is globally bounded. Specifically, we can assume that  $\nbr{\alpha_k}_\infty\le L_\alpha$.

To characterize the properties of the confidence set Under these assumptions, we first define an event $\cE$ as
\begin{align}
    \cE &= \Big\{\abr{\EE_{\cD_k}\sbr{\alpha_k(\vh, \cY_k) \theta_k(\cZ_k)} - \EE_{\mu_k}\sbr{\alpha_k(\vh, \cY_k) \theta_k(\cZ_k)}} \le \eta_k\rbr{L_\alpha\nbr{\theta_k}_{\mu_k, 2}+\eta_k},\nend
    &\quad \abr{\norm{\theta_k}^2_{\cD_k, 2}-\norm{\theta_k}^2_{\mu_k, 2}}\le \frac 1 2\rbr{\norm{\theta_k}^2_{\mu_k, 2}+\eta_k^2}, 
    \forall \vh\in \vH, \forall \theta_k\in\Theta_k, \forall k\in\{1, \cdots, K\}\Big\}.\label{def:cE}
\end{align}
where $\eta_k$ bounds the maximal critic radius for function classes $\cQ_k$ and $\Theta_k$ with respect to $\xi\in(0, 1)$. Here, we define function class $\cQ_k$ as
\begin{align*}
    \cQ_k=\cbr{\alpha_k(\vh(\vX_k), \vY_k)\theta_k: \forall \vh\in\vH, \theta_k\in\Theta_k}.
\end{align*}
See \S\ref{app:critical radius} for calculation of the critical radius in the case of linear function class. 
We let $\eta^2=\sum_{k=1}^K \eta_k^2$ for simplicity. Now we give the following theorem, which shows that event $\cE$ holds with high probability and the confidence set built for uncertainty quantification enjoys some good properties.
\begin{theorem}[Uncertainty Quantification]\label{thm:Fast rate}
Suppose that Assumptions \ref{asp:Realizability}, \ref{asp:compatibility} and \ref{asp:regularity}  hold. 
Event $\cE$ holds with probability at least $1-2K\xi$ and the confidence set enjoys the following properties on $\cE$, 
\begin{itemize}
    \item[(i).] For the $\vhHstar$ satisfying Assumption \ref{asp:Realizability}, it holds that $\cL_\cD(\vhHstar) \le 2\vareH^2 + \rbr{2 L_\alpha^2+5 /4}\eta^2$. Moreover, if we set $e_\cD>2\vareH^2 + \rbr{2 L_\alpha^2+ 5/ 4}\eta^2$, it holds that $\gHstar\in\CICATE(e_\cD)$.
    \item[(ii).] For all $\vh\in\CIH(e_\cD)$, we have,
\begin{align*}
    \sup_{k\in\{1, \cdots, K\}}\nbr{\cT_k\vh}_{\mu_k, 2}\overset{\cE}{\lesssim} \cO(\vareTheta) + \cO(\vareH) + \cO\rbr{\sqrt{e_\cD}} +  \cO\rbr{\eta}.
\end{align*}
\end{itemize}
\begin{proof}
See \S\ref{pro:Fast rate} for a detailed proof.
\end{proof}
\end{theorem}
Theorem \ref{thm:Fast rate} shows that event $\cE$ holds with a high probability.
We see from (i) that it is theoretically guaranteed that $\vhHstar$ lies within the confidence set by properly setting the threshold $e_\cD$. As will be shown shortly after, such a property is vital for the use of pessimism.
Property (ii) in Theorem \ref{thm:Fast rate} shows that the RMSE for any $\vh\in\CIH(e_\cD)$ is well controlled on event $\cE$.
Now we are ready to present the convergence results for the sub-optimality defined in \eqref{def: SubOpt}. We give the following theorem on the sub-optimality for the CCB-IV.
\begin{theorem}[Convergence of sub-optimality for CCB-IV]\label{thm:IV subopt}
Suppose that Assumption 
\ref{asp:CCB-IV} for the CCB-IV model and the conditions in Remark \ref{rmk:IV existence} hold.
Suppose that Assumptions
\ref{asp:Realizability},  \ref{asp:compatibility}, and 
\ref{asp:regularity} for function classes $\vH$ and $\Theta$ hold.
The threshold $e_\cD$ for the confidence set is set to $e_{\cD}>2\vareH^2 + (2 L_\alpha^2+5/ 4)\eta^2$.
For the marginal distribution of context $\tpr$ in the interventional process and the optimal interventional policy $\piestar$, suppose that there exists $b_1:\cZ\rightarrow \RR$ satisfying 
\begin{align}
    \EEob\sbr{b_1(Z)\given A=a, X=x, R_Z=1} = \frac{\tpr(x)\piestar(a\given x)}{\pob(x, a\given R_Z=1)}.\label{cond:CCB-IV b1 exist}
\end{align}
The sub-optimality corresponding to $\piepessi$ for the CCB-IV is bounded on event $\cE$ with probability at least $1-4\xi$ by
\begin{align*}
    \SubOpt(\piepessi) \overset{\cE}{\lesssim} \sum_{k=1}^2 \nbr{b_k}_{\mu_k, 2}\cdot \big(\cO(\vareTheta) + \cO(\vareH) + \cO\rbr{\sqrt{e_\cD}} +  \cO\rbr{\eta}\big),
\end{align*}
where $b_2:\cA\times\cX\times\cZ\rightarrow \RR$ is defined as,
\begin{gather}
    b_2(a, x, z)=b_1(z)\frac{\pob(a, x, z\given R_Z=1)}{\pob(a, x, z\given(R_X, R_Z)=\ind)}.\label{eq:b2 IV}
\end{gather}
\begin{proof}
See \S\ref{proof: pessimism} and \S\ref{proof: subopt of IV} for a detailed proof.
\end{proof}
\end{theorem}
Similarly, the convergence of sub-optimality for CCB-PV is given by
\begin{theorem}[Convergence of sub-optimality for  CCB-PV]\label{thm:PV subopt}
    Suppose that Assumption 
\ref{asp:CCB-PV} for the CCB-PV model and the conditions in Remark \ref{rmk:PV existence} hold.
Suppose that Assumptions
\ref{asp:Realizability}, \ref{asp:compatibility}, and
\ref{asp:regularity} for function classes $\vH$ and $\Theta$ hold.
The threshold $e_\cD$ for the confidence set is set to $e_{\cD}>2\vareH^2 + (2 L_\alpha^2+5/ 4)\eta^2$.
For the marginal context distribution  $\tpr$ in the interventional process and the optimal interventional policy $\piestar$, suppose that there exists $b_1:\cX\times\cA\times\cZ\rightarrow\RR$ satisfying
    \begin{align}
        \EEob\sbr{b_1(X, A, Z)\given X=x, U=u, A=a, R_Z=\ind} = \frac{\pob(u\given x)\tpr(x)\piestar(a\given x)}{\pob(u, x, a\given  R_Z=1)}.\label{cond:CCB-PV b1 exist}
    \end{align}
    The sub-optimality corresponding to $\piepessi$ for the CCB-PV is bounded with probability at least $1-8\xi$ by
    \begin{align*}
        \SubOpt(\piepessi) \lesssim \sum_{k=1}^4 \nbr{b_k}_{\mu_k, 2}\cdot \big(\cO(\vareTheta) + \cO(\vareH) + \cO\rbr{\sqrt{e_\cD}} +  \cO\rbr{\eta}\big),
    \end{align*}
    where $b_2:\cA\times\cW\times\cX\times\cZ$, $b_3:\cW\times\cX\times\cA\times\cA'$ and $b_4:\cX\times\cA'\rightarrow\RR$ are defined as
    \begin{gather}
        b_2(a, w, x, z) = b_1(x, a, z)\frac{\pob(a, w, x\given (R_X, R_Z)=\ind)}{\pob(a, w, x\given (R_W, R_X, R_Z)=\ind)}.\nend
        b_3(w, x, a, a') = \frac{\tpr(x)\piestar(a'\given x)\pob(a, w\given x, R_X=1)}{u(a')\pob(x, a, w\given (R_W, R_X)=\ind)}, \quad
        b_4(x, a') = \frac{\tpr(x)\piestar(a'\given x)}{\pob(x\given R_X=1)u(a')}.\nonumber
    \end{gather}
\begin{proof}
See \S\ref{proof: pessimism} and \S\ref{proof: subopt of PV} for a detailed proof.
\end{proof}
\end{theorem}
\paragraph{Remarks on the existence of $b_1$.} We remark that the existence of $b_1$ in \eqref{cond:CCB-IV b1 exist} and \eqref{cond:CCB-PV b1 exist} are also related to the linear inverse problem,  as is discussed in \S\ref{app:linear inverse}. In the discrete setting, \eqref{cond:CCB-IV b1 exist} is automatically satisfied if the distribution shift ratio on the right-hand side is globally bounded. This is because we already have $\text{rank}(P(Z\given (A,X), R_Z=1))\ge |\cA|\times|\cX|$ by the IV completeness assumption and thus the Moore-Penrose inverse  $P(Z\given (A,X), R_Z=1)^+$ exists. Similarly, \eqref{cond:CCB-PV b1 exist} is also automatically satisfied in the discrete setting following the PV completeness assumption.

% \paragraph{Assumptions categories.} The assumptions that appear in Theorems \ref{thm:IV subopt} and \ref{thm:PV subopt} fall into the following three categories: 
% \begin{itemize}
%     \item[(i)] Assumptions on the causal model, which contains standard model assumptions on the IV/PV and the unconfounded and outcome-independent assumptions on the missingness indicators (Assumptions \ref{asp:CCB-IV} and \ref{asp:CCB-PV}). We also pose additional completeness conditions for the conditional moment equations to admit a solution (see arguments following Theorems \ref{thm:IV identification} and \ref{thm:PV ID}). Specifically, the completeness conditions on the outcome $Y$ with respect to the missing variables ensure that the information loss in the missing variable can be recovered for estimating the CATE.
%     \item[(ii)] Assumptions on the function classes for estimation, which contains the realizability assumption on the hypothesis class (Assumption \ref{asp:Realizability}), the compatibility assumption on the dual function class (Assumption \ref{asp:compatibility}), and the regularity assumption in Assumption \ref{asp:regularity}. Note that we don't require full realizability or compatibility.
%     \item[(iii)] Assumptions on distribution shift ratio, which contains the existence of $b_1$ to both \eqref{cond:CCB-IV b1 exist} and \eqref{cond:CCB-PV b1 exist} and bounded squared norm of functions $b_1, b_2, b_3$ and $b_4$. 
% \end{itemize}
% \paragraph{Distribution shift.} We remark that the following three factors contribute to the distribution shift arising in $b_k$: (i) the optimal interventional policy $\piestar$ differs from the observable policy $\pib$; (ii) the marginal distribution for the context $\tpr(x)$ in the interventional process differs from the marginal distribution for the context $\pob(x)$ in the dataset; (iii) the joint distribution with different conditions on the missing indicators differs since we have the issue of missing not at random.
% Notice that we only have to deal with the distribution shift corresponding to the optimal interventional policy $\piestar$ instead of the whole policy class $\Pie$, which makes the assumptions on bounded norm of the distribution shift ratio much weaker. 
% Such a benefit comes from the using of pessimism, which mitigates the error term (ii) with spurious correlation in \eqref{eq:spurious subopt} and only leaves the error in term (i) corresponding to the optimal interventional policy $\piestar$.
\paragraph{Significance of the main theorems.} 
Theorem \ref{thm:IV subopt} and \ref{thm:PV subopt} establish the convergence of the sub-optimality for the CAP algorithm in the offline confounded contextual bandit with missingness. 
Such results are achieved by using the minimax estimator, building a confidence set for the CATE, and integrating pessimism in the policy optimization step.
Our theories deal with model misspecification in the hypothesis space and the dual function class. Moreover, the sub-optimality does not rely on the “uniform coverage” of the observational dataset, e.g., uniformly lower bounded densities of visitation measures \citep{yang2020off, liao2020batch, duan2020minimax}.
This is because the distribution shift ratio $b_k$ only depends on the optimal policy $\piestar$, which is intrinsic to the bandit model, instead of the whole policy class $\Pie$. Therefore, we only require the dataset to “cover” certain distributions induced by $\piestar$.

\paragraph{Implications of the main theorems.} 
When it holds that $\eta\sim\tcO(n^{-1/2})$ (see \S\ref{app:critical radius} for a calculation of the critical radius of the linear function class) and $\vareH=\vareTheta=0$, by setting $e_\cD\sim\tcO(n^{-1})$, Theorem \ref{thm:IV subopt} and \ref{thm:PV subopt} indicates that $\SubOpt(\piepessi)\sim\tcO(n^{-1/2})$, which corresponds to a “fast statistical rate” for minimax estimation \citep{uehara2021finite}.
% Such a result relies on the analysis of the risk for the condidence set $\CIH(e_\cD)$ in Theorem \ref{thm:Fast rate}.
However, given the fact that the distribution shift ratio $b_k$ also stems from the missingness issue, we require some “compliance” of the data distribution with missingness in order for $b_k$ to be bounded. For instance, we require $\pob(a, x, z\given  R_Z=1)=\cO(\pob(a, x, z\given (R_X, R_Z)=\ind))$ for $b_2$ in \eqref{eq:b2 IV} to be bounded. Such a condition is reasonable if we consider a counterexample where $\pob(x_0\given R_Z)>0$ but $\pob(x_0\given (R_X, R_Z)=\ind)=0$ for $x_0\in \cX$, meaning that $x_0$ is totally missing from the observed dataset $\cD$. Therefore, there is no way to learn the CATE corresponding to $x_0$. 
On the other hand, following the discussion in \S\ref{app:critical radius} on the critical radius of a linear function class, we have $\eta_k\sim\cO(\sqrt{\log T_k/T_k})$ where $T_k=|\cD_k|$. Recall that $\cD_k$ may have different size since the conditional moment equations in the IES are subject to different missingness conditions. 
Therefore, we also require $T_k$ being of the same order of $T$ for $\SubOpt(\piepessi)$ to enjoy a fast statistical rate.