% !TEX root = paper.tex
%\begin{flushleft}
% \section{Confounders Missing not at Random} \label{sec:missing value}
\section{Extended Policy Class for CCB-PV}\label{sec:extended CCB-PV}
\paragraph{Motivation.} 
In the previous discussion, since we assume that the side observations are not accessible in the interventional process, we restrict our interventional policy to the class $\pie:\cX\rightarrow \Delta(\cA)$. 
In this section, we discuss an extension to the setting with accessible side observations in the interventional process.
Note that we have $Z\indep Y\given (X, A)$ in the CCB-IV, meaning that including the side observation $Z$ adds no additional information to the outcome and therefore the policy class $\pie:\cX\rightarrow \Delta(\cA)$ is good enough. 
In the CCB-PV setting, however, it is possible to improve the performance by allowing the interventional policy to also depend on the side observations. 
Specifically, we consider an extension of the CCB-PV setting where the interventional policy class is given by $\pie\in\Pie:\cX\times\cW\rightarrow \Delta(\cA)$. 
We have to redefine the CATE and average reward function for the extended policy class by
%\begin{gather}
\begin{align}
    &\CATE(a, x, w) = \EEin\sbr{Y\given X=x, W=x, \doopt(a)}, \nonumber\\
    &v^\pi(x) = \EE_{\pin{\pie}} \sbr{\CATE(A, X, W)\given X=x},\label{def:extended v}
\end{align}
%\end{gather}
where $\pin{\pie}$ is given  by plugging  $\pie=(a\given x, w)$ into the joint distribution of the interventional process in \eqref{def:pin}. 
Following the model in Assumption \ref{asp:CCB-PV}, we provide the identification IES as follows.
\begin{theorem}[IES for CCB-PV with extended policy]\label{thm:CCB-PV ID extension} 
Suppose Assumption \ref{asp:CCB-PV} holds. For any interventional policy $\pie:\cX\times\cW\rightarrow\Delta(\cA)$, if there exist bridge functions $h_1:\cY\times\cA\times\cX\times\cZ\rightarrow\RR$, $h_2:\cA\times\cW\times\cX\rightarrow \RR$, $h_3:\cY\times\cA\times\cX\times\cA\rightarrow\RR$ and $g:\cX\rightarrow \RR$ satisfying,
%\begin{gather}
\begin{align}
    &\EEob\sbr{h_1(Y, A, X, Z)\given a,x,z, (R_X, R_Z)=\ind} = 0,
    % \label{eq:PV ID extension 1}
    \nend
    &\EEob[h_2(A, W, X) - h_1(Y, A, X, Z)-Y\pie(A\given X, W)
    \given a, w, x, z, (R_W, R_X, R_Z)=\ind]=0, 
    % \label{eq:PV ID extension 2}
    \nend
    &\EEob\sbr{h_3(Y, A, X)-\sum_{a'\in\cA}h_2(a', W, X)\given a, w, x, (R_W, R_X)=\ind}=0,
    \label{eq:PV ID extension 3} 
    \\
    &\EEob\sbr{g(X) - h_3(Y, A, X)\given x, R_X=1}=0, 
    \label{eq:PV ID extension 4}
\end{align}
%\end{gather}
it follows that $ v^\pie(x)\aseq g(x)$ where $v^\pie$ is the average reward.
\begin{proof}
See \S\ref{pro:CCB-PV ID extension} for a detailed proof.
\end{proof}
\end{theorem}
\paragraph{Existence of the solution. }
The conditions for existence of a solution to such an IES is similar to Remark \ref{rmk:PV existence}, except that the first condition is adjusted by assuming there exists a solution $h_2$ to $\EEob\sbr{h_2(A, W, X)-Y\pie(A\given W, X)\given A=a, X=x, U=u}$ and the rest two conditions are just the same.
% It suffices for a solution to exists if the following three conditions are satisfied:
% \begin{itemize}
%     \item[(i)] There exists a solution $h^\pie_2$ to $\EEob\sbr{h^\pie_2(A, W, X)-Y\pie(A\given W, X)\given A=a, X=x, U=u}$;
%     \item[(ii)] For any $h^\pie_2$ satisfying (i), there exists a solution $h^\pie_1$ to \eqref{eq:PV ID extension 2};
%     \item[(iii)] For any $h^\pie_2$ satisfying (i), there exists a solution $h^\pie_3$ to \eqref{eq:PV ID extension 3}.
% \end{itemize}
\paragraph{A comparison to Theorem \ref{thm:PV ID}.}
The differences between these two versions of identification formula are in three folds: (i) The identification equations in Theorem \ref{thm:CCB-PV ID extension} are policy specific while those in Theorem \ref{thm:PV ID} hold for any interventional policy; (ii) there is no need for introducing a pseudo variable $A'$ here since $a'$ is already marginalized in \eqref{eq:PV ID extension 3}; (iii) $g$ corresponds to the average reward instead of the CATE.
% \eqref{eq:PV ID extension 3} indicates we only deal with discrete $\cA$.

\paragraph{Algorithm.} 
Note that the linear function $\alpha$, the operator $\cT$ and the loss function $\cL_\cD$ should depend on policy $\pie$ and we denote them by $\alpha^\pie$, $\cT^\pie$ and $\cL_\cD^\pie$, respectively.
The confidence set is built for each $\pie$ by
\begin{align*}
    \CICATE^\pie = \cbr{g\in\cG: \exists \vh\in\vH \text{, s.t., } g=\vh^{(K)} \text{ and }\cL_\cD^\pie(\vh)\le \inf_{\vh\in\vH}\cL_\cD^\pie(\vh)+e_\cD}, 
\end{align*}
% where the empirical total loss function $\cL_\cD^\pie$ is defined in the same way as \eqref{def:L_D}. 
Therefore, the estimated policy with pessimism is given by 
\begin{align}
    \piepessi = \arg\sup_{\pie\in\Pie} \inf_{g\in\CICATE^\pie(e_\cD)} v(g), \quad \text{where } v(g) = \int_{\cX} g(x) \tpr(x)\rd x.\label{def:extended piepessi}
\end{align}
% For the realizability assumption, we just need a slight adjustment for the sake that both the linear operator $\vT^\pie$ and the bridge function $\vh^\pie$ are policy dependent. 
Before we give the main theorem, we restate the realizability assumption (Assumption \ref{asp:Realizability}) as follows.
\begin{assumption}[Realizability of hypothesis class for extended CCB-PV]\label{asp:extended realizability}
Let $\vareH>0$ be the minimal positive value such that there exists $\vhHpie=\{\hHpiek{1}, \cdots, \hHpiek{K-1}, \gHpie\}\in\vH$ satisfying,
\begin{itemize}
    \item[(i)] $\sup_{\pie\in\Pie}\nbr{\vT^\pie\vhHpie}_{\mu, 2} \le \vareH$
    \item[(ii)] $\sup_{\pie\in\Pie}\nbr{\gHpie-g^\pie}_{\tpr, 2}\le \vareH$, where $g^\pie$ is the exact solution to the identification equations in Theorem \ref{thm:CCB-PV ID extension}.
\end{itemize}
\end{assumption}
The compatibility assumption can be easily adjusted by assuming that $\inf_{\theta_k\in\Theta_k} \nbr{\theta_k - \cT_k^\pie\vh}_{\mu_k, 2}\le \vareTheta$ for any $\pie\in \Pie$. The regularity assumption remains the same, except that we also assume  $\nbr{\pie}_\infty$ to be bounded in order to have $\alpha^\pie_k$ globally bounded.
Now we provide the following theorem to characterize the convergence of sub-optimality for the CCB-PV with extended interventional policy. 

\begin{theorem}[Convergence of sub-optimality of CCB-PV with extended policy class]\label{thm:extended PV subopt}
Suppose that Assumptions \ref{asp:CCB-PV}, \ref{asp:extended realizability}, \ref{asp:compatibility},
\ref{asp:regularity} hold and the solution to the IES in Theorem \ref{thm:CCB-PV ID extension} exists.
let $e_{\cD}>2\vareH^2 + (2 L_\alpha^2+5/ 4)\eta^2$, where $\eta=\sum_{k=1}^K\eta_k^2$ and $\eta_k$ bounds the maximal critic radius for function classes $\tilde\cQ_k=\cbr{\alpha_k^\pie(\vh(\vX_k), \vY_k)\theta_k: \forall \vh\in\vH, \theta_k\in\Theta_k, \pie\in\Pi}$ and $\Theta_k$. Suppose that for any $x\in\cX, u\in\cU$ and $a\in\cA$ there exists $b_1:\cX\times\cA\times\cZ\rightarrow\RR$ satisfying
\begin{align}
    \EEob\sbr{b_1(X, A, Z)\given x, u, a, R_Z=1} = \frac{\pob(u\given x)\tpr(x)}{\pob(u, x, a\given R_Z=1)}.\label{cond:b1 PV extended}
\end{align}
The sub-optimality corresponding to $\piepessi$ for the CCB-PV with extended policy class is bounded with probability at least $1-8\xi$ by
    \begin{align*}
        \SubOpt(\piepessi) \lesssim \sum_{k=1}^4 \nbr{b_k}_{\mu_k, 2} \cdot \rbr{O(\vareTheta) + O(\vareH) + O\rbr{\sqrt{e_\cD}} +  O\rbr{\eta}}, 
    \end{align*}
    where $b_2:\cA\times\cW\times\cX\times\cZ\rightarrow\RR$, $b_3:\cW\times\cX\times\cA\rightarrow\RR$ and $b_4:\cX\rightarrow\RR$ are defined by
    %\begin{gather*}
    \begin{align*}
        &b_2(a, w, x, z) = b_1(x, a, z)\frac{\pob(a, w, x\given (R_X, R_Z)=\ind)}{\pob(a, w, x\given (R_W, R_X, R_Z)=\ind)}, \\
        &b_3(w, x, a) = \frac{\tpr(x)\pob(a, w\given x, R_X=1)}{\pob(x, a, w\given (R_W, R_X)=\ind)}, \\
        &b_4(x) = \frac{\tpr(x)}{\pob(x\given R_X=1)}.
    \end{align*}
    %\end{gather*}
\begin{proof}
See \S\ref{proof:extended PV Subopt} for a detailed proof.
\end{proof}
\end{theorem}
The arguments are similar except that each action should be “uniformly covered” in the observational process if we want $b_1$ to be bounded by \eqref{cond:b1 PV extended}, which implies that $|\cA|$ should be finite or a bounded set. Moreover, $\eta_k$ bounds the maximal critical radius for the function class $\tilde\cQ_k$, which also bounds the critical radius of the policy class $\Pie$. In \S\ref{app:POMDP critical radius}, such a critical radius is calculated with linear function class assumptions for the one-step POMDP. 





