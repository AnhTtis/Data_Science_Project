% !TEX root = paper.tex


Our work adds to the vast body of existing literature on offline RL (also known as 
batch RL) \citep{lange2012batch, levine2020offline}, 
where a learner only has access to a dataset collected a priori. Existing literature studies two tasks: (i) offline policy evaluation, which estimates the expected cumulative reward or (action- and state-) value functions of a target policy, and (ii) offline policy optimization, which learns an optimal policy that maximizes the expected cumulative reward. Note that (i) is also known as off-policy policy evaluation, which can be adapted to handle the online setting. Also, note that the target policy in (i) is known, while the optimal policy in (ii) is unknown. As (ii) is more challenging than (i), various algorithms for solving (ii), especially the value-based approaches, can be adapted to solve (i). Although we focus on (ii), we discuss the existing works on (i) and (ii) together.  

%RL agent is only given access to a fixed dataset collected a priori using one or many behavior policies. 
%Under such an offline setting, 
%the agent usually has one of the following two objectives: (i) predicting the expected total return or the value function associated with a desired policy, also known as the target policy, or (ii) learning the optimal policy  of the underlying MDP.  
%The first task is also referred to as off-policy evaluation (OPE) in the literature. 
%For the second task, a  value-based RL method learns the optimal policy  by estimating the corresponding value function, and thus a value-based algorithm 
%often can be readily modified for solving OPE. 
%This is also the case for PEVI ({\red Maybe say this in the algorithm section?} ). 
%Thus, in this section, we discuss the existing works on these two tasks together.   

A key challenge of offline RL is the insufficient coverage of the dataset \citep{wang2020statistical}, which arises from the lack of continuing exploration \citep{szepesvari2010algorithms}. In particular, the trajectories given in the dataset and those induced by the optimal policy (or the target policy) possibly have different distributions, which is also known as distribution shift \citep{levine2020offline}. As a result, intertwined with overparameterized function approximators, e.g., deep neural networks, offline RL possibly suffers from the extrapolation error \citep{fujimoto2019off}, which is large on the states and actions that are less covered by the dataset. Such an extrapolation error further propagates through each iteration of the algorithm for offline RL, as it often relies on bootstrapping \citep{sutton2018reinforcement}. 

To address such a challenge, the recent works \citep{fujimoto2019off, laroche2019safe, jaques2019way, wu2019behavior, kumar2019stabilizing, kumar2020conservative, agarwal2020optimistic, yu2020mopo, kidambi2020morel, wang2020critic, siegel2020keep, nair2020accelerating, liu2020provably} demonstrate the empirical success of various algorithms, which fall into two (possibly overlapping) categories: (i) regularized policy-based approaches and (ii) pessimistic value-based approaches. Specifically, (i) regularizes (or equivalently, constrains) the policy to avoid visiting the states and actions that are less covered by the dataset, while (ii) penalizes the (action- or state-) value function on such states and actions. 
 
On the other hand, the empirical success of offline RL mostly eludes existing theory. Specifically, the existing works require various assumptions on the sufficient coverage of the dataset, which is also known as data diversity \citep{levine2020offline}. For example, offline policy evaluation often requires the visitation measure of the behavior policy to be lower bounded uniformly over the state-action space. An alternative assumption requires the ratio between the visitation measure of the target policy and that of the behavior policy to be upper bounded uniformly over the state-action space. See, e.g., \cite{jiang2016doubly, thomas2016data, farajtabar2018more, liu2018breaking, xie2019towards, nachum2019dualdice, nachum2019algaedice, tang2019doubly, kallus2019efficiently, kallus2020doubly, jiang2020minimax, uehara2020minimax, duan2020minimaxoptimal, yin2020asymptotic, yin2020near, nachum2020reinforcement, yang2020off, zhang2019gendice} and the references therein. As another example, offline policy optimization often requires the concentrability coefficient to be upper bounded, whose definition mostly involves taking the supremum of a similarly defined ratio over the state-action space. See, e.g., \cite{antos2007fitted, antos2008learning, munos2008finite, farahmand2010error, farahmand2016regularized, scherrer2015approximate, chen2019information, liu2019neural, wang2019neural, fu2020single, fan2020theoretical, xie2020batch, xie2020q, liao2020batch, zhang2020variational} and the references therein.

In practice, such assumptions on the sufficient coverage of the dataset often fail to hold \citep{fujimoto2019off, agarwal2020optimistic, fu2020d4rl, gulcehre2020rl}, which possibly invalidates existing theory. For example, even for the MAB, a special case of the MDP, it remains unclear how to maximally exploit the dataset without such assumptions, e.g.,  when each action (arm) is taken a different number of times. As illustrated in Section \ref{sec:demo}, assuming there exists a suboptimal action that is less covered by the dataset, it possibly interferes with the learned policy via the spurious correlation. As a result, it remains unclear how to learn a policy whose suboptimality only depends on how well the dataset covers the optimal action instead of the suboptimal ones. In contrast, our work proves that pessimism resolves such a challenge by eliminating the spurious correlation, which enables exploiting the essential information, e.g., the observations of the optimal action in the dataset, in a minimax optimal manner. Although the optimal action is unknown, our algorithm adapts to identify the essential information in the dataset via the oracle property. See Section \ref{sec:pess} for a detailed discussion. 

Our work adds to the recent works on pessimism \citep{yu2020mopo, kidambi2020morel, kumar2020conservative, liu2020provably, buckman2020importance}. Specifically, \cite{yu2020mopo, kidambi2020morel} propose a pessimistic model-based approach, while \cite{kumar2020conservative} propose a  pessimistic value-based approach, both of which demonstrate  empirical successes. From a theoretical perspective, \cite{liu2020provably} propose a regularized (and pessimistic) variant of the fitted Q-iteration algorithm \citep{antos2007fitted, antos2008learning, munos2008finite}, which attains the optimal policy within a restricted class of policies without assuming the sufficient coverage of the dataset. In contrast, our work imposes no restrictions on the affinity between the learned policy and behavior policy. In particular, our algorithm attains the information-theoretic lower bound for the linear MDP \citep{yang2019sample, jin2020provably} (up to multiplicative factors of the dimension and horizon), which implies that given the dataset, the learned policy serves as the ``best effort'' among all policies since no other can do better. From another theoretical perspective, \cite{buckman2020importance} characterize the importance of pessimism, especially when the assumption on the sufficient coverage of the dataset fails to hold. In contrast, we propose a principled framework for achieving pessimism via the notion of uncertainty quantifier, which serves as a sufficient condition for general function approximators. See Section \ref{sec:pess} for a detailed discussion. Moreover, we instantiate such a framework for the linear MDP and establish its minimax optimality via the information-theoretic lower bound.~In other words, our work complements \cite{buckman2020importance} by proving that pessimism is not only ``important'' but also optimal in the sense of information theory. 

%A more related work is \cite{buckman2020importance}, which proves the importance of   pessimism in offline RL, especially when the dataset if not informative for any policy. 
%Compared with this work, in terms of  algorithm design, we propose a principled methodology for achieving pessimism via the notion of $\xi$-uncertainty quantifiers, which readily incorporates function approximation. 
%Besides, in terms of theory,  we establish an upper bound on the suboptimality  in terms of   $\xi$-uncertainty quantifiers, which is shown to be minimax-optimal under the linear setting. 
%Our work complements \cite{buckman2020importance} by showing that pessimism is not only ``important'' but also minimax-optimal. 

%finds the optimal policy within a restricted subclass of policies without assuming the behavior policy is explorative. 

%liu2020provably, yu2020mopo, kidambi2020morel, kumar2020conservative, buckman2020importance

%addresses such a challenge by 

%assumptions fail, liu/buckman

%our contribution

%also explains empirical work

%{\red ---------}
%
%Since    trajectories in the given dataset and that generated by the target policy or the optimal policy can have diverse distributions,
%the discrepancy between these distributions, also known as distributional shift \citep{levine2020offline}, 
%essentially captures the fundamental challenge of offline RL. 
%For large-scale problems where the state and action spaces are enormous,   coupled with function approximators, such a challenge is further exacerbated.   
%A line of research proposes a variety of offline deep RL algorithms that aim to address such a challenge \citep{fujimoto2019off, kumar2019stabilizing, wu2019behavior, laroche2019safe, agarwal2020optimistic, wang2020critic, siegel2020keep, nair2020accelerating},  but  the efficacy of these methods are only demonstrated via experiments. 
%From a theoretical perspective, however, 
%nearly all of the existing theories hinge on   certain distributional assumptions on the behavior policy. 
%Specifically,  
%various works 
%assume  that the visitation measure induced by the  behavior policy is  well-explored in the sense that it  has uniform coverage over the state-action or the feature space 
%  \citep{yin2020asymptotic, yin2020near, qu2020finite, li2020sample,duan2020minimaxoptimal,   wang2020statistical}.  
%Besides, under the tabular or function approximation setting, a large body of works have constructed various  offline estimators under the assumption that the 
% importance-sampling ratio or the visitation measure density ratio  involving the   behavior   and  target policy is bounded. See, e.g., \cite{jiang2016doubly, thomas2016data, farajtabar2018more, liu2018breaking, xie2019towards,tang2019doubly, 
%chen2019information,nachum2019dualdice, 
%uehara2020minimax,
%kallus2019efficiently,
%kallus2020doubly, 
%jiang2020minimax, nachum2020reinforcement,   liao2020batch, xie2020batch, 
%xie2020q, 
%nachum2019algaedice,
%zhang2019gendice,  
%yang2020off, zhang2020variational} and the references therein. 
%Furthermore, to  incorporate powerful function approximators such as kernel functions and neural networks, another line of works adopts the assumption that the concentrability coefficients involving the behavior policy  are bounded \citep{antos2007fitted, munos2008finite, antos2008learning, farahmand2010error, scherrer2015approximate, farahmand2016regularized,chen2019information, fan2020theoretical,liu2019neural, wang2019neural, fu2020single}.
%However, when these distributional assumptions on the data-generating process  fail, the theories in all of these aforementioned works are brittle. 
%
%
%Furthermore, our work is closely related to a recent strand of research that develops offline RL algorithms based on the pessimism principle \citep{liu2020provably, yu2020mopo, kidambi2020morel, kumar2020conservative, buckman2020importance}.  Specifically, \cite{liu2020provably} proposes a pessimistic modification of fitted Q-iteration   \citep{antos2007fitted,munos2008finite}, which provably finds the optimal policy within a restricted subclass of policies without assuming the  behavior policy is   explorative. 
%Meanwhile, \cite{yu2020mopo,kidambi2020morel} and \cite{kumar2020conservative} propose   pessimistic 
%variants of model-based RL  and Q-learning 
%algorithms, respectively. 
%A more related work is \cite{buckman2020importance}, which proves the importance of   pessimism in offline RL, especially when the dataset if not informative for any policy. 
%Compared with this work, in terms of  algorithm design, we propose a principled methodology for achieving pessimism via the notion of $\xi$-uncertainty quantifiers, which readily incorporates function approximation. 
%Besides, in terms of theory,  we establish an upper bound on the suboptimality  in terms of   $\xi$-uncertainty quantifiers, which is shown to be minimax-optimal under the linear setting. 
%Our work complements \cite{buckman2020importance} by showing that pessimism is not only ``important'' but also minimax-optimal. 
%
%{\red \bf ENDS HERE}
%\vspace{5pt}
%
%
%Robust MDP, empirical likelihood
%
% \iffalse 
%
%Survey: \cite{levine2020offline, lange2012batch} 
%
%
%Dataset \cite{fu2020d4rl}
%
%
%Pessimism: \cite{yu2020mopo, kidambi2020morel, kumar2020conservative, buckman2020importance} (contain both experiments and theory works here) 
%
%
%Heuristic methods: \cite{fujimoto2019off, kumar2019stabilizing, wu2019behavior, laroche2019safe, agarwal2020optimistic, wang2020critic, siegel2020keep, nair2020accelerating} 
%
%Benchmarks: \cite{gulcehre2020rl}
%
%Compare with \cite{liu2020provably}
%
%Copy refs from \cite{nair2020accelerating}
%
%
%
%off-policy evaluation. 
%
%Pessimism 
%
%Robust MDP 
%
% \fi 