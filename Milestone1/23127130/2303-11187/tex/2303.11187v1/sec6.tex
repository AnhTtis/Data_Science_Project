% !TEX root = paper.tex
%\begin{flushleft}
\section{Application of CCB-IV: Linear Dynamic Treatment Regimes} \label{sec:DTR}




% \begin{figure}
% \centering
%   % Requires \usepackage{graphicx}
%   \subfloat[DAG of the observational process in DTR]{\includegraphics[width=4.5cm]{figure/DTR_behavior.pdf}
%   % \label{fig:DTR behavior}
%   }
%   \hspace{1.5cm}
%   \subfloat[DAG of the interventional process in DTR]{\includegraphics[width=4.5cm]{figure/DTR_evaluation.pdf}
%   % \label{fig:DTR evaluation}
%   }\\
% %   \vspace{0.5cm}
%    \caption{A DAG illustrating the DTR model. Note that $Y_2$ depends on the whole trajectory $H$}\label{fig:DTR}
% \end{figure}
\paragraph{Background.} Dynamic treatment regimes (DTRs) is an extension of the individualized treatment rules (ITRs) to multi-steps. Estimating optimal policy can be challenging with unmeasured confounders in the observational dataset
\cite{chen2021estimating, qi2021proximal, NEURIPS2019_8252831b, singh2022automatic}.
We consider a DTRs with two steps, which is graphically represented in Figure \ref{fig:DTR}. 

\begin{figure}[h]  
\centering 
  \begin{subfigure}[b]{0.4\linewidth}
  \centering
    \begin{tikzpicture}
        \node[missingstate] (X1) {$X_1$};
        \node[missingstate] (X2) [right=of X1] {$X_2$};
        \node[state] (A1) [below=of X1] {$A_1$};
        \node[state] (A2) [below=of X2] {$A_2$};
        \node[state] (Y1) [above=of X2] {$Y_1$};
        \node[state] (Y2) [right=of X2] {$Y_2$};
        \node[hiddenstate] (U) [below=of Y2] {$U$};
        \node[draw, fit=(X1) (X2) (A1) (A2)] (H) {};

        \path[normal] (X1) edge (A1);
        \path[normal, blue] (X1) edge (A2);
        \path[normal] (X1) edge (X2);
        \path[normal] (X1) edge (Y1);
        \path[normal, blue] (X2) edge (A2);
        \path[normal] (X2) edge (Y1);
        \path[normal] (A1) edge (Y1);
        \path[normal] (A1) edge (X2);
        \path[normal, blue] (A1) edge (A2);
        \path[normal, blue] (Y1) edge[bend left=40] (A2);
        \path[normal] (U) edge (X2);
        \path[normal] (U) edge (Y2);
        \path[normal, blue] (U) edge (A2);
        \path[normal] (H) edge (Y2);
    \end{tikzpicture}
    \caption{DAG of the observational process in DTR}
  \end{subfigure}\qquad
\begin{subfigure}[b]{0.4\linewidth}
\centering
  \begin{tikzpicture}  
        \node[state] (X1) {$X_1$};
        \node[state] (X2) [right=of X1] {$X_2$};
        \node[state] (A1) [below=of X1] {$A_1$};
        \node[state] (A2) [below=of X2] {$A_2$};
        \node[state] (Y1) [above=of X2] {$Y_1$};
        \node[state] (Y2) [right=of X2] {$Y_2$};
        \node[hiddenstate] (U) [below=of Y2] {$U$};
        \node[draw, fit=(X1) (X2) (A1) (A2)] (H) {};

        \path[normal] (X1) edge (A1);
        \path[normal, red] (X1) edge (A2);
        \path[normal] (X1) edge (X2);
        \path[normal] (X1) edge (Y1);
        \path[normal, red] (X2) edge (A2);
        \path[normal] (X2) edge (Y1);
        \path[normal] (A1) edge (Y1);
        \path[normal] (A1) edge (X2);
        \path[normal, red] (A1) edge (A2);
        \path[normal] (U) edge (X2);
        \path[normal] (U) edge (Y2);
        \path[normal] (H) edge (Y2);
\end{tikzpicture}
\caption{DAG of the interventional process in DTR}
\end{subfigure}
\caption{A DAG illustrating the DTR model. Note that $Y_2$ depends on the whole trajectory $H$}\label{fig:DTR}
\end{figure}  

In the observational process, at stage $i\in\{1, 2\}$, the treatment $A_i$ is selected based on the current state $X_i$ and the historical information $\{(X_j, A_j, Y_j)\}_{j=1}^{i-1}$. Then the state transits to $X_{i+1}$ and a reward $Y_i$ is generated. At the second state, $X_2$, $A_2$ and $Y_2$ are confounded by an unmeasured confounder $U$. Since the first step is not influenced by $U$, we have $Y_1\indep U\given (X_1, X_2, A_1)$. Therefore, we see that the first state reward $Y_1$ serves as an instrumental variable to $A_2$. Here, we provide table \ref{tab:DTRs} to illustrate the mapping from this two-stage DTRs to the CCB-IV model. 
\begin{table}[]
    \centering
    \begin{tabular}{  c  c c c} 
  \hline
  Variable Type & Two-step DTRs & Observability in $\cD$ & Correspondence to CCB-IV\\ 
  \hline
  confounder    &   $U$                 & unobservable  &   $U$\\
  context       &   $(X_1, X_2, A_1)$   & partially missing  & $X$ \\
  treatment     &   $A_2$               & observable & $A$\\
  outcome       &   $Y_2$               & observable & $Y$\\
  IV            &   $Y_1$               & observable & $Z$\\
  \hline
\end{tabular}
    \caption{Mapping of variables from the two-state DTRs to the CCB-IV.}
    \label{tab:DTRs}
\end{table}
We assume that $Y_2=f(X_1, X_2, A_1, A_2)+\varepsilon$ where $\varepsilon\indep (A_1, A_2)\given (U, X_1, X_2)$ and that $Y_1$ and $Y_2$ are fully observed, which satisfies the structured reward assumption. 
For $Y_1$ to function as an IV, we require that $Y_1$ is complete over $(X_1, X_2, A_1, A_2)$. 
Additionally, in the observational process, the missingness indicator $R_{X_1}$ is caused by $(X_1, A_1)$ and the missingness indicator $R_{X_2}$ is caused by $(X_1, A_1, X_2, A_2)$. 
Therefore, the model assumptions for CCB-IV are satisfied. 
By Theorem \ref{thm:IV identification}, the CATE $g(x_1, a_1, x_2, a_2)=\EEob\sbr{Y_2\given x_1, a_1, x_2, \doopt(a_2)}$ is identified by
%\begin{gather}
\begin{align}
    &\EEob\sbr{h_1(A, Y) - Y_2\given Y_1=y_1}=0, \label{eq:DTR ID 1}\\
    &\EEob\sbr{g(X, A)-h_1(A, Y) \given (X, A, Y_1)=(x, a, y_1), (R_{X_1}, R_{X_2})=\ind}=0.\label{eq:DTR ID 2}
\end{align}
%\end{gather}
We assume that the solutions $h_1$ and $g$ always exist, which requires certain completeness conditions for $Y_2$ to restore the missingness in $X_1$ and $X_2$.
Here, we let $X=(X_1, X_2)$, $A=(A_1, A_2)$ and $Y=(Y_1, Y_2)$ in the remaining part of the section for DTRs example. 
% Notice that the definitions of $X$, $A$ and $Y$ here are different from their previous ones. 
Note that $Y_2\indep \pib_1\given (X_1, A_1, X_2)$ in the observational settings, since $A_1$ is not confounded. We thereby have $g(x, a)=\EEob\sbr{Y_2\given x, \doopt(a)}$.
Our optimization target thereby corresponds to maximizing the average reward function on $\pie=(\pi_1, \pi_2)\in\Pi$,
\begin{align*}
    v(g, \pie)=&\int_{\cX\times\cA} g(x,a) \tpr(x_1) \pob(x_2\given x_1, a_1)   \pi_1(a_1\given x_1) \pi_2(a_2\given x_1, a_1, x_2)
    \rd x\rd a,
\end{align*}
where we assume that $\pob(x_2\given x_1, a_1)$ is already known for brevity, although a little extension of our framework is capable of dealing with unknown $\pob(x_2\given x_1, a_1)$ by learning from data. Moreover, we only consider $Y_2$ as the reward. We remark that, since the first stage is not confounded, $\EEob\sbr{Y_1\given x_1, \doopt(a_1), x_2}$ can also be easily learned and integrated into the average reward. Now we pose the following assumptions on the linearity of the DTRs model.

\paragraph{Linear function class.} We make the following assumptions to ensure the existence of the bridge functions and the linearity of our DTRs model.
\begin{assumption}[Existence of linear bridge functions]\label{asp:DTRs linear 1}
We assume that a solution $\vhstar=(\hstark{1}, \gstar)$ exists to the IES given by \eqref{eq:DTR ID 1} and \eqref{eq:DTR ID 2}. Furthermore, we assume that $\hstark{1}, \gstar$ fall into the following function classes,
%\begin{gather*}
\begin{align*}
    &\cH_1=\{h_1\given h_1(\cdot)=w_1^\top \phi_1(\cdot), \nbr{w_1}_2\le C_1, \nbr{\phi_1(\cdot)}_2\le 1\}, \\
    &\cG=\{g\given g(\cdot)=w_2^\top \phi_2(\cdot), \nbr{w_2}_2\le C_2, \nbr{\phi_2(\cdot)}_2\le 1\}, 
\end{align*}
%\end{gather*}
where $\phi_1:\cA\times\cY\rightarrow \RR^{m_1}$ and $\phi_2:\cX\times\cA\rightarrow \RR^{m_2}$.
Moreover, we assume that $\hstark{1}=(w_1^*)^\top \phi_1$ and $\gstar=(w_2^*)^\top\phi_2$.
% \begin{itemize}
% \item[(i)] For the exact CATE, we assume that $\gstar\in\cG$, where $\cG=\{w_2\in\RR^{m_2}:\cX_1\times \cA_1\times \cX_2\times \cA_2\rightarrow w_2^\top\phi_2(\cdot), \nbr{w_2}_2\le C_2, \nbr{\phi_2(\cdot)}_2\le 1\}$ is a linear function class with kernel $\phi_2$ and $w_2^*$ is the corresponding parameter for $g^*$. 
% \item[(ii)] We assume that there exists $\hstark{1}\in \cH_1$ satisfying \eqref{eq:DTR ID 2} with $g=\gstar$, where $\cH_1=\{w_1\in\RR^{m_1}:\cA_1\times \cY_1\times \cA_2\times \cY_2\rightarrow w_1^\top\phi_1(\cdot), \nbr{w_1}_2\le C_1, \nbr{\phi_1(\cdot)}_2\le 1\}$ and $w_1^*$ is the corresponding parameter for $\hstark{1}$. 
% \end{itemize}
\end{assumption}
Assumption \ref{asp:DTRs linear 1} assumes the existence and linearity of $\vh$.
We remark that it suffices for (i) to hold if $\EEob\sbr{Y_2\given y_1}$ is captured by the linear kernel $\EEob\sbr{\phi_2(X, A)\given y_1}$, and it suffices for assumption (ii) to hold if $\gstar(x, a)$ is captured by the linear kernel $\EEob\sbr{\phi_1(A, Y)\given y_1, x, a}$ for any $y_1\in\cY_1$.
In addition, Assumption \ref{asp:DTRs linear 1} also suggests that by using $\cH=\cH_1\times \cG$ as the hypothesis class, we have no realizability error, i.e., $\vareH=0$.
For the linear kernel $\phi_1$ and $\phi_2$, we continue to assume that their conditional expectation also falls into some linear spaces. 
\begin{assumption}[Linearity of dual function class]\label{asp:DTRs linear 2}
We assume that the conditional expectations of kernel $\phi_1$ and $\phi_2$ with respect to \eqref{eq:DTR ID 1}  and \eqref{eq:DTR ID 2} satisfy,
\begin{itemize}
\item[(i)] $\EEob\sbr{\phi_1(A, Y)\given y_1} = W_1 \psi_1(y_1)$, where $\psi_1:\cY_1\rightarrow \RR^{d_1}$, $W_1\in\RR^{m_1\times d_1}$.
\item[(ii)] $\EEob\sbr{\phi_1(A, Y)\given (x, a, y_1), (R_{X_1}, R_{X_2})=\ind}=W_2\psi_2(x, a, y_1)$, where $\psi_2:\cX_1\times\cA_1\times\cX_2\times\cA_2\times\cY_1\rightarrow \RR^{d_2}$, $W_2\in\RR^{m_1\times d_2}$.
\item[(iii)] $\EEob\sbr{\phi_2(X, A)\given (x, a, y_1), (R_{X_1}, R_{X_2})=\ind}=W_3\psi_2(x, a, y_1)$, where $W_3\in\RR^{m_2\times d_2}$.
\end{itemize}
\end{assumption}
In Assumption \ref{asp:DTRs linear 2}, we remark that if the operator $T:\cF(\cA\times\cY)\rightarrow \cF(\cY_1)$ defined as $Tf(y_1) =\EEob\sbr{f(A, Y)\given y_1}$ is captured by the kernel $\psi_1(y_1)$, it suffices for $W_1$ to exists. Similarly, it suffices for (ii), (iii) to hold if the corresponding operators are captured by the linear kernel $\psi_2(x, a, y_1)$.
Following Assumption \ref{asp:DTRs linear 2}, it holds for the linear operator $\cT$ that
%\begin{gather*}
\begin{align*}
    &\cT_1 \vh(y_1) = (w_1-w_1^*)^\top W_1 \psi_1(y_1), \\
    &\cT_2 \vh(x, a, y_1) = \rbr{(w_2-w_2^*)^\top W_3-(w_1-w_1^*)^\top W_2}\psi_2(x, a, y_1), 
\end{align*}
%\end{gather*}
which suggests that $\cT_1\vh$ and $\cT_2\vh$ fall into the following linear function classes, 
\begin{align*}
    \Theta_k=\{\theta_k\given \theta_k(\cdot)= \beta_k^\top \psi_k(\cdot), \beta_k\in\RR^{d_k}, \nbr{\beta_k}\le D_k, \nbr{\psi_k(\cdot)}_2\le 1\}, 
\end{align*}
where we have $D_1>2C_1 \nbr{W_1}_F$ and $D_2> 2 (C_1\nbr{W_2}_F+C_2\nbr{W_3}_F)$.
By further letting $\vTheta=\Theta_1\times \Theta_2$, the dual function space has full compatibility, i.e., $\vareH$ and $\vareTheta$ are both zero. By Theorem \ref{thm:IV subopt}, we have the following corollary to establish the convergence of the sub-optimality for the two-step linear DTRs.
\begin{corollary}[Convergence of sub-optimality for linear DTRs]
Suppose that Assumptions \ref{asp:DTRs linear 1} and \ref{asp:DTRs linear 2} hold. Let $e_{\cD}>(2 L_\alpha^2+5/ 4)\eta^2$ where $\eta=\sum_{k=1}^2\eta_k^2$ and $\eta_k$ bounds the critic radius for function class $\cQ_k=\{\alpha_k(\vh, \cdot)\theta_k: \vh\in\vH, \theta_k\in\Theta_k\}$. Assume that for any $x\in\cX$ and $a\in\cA$, there exists $b_1:\cY_1\rightarrow \RR$ satisfying
\begin{align*}
    \EEob\sbr{b_1(Y_1)\given x, a} = \frac{\tpr(x_1)\pob(x_2\given x_1, a_1)\piestar_1(a_1\given x_1)\piestar_2(a_2\given x_1, a_1, x_2)}{\pob(x_1, a_1, x_2, a_2)}.
\end{align*}
The sub-optimality of $\piepessi$ for the two-step DTRs is bounded with probability at least $1-4\xi$ by
\begin{align*}
    \SubOpt(\piepessi) \lesssim  \rbr{\nbr{b_1}_{\mu_1, 2}+\nbr{b_2}_{\mu_2, 2}}\cdot \rbr{O\rbr{\sqrt{e_\cD}} +  O\rbr{\eta}},
\end{align*}
where $b_2:\cX\times\cA\times\cY_1\rightarrow \RR$ is defined by
\begin{gather*}
    b_2(x,a, y_1)=\frac{b_1(y_1)\pob(x_1, a_1, x_2, a_2, y_1)}{\pob(x_1, a_1, x_2, a_2, y_1\given(R_{X_1}, R_{X_2})=\ind)}.
\end{gather*}
\end{corollary}
As is proved in \S\ref{app:DTRs critical radius}, the critical radiuses are of order $\eta_1=\cO(\sqrt{(m_1+d_1)\log T/T})$ and $\eta_2=\cO(\sqrt{\max\{m_1+m_2+d_2\}\log T_2/T_2})$, where $T$ corresponds to the size of the whole dataset $\cD$ and $T_2$ corresponds to the size of the dataset satisfying $(R_{X_1}, R_{X_2})=\ind$. Such a result shows that the convergence rate is of the order $\cO(\sqrt{\log T_2/T_2})$ if we choose $e_\cD=\cO(\log T_2/T_2)$.
Note that $T_2$ is the total number of samples that are subject to no missingness, which requires that there should be a fixed proportion of samples on which we have fully observed contexts and side-observations to guarantee the fast convergence rate.