% !TEX root = paper.tex
\section{Causal Identification of CATE}\label{sec:Identification}
In this section, we show how to identify the CATE for CCB-IV and CCB-PV with missingness. Under certain completeness assumptions, the CATE is learnable through solving an integral equation system (IES). We also give explanations for the IES in the matrix form and compare the IES to standard identification equations without missingness to highlight how we address the missingness issue. A united form for the IES with a linear operator $\cT$ is provided in \S\ref{sec:united form}.

\subsection{Identification in Confounded Contextual Bandit with Instrumental Variable}

\begin{figure}[h]  
\centering 
  \begin{subfigure}[b]{0.4\linewidth}
  \centering
    \begin{tikzpicture}
        \node[hiddenstate] (U) {$U$};
        \node[missingstate] (X) [below=of U] {$X$};
        \node[state] (A) [left=of X, xshift=.27cm, yshift=-1cm] {$A$};
        \node[state] (Y) [right=of X, xshift=-.27cm, yshift=-1cm] {$Y$};
        
        \path[double] (U) edge (X);
        \path[blue, normal] (U) edge (A);
        \path[blue, normal] (X) edge (A);
        \path[normal] (A) edge (Y);
        \path[normal] (U) edge (Y);
        \path[normal] (X) edge (Y);

        \node[missingstate] (Z) [above=of A] {$Z$};
        \path[blue, normal] (Z) edge (A);
        \path[normal] (X) edge (Z);
    \end{tikzpicture}%
    \caption{DAG of the observational process in CCB-IV}\label{fig:IV behavior}
  \end{subfigure}\qquad
\begin{subfigure}[b]{0.4\linewidth}
\centering
  \begin{tikzpicture}  
        \node[hiddenstate] (U) {$U$};
        \node[missingstate] (X) [below=of U] {$X$};
        \node[state] (A) [left=of X, xshift=.27cm, yshift=-1cm] {$A$};
        \node[state] (Y) [right=of X, xshift=-.27cm, yshift=-1cm] {$Y$};
        
        \path[double] (U) edge (X);
        \path[red, normal] (X) edge (A);
        \path[normal] (A) edge (Y);
        \path[normal] (U) edge (Y);
        \path[normal] (X) edge (Y);

        \node[hiddenstate] (Z) [above=of A] {$Z$};
        \path[normal] (X) edge (Z);
\end{tikzpicture}
\caption{DAG of the interventional process in CCB-IV}\label{fig:IV evaluation}
\end{subfigure}
\caption{A DAG illustrating the introduction of side observations ($O=Z$) in CCB-IV and the difference between the observational model and the interventional model in CCB-IV. Here,  the white nodes represent observed variables, the light grey nodes represent the variables with missingness not at random and the dark nodes represent unmeasured variables. A line with arrows at both ends means that the causal effect going in each way is allowed and therefore, the direction is not specified.}\label{fig:CCB-IV}
\end{figure}  


% % \label{subsec:CCB-IV}
% \begin{figure}
% \centering
%   % Requires \usepackage{graphicx}
% %   \subfloat[DAG of the CCB]{\includegraphics[height=3.5cm]{figure/CCB.pdf}
% %   } \hspace{0.25cm}
% %   $\Rightarrow$
% %   \hspace{0.25cm}
%   \subfloat[DAG of the observational process in CCB-IV]{\includegraphics[height=3.5cm]{figure/IV_behavior.pdf}\label{fig:IV behavior}}
%   \hspace{0.5cm}
%   \subfloat[DAG of the interventional process in CCB-IV]{\includegraphics[height=3.5cm]{figure/IV_evaluation.pdf}
%   % \label{fig:IV evaluation}
%   }\\
% %   \vspace{0.5cm}
%    \caption{A DAG illustrating the introduction of side observations ($O=Z$) in CCB-IV and the difference between the observational model and the interventional model in CCB-IV. Here,  the white nodes represent observed variables, the light grey nodes represent the variables with missingness not at random and the dark nodes represent unmeasured variables. A line with arrows at both ends means that the causal effect going in each way is allowed and therefore, the direction is not specified.}\label{fig:CCB-IV}
% \end{figure}


Instrumental variable (IV) regression is a method in causal statistics for estimating the confounded causal effect of treatment $A$ on outcome $Y$. Researchers in
economics employ IV to overcome issues of strategic interaction, e.g., supply cost shifters ($Z$) only influence sales ($Y$) via price ($A$), thereby identifying counterfactual demand even though prices are confounded by supply and demand market forces \citep{wright1928tariff, blundell2012measuring}.
% Another example is that draft lottery number
% ($Z$) only influences lifetime earnings ($Y$) via military service ($A$), thereby identifying the counterfactual
% effect of military service on earnings despite selection bias in enlistment \citep{angrist1990lifetime}.


Our model for confounded contextual bandit with instrumental variable (CCB-IV) is illustrated in Figure \ref{fig:CCB-IV}. In contrast to the standard IV model without context, we assume the IV to depend on the context $X$, for the reason that the IV usually appears as a recommendation of the treatment given by an advisor based on the current context.
The model assumptions for CCB-IV are summarized as follows.
\begin{assumption}[Model assumptions for observational process in CCB-IV]\label{asp:CCB-IV}
We assume that the following assumptions hold for the observational process of the CCB-IV.
\begin{itemize}
    \item[(i)] (Structured reward). $Y=f(A, X)+\epsilon$ where $\epsilon \indep A\given (X, U)$;
    \item[(ii)] (IV completeness). $\EEob[\sigma(X, A)\given Z=z, R_Z=1] = 0$ holds for all $z\in\cZ$ if and only if $\sigma(X, A)=0$ holds almost surely;
    \item[(iii)] (IV independence). For the IV, we assume that $Z\indep (U, \epsilon)\given X$;
    % \item[(iii)] (Outcome completeness). For any $z\in\cZ$, $a\in\cA$, $\EE[\sigma(X)\given Y=y, Z=z, A=a]=0$ holds for all $y\in\cY$ if and only if $\sigma(X)=0$ holds almost surely.
    \item[(iv)] (Unconfounded and outcome-independent missingness). 
    We allow $R_X$ to be caused by $(Z, X, A)$ and $R_Z$ to be caused by $(Z, X)$.
    % We assume that $Z$ is bounded complete over $(X, A)$ and that $Y$ is bounded complete over $X$ given $(A, Z)$.
\end{itemize}
\end{assumption}
The model assumption in (i) can be viewed as a generalization of the semi-parametric contextual bandits, whose outcome is given by $Y(a)=\langle \theta, X_a\rangle+g(X)+\epsilon$ when selecting treatment $a$ \citep{krishnamurthy2018semiparametric}.
%Here, the context $X=(X_{a_1}, \cdots, X_{a_{|\cA|}})$ is a tuple and each element corresponds to a feature for an action.
The context $X=(X_{a_1}, \cdots, X_{a_{|\cA|}})$ is a tuple and each element corresponds to a feature for an action.
Moreover, $\epsilon$ can be viewed as a treatment-independent noise.
The IV completeness assumption in (ii) ensures that different IV ($Z$) generates enough variation in $(X, A)$ and the IV independence assumption in (iii) ensures that the IV is not confounded and is independent of the noise.
Combining (i) and (iii) we see that the IV is also outcome-independent, i.e., $Z\indep Y\given (A,U,X)$.
We remark that assumptions (i)-(iii) are standard in the IV literature \citep{baiocchi2014instrumental, newey2003instrumental, singh2019kernel, chen2016methods, chen2011nonlinear}, and an IV satisfying these assumptions is referred to as a valid IV. 
Assumption (iv) shows that the missingness is unconfounded and outcome-independent, since $R_X$ and $R_Z$ are neither caused nor have a direct effect on $U$ and $Y$.
We remark that the missingness issue cannot be addressed trivially by only using the dataset subject to $(R_X, R_Z)=\ind$. Note that we have $(U, Z)\rightarrow A$ and $A\rightarrow R_X$ in Figure \ref{fig:IV behavior}. Conditioning on $R_X=\ind$ will therefore create an edge between $Z$ and $U$ and break the IV independence assumption.
Fortunately, we have the following theorem to identify the CATE in the CCB-IV model with missingness.
\begin{theorem}[IES for CATE identification in CCB-IV]\label{thm:IV identification}
Suppose that Assumption \ref{asp:CCB-IV} holds.
If there exist functions $h_1:\cY\times\cA\times\cZ\rightarrow \RR$ and $g:\cA\times\cX\rightarrow \RR$ satisfying,
\begin{align}
    &\EEob\sbr{h_1(Y, A, Z) - Y\given Z=z, R_Z=1} = 0, \label{eq:IV bridge 1}\\
    &\EEob\sbr{g(X, A) - h_1(Y, A, Z)\given A=a, X=x, Z=z, (R_Z, R_X)=\ind}=0 \label{eq:IV bridge 2}, 
\end{align}
it follows that $g(x, a)\overset{\text{a.s.}}{=}\gstar(x, a)$ where $\gstar(x, a)$ is the CATE.
\begin{proof}
See \S\ref{pro:IV identification} for a detailed proof.
\end{proof}
\end{theorem}
\paragraph{An understanding of Theorem \ref{thm:IV identification} in the matrix form.}
We give a matrix explanation of the method we use to overcome the confounding and missingness issue in Theorem \ref{thm:IV identification}. 
We first study what happens if there is no missingness issue but just confounding effect.
Suppose $R_Z\equiv R_X\equiv 1$, a simple Combination of \eqref{eq:IV bridge 1} and \eqref{eq:IV bridge 2} gives the following identification equation, 
\begin{align}
    \EEob\sbr{Y\given Z=z}
    % =\sum_{a\in\cA} g(a, X)P(X, a\given z, R_Z=1)
    =\EEob\sbr{g^*(A, X)\given Z=z},\label{eq:IV standard id}
\end{align}
which corresponds to the standard identification in the IV model \citep[Proposition 3.1]{liao2021instrumental}. Since \eqref{eq:IV standard id} is learnable from the dataset without missingness, we can thus overcome the confounding issue and recover the CATE with the distributional information encoded in the side observation $Z$.

For the missingness issue, we remark that additionally conditioning on $R_Z=1$ on both sides of \eqref{eq:IV standard id} still recovers the exact CATE, i.e., 
\begin{align}\label{eq:iv matrix id with R_Z}
\EEob\sbr{Y\given Z=z, R_Z=1}
    % =\sum_{a\in\cA} g(a, X)P(X, a\given z, R_Z=1)
    =\EEob\sbr{g^*(A, X)\given Z=z, R_Z=1},
\end{align}
which is proved in \S\ref{pro:IV identification}.
Thus, we just need to focus on the missingness in $X$.
The difficulty is that we cannot simply evaluate the right-hand side of \eqref{eq:IV standard id} based on the observed data since $\pob(x, a\given z, R_Z=1)\neq \pob(x, a\given z, (R_X, R_Z)=\ind)$. 
% Moreover, conditioning on $R_X=1$ on both sides of \eqref{eq:IV standard id} does not apply either, since $R_X=1$ breaks the IV independent assumption that $Z\indep U\given X$.
To address the problem of missingness in $X$, we have the following observation
\begin{align}
    P(Y, a\given z, R_Z=1) = P(Y\given X, a, z, (R_Z, R_X)=\ind) \cdot  P(X, a\given z, R_Z=1), \label{eq:IV chain rule}
\end{align}
where $P(Y\given X)=\{\pob(y_i\given x_j)\}_{ij}$ denotes a matrix of size $|\cY|\times |\cX|$ whose element in row $i$ and column $j$ is $\pob(y_i\given x_j)$. In the following, we use capital $P$ to denote the matrix formed by matrixing the mass function. 
We remark that \eqref{eq:IV chain rule} is a direct result following the chain rule and the fact that $R_X$ is outcome-independent, i.e., $R_X\indep (Y, R_Z)\given (A, Z, X)$.
By assuming $\text{rank}(P(Y\given X, a, z, (R_X, R_Z)=\ind))=|\cX|$, the Moore-Penrose inverse exists and we have $P(X, a\given z, R_Z=1)=P(Y\given X, a, z, (R_Z, R_X)\allowbreak =\ind)^\dagger P(Y, a\given z, R_Z=1)$.
Now we can rewrite \eqref{eq:iv matrix id with R_Z} as
\begin{align}
    \EEob\sbr{Y\given Z=z, R_Z=1} &=\sum_{a\in\cA}\underbrace{g(a, X)P(Y\given X, a, z, (R_X, R_Z)=\ind)^\dagger}_{\displaystyle{=: h_1(Y, a, z)}} P(Y, a\given z, R_Z=1),\label{eq:IV matrix id}
\end{align}
where $g(X, a)=\{g(x_i, a)\}_{i}$ and $P(Y, a\given z, R_Z=1)=\{p(y_i, a\given z, R_Z=1)\}_i$ are column vectors and $h_1(Y, a, z)=\{h_1(y_i, a, z)_i\}$ is a column vector defined by the under-brace in \eqref{eq:IV matrix id}.
The benefit of introducing $h_1$ is that $h_1$ can be directly learned from the observed dataset.
As a matter of fact, the definition of $h_1$ leads to \eqref{eq:IV bridge 2} and plugging $h_1$ into \eqref{eq:IV matrix id} gives \eqref{eq:IV bridge 1}.
For the bridge function $h_1$ to exist, we need $\text{rank}(P(Y\given X, a, z))=|\cX|$, which implies that the conditional distribution of $Y$ should be informative enough to recover the missing distribution of $X$.
So to overcome the missingness issue, we additionally exploit the \emph{distributional information} of the outcome rather than merely using the \emph{average}. 
In the continuous setting, the equivalent condition for $h_1$ to exist can be expressed as follows.
\begin{remark}[Condition for the existence of a solution to the IES in Theorem \ref{thm:IV identification}]\label{rmk:IV existence}
A solution $h=(h_1, g)$ to the IES in Theorem \ref{thm:IV identification} exists if and only if there exists a solution $h_1$ to the following equation, 
\begin{align}
    \EEob\sbr{\gstar(A, X)-h_1(Y, A, Z)\given A=a, X=x, Z=z, R_Z=1}=0, \quad \forall (a, x, z)\in \cA\times\cX\times\cZ, \label{cond:iv bridge exist}
\end{align}
where $\gstar$ is the exact CATE. We leave the proof for \S\ref{pro:rmk IV existence}.
\end{remark}
Such a condition corresponds to the study of the linear inverse problem. It follows from Picard's theorem that certain completeness conditions are required \citep[Theorem 2.41]{miao2018identifying, carrasco2007linear}. See \S\ref{app:linear inverse} for more details.
 

\paragraph{Integral equation system.} 
In the following, we assume that the condition in Remark \ref{rmk:IV existence} holds. 
We remark that \eqref{eq:IV bridge 1} and \eqref{eq:IV bridge 2} form an integral equation system (IES), meaning that separately solving \eqref{eq:IV bridge 1} or \eqref{eq:IV bridge 2} alone would not  give the correct answer. The reason is that not all $h_1$ satisfying \eqref{eq:IV bridge 1} respect \eqref{eq:IV bridge 2}.
% Statement (i) can be easily verified by letting $|\cY|>|\cX|$ in \eqref{eq:IV matrix id} and $h_1=g(a, X)  P(Y\given X, a, z$ $, (R_X, R_Z) = \ind)^+$ is therefore not unique.
To illustrate the point, let us consider a special tabular case. Suppose $\epsilon=0$ and $y=f(x, a)$ where $f$ is invertible with respect to $x$ for any fixed $a$. We thus have $g(x, a)=f(x,a)$ as the CATE and the independent condition $Y\indep Z\given (X, A)$. It follows from \eqref{eq:IV bridge 2} that
\begin{align*}
   h_1(Y, a, z)\given_{Y=y}=g(a, X)P(Y\given X, a, (R_X, R_Z)=\ind)^{-1}\given_{Y=y} = g(a, f^{-1}(y, a))=y,
\end{align*}
is the \textit{unique} solution for $h_1$.
% We see straightforward that any $h_1(y, a, z)$ satisfying \eqref{eq:IV bridge 2} should be independent of $z$. 
However, solving \eqref{eq:IV bridge 1} alone might give the following solution,
\begin{align*}
    h_1(y, a, z)=\EEob\sbr{Y\given Z=z, R_Z=1}.
\end{align*}
Apparently, such a solution does not respect the solution $h_1(y, a, z)=y$ given by \eqref{eq:IV bridge 2}.
Therefore, we see that \eqref{eq:IV bridge 1} and \eqref{eq:IV bridge 2} are coupled together. What matters about the IES is that we have to construct the confidence set for $h_1$ and $g$ as a whole instead of using a nested structure. Details for quantifying the uncertainty that arises from the solving the IES empirically are defered to \S\ref{sec:Estimation}.


\subsection{Identification for Confounded Contextual Bandit with Proximal Variable}
% \label{sec:CCB-PV}





% \begin{figure}[htbp]
% \centering
%   % Requires \usepackage{graphicx}
% %   \subfloat[DAG of the CCB]{\includegraphics[height=3.5cm]{figure/CCB.pdf}
% %   } \hspace{0.25cm}
% %   $\Rightarrow$
% %   \hspace{0.25cm}
%   \subfloat[$\Gb$: DAG of the observational process in CCB-PV]{
%   \includegraphics[height=3.5cm]{figure/PV_behavior.pdf}
%   % \label{fig:PV observational}
%   }
%   \hspace{0.5cm}
%   \subfloat[$\Ge$: DAG of the interventional process in CCB-PV]{\includegraphics[height=3.5cm]{figure/PV_evaluation.pdf}
%   % \label{fig:PV interventional}
%   }
% %   \vspace{0.5cm} 
%    \caption{A DAG illustrating the introduction of side observations ($O=(Z, W)$) in CCB-PV and the difference between the observational model and the interventional model in CCB-PV. Here,  the white nodes represent observed variables, the light grey nodes represent the variables with missingness not at random and the dark nodes represent unmeasured variables. A dashed line between two variables means they can either have explicit causal effect or not. A line with arrows at both ends means that the causal effect going in each way is allowed and therefore, the direction is not specified. }\label{fig:CCB-PV}
% \end{figure}

The idea behind CCB-PV is to identify the causal effect using two auxiliary side observations $Z$ and $W$ as negative controls to check for spurious relationships in the existence of unobserved confounder \citep{singh2020kernel, miao2018confounding}.
The model is depicted in Figure \ref{fig:CCB-PV}. 
% These side observations using for negative control requires that the proxy for treatment ($Z$) has no effect on the outcome ($Y$) and the proxy for the outcome ($W$) is not affected by the treatment ($A$). Moreover, additional completeness assumptions are required for the proxy to capture sufficient information of the confounding bias. 
We present the model assumptions as follows.

\begin{figure}[h]  
\centering 
  \begin{subfigure}[b]{0.4\linewidth}
  \centering
    \begin{tikzpicture}
        \node[hiddenstate] (U) {$U$};
        \node[missingstate] (X) [below=of U] {$X$};
        \node[state] (A) [left=of X, xshift=.27cm, yshift=-1cm] {$A$};
        \node[state] (Y) [right=of X, xshift=-.27cm, yshift=-1cm] {$Y$};
        
        \path[double] (U) edge (X);
        \path[blue, normal] (U) edge (A);
        \path[blue, normal] (X) edge (A);
        \path[normal] (A) edge (Y);
        \path[normal] (U) edge (Y);
        \path[normal] (X) edge (Y);

        \node[missingstate] (Z) [above=of A] {$Z$};
        \node[missingstate] (W) [above=of Y] {$W$};
        \path[normal] (A) edge (Z);
        \path[dashed, double] (X) edge (Z);
        \path[dashed, normal] (X) edge (W);
        \path[double] (U) edge (Z);
        \path[normal] (U) edge (W);
        \path[normal] (W) edge (Y);
    \end{tikzpicture}
    \caption{DAG of the observational process in CCB-PV}\label{fig:PV behavior}
  \end{subfigure}\qquad
\begin{subfigure}[b]{0.4\linewidth}
\centering
  \begin{tikzpicture}  
        \node[hiddenstate] (U) {$U$};
        \node[missingstate] (X) [below=of U] {$X$};
        \node[state] (A) [left=of X, xshift=.27cm, yshift=-1cm] {$A$};
        \node[state] (Y) [right=of X, xshift=-.27cm, yshift=-1cm] {$Y$};
        
        \path[double] (U) edge (X);
        \path[red, normal] (X) edge (A);
        \path[normal] (A) edge (Y);
        \path[normal] (U) edge (Y);
        \path[normal] (X) edge (Y);

        \node[hiddenstate] (Z) [above=of A] {$Z$};
        \node[hiddenstate] (W) [above=of Y] {$W$};
        \path[normal] (A) edge (Z);
        \path[dashed, double] (X) edge (Z);
        \path[dashed, normal] (X) edge (W);
        \path[double] (U) edge (Z);
        \path[normal] (U) edge (W);
        \path[normal] (W) edge (Y);
\end{tikzpicture}
\caption{DAG of the interventional process in CCB-PV}\label{fig:PV evaluation}
\end{subfigure}
\caption{A DAG illustrating the introduction of side observations ($O=(Z, W)$) in CCB-PV and the difference between the observational model and the interventional model in CCB-PV. Here,  the white nodes represent observed variables, the light grey nodes represent the variables with missingness not at random and the dark nodes represent unmeasured variables. A dashed line between two variables means they can either have explicit causal effect or not. A line with arrows at both ends means that the causal effect going in each way is allowed and therefore, the direction is not specified. }\label{fig:CCB-PV}
\end{figure}  

\begin{assumption}[Model assumptions for the observational process in CCB-PV]\label{asp:CCB-PV}
We assume that for the CCB-PV with outcome independent missingness, the following assumptions hold for the observational process,
\begin{itemize}
    \item[(i)] (PV completeness). 
    % For any $a\in\cA, x\in\cX$,  $\EEob\sbr{\sigma(U)\given X=x, A=a, W=w}=0$ holds for any $w\in\cW$ if and only if $\sigma(u)\overset{\text{a.s.}}{=} 0$ holds.
    For any $a\in\cA, x\in\cX$,  $\EEob\sbr{\sigma(U)\given X=x, A=a, Z=z, R_Z=1}=0$ holds for any $z\in\cZ$ if and only if $\sigma(u)\overset{\text{a.s.}}{=} 0$ holds.
    \item[(ii)] (PV independence). $W\indep A\given (U, X)$ and $Z\indep (Y, W)\given (A, X,U)$.
    \item[(iii)] (Unconfounded and outcome-independent missingness). We assume that $R_W$ is caused by $(W, X, A)$, $R_X$ is caused by $X$, and $R_Z$ is caused by $(Z, U, A, X)$.
\end{itemize}
\end{assumption}
We remark that (i) and (ii) in Assumption \ref{asp:CCB-PV} are standard for identification in the PV setting \citep{miao2018confounding, miao2018identifying, nair2021spectral, cui2020semiparametric, bennett2021proximal}. Here, (iii) is the unconfounded and outcome-independent missingness assumption, with an exception that $Z$ is allowed to be confounded. 
We remark that the missingness issue cannot be addressed trivially by conditioning on $(R_Z, R_X, R_W)=\ind$, since the PV independence assumption $W\indep A\given (U, X)$ no longer holds when conditioning on $R_W=1$. Moreover, conditioning on $R_Z=1$ also yields a distribution shift in $U$, rendering a bias in identifying the CATE. 
An example of POMDP with such a missingness mechanism is given in \S\ref{sec:POMDP}. 
Now, we provide the identification formula for the CCB-PV as follows.
\begin{theorem}[IES for CATE identification in CCB-PV]\label{thm:PV ID}
Suppose Assumption \ref{asp:CCB-PV} holds.
If there exist bridge functions $h_1:\cY\times\cA\times\cX\times\cZ\rightarrow\RR$, $h_2:\cA\times\cW\times\cX\rightarrow \RR$, $h_3:\cY\times\cA\times\cX\times\cA\rightarrow\RR$ and $g:\cX\times\cA\rightarrow \RR$ satisfying,
%\begin{gather}
\begin{align}
    &\EEob\sbr{h_1(Y, A, X, Z) - Y\given A=a, X=x, Z=z, (R_X, R_Z)=\ind} = 0,\label{eq:PV ID 1}\\
    &\EEob\sbr{h_2(A, W, X) - h_1(Y, A, X, Z)\given (A, W, X, Z)=(a, w, x, z), (R_W, R_X, R_Z)=\ind}=0, \label{eq:PV ID 2}\\
    &\EEob\sbr{h_3(Y, A, X, a')-h_2(a', W, X)\given A=a, W=w, X=x, (R_W, R_X)=\ind}=0 \label{eq:PV ID 3}, \\
    &\EEob\sbr{g(X, a') - h_3(Y, A, X, a')\given X=x, R_X=1}=0, \label{eq:PV ID 4}
\end{align}
for any $(x, a, z, w, a')\in\cX\times\cA\times\cZ\times\cW\times\cA$,
it follows that $g(x, a)\overset{\text{a.s.}}{=}\gstar(x, a)$ where $\gstar(x, a)$ is the CATE.
\begin{proof}
See \S\ref{pro:PV ID} for a detailed proof. 
\end{proof}
\end{theorem}
The matrix explanation for the CCB-PV is similar to the CCB-IV case. See \S\ref{app:PV details} for more details. We give the following conditions on the existence of a solution to the above-mentioned IES.
\begin{remark}[Conditions for existence of a solution to the IES in Theorem \ref{thm:PV ID}]\label{rmk:PV existence}
A solution $h=(h_1, h_2, h_3, g)$ to the IES in Theorem \ref{thm:PV ID} exists if and only if the following three conditions are satisfied:
\begin{itemize}
    \item[(i)] There exists a solution $h_2$ to $\EEob\sbr{h_2(A, W, X)-Y\given A=a, X=x, U=u}=0$;
    \item[(ii)] For any solution $h_2$ in (i), there exists a solution $h_1$ to \eqref{eq:PV ID 2} and a solution $h_3$ to \eqref{eq:PV ID 3}.
\end{itemize}
The proof is given in \S\ref{pro:rmk PV existence}.
\end{remark}
Conditions (i)-(ii) also require certain property of completeness, as is discussed in the CCB-IV case. See \S\ref{app:linear inverse} for more details.
Note that (i) is a standard condition in proximal causal learning \citep{miao2018identifying, miao2018confounding, cui2020semiparametric}. In a discrete setting, condition (i) means $\text{rank}(P(W\given U, x, a))=|U|$, showing that $W$ should couple enough information from the unmeasured confounder.
For $h_1$ and $h_3$ to exist, we just need $\text{rank}(P(Y\given W, a, x, z))=|\cW|$ and $\text{rank}(P(Y\given W, a, x))=|\cW|$, implying that $Y$ couples sufficient information of the missing variable $W$. 
Analogue to the CCB-IV case, we remark that equations in the IES for CCB-PV are coupled and should be solved at the same time. 

% To overcome the missingness of $W$, we see  that $P(W\given a,x,z, (R_X, R_Z)=\ind)$ is replaced by $P(Y\given W, a, x, z, (R_Z, R_X, R_W)=\ind)^+P(Y\given a, x, z, (R_X, R_Z)=\ind)$ in \eqref{eq:PV matrix 1} and $P(W\given x, R_X=1)$ is replaced by $P(Y\given W, a, x, (R_W, R_X)=\ind)^+P(Y, a\given x, R_x=1)$ in \eqref{eq:PV matrix 2}. Such a technique is similar to the CCB-IV case and requires that the distribution of  $Y$ contains enough information of $W$, which is inherent in the invertibility of $P(Y\given W, a, x, z, (R_Z, R_X, R_W)=\ind)$ and $P(Y\given W, a, x, (R_W, R_X)=\ind)$.

\paragraph{Pseudo random variable $A'$.}
Here in the CCB-PV setting, we encounter an issue concerning $a'$ that appears in \eqref{eq:PV ID 3} and \eqref{eq:PV ID 4}. Note that \eqref{eq:PV ID 3} and \eqref{eq:PV ID 4} should hold point-wise with respect to $a'$. However, if we treat each $a'\in\cA$ separately, we need to solve $\abr{\cA}$ equations, and the problem becomes intractable as $|\cA|$ grows larger, or we have a continuous treatment space. To overcome such a difficulty, we propose to treat $a'$ as a realization of a pseudo random variable $A'$, which is independent of the CCB-PV model and uniformly distributed across the action space. 
% Here, by letting $A'$ uniformly distributed, we assume $\cA$ to be a bounded treatment set. 
Therefore, the joint distribution $\mu$ including $A'$ is given by
\begin{align*}
    \mu(y, a, w, x, z, u, r_W, r_X, r_Z, a')=\pob(y, a, w, x, z, u, r_W, r_X, r_Z) u(a').
\end{align*}
Thereby, \eqref{eq:PV ID 3} and \eqref{eq:PV ID 4} can be written as, 
%\begin{gather}
\begin{align}
    &\EEob\sbr{h_3(Y, A, X,  A')-h_2(A', W, X)\given A=a, W=w, X=x, A'=a', (R_W, R_X)=\ind}=0 \label{eq:PV ID 5}, \\
    &\EEob\sbr{h_4(X, A') - h_3(Y, A, X, A')\given X=x, A'=a', R_X=1}=0. \label{eq:PV ID 6}
\end{align}
%\end{gather}
Here, by letting $A'\sim u(a)$, we try to learn an estimator that yields small error in each $a'$. Moreover, the dataset $\cD$ can be easily adjusted by adding an element $a_t'$ which is uniformly selected from $\cA$ and added to each sample, i.e., $\cD=\{\check x_t, a_t, y_t, \check w_t, \check z_t, a'_t\}_{t=1}^T$. By treating $A'$ as a pseudo random variable and $a'$ as its realization, we transform \eqref{eq:PV ID 3} and \eqref{eq:PV ID 4} into conditional moment equations, which facilitates our analysis of the IES in the sequel. 
% Consequently, we see that identifying with the pseudo random variable $A'$ in CCB-PV is no different from the CCB-IV case. 


\subsection{United Form for the IES}\label{sec:united form}
We summarize the IES discussed in both the CCB-IV and CCB-PV cases into the following united form
\begin{align}\label{eq:united form}
    \EE_{\mu_k}[\alpha_k(\vh(\vX), \vY_k)\given \vZ_k]=0, \quad \forall k\in\cbr{1, \cdots, K}, 
\end{align}
where 
\begin{description}
    \item[$K$] denotes the total number of equations in the IES,
    \item[$\vh$] represents the vector of bridge functions to learn, i.e., $ \vh(\vX)= (h_1(\vX_1),\cdots, h_{K-1}(\vX_{K-1}), g(\vX_K))$ where $X_k$ is the random variable vector that $h_k$ depends on and $X$ is a union of $\{X_k\}_{k=1}^K$, 
    \item[$\alpha_k$] is the linear function that is taken expectation with in the $k$-th equation of the IES, which depends on $h(X)$ and random variable vector $Y_k$, 
    \item[$\vZ_k$] is the random variable vector that is conditioned on in the $k$-th equation of the IES,
    \item[$\mu_k$] denotes the joint distribution for $(\vX, \vY_k, \vZ_k)$ and $\EE_{\mu_k}$ is the expectation taken with respect to $\mu_k$, 
\end{description}
% where $K$ denotes the total equation number in the identification formula, $\vh(\vX)$ represents the vector of bridge functions to learn, i.e., $ \vh(\vX)= \{h_1(\vX_1),\cdots, h_{K-1}(\vX_{K-1}), g(\vX_K)\}$ , $\vX_k$, $\vY_k$ and $\vZ_k$ are random variable vectors, $\mu_k$ denotes the joint distribution for $(\vX, \vY_k, \vZ_k)$ in the $k$th equation, and function $\alpha_k(\cdot)$ is linear with respect to both $\vh$ and $\vY_k$.
We give an example to illustrate the united form in \eqref{eq:united form}.
Following the IES of CCB-PV in Theorem \ref{thm:PV ID}, we have $K=4$, $X=\bigcup_{k=1}^K \vX_k = \{Y, A, X, Z, W, A'\}$, $Y_1=Y$, $Z_1=\{A, X, Z\}$, $\mu_1(x, a, y, z)=\pob(x, a, y, z\given (R_X, R_Z)=\ind)$, and $\alpha_1(\vh(\vX), \vY_1)=h_1(Y, A, X, Z)-Y$.  Note that the joint distribution $\mu_k$ for the variables in each equation can be different. For example, the joint distribution for \eqref{eq:PV ID 1} is conditioned on $(R_X, R_Z)=\ind$ while the joint distribution for \eqref{eq:PV ID 2} is conditioned on $(R_W, R_X, R_Z)=\ind$. 
Moreover, let $\cD_k$ denote the subset data of $\cD$ corresponding to the missingness condition in the $k$-th equation of the IES, e.g., in the CCB-PV case we have $\cD_1=\{(\check x_t, a_t, y_t, \check w_t, \check z_t, a'_t): (r_{X, t}, r_{Z, t})=\ind\}$, which is a subset of $\cD=\{(\check x_t, a_t, y_t, \check w_t, \check z_t, a'_t)\}_{t=1}^T$ and $\cD_1\sim\mu_1$.
Let $\cF(\vZ_k)$ denotes the functional space on $\vZ_k$.
Since $\alpha_k$ is linear, we can define a linear operator $\cT_k: \vH\rightarrow \cF(\vZ_k)$ for the united form of the IES in \eqref{eq:united form} as
\begin{align}
    \cT_k \vh (\cdot) :=\EE_{\mu_k}\sbr{\alpha_k(\vh(X), \vY_k)\given \vZ_k=\cdot}. \label{def:cT}
\end{align}
By letting $\vT\vh(z)=(\cT_1 h (z_1), \cdots, \cT_K h(z_K))$, the IES in \eqref{eq:united form} can be alternatively expressed as 
\begin{align}
    \vT\vh(z) = 0.\label{eq:operator=0}
\end{align}
Note that the IES \eqref{eq:operator=0} comprises a series of conditional moment equations, which is hard to solve with offline data. In the next section, we propose to transform the conditional moment equations into unconditional moment estimators.  
% \begin{assumption}[Completeness]
% For any policy $\pi\in\Pi$ and any $\vh\in\vH$, we assume that there exists $\theta_k^*\in\Theta_k$ such that $\EE\sbr{\alpha^\pi_k(\vh_k, \cX)\given \cZ_k}=\theta_k^*(\cZ_k)$ for all $k=\{1, 2, \cdots, K\}$.
% \end{assumption}



% \iffalse
% \section{Min-max Integral Equation Solver}

% \subsection{Problem Formalization}

% Let $X$ be a random vector and the components of $X$ are indexed by $I$.
% For a subset $S\subseteq I$, we use $X_S$ to denote the subvector indexed by $S$.
% Suppose $I_1$ and $I_2$ are two subsets of $I$.
% Let $\cH$ be the space of $L^2(X_{I_1})$ and $\cE$ be the space of $L^2(X_{I_2})$.
% We define operator $A:\cH\rightarrow\cE$ as
%     $$(Af)(x_2) = \E[f(X_{I_1})~|~X_{I_2}=x_2]$$
% Then our goal is to estimate $f$ in 
% \begin{equation}\label{eqn:integral equation}
%     Af = b
% \end{equation}
% where $f\in\cH$ is the unknown function to be estimated and $b\in\cE$ is a known or estimable function.
% When $b$ in \eqref{eqn:integral equation} is unknown and thus need to be estimated, \eqref{eqn:integral equation} is called stochastic ill-posed problem.

% Notice that to have a continuous inverse, the domain of a compact operator must be finite dimensional.
% This fact would cause stability issue of equation \eqref{eqn:integral equation}.
% A common method introduced to solve the instablity issue is Tikhonov regularization~\cite{kress1989linear}.
% Let $\alpha^\pi > 0$ be a regularization parameter, we define $f^\alpha^\pi$ as
% \begin{equation}\label{eqn:Tikhonov regularization}
%     f^\alpha^\pi = \argmin_{f\in\cH}\frac{1}{2} \|Af-b\|_\cE^2 + \frac{\alpha^\pi}{2}\|f\|_\cH^2
% \end{equation}

% \subsection{Min-max Solver}
% As an optimization problem, $f^\alpha^\pi$ defined in \eqref{eqn:Tikhonov regularization} is hard to solve directly in practice.

% The first difficulty comes from the limited number of samples.
% In many situations, achieving samples can be expensive and time-consuming.
% In the worst case, for a specific value $x_2$ we might only have one sample for $X_{I_2}=x_2$.
% Hence the conditional expectation can be difficult to estimate.

% We might also have double-sample problem.
% Consider that we are going to parameterize $f$ by $f(X;\theta)$ and let the loss function be $L(\theta)=\frac{1}{2}\E_Z[\E[f(X;\theta)-Y|Z]^2]$.
% Notice that the gradient of $L(\theta)$ would be $\nabla L(\theta) = \E_Z[\E[g(X;\theta)-Y|Z]\E[\nabla_\theta g(X;\theta)|Z]]$.
% To make the estimation of the product unbiased, we need two independent samples $(X_1,Y_1,Z_1)$ and $(X_2, Y_2, Z_2)$ with $Z_1 = Z_2$.
% In practice, unless we can rewind the environment, e.g. via simulator, we should not expect we are able to get samples satisfying $Z_1=Z_2$.

% Based on the fact that solving \eqref{eqn:Tikhonov regularization} directly is hard, we express our problem via a min-max game.
% Let $R:\cH\rightarrow\mathbb{R}_{\ge 0}$ be a norm of $f$ measures the smoothness of $f$, e.g. $R(f) = \|f\|_\cH$ in \eqref{eqn:Tikhonov regularization}.
% We write \eqref{eqn:Tikhonov regularization} in the form of
% \begin{align*}
%     & \min_{f\in\cH}\frac{1}{2}R(f)^2 \\
%     & \text{s.t. } Af = b
% \end{align*}
% Then we define the Lagarangian
% $$\tilde{L}(f, u) = \frac{1}{2} R(f) + \langle Af-b, u \rangle_\cE - \frac{\alpha^\pi}{2}\|u\|^2_\cE$$
% where $u \in \cE$ is the multiplier.
% Notice that $\langle Af, u \rangle_\cE = \E_{X_{I_2}}[\E[f(X_{I_1})|X_{I_2}]u(X_2)] = \E[f(X_{I_1})u(X_{I_2})]$.
% Finally, instead of solving \eqref{eqn:Tikhonov regularization} directly, we are going to solve
% \begin{equation}\label{eqn:saddle-point problem}
%     \min_{f\in L^2(X_{I_1})} \max_{u\in L^2(X_{I_2})} \E[(f(X_{I_1})-b(X_{I_2}))u(X_{I_2}) + \frac{\alpha^\pi}{2} f(X_{I_1})^2 - \frac{1}{2} u(X_{I_2})^2]
% \end{equation}
% %\begin{flushleft}


% \fi