\section{Problem Formulation}\label{sec:problem formulation}
In this section, we formalize the contextual bandit problem with confounding bias and missing observations.
We describe the casual structure of confounded contextual bandit in \S\ref{sec:confounded contextual bandit}, the procedure of data collecting in \S\ref{sec:observational process}, and the performance metric in \S\ref{sec:interventional process}.
%We further show the major challenges of policy learning for counfounded contextual bandit with missing observations in \S\ref{sec:challenges}.
%{\color{blue} In \S\ref{sec:algorithm outline} we present our policy learning algorithm outline.}
%{\color{orange} After determining whether to move the subsection ``algorithm outline'' to somewhere else or reserve it in this section, we either delete the blue sentence or change it to black.}

\subsection{Confounded Contextual Bandit}\label{sec:confounded contextual bandit}
%Our setting is offline policy learning in confounded contextual bandit (CCB), whose random variables in each trial are denoted by tuple 
%$$(U, X, A, Y, O), $$
%where $U\in\cU$ is the confounder, $X\in\cX$ is the context, $A\in\cA$ is the treatment, $Y\in\mathbb{R}$ is the reward, and $O\in\cO$ denotes the side observations. 
In this paper we study the offline policy learning in confounded contextual bandit (CCB).
Each trial can be represented by a tuple of random variables
$$(U, X, A, Y, O), $$
where $U\in\cU$ is the confounder, $X\in\cX$ is the context, $A\in\cA$ is the treatment, $Y\in\mathbb{R}$ is the reward, and $O\in\cO$ denotes the side observations. 
We assume that variables for different trials are independent and identically distributed. 
In offline learning, the data is collected through the observational process, and a newly selected policy is carried out in the interventional process.

The observational process and the interventional process of a CCB with side observations are depicted in Figure \ref{fig:CCB}.
%In each trial, we have a confounder $U$ staying unmeasured for whatever reason, which for example can be interpreted by containing sensitive information that cannot be revealed.
In each trial, there is an unmeasured confounder $U$ which has impacts on $O$, $A$, $X$, and $Y$.
As $U$ is not measured, the value of $U$ is not accessible in the data collected.
For example, sensitive information that is not allowed to reveal could be modeled by such unmeasured confounder. 
Since $U$ affects all of $O$, $A$, $X$, and $Y$, the confounder $U$ serves as a common cause for the model.
Context $X$ is coupled with confounder $U$, and side observations $O$ can be caused by both confounder $U$ and context $X$. 
%A treatment $A$ is then selected following some policy, which can be different in the observational process and the interventional process, as we will explain soon after.
A treatment $A$ is then selected following some policy $\pib(\cdot|U, X, O)$.
After the treatment is carried out, the environment generates a reward $Y$.
Suppose there are $T$ trials in total, then the \textbf{full dataset} $\tilde{\cD}$, which is distinguished from the dataset $\cD$ defined later, can be represented as
$$\tilde{\cD}=\{(u_t, x_t,a_t,y_t,o_t)\}_{t=1}^T.$$
Then by some missingness mechanism, records of $x_t$ and $o_t$ in some trials are possibly lost.
Additonally, recall that $u_t$ is also unmeasured and thus not included in the dataset.
The dataset collected for policy learning is defined as
$$\cD=\{(\check x_t, a_t, y_t, \check o_t)\}_{t=1}^T,$$
where $\check x_t$ either takes the value of $x_t$, if $x_t$ is not lost, or takes a special value of $\text{None}$, if $x_t$ is lost.
Similarly, $\check o_t$ either takes value of $r_t$ or $\text{None}$, depending on whether $o_t$ is lost.
More details about the observational process is presented in \S\ref{sec:observational process}.
% A Directed Acyclical Graphs (DAG) for such a CCB model is depicted in Figure \ref{fig:CCB}.
%We now give a detailed description of the observational process and the interventional process.


Given the dataset $\cD$ collected in the observational process, the goal of policy learning in CCB is to build a new policy $\pi$, which is called the interventional policy.
The interventional policy $\pi$ is executed in the interventional process.
In the interventional process, the unmeasured confounder $U$ still has impacts on both the context $X$ and the reward $Y$.
As the confounder $U$ is unmeasured, the agent could only observe the context $X$ and must decide an action $A$ to take only depending on the context $X$.
The rules for the agent to make decision in the interventional process is modeled by the interventional policy $\pi: \cX\rightarrow\Delta(\cA)$.
The agent aims to learn an interventional policy $\pi$ that maximizes the expected reward.
We discuss more details about the interventional process and performance metric in \S\ref{sec:interventional process}.

% \begin{figure}
% \centering
%   \subfloat[DAG of the observational process in  CCB]{\includegraphics[height=3.5cm]{figure/CCB_behavior.pdf}\label{fig:CCB_behavior}
%   } 
%   \hspace{0.5cm}
%   \subfloat[DAG of the interventional process in CCB]{\includegraphics[height=3.5cm]{figure/CCB_evaluation.pdf}\label{fig:CCB_evaluation}}
%    \caption{A DAG illustrating the observational and the interventional process in CCB. Here,  the white nodes represent the observed variables, the light gray nodes represent the variables missing not at random and the dark nodes represent unmeasured variables. A dashed line means the causal effect might either exist or not. }\label{fig:CCB}
% \end{figure}

\begin{figure}[h]  
\centering 
  \begin{subfigure}[b]{0.4\linewidth}
  \centering
    \begin{tikzpicture}
        \node[hiddenstate] (U) {$U$};
        \node[missingstate] (X) [below=of U] {$X$};
        \node[state] (A) [left=of X, xshift=.27cm, yshift=-1cm] {$A$};
        \node[state] (Y) [right=of X, xshift=-.27cm, yshift=-1cm] {$Y$};
        
        \path[double] (U) edge (X);
        \path[blue, normal] (U) edge (A);
        \path[blue, normal] (X) edge (A);
        \path[normal] (A) edge (Y);
        \path[normal] (U) edge (Y);
        \path[normal] (X) edge (Y);

        \node[missingstate] (O) [above=of A] {$O$};
        \path[blue, dashed, normal] (O) edge (A);
        \path[dashed, normal] (U) edge (O);
        \path[dashed, normal] (X) edge (O);
        \path[dashed, normal] (O) edge[bend left=-20] (Y);
    \end{tikzpicture}%
    \caption{DAG of the observational process in CCB} \label{fig:CCB_behavior}
  \end{subfigure}\qquad
\begin{subfigure}[b]{0.4\linewidth}
\centering
  \begin{tikzpicture}  
      \node[hiddenstate] (U) {$U$};
        \node[missingstate] (X) [below=of U] {$X$};
        \node[state] (A) [left=of X, xshift=.27cm, yshift=-1cm] {$A$};
        \node[state] (Y) [right=of X, xshift=-.27cm, yshift=-1cm] {$Y$};
        
        \path[double] (U) edge (X);
        \path[red, normal] (U) edge (A);
        \path[red, normal] (X) edge (A);
        \path[normal] (A) edge (Y);
        \path[normal] (U) edge (Y);
        \path[normal] (X) edge (Y);

        \node[hiddenstate] (O) [above=of A] {$O$};
        \path[dashed, normal] (U) edge (O);
        \path[dashed, normal] (X) edge (O);
        \path[dashed, normal] (O) edge[bend left=-20] (Y);
\end{tikzpicture}
\caption{DAG of the interventional process in CCB} \label{fig:CCB_evaluation} 
\end{subfigure}
\caption{A DAG illustrating the observational and the interventional process in CCB. Here,  the white nodes represent the observed variables, the light gray nodes represent the variables missing not at random and the dark nodes represent unmeasured variables. A dashed line means the causal effect might either exist or not. }\label{fig:CCB}
\end{figure}  


\subsection{Observational Process}\label{sec:observational process}
The observational process describes how the offline dataset is collected, which is depicted in Figure \ref{fig:CCB_behavior}.
In the $t$-th trial of the observational process, the environment selects $(u_t, x_t, o_t)$ as a realization of $(U, X, O)$ according to the prior $\pr(u, x, o)$.
% {\color{orange} Maybe we could find a new name of $p$. Not sure if the reviewer will understand ``prior'' as the prior in Bayes.} 
The agent then conducts a treatment according to the observational policy $\pib: \cX\times \cO \times \cU\rightarrow \Delta(\cA)$. We remark that it is very common for the observational policy to be confounded by $U$, which can be understood as the agent's natural predilections, e.g., the playing agent in the observational procedure has a preference for certain treatments due to some hidden causes encoded by $U$ \citep{bareinboim2015bandits}.
After the treatment is conducted, a reward $Y$ depending on $(U, X, O, A)$ is received by the agent. The joint distribution $\pob$ in the observational process is thereby given by
\begin{align}\label{def:pob}
    \pob(u, x, o, a, y) = \pr(u, x, o) \cdot  \pib(a\given u, x, o) \cdot \pr(y\given u, x, o, a).
\end{align}
%Here, we provide two examples to illustrate the role of the side observations in the observational process.
Here we provide two typical examples of side observations in the observational process.
\begin{example}[Side observations as instrumental variable]\label{ex:IV}
In a confounded contextual bandit with instrumental variable (CCB-IV) shown in Figure \ref{fig:CCB-IV}, $O$ corresponds to instrumental variable $Z$, which is assumed to be independent of confounder $U$ and outcome $Y$. The observational policy is given by $\pib(a\given u, x, z)$.
\end{example}
\begin{example}[Side observations as proxy variables]\label{ex:PV}
In a confounded contextual bandit with proxy variables (CCB-PV) shown in Figure \ref{fig:CCB-PV}, $O$ corresponds to the negative controls $(Z, W)$. It is assumed that $W\indep A\given (U, X)$ holds for the outcome proxy $W$ and $Z\indep (Y, W)\given (A, X, U)$ holds for the treatment proxy $Z$. The observational policy is given by $\pib(a\given u, x, z)$.
\end{example}
Suppose there are $T$ trials in the observational process and the full dataset is denoted by $\tilde \cD = \{(u_t, x_t, a_t, y_t, o_t)\}_{t=1}^T$.
We emphasize that $\tilde\cD$ is not the dataset that will be used for policy learning due to the unmeasurement of the confounder $u_t$ and the missingness in both the contexts $x_t$ and the side observations $o_t$.
%The missingness mechanism is stated as follows.
We formally discuss the missingness mechanism in the following paragraph.

\paragraph{Missingness Mechanism. }
% We discuss the missingness issue in the observational process.
In addition to the unmeasurement of confounder $U$, we assume that side observations $O$ and context $X$ in our model are subject to missingness, which extends the missingness mechanism in \cite{yang2019causal, yang2017nonparametric}  %where they assume missingness to be not at random but  independent of outcome $Y$.
where missingness was assumed to be not at random but independent of outcome $Y$.
We denote the observed dataset as $\cD = \{(\check x_t, a_t, y_t, \check o_t)\}_{t=1}^T$, where $\check x_t$ and $\check o_t$ denote the context and the side observations that we truly observe.  
%For a rigorous illustration of the missingness, let random variables $R_X$ and $R_O$ denote the missingness indicators for $X$ and $O$, respectively.
Let random variables $R_X$ and $R_O$ denote the missingness indicators for $X$ and $O$, respectively.
Let $r_{X, t}$ and $r_{O, t}$ denote realizations of $R_X$ and $R_O$ in the $t$-th trial.
%Here, $r_{X, t}=1$ indicates that $x_t$ is not missing, i.e., $\check x_t=x_t$, and $r_{X, t}=0$ indicates that $x_t$ is missing, i.e., $\check x=\text{NaN}$ where NaN means missing value.
When $r_{X,t}=1$, the record of $x_t$ is not missing and $r_{X,t}=0$ indicates that the record of $x_t$ is lost.
We introduce a special dummy value $\text{None}$ to represent a missing record.
So $\check x_t$ takes values in $\{x_t, \text{None}\}$ by the rule of
\begin{equation*}
    \check x_t = \left\{
    \begin{aligned}
    & x_t & \quad \text{if } r_{X,t} = 1 \\
    & \text{None} & \quad \text{if } r_{X,t} = 0
    \end{aligned}
    \right.
\end{equation*}
%In contrast to assuming the observations to be missing randomly \citep{rubin2004multiple, qu2009propensity, crowe2010comparison, mitra2011estimating, seaman2014inverse}, we are considering a more challenging setting where the missingness is not at random, i.e., $R_X$ and $R_O$ are not independent of the model. 
In contrast to assuming the observations to be missing randomly \citep{rubin2004multiple, qu2009propensity, crowe2010comparison, mitra2011estimating, seaman2014inverse}, in this paper we study a more general and challenging setting in which the missingness is not at random, i.e., $R_X$ and $R_O$ are not independent of the model $(U,X,A,Y,O)$.
As $R_X$ and $R_O$ could be dependent of $(U,X,A,Y,O)$, our results covers the case of malicious adversarial missing.
We revisit the previous two examples to illustrate the missingness.
\begin{example}[Example \ref{ex:IV} revisited]
In the CCB-IV, we allow $R_X$ to be caused by $(Z, X, A)$ and $R_Z$ to be caused by $(Z, X)$.
\end{example}
\begin{example}[Example \ref{ex:PV} revisited]
In the CCB-PV, we allow $R_X$ to be caused by $X$, $R_Z$ to be caused by $(Z, U, A, X)$, and $R_W$ to be caused by $(W, X, A)$.
\end{example}
Identifying the causal effect in the presence of missingness is nontrivial because the missingness interferes with the structure of the observational dataset.
For instance, when conditioning on $R_X=1$ in the CCB-IV, the instrumental variable $Z$ might no longer be independent of the confounder $U$, which leads to failure of conventional identification approaches.
More details on the difficulties brought by the missingness mechanism as well as the method we use to address the missingness issue will be provided case by case in \S\ref{sec:Identification}. 



\subsection{Interventional Process} \label{sec:interventional process}
In the interventional process, an interventional policy is carried out after the model is learned from the offline dataset.
The interventional process is different from the observational process in the following three aspects: 
\begin{description}
\item[(i)] side observations $O$ appearing in the dataset are unmeasurable while context $X$ is fully measurable in the interventional process;
\item[(ii)] the agent follows an interventional policy $\pie:\cX\rightarrow \Delta(\cA)$ which is independent of $U$ and $O$ since they are unable to measure in the interventional process;
\item[(iii)] context $X$ follows a new marginal distribution $\tilde p(x)$ in the interventional process.
\end{description}
Aspect (i) indicates that only the context is revealed to the agent in the interventional process. Therefore, the interventional policy is context-dependent, as is stated in (ii).
We remark that (iii) can be understood through the idea of a marginal distribution shift in $X$ between the observational group and the interventional group, which is very common in real-world practice.
% Note that the unshifted context distribution can be viewed as a special case where $\tpr(x)=\pr(x)$. 
% We remark that it is common to have a shifted context distribution in practice. 
For example, when studying the effect of recommended ads' type ($A$) on the clicking rates ($Y$) with users' age ($X$) serving as the context, we might have an interventional group whose age distribution differs from the observational group.
Another example is the in-context learning paradigm,  where the task specification procedure can be viewed as "conditioning" the model on a certain context presented by the input texts/token \citep{brown2020language,radford2019language}.
Following (i)-(iii), the joint distribution $\pin{\pie}$ of random variables in the interventional process is given by
\begin{align}\label{def:pin}
    \pin{\pie}(u, x, o, a, y) = \tpr(x) \pr(u, o\given x) \pie(a\given x) \pr(y\given u, x, o, a).
\end{align}
Diagrammatic explanations of the interventional process are given in Figure \ref{fig:CCB_evaluation}. 
We see that the DAG of the interventional model is given by substituting the blue incoming edges to treatment $A$ encoded by $\pib$ in the observational model with the orange incoming edges encoded by $\pie$, while the remaining part of the DAG remains unchanged except for the marginal distribution of $X$.

\paragraph{Reward function and policy optimization.}
In the interventional process, the average reward $v^\pie$ is defined as
\begin{align}
    v^\pie = \EE_{\pin{\pie}}\sbr{Y},\label{def:v}
\end{align}
where $\EE_{\pin{\pie}}$ corresponds to the expectation taken with respect to $\pin{\pie}$ defined in \eqref{def:pin}.
Our target is to find  $\piestar\in\Pie:\cX\rightarrow \Delta(\cX)$ that optimizes the average reward, 
\begin{align*}
    \piestar= \arg\max_{\pie\in\Pie} v^\pie.
\end{align*}
Correspondingly, we define the performance metric as the following sub-optimality, 
\begin{align}
    \text{SubOpt}(\pie) = v^\piestar - v^\pie.\label{def: SubOpt}
\end{align}

In summary, our goal is to design a learning algorithm that returns a policy $\hat \pi$ based on the offline dataset $\cD$ collected in the observational process. Here the dataset is subject to unmeasured confounder and missingness. 

% For brevity, we denote by $g(x, a)$ the function to approximate the CATE and $\gstar(x, a)=\CATE(x, a)$ the actual CATE.

% where $v^\pie$ corresponds to the case $g(x,a)=\CATE(x, a)$.

\section{CAP Algorithm}\label{sec:CAP algorithm}
In this section, we first investigate the main challenges of such an offline bandit problem, including the issue of confounding and missing data and also the spurious correlation that arises in the decomposition of the sub-optimality in \S\ref{sec:challenges}.
We then put forward an algorithm framework named Causal-Adjusted Pessimistic (CAP) policy learning in the face of such challenges in \S\ref{sec:algorithm outline}.
\subsection{Challenges in the Offline Setting }\label{sec:challenges}
The offline learning problem in the CCB boils down to the following two questions:
(i) how to evaluate the average reward given an interventional policy; (ii) how to efficiently find an interventional policy that maximizes the average reward.
When trying to answer these two questions, we encounter two major challenges: (i) confounded and missing data; (ii) spurious correlation in the sub-optimality. 
We briefly discuss where these challenges stem from and what technologies we use to overcome these challenges in this subsection.
\paragraph{Challenges in average reward evaluation: confounded and missing data.}
The key to the problem of evaluating the average reward \eqref{def:v} in the interventional process is to learn the conditional average treatment effect (CATE) defined as
\begin{align*}
    \gstar(x, a)=\EEob\sbr{Y\given X=x, \doopt(A=a)}, 
\end{align*}
where $\EEob$ is an abbreviation for $\EE_{\pob}$.
Here, the do-calculus $\doopt(A=a)$ in the condition means that the expectation is taken with respect to the distribution obtained by deleting $\pib(a\given u, x, o)$ from the product decomposition of $\pob$ in \eqref{def:pob} and restricting $A=a$. 
% A diagrammatic explanation is given by removing all the blue arrows pointing into variable $A$ and then letting $A=a$ in Figure \ref{fig:CCB_behavior}. 
Learning the CATE is important since the average reward is related to the CATE by
\begin{align}\label{eq:v to CATE}
    v^\pie = \EE_{\pin{\pie}}\sbr{\gstar(X, A)}.
\end{align}
In the presence of confounding bias \citep{vanderweele2008causal, jager2008confounding}, learning the CATE needs tools borrowed from causal inference.
To control for the confounding bias, a typical way is to exploit side observations $O$ in the offline data \citep{lipsitch2010negative, singh2020kernel}.
Instances of controlling the confounding bias using side observations are presented in Examples \ref{ex:IV} and \ref{ex:PV} where instrumental variable (IV) \citep{cragg1993testing, arellano1995another,  newey2003instrumental} or proxy variables (PV) \citep{tchetgen2020introduction, ying2021proximal} are introduced for negative controls.

However, our problem is still challenging given the fact that the missingness bias is coupled with the confounding bias. 
Note that identification with outcome-independent missingness is rather trivial in the unconfounded contextual bandit setting with tuple $(X, A, Y, R_X)$ where $R_X$ is caused by $(X, A)$.
The simplest way is to use the dataset without missingness, i.e., conditioning on $R_X=1$ and estimate $\EEob\sbr{Y\given X=x, A=a, R_X=1}$. 
Such a method is valid since we have $R_X\indep Y\given (X, A)$ without confounders.
However, in the confounded contextual bandit setting, the causal effect is identified with the aid of side observations, and some model assumptions related to these side observations are broken when simply conditioning on $(R_X, R_O)=\ind$.
Take the CCB-IV case for instance. The IV independence assumption $Z\indep U\given X$ is broken by conditioning on $R_X=1$ since $R_X$ also depends on confounded action $A$. 
% Thereby, we have to find an alternative way to get around the missingness issue and identify the CATE. 
As we will show in \S\ref{sec:Identification}, the CATE learning problem is addressed by solving a novel integral equation system (IES), in which the integral equations are coupled together, and the CATE is obtained as the solution to the IES.

% We will formally state the missingness mechanism and show the methodologies to overcome such a problem in \S \ref{sec:Identification}. 

\paragraph{Challenges in policy optimization: spurious correlation.}
We discuss the second question on how to efficiently optimize the interventional policy.
Let $g$ denote an estimation of the CATE and $\gstar$ denote the exact $\CATE$ thereafter.
Following \eqref{eq:v to CATE}, we define the average reward function corresponding to $g$ and $\pie$ as
\begin{align}
    v(g, \pie) = \EE_{\pin{\pie}}\sbr{g(X, A)}.\label{def:v approx}
\end{align}
It is okay if we simply take a greedy policy $\hat \pi$ that maximizes $v(g, \pie)$. However, such a greedy policy can sometimes be misleading.
A little calculation of the sub-optimality helps gain intuition. 
We let $\tpr(x)=\ind(x=x_0)$ for brevity. By definition of the sub-optimality in \eqref{def: SubOpt} and the fact that $\hat \pie$ is greedy with respect to $g$, we have
\begin{align}
    \SubOpt(\hat \pi) \le \underbrace{\langle \gstar(x_0, \cdot)-g(x_0, \cdot), \piestar(\cdot\given x_0) \rangle}_{\text{(i)}} + \underbrace{\langle g(x_0,\cdot) - \gstar(x_0, \cdot), \hat\pie(\cdot\given x_0) \rangle}_{\text{(ii)}}. \label{eq:spurious subopt}
\end{align}
Note that $\piestar$ in term (i) is intrinsic to the bandit model and does not depend on $g$. In contrast, $\hat \pie$ in term (ii) is coupled with the estimated $g$, which yields the spurious correlation \citep{jin2021pessimism} and makes term (ii) hard to control.
Bounding term (ii) usually needs strong assumptions on the "uniform coverage" of the dataset $\cD$ as in the existing bandit and RL literature \citep{brandfonbrener2020bandit, tennenholtz2021bandits, laroche2019safe}, which occasionally fails to hold in practice. 


Instead, we adopt the technique of uncertainty quantification and pessimism to cope with the spurious correlation challenges. Similar techniques have been applied to other problems in the existing literature \citep{jin2021pessimism, uehara2021pessimistic, zhan2022offline, rashidinejad2021bridging}. Our work successfully integrates such techniques with the confounded and missing data scenarios. Specifically, we first construct a confidence set $\CICATE$ for the estimated $g$ based on the offline data such that $\gstar\in\CICATE$ holds with high probability.
If the policy is optimized with respect to $g\in\CICATE$ that minimizes $v(g, \cdot)$, it follows that $v(g, \hat \pie)\le v(\gstar, \hat \pie)$, and the spurious correlation in term (ii) vanishes. Then, the estimated policy is given by
\begin{align*}
    \piepessi = \arg\sup_{\pie\in\Pie} \inf_{g\in\CICATE} v(g, \pie).
\end{align*}
Moreover, it is also shown that pessimism can promote exploration \citep{auer2008near, azar2017minimax} and help weaken the assumptions on the concentrability coefficient or the data coverage \citep{buckman2020importance}.


\subsection{Algorithm Outline}\label{sec:algorithm outline}
% {\color{orange} Shall we put the subsection ``algorithm outline'' somewhere else? Section 2 is called ``problem formulation''.}
Now that we have answered the two questions raised in the last subsection by (i) identifying the CATE from an integral equation system (IES); (ii) optimizing the policy with a pessimistic estimator $g$ selected from some confidence set $\CICATE$.
What remains to clarify is how to construct the confidence set $\CICATE$ based on the IES.
As we will show in  \S\ref{sec:Estimation}, learning the CATE from the IES can be alternatively done by minimizing some empirical loss function $\cL_\cD(\vh)$ on the hypothesis class $\vH$, where $\vh$ is an estimated solution to the IES.
We are then inspired to construct the confidence set as a level set of $\vH$ with respect to metric $\cL_\cD(\cdot)$ and a threshold $e_\cD$. 
The whole procedure is summarized in the following Causal-Adjusted Pessimistic (CAP) policy learning algorithm.

\begin{algorithm}
\caption{Causal-Adjusted Pessimistic (CAP) policy learning}\label{alg:meta}
\small
\begin{algorithmic}
\REQUIRE dataset $\cD = \{\check x_t, a_t, y_t, \check o_t\}_{t=1}^T$ from the observational process, hypothesis space $\vH$, policy class $\Pie$, threshold $e_\cD$.
\STATE (i) Construct confidence set $\CICATE(e_\cD)$ as the level set of $\vH$ with respect to metric $\cL_\cD(\cdot)$ and threshold $e_\cD$.
\STATE (ii) $\piepessi=\arg\sup_{\pie\in\Pie}\inf_{g\in \CICATE(e_\cD)} v(g, \pie)$.
\ENSURE $\piepessi$.
\end{algorithmic}
\end{algorithm}
The IESs for identifying the CATE in both the CCB-IV and the CCB-PV settings are formulated in \S\ref{sec:Identification}, and a united form is presented with use of a linear operator $\cT$. 
Based on such a united form, the loss function $\cL_\cD(\cdot)$ and the confidence set $\CICATE$ is constructed using the technique of minimax estimator. More details about constructing $\CICATE$ are provided in \S\ref{sec:Estimation}. %The convergence results for the CAP algorithm in both the CCB-IV and the CCB-PV settings are provided in \S\ref{sec: theoretical results} together with an extension of the CCB-PV case in \S\ref{sec:extended CCB-PV}. Lastly, we apply the CAP algorithm under the CCB-IV setting to the linear Dynamic Treatment Regime (DTRs) problem in \S\ref{sec:DTR} and apply the CAP algorithm under the CCB-PV setting (with extended policy class) to the one-step linear Partially Observable Markov Decision Process (POMDP) in \S\ref{sec:POMDP}, both with the sub-optimality guaranteed to converge at a rate of $\tcO(T^{-1/2})$ theoretically.
 