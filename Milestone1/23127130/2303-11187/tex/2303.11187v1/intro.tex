% !TEX root = paper.tex
%\begin{flushleft}
\section{Introduction}
Contextual bandit is a mathematical framework that models the  decision-making problem under uncertainty. 
In specific, in a contextual bandit, the agent chooses an action 
based on an observation (also known as the context), and observes a random reward that depends on the observation and the action taken. 
Such a framework 
%is a special case of reinforcement learning (\cite{sutton2018reinforcement}), 
finds wide applications  in areas such as  healthcare (\cite{raghu2017continuous, prasad2017reinforcement, komorowski2018artificial}), robotics (\cite{pinto2016supersizing}), and computational advertising (\cite{bottou2013counterfactual}).
%In contextual bandit problem, the taken action does not affect the state in the next step.
Typical online policy learning algorithms require many interactions between the agent and the environment.
However, in various applications of offline contextual bandit, e.g., autonomous driving (\cite{shalev2016safe}) and healthcare (\cite{gottesman2019guidelines}), collecting online generated data could take too much cost and be unethical.
On the other hand, there are many historically recorded dataset for tasks where policy learning could be applied.
For example, driving data generated by human (\cite{sun2020scalability}) and medical records (\cite{chakraborty2014dynamic}).
Therefore, in this work, we study the offline policy learning problem, aiming to learn an optimal policy from previously collected dataset and require no interactions with the environment.

To learn the optimal policy from an offline dataset in real-world practice, three challenges must be addressed: confounding effects, partially missing observations, and partial data coverage. 
Confounding effects arise because it is often impossible to conduct randomized controlled trials or collect all necessary covariates. \citep{pearl2009causality, hernan2010causal}.
For instance, medical data may intentionally omit a patient's health condition and medical history due to privacy concerns. \citep{brookhart2010confounding}.
In this example, the hidden information serves as the confounders. To address this, the power of side observations is used to adjust for unobserved confounders.

However, key information like side observations or context are often subject to partial missingness due to various reasons such as privacy concerns, attrition, or experimental errors. For example, lab test results that serve as side observations may be lost due to inability to conduct the tests or failure to store the results. Even worse, as the causes of missing observation problem vary, the missing pattern might be either at random or not at random \citep{rubin1976inference, yang2019causal}. When the partial missingness is at random, the full data distribution is identifiable and multiple methods can provide reasonable estimates \citep{qu2009propensity, seaman2014inverse}. On the other hand, missingness not at random is a more challenging issue, since the missingness mechanism can depend on other factors or even the missing value itself. In this work, we aim to handle the challenge of side observations missing not at random.

The last main challenge is the partial coverage of the actions in the offline dataset. the observational dataset is collected beyond the control of the learner, and the action space can usually be too large for sufficient exploration. This means that conditions on full coverage of the action space by the data set in order to learn a good interventional policy usually fails to hold in real world practice \citep{fujimoto2019off, fu2020d4rl}. In this paper, we therefore explore the question:
 \begin{center}
{\it Is it possible to design a provably efficient algorithm for offline learning confounded contextual \\ bandit problem with side observations and missing values under mild assumptions on the dataset?}
\end{center}


% Unobserved confounders make estimation of the reward function difficult.
% The missing observation issue further hinders us from adjusting the confounding bias.

% One major challenge of policy learning in real-world applications with observational data, when randomized controlled experiment is not feasible, is the potential existence of unobserved confounders (\cite{pearl2009causality, hernan2010causal}).
% It often happens that some variables having impacts on both the action and the reward are not contained in the dataset.
% Since we do not know the joint distribution of the action and the state as in the setting of randomized controlled experiments, there might be confounding bias that can not be adjusted.
% For example, in healthcare some information about patients' DNA sequence might not be available while the existence of some specific DNA subsequence affects both the behavior of taking some action and the risk of developing some disease.
% More details and examples of the confounder issue in healthcare datasets are discussed in \cite{brookhart2010confounding}.
% Policy evaluation could be hard when such confounders exist.



%--------------------- missing value ---------------------
% Another challenge often arises in study of observational data is missing observation problem (\cite{rubin1976inference, yang2019causal}).
% Due to various reasons, e.g. privacy consideration, attrition, and mistakes in the experiments, some entries of data might not be included in the dataset.
% As the causes of missing observation problem vary, the missing mechanism might be either at random or not at random.
% When the data are missing at random, one simple solution is to remove data points with missing observations from the dataset and then it reduces to the case without missing observations.
% However, only reserving data points with full data could be extremely inefficient.
% It is even worse that the missingness could happen not at random.
% In the missing not at random case, only preserving samples without missing observations can lead to biases too.



% We summarize three aspects of offline datasets often used in practice: 


Our answer to this question is affirmative.
Fortunately, there are methods that can help us accurately estimate the value of a policy from offline data affected by unobserved confounders if we have access to some side observations.
Two typical examples of side observations we explore in this work are instrumental variables (IV) (\cite{baiocchi2014instrumental, wong2021integral, chen2011nonlinear}) and proxy variables (PV) (\cite{miao2018identifying}).
Informally, IVs are variables that affect the reward only through the action, while PVs serve as negative control for the confounding effects.
Therefore, we investigate the use of side observations to mitigate the confounding bias in this paper.

For the partially missing observation challenge, we consider the case where both the context of the bandit and the side observations collected in the offline dataset are subject to missingness not at random. 
Let us take the missingness of context $X$ for example. Let $R_X$ denote the binary missingness indicator for $X$. We assume that $X$ is totally missing if $R_X=0$. 
The key idea we adopt to overcome the missingness issue is using the \textit{distributional information} of the outcome to compensate for the missingness in the contexts or oberservations under certain completeness conditions.
Similar ideas can be found in \citep{yang2019causal, ding2014identifiability}, though they consider a different setting with missing elementary values in a vectorized random variable.
To address the challenges of confounding effects and non-random missing observations, we present a new approach that formulates the policy evaluation problem as solving an integral equation system (IES) with bridge functions. We demonstrate that the solution to the IES preserves the conditional average treatment effect (CATE), which enables us to further optimize the policy. In contrast to conventional causal methods that rely only on summary statistics such as the expectation of the outcome, our method leverages the full distributional information of the outcome.

Given the limitations of finite samples and incomplete coverage of the action set in our offline dataset, we incorporate the principle of pessimism into our approach for learning the optimal policy \citep{jin2021pessimism, buckman2020importance, xie2021bellman}.
In order to do pessimistic policy optimization, We must quantify the uncertainty in estimating the CATE from the IES.
To do so, we propose a Causal-Adjusted Pessimistic (CAP) policy optimization algorithm, which has two components: the uncertainty quantification step for estimating the CATE from the IES, and the policy optimization step based on the uncertainty quantification result.
In the CAP algorithm, the IES is solved by reducing the moment restriction equation system to a minimax problem via Fenchel duality.
After the uncertainty quantification step, the CAP algorithm takes greedy policy based on the confidence set constructed.



% We propose a new framework to formulate the contextual bandit problem with confounders and missing observations.
% Our new framework is general in the sense that it can be applied to many typical side observations.
% In addition to two well-known examples, i.e., IV and PV, side observations with more complex causal structures like matrix-equation-identified side observations (\cite{lee2021causal}) also fit into our framework.
% %In addition to instrumental variables and proxy variables, the side observation with structure described in \cite{lee2021causal} also fits our framework.
% We then develop a Causal-Adjusted Pessimistic (CAP) algorithm to find the optimal policy.


%--------------------- side information (IV/PV) ---------------------



Our contribution can be summarized in three perspectives. 
\textbf{First,} we developed a general framework to model the contextual bandit problem with confounded offline dataset and missing observations not at random.
Under this framework, we derive a novel integral equation system (IES) for identification.
\textbf{Second, } 
We convert the IES to a minimax optimization problem, whose solution respects the CATE and the loss function of the minimax optimization problem paves the way for uncertainty quantification.
We emphasize that when the side observations are likely missing and the conditional moment restrictions form a system of equations rather than a single equation, it is non-trivial to select a proper way to do uncertainty quantification.
\textbf{Finally,} we propose a Causal-Adjusted Pessimistic (CAP) policy optimization algorithm and prove that our algorithms achieves fast statistical rate of sub-optimality for contextual bandit with side observations serving as instrumental variable and proxy variables.
To the best of our knowledge, this paper is the first one proposing a provably efficient algorithm for offline confounded contextual bandit with observations missing not at random.

\subsection{Related Works}
% \label{sec:rw}
% Move this paragraph to related work or other places
%--------------------- why previous papers does not work ---------------------
\paragraph{Offline reinforcement learning.}
Literatures on offline reinforcement learning, especially those on pessimistic algorithms, are related to our work (\cite{antos2007fitted, munos2008finite, chen2019information, liu2020provably, zanette2021exponential, xie2021bellman, yin2021towards, rashidinejad2021bridging, zhan2022offline, yin2022near, yan2022efficacy}).
The major difficulty of offline reinforcement learning is the distribution shift between the policy generating the collected data and the class of target policies.
To overcome the distribution shift problem, we have to incorporate pessimism \citep{jin2021pessimism, xie2021bellman, buckman2020importance, uehara2021pessimistic}.

\paragraph{Causal inference.}
A series of previous works in causal inference address adjusting the confounding bias via use of side observations like instrumental variables \citep{chen2011nonlinear, chen2021efficient, chen2016methods, bennett2019deep, bennett2023minimax, wong2021integral, athey2019estimating} or proxy variables \citep{bennett2021proximal, pearl2009causality, miao2018identifying, lee2021causal}.
Among these works, \cite{pearl2009causality, miao2018identifying, lee2021causal} are most relevant to this paper since the our framework covers side observations proposed in them.
In comparison, these works studied the identification of certain side observations.
As the task in this paper is to learn the optimal policy, we make further effort to construct confidence sets.
Additionally, a large volume of works discuss the missing observation problem in causal inference (\cite{rubin2004multiple, qu2009propensity, crowe2010comparison, mitra2011estimating, seaman2014inverse, yang2019causal}).
\cite{yang2019causal} is particularly related to our work as it discusses identification for the case in which confounders are missing not at random.
The model in \cite{yang2019causal} assumes that there is chance to observe some data of confounders, while we study a model assuming confounders are completely missing and the side observations are missing not at random.



%Another recent work (\cite{liao2021instrumental}) studied the reinforcement learning with instrumental variables.
%However, the method proposed in this work does not deal with the missing value issue and could only be applied to the specific kind of side observations, i.e., instrumental variables.


\subsection{Preliminaries}
% \label{sec:preliminaries}
% \input{related_work}
\paragraph{Notations.}
In this paper, we let $\Delta(\cA)$ denote distributions over $\cA$.
We denote the inner product by  $\langle \cdot, \cdot \rangle$.
For any function $f$, we let $\cO(f)$ denote $Cf$ and $\tilde{\cO}(f)$ to denote $\cO(f\cdot\text{poly}(\log f))$.
We use calligraphic symbols $\sF$ and $\vH$ to represent function classes.
We use $\overset{\cE}{\lesssim}$ and $\overset{\cE}{\gtrsim}$ to represent the inequality that holds on some event $\cE$.

\paragraph{Critical radius.} We define the localized empirical Rademacher complexity with respect to data set $\cD=\{x_i\}_{i=1}^n$ and function class $\sF:\cX\rightarrow [-c, c]$ as 
\begin{align*}
    \cR_\cD(\eta; \sF)=\EE\sbr{\sup_{f\in\sF, \nbr{f}_\cD\le \eta}\frac 1 n \sum_{i=1}^n\varepsilon_i f(x_i)}.
\end{align*}
The critical radius of $\sF$ on dataset $\cD$ is defined as any positive solution $\eta_\cD$ to $\cR_\cD(\eta;\sF)\le \eta^2/c$. Note that the critical radius $\eta_\cD$ is also a random quantity. 

\subsection{Roadmap}
In \S\ref{sec:problem formulation} we formalize the problem of policy learning for contextual bandit problem with confounding bias and missing observations.
In \S\ref{sec:CAP algorithm} we discuss the challenges of policy learning problem shown in \S\ref{sec:problem formulation} and show how these challenges motivate us to develop an algorithm framework named CAP policy learning to solve them.
In \S\ref{sec:Identification} and \S\ref{sec:Estimation} we expand details about the step of constructing confidence set in the CAP algorithm described in \S\ref{sec:CAP algorithm}.
The convergence results for the CAP algorithm are provided in \S\ref{sec: theoretical results}.
In \S\ref{sec:extended CCB-PV} we give convergence analysis of the CAP algorithm in an extended policy class.
Lastly, we show that the CAP algorithm could be applied to the linear Dynamic Treatment Regime (DTRs) problem in \S\ref{sec:DTR} and to the one-step linear Partially Observable Markov Decision Process (POMDP) in \S\ref{sec:POMDP}, both with the sub-optimality guaranteed to converge at a rate of $\tcO(T^{-1/2})$.
 

% 大背景 (done)
%    小背景 (done)
%    前人工作为什么不可以 (done)
%    setting (confounder + missing ) (done)
%    目标 (learn opt policy) (done)
%    难点 (done)
%    解决方法
%          pessimism-based (remove ... ) 介绍什么是pessimism
%          估计CATE method of moment -> min-max
%          Confidence set construct
%          allow model mispecification
%    theoretical result (achieve subopt)
%    single policy coverage + model 
%    first provably efficient work for offline bandit with missing data

% confounding bias

% hardness from quality of offline data 
%     confounded + missing value
%     offline contextual bandit
%          this problem makes estimation hard
%     identification hard
%     missing value hard
%     identification + missing value even harder
%     suprious ... (must do uncertainty quantification)
%     so how to do uncertainty quantification is unknown
%     matrix equation could also be done. add it
%     reward as a solution of integral equation (because of counfounder)
%     solve integral equation using 
%     estimate 
%     finite data -> distribution shift -> uncertainty quantification = construct confidence set
%     
%
%
