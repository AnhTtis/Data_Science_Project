\documentclass[journal,twoside,web]{ieeecolor}
\usepackage{generic}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[scr]{rsfso}
\usepackage{soul}
\usepackage{hyperref}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\markboth{\journalname, VOL. XX, NO. XX, XXXX 2017}
{Author \MakeLowercase{\textit{et al.}}: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS (February 2017)}
\begin{document}
\title{Neural Operators for %Provably 
Bypassing Gain and Control Computations in PDE Backstepping}
\author{  Luke Bhan,
        Yuanyuan Shi, 
        and Miroslav Krstic
\vspace*{-3em}
        \thanks{The authors are with the University of California, San Diego, USA, lbhan@ucsd.edu, yyshi@eng.ucsd.edu, {krstic@ucsd.edu}}
}

\maketitle

%\vspace*{-2em}

\begin{abstract}
We introduce a framework for eliminating the computation of controller gain functions in PDE control. We learn the nonlinear operator from the plant parameters to the control gains with a (deep) neural network. We provide closed-loop stability guarantees (global exponential) under an NN-approximation of the feedback gains. While, in the existing PDE backstepping, finding the gain kernel requires (one offline) solution to an integral equation, the neural operator (NO) approach we propose learns the mapping from the functional coefficients of the plant PDE to the kernel function by employing a sufficiently high number of offline numerical solutions to the kernel integral equation, for a large enough number of the PDE model's different functional coefficients. We prove the existence of a DeepONet approximation, with arbitrarily high accuracy, of the exact nonlinear continuous operator 
mapping PDE coefficient functions into gain functions. 
%from the functional coefficient of the PDE model to the kernel function of the PDE controller, which are both continuously differentiable functions on their spatial domains. 
Once proven to exist, learning of the NO is standard, completed ``once and for all'' (never online) 
and the kernel integral equation doesn't need to be solved ever again, for any new functional coefficient 
%in the plant model, provided the functional coefficient is continuously differentiable and 
not exceeding the magnitude of the functional coefficients used for training. 
%The system is shown to be stabilized with the DeepONet approximate kernels. 
We also present an extension from approximating the gain kernel operator to approximating the full feedback law mapping, from plant parameter functions and state measurement functions to the control input, with semiglobal practical stability guarantees.
Simulation illustrations are provided and code is available on \href{https://github.com/lukebhan/NeuralOperatorsForGainKernels}{github}. This framework, eliminating real-time recomputation of gains, has the potential to be game changing for adaptive control of PDEs and gain scheduling control of nonlinear PDEs.

The paper requires no prior background in machine learning or neural networks.
\end{abstract}


\section{Introduction}

\paragraph{ML as a tool for learning control methodologies}
In \cite{https://doi.org/10.48550/arxiv.2302.14265} we introduced a  learning-based control {\em framework} which devises a new role for machine learning (ML): learn an entire control design methodology, in the form of a mapping from the plant model to the controller gains, or even to the control inputs. This framework is neither model-free nor methodology-agnostic. On the contrary, it is method-specific and, for broad classes of plants, after a large number of training calculations of the controller gains, using a particular method (LQR, pole placement, MPC, backstepping, or something more complex), on a sample set of plant models, a neural network (NN) approximated mapping of the method is learned. Once learned as a plant-to-gains mapping, the control design for a new plant does not require another solution of the design equations (Riccati, Bezout, or something more complex) but merely entails an ``evaluation of the NN map'' to obtain the control gains. 

One would argue that a dire need for such a learning-based capability, where an entire methodology is ``encoded'' into a neural mapping is not needed for LQR or just about any other linear finite-dimensional design---the cost of the solution of a design problem is not prohibitive. And, indeed, we are motivated by more challenging design problems---those arising in control of PDEs. 

In PDE control, the design problems are not matrix equations. In the easier of cases, the PDE control design problems are themselves functional equations, i.e., PDEs. The infinite-dimensional state of a PDE is typically a function of spatial variables. And the controller gain is, therefore, a function of spatial variables as well. Hence, the control design problem for PDEs is typically a PDE problem in space---but not in time. 

While solving a matrix equation may no longer be a numerically formidable problem, solving a PDE for a new plant may well be a numerical problem that is complex enough that it stand in the way of an application of the PDE control methodology. It is therefore of interest, in PDE control, to have a capability where producing the control gain functions is just an evaluation of a neural mapping which has already learned the design methodology on a large set of previously offline-solved control design problems for a sample set of PDEs in a certain class. 

\paragraph{Neural operators for approximating mappings of functions into functions} Just as it is appropriate to understand control designs for linear finite-dimensional systems as mappings of matrices (such as the plant data $A$ and $B$) into matrices (such as the control gain $K$), control designs for PDEs should be understood as mappings of functions (the spatially-dependent coefficients in PDE models) into functions (spatially-dependent control gains). 

Our inspiration for encoding PDE control methodologies into NNs comes from recent major advances in the mathematics of machine learning. Motivated by the tasks of finding solution/flow maps (from the initial conditions into future states) for physical PDEs (such as the notoriously difficult Navier-Stokes), research teams led by George Karniadakis,  Anima Anandkumar and Andrew Stuart, and George Pappas and Manfred Morari, have developed neural approximation methods with provable properties for nonlinear operators acting on functions and producing functions. These approaches are not simply discretizing PDEs and finding solution maps to the resulting large ODE solution problems. In the language of distributed parameter systems, they are not ``early lumping'' methods of learning solution maps. They approximate (non-discretized) function-to-function nonlinear operators and provide the guarantees of accuracy of approximation in terms of the required sizes of the training sets and neural networks. 

The value of such a capability in PDE control cannot be understated. With a theoretically rigorous and numerically powerful capability like this, specific PDE control methods, for specific classes of PDEs, can be ``learned'' once and for all and encoded as neural operators, ready to produce the control gain functions for any new functional coefficients of the PDE. 

In a theoretically rigorous field like PDE control, a computational capability with rigorous approximation guarantees has a value primarily if it allows the retention of the theoretical properties proven for the ``exact design.'' This is indeed what we show in the paper \cite{https://doi.org/10.48550/arxiv.2302.14265}  in which we introduce the framework: approximate neural operator representations of a particulard PDE control method---PDE backstepping---preserves its stability guarantees in spite of the control gains not being generated by solving the design PDEs but by the gains being generated from the learned ``neural model'' of PDE backstepping. 

\paragraph{Extension of PDE backstepping neural operators from hyperbolic \cite{https://doi.org/10.48550/arxiv.2302.14265}  to parabolic PDEs} Hyperbolic PDEs involve only first derivatives in space and time. This makes them (all else being equal) the ``simplest'' PDE class for control. Delay systems combine ODEs with delays---the simplest form of a PDE. While the simplest among PDEs, hyperbolic PDEs are not necessarily easy to control. They can be unstable, with many unstable eigenvalues, and only one input acting at the boundary of a domain. This mix of simplicity within the PDE family, with the non-triviality for control, makes hyperbolic PDEs the ideal entry point for any new study in PDE control, including the introduction of a new framework for learning-based control in our \cite{https://doi.org/10.48550/arxiv.2302.14265}. 

Control design problems for hyperbolic PDEs are hyperbolic PDEs themsleves, namely, equations with only first derivatives in multiple spatial variables. Parabolic PDEs, with their first derivative in time but second derivatives in space, are the natural next challenge for learning the PDE backstepping methodology using neural operators. This is what we undertake in this paper. 




ML/AI is often (not entirely unjustifiably) thought of as an `existential threat' to model-based sciences, from physics to conventional control theory. In recent years, a framework has emerged \cite{lu2019deeponet,lu2021deeponet, li2020neural,li2021fourier}, initiated by George Karniadakis, his coauthors, and teams led by Anima Anandkumar and Andrew Stuart, which promises to unite the goals of physics and learning, rather than presenting learning as an alternative or substitute to first-principles physics. In this framework, often referred to as neural operators (NO), which is formulated as learning of mappings from function spaces into function spaces, and is particularly suitable for PDEs, solution/``flow'' maps can be learned after a  sufficiently large number of simulations for different initial conditions. (In some cases,  parameters of models can also be identified from experiments.)

\paragraph{Mappings of plant parameters to control gains and learning of those maps} 
One can't but ask
%It is impossible not to ask oneself 
what the neural operator reasoning can offer to control theory, namely, to the design of controllers, observers, and online parameter estimators. This paper is the first venture in this direction, a breakthrough with further possibilities, and a blueprint (of a long series of steps) to learn PDE control designs and prove their stability. 

In control systems (feedback controllers, observers, identifiers), various kinds of nonlinear maps arise, some from vector  into vector spaces, others from vector or function spaces into function spaces. Some of the maps have time as an argument (making the domain infinite) and others are mappings from compact domains into compact image sets, such as mappings converting system coefficients into controller coefficients, such as the mapping $K(A,B)$ for the closed-loop system $\dot x = Ax + Bu,\ u=Kx$ (under either pole placement or LQR). 

While learning nonlinear maps for various design problems for nonlinear ODEs would be worth a study, we focus in this initial work one step beyond, on a benchmark PDE control class. Our focus on an uncomplicated---but unstable---PDE control class is for pedagogical reasons. Combining the operator learning with
%ideas with PDE control, particularly with 
{\em PDE backstepping} is complex enough even for the simplest-looking among PDE stabilization problems. 
%So it is for this reason that we start here from a benchmark problem. 

\paragraph{PDE backstepping control with the gain computation obviated using neural operators}
Consider 1D hyperbolic partial integro-differential equation systems of the general form $v_t(x,t) = v_x(x,t) + \lambda(x) v(x,t) + g(x) v(0,t) +\int_0^x f(x,y) v(y,t) dy$ on the  unit interval $x\in[0,1]$, which are  transformable, using an invertible backstepping ``pre-transformation'' introduced in \cite{Bernard2014} into the simple PDE % of the form
\begin{eqnarray}
    \label{eq-PDE}
	u_t(x,t) &=& u_x(x,t) + \beta(x) u(0, t) %, \qquad \mbox{$x\in[0,1)$}
 \\  \label{eq-PDEBC}
	u(1, t) &=& U(t). 
\end{eqnarray}
Our goal is the design of a PDE backstepping boundary control 
%law of the integral (in space) form
\begin{equation}\label{eq-bkstfbkk}
    U(t) = \int_0^1 k(1-y) u(y,t) dy. 
\end{equation}
Physically, \eqref{eq-PDE} is a ``transport process (from $x=1$ towards $x=0$) with recirculation'' of the outlet variable $u(0,t)$. Recirculation causes instability when the  coefficient $\beta(x)$ is positive and large. This instability is prevented by the backstepping boundary feedback \eqref{eq-bkstfbkk} with the gain  function $k(\cdot)$ as a kernel in the spatial integration of the measured state $u(y,t)$. (The full state does not need to be measured, as explained in Remark \ref{rem-observer} at the end of Section \ref{sec-stabilityDeepONet}.)

Backstepping  produces the gain kernel $k$ for a given  $\beta$. The mapping ${\cal K} : \beta \mapsto k$ is nonlinear, continuous, and we learn it. 
%More specifically, it is a locally Lipschitz mapping of continuous functions into continuous functions. Our interest here is in learning the nonlinear mapping ${\cal K}$. 


Why do we care to learn ${\cal K}$? The kernel {\em function} $k$ can always be computed for a particular $\beta$, so what is the interest in learning the {\em functional mapping/operator}? Once ${\cal K}$ is learned, $k$ no longer needs to be sought, for a new $\beta$, as a solution to a partial differential or integral equation. For the next/new $\beta$, finding $k$ is simply a ``function evaluation'' of the learned mapping ${\cal K}$. This provides  benefits in 
%several contexts, most of all in the context of 
both adaptive control where, at each time step, the gain  estimate $\hat k$ has to be computed for a new parameter update $\hat\beta$, and in gain scheduling for nonlinear PDEs where the gain  has to be recomputed at each current value of the state. 
%Not having to solve a partial differential or integral equation, again and again, at each time step, turns adaptive and gain scheduling control of PDEs from nice theoretical concepts into usable, computationally feasible technologies. 


\begin{figure}[t]
    \centering \includegraphics{algorithm.pdf}
	\caption{An algorithmic representation of our design paradigm of employing neural operators in boundary control of PDEs. Three major step clusters are performed: (1)  \underline{derivation} of the integral equations for the backstepping kernels, performed only once; (2)  \underline{learning} of the mapping from the plant parameter functions into the backstepping kernel functions, also performed only once; and (3) \underline{implementation} of the controller for specific plant parameters. The task in the top box has  been completed in \cite{krstic2008Backstepping}. In this paper, the task in the middle box is introduced and stability guarantees for the task in the bottom box are provided.}
    \label{fig:0}
\end{figure}


As well known, learning (ML, in general, and its operator learning varieties: DeepONet, FNO, LOCA, NOMAD, etc.) comes with an upfront price. Large data sets need to be first produced, and then large (possibly ``deep'') neural networks need to be trained. There is no exception to this in the approach we propose. For a large sample set of recirculation functions $\beta_i$, we need to first solve for the corresponding backstepping kernels $k_i$. After that, a NN approximation of ${\cal K}$ needs to be trained on that data set of the $(\beta_i, k_i)$ pairs. 
%This is the framework we introduce in this paper. 

One can stop at producing the NN approximation of the mapping ${\cal K}$ and proceed with a heuristic use of the  approximated gains $\hat k$. But we don't stop there. We ask whether the PDE system will be still stabilized with the NN-approximated gain kernel $\hat k$. Our main theoretical result is affirmative. With a large enough data set of solved pairs $(\beta_i, k_i)$, and a large enough trained (deep) NN, closed-loop stability is guaranteed for a new $\beta$, not in the training set. 

When ML is applied in the control  context (as RL or other approaches), it is usually regarded as a model-free design. Our design, summarized in Figure \ref{fig:0}, is not model-free; it is model-based. It is only that the computational portion of this model-based (PDE backstepping) design is obviated through ML. 

Our learning is offline; not as in adaptive control \cite{Bernard2014,Anfinsen2019Adaptive}. 

\paragraph{Neural operator literature---a brief summary}
    Neural operators are NN-parameterized maps for learning relationships between function spaces. They originally gained popularity due to their success in mapping PDE solutions while remaining discretization-invariant. Generally, nonlinear operators consist of three components: an encoder, an approximator, and a reconstructor \cite{lanthaler2022errorDeeponet}. The encoder is an interpolation from an infinite-dimensional function space to a finite-dimensional vector representation. %For example, in our work we consider the pointwise evaluations of our function on the interval $[0, 1]$. 
    The approximator aims to mimic the infinite map using a finite-dimensional representation of both the domain function space and the target function space. 
    The reconstructor then transforms the approximation output into the infinite-dimensional target function space. 
    The implementation of both the approximator and the reconstructor is generally coupled and can take many forms. For example, the original DeepONet \cite{lu2021deeponet} contains a ``branch'' net that represents the approximation network and a ``trunk'' net that builds a basis for the target function space. The outputs of the two networks are then taken in linear combination with each other to form the operator. FNO \cite{li2021fourier} utilizes the approximation network in a Fourier domain where the reconstruction is done on a basis of the trigonometric polynomials. LOCA \cite{kissas2022loca} integrates the approximation network and reconstruction step with a unified attention mechanism. 
    NOMAD \cite{seidman2022nomad} extends the linear reconstructor map in DeepONet to a nonlinear map that is capable of learning on nonlinear submanifolds in function spaces.
    %this so that the approximation output is appended to the encoder's evaluation domain fed into a neural network changing the reconstruction step of a DeepONet into a non-linear mapping. 
    %Additionally, different approaches have been developed for the approximation network. For example, 
    There have been many more extensions to the neural operator architectures omitted here as they are usually designed around domain-specific enhancements \cite{wang2021physicsinformedNeuralOperators} \cite{li2022dissipative} \cite{Pickering2022}. Another line of work, called physics-informed neural networks (PINNs) \cite{raissi2019physics, karniadakis2021physics}, which can be used as generic solvers of PDEs by adding physics constraint loss to neural networks. However, PINNs need to be re-trained for new recirculation function $\beta$, thus not providing as much acceleration for the computation of the backstepping kernels as the neural operators.

\paragraph{Advances in learning-based control}
Among the first in demonstrating the stability of learning-based model predictive controllers (MPC) were the papers \cite{ASWANI20131216,rosolia2017learning}, followed in several directions. First, for nonlinear systems, deep learning-based approaches consist of jointly learning the controller and(or) Lyapunov functions via NNs~\cite{tedrakeNNControl, chang2019neural, chen2020learning, chen2021learning, chen2021learning2, dawson2022safe, chen2022large}. \cite{chang2019neural} proposed a method for learning control policies and NN Lyapunov functions using an empirical Lyapunov loss and then validating using formal verification. \cite{chen2020learning, chen2021learning} generalize the method to learning Lyapunov functions for piecewise linear and hybrid systems, and \cite{chen2021learning2} for learning regions of attraction of  nonlinear systems. 
In addition, \cite{taylor2019control, nguyen2021}
have explored how learning-based control will affect nominal systems with known Lyapunov functions, and 
\cite{boffi2021learning, pfrommer2022tasil, claudioNonlinear2023} studied the problem of learning stability certificates and stable controllers directly from data.
%\cite{nguyen2021, taylor2019control}. 
% Furthermore, \cite{richards2018lyapunov, boffi2021regret} has explored learning adaptive stability certification with deep neural networks. 
In a similar vein, \cite{frank2022} has developed a provable stable data-driven algorithm based on system measurements and prior knowledge for linear time-invariant systems. 

 In a separate, but related direction, many reinforcement learning (RL) \cite{Bert05,sutton2018reinforcement} control approaches have been developed over the past few years. On the one side, model-based RL has been studied due to its superior sample efficiency and interpretable guarantees. The main focus has been on learning the system dynamics and 
 providing closed-loop guarantees in \emph{finite-time} for both linear systems \cite{dean2018regret,chen2021black,lale2022reinforcement,faradonbeh2018finite,tsiamis2022learning} (and references within), and nonlinear systems~\cite{berkenkamp2017safe,kakade2020information,singh2021learning,lale2022kcrl}. %While deriving these guarantees, the finite-time stability guarantees are also derived for linear systems~\cite{} and nonlinear systems that have quadratic Lyapunov functions~\cite{}. 
 For model-free RL methods, \cite{fazel2018global,mohammadi2021convergence,jiang2022,zhao2023global} proved the convergence of policy optimization, a popular model-free RL method, to the optimal controller for linear time-invariant systems, ~\cite{PANG2020109035, pramod2022} for linear time-varying systems, ~\cite{tang2021analysis} for partially observed linear systems. See \cite{hu2022towards} for a recent review of policy optimization methods for continuous control problems such as the LQR, $H_{\infty}$ control, risk-sensitive control, LQG, and output feedback synthesis. For nonlinear systems, \cite{chow2018lyapunov,choi2020reinforcement, cui2022structured,shi2022stability} investigated policy optimization with stability guarantees in which the stability constraints are derived from control Lyapunov functions.
 In addition to policy optimization methods, \cite{vamvoudakis2010online, lewis2012reinforcement, bhasin2013novel, vamvoudakis2017q} have studied and proved the stability and asymptotic convergence of other model-free RL algorithms such as actor-critic methods~\cite{vamvoudakis2010online, lewis2012reinforcement} and Q-learning~\cite{vamvoudakis2017q} in control affine systems.
In the domain of cyber-physical systems (CPS), a theoretical framework has been developed for learning-based control to handle partially observable systems \cite{andreasSeparation2023}.

Many advances have been made in learning-based control in games and multi-agent systems \cite{zhang2019policy, mazumdar2020gradient, fiez2020implicit, mojica2022stackelberg, zhang2021multi, mao2022provably, %mojica2022stackelberg, 
poveda2022fixed, fiez2020implicit, vamvoudakis2017game, qu2020scalable, lin2021multi}. Convergence is characterized for various learning-based methods to Nash equilibria in zero-sum linear quadratic games~\cite{zhang2019policy}, continuous games~\cite{mazumdar2020gradient}, Stackelberg games~\cite{fiez2020implicit, mojica2022stackelberg}, Markov games~\cite{zhang2020model,mao2022provably}, and multi-agent learning over networked systems~\cite{qu2020scalable, lin2021multi, poveda2022fixed}. A recent review for learning-based control in games is in~\cite{zhang2021multi}. 

We focus on learning-based control for PDE systems. In our previous work~\cite{shi2022machine}, we demonstrate the empirical success of using NOs for accelerating PDE backstepping observers, without theoretical guarantees. This work represents the first step towards using NOs for provably bypassing gain computations (with exponential stability guarantees) or directly learning the controller (with practical stability) in PDE backstepping. 
%given that most learning methods give only practical stability - actually most ML control do not have 

\paragraph{Backstepping control of first-order hyperbolic PDEs}
The PDE system \eqref{eq-PDE}, \eqref{eq-PDEBC} is the simplest open-loop unstable PDE of any kind which can be of interest to the researcher working on PDE stabilization by boundary control. This system is treated here as a technical benchmark, as was done as well in \cite{Bernard2014} and a number of other references offering methodological advances in PDE stabilization. System \eqref{eq-PDE}, \eqref{eq-PDEBC} is a particular case of a single-PDE hyperbolic class in \cite{krstic2008Backstepping} for which PDE backstepping was first introduced in the hyperbolic setting. Coupled systems of first-order hyperbolic PDEs are of greater interest because they arise in fluid flows, traffic flows, elastic structures, and other applications. The first result on backstepping for a {\em pair} of coupled hyperbolic PDEs was in \cite{Coron2013Local}. The extension from two to $n+1$ hyperbolic PDEs, with actuation of only one and with counterconvection of $n$ other PDEs was introduced in \cite{Meglio2013Stabilization}. An extension from $n+1$ to $n+m$ coupled PDEs, with actuation on $m$ ``homodirectional'' PDEs, was provided in \cite{Hu2016Control,hu2019boundary}. Redesigns that are robust to delays were provided in \cite{Auriol2018Delay1}. 
An extension from coupled hyperbolic PDEs to cascades with ODEs was presented in \cite{DIMEGLIO2018281}. An extension from hyperbolic PDE-ODE cascades to ``sandwiched'' ODE-PDE-ODE systems was presented in \cite{WANG2020109131} and an event-triggered design for such systems was given in \cite{9319184}. The extension of PDE backstepping to output-feedback regulation with disturbances is proposed in \cite{DEUTSCHER201556,deutscher2018}. For coupled hyperbolic PDEs with unknown parameters, a comprehensive collection of adaptive control designs was provided in the book \cite{Anfinsen2019Adaptive}. Applications of backstepping to coupled hyperbolic PDE models of traffic are introduced in \cite{Yu2019Traffic,Yu2022}. 



\paragraph{Paper outline and contributions}
After a brief introduction to the backstepping design in Section \ref{sec-bkst-intro}, for system \eqref{eq-PDE}, \eqref{eq-PDEBC}, in Section \ref{sec-kernelLip} we prove that the backstepping kernel operator is locally Lipschitz, between the spaces of continuous functions, with which we satisfy a sufficient condition for the existence of a neural operator approximation of a nonlinear operator to arbitrarily high accuracy---stated at the section's end in a formal result and illustrated with an example of approximating the operator $k={\cal K}(\beta)$. 
%After a brief introduction of a DeepONet-approximated backstepping transformation in Section \ref{sec-bkstapprox}, 
In Section \ref{sec-stabilityDeepONet} we present the first of our main results: the closed-loop stabilization (not merely practical but exponential) with a DeepONet-approximated backstepping gain kerne  l function. In Section \ref{sec-simulations} we present simulation results that illustrate stabilization under DeepONet-approximated gains. Then, in Section~\ref{sec-beta,u->u} we pose the question of whether we can not only approximate the gain kernel mapping $\beta(x)\mapsto k(x)$, as in Sections \ref{sec-kernelLip}
%\ref{sec-bkstapprox}, 
and \ref{sec-stabilityDeepONet}, but the entire feedback law mapping $(\beta(x),u(x,t)) \mapsto \int_0^1 k(1-y)u(y,t)dy$ at each time instant $t$; we provide an affirmative answer and a guarantee of semiglobal practical exponential stability under such a DeepONet approximation. In Section \ref{sims-fbklawapprox} we illustrate this feedback law approximation with a theory-confirming simulation. Then, in Section \ref{sec-PIDEextension}, we present the paper's most general result, which we leave for the end for pedagogical reasons, since it deals with Volterra operator kernel functions of two variables, $(x,y)$, on a triangular domain, and requires continuity of mappings between  spaces of functions that are not just continuous but continuously differentiable, so that not only the backstepping kernel is accurately approximable but also the kernel's spatial derivatives, as required for closed-loop stability. We close with a numerical illustration for this general case in Section \ref{sec-simulationsQ}.

In summary, the paper's contributions are the  PDE stabilization under DeepONet approximations of backstepping gain kernels (Theorems \ref{thm-stabDeepONet} and \ref{thm-stabDeepONet-gf}) and under the approximation of backstepping feedback laws (Theorem \ref{thm-semiglobalpractical}). Our stabilization results also hold for any  other neural operators with a universal approximation property (shown for LOCA~\cite{kissas2022loca} and for FNO on the periodic domain~\cite{kovachki2021universal}).   


\bibliography{refs}
\bibliographystyle{abbrv}
\end{document}


