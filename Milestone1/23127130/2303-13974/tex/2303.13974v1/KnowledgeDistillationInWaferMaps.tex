\documentclass{article}


\usepackage{PRIMEarxiv}
\usepackage{bbm}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{images/}}     % organize your images and other figures under media/ folder

\begin{document}
%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
% \fancyhead[LO]{Running Title for Header}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
\title{Mixed-Type Wafer Classification For Low Memory Devices Using Knowledge Distillation
%%%% Cite as
%%%% Update your official citation here when published 

}

\author{
\begin{tabular}[t]{c@{\extracolsep{4em}}c@{\extracolsep{4em}}c} 
Nitish Shukla  & Anurima Dey & Srivatsan K\\
\textit{Independent researcher } & \textit{Independent researcher } & \textit{Independent researcher }\\
\texttt{nitishshukla86@gmail.com} & \texttt{anurima10@gmail.com} &\texttt{srivatsanraman3@gmail.com} 
\end{tabular}
}

% The paper headers
\markboth{IEEE Transactions on Semiconductor Manufacturing } {Shukla \MakeLowercase{\textit{et al.}}: KD for efficient DPR}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle


\begin{abstract}
Manufacturing wafers is an intricate task involving
thousands of steps. Defect Pattern Recognition (DPR) of wafer
maps is crucial for determining the root cause of production
defects, which may further provide insight for yield improvement
in wafer foundry. During manufacturing, various defects may
appear standalone in the wafer or may appear as different
combinations. Identifying multiple defects in a wafer is generally
harder compared to identifying a single defect. Recently, deep
learning methods have gained significant traction in mixed-type
DPR. However, the complexity of defects requires complex and
large models making them very difficult to operate on low-memory embedded devices typically used in fabrication labs.
Another common issue is the unavailability of labeled data to train
complex networks. In this work, we propose an unsupervised
training routine to distill the knowledge of complex pre-trained
models to lightweight deployment-ready models. We empirically
show that this type of training compresses the model without
sacrificing accuracy despite being up to 10 times smaller than
the teacher model. The compressed model also manages to outperform
contemporary state-of-the-art models.

\end{abstract}


\keywords{Knowledge Distillation, \and Wafer Defect Classification, \and Embedded devices}

\section{Introduction}
Wafers are silicon bases on which the fabrication of the Integrated Circuits (ICs) takes place. Wafer fabrication and testing are the two most crucial steps of the IC manufacturing process. Wafer fabrication consists of a series of complex steps laid down in the design of the particular type of IC. The layers of fabrication are compassed at the nanoscale by highly sophisticated machines. Defects incorporated in a wafer during this fabrication process, if not amended at an early stage, may cause severe damage to the production line. The root causes of such defects variation can be due to any of the 6Ms\cite{ref-6} namely Man, Method, Machine, Material, Milieu, and Measurement which may have blended during the fabrication process. After wafer testing is done, these defects are observed in the form of failed dies on a wafer, known as a wafer map. Studying these patterns on the wafer map enables the engineers to determine the probable cause of the die failures which in turn allows them to improve the product yield \cite{ref-5}. The study of such defects on wafer maps is commonly known as Defect Pattern Recognition (DPR). 

Historically, such defects were manually analyzed using various statistical methods by experienced engineers and subject matter experts. However, with the advent of Deep Neural Networks and CNNs\cite{ref-4}, detecting such defects is slowly becoming more efficient. Various supervised and unsupervised models are currently in use for detecting the type of defects on the wafer map. A recent study on semi-supervised pattern recognition\cite{ref-3} on wafer maps, has found a total of 14 defect types \cite{ref-2}. Some of the common ones are “Centre”, “Donut”, “Scratch”, “Edge-ring” etc. However, adding to the problem, these defects do not always stand alone. Due to the complexity of the entire manufacturing process, each wafer passes through a series of chemical processes occurring in layers. The compactness of ICs has increased over time according to Moore’s law \cite{ref-1}, which resulted in more layers. It so happens that due to one of the 6M cause and effect \cite{ref0}, a particular defect type may appear in one such layer and another may appear in another layer on a single wafer. When this wafer is manufactured, there can be two or more such defects appearing on a single wafer map. Such patterns are called mixed-type wafer defects and have gained a lot of attention in recent times. 

Deep neural networks have been very successful in fields ranging from computer vision \cite{ref1} \cite{ref2} \cite{ref3}, reinforcement
learning \cite{ref4} \cite{ref5} to natural language processing \cite{ref6} and speech recognition \cite{ref7}. However, deep models are generally very huge making deployment to low-memory devices with limited computation complexity very challenging. To address this issue, Bucilua et al\cite{ref8} first proposed model compression to transfer learned information from a large model or set of models to smaller models without a significant drop in accuracy. A semi-supervised version of knowledge transfer was also introduced by Urner et al., 2011\cite{ref9} allowing the student model to learn without using ground truth labels. The central idea is that the student model mimics the teacher model
in order to obtain a competitive or even superior
performance. The key problem is how to transfer the
knowledge from a large teacher model to a small student
model.

\section{Motivation}
Accurately classifying the defect patterns on the wafer is a computer vision problem.
In the initial phases of this research, mostly shallow models were used like SVMs, RBFN, and decision tree \cite{ref15}. However, the efficiency of these methods is limited to very good input features \cite{ref16} and image dimensionality. In recent times deep learning has received a lot of popularity in this field since they extract feature from raw data automatically. Particularly, CNNs received great admiration when Nakazawa and Kulkarni \cite{ref17} could identify 22 defects mixed by six basic types  (random, ring, edge,
scratch, cluster, gross). Kyeong and Kim \cite{ref18} applied the CNN model to obtain 16 defects combining circle, ring, scratch, and zone. Wang et al. \cite{ref19} have proposed a Deformable CNN (DC-Net) capable of identifying up to 38 mixed-type defects comprising 8 basic defect types. 
\begin{figure}
    \centering
    \includegraphics[scale=0.5]{images/arc.pdf}
    \caption{Architecture of the knowledge distillation routine. The distillation loss depends on the logits produced by $T$, $S$ and the soft labels obtained from the predictions made by $T$.}
    \label{fig:architecture}
\end{figure}


While all these models have commendable performance in detecting the patterns, a major disadvantage lies in the fact that these  deep neural networks have lots of parameters adding to the inherent complexity. Due to the size of the models, they often become very computationally expensive and time-consuming while dealing with real-life fabrication data sets. This makes it nearly impossible for deploying on smaller machines with limited memory capacity. Even when resources are abundant, a lightweight efficient model does no harm since it would be able to serve more clients at a lower cost. As a result, the serviceability of deep neural network models comes with a lot of constraints. One probable way of mitigating this problem is by using simpler models like decision trees, however as mentioned previously they would require very comprehensive feature engineering for comparable accuracy. Even CNNs with reasonably limited capacity do not promise good performance. Naturally, a high-performing lightweight model is the optimal middle ground considering performance and size. This motivated us to apply Knowledge Distillation(KD) which can help in creating a smaller model, with comparable accuracy to the complicated bigger model. Knowledge Distillation is a way of model compression where the smaller network is trained to approximate the teacher network by trying to replicate its output at every level \cite{ref20}.

In deep learning, Knowledge Distillation is a widely used effective technique that was first defined and generalized by Hilton et al.\cite{ref-5}. Knowledge Distillation methods for knowledge transfer have traditionally used supervised learning and semi-supervised learning. Here, we propose an unsupervised Knowledge Distillation learning routine that competes well against current state-of-the-art methods for identifying mixed-type wafer defects. 

%\afterpage{%
\begin{figure}
    \centering 
     \fbox{\includegraphics[width=12cm,height=5.5cm]{images/One_defect.pdf}}\\
     \fbox{\includegraphics[width=12cm,height=8cm]{images/Two_defect.pdf}}\\
     \fbox{\includegraphics[width=12cm,height=5.5cm]{images/Three_defect.pdf}}\\
      \fbox{\includegraphics[width=12cm,height=2.7cm]{images/Four_defect.pdf}}
    \caption{The 38 types of original wafer map defects in the MixedWM38 dataset.   }
    \label{fig:my_label}
\end{figure}
%}

\section{Knowledge Distillation}
Knowledge distillation (KD) is an important technique in the domain of model compression. In knowledge distillation, a small lightweight \textit{student} model is trained to mimic the performance of a compute-intensive high-performing \textit{teacher} model. Surprisingly enough, sometimes the student performs much better than the teacher even though both have the same capacity\cite{ref22}\cite{ref23}\cite{ref24}. Large-scale deep models
have achieved profuse successes in various fields ranging from computer vision \cite{ref25} to NLP \cite{ref26}, however, the
huge computational complexity and massive storage
requirements make it a great challenge to deploy them
in real-time applications, especially on devices with
limited resources, such as video surveillance and autonomous driving cars.

A knowledge distillation system is
generally composed of three key components: knowledge, distillation algorithm, and teacher-student architecture. Figure \ref{fig:architecture} shows a common teacher-student framework for knowledge distillation. Typically, knowledge distillation routines can be classified into the following categories:


\textbf{Response-Based Knowledge Distillation: } The main idea in response-based knowledge is to directly mimic the outputs of the teacher. Response-based knowledge distillation is a simple yet effective technique for model compression and it is widely used in different applications. Given a teacher model $T$ and student model $S$, response-based knowledge distillation optimizes the distillation loss between the logits $z_t=T(x)$ and $z_s=S(x)$  formulated as
\begin{equation}
    \mathcal{L}_{KD}=\mathcal{L}(z_t,z_s)
\end{equation}
More than often, $\mathcal{L}_{KD}$ is employed as Kullback-Leibler loss or even simpler mean-square-loss(MSE) loss. Clearly, minimizing $\mathcal{L}_{KD}$ forces the student logits to match the teacher logits, this essentially results in the student model mimicking the teacher model.

\textbf{Feature-Based Knowledge Distillation} Neural Networks are exceptionally well at learning representations of input concepts in increasing levels of abstraction. Feature-based knowledge distillation employs the distillation loss between each feature layer in the teacher and student model respectively. Feature-based distillation is an effective extension of response-based learning, especially for thinner and deeper models. The loss functionally generally takes the form
\begin{equation}
    \mathcal{L}_{KD}=\mathcal{L}(\Phi_T(T(x)),\Phi_S(S(x)))
\end{equation}
where $T(x)$ and $S(x)$ are the feature maps of the intermediate layers of the teacher and student model. The transformation $\Phi_T$ and $\Phi_S$ are applied when the feature map has different shapes.


\textbf{Relation-Based Knowledge Distillation}: Relation-based knowledge distillation further explores the relationships between the different layers in the teacher and student model. In general, the distillation loss of relation-based
knowledge based on the relations of feature maps can
be formulated as
\begin{equation}
    \mathcal{L}_{KD}=\mathcal{L}_{R}(\Psi_T(\hat{T}(x),\check{T}(x)),\Psi_S(\hat{S}(x),\check{S}(x)))
\end{equation}
where $T(x)$ and $S(x)$ are feature maps from different layers in teacher and student models, respectively. Pairs of feature maps are picked from both teacher and student models, $\hat{T}(x)$ and $\check{T}(x)$ from the teacher and $\hat{S}(x)$, $\check{S}(x)$ from the student. $\Psi_T$ and $\Psi_S$ are the similarity functions for the pair of feature maps sampled and $\mathcal{L}_R$ is the correlation function between the teacher and student network.


\section{Method}

\subsection{Dataset and Preprocessing}
\label{sec:dataset}
We conduct our experiments on  MixedWM38 WaferMap\cite{ref40} dataset.
It comprises more than 38000 original wafer maps as well as synthetically generated wafer maps divided into 38 classes based on
their defect patterns. 
Out of the 38 defect pattern classes, one class is the normal class where the wafer does not contain any defects, 8 classes are of the single defect patterns
and the rest are various combinations of single defect patterns.
`Center(C)' indicates a defect pattern concentrated at the center of the wafer, 
`Donut(D)' refers to a defect pattern in the shape of an annular disc/donut,
`Edge-Loc (EL)' is a defect pattern where the defect is concentrated around the edge of the wafer,
`Edge-Ring (ER)' is a global defect pattern occurring around the entire circumference of the wafer,
`Loc(L)' refers to a defect pattern that is in a concentrated region other than the center.
`Near-Full (NF)' is a seldom occurring defect that is present throughout the entirety of the wafer,
`Scratch(S)' indicates a defect pattern that is in the form of a narrow line along the wafer. Finally,
`Random(R)' occurs when dies are failed at random locations on the wafer.

Other than these single defect patterns, there are various defect patterns 
that comprise two or more single defect patterns in the same wafer, which are
known as mixed defect patterns.
Based on the number of single defects, the mixed defect patterns have been named as
2 mixed-type, 3 mixed-type, and 4 mixed-type.

The 38 classes identified as C1 through C38, have C1 as the normal no-defect class while
C2-C9 form the single defect patterns. The 13 mixed defect patterns from C10-C22 belong to 
wafers with 2 single-type defects on them, while the 12 mixed defect patterns identified as C23 through C34 belong to 
wafers with 3 single-type defects on them. Finally, the wafers with 4 single-type defects on them are
identified as C35-C38.

The pattern name for mixed defect patterns indicates which single type pattern it consists of.
For example, class C24, named C + EL + S, consists of three single defect patterns: Center, Edge-Loc, and Scratch.
define the dataset 

\textbf{Label encoding.} The classes are represented in the form of a one-hot encoded vector.
Since there are 8 primary defects, an 8-length binary vector is used as the class label.
The presence of `1' at a position indicates a certain type of defect being present.
For example, class C24 is encoded as \texttt{[1 0 1 0 0 0 1 0]} having 1's at positions 1, 3, and 7,
which corresponds to Center, Edge-Loc, and Scratch patterns being together simultaneously.
\begin{table}[]
    \centering
    \begin{tabular}{cc}
        \includegraphics[scale=0.3]{images/accuracies_1-9.pdf} & \includegraphics[scale=0.3]{images/accuracies_10-22.pdf} \\
         \includegraphics[scale=0.3]{images/accuracies_23-34.pdf}& \includegraphics[scale=0.3]{images/accuracies_34-38.pdf}
    \end{tabular}
    \caption{Accuray of compared models. (Top, Left to Right) The accuracy of compared models on single-type and 2-type mixed defects. The bigger distilled network (in red) competes well with contemporary models. (Bottom, Left to Right) Accuracy of compared models on 3 and 4-type mixed defects. }
    \label{tab:accs}
\end{table}
% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=\columnwidth]{images/accuracies_1-9.pdf}
%     \caption{The accuracy of compared models on single type defects. The bigger distilled network (in red) competes well with contemporary models.}
%     \label{fig:acc1}
% \end{figure}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=\columnwidth]{images/accuracies_10-22.pdf}
%     \caption{The accuracy of compared models on 2 mixed defects. The bigger distilled network (in red) performs competitively.}
%     \label{fig:acc10}
% \end{figure}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=\columnwidth]{images/accuracies_23-34.pdf}
%     \caption{The accuracy of compared models on 3 mixed defects. The bigger distilled network (in red) performs well on complex mixture of defects.}
%     \label{fig:acc23}
% \end{figure}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=\columnwidth]{images/accuracies_34-38.pdf}
%     \caption{The accuracy of compared models on 4 mixed defects. The bigger distilled network (in red) performs well on complex mixture of defects.}
%     \label{fig:acc34}
% \end{figure}



\begin{figure}[ht]
    \centering
    \includegraphics[width=\columnwidth]{images/train_test_acc.pdf}
    \caption{The accuracy curve of teacher model (left), bigger distilled network (middle) and smaller distilled network(right).}
    \label{fig:train_test_acc}
\end{figure}
\begin{figure}[ht]
    \centering
    \includegraphics[width=\columnwidth]{images/train_test_loss.pdf}
    \caption{The loss curve of teacher model (left), bigger distilled network (middle) and smaller distilled network(right).}
    \label{fig:train_test_loss}
\end{figure}

\subsection{Architecture}
We follow the response-based teacher-student model of knowledge distillation. Typically, the teacher model is a large neural network capable of performing complex tasks. The student model, on the other hand, is generally, a small network that we wish to train for specific tasks with the aim that the student model mimics closely the performance of the teacher model for the chosen task. 

Let $S$ be the student model and $T$ be the teacher model parameterized by ${\theta}$ and ${\theta'}$ respectively. The learning for the student model aims to minimize the loss


% \begin{equation}

% \resizebox{0.91\hsize}{!}{ 
% \mathcal{L_{KD}}(\theta,\textbf{x})= \alpha \mathcal{L}_{mse} (S(\textbf{x}),T(\textbf{x})) +(1-\alpha) \mathcal{L}_{bce} (\sigma(S(\textbf{x})),\mathbbm{1}_{\x\geq \beta} (\sigma(T(\textbf{x}))))
%     \label{eq:loss}
%     }
% \end{equation}


%    \mathcal{L_{KD}}= \alpha\sum_{i=1}^m(f(x^{(i)})-g(x^{(i)}))^2 + (1-\alpha) \sum_{i=1}^m  \mathcal{L}_{bce}(g(x^{(i)}),\textit{rnd}(f(x^{(i)})) )
where $\sigma$ is the \textit{sigmoid} function defined as $\sigma(x)=\frac{1}{1+e^{-x}}$, $\mathcal{L}_{mse}$ is the \textit{mean-square-error}, $\mathcal{L}_{bce}$ is the \textit{binary cross entropy} error defined as $\mathcal{L}_{bce}(x,y)= -ylog(x)-(1-y)log(1-x)$ and  $\mathbbm{1}_{x\geq \beta}$ is an indicator variable defined as  
\[\mathbbm{1}_{x\geq \beta}= \left\{
\begin{array}{ll}

      1 & x\geq \beta \\
      0 & otherwise \\
\end{array} 
\right. \]

    The first part of the loss function encourages $S$ to learn logits that are similar to what $T$ has produced by minimizing the $L_2$ distance between both sets of logits obtained on training images. The second part, on the other hand, penalizes misclassification by $S$ on the predictions made by $T$. The parameters $\alpha$ and $\beta$ are hyperparameters that control 1) emphasis given to logits or final classification and 2) sensitivity of predictions made by $T$. $\mathcal{L_{KD}}$ only depends on $\theta$ as the weights of $T$ are frozen during training and is free of hard labels $\textbf{y}$ which are approximated by $T$'s predictions on $\textbf{x}$. For our experiments, we set both  $\alpha$ and $\beta$ as 0.5.

For the teacher network, we use a ResNet-18 \cite{ref2} modified to output 8 scores which is the number of unique defects. The network is trained to minimize the loss
\begin{equation}
    \mathcal{L_{T}}(\theta',\textbf{x},\mathbf{y})= \mathcal{L}_{bce} (\sigma(T(\textbf{x})),\mathbf{y})
\end{equation}
where $\sigma$ is the \textit{sigmoid} function described above. \textbf{x} is the set of input wafer maps and \textbf{y} is the corresponding labels. 

The first student model contains three normal convolution layers, where the input wafer maps are convoluted with 6, 16, and 32 convolution kernels of size 5$\times$5 pixels. Each of the convolution layers is interspersed with a max pool layer of size $2\times2$ and ReLU nonlinearity operation. Finally, a fully-connected classification head is attached. Similarly, the second student contains two normal convolution layers, where the input wafer maps are convoluted with 6, 16 convolution kernels of size 5$\times$5 pixels. The number of learnable parameters in both models is roughly $60K$ and $4K$ respectively contrasting to a total of around 11M learnable parameters in the teacher model.

\section{Experiments and Results}
We Experiment on the MixedWM38 dataset containing 38K wafer images as described in \ref{sec:dataset}. We split the dataset into train-test using 80\% of the images as the training set and the remaining as a test set. All the networks are trained on possible different train and test set which is sampled before each experiment independently. The experiments are conducted in Python compiler, Pytorch 1.12, and CUDA 11.3, the computer with option: Linux system, Intel(R) Xeon(R) CPU @ 2.20GHz, and Tesla T4 GPU. 

\begin{table}[]
    \centering
\resizebox{.7\textwidth}{!}{%
\begin{tabular}{cccc|ccc}
\toprule
Class & \multicolumn{3}{c}{Distilled Net-1} & \multicolumn{3}{c}{Distilled Net-2}\\

 & \textit{Precision} & \textit{Recall}  & \textit{F1}  
   & \textit{Precision} & \textit{Recall}  & \textit{F1}   \\
   \hline
\midrule
     C1 &                92.76\% &              99.51\% &           96.02\% &                40.79\% &              100.0\% &           57.95\% \\
     C2 &                94.62\% &              98.32\% &           96.44\% &                73.81\% &              86.59\% &           79.69\% \\
     C3 &                93.01\% &              97.71\% &            95.3\% &                 76.1\% &              87.61\% &           81.45\% \\
     C4 &                94.47\% &              94.95\% &           94.71\% &                90.26\% &               70.2\% &           78.98\% \\
     C5 &                95.11\% &              99.53\% &           97.27\% &                89.81\% &              90.23\% &           90.02\% \\
     C6 &                92.12\% &              96.89\% &           94.44\% &                68.18\% &              85.49\% &           75.86\% \\
     C7 &                100.0\% &              81.25\% &           89.66\% &                95.45\% &              65.62\% &           77.78\% \\
     C8 &                91.95\% &              98.77\% &           95.24\% &                68.63\% &              86.42\% &            76.5\% \\
     C9 &                99.26\% &               93.1\% &           96.09\% &                100.0\% &              82.07\% &           90.15\% \\
    C10 &                97.94\% &              93.14\% &           95.48\% &                82.21\% &              65.69\% &           73.02\% \\
    C11 &                92.54\% &              98.41\% &           95.38\% &                88.08\% &              89.95\% &           89.01\% \\
    C12 &                92.79\% &              93.69\% &           93.24\% &                 71.0\% &              79.61\% &           75.06\% \\
    C13 &                95.81\% &              98.56\% &           97.17\% &                68.67\% &              81.82\% &           74.67\% \\
    C14 &                93.19\% &              94.68\% &           93.93\% &                80.84\% &              71.81\% &           76.06\% \\
    C15 &                93.95\% &              93.52\% &           93.74\% &                85.92\% &              84.72\% &           85.31\% \\
    C16 &                90.57\% &              91.87\% &           91.21\% &                71.24\% &              79.43\% &           75.11\% \\
    C17 &                96.35\% &              94.39\% &           95.36\% &                72.09\% &              79.08\% &           75.43\% \\
    C18 &                92.92\% &              92.92\% &           92.92\% &                79.69\% &               67.7\% &           73.21\% \\
    C19 &                97.96\% &              88.89\% &            93.2\% &                 83.7\% &               71.3\% &            77.0\% \\
    C20 &                93.58\% &              96.69\% &           95.11\% &                88.37\% &              83.98\% &           86.12\% \\
    C21 &                96.67\% &              98.54\% &            97.6\% &                89.95\% &              91.26\% &            90.6\% \\
    C22 &                93.53\% &               94.0\% &           93.77\% &                66.93\% &               84.0\% &            74.5\% \\
    C23 &                94.09\% &              86.21\% &           89.97\% &                79.14\% &              54.19\% &           64.33\% \\
    C24 &                 97.7\% &              96.95\% &           97.32\% &                90.15\% &              74.37\% &            81.5\% \\
    C25 &                89.47\% &              91.89\% &           90.67\% &                89.17\% &              75.68\% &           81.87\% \\
    C26 &                93.62\% &              98.32\% &           95.91\% &                84.36\% &              84.36\% &           84.36\% \\
    C27 &                93.02\% &              97.09\% &           95.01\% &                72.29\% &              81.07\% &           76.43\% \\
    C28 &                92.86\% &              87.11\% &           89.89\% &                75.32\% &              61.34\% &           67.61\% \\
    C29 &                95.02\% &              89.25\% &           92.05\% &                81.88\% &              61.21\% &           70.05\% \\
    C30 &                93.75\% &              92.11\% &           92.92\% &                87.76\% &              75.44\% &           81.13\% \\
    C31 &                86.94\% &              95.54\% &           91.04\% &                84.16\% &              84.16\% &           84.16\% \\
    C32 &                87.62\% &              92.67\% &           90.08\% &                62.85\% &              83.25\% &           71.62\% \\
    C33 &                 94.8\% &               84.1\% &           89.13\% &                75.34\% &              56.41\% &           64.52\% \\
    C34 &                91.84\% &              92.31\% &           92.07\% &                 85.0\% &              87.18\% &           86.08\% \\
    C35 &                94.51\% &              86.87\% &           90.53\% &                76.92\% &              60.61\% &            67.8\% \\
    C36 &                91.67\% &               89.9\% &           90.78\% &                88.52\% &              77.88\% &           82.86\% \\
    C37 &                93.85\% &              84.72\% &           89.05\% &                77.78\% &              51.85\% &           62.22\% \\
    C38 &                90.24\% &              92.04\% &           91.13\% &                86.59\% &              77.11\% &           81.58\% \\
    \hline
Average &                \textbf{93.74\%} &              \textbf{93.33\%} &           \textbf{93.44\%} &                \textbf{79.71\%} &              \textbf{77.12\%} &           \textbf{77.41\%} \\
\bottomrule
\hline
\end{tabular}
}
    \caption{Precision and recall of Distilled Nets}
    \label{tab:prec_rec}
\end{table}

We train all the networks with the setting mentioned in Table \ref{tab:experimental setup}. We use ResNet-18 as the teacher network which is trained for 200 epochs gaining an overall accuracy of $98.34\%$. Next, we train two much smaller student networks that try to mimic the learning of the teacher model from unlabelled data. The size of the student model weights are 248KB and 21KB contrasting to 43MB of the teacher model.  We compare our distilled models with state-of-the-art methods.
\begin{table}[h]
    \centering
    \begin{tabular}{c|c}
        Description & Value\\
        \hline
         Initial Learning rate & 0.01 \\
         Optimizer & SGD\\
         Scheduler & Cosine Annealing \\
         Batch Size & 64\\
         Decay &  $5e^{-4}$\\
         Momentum &   0.9\\
    \end{tabular}
    
    \caption{Experimental Parameter Settings used while Training}
    \label{tab:experimental setup}
\end{table}

During training, the accuracy and loss curves of all three models are presented in Figure \ref{fig:train_test_acc} and Figure \ref{fig:train_test_loss}. We observe that the  first student model very quickly learns to mimic the teacher model and converges nicely over the course of 200 epochs while the second lags behind limited by its capacity. Despite its size and complexity, the second student model still outperforms Takeshi CNN and Kiryong CNN as presented in Table \ref{tab:accuracies}. 

As reported in Figure\ref{tab:accs}, the test accuracy on the validation set of both the distilled networks is competitive in their class. The average accuracy of a bigger distilled network is $93.33\%$ which is $20.81\%$ higher than Takeshi-CNN and $27.48\%$ higher than Kiryong-CNN and compares very well to DC-Net with $0.10\%$ higher test accuracy whereas the smaller network achieves an accuracy of $77.11\%$ which is $4.6\%$ higher than Takeshi CNN and $11.27\%$ higher than Kiryong CNN. We also observe the accuracy based on a number of defects on the wafer and report in Table \ref{tab:per-type-defect}. The bigger distilled network achieves competitive performance among the evaluated models especially when the number of defects on the wafer is greater than 2.
The results demonstrate that despite being very small and by only using a tiny number of convolutional layers, the distilled networks were effective for detecting mixed-type patterns when compared to their $\textit{hard}$ trained counterparts. From a manufacturing point of view, the detection of wafers without any defects \textit{i.e} normal wafers is crucial since any misclassification would result in passing a faulty wafer forward, which leads to downstream reliability issues. The accuracy of the proposed method with “C1” is $99.51\%$, which demonstrates the effectiveness of knowledge distillation in industrial practice.

We also evaluate the performance of our student networks in the false recognition and missing recognition of wafer defects. We use the statistical metrics of Precision, Recall, and F1 score which is defined as:

True Positive (TP): predicting positive, the actual is positive.

False Positive (FP): predicting positive, the actual is negative.

False Negative (FN): predicting negative, the actual is positive.

True Negative (TN): predicting negative, the actual is negative.

\begin{equation}
    Recall= \frac{TP}{TP+FN} \\
\end{equation}
\begin{equation}
    Precision= \frac{TP}{TP+FP} \\
\end{equation}
\begin{equation}
    F1= \frac{2}{\frac{1}{Recall}+\frac{1}{Precision}} \\
\end{equation}
We report the performance of both the distilled networks in Table \ref{tab:prec_rec}. Higher values of $Precision$ indicate less false recognition and higher value of $Recall$ indicate less missing recognition. As observed, the average precision and recall of our distilled net are very high which demonstrates that the distilled networks make a very less false prediction.

\begin{table}[h]
    \centering
    \resizebox{.7\textwidth}{!}{%
\begin{tabular}{cccccc}
\toprule
  Class & Takeshi CNN & Kiryong CNN & DC Net & Distilled Net-1 & Distilled Net-2 \\
  \hline


\midrule
  C1-C9 &      87.37\% &       64.8\% & 97.04\% &          95.32\% &          81.99\% \\
C10-C22 &      72.46\% &      65.67\% & 94.64\% &          94.03\% &          78.37\% \\
C23-C34 &      62.36\% &      67.78\% & 89.74\% &          92.02\% &          73.76\% \\
C35-C38 &      66.05\% &      63.24\% & 89.61\% &           90.0\% &          70.99\% \\
\bottomrule
\hline
\end{tabular}
}
    \caption{Accuracy of the comparable models in single type defect and mixed type defects.}
    \label{tab:per-type-defect}
\end{table}



\begin{table}[]
    \centering
    \resizebox{.7\textwidth}{!}{%
\begin{tabular}{cccccc}
\toprule
  Class & Takeshi CNN & Kiryong CNN & DC Net & Distilled Net-1 & Distilled Net-2 \\
  \hline
\midrule
     C1 &      89.89\% &      70.76\% &  99.7\% &          99.51\% &          100.0\% \\
     C2 &      79.87\% &      56.47\% &  97.8\% &          98.32\% &          86.59\% \\
     C3 &      81.59\% &      55.69\% &  96.5\% &          97.71\% &          87.61\% \\
     C4 &      84.22\% &      67.36\% &  94.4\% &          94.95\% &           70.2\% \\
     C5 &      92.11\% &      70.08\% &  99.8\% &          99.53\% &          90.23\% \\
     C6 &      94.74\% &      74.21\% &  93.8\% &          96.89\% &          85.49\% \\
     C7 &       82.9\% &      64.32\% &  95.8\% &          81.25\% &          65.62\% \\
     C8 &      94.74\% &      66.92\% &  93.4\% &          98.77\% &          86.42\% \\
     C9 &      85.53\% &      63.76\% & 100.0\% &           93.1\% &          82.07\% \\
    C10 &      88.16\% &      58.41\% &  99.2\% &          93.14\% &          65.69\% \\
    C11 &      75.01\% &      74.54\% &  97.9\% &          98.41\% &          89.95\% \\
    C12 &      92.11\% &      60.18\% &  98.5\% &          93.69\% &          79.61\% \\
    C13 &      68.48\% &      73.24\% &  96.7\% &          98.56\% &          81.82\% \\
    C14 &      90.79\% &      67.46\% &  99.3\% &          94.68\% &          71.81\% \\
    C15 &      73.69\% &      68.78\% &  96.1\% &          93.52\% &          84.72\% \\
    C16 &      69.48\% &      61.99\% &  98.3\% &          91.87\% &          79.43\% \\
    C17 &      78.96\% &      58.44\% &  92.8\% &          94.39\% &          79.08\% \\
    C18 &      72.38\% &      55.06\% &  93.9\% &          92.92\% &           67.7\% \\
    C19 &      70.79\% &      57.99\% &  92.3\% &          88.89\% &           71.3\% \\
    C20 &      60.79\% &      70.45\% &  94.6\% &          96.69\% &          83.98\% \\
    C21 &      64.22\% &      72.98\% &  90.7\% &          98.54\% &          91.26\% \\
    C22 &      61.86\% &      67.85\% &  90.3\% &           94.0\% &           84.0\% \\
    C23 &      63.42\% &      64.69\% &  88.9\% &          86.21\% &          54.19\% \\
    C24 &      64.49\% &      73.06\% &  89.4\% &          96.95\% &          74.37\% \\
    C25 &      52.11\% &      94.69\% &  91.4\% &          91.89\% &          75.68\% \\
    C26 &      64.22\% &      61.61\% &  92.5\% &          98.32\% &          84.36\% \\
    C27 &       62.9\% &      56.29\% &  90.5\% &          97.09\% &          81.07\% \\
    C28 &      67.12\% &      74.16\% &  88.3\% &          87.11\% &          61.34\% \\
    C29 &      54.22\% &      55.31\% &  90.5\% &          89.25\% &          61.21\% \\
    C30 &      67.12\% &      57.48\% &  92.3\% &          92.11\% &          75.44\% \\
    C31 &       65.8\% &      70.78\% &  91.5\% &          95.54\% &          84.16\% \\
    C32 &      63.17\% &      65.11\% &  88.3\% &          92.67\% &          83.25\% \\
    C33 &      50.27\% &      70.67\% &  86.2\% &           84.1\% &          56.41\% \\
    C34 &      73.69\% &      73.79\% &  89.0\% &          92.31\% &          87.18\% \\
    C35 &      63.17\% &      60.43\% &  87.0\% &          86.87\% &          60.61\% \\
    C36 &       56.6\% &      56.32\% &  90.6\% &           89.9\% &          77.88\% \\
    C37 &      69.75\% &       63.4\% &  86.4\% &          84.72\% &          51.85\% \\
    C38 &      65.33\% &      67.39\% &  88.2\% &          92.04\% &          77.11\% \\
\hline
Average &      \textbf{72.52\%} &      \textbf{65.85\%} & \textbf{93.23\%} &          \textbf{93.33\%} &          \textbf{77.12\%} \\
\hline
\bottomrule
\end{tabular}
}

    \caption{Accuracy of the distilled models compared with state-of-the-art models.}
    \label{tab:accuracies}
\end{table}

\section{Discussion}
To further evaluate the effectiveness of the training, we visualize the embeddings of test wafers produced by the teacher and both the student models. In each visualization plot, the defect class is represented by a unique color. As is shown in Figure \ref{fig:tsne}, the embeddings produced by bigger distilled net resemble well the separation achieved by the teacher net. This is also apparent in the t-SNE visualization of the pair. The smaller distilled network, though limited greatly by its size, also achieves reasonable separation. This demonstrates graphically the learning capability of the student network to mimic the structure of the teacher network in the latent space. Future work would be to positively apply other distillation schemes to train lightweight DPR models that can be used efficiently in the fabrication process.

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{images/umap_tsne.pdf}
    \caption{(Top) U-MAP visualization of the embeddings produced by (left to right) Teacher model, Distilled model-1 and Distilled model 2. (Bottom) t-SNE visualization of the embeddings produced by (left to right) Teacher model, Distilled model-1 and Distilled model 2.}
    \label{fig:tsne}
\end{figure}
\section{Conclusion}
In this study, we discuss the identification of the mixed-type defective pattern produced during the wafer manufacturing process. Correctly identifying and classifying these patterns gives valuable insights into the manufacturing operation, especially during the productivity improvement phase. The contribution of this article can be summarized as follows:
\begin{itemize}
    \item This study introduces an unsupervised form of knowledge distillation that benefits lightweight models to achieve great accuracy compared to models trained from scratch. Results clearly show that distilled models benefit the accuracy of mixed-type defect pattern recognition(DPR) of wafer maps without using ground-truth labels.
    \item Compare with conventional DPR works, the distilled models are lighter, and easily deployable on low-memory devices without compromising the performance. In fact, the distilled model may sometimes perform better than the teacher model it is trained from.
\end{itemize}
Taking all points into account, future work will be worthwhile to apply the proposed method to detect the root causes of defects. Besides, wafer defect modeling and analysis to improve wafer fabrication quality will be explored in further research.

\begin{thebibliography}{1}
% \bibliographystyle{IEEEtran}

\bibitem{ref-6}
Liliana, L. (2016). A new model of Ishikawa diagram for quality assessment. IOP Conference Series: Materials Science and Engineering, 161(1), 012099. https://doi.org/10.1088/1757-899X/161/1/012099

\bibitem{ref-5}
G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural network,” NIPS Deep Learning Workshop,
2015.
\bibitem{ref-4}
A. Krizhevsky, I. Sutskever and G. E. Hinton, "ImageNet classification with deep convolutional neural networks", Proc. Adv. Neural Inf. Process. Syst., pp. 1097-1105, 2012.
\bibitem{ref-3}
Y. Kong and D. Ni, "A Semi-Supervised and Incremental Modeling Framework for Wafer Map Classification," in IEEE Transactions on Semiconductor Manufacturing, vol. 33, no. 1, pp. 62-71, Feb. 2020, doi: 10.1109/TSM.2020.2964581.
\bibitem{ref-2}
K. S. -M. Li et al., "Wafer Defect Pattern Labeling and Recognition Using Semi-Supervised Learning," in IEEE Transactions on Semiconductor Manufacturing, vol. 35, no. 2, pp. 291-299, May 2022, doi: 10.1109/TSM.2022.3159246.

\bibitem{ref-1}
R. R. Schaller, "Moore's law: past, present and future," in IEEE Spectrum, vol. 34, no. 6, pp. 52-59, June 1997, doi: 10.1109/6.591665.

\bibitem{ref0}
R. Ufuk Bilsel \& Dennis K.J. Lin (2012) Ishikawa Cause and Effect Diagrams Using Capture Recapture Techniques, Quality Technology \& Quantitative Management, 9:2, 137-152, DOI: 10.1080/16843703.2012.11673282

\bibitem{ref1}
Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E, "ImageNet Classification with Deep Convolutional Neural Networks",  Advances in Neural Information Processing Systems, 2012

\bibitem{ref2}
He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian, "Deep Residual Learning for Image Recognition",  2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 

  
\bibitem{ref3}
S. Liu and W. Deng, "Very deep convolutional neural network based image classification using small training sample size," 2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR), Kuala Lumpur, Malaysia, 2015.

\bibitem{ref4}
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D. \& Riedmiller, M. (2013). Playing Atari with Deep Reinforcement Learning

\bibitem{ref5}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex
Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charlie Beattie, Amir Sadik,
Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-
level control through deep reinforcement learning. Nature, 518:529–533, 2015.

\bibitem{ref6}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference on Neural
Information Processing Systems, page 6000–6010, 2017

\bibitem{ref7}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align
and translate. CoRR, abs/1409.0473, 2015.

\bibitem{ref8}
Cristian Buciluǎ, Rich Caruana, and Alexandru Niculescu-Mizil. 2006. Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD '06). Association for Computing Machinery, New York, NY, USA, 535–541. https://doi.org/10.1145/1150402.1150464

\bibitem{ref9}
Ruth Urner, Shai Shalev-Shwartz, \& Shai Ben-David (2011). Access to Unlabeled Data can Speed up Prediction Time. In Proceedings of the 28th International Conference on Machine Learning (ICML-11) (pp. 641–648). ACM.

\bibitem{ref15}
F. Adly et al., "Simplified Subspaced Regression Network for Identification of Defect Patterns in Semiconductor Wafer Maps," in IEEE Transactions on Industrial Informatics, vol. 11, no. 6, pp. 1267-1276, Dec. 2015, doi: 10.1109/TII.2015.2481719.
\bibitem{ref16}
J. Wang, J. Zhang and X. Wang, "A Data Driven Cycle Time Prediction With Feature Selection in a Semiconductor Wafer Fabrication System," in IEEE Transactions on Semiconductor Manufacturing, vol. 31, no. 1, pp. 173-182, Feb. 2018, doi: 10.1109/TSM.2017.2788501.

\bibitem{ref17}
T. Nakazawa and D. V. Kulkarni, "Wafer Map Defect Pattern Classification and Image Retrieval Using Convolutional Neural Network," in IEEE Transactions on Semiconductor Manufacturing, vol. 31, no. 2, pp. 309-314, May 2018, doi: 10.1109/TSM.2018.2795466.

\bibitem{ref18}
K. Kyeong and H. Kim, "Classification of Mixed-Type Defect Patterns in Wafer Bin Maps Using Convolutional Neural Networks," in IEEE Transactions on Semiconductor Manufacturing, vol. 31, no. 3, pp. 395-402, Aug. 2018, doi: 10.1109/TSM.2018.2841416.
\bibitem{ref19}
X. Wang, K. C. K. Chan, K. Yu, C. Dong and C. C. Loy, "EDVR: Video Restoration With Enhanced Deformable Convolutional Networks," 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), Long Beach, CA, USA, 2019, pp. 1954-1963, doi: 10.1109/CVPRW.2019.00247.

\bibitem{ref20}
F. Tung and G. Mori, "Similarity-Preserving Knowledge Distillation," 2019 IEEE/CVF International Conference on Computer Vision (ICCV), Seoul, Korea (South), 2019, pp. 1365-1374, doi: 10.1109/ICCV.2019.00145.


\bibitem{ref22}
Bagherinezhad, H., Horton, M., Rastegari, M., Farhadi, A.: Label refinery:
Improving imagenet classification through label progression.(2018)
\bibitem{ref23}
Dong, B., Hou, J., Lu, Y., Zhang, Z.: Distillation ~ early stopping? harvesting
dark knowledge utilizing anisotropic information retrieval for overparameterized
neural network. (2019)
\bibitem{ref24}
Furlanello, T., Lipton, Z., Tschannen, M., Itti, L., Anandkumar, A.: Born again
neural networks. In: International Conference on Machine Learning. pp. 1607–1616.
PMLR (2018)

\bibitem{ref25}
N. Aloysius and M. Geetha, "A review on deep convolutional neural networks," 2017 International Conference on Communication and Signal Processing (ICCSP), Chennai, India, 2017, pp. 0588-0592, doi: 10.1109/ICCSP.2017.8286426.

\bibitem{ref26}
Z. Li et al., "A Unified Understanding of Deep NLP Models for Text Classification," in IEEE Transactions on Visualization and Computer Graphics, vol. 28, no. 12, pp. 4980-4994, 1 Dec. 2022, doi: 10.1109/TVCG.2022.3184186.
\bibitem{ref40}
J. Wang, C. Xu, Z. Yang, J. Zhang and X. Li, "Deformable Convolutional Networks for Efficient Mixed-Type Wafer Defect Pattern Recognition," in IEEE Transactions on Semiconductor Manufacturing, vol. 33, no. 4, pp. 587-596, Nov. 2020, doi: 10.1109/TSM.2020.3020985.
\end{thebibliography}
\end{document}
