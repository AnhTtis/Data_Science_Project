{
    "arxiv_id": "2303.14991",
    "paper_title": "Empowering Dual-Encoder with Query Generator for Cross-Lingual Dense Retrieval",
    "authors": [
        "Houxing Ren",
        "Linjun Shou",
        "Ning Wu",
        "Ming Gong",
        "Daxin Jiang"
    ],
    "submission_date": "2023-03-27",
    "revised_dates": [
        "2023-03-28"
    ],
    "latest_version": 1,
    "categories": [
        "cs.IR"
    ],
    "abstract": "In monolingual dense retrieval, lots of works focus on how to distill knowledge from cross-encoder re-ranker to dual-encoder retriever and these methods achieve better performance due to the effectiveness of cross-encoder re-ranker. However, we find that the performance of the cross-encoder re-ranker is heavily influenced by the number of training samples and the quality of negative samples, which is hard to obtain in the cross-lingual setting. In this paper, we propose to use a query generator as the teacher in the cross-lingual setting, which is less dependent on enough training samples and high-quality negative samples. In addition to traditional knowledge distillation, we further propose a novel enhancement method, which uses the query generator to help the dual-encoder align queries from different languages, but does not need any additional parallel sentences. The experimental results show that our method outperforms the state-of-the-art methods on two benchmark datasets.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.14991v1"
    ],
    "publication_venue": "EMNLP 2022 main conference"
}