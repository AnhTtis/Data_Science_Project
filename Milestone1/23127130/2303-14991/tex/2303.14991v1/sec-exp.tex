\section{Experiments} \label{sec:exp}

In this section, we construct experiments to demonstrate the effectiveness of our method.

\subsection{Experimental Setup}

\paratitle{Datasets.} We evaluate the proposed method on two public cross-lingual retrieval datasets: XOR-Retrieve~\cite{asai2020xor} and MKQA~\cite{longpre2020mkqa}. The detailed descriptions of the two datasets are presented in Appendix~\ref{sec:dataset}.

\paratitle{Evaluation Metrics.} Following previous works \cite{asai2020xor, soro2022ask}, we use R@2kt and R@5kt as evaluation metrics for the XOR-Retrieve dataset and R@2kt as evaluation metrics for the MKQA dataset. The metrics measure the proportion of queries to which the top k retrieved tokens contain the span answer, which is fairer with different passage sizes.

\paratitle{Implementation Details.} For the warm-up training stage, we follow XOR-Retrieve to first train the model on NQ~\cite{kwiatkowski2019natural} data and then fine-tune the model with XOR-Retrieve data. For the iteratively training stage, we generate seven queries for each case~(because the XOR-Retrieve data contains seven languages). We set the number of retrieved passages as 100, the number of iterations as $5$, threshold $T$ in Eq.~\eqref{eq:thresh} as 0.3 and coefficient $\alpha$ in Eq.~\eqref{eq:final} as 0.5. The detailed hyper-parameters are shown in Appendix~\ref{sec:param}. And we conduct more experiments to analyze the parameter sensitivity in Appendix~\ref{sec:sens}.

All the experiments run on 8 NVIDIA Tesla A100 GPUs. The implementation code is based on HuggingFace Transformers~\cite{wolf2020transformers}. For the dual-encoder, we use XLM-R Base~\cite{conneau2019unsupervised} as the pre-trained model and use the average hidden states of all tokens to represent the sentence. For the query generator, we leverage mT5 Base~\cite{xue2020mt5} as the pre-trained model, which has almost the same number of parameters as a large cross-encoder.

\begin{table*}[t] \footnotesize
\centering
\setlength\tabcolsep{4pt}
\caption{Comparison results on XOR-Retrieve dev set. The best results are in bold. ``$\ast$'' denotes the results are copied from the source paper. Results unavailable are left blank.}
\begin{tabular}{l|ccccccc|c|ccccccc|c} \toprule
    \multirow{2.5}{*}{Methods} & \multicolumn{8}{c|}{R@2kt} & \multicolumn{8}{c}{R@5kt} \\ \cmidrule(lr){2-17}
    ~ & Ar & Bn & Fi & Ja & Ko & Ru & Te & Avg & Ar & Bn & Fi & Ja & Ko & Ru & Te & Avg \\ \midrule
    mDPR$^{\ast}$ & 38.8 & 48.4 & 52.5 & 26.6 & 44.2 & 33.3 & 39.9 & 40.5 & 48.9 & 60.2 & 59.2 & 34.9 & 49.8 & 43.0 & 55.5 & 50.2 \\
    DPR + MT$^{\ast}$ & 43.4 & 53.9 & 55.1 & 40.2 & 50.5 & 30.8 & 20.2 & 42.0 & 52.4 & 62.8 & 61.8 & 48.1 & 58.6 & 37.8 & 32.4 & 50.6 \\ \midrule
    Sentri$^{\ast}$ & 47.6 & 48.1 & 53.1 & 46.6 & 49.6 & 44.3 & 67.9 & 51.0 & 56.8 & 62.2 & 65.5 & 53.2 & 55.5 & 52.3 & 80.3 & 60.8 \\
    ~~~ w/ Bi-Encoder$^{\ast}$ & 47.8 & 39.1 & 48.9 & 51.2 & 40.2 & 41.2 & 49.4 & 45.4 & 55.1 & 43.3 & 59.5 & 59.4 & 51.2 & 52.0 & 56.9 & 53.9 \\ \midrule
    DR.DECR$^{\ast}$ & - & - & - & - & - & - & - & 66.0 & 70.2 & \textbf{85.9} & 69.4 & 65.1 & 68.8 & 68.8 & 83.2 & 73.1 \\
    ~~~ w/o KD$_{XOR}^{\ast}$ & - & - & - & - & - & - & - & 60.6 & - & - & - & - & - & - & - & 68.6 \\
    ~~~ w/o KD$_{PC}^{\ast}$ & - & - & - & - & - & - & - & 56.6 & - & - & - & - & - & - & - & 63.6 \\\midrule
    \name & 52.8 & 70.1 & 62.2 & 54.8 & 62.8 & 57.8 & 70.6 & 61.3 & 63.8 & 78.0 & 65.3 & 63.5 & 69.8 & 67.1 & 74.8 & 68.9 \\ 
    \name w/ LaBSE & \textbf{67.3} & \textbf{78.9} & \textbf{65.9} & \textbf{59.8} & \textbf{66.3} & \textbf{63.7} & \textbf{80.7} & \textbf{68.9} & \textbf{72.2} & 83.2 & \textbf{69.7} & \textbf{68.0} & \textbf{70.9} & \textbf{71.7} & \textbf{84.9} & \textbf{74.4} \\ \bottomrule
\end{tabular}
\label{tab:xor-dev}
\end{table*}

\begin{table}[t] \small
\centering

\caption{Comparison results on XOR-Retrieve test set. }
\begin{tabular}{l|cc} \toprule
    Methods & R@2kt & R@5kt \\ \midrule
    GAAMA & 52.8 & 59.9 \\
    Sentri & 52.7 & 61.0 \\
    CCP & 54.8 & 63.0 \\
    Sentri 2.0 & 58.5 & 64.6 \\
    DR.DECR & 63.0 & 70.3 \\
    \name w/ LaBSE & \textbf{65.6} & \textbf{72.0} \\ \bottomrule
\end{tabular}
\label{tab:xor-test}
\end{table}

\subsection{Results}

\paratitle{Baselines.} We compare the proposed \name with previous state-of-the-art methods, including mDPR, DPR+MT~\cite{asai2020xor}, Sentri~\cite{soro2022ask}, DR.DECR~\cite{li2021learning}. Note that Sentri introduces a shared encoder with large size, DR.DECR introduces parallel queries and parallel corpus, but our method only utilizes an encoder with base size, XOR-Retrieve and NQ training data. For more fairly comparison, we also report their ablation results. Here, ``Bi-Encoder'' denotes two unshared encoders with base size. ``KD$_{XOR}$'' denotes a distillation method which introduces synonymous English queries. ``KD$_{PC}$'' denotes a distillation method which introduces parallel corpus. 
In addition, we also employ LaBSE base~\cite{feng2022language} to evaluate the proposed \name with parallel corpus, which is a state-of-the-art model pre-trained with parallel corpus.

\paratitle{XOR-Retrieve.}
Table~\ref{tab:xor-dev} shows the results on XOR-Retrieve dev set. The proposed \name outperforms mDPR, DPR+MT, and Sentri with a clear edge in almost all languages. Although \name does not introduce any parallel corpus, it also outperforms DR.DECR w/o KD$_{XOR}$. Finally, \name based on LaBSE outperforms all baselines, especially DR.DECR w/o KD$_{XOR}$, and even outperforms DR.DECR which utilizes both parallel queries and parallel corpus. Note that knowledge distillation with parallel corpus in DR.DECR is designed for cross-lingual dense retrieval, but LaBSE is a general pre-trained model for all cross-lingual tasks. These results show the effectiveness of the proposed \name. Our method combines two methods in dense retrieval and cross-lingual tasks, namely distillation and alignment. We further analyze the contribution of each component in Section~\ref{sec:abla}.

In addition, we show the results on XOR-Retrieve test set in Table~\ref{tab:xor-test}, which is copied from the leaderboard\footnote{\url{https://nlp.cs.washington.edu/xorqa}} on June 15, 2022. As we can see, our method achieves the top position on the leaderboard of XOR-Retrieve.

\begin{table}[t] \small
\centering

\caption{Average performance of 20 unseen languages in MKQA test set. ``$\ast$'' denotes the results are copied from the Sentri paper.}
\begin{tabular}{l|c} \toprule
    Methods & R@2kt \\ \midrule
    CORA$^{\ast}$ & 41.1 \\
    BM25 + MT$^{\ast}$ & 42.0 \\
    Sentri$^{\ast}$ & 53.3 \\
    ~~~ w/ Bi-Encoder$^{\ast}$ & 45.3 \\ \midrule
    \name & 53.4 \\
    \name w/ LaBSE & \textbf{60.3} \\ \bottomrule
\end{tabular}
\label{tab:mkqa}
\end{table}

\paratitle{MKQA.}
Furthermore, we evaluate the zero-shot performance of our method on the MKQA test set. Following previous works~\cite{soro2022ask}, we directly evaluate the dual-encoder training on XOR-Retrieve data and report the performance of unseen languages on MKQA. As shown in Table~\ref{tab:mkqa}, our method outperforms all baselines and even performs better than Sentri. Note that Sentri uses a shared encoder with large size. The comparison between Sentri and Sentri w/ Bi-Encoder shows that the large encoder has better transfer ability. Finally, the proposed \name w/ LaBSE outperforms all baselines with a clear edge.  It shows the better transfer ability of our methods.

\begin{table}[t] \small
\centering

\caption{Ablation results on XOR-Retrieve dev set. }
\begin{tabular}{l|cc} \toprule
    Methods & R@2kt & R@5kt \\ \midrule
    \name & \textbf{61.3} & \textbf{68.9} \\
    w/o Sampling & 59.5 & 67.5 \\
    w/o Alignment & 59.9 & 67.1 \\
    w/o Generation & 58.8 & 65.9 \\ 
    w/o All & 41.5 & 53.4 \\ \bottomrule
\end{tabular}
\label{tab:ablation}
\end{table}

\subsection{Methods Analysis} \label{sec:abla}

\paratitle{Ablation Study.} 
Here, we check how each component contributes to the final performance. We construct the ablation experiments on XOR-Retrieve data. We prepare four variants of our method: 
(1) \uline{w/o Sampling} denotes without the scheduled sampling but keep the threshold $T$ for $c'$, \aka if $c' \ge T$, then $c' = 1$, otherwise $c' = 0$;
(2) \uline{w/o Alignment} denotes without $\mathcal{L}_A$ in Eq.~\eqref{eq:final};
(3) \uline{w/o Generation} denotes without $\mathcal{L}_D'$ and $\mathcal{L}_A$ in Eq.~\eqref{eq:final};
(4) \uline{w/o All} denotes without the enhanced training, \aka the warm-up dual-encoder.

Table~\ref{tab:ablation} presents all comparison results of the four variants. 
As we can see, the performance rank of R@5kt can be given as: w/o All < w/o Generation < w/o Alignment < w/o Sampling < \name. These results indicate that all components are essential to improve performance. And we can find the margin between w/o Alignment and w/o Sampling is small, it denotes that the generated queries are noisy and demonstrate the effectiveness of our schedule sampling strategy.

\paratitle{Effect of Alignment.}
As we mentioned in Section~\ref{sec:intro}, the alignment established in the pre-training stage may be damaged without any constraint in the fine-tuning stage. Here, we construct experiments on both XLM-R and LaBSE to analyze the effectiveness of the proposed alignment training. As shown in Table~\ref{tab:align}, the proposed alignment training is effective based on the two models. It indicates that the alignment constraint in the fine-tuning stage is effective for models which pre-trained with parallel corpus. And we find that the gains of alignment training based on XLM-R are larger than LaBSE, which shows that the alignment constraint is more effective for models which do not pre-trained with parallel corpus.

\begin{table}[t] \small
\centering
\setlength\tabcolsep{4pt}

\caption{Effect of alignment based on different pre-trained languages models.}
\begin{tabular}{l|cc|cc} \toprule
    \multirow{2.5}{*}{Methods} &\multicolumn{2}{c|}{XLM-R} & \multicolumn{2}{c}{LaBSE} \\ \cmidrule(lr){2-5}
    ~ & R@2kt & R@5kt & R@2kt & R@5kt \\ \midrule
    \name & \textbf{61.3} & \textbf{68.9} & \textbf{68.9} & \textbf{74.4} \\
    ~~~ w/o Alignment & 59.9 & 67.1 & 67.9 & 73.8 \\ \bottomrule
\end{tabular}
\label{tab:align}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.78\columnwidth]{figures/rerank.pdf}
    \caption{Re-ranking performance of cross-encoder and query generator on XOR-Retrieve dev set with different numbers of candidate passages.}
    \label{fig:depth}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\columnwidth]{figures/iteration.pdf}
    \caption{The changes of R@2kt during the iteratively training on XOR-Retrieve dev set. Here, ``QG'' denotes Query Generator and ``CE'' denotes Cross Encoder.}
    \label{fig:iteration}
\end{figure}

\paratitle{Cross-Encoder versus Query Generator.} Here, we analyze the re-ranking ability of cross-encoder and query generator. Here, we use the warm-up dual-encoder to retrieve passages, vary the number of candidate passages, and then evaluate the re-ranked result. As shown in Figure~\ref{fig:depth}, when we use the top-100 candidate passages, the performance of the cross-encoder and generator is almost the same. But as the number of candidate passages increases, especially when it surpasses 500, the gap between the performance of the cross-encoder and query generator gradually becomes larger. It shows low generalization performance of the cross-encoder when there are not enough training samples.

\paratitle{Visualization of the Training Procedure.} We visualize the performance changes of R@2kt during the training of both dual-encoder and query generator re-ranker which re-ranks the retrieved top-100 passages. We also incorporate a cross-encoder~(initialized with XLM-R Large) to perform distillation and re-ranking for comparison. As shown in Figure~\ref{fig:iteration}, the R@2kt of all models gradually increases as the iteration increases. While the training advances closer to convergence, the improvement gradually slows down. In the end, the performance of the dual-encoder is improved by approximately 17\%, and the performance of the query generator is improved by approximately 20\%. 
Finally, comparing the performance of the cross-encoder and the query generator, we can find that there are approximately 6\% gaps for both teachers and students. It shows the effectiveness of our method. 
