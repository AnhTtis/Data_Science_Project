\section{Introduction} \label{sec:intro}

Information Retrieval~(IR) aims to retrieve pieces of evidence for a given query. 
Traditional methods mainly use sparse retrieval systems such as BM25~\cite{robertson2009probabilistic}, which depend on keyword matching between queries and passages. 
With the development of large-scale pre-trained language models~(PLMs)~\cite{vas2017attention,devlin2018bert} such as BERT, dense retrieval methods~\cite{lee2019latent,karpukhin2020dense} show quite effective performance. 
These methods usually employed a dual-encoder architecture to encode both queries and passages into dense embeddings and then perform approximate nearest neighbor searching~\cite{johnson2019billion}.

Recently, leveraging a cross-encoder re-ranker as the teacher model to distill knowledge to a dual-encoder has shown quite effective to boost the dual-encoder performance. Specifically, these methods first train a warm-up dual-encoder and a warm-up cross-encoder. Then, they perform knowledge distillation from the cross-encoder to the dual-encoder by KL-Divergence or specially designed methods. For example, RocketQAv2~\cite{qu2021rocketqa} proposed dynamic distillation, and AR2~\cite{zhang2021adver} proposed adversarial training.

\begin{figure}[t]
    \centering
    \subfigure[BM25.]{
        \includegraphics[width=0.46\columnwidth]{figures/sparse.pdf}
    }
    \subfigure[DPR.]{
        \includegraphics[width=0.46\columnwidth]{figures/dense.pdf}
    }
    \caption{The performance of cross-encoder and query generator when varying the number of training samples and retrievers. We use BM25 and DPR as retrievers, respectively. For the cross-encoder~(BERT-Large), we use retrieved top-100 passages which do not contain the answer as negative and contrastive loss for training. For the query generator~(T5-Base), we firstly train it with the query generation task and then fine-tune the model with the same setting as BERT-Large. The reported performance is the top-5 score of re-ranked top 500 passages on the NQ test set.}
    \label{fig:re-rank}
\end{figure}

However, there are two major problems when scaling the method to the cross-lingual dense retrieval setting.
Firstly, the cross-encoder typically requires large amounts of training data and high-quality negative samples  due to the gap between pre-training (token-level task) and fine-tuning (sentence-level task), which are usually not satisfied in the cross-lingual setting~\cite{asai2020xor}.
Due to expensive labeling and lack of annotators in global languages, especially low-resource languages, the training data in cross-lingual are quite limited. Then with the limited training data, the dual-encoder is not good enough to provide high-quality negative samples to facilitate the cross-encoder. 
Secondly, the cross-lingual gaps between different languages have a detrimental effect on the performance of cross-lingual models. Although some cross-lingual pre-training methods such as InfoXLM~\cite{chi2020infoxlm} and LaBSE~\cite{feng2022language} have put lots of effort into this aspect by leveraging parallel corpus for better alignment between different languages, these parallel data are usually expensive to obtain and the language alignment could be damaged in the fine-tuning stage if without any constraint.

To solve these problems, we propose to employ a query generator in the cross-lingual setting, which uses the likelihood of a query against a passage to measure the relevance. 
On the one hand, the query generator can utilize pre-training knowledge with small training data in fine-tuning stage, because both of its pre-training and fine-tuning have a consistent generative objective. 
On the other hand, the query generation task is defined over all tokens from the query rather than just the \emph{[CLS] token} in the cross-encoder, which has been demonstrated to be a more efficient training paradigm~\cite{clark2020electra}. As shown in Figure~\ref{fig:re-rank}, with the number of training samples dropping, the performance of BERT-Large drops more sharply than T5-Base. 
Besides, the query generator is less sensitive to high-quality negative samples. As we can see, using BM25 as the retriever to mine negative samples for re-ranker training, the gap between cross-encoder and query generator is smaller than the gap using DPR as the retriever. Finally, the query generator can provide more training data by generation, which is precious in the cross-lingual setting. To sum up, the query generator is more effective than the cross-encoder in the cross-lingual setting.

Based on these findings, we propose a novel method, namely \name, which stands \uline{Qu}ery generator \uline{i}mproved dual-encoder by \uline{C}ross-lingual \uline{K}nowledge distillation. Firstly, at the passage level, we employ a query generator as the teacher to distill the relevant score between a query and a passage into the dual-encoder. Secondly, at the language level, we use the query generator to generate synonymous queries in other languages for each training sample and align their retrieved results by KL-Divergence. Considering the noise in the generated queries, we further propose a scheduled sampling method to achieve better performance.

The contributions of this paper are as follows:
\begin{itemize}
    \item We propose a cross-lingual query generator as a teacher model to empower the cross-lingual dense retrieval model and a novel iterative training approach is leveraged for the joint optimizations of these two models.
    \item On top of the cross-lingual query generator, a novel cost-effective alignment method is further designed to boost the dense retrieval performance in low-resource languages, which does not require any additional expensive parallel corpus.
    \item Extensive experiments on two public cross-lingual retrieval datasets demonstrate the effectiveness of the proposed method.
\end{itemize}
