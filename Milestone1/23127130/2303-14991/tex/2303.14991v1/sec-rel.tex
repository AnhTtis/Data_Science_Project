\section{Related Work}

\paratitle{Retrieval.} 
Retrieval aims to search relevant passages from a large corpus for a given query. Traditionally, researchers use bag-of-words~(BOW) based methods such as TF-IDF and BM25~\cite{robertson2009probabilistic}. These methods use a sparse vector to represent the text, so we call them sparse retrievers. Recently, some studies use neural networks to improve the sparse retriever such as DocTQuery~\cite{nogueira2019doc2query} and DeepCT~\cite{dai2019context}. 

In contrast to sparse retrievers, dense retrievers usually employ a dual-encoder to encode both queries and passages into dense vectors whose lengths are much less than sparse vectors. These methods mainly focus on two aspects: pre-training~\cite{lee2019latent,guu2020realm,lu2021less,gao2021condenser,gao2021unsupervised,zhou2022hyperlink} and fine-tuning methods, including negative sampling~\cite{karpukhin2020dense,luan2020sparse,xiong2020approximate,zhan2021optimizing} and multi-view representations~\cite{khattab2020colbert,humeau2020poly,tang2021improving,zhang2022multi}. 
Another fine-tuning method is jointly training the dual-encoder with a cross-encoder. For example, RDR~\cite{yang2020retriever} and FID-KD~\cite{izacard2020distilling} distill knowledge from a reader to the dual-encoder;
RocketQA~\cite{qu2021rocketqa}, PAIR~\cite{ren2021pair}, RocketQAv2~\cite{ren2021rocketqav2}, and AR2~\cite{zhang2021adver} jointly train the dual-encoder with a cross-encoder to achieve better performance. 

\begin{figure*}[t]
    \centering
    \subfigure[Dual-Encoder.]{
        \includegraphics[width=0.6\columnwidth, page=1]{figures/encoder.pdf}
    }
    \hspace{0.3cm}
    \subfigure[Cross-Encoder.]{
        \includegraphics[width=0.6\columnwidth, page=2]{figures/encoder.pdf}
    }
    \hspace{0.3cm}
    \subfigure[Query Generator.]{
        \includegraphics[width=0.6\columnwidth, page=3]{figures/encoder.pdf}
    }
    \caption{Overview of different model architectures designed for retrieval or re-ranking.}
    \label{fig:encoder}
\end{figure*}

Recently, with the development of cross-lingual pre-trained models~\cite{conneau2019unsupervised}, researchers pay more attention to cross-lingual dense retrieval~\cite{asai2020xor,longpre2020mkqa}. For example, CORA~\cite{asai2021one} leverages a generator to help mine retrieval training data, Sentri~\cite{soro2022ask} proposes a single encoder and self-training, and DR.DECR~\cite{li2021learning} uses parallel queries and sentences to perform cross-lingual knowledge distillation. 

\paratitle{Re-ranking.}
Re-ranking aims to reorder the retrieved passages as the relevant scores. Due to the small number of retrieved passages, re-ranking usually employs high-latency methods to obtain better performance, \eg cross-encoder.
Traditionally, the re-ranking task is heavily driven by manual feature engineering~\cite{guo2016deep,hui2018pacrr}. With the development of pre-trained language models~(\eg BERT), researchers use the pre-trained models to perform re-ranking tasks ~\cite{nogueira2019passage,li2020parade}.
In addition to cross-encoder, researchers also try to apply generator to re-ranking. For example, monoT5~\cite{nogueira2020doc} proposes a prompt-based method to re-rank passages with T5~\cite{raffel2020exploring} and other studies~\cite{santos2020beyond,zhuang2021deep,lesota2021modern} propose to use the log-likelihood of the query against the passage as the relevance to perform the re-ranking task. 

Recently, with the size of pre-trained models scaling up, the generative models show competitive zero-shot and few-shot ability. Researchers start to apply large generative models to zero-shot and few-shot re-ranking. For example, SGPT~\cite{muen2022sgpt} and UPR~\cite{sachan2022improving} propose to use generative models to perform zero-shot re-ranking. P$^3$ Ranker~\cite{hu2022ranker} demonstrates that generative models achieve better performance in the few-shot setting. Note that all of these works are concurrent to our work. Instead of using a query generator as a re-ranker only,  we propose to leverage the query generator as a teacher model to enhance the performance of the cross-lingual dual-encoder. In addition to the traditional knowledge distillation, we further propose a novel cost-effective alignment method to boost the dense retrieval performance in low-resource languages.

