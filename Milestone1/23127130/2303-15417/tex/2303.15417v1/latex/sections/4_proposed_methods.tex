\section{BlurHandNet}
% The overall architecture of our BlurHandNet is shown in Figure~\ref{fig:model}.
Figure~\ref{fig:model} shows the overall architecture of our BlurHandNet.
Our BlurHandNet, which consists of three modules; Unfolder, KTFormer, and Regressor, reconstructs sequential hand meshes from a single blurry hand image.
We describe the details of each module in the following sections.

\subsection{Unfolder}
\noindent\textbf{Unfolding a blurry hand image.}
% Given a single RGB blurry hand image~$\mathbf{I} \in \mathbb{R}^{H \times W \times 3}$, Unfolder outputs feature maps and 3D joint coordinates of three sequential hands, each corresponding to the hand in both ends and the middle of the motion.
Given a single RGB blurry hand image~$\mathbf{I} \in \mathbb{R}^{H \times W \times 3}$, Unfolder outputs feature maps and 3D joint coordinates of the three sequential hands, \ie, temporal unfolding, where each corresponds to the hand from both ends and the middle of the motion.
Here, $H=256$ and $W=256$ denote the height and width of the input image, respectively.
% In this process, useful temporal information could be extracted from a single image.
% This process could extract useful temporal information from a single image, and we note that effectively utilizing them is one of the core ideas of our methods.
The temporal unfolding could extract useful temporal information from a single blurry image, and we note that effectively utilizing them is one of the core ideas of our methods.
% To do this, we first feed the blurry hand image~$\mathbf{I}$ into ResNet50~\cite{he2016deep}, pre-trained on ImageNet~\cite{deng2009imagenet}, to extract the hand feature map~$\mathbf{F} \in \mathbb{R}^{\nicefrac{H}{32} \times \nicefrac{W}{32} \times C}$, where $C=2048$ denotes the channel dimension of $\mathbf{F}$.
To this end, we first feed the blurry hand image~$\mathbf{I}$ into ResNet50~\cite{he2016deep}, pre-trained on ImageNet~\cite{deng2009imagenet}, to extract the \emph{blurry} hand feature map~$\mathbf{F}_{\text{B}} \in \mathbb{R}^{\nicefrac{H}{32} \times \nicefrac{W}{32} \times C}$, where $C=2048$ denotes the channel dimension of $\mathbf{F}_{\text{B}}$.
% Then, we extract three temporal features from a hand feature~$\mathbf{F}$ by employing corresponding independent decoders, as shown in Figure~\ref{fig:model}.
% Then, we predict three temporal features from a blurry hand feature~$\mathbf{F}_{\text{B}}$ by employing corresponding separate decoders as shown in Figure~\ref{fig:model}.
Then, we predict three temporal features from a blurry hand feature~$\mathbf{F}_{\text{B}}$ through corresponding separate decoders, as shown in Figure~\ref{fig:model}.
% As a result, three sequential hand features~$\mathbf{F}_{\text{E1}}$, $\mathbf{F}_{\text{E2}}$, and $\mathbf{F}_{\text{M}}$ with dimension~$\mathbb{R}^{h \times w \times c}$ are obtained, where each of them corresponds to hand at both ends and middle of the motion.
As a result, we obtain three sequential hand features~$\mathbf{F}_{\text{E1}}$, $\mathbf{F}_{\text{E2}}$, and $\mathbf{F}_{\text{M}}$ with dimension~$\mathbb{R}^{h \times w \times c}$, where each corresponds to the hand at both ends and the middle of the motion.
% We note that $h=\nicefrac{H}{4}$, $w=\nicefrac{W}{4}$, and $c=512$ denote the height, width, and channel dimension of each hand feature, respectively.
Here, $h=\nicefrac{H}{4}$, $w=\nicefrac{W}{4}$, and $c=512$ denote the height, width, and channel dimension of each hand feature, respectively.


% Among these 
% Though $\mathbf{F}_{\text{M}}$ can be specified to the hand at the middle frame without ambiguity, we can not identify whether the hand at each end (\ie, $\mathbf{F}_{\text{E1}}$ or $\mathbf{F}_{\text{E2}}$) is from the initial or final frame.
% Similar to the conventional deblurring approaches~\cite{chen2022simple, zamir2022restormer}, the hand feature at the middle of the motion ~$\mathbf{F}_{\text{M}}$ can be specified.
Among the three sequential features, the hand feature at the middle of the motion ~$\mathbf{F}_{\text{M}}$ can be specified as similar to the conventional deblurring approaches~\cite{Nah_2017_CVPR, zamir2022restormer}.
% % However, we can not identify whether the hand at each end (\ie, $\mathbf{F}_{\text{E1}}$ or $\mathbf{F}_{\text{E2}}$) is from the initial or final frame.
However, we can not identify whether the hand at each end (\ie, $\mathbf{F}_{\text{E1}}$ or $\mathbf{F}_{\text{E2}}$) is come from the initial or final location of the motion due to the temporal ambiguity~\cite{Jin_2018_CVPR,purohit2019bringing, Zhang_2020_ACMMM, PAN_2019_CVPR}.
For example, suppose that we obtain the blurry hand image shown in Figure~\ref{fig:temporal_orderingd}.
Then we can not determine whether the blurry hand image comes from the motion of extending or folding.
% we can not identify whether the hand at each end (\ie,
% This is because the direction of the motion cannot be uniquely determined from a blurry image~\cite{Jin_2018_CVPR,purohit2019bringing, Zhang_2020_ACMMM, PAN_2019_CVPR}.
% This is because the direction of the motion cannot be uniquely determined from a single blurry image~\cite{Jin_2018_CVPR,purohit2019bringing, Zhang_2020_ACMMM, PAN_2019_CVPR}, where the temporal ordering information does not exist.
% 
% For example, suppose that we obtain the blurry hand image shown in Figure~\ref{fig:temporal_orderingd}.
% Since the blurry image does not contain temporal ordering information, we can not determine whether the blurry image is from the motion of extending or folding.
% Therefore, enforcing the network to predict hands in a given sequence may restrict the network from producing optimized temporal information for a given motion.
% In that regard, Unfolder outputs hand features from both ends of the motion (\ie, $\mathbf{F}_{\text{E1}}$ and $\mathbf{F}_{\text{E2}}$) without considering temporal order.
% This is because the direction of the motion cannot be uniquely determined from a single blurry image~\cite{Jin_2018_CVPR,purohit2019bringing, Zhang_2020_ACMMM, PAN_2019_CVPR} due to ill-posedness, as shown in Figure~\ref{fig:temporal_ordering}.
% This is because the direction of the motion cannot be uniquely determined from a single blurry image~\cite{Jin_2018_CVPR,purohit2019bringing, Zhang_2020_ACMMM, PAN_2019_CVPR} due to ill-posedness.
% In other words, it is challenging to estimate the correct motion direction from a single blurry hand image as both forward and backward motion produce almost identical blur.
% In other words,
In that regard, Unfolder outputs hand features from both ends of the motion (\ie, $\mathbf{F}_{\text{E1}}$ and $\mathbf{F}_{\text{E2}}$) without considering temporal order.
% 
We note that exploiting the temporal information still benefits without explicitly considering the temporal order, and can further be stably optimized with the training loss introduced in Section~\ref{ssec:training_loss}.
% Therefore, Unfolder outputs hand features from both ends of the motion (\ie, $\mathbf{F}_{\text{E1}}$ and $\mathbf{F}_{\text{E2}}$) without considering temporal order.
% 
% In other words, the $\mathbf{F}_{\text{E1}}$ and $\mathbf{F}_{\text{E2}}$  temporal order.
% 

\input{latex/figures/all_frames/all_frames}

\noindent\textbf{Extracting temporal joint features.}
% From produced three sequential feature maps, we extract corresponding joint features, which contain essential hand articulation information~\cite{moon2022accurate}, helpful in recovering 3D hand meshes.
% From produced three sequential feature maps, we extract corresponding joint features, which contain essential hand articulation information~\cite{moon2022accurate} that helps to recover 3D hand meshes.
From produced three sequential hand features, we extract the corresponding joint features, which contain essential hand articulation information~\cite{moon2022accurate} that helps to recover 3D hand meshes.
% We first project the sequential temporal features~$\mathbf{F}_{\text{E1}}$, $\mathbf{F}_{\text{E2}}$, and $\mathbf{F}_{\text{M}}$ into $dJ$ dimensional feature through $1 \times 1$ convolution layer, and reshape them into 3D heatmaps with the dimension of $\mathbb{R}^{h \times w \times d \times J}$.
% We first project the sequential hand features~$\mathbf{F}_{\text{E1}}$, $\mathbf{F}_{\text{E2}}$, and $\mathbf{F}_{\text{M}}$ into $dJ$ dimensional feature through $1 \times 1$ convolution layer, and reshape them into 3D heatmaps with the dimension of $\mathbb{R}^{h \times w \times d \times J}$.
% Note that $d=32$ is the depth discretization size and $J=21$ is the number of hand joints.
% We first project the sequential hand features~$\mathbf{F}_{\text{E1}}$, $\mathbf{F}_{\text{E2}}$, and $\mathbf{F}_{\text{M}}$ into $dJ$ dimensional feature through $1 \times 1$ convolution layer, and reshape them into 3D heatmaps with the dimension of $\mathbb{R}^{h \times w \times d \times J}$, where $d=32$ is a depth discretization size and $J=21$ is a number of hand joints.
We first project the sequential hand features~$\mathbf{F}_{\text{E1}}$, $\mathbf{F}_{\text{E2}}$, and $\mathbf{F}_{\text{M}}$ into $dJ$ dimensional feature through $1 \times 1$ convolution layer, and reshape them into 3D heatmaps with the dimension of $\mathbb{R}^{J \times h \times w \times d}$, where $d=32$ is a depth discretization size and $J=21$ is a number of hand joints.
% Then, we perform a soft-argmax operation~\cite{sun2018integral} on each heatmap to obtain the 3D joint coordinates of three temporal hands, $\mathbf{J_{\text{E1}}}$, $\mathbf{J_{\text{E2}}}$, and $\mathbf{J_{\text{M}}}$.
Then, we perform a soft-argmax operation~\cite{sun2018integral} on each heatmap to obtain the 3D joint coordinates of three temporal hands, $\mathbf{J_{\text{E1}}}$, $\mathbf{J_{\text{E2}}}$, and $\mathbf{J_{\text{M}}}$ with dimension of $\mathbb{R}^{J \times 3}$.
Using 3D joint coordinates in each temporal hand, we perform grid sampling~\cite{jaderberg2015spatial, moon2022accurate} on the corresponding feature map.
% 
% By doing so, we obtain temporal joint features $\mathbf{F}_{\text{E1}}$, $\mathbf{F}_{\text{E2}}$, and $\mathbf{F}_{\text{M}}$ with a dimension of $\mathbb{R}^{J \times c}$, which enable the following module to exploit temporal information effectively.
By doing so, we obtain temporal joint features $\mathbf{F}_{\text{J}_{\text{E1}}}$, $\mathbf{F}_{\text{J}_{\text{E2}}}$, and $\mathbf{F}_{\text{J}_{\text{M}}}$ with a dimension of $\mathbb{R}^{J \times c}$, which enable the following module to exploit temporal information effectively.
% By doing so, we obtain temporal joint features $\mathbf{F}_{\text{E1}}$, $\mathbf{F}_{\text{E2}}$, and $\mathbf{F}_{\text{M}}$, which enables the following module to exploit temporal information effectively.
% The joint features contain essential hand articulation information~\cite{moon2022accurate}, useful for 3D hand mesh estimation.



\input{latex/sections/figures/tjformer}
% \subsection{Temporal joint Transformer~(TJFormer)}
\subsection{KTFormer}
\label{ssec:ktformer}
% 
\noindent\textbf{Kinematic-temporal positional embedding.}
The illustration of KTFormer is shown in Figure~\ref{fig:tjformer}.
% KTFormer, a Transformer~\cite{vaswani2017attention}-based module, refines the joint feature~$\mathbf{F}_{\text{J}_{\text{E1}}}$, $\mathbf{F}_{\text{J}_{\text{E2}}}$, and $\mathbf{F}_{\text{J}_{\text{M}}}$ by considering the correlation between not only \emph{joints at the same time step} but also \emph{joints at different time steps}.
% KTFormer, a Transformer~\cite{vaswani2017attention}-based module, refines the joint feature~$\mathbf{F}_{\text{J}_{\text{E1}}}$, $\mathbf{F}_{\text{J}_{\text{E2}}}$, and $\mathbf{F}_{\text{J}_{\text{M}}}$ by considering the correlation between not only \emph{joints at the same time step} but also \emph{joints at different time steps}.
KTFormer is the Transformer~\cite{vaswani2017attention}-based module that refines the joint feature~$\mathbf{F}_{\text{J}_{\text{E1}}}$, $\mathbf{F}_{\text{J}_{\text{E2}}}$, and $\mathbf{F}_{\text{J}_{\text{M}}}$ by considering the correlation between not only \emph{joints at the same time step} but also \emph{joints at different time steps}.
To utilize the temporal joint features as an input of Transformer, we first concatenate the three features along the joint dimension, producing $\mathbf{F}_{\text{J}} \in \mathbb{R}^{3J \times c}$.
% Then, a learnable positional embedding is applied to $\mathbf{F}_{\text{J}}$.
% To this end, we employ kinematic and temporal positional embeddings.
Then, a learnable positional embedding, namely kinematic and temporal positional embeddings, is applied to $\mathbf{F}_{\text{J}}$.
%As the kinematic information between joints can represent the spatial relationship~\cite{wan2021encoder}, we employ kinematic positional embedding~$\in \mathbb{R}^{J \times c}$ along the joints dimension.
% The kinematic positional embedding~$\in \mathbb{R}^{J \times c}$ is applied along the joints dimension, and the temporal positional embedding~$\in \mathbb{R}^{3 \times c}$ is applied along the temporal dimension.
The kinematic positional embedding~$\in \mathbb{R}^{J \times c}$ is applied along the joints dimension, while the temporal positional embedding~$\in \mathbb{R}^{3 \times c}$ is applied along the temporal dimension.
%In addition, to utilize temporal information from the previous Unfolder, we adopt temporal positional embedding.
% With three unfolded sequential hands from the previous Unfolder, spatial and temporal positional embedding have dimension~$\mathbb{R}^{J \times c}$ and $\mathbb{R}^{3 \times c}$, respectively.
The kinematic and temporal positional embedding provide relative positions in kinematic and temporal space, respectively.


%As shown in Figure~\ref{fig:temporal_ordering}, two temporal orders (\ie, $\mathbf{F}_{\text{J}_{\text{E1}}} \xrightarrow{} \mathbf{F}_{\text{J}_{\text{C}}} \xrightarrow{} \mathbf{F}_{\text{J}_{\text{E2}}}$ and $\mathbf{F}_{\text{J}_{\text{E2}}} \xrightarrow{} \mathbf{F}_{\text{J}_{\text{C}}} \xrightarrow{} \mathbf{F}_{\text{J}_{\text{E1}}}$) are possible given a single blurry image.
%Despite such ambiguity, our temporal embedding can effectively provide .
%Here, since the temporal ordering is not determined, one may wonder about application of our temporal embedding.
%Different from previous temporal embedding~\cite{goyal2019embedding,torricelli2020weg2vec}, where the order of the temporal information has an extreme effects on learning the temporal relationship, ours learns the temporal relationship robustly without being affected by the order.
%The reason for this is that our unfolding strategy produces \emph{three} temporal information.
%Let us take an example from Figure~\ref{fig:temporal_ordering} with two possible situations: extending fingers~((a)$\xrightarrow{}$(b)$\xrightarrow{}$(c)) and folding fingers~((c)$\xrightarrow{}$(b)$\xrightarrow{}$(a)).
%While relationship between Figure~\ref{fig:temporal_orderinga}-\ref{fig:temporal_orderingb} and Figure~\ref{fig:temporal_orderingc}-\ref{fig:temporal_orderingb} seems different, they share identical properties as they are in neighboring relationship.
%Also, relation between Figure~\ref{fig:temporal_orderinga}-\ref{fig:temporal_orderingc} always considered between both ends of the motion.
%Therefore, regardless of the order, temporal embedding can learn the correlation between temporal information with same interval.
% Furthermore, the two possible situations can provide \emph{twice} more information to temporal embedding to learn the neighboring relationship.


% Different from previous temporal embedding~\cite{goyal2019embedding,torricelli2020weg2vec} where the order of the temporal information has an extreme effects on learning the temporal relationship, ours can learn the relationship between temporal information robustly without being affected by the order of neighboring joint features~$\mathbf{F}_{\text{J}_{\text{E1}}}$ and $\mathbf{F}_{\text{J}_{\text{E2}}}$.
% The core reason for this is our Unfolder which outputs \emph{only} three temporal joint features which correspond to the hands of both ends~($\text{E1}$ and $\text{E2}$) and center~($\text{C}$).
% Specifically, in the sense that $\text{C}-\text{E1}$ and $\text{C}-\text{E2}$ are two hands that exist at the same time interval, $\text{C}-\text{E1}$ and $\text{C}-\text{E2}$ share a similar positional relationship.






\noindent\textbf{Joint feature refinement with self-attention.}
% Afterward, KTFormer performs self-attention within $\mathbf{F}_{\text{J}}$ by extracting query~$\mathbf{q}$, key~$\mathbf{k}$, and value~$\mathbf{v}$ with three $1 \times 1$ linear projection layers.
KTFormer performs self-attention within $\mathbf{F}_{\text{J}}$ by extracting query~$\mathbf{q}$, key~$\mathbf{k}$, and value~$\mathbf{v}$ through three fully-connected layers.
% Analogous to the standard Transformer~\cite{vaswani2017attention}, refined joint features for sequential hands~$\mathbf{F}^{\prime}_{\text{J}_{\text{E1}}}$, $\mathbf{F}^{\prime}_{\text{J}_{\text{E2}}}$, and $\mathbf{F}^{\prime}_{\text{J}_{\text{C}}}$ are obtained as:
% Analogous to the standard Transformer~\cite{vaswani2017attention}, refined joint features for sequential hands~$\mathbf{F}^{\prime}_{\text{J}_{\text{E1}}}$, $\mathbf{F}^{\prime}_{\text{J}_{\text{E2}}}$, and $\mathbf{F}^{\prime}_{\text{J}_{\text{C}}}$ are formulated as follows:
Following the formulation of the standard Transformer~\cite{vaswani2017attention}, refined joint features for sequential hands~$\mathbf{F}^{\prime}_{\text{J}_{\text{E1}}}$, $\mathbf{F}^{\prime}_{\text{J}_{\text{E2}}}$, and $\mathbf{F}^{\prime}_{\text{J}_{\text{M}}}$ are formulated as follows:
\begin{equation}
\text{Att}(\mathbf{q},\mathbf{k},\mathbf{v}) = \text{softmax}(\frac{{\mathbf{q}}{\mathbf{k}}^{T}}{\sqrt{d_{\mathbf{k}}}})\mathbf{v},
\label{eq:1}
\end{equation}
\vspace{-2mm}
\begin{equation}
\mathbf{R} = \mathbf{F}_{\text{J}} + \text{Att}(\mathbf{q},\mathbf{k},\mathbf{v}),
\label{eq:2}
\end{equation}
\vspace{-2mm}
\begin{equation}
\mathbf{F}^{\prime}_{\text{J}_{\text{E1}}},\mathbf{F}^{\prime}_{\text{J}_{\text{E2}}},\mathbf{F}^{\prime}_{\text{J}_{\text{M}}} = \psi(\mathbf{F}_{\text{J}} + \mathrm{MLP}(\mathbf{R})),
\label{eq:3}
\end{equation}
where $d_{\mathbf{k}}=512$ is the feature dimension of the key~$\mathbf{k}$, and $\mathbf{R}$ is the residual feature.
% $\mathrm{MLP}$ denotes multi-layer perceptron, and $\psi$  denotes a dividing operation, which divides features in dimension~$\mathbb{R}^{3J \times c}$ to three $\mathbb{R}^{J \times c}$.
$\mathrm{MLP}$ denotes multi-layer perceptron, and $\psi$  denotes a dividing operation, which separates features in dimension~$\mathbb{R}^{3J \times c}$ to three $\mathbb{R}^{J \times c}$.
% Consequently, three joint features~$\mathbf{F}^{\prime}_{\text{J}_{\text{E1}}}$, $\mathbf{F}^{\prime}_{\text{J}_{\text{E2}}}$, and $\mathbf{F}^{\prime}_{\text{J}_{\text{M}}}$ are obtained by attentively utilizing the kinematic and temporal information of joints.
Consequently, three joint features~$\mathbf{F}^{\prime}_{\text{J}_{\text{E1}}}$, $\mathbf{F}^{\prime}_{\text{J}_{\text{E2}}}$, and $\mathbf{F}^{\prime}_{\text{J}_{\text{M}}}$ are obtained by attentively utilizing kinematic and temporal information.


\subsection{Regressor}
The Regressor produces MANO~\cite{romero2022embodied} shape~(\ie, ${\beta}_{\text{E1}}$, ${\beta}_{\text{E2}}$, and ${\beta}_{\text{M}}$) and pose ~(\ie, ${\theta}_{\text{E1}}$, ${\theta}_{\text{E2}}$, and ${\theta}_{\text{M}}$) parameters, which correspond to sequential hands.
% Then, the pose and shape parameters are forwarded to the MANO layer to produce 3D hand meshes, $\mathbf{V}_{\text{E1}}$, $\mathbf{V}_{\text{E2}}$, and $\mathbf{V}_{\text{M}}$.
% Since pose and shape parameters at different timesteps are obtained in the same manner, let us describe the process of producing those parameters of the middle hand, ${\beta}_{\text{M}}$ and ${\theta}_{\text{M}}$, as a representative procedure. 
% We describe the regression process of the middle hand, \ie, ${\beta}_{\text{M}}$ and ${\theta}_{\text{M}}$, as a representative procedure, and note that the process at different timesteps can be easily obtained in the same manner.
% We describe the regression process of the middle hand (\ie, ${\beta}_{\text{M}}$ and ${\theta}_{\text{M}}$) as a representative procedure, and the process at different timesteps can be easily obtained in the same manner.
We describe the regression process of the middle hand (\ie, ${\beta}_{\text{M}}$ and ${\theta}_{\text{M}}$) as a representative procedure, and note that the process at different timesteps can be obtained in the same manner.
% First, the shape parameter~${\beta}_{\text{M}}$ is obtained by forwarding the hand features~$\mathbf{F}_{\text{M}}$ to a fully-connected layer after global average pooling~\cite{lin2013network}.
First, the shape parameter~${\beta}_{\text{M}}$ is obtained by forwarding the hand feature~$\mathbf{F}_{\text{M}}$ to a fully-connected layer after global average pooling~\cite{lin2013network}.
% Second, the pose parameter~${\theta}_{\text{M}}$ is produced by considering the kinematic correlation between hand joints.
Second, the pose parameter~${\theta}_{\text{M}}$ is obtained by considering the kinematic correlation between hand joints.
% To this end, we first concatenate  joint feature~$\mathbf{F}^{\prime}_{\text{J}_{\text{M}}}$ with the corresponding 3D coordinates~$\mathbf{J_{\text{M}}}$.
To this end, we first concatenate refined joint feature~$\mathbf{F}^{\prime}_{\text{J}_{\text{M}}}$ with corresponding 3D coordinates~$\mathbf{J_{\text{M}}}$.
Then, we flatten the concatenated feature into one-dimensional vector~$\mathbf{f}_{\text{M}} \in \mathbb{R}^{J(c+3)}$.
% Instead of jointly regressing poses of entire joints from~$\mathbf{f}_{\text{M}}$, the Regressor gradually estimates pose for each joint along the hierarchy of hand kinematic tree, following \cite{wan2021encoder}.
Instead of regressing poses of entire joints from~$\mathbf{f}_{\text{M}}$ at once, the Regressor gradually estimates pose for each joint along the hierarchy of hand kinematic tree, following \cite{wan2021encoder}.
% Specifically, to regress the pose of the certain joint, its ancestral joint information~(pose) and $\mathbf{f}_{\text{M}}$ are concatenated and forwarded to a fully-connected layer.
In detail, for a specific joint, its ancestral pose parameters and $\mathbf{f}_{\text{M}}$ are concatenated, and forwarded to a fully-connected layer to regress the pose parameters.
By adopting the same process for both ends, three MANO parameters are obtained from the Regressor.
% Note that MANO parameters are forwarded to the MANO layer to produce 3D hand meshes~$\mathbf{V}_{\text{E1}}$, $\mathbf{V}_{\text{E2}}$, and $\mathbf{V}_{\text{M}}$, where each denotes to meshes at both ends and middle, respectively.
Then, the MANO parameters are forwarded to the MANO layer to produce 3D hand meshes~$\mathbf{V}_{\text{E1}}$, $\mathbf{V}_{\text{E2}}$, and $\mathbf{V}_{\text{M}}$, where each denotes to meshes at both ends and middle, respectively.

% Specifically, $\mathbf{f}_{\text{M}}$ is first forwarded to a fully-connected layer to regress the pose of the \emph{wrist} (root node in the hand kinematic tree).


% For the other joints, we concatenate $\mathbf{f}_{\text{M}}$ and poses of their ancestral joints, then forwarded the concatenated vector to fully-connected layer.
% Conseqeuently, 


% For the other joints, the concatenation of $\mathbf{f}_{\text{M}}$ and regressed poses of ancestral joints is forwarded to fully-connected layer to regress the pose of target joint.


% Starting from estimating the pose parameters of the \emph{root joint (\ie, wrist)} by forwarding flattened vectors of the three timesteps to a fully-connected layer, we gradually concatenate the kinematic information to predict the pose of the next joint.
% Following \cite{wan2021encoder}, we use the ancestor's predicted pose parameters as kinematic information.
% Consequently, three pose parameters are obtained by referencing the kinematic correlation between the joints.


% The Regressor produces MANO~\cite{romero2022embodied} pose~${\theta}_{\text{E1}}$, ${\theta}_{\text{E2}}$, and ${\theta}_{\text{C}}$, and shape parameters~${\beta}_{\text{E1}}$, ${\beta}_{\text{E2}}$, and ${\beta}_{\text{C}}$ for corresponding sequential hands, and those parameters are forwarded to the MANO layer to produce 3D hand meshes.
% Therefore, the final outputs of our BlurHandNet are three hand meshes that exist in both ends and center of motion.
% In order to obtain shape parameters, we feed hand features~$\mathbf{F}_{\text{E1}}$, $\mathbf{F}_{\text{E2}}$, $\mathbf{F}_{\text{C}}$ into a fully-connected layer after the global average pooling~\cite{lin2013network}.
% % Here, camera parameters~${\phi}_{\text{E1}}$, ${\phi}_{\text{E2}}$, and ${\phi}_{\text{C}}$ are also obtained from the same process of obtaining the shape parameters. 
% On the other hand, to obtain pose parameters, we first concatenate the joint features~$\mathbf{F}^{\prime}_{\text{J}_{\text{E1}}}$, $\mathbf{F}^{\prime}_{\text{J}_{\text{E2}}}$, and $\mathbf{F}^{\prime}_{\text{J}_{\text{C}}}$ and their corresponding 3D joint coordinates~$\mathbf{J_{\text{E1}}}$, $\mathbf{J_{\text{E2}}}$, and $\mathbf{J_{\text{C}}}$, respectively.
% Then, we flatten them each into a one-dimensional vector in dimension~$\mathbb{R}^{J(c+3)}$.
% Finally, pose parameters~${\theta}_{\text{E1}}$, ${\theta}_{\text{E2}}$, and ${\theta}_{\text{C}}$ are produced by passing flattened vectors into a fully-connected layer.
% Here, considering the kinematic information between the joints, we further concatenate related information to estimate pose of the certain joint.
% To this end, following \cite{wan2021encoder}, we use 3D joint coordinates of ancestor as relation information to supplement the representation of the certain joint.
% By using 3D joint coordinates of ancestor~\cite{wan2021encoder} as related information, those information can compensate for the insufficient information of the certain joint.
% Here, as an input of a fully-connected layer to predict a certain joint, we concatenate the flattened vector and 3D joint coordinates of its ancestor, following kinematic topology decoder~(KTD)~\cite{wan2021encoder}.
% This is especially effective in our task, as the tip of the hands often contain sparse information due to the blur.
% This is to compensate for the insufficient information due to the blur, especially in the tip of the hands.




\subsection{Training loss}
\label{ssec:training_loss}
%
During the training, a prediction on the middle of the motion can be simply supervised with GT of the middle frame.
On the other hand, it is ambiguous to supervise both ends of motion as the temporal order is not uniquely determined, as shown in Figure~\ref{fig:temporal_ordering}.
%In contrast to center prediction which can be directly supervised with GT of the current frame, each end prediction cannot be directly supervised with GTs of past and future frames as the temporal order of both ends is not determined.
%
%To handle this problem, we employ temporal order-invariant loss that is not affected by GT temporal order~\cite{Rozumnyi2021defmo}.
To resolve such temporal ambiguity during the loss calculation, we propose two items.
% First, we employ \emph{temporal order-invariant loss}, invariant to GT temporal order~\cite{Rozumnyi2021defmo}.
First, we employ \emph{temporal order-invariant loss}, which is invariant to GT temporal order~\cite{Rozumnyi2021defmo}.
% To be specific, the temporal order in our loss function is determined in the direction of minimizing loss functions more, not by the temporal order of GT.
To be specific, the temporal order in our loss function is determined in the direction that minimizes loss functions, not by the GT temporal order.
% Second, we propose to use a \emph{Unfolder-driven temporal ordering}.
% It determines the temporal order based on the output of Unfolder and uses the determined temporal order to supervise outputs of Regressor instead of determining the temporal order of two modules separately.
Second, we propose to use a \emph{Unfolder-driven temporal ordering}.
It determines the temporal order based on the output of Unfolder, then uses the determined temporal order to supervise the outputs of Regressor rather than determining the temporal order of two modules separately.
% The effectiveness of the two items will be demonstrated in the experimental section.
The effectiveness of the two items is demonstrated in the experimental section.

%
%Furthermore, we set matched temporal order when calculating loss for the output of Unfolder and utilize it when calculating loss for the output of Regressor.
%
The overall loss function $\mathcal{L}$ is defined as follows:
\vspace*{-1mm}
\begin{equation}
\begin{split}
\mathcal{L} & = \mathcal{L}_\text{U} + \mathcal{L}_\text{R} \\
 & = \mathcal{L}_\text{U,M} + \mathcal{L}_\text{U,E} + \mathcal{L}_\text{R,M} + \mathcal{L}_\text{R,E},
\end{split}
\end{equation}
%
where $\mathcal{L}_\text{U}$ and $\mathcal{L}_\text{R}$ are loss functions applied to outputs of the Unfolder and the Regressor, respectively.
%
% The subscripts M and E stand for loss functions of the middle prediction and both ends.
The subscripts M and E stand for prediction of the middle and both ends.

%
%For the output of Unfolder, the loss functions $\mathcal{L}_\text{U,C}$ and $\mathcal{L}_\text{U,E}$ are defined as follows:
To supervise outputs of Unfolder, we define $\mathcal{L}_\text{U,M}$ and $\mathcal{L}_\text{U,E}$ as follows:
%
\begin{equation}
\mathcal{L}_\text{U,M} = \mathcal{L}_\text{joint}(\mathbf{J_{\text{M}}}, \mathbf{J^*_{\text{middle}}}),
\end{equation}
\vspace{-6mm}
\begin{alignat}{2}~\label{eq:loss_unfolder_end}
\mathcal{L}_\text{U,E} = \operatorname*{min} (~&\mathcal{L}_{\text{joint}}(\mathbf{J_{\text{E1}}}, \mathbf{J^*_{\text{initial}}}) &+~&~\mathcal{L}_{\text{joint}}(\mathbf{J_{\text{E2}}}, \mathbf{J^*_{\text{final}}}), \nonumber\\
&\mathcal{L}_{\text{joint}}(\mathbf{J_{\text{E1}}}, \mathbf{J^*_{\text{final}}}) &+ ~&~\mathcal{L}_{\text{joint}}(\mathbf{J_{\text{E2}}}, \mathbf{J^*_{\text{initial}}})~),
\end{alignat}
%
where $\mathbf{J^*_{\text{middle}}}$, $\mathbf{J^*_{\text{initial}}}$, and $\mathbf{J^*_{\text{final}}}$ are GT 3D joint coordinates of the middle, initial and final frame, respectively. $\mathcal{L}_{\text{joint}}$ is $L1$ distance between  predicted and GT joint coordinates.
%
%Here, we note that matched temporal order is \emph{forward} if the first term in $\mathrm{min}$ of Eq.~\ref{eq:loss_unfolder_end} is selected as minimum and \emph{backward} otherwise.
The temporal order is determined to \emph{forward} if the first term in $\mathrm{min}$ of Eq.~\ref{eq:loss_unfolder_end} is selected as minimum and \emph{backward} otherwise.
Therefore, our loss function is \emph{invariant to the temporal order of GT}.
%

%
%For the output of Regressor, the loss function $\mathcal{L}_\text{R,C}$ and $\mathcal{L}_\text{R,E}$ are defined as follows: 
To supervise the outputs of the Regressor, we define the loss function $\mathcal{L}_\text{R,M}$ and $\mathcal{L}_\text{R,E}$ as follows: 
%
\begin{equation}
\mathcal{L}_\text{R,M} = \mathcal{L}_{\text{mesh}}(\Theta_\text{M}, \Theta^*_\text{middle}),
\end{equation}
%
\vspace{-6mm}
\begin{alignat}{5}
        \mathcal{L}_\text{R,E}=
          &\mathds{1}_\textbf{f} &~( &~\mathcal{L}_{\text{mesh}}(\Theta_\text{E1}, \Theta^*_\text{initial})  & +~ &\mathcal{L}_{\text{mesh}}(\Theta_\text{E2}, \Theta^*_\text{final}) &~) \nonumber \\
        + &\mathds{1}_\textbf{b} &~( &~\mathcal{L}_{\text{mesh}}(\Theta_\text{E1}, \Theta^*_\text{final})&+~ &\mathcal{L}_{\text{mesh}}(\Theta_\text{E2}, \Theta^*_\text{initial})   &~),
    \label{eq:loss_with_order}
\end{alignat}
% 
where $\mathds{1}_\textbf{f} = 1$ when the temporal order is determined to forward in Eq.~\ref{eq:loss_unfolder_end} otherwise 0, and $\mathds{1}_\textbf{b} = 1 - \mathds{1}_\textbf{f}$. 
In other words, the temporal order of Eq.~\ref{eq:loss_with_order} follows that of Eq.~\ref{eq:loss_unfolder_end}, which we call \emph{Unfolder-driven temporal ordering}.
%_\ast?
$\Theta_\bullet = \{\theta_\bullet, \beta_\bullet\}$ is GT or predicted MANO parameters, where the superscript $^*$ denotes GT.
$\mathcal{L}_{\text{mesh}}$ is the summation of three $L1$ distances between GT and prediction of: 1) MANO parameters 2) 3D joint coordinates obtained by multiplying joint regression matrix to hand mesh 3) 2D joint coordinates projected from 3D joint coordinates. 

% $\Theta_{(\cdot)} = \{\theta_{(\cdot)}, \beta_{(\cdot)}\}$ is GT or predicted MANO paramters.
% $\mathcal{L}_{\text{mesh}}$ is the summation of three $L1$ distances between GT and prediction of: 1) MANO parameters 2) 3D joint coordinates obtained by multiplying joint regression matrix to hand mesh 3) 2D joint coordinates projected from 3D joint coordinates. 

% In detail, we match end predictions of the Unfolder ($\mathbf{J^{3D}_{\text{E1}}}$, $\mathbf{J^{3D}_{\text{E2}}}$) to GT ($\mathbf{J^{3D}_{\text{past}}}$, $\mathbf{J^{3D}_{\text{future}}}$) or ($\mathbf{J^{3D}_{\text{future}}}$, $\mathbf{J^{3D}_{\text{past}}}$), then calculate the $L_\text{U,E}$ as follows:
% %
% \begin{alignat}{2}
%                 \mathcal{L}_\text{U,E} = \operatorname*{min} \{~
%                 &\mathcal{L}_{\text{joint}}(\mathbf{J^{3D}_{\text{E1}}}, \mathbf{J^{3D}_{\text{past}}})   &+~ &~\mathcal{L}_{\text{joint}}(\mathbf{J^{3D}_{\text{E2}}}, \mathbf{J^{3D}_{\text{future}}}), \nonumber\\
%                 &\mathcal{L}_{\text{joint}}(\mathbf{J^{3D}_{\text{E1}}}, \mathbf{J^{3D}_{\text{future}}}) &+~ &~\mathcal{L}_{\text{joint}}(\mathbf{J^{3D}_{\text{E2}}}, \mathbf{J^{3D}_{\text{past}}})~\},
%         \label{eq:minimum_loss}
% \end{alignat}
% %
% where $\mathcal{L}_{\text{joint}}$ is $L1$ distance between GT and predicted 3D joint coordinates.
%
% Here, we note that matched temporal order is \emph{forward} if the first term in the minimum operation is selected as minimum and \emph{backward} otherwise.
%
% For the $\mathcal{L}_\text{R,E}$, we use the matched temporal order from the loss calculation of the Unfolder and the loss is defined as follows: 
% For the $\mathcal{L}_\text{R,E}$, we use the matched temporal order from the $\mathcal{L}_\text{U,E}$ and the loss is defined as follows:

%
% Above loss formulation, we call \emph{arranged} temporal order-invariant loss, prevents matched temporal order from being shuffled while passing through the TJFormer.
%
% The proposed arranged temporal order-invariant loss makes the training process stable and shows better performance.
%














%
%To effectively incorporate temporal information into the proposed BlurHandNet, employing temporal loss is essential.
%
%However, since averaging over time destroys the temporal ordering~\cite{Jin_2018_CVPR}, \ie, forwarding or backwarding in the time domain, imposing training loss for each temporal module is challenging.
%
% Therefore, we design the temporal module without order preference, then calculate the loss through the direction where the loss is minimized~\cite{Rozumnyi2021defmo}.
%
%The problem becomes more arduous in the proposed BlurHandNet, since multiple temporal loss calculations are required during a single forwarding process.
%
% To handle the temporal ordering ambiguity, we propose \emph{arranged} order-invariant loss that is not affected by temporal order while keeping consistency between each temporal module.
%

% Let us denote the input tensor by $x$, the most forefront three temporal modules as $m_p, m_c, m_f$, and the training target for each time step~(past,~current,~future) as $t_p, t_c, t_f$, respectively.
%
% Since temporal ordering ambiguity exists in loss calculation on (past,~future) time step, we calculate the temporal loss through the direction where the loss is minimized to design the order invariant loss~\cite{Rozumnyi2021defmo}.
%
% In other words, temporal loss calculation for the forefront temporal modules is defined as follows:
%
% \begin{equation}
%     \begin{split}
%             \mathcal{L}_\text{temporal} =~\mathcal{L}~(m_c~(x),~&t_c) \\
%         + \operatorname*{min} \{~\mathcal{L}~(&m_p~(x), t_p)+\mathcal{L}~(m_f~(x), t_f) \\
%         \mathcal{L}~(&m_p~(x), t_f)+\mathcal{L}~(m_f~(x), t_p)~\},
%     \end{split}
%     \label{eq:minimum_loss}
% \end{equation}
%
% where $\mathcal{L}\paren{\cdot,\cdot}$ denotes arbitrary type of loss calculation.
%
% Note that \eqref{eq:minimum_loss} means shuffle the order of the outputs if the direction is reversed.
%

% Then, let us denote another three temporal modules behind the forefront one as $m'_p, m'_c, m'_f$, \ie, the temporal modules that take the output of the forefront temporal modules as input.
%
% To keep temporal consistency on final hand mesh estimation, we use \emph{same} direction that is used in forefront temporal modules.
%
% Therefore, the training loss for the following temporal modules is defined as follows:
%
% \begin{equation}
%     \begin{split}
%         \mathcal{L'}_\text{temporal} =~&\mathcal{L}~(m'_c~(x),~t_c) \\
%         &+~\mathds{1}_\textbf{f}~\{~\mathcal{L}~(m'_p~(x), t_p)+\mathcal{L}~(m'_f~(x), t_f)~\} \\
%         &+~\mathds{1}_\textbf{b}~\{~\mathcal{L}~(m'_p~(x), t_f)+\mathcal{L}~(m'_f~(x), t_p)~\},
%     \end{split}
%     \label{eq:loss_with_order}
% \end{equation}
% % 
% where $\mathds{1}_\textbf{b} = 1$ when order shuffling arises in \eqref{eq:minimum_loss} otherwise 0, and $\mathds{1}_\textbf{f} = 1 - \mathds{1}_\textbf{b}$.
%
% We note that the arranged order invariant loss is also employed in the evaluation process.
% how about using the term "each end" rather than "past and future"?