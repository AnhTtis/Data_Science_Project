\section{Experiments}

% suppl
\iffalse
\subsection{Implementation details}
All implementations are done with PyTorch~\cite{paszke2019pytorch}.
We use Adam optimizer~\cite{kingma2014adam} with batch size 48 for our training.
The initial learning rate is set to $1\times 10^{-4}$ and reduced by a factor of 10 at the 10th and 12th epochs.
We train the BlurHandNet for 13 epochs with four NVIDIA 2080 Ti GPUs, and it takes 15 hours.
All other details will be available in our codes.
\fi


\subsection{Datasets and evaluation metrics}

\noindent\textbf{BlurHand.}
% The newly proposed BlurHand is synthesized based on InterHand2.6M dataset, as described in Section~\ref{sec:blurhand_dataset}.
% The BlurHand~(BH) is our newly presented 3D hand pose dataset containing realistic blurry hand images as introduced in Section~\ref{sec:blurhand_dataset}.
The BlurHand~(BH) is our newly presented 3D hand pose dataset containing realistic blurry hand images as introduced in Section~\ref{sec:blurhand_dataset}.
% We utilize this dataset to train and test 3D hand mesh estimation models.
% 
% We train and test the 3D hand mesh estimation networks on its training and testing split, respectively.
We train and test the 3D hand mesh estimation networks on the train and test splits of the BH.
% We also train the state-of-the-art deblurring model~\cite{chen2022simple} on BlurHand and obtain deblurred images.
% 
%In addition, we also adopt the deblurred version of the BlurHand, which we denote as BlurHand-D.
% 
%In detail, the BlurHand-D is obtained by applying the state-of-the-art deblurring model~\cite{chen2022simple} to the BlurHand.
% , where the deblurring model is trained to restore the sharp hand images.
% using the BlurHand following the authors' instruction~\cite{chen2022simple}.
% 
% We denote the collected dataset after applying the deblurring method by BlurHand-D.
%We note that BlurHand-D is also used to train and test the 3D hand mesh estimation methods.
% 

% Table 1
\input{latex/tables/5.3/main_table}

\input{latex/sections/figures/deblur_example}


\noindent\textbf{InterHand2.6M.}
InterHand2.6M~\cite{moon2020interhand2} (IH2.6M) is a recently presented large-scale 3D hand dataset.
It is captured under highly calibrated camera settings and provides accurate 3D annotations for hand images.
% We employ InterHand2.6M as a representative of sharp hand frames, as the hand images in InterHand2.6M do not contain the blur.
% We use a subset of InterHand2.6M for training and testing purposes, where the subset is a set of the third sharp frame in \figref{manufacture_diagram} for each image of BlurHand.
We employ IH2.6M as a representative of sharp hand frames, as the hand images in IH2.6M do not contain blur.
We use a subset of IH2.6M for training and testing purposes, where the subset is a set of the third sharp frame in \figref{manufacture_diagram} for each image of BH.


% For the fair comparison, we use a subset of InterHand2.6M, which consists of images with blur image pair from BlurHand throughout our experiments.
% Note that we use the subset of InterHand2.6M, which consists of images corresponding to the blur image pair from BlurHand throughout our experiments.
% Since the BlurHand is synthesized based on InterHand2.6M, and it contains the sharp counterpart hand image corresponding to the blurry hand frame in the BlurHand.
% 
% 
%Note that the InterHand2.6M includes sharp counterpart hand images of the blurry image in BlurHand, as the BlurHand is synthesized from InterHand2.6M.
% 
%Therefore, rather than using entire training and test samples, we only use those images corresponding to the BlurHand samples as training and test images for a fair comparison.
%  to the model trained on BlurHand,
% 
% Rather than employing entire training and test samples in InterHand2.6M, , for fair comparison.
% 
% Note that we utilize the InterHand2.6M as the sharp counterpart of the BlurHand.
% 
% Note that we use the subset of InterHand2.6M which consists of images correspondingn to the blur image pair from BlurHand throughout our experiments.
% We utilize InterHand2.6M to train and test 3D hand mesh estimation models.


% The BlurHand dataset consists of 121,839 training and 34,057 evaluation samples.
% \jaeha{refer that we used camera-view sampled BlurHand}





\noindent\textbf{YT-3D.}
YouTube-3D-hands~(YT-3D)~\cite{kulon2020weakly} is a 3D hand dataset with diverse and non-laboratory videos collected from youtube.
We utilize the YT-3D as an additional training dataset when testing on YT-3D.
Since YT-3D does not provide 3D GTs, we only provide a qualitative comparison of this dataset without quantitative evaluations.
% The 3D annotations are provided only for some frames, and the frames are sharp images.
% Thus, we manually collected blurry frames in YT-3D and show a qualitative comparison on these frames.




\noindent\textbf{Evaluation metrics.}
We use mean per joint position error (MPJPE) and mean per vertex position error (MPVPE) as our evaluation metrics.
The metrics measure Euclidean distance (mm) between estimated coordinates and groundtruth coordinates.
Before calculating the metrics, we align the translation of the root joint (\ie, wrist).

% \input{latex/tables/5.3/main_table}
\subsection{Ablation study}
% \input{latex/tables/5.3/dataset_effectiveness.tex}

% Table 2
\input{latex/tables/5.3/with_without_deblur}

\input{latex/figures/comparison_yt3d/comparison_yt3d}




\noindent\textbf{Benefit of BlurHand dataset.}
% 
% Table~\ref{table:baseline_performance} shows the usefulness of the presented BlurHand with quantitative results.
% % 
% that 3D hand mesh estimation models trained on sharp images (InterHand2.6M) suffer from severe performance degradation when they are tested on blurry images (BlurHand).
Directly measuring how much synthesized blur is close to the real one is still an open research problem in the deblurring community~\cite{zhang2020deblurring,rim_2022_ECCV}. 
Hence, we justify the usefulness of the presented BlurHand using indirect commonly used protocols~\cite{rim_2022_ECCV}, \ie, train the model with the presented dataset and test it on unseen blurry images.
%Since there is no exact way to calculate whether the blur is realistic, we justify the usefulness of the presented BlurHand both quantitatively and qualitatively.
% 
% First, we observe the performance on BlurHand~(blurry), when the model is trained on InterHand2.6M~(sharp).
% 
% Table~\ref{table:baseline_performance} shows that for all 3D hand mesh recovery networks, networks trained on InterHand2.6M suffer severe performance drops when they are tested on BlurHand, while networks trained on BlurHand perform well.
Table~\ref{table:baseline_performance} shows that all 3D hand mesh recovery networks trained on InterHand2.6M suffer severe performance drops when they are tested on BlurHand, while networks trained on BlurHand perform well.
% 
These experimental results validate that training on BlurHand is necessary when handling the blurry hand.
% In addition, networks trained on BlurHand perform well on InterHand2.6M, which consists of sharp images.
In addition, networks trained on BlurHand also perform well on InterHand2.6M, which consists of sharp images.
This shows the generalizability of our dataset to sharp images.
% 
% We note the models trained BlurHand show comparable results when they are tested on InterHand2.6M, validating that 

% Table~\ref{table:comparison_sota} shows that our BlurHand is more useful than applying deblurring methods~\cite{chen2022simple}.
% The deblurring method~\cite{chen2022simple} is trained on our BlurHand as a pre-trained deblurring network performs badly on our BlurHand.
% The networks trained on either sharp images (IH2.6M) or deblurred images and tested on deblurred images perform worse than networks that are trained and tested on our BlurHand.
% Such comparisons demonstrate the usefulness of our BlurHand dataset compared to utilizing deblurring methods.
% One reason is that as Figure~\ref{fig:deblur_example} shows, deblurring methods often fail due to complicated hand motions.
% Another reason is that deblurring removes temporal information from the blurry image, which is useful for 3D hand mesh recovery.
Table~\ref{table:comparison_sota} shows that utilizing presented BlurHand is more valuable than applying deblurring methods~\cite{chen2022simple}.
The deblurring method~\cite{chen2022simple}, trained on our BlurHand as a pre-trained deblurring network, performs poorly on our BlurHand.
Moreover, the networks trained on sharp images~(IH2.6M) or deblurred images and tested on deblurred images perform worse than those trained and tested on our BlurHand.
Such comparisons demonstrate the usefulness of our BlurHand dataset compared to applying deblurring methods.
One reason is that, as Figure~\ref{fig:deblur_example} shows, deblurring methods often fail to restore sharp hand images due to complicated hand motions.
Another reason is that deblurring removes temporal information from the blurry image, which is helpful for reconstructing accurate 3D hand mesh sequences.


Figure~\ref{fig:comparison_yt3d} provides a qualitative comparison of real-world blurry images in YT-3D, which further demonstrates the usefulness of our BlurHand.
% 
We train BlurHandNet on three different combinations of datasets: 1) InterHand2.6M and YT-3D (\ref{fig:comparision_yt3db}), 2) deblurred BlurHand and YT-3D (\ref{fig:comparision_yt3dc}), and 3) BlurHand and YT-3D (\ref{fig:comparision_yt3dd}).
% The comparison shows that networks trained on our BlurHand produce the most robust 3D hand meshes.
% Such comparisons on real-world blurry images show the generalizability of our BlurHand.
The comparison shows that networks trained on BlurHand produce the most robust 3D meshes, demonstrating the generalizability of BlurHand.
% We report more qualitative results on YT3D in our supplementary materials.




% We further show visual results on YT-3D in Figure~\ref{fig:comparison_yt3d}.
% Specifically, we train our BlurHandNet on three different dataset combinations: 1) InterHand2.6M and YT-3D (\ref{fig:comparision_yt3db}), 2) BlurHand-B and YT-3D (\ref{fig:comparision_yt3dc}), and 3) BlurHand and YT-3D (\ref{fig:comparision_yt3dd}).
% %Specifically, we train our BlurHandNet on three types of dataset combinations: 1) InterHand2.6M and YT-3D (\ref{fig:comparision_yt3db}), 2) BlurHad and YT-3D (\ref{fig:comparision_yt3dc}), and 3) BlurHad and YT-3D (\ref{fig:comparision_yt3dd}).
% % Among the images of the InterHand2.6M, we only use the images that has blur image pair in BlurHand for fair comparision.
% As shown in the Figure~\ref{fig:comparison_yt3d}, the networks can robustly reconstruct 3D hand mesh when trained with BlurHand.
% This demonstrates that the images in our BlurHand contain realistic and challenging blurs.
% We report more qualitative results on YT3D in our supplemantary materials.


% Table~\ref{table:baseline_performance} shows usefulness of the presented BlurHand with quantitative results.
% % Table~\ref{table:baseline_performance} shows that 3D hand mesh estimation model trained on sharp images (InterHand2.6M) suffer from severe performance degradation when they are tested on blurry images (BlurHand).
% % 
% On the other hand, presented BlurHand consistently improves blur-robustness of 3D hand mesh estimation methods on blurry images.
% We further show visual results on YT-3D in Figure~\ref{fig:comparison_yt3d}.
% Specifically, we train our BlurHandNet on three different dataset combinations: 1) InterHand2.6M and YT-3D (\ref{fig:comparision_yt3db}), 2) BlurHand-B and YT-3D (\ref{fig:comparision_yt3dc}), and 3) BlurHand and YT-3D (\ref{fig:comparision_yt3dd}).
% %Specifically, we train our BlurHandNet on three types of dataset combinations: 1) InterHand2.6M and YT-3D (\ref{fig:comparision_yt3db}), 2) BlurHad and YT-3D (\ref{fig:comparision_yt3dc}), and 3) BlurHad and YT-3D (\ref{fig:comparision_yt3dd}).
% % Among the images of the InterHand2.6M, we only use the images that has blur image pair in BlurHand for fair comparision.
% As shown in the Figure~\ref{fig:comparison_yt3d}, the networks can robustly reconstruct 3D hand mesh when trained with BlurHand.
% This demonstrates that the images in our BlurHand contain realistic and challenging blurs.
% We report more qualitative results on YT3D in our supplemantary materials.






% Table 3
\input{latex/tables/5.3/each_modules}
\input{latex/tables/5.3/unfoldingnumber}

\noindent\textbf{Effectiveness of Unfolder and KTFormer.}
Table~\ref{table:ablation_each_modules} shows that using both Unfolder and KTFormer improves 3D mesh estimation accuracy by a large margin.
% As the Unfolder allows a single image to be regarded as three sequential hands, we evaluate hands in both ends and middle of the motion.
As the proposed Unfolder allows a single image to be regarded as three sequential hands, we evaluate hands in both ends and middle of the motion.
Since the temporal order of hand meshes in both ends~(\ie, $\mathbf{V}_{\text{E1}}$ and $\mathbf{V}_{\text{E2}}$) is not determined, we report better MPJPE among the initial-final and final-initial pairs following~\cite{Rozumnyi2021defmo, Argaw_2021_CVPRW}.
% Specifically, solely employing one of Unfolder and KTFormer (the second and third rows) shows a slight improvement over the baseline network, which is designed without any of the proposed module (the first row).
Solely employing one of Unfolder or KTFormer~(the second and third rows) shows a slight improvement over the baseline network, which is designed without any of the proposed modules~(the first row).
% On the other hand, our setting~(the fourth row) shows that the two modules effectively complements each other, which results in great performance boosts.
On the other hand, our BlurHandNet~(the fourth row) results in great performance boosts, by benefiting from the combination of two modules that effectively complement each other.
% In particular, KTFormer can benefit from temporal information, provided by Unfolder.
In particular, KTFormer benefits from temporal information which is provided by Unfolder.
Consequently, introducing both Unfolder and KTFormer, which have strong synergy, consistently improves the 3D errors in all time steps.


% Interestingly, in the point of the baseline~(the first row), using our two proposed modules, Unfolder and KTFormer, leads to more performance gain~(the fourth row) than training and testing a network on deblurred BlurHand~(the fifth row).
In the point of the baseline~(the first row), using our two proposed modules, Unfolder and KTFormer, leads to more performance gain~(the fourth row) than training and testing a network on deblurred BlurHand~(the fifth row).
% This comparison shows that our Unfolder and KTFormer are more effective than using deblurring methods.
This comparison shows that utilizing proposed modules is more effective than using deblurring methods.
% Furthermore, using our two modules when training and testing on deblurred BlurHand~(the last row) does not bring performance gain compared to the deblur baseline~(the fifth row).
% Furthermore, using our two modules does not bring performance gain when training and testing on deblurred BlurHand~(the last row) compared to the deblur baseline~(the fifth row).
Interestingly, using our two modules does not bring performance gain when training and testing on deblurred BlurHand~(the last row) compared to the deblur baseline~(the fifth row).
This validates our statement that deblurring prohibits networks from utilizing temporal information.




\input{latex/figures/comparison_interhand/comparison_interhand2}
\input{latex/figures/attn_vis/attn_vis}



\noindent\textbf{Effect of the number of unfolded hands.}
Table~\ref{table:unfoldingnumber} shows that unfolding more sequential hands further improves the 3D errors.
As our KTFormer utilizes temporal information to enhance the joint feature, the number of hand sequences can affect the overall performance.
Although unfolding a blurry hand into five sequential hands shows the best results, the performance is nearly saturated when a blurry hand is unfolded into three sequential hands.
% Considering the increased computational costs of producing additional hands and the increased temporal input size of KTFormer, we design our Unfolder to produce three sequential hands.
Considering the increased computational costs of producing additional hands and the temporal input size of KTFormer, we design our Unfolder to produce three sequential hands.
We note that our BlurHandNet can be easily extended if more number of unfolding is needed for some applications.

\input{latex/tables/5.3/positional_embedding}
\noindent\textbf{Effect of the kinematic and temporal positional embeddings.}
Table~\ref{table:positional_embedding} shows that our positional embedding setting, which uses both kinematic and temporal positional embedding, achieves the best performance.
We design four variants with different positional embedding settings.
The second and third rows, where either one of kinematic and temporal positional embedding is applied, achieve better results than a baseline without any positional embedding (the first row), but worse results than ours (the last row).
%This is because one of the existing two relative positions~(\ie, kinematic or temporal) is not considered in their settings.
% This indicates that from the point of KTFormer, positional information of both kinematic and temporal dimensions is necessary.
This indicates that positional information of both kinematic and temporal dimensions is necessary for KTFormer.
% Our setting can effectively provide relative position between joints both temporally and kinetically, showing the best results.





% Table~\ref{table:positional_embedding} shows that our positioning embedding strategy results in the best performance.
% We apply kinematic and temporal positional embedding, each with dimension~$\mathbb{R}^{J \times c}$ and $\mathbb{R}^{3 \times c}$, to retain the spatially and temporally positional relationship.
% Since the required positional relation between spatial and temporal tokens may differ, corresponding two types of positional embedding can successfully manage the positional relationship between the tokens.
% \joonkyu{fix later...}
% In addition, ours performs better than the third row in Table~\ref{table:positional_embedding}, which generates positional embedding values for all separate joints in three time steps.
% Here,positional embedding has dimension~$\mathbb{R}^{3J \times c}$.
% By sharing the positional embedding values in temporal hands, which implies similar spatial position relationships, our spatial embedding can learn more spatial information.
% As a result, ours exhibits better performance than the methods using only one kind of positional embedding, as shown in Table~\ref{table:positional_embedding}.


% \noindent\textbf{Effect of KTD.}



\input{latex/tables/5.3/without_minimum}
\noindent\textbf{Effectiveness of the proposed loss functions.}
% Table~\ref{table:minimum} shows that two proposed items in our loss function, \emph{temporal order-invariant loss} and \emph{Unfolder-driven temporal ordering} of Section~\ref{ssec:training_loss}, are necessary for the high performance.
Table~\ref{table:minimum} shows that two proposed items in our loss function, \emph{temporal order-invariant loss} and \emph{Unfolder-driven temporal ordering} as introduced in Section~\ref{ssec:training_loss}, are necessary for the high performance.
%We compare three settings: 1) fixed temporal order loss, 2) temporal order-invariant loss without Unfolder-driven temporal ordering (T w/o U), and 3) temporal order-invariant loss with Unfolder-driven temporal loss (T w/ U).
% The table provides three variants, and for all three settings, we use the same loss function for the middle of the motion.
We compare three variants for loss design at both ends of the motion, while keeping the loss function for the middle of the motion the same.
%The fixed temporal order loss setting means that the prediction of BlurHandNet for each end is supervised by a fixed GT timestep.
%For the second setting, we replace the loss function for the Regressor ~\eqref{eq:loss_with_order} with the form of \eqref{eq:loss_unfolder_end}, \ie independently determine temporal order for the output of the Unfolder and output of the Regressor.
%The third setting corresponds to our proposed training loss.
%As shown in Table~\ref{table:minimum}, our loss function setting achieves the best result for all timesteps.
% The settings without \emph{temporal order-invariant loss} are supervised with 3D meshes following the temporal order of GT instead of determining the order based on Eq.~\ref{eq:loss_unfolder_end}.
In detail, the settings without \emph{temporal order-invariant loss} are supervised with 3D meshes following the GT temporal order instead of determining the order based on Eq.~\ref{eq:loss_unfolder_end}.
On the other hand, the setting without \emph{Unfolder-driven temporal ordering} supervises Regressor with Regressor-driven temporal ordering, which indicates that Unfolder and Regressor can be supervised with different temporal ordering.
% Since inferring the temporal order of GT is ambiguous, the setting without \emph{temporal order-invariant loss} degrades the performance as it strictly follows the temporal order of GT.
Since inferring the temporal order of GT is ambiguous, the setting without \emph{temporal order-invariant loss} degrades the performance as it forces to strictly follows the temporal order of GT.
%fixed temporal order setting degrades the performance of BlurHandNet.
% The Unfolder-driven temporal ordering is beneficial as it provides consistent temporal order to Unfolder and Regressor when calculating loss functions.
Utilizing the proposed \textit{Unfolder-driven temporal ordering} performs the best, as it provides consistent temporal order to both Unfolder and Regressor, making the training stable.
%as it prevents KTFormer from shuffling the determined temporal order.
% Such consistent temporal order makes the training stable and results in better performance of BlurHandNet.




% Rather than enforcing the network to determine the temporal order, which is very challenging due to the temporal ambiguity~\cite{Jin_2018_CVPR}, we let the network decide both ends' order.
% The first row, where the deployed loss follows the global order~(\ie $\mathds{1}_\textbf{f}$ is always set to $1$ in Eqauation~\ref{eq:loss_with_order}), cannot tackle the problem comes from temporal ambiguity.
% On the contrary, our proposed loss effectively addresses the temporal ambiguity by providing flexible temporal ordering to the network, consistently improving metrics in all time steps.

% The first row in Table~\ref{table:minimum} represents using fixed temporal order for unfolded hands.
% Since the temporal order is not clearly defined in a single blurry hand image, it hinders the network from expressing the diverse possibility of temporal order.
% On the contrary, our proposed loss, which adaptively shuffles temporal order, shows better results by letting the network determine the temporal ordering.



\noindent\textbf{Visualization of attention map from KTFormer.}
Figure~\ref{fig:attn_vis} shows a visualized attention map, obtained by the self-attention operation in KTFormer.
%We visualize the attention maps to demonstrate the role of KTFormer in .
%The attention map is obtained from the self-attention scheme of KTFormer.
Here, query and key are obtained from a combination of temporal joint features $\mathbf{F}_{\text{J}_{\text{E1}}}$, $\mathbf{F}_{\text{J}_{\text{C}}}$, and $\mathbf{F}_{\text{J}_{\text{E1}}}$, as described in Section~\ref{ssec:ktformer}.
The figure shows that our attention map produces two diagonal lines, representing a strong correlation between the corresponding query and key.
Specifically, features from both ends of motion,~$\mathbf{F}_{\text{J}_{\text{E1}}}$ and $\mathbf{F}_{\text{J}_{\text{E2}}}$~(the first and third queries), show high correlation with the middle hand feature~$\mathbf{F}_{\text{J}_{\text{M}}}$ (the second key), and $\mathbf{F}_{\text{J}_{\text{M}}}$~(the second query) shows high correlation with $\mathbf{F}_{\text{J}_{\text{E1}}}$ and $\mathbf{F}_{\text{J}_{\text{E2}}}$ (the first and third keys).
This indicates that temporal information is highly preferred to compensate for insufficient joint information in a certain time step.
This is also consistent with the result in Table~\ref{table:ablation_each_modules}.
% The results are also consistent with Table~\ref{table:ablation_each_modules}, meaning that employing the temporal information is essential in handling blurry hands.
% This is also consistent with the result in Table~\ref{table:ablation_each_modules}, meaning that employing the temporal information is crucial in improving the performance.
In the second row of Table~\ref{table:ablation_each_modules}, solely employing KTFormer without Unfolder shows slight performance improvement over the baseline due to the lack of opportunity to exploit temporal information from both ends.
% In short, the diagonal lines created in the attention map represent that certain joint strongly correlates with the same joint at different time steps.
% Specifically, each diagonal line in the first and third row represent that joint features of both ends, $\mathbf{F}_{\text{J}_{\text{E1}}}$ and $\mathbf{F}_{\text{J}_{\text{E1}}}$, bring temporal information from the corresponding joints in the center time step.
% And the two diagonal lines in the second row demonstrates that joint features of center hand strongly exploit information from the neighboring hand's joints.

% We visualize the attention maps to demonstrate the role of KTFormer in Figure~\ref{fig:attn_vis}.
% As shown in Figure~\ref{fig:attn_vis}, temporal information is effectively exchanged by our KTFormer.
% Specifically, features from both ends of motion~$\mathbf{F}_{\text{J}_{\text{E1}}}$ and $\mathbf{F}_{\text{J}_{\text{E2}}}$ show high correlation with the center hand feature~$\mathbf{F}_{\text{J}_{\text{C}}}$, and $\mathbf{F}_{\text{J}_{\text{C}}}$ shows high correlation with $\mathbf{F}_{\text{J}_{\text{E1}}}$ and $\mathbf{F}_{\text{J}_{\text{E2}}}$.
% This indicates that temporal information is highly preferred to compensate for insufficient joint information in the certain time step than spatial information.
% This is also consistent with the result in Table~\ref{table:ablation_each_modules}.
% In the second row of Table~\ref{table:ablation_each_modules}, only adopting spatial self-attention shows slight performance improvement over the baseline.
% Furthermore, the diagonal lines created in the attention map represents that certain joint has strong correlation with the same joint at different time steps.
% Specifically, each diagonal line in the first and third row represent that joint features of both ends, $\mathbf{F}_{\text{J}_{\text{E1}}}$ and $\mathbf{F}_{\text{J}_{\text{E1}}}$, bring temporal information from the corresponding joints in the center time step.
% And the two diagonal lines in the second row demonstrates that joint features of center hand strongly exploit information from the neighboring hand's joints.




\subsection{Comparison with state-of-the-art methods}
Table~\ref{table:baseline_performance} and ~\ref{table:comparison_sota} show that our BlurHandNet outperforms the previous state-of-the-art 3D hand mesh estimation methods in all settings.
As the previous works~\cite{moon2020i2l,lin2021end,moon2022accurate} do not have a special module to address blurs, they fail to produce accurate 3D meshes from blurry hand images.
On the contrary, by effectively handling the blur using temporal information, our BlurHandNet robustly estimates the 3D hand mesh, even under abrupt motion.
Figure~\ref{fig:comparison_interhand2} further shows that our BlurHandNet produces much better results than previous methods on BlurHand.
%In addition, to validate the effectiveness of the proposed approach, we compare the straightforward methods that deploy deblurring before estimating the 3D hand meshes in Table~\ref{table:baseline_performance}.



%In addition, the table shows that for most methods, training and testing a network on deblurred images produces worse results than training and testing a network on our BlurHand.
%Please note that the deblurring method~\cite{chen2022simple} is trained on our BlurHand as pre-trained deblurring network performs bad on our BlurHand.
%This shows benefit of training a network on our BlurHand dataset over applying the deblurring method.
%Since the blur artifacts remain and useful motion information may vanish after deblurring, as shown in Figure~\ref{fig:deblur_example}, the 3D reconstruction accuracy consistently decreases.




% Since the proposed Unfolder effectively unfolds temporal information, then KTFormer actively utilizes that information, ours consistently shows marginal improvement in all metrics.
% Furthermore, to validate effectiveness of the proposed approach, we compare the 



% Table~\ref{table:baseline_performance} shows that our BlurHandNet outperforms the previous state-of-the-art 3D hand mesh estimation methods on BlurHand.
% We also provide results of using the state-of-the art deblur method~\cite{chen2022simple} and the 3D hand mesh estimation methods together.
% In the right side of Table~\ref{table:baseline_performance}, since the blur artifacts are still remain and useful motion information may vanish after deblurring, both MPJPE and MPVPE are consistently decreased.
% The visual results of deblurring is also provided in Figure~\ref{fig:deblur_example}.



% % Since the blur artifacts are still remain and useful motion information vanishes after deblurring, as shown in Figure~\ref{fig:deblur_example}, the 3D hand mesh estimation accuracy are consistently decreased.
% In addition, due to the lack of method to manage the blur, previous methods~\cite{moon2020i2l,moon2022accurate,lin2021end,lin2021mesh} fails to reconstruct accurate 3D meshes from a blurry hand.
% On the contrary, by effectively handling the blur as temporal information, our BlurHandNet consistently exhibits the 3D mesh estimation by large margin.
% Furthermore, we present qualitative comparison with other methods~\cite{lin2021end,moon2022accurate} in Figure~\ref{fig:comparison_interhand2}.
% While \cite{moon2022accurate} and \cite{lin2021end} fail to capture accurate 3D meshes due to the strong motion blur, ours shows robustness to blurs by successfully estimating the accurate 3D meshes even when the blur is severe.
