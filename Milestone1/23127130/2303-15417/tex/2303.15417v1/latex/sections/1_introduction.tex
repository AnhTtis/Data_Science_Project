\section{Introduction}

\begin{figure}[t]
\begin{center}
\vspace{2mm}
\begin{subfigure}{0.93\linewidth}
\centering
\includegraphics[width=1.\linewidth]{latex/figures/overall/blurhandexample.pdf}
\caption{\textbf{Examples of the presented BlurHand dataset.}
\label{fig1-a}}
\end{subfigure}
\\
\vspace{2mm}
% 
\begin{subfigure}{0.93\linewidth}
\centering
\includegraphics[width=1.\linewidth]{latex/figures/overall/unfolding_example_single2.pdf}
\caption{\textbf{Illustration of the temporal unfolding.} \label{fig1-b}}
\end{subfigure}
\end{center}
\vspace{-6mm}
\caption{\textbf{Proposed BlurHand dataset and BlurHandNet.}
%
(a) We present a novel BlurHand dataset, providing natural blurry hand images with accurate 3D annotations.
%
(b) While most previous methods produce a single 3D hand mesh from a sharp image, our BlurHandNet unfolds the blurry hand image into three sequential hand meshes.}
%(b) Different from previous methods, our BlurHandNet unfolds the blurry hand image into three temporal hands.
\vspace{-3mm}
\label{fig:overview}
\end{figure}

% P1: problem -> challenging
Since hand images frequently contain blur when hands are moving, developing a blur-robust 3D hand mesh estimation framework is necessary.
As blur makes the boundary unclear and hard to recognize, it significantly degrades the performance of 3D hand mesh estimation and makes the task challenging.
Despite promising results of 3D hand mesh estimation from a single sharp image~\cite{choi2020pose2mesh, moon2020i2l, kulon2020weakly, lin2021end, lin2021mesh}, research on blurry hands is barely conducted.

% P2: our dataset
A primary reason for such lack of consideration is the absence of datasets that consist of blurry hand images with accurate 3D groundtruth~(GT).
Capturing blurry hand datasets is greatly challenging.
The standard way of capturing markerless 3D hand datasets~\cite{hampali2020honnotate, moon2020interhand2,zimmermann2019freihand} consists of two stages: 1) obtaining multi-view 2D grounds (\eg, 2D joint coordinates and mask) manually~\cite{zimmermann2019freihand} or using estimators~\cite{wei2016convolutional,chen2017rethinking,li2019rethinking} and 2) triangulating the multi-view 2D grounds to the 3D space.
% For the markerless capture, manual annotation or estimators in the first stage are performed from images.
Here, manual annotations or estimators in the first stage are performed from images.
Hence, they become unreliable when the input image is blurry, which results in triangulation failure in the second stage.

Contemplating these limitations, we present the BlurHand, whose examples are shown in Figure~\ref{fig1-a}.  
Our BlurHand, the first blurry hand dataset, is synthesized from InterHand2.6M~\cite{moon2020interhand2}, which is a widely adopted video-based hand dataset with accurate 3D annotations.
Following state-of-the-art blur synthesis literature~\cite{Su_2017_CVPR,Nah_2017_CVPR,Nah_2019_ICCV_Workshops}, we approximate the blurry images by averaging the sequence of sharp hand frames. 
As such technique requires high frame rates of videos, we employ a widely used video interpolation method~\cite{Niklaus_ICCV_2017} to complement the low frame rate~(30 frames per second) of InterHand2.6M. 
We note that our synthetic blur dataset contains realistic and challenging blurry hands.

% P3: deblur->hand estimation
For a given blurry hand image, the most straightforward baseline is sequentially applying state-of-the-art deblurring methods~\cite{chen2022simple,zamir2022restormer,park2022pay,park2022recurrence} on blurry images and 3D hand mesh estimation networks~\cite{moon2020i2l,spurr2018cross,moon2022accurate} on the deblurred image.
However, such a simple baseline suffers from two limitations.
First, since hands contain challenging blur caused by complex articulations, even state-of-the-art deblurring methods could not completely deblur the image.
Therefore, the performance of the following 3D hand mesh estimation networks severely drops due to remaining blur artifacts.
Second, since conventional deblurring approaches only restore the sharp images corresponding to the middle of the motion, it limits the chance to make use of temporal information, which might be useful for 3D mesh estimation.
% Second, since conventional deblurring approaches only restore the sharp images corresponding to the mid-point of the motion, it limits the chance to make use of temporal information, which might be useful for 3D mesh estimation.
In other words, the deblurring process restricts networks from exploiting the motion information in blurry hand images.

To overcome the limitations, we propose BlurHandNet, which recovers a 3D hand mesh sequence from a single blurry image, as shown in Figure~\ref{fig1-b}.
Our BlurHandNet effectively incorporates useful temporal information from the blurry hand.
% Unlike previous methods~\cite{moon2022accurate,lin2021end,kulon2020weakly,park2022handoccnet} that utilize only the given hand's information, our BlurHandNet effectively incorporates useful temporal information from the blurry hand.
The main components of BlurHandNet are Unfolder and a kinematic temporal Transformer~(KTFormer).
Unfolder outputs hand features of three timesteps, \ie, middle and both ends of the motion~\cite{Jin_2018_CVPR, Purohit_2019_CVPR, PAN_2019_CVPR, Rozumnyi2021defmo}.
The Unfolder brings benefits to our method in two aspects.
First, Unfolder enables the proposed BlurHandNet to output not only 3D mesh in the middle of the motion but also 3D meshes at both ends of the motion, providing more informative results related to motion.
We note that this property is especially beneficial for the hands, where the motion has high practical value in various hand-related works.
For example, understanding hand motion is essential in the domain of sign language~\cite{boulares2019sign, Rodríguez2020signdataset} and hand gestures~\cite{Sugimura2016motiongesture}, where the movement itself represents meaning.
% Second, while a hand feature in a single time step already forms a kinematic correlation~\cite{zheng2021poseformer}, further extracting temporal features at multiple time steps enables the following modules to employ temporal information effectively.
Second, extracting features from multiple time steps enables the following modules to employ temporal information effectively.
Since hand features in each time step are highly correlated, exploiting temporal information benefits reconstructing more accurate 3D hand mesh estimation.

To effectively incorporate temporal hand features from the Unfolder, we propose KTFormer as the following module.
The KTFormer takes temporal hand features as input and leverages self-attention to enhance the temporal hand features.
The KTFormer enables the proposed BlurHandNet to implicitly consider both the kinematic structure and temporal relationship between the hands in three timesteps.
%The KTFormer utilizes hand features from every joint in the whole time step as a token, which enables the proposed BlurHandNet to implicitly consider both the kinematic structure and temporal relationship between the hand joints.
The KTFormer brings significant performance gain when coupled with Unfolder, demonstrating that employing temporal information plays a key role in accurate 3D hand mesh estimation from blurry hand images.


% Since hands in each time step have highly correlated features, we propose a temporal-joint transformer which takes every joint at each time step as tokens of transformer~\cite{vaswani2017attention}.
% We further propose a temporal joint Transformer~(KTFormer) which takes every joint at each time step as tokens, and experimentally demonstrates that temporal information benefits to estimate accurate 3D hand mesh. % ~\cite{arnab2021vivit, zheng2021poseformer}
% We experimentally demonstrate that the proposed BlurHandNet can output accurate 3D hand meshes in each time step by effectively utilizing temporal information, and further generalize well on in-the-wild blurry hand scenario.
% We experimentally valid that the proposed KTFormer significantly boosts the accuracy.

With a combination of BlurHand and BlurHandNet, we first tackle 3D hand mesh recovery from blurry hand images.
We show that BlurHandNet produces robust results from blurry hands and further demonstrate that BlurHandNet generalizes well on in-the-wild blurry hand images by taking advantage of effective temporal modules and BlurHand.
As this problem is barely studied, we hope our work could provide useful insights into the following works.
We summarize our contributions as follows:
\begin{itemize}
    \item We present a novel blurry hand dataset, BlurHand, which contains natural blurry hand images with accurate 3D GTs. 
    \item We propose the BlurHandNet for accurate 3D hand mesh estimation from blurry hand images with novel temporal modules, Unfolder and KTFormer.
    \item We experimentally demonstrate that the proposed BlurHandNet achieves superior 3D hand mesh estimation performance on blurry hands. 
\end{itemize}


% The main component of our BlurHandNet is a temporal unfolding, shown in Figure~\ref{fig:overview}.
% Rather than expecting the off-the-shelf deblurring methods to remove blurry artifacts, we try to extract useful temporal information from the blurry artifacts.
% Specifically, three temporal joint features, each corresponding to the past, current, and future hand, are obtained by unfolding process.
% Conventionally~\cite{fan2021learning,spurr2018cross,moon2020i2l,choi2020pose2mesh}, given a single hand image as an input, corresponding single 3D hand mesh is reconstructed from the input.
% On the other hand, our BlurHandNet unfolds the images in the temporal space, extracting three temporal joints features.
% Here, we regard the hand from the middle frame as current, and denote hand from both end frames as past and future hands.
% By rethinking a blurry hand as sequential hands that exist in three different times, our BlurHandNet can utilize semantic information from hands.
% By rethinking a blurry hand as sequential hands that exist in three different times, our BlurHandNet can utilize not only spatial but also temporal information.
% As the same hand moving in time could share similar properties, we let the model to exchange useful temporal information between the joints and regressed them to 3D hand meshes.
% Thus, the final output of our BlurHandNet is three 3D hand meshes, each corresponding to the past, current, and future hands.



% Second, we propose a temporal joint Transformer~(KTFormer) to exploit related information by considering attentive correlation between temporal joints.
% Conventionally, given token as input, Transformers~\cite{vaswani2017attention,dosovitskiy2020image} models correlation between tokens by computing the distance between channel dimension.
% Although they can exploit relevant information across the spatial dimension, they cannot deliver the information across the channel dimension, which might contain fluent pose information~(\ie depth, rotation) in hand features.
% Therefere, to exchange the information between temporal hands both spatial and channel-wisely, we introduce two sub-modules in our KTFormer, spatial-wise self-attention~(SSA) and channel-wise self-attention~(CSA).
% While SSA follows traditional Transformer~\cite{vaswani2017attention}, CSA computes correlation across the spatial dimension to exchange information across the channel.





% To exchange useful information between temporal hands, we propose temporal joint Transformer~(KTFormer), which could deliver related information by considering attentive correlation between temporal joints.
% Our KTFormer has two distinctive points compared to the conventional Transformer~\cite{vaswani2017attention}.
% First, KTFormer factorises~\cite{ho2019axial,weissenborn2019scaling,arnab2021vivit} the attention operation to only compute self-attention within \emph{spatial joints}, and then \emph{temporal joints}.
% Here, spatial joints represent different articulation existed in the same timestep.
% On the other hand, temporal joints refer to the same articulation in different timesteps.
% To this end, we employ two sub-modules, spatial self-attention block~(SSA) and temporal self-attention~(TSA), which are responsible for finding proper attentive correlation in spatial and temporal axis.
% Second, we employ regularization to alleviate the attention value within the same joints.
% Without any regularization, we found that each joint shows overwhelmingly strong correlation with itself, simultaneously lower the attention value with different spatial and temporal joints.
% While strong correlation with joint itself can filter the unwanted high correlation from the other joints, in the situation of hands where strong correlation is guaranteed, exploiting self information is redundant.
% To address this issue, we compulsively lower the attention value with oneself.

% We demonstrate the justification of our BlurHand dataset, through extensive generalization experiments on in-the-wild blurry hand images.
% % Although, our new dataset is composed with characteristic background~(indoor studio), it shows reasonable qualitative improvements.
% Furthermore, we validate the effectiveness of our proposed BlurHandNet with rigorous ablation study and comparisons with stat-of-the-art 3D hand pose estimation methods.
% We summarize our contributions as follows:
% \begin{itemize}
%     \item We firstly introduce blurry hand dataset, BlurHand, which contains realistic blur. 
%     \item We propose BlurHandNet with novel Transformer module, temporal joint Transformer~(KTFormer).
%     KTFormer effectively handle the spatio-temporal attention by factorising the attention mechanism.
%     In addition, we propose novel regularization term in KTFormer to boost the usage of spatial and temporal information.
%     \item We show that our proposed BlurHandNet that consists of KTFormer, achieves superior 3D hand pose estimation accuracy compared with the previous state-of-the-art methods both qualitatively and quantitatively.
% \end{itemize}


% To exchange useful information between temporal hands, we propose temporal joint Transformer~(KTFormer), which could deliver related information by considering attentive correlation between temporal joints.
% As the transaction between temporal joints can take place in two manners, features and coordination, we factorise the Transformer operation to compute two different types of self-attention within features and coordination, respectively.




% To this end, motivated by factorising attention scheme~\cite{ho2019axial,weissenborn2019scaling,arnab2021vivit}, we first employ two Transformer~\cite{vaswani2017attention,dosovitskiy2020image}-based sub-modules, coordination Transformer~(coord-T) and feature Transformer~(feat-T) in our KTFormer.
% The coord-T enhances the 3D joints coordination by considering correlation between coordination from temporal joints.
% To exchange useful information between temporal hands, we propose temporal joint Transformer~(KTFormer), which could deliver related information by considering attentive correlation between temporal joints.
% In KTFormer, we build two Transformer~\cite{vaswani2017attention,dosovitskiy2020image}-based sub-modules, coordination Transformer~(coord-T) and feature Transformer~(feat-T), according to their different roles.





% Under the temporal motion of hands, we observe that hands can move spatial and depth wisely.
% Thus both direction of movement should be handled to properly utilize the temporal information.
% From this point of view, motivated by recent success of Transformer~\cite{dosovitskiy2020image} in estimating 3D hand pose and shape~\cite{hampali2022keypoint,li2022interacting,park2022handoccnet}, we propose novel Transformer-based module, voxel Transformer~(VoT) to utilize spatial-depth wise information from temporal sequences.

% s shown in Fig. 3, instead of aiming to localize and rec-
% ognize the hand joints simultaneously, we estimate the 3D
% poses of the hands in three stages: (1) We first detect “key-
% points”, which are potential joint locations in the image, by
% predicting a single heatmap. These keypoints do not have to
% exactly match all the hand joints: The 3D poses we predict
% are still correct if some joints are not detected as keypoints,
% and if some keypoints do not correspond to joints. (2) Then,
% we associate the keypoints with the corresponding joint or
% to the background in the case of false positives, on the ba-
% sis of the keypoint locations and their image features. This
% is done for all the keypoints simultaneously to exploit mu-
% tual constraints, using the self-attention mechanism. (3) Fi-
% nally, we predict the 3D hand poses using a cross-attention
% module, which selects keypoints associated with each of the
% hand joints. Our approach is agnostic to the parameteriza-
% tion of the pose and we consider three different hand pose
% representations.
% Our architecture, which we call “Keypoint Trans-
% former”, is therefore designed to explicitly disambiguate
% the identity of the keypoints and performs very well even
% on complex configurations. Fig. 1 shows its output on two
% challenging examples, using the MANO [41] mesh as the
% output representation. Our architecture is related to the “De-
% tection Transformer” (DETR) [8] architecture. DETR uses
% all the spatial features from a low-resolution CNN feature
% map, combined with learned location queries to detect ob-
% jects in an image. The high computational complexity of
% the Transformer restricts DETR from using higher resolu-
% tion CNN feature maps. As we show in our experiments,
% using the DETR-style architecture for hand pose estimation
% results in lower accuracy and we hypothesize that this is
% due to the use of lower resolution feature maps and features
% from the entire image

% \input{latex/sections/figures/swin}
% While analogous to the typical attention module and mlp module, our VoT has distinctive points compared to the previous Transformers.
% First, tokens are formed from 3D~(x,y,z) spaces, rather than 2D~(x,y).
% As shown in Figure~\ref{fig:overview}, we first map the blurry hand images~$\mathbf{I}$ to the 3D heatmap features for each joint, and 3D heatmap features are divided into smaller voxels.
% Here, different from the conventional 3D hand mesh estimation~\cite{fan2021learning,spurr2018cross}, we unfold the temporal domain into three~(past, current, and future), and produce three heatmap features~$\mathbf{H}_{\text{P}}$, $\mathbf{H}_{\text{C}}$, and $\mathbf{H}_{\text{F}}$, respectively.
% Then, subsets of voxel, regarded as ``tokens", are fed into the VoT.
% Second, motivated by recent success of Swin-Transformer~\cite{liu2021swin}, we also adopt the \emph{shift} to the heatmap features.
% However, different from \cite{liu2021swin}, we shift the heatmap both spatial and depth wisely, named 3D shifting.
% An illustration of our 3D shifting is shown in Figure~\ref{fig:swin}.
% While voxels divided by fixed grid might suffer from fixed boundary, VoT with 3D shifting enjoy robustness to boundary effects.


% Our VoT can explicitly utilize distant features both spatial and depth wisely by considering the attentive correlation.
% We first map the blurry hand images to the 3D features for each joint, and 3D features are divided into smaller voxels.
% Here, different from the conventional 3D hand pose estimation~\cite{fan2021learning,spurr2018cross}, we unfold the temporal domain into three~(past, current, and future), and produce three 3D features, respectively.
% Then, subsets of voxel, regarded as ``tokens", are fed into the VoT.
% Analogous to the previous Transformer~\cite{vaswani2017attention,dosovitskiy2020image}, our VoT transfers information from voxels to voxels by considering the correlation between them.

% Stepping forward, motivated 

% To handle the temporal information and scattered information, we employ two voxel Transformer for different purposes, Temporal-voxel Transformer~(T-VoT) and Joint-voxel Transformer~(J-VoT).
% The T-VoT takes temporal voxels as input tokens to utilize the temporal correlation.
% The following J-VoT, on the other hand, takes subsets from the only same joint as input tokens, and manage the network to focus on the accurate 3D feature among the scattered features.

% We demonstrate the justification of our BlurHand dataset, through extensive generalization experiments on in-the-wild blurry hand images.
% Although, our new dataset is composed with characteristic background~(indoor studio), it shows reasonable qualitative improvements.
% Furthermore, we validate the effectiveness of our proposed voxel Transformer~(VoT) with rigorous ablation study and comparisons with stat-of-the-art 3D hand pose estimation methods.

% % We summarize our contributions as follows:
% % \begin{itemize}
% %     \item For our knowledge, we first introduced blurry hand dataset, BlurHand, to imitate the real world situations, accompanying the blurriness. 
% %     \item We proposed BlurHandNet with novel Transformer module, voxel Transformer~(VoT), to find a proper attentive correlation between the 3D~(x,y,z) spaces.
% %     By applying VoT, our BlurHandNet could handle temporal information in both spatial and depth wisely.
% %     \item We show that our proposed BlurHandNet that consists of VoT, achieves superior 3D hand pose estimation accuracy compared with the previous state-of-the-art methods both qualitatively and quantitatively.
% % \end{itemize}



% The main concept of the proposed BlurHandNet is unfolding a temporal information from the single blurry hand images.
% The conventional 3D hand pose estimation methods~\cite{fan2021learning,spurr2018cross} that produce 3D pose of one hand from one input image.
% However, as blurry image contains contents from temporal hands, we try to estimate 3D poses of three~(past, current, and  future) hands, and utilize correlation among three hands.

% To address the temporal correlation in our BlurHandNet, we propose novel Transformer architecture, Grid Transformer.
% Motivated by recent success of Transformer~\cite{dosovitskiy2020image} in estimating 3D hand pose~\cite{hampali2022keypoint,li2022interacting,park2022handoccnet}, Transformer-based architecture is also employed in our the Grid Transformer, but in novel way.
% We first map the blurry hand images to the three 3D features.
% Here, each 3D features represent temporal information of past, current, and future.
% Then, each 3D feature is divided into smaller 3D subsets by 3D grid, and subsets , regarded as tokens, are fed into the Transformer.
% Thus, Transformer can enjoy delivering the information 







% P5: 

% 

% \begin{figure*}[t]
% \begin{center}
% % 
% \vspace{2mm}
% \begin{subfigure}{0.48\linewidth}
% \centering
% \includegraphics[width=1.\linewidth]{latex/figures/overall/unfolding example.pdf}
% \caption{\textbf{Overview of the proposed BlurHandNet.} \label{fig1-b}}
% \end{subfigure}
% % 
% \begin{subfigure}{0.48\linewidth}
% \centering
% \includegraphics[width=1.\linewidth]{latex/figures/blurimage/blurhands.pdf}
% \caption{\textbf{Examples of the presented BlurHand dataset.}
% \label{fig1-a}}
% \end{subfigure}
% % 
% \end{center}
% \caption{\textbf{Proposed BlurHandNet and BlurHand dataset.} 
% %
% (a) Different from previous methods, our BlurHandNet unfolds the blurry hand image into three temporal hands.
% %
% (b) We present a novel BlurHand dataset, providing natural blurry hand with accurate 3D annotations.
% }
% \label{fig:overview}
% \end{figure*}

% The hand, which is one of the most dynamic parts of the body, often accompanies the blur due to its motions.
% The hand, one of the most dynamic parts of the body, often accompanies the blur due to its active motions.
% As the blur leaves afterimages on the hand images and interferes the network from focusing on the hand in certain timestep, it can eventually degrade the performance of 3D hand mesh estimation and make the task challenging.
% As the blur leaves afterimages on the hand images and interferes the network from focusing on the target hand image, it can eventually degrade the performance of 3D hand mesh estimation and make the task challenging.
% Despite the occurrence of blur is prevalent in hands, the most existing methods have focused on occluded hand~\cite{park2022handoccnet}, hand holding
%Although previous methods have handled various practical scenarios such as interacting hands~\cite{moon2020interhand2,hampali2022keypoint,li2022interacting}, hand-object interaction~\cite{liu2021semi,hasson2019learning,hasson2020leveraging}, and occluded hands~\cite{park2022handoccnet}, existing methods have less consideration for the blurry hand.
% The main reason for the absence of the 3D hand mesh estimation on the blurry hand images is the absence of the proper dataset.
% Although the problem statement is clear and a solution is desperately required, the most existing methods~\cite{park2022handoccnet,liu2021semi,hasson2019learning,hasson2020leveraging,moon2020interhand2,hampali2022keypoint,li2022interacting} have concentrated on other various hand situations.
% They have focused on occluded hand~\cite{park2022handoccnet}, hand holding objects~\cite{liu2021semi,hasson2019learning,hasson2020leveraging}, and interacting hands~\cite{moon2020interhand2,hampali2022keypoint,li2022interacting} without considering the blur in hand.
% We found that the absence of the 3D hand mesh estimation on the blurry hand images is due to the absence of the dataset.
% Therefore, to address the issue, we firstly propose a blurry hand dataset, BlurHand dataset, and a baseline, BlurHandnet, for blurry hand mesh estimation.

% While the longer exposure time of camera leads to brighter image, the relative motion during the interval leads the resulting image to contain a blurring artifact.

%For the blurry images, however, the 2D geometric grounds are not reliable and lead inaccurate 3D annotations.
%There is also a way to use a sensor~\cite{garcia2018first}, but it entails unnatural appearance of hands.

% Motivated by this, we firstly propose a blurry hand dataset, BlurHand, and a baseline, BlurHandnet, for estimating 3D hand mesh from the blurry hand.
% We present BlurHand, the first blurry hand dataset.
% The BlurhHand is made from InterHand2.6M~\cite{moon2020interhand2}, widely adopted video-based hand dataset.
% Analogous to the synthetic blur generation technique in image restoration community~\cite{Su_2017_CVPR,Nah_2017_CVPR,Nah_2019_ICCV_Workshops}, we approximate the blurry images taken at a longer exposure time by averaging the temporal frames.
% taken at a longer exposure time 
% Furthermore, we iteratively interpolate the middle frames of Interhand2.6M to imitate the high speed camera, producing more realistic blur, as shown in Figure~\ref{fig1-a}.
% In detail, we average the sequence of sharp hand images from Interhand2.6M to imitate the high speed camera, producing more realistic blur, as shown in Figure~\ref{fig1-a}.
% This is to compensate low frame rate~(30fps) of InterHand2.6M, which might generate unnatural and discontinuous blur, when simply averaged.
% The proposed blurry hand dataset, BlurHand dataset, is  synthetic blur generation technique in image restoration community~\cite{Su_2017_CVPR,Nah_2017_CVPR,Nah_2019_ICCV_Workshops}.

% Given our synthesized BlurHand, the most straightforward 3D hand mesh estimation baseline is a combination of deblurrring~\cite{chen2022simple,zamir2022restormer} and 3D hand mesh estimation networks~\cite{moon2020i2l,spurr2018cross,moon2022accurate}.
% To solve the 3D blurry hand mesh estimation, it is possible to introduce deblurring the blurry inputs before conducting the mesh estimation, assuming that deblurring could sharpen the blurry hand images and let the mesh estimation model to focus on it.
% However, we found that such baseline suffers from two limitations.
% This is because hands contain very complex and diversely moving particles~(articulations), which makes the model difficult to gather spatially scattered information.
% Second, nevertheless, even though deblur method completely reconstruct the sharp hand image, they have limited use of the information available from the blurry hand.
% Specifically, although the blur comes from spatially moving semantic objects~(hands) in temporal spaces, the most deblurring methods~\cite{chen2022simple,zamir2022restormer,suin2022adaptive} have focused on gathering spatially dispersed information in pixel-level without considering the semantic information.
% Specifically, although the blur comes from spatially moving objects~(hands) in temporal spaces, utilizing the temporal information is frequently neglected.
% They~\cite{chen2022simple,zamir2022restormer,suin2022adaptive} have only focused on gathering spatially dispersed information.
% Therefore, na\"ively performing the deblurring network before the hand pose estimation network is not the best approach for estimating accurate 3D mesh in blurry hand images.

% First, deblurring removes temporal information and deprives the chance in which temporal information can be involved.
% As information of hand in certain timestep is faded out in blurry hand images, temporal information can play a key role in compensating for insufficient hand representation.
% Second, even the state-of-the-art deblurring methods~\cite{chen2022simple} cannot completely deblur the blurry image, still remaining blurry artifacts.
% While most deblurring methods~\cite{chen2022simple,zamir2022restormer} have concentrated on natural objects, sharing blur kernels, moving hands possesses diverse directions and motion strength of each articulation, which makes deblurring them difficult.
% % Second, even the state-of-the-art deblurring methods~\cite{chen2022simple} cannot completely deblur the blurry image, still remaining blurry information of hand.
% Therefore, na\"ively performing the deblurring network before the hand pose estimation network is not suitable for estimating 3D mesh in blurry hand images.
%P4: our approach
% On this basis, we propose BlurHandNet, which can explicitly utilize the temporal information to improve the reconstruction of 3D mesh of blurry hands.
% To overcome the above issues of the combination of deblurring and 3D hand mesh estimation methods, we propose BlurHandNet, which can explicitly utilize the spatio-temporal information in the blurry hand image to improve
% To overcome the above issues, we propose BlurHandNet, which can explicitly utilize the temporal information to improve the reconstruction of 3D mesh of blurry hands.
% We propose two main components supplementing the BlurHandNet.
% First, we present a temporal unfolding, as shown in Figure~\ref{fig1-b}.
%Exploiting useful temporal information directly from a single blurry image is not straightforward, since various sharp images are convoluted in blurry images.
% Even though blurry images are the results of accumulated from sharp sequential frames,
% multiple possible solutions exist that lie in motion trajectory.
%To handle the problem, we present a novel temporal module, namely Unfolder, that 
% The KTFormer utilizes hand features from every joint in the whole time step as a token, which enables the proposed BlurHandNet to exploit meaningful information in temporal domains.
%
%The proposed BlurHandNet performs the best when using Unfolder and temporal-joint transformers together, demonstrating that employing temporal information plays a key role in accurate 3D hand mesh estimation in blurry hands.