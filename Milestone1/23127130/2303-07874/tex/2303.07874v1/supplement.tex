\documentclass[twoside]{article}

\usepackage{aistats2022}
% If your paper is accepted, change the options for the package
% aistats2022 as follows:
%
%\usepackage[accepted]{aistats2022}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

% If you set papersize explicitly, activate the following three lines:
%\special{papersize = 8.5in, 11in}
%\setlength{\pdfpageheight}{11in}
%\setlength{\pdfpagewidth}{8.5in}

% If you use natbib package, activate the following three lines:
%\usepackage[round]{natbib}
%\renewcommand{\bibname}{References}
%\renewcommand{\bibsection}{\subsubsection*{\bibname}}

% If you use BibTeX in apalike style, activate the following line:
%\bibliographystyle{apalike}

%MY stuff
\usepackage{amsmath,amssymb,amsthm}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage[utf8]{inputenc}

\usepackage{graphicx}
\usepackage{xcolor}

\usepackage{subfig}
\usepackage{verbatim,hyperref}
\usepackage{dsfont}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{thmtools,thm-restate}


\usepackage{mathtools}
\usepackage{enumerate}
\usepackage{dirtytalk}

\usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{marginnote} % for better margin notes
%\usepackage{easyReview}

%algorithms
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
%\usepackage[margin=1in]{geometry}
%%%%%%%%%%%%%

%tables
\usepackage{booktabs}
\usepackage{siunitx}
%%%%%%%%%%%%%%
\usepackage{bbm}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{observation}{Observation}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{claim}{Claim}
\newtheorem{proposition}{Proposition}
\newtheorem{fact}{Fact}
\newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}
\newtheorem{lie}{Lie theory derivation}

\newtheorem{definition}{Definition}
\newtheorem{example}{Example}

\newcommand{\e}{\epsilon}

\newcommand{\wh}[1]{\widehat{#1}}
\newcommand{\poly}{\text{poly}}
\newcommand{\supp}{\text{supp}}

% our macros
\newcommand{\samp}{{\mathcal S}}
\newcommand{\dist}{{\mathcal D}}
\newcommand{\family}{{\mathcal H}}
\newcommand{\loss}{{\ell}}
\newcommand{\expectation}{{\mathbb E}}
\newcommand{\x}{\pmb{x}} % generic vector of functions when we talk about Lagrangian 
\newcommand{\xdot}{\dot{\pmb{x}}} % time derivative of \x
\newcommand{\xddot}{\ddot{\pmb{x}}} %second time derivative of \x
\newcommand{\dx}{\pmb{\xi}} % time derivative of \x
\newcommand{\dxdot}{\dot{\pmb{\xi}}} %second time derivative of \x

\newcommand{\w}{\pmb{w}} % generic vector of
\newcommand{\bias}{b} % generic vector of 
\newcommand{\wdot}{\dot{\pmb{w}}} % time derivative of \x
\newcommand{\wddot}{\ddot{\pmb{w}}} %second time derivative of \x
\newcommand{\gen}{\pmb{g}} %generator of symmetries
\newcommand{\y}{\pmb{y}} 
\newcommand{\uu}{\pmb{u}} 
\newcommand{\alphap}{\pmb{\alpha}} 
\newcommand{\variation}{V}

\newcommand{\bet}{\pmb{\beta}} %vector of parameters for Swish
\newcommand{\betdot}{\pmb{\beta}} %time derivative of vector of parameters for Swish
\newcommand{\betddot}{\pmb{\beta}} %second time deriv. of vector of parameters for Swish

\newcommand{\z}{\pmb{z}} % generic vector of values
\newcommand{\Lag}{{\mathcal L}} % Lagrangian
\newcommand{\pot}{U} % Lagrangian
\newcommand{\Act}{J} % the action associated to a Lagrangian
\newcommand{\vd}{\nabla \Act} % variational derivative


\newcommand{\trace}{{\text{tr}}} % trace
\newcommand{\pred}{{f}}

\newcommand{\cnstdd}{{\kappa_2}}
\newcommand{\cnstd}{{\kappa_1}}

\newcommand{\noise}{{\eta}}

%%%%%%% from other paper
\newcommand{\code}{{\mathcal C}}
\newcommand{\coset}{{\mathcal K}}
\newcommand{\group}{{\mathcal G}}
\newcommand{\PB}{P_{\text{B}}^{\text{MAP}}}
\newcommand{\set}{{\mathcal S}}
\newcommand{\pset}{{\mathcal P}}
\newcommand{\Ldens}{{}}
\newcommand{\shell}{{\mathcal S}}
\newcommand{\eset}{{\mathcal E}}
\newcommand{\vset}{{\mathcal V}}
\newcommand{\prob}{{\mathbb P}}
\newcommand{\reals}{{\mathbb R}}
\newcommand{\field}{{\mathbb F}_2}
\newcommand{\naturals}{{\mathbb N}}
\newcommand{\cw}{c}
\newcommand{\argmax}{\text{argmax}}
\newcommand{\ind}{\mathbf 1}
\newcommand{\xv}{\mathbf x}
\newcommand{\vv}{\mathbf v}
\newcommand{\yv}{\mathbf y}
\newcommand{\Xv}{\mathbf X}
\newcommand{\muv}{\mathbf \mu}
\newcommand{\wv}{\mathbf w}
\newcommand{\Wv}{\mathbf W}
\newcommand{\Dv}{\mathbf D}
\newcommand{\Rv}{\mathbf R}
\newcommand{\Zv}{\mathbf Z}
\newcommand{\Uv}{\mathbf U}
\newcommand{\Iv}{\mathbf I}
\newcommand{\Sigmav}{\mathbf \Sigma}
%\newcommand{\muv}{\mathbf \mu}
\newcommand{\Kv}{\mathbf K}
\newcommand{\model}{\mathcal H}
\newcommand{\indim}{m}
\newcommand{\famdim}{k}
\newcommand{\outdim}{n}
\newcommand{\inspace}{{\mathcal X}}
\newcommand{\outspace}{{\mathcal Y}}
\newcommand{\sampspace}{{\mathcal Z}}

%%%%%%%% Check if needed
\usepackage{comment}

\usepackage{todonotes}
%%% USE TO REMOVE COMMENTS
%\usepackage[disable]{todonotes}

\newcounter{mycomment}
\newcommand{\comm}[2]{%initials of the author (optional) + note in the margin
\refstepcounter{mycomment}
{%
    %\setstretch{0.7}% spacing
    \todo[author = \textbf{#1~\#~\themycomment}, color={red!100!green!35}, fancyline, size = \footnotesize]{%
        #2}%
    }
}

\newenvironment{proofsketch}{\paragraph{Proof Sketch:}}{\hfill$\square$}

%equations in tables
\usepackage{tabularx}
\usepackage{array,collcell}
\newcommand\AddLabel[1]{%
  \refstepcounter{equation}% increment equation counter
  (\theequation)% print equation number
  \label{#1}% give the equation a \label
}
\newcolumntype{M}{>{\hfil$\displaystyle}X<{$\hfil}} % mathematics column
\newcolumntype{L}{>{\collectcell\AddLabel}r<{\endcollectcell}}

\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

% Supplementary material: To improve readability, you must use a single-column format for the supplementary material.
\onecolumn
\aistatstitle{Supplementary Material}


\section{$\e$-Complexity}

\paragraph{Variational Complexity}
Consider the case $d=1$, and let $f : [0, 1] \xrightarrow{} \R$ be continuous and piece-wise linear. I.e., there is a sequence of points $0=x_1 < x_2 < \cdots < x_{k+1}=1$ so that for $x \in [x_i, x_{i+1}]$, $1 \leq i < k+1$,
\begin{align} \label{equ:polygone}
f(x) = c_i + \alpha_i (x-x_i),
\end{align}
for some constants $c_i$ and $\alpha_i$, where $c_1=c$ and $c_{i+1} = c_i + \alpha_i (x_{i+1}-x_i)$. Then $f$ can be written as a sum of ReLU functions, 
\begin{align} \label{equ:firstrepresentation}
f(x) = c + \sum_{i=1}^{k} a_i [x-x_i]_+,
\end{align}
where $a_1=\alpha_1$ and $a_{i}=\alpha_{i}-\alpha_{i-1}$, $i=2, \cdots, k$. 
Let us now introduce a
\say{complexity} measure for a function $f_{\theta}$ implemented by the NN with $k$ hidden nodes via the parameters $\theta$. We start by introducing a complexity measure for a particular choice of the network parameters. The complexity of the function will then be the minimum complexity of the network that represents this function. We choose
\begin{align} \label{equ:complexitymeasure}
C_k(\theta) = \frac12 \| \theta_w\|^2 = \frac12\left( \|W^{(1)}\|_F^2 + \|\w^{(2)}\|_2^2 \right),
\end{align}
i.e., it is half the squared Euclidean norm of the {\em weight parameters}. 

If we use the representation (\ref{equ:firstrepresentation}) in its natural form, i.e.,  $w^{(2)}_i =a_i$ and $W^{(1)}_i = 1$, then we have $C_k(\theta) = \frac12 \sum_{i=1}^{k} (a_i^2+1)$. But we can do better. Write
\begin{align} \label{equ:secondrepresentation}
f(x) = c + \sum_{i=1}^{k} w^{(2)}_i [W^{(1)}_i(x-x_i)]_+,
\end{align}
where $w^{(2)}_i =a_i/\sqrt{|a_i|}$ and $W^{(1)}_i = |w^{(2)}_i |$. This gives us a complexity measure $C_k(\theta) = \sum_{i=1}^{k} |a_i| = \sum_{i=1}^{k} |\alpha_i-\alpha_{i-1}|$, where $\alpha_0=0$. Indeed, it is not very hard to see, and it is proved in \citet{srebronormbound}, that this is the best one can do even if we keep $f(x)$ fixed and are allowed to let the number $k$ of hidden nodes tend to infinity. In other words, for the function $f$ described in (\ref{equ:polygone}) we have
\begin{align*}
C(f) = \inf_{k \in \N, \theta: f_\theta = f} C_k(\theta) = \variation(f'),
\end{align*}
where $\variation(f')$ denotes the total variation of $f'$, the derivative of $f$. Why total variation?
Note that $\alpha_i$ denotes the derivative of the function so that $|\alpha_i-\alpha_{i-1}|$ is the change in the derivative at the point $x_i$. Therefore, $\sum_{i=1}^{k} |\alpha_i-\alpha_{i-1}|$ is the total variation associated to this derivative. 

% need to incorporate this material
%Background: proof of Weierstrass theorem (see Pinkus, 2000)
%Any f ∈ C[0; 1] can be uniformly approximated to arbitrary precision by a polygonal line (cf. Shektman, 1982)
%Lebesgue (1898): polygonal line with m pieces can be written
%m−1
%g(x) = ax + b + �� ci(x − xi)+
%i=1
%�� knots: 0=x0 <x1 <···<xm−1 <xm =1 �� m+1parametersa,b,ci ∈R
%�� ReLU function approximation in 1d
%% end

If we consider a general function $f: [0, 1] \xrightarrow{} \R$ then for every $\epsilon>0$, $f$ can be uniformly approximated by a piecewise linear function, see \citet{shekhtman82}. As $\epsilon$ tends to $0$ for the \say{best} approximation the variation of the piece-wise linear function converges to the total variation of $f'$. This can equivalently be written as the integral of 
$|f''|$.
It is therefore not surprising that if we look at general functions $f: \R \xrightarrow{} \R$ and let the network width tend to infinity then the lowest cost representation has a complexity of
\begin{align} \label{equ:complexity}
C(f) = \max \left(\int |f''(x)| dx, |f'(-\infty) + f'(+\infty)| \right).
\end{align}
As we previously mentioned, this concept of the complexity of a function was introduced in \citet{srebronormbound} and this paper also contains a rigorous proof of (\ref{equ:complexity}). (Note: The second term in \eqref{equ:complexity} is needed
when we go from a function that is supported on a finite domain to $\R$. To see this consider the complexity of $f(x) = \alpha x$. It is equal to $2\alpha$ ($f(x) = \sqrt{\alpha} [\sqrt{\alpha} x]_+ - \sqrt{\alpha} [-\sqrt{\alpha} x]_+$) but $\int |f''(x)| dx = 0$.)

%\begin{align*}
%f(x) = \int_{\R} \left( \alpha(1,b) [x-b]_+ + \alpha(-1,b) [b-x]_+ \right) db + c.
%\end{align*}
%Then taking the second derivative of $f$ we get
%$$
%f(x) = \alpha(1,x) + \alpha(-1,x) = \alpha_+(x).
%$$
%We see that the measure $\alpha$ is almost uniquely defined by $f$. 


 %\citet{srebronormbound} it was shown that for 2-layer NN with ReLU activation %functions (in the infinite width limit) the minimal network euclidean norm for %representing $f$ is equal
%$$
%\max \left(\int |f''(x)| dx, |f'(-\infty) + f'(+\infty)| \right).
%$$
%Formally we consider functions of the form
%$$
%\sum_{i=1}^k w_i^{(2)} \left[\langle \w_i^{(1)}, \x \rangle + b_i^{(1)} \right]_+ + %b^{(2)}.
%$$
%We want to minimize $\|\w^{(2)}\|_2^2 + \|W^{(1)}\|_F^2$. Minimizing this norm is %equivalent to constraining norms of rows of $W^{(1)}$ and minimizing L1 norm of %$\w^{(2)}$. That is ultimately we minimize
%$$
%R(f) := \inf_{\theta \in \Theta}\|\w^{(2)}\|_1 \text{ s.t. } h_{\theta} = f, %\forall_{i} \|\w_i^{(1)}\|_2 = 1
%$$

%As we discussed when we introduced the complexity measure, the motivation for doing so is to impose a complexity hierarchy on the functions that are representable by NNs from \say{simple} to \say{most complex}. The thesis is then that NNs, when learning with SGLD (and to some extend SGD), will not pick a function that best approximates the given data. Rather, the criterion to be minimized consists of two parts, namely the approximation error plus the complexity. In other words, we will learn the lowest complexity function that reasonably approximates the given data. This explains why NNs do not overfit despite the large inherent overparametrization.

%Let us summarize. Minimizing (\ref{equ:regularizedopt}) corresponds to minimizing the  sum of the empirical mean and a natural complexity measure of the function. Therefore, in this case, the regularization has a very pleasing interpretation as favoring \say{low complexity} functions.


\paragraph{Sharp versus Variational Complexity.} Now we explain how the notion of sharp complexity is, in some regimes, equivalent to the variational complexity. This gives a concrete example of our promise that sharp complexity aligns well with natural complexity measures.

Recall that we consider a NN with $k$ nodes in the intermediate layer, a scalar input and a scalar output. We therefore have $ 2k$ weights, namely $k$ weights from the scalar {\em input} to the intermediate nodes (which we henceforth call {\em input} weights) and $k$ weights from the intermediate nodes to the {\em output} (which in the sequel we will call {\em output} weights). Let us start by looking at a simplified model where we only consider the $k$ output weights. We still assume that the incoming weights behave as they should (as we will see shortly, they will take on values equal to the outgoing weights) but we ignore them in the probabilistic expression.
We discuss the real model at the end. Not much will change.

Assume at first that the function we want to represent is of the form (\ref{equ:secondrepresentation}) and requires only a single change of the derivative. I.e., the piece-wise linear function consists of two pieces and we require only one term in the sum. Call this function $g_1$,\comm{GG}{Change $f_1$ to $g_1$} where the $1$ indicates that there is only a single change of the derivative. Let us assume that this change is of magnitude $a$. Further, assume that the prior $\alpha(\theta)$ is Gaussian  where the components are independent. The vector of means is $\mu_{\theta} = (\mu_{\theta_w}=(0,\cdots, 0), \mu_{\theta_b})$ and the vector of variances is $\sigma^2_{\theta} = (\sigma^2_{\theta_w}=(\sigma^2_w,\cdots, \sigma^2_w), \sigma^2_{\theta_b}=(0,\cdots, 0))$. Then $\alpha(\theta) \propto e^{-\frac{1}{2 \sigma_w^2} \| \theta_w\|^2}$. 


We now ask what is the value of $\chi^\#(\dist_\xv, g_1, \e)$- as this is what appears in our bound from Section~\ref{sec:intro}. We claim that for small $\e$
\[
-\log \left( \prob[\expectation_{x \sim \dist_x} [(g_1(x) - f_{\theta}(x))^2] \leq \e] \right) \approx \frac{a}{2\sigma_w^2}.
\]
To show that we estimate the probability of $f_\theta$'s close to $g_1$.

If we represent this change of derivative by a single node then we know that we can assign $\sqrt{a}$ to the outgoing weight of this node and $\sqrt{a}$ to the incoming weight of this node. This corresponds to a network where all outgoing weights are zero except one that is $\sqrt{a}$ (and the same for the incoming weights). But we can also take the weight $\sqrt{a}$ and \say{spread it out} over the $k$ nodes in any fashion we want, assuming only that the sum of squares of the weights equals $a$. In fact, take a vector $\vv$ of length $k$ of Euclidean norm squared equal to $1$. Let the vector have components $\vv_i$ and assign the value $\sqrt{a} \vv_i$ to the outgoing weight of node  $i$ (and of course pick the bias terms accordingly and also the incoming weights). Then together all these $k$ nodes will represent the same change in the derivative and the complexity will also be the same. In principle all the components $\vv_i$ should be non-negative so that we are working in the positive orthant. In order to avoid this complication let us assume that we have the following further over-parametrization and that along each connection we not only have the weight but we have in addition the possibility to multiply by an element of $\{\pm1\}$ \comm{GG}{Does this plus minus change anything}. In this way we have no restriction on the sign of the weight. Of course, the two models have equal expressive power. For the rest of this section we will assume this model.

In words, if we limit ourselves to the outgoing weights then we have a $k$-dimensional sphere of radius $\sqrt{a}$, where every single point on this sphere represents $f_1$ exactly. Further, if we do not insist that $f_1$ is represented exactly but allow a small deviation then we can extend the sphere to a spherical shell. Every point in this shell gives an approximate representation of $f_1$, each having roughly the same $\|\theta_w\|^2$. Figure~\ref{fig:sphericalshell} depicts this situation for the case $k=3$. In order to show the spherical shell the sphere is cut open.
\begin{figure}[tb]
\begin{center} \includegraphics[width=8cm]{eps/sphericalshell.eps}
\end{center}
\caption{Spherical shell of parameters that approximately represent $f_1$. The blue surface shows the sphere of parameters that give exact representations. The green and the orange surface indicate the outer and the inner boundary of the spherical shell giving approximate representations.}
\label{fig:sphericalshell} 
\end{figure}

What is the volume of all these vectors $\theta$? (Recall, to simplify our current presentation we limit $\theta$ to the outgoing weights.) As discussed, the exact representations lie on the surface of a $k$-dimensional sphere of radius $\sqrt{a}$. The area of this sphere is equal to $
    a^{\frac{k-1}{2}} \frac{2 \pi^{k/2}}{\Gamma(k/2)}$.
Thus the probability is \comm{GG}{Proportional versus equal} proportional to
\begin{align}
    a^{\frac{k-1}{2}} \frac{2 \pi^{k/2}}{\Gamma(k/2)} \cdot \epsilon'  \cdot e^{ - \frac{1}{ 2\sigma_w^2} a}. \label{eq:probabilityofg1}
\end{align}
\comm{GG}{Change all $A(f,S)$ to $N L_\samp(f)$}
We arrive at this expression by starting with the surface area, multiplying this by the \say{thickness} of the spherical shell, which we call $\epsilon'$ and finally multiplying with the \say{height} of the density inside the shell, which is proportional to $e^{-\frac{1}{2\sigma_w^2}a}$ as discussed. 

Let us write this in the form 

\begin{comment}

\begin{align*}
    4 (\sigma_w^2\pi)^{k/2} \underbrace{\left(\frac{(2a)^{k/2-1} e^{-\frac{(2a)}{ 2\sigma_w^2}}}{
    (2 \sigma_w^2)^{k/2} \Gamma(k/2)} \right)}_{(*)} \cdot \epsilon \sqrt{a} \cdot e^{-\frac{1}{ \sigma_y^2} A(f, \samp)}.
\end{align*}

Recall that the density of a chi-square distribution with $k$ degrees of freedom, i.e.,
the distribution of the sum of the squares of $k$ independent Gaussians, has the form
\begin{align*}
\frac{x^{k/2-1} e^{-x/2}}{2^{k/2} \Gamma(k/2)}.
\end{align*}
Further recall that this density is unimodal for $k>1$, i.e., it has a unique maximum and it first increases up to this maximum and then decreases back to zero thereafter. The maximum is at $x=k-2$.

Perhaps we should use instead the chi-distribution that has density 
\begin{align*}
\frac{x^{k-1} e^{-x^2/2}}{2^{k/2-1} \Gamma(k/2)}.
\end{align*}
This is the density of the square root of the sum of the squares instead of the sum of the squares. In this case our density can be written as 

\end{comment}

\begin{align*}
    2 \sqrt{2} (\sigma_w^2\pi)^{k/2} \underbrace{\left(\frac{\sqrt{2a}^{k-1} e^{-\frac{\sqrt{2 a}^2}{ 2\sigma_w^2}}}{\sigma_w^2
    (2 \sigma_w^2)^{k/2-1} \Gamma(k/2)} \right)}_{(*)} \cdot \epsilon.
\end{align*}
\comm{GG}{Theres a weird sigmaw inside the denominator}
Recall that the density of a chi-distribution with $k$ degrees of freedom, i.e.,
the distribution of the square root of the sum of the squares of $k$ independent Gaussians, has the form
\begin{align*}
\frac{x^{k-1} e^{-x^2/2}}{2^{k/2-1} \Gamma(k/2)}.
\end{align*}
Further recall that this density is unimodal for $k>1$, i.e., it has a unique maximum and it first increases up to this maximum and then decreases back to zero thereafter. The maximum is at $x=\sqrt{k-1}$.

%The mode is at ${\sqrt{k-1}}$. Now we do not have the annoying factor. So want to have $\sqrt{k-1} = \sqrt{2 a}/\sigma$.

We conclude that $(*)$ corresponds to a scaled chi-distribution with $k$ degrees of freedom where instead of zero-mean unit-variance Gaussians we pick zero-mean Gaussians with variance $\sigma_w^2$. Note that such a scaled chi-distribution takes on its maximum value at $\sqrt{k-1} \sigma_w$. The standard scaling of the variance of the weights is $\sigma_w^2 \propto 1/k$. For simplicity let's assume that $\sigma_w^2 = 1/(k-1)$ \comm{GG}{is it ok?}. Then the maximum value is attained at $1$.

%Let us therefore assume that we set $\sqrt{2 a} = \sqrt{k-1} \sigma_w$, or $\sigma_w^2 = (2 a)/(k-1)$. Note that in practice we do not know the variational complexity of the function. But this choice can always be achieved by performing a one-dimensional hyper-parameter search. Note further that these rules leads to the standard scaling of the variance of the weights, namely $\sigma_w^2 \propto 1/k$.

Taking $-\log$ of \eqref{eq:probabilityofg1} we get that
\begin{align*}
\chi^\#(\dist_\xv, g_1, \e) 
&\leq \frac{a}{2\sigma_w^2} -\frac{k-1}{2}\log(a) - \log(\frac{2\pi^{k/2}}{\Gamma(k/2)}) - \log(\e') \\
&= \frac{C(g_1)}{2\sigma_w^2} - \poly(k,\log(a),\log(\e')) \\
&= \frac{(k-1)C(g_1)}{4} - \poly(k,\log(a),\log(\e'))
\end{align*}
Why do we have an inequality instead of equality in the first line? The reason is that
there can be other functions that still count towards the probability but are of different form. We will show however that these other functions contribute negligibly to $\chi^\#$.

So consider a second function, call it $f_d$ which in addition to the one change of its derivative also has some other \say{wiggles}, so that the total number of slope changes is $d$. Necessarily its complexity is higher, call it $a'$. Consider the probability of functions close to $f_d$. Recall that $a'>a$. Hence the squared radius of the sum of the squares of the outgoing weights \comm{GG}{Is radius $a$ or $2a$} representing $f_d$ is $a'>a$. Thus the density of $\alpha(\theta)$ is smaller for these functions than for $g_1$. But there is a second, even more important, factor that reduce the probability of $f_d$ compared to $g_1$. Not all the points on the sphere of square radius $a'$ correspond to exact representations of $f_d$. In fact, the points that correspond to exact representations of $f_d$ lie on a lower dimensional sub-manifold, reducing the probability further significantly. We claim that this probability has the form
\begin{align*}
   S(f, k-d) \cdot e^{- \frac{2 a}{ 2 \sigma_w^2}} \cdot \epsilon^{d} \cdot e^{-\frac{1}{ \sigma_y^2} A(f, \samp)},
\end{align*}
where $S(f, k-d)$ denotes the area of the subset of the sphere in $k$ dimensions that represents this function exactly, seen as a $k-d$ dimensional manifold. 

All this is easiest seen by looking at the case $k=3$. As we discussed, for $f_1$ the set of exact representations forms a sphere in $\R^3$ since all three nodes can be used to implement this change but there is one global constraint. This leads to the spherical shell of approximate representations as seen in Figure~\ref{fig:sphericalshell}. Next consider $f_2$, which contains one more change of the derivative. At least one of the three nodes will be needed to implement this second change of the slope, leaving only two nodes to implement the main change of the derivative.
So the exact representations will now lie on a slightly bigger sphere of squared radius $2 a'$ in $\R^3$ and they will correspond to circles of squared radius $2 a$ (each such of the six circles corresponds to picking two of the three nodes and implementing the change of the slope already present in $f_1$ via these two nodes, hence a circle since we can freely distribute the weight between these two nodes). If we \say{expand} one such circle by allowing representations \say{close by} we get a torus.
Hence the approximate implementations will now correspond to tori that are embedded on the surface of a sphere. This is shown in Figure~\ref{fig:tori}. As mentioned, the sphere itself is slightly larger, due to the larger complexity of the function. \comm{GG}{Here we kind of need that chi distribution business} But as we discussed, even if we took the whole spherical shell that corresponds to this slightly larger sphere it would contain less probability. In addition, we only take a small subset of the probability, due to the extra combinatorial constraint. In summary, if we assume that we sample from the posterior distribution, there is a strong regularization term that gives preference to \say{simple} functions, hence avoiding over-fitting.

\begin{figure}[tb]
\begin{center} \includegraphics[width=8cm]{eps/tori.eps}
\end{center}
\caption{The six tori indicate the region of the parameter space (outgoing weights) that approximately represent the function $f_2$, a function with two changes of the derivative. Note that the exact representations correspond to six circles, i.e., it has dimension $1$, whereas for a single change of the derivative it has dimension $2$. If we want to implement a function with three changes of the derivative, then the exact representations are points (dimension $0$).}
\label{fig:tori} 
\end{figure}

So far we considered a simplified model where we only looked at the outgoing weights. But not much changes if we look at the outgoing {\em and} incoming weights.  The manifold that contains the exact representations is now embedded in $2k$-dimensional instead of $k$-dimensional space since
for each node the incoming weight is identical to the outgoing weight. If we apply a suitable unitary transform we see that we can get to essentially the original case but where now all the coordinates that are involved in the manifold are stretched by a factor $\sqrt{2}$, and of course we are in an ambient space of dimension $2k$ instead of $k$. E.g., if we consider the function $f_1$ then 
the associated posterior probability of all it's approximate realization is proportional to  
\begin{align*}
   (2a)^{\frac{k-1}{2}} \frac{2 \pi^{k/2}}{\Gamma(k/2)} \cdot \epsilon^{k+1} \cdot e^{-\frac{1}{ \sigma_y^2} A(f, \samp) - \frac{1}{ \sigma_w^2} a}.
\end{align*}

Let us write this as
%\begin{align*}
%    \frac{2\Gamma(k) (\sigma_w^2)^k \pi^{k/2}}{\Gamma(k/2)}   \underbrace{\left( \frac{(2 a)^{k-1} e^{-\frac{(2a)}{ 2\sigma_w^2}}}{{
%    (2 \sigma_w^2)^{k} \Gamma(k)}} \right)}_{(**)} \cdot (\frac{\epsilon}{\sqrt{2a}})^{k-1} 
%    \epsilon^2 
%    e^{-\frac{1}{ \sigma_y^2} A(f, \samp)}.
%\end{align*}
\begin{align*}
    2(2\sigma_w^2)^{k/2-1} \pi^{k/2}   \underbrace{\left( \frac{(\sqrt{2 a})^{k-1} e^{-\frac{(2a)}{ 2\sigma_w^2}}}{{
    (2 \sigma_w^2)^{k/2-1}} \Gamma(k/2)} \right)}_{(**)} \cdot 
    \epsilon^{k+1} 
    e^{-\frac{1}{ \sigma_y^2} A(f, \samp)}.
\end{align*}
%Recall that the density of a chi-distribution with $k$ degrees of freedom is
%\begin{align*}
%\frac{x^{k-1} e^{-x^2/2}}{2^{k/2-1} \Gamma(k/2)}.
%\end{align*}

\end{document}
